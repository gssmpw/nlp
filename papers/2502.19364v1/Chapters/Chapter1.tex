%----------------------------------------------------------------------------------------
%
% Pathologie numérique et apprentissage profond
%
%----------------------------------------------------------------------------------------
\chapter{State Of The Art For Time Series Analysis: Supervised and Unsupervised Learning} 
\label{chapitre_1}



%----------------------------------------------------------------------------------------
%
% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}
\newcounter{mydefinitioncounter}
\newcommand\mydefinition{\stepcounter{mydefinitioncounter}\paragraph*{Definition \arabic{mydefinitioncounter}}}
%
%----------------------------------------------------------------------------------------



%----------------------------------------------------------------------------------------
%
% Pathologie numérique
%
%----------------------------------------------------------------------------------------


\section{Introduction}

Time series analysis, a critical aspect of data science, leverages both supervised and unsupervised
learning methods to extract meaningful insights from time-dependent data. In supervised learning, 
the goal is to predict future values based on past observations. This includes tasks such as extrinsic regression, 
where continuous future values are forecasted, and classification, where future events are categorized 
into predefined classes. Examples include predicting stock prices or classifying email as spam or regular 
based on historical data.
Unsupervised learning, on the other hand, involves discovering inherent structures or patterns within 
the data without predefined labels. Key tasks include clustering, where similar data points are grouped 
together, and anomaly detection, which identifies unusual patterns that deviate from the norm. 
Applications of these techniques range from segmenting customers based on purchasing behavior 
to detecting fraudulent transactions in financial systems. By employing both supervised and unsupervised 
learning, time series analysis can effectively address a wide array of predictive and descriptive tasks, 
driving informed decision-making across various fields.

In the rest of this chapter, we will detail the state-of-the-art literature for both cases.
For supervised learning, we will explore classification and extrinsic regression techniques, while for 
unsupervised learning, we will focus on clustering, prototyping, and self-supervised methods. 
This comprehensive review aims to provide a thorough understanding of the latest advancements 
and applications in time series analysis.

\section{Supervised Learning: Time Series Classification and Extrinsic Regression}

This subsection covers two main tasks: classification, which categorizes time series data into predefined 
classes, and extrinsic regression, which predicts continuous values. We will review state-of-the-art models and 
techniques for these tasks, discussing their applications, strengths, and limitations.

\subsection{Time Series Classification}

The task of TSC has been addressed for the last three decades in various approaches ranging from distance
based approaches to recent deep learning methods.
Such type of data can be found in various domains ranging from human activity recognition~\cite{human-motion-example-paper} to wireless communication~\cite{bertalanivc2022resource}.
With the availability of new TSC datasets, a significant amount of models has been proposed in the literature.
Collecting such data and preprocessing them to become available for benchmarking is not a simple task, for this reason
the UCR/UEA~\cite{ucr-archive,uea-archive} archives had such a significant impact in the last decade on the amount of research in the TSC field.

In this section, we present the prerequisite definitions needed to understand all the materials.
We follow these definitions by an extensive detail view over some state-of-the-art models in both deep and non deep learning methods for TSC.

\mydefinition A Univariate Time Series (UTS) $\textbf{x} = \{x_1,x_2,\ldots,x_L\}$ is a sequence of ordered real values.
The length of this sequence is $L$.
\mydefinition A Multivariate Time Series (MTS) of $M$ dimensions (also referred to as channels) $\textbf{x}=\{\textbf{x}^1,\textbf{x}^2,\ldots,\textbf{x}^M\}$ is a set of $M$ univariate time series of length $L$,
where $\textbf{x}^m=\{x_1^m,x_2^m,\ldots,x_L^m\}$ is a univariate series of length $L$
and $\textbf{x}_t=\{x_t^1,x_t^2,\ldots,x_t^M\}$ a one dimensional vector of shape $(M,)$, $m~\in~[1,M]$ and $t=[1,L]$.
\mydefinition A TSC dataset $\mathcal{D}=\{\textbf{x}_i,\textbf{y}_i\}_{i=1}^N$
is a collection of $N$ pairs of time series and their corresponding label $\textbf{y}_i$ where
$\textbf{x}_i=\{\textbf{x}_{i,1},\textbf{x}_{i,2},\ldots,\textbf{x}_{i,L}\}$ is an MTS of $M$ dimensions and length $L$.
The label $\textbf{y}_i$ is a vector of length $C$ where $C$ is the number of possible classes in $\mathcal{D}$.
Each element $c~\in~[1,C]$ in $\textbf{y}_i$ is one if $\textbf{x}_i$ belongs to class $c$ and zero otherwise.

The task of TSC comes down to constructing a model $\mathcal{F}$ that can achieve correct predictions of 
labels associated to each time series in the dataset.
This is done by teaching the model how to predict a discrete probability distribution of $C$ elements with the goal of having the highest probability assigned to the correct class.
\begin{equation}\label{equ:classif-proba}
    \mathcal{F}(\textbf{x}) = [p_1,p_2,\ldots,p_C]
\end{equation}
\noindent where $\sum_{c=1}^C p_c = 1$ and $0 \leq p_c \leq 1$.

For many years, the most famous approach known in the literature to address TSC was the use of Nearest Neighbor (NN) coupled
with Dynamic Time Warping (DTW) similarity measure and was used as a baseline~\cite{bakeoff-tsc-1}.
Some work also tried to address an adaptation of Support Vector Machines (SVMs)~\cite{svm-paper} for TSC, such as the usage 
of edit distance kernels~\cite{marteau2014recursive, cuturi2007kernel}.
Ever since the release of the first TSC review by~\cite{bakeoff-tsc-1}, much more classifiers have been published.
With the rise of available data,~\cite{dl4tsc} presents a detailed review over all deep learning models from the literature
addressed for the task of TSC and evaluated them on the UCR/UEA archives.
The 2019 deep learning for TSC review~\cite{dl4tsc} highlighted the importance of deep learning models that were 
missed in the TSC review of~\cite{bakeoff-tsc-1}, showcasing their competitive performance
with non-deep learning models.
Moreover, the number of TSC models have increased significantly, which led to the second TSC review~\cite{bakeoff-tsc-2}.
The models in the literature can be divided into eight different sections based on the method used to solve the TSC task.
These sections, presented in Figure~\ref{fig:bakeoff}, are: \textbf{distance based methods}, \textbf{feature based methods}, \textbf{interval based methods},
\textbf{dictionary based methods}, \textbf{convolution based methods}, \textbf{shapelet based methods}, \textbf{hybrid based methods}
and \textbf{deep learning based methods}.
\begin{figure}
    \centering
    \caption{Eight sections of \protect\mycolorbox{0,66,58,0.25}{Time Series Classification} models from the literature:
    \protect\mycolorbox{181,133,199,0.35}{shapelet based},
    \protect\mycolorbox{243,102,74,0.35}{convolution based},
    \protect\mycolorbox{0,189,199,0.35}{distance based},
    \protect\mycolorbox{0,189,87,0.35}{feature based},
    \protect\mycolorbox{0,58,199,0.35}{interval based},
    \protect\mycolorbox{191,130,120,0.35}{dictionary based},
    \protect\mycolorbox{243,120,199,0.35}{hybrid based} and 
    \protect\mycolorbox{255,0,0,0.35}{deep learning based}.
    }
    \includegraphics[width=\textwidth]{Figures/chapter_1/TSC/bake_off/bakeoff.pdf}
    \label{fig:bakeoff}
\end{figure}

In this section, we go through some approaches of solving the task of TSC with non-deep learning methods.

\subsubsection{Distance Based Methods}\label{sec:tsc-distance}

In this section we present the distance based methods to solve the TSC task which utilizes measures such as DTW or MSM~\cite{msm-distance} etc.

\paragraph{$k$-Nearest Neighbor - Dynamic Time Warping ($k$-NN-DTW) and Variants}

As mentioned before, the most famous method to solve TSC was based on the $k$-NN algorithm coupled with a similarity measure.
While for other types of data $k$-NN is coupled with the Euclidean Distance (ED) presented in Eq.~\ref{equ:ED}, it does not capture the temporal
aspect of time series.
\begin{equation}\label{equ:ED}
    ED(\textbf{x}_1,\textbf{x}_2) = \sqrt{\sum_{t=1}^L\sum_{m=1}^M (x^m_{1,t}-x^m_{2,t})^2}
\end{equation}
For instance if we have two time series $\textbf{x}_1=[1,1,0,0,0,1,0]$ and $\textbf{x}_2=[0,1,1,0,0,0,1]$, ED would 
produce a value of $2$ however the series are identical with a simple shift of one time stamp between them.
For this reason, DTW was proposed in order to capture this kind of temporal distortion.
DTW finds the optimal alignment path between two time series before applying the Minkowski (ED if $q=2$) over the aligned series.
The mathematical formulation of DTW is as follows:
\begin{equation}\label{equ:dtw}
    DTW_q(\textbf{x}_1,\textbf{x}_2) = \min_{\pi\in\mathcal{A}(\textbf{x}_1,\textbf{x}_2)}(\sum_{(t_1,t_2)\in\pi}\sum_{m=1}^M(x^m_{1,t_1}-x^m_{2,t_2})^q)^{1/q}
\end{equation}
\noindent where $\pi$ is an alignment path of length $L_{\pi}$ and is a sequence of $L_{\pi}$ pairs of indices
$[(t_{11},t_{21}),(t_{12},t_{22}),\ldots,(t_{1,L_{\pi}},t_{2,L_{\pi}})]$.
$\mathcal{A}(\textbf{x}_1,\textbf{x}_2)$ is the set of all acceptable paths between the two series.
A path $\pi$ is considered acceptable if:
\begin{enumerate}
    \item Start and ending point match the ones of the series:
    \begin{itemize}
        \item $\pi_1 = (1,1)$
        \item $\pi_{L_{\pi}} = (L_1,L_2)$
    \end{itemize}
    \item The sequence is monotonically increasing:
    \begin{itemize}
        \item $t_{1,l-1} \leq i_l \leq t_{1,l-1}+1$
        \item $t_{2,l-1} \leq j_l \leq t_{2,l-1}+1$
    \end{itemize}
    where $l\in[1,L_{\pi}]$
\end{enumerate}
The distance is usually set to the squared error so $q=2$.
A detailed view of the DTW algorithm is presented in Algorithm~\ref{alg:dtw}.
As presented in the detailed algorithm, for each element in the distance matrix, the squared error between the current time stamps first fills the matrix's cell.
Second, at each cell, the smallest element between its three neighbors is added, the upper neighbor indicates inserting a time stamp from one series to another, the left neighbor indicates deleting an element from one series and the bottom neighbor indicates that these two time stamps are aligned so no need for an operation.
The time complexity of the DTW algorithm is $\mathcal{O}(L_1.L_2)$ and $\mathcal{O}(L^2)$ if both series are of the same length.
This complexity is considered very high, and when coupled with NN the whole complexity is $\mathcal{O}(N_{train}.N_{test}.L^2)$,
however some work has optimized such complexity by defining a lower bound for DTW~\cite{lb-keogh-paper,lb-webb-paper}.

\begin{algorithm}
    \caption{Dynamic Time Warping (DTW)}
    \label{alg:dtw}
    \begin{algorithmic}[1]
        \REQUIRE Two Time Series $\textbf{x}_1$ and $\textbf{x}_2$ of length $L$ and dimension $M$
        \ENSURE DTW measure between \(\textbf{x}_1\) and \(\textbf{x}_2\)
        
        \STATE $D = array [L+1,L+1]$
        \FOR{\(t_1 = 1\) to \(L+1\)}
            \FOR{\(t_2 = 1\) to \(L+1\)}
                \STATE $D[t_1,t_2] = +\infty$
            \ENDFOR
        \ENDFOR

        \STATE D[0,0] = 0.0

        \FOR{\(t_1 = 2\) to \(L+1\)}
            \FOR{\(t_2 = 2\) to \(L+1\)}
                \STATE $cost = \sum_{m=1}^M(x^m_{1,t_1-1}-x^m_{2,t_2-1})^2$
                \STATE $up\_insertion = D[t_1-1,t_2]$
                \STATE $left\_deletion = D[t_1,t_2-1]$
                \STATE $diagonal\_match = D[t_1-1,t_2-1]$
                \STATE $D[t_1,t_2] = cost + min(up\_insertion,left\_deletion,diagonal\_match)$
            \ENDFOR
        \ENDFOR

        \STATE \textbf{Return:} $D[L+1,L+1]$
        
    \end{algorithmic}
\end{algorithm}

An example of DTW alignment path computation between two time series of the \texttt{ItalyPowerDemand} dataset of the UCR archive~\cite{ucr-archive} is presented in Figure~\ref{fig:dtw-example-uni}.
\begin{figure}
    \centering
    \caption{Example of DTW alignment path computation between two series 
    (\protect\mycolorbox{255,0,0,0.5}{in red} and 
    \protect\mycolorbox{0,0,255,0.5}{in blue}) from the 
    \texttt{ItalyPowerDemand} dataset of the UCR archive.
    The DTW optimal alignment path between both series 
    is presented \protect\mycolorbox{128,128,128,0.7}{in gray}.
    }
    \label{fig:dtw-example-uni}
    \includegraphics[width=0.7\textwidth]{Figures/chapter_1/TSC/dtw/dtw-matrix-ItalyPowerDemand.pdf}
\end{figure}
To showcase the need of a DTW alignment instead of simply using a Euclidean Distance that assume a perfect alignment, we present in Figure~\ref{fig:dtw-ed-alignment} for the same series used in Figure~\ref{fig:dtw-example-uni} both the perfect alignment that ED assumes vs the DTW alignment.
\begin{figure}
    \centering
    \caption{DTW optimal \protect\mycolorbox{0,128,0,0.7}{alignment} vs the ED's assumption of a perfect \protect\mycolorbox{0,128,0,0.7}{alignment} on two time series of the \texttt{ItalyPowerDemand} dataset of the UCR archive.}
    \label{fig:dtw-ed-alignment}
    \subfloat[Alignment using ED]{\includegraphics[width=0.5\textwidth]{Figures/chapter_1/TSC/dtw/ItalyPowerDemand/ed-alignment.pdf}}
    \subfloat[Alignment using DTW]{\includegraphics[width=0.5\textwidth]{Figures/chapter_1/TSC/dtw/ItalyPowerDemand/dtw-alignment.pdf}}
\end{figure}

The DTW measure is then utilized to calculate the similarity between each testing sample to all training samples, the predicted label for the test sample is the same as its $k$ nearest neighbors following the used similarity measure.
This algorithm has been developed over the years by simply fine-tuning the parameters of DTW, or by changing the similarity measure.
For instance, many versions of DTW have been proposed over the years, such as SoftDTW~\cite{soft-dtw-distance} and ShapeDTW~\cite{shape-dtw-distance}.

The SoftDTW~\cite{soft-dtw-distance} version addresses the issue of differentiability of DTW
especially because of the minimization step in Algorithm~\ref{alg:dtw}.
The authors of SoftDTW~\cite{soft-dtw-distance} argues the need of a differentiable 
DTW in order to be able to construct an optimization problem used for many applications such as clustering and deep learning.
SoftDTW solved this issue by replacing the $hard-\min$ operation by a $soft-\min$ operation.
The $soft-\min$ operation is used as follows:
\begin{equation}\label{equ:soft-min}
    soft-\min~^{\gamma}(a_1,a_2,\ldots,a_N) = -\gamma.\log\sum_{i=1}^N e^{-a_i/\gamma}
\end{equation}
\noindent where $\gamma$ is the smoothing factor, and as it tends to the value $0^{+}$, then the $soft-\min$ becomes the $hard-\min$ hence SoftDTW becomes the original DTW.

In~\cite{shape-dtw-distance}, a variation of DTW was introduced, which aligns transformations of
sub-sequences within time series instead of aligning all time series simultaneously.
This approach aims to maintain the consideration of neighborhood structure when
aligning timestamps across different time series. To define ShapeDTW mathematically,
let $\mathcal{F}$ be a descriptor function, $\textbf{x}_1$ and $\textbf{x}_2$ be two MTS of length $L$ and dimension $M$.
The process begins by extracting sub-sequences over all channels of length
$r$ (referred to as reach) and transform them using a descriptor $\mathcal{F}: \mathds{R}^r~\to~\mathds{R}^d$.
This results in two new MTS $\mathcal{D}_1$ and $\mathcal{D}_2$ of length $L$ and dimension $M.d$, associated to $\textbf{x}_1$
and $\textbf{x}_2$ respectively.

The DTW alignment path is then computed on the transformed version of the series $\mathcal{D}_1$ and $\mathcal{D}_2$,
followed by the optimal path being transferred onto the original series space to calculate the measure between
the original time stamps instead of the sub-sequences.
In this manner, the DTW algorithm will calculate the distance between time stamps following their neighborhood alignments.
The ShapeDTW measure can be formulated as the following optimization problem:
\begin{equation}\label{equ:shape-dtw}
    ShapeDTW_q(\textbf{x}_1, \textbf{x}_2) = (\sum_{(t_1,t_2)\in\pi^{*}}\sum_{m=1}^M(x^m_{1,t_1}-x^m_{2,t_2})^q)^{1/q}
\end{equation}
\noindent where $\pi^{*}$ is the optimal path obtained by the DTW alignment path between transformed series as follows:
\begin{equation}\label{equ:shape-dtw-path}
    \pi^{*} = arg\min_{\pi\in\mathcal{A}(\mathcal{D}_1,\mathcal{D}_2)}(\sum_{(\tilde{t}_1,\tilde{t}_2)\in\pi}\sum_{m=1}^M(\mathcal{D}^m_{1,\tilde{t}_1}-\mathcal{D}^m_{2,\tilde{t}_2})^q)^{1/q}
\end{equation}
The distance is usually set to the squared error so $q=2$.

\paragraph{Elastic Ensemble}

Given the high number of similarity measures for time series data,~\cite{elastic-ensemble} proposed to do a weighted Elastic Ensemble (EE) of $11$ NN classifiers each using a different similarity measure.
Below, we define what an ensemble of classifiers is, a concept that will be used throughout the rest of this work.
\mydefinition Ensembling different classifiers is motivated by the idea that combining multiple opinions 
often leads to a more robust decision. Since each classifier generates a probability distribution for 
each series across the possible classes, the ensemble method averages these probability distributions 
from all classifiers.

\paragraph{Proximity Forest and Proximity Forest2.0}

Proximity Forest (PF)~\cite{proximity-forest} is a Random Forest (RF)~\cite{random-forest} classifier adaptation for the time series classification task.
PF utilizes the same $11$ similarity measures that EE~\cite{elastic-ensemble} uses, however it randomly sets at each branch one of the similarity measures to be used for the fed time series.
Until 2023, PF was the state-of-the-art distance based classifier for the task of TSC on the UCR archive~\cite{ucr-archive} following the recent TSC review~\cite{bakeoff-tsc-2}.
However, recently, the same group that developed PF upgraded the algorithm and developed
PF2.0~\cite{proximity-forest-2} and it was significantly better than PF.
PF2.0 differs from the original PF by three main features: (1) being efficiently better than much 
faster, (2) the addition of a new similarity measure Amerced Dynamic Time Warping (ADTW)~\cite{adtw-distance}
and (3) tuning the parameters of the cost function.

\subsubsection{Feature Based Methods}\label{sec:tsc-feature}

Using traditional machine learning classifiers such as RF~\cite{random-forest} or RIDGE classifier~\cite{hoerl1970ridge}
is insufficient on raw time series data as these classifiers are constructed to use tabular input.
In order to overcome this issue, some feature based classifiers were proposed for TSC which consist 
of a pipeline of feature extraction methods followed by a simple classifier designed for tabular data.
In what follows, we present briefly some state-of-the-art feature based methods of TSC.

\paragraph{The Canonical Time Series Characteristics (Catch22)}

Building on the original work of~\cite{hctsa}, which proposed the
\emph{Highly Comparative Time-Series Analysis (hctsa)} tool to extract
around $7700$ features from each time series, the authors in~\cite{catch22} did an extensive amount of 
experiment in order to identify the most effective $22$ \emph{hctsa} features.
This new set of features proposed in~\cite{catch22} is called Catch22, which is then followed by an 
RF classifier~\cite{random-forest}.

\paragraph{Time Series Feature Extraction based on Scalable Hypothesis Tests (TSFresh) and The FreshPRINCE}

TSFresh~\cite{tsfresh} is a set of around $800$ features that are extracted from each time series.
This set of features are not all utilized for the classification, instead, the authors in~\cite{tsfresh} proposed
the usage of a feature selection method called FRESH~\cite{fresh}.
The selected features are then fed into an RF classifier or an AdaBoost classifier~\cite{adaboost}.
The TSFresh features were then used recently by~\cite{fresh-prince} where the authors removed the feature selection method
and utilize a Rotation Forest (RotF) classifier~\cite{rotation-forest} on top of the TSFresh features, to produce the FreshPRINCE feature-based classifier.

Until now, the FreshPRINCE classifier is the state-of-the-art feature-based method for TSC on the UCR archive following the recent TSC review~\cite{bakeoff-tsc-2}.


\subsubsection{Convolutional Based Methods}\label{sec:tsc-convolution}

Convolution based approaches have shown to be very effective on image classification since the birth of 
Convolutional Neural Networks (CNNs)~\cite{lecun2015deep}.
In the case of images, convolution filters are two-dimensional operation where the filter slides all over the image
in order to extract some meaningful features.
However, in the case of time series, the convolution operation is one dimensional and slides all over the temporal axis
of the time series in order to extract temporal features and local dependencies.

\mydefinition A one-dimensional convolution operation over a univariate time series $\textbf{x}$ of
length $L$ with a kernel $\textbf{w}=\{w_1,w_2,\ldots,w_K\}$ of length $K$ is defined as follows:
\begin{equation}\label{equ:convolution1d-uni}
    o_t = \sum_{k=1}^K x_{t+k-1}.w_k
\end{equation}
\noindent with $t~\in~[1,L-K+1]$ and $\textbf{o}=\textbf{x}*\textbf{w}=\{o_1,o_2,\ldots,o_{L-K+1}\}$ is the output
series of the one-dimensional convolution and $*$ is the convolution operator.
Whenever a value in $\textbf{o}$, representing a segment in $\textbf{x}$ of length $K$, is positive, it is referred to 
as the convolution kernel being activated at that segment in $\textbf{x}$, thus a pattern is detected.
\emph{It is important to note that the above definition of a 1d convolution operation uses a stride of $1$, which represents the amount 
of time stamps the convolutional kernel shifts when sliding on the temporal axis.
By default, in the rest of this work, all convolutional operations use a stride of $1$.}

In the above approach, the convolution is being applied to a consecutive set of time stamps.
However, it can be interesting to extract features of time stamps with wider temporal distance between them.
This can be done by simply increasing the length of the convolution kernel, however it would 
increase the number of parameters used.
A more constructive approach is to use dilated convolution to increase the view of 
the kernel over the time series sample.

\mydefinition Dilated one-dimensional convolution between a series $\textbf{x}$ and a kernel $\textbf{w}$ of
lengths $L$ and $K$ respectively with a dilation rate $d > 1$ is defined as follows:
\begin{equation}\label{equ:convolution1d-dilation}
    o_t = \sum_{k=1}^K x_{t+(k-1).d}.w_k
\end{equation}
\noindent where $t~\in~[1, L-(K-1).d]$ and $\textbf{o}$ the output
series of the one-dimensional dilated convolution.
If $d = 1$ then this comes down to applying the convolution as in Eq.~\ref{equ:convolution1d-uni}.
The dilation rate allows the convolution operation to skip some elements in the input series to detect longer patterns.

\paragraph{RandOm Convolutional KErnel Transform (ROCKET)}
\cite{rocket}
% , Angus Dempster proposed the first convolutional based approach for TSC that is both accurate and efficient.
% The work in~\cite{rocket} 
proposed ROCKET, a convolution based model that randomly generates a large set of kernels
following a standard Gaussian distribution $\mathcal{N}(0,1)$, with random dilation rates and random 
biases sampled from a uniform distribution $\mathcal{U}(-1,1)$.
This set of filters is then applied on each of training time series samples followed by two aggregation functions.
The first aggregation is choosing the maximum value of the convolution output and the second is taking the Proportion
of Positive Values (PPVs).
The PPV is obtained as detailed in Eq.~\ref{equ:ppv}:
\begin{equation}\label{equ:ppv}
    PPV(\textbf{o} = \textbf{x}*\textbf{w}) = \dfrac{1}{L-K+1}\sum_{t=1}^{L-K+1} \mathds{1}[\textbf{o}_t > 0]
\end{equation}
\noindent $\mathds{1}[condition]$ is the indicator function defined as:
\begin{equation}
    \mathds{1}[condition]=
    \begin{cases}
        1 & if\text{ }condition\text{ }is\text{ }True\\
        0 & if\text{ }condition\text{ }is\text{ }False
    \end{cases}
\end{equation}

Assuming that ROCKET uses $\mathcal{K}$ convolution filters, the output space dimension is 
$2\mathcal{K}$ per time series sample. This latent space of the training set is then used 
to optimize the parameters of a RIDGE classifier~\cite{hoerl1970ridge}. A unique feature of 
ROCKET, compared to other classifiers, is that it is entirely independent of the training 
dataset during its feature extraction phase,
with its parameters being randomly generated. ROCKET's computational runtime is 
significantly smaller than that of other state-of-the-art classifiers, and it has consistently 
been one of the top-performing models in the literature across widely used community 
benchmarks~\cite{ucr-archive,uea-archive}.

\paragraph{MiniROCKET \& MultiROCKET}

The same authors of ROCKET~\cite{rocket} proposed in 2021 a new version called MiniROCKET~\cite{mini-rocket} that is almost
a deterministic version of the original model in order to reduce its randomness and add some dependency with the input data.
The key differences between ROCKET and MiniROCKET can be summarized in the following:
\begin{enumerate}
    \item MiniROCKET fixed the length of the filters to $9$
    \item MiniROCKET randomly generates the values of the filters from a discrete set of values $\{-1,2\}$ instead of using
a Gaussian distribution
    \item MiniROCKET drops the maximum aggregation and utilizes only the PPV
    \item MiniROCKET samples the bias values from the quantiles of the convolution output, making it dependent on the training data
    \item MiniROCKET fixed the number of possible dilation rates from $1$ to $\log_2(\dfrac{L-1}{K-1})$ where $L$ is the time series
    length and $K$ the kernel length.
\end{enumerate}
MiniROCKET highlights that by reducing the degree of freedom of ROCKET, than both the accuracy and efficiency can increase.

The same group proposed MultiROCKET in the following years in~\cite{multi-rocket}, that utilizes the MiniROCKET setup however
it applies the transformation over the original time series and its first order derivative.
MultiROCKET does not rely only on the PPV features however it produces three new features:
\begin{enumerate}
    \item Mean of Positive Values (MPV) that averages the positive values of the convolution output
    \item Mean of Indices of Positive Values (MIPV) that averages the indices of the positive values of the convolution output
    \item Longest Stretch of Positive Values (LSPV) that finds the length of the longest subsequences containing positive consecutive values in the convolution otuput
\end{enumerate}
MultiROCKET adds some computation complexity to MiniROCKEt, however it achieved state-of-the-art results over the UCR archive
for the TSC task in 2022.

\paragraph{HYbrid Dictionary-Rocket Architecture}

In 2023, a new adaptation of ROCKET based framework was proposed in~\cite{hydra} called HYDRA.
Unlike ROCKET, HYDRA does not rely on the actual output activation of the filter, instead it leverages over how many times
a convolution kernel is activated the most between a set of kernels.
In other words, HYDRA randomly defines a set of kernels, called group and applies the convolution such as in ROCKET
followed by assigning each kernel in this group the number of time stamps it is activated the most in the group.
HYDRA employs $\mathcal{G}$ groups with $\mathcal{K}$ kernels in each group, resulting in an output feature space of dimension
$\mathcal{G}$x$\mathcal{K}$ containing integer values.
This feature space is subsequently used to train a RIDGE classifier~\cite{hoerl1970ridge}.
\cite{hydra} concluded that by combining the feature space of HYDRA with the feature space of MultiROCKEt,
resulting in HydraMR (HYDRA-MultiROCKET), achieves state-of-the-art performance for the task of TSC on the UCR archive~\cite{ucr-archive}.

HydraMR is currently one of the state-of-the-art models for TSC, not only in convolution based methods, 
but overall as well.

\subsubsection{Shapelet Based Methods}\label{sec:tsc-shapelet}

Shapelet-based time series classification methods focus on identifying and using small, discriminative
subsequences, known as shapelets, to distinguish between different classes. These methods extract shapelets
that capture local patterns highly indicative of the target class, providing interpretable and precise models.
This approach is particularly useful in applications like medical diagnosis, where specific patterns in data can
be crucial for accurate classification. Shapelet-based classifiers are valued for their robustness and interpretability,
making them a powerful tool in time series analysis.

Shapelets were first introduced in~\cite{image-countour-shapelets-paper,shapelets} as discriminative subsequences used within decision tree
classifiers for time series classification. Since then, the research community in TSC has extensively developed
and expanded this concept, leading to a variety of advanced algorithms and applications that leverage shapelets
for improved accuracy, interpretability, and computational efficiency.

\paragraph{Shapelet Transform Classifier (STC)}

The STC~\cite{stc} is a two steps classifier. First the model searches for shapelets in the set 
of training samples and transforms the series
to a vector of distances between the shapelet and a set of other shapelets from the series itself.
Second, a decision tree classifier is trained on top of the transformed space of the series.

\paragraph{Random Dilated Shapelet Transform (RDST)}

The RDST model~\cite{rdst}, motivated by ROCKET~\cite{rocket}, leverages from the randomness techniques to select the shapelets 
from the training samples.
Instead of learning the shapelets, RDST randomly selects a high number of shapelets from the training data.
Similar to convolution based methods, RDST employs the dilation technique to enrich the shapelet transform.
The transformed space of RDST is then used to train a RIDGE classifier~\cite{hoerl1970ridge}.

Currently, RDST is still the state-of-the-art shapelet based methods for TSC evaluated on the UCR archive.

\subsubsection{Dictionary Based Methods}\label{sec:tsc-dictionary}

This approach of solving TSC is based on finding discriminative patterns in the time series and counting the number of times 
it was repeated, followed by using this information to train a classifier.
The patterns detected are not from the raw input, instead the time series is transformed first into a discrete space using a
symbolic transformation.
A very famous symbolic transformation proposed in~\cite{sax} called Symbolic Aggregate approXimation (SAX) defines a set of
discrete symbols that represent a segment of the time series.
SAX employs this symbolic transformation as follows:
\begin{itemize}
    \item \textbf{First}, each time series $\textbf{x}$, supposing being univariate, of length $L$ is z-normalized to have a zero mean and unit standard deviation.
    \item \textbf{Second}, the time series is divided into non-overlapping segments of length $l$ each:
    $\{\textbf{x}[(t-1).l:t.l]\}_{t=1}^{\lfloor L/l \rfloor}$
    \item \textbf{Third}, each segment is replaced by its mean value following the Piecewise Aggregate Approximation (PAA)~\cite{paa}
    dimensionality reduction technique to obtain:
    $\{p_t=mean(\textbf{x}[(t-1).l:t.l])\}_{t=1}^{\lfloor L/l \rfloor}$
    \item \textbf{Fourth}, the dictionary of symbols $\mathcal{D}_{ict}=\{s_1,s_2,\ldots,s_{\alpha}\}$ is defined for specific number of alphabet $\alpha$ (a SAX hyper-parameter)
    using the percent point function of the standard Gaussian distribution to obtain $\alpha-1$ breakpoints $\{b_j\}_{j=1}^{\alpha-1}$
    \item \textbf{Finally}, for each segment $\textbf{x}[(t-1).l:t.l]$ for $i~\in~[1,\lfloor L/l \rfloor]$
    replaced by its mean value, a one-to-one mapping function is used to choose 
    the replacement symbol from the dictionary as follows:
    \begin{equation}\label{equ:sax}
        SAX(p_i) = 
        \begin{cases}
            s_1 & if~-\infty < p_i \leq b_1\\
            s_2 & if~b_1 < p_i \leq b_2\\
            \ldots & \\
            s_{\alpha} & if~b_{\alpha-1} < p_i < +\infty
        \end{cases}
    \end{equation}
\end{itemize}

The above steps can be applied in the same way on MTS data, by going through each dimension independently.
The core idea of SAX is simply representing each segment of the series by a discrete 
symbol to form a word (sequence of symbols) that is chosen following
the Gaussian distribution, this is argued by the authors in~\cite{sax} by saying
``\emph{$\ldots$ normalized time series have a Gaussian distribution}``.
This symbolic representation was then used for classification in~\cite{sax-vsm} with the first dictionary based classifier 
for time series, called SAX Vector Space Model (SAX-VSM).
SAX-VSM begins by generating the Symbolic Aggregate approXimation (SAX) representation for all time series 
within each class, while also preserving the frequency of the symbol sequences. When presented with a new, 
unlabeled time series, it undergoes SAX transformation to obtain its symbolic sequence. Then, by comparing 
the frequencies derived from the precomputed set of symbol sequences, the unlabeled series is assigned to 
the class that best matches its sequence frequencies.

In~\cite{sfa}, the authors proposed a novel version, six years after the breakthrough of SAX, called 
the Symbolic Fourier Approximation (SFA).
First, SFA decomposes the series into segments and then z-normalize the sub-sequences instead of normalizing them prior to
the decomposition.
Second, SFA utilizes the Discrete Fourier Transform as a dimensionality reduction technique instead of PAA~\cite{paa}.
Third, SFA uses a binning technique proposed in~\cite{sfa} called Multiple Coefficient Binning (MCB) that is based on the 
distributions of real and imaginary values of the Fourier Transform.
Finally, Those distributions go through the binning mechanism to generate the symbols.

\paragraph{Bag-of-SFA-Symbols (BOSS)}

In~\cite{boss}, the authors proposed a novel approach for dictionary based TSC called BOSS that utilizes on SFA.
BOSS applies the SFA transformation on overlapping windows of the time series instead of considering the whole time series at once.
This results in a sequence of words for each series instead of producing a sequence of symbol (one word only).
A BOSS classifier utilizes a non-symmetric distance in the setup of a NN classifier, and multiple BOSS classifiers are finally ensembled
to form the final BOSS model.
Until 2015, BOSS was the state-of-the-art dictionary based method for TSC evaluated on the UCR archive.

\paragraph{Word Extraction for Time Series Classification (WEASEL1.0 and WEASEL2.0)}

WEASEL1.0 is a novel dictionary based model proposed in~\cite{weasel} for TSC, that in contrary to BOSS,
its goal is to identify meaningful words in the output transformation of SFA.
This is done by applying the transformation using SFA on a large set of possible parameters, followed by a Chi-squared test
to identify the words that have the highest power and discard the words that have a power lower than a specified threshold.
The output space is then used to train a RIDGE classifier~\cite{hoerl1970ridge}.

Although WEASEL1.0 have seen to outperform BOSS on the UCR archive, it still however suffer from the dimensionality curse
and runtime curse because of the large grid search space.
The same authors~\cite{weasel2} proposed a new version denoted by WEASEL2.0 that utilizes the randomness technique of ROCKET
and randomly generate a set of parameters for the SFA transformation thus controlling the searching space.
WEASEL2.0 sets a random dilation rate as well for the windowing phase of the workflow, motivated from the impact of dilation 
on the ROCKET transformation.
WEASEL2.0 became the state-of-the-art dictionary based method for TSC in terms of both accuracy and efficiency.

\subsubsection{Interval Based Methods}\label{sec:tsc-interval}

Interval based methods, first proposed in~\cite{tsf} as an RF~\cite{random-forest} based classifier, is a technique of ensembling 
different classifiers trained on different transformations of extracted intervals from the time series samples.
Most approaches randomly generate the intervals' bounds that are used throughout all the samples in the dataset.
The motivation of using such technique instead of feature based methods where the transformation is done over all the series at 
the same time, is to avoid noisy features that will lead in miss-classification and confusing the classifier.

\paragraph{Time Series Forest (TSF)}

The TSF~\cite{tsf} model employs for each decision tree $\sqrt{L}$ intervals, where $L$ is the time series length, of randomly selected bounds.
For each of the selected intervals, TSF extracts the mean, variance and slope and concatenate them into one feature vector that is then
used to build the decision tree.
All the decision trees are then ensembled through a voting mechanism to form the TSF classifier.

\paragraph{Canonical Interval Forest and Diverse Representation Canonical Interval Forest (CIF and DrCIF)}

Similar to TSF~\cite{tsf}, CIF~\cite{cif} is an ensemble of decision tree classifiers, however it utilizes the Catch22 features
alongside the mean, variance and slope features of TSF.
The concatenated vector is then used to build the decision tree.
CIF leverages over TSF by being suitable for multivariate time series, as it the number of intervals for each tree is
$\sqrt{L}.\sqrt{M}$, where $L$ and $M$ are the length and number of channels of the time series samples respectively.
In order to keep the selected intervals in the one-dimensional space, CIF randomly assigns a channel for each of the selected 
intervals.

In~\cite{hive-cote2.0}, the same authors proposed DrCIF by incorporating two new extracted features alongside the ones of CIF:
(1) the periodograms to identify and quantify the frequency components present within the time series and (2) 
the first order derivative.

\paragraph{QUANT}

In 2023, the QUANT model, as referenced in~\cite{quant}, discarded all previously employed
features identified through interval based methods.
Instead, QUANT relies on quantiles that represent the empirical distribution of the intervals.
However, it extracts quantiles over four different representations including the time series itself, the first and second order
derivative, the Fourier transform.
The choice of the intervals is not random in QUANT, instead it is fixed and dyadic (defined based on the powers of $2$).
For each interval of length $l$, QUANT defines sub-intervals of length $l/4$ and extracts two features called quantiles from 
each sub-interval.
These two features are the median of the sub-interval and the median of zero centered sub-interval (mean of the sub-interval is 
extracted before finding the median).
The output features of all intervals are then concatenated representing a new quantized version of the input time series.
QUANT utilizes extremely randomized trees~\cite{extremely-randomized-trees} for the classification task, where the transformed quantized space is used to train the 
tree based classifier.

In the recent TSC review~\cite{bakeoff-tsc-2}, it has been shown that QUANT is the current 
state-of-the-art interval based method
for TSC evaluated on the UCR archive~\cite{ucr-archive} in both accuracy and efficiency 
as it is significantly faster than other interval based methods.

\subsubsection{Hybrid Based Methods}\label{sec:tsc-hybrid}

Given that time series data does not have a unified approach to address its classification task, it is most of the time a
difficult challenge to choose from the pool of methods.
For this reason, hybrid models have been proposed throughout the literature in a way to combine different methods, e.g.
distance based and interval based methods.

\paragraph{Time Series Combination of Heterogeneous and Integrated Embedding Forest (TS-CHIEF)}

TS-CHIEF~\cite{ts-chief-paper} is a tree-based ensemble method where the nodes within each 
tree perform splits using three distinct feature criteria: distance-based, dictionary-based, 
and spectral interval-based. The parameters are randomly initialized to ensure diversity 
within the ensemble. The distance-based splits are derived from the
EE\cite{elastic-ensemble}, the dictionary-based splits are inspired by BOSS\cite{boss}, 
and the interval-based splits are based on the Random Interval Spectral Ensemble 
(RISE)~\cite{rise}.

\paragraph{HIVE-COTE1.0 and HIVE-COTE2.0}

The HIVE-COTE (HC) method has been developed throughout the years, starting with the Collective Of
Transformation-based Ensemble (COTE)~\cite{cote} which is an ensemble of $35$ time series classifiers of different approaches.
This model was developed to the HIerarchical VotE Collective Of Transformation Ensemble (HIVE-COTE)~\cite{hive-cote} that only utilizes five
classifiers and are ensembled through the  Cross-validation Accuracy Weighted Probabilistic Ensemble (CAWPE)~\cite{cawpe}.
HIVE-COTE utilizes a distance based, dictionary based, shapelet based, interval based and spectral based classifiers.
However, the distance based model used in HIVE-COTE is computationally expensive, for this reason it was dropped in HIVE-COTE1.0~\cite{hive-cote1}
and a more performing dictionary based classifiers is used.
The most recent HC based model is HIVE-COTE2.0~\cite{hive-cote2.0} which changed the set of classifiers to more recent ones that 
are much more performing.
A unique feature of HC2 over HC1.0, HC and COTE, is that the classifiers used in its hybrid ensemble are suitable for multivariate 
datasets.
HC2 is currently one of the state-of-the-art models for TSC evaluated over the UCR archive, not only in hybrid based methods, 
but overall as well.

\subsubsection{Deep Learning Methods}\label{sec:tsc-deep}

Although the previously presented methods for TSC are performing well on the available benchmarks~\cite{ucr-archive,uea-archive},
most of them lack the capability of parallelization of their calculation over GPUs, which can decrease their efficiency.
Another critical limitation of most of these classifiers is their lack of explainability, which is increasingly 
important for understanding model decisions, ensuring transparency, and gaining trust in applications where 
decision-making is crucial.
For these reasons, deep learning methods can be a suitable solution, however we do not claim it should 
be the only solution for TSC 
as there are still some models that can achieve better performance compared to deep learning models 
but are less suitable in terms of scalability.

Deep learning~\cite{lecun2015deep} methods leverage over all the previous techniques with the ability of
parallelization over multiple GPUs making them much faster during training and
inference.
Moreover, deep learning methods can conduct two steps including the feature extraction
and the classification task at the same time instead of manually constructing the features phase
and only training the classifier.
Deep learning methods consist on many neural network architectures, such as Convolutional
Neural Networks (CNNs), Recurrent Neural Networks (RNNs) and Transformers.
However, an extensive review of deep learning methods for TSC has been conducted in~\cite{dl4tsc}
highlighted that CNNs outperform other architectures.

A deep learning model for TSC consists on applying $\Lambda$ parametrized layers of different
characteristics.
Each of the layers $\lambda_i$ where $i~\in~[1,\Lambda]$ represents a function $f_{\lambda_i}$ parametrized by
a set of parameters $\theta_{\lambda_i}$.
Each layer $\lambda_i$ takes as input the output of the previous layer $\lambda_{i-1}$ and applied a non-linear transformation
over it that is controlled by $\theta_i$.
Given an input time series $\textbf{x}$, feeding to a neural network of $\Lambda$ layers comes down to the following pipeline:
\begin{equation}\label{equ:dl-pipeline}
    f_{\Lambda}(\theta_{\Lambda},\textbf{x}) = f_{\Lambda-1}(\theta_{\Lambda-1},f_{\Lambda-2}(\theta_{\Lambda-2},\ldots,f_{1}(\theta_{1},\textbf{x})))
\end{equation}
% \noindent where each of $f_i$ represents a non-linear transformation at layer $\lambda_i$.
The above pipeline is referred to in the community as the feed-forward propagation.

Since the task at hand is classification, the last layer of the deep learning model
outputs a probability distribution for each sample belonging to each of the possible
classes.
The parameters of all the layers are then optimized using the back-propagation algorithm~\cite{rumelhart1986learning}.
In what follows, we present the different types of layers than are used for time series in the literature.

\paragraph{Types of Layers}

In this section, we go through some layer types used in the literature's architectures.
These layers are based on non-linear transformations that consists on either extracting information, detecting some patterns
or combining some features in the time series samples.

\paragraph*{Fully Connected (FC) Layers}
The FC layers are simply a linear transformation followed by applying a non-linear activation
such as ReLU, sigmoid etc.
This linear transformation is computed using matrix multiplication.
For instance, if the input dimension is $n$ and the output dimension is $m$, then the FC
layer consists on weight matrix $W$ of shape $(m,n)$ and the output of the FC layer is computed as follows:
\begin{equation}\label{equ:fc-layer}
    \textbf{o} = \sigma(W\odot \textbf{x} + \textbf{b})
\end{equation}
\noindent where $\textbf{x}$ is the input, $\textbf{o}$ is the output, $W$ is the transformation matrix,
$\textbf{b}$ is the bias vector of dimension $m$, $\sigma(.)$ is a non-linear activation function
and $\odot$ is the matrix multiplication operation.

This type of layer is almost always used as the last layer in a deep learning model for a classification
task while setting the activation function to the softmax function and the output dimension to $C$, the number of possible classes.
This function ensures that the output vector is a probability distribution and each element in the output vector $\textbf{o}$ is computed as follows:
\begin{equation}\label{equ:softmax}
    o_c = \dfrac{e^{W[c,:]\odot\textbf{x} + b_c}}{\sum_{\tilde{c}=1}^{C}e^{W[\tilde{c},:]\odot\textbf{x} + b_{\tilde{c}}}}
\end{equation}
\noindent where $o_c$ is the probability of $\textbf{x}$ belonging to class $c~\in~[1,C]$

In order to find the optimal weights of Eq.~\ref{equ:fc-layer} and~\ref{equ:softmax},
we can use an optimization algorithm to minimize the error in the model's predictions.
This error is measured through a loss function, that should be differentiable given that the
optimization algorithm is gradient based.
The common loss function to be used for the classification tasks is the categorical cross 
entropy, that measures the difference between two probability distributions, defined as follows
on the $i_{th}$ example of the dataset:
\begin{equation}\label{equ:cross-entropy}
    \mathcal{L}_i(\textbf{y}_i,\hat{\textbf{y}}_i) = -\sum_{c=1}^{C}y_{i,c}.\log_{2}(\hat{y}_{i,c})
\end{equation}
\noindent where $C$ is the total number of classes in the dataset, $\textbf{y}_i$ is the ground truth
label of the $i_{th}$ series, denoted as a one hot encoding, e.g. if the ground truth label is $C_2$ out of a set
$\{C_1,C_2,C_3\}$ then $\textbf{y}_i=[0,1,0]$ representing a discrete deterministic probability distribution.
$\hat{\textbf{y}}_i$ is a vector of length $C$ representing a discrete probability distribution where 
each element $\hat{y}_{i,c}$~$c~\in~[1,C]$ is the probability of the $i_{th}$ sample belonging to class $c$.

The total loss over a batch of $N$ samples is the average loss over all the samples in the batch:
\begin{equation}\label{equ:batch-loss}
    \mathcal{L} = \dfrac{1}{N} \sum_{i=1}^{N}\mathcal{L}_i(\textbf{y}_i,\hat{\textbf{y}}_i)
\end{equation}

In order to update the weights of Eq.~\ref{equ:fc-layer} and~\ref{equ:softmax}, a gradient based 
optimizer can be used such as Stochastic Gradient Descent (SGD) as follows:
\begin{equation}\label{equ:sgd}
    W = W - \alpha.\dfrac{\partial \mathcal{L}}{\partial W}
\end{equation}
\noindent where $\alpha$ is the learning rate hyper-parameter controlling the step size of the 
optimization algorithm.

In the current literature, deep learning models consist of a very high number of layers on top 
of each other, in this case, the partial derivative of Eq.~\ref{equ:sgd} cannot be calculated.
Instead, for the last 20 years, neural networks utilize the derivative chain rule, the core idea 
of the back-propagation algorithm~\cite{back-prop}.

\paragraph*{Convolution Layers}

The convolution operation, as explained in Eq.~\ref{equ:convolution1d-uni}, is applied the same way 
in a convolution layer, where the optimization algorithm learns the best weights of the convolution 
kernel.
A convolution layer applies $\mathcal{K}$ filters $\{\textbf{w}\}_{j=1}^{\mathcal{K}}$ of the same
length $K$ and same dilation rate over the input 
time series.
If the input time series is univariate $\textbf{x}$ of length $L$, the output of the convolution layer is 
a multivariate time series computed as follows:
\begin{equation}\label{equ:conv-layer-uni}
    \textbf{o} = concat(\{\textbf{x} * \textbf{w}_j\}_{j=1}^{\mathcal{K}})
\end{equation}
\noindent where $concat$ is the concatenation operation and $\textbf{o}$ is a multivariate time series 
of $\mathcal{K}$ dimensions with length $L-K+1$ each.

In the case $\textbf{x}$ is a multivariate time series of $M$ channels of length $L$ each, and the
target output dimension is $\mathcal{K}$ (the chosen number of filters), then in reality, the number 
of filters to learn is $M.\mathcal{K}$.
This is done by simply learn $M$ filters, one for each of the input dimensions and summing the output.
This is repeated $\mathcal{K}$ times and the output sums are concatenated to produce the output MTS $\textbf{o}$.
The mathematical formulation of the above operation is defined as follows:
\begin{equation}\label{equ:conv-layer-multi}
    \textbf{o} = concat(\{\sum_{m=1}^{M}\textbf{x}^m*\textbf{w}_{m,j}\}_{j=1}^{\mathcal{K}})
\end{equation}
\noindent where the above summation in Eq.~\ref{equ:conv-layer-multi} is over the temporal axis of all
the series inside the sum, producing after each summation a univariate time series.

This type of convolution layer is referred to in the rest of this work as Standard Convolution (SC) layer.
A visualization of the SC layer with a chosen number of filters set to $2$ with a kernel size of $8$
is presented in Figure~\ref{fig:std-conv} applied on an input MTS of dimension $3$.
It can be seen from this figure that the total number of filters to learn is $6$ instead of $2$ (the chosen output dimension).

\begin{figure}[t]
\centering
\caption{Standard Convolution applied on a 
\protect\mycolorbox{0,0,255,0.5}{multivariate input time series }
of dimensions $3$, convoluted with two times with three different 
\protect\mycolorbox{0,125,0,0.5}{convolutional filters}, producing 
a \protect\mycolorbox{255,161,0,0.5}{convolutional output} per filter,
that are then summed together to produce two \protect\mycolorbox{255,0,0,0.5}{final outputs}.
The convolution operation starts with an element wise
\protect\mycolorbox{138,74,171,0.7}{multiplication}
followed by a \protect\mycolorbox{202,120,115,0.7}{summation operation}.}
\label{fig:std-conv}
\includegraphics[width=\textwidth]{Figures/chapter_1/convolutions/stdConv.pdf}
\end{figure}

\paragraph*{DepthWise Separable Convolution (DWSC)}

First used for image classification in MobileNets~\cite{mobilenets}, this type of convolution layer has a unique feature 
of having a very low number of trainable parameters.
DWSCs are in fact a pipeline made of two different convolution layers: (1) DepthWise Convolution (DWC) followed by (2) PointWise
Convolution (PWC).
DWSC are more common to be used on MTS input data.
% if they are used on the raw data, or in the middle of the 
% architecture where the input has 
% a higher dimension to $1$ (MTS has dimensions $M > 1$).
The DWC layer (first phase of DWSCs) consists on learning $M$ filters where $M$ 
is the dimension of the input MTS, hence the reason to why DWSCs are
commonly used on multivariate input, or else we learn only one filter.
For instance, if the input raw MTS $\textbf{x}$
% ,
% or the output MTS at any given layer of the model,
has $M$ dimensions of length $L$,
applying a DWC layer with kernel size $K$ is defined as follows:
\begin{equation}\label{equ:depthwise-conv}
    \textbf{o} = concat(\{\textbf{x}^m*\textbf{w}_m\}_{m=1}^{M})
\end{equation}
\noindent where $\textbf{o}$ is the output of the DWC layer, also with $M$ dimensions of length $L-K+1$

\begin{figure}[t]
    \centering
    \caption{DepthWise Separable convolution
    \protect\mycolorbox{0,0,255,0.5}{multivariate input time series }
of dimensions $3$, convoluted one time with three different 
\protect\mycolorbox{0,125,0,0.5}{convolutional filters}, producing 
a \protect\mycolorbox{255,161,0,0.5}{convolutional output} per filter,
that go through a weighted summed  to produce two \protect\mycolorbox{255,0,0,0.5}{final outputs}.
The convolution operation starts with an element wise
\protect\mycolorbox{138,74,171,0.7}{multiplication}
followed by a \protect\mycolorbox{202,120,115,0.7}{summation operation}.}
    \label{fig:sep-conv}
    \includegraphics[width=\textwidth]{Figures/chapter_1/convolutions/sepConv.pdf}
\end{figure}

The PWC layer, also referred to as bottleneck layer, consists on a change of dimensionality through a standard convolution layer 
with a kernel size of $1$.
% The goal of a PWC layer (bottleneck layer) is to change the dimensions of the input MTS to any other target dimension.
Applying a PWC layer with target dimension $\mathcal{K}$ on an input MTS $\textbf{x}$ of $M$ dimensions of length $L$
is defined as follows:
\begin{equation}\label{equ:pointwise-conv}
    \textbf{o} = concat(\{\sum_{m=1}^{M}\textbf{x}^m.w_{m,j}\}_{j=1}^{\mathcal{K}})
\end{equation}
\noindent where $W=\{\{w_{m,j}\}_{m=1}^{M}\}_{j=1}^{\mathcal{K}}$ is a two-dimensional matrix of real values and $\textbf{o}$
is the output of the PWC layer with the same length as the input and $\mathcal{K}$ dimensions.

Finally, the pipeline of DWSC layer of kernel size $K$ and target dimension $\mathcal{K}$
applied on an input MTS $\textbf{x}$ of $M$ dimensions of length $L$ can be defined as follows:
\begin{equation}\label{equ:depthwise-sep-conv}
    \textbf{o} = concat(\{\sum_{m=1}^{M}concat(\{\textbf{x}^m*\textbf{w}_m\}_{m=1}^{M})^m.w_{m,j}\}_{j=1}^{\mathcal{K}})
\end{equation}
\noindent where $\{\textbf{w}_m\}_{m=1}^{M}$ is a set of convolution kernels of length $K$ each and
$\{\{w_{m,j}\}_{m=1}^{M}\}_{j=1}^{\mathcal{K}}$ is a set of real values and $\textbf{o}$ is the output of the DWSC layer 
with $\mathcal{K}$ dimensions of length $L-K+1$.

A visualization of the DWSC layer with a chosen number of target dimension set to $2$ with a kernel size of $8$
is presented in Figure~\ref{fig:sep-conv} applied on an input MTS of dimension $3$.
It can be seen from this figure that the total number of filters to learn is $3$ instead of $6$ (SC layer)
with additional $6$ real values to learn (the chosen output dimension).

The total number of parameters learned by an SC and DWSC layers are $M.\mathcal{K}.K$ and $M.K + M.\mathcal{K}$ respectively.


\paragraph*{Residual Connections}

Deep learning models sometimes suffer from a common issue referred to as the vanishing gradient.
This issue is more common when the network's depth gets higher, resulting in a very deep model,
and during the backward phase of the optimization, the gradient may become zero.
To avoid this issue, the authors in~\cite{resnet-images} proposed the residual connections,
where instead of having one branch, the network gets divided into two branches.
The first branch serves as the encoding and feature extraction, and the second serves as the skip 
branch, where it simply uses an almost identity like function.
Both branches meet after a specific number of layers in an element-wise addition operation, resulting 
in what follows at layer $\lambda_l$:
\begin{equation}\label{equ:resnet}
    f_{l}(\theta_{l},g_{l-1}) = f_{1,l}(\theta_{1,l},g_{l-1}) + f_{2,l}(\theta_{2,l},g_{l-1})
\end{equation}
\noindent where $g_{l-1}$ is the output of the $l-1_{th}$ layer,
$f_{1,l}$ is an identity like function with parameters $\theta_{1,l}$,
and $f_{2,l}$ is a stack of layers with parameters $\theta_{2,l}$.

\paragraph*{Batch Normalization}

Batch Normalization (BN) is a technique used to improve the training speed and stability of neural networks 
by normalizing the inputs of each layer to have a mean of zero and a standard deviation of one.
This is necessary in order to reduce the chance of gradient exploding due to different range of values
in the features going from one layer to another.
This is done by calculating the mean and variance of the inputs within each mini-batch during 
training and then scaling and shifting the inputs based on these statistics. In the context of 
time series analysis, especially when used after 1D convolution layers, batch normalization can 
be particularly beneficial. Since 1D convolution layers are often used to extract temporal features 
from the time series data, the distribution of features across different time steps may vary significantly. 

Similar to other neural network layers, BN has two trainable parameters, called $\gamma$ and $\beta$.
Following the first z-normalization step that produces zero-mean and unit variance features, the BN layer 
learns how to shift and scale all the features to a new mean and variance.
% , however it will be the same mean and variance 
% shared between all features.
Supposing a batch of $B$ MTS $\{\textbf{x}_i\}_{i=1}^{B}$ produced
by a convolution layer with $M$ channels of length $L$, applying the BN layer with parameters
$\gamma_1,\gamma_2,\ldots,\gamma_M$ and $\beta_1, \beta_2, \ldots, \beta_M$ is defined as follows:
\begin{equation}\label{equ:batch-norm-train}
    \textbf{o}_i = concat(\{\gamma_m.\dfrac{\textbf{x}_i^m - \mu_m}{\sigma_m} + \beta_m\}_{m=1}^M)
\end{equation}
\noindent where $\{\textbf{o}_i\}_{i=1}^{B}$ is a set of MTS of same shape as $\textbf{x}_i$, $\mu_m$ is the average of the channel $m$ of all samples in the batch
over the temporal axis, and $\sigma_m$ is its standard deviation.

Moreover, the BN layer also has two non-trainable parameters called $\mu_{mov}$ and $\beta_{mov}$ which contains a moving average 
of the mean and standard deviation of the input data $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$, for each dimension separately.
These two non-trainable are then used during inference to scale the features of new unseen samples.
These two parameters are calculated during training as follows:
\begin{equation}\label{equ:batch-norm-mvg1}
    \mu_{mov} = concat(\{\alpha_{BN}.\mu_{mov_m} + (1-\alpha_{BN}.\mu_{m})\}_{m=1}^M)
\end{equation}
\begin{equation}\label{equ:batch-norm-mvg2}
    \beta_{mov} = concat(\{\alpha_{BN}.\beta_{mov_m} + (1-\alpha_{BN}.\beta_{m})\}_{m=1}^M)
\end{equation}
\noindent where $\alpha_{BN}$ is the moving average parameter and $0 < \alpha_{BN} < 1$.


\paragraph*{Local Pooling (Max and Average)}

Pooling operations have been shown to be very effective for images throughout the years ever since the birth of deep learning~\cite{lecun2015deep}.
The motivation of doing local pooling operations is to reduce dimensionality resulting in a focus on more local important 
features extracted by the network.

There exist many local pooling layers, however two types are more common in the time series classification 
literature, the first being max pooling and the second being average pooling.
For the max pooling layer, such as for convolution layers, a kernel is defined with a specific length, however in this case 
the kernel does not have any trainable weights.
Instead, the kernel slides over the temporal axis of the series, and the values seen by the kernel are replaced by their maximum 
value.
The operation done by the max pooling layer with kernel size $K$ is defined below on an input univariate series $\textbf{x}$
of dimensions $(L)$:
\begin{equation}\label{equ:max-pooling}
    o_t = \max(x[t:t+k])
\end{equation}
\noindent with $t$ $\in~[1,L-K+1]$ and $\textbf{o}=\{o_1,o_2,\ldots,o_{L-K+1}\}$ is a univariate series with length $L-K+1$.

The average pooling layer is defined in the same way as the max pooling, however instead of choosing the maximum between 
the values seen by the kernel, the average value replaces them.
The operation done by the average pooling layer with kernel size $K$ is defined below on an input univariate series $\textbf{x}$
of dimensions $(L)$:
\begin{equation}\label{equ:avg-pooling}
    o_t = \dfrac{1}{K}\sum_{k=1}^{K} x[t:t+k]
\end{equation}
\noindent with $t$ $\in~[1,L-K+1]$ and $\textbf{o}=\{o_1,o_2,\ldots,o_{L-K+1}\}$ is a univariate series with length $L-K+1$.

Similar to convolution layers, max and average pooling layers can both be applied on MTS, however in this case the operation is 
applied on each channel independently and the output dimension will be the same as the input.
Moreover, dilation rates can also be used for local pooling layers similar to convolutions, see Eq.~\ref{equ:convolution1d-dilation}.

\emph{It is important to note, that by default all local pooling layers utilize a stride equals to the kernel size, unless 
specified to use another stride. The above output length calculations ($L-K+1$) is in the case where strides are set to $1$,
however in default mode, the output length is $\lceil \dfrac{L - K + 1}{K}\rceil$.}

\paragraph*{Global Pooling (Max and Average)}

Global pooling is a powerful technique often employed in neural network architectures 
for dimensionality reduction and feature summary. Unlike local pooling, which focuses on local 
features within specific regions, global pooling computes the summary of feature maps across the entire spatial 
dimensions, providing a global perspective of the input data.

Similar to local pooling, two main global pooling layers are used for time series classification in deep learning models,
the first being Global Max Pooling (GMP) and the second being Global Average Pooling (GAP).
In the case of time series, the global pooling layers are mostly used posterior to all the feature extraction layers.

The GMP layer receives an input dimension of $(L,M)$, where $L$ is the length of the series and $M$ is its dimensions,
and outputs a vector, per series, of dimension $(M,)$, where each point of the vector is the maximum value over all the time 
axis of each dimension.
We define below the GMP operation over an input time series $\textbf{x}$ of dimension $(L,M)$:
\begin{equation}\label{equ:gmp-layer}
    \textbf{v} = concat(\{\max(\textbf{x}^m[1:L])\}_{m=1}^{M})
\end{equation}
\noindent where $\textbf{v} = \{v_1,v_2,\ldots,v_M\}$ is a vector of dimension $(M,)$.

Similar to the GMP layer, the GAP layer receives the same input dimension and outputs a vector $\textbf{v}$ also of dimension 
$(M,)$, however each point of the vector is the average value over the time axis of each dimension.
In simpler ways, we define in what follows the operation done in the GAP layer over a series $\textbf{x}$ of dimensions $(L,M)$:

\begin{equation}\label{equ:gap-layer}
    \textbf{v} = concat(\{\dfrac{1}{L}\sum_{t=1}^{L}x_t^m\}_{m=1}^{M})
\end{equation}
\noindent where $\textbf{v} = \{v_1,v_2,\ldots,v_M\}$ is a vector of dimension $(M,)$.

As mentioned above, global pooling layers are used at the last feature extraction step of the network,
this is because it can be now fed  to an FC layer (see Eq.~\ref{equ:fc-layer}) with a softmax activation
for the classification task.

\paragraph*{Temporal Self-Attention}

The Self-Attention mechanism has shown to have a significant impact in Natural Language Processing (NLP) ever since the 
birth of Transformers for language translation in the paper \emph{Attention Is All You Need}~\cite{attention-all-you-need}.
The Self-Attention mechanism, adapted from the original Attention mechanism~\cite{attention-mechanism}, allows the model 
to learn about the dependency between features spread along a temporal axis.
This information is then used to transform the input features into a new space that is more compact and contains denser information 
about important features.

To explain how does the Self-Attention layer is able to do the operation mentioned above, we will assume again an input time series $\textbf{x}$
of shape $(L,M)$, supposing that this time series is actually the output of previous feature extraction layers such as CNNs.
We detail below each step of the Self-Attention mechanism:

\textbf{First}, Self-Attention mechanism has a unique feature of being order invariant, for this reason we use Positional Encoders (PEs)
to add position information to each element in the sequence. Sinusoidal functions generate these encoding,
which are added (element-wise) to the input embeddings, ensuring the model can use the sequence order. The PE for position 
$pos~\in~[1,L]$:

\begin{equation}\label{equ:pe-sin}
PE_{(pos, 2k)} = \sin\left(\frac{pos}{w_k}\right)
\end{equation}
\begin{equation}\label{equ:pe-cos}
PE_{(pos, 2k+1)} = \cos\left(\frac{pos}{w_k}\right)
\end{equation}
\noindent where $k~\in~[0,\dfrac{d_{model}}{2}]$, the frequency 
$w_k=10000^{2k/d_{model}}$ and $d_{model}$ is the dimension of the embeddings.

These encoding ensure the sequence includes both content and positional information.
This type of PE is commonly referred to in the literature as Absolute Positional Encoding (APE). 

\textbf{Second}, the Self-Attention layers transforms each time stamp of the input time series $\textbf{x}$ from dimension $M$ to 
dimension $d_{model}$, a hyper-parameter of the Self-Attention layer.
This first step is done three times independently to produce three different representations of the input series, referred to as:
(1) Query $\textbf{Q}$, (2) Key $\textbf{K}$ and (3) Value $\textbf{V}$.
The Query and Key are used to find the dependency information between time stamps in $\textbf{x}$, for which this information 
is then used to transform the Value to a new more compact space.
This is done by simply defining three FC layers (see Eq.~\ref{equ:fc-layer}) with weight matrices $W_{\textbf{Q}}$,
$W_{\textbf{K}}$ and $W_{\textbf{Q}}$, for the Query, Key and Value respectively of shape $(M,d_{model})$ each.
The three matrices are used to transform each time stamp of $\textbf{x}$ to a new space of different dimensions, as defined below 
to produce $\textbf{Q}$, $\textbf{K}$ and $\textbf{V}$ of shape $(L,d_{model})$ each:
\begin{equation}\label{equ:attention-query}
    \textbf{Q} = concat_{temporal~axis}(\{\textbf{x}_t\circ W_{\textbf{Q}}\}_{t=1}^{L})
\end{equation}
\begin{equation}\label{equ:attention-key}
    \textbf{K} = concat_{temporal~axis}(\{\textbf{x}_t\circ W_{\textbf{K}}\}_{t=1}^{L})
\end{equation}
\begin{equation}\label{equ:attention-value}
    \textbf{V} = concat_{temporal~axis}(\{\textbf{x}_t\circ W_{\textbf{V}}\}_{t=1}^{L})
\end{equation}
\noindent where the concatenation is over the temporal axis and the matrix multiplication $\circ$ is over the dimension axis.

\textbf{Third}, given that both $\textbf{Q}$ and $\textbf{V}$ are different representations but of the same input sequence 
$\textbf{x}$, the Self-Attention layer utilizes these two sequences in order to find dependency information between each time stamp 
and all the other time stamps.
This is done by calculating the attention score matrix as follows:
\begin{equation}\label{equ:attention-score}
    Att = soft\max(\dfrac{Q\circ K^{T}}{\sqrt{d_{model}}})
\end{equation}
\noindent where $Att$ is called the attention score matrix of shape $(L,L)$ and the $soft\max$ operation is performed over the 
column's axis, producing per row a probability distribution how much the row time stamp is correlated with all the column time 
stamps.
The scaling factor $1/\sqrt{d_{model}}$ is utilized to avoid high values produced in the dot products, resulting in values close 
to the $soft\max$ limits where the gradient can be very small.

\textbf{Fourth}, the above attention matrix $Att$ is then used to transform the sequence $\textbf{V}$ into a new representation,
making the output sequence more compact in terms of dependency information between time stamps.
The transformed sequence goes through a dimension change using another FC layer with weight matrix $W_{\textbf{o}}$ with shape 
$(d_{model},M)$ to change back to the original dimension of $\textbf{x}$:
\begin{equation}\label{equ:attention-transform}
    \textbf{o} = (Att\circ\textbf{V}) \circ W_{\textbf{o}}
\end{equation}
\noindent where $\textbf{o}$ has the same dimension as $\textbf{x}$.

It is common to use the concept of multi-head attention, where the same procedure described above is repeated independently 
$H$ times (in parallel), where $H$ is the number of heads.
The outputs of each head are finally concatenated and the final transformation matrix $W_{\textbf{o}}$ is used on the concatenated 
transformations of all heads.

\paragraph*{Recurrent Layers}

Recurrent Neural Networks (RNNs)~\cite{rnn-paper} are specialized neural networks designed for processing sequential data. 
They maintain a hidden state that captures information from previous inputs, making them ideal for tasks 
like language modeling, speech recognition~\cite{graves2013speech}, and sequence-to-sequence learning~\cite{seq-to-seq}
RNNs are effective at learning patterns and dependencies in sequences, leveraging their ability to remember context over time.

There exists three different recurrent layers that have been proposed during the last three decades:
(1) Simple RNN~\cite{rnn-paper}, (2) Long Short-Term Memory (LSTM)~\cite{lstm-paper} and Gated Recurrent Unit (GRU)~\cite{gru-paper}.
In what follows, we present each of these layers briefly with their mathematical formulation.

\begin{enumerate}
    \item \textbf{Elman Recurrent Neural Network (Simple RNN)}, proposed in~\cite{rnn-paper}, is one of the simplest
    forms of RNNs. It consists of a single hidden layer that maintains a recurrent connection to itself, allowing it
    to capture sequential dependencies. For an input time series $\textbf{x}$ of length $L$ and dimension $M$, applying 
    once recurrence step $t$ where $t~\in~[1,L]$ using the Simple RNN is defined as:
    \begin{equation}\label{equ:rnn-cell-hidden}
        \textbf{h}_t = \sigma(W_{hx}\circ\textbf{x}_t + W_{hh}\circ\textbf{h}_{t-1}+\textbf{b}_{h})
    \end{equation}
    \begin{equation}\label{equ:rnn-cell-output}
        \textbf{o}_t = \sigma(W_{oh}\circ\textbf{h}_t + \textbf{b}_{o})
    \end{equation}
    \noindent where $\textbf{h}_t$ is the hidden state of time stamp $t$ of dimension $d_{hidden}\neq M$,
    $W_{hx}$ is the input-to-hidden transformation matrix of shape $(d_{hidden},M)$, 
    $W_{hh}$ is the hidden-to-hidden transformation matrix of shape $(d_{hidden},d_{hidden})$,
    $b_{h}$ is the hidden layer bias vector of dimension $d_{hidden}$,
    $W_{oh}$ is the hidden-to-output transformation matrix of shape $(M,d_{hidden})$,
    $\textbf{b}_{o}$ is the output bias vector of dimension $M$,
    $\sigma$ is the activation function commonly a sigmoid or hyperbolic tangent function
    and $\textbf{o}$ is the output series of length $L$ and dimension $M$

    \item \textbf{Long Short-Term Memory (LSTM)}, proposed in~\cite{lstm-paper}, addresses the vanishing gradient 
    problem faced by traditional RNNs, enabling them to capture long-range dependencies more effectively. 
    LSTM introduces a gating mechanism that regulates the flow of information, allowing the network to 
    selectively remember or forget information over time.
    The mathematical formulation of the LSTM layer is defined as follows:
    \begin{equation}\label{equ:lstm-cell-forget}
        \textbf{f}_t = sigmoid(W_\textbf{f}\circ concat(\textbf{h}_{t-1},\textbf{x}_t) + \textbf{b}_f)
    \end{equation}
    \begin{equation}\label{equ:lstm-cell-input}
        \textbf{e}_t = sigmoid(W_\textbf{e}\circ concat(\textbf{h}_{t-1},\textbf{x}_t) + \textbf{b}_e)
    \end{equation}
    \begin{equation}\label{equ:lstm-cell-output}
        \textbf{o}_t = sigmoid(W_\textbf{o}\circ concat(\textbf{h}_{t-1},\textbf{x}_t) + \textbf{b}_o)
    \end{equation}
    \begin{equation}\label{equ:lstm-cell-candidate}
        \tilde{\textbf{C}}_t = \tanh(W_\textbf{C}\circ concat(\textbf{h}_{t-1},\textbf{x}_t) + \textbf{b}_C)
    \end{equation}
    \begin{equation}\label{equ:lstm-cell-state}
        \textbf{C}_t = \textbf{f}_t\odot\textbf{C}_{t-1} + \textbf{e}_t\odot\tilde{\textbf{C}}_t
    \end{equation}
    \begin{equation}\label{equ:lstm-cell-hidden}
        \textbf{h}_t = \textbf{o}_t\odot\tanh(\textbf{C}_t)
    \end{equation}
    \noindent where $\textbf{h}_t$, is the hidden state vector of dimension $d_{hidden}$,
    $\textbf{f}_t$, $\textbf{e}_t$ and $\textbf{o}_t$ are the forget, input and output gates vectors respectively
    of dimension $d_{hidden}$ with
    $W_\textbf{f}$, $W_\textbf{e}$, $W_\textbf{o}$ as their respective transformation matrices
    of shape $(d_{hidden},M+d_{hidden})$ each and
    $\textbf{b}_\textbf{f}$, $\textbf{b}_\textbf{e}$, $\textbf{b}_\textbf{o}$ their respective bias vectors
    of dimension $d_{hidden}$ each.
    $\tilde{\textbf{C}}_i$ is the candidate cell state vector, $\textbf{C}_i$ is the cell state vector both of 
    dimension $d_{hidden}$ and finally $W_\textbf{C}$ is the state cell candidate transformation matrix of shape 
    $(d_{hidden}, M)$ and $\textbf{b}_\textbf{C}$ is the cell state bias vector of dimension $d_{hidden}$.
    $\odot$ denotes element-wise and $\circ$ denotes the matrix multiplication operation.

    \item \textbf{Gated Recurrent Unit (GRU)}, proposed in~\cite{gru-paper}, is a variation of the LSTM network 
    that simplifies its architecture while maintaining comparable performance. The GRU combines the forget and 
    input gates into a single update gate, reducing the number of parameters and computational complexity. 
    The mathematical formulation of the GRU layer is defined as follows:
    \begin{equation}\label{equ:gru-update-gate}
        \textbf{z}_t = \sigma(W_\textbf{z} \circ concat(\textbf{h}_{t-1}, \textbf{x}_t) + \textbf{b}_\textbf{z})
    \end{equation}
    \begin{equation}\label{equ:gru-reset-gate}
        \textbf{r}_t = \sigma(W_\textbf{r} \circ concat(\textbf{h}_{t-1}, \textbf{x}_t) + \textbf{b}_\textbf{r})
    \end{equation}
    \begin{equation}\label{equ:gru-candidate-activation}
        \tilde{\textbf{h}}_t = \tanh(W_\textbf{h} \circ concat(\textbf{r}_t \odot \textbf{h}_{t-1}, \textbf{x}_t) + \textbf{b}_\textbf{h})
    \end{equation}
    \begin{equation}\label{equ:gru-hidden-state}
        \textbf{h}_t = (\textbf{1}_{d_{hidden}} - \textbf{z}_t) \odot \textbf{h}_{t-1} + \textbf{z}_t \odot \tilde{\textbf{h}}_t
    \end{equation}
    \noindent where $\textbf{z}_t$ and $\textbf{r}_t$ are the update and reset gates respectively,
     $\tilde{\textbf{h}}_t$ is the candidate activation, and $\textbf{h}_t$ is the hidden state, of dimension $d_{hidden}$.
     $\odot$ denotes element-wise and $\circ$ denotes the matrix multiplication operation.
     $W_\textbf{z}$, $W_\textbf{r}$, and $W_\textbf{h}$ are weight matrices of shape $(d_{hidden}, M + d_{hidden})$,
     and $\textbf{b}_\textbf{z}$, $\textbf{b}_\textbf{r}$, and $\textbf{b}_\textbf{h}$ are bias terms of 
     dimension $d_{hidden}$. $\textbf{1}_{d_{hidden}}$ is a column vector of $1$s of dimension $d_{hidden}$.
\end{enumerate}

\paragraph{Different Neural Network Architectures for Time Series Classification}

During the last decade, a significant amount of architectures has been proposed addressing the task of TSC.
A detailed benchmark paper~\cite{dl4tsc} questioned the need of a fair comparison between most of these architectures
over all the datasets of the UCR/UEA archives~\cite{ucr-archive,uea-archive}.
Their choice of architectures to include in the benchmark depended on their ability to reproduce the model from scratch.
This benchmark, on some level, became a starting point of addressing TSC tasks with deep learning methods.
In this section, we present some architectures used in this benchmark~\cite{dl4tsc} as well as new architectures proposed
for both univariate and multivariate TSC ever since 2019 (the publication year of the benchmark).

\paragraph*{Multi Layer Perceptron (MLP)}

The concept of Multilayer Perceptrons (MLPs) originated from the field of artificial neural networks, where they 
were developed as a class of feedforward neural networks consisting of multiple layers of nodes \cite{mlp-original-paper}. 
Researchers began exploring the application of MLPs to time series classification due to their ability to model complex 
relationships between input features.
The authors in~\cite{fcn-resnet-mlp-paper} proposed an MLP architecture, presented in Figure~\ref{fig:mlp}, for TSC.
The architecture is made of three hidden FC layers each followed by a ReLU activation function and a Dropout layer.
Dropout layers simply uses a random $p \%$ of the input neurons and set them to zero and scale the non-dropped input neurons 
by $\dfrac{1}{1-p}$, used to avoid overfitting the model on training data, where $p$ is the drop rate parameter.
The last Dropout layer's output is then fed to an FC layer for the classification task, see Figure~\ref{fig:mlp} for a detailed 
view on the parameters of the MLP architecture.
However, a significant limitation of MLPs in this context is their inability to
effectively capture local temporal dependencies within the data, as they process input data in a fixed manner without 
considering the sequential nature of time series data. This limitation led to the exploration of other neural network
architectures better suited for temporal data, such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks 
(CNNs).

\begin{figure}
    \centering
    \caption{The MultiLayer Perceptron (MLP) architecture~\cite{fcn-resnet-mlp-paper} for Time Series Classification.}
    \label{fig:mlp}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tsc/mlp.pdf}
\end{figure}

\paragraph*{Time Convolutional Neural Network (TimeCNN)}

Ever since AlexNet~\cite{alexnet-paper} has been released in 2012 and highlighted the performance of deep CNN models on image 
classification~\cite{imagenet-paper}, a significant amount of researchers started to wonder on the need to include deep learning 
into other applications.
The authors in~\cite{time-cnn} proposed a CNN architecture, based on the image classification CNN in~\cite{lecun2015deep},
on one dimensional temporal data.
The architecture, presented in Figure~\ref{fig:time-cnn}, consists of two convolution blocks each containing a 1D convolution 
layer followed by a sigmoid activation function and a local average pooling layer of default strides (see Figure~\ref{fig:time-cnn} for detailed 
view on the parameters of TimeCNN).
Following the second convolution block, the output is an MTS of $7$ channels and the length depending on the input time series 
characteristics.
This output MTS is flattened to form a large one dimensional vector that is then fed to an FC layer for the classification 
task.

\begin{figure}
    \centering
    \caption{Time-CNN~\cite{time-cnn} architecture for Time Series Classification.}
    \label{fig:time-cnn}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tsc/time-cnn.pdf}
\end{figure}

\paragraph*{Fully Convolutional Network (FCN)}

The authors in~\cite{fcn-resnet-mlp-paper} questioned the need of local pooling layers and proposed instead a 
Fully Convolutional Network (FCN), composed of three convolution blocks, each containing a one dimensional convolution 
layer followed by a batch normalization layer and a ReLU activation function.
The FCN architecture is presented in Figure~\ref{fig:fcn} including all the parameter setup proposed in~\cite{fcn-resnet-mlp-paper}.
The authors of FCN argues that the replacement of local pooling layers by the batch normalization not only enhances the 
performance given that local pooling can lose some information, but increases the speed of convergence of the model as well.
The convolution layers used in FCN applies a zero-padding on the input, hence the length of the series is preserved throughout 
the network.
This padding operation ensures that the network can detect some patterns on the edges of the series.
The FCN architecture feeds the last activation layer to a global pooling layer, specifically a GAP, instead of flattening
in order to reduce the number of parameters to learn in the last classification FC layer.
\begin{figure}
    \centering
    \caption{The Fully Convolutional Network (FCN)~\cite{fcn-resnet-mlp-paper} architecture for Time Series Classification.}
    \label{fig:fcn}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tsc/fcn.pdf}
\end{figure}

\paragraph*{Residual Network (ResNet)}

Since the impact of residual connections have been successful for image classification~\cite{resnet-images}, the authors 
in~\cite{fcn-resnet-mlp-paper} proposed to enhance the FCN architecture with this kind of operations.
The authors in~\cite{fcn-resnet-mlp-paper} argues the need of residual connections given that neural networks also may suffer 
from the vanishing gradient problem for TSC.
For this reason, the authors proposed ResNet for TSC, presented in Figure~\ref{fig:resnet} with its parameter setup.
This architecture consists of three residual blocks, where each block is an FCN architecture without the GAP and classification 
layer.
Each residual block contains an element-wise addition between its input layer and output layer, with the residual connection 
including a bottleneck layer (PWC see Eq.~\ref{equ:pointwise-conv}) to adjust dimensions.
The convolution layers, such as in FCN, utilize a zero-padding to ensure the edge pattern detection, resulting as well in 
equal length input/output at the beginning and end of the network.
The last activation layer of ResNet is fed to a GAP layer followed by an FC layer for classification.

\begin{figure}
    \centering
    \caption{The Residual Network (ResNet) architecture~\cite{fcn-resnet-mlp-paper} for Time Series Classification.}
    \label{fig:resnet}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tsc/ResNet.pdf}
\end{figure}

\paragraph*{Encoder}

Motivated by FCN~\cite{fcn-resnet-mlp-paper}, the authors in~\cite{encoder-paper} proposed a novel hybrid deep learning model, 
Encoder, that replaces the GAP layer by a slightly different version of Self-Attention.
The Encoder architecture, presented in Figure~\ref{fig:encoder}, consists on three convolution blocks, each containing a one dimensional convolutional
layer, followed by an Instance Normalization (IN) layer instead of a BN layer, a Parametric ReLU activation function, a dropout layer and 
finally a local max pooling layer.
The IN layer consists on using the same normalization concept of BN however it is done per example in the batch instead of averaging
statistics over all samples in the batch.
The third convolution block however does not contain a local max pooling layer, instead the outputs are split on the dimension axis 
into two parts used for a Self-Attention mechanism.
The output of the attention layer goes through an FC transformation layer followed by a flattening and the last FC classification layer.

\begin{figure}
    \centering
    \caption{The Encoder architecture~\cite{encoder-paper} for Time Series Classification.}
    \label{fig:encoder}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tsc/encoder.pdf}
\end{figure}

\paragraph*{Neural Network Ensemble (NNE)}

Ensemble models in machine learning~\cite{hive-cote2.0} has shown to have 
a significant impact, for instance until the year 2020, the state-of-the-art 
hybrid ensemble model, HIVE-COTE~\cite{hive-cote}, consisted of $36$ classifiers 
ensembled.
The authors in~\cite{nne-paper} studied the impact of ensembling in deep 
learners given that HIVE-COTE~\cite{hive-cote} ensembles non-deep learners.
In~\cite{nne-paper}, the authors proposed the Neural Network Ensemble (NNE)
consisting of $6$ different deep learning architectures with $10$ different initialization,
hence a total of $60$ models, ensembled posterior to training.
NNE highlighted that it can achieve the performance of HIVE-COTE over the UCR 
archive as well as outperform significantly an ensemble of any other architecture 
alone, highlighting that the importance of hybrid ensembles.

\paragraph*{InceptionTime}

In~\cite{inceptiontime-paper}, the authors argued the need to \emph{find the 
AlexNet for Time Series Classification} given the increase in number of datasets 
available and the high similarity that exists between them.
For this reason, a deeper architecture should be proposed to outperform the 
current state-of-the-art deep learning model ResNet~\cite{fcn-resnet-mlp-paper,dl4tsc}.
Motivated from the impact of Inception architecture for image classification,
the authors in~\cite{inceptiontime-paper} proposed an adaptation of the Inception
architecture for TSC, specifically the authors were based on the fourth version 
of Inception on image classification~\cite{inceptionv4-paper}.
The Inception architecture adapted for time series data is presented in 
Figure~\ref{fig:inception} with a detailed view on its parameters' setup.

The Inception architecture consists of two Inception-blocks each containing a
residual connection~\cite{resnet-images} connecting their input and output.
Within each Inception-block, there is three Inception-modules connected in series,
each containing three convolution layers in parallel (this is referred to later as multiplexing convolution)
of different kernel size applied 
on the same input, and a local max pooling layer followed by a PWC layer for dimension adjustment.
Each Inception-module, if its input has dimension higher to $1$, applies a PWC layer 
in order to reduce the number of filters to learn in the following convolution layers.
The output of the three convolution and the max pooling layers are concatenated on the 
channel axis and fed to a BN layer and a ReLU activation function.
The last activation layer goes through a global pooling operation, specifically a GAP layer,
before being fed to an FC classification layer.

The authors in~\cite{inceptiontime-paper}, seeing the impact of ensemble models,
proposed InceptionTime, an ensemble of five Inception architectures each trained with 
different initialization.
In 2020, InceptionTime became the state-of-the-art deep learning model for TSC and shown to 
have even less difference in performance, statistically, with HIVE-COTE2.0~\cite{hive-cote2.0}.
InceptionTime, not only highlighted its ability to achieve HC2.0 performance, but as well as it being 
more efficient in terms of training runtime in function of both training dataset size and length of
time series.

\begin{figure}
    \centering
    \caption{The Inception architecture~\cite{inceptiontime-paper}
    for Time Series Classification.}
    \label{fig:inception}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tsc/Inception.pdf}
\end{figure}

\paragraph*{Disjoint Convolutional Neural Network (Disjoint-CNN)}

All the previously presented architectures were originally proposed for a 
general setup of TSC, meaning they were not constructed in a manner to address 
univariate time series or multivariate time series specifically, and they can be applied 
to both and have been evaluated on both.
However, some researchers argued that handling MTS data is not the same as handling 
UTS data and questioned the way convolution operations are being done over MTS.

In~\cite{disjoint-cnn-paper}, the authors proposed the Disjoint Convolutional 
Neural Network (Disjoint-CNN), composed of, following the naming of the authors,
1+1D convolution layers (see Figure~\ref{fig:disjoin-cnn} for a detailed view on the architecture).
The 1+1D convolution layer are two convolution operations operated in series,
the first being a temporal convolution and the second being a spatial convolution.
Given an input time series of $M$ dimensions and length $L$, the temporal convolution layer 
is, in other words, a 2D convolution layer with a kernel of width $K>1$ and height of $1$.
This ensures that the convolution layer will not sum up the outputs as done on MTS data 
with 1D convolution layers (see Eq.~\ref{equ:conv-layer-multi}).
The spatial convolution is as well a 2D convolution layer however with a kernel of 
width $1$ and height $M$, hence learning a linear combination of temporal features over
different dimensions.
The core idea of 1+1D convolution blocks is very similar to what 1D DWSC does, however in this case 
more parameters are learned in the network.

Each of the temporal and spatial convolution layers is followed by a BN layer 
and an ELU activation function, forming a 1+1D convolution block.
The Disjoin-CNN architecture is made of four 1+1D convolution block in series,
followed by a local max pooling operation and finishing, just like Inception~\cite{inceptiontime-paper},
by a GAP layer and an FC classifier.

\begin{figure}
    \centering
    \caption{The Disjoint-CNN architecture~\cite{disjoint-cnn-paper} for Time 
    Series Classification.}
    \label{fig:disjoin-cnn}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tsc/disjoint-cnn.pdf}
\end{figure}

\paragraph*{Convolutional Transformer (ConvTran)}

Researchers wondered the impact of Transformers and Self-Attention~\cite{attention-all-you-need}
when addressing the task of TSC.
Although no work has been published addressing univariate data, the authors in~\cite{convtran-paper}
proposed the first working transformer on multivariate TSC.
The proposed network, the Convolutional Transformer (ConvTran), consists of two 
phases, the time series encoder and the Self-Attention mechanism.
We present in Figure~\ref{fig:conv-tran} a detailed view on the ConvTran architecture with its detailed parameters' setup.
The time series encoder ensures that the attention mechanism is being applied 
over a space in which each time stamp represents one patch of the input MTS space.
The encoder used in ConvTran is a 1+1D convolution block~\cite{disjoint-cnn-paper}.

The ConvTran has two more contributions as well, as it adapts the frequency 
of the $sin$ and $cos$ functions of the Positional Encoder in
Eqs.~\ref{equ:pe-sin} and~\ref{equ:pe-cos}.
This frequency adjustment is a form of normalization to the input length and 
dimension before feeding the embeddings to the Self-Attention layer.
This normalization step is essential given that the original PE~\cite{attention-all-you-need}
was proposed for language models presenting a high dimensionality, which is not the case on average 
with the MTS datasets available in the literature.
The authors in~\cite{convtran-paper} showcased that the unnormalized PE suffers from 
its lack of ability to reflect similarity between different time stamps.
The proposed normalization to the Absolute Positional Encoder (APE) is simply the following:
\begin{equation}\label{equ:tape}
    w_k = \dfrac{w_k . d_{model}}{L}
\end{equation}
\noindent where $w_k$ is the frequency of the $cos$ and $sin$ functions in 
Eqs.~\ref{equ:pe-sin} and~\ref{equ:pe-cos}, $d_{model}$ is the dimension of the input
embedding and $L$ is the length of the series.
This normalized version of APE is referred to as Time APE (tAPE).

The second contribution of the ConvTran~\cite{convtran-paper} is an adaptation of the Relative 
Positional Encoding (RPE), which was first proposed on language models~\cite{relative-pe-paper1}.
The RPE is applied at the query and keys space.
RPE in self-attention is motivated by the need 
to encode the relative positions of tokens, rather than absolute positions, 
to better capture the relationships between tokens irrespective of their 
absolute positions in the sequence. This approach improves the model's 
ability to generalize across different sequence lengths and better handles 
long-range dependencies. It enhances performance in tasks where the relative 
positioning of words or tokens is crucial for understanding context and 
meaning.
ConvTran utilizes a shift based RPE instead of an index based one, taking into consideration 
that there should be a unique positional encoding for indices with a specific shift between them.
This results in a set of scalars $w_{\delta=|i-j|}$, learnable, where $i$ and $j$ 
$\in~[1,L]$ and $L$ is the length of the embedded series.
This would reduce the number of parameters to learn in the RPE to $2L-1$ instead of $(2L-1)d_{model}$.
This proposed RPE is referred to as Efficient RPE (eRPE).

In 2023, ConvTran became the state-of-the-art deep and non-deep learning model 
for multivariate TSC evaluated on the multivariate TSC UEA archive~\cite{uea-archive},
outperforming the last state-of-the-art models,
InceptionTime and ROCKET.

\begin{figure}
    \centering
    \caption{The ConvTran architecture~\cite{convtran-paper} for Time 
    Series Classification.}
    \label{fig:conv-tran}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tsc/conv_tran.pdf}
\end{figure}

\subsection{Time Series Extrinsic Regression}

Time Series Extrinsic Regression (TSER) stands as a significant advancement in the 
domain of time series analysis, offering a departure from traditional 
intrinsic methods by focusing on predicting continuous scores rather 
than discrete classes. Unlike its classification counterpart, which 
aims to assign time series data into predefined categories, extrinsic 
regression is concerned with forecasting continuous values based on both 
temporal dynamics and external factors.
In the last decade, many models have been proposed to address the task 
of TSER.
These models leverage a variety of techniques, ranging from classical 
linear regression to more sophisticated algorithms such as ensemble 
methods and neural networks.

Extrinsic regression exhibits a wide array of applications, extending 
its reach across domains such as finance, healthcare, and environmental 
science. For example, it serves as a vital tool in satellite image analysis, 
where its application involves estimating live fuel moisture content in 
vegetation~\cite{tser-live-fuel-moisture} to mitigate the risk of wildfires. Additionally, in healthcare, 
extrinsic regression plays a pivotal role, particularly in predicting heart 
rates using electrocardiogram (ECG) signals~\cite{heart-rate-estimattion} from patients, aiding in the 
diagnosis and management of cardiovascular conditions.

As more data becomes available and computational methods advance, 
extrinsic regression in time series analysis remains an active and 
promising field. Researchers and practitioners are constantly seeking 
new methods and applications, pushing the limits of predictive modeling 
and empirical analysis.
This impact was particularly significant when Monash University published 
the TSER archive~\cite{tser-archive}, which includes 19 different TSER 
datasets spanning applications from healthcare to energy monitoring.
Released in 2021, the TSER archive features 4 univariate and 15 multivariate 
time series datasets. Similar to the UCR/UEA TSC archives, the TSER archive 
facilitates benchmarking, enabling researchers to evaluate their contributions 
in TSER across a comprehensive set of datasets.

More recently, the authors in~\cite{bakeoff-tser} contributed $44$ new TSER 
datasets including $24$ univariate and $20$ multivariate, resulting in a total 
of $63$ TSER datasets when combined with the original archive~\cite{tser-archive}.
The authors in~\cite{bakeoff-tser} adapted as well some classification based 
algorithms in~\cite{bakeoff-tsc-2} to work with regression tasks.
The authors concluded in~\cite{bakeoff-tser} that the best regressors available 
now are feature based algorithms, especially DrCIF~\cite{hive-cote2.0} and 
FreshPRINCE~\cite{fresh-prince}.

In this section, we define the task at hand and detail briefly some alternations
done over some classification models to work with regression problems.

\mydefinition A TSER dataset $\mathcal{D}=\{\textbf{x}_i,y_i\}_{i=1}^{N}$ is a
a collection of $N$ multivariate time series of $M$ dimensions and length $L$ 
$\textbf{x}_i$ and their corresponding continuous real label $y_o$.

The task of TSER comes down to constructing a model $\mathcal{F}$ that can 
achieve correct continuous predictions as accurate as possible.
Unlike in TSC, the task is done by learning the parameters of a model 
$\mathcal{F}$ to correctly predict real values instead of categorized classes.
\begin{equation}\label{equ:tser}
    \mathcal{F}(\textbf{x}) = \hat{y}~\in~\mathds{R}
\end{equation}

\subsubsection{Distance Based Methods: $k$-NN}\label{tser-distance}

Similar to classification (Section~\ref{sec:tsc-distance}), distance based 
methods can be used for the task of TSER as well.
For instance, the $k$-NN model coupled with any similarity 
measure, where for each new test sample, the predicted label is simply the 
arithmetic mean of the labels of the $k$ nearest neighbors as follows:

\begin{equation}\label{equ:tser-knn}
    \hat{y} = \dfrac{1}{k}\sum_{i=1}^{k}y_{neighbor_{i}}
\end{equation}

\subsubsection{Convolution Based Methods: ROCKET and MultiROCKET}\label{sec:tser-convolution}

In~\cite{tser-archive}, the authors adapted the ROCKET~\cite{rocket} transformation model 
to work with TSER by simply replacing the RIDGE classification model 
by a RIDGE regression model.
This was done as well for MultiROCKET~\cite{multi-rocket}, the newest
adaptation of ROCKET for TSC (see Section~\ref{sec:tsc-convolution}), to work on TSER in the same way as ROCKET.

\subsubsection{Feature Based Methods: FreshPRINCE}\label{sec:tser-feature}

Feature based approach in the case of TSER should use an unsupervised feature 
extraction method as the label space, unlike in TSC in Section~\ref{sec:tsc-feature}, is not discrete.
The FreshPRINCE, consisting of a pipeline of TSFresh transformation~\cite{tsfresh}
followed by a Rotation Forrest~\cite{rotation-forest}, is adapted to TSER in~\cite{bakeoff-tser}
by replacing the C4.5 methods of tree generation by the 
Classification and Regression Tree (CART)~\cite{cls-and-res-tree}.
The prediction of all trees are finally averaged and produce the predicted value for new test samples.
The TSFresh phase of FreshPRINCE does not change.

\subsubsection{Interval Based Methods: DrCIF}\label{sec:tser-interval}

For TSC, interval based methods require \textbf{first} to extract phase 
independent intervals from the time series, \textbf{second} to extract features 
from each interval, \textbf{third} to train a classifier per features per interval and 
\textbf{fourth} to ensemble the classifiers trained.
In the case of TSER, the same pipeline is used, however the interval selection
must be purely unsupervised as the label space is not discrete, and the classifier 
is replaced by a regressor.
In the case of DrCIF (Section~\ref{sec:tsc-interval}) the regressors used are 
tree regressors and the ensemble is simply the average predicted value from each 
tree.

\subsubsection{Deep Learning Methods}\label{sec:tser-deep}

For TSC, the deep learning models in Section~\ref{sec:tsc-deep} are trained to predict 
a discrete probability distribution of each sample belonging to each class.
In the case of TSER, the label space is not discrete but rather continuous, for this reason
the deep learning model should predict one real value instead of a vector of $C$ values with a softmax activation.
To make this alternation, the last FC layer in all deep learning architectures is changed to have one output neuron
with no activation (linear by default) given there is no assumption of constraints over the label values.

In TSC, the cost function used to train the deep learning model's parameters is the categorical cross entropy 
(see Eq.~\ref{equ:cross-entropy}) as the predicted and ground truth values are probability distributions.
In the case of TSER, given the predicted and ground truth values are in fact real values, the cost function used 
is the Squared Error as such for sample $i~\in~[1,N]$ in the dataset:

\begin{equation}\label{equ:se-loss}
    \mathcal{L}_i(y_i,\hat{y}_i) = (y_i - \hat{y}_i)^2
\end{equation}

The total loss over a batch of $N$ samples is simply the average loss in Eq.~\ref{equ:se-loss}:
\begin{equation}\label{equ:se-loss-batch}
    \mathcal{L} = \dfrac{1}{N}\sum_{i=1}^{N}\mathcal{L}_i
\end{equation}

\section{Unsupervised Learning: Prototyping, Clustering and Self-Supervised}

Unsupervised learning techniques are crucial in time series analysis, 
especially when labeled data is scarce or unavailable. These methods 
facilitate the discovery of inherent patterns and structures within 
time series data. Key approaches include prototyping, clustering, and 
Self-Supervised Learning (SSL). Prototyping involves creating representative 
samples or profiles of time series, aiding in data summarization and 
visualization. Clustering groups similar time series together, enabling 
the identification of common behaviors and anomalies. Self-supervised 
learning, a form of representation learning in deep learning, leverages 
the intrinsic structure of the data to learn meaningful features, 
enhancing the performance of subsequent tasks such as classification 
or forecasting. These techniques collectively expand the toolkit for 
analyzing complex time series datasets, offering valuable insights 
across various applications.

In this section, we explore the three aforementioned unsupervised tasks: 
prototyping, clustering, and SSL. 
We will provide the necessary background material for understanding 
these concepts and discuss some state-of-the-art approaches 
addressing each task.

\subsection{Time Series Prototyping (TSP)}~\label{sec:ts-prototyping}

\begin{figure}
    \centering
    \caption{
        Time Series Prototyping comes down to finding a
        \protect\mycolorbox{255,0,0,0.45}{good representative} of the
        \protect\mycolorbox{0,40,255,0.5}{input set of time series}.
        This example uses the ECG5000 dataset of the UCR archive~\cite{ucr-archive}.
    }
    \label{fig:tsp}
    \includegraphics[width=\textwidth]{Figures/chapter_1/prototyping/tsp.pdf}
\end{figure}

Time Series Prototyping (TSP)~\cite{eamonn-prototyping} involves creating 
representative profiles or 
prototypes of time series data, as summarized in Figure~\ref{fig:tsp}, 
which can simplify the analysis and 
interpretation of large datasets. This technique is particularly useful 
in applications such as anomaly detection, pattern recognition, and data 
summarization. For instance, in healthcare, having a representative time 
series for each disease based on ECG data can significantly speed up the 
classification of new patients. By comparing a new patient's ECG time 
series to these prototypes, medical professionals can quickly identify 
the most likely diagnosis.

The task of finding a prototype for a group of time series involves 
identifying the series that minimizes the average dissimilarity to 
the others in the group.

\mydefinition Given a group of $N$ time series $\{\textbf{x}_i\}_{i=1}^{N}$,
finding the group prototype comes down to solving the following:
\begin{equation}\label{equ:tsp-task}
    \textbf{x}^{*} = arg\min_{\textbf{x}}\dfrac{1}{N}\sum_{i=1}^{N}d(\textbf{x},\textbf{x}_i)
\end{equation}
\noindent where $d(.,.)$ represents any similarity measure between two time series.

In the rest of this section, we will present some traditional ways of 
prototyping as well as developed methods presented throughout the years.

\subsubsection{Arithmetic Mean}\label{sec:arithmetic-mean}

A naive approach to time series prototyping involves calculating the 
arithmetic mean of corresponding data points from multiple time series. 
This method, often referred to as the ``mean prototype'' does not take 
into account the temporal alignment or any variations in the time series 
but simply averages the values at each time point.

Given a group of $N$ time series $\{\textbf{x}_i\}_{i=1}^{N}$
of $M$ dimensions and length $L$, the mean prototype $\textbf{x}_p$
is calculated as follows:

\begin{equation}\label{equ:mean-prototype}
    x^m_{p,t} = \dfrac{1}{N}\sum_{i=1}^{N}x^m_{i,t}
\end{equation}
\noindent where $t~\in~[1,L]$ and $m~\in~[1,M]$.

This method is straightforward but often fails to capture important 
temporal dynamics and variations in the data, making it less effective 
for applications where the temporal order and shape of the time series 
are crucial.

\subsubsection{Piece-wise Linear Segmentation and Weighting}

The approach proposed in~\cite{eamonn-prototyping} introduces an enhanced method 
for time series prototyping that combines piece-wise linear segmentation 
with a weighting scheme to capture the importance of different segments. 
This method, referred to as ``Weighted Piece-wise Linear Segmentation'', 
involves representing each time series as a series of linear segments 
and assigning weights to these segments based on their relevance.

Prior to prototype mining, each series $\textbf{x}_i$ goes through a segmentation 
step and $S$ segments are extracted:
\begin{equation}\label{equ:segmented}
    \{(\textbf{x}_{t_{1l}},\textbf{x}_{t_{1r}}),
    (\textbf{x}_{t_{2l}},\textbf{x}_{t_{2r}}),\ldots,
    (\textbf{x}_{t_{2Sl}},\textbf{x}_{t_{Sr}})\}
\end{equation}
\noindent where $(\textbf{x}_{t_{sl}},\textbf{x}_{t_{sr}})$ denotes the start (left: l) and end (right: r) 
of a segment under the constraint: $t_{sl} < t_{sr}$.
Each segment is assigned, by the segmentation algorithm, a weight $w_s$
and $s~\in~[1,S]$.

To be able to merge two series of the group: $\textbf{x}_i$ and $\textbf{x}_j$ where
$i$ and $j~\in~[1,N]$, the following steps are taken for each segment $s~\in~[1,S]$
and each dimension $m~\in~[1,M]$,

\begin{itemize}
    \item \textbf{Step 1:} Compute the $sign$ and $mag$ (magnitude difference):
    \begin{equation}\label{equ:eamonn-prot-step1-sign}
        sign = 
        \begin{cases} 
        -1 & \text{if } w_{i,s} \cdot w_{j,s} < 0 \\
        1 & \text{otherwise}
        \end{cases}
    \end{equation}
    \begin{equation}\label{equ:eamonn-prot-step1-mag}
        mag = \dfrac{\min(|w_{i,s}|,|w_{j,s}|)}{\max(|w_{i,s}|,|w_{j,s}|)}
    \end{equation}
    \item \textbf{Step 2:} Compute the combined segment values:
    \begin{equation}\label{equ:eamonn-prot-step2-left}
        x^m_{p,t_s} = \dfrac{x^m_{i,t_{sl}}.w_{i,s} + x^m_{j,t_{sl}}.w_{j,s}}{w_{i,s}+w_{j,s}}
    \end{equation}
    \begin{equation}\label{equ:eamonn-prot-step1-right}
        x^m_{p,t_{s+1}} = \dfrac{x^m_{i,t_{sr}}.w_{i,s} + x^m_{j,t_{sr}}.w_{j,s}}{w_{i,s}+w_{j,s}}
    \end{equation}
    \item \textbf{Step 3:} Compute the weight for the segment:
    \begin{equation}\label{equ:eamonn-prot-step3-weight}
        w_{p,s} = (w_{i,s}.w_{j,s}).(1+\dfrac{sign.mag}{1+d})
    \end{equation}
    \noindent where $d$ is a scale factor calculated as follows:
    \begin{equation}\label{equ:eamonn-prot-step3-scale}
        d = |\dfrac{(x^m_{i,t_{sl}} - x^m_{j,t_{sl}}) - (x^m_{i,t_{sr}} - x^m_{j,t_{sr}})}{t_{sr} - t_{sl}}|.norm
    \end{equation}
    \noindent and $norm$ is calculated as follows:
    \begin{equation}\label{equ:eamonn-prot-step3-norm}
        norm = \max(\max(\textbf{x}^m_{i,:l}), \max(\textbf{x}^m_{i,:r})) - \min(\min(\textbf{x}^m_{i,:l}),\min(\textbf{x}^m_{i,:r}))
    \end{equation}
\end{itemize}

To reconstruct the time series from the new segments, the authors in~\cite{eamonn-prototyping}
used the piece-wise linear representation created by the merging process.

\subsubsection{Elastic Barycenter Averaging Methods}

Elastic Barycenter Averaging (EBA) is a technique first addressed by~\cite{dba-paper} that proposed a combination between 
the elastic similarity measure DTW (see Alg.~\ref{alg:dtw}) in order to find a prototype of a group of time series.
The core difference between the arithmetic Mean in Section~\ref{sec:arithmetic-mean} and the first proposed method based on
EBA: DTW Barycenter Averaging (DBA)~\cite{dba-paper} is that DBA takes into consideration the alignment information between 
all samples in the group.
This results in a prototype representing the \emph{average warping} as well as the average amplitude, on contrary with 
arithmetic Mean which considers all series are aligned.
Another powerful unique feature with EBA methods, starting with DBA in~\cite{dba-paper}, is their ability to find 
prototypes over a group of unequal length time series samples.

The detailed working of DBA is presented in Algorithm~\ref{alg:dba}.
DBA initializes a prototype by randomly choosing one series in the group, and iteratively optimizes this prototype by 
finding for each of its time stamps, the aligned time stamps with all other series in the group (referred to as associates of the 
prototype's time stamp).
The value of the time stamp of the current prototype is then replaced by the barycenter (arithmetic mean) of the aligned values with it.
Theoretically, DBA works with any value of $q$ in $DTW_q$ (see Eq.~\ref{equ:dtw}), however the authors in~\cite{neares-centroid-dba-paper}
proved that in the case of $q=2$, DBA converges and the optimal solution is using the arithmetic mean over aligned points.

\begin{algorithm}
    \caption{DTW Barycenter Averaging (DBA)}
    \label{alg:dba}
    \begin{algorithmic}[1]
        \REQUIRE Group of $N$ Time Series $\{\textbf{x}\}_{i=1}^{N}$ of length $L$ and dimension $M$ each
        \REQUIRE Initial prototype series $\textbf{x}_p$ of length $L$ and dimension $M$
        \ENSURE Time Series of length $L$ and dimension $M$: The DBA prototype representative of the group

        \STATE $CountAssociates\gets zeros(shape=(L,))$

        \FOR{$i=1~\to~N$}
            \STATE $\pi \gets DTW_{path}(\textbf{x}_p,\textbf{x}_i)$ 

            \FOR{$j=1~\to~len(\pi)$}
                \STATE $t_1\gets \pi_{j,1}$
                \STATE $t_2\gets \pi_{j,2}$
                \STATE $\textbf{x}_{p,t_1} = \textbf{x}_{p,t_1} + \textbf{x}_{i,t_2}$
                \STATE $CountAssociates_{t_1}\gets CountAssociates_{t_1} + 1$
            \ENDFOR
        \ENDFOR

        \STATE \textbf{Return:} $\textbf{x}_p / CountAssociates$

    \end{algorithmic}
\end{algorithm}

More recently, a new version of DBA has been proposed in~\cite{mba-paper} that replaces the DTW similarity measure 
by the Move-Split-Merge (MSM) measure~\cite{msm-distance}, given that it has been seen to outperform DTW for 
clustering~\cite{elastic-clustering-review}.
The proposed method, MSM Barycenter Averaging (MBA) in~\cite{mba-paper} outperformed the usage of DBA for clustering.

% Elastic distance measures like DTW use three types of moves: diagonal, horizontal, and vertical, to align sequences. 
% DTW does not penalize non-diagonal moves explicitly but instead relies on path length and a window cutoff to prevent 
% excessive warping. In contrast, edit distances such as MSM consider diagonal moves as matches, vertical moves as 
% insertions, and horizontal moves as deletions, explicitly defining the cost of each type of move to provide a different 
% approach to sequence alignment.

Elastic similarity measures such as DTW, as explained in Algorithm~\ref{alg:dtw}, uses three moves on the cost matrix,
diagonal move, horizontal move and vertical move.
In terms of DTW, the algorithm penalizes a miss alignment by simply producing longer warping path with higher number 
of non-diagonal alignments.
This can be problematic in some cases, as we need the algorithm to penalize directly the non-diagonal alignment itself 
rather than utilizing the outcome as one global penalty.
Moreover, edit distances such as MSM~\cite{msm-distance} considers that a diagonal move is a match,
and vertical/horizontal moves are considered as insertion/deletion (split/merge) and are penalized instantly when the move 
is made.
This penalty in MSM depends on a hyper-parameter called $c$ which represents the penalty minimum cost.
For a match movement (diagonal), the MSM uses the absolute difference between matched values instead of the squared error such as 
in DTW.
However, when two values are not matched (horizontal or vertical), the MSM uses a different functionality.
For instance assuming two series $\textbf{x}_1$ and $\textbf{x}_2$ for which the MSM is finding the alignment path between them,
the cost for a split (insertion/vertical) movement is calculated as such on dimension $m~\in~[1,M]$:
\begin{equation}\label{equ:msm-split}
    C(x^m_{1,t_1},x^m_{1,t_1-1},x^m_{2,t_2},c) = 
    \begin{cases}
        c  \hspace*{2cm}\textbf{if } x^m_{1,t_1-1}\leq x^m_{1,t_1} \leq x^m_{2,t_2}\\
        c  \hspace*{2cm}\textbf{if } x^m_{1,t_1-1}\geq x^m_{1,t_1} \geq x^m_{2,t_2}\\
        c + \min(|x^m_{1,t_1}-x^m_{1,t_1-1}|,|x^m_{1,t_1}-x^m_{2,t_2}|)  \textbf{  otherwise}
    \end{cases}
\end{equation}
\noindent where the above operation consists on inserting $x^m_{1,t_1}$ between $x^m_{1,t_1-1}$ and 
$x^m_{2,t_2}$.

For the deletion operation (horizontal movement), its the inverse of the insertion operation as the series swap in Eq.~\ref{equ:msm-split}
by calling $C(x^m_{1,t_2},x^m_{2,t_2-1},x^m_{1,t_1},c)$.

The MSM algorithm is detailed in Algorithm~\ref{alg:msm}.

\begin{algorithm}[t]
    \caption{Move-Split-Merge (MSM)}
    \label{alg:msm}
    \begin{algorithmic}[1]
        \REQUIRE Two Time Series $\textbf{x}_1$ and $\textbf{x}_2$ of length $L$ and dimension $M$
        \REQUIRE The cost penalty parameter $c$
        \ENSURE MSM measure between \(\textbf{x}_1\) and \(\textbf{x}_2\)
        
        \STATE $\tilde{D} \gets array [L,L] = \{0.0\}$
        \FOR{$m=1~\to~M$}
        \STATE $D \gets array [L,L] = \{0.0\}$
            \STATE $D[1,1]\gets |x^m_{1,1} - x^m_{2,1}|$
            \FOR{$t=2~\to~L$}
                \STATE $D[t,1]\gets D[t-1,1] + C(x^m_{1,t},x^m_{1,t-1},x^m_{2,1},c)$
            \ENDFOR
            \FOR{$t=2~\to~L$}
                \STATE $D[1,t]\gets D[1,t-1] + C(x^m_{2,t},x^m_{2,t-1},x^m_{1,1},c)$
            \ENDFOR

            \FOR{$t_1=2~\to~L$}
                \FOR{$t_2=2~\to~L$}
                    \STATE $move\gets D[t_1-1,t_2-2] + |x^m_{1,t_1} - x^m_{2,t_2}|$
                    \STATE $split\gets D[t_1-1,t_2] + C(x^m_{1,t_1},x^m_{1,t_1-1},x^m_{2,t_2},c)$
                    \STATE $merge\gets D[t_1,t_2-1] + C(x^m_{2,t_2},x^m_{2,t_2-1},x^m_{1,t_1},c)$

                    \STATE $D[t_1,t_2] = \min(move,split,merge)$
                \ENDFOR
            \ENDFOR
            \STATE $\tilde{D} = \tilde{D} + D$
        \ENDFOR

        \STATE \textbf{Return:} $\tilde{D}[L,L]$
        
    \end{algorithmic}
\end{algorithm}

\subsubsection{Differentiable EBA: SoftDTW Barycenter Averaging (SoftDBA)}

As mentioned in Section~\ref{sec:tsc-distance}, the authors in~\cite{soft-dtw-distance} argue the need of a differentiable
version of DTW and proposed SoftDTW.
However, this is not the only contribution of~\cite{soft-dtw-distance}, as in fact the authors proposed a novel version of
DBA~\cite{dba-paper} which does not simply replace DTW by SoftDTW, but learns the optimal barycenter using a gradient based 
optimization algorithm.
The reason that makes this possible is the fact that SoftDTW is differentiable and the solution to Eq.~\ref{equ:tsp-task}
can be found through a gradient optimization approach, where the gradient to be found is the following:
\begin{equation}\label{equ:softdtw-gradient}
    \nabla_{\textbf{x}} DTW^{\gamma}(\textbf{x},\textbf{x}_i)
\end{equation}
\noindent where $\textbf{x}_i$ is one series in the group of size $N$ to be prototyped $i~\in~[1,N]$ and $DTW^{\gamma}$ 
is the SoftDTW algorithm with smoothness parameter $\gamma$.

\subsubsection{Time Elastic Kernel Averaging (TEKA)}

\cite{marteau2019times} proposes a probabilistic approach, TEKA, to time series averaging and denoising based 
on time-elastic kernels. By interpreting kernel alignment matrices probabilistically, the method 
introduces a stochastic alignment automaton to compute the centroid of a set of time series. This 
process effectively captures both temporal dynamics and structural shape, allowing for robust 
averaging and noise reduction. Empirical evaluations across 45 datasets highlight its effectiveness, 
showing significant performance improvements for centroid-based classifiers over medoid-based counterparts. 
Moreover, the method proves valuable in reducing training set sizes for applications such as gesture 
recognition, demonstrating its utility in both denoising and efficient representation (condensing).

\subsection{Time Series CLustering (TSCL)}\label{sec:tscl}

Clustering is a fundamental technique in data analysis that involves grouping a set of objects in such a way that objects in 
the same group (or cluster) are more similar to each other than to those in other groups. This similarity is measured based 
on certain features or characteristics of the data. Clustering is widely used in various fields such as marketing for customer 
segmentation, biology for classifying species, and document clustering in text analysis.
Time Series Clustering (TSCL) specifically deals with temporal data, where the objective is to group time series that exhibit 
similar behaviors or patterns over time. Unlike traditional clustering, time series clustering must handle the unique 
characteristics of time-dependent data, such as temporal ordering, trends, and seasonality. Effective clustering of 
time series can reveal important insights in fields like finance, healthcare, and climate science.
In this section we review three common approaches in the literature of TSCL: (1) $k$-means clustering 
algorithm coupled with elastic similarity measures and EBA methods, (2) shape based algorithms and (3) deep learning methods.

\subsubsection{$k$-means with Elastic Barycenter Averaging}

$k$-means with Elastic Barycenter Averaging (EBA) is an enhanced version of the traditional $k$-means algorithm~\cite{kmeans-paper} 
designed for time series data. EBA addresses the alignment and averaging challenges of time series by allowing 
temporal distortions during the centroid calculation. This makes $k$-means more robust and accurate for clustering 
time series data, as it can handle shifts and variations in the time sequences.
A detailed view on how $k$-means work for TSCL is presented in Algorithm~\ref{alg:kmeans-eba}.
$k$-means initializes random centroids $\{\textbf{s}_j\}_{j=1}^k$ at the beginning and then updates the centroids following the set of nearest samples in the 
data $\mathcal{D} = \{\textbf{x}_i\}_{i=1}^N$.
This algorithm converges when the inertia presents a small non-significant change, compared to a threshold.
The inertia is computed as the sum of distances between each sample and its nearest centroid:
\begin{equation}\label{equ:inertia-kmeans}
    inertia = \sum_{j=1}^k~\sum_{\textbf{x}_i~\in~NN(\textbf{s}_j,\mathcal{D},d)}~d(\textbf{x}_i,\textbf{s}_j)
\end{equation}
\noindent where $d(.,.)$ is a similarity measure between two time series and $NN(\textbf{s}_j,\mathcal{D})$ is the set of time 
series in $\mathcal{D}$ that are nearest to centroid $\textbf{s}_j$ following the measure $d(.,.)$.

The authors of~\cite{dba-paper} proposed the setup of $k$-means coupled with DTW similarity measure and DBA averaging method.
Through extensive experiments on different datasets, the proposed setup for $k$-means outperformed the original setup 
which relies on using Euclidean Distance and Arithmetic Mean.

This setup was changed by~\cite{soft-dtw-distance} by using the SoftDTW similarity measure and SoftDBA averaging method 
for $k$-means and showcased how it can outperform the previous setup with DBA proposed in~\cite{dba-paper}.
The authors of \cite{soft-dtw-distance} argue that the optimization steps of SoftDBA, facilitated by the differentiability 
of SoftDTW, effectively eliminate noise in the time series samples. This process yields more accurate centroids, 
thereby enhancing clustering performance.

\begin{algorithm}[H]
\caption{$k$-means with Elastic Barycenter Averaging}
\label{alg:kmeans-eba}
\begin{algorithmic}[1]
\REQUIRE $N$ time series samples $\mathcal{D}=\{\textbf{x}_i\}_{i=1}^N$
\REQUIRE Number of clusters $k$
\REQUIRE Similarity measure $d(.,.)$ between two time series 
\REQUIRE Averaging method $A()$ between a set of time series
\REQUIRE Maximum number of iterations $max_{itr}$ in case of no convergence
\REQUIRE Threshold $\epsilon$ for inertia convergence check
\ENSURE Cluster centroids $\{\textbf{s}_j\}_{j=1}^k$ and cluster assignments
\FOR{$j=1~\to~k$}
\STATE $\textbf{s}_j~\gets~random\_choice(\mathcal{D})$
\ENDFOR
\STATE $previous\_inertia~\gets~\infty$
\FOR{$itr=1~\to~max_{itr}$}
    \STATE $previous\_inertia~\gets~current\_inertia$
    \STATE $assigned\_cluster~\gets~zeros(size=(N,))$
    \FOR{$i=1~\to~N$}
        \STATE $distances~\gets~zeros(size=(k,))$
        \FOR{$j=1~\to~k$}
            \STATE $distances[j]~\gets~d(\textbf{x}_i,\textbf{s}_j)$
        \ENDFOR
        \STATE $assigned\_cluster[i]~\gets~arg\min_{j~\in~[1,k]} distances$
        \STATE $current\_inertia~\gets~current\_inertia + \min(distances)$
    \ENDFOR
    \IF{$|current\_inertia - previous\_inertia| < \epsilon$}
        \STATE $break$
    \ENDIF
    \STATE $previous\_inertia~\gets~current\_inertia$
    \FOR{$j=1~\to~k$}
        \STATE $\textbf{s}_j~\gets~A(\{\textbf{x}_i~|~assigned\_cluster[i] = j\})$
    \ENDFOR
\ENDFOR
\STATE \textbf{Returns:} $\{\textbf{s}_j\}_{j=1}^k$, $assigned\_cluster$
\end{algorithmic}
\end{algorithm}

\subsubsection{Shape Based Method: $k$-shape}

The $k$-shape algorithm proposed in~\cite{kshape-paper} not only outperformed previous approaches with $k$-means,
but also presented a significantly faster TSCL approach.
For instance $k$-shape does not rely on using elastic similarity measures that suffer from time complexity,
instead it utilizes a Shape Based Distance (SBD).
Given two time series $\textbf{x}_1$ and $\textbf{x}_2$ of length $L$, the SBD used in~\cite{kshape-paper} relies on the
Normalized Cross Correlation (NCC) between $\textbf{x}_1$ and the shifted version of $\textbf{x}_2$.
The shifted version of $\textbf{x}_2$ of shift $s$ is computed as follows:
\begin{equation}\label{equ:kshape-shift}
    \textbf{x}_2(s) = 
    \begin{cases}
        \{\overbrace{0,\ldots,0}^{length=s},x_{2,1},x_{2,2},\ldots,x_{2,L-s}\} & if~s > 0\\
        \{x_{2,1-s},x_{2,2-s},\ldots,x_{2,L},\overbrace{0,\ldots,0}^{length=s}\} & if~s < 0
    \end{cases}
\end{equation}
And the NNC between both series is computed as follows:
\begin{equation}\label{equ:kshape-ncc}
    NCC(\textbf{x}_1,\textbf{x}_2) = \max_{s~\in~\mathcal{S}}~
    \dfrac{\sum_{t=1}^L~x_{1,t}.x_{2,t+s}}{L.module(\textbf{x}_1).module(\textbf{x}_2)}
\end{equation}
\noindent with the assumption that both series are normalized to zero mean and unit standard deviation.
The $module(.)$ operation is simply the sum of squares of all elements in the series and
$\mathcal{S}=\{-L+1,-L+2,\ldots,L-2,L-1\}$.
The final SBD is calculated as follows:
\begin{equation}\label{equ:kshape-sbd}
    SBD(\textbf{x}_1,\textbf{x}_2) = 1 - NCC(\textbf{x}_1,\textbf{x}_2)
\end{equation}
\noindent For the case of MTS data, the SBD is simply the aggregation of SBD applied on each dimensions independently.
The SBD returns as well the aligned version of $\textbf{x}_2$ (referred to as $\textbf{x}_2^{'}$) following Eq.~\ref{equ:kshape-shift} using the optimal shift
from Eq.~\ref{equ:kshape-ncc}.

The centroid finding technique used in $k$-shape over a set of time series $\mathcal{D}=\{\textbf{x}_i\}_{i=1}^N$
is the solution to the following equation:
\begin{equation}\label{equ:kshape-equation-avg}
    \textbf{s}_j^* = arg\max_{\textbf{s}_j}~
    \sum_{\textbf{x}_i~\in~NN(\textbf{s}_j,\mathcal{D},SBD)}NNC(\textbf{x}_i,\textbf{s}_j)^2
\end{equation}
\noindent where $i~\in~[1,N]$ and $j~\in~[1,k]$.

The authors in~\cite{kshape-paper} mentioned that the above optimization problem, after some alternations,
can be solved using the problem of maximization of the Rayleigh Quotient~\cite{golub2013matrix}.
One iteration of the $k$-shape centroid extraction (shape extraction as referred to in the paper) 
phase is presented in Algorithm~\ref{alg:kshape-centroid}.

\begin{algorithm}[H]
\caption{$k$-shape: Shape Extraction, One Iteration}
\label{alg:kshape-centroid}
\begin{algorithmic}[1]

\REQUIRE Set of Time Series in the same cluster $\textbf{X}=\{\textbf{x}_i\}_{i=1}^N$, assumed z-normalized
\REQUIRE The current centroid of this cluster $\textbf{s}$
\ENSURE A more accurate centroid $\textbf{s}^{'}$

\STATE $\textbf{X}^{'}$ = [~]
\STATE $\textbf{O}~\gets~[[1~for~o=1~\to~L]~for~oo=1~\to~L]$
\FOR{$i=1~\to~N$}
    \STATE $distance, \textbf{x}_i^{'}~\gets~SBD(\textbf{s},\textbf{x}_i)$
    \STATE $\textbf{X}^{'}.append(\textbf{x}_i^{'})$
\ENDFOR
\STATE $\textbf{A}~\gets~\textbf{X}^{'T}\circ \textbf{X}^{'}$
\STATE $\textbf{Q}~\gets~I_L - \dfrac{1}{L}.\textbf{O}$
\STATE $\textbf{M}~\gets~\textbf{Q}^{T}\circ \textbf{A}\circ \textbf{Q}$
\STATE $\textbf{s}^{'}~\gets EigVectors(\textbf{M})$
\STATE \textbf{Returns:} $\textbf{s}^{'}$

\end{algorithmic}
\end{algorithm}

\subsubsection{Deep Learning Methods}

While deep learning has revolutionized Time Series Classification~\cite{dl4tsc}, its potential for Time Series Clustering is still 
being explored. Recognizing the success of deep learning in classification tasks, researchers have hypothesized that 
it can also significantly enhance clustering methods. Consequently, various deep learning-based approaches for time 
series clustering have been proposed and reviewed in recent literature~\cite{deep-tscl-bakeoff}.
In the mentioned review~\cite{deep-tscl-bakeoff}, the authors not only compared deep learning models for TSCL on the 
architecture level, however they also compared different methods to train these models for better clustering downstream task.

Deep learning models, such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Auto-Encoders (AEs), 
offer several advantages for clustering tasks. These models can automatically learn complex features and patterns directly 
from raw time series data, bypassing the need for manual feature extraction. RNNs, particularly Long Short-Term Memory 
(LSTM) networks, are adept at capturing long-term dependencies in sequential data, which are often overlooked by traditional 
clustering methods. CNNs, on the other hand, can efficiently process large-scale time series data in parallel, making 
them suitable for handling extensive datasets.

The adaptability and scalability of deep learning models make them promising for time series clustering. 
Their ability to model non-linear relationships and intricate temporal dependencies leads to more accurate 
and meaningful clustering results. As researchers continue to explore and refine these methods, deep learning 
is poised to offer robust solutions for clustering complex time series data, paralleling its success in 
classification tasks~\cite{dl4tsc}.

\begin{figure}
    \centering
    \caption{AE based architecture with a reconstruction loss
    for Time Series CLustering. The \protect\mycolorbox{255,0,0,0.47}{first step} is to train 
    the AE architecture to reconstruction the \protect\mycolorbox{0,40,255,0.5}{input time series}.
    The \protect\mycolorbox{189,109,226,0.47}{second step} is to generate the 
    \protect\mycolorbox{255,165,9,0.6}{latent features} and apply a clustering algorithm on top
    of these feature.}
    \label{fig:auto-encoding-clustering}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tscl/rec-auto-encoder.pdf}
\end{figure}

In this section, we will detail different architectural methods for deep clustering. Deep clustering can be performed 
using AEs or even standalone encoders with various pretext losses. These pretext losses include traditional 
reconstruction loss, multi-reconstruction~\cite{multi-rec-paper} loss between encoder and decoder,
triplet loss~\cite{triplet-loss-paper}, clustering losses~\cite{clustering-loss-paper}, and more.
An overview of AE architectures used for TSCL is presented in Figure~\ref{fig:auto-encoding-clustering}.
The review~\cite{deep-tscl-bakeoff} concluded that the best-performing architectures were a 
ResNet-based~\cite{fcn-resnet-mlp-paper} 
AE with multi-reconstruction loss 
and no clustering loss for univariate time series, and a non-symmetrical Dilated Recurrent Neural 
Network (DRNN)~\cite{drnn-paper} AE 
with multi-reconstruction loss and no clustering loss for multivariate time series.

\paragraph{Background on Auto-Encoders and Variational Auto-Encoders}

Auto-Encoders (AEs) are a type of artificial neural network initially 
introduced in the 1980s by~\cite{kramer1991nonlinear} for the purpose of unsupervised learning, 
particularly for tasks like dimensionality reduction and feature 
learning. An AE consists of two main parts: an encoder and 
a decoder. The encoder maps the input series $\textbf{x}$ of length $L$ and dimension $M$ into a latent space 
representation $\textbf{z}$, and the decoder reconstructs the input data from 
this latent representation.
This structure allows the AE to learn efficient codings of 
the data by minimizing the reconstruction error between the original 
input and its reconstructed output. The mathematical representation 
is as follows:
\begin{enumerate}
    \item Encoder: $\textbf{z} = E_{\theta}(\textbf{x})$
    \item Decoder: $\hat{\textbf{x}} = D_{\phi}(\textbf{z})$
\end{enumerate}
\noindent where each of the encoder $E_{\theta}(.)$ and decoder $D_{\theta}(.)$ is parametrized 
by a set of parameters for each of their layers, $\theta$ and $\phi$ respectively.

The objective is to minimize the reconstruction error, 
typically measured by the Mean Squared Error (MSE):

\begin{equation}\label{equ:ae-mse}
    \mathcal{L}_{mse}(\textbf{x},\hat{\textbf{x}}) = 
    \dfrac{1}{L.M}\sum_{t=1}^L~\sum_{m=1}^M (x^m_t - \hat{x}^m_t)^2
\end{equation}

However, traditional AEs face limitations in generating new 
data samples because they do not provide a probabilistic framework 
for the latent space.
This is where Variational Auto-Encoders (VAEs)~\cite{vae-paper} come into play. 
VAEs introduce a probabilistic approach to the latent space, 
allowing for both data reconstruction and generation of new data 
samples. By leveraging the principles of variational inference, 
VAEs model the distribution of the latent variables explicitly, 
which makes them highly effective for generative tasks.

The  (VAE) consists, like the traditional AE, of an encoder-decoder framework. However, unlike in AEs, the 
latent space in VAEs is regularized to follow a specific distribution, 
usually a Gaussian distribution. In traditional AEs, the latent space 
is not constrained to follow any distribution, which can lead to 
significant diversity in the placement of latent variables, potentially 
causing model collapse.

VAEs address this issue by incorporating a Gaussian projection step in 
the latent space. Specifically, the encoder in a VAE outputs two vectors: 
one representing the mean and the other representing the variance of a 
Gaussian distribution. This Gaussian distribution is then used to randomly 
sample points in the latent space before feeding them to the decoder. 
This sampling ensures continuity and smoothness in the latent space, 
which helps in generating coherent outputs.
To prevent the encoder from producing a high-variance Gaussian distribution, 
which could also lead to model collapse, VAEs include a regularization loss. 
This regularization loss is the Kullback-Leibler (KL) divergence between 
the learned Gaussian distribution $q_{\theta}(\textbf{z}|\textbf{x})$ 
and the standard normal distribution $\mathcal{N}(0,1)$. 
The KL divergence loss ensures that the learned distribution 
remains close to the prior distribution, maintaining the stability 
and structure of the latent space:
\begin{equation}\label{equ:vae-kl}
\begin{split}
    \mathcal{L}_{KL} &= D_{KL}(q_{\theta}(\textbf{z}|\textbf{x}),\mathcal{N}(0,1))\\
    &= -\dfrac{1}{2}~\sum_{d=1}^\textbf{d}(1+\log\sigma_d^2-\mu_d^2-\sigma_d^2)
\end{split}
\end{equation}
\noindent where $\boldsymbol{\mu}$ and $\log~\boldsymbol{\sigma}^2$ are both the mean
and $\log$ of the variance learned by the encoder part of the VAE,
and $\textbf{d}$ is the dimension of the latent space.

The total loss of the VAE used to optimize its parameters is a weighted sum of both
the reconstruction and KL loss:
\begin{equation}\label{equ:vae-total-loss}
    \mathcal{L}_{vae} = (1-\beta).\mathcal{L}_{mse} + \beta.\mathcal{L}_{KL}
\end{equation}
\noindent where $\beta$ is a hyperparameter between $0$ and $1$ used 
to control the amount of impact of the KL loss on the training phase, which was introduced 
in~\cite{beta-vae-paper}.

\paragraph{Using AEs and VAEs for TSCL}

\begin{figure}
    \centering
    \caption{AE based architecture with a multi-reconstruction loss~\cite{multi-rec-paper}
    for Time Series CLustering. The first step is to train 
    the AE architecture to reconstruction the \protect\mycolorbox{0,40,255,0.5}{input time series}
    as well as \protect\mycolorbox{255,0,0,0.47}{each layer of the AE network}.
    The second step is the same as in Figure~\ref{fig:auto-encoding-clustering}}
    \label{fig:multi-rec-auto-encoding-clustering}
    \includegraphics[width=\textwidth]{Figures/chapter_1/dl4tscl/multi-rec.pdf}
\end{figure}

The review in~\cite{deep-tscl-bakeoff} presented multiple approaches 
from the literature that addressed the task of TSCL through deep learning.
The best winning approaches are AE and VAE based networks that uses a
multi-reconstruction loss.
The multi-reconstruction loss has the same functionality as the reconstruction 
loss in Eq.~\ref{equ:ae-mse}, however it is applied between each depth of the 
encoder with its symmetrical depth in the decoder.
This is illustrated in Figure~\ref{fig:multi-rec-auto-encoding-clustering}.

In the case of univariate datasets, the review~\cite{deep-tscl-bakeoff}
highlights that the winning model is an AE based architecture that utilizes
ResNet~\cite{fcn-resnet-mlp-paper} as a backbone network.
In order to define a ResNet decoder, a symmetrical architecture is defined that 
replaces the standard convolutions by transpose convolutions.
\mydefinition
The one dimensional transpose convolution increases the length of the input series 
instead of decreasing it.
This is done through a de-convolution step.
Given an input univariate series $\textbf{x}$ of length $L$ and a kernel $\textbf{w}$ of length $K$,
the one dimensional transpose convolution is applied as follows:
\begin{equation}\label{equ:1d-transpose-convolution}
    o_t = \sum_{t^{'}=1}^L~\sum_{k=1}^K~x_{t^{'}}.w_k.\delta(t,s.t^{'}+k)
\end{equation}
\noindent where $\textbf{o}=\{o_1,o_2,\ldots,o_{(L-1).s+k}\}$ is the output series 
of length $(L-1).s+k$ and $s$ is the number of strides.
In the case of multivariate input series and multiple kernels used, transpose convolution 
follow the same protocol as standard convolutions, see Eq.~\ref{equ:conv-layer-multi}.

Using the above transpose convolution, the ResNet based AE network can be defined.
Posterior to training the AE network, the latent space is subsequently used to train 
a simple $k$-means cluster using the arithmetic mean and ED as parameters.

The review of deep TSCL methods~\cite{deep-tscl-bakeoff} showed however that using the VAE 
regularization can degrade the clustering performance.

Moreover, for multivariate datasets, the winning approach was an RNN based 
AE using  the Dilated-RNN (DRNN) architecture~\cite{drnn-paper}.
The DRNN AE architecture consists of three bidirectional GRU layers stacked 
on top of each other in the encoder part with a single GRU layer for the decoder.
This architecture performed as the best deep clustering model for MTS data 
coupled with the reconstruction loss, unlike ResNet with the multi-reconstruction loss
in the case of UTS data.
The main reason to why the multi-reconstruction loss was not utilized in the case of 
DRNN AE network is that its a non-symmetrical AE architecture.

\subsection{Self-Supervised Learning for Time Series Analysis}

\begin{figure}
    \centering
    \caption{The difference between using deep learning for Time Series Classification,
    Extrinsic Regression and Representation Learning (Self-Supervised Learning, SSL).}
    \label{fig:deep-ssl}
    \includegraphics[width=\textwidth]{Figures/chapter_1/ssl/deep-ssl.pdf}
\end{figure}

SSL, sometimes referred to as representation learning, 
is a machine learning paradigm where the model learns a compact representation 
of its input data without relying on labeled samples.
The primary goal is to learn useful representations from the input data 
that can be utilized for various downstream tasks. By learning from large 
volumes of unlabeled data, SSL enables models to 
capture intricate patterns and structures, reducing the dependency on 
extensive labeled datasets.
Figure~\ref{fig:deep-ssl} summarizes the pipeline of using a deep learning model for SSL,
and the difference with other supervised tasks.
Unlike supervised models, where deep learning models need to predict target values that are 
commonly known, deep learnign for SSL tries to find the best latent representation of the input data.

In the context of time series data, SSL can be 
particularly beneficial.
Time series data, which involves patterns and dependencies over time, can be 
difficult to label accurately and thoroughly
By employing self-supervised techniques, 
models can learn to understand these patterns and temporal structures 
by predicting future points from past points or filling in missing data 
segments etc. This pre-training process results in robust feature 
representations that can significantly enhance the performance 
of downstream tasks such as forecasting, anomaly detection, and 
classification, even when labeled data is scarce.

Pre-trained models from SSL can greatly enhance 
generalization in downstream tasks, especially in scenarios with 
limited labeled data. These models, having been trained on large 
unlabeled datasets, possess a rich understanding of the underlying 
data distribution and can transfer this knowledge to specific tasks. 
This transfer learning approach is particularly useful in semi-supervised 
setups, where only a small fraction of the data is labeled. 
By fine-tuning pre-trained models on the available labeled data, we 
can achieve superior performance compared to training models from scratch. 
Thus, SSL not only alleviates the challenge of 
data scarcity but also promotes better generalization and adaptability 
in various real-world applications.

In this section, we present briefly some starting work of self-supervised 
learning for time series analysis, mostly used for downstream classification 
tasks.

\subsubsection{Background on Siamese Neural Networks and SimCLR}

Siamese networks, introduced in the early 1990s~\cite{siamese-networks}, 
are a type of neural network architecture used to determine the 
similarity between two input samples. These networks consist of 
two identical subnetworks that share the same parameters and weights. 
They are trained using pairs of inputs, where the goal is to learn a 
function that can effectively distinguish between similar and 
dissimilar pairs. This method is particularly well-known for its 
application in signature verification and face verification tasks.

The original approach to training a Siamese network consists on defining 
for each sample a positive and negative representation of it.
For instance in the case of signature 
verification task, it would consist on the same signature written twice, once by the same anchor subject 
and the second forged by a second subject.
First, $\textbf{x}_1$ and $\textbf{x}_2$, are fed to the same 
network to obtain $\textbf{z}_1$ and $\textbf{z}_2$, two feature representations
of the two input samples.
Second, the ED is calculated between these two feature vectors.
Third, the contrastive loss is computed as follows:
\begin{equation}\label{equ:contrastive-siamese}
    \mathcal{L}_{contrastive} = \dfrac{1}{2}.(1-y).ED(\textbf{z}_1,\textbf{z}_2)^2 
    + \dfrac{1}{2}.y.\max(0, \alpha-ED(\textbf{z}_1,\textbf{z}_2)^2)
\end{equation}
\noindent where $y$ indicates if the pair of samples are an anchor with its positive 
representation or an anchor with its negative representation.
$\alpha$ is the boundary parameter that represent a penalty for the negative pairs.
The goal of the Siamese network is to learn how to increase the distance between an anchor 
sample and its negative representation and decrease the distance between the anchor and its positive 
representation.

This SSL approach was then adapted in~\cite{simCLR-paper}
to use an unlabeled setup for computing the contrastive loss.
The authors in~\cite{simCLR-paper} proposed a Simple Contrastive LeaRning 
(SimCLR) self-supervised model for image representation.
SimCLR does not rely on a labeled pair of samples such as in the original 
Siamese network.
Instead, SimCLR takes as input two augmented versions of the same sample, and never 
the original sample, and minimizes the distance between the feature representation 
of the two augmented versions.
This ensures that the latent space provides the same distance based information 
as the original space, hence can be used for downstream task e.g. simple 
linear classifier in the latent space for the classification task.

\subsubsection{Self-Supervised Learning Models for Time Series Analysis}\label{sec:self-supervised}

The above explained approach has been used as a base for almost all SSL research work 
and has been adapted to almost all domains in data science, such as time series.
In what follows, we present briefly some of these adapted work in the last five years for time series data.

\paragraph{Dilated Causal CNN (DCCNN) with Triplet Loss}

The first ever work to address SSL for time series data 
was in~\cite{triplet-loss-paper}.
The authors proposed a new architecture that they used for the self-supervised 
setup.
The proposed architecture, DCCNN, consists on multiple dilated causal convolutions 
stack on top of each other with residual networks between them.
Causal convolutions are a type of convolution that ensures predictions at any point in time only use current
and past data, never future data, which is important for time-based sequences.
However, the main contribution of the paper was not the architecture, instead it was the proposal
of a novel pretext loss, the triplet loss~\cite{facenet-paper}.
The triplet loss mechanism differs from the contrastive loss detailed in 
Eq.~\ref{equ:contrastive-siamese}, by computing the loss on both positive and negative pairs 
at the same time, without using the labeling of positive/negative pairs $y$.
The proposed loss in~\cite{triplet-loss-paper} is computed as follows:
\begin{equation}\label{equ:frans-triplet-loss}
    \mathcal{L}_{triplet} = -\log(\sigma(\textbf{f}(\textbf{x}_{ref})^T\odot\textbf{f}(\textbf{x}_{pos})))
    - \sum_{n_{neg}=1}^{N_{neg}}~\log(\sigma(-\textbf{f}(\textbf{x}_{ref})^T\odot\textbf{f}(\textbf{x}_{n_{neg}})))
\end{equation}
\noindent where $\sigma$ is the sigmoid function, $\textbf{f}$ 
is a deep model used to encode feature vectors, $\odot$ is the matrix 
multiplication operation, $^T$ is the transpose operation, $N_{neg}$
is the number of negative samples $\{\textbf{x}_{n_{neg}}\}_{n_{neg}=1}^{N_{neg}}$ per reference sample $\textbf{x}_{ref}$, and 
$\textbf{x}_{pos}$ is the positive representation of $\textbf{x}_{ref}$.

Given the loss defined in Eq.\ref{equ:frans-triplet-loss}, there should be a way 
to define the triplets $(\textbf{x}_{ref},\textbf{x}_{pos},\{\textbf{x}_{n_{neg}}\}_{n_{neg}=1}^{N_{neg}})$.
In~\cite{triplet-loss-paper}, the authors proposed the following approach to construct these triplets:
\begin{enumerate}
    \item Choose one reference series $\textbf{r}$
    \item Define $\textbf{x}_{ref}$ as a random subsequence from $\textbf{r}$
    \item Define $\textbf{x}_{pos}$ as another random subsequence from $\textbf{r}$
    \item Define $\{\textbf{x}_{n_{neg}}\}_{n_{neg}=1}^{N_{neg}}$ as being $N_{neg}$ random subsequences from another 
    series $\textbf{r}^{'}\neq\textbf{r}$
\end{enumerate}

\paragraph{Mixup Contrastive Learning (MCL)}

Instead of relying on subsequences, the authors in~\cite{mixing-up-paper}
proposed to use a novel approach to learn a compact representation of time 
series data.
This approach, Mixup Contrastive Learning (MCL), does not rely on the concept 
of negative and positive representations
directly.
Instead, given two input series $\textbf{x}_1$ and $\textbf{x}_2$,
the proposed approach defines a weighted average $\bar{\textbf{x}}$ of these two series as follows:
\begin{equation}\label{equ:mixing-up-augment}
    \bar{\textbf{x}} = \lambda.\textbf{x}_1 + (1-\lambda).\textbf{x}_2
\end{equation}
\noindent where $\lambda$ is a real value between $0$ and $1$, representing
the amount of mixing up from each of the two series.
This parameter follows a beta distribution.

The self-supervised setup in Mixing Up~\cite{mixing-up-paper} tries to predict 
the amount of contribution from each of the two input series.
Assuming an input batch of $N$ series, this batch is then randomly shuffled to two new batches 
$\{\textbf{x}_1^{(1)},\textbf{x}_2^{(1)},\ldots,\textbf{x}_N^{(1)}\}$ and $\{\textbf{x}_1^{(2)},\textbf{x}_2^{(2)},\ldots,\textbf{x}_N^{(2)}\}$.
These two new batches now produces a set of weighted averages:
$\{\bar{\textbf{x}}_1,\bar{\textbf{x}}_2,\ldots,\bar{\textbf{x}}_N\}$.
The contrastive loss is then computed as follows for each sample in the batch:

\begin{equation}\label{equ:mixing-up-loss}
\begin{split}
    l_i &= -\lambda.\log\dfrac{exp(D_c(\textbf{f}(\bar{\textbf{x}}_i),\textbf{f}(\textbf{x}_i^{(1)}))/\tau)}{\sum_{j=1}^N(exp(D_c(\textbf{f}(\bar{\textbf{x}}_i),\textbf{f}(\textbf{x}_j^{(1)}))/\tau) + exp(D_c(\textbf{f}(\bar{\textbf{x}}_i),\textbf{f}(\textbf{x}_j^{(2)}))/\tau))}\\
    &- (1-\lambda).\log\dfrac{exp(D_c(\textbf{f}(\bar{\textbf{x}}_i),\textbf{f}(\textbf{x}_i^{(2)}))/\tau)}{\sum_{j=1}^N(exp(D_c(\textbf{f}(\bar{\textbf{x}}_i),\textbf{f}(\textbf{x}_j^{(1)}))/\tau) + exp(D_c(\textbf{f}(\bar{\textbf{x}}_i),\textbf{f}(\textbf{x}_j^{(2)}))/\tau))}
\end{split}
\end{equation}
\noindent where $\textbf{f}$ is the deep feature extractor, 
$\tau$ is the smoothness temperature parameter and $D_c(.,.)$ refers to the 
cosine similarity function.
The final loss over all the batch of samples is the average loss over each 
sample as follows:
\begin{equation}\label{equ:mixing-up-total-loss}
    \mathcal{L}_{MixingUp} = \dfrac{1}{N}~\sum_{i=1}^{N}~l_i
\end{equation}

The core idea of the above loss proposed in~\cite{mixing-up-paper} is to 
somehow predict the amount of information each series contributed into the
new weighted average series, however in the latent space instead of the original
one.
The backbone architecture used in the Mixing Up model is the FCN~\cite{fcn-resnet-mlp-paper}.

\paragraph{Time Series Self-Supervised
Contrastive Learning framework for Representation (TimeCLR)}

Instead of basing the triplet generation on subsequences or weighted averages,
TimeCLR~\cite{time-clr-paper} defines, prior to training a self-supervised 
model, an AE network to be used for the generation of new samples.
This AE network is trained to approximate the DTW distance between two raw 
series, by the ED between their latent representation extracted by the AE.
The AE takes as input two series, and is trained to reconstruct both of them, such as 
in Eq.~\ref{equ:ae-mse} using the MSE loss.
However, the MSE loss is also computed between the DTW measure between those 
two series and the ED between their latent representation, as follows:
\begin{equation}\label{equ:time-clr-dtw}
    \mathcal{L}_{distance} = (DTW(\textbf{x}_1,\textbf{x}_2) - ED(\textbf{z}_1,\textbf{z}_2))^2
\end{equation}
\noindent where $\textbf{x}_1$ and $\textbf{x}_2$ are the two input series 
with their latent representations (encoder's output of the AE) $\textbf{z}_1$
and $\textbf{z}_2$ respectively.

This AE is then used to perform some augmentation of the input series.
For instance, posterior to training the AE, each input series can now be 
transformed to a new series by simply extracting the latent feature vector from 
the encoder part of the AE, and feeding the decoder a noisy version of this 
vector.
The output of the decoder is now an augmented version of the original input series.
The above pipeline over one input series $\textbf{x}$ is summarized as follows:
\begin{equation}\label{equ:time-clr-augment}
    \hat{\textbf{x}} = D(E(\textbf{x}) + \mathcal{N}(0,1))
\end{equation}
\noindent where $E(.)$ and $D(.)$ are both the encoder and decoder of the pre-trianed 
AE network.

This augmentation method is used to generated two views of each series in 
the dataset, for which these two views are fed to a deep learning model, 
with Inception~\cite{inceptiontime-paper} as a backbone, to be used in a 
contrastive learning setup.
For instance, each series $\textbf{x}_i$ in a batch of $N$ samples goes through the above augmentation method two times
to obtain $\hat{\textbf{x}}_i^{(1)}$ and $\hat{\textbf{x}}_i^{(2)}$.
These two augmented series are fed to the Inception network, with no final task 
layer, and the model's parameters are optimized using the following contrastive loss on each sample in the batch:
\begin{equation}\label{equ:time-clr-contrastive}
    l_i = -\log\dfrac{exp(D_c(\textbf{f}(\hat{\textbf{x}}_i^{(1)}),\textbf{f}(\hat{\textbf{x}}_i^{(2)}))/\tau)}{
        \sum_{j\neq i}(exp(D_c(\textbf{f}(\hat{\textbf{x}}_i^{(1)}),\textbf{f}(\hat{\textbf{x}}_j))/\tau) + exp(D_c(\textbf{f}(\hat{\textbf{x}}_i^{(2)}),\textbf{f}(\hat{\textbf{x}}_j))/\tau))
    }
\end{equation}
\noindent where $D_c(.,.)$ is the cosine similarity, 
$\textbf{f}$ is the deep learning feature extractor with the Inception architecture, and $\tau$ is the smoothness temporature parameter.
The final loss over the whole batch is the average loss over all $N$ samples:
\begin{equation}\label{equ:time-clr-total-loss}
    \mathcal{L}_{TimeCLR} = \dfrac{1}{N}~\sum_{i=1}^{N}~l_i
\end{equation}

The TimeCLR model learns how to represent two series that are an augmentation 
of the same original series, as much as close in the feature space,
hence learning a compact representation space.

\section{Conclusion}

This Chapter has provided an in-depth exploration of the state-of-the-art 
methodologies in time series analysis, a crucial field in data science 
that focuses on extracting meaningful insights from time-dependent data. 
This chapter has navigated through both supervised and unsupervised 
learning techniques, each offering unique advantages and applications.

In the domain of supervised learning, we examined two primary tasks: 
Time Series Classification and Extrinsic Regression. Time Series 
Classification has seen a range of approaches over the years, 
from traditional distance-based methods like k-Nearest Neighbor 
with Dynamic Time Warping (k-NN-DTW) to modern deep learning architectures. 
Distance-based methods, while foundational, have evolved with innovations 
like SoftDTW and ShapeDTW, enhancing their ability to handle temporal 
distortions. Feature-based methods such as Catch22 and TSFresh have 
streamlined the process of extracting significant characteristics from 
time series data, facilitating their use in various classifiers. 
Interval-based methods, dictionary-based methods like BOSS and WEASEL, 
and convolution-based methods such as ROCKET and its variants have 
all contributed to the growing arsenal of tools for Time Series 
Classification, each addressing different aspects of the problem. 
Notably, hybrid models like HIVE-COTE have demonstrated the power 
of combining multiple approaches to achieve superior performance.

Deep learning methods have emerged as a dominant force in Time Series 
Classification, leveraging the parallelization capabilities of GPUs 
and the comprehensive feature extraction capabilities of architectures 
like Convolutional Neural Networks (CNNs), Recurrent Neural Networks 
(RNNs), and Self-Attention. These models have shown remarkable performance 
improvements, driven by their ability to learn complex temporal patterns 
directly from the data.

For unsupervised learning, the chapter covered essential tasks 
such as clustering, prototyping, and SSL. 
Clustering methods group similar time series, aiding in tasks like 
customer segmentation and anomaly detection. Prototyping techniques 
create representative examples of time series, simplifying the analysis 
of large datasets. SSL methods, which leverage 
the data itself to generate supervisory signals, have opened new avenues 
for extracting valuable insights without the need for labeled data.

% The applications of these techniques are vast and varied, spanning 
% fields from finance and healthcare to engineering and beyond. 
% In finance, time series analysis is pivotal for tasks like stock price 
% prediction and fraud detection. In healthcare, it aids in monitoring 
% patient vital signs and diagnosing diseases based on temporal patterns 
% in medical data. The engineering domain benefits from time series analysis 
% in predictive maintenance and quality control processes.

In summary, this chapter has laid a comprehensive foundation for understanding 
the current landscape of time series analysis. This foundation sets the 
stage for the subsequent chapters, which will explore deeper into specific 
methodologies, their applications, and the nuances of implementing these 
advanced techniques in real-world scenarios. In the next chapter, we will 
explore the methods for comparing these models against each other by 
utilizing a wide range of different datasets to benchmark their performance.