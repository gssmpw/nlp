\chapter{Semi-Supervised and Self-Supervised Learning for Time Series Data with a Lack of Labels}\label{chapitre_5}

\section{Introduction}

The challenge of effectively classifying time series data has garnered significant attention within the 
field of machine learning. Previous chapters have discussed traditional approaches, which most of the time
primarily rely on supervised learning techniques demanding the availability of labeled data. However, acquiring sufficient 
labeled time series data is often prohibitively difficult due to the need for expert annotation and the inherent 
complexity of the data itself. Consequently, there is a growing interest in methodologies that can leverage 
limited labeled data while making the most of the abundant unlabeled data available.

This chapter explores semi-supervised and self-supervised learning techniques as promising methods to address 
the issue of time series classification when labeled data is scarce. Semi-supervised learning and 
self-supervised learning methods aim to reduce the dependency on labeled data by utilizing unlabeled 
data to improve model performance. These techniques have shown considerable potential in 
various domains, and are becoming increasingly popular within the time series data mining research community.

In this chapter, we propose a novel self-supervised approach for enhancing TSC.
Our method, 
named TRIplet Loss In TimE (TRILITE), is built upon the concept of triplet loss, a mechanism traditionally used 
in tasks like face recognition to learn effective representations without the need for extensive labeled data
(see Chapter~\ref{chapitre_1} Section~\ref{sec:self-supervised}). 
TRILITE employs a specialized augmentation technique adapted to the characteristics of time series data, allowing 
the model to learn discriminative features from unlabeled data.

We investigate two specific use cases to evaluate the efficacy of TRILITE. The first scenario considers the augmentation 
of a supervised classifier's performance when only a small amount of labeled data is available. Here, TRILITE acts as a 
booster, enhancing the classifier by providing additional, meaningful representations. The second scenario addresses a 
semi-supervised learning context, where the dataset comprises both labeled and unlabeled samples.In this scenario, TRILITE 
is utilized to effectively harness the unlabeled data, resulting in improved overall classification accuracy.

Through extensive experiments conducted on 85 datasets from the UCR archive, we demonstrate the potential of 
TRILITE in both scenarios. The results indicate that our approach not only boosts performance in low-labeled 
data settings but also effectively incorporates unlabeled data to create more robust classifiers. This chapter 
outlines the methodology, experimental setup, and findings, contributing to the broader understanding of 
semi-supervised and self-supervised learning in time series classification.

By addressing the limitations of traditional supervised learning models and harnessing the power of unlabeled data, 
this work paves the way for more efficient and scalable solutions in the analysis and classification of time series data.

\section{TRILITE: TRIplet Loss In TimE}

This section presents the proposed self-supervised learning approach for time series classification, named TRIplet 
Loss In TimE (TRILITE). Our approach leverages triplet loss to learn meaningful representations from time series 
data without requiring extensive labeled data. We describe the architecture of the TRILITE model, the triplet 
loss mechanism, and the specific data augmentation techniques employed to generate effective triplets.
The term \emph{data augmentation} in this chapter does not mean training on more samples, it simply means 
transforming the input series to a new series that is somehow similar to the reference series.

\subsection{Model Construction}

Our TRILITE model features a trio of encoders, all sharing identical weights to ensure consistency. 
This configuration effectively functions as a single encoder processing the generated triplets. We have 
adopted the FCN architecture~\cite{fcn-resnet-mlp-paper}, but modified it by removing the classification layer 
to suit our self-supervised learning framework. Each component of the triplet, the reference $ref$, 
positive $pos$, and negative $neg$ samples, is input into the model, producing their respective latent 
representations ($ref_l$, $pos_l$, and $neg_l$).
In this case, positive and negative samples refer to similar and dissimilar representations of the anchor (reference)
sample.
These representations are streamlined to a fixed size of 128 
dimensions, enabling robust and efficient feature extraction.
The three representations are then used to calculate the triplet loss, defined in the following section,
as presented in Figure~\ref{fig:trilite-model}.

\begin{figure}
    \centering
    \caption{Overview of our TRILITE model with a triplet input example taken form the Beef dataset 
    of the UCR archive~\cite{ucr-archive}.}
    \label{fig:trilite-model}
    \includegraphics[width=0.6\textwidth]{Figures/chapter_5/model.pdf}
\end{figure}

\subsection{Triplet Loss}\label{sec:trilite-loss}

\begin{figure}
    \centering
    \caption{Schema of the relaxed spaced controlled by the
    margin $\alpha$ with an input example taken form the Beef dataset 
    of the UCR archive~\cite{ucr-archive}.}
    \label{fig:trilite-margin}
    \includegraphics[width=0.6\textwidth]{Figures/chapter_5/margin.pdf}
\end{figure}


The core of our approach is the triplet loss~\cite{facenet-paper}, which is designed to create a discriminative latent space 
by minimizing the distance between a reference sample and its positive representation while maximizing the 
distance between the reference and a negative representation. Formally, the triplet loss for a given triplet 
(reference, positive, and negative) is defined as:

\begin{equation}\label{equ:trilite-loss}
    \mathcal{L}_{triplet}(ref_l,pos_l,neg_l) = \max(0, \alpha+d(ref_l,pos_l) - d(ref_l,neg_l))
\end{equation}

where $d(.,.)$ is the Euclidean Distance and $\alpha$ is a margin hyperparameter that controls the separation between 
positive and negative pairs. The loss encourages the model to learn embeddings where similar samples are close 
together, and dissimilar samples are far apart.

The objective of the triplet loss function is to maximize the distance between the reference latent representation 
$ref_l$ and the negative latent representation $neg_l$, while minimizing the distance between the reference and 
positive latent representations $pos_l$. 
% To manage this optimization, the loss~\cite{facenet-paper}
% introduces a hyperparameter $\alpha$, which 
% controls the margin between these distances.
Consequently, we can identify three situations for triplets
as illustrated in Figure~\ref{fig:trilite-margin}:

\begin{itemize}
    \item \textbf{Easy Triplet}: The loss equals $0$ because $d(ref_l,pos_l)+\alpha<d(ref_l,neg_l)$
    \item \textbf{Hard Triplet}: The negative representation is closer to the reference than the positive representation,
    i.e. $d(ref_l,neg_l) < d(ref_l,pos_l)$
    \item \textbf{Semi-Hard Triplet}: The positive representation is closer than the negative, i.e. 
    $d(ref_l,pos_l) < d(ref_l,neg_l)$, but there is still a strictly positive loss.
\end{itemize}

Setting $\alpha$ to $0$ would limit us to identifying only easy and hard triplets. Exclusively using easy triplets 
would likely cause the model to overfit, while relying solely on hard triplets could lead to underfitting. 
Therefore, the introduction of the $\alpha$ hyperparameter is crucial, as it facilitates the creation of semi-hard 
triplets, striking a balance between these extremes. Moreover, the incorporation of the max operation in the 
loss function ensures that the optimization problem remains convex, promoting more effective and stable training.

In the following section, we detail the proposed method of generating these triplets $ref$, $pos$ and $neg$.

\subsection{Triplet Generation}\label{sec:trilite-triplets}

\begin{algorithm}
\caption{Triplet\_Generation}
\label{alg:trilite-triplets}
\begin{algorithmic}[1]
    \REQUIRE~A time series dataset $\mathcal{D}$ of $N$ samples of length $L$ each
    \ENSURE~Three sets of triplets $ref$ $pos$ and $neg$ of same shape as $\mathcal{D}$ each
    \STATE~$shuffle(\mathcal{D})$
    \STATE~$w \gets random(0.6,1)$ 
    \FOR{$i$~: $0 \rightarrow N$}
        \STATE~$ref[i]  \gets data[i]$
        \STATE~$ts_1 \gets random\_sample(data)$
        \STATE~$ts_2 \gets random\_sample(data)$
        \STATE~$pos[i] \gets w.ref[i] + (\frac{1-w}{2}).(ts_1+ts_2)$
        \STATE~$ts_3 \gets random\_sample(data)$
        \STATE~$ts_4 \gets random\_sample(data)$
        \STATE~$ts_5 \gets random\_sample(data)$
        \STATE~$neg[i] = w.ts_3 + (\frac{1-w}{2}).(ts_4+ts_5)$
        \STATE~$pos[i],neg[i] \gets Mask(pos[i],neg[i])$
    \ENDFOR
    \STATE~$pos \gets Znormalize(pos)$
    \STATE~$neg \gets Znormalize(neg)$
    \STATE~\textbf{Return} $ref,pos,neg$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Mask}
\label{alg:trilite-masking}
\begin{algorithmic}[1]
    \REQUIRE~Two input time series $x$ and $y$ of length $L$
    \ENSURE~Masked version of $x$ and $y$
    \State~$l \gets len(x)$
    \State~$start \gets random\_randint(0,L-1)$
    \State~$stop \gets random\_randint(start+\frac{L-1-start}{10},start+\frac{L-1-start}{2.5})$
    \State~$x[0:start] \gets noise$
    \State~$x[stop+1:] \gets noise$
    \State~$y[0:start] \gets noise$
    \State~$y[stop+1:] \gets noise$
    \State~\textbf{Return} $x,y$
\end{algorithmic}
\end{algorithm}

Generating effective triplets is crucial for the success of the TRILITE model. We combine two main strategies for triplet
generation: mixing up and masking.
In~\cite{mixing-up-paper}, a mixing-up strategy is employed, while in~\cite{triplet-loss-paper},
a masking approach is utilized.
The proposed triplet generation setup is detailed in
Algorithms~\ref{alg:trilite-triplets} and~\ref{alg:trilite-masking}.

First, we generate a positive sample $pos$ by computing a weighted sum of three time series, including the reference
$ref$. For the negative sample $neg$, the reference is excluded from the weighted sum. We limit the mixed samples
to three, ensuring each sample contributes significantly. The process can be represented by the following equations:

\begin{equation}\label{equ:trilite-pos-gen}
    pos = w.ref+\dfrac{1-w}{2}.(ts_1+ts_2)
\end{equation}
\begin{equation}\label{equ:trilite-neg-pos}
    neg = w.ts_3+\dfrac{1-w}{ts_4+ts_5}
\end{equation}

\noindent where $ts_1$ and $ts_2$ are randomly selected time series distinct from the reference, and the contribution 
weight $w$ is randomly chosen between $0.6$ and $1.0$. This ensures the positive sample has a greater influence 
from the reference compared to $ts_1$ and $ts_2$.
For the negative sample, three distinct samples, $ts_3$, $ts_4$ and $ts_5$ are randomly chosen from the training dataset 
excluding the $ref$ sample.

Next, we apply a random-length mask to both the positive and negative samples. This masking strategy simplifies 
the training process by allowing the model to focus on learning specific segments of the representations 
rather than the entire sequence.

Finally, the unmasked segments of the time series are replaced with random Gaussian noise, enhancing the 
robustness of the model. Figures~\ref{fig:trilite-pos-gen} and~\ref{fig:trilite-neg-gen} provides 
a visualization of the positive and negative samples generation. Importantly, 
triplet generation occurs online during each training epoch, promoting better generalization of the model.

\begin{figure}
    \centering
    \caption{
        A \protect\mycolorbox{255,165,0,0.6}{mixed up $pos$} is built from
        \protect\mycolorbox{0,90,255,0.6}{three 
        time series including the $ref$}.
        The resulting time series is close to
        the $ref$ except some areas as highlighted in the 
        \protect\mycolorbox{255,30,0,0.6}{red circle}. A \protect\mycolorbox{0,164,0,0.6}{mask}
        is then applied on the
        \protect\mycolorbox{255,165,0,0.6}{mixed up $pos$} 
        to generate the \protect\mycolorbox{0,164,0,0.6}{final
        sample}, where the 
        unmasked parts are replaced by a Gaussian noise.
    }
    \label{fig:trilite-pos-gen}
    \includegraphics[width=\textwidth]{Figures/chapter_5/pos.pdf}
\end{figure}
\begin{figure}
    \centering
    \caption{
        A \protect\mycolorbox{255,165,0,0.6}{mixed up $neg$} is built from
        \protect\mycolorbox{0,90,255,0.6}{three 
        time series excluding the $ref$} used to generate the $pos$ sample.
        The resulting time series is close to
        the $not~ref$ except some areas as highlighted in the 
        \protect\mycolorbox{255,30,0,0.6}{red circle}. A \protect\mycolorbox{0,164,0,0.6}{mask}
        is then applied on the
        \protect\mycolorbox{255,165,0,0.6}{mixed up $neg$} 
        to generate the \protect\mycolorbox{0,164,0,0.6}{final
        sample}, where the 
        unmasked parts are replaced by a Gaussian noise.
    }
    \label{fig:trilite-neg-gen}
    \includegraphics[width=\textwidth]{Figures/chapter_5/neg.pdf}
\end{figure}

\section{Experimental Setup}

For our experiments, we utilized 85 datasets from the UCR
archive~\cite{ucr-archive}~\footnote{More experiments will be done for this technique.}.
All datasets were z-normalized 
prior to training. We employed the Adam optimizer with an initial learning rate of $10^{-3}$. Triplet generation 
occurred online for each epoch to ensure robust generalization. For evaluation on the test set, we used the final 
trained model.
% All experiments were conducted on an NVIDIA GeForce GTX 1080 with 8 GB of memory.
The models were trained for $1000$ epochs with batch size $32$.
% The code for our 
% experiments is publicly available at \url{https://github.com/MSD-IRIMAS/TRILITE}.

To identify the optimal hyperparameter $\alpha$, we conducted a thorough exploration across a range of values:
$\{10^{-6},10^{-5},10^{-4},10^{-3},10^{-2},10^{-1}\}$.
By meticulously visualizing the resulting latent representations, we discerned that modifying $\alpha$
primarily affected the scale of these representations without altering the fundamental classification of 
triplet types (as elaborated in Section~\ref{sec:trilite-loss} of this chapter).
This nuanced understanding led us to conclude that an $\alpha$ value 
of $10^{-2}$ struck the right balance. This value was meticulously fine-tuned using a representative subset 
of the UCR archive, ensuring robustness and generation in our model's performance.

\section{Experimental Results}

\subsection{Comparing To State-Of-The-Art}

\begin{figure}
    \centering
    \caption{Comparing the proposed TRILITE approach to two state-of-the-art SSL models: DCNN~\cite{triplet-loss-paper}
    and MCL~\cite{mixing-up-paper}. We compare the classification of TRILITE latent spaces to MCL's latent space using 
    a 1NN and to DCNN's latent space using both 1NN and a Support Vector Machine classifier~\cite{svm-paper}
    following the original work of both comparates.}
    \label{fig:trilite-vs-sota}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/chapter_5/results/with_sota/TRILITE+1NN-vs-MCL+1NN.pdf}
        \caption{\null}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/chapter_5/results/with_sota/TRILITE+1NN-vs-DCNN+1NN.pdf}
        \caption{\null}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/chapter_5/results/with_sota/TRILITE+SVM-vs-DCNN+SVM.pdf}
        \caption{\null}
    \end{subfigure}
\end{figure}

Comparing our approach to the two state-of-the-art models, Dilated Convolutional Neural Network (DCNN) with its 
version of triplet loss~\cite{triplet-loss-paper} and Mixup Contrastive Learning (MCL)~\cite{mixing-up-paper}, 
is essential as this work draws significant motivation from their methodologies.
However, the goal was not to outperform these models as our objectives differ.
In this study, the aim is to demonstrate how self-supervised models can enhance supervised learning, particularly
in scenarios with limited data and a lack of labeled data.
For each self-supervised 
model, DCNN, MCL, and TRILITE (ours), we applied a classifier to their latent features to evaluate performance,
posterior to training. 
Specifically, we compared TRILITE and MCL using a 1NN classifier with Euclidean distance, consistent with the 
evaluation method used in~\cite{mixing-up-paper}. Additionally, we compared TRILITE and DCNN
with both 1NN and SVM classifiers, as presented in~\cite{triplet-loss-paper}.
The choice of classifier when comparing to each of the two comparators aligns with those used in the original
experiments of DCNN and MCL.
We present in Figure~\ref{fig:trilite-vs-sota} the three 1v1 scatter plots illustrating these comparisons.
Although our model does not consistently surpass the performance of DCNN and MCL across 
all datasets, it does perform comparably on several datasets. This indicates that TRILITE has potential. Now, 
we will present the two cases addressing the challenges of small labeled datasets and the lack of labeled samples.
In scenarios with small labeled datasets, the primary challenge is that models tend to overfit, as they struggle 
to learn patterns that generalize well beyond the limited training data. Such scenario results in poor performance on unseen 
data, limiting the model's utility. Moreover, the lack of labeled samples poses a significant challenge 
in training supervised models, as they rely on labeled data to learn associations between inputs and outputs. 
Without sufficient labeled data, models cannot effectively learn or make accurate predictions, leading to 
unreliable outcomes in practical applications.

\subsection{Use Case 1: Small Annotated Time Series Datasets}

\begin{figure}
    \centering
    \caption{
        Scatter plots comparing the proposed TRILITE model to FCN.
    }
    \label{fig:trilite-vs-fcn-and-concat}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/chapter_5/results/case1/TRILITE+1LP-vs-FCN.pdf}
        \caption{TRILITE+1LP VS FCN}
        \label{fig:trilite-vs-fcn}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/chapter_5/results/case1/concat_TRILITE,FCN_+1LP-vs-FCN.pdf}
        \caption{concat(TRILITE, FCN)+1LP VS FCN}
        \label{fig:trilite-concat-vs-fcn}
    \end{subfigure}
\end{figure}

To address the first case of having a small annotated time series dataset, we compared the TRILITE model, 
followed by a fully connected layer with $softmax$ activation (denoted as TRILITE+1-LP), against the fully supervised 
FCN model.
It is important to note that the TRILITE+1LP approach is a two step training, first the TRILITE is trained on the self-supervised 
task and this is subsequently followed by training a 1LP classifier on the pre-trained TRILITE's latent space.
The 1v1 scatter plots are reported in Figure~\ref{fig:trilite-vs-fcn}.
As expected, the supervised model generally 
outperforms the self-supervised one. However, for certain datasets, the self-supervised features notably improve 
classification accuracy. This observation motivated us to explore the contribution 
of self-supervised features within a supervised learning context.

To do this, we concatenated the latent representations from the self-supervised TRILITE model (each of size $128$) 
with those from the supervised FCN model (also of size $128$) for both the training and test sets. The concatenated 
features were then used to train a classifier comprising a single fully connected layer with softmax activation (1LP).
Subsequently, this pipeline is evaluated on the concatenated features of the test set. 
We compared this approach, denoted as concat(TRILITE,FCN)+1LP, against the fully supervised FCN in
Figure~\ref{fig:trilite-concat-vs-fcn}.
This Win-Tie-Loss comparison highlights that the concatenation method is never significantly worse than the single FCN, 
in terms of magnitude of accuracy difference. 
This can be attributed to the fact that supervised features are not negatively impacted by the SSL features; in the worst 
case, the linear classifier can simply ignore the SSL features if they do not aid classification.
% This is because the supervised features remain unaffected by the self-supervised features. 
% In the worst-case scenario, the linear classifier can disregard the self-supervised features 
% if they interfere with the classification task.

\begin{figure}
    \centering
    \caption{The MCM (Chapter~\ref{chapitre_2}) comparing concat(TRILITE,FCN)+1LP to FCN and TRILITE+1LP
    over the 85 datasets of the UCR archive.}
    \label{fig:trilite-mcm}
    \includegraphics[width=\textwidth]{Figures/chapter_5/results/case1/mcm_concat.pdf}
\end{figure}

Furthermore, we present in Figure~\ref{fig:trilite-mcm} the MCM comparing the three models: TRILITE+1LP, FCN, 
and the concatenation method concat(TRILITE, FCN)+1LP. The MCM highlights that the concatenation method 
is nearly $2\%$ better in terms of average accuracy, demonstrating the boosting effect.
This indicates that SSL generates features distinct from those produced by supervised learning. Consequently, 
the combination of both sets of features enhances classification performance. Notably, the peaks in performance 
improvement are observed in datasets with a small number of samples, such as DiatomSizeReduction, which has only 
16 samples in the training set.

\subsection{Use Case 2: Partially Annotated Time Series Datasets}

\begin{figure}
    \centering
    \caption{
        Comparison of experiment 1 and experiment
2. In experiment 1, the TRILITE model is trained only
on the labeled subset ($30\%$ of the data). On the contrary, in
experiment 2, the TRILITE model is trained on the whole
train set. The evaluation is done on the whole test set.
    }
    \label{fig:trilite-semi-supervised}
    \includegraphics[width=0.45\textwidth]{Figures/chapter_5/results/case2/Experiment_2-vs-Experiment_1.pdf}
\end{figure}

In this second case, we explore a semi-supervised scenario where only a portion of the data is labeled. 
Our objective is to assess how self-supervised learning can address the challenge of limited labels. Assuming that only $30\%$ of 
the training set is labeled, we proceed with the following steps:

\begin{enumerate}
    \item \textbf{Self-supervised training}: We generate self-supervised latent representations by training our TRILITE model:
    \begin{itemize}
        \item \textbf{Experiment 1}: Training is conducted solely on the labeled subset.
        \item \textbf{Experiment 2}: Training is conducted on the entire training set,
        including both labeled and unlabeled data.
    \end{itemize}
    \item \textbf{Supervised learning}: The latent representations derived from the labeled set 
    (from either Experiment 1 or Experiment 2) are fed into a Ridge classifier~\cite{hoerl1970ridge}.
    \item \textbf{Evaluation}: The performance of the trained classifier is then evaluated on the test set.
\end{enumerate}

To ensure the reliability and robustness of our results, these steps are repeated across 25 runs, 
with the average accuracy calculated for each run.
The high number of experiments motivated the usage of Ridge classifier instead of SVM 
as proposed by~\cite{triplet-loss-paper} given its fast training time.
The same labeled subset is utilized for both experiments 
within each run. The 1v1 scatter plot comparison between \textbf{Experiment 1} and \textbf{Experiment 2}
is illustrated in Figure~\ref{fig:trilite-semi-supervised}.

The comparison reveals that Experiment 2 outperforms \textbf{Experiment 1} more frequently.
On average, when \textbf{Experiment 2} 
prevails, the accuracy difference is 2.12 ± 2.13. Conversely, when Experiment 1 has better performance, 
the accuracy difference averages 1.17 ± 1.21. 
This disparity underscores the effectiveness of self-supervised learning in cultivating more nuanced 
and comprehensive latent representations. By integrating both labeled and unlabeled data, 
\textbf{Experiment 2} is able to capture a wider spectrum of underlying patterns and structures within the data. 
This setup not only enhances the overall performance of the model but also demonstrates that self-supervised methods
contribute to greater stability and adaptability across diverse datasets. These findings emphasize the potential 
of self-supervised learning to bridge gaps in data quality and quantity, leading to models that are better 
equipped to generalize across different tasks and challenges.
\section{Conclusion}

In this chapter, we have explored innovative approaches in semi-supervised and self-supervised learning for 
TSC. The primary 
goal of our research was to utilize self-supervised models to enhance supervised models in two specific cases: 
first, when there is a lack of data but all available data is labeled, and second, when there is a scarcity of 
labeled data in a semi-supervised learning context.

Our TRILITE model, based on triplet loss, was developed to generate meaningful latent representations from 
time series data. We conducted extensive experiments comparing TRILITE with state-of-the-art models such as 
DCNN~\cite{triplet-loss-paper} and MCL~\cite{mixing-up-paper}.
While TRILITE did not consistently outperform these models, it demonstrated competitive 
performance on several datasets. This indicates the potential of self-supervised learning in improving 
TSC, particularly in challenging scenarios with limited labeled data.

In the first use case, where only a small annotated time series dataset is available, we showed that 
incorporating self-supervised features with supervised learning models can enhance classification 
accuracy. Specifically, by concatenating the latent representations from the TRILITE model with those 
from a supervised FCN model, we observed significant improvements in performance. This approach, 
termed concat(TRILITE, FCN), consistently achieved comparable results to the single FCN model and better performance 
on small datasets, showcasing 
the complementary nature of self-supervised and supervised features.

In the second use case, involving a semi-supervised scenario with only a portion of the data labeled, 
we demonstrated how self-supervised learning can effectively address the lack of labels. By training 
the TRILITE model on both the labeled subset and the entire training set, we obtained more robust latent 
representations that improved the performance of the downstream classifier. The results from repeated 
experiments confirmed that leveraging unlabeled data in self-supervised training leads to more meaningful 
feature extraction, which in turn enhances classification accuracy.

The findings in this chapter underscore the importance of integrating self-supervised learning techniques 
to enhance supervised learning models, especially in the context of time series data with limited labeled 
samples. The potential of TRILITE to generate useful features from both labeled and unlabeled data opens 
new avenues for future research. Moving forward, further refinement of these techniques could lead to more 
robust and efficient models, advancing the field of TSC, and potentially other tasks, addressing the persistent 
challenges of data scarcity and annotation costs.