%----------------------------------------------------------------------------------------
%
% Détection de mutations génétiques dans le cancer du côlon
%
%----------------------------------------------------------------------------------------
\chapter{Towards Finding Foundation Models for Time Series Classification} 
\label{chapitre_3}


\section{Introduction}

In the dynamic field of TSC, the quest for developing models that are robust and adaptable across
diverse datasets remains a significant challenge.
Foundation models, which are large pre-trained models capable of generalizing across various tasks, 
offer a promising solution to this problem.
The necessity of foundation models arises from their ability to simplify and expedite the fine-tuning process. 
In many real-world applications, such as predicting heart conditions from ECG signals or forecasting traffic patterns, 
starting model training from scratch is time-consuming and computationally expensive. Foundation models mitigate these 
challenges by offering a pre-trained base that already understands the fundamental patterns within a domain. Consequently, 
fine-tuning becomes a matter of adapting this base model to the nuances of a specific dataset, leading to faster and 
more efficient training with improved performance.
For instance, in the medical field, a model pre-trained on various ECG datasets can be fine-tuned to detect 
arrhythmias with greater accuracy and speed. Similarly, in traffic management, a model pre-trained on traffic 
data from multiple cities can be fine-tuned to predict congestion patterns in a specific city. This allows 
for more accurate and efficient traffic control solutions tailored to local conditions.

This chapter introduces two key contributions aimed at advancing towards
deep foundation models: the creation of hand-crafted convolution filters
% ~\cite{hand-crafted-filters-paper}
to enhance model generalization, and the utilization of a pre-training
methodology
% ~\cite{pretext-task-paper}
to fine-tune these models for specific classification tasks.
% Our first contribution involves designing hand-crafted convolution filters that enhance the model's generalization capabilities. 
These filters are designed to shift the model's focus from specific features to more fundamental, general 
characteristics that are independent of any particular domain. This ensures that the model can identify 
and leverage intrinsic patterns within 
the data, making it more adaptable across various tasks.
Building on this, our second contribution incorporates a model that utilizes our hand-crafted filters
% proposed in~\cite{hand-crafted-filters-paper}, which
% utilizes these hand-crafted filters
to develop a preliminary foundation model. This model undergoes extensive pre-training on multiple datasets within the
same domain, such as ECG or traffic data, aiming to predict the original dataset of each series. This pre-training equips
the model with a broad understanding of domain-specific patterns, providing a strong foundation for subsequent fine-tuning.
Once pre-trained, the model is fine-tuned on individual datasets to address specific classification tasks. 

By combining the strengths of hand-crafted filters with a robust pre-training and fine-tuning methodology, we aim to 
construct a foundation model that is both versatile and powerful. These contributions not only enhance the model's 
performance across varied TSC tasks but also represent a significant step towards the development 
of deep foundation models in this domain.
% This approach not only improves the efficiency of developing specialized 
% models but also ensures that they deliver better and faster results in real-world applications.

\section{Hand-Crafted Convolution Filters}~\label{sec:hcf}

The development of deep learning models for time series classification often encounters significant challenges, 
such as overfitting, computational complexity, and redundancy in learned filters. Traditional CNNs~\cite{inceptiontime-paper}
typically learn filters through back-propagation, where filters are initialized randomly and refined 
during training. While effective, this process can lead to several issues:

\begin{itemize}
    \item \textbf{Overfitting}: Learned filters may become overly specialized to the training data, reducing the model's
    ability to generalize to new, unseen data
    \item \textbf{Time Spent on Learning ``Easy'' Features}: During training, models may spend significant time learning simple,
    generic features that could have been predefined, rather than focusing on complex features that are more difficult
    to construct by hand.
    \item \textbf{Difficulty in Finding Generic Filters}: While learning simple filters from scratch is feasible, 
    the process becomes significantly more challenging for complex filters due to error propagation, making it 
    difficult to generalize filters that work effectively across various datasets.
\end{itemize}

\subsection{Are There Any Common Learned Convolution Filters Between Datasets?}\label{sec:common-filters}

One approach to address issues like overfitting, excessive focus on ``easy'' features, and the difficulty of finding
generic filters is to construct hand-crafted convolution filters that detect generic patterns in time series data. 
Before constructing these filters, we must assume that if such generic convolution filters can be manually created, 
deep CNN models should be able to learn them across different datasets.

To test this assumption, we analyzed the  t-distributed Stochastic Neighbor Embedding (t-SNE)~\cite{t-sne-paper}
two-dimensional projection of the filter space learned by CNN models, 
as shown in Figure~\ref{fig:common-filters}. We focused on the filters from the first layer since it is more practical 
to identify generic patterns in raw data than in the deeper feature spaces. The t-SNE space was generated using 
the DTW similarly measure to ensure shift independence between the normalized filters.

Figure~\ref{fig:common-filters} shows that a significant number of convolution filters coincide in the t-SNE space across 
four different ECG-based datasets with varying characteristics (training size, time series length). This suggests that certain 
filters may be shared or common across different datasets.
Furthermore, we can consider the model's ability to find optimal solutions as having a specific amount of energy,
$\mathcal{E}_{\text{total}}$. This energy is split into two parts:

\begin{equation}\label{equ:energy-analogy}
    \mathcal{E}_{\text{total}} = \mathcal{E}_1 + \mathcal{E}_2
\end{equation}
\noindent where $\mathcal{E}_1$ is the energy used to find a set of simple filters, and $\mathcal{E}_2$ is the energy used to 
find the remaining filters. By providing the model with some untrained hand-crafted convolution filters that it would 
have found on its own, we effectively set $\mathcal{E}_1$ to 0. This allows the model to focus all its energy
$\mathcal{E}_2$ on finding other filters, reducing overfitting and improving performance.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/chapter_3/hcf/common-filters.pdf}
    \caption{t-SNE two-dimensional projection of the first-layer 
    convolution filters learned by CNN models on
    four ECG datasets (
        \protect\mycolorbox{98,120,166,0.5}{ECG200}, 
        \protect\mycolorbox{98,100,255,0.5}{ECG5000}, 
        \protect\mycolorbox{115,77,16,0.5}{ECGFiveDays}, 
        \protect\mycolorbox{153,50,42,0.5}{TwoLeadECG}). 
    The \protect\mycolorbox{255,0,255,0.3}{clustering of filters} in the t-SNE space
    suggests that deep CNN models can identify common, 
    generic filters across different datasets.}
    \label{fig:common-filters}
\end{figure}

This idea aligns with our approach of introducing hand-crafted convolution filters specifically designed to detect 
fundamental patterns in time series data. These filters are fixed and not adjusted during training, allowing the model 
to focus on learning more complex and nuanced patterns. This approach has already been addressed in the computer vision 
community with the construction of untrained Sobel convolution filters~\cite{sobel-paper1,sobel-paper2}.
In the following section, we detail the construction of the proposed hand-crafted convolution filters.

\subsection{Construction of Hand-Crafted Filters}~\label{sec:construction-hcf}

The hand-crafted filters are designed to capture specific types of patterns that are common and crucial in
time series data. We propose three types of filters: (1) increasing trend detection, (2) decreasing trend detection 
and (3) peak detection, for which we define as such:

\mydefinition{Increasing Trend Detection Convolution Filter:}

An increasing trend is a sub-sequence of a time series $\textbf{x}$ where the values are strictly increasing in time.
The filter detecting this trend is designed to detect subsequences where values are strictly increasing over time.
An increasing trend detection filter of length $K$ is defined as:

\begin{equation}\label{equ:increasing-trend-filter}
    \textbf{w}_{I_K} = \{(-1)^{k}\}_{k=1}^{K}
\end{equation}

\begin{theorem}[Increasing Trend Dection Convolution Filter]\label{the:increasing-trend-filters}
    Let $K$ be an even positive integer, a convolutional filter 
    $\textbf{w}_{I_K}=[(-1)^{k}$ for $k \in \{1,...,K\}]$ is an 
    increasing trend detection filter of time series, i.e. it only activates
    (produces positive values) on increasing trend segments.
\end{theorem}

\textit{proof}:
Given a time series $\textbf{x}$ of length $L$ and univariate for simplicity,
that contains increasing, decreasing and stationary trends, we will prove in what follows
that the increasing trend detection filter only activates on increasing trends.

For stationary trends: Suppose $\exists (t_0,t_1), \epsilon$ 
where $t_1 > t_0$ and $\forall t \in [t_0,t_1]$ we have 
$|x_{t+1} - x_t| < \epsilon$.
By convolving this segment of the time series $\textbf{x}$ with the filter $w_{I_K}$ we get:
\begin{equation}
    \begin{split}
        \forall t \in [t_0,t_1], s[t] &= \sum_{k=1}^{K} x_{t+k-1}.w_k\\
        &= \underbrace{-x_t + x_{t+1}}_{\textstyle < \epsilon} \underbrace{- x_{t+2} + x_{t+3}}_{\textstyle < \epsilon}- \dots \underbrace{-x_{t+k-1} +x_{t+k}}_{\textstyle < \epsilon}\\
        &\approx 0~\text{ not activated}
    \end{split}
\end{equation}

\textit{continued proof}:
For decreasing trends: Suppose $\exists (t_0,t_1), \epsilon$ 
where $t_1 > t_0$ and $\forall t \in [t_0,t_1]$ we have 
$x_{t+1} < x_t $.
By convolving this segment of the time series $\textbf{x}$ with the filter $w_{I_K}$ we get:
\begin{equation}
    \begin{split}
        \forall t \in [t_0,t_1], s[t] &= \sum_{k=1}^{K} x_{t+k-1}.w_k\\
        &= \underbrace{-x_t + x_{t+1}}_{\textstyle < 0} \underbrace{- x_{t+2} + x_{t+3}}_{\textstyle < 0}- \dots \underbrace{-x_{t+k-1} +x_{t+k}}_{\textstyle < 0}\\
        &< 0~\text{ not activated}
    \end{split}
\end{equation}

\textit{continued proof}:
For increasing trends: Suppose $\exists (t_0,t_1), \epsilon$ 
where $t_1 > t_0$ and $\forall t \in [t_0,t_1]$ we have 
$x_{t+1} > x_t $.
By convolving this segment of the time series $\textbf{x}$ with the filter $w_{I_K}$ we get:
\begin{equation}
    \begin{split}
        \forall t \in [t_0,t_1], s[t] &= \sum_{k=1}^{K} x_{t+k-1}.w_k\\
        &= \underbrace{-x_t + x_{t+1}}_{\textstyle > 0} \underbrace{- x_{t+2} + x_{t+3}}_{\textstyle > 0}- \dots \underbrace{-x_{t+k-1} +x_{t+k}}_{\textstyle > 0}\\
        &> 0~\text{ activated}
    \end{split}
\end{equation}


\mydefinition{Decreasing Trend Detection Convolution Filter:}

A decreasing trend is a sub-sequence of a time series $\textbf{x}$ where the values are strictly decreasing in time.
The filter detecting this trend is designed to detect subsequences where values are strictly decreasing over time.
A decreasing trend detection filter of length $K$ is defined as:

\begin{equation}~\label{equ:decreasing-trend-filter}
    \textbf{w}_{D_K} = \{(-1)^{k+1}\}_{k=1}^{K}
\end{equation}

\begin{theorem}[Decreasing Trend Dection Convolution Filter]\label{the:decreasing-trend-filters}
    Let $K$ be an even positive integer, a convolutional filter 
    $\textbf{w}_{D_K}=[(-1)^{k+1}$ for $k \in \{1,...,K\}]$ is a 
    decreasing trend detection filter of time series, i.e. it only activates
    (produces positive values) on decreasing trend segments.
\end{theorem}

\textit{proof}: The proof of Theorem~\ref{the:decreasing-trend-filters}
follows the same methodology of the proof of Theorem~\ref{the:increasing-trend-filters}.

\mydefinition{Peak Detection Convolution Filter:}

A peak is a sub-sequence of a time series $\textbf{x}$ where the values changed with a large variation increasingly and then
decreasingly.
To detect peaks, we use a filter inspired by the shape of the negative second derivative of a Gaussian function.
The filter mimics this shape using a squared parabolic function divided into three parts: a negative parabolic segment,
a positive parabolic segment, and another negative parabolic segment.
For example, a peak detection filter of length 12 is:
\begin{equation}\label{equ:peak-filter-example}
    \textbf{w}_{P_{12}} = \{-0.25,-1,-1,-0.25,0.5,2,2,0.5,-0.25,-1,-1,-0.25\}
\end{equation}

These hand-crafted filters are generic and applicable across various datasets without modification,
making them robust tools for initial feature extraction in time series classification tasks.
An example of these three filters can be seen in Figure~\ref{fig:hcf-summary} on the Meat dataset of
the UCR archive~\cite{ucr-archive}.
The output convolution between the input series and each of the hand-crafted filters go through a ReLU activation,
this operation will filter out the negative outcomes, keeping the parts where the filters are activated (the target patterns).

\begin{figure}
    \centering
    \caption{Three \protect\mycolorbox{0,128,0,0.5}{hand-crafted filters} detecting: 
    (1) increasing trends, (2) decreasing
    trends and (3) peaks in a time series. The 
    \protect\mycolorbox{255,165,0,0.5}{orange points} indicates on which
    time stamps the filters are activated after being convolved 
    with an \protect\mycolorbox{0,0,255,0.5}{input time
    series} from the Meat dataset of the UCR Archive.}
    \label{fig:hcf-summary}
    \includegraphics[width=\textwidth]{Figures/chapter_3/hcf/summary.pdf}
\end{figure}


\subsection{Integration Into Deep Learning Architectures}\label{sec:hcf-archis}

To evaluate the impact of these hand-crafted filters, we integrate them into existing deep learning architectures,
mainly FCN~\cite{fcn-resnet-mlp-paper} and Inception~\cite{inceptiontime-paper}.
We propose two adapted versions for FCN and one for Inception:

\subsubsection{Custom Only-Fully Convolutional Network (CO-FCN)}\label{sec:co-fcn}

In this architecture, the first convolution layer of the standard FCN is replaced entirely by the hand-crafted filters.
This adaptation ensures that the initial feature extraction is driven by these fixed filters, allowing the subsequent
layers to focus on learning more complex patterns.
We refer to this architecture as Custom Only-Fully Convolutional Network (CO-FCN), which evaluates the usage of the 
proposed hand-crafted filters alone with no learnable layers.
The details of this architecture are presented in Figure~\ref{fig:co-fcn}.
To avoid choosing a specific length for hand-crafted filters, we apply a set of different lengths for each of the three 
proposed filters.
We retained the parameters setup used in the last two layers of FCN as detailed in Figure~\ref{fig:fcn}
of Chapter~\ref{chapitre_1}.

\begin{figure}
    \centering
    \caption{The CO-FCN architecture, applied non trainable hand-crafted convolution filters on the input data,
    followed by the rest of the FCN architecture~\cite{fcn-resnet-mlp-paper}.}
    \label{fig:co-fcn}
    \includegraphics[width=\textwidth]{Figures/chapter_3/hcf/archis/co-fcn.pdf}
\end{figure}

\subsubsection{Hybrid-Fully Convolutional Network (H-FCN)}\label{sec:h-fcn}

The H-FCN architecture enhances the standard FCN by incorporating both hand-crafted and trainable filters in the 
first convolution layer. Features extracted by the hand-crafted filters are concatenated with those from the trainable 
filters, enabling the model to leverage the strengths of both approaches.
The details of this architecture are presented in Figure~\ref{fig:h-fcn}, and such as CO-FCN, we utilize a set of different 
lengths for the hand-crafted filters.
The rest of the FCN architecture is used, however,
unlike the original FCN, given we incorporate hand-crafted filters, we reduce the number of filters throughout all the 
rest of the network by half.
This is due to the fact that the model does not need many filters now given the presence of the hand-crafted filters,
increasing the model's efficiency.

\begin{figure}
    \centering
    \caption{The H-FCN architecture using non trainable hand-crafted filters in parallel to trainable 
    convolution filters.}
    \label{fig:h-fcn}
    \includegraphics[width=\textwidth]{Figures/chapter_3/hcf/archis/h-fcn.pdf}
\end{figure}

\subsubsection{Hybrid-Inception (H-Inception)}\label{sec:hinceptiontime}

The Hybrid-Inception (H-Inception) architecture integrates hand-crafted filters into the Inception model~\cite{inceptiontime-paper}
(see Figure~\ref{fig:inception} of Chapter~\ref{chapitre_1} for the network details), which is known for 
its superior performance in TSC. Similar to the H-FCN, the H-Inception network combines 
the features captured by the hand-crafted filters with those extracted by the first Inception block. This concatenation 
occurs before the data is processed by the remaining layers of the Inception network. Additionally, we construct H-InceptionTime 
which leverages an ensemble of five H-Inception models (similar to InceptionTime) to further enhance its performance.
The integration of hand-crafted filters in this complex architecture allows it to maintain its depth and 
capacity to capture diverse patterns, while the ensemble approach ensures robustness and improved accuracy 
across various datasets.
Figure~\ref{fig:h-inception} presents the detailed architecture of H-Inception.
The rest of the Inception architecture and parameters are retained the same.
The reason to why in H-Inception we do not reduce the number of filters to learn, is that the number of filters per 
convolution is small, and much smaller than the one set in FCN (32 < 128).

\begin{figure}
    \centering
    \caption{The H-Inception architecture using non trainable hand-crafted filters in parallel to trainable 
    convolution filters of Inception~\cite{inceptiontime-paper}.}
    \label{fig:h-inception}
    \includegraphics[width=\textwidth]{Figures/chapter_3/hcf/archis/H-Inception.pdf}
\end{figure}

In the following section, we present a detailed evaluation of these three proposed architectures on 128 TSC datasets 
of the UCR archive~\cite{ucr-archive}, proving that the hand-crafted filters help the models to generalize, paving
the way to constructing a foundation generic model for TSC tasks.

\subsection{Experimental Setup}

The evaluation of the proposed architectures was conducted using the UCR Archive, which includes 128 labeled univariate 
time series datasets. Each time series in these datasets undergoes z-normalization to achieve a zero-mean and
unit-standard-deviation.
The performance of each model was measured by comparing their accuracy on these datasets. The models were trained 
using the Adam optimizer with a learning rate decay, and the best-performing model based on training loss was selected 
for evaluation on the test set. To ensure robustness, the training process was repeated five times with different initialization, 
and the results were averaged.
% All experiments were conducted on a NVIDIA GeForce GTX 1080 with 8GB of memory. 
% The code is available here: \url{https://github.com/MSD-IRIMAS/CF-4-TSC}.

We compared the performance of our adapted architectures with hand-crafted filters to the original models. 
For each pair of models, we compared the accuracy on each dataset and computed the number of wins, ties, and losses. 
These comparative results are presented using Win/Tie/Loss one-vs-one plots, showing the Win/Tie/Loss count between 
two different classifiers on the 128 datasets of the UCR Archive. Each point in the plots represents a single dataset from the UCR Archive. 
The axes display the accuracy of each classifier between $0$ and $1$. Additionally, to assess the significance of the comparisons, 
the Wilcoxon Signed Rank Test~\cite{wilcoxon-paper} was performed for each pair of classifiers. 
The resulting statistical measure, the p-value, is shown in the legend of each plot. 
We set the $\alpha$ threshold for the p-values to $0.05$, as done in the literature~\cite{dl4tsc}.

\subsection{Experimental Results}

In this section we compared the proposed architectures to their original network, as well as to other deep learning models 
for TSC such as ResNet (see Figure~\ref{fig:resnet}).
We follow this comparison by looking into the standing of our proposed models to non-deep learning state-of-the-art methods.
Finally, we present a detailed analysis into the changes the FCN variants go through when using the hand-crafted filters.

\subsubsection{Comparing To Original Networks}

\begin{figure}
    \centering
    \caption{Results over 128 datasets of the UCR archive~\cite{ucr-archive} presented in a 1v1 scatter plot format
    between FCN, InceptionTime (IT) and their variants CO-FCN, H-FCN and H-InceptionTime (H-IT) using the
    hand-crafted convolution filters.}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/chapter_3/hcf/results/cofcn-vs-fcn.pdf}
        \caption{\null}
        \label{fig:cofcn-vs-fcn}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/chapter_3/hcf/results/hfcn-vs-fcn.pdf}
        \caption{\null}
        \label{fig:hfcn-vs-fcn}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/chapter_3/hcf/results/hit-vs-it.pdf}
        \caption{\null}
        \label{fig:hit-vs-it}
    \end{subfigure}
\end{figure}

The CO-FCN model, which replaces the first convolutional layer of the FCN with hand-crafted filters, demonstrated 
improved performance over the original FCN in most cases. The Win/Tie/Loss analysis, presented in Figure~\ref{fig:cofcn-vs-fcn}, showed that CO-FCN outperformed 
the original FCN on a majority of datasets, indicating that hand-crafted filters can effectively replace learned filters 
in the initial layer of the network.
The p-value between the performances of CO-FCN and FCN is $0.0168<0.05$ indicating that on the 128 datasets of the UCR archive,
CO-FCN outperforms FCN with a, statistically, significant difference of performance.
However, there were instances where the original FCN performed better, suggesting that
a hybrid approach could be beneficial, highlighting again the need to quantify the win/loss margin between two models.

The H-FCN model, which combines hand-crafted and learnable filters, showed significant improvements over the original FCN 
model. The Win/Tie/Loss analysis, presented in Figure~\ref{fig:hfcn-vs-fcn}, highlighted that H-FCN achieved better
accuracy in a substantial number 
of datasets compared to these models.
The performance difference between FCN and H-FCN is statistically significant (p-value$<0.05$), with FCN having a very
small winning margin in terms of accuracy values. 
This demonstrates that by adding just a few hand-crafted filters, the performance of FCN can be significantly enhanced, 
while also being more efficient, as H-FCN ($77~440$ parameters) reduces the number of parameters by 
almost three times compared to FCN ($264~704$ parameters).

The H-InceptionTime model, variant of the InceptionTime model with hand-crafted filters, 
demonstrated significant performance improvements. As seen in Figure~\ref{fig:hit-vs-it}, H-InceptionTime surpassed
the InceptionTime ensemble in terms of accuracy,
with a statistical significance in terms of difference of performance as the p-value is less than the threshold. 
This makes H-InceptionTime the newest state-of-the-art deep learning model for TSC.

\subsubsection{Comparing To Other Networks}\label{sec:hfc-between-models-compare}

\begin{figure}
    \centering
    \caption{Critical Difference Diagram~\cite{cdd-benavoli-paper} presenting a comparison between three proposed 
    networks and three state-of-the-art deep learning models for TSC, aggregated over the 128 datasets of the UCR archive.}
    \label{fig:hcf-cdd-deep}
    \includegraphics[width=\textwidth]{Figures/chapter_3/hcf/results/cd-diagram.pdf}
\end{figure}

After demonstrating that hand-crafted filters can enhance a network's performance on the UCR archive, as shown in the previous section, 
we now present a cross-comparison between models. 
For instance, Figure~\ref{fig:hcf-cdd-deep} illustrates the CD diagram from~\cite{cdd-benavoli-paper}, implemented 
in\footnote{https://github.com/hfawaz/cd-diagram}. 
This CD diagram provides a multi-classifier comparison between the three proposed networks and three state-of-the-art baseline networks: FCN, ResNet, and InceptionTime.

The CD diagram highlights that H-FCN significantly outperforms ResNet,
while ResNet significantly outperforms FCN, 
demonstrating the substantial impact of the hand-crafted filters. 
Furthermore, the diagram shows that, in terms of average rank on the accuracy metric, H-InceptionTime is the top performer. 
However, the diagram also indicates a non-significant difference in performance between H-InceptionTime and InceptionTime, 
which contradicts the 1v1 scatter plot in Figure~\ref{fig:hit-vs-it}, highlighting the issues associated with Holm correction 
discussed in Chapter~\ref{chapitre_2}, Section~\ref{sec:holm-manipulation}. 
A similar argument applies to the comparison between CO-FCN and FCN. This issue can also lead to misleading conclusions about 
the non-significance in difference of performance between CO-FCN and ResNet,whether it exists due to Holm correction.

\begin{figure}
    \centering
    \caption{Multi-Comparison Matrix (Chapter~\ref{chapitre_2}) presenting a comparison between three proposed 
    networks and three state-of-the-art deep learning models for TSC, aggregated over the 128 datasets of the UCR archive.}
    \label{fig:hcf-mcm-deep}
    \includegraphics[width=\textwidth]{Figures/chapter_3/hcf/results/mcm_row.pdf}
\end{figure}

For these reasons, in this section and the rest of this thesis, we utilize the MCM (Chapter~\ref{chapitre_2}) for both 1v1 and multiple
model comparisons, as shown in Figure~\ref{fig:hcf-mcm-deep}.
In this MCM, it is clear that CO-FCN significantly outperforms FCN, with no significant difference compared to ResNet.
The MCM also shows that H-FCN outperforms ResNet in terms of average accuracy.
These findings are crucial, as ResNet is a complex model with nearly $500,000$ parameters to train, 
while CO-FCN and H-FCN have $122,496$ and $77,440$ parameters, respectively.
This questions the direct relationship between the number of parameters and performance, indicating that adding 
hand-crafted filters can significantly improve a ``poorly performing'' model like FCN, enabling it to surpass a state-of-the-art model like ResNet.

\subsubsection{Comparison With Non-Deep Models}\label{sec:hcf-results-non-deep}

This section demonstrates how hand-crafted filters boost InceptionTime to achieve better average accuracy compared to ROCKET and 
approach the performance of MultiROCKET. 
The MCM in Figure~\ref{fig:hcf-mcm-non-deep} highlights that the p-value between H-InceptionTime and MultiROCKET 
is closer to the threshold than the p-value between InceptionTime and MultiROCKET.
We conclude that this performance boost is attributed to the hand-crafted convolution filters.

\begin{figure}
    \centering
    \caption{Multi-Comparison Matrix (Chapter~\ref{chapitre_2}) presenting a comparison between H-InceptionTime, InceptionTime,  
    and two state-of-the-art non-deep learning models for TSC, ROCKET and MultiROCKET, aggregated over the 128
    datasets of the UCR archive.}
    \label{fig:hcf-mcm-non-deep}
    \includegraphics[width=\textwidth]{Figures/chapter_3/hcf/results/mcm_row_non-deep.pdf}
\end{figure}


\subsection{Analysis}

To verify the hypothesis that the original models could identify shapes similar to our proposed filters, we first analyzed if 
the original models were able to find similar patterns. Additionally, we needed to verify that models using 
the hand-crafted filters did not redundantly learn the same filters, as this would be ineffective. Finally, 
we aimed to understand why the hand-crafted filters enhance model performance, specifically whether they help 
the model generalize better. By examining these aspects, we sought to confirm the value of hand-crafted filters 
in capturing critical patterns and improving the model's robustness across different datasets.

\subsubsection{Learned vs Hand-Crafted Convolution Filters}\label{sec:hcf-learned-vs-hcf}

To evaluate the effectiveness of our hand-crafted filters, we compared them to the learned filters in the original models. 
Our primary goal was to determine whether the original models were learning filters similar to our hand-crafted ones. 
In this experiment, we analyzed the 128 filters learned by the first layer of the original FCN model on the CinCECGTorso 
dataset. For each hand-crafted filter, we identified the closest learned filter by computing the DTW distance for each pair, 
after Z-normalizing the learned filters.

For example, the hand-crafted increasing filter of size $K=8$ and its closest learned filter on the CinCECGTorso 
dataset are shown in Figure~\ref{fig:hcf-fixed_vs_learned}. The original FCN model learned a similar filter for detecting 
increasing trends in the time series. The learned filter is a weighted version of our hand-crafted filter. 
Additionally, the first part of the learned filter resembles the peak detection filter, while the rest matches the 
increasing trend detection. This suggests that the FCN model learned to construct filters capturing multiple patterns 
simultaneously, aligning with our approach in the H-FCN model that combines hand-crafted and learned filters.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/chapter_3/hcf/analysis/learned_vs_fixed.pdf}
    \caption{\protect\mycolorbox{0,82,255,0.7}{Hand-crafted increasing trend detection filter} 
    of size $K=8$ and its \protect\mycolorbox{255,169,0,0.7}{closest learned filter} on 
    the CinCECGTorso dataset. The learned filter is from the first layer of the original FCN.}
    \label{fig:hcf-fixed_vs_learned}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/chapter_3/hcf/analysis/TSNE_filters.pdf}
    \caption{T-SNE two-dimensional projection of the 128 filters 
    learned by the first layer of the original 
    \protect\mycolorbox{0,0,255,0.2}{FCN} and the 64 filters learned by the first layer of the 
    \protect\mycolorbox{255,165,0,0.2}{H-FCN} on the CricketY dataset of the UCR Archive. 
    The two hand-crafted \protect\mycolorbox{255,0,0,0.7}{increasing} and 
    \protect\mycolorbox{0,128,0,0.7}{decreasing} trend 
    detection filters are also projected in the two-dimensional 
    space.}
    \label{fig:hcf-tsne_filters}
\end{figure}


We further assessed the impact of incorporating hand-crafted filters into the H-FCN model. We compared the 64 
learned filters in the first layer of H-FCN with the 128 filters in the first layer of the original FCN, including 
our hand-crafted increasing and decreasing filters. After computing the DTW distance for each pair of filters, 
we projected them into a two-dimensional space using t-SNE~\cite{t-sne-paper}, 
as shown in Figure~\ref{fig:hcf-tsne_filters} on the CricketY dataset.

We can see that the filters learned by H-FCN (in orange) are quite similar to those learned by FCN (in blue). 
However, there are noticeable gaps in the H-FCN filter distribution, marked by red and green ellipsoids. 
These gaps correspond to the hand-crafted increasing filter (red triangle) and decreasing filter (green triangle).
These hand-crafted filters serve as representative prototypes for several FCN learned filters. 
By integrating these hand-crafted filters, H-FCN reduces the number of learned filters needed, allowing 
the model to concentrate on learning other important patterns.

In the following analysis, we examine the model's behavior during training, both with and without our filters, 
to determine if the inclusion of hand-crafted filters improves the model's ability to generalize.

\subsubsection{Do Hand-Crafted Filters Help Models Generalize?}

\begin{figure}
    \centering
    \caption{
        Training phase of the FCN and H-FCN architectures on the 
        FiftyWords dataset while monitoring the validation loss on the test set.
    The \protect\mycolorbox{224,171,172,0.6}{training loss of H-FCN} converges faster than 
    the \protect\mycolorbox{0,128,156,0.6}{training loss of FCN}.
    Also, the \protect\mycolorbox{226,128,156,0.6}{validation loss of H-FCN} is always 
    below the \protect\mycolorbox{121,195,156,0.6}{validation loss of FCN}.
    This shows that H-FCN generalizes better than FCN 
    on this dataset.
    }
    \label{fig:hcf-generalization}
    \includegraphics[width=0.8\textwidth]{Figures/chapter_3/hcf/analysis/generalization-1.pdf}
\end{figure}

Experimental results in Section~\ref{sec:hcf-learned-vs-hcf} demonstrated that adding hand-crafted filters to Deep
Learning models 
improves performance. However, as shown in Section~\ref{sec:hcf-learned-vs-hcf}, some of these hand-crafted filters 
in H-FCN resemble 
filters learned by the original FCN. This raises a critical question: if FCN can independently learn these filters, 
why does including hand-crafted filters in H-FCN enhance accuracy?

The answer likely lies in the superior generalization ability of hand-crafted filters. In models like FCN 
without hand-crafted filters, the optimization process focuses on fitting the training data as closely as possible, 
which increases the risk of overfitting,a common problem in Deep Learning for TSC. Conversely, the inclusion of generic 
hand-crafted filters in H-FCN helps the model capture broader patterns that are more applicable to unseen test data.

To test this hypothesis, we compared the training and validation loss curves for both FCN and H-FCN models using the 
FiftyWords dataset, as illustrated in Figure~\ref{fig:hcf-generalization}.
Since the UCR Archive provides only training and test sets, 
we used the test set to compute the validation loss, solely for monitoring generalization and not for tuning 
hyper-parameters. The results show that the validation loss for H-FCN converges to a significantly lower value 
than that of FCN, indicating better generalization. This explains why H-FCN achieves a $15\%$
higher accuracy on the test set compared to FCN.

\subsubsection{Can We Use Hand-Crafted Filters To Construct Foundation Models?}

In conclusion, hand-crafted filters significantly aid in generalization because they are independent of specific datasets, 
allowing models to capture broad patterns applicable across various data. This approach is a step toward developing 
foundation models, which aim to be versatile and effective across multiple tasks and datasets. The next logical step is 
to explore constructing a foundation model by leveraging the new state-of-the-art model, H-InceptionTime, 
which incorporates hand-crafted filters. This will be discussed in the following section, where we will dig 
into the methodology and potential benefits of using H-InceptionTime as a foundation model.


\section{Finding Foundation Models for Time Series Classification Using A Pretext Task}\label{sec:pretext-task}

In this section, we introduce a foundation model for Time Series Classification (TSC) that leverages the H-Inception 
architecture, incorporating the benefits of hand-crafted filters. The core idea behind foundation models is to develop 
robust, pre-trained models that can generalize across diverse datasets, significantly enhancing the performance 
and efficiency of deep learning models in TSC.

In many real-world applications, starting from scratch with a deep learning model can be a significant downside. 
Collecting large amounts of labeled data is often costly and time-consuming, and in many cases, acquiring such 
extensive datasets is impractical. For instance, in the medical field, gathering sufficient data for training 
models to detect heart diseases from ECG signals involves not only extensive time and financial resources but 
also the expertise of medical professionals to annotate the data accurately. Similarly, in industrial applications 
like predictive maintenance, collecting sensor data from machinery involves prolonged monitoring periods and expert 
annotation to identify failure modes.

This is where pre-trained foundation models prove to be invaluable. By starting with a robust, pre-trained model 
that has already learned generalizable features from a wide array of datasets, we can significantly reduce the 
amount of new data needed. Fine-tuning these models on small, domain-specific datasets can lead to better performance 
without the risk of overfitting, which is a common issue when training from scratch with limited data.

Our foundation model, based on the H-Inception architecture, addresses these challenges effectively.
The H-Inception architecture (see Section~\ref{sec:hinceptiontime}) integrates hand-crafted filters (see
Section~\ref{sec:construction-hcf}), which have shown strong generalization capabilities, 
making them independent of specific datasets. This attribute aligns well with the objective of creating foundation 
models that perform consistently across different data sources.

To construct our foundation model, we benefit from the extensive UCR archive~\cite{ucr-archive},
which includes 128 datasets divided 
into 8 distinct domains, such as ECG, sensor data, and motion. This diverse collection allows us to train a model 
on a common task across datasets within the same domain. By leveraging these varied datasets, the pre-trained model 
learns to identify patterns that are relevant across different, yet related, data sources. This pre-training on a 
broad set of tasks enhances the model's ability to generalize and perform well when fine-tuned on specific datasets 
within each domain.

The contribution of this part of the chapter is twofold as detailed in Figure~\ref{fig:pretext-summary}: Firstly,
we propose a novel pre-training strategy that involves
a pretext task designed to predict the originating dataset of each time series sample. This approach enables the
model to learn generic features that are applicable across multiple datasets. Secondly, we fine-tune the pre-trained
model on specific datasets, enhancing its ability to adapt to the unique characteristics of each dataset while retaining
the generalized knowledge acquired during pre-training.

\begin{figure}
    \centering
    \caption{Summary of the proposed pretext task approach.
    Given an archive of $N$ datasets, the first step is to train a
    \protect\mycolorbox{0,175,255,0.6}{\emph{pre-trained} model}
    on all of the datasets, where the classification task is to 
    \protect\mycolorbox{255,123,0,0.6}{predict
    the dataset each time series belongs to}.
    The second step is to copy the \protect\mycolorbox{0,175,255,0.6}{\emph{pre-trained} model}
    and follow it with an \protect\mycolorbox{0,175,57,0.6}{\emph{addon} 
    model randomly initialized}.
    The second step is done for each of the $N$ datasets of the archive independently.
    After constructing the $N$ new models, they are fine-tuned on each dataset 
    depending on \protect\mycolorbox{255,0,0,0.6}{\textcolor{white}{the task of each one}}.}
    \label{fig:pretext-summary}
    \includegraphics[width=\textwidth]{Figures/chapter_3/pretext_task/PHIT.pdf}
\end{figure}

After fully training the pre-trained model on the pretext task, we can proceed with fine-tuning through two different 
approaches. The first approach involves directly fine-tuning the pre-trained model, followed by adding a classification 
layer tailored to the dataset's specific task. The second approach fine-tunes the pre-trained model by cascading it with 
additional deeper layers before adding the classification layer, allowing for the extraction of more intricate features. 
Previous research~\cite{transfer-learning-paper} employed the first approach to study transfer learning for TSC,
but the results were suboptimal 
due to target datasets being highly sensitive to the source dataset used. In our work, we opted for the second approach, 
believing it to be more robust. This decision stems from the understanding that the first method assumes the pre-trained 
model has already identified the optimal convolution filters, potentially neglecting deeper, dataset-specific features 
during fine-tuning. By incorporating deeper layers, our approach ensures that the model refines its feature extraction 
capabilities, leading to better generalization and performance across diverse datasets.

\subsection{Foundation Model Architecture Construction}\label{sec:pretext-construction}

Given a backbone deep learning model for TSC (H-Inception in our case)
consisting of $\Lambda$ layers, we divided the model into two distinct sub-models. 
The first sub-model, referred to as the pre-trained model, is designed to learn a pretext task. The second sub-model, 
which is randomly initialized, serves as an extension to the pre-trained model and focuses specifically on the TSC task.

The pretext task selected for this work involves the pre-trained model predicting the dataset of origin for each sample 
from a set of $N$ datasets (see Algorithm~\ref{alg:pretext-step1}). While it might seem more straightforward to combine all
datasets and 
classes into a single large class distribution for prediction, this approach has significant limitations. When there is 
no correlation between classes from different datasets, the combined class distribution would lack meaningful representation. 
Thus, using a pretext task to first train the model ensures a more structured and effective learning process.

\begin{algorithm}
\caption{Train the Pre-Trained Model on pretext Task}
\label{alg:pretext-step1}
\begin{algorithmic}[1]
    \REQUIRE
        $\mathcal{D}=\{\mathcal{D}_1,\mathcal{D}_2\ldots\mathcal{D}_N\}$ N datasets where
        $\mathcal{D}_i=\{\textbf{x}_{ij},y_{ij}\}_{j=1}^{M_i}$, the number of layers for the pre-trained mode $L_{PT}$
    \ENSURE A pre-trained model $PT(.)$ trained on the pretext task over all the datasets in $\mathcal{D}$
    
    \STATE Define $M = sum(M_1,M_2,\ldots,M_{N})$
    \STATE Define $\mathcal{D}_{PT} = empty List$
    \STATE Build $PT(.)$ a neural network with $\Lambda_{PT}$ layers and $M$ output units with $softmax$ activation
    \FOR{$i=1$ to $N$}
        \FOR{$j=1$ to $M_i$}
            \STATE $\mathcal{D}_{PT}.append([\textbf{x}_{ij},i])$
        \ENDFOR
    \ENDFOR
    \STATE $PT.train(\mathcal{D}_{PT})$
    \STATE \textbf{Return:} $PT(.)$
    
\end{algorithmic}
\end{algorithm}

Upon completing the training of the pre-trained model, we enhance it by integrating a randomly initialized sub-model.
This newly constructed composite model, consisting of the pre-trained and the new sub-model, is subsequently
fine-tuned for the TSC task on each dataset independently (refer to Algorithm~\ref{alg:pretext-step2}).

\begin{algorithm}
\caption{Fine Tuning on Each Dataset}
\label{alg:pretext-step2}
\begin{algorithmic}[1]
    \REQUIRE $\mathcal{D}=\{\mathcal{D}_1,\mathcal{D}_2\ldots\mathcal{D}_N\}$ N datasets
    where $\mathcal{D}_i=\{\textbf{x}_{ij},y_{ij}\}_{j=1}^{M_i}$, a pre-trained model $PT(.)$ of $\Lambda_{PT}$ layers
    trained on the pretext task, the number of layers of an addon model while fine tuning $\Lambda_{FT}$
    \ENSURE $\{FT_1(.),FT_2(.),\ldots FT_N(.)\}$ $N$ fine tuned models of $\Lambda_{PT}+\Lambda_{FT}$ layers trained
    on the task of each dataset independently
    
    \STATE Build $\{FT_1(.),FT_2(.),\ldots, FT_N(.)\}$ neural networks of $\Lambda_{PT}+\Lambda_{FT}$ layers with
    output nodes respecting the number of classes of each dataset in $\mathcal{D}$ respectively
    \STATE Fill the first $\Lambda_{PT}$ layers in $\{FT_1(.),FT_2(.),\ldots,FT_N(.)\}$ by the learned parameters from the 
    feature extraction part of $PT(.)$
    \FOR{$i=1$ to $N$}
        \STATE $FT_i.train(\mathcal{D}_i)$
    \ENDFOR
    
    \STATE \textbf{Return:} $\{FT_1(.),FT_2(.),\ldots, FT_N(.)\}$
    
\end{algorithmic}
\end{algorithm}

\subsection{Backbone Selection}\label{sec:pretext-backbone}

Our model is based on the state-of-the-art deep learning architecture for TSC, the H-Inception network
(Section~\ref{sec:hinceptiontime}).
Figure~\ref{fig:pretext-archi} illustrates how the H-Inception backbone is divided for our approach. The original H-Inception
architecture has six Inception modules. We designate the first three modules for the pre-trained model
and the last three for the fine-tuning phase.
H-InceptionTime is an ensemble of five H-Inception models with different initializations.
Thus, we adopt the H-Inception architecture as our backbone and use model ensemble, consistent with the original
works~\cite{inceptiontime-paper}.
We call this approach Pre-trained H-InceptionTime (PHIT).

\begin{figure}
    \centering
    \caption{
        The architecture of H-Inception divided into two sub-models.
    The \protect\mycolorbox{0,121,30,0.6}{first model} 
    is the pre-trained model, trained on the pretext 
    task, while the second model is the \protect\mycolorbox{255,0,0,0.6}{randomly initialized 
    add-on model}.
    The H-Inception model is made of six Inception modules, 
    where each module contains three \protect\mycolorbox{255,160,0,1.0}{convolution layers}
    and a \protect\mycolorbox{252,0,190,1.0}{Max Pooling layer} followed by a
    \protect\mycolorbox{255,221,55,1.0}{concatenation}, a
    \protect\mycolorbox{210,221,55,1.0}{batch normalization layer}
    and an \protect\mycolorbox{255,55,100,1.0}{activation function}.
    Each Inception module, except the first one, is proceeded 
    by a \protect\mycolorbox{240,148,255,1.0}{bottleneck layer}
    to reduce the dimensionality and hence the number of parameters.
    The first Inception module contains the hybrid addition, which 
    is the \protect\mycolorbox{0,232,132,1.0}{hand-crafted convolution filter}.
    \protect\mycolorbox{0,212,247,1.0}{Residual connections} exist between 
    the input and the third module, as well as between the third module
    and the output.
    }
    \label{fig:pretext-archi}
    \includegraphics[width=\textwidth]{Figures/chapter_3/pretext_task/H-Inception.pdf}
\end{figure}

\subsection{Does It Make Sense To Use Batch Normalization On Different Datasets?}\label{sec:pretext-batchnorm}

Most cutting-edge deep learning models for TSC~\cite{dl4tsc}, which excel on the UCR archive~\cite{ucr-archive},
are convolution-based
architectures that utilize Batch Normalization layers (Chapter~\ref{chapitre_1} Section~\ref{sec:tsc-deep})
to speed up training. In the H-Inception model we selected, each convolution layer is followed
by Batch Normalization. This process adjusts the batch samples to achieve 
zero mean and unit variance. However, this approach can be problematic when the batch samples come from different 
distributions, such as different datasets, which is the scenario for our pre-trained model.

To mitigate this issue, we introduce multiple Batch Normalization layers, each dedicated to a specific dataset,
instead of the single Batch Normalization layer typically used in CNN architectures for TSC. This setup requires
the model to appropriately connect each sample in the batch to its corresponding Batch Normalization layer.

Our innovative Batch Normalization Multiplexer (BNM) is depicted in Figure~\ref{fig:pretext-batchnorm}.
The BNM takes as input the output
from the preceding layer, along with the dataset information for each series being processed. This dataset information,
which the model is also attempting to predict, guides the control node of the BNM to select the correct Batch
Normalization layer for the output node. This design ensures that proper normalization is applied, even when dealing
with diverse datasets, thereby enhancing the pre-trained model's robustness and performance.

\begin{figure}
    \centering
    \caption{
        An example using the proposed
        \protect\mycolorbox{0,164,0,0.6}{Batch Normalizing Multiplexer (BNM)} 
        that solves the problem of 
        learning a batch normalization layer on multiple samples of 
        different distributions (datasets).
    The BNM is made of \protect\mycolorbox{210,221,55,1.0}{multiple batch normalization layers} 
    (with \protect\mycolorbox{0,0,255,1.0}{\textcolor{white}{blue}}
    and \protect\mycolorbox{255,0,0,1.0}{\textcolor{white}{red}} contours) 
    proceeded by a multiplexer.
    This multiplexer has three different nodes: (a) input node, 
    where the input time series goes through, 
    (b) the control node, where the information about the dataset this 
    input time series belong to goes through, 
    and (c) the output node.
    The path selected for the output node is controlled by the node (b).
    It is important to note that the BNM, such as the traditional batch 
    normalization layer, learns on the whole batch.
    The only difference is that more than one batch normalization 
    layer will be fed by parts of this batch, which 
    intuitively means the flow of information is slower when using the BNM.
    }
    \label{fig:pretext-batchnorm}
    \includegraphics[width=\textwidth]{Figures/chapter_3/pretext_task/batch-norm.pdf}
\end{figure}

\subsection{Experimental Setup}

\subsubsection{Datasets}
To evaluate the performance of our proposed approach, we conducted a series of experiments using the UCR archive dataset,
which consists of 128 datasets. Due to redundancies, we narrowed our study to 88 datasets. For example, some datasets 
appear multiple times with different train-test splits for various classification tasks, which could interfere with our 
model's objective of predicting the source dataset of a sample. Including identical series from different datasets in the 
training set could confuse the model. Additionally, some datasets were excluded because they only varied in class counts 
or were truncated versions of others. A detailed explanation of the exclusions is provided in Table~\ref{tab:pretext-ucr-not-used}.

To ensure consistency, all datasets were z-normalized before training, achieving a zero mean and unit variance. 
Since the sample lengths varied, we applied zero padding within each batch (instead of before training) to match 
the length of the longest series in that batch. This approach maintains the integrity of the model's training and 
evaluation processes.

\begin{table*}
    \centering
    \caption{Excluded datasets from the UCR archive in this study.
    Each dataaset is followed by its information and a reason for its exclusion.
    % The authors would like to thank the maintainer of the websites \text{https://www.timeseriesclassification.com}  and \text{https://www.cs.ucr.edu/\~{}eamonn/time\_series\_data\_2018/} from which the information presented in this table were gathered.
    }
    \label{tab:pretext-ucr-not-used}
    \scriptsize
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \textbf{Type} &
      \textbf{Dataset} &
      \textbf{Train Samples} &
      \textbf{Test Samples} &
      \textbf{Length} &
      \textbf{reason (if excluded)} \\ \hline
    EOG &
      EOGHorizontalSignal &
      362 &
      362 &
      1250 &
      \begin{tabular}[c]{@{}c@{}}same datasets multivariate\\ with 2 channels divided\\ into 2 univariate datasets\end{tabular} \\ \cline{2-5}
     &
      EOGVerticalSignal &
      362 &
      362 &
      1250 &
       \\ \hline
    EPG &
      InsectEPGRegularTrain &
      62 &
      249 &
      601 &
      \begin{tabular}[c]{@{}c@{}}same test set, different\\ train set size, a combination\\ of both train sets is better\\ than doing a pretext task\end{tabular} \\ \cline{2-5}
     &
      InsectEPGSmallTrain &
      17 &
      249 &
      601 &
       \\ \hline
     &
      PigAirwayPressure &
      104 &
      208 &
      2000 &
       \\ \cline{2-5}
    Hemodynamics &
      PigArtPressure &
      104 &
      208 &
      2000 &
      \begin{tabular}[c]{@{}c@{}}correlation unclear between\\ these three datasets\end{tabular} \\ \cline{2-5}
     &
      PigCVP &
      104 &
      208 &
      2000 &
       \\ \hline
    HRM &
      Fungi &
      18 &
      186 &
      201 &
      Only one dataset in this type \\ \hline
     &
      DistalPhalanxOutlineAgeGroup &
      400 &
      139 &
      80 &
      \begin{tabular}[c]{@{}c@{}}same samples as DistalPhalanxTW\\ with different classification and\\ train test split\end{tabular} \\ \cline{2-5}
     &
      DistalPhalanxOutlineCorrect &
      600 &
      276 &
      80 &
       \\ \cline{2-6} 
     &
      FaceAll &
      560 &
      1690 &
      131 &
      \begin{tabular}[c]{@{}c@{}}same as FacesUCR with different\\ train test split\end{tabular} \\ \cline{2-6} 
     &
      FiftyWords &
      450 &
      455 &
      270 &
      \begin{tabular}[c]{@{}c@{}}same as WordSynonyms with\\ more classes\end{tabular} \\ \cline{2-6} 
    Image &
      MiddlePhalanxOutlineAgeGroup &
      400 &
      154 &
      80 &
      Same reason as DistalPhalanx \\ \cline{2-5}
     &
      MiddlePhalanxOutlineCorrect &
      600 &
      291 &
      80 &
       \\ \cline{2-6} 
     &
      ProximalPhalanxOutlineAgeGroup &
      400 &
      205 &
      80 &
      Same reason as DistalPhalanx \\ \cline{2-5}
     &
      ProximalPhalanxOutlineCorrect &
      600 &
      291 &
      80 &
       \\ \cline{2-6} 
     &
      MixedShapesRegularTrain &
      500 &
      2425 &
      1024 &
      \begin{tabular}[c]{@{}c@{}}Bigger version of\\ MixedShapesSmallTrain\end{tabular} \\ \cline{2-6} \hline
     &
      GunPoint &
      50 &
      150 &
      150 &
      \begin{tabular}[c]{@{}c@{}}GunPointAgeSpan is the new\\ version with more samples\end{tabular} \\ \cline{2-6} 
     Motion &
      WormsTwoClass &
      181 &
      77 &
      900 &
      \begin{tabular}[c]{@{}c@{}}Same as Worms with different\\ number of classes\end{tabular} \\ \cline{2-6}
     &
      GunPointMaleVersusFemale &
      135 &
      316 &
      150 &
      \begin{tabular}[c]{@{}c@{}}Same as AgeSpan version with\\ different train test split\end{tabular} \\ \cline{2-5}
     &
      GunPointOldVersusYoung &
      136 &
      315 &
      150 &
       \\ \hline
    Power &
      PowerCons &
      180 &
      180 &
      144 &
      Only one dataset for this type \\ \hline
     &
      AllGestureWiimoteX &
      300 &
      700 &
      Vary &
       \\ \cline{2-5}
     &
      AllGestureWiimoteY &
      300 &
      700 &
      Vary &
      \begin{tabular}[c]{@{}c@{}}too much Variable length datasets\\ to handle in this type for the pretext\\ task which already has the variable\\ length issue/instability\end{tabular} \\ \cline{2-5}
     &
      AllGestureWiimoteZ &
      300 &
      700 &
      Vary &
       \\ \cline{2-6} 
     &
      DodgerLoopDay &
      78 &
      80 &
      288 &
       \\ \cline{2-5}
     &
      DodgerLoopGame &
      20 &
      138 &
      288 &
      \begin{tabular}[c]{@{}c@{}}All dodger datasets are the same\\ with different train test split\\ with too many missing values\end{tabular} \\ \cline{2-5}
    Sensors &
      DodgerLoopWeekend &
      20 &
      138 &
      288 &
       \\ \cline{2-6} 
     &
      FreezerRegularTrain &
      150 &
      2850 &
      301 &
      \begin{tabular}[c]{@{}c@{}}Same as FreezerSmallTrain with\\ more training examples\end{tabular} \\ \cline{2-6} 
     &
      GesturePebbleZ1 &
      132 &
      172 &
      Vary &
       \\ \cline{2-5}
     &
      GesturePebbleZ2 &
      146 &
      158 &
      Vary &
      \begin{tabular}[c]{@{}c@{}}too much Variable length datasets\\ to handle in this type for the pretext\\ task which already has the variable\\ length issue/instability\end{tabular} \\ \cline{2-5}
     &
      PickupGestureWiimoteZ &
      50 &
      50 &
      Vary &
       \\ \cline{2-5}
     &
      ShakeGestureWiimoteZ &
      50 &
      50 &
      Vary &
       \\ \hline
     &
      Rock &
      20 &
      50 &
      2844 &
      Not a time series \\ \cline{2-6} 
     &
      SemgHandGenderCh2 &
      300 &
      600 &
      1500 &
       \\ \cline{2-5}
    Spectrum &
      SemgHandMovementCh2 &
      450 &
      450 &
      1500 &
      \begin{tabular}[c]{@{}c@{}}Same datasets different split\\ if we include one of them we end up\\ with one dataset for this type\end{tabular} \\ \cline{2-5}
     &
      SemgHandSubjectCh2 &
      450 &
      450 &
      1500 &
       \\ \hline
     &
      GestureMidAirD1 &
      208 &
      130 &
      Vary &
       \\ \cline{2-5}
    Trajectory &
      GestureMidAirD2 &
      208 &
      130 &
      Vary &
      \begin{tabular}[c]{@{}c@{}}Only datasets of variable length.\\ The three datasets are from the same\\ distribution of a 3D multivariate\\ time series with each being a dimension\\ a more suitable approach is to combine\\ the three datasets and solve\\ a multivariate TSC task\end{tabular} \\ \cline{2-5}
     &
      GestureMidAirD3 &
      208 &
      130 &
      Vary &
       \\ \hline
\end{tabular}
    }
\end{table*}
    

\subsubsection{Division of the Datasets into Types}

The goal of using a pre-trained model is to boost the performance of deep learning classifiers on small datasets 
by utilizing knowledge gained from larger datasets. This strategy is particularly effective when there is some 
shared basic information between the large and small datasets. To explore this, we conducted eight different 
pretext experiments, each corresponding to a different type of dataset in the UCR archive. For each experiment, 
we trained a pre-trained model using all datasets of a specific type, such as ECG, and then fine-tuned the model 
on each dataset individually. The eight dataset types and their corresponding numbers of datasets are as follows:

\begin{itemize} 
    \item Electrocardiogram (ECG): 7 datasets
    \item Sensors: 18 datasets
    \item Devices: 9 datasets
    \item Simulation: 8 datasets
    \item Spectrogram: 8 datasets
    \item Motion: 13 datasets
    \item Traffic: 2 datasets
    \item Image Contours: 23 datasets
\end{itemize}

\subsubsection{Implementation Details}

% The proposed method is implemented in Python using TensorFlow~\cite{tensorflow-paper}, and the code is available to the public
% here: \url{https://github.com/MSD-IRIMAS/DomainFoundationModelsTSC}.
We maintained the same parameters for the H-Inception model as in the original study (first contribution of this chapter).
Each experiment 
was conducted with five different initializations, covering both the pre-trained and fine-tuned models. 
We aggregated the results from these multiple runs, selecting the best-performing model based on training loss for evaluation.

To optimize training, we utilized a learning rate decay with the ReduceLROnPlateau function in Keras~\cite{keras-book}, 
which halves the learning rate when the training loss stabilizes. All models were trained with a batch 
size of 64. Both the pre-trained and fine-tuned models were trained for 750 epochs each, ensuring that 
the total training duration did not exceed the 1500 epochs used for the baseline model in the original
study (first contribution in this chapter).

% All experiments were executed on an Ubuntu 22.04 machine with an NVIDIA GeForce RTX 3090 graphics card,
% featuring 24GB of memory.

\subsection{Experimental Results}

In this section, we present the results of PHIT compared to the baseline model, followed by a comparison with 
state-of-the-art deep and non-deep models for TSC on the UCR archive.

\subsubsection{Comparing With Baseline With No Pre-Training}

\begin{figure}
    \centering
    \caption{
        A 1v1 scatter plot that compares the performance of H-InceptionTime (baseline) and PHIT following the accuracy
metric.
    }
    \label{fig:pretext-1v1-baseline}
    \includegraphics[width=0.5\textwidth]{Figures/chapter_3/pretext_task/results/PHIT-vs-baseline.pdf}
\end{figure}

In this section, we present a direct comparison between our pre-training approach using the H-Inception 
architecture and the baseline model, both evaluated in their ensemble forms.
Figure~\ref{fig:pretext-1v1-baseline} illustrates this comparison with a scatter 1v1 plot.
The x-axis indicates the accuracy of H-InceptionTime, while the y-axis shows the accuracy of PHIT, 
both measured on the test sets.
Our findings show that PHIT outperforms on average the baseline across 88 datasets, with PHIT achieving higher accuracy in 
48 datasets compared to the baseline's 23. To assess the statistical significance of this difference, we used 
the Wilcoxon Signed-Rank Test to produce a p-value reflecting the confidence level of the performance 
difference. With a p-value of approximately $0.021 < 0.05$, it is clear that PHIT significantly outperforms the baseline.
This demonstrates that the pre-trained model was able to generalize to the test set better, a result that 
we will analyze in more detail in Section~\ref{sec:pretext-analyze}.

\subsubsection{Comparing To State-Of-The-Art}

\begin{figure}
    \centering
    \caption{
        A Multi-Comparison Matrix (MCM) representing the comparison between the proposed approach PHIT with the
state-of-the-art approaches. The winning approach following the average performance is MultiROCKET and in second comes
our approach. No conclusion can be found on the difference of performance between MultiROCKET and PHIT given the
high p-value.
    }
    \label{fig:pretext-mcm-row}
    \includegraphics[width=\textwidth]{Figures/chapter_3/pretext_task/results/mcm_row.pdf}
\end{figure}

Figure~\ref{fig:pretext-mcm-row} presents the MCM (Chapter~\ref{chapitre_2} Section~\ref{sec:MCM})
comparing PHIT with state-of-the-art approaches, encompassing both deep and non-deep 
learning models. The results demonstrate that PHIT outperforms all deep learning approaches based on the average 
performance metric across the 88 datasets in the UCR archive. Moreover, the MCM also reveals that there is no 
statistically significant difference in performance between PHIT and the state-of-the-art MultiROCKET model, 
indicating that while PHIT shows strong performance, it matches rather than surpasses MultiROCKET in statistical significance.

\subsection{Analysis}\label{sec:pretext-analyze}

In this section, we present a detailed analysis aimed at understanding why the pre-training phase was able to 
outperform the baseline model. We achieve this by examining the performance differences between the pre-trained 
model and the baseline across various domains. Additionally, we analyze the impact of training set size for 
each dataset within these domains. Finally, we visualize the filter space to observe the effects of training 
for the three methods: baseline, pre-trained, and fine-tuned.

\subsubsection{Analysing Performance Per Domain}\label{sec:pretext-per-domain}

% \begin{table}
%     \centering
%     \caption{The Win/Tie/Loss count between the proposed PHIT approach and the baseline (H-InceptionTime) per dataset domain.
%     The first column presents the number of datasets included per domain followed by the number of Wins for PHIT, number of Ties, and number of Wins for the baseline.
%     We include as well the percentage of number of losses and the average difference in accuracy (PHIT - baseline).
%     A positive value in the last column indicates that on average of all datasets in a specific domain, PHIT performs better than the baseline on the accuracy metric (lowest value 0.0 and highest value 1.0).}
%     \label{tab:pretext-wins-per-type}
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{|c|c|c|c|c|c|c|}
%     \hline
%     \scriptsize
%     \textbf{\begin{tabular}[c]{@{}c@{}}Dataset\\ Type\end{tabular}} &
%       \textbf{\begin{tabular}[c]{@{}c@{}}Number of\\ Datasets\end{tabular}} &
%       \textbf{\begin{tabular}[c]{@{}c@{}}Wins of\\ PHIT\end{tabular}} &
%       \textbf{\begin{tabular}[c]{@{}c@{}}Ties of\\ PHIT\end{tabular}} &
%       \textbf{\begin{tabular}[c]{@{}c@{}}Losses of\\ PHIT\end{tabular}} &
%       \begin{tabular}[c]{@{}c@{}}\textbf{Percentage}\\ \textbf{of Losses}\end{tabular} &
%       \textbf{\begin{tabular}[c]{@{}c@{}}Difference in\\ Average Accuracy\\ (PHIT - Baseline)\end{tabular}} \\ \hline
%     \textbf{Devices}    & 9  & 4  & 0 & \textbf{5} & \textbf{55.55 \%} & +0.0046 \\ \hline
%     \textbf{ECG}        & 7  & \textbf{3}  & 2 & 2 & \textbf{28.57 \%} & +0.0012 \\ \hline
%     \textbf{Images}     & 23 & \textbf{14} & 2 & 7 & \textbf{30.43 \%} & +0.0087 \\ \hline
%     \textbf{Motion}     & 13 & \textbf{11} & 1 & 1 & \textbf{07.69 \%} & +0.0179 \\ \hline
%     \textbf{Sensors}    & 18 & \textbf{7}  & 5 & 6 & \textbf{33.33 \%} & +0.0002 \\ \hline
%     \textbf{Simulation} & 8  & \textbf{3}  & \textbf{3} & 2 & \textbf{25.00 \%} & +0.0051 \\ \hline
%     \textbf{Spectro}    & 8  & \textbf{3}  & 2 & \textbf{3} & \textbf{37.50 \%} & +0.0115 \\ \hline
%     \textbf{Traffic}    & 2  & 0  & 0 & 2 & \textbf{100.0 \%} & -0.0333 \\ \hline
%     \end{tabular}
%     }
% \end{table}

\begin{table}
    \centering
    \caption{The Win/Tie/Loss count between the proposed PHIT approach and the baseline (H-InceptionTime) per dataset domain.
    The first column presents the number of datasets included per domain followed by the number of Wins for PHIT, number of Ties, and number of Wins for the baseline.
    We include as well the percentage of number of losses and the average difference in accuracy (PHIT - baseline).
    A positive value in the last column indicates that on average of all datasets in a specific domain, PHIT performs better than the baseline on the accuracy metric (lowest value 0.0 and highest value 1.0).}
    \label{tab:pretext-wins-per-type}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \scriptsize
    \textbf{\begin{tabular}[c]{@{}c@{}}Dataset\\ Type\end{tabular}} &
      \textbf{\begin{tabular}[c]{@{}c@{}}Number of\\ Datasets\end{tabular}} &
      \textbf{\begin{tabular}[c]{@{}c@{}}Wins of\\ PHIT\end{tabular}} &
      \textbf{\begin{tabular}[c]{@{}c@{}}Ties of\\ PHIT\end{tabular}} &
      \textbf{\begin{tabular}[c]{@{}c@{}}Losses of\\ PHIT\end{tabular}} &
      \textbf{\begin{tabular}[c]{@{}c@{}}Difference in\\ Average Accuracy\\ (PHIT - Baseline)\end{tabular}} &
      \begin{tabular}[c]{@{}c@{}}\textbf{Percentage}\\ \textbf{of Losses}\end{tabular} \\ \hline
    \textbf{Devices}    & 9  & 4  & 0 & \textbf{5} & +0.0046 & \textbf{55.55 \%} \\ \hline
    \textbf{ECG}        & 7  & \textbf{3}  & 2 & 2 & +0.0012 & \textbf{28.57 \%} \\ \hline
    \textbf{Images}     & 23 & \textbf{14} & 2 & 7 & +0.0087 & \textbf{30.43 \%} \\ \hline
    \textbf{Motion}     & 13 & \textbf{11} & 1 & 1 & +0.0179 & \textbf{07.69 \%} \\ \hline
    \textbf{Sensors}    & 18 & \textbf{7}  & 5 & 6 & +0.0002 & \textbf{33.33 \%} \\ \hline
    \textbf{Simulation} & 8  & \textbf{3}  & \textbf{3} & 2 & +0.0051 & \textbf{25.00 \%} \\ \hline
    \textbf{Spectro}    & 8  & \textbf{3}  & 2 & \textbf{3} & +0.0115 & \textbf{37.50 \%} \\ \hline
    \textbf{Traffic}    & 2  & 0  & 0 & 2 & -0.0333 & \textbf{100.0 \%} \\ \hline
    \end{tabular}
    }
\end{table}


In Table~\ref{tab:pretext-wins-per-type}, we provide a comprehensive analysis of the PHIT approach's 
performance compared to the baseline 
across different dataset domains. For each domain in the UCR archive, we list the total number of datasets, 
the Win/Tie/Loss count, and the average difference in performance in the final column. A positive value in 
this column indicates that, on average, PHIT surpasses the baseline in terms of accuracy. Additionally, the 
last column shows the percentage of datasets where PHIT performed worse than the baseline.

The table reveals that the percentage of losses for PHIT exceeds $50\%$ in only two instances, and the 
average performance difference is positive for all domains except Traffic, which contains only two datasets. 
These results highlight that PHIT generally outperforms the baseline across most domains in the UCR archive.

This analysis demonstrates that fine-tuning a pre-trained model on a common task shared by multiple datasets 
is significantly more effective than the traditional approach. In the following section, we will investigate deeper 
into the scenarios where the pre-trained model outperforms the baseline by examining the size of the training sets.

\subsubsection{Larger Datasets Helping Smaller Datasets}\label{sec:pretext-large-help-small}

\begin{figure}
    \centering
    \caption{
        Comparing the performance of the proposed approach and its change 
        with respect to the training set size. The
curve represents the \protect\mycolorbox{0,0,255,0.6}{\textcolor{white}{difference in 
performance between the proposed approach and the baseline}}. A positive value
(above the \protect\mycolorbox{255,0,0,0.6}{tie line})
represents a win for the pre-training approach. For each plot, 
we show this comparison on the datasets of the same type in the
UCR archive. The $x$-axis represents the number of training 
examples (in $\log_{10}$ scale). The y-axis represents the difference
of accuracy between the usage of our pre-training approach and the baseline.
    }
    \label{fig:pretext-per-training-size}
    \includegraphics[width=\textwidth]{Figures/chapter_3/pretext_task/results/accuracy_vs_train_size_plots.pdf}
\end{figure}

As outlined earlier (Section~\ref{sec:pretext-task}), the primary aim of the pretext task is to
enhance the performance of deep learning models on 
TSC tasks, particularly when faced with datasets that have a limited number of training samples. In this section, 
we explore the effect of the pretext task on each of the 8 dataset types, considering the number of training samples 
available. This analysis is illustrated in Figure~\ref{fig:pretext-per-training-size},
where the $y$-axis represents the difference in accuracy between 
PHIT and the baseline, and the $x$-axis (in $\log_{10}$ scale) denotes the training set size. The study is presented across 8 
distinct plots, one for each dataset type. Positive values in the blue curves indicate that PHIT outperforms the baseline.

Our observations reveal that, on average, the pretext task significantly benefits datasets with fewer than $10^3$ 
training samples. This is evident in most cases, though not in every case. We hypothesize that this effect arises 
because the pretext task allows the model to glean more knowledge from larger datasets, which can then be effectively 
transferred to smaller ones. This transfer process provides the fine-tuning stage with rich, informative data for 
small datasets while introducing a degree of noise for larger ones. Larger datasets require the model's full attention 
on their specific tasks, whereas smaller datasets gain a crucial boost from the additional information, which the model 
alone might struggle to learn without external guidance.

\subsubsection{Analyzing The Filters Space}\label{sec:pretext-filters}

\begin{figure}
    \centering
    \caption{
        A two dimensional representation of the filters coming 
        from the \protect\mycolorbox{0,62,255,0.6}{first Inception module of the baseline}, 
        \protect\mycolorbox{255,16,0,0.6}{pre-trained} and 
        \protect\mycolorbox{0,127,0,0.6}{fine tuned} models.
    The used datasets in this study are ECG200 (left) 
    and NonInvasiveFetalECGThorax1 (right).
    The two dimensional representation is done using $t$-SNE coupled with DTW to 
    as a distance measure.
    Some \protect\mycolorbox{255,100,255,0.6}{areas} can be seen to be in common 
    between the three models in the case of large datasets (right) however it is not 
    the case for small datasets (left).
    }
    \label{fig:pretext-filters}
    \includegraphics[width=\textwidth]{Figures/chapter_3/pretext_task/results/filters_2d_ECG_datasets.pdf}
\end{figure}

Given our focus on CNNs, we can compare the learned filter spaces to understand the impact of our pre-training approach. 
To visualize this, we employed the t-SNE technique~\cite{t-sne-paper}, reducing the 
dimensionality of the filters to a two-dimensional space. We used DTW for the t-SNE technique as explained before, 
to have a shift-invariant two-dimensional projection.
We visualized the filters from the first Inception module of the baseline, pre-trained, and fine-tuned models in 
Figure~\ref{fig:pretext-filters}, focusing on the ECG datasets: ECG200 and NonInvasiveFetalECGThorax1 from the UCR 
archive~\cite{ucr-archive}.
These datasets were chosen deliberately 
due to their differing training set sizes, with ECG200 having $100$ training examples and
NonInvasiveFetalECGThorax1 having $1800$.

Figure~\ref{fig:pretext-filters} displays the filter distributions for the baseline, pre-trained,
and fine-tuned models for each dataset. 
One prominent observation is that the blue points (baseline filters) are markedly different from the red and 
green points (pre-trained and fine-tuned filters). This demonstrates that the pre-training followed by fine-tuning 
leads to the learning of different convolution filters compared to the traditional baseline approach.
Another key observation is the variation between the two plots. For ECG200 (left plot), there is minimal overlap 
between the filters of the three models, indicating distinct learning outcomes. In contrast, for 
NonInvasiveFetalECGThorax1 (right plot), there are numerous overlapping areas among the filters of 
different models. This supports our earlier argument in Section 4.3 that larger datasets tend to refine 
existing knowledge rather than discovering new features. However, the presence of new regions for the 
pre-trained and fine-tuned filters (green and red) indicates that even large datasets can benefit from 
the new filters explored during pre-training, leveraging insights gained from other datasets.

In summary, the filter distributions show that pre-training allows models to learn different and sometimes 
more complex filters than the baseline approach. While smaller datasets like ECG200 encourage the discovery 
of new, unique features, larger datasets like NonInvasiveFetalECGThorax1 tend to refine knowledge, though 
they can still benefit from the exploration of new filters during pre-training.

\section{Conclusion}\label{sec:pretext-conclusion}

In this chapter, we have explored the integration of hand-crafted convolution filters into deep learning architectures 
for time series classification (TSC). We started by examining the rationale and construction of these filters, 
demonstrating their potential to improve model performance significantly. Our experiments showed that hand-crafted 
filters could generalize across various datasets, enhancing the robustness and accuracy of the state-of-the-art 
deep learning model InceptionTime by proposing it's new hybrid version H-InceptionTime.

We then introduced a pre-training approach using the H-Inception architecture, leveraging the generalization 
capabilities of hand-crafted filters. The pretext task, designed to predict the dataset of origin for each time 
series, proved effective in transferring knowledge from larger to smaller datasets. This methodology helped 
mitigate the common issue of overfitting, particularly when dealing with limited training samples.

The experimental results validated our approach, with the Pre-trained H-InceptionTime (PHIT) model 
consistently outperforming the baseline across different dataset domains. Our analysis also highlighted 
the importance of fine-tuning with additional layers to capture deeper features, further boosting performance.

In summary, this chapter demonstrates that incorporating hand-crafted filters and a strategic pre-training 
approach can significantly enhance the performance of deep learning models for TSC. These findings pave 
the way towards developing foundation models that can generalize well across diverse datasets, reducing 
the need for extensive data collection and training from scratch.

One of the key insights from this chapter was that simply adding hand-crafted filters to the FCN "poorly performing" 
model enabled it to achieve state-of-the-art performance. This raises an intriguing question as we move forward: 
can we reduce the complexity of all these models while maintaining or even enhancing their performance? 
This question will be the focus of the next chapter.