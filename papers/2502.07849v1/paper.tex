\newif\ifarxiv
\arxivtrue % compiles arxiv version 
% \arxivfalse % compiles ICML version
\ifarxiv
    \documentclass[]{fairmeta} % Option "twocolumn" available, but please prioritize single-column
    \usepackage{algorithm}
    \usepackage{lmodern} % K: moved it here from preamble.tex
    \usepackage{amsmath}
    \usepackage{amssymb}
    \usepackage{subcaption}
    % \usepackage{subfigure}
    \usepackage{adjustbox}
\else 
    \documentclass{article}
    \usepackage{adjustbox}

    % Recommended, but optional, packages for figures and better typesetting:
    \usepackage{microtype}
    \usepackage{graphicx}
    \usepackage{subfigure}
    \usepackage{booktabs} % for professional tables
    
    \usepackage{nicematrix} 
    \usepackage{tikz} 
    \usepackage{sidecap} 
    \usepackage{arydshln}
    \usepackage{makecell}
    \usepackage{multirow}


    % hyperref makes hyperlinks in the resulting PDF.
    % If your build breaks (sometimes temporarily if a hyperlink spans a page)
    % please comment out the following usepackage line and replace
    % \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
    \usepackage{hyperref}
    
    
    % Attempt to make hyperref and algorithmic work together better:
    \newcommand{\theHalgorithm}{\arabic{algorithm}}
    
    % Use the following line for the initial blind version submitted for review:
    \usepackage{icml2025}
    % If accepted, instead use the following line for the camera-ready submission:
    %\usepackage[accepted]{icml2025}
    
    % For theorems and such
    \usepackage{amsmath}
    \usepackage{amssymb}
    \usepackage{mathtools}
    \usepackage{amsthm}
    
    \usepackage[capitalize,noabbrev]{cleveref}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % THEOREMS
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \theoremstyle{plain}
    \newtheorem{theorem}{Theorem}[section]
    \newtheorem{proposition}[theorem]{Proposition}
    \newtheorem{lemma}[theorem]{Lemma}
    \newtheorem{corollary}[theorem]{Corollary}
    \theoremstyle{definition}
    \newtheorem{definition}[theorem]{Definition}
    \newtheorem{assumption}[theorem]{Assumption}
    \theoremstyle{remark}
    \newtheorem{remark}[theorem]{Remark}
    
    % Todonotes is useful during development; simply uncomment the next line
    %    and comment out the line below the next line to turn off comments
    %\usepackage[disable,textsize=tiny]{todonotes}
    %\usepackage[textsize=tiny]{todonotes}
\fi



\input{preamble}


\ifarxiv
    \author[1,2]{Krunoslav Lehman Pavasovic}
    \author[1]{Jakob Verbeek}
    \author[2,\dagger]{Giulio Biroli}
    \author[3,\dagger]{Marc Mezard}
    
    \affiliation[1]{FAIR at Meta}
    \affiliation[2]{École Normale Supérieure, Paris}
    \affiliation[3]{Bocconi University, Milan}
    
    

    % \contribution[*]{Work done at Meta}
    \contribution[\dagger]{Joint last author}
    
    \abstract{
    \input{0-abstract.tex}
    }
    
    \date{\today}
    \correspondence{Krunoslav Lehman Pavasovic at \email{krunolp@meta.com}}
    
    % You can add additional metadata fields as follows 
    %\metadata[Code]{\url{https://github.com/facebookresearch/repo}}
    %\metadata[Blogpost]{\url{https://ai.meta.com/blog/?page=1}}

\else
%
\fi





\ifarxiv
%\title{Classifier-free guidance becomes exact for large dimensional data}
\title{Understanding Classifier-Free Guidance: High-Dimensional Theory and Non-Linear Generalizations}
\else
% A short form for the running title is supplied here:
    \icmltitlerunning{Understanding CFG: High-Dimensional Theory and Non-Linear Generalizations}
\fi


\begin{document}

\ifarxiv
    \maketitle
\else
    \twocolumn[
    \icmltitle{Understanding Classifier-Free Guidance:\\
    High-Dimensional Theory and  Non-Linear Generalizations}
    
    % It is OKAY to include author information, even for blind
    % submissions: the style file will automatically remove it for you
    % unless you've provided the [accepted] option to the icml2025
    % package.
    
    % List of affiliations: The first argument should be a (short)
    % identifier you will use later to specify author affiliations
    % Academic affiliations should list Department, University, City, Region, Country
    % Industry affiliations should list Company, City, Region, Country
    
    % You can specify symbols, otherwise they are numbered in order.
    % Ideally, you should not use this facility. Affiliations will be numbered
    % in order of appearance and this is the preferred way.
    \icmlsetsymbol{equal}{*}
    
    \begin{icmlauthorlist}
    \icmlauthor{Firstname1 Lastname1}{equal,yyy}
    \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
    \icmlauthor{Firstname3 Lastname3}{comp}
    \icmlauthor{Firstname4 Lastname4}{sch}
    \icmlauthor{Firstname5 Lastname5}{yyy}
    \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
    \icmlauthor{Firstname7 Lastname7}{comp}
    %\icmlauthor{}{sch}
    \icmlauthor{Firstname8 Lastname8}{sch}
    \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
    \end{icmlauthorlist}
    
    \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
    \icmlaffiliation{comp}{Company Name, Location, Country}
    \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}
    
    \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
    \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}
    
    % You may provide any keywords that you
    % find helpful for describing your paper; these are used to populate
    % the "keywords" metadata in the PDF but will not be shown in the document
    \icmlkeywords{Machine Learning, ICML}
    
    \vskip 0.3in
    ]
    
    % this must go after the closing bracket ] following \twocolumn[ ...
    
    % This command actually creates the footnote in the first column
    % listing the affiliations and the copyright notice.
    % The command takes one argument, which is text to display at the start of the footnote.
    % The \icmlEqualContribution command is standard text for equal contribution.
    % Remove it (just {}) if you do not need this facility.
    
    %\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
    % \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
    
    \begin{abstract}
        \input{0-abstract}
    \end{abstract}
    
\fi

\section{Introduction}

\looseness=-1Diffusion models \citep{sohldickstein2015deepunsupervisedlearningusing, song2020generativemodelingestimatinggradients, ho2020denoisingdiffusionprobabilisticmodels} have become the state-of-the-art algorithms to generate high-quality images, audio and video. By simulating  Orstein-Uhlenbeck Langevin dynamics, noise is first progressively added to data until it becomes completely random. Diffusion models then learn to reverse this process and generate new samples through a time-reversed Langevin equation. This backward evolution is steered by a force, the score, that is estimated from the data. 
An important task for diffusion models is generating data conditioned on a label, e.g., in text-to-image generation, on a textual description which allows the model to generate images that match the content of the description. This can be achieved through the concept of guidance \citep{dhariwal2021diffusion, ho2022classifier}. This approach allows the model to generate more specific outputs that align with user intentions or desired properties, rather than producing generic samples from the model of the data distribution. Guidance allows control over the generative sampling process, making it possible to create outputs with higher fidelity and relevance to the input criteria. 

\looseness=-1The first form of guidance introduced was classifier guidance \citep{ song2020score, dhariwal2021diffusion}. In this approach, one learns scores conditioned on class labels using a classifier that estimates the probability that a given noised image corresponds to the given label. This technique allows for precise control over the attributes of the generated outputs, enhancing their alignment with specific categories. However, the reliance on a pre-trained classifier can be computationally expensive and may introduce biases inherent to the classifier itself.

Classifier-Free Guidance (CFG) \citep{ho2022classifier} was developed  as an alternative to overcome these challenges, and was quickly adopted as a standard technique in 
state-of-the-art diffusion models such as GLIDE~\citep{nichol2021glide}, %poole2022dreamfusion}. CFG plays a critical role in 
%image super-resolution, e.g.,  as well as 
%state-of-the-art image-to-text models like 
%cascaded diffusion models \citep{ho2022cascaded}, 
DALL-E 3 \citep{betker2023improving}, Imagen \citep{saharia2022photorealistic}, and EMU~\citep{dai23arxiv}, as well as flow matching based models \citep{esser2024scaling}.
%
CFG eliminates the need for an external classifier by training the diffusion model to conditionally generate samples based on class labels/textual prompts directly. During training, the model learns to produce both class-conditional and unconditional samples. 
The score used in classifier-free guidance, however, {\it is not} the one guaranteed to give the target distribution. 
Instead, one adds to the latter an extra term whose role is to better guide the backward process toward the desired labeled data. 

CFG sampling of diffusion models is a surprising procedure in many ways. On the one hand, it lacks a theoretical foundation, and 
it has been proven wrong
in simple examples like one-dimensional Gaussian mixtures,  as it leads to a wrong target distribution \citep{chidambaram2024does}. In fact, several recent theoretical studies \citep{wu2024theoretical, xia2024rectified, bradley2024classifier} explored this phenomenon in low-dimensional settings, concluding that CFG is ineffective and may result in diffusion \textit{overshooting} the target distribution, as well as \textit{shrinking} the variance. The left panel of \Cref{fig:1_histograms} shows an example for a two-dimensional Gaussian mixture. On the other hand, CFG is beneficial in real applications, and used to generate high-quality samples that are consistent with the input conditioning. Understanding why CFG is wrong in theory but good in practice is the riddle that we address in this work. 

\paragraph{Our Contributions.} We show through theoretical arguments and empirical results that the high-dimensionality of the data, which results in the emergence of distinct dynamical regimes, allows CFG to correctly reproduce the target distribution, as shown in the right panel of  \Cref{fig:1_histograms} for a 200-dimensional Gaussian mixture defined in Section \ref{sec:gauss_mixt}. The conditions are the same as for the case of $d=2$ (left panel) which instead exhibits overshooting of the mean and variance shrinking. Based on these theoretical results, we propose a more general class of non-linear  CFG schemes. We apply these to class-conditional and text-to-image diffusion models, and show that these general CFGs are beneficial in practice in terms of image quality and diversity.


Our contributions are therefore two-fold:

\textbf{\rom{1}}. We explain the correctness of CFG for high-dimensional data by leveraging the emergence of separate dynamical regimes in the backward diffusion process \citep{biroli2024dynamical}. 
CFG acts in the first regime associated to the formation of classes and to symmetry-breaking \citep{biroli2023generative, raya2024spontaneous} by guiding faster toward the desired class, but it does not have any effect in the second regime where the generation conditioned to the class actually takes place. We provide a thorough theoretical analysis of this blessing-of-dimensionality phenomenon for high-dimensional Gaussian mixtures. For lower dimensional data, CFG generates a distribution which has a perturbed mean (with higher magnitude) and a lower variance. We characterize this result exactly for Gaussian mixture in large but finite dimension. We show that our theory agrees with numerical simulations,
state-of-the-art diffusion models trained on image data, as well as with  lower diversity observed in practice~\citep{sadat2023cads, sehwag2022generating}.  
    
\textbf{\rom{2}}. Leveraging our theoretical analysis, we show that more general versions of CFG are possible, which enjoy the same properties as the standard linear CFG. We propose generalizations of CFG and experimentally demonstrate in the high-dimensional Gaussian mixture case their desirable properties, including smaller overshoot, dampened variance shrinkage and faster convergence to the target distribution. We apply non-linear CFG  %first in the high-dimensional Gaussian mixture case, for comparison to the theoretical results, and then 
to class-conditional models (DiT  \citet{peebles2023scalable} and EDM2  \citet{karras2024analyzing}) trained on ImageNet-1K \citep{deng2009imagenet}, as well as text-to-image models (MMDiT,  \citet{esser2024scaling} and MDTv2, \citet{gao2023masked}) trained on CC12M \citep{changpinyo2021conceptual}, YFCC100M \citep{thomee2016yfcc100m} and a large internal dataset consisting of 320M Shutterstock images. 
Our findings show that non-linear CFG provides greater flexibility and improves generation quality. 
   
\newcommand{\labelonehist}{\label{fig:1_histograms}}

\ifarxiv
    \begin{figure}[t]
        \centering
        \hspace{-0.4cm}
        \includegraphics[width=0.525\textwidth]{assets/plots/1_histograms.pdf}
    \caption{\textbf{CFG  only overshoots in low dimensions.} Model:  mixture of two Gaussians (only one component shown) with dimension $d=2$ (left) and $d=200$ (right), centered in $\pm \vec m$ and variance $\sigma^2=1$,  $\omega\in\{0, 0.2, 15\}$, see \Cref{eqn:gm_0}. We show the histograms of $q(t=0)=\vec{x}\cdot\vec{m}/|\vec{m}|$, the generated samples projected onto the normalized mean vector $\vec{m}$ obtained with backward diffusion with 10,000 trajectories. 
    The guidance strength is given by $\omega$, and $\omega=0$ corresponds to sampling without CFG.
    For $d=2$, CFG  generates a distribution with larger magnitude mean (dashed vertical line) and smaller variance. This effect diminishes when the dimension increases: for $d=200$ it is practically absent. 
    } 
    \labelonehist
    \end{figure}
\else 
    \begin{figure}[t]
        \centering
        \hspace{-0.4cm}
        \includegraphics[width=0.485\textwidth]{assets/plots/1_histograms.pdf}
    \caption{\textbf{CFG  only overshoots in low dimensions.} Model:  mixture of two Gaussians (only one component shown) with dimension $d=2$ (left) and $d=200$ (right), centered in $\pm \vec m$ and variance $\sigma^2=1$,  $\omega\in\{0, 0.2, 15\}$, see \Cref{eqn:gm_0}. We show the histograms of $q(t=0)=\vec{x}\cdot\vec{m}/|\vec{m}|$, the generated samples projected onto the normalized mean vector $\vec{m}$ obtained with backward diffusion with 10,000 trajectories. 
    The guidance strength is given by $\omega$, and $\omega=0$ corresponds to sampling without CFG.
    For $d=2$, CFG  generates a distribution with larger magnitude mean (dashed vertical line) and smaller variance. This effect diminishes when the dimension increases: for $d=200$ it is practically absent. 
    } 
    \labelonehist
    \end{figure}
\fi





\section{Related Work} 

Classifier-Free Guidance (CFG) has been a topic of interest in recent research, with the original work introducing CFG \citep{ho2022classifier} highlighting the trade-off between image quality, measured by Fréchet inception distance (FID, \citet{heusel2017gans}), and diversity, measured by inception score \citep{salimans2016improved} when adjusting the guidance strength parameter $\omega$. Since then, a significant body of research has examined CFG from various perspectives.

\paragraph{Theoretical works on CFG.} From a theoretical standpoint, several works also employed Gaussian mixture models to analyze diffusion and guidance, including \citet{chidambaram2024does, shah2023learning, liang2024unraveling, cui2023analysis, bai2024expectation}. In contrast, \citet{du2023reduce} explored alternative approaches to conditioning, modifying, and reusing diffusion models for compositional generation and guidance tasks. \citet{bradley2024classifier} characterized CFG as a predictor-corrector \citep{song2020score}, positioning it within a broader context of sampling approaches in order to improve its theoretical understanding, similar to the approach in this paper, however from the perspective of denoising and sharpening processes.


\paragraph{CFG variants and experimental analyses.} Among experimental evaluations of CFG, \citet{karras2024guiding} showed that guiding generation using a smaller, less-trained version of the model itself can achieve disentangled control over image quality without compromising variation. \citet{kynkaanniemi2024applying} proposed applying CFG in a limited interval, and \citet{wang2024analysis} proposed using weight schedulers for the classifier strength parameter. Several alternatives to standard CFG have been proposed, such as rectified classifier guidance \citep{xia2024rectified} using pre-computed guidance coefficients, projected score guidance \citep{kadkhodaie2024feature} pushing the image feature vector toward a feature centroid of the target class, characteristic guidance \citep{zheng2023characteristic} as a non-linear correction of CFG obtained using numerical solvers, and second-order CFG \citep{sun2023inner} assuming locally-cone shaped condition space. All these variants can be directly combined with our proposed generalization of CFG.


\looseness=-1\paragraph{Dynamical regimes, statistical physics and high-dimensional settings.} Akin to this work, several works recently studied dynamical regimes of diffusion models, primarily focusing on the standard, non-CFG version \citep{biroli2023generative,raya2024spontaneous,biroli2024dynamical,sclocchi2024phase,yu2024nonequilbrium,li2024critical,aranguri2025optimizing}. 
Statistical physics methods have been particularly useful in analyzing high-dimensional settings, e.g., data drawn from the Curie-Weiss model \citep{biroli2023generative}, high-dimensional Gaussian mixtures \citep{biroli2024dynamical}, and hierarchical models \citep{sclocchi2024phase}. 
Other relevant statistical-physics studies include \citet{ghio2024sampling}, who provided a comprehensive theoretical comparison between flow, diffusion, and autoregressive models from a spin glass perspective; \citet{achilli2024losing}, who extended the theory of memorization in generative diffusion to manifold-supported data; \citet{cui2025precise,cui2023analysis}, who analyzed sample complexity for 
high-dimensional Gaussian mixtures. A rigorous formulation of diffusion models in infinite dimensional setting was developed by \citet{pidstrigach2023infinitedimensionaldiffusionmodels}.  
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Background and High-Level Discussion}
\label{sec:2}


\subsection{General Setup} 
We begin by providing an overview of the standard framework for generative diffusion, serving as the foundation for our analysis. We let $\{\vec{a}_i\}_{i=1}^n \in \mathbb{R}^d$ represent $n$ independent data points sampled from the \textit{true} underlying data distribution $P_0(\vec{a})$ that we aim to model. 

The forward diffusion process, starting from the data points $\{\vec{a}_i\}_{i=1}^n$, is modeled by an Ornstein-Uhlenbeck process, described by the following stochastic differential equation:

\begin{equation}
    d \vec{x}(t) = f(t)\vec{x}(t) \, dt + g(t) \, d \vec{B}(t),
\label{eqn:original_ou_process}
\end{equation}

where $d \vec{B}(t)$ denotes the standard Wiener process (also known as Brownian motion) in $\mathbb{R}^d$. At any given time $t$, the state $\vec{x}(t)$ is distributed according to a Gaussian distribution with mean $s(t) \vec{a} $ and variance $s(t)^2\sigma(t)^2$, where $s(t)$ and $\sigma(t)$ are related to the functions $f(t)$ and $g(t)$ of \Cref{eqn:original_ou_process} by 
$s(t)=\exp\int _0^t d\tau f(\tau)$ and $\sigma(t)=\int_0^t d\tau g(\tau)^2/s(\tau)^2$. The forward process is terminated at time $t_f \gg 1$, when $\vec{x}(t_f)$ is effectively pure Gaussian noise, distributed as $\mathcal{N}(0, s(t_f)^2\sigma(t_f)^2\mathcal{I}_d)$, with $\mathcal{I}_d$ being the identity matrix in $\mathbb{R}^d$. 
Note that for $\{\vec{a}_i\}_{i=1}^n$ drawn from $P_0$, the distribution $P_t(\vec{x})$ at time $t$ is the convolution of the original distribution $P_0$ with a Gaussian kernel.


\looseness=-1The backward diffusion process operates in reverse time. Denoting reverse time variable by $\tau = t_f - t$, we can describe the process by the following stochastic equation:
\begin{align}
    d \vec{x}(\tau) = -f(\tau)\vec{x} \, d\tau + g(\tau)^2 \vec S(\vec{x}, \tau) \, d\tau + g(\tau) \, d \vec{B}(\tau),
\label{eqn:backw_dyn}
\end{align}
where $\vec S(\vec x,\tau)=\vec\nabla \log P_\tau(\vec x)$ denotes the score function. The backward diffusion process generates points $\vec{x}$ sampled from the distribution $P_t(\vec{x})$ for every time step $\tau$. At the end of the backward process, i.e., when $\tau = t_f$, the process generates points drawn from the original distribution $P_0$.




In this work, we focus on generating data that can be categorized into distinct classes. We begin by assuming that the underlying data distribution is a $d$-dimensional probability distribution $P_0(\vec{x}, c)$, where $c$ represents a discrete class index and $\vec{x}$ a $d$-dimensional vector. The aim is to generate data conditioned on $c$, the class label.  The procedure that is mathematically guaranteed to generate the correct (conditioned) target distribution consists of using the true conditional score, $\vec S_t(\vec x,c)=\vec\nabla \log P_t(\vec x|c)$ in  \Cref{eqn:backw_dyn}. CFG, however,  does not do that; it instead further directs diffusion in a manner proportional to the difference between conditional and unconditional scores:
\begin{align}
S_t^{\text{CFG}}(\vec{x}, c) = (1 + \omega) S_t(\vec{x}, c) - \omega S_t(\vec{x}),
\label{eqn:score_CFG_linear}
\end{align}
where $\omega$ denotes the guidance strength parameter. 
%In order to understand this disconnect, we first analyze the behavior of CFG in the limit of large dimensions.



\newcommand{\labeltwodiffhist}{\label{fig:2_diffusion_regimes}}

\ifarxiv
    \begin{figure}[t]
        \centering

        \begin{NiceTabular}{cc}
            \hspace{-0.65cm}
            \includegraphics[width=0.562\columnwidth]{assets/plots/0_diffusion_regimes_and_pot_A.pdf} &
            \hspace{-0.6cm}
            \includegraphics[width=0.32\columnwidth]{assets/plots/0_diffusion_regimes_and_pot_B.pdf} 
        \end{NiceTabular}
        \caption{\textbf{Dynamical regimes in diffusion models.} \textbf{Left:} Illustration of the speciation phenomenon using a one-dimensional Gaussian mixture. Starting from pure Gaussian noise at large time $t$, the backward diffusion begins in Regime \rom{1}, where the class has not been decided yet. After speciation time $t_s$ (dashed line), the class membership is decided (speciation is a cross-over in $d=1$, it becomes sharp for large $d$). \textbf{Right:} time evolution of the effective potential for high-dimensional Gaussian mixture showcasing the symmetry breaking phenomenon.
        }
        \labeltwodiffhist
    \end{figure}
\else 
    \begin{figure}[t]
        \centering

        \begin{NiceTabular}{cc}
            \hspace{-0.65cm}
            \includegraphics[width=0.68\columnwidth]{assets/plots/0_diffusion_regimes_and_pot_A.pdf} &
            \hspace{-0.6cm}
            \includegraphics[width=0.39\columnwidth]{assets/plots/0_diffusion_regimes_and_pot_B.pdf} 
        \end{NiceTabular}
        \caption{\textbf{Dynamical regimes in diffusion models.} \textbf{Left:} Illustration of the speciation phenomenon using a one-dimensional Gaussian mixture. Starting from pure Gaussian noise at large time $t$, the backward diffusion begins in Regime \rom{1}, where the class has not been decided yet. After speciation time $t_s$ (dashed line), the class membership is decided (speciation is a cross-over in $d=1$, it becomes sharp for large $d$). \textbf{Right:} time evolution of the effective potential for high-dimensional Gaussian mixture showcasing the symmetry breaking phenomenon.
        }
        \labeltwodiffhist
    \end{figure}
\fi





\subsection{Classifier-Free Guidance Works in High Dimension} 

\paragraph{CFG and distinct dynamical regimes.} 

\looseness=-1\citet{biroli2023generative} and \citet{biroli2024dynamical} analyze the dynamical regimes of the backward generative process in \Cref{eqn:backw_dyn} for the case of two classes as $d\to\infty$ through the lens of statistical physics. They identify two distinct regimes\footnote{Note that if the perfect empirical score is used, there exists a third regime, Regime \rom{3}, characterized by \textit{collapse},  i.e.,\ memorization of the training set, but we shall not discuss it here as it is not relevant to CFG.} in the time-reversed process: 
in Regime \rom{1}, the random dynamical trajectories have not yet committed to a particular class of data.
In  Regime \rom{2}, the trajectories have committed to a class and generate the features necessary to produce samples from that class. This distinction is illustrated on a toy example in \Cref{fig:2_diffusion_regimes}. 
The transition between the two distinct dynamical regimes occurs at a crossover point called the \emph{speciation time}, denoted $t_\textrm{s}$. 
%For backward times after $t_\textrm{s}$, i.e., $\tau\gg t_f-t_\textrm{s}$, the trajectories are committed to one of the classes. 
Additionally, in cases with more structured data with several classes and subclasses, multiple speciation times can exist, e.g., see \citet{li2024critical,sclocchi2024phase}. The emergence of these two dynamical regimes is related to a concentration phenomenon. In Regime \rom{2}, the probability $P_t(\vec x)$ consists of separated non-overlapping lumps, related to the class to which the trajectories are heading.  

We first provide a general explanation of CFG in this high-dimensional setting. In the next section, we present a detailed analysis for Gaussian mixtures. 
The unconditional score can be written in terms of the conditional scores as follows:
 \begin{align}
    \vec{S}_t(\vec{x})=\vec \nabla \log P_t(\vec x)= \frac{\sum_{c=1}^K P_t(\vec x|c)p(c)\; \vec{S}_t(\vec{x}, c)}{\sum_{c=1}^K P_t(\vec x|c)p(c)},
     \label{eq:basicCFG}
 \end{align}
 where $p(c)$ is the probability of class $c$ in the original data (we assume the presence of $K$ classes). 

In high-dimensional settings, the speciation phenomenon ensures that once the backward process enters Regime \rom{2}, any sample  $\vec{x}$ drawn from $P_t(\vec{x})$  will almost surely belong to a single, well-defined class  $c$.
 If $\vec x$ is in the lump associated with class $c$, then $ \sum_{c'=1}^K P_t(\vec x|c')p(c')\rightarrow P_t(\vec x|c)p(c) $. \Cref{eq:basicCFG} then implies that the full score equals the one computed in the class $c$, \ie, $\vec{S}_t(\vec{x},c)=\vec{S}_t(\vec{x})$, 
 and the additional guidance term introduced by CFG vanishes. Hence,
CFG has no effect in Regime \rom{2}. In contrast, during Regime \rom{1}, before speciation occurs, CFG steers the dynamics toward the cluster $c$ faster. 
However, this does not compromise the final sample quality, since the actual class-conditioned generation occurs in Regime \rom{2}, where CFG no longer alters the distribution.

\looseness=-1In conclusion, the existence of two regimes for high-dimensional data, Regime \rom{1} in which the trajectory chooses the class, and Regime \rom{2} in which the generation of the conditioned data takes place, offers a theoretical justification of the ability of CFG to reproduce the correct target distribution.

\section{CFG in the High-Dimensional Limit of Gaussian Mixtures}
\label{sec:gauss_mixt}


Here, we demonstrate the applicability of the general argument presented in Section \ref{sec:2} in a concrete case corresponding to high-dimensional Gaussian mixtures. We describe here the main findings, while the detailed analysis is provided in App.~\ref{sec:appx_gm}. Specifically, we examine the case in which  $P_0(\vec{a})$ represents a superposition of two Gaussians, each carrying equal weight, with means at $\pm \vec{m}$ and shared isotropic variance $\sigma^2$. To ensure that the two Gaussian clusters remain well separated in the limit of high dimensionality, we take the large $d$ limit with fixed values of $|\vec{m}|^2 / d $ and  $\sigma$. We also assume that the score is correctly estimated. 
The extension to more than two Gaussian is straightforward and discussed in App. \ref{sec:appx_4mg}. To simplify the notation, we focus on the 
SDE forward equation corresponding to $f(t)=-1$ and $g(t)=\sqrt{2}$ (see \Cref{eqn:original_ou_process}). All results can be easily translated for general choices of $f(t),g(t)$.  

In this setting, the speciation transition resembles a symmetry-breaking phenomenon, similar to what occurs in thermodynamic phase transitions. \citet{biroli2024dynamical} investigated this phenomenon by analyzing the backward dynamics and determined the speciation timescale as $ t_\textrm{s} = \frac{1}{2} \log(d) $. In their analysis, $ t_\textrm{s}$ emerges as the timescale at which diffusion paths commit to a specific category. This is related to a change in the form of the potential in the backward Langevin equation, see  \Cref{fig:2_diffusion_regimes} (right). 
Applying this framework we find that {\it{the speciation time aligns precisely with the time until which CFG is effective}} in aiding class selection. Beyond this point, as the trajectory has committed to a class, CFG no longer influences the generated outcome.


\subsection{Deriving the CFG Score}

The distribution of the points at time $t$, $P_t(\vec{x})$, is the convolution of the initial distribution $ P_0 $ and a diffusion kernel proportional to $ e^{-\left(\vec{x} - \vec{a} e^{-t}\right)^2 / (2 \Delta_t)}$:
\ifarxiv
 \begin{align*}
    P_t(\vec{x})=\frac1{2\sqrt{2 \pi \Gamma_t}^d}\left[e^{-\left(\vec{x}-\vec{m} e^{-t}\right)^2 /\left(2 \Gamma_t\right)}+e^{-\left(\vec{x}+\vec{m} e^{-t}\right)^2 /\left(2 \Gamma_t\right)}\right],
\end{align*}
where
\else
\begin{align*}
    P_t(\vec{x})=c_n\left[e^{-\left(\vec{x}-\vec{m} e^{-t}\right)^2 /\left(2 \Gamma_t\right)}+e^{-\left(\vec{x}+\vec{m} e^{-t}\right)^2 /\left(2 \Gamma_t\right)}\right],
\end{align*}
where $c_n^{-1}=2\sqrt{2 \pi \Gamma_t}^d$ and 
\fi
$ \Gamma_t = \sigma^2 e^{-2t} + \Delta_t=1+(\sigma^2-1)e^{-2t}$, which approaches 1 as $ t $ becomes large.  In this case, the classifier-free guidance formula in \Cref{eqn:score_CFG_linear}, with details given in App. \ref{sec:appx_sec_A2}, can be rewritten as:
\newcommand{\labeleqncfgscore}{\label{eqn:cfg_score_form}}
\ifarxiv
    \begin{align}
        S_t^{\text{CFG}}(\vec{x},c) = -\frac{\vec{x}}{\Gamma_t}  + \frac{c \vec{m} e^{t}}{\Gamma_t}+ \omega\frac{\vec m e^{-t}}{\Gamma_t}\left\{c-\tanh{\left(\frac{\vec{x}\cdot\vec{m}e^{-t}}{\Gamma_t}\right)}\right\},
        \labeleqncfgscore
    \end{align}
\else
    \begin{align}
        S_t^{\text{CFG}}(\vec{x},c) &= -\frac{\vec{x}}{\Gamma_t}  + \frac{c \vec{m} e^{t}}{\Gamma_t}+ \nonumber\\
        &+ \omega\frac{\vec m e^{-t}}{\Gamma_t}\left\{c-\tanh{\left(\frac{\vec{x}\cdot\vec{m}e^{-t}}{\Gamma_t}\right)}\right\},
        \labeleqncfgscore
    \end{align}
\fi
with $c=\pm1$ and $\omega>0$.


\subsection{CFG Provides an Additional Push Toward the Desired Class in Regime \rom{1}}

We can analyze the score in \Cref{eqn:cfg_score_form} to identify which directions of the backward process are affected by CFG. Following \citet{biroli2024dynamical}, we first focus on Regime \rom{1}, taking place on time-scales  $t_\textrm{s}+O(1)=(1/2)\log d +O(1)$. 
 \Cref{eqn:cfg_score_form} shows that the CFG-dependent part of the score is aligned on the $\vec m$ direction, therefore CFG has no effect on the ``transverse'' directions $\vec{v}\perp\vec{m}$.
The projection of the backward \Cref{eqn:backw_dyn} on a unit vector in the space orthogonal to $\vec m$ follows the equation $dp=p (1-2/\Gamma_{t_f-\tau})d\tau +\sqrt{2} dB $. This is the backward equation for an initial Gaussian $\mathcal{N}(0, \sigma^2)$, thus explicitly showing that all components of $\vec{x}$ orthogonal to $\vec m$ are unaffected by CFG and distributed as in the target distribution for $\tau\to t_f$. 
  

However, when projecting onto $\vec{m}$  we observe that the CFG score $\vec{S}_t^{\text{CFG}}$ influences the component along $\vec m$. Defining $q(t) := \frac{\vec{x} \cdot \vec{m}}{|\vec{m}|}$ where $|\vec{m}| = \sqrt{d}$, the evolution of the backward system in \Cref{eqn:backw_dyn} guided to class $c=1$ is given by the following:
\newcommand{\labelcfgeqn}{\label{cfg:eqn}}
\ifarxiv
    \begin{equation}
        \begin{aligned}
            d q = 
            \Big(q + 2\Big[-q+ e^{-(t_f-t_\textrm{s}-\tau)} \Big((1+\omega) - \omega\tanh{\left(qe^{-(t_f-t_\textrm{s}-\tau)}\right)}\Big)\Big]\Big)d\tau+d\eta(\tau),
        \end{aligned}
        \labelcfgeqn
    \end{equation}

\else
    \begin{equation}
        \begin{aligned}
            d q &= 
            \Big(q + 2\Big[-q+ e^{-(t_f-t_\textrm{s}-\tau)} \Big((1+\omega) \\
            &\quad - \omega\tanh{\left(qe^{-(t_f-t_\textrm{s}-\tau)}\right)}\Big)\Big]\Big)d\tau+d\eta(\tau),
        \end{aligned}
        \labelcfgeqn
    \end{equation}
\fi

where again $\tau=t_{f}-t$, with $t_\textrm{s}=(1/2)\log d$, $\eta(\tau)$ is the square root of two times a Brownian motion, and we used the fact that in Regime \rom{1} we have $\Gamma_t\simeq 1$. 
From now onward, we omit the dependency $t(\tau)$ for backward time and simply use the notation $t$ to avoid clutter and keep the notation simple.

\newcommand{\labelfourpotentials}{\label{fig:4_potentials}}

\ifarxiv
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.7\textwidth]{assets/plots/potentials_2.pdf}
        \vspace{-.5cm}
        \caption{\textbf{Effect of CFG on the guiding potential of a Gaussian mixture.} The backward diffusion for the variable $q$ giving the projection of $\vec x$ on the center $\vec m$ of the Gaussian where one wants to guide the backward diffusion. From left to right: Potential within the class, CFG-added-potential $V_\text{extra}$ with $\omega=2$, and their sum as in \Cref{eqn:eff_pot}. CFG exhibits faster convergence to the target ($t=0$), but results in narrower potential for small $t$ (with $t$ ranging from 0 to 8, as indicated on the right panel).
        } 
        \labelfourpotentials
    \end{figure}
\else 
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.5\textwidth]{assets/plots/potentials_2.pdf}
        \caption{\textbf{Effect of CFG on the guiding potential of a Gaussian mixture.} The backward diffusion for the variable $q$ giving the projection of $\vec x$ on the center $\vec m$ of the Gaussian where one wants to guide the backward diffusion. From left to right: Potential within the class, CFG-added-potential $V_\text{extra}$ with $\omega=2$, and their sum as in \Cref{eqn:eff_pot}. CFG exhibits faster convergence to the target ($t=0$), but results in narrower potential for small $t$ (with $t$ ranging from 0 to 8, as indicated on the right panel).
        } 
        \labelfourpotentials
    \end{figure}
\fi
  
\Cref{cfg:eqn} can be rewritten as: $d q  = -\pdv{V^\textrm{CFG}(q,\tau)}{q} d \tau+d \eta(\tau)$, where the effective potentials has two contributions:

\newcommand{\labeleffpot}{\label{eqn:eff_pot}}

\ifarxiv
    \begin{align}
        V^\textrm{CFG} = \underbrace{\left(\frac1{2}q^2-2e^{-(t-t_\textrm{s})}q\right)}_\text{Classifier's potential} +\ \omega\ \underbrace{\left[-qe^{-(t-t_\textrm{s})}+\ln\cosh{\left(qe^{-(t-t_\textrm{s})}\right)}\right]}_\text{Extra potential $V_{\text{extra}}$}. 
        \labeleffpot
    \end{align}
\else
    \begin{align}
        V^\textrm{CFG} &= \underbrace{\left(\frac1{2}q^2-2e^{-(t-t_\textrm{s})}q\right)}_\text{Classifier's potential} \nonumber \\
        &+\omega\underbrace{\left[-qe^{-(t-t_\textrm{s})}+\ln\cosh{\left(qe^{-(t-t_\textrm{s})}\right)}\right]}_\text{Extra potential $V_{\text{extra}}$}. 
        \labeleffpot
    \end{align}
\fi

From \Cref{eqn:eff_pot}, we observe that the effect of CFG is adding to the classifier potential an extra contribution, plotted in \Cref{fig:4_potentials}. The extra potential adds an additional push toward the positive values of $q$, the ones corresponding to the class $c=1$. As shown from the figure, the effect of CFG is particularly strong for trajectories that deviate from the typical behavior and correspond to values of $q$ associated to the wrong class (in the case $c=1$, negative values of $q$). For large $\tau$, when the dynamics is exiting Regime \rom{1}, the value of $qe^{-(t-t_\textrm{s})} $ becomes very large and the extra potential vanishes, thus showing that the effect of CFG fades out. 


\newcommand{\labelintrotrajec}{\label{fig:intro_traject}}

\ifarxiv
    \begin{wrapfigure}{r}{0.32\columnwidth}
        \vspace{-.5cm}
        \includegraphics[width=\linewidth]{assets/plots/intro_traject.pdf}
        \caption{\looseness=-1
            {\bf Evolution of the mean  of $\bm{q(t)}$.}
            It starts from $0$ at large forward time $t$, and is pushed towards positive values (cluster centered in $+\vec m$) when $t$ decreases. It can be observed that CFG is effective when $t\gtrsim t_s$ (vertical line) and stronger for larger values of $\omega$. In Regime \rom{2}, the trajectories of $q$ for various values of $\omega$ merge.
        }
        \labelintrotrajec
        \vspace{-.75cm}
    \end{wrapfigure}
\else
\begin{figure}[h!]
    \begin{minipage}{0.47\columnwidth}
        \includegraphics[width=\textwidth]{assets/plots/intro_traject.pdf}
    \end{minipage}
    \begin{minipage}{0.47\columnwidth}
        \caption{
        {\bf Evolution of the mean  of $\bm{q(t)}$.}
        It starts from $0$ at large $t$, and is pushed towards positive values (cluster centered in $+\vec m$) when $t$ decreases. CFG is effective when $t\gtrsim t_s$ (vertical line) and stronger the larger $\omega$. In Regime \rom{2}, the trajectories of $q$ for various values of $\omega$ merge. }
        \labelintrotrajec
    \end{minipage}
    \end{figure}
\fi



The dynamical effect of CFG can be seen in \Cref{fig:intro_traject}, where we simulate \Cref{cfg:eqn} for $t_s=1000, \sigma=1$, averaging over 10,000 trajectories. In agreement with the previous arguments, this figure shows that CFG 
pushes $\vec x$ faster toward the desired class (higher values of $q$ for $c=1$). Large $\omega$  leads to an overshoot, but in the large $d$ limit $q$ then aligns with one of the classes when exiting Regime \rom{1}, contrary to empirical observations made for  $d=1$, e.g., by  \citet{chidambaram2024does}, where the paths never realign.


\subsection{CFG plays no role in Regime \rom{2}}
The variable $q$ diverges at the end of Regime \rom{1} and becomes of order $\sqrt{d}$ in Regime \rom{2} (as explained by \citet{biroli2024dynamical}). One has therefore to focus on the rescaled variable $\vec{x}\cdot\vec{m}/d$. 
At the end of Regime \rom{1}, $q$ has realigned with the value it would have had for $\omega=0$. Hence, the initial condition in Regime \rom{2} (when guiding towards either of the classes, $c=1$ or $c=-1$) is 
$\vec{x}\cdot\vec{m}/d= 0$, independently of $\omega$. 
Moreover, since in Regime  \rom{2}, $ |\vec{x}\cdot\vec{m}| e^{-t}/\Gamma_t$ is  $O(d) $ and $ \sign(\vec{x}\cdot\vec{m})=1$, one finds that the extra CFG term in \Cref{eqn:cfg_score_form} is zero since 
$1-\tanh{(\vec{x}\cdot\vec{m}e^{-t}/\Gamma_t)}\rightarrow 0$ for $d\rightarrow \infty$. Therefore, we are able to conclude that for $t\ll t_\textrm{s}$, for any $\omega$, in the large $d$ limit the score obtained by CFG is equal to the score within the class $S_t^{\text{CFG}} (\vec{x}, c)= S_t(\vec{x})$, thus showing that  CFG has no effect in Regime \rom{2}.   


Furthermore, using that at the time of exit from Regime \rom{1}, $q$ has the same value as for $\omega=0$, the backward dynamics during Regime \rom{2} leads to the correct conditioned target distribution at the end of the backward process for any $\omega$. This effect is shown in \Cref{fig:1_histograms} (right panel) in the case of $d=200$ and in \Cref{fig:intro_traject}.

\subsection{Finite Dimensional Effects: Overshoot of the Mean and Reduction of the Variance}
\label{sec:fin_dim_eff}


So far, we have shown that for any value of $\omega$ the target distribution is correctly reproduced in the infinite-$d$ limit. 
We now consider the changes brought by considering $d$ large but finite. A detailed analysis is presented in App. \ref{sec:appx_fin_dim}; here we present the main results. 


\looseness=-1 The extra CFG term in the score, see \Cref{eqn:cfg_score_form},  is of the same order as the one corresponding to the conditional score within Regime \rom{1} and remains so, even for finite $d$. In contrast, in Regime \rom{2}, the extra CFG term is zero for $d\rightarrow \infty$ and 
exponentially small in $d$ for finite $d$. Thus, for large but finite dimensions, results obtained for $d\rightarrow \infty$ carry over: CFG has a substantial effect only in Regime \rom{1} and, hence, only on the initial condition for the dynamics in  Regime \rom{2}.

The effect of CFG in Regime \rom{1} is to add an extra push toward the desired class. Indeed, the sign of the term introduced by CFG in the score accords with the desired class: in the Langevin equation it gives an extra contribution to the force which is positive for $c\!=\!+1$, and negative for $c\!=\!-1$. In consequence, the generated distribution shows an overshoot corresponding to the target distribution since its mean is larger for $c=+1$, smaller for $c=-1$. The relative amplitude of the overshoot for Regime \rom{2} is of order $1/\sqrt{d}$, and it is due to the role of CFG on the dynamics of Regime \rom{1}. 
The extra term due to CFG has another important effect: it leads to a larger second derivative of the potential $V^\textrm{CFG}(q,t)$, see  \Cref{fig:4_potentials} and  App. \ref{sec:appx_fin_dim}. Thus, the resulting CFG Langevin equation is associated to a potential that is more confining. This explains the numerical observation where we find that the CFG-generated distribution has a smaller variance than the one of the target distribution. The mean overshoot and the decrease in the variance for different guidance strengths are shown in \Cref{fig:switchomega} in App. \ref{sec:appx_fin_dim}.

\looseness=-1Two main conclusions result from this analysis. First, the error due to CFG for finite $d$ agrees well with what was found in numerical experiments and in analysis of very low dimensional cases \citep{wu2024theoretical, chidambaram2024does, xia2024rectified, bradley2024classifier}.
Second, since CFG has no role in Regime \rom{2}, as was implicitly observed by \citet{kynkaanniemi2024applying}\footnote{ \citet{kynkaanniemi2024applying} observe in Fig.~5 (right) of their paper that stopping guidance after noise level $\sigma_{low}\approx 0.85$ has no effect on image quality, measured by Fréchet inception distance.}, and causes an overshoot when it is kept too long in Regime \rom{1}, it would be desirable to switch it off before $t_\textrm{s}$, as shown 
for high-dimensional Gaussian mixtures in  \Cref{fig:switchomegamean} in App. \ref{sec:appx_fin_dim}.
In the following section, we propose a generalization of CFG that automatically switches off in Regime \rom{2}, showing in practice it can yield better quality, higher diversity and faster convergence to the target distribution, while being less computationally expensive. 


\newcommand{\labelfigsix}{\label{fig:6_cfg_score_diff}}
\ifarxiv
    \begin{figure}[t]
        \centering
        \begin{minipage}{0.6\columnwidth}
            \includegraphics[width=.49\textwidth]{assets/plots/gauss_mixt.pdf}
            \includegraphics[width=.49\textwidth]{assets/plots/score_evolution.pdf}
        \end{minipage}
        \caption[Caption for LOF]{\looseness=-1\textbf{Left:} Numerically simulating mixture of two, four, and eight Gaussians with equidistant means on a sphere ($r=\sqrt{d}$), with varying dimension $d$, with $\omega=4, \sigma^2=1$, averaged over 10,000 trajectories. As $d$ increases, the score difference is substantial on earlier backward times $\tau$ (in Regime \rom{1}). Additionally, as the number of classes increases, the magnitude of the score difference grows, as well as the duration of large difference between the scores. \textbf{Right:} We replicated the same experiment using  class-conditional (DiT/XL-2 and EDM2-S) and text-to-image (MMDiT and MDTv2) diffusion models. For presentation clarity, we normalized x-axis timesteps and y-axis score difference to range $[0,1]$. We observed a consistent pattern with theory: monotonically increasing score difference followed by decay after a certain point.}
        \labelfigsix
    \end{figure}
\else
    \begin{figure}[t]
        \begin{minipage}{\columnwidth}
            \includegraphics[width=.485\textwidth]{assets/plots/gauss_mixt.pdf}
            \includegraphics[width=.485\textwidth]{assets/plots/score_evolution.pdf}
        \end{minipage}
        \caption[Caption for LOF]{\textbf{Left:} Numerically simulating mixture of two, four, and eight Gaussians with equidistant means on a sphere ($r=\sqrt{d}$), with varying dimension $d$, with $\omega=4, \sigma^2=1$, averaged over 10,000 trajectories. As $d$ increases, the score difference is substantial on earlier backward times $\tau$ (in Regime \rom{1}). Additionally, as the number of classes increases, the magnitude of the score difference grows, as well as the duration of large difference between the scores. \textbf{Right:} We replicated the same experiment using  class-conditional (DiT/XL-2 and EDM2-S) and text-to-image (MMDiT and MDTv2) diffusion models. For clarity, we normalized x-axis timesteps and y-axis score difference to range $[0,1]$. We observed a consistent pattern with theory: monotonically increasing score difference followed by decay after a certain point.}
        \labelfigsix
    \end{figure}
\fi


\section{More General Guidance Forms}
\label{sec:non_lin_class_guid}


\subsection{Non-Linear Classifier-Free Guidance}
\label{sec:nlg}
The blessing of dimensionality that makes CFG work in high-dimensions is due to two main properties: (1) CFG acts in Regime \rom{1} pushing further in the direction of the desired class, (2) CFG does not play any role in Regime \rom{2} where the detailed property of the data are generated. 

This observation makes clear that CFG can be generalized to encompass other guidance schemes that still enjoy the two properties cited above.  With this idea in mind, we propose non-linear variants of the CFG score of the type:
\newcommand{\lalbeleqnnonlin}{\label{eqn:nonlin_cfg}}
\ifarxiv
    \begin{flalign}
        \lalbeleqnnonlin
        S_t^{\textrm{CFG-NL}}&(\vec{x},c)= S_t(\vec{x}, c) + \left[S_t(\vec{x}, c) -  S_t(\vec{x})\right] \phi_t \left(\left|\vec{S}_t(\vec{x}, c) -  \vec{S}_t(\vec{x})\right|\right).
    \end{flalign}
\else
    \begin{flalign}
        \lalbeleqnnonlin
        S_t^{\textrm{CFG-NL}}&(\vec{x},c)= S_t(\vec{x}, c) 
        &&  \\
        &+ \left[S_t(\vec{x}, c) -  S_t(\vec{x})\right] \phi_t \left(\left|\vec{S}_t(\vec{x}, c) -  \vec{S}_t(\vec{x})\right|\right).&& \nonumber
    \end{flalign}
\fi
\looseness=-1For constant $\phi_t(s)=\omega$, this reduces to standard CFG.
As long as the function  $\phi_t(s)$ satisfies $\lim_{s\to 0} \left[s \phi_t(s)\right]{}=0$, we know that in Regime \rom{2} the extra contribution to the score due to $\phi_t$ vanishes, thus leading to a correct target distribution in high-dimension. The freedom in the choice of $\phi_t$ can be used to improve the effect of CFG in Regime \rom{1}, helping to push the system in the direction of class $c$, and reducing the unwanted finite dimensional drawbacks leading to mean overshoot and reduced variance. In the following, as a proof of principle, we propose a first candidate for $\phi_t$. As we shall show, this choice already allows to improve very efficient generative models. In the long run, we envisage that the whole function $\phi_t$ can be optimized as hyper-parameters. 

 
 
 \subsection{Power-Law CFG}
 We can choose $\phi_t(s)=\omega s^{-\alpha}$ with $\alpha<1$ to obtain the following guidance scheme:
\ifarxiv
    \begin{align*}
        \vec S_{t}^\textrm{PL}(\vec{x},c)= S_t(\vec{x}, c)+ \omega \left[S_t(\vec{x}, c) -  S_t(\vec{x})\right] \left|\vec{S}_t(\vec{x}, c) -  \vec{S}_t(\vec{x})\right|^{-\alpha}. 
    \end{align*}
\else
    \begin{align*}
        \vec S_{t}^\textrm{PL}(\vec{x},c)&= S_t(\vec{x}, c)  \\
        &+ \omega \left[S_t(\vec{x}, c) -  S_t(\vec{x})\right] \left|\vec{S}_t(\vec{x}, c) -  \vec{S}_t(\vec{x})\right|^{-\alpha}. 
        \nonumber
    \end{align*}
\fi
One can understand the effect of this non-linear guidance as follows. The $\ell_2$ distance between scores $\delta S =|\vec{S}_t(\vec{x}, c) -  \vec{S}_t(\vec{x})|$ vanishes as $e^{-t}$ at large (forward) times, as all the forward trajectories starting from an initial point $\vec a$ build at time $t$ a Gaussian cloud. Therefore w.r.t.\ the standard CFG in \Cref{eqn:score_CFG_linear}, this new score acts with an effective guidance strength which changes $\omega$ to $ \omega [\delta S]^{-\alpha}$, making it stronger at the beginning of the backward process. Then, when the backward process gets closer to the speciation time, $\delta S$ decreases, the non-linear strength becomes weaker, and it vanishes in Regime \rom{2}\footnote{For computational reasons, one could stop applying guidance once the score difference becomes sufficiently small.}, as displayed in \Cref{fig:11_3_versions} in App. \ref{sec:appx_related_work}.
 
 \begin{figure*}[t]
    \centering
      \begin{NiceTabular}{ccc}
          \Block[borders={right}]{1-1}{}
          \begin{minipage}[b]{0.46\textwidth}
            \centering
            \vspace{-0.5cm}
            {\footnotesize Improved image quality}
            \includegraphics[width=\textwidth]{assets/plots/plots_1a.pdf}
          \end{minipage} &
          \begin{minipage}[b]{0.46\textwidth}
            \centering
            {\small Higher sample diversity}
            \includegraphics[width=\textwidth]{assets/plots/plots_1.pdf}
          \end{minipage} &
      \end{NiceTabular}
    \caption{Qualitative comparison between Standard CFG ($\omega=4$) and Power-Law CFG ($\omega=4, \alpha=0.1$) on DiT/XL-2 trained on ImageNet-1K at  256$\times$256 resolution. \textbf{Left panel:} Each paired image starts from same initial noise with standard (left) and Power-Law CFG (right). Power-Law often exhibits improved details which might contribute to the improved FID. \textbf{Right panel:} For three classes, starting from same seeds (0, 1 and 2), Power-Law CFG generates images with higher diversity. Further qualitative examples are provided in App. \ref{appx:ex_by_dit}.}
    \label{fig:qualitative_expls}
\end{figure*}


\looseness=-1\paragraph{Additional Non-Linear CFG forms.}
Another valid choice is $\phi_t(s) = \omega \cdot \mathbb{I}_{[t_1,t_2)}(t)$ where $\mathbb{I}$ denotes the indicator function, applying CFG only at $t\in[t_1,t_2)$, as examined by \citet{kynkaanniemi2024applying} and  
detailed in App. \ref{sec:appx_stepwise_cfg}. 
The authors perform a grid search for hyperparameters $t_1$ and $t_2$, but this can be costly. Instead, we propose a heuristic that performs almost nearly as well: estimating $t_1$ as the first backward time when $\phi_t(s)$ exceeds its mean, and $t_2$ as the timestep of the mode of $\phi_t(s)$, estimated through a few warm-up sampling runs. Another effective choice for $\phi_t(s)=\omega_t$ relies only on weight schedulers for the guidance parameter (disregarding the score difference), as examined by \citet{wang2024analysis}. 

\ifarxiv
\looseness=-1We emphasize that the non-linear CFG and the rest of our framework, although introduced for backward SDEs, can be applied to Flow Matching \citep{lipman2022flow} and Stochastic Interpolants \citep{albergo2023stochastic}.
\else
We emphasize that the non-linear guidance and the rest of our framework, although introduced for a backward SDE process, can be applied to Flow Matching \citep{lipman2022flow} and Stochastic Interpolants \citep{albergo2023stochastic}.
\fi

\newcommand{\labeltableres}{\label{fig:table_comparison}}
\ifarxiv
\begin{table}[t]
    \caption[Caption for LOF]
    {\textbf{Quantitative results.} 
    Image quality (FID, top block) and sample diversity (Recall, bottom block)  
    across several model architectures and datasets.
    Compared to Standard CFG and the powered cosine schedule (first and second row respectively), Power-Law CFG (\textit{NL-Stand.}) improves both metrics.
    %
    When combined with limited guidance scheme using the proposed heuristic (\textit{NL-Lim.v2}), performance further improves. Column \textit{\#p.} indicates the number of hyperparameters, '-FM' denotes flow matching training objective, an asterisk '*' denotes full guidance outperformed limited interval performance. Experimental details are provided in App. \ref{sec:appx_real_world}.}    
    \vskip 0.15in
    \centering
    \resizebox{0.75\columnwidth}{!}{%
        \begin{tabular}{c|c|c|ccccc}
            \toprule 
            Metric & \textit{Type} & \#p. & \textbf{DiT/XL-2} & \textbf{EDM2-S} & \textbf{MMDiT} & \textbf{MDTv2} & \textbf{MMDiT-FM} \\
            & & & \small\textit{IM256} & \small\textit{IM512} & \small\textit{CC12M} & \small\textit{CC12M} & \small\textit{COCO} \\                                
            \midrule
            \multirow{6}{*}{FID $(\downarrow)$} 
            & Standard     & 1 & 2.24 & 2.23 & 8.6  & 8.5 & 5.2 \\
            & Scheduler  & 2 & 2.14 & 1.95 & \textbf{8.3}   & 8.2 & 5.0 \\
            & NL-Stand.  & 2 & \textbf{2.09}  & \textbf{1.86}  & \textbf{8.3}  & \textbf{8.1} & \textbf{4.9} \\
            \cmidrule{2-8}
            & Limited    & 3 & \textbf{1.97} & 1.69 & 8.58  & \textbf{8.5*} & \textbf{5.0} \\
            & Lim. v2    & 1 & 2.01  & 1.7  & 8.65  & 8.8  & 5.1 \\
            & NL-Lim.v2  & 2 & 2.00  & \textbf{1.67}  & \textbf{8.57}  & 8.6  & \textbf{5.0} \\
            \midrule
            \multirow{6}{*}{Recall $(\uparrow)$} 
            & Standard     & 1 & 0.575 & 0.582 & 0.562 & 0.522 & 0.594 \\
            & Scheduler  & 2 & \textbf{0.599} & 0.591 & 0.559  & 0.534 & 0.606 \\
            & NL-Stand.  & 2 & 0.596 & \textbf{0.600} & \textbf{0.565} & \textbf{0.537} & \textbf{0.613} \\
            \cmidrule{2-8}
            & Limited    & 3 & 0.602  & 0.598 & \textbf{0.553} & 0.527 & 0.608 \\
            & Lim. v2    & 1 & 0.600  & 0.602 & 0.549 & 0.524 & 0.601 \\
            & NL-Lim.v2  & 2 & \textbf{0.614}  & \textbf{0.612} & 0.552 & \textbf{0.531} & \textbf{0.615} \\
            \bottomrule
        \end{tabular}
    }
    \labeltableres
\end{table}
\else
\begin{table}[t]
    \caption[Caption for LOF]
    {\textbf{Quantitative results.} 
    Image quality (FID, top block) and sample diversity (Recall, bottom block)  
    across several model architectures and datasets.
    Compared to Standard CFG and the powered cosine schedule (first and second row respectively), Power-Law CFG (\textit{NL-Stand.}) improves both metrics.
    %
    When combined with limited guidance scheme using the proposed heuristic (\textit{NL-Lim.v2}), performance further improves. Column \textit{\#p.} indicates the number of hyperparameters, '-FM' denotes flow matching training objective, an asterisk '*' denotes full guidance outperformed limited interval performance. Experimental details are provided in \Cref{sec:appx_real_world}.}    
    \vskip 0.15in

    \centering
    \begin{minipage}{\columnwidth}
        \centering
        \resizebox{8cm}{!}{%
            \begin{tabular}{c|c|ccccc}
                \toprule 
                \multirow{ 2}{*}{\textit{FID $(\downarrow)$}} & \multirow{ 2}{*}{\#p.} & \textbf{DiT/XL-2} & \textbf{EDM2-S} & \textbf{MMDiT} & \textbf{MDTv2} & \textbf{MMDiT-FM} \\
                 & & \small\textit{IM256} & \small\textit{IM512} & \small\textit{CC12M} & \small\textit{CC12M} & \small\textit{COCO} \\                                
                \midrule
                Standard     & 1 & 2.24 & 2.23 & 8.6  & 8.5 & 5.2 \\
                Scheduler  & 2 & 2.14 & 1.95 & \textbf{8.3}   & 8.2 & 5.0 \\
                NL-Stand.  & 2 & \textbf{2.09}  & \textbf{1.86}  & \textbf{8.3}  & \textbf{8.1} & \textbf{4.9} \\
                \midrule
                Limited    & 3 & \textbf{1.97} & 1.69 & 8.58  & \textbf{8.5*} & \textbf{5.0} \\
                Lim. v2    & 1 & 2.01  & 1.7  & 8.65  & 8.8  & 5.1 \\
                NL-Lim.v2  & 2 & 2.00  & \textbf{1.67}  & \textbf{8.57}  & 8.6  & \textbf{5.0} \\
                \bottomrule
            \end{tabular}
        }
        
        \vspace{10pt} % Add some vertical space between the tables
        
        \resizebox{8cm}{!}{%
            \begin{tabular}{c|c|ccccc}
                \toprule 
                \multirow{ 2}{*}{\textit{Recall $(\uparrow)$}} & \multirow{ 2}{*}{\#p.} & \textbf{DiT/XL-2} & \textbf{EDM2-S} & \textbf{MMDiT} & \textbf{MDTv2} & \textbf{MMDiT-FM} \\
                 & & \small\textit{IM256} & \small\textit{IM512} & \small\textit{CC12M} & \small\textit{CC12M} & \small\textit{COCO} \\                    \midrule
                Standard     & 1 & 0.575 & 0.582 & 0.562 & 0.522 & 0.594 \\
                Scheduler  & 2 & \textbf{0.599} & 0.591 & 0.559  & 0.534 & 0.606 \\
                NL-Stand.  & 2 & 0.596 & \textbf{0.600} & \textbf{0.565} & \textbf{0.537} & \textbf{0.613} \\
                \midrule
                Limited    & 3 & 0.602  & 0.598 & \textbf{0.553} & 0.527 & 0.608 \\
                Lim. v2    & 1 & 0.600  & 0.602 & 0.549 & 0.524 & 0.601 \\
                NL-Lim.v2  & 2 & \textbf{0.614}  & \textbf{0.612} & 0.552 & \textbf{0.531} & \textbf{0.615} \\
                \bottomrule
            \end{tabular}
        }
    \end{minipage}
    \labeltableres
\end{table}
\fi


\subsection{Generative image model experiments}
\label{sec:nonlin}


\looseness=-1\paragraph{Models, architectures and datasets.} We validate our theoretical findings and test the performance of non-linear CFG both through numerical simulations with Gaussian mixtures with varying dimensions and number of classes, and five generative image models. The latter include: DiT \citep{peebles2023scalable} as a standard diffusion framework and EDM2 \citep{karras2024analyzing} as a state-of-the-art one. We test DiT-XL/2 and EDM2-S versions, trained and evaluated on ImageNet-1k (resolution 256 and 512 respectively) for class conditional generation. 
We also consider three text-to-image models. The first two models are trained on ImageNet-1k and CC12M \citep{changpinyo2021conceptual} and evaluated on CC12M, using the diffusion DDPM training objective \citep{ho2020denoisingdiffusionprobabilisticmodels} with MMDiT (\citet{esser2024scaling}, similar to SD3) and MDTv2 \citep{gao2023masked} architectures scaled to 800M parameters. 
The last model uses a MMDiT architecture scaled to 1.6B parameters and is trained with a flow matching objective on YFCC100M \citep{thomee2016yfcc100m}, CC12M  and a proprietary dataset of 320M Shutterstock images, evaluated on COCO dataset \citep{lin2014microsoft}.
%
%\paragraph{Datasets used for evaluation.} We conducted a comprehensive evaluation on three datasets with varying scales and distributions: ImageNet (IMNET), widely used benchmark dataset for image classification tasks. Conceptual 12M (CC12M): A large-scale dataset containing images and captions. Shutterstock (SHST): An internal dataset comprising 320 million Shutterstock images. 
To ensure the protection of personal data, we blurred human faces in ImageNet-1K and CC12M. Additionally, we utilized Florence-2~\citep{xiao2023florence} to recaption images resulting in more accurate image content descriptions.

\looseness=-1\paragraph{Comparing numerical simulations of a mixture of Gaussians to real-world experiments.} In \Cref{fig:6_cfg_score_diff} we observe similar hump-shaped behavior of the difference between conditional and unconditional score $|S_t(\vec{x},c)-S_t(\vec{x})|$ for Gaussian mixtures and that of real-world models, both for class-conditional and text-to-image models, thus validating one of the main points of our theoretical framework. 
Using the parameter $\alpha$ in Power-law CFG, we can alter the shape of these curves, therefore obtaining more flexible frameworks generalizing standard CFG (see  \Cref{fig:11_3_versions} in App. \ref{sec:appx_gm}). We find that this flexibility enables faster convergence, yielding paths that consistently have smaller Jensen-Shannon divergence to the target distribution (across all time $\tau$), reaching the target distribution faster as well as reducing the overshoot of the target distribution (see Fig. \ref{fig:large}, App. \ref{sec:appx_4mg}). 

\looseness=-1\paragraph{Improving image quality and diversity by non-linear CFG.} To evaluate our method, we employed FID \citep{heusel2017gans} measuring image quality, and recall \citep{sajjadi2018assessing} measuring diversity. 
Exact experimental configurations can be found in App. \ref{sec:appx_hyperparam_configs}.
We benchmarked against the best-performing weight scheduler heuristic from \citet{wang2024analysis}, the dynamic powered-cosine $\omega_t = 0.5(1-\cos \pi (1-t/T)^s) \omega$, with $s, \omega$ tunable parameters and $T$ the number of denoising steps. As shown in \Cref{fig:table_comparison}, Power-Law CFG performs favorably both in terms of FID and recall. 
Furthermore, our proposed step-wise CFG heuristic achieved comparable performance to performing hyperparameter search over $t_1$ and $t_2$ but with two fewer hyperparameters, while further combining it with Power-Law yielded slight improvement in FID and notable improvement in recall.
Qualitative examples are provided in  \Cref{fig:qualitative_expls}, with additional examples included in App. \ref{appx:ex_by_dit} (class-conditional) and App. \ref{appx:ex_by_tti} (text-to-image), displaying the improved realism and the increased diversity of the generated images.


While Power-Law CFG demonstrates consistent benefits across different training objectives, noise schedulers, sampling methods, network architectures and datasets, its performance relative to other possible non-linear guidance strategies remains unexplored. We believe there may be more effective and advantageous strategies, which are worth exploring in future work. 

\section{Conclusion}

\looseness=-1We studied the theoretical foundations of classifier-free guidance (CFG), a widely used method with practical benefits but limited theoretical understanding. Our research revealed that CFG's inaccuracies in low-dimensional spaces do not hold true in the high-dimensional limit, yielding a "blessing-of-dimensionality" result. We extended our conclusions to large but finite dimensions, characterizing the discrepancies in lower dimensions. 
Building on our theoretical analysis, we developed a generalized version of CFG and confirmed its effectiveness through both numerical and real-world experiments, applying it successfully to leading efficient diffusion models.
Our results demonstrated improved sample quality and diversity, while reducing the required computation.
Future research directions could include a comprehensive evaluation of the advantages and limitations of non-linear CFG in comparison to alternative guidance methods, as well as designing new non-linear CFG techniques.

\ifarxiv
\else
\clearpage
\newpage
\fi

\section*{Impact Statement}  % does not count to 8 page limit

This study contributes to the growing body of research aimed at deepening our theoretical understanding of diffusion models and their broader implications for generative modeling. By bridging the gap between theory and practice, we strive to improve the performance and efficiency of these models, which have far-reaching applications in various fields.

However, as with any powerful technology, there are also potential risks associated with the development and deployment of advanced generative models. The increasing sophistication of deepfakes, for instance, raises concerns about misinformation, propaganda, and the erosion of trust in digital media. Moreover, the misuse of generative models for malicious purposes, such as creating fake identities or spreading disinformation, poses significant threats to individuals, communities, and society as a whole.

In light of these challenges, we hope that our paper, along with many others that aim to ameliorate understanding of these models, will contribute to a deeper understanding of their strengths and limitations. We believe that this is essential for developing effective strategies to mitigate the risks associated with generative models, and we hope that our work will be a step toward achieving this goal.

\ifarxiv
\section*{Acknowledgements}
This work has received funding from the French government, managed by the National Research Agency (ANR), under the France 2030 program with the reference ANR-23-IACL-0008. Furthermore, this paper is supported by PNRR-PE-AI FAIR project funded by the NextGeneration EU program. We would like to thank Mathurin Videau, João Maria Janeiro, Kunhao Zheng, Tariq Berrada Ifriqi, Wes Bouaziz and Tony Bonnaire for fruitful discussions regarding the numerical experiments. We would further like to thank Levent Sagun, David Lopez-Paz, Brian Karrer, Yaron Lipman and Luke Zettlemoyer for feedback and support. Finally, we also thank Carolyn Krol and Carolina Braga for extensive consultation and support throughout this project.
\clearpage
\newpage
\else
\fi



\ifarxiv
    \bibliographystyle{assets/plainnat}
    \bibliography{paper}
    \clearpage
    \newpage
    \beginappendix
\else
    \bibliography{paper}
    \bibliographystyle{icml2025}
    \newpage
    \appendix
    \onecolumn
\fi

The supplementary material is structured as follows:
\begin{itemize}
    \item In Section \ref{sec:appx_related_work}, we give a brief introduction of related work, mainly focusing on the work by \citet{biroli2024dynamical}.
    \item In Section \ref{sec:appx_gm}, we present experimental details for numerical simulations involving Gaussian Mixtures.
    \item In Section \ref{sec:appx_fin_dim}, we present the theoretical and numerical findings for finite dimension (including low dimension $d$).
    \item In Section \ref{sec:appx_real_world} we provide experimental details involving real-world experiments.
    \item In Section \ref{sec:appx_4mg}, we give the calculation required for computing the CFG score of a mixture of four Gaussians.
    \item In Section \ref{sec:appx_nonlin_cfgs}, we perform further numerical experiments examining the non-linear CFG forms.
    \item In Section \ref{sec:appx_stepwise_cfg}, we draw a parallel to the work of \citet{kynkaanniemi2024applying}, showing that the time interval at which the guidance is applied aligns with our findings.
\end{itemize}


\section{Introduction to related work: Classifier-free Guidance (CFG) and Specification Time in the High-Dimensional Limit}
\label{sec:appx_related_work}
We start by briefly introducing the calculation required for estimating the speciation time $t_s$ for a case of two equally weighted Gaussians. This section is a direct adaptation of the framework introduced by \citet{biroli2024dynamical}. The diffusion process, consisting of $d$ independent Ornstein-Uhlenbeck Langevin equations, reads as follows (using $f(t)=-1$ and $g(t)=\sqrt{2}$ in \Cref{eqn:original_ou_process}):

\begin{align}
    d\vec{x}(t)=-\vec{x}dt+d\vec{B}(t),
    \label{eqn:ou}
\end{align}

where $d\vec{B}(t)$ equals the square root of two times the standard Brownian motion in $\mathbb{R}^d$. At time $t=0$, the process starts from the probability distribution $P_0(\vec{a})$, consisting of two Gaussian clusters that have means at $\pm \vec{m}$ and share the same variance $\sigma^2$. To guarantee that these Gaussians remain distinct in high-dimensional space, we assume that $|\vec{m}|^2 = d \tilde{\mu}^2$, where both $\sigma$ and $\tilde{\mu}$ are of order 1.

As the process evolves, the emergence of speciation resembles symmetry breaking observed during thermodynamic phase transitions. A common approach to analyzing this phenomenon is to construct a perturbative expansion of the free energy as a function of the field. Therefore, \citet{biroli2024dynamical} derive an expression for $\log P_t(\vec{x})$ using a perturbative expansion in terms of $e^{-t}$, which is valid for large time values. This method is justified since speciation occurs at large times.


One can rewrite the probability to be at $\vec{x}$ at time $t$ as
\begin{align*}
P_t(\vec{x}) & =\int d \vec{a} P_0(\vec{a}) \frac{1}{\sqrt{2 \pi \Delta_t^d}} \exp \left(-\frac{1}{2} \frac{\left(\vec{x}-\vec{a} e^{-t}\right)^2}{\Delta_t}\right) \\
& =\frac{1}{\sqrt{2 \pi \Delta_t}} \exp \left(-\frac{1}{2} \frac{\vec{x}^2}{\Delta_t}+g(\vec{x})\right),
\end{align*}


where the function $g(\vec{x})$, defined as
\begin{align*}
    g(\vec{x})=\log \int d \vec{a} P_0(\vec{a}) \exp \left(-\frac{1}{2} \frac{\vec{a}^2 e^{-2 t}}{\Delta_t}\right) \exp \left(\frac{e^{-t} \vec{x} \cdot \vec{a}}{\Delta_t}\right)
\end{align*}

can be viewed through a field-theoretic (or equivalently, a probabilistic) approach, where it serves as a generative function for connected correlations among the variables $\vec{a}$ \citep{zinn2021quantum}. By expanding this function at large times, one can show:
\begin{align*}
    g(\vec{x})=\frac{e^{-t}}{\Delta_t} \sum_{i=1}^d x_i\left\langle a_i\right\rangle+\frac{1}{2} \frac{e^{-2 t}}{\Delta_t^2} \sum_{i, j=1}^d x_i x_j\left[\left\langle a_i a_j\right\rangle-\left\langle a_i\right\rangle\left\langle a_j\right\rangle\right]+O\left(\left(x e^{-t}\right)^3\right),
\end{align*}

where we utilize the brackets $\langle\cdot\rangle$ to denote the expectation value with respect to the effective distribution $P_0(\vec{a}) e^{-\vec{a}^2 e^{-2 t} /\left(2 \Delta_t\right)}$. Therefore, the expansion can be used to show that at large times:

\begin{align*}
    \log P_t(\vec{x})=C+\frac{e^{-t}}{\Delta_t} \sum_{i=1}^d x_i\left\langle a_i\right\rangle-\frac{1}{2 \Delta_t} \sum_{i, j=1}^d x_i M_{i j} x_j+O\left(\left(x e^{-t}\right)^3\right),
\end{align*}

where $C$ is an $\vec{x}$-independent term and
\begin{align*}
    M_{i j}=\delta_{i j}-e^{-2 t}\left[\left\langle a_i a_j\right\rangle-\left\langle a_i\right\rangle\left\langle a_j\right\rangle\right].
\end{align*}

The curvature of $\log P_t(\vec{x})$ is closely linked to the spectral properties of the matrix $M$. In the large time regime, $M$ approaches the identity matrix, and consequently, all its eigenvalues are positive. However, a qualitative shift in shape occurs at the maximum time $t_s$, where the largest eigenvalue of $M$ transitions through zero. This marks the onset of the \textbf{speciation time}, distinguished by a change in curvature of the effective potential $-\log P_t(\vec{x})$. In this case, it can be easily computed: the matrix $M$ is given by $M_{i j}=\left(1-\sigma^2 e^{-2 t}\right) \delta_{i j}-e^{-2 t} m_i m_j$ and its largest eigenvalue is ( $1-\sigma^2 e^{-2 t}-d \tilde{\mu}^2 e^{-2 t}$ ). We get therefore in the large $d$ limit $t_s=\frac{1}{2} \log \left(d \tilde{\mu}^2\right)$ which up to subleading corrections identifies the speciation timescale as

\begin{align*}
    t_s=\frac{1}{2} \log (d).
\end{align*}

\subsection{Asymptotic stochastic process in Regime \rom{1} and symmetry breaking}

In the limit of large dimensions, a comprehensive analytical examination of the dynamics in Regime \rom{1}, taking place on time-scales  $t_\textrm{s}+O(1)=(1/2)\log d +O(1)$, can be provided, specifically at the beginning of the backward process. Assuming no collapse (for further details, refer to \citet{biroli2024dynamical}), an investigation into diffusion dynamics shows that the empirical distribution $P_t^e(\vec{x})$ at time $t$ can be approximated with high accuracy by $P_t(\vec{x})$. This approximation represents the convolution of the initial distribution $P_0$, comprising a mixture of Gaussians centered at $\pm \vec{m}$, and a diffusion kernel proportional to $e^{-\left(\vec{x}-\vec{a} e^{-t}\right)^2 / 2}$. Consequently, the explicit expression for this approximation is


\begin{align}
    P_0(\vec{x})=\frac{1}{2 \left(\sqrt{2 \pi \sigma^2}\right)^d}\left[e^{-\left(\vec{x}-\vec{m} \right)^2 /\left(2 \sigma^2\right)}+e^{-\left(\vec{x}+\vec{m} \right)^2 /\left(2 \sigma^2\right)}\right] \text{, and}
\label{eqn:gm_0}
\end{align}


\begin{align*}
    P_t(\vec{x})=\frac{1}{2 \left(\sqrt{2 \pi \Gamma_t}\right)^d}\left[e^{-\left(\vec{x}-\vec{m} e^{-t}\right)^2 /\left(2 \Gamma_t\right)}+e^{-\left(\vec{x}+\vec{m} e^{-t}\right)^2 /\left(2 \Gamma_t\right)}\right]
\end{align*}

where $\Gamma_t=\sigma^2 e^{-2 t}+\Delta_t$ goes to 1 at large times. The log of this probability is

\begin{align*}
    \log P_t(\vec{x})=-\frac{\vec{x}^2}{2 \Gamma_t}+\log \cosh \left(\vec{x} \cdot \vec{m} \frac{e^{-t}}{\Gamma_t}\right),
\end{align*}

and hence the score reads

\begin{align}
    S_t^i(\vec{x})=-\frac{x^i}{\Gamma_t}+m_i \frac{e^{-t}}{\Gamma_t} \tanh \left(\vec{x} \cdot \vec{m} \frac{e^{-t}}{\Gamma_t}\right).
    \label{eqn:uncond_score}
\end{align}


As there are two classes: $+\vec{m}$ and  $-\vec{m}$, the score conditioned to one class equals the score associated to a given Gaussian. Therefore, for the two classes we have:

\begin{align}
    \begin{split}
        &+\vec{m}: S_t^i(\vec{x}, +)=\frac{-x^i+m_i e^{-t}}{\Gamma_t}, \text{and} \\
        &-\vec{m}: S_t^i(\vec{x}, -)=\frac{-x^i-m_i e^{-t}}{\Gamma_t}. 
    \end{split}
    \label{eqn:cond_score}
\end{align}

\subsection{When does classifier-free guidance take effect?}
\label{sec:appx_sec_A2}

We can proceed to answer this question by examining the classifier-free guidance score, as defined in \citet{ho2022classifier}:

\begin{align}
S_{t_{CFG}}^i(\vec{x},c)= (1+\omega) S^i_t(\vec{x}, c) - \omega S_t^i(\vec{x}),
\label{appx:score_CFG_linear}
\end{align}

where $c=\pm1$ and $\omega>0$. By plugging in the conditional (\ref{eqn:cond_score}) and unconditional scores (\ref{eqn:uncond_score}), we can obtain:


\begin{align}
    S_{t_{CFG}}^i(\vec{x},c) &= -\frac{x^i}{\Gamma_t} + (1+\omega)\frac{c m_i e^{-t}}{\Gamma_t} - \omega \frac{m_i e^{-t}}{\Gamma_t} \tanh{\left(\frac{\vec{x} \cdot \vec{m} e^{-t}}{\Gamma_t}\right)}   \notag  \\
    &= -\frac{x^i}{\Gamma_t} + \omega\frac{m_i e^{-t}}{\Gamma_t}\left\{c-\tanh{\left(\frac{\vec{x}\cdot\vec{m}e^{-t}}{\Gamma_t}\right)}\right\} + \frac{cm_ie^{t}}{\Gamma_t}.
    \label{eqn:cfg_score_appx_1}
\end{align}

Now, in Regime \rom{2}, when the trajectory has committed to a given class, $\vec{x}\cdot\vec{m}\sim O(d)$ and $ \sign(\vec{x}\cdot\vec{m})=c$. Therefore, $c-\tanh{\left(\frac{\vec{x}\cdot\vec{m}e^{-t}}{\Gamma_t}\right)}\approx0$, and one finds from (\ref{eqn:cfg_score_appx_1}), that  $S^i_{t_{CFG}} (\vec{x}, c)= S_t^i(\vec{x})$. This implies that, within this regime, classifier free guidance equals the conditional score. Therefore, Classifier free-guidance only affects Regime \rom{1}, as $S^i_{t_{CFG}} (\vec{x}, c)= S_t^i(\vec{x})$ for $t>t_s=\frac1{2}\log(d)$. This allows us to conclude one of the two results:

\paragraph{Result 1.} In Regime \rom{2}, CFG is innocuous. 

\subsection{What is the role of classifier-free guidance?}


Let us first analyze the ``transverse'' directions $\vec{v}\perp\vec{m}$. For these directions, for all $\omega$, the score is the same and equals $\vec{S}_t^{\text{CFG}} (\vec{x}, c) \cdot \vec{v} = -\frac{\vec{x}\cdot\vec{v}}{\Gamma_t}$. 
Let us project the backward \Cref{eqn:backw_dyn} on a unit vector $\vec v\perp\vec{m}$. 
We write $p=\vec x\cdot\vec v$, and the backward equation now reads $dp=p (1-2/\Gamma_{t_f-\tau})d\tau +\sqrt{2} dB $
which is the backward equation for a single Gaussian variable.  When $\tau\to t_f$ the projection $p=\vec x\cdot\vec v$ is thus distributed as $\mathcal{N}(0, \sigma^2)$, for all values of $\omega$.

Therefore, as all the components except the one in the $\vec{m}$ direction are not affected, we can consider only the component along $\vec{m}$:

\begin{align*}
    \vec{S}_{t_{CFG}}(\vec{x},c) \cdot \frac{\vec{m}}{\abs{\vec{m}}} = - \frac{\vec{x}\cdot \vec{m}/\abs{\vec{m}}}{\Gamma_t} + \omega \frac{\abs{\vec{m}}^2 e^{-t}}{\abs{\vec{m}} \Gamma_t} \cdot \left\{c-\tanh{\left(\frac{\vec{x}\cdot\vec{m} e^{-t}}{\Gamma_t}\right)}\right\} +\frac{\abs{\vec{m}} e^{-t} c}{\Gamma_t}.
\end{align*}

By denoting $\frac{\vec{x}\cdot\vec{m}}{\abs{\vec{m}}}=q(t)$, where $\abs{\vec{m}}=\sqrt{d}$, we can obtain the backward equation:

\begin{align*}
    d{x^i} = (x^i + 2 S_{\tau_{CFG}}^i)d\tau + d\eta_i(\tau),
\end{align*}

where $\tau=t_{f}-t$, i.e., the backward time. Therefore, we can obtain for Regime \rom{1} and by projecting onto the $\frac{\vec{m}}{\abs{\vec{m}}}$ direction, we have that:


\begin{align*}
    d q = dx^i\cdot\frac{\vec{m}}{\abs{\vec{m}}} = \Big(q + 2\Big[-q+ e^{-(t_f-t_\textrm{s}-\tau)} \Big((1+\omega) - \omega\tanh{\left(qe^{-(t_f-t_\textrm{s}-\tau)}\right)}\Big)\Big]\Big)d\tau+d\eta(\tau),
\end{align*}

as, in Regime $\rom{1}$, we have that $\Gamma_t\approx1$, and also $\sqrt{d}=e^{-t_s}$.

Again, from this point onward by $t(\tau)$ we denote the backward time for ease of notation. This is like having an effective potential:

\begin{align*}
    d q = -\pdv{V^\textrm{CFG}(q,\tau)}{q}d\tau+d\eta(\tau),
\end{align*}

where 


\begin{align*}
    V^\textrm{CFG} &= \frac1{2}q^2 +2\left[-(1+\omega)cqe^{-(t-t_s)}+\omega \ln \cosh{\left(qe^{-(t-t_s)}\right)}\right] \notag \\
    &= \underbrace{(\frac1{2}q^2-2e^{-(t-t_s)}cq)}_\text{Classifier's potential}+\omega\underbrace{\left[-cqe^{-(t-t_s)}+\ln\cosh{\left(qe^{-(t-t_s)}\right)}\right]}_\text{Extra potential $V_{\text{extra}}$}.
\end{align*}


Therefore, for class $c=+1$ (equivalently for $c=-1$), there is little effect for $qe^{-(t-t_s)}\gg1$, as then $-qe^{-(t-t_s)}+\ln\cosh{\left(qe^{-(t-t_s)}\right)} \approx 0$. Instead, for $qe^{-(t-t_s)}\ll-1$, we have that $-qe^{-(t-t_s)}+\ln\cosh{\left(qe^{-(t-t_s)}\right)}\approx-qe^{-(t-t_s)}\gg1$.  Therefore, we can conclude our remaining result:

\paragraph{Result 2.} In Regime \rom{1}, the effect of CFG is to push in the correct direction (corresponding to class $c$).

The utility of CFG is therefore to "push" in the right direction in Regime \rom{1} where arguably the class-based score/potential is likely not accurate in the rare region ($q>0$ for $c=-1$ and $q<0$ for $c=+1$). The behavior of the two potentials is displayed in Figure \ref{fig:potential_preliminary}.


\section{Experimental details: Gaussian mixtures}
\label{sec:appx_gm}
In this section, we present experimental details for the numerical simulations involving Gaussian mixtures, describing the procedures and the hyperparameter configurations.

\paragraph{Numerical simulations.} In the case of a mixture of two Gaussian clusters centered on $\pm \vec{m} \in \mathbb{R}^d$ with variance $\sigma^2$, the score function reads as

\begin{align*}
    S_{t_{CFG}}(\vec{x}(t),c) = -\frac{x(t)}{\Gamma_t} + \omega\frac{\vec{m} e^{-t}}{\Gamma_t}\left\{c-\tanh{\left(\frac{\vec{x}(t)\cdot\vec{m}e^{-t}}{\Gamma_t}\right)}\right\} + \frac{c\vec{m} e^{t}}{\Gamma_t}.
\end{align*}

    
where $\Gamma_t=\Delta_t+\sigma^2 e^{-2 t}$, with $\Delta_t=1-e^{-2 t}$. We can then discretize the stochastic differential equation associated to the backward process as
\begin{align*}
    \vec{x}(t+1)=\vec{x}(t)+\eta\left[\vec{x}(t)+2 S_{t_{CFG}}(\vec{x}(t),c)\right]+\vec{\eta} \sqrt{2 \tau/L}
\end{align*}

where $\vec{\eta} \sim \mathcal{N}(0, I)$, with $t_f=8$ the time horizon and $t_f/L=0.01$. We use $\vec{m}=[1, \ldots, 1], \sigma^2=1$, and each point is obtained by averaging over 100 initial conditions. The speciation time $t_s$ is calculated as $t_s = -\frac1{2}\log d$. Throughout the  experiments, we plot the evolution of $q(t)=\frac{\vec{x}\cdot\vec{m}}{|\vec{m}|}$, conditioning the guidance on the positive class with $c=1$. 



\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{assets/plots/potentials_3.pdf}
    \caption{The evolution of the conditional classifier potential, and the CFG potential during the backward process, as a function of backward time $\tau$. We can see the faster convergence to the target of the CFG, as well as the increased "narrowness" of the potential, leading to reduced variance of the generated distribution. }
    \label{fig:potential_preliminary}
\end{figure}

Furthermore, we can observe the flexibility of the non-linear guidance in Figure \ref{fig:11_3_versions}, where we plot the distance between the conditional and unconditional score for linear (standard) and proposed non-linear, Power-Law version of CFG. We average over 5000 samples with $\sigma^2=1$, $\omega=4$, and for non-linear versions $d=50$ and $\alpha\in\{0,-0.25, -0.5, -0.75, -0.9\}$, $\gamma\in\{0,2,4,8,16\}$. Across various hyperparameter configurations and CFG versions, the score difference is consistently zero at the beginning and end of the backward process.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{assets/plots/10_newfig_2_versions.pdf}
    \caption{\textbf{CFG is innocuous in Regime \rom{2}.} The distance between conditional and unconditional score for linear and two proposed non-linear versions of CFG, averaged over 5000 samples with $\sigma^2=1$, $\omega=4$, and for non-linear versions $d=50$ and $\alpha\in\{0,-0.25, -0.5, -0.75, -0.9\}$, $\gamma\in\{0,2,4,8,16\}$. Across various hyperparameter configurations and CFG versions, the score difference is consistently zero at the beginning and end of the backward process.
    }  
    \label{fig:11_3_versions}
\end{figure}

\section{Finite dimension}
\label{sec:appx_fin_dim}

In this section, we give exact analyses describing the effect of CFG in finite- (possibly low-) dimensional settings, outlined in Section \ref{sec:fin_dim_eff} in the main manuscript. We start the backward equation at a time $t_f$ large enough that the distribution of $x$ is a isotropic  Gaussian with variance one.
The backward equation for \( x(t) \) with the CFG score reads:

\begin{align}
    \frac{dx_i}{d\tau} =& x_i \left(1 - \frac{2}{\Gamma(t_f - \tau)}\right) + \frac{2m_i}{\Gamma(t_f - \tau)} e^{-(t_f - \tau)} 
    \nonumber\\
    &+2 \omega m_i\frac{e^{-(t_f-\tau)}}{\Gamma_{t_f-\tau}}\left\{1-\tanh\left(\frac{\vec x\cdot\vec m e^{-(t_f-\tau)}}{\Gamma_{t_f-\tau}}\right)\right\}+ 
    \eta_i(\tau)
\end{align}
where \( \tau = 0 \) at the beginning of the backward process and \( \tau = t_f (\gg 1) \) at the end.

This can be projected on the evolution of the single parameter $q(\tau)=\vec x\cdot\vec m /\sqrt{d}$.
We obtain
\begin{align}
    \frac{dq}{d\tau} =& q \left(1 - \frac{2}{\Gamma(t_f - \tau)}\right) + \frac{2\sqrt{d}}{\Gamma(t_f - \tau)} e^{-(t_f - \tau)} \nonumber\\
    &+2 \omega \sqrt{d}\frac{e^{-(t_f-\tau)}}{\Gamma_{t_f-\tau}}\left\{1-\tanh\left(\frac{q \sqrt{d} e^{-(t_f-\tau)}}{\Gamma_{t_f-\tau}}\right)\right\}+ 
    \eta(\tau).
    \label{basic_backward_eq_q}
\end{align}

Considering the right-hand side as a force due to a moving external potential $-\partial_q V(q,t)$, the effect of CFG is to add an extra term which has two main effects: (1) it adds a positive term to the force and, in consequence, it pushes $q$ faster away from zero, (2) it increases the value of the Hessian at any point in $q$ with respect to its $\omega=0$ counterpart, thus making the potential more confining.

The initial condition is \( q(\tau = 0) \sim \mathcal{N}(0, \sigma^2) \) and
\begin{equation}
    \Gamma(t_f - \tau) = \sigma^2 e^{-2(t_f - \tau)} + 1 - e^{-2(t_f - \tau)}.
\end{equation}

\subsection*{Case: $\omega=0$}
The solution of the backward equation is:

\begin{equation}\label{bsol}
    q(\tau) = q(0)e^{ \tau-2\int_0^\tau \frac{1}{\Gamma(t_f-\tau'')}d\tau''} + \int_{0}^{\tau} \left[\frac{2\sqrt{d} e^{-(t_f - \tau')}}{\Gamma(t_f-\tau')} + \eta_i(\tau')\right] e^{(\tau - \tau')-2\int_{\tau'}^\tau \frac{1}{\Gamma(t_f-\tau'')}d\tau''} d\tau'.
\end{equation}
Its probability distribution must coincide with the one of the solutions of the forward equation, which reads:
\begin{equation*}
    q(t) = \sqrt{d} e^{-t} + \sqrt{1 - e^{-2t}} z_i + e^{-t} \sigma \tilde{z}_i,
\end{equation*}
where \( z_i, \tilde{z}_i \sim \mathcal{N}(0,1) \) and \( t = t_f - \tau \). Let us now focus on the mean of $q$. When we consider the term
\begin{equation*}
  \int_{0}^{\tau} \left[\frac{2\sqrt{d} e^{-(t_f - \tau')}}{\Gamma(t_f-\tau')}\right] e^{(\tau - \tau')-2\int_{\tau'}^\tau \frac{1}{\Gamma(t_f-\tau'')}d\tau''} d\tau',
\end{equation*}
using that 
\begin{equation*}
    \frac{d}{d\tau'} \exp\left[-2\int_{\tau'}^{\tau} \frac{1}{\Gamma(t_f - \tau'')} d\tau''\right] = \frac{2}{\Gamma(t_f - \tau')}\exp\left[-2\int_{\tau'}^{\tau} \frac{1}{\Gamma(t_f - \tau'')} d\tau''\right],
\end{equation*}
one finds that the mean of $q$ for the evolution with $\omega=0$, starting from any value $q(0)$ at any time $t_f$, is 
\begin{equation}\label{bsol2}
    q(\tau) = q(0)e^{ \tau-2\int_0^\tau \frac{1}{\Gamma(t_f-\tau')}d\tau'} + \sqrt{d} e^{-(t_f - \tau)}\left( 1- \exp\left(-2 \int_{0}^\tau \frac{1}{\Gamma(t_f-\tau')}d\tau' 
    \right)
    \right).
   \end{equation}
Using
\begin{equation*}
    \int_0^\tau \frac{1}{\Gamma(t_f-\tau')}d\tau'=-\frac{1}{2}\log\frac{e^{-2\tau}+(\sigma^2-1)e^{-2 t_f}}{1+(\sigma^2-1)e^{-2 t_f}},
\end{equation*}
we find that
\begin{equation}\label{bsol3}
    q(\tau) = q(0)\; e^\tau \; 
    \frac{e^{-2\tau}+(\sigma^2-1)e^{-2 t_f}}{1+(\sigma^2-1)e^{-2 t_f}} + \sqrt{d} \; e^{-(t_f - \tau)} \; 
    \frac{1-e^{-2\tau}}{1+(\sigma^2-1)e^{-2 t_f}}.
   \end{equation}
One can check that, when $q(0)$ is obtained by the equilibrium process with $\omega=0$, namely $q(0)=\sqrt{d}e^{-t_f}$, then at all times $q(\tau)=\sqrt{d}e^{-(t_f-\tau)}$.

\subsection*{Case: interrupted guidance}
Now let us consider a protocol of interrupted guidance. We start the backward process at $t_f\gg 1$ with a CFG coefficient $\omega>0$. Then at time backward time $\tau_1$ (forward time $t_1= t_f-\tau_1$) we switch to $\omega=0$.
At time $t_1$ the mean of $q$ obtained from the backward process with $\omega>0$ is larger than the value $ \sqrt{d}e^{-t_1} $ which would be obtained with the $\omega=0$ dynamics (the reason is that the extra force in (\ref{basic_backward_eq_q}) is positive). Let us write this mean as 

\begin{equation*}
    q(t_1,\omega)= \sqrt{d}\; e^{-t_1}+\delta q(t_1,\omega).
\end{equation*}

Let us measure the backward time starting from $t=t_1$. We thus write $t=t_1-\tilde \tau$. We can use formula (\ref{bsol3}) with $t_f\to t_1$, $\tau\to \tilde \tau$ and $q(0)\to q(t_1,\omega)$. This gives for the mean value of $q$:
\begin{equation*}
\tilde q(\tilde \tau,\omega)= \sqrt{d} e^{-(t_1-\tilde \tau)}+\delta q(t_1,\omega) \; \frac{e^{-\tilde\tau}+(\sigma^2-1)e^{\tilde\tau-2 t_1}}{1+(\sigma^2-1)e^{-2 t_1}},
\end{equation*}
which, translated in terms of the forward time $t=t_1-\tilde \tau$, gives:
\begin{equation}
    q(t)= \sqrt{d} e^{-t}+ \delta q (t_1,\omega) \; e^{t-t_1}\; \frac{1+(\sigma^2-1)e^{-2t}}{1+(\sigma^2-1)e^{-2t_1}}.
    \label{eq:qav_switch_pred}
\end{equation}

In particular at the end of the backward process, for $\tilde\tau=t_1$ we get 

\begin{equation*}
 q(t=0)= \sqrt{d} +\delta q(t_1,\omega) \; e^{-t_1}\; \frac{\sigma^2}{1+(\sigma^2-1)e^{-2 t_1}}
\end{equation*}
If we choose $t_1=t_s=(1/2)\log d$, and assuming that the dynamics at $t>t_1$ has produced an average $q(t_1)=\sqrt{d} e^{-t_1}+\delta q$, we find that 
\begin{equation*}
 q(t=0)= \sqrt{d} \left(1+\delta q  \; \frac{\sigma^2/d}{1+(\sigma^2-1)/d}\right).
\end{equation*}



\begin{figure}[t]
    \centering
    \includegraphics[width=0.38\textwidth]{assets/plots/6_predictions.pdf}
    \caption{Mean value of $q$ obtained from the backward diffusion in a Gaussian mixture model with $d=16,\sigma^2=4$ (speciation time $t_s=1.38$). The CFG is run with $\omega=8$ from $t=5$ to $t=t_1$, then one switches to the class guidance $\omega=0$. The top curve is when CFG is kept all the time ($t_1=0)$. The bottom curve is the case without CFG ($\omega=0$). Three values of $t_1$ are studied $t_1=0.69,1.38,3.19$ (vertical lines). The dashed curves give the mean value of $q$ for each of these three cases. They are in perfect agreement with the theoretical prediction (\ref{eq:qav_switch_pred}).
       }
    \label{fig:switchomegamean}
\end{figure}



\begin{figure}[t]
    \centering
     \includegraphics[width=1.\linewidth]
     {assets/plots/7_hists.pdf}
    \caption{Histograms of $q(t=0)$ obtained from the backward diffusion in a Gaussian mixture model with $d=16,\sigma^2=4$ (the speciation time is $1.38$), run with $200,000$ trajectories. Left: CFG with $\omega=8$ is applied at all times. The final distribution has a larger mean and a smaller variance than the desired class distribution (full line). Next three figures:
    The CFG is run with $\omega=8$ from $t=5$ to $t=t_1$, then one switches to standard CFG $\omega=0$. From left to right, $t_1=0.69,1.38,3.19$. 
    The mean values of $q$ in the four cases are respectively $5.56, 5.51,5.29,4.12 $ and the standard deviations $1.68, 1.74 ,1.87 , 1.98$, with targets $\mu=4, \sigma=2$. 
    In order to minimize the bias due to CFG one must interrupt it before the speciation takes place in the background diffusion, hence at $t_1>t_s$.  
    }
    \label{fig:switchomega}
\end{figure}

This shows that the guidance interrupted at $t_s$ gives a good result only in the limit $\sigma^2/d\ll 1$. 
Figs. \ref{fig:switchomegamean} and \ref{fig:switchomega} illustrate the effect of the choice of $t_1$. 

\subsection*{CFG contribution to the magnetization in Regime \rom{1}}

Using Equation (14), one can derive the equation for the average \( \langle q(\tau) \rangle_\omega \):

\begin{align}
    \frac{d\langle q(\tau) \rangle_\omega}{d\tau} 
    &= \langle q(\tau) \rangle_\omega \left( 1 - \frac{2}{\Gamma(t_f-\tau)} \right) 
    + \frac{2\sqrt{d}}{\Gamma(t_f-\tau)} e^{-(t_f-\tau)} \nonumber \\
    &\quad + 2\omega\sqrt{d} \frac{e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} 
    \left\langle 1 - \tanh\left( \frac{q \sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} \right) \right\rangle_\omega.
\end{align}

The extra \( \omega \) term is strictly positive. Therefore, we have:

\begin{equation*}
    \langle q(\tau) \rangle_\omega \geq \langle q(\tau) \rangle_{\omega=0}, \quad \forall \tau.
\end{equation*}

Moreover, using that the right-hand side is less than or equal to:

\begin{equation*}
    \langle q(\tau) \rangle_\omega \left( 1 - \frac{2}{\Gamma(t_f-\tau)} \right) 
    + \frac{2(1+\omega)\sqrt{d}}{\Gamma(t_f-\tau)} e^{-(t_f-\tau)},
\end{equation*}

which corresponds to the backward equation one would obtain if \( \|\vec{m}\|^2 =  (1+\omega)d \). We then find:

\begin{equation*}
   \langle q(\tau) \rangle_{\omega=0} <  \langle q(\tau) \rangle_\omega <\sqrt{d} e^{-t} (1+\omega).
\end{equation*}

We conclude that \( \langle q(\tau) \rangle_\omega \) gets an extra contribution due to CFG of the order \( \sqrt{d} e^{-t} \).

CFG indeed shifts the mean value. The amount of shift is of order $\sqrt{d}e^{-t}$ in Regime \rom{1}. However, as we shall see next the CFG has almost no effect in Regime \rom{2}, so we can use the result of the previous section to argue that the total shift due to CFG is the one of CFG in Regime \rom{1} followed by a switch at $\omega=0$ in Regime \rom{2}, i.e., it is of order one.  

\subsection*{CFG contribution to the score in Regime \rom{1} vs in Regime \rom{2}}

Another interesting inequality can be derived for the difference between the CFG and the standard, non-guided score, \( S_{\text{CFG}} - S_C \), evaluated on trajectories corresponding to CFG:

\begin{align}
    S_{\text{CFG}} - S_C 
    &= \omega \frac{\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} 
    \left( 1 - \tanh\left( \frac{q \sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} \right) \right).
\end{align}

We use the fact that for the same thermal noise, we have \( q_\omega(\tau) \geq q_{\omega=0}(\tau) \) because the CFG force is always equal or larger than the $\omega=0$ one. Therefore for a given (the same) thermal history we have: 

\begin{equation}
    -\tanh\left(\frac{q_\omega(\tau)\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)}\right) 
    \leq -\tanh\left(\frac{q_{\omega=0}(\tau)\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)}\right),
\end{equation}

and we can obtain:

\begin{align}
    S_{\text{CFG}} - S_C 
    &\leq \frac{\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} 
    \left( 1 - \tanh\left(\frac{q_{\omega=0}(\tau)\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)}\right) \right).
\end{align}

This inequality tells us, as expected, that the extra CFG contribution to the score is very small at the beginning of the backward process. Its mean increases, and is of the order of one during the backward process in Regime \rom{1}. However, after the speciation time $q_{\omega=0}(\tau)$ is a Gaussian variable with a mean $\sqrt{d}e^{-(t_f-\tau)}$ much larger than the square root of the variance. Therefore, replacing the fluctuating variable by its mean we obtain  
\begin{align}
    S_{\text{CFG}} - S_C 
    &\leq \frac{\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} 
    \left( 1 - \tanh\left(\frac{{d} e^{-2(t_f-\tau)}}{\Gamma(t_f-\tau)}\right) \right).
\end{align}
In Regime \rom{2}, $t_f-\tau$ is of order one, and using the asymptotic form of the hyperbolic tangent one finds that
\begin{align}
    S_{\text{CFG}} - S_C 
    &\leq \frac{\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} 
   \exp\left(-2\frac{{d} e^{-2(t_f-\tau)}}{\Gamma(t_f-\tau)}\right) .
\end{align}
Therefore in Regime \rom{2} the extra contribution to the score is exponentially small in $d$ and its effect is completely negligible with respect to the one in Regime \rom{1}. 
\subsection*{Analysis of the CFG effect on the variance}

Let us derive the equation for \( \langle q^2(\tau) \rangle_\omega - \langle q(\tau) \rangle_\omega^2 \).

Using Itô calculus, we have (multiplying by \( q(\tau) \) in the equation for \( \frac{dq(\tau)}{d\tau} \)):

\begin{align}
    \frac{d q^2(\tau) }{d\tau} 
    &= 2 + 2  q^2(\tau)  \left( 1 - \frac{2}{\Gamma(t_f-\tau)} \right) 
    + 2  q(\tau)  \frac{2\sqrt{d}}{\Gamma(t_f-\tau)} e^{-(t_f-\tau)} \nonumber \\
    &\quad + 2 \frac{2\omega\sqrt{d}}{\Gamma(t_f-\tau)} e^{-(t_f-\tau)}
    \left( q(\tau) - q(\tau) \tanh\left( \frac{q(\tau)\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} \right) \right) \nonumber \\
    &\quad + 2q(\tau) \eta(\tau).
\end{align}


Taking the average and subtracting \( 2\langle q(\tau) \rangle_\omega \frac{d\langle q(\tau) \rangle_\omega}{d\tau} \), we find the equation for 
\( \langle q^2(\tau) \rangle_\omega - \langle q(\tau) \rangle_\omega^2 \):

\begin{align}
    \frac{d \langle q^2(\tau) \rangle_\omega - \langle q(\tau) \rangle_\omega^2}{d\tau} 
    &= 2 + 2 \left( \langle q^2(\tau) \rangle_\omega - \langle q(\tau) \rangle_\omega^2\right) \left( 1 - \frac{2}{\Gamma(t_f-\tau)} \right) \nonumber \\
    &\quad + \omega \frac{4\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} 
    \left( \langle q(\tau) \rangle_\omega 
    \left\langle \tanh\left(\frac{q\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} \right) \right\rangle_\omega \right. \nonumber \\
    &\quad \left. - \langle q(\tau) \tanh\left(\frac{q\sqrt{d} e^{-(t_f-\tau)}}{\Gamma(t_f-\tau)} \right) \rangle_\omega \right).
\end{align}

%\rd{**Question:** Can one determine the sign of the CFG extra term?}

At the beginning of the backward process, one can expand \( \tanh(x) \) and observe that the term in the parentheses is proportional to:

\begin{equation}
    -\left(\langle q(\tau)^2 \rangle_\omega - \langle q(\tau) \rangle_\omega^2\right),
\end{equation}

which is negative. Therefore, we can conclude that the classifier-free-guidance-added term will result in shrinkage of the variance.

As for the mean, the main CFG effect on the variance is produced in Regime \rom{1}, since the CFG score term is exponentially small in Regime \rom{2}. 


\section{Real-world experiments}
\label{sec:appx_real_world}
\subsection{Performing time reparameterization}
\label{sec:time_reparameterization}
In the second part of the paper, we evaluate the score of Diffusion Model with Transformers (DiT), in discrete time, as introduced by \citet{peebles2023scalable}. In this context, the forward process has a linear variance schedule $\left\{\beta_t^{\prime}\right\}_{t^{\prime}=1}^L$, where $L$ is the time horizon given as a number of steps. Here, the variance is evolving linearly from $\beta_1=10^{-4}$ to $\beta_{1000}=2 \times 10^{-2}$. A sample (without any guidance), at timestep $t^{\prime}$, denoted $\vec{x}\left(t^{\prime}\right)$ can therefore be expressed readily from its initial state, $\vec{x}(0)=\vec{a}$, as
\begin{align*}
    \vec{x}\left(t^{\prime}\right)=\sqrt{\bar{\alpha}}\left(t^{\prime}\right) \vec{a}+\sqrt{1-\bar{\alpha}\left(t^{\prime}\right)} \vec{\xi}\left(t^{\prime}\right)
\end{align*}

where $\bar{\alpha}\left(t^{\prime}\right)=\prod_{s=1}^{t^{\prime}}\left(1-\beta_s\right)$ and $\vec{\xi}$ is a standard and centered Gaussian noise. This equation in fact corresponds to the discretization of the Ornstein-Uhlenbeck Eq. (\ref{eqn:ou}) under the following reparameterization of the timestep $t^{\prime}$,

\begin{align*}
    t=-\frac{1}{2} \log \left(\bar{\alpha}\left(t^{\prime}\right)\right),
\end{align*}

where $t$ is the time as defined in the main manuscript. This gives the  map between the timescale used in our theoretical approach and numerical simulations of Gaussian mixtures, and the one used in our real-world settings. We note that, as the neural network predicts the noise, in order to calculate the score, one needs to normalize the output by the standard deviation (depending on the variance schedule). In this case, this corresponds to dividing the neural network output by $\sigma(t')=\sqrt{1-\bar{\alpha}(t')}$. In numerical experiments, we divide by $\sigma(t')+1$ to avoid numerical errors. This is theoretically justified due to the fact that, as discussed in main paper, the score difference $|S_{t'}(\vec{x},c)-S_{t'}(\vec{x})|$ for large forward times decays exponentially (as $e^{-t'}$) to zero.

In addition, for completeness, we now present the full comparison of numerical simulations to real-world using the time-reparameterization to plot the timesteps on the same time-scale. Our findings are portrayed in Figure \ref{fig:correct_time}. As each framework uses a separate time reparameterization, the x-axis needs to be recalculated accordingly. For the EDM2 framework \citep{karras2022elucidating}, this can be done as follows: given a noise schedule $\sigma(t)$, the reparameterization can be calculated as $t' (t)=(1/2) \log(1+\sigma^2(t))$, assuming that $s(t)=1$. For the case $s(t)$, one needs to resort to equation \Cref{eqn:backw_dyn}. 


\subsection{Hyperparameter configurations}
\label{sec:appx_hyperparam_configs}
Here, we give exact hyperparameters used for reproducing all our experiments.

In \textbf{Figure \ref{fig:1_histograms}}, we plot the histograms of the samples generated using the backward process with dimensions $d\in\{2,200\}$ and guidance parameter $\omega\in\{0, 0.2, 15\}$, with $\sigma^2=1$, averaged over $10,000$ trajectories. 

In \textbf{Figure \ref{fig:2_diffusion_regimes}}, we plot the evolution of the 1D backward dynamics with means at $\pm4$ and unit variance. The potential plotted corresponds to equation $V(q,t)=\frac1{2}q^2-2\log\cosh(q e^{-(t-t_s)})$. For the derivation of this potential, see Appendix B.2 in \citet{biroli2024dynamical}. 

In \textbf{Figure \ref{fig:4_potentials}}, we examine the following functions:
\begin{align*}
V_{\text{class}}(q, t; c) &= \frac{1}{2}q^2 - ce^{-(t-t_s)}q + 2 \\
V_{\text{extra}}(q, t; c) &= -ce^{-(t-t_s)}q + \log\left(\cosh\left(qe^{-(t-t_s)}\right)\right) + \log(2),
\end{align*}
where the plots correspond to $V_{\text{class}}$, $V_{\text{extr}}$ and $(V_{\text{class}}+\omega V_{\text{extr}})$ with $\omega=3$ respectively. We select $c=1$, and fix the speciation time to $t_s=.5$. The additive constants are manually added for clarity of presentation.

In \textbf{Figure \ref{fig:intro_traject}}, we examine the evolution of the mean of $q(t)$. We set $d=1000$ (therefore $t_s=2$), with $\sigma^2=1.$ and average over $10,000$ trajectories.


In \textbf{left part} of \textbf{Figure \ref{fig:6_cfg_score_diff}}, we evaluate the difference of the conditional and unconditional score for Gaussian mixtures, $|S_t(x,c)-S_t(x)|$ for dimensions $d\in\{10,100\}$ with 2,4 and 8 classes, using $\omega=4, \sigma^2=1$, averaged over $10,000$ trajectories. The eight Gaussians are centered to be on equidistant means on a sphere with radius $r=\sqrt{d}$.

In \textbf{right part} of \textbf{Figure \ref{fig:6_cfg_score_diff}}, we plot the score difference for four real-world models: DiT/XL-2, EDM2-S, MMDiT and MDTv2, all trained using the diffusion objective. As each of the models' default hyperparameters have different number of sampling steps, we normalize the x-axis from 0 to 1: e.g.,  DiT framework uses 250 sampling steps, EDM2 uses 32 and for the text-to-image models we use 50 sampling steps. The y-axis is normalized to be between 0 and 1 for easier readability. All hyperparameters are set to the default ones.

In \textbf{Figure \ref{fig:qualitative_expls}}, we provide qualitative analysis of Standard and Power-Law CFG using DiT/XL-2 trained on ImageNet-1K (256x256) with $\omega=4$ in both cases and $\alpha=0.1$ in the non-linear version. All other hyperparameters are set to the default values.

In \textbf{Figure \ref{fig:potential_preliminary}}, we use the same hyperparameter configuration as in Figure \ref{fig:4_potentials}, plotting for various values of the backward time $\tau$: 3.5, 6.5 and 7.7.

In \textbf{Figure \ref{fig:11_3_versions}}, we plot the difference of conditional and unconditional score for standard and Power-Law CFG, averaged over $5,000$ samples with $\sigma^2=1$, $\omega=4$, and for non-linear version $d=50$ and $\alpha\in\{0,-0.25, -0.5, -0.75, -0.9\}$. 

In \textbf{Figure \ref{fig:switchomegamean}}, we plot the backward diffusion in a Gaussian mixture model with $d=16,\sigma^2=4, \omega=8$. The CFG is either run at all times (top curve), stopped at times $t_1$ or not used at all (bottom curve).

In \textbf{Figure \ref{fig:switchomega}}, we perform linear CFG with $\omega=8$ from $t=5$ to $t=t_1$, after which we turn CFG off ($\omega=0$) at times $t_1=0.69,1.38,3.19$.

In \textbf{Figure \ref{fig:correct_time}}, we perform the same experiment as in the left part of, however with DiT/XL-2 model trained on 2, 500 and 1000 classes. For 2 classes, we have selected the same classes as in \citet{biroli2024dynamical}, and for the 500 we selected the first 500 classes in ImageNet-1K. The x-axis represents the Forward time $t$, where the parameterization is obtained as explained in \Cref{sec:time_reparameterization}.

In \textbf{Figures \ref{fig:newnonlin1}-\ref{fig:newnonlin2}}, we use $d=16$ and $\sigma^2=4$. 




\begin{figure*}[t]
    \centering
    \begin{NiceTabular}{ccc}
        \includegraphics[width=0.3\textwidth]{assets/plots/6_cfg_score_diff_A.pdf} &
        \includegraphics[width=0.3\textwidth]{assets/plots/6_cfg_score_diff_B.pdf} &
        \includegraphics[width=0.3\textwidth]{assets/plots/edm2_cc12m_coco.pdf}
    \end{NiceTabular}
      \caption[Caption for LOF]{\textbf{Evolution of the score differences for numerical simulations and real-world experiments} projected onto the same time-scale for direct comparison. \textbf{First:} Numerically simulating mixture of two, four, and eight Gaussians with equidistant means on a sphere ($r=\sqrt{d}$), with varying dimension $d$, with $\omega=4, \sigma^2=1$, averaged over 10,000 trajectories. As $d$ increases, the score difference starts to increase at an earlier backward time $\tau$. Additionally, as the number of classes increases, the magnitude of the score difference grows, as well as the duration of large difference between the scores. \textbf{Second:} Three DiT/XL-2 models trained on ImageNet-1K using 2, 500, and 1000 classes (image size $512\times 512$). We observe a similar pattern: as $d$ increases, the score difference becomes larger at an earlier time. Furthermore, as the number of classes increases, the magnitude of the score difference increases, together with the duration for which the difference remains large. \textbf{Third:} evolution of the remaining models used in our experiments (EDM2-S, MMDiT and MDTv2). We observe a similar behavior to theory and the DiT/XL-2 models.}
      \label{fig:correct_time}
\end{figure*}


In the \textbf{first column} of \textbf{Figure \ref{fig:large}}, we plot the estimated Jensen-Shannon Divergence between the target samples corresponding to a randomly selected class and the diffusion particles throughout the backward trajectory. Note that this is performed in latent space. For obtaining the \textbf{middle column}, we first take all data samples from one class, embed them into the latent space and calculate the centroid corresponding to this class. Then, we normalize the centroid (making it unit norm) and plot the dot product of the particles throughout the backward diffusion process with the calculated centroid. The \textbf{right column} corresponds to the score difference. Across all experiments, we selected $\omega=4$, sampled using DDPM \citep{ho2020denoisingdiffusionprobabilisticmodels} using 250 sampling steps, averaged over 25 samples. All other hyperparameter configurations are set to the default.

Finally, in \textbf{Figure \ref{fig:estim_apply_cfg}}, we perform the same experiment as in Figure \ref{fig:6_cfg_score_diff} right, however we do not normalize the x- nor the y-axis. 


\textbf{Table \ref{tab:hyperparameters}} displays the hyperparameters used to obtain the results  given in Table \ref{fig:table_comparison}. The evaluation code relied on EvalGIM library by \citet{hall2024evalgim}. We observe that the optimal hyperparameter configuration aligns with the findings of \citet{kynkaanniemi2024applying}, which suggest that when guidance is applied over a shorter interval (analogous to non-linear CFG), a stronger or larger value of $\omega$ is typically necessary for optimal performance.

\begin{table}[t]
    \centering
    \caption{Hyperparameters used throughout our experiments. All other hyperparameters are set to the default values.}
        \begin{tabular}{c|c|ccccc}
            \toprule 
            \multirow{ 2}{*}{} & \multirow{ 2}{*}{param.} & \textbf{DiT/XL-2} & \textbf{EDM2-S} & \textbf{MMDiT} & \textbf{MDTv2} & \textbf{MMDiT-FM} \\
             & & \small\textit{IM256} & \small\textit{IM512} & \small\textit{CC12M} & \small\textit{CC12M} & \small\textit{COCO} \\                                
            \midrule
            Stand.     & $\omega$         & 1.5 & 1.4   & 1.6 & 1.5 & 1.65  \\
            Scheduler  & $s, \omega$      & 1.6, 1.95 & 2, 3.5 & 0.1, 1.75  & 1.5, 2.0 & 0.5, 3.25 \\
            NL-Stand.  & $\alpha, \omega$ & -0.1, 1.6 & -0.06, 2.35 & 0.15, 2.15 & 0.25, 1.75 & 0.3, 3.5 \\
            \midrule
            Limited    & $\omega$ & 1.8 & 2.1 & 1.6 & 1.5 & 2.15 \\
            Lim. v2    & $\omega$ & 1.85 & 2.0 & 1.7 & 1.6 & 2.25 \\
            NL-Lim.v2  & $\alpha, \omega$ & -0.03, 2.1 & -0.05, 2.05 & 0.225, 2.55 & 0.2, 2.35 & 0.05, 3.25 \\
            \bottomrule
        \end{tabular}
    \label{tab:hyperparameters}
\end{table}



\section{Computation for the mixture of four Gaussians}
\label{sec:appx_4mg}

Here we present the computation for a mixture of four Gaussians, in order to analyze the behavior of the system for increasing number of classes. As before, assuming no collapse, we can approximate the empirical distribution $P_t^e(\vec{x})$ at time $t$ by $P_t(\vec{x})$ with high accuracy. In this case, the approximation represents the convolution of the initial distribution $P_0$, being a mixture of 4 Gaussians centered at $\pm\m1 \pm \m2$, s.t. $\m1\cdot\m2=0$, and a diffusion kernel proportional to $e^{-\left(\vec{x}-\vec{a} e^{-t}\right)^2 / 2}$. The explicit expression for the distribution is:



\begin{align*}
    P_0(\vec{x})=\frac{1}{4 \left(\sqrt{2 \pi \sigma^2}\right)^d}&\left[e^{-\left(\vec{x}-(\m1 - \m2) \right)^2 /\left(2 \sigma^2\right)}+e^{-\left(\vec{x}-(\m1 
    + \m2) \right)^2 /\left(2 \sigma^2\right)}\right.\\
    &\left.+e^{-\left(\vec{x}+(\m1 - \m2) \right)^2 /\left(2 \sigma^2\right)}+e^{-\left(\vec{x}+(\m1 + \m2) \right)^2 /\left(2 \sigma^2\right)}\right]
\end{align*}

and 
\begin{align*}
    P_t(\vec{x})=\frac{1}{4 \left(\sqrt{2 \pi \Gamma_t}\right)^d}&\left[e^{-\left(\vec{x}-(\m1 - \m2)e^{-t} \right)^2 /\left(2 \Gamma_t\right)}+e^{-\left(\vec{x}-(\m1 + \m2)e^{-t} \right)^2 /\left(2 \Gamma_t\right)}\right.\\
    &+ \left. e^{-\left(\vec{x}+(\m1 - \m2)e^{-t} \right)^2 /\left(2 \Gamma_t\right)}+e^{-\left(\vec{x}+(\m1 + \m2)e^{-t} \right)^2 /\left(2 \Gamma_t\right)}\right]
\end{align*}


where $\Gamma_t=\sigma^2 e^{-2 t}+\Delta_t$ goes to 1 at large times. This can be rewritten as:

\begin{align*}
    P_t(\vec{x})&= \frac{1}{2 \left(\sqrt{2 \pi \Gamma_t}\right)^d}e^{-(\vec{x}^2+\m1^2e^{-2t}+\m2^2e^{-2t})/(2\Gamma_t)}\left[e^{-\m1\cdot\m2e^{-2t}/\Gamma_t} \cosh (\vec{x}\cdot(\m1+\m2)\frac{e^{-t}}{\Gamma_t}) \right. \\
    & \left. + e^{\m1\cdot\m2e^{-2t}/\Gamma_t} \cosh (\vec{x}\cdot(\m1-\m2)\frac{e^{-t}}{\Gamma_t}) \right]
\end{align*}


The log of this probability is:

\begin{align*}
    \log P_t(\vec{x})=\frac{-\vec{x}^2}{2\Gamma_t}+\log\left(e^{-\m1\cdot\m2e^{-2t}/\Gamma_t} \cosh (\vec{x}\cdot(\m1+\m2)\frac{e^{-t}}{\Gamma_t}) + e^{\m1\cdot\m2e^{-2t}/\Gamma_t} \cosh (\vec{x}\cdot(\m1-\m2)\frac{e^{-t}}{\Gamma_t})\right)
\end{align*}


And the score reads:

\begin{align*}
    S^i_t(\vec{x})&=\frac{-x^i}{\Gamma_t}\\
    &+\frac{e^{-t}}{\Gamma_t}
    \frac
        {(\m1+\m2)_ie^{-\m1\cdot\m2e^{-2t}/\Gamma_t} \sinh (\vec{x}\cdot(\m1+\m2)\frac{e^{-t}}{\Gamma_t})+(\m1-\m2)_ie^{\m1\cdot\m2e^{-2t}/\Gamma_t} \sinh (\vec{x}\cdot(\m1-\m2)\frac{e^{-t}}{\Gamma_t})}
        {e^{-\m1\cdot\m2e^{-2t}/\Gamma_t} \cosh (\vec{x}\cdot(\m1+\m2)\frac{e^{-t}}{\Gamma_t}) + e^{\m1\cdot\m2e^{-2t}/\Gamma_t} \cosh (\vec{x}\cdot(\m1-\m2)\frac{e^{-t}}{\Gamma_t})}
\end{align*}

As $\vec{x}$ approaches one of the means $\pm \m1 \pm \m2$, the second summand reduces to $(\m1 \pm \m2) \tanh(x\cdot(\m1 \pm \m2)\frac{e^{-t}}{\Gamma_t})$\footnote{For large values of $x\cdot(\m1 \pm \m2)e^{-t}/\Gamma_t$, we utilized the \textit{log-sum-exp trick} to calculate the value of the fraction.}, resulting in an expression akin to the one for mixture of 2 Gaussians in (\ref{eqn:uncond_score}).


\section{Further notes on non-linear CFG}
\label{sec:appx_nonlin_cfgs}
Our first non-linear CFG, given by

\begin{align*}
S_{t_{CFG-NL}}(\vec{x},c)= S_t(\vec{x}, c) + \left[S_t(\vec{x}, c) -  S_t(\vec{x})\right] f \left(|S_t(\vec{x}, c) -  S_t(\vec{x})|\right),
%\label{eqn:nonlin_cfg}
\end{align*}

with $f(x)=\omega x^\alpha$ and $\alpha>-1$, is generally used with $\alpha$ negative or of small magnitude. This is very efficient at pushing the trajectories in the  direction of the desired class at early times of the background process. The reason is that, at large $t$, the scores $S_t(\vec{x})$ and  $ S_t(\vec{x}, c)$ go to zero as $e^{-t}$, and the non-linear term becomes large. However this effect also occurs when $t<t_s$, where the difference $ |S_t(\vec{x}, c) -  S_t(\vec{x})| $ becomes small. Although in the regime of $t\ll t_s$ the non-linear term does not matter, in the intermediate regime $t$ close to $t_s$ the strong non-linear term impacts the trajectory. In finite dimension it biases the distribution obtained at $t=0$ (see Fig.\ref{fig:large}). 


One would like to have different non-linearities applying to the regimes $t\gg t_s$ and $t<t_s$. One possibility is to use the following version.

\paragraph{Rescaled Power-law CFG.}
Here, by denoting with $\langle \cdot \rangle$ the expectation \wrt the effective distribution $P_0(\vec{a}) e^{-\vec{a}^2 s(t) /\left(2 s(t)^2\sigma(t)^2\right)}$, the score difference can be expressed as $|\vec{S}_t(\vec{x}, c) -  \vec{S}_t(\vec{x})|=(1/(s(t)\sigma(t)^2)\;  |\langle \vec a\rangle_{\vec x, c}-\langle \vec a\rangle_{\vec x} |$, where $s(t)$ and $\sigma(t)$ are related to the functions $f(t)$ and $g(t)$ of \Cref{eqn:original_ou_process} by 
$s(t)=\exp\int _0^t d\tau f(\tau)$ and $\sigma(t)=\int_0^t d\tau g(\tau)^2/s(\tau)^2$.
Therefore the non-linear function depends on the difference between the estimators of the initial value $\vec a$, given $\vec x(t)$, in the class and in the full distribution. This difference is typically a function that decreases with the time of the backward process. 
This suggests to use a non-linear CFG of the form 

\begin{align}
 \vec S_{t}^\textrm{RPL}(\vec{x},c)
 &= 
 \vec{S}_t(\vec{x}, c) + \omega \; \left[\vec{S}_t(\vec{x}, c) -  \vec{S}_t(\vec{x})\right] \; 
\left|\langle \vec a\rangle_{\vec x, c}-\langle \vec a\rangle_{\vec x} \right|^\gamma\nonumber\\
&= 
\vec{S}_t(\vec{x}, c) + \omega \; \left[\vec{S}_t(\vec{x}, c) -  \vec{S}_t(\vec{x})\right] \; 
\left|\vec{S}_t(\vec{x}, c) -  \vec{S}_t(\vec{x})\right|^{\gamma} s(t)^\gamma \sigma(t)^{2\gamma}
\label{eqn:nonlin_cfg2}
\end{align}

with positive $\gamma$. 
As we will show in Figures \ref{fig:newnonlin1}-\ref{fig:newnonlin2}, this non-linear guidance term has interesting performance in terms of combining a rapid drift toward the desired class $c$ at early stages of the backward process together with small bias in the finite distribution in finite dimensional problems. 

The behavior of both versions is further displayed in Figure \ref{fig:large}: both non-linear versions can yield smaller bias at $t=0$. Furthermore, Figure \ref{fig:large} also displays additional experiments highlighting the benefits of non-linear versions.

\begin{figure}[ht]
    \centering
     \includegraphics[width=0.35\linewidth]
{assets/plots/q_nlin_score1_p75_omegap5_1p5_score2_x_exposant_4_omega_8_16_lin.pdf}
    \caption{
    $\langle q\rangle $ versus time in the Gaussian binary mixture with $d=16$ and $\sigma^2=4$. The dashed line are obtained by the standard linear CFG with $\omega=0,8,16$ from bottom to top. The dotted line are obtained with the Power-Law non-linear scheme $f(x)=\omega x^{-.75}$ with $\omega=.5, 1.5 $ from bottom to top. The full lines are obtained with the non-linear guidance of Eq.(\ref{eqn:nonlin_cfg2}) with $\gamma=4$ and $\omega=8, 16$
    from bottom to top. The Rescaled Power-law non-linear scheme departs from $q=0$ at large time on a trajectory similar to the linear scheme and to the Power-Law non-linear scheme. But it gives a smaller bias at $t=0$.}
    \label{fig:newnonlin1}
\end{figure}

\begin{figure}[ht]
    \centering
     \includegraphics[width=0.325\linewidth]
    {assets/plots/aexpect_diff_nlin_score1_p75_omegap5_1p5_score2_x_exposant_4_omega_8_16_lin.pdf}
    \includegraphics[width=0.325\linewidth]
    {assets/plots/norm_diff_nlin_score1_p75_omegap5_1p5_score2_x_exposant_4_omega_8_16_lin.pdf}
    \caption{We perform the same experiment as in Fig. \ref{fig:newnonlin1}.
    We plot on the left the value of $|\langle \vec a\rangle_{\vec x=\vec 0, c}-\langle \vec a\rangle_{\vec x=\vec 0} |$, on the right the value of $|S_t(\vec{x}, a) -  S_t(\vec{x})|$, with the same linestyle and color code as in Fig. \ref{fig:newnonlin1}.
    }
    \label{fig:newnonlin2}
\end{figure}


\newcommand{\labelfiglarge}{\label{fig:large}}


\ifarxiv
    \begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{assets/plots/9_newfig.pdf}
        % \caption{First version of non-linear CFG.}
        % \label{fig:9_nonlin_v1}
        \end{subfigure}
        
        \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{assets/plots/12_newfig.pdf}
        % \caption{Second version of non-linear CFG.}
        % \label{fig:10_nonlin_v2}
        \end{subfigure}
        \caption[Caption for LOF]{Real-world experiments using DiT/XL-2 \citep{peebles2023scalable} trained on ImageNet-1000 \citep{deng2009imagenet}: randomly selected class with $\omega=4$, using DDPM \citep{ho2020denoisingdiffusionprobabilisticmodels} with $250$ sampling steps, averaged over $25$ samples. 
        \textbf{First column:} Power-Law CFG. \textbf{Second column:} Rescaled Power-Law CFG (\ref{eqn:nonlin_cfg2}).
        \textbf{Left column:} Jensen-Shannon Divergence between the embedded data points corresponding to randomly selected class and the generated samples as a function of reverse time $\tau$.
        \textbf{Middle column:} mean dot product of the normalized class centroid and the diffusion trajectories $\vec{x}\cdot\vec{c_i}/\|\vec{c_i}\|$ (both in the latent space) as a function of reverse time $\tau$. 
        \textbf{Right column:} Evolution of the distance between conditional and unconditional scores. From all three plots, we can see that using first (second) version of non-linear CFG with $\alpha<0$ ($\gamma>0$) results in paths that have smaller JSD, estimated as in \citet{wang2009divergence}, throughout the whole trajectory and smaller overshoot of the distribution's mean at $\tau=0$. 
        We can also see that the score difference $|S_\tau(x,c)-S_\tau(x)|$ has the same qualitative behavior as in numerical simulations of Gaussian mixtures.
        }
        \labelfiglarge
    \end{figure*}
\else
    \begin{figure*}[t]
    \centering
    \begin{subfigure}
    \centering
    \includegraphics[width=.8\textwidth]{assets/plots/9_newfig.pdf}
    % \caption{First version of non-linear CFG.}
    % \label{fig:9_nonlin_v1}
    \end{subfigure}
    
    \begin{subfigure}
    \centering
    \includegraphics[width=.8\textwidth]{assets/plots/12_newfig.pdf}
    % \caption{Second version of non-linear CFG.}
    % \label{fig:10_nonlin_v2}
    \end{subfigure}
    \caption[Caption for LOF]{Real-world experiments using DiT/XL-2 \citep{peebles2023scalable} trained on ImageNet-1000 \citep{deng2009imagenet}: randomly selected class with $\omega=4$, using DDPM \citep{ho2020denoisingdiffusionprobabilisticmodels} with $250$ sampling steps, averaged over $25$ samples. 
    \textbf{First column:} Power-Law CFG. \textbf{Second column:} Rescaled Power-Law CFG (\ref{eqn:nonlin_cfg2}).
    \textbf{Left column:} Jensen-Shannon Divergence between the embedded data points corresponding to randomly selected class and the generated samples as a function of reverse time $\tau$.
    \textbf{Middle column:} mean dot product of the normalized class centroid and the diffusion trajectories $\vec{x}\cdot\vec{c_i}/\|\vec{c_i}\|$ (both in the latent space) as a function of reverse time $\tau$. 
    \textbf{Right column:} Evolution of the distance between conditional and unconditional scores. From all three plots, we can see that using first (second) version of non-linear CFG with $\alpha<0$ ($\gamma>0$) results in paths that have smaller JSD, estimated as in \citet{wang2009divergence}, throughout the whole trajectory and smaller overshoot of the distribution's mean at $\tau=0$. 
    We can also see that the score difference $|S_\tau(x,c)-S_\tau(x)|$ has the same qualitative behavior as in numerical simulations of Gaussian mixtures.
    }
    \labelfiglarge
    \end{figure*}
\fi


\section{Applying Classifier-Free Guidance in a limited interval}
\label{sec:appx_stepwise_cfg}


In \citet{kynkaanniemi2024applying}, the authors propose to apply standard CFG within a limited time interval. Their method therefore applies the following schedule for the classifier parameter $\omega$:

\begin{align*}
w(\sigma_t)= \begin{cases}w & \text { if } \sigma_t \in\left(t_1, t_2\right] \\ 0 & \text { otherwise. }\end{cases}
\end{align*}

Here, $\sigma_t$ represents the noise level at time $t$ of the forward process. Therefore, $t_1$ and $t_2$ correspond to points in the sampling chain in which they disable and enable guidance respectively. These are treated as additional hyperparameters over which grid search is performed (alongside $\omega$). The standard CFG is recovered with $t_1=0$ and $t_2=\infty$.


By performing the reparameterization introduced in Section \ref{sec:time_reparameterization}, we can map $t_1$ and $t_2$ (obtained by minimizing the FID and $FD_{DINOv2}$ objectives respectively) to our timescale. The findings are shown in Figure \ref{fig:estim_apply_cfg}: on the left, the hyperparameter configuration that minimizes the FID objective is obtained with $\omega=2.5$, $t_1=0.34$ and $t_2=1.04$. The hyperparameter configuration that minimizes the test $FD_{DINOv2}$ loss corresponds to $\omega=4.0$, $t_1=0.45$ and $t_2=1.23$.

Our findings suggest that stopping CFG just before the score difference begins to decay may coincide with the speciation time. Interestingly, delaying the start of guidance appears to have a positive impact on the FID metric, despite theoretical expectations. We attribute this phenomenon to imperfect score estimation, particularly at high noise levels, which deviates from our assumption of a perfect score estimator. Further investigation of this observation is left for future research.

Additionally, we experimented with stopping guidance after the score difference becomes significantly small, as theory suggests this should not affect generation in Regime \rom{2} and is more computationally efficient. While this held true in practice, it was only observed in the EDM2 framework, whereas other scores remained substantial until almost the end, as shown in Figures \ref{fig:6_cfg_score_diff} and \ref{fig:estim_apply_cfg}.


\subsection{Estimating the starting and stopping point}

As described in the main manuscript, we employ an initial set of warm-up runs to estimate $t_1$ and $t_2$. Specifically, we utilize the average of $1,000$ paths to determine the first timestep at which the score difference $|S_t(\vec{x},c)-S_t(\vec{x})|$ surpasses its mean value, thereby obtaining $t_2$. Subsequently, we identify the timestep corresponding to the mode of the curve, marking the onset of decay, which yields $t_1$. A comparison between the $t_1$ and $t_2$ values obtained through hyperparameter optimization and our proposed heuristic is presented in Figure \ref{fig:estim_apply_cfg}.



\begin{figure}[ht]
    \centering
    \begin{NiceTabular}{ccc}
        \includegraphics[width=0.3\textwidth]{assets/plots/appx_dit.pdf} &
        \includegraphics[width=0.3\textwidth]{assets/plots/appx_edm2.pdf} &
        \includegraphics[width=0.3\textwidth]{assets/plots/appx_mmdit.pdf}
    \end{NiceTabular}
      \caption[Caption for LOF]{Timescales $t_2$ (left) and $t_1$ (right) at which Standard CFG is turned on and off respectively. Timesteps obtained through hyperparameter sweep are portrayed with red dashed lines, and using our heuristic in green solid lines. Notably, our heuristic achieves comparable performance to the baseline for DiT/XL-2 and EDM2-S, while requiring two less hyperparameters. This reduction in hyperparameters can be particularly advantageous in computationally expensive applications, such as diffusion models. Our experiments with the MMDiT (diffusion) model yielded smaller improvement in the FID, whereas with the MDTv2 architecture experiments revealed that limiting the guidance interval did not yield significant benefits, even after conducting an exhaustive grid search over $t_1$ and $t_2$. }
      \label{fig:estim_apply_cfg}
\end{figure}


\input{generated_pictures}

\end{document}



