\section{Related Work}
Hallucinations in LLMs were proved to be inevitable \citep{xu_hallucination_2024}, and to detect them, one can leverage either \textit{black-box} or \textit{white-box} approaches. The former approach uses only the outputs from an LLM, while the latter uses hidden states, attention maps, or logits corresponding to generated tokens.

Black-box approaches focus on the text generated by LLMs. For instance, \citep{li_dawn_2024} verified the truthfulness of factual statements using external knowledge sources, though this approach relies on the availability of additional resources. Alternatively, \textit{SelfCheckGPT} \citep{manakul_selfcheckgpt_2023} generates multiple responses to the same prompt and evaluates their consistency, with low consistency indicating potential hallucination.

White-box methods have emerged as a promising approach for detecting hallucinations \citep{farquhar_detecting_2024, azaria_internal_2023, arteaga_hallucination_2024, orgad_llms_2025}. These methods are universal across all LLMs and do not require additional domain adaptation compared to black-box ones \citep{farquhar_detecting_2024}. They draw inspiration from seminal works on analysing the internal states of simple neural networks \citep{alain_understanding_2016}, which introduced \textit{linear classifier probes} -- models operating on the internal states of neural networks. Linear probes have been widely applied to the internal states of LLMs, e.g., for detecting hallucinations. 

One of the first such probes was SAPLMA \citep{azaria_internal_2023}, which demonstrated that one could predict the correctness of generated text straight from LLM's hidden states. Further, the INSIDE method \citep{chen_inside_2024} tackled hallucination detection by sampling multiple responses from an LLM and evaluating consistency between their hidden states using a normalised sum of the eigenvalues from their covariance matrix. Also, \citep{farquhar_detecting_2024} proposed a complementary probabilistic approach, employing entropy to quantify the model's intrinsic uncertainty. Their method involves generating multiple responses, clustering them by semantic similarity, and calculating Semantic Entropy using an appropriate estimator. To address concerns regarding the validity of LLM probes, \citep{marks_geometry_2024} introduced a high-quality QA dataset with simple \textit{true}/\textit{false} answers and causally demonstrated that the truthfulness of such statements is linearly represented in LLMs, which supports the use of probes for short texts.

Self-consistency methods \citep{liang_internal_2024}, like INSIDE or Semantic Entropy, require multiple runs of an LLM for each input example, which substantially lowers their applicability. Motivated by this limitation, \citep{kossen_semantic_2024} proposed to use \textit{Semantic Entropy Probe}, which is a small model trained to predict expensive Semantic Entropy \citep{farquhar_detecting_2024} from LLM's hidden states. Notably, \citep{orgad_llms_2025} explored how LLMs encode information about truthfulness and hallucinations. First, they revealed that truthfulness is concentrated in specific tokens. Second, they found that probing classifiers on LLM representations do not generalise well across datasets, especially across datasets requiring different skills. Lastly, they showed that the probes could select the correct answer from multiple generated answers with reasonable accuracy, which they concluded with the LLM making mistakes at the decoding stage besides knowing the correct answer.

Recent studies have started to explore hallucination detection exclusively from attention maps. \citep{chuang_lookback_2024} introduced the \textit{lookback ratio}, which measures how much attention LLMs allocate to relevant input parts when answering questions based on the provided context. The work most closely related to ours is \citep{sriramanan_llm-check_2024}, which introduces the $\attnscore$ method. Although the process is unsupervised and computationally efficient, the authors note that its performance can depend highly on the specific layer from which the score is extracted. We also demonstrate that it performs poorly on the datasets we evaluated. Nonetheless, we drew inspiration from their approach, particularly using the lower triangular structure of matrices when constructing features for the hallucination probe.