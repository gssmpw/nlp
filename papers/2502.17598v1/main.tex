% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% our packages
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{subcaption}
\usepackage{rotating}
\usepackage[shortlabels]{enumitem}\setlist[enumerate, 1]{1.}
\usepackage{listings}

% ------ Symbols -------
\newcommand\topk{\operatorname{top-k}}

\newcommand\attnscore{\operatorname{AttentionScore}}
\newcommand\attneig{\operatorname{AttnEigvals}}
\newcommand\lapeig{\operatorname{LapEigvals}}
\newcommand\attnlogdet{\operatorname{AttnLogDet}}


\newcommand\diag[1]{\operatorname{diag\left(#1\right)}}
\newcommand\sort[1]{\operatorname{sort\left(#1\right)}}
\DeclareMathOperator*{\concat}{%
    \mathchoice%
        {\Big\Vert}%
        {\big\Vert}%
        {\Vert}%
        {\Vert}%
}

\newcommand\todo[1]{\noindent\textbf{\textcolor{red}{$\rightarrow$ #1}}}
\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}

\lstdefinestyle{prompt}{
    basicstyle=\small\ttfamily,
    backgroundcolor=\color{aliceblue},
    frame=single,
    framesep=5pt,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}


\title{Hallucination Detection in LLMs Using Spectral Features of Attention Maps}


\author{
 \textbf{Jakub Binkowski\textsuperscript{1}},
 \textbf{Denis Janiak\textsuperscript{1}},
 \textbf{Albert Sawczyn\textsuperscript{1}},
 \textbf{Bogdan Gabrys\textsuperscript{2}},
 \textbf{Tomasz Kajdanowicz\textsuperscript{1}}
\\
 \textsuperscript{1}Wroclaw University of Science and Technology,
 \textsuperscript{2}University of Technology Sydney,
\\
\small{
   \textbf{Correspondence:} \href{mailto:jakub.binkowski@pwr.edu.pl}{jakub.binkowski@pwr.edu.pl}
 }
}

\begin{document}
\newcommand{\llamabig}{\texttt{Llama-3.1-8B}}
\newcommand{\llamasmall}{\texttt{Llama-3.2-3B}}
\newcommand{\philllm}{\texttt{Phi-3.5}}
\newcommand{\llmjudge}{\textit{llm-as-judge}}
\newcommand{\gptmini}{\texttt{gpt-4o-mini}}

\maketitle

\begin{abstract}
Large Language Models (LLMs) have demonstrated remarkable performance across various tasks but remain prone to hallucinations. Detecting hallucinations is essential for safety-critical applications, and recent methods leverage attention map properties to this end, though their effectiveness remains limited. In this work, we investigate the spectral features of attention maps by interpreting them as adjacency matrices of graph structures. We propose the $\lapeig$ method, which utilises the top-$k$ eigenvalues of the Laplacian matrix derived from the attention maps as an input to hallucination detection probes. Empirical evaluations demonstrate that our approach achieves state-of-the-art hallucination detection performance among attention-based methods. Extensive ablation studies further highlight the robustness and generalisation of $\lapeig$, paving the way for future advancements in the hallucination detection domain.
\end{abstract}

\section{Introduction}
The recent surge of interest in Large Language Models (LLMs), driven by their impressive performance across various tasks, has led to significant advancements in their training, fine-tuning, and application to real-world problems. Despite progress, many challenges remain unresolved, particularly in safety-critical applications where the cost of errors is high. A significant issue is that LLMs are prone to hallucinations, i.e. generating "content that is nonsensical or unfaithful to the provided source content" \citep{farquhar_detecting_2024, huang_survey_2023}. Since eliminating hallucinations is impossible \citep{lee_mathematical_2023,xu_hallucination_2024}, there is a pressing need for methods to detect when a model produces hallucinations. In addition, uncovering internal behaviour while studying hallucinations of LLMs might reveal significant progress in understanding their characteristics, fostering further development in the field. Recent studies have shown that hallucinations can be detected using internal states of the model, e.g., hidden states \citep{chen_inside_2024} or attention maps \citep{chuang_lookback_2024}, and that LLMs can internally "know when they do not know" \citep{azaria_internal_2023, orgad_llms_2025}. We provide new insights showing that spectral features of attention maps coincide with hallucinations, and based on that observation, we introduce a novel method for detecting hallucinations.

As highlighted by \citep{barbero_transformers_2024}, attention maps can be viewed as weighted adjacency matrices of graphs. Building on this perspective, we performed statistical analysis and demonstrated that the eigenvalues of a Laplacian matrix derived from attention maps serve as good predictors of hallucinations. We propose the $\lapeig$ method, which utilises the top-$k$ eigenvalues of the Laplacian as input features of a probing model to detect hallucinations. We share full implementation in a public repository: \url{https://github.com/graphml-lab-pwr/lapeig}. 

We summarise our contributions as follows:
\begin{enumerate}[(1)]
    \itemsep0em 
    \item We perform statistical analysis of the Laplacian matrix derived from attention maps and show that it could serve as a better predictor of hallucinations compared to the previous method relying on the log-determinant of the maps.
    \item Building on that analysis and advancements in the graph-processing domain, we propose leveraging the top-$k$ eigenvalues of the Laplacian matrix as features for hallucination detection probes and empirically show that it achieves state-of-the-art performance among attention-based approaches.
    \item Through extensive ablation studies, we demonstrate properties, robustness and generalisation of $\lapeig$ and suggest promising directions for further development.
\end{enumerate}

\section{Motivation}
\label{sec:motivation}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/motivation/laplacian_vs_attn_score_pvalues_trivia_qa_llama_3.1_8b_instruct_temp_0.1_attentions_only__prompt_qa_short_few_shot_sep__seed_42.png}
    \caption{Visualisation of $p$-values from the two-sided Mann-Whitney U test for all layers and heads of \llamabig\ across two feature types: $\attnscore$ and the $k=10$ Laplacian eigenvalues. These features were derived from attention maps collected when the LLM answered questions from the TriviaQA dataset. Higher $p$-values indicate no significant difference in feature values between hallucinated and non-hallucinated examples. For $\attneig$, $80\%$ of heads have $p<0.05$, while for Laplacian eigenvalues, this percentage is $91\%$. Therefore, Laplacian eigenvalues may be better predictors of hallucinations, as feature values across more heads exhibit statistically significant differences between hallucinated and non-hallucinated examples.}
    \label{fig:motivation}
\end{figure*}

Considering the attention matrix as an adjacency matrix representing a set of Markov Chains, each corresponding to one layer of an LLM \citep{barbero_transformers_2024} (Figure \ref{fig:graph}), we can leverage its spectral properties, as was done in many successful graph-based methods \citep{hahn_applications_1997,von_luxburg_tutorial_2007,bruna_spectral_2013, topping_understanding_2022}. In particular, it was shown that graph Laplacian might help to describe several graph properties, like the presence of bottlenecks \citep{topping_understanding_2022, black_understanding_2023}. We hypothesise that hallucinations may be related to disturbance of information flow caused by some form of bottleneck.

To assess whether our hypothesis holds, we measured if graph spectral features provide a stronger coincidence with hallucinations than the previous attention-based method - $\attnscore$ \citep{sriramanan_llm-check_2024}. We prompted an LLM with questions from the TriviaQA dataset \citep{joshi_triviaqa_2017} and extracted attention maps, differentiating by layers and heads. We then computed the spectral features, i.e., the 10 largest eigenvalues of the Laplacian matrix from each head and layer. Further, we conducted a two-sided Mann-Whitney U test to compare whether Laplacian eigenvalues and the values of $\attnscore$ are different between hallucinated and non-hallucinated examples. Figure~\ref{fig:motivation} shows $p$-values for all layers and heads, indicating that $\attnscore$ often results in higher $p$-values compared to Laplacian eigenvalues. Overall, we studied 6 datasets and 2 LLMs and found similar results, and present all results in Appendix~\ref{sec:appendix_stat_test}. Based on these findings, we propose leveraging top-$k$ Laplacian eigenvalues as features for a hallucination probe.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/transformer_graph_inference.png}
    \caption{The autoregressive inference process in an LLM is depicted as a graph for a single attention head $h$ (as introduced by \citep{vaswani_attention_2017}) and three generated tokens ($\hat{x}_1, \hat{x}_2, \hat{x}_3$). Here, $\mathbf{h}^{(l)}_{i}$ represents the hidden state at layer $l$ for the input token $i$, while $a^{(l, h)}_{i, j}$ denotes the scalar attention score between tokens $i$ and $j$ at layer $l$ and attention head $h$. Arrows direction refers to information flow during inference.}
    \label{fig:graph}
\end{figure}

\section{Method}
\label{sec:method}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figures/methodology.png}
    \caption{Overview of the methodology used in this work. Solid lines indicate the test-time pipeline, while dashed lines represent additional pipeline steps for generating labels for training the hallucination probe (logistic regression). The primary contribution of this work is leveraging the top-$k$ eigenvalues of the Laplacian as features for the hallucination probe, highlighted with a bold box on the diagram.}
    \label{fig:methodology}
\end{figure*}

In our method, we train a hallucination probe using only attention maps extracted during LLM inference, as illustrated in Figure \ref{fig:graph}. The attention map is a matrix containing attention scores for all tokens processed during inference, while the hallucination probe is a logistic regression model that uses features derived from attention maps as input. This work's core contribution is using the top-$k$ eigenvalues of the Laplacian matrix as input features, which we detail below.

Denote $\mathbf{A}^{(l, h)} \in \mathbb{R}^{T \times T}$ as the attention map matrix for layer $l \in \{1\dotsc L\}$ and attention head $h \in \{1 \dotsc H\}$, where $T$ is the total number of tokens generated by an LLM (including input tokens), $L$ the number of layers (transformer blocks), and $H$ the number of attention heads. The attention matrix is row-stochastic, meaning each row sums to 1 ($\sum_{j=0}^{T} \mathbf{A}^{(l, h)}_{:,j} = \mathbf{1}$). It is also lower triangular ($a^{(l, h)}_{ij} = 1$ for all $j > i$) and non-negative ($a^{(l, h)}_{ij} \geq 0$ for all $i, j$). We can view $\mathbf{A}^{(l, h)}$ as a weighted adjacency matrix of a directed graph, where each node represents processed token, and each directed edge from token $i$ to token $j$ is weighted by the attention score, as depicted in Figure~\ref{fig:graph}.

Then, we define the Laplacian of a layer $l$ and attention head $h$ as:
\begin{equation}
    \mathbf{L}^{(l, h)} = \mathbf{D}^{(l, h)} - \mathbf{A}^{(l, h)},
\end{equation}
 where $\mathbf{D}^{(l, h)}$ is a diagonal degree matrix. Since the attention map defines a directed graph, we distinguish between the \textit{in-degree} and \textit{out-degree} matrices. The \textit{in-degree} is computed as the sum of attention scores from preceding tokens, and due to the softmax normalization, it is uniformly 1. Therefore, we define $\mathbf{D}^{(l, h)}$ as the \textit{out-degree} matrix, which quantifies the total attention a token receives from tokens that follow it. To ensure these values remain independent of the sequence length, we normalize them by the number of subsequent tokens (i.e., the number of outgoing edges).

\begin{equation}
     d^{(l,h)}_{ii} = \frac{\sum_{u}{a^{(l, h)}_{ui}}}{\sum_{v}{\mathbb{I}\left\{a^{(l, h)}_{vi} \neq 0\right\}}},
\end{equation}
where $\mathbb{I}$ is an indicator function, and $i, u,v \in \{1 \dots T\}$ denote token indices.  
Intuitively, the resulting Laplacian for each processed token represents the average attention score to previous tokens reduced by the attention score to itself. Due to the structure of LLM inference, this formulation ensures that the Laplacian remains a lower triangular matrix. Hence, diagonal entries are eigenvalues, and there is no need for expensive eigendecomposition. 

A recent study by \citep{orgad_llms_2025} has demonstrated that selecting the appropriate token and layer to take hidden states from is complex and significantly affects hallucination detection. Conversely, \citep{zhu_pollmgraph_2024} found that using the entire sequence of hidden states, rather than a single token, improves the detection performance. In addition, \citep{kim_detecting_2024} showed that using information from all layers instead of one in isolation leads to better performance on this task. Motivated by these results, in our method, we consider all layers and all tokens.

Therefore, we take eigenvalues of $\mathbf{L}^{(l, h)}$, which are diagonal entries due to the lower triangularity of the matrix, and sort them:
\begin{equation}
    \Tilde{z}^{(l, h)} = \sort{\diag{\mathbf{L}^{(l, h)}}}
\end{equation}
Finally, we take the top-$k$ largest values and concatenate them into a single feature vector $z$, where $k$ is a hyperparameter of our method:
\begin{equation}
    z = \concat_{\forall l \in L, \forall h \in H} \left[\Tilde{z}^{(l, h)}_{T}, \Tilde{z}^{(l, h)}_{T-1}, \dotsc, \Tilde{z}^{(l, h)}_{T-k}\right]
\end{equation}
Since LLMs contain dozens of layers and heads, vector $z$ would suffer from large dimensionality. Thus, we decompose it to lower dimensionality using the PCA \citep{jolliffe_principal_2016}. We call our approach $\lapeig$.
 
\section{Experimental setup}

\subsection{Dataset construction}
We use annotated QA datasets to construct the hallucination detection datasets and label incorrect LLM answers as hallucinations. To determine whether the generated answers were correct, we adopted the \llmjudge\ approach \citep{zheng_judging_2023}, as in previous studies \citep{orgad_llms_2025}. Specifically, we prompted a large LLM to classify each response as either \textit{hallucination}, \textit{non-hallucination}, or \textit{rejected}, where \textit{rejected} indicates that it was unclear whether the answer was correct, e.g., the model refused to answer due to insufficient knowledge. Based on the manual qualitative inspection of several LLMs, we employed \gptmini\ \citep{openai_gpt-4_2024} as the judge model since it provides the best trade-off between accuracy and cost.

For experiments, we selected 6 QA datasets previously utilised in the context of hallucination detection \citep{chen_inside_2024, kossen_semantic_2024, chuang_dola_2024, mitra_factlens_2024}. Specifically, we used the validation set of NQOpen \citep{kwiatkowski_natural_2019}, comprising $3610$ question-answer pairs, and the validation set of TriviaQA \citep{joshi_triviaqa_2017}, containing 7983 pairs. To evaluate our method on longer inputs, we employed the development set of CoQA \citep{reddy_coqa_2019} and the \textit{rc.nocontext} portion of the SQuADv2 \citep{rajpurkar_know_2018} datasets, with 5928 and 9960 examples, respectively. Additionally, we incorporated the QA part of the HaluEval \citep{li_halueval_2023} dataset, containing $10000$ examples, and the \texttt{generation} part of the TruthfulQA \citep{lin_truthfulqa_2022} benchmark with 817 examples. For TriviaQA, CoQA, and SQuADv2, we followed the same preprocessing procedure as \citep{chen_inside_2024}.

We generate answers using 3 open-source LLMs: \llamabig\footnote{\href{https://hf.co/meta-llama/Llama-3.1-8B-Instruct}{hf.co/meta-llama/Llama-3.1-8B-Instruct}} and \llamasmall\footnote{\href{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}{hf.co/meta-llama/Llama-3.2-3B-Instruct}} \citep{grattafiori_llama_2024}, \philllm\footnote{\href{https://huggingface.co/microsoft/Phi-3.5-mini-instruct}{hf.co/microsoft/Phi-3.5-mini-instruct}} \citep{abdin_phi-3_2024}. We use two \texttt{softmax} temperatures for each LLM when decoding ($temp \in \{0.1, 1.0\}$) and one prompt (showed on Listing~\ref{lst:p3}). Overall, we evaluated hallucination detection probes on 6 LLM configurations and 6 QA datasets. We present the frequency of classes for answers from each configuration in Figure~\ref{fig:ds_sizes} (Appendix~\ref{sec:ds_sizes}).

\subsection{Hallucination Probe}
As a hallucination probe, we take a logistic regression model, using the implementation from scikit-learn \citep{pedregosa_scikit-learn_2011} with all parameters default, except for ${max\_iter=2000}$ and ${class\_weight={''balanced''}}$. For top-$k$ eigenvalues, we tested 5 values of $k \in \{5, 10, 20, 50, 100\}$\footnote{For datasets with examples having less than 100 tokens, we stop at $k=50$} and selected the result with the highest efficacy. All eigenvalues are projected with PCA onto 512 dimensions, except in \textit{per-layer} experiments where there may be fewer than 512 features. In these cases, we apply PCA projection to match the input feature dimensionality, i.e., decorrelating them. As an evaluation metric, we use AUROC on the test split.

\subsection{Baselines}
Our method is a supervised approach to detect hallucinations solely from attention maps. For a fair comparison, we modify unsupervised $\attnscore$ \citep{sriramanan_llm-check_2024} to take log-determinants for each head as features instead of summing them. We also add original $\attnscore$ with the summation over heads for a reference. To evaluate the effectiveness of our proposed Laplacian eigenvalues, we also compare it to using raw attention maps and call it $\attneig$. Additionally, in Appendix~\ref{sec:appendix_detailed_results}, we provide results for each approach but \textit{per-layer}.

\subsection{Implementation details}
In our experiments, we used HuggingFace Transformers \citep{wolf_transformers_2020}, PyTorch \citep{ansel_pytorch_2024}, and scikit-learn \citep{pedregosa_scikit-learn_2011}. We utilised Pandas \citep{team_pandas-devpandas_2020} and Seaborn \citep{waskom_seaborn_2021} for visualisations and analysis. To version data, we employed DVC \citep{ruslan_kuprieiev_dvc_2025}. We acquired attention maps using a single Nvidia A40 with 40GB VRAM, remaining computations were carried out on CPU. To compute labels using the \llmjudge\ approach, we leveraged OpenAI API. Detailed hyperparameter configurations and code to reproduce the experiments is available in the public Git repository.

\section{Results}
\label{sec:results}

\begin{table*}[!htb]
    \centering
    \caption{Test AUROC for $\lapeig$ and several baseline methods. AUROC values were obtained in the single run of logistic regression training on features from a dataset generated with $temp=1.0$. We marked results for $\attnscore$ in \textcolor{gray}{gray} as it is unsupervised approach, not directly comparable to the other ones. In \textbf{bold}, we highlight the best performance individually for each dataset and LLM. See Appendix~\ref{sec:appendix_detailed_results} for extended results.}
    \label{tab:main_results}
    \small
    \input{tables/main_paper_table}
\end{table*}

Table~\ref{tab:main_results} presents the results of our method compared to the baselines. $\lapeig$ achieved the best performance among all tested methods on 5 out of 6 datasets. Moreover, our method consistently performs well across all three LLM architectures. TruthfulQA was the only exception where $\lapeig$ was the second-best approach, yet it might stem from the small size of the dataset or severe class imbalance (depicted in Figure~\ref{fig:ds_sizes}). In contrast, using eigenvalues of vanilla attention maps in $\attneig$ leads to worse performance, which suggests that transformation to Laplacian is the crucial step to uncover latent features of an LLM corresponding to hallucinations. In Appendix~\ref{sec:appendix_detailed_results}, we show that $\lapeig$ consistently demonstrates a smaller generalisation gap, i.e., the difference between training and test performance is smaller for our method. While the $\attnlogdet$ method performed poorly, it is fully unsupervised and should not be directly compared to other approaches. However, its supervised counterpart -- $\attnlogdet$ -- remains inferior to methods based on spectral features, namely $\lapeig$ and $\attneig$. In Table~\ref{tab:detailed_results} in Appendix~\ref{tab:detailed_results}, we present extended results, including \textit{per-layer} and \textit{all-layers} breakdowns, two temperatures used during answer generation, and a comparison between training and test AUROC.

\section{Ablation studies}
To better understand the behaviour of our method under different conditions, we conduct a comprehensive ablation study. This analysis provides valuable insights into the factors driving the $\lapeig$ performance and highlights the robustness of our approach across various scenarios. In order to ensure reliable results, we perform all studies on the TriviaQA dataset, which has a reasonable input size and number of examples.

\subsection{How does the number of eigenvalues influence performance?}
First, we verify how the number of eigenvalues influences the performance of the hallucination probe and present results in Figure~\ref{fig:top_k_ablation}. Generally, using more eigenvalues improves performance, but there is less variation in performance among different values of $k$ for $\lapeig$. Moreover, $\lapeig$ achieves significantly better performance with smaller input sizes, as $\attneig$ with the largest $k=100$ fails to surpass $\lapeig$'s performance at $k=5$. These results confirm that spectral features derived from the Laplacian carry a robust signal indicating the presence of hallucinations and highlight the strength of our method.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\columnwidth]{figures/ablation_top_k/topk_auc_prompt_qa_short_few_shot_sep_1.0_100.png}
    \caption{Model performance across different values of top-$k$ eigenvalues: $k \in \{5, 10, 25, 50, 100\}$ for TriviQA dataset with $temp=1.0$.}
    \label{fig:top_k_ablation}
\end{figure}

\subsection{Does using all layers at once improve performance?}
Second, we demonstrate that using all layers of an LLM instead of a single one improves performance. In Figure~\ref{fig:layer_idx_ablation}, we compare \textit{per-layer} to \textit{all-layer} efficacy. For the \textit{per-layer} approach, better performance is generally achieved in later LLM layers. Notably, peak performance varies across LLMs, requiring an additional search for each new LLM. In contrast, the \textit{all-layer} probes consistently outperform the best \textit{per-layer} probes across all LLMs. This finding suggests that information indicating hallucinations is spread across many layers of LLM, and considering them in isolation limits detection accuracy. Further, Table~\ref{tab:detailed_results} in Appendix~\ref{sec:appendix_detailed_results} summarises outcomes for the two variants on all datasets and LLM configurations examined in this work.

\label{sec:all_layers_vs_per_layer}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\columnwidth]{figures/ablation_layer_idx/layer_idx_auc_prompt_qa_short_few_shot_sep_1.0_100.png}
    \caption{Analysis of model performance across different layers and LLMs for TriviQA dataset with $temp=1.0$ and $k=100$ top eigenvalues (results for models operating on all layers provided for reference).}
    \label{fig:layer_idx_ablation}
\end{figure}

\subsection{Does sampling temperature influence results?}
\label{sec:ablation_temperature}

Here, we compare $\lapeig$ to baselines on hallucination datasets produced with several temperatures used during decoding. Higher temperatures typically produce more hallucinated examples \citep{lee_mathematical_2023, renze_effect_2024}, leading to dataset imbalance. Thus, to mitigate the effect of data imbalance, we sample a subset of $1000$ hallucinated and $1000$ non-hallucinated examples $10$ times for each temperature and train hallucination probes. Interestingly, in Figure~\ref{fig:temperature_ablation}, we observe that all models improve their performance at higher temperatures, but $\lapeig$ consistently achieves the best accuracy on all considered temperature values. The correlation of efficacy with temperature may be attributed to differences in the characteristics of hallucinations at higher temperatures compared to lower ones \citep{renze_effect_2024}. Also, hallucination detection might be facilitated at higher temperatures due to underlying properties of \texttt{softmax} function \citep{velickovic_softmax_2024}, and further exploration of this direction is left for future work.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\columnwidth]{figures/ablation_temperature/ratio_over_balanced_llama_3.1_8b_instruct__prompt_qa_short_few_shot_sep__seed_42__top_eigval_100__pca_dim_512_eigvals_100_samples_10.png}
    \caption{Test AUROC for different sampling $temp$ values during answer decoding on the TriviaQA dataset, using $k=100$ eigenvalues for $\lapeig$ and $\attneig$ with the \llamabig\ LLM. Error bars indicate the standard deviation over 10 balanced samples containing $N=1000$ examples per class.}
    \label{fig:temperature_ablation}
\end{figure}

\subsection{How does \texorpdfstring{$\lapeig$}{\textit{LapEigvals}} generalizes?}
\label{sec:generalization}
To check whether our method generalises across datasets, we trained the hallucination probe on features from the training split of one QA dataset and evaluated it on the features from the test split of a different QA dataset. Due to space limitations, we present results for selected datasets and provide extended results and absolute efficacy values in Appendix~\ref{sec:appendix_generalization}. Figure \ref{fig:generalization_main} showcases the percentage drop in Test AUROC when using a different training dataset compared to training and testing on the same QA dataset. We can observe that $\lapeig$ provides a performance drop comparable to other baselines, and in several cases, it generalises best. Interestingly, all methods exhibit poor generalisation on TruthfulQA, possibly due to dataset size or imbalance. Additionally, in Appendix~\ref{sec:appendix_generalization}, we show that $\lapeig$ achieves the highest test performance in all scenarios (except for TruthfulQA).
\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/ablation_generalization/generalization_drop_small_plot_llama_3.1_8b_instruct_temp_1.0_attentions_only__prompt_qa_short_few_shot_sep__seed_42_100.png}
    \caption{Generalisation across datasets measured as a per cent performance drop in Test AUROC (less is better) when trained on one dataset and tested on the other. Training datasets are indicated in the plot titles, while test datasets are shown on the $x$-axis. Results computed on \llamabig\ with $k=100$ top eigenvalues and $temp=1.0$. Results for all datasets are presented in Appendix~\ref{sec:appendix_generalization}.}
    \label{fig:generalization_main}
\end{figure*}

\subsection{How does performance vary across prompts?}
\label{sec:ablation_prompts}
Lastly, to assess the stability of our method across different prompts used for answer generation, we compared the results of the hallucination probes trained on features from four distinct prompts, the content of which is included in Appendix~\ref{sec:appendix_prompts}. As shown in Table~\ref{tab:prompt_comparison}, $\lapeig$ consistently outperforms all baselines across all four prompts. While we can observe variations in performance across prompts, $\lapeig$ demonstrates the lowest standard deviation ($0.05$) compared to $\attnlogdet$ ($0.016$) and $\attneig$ ($0.07$), indicating its greater robustness.

\begin{table}[!htb]
    \centering
    \caption{Test AUROC across four different prompts for answers on the TriviaQA dataset using \llamabig\, with $temp=1.0$ and $k=50$ (some prompts have led to fewer than 100 tokens). Prompt $\boldsymbol{p_3}$ was the main one used to compare our method to baselines, as presented in Tables~\ref{tab:main_results}.}
    \resizebox{\columnwidth}{!}{
        \input{tables/prompt_ablation}
    }
    \label{tab:prompt_comparison}
\end{table}

\section{Related Work}
Hallucinations in LLMs were proved to be inevitable \citep{xu_hallucination_2024}, and to detect them, one can leverage either \textit{black-box} or \textit{white-box} approaches. The former approach uses only the outputs from an LLM, while the latter uses hidden states, attention maps, or logits corresponding to generated tokens.

Black-box approaches focus on the text generated by LLMs. For instance, \citep{li_dawn_2024} verified the truthfulness of factual statements using external knowledge sources, though this approach relies on the availability of additional resources. Alternatively, \textit{SelfCheckGPT} \citep{manakul_selfcheckgpt_2023} generates multiple responses to the same prompt and evaluates their consistency, with low consistency indicating potential hallucination.

White-box methods have emerged as a promising approach for detecting hallucinations \citep{farquhar_detecting_2024, azaria_internal_2023, arteaga_hallucination_2024, orgad_llms_2025}. These methods are universal across all LLMs and do not require additional domain adaptation compared to black-box ones \citep{farquhar_detecting_2024}. They draw inspiration from seminal works on analysing the internal states of simple neural networks \citep{alain_understanding_2016}, which introduced \textit{linear classifier probes} -- models operating on the internal states of neural networks. Linear probes have been widely applied to the internal states of LLMs, e.g., for detecting hallucinations. 

One of the first such probes was SAPLMA \citep{azaria_internal_2023}, which demonstrated that one could predict the correctness of generated text straight from LLM's hidden states. Further, the INSIDE method \citep{chen_inside_2024} tackled hallucination detection by sampling multiple responses from an LLM and evaluating consistency between their hidden states using a normalised sum of the eigenvalues from their covariance matrix. Also, \citep{farquhar_detecting_2024} proposed a complementary probabilistic approach, employing entropy to quantify the model's intrinsic uncertainty. Their method involves generating multiple responses, clustering them by semantic similarity, and calculating Semantic Entropy using an appropriate estimator. To address concerns regarding the validity of LLM probes, \citep{marks_geometry_2024} introduced a high-quality QA dataset with simple \textit{true}/\textit{false} answers and causally demonstrated that the truthfulness of such statements is linearly represented in LLMs, which supports the use of probes for short texts.

Self-consistency methods \citep{liang_internal_2024}, like INSIDE or Semantic Entropy, require multiple runs of an LLM for each input example, which substantially lowers their applicability. Motivated by this limitation, \citep{kossen_semantic_2024} proposed to use \textit{Semantic Entropy Probe}, which is a small model trained to predict expensive Semantic Entropy \citep{farquhar_detecting_2024} from LLM's hidden states. Notably, \citep{orgad_llms_2025} explored how LLMs encode information about truthfulness and hallucinations. First, they revealed that truthfulness is concentrated in specific tokens. Second, they found that probing classifiers on LLM representations do not generalise well across datasets, especially across datasets requiring different skills. Lastly, they showed that the probes could select the correct answer from multiple generated answers with reasonable accuracy, which they concluded with the LLM making mistakes at the decoding stage besides knowing the correct answer.

Recent studies have started to explore hallucination detection exclusively from attention maps. \citep{chuang_lookback_2024} introduced the \textit{lookback ratio}, which measures how much attention LLMs allocate to relevant input parts when answering questions based on the provided context. The work most closely related to ours is \citep{sriramanan_llm-check_2024}, which introduces the $\attnscore$ method. Although the process is unsupervised and computationally efficient, the authors note that its performance can depend highly on the specific layer from which the score is extracted. We also demonstrate that it performs poorly on the datasets we evaluated. Nonetheless, we drew inspiration from their approach, particularly using the lower triangular structure of matrices when constructing features for the hallucination probe.

\section{Conclusions}
In this work, we demonstrated that the spectral features of LLMs' attention maps, specifically the eigenvalues of the Laplacian matrix, carry a signal capable of detecting hallucinations. Specifically, we proposed the $\lapeig$ method, which employs the top-$k$ eigenvalues of the Laplacian as input to the hallucination detection probe. Through extensive evaluations, we empirically showed that our method consistently achieves state-of-the-art performance among all tested approaches. Furthermore, multiple ablation studies demonstrated that our method remains stable across varying numbers of eigenvalues, diverse prompts, and generation temperatures while offering reasonable generalisation.

In addition, we hypothesise that self-supervised learning \citep{balestriero_cookbook_2023} could yield a more robust and generalisable approach while uncovering non-trivial intrinsic features of attention maps. Notably, results such as those in Section~\ref{sec:ablation_temperature} suggest intriguing connections to recent advancements in LLM research \citep{velickovic_softmax_2024, barbero_transformers_2024}, highlighting promising directions for future investigation.

\section*{Limitations}
\textit{\textbf{Supervised method}} In our approach, one must provide labelled hallucinated and non-hallucinated examples to train the hallucination probe. While this can be handled by the $\llmjudge$, it might introduce some noise or pose a risk of overfitting. \textit{\textbf{Limited generalisation across LLM architectures}} The method is incompatible with LLMs having different head and layer configurations. Developing architecture-agnostic hallucination probes is left for future work. \textit{\textbf{Minimum length requirement}} Computing $\topk$ Laplacian eigenvalues demands attention maps of at least $k$ tokens (e.g., $k=100$ require 100 tokens). \textbf{\textit{Open LLMs}} Our method requires access to the internal states of LLM thus it cannot be applied to closed LLMs.
\textit{\textbf{Risks}} Please note that the proposed method was tested on selected LLMs and English data, so applying it to untested domains and tasks carries a considerable risk without additional validation.

\section*{Acknowledgements}
We sincerely thank Piotr Bielak for his valuable review and insightful feedback, which helped improve this work. This work was funded by the European Union under the Horizon Europe grant OMINO – Overcoming Multilevel INformation Overload (grant number 101086321, \url{https://ominoproject.eu/}). Views and opinions expressed are those of the authors alone and do not necessarily reflect those of the European Union or the European Research Executive Agency. Neither the European Union nor the European Research Executive Agency can be held responsible for them. It was also co-financed with funds from the Polish Ministry of Education and Science under the programme entitled International Co-Financed Projects, grant no. 573977.  This work was co-funded by the National Science Centre, Poland under CHIST-ERA Open \& Re-usable Research Data \& Software  (grant number 2022/04/Y/ST6/00183).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{references}

\clearpage
\appendix
\section{Details of motivational study}
\label{sec:appendix_stat_test}
We present a detailed description of the procedure used to obtain the results presented in Section~\ref{sec:motivation} along with additional results for other datasets and LLM. 

To test whether there is a statistically significant difference in the values of $\attneig$ and Laplacian eigenvalues, we first took QA datasets and ran inference with three LLMs, namely \llamabig\, \llamasmall\, and \philllm\ . Then, we extracted attention maps and computed $\attnscore$ \citep{sriramanan_llm-check_2024}, i.e., log-determinant of attention maps. Unlike original work, we did not sum the scores over heads as we performed analysis at a single-head level of granularity. Also, we computed the Laplacian according to the definition presented in Section~\ref{sec:method}, took the 10 largest eigenvalues for each head, and treated each eigenvalue as a separate example. Finally, we ran the Mann–Whitney U test, leveraging SciPy implementation \citep{virtanen_scipy_2020}, and gathered $p$-values presented in Figure~\ref{fig:motivation}.

Table~\ref{tab:full_motivation_results} presents the percentage of heads having a statistically significant difference in feature values between hallucinated and non-hallucinated examples, as indicated by $p < 0.05$ from the Mann-Whitney U test. These results show that the Laplacian eigenvalues better distinguish between the two classes for all considered LLMs and datasets.

\begin{table}[htb]
    \centering
    \caption{Percentage of heads having a statistically significant difference in feature values between hallucinated and non-hallucinated examples, as indicated by $p < 0.05$ from the Mann-Whitney U test. Results were obtained for $\attnscore$ and the 10 largest Laplacian eigenvalues on 6 datasets and 3 LLMs.}
    \resizebox{\columnwidth}{!}{
        \input{tables/p_values_percentage}
    }
    \label{tab:full_motivation_results}
\end{table}


\section{Details of QA datasets}
In this work, we used 6 open and publicly available question answering datasets: NQ-Open \citep{kwiatkowski_natural_2019} (CC-BY-SA-3.0 license), SQuADv2 \citep{rajpurkar_know_2018} (CC-BY-SA-4.0 license), TruthfulQA (Apache-2.0 license) \citep{lin_truthfulqa_2022}, HALUEval (MIT license) \citep{li_halueval_2023}, CoQA \citep{reddy_coqa_2019} (domain-dependent licensing, detailed on \url{https://stanfordnlp.github.io/coqa/}), while TriviaQA (lacks clear licensing information, but was primarily shared as public benchmark). Research purposes fall into the intended use of these datasets. To preprocess and filter TriviaQA, CoQA, and SQuADv2 we utilized open-source code of \citep{chen_inside_2024}\footnote{\url{https://github.com/alibaba/eigenscore} (MIT license)}, which also borrows from \citep{farquhar_detecting_2024}\footnote{\url{https://github.com/lorenzkuhn/semantic_uncertainty} (MIT license)}.
In Figure \ref{fig:token_stats}, we provide histogram plots of the number of tokens for $question$ and $answer$ of each dataset computed with \texttt{meta-llama/Llama-3.1-8B-Instruct} tokeniser. 

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/token_stats/token_counts_CoQA.png}
        \caption{CoQA}
        \label{fig:token_coqa}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/token_stats/token_counts_NQOpen.png}
        \caption{NQOpen}
        \label{fig:tokens_nq_open}
    \end{subfigure}

    \vspace{1em}
    
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/token_stats/token_counts_HalueEvalQa.png}
        \caption{HaluEval}
        \label{fig:tokens_halueval_qa}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/token_stats/token_counts_SQuADv2.png}
        \caption{Squadv2}
        \label{fig:tokens_squad_v2}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/token_stats/token_counts_TriviaQA.png}
        \caption{TriviaQA}
        \label{fig:tokens_trivia_qa}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/token_stats/token_counts_TruthfulQA.png}
        \caption{TruthfulQA}
        \label{fig:tokens_truthful_qa}
    \end{subfigure}

    \caption{Token count histograms for datasets used in the experiments. Number of tokens determined separately for $question$ (left-hand-side plots) and gold $answer$ (right-hand-side plots) of each example in the datasets with \texttt{meta-llama/Llama-3.1-8B-Instruct} tokeniser (whenever multiple possible answers occurred, they were flattened).}
    \label{fig:token_stats}
\end{figure*}


\section{Hallucination dataset sizes}
\label{sec:ds_sizes}
In Figure \ref{fig:ds_sizes}, we show the number of examples for each label determined with the \llmjudge\ heuristic. It is worth noting that different generation configurations result in different splits, as LLMs might produce different answers. All examples classified as $Rejected$ were discarded from further experiments. We can observe that in most cases, datasets are imbalanced, underrepresenting non-hallucinated examples. Only for TriviaQA, there is an approximately balanced number of examples or even more non-hallucinated ones, depending on the configuration used. We split each dataset into 80\% training examples and 20\% test examples. Splits were stratified according to hallucination labels.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\textwidth, height=0.9\textheight, keepaspectratio]{figures/ds_sizes_summary.png}
    \caption{Number of examples per each label in generated datasets ($Hallucination$ - number of hallucinated examples, $Non{-}Hallucination$ - number of truthful examples, $Rejected$ - number of examples unable to evaluate). }
    \label{fig:ds_sizes}
\end{figure*}

\section{Extended results}
\label{sec:appendix_detailed_results}

\subsection{Extended method comparison}
In Table~\ref{tab:detailed_results}, we present the extended results from Table~\ref{tab:main_results} in the main part of this paper. These results cover probes trained with both \textit{all-layers} and \textit{per-layer} variants across all models, as well as lower temperature ($temp \in \{0.1, 1.0\}$). In all cases, the \textit{all-layers} variant outperforms the \textit{per-layer} variant, suggesting that hallucination-related information is distributed across multiple layers. Additionally, we observe a smaller generalisation gap (measured as the difference between test and training performance) for the $\lapeig$ method, indicating more robust features present in the Laplacian eigenvalues. Finally, as demonstrated in Section~\ref{fig:temperature_ablation}, increasing the temperature during answer generation improves probe performance, which is also evident in Table~\ref{tab:detailed_results}, where probes trained on answers generated with $temp=1.0$ consistently outperform those trained on data generated with $temp=0.1$.

\begin{sidewaystable*}[htb]
    \centering
    \small
    \caption{Performance comparison of methods evaluated in this work on an extended set of configurations. We marked results for $\attnscore$ in \textcolor{gray}{gray} as it is unsupervised approach, not directly comparable to the other ones. In \textbf{bold}, we highlight the best performance on the test split of data, individually for each dataset, LLM, and temperature.}
    \label{tab:detailed_results}
    \resizebox{\textwidth}{!}{
    \input{tables/all_results}
    }
\end{sidewaystable*}

\subsection{Best found hyperparameters}
We present the hyperparameter values corresponding to the results in Table~\ref{tab:main_results} and Table~\ref{tab:detailed_results}. Table~\ref{tab:best_top_k} shows the optimal hyperparameter $k$ for selecting the top-$k$ eigenvalues from either the attention maps in $\attneig$ or the Laplacian matrix in $\lapeig$. While fewer eigenvalues were sufficient for optimal performance in some cases, the best results were generally achieved with the highest tested value, $k=100$.

Table~\ref{tab:best_layer_idx} reports the layer indices that yielded the highest performance for the \textit{per-layer} models. Performance typically peaked in layers above the 10th, especially for \llamabig, where attention maps from the final layers more often led to better hallucination detection. Interestingly, the first layer's attention maps also produced strong performance in a few cases. Overall, no clear pattern emerges regarding the optimal layer, and as noted in prior work, selecting the best layer in the \textit{per-layer} setup often requires a search.

\begin{table*}[htb]
    \centering
    \caption{Values of $k$ hyperparameter, denoting how many highest eigenvalues are taken from the Laplacian matrix, corresponding to the best results in Table~\ref{tab:main_results} and Table~\ref{tab:detailed_results}.}
    \resizebox{\textwidth}{!}{
        \input{tables/best_top_k}
    }
    \label{tab:best_top_k}
\end{table*}

\begin{table*}[htb]
    \centering
    \caption{Values of a layer index (numbered from 0) corresponding to the best results for \textit{per-layer} models in Table~\ref{tab:detailed_results}.}
    \resizebox{\textwidth}{!}{
    \input{tables/best_layer_idx}
    }
    \label{tab:best_layer_idx}
\end{table*}

\section{Extended results of generalisation study}
\label{sec:appendix_generalization}
We present the complete results of the generalisation ablation discussed in Section~\ref{sec:generalization} of the main paper. Table~\ref{tab:extended_generalization} reports the absolute Test AUROC values for each method and test dataset. Except for TruthfulQA, $\lapeig$ achieves the highest performance across all configurations. Notably, some methods perform close to random, whereas $\lapeig$ consistently outperforms this baseline. Regarding relative performance drop (Figure~\ref{fig:extended_generalization}), $\lapeig$ remains competitive, exhibiting the lowest drop in nearly half of the scenarios. These results indicate that our method is robust but warrants further investigation across more datasets, particularly with a deeper analysis of TruthfulQA.

\begin{table*}[htb]
    \centering
    \caption{Full results of the generalisation study. By \textcolor{gray}{gray} color we denote results obtained on test split from the same QA dataset as training split, otherwise results are from test split of different QA dataset. We highlight the best performance in \textbf{bold}.}
    \small
    \input{tables/generalization_all_results}
    \label{tab:extended_generalization}
\end{table*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\textwidth]{figures/ablation_generalization/generalization_drop_full_plot_llama_3.1_8b_instruct_temp_1.0_attentions_only__prompt_qa_short_few_shot_sep__seed_42_100.png}
    \caption{Generalisation across datasets measured as a per cent performance drop in Test AUROC (less is better) when trained on one dataset and tested on the other. Training datasets are indicated in the plot titles, while test datasets are shown on the $x$-axis. Results computed on \llamabig\ with $k=100$ top eigenvalues and $temp=1.0$.}
    \label{fig:extended_generalization}
\end{figure*}

\section{QA prompts}
\label{sec:appendix_prompts}
Following, we describe all prompts for QA used to obtain the results presented in this work:
\begin{itemize}
    \item prompt $p_1$ -- medium-length one-shot prompt with single example of QA task (Listing~\ref{lst:p1}),
    \item prompt $p_2$ -- medium-length zero-shot prompt without examples (Listing~\ref{lst:p2}),
    \item prompt $p_3$ -- long few-shot prompt; the main prompt used in this work; modification of prompt used by \citep{kossen_semantic_2024} (Listing~\ref{lst:p3}),
    \item prompt $p_4$ -- short-length zero-shot prompt without examples (Listing~\ref{lst:p4}).
\end{itemize}

\begin{figure*}[htb]
\centering
\begin{lstlisting}[style=prompt, caption={One-shot QA (prompt $p_1$)}, label=lst:p1]
Deliver a succinct and straightforward answer to the question below. Focus on being brief while maintaining essential information. Keep extra details to a minimum.

Here is an example:
Question: What is the Riemann hypothesis?
Answer: All non-trivial zeros of the Riemann zeta function have real part 1/2

Question: {question}
Answer:
\end{lstlisting}
\end{figure*}

\begin{figure*}[htb]
\centering
\begin{lstlisting}[style=prompt, caption={Zero-sho QA (prompt $p_2$).}, label=lst:p2]
Please provide a concise and direct response to the following question, keeping your answer as brief and to-the-point as possible while ensuring clarity. Avoid any unnecessary elaboration or additional details.
Question: {question}
Answer:
\end{lstlisting}
\end{figure*}

\begin{figure*}[htb]
\centering
\begin{lstlisting}[style=prompt, mathescape, caption={Few-shot QA prompt (prompt $p_3$), modified version of prompt used by \citep{kossen_semantic_2024}.}, label=lst:p3]
Answer the following question as briefly as possible.
Here are several examples:
Question: What is the capital of France?
Answer: Paris

Question: Who wrote *Romeo and Juliet*?
Answer: William Shakespeare

Question: What is the boiling point of water in Celsius?
Answer: 100${^\circ}$C

Question: How many continents are there on Earth?
Answer: Seven

Question: What is the fastest land animal?
Answer: Cheetah

Question: {question}
Answer:
\end{lstlisting}
\end{figure*}

\begin{figure*}[htb]
\centering
\begin{lstlisting}[style=prompt, caption={Zero-shot shor QA prompt (prompt $p_4$).}, label=lst:p4]
Answer the following question as briefly as possible.
Question: {question}
Answer:
\end{lstlisting}
\end{figure*}

\section{LLM-as-Judge prompt}
During hallucinations dataset construction we leveraged \llmjudge\ approach to label answers generated by the LLMs. To this end, we utilised \texttt{gpt-4o-mini} with prompt in Listing~\ref{lst:judge_prompt}, which is an adapted version of the prompt used by \citep{orgad_llms_2025}.

\begin{figure*}[htb]
\centering
\begin{lstlisting}[style=prompt, caption={Prompt used in \llmjudge\ approach for determining halucination labels. Prompt is a modified version of the one used by \citep{orgad_llms_2025}.}, label=lst:judge_prompt]
You will evaluate answers to questions. For each question, I will provide a model's answer and one or more correct reference answers.
You would have to determine if the model answer is correct, incorrect, or model refused to answer. The model answer to be correct has to match from one to all of the possible correct answers.
If the model answer is correct, write 'correct' and if it is not correct, write 'incorrect'. If the Model Answer is a refusal, stating that they don't have enough information, write 'refuse'.
For example:

Question: who is the young guitarist who played with buddy guy?
Ground Truth: [Quinn Sullivan, Eric Gales]
Model Answer: Ronnie Earl
Correctness: incorrect

Question: What is the name of the actor who plays Iron Man in the Marvel movies?
Ground Truth: [Robert Downey Jr.]
Model Answer: Robert Downey Jr. played the role of Tony Stark/Iron Man in the Marvel Cinematic Universe films.
Correctness: correct

Question: what is the capital of France?
Ground Truth: [Paris]
Model Answer: I don't have enough information to answer this question.
Correctness: refuse

Question: who was the first person to walk on the moon?
Ground Truth: [Neil Armstrong]
Model Answer: I apologise, but I cannot provide an answer without verifying the historical facts.
Correctness: refuse

Question: {{question}}
Ground Truth: {{gold_answer}}
Model Answer: {{predicted_answer}}
Correctness:
\end{lstlisting}
\end{figure*}


\end{document}