
@misc{ruslan_kuprieiev_dvc_2025,
	title = {{DVC}: {Data} {Version} {Control} - {Git} for {Data} \& {Models}},
	copyright = {Apache License 2.0},
	url = {https://zenodo.org/doi/10.5281/zenodo.3677553},
	publisher = {Zenodo},
	author = {Ruslan Kuprieiev and skshetry and Peter Rowland and Dmitry Petrov and Pawel Redzynski and Casper da Costa-Luis and David de la Iglesia Castro and Alexander Schepanovski and Ivan Shcheklein and Gao and Batuhan Taskaya and Jorge Orpinel and Fábio Santos and Daniele and Ronan Lamy and Aman Sharma and Zhanibek Kaimuldenov and Dani Hodovic and Nikita Kodenko and Andrew Grigorev and Earl and Nabanita Dash and George Vyshnya and Dave Berenbaum and maykulkarni and Max Hora and Vera and Sanidhya Mangal},
	year = {2025},
	doi = {10.5281/ZENODO.3677553},
	keywords = {ai, collaboration, data-science, data-version-control, developer-tools, git, machine-learning, notion, python, reproducibility},
}

@inproceedings{zheng_judging_2023,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '23},
	title = {Judging {LLM}-as-a-judge with {MT}-bench and {Chatbot} {Arena}},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	year = {2023},
	note = {event-place: New Orleans, LA, USA},
	keywords = {notion},
}

@article{vaswani_attention_2017,
	title = {Attention is all you need},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, A},
	year = {2017},
	keywords = {notion},
}

@inproceedings{sriramanan_llm-check_2024,
	title = {{LLM}-{Check}: {Investigating} {Detection} of {Hallucinations} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=LYx4w3CAgy},
	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Sriramanan, Gaurang and Bharti, Siddhant and Sadasivan, Vinu Sankar and Saha, Shoumik and Kattakinda, Priyatham and Feizi, Soheil},
	year = {2024},
	keywords = {notion},
}

@inproceedings{topping_understanding_2022,
	title = {Understanding over-squashing and bottlenecks on graphs via curvature},
	url = {https://openreview.net/forum?id=7UmjRGzp-A},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Topping, Jake and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	year = {2022},
	keywords = {notion},
}

@article{reddy_coqa_2019,
	title = {{CoQA}: {A} {Conversational} {Question} {Answering} {Challenge}},
	volume = {7},
	shorttitle = {{CoQA}},
	url = {https://aclanthology.org/Q19-1016/},
	doi = {10.1162/tacl_a_00266},
	abstract = {Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4\%, which is 23.4 points behind human performance (88.8\%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa.},
	urldate = {2025-02-13},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
	editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
	year = {2019},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	keywords = {notion},
	pages = {249--266},
}

@inproceedings{rajpurkar_know_2018,
	address = {Melbourne, Australia},
	title = {Know {What} {You} {Don}`t {Know}: {Unanswerable} {Questions} for {SQuAD}},
	shorttitle = {Know {What} {You} {Don}`t {Know}},
	url = {https://aclanthology.org/P18-2124/},
	doi = {10.18653/v1/P18-2124},
	abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD achieves only 66\% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	keywords = {notion},
	pages = {784--789},
}

@inproceedings{marks_geometry_2024,
	title = {The {Geometry} of {Truth}: {Emergent} {Linear} {Structure} in {Large} {Language} {Model} {Representations} of {True}/{False} {Datasets}},
	url = {https://openreview.net/forum?id=aajyHYjjsk},
	booktitle = {First {Conference} on {Language} {Modeling}},
	author = {Marks, Samuel and Tegmark, Max},
	year = {2024},
	keywords = {notion},
}

@inproceedings{manakul_selfcheckgpt_2023,
	address = {Singapore},
	title = {{SelfCheckGPT}: {Zero}-{Resource} {Black}-{Box} {Hallucination} {Detection} for {Generative} {Large} {Language} {Models}},
	shorttitle = {{SelfCheckGPT}},
	url = {https://aclanthology.org/2023.emnlp-main.557/},
	doi = {10.18653/v1/2023.emnlp-main.557},
	abstract = {Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose “SelfCheckGPT”, a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	keywords = {notion},
	pages = {9004--9017},
}

@article{liang_internal_2024,
	title = {Internal {Consistency} and {Self}-{Feedback} in {Large} {Language} {Models}: {A} {Survey}},
	volume = {abs/2407.14507},
	url = {https://doi.org/10.48550/arXiv.2407.14507},
	journal = {CoRR},
	author = {Liang, Xun and Song, Shichao and Zheng, Zifan and Wang, Hanyu and Yu, Qingchen and Li, Xunkai and Li, Rong-Hua and Xiong, Feiyu and Li, Zhiyu},
	year = {2024},
	keywords = {notion},
}

@article{von_luxburg_tutorial_2007,
	title = {A tutorial on spectral clustering},
	volume = {17},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-007-9033-z},
	doi = {10.1007/s11222-007-9033-z},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
	number = {4},
	journal = {Statistics and Computing},
	author = {von Luxburg, Ulrike},
	month = dec,
	year = {2007},
	keywords = {notion},
	pages = {395--416},
}

@inproceedings{li_dawn_2024,
	address = {Bangkok, Thailand},
	title = {The {Dawn} {After} the {Dark}: {An} {Empirical} {Study} on {Factuality} {Hallucination} in {Large} {Language} {Models}},
	shorttitle = {The {Dawn} {After} the {Dark}},
	url = {https://aclanthology.org/2024.acl-long.586/},
	doi = {10.18653/v1/2024.acl-long.586},
	abstract = {In the era of large language models (LLMs), hallucination (the tendency to generate factually incorrect content) poses great challenges to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucinations, focused on the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and design a simple yet effective detection method for LLM hallucinations. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucinations. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs.},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Junyi and Chen, Jie and Ren, Ruiyang and Cheng, Xiaoxue and Zhao, Xin and Nie, Jian-Yun and Wen, Ji-Rong},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	keywords = {notion},
	pages = {10879--10899},
}

@inproceedings{joshi_triviaqa_2017,
	address = {Vancouver, Canada},
	title = {{TriviaQA}: {A} {Large} {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading} {Comprehension}},
	shorttitle = {{TriviaQA}},
	url = {https://aclanthology.org/P17-1147/},
	doi = {10.18653/v1/P17-1147},
	abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
	editor = {Barzilay, Regina and Kan, Min-Yen},
	month = jul,
	year = {2017},
	keywords = {notion},
	pages = {1601--1611},
}

@article{huang_survey_2025,
	title = {A {Survey} on {Hallucination} in {Large} {Language} {Models}: {Principles}, {Taxonomy}, {Challenges}, and {Open} {Questions}},
	volume = {43},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/3703155},
	doi = {10.1145/3703155},
	abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
	number = {2},
	journal = {ACM Trans. Inf. Syst.},
	author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
	month = jan,
	year = {2025},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Factuality, Faithfulness, Hallucination, Large Language Models, notion},
}

@inproceedings{chuang_dola_2024,
	title = {{DoLa}: {Decoding} by {Contrasting} {Layers} {Improves} {Factuality} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=Th6NyL07na},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James R. and He, Pengcheng},
	year = {2024},
	keywords = {notion},
}

@inproceedings{chuang_lookback_2024,
	address = {Miami, Florida, USA},
	title = {Lookback {Lens}: {Detecting} and {Mitigating} {Contextual} {Hallucinations} in {Large} {Language} {Models} {Using} {Only} {Attention} {Maps}},
	shorttitle = {Lookback {Lens}},
	url = {https://aclanthology.org/2024.emnlp-main.84/},
	doi = {10.18653/v1/2024.emnlp-main.84},
	abstract = {When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such **contextual hallucinations**. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these \_lookback ratio\_ features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector—**Lookback Lens**—is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6\% in the XSum summarization task.},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Chuang, Yung-Sung and Qiu, Linlu and Hsieh, Cheng-Yu and Krishna, Ranjay and Kim, Yoon and Glass, James R.},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	keywords = {notion},
	pages = {1419--1436},
}

@inproceedings{chen_inside_2024,
	title = {{INSIDE}: {LLMs}' {Internal} {States} {Retain} the {Power} of {Hallucination} {Detection}},
	url = {https://openreview.net/forum?id=Zj12nzlQbz},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping},
	year = {2024},
	keywords = {notion},
}

@inproceedings{azaria_internal_2023,
	address = {Singapore},
	title = {The {Internal} {State} of an {LLM} {Knows} {When} {It}`s {Lying}},
	url = {https://aclanthology.org/2023.findings-emnlp.68/},
	doi = {10.18653/v1/2023.findings-emnlp.68},
	abstract = {While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM`s internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\% to 83\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier`s performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.},
	urldate = {2025-02-13},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Azaria, Amos and Mitchell, Tom},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	keywords = {notion},
	pages = {967--976},
}

@inproceedings{arteaga_hallucination_2024,
	title = {Hallucination {Detection} in {LLMs}: {Fast} and {Memory}-{Efficient} {Finetuned} {Models}},
	url = {https://openreview.net/forum?id=8T8QkDsuO9},
	booktitle = {Northern {Lights} {Deep} {Learning} {Conference} 2025},
	author = {Arteaga, Gabriel Y. and Schön, Thomas B. and Pielawski, Nicolas},
	year = {2024},
	keywords = {notion},
}

@inproceedings{orgad_llms_2025,
	title = {{LLMs} {Know} {More} {Than} {They} {Show}: {On} the {Intrinsic} {Representation} of {LLM} {Hallucinations}},
	url = {https://openreview.net/forum?id=KRnsX5Em3W},
	booktitle = {The {Thirteenth} {International} {Conference} on {Learning} {Representations}},
	author = {Orgad, Hadas and Toker, Michael and Gekhman, Zorik and Reichart, Roi and Szpektor, Idan and Kotek, Hadas and Belinkov, Yonatan},
	year = {2025},
	keywords = {notion},
}

@misc{team_pandas-devpandas_2020,
	title = {pandas-dev/pandas: {Pandas}},
	url = {https://doi.org/10.5281/zenodo.3509134},
	publisher = {Zenodo},
	author = {team, The pandas development},
	month = feb,
	year = {2020},
	doi = {10.5281/zenodo.3509134},
	keywords = {notion},
}

@article{waskom_seaborn_2021,
	title = {seaborn: statistical data visualization},
	volume = {6},
	url = {https://doi.org/10.21105/joss.03021},
	doi = {10.21105/joss.03021},
	number = {60},
	journal = {Journal of Open Source Software},
	author = {Waskom, Michael L.},
	year = {2021},
	note = {Publisher: The Open Journal},
	keywords = {notion},
	pages = {3021},
}

@misc{noauthor_llm-check_nodate,
	title = {{LLM}-check: investigating detection of hallucinations in large language models {\textbar} bytez},
	shorttitle = {{LLM}-check},
	url = {https://bytez.com/docs/neurips/95584/paper},
	abstract = {This paper studies how Large Language Models (LLMs), like GPT, can sometimes make mistakes called "hallucinations," where they create false or misleading information that sounds believable. The authors propose a new method called LLM-Check to find these mistakes in a single response without needing complex or slow processes. They show that their method is much faster and more effective than previous methods, helping to make LLMs more reliable for important tasks.},
	language = {en},
	urldate = {2025-02-13},
}

@article{farquhar_detecting_2024,
	title = {Detecting hallucinations in large language models using semantic entropy},
	volume = {630},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07421-0},
	doi = {10.1038/s41586-024-07421-0},
	abstract = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
	language = {en},
	number = {8017},
	urldate = {2025-02-13},
	journal = {Nature},
	author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
	month = jun,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Information technology},
	pages = {625--630},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{zheng_judging_2023-1,
	title = {Judging {LLM}-as-a-judge with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {https://www.semanticscholar.org/paper/Judging-LLM-as-a-judge-with-MT-Bench-and-Chatbot-Zheng-Chiang/a0a79dad89857a96f8f71b14238e5237cbfc4787},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2025-02-13},
	journal = {ArXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, E. and Zhang, Haotong and Gonzalez, Joseph E. and Stoica, Ion},
	month = jun,
	year = {2023},
}

@article{lin_generating_2023,
	title = {Generating with {Confidence}: {Uncertainty} {Quantification} for {Black}-box {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Generating with {Confidence}},
	url = {https://arxiv.org/abs/2305.19187},
	doi = {10.48550/ARXIV.2305.19187},
	abstract = {Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate *uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty measures, applying them to *selective NLG* where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple measure for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at https://github.com/zlin7/UQ-NLG.},
	urldate = {2025-02-13},
	author = {Lin, Zhen and Trivedi, Shubhendu and Sun, Jimeng},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	doi = {10.48550/arXiv.2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06825 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{thakur_judging_2025,
	title = {Judging the {Judges}: {Evaluating} {Alignment} and {Vulnerabilities} in {LLMs}-as-{Judges}},
	shorttitle = {Judging the {Judges}},
	url = {http://arxiv.org/abs/2406.12624},
	doi = {10.48550/arXiv.2406.12624},
	abstract = {Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges, focusing on a clean scenario in which inter-human agreement is high. Investigating thirteen judge models of different model sizes and families, judging answers of nine different 'examtaker models' - both base and instruction-tuned - we find that only the best (and largest) models achieve reasonable alignment with humans. However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores. In terms of their ranking of the nine exam-taker models, instead, also smaller models and even the lexical metric contains may provide a reasonable signal. Through error analysis and other studies, we identify vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency. The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups. Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Thakur, Aman Singh and Choudhary, Kartik and Ramayapally, Venkat Srinik and Vaidyanathan, Sankaran and Hupkes, Dieuwke},
	month = jan,
	year = {2025},
	note = {arXiv:2406.12624 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{zhu_pollmgraph_2024,
	address = {Mexico City, Mexico},
	title = {{PoLLMgraph}: {Unraveling} {Hallucinations} in {Large} {Language} {Models} via {State} {Transition} {Dynamics}},
	shorttitle = {{PoLLMgraph}},
	url = {https://aclanthology.org/2024.findings-naacl.294/},
	doi = {10.18653/v1/2024.findings-naacl.294},
	urldate = {2025-02-11},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Derui and Chen, Dingfan and Li, Qing and Chen, Zongxiong and Ma, Lei and Grossklags, Jens and Fritz, Mario},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	keywords = {notion},
	pages = {4737--4751},
}

@inproceedings{black_understanding_2023,
	title = {Understanding {Oversquashing} in {GNNs} through the {Lens} of {Effective} {Resistance}},
	url = {http://arxiv.org/abs/2302.06835},
	doi = {10.48550/arXiv.2302.06835},
	abstract = {Message passing graph neural networks (GNNs) are a popular learning architectures for graph-structured data. However, one problem GNNs experience is oversquashing, where a GNN has difficulty sending information between distant nodes. Understanding and mitigating oversquashing has recently received significant attention from the research community. In this paper, we continue this line of work by analyzing oversquashing through the lens of the effective resistance between nodes in the input graph. Effective resistance intuitively captures the ``strength'' of connection between two nodes by paths in the graph, and has a rich literature spanning many areas of graph theory. We propose to use total effective resistance as a bound of the total amount of oversquashing in a graph and provide theoretical justification for its use. We further develop an algorithm to identify edges to be added to an input graph to minimize the total effective resistance, thereby alleviating oversquashing. We provide empirical evidence of the effectiveness of our total effective resistance based rewiring strategies for improving the performance of GNNs.},
	urldate = {2025-02-06},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Black, Mitchell and Wan, Zhengchao and Nayyeri, Amir and Wang, Yusu},
	month = jun,
	year = {2023},
	note = {arXiv:2302.06835 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	pages = {2528--2547},
}

@inproceedings{lin_truthfulqa_2022,
	address = {Dublin, Ireland},
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {https://aclanthology.org/2022.acl-long.229/},
	doi = {10.18653/v1/2022.acl-long.229},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2025-02-11},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	keywords = {notion},
	pages = {3214--3252},
}

@article{bengio_representation_2014,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	shorttitle = {Representation {Learning}},
	url = {http://arxiv.org/abs/1206.5538},
	doi = {10.48550/arXiv.1206.5538},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	urldate = {2025-02-11},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = apr,
	year = {2014},
	note = {arXiv:1206.5538 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	pages = {1798--1828},
}

@inproceedings{renze_effect_2024,
	address = {Miami, Florida, USA},
	title = {The {Effect} of {Sampling} {Temperature} on {Problem} {Solving} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.findings-emnlp.432/},
	doi = {10.18653/v1/2024.findings-emnlp.432},
	abstract = {In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature},
	urldate = {2025-02-11},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Renze, Matthew},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	keywords = {notion},
	pages = {7346--7356},
}

@misc{honovich_true_2022,
	title = {{TRUE}: {Re}-evaluating {Factual} {Consistency} {Evaluation}},
	shorttitle = {{TRUE}},
	url = {http://arxiv.org/abs/2204.04991},
	doi = {10.48550/arXiv.2204.04991},
	abstract = {Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous meta-evaluation protocols focused on system-level correlations with human annotations, which leave the example-level accuracy of such metrics unclear. In this work, we introduce TRUE: a comprehensive survey and assessment of factual consistency metrics on a standardized collection of existing texts from diverse tasks, manually annotated for factual consistency. Our standardization enables an example-level meta-evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures. Across diverse state-of-the-art metrics and 11 datasets we find that large-scale NLI and question generation-and-answering-based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better evaluation methods.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Honovich, Or and Aharoni, Roee and Herzig, Jonathan and Taitelbaum, Hagai and Kukliansy, Doron and Cohen, Vered and Scialom, Thomas and Szpektor, Idan and Hassidim, Avinatan and Matias, Yossi},
	month = may,
	year = {2022},
	note = {arXiv:2204.04991 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{honovich_true_2022-1,
	title = {{TRUE}: {Re}-evaluating {Factual} {Consistency} {Evaluation}},
	shorttitle = {{TRUE}},
	url = {http://arxiv.org/abs/2204.04991},
	doi = {10.48550/arXiv.2204.04991},
	abstract = {Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous meta-evaluation protocols focused on system-level correlations with human annotations, which leave the example-level accuracy of such metrics unclear. In this work, we introduce TRUE: a comprehensive survey and assessment of factual consistency metrics on a standardized collection of existing texts from diverse tasks, manually annotated for factual consistency. Our standardization enables an example-level meta-evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures. Across diverse state-of-the-art metrics and 11 datasets we find that large-scale NLI and question generation-and-answering-based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better evaluation methods.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Honovich, Or and Aharoni, Roee and Herzig, Jonathan and Taitelbaum, Hagai and Kukliansy, Doron and Cohen, Vered and Scialom, Thomas and Szpektor, Idan and Hassidim, Avinatan and Matias, Yossi},
	month = may,
	year = {2022},
	note = {arXiv:2204.04991 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{jolliffe_principal_2016,
	title = {Principal component analysis: a review and recent developments},
	volume = {374},
	shorttitle = {Principal component analysis},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202},
	doi = {10.1098/rsta.2015.0202},
	abstract = {Large datasets are increasingly common and are often difficult to interpret. Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue/eigenvector problem, and the new variables are defined by the dataset at hand, not a priori, hence making PCA an adaptive data analysis technique. It is adaptive in another sense too, since variants of the technique have been developed that are tailored to various different data types and structures. This article will begin by introducing the basic ideas of PCA, discussing what it can and cannot do. It will then describe some variants of PCA and their application.},
	number = {2065},
	urldate = {2025-02-10},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Jolliffe, Ian T. and Cadima, Jorge},
	month = apr,
	year = {2016},
	note = {Publisher: Royal Society},
	keywords = {dimension reduction, eigenvectors, multivariate analysis, notion, palaeontology},
	pages = {20150202},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: {Fundamental} {Algorithms} for {Scientific} {Computing} in {Python}},
	volume = {17},
	doi = {10.1038/s41592-019-0686-2},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and {SciPy 1.0 Contributors}},
	year = {2020},
	keywords = {notion},
	pages = {261--272},
}

@misc{garrido_rankme_2023,
	title = {{RankMe}: {Assessing} the downstream performance of pretrained self-supervised representations by their rank},
	shorttitle = {{RankMe}},
	url = {http://arxiv.org/abs/2210.02885},
	doi = {10.48550/arXiv.2210.02885},
	abstract = {Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-parameters to tune. Through thorough empirical experiments involving hundreds of training episodes, we demonstrate how RankMe can be used for hyperparameter selection with nearly no reduction in final performance compared to the current selection method that involve a dataset's labels. We hope that RankMe will facilitate the deployment of JE-SSL towards domains that do not have the opportunity to rely on labels for representations' quality assessment.},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Garrido, Quentin and Balestriero, Randall and Najman, Laurent and Lecun, Yann},
	month = jun,
	year = {2023},
	note = {arXiv:2210.02885 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{roy_effective_2007,
	title = {The {Effective} {Rank}: a {Measure} of {Effective} {Dimensionality}},
	abstract = {Many signal processing algorithms include numerical problems where the solution is obtained by adjusting the value of parameters such that a speciﬁc matrix exhibits rank deﬁciency. Since rank minimization is generally not practicable owing to its integer nature, we propose a real-valued extension that we term effective rank. After proving some of its properties, the effective rank is provided with an operational meaning using a result on the coefﬁcient rate of a stationary random process. Finally, the proposed measure is assessed in a practical scenario and other potential applications are suggested.},
	language = {en},
	author = {Roy, Olivier and Vetterli, Martin},
	year = {2007},
}

@article{vashurin_benchmarking_2024,
	title = {Benchmarking {Uncertainty} {Quantification} {Methods} for {Large} {Language} {Models} with {LM}-{Polygraph}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2406.15627},
	doi = {10.48550/ARXIV.2406.15627},
	abstract = {The rapid proliferation of large language models (LLMs) has stimulated researchers to seek effective and efficient approaches to deal with LLM hallucinations and low-quality outputs. Uncertainty quantification (UQ) is a key element of machine learning applications in dealing with such challenges. However, research to date on UQ for LLMs has been fragmented in terms of techniques and evaluation methodologies. In this work, we address this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines and offers an environment for controllable and consistent evaluation of novel UQ techniques over various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across eleven tasks, identifying the most effective approaches. Code: https://github.com/IINemo/lm-polygraph Benchmark: https://huggingface.co/LM-Polygraph},
	urldate = {2025-02-09},
	author = {Vashurin, Roman and Fadeeva, Ekaterina and Vazhentsev, Artem and Rvanova, Lyudmila and Tsvigun, Akim and Vasilev, Daniil and Xing, Rui and Sadallah, Abdelrahman Boda and Grishchenkov, Kirill and Petrakov, Sergey and Panchenko, Alexander and Baldwin, Timothy and Nakov, Preslav and Panov, Maxim and Shelmanov, Artem},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@inproceedings{zha_alignscore_2023,
	title = {{AlignScore}: {Evaluating} {Factual} {Consistency} with a {Unified} {Alignment} {Function}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{AlignScore}},
	url = {https://arxiv.org/abs/2305.16739},
	doi = {10.48550/ARXIV.2305.16739},
	abstract = {Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. AlignScore achieves substantial improvement over a wide range of previous metrics. Moreover, AlignScore (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger.},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Zha, Yuheng and Yang, Yichi and Li, Ruichen and Hu, Zhiting},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{zhang_how_2023,
	title = {How {Language} {Model} {Hallucinations} {Can} {Snowball}},
	url = {http://arxiv.org/abs/2305.13534},
	doi = {10.48550/arXiv.2305.13534},
	abstract = {A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67\% and 87\% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Zhang, Muru and Press, Ofir and Merrill, William and Liu, Alisa and Smith, Noah A.},
	month = may,
	year = {2023},
	note = {arXiv:2305.13534 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{lee_mathematical_2023,
	title = {A {Mathematical} {Investigation} of {Hallucination} and {Creativity} in {GPT} {Models}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/11/10/2320},
	doi = {10.3390/math11102320},
	abstract = {In this paper, we present a comprehensive mathematical analysis of the hallucination phenomenon in generative pretrained transformer (GPT) models. We rigorously define and measure hallucination and creativity using concepts from probability theory and information theory. By introducing a parametric family of GPT models, we characterize the trade-off between hallucination and creativity and identify an optimal balance that maximizes model performance across various tasks. Our work offers a novel mathematical framework for understanding the origins and implications of hallucination in GPT models and paves the way for future research and development in the field of large language models (LLMs).},
	language = {en},
	number = {10},
	urldate = {2025-02-08},
	journal = {Mathematics},
	author = {Lee, Minhyeok},
	month = may,
	year = {2023},
	keywords = {notion},
	pages = {2320},
}

@misc{noauthor_mathematical_nodate,
	title = {A {Mathematical} {Investigation} of {Hallucination} and {Creativity} in {GPT} {Models}},
	url = {https://www.mdpi.com/2227-7390/11/10/2320},
	urldate = {2025-02-08},
	keywords = {notion},
}

@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2025-02-08},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, notion},
}

@misc{abdin_phi-3_2024,
	title = {Phi-3 {Technical} {Report}: {A} {Highly} {Capable} {Language} {Model} {Locally} on {Your} {Phone}},
	shorttitle = {Phi-3 {Technical} {Report}},
	url = {http://arxiv.org/abs/2404.14219},
	doi = {10.48550/arXiv.2404.14219},
	abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\%, 78\% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.},
	urldate = {2025-02-08},
	publisher = {arXiv},
	author = {Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, Sébastien and Cai, Martin and Cai, Qin and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and Chen, Weizhu and Chen, Yen-Chun and Chen, Yi-Ling and Cheng, Hao and Chopra, Parul and Dai, Xiyang and Dixon, Matthew and Eldan, Ronen and Fragoso, Victor and Gao, Jianfeng and Gao, Mei and Gao, Min and Garg, Amit and Giorno, Allie Del and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Hu, Wenxiang and Huynh, Jamie and Iter, Dan and Jacobs, Sam Ade and Javaheripi, Mojan and Jin, Xin and Karampatziakis, Nikos and Kauffmann, Piero and Khademi, Mahoud and Kim, Dongwoo and Kim, Young Jin and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and Liden, Lars and Lin, Xihui and Lin, Zeqi and Liu, Ce and Liu, Liyuan and Liu, Mengchen and Liu, Weishung and Liu, Xiaodong and Luo, Chong and Madan, Piyush and Mahmoudzadeh, Ali and Majercak, David and Mazzola, Matt and Mendes, Caio César Teodoro and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Ren, Liliang and Rosa, Gustavo de and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Shen, Yelong and Shukla, Swadheen and Song, Xia and Tanaka, Masahiro and Tupini, Andrea and Vaddamanu, Praneetha and Wang, Chunyu and Wang, Guanhua and Wang, Lijuan and Wang, Shuohang and Wang, Xin and Wang, Yu and Ward, Rachel and Wen, Wen and Witte, Philipp and Wu, Haiping and Wu, Xiaoxia and Wyatt, Michael and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian and Xue, Jilong and Yadav, Sonali and Yang, Fan and Yang, Jianwei and Yang, Yifan and Yang, Ziyi and Yu, Donghan and Yuan, Lu and Zhang, Chenruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
	month = aug,
	year = {2024},
	note = {arXiv:2404.14219 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
}

@misc{balestriero_cookbook_2023,
	title = {A {Cookbook} of {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2304.12210},
	doi = {10.48550/arXiv.2304.12210},
	abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
	urldate = {2025-02-08},
	publisher = {arXiv},
	author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
	month = jun,
	year = {2023},
	note = {arXiv:2304.12210 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
}

@misc{mitra_factlens_2024,
	title = {{FactLens}: {Benchmarking} {Fine}-{Grained} {Fact} {Verification}},
	shorttitle = {{FactLens}},
	url = {http://arxiv.org/abs/2411.05980},
	doi = {10.48550/arXiv.2411.05980},
	abstract = {Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift toward fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.},
	urldate = {2025-02-07},
	publisher = {arXiv},
	author = {Mitra, Kushan and Zhang, Dan and Rahman, Sajjadur and Hruschka, Estevam},
	month = nov,
	year = {2024},
	note = {arXiv:2411.05980 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{ansel_pytorch_2024,
	title = {{PyTorch} 2: {Faster} {Machine} {Learning} {Through} {Dynamic} {Python} {Bytecode} {Transformation} and {Graph} {Compilation}},
	url = {https://pytorch.org/assets/pytorch2-2.pdf},
	doi = {10.1145/3620665.3640366},
	booktitle = {29th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}, {Volume} 2 ({ASPLOS} '24)},
	publisher = {ACM},
	author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
	month = apr,
	year = {2024},
	keywords = {notion},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://aclanthology.org/2020.emnlp-demos.6/},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2025-02-06},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	editor = {Liu, Qun and Schlangen, David},
	month = oct,
	year = {2020},
	keywords = {notion},
	pages = {38--45},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013/},
	urldate = {2025-02-06},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	keywords = {notion},
	pages = {74--81},
}

@article{bruna_spectral_2013,
	title = {Spectral {Networks} and {Locally} {Connected} {Networks} on {Graphs}},
	url = {https://www.semanticscholar.org/paper/Spectral-Networks-and-Locally-Connected-Networks-on-Bruna-Zaremba/5e925a9f1e20df61d1e860a7aa71894b35a1c186},
	abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
	urldate = {2025-02-06},
	journal = {CoRR},
	author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
	month = dec,
	year = {2013},
	keywords = {notion},
}

@article{wilson_study_2008,
	title = {A study of graph spectra for comparing graphs and trees},
	volume = {41},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320308000927},
	doi = {10.1016/j.patcog.2008.03.011},
	abstract = {The spectrum of a graph has been widely used in graph theory to characterise the properties of a graph and extract information from its structure. It has also been employed as a graph representation for pattern matching since it is invariant to the labelling of the graph. There are however a number of potential drawbacks in using the spectrum as a representation of a graph; Firstly, more than one graph may share the same spectrum. It is well known, for example, that very few trees can be uniquely speciﬁed by their spectrum. Secondly, the spectrum may change dramatically with a small change structure.},
	language = {en},
	number = {9},
	urldate = {2025-02-06},
	journal = {Pattern Recognition},
	author = {Wilson, Richard C. and Zhu, Ping},
	month = sep,
	year = {2008},
	keywords = {notion},
	pages = {2833--2841},
}

@article{levin_markov_nodate,
	title = {Markov {Chains} and {Mixing} {Times}},
	language = {en},
	author = {Levin, David A and Peres, Yuval and Wilmer, Elizabeth L},
	keywords = {notion},
}

@incollection{brefeld_graph_2020,
	address = {Cham},
	title = {Graph {Signal} {Processing} for {Directed} {Graphs} {Based} on the {Hermitian} {Laplacian}},
	volume = {11906},
	isbn = {978-3-030-46149-2 978-3-030-46150-8},
	url = {http://link.springer.com/10.1007/978-3-030-46150-8_27},
	abstract = {Graph signal processing is a useful tool for representing, analyzing, and processing the signal lying on a graph, and has attracted attention in several ﬁelds including data mining and machine learning. A key to construct the graph signal processing is the graph Fourier transform, which is deﬁned by using eigenvectors of the graph Laplacian of an undirected graph. The orthonormality of eigenvectors gives the graph Fourier transform algebraically desirable properties, and thus the graph signal processing for undirected graphs has been well developed. However, since eigenvectors of the graph Laplacian of a directed graph are generally not orthonormal, it is diﬃcult to simply extend the graph signal processing to directed graphs. In this paper, we present a general framework for extending the graph signal processing to directed graphs. To this end, we introduce the Hermitian Laplacian which is a complex matrix obtained from an extension of the graph Laplacian. The Hermitian Laplacian is deﬁned so as to preserve the edge directionality and Hermitian property and enables the graph signal processing to be straightforwardly extended to directed graphs. Furthermore, the Hermitian Laplacian guarantees some desirable properties, such as non-negative real eigenvalues and the unitarity of the Fourier transform. Finally, experimental results for representation learning and signal denoising of/on directed graphs show the eﬀectiveness of our framework.},
	language = {en},
	urldate = {2025-02-02},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Furutani, Satoshi and Shibahara, Toshiki and Akiyama, Mitsuaki and Hato, Kunio and Aida, Masaki},
	editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, Céline},
	year = {2020},
	doi = {10.1007/978-3-030-46150-8_27},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {notion},
	pages = {447--463},
}

@article{chung_laplacians_2005,
	title = {Laplacians and the {Cheeger} {Inequality} for {Directed} {Graphs}},
	volume = {9},
	copyright = {http://www.springer.com/tdm},
	issn = {0218-0006, 0219-3094},
	url = {http://link.springer.com/10.1007/s00026-005-0237-z},
	doi = {10.1007/s00026-005-0237-z},
	abstract = {We consider Laplacians for directed graphs and examine their eigenvalues. We introduce a notion of a circulation in a directed graph and its connection with the Rayleigh quotient. We then deﬁne a Cheeger constant and establish the Cheeger inequality for directed graphs. These relations can be used to deal with various problems that often arise in the study of non-reversible Markov chains including bounding the rate of convergence and deriving comparison theorems.},
	language = {en},
	number = {1},
	urldate = {2025-02-02},
	journal = {Annals of Combinatorics},
	author = {Chung, Fan},
	month = apr,
	year = {2005},
	keywords = {notion},
	pages = {1--19},
}

@incollection{hahn_applications_1997,
	address = {Dordrecht},
	title = {Some applications of {Laplace} eigenvalues of graphs},
	isbn = {978-90-481-4885-1 978-94-015-8937-6},
	url = {http://link.springer.com/10.1007/978-94-015-8937-6_6},
	abstract = {In the last decade important relations between Laplace eigenvalues and eigenvectors of graphs and several other graph parameters were discovered. In these notes we present some of these results and discuss their consequences. Attention is given to the partition and the isoperimetric properties of graphs, the max-cut problem and its relation to semideﬁnite programming, rapid mixing of Markov chains, and to extensions of the results to inﬁnite graphs.},
	language = {en},
	urldate = {2025-02-02},
	booktitle = {Graph {Symmetry}},
	publisher = {Springer Netherlands},
	author = {Mohar, Bojan},
	editor = {Hahn, Geňa and Sabidussi, Gert},
	year = {1997},
	doi = {10.1007/978-94-015-8937-6_6},
	keywords = {notion},
	pages = {225--275},
}

@misc{liu_attention-guided_2025,
	title = {Attention-guided {Self}-reflection for {Zero}-shot {Hallucination} {Detection} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2501.09997},
	doi = {10.48550/arXiv.2501.09997},
	abstract = {Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational complexity, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Liu, Qiang and Chen, Xinlong and Ding, Yue and Xu, Shizhen and Wu, Shu and Wang, Liang},
	month = jan,
	year = {2025},
	note = {arXiv:2501.09997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	keywords = {notion},
	pages = {2825--2830},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	keywords = {notion},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	keywords = {notion},
}

@misc{kissane_interpreting_2024,
	title = {Interpreting {Attention} {Layer} {Outputs} with {Sparse} {Autoencoders}},
	url = {http://arxiv.org/abs/2406.17759},
	doi = {10.48550/arXiv.2406.17759},
	abstract = {Decomposing model activations into interpretable components is a key open problem in mechanistic interpretability. Sparse autoencoders (SAEs) are a popular method for decomposing the internal activations of trained transformers into sparse, interpretable features, and have been applied to MLP layers and the residual stream. In this work we train SAEs on attention layer outputs and show that also here SAEs find a sparse, interpretable decomposition. We demonstrate this on transformers from several model families and up to 2B parameters. We perform a qualitative study of the features computed by attention layers, and find multiple families: long-range context, short-range context and induction features. We qualitatively study the role of every head in GPT-2 Small, and estimate that at least 90\% of the heads are polysemantic, i.e. have multiple unrelated roles. Further, we show that Sparse Autoencoders are a useful tool that enable researchers to explain model behavior in greater detail than prior work. For example, we explore the mystery of why models have so many seemingly redundant induction heads, use SAEs to motivate the hypothesis that some are long-prefix whereas others are short-prefix, and confirm this with more rigorous analysis. We use our SAEs to analyze the computation performed by the Indirect Object Identification circuit (Wang et al.), validating that the SAEs find causally meaningful intermediate variables, and deepening our understanding of the semantics of the circuit. We open-source the trained SAEs and a tool for exploring arbitrary prompts through the lens of Attention Output SAEs.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Kissane, Connor and Krzyzanowski, Robert and Bloom, Joseph Isaac and Conmy, Arthur and Nanda, Neel},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17759 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{velickovic_softmax_2024,
	title = {softmax is not enough (for sharp out-of-distribution)},
	url = {http://arxiv.org/abs/2410.01104},
	doi = {10.48550/arXiv.2410.01104},
	abstract = {A key property of reasoning systems is the ability to make sharp decisions on their input data. For contemporary AI systems, a key carrier of sharp behaviour is the softmax function, with its capability to perform differentiable query-key lookups. It is a common belief that the predictive power of networks leveraging softmax arises from "circuits" which sharply perform certain kinds of computations consistently across many diverse inputs. However, for these circuits to be robust, they would need to generalise well to arbitrary valid inputs. In this paper, we dispel this myth: even for tasks as simple as finding the maximum key, any learned circuitry must disperse as the number of items grows at test time. We attribute this to a fundamental limitation of the softmax function to robustly approximate sharp functions, prove this phenomenon theoretically, and propose adaptive temperature as an ad-hoc technique for improving the sharpness of softmax at inference time.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Veličković, Petar and Perivolaropoulos, Christos and Barbero, Federico and Pascanu, Razvan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.01104 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Information Theory, notion},
}

@misc{kostenok_uncertainty_2024,
	title = {Uncertainty {Estimation} of {Transformers}' {Predictions} via {Topological} {Analysis} of the {Attention} {Matrices}},
	url = {http://arxiv.org/abs/2308.11295},
	doi = {10.48550/arXiv.2308.11295},
	abstract = {Transformer-based language models have set new benchmarks across a wide range of NLP tasks, yet reliably estimating the uncertainty of their predictions remains a significant challenge. Existing uncertainty estimation (UE) techniques often fall short in classification tasks, either offering minimal improvements over basic heuristics or relying on costly ensemble models. Moreover, attempts to leverage common embeddings for UE in linear probing scenarios have yielded only modest gains, indicating that alternative model components should be explored. We tackle these limitations by harnessing the geometry of attention maps across multiple heads and layers to assess model confidence. Our approach extracts topological features from attention matrices, providing a low-dimensional, interpretable representation of the model's internal dynamics. Additionally, we introduce topological features to compare attention patterns across heads and layers. Our method significantly outperforms existing UE techniques on benchmarks for acceptability judgments and artificial text detection, offering a more efficient and interpretable solution for uncertainty estimation in large-scale language models.},
	urldate = {2025-01-04},
	publisher = {arXiv},
	author = {Kostenok, Elizaveta and Cherniavskii, Daniil and Zaytsev, Alexey},
	month = sep,
	year = {2024},
	note = {arXiv:2308.11295 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{naderi_mind_2024,
	title = {Mind the {Gap}: a {Spectral} {Analysis} of {Rank} {Collapse} and {Signal} {Propagation} in {Transformers}},
	shorttitle = {Mind the {Gap}},
	url = {http://arxiv.org/abs/2410.07799},
	doi = {10.48550/arXiv.2410.07799},
	abstract = {Attention layers are the core component of transformers, the current state-of-the-art neural network architecture. However, {\textbackslash}softmaxx-based attention puts transformers' trainability at risk. Even {\textbackslash}textit\{at initialisation\}, the propagation of signals and gradients through the random network can be pathological, resulting in known issues such as (i) vanishing/exploding gradients and (ii) {\textbackslash}textit\{rank collapse\}, i.e. when all tokens converge to a single representation {\textbackslash}textit\{with depth\}. This paper examines signal propagation in {\textbackslash}textit\{attention-only\} transformers from a random matrix perspective, illuminating the origin of such issues, as well as unveiling a new phenomenon -- (iii) rank collapse {\textbackslash}textit\{in width\}. Modelling {\textbackslash}softmaxx-based attention at initialisation with Random Markov matrices, our theoretical analysis reveals that a {\textbackslash}textit\{spectral gap\} between the two largest singular values of the attention matrix causes (iii), which, in turn, exacerbates (i) and (ii). Building on this insight, we propose a novel, yet simple, practical solution to resolve rank collapse in width by removing the spectral gap. Moreover, we validate our findings and discuss the training benefits of the proposed fix through experiments that also motivate a revision of some of the default parameter scaling. Our attention model accurately describes the standard key-query attention in a single-layer transformer, making this work a significant first step towards a better understanding of the initialisation dynamics in the multi-layer case.},
	urldate = {2025-01-04},
	publisher = {arXiv},
	author = {Naderi, Alireza and Saada, Thiziri Nait and Tanner, Jared},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07799 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{huang_dynamic_2025,
	title = {Dynamic {Attention}-{Guided} {Context} {Decoding} for {Mitigating} {Context} {Faithfulness} {Hallucinations} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2501.01059},
	doi = {10.48550/arXiv.2501.01059},
	abstract = {Large language models (LLMs) often suffer from context faithfulness hallucinations, where outputs deviate from retrieved information due to insufficient context utilization and high output uncertainty. Our uncertainty evaluation experiments reveal a strong correlation between high uncertainty and hallucinations. We hypothesize that attention mechanisms encode signals indicative of contextual utilization, validated through probing analysis. Based on these insights, we propose Dynamic Attention-Guided Context Decoding (DAGCD), a lightweight framework that integrates attention distributions and uncertainty signals in a single-pass decoding process. Experiments across QA datasets demonstrate DAGCD's effectiveness, achieving significant improvements in faithfulness and robustness while maintaining computational efficiency.},
	urldate = {2025-01-04},
	publisher = {arXiv},
	author = {Huang, Yanwen and Zhang, Yong and Cheng, Ning and Li, Zhitao and Wang, Shaojun and Xiao, Jing},
	month = jan,
	year = {2025},
	note = {arXiv:2501.01059 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@misc{barbero_transformers_2024,
	title = {Transformers need glasses! {Information} over-squashing in language tasks},
	url = {http://arxiv.org/abs/2406.04267},
	doi = {10.48550/arXiv.2406.04267},
	abstract = {We study how information propagates in decoder-only Transformers, which are the architectural backbone of most existing frontier large language models (LLMs). We rely on a theoretical signal propagation analysis -- specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction. Our analysis reveals a representational collapse phenomenon: we prove that certain distinct sequences of inputs to the Transformer can yield arbitrarily close representations in the final token. This effect is exacerbated by the low-precision floating-point formats frequently used in modern LLMs. As a result, the model is provably unable to respond to these sequences in different ways -- leading to errors in, e.g., tasks involving counting or copying. Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of over-squashing in graph neural networks. We provide empirical evidence supporting our claims on contemporary LLMs. Our theory also points to simple solutions towards ameliorating these issues.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Barbero, Federico and Banino, Andrea and Kapturowski, Steven and Kumaran, Dharshan and Araújo, João G. M. and Vitvitskyi, Alex and Pascanu, Razvan and Veličković, Petar},
	month = oct,
	year = {2024},
	note = {arXiv:2406.04267 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@misc{han_emergence_2024,
	title = {Emergence of {Abstractions}: {Concept} {Encoding} and {Decoding} {Mechanism} for {In}-{Context} {Learning} in {Transformers}},
	shorttitle = {Emergence of {Abstractions}},
	url = {http://arxiv.org/abs/2412.12276},
	doi = {10.48550/arXiv.2412.12276},
	abstract = {Humans distill complex experiences into fundamental abstractions that enable rapid learning and adaptation. Similarly, autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. In this paper, we propose concept encoding-decoding mechanism to explain ICL by studying how transformers form and use internal abstractions in their representations. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of concept encoding and decoding. As the model learns to encode different latent concepts (e.g., ``Finding the first noun in a sentence.") into distinct, separable representations, it concureently builds conditional decoding algorithms and improve its ICL performance. We validate the existence of this mechanism across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B). Further, through mechanistic interventions and controlled finetuning, we demonstrate that the quality of concept encoding is causally related and predictive of ICL performance. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Han, Seungwook and Song, Jinyeop and Gore, Jeff and Agrawal, Pulkit},
	month = dec,
	year = {2024},
	note = {arXiv:2412.12276 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@misc{katz_visit_2023,
	title = {{VISIT}: {Visualizing} and {Interpreting} the {Semantic} {Information} {Flow} of {Transformers}},
	shorttitle = {{VISIT}},
	url = {http://arxiv.org/abs/2305.13417},
	doi = {10.48550/arXiv.2305.13417},
	abstract = {Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them more human interpretable. In this paper, we investigate LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on our discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that can reflect the models' internal processing, uncovering the contribution of each component to the models' final prediction. Our visualization also unveils new insights about the role of layer norms as semantic filters that influence the models' output, and about neurons that are always activated during forward passes and act as regularization vectors.},
	urldate = {2024-12-16},
	publisher = {arXiv},
	author = {Katz, Shahar and Belinkov, Yonatan},
	month = nov,
	year = {2023},
	note = {arXiv:2305.13417 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@misc{kim_detecting_2024,
	title = {Detecting {LLM} {Hallucination} {Through} {Layer}-wise {Information} {Deficiency}: {Analysis} of {Unanswerable} {Questions} and {Ambiguous} {Prompts}},
	shorttitle = {Detecting {LLM} {Hallucination} {Through} {Layer}-wise {Information} {Deficiency}},
	url = {http://arxiv.org/abs/2412.10246},
	doi = {10.48550/arXiv.2412.10246},
	abstract = {Large language models (LLMs) frequently generate confident yet inaccurate responses, introducing significant risks for deployment in safety-critical domains. We present a novel approach to detecting model hallucination through systematic analysis of information flow across model layers when processing inputs with insufficient or ambiguous context. Our investigation reveals that hallucination manifests as usable information deficiencies in inter-layer transmissions. While existing approaches primarily focus on final-layer output analysis, we demonstrate that tracking cross-layer information dynamics (\${\textbackslash}mathcal\{L\}\$I) provides robust indicators of model reliability, accounting for both information gain and loss during computation. \${\textbackslash}mathcal\{L\}\$I improves model reliability by immediately integrating with universal LLMs without additional training or architectural modifications.},
	urldate = {2024-12-16},
	publisher = {arXiv},
	author = {Kim, Hazel and Bibi, Adel and Torr, Philip and Gal, Yarin},
	month = dec,
	year = {2024},
	note = {arXiv:2412.10246 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{burns_discovering_2024,
	title = {Discovering {Latent} {Knowledge} in {Language} {Models} {Without} {Supervision}},
	url = {http://arxiv.org/abs/2212.03827},
	doi = {10.48550/arXiv.2212.03827},
	abstract = {Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4{\textbackslash}\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
	month = mar,
	year = {2024},
	note = {arXiv:2212.03827},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{beigi_internalinspector_2024,
	address = {Miami, Florida, USA},
	title = {{InternalInspector} {I}{\textasciicircum}2: {Robust} {Confidence} {Estimation} in {LLMs} through {Internal} {States}},
	shorttitle = {{InternalInspector} {I}{\textasciicircum}2},
	url = {https://aclanthology.org/2024.findings-emnlp.751},
	abstract = {Despite their vast capabilities, Large Language Models (LLMs) often struggle with generating reliable outputs, frequently producing high-confidence inaccuracies known as hallucinations. Addressing this challenge, our research introduces InternalInspector, a novel framework designed to enhance confidence estimation in LLMs by leveraging contrastive learning on internal states including attention states, feed-forward states, and activation states of all layers. Unlike existing methods that primarily focus on the final activation state, InternalInspector conducts a comprehensive analysis across all internal states of every layer to accurately identify both correct and incorrect prediction processes. By benchmarking InternalInspector against existing confidence estimation methods across various natural language understanding and generation tasks, including factual question answering, commonsense reasoning, and reading comprehension, InternalInspector achieves significantly higher accuracy in aligning the estimated confidence scores with the correctness of the LLM's predictions and lower calibration error. Furthermore, InternalInspector excels at HaluEval, a hallucination detection benchmark, outperforming other internal-based confidence estimation methods in this task.},
	urldate = {2024-11-25},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Beigi, Mohammad and Shen, Ying and Yang, Runing and Lin, Zihao and Wang, Qifan and Mohan, Ankith and He, Jianfeng and Jin, Ming and Lu, Chang-Tien and Huang, Lifu},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {12847--12865},
}

@inproceedings{laurito_cluster-norm_2024,
	address = {Miami, Florida, USA},
	title = {Cluster-{Norm} for {Unsupervised} {Probing} of {Knowledge}},
	url = {https://aclanthology.org/2024.emnlp-main.780},
	abstract = {The deployment of language models brings challenges in generating reliable text, especially when these models are fine-tuned with human preferences. To extract the encoded knowledge in these models without (potentially) biased human labels, unsupervised probing techniques like Contrast-Consistent Search (CCS) have been developed (Burns et al., 2022). However, salient but unrelated features in activation space can mislead these probes (Farquhar et al., 2023). Addressing this, we propose a cluster-normalization method to minimize the impact of such features by clustering and normalizing activations of contrast pairs before applying unsupervised probing techniques. While this approach does not address the issue of distinguishing between latent knowledge and that portrayed by a simulated agent—a major issue in the literature of eliciting latent knowledge (Paul Christiano and Xu, 2021)—it still significantly improves the accuracy of probes in identifying the intended knowledge amidst distractions.},
	urldate = {2024-11-25},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Laurito, Walter and Maiya, Sharan and Dhimoïla, Grégoire and Yeung, Owen Ho Wan and Hänni, Kaarel},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {14083--14112},
}

@inproceedings{qi_model_2024,
	address = {Miami, Florida, USA},
	title = {Model {Internals}-based {Answer} {Attribution} for {Trustworthy} {Retrieval}-{Augmented} {Generation}},
	url = {https://aclanthology.org/2024.emnlp-main.347},
	abstract = {Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs' context usage throughout the generation. In this work, we present MIRAGE – Model Internals-based RAG Explanations – a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE's attributions and underscores the promising application of model internals for RAG answer attribution. Code and data released at https://github.com/Betswish/MIRAGE.},
	urldate = {2024-11-20},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Qi, Jirui and Sarti, Gabriele and Fernández, Raquel and Bisazza, Arianna},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {6037--6053},
}

@inproceedings{gottesman_estimating_2024,
	address = {Miami, Florida, USA},
	title = {Estimating {Knowledge} in {Large} {Language} {Models} {Without} {Generating} a {Single} {Token}},
	url = {https://aclanthology.org/2024.emnlp-main.232},
	urldate = {2024-11-20},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Gottesman, Daniela and Geva, Mor},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {3994--4019},
}

@misc{kalogeropoulos_scale_2024,
	title = {Scale {Equivariant} {Graph} {Metanetworks}},
	url = {http://arxiv.org/abs/2406.10685},
	abstract = {This paper pertains to an emerging machine learning paradigm: learning higher-order functions, i.e. functions whose inputs are functions themselves, \${\textbackslash}textit\{particularly when these inputs are Neural Networks (NNs)\}\$. With the growing interest in architectures that process NNs, a recurring design principle has permeated the field: adhering to the permutation symmetries arising from the connectionist structure of NNs. \${\textbackslash}textit\{However, are these the sole symmetries present in NN parameterizations\}\$? Zooming into most practical activation functions (e.g. sine, ReLU, tanh) answers this question negatively and gives rise to intriguing new symmetries, which we collectively refer to as \${\textbackslash}textit\{scaling symmetries\}\$, that is, non-zero scalar multiplications and divisions of weights and biases. In this work, we propose \${\textbackslash}textit\{Scale Equivariant Graph MetaNetworks - ScaleGMNs\}\$, a framework that adapts the Graph Metanetwork (message-passing) paradigm by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant to valid scalings. We introduce novel building blocks, of independent technical interest, that allow for equivariance or invariance with respect to individual scalar multipliers or their product and use them in all components of ScaleGMN. Furthermore, we prove that, under certain expressivity conditions, ScaleGMN can simulate the forward and backward pass of any input feedforward neural network. Experimental results demonstrate that our method advances the state-of-the-art performance for several datasets and activation functions, highlighting the power of scaling symmetries as an inductive bias for NN processing.},
	urldate = {2024-10-28},
	publisher = {arXiv},
	author = {Kalogeropoulos, Ioannis and Bouritsas, Giorgos and Panagakis, Yannis},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10685},
	keywords = {Computer Science - Machine Learning},
}

@misc{lim_graph_2023,
	title = {Graph {Metanetworks} for {Processing} {Diverse} {Neural} {Architectures}},
	url = {https://arxiv.org/abs/2312.04501v2},
	abstract = {Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks - neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.},
	language = {en},
	urldate = {2024-08-01},
	journal = {arXiv.org},
	author = {Lim, Derek and Maron, Haggai and Law, Marc T. and Lorraine, Jonathan and Lucas, James},
	month = dec,
	year = {2023},
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: {A} {Benchmark} for {Question} {Answering} {Research}},
	volume = {7},
	shorttitle = {Natural {Questions}},
	url = {https://aclanthology.org/Q19-1026},
	doi = {10.1162/tacl_a_00276},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	urldate = {2024-10-24},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
	year = {2019},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {452--466},
}

@misc{alain_understanding_2016,
	title = {Understanding intermediate layers using linear classifier probes},
	url = {https://arxiv.org/abs/1610.01644v4},
	abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
	language = {en},
	urldate = {2024-10-22},
	journal = {arXiv.org},
	author = {Alain, Guillaume and Bengio, Yoshua},
	month = oct,
	year = {2016},
}

@article{tian_fine-tuning_2023,
	title = {Fine-tuning {Language} {Models} for {Factuality}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2311.08401},
	doi = {10.48550/ARXIV.2311.08401},
	abstract = {The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58\% and 40\% reduction in factual error rate when generating biographies and answering medical questions, respectively.},
	urldate = {2024-09-27},
	author = {Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D. and Finn, Chelsea},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{chen_calibrating_2024,
	title = {Calibrating {Transformers} via {Sparse} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2303.02444},
	doi = {10.48550/arXiv.2303.02444},
	abstract = {Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Chen, Wenlong and Li, Yingzhen},
	month = jan,
	year = {2024},
	note = {arXiv:2303.02444 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hou_decomposing_2023,
	title = {Decomposing {Uncertainty} for {Large} {Language} {Models} through {Input} {Clarification} {Ensembling}},
	url = {http://arxiv.org/abs/2311.08718},
	doi = {10.48550/arXiv.2311.08718},
	abstract = {Uncertainty decomposition refers to the task of decomposing the total uncertainty of a model into data (aleatoric) uncertainty, resulting from the inherent complexity or ambiguity of the data, and model (epistemic) uncertainty, resulting from the lack of knowledge in the model. Performing uncertainty decomposition for large language models (LLMs) is an important step toward improving the reliability, trustworthiness, and interpretability of LLMs, but this research task is very challenging and remains unresolved. The existing canonical method, Bayesian Neural Network (BNN), cannot be applied to LLMs, because BNN requires training and ensembling multiple variants of models, which is infeasible or prohibitively expensive for LLMs. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarifications ensemble, which bypasses the need to train new models. Rather than ensembling models with different parameters, our approach generates a set of clarifications for the input, feeds them into the fixed LLMs, and ensembles the corresponding predictions. We show that our framework shares a symmetric decomposition structure with BNN. Empirical evaluations demonstrate that the proposed framework provides accurate and reliable uncertainty quantification on various tasks. Code will be made publicly available at https://github.com/UCSB-NLP-Chang/llm\_uncertainty .},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Hou, Bairu and Liu, Yujian and Qian, Kaizhi and Andreas, Jacob and Chang, Shiyu and Zhang, Yang},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08718 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{huang_look_2023,
	title = {Look {Before} {You} {Leap}: {An} {Exploratory} {Study} of {Uncertainty} {Measurement} for {Large} {Language} {Models}},
	shorttitle = {Look {Before} {You} {Leap}},
	url = {http://arxiv.org/abs/2307.10236},
	doi = {10.48550/arXiv.2307.10236},
	abstract = {The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Huang, Yuheng and Song, Jiayang and Wang, Zhijie and Zhao, Shengming and Chen, Huaming and Juefei-Xu, Felix and Ma, Lei},
	month = oct,
	year = {2023},
	note = {arXiv:2307.10236 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@inproceedings{kapoor_calibration-tuning_2024,
	address = {St Julians, Malta},
	title = {Calibration-{Tuning}: {Teaching} {Large} {Language} {Models} to {Know} {What} {They} {Don}'t {Know}},
	shorttitle = {Calibration-{Tuning}},
	url = {https://aclanthology.org/2024.uncertainlp-1.1},
	abstract = {Large language models are increasingly deployed for high-stakes decision making, for example in financial and medical applications. In such applications, it is imperative that we be able to estimate our confidence in the answers output by a language model in order to assess risks. Although we can easily compute the probability assigned by a language model to the sequence of tokens that make up an answer, we cannot easily compute the probability of the answer itself, which could be phrased in numerous ways.While other works have engineered ways of assigning such probabilities to LLM outputs, a key problem remains: existing language models are poorly calibrated, often confident when they are wrong or unsure when they are correct. In this work, we devise a protocol called *calibration tuning* for finetuning LLMs to output calibrated probabilities. Calibration-tuned models demonstrate superior calibration performance compared to existing language models on a variety of question-answering tasks, including open-ended generation, without affecting accuracy. We further show that this ability transfers to new domains outside of the calibration-tuning train set.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the 1st {Workshop} on {Uncertainty}-{Aware} {NLP} ({UncertaiNLP} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Kapoor, Sanyam and Gruver, Nate and Roberts, Manley and Pal, Arka and Dooley, Samuel and Goldblum, Micah and Wilson, Andrew},
	editor = {Vázquez, Raúl and Celikkanat, Hande and Ulmer, Dennis and Tiedemann, Jörg and Swayamdipta, Swabha and Aziz, Wilker and Plank, Barbara and Baan, Joris and de Marneffe, Marie-Catherine},
	month = mar,
	year = {2024},
	pages = {1--14},
}

@misc{kuhn_semantic_2023,
	title = {Semantic {Uncertainty}: {Linguistic} {Invariances} for {Uncertainty} {Estimation} in {Natural} {Language} {Generation}},
	shorttitle = {Semantic {Uncertainty}},
	url = {http://arxiv.org/abs/2302.09664},
	doi = {10.48550/arXiv.2302.09664},
	abstract = {We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of "semantic equivalence" -- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
	month = apr,
	year = {2023},
	note = {arXiv:2302.09664 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{manakul_selfcheckgpt_2023-1,
	title = {{SelfCheckGPT}: {Zero}-{Resource} {Black}-{Box} {Hallucination} {Detection} for {Generative} {Large} {Language} {Models}},
	shorttitle = {{SelfCheckGPT}},
	url = {http://arxiv.org/abs/2303.08896},
	doi = {10.48550/arXiv.2303.08896},
	abstract = {Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.},
	month = oct,
	year = {2023},
	note = {arXiv:2303.08896 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{tian_just_2023,
	title = {Just {Ask} for {Calibration}: {Strategies} for {Eliciting} {Calibrated} {Confidence} {Scores} from {Language} {Models} {Fine}-{Tuned} with {Human} {Feedback}},
	shorttitle = {Just {Ask} for {Calibration}},
	url = {http://arxiv.org/abs/2305.14975},
	doi = {10.48550/arXiv.2305.14975},
	abstract = {A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50\%.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D.},
	month = oct,
	year = {2023},
	note = {arXiv:2305.14975 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{su_unsupervised_2024,
	title = {Unsupervised {Real}-{Time} {Hallucination} {Detection} based on the {Internal} {States} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2403.06448},
	doi = {10.48550/arXiv.2403.06448},
	abstract = {Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Su, Weihang and Wang, Changyue and Ai, Qingyao and HU, Yiran and Wu, Zhijing and Zhou, Yujia and Liu, Yiqun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.06448 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{ulmer_calibrating_2024,
	title = {Calibrating {Large} {Language} {Models} {Using} {Their} {Generations} {Only}},
	url = {http://arxiv.org/abs/2403.05973},
	doi = {10.48550/arXiv.2403.05973},
	abstract = {As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge. We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively in terms of calibration error for white-box and black-box LLMs on closed-book question-answering to detect incorrect LLM answers.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Ulmer, Dennis and Gubri, Martin and Lee, Hwaran and Yun, Sangdoo and Oh, Seong Joon},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05973 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{gupta_language_2024,
	title = {Language {Model} {Cascades}: {Token}-level uncertainty and beyond},
	shorttitle = {Language {Model} {Cascades}},
	url = {http://arxiv.org/abs/2404.10136},
	abstract = {Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. Cascading offers a simple strategy to achieve more favorable cost-quality tradeoffs: here, a small model is invoked for most "easy" instances, while a few "hard" instances are deferred to the large model. While the principles underpinning cascading are well-studied for classification tasks - with deferral based on predicted class uncertainty favored theoretically and practically - a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across examples. To mitigate this issue, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.},
	urldate = {2024-04-17},
	publisher = {arXiv},
	author = {Gupta, Neha and Narasimhan, Harikrishna and Jitkrittum, Wittawat and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10136 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{guerreiro_looking_2023,
	address = {Dubrovnik, Croatia},
	title = {Looking for a {Needle} in a {Haystack}: {A} {Comprehensive} {Study} of {Hallucinations} in {Neural} {Machine} {Translation}},
	shorttitle = {Looking for a {Needle} in a {Haystack}},
	url = {https://aclanthology.org/2023.eacl-main.75},
	doi = {10.18653/v1/2023.eacl-main.75},
	abstract = {Although the problem of hallucinations in neural machine translation (NMT) has received some attention, research on this highly pathological phenomenon lacks solid ground. Previous work has been limited in several ways: it often resorts to artificial settings where the problem is amplified, it disregards some (common) types of hallucinations, and it does not validate adequacy of detection heuristics. In this paper, we set foundations for the study of NMT hallucinations. First, we work in a natural setting, i.e., in-domain data without artificial noise neither in training nor in inference. Next, we annotate a dataset of over 3.4k sentences indicating different kinds of critical errors and hallucinations. Then, we turn to detection methods and both revisit methods used previously and propose using glass-box uncertainty-based detectors. Overall, we show that for preventive settings, (i) previously used methods are largely inadequate, (ii) sequence log-probability works best and performs on par with reference-based methods. Finally, we propose DeHallucinator, a simple method for alleviating hallucinations at test time that significantly reduces the hallucinatory rate.},
	urldate = {2024-05-31},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Guerreiro, Nuno M. and Voita, Elena and Martins, André},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {1059--1075},
}

@inproceedings{zhang_enhancing_2023,
	address = {Singapore},
	title = {Enhancing {Uncertainty}-{Based} {Hallucination} {Detection} with {Stronger} {Focus}},
	url = {https://aclanthology.org/2023.emnlp-main.58},
	doi = {10.18653/v1/2023.emnlp-main.58},
	abstract = {Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information.},
	urldate = {2024-05-31},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Tianhang and Qiu, Lin and Guo, Qipeng and Deng, Cheng and Zhang, Yue and Zhang, Zheng and Zhou, Chenghu and Wang, Xinbing and Fu, Luoyi},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {915--932},
}

@misc{liu_uncertainty_2024,
	title = {Uncertainty {Estimation} and {Quantification} for {LLMs}: {A} {Simple} {Supervised} {Approach}},
	shorttitle = {Uncertainty {Estimation} and {Quantification} for {LLMs}},
	url = {http://arxiv.org/abs/2404.15993},
	doi = {10.48550/arXiv.2404.15993},
	abstract = {In this paper, we study the problem of uncertainty estimation and calibration for LLMs. We first formulate the uncertainty estimation problem for LLMs and then propose a supervised approach that takes advantage of the labeled datasets and estimates the uncertainty of the LLMs' responses. Based on the formulation, we illustrate the difference between the uncertainty estimation for LLMs and that for standard ML models and explain why the hidden neurons of the LLMs may contain uncertainty information. Our designed approach demonstrates the benefits of utilizing hidden activations to enhance uncertainty estimation across various tasks and shows robust transferability in out-of-distribution settings. We distinguish the uncertainty estimation task from the uncertainty calibration task and show that a better uncertainty estimation mode leads to a better calibration performance. Furthermore, our method is easy to implement and adaptable to different levels of model accessibility including black box, grey box, and white box.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Liu, Linyu and Pan, Yu and Li, Xiaocheng and Chen, Guanting},
	month = apr,
	year = {2024},
	note = {arXiv:2404.15993 [cs]
version: 1},
	keywords = {68T07, 68T50, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{malinin_uncertainty_2021,
	title = {Uncertainty {Estimation} in {Autoregressive} {Structured} {Prediction}},
	url = {http://arxiv.org/abs/2002.07650},
	abstract = {Uncertainty estimation is important for ensuring safety and robustness of AI systems. While most research in the area has focused on un-structured prediction tasks, limited work has investigated general uncertainty estimation approaches for structured prediction. Thus, this work aims to investigate uncertainty estimation for autoregressive structured prediction tasks within a single uniﬁed and interpretable probabilistic ensemble-based framework. We consider: uncertainty estimation for sequence data at the token-level and complete sequence-level; interpretations for, and applications of, various measures of uncertainty; and discuss both the theoretical and practical challenges associated with obtaining them. This work also provides baselines for token-level and sequence-level error detection, and sequence-level out-of-domain input detection on the WMT’14 English-French and WMT’17 English-German translation and LibriSpeech speech recognition datasets.},
	language = {en},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Malinin, Andrey and Gales, Mark},
	month = feb,
	year = {2021},
	note = {arXiv:2002.07650 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{aghajanyan_intrinsic_2020,
	title = {Intrinsic {Dimensionality} {Explains} the {Effectiveness} of {Language} {Model} {Fine}-{Tuning}},
	url = {http://arxiv.org/abs/2012.13255},
	abstract = {Although pretrained language models can be ﬁne-tuned to produce state-of-theart results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing ﬁne-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for ﬁne-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90\% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a ﬁxed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.},
	language = {en},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
	month = dec,
	year = {2020},
	note = {arXiv:2012.13255 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{xiong_can_2023,
	title = {Can {LLMs} {Express} {Their} {Uncertainty}? {An} {Empirical} {Evaluation} of {Confidence} {Elicitation} in {LLMs}},
	shorttitle = {Can {LLMs} {Express} {Their} {Uncertainty}?},
	url = {https://openreview.net/forum?id=gjeQKFxFpZ},
	abstract = {Empowering large language models (LLMs) to accurately express confidence in their answers is essential for reliable and trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on *white-box access* to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of *black-box* approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: *prompting* strategies for eliciting verbalized confidence, *sampling* methods for generating multiple responses, and *aggregation* techniques for computing consistency. We then benchmark these methods on two key tasks—confidence calibration and failure prediction—across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be *overconfident*, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve, yet still far from ideal performance. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs. The code is publicly available at https://github.com/MiaoXiong2320/llm-uncertainty.},
	language = {en},
	urldate = {2024-05-31},
	author = {Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
	month = oct,
	year = {2023},
}

@misc{wang_hidden_2024,
	title = {Hidden {Question} {Representations} {Tell} {Non}-{Factuality} {Within} and {Across} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.05328},
	doi = {10.48550/arXiv.2406.05328},
	abstract = {Despite the remarkable advance of large language models (LLMs), the prevalence of non-factual responses remains a common issue. This work studies non-factuality prediction (NFP), which predicts whether an LLM will generate non-factual responses to a question before the generation process. Previous efforts on NFP usually rely on extensive computation. In this work, we conduct extensive analysis to explore the capabilities of using a lightweight probe to elicit ``whether an LLM knows'' from the hidden representations of questions. Additionally, we discover that the non-factuality probe employs similar patterns for NFP across multiple LLMs. Motivated by the intriguing finding, we conduct effective transfer learning for cross-LLM NFP and propose a question-aligned strategy to ensure the efficacy of mini-batch based training.},
	urldate = {2024-07-31},
	publisher = {arXiv},
	author = {Wang, Yanling and Li, Haoyang and Zou, Hao and Zhang, Jing and He, Xinlei and Li, Qi and Xu, Ke},
	month = jun,
	year = {2024},
	note = {arXiv:2406.05328 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{geng_survey_2024,
	address = {Mexico City, Mexico},
	title = {A {Survey} of {Confidence} {Estimation} and {Calibration} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.naacl-long.366},
	doi = {10.18653/v1/2024.naacl-long.366},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impressive performance, they can be unreliable due to factual errors in their generations. Assessing their confidence and calibrating them across different tasks can help mitigate risks and enable LLMs to produce better generations. There has been a lot of recent research aiming to address this, but there has been no comprehensive overview to organize it and to outline the main lessons learned. The present survey aims to bridge this gap. In particular, we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration. We further discuss their applications and suggest promising directions for future work.},
	urldate = {2024-08-01},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Geng, Jiahui and Cai, Fengyu and Wang, Yuxia and Koeppl, Heinz and Nakov, Preslav and Gurevych, Iryna},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {6577--6595},
}

@misc{ju_how_2024,
	title = {How {Large} {Language} {Models} {Encode} {Context} {Knowledge}? {A} {Layer}-{Wise} {Probing} {Study}},
	shorttitle = {How {Large} {Language} {Models} {Encode} {Context} {Knowledge}?},
	url = {http://arxiv.org/abs/2402.16061},
	abstract = {Previous work has showcased the intriguing capability of large language models (LLMs) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of LLMs through probing tasks. We leverage the powerful generative capability of ChatGPT to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ V-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens at lower layers while progressively expanding more knowledge within other tokens at upper layers; and (3) gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence. Code is publicly available at https://github.com/Jometeorie/probing\_llama.},
	language = {en},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Ju, Tianjie and Sun, Weiwei and Du, Wei and Yuan, Xinwei and Ren, Zhaochun and Liu, Gongshen},
	month = mar,
	year = {2024},
	note = {arXiv:2402.16061 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yin_characterizing_2024,
	title = {Characterizing {Truthfulness} in {Large} {Language} {Model} {Generations} with {Local} {Intrinsic} {Dimension}},
	url = {http://arxiv.org/abs/2402.18048},
	doi = {10.48550/arXiv.2402.18048},
	abstract = {We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep\#abstractsf our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Yin, Fan and Srinivasa, Jayanth and Chang, Kai-Wei},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18048 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{subramani_extracting_2022,
	title = {Extracting {Latent} {Steering} {Vectors} from {Pretrained} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2205.05124},
	doi = {10.48550/ARXIV.2205.05124},
	abstract = {Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (\&gt; 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.},
	urldate = {2024-08-01},
	author = {Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E.},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{kossen_semantic_2024,
	title = {Semantic {Entropy} {Probes}: {Robust} and {Cheap} {Hallucination} {Detection} in {LLMs}},
	shorttitle = {Semantic {Entropy} {Probes}},
	url = {http://arxiv.org/abs/2406.15927},
	doi = {10.48550/arXiv.2406.15927},
	abstract = {We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Kossen, Jannik and Han, Jiatong and Razzak, Muhammed and Schut, Lisa and Malik, Shreshth and Gal, Yarin},
	month = jun,
	year = {2024},
	note = {arXiv:2406.15927 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {Eliciting} {Latent} {Predictions} from {Transformers} with the {Tuned} {Lens} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/reader/762ca2711eb167f19b79e39c175708ca15e1f5d7},
	urldate = {2024-08-01},
}

@article{pal_future_2023,
	title = {Future {Lens}: {Anticipating} {Subsequent} {Tokens} from a {Single} {Hidden} {State}},
	shorttitle = {Future {Lens}},
	url = {https://aclanthology.org/2023.conll-1.37},
	doi = {10.18653/v1/2023.conll-1.37},
	abstract = {We conjecture that hidden state vectors corresponding to individual input tokens encode information sufficient to accurately predict several tokens ahead. More concretely, in this paper we ask: Given a hidden (internal) representation of a single token at position t in an input, can we reliably anticipate the tokens that will appear at positions ≥ t + 2? To test this, we measure linear approximation and causal intervention methods in GPT-J-6B to evaluate the degree to which individual hidden states in the network contain signal rich enough to predict future hidden states and, ultimately, token outputs. We find that, at some layers, we can approximate a model’s output with more than 48\% accuracy with respect to its prediction of subsequent tokens through a single hidden state. Finally we present a “Future Lens” visualization that uses these methods to create a new view of transformer states.},
	language = {en},
	urldate = {2024-08-01},
	journal = {Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)},
	author = {Pal, Koyena and Sun, Jiuding and Yuan, Andrew and Wallace, Byron and Bau, David},
	year = {2023},
	note = {Conference Name: Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)
Place: Singapore
Publisher: Association for Computational Linguistics},
	pages = {548--560},
}

@misc{noauthor_pdf_nodate-1,
	title = {[{PDF}] {Discovering} {Latent} {Knowledge} in {Language} {Models} {Without} {Supervision} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/reader/89c3bd70ad33c4f8832f00ab98872b77861ee0ec},
	urldate = {2024-08-01},
}

@misc{valeriani_geometry_2023,
	title = {The geometry of hidden representations of large transformer models},
	url = {http://arxiv.org/abs/2302.00294},
	doi = {10.48550/arXiv.2302.00294},
	abstract = {Large transformers are powerful architectures used for self-supervised data analysis across various data types, including protein sequences, images, and text. In these models, the semantic structure of the dataset emerges from a sequence of transformations between one representation and the next. We characterize the geometric and statistical properties of these representations and how they change as we move through the layers. By analyzing the intrinsic dimension (ID) and neighbor composition, we find that the representations evolve similarly in transformers trained on protein language tasks and image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic information of the dataset is better expressed at the end of the first peak, and this phenomenon can be observed across many models trained on diverse datasets. Based on our findings, we point out an explicit strategy to identify, without supervision, the layers that maximize semantic content: representations at intermediate layers corresponding to a relative minimum of the ID profile are more suitable for downstream learning tasks.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Valeriani, Lucrezia and Doimo, Diego and Cuturello, Francesca and Laio, Alessandro and Ansuini, Alessio and Cazzaniga, Alberto},
	month = oct,
	year = {2023},
	note = {arXiv:2302.00294 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ahdritz_distinguishing_2024,
	title = {Distinguishing the {Knowable} from the {Unknowable} with {Language} {Models}},
	url = {http://arxiv.org/abs/2402.03563},
	abstract = {We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM’s uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings.},
	language = {en},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Ahdritz, Gustaf and Qin, Tian and Vyas, Nikhil and Barak, Boaz and Edelman, Benjamin L.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03563 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{bi_is_2024,
	title = {Is {Factuality} {Decoding} a {Free} {Lunch} for {LLMs}? {Evaluation} on {Knowledge} {Editing} {Benchmark}},
	shorttitle = {Is {Factuality} {Decoding} a {Free} {Lunch} for {LLMs}?},
	url = {http://arxiv.org/abs/2404.00216},
	abstract = {The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.},
	language = {en},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Bi, Baolong and Liu, Shenghua and Wang, Yiwei and Mei, Lingrui and Cheng, Xueqi},
	month = mar,
	year = {2024},
	note = {arXiv:2404.00216 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{li_inference-time_2024,
	title = {Inference-{Time} {Intervention}: {Eliciting} {Truthful} {Answers} from a {Language} {Model}},
	shorttitle = {Inference-{Time} {Intervention}},
	url = {http://arxiv.org/abs/2306.03341},
	doi = {10.48550/arXiv.2306.03341},
	abstract = {We introduce Inference-Time Intervention (ITI), a technique designed to enhance the "truthfulness" of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5\% to 65.1\%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Li, Kenneth and Patel, Oam and Viégas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
	month = jun,
	year = {2024},
	note = {arXiv:2306.03341 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{varshney_stitch_2023,
	title = {A {Stitch} in {Time} {Saves} {Nine}: {Detecting} and {Mitigating} {Hallucinations} of {LLMs} by {Validating} {Low}-{Confidence} {Generation}},
	shorttitle = {A {Stitch} in {Time} {Saves} {Nine}},
	url = {http://arxiv.org/abs/2307.03987},
	doi = {10.48550/arXiv.2307.03987},
	abstract = {Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to 'hallucinate' which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with GPT-3.5 (text-davinci-003) on the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of {\textasciitilde}88\% and the mitigation technique successfully mitigates 57.6\% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. Then, we show that the proposed active detection and mitigation approach successfully reduces the hallucinations of the GPT-3.5 model from 47.5\% to 14.5\% on average. We further demonstrate the effectiveness and wide applicability of our approach through additional studies including performance on different types of questions (multi-hop and false premise questions) and with another LLM from a different model family (Vicuna). In summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Varshney, Neeraj and Yao, Wenlin and Zhang, Hongming and Chen, Jianshu and Yu, Dong},
	month = aug,
	year = {2023},
	note = {arXiv:2307.03987 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{duan_llms_2024,
	title = {Do {LLMs} {Know} about {Hallucination}? {An} {Empirical} {Investigation} of {LLM}'s {Hidden} {States}},
	shorttitle = {Do {LLMs} {Know} about {Hallucination}?},
	url = {http://arxiv.org/abs/2402.09733},
	doi = {10.48550/arXiv.2402.09733},
	abstract = {Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023). Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidden representation space to mitigate hallucination. We believe this work provides insights into how LLMs produce hallucinated answers and how to make them occur less often.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Duan, Hanyu and Yang, Yi and Tam, Kar Yan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09733 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{liu_cognitive_2023,
	address = {Singapore},
	title = {Cognitive {Dissonance}: {Why} {Do} {Language} {Model} {Outputs} {Disagree} with {Internal} {Representations} of {Truthfulness}?},
	shorttitle = {Cognitive {Dissonance}},
	url = {https://aclanthology.org/2023.emnlp-main.291},
	doi = {10.18653/v1/2023.emnlp-main.291},
	abstract = {Neural language models (LMs) can be used to evaluate the truth of factual statements in two ways: they can be either queried for statement probabilities, or probed for internal representations of truthfulness. Past work has found that these two procedures sometimes disagree, and that probes tend to be more accurate than LM outputs. This has led some researchers to conclude that LMs"lie"or otherwise encode non-cooperative communicative intents. Is this an accurate description of today's LMs, or can query-probe disagreement arise in other ways? We identify three different classes of disagreement, which we term confabulation, deception, and heterogeneity. In many cases, the superiority of probes is simply attributable to better calibration on uncertain answers rather than a greater fraction of correct, high-confidence answers. In some cases, queries and probes perform better on different subsets of inputs, and accuracy can further be improved by ensembling the two. Code is available at github.com/lingo-mit/lm-truthfulness.},
	language = {en},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Kevin and Casper, Stephen and Hadfield-Menell, Dylan and Andreas, Jacob},
	year = {2023},
	pages = {4791--4797},
}

@inproceedings{zhao_towards_2024,
	title = {Towards {Uncovering} {How} {Large} {Language} {Model} {Works}: {An} {Explainability} {Perspective}},
	shorttitle = {Towards {Uncovering} {How} {Large} {Language} {Model} {Works}},
	url = {https://www.semanticscholar.org/paper/Towards-Uncovering-How-Large-Language-Model-Works%3A-Zhao-Yang/4f60009234e76f9f8969f6cca23b3b07e944e984},
	abstract = {Large language models (LLMs) have led to breakthroughs in language tasks, yet the internal mechanisms that enable their remarkable generalization and reasoning abilities remain opaque. This lack of transparency presents challenges such as hallucinations, toxicity, and misalignment with human values, hindering the safe and beneficial deployment of LLMs. This paper aims to uncover the mechanisms underlying LLM functionality through the lens of explainability. First, we review how knowledge is architecturally composed within LLMs and encoded in their internal parameters via mechanistic interpretability techniques. Then, we summarize how knowledge is embedded in LLM representations by leveraging probing techniques and representation engineering. Additionally, we investigate the training dynamics through a mechanistic perspective to explain phenomena such as grokking and memorization. Lastly, we explore how the insights gained from these explanations can enhance LLM performance through model editing, improve efficiency through pruning, and better align with human values.},
	urldate = {2024-08-09},
	author = {Zhao, Haiyan and Yang, Fan and Shen, Bo and Lakkaraju, Himabindu and Du, Mengnan},
	month = feb,
	year = {2024},
}

@article{zou_representation_2023,
	title = {Representation {Engineering}: {A} {Top}-{Down} {Approach} to {AI} {Transparency}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Representation {Engineering}},
	url = {https://arxiv.org/abs/2310.01405},
	doi = {10.48550/ARXIV.2310.01405},
	abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
	urldate = {2024-08-09},
	author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Computers and Society (cs.CY), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{duan_shifting_2024,
	title = {Shifting {Attention} to {Relevance}: {Towards} the {Predictive} {Uncertainty} {Quantification} of {Free}-{Form} {Large} {Language} {Models}},
	shorttitle = {Shifting {Attention} to {Relevance}},
	url = {http://arxiv.org/abs/2307.01379},
	doi = {10.48550/arXiv.2307.01379},
	abstract = {Large Language Models (LLMs) show promising results in language generation and instruction following but frequently "hallucinate", making their outputs less reliable. Despite Uncertainty Quantification's (UQ) potential solutions, implementing it accurately within LLMs is challenging. Our research introduces a simple heuristic: not all tokens in auto-regressive LLM text equally represent the underlying meaning, as "linguistic redundancy" often allows a few keywords to convey the essence of long sentences. However, current methods underestimate this inequality when assessing uncertainty, causing tokens with limited semantics to be equally or excessively weighted in UQ. To correct this, we propose Shifting Attention to more Relevant (SAR) components at both token- and sentence-levels for better UQ. We conduct extensive experiments involving a range of popular "off-the-shelf" LLMs, such as Vicuna, WizardLM, and LLaMA-2-chat, with model sizes extending up to 33B parameters. We evaluate various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q\&A, and medical Q\&A. Our experimental results, coupled with a comprehensive demographic analysis, demonstrate the superior performance of SAR. The code is available at https://github.com/jinhaoduan/SAR.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Duan, Jinhao and Cheng, Hao and Wang, Shiqi and Zavalny, Alex and Wang, Chenan and Xu, Renjing and Kailkhura, Bhavya and Xu, Kaidi},
	month = may,
	year = {2024},
	note = {arXiv:2307.01379 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{lu_insights_2024,
	title = {Insights into {LLM} {Long}-{Context} {Failures}: {When} {Transformers} {Know} but {Don}'t {Tell}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Insights into {LLM} {Long}-{Context} {Failures}},
	url = {https://arxiv.org/abs/2406.14673},
	doi = {10.48550/ARXIV.2406.14673},
	abstract = {Large Language Models (LLMs) exhibit positional bias, struggling to utilize information from the middle or end of long contexts. Our study explores LLMs' long-context reasoning by probing their hidden representations. We find that while LLMs encode the position of target information, they often fail to leverage this in generating accurate responses. This reveals a disconnect between information retrieval and utilization, a "know but don't tell" phenomenon. We further analyze the relationship between extraction time and final accuracy, offering insights into the underlying mechanics of transformer models.},
	urldate = {2024-08-09},
	author = {Lu, Taiming and Gao, Muhan and Yu, Kuai and Byerly, Adam and Khashabi, Daniel},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{liang_learning_2024,
	title = {Learning to {Trust} {Your} {Feelings}: {Leveraging} {Self}-awareness in {LLMs} for {Hallucination} {Mitigation}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Learning to {Trust} {Your} {Feelings}},
	url = {https://arxiv.org/abs/2401.15449},
	doi = {10.48550/ARXIV.2401.15449},
	abstract = {We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85\% accuracy in knowledge probing. However, LLMs often fail to express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, Dreamcatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks.},
	urldate = {2024-08-09},
	author = {Liang, Yuxin and Song, Zhuoyang and Wang, Hao and Zhang, Jiaxing},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{stolfo_confidence_2024,
	title = {Confidence {Regulation} {Neurons} in {Language} {Models}},
	url = {http://arxiv.org/abs/2406.16254},
	abstract = {Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this uncertainty: the recently discovered entropy neurons and a new set of components that we term token frequency neurons. Entropy neurons are characterized by an unusually high weight norm and influence the final layer normalization (LayerNorm) scale to effectively scale down the logits. Our work shows that entropy neurons operate by writing onto an unembedding null space, allowing them to impact the residual stream norm with minimal direct effect on the logits themselves. We observe the presence of entropy neurons across a range of models, up to 7 billion parameters. On the other hand, token frequency neurons, which we discover and describe here for the first time, boost or suppress each token’s logit proportionally to its log frequency, thereby shifting the output distribution towards or away from the unigram distribution. Finally, we present a detailed case study where entropy neurons actively manage confidence in the setting of induction, i.e. detecting and continuing repeated subsequences.},
	language = {en},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Stolfo, Alessandro and Wu, Ben and Gurnee, Wes and Belinkov, Yonatan and Song, Xingyi and Sachan, Mrinmaya and Nanda, Neel},
	month = jun,
	year = {2024},
	note = {arXiv:2406.16254 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ye_benchmarking_2024,
	title = {Benchmarking {LLMs} via {Uncertainty} {Quantification}},
	url = {http://arxiv.org/abs/2401.12794},
	abstract = {The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs.},
	language = {en},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Ye, Fanghua and Yang, Mingming and Pang, Jianhui and Wang, Longyue and Wong, Derek F. and Yilmaz, Emine and Shi, Shuming and Tu, Zhaopeng},
	month = apr,
	year = {2024},
	note = {arXiv:2401.12794 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{phd_about_2023,
	title = {About {LayerNorm} {Variants} in the {Original} {Transformer} {Paper}, and {Some} {Other} {Interesting} {Historical} {Tidbits} {About} {LLMs}},
	url = {https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure},
	abstract = {A few months ago, I shared the article, Understanding Large Language Models: A Cross-Section of the Most Relevant Literature To Get Up to Speed, and the positive feedback was very motivating!},
	language = {en},
	urldate = {2024-08-12},
	author = {PhD, Sebastian Raschka},
	month = apr,
	year = {2023},
}

@misc{noauthor_llms_nodate,
	title = {{LLMs} {May} {Not} {Need} {Dense} {Self} {Attention} {\textbar} by {Building} {Blocks} {\textbar} {Medium}},
	url = {https://medium.com/@buildingblocks/llms-may-not-need-dense-self-attention-1fa3bf47522e},
	urldate = {2024-08-10},
}

@misc{wang_blob_2024,
	title = {{BLoB}: {Bayesian} {Low}-{Rank} {Adaptation} by {Backpropagation} for {Large} {Language} {Models}},
	shorttitle = {{BLoB}},
	url = {http://arxiv.org/abs/2406.11675},
	abstract = {Large Language Models (LLMs) often suffer from overconfidence during inference, particularly when adapted to downstream domain-specific tasks with limited data. Previous work addresses this issue by employing approximate Bayesian estimation after the LLMs are trained, enabling them to quantify uncertainty. However, such post-training approaches’ performance is severely limited by the parameters learned during training. In this paper, we go beyond post-training Bayesianization and propose Bayesian Low-Rank Adaptation by Backpropagation (BLoB), an algorithm that continuously and jointly adjusts both the mean and covariance of LLM parameters throughout the whole fine-tuning process. Our empirical results verify the effectiveness of BLoB in terms of generalization and uncertainty estimation, when evaluated on both in-distribution and out-of-distribution data.},
	language = {en},
	urldate = {2024-08-14},
	publisher = {arXiv},
	author = {Wang, Yibin and Shi, Haizhou and Han, Ligong and Metaxas, Dimitris and Wang, Hao},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11675 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yadkori_believe_2024,
	title = {To {Believe} or {Not} to {Believe} {Your} {LLM}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2406.02543},
	doi = {10.48550/ARXIV.2406.02543},
	abstract = {We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.},
	urldate = {2024-09-15},
	author = {Yadkori, Yasin Abbasi and Kuzborskij, Ilja and György, András and Szepesvári, Csaba},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{farquhar_challenges_2023,
	title = {Challenges with unsupervised {LLM} knowledge discovery},
	url = {http://arxiv.org/abs/2312.10029},
	abstract = {We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent. The idea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure, which can be used to discover knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method, contrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a series of experiments showing settings in which unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. Conceptually, we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods.},
	language = {en},
	urldate = {2024-08-14},
	publisher = {arXiv},
	author = {Farquhar, Sebastian and Varma, Vikrant and Kenton, Zachary and Gasteiger, Johannes and Mikulik, Vladimir and Shah, Rohin},
	month = dec,
	year = {2023},
	note = {arXiv:2312.10029 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{noauthor_pdf_nodate-2,
	title = {[{PDF}] {Examining} {LLMs}' {Uncertainty} {Expression} {Towards} {Questions} {Outside} {Parametric} {Knowledge} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Examining-LLMs'-Uncertainty-Expression-Towards-Liu-Wang/1834f8126e97057e321149b50e342754a096d14d},
	urldate = {2024-08-20},
}

@misc{noauthor_240712831_nodate,
	title = {[2407.12831] {Truth} is {Universal}: {Robust} {Detection} of {Lies} in {LLMs}},
	url = {https://arxiv.org/abs/2407.12831},
	urldate = {2024-09-27},
}

@misc{noauthor_pdf_nodate-3,
	title = {[{PDF}] {Truth} is {Universal}: {Robust} {Detection} of {Lies} in {LLMs} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Truth-is-Universal%3A-Robust-Detection-of-Lies-in-B%C3%BCrger-Hamprecht/b169c56cbe546d3e1ff12003ff5ab6f57cba608e?citedSort=relevance},
	urldate = {2024-09-27},
}

@misc{noauthor_190510650_nodate,
	title = {[1905.10650] {Are} {Sixteen} {Heads} {Really} {Better} than {One}?},
	url = {https://arxiv.org/abs/1905.10650},
	urldate = {2024-09-27},
}

@misc{du_haloscope_2024,
	title = {{HaloScope}: {Harnessing} {Unlabeled} {LLM} {Generations} for {Hallucination} {Detection}},
	shorttitle = {{HaloScope}},
	url = {https://arxiv.org/abs/2409.17504v1},
	abstract = {The surge in applications of large language models (LLMs) has prompted concerns about the generation of misleading or fabricated information, known as hallucinations. Therefore, detecting hallucinations has become critical to maintaining trust in LLM-generated content. A primary challenge in learning a truthfulness classifier is the lack of a large amount of labeled truthful and hallucinated data. To address the challenge, we introduce HaloScope, a novel learning framework that leverages the unlabeled LLM generations in the wild for hallucination detection. Such unlabeled data arises freely upon deploying LLMs in the open world, and consists of both truthful and hallucinated information. To harness the unlabeled data, we present an automated membership estimation score for distinguishing between truthful and untruthful generations within unlabeled mixture data, thereby enabling the training of a binary truthfulness classifier on top. Importantly, our framework does not require extra data collection and human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiments show that HaloScope can achieve superior hallucination detection performance, outperforming the competitive rivals by a significant margin. Code is available at https://github.com/deeplearningwisc/haloscope.},
	language = {en},
	urldate = {2024-09-30},
	journal = {arXiv.org},
	author = {Du, Xuefeng and Xiao, Chaowei and Li, Yixuan},
	month = sep,
	year = {2024},
}

@misc{noauthor_pdf_nodate-4,
	title = {[{PDF}] {In}-{Context} {Sharpness} as {Alerts}: {An} {Inner} {Representation} {Perspective} for {Hallucination} {Mitigation} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/In-Context-Sharpness-as-Alerts%3A-An-Inner-for-Chen-Xiong/ed8f46f493abce9498851e647d900a4628f6ff7f},
	urldate = {2024-09-27},
}

@article{arditi_refusal_2024,
	title = {Refusal in {LLMs} is mediated by a single direction},
	url = {https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction},
	abstract = {This work was produced as part of Neel Nanda's stream in the ML Alignment \& Theory Scholars Program - Winter 2023-24 Cohort, with co-supervision from…},
	language = {en},
	urldate = {2024-09-30},
	author = {Arditi, Andy and Obeso, Oscar and Aaquib111 and wesg and Nanda, Neel},
	month = apr,
	year = {2024},
}

@article{burger_truth_2024,
	title = {Truth is {Universal}: {Robust} {Detection} of {Lies} in {LLMs}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Truth is {Universal}},
	url = {https://arxiv.org/abs/2407.12831},
	doi = {10.48550/ARXIV.2407.12831},
	abstract = {Large Language Models (LLMs) have revolutionised natural language processing, exhibiting impressive human-like capabilities. In particular, LLMs are capable of "lying", knowingly outputting false statements. Hence, it is of interest and importance to develop methods to detect when LLMs lie. Indeed, several authors trained classifiers to detect LLM lies based on their internal model activations. However, other researchers showed that these classifiers may fail to generalise, for example to negated statements. In this work, we aim to develop a robust method to detect when an LLM is lying. To this end, we make the following key contributions: (i) We demonstrate the existence of a two-dimensional subspace, along which the activation vectors of true and false statements can be separated. Notably, this finding is universal and holds for various LLMs, including Gemma-7B, LLaMA2-13B and LLaMA3-8B. Our analysis explains the generalisation failures observed in previous studies and sets the stage for more robust lie detection; (ii) Building upon (i), we construct an accurate LLM lie detector. Empirically, our proposed classifier achieves state-of-the-art performance, distinguishing simple true and false statements with 94\% accuracy and detecting more complex real-world lies with 95\% accuracy.},
	urldate = {2024-09-30},
	author = {Bürger, Lennart and Hamprecht, Fred A. and Nadler, Boaz},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{levinstein_still_2024,
	title = {Still {No} {Lie} {Detector} for {Language} {Models}: {Probing} {Empirical} and {Conceptual} {Roadblocks}},
	issn = {0031-8116, 1573-0883},
	shorttitle = {Still {No} {Lie} {Detector} for {Language} {Models}},
	url = {http://arxiv.org/abs/2307.00175},
	doi = {10.1007/s11098-023-02094-3},
	abstract = {We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.},
	language = {en},
	urldate = {2024-09-30},
	journal = {Philosophical Studies},
	author = {Levinstein, B. A. and Herrmann, Daniel A.},
	month = feb,
	year = {2024},
	note = {arXiv:2307.00175 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kadavath_language_2022,
	title = {Language {Models} ({Mostly}) {Know} {What} {They} {Know}},
	url = {http://arxiv.org/abs/2207.05221},
	abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We ﬁrst show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to ﬁrst propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We ﬁnd encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one speciﬁc possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
	language = {en},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = nov,
	year = {2022},
	note = {arXiv:2207.05221 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_pdf_nodate-5,
	title = {[{PDF}] {LLM} {Factoscope}: {Uncovering} {LLMs}' {Factual} {Discernment} through {Inner} {States} {Analysis} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/reader/b9ee59e296dc2d7d4ae3f3346a1f0ba7ea881570},
	urldate = {2024-09-30},
}

@inproceedings{jiang_large_2024,
	address = {Mexico City, Mexico},
	title = {On {Large} {Language} {Models}' {Hallucination} with {Regard} to {Known} {Facts}},
	url = {https://aclanthology.org/2024.naacl-long.60},
	doi = {10.18653/v1/2024.naacl-long.60},
	abstract = {Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen.Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space.We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token's information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model.Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88\% success rate. Our study shed light on understanding the reasons for LLMs' hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.},
	urldate = {2024-09-30},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Che and Qi, Biqing and Hong, Xiangyu and Fu, Dayuan and Cheng, Yang and Meng, Fandong and Yu, Mo and Zhou, Bowen and Zhou, Jie},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {1041--1053},
}

@inproceedings{yuksekgonul_attention_2023,
	title = {Attention {Satisfies}: {A} {Constraint}-{Satisfaction} {Lens} on {Factual} {Errors} of {Language} {Models}},
	shorttitle = {Attention {Satisfies}},
	url = {https://openreview.net/forum?id=gfFVATffPd},
	abstract = {We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as constraint satisfaction problems and use this framework to investigate how the LLM interacts internally with factual constraints. We find a strong positive relationship between the LLM's attention to constraint tokens and the factual accuracy of generations. We curate a suite of 10 datasets containing over 40,000 prompts to study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification. The approach and findings take another step towards using the mechanistic understanding of LLMs to enhance their reliability.},
	language = {en},
	urldate = {2024-09-30},
	author = {Yuksekgonul, Mert and Chandrasekaran, Varun and Jones, Erik and Gunasekar, Suriya and Naik, Ranjita and Palangi, Hamid and Kamar, Ece and Nushi, Besmira},
	month = oct,
	year = {2023},
}

@inproceedings{golechha_challenges_2024,
	title = {Challenges in {Mechanistically} {Interpreting} {Model} {Representations}},
	url = {https://openreview.net/forum?id=wfemKUcgoB},
	abstract = {Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities important for safety and trust are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We formalize representations for features and behaviors, highlight their importance and evaluation, and perform an exploratory study of dishonesty representations in ‘Mistral-7B-Instruct-v1‘. We justify that studying representations is an important and under-studied field, and highlight several challenges that arise while attempting to do so through currently established methods in MI, showing their insufficiency and advocating work on new frameworks for the same.},
	language = {en},
	urldate = {2024-10-04},
	author = {Golechha, Satvik and Dao, James},
	month = jun,
	year = {2024},
}

@article{sharma_truth_2023,
	title = {The {Truth} is in {There}: {Improving} {Reasoning} in {Language} {Models} with {Layer}-{Selective} {Rank} {Reduction}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {The {Truth} is in {There}},
	url = {https://arxiv.org/abs/2312.13558},
	doi = {10.48550/ARXIV.2312.13558},
	abstract = {Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to significantly improve the performance of LLMs by selectively removing higher-order components of their weight matrices. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires no additional parameters or data. We show extensive experiments demonstrating the generality of this finding across language models and datasets, and provide in-depth analyses offering insights into both when LASER is effective and the mechanism by which it operates.},
	urldate = {2024-10-04},
	author = {Sharma, Pratyusha and Ash, Jordan T. and Misra, Dipendra},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@inproceedings{lad_remarkable_2024,
	title = {The {Remarkable} {Robustness} of {LLMs}: {Stages} of {Inference}?},
	shorttitle = {The {Remarkable} {Robustness} of {LLMs}},
	url = {https://openreview.net/forum?id=R5unwb9KPc},
	abstract = {We demonstrate and investigate the remarkable robustness of Large Language Models by deleting and swapping adjacent layers. We find that deleting and swapping interventions retain 72-95{\textbackslash}\% of the original model's prediction accuracy without fine-tuning, whereas models with more layers exhibit more robustness. Based on the results of the layer-wise intervention and further experiments, we hypothesize the existence of four universal stages of inference across eight different models: detokenization, feature engineering, prediction ensembling, and residual sharpening. The first stage integrates local information, lifting raw token representations into higher-level contextual representations. Next is the iterative refinement of task and entity-specific features. Then, the second half of the model begins with a phase transition, where hidden representations align more with the vocabulary space due to specialized model components. Finally, the last layer sharpens the following token distribution by eliminating obsolete features that add noise to the prediction.},
	language = {en},
	urldate = {2024-10-04},
	author = {Lad, Vedang and Gurnee, Wes and Tegmark, Max},
	month = jun,
	year = {2024},
}

@inproceedings{arditi_refusal_2024-1,
	title = {Refusal in {Language} {Models} {Is} {Mediated} by a {Single} {Direction}},
	url = {https://openreview.net/forum?id=EqF16oDVFf},
	abstract = {Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.},
	language = {en},
	urldate = {2024-10-04},
	author = {Arditi, Andy and Obeso, Oscar Balcells and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
	month = jun,
	year = {2024},
}

@article{chen_lower_2024,
	title = {Lower {Layer} {Matters}: {Alleviating} {Hallucination} via {Multi}-{Layer} {Fusion} {Contrastive} {Decoding} with {Truthfulness} {Refocused}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Lower {Layer} {Matters}},
	url = {https://arxiv.org/abs/2408.08769},
	doi = {10.48550/ARXIV.2408.08769},
	abstract = {Large Language Models (LLMs) have demonstrated exceptional performance across various natural language processing tasks, yet they occasionally tend to yield content that factually inaccurate or discordant with the expected output, a phenomenon empirically referred to as "hallucination". To tackle this issue, recent works have investigated contrastive decoding between the original model and an amateur model with induced hallucination, which has shown promising results. Nonetheless, this method may undermine the output distribution of the original LLM caused by its coarse contrast and simplistic subtraction operation, potentially leading to errors in certain cases. In this paper, we introduce a novel contrastive decoding framework termed LOL (LOwer Layer Matters). Our approach involves concatenating the contrastive decoding of both the final and lower layers between the original model and the amateur model, thereby achieving multi-layer fusion to aid in the mitigation of hallucination. Additionally, we incorporate a truthfulness refocused module that leverages contextual guidance to enhance factual encoding, further capturing truthfulness during contrastive decoding. Extensive experiments conducted on two publicly available datasets illustrate that our proposed LOL framework can substantially alleviate hallucination while surpassing existing baselines in most cases. Compared with the best baseline, we improve by average 4.5 points on all metrics of TruthfulQA. The source code is coming soon.},
	urldate = {2024-10-05},
	author = {Chen, Dingwei and Fang, Feiteng and Ni, Shiwen and Liang, Feng and Xu, Ruifeng and Yang, Min and Li, Chengming},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{noauthor_can_2024,
	title = {Can the {Training} {Loss} be {Predictive} for {Out}-of-{Distribution} {Generalization}?},
	url = {https://openreview.net/forum?id=pcIDLhnYL9},
	abstract = {Traditional model selection in deep learning relies on carefully tuning several hyper-parameters (HPs) controlling regularization strength on held-out validation data, which can be challenging to obtain in scarce-data scenarios or may not accurately reflect real-world deployment conditions due to distribution shifts. Motivated by such issues, this paper investigates the potential of using solely the training loss to predict the generalization performance of neural networks on out-of-distribution (OOD) test scenarios. Our analysis reveals that preserving consistent prediction variance across training and testing distributions is essential for establishing a correlation between training loss and OOD generalization. We propose architectural adjustments to ensure \${\textbackslash}textit\{variance preservation\}\$, enabling reliable model selection based on training loss alone, even in over-parameterized settings with a sample-to-parameter ratio exceeding four orders of magnitude. We extensively assess the model-selection capabilities of \${\textbackslash}textit\{variance-preserving\}\$ architectures on several scarce data, domain-shift, and corruption benchmarks by optimizing HPs such as learning rate, weight decay, batch size, and data augmentation strength.},
	language = {en},
	urldate = {2024-10-10},
	month = oct,
	year = {2024},
}

@inproceedings{noauthor_out--distribution_2024,
	title = {On the {Out}-of-{Distribution} {Generalization} of {Self}-{Supervised} {Learning}},
	url = {https://openreview.net/forum?id=22ywev7zMt},
	abstract = {In this paper, we focus on the out-of-distribution (OOD) generalization of self-supervised learning (SSL). By analyzing the mini-batch construction during SSL training phase, we first give one plausible explanation for SSL having OOD generalization. Then, from the perspective of data generation and causal inference, we analyze and conclude that SSL learns spurious correlations during the training process, which leads to a reduction in OOD generalization. To address this issue, we propose a post-intervention distribution (PID) grounded in the Structural Causal Model. PID offers a scenario where the relationships between variables are free from the influence of spurious correlations. Besides, we demonstrate that if each mini-batch during SSL training satisfies PID, the resulting SSL model can achieve optimal worst-case OOD performance. This motivates us to develop a batch sampling strategy that enforces PID constraints through the learning of a latent variable model. Through theoretical analysis, we demonstrate the identifiability of the latent variable model and validate the effectiveness of the proposed sampling strategy. Experiments conducted on various downstream OOD tasks demonstrate the effectiveness of the proposed sampling strategy.},
	language = {en},
	urldate = {2024-10-10},
	month = oct,
	year = {2024},
}

@inproceedings{noauthor_intermediate_2024,
	title = {Intermediate {Layer} {Classifiers} for {OOD} generalization},
	url = {https://openreview.net/forum?id=ByCV9xWfNK},
	abstract = {Deep classifiers are known to be sensitive to data distribution shifts, primarily due to their reliance on spurious correlations in training data. It has been suggested that these classifiers can still find useful features in the network's last layer that hold up under such shifts. In this work, we question the use of last-layer representations for out-of-distribution (OOD) generalisation and explore the utility of intermediate layers. To this end, we introduce Intermediate Layer Classifiers (ILCs). We discover that intermediate layer representations frequently offer substantially better generalisation than those from the penultimate layer. In many cases, zero-shot OOD generalisation using earlier-layer representations approaches the few-shot performance of retraining on penultimate layer representations. This is confirmed across multiple datasets, architectures, and types of distribution shifts. Our analysis suggests that intermediate layers are less sensitive to distribution shifts compared to the penultimate layer. These findings highlight the importance of understanding how information is distributed across network layers and its role in OOD generalisation, while also pointing to the limits of penultimate layer representation utility.},
	language = {en},
	urldate = {2024-10-10},
	month = oct,
	year = {2024},
}

@misc{ji_survey_2024,
	title = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
	url = {http://arxiv.org/abs/2202.03629},
	doi = {10.48550/arXiv.2202.03629},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation; and (3) hallucinations in large language models (LLMs). This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	urldate = {2024-10-16},
	publisher = {arXiv},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Chen, Delong and Dai, Wenliang and Chan, Ho Shu and Madotto, Andrea and Fung, Pascale},
	month = jul,
	year = {2024},
	note = {arXiv:2202.03629},
	keywords = {Computer Science - Computation and Language},
}

@misc{xu_hallucination_2024,
	title = {Hallucination is {Inevitable}: {An} {Innate} {Limitation} of {Large} {Language} {Models}},
	shorttitle = {Hallucination is {Inevitable}},
	url = {http://arxiv.org/abs/2401.11817},
	doi = {10.48550/arXiv.2401.11817},
	abstract = {Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.},
	urldate = {2024-10-16},
	publisher = {arXiv},
	author = {Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
	month = jan,
	year = {2024},
	note = {arXiv:2401.11817},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{huang_survey_2023,
	title = {A {Survey} on {Hallucination} in {Large} {Language} {Models}: {Principles}, {Taxonomy}, {Challenges}, and {Open} {Questions}},
	shorttitle = {A {Survey} on {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.05232},
	abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.},
	language = {en},
	urldate = {2024-08-06},
	publisher = {arXiv},
	author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05232 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhang_sirens_2023,
	title = {Siren's {Song} in the {AI} {Ocean}: {A} {Survey} on {Hallucination} in {Large} {Language} {Models}},
	shorttitle = {Siren's {Song} in the {AI} {Ocean}},
	url = {http://arxiv.org/abs/2309.01219},
	abstract = {While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.},
	language = {en},
	urldate = {2024-08-06},
	publisher = {arXiv},
	author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01219 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{ji_llm_2024,
	title = {{LLM} {Internal} {States} {Reveal} {Hallucination} {Risk} {Faced} {With} a {Query}},
	url = {http://arxiv.org/abs/2407.03282},
	doi = {10.48550/arXiv.2407.03282},
	abstract = {The hallucination problem of Large Language Models (LLMs) significantly limits their reliability and trustworthiness. Humans have a self-awareness process that allows us to recognize what we don't know when faced with queries. Inspired by this, our paper investigates whether LLMs can estimate their own hallucination risk before response generation. We analyze the internal mechanisms of LLMs broadly both in terms of training data sources and across 15 diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets. Our empirical analysis reveals two key insights: (1) LLM internal states indicate whether they have seen the query in training data or not; and (2) LLM internal states show they are likely to hallucinate or not regarding the query. Our study explores particular neurons, activation layers, and tokens that play a crucial role in the LLM perception of uncertainty and hallucination risk. By a probing estimator, we leverage LLM self-assessment, achieving an average hallucination estimation accuracy of 84.32{\textbackslash}\% at run time.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Ji, Ziwei and Chen, Delong and Ishii, Etsuko and Cahyawijaya, Samuel and Bang, Yejin and Wilie, Bryan and Fung, Pascale},
	month = jul,
	year = {2024},
	note = {arXiv:2407.03282 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{nonkes_leveraging_2024,
	title = {Leveraging {Graph} {Structures} to {Detect} {Hallucinations} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2407.04485},
	abstract = {Large language models are extensively applied across a wide range of tasks, such as customer support, content creation, educational tutoring, and providing financial guidance. However, a well-known drawback is their predisposition to generate hallucinations. This damages the trustworthiness of the information these models provide, impacting decision-making and user confidence. We propose a method to detect hallucinations by looking at the structure of the latent space and finding associations within hallucinated and non-hallucinated generations. We create a graph structure that connects generations that lie closely in the embedding space. Moreover, we employ a Graph Attention Network which utilizes message passing to aggregate information from neighboring nodes and assigns varying degrees of importance to each neighbor based on their relevance. Our findings show that 1) there exists a structure in the latent space that differentiates between hallucinated and non-hallucinated generations, 2) Graph Attention Networks can learn this structure and generalize it to unseen generations, and 3) the robustness of our method is enhanced when incorporating contrastive learning. When evaluated against evidence-based benchmarks, our model performs similarly without access to search-based methods.},
	language = {en},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Nonkes, Noa and Agaronian, Sergei and Kanoulas, Evangelos and Petcu, Roxana},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04485 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yang_regularizing_2024,
	title = {Regularizing {Hidden} {States} {Enables} {Learning} {Generalizable} {Reward} {Model} for {LLMs}},
	url = {http://arxiv.org/abs/2406.10216},
	doi = {10.48550/arXiv.2406.10216},
	abstract = {Reward models trained on human preference data have been proven to be effective for aligning Large Language Models (LLMs) with human intent within the reinforcement learning from human feedback (RLHF) framework. However, the generalization capabilities of current reward models to unseen prompts and responses are limited. This limitation can lead to an unexpected phenomenon known as reward over-optimization, where excessive optimization of rewards results in a decline in actual performance. While previous research has advocated for constraining policy optimization, our study proposes a novel approach to enhance the reward model's generalization ability against distribution shifts by regularizing the hidden states. Specifically, we retain the base model's language model head and incorporate a suite of text-generation losses to preserve the hidden states' text generation capabilities, while concurrently learning a reward head behind the same hidden states. Our experimental results demonstrate that the introduced regularization technique markedly improves the accuracy of learned reward models across a variety of out-of-distribution (OOD) tasks and effectively alleviate the over-optimization issue in RLHF, offering a more reliable and robust preference learning paradigm.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Yang, Rui and Ding, Ruomeng and Lin, Yong and Zhang, Huan and Zhang, Tong},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10216 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{ji_anah_2024,
	title = {{ANAH}: {Analytical} {Annotation} of {Hallucinations} in {Large} {Language} {Models}},
	shorttitle = {{ANAH}},
	url = {http://arxiv.org/abs/2405.20315},
	doi = {10.48550/arXiv.2405.20315},
	abstract = {Reducing the `\${\textbackslash}textit\{hallucination\}\$' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community. Thus, we present \${\textbackslash}textbf\{ANAH\}\$, a bilingual dataset that offers \${\textbackslash}textbf\{AN\}\$alytical \${\textbackslash}textbf\{A\}\$nnotation of \${\textbackslash}textbf\{H\}\$allucinations in LLMs within Generative Question Answering. Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of {\textasciitilde}12k sentence-level annotations for {\textasciitilde}4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions.},
	urldate = {2024-09-14},
	publisher = {arXiv},
	author = {Ji, Ziwei and Gu, Yuzhe and Zhang, Wenwei and Lyu, Chengqi and Lin, Dahua and Chen, Kai},
	month = may,
	year = {2024},
	note = {arXiv:2405.20315 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{rahman_defan_2024,
	title = {{DefAn}: {Definitive} {Answer} {Dataset} for {LLMs} {Hallucination} {Evaluation}},
	shorttitle = {{DefAn}},
	url = {http://arxiv.org/abs/2406.09155},
	doi = {10.48550/arXiv.2406.09155},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities, revolutionizing the integration of AI in daily life applications. However, they are prone to hallucinations, generating claims that contradict established facts, deviating from prompts, and producing inconsistent responses when the same prompt is presented multiple times. Addressing these issues is challenging due to the lack of comprehensive and easily assessable benchmark datasets. Most existing datasets are small and rely on multiple-choice questions, which are inadequate for evaluating the generative prowess of LLMs. To measure hallucination in LLMs, this paper introduces a comprehensive benchmark dataset comprising over 75,000 prompts across eight domains. These prompts are designed to elicit definitive, concise, and informative answers. The dataset is divided into two segments: one publicly available for testing and assessing LLM performance and a hidden segment for benchmarking various LLMs. In our experiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and Zephyr-revealing that overall factual hallucination ranges from 59\% to 82\% on the public dataset and 57\% to 76\% in the hidden benchmark. Prompt misalignment hallucination ranges from 6\% to 95\% in the public dataset and 17\% to 94\% in the hidden counterpart. Average consistency ranges from 21\% to 61\% and 22\% to 63\%, respectively. Domain-wise analysis shows that LLM performance significantly deteriorates when asked for specific numeric information while performing moderately with person, location, and date queries. Our dataset demonstrates its efficacy and serves as a comprehensive benchmark for LLM performance evaluation. Our dataset and LLMs responses are available at {\textbackslash}href\{https://github.com/ashikiut/DefAn\}\{https://github.com/ashikiut/DefAn\}.},
	urldate = {2024-09-14},
	publisher = {arXiv},
	author = {Rahman, A. B. M. Ashikur and Anwar, Saeed and Usman, Muhammad and Mian, Ajmal},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_halueval_2023,
	title = {{HaluEval}: {A} {Large}-{Scale} {Hallucination} {Evaluation} {Benchmark} for {Large} {Language} {Models}},
	shorttitle = {{HaluEval}},
	url = {http://arxiv.org/abs/2305.11747},
	doi = {10.48550/arXiv.2305.11747},
	abstract = {Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about \$19.5{\textbackslash}\%\$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.},
	urldate = {2024-09-14},
	publisher = {arXiv},
	author = {Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
	month = oct,
	year = {2023},
	note = {arXiv:2305.11747 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{gu_anah-v2_2024,
	title = {{ANAH}-v2: {Scaling} {Analytical} {Hallucination} {Annotation} of {Large} {Language} {Models}},
	shorttitle = {{ANAH}-v2},
	url = {http://arxiv.org/abs/2407.04693},
	doi = {10.48550/arXiv.2407.04693},
	abstract = {Large language models (LLMs) exhibit hallucinations in long-form question-answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators. To facilitate the scalable oversight of LLM hallucinations, this paper introduces an iterative self-training framework that simultaneously and progressively scales up the hallucination annotation dataset and improves the accuracy of the hallucination annotator. Based on the Expectation Maximization (EM) algorithm, in each iteration, the framework first applies a hallucination annotation pipeline to annotate a scaled dataset and then trains a more accurate hallucination annotator on the dataset. This new hallucination annotator is adopted in the hallucination annotation pipeline used for the next iteration. Extensive experimental results demonstrate that the finally obtained hallucination annotator with only 7B parameters surpasses the performance of GPT-4 and obtains new state-of-the-art hallucination detection results on HaluEval and HalluQA by zero-shot inference. Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference (NLI) metric increasing from 25\% to 37\% on HaluEval.},
	urldate = {2024-09-14},
	publisher = {arXiv},
	author = {Gu, Yuzhe and Ji, Ziwei and Zhang, Wenwei and Lyu, Chengqi and Lin, Dahua and Chen, Kai},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04693 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}
