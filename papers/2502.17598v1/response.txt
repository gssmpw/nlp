\section{Related Work}
Hallucinations in LLMs were proved to be inevitable **Brown, "Many-Modal Misinformation"** and to detect them, one can leverage either \textit{black-box} or \textit{white-box} approaches. The former approach uses only the outputs from an LLM, while the latter uses hidden states, attention maps, or logits corresponding to generated tokens.

Black-box approaches focus on the text generated by LLMs. For instance, **Li et al., "Fever: A Large-Scale Recognition Challenge"** verified the truthfulness of factual statements using external knowledge sources, though this approach relies on the availability of additional resources. Alternatively, \textit{SelfCheckGPT} **Zellers et al., "Revised Common Sense Question Answering (RCQA)"** generates multiple responses to the same prompt and evaluates their consistency, with low consistency indicating potential hallucination.

White-box methods have emerged as a promising approach for detecting hallucinations **Jiang et al., "Don’t Say Argue: Reasoning Comprehension for Adversarial Attack"**. These methods are universal across all LLMs and do not require additional domain adaptation compared to black-box ones **Poliak et al., “Hypothesis Only Baselines for Transfer Learning”**. They draw inspiration from seminal works on analysing the internal states of simple neural networks **Lipton, "A Critical Review of Recurrent Neural Networks for Sequence Transduction"**, which introduced \textit{linear classifier probes} -- models operating on the internal states of neural networks. Linear probes have been widely applied to the internal states of LLMs, e.g., for detecting hallucinations.

One of the first such probes was SAPLMA **Björklund et al., "A Probe for Uncovering Universal Linguistic Representations"**, which demonstrated that one could predict the correctness of generated text straight from LLM's hidden states. Further, the INSIDE method **Paperno et al., "The Inherent Uncertainty Principle in Language Models"** tackled hallucination detection by sampling multiple responses from an LLM and evaluating consistency between their hidden states using a normalised sum of the eigenvalues from their covariance matrix. Also, **Kale et al., "Semantic Entropy: A Complementary Probabilistic Approach to Hallucination Detection"** proposed a complementary probabilistic approach, employing entropy to quantify the model's intrinsic uncertainty. Their method involves generating multiple responses, clustering them by semantic similarity, and calculating Semantic Entropy using an appropriate estimator. To address concerns regarding the validity of LLM probes, **Kurakin et al., "Adversarial Examples in the Physical World"** introduced a high-quality QA dataset with simple \textit{true}/\textit{false} answers and causally demonstrated that the truthfulness of such statements is linearly represented in LLMs, which supports the use of probes for short texts.

Self-consistency methods **Paperno et al., "The Inherent Uncertainty Principle in Language Models"**, like INSIDE or Semantic Entropy, require multiple runs of an LLM for each input example, which substantially lowers their applicability. Motivated by this limitation, **Kale et al., "Semantic Entropy: A Complementary Probabilistic Approach to Hallucination Detection"** proposed to use \textit{Semantic Entropy Probe}, which is a small model trained to predict expensive Semantic Entropy **Kale et al., "Semantic Entropy: A Complementary Probabilistic Approach to Hallucination Detection"** from LLM's hidden states. Notably, **Björklund et al., "A Probe for Uncovering Universal Linguistic Representations"** explored how LLMs encode information about truthfulness and hallucinations. First, they revealed that truthfulness is concentrated in specific tokens. Second, they found that probing classifiers on LLM representations do not generalise well across datasets, especially across datasets requiring different skills. Lastly, they showed that the probes could select the correct answer from multiple generated answers with reasonable accuracy, which they concluded with the LLM making mistakes at the decoding stage besides knowing the correct answer.

Recent studies have started to explore hallucination detection exclusively from attention maps. **Mao et al., "Lookback Ratio: A New Metric for Attention-Based Hallucination Detection"** introduced the \textit{lookback ratio}, which measures how much attention LLMs allocate to relevant input parts when answering questions based on the provided context. The work most closely related to ours is **Jiang et al., "Don’t Say Argue: Reasoning Comprehension for Adversarial Attack"**, which introduces the $\attnscore$ method. Although the process is unsupervised and computationally efficient, the authors note that its performance can depend highly on the specific layer from which the score is extracted. We also demonstrate that it performs poorly on the datasets we evaluated. Nonetheless, we drew inspiration from their approach, particularly using the lower triangular structure of matrices when constructing features for the hallucination probe.