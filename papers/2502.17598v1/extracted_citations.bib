@misc{alain_understanding_2016,
	title = {Understanding intermediate layers using linear classifier probes},
	url = {https://arxiv.org/abs/1610.01644v4},
	abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
	language = {en},
	urldate = {2024-10-22},
	journal = {arXiv.org},
	author = {Alain, Guillaume and Bengio, Yoshua},
	month = oct,
	year = {2016},
}

@inproceedings{arteaga_hallucination_2024,
	title = {Hallucination {Detection} in {LLMs}: {Fast} and {Memory}-{Efficient} {Finetuned} {Models}},
	url = {https://openreview.net/forum?id=8T8QkDsuO9},
	booktitle = {Northern {Lights} {Deep} {Learning} {Conference} 2025},
	author = {Arteaga, Gabriel Y. and Schön, Thomas B. and Pielawski, Nicolas},
	year = {2024},
	keywords = {notion},
}

@inproceedings{azaria_internal_2023,
	address = {Singapore},
	title = {The {Internal} {State} of an {LLM} {Knows} {When} {It}`s {Lying}},
	url = {https://aclanthology.org/2023.findings-emnlp.68/},
	doi = {10.18653/v1/2023.findings-emnlp.68},
	abstract = {While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM`s internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\% to 83\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier`s performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.},
	urldate = {2025-02-13},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Azaria, Amos and Mitchell, Tom},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	keywords = {notion},
	pages = {967--976},
}

@inproceedings{chen_inside_2024,
	title = {{INSIDE}: {LLMs}' {Internal} {States} {Retain} the {Power} of {Hallucination} {Detection}},
	url = {https://openreview.net/forum?id=Zj12nzlQbz},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping},
	year = {2024},
	keywords = {notion},
}

@inproceedings{chuang_lookback_2024,
	address = {Miami, Florida, USA},
	title = {Lookback {Lens}: {Detecting} and {Mitigating} {Contextual} {Hallucinations} in {Large} {Language} {Models} {Using} {Only} {Attention} {Maps}},
	shorttitle = {Lookback {Lens}},
	url = {https://aclanthology.org/2024.emnlp-main.84/},
	doi = {10.18653/v1/2024.emnlp-main.84},
	abstract = {When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such **contextual hallucinations**. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these \_lookback ratio\_ features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector—**Lookback Lens**—is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6\% in the XSum summarization task.},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Chuang, Yung-Sung and Qiu, Linlu and Hsieh, Cheng-Yu and Krishna, Ranjay and Kim, Yoon and Glass, James R.},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	keywords = {notion},
	pages = {1419--1436},
}

@article{farquhar_detecting_2024,
	title = {Detecting hallucinations in large language models using semantic entropy},
	volume = {630},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07421-0},
	doi = {10.1038/s41586-024-07421-0},
	abstract = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
	language = {en},
	number = {8017},
	urldate = {2025-02-13},
	journal = {Nature},
	author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
	month = jun,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Information technology},
	pages = {625--630},
}

@misc{kossen_semantic_2024,
	title = {Semantic {Entropy} {Probes}: {Robust} and {Cheap} {Hallucination} {Detection} in {LLMs}},
	shorttitle = {Semantic {Entropy} {Probes}},
	url = {http://arxiv.org/abs/2406.15927},
	doi = {10.48550/arXiv.2406.15927},
	abstract = {We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Kossen, Jannik and Han, Jiatong and Razzak, Muhammed and Schut, Lisa and Malik, Shreshth and Gal, Yarin},
	month = jun,
	year = {2024},
	note = {arXiv:2406.15927 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{li_dawn_2024,
	address = {Bangkok, Thailand},
	title = {The {Dawn} {After} the {Dark}: {An} {Empirical} {Study} on {Factuality} {Hallucination} in {Large} {Language} {Models}},
	shorttitle = {The {Dawn} {After} the {Dark}},
	url = {https://aclanthology.org/2024.acl-long.586/},
	doi = {10.18653/v1/2024.acl-long.586},
	abstract = {In the era of large language models (LLMs), hallucination (the tendency to generate factually incorrect content) poses great challenges to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucinations, focused on the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and design a simple yet effective detection method for LLM hallucinations. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucinations. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs.},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Junyi and Chen, Jie and Ren, Ruiyang and Cheng, Xiaoxue and Zhao, Xin and Nie, Jian-Yun and Wen, Ji-Rong},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	keywords = {notion},
	pages = {10879--10899},
}

@article{liang_internal_2024,
	title = {Internal {Consistency} and {Self}-{Feedback} in {Large} {Language} {Models}: {A} {Survey}},
	volume = {abs/2407.14507},
	url = {https://doi.org/10.48550/arXiv.2407.14507},
	journal = {CoRR},
	author = {Liang, Xun and Song, Shichao and Zheng, Zifan and Wang, Hanyu and Yu, Qingchen and Li, Xunkai and Li, Rong-Hua and Xiong, Feiyu and Li, Zhiyu},
	year = {2024},
	keywords = {notion},
}

@inproceedings{manakul_selfcheckgpt_2023,
	address = {Singapore},
	title = {{SelfCheckGPT}: {Zero}-{Resource} {Black}-{Box} {Hallucination} {Detection} for {Generative} {Large} {Language} {Models}},
	shorttitle = {{SelfCheckGPT}},
	url = {https://aclanthology.org/2023.emnlp-main.557/},
	doi = {10.18653/v1/2023.emnlp-main.557},
	abstract = {Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose “SelfCheckGPT”, a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	keywords = {notion},
	pages = {9004--9017},
}

@inproceedings{marks_geometry_2024,
	title = {The {Geometry} of {Truth}: {Emergent} {Linear} {Structure} in {Large} {Language} {Model} {Representations} of {True}/{False} {Datasets}},
	url = {https://openreview.net/forum?id=aajyHYjjsk},
	booktitle = {First {Conference} on {Language} {Modeling}},
	author = {Marks, Samuel and Tegmark, Max},
	year = {2024},
	keywords = {notion},
}

@inproceedings{orgad_llms_2025,
	title = {{LLMs} {Know} {More} {Than} {They} {Show}: {On} the {Intrinsic} {Representation} of {LLM} {Hallucinations}},
	url = {https://openreview.net/forum?id=KRnsX5Em3W},
	booktitle = {The {Thirteenth} {International} {Conference} on {Learning} {Representations}},
	author = {Orgad, Hadas and Toker, Michael and Gekhman, Zorik and Reichart, Roi and Szpektor, Idan and Kotek, Hadas and Belinkov, Yonatan},
	year = {2025},
	keywords = {notion},
}

@inproceedings{sriramanan_llm-check_2024,
	title = {{LLM}-{Check}: {Investigating} {Detection} of {Hallucinations} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=LYx4w3CAgy},
	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Sriramanan, Gaurang and Bharti, Siddhant and Sadasivan, Vinu Sankar and Saha, Shoumik and Kattakinda, Priyatham and Feizi, Soheil},
	year = {2024},
	keywords = {notion},
}

@misc{xu_hallucination_2024,
	title = {Hallucination is {Inevitable}: {An} {Innate} {Limitation} of {Large} {Language} {Models}},
	shorttitle = {Hallucination is {Inevitable}},
	url = {http://arxiv.org/abs/2401.11817},
	doi = {10.48550/arXiv.2401.11817},
	abstract = {Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.},
	urldate = {2024-10-16},
	publisher = {arXiv},
	author = {Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
	month = jan,
	year = {2024},
	note = {arXiv:2401.11817},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

