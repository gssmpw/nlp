\section{Related Work}
\label{sec:related}\vspace{-2pt} 
\textbf{Stable Diffusion Generative Models.} Stable Diffusion~\cite{rombach2022high} has emerged as a major advancement in generative models, surpassing traditional models such as GANs~\cite{goodfellow2014generative,radford2015unsupervised,karras2019style} and VAEs~\cite{kingma2013auto,tolstikhin2017wasserstein} in terms of computational efficiency, scalability, and output quality. Stable Diffusion employs a denoising diffusion process within a low-dimensional latent space, enhancing robustness and significantly reducing memory and computational requirements compared to pixel-space methods ~\cite{ho2020denoising,song2020score} while maintaining high-quality image generation. Across different versions, Stable Diffusion has introduced key improvements in model architecture, training datasets, and noise-handling techniques, resulting in higher resolution outputs and improved alignment with textual prompts~\cite{rombach2022high}. These advancements have transformed creative applications, enabling breakthroughs in art generation, design prototyping, and visual storytelling~\cite{han2023design,wu2023not,wang2024diffusion}.

\textbf{Text-guided Image Editing Methods.} Text-guided image editing methods~\cite{li2020manigan,choi2023custom,wang2023imagen,ravi2023preditor} have seen significant progress with the advent of conditional diffusion models~\cite{nichol2021improved,dhariwal2021diffusion}, which enhance controllability and flexibility in generating or editing images. InstructPix2Pix~\cite{brooks2023instructpix2pix} is a conditional diffusion model designed explicitly for text-guided image editing tasks. It takes an input image and a natural language instruction as conditions, enabling precise edits to the image. By fine-tuning instruction-image pairs, it excels in tasks like style transfer and scene adjustments. Similarly, ControlNet~\cite{zhang2023adding,zhao2024uni,li2025controlnet} extends the framework of conditional diffusion models by incorporating structural guidance inputs such as edge maps, pose data or segmentation maps. ControlNet preserves spatial coherence and structural integrity while enabling high-fidelity edits based on textual and structural conditions. While these models represent state-of-the-art advancements in text-guided image editing, they also introduce challenges for origin attribution. Specifically, iterative use of these models can produce images with complex modification histories, complicating efforts to trace the source or the sequence of edits. Addressing these challenges is critical for ensuring content traceability and accountability.

\textbf{Generative Content Provenance Methods.} Current methods for tracing the origins of generated images can be broadly divided into two main approaches:
Embedding-based detection and latent space reverse-engineering. Embedding-based detection methods~\cite{tancik2020stegastamp,yu2021artificial,yu2020responsible,jeong2022fingerprintnet,sinitsa2024deep}, often referred to as fingerprinting or watermarking techniques, embed unique identifiers into models during training to trace the origin of model-generated images. These identifiers can later be detected in generated outputs to verify image provenance. While effective, they require training modifications, making them impractical for pre-trained or open-source models, and are vulnerable to adversarial removal or forgery~\cite{xu2020adversarial,sun2021detect}. Additionally, embedding increases complexity and frequently affects image quality.

Latent space reverse-engineering methods~\cite{creswell2018inverting,wang2023alteration,wang2024did,wang2024trace} trace the origin of generated images by % analyzing their latent representations and iteratively optimizing the latent vector
optimizing latent vectors to minimize reconstruction loss, thereby avoiding model modifications. However, these methods are limited to original model-generated images and % fail to handle the cumulative perturbations introduced by 
fail against cumulative perturbations from iterative text-guided editing models~\cite{brooks2023instructpix2pix,zhang2023adding,zhao2024uni,li2025controlnet}, limiting effectiveness in open environments. To address this gap, we propose \textsc{LambdaTracer}, a more robust latent space reverse-engineering approach that can both attribute origins and detect iterative modifications, substantially expanding the scope of provenance tracing in real-world scenarios.