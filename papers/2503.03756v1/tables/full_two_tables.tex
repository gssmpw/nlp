\begin{table*}[h]
%\caption{Wav2Vec 2.0 results and training time. We report the number of transformer layers trained (`Layers'), activation and valence CCC on MSP-Podcast ``Test1" split (`Act' and `Val'), wall-clock training time in hours for 5 epochs (`Time'), and number of trainable parameters ('Params'). We report results for single precision (SP) and mixed precision (MP) separately. * indicates statistically significant improvement, and $\dagger$ indicates statistically significant decrease from full 12-layer finetune. Significance for caching results are reported relative to their partial finetuning counterpart. We report `NS' when the difference between non-caching and its caching counterpart is nonsignificant. These experiments were conducted on NVIDIA A40 (non-caching) and RTX 2080 Ti (caching) GPUs.}
\caption{Results and training time for the number of transformer layers trained (`Layers'), activation (`Act') and valence (`Val') CCC, wall-clock training time in hours for 5 epochs (`Time'), and trainable parameters (`Params'), for single precision (SP) and mixed precision (MP). Non-caching results are compared to full finetuning SP (shaded). Caching results are compared to the non-caching equivalent. $\dagger$ indicates significant decrease. `NS' (not significant) shows that caching had no significant impact on performance. Experiments were conducted on NVIDIA A40 (non-caching) and RTX 2080 Ti (caching) GPUs.}
\label{tab:combined_results_table}
\centering
\begin{tabular}{l c c c c c||c c c c}
\hline
\multicolumn{6}{c||}{\textbf{Non-Caching Results}} & \multicolumn{4}{c}{\textbf{Caching Results}} \\

\textbf{Layers} & \textbf{Precision} & \textbf{Act} & \textbf{Val} & \textbf{Time (h)} & \textbf{Params} & \textbf{Act} & \textbf{Val} & \textbf{Time (h)} & \textbf{Params} \\

\hline
\multirow{1}{*}{LoRA} & SP & 0.623±0.02 & 0.506±0.01$\dagger$ & 2.476±0.02 & 300K & -- & -- & -- & -- \\
\hline
\multirow{2}{*}{1} & SP & 0.622±0.02 & 0.458±0.01$\dagger$ & 2.443±0.002 & \multirow{2}{*}{12M} & 
NS % 0.631±0.01 
& NS % 0.461±0.01 
& 0.353±0.002 & \multirow{2}{*}{7M}\\
& MP & 0.625±0.01 & 0.462±0.01$\dagger$ & 0.965±0.01 && 
NS % 0.631±0.01 
& NS % 0.461±0.01 
& 0.155±0.01 & \\
\hline

\multirow{2}{*}{2} & SP & 0.638±0.01 & 0.508±0.01$\dagger$ & 2.477±0.01 & \multirow{2}{*}{19M}& 
NS % 0.637±0.02 
& NS % 0.507±0.004 
& 0.689±0.002 & \multirow{2}{*}{14M} \\

& MP & 0.645±0.01 & 0.507±0.004$\dagger$ & 0.982±0.003 && 
NS % 0.634±0.01
& NS % 0.508±0.01 
& 0.253±0.002 & \\
\hline

\multirow{2}{*}{\textbf{3}} & SP & 0.648±0.01 & 0.565±0.002 & 2.519±0.02 & \multirow{2}{*}{26M} & 
NS % 0.644±0.01 
& 0.558±0.003$\dagger$ & 1.023±0.002 & \multirow{2}{*}{21M}\\
& \textbf{MP} & \textbf{0.655±0.01} & \textbf{0.568±0.01} & 0.991±0.003 && 0.639±0.01$\dagger$ & 
NS % 0.555±0.01 
& 0.353±0.002 & 
\\
\hline

\multirow{2}{*}{12} & \cellcolor{gray!20}SP & \cellcolor{gray!20}0.637±0.01 & \cellcolor{gray!20}0.567±0.02 & \cellcolor{gray!20}2.980±0.01 & \multirow{2}{*}{90M} & \multirow{2}{*}{--} & \multirow{2}{*}{--} & \multirow{2}{*}{--} & \multirow{2}{*}{--}\\
 & MP & 0.641±0.01 & 0.566±0.01 & 1.145±0.01 & & \\
%& & & & & \\
\hline
\end{tabular}
\end{table*}

