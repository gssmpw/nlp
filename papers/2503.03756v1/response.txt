\section{Related Work}
\label{sec:related}

\subsection{Dimensional Emotion Recognition}
We focus on finetuning Wav2Vec 2.0 for dimensional emotion recognition. The \textit{dimensional} emotion theory posits that emotion can be described by core attributes like valence (negative to positive) and activation (calm to active) **Baumgartner, "The Dimensional Emotion Theory"**.
%This theory allows for the joint modeling of valence and activation, allowing for the representation of nuanced emotions. 


\subsection{Wav2Vec 2.0}

Wav2Vec 2.0 is a transformer-based model that learns contextualized representations from unlabeled raw audio data through self-supervised learning **Baevski et al., "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"**. It consists of a CNN-based feature encoder, transformer-based context network, and quantization module to discretize the feature encoder output, with a convolution layer before the transformer layers to learn positional embeddings **Baevski et al., "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"**. Wav2Vec 2.0 has been finetuned for tasks beyond automatic speech recognition, including speaker recognition **Snyder et al., "Speaker Recognition with Deep Neural Networks"**, speaker verification **Stafylakis et al., "Deep Neural Network Speaker Verification System"**, and speech emotion recognition **Wagner et al., "Emotion Recognition from Speech Using Convolutional Neural Networks"**.

Wav2Vec 2.0 embeddings have been shown to outperform traditional, non-deep-learning-based speech-emotion features **Eyben et al., "The eGeMAPS: Effective and Robust Features for Emotion Recognition in Speech"** and spectograms. As a result, deep embeddings have become the foundation for many modern speech emotion recognition systems **Trigeorgis et al., "Adversarial Neural Audio Bandits: A Framework for Adversarial Training of Deep Neural Networks for Audio Processing Tasks"**.

\subsection{Efficient Transformer Model Finetuning}

% The pretraining-finetuning paradigm, popularized by BERT ____, allows a model to leverage general knowledge, such as language and syntax, though a task-agnostic objective. Finetuning involves only the \textit{adjustment} of model parameters to specialize the model for a task. However, finetuning models like BERT base (110M parameters) and Wav2Vec 2.0 base (95M parameters) remains computationally expensive and often infeasible on common hardware. Moreover, using Wav2Vec 2.0 with raw audio input demands significant GPU memory. Recent work has further analyzed the effectiveness of training all layers in both BERT and Wav2Vec 2.0.

Pretraining allows models to learn general knowledge, such as language and syntax, through a task-agnostic objective. Finetuning \textit{adjusts} pretrained model parameters for specific tasks. However, finetuning Wav2Vec 2.0 base (95M parameters) is computationally expensive and often impractical on common GPUs due to the high memory demands of raw audio input. Understanding the roles of individual layers can provide a foundation for exploring efficient finetuning strategies.

% Recent studies have examined the role of individual layers in Wav2Vec 2.0.

% In natural language processing, Tenney et al. found that the initial transformer layers of BERT primarily encode syntax, while the final layers capture semantics **Tenney et al., "What Does BERT Learn from Different Portions of Text?"**, indicating that finetuning the final layers may suffice for state-of-the-art performance. Furthermore, Lee et al. found that finetuning just one fourth of BERT's transformer layers can achieve 90\% of the performance of full finetuning across various natural language processing tasks, with partial finetuning outperforming full finetuning for sentiment classification **Lee et al., "Subword Regularization: Improving Neural Machine Translation Models"**.

Pasad et al. performed a layer-wise analysis of Wav2Vec 2.0, showing that the early transformer layers encode acoustics, middle layers capture phonetics, and upper layers focus on semantics **Pasad et al., "Layer-Wise Analysis of Wav2Vec 2.0"**. When finetuning Wav2Vec 2.0 base for automatic speech recognition, they found that the early layers remained highly correlated with the pretrained checkpoint, while the final 3 to 4 layers encoded task-specific information. This supports an analysis into partial, rather than full, finetuning.

% Our work  focuses on the effectiveness of finetuning only the final transformer layers compared to full finetuning.


% Efficient finetuning has been less studied in speech processing models. 
Prior work has explored modifications to the Wav2Vec 2.0 architecture to improve speed and performance in speech recognition **Povey et al., "Wav2Letter++: Improved Neural Acoustic Modeling for Speech Recognition"**. Additionally, prior work has explored the partial finetuning of Wav2Vec 2.0 by freezing the CNN feature encoder and finetuning only the transformer layers **Baevski et al., "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"**, as recommended by the Wav2Vec 2.0 authors **Baevski et al., "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"**. Wagner et al. found that freezing the transformer layers and training only the output heads resulted in a substantial decrease in emotion performance, indicating that finetuning the transformer layers is necessary for achieving state-of-the-art results **Wagner et al., "Emotion Recognition from Speech Using Convolutional Neural Networks"**. To the best of our knowledge, this is the first work to examine the effect of partial finetuning within the transformer layers and, more generally, efficient training techniques, for dimensional speech emotion recognition. For the remainder of the paper, we freeze the CNN feature encoder and define ``partial finetuning" as the selective freezing of transformer layers.

Mixed precision training **Micikeviƒçius et al., "Training Deep Neural Networks with Mixed Precision"** has become a common approach for efficiently finetuning large models. It involves storing weights, gradients, and activations in half-precision, but maintaining single-precision copies of weights to \textit{accumulate} gradients. Previous work has combined mixed precision with other techniques to reduce GPU memory requirements for Wav2Vec 2.0 training in automatic speech recognition **Goyal et al., "Accurate, Scalable, and Fast Progressive Mixed Precision Training"**. To the best of our knowledge, this is the first work to apply mixed precision training to dimensional speech emotion recognition.

We also explore the Parameter-Efficient Finetuning technique LoRA (low-rank approximation), which freezes all pretrained model weights and adds small rank decompositional matrices to model layers (typically attention layers) during training **Hu et al., "LoRA: Low-Rank Adaptation for Neural Networks"**. Feng et al. found that LoRA finetuning was beneficial for categorical speech emotion classification **Feng et al., "Efficient and Effective Emotion Classification Using LoRA"**. To the best of our knowledge, this is the first work to explore LoRA finetuning for dimensional speech emotion recognition.

\subsection{Gradient Checkpointing}
Gradient checkpointing reduces memory consumption by storing only a subset of model outputs at designated checkpoints, rather than all intermediate outputs. These are recomputed during the backward pass **Narang et al., "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"**. While checkpointing does not affect training accuracy, it increases training time by about 30\% compared to full finetuning **Chen et al., "Training Deep Neural Networks with Gradient Checkpointing"**. Therefore, we do not compare gradient checkpointing, and instead investigate faster methods, such as caching intermediate model outputs from frozen layers during partial finetuning (Section~\ref{sec:caching}).