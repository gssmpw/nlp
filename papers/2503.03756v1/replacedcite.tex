\section{Related Work}
\label{sec:related}

\subsection{Dimensional Emotion Recognition}
We focus on finetuning Wav2Vec 2.0 for dimensional emotion recognition. The \textit{dimensional} emotion theory posits that emotion can be described by core attributes like valence (negative to positive) and activation (calm to active) ____. 
%This theory allows for the joint modeling of valence and activation, allowing for the representation of nuanced emotions. 


\subsection{Wav2Vec 2.0}

Wav2Vec 2.0 is a transformer-based model that learns contextualized representations from unlabeled raw audio data through self-supervised learning ____. It consists of a CNN-based feature encoder, transformer-based context network, and quantization module to discretize the feature encoder output, with a convolution layer before the transformer layers to learn positional embeddings ____. Wav2Vec 2.0 has been finetuned for tasks beyond automatic speech recognition, including speaker recognition ____, speaker verification ____, and speech emotion recognition ____.

Wav2Vec 2.0 embeddings have been shown to outperform traditional, non-deep-learning-based speech-emotion features ____, such as eGeMAPS ____ and spectograms. As a result, deep embeddings have become the foundation for many modern speech emotion recognition systems ____.

\subsection{Efficient Transformer Model Finetuning}

% The pretraining-finetuning paradigm, popularized by BERT ____, allows a model to leverage general knowledge, such as language and syntax, though a task-agnostic objective. Finetuning involves only the \textit{adjustment} of model parameters to specialize the model for a task. However, finetuning models like BERT base (110M parameters) and Wav2Vec 2.0 base (95M parameters) remains computationally expensive and often infeasible on common hardware. Moreover, using Wav2Vec 2.0 with raw audio input demands significant GPU memory. Recent work has further analyzed the effectiveness of training all layers in both BERT and Wav2Vec 2.0.

Pretraining allows models to learn general knowledge, such as language and syntax, through a task-agnostic objective. Finetuning \textit{adjusts} pretrained model parameters for specific tasks. However, finetuning Wav2Vec 2.0 base (95M parameters) is computationally expensive and often impractical on common GPUs due to the high memory demands of raw audio input. Understanding the roles of individual layers can provide a foundation for exploring efficient finetuning strategies.

% Recent studies have examined the role of individual layers in Wav2Vec 2.0.

% In natural language processing, Tenney et al. found that the initial transformer layers of BERT primarily encode syntax, while the final layers capture semantics ____, indicating that finetuning the final layers may suffice for state-of-the-art performance. Furthermore, Lee et al. found that finetuning just one fourth of BERT's transformer layers can achieve 90\% of the performance of full finetuning across various natural language processing tasks, with partial finetuning outperforming full finetuning for sentiment classification ____.

Pasad et al. performed a layer-wise analysis of Wav2Vec 2.0, showing that the early transformer layers encode acoustics, middle layers capture phonetics, and upper layers focus on semantics ____. When finetuning Wav2Vec 2.0 base for automatic speech recognition, they found that the early layers remained highly correlated with the pretrained checkpoint, while the final 3 to 4 layers encoded task-specific information. This supports an analysis into partial, rather than full, finetuning.

% Our work  focuses on the effectiveness of finetuning only the final transformer layers compared to full finetuning.


% Efficient finetuning has been less studied in speech processing models. 
Prior work has explored modifications to the Wav2Vec 2.0 architecture to improve speed and performance in speech recognition ____. Additionally, prior work has explored the partial finetuning of Wav2Vec 2.0 by freezing the CNN feature encoder and finetuning only the transformer layers ____, as recommended by the Wav2Vec 2.0 authors ____. Wagner et al. found that freezing the transformer layers and training only the output heads resulted in a substantial decrease in emotion performance, indicating that finetuning the transformer layers is necessary for achieving state-of-the-art results ____. To the best of our knowledge, this is the first work to examine the effect of partial finetuning within the transformer layers and, more generally, efficient training techniques, for dimensional speech emotion recognition. For the remainder of the paper, we freeze the CNN feature encoder and define ``partial finetuning" as the selective freezing of transformer layers.

Mixed precision training ____ has become a common approach for efficiently finetuning large models. It involves storing weights, gradients, and activations in half-precision, but maintaining single-precision copies of weights to \textit{accumulate} gradients. Previous work has combined mixed precision with other techniques to reduce GPU memory requirements for Wav2Vec 2.0 training in automatic speech recognition ____. To the best of our knowledge, this is the first work to apply mixed precision training to dimensional speech emotion recognition.

We also explore the Parameter-Efficient Finetuning technique LoRA (low-rank approximation), which freezes all pretrained model weights and adds small rank decompositional matrices to model layers (typically attention layers) during training ____. Feng et al. found that LoRA finetuning was beneficial for categorical speech emotion classification ____. To the best of our knowledge, this is the first work to explore LoRA finetuning for dimensional speech emotion recognition.

\subsection{Gradient Checkpointing}
Gradient checkpointing reduces memory consumption by storing only a subset of model outputs at designated checkpoints, rather than all intermediate outputs. These are recomputed during the backward pass ____. While checkpointing does not affect training accuracy, it increases training time by about 30\% compared to full finetuning ____. Therefore, we do not compare gradient checkpointing, and instead investigate faster methods, such as caching intermediate model outputs from frozen layers during partial finetuning (Section~\ref{sec:caching}).


% Gradient checkpointing decreases memory usage by increasing computation time. It only outputs at designated checkpoints that are saved during the model's forward pass, discarding values in between checkpoints.  During the backward pass, these are recomputed ____. We do not compare gradient checkpointing as it should not significantly change model outputs and is slower than full finetuning.  Instead, we investigate faster methods, like caching intermediate model outputs from frozen layers when performing partial finetuning (Section~\ref{sec:caching}).