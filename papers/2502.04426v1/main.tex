%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads
\newcommand\el[1]{\textcolor{orange}{\textbf{#1}}}
\newcommand\jn[2]{\textcolor{violet}{\textbf{#1}}}

\usepackage[most]{tcolorbox}

\begin{document}

\title[Decoding AI Judgment: How LLMs Assess News Credibility and Bias]{Decoding AI Judgment: How LLMs Assess News Credibility and Bias}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Edoardo} \sur{Loru}}%\email{edoardo.loru@uniroma1.it}

\author[2]{\fnm{Jacopo} \sur{Nudo}}

\author[2]{\fnm{Niccolò} \sur{Di Marco}}

\author[2]{\fnm{Matteo} \sur{Cinelli}}

\author*[2]{\fnm{Walter} \sur{Quattrociocchi}}
\email{walter.quattrociocchi@uniroma1.it}



\affil[1]{\orgdiv{Department of Computer, Control and Management Engineering}, \orgname{Sapienza University of Rome}, \orgaddress{\street{Viale Ariosto 25}, \city{Rome}, \postcode{00185}}}

\affil[2]{\orgdiv{Department of Computer Science}, \orgname{Sapienza University of Rome}, \orgaddress{\street{Viale Regina Elena 295}, \city{Rome}, \postcode{00161}}}


%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Large Language Models (LLMs) are increasingly used to assess news credibility, yet little is known about how they make these judgments. While prior research has examined political bias in LLM outputs or their potential for automated fact-checking, their internal evaluation processes remain largely unexamined. Understanding how LLMs assess credibility provides insights into AI behavior and how credibility is structured and applied in large-scale language models.

This study benchmarks the reliability and political classifications of state-of-the-art LLMs—Gemini 1.5 Flash (Google), GPT-4o mini (OpenAI), and LLaMA 3.1 (Meta)—against structured, expert-driven rating systems such as NewsGuard and Media Bias Fact Check. Beyond assessing classification performance, we analyze the linguistic markers that shape LLM decisions, identifying which words and concepts drive their evaluations. We uncover patterns in how LLMs associate credibility with specific linguistic features by examining keyword frequency, contextual determinants, and rank distributions.

Beyond static classification, we introduce a framework in which LLMs refine their credibility assessments by retrieving external information, querying other models, and adapting their responses. This allows us to investigate whether their assessments reflect structured reasoning or rely primarily on prior learned associations.

% By comparing LLM output with structured human evaluations, this study explores how LLMs encode, interpret, and apply credibility criteria, revealing the key linguistic and contextual characteristics that shape their decisions. Rather than assessing LLM accuracy alone, we investigate what their language-based evaluation processes reveal about the structured credibility assessment. 
}
%%================================%%l
%% Sample for structured abstract %%
%%================================%%

% \keywords{LLM, Reliability Assessment, Evaluation Task}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1
}
% misinformation and polarization
In a digital environment where information is constantly produced and consumed \cite{holton2015reciprocity,khan2017social} and users interact and discuss \cite{avalle2024persistent,kubin2021role},  assessing the credibility of sources is a key challenge \cite{budak2024misunderstanding,lazer2018science,del2016spreading}. The way reliability is determined influences public trust \cite{gallup2018indicators,newman2018digital}, shapes social and political discussions \cite{bail2018exposure,kubin2021role,falkenberg2022growing}, and affects decision-making in critical areas like public health \cite{cinelli2020covid,kim2019incorporating}. While human evaluators rely on structured criteria to assess credibility \cite{metzger2015psychological,rieh2010credibility,metzger2010social}, the rise of Large Language Models raises new questions about how these systems process, interpret, and replicate such judgments.

News rating agencies like NewsGuard and Media Bias Fact Check (MBFC) provide structured, expert-driven assessments of news reliability. These assessments are based on rigorous evaluation criteria, such as factual accuracy, transparency, and editorial independence, and are developed through years of systematic work by human evaluators \cite{newsguard2024}. These benchmarks serve as operational gold standards in media assessment, widely used by researchers, platforms, and policymakers \cite{luhring2025best}. However, their reliance on human expertise makes them costly and time-consuming \cite{herrero2022emerging,aslett2022news}. 

On the other hand, LLMs, including GPT-4o (OpenAI) \cite{openai2024}, Gemini 1.5 Flash (Google) \cite{team2024gemini}, and LLaMA 3.1 (Meta) \cite{meta2024}, have demonstrated advanced capabilities in tasks such as text classification \cite{tornberg2024large,gilardi2023chatgpt,wu2023large,chiang2023can}, sentiment analysis \cite{krugmann2024sentiment}, and fact-checking \cite{yang2023large,hoes2023leveraging,quelle2024perils,hernandes2024llms}.
Moreover, recent research has increasingly focused on how human heuristics and biases manifest in artificial intelligence models \cite{hu2024generative,yax2024studying,motoki2024assessing}. Beyond surface-level bias detection, studies are also investigating whether LLMs encode psychological traits and value orientations, shedding light on the broader implications of their training data and decision-making processes \cite{strachan2024testing,coppolillo2025unmasking,safdari2023personality}.

This raises a fundamental question: to what extent do LLMs replicate, diverge from, or even reveal new dimensions of these structured human evaluations?
Indeed, little is known about how these models internally process information and build their evaluations. To what extent do LLMs reflect human-driven evaluations' biases, priorities, and heuristics? How do their decision-making processes differ from or align with those of human experts?   

This study examines how Large Language Models (LLMs) make decisions when evaluating the reliability and political orientation of a sample of 2,302 news outlets. Instead of merely assessing their alignment with expert evaluations, we focus on how these models build them. We address the underlying patterns shaping their reasoning by analyzing the linguistic markers, heuristics, and contextual cues that factor into their classifications. Through a systematic comparison with structured human evaluations (i.e., NewsGuard and MBFC), we explore whether LLMs rely on similar principles or develop distinct strategies for credibility assessment.

We also investigate how LLMs behave within an agentic workflow in which models can refine their assessments by retrieving additional information, querying external sources, or interacting with other AI systems. This approach allows us to examine whether LLMs can self-correct, reinforce biases, or adapt their reasoning when faced with new inputs. By integrating these dynamics, we move beyond mere classification and lay the groundwork for a structured human-AI comparison to provide deeper insights into the cognitive mechanisms underlying credibility judgments.

\section{Results and Discussion}\label{sec2}
We investigate how three state-of-the-art LLMs—Gemini 1.5 Flash, GPT-4o mini, and LLaMA 3.1 405B—encode and apply credibility assessments by comparing their outputs to expert human benchmarks from NewsGuard and MBFC. Rather than merely measuring classification accuracy, we aim to address the underlying processes guiding their evaluations. To ensure a diverse and representative dataset, we select 7,715 English-speaking news domains, evenly split between those labeled as Reliable and Unreliable by NewsGuard. These sources span multiple countries and include outlets with national or international focus.
We retrieve a snapshot of each domain's homepage, filtering out nonessential elements (e.g., scripts, styling) to isolate relevant textual components, such as news headlines and descriptions. This pre-processing step ensures that all LLMs are evaluated based on the same contextual information a human assessor might use. The final dataset consists of 2,302 active domains with sufficient content for classification. By analyzing not only the final classification labels assigned by the LLMs but also the process behind their assessments, we aim to provide deeper insights into how these models encode the notion of reliability. A detailed breakdown of the data collection and processing is provided in Methods.

We begin our assessment by querying each model using a zero-shot, closed-book approach, meaning no prior examples or explicit definitions of reliability are provided. This ensures that the models, without further context, solely rely on their internalized knowledge and learned heuristics to classify news outlets. By doing so, we aim to investigate these models' interpretative framework and assess how their reliability assessments mirror or diverge from structured human evaluations.
To this end, beyond a simple binary classification (Reliable or Unreliable), we prompt the models to assign a political orientation label to each news outlet and to justify their assessment by generating explanatory keywords. This additional layer of analysis allows us to explore how LLMs construct reliability assessments, whether their justifications align with human evaluators, and whether emerging discrepancies can be observed in their decision-making. Further, as detailed in Methods, we use the same prompt for all three LLMs to allow for a direct comparison between models.
Finally, we introduce an agentic framework where LLMs refine their credibility assessments by retrieving external information, interacting with other models, and adapting their responses. This approach allows us to examine whether LLMs apply structured reasoning beyond their internalized priors and sets up the conditions for a direct comparison between human and AI-driven evaluation strategies.

\subsection{LLMs vs. Expert-Driven Assessments}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/fig1_conf_matrix.pdf}
    \caption{\textbf{LLMs' classification against expert human evaluators.} (A) Each panel compares how domains rated as ``Reliable'' or ``Unreliable'' by NewsGuard are classified by each LLM (Gemini 1.5 Flash, GPT-4o mini, Llama 3.1 405B). All three models accurately identify ``Unreliable'' sources, with agreement ranging from $85$ to $97\%$ across models. However, ``Reliable'' domains show greater classification variability, particularly in GPT-4o mini, which classifies a significant portion (33\%) as "Unreliable".
    (B) Each panel shows how MBFC’s ``Credibility'' ratings (High, Medium, Low) align with LLM classifications. The models strongly agree on both high- and low-credibility domains, classifying them correctly over 90\% of the time. However, ``Medium'' credibility sources exhibit greater inconsistency across models, with GPT-4o mini and Llama 3.1 tending to classify them as "Unreliable" (75\% and 66\%, respectively), while Gemini remains more balanced (52\%-48\%). This further suggests that LLMs are particularly sensitive to sources with lower credibility signals but struggle with intermediate cases.
    }
    \label{fig:conf_matrix}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figures/fig2_errors_per_orientation.pdf}
    \caption{\textbf{LLMs' reliability rating misclassification across political orientation.} We randomly sample 40 domains from each pairing of NewsGuard's political orientation and reliability rating, and estimate the average frequency over 10,000 resamples of reliability misclassification for each. The error bars report the first and third quartile of the resulting frequencies per group.
    Compared with NewsGuard, LLMs appear to overestimate or underestimate the reliability of news outlets based on their political orientation. In particular, Right-leaning news outlets tend to be consistently misclassified by the LLMs as unreliable, whereas the Center and Left-leaning as reliable.
    }
    \label{fig:model_errors_per_orientation}
\end{figure}

In Fig. \ref{fig:conf_matrix}A, we illustrate how the classifications of each model compare with the reliability ratings assigned by NewsGuard. It is important to note that NewsGuard's ratings are not arbitrary judgments but the result of a structured, operationalized evaluation framework, developed through rigorous, systematic assessments of news outlets. At the same time, LLMs operate without explicit knowledge of these guidelines, meaning their decisions emerge from their internal processes, rather than from strict adherence to predefined criteria.

All three models accurately identify ``Unreliable'' sources, consistently flagging domains that NewsGuard marks for lack of credibility or transparency. Conversely, classifying ``Reliable'' sources appears to be more challenging for all three LLMs, with GPT-4o mini in particular showing a higher misclassification rate (33\%) than the rest. This discrepancy may reflect that NewsGuard’s methodology incorporates multiple dimensions of evaluation, such as editorial standards, correction policies, and transparency, which may not be directly inferable from the homepage's content alone.

To further assess how the models' ratings match against expert human evaluators, Fig. \ref{fig:conf_matrix}B shows the alignment with the "Credibility" ratings from Media Bias Fact Check (MBFC), a well-established service that categorizes sources using a formalized three-tier framework based on factual accuracy, bias, and traffic/longevity \cite{mbfc_methodology}. Among the 977 domains overlapping with MBFC's dataset, LLMs exhibit strong agreement for sources with a ``Low'' or ``High'' credibility rating, classifying over 90\% of them as ``Unreliable'' and ``Reliable'', respectively. However, for ``Medium'' credibility sources, the models show differences both compared to MBFC and among themselves: GPT-4o mini and LLaMA 3.1 classify most of these sources as ``Unreliable'' (75\% and 66\% of them, respectively), whereas Gemini 1.5 Flash remains more balanced. This suggests that LLMs may rely on clear-cut textual cues associated with highly credible or noncredible sources.

Although these models do not have explicit access to the rating process of NewsGuard and MBFC, nor are they provided their methodological criteria in the prompt, their responses suggest that they possess distinct but systematic heuristics that generally approximate human-defined credibility standards.

In light of this, we now investigate the cases of models' ratings disagreeing with human evaluators. Notably, we analyze whether these classification errors are distributed evenly across the political orientation labels assigned by NewsGuard or only characterize some. To this end, we consider a random sample of domains for each of NewsGuard's orientation and reliability labels and calculate the percentage of the domains whose reliability rating is misclassified by the LLM. In particular, we focus on a random sample of 40 domains per NewsGuard's political orientation and rating, as it is the least populated among these groups. Then, we repeat this sampling procedure 10,000 times to obtain average misclassification frequencies. 

The results in Fig. \ref{fig:model_errors_per_orientation} show that classification errors are not uniformly distributed across the political spectrum. In particular, focusing on domains rated as ``reliable'' by NewsGuard, we observe that Right-leaning domains are classified by all models as ``unreliable'' substantially more often than the Center and Left-leaning, whose reliability appears to be overestimated with respect to NewsGuard.

Finally, beyond assessing the models' performance in reliability classification, we measure how the political orientation labels they assign to news outlets compare against human evaluators. All three LLMs show strong agreement with human annotations, as seen in Supplementary Fig. \ref{suppfig:conf_matrix_orientation}. Comparing the political labels assigned by the models to those assigned by NewsGuard, we find a substantial overlap across the political spectrum for all three models. However, some differences can be observed due to NewsGuard employing fewer orientation labels than the models. This alignment is further confirmed by comparing the models' political orientation assessments with the ``Bias Rating'' from MBFC, focusing specifically on strictly political labels.

\subsection{Explaining Reliability Ratings with Keywords}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/fig3_keywords_rank_frequency.pdf}
    \caption{\textbf{Rank-frequency distributions of keywords used by each LLM to describe domains}. Each panel presents the most frequently used classification and determinant keywords for Reliable and Unreliable domains. Only the five most common keywords per panel are labeled to enhance readability.
    The color gradient represents the inferred political orientation of each keyword, ranging from Left-leaning to Right-leaning, based on the political leaning of the domains they are most frequently associated with. Right-leaning keywords appear almost exclusively in descriptions of Unreliable domains, whereas politically neutral or Left-leaning keywords are more characteristic of Reliable domains.
    All distributions exhibit heavy-tailed behavior, as indicated by their roughly linear shape on a log-log scale, where a small set of highly frequent keywords dominate the descriptions, while the majority appear less frequently. This indicates that LLMs produce consistent markers when explaining their reliability evaluations.} 
    \label{fig:keywords_rank_frequency}
\end{figure}

We now investigate the main factors driving LLMs' reliability ratings and how they relate to the content of a news outlet's homepage. To achieve this, we analyze three distinct sets of keywords generated by the models for each outlet, alongside their reliability ratings and political orientation. By examining what keywords are used and how they relate to reliability and political orientation, we aim to gain further insights into the mechanisms these models employ to reach their reliability evaluation. Unlike human evaluators, LLMs do not explicitly follow predefined scoring guidelines, so exploring the patterns they exhibit when assigning reliability ratings is essential. 

For all domains, each LLM is tasked to provide three types of keywords. The first set of keywords, referred to as ``classification keywords'', reflects the model's rationale behind its classification and summarizes its rating. The second set, ``determinant keywords'', comprises terms extracted directly from the domain's homepage that were critical for the model's reliability judgment. The final set, ``summary keywords'', includes terms that broadly summarize the contents of the domain's homepage. Before analysis, we convert all keywords to lowercase. Importantly, we do not give the models any constraints on the number of keywords to output. By omitting this constraint, we can observe the typical number of keywords each model associates with a given input and examine whether this number varies between ``reliable'' and ``unreliable'' sources or across different models. Furthermore, imposing such a limit may hinder the explainability of each model's reliability ratings by reducing their expressive power. 

Using the political orientation label assigned by the models to each domain, we infer the political leaning of each keyword as the average political leaning of the domains it is associated with. To achieve this, we transform the political orientation labels into numerical values ranging from $-1$ to $1$, assigning $-1$ to Left, $-0.5$ to Center-Left, $0$ to Center, $0.5$ to Center-Right, and $1$ to Right.

We construct separate rank-frequency distributions for each model, keyword type, and reliability rating to analyze the models' keyword usage. A rank-frequency distribution calculates how often an element appears in a sample relative to its rank, where elements are ordered from most to least frequent. These distributions frequently exhibit a heavy-tailed behavior, characterized by a few elements dominating in frequency and the majority appearing rarely. This pattern, commonly observed in natural language studies, reflects the typical number of occurrences of words in a corpus of documents, where a few high-frequency terms account for the bulk of occurrences, and many others are used infrequently.

Fig. \ref{fig:keywords_rank_frequency} displays the rank-frequency distributions of ``classification'' and ``determinant'' keywords obtained per model and reliability rating, revealing a consistent heavy-tailed behavior across all models and keyword types. This suggests that LLMs may rely on a core set of linguistic markers to evaluate reliability.

As shown in Fig. \ref{fig:keywords_rank_frequency}, classification keywords highlight key markers of a model's reliability assessments. Reliable domains are frequently associated with terms denoting neutrality, transparency, and factual reporting. Llama also focuses on ``neutral language'' and ``objectivity'', reinforcing the importance of tone and professional framing in its assessments. Conversely, unreliable domains are consistently associated with terms relating to sensationalism, bias, or conspiracy theories. Words like "misinformation", "conspiracy", and "partisan" frequently appear, reflecting the models' alignment with human evaluative criteria for detecting unreliable sources. These findings indicate that LLMs develop structured linguistic heuristics, mirroring some aspects of human reliability evaluation.

Analyzing determinant keywords reveals further insights into the mechanisms driving the models' classification. Reliable domains are frequently linked to editorial practices and institutional transparency. Notably, GPT-4o mini and Llama 3.1 emphasize ``local news'' as a relevant descriptor for reliability, suggesting that community-based reporting is perceived as an indicator of credibility. Unreliable domains, in contrast, are strongly associated with politically charged and controversial terms. Words such as ``trump'', ``biden'', ``deep state'', and ``fake news'' dominate the descriptions of unreliable sources, indicating that highly politicized content correlates with lower reliability classifications. 

Additionally, Fig. \ref{fig:keywords_rank_frequency} shows that right-leaning terms appear more frequently in descriptions of unreliable sources, while neutral or left-leaning terms are more common in reliable sources. However, the presence of political keywords alone does not determine reliability. For example, "politics" frequently appears in descriptions of reliable and unreliable sources, suggesting that it is not the topic itself but how it is framed and presented that influences model assessments. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figures/fig4_keywords_rank_rel_v_unrel.pdf}
    \caption{\textbf{Keywords' rank among ``reliable'' and ``unreliable'' domains.} We label only keywords sufficiently distant from the diagonal, meaning they are predominantly used to describe reliable or unreliable domains rather than being evenly distributed across both classifications. Additionally, we label the top 5 keywords per reliability rating. The color gradient represents the inferred political orientation of each keyword, from Left-leaning to Right-leaning, based on the domains with which they are most frequently associated.
    While summary keywords (bottom row) appear with similar frequency in both reliable and unreliable domains, classification and determinant keywords (top and middle rows) exhibit sharper separation. This result suggests that reliable and unreliable sources may cover similar topics but differ in framing tone or contextual emphasis. Notably, keywords related to transparency, objectivity, and credibility are more common among reliable domains. At the same time, sensationalist and politicized terms such as ``misinformation'', ``propaganda'', and ``bias'' are frequently linked to unreliable sources.
    }
    \label{fig:keywords_rank_rel_v_unrel}
\end{figure}

Keywords used to describe both ``reliable'' and ``unreliable'' domains are presented in Fig. \ref{fig:keywords_rank_rel_v_unrel}, which compares their ranks across the two classifications. In this visualization, the further a keyword is from the diagonal, the more characteristic it is of one of the two ratings. 

When examining ``classification'' and ``determinant'' keywords, we observe that the difference in keyword usage between the two groups is apparent. 
Reliable classifications produce terms such as ``local news'', ``scientific'', ``diverse'' and ``data-driven''. In contrast, when explaining unreliable classifications, the models utilize more controversial or politically charged terms, including politician names (e.g., ``trump'', ``biden'') as well as topics such as ``genocide'' and ``vaccines''.
On the other hand, ``summary'' keywords, which broadly describe the content of a domain's homepage, tend to show substantial overlap between reliable and unreliable news outlets. This suggests that both types of news outlets cover similar general themes and reinforce the idea that the difference does not necessarily lie in the topics discussed but rather how those topics are framed and communicated. Additionally, some terms that do not inherently indicate reliability or unreliability appear consistently associated with one category over the other, hinting at underlying stylistic or contextual differences that influence model evaluations.

These findings suggest that LLMs do not simply categorize news outlets based on explicit criteria but instead rely on an implicit understanding of reliability, possibly shaped by their training data. Their assessments also appear to be guided by linguistic framing, recurring stylistic patterns, and contextual signals, rather than just the presence of specific factual claims. This raises important questions about how LLMs internalize and apply credibility heuristics and whether they construct their evaluative frameworks based on patterns observed in human discourse. 

\subsection{Agentic Framework for Investigating LLM Decision-Making}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figures/fig5_agent_criteria_rating_bias.pdf}
    \caption{\textbf{LLMs' reliability criteria against reliability and political orientation classification.} (A) Frequency of each reliability criterion among news outlets rated as reliable and unreliable by the LLMs. (B) Proportion of domains of political orientation, as classified by the LLMs, for each criterion. We note that Opinion vs News is missing from GPT's panels as no criteria provided by the model could be associated with it.}
    \label{fig:criteria_rating_bias}
\end{figure}

Our analysis shows that LLMs often produce reliability ratings that closely align with expert evaluations from NewsGuard and MBFC. This suggests that these models have developed internal heuristics that approximate human assessments, despite not having explicit access to structured evaluation criteria. However, a critical question remains: how do LLMs actually reach these conclusions?

A key observation emerges when we prompt the models with nothing more than the URL of a domain, without any extracted content from its homepage \cite{yang2023large}. Even in this minimal setting, the models generate reliability ratings that broadly align with those assigned by human evaluators. For instance, Gemini achieves an F1-score of 0.78--slightly lower than the 0.85 obtained with the HTML homepage---and GPT an F1-score of 0.77, instead of 0.79.
This raises an important issue: are LLMs actively analyzing information, or are they simply recalling prior associations learned during training? If models can classify a news source without even seeing its content, it suggests that their evaluations may be shaped more by pre-existing knowledge than by real-time evaluation. This makes it difficult to determine whether their classifications are based on an actual assessment of the content or just on patterns they have already internalized.

To address this, we introduce a structured agentic workflow designed to probe how LLMs interpret and classify news outlets, which we test with Gemini and GPT on a sample of 71 news outlet domains. Rather than treating these models as black boxes that output a binary reliability label, we create an agent that can actively gather and analyze information before providing a final assessment. We equip this agent with three main tools: the ability to scrape webpage content directly from a domain’s homepage or subpages found within, perform web searches to retrieve additional information, and prompt another LLM for text analysis. By observing how the agent uses these tools and in what order, we can gain insights into what information LLMs prioritize, how they refine their assessments, and whether their decision-making process resembles human reasoning. This workflow shifts the focus from whether LLMs can classify news sources to how they reach those classifications, by examining them as evaluators that are capable of following multi-step processes to reach conclusions. 

In the prompt, we ask the agent to produce a detailed and structured report of the actions it performs. These actions can be summarized as a series of steps, each characterized by several attributes: the criterion the model is assessing, the specific news articles analyzed (if any), the prompt used to query a second LLM for content analysis, and details of any web search performed during that step. Tracking the agent’s steps toward its final classification allows us to distinguish between different strategies the models might employ. For instance, if an agent frequently consults external sources or analyzes specific articles, this suggests it is actively seeking verification rather than relying purely on prior knowledge. By studying these behavioral patterns, we can better investigate the mechanisms LLMs use to evaluate reliability and understand whether they exhibit adaptive capabilities.

We start by manually inspecting the reliability criteria that the models provided and thus evaluated. Overall, we observe that the same criteria are consistently assessed, although titled with different wording. Therefore, we manually annotate each so they fit into eight more generic criteria: Bias, Correction Policy, False or Misleading, Language or Tone, Opinion vs News, Responsible Information, Transparency, and Other. Then, we investigate how their use relates to the final reliability rating and political orientation label output by the model.

Figure \ref{fig:criteria_rating_bias}A shows how commonly each criterion is employed to evaluate domains classified as reliable/unreliable. The resulting frequencies show that most criteria are generally evaluated with the same frequency for both reliable and unreliable domains, by both models. Some exceptions include `Correction Policy', which both models more often employ for reliable news outlets, and `Language or Tone', which is more common among the unreliable. Overall, Gemini shows greater variability in the choice of criteria compared to GPT. Among these, `Transparency' is the most commonly evaluated criterion by both models, particularly by GPT.

In Fig. \ref{fig:criteria_rating_bias}B, we focus on the relationship between reliability criteria and political orientation. In this analysis, the differences between models and among criteria are more apparent. For instance, while `Language or Tone' is predominantly used by Gemini to evaluate right-leaning domains, GPT employs it more uniformly across the political spectrum. To a lesser extent, similar considerations apply to other criteria such as `Fake or Misleading', which Gemini uses substantially more often for center-leaning domains, unlike GPT, and `Correction Policy'.

Concerning how often the models actively seek additional information, we note slight differences between the two agents. In our experiments, Gemini focused on specific articles rather than just the whole homepage for 21\% of the domains, while GPT for the 33\%. Conversely, the models used with similar frequency the search tool to retrieve further information from the Web, with Gemini employing it for 23\% of the domains and GPT for the 20\%.

Even if on a small sample, our results show that this approach offers a clearer picture of LLMs' decision-making process, providing a foundation for future studies on explainability, adaptability, and the extent to which these models replicate human-like reasoning. 
This framework also opens the door to direct comparisons between LLMs and human decision-making in the same task. By designing experiments where human participants are given similar tools—search engines, document retrieval, and rating prompts—we can analyze how human evaluators approach credibility assessments. Comparing their behavior to that of the agent helps us understand the similarities and differences in how humans and LLMs prioritize and process information. Do humans rely more on search results and external verification, while LLMs default to internalized knowledge? Do both exhibit patterns of confirmation bias, favoring information that aligns with prior beliefs? By structuring the problem in a way that allows side-by-side analysis, it is possible to go beyond simple accuracy comparisons and begin to explore the deeper question of how LLMs and humans make complex decisions in uncertain environments.

\section{Conclusions}\label{sec12}
This study investigates how Large Language Models (LLMs) evaluate news outlet reliability, comparing their judgments to structured human benchmarks provided by NewsGuard and Media Bias Fact Check (MBFC). While prior research has often treated LLMs as potential tools for automating reliability assessments, our findings suggest a broader question: how do these models construct their reasoning, and how does it compare to human evaluative frameworks?

Our results reveal a strong alignment between LLM classifications and human expert ratings, particularly in identifying ``unreliable" sources. The models consistently flag domains associated with conspiracy theories, sensationalism, and bias, echoing key criteria used in expert evaluations. However, their classification of ``reliable" sources is less consistent, revealing differences in how they interpret credibility when contextual signals may be limited. Interestingly, when analyzing how errors in reliability classification are distributed across the political spectrum, we find that right-leaning news outlets tend to be consistently misclassified as ``unreliable'', while the center and the right-leaning as ``reliable''. These results raise critical questions about whether LLMs inherit biases from training corpora, how these biases interact with structured evaluative frameworks, and whether their reasoning patterns reflect genuine assessment or learned associations. This is further corroborated by the models producing similar ratings even when prompted only with domain URLs, rather than the scraped domain homepage.

By analyzing keyword usage via their rank-frequency distributions, we further explore how LLMs operationalize reliability. Our findings indicate that all models consistently use certain terms to explain their ratings, as shown by the characteristic heavy-tailed behavior of the distributions. Overall, we find that keywords referring to local news, factual reporting, or neutral language are typically associated with ``reliable'' domains. Conversely, ``unreliable'' domains are often characterized by terms relating to sensationalism, controversies, or bias, which reflect commonly used markers employed by human evaluators to identify low credibility sources. Additionally, our results show that keywords that summarize the contents of the webpage are often common to both reliable and unreliable news outlets, pointing toward the role of tone and framing in the models' reliability evaluations.

Moving beyond simple classification, we introduce an agentic workflow to investigate how LLMs structure their evaluation procedure when given tools to actively seek information. By equipping an AI agent with a webpage scraper, a search engine, and the possibility to query a LLM for content analysis, we gain a more granular view of how these models reach their conclusions. Analyzing whether the criteria the models decide to evaluate change with the final reliability rating, we find that `Transparency' and `Bias' emerge as the more commonly evaluated criteria for both reliable and unreliable domains, while `Language or Tone' or `Correction Policy' are not as employed. Overall, for both Gemini and GPT we observe no substantial differences between reliable and unreliable news outlets in terms of what criteria are prioritized. Conversely, discrepancies emerge when exploring the relationship between the criteria and the final political orientation label. In this case, certain criteria such as `Language or Tone' and `False or Misleading' for Gemini, and `Correction Policy' for GPT, are more often employed for specific orientations. 

Future research should expand this framework by incorporating direct human comparisons, examining how real-world evaluators navigate the same task, and testing whether LLM-based agents can develop more autonomous, context-aware decision strategies.

Ultimately, this study reframes LLMs not merely as automated credibility classifiers but as windows into the cognitive structures underlying both human and machine reasoning. Their evaluations do not just reflect computational heuristics; they offer insight into the challenges of operationalizing credibility in an information ecosystem shaped by competing narratives, institutional frameworks, and algorithmic decision-making. By unpacking their reasoning processes, we move closer to understanding the extent to which LLMs simulate structured evaluation, whether they can adapt to new decision-making environments, and how they compare to human cognitive strategies in complex judgment tasks.

\section{Methods}\label{sec:methods}
\subsection{Data collection and pre-processing}\label{sec:methods_data}
All data was collected by downloading the HTML homepages of domains rated by NewsGuard as ``reliable'' or ``unreliable'', using the \texttt{requests} library available on Python. These domains have been selected among outlets reported by NewsGuard as English-speaking, based in an English-speaking country (US, GB, CA, AU, NZ), and with a National or International focus. Not all domains could be downloaded, as many were either no longer active at the time of downloading, only accessible from specific regions, or designed in such a way to render automatic scraping difficult.

The downloaded pages are then filtered to retain only the information relevant to the LLMs to evaluate the reliability of each domain. This process involves removing unnecessary HTML components, such as scripts, styles, and navigation elements, using the \texttt{beautifulsoup4} library on Python. The resulting document almost exclusively includes textual content, particularly related to news or information about the outlet. While domain-specific pre-processing techniques can also be implemented, we opted for a more conservative, universal approach that can easily be applied to any news outlet's homepage. The final dataset employed includes a total of 2,302 domains with at least 100 characters of content, with 1,196 rated by NewsGuard as ``reliable'' and the remaining 1,106 as ``unreliable''.

\subsection{Model prompting}\label{sec:methods_prompt}
\begin{figure}[t]
    \begin{tcolorbox}[colback=gray!10!white, colframe=gray!70!black, sharp corners=south, boxrule=0.8pt, width=\textwidth]
    Given the HTML of the homepage of a news outlet, you classify it as either `reliable' or `unreliable' based on its content. Focus on the entire homepage, rather than just the main content or story that is presented. Make your judgment solely on the contents of the webpage.
    
    \vspace{0.5em}
    
    What rating (`reliable', `unreliable') would you assign to the news outlet with the homepage in the following HTML? I want the output as a JSON with the following attributes:
    \begin{itemize}
        \item `rating': your overall classification rating (`reliable', `unreliable')
        \item `text': your raw response
        \item `orientation': political orientation of the contents of the webpage (`left', `center-left', `center', `center-right', `right')
        \item `classification\_keywords': a list of keywords to justify your reasoning for classification
        \item `determinant\_keywords': a list of keywords from the webpage that were determinant for your classification
        \item `summary\_keywords': a list of keywords that summarize the contents of the webpage
    \end{itemize}
    
    \vspace{0.5em}
    
    HTML:
    
    \texttt{\{scraped HTML\}}
    \end{tcolorbox}
    \caption{Prompt used for all LLMs when provided the scraped HTML homepage.}
    \label{fig:prompt_html}
\end{figure}
We follow a zero-shot, closed-book approach to prompting for all three LLMs we experiment with, by providing no example classifications and no definitions of ``reliability'', respectively. Our prompt, which we use for all models, is split into two parts. 

The first part gives the model basic instructions regarding the classification task, providing no context to the concept of ``reliability'' beyond mentioning the terms ``reliable'' and ``unreliable''. In the second part, we provide the models with specific instructions concerning their output, specifying both the required content and its structure. Figure \ref{fig:prompt_html} reports the exact prompt used for all models.

Gemini 1.5 Flash and GPT-4o mini were prompted by performing calls to the official API endpoints made available by Google and OpenAI, respectively. LLaMA 3.1 405B was prompted by requests to SambaNova Cloud, a third-party online service that allows fast inference with LLMs. However, since LLaMA's weights are available for download, local inference is also a viable option.

Queries sent to GPT and Llama were truncated to ensure they fit within the models' context length (128,000 tokens for both), which is the maximum number of tokens they can process at once. Specifically, the scraped webpages provided to GPT and Llama were limited to the first 50,000 characters. However, this truncation affected less than 2\% of the domains.

Each domain was evaluated individually, as simultaneous classification of multiple inputs may introduce unwanted bias. For example, reliability might be assessed relative to the specific subset of domains provided in the query, rather than based on the model's inherent notion of ``reliability''. 

When evaluating the LLMs' ability to classify news outlets using only their domain names, we slightly altered the prompt in Fig. \ref{fig:prompt_html} by substituting the first paragraph with the text ``\textit{Given the domain of a news outlet, you classify it as either 'reliable' or 'unreliable' based on its content.}'', and by replacing all other occurrences of `HTML' with `URL'.

\subsection{Agentic workflow}\label{sec:methods_agents}
\begin{figure}[t]
    \begin{tcolorbox}[colback=gray!10!white, colframe=gray!70!black, sharp corners=south, boxrule=0.8pt, width=\textwidth]
    Given the URL of a news outlet, you classify it as either `reliable' or `unreliable' based on its content. Focus on the entire homepage. Don't use any prior knowledge you may have about the news outlet.

    \vspace{0.5em}
    
    This is what you are allowed to do:
    \begin{itemize}
        \item scrape web pages using the webpage\_scraper\_tool
        \item scrape specific articles using the webpage\_scraper\_tool
        \item use the web\_search tool if you fail to scrape a page
        \item use the web\_search tool to find information not contained in the scraped pages
        \item use the llm\_language\_analysis tool to analyze content requiring NLP techniques
        \item use the llm\_language\_analysis tool to analyze the search results
    \end{itemize}

    \vspace{0.5em}
    
    This is what you are forbidden to do:
    \begin{itemize}
        \item use the web\_search tool to find human reliability ratings or opinions
        \item simulate content, URLs, or search results
        \item use examples
        \item use prior knowledge you have about the news outlet
        \item analyze the website's structural elements, such as its layout or navigation 
    \end{itemize}

    \vspace{0.5em}
    
    This is what you must do:
    \begin{enumerate}
        \item decide how many and which criteria of 'news outlet reliability' you must evaluate and save them in an ordered list
        \item then, evaluate them one by one and in order with the tools you are provided, which are the most sophisticated available
        \item after each step ask yourself: can I use the webpage\_scraper\_tool or web\_search tool to improve my assessment?
    \end{enumerate}

    \vspace{0.5em}
    
    Rely on the webpage\_scraper\_tool and web\_search tool as fallback. Avoid using placeholders or simulated examples.

    \vspace{0.5em}
    
    It is a step-by-step process. Use the information gathered at each step to help you with the next.
    \end{tcolorbox}
    \caption{First part of the prompt used for the LLM agent, where instructions about the task are provided.}
    \label{fig:prompt_agent_part1}
\end{figure}

\begin{figure}[t]
    \begin{tcolorbox}[colback=gray!10!white, colframe=gray!70!black, sharp corners=south, boxrule=0.8pt, width=\textwidth]
    At the end, your output must be a JSON with the following attributes
    \begin{itemize}
    \item `webpage\_url'
    \item `rating\_report': a report containing a summary of your final assessment
    \item `reliability\_criteria\_evaluatated': ordered list of reliability criteria evaluated
    \item `rating': your final reliability rating (`reliable' / `unreliable')
    \item `orientation': political orientation of the news outlet (`left', `center-left', `center', `center-right', `right')
    \item `additional\_comment': if you have any additional comments that are related to the rating (such as your inability to perform certain steps) then put them here
    \item `steps': a list of all steps you made, each reported as a JSON with the following attributes
    
        \begin{itemize}
            \item `step\_number'
            \item `reliability\_criterion': which criterion from your list are you evaluating in this step
            \item `step\_scope': what is the step aimed at
            \item `step\_reason': why have you decided to make this step
            \item `step\_outcome': explain in natural language the results obtained at this step
            \item `analyzed\_article': if at this step you analyze a specific article or list of articles, report the list of URLs here
            \item `llm\_prompt' (question asked to the LLM, without the content you have asked to analyze)
            \item `search\_results\_analyzed': if during this step you have analyzed search results, then provide them here as a list, each reported as JSON with attributes `search\_query', `webpage\_urls' (list), `reason\_for\_checking\_search\_result', `llm\_summary\_of\_results'
        \end{itemize}
    \end{itemize}
    
    \vspace{0.5em}
    
    Based on all instructions I provided you, scrape and then give me a reliability rating of the news outlet at this URL: \texttt{\{URL of the news outlet's homepage\}}

    \vspace{0.5em}
    
    You have no time constraints. Take as long as you need. You have all the sophisticated tools and information you need.  
    \end{tcolorbox}
    \caption{Second part of the prompt used for the LLM agent, where instructions about the output format and the URL of the news outlet to classify are provided.}
    \label{fig:prompt_agent_part2}
\end{figure}

We implemented the agentic workflow for outlet reliability classification with \texttt{smolagents}, a library for Python developed by Hugging Face. In particular, we implement a so-called Code Agent, which is an agent that performs actions via code writing \cite{wang2024executable}. By allowing an agent to write its actions in code and providing it with a set of tools that can be utilized via code, we obtain a model that is capable of designing and executing a workflow, in our case aimed at news outlet reliability classification. In particular, we provide the agent with these three tools:
\begin{itemize}
    \item \texttt{webpage\_scraper\_tool}, which is a function that downloads a webpage into a Markdown-formatted document using the \texttt{markitdown} Python library
    \item \texttt{llm\_language\_analysis}, which is a function that prompts a LLM (the same model behind the agent)
    \item \texttt{web\_search}, which is a tool built in \texttt{smolagents} that retrieves web search results using the DuckDuckGo API, which we limit to the first 20 entries
\end{itemize}

The exact prompt we provide the agent is reported in Fig. \ref{fig:prompt_agent_part1} and \ref{fig:prompt_agent_part2}, whereas we leave unchanged the default system instructions implemented in \texttt{smolagents}. Finally, we limit the agent's workflow to a maximum of 20 steps, to prevent infinite loops or an excessive number of calls to LLMs.

\bmhead{Acknowledgements}
SERICS (PE00000014) under the NRRP MUR program funded by the European Union - NextGenerationEU, project CRESP from the Italian Ministry of Health under the program CCM 2022, PON project “Ricerca e Innovazione” 2014-2020, and PRIN Project MUSMA for Italian Ministry of University and Research (MUR) through the PRIN 2022.

%\section*{Declarations}


%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
%\bigskip
%\begin{flushleft}%
%Editorial Policies for:

%\bigskip\noindent
%Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

%\bigskip\noindent
%Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

%\bigskip\noindent
%\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

%\bigskip\noindent
%BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
%\end{flushleft}

%\begin{appendices}

%\section{Section title of first appendix}\label{secA1}

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

%\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

\newpage

\section*{Supplementary information}
\setcounter{figure}{0}
\renewcommand{\thefigure}{S\arabic{figure}}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/figS1_conf_matrix_orientation.pdf}
    \caption{\textbf{LLMs' classification against expert human evaluators.} All of the three models show to be able to correctly predict the political leaning of the news outlet they are analyzing.  (A) Comparing the answers of the models with the NewsGuard labels, the accuracy is high, with a few error on some bias news outlet, classified as center.
    (B) Using as ground truth the labels of MBFC the accuracy is the same, with a higher value of accordance  using LLaMA 3.1 405B.}
    \label{suppfig:conf_matrix_orientation}
\end{figure}

\end{document}
