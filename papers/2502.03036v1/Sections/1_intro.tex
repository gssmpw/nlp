\section{INTRODUCTION}
%%% (1) 

Recent advancements \cite{gpt3, achiam2023gpt4, kaplan2020scaling, guo2024scaling} in scaling laws have revealed that the performance of Large Language Models (LLMs) systematically improves predictably as the number of model parameters, the volume of training data, and computational resources increase. These findings are crucial as they provide researchers and practitioners with a framework for efficiently allocating limited computational resources to optimize model performance. Building on this foundation, we propose to investigate whether recommendation models also conform to scaling laws. By identifying such models, scaling laws can be utilized to guide the training of larger models, thus enhancing their performance.

Besides the scaling laws found in LLMs such as GPTs \cite{gpt3, achiam2023gpt4}, LLaMAs \cite{touvron2023llama, dubey2024llama}, autoregressive sequential models have been shown to adhere to scaling laws across various domains, including generative image modeling, video modeling, etc \cite{henighan2020scaling}.  
The expansion of Vision Transformers (ViT) has also achieved significant success in the field of computer vision \cite{zhai2022scalingvisiontransformers, dehghani2023scalingvisiontransformers22}.
This revolutionary innovation has also been extended to recommendation models.
Recent studies \cite{zhang2023scaling, shen2024predictive} demonstrate that autoregressive sequence recommendation models also follow these scaling laws. The success of projects like HSTU \cite{hstu, chen2024hllm, wu2024survey} indicates that scaling up sequential recommendation models in accordance with these laws is an effective strategy for developing large-scale recommendation systems.

Sequential recommendation models have been a focal point of research in the field of recommender systems, characterized by a wide array of architectural innovations \cite{xie2024breaking, yin2024dataset, xie2024bridging, shen2024exploring, wangmf, liu2023user,wang2021hypersorec}.
Initially, pooling operations were employed to manage interaction sequences \cite{covington2016deep}. 
With the development of deep learning, more sophisticated models emerged, including CNN-based architectures such as Caser \cite{caser}, GNN-based models like SR-GNN \cite{wu2019session}, RNN-based frameworks like GRU4Rec \cite{hidasi2015session}. Inspired by the huge success of Transformers in NLP, models based on self-attention mechanisms were proposed, leading to notable architectures such as SASRec \cite{kang2018self} and Bert4Rec \cite{bert4rec}.

Besides sequential recommendation models, traditional Deep Learning Recommendation Models (DLRMs), such as DCN \cite{dcn} and xDeepFM \cite{xdeepfm}, also play a crucial role in recommender systems. A fundamental concept in these DLRMs is feature interaction, which is pivotal for enhancing model performance. Feature interactions are categorized into two types: explicit and implicit. Explicit interactions model feature relationships directly through various operators, such as the dot product \cite{fm, dcn}, bilinear functions \cite{xdeepfm}, and attention mechanisms \cite{song2019autoint}. Conversely, implicit interactions are facilitated by applying deep neural networks (DNNs). Although such an approach lacks interpretability, it is extensively used in state-of-the-art DLRMs such as DCN \cite{dcn}, DCNv2 \cite{dcnv2}, DeepFM \cite{deepfm}, and PNN \cite{pnn}. In fact, the integrated DNNs in such models are a key driver of their superior performance. However, previous studies \cite{guo2023embedding, ardalani2022understanding} have indicated that DLRMs do not necessarily exhibit significant performance improvements with increased model size. Nonetheless, the concept of feature interaction can still guide us in designing models.

From the perspective of feature interaction, sequential recommendation models can be conceptualized as exploring the interplay between various features over time. Pooling methods \cite{covington2016deep} have limited expressive capabilities because they overlook the semantic richness of interaction sequences. CNN-based methods \cite{caser} are constrained by a fixed window size, limiting their ability to capture long-range dependencies. RNN-based models \cite{hidasi2015session} interact directly with the previous timestep's hidden state, which can restrict their capacity to model complex interactions. GNN-based approaches \cite{wu2019session} limit feature interactions to directly connected items, thereby narrowing their scope. In contrast, attention-based models, including SASRec \cite{kang2018self}, BERT4Rec \cite{bert4rec}, TiSASRec \cite{tisasrec}, and HSTU \cite{zhai2024actions}, enable comprehensive item interactions. Consequently, these models are more effective at capturing dynamic user interests through interaction sequences. TiSASRec \cite{tisasrec} further improves on SASRec by incorporating time intervals and relative position information, enhancing its performance. HSTU \cite{zhai2024actions} advances this by utilizing positional and temporal information alongside element-wise multiplication to model explicit interactions between items, thereby demonstrating superiority over its predecessors.

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{0mm}
    \setlength{\belowcaptionskip}{-10px}
    \includegraphics[width=0.8\linewidth]{images/intro-temporal_information.pdf}
    \caption{Different temporal intervals or orders between objects may lead to varying subsequent interacted items.}
    \label{fig:intro}
\end{figure}

Despite the significant advancements made in the aforementioned work, there remain several shortcomings that need to be addressed. 
Firstly, previous studies fail to fully leverage temporal and positional information in explicit interactions. They integrate this information by simply adding embeddings to input sequences \cite{kang2018self}, incorporating them into the query and key matrices used in self-attention layers \cite{tisasrec}, or adjusting attention weights \cite{zhai2024actions}. Compared to various methods that facilitate feature interactions, this simple addition lacks expressive capacity. Understanding positional and temporal information is crucial for sequential recommendation because different cues can lead to varying results, as illustrated in Figure \ref{fig:intro}. However, existing models have limited feature interaction with temporal and positional information, hence severely restricting their ability to effectively convey the corresponding temporal and positional cues. Secondly, while HSTU emphasizes explicit interactions, it underemphasizes implicit feature interactions, potentially leading to a loss of nuanced learning processes post-interaction and thus constraining the model's expressiveness.

To address the aforementioned challenges, we propose a novel attention-based model named \textit{FuXi}-$\alpha$. Our approach introduces an Adaptive Multi-channel Self-attention (AMS) layer, which resolves the issue of insufficient feature interactions by modeling the temporal and positional information separately. Furthermore, we integrate a multi-stage feedforward neural network (MFFN) layer to facilitate implicit feature interactions, thereby boosting the model's expressiveness. The proposed method outperforms state-of-the-art sequential recommendation techniques across several benchmark datasets. We also evaluate the model's adherence to scaling laws using a large-scale industrial dataset. The results indicate that performance consistently improves with increased model complexity, highlighting its potential for large-scale recommendation systems. Our contributions are summarized as follows:
\begin{itemize}[leftmargin=*,align=left]
    \item We propose a novel model, \textit{FuXi}-$\alpha$, which adheres to the scaling law by leveraging the perspective of feature interactions.
    \item We design an Adaptive Multi-channel Self-attention (AMS) layer that disentangles the modeling of temporal, positional, and semantic information. We demonstrate that it permits a more expressive representation of temporal and positional information. Additionally, we introduce a Multi-stage Feedforward Network (MFFN) to enhance implicit feature interactions.
    \item We conducted extensive experiments on multiple real-world datasets and online A/B tests on Huawei Music, demonstrating our model's strong performance. Specifically, the online deployment led to an increase of 4.76\% in the average number of song plays per user and a 5.10\% enhancement in the average duration of song playback per user. 
\end{itemize}