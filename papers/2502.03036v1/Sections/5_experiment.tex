\section{EXPERIMENTS}\label{experiment}
\input{Sections/table_figure}
% In this section, We conduct extensive experiments on multiple datasets to answer the following research questions:
% \begin{itemize}[leftmargin=*,align=left]
% \item \textbf{(RQ1)} How does our proposed FuXi-$\alpha$ architecture perform when compared with other models?
% \item \textbf{(RQ2)} How does the efficiency of our proposed FuXi-$\alpha$ architecture when compared with other models?
% \item \textbf{(RQ3)} How do different modules influence the performance of FuXi-$\alpha$? 
% \item \textbf{(RQ4)} How does the key parameters affect the performance of FuXi-$\alpha$?
% \end{itemize}

\subsection{Experiment Setup}\label{ExperimentSetup}

\subsubsection{Datasets}

To evaluate the performance of the proposed \textit{FuXi}-$\alpha$ architecture, we conduct extensive experiments on four real-world datasets, including three public datasets and one private large-scale dataset, which are described as follows:

\begin{itemize}[leftmargin=*,align=left]
  \item \textbf{MovieLens-1M} and \textbf{MovieLens-20M}\footnote{https://grouplens.org/datasets/movielens/}. 
    The MovieLens dataset is a widely used movie recommendation dataset, which contains users' rating and tagging activities.
    It has multiple subsets of different sizes.
    We select two subsets, MovieLens-1M and MovieLens-20M for our experiments.
  \item \textbf{KuaiRand}\footnote{https://kuairand.com/}. 
    This dataset is collected with the user logs of a video-sharing app from kuaishou.
    Users in this platform are usualy very active, with more than 200 interactions on average.
  \item \textbf{Industrial} 
    This dataset is constructed from user records of a mainstream music listening app, which has tens of millions active users every month.
    We construct users' behavior sequence with over a month of positive behaviors, including collect, like, play and so on.
\end{itemize}
For the first two datasets (\textbf{MovieLens-1M} and \textbf{MovieLens-20M}), we use the pre-processed train/validation/test set \footnote{https://github.com/facebookresearch/generative-recommenders} as in HSTU \cite{zhai2024actions} from Meta exactly.
For the latter two datasets (\textbf{KuaiRand} and \textbf{Industrial}), we process them using a similar manner to HSTU \cite{zhai2024actions} by ourself.
The statistics are shown in Table \ref{tab:dataset_statistics}. 


\subsubsection{Compared Baseline}
For a comprehensive comparison, we compare \textit{FuXi}-$\alpha$ against two types of representative baselines: 
i) conventional models, including BPRMF \cite{rendle2012bpr}, GRU4Rec \cite{hidasi2015session}, and NARM \cite{li2017neural}; ii) autoregressive generative models, including SASRec \cite{kang2018self}, LLaMa \cite{dubey2024llama}, and HSTU \cite{zhai2024actions}.


\subsubsection{Evaluation Metrics}
We employ the widely used top-K Hit Ratio (HR@$K$), Normalized Discounted Cumulative
Gain (NDCG@$K$) and Mean Reciprocal Rank (MRR) to evaluate the recall performances.
For all metrics, higher value means better performance. 
We rank the ground-truth item from full set of items and report the performance of $K = 10, 50$ by default.

\subsubsection{Parameter Settings}
We implement our proposed \textit{FuXi}-$\alpha$ with Pytorch \cite{paszke2019pytorch}.
To enable large-scale model training, we apply the multi-machine and multi-card parallelism with the Accelerate library \cite{kotter2012accelerate}.
For a fair comparison, we maintain the same model parameters as HSTU \cite{zhai2024actions} in the first two datasets,
except for the number of layers.
For the KuaiRand dataset, we set the hidden dimension as 50, and the number of negative samples as 128 by default.
All other parameters like optimizer, learning rate and weight decay are consistent with HSTU \cite{zhai2024actions}.
For all the three datasets, the embedding dimensions and self-attention hidden vector dimensions are identical.
For the basic modeling capacity comparison, we set the number of layers as 2.
We also extend these generative models to deeper layers by stacking 4x number of layers (8 layers) and denoting it as "XX-Large" to analyze scaling effects.

\subsection{Performance Comparison  (RQ1)}\label{PerformanceComparison}
\subsubsection{Public Dataset Performance}
The overall performance comparison of the proposed \textit{FuXi}-$\alpha$ and baseline models are shown in Table \ref{tab:public_performance}.
Based on the results, we have the following observations:


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}[leftmargin=*,align=left]
    \item Firstly, the generative models (i.e., SASRec, LLaMa, HSTU, and \textit{FuXi}-$\alpha$) outperform conventional models (i.e., BPRMF, GRU4Rec and NARM), 
    even when equipped with just two layers of parameters.
    This demonstrates the generative models' superior ability in capturing complex item relationships and diverse user preferences by their autoregressive modeling paradigm.
    
    \item Secondly, as an early sequential model, SASRec fails to scale up to 8 layers across all three datasets, with a significant performance drop when the number of layers is increased to 8.
    In contrast, the two recently proposed transformer-based architectures, LLaMa and HSTU, show substantial improvements in the first two datasets.
  
    \item Finally, \textit{FuXi}-$\alpha$ consistently obtains the best results on all three datasets with all evaluation metrics, no matter it's a shallow network or a deep network.
    This demonstrates the outstanding ability of our proposed \textit{FuXi}-$\alpha$.
    Specifically, for shallow network, it outperforms the strongest baseline HSTU by 13.24\% in NDCG@10 (10.59\% in NDCG@50, 10.81\% in HR@10, 6.94\% in HR@50, 13.72\% in MRR) on average of the three datasets.
    For deep network, it outperforms the strongest baseline HSTU-Large by 7.26\% in NDCG@10 (5.24\% in NDCG@50, 6.14\% in HR@10, 3.19\% in HR@50, 6.90\% in MRR) on average of the three datasets.
    The excellent performance of \textit{FuXi}-$\alpha$ demonstrates the great utility of introducing explicit and implicit feature interaction for dedicated user behavior modeling.

\end{itemize}

\subsubsection{Industrial Dataset Performance}
Table \ref{tab:industrial_performance} presents the performance comparison of our proposed \textit{FuXi}-$\alpha$ against several baseline models on a private, large-scale industrial dataset.
The current online baseline in this scenario is a multi-channel recall system, with SASRec as one of the channels that recalls items based on embedding similarity. 
The music recalled from multiple channels is mixed together and then passed through a cascaded pre-ranking and ranking process to obtain the final recommended music list.
From Table \ref{tab:industrial_performance}, we have two key observations.
Firstly, the newly proposed LLaMa and HSTU significantly outperform SASRec in this music recommendation scenario, achieving gains of 64.82\% and 71.75\% in NDCG@10, respectively.
Secondly, our \textit{FuXi}-$\alpha$ outperforms both LLaMa and HSTU by 11.54\% and 8.19\%, respectively.
These substantial improvements highlight the potential of scaling laws, and the superiority of our proposed \textit{FuXi}-$\alpha$.

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{-15pt}
        \includegraphics[width=0.8\linewidth]{images/industrial_scaling1.pdf}
    \caption{Scaling of \textit{FuXi}-$\alpha$ on Industrial Dataset.}
    \label{fig:industrial_scaling}
%     x = [1, 2, 3, 4, 5]
% y = [0.1764, 0.1875, 0.1971, 0.2015, 0.2054]
% z = [0.3094, 0.3230, 0.3364, 0.3424, 0.3476]
\end{figure}

\subsubsection{Scaling of \textit{FuXi}-$\alpha$ on Industrial Dataset}
Figure~\ref{fig:industrial_scaling} presents the performance of our proposed \textit{FuXi}-$\alpha$ on the industrial dataset when scaling up the number of layers while keeping all other hyper-parameters unchanged. 
Due to the memory limitation, we only scale up the layers to 32.
We observe that \textit{FuXi}-$\alpha$ adheres to the scaling law, as the results show a positive relationship between the model's performance and its size.
This is a highly attractive property as the performance can be further improved by scaling up of the model size, its training data, and the computational resources used.

\subsection{Efficiency Comparison (RQ2)}\label{EfficiencyComparison}

We assess the efficiency of the \textit{FuXi}-$\alpha$ architecture by comparing its Throughput Per Second (TPS) with generative baseline models. Experiments were conducted on the KuaiRand dataset with sequence lengths ranging from 200 to 800. Each experiment involved three complete forward and backward propagations across the dataset, calculating the average number of training samples processed per second. All hyperparameters, except sequence length, were consistent with previous experiments. Table \ref{tab:efficiency} shows the TPS results. As sequence length increases, TPS for all models decreases. Notably, SASRec and LLaMa outperform HSTU and \textit{FuXi}-$\alpha$ in TPS, likely due to their exclusion of temporal information encoding, which, while performance-enhancing, is time-intensive. Consequently, \textit{FuXi}-$\alpha$ achieves similar TPS to HSTU but significantly better overall performance, as seen in Tables \ref{tab:public_performance} and \ref{tab:industrial_performance}.

 
\subsection{Ablation Study (RQ3)}
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To assess the effectiveness of sub-modules in our \textit{FuXi}-$\alpha$ architecture, we analyze three model variants: 
(1) \textbf{\textsl{Base Model}}: Replaces the AMS module with the vanilla self-attention layer from SASRec and substitutes the MFFN module with a single-stage MLP from HSTU. 
(2) \textbf{\textsl{w/o AMS}}: Replaces the AMS module with the vanilla self-attention layer. 
(3) \textbf{\textsl{w/o MFFN}}: Substitutes the MFFN module with a single-stage MLP.

Table \ref{tab:Impact of main parts} presents the ablation results, revealing the critical role of each component in model performance. Notably, removing the second stage of the MFFN results in a significant performance drop, emphasizing the importance of thorough implicit feature interactions. Despite this, the model still outperforms HSTU, demonstrating the effectiveness of our approach in capturing temporal and positional information. Additionally, replacing the AMS with the vanilla self-attention layer leads to a marked performance decline, highlighting the necessity of explicit feature interactions and effective use of temporal and positional data in recommendation tasks. These results confirm the essential contributions of each module to the model's predictive capability.

\subsection{Hyperparameter Study (RQ4)}


\begin{figure}[t]
    \centering    
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{-5pt}
    \subfigure[MovieLens-1M] {
     \label{fig:a}     
    \includegraphics[width=0.47\columnwidth]{images/ml1m_layer1.pdf}  
    }     
%     x = [1, 2, 3, 4]
% y = [0.1831, 0.1911, 0.1934, 0.1869]
% z = [0.3244, 0.3307, 0.3359, 0.3257]
    \subfigure[KuaiRand] { 
    \label{fig:b}     
    \includegraphics[width=0.47\columnwidth]{images/kuairand_layer.pdf}     
    }    
    \caption{Performances with different number of layers.}     
    \label{fig:layer}   
%     x = [1, 2, 3, 4]
% y = [0.0537, 0.0547, 0.0555, 0.0566]
% z = [0.1067, 0.1088, 0.1105, 0.1126]
\end{figure}


\begin{figure}[t]
    \centering    
    \setlength{\abovecaptionskip}{0mm}
    \setlength{\belowcaptionskip}{-10pt}
    \subfigure[MovieLens-1M] {
     \label{fig:a}     
    \includegraphics[width=0.47\columnwidth]{images/ml1m_width.pdf}  
    }     
%     x = [1, 2, 3, 4]
% y = [0.1123, 0.1542, 0.1862, 0.1903]
% z = [0.2091, 0.2803, 0.3306, 0.3328]
    \subfigure[KuaiRand] { 
    \label{fig:b}     
    \includegraphics[width=0.47\columnwidth]{images/kuairand_width.pdf}     
    }    
%     x = [1, 2, 3, 4]
% y = [0.0371, 0.0457, 0.0505, 0.0548]
% z = [0.0744, 0.0926, 0.1023, 0.1082]
    \caption{Performances with different hidden dimension.}     
    \label{fig:dimension}     
\end{figure}

\begin{figure}[ht]
\centering
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
% \setlength{\abovecaptionskip}{2mm}
% \setlength{\belowcaptionskip}{-2mm}
\subfigure[MovieLens-1M] {
 \label{fig:a}     
\includegraphics[width=0.47\columnwidth]{images/ml1m_negative.pdf}  
}     
% x = [1, 2, 3, 4]
% y = [0.1828, 0.1867, 0.1934, 0.1930]
% z = [0.3250, 0.3299, 0.3359, 0.3372]
\subfigure[KuaiRand] { 
\label{fig:b}     
\includegraphics[width=0.47\columnwidth]{images/kuairand_negative.pdf}     
}    
% x = [1, 2, 3, 4]
% y = [0.0464, 0.0482, 0.0555, 0.0583]
% z = [0.0916, 0.0990, 0.1105, 0.1135]
\caption{Diverse negative sample counts in performances.
% \jy{x-axis should be # of negative samples}
}
\label{fig:negative}
\end{figure}

We examine the effects of various hyper-parameters on \textit{FuXi}-$\alpha$, focusing on (1) the number of layers, (2) the hidden dimension, and (3) the number of negative samples for training. Due to space constraints, we present only NDCG@10 and HR@10 results for the MovieLens-1M and KuaiRand datasets. Results for other metrics (NDCG@50, HR@50, MRR) and datasets (MovieLens-20M) are similar but omitted. We alter one hyper-parameter at a time while keeping others constant to ensure fair comparisons.

\subsubsection{The number of layers}
Increasing layers is a rapid method to scale model parameters and enhance \textit{FuXi}-$\alpha$'s representational capacity. We vary layers from 2 to 16, as shown in Figure \ref{fig:layer}. On MovieLens-1M, performance improves from 2 to 8 layers, but declines at 16 layers. Conversely, on KuaiRand, performance consistently increases from 2 to 16 layers. This may be due to MovieLens-1M's smaller size limiting parameter scaling.

\subsubsection{The hidden dimension}
Uniform embedding and self-attention hidden dimensions are used across datasets. Increasing hidden dimensions enhances item representation and self-attention similarity accuracy. Adjusting dimensions from 8 to 64, Figure \ref{fig:dimension} shows performance on MovieLens-1M saturates at 32 dimensions, with minimal gains beyond. In contrast, KuaiRand performance steadily improves across all dimensions.

\subsubsection{Negative Samples}
The influence of negative sampling on large recommendation models has been overlooked in studies on LLM scaling laws \cite{kaplan2020scaling,hoffmann2022training}. We vary negative samples from 32 to 256, with results in Figure~\ref{fig:negative}. Performance improves on both datasets even beyond 64 negative samples, with gains from negative sampling surpassing those from layer increases. This underscores the critical role of negative sampling in enhancing 
%large recommendation 
models' performance.

\subsection{Online A/B Test}

In a main scenario of Huawei Music, we conducted a 7-day online A/B test to evaluate the performance of our new model, \textit{FuXi}-$\alpha$, utilizing 30\% of the user traffic. The results demonstrated that \textit{FuXi}-$\alpha$ achieved significant improvements compared to a well-optimized multi-channel retrieval baseline that has been refined over several years. Specifically, the average number of songs played per user increased by 4.67\%, while the average listening duration per user rose by 5.10\%. These findings indicate that \textit{FuXi}-$\alpha$ excels in enhancing user interaction and engagement, particularly by improving user experience and increasing platform usage time.
After evaluation of several weeks, the \textit{FuXi}-$\alpha$ had become an inherent channel in this scenario to serve most of the online traffic.

% \subsection{Scalability Analysis (RQ4)}