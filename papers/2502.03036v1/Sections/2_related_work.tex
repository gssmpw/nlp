\section{RELATED WORK}

\subsection{Scaling Law}

Scaling laws, prevalent in Natural Language Processing (NLP) \cite{gpt3, achiam2023gpt4, kaplan2020scaling, yin2024entropy}, describe the relationship between a model's performance and its size, training data, and computational resources. These laws extend beyond NLP to domains like autoregressive generative models \cite{henighan2020scaling} and visual processing \cite{zhai2022scaling, yang2023lever, yang2024exploring, penglive,  li2024configure}. In the recommendation domain, applying scaling laws is challenging. Studies show that scaling benefits do not always apply to recommendation models \cite{guo2023embedding, ardalani2022understanding}. Issues such as embedding collapse have been reported \cite{guo2023embedding}, and increasing non-embedding parameters in Deep Learning Recommendation Models (DLRMs) offers minimal gains \cite{ardalani2022understanding}.

Despite these challenges, research into scaling laws for recommendation models persists. Studies have explored scaling in user ad activity sequences with generative models \cite{chitlangia2023scaling} and efforts to scale user representation models \cite{shin2023scaling}. A sequential recommendation model with 0.8 billion parameters has been developed, highlighting scaling laws in this domain \cite{zhang2023scaling}. Additionally, it was found that increasing computational resources benefits DLRM less than Generative Recommendations (GR) \cite{zhai2024actions}. This led to the development of HSTU, enhancing the GR paradigm in feature processing, model architecture, and efficiency \cite{zhai2024actions}. 

Our study proposes a model designed to adhere to scaling laws, facilitating its expansion into a large-scale recommendation model for improved performance.

\subsection{Sequential Recommendation}

Sequential recommendation focuses on predicting users' future interests based on past interactions \cite{yin2024learning, han2024efficient, yin2023apgl4sr, han2023guesr}. Early approaches used Markov Chain models \cite{rendle2010factorizing}. With advancements in neural networks, various architectures have enhanced sequential modeling. GRU4Rec \cite{hidasi2015session} uses Gated Recurrent Units to capture sequential data, while Caser \cite{caser} employs CNNs for short-term preference patterns. To model long-term preferences, memory network-based methods \cite{chen2018sequential, huang2018improving, zhang2024learning} were developed. Wu et al. \cite{wu2019session} introduced graph-based interaction modeling. SASRec \cite{kang2018self} and BERT4Rec \cite{bert4rec} leverage self-attention mechanisms for improved recommendations.

% origin paragraph
In traditional recommendation systems, discriminative-based models typically rank items using a multi-level scoring approach. 
In contrast, generative recommendation models can directly generate the items to be recommended. Following the introduction of HSTU, it has become feasible for autoregressive sequence models that adhere to scaling laws to evolve into generative recommendation models by increasing their model size. HLLM \cite{chen2024hllm} transforms the input IDs into text information encoded by large language models (LLMs), and leverage another LLM for generative sequence recommendation. MBGen \cite{liu2024multi} incorporates behavior tokens into the sequence, thereby improving the model's multi-task capabilities.

In this study, we adopt the autoregressive sequence modeling paradigm to develop a new large-scale recommendation model.

\subsection{Feature Interactions}

Feature interactions play an important role in recommender systems \cite{zhang2022clustering, xu2024multi, zhang2024unified, wang2021hypersorec} and can be divided into explicit and implicit methods.

Explicit interactions are categorized into four types based on their operations: dot product \cite{fm,deepfm,pnn,dcn}, bilinear function \cite{dcnv2,xdeepfm,dcnv3}, convolution \cite{ccpm,liu2019FGCNN,li2019fi-gnn, zhang2019graph, zhang2022hierarchical, zhang2022cglb, zhang2020context, wang2019mcne}, and attention mechanisms \cite{song2019autoint,li2020interhat}. Dot product-based methods like Factorization Machines (FM) and DeepFM extend logistic regression by capturing pairwise interactions \cite{fm, deepfm}. DCN \cite{dcn} models higher-order interactions through product-based cross networks, while DCNv2 \cite{dcnv2} enhances DCN with bilinear functions. DCNv3 \cite{dcnv3} introduces the Exponential Cross Network for more refined modeling. CCPM \cite{ccpm} and FGCNN \cite{liu2019FGCNN} use CNNs for interactions, and Fi-GNN \cite{li2019fi-gnn} applies GNNs. Attention-based methods like AutoInt \cite{song2019autoint} use attention mechanisms, and InterHAt \cite{li2020interhat} employs self-attention for interpretable high-order interactions.

Implicit interactions often use deep neural networks (DNNs) \cite{zhang2016dnn} to simultaneously engage all features. This approach is often combined with explicit interaction structures to enhance overall interaction capabilities. For example, dual-tower architectures like Wide \& Deep and DeepFM integrate low-order explicit interactions with high-order implicit interactions \cite{cheng2016wideanddeep, deepfm}. Models like xDeepFM, DCN, and DCNv2 use DNNs to compensate for certain limitations of explicit feature interactions. \cite{xdeepfm, dcn, dcnv2}. Single-tower structures improve the expressiveness of explicitly crossed features by employing stacked DNNs after explicit interaction structures \cite{he2017nfm, pnn, qu2018pin}.

Inspired by successful feature interaction applications in recommendation models, our work aims to enhance large-scale recommendation models through improved feature interactions.


