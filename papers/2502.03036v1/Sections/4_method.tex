\section{METHODOLOGY}
% \textcolor{orange}{implicit -> semantic}

The overview of our model architecture is depicted in Figure \ref{fig:structure-overview}, which is composed of a stack of $b$ \textit{FuXi} Blocks. In the following sections, we will introduce each module individually. Finally, we will discuss the optimization objectives.

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{0mm}
    \setlength{\belowcaptionskip}{-3mm}
    \includegraphics[width=0.4\linewidth]{images/structure-overview.pdf}
    \caption{The overall architecture of the proposed \textit{FuXi}-$\alpha$.}
    \label{fig:structure-overview}
\end{figure}

\subsection{Embedding Layer}

We convert each user's interaction sequence into a fixed-length sequence of length $n$ through truncation or padding before the embedding layer. Sequences shorter than $n$ are padded with a special "padding item". 
In the embedding layer, each item $i \in \mathcal{I}$ is mapped to a $d$-dimensional vector using a learnable embedding matrix $\mathbf{E} \in \mathbb{R}^{|\mathcal{I}| \times d}$ where $d$ is the latent vector dimensionality. We also employ learnable positional encodings \cite{gehring2017convolutional}, where $\boldsymbol{p}_i$ denotes the positional embedding of the $i$-th position in the sequence. For a user $u$ with a sequence $\mathcal{S}_u = [i_1^{(u)}, \ldots, i_{n_u}^{(u)}]$, the output is $\mathbf{x}^{0} = [\mathbf{e}_1^{(u)} + \boldsymbol{p}_1, \ldots, \mathbf{e}_{n_u}^{(u)} + \boldsymbol{p}_{n_u}, \boldsymbol{0}, \cdots, \boldsymbol{0}]$, where the zero vectors denote the padding items for positions beyond $n_u$ up to $n$.

\subsection{FuXi Block}

The core component of our model is composed of $b$ stacked layers of \textit{FuXi} block which are similar to the transformer decoder \cite{transformer}. Each \textit{FuXi} block consists of an Adaptive Multi-channel Self-attention (AMS) layer and a Multi-stage Feed-Forward Network (MFFN). The adaptive multi-channel self-attention is a variant of the multi-head self-attention \cite{transformer}, while the multi-stage FFN first combines the multi-channel outputs of the AMS layer and then performs implicit feature interactions. 
In this architecture, let $\mathbf{x}^{l-1} \in \mathbb{R}^{n \times d}$ denote the input to the $l$-th layer, and $\mathbf{x}^{l} \in \mathbb{R}^{n \times d}$ denote the output of the $l$-th layer. The initial input for the first layer is given by $\mathbf{x}^{0}$.

\subsubsection{Adaptive Multi-channel Self-attention} 
The AMS layer is designed to effectively capture and utilize the user interest patterns inherent in sequential data. Unlike conventional multi-head self-attention mechanisms, which typically integrate positional encodings directly into the input embeddings, our \textit{FuXi} self-attention separates the processing of hidden states, positional information, and temporal signals into distinct attention heads. This separation allows each head to specialize in capturing different aspects of the sequence data, thereby enhancing the model's capacity to learn complex interest patterns.

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{5pt}
    \setlength{\belowcaptionskip}{-10pt}
    \includegraphics[width=0.9\linewidth]{images/structure-ams.pdf}
    \caption{Illustration of Adaptive Multi-channel Self-attention (AMS). In contrast to the conventional multi-head self-attention, AMS decouples the modeling of temporal and positional information from semantics information. }
    \label{fig:structure-fuxi-attention}
\end{figure}

As depicted in Figure \ref{fig:structure-fuxi-attention}, we define three types of channels: semantic, temporal, and positional channels. 
The attention weights in the temporal and positional channels depend only on the difference in relative timestamps and relative positions. 
Additionally, there is no further need to calculate the query and key matrices in these two channels. 
To circumvent the intricacy of the model, we opt not to employ extra value matrices for the temporal and positional heads. Instead, they will share the value matrices with the semantics channel. 
The following approach is used to compute these matrices which is similar to multi-head self-attention:
\begin{align}
    \tilde{\mathbf x}^{l} &= \text{RMSN}(\mathbf x^{l-1}) \\
    \mathbf q^{l} = \phi(\tilde{\mathbf x}^l\mathbf W_{q}^{l}),
    \mathbf k^{l} &= \phi(\tilde{\mathbf x}^l\mathbf W_{k}^{l}), 
    \mathbf v^{l} = \phi(\tilde{\mathbf x}^l\mathbf W_{h}^{l})
\end{align}
where $ \mathbf W_{q}^{l} \in \mathbb R^{d \times d_h}, \mathbf W_{k}^{l} \in \mathbb R^{d\times d_h}, \mathbf W_{v}^{l} \in \mathbb R^{d \times d_h} $ are the learnable parameters. 
RMSN denotes the root mean square (RMS) layer normalization operation \cite{DBLP:conf/nips/ZhangS19a}.
$\phi$ provides nonlinearity which we employ SiLU \cite{elfwing2018silu} here, and $ d_h $ represents the size of each head. The following describes the method for calculating the attention weights for semantic, temporal, and positional channels separately:
\begin{align}
    \mathbf a^{l}_h = \frac{1}{n}\phi(\mathbf q^{l}(\mathbf k^{l})^T), 
    (\mathbf a^{l}_t)_{i,j} = \alpha(t_j - t_i),     (\mathbf a^{l}_p)_{i,j} = \mathbf \beta_{j - i}
\end{align}
where, $\phi$ supplies nonlinearity, and we leverage SiLU once again. Previous studies have demonstrated that the use of SiLU function in self-attention layers outperforms softmax in sequence recommendation tasks \cite{zhai2024actions}. The term $\alpha(t_j - t_i)$ represents the mapping of the difference in timestamps into buckets, where each bucket is associated with a learnable parameter \cite{raffel2020t5}. On the other hand, \(\mathbf{\beta} \in \mathbf{R}^n\) denotes a vector of learnable parameters.

Subsequent to the computation of outputs from the channels, these outputs are concatenated and subjected to RMS layer normalization. 
Following this, the normalized result is element-wise multiplied with the matrix $U$, which is derived from $\tilde{x}^l$. 
The process is encapsulated by the following formula:
\begin{align}
    \mathbf{h}^l &= \text{RMSN}(\text{concat}(\mathbf a^{l}_h \mathbf v^{l}_h, \mathbf a^{l}_p \mathbf v^{l}_p, \mathbf a^{l}_t \mathbf v^{l}_t)) \otimes \phi(\mathbf x^l \mathbf W_u^l)
\end{align}
here $\mathbf W_u^l \in \mathbb R^{d\times 3d_h}$ denotes learnable parameters and $\phi$ denotes SiLU function. We adopted the design of the matrix $U$ in our architecture following HSTU \cite{zhai2024actions} to introduce explicit 2-order interactions. 
For simplicity and clarity, we describe the case with a single head in each channel here. 
However, this approach can be easily extended to multiple heads within each channel, similar to the multi-head self-attention \cite{transformer}.

\subsubsection{Multi-stage Feed-Forward Network} The MFFN encompasses two distinct stages as depicted in Figure \ref{fig:structure-mffn}. 
In the first stage, the outputs from different channels are fused with the original input of the current layer. Subsequently, in the second stage, implicit feature interactions are conducted.Â·
\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{-10pt}
    \includegraphics[width=0.5\linewidth]{images/structure-mffn.pdf}
    \caption{Diagram of MFFN: Stage 1 fuses outputs from different channels; Stage 2 facilitates implicit feature interactions.}
    \label{fig:structure-mffn}
\end{figure}

In the first stage, MFFN receives the outputs across different channels from the AMS layer and applies a projection transformation characterized by learnable parameters $W_o \in \mathbb R^{3d_h \times d}$. The output of this stage is obtained by combining the projected output with the input of current layer $\mathbf x^l$.
\begin{align}
    \mathbf o^{l} &= \mathbf{h}^l\textbf W_o^{l} + \mathbf x^{l-1}
\end{align}

In the second stage, the primary objective of MFFN is to conduct  implicit interactions. Following LLaMa \cite{touvron2023llama}, we apply RMS layer normalization to the output of the previous stage and followed by a SwiGLU activation \cite{shazeer2020glu} to enhance feature learning and then adding the residual connection:
\begin{align}
\mathbf{x}^{l} &= \text{FFN}_l (\text{RMSN}(\mathbf{o}^{l})) + \mathbf{o}^{l} \\
\text{FFN}_l(\mathbf{x}) &= \text{SwiGLU}(\mathbf{x})\mathbf{W}_3^l = (\phi(\mathbf{x} \mathbf{W}_1^l) \otimes (\mathbf{x} \mathbf{W}_2^l)) \mathbf{W}_3^l
\end{align}
where $\phi$ represents SiLU, $\otimes$ denotes element-wise multiplication, and $\mathbf{W}_1^l \in \mathbb{R}^{d\times d_{FFN}}, \mathbf{W}_2^l \in \mathbb{R}^{d\times d_{FFN}}, \mathbf{W}_3^l \in \mathbb{R}^{d_{FFN}\times d}$ are learnable parameters. This configuration allows the network to effectively capture complex interactions within the data while maintaining efficient gradient flow through the residual connections.

\subsection{Prediction Layer \& Optimization objective}

After passing through $b$ layers of \textit{FuXi} blocks, each position has obtained sufficient information about the previously interacted items. We employ a multiplication with the transpose of the input embedding matrix, followed by a softmax function to obtain a probability distribution over predicted items. The transformation can be mathematically represented as follows:
\begin{shrinkeq}{-5px}
\begin{align}
    P\left(i_{t}^{(u)} = i \mid i_1^{(u)}, \dots, i_{t-1}^{(u)} \right) = softmax\left (\mathbf x^{b} \mathbf E^T   \right)_i
\end{align}
\end{shrinkeq}
In order to accelerate the training process, we adopt the sampled softmax loss with $N$ randomly sampled negative samples \cite{Klenitskiy_2023}.

\section{ANALYSIS}

\subsection{Space and Time Complexity}

\textbf{Space Complexity} 
Each \textit{FuXi} block comprises an AMS layer and an MFFN. The AMS layer features four projection matrices totaling $6d \times d_H$ parameters, alongside positional and temporal embeddings with $O(n + n_B)$ parameters, where $n_B$ is the number of buckets. The MFFN includes four projection matrices, amounting to $3d_h \times d + 3d_{FFN} \times d$ parameters. The item embeddings have $|\mathcal I| \times d$ parameters. Typically, $d_h$ and $d_{FFN}$ are proportional to $d$, and $n$ is comparable to $n_B$. Therefore, we assume $d_h = O(d)$, $d_{FFN} = O(d)$, and $n_B = O(n)$. \textit{FuXi}-$\alpha$ is formed by stacking $b$ \textit{FuXi} layers, leading to a total space complexity of $O(b(d^2 + n) + |\mathcal I|d)$.

\textbf{Time Complexity} 
The time complexity for computing attention weights in the semantics channel is $O(n^2d)$, compared to $O(n^2)$ in other channels. Calculating the QKV matrices and the MFFN both require $O(nd^2)$. The cost for generating predictions is $O(n|\mathcal I|d)$. Thus, the overall time complexity is $O(bn^2d+n(bd^2+|\mathcal I|d))$.

\subsection{Polynomial Approximation}

Next, we examine the properties of explicit inter-item interactions implemented by \textit{FuXi}-$\alpha$. To better analyze these interactions, we simplify the $l$-th layer of the \textit{FuXi} Block by treating attention weights as constants and omitting the second stage of the MFFN, activation functions, and most projection transformations. This simplification yields:
\begin{shrinkeq}{-5px}
\begin{align}
f_{block}^{(l)}(x_i; x_1, \cdots x_n) = x_i \circ \left( \sum_{j=1}^n a_{i,j}^{(l)}x_j \right) + x_i
\end{align}
\end{shrinkeq}
where the vectors $x_1, \ldots, x_n$ are the latent representations input to the $l$-th layer of the \textit{FuXi} block; $\circ$ denotes the interaction operator, such as element-wise multiplication; and $a_{i,j}^{(l)}$ are the attention weights in the $l$-th layer. In this section, let $x_{l, i}$ denote the output latent representation of the $i$-th item after the $l$-th layer. Let $F_n$ denote a polynomial of the form $\sum_{\boldsymbol{\alpha}} w_{\boldsymbol{\alpha}} \prod_{i} x_{0,i}^{\alpha_i}$, where the sum includes all terms satisfying $\sum \alpha_i \leq n$. We will use mathematical induction to show that $x_{b, i} = x_{i, 0} F_{2^b - 1}$.
\subsubsection{Base Case} Consider $b = 0$. Here, $x_{0,i} = x_{0,i} \cdot 1 = x_{0,i} \cdot F_0$, confirming the equation holds.
\subsubsection{Inductive Step} Assume the property holds for some integer $l \geq 0$. Now consider $b = l + 1$:
\begin{shrinkeq}{-3px}
\begin{align}
  x_{l+1, i} &= x_{l, i} \circ \sum_{j = 1}^n a_{i, j}^{(l+1)} x_{l, j} + x_{l, i} \\
  &= x_{0, i}F_{2^{l}-1} \circ \left (\sum_{j=1}^na_{i, j}^{(l+1)} x_{0, j} F_{2^{l}-1} + 1\right)
\end{align}
\end{shrinkeq}
For any term of the form $\prod_j x_{0,j}^{\alpha_i}$, where $1 \leq \sum \alpha_i \leq 2^{l+1}$, it appears in the expression $\sum_{j=1}^n a_{i,j}^{(l+1)} x_{0,j} F_{2^l-1}$. Thus, we have
\begin{shrinkeq}{-3px}
    \begin{align}
      \sum_{j=1}^n a_{i,j}^{(l+1)} x_{0,j} F_{2^l-1} + 1 = F_{2^l} 
    \end{align}
\end{shrinkeq}
Therefore, it follows that $x_{l+1,i} = x_{0,i} F_{2^{l+1} - 1}$.

Consequently, after progressing through $b$ layers of the \textit{FuXi} blocks, $x_{b, i}$ incorporates the outcome of feature interaction between $x_{0, i}$ and the result of interactions among all the items being of any degree up to $2^l - 1$.

\subsection{Analysis of AMS}

 The formulation of relative positional embeddings in the T5 architecture \cite{raffel2020t5} is delineated as follows.
The attention weights $\mathbf A = (a_{i, j})_{n\times n}$ can be computed by the process:
\begin{shrinkeq}{-5px}
    \begin{align}
        \mathbf A = \phi\left((\mathbf x \mathbf{W}_q)(\mathbf x \mathbf W_k)^T + \mathbf{B}\right)
    \end{align}
\end{shrinkeq}
where $\phi$ denotes a non-linear function, such as softmax or SiLU, and $\mathbf B = (b_{i, j})_{n\times n}$ denotes the matrix of the relative positional bias term. Let $q_i \in \mathbb R^{1 \times n}$ denotes the query vector of the $i$-th item, and $k_i, v_i, u_i$ denotes the key vector, the value vector, the vector used for Hadamard product respectively. 
The output of multi-head self-attention $o_i$ of the $i$-th item is then computed as:
\begin{shrinkeq}{-5px}
\begin{align}
    o_i &= W_o\left (\left (\sum a_{i,j} v_j \right)\otimes u_i \right) \\
    &\approx W_o \left (\left (\sum \phi_1(q_i k_j^T) V_j \right) \otimes u_i\right) +  W_o \left (\left (\sum \phi_2(b_{i,j}) v_j \right) \otimes u_i \right)
\end{align}
\end{shrinkeq}

On the other hand, in the AMS layer, the calculation process
is expressed as:
\begin{shrinkeq}{-5px}
\begin{align}
    o_i &= W_{o1}\left (\left (\sum \phi(q_i k_j^T) V_j \right) \otimes u_{i}^{(1)}\right) +  W_{o2} \left (\left(\sum b_{i,j} V_j \right) \otimes u_{i}^{(2)} \right)
\end{align}
\end{shrinkeq}
where $W_{o1}, W_{o2}$ denote the parameters in the first stage of the MFFN, and vectors $u_{i}^{(1)}$ and $u_{i}^{(2)}$ correspond to the $u_i$ vectors within the semantics and positional channels, respectively.
This demonstrates that the AMS layer facilitates a more expressive representation of positional and temporal information compared to the direct addition of attention weights, suggesting an enhancement in the model's capacity to leverage the temporal and positional information.

\subsection{Relationship with Existing Models}

Our work shares structural similarities with three models: SASRec \cite{kang2018self}, LLaMa \cite{dubey2024llama}, and HSTU \cite{zhai2024actions}. Here, we highlight the key differences between these models and our approach.

\subsubsection{SASRec and LLaMa} Unlike SASRec and LLaMa, which employ standard NLP architectures for recommendation systems, our model introduces two major innovations. First, instead of the traditional multi-head self-attention layer, we use the AMS layer to independently model temporal, positional, and semantic information, improving the model's feature utilization. Second, our model incorporates the MFFN, diverging from the FFN used in SASRec and LLaMa, by processing multi-channel information from the self-attention layer and enabling implicit feature interaction.
%through two stages: merging multi-channel data and facilitating feature interaction.

\subsubsection{HSTU} HSTU incorporates relative temporal and positional data by adding these features directly to attention weights, which can dilute their impact. Moreover, HSTU lacks an FFN layer, relying solely on self-attention and explicit feature interactions, limiting its ability to capture complex item relationships. Our model overcomes these limitations by decoupling temporal, positional, and semantic information within the self-attention layer and leveraging the MFFN to facilitate implicit interactions.


