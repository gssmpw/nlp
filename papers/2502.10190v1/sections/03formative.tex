\section{Formative study}
%In Human-AI video co-creation, we hypothesize that video creators will benefit from a system that presents multiple variations of AI-edited videos.\amy{<-- problem feels abstract, I wonder if we can concretely state what happens when you use AI and why we expect multiple versions to be helpful (e.g., multiple right answers for a high level prompt, other systems do this); I think the primary problem we want to lay out here is that comparison is non-trivial so that's why we're doing a formative study?} To understand the motivation and design considerations of such a system \amy{a bit abstract, I would make it clear that we're focusing on the challenges of comparing multiple versions early?}, 
To better understand how video creators work with alternatives and the support they need for comparing and managing them, we conducted a formative study with 8 professional video editors. The formative study consisted of semi-structured interviews and a comparison task with 3 videos.
%edited videos to explore current strategies and challenges with comparing videos.

\subsection{Method}
\ipstart{Participants}
We recruited 8 professional video creators using Upwork\footnote{\url{https://www.upwork.com/}} (P1-P8, \S{A} Table~\ref{tab:participants}). We compensated the participants with their self-set hourly rate (\$28, SD=5.3) for a 1-hour remote study conducted via Zoom. Participants had an average of 10.5 years of experience (SD=3.0) and created a wide variety of videos including commercials, interviews, documentaries, vlogs, and short-form videos.

\ipstart{Procedure}
% background interview, comparison task, exit interview
We started the study with demographic and background questions about the participants' video editing experiences and current approaches for reviewing and comparing videos.
In the comparison task, participants reviewed and compared 3 videos edited from the same source footage. We pre-uploaded videos to YouTube so that users could use a familiar interface that offered speed controls, hover thumbnails, and the video transcript. 
%, either from LLM-edited or expert-edited versions. 
To become familiar with the original version, participants first skimmed the source footage. Then, the participants reviewed the 3 edited versions for 30 minutes. 
% Participants were free to watch the videos and review the transcripts.
%we provided three edited videos and transcripts which participants reviewed and compared for 30 minutes. 
They were allowed to take notes to help them remember any details. At the end of the task, participants were asked to summarize the differences between the three videos and choose one they preferred, explaining their choice. We conducted a post-task interview to understand their strategies for comparing videos and any challenges they encountered.

\ipstart{Materials}
For the comparison task, we used 2 different source videos (F1: cooking tutorial~\cite{cooking_video}, F2: TED talk~\cite{ted_talk_video}).  We chose these videos aiming for source footage that was largely unedited and varied in duration (F1: 1hr 11min, F2: 12 min) and visual information (F1: featuring many objects, F2: primarily talking head).
We used 2 approaches to create multiple edited versions of the source video -- 1) with AI and 2) with experts. This allowed us to see how users work with variations created today with the current capabilities of AI video tools and in the future when AI capabilities improve.
For AI edits, we created 3 versions of the summary for each source video using an LLM to extract subsets of the video. 
Using the video transcript, we instructed GPT-4 to generate 3 summaries on the video topic (F1: How to make Eggs Benedict, F2: Talk highlights). 
%GPT-4 prompting (edited V1s: 152sec, SD=24; edited V2s: 283sec, SD=110) \amy{keep terminology similar: e.g., rough cut vs. edited V1}. 
%These videos only varied in the parts included.\amy{In the prior two sentences it is a bit hard to understand what is done to these videos. We might want to explicitly say that we used GPT-4 to select a subset of shots to include in the short video (i.e. extractive summarization) or something like this}
% To create variations manually, 
%To further explore the variations of end-to-end edited videos, 
We also hired 6 professional video editors to manually create 3 edited versions of F1 and F2~\footnote{See Supplementary Materials for examples.}.
%mira: removing this because it will be in table mentoned above (edited V1s: 451sec \& SD=152, edited V2s: 323sec \& SD=21).  
The video editors made a variety of editing decisions including trimming content to enhance focus, adding subtitles, B-roll footage, and motion graphics, incorporating text overlays and background music, and making audio enhancements. Each participant was given 3 edited versions from the same source video created with the same approach either with AI or manually. For F1, LLM edited videos were 152 seconds (SD=24) and expert edited videos were 451 seconds (SD=152). For F2, LLM edited videos were 283 seconds (SD=110) and expert edited videos were 323 seconds (SD=21).
%While tools like CapCut and OpusClips can generate edited videos, they only extract a single highlight moment. Thus, we sought to explore summary videos that extract and merge multiple parts to explore complex and comprehensive comparisons.


% analysis
\ipstart{Analysis}
To understand how participants explore alternatives, we analyzed the study recordings and participants' notes. We transcribed the interviews and participants' spontaneous comments during the task and grouped the transcript according to 1) current practices, 2) comparison strategies, and 3) opportunities and challenges of video editing via comparison. 

\subsection{Findings}
\ipstart{Current Practices}
Participants reported that decision-making via comparison is a common process throughout the video editing process. They mentioned having to review and select which clips to include from the footage provided by their clients (P1, P3-4, P7-P8), decide on the appropriate B-roll images (P2, P4), and experiment with different logo styles or placements (P5-P6).

Participants often had to create multiple versions of videos upon clients' requests (P1, P3-P8). 
This was because clients often found it challenging to imagine the final look of the videos from the editors' descriptions alone. P5 and P6 who created ad videos for marketing companies created two versions for companies' A/B testing to see which version is more effective. 
Clients often requested videos that have different lengths, storylines, intro versions, caption styles, or music styles. P3 and P4 mentioned that clients often liked different aspects of multiple versions so they had to combine the versions in further editing. 
While creating multiple versions helped clients narrow down and make decisions, it required a lot of time and effort from video editors. P5 said ~\textit{``I know they (clients) want to see three completely different videos, but I try not to because it takes forever. I only tweak small things that involve minor changes and can be done quickly.''}
As a workaround, participants created variations and got intermediate feedback in each stage (\textit{e.g.,} sharing multiple variations of scripts before moving on to the storyboard) so that they could minimize the amount of rework required later in the process (P1, P5, P8). Alternatively, P4 and P6 created multiple versions of the introductory part of a video instead of the full videos as introductions can capture the essence of what the rest of the videos will look like.
% + how do they review edits


\ipstart{Comparison Strategies - Watching Videos}
In the comparison task, participants watched the 3 versions of the edited videos to understand their differences. 5 participants watched each video from beginning to end before moving on to the next video (P1-P4, P6) while 3 participants switched to play other videos (P5, P7-P8). P7 first watched the introductions of all three videos because it was easier to remember and compare shorter segments and the introductions often captured the holistic style of the video. P5 and P8 switched to other versions of edited videos when they were not satisfied with the current version's edits (\textit{e.g.,} text effects) to check if other videos offered better alternatives. 
While reviewing edited videos, P5 revisited the original unedited video to check which part of the source video was missing. 4 participants re-watched the same video multiple times to identify parts they missed in the first review attempt (P3, P5-P6, P8). 

\ipstart{Comparison Strategies - Taking Notes}
7 participants took notes while reviewing edited videos to better recall the differences (P2-P8). They took notes on the content covered or omitted (\textit{e.g., yolk separation step is not included}), errors (\textit{e.g., mid-sentence jumps, flash frames}), what they liked or disliked about the edits applied (\textit{e.g., Nice intro music, Too many text effects}), how they would have edited differently (\textit{e.g., I would have emphasized the end results by showing it longer}), and how the video compared to other versions (\textit{e.g., This is better at showing the sequence of events compared to the first one}). 
Participants reviewing videos edited with LLM noted errors in the edits, such as sentence cut-offs and jump cuts.
P1 did not take any notes because pausing and taking notes would make the reviewing process much longer.

\ipstart{Challenges}
All participants mentioned the comparison process to be very time-consuming as they had to watch multiple videos to identify the differences. As a workaround, 5 participants sped up the video and skipped around the videos to find the edit points (P3-4, P6-P8).
Although the transcripts of the edited videos were provided, participants noted that simply reviewing the transcript or thumbnail previews on the timeline cannot fully replace the watching experience. P1 explained~\textit{``Simply reading the transcript isn't enough because the transcript might seem fine, but the visuals might have been messed up.''} P3 who checked for edit errors in the video noted that errors such as sentence cut-offs or jump cuts cannot be spotted using the transcript or by scrubbing through the thumbnails; she had to watch the entire video to identify them.

Comparing edited videos was also mentally demanding as participants had to compare multiple aspects of videos at the same time (\textit{e.g.,} storyline, visual effects, B-rolls, subtitles, music). 
As a result, participants often reviewed each version multiple times to focus on reviewing a single aspect at a time (P3, P5-P6, P8). P5 described ~\textit{``I cannot focus on multiple aspects at once while reviewing. When I’m checking the colors, I only look at colors.''}
When comparing the content, it was challenging to switch between multiple videos as they extracted different parts of the original video. For instance, P3 wanted to check if all videos included a specific cooking step but found it difficult as the step appeared in different timestamps across the videos. 

Participants mentioned that it requires a lot of effort to manually identify edits made throughout the video, take notes to remember details, and understand both high-level and low-level differences among videos from their notes. In the post-task interview, P5 mentioned that he wanted AI to describe the high-level theme or goal of each edited version to help him better understand what he was reviewing and what to expect. P6 wanted to have a list of all the edits made in each video so that he would not miss subtle edits (\textit{e.g.,} color correction, small text effects) and could quickly jump to those specific edit points for preview.
% Understanding high-level story differences and effectiveness will take hours (vs. identifying low-level edit differences)

% \mira{let's add something here about errors and the differences between those who saw the AI generated videos vs those who saw the manually created videos}

%\ipstart{Benefits of exploring variations}
%\mina{do we need this section?} \amy{I think it would be good to further motivate our approach if participants said anything interesting as it was sort of a novel task. I might wonder if participants saw any value in this exercise}
%removing for now since we are short on time. We could bring it back later as it makes sense

\subsection{Design Guidelines}
Video editors in our formative study currently make decisions by comparison throughout the video editing process and mentioned the benefits of having multiple variations of edited videos. We envision that in near future, Human-AI video co-creation tools will more commonly provide users with multiple variations. 
Our formative study reveals design guidelines (D1-D6) to better support video creation with alternatives. Our work aims to address D1-D4 and D6 while leaving D5 as a future work.


\begin{itemize}
    \item[\textbf{D1.}] \textbf{Minimize redundant watching by aligning variations} 
    Participants found it difficult to repeatedly watch overlapping sections of edited videos to identify edit points and differences. They wanted to compare edited variations with the original video and across alternatives but found it challenging with mismatched timelines. Aligning variations can simplify comparisons and reduce redundant viewing. 
    
    \item[\textbf{D2.}] \textbf{Support quick skimming by highlighting differences}
    Prior work explored comparison in text~\cite{reza2023abscribe, gero2024supporting} or images~\cite{huh2023genassist, almeda2024prompting}, but the temporal aspect of videos makes it challenging to skim them for comparison. This can be time-consuming and tedious as video lengths increase or as more videos are compared. A system that describes or visualizes the differences among edited videos can help users quickly make decisions.
    \item[\textbf{D3.}] \textbf{Enable independent comparison for different editing stages} Compared to traditional video editing where editors go through an interactive process of making sequential edits and reviewing, recent AI products such as CapCut or OpusClip speed up this process and generate fully edited videos. The resulting videos can be mentally demanding to compare, as users have to consider multiple aspects (\textit{e.g.,} rough cuts, subtitle styles) together. Enabling users to compare aspects independently can reduce the cognitive load.
    % Mentally demanding to do one at a time --> find related work on this
    % \item[\textbf{D4.}] \textbf{Provide structural anchors for alignment and navigation}\\
     \item[\textbf{D4.}] \textbf{Support comparison via multiple modalities} Relying on a single medium, like transcripts or thumbnails, is insufficient for comparing video variations. Edits in visuals or timing can be unnoticed when only text is reviewed. Supporting comparison through multiple modalities—such as transcripts, visual thumbnails, and audio—enables users to identify various types of differences in the most effective way and ensures a more thorough comparison of edits.
    % Multimodalitieis --> clarify what they are
    \item[\textbf{D5.}] \textbf{Support verification of edit suggestions} As AI-powered tools automate suggestions, prior work has explored verifying AI's recommendations~\cite{huh2023genassist, ferdowsi2024validating}. In video creation, automated edit suggestions can often lead to additional errors (\textit{e.g.,} trimming a clip leading to jump cut) and thus needs careful validation. Future work can explore automatically identifying and communicating such erroneous suggestions to users.
    \item[\textbf{D6.}] \textbf{Support management and customization of variations}
    Formative study participants who reviewed a small number of alternatives all mentioned that they wanted to further refine the edits as none of the versions fully aligned with their preferences. Scaling up the number of alternatives increases the chance of finding a preferred version but it also makes it more difficult to manage and select from a larger pool of variations.
    
    % \item[\textbf{D6.}] \textbf{Allow for flexible refinement and regeneration}\\

    % \mina{should we add a design goal about the organization? (our sort feature, pin, etc)}
    % agency, 
    
    % authoring perspective\\
    % solutions: further refine interactions, users can also export and make further edits\\
    % \item[\textbf{DX.}] \textbf{Support organization of multiple variations}\\
    % solutions: filter/sort, overview of design space\\
    % Downside: reduced control, distraction → Therefore, an evaluation mechanism (an ‘‘idea monitor’’) is needed to judge the appropriateness of generated responses
    % Need to check if the branches still align with our goals
    % \item[\textbf{DX.}] \textbf{Facilitate divergent and convergent thinking}\\
    % solutions: in-situ branches (alternatives), post-edit alternatives
\end{itemize}



