\section{VideoDiff}
We present \sysname{} (Figure~\ref{fig:teaser}), a human-AI video co-creation tool designed to support efficient video editing with alternatives.
With \sysname{}, users can generate and review diverse AI recommendations for 3 different video editing tasks: making rough cuts, inserting B-roll images, and adding text effects (D3).
%(Figure~\ref{fig:teaser}.1)
VideoDiff supports easy comparison between alternatives by aligning videos (D1) and highlighting differences using timeline and transcript views (D2, D4)
%(Figure~\ref{fig:teaser}.2)
. Users can organize and customize edits by sorting, refining, and regenerating AI suggestions (D6). %(Figure~\ref{fig:teaser}.3). 


\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/system_overview.pdf}
  \caption{Overview of VideoDiff: Users can view an outline of the variations in the current editing stage (a). In this figure, we see 10 rough cut variations. The user can play videos of these different versions (b) and compare them in the transcript or timeline view (c).
  Users can toggle between the edited and source timelines (d) to align videos to the source or edited context or click on each section to navigate directly to that part of the video (e).
  Users can also sort variations by duration and the number of sections included, as well as pin, archive, or edit variations according to their preferences (f).}\label{fig:sys_overview}
\end{figure*}


\subsection{Scope}\label{sec:scope}
\revised{
While VideoDiff provides visualizations that allow users to quickly identify and skim differences between video versions (D2), it is designed to go beyond the functionality of a \textit{diff tool}, which typically focuses solely on highlighting changes or differences for review~\cite{tharatipyakul2018towards, baker2024interaction}. Instead, we designed VideoDiff as a human-AI co-creation tool that facilitates iterative \textit{generate-compare-refine} interactions, revealing how comparing alternatives can influence and enhance usersâ€™ creative processes.}

To help set context for \sysname{}, we define the different stages of video editing.
Rough cut creation is selecting good moments (or clips) from source footage to create a compelling story. Inserting B-roll images or video helps to make the video more interesting and dynamic. To insert good B-roll, creators should find effective images or videos that illustrate what is being said and place them appropriately. Inserting text effects helps emphasize parts of the narration through animation and stylized text. The AI algorithms for each of these editing tasks are in themselves hard technical problems, and we do not focus on them in this work. Our focus is on supporting users to work with the generated variations as part of the editing process. However, because we must support some editing in order to test our ideas, we have implemented basic versions of editing algorithms that leverage LLMs to process the video transcript and recommend rough cuts, B-roll, and text effects. A holistic solution with multimodal analysis of video, audio, and narration can offer better editing suggestions in the future.

Most video editing softwares offer more than the three stages that VideoDiff supports, such as applying color correction or cleaning up the audio. Our goal is not to create a fully functional video editor, but rather to explore how video editing might change as AI technologies enter more and more of the editing stages. \camready{As AI easily generates multiple editing recommendations, the video editing task shifts to involve more curation beyond just editing, and we must consider how to best support this transition.}




\subsection{Interface}
VideoDiff is a web-based video editing tool (Figure~\ref{fig:sys_overview}) where users can generate, review, and customize video alternatives. 
When the user uploads a video, \sysname{} first generates 10 rough cut recommendations. In the \textit{versions outline} on the left, users can see the list of all variations in the current editing stage (Figure~\ref{fig:sys_overview}.a). Users can play the video of each version (Figure~\ref{fig:sys_overview}.b) or skim the differences using a timeline or transcript view (Figure~\ref{fig:sys_overview}.c).  Users can toggle between showing only the edited or all of the source content for additional context (Figure~\ref{fig:sys_overview}.d).  They can also click on any section to navigate directly to that part (Figure~\ref{fig:sys_overview}.e).
%or switch to transcript view to compare the variation side by side for a more detailed comparison (Figure~\ref{fig:sys_overview}.E). With the \textit{variation control}, 
%
Users can sort, re-order, pin and archive the variations to organize them (Figure~\ref{fig:sys_overview}.f). Or they can refine or recombine existing variations or regenerate a new variation using text prompts. 
%mira: this is already written down in the organizing section so removing from up here so it's not repetitive
%Sorting varies by editing stage. In the rough cut stage, users can sort by the duration and number of sections. In the B-roll editing stage, users can sort by the number and style of the images. In the text effects stage they can sort by the number and style of the text effects. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/timelines_new.jpg}
  \caption{At each editing stage, VideoDiff provides glanceable timelines for users to easily compare different variations. Users can click on any section, B-roll image, or text effect to jump to that part of the video and preview the effects.}\label{fig:timelines}
\end{figure}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/edited_source_final.jpg}
  \caption{Users can switch between the edited and original source timeline in the transcript (a) and timeline (b) views. This helps users see the edits in the context of the source content and compare which sections are included at a glance. The source view (c) shows users the location of the edited content in the context of the source view.}\label{fig:extracted_original}
\end{figure*}


\ipstart{Comparing Variations with Timeline View}
VideoDiff provides different visualizations of timelines at each video editing stage (Figure\ref{fig:timelines}) so that users can quickly review and compare multiple alternatives (D3). 
In the rough cut stage, VideoDiff uses ~\textit{sections} to visualize the timing and coverage of content in each variation.
Drawing upon prior work that has shown that grouping footage into thematically coherent chunks can help creators make video editing decisions~\cite{leake2020generating, huh2023avscript}, we explore how chunking into sections can aid comparison of edited videos. 
Instead of segmenting each rough cut variation to identify sections, VideoDiff extracts sections from the source footage and applies them consistently across all variations. This allows users to easily align the variations and see how different versions include or exclude certain sections and cover varying parts or lengths of each (D1-D2). 
Users can toggle between edited and source timeline views (Figure~\ref{fig:extracted_original}.a) to align the videos based on the edited or source video timeline. The edited timeline allows users to quickly skim the video's overall duration, along with the placement and proportion of each section. By hovering over a section, users can view relevant video thumbnails and click to play the video from that point.
With the source timeline, users can easily understand which parts of the source video are extracted. In the formative study, video creators often compared edited videos to the source videos to verify any missing key information. Drawing from this observation and aligning with D1, we use the source video as an anchor for aligning rough cut variations for comparison. 
% \mira{it's great to include this example but this text doesn't align with the figure. }
For example in Figure~\ref{fig:extracted_original}.b, while the edited timeline shows that both versions have a similar duration for ``Campus Highlights'' section, the source timeline reveals that each version covers different parts of the source footage on ``Campus Highlights''. By hovering over the lighter-colored boxes indicating excluded parts, users can see which video thumbnails and topic keywords are not covered.



Once the user chooses a rough-cut version, they can generate 10 videos with different B-roll recommendations. In the B-roll stage, VideoDiff's timeline view shows the B-roll thumbnails on top of the rough cut timeline bar, along with the keyword used to search for the B-rolls (Figure~\ref{fig:timelines}). Users can hover over the B-roll thumbnails to see which video scene each B-roll covers and click thumbnails to play the video and preview how the B-roll is inserted into the footage. After selecting a B-roll version, users can generate 10 new videos with different text effect suggestions. Instead of showing video thumbnails with text effects, we display the keywords where the text effects are applied, as text effects on thumbnails are too small to skim.
Users can hover over a keyword to view the narration sentence for context, and click on the keyword to play the video and preview how the text effects are integrated into the footage.




\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/transcripts_new.jpg}
  \caption{At each editing stage, VideoDiff provides glanceable transcripts so that users can easily review and compare different variations. In rough cut transcripts, visually concrete keywords~\cite{leake2020generating} are emphasized in bold, allowing users to easily skim through the content of each variation. Users can click on section headings, B-roll images, or text effects to jump to that part of the video and preview the effects.}\label{fig:transcripts}
\end{figure}

\ipstart{Comparing Variations with Transcript View}
VideoDiff's transcript view allows users to quickly skim the transcripts to understand the differences of variations in each editing stage (Figure~\ref{fig:transcripts}). 
The transcript is divided into sections and users can click on the section headings to play the video from the starting point of each section.
\camready{During the rough cut phase, VideoDiff aids users in maintaining context while toggling between transcript and timeline views by showing a mini timeline within the transcript view. It also uses consistent color coding to denote sections across both views.}
In the transcript, visually concrete keywords~\cite{leake2017computational} are emphasized in bold, allowing users to skim through the content of each variation~\footnote{We used GPT-o to identify visually concrete keywords with few-shot examples.}.
Similar to the timeline view, users can also switch to the source transcript view (Figure~\ref{fig:extracted_original}.c) and see the complete transcript and identify which parts of the source text are excluded in each section. We synchronize the scrolling across multiple transcripts, allowing users to easily compare variations side-by-side (D1). 

For quick skimming of B-roll and text effects options, VideoDiff highlights sentences in the transcript where these effects are applied (D2, Figure~\ref{fig:transcripts}). Users can click on the B-roll thumbnails or text effects to preview the video from the moment the effect appears.

% \mira{why only show the visual concrete words in the rough cut version?}

\ipstart{Organizing Variations}
To help users to easily explore and narrow down the search space of alternatives, VideoDiff supports sorting of variations in each stage. In the rough cut stage, users can sort based on the edited video duration and the number of sections, and in the B-roll and text effects stages, users can sort based on the number of effects and the media type of B-rolls (image, illustration, video) or style of text effects (title, subtitle, lower thirds). 
Additionally, users can manually pin preferred versions to the top or archive unwanted versions to the bottom, which is also reflected in the outline view.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/refine.jpg}
  \caption{Using VideoDiff, users can edit a variation, recombine multiple variations, and generate a new variation using text prompts. For each new generation, VideoDiff summarizes the changes so that users can easily verify the result.}\label{fig:refine}
\end{figure}
    
\ipstart{Customizing Variations}
When users are not satisfied with the initial recommendations of VideoDiff, they can further edit existing versions or generate new alternatives (D6, Figure~\ref{fig:refine}).
Users can provide a text prompt to guide the generation of new alternatives (\textit{e.g., Show many text effects when I talk about grocery items.}), or click \textit{``Surprise me''} to have VideoDiff suggest a new alternative that is different from existing variations. 
Users can also recombine existing versions by specifying the version IDs and how to merge them in the text prompt (\textit{e.g., Use first two B-roll images from \#3 and last B-roll image from \#7.}). By default, generated new results are pinned to the top for easy discovery.
Using VideoDiff, users can also edit existing versions with a prompt (\textit{e.g., Shorten the part when I'm talking about the meal plans.}). The new generation from the edit prompt is displayed right below the original version for quick comparison and VideoDiff also describes specific changes made for quick verification of edits (D2). For example, in Figure~\ref{fig:refine} the user tells VideoDiff to ``shorten the part about dining halls'' and the system responds:
``Shortened the description of dining halls within the Dining and Housing section''. 
% \mira{how do you do this part with GPT?}




\subsection{Implementation \& Prompt Engineering}
We implemented VideoDiff using React.js and d3.js. For embedding a video player, we used Remotion~\cite{remotion} to render the edited video and overlay B-rolls and text effects. 
% Figure~\ref{fig:pipeline} illustrates the pipeline of VideoDiff.
When users upload a video, VideoDiff uses OpenAI's Whisper API to transcribe the video. VideoDiff is powered by OpenAI's GPT-4o and uses prompt engineering for 1) segmenting video into sections and identifying visually concrete keywords, 2) generating multiple alternatives of edit recommendations for rough cuts, B-rolls, and text effects, and 3) parsing and executing users' new generation prompts and summarizing changes. 
To ensure VideoDiff provides diverse edit recommendations in each stage, we use ~\textit{augmentation prompts} (\S\ref{apndx:augmentation_pipeline}) to control the generation of suggestion (\textit{e.g.,} by specifying duration and section coverage for each rough cut recommendation). 
%\mina{can we share the prompts in the appendix? Should ask Mira and Ding}
% - why we use them? -> to explore variable outputs, prior work utilized high-temperature, but it is difficult to describe their differences and adhere to user query
% - Luminate used dimensions to generate variations with prompting
