\section{Discussion}
In this section, we reflect on our findings from the design and evaluation of VideoDiff. We also discuss future opportunities to improve and extend VideoDiff. 

\ipstart{Creating Videos with Alternatives}
VideoDiff is a tool designed to support human-AI co-creation of video with alternatives. Our focus on alternatives is driven by prior work that highlights the value of divergent thinking in creative process~\cite{suh2024luminate, dow2010parallel, reza2023abscribe}. Video creators in our user study highlighted 3 benefits of having alternatives. First, it helped to consider alternative edit styles that they are not familiar with. Second, when users give the AI an ambiguous edit request, having multiple alternatives allows them to choose the one that best matches their intent, rather than relying on a single suggestion from the AI that might be incorrect. Third, with alternatives, creators can quickly prototype multiple versions of end-to-end videos adapted to different audiences, settings, and platforms. 
\revised{While editing videos with alternatives is not a one-size-fits-all solution, it represents existing practices in video editing and other generative AI tasks. In this work, we focus on minimizing the burden of working with alternatives in video creation and demonstrate how VideoDiff achieves it.
Still, some users may prefer incremental editing approaches, such as refining a single video~\cite{wang2024lave}, or workflows that prioritize error correction over creative decision-making~\cite{huh2023avscript}. In the future, we will consider diverse workflows by letting users select the number of suggestions (one vs many) and further explore how the scale of alternatives or the timing of providing alternatives can impact users' creation process.
}


\ipstart{\revised{Scope and Use Cases of VideoDiff}}
\revised{We reflect on the scope of VideoDiff based on user groups, creation stage, and control granularity based on the taxonomy of roles that creativity support tools play~\cite{chung2021intersection}. 
First, VideoDiff can support a diverse range of users. Novices or creators with accessibility needs (\textit{e.g.,} screen reader users) who find existing video editing software complicated to use can benefit from VideoDiff’s suggestions. Novices with limited experience with the authoring tool often do not know where to start~\cite{ashtari2020creating} and find it easier to start with examples rather than from scratch~\cite{kim2015motif}. 
For experienced editors, VideoDiff reduces the time spent on routine tasks such as planning and making rough cuts, allowing them to focus on fine-grained adjustments that improve the overall quality of their work.}

\revised{In terms of the creation stage, VideoDiff primarily supports idea generation and execution but not evaluation, requiring users to review and evaluate the quality. While automating execution, VideoDiff still emphasizes user control, allowing for post-editing and customization through natural-language prompts.
In terms of control, VideoDiff provides indirect control (\textit{i.e.,} text prompt) to prioritize a low learning curve over precise editing. This design choice contrasts with traditional video editing software, which offers precise but complex editing capabilities. Thus, it will be more useful in cases when users want to create videos under time constraints (\textit{e.g.,} social media content, event recaps) or budget limits (\textit{e.g.,} personal projects or prototyping for feedback before detailed editing). While users can export the EDL to further edit in other video editing tools, future VideoDiff can incorporate more direct control (\textit{e.g.,} timeline manipulation, dragging B-roll to adjust timing).}

% \revised{With this scope, VideoDiff is ideal for scenarios requiring quick drafts or iterative edits, such as creating social media content or event recaps under time constraints, producing personal or casual projects with limited budgets, or prototyping video ideas to gather feedback before detailed editing. It bridges the ease of natural-language interaction with the precision of traditional editing tools, addressing diverse use cases and user needs.}


\ipstart{Support for Flexible Comparison}
VideoDiff enables quick comparison of video alternatives by supporting users to align multiple videos and highlighting differences using timeline and transcript views. 
While creators using VideoDiff were successful with various comparison tasks, 2 participants were confused about which views to utilize for a few comparison tasks. 
To further minimize creators' mental load in search and comparison, a future iteration of VideoDiff can directly take creators' natural language comparison queries (\textit{e.g., Which video has fewer narration mistakes in the intro of the video?}). It can extract relevant parts of each video and determine the best views to communicate the differences and present them to creators. This can further reduce creators' effort to switch views and help scale up the number of variations for comparison.
Also, VideoDiff's pipeline can consider broader types of comparison beyond what is based on narration and visual content -- shot types (\textit{e.g., Which video features a higher proportion of a talking head?}), audio (\textit{e.g., Which videos use calm music?''}, errors (\textit{e.g., Which video has the least sentence cut-offs?}), or abstract comparison queries (\textit{e.g., Which video has a more hooking intro?}) 
\camready{Also, while D4 highlights comparison through multiple modalities, current VideoDiff focuses on visual and narration differences and does not consider sound or music differences. Prior work has explored audio tagging of videos~\cite{zhang2023peanut} and generating sound effects to augment video skimming~\cite{ning2024spica}. Similarly, future VideoDiff can consider audio-based comparison by surfacing differences in background noises, music inserted, or speech characteristics.}
As the pipeline evolves to identify and describe these various types of differences, finding effective ways to visualize them becomes another important challenge.


\ipstart{Implications of VideoDiff on Creativity}
% \ipstart{Implications of Quick, Parallel Video Authoring}
\revised{
While AI-powered creativity support tools risk design fixation~\cite{wadinambiarachchi2024effects} and lack of control~\cite{suh2024luminate}, we designed VideoDiff to mitigate these concerns. First, VideoDiff supports divergent thinking. Participants in our study mentioned that it is easy to get fixated on one video editing style with existing tools, whereas VideoDiff inspired them to explore alternative ideas, which they found valuable to incorporate.
Second, VideoDiff supports customization. Instead of generating a single result for users to accept as-is, it offers multiple options, enabling users to select and refine suggestions. By not delegating all edit decisions to AI, VideoDiff ensures creators remain in control of the final result. Finally, VideoDiff promotes efficiency by automating tedious edits, allowing creators to spend more time in the creative stages.}

\revised{To further support a deeper sense of agency and ownership, future iterations of VideoDiff can involve users in the planning and ideation stage~\cite{chen2019neural} and take user inputs before generating suggestions. For instance, users could specify requirements such as duration or style, provide text-based instructions, or upload reference videos to guide the editing process. They could also upload a partially edited video—for example, one with the first half completed—and request the system to finalize the remaining edits.
To support more flexible user control, future VideoDiff can allow users to directly accept or reject individual suggestions as in prior works ~\cite{laban2024beyond, han2020textlets}. To improve the transparency of AI suggestions, we can provide explanations for the generated suggestions~\cite{wan2024felt} or offer access to intermediate steps of LLM chaining used in generating them~\cite{wu2022ai}. This can enable users to better understand, utilize, and experiment with the tool~\cite{yeh2024ghostwriter}. 
Furthermore, future work can consider a personalized recommendation pipeline within VideoDiff, where the tool adapts to a user's unique style and preferences over time. This would not only speed up the editing process but also give creators a more tailored experience.} 
% not delegating everything to AI
% VideoDiff can mitigate these by supporting divergent thinking and letting users select and customize the edit suggestions flexibly. Video creators in our study highlighted that VideoDiff helped them be more creative by encouraging them to consider alternative ideas and allowing them to spend more time on the creative stage as they save time reviewing AI edits. 

As AI continues to accelerate the video creation process, there is also a risk that users may not thoroughly review or verify all the generated content. VideoDiff can help mitigate this by encouraging creators to do more sensemaking before making final selections. By offering clear visualizations of differences and allowing users to explore how each version fits their specific goals or audience, VideoDiff can assist creators in making more informed decisions.




\ipstart{\revised{Environmental and Societal Impact}}
\camready{
% To ensure that AI is developed and deployed responsibly, researchers investigated potential ethical issues including privacy~\cite{gupta2023chatgpt}, fairness~\cite{blodgett2020language}, toxicity~\cite{fortuna2018survey}, misinformation~\cite{weidinger2022taxonomy, huh2024long}, and computational cost~\cite{weidinger2022taxonomy}. In the long term, generative AI may also lead to potential challenges such as environmental impacts and job displacement.
We discuss the long-term impact of AI-powered creativity tools such as VideoDiff on the environment and society.
% We acknowledge that all tools leveraging GenAI, including ~\sysname{}, are not free from environmental and socioeconomic concerns. 
Like other AI tools, the language model that powers VideoDiff requires computational resources for training and inference which involves carbon emissions~\cite{weidinger2022taxonomy}. 
Yet, our study demonstrates that VideoDiff is highly valuable to users by streamlining complex editing tasks and enabling creative exploration. Also, VideoDiff may reduce the time users spend on traditional editing workflows, which often rely on resource-intensive video editing software and involv repeatedly rendering, storing, and transferring multiple large video files via cloud services. Instead, VideoDiff represents edit variations as structured text, eliminating the need to store multiple edited versions as full video files. We expect the financial and environmental costs of the models to change over time, and future work can explore how well smaller models can support tasks of suggesting video edits.}
% This time efficiency can result in lower overall energy consumption at the user level, offsetting some of the environmental impact associated with generative AI. 
% VideoDiff's task-specific generation and partial edits also help avoid unnecessary generations.

\revised{Generative AI tools like VideoDiff also raise concerns about job displacement in fields that rely heavily on manual video editing and production. However, VideoDiff does not automate the entire video editing process and retains key editing decisions with users who select and refine the suggestions. For novices, VideoDiff can lower the entry barrier to video editing and open doors for new roles. For experienced users, it reduces the time spent on tedious, repetitive tasks, allowing them to focus more on higher-order creative endeavors.
By fostering both accessibility and efficiency, VideoDiff can promote collaboration between humans and AI in a way that creates opportunities.}
% However, VideoDiff is designed to empower users across skill levels: it enables novices without much experience to create and edit videos effectively, lowering the barrier to entry for creative tasks. For experienced users, it reduces the time spent on tedious, repetitive tasks, allowing them to focus more on higher-order creative endeavors. By fostering both accessibility and efficiency, VideoDiff can promote collaboration between humans and AI in a way that aligns with the broader goals of responsible AI.

% \ipstart{Extending VideoDiff}
% % \ipstart{Parallel authoring}

% % \ipstart{Human-Human collaboration}
% tracking, 
% VideoDiff in Human-Human collaboration or version control setting

% % \ipstart{Beyond Video Editing}
% Video Diff in Video generation, 
% music editing --> requires novel consideration because cannot be conveyed with visual or text

% % \ipstart{Consumers curating or choosing video}
% pros: even consumers can edit the original content to their taste, 
% consumption scenarios choosing what I need from multiple videos
% (Prior work: comparing educational videos)


\ipstart{Limitations}
VideoDiff demonstrates the usefulness of video creation with alternatives in 3 representative video editing tasks: making rough cuts, inserting B-rolls, and adding text effects. 
Our study participants expressed excitement to use VideoDiff for a wider range of edits (\textit{e.g.,} motion graphics, and transitions). Future versions of VideoDiff could incorporate more diverse editing types.

While VideoDiff currently takes a single source video as input, study participants mentioned that VideoDiff will be useful for generating and comparing alternative curations of multiple clips when provided with a huge library of footage. To support this, we need to explore ways for users to efficiently skim through the coverage and sequence of different clips, while also accounting for the possibility that a single clip may appear multiple times in the final edit.

VideoDiff is powered by an LLM (GPT-4o) to generate edit recommendations and to follow users' commands to generate new variations. Our pipeline is prone to transcription errors and LLM hallucinations as few study participants have pointed out. GPT-4o also has a limit for input tokens (128k) and cannot process long videos with extensive length of transcripts.
Also, the current pipeline does not consider visual input when generating edit recommendations and thus cannot correctly follow edit requests related to the visual content. In the future, large multimodal models that take visual input (\textit{e.g.,} GPT-4V) can be integrated to improve the performance of the pipeline. 
