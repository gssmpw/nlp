\section{Related work}

\subsection{Human-AI video co-creation}
% \mira{I agree with Amy's comments for this subsection. The message of this section is not clear. What do you want the reader to get out of it? Right now you are listing a bunch of systems but you are not telling me why I (the reader) need to know about these systems. How are they related to VideoDiff? You don't have to tell me about the details of each one unless they are related. If your point is - AI has been part of video editing tools for a long time (cite, cite, cite, cite) but no one has looked at editing by building an experience for working with multiple variations. Or if someone has looked at it than say - X looked at the same problem, but we are different because Y.  }

% \amy{I think the thesis of this part is that there is something fundamentally different from traditional editing compared to this new "human-AI co-creation" and because of that we have to think about diffs for some reason. Ideally the high level difference between these two categories would be a more clear early on!}
Traditional video editing tools such as Premiere Pro~\cite{PremierePro} or iMovie~\cite{iMovie} require users to manually inspect the video, plan for what edits to make, and execute them by interacting with the complex UI in video editing tools. With the power of AI to understand the content and quality of videos, new forms of video editing systems have been introduced that speed up and automate the video creation process~\cite{invideo, opus, capcut, Descript}. 
Earlier systems supported users to structure narrated videos into multiple segments to help them quickly skim and select appropriate parts~\cite{truong2016quickcut, truong2019tool, wang2019write, leake2017computational, huh2023avscript}.
% \amy{<- are these the AI related systems you're talking about? might want to make that clear here and in the next sentence that there is automation}
These systems label clips with metadata such as speaker and topic to let users navigate and search~\cite{truong2019tool, huh2023avscript}, or recommend video segments that are semantically relevant to the script~\cite{truong2016quickcut, wang2019write, xia2020crosscast, leake2020generating}.
Beyond organization of clips, researchers have also proposed tools that recommend edits based on the video content and quality. B-Script~\cite{huber2019b} analyzes the video transcript and recommends possible B-roll positions and clips. 
AVscript~\cite{huh2023avscript} detects low-quality footage (\textit{e.g.,} blur, bad lighting) to cut out, and commercial tools such as Descript~\cite{Descript} and Vimeo~\cite{Vimeo} identify filler words and pauses for quick removal. 
Beyond post-editing, prior research has also proposed automatic video creation from various media including text documents~\cite{chi2022synthesis}, webpage~\cite{chi2020automatic}, markdown~\cite{chi2021automatic}, news articles~\cite{wang2024reelframer}, and music~\cite{liu2023generative}. 
% \amy{not quite clear to me how this is different than post-editing. what is happening in these works?} 

More recent tools proposed LLM-powered video editing tools that are capable of understanding users' editing intent in natural language and automatically executing them. ExpressEdit~\cite{tilekbay2024expressedit} allows users to edit videos by typing edit instructions and sketching on the video frame.
% \amy{<- might want to clarify what types of edits this does/does not enable. what's an example edit you can use?} 
LAVE~\cite{wang2024lave} provides an LLM-agent that assists users in planning and performing edit actions while also enabling users to manually refine agent actions. \
% amy{hmm it is a little hard to get a sense of what this does or does not support} 
To support the task of repurposing long videos into short and compelling videos, PodReels~\cite{wang2024podreels} and ROPE~\cite{wang2022record} explore the use of LLMs to create podcast summaries. 
% \amy{are these extractive summaries? it is useful to know the details of how they will be similar or not to tyour work}

Existing Human-AI video co-creation tools already create multiple variations for users to better explore the design space. For instance, ReelFramer~\cite{wang2024reelframer} suggests multiple narrative framing to let users explore and choose video topics, and ROPE~\cite{wang2022record} generates summaries of variable lengths. However, the new task of how users review, refine, and select among these variations of videos remains unexplored. 
% \amy{<- it is underexplored but I think it is not quite clear why it is a problem and how the prior work didn't solve it already? what is the issue that still exists here?}
In this era of AI-driven video creation, we investigate user challenges in reviewing and comparing AI-edited videos.


\subsection{Video summarization, skimming, and browsing}
% \mira{my feedback for this subsection is similar to the subsection above. We need to  say more than just "we build upon prior research. Are the specifics of our browing and skimming techniques similar? Are they novel? Do we leverage existing techniques? What is novel? Remember that the goal of this section is to make it clear to our reviewers what is novel as compared to prior work?}

Understanding and browsing video content is an important first step in video editing. Traditional video players commonly use a ~\textit{seek bar} which indicates the progress of the video and allows viewers to navigate to different parts of the video. To address the difficulty of previewing upcoming scenes, many video players on the web including YouTube provide picture-in-picture previews of thumbnails when viewers hover over the seek bar and video chapters that segment video into multiple sections and add context to each portion for quick skimming.

There is a large body of work that summarizes a video as images by selecting the important keyframes~\cite{truong2016quickcut, chang2021rubyslippers, barnes2010video, boreczky2000interactive, jin2017elasticplay}. 
Video editing tools such as Premiere Pro and iMovie utilize ~\textit{filmstrip}, a visual representation of a video's frames displayed in a linear sequence. This format allows editors to get a quick overview of the entire video at a glance and identify parts for cutting or inserting effects. To enable seamless zoom over filmstrip, Video Tapestries eliminated borders between frames for spatial continuity~\cite{barnes2010video}. 
Other works used grid-view of keyframes to summarize the videos~\cite{boreczky2000interactive, jin2017elasticplay}. Boreczkly et al. proposed a comic book presentation for videos that displayed keyframes with different sizes based on their importance~\cite{boreczky2000interactive} and ElasticPlay visualizes an interactive summary based on dynamic time budget using a grid of thumbnails~\cite{jin2017elasticplay}.

Researchers have also explored video browsing interfaces that leverage video transcripts. To support the navigation of informational videos, these systems highlight keywords in the transcript~\cite{chang2021rubyslippers}, or provide scaffolding steps~\cite{yang2022softvideo, fraser2020temporal} and concepts~\cite{liu2018conceptscape}.
Building upon prior research on video summarization and browsing techniques, our work explores visual representations to support efficient review of multiple edited videos. 

% - based on user viewing history
%     - Data-driven interaction techniques for improving navigation of educational videos
%     - catchlive


\subsection{Comparison and sensemaking}
\camready{
Comparison is crucial in creative tasks, helping users understand the design space and refine their decisions. Variation theory~\cite{marton2014necessary} suggests that learning occurs through experiencing contrasts. Thus, exposing creators to alternatives can help them understand model affordances and make more informed decisions.~\cite{gebreegziabher2024supporting}}

To visualize comparison, 3 main techniques are used: juxtaposition, superposition, and explicit encoding~\cite{gleicher2011visual}.
\textit{Juxtaposition} places distinct visual elements side-by-side to highlight similarities and differences. \textit{Superposition} overlays visuals within a common frame and  \textit{Explicit encoding} reveals the predefined relationship between visualizations. 
Prior work utilized these techniques for video comparison by building sequential and parallel video players~\cite{baker2024interaction, tharatipyakul2018towards}, and highlighting the subtle differences with color overlay~\cite{balakrishnan2015video, baker2024interaction}. 
To support the search of educational videos, VSedu supports video comparison based on topic relevance~\cite{benedetto2024visual}. Video Lens facilitated the search in a large collection of sports videos via rapid playback and event-based search~\cite{matejka2014video}. This line of work extends information visualization techniques to support the comparison of videos from multiple sources. Yet, they do not support video comparison in the editing context, where the comparison involves multiple variations of edited videos from the same source footage. 

\revised{As AI continues to speed up and automate content generation, it is becoming more common to work with multiple variations in parallel in creative tasks including writing~\cite{reza2023abscribe, suh2024luminate, buschek2021impact}, image generation~\cite{dalle3, midjourney, huh2023genassist}, video generation~\cite{sora}, and video editing~\cite{opus, capcut}. While facilitating divergent thinking and decision-making, this new workflow of working with variations demands an additional stage of comparison. This has led researchers to explore ways to compare and understand AI-generated content.} 
Tools have provided multiple levels of abstraction to support users in managing complex information generated by AI~\cite{suh2023sensecape, liu2024selenite}.
\camready{To facilitate collaboration with AI in creative tasks, researchers explored sensemaking and comparison of writing variations~\cite{reza2023abscribe, suh2024luminate, koch2014varifocalreader}, images~\cite{huh2023genassist, almeda2024prompting, koch2020imagesense, brade2023promptify}, slides~\cite{drucker2006comparing}, storyboards~\cite{benedetto2024visual}, and designs~\cite{matejka2018dream}. However, video presents unique challenges due to the temporal nature and multi-modal complexity. We contribute an interface that supports sensemaking of multiple variations of videos and provide empirical insights into how creators utilize variations when co-creating videos with AI.}

% \mira{this is ok but I am left wondering what you took away from previous work on sensemaking with AI generated content? Did you borrow any of those ideas in our design? How well did they generalize? }
% VAICo: Visual Analysis for Image Comparison

% \subsection{Working with multiple alternatives}

% \mira{I think we need a related work section on working with alternatives in other domains.}
