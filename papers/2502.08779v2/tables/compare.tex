\centering
\setlength{\tabcolsep}{2pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccc}
    \toprule
    \textbf{Benchmark} & \textbf{\# of Bias} & \textbf{\# of} & \textbf{\# Samples} & \textbf{Question} & \textbf{Real} & \textbf{Image}  & \textbf{Text Data} & \textbf{Visual Data}\\
    & \textbf{Categories} & \textbf{Images} & & \textbf{Types} & \textbf{Images} & \textbf{Variations} & \textbf{Source} & \textbf{Source} \\
    \midrule

     FACET \cite{gustafson2023facet} & 3 & 32,000 & 32,000 & ITM & \textcolor{darkgreen}{\checkmark} & \textcolor{darkgreen}{\checkmark} & - & SA-1B \\
    
     \rowcolor{lightgray!30} MMBias \cite{janghorbani2023multimodal} & 4 & 3,500 & 456k & ITM & \textcolor{darkgreen}{\checkmark} & \textcolor{darkgreen}{\checkmark} & RelatedWords & Web Search \\

      VisoGender \cite{hall2024visogender} & 2 & 690 & 690 & ITM & \textcolor{darkgreen}{\checkmark} & \textcolor{darkgreen}{\checkmark} & Manual  & Web Search \\

     \rowcolor{lightgray!30} SocialCounterfactuals \cite{howard2024socialcounterfactuals} & 3 & 171k & 171k & ITM & \textcolor{red}{\ding{55}} & \textcolor{darkgreen}{\checkmark} & - & SD \\
    
    \midrule

    \rowcolor{lightgray!30} B-AVIBench \cite{zhang2024b} & 9 & 1,400 & 55,000 & MCQ & Mixed & \textcolor{red}{\ding{55}} & - & Tiny LVLM-eHub, Lexica Aperture\\

    Ch3Ef \cite{shi2024assessment} & 6 & 506 & 506 & MCQ & Mixed & \textcolor{red}{\ding{55}} & GPT-4V & HOD, Rh20t, and DALL-E 3 \\

    \rowcolor{lightgray!30} ModSCAN \cite{jiang2024texttt} & 2 & 10,582 & 12,782 & OE, MCQ & Mixed & \textcolor{darkgreen}{\checkmark} & The Sims &  UTKFace, SD v2.1\\
    
    PAIRS \cite{fraser2024examining} & 3 & 200 & 1,200 & OE & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & - & Midjourney v4 and v5 \\

    
    
    \rowcolor{lightgray!30} GenderBias-VL \cite{xiao2024genderbias} & 2 & 34,581 & 415k & MCQ & \textcolor{red}{\ding{55}} & \textcolor{darkgreen}{\checkmark} & ChatGPT & SD XL \\
    
      BiasDora \cite{raj2024biasdora} & 9 & 6,880 & 43,659 & OE & \textcolor{red}{\ding{55}} & \textcolor{darkgreen}{\checkmark} & CrowS-pairs & DALL-E 3 and SD v1.5 \\

    

    VLStereoset \cite{zhou2022vlstereoset} & 4 & 1,028 & 1,958 & MCQ & \textcolor{darkgreen}{\checkmark} & \textcolor{red}{\ding{55}} &  StereoSet & Web Search \\ 

    \rowcolor{violet!10}\textbf{Ours} & 9 & 7,500 & 7,500 & MCQ & \textcolor{darkgreen}{\checkmark} & \textcolor{darkgreen}{\checkmark} & BBQ & Web Search \\

    \bottomrule
\end{tabular}%
}
\vspace{-0.5em}
\caption{Comparison of various LMM evaluation benchmarks with a focus on stereotype bias. Our approach is one of only three to assess nine bias types, is based on real images, unlike \texttt{B-AVIBench}, and unlike the Open-Ended \texttt{BiasDora} is easy to evaluate because of its Multiple-Choice design. The \textit{Question Types} are classified as `ITM` (Image-Text Matching), `OE' (Open-Ended) or MCQ (Multiple-Choice). }
\label{tab:methods_comparison}

%``Real Images" denotes that if they were generated using Diffusion based methods are curated with internet sources for real images.
