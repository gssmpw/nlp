\section{Conclusion}

In this paper, we introduce \SBbench, a novel benchmark designed to evaluate stereotype biases in Large Multimodal Models (LMMs) through visually grounded contexts. \SBbench comprises over 7.5k non-synthetic multiple-choice questions across nine domains and 60 sub-domains. We conduct an empirical analysis on nine LMMs, including four language family-scaled variants (LLaVA-OneVision, InternVL2, Qwen2VL, and GPT-4o), uncovering significant performance disparities. Notably, the best open-source model, InternVL2-8B, lags behind the proprietary GPT-4o by 51.21\% in fairness scores. Our findings reveal that LMMs exhibit the highest bias in categories such as Nationality, Age, and Appearance while performing relatively better on Race/Ethnicity and Gender Identity. Additionally, fairness improves with model scaling, yet bias remains more pronounced in LMMs compared to their corresponding LLM counterparts. This work underscores the limitations of state-of-the-art LMMs in mitigating social biases, highlighting key areas for future improvement.


\section{Ethical Consideration}
\textbf{Bias Categories.} In this work, we adopted nine social bias categories and their hierarchical taxonomy of sub-groups from the \cite{eeoc} guidelines and the BBQ dataset \cite{parrish2021bbq}. However, there may be additional types of biases that were not addressed in this study. Expanding the coverage to include more comprehensive bias categories is left as future work to improve the completeness of the dataset. \\
\textbf{Data.} The images in our bias categories were sourced from the internet, which may inherently contain implicit biases associated with certain stereotypes. These potential biases should be considered when analyzing the dataset. \\
\textbf{Limited Visual Samples.} For certain bias groups, such as \textit{Nationality}, \textit{Race/Ethnicity}, \textit{Religion}, and \textit{Socio-Economic Situation}, it was challenging to find images featuring two subjects interacting in specific ``actions," as seen in the BBQ dataset \cite{parrish2021bbq}. To address this, we ensured that all images in the \SBbench dataset are real by stitching two independent subject images side-by-side, as described in section \ref{sec:benchmark}. While this method ensured dataset authenticity, it opens up opportunities for future research to explore alternative and more refined approaches to dataset curation.


\section{Impact Statement}
Our benchmark provides a critical tool for assessing and addressing biases in AI systems that integrate both visual and textual data. By systematically evaluating how VLMs handle stereotypes related to attributes such as Race/Ethnicity, Gender Identity, and Nationality, this benchmark highlights the extent to which these models may perpetuate or amplify harmful biases. \SBbench's impact is twofold: it offers a rigorous framework for identifying biases in current models, while also guiding the development of more equitable and transparent LMMs. By encouraging the creation of models that better reflect diverse, unbiased perspectives, this work contributes to advancing fairness and inclusivity in AI research and application. While the benchmark provides a standardized method for detecting and addressing stereotype biases, promoting fairness in AI systems, it may not capture all subtle or emerging biases. Additionally, mitigating these biases could require significant resources and may not fully eliminate bias in all contexts, highlighting the ongoing challenge of achieving complete fairness in AI models.

\section{Acknowledgment}
This work was supported by the National Science Foundation (NSF) and Center for Smart Streetscapes (CS3) under NSF Cooperative Agreement No. EEC-2133516.