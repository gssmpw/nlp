\section{Stereotype-Bias Benchmark} \label{sec:benchmark}

\SBbench is the most comprehensive multiple-choice benchmark using real images to evaluate stereotype biases in LMMs. It includes 7,500 triplets of stereotypical images, contextual information, and multiple-choice questions, with each category further divided into several subcategories. \SBbench spans nine diverse domains: Age, Disability Status, Gender Identity, Nationality, Race/Ethnicity, Religion, Sexual Orientation, Physical Appearance, and Socio-Economic Status, with 60 subcategories in total, offering a detailed representation of stereotype biases. We extend the categorization from \cite{huang2023cbbq} and modify some subcategories for our specific use case. Tab. \ref{tab:biases} provides a summary of each bias type, including its definition and an example illustrating attested bias. \SBbench has been meticulously curated through a semi-automated pipeline to create a high-quality real dataset that captures the nuances of these nine bias categories. We show these domains and their subgroups in Fig. \ref{fig:pie_chart}. 


\subsection*{Data Collection and Annotation Pipeline}
For curating \SBbench, we focus on the Ambiguous context from BBQ dataset \cite{huang2023cbbq} because it presents inherently uncertain scenarios that rely on the model's ability to infer and interpret contextual clues. This makes it more effective in revealing how LMMs might propagate or challenge stereotypes when faced with vague or general prompts. By focusing on ambiguity, we can better assess the model’s responses in situations where bias and stereotyping are more likely to influence its decision-making process. We demonstrate our data collection and annotation pipeline in Fig. \ref{fig:pipeline}. Next, we present details of the modules of our pipeline.


\textbf{Visual Query Generator.} Our data collection pipeline consists of two main modules: Visual Query Generator (VQG) and Visual Information Retrieval. The VQG module transforms ambiguous descriptive text from the BBQ dataset into simpler, visually relevant queries that facilitate image retrieval. We achieve this using Qwen-2.5 LLM \cite{wang2024qwen2} with individual prompts for each domain as shown in Sec. \ref{sec:prompts_sb_bench} (Suppl. material), which extracts key elements such as subjects, scenes, and physical attributes while replacing specific details with more general visual information. 

We define a set of predefined rules for each category to ensure accurate and relevant image retrieval from web sources (see Sec. C suppl.). For instance, numeric age values are converted into relative terms such as young, middle-aged, or older. Similarly, when highly specific contexts make image retrieval challenging, such as ``Paul and Carol interviewing an engineering candidate at a software company'' — the description is simplified to ``a man and a woman in professional attire'' to improve the chances of finding relevant images while maintaining contextual accuracy.

\textbf{Visual Information Remover.} The Visual Information Remover (VIR) module processes the descriptive text context to remove personal identification data (PIDs) and visual clues from ambiguous text while maintaining contextual relevance and ambiguity. This ensures that in the visual benchmark, LMMs rely on the accompanying image rather than just the text when making predictions. The VIR module applies a set of anonymization rules and in-context examples to systematically remove visual information from the text using an LLM (Qwen-2.5), producing a ``Text Context without Visual Information.'' We discuss these anonymization rules and in-context examples in the suppl. material. 

\textbf{Paired Images.} Even after modifying the text and expanding the sample set, certain bias categories such as Nationality, Race/Ethnicity, Religion, and Socio-Economic status posed challenges in image retrieval. For instance, finding a single image featuring both a Burmese and a Mozambican woman was highly unlikely. Initially, we used diffusion models to generate images and performed reverse image searches to find visually similar real-world images. However, diffusion models struggled to accurately depict individuals from disadvantaged groups. To address this, we adopted a two-step approach: we searched for images of both subjects separately and later stitched them side by side. The VQG module still generates the queries, but with a minor modification: it now produces two separate search queries instead of one to facilitate this process. We show this process in Fig. \ref{fig:pairedpipeline}.

\textbf{Image Variations.} To ensure robustness in evaluation, we retain the top-10 retrieved images for \textit{single-image queries}, filtered using a CLIP score \cite{radford2021learning}. For \textit{paired-image queries}, we select 5 images per query and stitch them together, resulting in 25 curated pairs. Thus, for each text context, we obtain either 10 or 25 images, depending on the query type. These image variations help mitigate biases that may arise from relying on a single image. By testing the model across diverse images with different viewpoints, lighting conditions, and backgrounds, we can assess whether it truly understands the question rather than memorizing patterns from specific images. An unbiased and well-generalized LMM should provide consistent and accurate outputs across these variations, demonstrating its ability to reason beyond individual image characteristics.