\section{Related Works}

\subsection{Bias in Large Language Models}

Large Language Models (LLMs) often perpetuate societal biases, leading to representational harm \cite{raza2024safe}. Researchers have explored how these models detect and manifest bias in text generation \cite{huang2023cbbq, parrish2021bbq, yeh2023evaluating, dhingra2023queer}. Recent studies focus on quantifying bias in LLMs. \cite{sheng2019woman} found that generated text exhibited lower sentiment for certain groups, while \cite{zhuo2023red} assessed ChatGPTâ€™s toxicity and bias. Datasets like StereoSet \cite{nadeem2020stereoset} and BOLD \cite{dhamala2021bold} measure stereotypical biases across gender, profession, race, and religion. \cite{zhao2023gptbias} leveraged GPT-4 \cite{achiam2023gpt} to evaluate bias, and BBQ \cite{parrish2021bbq} used curated prompts for social bias analysis. Building on this foundation, our work extends the prompts and nine socially biased categories from the BBQ dataset \cite{parrish2021bbq} for further analysis.


\subsection{Bias in Vision-Language Foundation Models}
Vision Foundation models such as CLIP \cite{radford2021learning} have demonstrated remarkable zero-shot capabilities on several tasks \cite{cui2022can, esmaeilpour2022zero, li2022language, subramanian2022reclip}. Despite being trained on a vast dataset, CLIP \cite{radford2021learning} posits social biases such as gender and race \cite{agarwal2021evaluating} in various tasks such as text-based image editing task \cite{tanjim2024discovering}. Similar studies by \cite{berg2022prompt, wang2021gender, wang2022fairclip, wolfe2022american} explore the social bias in CLIP \cite{radford2021learning} based models. Another foundation model, BLIP \cite{li2022blip} has demonstrated impressive capabilities in tasks like image captioning and visual question answering \cite{li2022blip}. However, recent studies have highlighted the presence of social biases within these models. For instance, \cite{yang2024masking} indicates that BLIP can exhibit gender biases in image captioning tasks, often generating stereotypical descriptions based on gender cues present in images. These findings highlight the presence of social biases in the vision-language foundation models and necessitate ongoing evaluation and mitigation to ensure equitable and fair AI applications.

\subsection{Bias in Large Multimodal Models}

Large Multimodal Models (LMMs) have significantly advanced tasks integrating visual and textual data, such as image captioning and visual question answering (VQA) \cite{antol2015vqa, yue2024mmmu, vayani2024all}. However, despite their capabilities, they exhibit harmful social biases \cite{howard2024uncovering}, prompting the development of benchmarks to evaluate and mitigate these biases. Many existing benchmarks rely on synthetic datasets, which fail to capture the complexity of real-world biases. Examples include BiasDora \cite{raj2024biasdora}, which extends textual bias datasets to vision but remains synthetic, and VL-StereoSet \cite{zhou2022vlstereoset}, which includes fewer images and bias categories. PAIRS \cite{fraser2024examining} focuses on intersectional biases using parallel images for different genders and races. In contrast, more realistic benchmarks have emerged, such as SocialCounterfactuals \cite{howard2024socialcounterfactuals}, which evaluates biases by altering race, gender, and facial features in occupational settings. Extending this, Uncovering Bias in LMMs \cite{howard2024uncovering} introduces an open-ended evaluation where models generate stories, emotions, and self-descriptions, employing the Perspective API instead of GPT-based evaluation and concluding that base LLM size has little effect on toxicity. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/SB-Bench-Pie-Chart.pdf}
    % \vspace{-2em}
    \caption{The benchmark includes nine diverse domains and 60 sub-domains to rigorously assess the performance of LMMs in visually grounded stereotypical scenarios. \SBbench comprises over 7.5k questions on carefully curated non-synthetic images.}
    % \vspace{-2.1em}
    \label{fig:pie_chart}
\end{figure}


Other approaches, like ModSCAN \cite{jiang2024texttt}, evaluate biases through spatial location generation by pairing facial images and formulating prompts based on attributes from The Sims game. Similarly, \cite{shi2024assessment} covers 12 bias domains but is limited in sample size for discrimination tasks. Despite these efforts, existing benchmarks face limitations, including reliance on synthetic data, restricted bias categories, and insufficient sample sizes in discrimination-related evaluations. To address these gaps, we introduce \SBbench, a novel benchmark designed to provide a more comprehensive and realistic evaluation of social biases in LMMs by improving dataset diversity, real-world applicability, and evaluation methodologies. As the study of bias in LMMs evolves, moving beyond synthetic datasets to more realistic and nuanced evaluations is essential for ensuring fairness and mitigating harmful biases in vision-language models.




\begin{table*}[t]
\input{tables/biases}
\end{table*}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/multimodal_bias_pipeline.pdf}
    \vspace{-2.5em}
    \caption{\SBbench pipeline: We start with text bias evaluation question for a stereotype which includes descriptive text context detailing the scene and bias probing question. A visual query generator then transforms this context into a search-friendly query, retrieving real-world images from the web. The retrieved images are filtered using CLIP to ensure relevance. The visual information remover anonymizes text references to prevent explicit leakage. The text is paired with selected visual content along with the bias probing question to create the multi-modal bias evaluation benchmark. }
    \label{fig:pipeline}
    \vspace{-1em}
\end{figure*}