\section{Introduction}

\begin{table*}
\input{tables/compare}
\end{table*}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/teaser.pdf}
    \vspace{-2em}
    \caption{Bias in LMMs: The image presents a scenario where a family is selecting a babysitter between a university student and a transgender individual. Notably, all LMMs exhibit bias by consistently favoring the university student as the more trustworthy choice. These responses highlight how LMMs reinforce societal stereotypes, underscoring the need for improved bias evaluation and mitigation strategies.}
    \vspace{-1em}
    \label{fig:teaser}
\end{figure}

Large Multimodal Models (LMMs) are an advanced extension of Large Language Models (LLMs) that enable AI systems to process and integrate both text and images. These models have demonstrated impressive capabilities in tasks such as image captioning, visual question answering, and multimodal reasoning \cite{li2024llava, mahmood2024vurf}. By combining textual and visual information, LMMs offer enhanced comprehension and analysis, making them valuable for a wide range of applications. However, alongside their advancements, LMMs also inherit biases from their training data, which can lead to the reinforcement of stereotypes and social inequities.

Bias in LMMs is particularly concerning in real-world applications, where fairness and inclusivity are essential for equitable outcomes. Existing biases in training data often manifest in model responses, leading to unintended but impactful consequences. Addressing these biases is crucial to ensuring that LMMs contribute positively to society while minimizing potential harms. Researchers have attempted to analyze and mitigate these biases through various benchmarks and evaluation frameworks. However, current approaches typically categorize biases within a limited set of domains and often rely on synthetic datasets, which fail to capture the complexity of bias in real-world scenarios~\cite{fraser2024examining, zhou2022vlstereoset, howard2024socialcounterfactuals, raza2025responsible}.

Recent studies have introduced several bias evaluation benchmarks, such as VLStereoset and Social Counterfactuals, to assess bias in LMMs. While these benchmarks have contributed valuable insights, they suffer from notable limitations. Many of these approaches lack diversity in bias categories, restricting their effectiveness and generalizability. Additionally, their dependence on synthetic datasets means they lack real-world visual contexts, which are essential for understanding how biases emerge in everyday interactions~\cite{howard2024socialcounterfactuals, fraser2024examining}. These shortcomings highlight the need for a more comprehensive and realistic evaluation framework.

To address these challenges, we introduce Stereotype-Bias Bench (\SBbench), a diverse and extensive benchmark designed to evaluate stereotype biases in LMMs using non-synthetic images. \SBbench covers nine social bias domains and 60 sub-domains, forming one of the largest hierarchical taxonomies for social biases to date. The full list of categories and sub-domains is presented in Fig. \ref{fig:pie_chart}. For further sub-domain wise decomposition, refer to Fig. \ref{fig:category_distribution} (Suppl. material). Compared to the widely used non-synthetic VLStereoset benchmark~\cite{nadeem2020stereoset}, \SBbench offers 7x more visual images, ~4x more evaluation questions, and 2x more bias domains, significantly expanding the scope of bias assessment. By leveraging real-world images and a broader set of bias categories, \SBbench provides a more rigorous and standardized evaluation framework for next-generation multilingual LMMs.

With these advancements, \SBbench enables researchers to conduct more effective bias assessments, ultimately contributing to improved stereotype debiasing in AI models. By addressing the limitations of previous benchmarks, \SBbench paves the way for more fair and inclusive LMMs, ensuring that these powerful AI systems serve diverse communities equitably.

The contributions of our work can be summarized as follows:
\vspace{-2em}
\begin{itemize}
    \item We introduce \SBbench, a diverse multiple-choice benchmark featuring \textit{7,500} non-synthetic visual samples that span across nine categories and 60 sub-categories of social biases, providing a more accurate reflection of real-world contexts.
    \item \SBbench is meticulously designed to introduce visually grounded scenarios, explicitly separating visual biases from textual biases. This enables a focused and precise evaluation of visual stereotypes in LMMs.
    \item We benchmark both open-source and closed-source LMMs, along with their various scale variants, on \SBbench. Our analysis highlights critical challenges and provides actionable insights for developing more equitable and fair multimodal models. \\
\end{itemize}

\vspace{-2em}