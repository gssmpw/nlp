\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/paired_image_pipeline.pdf}
    \vspace{-2em}
    \caption{Paired Images Pipeline: For dual-image queries, the \textit{Dual Query Generator} creates two separate queries, each independently sent to a web search. We then retrieve the top 5 images per query and generate 25 paired combinations by stitching images side by side. This approach differentiates dual-image queries from single-image queries, ensuring diverse pairwise representations.}
    \label{fig:pairedpipeline}
    \vspace{-1em}
\end{figure}

\section{Benchmarking LMMs on SB-Bench}
\label{sec:main_results}

\begin{table*}[t]
    \input{tables/main_results}
\end{table*}

We benchmark nine state-of-the-art open- and closed-source LMMs on \SBbench, evaluating their performance across different model families and scales. Specifically, we assess multiple variants within families, such as InternVL2 (1B, 2B, 4B, 8B, 26B, 40B), LLaVA-OneVision (7B, 72B), Qwen2VL (7B, 72B), and GPT-4o (Mini, Full) as shown in Fig. \ref{fig:families_results}. Our analysis uncovers significant performance gaps and biases, offering critical insights and actionable recommendations for developing more equitable and fair multimodal models. For evaluation, the lower bias score indicates better fairness in LMMs and the higher range indicates a more pronounced bias. \SBbench contains 7,500 Multiple Choice Questions (MCQs). We employ \textit{Accuracy} metric on the responses of various LMMs to evaluate its performance. To ensure an unbiased evaluation of Language Multimodal Models (LMMs) in multiple-choice settings \cite{robinson2023larp}, we adopt a two-fold ablation strategy to address potential biases that could influence the results. 1) We prompt LMMs to provide a brief explanation of their chosen output to have open-ended answers to the question and show the correlation between MCQ and OE responses in Fig. \ref{fig:correlation}. 2) We randomize the order of multiple-choice options to evaluate the LMMs. We summarize the results in Tab. \ref{tab:std_dev_ablations}.

\paragraph{Overall Results:} Tab. \ref{tab:main_results} presents the category-wise performance of nine social biases on 9 LMMs on the \SBbench. The results highlight that \textbf{(a)} the closed-source proprietary models such as GPT-4o \cite{achiam2023gpt}, and Gemini-1.5-Flash \cite{team2023gemini} exhibits a lower level of bias across all categories compared to the open-source counterparts. The best closed-source model is GPT-4o with overall 10.79\% bias score as compared to the 62\% bias score of the best performing open-source LMM, InternVL-2-8B \cite{chen2024internvl}. The performance gap between proprietary and open-source models is significant, and our work highlights several key insights. Notably, closed-source models like GPT-4o \cite{achiam2023gpt} and Gemini-1.5-Flash \cite{team2023gemini} demonstrate highest-level of fairness in bias-related categories such as \textit{Age}, \textit{Disability}, \textit{Nationality}, and \textit{Religion}. For example, GPT-4o \cite{achiam2023gpt} demonstrates a bias-score of 23.99\% in the \textit{Age} category, followed by 12.05\% in the \textit{Disability} category. \textbf{(b)} Several open-source models (Qwen2-VL-7B-Instruct \cite{bai2023qwen}, Phi-3.5-Vision \cite{abdin2024phi}, LLaVA-OneVision \cite{li2024llava}, and LlaMA-3.2-11B-Vision-Instruct \cite{dubey2024llama}) achieve comparable overall performance while the Molmo-7B \cite{deitke2024molmo} performs poorly across all bias categories with the least fairness score of 90.92\%. Similar to closed-source models, open-source models also face challenges with social bias categories. For example, InternVL-2 \cite{chen2024internvl} performs the best in the \textit{Race/Ethnicity} category, with an accuracy of 42\%, but struggles in the \textit{Age} category, where its bias-score is only 81\%.

We present the overall performance of various LMMs across nine social bias in Tab. \ref{tab:main_results}. The results reveals that all the models perform poorly on \textit{Age} category such as Molmo \cite{deitke2024molmo} exhibits 96\% bias-score and other models like LLaMA-3.2 \cite{dubey2024llama}, Phi3.5 \cite{abdin2024phi}, and InternVL \cite{chen2024internvl}L exhibited nearly 75\% score on \textit{Age}. One noteworthy observation is the \textit{Race/Ethnicity} category where nearly all open-source models struggle, achieving a score of approximately 63\%, In contrast, InternVL-2-8B shows better fairness, with a significantly lower bias score of 42\%.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/correlation.pdf}
    \vspace{-2em}
    \caption{Bias evaluation on our MCQ benchmark is correlated with open ended evaluation of generated reasoning.}
    \label{fig:correlation}
    \vspace{-1.5em}
\end{figure}

\paragraph{Uncovering Implicit Biases in LMM's reasoning.} To investigate this, we tasked the LMMs in our evaluation suite with generating explanations for their selected answers in multiple-choice questions and conducted an in-depth analysis of their reasoning. Although \SBbench is focused on the quantitative MCQ evaluation, exploring the reasoning allows us to get qualitative evidence for the presence of specific biases in various LMMs. Our findings reveal that, in ambiguous contexts, models frequently default to crude, underlying stereotypes reflected in their reasoning. As illustrated in Fig. \ref{fig:success_vs_failure} \textit{(bottom-row)}, implicit biases emerge across various stereotype categories. For example, in the \textit{Religion} category, a question about a charity donor featuring a Buddhist and a Jewish individual reveals GPT-4o-miniâ€™s bias. The model associates Buddhism with compassion and altruism, stereotyping the Buddhist as the likely donor and failing to uphold fairness in its reasoning. This highlights GPT-4o-mini's inherent biases across various categories, demonstrating a gap in stereotype mitigation and the need for more robust strategies to ensure fairness and reduce reliance on oversimplified assumptions. For a quantitative assessment, we present a correlation plot between MCQ accuracy and open-ended responses in Fig. \ref{fig:correlation}, highlighting the overall consistency of 92.4\% between multiple-choice selections and the models' underlying reasoning.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.3, width=0.45\textwidth]{images/families_results.pdf}
    \vspace{-1em}
    \caption{Scaling trends across families of LMMs demonstrate a consistent decrease in bias as model size increases, highlighting the impact of model scale on fairness. We also present detailed results in Suppl. material, Tab. \ref{tab:model_family_results}.}
    \vspace{-1em}
    \label{fig:families_results}
\end{figure}

\paragraph{Assessing bias in LMMs across modalities:} To determine where bias emerges within LMMs, we conducted an experiment evaluating only the base LLMs of the models included in our evaluation. Specifically, we evaluated all nine categories in our proposed \SBbench dataset by prompting the LLMs with textual questions from the BBQ dataset \cite{huang2023cbbq}, without any visual input. The results of this analysis are summarized in Fig. \ref{fig:baseline_lmm}, revealing several important findings: \textbf{(a)} Despite being the most robust open-source LMM, InternVL2 demonstrates a significant gap compared to its base model, InternLM2, with an overall 22\% higher bias when visual input is included. In particular, the bias scores for InternVL2 increase by 36\% on \textit{Nationality}, 29\% on \textit{Religion}, and 26\% on \textit{Socio-Economic Status (SES)} when compared to its LLM counterpart. \textbf{(b)} Other open-source models, such as Phi-3, also exhibit a notable bias increase of 15.72\% when evaluated as an LMM compared to their base Phi-3 LLM. In contrast, models like Qwen2 and LLaMA-3.2 show smaller increases in bias, with performance degrading by 6.38\% and 9.80\%, respectively, when prompted with visual input. \textbf{(c)} These results highlight that incorporating visual input often amplifies bias in LMMs compared to their base LLMs, as seen in the substantial performance degradation across categories. This underscores the need for our robust and comprehensive visual-language benchmark, \SBbench to accurately evaluate LMMs and mitigate biases effectively. We also show the comparison of average bias scores between Vision LLMs and their underlying Base LLMs across different bias categories in Fig. \ref{fig:underlying_llm_vs_mllm_all} (Suppl. material).

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3, width=0.4\textwidth]{images/scale_results_internvl.pdf}
    \vspace{-1em}
    \caption{Scaling results for the InternVL2 family indicate a consistent reduction in bias across all stereotype categories as the model size increases from 1B to 40B scale.}
    \label{fig:internvl_results}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3, width=0.45\textwidth]{images/vllm_vs_base.pdf}
    \vspace{-1em}
    \caption{The figure illustrates the bias difference between LMMs and their corresponding LLM counterparts, showing that LMMs exhibit higher bias than their base LLMs.}
    % \vspace{-1em}
    \label{fig:baseline_lmm}
\end{figure}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/success_failure.pdf}
    \vspace{-1.5em}
    \caption{We present qualitative examples from both an open-source model, InternVL-2-8B, and a closed-source model, GPT-4o-mini, showcasing their performance on success cases \textit{(first row)} and failure cases \textit{(second row)} across various stereotype categories in our \SBbench. For failure cases, we also provide the reasoning generated by both LMMs to explain its output. For instance, The \textit{Socio-Economic-Situation} category mentions two common professions, chemical engineer and the line cook, and asks the question about who performed better in school. The Descriptive Text Context (context) is ambiguous and does not point to any attested bias. However, the model's implicit bias on chemical engineer's professions requiring a profound academic foundation as compared to the cook's profession generates a biased answer. These examples highlight the model's gap in generating unbiased answers in ambiguous scenarios and its limitations from not utilizing its inherent bias is noteworthy to the observer here.}
    \label{fig:success_vs_failure}
    \vspace{-1em}
\end{figure*}


\paragraph{Impact of model scale on stereotype biases.} We analyze the effect of the model scale on stereotype biases across various LMMs. We evaluate four model families: LLaVA-OneVision \cite{li2024llava} (7B and 72B), InternVL2 \cite{chen2024internvl} (1B, 2B, 4B, 8B, 26B), Qwen2-VL \cite{wang2024qwen2} (7B, 72B), and GPT \cite{achiam2023gpt} (GPT-4o-mini and GPT-4o), as illustrated in Fig. \ref{fig:families_results}.

Our findings reveal a clear trend that larger models tend to exhibit improved fairness. The performance of GPT-4o, the top-performing proprietary model, shows a substantial drop of 19.1\% in fairness when scaled from GPT-4o-mini to GPT-4o. A similar pattern is observed in the LLaVA-OneVision models, where the bias score decreased by nearly 10\% when scaling from the 7B to 72B model. Interestingly, the Qwen2-VL series displays the most dramatic improvement, with a 35.9\% reduction in bias score from the 7B to the 72B variant. The open-source InternVL-2 model also demonstrates a significant boost in fairness, with the bias score improving by 35.1\% when scaling from the 4B to the 40B model. We further analyze bias reduction across nine categories in InternVL (Fig. \ref{fig:internvl_results}), revealing that while all biases decrease with scale, some diminish more rapidly. For instance, \textit{Sexual Orientation} bias drops from 90\% to 50\% when scaling from 2B to 4B, then to 10\% at 40B. In contrast, biases in \textit{Age}, \textit{Race/Ethnicity}, \textit{Nationality}, \textit{Religion}, and \textit{Physical Appearance} decline more gradually.

While it's not surprising that larger models benefit from high-safety, anti-stereotype training data, the extent of bias reduction with scaling is noteworthy.

\paragraph{Stability of \SBbench Benchmark.} 1) \textit{Invariability in multiple-choice VQAs:} The evaluation of LMMs on MCQS are impacted by selection bias, as it ensures consistent responses when presented with different orders of answer choices for the same question \cite{robinson2023leveraginglargelanguagemodels, zheng2023large, pezeshkpour2023large}, which poses a key challenge to the stability of the benchmark. 

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
 & \small \textbf{Qwen2-VL} & \small \textbf{InternVL2} \\ 
\midrule
Average Bias & 69.38\% & 62.00\% \\ 
Option Shuffle & $\pm$ 0.27\% & $\pm$ 1.83\% \\ 
Image Shuffle & $\pm$ 2.12\% & $\pm$ 0.50\% \\ 
\bottomrule
\end{tabular}
% \vspace{-0.5em}
\caption{We evaluate the standard deviation for Qwen2-VL-7B and InternVL2-8B models on randomized multiple-choice orders and shuffled images in the paired image setting. Both models exhibit low variability and are consistent.}
\label{tab:std_dev_ablations}
\end{table}

To address this, we randomly shuffled the answer options during the evaluation. The results, shown in Table \ref{tab:std_dev_ablations}, reveal a standard deviation of $\pm$2.12\% for Qwen2-VL-7B and $\pm$0.50\% for InternVL-2-8B model indicating variability in our LMM evaluation setting. 2) \textit{Invariance in Stitched Images:} As previously mentioned, for categories such as \textit{Nationality}, \textit{Race/Ethnicity}, \textit{Religion}, and \textit{Socio-Economic}, we combined two individual images. To demonstrate the invariance in the order of the stitched images, we experimented on Qwen2-VL-7B and InternVL-2-8B, showing a standard deviation of $\pm$0.27\% for Qwen2-VL-7B and $\pm$1.83\% for InternVL-2-8B. This demonstrates the robustness and consistency of our dataset, further validating its stability across different evaluation conditions.