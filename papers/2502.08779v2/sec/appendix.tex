\clearpage
\renewcommand{\thefigure}{A.\arabic{figure}} %
\setcounter{figure}{0} 
\renewcommand{\thetable}{A.\arabic{table}}
\setcounter{table}{0} 
\renewcommand{\thesection}{\Alph{section}} %
\setcounter{section}{0}


\appendix
%\twocolumn
\section*{Appendix}

\begin{table*}[t]
    \input{tables/model_family_results}
\end{table*}

\section{SB-Bench Categories and Sub-Categories}
\SBbench covers nine social bias domains containing stereotypes of 60 sub-domains. For instance, the \textit{Age} category covers stereotypes related to younger and older people subcategories and \textit{Disability status} is sub-divided into Autistic, Physical disability, Mental illness, Deaf or hard of hearing, Down syndrome, Cognitive disability, Cerebral palsy, and Blind or low vision stereotypes. Figure \ref{fig:category_distribution} shows all the categories and their respective sub-categories.



\section{(More) Qualitative Examples}
Figure \ref{fig:more_examples} presents samples from the \SBbench benchmark. Each bias example includes an ambiguous context, a stereotype-probing question, and an accompanying image. The correct answer is always ``not known''. An LMM is considered biased if it selects any option other than ``not known''.


\section{Bias comparison within Model Family}

We further examine the impact of model scaling on bias across nine categories for InternVL2, Qwen2-VL, and LLaVA-OneVision, as summarized in Table \ref{tab:model_family_results}. The results reveal a general trend of decrease in bias as the model size grows, but with notable variations across categories and architectures.

For the InternVL2 family, bias consistently decreases as model size increases. However, the rate of decline differs across categories. Notably, Sexual Orientation bias drops sharply from 89.2\% in the 2B model to 50.6\% in the 4B model, and further to 20.8\% at 26B, indicating a substantial reduction. Other categories, such as Age, Race/Ethnicity, Nationality, Religion, and Physical Appearance, exhibit a more gradual decline up to 8B, followed by a significant drop—by approximately 50\% in most categories (except Race/Ethnicity, which decreases by only 10\%). Interestingly, the 40B variant does not show a strong improvement over the 26B model, suggesting a diminishing return in bias reduction at larger scales.

The Qwen2-VL models largely display the same trend. The 7B variant achieves relatively balanced high bias scores across most categories (69.38\% on average), but the 72B version shows a sharp drop of more than 50\% in bias, with an average score of 33.5\%. However, we do not see a large difference in Age category.

For the LLaVA-OneVision models, the 7B variant exhibits relatively consistent bias across categories. While the 72B model shows a moderate reduction in bias (from 70.48\% to 60.32\% on average), particularly in Sexual Orientation (dropping from 68.17\% to 36\%), the overall improvement is less pronounced compared to InternVL2 and Qwen2-VL, which achieved reductions of approximately 50\%. Notably, bias in the Age category increases with scaling, indicating that larger LLaVA-OneVision models do not consistently enhance fairness across all bias dimensions.


\begin{figure}[t]
    \centering
    \hspace{-0.1cm}
    \includegraphics[scale=0.5, width=0.48\textwidth]{images/vision_blind/vision_llm_vs_base_llm_avg.png}
    \caption{Comparison of average bias scores between Vision LLMs and their underlying Base LLMs across different bias categories. Vision LLMs exhibit higher bias across all categories, with the most significant increase observed in nationality (26.2\%), followed by religion (17.2\%) and race/ethnicity (16.8\%).}
    \vspace{-1em}
\label{fig:underlying_llm_vs_mllm_all}
\end{figure}


\section{LMM vs Base LLM}

Figure \ref{fig:underlying_llm_vs_mllm_ind} shows LMM and their underlying LLM's bias evaluated on \SBbench and on the same subset of BBQ dataset \cite{parrish2021bbq}. In general, the LMM has higher bias as compared to its underlying LLM. Figure \ref{fig:underlying_llm_vs_mllm_all} illustrates the mean bias across all evaluated LMMs and their language components.

%% A Category Distribution 
\begin{figure*}[h!]
    \hspace{-0.5cm}
    \includegraphics[width=1.05\textwidth]{images/category_distribution_v2.png}
    \caption{A detailed breakdown of categories from \SBbench. Each pie chart represents a specific bias category, displaying its sub-categories and their distribution by percent.}
\label{fig:category_distribution}
\end{figure*}


%% B Qualitative Examples
\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/More_Examples.pdf}
    \vspace{-2em}
    \caption{An example context, question, and image triplet for each bias category from the \SBbench benchmark.}
    \label{fig:more_examples}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.3, width=0.45\textwidth]{images/blind_results_v1.pdf}
    %\vspace{-1em}
    \caption{Vision Enabled vs Disabled: We compare bias across various LMMs when evaluated with text-only input (\textcolor{blue}{blue}) versus text + visual context (\textcolor{orange}{orange}).}
    \label{fig:blind}
    \vspace{-1em}
\end{figure}

On average, LMMs are 13\% more biased than their underlying LLM counterparts. Biases related to Age, Disability Status, Gender Identity, and Socio-Economic Status show a modest increase of approximately 7\%. In contrast, biases associated with Physical Appearance, Sexual Orientation, Race/Ethnicity, and Religion reveal significantly larger gaps, exceeding 10\%. Notably, Nationality demonstrates the most substantial difference, with a 26.2\% increase, highlighting that even after safety fine-tuning of LLMs, the visual component may introduce or amplify specific biases.



\begin{figure*}[h!]
    \centering
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/vision_blind/vision_llm_vs_base_llm_InternVL2.png}
        \captionof{subfigure}{InternVL2-8B vs InternLM2.5-7b}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/vision_blind/vision_llm_vs_base_llm_llama3.png}
        \captionof{subfigure}{Llama-3.2-11B vs Llama-3.1-8B}
    \end{minipage}
    
    \vspace{0.5cm}
    
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/vision_blind/vision_llm_vs_base_llm_phi3.png}
        \captionof{subfigure}{Phi-3.5-Vision vs Phi-3-mini-128k}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/vision_blind/vision_llm_vs_base_llm_Qwen2.png}
        \captionof{subfigure}{Qwen2-VL-7B vs Qwen2-7B}
    \end{minipage}
    
    \caption{Evaluating underlying LLM with BBQ dataset and comparing with LMM on \SBbench. We see a common trend that underlying LLM usually has lower bias than LMM, indicating that visual component of LMM introduces bias.}
    \label{fig:underlying_llm_vs_mllm_ind}
\end{figure*}



\section{Vision-enabled vs Vision-disabled}
We evaluate LMMs with and without image context using the \SBbench benchmark in our current setup. The models are prompted in a similar manner, but we remove the image and its corresponding tags to assess how the LMMs perform without the visual context. Our results (Figure \ref{fig:blind}) demonstrate that biases can still be triggered even in the absence of the image input. Molmo, LlavaOneVision, and Qwen2-VL exhibit a modest ($\approx$10\%) reduction in bias. However, Llama-3.2-Vision and InternVL-2 show a significant drop of approximately 20\%.

Phi-3.5-Vision does not exhibit a substantial difference between the vision and vision-blind evaluations. Upon qualitatively inspecting the generated reasonings from Phi-3.5-Vision, we observed that the model often hallucinates an image, even in the absence of one. Interestingly, this hallucination also carries biases. For instance, when prompted with \textit{“The people in the image are attending an academic conference. Who is the researcher?”}, the model generates \textit{“The man is likely the researcher since he is presenting a poster at the academic conference, which is a common activity for researchers to share their findings with others in their field.”} This explains the similar bias scores between Phi-3.5-Vision with text only and Phi-3.5-Vision with both text and visual context.


\section{Open-Ended Evaluation of LMM Reasoning}
To systematically investigate biases in the generated reasonings of LMMs, we utilize GPT-4o-mini as an evaluator. We begin by thoroughly defining each bias category to ensure clarity in the evaluation framework. The context, question, and reasoning are then provided to GPT, enabling it to assess whether the reasoning exhibits bias. Additionally, we prompt GPT-4o to assign the most appropriate bias category to the reasoning. Figure \ref{fig:prompt_gpt_reasoning} illustrates the prompt used for evaluation. Our analysis reveals a strong correlation (0.924) between the scores from our MCQ Evaluation method and GPT’s bias judgments, further demonstrating the robustness and alignment of our evaluation system with GPT.

% Figure~\ref{fig:correlation}.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{images/correlation_v1.pdf}
%     \vspace{-2em}
%     \caption{Bias evaluation through our MCQ benchmark is correlated with open ended evaluation of generated reasoning.}
%     \label{fig:correlation}
% \end{figure}


\begin{table}[h!]
    \input{tables/evaluated_models_info}
    \vspace{-1em}
\end{table}


\section{Implementation Details of LMMs}

To assess the state-of-the-art in Large Multimodal Models (LMMs), we conducted a comprehensive evaluation encompassing a diverse set of model architectures and families. Our study includes nearly all available architectural variants within each selected model family, ensuring a broad representation of existing approaches. This allows us to analyze the strengths and limitations of different model designs and training methodologies.

We initially attempted to evaluate LLaVA-OneVision-0.5B and Qwen-2VL-2B. However, most of their generations deviated significantly from our structured template, tending towards open-ended responses rather than adhering to the intended format. This inconsistency made direct comparison challenging, leading to their exclusion from our study.

For a fair evaluation, we refrain from applying low-bit quantization during inference \cite{thawakar2024mobillama}. Additionally, to ensure a balanced comparison across model families, we primarily include 7B parameter variants from each family, as this size is widely available across different architectures and provides a reasonable trade-off between performance and computational feasibility.

Table \ref{tab:models_info} provides a detailed breakdown of the evaluated LMMs, listing their vision encoders, the underlying Large Language Models (LLMs), and the corresponding parameter counts. This comparison highlights the diversity in model design, ranging from lightweight architectures such as InternVL2-1B to significantly larger models like Qwen2-VL-72B, which leverage bigger LLMs. The table also underscores the variety of vision encoders used, such as InternViT, QwenViT, SigLIP, and CLIP ViT-L/14, showing different approaches to integrating visual information into multimodal reasoning.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/main_results.pdf}
    \vspace{-2em}
    \caption{Bias scores across LMMs for different stereotype categories.}
    \label{fig:main_results}
\end{figure*}

\section{Visual Information Remover}
A common system instruction with a set of rules is given to the LLM to remove visual information from the context as shown in Figure \ref{fig:prompt_visual_info_remover}. We remove information such as names, physical appearance, and occupation while substituting them with appropriate words according to the context like man/woman/boy/girl. 


\section{Visual Query Generator}
We provide specialized system prompts per bias type for Visual Query Generation. This ensures that the generated queries are aligned and is a simplified version of the ambiguous context. 

A common set of rules like using relative words instead of numbers for age, and focusing on gender flows through all bias system prompts. Bias types like Nationality and Race / Ethnicity have additional instructions to focus on the country of origin and racial \& ethnic backgrounds of the person. Figure \ref{fig:prompt_age}-\ref{fig:prompt_ses} shows these specialized prompts for every bias type.


\section{Various Prompts used in \SBbench annotation and benchmarking}
\label{sec:prompts_sb_bench}
The prompt shown in Figure \ref{fig:prompt_evaluation} is designed for generating answers to multiple-choice questions. The model receives an ambiguous text passage, a question, and answer options, followed by a post-prompt instructing it to output responses strictly in JSON format for easier post-processing. Additionally, the model’s chat template integrates the prompt\_text and the image.  

To evaluate biases in LMM reasoning using GPT-4o, we employ the prompt in Figure \ref{fig:prompt_gpt_reasoning}. For removing visual information from the text, as implemented in the Visual Information Remover module, we use the prompt in Figure \ref{fig:prompt_visual_info_remover}.  

For generating Visual Queries across different bias contexts, we utilize distinct prompts. For example, the Age Bias context employs the prompt in Figure \ref{fig:prompt_age}. Similarly, prompts for other bias contexts include: Disability (Fig. \ref{fig:prompt_disability}), Gender Identity (Fig. \ref{fig:prompt_gender}), Physical Appearance (Fig. \ref{fig:prompt_appearance}), Sexual Orientation (Fig. \ref{fig:prompt_sexual_orientation}), Nationality (Fig. \ref{fig:prompt_nationality}), Race/Ethnicity (Fig. \ref{fig:prompt_race_ethnicity}), Religion (Fig. \ref{fig:prompt_religion}), and Socio-Economic Status (Fig. \ref{fig:prompt_ses}).

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_evaluation.pdf}
    \vspace{-2em}
    \caption{Prompt template for evaluating LMMs.}
    \label{fig:prompt_evaluation}
\end{figure}



\section{Main Results}
The \ref{fig:main_results} shows the evaluation results of nine LMMs evaluated on the \SBbench dataset. We describe the detailed results in Sec. \ref{sec:main_results}.



% \textcolor{red}{Category wise pie chart}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_gpt_reasoning.pdf}
    \vspace{-2em}
    \caption{Prompt and response format for evaluating biases of LMM's reasoning by GPT-4o.}
    \label{fig:prompt_gpt_reasoning}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_visual_info_remover.pdf}
    \vspace{-2em}
    \caption{Prompt for removing Visual Information from the context.}
    \label{fig:prompt_visual_info_remover}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_age.pdf}
    \vspace{-2em}
    \caption{Prompt for generating Visual Query of the Age bias's context.}
    \label{fig:prompt_age}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_disability.pdf}
    \vspace{-2em}
    \caption{Prompt for generating Visual Query of the Disability bias's context.}
    \label{fig:prompt_disability}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_gender.pdf}
    \vspace{-2em}
    \caption{Prompt for generating Visual Query of the Gender Identity bias's context.}
    \label{fig:prompt_gender}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_appearance.pdf}
    \vspace{-2em}
    \caption{Prompt for generating Visual Query of the Physical Appearance bias's context.}
    \label{fig:prompt_appearance}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_sexual_orientation.pdf}
    \vspace{-2em}
    \caption{Prompt for generating Visual Query of the Sexual Orientation bias's context.}
    \label{fig:prompt_sexual_orientation}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_nationality.pdf}
    \vspace{-2em}
    \caption{Prompt for generating Visual Query of the Nationality bias's context.}
    \label{fig:prompt_nationality}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_race_ethnicity.pdf}
    \vspace{-2em}
    \caption{Prompt for generating Visual Query of the Race / Ethnicity bias's context.}
    \label{fig:prompt_race_ethnicity}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_religion.pdf}
    \vspace{-2em}
    \caption{Prompt for generating Visual Query of the Religion bias's context.}
    \label{fig:prompt_religion}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/prompts/prompt_ses.pdf}
    \vspace{-2em}
    \caption{Prompt for generating Visual Query of the Socio-Economic Status bias's context.}
    \label{fig:prompt_ses}
\end{figure*}

