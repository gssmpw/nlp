@inproceedings{10.5555/3666122.3666203,
author = {Li, Xuhong and Du, Mengnan and Chen, Jiamin and Chai, Yekun and Lakkaraju, Himabindu and Xiong, Haoyi},
title = {M4: a unified XAI benchmark for faithfulness evaluation of feature attribution methods across metrics, modalities and models},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While Explainable Artificial Intelligence (XAI) techniques have been widely studied to explain predictions made by deep neural networks, the way to evaluate the faithfulness of explanation results remains challenging, due to the heterogeneity of explanations for various models and the lack of ground-truth explanations. This paper introduces an XAI benchmark named M4, which allows evaluating various input feature attribution methods using the same set of faithfulness metrics across multiple data modalities (images and texts) and network structures (ResNets, MobileNets, Transformers). A taxonomy for the metrics has been proposed as well. We first categorize commonly used XAI evaluation metrics into three groups based on the ground truth they require. We then implement classic and state-of-the-art feature attribution methods using InterpretDL and conduct extensive experiments to compare methods and gain insights. Extensive experiments have been conducted to provide holistic evaluations as benchmark baselines. Several interesting observations are made for designing attribution algorithms. The implementation of state-of-the-art explanation methods and evaluation metrics of M4 is publicly available at https://github.com/PaddlePaddle/InterpretDL.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {81},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{Lundberg2017AUA,
  title={A Unified Approach to Interpreting Model Predictions},
  author={Scott M. Lundberg and Su-In Lee},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:21889700}
}

@MISC{Meudec2021-le,
  title     = "tf-explain",
  author    = "Meudec, Raphael",
  abstract  = "Interpretability Methods for tf.keras models with TensorFlow 2.x",
  publisher = "Zenodo",
  year      =  2021
}

@article{Ribeiro2016WhySI,
  title={“Why Should I Trust You?”: Explaining the Predictions of Any Classifier},
  author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
  journal={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:13029170}
}

@inproceedings{Sarti_2023,
   title={Inseq: An Interpretability Toolkit for Sequence Generation Models},
   url={http://dx.doi.org/10.18653/v1/2023.acl-demo.40},
   DOI={10.18653/v1/2023.acl-demo.40},
   booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
   publisher={Association for Computational Linguistics},
   author={Sarti, Gabriele and Feldhus, Nils and Sickert, Ludwig and van der Wal, Oskar},
   year={2023} }

@article{Selvaraju2016GradCAMVE,
  title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
  author={Ramprasaath R. Selvaraju and Abhishek Das and Ramakrishna Vedantam and Michael Cogswell and Devi Parikh and Dhruv Batra},
  journal={International Journal of Computer Vision},
  year={2016},
  volume={128},
  pages={336 - 359},
  url={https://api.semanticscholar.org/CorpusID:15019293}
}

@inproceedings{Sundararajan2017AxiomaticAF,
  title={Axiomatic Attribution for Deep Networks},
  author={Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  booktitle={International Conference on Machine Learning},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:16747630}
}

@misc{agarwal2024openxaitransparentevaluationmodel,
      title={OpenXAI: Towards a Transparent Evaluation of Model Explanations}, 
      author={Chirag Agarwal and Dan Ley and Satyapriya Krishna and Eshika Saxena and Martin Pawelczyk and Nari Johnson and Isha Puri and Marinka Zitnik and Himabindu Lakkaraju},
      year={2024},
      eprint={2206.11104},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.11104}, 
}

@misc{hedström2023quantusexplainableaitoolkit,
      title={Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond}, 
      author={Anna Hedström and Leander Weber and Dilyara Bareeva and Daniel Krakowczyk and Franz Motzkus and Wojciech Samek and Sebastian Lapuschkin and Marina M. -C. Höhne},
      year={2023},
      eprint={2202.06861},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.06861}, 
}

@misc{kokhlikyan2020captumunifiedgenericmodel,
      title={Captum: A unified and generic model interpretability library for PyTorch}, 
      author={Narine Kokhlikyan and Vivek Miglani and Miguel Martin and Edward Wang and Bilal Alsallakh and Jonathan Reynolds and Alexander Melnikov and Natalia Kliushkina and Carlos Araya and Siqi Yan and Orion Reblitz-Richardson},
      year={2020},
      eprint={2009.07896},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.07896}, 
}

@misc{madsen2024interpretabilityneedsnewparadigm,
      title={Interpretability Needs a New Paradigm}, 
      author={Andreas Madsen and Himabindu Lakkaraju and Siva Reddy and Sarath Chandar},
      year={2024},
      eprint={2405.05386},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.05386}, 
}

@misc{sikder2024fairxcomprehensivebenchmarkingtool,
      title={FairX: A comprehensive benchmarking tool for model analysis using fairness, utility, and explainability}, 
      author={Md Fahim Sikder and Resmi Ramachandranpillai and Daniel de Leng and Fredrik Heintz},
      year={2024},
      eprint={2406.14281},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.14281}, 
}

@misc{wickstrøm2024flexibilitymanipulationslipperyslope,
      title={From Flexibility to Manipulation: The Slippery Slope of XAI Evaluation}, 
      author={Kristoffer Wickstrøm and Marina Marie-Claire Höhne and Anna Hedström},
      year={2024},
      eprint={2412.05592},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.05592}, 
}

