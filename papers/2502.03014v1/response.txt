\section{Related Work}
\subsection{Overview of Explainability Methods}
Machine learning models, especially deep neural networks, are often regarded as "black boxes" because of the difficulty in understanding how they arrive at specific predictions. To address this issue, explainability methods have been developed to make these models more interpretable. These methods can generally be categorized into global explainability and local explainability.

\subsubsection{Global v/s Local Explainbility}

Global explainability aims to provide a high-level understanding of a model’s overall behavior across all inputs. It focuses on identifying which features are most influential in determining the model's predictions over a large dataset, thus offering insights into how the model makes decisions in general. In contrast, local explainability focuses on explaining the model’s decision-making process for individual instances or predictions. Local explainability is particularly useful for understanding specific decisions made by a model in real-world applications, where a user may want to know why a model made a particular prediction for a specific input. A popular technique within local explainability is post-hoc explainability, where the explanation is generated after the model has been trained without altering its internal structure. These methods are crucial for understanding complex, black-box models like deep neural networks, especially in scenarios where interpretability of individual predictions is necessary.

For instance, local explainability methods are commonly applied in medical diagnostics, where a deep learning model might predict the presence of a disease in a patient based on their medical history and test results. A healthcare professional might want to know why the model predicted that a patient has a particular disease, which can be addressed by local explainability methods. Similarly, in credit scoring models, a financial institution might use local explainability to understand the rationale behind an individual's credit score prediction, helping to provide transparency for both customers and regulators.

\subsubsection{Post Hoc Local Explainibility MEthods}
There are several widely used methods to achieve local explainability, particularly for post-hoc explanations. These methods include:

SHAP (SHapley Additive exPlanations) \textit{Sundararajan et al., "Axiomatic Attribution for Deep Neural Networks"} is a model-agnostic method that assigns a value to each feature, reflecting its contribution to the prediction for a specific instance. SHAP is based on Shapley values from cooperative game theory, ensuring fairness and consistency in the attribution of feature importance. For example, in a loan approval model, SHAP can help explain why a specific feature, such as income, had a significant impact on the decision to approve or deny the loan for a particular applicant. This method can be applied to a variety of models, including tree-based models and deep learning architectures, and is widely used due to its strong theoretical foundation.

LIME (Local Interpretable Model-Agnostic Explanations) \textit{Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning"} is another widely used model-agnostic method. LIME approximates the local decision boundary of the model for individual predictions by training an interpretable surrogate model, such as a linear regression or decision tree, on perturbed data. It works by observing how the model behaves on slightly altered versions of the input data and generating an explanation based on this local behavior. For instance, in image classification, LIME can help explain which parts of an image are most influential in determining the model's prediction by perturbing different regions of the image and observing the model's response.

Grad-CAM (Gradient-weighted Class Activation Mapping) \textit{Selvaraju et al., "Grad-CAM: Visual Explanations for Convolutional Neural Networks"} is a method designed for convolutional neural networks (CNNs) that generates visual explanations by highlighting the regions of an image that are most important in the model's prediction. For example, in a medical imaging task where a CNN is used to diagnose diseases from X-ray images, Grad-CAM can generate a heatmap showing the areas of the X-ray image that contributed most to the prediction of a particular disease, providing intuitive visual explanations to the medical professionals.

Integrated Gradients \textit{Sundararajan et al., "Axiomatic Attribution for Deep Neural Networks"} works by integrating the gradients of the model’s output with respect to the input features, from a baseline input to the actual input. This method provides a smooth and consistent explanation for model predictions, ensuring that each feature’s contribution is computed along a continuous path. For example, in sentiment analysis, Integrated Gradients can be used to explain which words in a sentence were most responsible for a positive or negative sentiment prediction, helping to clarify the rationale behind the model's decision.

Backpropagation-Based Explainability Methods, such as DlBacktrace \textit{Ferencz et al., "DlBacktrace: Tracing the Decision-Making Process of Deep Learning Models"} , trace the relevance of each component in a neural network from the output back to the input. This allows for a detailed analysis of how each layer in the network contributes to the final prediction. For example, in image classification, backpropagation-based methods can show how the features learned by the lower layers of the neural network (e.g., edges and textures) contribute to higher-level features (e.g., shapes or objects) that ultimately determine the model’s output. DlBacktrace is particularly useful as it provides insights into the layer-wise contribution to predictions.

These local explainability methods help demystify the decision-making processes of machine learning models, particularly in complex, real-world scenarios. Whether used to explain a healthcare model's diagnosis, a financial institution's credit decision, or a computer vision model's image classification, these techniques provide crucial insights into individual predictions, enhancing the trust and transparency of AI systems.

\subsection{Existing Evaluation Frameworks and Benchmarking Tools for Model Explainability}
Despite significant advancements in Explainable AI (XAI), evaluating model explanations remains a challenge due to the lack of a standardized and comprehensive assessment framework. Existing evaluation methodologies often rely on simplistic interpretability metrics that fail to capture essential aspects such as robustness, generalizability, and human alignment. This has led to inconsistencies in how explanations are assessed across different domains and applications.  

Recent research has highlighted the limitations of current evaluation paradigms. Madsen et al. \textit{Madsen et al., "On the limitations of explanation methods for deep learning"} argue that most existing approaches prioritize faithfulness while overlooking robustness and usability, leading to incomplete assessments of explanation quality. Wickstrøm et al. \textit{Wickstrøm et al., "Evaluation metrics for explainable AI"} further emphasize that interpretability metrics are often inconsistent across domains, which complicates their application in real-world scenarios and raises concerns about their susceptibility to manipulation.  

Several benchmarking tools have been introduced to address these issues, each with its own strengths and limitations. The M4 Benchmark \textit{M4 Benchmark} provides a structured framework for evaluating feature attribution methods, placing significant emphasis on faithfulness. However, it does not explicitly assess robustness against adversarial perturbations or stability across different data distributions, which are crucial for ensuring reliability in high-stakes applications. OpenXAI \textit{OpenXAI} offers a flexible evaluation framework, though its reliance on synthetic data generation raises concerns about the generalizability of its findings to real-world settings. Quantus \textit{Quantus} incorporates a diverse set of evaluation metrics, covering faithfulness, robustness, and complexity, yet it lacks an explicit mechanism to assess whether the generated explanations align with human intuition. FairX \textit{FairX} extends evaluation to fairness and bias considerations but does not provide a comprehensive framework for post-hoc explainability. Similarly, Captum \textit{Captum} and TF-Explain \textit{TF-Explain} focus on generating explanations for deep learning models but do not include built-in benchmarking capabilities to assess explanation quality systematically. Inseq \textit{Inseq} , while valuable for sequence generation models, is specialized for NLP tasks and does not generalize well to other domains such as tabular or vision-based data.  

The fragmentation of existing evaluation frameworks highlights the need for a more robust and flexible approach to assessing model explanations. Many existing tools prioritize faithfulness while neglecting complementary factors such as robustness, sensitivity, and usability, leading to an incomplete understanding of explanation quality. Others are designed for specific model architectures or data modalities, making it difficult to conduct cross-domain comparisons. Additionally, many commonly used evaluation metrics do not align well with human judgment, which limits their applicability in decision-critical environments where interpretability is essential.  

To address these challenges, we introduce \texttt{xai\_evals}, a framework that integrates explanation generation and evaluation into a single, standardized package. Unlike existing tools, \texttt{xai\_evals} allows researchers to systematically assess explanation quality across multiple methods, models, and datasets using a broad range of evaluation metrics.