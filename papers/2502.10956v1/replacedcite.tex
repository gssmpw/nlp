\section{Related Work}
\paragraph{Direct learning from real-world experience}
Training policies directly in the real world can circumvent the complexities associated with modeling hard-to-simulate factors. However, previous methods ____ typically achieve results only in low-performance regions characterized by slow walking speeds. On the other hand, methods that fine-tune simulation-trained policies in real-world ____ primarily addresses the shift in dynamics due to sim-to-real transferring. These studies often do not extend to optimizing objectives that are unseen in simulation pre-training. Furthermore, in contrast to these methods, which are generally tailored for specific learning algorithms ____, our approach offers a versatile integration across various policy optimization frameworks.




\paragraph{Augmenting simulation with data-driven models} 
Recent advances have embraced hybrid simulations, combining analytic physics with data-driven models to capture complex dynamics ____. Such hybrid simulations find applications in diverse robotic tasks, including legged locomotion ____, drone racing ____,  and modeling human behavior in sports ____. These studies, however, primarily focus on enhancing the fidelity of simulation \textit{dynamics}. In contrast, our setting requires data-driven models to be explicit \textit{objectives} for policy optimization, which exacerbates issues related to out-of-distribution and model exploitation.

\paragraph{Energy efficiency in quadruped robots} 
Total energy efficiency optimizations are typically reserved for robot hardware design ____. On the controller side, previous works have employed various strategies to estimate and optimize energy consumption in legged locomotion. Techniques include using mechanical power and Joule heating as reward functions ____ or as constraints ____. Rather than relying on hand-designed proxies, our approach aims to minimize total power consumption through data-driven fine-tuning techniques.