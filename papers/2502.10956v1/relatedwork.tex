\section{Related Work}
\paragraph{Direct learning from real-world experience}
Training policies directly in the real world can circumvent the complexities associated with modeling hard-to-simulate factors. However, previous methods \cite{ha2020learning, haarnoja2018learning, smith2022walk, smith2023grow} typically achieve results only in low-performance regions characterized by slow walking speeds. On the other hand, methods that fine-tune simulation-trained policies in real-world \cite{smith2022legged, lei2023uni, shi2024efficient} primarily addresses the shift in dynamics due to sim-to-real transferring. These studies often do not extend to optimizing objectives that are unseen in simulation pre-training. Furthermore, in contrast to these methods, which are generally tailored for specific learning algorithms \cite{hiraoka2021dropout, shao2020controlvae}, our approach offers a versatile integration across various policy optimization frameworks.




\paragraph{Augmenting simulation with data-driven models} 
Recent advances have embraced hybrid simulations, combining analytic physics with data-driven models to capture complex dynamics \cite{heiden2021neuralsim,qiao2021efficient,heiden2022inferring,sanchez2020learning,ajay2018augmenting}. Such hybrid simulations find applications in diverse robotic tasks, including legged locomotion \cite{hwangbo2019learning, margolis2023walk}, drone racing \cite{kaufmann2023champion},  and modeling human behavior in sports \cite{abeyruwan2023sim2real}. These studies, however, primarily focus on enhancing the fidelity of simulation \textit{dynamics}. In contrast, our setting requires data-driven models to be explicit \textit{objectives} for policy optimization, which exacerbates issues related to out-of-distribution and model exploitation.

\paragraph{Energy efficiency in quadruped robots} 
Total energy efficiency optimizations are typically reserved for robot hardware design \cite{seok2014design, katz2019mini, krimsky2024elastic}. On the controller side, previous works have employed various strategies to estimate and optimize energy consumption in legged locomotion. Techniques include using mechanical power and Joule heating as reward functions \cite{hwangbo2019learning, rudin2022learning, margolis2024rapid, margolis2023walk, agarwal2023legged, zhuang2023robot, yang2022fast, fu2021minimizing} or as constraints \cite{mahankali2024maximizing, kim2024not, chane2024cat}. Rather than relying on hand-designed proxies, our approach aims to minimize total power consumption through data-driven fine-tuning techniques.