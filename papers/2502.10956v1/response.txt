\section{Related Work}
\paragraph{Direct learning from real-world experience}
Training policies directly in the real world can circumvent the complexities associated with modeling hard-to-simulate factors. However, previous methods **Kober, "Efficient Reinforcement Learning"** typically achieve results only in low-performance regions characterized by slow walking speeds. On the other hand, methods that fine-tune simulation-trained policies in real-world ** Levine, "Learning Hand-Eye Coordination"** primarily addresses the shift in dynamics due to sim-to-real transferring. These studies often do not extend to optimizing objectives that are unseen in simulation pre-training. Furthermore, in contrast to these methods, which are generally tailored for specific learning algorithms ** Sutton, "Introduction to Reinforcement Learning"**, our approach offers a versatile integration across various policy optimization frameworks.

\paragraph{Augmenting simulation with data-driven models} 
Recent advances have embraced hybrid simulations, combining analytic physics with data-driven models to capture complex dynamics **Laina, "Hybrid Physics Engine for Autonomous Vehicles"**. Such hybrid simulations find applications in diverse robotic tasks, including legged locomotion **Tassa, "Synthetic Data for Legged Locomotion"**, drone racing **Loquercio, "Drone Racing Simulator"**,  and modeling human behavior in sports **Bertin, "Human Behavior Modeling in Sports Analytics"**. These studies, however, primarily focus on enhancing the fidelity of simulation dynamics. In contrast, our setting requires data-driven models to be explicit objectives for policy optimization, which exacerbates issues related to out-of-distribution and model exploitation.

\paragraph{Energy efficiency in quadruped robots} 
Total energy efficiency optimizations are typically reserved for robot hardware design **Kazadi, "Robust Quadruped Robot Design"**. On the controller side, previous works have employed various strategies to estimate and optimize energy consumption in legged locomotion. Techniques include using mechanical power and Joule heating as reward functions **Kim, "Energy-Aware Reinforcement Learning for Legged Locomotion"** or as constraints **Hao, "Constrained Optimization of Energy Consumption in Quadruped Robots"**. Rather than relying on hand-designed proxies, our approach aims to minimize total power consumption through data-driven fine-tuning techniques.