%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
\usepackage{float} % for figure placement
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{algorithm} 
\usepackage{algpseudocodex}
\usepackage[font={small}]{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{makecell}

\title{\LARGE \bf
Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving 
}


\author{Ruiqian Nai$^{123}$, Jiacheng You$^{123}$, Liu Cao$^{4}$, Hanchen Cui$^{3}$, Shiyuan Zhang$^{1}$, Huazhe Xu$^{123}$, and Yang Gao$^{123}$% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$ Institute for Interdisciplinary Information Sciences, Tsinghua University. $^{2}$ Shanghai AI Lab. $^{3}$ Shanghai Qi Zhi Institute. $^{4}$ Department of Electronic Engineering, Tsinghua University.
        {\tt\small nrq22@mails.tsinghua.edu.cn}}%
% \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
%         Dayton, OH 45435, USA
%         {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}

\newcommand{\rmain}{\ensuremath{{r}^\text{main}}}
\newcommand{\rhard}{\ensuremath{{r}^\text{hard}}}
\newcommand{\Rhard}{\ensuremath{{R}^\text{hard}}}
\newcommand{\yhard}{\ensuremath{{y}^\text{hard}}}
\newcommand{\pianchor}{\ensuremath{{\pi}^\text{anchor}}}
\newcommand{\pipretrained}{\ensuremath{{\pi}^\text{pre-trained}}}

\newcommand{\rms}{\ensuremath{\mathbf{s}}}
\newcommand{\rma}{\ensuremath{\mathbf{a}}}

\newcommand{\simulation}{\ensuremath{\text{sim}}}
\newcommand{\real}{\ensuremath{\text{real}}}

% \setlength{\textfloatsep}{10pt}  % Adjust this value to control the space



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Legged locomotion is not just about mobility; it also encompasses crucial objectives such as energy efficiency, safety, and user experience, which are vital for real-world applications. However, key factors such as battery power consumption and stepping noise are often inaccurately modeled or missing in common simulators, leaving these aspects poorly optimized or unaddressed by current sim-to-real methods. Hand-designed proxies, such as mechanical power and foot contact forces, have been used to address these challenges but are often problem-specific and inaccurate. 
                        
In this paper, we propose a data-driven framework for fine-tuning locomotion policies, targeting these hard-to-simulate objectives. Our framework leverages real-world data to model these objectives and incorporates the learned model into simulation for policy improvement. We demonstrate the effectiveness of our framework on power saving for quadruped locomotion, achieving a significant 24-28\% net reduction in total power consumption from the battery pack at various speeds. In essence, our approach offers a versatile solution for optimizing hard-to-simulate objectives in quadruped locomotion, providing an easy-to-adapt paradigm for continual improving with real-world knowledge. Project page \tt{https://hard-to-sim.github.io/}.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Legged locomotion grants robots the ability to traverse complex terrains, offering universal mobility \cite{ha2024learning}. Beyond the goal of command following, broader considerations such as energy efficiency, payload capacity, user-friendly interaction, and safety are critical for extending their utility in industrial and everyday environments \cite{biswal2021development, DUFFY2003177, breazeal2004designing, hagele2016industrial}. For quadruped robots in particular, constraints such as limited battery life per charge significantly impact the effectiveness of robots in tasks like patrolling or rescue operations \cite{unitreego1, unitreego2, katz2019mini}. Moreover, issues like disruptive stepping noise can negatively affect user experience \cite{Trovato2018Sound}, and motor overheating poses risks to the robot's operational lifespan \cite{trujillo2009thermally}.

The current successes of learning-based approaches heavily depend on the sim-to-real transfer paradigm \cite{tan2018sim,peng2018sim}, where policies trained in simulations are directly applied to real-world robots \cite{hwangbo2019learning,xiong2024adaptive,agarwal2023legged,margolis2023walk,zhuang2023robot,margolis2024rapid,peng2020learning,escontrela2022adversarial}. These methods leverage physics simulators like Isaac Gym \cite{isaacgym}, MuJoCo \cite{todorov2012mujoco}, and Bullet \cite{coumans2019bullet}, which mainly focus on dynamics and kinematics. However, critical factors like power consumption, stepping noise and safety features are not available or inaccurately modeled. These factors are hard to simulate due to the complexity of the underlying mechanisms. 
For instance, the intricate dynamics of Permanent Magnet Synchronous Motors (PMSMs) pose significant challenges in predicting total power consumption \cite{kundur2007power,mellor2009computationally}. Furthermore, the implementation of control strategies, such as Field-Oriented Control (FOC), adds another layer of complexity to accurately forecasting motor power requirements \cite{unitreego1motor,wang2018advanced}.

Traditional approaches have utilized hand-designed proxies as reward functions for training policies, applying metrics like mechanical power or foot contact forces to approximate energy consumption or noise \cite{mahankali2024maximizing, yang2022fast, fu2021minimizing, rudin2022learning, margolis2023walk}. These methods demand expert knowledge and intensive tuning, and their efficacy is limited by the accuracy of the proxies used. In contrast, learning from real-world experience offers a more precise and efficient alternative for optimizing these challenging objectives.

In this paper, we introduce a data-driven fine-tuning approach to optimize hard-to-simulate objectives in locomotion policies. Our method begins with the collection of real-world data using a pre-trained policy. We then develop a measurement model to predict hard-to-simulate factors from this data, which is integrated into the simulation as a reward function. Our approach performs iterative policy improvement through cycles of data collection and policy updating.

We present experimental results demonstrating a significant 24-28\% reduction in total power consumption from the battery pack for quadruped locomotion. The task objective—minimizing power consumption while maintaining locomotion performance—highlights the operational time constraints faced by current low-cost quadruped robots \cite{unitreego1, unitreego2, minicheetahbattery}. Despite the complexities of modeling total power consumption \cite{seok2014design,wang2018advanced}, our method effectively manages these challenges, illustrating its potential to address demanding objectives in quadruped locomotion.

We believe that our proposed framework could be applied to a wide range of hard-to-simulate objectives. It utilizes a data-driven measurement model, which is designed to be objective-agnostic and has the potential to automatically adapt to various challenging objectives. Technically, our approach requires only minimal modifications to the existing sim-to-real pipeline, offering a plug-and-play method for integrating empirical data from the physical world to enhance locomotion performance.

\begin{figure*}[t]
\centering
\includegraphics[width=0.99\linewidth]{figures/method.pdf}
\caption{Fine-tuning procedure at the $i$-th iteration. Real-world data is collected using the pre-trained policy or the policy batch from the last iteration. A measurement model is trained to estimate hard-to-simulate factors based on the data, which is later integrated into simulation. Policy updates are performed in simulation, generating a batch of policies with varying configurations. Hierarchical policy selection is then conducted based on performance in simulation and real-world evaluations, determining policies retained for the next iteration.}
\label{fig:method}
\end{figure*}


\begin{algorithm}[t]
\caption{Fine-tuning hard-to-simulate objectives}
\begin{algorithmic}[1]
        \footnotesize
        \Require Pre-trained policy \pipretrained, an empty real-world dataset $\mathcal{D}_0$, measurement model training algorithm $\mathcal{A}: \mathcal{D} \mapsto f$, parameter search space for $w_\cdot$ and $c$: $\mathcal{W}$ and $\mathcal{C}$, and the size of the elite policy batch: $K$.      
        \State $\Pi_0^{\text{elites}} \gets \left\{ \pipretrained \right\}$.
        % for i in 1, 2, ...
        \For {$i = 0,  1, 2, \ldots$}
        \State{\textcolor{gray}{// \textsc{Real-world data collection:}}}
        \State{$D_i \gets \varnothing$}
        \For {$\pi \in \Pi_i^\text{elites}$}
        \State{Roll out $\pi$ in the real world to collect data, $\left\{\rms^\simulation_t, \rms^\real_t\right\}^{\pi}$}.
        \State{${D}_i \gets D_i \cup \left\{\rms^\simulation_t, \rms^\real_t\right\}^{\pi}$}.
        \EndFor
        \State{Accumulate all real data: $\mathcal{D}_i \gets \bigcup_{j=0}^{i} {D}_j$}.
        \State{Train a measurement model: $f_i \gets \mathcal{A}(\mathcal{D}_i)$}.
        \State{\textcolor{gray}{// \textsc{Simulation fine-tuning:}}}
        \State{$\Pi^\text{candidates}_i \gets \varnothing $}
        \For {$\pianchor, w_\cdot, c \in \left\{\pi_i^{\text{best}}, \pipretrained \right\} \times \mathcal{W} \times \mathcal{C} $} \label{alg:main:search}
        \State{$\pi^\text{fine-tuned} \gets$ fine-tune with the objective in Eq. \ref{eq:opt-sim}}.
        \State{$\Pi_i^\text{candidates} \gets \Pi_i^\text{candidates} \cup \left\{\pi^\text{fine-tuned}\right\}$}.
        \EndFor
        \State{\textcolor{gray}{// \textsc{Hierarchical policy selection:}}}
        \State{$\Pi^\text{elites}_{i+1} \gets$ top-$K$ of $\Pi^\text{candidates}$ measured by simulation performances}. 
        \State{$\pi^{\text{best}}_{i+1} \gets   $ best of $\Pi^\text{elites}_{i+1}$ evaluated by real-world performances}.
        \EndFor
\end{algorithmic}
\label{alg:main}
\end{algorithm}
\section{Related Work}

\paragraph{Direct learning from real-world experience}
Training policies directly in the real world can circumvent the complexities associated with modeling hard-to-simulate factors. However, previous methods \cite{ha2020learning, haarnoja2018learning, smith2022walk, smith2023grow} typically achieve results only in low-performance regions characterized by slow walking speeds. On the other hand, methods that fine-tune simulation-trained policies in real-world \cite{smith2022legged, lei2023uni, shi2024efficient} primarily addresses the shift in dynamics due to sim-to-real transferring. These studies often do not extend to optimizing objectives that are unseen in simulation pre-training. Furthermore, in contrast to these methods, which are generally tailored for specific learning algorithms \cite{hiraoka2021dropout, shao2020controlvae}, our approach offers a versatile integration across various policy optimization frameworks.




\paragraph{Augmenting simulation with data-driven models} 
Recent advances have embraced hybrid simulations, combining analytic physics with data-driven models to capture complex dynamics \cite{heiden2021neuralsim,qiao2021efficient,heiden2022inferring,sanchez2020learning,ajay2018augmenting}. Such hybrid simulations find applications in diverse robotic tasks, including legged locomotion \cite{hwangbo2019learning, margolis2023walk}, drone racing \cite{kaufmann2023champion},  and modeling human behavior in sports \cite{abeyruwan2023sim2real}. These studies, however, primarily focus on enhancing the fidelity of simulation \textit{dynamics}. In contrast, our setting requires data-driven models to be explicit \textit{objectives} for policy optimization, which exacerbates issues related to out-of-distribution and model exploitation.

\paragraph{Energy efficiency in quadruped robots} 
Total energy efficiency optimizations are typically reserved for robot hardware design \cite{seok2014design, katz2019mini, krimsky2024elastic}. On the controller side, previous works have employed various strategies to estimate and optimize energy consumption in legged locomotion. Techniques include using mechanical power and Joule heating as reward functions \cite{hwangbo2019learning, rudin2022learning, margolis2024rapid, margolis2023walk, agarwal2023legged, zhuang2023robot, yang2022fast, fu2021minimizing} or as constraints \cite{mahankali2024maximizing, kim2024not, chane2024cat}. Rather than relying on hand-designed proxies, our approach aims to minimize total power consumption through data-driven fine-tuning techniques.






\section{Fine-tuning Hard-to-Simulate Objectives}
\subsection{Problem Formulation}
We model locomotion as a Markov Decision Process (MDP) with state space $\mathcal{S} \subset \mathbb{R}^n$, action space $\mathcal{A} \subset \mathbb{R}^m$, transition function $p(\cdot \mid \rms, \rma) : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$, and reward function $r: \mathcal{S} \rightarrow \mathbb{R}$. The objective is to find a policy $\pi : \mathcal{S} \rightarrow \mathcal{A}$ that maximizes the expected sum of discounted rewards $\mathbb{E}\left[\sum_{t=0}^T \gamma^t r(\rms_t)\right]$, where $\gamma$ is the discount factor and $T$ is the time horizon. To address hard-to-simulate factors, we divide the state space into $\mathcal{S} = \mathcal{S}^\simulation \times \mathcal{S}^\real$, where $\mathcal{S}^\simulation$ represents states available in simulation and $\mathcal{S}^\real$ represents states observed only in the real world, i.e., the hard-to-simulate factors. 

We formulate the locomotion task as a multi-objective optimization problem \cite{sener2018multi}, highlighting objectives beyond mobility. For example, besides following velocity commands, the robot should minimize power consumption, ensure safety, and interact friendly with humans. To this end, we implement linear scalarization \cite{gunantara2018review} to combine multiple objectives into a single reward function, following common practice \cite{hwangbo2019learning, rudin2022learning, margolis2023walk}.
Specifically, we factorize the reward as:
$ r(\rms_t) = \sum_i w_i r_i^\simulation(\rms^\simulation_t) + \sum_j w_j r_j^\real(\rms^\real_t)$,
where $w_\cdot$ are the weighting factors for each objective. To summarize, the optimization problem of fine-tuning hard-to-simulate objectives is:
\begin{equation}
\begin{aligned}
\max_{\pi} & \mathop{\mathbb{E}}_{
        \substack{
                \rma_t \sim \pi(\cdot \mid \rms_t) \\
                \rms_{t+1} \sim p^\real(\cdot \mid \rms_t, \rma_t)
        } 
} \left[ \sum_{t=0}^T \gamma^t r(\rms_t)\right], \\
\text{where}  \quad & r(\rms_t) = \sum_i w_i r_i^\simulation(\rms^\simulation_t) + \sum_j w_j r_j^\real(\rms^\real_t).
\end{aligned}
\label{eq:opt}
\end{equation}
Note that our ultimate goal is to optimize real-world performance. Therefore, the expectation here is taken over the real-world transition function $p^\real$. And the rewards related to hard-to-simulate factors,   $r^\real_\cdot$, are calculated based on real-world measurements $\rms^\real_t$.
%, rather than model predictions, $\widehat{\rms^\real_t}$.

\subsection{Algorithm Design and Motivation}
The fine-tuning process is depicted in Figure~\ref{fig:method} and detailed in Algorithm~\ref{alg:main}. We employ a data-driven approach to model hard-to-simulate factors, training a measurement model to estimate these factors from real-world data. To tackle the issues of out-of-distribution data and potential model exploitation during fine-tuning, we impose constraints on the size of policy updates, as discussed in Section~\ref{sec:measurement}. Considering the incremental improvements from each update, we iteratively conduct policy learning and measurement model training. In each cycle, we sweep through multiple training configurations to adapt to the evolving characteristics of the measurement model (see Section~\ref{sec:iterative}). At the conclusion of each iteration, we systematically select the most effective policies through a hierarchical process, initially in simulation followed by real-world evaluations, as described in Section~\ref{sec:hierarchical}.

Our fine-tuning methodology consists of three steps: gathering real-world data, updating policies through simulation, and hierarchically selecting the most effective policies. This iterative process continues until the desired performance metrics are achieved.



\subsection{Data-driven Measurement Model}\label{sec:measurement}
To address the hard-to-simulate factors, we develop a measurement model that is trained end-to-end using real data. This model predicts these factors from observations available in simulation, denoted as $f: \rms^\simulation \mapsto \widehat{\rms^\real}$. 

The training of the measurement model is straightforward: we deploy a trained policy and collect pairs of observations and hard-to-simulate factors, $\mathcal{D}=\left\{\rms^\simulation_t, \rms^\real_t\right\}$. We then train the model $f$ with an algorithm $\mathcal{A}: \mathcal{D} \mapsto f$. E.g., to minimize the prediction error, $\mathbb{E}_{\mathcal{D}}\left[\left\| f(\rms^\simulation_t) - \rms^\real_t \right\|^2\right]$, on the real dataset. Subsequently, we integrate the measurement model into the simulation to generate rewards, $r^\real\left(f(\rms^\simulation_t)\right)$, for optimizing hard-to-simulate objectives.

However, the distribution of the training data for the measurement model is heavily dependent on the data-collecting policy. As the policy deviates from the data-collecting policy during fine-tuning, prediction accuracy may decrease due to out-of-distribution issues. Furthermore, since the model serves as an explicit objective, the optimization algorithm may exploit it. To address this, we constrain the policy update step size using a KL divergence penalty \cite{schmitt2018kickstarting}. This penalty encourages the policy to stay close to the data-collecting policy, ensuring the measurement model's reliability.

Therefore, the objective for policy optimization in simulation is:
\begin{equation}
\begin{aligned}
    \max_{\pi} \; & \mathop{\mathbb{E}}_{
            \substack{
                \rma_t \sim \pi(\cdot \mid \rms_t) \\
                \rms_{t+1} \sim \textcolor{gray}{p^\simulation(\cdot \mid \rms_t, \rma_t)}
            } 
    } \left[ \sum_{t=0}^T \gamma^t r(\rms_t) \right], \\
    \text{s.t.} \; & \mathop{\mathbb{E}}_{\rms \sim \pi}\left[\text{KL}\left(\pi(\cdot \mid \rms) \parallel \pianchor(\cdot \mid \rms)\right) \right] \leq c, \\
    \text{where} \; & r(\rms_t) = \sum_i w_i r_i^\simulation(\rms^\simulation_t) + \sum_j w_j \textcolor{cyan}{r_j^\real\left(f(\rms^\simulation_t)\right)}.
\end{aligned}
\label{eq:opt-sim}
\end{equation}
Here, \pianchor represents the policy initialization for fine-tuning, which is detailed in Section \ref{sec:iterative}. The KL divergence penalty is controlled by the parameter $c$. The hard-to-simulate factors are estimated using the measurement model $f$.

\subsection{Iterative Policy Fine-tuning} \label{sec:iterative}
Policy improvement is often limited by constraints on the update step size. To address this limitation, we employ an iterative approach that involves collecting real data and updating the policy multiple times. Additionally, we aggregate all data collected in previous iterations to train the measurement model, enhancing data diversity and mitigating the out-of-distribution issue \cite{abeyruwan2023sim2real}. This iterative process benefits both the modeling of hard-to-simulate factors and policy optimization, leading to improved overall performance.

Our empirical findings indicate that iterating with fixed hyper-parameters is suboptimal. This is because the appropriate weighting factors $w_\cdot$ and the KL divergence penalty $c$ depend on the characteristics of the measurement model $f$, and their optimal values may shift as the model evolves. Consequently, we adapt our policy learning approach in each iteration by varying configurations, sweeping through different combinations of $w_\cdot$ and $c$. Additionally, for the policy initialization, \pianchor, we vary it between the pre-trained policy \pipretrained{} and the best policy from the previous iteration $\pi^{\text{best}}$. Resetting the policy to \pipretrained{}—but updated with the improved measurement model $f$—provides extra flexibility and adaptation capabilities \cite{nikishin2022primacy,schwarzer2023bigger,smith2023grow}.

\subsection{Hierarchical Policy Selection}\label{sec:hierarchical}

Policy fine-tuning in simulation using parameter sweeping generates multiple policy candidates. Deciding which policies to retain for subsequent iterations poses a challenge due to the sim-to-real gap, which leads to discrepancies between real-world and simulation performances. Formally, differences between $p^\real$ and $p^\simulation$ lead to variations in the objectives, as denoted by Eq.~\ref{eq:opt} and Eq.~\ref{eq:opt-sim}. Therefore, we propose a hierarchical policy selection method: first, we select the top-$K$ policies in simulation, and then we further select the best-performing policy in the real world.

Initially, we select a set of elite policies, ${\Pi}^\text{elites}$, based on their performance in simulation. Subsequently, we deploy ${\Pi}^\text{elites}$ for real-world evaluations. The top-performing policy in the real world, $\pi^{\text{best}}$, is then designated as the anchor policy for the next iteration. Concurrently, the real-world data collected from ${\Pi}^\text{elites}$ is incorporated into $\mathcal{D}$, further enhancing the training of the measurement model.


\section{Experimental Design}

To evaluate the efficacy of our framework, we concentrate on optimizing the challenging objective of total power savings. This metric is crucial in real-world applications as it determines the operating duration per charge for quadruped robots \cite{unitreego1, unitreego2, katz2019mini}. The accurate simulation of total power consumption poses significant challenges due to the intricate energy interactions among different subsystems \cite{seok2014design} and the complex, time-variant dynamic of Permanent Magnet Synchronous Motors (PMSMs) \cite{unitreego1motor,kundur2007power,mellor2009computationally}.

Conventionally, optimization for power saving focuses on hand-designed proxies that represent the \textit{analytical} power consumption, including mechanical power and Joule heating. 
In contrast, our approach targets \textit{total} power consumption, which refers to the energy drawn directly from the battery pack.
% We compute this as: $ P = \int_{t=0}^{T} u_t i_t \, dt $, where $u_t$ and $i_t$ represent the voltage and current from the battery at time $t$, respectively.  
% As direct measurements of total power consumption are not feasible in simulations, we optimize a data-driven proxy based on power estimates predicted by a learned model.

\subsection{Implementation of Our Framework and the Baseline}
\label{sec:implementation}
To estimate total power consumption, we develop a data-driven model to predict instantaneous currents $i_t$, as the voltage remains stable within a test run. Inputs to the model are motor torques and angular velocities, and we use an LSTM network, \( f \), for predictions: \( \widehat{i_t} = f(\tau_t, \dot{q}_t) \). All features and output currents are normalized using their means and standard deviations.

The locomotion policy is initially pre-trained in simulation environments with reward functions from \cite{rudin2022learning} and terrain and command curricula from \cite{rudin2022learning} and \cite{margolis2024rapid}. For fine-tuning, the real-world objective in Eq. \ref{eq:opt-sim} focuses on power saving, defined as $r^\real\left(\widehat{i_t}\right) = -\widehat{i_t}$. We apply the reward centering technique \cite{naik2024reward} to deal with statistical shifts of measurement models. The current estimation model $f$ is integrated into the simulation for current predictions, and policy updates are implemented using the Proximal Policy Optimization (PPO) algorithm \cite{schulman2017proximal}.

We simplify the combination of reward functions as \( r = r^\simulation + \lambda r^\real \). In each iteration, we adjust the reward weightings, \( \lambda \in \{0.5, 1, 5\} \), and the KL divergence penalty, \( c \in \{0.2, 0.5, 1\} \). The top-6 policies, ranked by their unweighted mean energy reward in simulation, are selected as the elite set $\Pi^{\text{elites}}$. These \(\Pi^{\text{elites}}\) will be used for real-world evaluations. The best-performing policy (\( \pi^{\text{best}} \)) in the real world--measured by total power consumption--becomes the anchor for the subsequent iteration. Data is concurrently recorded during evaluation to train the measurement model.

Conversely, the baseline fine-tunes an identical pre-trained policy in simulation but with an analytical energy reward: $r^\real \left(\rms^\simulation_t\right) = -\sum_{i=0}^{12}\max \left(\tau_i \dot{q}_i + \frac{r}{k^2}\tau_i^2, 0\right) $, where $\tau_i$ and $\dot{q}_i$ denote the torque and angular velocity of the $i$-th motor, and $r,k$ are motor constants \cite{yang2022fast,mahankali2024maximizing}. The simulation setup and policy optimization procedures mirror those used in our framework's fine-tuning phase, differing only in the energy reward function.

The hyper-parameter search space for the baseline is expanded to \( \lambda \in \{5 \times 10^{-5}, 1 \times 10^{-4}, 5 \times 10^{-4}, 1 \times 10^{-3}, 5 \times 10^{-3}\} \) and \( c \in \{0.1, 0.2, 0.5, 1.0, 5.0, \infty\} \). We fine-tune the policy using the same number of samples accumulated across all iterations of our framework. Subsequently, the top 24 policies, evaluated by the analytical energy metric, are tested in the real world, and we report the highest power reduction observed. Note that the number of real-world evaluated policies for the baseline is equal to that of our framework (\(6 \text{ elite} \times 4 \text{ iteration}\)).

\subsection{Power Measurement and Metric}
\label{sec:power-measurement}
We assess the framework by deploying the trained policies on a Unitree Go1 robot, which is commanded to maintain a constant forward velocity \( v \). Power consumption is monitored by measuring the battery's current draw at 50 Hz using the Unitree SDK, while the robot's speed is tracked using an Intel RealSense T265 camera. Each policy's power usage is evaluated over 160 seconds by calculating and averaging the current integrals from all 1-second segments that meet the velocity criteria (within $\pm 10\%$ of $v$).


For fair comparisons, we conduct direct `head-to-head' tests between the pre-trained and fine-tuned policies. These policies are alternated every 80 seconds without delay on the same robot to control for measurement variability and environmental factors. 

The primary metric is the percentage reduction in power consumption of the fine-tuned policy compared to the pre-trained one, given by \( \Delta P = \frac{P^\text{pre-trained} - P^\text{fine-tuned}}{P^\text{pre-trained}} \times 100\% \). We report both the \textit{gross power reduction} and the \textit{net power reduction}, with the latter adjusting to isolate the power consumed by locomotion processes. Specifically, we subtract the power measured when all motors are idle.

We report power reduction results at multiple speeds: $v=0.5, 0.8, 1.1$ m/s. However, for hierarchical selection during iterations, we only consider performances at \(v = 0.8\) m/s. The performance at other speeds is evaluated using the final policy after the fine-tuning process is completed.


\begin{table}[t]
\centering        
\def\arraystretch{1.2}
\begin{tabular}{l|ccc}
\toprule
 & $v=0.5$m/s & $v=0.8$m/s & $v=1.1$m/s \\ \hline
\makecell{Analytical \\ proxy} & 11.8\% (8.3\%) & 6.2\% (4.5\%) & 5.0\% (3.9\%) \\ 
\hline
\makecell{Data-driven \\ proxy  (ours)} & \textbf{28.4\% (19.6\%)} & \textbf{27.0\% (20.3\%)} & \textbf{24.2\% (19.4\%)} \\ 
\bottomrule
\end{tabular}
\caption{Comparison of net (gross) power reduction between our framework with data-driven modeling and the baseline with the analytical proxy.}
\label{tab:final-results}
\vspace{-2em}
\end{table}

\begin{figure*}[t]
\begin{subfigure}{0.15\linewidth}
\centering
\includegraphics[width=0.99\linewidth]{figures/indoor-env.jpg}
% \vspace{2em}
\caption{Indoor evaluation.}
\label{fig:indoor-env}
\end{subfigure}
\begin{subfigure}{0.33\linewidth}
\centering
\includegraphics[width=0.99\linewidth]{figures/indoor_soc.pdf}
\vspace{-1.8em}
\caption{SoC during the in-door long-distance evaluations (averaged over 3 runs). }
\label{fig:indoor-soc}
\end{subfigure}
% \\
\begin{subfigure}{0.15\linewidth}
\centering
% \includegraphics[width=0.99\linewidth]{figures/indoor_env.jpg}
\includegraphics[width=0.99\linewidth]{figures/outdoor-env-0.jpg}
\includegraphics[width=0.99\linewidth]{figures/outdoor-env-1.jpg}
\includegraphics[width=0.99\linewidth]{figures/outdoor-env-2.jpg}
% \vspace{0.1em}
\caption{Outdoor evaluation.}
\label{fig:outdoor-env}
\end{subfigure}
\begin{subfigure}{0.35\linewidth}
\centering
\includegraphics[width=0.99\linewidth]{figures/outdoor_soc.pdf}
\caption{SoC during the outdoor long-distance evaluations. }
\label{fig:outdoor-soc}
\end{subfigure}
\caption{In-the-wild evaluation environments and battery life improvements.}
\vspace{-2em}
\label{fig:in-the-wild}
\end{figure*}


\begin{figure}[t]
\centering
\includegraphics[width=0.79\linewidth]{figures/power_dist.pdf}
\caption{Distribution of normalized net power ($\frac{P^\text{fine-tuned}}{\operatorname{Mean}\left(P^\text{pre-trained}\right)}$) for the policies fine-tuned with the baseline  and our framework. Each point represents a 1-second segment within the test run.}
\label{fig:power-dist}  
\vspace{-2em}
\end{figure}


% \begin{figure}[t]
% \centering
% \begin{subfigure}{0.32\linewidth}
% \centering
% \includegraphics[width=0.99\linewidth]{figures/v=0.5.png}
% \caption{$v=0.5$ m/s}
% \end{subfigure}
% \begin{subfigure}{0.32\linewidth}
% \centering
% \includegraphics[width=0.99\linewidth]{figures/v=0.8.png}
% \caption{$v=0.8$ m/s}
% \end{subfigure}
% \begin{subfigure}{0.32\linewidth}
% \centering
% \includegraphics[width=0.99\linewidth]{figures/v=1.1.png}
% \caption{$v=1.1$ m/s}
% \end{subfigure}
% \caption{Behaviors of the pre-trained policy (left) and our fine-tuned policy (right) at different speeds.}
% \label{fig:behaviors}
% \end{figure}




\section{Main Results: Power Reduction Comparison}
We aim to demonstrate the effectiveness of our framework in optimizing hard-to-simulate objectives, focusing on total power savings. We first present the quantitative results of the metric described in Section~\ref{sec:power-measurement}, showing the net and gross power reductions achieved by our framework and the baseline. Additionally, we discuss qualitative insights into the robot's behavior changes. Finally, we conduct an in-the-wild study, evaluating battery life improvements in both indoor and outdoor long-distance tests.

\paragraph{Quantitative Results}
The final results, as summarized in Table~\ref{tab:final-results}, demonstrate that our framework achieves a significant net power reduction of 24--28\% compared to the pre-trained policy. This reduction is noteworthy, even when considering the gross power, which includes power consumption not directly related to locomotion, with our framework achieving a 20\% decrease. In contrast,  the baseline—despite thorough parameter tuning and policy selection (see Section~\ref{sec:implementation}) —still falls short, exhibiting only a 12\% net reduction at a lower speed ($v = 0.5$ m/s) and a marginal power saving (about 5\%) at higher speeds.

Figure~\ref{fig:power-dist} further illustrates these points by showing the integrated power consumption for each 1-second segment of the test run. The distribution of normalized net power, defined as $\frac{P^\text{fine-tuned}}{\operatorname{Mean}(P^\text{pre-trained})}$, highlights a consistent reduction in power consumption across all segments when using our fine-tuned policies. Conversely, the baseline policies display a more variable distribution, with some segments even surpassing the power consumption levels of the pre-trained policy.

\begin{figure}[t]
\centering
\begin{subfigure}{0.7\linewidth}
\centering
\includegraphics[width=0.99\linewidth]{figures/torque-RL.pdf}
\caption{Joint torque profiles of the rear-right leg for pre-trained and fine-tuned policies.}
\label{fig:torque}
\end{subfigure}
\hfill
\begin{subfigure}{0.27\linewidth}
\centering
\includegraphics[width=0.99\linewidth]{figures/front-leg.001.jpeg}
\caption{The fine-tuned policy (right) yields a more natural front leg stance compared to the pre-trained policy (left).}
\label{fig:front-leg}
\end{subfigure}
\caption{Comparison of joint torque profiles and front leg stance between pre-trained and fine-tuned policies.}
\vspace{-2em}
\end{figure}


\paragraph{Behavioral Changes}
After fine-tuning with our framework, the robot exhibited increased compliance in its walking gait (see the accompanying video). Figure~\ref{fig:torque} illustrates the torque profiles of the rear-right leg's joints during a 0.8~m/s walk. The fine-tuned policy demonstrates smoother torque transitions and reduced amplitudes at critical joints. Notably, the front legs shifted from an outward-pointing stance to a position closer to the body (see Figure~\ref{fig:front-leg}). These adaptations indicate that the fine-tuned policy is effective in uncovering behaviors that enhance energy efficiency.

% Conversely, policies that scored well using the analytical proxy exhibited unnatural behaviors during real-world evaluations, including jerky movements and sporadic instability. These issues led to higher energy consumption than the pre-trained policy. Interestingly, the baseline's most energy-efficient policy showed only minimal deviations from the pre-trained policy but achieved limited power reduction. This discrepancy underscores the limitations of analytical proxies in accurately capturing complex, hard-to-simulate factors, as illustrated in Figure~\ref{fig:proxy-comparison} and discussed further in Section~\ref{sec:analysis}.




\paragraph{In-the-Wild Study}
To replicate conditions resembling realistic, uncontrolled usage scenarios, we conducted in-the-wild evaluations both indoors and outdoors, focusing on the battery's state-of-charge (SoC) relative to the distance traveled. The indoor evaluation involved a 400-meter journey around a tiled corridor (see Figure~\ref{fig:indoor-env}), while the outdoor evaluation encompassed a 500-meter route around a platform among office buildings (see Figure~\ref{fig:outdoor-env}). These long-distance evaluations required the policy to effectively manage energy over extended periods.

We initiated these tests by first fully charging the battery pack, then discharging it to 81\% for indoor tests and 49\% for outdoor tests. Starting from a fully charged battery was not feasible due to the time required to boot the programs and set up the hardware. The robot was controlled using a PID controller to maintain constant speeds. For the indoor evaluations, the target speeds were uniformly sampled from $v \in [0.5, 1.1]$ m/s, with speed adjustments every 20 seconds. For outdoor evaluations, the robot maintained a constant speed of 0.8 m/s.

Results, illustrated in Figure~\ref{fig:in-the-wild}, indicate that our fine-tuned policy maintained the highest final SoC under both indoor and outdoor conditions. The SoC of all tested methods started similarly, but as the distance traveled increased (beyond 100 meters), our fine-tuned policy exhibited a slower decline. These findings suggest that our framework not only conserves power under less controlled real-world conditions but also effectively extends battery life in more realistic usage scenarios.


\paragraph{Summary}
Our proposed framework effectively optimizes hard-to-simulate objectives, resulting in significant power savings across a range of commanded speeds. This optimization is reflected in the increased compliance observed in the robot's behaviors. Moreover, our in-the-wild evaluations further demonstrate the practical benefits of our framework, showcasing enhanced battery life in real-world scenarios.



\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/iterative_net_improvement.pdf}
\caption{Net power reduction over iterations. }
\vspace{-2em}
\label{fig:iterative-net}
\end{figure}


\section{Analysis}
\label{sec:analysis}
In this section, we analyze the core components of our framework. We begin with a detailed discussion on the iterative fine-tuning process, followed by a comparative analysis between our data-driven model and the analytical proxy.

\paragraph{Iterative Fine-tuning Process}
Figure~\ref{fig:iterative-net} illustrates the net power reduction achieved by our framework across successive iterations. The learning curve shows steady incremental improvements during the first three iterations, which supports the hypothesis that performance benefits from the accumulation of real-world data. This trend demonstrates robustness across all tested operational speeds. We attribute the effective adaptation of hyper-parameters to a finely balanced update step size and the reliability of the measurement model.

Convergence is achieved after four iterations, equivalent to 384,000 real-world samples (calculated as $4 \times 6 \times 16,000$). The final iteration shows a slight decrease in performance, likely due to the staleness of earlier samples. These samples, generated by older versions of the policy, become less informative over time. Consequently, the measurement model allocates capacity to memorizing these outdated samples, ultimately reaching its limit. Nonetheless, having achieved the desired level of power reduction, we conclude the iterative process at this point.

\begin{figure}[!t]
\centering
\includegraphics[width=0.89\linewidth]{figures/analytical_vs_real_power_improvement.pdf}
\caption{Comparison of the effectiveness of the analytical proxy versus our data-driven model in net power reduction. The shaded area represents the confidence interval of the regression line.}
\vspace{-2.0em}
\label{fig:proxy-comparison}
\end{figure}

\paragraph{Data-driven vs. Analytical Proxy}
The role of the optimization proxy is critical in fine-tuning for factors that are challenging to simulate accurately. It serves as both the target for policy optimization and the criterion for initial policy selection. We compare the effectiveness of our data-driven model against the analytical proxy, focusing on their ability to accurately predict real-world power consumption.

Figure~\ref{fig:proxy-comparison} plots the power reductions measured in the real world against those predicted by both proxies. For the analytical proxy, we include all policies tested in the real world, noting that several were excluded due to deployment failure. The data-driven proxy data comprises the elite set $\Pi^\text{elites}$ from the last two iterations, using the corresponding measurement models as the proxy.

The data-driven proxy overall exhibits a positive correlation with actual power reductions observed in real-world operations, indicating its reliability and informativeness. In contrast, the analytical proxy generally shows a negative correlation, where many policies predicted to improve efficiency actually increase power consumption. This discrepancy highlights the shortcomings of the analytical proxy in capturing real-world dynamics, potentially leading to misguided policy optimization and selection.

Despite the general efficacy of the data-driven model, discrepancies between its predictions and the actual power reductions remain, highlighting the necessity for real-world testing of multiple policies to ascertain and retain the most effective policy. This requirement supports the incorporation of a hierarchical policy selection mechanism within our framework.


\section{Conclusions}
In this work, we have developed a fine-tuning methodology aimed at enhancing the performance of quadruped locomotion. This methodology specifically optimizes hard-to-simulate objectives by leveraging a data-driven measurement model. Our approach demonstrates a substantial reduction in total power consumption compared to traditional methods that rely on analytical proxies. Importantly, our framework requires minimal adaptations for integrating real-world knowledge into existing locomotion pipelines, enhancing its practical applicability.

Our study has several limitations. Algorithmically, the sim-to-real gap introduces out-of-distribution (OOD) challenges that compromise the reliability of our measurement model. Additionally, the framework requires a relatively large dataset of real-world samples to achieve convergence. Experimentally, due to hardware constraints, we were unable to test the transferability of our framework to other robot models. Furthermore, our experimental evaluation was limited to flat terrain, as measuring speed on more complex surfaces—such as slopes and stairs—presents additional challenges. Lastly, our focus was primarily on reducing total power consumption. Nevertheless, we are optimistic that, with minimal modifications, our framework could be extended to address other hard-to-simulate objectives.



% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{ACKNOWLEDGMENT}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, IEEEfull}{}



\end{document}
