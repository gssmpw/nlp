\section{Related Work}
\label{sec:related}



Transformers \cite{attention_is_all_you_need} have revolutionized NLP, significantly advancing zero-shot and few-shot capabilities in language and vision models. This has led researchers to explore the application of transformers to time-series forecasting, leading to a substantial body of work including but not limited to \cite{ haoyietal-informer-2021, wu2021autoformer, zhou2022fedformer,logtrans, Yuqietal-2023-PatchTST, liu2022pyraformer, zhang2023crossformer}.  Informer by \cite{liu2023itransformer} introduces the ProbSparse attention mechanism, which alleviates the quadratic complexity of the naive transformer to log-linear complexity, to mitigate the scalability issues in long sequence time-series forecasting (LSTF). \cite{zhou2022fedformer} uses the sparsity of the time-series in the fourier domain to enhance the performance in LSTF. PatchTST \cite{Yuqietal-2023-PatchTST} uses patching with overlapping strides as a tokenization mechanism to address issues associated with naive tokenization of time-series data. This approach yields patch-based tokens that are interpretable while maintaining channel independence, treating each channel as univariate but facilitating joint learning across all channels through the same set of shared weights. Our architecture deviates from PatchTST by incorporating convolutional layers before patching and using channel-mixing to capture interactions between tokens from different channels. The advantages of convolutions are highlighted in the speech literature by \cite{wav2vec,hsu_conv}. On the other hand, iTransformer \cite{liu2023itransformer} treats each variate as a single token, showing the potential benefits of utilizing inter-channel relationships.


In zero-shot forecasting, a line of work has emerged \cite{orozco, Oreshkin_Carpov_Chapados_Bengio_2021, domain-adapt, dooley2023forecastpfn, ansari2024chronos}. More recently, \cite{ansari2024chronos} has developed novel tokenization methods, employed quantization, and made time series data resemble language, enabling the training of LLM architectures for probabilistic univariate forecasting in a framework called Chronos. Chronos employs a data augmentation technique called KernelSynth, which generates synthetic time-series data using Gaussian processes to improve the generalization capability of the model. Meanwhile, another line of work, ForecastPFN \cite{dooley2023forecastpfn}, is trained entirely on a synthetic dataset following the framework of Prior-data Fitted Networks (PFNs). Initially proposed by \cite{muller2022transformers}, PFNs are designed to approximate Bayesian inference. Another study \cite{verdenius2024lat} integrates Joint-Embedding Predictive Architectures with PFNs for zero-shot forecasting. In addition to the mentioned models, Mamba4Cast \cite{bhethanabhotla2024mamba4castefficientzeroshottime} is also trained entirely on synthetic data using the Mamba architecture as its backbone \cite{gu2024mambalineartimesequencemodeling}. While the mentioned literature addresses univariate settings, our work introduces the first multivariate Prior-data Fitted Network, to the best of our knowledge, featuring an architecture that enables strong zero-shot and few-shot performances on MTS forecasting. 
%%%%END%%%%%%

%%%%%%METHOD%%%%%