\section{Implementation Details}
We implemented \name entirely in PyTorch. We optimized the pretraining task using a synthetic dataset generated with LMC-Synth, employing the Adam optimizer \cite{KingBa15} and adhering to a one-cycle learning rate policy with a maximum learning rate of $lr=0.0005$ \cite{onecycleLR}. In the few-shot evaluations, we fine-tuned the \name with maximum $lr=0.0002$ using AdamW optimizer \cite{loshchilov2019decoupledweightdecayregularization} with one-cycle learning rate policy. In training \name with synthetic dataset, we observed that making model see the independently generated channels first, corresponding to the case with $C_i(\vct{t}) =  l_i(\vct{t})$, then introducing the inter-channel dependent data, significanly improves the learning speed. The explanation is simple, with the case with $C_i(\vct{t}) =  l_i(\vct{t})$, the model sees much more time-series patterns, as the time-series channels are all independently generated by Gaussian processes. Thus, after the model learns to make channel-independent decisions, we introduced the channel-dependent data, similar to curriculum learning scenario \cite{curriculum_learning}. Moreover, while training on synthetic data, we added a multiplicative Gaussian noise to each MTS data point as a regularization, with $\sigma = 0.1, mean = 1$.  In the end, \name is trained on 1.5 million MTS data points with 160 channels. 

In \name, we used the token embedding dimension of 256, and the latent space dimension of 1024, while the feed-forward network dimension is set to 512. We did not do any hyperparameter tuning and chose those values from checking similar works such as \cite{Yuqietal-2023-PatchTST, liu2023itransformer}. In fine-tuning, we always used the same learning rate with the same number of epochs accross different datasets (0.0002 and 8 epochs). Thus, we run the evaluations once. While doing all those, we fixed the seed to a random value of 2023.  

Throughout the experiments, we used a single L40S GPU. In addition the GPU, we had access to 128 GB of RAM and a 32-core CPU, which facilitated the acceleration of synthetic time-series data generation. Our codebase is developed based on \cite{liu2023itransformer}. Overall, we used approximately 300 GPU hours, as we conducted benchmarks not only for our own model but also for many others. We are providing the full source code for \name, including the synthetic data generation, architecture, and the training and evaluation scripts. Furthermore, we are providing the model weights of \name.