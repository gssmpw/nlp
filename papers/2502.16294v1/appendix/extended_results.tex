\newpage
\section{Extended Results}

\input{tables/main_table_app}
\input{tables/univariate_bigger}

In addition to the budget scenarios presented in the main body, we also conducted experiments with data budgets of 100 and 1,000 to fully characterize our experimental results. Furthermore, the average accuracy across these data budgets is provided for reference. Table 5 showcases all these evaluations. In Table 6, we present the raw results of the univariate forecasting task for zero-shot forecasting.

\subsection{Multivariate Forecasting}
As shown in Table 5, \name consistently achieves the best results with a data budget of 100 and significantly outperforms all other models with a budget of 1,000, leading in 7 out of 9 datasets. \name excels particularly in datasets with a multivariate nature. Consider that PatchTST \cite{Yuqietal-2023-PatchTST} assumes channel independence, whereas iTransformer \cite{liu2023itransformer} treats each variate as a token, demonstrating extreme channel dependence. In the full budget scenario, where the entire dataset is utilized, the difference in forecasting performance between iTransformer and PatchTST is revealing, particularly in detecting datasets with high inter-channel dependencies. For instance, in the ECL and Traffic datasets, which have a large number of variates (which does not mean high channel dependence by itself), iTransformer shows superior forecasting performance compared to PatchTST. Conversely, in the ETT datasets, PatchTST performs comparatively better. Extrapolating from there, we realize that \name excels in datasets with a high multivariate nature, even in full budget scenarios, and also yields good and competitive performance in datasets with comparatively low multivariate characteristic in full budget scenarios. With limited budgets, we see that \name is the leading model among the baselines.  

\subsection{Univariate Forecasting}
Although \name is specifically designed for multivariate time series forecasting, we also assessed its performance in zero-shot univariate forecasting, compared to similar models. See Table 6 for more details. On average, \name-36 is the most successful model among other models, and uniformly better than all other deep-learning based baselines in our setting. Generally, Chronos-small \cite{ansari2024chronos} outperforms \name-36 with shorter prediction lengths, while TimePFN-36 excels at longer prediction lengths, outperforming the other models. This outcome is expected, as \name is specifically trained to handle an input length of 96 and predict the same distance ahead. For these comparisons, we trimmed \name's predictions to match the given prediction lengths. Given \name's focus on longer prediction horizons, it's no surprise that Chronos-small performs better at shorter lengths. For \name-36, we padded the first 60 sequences of the 96-sequence input with the average of a 36-sequence input to minimize distribution shift. We also included results for \name-96, which uses the full 96-sequence input length without padding, to demonstrate our modelâ€™s complete performance. 














