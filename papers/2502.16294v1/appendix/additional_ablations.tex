\section{Additional Ablations}
\input{tables/ablation_TimePFN_synthetic_training}

\input{tables/ablation_iTransformer_z_sh}

\input{tables/iTransformer_PFN}

In addition to the ablation studies we provided, we include three additional case studies, with one focusing on the performance of the architecture of \name when it is not pretrained synthetic data, and the second one focuses on the performace of iTransformer \cite{liu2023itransformer} when it is used as the PFN backbone in zero-shot setting. In the third case study, we evaluate this iTransformer-PFN compared to iTransformer in various data budgets to demonstrate the generality of the framework of large-scale synthetic training and to demonstrate the architectural novelties of \name. Table 7, Table 8  and Table 9 contain the results of those ablation studies, respectively. 

\textbf{Pretraining TimePFN with LMC-Synth.} To better understand the impact of synthetic training in \name, we removed the synthetic training component and directly trained the architecture, which we referred to as \name-w/o-synthetic. As shown in Table 7, the forecasting performance of \name is significantly better than that of \name-w/o-synthetic, demonstrating its contribution in both full budget and limited budget settings.

\textbf{iTransformer as PFN.} To evaluate the performance of other architectures with prior-data-fitting, we trained an iTransformer architecture with LMC-Synth in addition to PatchTST-PFN. As shown in Table 8, compared to other variations, \name demonstrates significantly better zero-shot forecasting capability, with uniformly better results than those of the competing architectures. This supports our architectural design principles, involving 1D convolutions and channel-mixing. 


\textbf{iTransformer-PFN with Data Budgets.} To demonstrate the behavior of another architecture with synthetic training, we pre-trained the iTransformer \cite{liu2023itransformer} with data generated by LMC-Synth and fine-tuned it using specified data budgets. As shown in Table 9, synthetic pre-training improves the performance of the model in most cases, demonstrating the generality of the framework. However, the contribution is limited when using the entire dataset, in contrast to those instances with \name, thereby highlighting \name's superior performance.