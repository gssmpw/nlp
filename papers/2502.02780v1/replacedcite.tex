\section{Related Work}
\label{sec:related}

%xyz: Need to avoid overlapping with the intro and other sections. (ok)


Our work draws inspirations from and advances the knowledge in the following three categories of research.


\subsection{Generative Agents in HCI}

% Agents for HCI

The rapid advancement of LLMs has inspired a wide range of HCI applications, including social behaviors (Generative Agents ____), virtual reality ____ with tour guidance (VirtuWander ____), Human-AI collaboration (AI Chains ____, ____), creative tasks (CharacterMeet ____, Luminate ____, C2Ideas ____, ABScribe ____, AngleKindling ____, PopBlends ____), healthcare (MindfulDiary ____, ChaCha ____, Narrating Fitness ____, ____) with health intervention (MindShift ____, ____), web interaction ____ with UI design (ReactGenie ____, ____, ____), coding (CollabCoder ____, ____), behavioral change (CatAlyst ____,____) with human augmentation (Memoro ____, ____), business (Marco ____), and so on.

% Agents for Education

In educational context, LLMs-powered agents have been utilized to serve as teachable agents (Mathemyths ____, ____, ____) to provide instructions ____, recommend learning concepts ____, and give feedback ____. For example, DevCoach ____ supports students' learning in software development at scale with LLMs-powered generative agents. ReadingQizMaker ____ proposes a Human-NLP collaborative system which supports instructors to design high-quality reading quiz. In programming education, CodeTailor ____ uses LLM-powered personalized parsons puzzles to support engagement in programming. PaTAT ____ presents a human-AI collaborative qualitative coding system using LLMs for explainable interactive rule synthesis. 

Such existing work either uses generative agents as the instructors to directly teach students ____ or serves as student agents ____ to augment intelligent tutoring systems. 
%xyzr1: how does our work differ?
Our work focuses on the second aspect. In what follows, we specifically discuss related work in student simulation using either machine learning or generative agents.


\subsection{Student Simulation}

Student simulation aims to predict student learning behaviors in education, thus providing insights for supporting intelligent tutoring systems ____. A majority of existing research formulates student simulation as a knowledge tracing problem, i.e. predicting students' future learning performance based on their past records  ____. This learning performance is usually represented by the question answering accuracy in the course to measure students' skill levels for specific course concepts. Early work in this domain employed classical Bayesian models ____. In recent years, deep learning models ____ have been the predominant approach for knowledge tracing, combining graph models ____, cognitive theories ____, memory-augmented components ____, and so on.
% In what follows, we introduce related work using LLMs-based approaches for student simulation.

\subsection{LLM-based Student Simulation}

Recent work has explored the feasibility of using LLMs directly for predicting students' learning performance ____ or for knowledge tracing ____ in open-ended questions ____. 
%
These methods have better explainability than deep learning models, owing to LLMs' capability to reveal the reasoning process____.
%
They can also augment deep learning-based knowledge tracing ____.
%
In HCI, researchers have developed multi-agent collaboration environment to simulate the whole classroom interaction ____ and used LLM-simulated student profiles to support question item evaluation ____. These approaches can enable adaptive and personalized exercise generation to augment student learning performance ____.
\mytextcolor{For example, Sarshartehrani et al. ____ further leveraged embodied AI tutors for personalized educational adaptation.
GPTeach ____ also demonstrated the feasibility of using GPT-based virtual students for interactive TA training. In addition, MATHVC ____ explored the effectiveness of LLM-simulated multi-character virtual classroom for mathematics education. Moreover, LLMs can help students engage in post-lesson self-reflection ____ and also support language learning and growth mindset cultivation ____.}




However, there is also evidence ____ showing the 
%xyzr1: What limitation? Low accuracy? 
limitation of LLMs in student performance prediction compared with deep learning (DL) models. We argue that this is mainly because of the lack of contextual course materials. Specifically, existing LLM-based approaches ____ simply treat student simulation as a sequence prediction problem, which predicts future test performance based on past records, ignoring the modulation effect of course materials. In this case, DL models are very likely to work better than LLMs owing to their capacity to learn from historical data ____. In contrast, LLMs are better at few-shot contextual learning based on their large pretrained \textit{knowledge base} ____. Therefore, incorporating contextual course materials could better unleash the power of LLMs to capture the potential effects of course materials on learning performance even with limited data, thus enabling more accurate student simulation.

Existing work in this respect is quite limited, due to both the model limitations and the lack of dataset containing course materials (Section \ref{sec:intro}). 
EduAgent ____ incorporated course materials to simulate students' cognitive states and post-test performance, but the lectures' duration was too short (5 minutes) to represent the learning process across a typical lecture. By contrast, our work conducts longer-term experiments (6-week, 12 lectures, 1 hour per lecture) to collect high quality learning behavioral data. Our study employs a self-developed online education system integrating sensing techniques and action recommendations to help instructors increase students' learning engagement. Moreover, our proposed transferable iterative reflection module further augments the student simulation performance for both finetuning-based and prompting-based models, which departs from existing approaches in language models ____ and deep learning models ____.


% One reason is the lack of such fine-grained datasets that contain annotations of course materials with corresponding student performance. Most existing datasets (such as Ednet ____, Junyi ____ and Assistments 2009-2010 benchmark dataset\footnote{https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data}) only contain question materials without course materials. EduAgent dataset ____ indeed contains the course stimuli data, but the lecture length is too short (5-min) to reveal the student learning process in the whole lecture. To tackle this problem, we run a new user study in the form of a 6-week online education workshop including 12 lectures (1 hour per lecture) and collected fine-grained annotations of course materials and student learning performance. Another challenge is that data collection can not easily guarantee the data quality since students' engagement may drop during learning ____. We solve this problem by developing a new online education system that integrates sensing techniques to monitor students' learning behaviors and prompt instructors to take recommended actions to increase students' learning engagement and thus improve data quality. 

% Another reason that prevents student simulation from integrating course materials is the limitation of existing language models to deal with long contextual data. 
% %
% Existing work has demonstrated the effectiveness of using BERT-based language models for student performance prediction ____. However, BERT has very low token limits (512 tokens), which can only deal with simple textual input but fails to capture complex contextual information such as course materials. 
% %
% LLMs such as GPT4 could support longer contextual text input, but their performance also drops when the context is too long and overwhelming ____.
% Inspired by the self-reflection ability of LLMs ____ to distill knowledge ____, our framework solves this problem by introducing a transferable iterative reflection (TIR) module which first asks LLMs to perform reflections on specific course materials and compress the learnt knowledge to augment LLMs-based student simulation. Different from a straightforward self-reflection ____, our TIR module uses an interactive architecture between a novice agent and reflective agent to ensure that the reflections could be generalized into new domains to enable transferable reflections. More details are depicted in Section. \ref{sec:model}.

\begin{figure*}%[tbhp]
\centering
\includegraphics[width=1\linewidth]{figures/model_prompt.pdf}
%xyz: This is too long (revised)
\caption{\mytextcolor{Training (left) and testing (right) schemes for prompting-based models. }
% In both training and testing, we organize past and future questions related information as the organized prompt. During training, we use a novice agent to predict future question correctness and compare it with actual labels. This guides the reflective agent in generating reflections, which help the novice agent make better predictions. The reflective and novice agents interact multiple times to improve reflections. In testing, we use the reflection database and organized prompts to generate future question correctness predictions. 
}
\Description{This figure shows the training (left) and testing (right) schemes for prompting-based models. For both training and testing, we organize past questions contents, correctness, and related course materials along with future questions contents and related course materials as the organized prompt. In training (left figure), we feed the organized prompt to a LLM to generate initial future question correctness prediction. Then, we compare that with the future question correctness label to inform the reflective agent to generate reflections. Finally, we use the generated reflection and organized prompts to make the novice agent make predictions and reasoning. We let the reflective agent and novice agent interact interactively so that the reflective agent has more chances to provide better reflections. In testing (right figure), we use the successful reflection database generated by the reflective agent during training along with the organized prompt to generate future question correctness predictions.}
\label{framework:prompt}
\end{figure*}