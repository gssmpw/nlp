% Keep appendix in overleaf so that the main paper can cite and refer it.
% However, in submission, delete the appendix in main paper and only submit it in supplymental material to reduce overall paper page length.

% \newpage

\section{Appendix}

\subsection{Deep Learning Baseline Models}
\label{appendix: deep learning baseline}

\begin{enumerate}
    \item Attention-Based Models: 
    \begin{itemize}
        \item AKT: The AKT model is composed of four key components: two self-attentive encoders for processing questions and knowledge acquisition, an attention-based knowledge retriever, and a feed-forward response prediction model. The question encoder generates context-aware representations of questions based on learner's past interactions, while the knowledge encoder does the same for the acquired knowledge. Then the knowledge retriever selects relevant information from the past knowledge, which is used by the response prediction model to predict learner's response to new questions\cite{10.1145/3394486.3403282}.
        \item SimpleKT: This model is similar to the AKT model. Both models fall under the attention-based category, but they differ in several aspects. SimpleKT simplifies AKT through excluding the self-attentive encoders, using dot-product function instead of time-decayed attention mechanism, and computing interaction representations without extra parameters for question difficulty. Despite these simplifications, simpleKT outperforms many deep learning models in knowledge tracing tasks\cite{DBLP:conf/iclr/0001L0H023}. 
    \end{itemize}

    \item Adversarial-Based Models:
    \begin{itemize}
        \item ATKT: The ATKT model aims to improve the generalization of knowledge tracing models by training on both original clean inputs and their adversarial examples. It consists of three key components: Interaction Projection, Attentive-LSTM, and Response Prediction. Interaction Projection maps student interactions onto interaction embeddings, providing the input for subsequent processing. The Attentive-LSTM, functioning as the backbone of the model, uses Knowledge Hidden State (KHS) modeling modules to aggregate information from prior KHSs, resulting in a composite representation. This composite representation is then fed into the Response Prediction module to generate the final predictions\cite{10.1145/3474085.3475554}.
    \end{itemize}

    \item Deep Sequential Models: 
    \begin{itemize}
        \item DKT: This model utilizes Long Short-Term Memory (LSTM) network to model learner's learning process by using a sequence of their interactions. The main idea is to represent the learner's knowledge state as a hidden state within LSTM network, capturing how the learner's knowledge evolves\cite{piech2015deepknowledgetracing}.
    \end{itemize}

    \item Memory-Augmented Models:
    \begin{itemize}
        \item DKVMN: The Dynamic Key-Value Memory Network (DKVMN) model utilizes a key-value structure to effectively track and assess a student's mastery of underlying concepts. The model consists of a static key matrix that holds the knowledge concepts and a dynamic value matrix that updates and stores the mastery levels of these concepts \cite{zhang2017dynamickeyvaluememorynetworks}.
    \end{itemize}
\end{enumerate}


% \begin{algorithm}
% \caption{Pipeline Training Process}
% \begin{algorithmic}[1]
% \Require Student's interaction sequence $\mathcal{S}$
% \Ensure Trained model $\mathcal{M}(\Theta)$ with parameters $\Theta$, number of epochs $N$, model's loss function $\mathcal{L}$, learning rate $\alpha$
% \State Initialize model M with parameters $\Theta$
% \For {epoch $= 1$ to $N$}
%     \State Perform forward pass to compute predictions $y$ using past or past and future interaction sequence
%     \State Compute loss $\mathcal{L}_{\text{batch}}$ using binary cross-entropy
%     \State Perform backpropagation and update $\Theta \gets \Theta - \alpha \nabla \mathcal{L}_{\text{batch}}$
% \EndFor
% \end{algorithmic}
% \end{algorithm}

\subsection{Gaze spectral clustering}
\label{appendix sub sec: gaze cluster}

The server-side aggregates fixations from all students to generate areas of interest (AoIs) feedback for the instructor. To propose these AoIs, we employ a spectral clustering method that leverages inherent geometric relationships among gaze points to analyze attended regions. We apply a Gaussian kernel to pairwise Euclidean distances to measure fixation affinity, then derive a normalized Laplacian matrix from this affinity matrix. Eigendecomposition of the Laplacian provides an eigenvalue spectrum encoding the data's underlying structure. The optimal cluster count is determined by identifying the maximal gap within the initial half of sorted eigenvalues, allowing adaptive clustering that reflects changes in the subject matter being discussed. Finally, k-means clustering is applied to the fixations to generate AoI feedback for the instructor.

% \subsection{Workshop Syllabus}


% \subsection{Example Prompts}

\begin{figure*}%[tbhp]
    \centering
    \includegraphics[width=1\linewidth]{figures/Standard_prompt_ex.pdf}
\caption{Example prompts of standard prompting without (left) and with (right) TIR. Contents are truncated.}
    \Description{This figure shows the example prompts of standard prompting without (left) and with (right) TIR. Contents are truncated.
}
    \label{fig:std_prompt_ex}
\end{figure*}

\begin{figure*}%[tbhp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/CoT_prompt_ex.pdf}
    \caption{Example prompts of CoT prompting without (left) and with (right) TIR. Contents are truncated.}
    \Description{This figure shows the example prompts of CoT prompting without (left) and with (right) TIR. Contents are truncated.
}
    \label{fig:CoT_prompt_ex}
\end{figure*}


\begin{figure*}%[tbhp]
    \centering
    \includegraphics[width=1\linewidth]{figures/BertKT_ex.pdf}
    \caption{Example prompts of BertKT without (left) and with (right) our TIR module. The input prompts for BertKT are denoted as "Input" while the output of BertKT is denoted as "Output". Contents are truncated. Green ticks represent correct prediction while red crosses represent incorrect prediction.}
\Description{The left figure shows example prompts of BertKT without our TIR module. Its input includes text below: "
Input:
The past questions are depicted below: ...
Question 1: ...
This question is related to course materials below:...
(Question content + related course materials of all past questions)...
The future questions are depicted below:...
Question 6: ...
This question is related to course materials below:..."
The output includes text below: 
"Output: Incorrect" (which is wrong prediction according to the label)
The right figure shows example prompts of BertKT with our TIR module. Its input includes text below: "
Input: 
The future questions are depicted below: ...
Question 6: ...
This question is related to course materials below: ...
The student has previously answered questions related to the three ways of learning correctly, indicating a good understanding of this topic.
My new reflection for the new student is that they have demonstrated a good understanding of neural networks and learning methods, as evidenced by their correct answers to Questions 2, 4, and 5. However, they may struggle with logic-based questions, as seen in their incorrect answers to Questions 1 and 3. This suggests that while they are comfortable with certain technical concepts, they might have difficulty with questions requiring logical reasoning or the application of logic-based AI concepts."
The output includes text below: 
"Output: Correct" (which is right prediction according to the label).
}
\label{fig:BertKT_io_eg}
\end{figure*}


\begin{table*}[]
\caption{Workshop Syllabus}
\Description{This table shows our workshop syllabus for data collection including 12 lectures around different topics of AI.}
\label{tab:workshop syllabus}
\begin{tabular}{p{1cm} p{4cm} p{10cm}}
\hline
Lecture  & Content  & Details \\ \hline
1 & What is artificial intelligence & This is a quick introduction to AI. It inspires students with how AI builds our modern life, and also some brief ideas of how AI is capable of doing so.     \\ \hline
2 &The development of artificial intelligence&This is a further exploration of the history of AI. Students will learn about one of the most popular models used in the field: artificial neural networks (ANNs). The lecture discusses the origin and evolution of ANNs.\\ \hline
3 &Artificial neural networks that learn from teachers&This lecture presents the neural networks that learn from teachers, which is called supervised learning. Students will learn what supervised learning is and how artificial neural networks adopt this method.\\ \hline
4 &Algorithms that learn from teachers&Following the previous lecture, students dive further into the supervised learning. Many other methods also adopt this method but they are different from neural networks. Some classic algorithms will be introduced including the decision tree.\\ \hline
5 &Artificial neural networks that learn from examples (I)&This lecture introduces unsupervised learning, which means the model learns from the examples themselves, without correct answers. This lecture will show students what unsupervised learning is and an ANN model that falls into this category.\\ \hline
6 &Artificial neural networks that learn from examples (II)&This is the second part of the introduction of unsupervised learning, it contains the ANNs that make groups and compress. \\ \hline
7 &Algorithms that learn through try and error (I) &Lecture then moves to reinforcement learning. It is meant to introduce some basic methods and ideas of reinforcement learning. V-learning is introduced.\\ \hline
8 &Algorithms that learn through try and error (II) &Following the previous lecture, students are further introduced to Q-learning, another important method used in reinforcement learning. Some applications are introduced as well.\\ \hline
9 &Talking artificial neural networks&This is an introduction to voice controlled assistants. We will show students how the voice controlled assistant works.\\ \hline
10 &Seeing artificial intelligence&This is an introduction to systems that process real world images. We will show students how images are presented in computers and understood.\\ \hline
11 &Artificial and biological neural networks&This lecture serves as an introduction to the relationship between AI and biology. We will introduce the connection between AI and the human brain and even the application of brain-machine interface.\\ \hline
12 &Artificial intelligence:
Today and tomorrow&This lecture sums up the whole curriculum. It focuses on the application of AI, and also what risks it can have, so they are more interested in exploring further after the curriculum.\\ \hline
\end{tabular}
\end{table*}




% \begin{figure*}%[tbhp]
% \centering
% \includegraphics[width=1\linewidth]{figures/ui.pdf}
% \caption{(a). The UI of the server end of CogEdu. (b). The UI of the user end of CogEdu.}
% \Description{Figure (a) shows the UI of the server end of CogEdu. There are some entries for the system administrator to enter: lecture title, lecture abstract, lecture instructor, lecture time, lecture Zoom ID, gaze information on/off, and cognitive information on/off. There is also a participant list. Figure (b) shows the UI of the user end of CogEdu. There are a few entries for the user to enter: identity (student/teacher), passcode, and name. There is also a lecture title and abstract on the bottom of the page.}
% \label{system ui}
% \end{figure*}



% \begin{figure*}%[tbhp]
% \centering
% \includegraphics[width=1\linewidth]{figures/simulation_school_crop.pdf}
% \caption{(Left): the average simulation accuracy of BertKT with and without TIR on our newly collected dataset in different schools. (Right): the average simulation F1 scores of BertKT with and without TIR on our newly collected dataset in different schools}
% \Description{There are two bar graphs. The left one shows the average simulation accuracy of BertKT with and without TIR on our newly collected dataset in different schools. BertKT with TIR has higher accuracy than BertKT without TIR in both elementary and high school. The right one shows the average simulation F1 scores of BertKT with and without TIR on our newly collected dataset in different schools. BertKT with TIR also has higher F1 scores than BertKT without TIR in both elementary and high school.}
% \label{school-level accuracy}
% \end{figure*}

% \begin{figure*}%[tbhp]
% \centering
% \includegraphics[width=1\linewidth]{figures/simulation_question_crop.pdf}
% \caption{The left figure shows the average simulation accuracy of BertKT with and without TIR on our newly collected dataset for different questions. The right figure shows the average simulation F1 scores of BertKT with and without TIR on our newly collected dataset for different questions.}
% \Description{There are two bar graphs. The left one shows the average simulation accuracy of BertKT with and without TIR on our newly collected dataset for different questions. BertKT with TIR has the same or higher accuracy than BertKT without TIR in 7 out of 7 different questions. The right one shows the average simulation F1 scores of BertKT with and without TIR on our newly collected dataset for different questions. BertKT with TIR has the same or higher F1 scores than BertKT without TIR in 6 out of 7 different questions.}
% \label{question-level accuracy}
% \end{figure*}

% \begin{figure*}%[tbhp]
% \centering
% \includegraphics[width=1\linewidth]{figures/simulation_lecture_crop.pdf}
% \caption{The top figure shows the average simulation accuracy of BertKT with and without TIR on our newly collected dataset for different lectures. The bottom figure shows the average simulation F1 scores of BertKT with and without TIR on our newly collected dataset for different lectures.}
% \Description{There are two bar graphs. The top one shows the average simulation accuracy of BertKT with and without TIR on our newly collected dataset for different lectures. BertKT with TIR has the same or higher accuracy than BertKT without TIR in 10 out of 12 lectures. The bottom one shows the average simulation F1 scores of BertKT with and without TIR on our newly collected dataset for different lectures. BertKT with TIR has the same or higher F1 scores than BertKT without TIR in 10 out of 12 lectures.}
% \label{lecture-level accuracy}
% \end{figure*}

% \begin{figure*}%[tbhp]
% \centering
% \includegraphics[width=1\linewidth]{figures/model_compare_f1.pdf}
% \caption{Model F1 score comparison on our newly collected dataset. Left (a) shows comparison among deep learning models (akt, atkt, dkt, dkvmn, and simpleKT), LLMs-based models (Standard, CoT, BertKT) with or without TIR using GPT-4o. Right (b) shows comparison of LLMs-based models with and without TIR using GPT-4o v.s. GPT-4o mini.}
% \Description{Figure (a) illustrates that LLMs-based models get improved with TIR and outperform all deep learning baseline models (akt, atkt, dkt, dkvmn, and simpleKT). The F1 scores of deep learning models (akt, atkt, dkt, dkvmn, and simpleKT) are 0.6, 0.529, 0.553, 0.606, and 0.649 respectively. The F1 scores of LLMs-based models (Standard, CoT, BertKT) are 0.678, 0.682, and 0.651 respectively. The F1 scores of LLMs-based models with TIR (Standard+TIR, CoT+TIR, BertKT+TIR) are 0.706, 0.713, and 0.703 respectively. Figure (b) illustrates the comparison of LLMs-based models with and without TIR using GPT-4o v.s. GPT-4o mini and shows that GPT-4o mini with TIR outperforms GPT-4o without TIR.}
% \label{model compare f1}
% \end{figure*}


