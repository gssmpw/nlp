

\section{Simulation Study}
\label{sec: eduagent results}



We first explored the feasibility of our framework compared with baseline models in the public dataset named EduAgent\cite{xu2024eduagent}. 

\subsection{The EduAgent Dataset}
The EduAgent dataset was collected from N = 301 students, who were asked to watch 5-min online course videos. After that, students were prompted to finish a post test which comprises 10-12 questions. The dataset contained students' correctness on each post test question, as well as corresponding question contents and course materials which were specifically related to each question. More details about this dataset could be obtained from \cite{xu2024eduagent}. 


\subsection{Experiment Settings}
We split the dataset into training and testing set by following a individual-wise manner with 0.8 ratio. Specifically, all post test performance of 80\% students were used as the training set and all post test performance of another 20\% students were testing set. The training set was further divided into model training and model validation set following the same individual-wise manner with 0.8 ratio as well. 
We set the first five questions as past questions of the student history and other questions as future questions for prediction.
As depicted in Fig. \ref{framework:prompt}, the simulation model input included the correctness of past questions of real students, as well as corresponding past questions contents and course materials, which were specifically related to each corresponding past question. Moreover, the model input also included future question contents and course materials which are specifically related to each future question. The model output was the correctness of each future question for predictions. 

As depicted in Section~\ref{sec:model}, our TIR module could augment both prompting-based simulation (standard prompt, CoT prompt) and finetuning-based (BertKT) simulation performance. Therefore, in the experiment, we show results of both simulation types with or without the integration of our TIR module. All LLMs-based models used GPT4o-mini. We also compared with five state-of-the-art knowledge tracing models based on deep learning, as depicted in Section. \ref{sec:model}.



\subsection{Results and Analysis}

Results were depicted in Table. \ref{tab:result_eduagent}. We found that the integration of the TIR module improved the simulation performance so that both the simulation accuracy and f1 score were better than all deep learning baseline models. Specifically, the best deep learning model was SimpleKT with 0.6772 accuracy and 0.6698 F1 score. Without the TIR module, the best LLMs-based model was CoT-based prompting with 0.6222 accuracy and 0.5610 F1 score. However, after integrating the TIR module, the best LLMs-based model was finetuning-based BertKT model with 0.7012 accuracy and 0.6880 F1 score, which was superior than the best deep learning model. 

Moreover, we found that the integration of the TIR module could improve all LLMs-based models including standard prompting, CoT prompting, and BertKT, as supported by Table. \ref{tab:result_eduagent}. Although the accuracy in CoT slightly decreased, its F1 score was however obviously improved. 

These results demonstrate the feasibility and effectiveness of our TIR module to enhance existing LLMs-based approaches for more realistic student simulation, which were even better than deep learning models.

