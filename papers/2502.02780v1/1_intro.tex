\section{Introduction}
\label{sec:intro}


\mytextcolor{
Accurate simulation of students' learning behaviors in online education settings can help building a ``digital twin'' classroom, which can serve as a high-fidelity sandbox for the instructors to explore diverse pedagogies. This can in turn help improve students' learning performance.} With the rapid advancement of generative AI, using large language models (LLMs) for student simulation is becoming a promising approach. For example, GPTeach \cite{markel2023gpteach} used GPT-based virtual students for interactive TA training and MATHVC \cite{yue2024mathvcllmsimulatedmulticharactervirtual} used LLMs-based virtual classroom for mathematics education. However, these approaches did not systematically evaluate the realism of the virtual students. On the other hand, LLMs-based knowledge tracing models \cite{fu2024sinktstructureawareinductiveknowledge,jung2024clst,li2024integrating,lee2024monacobert,lee2024language} demonstrated high accuracy, but they focus on students' performance prediction rather than multi-faceted behavioral simulation.  
%predict future question correctness based on students' historical test performance, ignoring the modulation effect of course materials. 
% Such problem simplification makes the student simulation easier, but limits its application in real-time student-instructor interactions. 
We argue that an accurate digital twin should encompass contextual simulation of students' behaviors, 
%successful contextual student simulation should not only predict student learning performance accurately, but also 
capturing the dynamic modulation effect of course materials on both individual students' learning performance and correlations among students. Such dynamism can be reflected in various contextual factors, such as lecture content, individual background, questions, skills, and so on.  


However, two main challenges hinder the integration of course materials' effects into student simulation.
%
The first lies in the lack of fine-grained datasets that annotate course materials along with students' real-time performance. Most existing datasets (such as Ednet \cite{choi2020ednet}, Junyi \cite{JunyiOnlineLearningDataset} and Assistments 2009-2010 benchmark \footnote{https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data}) only contain the test question without course materials. The EduAgent dataset \cite{xu2024eduagent} indeed contains the course content, but the lecture length is too short (5 minutes) to reveal the student learning process across a whole lecture. Moreover, students may get tired easily during the course and the resulting data quality may not be guaranteed \cite{wang2019effects}, which unfortunately is not considered by most existing data collection efforts \cite{choi2020ednet,JunyiOnlineLearningDataset}.


A second challenge is that existing language models can only deal with limited contextual data when learning from example demonstrations. LLM-based simulation typically adopts either finetuning-based or prompting-based approaches. The former \cite{lee2024language} enable pre-trained models to learn from new training data directly through model finetuning. But they require a significant amount of computational resources. Therefore, researchers usually resort to smaller language models such as BERT for student simulation \cite{li2024integrating,lee2024monacobert,lee2024language}. However, such smaller language models have very low token limits (e.g., 512 tokens), which can only deal with short textual input but fail to capture complex contextual information such as course materials. Advanced LLMs such as GPT4 could support longer contextual text input, but their performance also drops under a long context \cite{li2024long} and they need significantly more computational resources for fine-tuning.
On the other hand, prompting-based methods \cite{li2024explainable} do not need model training since they directly learn from contextual prompts and example demonstrations. However, the model's in-context learning ability drops significantly in the presence of long demonstrations \cite{li2024long}.   




To tackle the first challenge, we run a new user study (N = 60 students and N = 8 instructors for real-time teaching) in the form of a 6-week online education workshop including 12 lectures (1 hour per lecture). We collected student learning performance and fine-grained annotations of course materials and mapped them to specific post-test questions. To guarantee the data quality and improve students' engagement during learning \cite{wang2019effects}, we developed a new online education system that integrated multi-modality sensing techniques to monitor students' cognitive states and prompt instructors to take recommended actions to increase students' learning engagement in real-time.



Furthermore, to deal with the second challenge, inspired by the self-reflection ability of LLMs \cite{madaan2024self} to distill knowledge \cite{Xu2024ASO,gu2024minillm}, we propose a new LLM-based student simulation framework by introducing a transferable iterative reflection (TIR) module, which guides the LLMs to perform reflections on specific course materials and compress the learned knowledge to augment the LLM simulation. Different from a straightforward self-reflection \cite{madaan2024self}, our TIR architecture incorporates iterations between a novice agent and reflective agent to ensure that the reflections could be generalized into new domains to enable transferable reflections (Section \ref{sec:model}). As a result, this TIR module can augment and solve the bottlenecks of both finetuning-based models and prompting-based models. For finetuning-based models, TIR overcomes the token limit issue by focusing its reflections on specific course materials so as to compress the learned knowledge. For prompting-based models, TIR provides an efficient way to enable the LLMs to learn from example demonstrations more effectively, where the learned knowledge can be transferred to 
%xyz: What does "new LLM" mean? Why do you need new LLM? (revised)
a new simulation without example demonstrations. This ensures that LLMs learn general knowledge instead of locally optimal knowledge from example demonstrations.  
%thus enabling LLMs to learn general and effective knowledge from example demonstrations.



We have evaluated the student simulation performance in both the EduAgent public dataset \cite{xu2024eduagent} and our newly collected dataset. The results show that our TIR modules enhance the LLM-based student simulation, making it even more powerful than deep learning methods that are trained and fit to given datasets. 
\mytextcolor{
Specifically, the evaluation examines whether our model can better capture the dynamics of student behaviors. Existing research generally defines human behavior as a collection of observable actions and reactions in response to internal (genetic factors) and external (environmental factors) stimuli \cite{longino2019studying}. 
% Therefore, to demonstrate that our simulator can replicate student learning behaviors, we need to capture student response to both internal and external stimuli. The student response can be represented as the post-test accuracy after learning knowledge from the course. Here the course knowledge (contents in the lecture slides) serves as the external stimuli. The internal stimuli come from the students' individual differences since different students have their different knowledge backgrounds. To this end, we explore the effectiveness of our model in capturing individual-level, lecture-level, question-level, skill-level correlation, which measure whether the simulator can replicate varying responses (i.e. post-test accuracy) across different students, lectures, questions, skills, respectively. 
% Specifically, we evaluate the model's ability to capture the dynamics of student behaviors, generally defined as observable actions and reactions to internal (genetic) and external (environmental) stimuli by existing research \cite{longino2019studying}. 
Therefore, to demonstrate that our simulator replicates student learning behaviors, we model student responses to these stimuli, represented by post-test accuracy after engaging with course content. Course knowledge, delivered via lecture slides, constitutes external stimuli, while internal stimuli arise from individual differences such as prior knowledge. Accordingly, we assess the model's ability to capture variations in post-test accuracy at multiple levels: individual (per student), lecture (per session), question (per post-test item), and skill (per knowledge concept). Additionally, we evaluate inter-student correlations to determine whether the simulation model accurately reflects the response patterns between student pairs.
% we assess the model's effectiveness in capturing correlations at the individual, lecture, question, and skill levels, examining whether it replicates variations in post-test accuracy across these dimensions.
% Here the individual-level simulation difference is measured by the average post-test accuracy for each specific student. The lecture-level simulation difference is measured by the average post-test accuracy for each specific lecture happening in our workshop. The question-level simulation difference is measured by the average post-test accuracy for each specific question in the post-test after each lecture. The skill-level simulation difference is measured by the average post-test accuracy where the questions are corresponding to a specific slide (i.e. one knowledge concept) per lecture.  
% We also consider the inter-student correlation, which captures whether the simulator can capture response correlations between different student pairs.
}
Overall, the results show that our approach better captures the granular dynamism of learning performance and inter-student correlations in classroom, pointing towards a potential ``digital twin'' for online classrooms.

To summarize, the main contributions in this paper include:

\begin{itemize}
    \item We run a new 6-week online education experiment with N = 60 students and N = 8 instructors to collect student learning performance with fine-grained annotations of course materials. This is powered by our newly developed online education system that integrates sensing techniques and feedback recommendations to increase student learning engagement during real-time online instruction. The online education system implementation is available at: https://github.com/cogteach-admin/CogTeach, and the data/model implementation is available at: https://github.com/songlinxu/ClassroomSimulacra.
    \item We propose a transferable iterative reflection module that can augment the student simulation performance in both finetuning-based models and prompting-based models.
    \item We systematically evaluate the student simulation performance and show how it can capture the dynamic modulation effect of course materials by revealing lecture-level, individual-level, question-level, skill-level differences and inter-student correlation in classroom.
\end{itemize}



