\begin{figure}%[tbhp]
\centering
\includegraphics[width=1\linewidth]{figures/system.pdf}
%xyz: Too long (revised)
\caption{
\mytextcolor{
(a). Illustration of our CogEdu system. 
(b). Our action prompt strategy for instructors based on attention ratio and knowledge ratio.
(c,d). The UI of the user end (c) and server end (d) of CogEdu. 
}
}
\Description{Figure (a) shows N student clients on the right side and one teacher client on the left. The teacher client has access to the three modules of feedback: area of interest (AoI) feedback, general cognitive feedback, and action prompt. The bounding boxes of AoI in the middle of the screen represent the area where the students were looking at. The CogBar at the top left corner summarizes the general cognitive feedback. The knowledge ratio, shown as a horizontal bar, is calculated by the ratio of students that were not confused about the contents over all students. The attention ratio, also shown as a horizontal bar below the knowledge ratio, is calculated by the ratio of students that were paying attention over all students. On the top right, the action prompt is the suggestion given to the instructor based on the values of CogBar. In the figure, the action prompt is "Draw attention!" in red. Figure (b) shows our action prompt strategy based on attention ratio and knowledge ratio. Low knowledge ratio triggers "Repeat the current content" recommendation. Low attention ratio triggers "Draw attention" recommendation. The horizontal axis denotes the attention ratio while the vertical axis denotes the knowledge ratio. Each ratio is separated into 3 buckets: low, medium, and high. When both attention and knowledge ratio are high, no feedback was given. When both attention and knowledge ratio are low, "draw attention" was given as the feedback. Other than that, when the knowledge ratio is lower, "repeat" was given as the feedback. When the attention ratio is lower, "draw attention" was given as the feedback to instructors. Figure (c) shows the UI of the user end of CogEdu. There are a few entries for the user to enter: identity (student/teacher), passcode, and name. There is also a lecture title and abstract on the bottom of the page. Figure (d) shows the UI of the server end of CogEdu. There are some entries for the system administrator to enter: lecture title, lecture abstract, lecture instructor, lecture time, lecture Zoom ID, gaze information on/off, and cognitive information on/off. There is also a participant list. 
}
\label{cogedu system}
\end{figure}

\section{Online Education Workshop and Dataset}
\label{sec:workshop}

Although our simulation experiment on the EduAgent dataset demonstrated the effectiveness of our framework compared with baseline models, the EduAgent dataset itself only contains 5-min lectures. Such short duration may not capture the fine-grained effect of course stimuli on student learning performance.
Therefore, it is necessary to examine the student simulation in lectures with longer duration to reveal further insights. 


\begin{figure}%[tbhp]
\centering
\includegraphics[width=1\linewidth]{figures/course_demo.pdf}
\caption{A real online education example of our live CogEdu system shown in Fig. \ref{cogedu system}}
\Description{This figure shows a screenshot of a live example of the online education system shown in Fig. \ref{cogedu system}. The system is in a browser that integrates the Zoom interface. In the middle, there is a course slide. In addition, there are two bounding boxes (AoI) on the slide. In the bottom there are two bars: knowledge ratio at 33.3\% and attention ratio at 100\%.}
\label{course system demo}
\end{figure}




\subsection{Workshop Design}

To this end, we conducted a 6-week online education workshop to deliver 12 lectures, where each lecture lasted 1 hour. The long-duration lectures could not only verify the simulation results, but also reveal new insights about how the simulation models can capture students' learning performance variations across the whole lecture (depicted in Section \ref{subsec: dynamism}). \mytextcolor{The workshop syllabus is depicted in Appendix Table. \ref{tab:workshop syllabus}}.

\subsubsection{\textbf{Participants}}

We recruited 30 elementary school students, 30 high school students, and 8 instructors from high schools and universities in the local area using emails and social media. 
We removed the demographic information (such as age and gender) for privacy concerns.
Our data collection was approved by the Institutional Review Board (IRB). All participated students and instructors were informed of the experiment form and then signed consent forms. For participants under 18 years old, we obtained the written consent form from both participants and their parents.







\subsubsection{\textbf{Task and Procedure}}
We first prompted the students and instructors to watch an introduction video about how to use our online education system to facilitate learning and teaching, as well as the detailed procedures of our data collection (Fig.~\ref{system procedure}).
%
After that, students were required to first go through a gaze calibration process (depicted in Section. \ref{subsubsec: student client}) for accurate gaze collection.
%
Then students were prompted to perform facial expressions (including confused and neutral expressions) in order to train a model for cognitive information detection (more details in Section \ref{subsubsec: student client}). 
%
After that, both students and instructors were in the same online video conference system (Section \ref{subsec: cogedu}) and instructors presented the course materials to the students. The lecture materials were slides prepared by our research team. During the lectures, our online education system provided visual feedback to the instructors about the students' learning status, and the instructors could adapt their teaching strategies accordingly (depicted in Section. \ref{subsubsec: teacher client}).
%
After the lecture, students were required to finish a post-test composed of 10-12 questions related to each specific lecture to measure their learning outcome. 

\begin{figure}%[tbhp]
\centering
\includegraphics[width=1\linewidth]{figures/system_process.pdf}
\caption{
\mytextcolor{The procedure to use our online learning system. (a)(b)(c). Gaze calibration process for gaze tracking. (d)(e). Facial expression model training data collection process. (f)(g). Students and teachers join in the online video calling from their own clients in class.}}
\Description{Figure (a). A popup window saying "[Before start: 1/2] Please calibrate the GazeCloud for gaze collection." (b). A cartoon image of a face on the top. A button saying "Start Gaze Calibration" in the middle. 4 sentences describing what the user must make sure to check at the bottom. (c). A sentence saying "Look at dot". A dot in the middle of the screen. (d). A window saying "Please make no expression." (e). A window saying "Please make confused expression." (f). A window with the word "student" on top of the button "Join Audio by Computer." (g). A window with the word "teacher" on top of the button "Join Audio by Computer."}
\label{system procedure}
\end{figure}


\subsubsection{\textbf{Experiment Design}}
Our six-week workshop was composed of a series of 12 lectures about the basics of artificial intelligence, covering different topics such as basic concepts in machine learning, computer vision, natural language processing, reinforcement learning, etc. Each lecture lasted one hour. The difficulty of the lectures was tailored to match the knowledge level of elementary and high school students, respectively.
%
For each week, the instructors delivered two lectures. Students were encouraged to select the same time slot for real-time and synchronous teaching among all students together (Fig. \ref{cogedu system}(a)). If students had time conflicts with the instructors, we made new time arrangements for these students for additional data collection.
%
Each student was encouraged to attend as many lectures as they could. Overall, each student attended 3 lectures on average. 



\subsubsection{\textbf{Measurement}}
As depicted above, for students, we mainly collected their gaze, facial expressions, and post-test answers. The gaze and facial expressions were mainly used to generate student status feedback to instructors so that the instructors could take specific actions to increase the students' engagement and improve the quality of collected data. The post-test answers were used to measure the student learning outcomes.


\begin{figure*}%[tbhp]
\centering
\includegraphics[width=1\linewidth]{figures/bertkt_train_loss.pdf}
\caption{
\mytextcolor{
(a)(b). BertKT training curve without (a) and with (b) TIR across different epochs. Vertical axis on the left is the loss value (solid lines). Vertical axis on the right is the metrics (accuracy and F1 score) value (dotted lines).}}
\Description{Figure (a)(b) show the BertKT training curve (training/validation loss, accuracy, and F1 score) with(a) and without(b) TIR across different epochs. Vertical axis on the left is the loss value. Vertical axis on the right is the metrics (accuracy and F1 score) value. The dotted lines represent metrics (accuracy and F1 score) while the solid lines represent training and validation losses. On the top (a), BertKT without TIR training loss curve starts at around 0.69 and ends at around 0.59 with 100 epochs. On the bottom (b), BertKT with TIR training curve starts at around 0.69 and ends at around 0.52 with 100 epochs. On the top (a), BertKT without TIR validation loss curve starts at around 0.73 and ends at around 0.67 with 100 epochs. On the bottom (b), BertKT with TIR validation loss curve starts at around 0.72 and ends at around 0.60 with 100 epochs. On the top (a), BertKT without TIR accuracy curve starts at around 0.45 and ends at around 0.62 with 100 epochs. On the bottom (b), BertKT with TIR accuracy curve starts at around 0.42 and ends at around 0.71 with 100 epochs. F1 scores of BertKT with and without TIR follow similar trends as accuracy scores of BertKT with and without TIR. All the curves are more smooth for BertKT with TIR compared to BertKT without TIR.}
\label{loss}
\end{figure*}

\subsection{Online Education System}
\label{subsec: cogedu}




Although existing video conference software such as Zoom\footnote{https://zoom.us/} provided a stable solution for online education, the subtle student behaviors may not be captured to provide insights to teachers. Moreover, research showed that students' learning performance might become worse compared with in-person instructions\cite{nguyen2015effectiveness}. As a result, the quality of our collected data could be severely compromised. 
%
Therefore, to solve this problem and facilitate subtle communication between students and instructors, we developed an online education system named \textbf{CogEdu} that could support synchronized teaching between students and teachers in a client on the computer, while also providing real-time student status feedback to teachers to enhance the education process.
%
Based on the ubiquitous webcams on laptops, we collected the gaze information and facial expressions of students. By analyzing the collected data, we provided real-time feedback to the instructors about the understanding of current contents, the attention status, and a fine-grained visualization of contents that students were concerned about. Understanding about contents (or confusion) and attention were referred to as \textit{cognitive information}. To further assist instructors, the system also provided teaching strategy suggestions based on the collected data. 

As a result, this system could augment student learning engagement and teaching effectiveness to enable high-quality data collection. More details were depicted below.



\subsubsection{\textbf{System Implementation}}

We implemented the CogEdu system on a cloud server. Users (students and instructors) could access the system using their browsers (Fig. \ref{cogedu system}). Considering that most users were more familiar with Zoom, a video teleconferencing software program, we implemented the video conference function based on Zoom APIs\footnote{https://developers.zoom.us/docs/api/}. All the feedback was overlaid over the embedded Zoom interface. To support the large flow of facial expression data before and during the lecture, and to enhance the robustness of the system, we adopted Kubernetes on the google cloud\footnote{https://cloud.google.com/kubernetes-engine} to manage the deployment, scaling, and management. Instances scaled up when the load was growing to reduce latency and achieve satisfying real-time performance.



\subsubsection{\textbf{Student Client}}
\label{subsubsec: student client}
On the studentâ€™s side, students were required to first go through a gaze calibration process and then collected facial expressions for cognitive information detection. To collect gaze information from the students, we used the service from GazeRecorder\footnote{https://gazerecorder.com/}. Around 28 gaze positions were provided from the service per second, which were then labeled as fixations or saccades using a velocity-based method \cite{engbert2003microsaccades}. Meanwhile, the system sent facial expressions to the server for cognitive information detection every second. The students' side uploaded all gaze information and cognitive information every five seconds.

The algorithm we used to transform raw gaze into fixations was from \cite{engbert2003microsaccades}. The basic idea was to calculate a velocity threshold, and gaze points with velocity below were labeled as fixations. 
%
The confusion information of students was detected using a support vector machine (SVM). Before the lecture started, students were asked to make confused expressions and neutral expressions. Collected data were cropped to focus on the eyebrow-eye region and then fed to principal component analysis (PCA) to extract features. An SVM was trained based on the features to classify either confused or neutral expressions.

Attention detection was facilitated by gaze detection, confusion detection, and browser built-in properties. When the user switched to another tab or application, \textit{document.visibilityState} in the browser became hidden. This property was checked together with confusion detection. When the confusion detection algorithm on the server failed to detect a face, which meant the student's face was out of the camera, the system then asserted the student to be not attentive. Thirdly, when the gaze of the student fell out of the screen, the student was labeled as not attentive as well. 

%xyzr1: Use hatch patterns in the bars to make it better for black-and-white printing 
\begin{figure*}
\centering
\includegraphics[width=0.8\linewidth]{figures/model_compare_accuracy.pdf}
\caption{
\mytextcolor{
Model accuracy and F1 score comparison on our newly collected dataset. Left (a,c) shows comparison (a: accuracy, c: F1 score) among deep learning models and LLMs-based models using GPT-4o. Right (b,d) shows comparison (b: accuracy, d: F1 score) of LLMs-based models using GPT-4o v.s. GPT-4o mini.}}
\Description{Figure (a) illustrates that large language model based models get improved with TIR and outperform all deep learning baseline models (akt, atkt, dkt, dkvmn, and simpleKT). The accuracy of deep learning models (akt, atkt, dkt, dkvmn, and simpleKT) is 0.625, 0.527, 0.551, 0.606, and 0.656 respectively. The accuracy of large language model based models (Standard, CoT, BertKT) is 0.629, 0.633, and 0.602 respectively. The accuracies of large language model based models with TIR (Standard+TIR, CoT+TIR, BertKT+TIR) are 0.668, 0.676, and 0.684 respectively. Figure (b) illustrates that in most cases (standard and CoT prompts), without TIR, GPT-4o outperforms GPT-4o mini. With TIR, GPT-4o mini outperforms GPT-4o without TIR. Figure (c) illustrates that LLMs-based models get improved with TIR and outperform all deep learning baseline models (akt, atkt, dkt, dkvmn, and simpleKT). The F1 scores of deep learning models (akt, atkt, dkt, dkvmn, and simpleKT) are 0.6, 0.529, 0.553, 0.606, and 0.649 respectively. The F1 scores of LLMs-based models (Standard, CoT, BertKT) are 0.678, 0.682, and 0.651 respectively. The F1 scores of LLMs-based models with TIR (Standard+TIR, CoT+TIR, BertKT+TIR) are 0.706, 0.713, and 0.703 respectively. Figure (d) illustrates the comparison of LLMs-based models with and without TIR using GPT-4o v.s. GPT-4o mini and shows that GPT-4o mini with TIR outperforms GPT-4o without TIR.}
\label{model compare accuracy}
\end{figure*}

\subsubsection{\textbf{Teacher Client}}
\label{subsubsec: teacher client}
On the instructor's side, the system fetched all information that students uploaded to the server every five seconds and then processed the information to provide the instructor with feedback. The feedback provided to instructors consisted of three modules: area of interest (AoI) feedback, general cognitive feedback, and action prompt.


\textbf{Area of Interest Feedback}: This module took as input the gaze and cognitive information and visualized feedback as bounding boxes on the ongoing lecture. These bounding boxes (depicted in Fig. \ref{cogedu system} and Fig. \ref{course system demo}) were clustered from gazes that were labeled as fixations using a spectral clustering algorithm. The bounded area was where students were looking. The opacity of the bounding box represented the ratio of students looking at this area over all students, and the color represented the ratio of students that were confused about the contents in the area over students looking at this area. More details about the spectral clustering method were depicted in Appendix \ref{appendix sub sec: gaze cluster}.


\textbf{General Cognitive Feedback}: This module took as input the cognitive information, and visualized feedback as a summarized bar chart (depicted in Fig. \ref{cogedu system} and Fig. \ref{course system demo}). We displayed the ratio of students that were not confused about the contents over all students (knowledge ratio), and the ratio of students that were paying attention over all students (attention ratio). 

\textbf{Action Prompt}: Based on the general cognitive information, the system provided teaching suggestions to the instructor (depicted in Fig. \ref{cogedu system} and Fig. \ref{course system demo}). When the attention ratio fell lower, instructors were prompted to draw attention from students. When the knowledge ratio fell lower, repeating the current contents was recommended (Fig.~\ref{cogedu system}(b)).



% \begin{figure*}
% \centering
% \includegraphics[width=1\linewidth]{figures/simulation_student_crop.pdf}
% \caption{Simulation accuracy (top) and F1 score (bottom) of BertKT with and without TIR on our new dataset in individual student level.}
% \Description{This figure contain two bar graphs that compare the performance of BertKT with and without TIR in individual student level. At the top it shows the comparison of BertKT with and without TIR in terms of their simulation accuracy in individual student level. BertKT with TIR performs better or similarly with BertKT without TIR in 16 out of 18 individual students simulation accuracy. At the bottom it shows the comparison of BertKT with and without TIR in terms of their simulation F1 scores in individual student level. BertKT with TIR performs better or similarly with BertKT without TIR in 13 out of 18 individual students F1 scores.}
% \label{individual student accuracy}
% \end{figure*}






