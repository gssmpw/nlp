\section{Simulation Methodology}
\label{sec:model}
Our classroom simulacra framework aims to build LLM-based generative student agents that could mimic real students' learning behaviors based on their learning histories. The agents can then simulate the students' future learning performance, which is represented by the question answering correctness in the post-lecture tests. 
%This design follows the common practice in existing work on knowledge tracing and student simulation \cite{piech2015deepknowledgetracing,10.1145/3474085.3475554,10.1145/3394486.3403282}.

\subsection{Problem Formulation}
\label{subsec: problem formulate}
In our online education scenario, students first listen to the lecture and then finish a post-course test, which evaluates their learning performance based on the accuracy of their answers. As depicted in Fig.~\ref{framework:finetune}, the \textit{input} of our simulation includes past learning history ($l_{past}$) and future learning information ($l_{future}$). Here $l_{past}$ includes past questions' contents ($q_{past}$), past answers' correctness (i.e. labels in past questions, denoted as $y_{past}$), and course materials related to specific questions ($c_{past}$) in the learning history. The future learning information $l_{future}$ includes future question contents ($q_{future}$) and corresponding course materials ($c_{future}$). The \textit{output} of the model is the sequence of future answers' correctness ($\hat{y}_{future}$), with the corresponding ground truth denoted as $y_{future}$.

\mytextcolor{Note that the course material inputs are represented by text only to match the LLM's requirement. Although there might be images in the actual lecture slides, we have converted the images into textual descriptions during our dataset annotation process. 
%In the online education scenario, the lecturer delivers knowledge and presents slides to students. 
The course materials include the titles/bullet points on slides, or human-annotated descriptions of images on slides. The lecturers' didactics typically align with the slides but are not delivered as a word-for-word readout. Therefore, we do not use the lecturer's transcripts as course materials nor model input.
Examples of model input can be found in Fig. \ref{prompt example}.}

% and the model output is the prediction of students' correctness in future questions.

% The goal of KT is to predict whether a student will correctly answer future questions given the student's past interaction record. Traditionally, the past interaction record consists of 2 parts: a question id and whether the student answered that question correctly. For each question, we might also have its textual content (eg. "What is 1+1?"). However, we believe that this is not enough to capture the full picture of the student's learning behavior. Thus, in our dataset and problem formulation, we also include related course content (eg. content of a math lecture slide) of each question.
    
% The past interaction record, for example, might be (q1, correct), (q2, incorrect), (q3, incorrect), (q4, correct), (q5, correct). We want to predict whether the student will correctly answer the future questions q6$\sim$q10. The model will have access to information about the past questions q1$\sim$q5 (their content, course material, and the correctness of the student's answers) and information about the future questions q6$\sim$q10 (their content and course material). The output of the model is simply whether the student will get each of the future questions q6$\sim$q10 correct.
    
% For simplicity, the past interaction record is denoted as $Past$ and the future question content and its related course material is denoted as $Future$. In addition, the $i$th ground truth future question correctness is denoted as $a_i$ and the prediction that the model made is denoted as $\hat{a}_i$. Under this notation, the input to the model of the previous example is $Past+Future$ where $+$ is concatenation while the output from the model is $\hat{a}_1, \hat{a}_2, ..., \hat{a}_5$ which corresponds to whether the student will get q6, q7, ..., q10 correct. Since the goal is to predict whether a student will correctly answer future questions correctly, the more $\hat{a}_i$ resembles $a_i$ the better.

\subsection{Model Training and Evaluation}
\label{subsec: traing testing split}
We totally have three kinds of simulation models: prompting-based LLM simulation, finetuning-based LLM simulation, and deep learning-based simulation (baseline).
In order to evaluate the models in a comparable manner, all kinds of models use the same training set to train the model and the same testing set to evaluate the simulation performance. However, some prompting-based models only need part of the training set, which will be specified later.
For each dataset, we first split it into training and testing set by following an individual-wise manner with a specific ratio $R$\% (detailed ratio is depicted in each experiment). Specifically, all test performance of $R$\% students was used as the training set and all test performance of another (1-$R$\%) students was testing set. The training set was further divided into model training and model validation set following the same individual-wise manner with the same $R$\% ratio. 
The reason why we use the individual-wise dataset splitting is that our classroom simulacra instantiates each digital student based on the corresponding real student's past learning history and simulates that student's future learning process. 
%Therefore, the individual-wise splitting could better follow our classroom simulacra design.
We set the first five questions as past questions ($q_{past}$) of the student history and other questions as future questions ($q_{future}$) for prediction.

\begin{figure*}%[tbhp]
\centering
\includegraphics[width=1\linewidth]{figures/model_finetune.pdf}
%xyz: Make these more concise (revised)
\caption{\mytextcolor{Training (left) and testing (right) scheme for finetuning-based models. }
% In training, we use a similar TIR approach like prompting-based models to generate reflections. We then use the reflection, future question contents, and initial novice agent prediction with reasoning as input and future questions correctness as labels to fine-tune a BERT-based classifier. In testing, we use the reflection database and organized prompts to generate new reflection. Then, we use the new reflection, new future question contents, and corresponding initial novice agent prediction as input to let the trained BERT classifier make future questions correctness predictions.
}
\Description{This figure shows the training (left) and testing (right) schemes for finetuning-based models. In training, we use a similar TIR approach like prompting-based models to generate reflections. Then, we use the reflection, future question contents, and initial novice agent prediction as input and future questions correctness as labels and output to fine-tune a BERT based classifier.  In testing, we use the successful reflection database generated by the reflective agent during training as the context along with the organized prompt to a new reflective agent to generate the reflection for the future questions. Then, we use the new reflection, new future question contents, and initial novice agent prediction as input to for the trained BERT to predict future question correctness.}
\label{framework:finetune}
\end{figure*}





\subsection{Transferable Iterative Reflection (TIR)}
\label{subsec: TIR}

The objective of \textbf{t}ransferable \textbf{i}terative \textbf{r}eflection (TIR) is to improve the LLM-based student simulation by learning from the students' past learning history more effectively. 
The main difference between the TIR and the existing (multi-round) reflection-based methods \cite{madaan2024self,ji2023towards,yan2024mirror,kumar2024supporting,hui2024rot,wang2024taste,li2024think} lies in the transferable feature in the model design. Traditional reflection methods simply ask LLMs to reflect based on the difference between their predictions and labels. 
In contrast, the TIR module iteratively prompts LLMs to reflect on its previous simulations by comparing with labels in the example demonstrations (i.e. past learning history) so that LLMs could generate general reflections results that could be easily 
%xyz: What's novice LLM? (revised)
transferred to novice LLMs which do not have the example demonstrations. This ensures the generalization of such reflected results to be applied into 
%xyzr1: Unclear what this means. DO you mean a different student, different demonstration..?
new future learning information. As a result, the reflected results can not only directly improve the simple prompting-based simulation by increasing the data utilization efficiency, but also compress the information while avoiding missing important information 
%xyzr1: Why is this only applicalbe to finetuning based LLM? 
to improve the finetuning-based simulation.
%
At a higher level, the TIR module consists of four phases: initial prediction, reflection, testing, and iteration 
%xyz: Are you referring to Fig. 2 instead? (both are fine)
(Fig. \ref{framework:prompt}). 

\begin{enumerate}
    \item \textit{Initial prediction:} Ask the LLM (reflective agent) to predict future question correctness (i.e., $\hat{y}_{future}$) based on $l_{past}$ and $l_{future}$, and obtain the initial prediction accuracy ($acc_0$) by comparing $\hat{y}_{future}$ with the ground truth ($y_{future}$), as depicted in Section \ref{subsec: problem formulate} and Fig. \ref{framework:finetune}. 
    \item \textit{Reflection:} Provide the reflective agent with the ground truth of future question correctness (i.e. label: $y_{future}$), and ask it to reflect on why it fails to predict some future answers' correctness. The reflection at iteration $k$ is denoted as $r_k$.
    \item \textit{Testing:} Use $r_k$ together with $l_{past}$ and $l_{future}$ to ask another novice agent which has not experienced the ground truth to make a new prediction. Denote the predictions from the novice agent in this iteration $k$ as $\hat{y}_{novice,k}$.
    \item \textit{Iteration:} Obtain the prediction accuracy ($acc_k$) by comparing $\hat{y}_{novice,k}$ with the ground truth ($y_{future}$). If $acc_k$ is lower than the initial prediction accuracy ($acc_0$), we ask the reflective agent to reflect in a different direction in the next iteration. Otherwise, we inform the reflective agent that the accuracy has indeed improved and it could reflect towards the similar direction. \mytextcolor{The iteration ends when the novice agent achieves 100\% accuracy with the help of $r_k$ or we reach the maximum number of iterations.}
\end{enumerate}

Finally, \mytextcolor{we select the reflection that yields the highest prediction accuracy by the novice agent as the best reflection} ($r_{best}=r_{argmax(acc_k)}$) and log it into the reflection database. This database will be used to augment existing prompting-based LLMs by giving example demonstrations or augment finetuning-based models to provide reflections. 



\mytextcolor{
The iterative reflection in TIR improves the simulation performance by adjusting the reasoning process of LLMs during reflection. Due to the 
diversity of individuals and course contents, responses to course stimuli varies a lot across students. With a simple reflection, LLMs’ results can be biased due to its pre-training corpus \cite{hadi2023survey}, and cannot be well-adapted to the specific student and course stimuli. The iterative reflection helps LLMs to overcome such bias from its pre-training corpus and iteratively adjust the reasoning process during reflection regarding the causality of how course stimuli modulate students' behaviors while also respecting the students’ past history. 
%
One example is depicted in Fig. \ref{prompt example}. When the LLM had a straightforward reflection in $r_1$, it overestimated the student’s comprehension and thought the student could answer Question 12 correctly. However, After iterative reflection in $r_3$, the LLM found a potential misunderstanding or oversight that led to the wrong prediction. Such an experience was stored in the successful reflection database. Once retrieved, it could give inspiration for 
the future new reflection agent to consider such oversight in the testing set. 
}

\begin{figure*}%[tbhp]
\centering
\includegraphics[width=1\linewidth]{figures/prompt_detail.pdf}
%xyz: Make these more concise (revised)
\caption{\mytextcolor{Prompt examples in the Transferable Iterative Reflection process.}
}
\Description{
This figure shows the prompt examples in the Transferable Iterative Reflection process. 
In the first reflection, LLM says: The reason why I make a wrong prediction for question 12 is that I underestimated the student's comprehension of image processing concepts. Specifically, I may have overlooked the student's ability to identify incorrect statements related to image processing based on the previous questions answered correctly. If I have a new chance to make new predictions, I will consider the student's proficiency in recognizing key concepts in image processing and representation. 
Then user says: I tried to use your reflection to let a novice agent make the prediction again. However, Question 12 is still Wrongly predicted. 
Below are the new predictions and reasoning from a novice agent based on your previously provided reflections.
Question 12: Correct, Reason: Acknowledging the student's comprehension of image processing concepts, it is likely that the student will correctly identify the incorrect statement related to image processing based on the previous questions answered correctly.
Please continue reflection in another direction to see whether we can further improve the prediction accuracy.
In the second reflection, LLM says: The reason why I made a wrong prediction for question 12 previously was that I underestimated the student's comprehension of image processing concepts. By acknowledging the student's proficiency in this area, it is more likely that the student will correctly identify the incorrect statement related to image processing based on the previous questions answered correctly.
Then user says: I tried to use your reflection to let a novice agent make the prediction again. However, Question 12 is Wrongly predicted. Below are the new predictions and reasoning from a novice agent based on your previously provided reflections.
Question 12: Correct, Reason: The student has shown proficiency in image processing concepts, making it probable that the student will correctly identify the incorrect statement related to image processing based on previous questions answered correctly.
Please continue reflection in another direction to see whether we can further improve the prediction accuracy. 
In the third reflection, LLM says: The reason why I made a wrong prediction for question 12 previously was that I overestimated the student's comprehension of image processing concepts. Despite the student's history of correct answers in this area, there may have been a misunderstanding or oversight that led to the incorrect response. If I have a new chance to make new predictions, I will pay more attention to the specific details of the concepts related to image processing. 
Then the user says: I tried to use your reflection to let a novice agent make the prediction again. The prediction accuracy is indeed improved.
Below are the new predictions and reasoning from a novice agent based on your previously provided reflections.Question 12: Wrong, Reason: Despite the student's history of correct answers in image processing concepts, there may have been a misunderstanding or oversight that could lead to an incorrect response for this question.
}
\label{prompt example}
\end{figure*}


For prompt-based models, TIR can directly improve the LLMs' performance without fine-tuning. For finetuning-based models, TIR can effectively compress the overflowed information in $l_{past}$ and $l_{future}$ to deal with the token limit problem.
More details are described in the following sections.


% \begin{table*}[]
% \caption{Simulation results on EduAgent dataset}
% \label{tab:result_eduagent}
% \begin{tabular}{l|lll|lllll}
% \hline
% \multirow{2}{*}{Metric} & \multicolumn{3}{l|}{GPT4o-Mini (Without/With TIR)} &  \multicolumn{5}{l}{Deep Learning Models (Without/With Future Question Information)} \\ \cline{2-9} 
%                         & Standard  & CoT  & BertKT  & DKT  & AKT  & ATKT  & DKVMN  & SimpleKT  \\ \hline
% Accuracy  & 60.25+4.69  & 62.22-0.49  & 60.74+9.38  & 63.51+0.00  & 61.71+0.00  & 58.56+5.40  & 62.61-0.90  & 61.71+6.01  \\
% F1      & 51.28+13.46  & 56.10+3.41  & 61.10+7.70  & 63.52+0.00  & 60.51+0.00  & 57.45+6.45  & 62.07-1.56  & 59.05+7.93  \\ \hline
% \end{tabular}
% \end{table*}

\subsection{TIR Augments Standard Prompts}
\label{subsec: TIR augments prompts}
Here we describe how to apply the TIR module to augment the standard prompting-based simulation models. 

A standard simulation model simply uses LLMs to take all information as prompt input ($l_{past}$ and $l_{future}$) and predict future question correctness sequence ($\hat{y}_{future}$), as depicted in Fig. \ref{fig:std_prompt_ex}. Directly inputting all of students' data from the training set as example demonstrations poses an obvious challenge. The data including course materials often exceed the token limits of LLMs, hampering their capability to extract useful information.

To this end, we apply the TIR module to enable the LLM to effectively learn from the training data set.
Specifically, in the training stage, we first run the TIR module for each student in the training set, following the procedure in Section \ref{subsec: TIR}. The output reflections are stored into a \textit{successful reflection database}. 
In the testing stage, as depicted in Fig. \ref{framework:prompt}, we do not run the TIR module. Instead, we use a new reflective agent powered by LLMs to retrieve reflections from the successful reflection database. For each simulated student in the testing set, we retrieve the reflections of 
%xyz: Why four? (revised)
$M$ students from the reflection database. \mytextcolor{The $M$ students from the training set are randomly selected but we make sure they are in the same course as the simulated student in the testing set. This is the only criteria of retrieving reflections, which ensures contextual consistency during reflection. Random selection ensures that the example demonstrations are not manually biased. However, we use a random seed to also ensure that we can replicate such random selection to enhance the replicability of our results. } Our pilot experiment shows that $M=4$ is enough to achieve reasonable simulation performance.
\mytextcolor{
The retrieved reflection from $M$ students serves as the example demonstrations in the same course so that the new reflection agent in the testing set can leverage the experience from the retrieved reflections to perform a transferable reflection.  }
Based on the retrieved reflections and past learning history ($l_{past}$) and future learning information ($l_{future}$), the new reflective agent conducts simulation for the specific student in the testing set.  To prevent label leakage, this new reflective agent does not experience any other training data. So it is different from the reflective agent in the training set. 


\begin{table*}[]
\caption{Simulation results on EduAgent dataset}
\Description{This table shows the simulation results on EduAgent dataset.
We found that the integration of the TIR module improved the simulation performance so that both of the simulation accuracy and f1 score were better than all deep learning baseline models. Specifically, the best deep learning model was SimpleKT with 0.6772 accuracy and 0.6698 F1 score. Without the TIR module, the best LLMs-based model was CoT-based prompting with 0.6222 accuracy and 0.5610 F1 score. However, after integrating the TIR module, the best LLMs-based model was finetuning-based BertKT model with 0.7012 accuracy and 0.6880 F1 score, which was superior than the best deep learning model. 
Moreover, we found that the integration of the TIR module could improve all LLMs-based models including standard prompting, CoT prompting, and BertKT. Although the accuracy in CoT slightly decreased, its F1 score was however obviously improved. 
}
\label{tab:result_eduagent}
\begin{tabular}{l|lll|lllll}
\hline
\multirow{2}{*}{Metric} & \multicolumn{3}{l|}{GPT4o-Mini (Without/With TIR)} &  \multicolumn{5}{l}{Deep Learning Models} \\ \cline{2-9} 
                        & Standard  & CoT  & BertKT  & DKT  & AKT  & ATKT  & DKVMN  & SimpleKT  \\ \hline
Accuracy & 0.6025+0.0469  & 0.6222-0.0049  & \textbf{0.6074+0.0938}  & 0.6351  & 0.6171 & 0.6396  & 0.6171  & 0.6772  \\
F1 score     & 0.5128+0.1346  & 0.5610+0.0341  & \textbf{0.6110+0.0770}  & 0.6352  & 0.6051  & 0.6390  & 0.6051  & 0.6698 \\ \hline
\end{tabular}
\end{table*}


\mytextcolor{
It is worth noting that the iterative reflection process only happens in the training set. Moreover, we do not limit the LLMs’ reflection to be either content-specific or metacognitive, in order to give LLMs free enough space to do reflection. But we make it generalizable by evaluating whether the reflections can be transferred to a novice agent for prediction. In addition, the reflection has to be both content-specific and metacognitive, because the course modulation effect is usually different across different lectures. So content-specific reflection is necessary to adapt to specific course context. However, LLMs also have metacognitive reflections because the example demonstration students in the training set are different from the simulated students in the testing set.
%
In our current setting, \textit{we only need to run the iterative reflection once per lecture to generate the successful reflection database for that speicific lecture}. There is no need to run it for each question/knowledge concept/student. Running reflection offline for each lecture is reasonable, because the lecture materials are usually prepared in advance and available well before the class in real-world teaching scenarios. 
We have not tested if TIR can generalize across different lectures by using one single lecture’s reflection database, but this can be one promising future exploration.
%
The different reflection direction means that LLMs are instructed to reflect in another reasoning about why a wrong prediction is made. But we do not limit the specific direction content to give LLMs enough space to explore. Examples about such different directions are in Fig. \ref{prompt example} and Appendix Fig. \ref{fig:std_prompt_ex}.  
}



\subsection{TIR Augments CoT-based Models}

Existing work has shown that using the chain of thought (CoT) prompting strategy can improve the capability of LLMs \cite{wei2023chainofthoughtpromptingelicitsreasoning}. The idea of CoT is to use prompts to guide the LLMs to reason step by step like a human, instead of solving the problems all at once.  
Our TIR module can be integrated into CoT to further improve the simulation accuracy of prompting-based models. The integration works similarly to that in standard prompting-based model in Section \ref{subsec: TIR augments prompts}. \mytextcolor{One example workflow is depicted in Appendix Fig. \ref{fig:CoT_prompt_ex}}. The only difference is that we provide step-by-step guidance to the LLMs 
%xyz: What does this mean? 
whenever predicting future questions' correctness, as depicted below. 
    
\begin{enumerate}
    \item Analyze the student's past performance:
    \begin{itemize}
        \item Identify course concepts in past questions the student has performed well in and those they have struggled with.
        \item Consider the complexity of the questions and the related course materials.
    \end{itemize}
    \item Review the course concepts and related lecture materials of the future questions:
    \begin{itemize}
        \item Determine the difficulty level of the future questions based on the related course concepts and course materials.
        \item Identify if the future questions are related to the concepts of past questions that the student has previously struggled with or excelled in.
    \end{itemize}
    \item Predict the student's performance in future questions:
    \begin{itemize}
        \item Based on the analysis from steps (1) and (2), predict whether the student will answer each future question correctly or not.
    \end{itemize}
\end{enumerate}



\subsection{TIR Augments Finetuning-based Models}
In addition to prompting-based methods, TIR can also improve finetuning-based language models by compressing the input tokens to avoid token overflow. We fine-tune BERT (Bidirectional Encoder Representations from Transformers), a language representation model that has pre-trained weights \cite{DBLP:journals/corr/abs-1810-04805}. The input of BERT is a sentence and the output could be anything from question answering to semantic classification. However, BERT has very low token limits (512 tokens), which apparently can not directly handle all past/future question inputs or related course materials. The TIR module solves this problem by distilling useful reflections from such data so that there is no need to input the extremely long course materials into BERT.
As depicted in Fig. \ref{framework:finetune}, the input of the TIR augmented BERT is composed of three parts: future question contents $q_{future}$, initial LLMs-based future question correctness prediction results, and reflections from the TIR module. The model output is a binary value to decide whether one student answers one future question correctly or not. This is achieved by finetuning the BERT-based classifier from HuggingFace \footnote{https://huggingface.co/docs/transformers}.
In the training stage, we have the labels for the training set, so the TIR module directly runs on the training set to generate successful reflections, as depicted in Fig.~\ref{framework:finetune}(a) and Section \ref{subsec: TIR}. However, in the testing stage, the labels can not be used for TIR to avoid label leakage. Therefore, we instead use a new \textit{reflective agent} to generate new reflections based on the retrieved reflections as example demonstrations from the successful reflection database in the training set. 
%which is similar to the case in Section. \ref{subsec: TIR augments prompts}.



To show the effectiveness of our TIR module in augmenting the BERT model, we prepare another baseline BERT model without TIR, which directly takes all information as input (past questions $q_{past}$ with related course materials $c_{past}$ and real past question correctness labels $y_{past}$, future questions $q_{future}$ with related course materials $c_{future}$) and predict the correctness of future questions.
%
We denote the BERT model without TIR as \textit{BertKT}, in contrast to that with TIR (\textit{BertKT+TIR}). \mytextcolor{One example of the workflow is depicted in Appendix Fig. \ref{fig:BertKT_io_eg}.}

\mytextcolor{
For a fair comparison, the fine-tuned data is the same as the training set of deep learning models.  Therefore, we only fine-tune the BertKT once in our data. However, for future potential applications to extend the fine-tuned models in external datasets, it is necessary to fine-tune models again in such new datasets, which is similar to deep learning models that use training data to update model weights. 
}




\subsection{Deep Learning Models}

We have also implemented five deep learning models with pyKT \cite{NEURIPS2022_75ca2b23} for student simulation as baseline models, which come from recent state-of-the-art knowledge tracing models. These five models are from four categories: attention-based models (AKT \cite{10.1145/3394486.3403282}, SimpleKT \cite{DBLP:conf/iclr/0001L0H023}), adversarial-based models (ATKT \cite{10.1145/3474085.3475554}), deep sequential models (DKT \cite{piech2015deepknowledgetracing}), and memory-augmented models (DKVMN \cite{zhang2017dynamickeyvaluememorynetworks}). 
\mytextcolor{
These models are widely used knowledge tracing models in the computational education domain to model student learning \cite{10.1145/3394486.3403282, DBLP:conf/iclr/0001L0H023, 10.1145/3474085.3475554, piech2015deepknowledgetracing, zhang2017dynamickeyvaluememorynetworks}. For example, DKT is the first architecture that applies deep learning to model student learning behaviors \cite{piech2015deepknowledgetracing}, which has become the standard baseline model for benchmarking in computational education domain. AKT \cite{10.1145/3394486.3403282} is the first model that applies the monotonic attention mechanism into student modeling.
}
The Appendix Section \ref{appendix: deep learning baseline} includes more details about each model.

The training and testing scheme in deep learning models are the same as other models for fair comparison, as depicted in Section \ref{subsec: traing testing split}. The model input is the same as BertKT, i.e. past questions $q_{past}$ with related course materials $c_{past}$ and real past question correctness labels $y_{past}$, future questions $q_{future}$ with related course materials $c_{future}$. The model output is the prediction of future question correctness $\hat{y}_{future}$. The difference is that deep learning models can not directly take textual data as input. Therefore, we use BERT again to extract embeddings from the textual data (such as question contents and course materials) as deep learning model input, which is a common practice for knowledge tracing models to predict student performance \cite{DBLP:conf/iclr/0001L0H023}.
Each model is initialized using the default configurations in pyKT. The models are trained with Binary Cross Entropy Loss and the AdamW optimizer with a learning rate of 1e-5. Our pilot experiments show that the model validation accuracy stablizes after about 15 epochs. Therefore, we train each model for 30 epochs and select the best model in validation for testing.












