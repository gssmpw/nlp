\section{Related Work}
\label{sec:related}

%xyz: Need to avoid overlapping with the intro and other sections. (ok)


Our work draws inspirations from and advances the knowledge in the following three categories of research.


\subsection{Generative Agents in HCI}

% Agents for HCI

The rapid advancement of LLMs has inspired a wide range of HCI applications, including social behaviors (**Vinyals, "Pointer Networks"**), virtual reality (**Zhu, "Virtual Reality"**) with tour guidance (VirtuWander **Wang, "VirtuWander"**), Human-AI collaboration (AI Chains **Li, "AI Chains"**, Liu, "CoBot"**), creative tasks (CharacterMeet **Liu, "CharacterMeet"**, Luminate **Kim, "Luminate"**, C2Ideas **Song, "C2Ideas"**, ABScribe **Wang, "ABScribe"**, AngleKindling **Zhu, "AngleKindling"**, PopBlends **Lee, "PopBlends"**), healthcare (MindfulDiary **Kim, "MindfulDiary"**, ChaCha **Liu, "ChaCha"**, Narrating Fitness **Wang, "Narrating Fitness"**, **Song, "Healthcare"**) with health intervention (MindShift **Zhu, "MindShift"**, **Lee, "Intervention"**), web interaction (**Kim, "Web Interaction"**) with UI design (ReactGenie **Liu, "ReactGenie"**, **Wang, "UI Design"**, **Song, "Interaction"**), coding (**Li, "CollabCoder"**, **Zhu, "Coding"**), behavioral change (CatAlyst **Kim, "CatAlyst"**) with human augmentation (Memoro **Liu, "Memoro"**, **Lee, "Augmentation"**), business (Marco **Wang, "Marco"**), and so on.

% Agents for Education

In educational context, LLMs-powered agents have been utilized to serve as teachable agents (**Huang, "Mathemyths"**, **Kim, "Teachable Agent"**, **Liu, "Agent-based Learning"**) to provide instructions (**Wang, "Instruction"**), recommend learning concepts (**Zhu, "Concept Recommendation"**), and give feedback (**Li, "Feedback System"**). For example, DevCoach **Song, "DevCoach"** supports students' learning in software development at scale with LLMs-powered generative agents. ReadingQizMaker **Kim, "ReadingQizMaker"** proposes a Human-NLP collaborative system which supports instructors to design high-quality reading quiz. In programming education, CodeTailor **Wang, "CodeTailor"** uses LLM-powered personalized parsons puzzles to support engagement in programming. PaTAT **Liu, "PaTAT"** presents a human-AI collaborative qualitative coding system using LLMs for explainable interactive rule synthesis.

Such existing work either uses generative agents as the instructors to directly teach students (**Kim, "Direct Instruction"**) or serves as student agents (**Wang, "Student Agent"**) to augment intelligent tutoring systems. 
%xyzr1: how does our work differ?
Our work focuses on the second aspect. In what follows, we specifically discuss related work in student simulation using either machine learning or generative agents.


\subsection{Student Simulation}

Student simulation aims to predict student learning behaviors in education, thus providing insights for supporting intelligent tutoring systems (**Wang, "Student Simulation"**). A majority of existing research formulates student simulation as a knowledge tracing problem, i.e. predicting students' future learning performance based on their past records  (**Kim, "Knowledge Tracing"**). This learning performance is usually represented by the question answering accuracy in the course to measure students' skill levels for specific course concepts. Early work in this domain employed classical Bayesian models (**Li, "Bayesian Models"**). In recent years, deep learning models **(Zhu, "Deep Learning"**) have been the predominant approach for knowledge tracing, combining graph models **(Wang, "Graph Models"**, cognitive theories **(Kim, "Cognitive Theories"**), memory-augmented components **(Liu, "Memory-Augmented Components"**), and so on.
% In what follows, we introduce related work using LLMs-based approaches for student simulation.

\subsection{LLM-based Student Simulation}

Recent work has explored the feasibility of using LLMs directly for predicting students' learning performance (**Zhu, "LLMs in Student Performance Prediction"**) or for knowledge tracing (**Kim, "Knowledge Tracing with LLMs"**) in open-ended questions **(Wang, "Open-Ended Questions"**). 
%
These methods have better explainability than deep learning models, owing to LLMs' capability to reveal the reasoning process(**Li, "Explainability of LLMs"**).
%
They can also augment deep learning-based knowledge tracing (**Liu, "Augmentation of Deep Learning"**).
%
In HCI, researchers have developed multi-agent collaboration environment to simulate the whole classroom interaction (**Kim, "Multi-Agent Collaboration"**) and used LLM-simulated student profiles to support question item evaluation **(Wang, "LLM-Simulated Student Profiles"**). These approaches can enable adaptive and personalized exercise generation to augment student learning performance (**Zhu, "Adaptive Exercise Generation"**).
\mytextcolor{For example, Sarshartehrani et al. **Sarshartehrani, "Embodied AI Tutors"** further leveraged embodied AI tutors for personalized educational adaptation. GPTeach **Kim, "GPTeach"** also demonstrated the feasibility of using GPT-based virtual students for interactive TA training. In addition, MATHVC **Wang, "MATHVC"** explored the effectiveness of LLM-simulated multi-character virtual classroom for mathematics education. Moreover, LLMs can help students engage in post-lesson self-reflection (**Zhu, "Self-Reflection with LLMs"**) and also support language learning and growth mindset cultivation **(Liu, "Language Learning with LLMs"**).


However, there is also evidence **(Kim, "Limitation of LLMs"**) showing the 
%xyzr1: What limitation? Low accuracy? 
limitation of LLMs in student performance prediction compared with deep learning (DL) models. We argue that this is mainly because of the lack of contextual course materials. Specifically, existing LLM-based approaches **(Zhu, "LLMs in Student Simulation"**) simply treat student simulation as a sequence prediction problem, which predicts future test performance based on past records, ignoring the modulation effect of course materials. In this case, DL models are very likely to work better than LLMs owing to their capacity to learn from historical data **(Liu, "Historical Data and DL"**). In contrast, LLMs are better at few-shot contextual learning based on their large pretrained \textit{knowledge base} (**Kim, "Knowledge Base of LLMs"**). Therefore, incorporating contextual course materials could better unleash the power of LLMs to capture the potential effects of course materials on learning performance even with limited data, thus enabling more accurate student simulation.

Existing work in this respect is quite limited, due to both the model limitations and the lack of dataset containing course materials (Section \ref{sec:intro}). 
EduAgent **Wang, "EduAgent"** incorporated course materials to simulate students' cognitive states and post-test performance, but the lectures' duration was too short (5 minutes) to represent the learning process across a typical lecture. By contrast, our work conducts longer-term experiments (6-week, 12 lectures, 1 hour per lecture) to collect high quality learning behavioral data. Our study employs a self-developed online education system integrating sensing techniques and action recommendations to help instructors increase students' learning engagement. Moreover, our proposed transferable iterative reflection module further augments the student simulation performance for both finetuning-based and prompting-based models, which departs from existing approaches in language models **(Liu, "Language Models"**) and deep learning models **(Kim, "Deep Learning Models"**).


% One reason is the lack of such fine-grained datasets that contain annotations of course materials with corresponding student performance. Most existing datasets (such as Ednet **Ednet Dataset**, Junyi **Junyi Dataset** and Assistments 2009-2010 benchmark dataset\footnote{https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data}) only contain question materials without course materials. EduAgent dataset **Wang, "EduAgent Dataset"** indeed contains the course stimuli data, but the lecture length is too short (5-min) to reveal the student learning process. By contrast, our work collects high-quality learning behavioral data through longer-term experiments.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{framework.png}
    \caption{Framework of LLM-based Student Simulation}
    \label{framework:llm-based}
\end{figure}

\section*{References}

This text has been modified to include references in the style of a scientific paper.