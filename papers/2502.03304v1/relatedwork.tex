\section{Related Work}
\subsection{Fine-tuning of Pre-trained Models}

% Fine-tuning a pre-trained model is a widely used training paradigm that enables pre-trained models containing rich information to adapt to downstream tasks quickly, greatly reduce the cost compared to train from scratch, and achieve even better results~\cite{ gururangan2020don,ouyang2022training}. Starting from the success of fine-tuning pre-trained NLP models like BERT~\cite{devlin2018bert}, RoBERTa~\cite{liu2019roberta}, and GPT~\cite{chen2022visualgpt}, such techniques also show advantages in fine-tuning pre-trained vision models like CLIP~\cite{radford2021learning}, SWAG~\cite{singh2022revisiting}, and AlBEF~\cite{li2022align}. 

Fine-tuning a pre-trained model offers a powerful way to reuse learned representations and reduce training costs compared to building models from scratch, often achieving superior performance~\cite{gururangan2020don,ouyang2022training}. Initially successful in NLP with models like BERT, RoBERTa, and GPT~\cite{devlin2018bert,liu2019roberta,chen2022visualgpt}, fine-tuning has also shown promise in vision tasks such as CLIP and SWAG~\cite{radford2021learning,singh2022revisiting}. Recent parameter-efficient fine-tuning (PEFT), including LoRA~\cite{hu2021lora}, and prefix tuning~\cite{li2021prefix}, further minimize resource needs by updating only a small subset of parameters, preserving most of the pre-trained weights and ensuring valuable knowledge is retained.
% Recently, some research shows that the distance between the pre-trained model and the fine-tuned model may affect the capacity of the fine-tuned model, the distance becomes an effective indicator for measuring the degree of forgetting and robustness of the fine-tuned model. \citet{dong2021should} leverage mutual information between the pre-trained and fine-tuned model to control the process of fine-tuning, while \citet{wang2024pre} replace the mutual information with KL divergence and add it as a training target.

\subsection{Zeroth-order Optimization and Acceleration}

ZO optimization emerges as an attractive technique that optimizes the model without backpropagation~\cite{ chen2023deepzero, chen2017zoo, ye2018hessian, verma2023certified, dhurandhar2018explanations, dhurandhar2019model}. Unlike most frequently used FO optimization which directly obtains and leverages the gradient for optimization, the zeroth-order method utilizes objective function value oracle only, estimating the gradient by finite differences.
ZO method has a wide range of applications in machine learning fields, including adversarial attack and defense~\cite{ chen2017zoo, ye2018hessian, verma2023certified}, machine learning explainability~\cite{dhurandhar2018explanations, dhurandhar2019model}, reinforcement learning~\cite{vemula2019contrasting}, and on-chip training~\cite{gu2021efficient}. 
Recently, the ZO method has been proposed to be leveraged on LLM fine-tuning to address the significant memory usage. \citet{malladi2023fine} proposed MeZO, first scaling ZO optimization to fine-tuning parameter-intensive LLMs, greatly reducing memory utilization. On top of MeZO, \citet{zhao2024second} proposed HiZOO, leveraging the estimated Hessian information for better learning capacity, but reducing the throughput of MeZO to some extent.


% ZO optimization, although gradient-free, converges much slower than FO methods due to higher variance from random search. 
% \citet{liu2018zeroth} introduced ZO-SVRG by incorporating variance reduction techniques~\cite{johnson2013accelerating}. \citet{shu2023zeroth} proposed using a Gaussian process to model objective function queries, thereby reducing query complexity and allowing more frequent queries to lower gradient variance. \citet{sener2020learning} performed random search on a learned low-dimensional manifold, reducing the number of needed objective queries. 
ZO optimization, although it significantly saves memory, converges more slowly than FO methods due to higher variance from random search. 
% Prior work has incorporated variance reduction~\cite{liu2018zeroth}, Gaussian processes~\cite{shu2023zeroth}, or dimensionality reduction~\cite{sener2020learning} to address the issue.
\citet{liu2018zeroth} introduced ZO-SVRG by incorporating variance reduction techniques~\cite{johnson2013accelerating}. \citet{shu2023zeroth} proposed using a Gaussian process to model objective function queries, thereby reducing query complexity and allowing more frequent queries to lower gradient variance. \citet{sener2020learning} performed random search on a learned low-dimensional manifold, reducing the number of needed objective queries. 
However, existing ZO accelerators face two main challenges when adapting to ZO fine-tuning for LLMs. First, these approaches were typically designed for smaller-scale tasks involving fewer parameters and less data, and cannot be directly extended to large-scale LLMs. Second, many prior methods focus on improving query efficiency, whereas recent work has shown that a single query can suffice for LLM fine-tuning~\cite{malladi2023fine}. How to effectively accelerate ZO optimization on large model fine-tuning remains a problem.



% Considering the model size of language models (LMs) increased exponentially, many recent works leverage ZO for fine-tuning to alleviate the effort in memory, as ZO only needs two forward processes to estimate the gradient, thus providing change to get free from storing the most memory-consuming information in backpropagation, i.e., weight gradient and intermediate activation. MeZO~\cite{malladi2023fine} introduced a ZO-SGD algorithm to fine-tune LLMs in a memory-efficient manner. \cite{liu2024sparse} introduced parameters-efficient fine-tuning (PEFT) on top of MeZO, which applies ZO only to a carefully chosen subset of parameters, resulting in obvious speed up without performance drop. \cite{tang2024private} proposed differentially private ZO, efficiently fine-tuning LLMs in a privacy-preserved manner. \cite{ling2024convergence} explored the integration of Memory-efficient Zeroth-Order Optimization within a federated setting, aiming to deploy on clients with limited memory.