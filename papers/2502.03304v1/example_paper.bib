@article{hoffmann2022empirical,
  title={An empirical analysis of compute-optimal large language model training},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30016--30030},
  year={2022}
}
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{gholami2024ai,
  title={AI and memory wall},
  author={Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W and Keutzer, Kurt},
  journal={IEEE Micro},
  year={2024},
  publisher={IEEE}
}
@inproceedings{zeng2024flightllm,
  title={Flightllm: Efficient large language model inference with a complete mapping flow on fpgas},
  author={Zeng, Shulin and Liu, Jun and Dai, Guohao and Yang, Xinhao and Fu, Tianyu and Wang, Hongyi and Ma, Wenheng and Sun, Hanbo and Li, Shiyao and Huang, Zixiao and others},
  booktitle={Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
  pages={223--234},
  year={2024}
}
@article{chen2024understanding,
  title={Understanding the potential of fpga-based spatial acceleration for large language model inference},
  author={Chen, Hongzheng and Zhang, Jiahao and Du, Yixiao and Xiang, Shaojie and Yue, Zichao and Zhang, Niansong and Cai, Yaohui and Zhang, Zhiru},
  journal={ACM Transactions on Reconfigurable Technology and Systems},
  year={2024},
  publisher={ACM New York, NY}
}
@article{hur2023fast,
  title={A fast and flexible FPGA-based accelerator for natural language processing neural networks},
  author={Hur, Suyeon and Na, Seongmin and Kwon, Dongup and Kim, Joonsung and Boutros, Andrew and Nurvitadhi, Eriko and Kim, Jangwoo},
  journal={ACM Transactions on Architecture and Code Optimization},
  volume={20},
  number={1},
  pages={1--24},
  year={2023},
  publisher={ACM New York, NY}
}
@article{zhang2024revisiting,
  title={Revisiting zeroth-order optimization for memory-efficient llm fine-tuning: A benchmark},
  author={Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D and Yin, Wotao and Hong, Mingyi and others},
  journal={arXiv preprint arXiv:2402.11592},
  year={2024}
}
@article{liu2024sparse,
  title={Sparse mezo: Less parameters for better performance in zeroth-order llm fine-tuning},
  author={Liu, Yong and Zhu, Zirui and Gong, Chaoyu and Cheng, Minhao and Hsieh, Cho-Jui and You, Yang},
  journal={arXiv preprint arXiv:2402.15751},
  year={2024}
}
@article{malladi2023fine,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}
@article{gautam2024variance,
  title={Variance-reduced zeroth-order methods for fine-tuning language models},
  author={Gautam, Tanmay and Park, Youngsuk and Zhou, Hao and Raman, Parameswaran and Ha, Wooseok},
  journal={arXiv preprint arXiv:2404.08080},
  year={2024}
}
@article{verma2023certified,
  title={Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser},
  author={Verma, Astha and Bangar, Siddhesh and Subramanyam, A Venkata and Lal, Naman and Shah, Rajiv Ratn and Satoh, Shin'ichi},
  journal={arXiv preprint arXiv:2304.06430},
  year={2023}
}
@article{dhurandhar2019model,
  title={Model agnostic contrastive explanations for structured data},
  author={Dhurandhar, Amit and Pedapati, Tejaswini and Balakrishnan, Avinash and Chen, Pin-Yu and Shanmugam, Karthikeyan and Puri, Ruchir},
  journal={arXiv preprint arXiv:1906.00117},
  year={2019}
}
@article{wang2022zarts,
  title={Zarts: On zero-order optimization for neural architecture search},
  author={Wang, Xiaoxing and Guo, Wenxuan and Su, Jianlin and Yang, Xiaokang and Yan, Junchi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12868--12880},
  year={2022}
}
@inproceedings{gu2021efficient,
  title={Efficient on-chip learning for optical neural networks through power-aware sparse zeroth-order optimization},
  author={Gu, Jiaqi and Feng, Chenghao and Zhao, Zheng and Ying, Zhoufeng and Chen, Ray T and Pan, David Z},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={9},
  pages={7583--7591},
  year={2021}
}
@article{liu2018zeroth,
  title={Zeroth-order stochastic variance reduction for nonconvex optimization},
  author={Liu, Sijia and Kailkhura, Bhavya and Chen, Pin-Yu and Ting, Paishun and Chang, Shiyu and Amini, Lisa},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{shamir2017optimal,
  title={An optimal algorithm for bandit and zero-order convex optimization with two-point feedback},
  author={Shamir, Ohad},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={52},
  pages={1--11},
  year={2017}
}
@inproceedings{ji2019improved,
  title={Improved zeroth-order variance reduced algorithms and analysis for nonconvex optimization},
  author={Ji, Kaiyi and Wang, Zhe and Zhou, Yi and Liang, Yingbin},
  booktitle={International conference on machine learning},
  pages={3100--3109},
  year={2019},
  organization={PMLR}
}
@inproceedings{sarafian2020explicit,
  title={Explicit Gradient Learning for Black-Box Optimization.},
  author={Sarafian, Elad and Sinay, Mor and Louzoun, Yoram and Agmon, Noa and Kraus, Sarit},
  booktitle={ICML},
  pages={8480--8490},
  year={2020}
}
@inproceedings{maheswaranathan2019guided,
  title={Guided evolutionary strategies: Augmenting random search with surrogate gradients},
  author={Maheswaranathan, Niru and Metz, Luke and Tucker, George and Choi, Dami and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning},
  pages={4264--4273},
  year={2019},
  organization={PMLR}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{zhao2024second,
  title={Second-order fine-tuning without pain for llms: A hessian informed zeroth-order optimizer},
  author={Zhao, Yanjun and Dang, Sizhe and Ye, Haishan and Dai, Guang and Qian, Yi and Tsang, Ivor W},
  journal={arXiv preprint arXiv:2402.15173},
  year={2024}
}
@article{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}
@inproceedings{voorhees2000building,
  title={Building a question answering test collection},
  author={Voorhees, Ellen M and Tice, Dawn M},
  booktitle={Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={200--207},
  year={2000}
}
@inproceedings{yao2020pyhessian,
  title={Pyhessian: Neural networks through the lens of the hessian},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={2020 IEEE international conference on big data (Big data)},
  pages={581--590},
  year={2020},
  organization={IEEE}
}
@article{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}
@inproceedings{dagan2005pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine learning challenges workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}
@inproceedings{bar2006second,
  title={The second pascal recognising textual entailment challenge},
  author={Bar-Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
  booktitle={Proceedings of the second PASCAL challenges workshop on recognising textual entailment},
  volume={1},
  year={2006},
  organization={Citeseer}
}
@inproceedings{giampiccolo2007third,
  title={The third pascal recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, William B},
  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages={1--9},
  year={2007}
}
@article{bentivogli2009fifth,
  title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  journal={TAC},
  volume={7},
  number={8},
  pages={1},
  year={2009},
  publisher={Citeseer}
}
@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}
@inproceedings{roemmele2011choice,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={2011 AAAI spring symposium series},
  year={2011}
}
@inproceedings{khashabi2018looking,
  title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={252--262},
  year={2018}
}
@article{zhang2018record,
  title={Record: Bridging the gap between human and machine commonsense reading comprehension},
  author={Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1810.12885},
  year={2018}
}
@article{liu2019roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692}
}
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{pilehvar2018wic,
  title={WiC: the word-in-context dataset for evaluating context-sensitive meaning representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  journal={arXiv preprint arXiv:1808.09121},
  year={2018}
}
@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}
@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, P},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}
@inproceedings{tu2019autozoom,
  title={Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks},
  author={Tu, Chun-Chen and Ting, Paishun and Chen, Pin-Yu and Liu, Sijia and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho-Jui and Cheng, Shin-Ming},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={742--749},
  year={2019}
}
@inproceedings{chen2017zoo,
  title={Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models},
  author={Chen, Pin-Yu and Zhang, Huan and Sharma, Yash and Yi, Jinfeng and Hsieh, Cho-Jui},
  booktitle={Proceedings of the 10th ACM workshop on artificial intelligence and security},
  pages={15--26},
  year={2017}
}
@article{ye2018hessian,
  title={Hessian-aware zeroth-order optimization for black-box adversarial attack},
  author={Ye, Haishan and Huang, Zhichao and Fang, Cong and Li, Chris Junchi and Zhang, Tong},
  journal={arXiv preprint arXiv:1812.11377},
  year={2018}
}
@article{zhang2022robustify,
  title={How to robustify black-box ml models? a zeroth-order optimization perspective},
  author={Zhang, Yimeng and Yao, Yuguang and Jia, Jinghan and Yi, Jinfeng and Hong, Mingyi and Chang, Shiyu and Liu, Sijia},
  journal={arXiv preprint arXiv:2203.14195},
  year={2022}
}

@article{dhurandhar2018explanations,
  title={Explanations based on the missing: Towards contrastive explanations with pertinent negatives},
  author={Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@inproceedings{vemula2019contrasting,
  title={Contrasting exploration in parameter and action space: A zeroth-order optimization perspective},
  author={Vemula, Anirudh and Sun, Wen and Bagnell, J},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2926--2935},
  year={2019},
  organization={PMLR}
}
@article{gu2021optimizing,
  title={Optimizing large-scale hyperparameters via automated learning algorithm},
  author={Gu, Bin and Liu, Guodong and Zhang, Yanfu and Geng, Xiang and Huang, Heng},
  journal={arXiv preprint arXiv:2102.09026},
  year={2021}
}
@inproceedings{wang2024pre,
  title={Pre-trained model guided fine-tuning for zero-shot adversarial robustness},
  author={Wang, Sibo and Zhang, Jie and Yuan, Zheng and Shan, Shiguang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24502--24511},
  year={2024}
}
@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@inproceedings{reddi2016stochastic,
  title={Stochastic variance reduction for nonconvex optimization},
  author={Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
  booktitle={International conference on machine learning},
  pages={314--323},
  year={2016},
  organization={PMLR}
}
@inproceedings{shu2023zeroth,
  title={Zeroth-order optimization with trajectory-informed derivative estimation},
  author={Shu, Yao and Dai, Zhongxiang and Sng, Weicong and Verma, Arun and Jaillet, Patrick and Low, Bryan Kian Hsiang},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}
@article{sener2020learning,
  title={Learning to guide random search},
  author={Sener, Ozan and Koltun, Vladlen},
  journal={arXiv preprint arXiv:2004.12214},
  year={2020}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@inproceedings{chen2022visualgpt,
  title={Visualgpt: Data-efficient adaptation of pretrained language models for image captioning},
  author={Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18030--18040},
  year={2022}
}
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@inproceedings{singh2022revisiting,
  title={Revisiting weakly supervised pre-training of visual perception models},
  author={Singh, Mannat and Gustafson, Laura and Adcock, Aaron and de Freitas Reis, Vinicius and Gedik, Bugra and Kosaraju, Raj Prateek and Mahajan, Dhruv and Girshick, Ross and Doll{\'a}r, Piotr and Van Der Maaten, Laurens},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={804--814},
  year={2022}
}
@inproceedings{li2022align,
  title={Align and prompt: Video-and-language pre-training with entity prompts},
  author={Li, Dongxu and Li, Junnan and Li, Hongdong and Niebles, Juan Carlos and Hoi, Steven CH},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4953--4963},
  year={2022}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{de2019commitmentbank,
  title={The commitmentbank: Investigating projection in naturally occurring discourse},
  author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
  booktitle={proceedings of Sinn und Bedeutung},
  volume={23},
  number={2},
  pages={107--124},
  year={2019}
}
@article{dua2019drop,
  title={DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs},
  author={Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  journal={arXiv preprint arXiv:1903.00161},
  year={2019}
}
@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}
@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{chen2023deepzero,
  title={Deepzero: Scaling up zeroth-order optimization for deep model training},
  author={Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia},
  journal={arXiv preprint arXiv:2310.02025},
  year={2023}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{dong2021should,
  title={How should pre-trained language models be fine-tuned towards adversarial robustness?},
  author={Dong, Xinshuai and Luu, Anh Tuan and Lin, Min and Yan, Shuicheng and Zhang, Hanwang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4356--4369},
  year={2021}
}
@article{oh2023towards,
  title={Towards calibrated robust fine-tuning of vision-language models},
  author={Oh, Changdae and Kim, Mijoo and Lim, Hyesu and Park, Junhyeok and Jeong, Euiseog and Cheng, Zhi-Qi and Song, Kyungwoo},
  journal={arXiv preprint arXiv:2311.01723},
  year={2023}
}
@article{zhai2023investigating,
  title={Investigating the catastrophic forgetting in multimodal large language models},
  author={Zhai, Yuexiang and Tong, Shengbang and Li, Xiao and Cai, Mu and Qu, Qing and Lee, Yong Jae and Ma, Yi},
  journal={arXiv preprint arXiv:2309.10313},
  year={2023}
}
@inproceedings{li2024flexnn,
  title={FlexNN: Efficient and Adaptive DNN Inference on Memory-Constrained Edge Devices},
  author={Li, Xiangyu and Li, Yuanchun and Li, Yuanzhe and Cao, Ting and Liu, Yunxin},
  booktitle={Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
  pages={709--723},
  year={2024}
}
@inproceedings{ye2024asteroid,
  title={Asteroid: Resource-Efficient Hybrid Pipeline Parallelism for Collaborative DNN Training on Heterogeneous Edge Devices},
  author={Ye, Shengyuan and Zeng, Liekang and Chu, Xiaowen and Xing, Guoliang and Chen, Xu},
  booktitle={Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
  pages={312--326},
  year={2024}
}
@article{yang2019end,
  title={End-to-end open-domain question answering with bertserini},
  author={Yang, Wei and Xie, Yuqing and Lin, Aileen and Li, Xingyu and Tan, Luchen and Xiong, Kun and Li, Ming and Lin, Jimmy},
  journal={arXiv preprint arXiv:1902.01718},
  year={2019}
}
@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}
@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}
@article{zheng2020end,
  title={End-to-end object detection with adaptive clustering transformer},
  author={Zheng, Minghang and Gao, Peng and Zhang, Renrui and Li, Kunchang and Wang, Xiaogang and Li, Hongsheng and Dong, Hao},
  journal={arXiv preprint arXiv:2011.09315},
  year={2020}
}
