%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsthm}
% \usepackage{subcaption}
% \newcommand{\changed}[1]{\textcolor{blue}{#1}}
% \newcommand{\todo}[1]{\textcolor{red}{#1}}
\newtheorem{hypothesis}{Hypothesis}
\usepackage{caption}
\usepackage{float} 
%\usepackage{subfigure}
% \usepackage{subcaption}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{diagbox}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{enumitem}

% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{graphicx}

\newcommand{\cmark}{\textcolor{green!50!black}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\newcommand{\neutral}{\textcolor{blue!50!white}{\ding{108}}}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{colortbl}
\usepackage{tikz}      % 用于绘制图形
\usepackage{colortbl}
\usepackage{float}
\usepackage{listings}
\lstset{
    language=Python,              % 设置语言为 Python
    basicstyle=\rmfamily\small,   % 基础字体样式
    keywordstyle=\color{red},    % 关键词颜色
    commentstyle=\color{green!50!black},    % 注释颜色
    frame=single,                 % 添加边框
    breaklines=true,              % 自动换行
    tabsize=4                     % Tab 键大小
}

% \definecolor{background}{RGB}{245,245,245} % 背景色
% \definecolor{keyword}{RGB}{183,72,182}     % 关键词颜色
% \definecolor{string}{RGB}{196,26,22}       % 字符串颜色
% \definecolor{comment}{RGB}{0,128,0}        % 注释颜色
% \definecolor{variable}{RGB}{51,51,255}     % 变量颜色
% \definecolor{number}{RGB}{170,13,145}      % 数字颜色

% % 自定义minted样式
% \lstset{
%     language=Python,
%     bgcolor=background,          % 背景色
%     frame=lines,                 % 边框
%     framesep=2mm,                % 边框和内容间距
%     linenos,                     % 显示行号
%     numbersep=5pt,               % 行号间距
%     fontfamily=tt,               % 等宽字体
%     fontsize=\small,             % 字体大小
%     xleftmargin=10pt,            % 左边距
%     breaklines=true,             % 自动换行
%     tabsize=4,                   % Tab 大小
%     keywordstyle=\color{keyword},% 关键词颜色
%     stringstyle=\color{string},  % 字符串颜色
%     commentstyle=\color{comment},% 注释颜色
%     numberstyle=\tiny\color{gray}, % 行号样式
% }

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
% \usepackage{ulem}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}


\begin{document}

\twocolumn[
\icmltitle{Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Qitao Tan}{uga}
\icmlauthor{Jun Liu}{neu}
\icmlauthor{Zheng Zhan}{neu}
\icmlauthor{Caiwen Ding}{umt}
\icmlauthor{Yanzhi Wang}{neu}
\icmlauthor{Jin Lu}{uga}
\icmlauthor{Geng Yuan}{uga}
\end{icmlauthorlist}

\icmlaffiliation{uga}{University of Georgia}
\icmlaffiliation{neu}{Northeastern University}
\icmlaffiliation{umt}{University of Minnesota Twin Cities}

\icmlcorrespondingauthor{Geng Yuan}{geng.yuan@uga.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
% Large language models (LLMs) demonstrate exceptional performance across a variety of tasks, yet standard first-order (FO) fine-tuning requires considerable memory, significantly limiting practical deployment. 
% Recently, zeroth-order (ZO) optimization stood out as a promising memory-saving training paradigm, foregoing memory-intensive backward passes and estimating gradients solely through forward passes, offering an attractive solution for resource-constrained scenarios. 
Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose \textbf{Di}vergence-driven \textbf{Z}eroth-\textbf{O}rder (\textbf{DiZO}) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning.
% Large language models (LLMs) excel in many tasks, but the memory-intensive first-order (FO) fine-tuning greatly limits real-world deployment. In contrast, zeroth-order (ZO) optimization, a promising approach that estimates gradients using only forward passes, significantly reduces memory burden, making it an attractive solution for memory-constrained applications. However, an obvious accuracy and convergence speed gap still exists between these methods and the FO methods. In this work, we first introduce a novel distance-based analysis to investigate the difference in training dynamics between FO and ZO. Aiming to resemble the learning capacity of FO from the findings, we propose \textbf{P}rojection-\textbf{e}nhanced \textbf{Z}eroth-\textbf{O}rder (\textbf{PeZO}) optimization. PeZO conducts projection to the updates from ZO steps, generating customized updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that PeZO significantly reduces the needed iterations for convergence, cutting GPU hours by up to 48\% on a broad range of datasets, all without sacrificing throughput. Moreover, PeZO consistently outperforms the representative ZO method in fine-tuning RoBERTa, OPT, and Llama on various downstream tasks, achieving superior accuracy.
\end{abstract}

\section{Introduction}



Fine-tuning pre-trained large language models (LLMs) with backpropagation demonstrates
superior performance for many natural language processing tasks~\cite{yang2019end, liu2019roberta,talmor2018commonsenseqa,chowdhery2023palm,zheng2020end}. However, the extensive parameterization imposes a substantial memory burden, limiting their practicality for widespread downstream applications.
% Fine-tuning pre-trained large language models (LLMs) is the state-of-the-art approach for many natural language processing tasks. However, the extensive parameterization and high computational costs have become significant constraints in wide applications. 
In line with the neural scaling laws~\cite{hoffmann2022empirical,kaplan2020scaling}, next-generation LLMs continue to increase in parameter count. Specifically, model sizes are expanding at a rate of 410× every two years, dramatically outpacing the scaling of DRAM bandwidth (1.4× every two years) and DRAM capacity (2× every two years). This disparity leads to the \emph{memory wall} challenge~\cite{gholami2024ai}, which becomes even more severe when deploying LLMs on memory-limited devices~\cite{zeng2024flightllm,chen2024understanding,hur2023fast}.

Recently, zeroth-order (ZO) optimization has emerged as a promising memory-efficient training paradigm for LLM fine-tuning, attracting significant attention~\cite{zhang2024revisiting,liu2024sparse,malladi2023fine, zhao2024second}.
By relying solely on forward passes (i.e., inference) to estimate gradients and update model parameters, ZO bypasses the need for backward propagation and significantly reduces extensive storage requirements for activations, gradients, and optimizer states.
% This method intends only to use forward passes (i.e., inference) to estimate the gradients and update the weights accordingly.
% By completely avoiding the computation of backward propagation, ZO simplifies the computation process and eliminates the need for storing extensive data, including activations, gradients, and optimizer state. 
As reported in~\citet{malladi2023fine}, fine-tuning LLMs via ZO optimization reduces up to 12$\times$ memory cost.
Nevertheless, ZO optimization still exhibits a \textbf{gap} in convergence speed and accuracy compared to the conventional first-order (FO) method (i.e., compute gradient via backpropagation). As shown in Table~\ref{table1}, one can observe that the FO method substantially outperforms ZO method in both accuracy and GPU hours. Though ZO method achieves higher throughput due to its computational simplicity, it requires more than 10× iterations for convergence, dramatically increasing GPU hours. Previous studies typically attribute this gap to the fact that ZO optimization leverages random perturbation for gradient estimation, and thus results in unavoidable estimation error, but without further exploration of other underlying causes~\cite{malladi2023fine, gautam2024variance, zhao2024second}. 


% \begin{table}[t]
% \centering
% \scalebox{0.78}{
% \begin{tabular}{lccccc}
% \toprule
% Model                      & \textbf{Type} & \textbf{Acc} & \textbf{Throughput} & \textbf{Iteration} & \textbf{GPU hours} \\ \hline
% \multirow{2}{*}{RoBERTa-L} & FO   & 91.9 & 2.32 it/s  & 6.6\%     & 12.3\%    \\
%                            & ZO   & 90.5 & 5.12 it/s  & 100.0\%     & 100.0\%     \\ \hline \hline
% \multirow{2}{*}{OPT-2.7B}  & FO   & 94.2 & 1.81 it/s  & 7.5\%     & 16.8\%    \\
%                            & ZO   & 90.0 & 3.28 it/s  & 100.0\%     & 100.0\%     \\
% \bottomrule
% \end{tabular}
% }
% \caption{Fine-tuning results on SST-2 datasets. FO outperforms the ZO method in terms of both accuracy and GPU hours.}
% \label{table1}
% \end{table}

% To fill up the identified capacity gap, we begin by analyzing the distinct optimization patterns exhibited by ZO and FO methods in LLM fine-tuning. Our analysis reveals that ZO and FO present notably different training dynamics, reflected by the layer-wise distance between the pre-trained model and the fine-tuned model. Specifically, FO benefits from more accurate gradient estimates, it applies \underline{layer-wise customized updates} according to layer-wise individual optimization needs. In contrast, due to the high-dimensional random search inherent nature of ZO, it applies \underline{layer-wise uniform-magnitude updates}. Motivated by these observations, we are interested in investigating: \emph{if we can also provide ZO with layer-wise customized updates, effectively accelerating the training and enhancing performance.} 
To bridge this gap, we begin by examining the distinct update patterns shown by ZO and FO methods during LLM fine-tuning. Our analysis reveals a substantial difference in their layer-wise update magnitudes. Specifically, ZO method relies on high-dimensional random search and tends to apply \underline{uniform-magnitude updates} without considering layer-wise individual characteristics. In contrast, FO method benefits from fine-grained gradient estimation and applies \underline{diverse-magnitude updates} precisely scaled to the layer-wise individual optimization needs.
Motivated by these observations, we are interested in investigating: \emph{if we can also provide ZO with diverse-magnitude updates, effectively achieving training acceleration and accuracy improvement. }
% as indicated by the variance of layer-wise weight distance gaps between the pre-trained and fine-tuned model. Specifically, ZO optimization, due to its reliance on high-dimensional random search, applies uniform-magnitude updates across all layers without considering individual layer characteristics.   \emph{if we can also provide ZO with layer-wise customized updates, effectively accelerating the training and enhancing performance.}


% To pinpoint the reasons for the gap, we begin by analyzing the distinct training dynamics exhibited by ZO and FO methods in LLM fine-tuning. Our analysis reveals that ZO and FO present notably different layer-wise divergence, reflected by the distance between the pre-trained model and the fine-tuned model. Specifically, FO benefits from more accurate gradient estimation, ensuring each layer receives updates precisely scaled to its individual optimization needs. In contrast, due to the inherent nature of high-dimensional random search, ZO applies non-careful uniform-magnitude updates across layers. Motivated by these observations, we are interested in investigating: \emph{if we can also provide ZO with layer-wise customized updates, effectively accelerating the training and enhancing performance.} 

Drawing on these insights, we propose \textbf{Di}vergence-driven \textbf{Z}eroth-\textbf{O}rder optimization (\textbf{DiZO}). DiZO conducts divergence-driven layer adaptation by incorporating projections, enabling layer-wise adaptive updates that closely resemble FO approaches. Notably, the projections can be optimized without gradients, ensuring that DiZO retains the appealing backpropagation-free features. Moreover, we validate DiZO on a variety of tasks, including classification and generation, using several LLMs such as RoBERTa-large, the OPT series, and the Llama series. Experimental results show that DiZO substantially decreases training iterations for convergence while maintaining throughput, cutting training GPU hours by up to 48\% on diverse datasets. Furthermore, our method can be seamlessly integrated with parameter-efficient tuning techniques like low-rank adaptation~\cite{hu2021lora} for additional speedups. DiZO also consistently outperforms the representative ZO baselines and, in some cases, surpasses memory-intensive FO fine-tuning. 
% \todo{add lora}
% 举个例子，4090 fine-tuning 2.7B， FO装不下，MeZO比较慢

% (某些数据集好于fine-tune，mezo不行) 
% Moreover, by substantially reducing the number of iterations required for convergence and without compromising throughput, PeZO cuts GPU hours by up to 50\% compared to vanilla ZO.

% accelerates convergence by up to 50\% relative to vanilla ZO, substantially reducing the number of training steps required.
% up to 50\% acceleration compared with the vanilla ZO. Moreover, an average 1.5\% accuracy increase is obtained over classification and 2.5\& perplexity increase over generation.

% We found that the slow optimization of ZO is not only due to the inappropriate optimization direction caused by its random search nature but also due to the inappropriate parameter update magnitude of each training step. In detail, the update magnitude of ZO depends on three terms, learning rate, finite difference of the target function, and the magnitude of the random sampled perturbation. The first two terms are the same for all network layers in one single step, however, the last term, due to the dimensional curse, when optimizing high-dimensioned LLM, is also the same for all layers, resulting in all layers and parameters having similar step magnitude. In contrast, the FO method provides distinguishing step magnitude and quickly accumulates layer-wise difference thus having better convergence speed and performance. We summarize the above finding and propose a hypothesis in Section~\ref{hypo}.
% propose the following hypothesis (details in \ref{hypo}): \textbf{\emph{layers of LLM prefer different magnitude gaps with the pre-trained model, ZO needs to take many more steps to accumulate such gaps}}. 


The summary of our contributions is as follows:
\begin{itemize}
    \item We introduce a novel layer-wise divergence analysis to uncover the fundamental differences in the updating patterns of FO and ZO methods.
    \item We introduce DiZO, a novel ZO method using divergence-driven layer adaptation,
    achieving a learning capacity closely resembling FO while maintaining the throughput benefit of ZO optimization.
    \item DiZO consistently exceeds existing baselines in both accuracy and GPU hours, and it can be seamlessly integrated with LoRA for additional benefits. These advantages hold across diverse tasks and LLM architectures.
\end{itemize}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}


% Despite the advantages taken by ZO optimization, it faces the dilemma of slow convergence speed. As shown in~\ref{table1}, ZO achieves higher throughput due to its computational simplicity, but it also takes 20× training steps for convergence and finally results in 10× GPU hours. We attribute the slow convergence to the indiscriminate sampled perturbations for each layer. As shown in Figure~\ref{figure1}, both in FO or ZO, different layers have their different magnitude gap with the pre-trained model (Figure~\ref{figure1} left). However, in ZO optimization, since all layers are perturbed by randomly sampled high-dimensioned random vectors, each of them is perturbed to have a similar magnitude gap with the pre-trained model. Thus, ZO optimization needs to accumulate the gap to achieve the discriminate magnitude gap preferred by each layer, and that is why ZO optimization needs many more steps for convergence compared with the FO method. Based on the above observation, we state the following hypothesis.

\begin{figure*}[ht]
\centering
% \includegraphics[width=1\linewidth]{icml2025/texs/figures/ZOFO_illustration.pdf}
\scalebox{0.95}{
\includegraphics[width=1\linewidth]{FOZO_illustration_new.pdf}
}
\caption{Comparison of the training dynamics of ZO and FO methods. The X-axis represents layer names, and the Y-axis represents the distance gap. Although they converge to different stable states, the divergence of the distance gap increases in both FO and ZO methods during training. FO accumulates divergence rapidly through diverse-magnitude updates, while ZO applies uniform-magnitude updates, requiring more iterations for an ideal divergence level. Results are obtained by fine-tuning OPT-2.7B on the SST-2 dataset, focusing on weights in the attention module: K (Key), V (Value), Q (Query), and O (Output projection). }

\label{zo_fo}
\end{figure*}
\section{Preliminaries and Pattern Analysis}


\subsection{Revisiting Zeroth-order Optimization}
Recently, ZO optimization has gained significant attention in machine learning~\cite{verma2023certified,dhurandhar2019model,wang2022zarts,gu2021efficient}.
Unlike conventional FO optimization, which calculates gradients via backpropagation, ZO optimization estimates gradients using only objective oracles via finite differences \cite{chen2023deepzero, liu2018zeroth, ye2018hessian}. This property can be leveraged for LLM fine-tuning to alleviate the extensive memory costs. Specifically, as ZO only needs two forward passes to obtain the estimated gradients, it avoids computing and storing the most memory-consuming information needed in the conventional FO training, i.e., activations in the forward process, gradients in the backward process, and the optimizer state.
% MeZO~\cite{malladi2023fine} introduced a ZO-SGD algorithm for fine-tuning LMs, reducing memory usage by up to 12× without compromising accuracy. 
% Building on this, \cite{liu2024sparse} applied sparse techniques to MeZO, optimizing only a subset of parameters for faster training without performance loss. \cite{tang2024private} proposed a differentially private ZO method for privacy-preserving fine-tuning, while \cite{ling2024convergence} integrated ZO into federated learning to support memory-constrained clients.

The core idea of ZO optimization is to estimate gradients by applying random perturbations to the weights and computing differences in the objective. For a mini-batch of data $\mathcal{B}$, sampled from a labeled dataset $\mathcal{D} = \{x_{i}, y_{i}\}_{i=1}^{|\mathcal{D}|}$, a model with parameters $\bm{\theta} \in \mathbb{R}^{d}$, where $d$ represents the dimension of the parameter space, and the corresponding loss function $\mathcal{L}(\bm{\theta}; \mathcal{B})$. The gradient is estimated as follows:
\begin{equation}
\label{equ:1}
    \nabla \mathcal{L}(\bm{\theta};\mathcal{B})=\frac{1}{q} \sum_{i=1}^q\left[\frac{\mathcal{L}\left(\bm{\theta}+\epsilon \bm{u}_{i};\mathcal{B}\right)-\mathcal{L}\left(\bm{\theta}-\epsilon \bm{u}_{i};\mathcal{B}\right)}{2 \epsilon} \bm{u}_{i}\right]
\end{equation}
where $\bm{u}_{i}$ is a random vector with the same dimension as the model weights and is typically drawn from standard Gaussian distribution $\mathcal{N}(0, \mathbf{I})$~\cite{malladi2023fine, zhang2024revisiting}, or from Gaussian
sampling over a unit sphere~\cite{liu2018zeroth, shamir2017optimal}, $q$ is the number of objective queries, and $\epsilon > 0$ is a small perturbation scalar for smoothing.
\begin{table}[t]
\centering
\caption{Fine-tuning results on SST-2 datasets. Although ZO method shows advantages in memory saving, left behind FO method in terms of both accuracy and GPU hours.
% \red{Move to page 2 top-right}
}
\vspace{5pt}
\scalebox{0.93}{

\begin{tabular}{lccccc}
\toprule
Model                     & \textbf{Type} & \textbf{Acc.} & \textbf{Memory} & \textbf{\begin{tabular}[c]{@{}c@{}}\#Train\\ Iter.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}GPU \\ Hours\end{tabular}} \\ \hline
\multirow{2}{*}{RoBERTa}  & FO            & 91.9         & 9.2 GB          & 6.6\%                                                          & 12.3\%                                                        \\
                          & ZO            & 90.5         & 4.5 GB          & 100.0\%                                                        & 100.0\%                                                       \\ \hline \hline
\multirow{2}{*}{OPT-2.7B} & FO            & 94.2         & 45.4 GB         & 7.5\%                                                          & 16.8\%                                                        \\
                          & ZO            & 90.0         & 6.8 GB          & 100.0\%                                                        & 100.0\%                                                       \\ \bottomrule
\end{tabular}
}
\vspace{-10pt}
\label{table1}
\end{table}

Given the learning rate $\eta$ and the mini-batch data $\mathcal{B}_{t}$ at $t$-th iteration, once the estimated gradient $\nabla \mathcal{L} (\theta;\mathcal{B}_{t})$ is obtained,
then ZO-SGD updates the parameters with the following rule:
\begin{equation}
\label{equ:2}
    \bm{\theta}_{t+1} = \bm{\theta}_{t} - \eta \nabla \mathcal{L}(\bm{\theta}_{t};\mathcal{B}_{t})
\end{equation}


\subsection{Layer-wise Divergence Analysis}
\label{hypo}
Drawing insight from the update formula of ZO optimization, we notice that ZO method applies uniform-magnitude updates across layers, e.g., the L2-norm of the updates is about the same for all layers in one iteration (see Appendix~\ref{proof} for proof). This fact may be the root of the inferior performance of ZO optimization. To measure how the divergence of update magnitude affects the convergence speed and accuracy, we investigate the training dynamics of ZO and FO methods, respectively.
% To better understand the difference of layer-wise update magnitudes between FO and ZO methods and assess the impact of these differences on training, we analyze their training dynamics respectively.

% Drawing insight from the update formula of ZO optimization, we notice that ZO applies uniform-magnitude updates across layers, e.g., the L2-norm of the updates is about the same for all layers in one iteration (see Appendix~\ref{proof} for proof). This characteristic may be the root of the performance gap between the FO and ZO methods. To this end, aiming to understand the differences in their update patterns further, we explore the divergence of layer-wise update magnitude and its impact on fine-tuning convergence speed and accuracy in the ZO and FO methods, respectively. 
% in the fine-tuning process using FO and ZO, respectively. 
% For simplicity, in the following sections, we use layer-wise divergence to refer to the divergence of layer-wise update magnitude.

% Previous studies on ZO optimization suggest that the convergence speed is related to the variance of estimated gradients, lower variance will result in faster convergence, thus, many ZO acceleration methods have been proposed around variance reduction~\cite{liu2018zeroth, ji2019improved, sener2020learning, sarafian2020explicit, maheswaranathan2019guided}. However, they rather focus on small-scale optimization problems or aim to reduce the number of the function queries (already proofed one is enough in LLM ZO optimization~\cite{malladi2023fine}), while there is little research trying to accelerate ZO LLM fine-tuning from the perspective of training dynamics. 

% For simplicity, in the following sections, we use layer-wise divergence to refer to the divergence of layer-wise update magnitude.

% of the fine-tuned model under both FO and ZO, 
% In the rest of the paper, we refer to it as layer-wise divergence in short.

% Previous works studying the training dynamics of ZO optimization, we investigate the distance between the fine-tuned model and the pre-trained model.}

% We define distance as ...., and we define distance gap

\textbf{Analysis indicator.} To quantify the effect of updates, we adapt the layer-wise L2-norm distance gap between the weights of the pre-trained and the fine-tuned model as an indicator.
The layer-wise L2-norm distance gap is defined as:
\begin{equation}
    \|\Delta \bm{\theta}_{t}^{(\ell)}\| = \|\bm{\theta}_{t}^{(\ell)} - \bm{\theta}_{0}^{(\ell)}\|_2
\end{equation}
where $t$ is $t$-th fine-tuning iteration, $\ell$ is $\ell$-th layer of the model, and $\bm{\theta}_{0}^{(\ell)}$ indicates the weights of $\ell$-th layer of pre-trained model. 

% \textbf{Analysis metric.} To quantify the layer-wise divergence, we adapt the layer-wise L2-norm distance gap between the weights of the pre-trained and the fine-tuned model as an indicator. Intuitively, if each layer undergoes diverse update magnitudes, its distance from the pre-trained model will also be diverse accordingly.
% % Our intuition is that if the layer-wise update magnitude is diverse, then the distance gap of each layer is also supposed to be diverse.  
% % There are distance gaps for layers, the larger the variance of the gaps, the larger the divergence, and vice versa. For example, 
% The layer-wise L2-norm distance gap is defined as:
% \begin{equation}
%     \|\Delta \bm{\theta}_{t}^{(\ell)}\| = \|\bm{\theta}_{t}^{(\ell)} - \bm{\theta}_{0}^{(\ell)}\|_2
% \end{equation}
% where $t$ is $t$-th fine-tuning iteration, $\ell$ is $\ell$-th layer of the model, and $\bm{\theta}_{0}^{(\ell)}$ indicates the $\ell$-th layer of pre-trained model. 
% The larger the variance of distance of layers, the higher the divergence of the layers.
% the layer-wise divergence increases over the fine-tuning process and ultimately converges to a stable state, suggesting that different layers profit from maintaining diverse distance gaps with the pre-trained model

% \textbf{Analysis result.} Figure~\ref{zo_fo} compares the training dynamics of FO and ZO methods. As one can observe, regardless of ZO or FO, the divergence of the distance gap increases during training, i.e., the line of distance gap gradually 'bends', suggesting that different layers profit from maintaining diverse gaps with the pre-trained model. However, FO and ZO differ in how the distance gap divergence is accumulated. FO employs fine-grained gradient estimations, resulting in diverse-magnitude updates (FO updates in Figure~\ref{zo_fo}). Therefore, it can rapidly reach the desired layer-wise distance gap in only a few iterations. In contrast, ZO relies on random search in high-dimensional parameter space and generates uniform-magnitude updates (ZO updates in Figure~\ref{zo_fo}), resulting in thousands more iterations required for accumulating a meaningful layer-wise distance gap. 

\textbf{Analysis result.} Figure~\ref{zo_fo} compares the training dynamics of FO and ZO methods. Regardless of whether ZO or FO is used, the divergence of distance gap among layers grows during training, i.e., the line of distance gap gradually 'bends'. This pattern implies that different layers benefit from maintaining diverse distance gaps with the pre-trained model. However, FO and ZO diverge in how the distance gap divergence is accumulated. FO employs fine-grained gradient estimations, resulting in diverse-magnitude updates (FO updates in Figure~\ref{zo_fo}). Therefore, it can rapidly reach the desired layer-wise distance gap in only a few iterations. In contrast, ZO relies on random search in high-dimensional parameter space and generates uniform-magnitude updates (ZO updates in Figure~\ref{zo_fo}), resulting in thousands more iterations required for accumulating a meaningful layer-wise distance gap. 
% This discrepancy highlights the core challenge of bridging the performance gap between ZO and FO methods.

% \textbf{Analysis result.} Figure~\ref{zo_fo} compares the update patterns of FO and ZO methods. As one can observe, regardless of ZO or FO, the layer-wise distance gap between the fine-tuned model and the pre-trained model becomes more diverse as training progresses and ultimately converges to a stable state. However, FO and ZO differ in how the layer-wise distance gap is accumulated. FO employs fine-grained gradient estimations, resulting in diverse-magnitude updates (FO updates in Figure~\ref{zo_fo}). Therefore, it can rapidly reach the desired layer-wise distance gap in only a few iterations. In contrast, ZO relies on random search in high-dimensional parameter space and generates uniform-magnitude updates (ZO updates in Figure~\ref{zo_fo}), resulting in thousands more iterations required for accumulating a meaningful layer-wise distance gap. 
% In both cases, the layer-wise distance variance increases over fine-tuning iterations and ultimately converges to a stable state, suggesting that different layers profit from maintaining diverse gaps with the pre-trained model. However, FO and ZO differ in how these distance gaps are accumulated. FO employs layer-specific gradient estimations, it rapidly increases the layer-wise divergence and converges in only a few iterations. In contrast, ZO relies on random search in a high-dimensional parameter space and generates non-careful uniform-magnitude updates across layers, resulting in thousands more iterations required for accumulating a meaningful divergence. 
% \bred{HERE!}
% Consequently, ZO needs substantially more iterations to achieve a similar level of layer-wise distance variance.
% optimization, due to the nature of random searching, ZO needs to sample a high-dimensioned perturbation for the update, one can only provide indifferent updates for each layer in one iteration and thus needs much more iteration to accumulate the layer-wise distance variance. 

With the above findings, we suspect the inferior performance of ZO stems from its inability to deliver layer-wise adaptive updates, a challenge that arises from its reliance on random perturbations for gradient estimation. 

% Consequently, in this work, we introduce a variant of ZO optimization capable of performing divergence-driven layer adaptation, thereby enhancing its overall learning capacity.
% \todo{Can we formalize the hypothesis or the dilemma caused by the hypothesis?}

% \begin{hypothesis} 
% \label{hyp:1}
% The slower convergence and inferior performance of ZO stem from its inability to provide layer-wise customized updates, a challenge that arises from its reliance on random perturbations for gradient estimation.
% \end{hypothesis}

% With Hypothesis~\ref{hyp:1}, to narrow the capacity gap between FO and ZO methods, one potential solution could be also to conduct ZO optimization with layer-wise customized updates.
% one may raise the question, \textit{Can we also provide ZO optimization with layer-wise different updates for enhancing?} Fortunately, the answer is positive, we will illustrate our method following.

% Figure~\ref{zo_fo} shows the result of the distance gap with ZO and FO optimization. Different layers have their different distance gap after training. However, for each step, ZO generates indiscriminate distance gaps across all layers, since all layers are perturbed with the same distributed random vector, due to the dimensional curse, they have similar modulus. In contrast, FO steps generate the preferred discriminate distance gap for each layer, which makes the model quickly converge to the ideal distance gap, while ZO needs numerous steps to accumulate such difference. 




\input{algorithm}
\section{Methodology}

We find that ZO applies uniform-magnitude updates for all layers, which could be the root of its inferior performance in accuracy and convergence speed. Consequently, we introduce a variant of ZO optimization which performs divergence-driven layer adaptation, thereby providing diverse-magnitude updates to enhance the overall learning capacity.

\subsection{Design of the Divergence-driven Layer Adaptation}
To provide layer-wise adaptive updates for ZO optimization, we apply projections to the updates of different layers, generating updates with diverse magnitudes. The pseudocode for the proposed method is shown in Algorithm~\ref{alg1}.

Specifically, We treat training iteration as a two-step process that iteratively updates the weights and the projection factor.
Our approach involves two key steps performed in an alternating manner. First, we perform vanilla ZO optimization as defined in Eq.~(\ref{equ:2}). Second, we identify the ideal projections for the weights and apply them, generating the projected weights. Formally, we define the ideal projection learning as solving the following minimization problem:
% \begin{equation}
% \label{equ:3}
% \min_{\bm{\gamma}_t} \mathcal{L}(\bm{\theta}_{0} + \frac{\bm{\gamma}_t}{\|\Delta \bm{\theta}_t\|}\Delta \bm{\theta}_t;\mathcal{B}_{t}) + \sigma \|\bm{\gamma}_t\|
% \end{equation}
\begin{equation}
\label{equ:3}
\min_{\bm{\gamma}_t} \mathcal{L}(\bm{\theta}_{0} + \frac{\bm{\gamma}_t}{\|\Delta \bm{\theta}_t\|}\Delta \bm{\theta}_t;\mathcal{B}_{t})
\end{equation}
% 向量加粗，标量不加粗，参照mezo
where $\bm{\gamma}_{t}=\{\gamma_{t}^{(\ell)}\}_{\ell=1}^{L}$ is a projection vector at $t$-th iteration, and $L$ is the number of layers. 
% Intuitively, $\bm{\gamma_t}$ controls the freedom of layer-wise updates, and thus controls the divergence. 
While searching for the ideal projection, we freeze the model weights and use the same mini-batch data $\mathcal{B}_t$ that is employed for the main ZO weight fine-tuning.

% \todo{add some description for the optimization}

After finding the ideal projection for the $t$-th ZO step, we project the weights as:
\begin{equation}
\label{equ:4}
    \bm{\theta}_{t} = \bm{\theta}_{0} + \frac{\bm{\gamma}_t}{\|\Delta \bm{\theta}_t\|}\Delta \bm{\theta}_t
\end{equation}
where we get the new $\bm{\theta}_t$ after projection, and then we use the projected one for the following fine-tuning. 
When the value of $\bm{\gamma_{t}}$ is larger than $\|\Delta \bm{\theta_t}\|$, enlarges the distance gap between the fine-tuned model and the pre-trained model
% the model moves away from the pre-trained model
% ; otherwise, the distance gap remains closer to it. 
, and vice versa.

% \begin{algorithm}[H]
% \resizebox{0.9\linewidth}{!}{
% \begin{minipage}{\linewidth}
% \caption{Projection-enhanced ZO Optimization (PeZO)}
% \SetAlgoLined
% \For{$t = 1$ \KwTo $T$}{
%     $\hat{\nabla} \mathcal{L} = \text{\texttt{GradEst}}(\theta_{t}, \delta, \mathcal{B})$\;

%     $\theta_{t} = \theta_{t-1} - \eta_{1}\hat{\nabla} \mathcal{L}$\;

%     $\gamma^{*}_{t} = \arg\min_{\gamma_t} \mathcal{L}(\theta_{0} + \frac{\gamma_t}{\|\theta_{t}-\theta_{0}\|}(\theta_{t}-\theta_{0});\mathcal{B}_{t}) + \sigma \|\gamma_t\|$\;

%     $\widetilde{\theta_{t}} = \text{\texttt{ApplyProjection}}(\theta_t,\theta_0,\gamma_t^{*})$\;
% }

% \SetKwFunction{FGradEst}{GradEst}
% \SetKwProg{Subroutine}{Subroutine}{:}{\KwRet}
% \Subroutine{\FGradEst{$\theta$, $\epsilon$}}{
%     \textbf{Sample:} $u_{1},\dots,u_{q} \backsim \mathcal{N}(0, \mathbf{I})$\;

%     \textbf{Query:} $y_{i} = \mathcal{L}(\theta+\delta u_{i}) - \mathcal{L}(\theta-\delta u_{i})$\;

%     \textbf{Estimator:} $\hat{\nabla} \mathcal{L} = \frac{q}{2\epsilon}\sum_{i=1}^{q}y_{i}u_{i}$\;

%     \Return $\hat{\nabla} \mathcal{L}$\;
% }

% \SetKwFunction{FApplyProjection}{ApplyProjection}
% \SetKwProg{Subroutine}{Subroutine}{:}{\KwRet}
% \Subroutine{\FApplyProjection{$\theta_t$, $\theta_0$, $\gamma_t$}}{
%     \For{$l=1,2, \dots, L$}{
%         $\theta_t^{l} = \theta_{0}^{l} + \frac{\gamma^{l}_t}{\|\theta_{t}^{l}-\theta_{0}^{l}\|}(\theta_{t}^{l} - \theta_{0}^{l})$\;
%     }
%     \Return $\theta_t$\;
% }
% \end{minipage}
% }
% \label{alg1}
% \end{algorithm}

\subsection{How to Learn the Projection?}

Although promising, finding the ideal projection (defined in Eq.~(\ref{equ:3})) remains challenging due to the high complexity of the objective. A straightforward solution is to also perform backpropagation for gradient computation and optimize the projection accordingly (FO-based method). For example, we use Adam optimizer to directly update $\bm{\gamma}_{t}$. The results are shown in Table~\ref{compare1}, one can observe that it significantly reduces 67.7\% of the iteration and 58.5\% of the training GPU hours, and increases by 3.4\% in accuracy. These results underscore the effectiveness of incorporating our proposed divergence-driven layer adaptation. 



% \begin{equation}
%     \gamma_{k+1} = \gamma_{k} - \eta_{2} \cdot \nabla_\gamma f(\gamma_k)
% \end{equation}
% where $\eta_{2}$ is the learning rate, and $f(\gamma_{k})$ is the sum of two terms in (\ref{equ:3}).

However, searching projection with the FO method makes DiZO only partially gradient-free. Specifically, while the model weights are updated via ZO, the per-layer projection parameter $\gamma_t^{(\ell)}$ is updated via FO, which still requires the backward pass and storing memory-intensive activation. The only memory saving, compared to the vanilla FO fine-tuning, is the optimizer state. As a result, relying on FO to find the ideal projection, though it achieves faster convergence speed and better accuracy in ZO optimization, offers limited overall benefit. It is worth noting that the peak memory usage during training of the FO-based DiZO is similar to that of low-rank adaptation (LoRA)~\cite{hu2021lora}.
% ; hence the .as it at best is a competitive method with LoRA considering the peak memory cost.

% Although we only update $\gamma$ by FO, and the size is related to the number of layers and is very small compared to the model parameters, the peak memory usage will be similar to LoRA~\cite{hu2021lora}. Thus, if we can only FO method to find an ideal $\gamma$, even though we achieve acceleration and better accuracy in the ZO optimization, it could be less meaningful, it at best is a competitive method with LoRA considering the peak memory cost. 

% \begin{figure}[htbp]
%  \centering
%  \includegraphics[width=0.8\linewidth]{icml2025/texs/figures/figure1.png}
%  \caption{Performance drop of directly leveraging ZO.}
%  \label{figure3}
% \end{figure}

Is the projection-based method for enhancing layer-wise divergence in ZO a failed idea that seems promising at first glance but is actually not after deliberation? Fortunately, the answer is no. We develop a ZO projection learning algorithm, which retains the memory-efficient advantages and also achieves training acceleration and accuracy enhancement. 

\begin{table}[tbp]
\centering
\caption{Fine-tuning OPT-2.7B on SST-2 dataset.~\neutral: partial gradient-free; DiZO\textsuperscript{\textdagger}: learning projection by FO method;
}
\vspace{5pt}
\begin{tabular}{lcccc}
\hline
Task Type                                                         & \textbf{\begin{tabular}[c]{@{}c@{}}Gradient\\ Free\end{tabular}} & \textbf{Acc.} & \textbf{\begin{tabular}[c]{@{}c@{}}\#Train \\ Iter.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}GPU \\ Hours\end{tabular}} \\ \hline
MeZO                                                              & \cmark                                            & 90.0         & 100\%                                                             & 100\%                                                         \\
DiZO\textsuperscript{\textdagger} (w. FO) & \neutral                                          & 93.4         & 33.3\%                                                            & 41.5\%                                                        \\
FT                                                                & \xmark                                            & 94.2         & 9.3\%                                                             & 16.8\%                                                        \\ \hline
\end{tabular}
% \scalebox{0.87}{
% \begin{tabular}{lcccc}
% \toprule
% Task Type & \textbf{Gradient-free} & \textbf{Acc} & \textbf{#Train Iter.} & \textbf{GPU hours} \\ \hline
% MeZO & \cmark                                                                & 90.0         & 100\%                & 100\%                                                         \\
% PeZO\textsuperscript{\textdagger} & \neutral                                                                & 93.4         & 33.3\%               & 41.5\%      \\
% FT   & \xmark                                                                & 94.2         & 9.3\%                 & 16.8\%                                                            \\
% \bottomrule
% \end{tabular}
% }

\label{compare1}
% \vspace{-10pt}
\end{table}
\subsection{Projection Learning by Zeroth-order Optimization}
\label{ZO}

% Another potentially promising solution is that we also utilize the ZO method to update the $\gamma$. However, directly applying vanilla ZO to search $\gamma$ is not feasible, the result is shown in Figure~\ref{figure3}, which will dramatically reduce the performance.

Our major goal is to find the ideal projection for adaptive updates while avoiding memory-intensive backpropagation. One potential promising solution is to also utilize the ZO method to update the projection. We estimate the gradient and update the projection as:
\begin{gather}
\label{equ:5}
    \nabla \widehat{\mathcal{L}}(\bm{\gamma}_{t};\bm{\theta}_{t})=\left[\frac{\widehat{\mathcal{L}}\left(\bm{\gamma}_{t}+\epsilon \bm{u};\bm{\theta}_{t}\right)-\widehat{\mathcal{L}}\left(\bm{\gamma}_{t}-\epsilon \bm{u};\bm{\theta}_{t}\right)}{2 \epsilon} \bm{u}\right] \\
    \bm{\gamma}_{t,j+1} = \bm{\gamma}_{t, j} - \eta \nabla \widehat{\mathcal{L}}(\bm{\gamma}_{t};\bm{\theta}_{t})
\end{gather}
where $\bm{u} \in \mathbb{R}^{L}$ is a random vector from $\mathcal{N}(0, \mathbf{I})$,  $\widehat{\mathcal{L}}$ is the objective defined in Eq.~(\ref{equ:3}).
% where all settings of the symbol are the same as the (\ref{equ:1}) and (\ref{equ:2}).

% Second, since the projection is determined by noisy ZO optimization in a few iterations, an improperly small value can force the fine-tuned model excessively close to the pre-trained model, nullifying many earlier updates and destabilizing the training process.

However, naively applying vanilla ZO optimization for the sub-optimization (projection learning) results in unsatisfactory enhancement. More critically, it can lead to sub-optimization failure and undermine the main fine-tuning process (see Appendix~\ref{ablation} for results). Two primary issues contribute to the failure. First, the values of projections are not only related to $\bm{\gamma}_t$ but also the distance gap $\|\Delta \bm{\theta}_t\|$. Ignoring the distance gap when searching for projections causes uninformative optimization and yields sub-optimal solutions.
Second, because the projection is derived through noisy ZO optimization over only a few iterations, there is a risk that the projection magnitude becomes inappropriately small or large. A small projection drives the fine-tuned model too close to the pre-trained model, nullifying many previous updates, while a large projection applies overly aggressive weight updates, destabilizing the training process.

To address the above issues, two strategies are devised. \\
\textbf{\uline{Re-initialization.}} To introduce the distance gap $\|\Delta \bm{\theta}_{t}\|$ into the projection learning process, the initial value $\bm{\gamma}_{t,0}$ is reset to $\|\Delta \bm{\theta}_{t}\|$ each time the projection is optimized. This means that, initially, the projection magnitude $\frac{\bm{\gamma}_t}{\|\Delta \bm{\theta}_t\|}=1$. If projection updates are not performed, DiZO reverts to standard ZO optimization. \\
\textbf{\uline{Projection clipping.}} To prevent drastic weight changes and maintain training stability, we introduce projection clipping. Specifically, given a clipping range $\tau > 0$, if the projection magnitude $\frac{\bm{\gamma}_t}{\|\Delta \bm{\theta}_t\|} \notin [1-\tau, 1+\tau]$, it is clipped to remain within this interval. This prevents aggressive model adjustments that could destabilize training.

With the above two strategies, we enhance the learning process of projection, more analysis can be found in Appendix~\ref{ablation}. We also provide a Pytorch-style implementation, please refer to Appendix~\ref{code} for details. 



% may caused by projection is that it will make the training unstable. For example, as the projection scalar is optimized by perturbation, if at any step the projection is perturbed improperly, for example to a very small value, the model remains overly close to the pre-trained model. 

% \todo{However, simply leveraging ZO for searching $\gamma_{t}$ only exhibits slight improvement (shown in Table~\ref{compare1}). We attribute it to the vanilla ZO treating the projection magnitude of different layers equally, i.e., sample $\hat{u}$ from the Gaussian distribution with the same parameters. As we present in Figure~\ref{figure4}, for different layers, the projection magnitude is quite different. Although the dimension of the projection is much lower than the parameters, which mitigates the effect of the dimensional curse, we can only use fewer steps to optimize $\gamma$, as the sub-optimization is not supposed to take a large proportion of the computational cost, and thus the magnitude of projections of different layers are still close.}

% % We attribute it to vanilla ZO still treating projection magnitude of layers equally, thus also implicitly violating our hypothesis proposed in Section~\ref{hypo}. Although the dimension of the projection is much lower than the parameters, which mitigates the effect of the dimensional curse, we can only use fewer steps to optimize $\gamma$, as the sub-optimization is not supposed to take a large proportion of the computational cost, and thus the magnitude of projections of different layers are still close. 
% % \todo{It is worth noting that we find that only projecting the weight of the attention layer can achieve the best result, similar result also be found in~\cite{hu2021lora}, thus in the following content, we only consider how to find $\gamma$ for attention layers.}

% \todo{In order to use ZO to quickly find the preferred projection scalar for each layer, we introduce the prior knowledge from FO optimization. In detail, we conduct FO optimization to search the projection and find that there is a consistent pattern across datasets and LLMs. The result is shown in Figure~\ref{figure4}. As one can observe, the weight of the Value in the attention layer prefers a larger projection scalar, while the weight of the query, key, and attention output layer have projection scalars in a similar magnitude.}

% \todo{Drawing inspiration from FO searching of $\gamma$, we develop an enhanced ZO $\gamma$ searching method. 
% Specifically, we use $\sigma$ to control the initialization and perturbation distribution of $\gamma$, having $\gamma_v$ for the Value layers, and $\gamma_o$ for the other layers. Every time we update $\gamma$, we first initialize its value, making the init projection value to be $\sigma$. Then we conduct random searches around $\sigma$ with different perturbation distributions. For Value layers, they have larger $\sigma_v$, and thus larger initial value and are perturbed by distribution with larger variance. In contrast, the query, key, and output layers, are tented to be optimized to smaller values by the effect of smaller $\sigma_o$.}

% \begin{figure}[t]
%  \centering
%  \includegraphics[width=0.9\linewidth]{icml2025/texs/figures/distribution.pdf}
%  \caption{Distribution difference of projection magnitude of Query weight and Value weight in attention layer.}
%  \label{figure4}
% \end{figure}

% \begin{algorithm}[htb]
% \resizebox{0.9\linewidth}{!}{
% \begin{minipage}{\linewidth}
% \caption{Searching Projection $\gamma$ with ZO Optimization}
% \SetAlgoLined
% \KwData{$\theta_t$, $\theta_0$, $\gamma_t$, $\eta_{2}$, $\sigma_v$, $\sigma_o$, $T$, $V_n$ (set of indices)}
% \For{$l = 1, 2, \dots, L$}{
%     \uIf{$l \in V_l$}{
%         $\gamma_t^l = \|\theta_t^{l} - \theta_0^{l}\| \cdot \sigma_v$\;
%     }
%     \Else{
%         $\gamma_t^l = \|\theta_t^{l} - \theta_0^{l}\| \cdot \sigma_o$\;
%     }
% }

% \For{$t = 1, 2, \dots, T$}{
%     $\ell_1 = \mathcal{L}(\theta_t, \mathcal{B})$\;

%     $\texttt{PerturbGamma}(\theta_t, \theta_0, \gamma_t, \epsilon, \sigma_v, \sigma_o, s)$\;
    
%     $\texttt{ApplyProjection}(\theta_t, \theta_0, \gamma_t)$\;

%     $\ell_2 = \mathcal{L}(\theta_t, \mathcal{B})$\;

%     \If{$\ell_1 < \ell_2$}{
%         $\texttt{PerturbGamma}(\theta_t, \theta_0, \gamma_t, -2\epsilon, \sigma_v, \sigma_o, s)$\;
%     }

%     $\texttt{ReverseApplyProjection}(\theta_t, \theta_0, \gamma_t)$\;
% }


% \SetKwFunction{FComputeGradient}{PerturbGamma}
% \SetKwProg{Subroutine}{Subroutine}{:}{}
% \Subroutine{\FComputeGradient{$\theta_t, \theta_0, \gamma_t, \epsilon, \sigma_v, \sigma_o, s$}}{
%     \text{Reset random number generator with seed} $s$\;
    
%     \For{$l=1,2, \dots, L$}{
%         \uIf{$l \in V_l$}{
%         $u_l \backsim \mathcal{N}(0, \|\theta_t^{l} - \theta_{0}^{l}\|\sigma_v)$
%         }
%         \Else{
%         $u_l \backsim \mathcal{N}(0, \|\theta_t^{l} - \theta_{0}^{l}\|\sigma_o)$
%         }
%         $\gamma_t^{l} = \gamma_t^{l} + \eta_{2} u_l$
%     }
    
%     \Return $\gamma_t$\;
% \label{alg2}
% }
% \end{minipage}
% }
% \end{algorithm}








\section{Discussion and Overhead Analysis}
\label{overheadana}
We have some discussion on our method and analyze the computational overhead here and elaborate further later.

\textbf{Would adjusting the learning rate be equally effective?}
As discussed in Section~\ref{hypo}, our main objective is to provide ZO optimization with diverse-magnitude updates. 
A seemingly straightforward alternative is to assign different learning rates to each layer. However, in practice, this approach yields results that are similar to or even worse than vanilla ZO in terms of accuracy and GPU hours. 
We attribute this to the noisy gradient estimation of one single ZO step, which is likely to yield imprecise update directions. 
Therefore, using unrefined layer-wise learning rates can intensify this noise and further destabilize the optimization process. In contrast, DiZO enables the awareness of the pre-trained model during fine-tuning (see Eq.~\ref{equ:4}), robustifies the training process~\cite{dong2021should, oh2023towards, zhai2023investigating, wang2024pre}, and mitigates the noise introduced by ZO's random perturbations. More results and analysis are shown in Appendix~\ref{alternative}.
% the noisy gradient estimation of a single ZO update, 
% ZO’s random perturbationsRecent literature in LLM fine-tuning points out that pre-trained models awarded fine-tuning can
% \todo{写得没那么夸张}
% and a PyTorch-style code for a gradient-free searching process of projection is also illustrated in Appendix~\ref{code}.



\textbf{Memory utilization.} Our method requires additional memory as it involves storing the pre-trained model and calculating the weight distance gap with the fine-tuned model, which can become costly when scaling to large LLMs. However, in DiZO, we find that projecting only the weight updates of the \emph{Query} and \emph{Value} layers in the attention module, instead of updating all layers, not only reduces memory requirements but also delivers better performance. As a result, we only need to store the weights of these two types of layers from the pre-trained model, accounting for approximately 16.7\% of the parameters in OPT-2.7B, which is a manageable overhead. Similarly, LoRA~\cite{hu2021lora} also focuses on weight decomposition for \emph{Query} and \emph{Value} layers, which echoes our observation. Further analysis and results on projection layer selection are provided in Appendix~\ref{ablation_l}.

\textbf{Computational overhead.} 
Our method introduces extra computational cost, as the projection is learned alongside the main optimization (fine-tuning). However, we observe that performing projection learning intermittently, only once every few training iterations, does not compromise performance and significantly reduces the added overhead. This strategy reduces the computational burden while maintaining efficiency, allowing DiZO to achieve throughput comparable to vanilla ZO fine-tuning. 
Additionally, the reduced frequency of projection updates ensures that DiZO remains scalable for larger models and datasets. 
Please refer to Section~\ref{memory_speed} and Appendix~\ref{speed} for more details on computational overhead.

% \textbf{Computational overhead.} Our method increases computational cost, as the projection is learned alongside the main optimization (fine-tuning). However, we find that performing the projection learning intermittently does not affect performance yet significantly reduces the additional overhead, i.e., only performing projection searching once every few training iterations. The intermittent learning strategy enables DiZO to present no noticeable throughput difference compared to vanilla ZO fine-tuning. Please refer to Section~\ref{memory_speed} and Appendix~\ref{speed} for more details on computational overhead.

% \textbf{Limitations.} 

% \textbf{Initialization of $\gamma_t$.} The optimization for $\gamma_t$ is gradient-free, the initialization has a considerable effect on the performance of our method. We re-init the projection scalar every update according to the distance with the pre-trained model of its corresponding layer. Therefore, the initial projection magnitude $\frac{\gamma_t}{\|\theta_{t} - \theta_{0}\|}$ is $\sigma_v$ or $\sigma_o$, whichever is chosen depends on the characteristics of the layer. In a other word, we search the ideal $\gamma_t$ around $\sigma$. 
% % Please refer to Appendix \todo{idx} for initialization selection.

% \textbf{Stabilize Training.} As shown in (\ref{equ:4}), if at any step the projection is perturbed improperly, for example to a very small value, the model remains overly close to the pre-trained model. This invalidates a large number of previous updates and destabilizes training. To address this issue, we apply a clipping operation to keep the projection within a specified magnitude range. 

\section{Experiments}
\label{implementation}

\input{roberta-large}
\begin{figure*}[h]
 \centering
 % \hspace*{-0.3cm}
 \includegraphics[width=\linewidth]{loss_roberta.pdf}
 \caption{Trajectory of training loss curves when using MeZO and DiZO to fine-tune Roberta-large on SST-2, MNLI, and RTE.}
 \label{speed_roberta}
\end{figure*}

\subsection{Experimental Settings}

\textbf{Models and datasets.} We evaluate DiZO with various models, including medium-sized masked models~\cite{liu2019roberta} (RoBERTa-large) and large-sized autoregressive models~\cite{zhang2022opt, touvron2023llama} with different size, including OPT-2.7B, OPT-6.7B, Llama3-3B, and Llama3-8B. The total parameter size is ranging from 355M to 8B.  
Both classification and generation tasks are included. More details on datasets are shown in Appendix~\ref{dataset}. 
% For RoBERTa series, we evaluate with datasets: SST-2~\cite{socher2013recursive}, SST-5~\cite{socher2013recursive}, SNLI, TREC~\cite{voorhees2000building}, MNLI~\cite{yao2020pyhessian}, and RTE~\cite{dagan2005pascal,bar2006second,bentivogli2009fifth,giampiccolo2007third}. 
% For OPT series, we evaluate classification datasets, including SST2~\cite{clark2019boolq}, RTE~\cite{dagan2005pascal,bar2006second,bentivogli2009fifth,giampiccolo2007third}, CB, WIC~\cite{pilehvar2018wic}, WSC~\cite{levesque2012winograd}, and MultiRC, for generation task, we evaluate on SQuAD~\cite{rajpurkar2016squad} and DROP.

\textbf{Baseline.} We mainly compare with two ZO works, memory-efficient ZO optimization (MeZO)~\cite{malladi2023fine} and Hessian-informed ZO optimization (HiZOO)~\cite{zhao2024second}. MeZO is a fundamental and representative work in ZO LLM fine-tuning but suffers from slow convergence speed. HiZOO\footnote{We implement HiZOO ourselves, please refer to Appendix~\ref{baseline_imp} for details.} is a recently proposed ZO acceleration for LLM fine-tuning, which leverages the estimated second-order information to speed up. In addition, we also incorporate the parameter-efficient fine-tuning (PEFT) technique LoRA~\cite{hu2021lora}, applying it on top of FO fine-tuning, MeZO, and HiZOO.

\textbf{Evaluation.} 
For training and evaluation, we follow previous works~\cite{gao2020making, malladi2023fine}. We study few-shot and many-shot settings on RoBERTa-large, randomly sampling $k$ samples per class for training and validation, and 1000 samples for testing. For RoBERTa models, we evaluate $k=16$ and $k=512$. For OPT and LLaMA, we sample 1000, 500, and 1000 samples for training, validation, and testing. All experiments are conducted on NVIDIA A100 and A6000 GPUs.

\subsection{Medium-sized masked language models}



We conduct experiments on RoBERTa-large across three types of datasets and compare DiZO with two ZO baselines. We also explore PEFT by integrating LoRA. Table~\ref{roberta-main} presents the results, while Figure~\ref{speed_roberta} shows the trajectory of training loss curves, indicating the convergence speed of DiZO and MeZO. Our key findings are as follows:

\textbf{DiZO greatly increases the convergence speed over MeZO.} By using divergence-driven layer adaptation, the loss curve of DiZO decreases much faster, cutting the required iterations by over 50\% on SST-2, MNLI, and RTE. In addition, DiZO improves accuracy by 1.7\%, 3.6\%, and 8.5\% on these three datasets, respectively.

\textbf{DiZO outperforms MeZO and achieves results on par with full fine-tuning.} From Table~\ref{roberta-main}, DiZO consistently surpasses MeZO on all six datasets. Notably, on SST-2 and RTE datasets, DiZO even shows better performance than FO full-parameter fine-tuning, increasing by 0.3\% and 1.5\%, respectively.

\textbf{DiZO is effective for both full-parameter fine-tuning and PEFT.} Although DiZO applies projections based on the distance with the pre-trained model, while such prior knowledge does not exist for the decomposed weights of LoRA, it still delivers some gains. 
% Indicating that the model benefits from layer-wise adaptive updates regardless of whether the associated weights stem from pre-trained parameters.


\subsection{Large autoregressive language models}
\input{OPT2P7B}
\input{OPT6P7B}


To assess the broader applicability of DiZO, we run experiments on the OPT and Llama series autoregressive LLMs covering both classification and generation tasks. The overall results are summarized in Table~\ref{opt2p7b-main}, Table~\ref{opt6p7b-main}, and Figure~\ref{llama_bar} for OPT-2.7B, OPT-6.7B, and Llama series, respectively. We also compare the convergence speeds of DiZO and MeZO on OPT-2.7B across multiple datasets in Figure~\ref{speed_opt}. Below, we highlight the key observations from these experiments.

\textbf{DiZO dramatically reduces the training GPU hours compared with the representative baseline MeZO.} By incorporating divergence-driven layer adaptation, DiZO quickly establishes meaningful divergence across layers, whereas MeZO requires many more iterations to achieve the desired layer-wise divergence. As shown in Table~\ref{speed_opt}, DiZO converges with far fewer iterations across nine datasets, resulting in up to a 48\% reduction in training GPU hours. Moreover, unlike HiZOO, which reduces the number of iterations needed but slows the throughput of MeZO by more than 1.5× due to Hessian estimation, DiZO keeps its throughput nearly on par with MeZO. This efficiency is achieved because the additional projection learning procedure needs only two forward passes and is performed intermittently.
% e.g., HiZOO~\cite{zhao2024second}, which incur a 1.5× increase in wall clock time per iteration, due to the additional projection optimization only needs a forward pass and is performed intermittently, keeping the throughput of DiZO nearly on par with MeZO.

\textbf{DiZO outperforms baselines in both standard and parameter-efficient settings.} From Table~\ref{opt2p7b-main}, DiZO surpasses MeZO and HiZOO with or without the LoRA, achieving results comparable to FO methods. Across seven classification datasets, DiZO ranks first on five, and it also leads in both text generation tasks. Table~\ref{opt6p7b-main} shows that these advantages persist even when scaling up to OPT-6.7B. Moreover, as illustrated in Figure~\ref{llama_bar}, the fine-tuning process of Llama-series model also benefits from layer-wise adaptive updates.


\begin{figure*}[h]
 \centering
 \hspace*{-0.3cm}
 \scalebox{1}{
 \includegraphics[width=1\linewidth]{Acceleration_bar.pdf}
 }
 \vspace{-10pt}
 \caption{
 % Comparison convergence iteration, forward pass, and training GPU hours. The results are all presented in proportion format and saved training GPU hours are highlighted.
 Comparison of convergence iterations, forward pass, and training GPU hours between MeZO and DiZO across multiple datasets. Results are presented as proportions, with the percentage of saved GPU hours highlighted for each dataset.}
 \vspace{-5pt}
 \label{speed_opt}
\end{figure*}

% Refer to Section~\ref{memory_speed} for a detailed analysis of optimization speed.
% Benefit from the projection-based update after ZO steps, DiZO is able to quickly accumulate discriminative distances from the pre-train model for different layers while MeZO requires much more iterations for such differences between layers. As a result of discriminate updates over layers, as shown in Table~\ref{speed_opt}, DiZO widely requires much fewer iterations for convergence compared with DiZO over nine datasets and thus saves up to 48\% GPU hours. The extra optimization process of projection only requires a forward pass and is only performed once every certain iteration. The throughput of DiZO is not significantly lower than that of MeZO. In contrast, HiZOO needs to estimate Hessian information at every iteration, which results in about 1.5× wall clock time per iteration. More details on optimization speed comparison are shown in Section \ref{memory_speed} and Appendix \todo{idx}.



\subsection{Memory and Speed Analysis}
\label{memory_speed}

\input{resource_speed2P7BRTE}
In this section, we examine the memory utilization and convergence speed of DiZO in comparison with both ZO baselines and FO fine-tuning approaches (with and without LoRA). Table~\ref{memory_rte} presents the results of fine-tuning OPT-2.7B on the RTE dataset, more results are shown in Appendix~\ref{speed}.

From the memory perspective, DiZO maintains the advantage of avoiding backpropagation, getting rid of the storage of memory-intensive data, and reducing memory usage by about 90\% compared to FO fine-tuning. As explained in Section~\ref{overheadana}, the additional memory requirement of DiZO stems from storing a portion of the pre-trained weights, including the weight of the \emph{Query} and \emph{Value}, amounting to only 16.7\% of the total parameters. In contrast, HiZOO needs to store Hessian information for all layers, with memory usage proportional to the size of the parameters. 
% on smaller datasets, HiZOO can even surpass the memory usage of gradient-based LoRA tuning (see Table~\ref{memory_speed_sst2}).

From the perspective of convergence speed, DiZO greatly reduces the required iterations while maintaining throughput similar to MeZO, resulting in significantly fewer training GPU hours. In contrast, HiZOO does not achieve comparable iteration savings and lowers the throughput of MeZO by about 1.5× because it requires Hessian information estimation. As a result, it only shows a modest acceleration in training GPU hours, in some settings, such as HiZOO with LoRA on RTE, it even consumes more training GPU hours than MeZO with LoRA.

A notable byproduct of our method is using a FO approach (e.g., with the Adam optimizer) to learn the projections. While this version has memory consumption comparable to LoRA and requires additional training GPU hours, it offers distinct advantages. Since DiZO does not update projections at every iteration, FO-based DiZO exhibits significantly lower average memory usage than FO-based LoRA, with an average memory overhead close to that of the ZO-based DiZO. Although average memory usage may seem less critical in single-process, single-GPU setup, many real-world on-device training scenarios involve multi-process environments~\cite{li2024flexnn, ye2024asteroid}. In such cases, the FO-based DiZO can stagger memory usage phases across processes, enabling parallel operations that purely FO methods cannot achieve. Furthermore, compared with ZO-based DiZO, the FO version reduces extra training GPU hours and delivers better performance. 
% the FO-based DiZO not only maintains an average memory overhead similar to the ZO-based variant but also reduces extra training GPU hours and delivers better performance. 
These qualities make it particularly appealing for specific on-device training cases.




\section{Conclusion}

In this paper, we propose a novel layer-wise divergence analysis to reveal the distinct update pattern between FO and ZO methods. Building on these insights, we present DiZO, an enhanced ZO method using divergence-driven layer adaptation to resemble the learning capacity of the FO method. DiZO achieves significant training acceleration and superior performance across diverse tasks and architectures. Moreover, our method can be seamlessly integrated with
PEFT techniques like LoRA for additional speedup. For future work, we plan to explore DiZO in other fields, particularly for fine-tuning large pre-trained vision models.

\section{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

% In this paper, we conduct a novel layer-wise distance analysis to reveal the distinct training dynamics between FO and ZO methods. Building on these insights, we introduce PeZO, an enhanced ZO method incorporating tailored layer-wise updates by projection. PeZO outperforms vanilla ZO optimization across various tasks and model architectures. Moreover, PeZO greatly reduces the GPU hours which used to be a concern in vanilla ZO methods. 
% For future work, we wish to investigate the potential of PeZO in fine-tuning pre-trained models in other fields, particularly in vision.


% \input{example_paper.bbl}

% \bibliography{icml2025/example_paper}
\bibliographystyle{icml2025}
\bibliography{example_paper}

\newpage
\appendix
\onecolumn

\section{Related Work}

\subsection{Fine-tuning of Pre-trained Models}

% Fine-tuning a pre-trained model is a widely used training paradigm that enables pre-trained models containing rich information to adapt to downstream tasks quickly, greatly reduce the cost compared to train from scratch, and achieve even better results~\cite{ gururangan2020don,ouyang2022training}. Starting from the success of fine-tuning pre-trained NLP models like BERT~\cite{devlin2018bert}, RoBERTa~\cite{liu2019roberta}, and GPT~\cite{chen2022visualgpt}, such techniques also show advantages in fine-tuning pre-trained vision models like CLIP~\cite{radford2021learning}, SWAG~\cite{singh2022revisiting}, and AlBEF~\cite{li2022align}. 

Fine-tuning a pre-trained model offers a powerful way to reuse learned representations and reduce training costs compared to building models from scratch, often achieving superior performance~\cite{gururangan2020don,ouyang2022training}. Initially successful in NLP with models like BERT, RoBERTa, and GPT~\cite{devlin2018bert,liu2019roberta,chen2022visualgpt}, fine-tuning has also shown promise in vision tasks such as CLIP and SWAG~\cite{radford2021learning,singh2022revisiting}. Recent parameter-efficient fine-tuning (PEFT), including LoRA~\cite{hu2021lora}, and prefix tuning~\cite{li2021prefix}, further minimize resource needs by updating only a small subset of parameters, preserving most of the pre-trained weights and ensuring valuable knowledge is retained.
% Recently, some research shows that the distance between the pre-trained model and the fine-tuned model may affect the capacity of the fine-tuned model, the distance becomes an effective indicator for measuring the degree of forgetting and robustness of the fine-tuned model. \citet{dong2021should} leverage mutual information between the pre-trained and fine-tuned model to control the process of fine-tuning, while \citet{wang2024pre} replace the mutual information with KL divergence and add it as a training target.

\subsection{Zeroth-order Optimization and Acceleration}

ZO optimization emerges as an attractive technique that optimizes the model without backpropagation~\cite{ chen2023deepzero, chen2017zoo, ye2018hessian, verma2023certified, dhurandhar2018explanations, dhurandhar2019model}. Unlike most frequently used FO optimization which directly obtains and leverages the gradient for optimization, the zeroth-order method utilizes objective function value oracle only, estimating the gradient by finite differences.
ZO method has a wide range of applications in machine learning fields, including adversarial attack and defense~\cite{ chen2017zoo, ye2018hessian, verma2023certified}, machine learning explainability~\cite{dhurandhar2018explanations, dhurandhar2019model}, reinforcement learning~\cite{vemula2019contrasting}, and on-chip training~\cite{gu2021efficient}. 
Recently, the ZO method has been proposed to be leveraged on LLM fine-tuning to address the significant memory usage. \citet{malladi2023fine} proposed MeZO, first scaling ZO optimization to fine-tuning parameter-intensive LLMs, greatly reducing memory utilization. On top of MeZO, \citet{zhao2024second} proposed HiZOO, leveraging the estimated Hessian information for better learning capacity, but reducing the throughput of MeZO to some extent.


% ZO optimization, although gradient-free, converges much slower than FO methods due to higher variance from random search. 
% \citet{liu2018zeroth} introduced ZO-SVRG by incorporating variance reduction techniques~\cite{johnson2013accelerating}. \citet{shu2023zeroth} proposed using a Gaussian process to model objective function queries, thereby reducing query complexity and allowing more frequent queries to lower gradient variance. \citet{sener2020learning} performed random search on a learned low-dimensional manifold, reducing the number of needed objective queries. 
ZO optimization, although it significantly saves memory, converges more slowly than FO methods due to higher variance from random search. 
% Prior work has incorporated variance reduction~\cite{liu2018zeroth}, Gaussian processes~\cite{shu2023zeroth}, or dimensionality reduction~\cite{sener2020learning} to address the issue.
\citet{liu2018zeroth} introduced ZO-SVRG by incorporating variance reduction techniques~\cite{johnson2013accelerating}. \citet{shu2023zeroth} proposed using a Gaussian process to model objective function queries, thereby reducing query complexity and allowing more frequent queries to lower gradient variance. \citet{sener2020learning} performed random search on a learned low-dimensional manifold, reducing the number of needed objective queries. 
However, existing ZO accelerators face two main challenges when adapting to ZO fine-tuning for LLMs. First, these approaches were typically designed for smaller-scale tasks involving fewer parameters and less data, and cannot be directly extended to large-scale LLMs. Second, many prior methods focus on improving query efficiency, whereas recent work has shown that a single query can suffice for LLM fine-tuning~\cite{malladi2023fine}. How to effectively accelerate ZO optimization on large model fine-tuning remains a problem.



% Considering the model size of language models (LMs) increased exponentially, many recent works leverage ZO for fine-tuning to alleviate the effort in memory, as ZO only needs two forward processes to estimate the gradient, thus providing change to get free from storing the most memory-consuming information in backpropagation, i.e., weight gradient and intermediate activation. MeZO~\cite{malladi2023fine} introduced a ZO-SGD algorithm to fine-tune LLMs in a memory-efficient manner. \cite{liu2024sparse} introduced parameters-efficient fine-tuning (PEFT) on top of MeZO, which applies ZO only to a carefully chosen subset of parameters, resulting in obvious speed up without performance drop. \cite{tang2024private} proposed differentially private ZO, efficiently fine-tuning LLMs in a privacy-preserved manner. \cite{ling2024convergence} explored the integration of Memory-efficient Zeroth-Order Optimization within a federated setting, aiming to deploy on clients with limited memory.

\section{Experiment Settings and Analysis}
\label{imp}

\subsection{Datasets and Evaluation}
\label{dataset}

\input{parameter_setting}
For the RoBERTa-large model, we use the following classification datasets: SST-2~\cite{socher2013recursive}, SST-5~\cite{socher2013recursive}, SNLI~\cite{bowman2015large}, TREC~\cite{voorhees2000building}, MNLI~\cite{yao2020pyhessian}, and RTE~\cite{dagan2005pascal,bar2006second,bentivogli2009fifth,giampiccolo2007third}. Following previous studies, we cap the test set size at 1000 samples. Two training settings are considered: $k=16$ and $k=512$, where we randomly select 16 or 512 samples per class for both training and validation.

For the OPT and Llama series models, we use the SuperGLUE benchmark~\cite{wang2019superglue}, which includes RTE~\cite{dagan2005pascal,bar2006second,bentivogli2009fifth,giampiccolo2007third}, CB~\cite{de2019commitmentbank}, BoolQ~\cite{clark2019boolq}, WIC~\cite{pilehvar2018wic}, WSC~\cite{levesque2012winograd}, and MultiRC~\cite{khashabi2018looking}. We also include SST-2~\cite{socher2013recursive} and two question answering datasets, SQuAD~\cite{rajpurkar2016squad} and DROP~\cite{dua2019drop}. For each of these datasets, we randomly sample 1000 instances for training, 500 for validation, and 1000 for testing.


% For RoBERTa series, we evaluate with datasets: SST-2~\cite{socher2013recursive}, SST-5~\cite{socher2013recursive}, SNLI, TREC~\cite{voorhees2000building}, MNLI~\cite{yao2020pyhessian}, and RTE~\cite{dagan2005pascal,bar2006second,bentivogli2009fifth,giampiccolo2007third}. 
% For OPT series, we evaluate classification datasets, including SST2~\cite{clark2019boolq}, RTE~\cite{dagan2005pascal,bar2006second,bentivogli2009fifth,giampiccolo2007third}, CB, WIC~\cite{pilehvar2018wic}, WSC~\cite{levesque2012winograd}, and MultiRC, for generation task, we evaluate on SQuAD~\cite{rajpurkar2016squad} and DROP.

\subsection{Implementation of Baselines}
\label{baseline_imp}

% \todo{Add something related to HiZOO-L}

\textbf{Memory-efficient ZO (MeZO)} MeZO~\cite{malladi2023fine} serves as a fundamental baseline for fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization. By resampling perturbations with a fixed random seed, MeZO eliminates the need to store perturbations that are the same size as the model, thereby saving memory. For our implementation of MeZO, we adapted the code released by the authors at \url{https://github.com/princeton-nlp/MeZO} with minimal modifications.

\textbf{Hessian-informed ZO (HiZOO)} HiZOO~\cite{zhao2024second} is a recently proposed method for ZO fine-tuning of LLMs that leverages estimated second-order information to accelerate optimization. During the implementation of HiZOO, we identified several bugs in the released code at \url{https://anonymous.4open.science/r/HiZOO-27F8}, such as overflows when computing the Hessian. Consequently, we implemented the baseline ourselves. Additionally, we used the parameter settings from the original code instead of those described in the paper, as they resulted in better performance according to our implementation. 

\subsection{Hyperparameter Setting}

We use the hyperparameters in Table~\ref{setting} for experiments on RoBERTa-large, OPT-series, and Llama-series models. Specifically, the choice of clip range did not significantly impact the performance. The selection of the projection update cycle and scalar for projection affects the performance somewhat. Generally, for datasets that need larger iterations for convergence, or for these harder datasets, DiZO prefers a larger update cycle, while for those less complicated datasets, DiZO benefits from a smaller update cycle.




\section{Closer look at DiZO}
\label{closer_look}
\subsection{Ablation for Projection Layers Selection}
\label{ablation_l}

Instead of applying projections to all layers, which would require storing the entire pre-trained model, we focus only on projecting the weights of the \emph{Query} and \emph{Value} in the attention modules. As shown in Table~\ref{ablation_layer}, this strategy achieves the best trade-off between the overall performance and extra storage requirements, does not reduce the performance and only 16.7\% of the parameters of the pre-trained model are needed to store. A Similar strategy has also been adopted in LoRA~\cite{hu2021lora}.

\input{ablation_layers}

\subsection{Ablation for Strategies in ZO Projection Learning}
\label{ablation}


As discussed in Section~\ref{ZO}, we introduce two strategies, \emph{re-initialization} (Re-init) and \emph{projection clipping} (Clipping), to enhance projection learning and improve the stability of fine-tuning. The ablation results for these strategies, along with the corresponding loss curves, are shown in Figure~\ref{ablation_strategy}.

Overall (left in Figure~\ref{ablation_strategy}), omitting either Re-init or Clipping significantly diminishes the benefits of DiZO, with MeZO outperforming DiZO in these cases. Specifically, without Re-init, accuracy drops sharply, falling below MeZO. Similarly, without Clipping, while DiZO slightly outperforms MeZO on simpler datasets like SST-2, it suffers from severe model collapse on more challenging datasets, leading to a significant decline in accuracy.

From the loss curve trajectory (right in Figure~\ref{ablation_strategy}), without Re-init, DiZO loses its advantage in training acceleration, as the loss curve becomes noticeably slower to decrease. Without Clipping, the loss curve exhibits significant oscillations during certain training steps. This instability arises when projections are optimized to unsuitable values, such as extremely large or small magnitudes. These inappropriate projections cause substantial changes in model weights, leading to pronounced oscillations in the loss.

\input{ablation_strategy}


\subsection{Does Other Alternative Strategies for Layer-wise Divergence Work?}
\label{alternative}

As discussed in Section~\ref{hypo}, our objective is to enhance layer-wise divergence in ZO optimization. Naturally, with consideration of this objective, one may raise two questions regarding the projection strategy we adopt: 1) Can we perform layer-wise projections on the learning rate? 2) When updating weight by projection at $t$-th iteration, why do we use the weights of pre-trained model $\bm{\theta}_0$ as the base of the update (shown in Eq.\ref{equ:4}) instead of the weights from the $(t-1)$-th iteration, $\bm{\theta}_{t-1}$?
\input{ablation_projection}

To answer the above two questions. We investigate two alternative projection strategies: 1) searching layer-wise ideal learning rate via ZO optimization and then applying the updates, and 2) conducting projection update based on $\bm{\theta}_{t-1}$ instead of $\bm{\theta}_{0}$. Results are illustrated in Table~\ref{ablation_projection}, neither approach achieves performance comparable to DiZO; both yield results closer to MeZO in terms of accuracy and required GPU hours.

\input{resource_speed2P7BSST2}
\input{resource_speed2P7BSQuAD}

We attribute this phenomenon to the high noise inherent in each ZO update, which relies on random perturbations and thus produces a highly imprecise update direction. In contrast, DiZO projects the optimization direction between the pre-trained and fine-tuned models, and this direction is supposed to be correct. Otherwise, the entire optimization would fail and the loss would not decrease.  Moreover, recent studies suggest that the fine-tuned model is often less robust than their pre-trained version due to catastrophic forgetting~\cite{dong2021should, oh2023towards, zhai2023investigating, wang2024pre}. Maintaining a connection with the pre-trained model helps robustify the fine-tuning process and mitigate some of the noise introduced by ZO’s random perturbations.




% ~\\
% \newpage


% \subsection{PyTorch-style code for gradient-free searching of projections}
% \label{code}




\section{More Experiment Results}

\subsection{Memory and Speed Analysis}
\label{speed}
We present the memory and speed results for OPT-2.7B on the SST-2 and SQuAD datasets in Table~\ref{memory_speed_sst2} and Table~\ref{memory_squad}, respectively. DiZO significantly reduces the number of required iterations while maintaining throughput comparable to MeZO, leading to substantially fewer training GPU hours. In contrast, HiZOO achieves only modest iteration savings and further reduces the throughput of MeZO by approximately 1.5× due to its reliance on second-order information estimation. As a result, HiZOO offers only a slight improvement over MeZO in terms of training GPU hours. In some cases, such as HiZOO combined with LoRA on SQuAD, it even consumes more training GPU hours than MeZO with LoRA.





\subsection{Llama Experiments}
\label{Llama}

To demonstrate the broader applicability of DiZO, we conducted experiments on the Llama-series models. The results for Llama3-3B and Llama3-8B are presented in Table~\ref{Llama-3B} and Table~\ref{Llama-8B}, respectively. DiZO consistently outperforms MeZO across both the 3B and 8B Llama models.

However, we observed that ZO LoRA performs poorly with Llama models (including DiZO, MeZO and HiZOO). The loss value remains stagnant, and the resulting accuracy is comparable to or even worse than zero-shot results. We leave it to future work to investigate why ZO LoRA fails with Llama models. We suspect that this limitation may be related to the Group Query Attention (GQA)~\cite{ainslie2023gqa} mechanism employed in Llama3.
\input{llama}
\input{llama-8B}


\newpage
\section{Proof}
\label{proof}
We consider a neural network with $L$ layers (or parameter blocks) and wish to estimate the gradient of some loss function $\mathcal{L}(\boldsymbol{\theta}; \mathcal{B})$ with respect to all parameters $\boldsymbol{\theta}$. We use a two-point finite-difference (zero-order) method with directions drawn from an isotropic distribution. We show below why the \emph{expected} norm-squared of the resulting gradient estimator is \emph{identical} (or follows the same dimension-based law) for each layer/block.

Consider the $\ell$-th layer. Its estimator is
\[
   \widehat{\nabla_{\boldsymbol{\theta}^{(\ell)}} \mathcal{L}}
   \;=\;
   \frac{1}{q}\,\sum_{i=1}^q
   \Bigl[\,
   \underbrace{
   \frac{
   \mathcal{L}\!\bigl(\boldsymbol{\theta} + \epsilon \boldsymbol{u}_{i}\bigr)
   \;-\;
   \mathcal{L}\!\bigl(\boldsymbol{\theta} - \epsilon \boldsymbol{u}_{i}\bigr)
   }{2\,\epsilon}
   }_{\displaystyle \Delta_i}
   \Bigr]
   \,\boldsymbol{u}_i^{(\ell)},
\]
where $\Delta_i$ is the same scalar for \emph{all} blocks. We want
\[
   \mathbb{E}\Bigl[\|\widehat{\nabla_{\boldsymbol{\theta}^{(\ell)}} \mathcal{L}}\|^2\Bigr].
\]
Note that:
\begin{enumerate}
\item $\Delta_i$ does not depend on $\ell$; it is a single scalar for each direction $i$.
\item $\boldsymbol{u}_i^{(\ell)}$ is the sub-vector of $\boldsymbol{u}_i$ associated to the $\ell$-th block.
\item $\boldsymbol{u}_i$ is drawn from an \emph{isotropic} distribution in $\mathbb{R}^d$, meaning each coordinate has zero mean, unit variance, and there is no cross-correlation between different coordinates. Thus, different blocks $\boldsymbol{u}_i^{(\ell)}$ and $\boldsymbol{u}_i^{(m)}$ (for $\ell \neq m$) are uncorrelated, and each block $\boldsymbol{u}_i^{(\ell)}$ has an identity covariance in its own subspace $\mathbb{R}^{d_\ell}$.
\end{enumerate}

Hence, when we expand 
\[
\|\widehat{\nabla_{\boldsymbol{\theta}^{(\ell)}} \mathcal{L}}\|^2
\;=\;
\Bigl\|
\frac{1}{q}\,\sum_{i=1}^q \Delta_i\,\boldsymbol{u}_i^{(\ell)}
\Bigr\|^2,
\]
the expectation w.r.t.\ $\{\boldsymbol{u}_i\}$ depends on $\ell$ \emph{only} through the dimension $d_\ell$, not through any other distributional asymmetry. If $d_\ell$ are the same for all $\ell$, then the second moment is \emph{literally} the same across all blocks. If $d_\ell$ differ, the dependence is only a (known) function of $d_\ell$.

In short, \textbf{isotropy} ensures that
\[
   \mathbb{E}\!\bigl[\|\widehat{\nabla_{\boldsymbol{\theta}^{(\ell)}} \mathcal{L}}\|^2\bigr]
   \quad
   \text{is the same functional form of }
   \|\nabla_{\boldsymbol{\theta}^{(\ell)}}\mathcal{L}\|^2
   \text{ for each layer } \ell.
\]
Therefore, in the simplest scenario where $d_\ell$ are all the same, each layer gets the \emph{same} second-moment behavior for its gradient estimator.

~\\
\newpage
\section{Implementation}
\label{code}
The following is an implementation of our “ZO projection learning” in PyTorch.

\begin{lstlisting}
def ZO_Projection_Learning(theta_t, theta_0, Gammas, delta, eta, tau, x):
    """
    Perform Zeroth-order Projection Learning.

    Args:
        theta_t: Current model parameters to be fine-tuned.
        theta_0: Pre-trained model parameters (anchor).
        Gammas: Projection parameters need to be optimized.
        delta: Smoothing parameter.
        eta: Learning rate for projection gradient descent.
        tau: Clipping factor for projection bounds.
        x: Input data for the forward pass.
    """
    
    # Calculate the L2 norm of the distance gap
    norms = {
        name: torch.norm(param.data - anchor.data)
        for (name, param), anchor in zip(theta_t.named_parameters(), theta_0.parameters())
    }

    # Initialize the projection values
    for name, gamma in Gammas.named_parameters():
        gamma.data = norms[name]

    for i in range(max_iters):
        # Step 1: Perturb and apply projection, then compute loss
        Gammas = PerturbGamma(Gammas, delta)
        ApplyProjection(theta_t, pre_trained, Gammas)
        loss1 = Forward(theta_t, x)
        ReverseProjection(theta_t)  # Reset the parameter before projection

        # Step 2: Reverse and apply projection, then compute loss
        Gammas = PerturbGamma(Gammas, -2 * delta)
        ApplyProjection(theta_t, pre_trained, Gammas)
        loss2 = Forward(theta_t, x)
        ReverseProjection(theta_t)  # Reset the parameter before projection

        # Step 3: Reset projection and compute gradient
        Gammas = PerturbGamma(Gammas, delta)  # Reset projection
        grad = (loss1 - loss2) / (2 * delta)

        # Step 4: Gradient descent with clipping
        for name, gamma in Gammas.named_parameters():
            torch.manual_seed(seed)  # For resampling perturbation
            z = torch.normal(mean=0, std=1, size=gamma.data.size())
            gamma.data = torch.clip(
                gamma.data - eta * grad * z,
                (1 - tau) * norms[name],
                (1 + tau) * norms[name],
            )  # Conduct descent and apply clipping

    return Gammas

\end{lstlisting}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % APPENDIX
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

