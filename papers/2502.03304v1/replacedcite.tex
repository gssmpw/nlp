\section{Related Work}
\subsection{Fine-tuning of Pre-trained Models}

% Fine-tuning a pre-trained model is a widely used training paradigm that enables pre-trained models containing rich information to adapt to downstream tasks quickly, greatly reduce the cost compared to train from scratch, and achieve even better results____. Starting from the success of fine-tuning pre-trained NLP models like BERT____, RoBERTa____, and GPT____, such techniques also show advantages in fine-tuning pre-trained vision models like CLIP____, SWAG____, and AlBEF____. 

Fine-tuning a pre-trained model offers a powerful way to reuse learned representations and reduce training costs compared to building models from scratch, often achieving superior performance____. Initially successful in NLP with models like BERT, RoBERTa, and GPT____, fine-tuning has also shown promise in vision tasks such as CLIP and SWAG____. Recent parameter-efficient fine-tuning (PEFT), including LoRA____, and prefix tuning____, further minimize resource needs by updating only a small subset of parameters, preserving most of the pre-trained weights and ensuring valuable knowledge is retained.
% Recently, some research shows that the distance between the pre-trained model and the fine-tuned model may affect the capacity of the fine-tuned model, the distance becomes an effective indicator for measuring the degree of forgetting and robustness of the fine-tuned model. ____ leverage mutual information between the pre-trained and fine-tuned model to control the process of fine-tuning, while ____ replace the mutual information with KL divergence and add it as a training target.

\subsection{Zeroth-order Optimization and Acceleration}

ZO optimization emerges as an attractive technique that optimizes the model without backpropagation____. Unlike most frequently used FO optimization which directly obtains and leverages the gradient for optimization, the zeroth-order method utilizes objective function value oracle only, estimating the gradient by finite differences.
ZO method has a wide range of applications in machine learning fields, including adversarial attack and defense____, machine learning explainability____, reinforcement learning____, and on-chip training____. 
Recently, the ZO method has been proposed to be leveraged on LLM fine-tuning to address the significant memory usage. ____ proposed MeZO, first scaling ZO optimization to fine-tuning parameter-intensive LLMs, greatly reducing memory utilization. On top of MeZO, ____ proposed HiZOO, leveraging the estimated Hessian information for better learning capacity, but reducing the throughput of MeZO to some extent.


% ZO optimization, although gradient-free, converges much slower than FO methods due to higher variance from random search. 
% ____ introduced ZO-SVRG by incorporating variance reduction techniques____. ____ proposed using a Gaussian process to model objective function queries, thereby reducing query complexity and allowing more frequent queries to lower gradient variance. ____ performed random search on a learned low-dimensional manifold, reducing the number of needed objective queries. 
ZO optimization, although it significantly saves memory, converges more slowly than FO methods due to higher variance from random search. 
% Prior work has incorporated variance reduction____, Gaussian processes____, or dimensionality reduction____ to address the issue.
____ introduced ZO-SVRG by incorporating variance reduction techniques____. ____ proposed using a Gaussian process to model objective function queries, thereby reducing query complexity and allowing more frequent queries to lower gradient variance. ____ performed random search on a learned low-dimensional manifold, reducing the number of needed objective queries. 
However, existing ZO accelerators face two main challenges when adapting to ZO fine-tuning for LLMs. First, these approaches were typically designed for smaller-scale tasks involving fewer parameters and less data, and cannot be directly extended to large-scale LLMs. Second, many prior methods focus on improving query efficiency, whereas recent work has shown that a single query can suffice for LLM fine-tuning____. How to effectively accelerate ZO optimization on large model fine-tuning remains a problem.



% Considering the model size of language models (LMs) increased exponentially, many recent works leverage ZO for fine-tuning to alleviate the effort in memory, as ZO only needs two forward processes to estimate the gradient, thus providing change to get free from storing the most memory-consuming information in backpropagation, i.e., weight gradient and intermediate activation. MeZO____ introduced a ZO-SGD algorithm to fine-tune LLMs in a memory-efficient manner. ____ introduced parameters-efficient fine-tuning (PEFT) on top of MeZO, which applies ZO only to a carefully chosen subset of parameters, resulting in obvious speed up without performance drop. ____ proposed differentially private ZO, efficiently fine-tuning LLMs in a privacy-preserved manner. ____ explored the integration of Memory-efficient Zeroth-Order Optimization within a federated setting, aiming to deploy on clients with limited memory.