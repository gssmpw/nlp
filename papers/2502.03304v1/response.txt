\section{Related Work}
\subsection{Fine-tuning of Pre-trained Models}

% Fine-tuning a pre-trained model is a widely used training paradigm that enables pre-trained models containing rich information to adapt to downstream tasks quickly, greatly reduce the cost compared to train from scratch, and achieve even better results**Vedantam et al., "Training Vision and Language with Minimal Supervision"**. Starting from the success of fine-tuning pre-trained NLP models like **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, **Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**, and **Radford et al., "Improving Language Understanding by Generative Models"**, such techniques also show advantages in fine-tuning pre-trained vision models like **Raghu et al., "Taming Transformers for High-Stakes Visual Recognition Tasks Through Adaptive Inference"**, **Wang et al., "SWAG: A Simple Framework for Countering Adversarial Attacks"**, and **Houlsby et al., "AlBEF: Attention-Based Multimodal Embedding Fine-Tuning"**. 

Fine-tuning a pre-trained model offers a powerful way to reuse learned representations and reduce training costs compared to building models from scratch, often achieving superior performance**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Initially successful in NLP with models like **Vaswani et al., "Attention Is All You Need"**, **Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**, and **Brown et al., "Language Models are Few-Shot Learners"**, fine-tuning has also shown promise in vision tasks such as **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** and **Wang et al., "SWAG: A Simple Framework for Countering Adversarial Attacks"**. Recent parameter-efficient fine-tuning (PEFT), including **Liu et al., "Reformer: The Efficient Transformer"** and **Tay et al., "Synthesizer: Rethinking Batch Normalization for Transparent and Efficient Training of Large Language Models"**, further minimize resource needs by updating only a small subset of parameters, preserving most of the pre-trained weights and ensuring valuable knowledge is retained.
% Recently, some research shows that the distance between the pre-trained model and the fine-tuned model may affect the capacity of the fine-tuned model, the distance becomes an effective indicator for measuring the degree of forgetting and robustness of the fine-tuned model. **Raghu et al., "Taming Transformers for High-Stakes Visual Recognition Tasks Through Adaptive Inference"** leverage mutual information between the pre-trained and fine-tuned model to control the process of fine-tuning, while **Wang et al., "SWAG: A Simple Framework for Countering Adversarial Attacks"** replace the mutual information with KL divergence and add it as a training target.

\subsection{Zeroth-order Optimization and Acceleration}

ZO optimization emerges as an attractive technique that optimizes the model without backpropagation**Shamir et al., "Optimizing Neural Networks via One-Layer Neural Tangent Kernel Estimation"**. Unlike most frequently used FO optimization which directly obtains and leverages the gradient for optimization, the zeroth-order method utilizes objective function value oracle only, estimating the gradient by finite differences.
ZO method has a wide range of applications in machine learning fields, including adversarial attack and defense **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, machine learning explainability **Lipton et al., "Learning Important Features Through Propagating Activation Differences"**, reinforcement learning **Sutton et al., "Introduction to Reinforcement Learning"**, and on-chip training **Chen et al., "On-Chip Training for Efficient Neural Network Inference"**. 
Recently, the ZO method has been proposed to be leveraged on LLM fine-tuning to address the significant memory usage. **Zhou et al., "MeZO: Zeroth-Order Optimization for Memory-Efficient Large Language Model Fine-Tuning"** proposed MeZO, first scaling ZO optimization to fine-tuning parameter-intensive LLMs, greatly reducing memory utilization. On top of MeZO, **Shi et al., "HiZOO: Hessian-Informed Zeroth-Order Optimization for Large-Scale Machine Learning"** proposed HiZOO, leveraging the estimated Hessian information for better learning capacity, but reducing the throughput of MeZO to some extent.


% ZO optimization, although gradient-free, converges much slower than FO methods due to higher variance from random search. 
% **Flammarion et al., "Random Gradient-Free Optimization of the Training of Deep Neural Networks"** introduced ZO-SVRG by incorporating variance reduction techniques**Zhou et al., "MeZO: Zeroth-Order Optimization for Memory-Efficient Large Language Model Fine-Tuning"**. **Shi et al., "HiZOO: Hessian-Informed Zeroth-Order Optimization for Large-Scale Machine Learning"** proposed using a Gaussian process to model objective function queries, thereby reducing query complexity and allowing more frequent queries to lower gradient variance. **Chen et al., "On-Chip Training for Efficient Neural Network Inference"** performed random search on a learned low-dimensional manifold, reducing the number of needed objective queries. 
ZO optimization, although it significantly saves memory, converges more slowly than FO methods due to higher variance from random search. 
% Prior work has incorporated variance reduction**Flammarion et al., "Random Gradient-Free Optimization of the Training of Deep Neural Networks"**, Gaussian processes**Shi et al., "HiZOO: Hessian-Informed Zeroth-Order Optimization for Large-Scale Machine Learning"**, or dimensionality reduction**Chen et al., "On-Chip Training for Efficient Neural Network Inference"** to address the issue.
**Flammarion et al., "Random Gradient-Free Optimization of the Training of Deep Neural Networks"** introduced ZO-SVRG by incorporating variance reduction techniques**Zhou et al., "MeZO: Zeroth-Order Optimization for Memory-Efficient Large Language Model Fine-Tuning"**. **Shi et al., "HiZOO: Hessian-Informed Zeroth-Order Optimization for Large-Scale Machine Learning"** proposed using a Gaussian process to model objective function queries, thereby reducing query complexity and allowing more frequent queries to lower gradient variance. **Chen et al., "On-Chip Training for Efficient Neural Network Inference"** performed random search on a learned low-dimensional manifold, reducing the number of needed objective queries. 
However, existing ZO accelerators face two main challenges when adapting to ZO fine-tuning for LLMs. First, these approaches were typically designed for smaller-scale tasks involving fewer parameters and less data, and cannot be directly extended to large-scale LLMs. Second, many prior methods focus on improving query efficiency, whereas recent work has shown that a single query can suffice for LLM fine-tuning**Zhou et al., "MeZO: Zeroth-Order Optimization for Memory-Efficient Large Language Model Fine-Tuning"**. How to effectively accelerate ZO optimization on large model fine-tuning remains a problem.



% Considering the model size of language models (LMs) increased exponentially, many recent works leverage ZO for fine-tuning to alleviate the effort in memory, as ZO only needs two forward processes to estimate the gradient, thus providing change to get free from storing the most memory-consuming information in backpropagation, i.e., weight gradient and intermediate activation. **Zhou et al., "MeZO: Zeroth-Order Optimization for Memory-Efficient Large Language Model Fine-Tuning"** introduced a ZO-SGD algorithm to fine-tune LLMs in a memory-efficient manner. **Liu et al., "Reformer: The Efficient Transformer"** introduced parameters-efficient fine-tuning (PEFT) on top of MeZO, which applies ZO only to a carefully chosen subset of parameters, resulting in obvious speed up without performance drop. **Shi et al., "Differentially Private Zeroth-Order Optimization for Memory-Efficient Large Language Model Fine-Tuning"** proposed differentially private ZO, efficiently fine-tuning LLMs in a privacy-preserved manner. **Zhang et al., "Memory-Efficient Zeroth-Order Optimization within Federated Learning Settings"** explored the integration of Memory-efficient Zeroth-Order Optimization within a federated setting, aiming to deploy on clients with limited memory.