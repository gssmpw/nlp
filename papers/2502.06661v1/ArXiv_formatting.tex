\documentclass[11pt,oneside]{article}

\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{authblk}
% \makeatletter
% \renewcommand\AB@authnote[1]{\rlap{\textsuperscript{\normalfont#1}}}
% \renewcommand\Authsep{,~\,}
% \renewcommand\Authands{,~\,and }
% \makeatother
\renewcommand\Affilfont{\itshape\small}
\usepackage{setspace}
\usepackage[parfill]{parskip}
\usepackage{lscape}

\usepackage{color}
\usepackage[colorlinks,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=blue]{hyperref}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\setlength{\tabcolsep}{3pt}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{nameref}
\usepackage{comment}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{xspace}

\newcommand{\iloco}{iLOCO\xspace}
\newcommand{\loco}{LOCO\xspace}

\usepackage{amsthm}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\test}{\mathrm{test}}
\newcommand{\spt}{\mathrm{split}}
\newcommand{\MP}{\mathrm{MP}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\err}{\mathrm{Error}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\ind}{\mathbb{I}}
\newtheorem{assump}{Assumption}
\newtheorem{theorem}{Theorem}

\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

%Definitions=======================================



\DeclareMathOperator{\Error}{Error}

\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand\NoIndent[1]{
    \par\vbox{\parbox[t]{\linewidth}{#1}}
}


\usepackage[natbib=true,maxcitenames=2,maxbibnames=20]{biblatex}
%\AtEveryBibitem{%
%  \clearfield{series}%
%  \ifentrytype{book}{}{% Remove publisher and editor except for books
%    \clearlist{publisher}
%    \clearname{editor}
%  }
%  \clearfield{isbn}
%  \clearlist{location}
%  \clearlist{publisher}
%}
% Courtesy of https://tex.stackexchange.com/a/10686
%\renewbibmacro{in:}{\ifentrytype{article}{}{\printtext{\bibstring{in}\intitlepunct}}}
%\renewbibmacro{in:}{\ifentrytype{article}{}{\printtext{~in~}}}
\renewbibmacro{in:}{}

\addbibresource{reference.bib}

\title{iLOCO: Distribution-Free Inference for Feature Interactions}

\author[1]{Camille Olivia Little}
\author[2]{Lili Zheng}
\author[3]{Genevera I. Allen}
\affil[1]{Department of Electrical and Computer Engineering, Rice University}
\affil[2]{Department of Statistics, University of Illinois Urbana-Champaign}
\affil[3]{Department of Statistics, Columbia University}



\begin{document}

\maketitle

\begin{abstract}
Feature importance measures are widely studied and are essential for understanding model behavior, guiding feature selection, and enhancing interpretability. However, many machine learning fitted models involve complex, higher-order interactions between features.  Existing feature importance metrics fail to capture these higher-order effects while existing interaction metrics often suffer from limited applicability or excessive computation; no methods exist to conduct statistical inference for feature interactions.  To bridge this gap, we first propose a new model-agnostic metric, interaction Leave-One-Covariate-Out (\iloco), for measuring the importance of higher-order feature interactions.  Next, we leverage recent advances in \loco inference to develop distribution-free and assumption-light confidence intervals for our \iloco metric. To address computational challenges, we also introduce an ensemble learning method for calculating the iLOCO metric and confidence intervals that we show is both computationally and statistically efficient.  We validate our \iloco metric and our confidence intervals on both synthetic and real data sets, showing that our approach outperforms existing methods and provides the first inferential approach to detecting feature interactions.


Keywords: Interactions, feature importance, interpretability, minipatch ensembles, LOCO inference, distribution-free inference

\end{abstract}

\clearpage
\onehalfspace

\begin{refsection}

%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
Machine learning systems are increasingly deployed in critical applications, necessitating human-understandable insights. Consequently, interpretable machine learning has become a rapidly expanding field \citep{murdoch:2019, du:2019}. For predictive tasks, one of the most important aspects of interpretation is assessing feature importance. While much focus has been placed on quantifying the effects of individual features, the real power of machine learning lies in its ability to model higher-order interactions and complex feature representations \citep{gromping:2009}. However, there is a notable gap in interpretability: few methods provide insights into the higher-order interactions that truly drive model performance. 

Understanding how features interact to influence predictions is a cornerstone of interpretable machine learning, particularly in applications requiring explainability. For instance, in genomics, interactions between genes can yield predictive insights into disease outcomes that individual genetic markers cannot. In drug discovery, uncovering how chemical properties interact can guide the identification of effective compounds in large-scale screening processes \cite{sachdev:2019}. In material science, feature interactions reveal relationships that enable the development of innovative materials with desired properties. In business, analyzing interactions between customer demographics and purchasing behaviors can inform personalized marketing strategies and enhance customer retention \citep{apel:2013}.

In addition to detecting feature interactions, it is crucial to quantify the uncertainty associated with these interactions to ensure they are not simply noise. Uncertainty quantification has emerged as a focus in machine learning, addressing the need for more reliable and interpretable models \citep{abdar:2021}. While much of this work has traditionally centered on quantifying uncertainty in predictions \citep{ribeiro:2016}, there has been a growing interest in extending these techniques to feature importance \citep{zhang:2022, williamson:2023}. To the best of our knowledge, there has been no uncertainty quantification or statistical inference studied for feature interaction importance.  

Motivated by these challenges, we introduce the interaction Leave-One-Covariate-Out (\iloco) metric and inference framework, which addresses key limitations of existing methods. Our framework provides a model-agnostic, distribution-free, statistically and computationally efficient solution for quantifying feature interactions and their uncertainties across diverse applications.


\subsection{Related Works}
There is an extensive body of literature on feature importance metrics \citep{samek:2021, altmann:2010, lipovetsky:2001, murdoch:2019}, but research on feature interactions is comparatively sparse. Shapley interaction values extend the game-theoretic concept of Shapley values to measure pairwise feature interactions, decomposing model predictions into main and interaction effects \citep{Tsai:2023, Sundararajan:2020, Rabitti:2019}. Similarly, the H-statistic, derived from partial dependence, quantifies the proportion of variance explained by feature interactions \citep{Friedman:2008}. While these methods are theoretically well-grounded and model-agnostic, their computational complexity increases exponentially with the number of features, limiting their practical applicability.


Some approaches have been developed to detect and quantify interactions within specific model classes \citep{Basu:2018, Hornung:2022}, while other approaches focus on detecting the presence of interactions without providing a formal quantitative measure \citep{Inglis2022Visualizing}. While these approaches can be effective, their insights may require manual inspection and interpretation, or may not generalize to other types of models. 

There is a growing body of research on uncertainty quantification for feature importance \citep{lei:2014, lei:2018, williamson:2020, williamson:2023, napolitano:2023}. However, to the best of our knowledge, no one has developed a technique for uncertainty quantification, specifically for feature interactions. Our goal is to develop confidence intervals for our feature interaction metric that reflect the statistical significance and uncertainty of this metric. Our method builds upon the Leave-One-Covariate-Out (\loco) metric and inference framework originally proposed by \cite{lei:2014}. 
Further, as computational considerations are a major challenge for interactions, we leverage the fast inference approach incorporating minipatches, simultaneous subsampling of observations and features, to enhance the computational efficiency of our approach.

\subsection{Contributions}
Our work focuses on two main contributions. First, we introduce \iloco, a distribution-free, model-agnostic method for quantifying feature interactions that can be applied to any predictive model without relying on assumptions about the underlying data distribution. Moreover, we introduce an efficient way of estimating this metric via minipatch ensembles. Second, we develop rigorous distribution-free inference procedures for interaction effects, enabling confidence intervals for the interactions detected. Collectively, our contributions offer notable advancements in quantifying feature interaction importance and even higher-order interactions in interpretable machine learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{iLOCO}
\subsection{Review: \loco Metric}
To begin, let us first review the widely used \loco metric that is used  for quantifying individual feature importance in predictive models \citep{lei:2015}. For a feature \( j \), \loco measures its importance by evaluating the change in prediction error when the feature is excluded. Formally, given a prediction function \( f: \mathcal{X} \to \mathbb{R} \) and an error measure \( \text{Error}() \), the \loco importance of feature \( j \) is:

\begin{align}
\label{eq:loco}
\Delta_j (\mathbf{X}, \mathbf{Y}) = \mathbb{E} \big[ 
    \text{Error}(Y, f^{-j}(X_{\setminus j}; \mathbf{X}_{:, \setminus j}, \mathbf{Y})) \nonumber - 
    \text{Error}(Y, f(X; \mathbf{X}, \mathbf{Y})) 
    \mid \mathbf{X}, \mathbf{Y} 
\big],
\end{align}

where \( f^{-j} \) is the prediction function trained without feature \( j \). A large $\Delta_{j}$ value, which must be non-negative by definition, suggests the importance of the feature by indicating performance degradation when the feature is excluded. While \loco is effective at measuring the importance of individual features, it has critical limitations in capturing feature interactions. If two features contribute to the prediction only when considered together, \loco may fail to identify their joint importance, as the removal of a single feature might have minimal impact on the model's performance. To address this challenge, the interaction \loco (\iloco) metric extends \loco to explicitly account for pairwise feature interactions.


\subsection{\iloco Metric}
Inspired by \loco, we seek to define a score that measures the influence of the interaction of between two variables $j$ and $k$. Consider the expected difference in error between a further reduced model $f^{-j, k}$  and the full model: 
\begin{align}
    \Delta_{j,k} (\mathbf{X}, \mathbf{Y})
    &= \mathbb{E} [
        \Error(Y,f^{-j,k}(X_{\setminus \{j, k\}}; \mathbf{X}_{:,\setminus \{j, k\}}, \mathbf{Y})  -\Error(Y,f (X; \mathbf{X}, \mathbf{Y})
        \mid \mathbf{X}, \mathbf{Y})]
\end{align}
The reduced model $f^{-j, k}$ removes covariates $j$ and $k$, their pairwise interaction, and any group involving $j$ or $k$. The quantity $\Delta_{j,k}$ captures the predictive power contributed by $j,k$ and all interactions that include them. This naturally leads to our \iloco metric:

\begin{definition}
For two features $j,k$, the  \iloco metric  is defined as:
\begin{align}
\text{\iloco}_{j,k} = \Delta_j + \Delta_k - \Delta_{j,k}.
\label{eq:iloco}
\end{align}
\end{definition}
In particular, \iloco finds the difference between the total influence of both $j$ and $k$ via $\Delta_j + \Delta_k$ and the influence specifically from $j$ and $k$ together, that is, $\Delta_{j,k}$.  Our metric takes a similar form to that of the $H$-statistic  \citep{Friedman:2008}. A strong positive \iloco metric indicates an important pairwise interaction.


To better understand and justify our proposed \iloco metric, let us assume that the data generating model follows a functional ANOVA decomposition, a common assumption to quantify feature importance in interpretable machine learning \citep{hooker:2007,lengerich:2020}.  Specifically, suppose $f^{*}()$ can be decomposed into a sum of orthogonal functions with mean zero: 
\begin{align*}
f^*(X) &=
 g_0 + \sum_{j=1}^p g_j(X_j) + \sum_{1 \leq j < k \leq p} g_{j,k}(X_j, X_k)  + \sum_{1 \leq j \leq k \leq l} g_{j,k,l}(X_j, X_k, X_l) + \cdots.
\end{align*}
which allows us to isolate the influence of each subset of variables.  Suppose we define $\Delta^*_j$, $\Delta^*_k$, and $\Delta^*_{j,k}$ and the resulting $\iloco^*_{j,k}$ based on this population model, $f^{*}()$.  Then, in this population case and for a common classification error function, our metric directly isolates the contribution of the pairwise interaction term from the functional ANOVA decomposition: 
\begin{prop}
Suppose $\text{Error}(Y,\hat{Y}) = 1 - \hat{Y}$, then  
\begin{align*}
\iloco^*_{j,k} = g_{j,k}.
\end{align*}
\end{prop}
For other common error metrics like MSE, we show that our population \iloco metric is proportional to $g_{j,k}$ in the functional ANOVA model; further details and proofs are given in the appendix.  This illustrative population analysis shows that \iloco directly captures the contributions of pairwise interactions.


% In practice, we likely cannot compute the expectations in $\Delta_j$, $\Delta_k$, and $\Delta_{j,k}$, so instead we compute an empirical version of \iloco. Using the decomposition, we define $\Delta^*_{k}$ as follows:  
% \begin{align*}
% \Error(Y, f^*_{\setminus k}) = &\\
%  \Error\Big(Y, f^* - g_k(X_k) - \sum_{j \neq k} g_{j,k}(X_j, X_k)\Big).
% \end{align*}
% The quantities $\Delta^*_{j}$ and $\Delta^*_{j,k}$ are defined analogously. In the population case, $\iloco^*$ is given by:  
% \begin{align*}
% \iloco^*_{j,k} = \Delta^*_{j} + \Delta^*_{k} + \Delta^*_{j,k}.
% \end{align*}
% This allows us to explicitly connect \iloco to the pairwise interaction term in the functional ANOVA decomposition.  

% \begin{prop}
% Given any two features $j$ and $k$, using classification error $1 - \hat{Y}$:  
% \begin{align*}
% \iloco^*_{j,k} = g_{j,k}.
% \end{align*}
% \end{prop}
% Thus, in the population case, \iloco isolates the contribution of the pairwise interaction between features $j$ and $k$ when classification accuracy is used.

\begin{figure*}[t!]
    \centering
    \includegraphics[width =0.95\textwidth]{figures/nonlin_class_comp.pdf}
    \caption{Success probability of detecting feature pair $(1,2)$ across SNR levels for KRBF, RF, and MLP classifiers on nonlinear classification simulations (i), (ii), and (iii); the first feature pair has signal in scenarios (i) and (ii) whereas it is null in scenario (iii).  Success probability is defined as the proportion of times the $(1,2)$ feature pair obtains the largest metric among all feature pairs. \iloco-MP and \iloco-Split consistently outperform existing approaches at detecting feature pair $(1,2)$ in the first two scenarios whereas \iloco-MP exhibits superior specificity in the null scenario (iii).}
    \label{fig:comp_rankings}
\end{figure*}

\subsection{\iloco Estimation}\label{sec:est}

To estimate our \iloco metric in practice, we need access to new data samples to evaluate the expectation of the differences in prediction error.  To this end, we introduce two estimation strategies, \iloco via data-splitting and \iloco via minipatch ensembles; the latter is specifically tailored to address the major computational burden that arises from fitting models leaving out all possible interactions.  

\textbf{iLOCO via Data Splitting}\\
The data-splitting approach estimates \(\text{iLOCO}\) by partitioning the dataset \((\mathbf{X}, \mathbf{Y})\) into a training set \(D_1 = (\mathbf{X}^{(1)}, \mathbf{Y}^{(1)})\) and a test set \(D_2 = (\mathbf{X}^{(2)}, \mathbf{Y}^{(2)})\). We train the full model $\hat{f}$ along with the models excluding $j$, $k$, and $j,k$ on training dataset $D_1$. The error functions for the full model and the corresponding feature-excluded error functions are evaluated on the test set $D_2$. This approach effectively computes \iloco, but it comes with certain challenges. Since it involves training multiple models and only utilizes a subset of the dataset for each, there may be concerns about computational efficiency and the stability of the resulting metric. 

%The data-splitting approach estimates \(\text{iLOCO}\) by partitioning the dataset \((\mathbf{X}, \mathbf{Y})\) into a training set \(D_1 = (\mathbf{X}^{(1)}, \mathbf{Y}^{(1)})\) and a test set \(D_2 = (\mathbf{X}^{(2)}, \mathbf{Y}^{(2)})\). Using \(D_1\), we train several models. The full model is $\hat{f}(X) = H(\mathbf{X}^{(1)}, \mathbf{Y}^{(1)})(X),$ which uses all features. A feature-excluded model is defined, for example, for feature \(j\) as $\hat{f}^{-j}(X)=H(\mathbf{X}^{(1)}_{:,\setminus j}, \mathbf{Y}^{(1)})(X_{\setminus j}).$ The definitions for excluding feature \(k\) or both \(j\) and \(k\) are analogous. Each model is then evaluated on the test set \(D_2\), and the prediction error for the full model is computed as $\Error(\mathbf{Y}^{(2)}, \hat{f}(X^{(2)})).$ Similarly, the errors for the feature-excluded models are defined analogously. These errors are then combined to estimate \iloco as defined in Equation~\ref{eq:iloco}. While this approach accurately computes \iloco, it requires training multiple models and uses only a portion of the data for each model, which may affect the computational efficiency and stability of the metric. \textcolor{blue}{Add one more sentence about tradeoff between computation and quality of metric}



\textbf{\iloco via Minipatches}\\
To address the computational limitations of data splitting, we introduce \iloco-MP (minipatches), an efficient estimation method that leverages ensembles of subsampled observations and features. This method repeatedly constructs minipatches by randomly sampling \(n\) observations and \(m\) features from the full dataset \citep{yao:2020}. For each minipatch, a model is trained on the reduced dataset, and we can evaluate predictions on left-out observations.  Specifically, for features \(j\) and \(k\), the leave-one-out and leave-two-covariates-out prediction is defined as $\hat{f}^{-j,k}_{-i}(X_i) = 
\frac{1}{\sum_{b=1}^{B} \mathbb{I}(i \notin I_b) \mathbb{I}(j, k \notin F_b)} \sum_{b=1}^{B} \mathbb{I}(i \notin I_b) \mathbb{I}(j, k \notin F_b) \hat{f}_b(X_i),$ where \(I_b \subset [N]\) is the set of observations selected for the \(b\)-th minipatch, \(F_b \subset [M]\) is the set of features selected, \(\hat{f}_b\) is the model trained on the minipatch, and \(\mathbb{I}(\cdot)\) is an indicator function that checks whether an index is excluded. This formulation ensures that the predictions exclude both the observation \(i\) and the features \(j\) and \(k\). We can define the other leave-one-out predictions accordingly to obtain a computationally efficient and stable approximation of the \iloco metric. The full \iloco-MP algorithm is given in the appendix.  Note that the \iloco-MP metric can only be computed for models in the form of minipatch ensembles built using any base model.  Further, note that after fitting minipatch ensembles, computing the \iloco metric requires no further model fitting and is nearly free computationally.  



\subsection{Extension to Higher-Order Interactions}
Detecting higher-order feature interactions is crucial in understanding complex relationships in data, as many real-world phenomena involve intricate dependencies among multiple features that cannot be captured by pairwise interactions alone. To extend the \iloco metric to account for such higher-order interactions, we generalize its definition to isolate the unique contributions of \(S\)-way interactions among features:
\begin{definition}
For an \(S\)-way interaction, the \iloco metric is defined as:
\begin{align*}
\text{\iloco}_S = \sum_{T \subseteq S} (-1)^{|S| - |T|} \Delta_T.
\end{align*}
\end{definition}
Here, we sum over all possible subsets of \(S\), and for each subset \(T\), \(\Delta_T\) denotes the contribution of \(T\) to the model error.  The term \((-1)^{|S| - |T|}\) alternates the sign based on the size of \(T\), ensuring proper aggregation and cancellation of irrelevant terms that appear multiple times; further derivations and justification are given in the appendix. 
By leveraging this generalized formulation, \iloco extends its ability to capture and quantify the contributions of higher-order interactions, providing a more complete understanding of complex feature relationships. 

\subsection{iLOCO for Correlated, Important Features}
Quantifying the importance of correlated features is a known challenge and unsolved problem in interpretable machine learning \citep{verdinelli:2024}. This problem is especially pronounced for \loco when correlated features are removed individually, the remaining correlated feature(s) can partially compensate, resulting in a small \loco metric that can miss important, correlated features.  Interestingly, our proposed \iloco metric can solve this problem.  Note that for a strongly correlated and important feature pair $j$ and $k$, $\Delta_j$ and $\Delta_k$ (and \loco) will both be near zero as they compensate for each other.  But, $\Delta_{j,k}$ will have a strong positive effect, and thus our \iloco metric will be strongly negative for important, correlated feature pairs.  Thus, our \iloco metric can serve dual purposes: positive values indicate an important pairwise interaction whereas negative values indicate individually important but correlated feature pairs.  Thus, this alternative application of \iloco helps solve an important open problem in feature importance.  


%By joint removal through $\Delta_{j,k}$, \iloco can  By subtracting this joint contribution from the sum of their individual effects, \iloco isolates the unique interaction between features $j$ and $k$, enabling it to highlight the shared and redundant importance of correlated features.  This makes \iloco particularly effective for capturing feature importance in cases where features are both correlated and important.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Distribution-Free Inference for iLOCO}

Beyond just detecting interactions via our \iloco metric, it is critical to quantify the uncertainty in these estimates to understand if they are significantly different than random noise.  To address this, we develop distribution-free confidence intervals for both our \iloco-Split and \iloco-MP estimators that are valid under only mild assumptions.  

\subsection{iLOCO Inference via Data Splitting}
As previously outlined, we can estimate the iLOCO metric via data splitting, where the training set $D_1$ is used for training predictive models $\hat{f},\,\hat{f}^{-j},\,\hat{f}^{-k},\,\hat{f}^{-j,k}$, while the test set $D_2$ with $N_2$ samples is used for evaluating the error functions for each trained model. Each test sample gives an unbiased estimate of the target iLOCO metric, $\iloco^{\spt}_{j,k}$, which is defined for the models trained on $(\bX^{(1)},\,\bY^{(1)})$. Specifically, $\iloco^{\spt}_{j,k} = \Delta_j^{\spt}+\Delta_{k}^{\spt}-\Delta_{j,k}^{\spt}$, where $\Delta_{j,k}^{\spt} = \bbE[\err(Y,\hat{f}^{-j,k}(X)) - \err(Y,\hat{f}(X))|\bX^{(1)},\bY^{(1)}],$ and $\Delta_j^{\spt}$, $\Delta_{k}^{\spt}$ are defined similarly. Here, the expectation is taken over the unseen test data, conditioning on $D_1$, which is a slightly different inference target than defined in \eqref{eq:iloco} which conditions on the model trained on all available data.  To perform statistical inference for $\iloco^{\spt}_{j,k}$, one natural idea is to collect its estimates on all test samples $\{\widehat{\iloco}_{j,k}(X_i^{(2)},Y_i^{(2)})\}_{i=1}^{N_2}$ and construct a confidence interval $\hat{\mathbb{C}}^{\spt}_{j,k}$ based on a normal approximation. The detailed inference algorithm for iLOCO-Split is included in the appendix. First, we need the following assumption:
\begin{assump}\label{assump:split_thirdmoment}
    Assume a bounded third moment: $\bbE(\widehat{\iloco}_{j,k}(X_i^{(2)},Y_i^{(2)})-\iloco^{\spt}_{j,k})^3/\sigma_{j,k}^3\leq C$, where $\sigma_{j,k}^2=\mathrm{Var}(\widehat{\iloco}_{j,k}(X_i^{(2)},Y_i^{(2)})|\bX^{(1)},\bY^{(1)})$.
\end{assump}
Then, the following theorem guarantees valid asymptotic coverage of $\hat{\mathbb{C}}^{\spt}_{j,k}$ for inference target $\iloco^{\spt}_{j,k}$.
\begin{theorem}[Coverage of iLOCO-Split]
     Suppose Assumption~\ref{assump:split_thirdmoment} holds and $\{(X_i,Y_i)\}_{i=1}^N$ are i.i.d., then we have $\lim_{N^{\test}\rightarrow\infty}\bbP(\iloco^{\spt}_{j,k}\in \mathbb{C}^{\spt}_{j,k})= 1-\alpha$.
     \label{thm:coverage_Split}
\end{theorem}
%The detailed assumption can also be found in the Supplementary material. 




\subsection{iLOCO Inference via Minipatches}
Recall that after fitting minipatch ensembles, estimating the \iloco metric is computationally free, as estimates aggregate over left out observations and/or features. Such advantages also carry over to statistical inference. In particular, we aim to perform inference for the target \iloco score defined in \eqref{eq:iloco}; note that this inference target is conditioned on all data instead of a random data split, $D_1$ as with \iloco-Split.  Then, for each sample $i$, we can compute the leave-one-observation-out predictors $\hat{f}_{-i}$, $\hat{f}_{-i}^{-j}$, $\hat{f}_{-i}^{-k}$, $\hat{f}_{-i}^{-j,k}$ simply by aggregating appropriate minipatch predictors, and evaluating these predictors on sample $i$ to obtain $\widehat{\iloco}_{j,k}(X_i,Y_i)$. For statistical inference, we collect $\{\widehat{\iloco}_{j,k}(X,Y)\}_{i=1}^N$ and construct a confidence interval $\mathbb{C}^{\MP}_{j,k}$ based on a normal approximation. The detailed inference procedure is given in the appendix. 


Despite the fact that $\widehat{\iloco}_{j,k}(X_i,Y_i)$ has a complex dependency structure since all the data is essentially used for both fitting and inference, we show asymptotically valid coverage of $\mathbb{C}^{\MP}_{j,k}$ under some mild assumptions.  First, let $h_{j,k}(X,Y) = \bbE_{\bX,\bY}[\mathrm{Err}(Y,\hat{f}^{-j}(X_{-j}))+\mathrm{Err}(Y,\hat{f}^{-k}(X_{-k}))-\mathrm{Err}(Y,\hat{f}^{-j,-k}(X_{-j,-k})) - \mathrm{Err}(Y,\hat{f}(X))|X,Y]$ be the interaction importance score of feature pair $(j,k)$ evaluated at sample $(X,Y)$, with expectation taken over the training data $(\bX,\bY)$ that gives rise to the predictive models. %when using the model trained on $(\bX,\bY)$ to predict data $(X,Y)$ 
Also let $\sigma_{j,k}^{\MP} = \mathrm{Var}(h_{j,k}(X_i,Y_i))$. 
% First, let $\sigma_{j,k}^{\MP} = \sqrt{\mathrm{Var}(\widetilde{\iloco}_{j,k}(X,Y))}$
% Let $\widetilde{\iloco}_{j,k}(X,Y)$ denote the iLOCO score evaluated on test sample $(X,Y)$, taking expectation over the training data $(\bX,\bY)$ ({\color{red}LZ comment: exact form of $\widetilde{\iloco}_{j,k}(X,Y)$ is included in the supplement}). Also define $\sigma_{j,k}^{\MP} = \sqrt{\mathrm{Var}(\widetilde{\iloco}_{j,k}(X,Y))}$. We require the following moment assumption on $\widetilde{\iloco}_{j,k}(X,Y)$:
\begin{assump}\label{assump:thirdmoment}  Assume a bounded third moment:
$\bbE[h_{j,k}(X,\,Y) - \bbE h_{j,k}(X,\,Y)]^3/(\sigma^{\MP}_{j,k})^3 \leq C$.
%    The normalized interaction importance r.v. satisfies the third moment condition: $\bbE[\widetilde{\iloco}_{j,k}(X,Y) - \bbE \widetilde{\iloco}_{j,k}(X,Y)]^3/(\sigma_{j,k}^{\MP})^3 \leq C$.
\end{assump}
%{\color{red}LZ comment: major difference btw current theory and LOCO-MP theory: we impose assumptions (moment condition in Assump. 2, variance condition in Assump. 5) on the iloco score function $\widetilde{\iloco}_{j,k}(X,Y)$ instead of loco score. Assumption \ref{assump:thirdmoment} is used to imply the uniform integrability assumption in the LOCO-MP paper; we change it to a moment condition to make it more interpretable. Maybe some assumptions that are exactly the same as LOCO-MP can be moved to the appendix?}
\begin{assump}\label{assump:err_lipschitz}
    The error function is Lipschitz continuous w.r.t. the prediction: for any $Y\in \bbR$ and any predictions $\hat{Y}_1,\,\hat{Y}_2\in \bbR^d$,
    $|\err(Y,\hat{Y}_1)-\err(Y,\hat{Y}_2)|\leq L\|\hat{Y}_1-\hat{Y}_2\|_2$.
\end{assump}
Common error functions like $\text{Error}(Y,\hat{Y}) = 1 - \hat{Y}$ for classification or mean absolute error for regression both trivially satisfy this assumption.
%{\color{red}LZ comment: I rewrote the following assumption since we never defined $\hat{\mu}_{I,F}$ throughout the paper.}
\begin{assump}\label{assump:bnd_mu}
    The prediction difference between the predictors trained on different minipatches are bounded by $D$ at any input value $X$. %$\|\hat{\mu}_{I,F}(X)-\hat{\mu}_{I',F'}(X)\|_2\leq D$ for any $X$, any minipatches $I,\,I'\subset[N]$, $F,\,F'\subset[M]$.
\end{assump}
\begin{assump}\label{assump:mpsize}
    The minipatch sizes $(m,\,n)$ satisfy $\frac{m}{M},\,\frac{n}{N}\leq \gamma$ for some constant $0<\gamma<1$, and $n=o(\frac{\sigma_{j,k}^{\MP}}{LD}\sqrt{N})$.
\end{assump}
\begin{assump}\label{assump:mpnumber}
     The number of minipatches satisfies $B\gg(\frac{D^2L^2N}{(\sigma_{j,k}^{\MP})^2}+1)\log N$.
\end{assump}
These are mild assumptions on the minipatch size and number. Further, note that predictions between any pair of minipatches must simply be bounded, which is a much weaker condition that stability conditions typical in the distribution-free inference literature \citep{kim2023black}.  
\begin{theorem}[Coverage of iLOCO-MP]\label{thm:coverage_MP}
    Suppose Assumptions \ref{assump:thirdmoment}-\ref{assump:mpnumber} hold and $\{(X_i,Y_i)\}_{i=1}^N$ i.i.d., then we have $\lim_{N\rightarrow\infty}\bbP(\iloco^{\MP}_{j,k}\in \mathbb{C}^{\MP}_{j,k})= 1-\alpha$. 
\end{theorem}
The detailed theorems, assumptions, and proofs can be found in the appendix. Note that the proof follows closely from that of \citep{gan2022inference}, with the addition of a third moment condition to imply uniform integrability.  Overall, our work has provided the first model-agnostic and distribution-free inference procedure for feature interactions that is asymptomatically valid under mild assumptions.  Further, note that while \iloco inference via minipatches requires utilizing minipatch ensemble predictors, it gains in both statistical and computational efficiency as it does not require data splitting and conducting inference conditional on only a random portion of the data.  


%The proof of Theorem \ref{thm:coverage_MP} follows the individual feature importance inference theory \citep{gan2022inference}, where the inherent stability of minipatch ensembles is exploited to address the dependency across $\widehat{\iloco}_{j,k}(X_i,Y_i)$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Studies}
\subsection{Simulation Setup and Results}
We design simulation studies to validate our proposed \iloco metric and the corresponding inference procedure.  We generate data $X_{i}\stackrel{i.i.d} \sim N(0,\mathbf{I})$ and let $M=10$ and $N=500$ in our base simulations.  We consider three major simulation scenarios for classification and regression as well as linear and non-linear settings: (i) $f(\mathbf{X}) = snr \cdot (X_1 \cdot X_2) + \mathbf{X}\boldsymbol{\beta}$, (ii) $f(\mathbf{X})  = snr \cdot (X_1 \cdot X_2) + (X_2 \cdot X_3) +  (X_3 \cdot X_4) +  (X_4 \cdot X_5) + \mathbf{X}\boldsymbol{\beta}$, and (iii) $f(\mathbf{X})  = snr \cdot (X_1 \cdot X_2 \cdot X_3) + \mathbf{X}\boldsymbol{\beta}$.  Here, $\beta_j \sim N(2,0.5)$ for $j = 1, \dots, 5$, and $\beta_j = 0$ for $j = 6, \dots, 10$ and as we are primary interested in detecting the first feature interaction, we let $snr$ denote the signal strength of the interaction.  For non-linear simulations, we modify the interaction terms in $f(\mathbf{X})$ by applying a hyperbolic tangent transformation ($\tanh$) to all products.  For regression scenarios, we let $Y = f(\mathbf{X}) + \epsilon$ where $ \epsilon \stackrel{i.i.d.}{\sim}N(0,1)$, and for classification scenarios, we employ a logistic model with $Y \stackrel{i.i.d.}{\sim} Bern(\sigma (f(\mathbf{X})))$, where $\sigma$ is the sigmoid function.  In our simulations, we set the number of minipatches for \iloco-MP to be $B = 10,000$ and the minipatch size to be $m = 20\%M$ and $n = 20\%N$; for \iloco-Split, we utilize 50\% of samples for training and 50\% for evaluating the predictions or inference.  several baselines including the H-Statistic \cite{Friedman:2008}, FaithShap \cite{Tsai:2023}, Iterative Forest\cite{Basu:2018}, and our proposed metrics \iloco-split and \iloco-MP. We report the success probability, which is defined as the proportion of times the (1,2) feature pair obtains the largest metric among all feature pairs. We report the success probability instead of the raw metric to enable consistent comparisons across methods and calibrate feature pair importance relative to others. 



%In the base simulation, we generate a linear model where $\mathbf{X} = \left[X_1,X_2, \dots,X_P \right] \in \mathbb{R}^{N\times P}$, where $X_{ij}\stackrel{i.i.d} \sim N(0,1)$ for all $i,j$. In our simulations, $P= 10$ and $N = 500$. We generate relevant signal information by introducing $\boldsymbol{\beta} \in \mathbb{R}^P$, where $\beta_j \sim N(2,0.5)$ for $j = 1, \dots, 5$, and $\beta_j = 0$ for $j = 6, \dots, 10$. We are primarily interested in detecting interactions between the first two features, and the signal strength of this interaction is controlled by the signal-to-noise ratio (SNR), which is some constant that we denote by $snr$. We consider three major simulation scenarios for classification, regression, linear, and non-linear settings. The linear simulations are generated as follows: (i) $f(\mathbf{X}) = snr \cdot (X_1 \cdot X_2) + \mathbf{X}\boldsymbol{\beta}$, (ii) $f(\mathbf{X})  = snr \cdot (X_1 \cdot X_2) + (X_2 \cdot X_3) +  (X_3 \cdot X_4) +  (X_4 \cdot X_5) + \mathbf{X}\boldsymbol{\beta}$, and (iii) $f(\mathbf{X})  = snr \cdot (X_1 \cdot X_2 \cdot X_3) + \mathbf{X}\boldsymbol{\beta}$. For non-linear simulations, we modify the interaction terms in $f(\mathbf{X})$ by applying a hyperbolic tangent transformation ($\tanh$) to all products. For example, the non-linear analogue of simulation (i) becomes $f(\mathbf{X}) = \text{snr} \cdot \tanh(X_1 \cdot X_2) + X_{ij}\beta_j$.  For regression scenarios, we let $Y = f(\mathbf{X}) + \epsilon$ where $ \epsilon \stackrel{i.i.d.}{\sim}N(0,1)$, and for classification scenarios, we employ a logistic model with $Y \stackrel{i.i.d.}{\sim} Bernoulli(\sigma (f(\mathbf{X})))$, where $\sigma$ is the sigmoid function. In our simulations, we set the number of minipatches size to be $B = 10,000$ and the minipatch size to be $m = 20\%M$ and $n = 20\%N$. Additional simulation results for both classification and regression tasks with varying minpatch sizes and correlated features are presented in the appendix. 


We begin by evaluating \iloco's ability to capture the influence of a single feature pair across our aforementioned simulations. Figure~\ref{fig:comp_rankings} presents the success probability for detecting feature (1,2) averaged over 20 replicates across SNR levels for kernel radius basis function (KRBF), random forest (RF), and multilayer perception (MLP) regressors on non-linear classification simulations (i), (ii), (iii). The first two rows of Figure~\ref{fig:comp_rankings} illustrate simulations (i) and (ii). In these cases, we would expect the success probability of detecting feature pair (1,2) to increase as the SNR increases because the first feature pair has signal in these scenarios. As shown, \iloco-Split (blue) and \iloco-MP (red) consistently achieve the highest success probabilities compared to other metrics, demonstrating that our proposed metric more effectively captures feature interactions than the baselines. Notably, the success probability for Faith-Shap (black) does not always start at 0 when SNR= 0, indicating that Faith-Shap may not be well-calibrated relative to other feature pairs. In contrast, simulation (iii), which involves a tertiary interaction, should have a different outcome because feature pair (1,2) is null in this scenario. Indeed, we observe that the success probabilities for H-Score (yellow), I-Forest (green), and \iloco-MP (red) remain near zero, aligning with expectations. However, Faith-Shap (black) and \iloco-Split (blue) show success probabilities hovering around 0.25. For \iloco-Split, this may be due to the reduced stability caused by splitting the data into training and test subsets, especially when the sample size (\(N\)) is smaller. Additionally, as noted earlier, Faith-Shap’s behavior suggests it is not well-calibrated. Additional simulation results for linear and linear, classification and regression, correlated features with $\boldsymbol{\Sigma} \neq \mathbf{I} $, and different minipatch sizes are in the appendix. 
 


\begin{figure}[h!]
    \centering
    \includegraphics[width =0.75\textwidth]{figures/fig2.pdf}\\
    \vspace{-.2cm}
    \caption{Panel (a) shows the success probability of detecting feature triple (1,2,3) for simulation (iii) using KRBF regressor and our higher-order extension of the \iloco metric (estimated via \iloco-MP). Panel (b) shows the success probability of detecting correlated, important feature pair (1,2).  The success probability for \iloco detection is defined as the proportion of times \iloco$_{1,2}$ is the most negative amongst all pairs at each SNR value. For the \loco detection, the success probability is defined as the proportion of times \loco for both features 1 and 2 obtain the highest two values compared to all other features at each SNR. This validates that negative values of our \iloco are able to identify features that are both correlated and important, and that are missed by existing feature importance metrics.}
    \label{fig:corr_imp}
    \vspace{-.2cm}
\end{figure}
\begin{table*}[htp!]
\small
\caption{Timing results (seconds) for various dataset sizes using Simulation 1 and the KRBF regressor for all methods except Iterative Forest, where RF regressor was used. As $M$ and $N$ grow, computing interaction importance scores using H-Statistic and Faith Shap becomes infeasible. Note that the (p) indicates the code for that method was distributed across multiple processes.}
\label{tab:timing_results}
\centering
\begin{tabular}{lllll}
\toprule
    Method         & $N$ = 250, $M$ = 10 & $N$ = 500, $M$ = 20 & $N$ = 1000, $M$ = 100 & N = 10000, M = 500 \\ 
    \toprule
H-Statistic          &       285.4         &          97201.2       &           $>$ 6 days & $>$ 6 days       \\ 
Faith-Shap           &          70.2       &             72801.3    &           $>$ 6 days    & $>$ 6 days    \\ 
Iterative Forest     &           14.1      &             17.8  &               20.3 &  76.8 \\
\midrule
iLOCO Data Splitting &       16.8 (p)         &        144.7 (p)         &           738.3 (p)     &   5481.2 (p) \\ 
iLOCO MP             &     24.3 (p)         &              27.2 (p)   &           38.7 (p)     & 193.5 (p) \\ 
\end{tabular}
\end{table*}

In Figure~\ref{fig:corr_imp}, we validate \iloco's extensions. We first validate \iloco's ability to extend to higher-order interactions by computing $\text{\iloco}_{j,k,\ell}$ on simulation (iii) using a KRBF regressor to capture the tertiary interaction among features (1, 2, 3). Figure~\ref{fig:corr_imp}a illustrates the success probability of detecting this feature trio using \iloco-MP. We employ $B = 50,000$ minipatches to account for the increased complexity and interaction space when evaluating three-way feature relationships.  As the SNR increases, \iloco-MP successfully detects the tertiary interaction nearly 90\% of the time, validating that \iloco properly extends to higher-order interactions. We also evaluate \iloco's ability to detect features that are both important and correlated. Figure~\ref{fig:corr_imp}b shows the success probability of detecting the correlated, important feature pair \((1, 2)\) for a KRBF regression model. In this setup, we let $f(\mathbf{X}) = \mathbf{X}\boldsymbol{\beta}$, and features are correlated (\(\Sigma_{1,2} = 0.3\) or \(\Sigma_{1,2} = 0.9\)). Here, SNR denotes $\beta_1 = \beta_2$. Success probability for \loco is the proportion of times features 1 and 2 rank highest, while for \iloco, it is the proportion where \iloco$_{1,2}$ is the most negative. As SNR increases, \iloco consistently detects correlated, important features, unlike \loco, which often fails. This effect becomes more pronounced with stronger correlations.

\begin{figure}[h!]
    \centering
    \includegraphics[width =0.75\textwidth]{figures/cov_lin_reg_0.pdf}
    \vspace{-.4cm}
    \caption{Coverage of 90\% confidence intervals for a null feature pair in synthetic regression simulation (i) using KRBF, MLP, and RF as the base estimators. \iloco-MP exhibits slight over-coverage whereas \iloco-Split has valid coverage greater 0.9 for sufficiently large sample size; this validates of inferential theory.}
    \label{fig:coverage}
    \vspace{-.6cm}
\end{figure}
Additionally, we construct an empirical study to demonstrate that the interaction feature importance confidence intervals generated by \iloco-MP and \iloco-Split have valid coverage for the inference target. Since the exact value of the inference target \iloco involves expectation and hence is hard to compute, we use Monte Carlo approximation with 10,000 test data points and $B=10,000$ random minipatches. We evaluate the coverage and width of the confidence intervals constructed from 50 replicates. Figure~\ref{fig:coverage} shows coverage rates for \iloco-Split and \iloco-MP of 90$\%$ confidence intervals for a null feature pair in regression simulation (i) using KRBF, MLP, and RF as the base estimators. \iloco-MP exhibits slight overage-coverage whereas \iloco-Split has valid coverage 0.9 for sufficiently large sample size, validating our inferential theory. We include additional studies where SNR = 2 in the appendix.

A key advantage of our \iloco approach, particularly \iloco-MP, is its exceptional computational efficiency. Table~\ref{tab:timing_results} compares the computational time required to calculate feature interaction metrics across all features in the dataset. Results are recorded on an Apple M2 Ultra 24-Core CPU at 3.24 GHz, 76-Core GPU, and 128GB of Unified Memory and across 11 processes when denoted using (p).  As the dataset size (\(N\)) and the number of features (\(M\)) increase, methods like H-Statistic and Faith-Shap quickly become computationally infeasible. In contrast, \iloco-MP shows significant efficiency gains, making it scalable to larger datasets without sacrificing performance. While Iterative Forest achieves reasonable computational times, it is restricted to random forest models. The efficiency and model-agnostic nature of \iloco-MP highlight its practicality for real-world use cases involving complex datasets.

\begin{figure*}[h!]
    \centering
    \includegraphics[width =0.95\textwidth]{figures/fig4.pdf}
    \vspace{-.4cm}
    \caption{\iloco (computed via \iloco-MP) marginal confidence intervals ($\alpha = 0.1$) on the Car Evaluation data (a) and Cocktail Recipe data (b). Confidence intervals that do not contain zero (blue dashed line) are statistically significant. Our inference procedure identifies meaningful, statistically significant interactions in both data sets, while also quantifying the uncertainty in our \iloco metric.}
    \label{fig:case_study}
    \vspace{-.4cm}
\end{figure*}

\subsection{Case Studies}
We validate \iloco by computing feature importance scores and intervals for two datasets. The Car Evaluation dataset with $N=1728$ and $M=15$ one-hot-encoded features seeks to predict car acceptability (classification task) \cite{car_evaluation_19}.  We fit a minipatch forest model with 10,000 decision trees and a minipatch size of \(m = 20\%M\) and \(n = 20\%M\).  Second, we gathered a new Cocktail Recipe dataset by web scrapping the Difford's Guide website \citep{cocktail:2025}.  For our analysis, we consider the top $M=200$ most frequent one-hot-encoded ingredients for $N=15,972$ cocktails and the task is to predict the official Difford's Guide ratings (regression task).  We fit a kernel regressor with an RBF kernel with 30,000 minipatches, setting \(m = 50\%M\) and \(n = 50\%N\). In both cases, we set the error rate \(\alpha = 0.1\) and report marginal confidence intervals which control the False Coverage Rate \citep{benjamini:2005}.  In Figure~\ref{fig:case_study}, confidence intervals that do not contain zero (blue line) are deemed statistically significant.  


In Figure~\ref{fig:case_study}a, we show the \iloco-MP metric with confidence intervals for all feature pairs in the car evaluation dataset. Notably, the most important interactions include ``medium price \& low maintenance,'' ``4 doors \& 5+ doors,'' and ``high maintenance \& 3+ doors." These interactions suggest that the combination of price and maintenance level, as well as the number of doors, play a significant role in determining the car’s overall evaluation. For example, a medium price combined with low maintenance suggests an optimal trade-off between cost and upkeep, which could appeal to consumers. Interestingly, the interaction between ``4 people \& low safety" has a high \iloco score but a lower bound below zero, indicating that while this combination might occasionally influence car acceptability, its contribution is not consistent across all cars. 

Figure~\ref{fig:case_study}b shows the \iloco-MP metric with confidence intervals for 40 ingredient interactions. \iloco revealed two notable interactions: ``Triple Sec \& Agave Syrup" and ``Coffee Liqueur \& Espresso Coffee." The former likely reflects their complementary nature, as they are common pairings in popular cocktails such as margaritas. The latter pairing highlights the natural harmony between coffee elements, commonly found in popular espresso-based martinis. Interestingly, ``Cranberry Juice \& Black Raspberry Liqueur" exhibit a relatively high \iloco score, likely due to their bold flavor combination blending tartness with rich berry notes. However, the higher degree of uncertainty may stem from variability in how drinkers perceive this pairing, as some may find it well-balanced while others see it as overpowering. Overall, these results emphasize the value of \iloco and our inference procedure in detecting meaningful interactions between features and the importance of using confidence intervals to reflect the uncertainty in feature interactions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
In this work, we propose \iloco, a model-agnostic and distribution-free metric to quantify feature interactions.  We also introduce the first inference procedure to construct confidence intervals to measure the uncertainty of feature interactions.  Additionally, we propose a computationally fast way to estimate \iloco and conduct inference using minipatch ensembles, allowing our approach to scale both computationally and statistically to large data sets.  Our empirical studies demonstrate the superior ability of \iloco to detect important feature interactions and highlight the important of uncertainty quantification in this context.  We also briefly introduced using \iloco to detect important correlated features as well as \iloco for higher-order interactions, but future work could consider these important challenges further.  Finally, for increasing numbers of features, detecting and testing pairwise and higher-order interactions becomes a major challenge.  Future work could consider adaptive learning strategies, perhaps paired with minipatch ensembles \citep{yao:2020}, to focus both computational and statistical efforts on only the most important interactions in a large data set. 



 
% \subsubsection*{Acknowledgments}
% Blinded.


\section*{Acknowledgements}
The authors acknowledge support from NSF DMS-1554821, NSF NeuroNex-1707400, and NIH 1R01GM140468. COL acknowledges support from the NSF Graduate Research Fellowship Program under grant number 1842494. 



\printbibliography
\end{refsection}

\clearpage
\appendix 
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\begin{refsection}
\section{Inference Algorithms}
\begin{algorithm}[htbp]
\label{alg:iloco-split}
\caption{\iloco-Split Estimation and Inference}
\textbf{Input:} Training data $(\bX^{\rm{tr}},\bY^{\rm{tr}})$, test data $(\bX^{\rm{test}},\bY^{\rm{test}})$, features $j,k$, base learners $H$.
\begin{enumerate}
    \item Split the data into disjoint training and test sets: $(\bX^{\rm{tr}},\bY^{\rm{tr}})$, $(\bX^{\rm{test}},\bY^{\rm{test}})$.
    \item Train prediction models $\hat{f}$, $\hat{f}^{- j}$, $\hat{f}^{- k}$, $\hat{f}^{- j,k}$ on $(\bX^{\rm{tr}},\bY^{\rm{tr}})$: 
    \begin{equation*}
    \begin{split}
        \hat{f}(X) = H(\bX^{\rm{tr}},\bY^{\rm{tr}})(X),\quad \hat{f}^{- j}(X) = H(\bX^{\rm{tr}}_{:,\backslash j},\bY^{\rm{tr}})(X_{\backslash j})\\
    \hat{f}^{- k}(X) = H(\bX^{\rm{tr}}_{:,\backslash k},\bY^{\rm{tr}})(X_{\backslash k}),\quad  \hat{f}^{- j,k}(X) = H(\bX^{\rm{tr}}_{:,\backslash j,k},\bY^{\rm{tr}})(X_{\backslash j,k})
    \end{split}
    \end{equation*}
    \item For the $i$th sample in the test set, compute the feature importance score and interaction score as follows:
    \[
    \hat{\Delta}_j(X^{\test}_i, Y^{\test}_i) = \text{Error}(Y^{\test}_i, \hat{f}^{- j}(X^{\test}_i)) - \text{Error}(Y^{\test}_i, \hat{f}(X^{\test}_i)),
    \]
    \[
    \hat{\Delta}_k(X^{\test}_i, Y^{\test}_i) = \text{Error}(Y^{\test}_i, \hat{f}^{- k}(X^{\test}_i)) - \text{Error}(Y^{\test}_i, \hat{f}(X^{\test}_i)),
    \]
    \[
    \hat{\Delta}_{j,k}(X^{\test}_i, Y^{\test}_i) = \text{Error}(Y^{\test}_i, \hat{f}^{- j,k}(X^{\test}_i)) - \text{Error}(Y^{\test}_i, \hat{f}(X^{\test}_i)).
    \]
    \item Calculate iLOCO Metric for each sample $i$: 
    \[
    \widehat{\iloco}_{j,k}(X^{\test}_i, Y^{\test}_i) = \hat{\Delta}_j(X^{\test}_i, Y^{\test}_i) + \hat{\Delta}_k(X^{\test}_i, Y^{\test}_i) - \hat{\Delta}_{j,k}(X^{\test}_i, Y^{\test}_i).
    \]
    \item Let $N_{\test}$ be the sample size of the test data. Obtain a $1-\alpha$ confidence Interval for $\iloco_{j,k}$: $$\mathbb{C}^{\spt}_{j,k} = \left[\overline{\iloco}_{j,k} - \frac{Z_{\alpha/2\hat{\sigma}_{j,k}}}{\sqrt{N_{\test}}}, \overline{\iloco}_{j,k} + \frac{Z_{\alpha/2\hat{\sigma}_{j,k}}}{\sqrt{N_{\test}}}\right],$$ with \\
    $$\overline{\iloco}_{jk} = \frac{1}{N_{\test}}\sum_{i=1}^{N_{\test}} \widehat{\iloco}_{jk}(X^{\test}_i, Y^{\test}_i)$$ being the sample mean and $$\hat{\sigma}_{j,k} = \sqrt{\frac{\sum_{i=1}^{N_{\test}}(\widehat{\iloco}_{jk}(X^{\test}_i, Y^{\test}_i) - \overline{\iloco}_{j,k})^2}{N_{\test}-1}}$$ being the sample standard deviation.
    
\end{enumerate}
\textbf{Output:} $\mathbb{C}^{\spt}_{j,k}$
\end{algorithm}
\begin{algorithm}[htbp]
\label{alg:ilocomp}
\caption{\iloco-MP Estimation and Inference}
\textbf{Input:} Training pairs $(\mathbf{X}, \mathbf{Y})$, features $j,k$, minipatch sizes $n,m$, number of minipatches $B$, base learners $H$.
\begin{enumerate}
    \item Perform Minipatch Learning: for $k = 1, \dots, B$
    \begin{itemize}
        \item Randomly subsample $n$ observations $I_b \subset [N]$ and $M$ features $F_b \subset [M]$.
        \item Train prediction model $\hat{f}_b$ on $(\mathbf{X}_{I_b, F_b}, \mathbf{Y}_{I_b})$:  
        $\hat{f}_b(X) = H(\mathbf{X}_{I_b,F_b}, \mathbf{Y}_{I_b})(X_{F_b}).$
    \end{itemize}
    \item Obtain predictions:  
      LOO prediction:  
        \[
        \hat{f}_{-i}(X_i) = \\
        \frac{1}{\sum_{b=1}^{B} \mathbb{I}(i \notin I_b)} \sum_{b=1}^{B} \mathbb{I}(i \notin I_b) \hat{f}_b(X_i).
        \]
        LOO + LOCO prediction (feature $j$):  
        \[
        \hat{f}^{-j}_{-i}(X_i) = \\
        \frac{1}{\sum_{b=1}^{B} \mathbb{I}(i \notin I_b) \mathbb{I}(j \notin F_b)} \sum_{b=1}^{B} \mathbb{I}(i \notin I_b) \mathbb{I}(j \notin F_b) \hat{f}_b(X_i).
        \]
        LOO + $\text{LOCO}_2$ prediction (features $j, k$):  
        \[
        \hat{f}^{-j, -k}_{-i}(X_i) = \\
        \frac{1}{\sum_{b=1}^{B} \mathbb{I}(i \notin I_b) \mathbb{I}(j, k \notin F_b)} \sum_{b=1}^{B} \mathbb{I}(i \notin I_b) \mathbb{I}(j, k \notin F_b) \hat{f}_b(X_i).
        \]
    \item Calculate LOO Feature Occlusion:  
    \[
    \hat{\Delta}_j(X_i, Y_i) = \text{Error}(Y_i, \hat{f}^{-j}_{-i}(X_i)) - \text{Error}(Y_i, \hat{f}_{-i}(X_i)),
    \]
    \[
    \hat{\Delta}_k(X_i, Y_i) = \text{Error}(Y_i, \hat{f}^{-k}_{-i}(X_i)) - \text{Error}(Y_i, \hat{f}_{-i}(X_i)),
    \]
    \[
    \hat{{\Delta}}_{j,k}(X_i, Y_i) = \text{Error}(Y_i, \hat{f}^{-j, -k}_{-i}(X_i)) - \text{Error}(Y_i, \hat{f}_{-i}(X_i)).
    \]
    \item Calculate iLOCO Metric for each sample $i$: 
    \[
    \widehat{\iloco}_{j,k}(X_i, Y_i) = \hat{\Delta}_j(X_i, Y_i) + \hat{\Delta}_k(X_i, Y_i) - \hat{\Delta}_{j,k}(X_i, Y_i).
    \]
    \item Obtain a $1-\alpha$ confidence Interval for $\iloco_{j,k}$: $\mathbb{C}^{\MP}_{j,k} = \left[\overline{\iloco}_{j,k} - \frac{Z_{\alpha/2\hat{\sigma}_{j,k}}}{\sqrt{N}}, \overline{\iloco}_{j,k} + \frac{Z_{\alpha/2\hat{\sigma}_{j,k}}}{\sqrt{N}}\right]$, with \\
    $\overline{\iloco}_{jk} = \frac{1}{N}\sum_{i=1}^N \widehat{\iloco}_{jk}(X_i,Y_i)$ being the sample mean and $\hat{\sigma}_{j,k} = \sqrt{\frac{\sum_{i=1}^N(\widehat{\iloco}_{jk}(X_i,Y_i) - \overline{\iloco}_{j,k})^2}{N-1}}$ being the sample standard deviation
    
\end{enumerate}
\textbf{Output:} $\mathbb{C}^{\MP}_{j,k}$
\end{algorithm}
\section{Inference Theory for iLOCO-Split}
\begin{assump}\label{assump:split_thirdmoment}
    The normalized iLOCO score on a random test sample satisfies the third moment assumption: $\bbE(\widehat{\iloco}_{j,k}(X_i^{\test},Y_i^{\test})-\iloco^{\spt}_{j,k})^3/(\sigma^{\spt}_{j,k})^3\leq C$, where $(\sigma^{\spt}_{j,k})^2=\mathrm{Var}(\widehat{\iloco}_{j,k}(X_i^{\test},Y_i^{\test})|\bX^{\tr},\bY^{\tr})$.
\end{assump}
We require Assumption \ref{assump:split_thirdmoment} to establish the central limit theorem for the iLOCO metrics evaluated on the test data. Prior theory on LOCO-Split \citep{rinaldo:2019} does not have this assumption, but they focus on a truncated linear predictor and consider injecting noise into the LOCO scores, which can imply a third moment assumption similar to Assumption \ref{assump:split_thirdmoment}.
\begin{theorem}[Coverage of iLOCO-Split]
     Suppose that data $(X_i,Y_i)\overset{i.i.d.}{\sim}\mathcal{P}$. Under Assumption \ref{assump:split_thirdmoment}, we have $\lim_{N\rightarrow\infty}\bbP(\iloco^{\spt}_{j,k}\in \mathbb{C}^{\spt}_{j,k})= 1-\alpha$.
\end{theorem}
\begin{proof}
    Recall our definition of $\widehat{\iloco}_{j,k}(X_i^{\test},Y_i^{\test})$ in Algorithm \ref{alg:iloco-split}:
    $$
    \widehat{\iloco}_{j,k}(X^{\test}_i, Y^{\test}_i) = \hat{\Delta}_j(X^{\test}_i, Y^{\test}_i) + \hat{\Delta}_k(X^{\test}_i, Y^{\test}_i) - \hat{\Delta}_{j,k}(X^{\test}_i, Y^{\test}_i).
    $$
    Since we have assumed that all samples of the test data $(X_i^{\test},Y_i^{\test})\overset{i.i.d.}{\sim}\mathcal{P}$, we note that $\bbE(\hat{\Delta}_j(X^{\test}_i, Y^{\test}_i)|\bX^{\tr},\bY^{\tr}) = \Delta_j^{\spt}$, $\bbE(\hat{\Delta}_k(X^{\test}_i, Y^{\test}_i)|\bX^{\tr},\bY^{\tr}) = \Delta_k^{\spt}$, $\bbE(\hat{\Delta}_k(X^{\test}_i, Y^{\test}_i)|\bX^{\tr},\bY^{\tr}) = \Delta_{j,k}^{\spt}$,
    and hence
    $$\bbE[\widehat{\iloco}_{j,k}(X_i^{\test},Y_i^{\test})|\bX^{\tr},\bY^{\tr}]=\Delta_j^{\spt}+\Delta_k^{\spt}-\Delta_{j,k}^{\spt}=\iloco^{\spt}_{j,k}.$$
    Furthermore, due to Assumption \ref{assump:split_thirdmoment}, the Lyapunov condition holds for $\widehat{\iloco}_{j,k}(X_i^{\test},Y_i^{\test})-\bbE(\widehat{\iloco}_{j,k}(X_i^{\test},Y_i^{\test})|\bX^{\tr},\bY^{\tr})/\sigma_{j,k}^{\spt}$, conditional on the training set $\bX^{\tr},\bY^{\tr}$, and hence we can invoke the central limit theorem to obtain that
    \begin{equation*}
        \frac{1}{\sigma^{\spt}_{j,k}\sqrt{N^{\test}}}\sum_{i=1}^{N^{\test}}[\widehat{\iloco}_{j,k}(X_i^{\test},Y_i^{\test})-\iloco^{\spt}_{j,k}]\overset{d.}{\rightarrow}\mathcal{N}(0,1).
    \end{equation*}
    Now, it remains to show the consistency of the variance estimate $\hat{\sigma}_{j,k} = \frac{1}{N^{\test}-1}\sum_{i=1}^{N^{\test}}(\widehat{\iloco}(X_i^{\test},Y_i^{\test})-\overline{\iloco})^2$ for $(\sigma^{\spt}_{j,k})^2$.

    Define the random variable $\xi_{N^{\test},i} = [\widehat{\iloco}_{j,k}(X_i^{\test},Y_i^{\test})-\iloco^{\spt}_{j,k}]^2/(\sigma_{j,k}^{\spt})^2$. We aim to show that $\frac{1}{N^{\test}-1}\sum_{i=1}^{N^{\test}} \xi_{N^{\test},i}\overset{p}{\rightarrow} 1$. By Assumption \ref{assump:split_thirdmoment}, we have $\bbE|\xi_{N^{\test},i}|^{3/2}=o(\sqrt{N})$. This implies the uniform integrability of $\{\xi_{N^{\test},i}\}_i$: 
    \begin{equation*}
        \begin{split}
    \bbE[|\xi_{N^{\test},i}|\ind(|\xi_{N^{\test},i}|>x)] \leq & [\bbE|\xi_{N^{\test},i}|^{3/2}]^{\frac{2}{3}}[\bbP(|\xi_{N^{\test},i}|>x)]^{1/3}\\
    \leq &[\bbE|\xi_{N^{\test},i}|^{3/2}]^{\frac{2}{3}}[\frac{\bbE|\xi_{N^{\test},i}|}{x}]^{1/3}\\
    = & C^{2/3}x^{-1/3},
        \end{split}  
    \end{equation*}
    which converges to zero as $x\rightarrow \infty$.
Here, we applied Holder's inequality on the first line, and applied Assumption \ref{assump:split_thirdmoment} on the last line. With the uniform integrability of $\xi_{N^{\test},i}$, we can follow the last part of the proof of Theorem 4 in \cite{bayle:2020}, and show that $$\frac{1}{N^{\test}}\sum_{i=1}^{N^{\test}} \xi_{N^{\test},i}-1\overset{p}{\rightarrow} 0,$$ which then implies $\frac{1}{N^{\test}-1}\sum_{i=1}^{N^{\test}} \xi_{N^{\test},i}\overset{p}{\rightarrow} 1$ as $N^{\test}\rightarrow\infty$, and hence $\hat{\sigma}_{j,k}/\sigma_{j,k}^{\MP}\overset{p}{\rightarrow}1$.

    Therefore, by Slutsky's theorem, we have 
    \begin{equation*}
        \frac{1}{\hat{\sigma}_{j,k}\sqrt{N^{\test}}}\sum_{i=1}^{N^{\test}}[\widehat{\iloco}_{j,k}(X_i^{\test},Y_i^{\test})-\iloco^{\spt}_{j,k}]\overset{d.}{\rightarrow}\mathcal{N}(0,1),
    \end{equation*}
    which implies the asymptotically valid coverage of $\mathbb{C}^{\spt}_{j,k}$ for $\iloco_{j,k}^{\spt}$.
\end{proof}
\section{Inference Theory for iLOCO-MP}
\paragraph{Notations:} Let $h_{j,k}(X,Y;\bX,\bY) = \err(Y,\mu_{\backslash j}(X_{\backslash j};\bX_{:,\backslash j},\bY))+\err(Y,\mu_{\backslash k}(X_{\backslash k};\bX_{:,\backslash k},\bY))-\err(Y,\mu_{\backslash j,k}(X_{\backslash j,k};\bX_{:,\backslash j,k},\bY)) - \err(Y,\mu(X;\bX,\bY))$ be the interaction importance score of feature pair $(j,k)$, when using the model trained on $(\bX,\bY)$ to predict data $(X,Y)$. Recall our definition of the $\iloco^{\MP}_{j,k}$, we can then write
\begin{equation*}
    \iloco^{\MP}_{j,k} = \bbE_{X,\,Y}[h_{j,k}(X,\,Y;\bX,\,\bY)|\bX,\,\bY],
\end{equation*}
where $(X,\,Y)$ is independent of the training data $(\bX,\,\bY)$. Also define $h_{j,k}(X,\,Y) = \bbE_{\bX,\,\bY}[h_{j,k}(X,\,Y;\bX,\,\bY)|X,Y]$, where the expectation is taken over the training but not test data. Under the notation in the main paper, $\widetilde{\iloco}_{j,k}(X,Y) = h_{j,k}(X,Y)$. Its variance $\mathrm{Var}(h_{j,k}(X_i,Y_i))$ is denoted by $(\sigma^{\MP}_{j,k})^2$. Let $\hat{h}_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})=\widehat{\iloco}_{j,k}(X_i,Y_i)$, the estimated pairwise interaction importance score at sample $i$ in Algorithm \ref{alg:ilocomp}. Let $\tilde{h}_{j,k}(X_i,Y_i;\bX_{\backslash i,;},\bY_{\backslash i})=h_{j,k}(X_i,Y_i;\bX_{\backslash i,;},\bY_{\backslash i})-h_{j,k}(X_i,Y_i)$. Denote the trained predictor on each minipatch $(I,F)$ by $\hat{\mu}_{I,F}(\cdot)$.
\begin{assump}\label{assump:thirdmoment}
    The normalized interaction importance r.v. satisfies the third moment condition: $\bbE[h_{j,k}(X,\,Y) - \bbE h_{j,k}(X,\,Y)]^3/(\sigma^{\MP}_{j,k})^3 \leq C$.
\end{assump}
\begin{assump}\label{assump:err_lipschitz}
    The error function is Lipschitz continuous w.r.t. the prediction: for any $Y\in \bbR$ and any predictions $\hat{Y}_1,\,\hat{Y}_2\in \bbR^d$,
    $$
    |\err(Y,\hat{Y}_1)-\err(Y,\hat{Y}_2)|\leq L\|\hat{Y}_1-\hat{Y}_2\|_2.
    $$    
\end{assump}
\begin{assump}\label{assump:bnd_mu}
    The prediction difference between different minipatches are bounded: $\|\hat{\mu}_{I,F}(X)-\hat{\mu}_{I',F'}(X)\|_2\leq D$ for any $X$, any minipatches $I,\,I'\subset[N]$, $F,\,F'\subset[M]$.
\end{assump}
\begin{assump}\label{assump:mpsize}
    The minipatch sizes $(m,\,n)$ satisfy $\frac{m}{M},\,\frac{n}{N}\leq \gamma$ for some constant $0<\gamma<1$, and $n=o(\frac{\sigma^{\MP}_{j,k}}{LD}\sqrt{N})$.
\end{assump}
\begin{assump}\label{assump:mpnumber}
     The minipatch size number satisfies $B\gg(\frac{D^2L^2N}{(\sigma^{\MP}_{j,k})^2}+1)\log N$.
\end{assump}
\begin{theorem}[Coverage of iLOCO-MP]
    Suppose the training data $(X_i,Y_i)$ are independent, identically distributed. Under Assumptions \ref{assump:thirdmoment}-\ref{assump:mpnumber}, we have $\lim_{N\rightarrow}\bbP(\iloco^{\MP}_{j,k}\in \mathbb{C}^{\MP}_{j,k})= 1-\alpha$.
\end{theorem}
\begin{proof}
    Our proof closely follows the argument and results in \cite{gan2022inference}. First, we can decompose the estimation error of the iLOCO interaction importance score at sample $i$ as follows:
    \begin{equation*}
        \begin{split}
            &\widehat{\iloco}_{j,k}(X_i,Y_i) - \iloco^{\MP}_{j,k}\\
            =&h_{j,k}(X_i,Y_i)-\bbE[h_{j,k}(X_i,Y_i)]+\hat{h}_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})-h_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})\\
            &+\bbE[h_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})|\bX_{\backslash i,:},\bY_{\backslash i}]-\iloco^{\MP}_{j,k}\\
            &+\tilde{h}_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})-\bbE[\tilde{h}_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})|\bX_{\backslash i,:},\bY_{\backslash i}]\\
            =:&h_{j,k}(X_i,Y_i)-\bbE[h_{j,k}(X_i,Y_i)] + \varepsilon_i^{(1)} + \varepsilon_i^{(2)} + \varepsilon_i^{(3)},
        \end{split}
    \end{equation*}
    where 
    \begin{equation*}
        \begin{split}
            \varepsilon_i^{(1)} = &\hat{h}_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})-h_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i}),\\
            \varepsilon_i^{(2)} = &\bbE[h_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})|\bX_{\backslash i,:},\bY_{\backslash i}]-\iloco^{\MP}_{j,k},\\
            \varepsilon_i^{(3)} = &\tilde{h}_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})-\bbE[\tilde{h}_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})|\bX_{\backslash i,:},\bY_{\backslash i}].
        \end{split}
    \end{equation*}
Let $\varepsilon^{(k)} = \frac{1}{\sigma^{\MP}_{j,k}\sqrt{N}}\sum_{i=1}^N\varepsilon_i^{(k)},k=1,\dots,3$, where $\sigma^{\MP}_{j,k}$ is the standard deviation of $h_{j,k}(X_i,Y_i)$, as we defined in notations. Assumption \ref{assump:thirdmoment}, $\{h_{j,k}(X_i,Y_i)\}_{i=1}^N$ satisfy the Lyapunov' condition, and hence we can apply central limit theorem to obtain that $\frac{1}{\sigma^{\MP}_{j,k}\sqrt{N}}\sum_{i=1}^Nh_{j,k}(X_i,Y_i)-\bbE[h_{j,k}(X_i,Y_i)]\overset{d.}{\rightarrow} \mathcal{N}(0,1)$. Following the same arguments as the proof of Theorem 1, 2 in \cite{gan2022inference}, we only need to show that $\varepsilon^{(1)},\,\varepsilon^{(2)},\,\varepsilon^{(3)}$ converge to zero in probability, and $\hat{\sigma}_{j,k}\overset{p}{\rightarrow}\sigma^{\MP}_{j,k}$, where $\hat{\sigma}_{j,k}^2 = \frac{1}{N-1}\sum_{i=1}^N(\widehat{\iloco}_{j,k}(X_i,Y_i) - \overline{\iloco}_{j,k})^2$ is defined as in Algorithm \ref{alg:ilocomp}.

\paragraph{Convergence of $\varepsilon^{(1)}$.} $\varepsilon^{(1)}$ characterizes the deviation of the random minipatch algorithm from its population version. In particular, by Assumption \ref{assump:err_lipschitz}, we can write
\begin{equation*}
\begin{split}
    |\varepsilon^{(1)}|\leq \frac{L}{\sigma_{j,k}\sqrt{N}}\sum_{i=1}^N\Big(&\|\mu_{-i}^{*-j,k}(X_i) - \hat{\mu}_{-i}^{-j,k}(X_i)\|_2+\|\mu_{-i}^{*-j}(X_i) - \hat{\mu}_{-i}^{-j}(X_i)\|_2\\
    &+\|\mu_{-i}^{*-k}(X_i) - \hat{\mu}_{-i}^{-k}(X_i)\|_2+\|\mu_{-i}^{*}(X_i) - \hat{\mu}_{-i}(X_i)\|_2\Big),
\end{split}
\end{equation*}
where 
$$\mu_{-i}^{*-j,k}(X_i) = \frac{1}{\binom{N-1}{n}\binom{M-2}{m}}\sum_{\substack{I\subset[N],|I|=n\\F\subset[M],|F|=m}}\ind(i\notin I)\ind(j,k\notin F)\hat{\mu}_{I,F}(X_i),$$ 
and $$\hat{\mu}_{-i}^{-j,k}(X_i) = \frac{1}{\sum_{b=1}^B\ind(i\notin I_b)\ind(j,k\notin F_b)}\sum_{b=1}^B\ind(i\notin I_b)\ind(j,k\notin F_b)\hat{\mu}_{I_b,F_b}(X_i).$$ $\mu_{-i}^{*-j}(X_i)$, $\hat{\mu}_{-i}^{-j}(X_i)$, $\mu_{-i}^{*-k}(X_i)$, $\hat{\mu}_{-i}^{-k}(X_i)$, $\mu_{-i}^{*}(X_i)$, $\hat{\mu}_{-i}(X_i)$ are defined similarly. We then follow the same arguments as those in Section A.7.1 of \cite{gan2022inference} to bound the MP predictor differences. The only difference lies in bounding $\|\mu_{-i}^{*-j,k}(X_i) - \hat{\mu}_{-i}^{-j,k}(X_i)\|_2$, where we need to concentrate $\frac{\sum_{b=1}^B\ind(i\in I_b)\ind(j,k\notin F_b)}{B}$ around $\bbP(i\in I_b,\,j,k\notin F_b)$. Since Assumption \ref{assump:mpsize} suggests that $\bbP(i\in I_b,\,j,k\notin F_b)\geq (1-\frac{n}{N})(1-\frac{m}{M})(1-\frac{m}{M-1})$ is lower bounded, all arguments in \cite{gan2022inference} can be similarly applied here, which also use Assumption \ref{assump:bnd_mu} and Assumption \ref{assump:mpnumber}. These arguments then lead to $\lim_{N\rightarrow}\bbP(|\varepsilon^{(1)}|>\epsilon)=0$ for any $\epsilon>0$.

\paragraph{Convergence of $\varepsilon^{(2)}$.} $\varepsilon^{(2)}$ captures the difference in iLOCO importance scores caused by excluding one training sample. In particular, we can write
\begin{equation*}
    \begin{split}
        |\varepsilon^{(2)}|\leq\frac{L}{\sigma_{j,k}\sqrt{N}}\sum_{i=1}^N\Big(&\bbE_X\|\mu_{-i}^{*-j,k}(X)-\mu^{*-j,k}(X)\|_2+\bbE_X\|\mu_{-i}^{*-j}(X)-\mu^{*-j}(X)\|_2\\
        &+\bbE_X\|\mu_{-i}^{*-k}(X)-\mu^{*-k}(X)\|_2+\bbE_X\|\mu_{-i}^{*}(X)-\mu^{*}(X)\|_2\Big),
    \end{split}
\end{equation*}
where
\begin{equation*}
    \begin{split}
        \mu^{*-j,k}(X)=\frac{1}{\binom{N}{n}\binom{M-2}{m}}\sum_{\substack{I\subset [N],|I|=n\\F\subset[M],|F|=m}}\ind(j,k\notin F)\hat{\mu}_{I,F}(X),\\
        \mu^{*-j}(X)=\frac{1}{\binom{N}{n}\binom{M-1}{m}}\sum_{\substack{I\subset [N],|I|=n\\F\subset[M],|F|=m}}\ind(j\notin F)\hat{\mu}_{I,F}(X),\\
        \mu^{*-k}(X) = \frac{1}{\binom{N}{n}\binom{M-1}{m}}\sum_{\substack{I\subset [N],|I|=n\\F\subset[M],|F|=m}}\ind(k\notin F)\hat{\mu}_{I,F}(X),\\
        \mu^{*}(X)=\frac{1}{\binom{N}{n}\binom{M}{m}}\sum_{\substack{I\subset [N],|I|=n\\F\subset[M],|F|=m}}\hat{\mu}_{I,F}(X),
    \end{split}
\end{equation*}
and $\mu_{-i}^{*-j,k}(X),\,\mu_{-i}^{*-j}(X),\,\mu_{-i}^{*-k}(X),\,\mu_{-i}^{*}(X)$ are defined as earlier. Following the same arguments as those in Section A.7.2 of \cite{gan2022inference}, we can bound the differences between the leave-one-out predictions and the full model predictions by $\frac{Dn}{N}$, Therefore, we have $|\varepsilon^{(2)}|\leq \frac{4LDn}{\sigma_{j,k}\sqrt{N}}$, and by Assumption \ref{assump:mpsize}, $\lim_{N\rightarrow\infty}\varepsilon^{(2)}=0$.

\paragraph{Convergence of $\varepsilon^{(3)}$.} Recall the definition of $\tilde{h}_j(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})$, we can write
\begin{equation*}
    \varepsilon^{(3)}=h_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})-\bbE[h_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})|\bX_{\backslash i,:},\bY_{\backslash i}]-h_{j,k}(X_i,Y_i)+\bbE[h_{j,k}(X_i,Y_i)].
\end{equation*}
The only difference of our proof here from Section A.7.3 in \cite{gan2022inference} is that we are looking at the interaction score function $h_{j,k}$ instead of individual feature importance function $h_j$. Therefore, the main argument is to bound the stability notion in \cite{bayle:2020} associated with function $h_{j,k}$:
\begin{equation*}
    \gamma_{loss}(h_{j,k})=\frac{1}{N-1}\sum_{l\neq i}\bbE[(h_{j,k}'(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})-h_{j,k}'(X_i,Y_i;\bX_{\backslash i,:}^{\backslash l},\bY_{\backslash i}^{\backslash l}))^2],
\end{equation*}
where $(\bX_{\backslash i,:},\bY_{\backslash i})$ denotes the training data excluding sample $i$, while $(\bX_{\backslash i,:}^{\backslash l},\bY_{\backslash i}^{\backslash l})$ excludes sample $i$ and substitutes sample $l$ by a new sample $(X_{N+1}, Y_{N+1})$. The function $h_{j,k}'(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})=h_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})-\bbE[h_{j,k}'(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})|\bX_{\backslash i,:},\bY_{\backslash i}]$. We note that
\begin{equation*}
\begin{split}
    \gamma_{loss}(h_{j,k})=&\frac{1}{N-1}\sum_{l\neq i}\bbE[\mathrm{Var}(h_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})-h_{j,k}(X_i,Y_i;\bX_{\backslash i,:}^{\backslash l},\bY_{\backslash i}^{\backslash l})|\bX_{\backslash i,:},\bY_{\backslash i},X_{N+1},Y_{N+1})]\\
    \leq&\frac{1}{N-1}\sum_{l\neq i}\bbE[(h_{j,k}(X_i,Y_i;\bX_{\backslash i,:},\bY_{\backslash i})-h_{j,k}(X_i,Y_i;\bX_{\backslash i,:}^{\backslash l},\bY_{\backslash i}^{\backslash l}))^2].
\end{split}
\end{equation*}
Recall our definitions of $\mu^{*-j}_{-i}(X),\,\mu^{*-k}_{-i}(X),\,\mu^{*-j,k}_{-i}(X)$ when showing the covergence of $\varepsilon^{(1)}$. In addition, we define $\mu^{*-j}_{-i}(X;l\leftarrow N+1),\,\mu^{*-k}_{-i}(X;l\leftarrow N+1),\,\mu^{*-j,k}_{-i}(X;l\leftarrow N+1)$ as the corresponding predictors if the training sample $(X_l,Y_l)$ were substituted by a new sample $(X_{N+1},Y_{N+1})$. Therefore, we can further show that
\begin{equation*}
\begin{split}
    \gamma_{loss}(h_{j,k})
    \leq&\frac{4L^2}{N-1}\sum_{l\neq i}\bbE\|\mu^{*-j}_{-i}(X_i) - \mu^{*-j}_{-i}(X_i;l\leftarrow N+1)\|_2^2\\
    &+\bbE\|\mu^{*-k}_{-i}(X_i) - \mu^{*-k}_{-i}(X_i;l\leftarrow N+1)\|_2^2\\
    &+\bbE\|\mu^{*-j,k}_{-i}(X_i) - \mu^{*-j,k}_{-i}(X_i;l\leftarrow N+1)\|_2^2\\
    &+\bbE\|\mu^*_{-i}(X_i) - \mu^*_{-i}(X_i;l\leftarrow N+1)\|_2^2,
\end{split}
\end{equation*}
where we have applied Assumption \ref{assump:err_lipschitz}.

For any subset $I\subset[N]$ that includes sample $l$, let $I^{\backslash l} = \{i\neq l:i\in I\}\cup \{N+1\}$. We then have
\begin{equation*}
\begin{split}
    &\|\mu^{*-j,k}_{-i}(X_i) - \mu^{*-j,k}_{-i}(X_i;l\leftarrow N+1)\|_2\\
    \leq &\frac{1}{\binom{N-1}{n}\binom{M-2}{m}}\sum_{\substack{I\subset[N],|I|=n\\F\subset[M],|F|=m}}\ind(i\notin I)\ind(j,k\notin F)\ind(l\in F)\|\hat{\mu}_{I,F}(X_i)-\hat{\mu}_{I^{\backslash l},F}(X_i)\|_2\\
    \leq&\frac{Dn}{N-1},
\end{split}
\end{equation*}
where the last line is due to Assumption \ref{assump:bnd_mu}.
Same bounds can also be shown for $\|\mu^{*-j}_{-i}(X_i) - \mu^{*-j}_{-i}(X_i;l\leftarrow N+1)\|_2$, $\|\mu^{*-k}_{-i}(X_i) - \mu^{*-k}_{-i}(X_i;l\leftarrow N+1)\|_2$, $\|\mu^{*}_{-i}(X_i) - \mu^{*}_{-i}(X_i;l\leftarrow N+1)\|_2.$ Therefore, we have $\gamma_{loss}(h_{j,k})\leq \frac{16L^2D^2n^2}{(N-1)^2}$. Applying Assumption \ref{assump:mpsize}, the rest of the arguments in Section A.7.3 of \cite{gan2022inference} follow directly, and we have $\varepsilon^{(3)}\overset{p}{\rightarrow}0$.
\paragraph{Consistency of variance estimate.} The consistency of variance estimate $\hat{\sigma}_{j,k}^2$ to $(\sigma^{\MP}_{j,k})^2$ can also be shown following similar proofs of Theorem 2 in \cite{gan2022inference}. Similar to the proof of Theorem \ref{thm:coverage_Split}, the moment condition in Assumption \ref{assump:split_thirdmoment} implies the uniform integrability of $h_{j,k}(X)/\sigma^{\MP}_{j,k}$, similar to the condition for the individual feature importance score in \cite{gan2022inference}. The main arguments are bounding the stability of the function $h_{j,k}$, and the difference between $h_{j,k}(X,Y;\bX_{\backslash i,:},\bY_{\backslash i})$ and $h_{j,k}(X,Y;\bX,\bY)$, which have already be shown earlier. Therefore, we can establish $\hat{\sigma}_{j,k}/\sigma^{\MP}_{j,k}\overset{p}{\rightarrow}1$, which finishes the proof. 
\end{proof}

\section{Proof of Proposition 1}
\begin{proof}
Let \( \mu^*(X) \) represent the population model given by the functional ANOVA decomposition:
\[
\mu^*(X) = g_0 + \sum_{j=1}^p g_j(X_j) + \sum_{1 \leq j < k \leq p} g_{j,k}(X_j, X_k) + \cdots,
\]
where \( g_{j,k}(X_j, X_k) \) represents the pairwise interaction between features \( j \) and \( k \). 

Define:
\[
\Delta^*_j = \Error(Y, \mu^*_{\setminus j}) - \Error(Y, \mu^*),
\]
where \( \Error(Y, \cdot) \) represents the classification error. 
Then:
\[
\iloco^*_{j,k} = \Delta^*_j + \Delta^*_k - \Delta^*_{j,k}.
\]
Using the functional ANOVA decomposition, the errors for the submodels are given as:

\textbf{(a) Error without feature \( j \):}
\[
\Error(Y, \mu^*_{\setminus j}) = \Error(Y, \mu^*(X) - g_j(X_j) - \sum_{k \neq j} g_{j,k}(X_j, X_k)).
\]

\textbf{(b) Error without feature \( k \):}
\[
\Error(Y, \mu^*_{\setminus k}) = \Error(Y, \mu^*(X) - g_k(X_k) - \sum_{j \neq k} g_{j,k}(X_j, X_k)).
\]

\textbf{(c) Error without features \( j \) and \( k \):}
\[
\Error(Y, \mu^*_{\setminus \{j,k\}}) = \Error(Y, \mu^*(X) - g_j(X_j) - g_k(X_k) - g_{j,k}(X_j, X_k)).
\]
From the definitions:
\[
\Delta^*_j = \Error(Y, \mu^*_{\setminus j}) - \Error(Y, \mu^*),
\]
\[
\Delta^*_k = \Error(Y, \mu^*_{\setminus k}) - \Error(Y, \mu^*),
\]
\[
\Delta^*_{j,k} = \Error(Y, \mu^*_{\setminus \{j,k\}}) - \Error(Y, \mu^*).
\]

Substituting the expressions for the errors into these terms:

\textbf{(a) Compute \(\Delta^*_j\):}
\[
\Delta^*_j = \Error(Y, \mu^*(X) - g_j(X_j) - \sum_{k \neq j} g_{j,k}(X_j, X_k)) - \Error(Y, \mu^*(X)).
\]

\textbf{(b) Compute \(\Delta^*_k\):}
\[
\Delta^*_k = \Error(Y, \mu^*(X) - g_k(X_k) - \sum_{j \neq k} g_{j,k}(X_j, X_k)) - \Error(Y, \mu^*(X)).
\]

\textbf{(c) Compute \(\Delta^*_{j,k}\):}
\[
\Delta^*_{j,k} = \Error(Y, \mu^*(X) - g_j(X_j) - g_k(X_k) - g_{j,k}(X_j, X_k)) - \Error(Y, \mu^*(X)).
\]
From the definition of \(\iloco^*_{j,k}\):
\[
\iloco^*_{j,k} = \Delta^*_j + \Delta^*_k - \Delta^*_{j,k}.
\]

Substituting the computed \(\Delta^*\) terms:
\[
\iloco^*_{j,k} = \Big(\Error(Y, \mu^*(X) - f_j(X_j) - \sum_{k \neq j} g_{j,k}(X_j, X_k)) - \Error(Y, \mu^*(X))\Big)
\]
\[
+ \Big(\Error(Y, \mu^*(X) - g_k(X_k) - \sum_{j \neq k} g_{j,k}(X_j, X_k)) - \Error(Y, \mu^*(X))\Big)
\]
\[
- \Big(\Error(Y, \mu^*(X) - g_j(X_j) - g_k(X_k) - g_{j,k}(X_j, X_k)) - \Error(Y, \mu^*(X))\Big).
\]

Simplify the terms:
\[
\iloco^*_{j,k} = \Error(Y, \mu^*(X) - g_j(X_j) - \sum_{k \neq j} g_{j,k}(X_j, X_k))
\]
\[
+ \Error(Y, \mu^*(X) - g_k(X_k) - \sum_{j \neq k} g_{j,k}(X_j, X_k))
\]
\[
- \Error(Y, \mu^*(X) - g_j(X_j) - g_k(X_k) - g_{j,k}(X_j, X_k))
- \Error(Y, \mu^*(X)).
\]
Observe that the terms involving \( g_j \), \( g_k \), and higher-order interactions cancel out except for \( g_{j,k}(X_j, X_k) \). After simplification:
\[
\iloco^*_{j,k,} = g_{j,k}.
\]
\end{proof}

\section{More Details for Tertiary Interaction}
To demonstrate that \(\iloco^*_{j,k,l}\) isolates the contribution of the third-order interaction, we begin with the functional ANOVA decomposition:
\[
\mu^*(X) = g_0 + \sum_{j=1}^p g_j(X_j) + \sum_{1 \leq j < k \leq p} g_{j,k}(X_j, X_k) + \sum_{1 \leq j < k < l \leq p} g_{j,k,l}(X_j, X_k, X_l) + \cdots,
\]
where \(g_{j,k,l}(X_j, X_k, X_l)\) captures the joint contribution of features \(j, k, l\).

By definition, the higher-order \(\iloco^*_{j,k,l}\) metric is given by
\[
\iloco^*_{j,k,l} = \Delta^*_j + \Delta^*_k + \Delta^*_l - \Delta^*_{j,k} - \Delta^*_{j,l} - \Delta^*_{k,l} + \Delta^*_{j,k,l}.
\]

Applying the ANOVA decomposition, the errors for the relevant submodels are:\\
Without feature \(j\):  
  \[
  \mu^*_{\setminus j} = \mu^*(X) - g_j(X_j) - \sum_{k \neq j} g_{j,k}(X_j, X_k) - \sum_{k \neq j, l \neq j} g_{j,k,l}(X_j, X_k, X_l),
  \]
and similarly for \(\mu^*_{\setminus k}\), \(\mu^*_{\setminus l}\), and their combinations.

Substituting these error terms into the definition of \(\iloco^*_{j,k,l}\) and simplifying, the contributions from lower-order terms (individual and pairwise effects) cancel out. What remains is the isolated contribution of the tertiary interaction term:
\[
\iloco^*_{j,k,l} = g_{j,k,l}(X_j, X_k, X_l).
\]

\section{More Details for MSE Error Metric}
Let the MSE Error for the baseline be defined as: 
  \[
    \text{Error}(Y,\mu^*) = (Y - \mu^*)^2.
    \]

Using the functional ANOVA decomposition, $\iloco^*_{j,k}$ can be expressed as:
\[
iLOCO^*_{j,k} = g_{j,k}^2 + 2(Y - \mu^*)g_{j,k} - 2\left(f_jf_k + \left(\sum_{\ell \neq k} g_{j\ell}\right)\left(\sum_{\ell \neq j} g_{\ell k}\right)\right),
\]
Using the functional ANOVA decomposition of \( \mu^*(X) \), we define the squared error for each reduced model:
\begin{itemize}

   \item \textbf{Model excluding feature \( j \):}
    \[
    \text{Error}(Y, \mu^*_{\setminus j}) = \left(Y - \mu^* + g_j + g_{j,k} + \sum_{\ell \neq k} g_{j\ell}\right)^2.
    \]

    \item \textbf{Model excluding feature \( k \):}
    \[
    \text{Error}(Y, \mu^*_{\setminus k}) = \left(Y - \mu^* + g_k + g_{j,k} + \sum_{\ell \neq j} g_{\ell k}\right)^2.
    \]

    \item \textbf{Model excluding both \( j \) and \( k \):}
    \[
    \text{Error}(Y, \mu^*_{\setminus \{j,k\}}) = \left(Y - \mu^* + g_j + f_k + g_{j,k} + \sum_{\ell \neq k} g_{j \ell} + \sum_{\ell \neq j} g_{\ell k}\right)^2.
    \]
\end{itemize}

Substitute the squared error terms into the definition of \(\iloco^*_{j,k} \):
\[
\iloco^*_{j,k} =  
\left(Y - \mu^* + g_j + g_{j,k} + \sum_{\ell \neq k} g_{j\ell}\right)^2
+ \left(Y - \mu^*+ g_k + g_{j,k} + \sum_{\ell \neq j} g_{\ell k}\right)^2
\]
\[
- \left(Y - \mu^* + g_j + g_k + g_{j,k} + \sum_{\ell \neq k} g_{j\ell} + \sum_{\ell \neq j} g_{\ell k}\right)^2
- \left(Y - \mu^*\right)^2 .
\]
Expand each squared term:

\begin{itemize}
    \item For \( \left(Y - \mu^* + g_j + g_{j,k} + \sum_{\ell \neq k} g_{j\ell}\right)^2 \), expand into:
    \[
    (Y - \mu^*)^2 + g_j^2 + g_{j,k}^2 + \left(\sum_{\ell \neq k} g_{j\ell}\right)^2 + 2(Y - \mu^*)g_j + 2(Y - \mu^*)g_{j,k} + \text{cross terms}.
    \]
    \item Similarly expand the other terms.
\end{itemize}

After expanding and combining terms, cross terms cancel, leaving:
\[
\iloco^*_{j,k} = g_{j,k}^2 + 2(Y - \mu^*)g_{j,k} - 2\left(g_jg_k + \left(\sum_{\ell \neq k} g_{j\ell}\right)\left(\sum_{\ell \neq j} g_{\ell k}\right)\right).
\]


\section{Additional Simulation Studies}
\begin{figure*}[h!]
    \centering
    \includegraphics[width =0.85\textwidth]{figures/nonlin_reg_comp.pdf}
    \caption{Success probability of detecting feature pair $(1,2)$ across SNR levels for KRBF, RF, and MLP regressors on nonlinear regression simulations (i), (ii), and (iii); the first feature pair has signal in scenarios (i) and (ii) whereas it is null in scenario (iii).  Success probability is defined as the proportion of times the $(1,2)$ feature pair obtains the largest metric among all feature pairs. \iloco-MP and \iloco-Split consistently outperform existing approaches at detecting feature pair $(1,2)$ in the first two scenarios whereas \iloco-MP exhibits superior specificity in the null scenario (iii).}
    \label{fig:comp_rankings}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width =0.85\textwidth]{figures/lin_class_comp.pdf}
    \caption{Success probability of detecting feature pair $(1,2)$ across SNR levels for KRBF, RF, and MLP classifiers on linear classification simulations (i), (ii), and (iii).}
    \label{fig:comp_rankings}
\end{figure*}
\begin{figure*}[h!]
    \centering
    \includegraphics[width =0.85\textwidth]{figures/lin_reg_comp.pdf}
    \caption{Success probability of detecting feature pair $(1,2)$ across SNR levels for KRBF, RF, and MLP regressors on linear regression simulations (i), (ii), and (iii)}
    \label{fig:comp_rankings}
\end{figure*}

In the correlated simulations, shown in Figure~\ref{fig:corr_rankings}, we evaluate our method on simulation (i), but we use an autoregressive design with $\boldsymbol{\Sigma}^{-1}_{j,j+1}=0.9$ instead of $\boldsymbol{\Sigma} = \mathbf{I}$ as in the uncorrelated $M$ simulations. 
\begin{figure}[h!]
    \centering
    \includegraphics[width =0.85\textwidth]{figures/corr_sim.pdf}
    \caption{Success probability of detecting feature pair $(1,2)$ across SNR levels for KRBF, RF, and MLP regressors on the correlated simulation.}
    \label{fig:corr_rankings}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width =0.75\textwidth]{figures/cov_lin_reg_2.pdf}
    \caption{Coverage of 90\% confidence intervals for $snr = 2$ feature pair in synthetic regression simulation (i) using KRBF, MLP, and RF as the base estimators. \iloco-MP exhibits slight over-coverage whereas \iloco-Split has valid coverage greater 0.9 for sufficiently large sample size; this validates of inferential theory.}
    \label{fig:coverage}
\end{figure}



\begin{figure}[h!]
    \centering
    \includegraphics[width =0.75\textwidth]{figures/mp_size_comp.pdf}
    \caption{Comparison of minipatch sizes for KRBF regressor on simulation (i). \iloco-MP detects feature pair (0,1) sooner when $m= 50\%M$ and $n = 50\%N$ than when $m= 10\%M$ and $n = 10N\%$, but the smaller minipatch size still consistently detects the feature pair when SNR = 5. This highlights the tradeoff between computational efficiency and accuracy and validates our use of $m= 20\%M$ and $n = 20\%N$ in our simulations.} 
    \label{fig:coverage}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width =0.75\textwidth]{figures/raw_iloco.pdf}
    \caption{\iloco-MP metric for KRBF, RF, and MLP regressors evaluated on simulation (i). } 
    \label{fig:coverage}
\end{figure}



\end{refsection}
\end{document}