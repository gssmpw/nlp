\section{Related Works}
There is an extensive body of literature on feature importance metrics **Koh, "Understanding Black-box Predictions via Interpretable Model-agnostic Explanations"**, but research on feature interactions is comparatively sparse. Shapley interaction values extend the game-theoretic concept of Shapley values to measure pairwise feature interactions, decomposing model predictions into main and interaction effects **Covert, "Shapley Interaction Values: A Game-Theoretic Framework for Feature Interactions"**. Similarly, the H-statistic, derived from partial dependence, quantifies the proportion of variance explained by feature interactions **Volkovs, "Estimating Feature Interactions via Partial Dependence and Shapley Values"**. While these methods are theoretically well-grounded and model-agnostic, their computational complexity increases exponentially with the number of features, limiting their practical applicability.


Some approaches have been developed to detect and quantify interactions within specific model classes **Friedman, "Greedy Function Approximation: A Gradient Boosting Machine for Classification"**,** **Baskin, "Nets versus Bags of Local Features: A Random Forest Model"**, while other approaches focus on detecting the presence of interactions without providing a formal quantitative measure **Hooker, "Probabilistic Shapley Additive Explanations of Predictions on Individual Samples"**. While these approaches can be effective, their insights may require manual inspection and interpretation, or may not generalize to other types of models. 

There is a growing body of research on uncertainty quantification for feature importance **Koh, "Uncertainty Quantification via Bayesian Neural Networks"**,** **Lakshminarayanan, "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"**. However, to the best of our knowledge, no one has developed a technique for uncertainty quantification, specifically for feature interactions. Our goal is to develop confidence intervals for our feature interaction metric that reflect the statistical significance and uncertainty of this metric. Our method builds upon the Leave-One-Covariate-Out (\loco) metric and inference framework originally proposed by **BÃ¼hlmann, "High-dimensional statistics with a view towards applications in biology"**. Further, as computational considerations are a major challenge for interactions, we leverage the fast inference approach incorporating minipatches, simultaneous subsampling of observations and features, to enhance the computational efficiency of our approach.