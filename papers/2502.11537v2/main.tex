%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage[hyphens,spaces,obeyspaces]{url}
\usepackage[breaklinks]{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xcolor}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}



%%%% OUR MACROS:
\usepackage{macros}
\usepackage{subcaption}

\newcommand{\kaixin}[1]{\textcolor{blue}{\textbf{Kaixin:} #1}}
\newcommand{\leor}[1]{\textcolor{red}{\textbf{Leor:} #1}}
\newcommand{\uri}[1]{\textcolor{green}{\textbf{Uri:} #1}}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Modular World Model over Streams of Tokens}
% \icmltitlerunning{A Modular World Model}

\begin{document}

\twocolumn[
\icmltitle{\textcolor[HTML]{67000c}{M\textsuperscript{3}}: A \textcolor[HTML]{67000c}{M}odular \rotatebox[origin=c]{180}{\textcolor[HTML]{67000c}{M}}orld \textcolor[HTML]{67000c}{M}odel over Streams of Tokens}
% \icmltitle{M3 A Modular World Model over Streams of Tokens}
% \icmltitle{\textsf{M\textsuperscript{3}}: \textsf{M}odular \rotatebox[origin=c]{180}{\textsf{M}}orld \textsf{M}odel over Streams of Tokens}
% color version


% Learning Dynamics and Control in a World of Tokens
% Learning Dynamics on a stream of Tokens is universally effective world models 
% Effectively learning to model diverse worlds over streams of tokens
% A sequence model over a stream of tokens is a universally effective world model
% Modeling Diverse Worlds over Streams of Tokens
% learning control within (learned) models of diverse worlds over streams of tokens
% learning dynamics and control of diverse worlds over streams of tokens
% Online? Learning models of diverse worlds over streams of tokens
% Online Learning Simulators of Diverse Worlds over Streams of Tokens
% online learning control within simulators of diverse worlds over streams of tokens
% online learning to control and simulate diverse worlds over streams of tokens.
% online learning to model and control diverse worlds over streams of tokens
% Sample efficient control through learned simulators of diverse worlds over streams of tokens 
% Modeling Diverse Worlds over Streams of Tokens for (online?) Sample Efficient Control


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lior Cohen}{technion}
\icmlauthor{Kaixin Wang}{equal,msr}
\icmlauthor{Bingyi Kang}{bd}
\icmlauthor{Uri Gadot}{technion}
\icmlauthor{Shie Mannor}{equal,technion}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{technion}{Technion â€“ Israel Institute of Technology}
\icmlaffiliation{bd}{ByteDance Seed}
\icmlaffiliation{msr}{Microsoft Research}
% \icmlaffiliation{tbd}{TBD}

\icmlcorrespondingauthor{Lior Cohen}{liorcohen5@campus.technion.ac.il}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML, Deep Learning, Deep RL, World Models}

\vskip 0.3in
]


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{\icmlEqualAdvising}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% Current state:
% Token based methods were proposed.
% In TBWMs, the optimization of repr., dynamics, and control is separated.
% separate optimization / modularity facilitates scaling (evidence exists in other ML areas)
% Promising performance. (on Atari)

% Recent advances in model-based reinforcement learning suggest that token-driven architectures are effective for world modeling.
% Recently, token-based world models demonstrated promising performance on visual environments.
% These methods optimize their representations, dynamics, and control separately from each other.

% Gap / Problem:
% Limited to visual envs and discrete actions.
% However, prior work is limited to visual environments and discrete actions.
% Moreover, existing evidence covers only the Atari benchmark, %... there is still uncertainty about the possible effectiveness of these methods on other settings.
% Moreover, current methods are not trivially applicable to broader settings.
% Proposed Solution:
% Hence, it is unclear whether these methods can be effective in other types of environments, and how to effectively extend them.
% In this paper, we propose a 
% Evidence to support it:

%Token-based world models have emerged as a promising approach for modeling visual environments, demonstrating strong performance through their ability to separately optimize representations, dynamics, and control. 
% However, existing research has largely been confined to environments with discrete action spaces, particularly the Atari benchmark. 
% This limitation raises important questions about the generalizability of these methods to diverse environmental contexts and the technical challenges of extending them to continuous action spaces and more complex domains.

% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.


% While these methods have demonstrated strong performance, current literature has primarily evaluated them on visual environments with discrete action spaces, particularly Atari games, leaving their broader applicability uncertain. 

% The key challenge lies in determining whether TBWMs can effectively handle diverse modalities beyond images and discrete actions, and in identifying the optimal architectural design principles for such generalization. 
% The challenge lies in identifying effective design principles for generalizing TBWMs to handle diverse modalities. % beyond images and discrete actions.
% The challenge is determining if TBWMs can handle diverse modalities beyond images and discrete actions, and identifying effective design principles for such generalization.
% In this paper, we introduce \AlgName{}, a \textbf{m}odular \textbf{w}orld \textbf{m}odel that can process various observation modalities given a set of tokenization modules and effectively handles continuous and discrete action spaces. 

% Drawing inspiration from Rainbow, our approach integrates several architectural improvements from existing literature to enhance agent performance. 
% Our approach integrates several improvements from existing literature to enhance agent performance. 

% Extensive empirical evaluation across three diverse benchmarks - Atari 100K, DeepMind Control, and Craftax - demonstrates \AlgName{}'s exceptional performance, becoming the first planning-free world model method to achieve a human-level median score on Atari 100K, exhibiting superhuman performance on 13 games - surpassing all existing approaches.
% Evaluated on Atari 100K (image), DeepMind Control (continuous), and Craftax (symbolic) benchmarks, \AlgName{} achieves unprecedented results: it is the first planning-free world model to reach a human-level median score on Atari 100K, with superhuman performance on 13 games.

% \todo{Include motivation for modular design - divide and conquer, scaling, easier to study, etc.}
% $\text{M}^{\text{3}}$
Token-based world models emerged as a promising modular framework, modeling dynamics over token streams while optimizing tokenization separately.
While successful in visual environments with discrete actions (e.g., Atari games), their broader applicability remains uncertain.
In this paper, we introduce $\text{M}^{\text{3}}$, a \textbf{m}odular \textbf{w}orld \textbf{m}odel that extends this framework, enabling flexible combinations of observation and action modalities through independent modality-specific components.
$\text{M}^{\text{3}}$ integrates several improvements from existing literature to enhance agent performance. 
Through extensive empirical evaluation across diverse benchmarks, $\text{M}^{\text{3}}$ achieves state-of-the-art sample efficiency for planning-free world models.
Notably, among these methods, it is the first to reach a human-level median score on Atari 100K, with superhuman performance on 13 games.
Our code and model weights are publicly available at \url{https://github.com/leor-c/M3}.
% We \href{https://github.com/leor-c/M3}{open-source our code and weights}.
\end{abstract}



\section{Introduction}

% \paragraph{World Models}
% % david ha, M-V-C structure, ...
% World models are deep learning methods for learning dynamics models of target environments.
% In the context of reinforcement learning (RL), world models refer to methods that learn control exclusively from simulated experience generated by a world model in a process called imagination \cite{ha2018worldmodels}. 
% The world model is trained alongside the controller from environment interaction data collected by the controller and stored in a replay buffer.


% Perhaps it is also a good idea to discuss the complexity of current world model methods in terms of code bases.
% most code bases of popular alg.s contain 1000s of lines and extending these alg.s to new modalities requires both expertise and in some cases significant efforts and time.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/intro_summary.pdf}
    \vspace{-20pt}
    \caption{Results overview. \AlgName{} exhibits state-of-the-art sample-efficiency performance for planning-free methods across all three benchmarks. $\dagger$ \cite{robine2023transformer}, $\ddagger$ \cite{dedieu2025TWM2}.}
    \label{fig:first-page-results}
\end{figure}


% World models aim to enable an agent to simulate or predict the outcomes of its actions in a learned representation of the environment, rather than relying solely on direct interactions with the real environment. This can improve sample efficiency and enable planning in complex environments.


% Opening
Modeling the dynamics of the world has been a long-standing and compelling topic in reinforcement learning, with foundational ideas proposed over 30 years ago~\cite{Sutton1991dyna,Schmidhuber1991Curious, Schmidhuber2010}.
It has been leveraged to enhance planning performance~\cite{schrittwieser2019mastering}, produce fictitious data for policy training~\cite{hafner2023mastering}, or simulate interactive experiences~\cite{genie2, oasis2024, agarwal2025cosmos}.
% In recent years, significant progress has been made in this field, enhancing modeling accuracy and broadening its applications~\cite{ha2018worldmodels,hafner2023mastering,micheli2022transformers,hu2023gaia1,hansen2024tdmpc}.

% In this study, we focus on \textit{token-based} world models \cite{micheli2022transformers, micheli2024efficient, cohen2024improving}, which learn the dynamics entirely within a (pre-)trained token space.
In this study, we focus on \textit{token-based} world models (TBWMs) \cite{micheli2022transformers, micheli2024efficient, cohen2024improving}, sample-efficient reinforcement learning (RL) methods that learn the dynamics entirely within a learned token space.
% Token-based world models offer a clear modularity between the representation and the dynamics.
TBWMs offer a clear modularity between the representation and the dynamics by separating their optimization.
Such modularity provides a clean and efficient way to unify different observation modalities within a shared token space, offloading representation learning to the tokenizer design step.
More generally, modular systems are easier to scale, develop, evaluate, and deploy, as individual modules can be treated independently and are easier to master through divide and conquer.

While TBWMs hold promise as a general framework for handling different modalities, existing work primarily focuses on image observations and discrete actions, such as Atari games.
It remains unclear whether token-based world models are truly effective for handling diverse modalities and how to design the model to achieve it.
This limits the broader adoption of token-based models.


% Action
% To address these limitations, we propose several effective model designs, leading to a new modular world model, termed M\textsuperscript{3}.
To address these limitations, we propose a \textbf{m}odular \textbf{w}orld \textbf{m}odel, termed \AlgName{}, which implements a modular framework for general-purpose token-based world models.
Specifically, \AlgName{} maintains a set of independent modality-specific tokenizers, embedding tables, and prediction heads to handle various modalities.
% Specifically, \AlgName{} preprocess and postprocess the representation of each modality independently using sets of modality-specific tokenizers, embedding tables, and prediction heads.
This modular design allows \AlgName{} to process any combination of observation and action modalities.

In addition, \AlgName{} combines several recent improvements from the literature.
Specifically, the control policy is intrinsically motivated to reduce the epistemic uncertainty of the world model \cite{pmlr-v97-shyam19a, sekar20aPlan2Explore}, a world model replay buffer further improves sample efficiency~\cite{kauvar2023curiousReplay}, and continuous values are predicted via classification models \cite{farebrother2024stop, hafner2023mastering}.
% Modularity can significantly accellerate world model progress, as it simplifies the study and the devepment of the algorithm by breaking this complex system into a set of smaller components, which are easier to conquer.
% Modular systems are much easier to scale, develop, evaluate, and deploy, as individual modules can be treated independently.
% In the context of deep learning, it is also easier to stabilize the optimization of modular networks.
% To achieve true modularity, an interface must be defined between the system components.
% We 
% We highlight some advantages of modularity:
% \begin{itemize}
%     \item Scalability: Easier to scale as individual modules can be updated, replaced, or scaled independently.
%     \item Flexibility: Modules can be developed, tested, and deployed independently, minimizing interference.
%     \item Maintainability: Easier to maintain and debug because each module is a smaller, self-contained unit.
%     % \item Reusability: Modules can often be reused in other projects, saving development time and resources.
%     \item Resilience: Failure in one module does not necessarily bring down the entire system.
%     % \item Diversity: Different modules can use different technologies, languages, or frameworks best suited for their specific tasks.
% \end{itemize}
% Specifically, we xxx
% Notably, xxx (Highlight the most novel or important aspect of our method.)

% Results
% Describe how we evaluate our method and show the exp results.
To validate the effectiveness of our method, we conducted extensive empirical evaluations across three diverse benchmarks, ranging from the visual Atari 100K, to the continuous proprioception tasks of the DeepMind Control Suite, to Craftax, which combines symbolic 2D grid maps with continuous state features.
There, \AlgName{} achieves state-of-the-art sample-efficiency for planning-free world models. 
% Our ablations show the importance of each new component.
Ablation studies further show the contribution of each component.
% Extensive empirical evaluation across three diverse benchmarks - Atari 100K, DeepMind Control, and Craftax

% Resolution
% Summary of contributions in bullet point and write 1-2 sentences about what our method brings to the field and community (implications)
Summary of contributions:
\begin{itemize}
    \item We propose \AlgName{}, a widely applicable token-based world model agent that follows a modular framework. 
    \item We conduct extensive empirical evaluation, providing strong evidence for the effectiveness of TBWMs as general methods. There, \AlgName{} achieves state-of-the-art (planning-free) performance across all benchmarks.
    \item To facilitate future research and broader adoption of TBWMs, we open-source our code and trained models.
\end{itemize}










\section{Method}
\label{sec:method}
% This section presents \AlgName{} in detail.
% A short overview of the algorithm's components is given, followed by a detailed description of each component.
% We provide an overview of the core components of our system.
% The full details of the algorithms and architectures are provided in Appendix \ref{sec:wm-additional-details}.

\paragraph{Notations}
We consider the Partially Observable Markov Decision Process (POMDP) setting.
% A POMDP is a tuple $(\statesSet, \actionsSet, \transKernel, \initialStatesDist, \rewardFn, \obsSet, \obsTransFn, \discountF)$ where $\statesSet$ is a set of hidden states, $\actionsSet$ is a set of actions, $\transKernel: \statesSet \times \actionsSet \rightarrow \simplexFn(\statesSet)$ is a transition function that models the hidden dynamics, $\initialStatesDist$ is an initial distribution of hidden states, $\rewardFn: \statesSet \times \actionsSet \rightarrow \mathbb{R}$ is a reward function, $\obsSet$ is a set of observations, $\obsTransFn: \statesSet \times \actionsSet \rightarrow \simplexFn(\obsSet)$ models observation emission probabilities, and $\discountF \in [0, 1)$ is a discount factor, where $\simplexFn(A) = \{[0, 1]^{|A|} | \sum_{a \in A} p(a) = 1 \}$ denotes the simplex on the set $A$.
% At each step $t$, the agent observes $\obs_{t}$ and picks an action $\action_{t}$. 
% Then, the environment evolves its hidden states $s_{t+1} \sim \transKernel(s_{t+1} | s_{t}, \action_{t})$ and emits the next observation $\obs_{t+1} \sim \obsTransFn(s_{t+1}, \action_{t})$ and reward $\reward_{t} = \rewardFn(s_{t}, \action_{t})$.
% The process repeats indefinitely or until a positive termination signal $\doneSgnl_{t} \in \{0, 1\}$ is obtained in an episodic setting.
% The agent's objective is to maximize its expected return $\E [\sum_{t=0}^{\infty} \discountF^{t} \reward_{t+1}]$.
However, since in practice the agent has no knowledge about the hidden state space, consider the following state-agnostic formulation.
Let $\obsSet, \actionsSet$ be the sets of observations and actions,  respectively.
At every step $t$, the agent observes $\obs_{t} \in \obsSet$ and picks an action $\action_{t} \in \actionsSet$.
From the agent's perspective, the environment evolves according to $\obs_{t+1}, \reward_{t}, \doneSgnl_{t} \sim p(\obs_{t+1}, \reward_{r}, \doneSgnl_{t} | \obs_{\leq t}, \action_{\leq t})$, where $\reward_{t}, \doneSgnl_{t}$ are the observed reward and termination signals, respectively.
% observations $\obs_{t} \in\obsSet$, actions $\action_{t} \in \actionsSet$, scalar rewards $\reward_{t} \in \mathbb{R}$, episodic termination signal $\doneSgnl_{t} \in \{0, 1\}$, dynamics $\obs_{t+1}, \reward_{t}, \doneSgnl_{t} \sim p(\obs_{t+1}, \reward_{r}, \doneSgnl_{t} | \obs_{\leq t}, \action_{\leq t})$ and discount factor $\discountF$.
The process repeats until a positive termination signal $\doneSgnl_{t} \in \{0, 1\}$ is obtained.
The agent's objective is to maximize its expected return $\E [\sum_{t=0}^{\infty} \discountF^{t} \reward_{t+1}]$ where $\discountF \in [0, 1]$ is a discount factor.

For multi-modal observations, let $\obs_{t} = \{ \obs_{t}^{(i)} \}_{i=1}^{|\modalitySet|}$ where $\modalitySet$ is the set of environment modalities and $\obs_{t}^{(i)}$ denotes the features of modality $\modalitySet_{i}$.





\paragraph{Overview}
\AlgName{} builds on REM \cite{cohen2024improving}.
The agent comprises a representation module $\Tokenizer$, a world model $\WM$, a controller $\Controller$, and a replay buffer.
To facilitate a modular design, following REM, each module is optimized separately.
The training process of the agent involves a repeated cycle of four steps: data collection, representation learning ($\Tokenizer$), world model learning ($\WM$), and control learning in imagination ($\Controller$).
% \todo{add figure or add details}
% An illustration of this process is provided in Figure \ref{fig:overview} \todo{add figure}.








% POP computes observations in parallel by maintaining a sequence of prediction tokens U and computing Y_{t+1} = f(X_1, ..., X_t, U), z_{z+1} ~ Y_{t+1}, a_{t+1} ~ policy(...), repeat
% This computation is performed efficiently by intelligently using the recurrent states of $f$




% \paragraph{Overview}
% This work is based on REM \cite{cohen2024improving}.
% The agent comprises a representation module $\Tokenizer$, a world model $\WM$, a controller $\Controller$, and a replay buffer.
% To facilitate a modular design, following REM, each module is optimized separately.

% As a reinforcement learning (RL) world model method \cite{ha2018worldmodels}, the training of \AlgName{} can be outlined as a repeated cycle of the following steps:
% The controller collects a small amount of experience by interacting with the environment and stores it in the replay buffer.
% Next, $\Tokenizer$ and $\WM$ are trained from uniformly sampled replay buffer data.
% Lastly, $\Controller$ is trained in a process called imagination.
% There, the controller interacts with $\WM$ to generate simulated episode segments starting from random replay buffer context samples.
% The generated data is then used to update the parameters of $\Controller$.
% An illustration of this is provided in Figure X \todo{add figure}.




% \subsection{Framework}
% To generalize the algorithm to support general observation and action spaces, we propose a modular framework.
% \todo{add overview of the structure - repr. module, seq. model, controller, ...}



% \paragraph{Modularity}
% A modular system has many advantages over a monolithic one when dealing with highly complex systems. \todo{describe advantages}
% Here, we propose to separate the optimization of the main components of world model agents, i.e. the representation module, the sequence model, and the controller.
% This separation requires establishing an \emph{interface} between these components.



\subsection{The Representation Module $\Tokenizer$}
$\Tokenizer$ is responsible for encoding and decoding raw observations and actions.
% \paragraph{Modular Design}
% $\Tokenizer$ comprises a set of encoder-decoder pairs, one for each input modality.
% Each encoder-decoder pair is an independent unit where the only requirement is that its encoded representation follows a fixed-length token sequence structure.
% $\Tokenizer$ is a modular tokenization system with encoder-decoder pairs for different input modalities. 
It is a modular tokenization system with encoder-decoder pairs for different input modalities. 
Encoders produce fixed-length token sequences, creating a common interface that enables combining tokens from various sources into a unified representation.
% $\Tokenizer$ consists of encoder-decoder pairs for different input modalities. Each pair operates independently, generating a fixed-length token sequence representation.
% This requirement establishes a common interface that allows to combine the tokens produced by any set of encoder-decoder pairs into a unified representation.
% Specifically, the token sequence requirement provides the common interface, while the fixed-length requirement simplifies the implementation.
% The resulting set of token sequences is unified into a common sequence representation only after embedding, as described in Section \ref{sec:wm-embedding}. 
After embedding, these token sequences are concatenated into a single representation, as described in Section \ref{sec:wm-embedding}.
% To combine the latents, $\Tokenizer$ defines an order among the modalities and concatenates their corresponding token sequences along the temporal axis according to that order. 
Note that encoder-decoder pairs need not be learning-based methods.
Although learned pairs are optimized independently.
This design enables $\Tokenizer$ to deal with any combination of input modalities, provided the respective encoder-decoder pairs.









\paragraph{Tokenization}
$\Tokenizer$ transforms raw observations $\obs$ to sets of fixed-length integer token sequences $\tokens = \{ \tokens^{(i)} \}_{i=1}^{|\modalitySet|}$ by applying the encoder of each modality $\tokens^{(i)} = \encoder_{i}(\obs^{(i)})$.
% and concatenating the resulting token sequences $\tokens = \tokens^{(1)}, \ldots, \tokens^{(|\modalitySet|)}$.
Actions $\action$ are tokenized using the encoder-decoder pair of the related modality to produce $\actionTokens$. 
The respective decoders reconstruct observations from their tokens: $\hat{\obs}^{(i)} = \decoder_i(\tokens^{(i)})$. 

\AlgName{} natively supports four modalities: images, continuous vectors, categorical variables, and image-like multi-channel grids of categorical variables, referred to as "2D categoricals".
More formally, 2D categoricals are elements of $([k_1]\times[k_2]\times\ldots\times [k_C])^{m \times n}$ where $k_1, \ldots , k_C$ are per channel vocabulary sizes, $C$ is the number of channels, $m, n$ are spatial dimensions, and $[k] = \{ 1,\ldots, k \}$.

Following REM, we use a VQ-VAE \cite{esser2021tamingVQGAN, vanDenOord2017vqvae} for image observations. 
% Details are elaborated in Appendix \ref{sec:appendix-vqvae}.
% For continuous vectors, we employ a similar tokenization approach to that of \cite{reed2022a}, where each feature is quantized to produce a token.
For the tokenization of continuous vectors, each feature is quantized to produce a token, as in \cite{reed2022a}.
Unbounded vectors are first transformed using the symlog function \cite{hafner2023mastering}, defined as $\symlog(x) = \sign(x)\ln(1 + |x|)$, which compresses the magnitude of large absolute values.
% Lastly, since categorical inputs are naturally in token form, no special tokenization is required.
Lastly, while no special tokenization is required for categorical inputs, 2D categoricals are flattened along the spatial dimensions to form a sequence of categorical vectors.
% However, 2D categorical inputs are flattened along the spatial dimensions to form a sequence of categorical vectors.
The embedding of each token vector is obtained by averaging the embeddings of its entries.
% \todo{move details about embedding tables to Appendix}










\subsection{The World Model $\WM$} 
\label{sec:method-wm}
The purpose of $\WM$ is to learn a model of the environment's dynamics.
% Here, $\WM$ is provided with trajectory segments in the form of token embeddings sequences $\tknEmbs_{1}, \tknEmbs_{2}, \ldots, \tknEmbs_{T}$, where $\tknEmbs_{t}= (\tknEmbs_{t}^{\text{o}}, \tknEmbs_{t}^{\text{a}})$ is a block of an observation embeddings sequence $\tknEmbs_{t}^{\text{o}} = \tknEmb_{t, 1}, \ldots, \tknEmb_{t, K}$ followed by an action embeddings sequence. 
Concretely, given trajectory segments $\tokensTraj_{t} = \tokens_{1}, \actionTokens_{1}, \ldots, \tokens_{t}, \actionTokens_{t}$ in token representation, $\WM$ models the distributions of the next observation and termination signal, and the expected reward:
% As embeddings sequences are composed hierarchically, we consider the following hierarchical notation:
% \begin{align*}
%     \text{Observation-action block:}\quad & \tknEmbs_{t} = \obsBlock_{t}, \actBlock_{t}, \\ 
%     \text{Observation block:}\quad &  \obsBlock_{t} = \obsModalityBlock{1}_{t}, \ldots, \obsModalityBlock{|\modalitySet|}_{t} \\
%     \text{Modality block:}\quad &  \obsModalityBlock{i}_{t} = \tknEmb_{t, 1}^{(i)}, \ldots, \tknEmb_{t, \tokensPerObs_i}^{(i)} \\
%     \text{Action block: \quad} & \actBlock_{t} = \tknEmb_{t, 1}^{\text{a}}, \ldots, \tknEmb_{t, \tokensPerAction}^{\text{a}}
% \end{align*}
% , where $\tknEmbs_{t}= (\tknEmbs_{t}^{\text{o}}, \tknEmbs_{t}^{\text{a}})$ is a block of an observation embeddings sequence $\tknEmbs_{t}^{\text{o}} = \tknEmb_{t, 1}, \ldots, \tknEmb_{t, K}$ followed by an action embeddings sequence. 
% At each time step $t$, $\WM$ models
\begin{align}
% \label{eq:wm-transitions-distributions}
    \text{Transition:}\quad & p_{\theta}(\hat{\tokens}_{t+1} \vert \tokensTraj_{t} ), \\
    % \label{eq:wm-reward-distributions}
    \text{Reward:}\quad & \hat{\reward}_{t} = \hat{\reward}_{\theta} ( \tokensTraj_{t} ), \\
    % \label{eq:wm-termination-distributions}
    \text{Termination:}\quad & p_{\theta}(\hat{\doneSgnl}_{t} \vert \tokensTraj_{t}).
\end{align}
where $\theta$ is the parameters vector of $\WM$ and $\hat{\reward}_{\theta}(\tokensTraj_{t})$ is an estimator of $\E_{\reward_{t} \sim p(\reward_{t} \vert \tokensTraj_{t} )} [ \reward_{t} ]$.
% \todo{motivate the reward estimation as variance reduction}

\paragraph{Architecture} $\WM$ comprises a sequence model $\seqModel$ and multiple heads for the prediction of tokens of different observation modalities, rewards, termination signals, and for the estimation of model uncertainty.
Concretely, $\seqModel$ is a retentive network (RetNet) \cite{sun2023retentive} augmented with a parallel observation prediction (POP) \cite{cohen2024improving} mechanism.
All heads are implemented as multilayer perceptrons (MLP) with a single hidden layer.
We defer the details about these architectures to Appendix \ref{sec:wm-additional-details}.





%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.395\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tokenization.pdf}
        \caption{Tokenization and embedding.}
        \label{fig:tokenization-and-emb}
    \end{subfigure}
    \begin{subfigure}[t]{0.595\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obs-prediction.pdf}
        \caption{Observation prediction.}
        \label{fig:obs-pred}
    \end{subfigure}
    % \includegraphics[width=0.4\linewidth]{figures/tokenization.pdf}
    % \includegraphics[width=0.4\linewidth]{figures/obs-prediction.pdf}
    \caption{An illustration of the independent processing of modalities for an observation with two modalities.}
    \label{fig:tokenization}
\end{figure}
%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




\paragraph{Embedding}
\label{sec:wm-embedding}
$\WM$ translates token trajectories $\tokensTraj$ into sequences of $\retnetDmodel$-dimensional embeddings $\tknEmbs$ using a set of embedding (look-up) tables.
% After tokenizing $\obs_{t}$, the resulting token sequence $\tokens_{t}$ is translated to an embedding sequence $\tknEmbs_{t}$.
% by mapping the tokens of every modality $\tokens^{(i)} = \token^{(i)}_{1}, \ldots, \token^{(i)}_{K_i}$ \todo{replace $K_i$} using the corresponding embedding table $\embTable^{(i)}$.
By design, each modality is associated with a separate table.
In cases where an embedding table is not provided by the appropriate encoder-decoder pair, $\WM$ and $\Controller$ learn dedicated tables separately and independently.
As embeddings sequences are composed hierarchically, we use the following hierarchical notation:
\begin{align*}
    \text{Observation-action block:}\quad & \tknEmbs_{t} = (\obsBlock_{t}, \actBlock_{t}) \\ 
    \text{Observation block:}\quad &  \obsBlock_{t} = (\obsModalityBlock{1}_{t}, \ldots, \obsModalityBlock{|\modalitySet|}_{t}) 
    % \text{Modality block:}\quad &  \obsModalityBlock{i}_{t} = \tknEmb_{t, 1}^{(i)}, \ldots, \tknEmb_{t, \tokensPerObs_i}^{(i)}, \\
    % \text{Action block: \quad} & \actBlock_{t} = \tknEmb_{t, 1}^{\text{a}}, \ldots, \tknEmb_{t, \tokensPerAction}^{\text{a}}.
\end{align*}
where $\tokensPerObs_{i}$ denotes the number of embedding vectors in $\tknEmbs^{(i)}_{t}$.
Similarly, $\tokensPerObs=\sum_{i=1}^{|\modalitySet|}\tokensPerObs_{i}$.
% To combine the latents of each $\tokens_{t}$ into a single sequence, $\Tokenizer$ defines an order among the modalities and concatenates their corresponding token sequences along the temporal axis according to that order. 
To combine latents of each $\tokens_{t}$, $\Tokenizer$ concatenates their token sequences along the temporal axis based on a predefined modality order.
% Embedding tables associated with encoder-decoder pairs that are not being learned are learned during world model training.
% \todo{move this to the WM and controller sections?} % these are not being learned by V
We defer the full details on the embedding process to Appendix \ref{sec:wm-additional-details}.







%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/wm-fig.pdf}
    \caption{World model training and imagination. To maintain visual clarity, we omitted token embedding details, as well as optimization details of rewards and termination signals.}
    \label{fig:wm}
\end{figure*}
%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~







\paragraph{Sequence Modeling}
% describe the input & output of the sequence model
Given a sequence of observation-action blocks $\tknEmbs = \tknEmbs_{1}, \ldots, \tknEmbs_{t}$, the matching outputs $\retnetOutput_{1}, \ldots, \retnetOutput_{t}$ are computed auto-regressively as follows:
\begin{equation*}
    (\retnetState_{t}, \retnetOutput_{t}) = \seqModel(\retnetState_{t-1}, \tknEmbs_{t})
\end{equation*}
where $\retnetState_{t}$ is a recurrent state that summarizes $\tknEmbs_{\leq t}$ and $\retnetState_{0}=0$.
However, the output $\retnetOutput^{\text{u}}_{t+1}$, from which $\hat{\tokens}_{t+1}$ is predicted, is computed using the POP mechanism via another call as
\begin{equation*}
    (\cdot, \retnetOutput_{t+1}^{\text{u}}) = \seqModel(\retnetState_{t}, \tknEmbs^{\text{u}})
\end{equation*}
where $\tknEmbs^{\text{u}} \in \mathbb{R}^{\tokensPerObs \times \retnetDmodel}$ is a learned embedding sequence.
Intuitively, $\tknEmbs^{\text{u}}$ acts as a learned prior, enabling the parallel generation of multiple tokens into the future.

% Given $\tknEmbs \in \mathbb{R}^{n \times \retnetDmodel} = \tknEmb_{1}, \ldots, \tknEmb_{n}$ for some $n \in \mathbb{N}$, $\seqModel$ outputs $\retnetOutput \in \mathbb{R}^{n \times \retnetDmodel} = \retnetOutputVec_{1}, \ldots, \retnetOutputVec_{n}$ auto-regressively where $\forall 1 \leq t \leq n: \retnetOutputVec_{t} = \seqModel(\tknEmb_{1}, \ldots, \tknEmb_{t})$.
% Here, inputs consist of observation-action blocks $\tknEmbs = \tknEmbs_{1}, \ldots, \tknEmbs_{t}$. 
% The output $\retnetOutput^{\text{u}}_{t+1}$, from which $\hat{\tokens}_{t+1}$ is predicted, is computed using the POP mechanism from a recurrent state $\retnetState_{t}$ that summarizes $\tknEmbs_{\leq t}$ and from a learned embedding sequence $\tknEmbs^{\text{u}} \in \mathbb{R}^{\tokensPerObs \times \retnetDmodel}$ where $\tokensPerObs=\sum_{i=1}^{|\modalitySet|}\tokensPerObs_{i}$.
% Intuitively, $\tknEmbs^{\text{u}}$ serves as a learned prior, allowing the parallel generation of multiple tokens into the future.
% \todo{ add some details about the way obs are predicted with POP? (U + $Y^u$)}


% To predict the distribution $p_{\theta}(\hat{\tokens}_{t+1} | \retnetOutput_{t+1}^{\text{u}})$ over the next observation tokens $\hat{\tokens}_{t+1}$, the distribution $p_{\theta}(\hat{\token} | \retnetOutputVec)$ of each token $\hat{\token}$ in $\hat{\tokens}^{(i)}_{t+1}$ of each modality $\modalitySet_i$ is modeled using a modality-specific prediction head implemented as a multilayer perceptron (MLP) with a single hidden layer and an output size equal to the vocabulary size of $\encoder_i$.
To model $p_{\theta}(\hat{\tokens}_{t+1} | \retnetOutput_{t+1}^{\text{u}})$, the distributions $p_{\theta}(\hat{\token} | \retnetOutputVec)$ of each token $\hat{\token}$ of $\hat{\tokens}^{(i)}_{t+1}$ of each modality $\modalitySet_i$ are modeled using modality-specific prediction heads implemented as MLPs with a single hidden layer and an output size equal to the vocabulary size of $\encoder_i$ (Figure \ref{fig:obs-pred}).
For 2D categoricals, $C$ heads are used to predict the $C$ tokens from each $\retnetOutputVec$.

Similarly, rewards and termination signals are predicted by additional prediction heads as $\hat{\reward}_{t} = \rewardPredHead(\retnetOutputVec), \hat{\doneSgnl}_{t} \sim p_{\theta}( \hat{\doneSgnl}_{t} | \retnetOutputVec)$, slightly abusing notations, where $\retnetOutputVec$ is the last vector of $\retnetOutput^{\text{u}}_{t+1}$.
An illustration is provided in Figure \ref{fig:wm}.
% \todo{describe $Y^u$}
% \todo{describe current alg or ideal? ($Y^u$ or $Y^o$)?}
% \todo{add details about HL-Gauss reward prediction?}






% \paragraph{Imagination}

\paragraph{Reducing Epistemic Uncertainty via Intrinsic Motivation}
% P idea: motivation for reducing epistemic uncertainty (accurate model, efficient exploration). an ensemble + JSD is a good Epi.Unc. estimator. it is equivalent to the utility / IG
% The world model $\WM$ is arguably the most critical component in the system.
% The quality of any controller in a world model system depends on and is limited by the quality of the world model.
% In a deep learning based system, the world model's quality highly depends on the quality of its data.
% Specifically, to accurately model dynamics, sufficient data must be collected, covering as much of the environment's dynamics as possible.
% However, in an online RL setting, this requires the controller to explore its environment efficiently.
% To do so, the controller should be guided towards uncharted or poorly covered regions of the dynamics.
% One prominent approach to achieve this is to estimate the world model's epistemic uncertainty and encourage the controller to collect transitions with high epistemic uncertainty estimates \cite{pmlr-v97-shyam19a, sekar20aPlan2Explore}.
The world model $\WM$ serves as the cornerstone of the entire system. 
Any controller operating within a world model framework can only perform as well as the underlying world model allows, making its quality a fundamental limiting factor. 
In deep learning methods, the model's performance depends heavily on the quality of its training data. 
Accurate dynamics modeling requires comprehensive data collection that captures the full spectrum of possible environmental behaviors. 
This presents a particular challenge in online RL, where the controller must systematically and efficiently explore its environment. 
Success depends on intelligently guiding the controller toward unexplored or undersampled regions of the dynamics space. 
An effective approach to this challenge involves estimating the world model's epistemic uncertainty and directing the controller to gather data from regions where this uncertainty is highest \cite{Schmidhuber2010, pmlr-v97-shyam19a, sekar20aPlan2Explore}. 
% \todo{add fundamental references?}

% We follow existing work on model-based exploration that proposes to estimate the epistemic uncertainty of the model to guide the controller to explore uncharted regions of the environment \cite{pmlr-v97-shyam19a, sekar20aPlan2Explore} \todo{cite relevant works}.
% JSD is the utility for exploration

% P idea: how we implement this in our work:
% How epistemic uncertainty is estimated:
% To estimate epistemic uncertainty, we maintain an ensemble of next observation prediction heads with parameters $\{ \phi_i \}_{i=1}^{\ensembleSize}$ \cite{sekar20aPlan2Explore, LakshminarayananNIPS2017uncertaintyEstDeepEns} and use the Jensen-Shannon divergence (JSD) as a measure of disagreement between the ensemble's distributions \cite{pmlr-v97-shyam19a}.
% Formally, given a set of probability distributions $P_1, \ldots, P_n$,
% \begin{equation*}
%     \JSD ( P_1, \ldots, P_n ) = \entropyFn(\frac{1}{n} \sum_{i=1}^{n} P_i ) - \frac{1}{n} \sum_{i=1}^{n} \entropyFn(P_i)
% \end{equation*}
% where $\entropyFn(P)$ is the Shannon entropy.
% Here, the ensemble predicts a set of distributions $\{ \obsHead_{\phi_i}(\hat{\token} | \retnetOutputVec) \}_{i=1}^{\ensembleSize}$ for every $\retnetOutputVec$ and appropriate token $\hat{\token}$ of the predicted observation from which a JSD value is computed.
% Since the prediction of each observation $\tokens_{t}$ involves multiple tokens, the per-token JSD values are averaged to produce a single aggregated value.
% % Since next observations prediction involves only discrete distributions, the computation of the JSD is simple and efficient.
% Note that since all JSD computations involve only discrete distributions, $\disagreement_t$ is bounded by $\frac{1}{|\tokens_{t}|} \sum_{\token \in \tokens_{t}} \log(\vocabSize(z))$ where $|\tokens_{t}|$ is the total number of tokens in $\tokens_{t}$.
% In addition, for discrete distributions the computation of $\entropyFn$ is simple and efficient.
% Each head in the ensemble is identical to the next observation prediction head of $\WM$.
% The predictors differ in their initializations and in the order at which they observe the data.
% Each input batch is divided equally between the ensemble predictors.
% In practice, we found the additional computational overhead induced by this ensemble to be negligible.

Our approach estimates epistemic uncertainty using an ensemble of $\ensembleSize=4$ next observation prediction heads $\left\{ p_{\phi_i}(\hat{\tokens} | \stopGrad( \retnetOutput^{\text{u}} ) ) \right\}_{i=1}^{\ensembleSize}$ with parameters $\{ \phi_i \}_{i=1}^{\ensembleSize}$ \cite{sekar20aPlan2Explore, LakshminarayananNIPS2017uncertaintyEstDeepEns} where $\stopGrad(\cdot)$ is the stop gradient operator. 
To quantify disagreement between the ensemble's distributions, we employ the Jensen-Shannon divergence (JSD) \cite{pmlr-v97-shyam19a}. 
For probability distributions $P_1, \ldots, P_n$, the JSD is defined as:
\begin{equation*}
\JSD ( P_1, \ldots, P_n ) = \entropyFn(\frac{1}{n} \sum_{i=1}^{n} P_i ) - \frac{1}{n} \sum_{i=1}^{n} \entropyFn(P_i)
\end{equation*}
where $\entropyFn(\cdot)$ denotes the Shannon entropy. 
Since observations comprise multiple tokens, we average the per-token JSD values to obtain a single uncertainty measure $\disagreement_{t}$. 
% Working with discrete distributions enables efficient entropy computation and ensures that $\disagreement_t$ is bounded by $\frac{1}{|\tokens_{t}|} \sum_{\token \in \tokens_{t}} \log(\vocabSize(z))$. 
% This discrete nature also enables efficient entropy computation.
Training data is divided equally among ensemble members, with each predictor processing a distinct subset of each batch.
% Thus the predictors differ not only in their initializations, but also in the order at which they process the data.
Despite the ensemble approach, our implementation maintains computational efficiency, with negligible additional overhead in practice.

% To avoid high computational costs, we follow \cite{sekar20aPlan2Explore} and use an ensemble of prediction heads operating on the outputs of $\WM$.
% The disagreement measures are used as intrinsic reward signals for the controller to encourage exploration.
% Maximizing the cumulative JSD has been shown to be equivalent to maximizing the information gain.
% A large body of evidence in the pure exploration literature suggests that providing the agent with intrinsic rewards based on estimates of the model's epistemic uncertainty improves the quality and generality of the learned world model \cite{}.
% To guide the controller to explore areas with high world model epistemic uncertainty, we incorporated a 

% using the UE as intrinsic rewards for exploration / Epi. Uncr. reduction:

To guide $\Controller$ towards regions of high epistemic uncertainty, $\WM$ provides $\Controller$ with additional intrinsic rewards $\intReward_{t} = \disagreement_t$ during imagination.
Here, the reward provided by $\WM$ at each step $t$ is given by 
\begin{equation*}
    \bar{\reward}_t = \intRewardScale \intReward_t + \extRewardScale \extRewardPred_t
\end{equation*}
where $\intRewardScale, \extRewardScale \in \mathbb{R}$ are hyperparameters that control the scale of each reward type.
% Note that since the controller is optimized in imagination, it learns to reach regions with high model uncertainty without any additional true-environment interaction.
Optimizing the controller in imagination allows it to reach areas of high model uncertainty without additional real-environment interaction.





\paragraph{Prioritized Replay}
% idea: motivation?
Recent work has demonstrated that prioritizing replay buffer sampling during world model training could lead to significant performance gains in intrinsically motivated agents \cite{kauvar2023curiousReplay}.
% While their approach showed promise by prioritizing high-loss or undersampled experiences, it required extensive hyperparameter tuning in practice. 
While their approach showed promise, it required extensive hyperparameter tuning in practice. 
% However, the method in \cite{kauvar2023curiousReplay} involves many hyperparameters which we found difficult to tune in our experiments.
% Here, we propose a simpler algorithm for prioritized sampling during world model optimization.
We propose a simpler, more robust prioritization scheme for world model training.

% idea: details on our method
% Our replay buffer keeps track of the most recent loss value of each example.
% New examples are initialized with a high value \todo{fill exact details}.
% Each batch is composed of a uniform part and a prioritized part.
% A hyperparameter $\replayBufRatio \in [0, 1]$ determines the portion of the prioritized part.
% For the prioritized part, the probability to sample each example $x_i$ in the replay buffer is given by
% \begin{equation*}
%     p(x_i) = \softmax(l)_i 
% \end{equation*}
% where $l$ is the vector of losses of all examples in the replay buffer.
% After each world-model optimization step, the losses of the examples in the batch are updated.

Here, the replay buffer maintains a world model loss value for each stored example, with newly added examples assigned a high initial loss value of $10$. During $\WM$'s training, we sample each batch using a mixture of uniform and prioritized sampling, controlled by a single parameter $\replayBufRatio \in [0, 1]$ that determines the fraction of prioritized samples. For the prioritized portion, we sample examples proportional to their softmax-transformed losses $p_i = \softmax(\mathcal{L})_i$.
% \begin{equation*}
% p_i = \softmax(\mathcal{L})_i
% \end{equation*}
% where $p_i$ is the probability assigned to the $i$-th example in the replay buffer and $\mathcal{L}$ represents the vector of current loss values across all buffer examples. 
The loss values are updated after each world model optimization step using the examples' current batch losses.

% We sample batches from the replay buffer such that $\replayBufRatio \in [0, 1]$ of the examples are uniformly sampled while the probability to sample each example for the other $1-\replayBufRatio$ samples are sampled with 


\paragraph{Training}
We use the cross-entropy loss for the optimization of all components of $\WM$.
Specifically, for each $t$, the loss of $p_{\theta}(\hat{\tokens}_{t} \vert \retnetOutput^{\text{u}}_{t})$ is obtained by averaging the cross-entropy losses of its individual tokens.
The same loss is used for each ensemble member $p_{\phi_i}(\hat{\tokens}_{t} \vert \stopGrad(\retnetOutput^{\text{u}}_{t}))$.
The optimization and design of the reward predictor is similar to that of the critic, as described in Section \ref{sec:classif-pred-cts-values}.
A formal description of the optimization objective can be found in Appendix \ref{sec:appendix-wm-optimization}.
% $\WM$ is trained by minimizing the following objective over trajectory segments sampled from the replay buffer:
% \begin{equation}
%     \mathcal{L}_{\WM} = \mathcal{L}_{\text{obs}} + \mathcal{L}_{\text{reward}} + \mathcal{L}_{\text{done}}
% \end{equation}
















\subsection{The Controller $\Controller$}
$\Controller$ is an extended version of the actor-critic of REM \cite{cohen2024improving} that supports additional observation and action spaces and integrates a classification-based value prediction.

\paragraph{Architecture} 
At the core of $\Controller$'s architecture, parameterized by $\CParams$, is an LSTM \cite{hochreiter1997lstm} sequence model.
At each step $t$, upon observing $\tokens_{t}$, a set of modality-specific encoders map each modality tokens $\tokens^{(i)}_{t}$ to a latent vector $\CLatent^{(i)}$, where we abuse our notation $\tknEmb$ as the context of the discussion is limited to $\Controller$.
The latents are then fused by a fully-connected network to obtain a single vector $\CLatent = \CLatentFuser(\CLatent^{(1)}, \ldots, \CLatent^{(|\modalitySet|)})$.
$\CLatent_{t} \in \mathbb{R}^{\CLSTMDim}$ is processed by $\Controller$'s sequence model to produce $\COutLatent_{t}, \CHiddenState_{t} = \LSTM(\CLatent_{t}, \COutLatent_{t-1}, \CHiddenState_{t-1}; \CParams)$ where $\COutLatent_{t}, \CHiddenState_{t}$ are the LSTM's hidden and cell states, respectively.
Lastly, two linear output layers produce the logits from which the actor and critic outputs $\policy( \action_{t} | \COutLatent_{t}), \valueFn(\COutLatent_{t})$ are derived.
For continuous action spaces, the actor uses a categorical distribution over a uniformly spaced discrete subset of $[-1, 1]$.
% \todo{correct this, critic is actually cat. regression}
We defer the full details about the encoding process to Appendix \ref{sec:controller-additional-details}.







% \paragraph{Continuous Action Spaces}
% idea: 
% For continuous action spaces, the actor uses a categorical distribution associated with a discrete uniformly spaced subset of $[-1, 1]$.
% \todo{revise this paragraph - remove? join?}
% To support continuous action spaces, we consider a similar quantization process as for continuous observations (see Section \todo{add reference}).
% However, since our action spaces are bounded in $[-1, 1]$, we do not use a symlog transformation.












\paragraph{Classification Based Prediction of Continuous Values}
\label{sec:classif-pred-cts-values}
Robustly handling unbounded reward signals has long been challenging as they can vary dramatically in both magnitude and frequency. 
\cite{hafner2023mastering} addressed this challenge by using a classification network that predicts the weights of exponentially spaced bins and employed a two-hot loss for the network's optimization. 
% Recent evidence from \cite{farebrother2024stop} supports using cross-entropy loss instead of the traditional mean squared error loss for value-based deep RL methods. 
\cite{farebrother2024stop} studied the use of cross-entropy loss in place of the traditional mean squared error loss for value-based deep RL methods. 
In their work, the HL-Gauss method was shown to significantly outperform the two-hot loss method.
Following these developments, we adopt \cite{hafner2023mastering}'s approach of using a classification network with exponential bins, while implementing the HL-Gauss method for the optimization of the network.
% idea: dealing with different reward / return magnitudes
% Robustly handling unbounded reward signals has long been challenging as they can vary dramatically in both magnitude and frequency.
% what is our solution?
% We employ a similar approach to that of \cite{hafner2023mastering} for handling the varying signal magnitudes, while following the HL-Gauss classification method of \cite{farebrother2024stop}.
% Recent evidence supports the use of the cross-entropy loss in place of the traditional mean squared error loss for valued based deep RL methods \cite{farebrother2024stop}.
% The approach of \cite{hafner} robustly mitigates the challenges of unbounded rewards by using a classification network with exponential bins that predict the weights of the bins and a two-hot loss.
% Recently, classification-based methods for the prediction of continuous value estimates in RL provided promising evidence for their effectiveness \cite{farebrother2024stop}. 
Concretely, the critic's value estimates are predicted using a linear output layer %classification network $\varphi_{\vartheta}(\cdot)$ 
parameterized by $\mathbf{W} \in \mathbb{R}^{m \times \CLSTMDim}$ with $m=128$ outputs corresponding to $m$ uniform bins defined by $m+1$ endpoints $\mathbf{b} = (b_0, \ldots, b_m)$.
The predicted value is given by
% The predicted value is computed by 
\begin{equation*}
    \hat{y} = \symexp\left(\softmax( \mathbf{W} \COutLatent )^{\Tr} \hat{\mathbf{b}} \right)
\end{equation*}
% \begin{equation*}
%     \hat{y} = \symexp\left(\softmax( \varphi_{\vartheta}(\mathbf{x}) )^{\Tr} \hat{\mathbf{b}} \right)
% \end{equation*}
% where $\symexp(x) = \sign(x) (\exp(|x|) - 1)$ is the inverse of the symlog function and $\hat{\mathbf{b}} = \left(\frac{b_1 + b_0}{2}, \ldots, \frac{b_{m} + b_{m-1}}{2}\right)$ are the bin centers. % and $\varphi_{\vartheta}(\textbf{x})$ is a neural network parameterized by $\vartheta$ with categorical logits outputs.
where $\symexp(x) = \sign(x) (\exp(|x|) - 1)$ is the inverse of the symlog function and $\hat{\mathbf{b}} = \left(\frac{b_1 + b_0}{2}, \ldots, \frac{b_{m} + b_{m-1}}{2}\right)$ are the bin centers.
% $f_{\theta}(\textbf{x})$ is a neural network parameterized by $\theta$ with categorical logits outputs and $m=128$ is the number of bins.
% For the optimization of $\theta$, we use the HL-Gauss method \cite{farebrother2024stop}.
Given the true target $y \in \mathbb{R}$, the HL-Gauss loss is given by
\begin{equation*}
    \mathcal{L}_{\text{HL-Gauss}}(\mathbf{W}, \COutLatent, y) = \tilde{\mathbf{y}}^{\Tr} \log  \softmax(\mathbf{W}\COutLatent)
\end{equation*}
where $\tilde{y}_i = \Phi(\frac{b_i - \symlog(y)}{\sigma}) - \Phi(\frac{b_{i-1} - \symlog(y)}{\sigma})$, $\Phi$ is the cumulative density function of the standard normal distribution and $\sigma$ is a standard deviation hyperparameter that controls the amount of label smoothing.










\paragraph{Training in Imagination}
$\Controller$ is trained entirely from simulated experience generated through interaction with $\WM$.
Specifically, $\WM$ and $\Controller$ are initialized with a short trajectory segment sampled uniformly from the replay buffer and interact for $\horizon$ steps.
% Then, $\Controller$ interacts with $\WM$ for $\horizon$ steps.
An illustration of this process is given in Figure \ref{fig:wm} (orange path).
$\lambda$-returns are computed for each generated trajectory segment and are used as targets for critic learning.
For policy learning, a REINFORCE \cite{sutton1999REINFORCE} objective is used, with a $\valueFn$ baseline for variance reduction.
% Details are elaborated in Appendix \ref{sec:appendix-controller-optimization}.
See Appendix \ref{sec:appendix-controller-optimization} for further details.
% The full details are deferred to Appendix \ref{sec:appendix-controller-optimization}.










\begin{table*}[t]
\caption{Mean returns on the 26 games of the Atari 100k benchmark followed by averaged human-normalized performance metrics. Each game score is computed as the average of 5 runs with different seeds. Bold face mark the best score.}
\label{table:main-results}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lcc ccccc cr}
\toprule

% \multicolumn{3}{c}{} & \multicolumn{5}{c}{Non-Token-Based} & \multicolumn{2}{c}{Token-Based} \\
% \cmidrule(lr){4-8} \cmidrule(lr){9-10} 
Game                 &  Random    &  Human     &  DreamerV3          &  TWM                &  STORM              &  DIAMOND            &  REM               &  \textsc{\AlgName} (ours)  \\
\midrule
Alien                &  227.8     &  7127.7    &  959.4              &  674.6              &  \textbf{983.6}     &  744.1              &  607.2             &  687.2                 \\
Amidar               &  5.8       &  1719.5    &  139.1              &  121.8              &  204.8              &  \textbf{225.8}     &  95.3              &  102.4                 \\
Assault              &  222.4     &  742.0     &  705.6              &  682.6              &  801.0              &  1526.4             &  1764.2            &  \textbf{1822.8}       \\
Asterix              &  210.0     &  8503.3    &  932.5              &  1116.6             &  1028.0             &  \textbf{3698.5}    &  1637.5            &  1369.1                \\
BankHeist            &  14.2      &  753.1     &  \textbf{648.7}     &  466.7              &  641.2              &  19.7               &  19.2              &  347.1                 \\
BattleZone           &  2360.0    &  37187.5   &  12250.0            &  5068.0             &  \textbf{13540.0}   &  4702.0             &  11826.0           &  13262.0               \\
Boxing               &  0.1       &  12.1      &  78.0               &  77.5               &  79.7               &  86.9               &  87.5              &  \textbf{93.5}         \\
Breakout             &  1.7       &  30.5      &  31.1               &  20.0               &  15.9               &  132.5              &  90.7              &  \textbf{148.9}        \\
ChopperCommand       &  811.0     &  7387.8    &  410.0              &  1697.4             &  1888.0             &  1369.8             &  2561.2            &  \textbf{3611.6}       \\
CrazyClimber         &  10780.5   &  35829.4   &  97190.0            &  71820.4            &  66776.0            &  \textbf{99167.8}   &  76547.6           &  93433.2               \\
DemonAttack          &  152.1     &  1971.0    &  303.3              &  350.2              &  164.6              &  288.1              &  \textbf{5738.6}   &  4787.6                \\
Freeway              &  0.0       &  29.6      &  0.0                &  24.3               &  0.0                &  \textbf{33.3}      &  32.3              &  31.9                  \\
Frostbite            &  65.2      &  4334.7    &  909.4              &  \textbf{1475.6}    &  1316.0             &  274.1              &  240.5             &  258.4                 \\
Gopher               &  257.6     &  2412.5    &  3730.0             &  1674.8             &  \textbf{8239.6}    &  5897.9             &  5452.4            &  4363.2                \\
Hero                 &  1027.0    &  30826.4   &  \textbf{11160.5}   &  7254.0             &  11044.3            &  5621.8             &  6484.8            &  7466.8                \\
Jamesbond            &  29.0      &  302.8     &  444.6              &  362.4              &  509.0              &  427.4              &  391.2             &  \textbf{678.0}        \\
Kangaroo             &  52.0      &  3035.0    &  4098.3             &  1240.0             &  4208.0             &  5382.2             &  467.6             &  \textbf{6656.0}       \\
Krull                &  1598.0    &  2665.5    &  7781.5             &  6349.2             &  8412.6             &  \textbf{8610.1}    &  4017.7            &  6677.3                \\
KungFuMaster         &  258.5     &  22736.3   &  21420.0            &  24554.6            &  26182.0            &  18713.6            &  25172.2           &  \textbf{31705.4}      \\
MsPacman             &  307.3     &  6951.6    &  1326.9             &  1588.4             &  \textbf{2673.5}    &  1958.2             &  962.5             &  1282.7                \\
Pong                 &  -20.7     &  14.6      &  18.4               &  18.8               &  11.3               &  \textbf{20.4}      &  18.0              &  19.9                  \\
PrivateEye           &  24.9      &  69571.3   &  881.6              &  86.6               &  \textbf{7781.0}    &  114.3              &  99.6              &  100.0                 \\
Qbert                &  163.9     &  13455.0   &  3405.1             &  3330.8             &  \textbf{4522.5}    &  4499.3             &  743.0             &  2425.6                \\
RoadRunner           &  11.5      &  7845.0    &  15565.0            &  9109.0             &  17564.0            &  20673.2            &  14060.2           &  \textbf{24471.8}      \\
Seaquest             &  68.4      &  42054.7   &  618.0              &  774.4              &  525.2              &  551.2              &  1036.7            &  \textbf{1800.4}       \\
UpNDown              &  533.4     &  11693.2   &  7567.1             &  \textbf{15981.7}   &  7985.0             &  3856.3             &  3757.6            &  10416.5               \\
\midrule
\#Superhuman (â†‘)     &  0         &  N/A       &  9                  &  8                  &  9                  &  11                 &  12                &  \textbf{13}           \\
Mean (â†‘)             &  0.000     &  1.000     &  1.124              &  0.956              &  1.222              &  1.459              &  1.222             &  \textbf{1.645}        \\
Median (â†‘)           &  0.000     &  1.000     &  0.485              &  0.505              &  0.425              &  0.373              &  0.280             &  \textbf{0.982}        \\
IQM (â†‘)              &  0.000     &  1.000     &  0.487              &  0.459              &  0.561              &  0.641              &  0.673             &  \textbf{0.990}        \\
Optimality Gap (â†“)   &  1.000     &  0.000     &  0.510              &  0.513              &  0.472              &  0.480              &  0.482             &  \textbf{0.412}        \\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}








\section{Experiments}
To evaluate sample efficiency, we used benchmarks that measure performance within a fixed, limited environment interaction budget. 
The selected benchmarks also addressed two key research questions: (1) whether \AlgName{} performs effectively in continuous environments and (2) whether it handles multi-modal observations successfully.
% Since world model agents are proposed as sample-efficient methods, their evaluation involves benchmarks where performance is measured under a fixed and limited environment interaction budget.
% As sample-efficient methods, the evaluation of world model agents involves benchmarks where performance is measured under a fixed and limited environment interaction budget.
% Here, in addition to the sample-efficiency evaluation, we selected benchmarks with the following research questions in mind:
% Here, benchmarks were also selected to investigate the following research questions:
% (1) Is \AlgName{} effective in continuous environments? and (2) Is \AlgName{} effective in environments with multi-modal observations?
% (1) How effectively does \AlgName{} perform in continuous environments?
% (2) How well does \AlgName{} handle environments with multi-modal observations?"
% \begin{itemize}
    % \item How does the performance of \AlgName{} compare to modality-specific experts?
    % \item Does \AlgName{} performs consistently across modalities when different tokenizers are used?
    % \item Is \AlgName{} effective in continuous environments?
    % \item Is \AlgName{} effective in environments with multi-modal observations?
    % \item Is \AlgName{} maintaining competitive performance across modalities, using different tokenizers?
    % \item Is \AlgName{} performs consistently across modalities? 
% \end{itemize}










\subsection{Experimental Setup}
\paragraph{Benchmarks:} We evaluate \AlgName{} on three sample-efficiency benchmarks of different observation and action modalities:
Atari 100K \cite{Kaiser2020Model}, DeepMind Control Suite (DMC) Proprioception 500K \cite{tunyasuvunakool2020DMC}, and Craftax-1M \cite{matthews2024craftax}. 

Atari 100K has become the gold standard in the literature for evaluating sample-efficient deep RL agents.
The benchmark comprises a subset of 26 games.
Within each game, agents must learn from visual image signal under a tightly restricted budget of 100K interactions, corresponding to roughly two hours of human gameplay.

The DeepMind Control Suite (DMC) is a set of continuous control tasks involving multiple agent embodiments ranging from simple single-joint models to complex humanoids.
Here, we follow the subset of proprioception tasks used for the evaluation of DreamerV3 \cite{hafner2023mastering}, where observations and actions are continuous vectors.
At each task, the agent's interaction budget is limited to 500K steps.

Craftax is a 2D open-world survival game benchmark inspired by Minecraft, designed to evaluate RL agents' capabilities in planning, memory, and exploration. The partially-observable environment features procedurally generated worlds where agents must gather and craft resources while surviving against hostile creatures.
Observations consist of a 9Ã—11 tile egocentric map, where each tile consists of 4 symbols, and 48 state features corresponding to state information such as inventory and health.
% The Craftax benchmark is a 2D open-world survival game inspired by Minecraft. It is a partially-observable RL environment with crafting mechanics, designed to test agents' abilities in planning, long-term memory, and exploration. It features procedurally generated worlds that require agents to gather resources and combine them in specific sequences to craft increasingly complex items.
% To survive, agents must defend against hostile creatures, collect vital resources, and build shelters to rest safely.
% To facilitate efficient computation, observations comprise an egocentric map in the form of a 2D grid of $9 \times 11$ tiles, each composed of 4 discrete symbols, followed by a vector of 48 continuous and categorical features corresponding to other state information such as inventory and health states.
Here, we consider the sample-efficiency oriented Craftax-1M variant which only allows an interaction budget of one million steps.




% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/dmc_main_results.pdf}
%     \caption{Results on the Deepmind Control Suite 500K benchmark. }
%     \label{fig:dmc-main-results}
% \end{figure*}



% These environments provide diverse challenges, from discrete action spaces (Atari) to continuous control (DMC), and complex exploration tasks (Craftex).
% \todo{add details about the obs space of Craftax - multiple modalities}

\paragraph{Baselines}
% \todo{Describe baseline for each benchmark}
% Therefore, for the Atari benchmark, we include \textbf{Random}, a naive strategy with no learned behavior; \textbf{Human}, which represents human-level performance; 
On Atari-100K, we compare \AlgName{} against DreamerV3 \cite{hafner2023mastering} and several methods restricted to image observations: TWM \cite{robine2023transformer}, STORM \cite{zhang2024storm}, DIAMOND \cite{alonso2024diffusion}, and REM \cite{cohen2024improving}.
% DreamerV3~\cite{hafner2023mastering}, a state-of-the-art model-based reinforcement learning method; TWM~\cite{robine2023transformer}, a transformer-based world model designed for sample-efficient RL; STORM~\cite{zhang2024storm}, which leverages stochastic transformers for efficient world modeling; DIAMOND~\cite{alonso2024diffusion}, a diffusion-based world model that excels in visual detail preservation, and REM~\cite{cohen2024improving}, a SOTA transformer based world model. 
% On DMC, we compare only against DreamerV3, as we couldn't find another planning-free world model method evaluated on the 500K proprioception benchmark.
% On Craftax-1M, we compare to \todo{add citations} RND, PPO-RNN, and E3B, which are currently the only baselines with available results, taken from the Craftax paper.
On DMC, we compare exclusively with DreamerV3, currently the only planning-free world model method with published results on the 500K proprioception benchmark. 
On Craftax-1M, we compare against TWM \cite{dedieu2025TWM2}, a concurrent work that proposes a Transformer based world model with a focus on the Craftax benchmark, and the baselines reported in the Craftax paper: Random Network Distillation (RND) \cite{burda2018exploration}, PPO \cite{schulman2017proximal} with a recurrent neural network (PPO-RNN), and Exploration via Elliptical Episodic Bonuses (E3B) \cite{henaff2022explorationE3B}.
As Craftax is a recent benchmark, there are no other published results in existing world models literature.
% \todo{add a description on each baseline?}
% In contrast, for the DMC and Craftex benchmarks, we focus our comparison solely on DreamerV3~\cite{hafner2023mastering}, which has demonstrated strong performance in continuous control and sparse reward environments. This domain-specific selection ensures a fair and meaningful evaluation of our method's capabilities.
Following the standard practice in the literature, we exclude planning-based methods \cite{hansen2024tdmpc, wang2024efficientzero}, as planning is an orthogonal component that operates on any given model, typically incurring significant computational overhead. Here, our focus is on the world model component. 
% \todo{add reason re BBF}
% Here, our focus is on the world model itself. 



%%%%%%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\begin{figure*}[t]
    \centering
    
% \begin{subfigure}[b]{\linewidth}
%    \includegraphics[width=\linewidth]{figures/dmc_main_results.pdf}
%    \caption{DeepMind Control Suite Proprioception 500K}
%    \label{fig:main-results-dmc}
% \end{subfigure}
% \begin{subfigure}[b]{\linewidth}
%    \includegraphics[width=\linewidth]{figures/aggregates.pdf}
%    \caption{Atari 100K}
%    \label{fig:main-results-atari}
% \end{subfigure}
    \includegraphics[width=\linewidth]{figures/dmc_main_results.pdf}
    \includegraphics[width=\linewidth]{figures/aggregates.pdf}
    \caption{Results on the DeepMind Control Suite 500K Proprioception (top) and Atari 100K (bottom) benchmarks. }
    \label{fig:dmc-atari-aggregated-main-results}
\end{figure*}
%%%%%%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




\paragraph{Metrics and Evaluation} 
For Atari, we report human-normalized scores (HNS) $\frac{\text{agent\_score} - \text{random\_score}}{\text{human\_score} - \text{random\_score}}$ \cite{mnih2015human}.
Following the protocol of \cite{Agarwal2021rliable} and using their toolkit, we report the mean, median, interquantile mean (IQM), and optimality gap metrics with 95\% stratified bootstrap confidence intervals.
% For DMC, we report the raw agent returns, as the return of all tasks is in the range $[0, 1000]$.
% For Craftax, we report the agent return as a \% of the maximal possible return (226).
For DMC and Craftax, we report the raw agent returns.
We use 5 random seeds per environment.
In each experiment, final performance is evaluated using 100 test episodes at the end of training and the mean score is reported.


% \paragraph{Evaluation}
% For each training run, which produces a trained agent, we generate 100 test episodes using the trained agent. The scores are averaged to obtain the final score of the run.
% After each training run, the weights of the trained agent are used to generate 100 evaluation episodes.
% The scores are averaged to obtain the final score of the run.
% Should we write explicitly that some baselines (Dreamer) does not follow the same evaluation protocol, does not explicitly share their evaluation protocol, and thier final per-seed results are not available?
% For each training run, performance is evaluated using 100 test episodes at the end of training and the mean score is reported.










\subsection{Results}
% Table~\ref{table:main-results} summarizes the average episodic return on individual Atari games, while Figure~\ref{fig:atari-aggregated-main-results} shows the aggregated mean, median, IQM, and optimality gap metrics. 
% \todo{Additional Atari results in Appendix}
% Figure~\ref{fig:dmc-main-results} shows the average return for each DMC task, as well as the mean return across all tasks.
% Figure~\ref{fig:craftax-main-results} depicts 
















% \subsection{Analysis}

% \paragraph{Sample-Efficiency}
% \todo{add that it's great on all benchmarks}
\AlgName{} achieves state-of-the-art performance across all three benchmarks (Figure \ref{fig:first-page-results}). 
On Atari 100k, \AlgName{} outperforms all baselines across all key metrics (Figure~\ref{fig:dmc-atari-aggregated-main-results}). 
Notably, \AlgName{} is the first planning-free world model to reach a humen-level IQM and median scores.
% It also attains the highest mean and median human-normalized scores of 1.645 and 0.982, respectively, a significant improvement over prior methods such as STORM (1.222, 0.425) and DIAMOND (1.459, 0.373) (Table~\ref{table:main-results}). 
% It also achieves the best median human-normalized score (0.982) and interquartile mean (IQM) (0.990), indicating consistent performance across games. 
% In addition, \AlgName{} has the lowest optimality gap (0.412), demonstrating its ability to approach human-level performance more closely than competing models. 
In addition, it achieves superhuman performance on \textbf{13} out of 26 games, surpassing all baselines. 
These results highlight \AlgName{}â€™s effectiveness in sample-efficient learning and its robustness across diverse tasks in the Atari 100k benchmark.




\paragraph{Is \AlgName{} effective in continuous environments?}
% Since all prior token-based approaches focused on discrete action spaces and relied on discrete image autoencoders, it was unclear whether these inherently discrete architectures could be effective in environments with continuous observation and action spaces.
% Figure~\ref{fig:dmc-main-results} shows that \AlgName{} matches DreamerV3's performance on most tasks while surpassing it on Cartpole Swingup Sparse and both Hopper tasks, suggesting a positive answer to this question.
% As prior token-based methods were limited to image observations and discrete actions, their effectiveness in continuous environments remained an open question. 
% While prior token-based methods exclusively handled discrete action spaces and relied on discrete image autoencoders, the effectiveness of these architectures in environments with continuous observation and action spaces remained an open question. 
Figure~\ref{fig:dmc-atari-aggregated-main-results} provides compelling evidence that token-based architectures can indeed excel in continuous domains: \AlgName{} achieves performance comparable to DreamerV3 across most tasks, and notably outperforms it on Cartpole Swingup Sparse and both Hopper tasks.


\paragraph{Is \AlgName{} effective in environments with multi-modal observations?}
Since modularity is at the core of \AlgName{}, we investigate its performance in Craftax, as it combines an image-like 2D grid map with a vector of features, involving multiple tokenizers ($\Tokenizer$).
% Here, we consider Craftax for this purpose, as it combines an image-like 2D grid map with a vector of features.
Figure~\ref{fig:craftax-main-results} shows that \AlgName{} maintains sample-efficiency in this multi-modal environment as well, as it significantly outperforms all model-free baselines, including exploration-oriented ones.
\AlgName{} also attains state-of-the-art performance compared to concurrent world model methods (Figure \ref{fig:first-page-results}).
% Notably, each observation in Craftax consists of 444 tokens arranged into 147 sequence vectors.
% Hence, even short trajectories involve thousands of tokens. 
% This demonstrates \AlgName{}'s ability to handle long sequences efficiently.
With 444 tokens per observation arranged into 147 sequences, even short trajectories in Craftax contain thousands of tokens, demonstrating \AlgName{}'s efficient handling of long sequences.









\subsection{Ablation Studies}
We ablate the intrinsic rewards, prioritized replay, and classification-based predictions to demonstrate their individual contributions to \AlgName{}'s performance.
% To understand the contribution of the intrinsic rewards, the prioritized replay, and the classification-based predictions to the performance of \AlgName{}, we performed a set of ablations.
In each ablation, \AlgName{} is modified so that only the component of interest is disabled. 
Due to limited computational resources, we consider a subset of 8 tasks for each of the Atari 100K and DMC benchmarks.
Concretely, for Atari 100K, we used "Assault", "Breakout", "Chopper Command", "Crazy Climber", "James bond", "Kangaroo", "Seaquest", and "Up'n Down", in which significant improvements were observed.
% This subset includes "Assault", "Breakout", "Chopper Command", "Crazy Climber", "James bond", "Kangaroo", "Seaquest", and "Up'n Down".
For DMC, we chose a subset that includes different embodiments: "acrobot swingup", "cartpole swingup sparse", "cheetah run", "finger turn hard", "hopper stand", "pendulum swingup", "reacher hard", and "walker run".
% In addition to its modular design, \AlgName{} incorporates several new algorithmic components, namely its intrinsic rewards mechanism, prioritized replay, and 

The results are presented in Figure~\ref{fig:ablations-results}.
Although all components contributed to the final performance of \AlgName{}, the intrinsic rewards proved crucial for achieving competitive performance, particularly on DMC.

More broadly, the results in Figure~\ref{fig:ablations-results} demonstrate that encouraging the controller to explore regions of high epistemic uncertainty through intrinsic rewards significantly improves its performance in world model agents, even in reward-rich environments.
% In DMC, intrinsic rewards were crucial for attaining competitive performance.
% This observation is non-trivial for multiple reasons.
This observation is non-trivial in a sample-efficiency setting, as model-driven exploration costs the agent some amount of interaction budget, which could be otherwise used for controller-driven exploration during data collection.
The latter type of exploration aims to collect new information about the true reward signal, which defines the task and its success metric.
On the other hand, model-driven exploration may guide the controller towards environment regions that are irrelevant to the task at hand.


% \paragraph{Prioritized Replay}



% \paragraph{Classification-Based Prediction of Continuous Values}





















\section{Related Work}
\paragraph{Offline Multi-Modal Methods}
Large-scale token-based sequence models for modeling and generating multi-modal trajectories of agent experience were proposed in \cite{lu2023unifiedio, Lu_2024_CVPR, reed2022a, schubert2023generalist}.
In Gato \cite{reed2022a} and TDM \cite{schubert2023generalist}, multi-modal inputs are first tokenized through predefined transformations, while in Unified IO \cite{lu2023unifiedio, Lu_2024_CVPR} pretrained models are also used.
The produced tokens are then embedded and processed by the sequence model, similarly to \AlgName{}.
% Importantly, image embeddings are learned jointly with the world model, while in our method the optimization is separated.
Importantly, these methods do not learn control through RL.
Instead, they learn from expert data.
In addition, these methods use large models with billions of parameters, large vocabulary sizes, and orders-of-magnitude more data and compute compared to sample efficient world models.
Hence, it is unclear whether the design choices of these methods would be effective in an online sample-efficiency setting, where the data is non-stationary and strictly limited.
% Unified-IO \cite{lu2023unifiedio, Lu_2024_CVPR} (not RL). 
% Gato \cite{reed2022a} and TDM \cite{schubert2023generalist} do not learn control, only imitation from expert data.
% These methods use large models with billions of parameters and are not designed to adapt in online dynamic environments.
% In the context of multi-modal token-based framework, these methods use large vocabulary sizes as they as not limited to a fixed budget.
% It is the other way around, they can afford massive amounts of data and compute.
% In addition, there is no explicit interface for handling general mixed-modality inputs.
% In addition, offline methods often fail catastrophically when encountering out-of-distribution data, which is common in any complex environment.
% These methods provide no evidence regarding the potential of the multi-modal token-based model in an online learning setting and under limited data and compute budgets (sample efficiency). 



%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/craftax_results.pdf}
    \caption{Craftax-1M training curves with mean and 95\% confidence intervals. }
    \label{fig:craftax-main-results}
\end{figure}
%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~





\paragraph{World Model Agents}
Model-based RL agents where the policy is learned exclusively from simulated data generated by the learned world model were originally proposed by \cite{ha2018worldmodels}.
Later, a line of works proposed the popular Dreamer algorithms \cite{Hafner2020Dreamerv1, hafner2021dreamerv2, hafner2023mastering}.
To learn dynamics, Dreamer's objective involves a Kullbackâ€“Leibler (KL) divergence term between its learned prior and posterior estimators, effectively coupling the optimization of the representation model and the recurrent neural network world model.
This design leads to a complex monolithic system that poses challenges for development and scaling.

Following the success of the Transformer architecture \cite{NIPS2017_attn_is_all_you_need} in language modeling \cite{brown2020LMsFewShotLearners}, Transformer-based variants of Dreamer \cite{zhang2024storm, robine2023transformer} were proposed.
In addition, token-based world models (TBWMs), which represent trajectories as language-like token sequences, were proposed \cite{micheli2022transformers, cohen2024improving}.
Notably, the evaluation of these methods is limited to the Atari 100K benchmark.
% Moreover, it is unclear how to extend TBWMs to support continuous inputs effectively. 

Recently, motivated by the success of diffusion generative models \cite{Rombach_2022_latentDiffusion}, DIAMOND \cite{alonso2024diffusion}, a diffusion-based world model agent was proposed.
Although it produces visually compelling outputs, it is currently limited to visual environments.



% Lastly, planning-based world models were proposed \cite{wang2024efficientzero, hansen2024tdmpc}, demonstrating state-of-the-art performance on sample efficiency benchmarks.
% However, these methods are compute intensive, often requiring multiple GPUs for a single training run.
% In addition, current methods only model the rewards and return dynamics, while not modeling observation dynamics.
% This limits their generality, as downstream tasks 







%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{figure}[t]
    \centering

% \subfigure[first caption.]{\includegraphics[width=0.48\textwidth]{figures/ablations_atari_summary.pdf}}
% \subfigure[first caption.]{\includegraphics[width=0.48\textwidth]{figures/ablations_dmc_summary.pdf}}
    
\begin{subfigure}[b]{\linewidth}
   \includegraphics[width=\linewidth]{figures/ablations_atari_summary.pdf}
   \caption{Atari 100K}
   \label{fig:ablations-atari}
\end{subfigure}
\vskip 0.1in
\begin{subfigure}[b]{\linewidth}
   \includegraphics[width=\linewidth]{figures/ablations_dmc_summary.pdf}
   \caption{DeepMind Control Suite Proprioception 500K}
   \label{fig:ablations-dmc}
\end{subfigure}
    
    % \vspace{-20pt}
    \caption{Ablations results on the Atari-100K (top) and DeepMind Control Proprioception 500K (bottom) benchmarks. A subset of 8 games was used for each ablation.}
    \label{fig:ablations-results}
\end{figure}
%%%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~





\paragraph{Large Video World Models}
Following recent advances in video generative modeling \cite{ho2022imagenVideo, blattmann2023stable, videoworldsimulators2024}, a recent body of work proposed large video world models \cite{genie2, agarwal2025cosmos, oasis2024, valevski2024gameNGen}.
These methods are trained offline on massive pre-collected data to predict future video frames.
However, these methods do not involve control learning, and RL in particular.
% Specifically, no RL is used.
% Consequently, there is no evidence to support the effectiveness of these methods in online RL settings. 
% These methods are not suitable for online RL settings since they are prone to failure when encountering out-of-distribution data due to their offline nature.
% Moreover, for such settings, their high computational demands make them impractical for most research labs.
% Due to their offline nature, these methods are prone to failure when encountering out-of-distribution data, while also impractical for online learning applications due to their high computational demands.
% For a comprehensive overview of world models and recent advances, we refer interested readers to~\cite{ding2024understanding}.


% \paragraph{Model-Free Sample-Efficient Methods}
% BBF














\section{Limitations and Future Work}
Here, we briefly highlight several limitations of this work.
% First, our results on Craftax-1M lack sample-efficient baselines.
% Evaluating world model baselines would strengthen our results.
% Due to limited computational resources, \AlgName{} was not properly tuned. Proper tuning could lead to improved performance.
First, although the feature quantization approach for tokenizing continuous vectors showed promise, it leads to excessive sequence lengths.
We believe that more efficient solutions can be found for dealing with continuous inputs.
Second, since rich multi-modal RL benchmarks are scarce, future work could wrap multiple single-modality environments into a unified multi-modal environment, which can be used to evaluate agents under different modality combinations as well as the agent's ability to perform multiple independent tasks concurrently.












\section{Conclusions}
In this paper, we proposed a modular world model, \AlgName{}, for sample-efficient RL.
\AlgName{} extends token-based world model agents via a modular framework to support a wide range of environments and modalities.
In addition, it combines multiple advances from existing literature to enhance performance.
Through extensive empirical evaluation, \AlgName{} exhibited state-of-the-art performance for planning-free world models on a diverse set of benchmarks, involving visual, continuous, and structured symbolic modalities.
Notably, on the well-established Atari-100K benchmark, \AlgName{} outperformed all baselines across all metrics.



\section{Impact Statement}
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

% \bibliography{references}
\bibliographystyle{icml2025}




\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2025)Agarwal, Ali, Bala, Balaji, Barker, Cai, Chattopadhyay, Chen, Cui, Ding, et~al.]{agarwal2025cosmos}
Agarwal, N., Ali, A., Bala, M., Balaji, Y., Barker, E., Cai, T., Chattopadhyay, P., Chen, Y., Cui, Y., Ding, Y., et~al.
\newblock Cosmos world foundation model platform for physical ai.
\newblock \emph{arXiv preprint arXiv:2501.03575}, 2025.

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and Bellemare]{Agarwal2021rliable}
Agarwal, R., Schwarzer, M., Castro, P.~S., Courville, A.~C., and Bellemare, M.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  29304--29320. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/f514cec81cb148559cf475e7426eed5e-Paper.pdf}.

\bibitem[Alonso et~al.(2024)Alonso, Jelley, Micheli, Kanervisto, Storkey, Pearce, and Fleuret]{alonso2024diffusion}
Alonso, E., Jelley, A., Micheli, V., Kanervisto, A., Storkey, A., Pearce, T., and Fleuret, F.
\newblock Diffusion for world modeling: Visual details matter in atari.
\newblock \emph{arXiv preprint arXiv:2405.12399}, 2024.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and Courville]{bengio2013estimating}
Bengio, Y., L{\'e}onard, N., and Courville, A.
\newblock Estimating or propagating gradients through stochastic neurons for conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Blattmann et~al.(2023)Blattmann, Dockhorn, Kulal, Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, et~al.]{blattmann2023stable}
Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et~al.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Brooks et~al.(2024)Brooks, Peebles, Holmes, DePue, Guo, Jing, Schnurr, Taylor, Luhman, Luhman, Ng, Wang, and Ramesh]{videoworldsimulators2024}
Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., and Ramesh, A.
\newblock Video generation models as world simulators.
\newblock 2024.
\newblock URL \url{https://openai.com/research/video-generation-models-as-world-simulators}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020LMsFewShotLearners}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Burda et~al.(2019)Burda, Edwards, Storkey, and Klimov]{burda2018exploration}
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=H1lJJnR5Ym}.

\bibitem[Cohen et~al.(2024)Cohen, Wang, Kang, and Mannor]{cohen2024improving}
Cohen, L., Wang, K., Kang, B., and Mannor, S.
\newblock Improving token-based world models with parallel observation prediction.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.
\newblock URL \url{https://openreview.net/forum?id=Lfp5Dk1xb6}.

\bibitem[Decart et~al.(2024)Decart, Quevedo, McIntyre, Campbell, Chen, and Wachen]{oasis2024}
Decart, Quevedo, J., McIntyre, Q., Campbell, S., Chen, X., and Wachen, R.
\newblock Oasis: A universe in a transformer, 2024.
\newblock URL \url{https://oasis-model.github.io/}.

\bibitem[Dedieu et~al.(2025)Dedieu, Ortiz, Lou, Wendelken, Lehrach, Guntupalli, Lazaro-Gredilla, and Murphy]{dedieu2025TWM2}
Dedieu, A., Ortiz, J., Lou, X., Wendelken, C., Lehrach, W., Guntupalli, J.~S., Lazaro-Gredilla, M., and Murphy, K.~P.
\newblock Improving transformer world models for data-efficient rl, 2025.
\newblock URL \url{https://arxiv.org/abs/2502.01591}.

\bibitem[DeepMind(2024)]{genie2}
DeepMind, G.
\newblock Genie 2: A large-scale foundation world model, 2024.
\newblock URL \url{https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/}.

\bibitem[Esser et~al.(2021)Esser, Rombach, and Ommer]{esser2021tamingVQGAN}
Esser, P., Rombach, R., and Ommer, B.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  12873--12883, 2021.

\bibitem[Farebrother et~al.(2024)Farebrother, Orbay, Vuong, Taiga, Chebotar, Xiao, Irpan, Levine, Castro, Faust, Kumar, and Agarwal]{farebrother2024stop}
Farebrother, J., Orbay, J., Vuong, Q., Taiga, A.~A., Chebotar, Y., Xiao, T., Irpan, A., Levine, S., Castro, P.~S., Faust, A., Kumar, A., and Agarwal, R.
\newblock Stop regressing: Training value functions via classification for scalable deep {RL}.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.
\newblock URL \url{https://openreview.net/forum?id=dVpFKfqF3R}.

\bibitem[Ha \& Schmidhuber(2018)Ha and Schmidhuber]{ha2018worldmodels}
Ha, D. and Schmidhuber, J.
\newblock Recurrent world models facilitate policy evolution.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\  2451--2463. Curran Associates, Inc., 2018.
\newblock URL \url{https://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution}.
\newblock \url{https://worldmodels.github.io}.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Ba, and Norouzi]{Hafner2020Dreamerv1}
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1lOTC4tDS}.

\bibitem[Hafner et~al.(2021)Hafner, Lillicrap, Norouzi, and Ba]{hafner2021dreamerv2}
Hafner, D., Lillicrap, T.~P., Norouzi, M., and Ba, J.
\newblock Mastering atari with discrete world models.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=0oabwyZbOu}.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and Lillicrap]{hafner2023mastering}
Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv preprint arXiv:2301.04104}, 2023.

\bibitem[Hansen et~al.(2024)Hansen, Su, and Wang]{hansen2024tdmpc}
Hansen, N., Su, H., and Wang, X.
\newblock {TD}-{MPC}2: Scalable, robust world models for continuous control.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=Oxh5CstDJU}.

\bibitem[Henaff et~al.(2022)Henaff, Raileanu, Jiang, and Rockt{\"a}schel]{henaff2022explorationE3B}
Henaff, M., Raileanu, R., Jiang, M., and Rockt{\"a}schel, T.
\newblock Exploration via elliptical episodic bonuses.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Xg-yZos9qJQ}.

\bibitem[Hendrycks \& Gimpel(2017)Hendrycks and Gimpel]{hendrycks2017bridging}
Hendrycks, D. and Gimpel, K.
\newblock Bridging nonlinearities and stochastic regularizers with gaussian error linear units, 2017.
\newblock URL \url{https://openreview.net/forum?id=Bk0MRI5lg}.

\bibitem[Ho et~al.(2022)Ho, Chan, Saharia, Whang, Gao, Gritsenko, Kingma, Poole, Norouzi, Fleet, et~al.]{ho2022imagenVideo}
Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.~P., Poole, B., Norouzi, M., Fleet, D.~J., et~al.
\newblock Imagen video: High definition video generation with diffusion models.
\newblock \emph{arXiv preprint arXiv:2210.02303}, 2022.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{hochreiter1997lstm}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Johnson et~al.(2016)Johnson, Alahi, and Fei-Fei]{johnson2016perceptual}
Johnson, J., Alahi, A., and Fei-Fei, L.
\newblock Perceptual losses for real-time style transfer and super-resolution.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14}, pp.\  694--711. Springer, 2016.

\bibitem[Kaiser et~al.(2020)Kaiser, Babaeizadeh, Mi{\l}os, Osi{\'n}ski, Campbell, Czechowski, Erhan, Finn, Kozakowski, Levine, Mohiuddin, Sepassi, Tucker, and Michalewski]{Kaiser2020Model}
Kaiser, {\L}., Babaeizadeh, M., Mi{\l}os, P., Osi{\'n}ski, B., Campbell, R.~H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., and Michalewski, H.
\newblock Model based reinforcement learning for atari.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1xCPJHtDB}.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{pmlr-v119-katharopoulos20a}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear attention.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  5156--5165. PMLR, 13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/katharopoulos20a.html}.

\bibitem[Kauvar et~al.(2023)Kauvar, Doyle, Zhou, and Haber]{kauvar2023curiousReplay}
Kauvar, I., Doyle, C., Zhou, L., and Haber, N.
\newblock Curious replay for model-based adaptation.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, ICML'23. JMLR.org, 2023.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and Blundell]{LakshminarayananNIPS2017uncertaintyEstDeepEns}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf}.

\bibitem[Larsen et~al.(2016)Larsen, SÃ¸nderby, Larochelle, and Winther]{pmlr-v48-larsen16}
Larsen, A. B.~L., SÃ¸nderby, S.~K., Larochelle, H., and Winther, O.
\newblock Autoencoding beyond pixels using a learned similarity metric.
\newblock In Balcan, M.~F. and Weinberger, K.~Q. (eds.), \emph{Proceedings of The 33rd International Conference on Machine Learning}, volume~48 of \emph{Proceedings of Machine Learning Research}, pp.\  1558--1566, New York, New York, USA, 20--22 Jun 2016. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v48/larsen16.html}.

\bibitem[Lu et~al.(2023)Lu, Clark, Zellers, Mottaghi, and Kembhavi]{lu2023unifiedio}
Lu, J., Clark, C., Zellers, R., Mottaghi, R., and Kembhavi, A.
\newblock {UNIFIED}-{IO}: A unified model for vision, language, and multi-modal tasks.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=E01k9048soZ}.

\bibitem[Lu et~al.(2024)Lu, Clark, Lee, Zhang, Khosla, Marten, Hoiem, and Kembhavi]{Lu_2024_CVPR}
Lu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten, R., Hoiem, D., and Kembhavi, A.
\newblock Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  26439--26455, June 2024.

\bibitem[Matthews et~al.(2024)Matthews, Beukman, Ellis, Samvelyan, Jackson, Coward, and Foerster]{matthews2024craftax}
Matthews, M., Beukman, M., Ellis, B., Samvelyan, M., Jackson, M., Coward, S., and Foerster, J.
\newblock Craftax: A lightning-fast benchmark for open-ended reinforcement learning.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2024.

\bibitem[Micheli et~al.(2023)Micheli, Alonso, and Fleuret]{micheli2022transformers}
Micheli, V., Alonso, E., and Fleuret, F.
\newblock Transformers are sample-efficient world models.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/pdf?id=vhFu1Acb0xb}.

\bibitem[Micheli et~al.(2024)Micheli, Alonso, and Fleuret]{micheli2024efficient}
Micheli, V., Alonso, E., and Fleuret, F.
\newblock Efficient world models with context-aware tokenization.
\newblock In \emph{Forty-first International Conference on Machine Learning, {ICML} 2024, Vienna, Austria, July 21-27, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=BiWIERWBFX}.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare, M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, KÃ¶pf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{Paszke2019Pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., KÃ¶pf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32, 2019.

\bibitem[Ramachandran et~al.(2018)Ramachandran, Zoph, and Le]{ramachandran2018searching}
Ramachandran, P., Zoph, B., and Le, Q.~V.
\newblock Searching for activation functions, 2018.
\newblock URL \url{https://openreview.net/forum?id=SkBYYyZRZ}.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov, Barth-maron, Gim{\'e}nez, Sulsky, Kay, Springenberg, Eccles, Bruce, Razavi, Edwards, Heess, Chen, Hadsell, Vinyals, Bordbar, and de~Freitas]{reed2022a}
Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.~G., Novikov, A., Barth-maron, G., Gim{\'e}nez, M., Sulsky, Y., Kay, J., Springenberg, J.~T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., Bordbar, M., and de~Freitas, N.
\newblock A generalist agent.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=1ikK0kHjvj}.
\newblock Featured Certification, Outstanding Certification.

\bibitem[Robine et~al.(2023)Robine, H{\"o}ftmann, Uelwer, and Harmeling]{robine2023transformer}
Robine, J., H{\"o}ftmann, M., Uelwer, T., and Harmeling, S.
\newblock Transformer-based world models are happy with 100k interactions.
\newblock \emph{arXiv preprint arXiv:2303.07109}, 2023.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{Rombach_2022_latentDiffusion}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  10684--10695, June 2022.

\bibitem[Schmidhuber(1991)]{Schmidhuber1991Curious}
Schmidhuber, J.
\newblock Curious model-building control systems.
\newblock In \emph{[Proceedings] 1991 IEEE International Joint Conference on Neural Networks}, pp.\  1458--1463 vol.2, 1991.
\newblock \doi{10.1109/IJCNN.1991.170605}.

\bibitem[Schmidhuber(2010)]{Schmidhuber2010}
Schmidhuber, J.
\newblock Formal theory of creativity, fun, and intrinsic motivation (1990-2010).
\newblock \emph{IEEE Transactions on Autonomous Mental Development}, 2, 2010.
\newblock ISSN 19430604.
\newblock \doi{10.1109/TAMD.2010.2056368}.

\bibitem[Schrittwieser et~al.(2019)Schrittwieser, Antonoglou, Hubert, Simonyan, Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, Lillicrap, and Silver]{schrittwieser2019mastering}
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap, T., and Silver, D.
\newblock Mastering atari, go, chess and shogi by planning with a learned model.
\newblock \emph{NATURE}, 2019.
\newblock \doi{10.1038/s41586-020-03051-4}.

\bibitem[Schubert et~al.(2023)Schubert, Zhang, Bruce, Bechtle, Parisotto, Riedmiller, Springenberg, Byravan, Hasenclever, and Heess]{schubert2023generalist}
Schubert, I., Zhang, J., Bruce, J., Bechtle, S., Parisotto, E., Riedmiller, M., Springenberg, J.~T., Byravan, A., Hasenclever, L., and Heess, N.
\newblock A generalist dynamics model for control.
\newblock \emph{arXiv preprint arXiv:2305.10912}, 2023.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sekar et~al.(2020)Sekar, Rybkin, Daniilidis, Abbeel, Hafner, and Pathak]{sekar20aPlan2Explore}
Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D.
\newblock Planning to explore via self-supervised world models.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  8583--8592. PMLR, 13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/sekar20a.html}.

\bibitem[Shyam et~al.(2019)Shyam, Ja{\'{s}}kowski, and Gomez]{pmlr-v97-shyam19a}
Shyam, P., Ja{\'{s}}kowski, W., and Gomez, F.
\newblock Model-based active exploration.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  5779--5788. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/shyam19a.html}.

\bibitem[Sun et~al.(2023)Sun, Dong, Huang, Ma, Xia, Xue, Wang, and Wei]{sun2023retentive}
Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F.
\newblock Retentive network: A successor to transformer for large language models.
\newblock \emph{arXiv preprint arXiv:2307.08621}, 2023.

\bibitem[Sutton(1991)]{Sutton1991dyna}
Sutton, R.~S.
\newblock Dyna, an integrated architecture for learning, planning, and reacting.
\newblock \emph{ACM SIGART Bulletin}, 2, 1991.
\newblock ISSN 0163-5719.
\newblock \doi{10.1145/122344.122377}.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour]{sutton1999REINFORCE}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock In Solla, S., Leen, T., and M\"{u}ller, K. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~12. MIT Press, 1999.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf}.

\bibitem[Tunyasuvunakool et~al.(2020)Tunyasuvunakool, Muldal, Doron, Liu, Bohez, Merel, Erez, Lillicrap, Heess, and Tassa]{tunyasuvunakool2020DMC}
Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S., Merel, J., Erez, T., Lillicrap, T., Heess, N., and Tassa, Y.
\newblock dm\_control: Software and tasks for continuous control.
\newblock \emph{Software Impacts}, 6:\penalty0 100022, 2020.
\newblock ISSN 2665-9638.
\newblock \doi{https://doi.org/10.1016/j.simpa.2020.100022}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S2665963820300099}.

\bibitem[Valevski et~al.(2024)Valevski, Leviathan, Arar, and Fruchter]{valevski2024gameNGen}
Valevski, D., Leviathan, Y., Arar, M., and Fruchter, S.
\newblock Diffusion models are real-time game engines.
\newblock \emph{CoRR}, abs/2408.14837, 2024.
\newblock URL \url{https://doi.org/10.48550/arXiv.2408.14837}.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and kavukcuoglu]{vanDenOord2017vqvae}
van~den Oord, A., Vinyals, O., and kavukcuoglu, k.
\newblock Neural discrete representation learning.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{NIPS2017_attn_is_all_you_need}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Wang et~al.(2024)Wang, Liu, Ye, You, and Gao]{wang2024efficientzero}
Wang, S., Liu, S., Ye, W., You, J., and Gao, Y.
\newblock Efficientzero v2: Mastering discrete and continuous control with limited data.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.
\newblock URL \url{https://openreview.net/forum?id=LHGMXcr6zx}.

\bibitem[Zhang et~al.(2024)Zhang, Wang, Sun, Yuan, and Huang]{zhang2024storm}
Zhang, W., Wang, G., Sun, J., Yuan, Y., and Huang, G.
\newblock Storm: Efficient stochastic transformer based world models for reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\end{thebibliography}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.



% \section{Summary of Changes}
% \paragraph{Observation Spaces}
% REM and other token-based methods \cite{micheli2022transformers} support only image observations.
% In contrast, \AlgName{} natively supports also continuous vectors, categorical variables, 2D categoricals (e.g., categorical image-like grids as in Craftax), and any combination of them.
% The modular design of \AlgName{} allows to easily add or replace encoder-decoder pairs.

% \paragraph{Action Spaces}
% Continuous and multi-discrete action spaces.

% \paragraph{Separate Actor-Critic}


% \paragraph{Training Stabilization}
% Sparse high-magnitude rewards can lead to instabilities in policy optimization.
% Specifically, when an entropy regularizer is used, the amount of regularization is determined by the relationship between the scales of the entropy and policy loss terms.
% The latter is determined by the scale of episodic returns.
% When rewards vary between tasks, both in scale and in sparsity, it affects the relationship between the scales.
% Hence, to maintain a stable relationship, one must either dynamically normalize the returns or dynamically adjust the scale of the entropy regularizer.
% Here, we follow DreamerV3 \cite{hafner2023mastering} and normalize the returns.
% We use running average estimators of the $2.5$ and $97.5$ percentiles based on windows of $500$ batches.

% \paragraph{}


\section{Models and Hyperparameters}
\label{sec:models-and-hyper-params}

\subsection{Hyperparameters}
We detail shared hyperparameters in \autoref{table:shared-hyperparams}, training hyperparameters in \autoref{table:training-hyperparams}, world model hyperparameters in \autoref{table:wm-hyperparams}, and controller hyperparameters in \autoref{table:controller-hyperparams}.
Environment hyperparameters are detailed in \autoref{table:atari-hyperparams} (Atari-100K) and \autoref{table:dmc-hyperparams} (DMC).

\paragraph{Tuning}
Due to limited computational resources, \AlgName{} was not properly tuned. We believe that proper tuning could further improve \AlgName{}'s performance.
% Our algorithm mostly relies on the hyperparameters of REM.
% We believe that additional tuning could further improve \AlgName{}'s performance.

\begin{table}[h]
\caption{Shared hyperparameters.}
\label{table:shared-hyperparams}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lccr}
\toprule
Description & Symbol & Value \\
\midrule
Eval sampling temperature &  & 0.5 \\
Optimizer &  & AdamW \\
Learning rate ($\Tokenizer$, $\WM$, $\Controller$) & & (1e-4, 2e-4, 2e-4) \\
AdamW $\beta_1$ &  & 0.9 \\
AdamW $\beta_2$ &  & 0.999 \\
Gradient clipping threshold ($\Tokenizer$, $\WM$, $\Controller$) & & (10, 3, 3) \\
Weight decay ($\Tokenizer$, $\WM$, $\Controller$) & & (0.01, 0.05, 0.01) \\

\midrule 

Prioritized replay fraction & $\alpha$ & 0.3 \\
$\WM$ ensemble size  & $\ensembleSize$ & 4 \\

\midrule

HL-Gauss num bins  &  & 128 \\
Label smoothing  &  $\sigma$  &  $\frac{3}{4} \text{bin\_width} = 0.1758 $ \\

\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}








\begin{table}[h]
\caption{Training hyperparameters.}
\label{table:training-hyperparams}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lccccr}
\toprule
Description & Symbol & Atari-100K & DMC & Craftax \\
\midrule
Horizon & $\horizon$ & 10 & 20 & 20 \\
Observation sequence length & $\tokensPerObs$ & 64 & 3-24 & 147 \\
Action sequence length & $\tokensPerObs_{\text{a}}$ & 1 & 1-6 & 1 \\
Tokenizer vocabulary size & $\tknzrVocabSize$ & 512 & 125 & (37,5,40,20,4,125) \\
\midrule
Epochs &  & 600 & 1000 & 10000 \\
Experience collection epochs &  & 500 & 1000 & 10000 \\
Environment steps per epoch &  & 200 & 500 & 100 \\
Batch size ($\Tokenizer$, $\WM$, $\Controller$) & & (128, 32, 128) & (-, 16, 128) & (-, 8, 128) \\ 
Training steps per epoch ($\Tokenizer$, $\WM$, $\Controller$) & & (200, 200, 80) & (-, 300, 100) & (-, 100, 50) \\
Training start after epoch ($\Tokenizer$, $\WM$, $\Controller$) & & (5, 25, 50) & (-, 15, 20) & (-, 250, 300) \\


% \midrule

% Environment reward weight & $\extRewardScale$ & 1 & 1 & 100 \\
% Intrinsic reward weight & $\intRewardScale$ & 1 & 10 & 1 \\
% Retention   & $\retnetEta_{\text{min}}, \retnetEta_{\text{max}}$ & 4, 16 \\

\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}








\begin{table}[h]
\caption{World model ($\WM$) hyperparameters.}
\label{table:wm-hyperparams}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lccccr}
\toprule
Description & Symbol & Atari-100K & DMC & Craftax \\
\midrule
Number of layers &  & 10 & 10 & 5 \\
Number of heads & & 4 & 3 & 4 \\
Embedding dimension & $\retnetDmodel$ & 256 & 192 & 256 \\
Dropout & & 0.1& 0.1 & 0.1 \\
Retention decay range   & $[\retnetEta_{\text{min}}, \retnetEta_{\text{max}}]$ & [4, 16] & [2, 2] & [8, 40] \\


% \midrule

% Environment reward weight & $\extRewardScale$ & 1 & 1 & 100 \\
% Intrinsic reward weight & $\intRewardScale$ & 1 & 10 & 1 \\
% Retention   & $\retnetEta_{\text{min}}, \retnetEta_{\text{max}}$ & 4, 16 \\

\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}





\begin{table}[h]
\caption{Actor-critic ($\Controller$) hyperparameters.}
\label{table:controller-hyperparams}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lccccr}
\toprule
Description & Symbol & Atari-100K & DMC & Craftax \\
\midrule
Environment reward weight & $\extRewardScale$ & 1 & 1 & 100 \\
Intrinsic reward weight & $\intRewardScale$ & 1 & 10 & 1 \\
Encoder MLP ($\CLatentFuser$) hidden layer sizes & & [512] & [384] & [512, 512] \\
Shared backbone & & True & False & True \\
Number of quantization values (continuous actions) & &  & 51 & \\
(2D) Categoricals embedding dimension & & & & 64 \\


% \midrule

% Environment reward weight & $\extRewardScale$ & 1 & 1 & 100 \\
% Intrinsic reward weight & $\intRewardScale$ & 1 & 10 & 1 \\
% Retention   & $\retnetEta_{\text{min}}, \retnetEta_{\text{max}}$ & 4, 16 \\

\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}










\begin{table}[h]
\caption{Atari 100K hyperparameters.}
\label{table:atari-hyperparams}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lccr}
\toprule
Description & Symbol & Value \\
\midrule

Frame resolution &  & $64 \times 64$ \\
Frame Skip &  & 4 \\
Max no-ops (train, test) &  & (30, 1) \\
Max episode steps (train, test) &  & (20K, 108K) \\
Terminate on live loss (train, test) &  & (No, Yes) \\


\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}








\begin{table}[t]
\caption{DeepMind Control Suite Proprioception hyperparameters.}
\label{table:dmc-hyperparams}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lccr}
\toprule
Description & Symbol & Value \\
\midrule
Action repeat &  & 2 \\
% Tokens per observation & \tokensPerObs & 64 \\
% Tokenizer vocabulary size & \tknzrVocabSize & 125 \\
% \midrule
% Epochs &  & 1000 \\
% Experience collection epochs &  & 500 \\
% Environment steps per epoch &  & 500 \\


\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}























% \newpage
\clearpage
% \pagebreak
\newpage
\subsection{The Representation Module $\Tokenizer$}
\label{sec:repr-module-additional-details}

\subsubsection{Image Observations}
\label{sec:appendix-vqvae}
Image observations are tokenized using a vector-quantized variational auto-encoder (VQ-VAE) \cite{vanDenOord2017vqvae, esser2021tamingVQGAN}.
A VQ-VAE comprises a convolutional neural network (CNN) encoder, an embedding table $\embTable \in \mathbb{R}^{n \times \retnetDmodel}$, and a CNN decoder.
Here, the size of the embedding table $n$ determines the vocabulary size.

The encoder's output $\tknzrLatent \in \mathbb{R}^{W \times H \times \retnetDmodel}$ is a grid of $W \times H$ multi-channel vectors of dimension $\retnetDmodel$ that encode high-level learned features.
Each such vector is mapped to a discrete token by finding the closest embedding in $\embTable$:
\begin{equation*}
    \token = \arg\min_{i} \Vert \tknzrLatent - \embTable(i) \Vert
\end{equation*}
where $\embTable(i)$ is the $i$-th row of $\embTable$.
To reconstruct the original image, the decoder first maps $\tokens$ to their embeddings using $\embTable$.
During training, the straight-through estimator \cite{bengio2013estimating} is used for backpropagating the learning signal from the decoder to the encoder: $\hat{\tknzrLatent} = \tknzrLatent + \stopGrad(\embTable_{\token} - \tknzrLatent)$.
The architecture of the encoder and decoder models is presented in Table \ref{table:tokenizer-arch}.

The optimization objective is given by
\begin{equation*}
    \mathcal{L}(\encoder, \decoder, \embTable) = \Vert \obs - \decoder(z) \Vert_{2}^{2} + \Vert 
\stopGrad(\encoder(\obs)) - \embTable(z) \Vert_{2}^{2} + \Vert \stopGrad(\embTable(z) - \encoder(\obs) \Vert_{2}^{2} + \mathcal{L}_{\text{perceptual}}(\obs, \decoder(z))
\end{equation*}
where $\mathcal{L}_{\text{perceptual}}$ is a perceptual loss \cite{johnson2016perceptual, pmlr-v48-larsen16}, proposed in \cite{micheli2022transformers}.

Crucially, the learned embedding table $\embTable$ is used for embedding the (image) tokens across all stages of the algorithm. 



\begin{table}[ht]
\caption{The encoder and decoder architectures of the VQ-VAE model. ``Conv(a,b,c)" represents a convolutional layer with kernel size $a \times a$, stride of $b$ and padding $c$. A value of $c=\text{Asym.}$ represents an asymmetric padding where a padding of 1 is added only to the right and bottom ends of the image tensor. ``GN" represents a GroupNorm operator with $8$ groups, $\epsilon=1e-6$ and learnable per-channel affine parameters. SiLU is the Sigmoid Linear Unit activation \cite{hendrycks2017bridging, ramachandran2018searching}. ``Interpolate" uses PyTorch's interpolate method with scale factor of 2 and the ``nearest-exact" mode.}
\label{table:tokenizer-arch}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lcr}
\toprule
Module & Output Shape \\
\midrule

Encoder \\
\midrule

Input & $3 \times 64 \times 64$ \\
Conv(3, 1, 1) & $64 \times 64 \times 64$ \\
EncoderBlock1 & $128 \times 32 \times 32$ \\
EncoderBlock2 & $256 \times 16 \times 16$ \\
EncoderBlock3 & $512 \times 8 \times 8$ \\
GN & $512 \times 8 \times 8$ \\
SiLU & $512 \times 8 \times 8$ \\
Conv(3, 1, 1) & $256 \times 8 \times 8$ \\

\midrule

EncoderBlock  \\
\midrule
Input & $c \times h \times w$ \\
GN & $c \times h \times w$ \\
SiLU & $c \times h \times w$ \\
Conv(3, 2, \text{Asym.}) & $2c \times \frac{h}{2} \times \frac{w}{2}$ \\
% Conv(3, 1, 1) & $2c \times \frac{h}{2} \times \frac{w}{2}$ \\


\midrule
Decoder \\
\midrule
Input & $256 \times 8 \times 8$ \\
BatchNorm & $256 \times 8 \times 8$ \\
Conv(3, 1, 1) & $256 \times 8 \times 8$ \\
DecoderBlock1 & $128 \times 16 \times 16$ \\
DecoderBlock2 & $64 \times 32 \times 32$ \\
DecoderBlock3 & $64 \times 64 \times 64$ \\
GN & $64 \times 64 \times 64$ \\
SiLU & $64 \times 64 \times 64$ \\
Conv(3, 1, 1) & $3 \times 64 \times 64$ \\


\midrule
DecoderBlock \\
\midrule
Input & $c \times h \times w$ \\
GN & $c \times h \times w$ \\
SiLU & $c \times h \times w$ \\
Interpolate & $c \times 2h \times 2w$ \\
Conv(3, 1, 1) & $\frac{c}{2} \times 2h \times 2w$ \\
% Conv(3, 1, 1) & $\frac{c}{2} \times 2h \times 2w$ \\


\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}







\subsubsection{Continuous Vectors}
The quantization of each feature uses $125$ values (vocabulary size) in the range $[-6, 6]$, where $63$ values are uniformly distributed in $[-\ln(1+\pi), \ln(1+\pi)]$ and the rest are uniformly distributed in the remaining intervals.


% \subsubsection{2D Categorical Observations}
% In contrast to other modalities where each token corresponds to a single embedding in the embedding sequence $\tknEmbs$, 2D categoricals map token vectors to embeddings.
% Concretely, 











\clearpage













\newpage
\subsection{The World Model $\WM$}
\label{sec:wm-additional-details}

\paragraph{Embedding Details}
Each token in $\tokens^{(i)}$ of each modality is mapped to a $\retnetDmodel$-dimensional embedding vector $\tknEmbs^{(i)}$ using the embedding (look-up) table $\embTable^{(i)}$ of modality $\modalitySet_i$.
The embedding vector that corresponds to token $\token$ is simply the $\token$-th row in the embedding table.
% Each modality embedding vector $\tknEmb^{(i)}_{t, j}$ is obtained by mapping the corresponding token $\token^{(i)}_{t, j}$ using the embedding table $\embTable^{(i)}$ of modality $\modalitySet_i$.
Formally, $\tknEmb^{(i)}_{t, j} = \embTable^{(i)}(l),\  l=\token^{(i)}_{t, j}$ where $\embTable(l)$ refers to the $l$-th row in $\embTable$.
In the special case of 2D categorical inputs, $\tknEmb^{(i)}_{t, j} = \frac{1}{C} \sum_{n=1}^{C} \embTable^{(i)}_{n}(l_n), \ l_n = \token^{(i)}_{t, j, n}$ where $C$ is the number of channels and $i$ is the index of the 2D categorical modality in $\modalitySet$.
% \todo{improve notations}

To concatenate the embeddings, we use the following order among the modalities: images, continuous vectors, categorical variables, and 2D categoricals.


\paragraph{Prediction Heads}
Each prediction head in $\WM$ is a multi-layer perceptron (MLP) with a single hidden layer of dimension 2$\retnetDmodel$ where $\retnetDmodel$ is the embedding dimension.



\paragraph{Epistemic Uncertainty Estimation}
Working with discrete distributions enables efficient entropy computation and ensures that the ensemble disagreement term $\disagreement_t$ is bounded by $\frac{1}{|\tokens_{t}|} \sum_{\token \in \tokens_{t}} \log(\vocabSize(z))$. 



% \paragraph{Reward Prediction}
% The output head uses $\mathbf{b} = \symlog \left( \frac{\symexp((-15, \ldots, 15))}{100} \right)$.



\subsubsection{Optimization}
\label{sec:appendix-wm-optimization}
For each training example in the form of a trajectory segment in token representation $\tau = \tokens_{1}, \action_{1}, \ldots, \tokens_{H}, \action_{H}$, the optimization objective is given by
\begin{multline*}
    \mathcal{L}_{\WM}(\theta, \phi, \tau) = 
    \sum_{t=1}^{H} \mathcal{L}_{\text{obs}}(\theta, \tokens_{t}, p_{\theta}(\hat{\tokens}_{t} \vert \retnetOutput^{\text{u}}_{t})) 
    + \mathcal{L}_{\text{reward}}(\theta, \reward_{t}, \hat{\reward}_{t}) 
    -\log(p_{\theta}({\doneSgnl}_{t} \vert \retnetOutput^{\text{u}}_{t})) 
    + \sum_{i=1}^{\ensembleSize} \mathcal{L}_{\text{obs}}(\phi_i, \tokens_{t}, p_{\phi_i}(\hat{\tokens}_{t} \vert \stopGrad( \retnetOutput^{\text{u}}_{t})) )
\end{multline*}
% where $\mathcal{L}_{\text{obs}}(\theta, \tknEmbs_{t}, p_{\theta}(\hat{\tokens}_{t} \vert \retnetOutput^{\text{u}}_{t}))$ is the average of the cross-entropy losses of the individual tokens $-\frac{1}{\tokensPerObs}\sum_{(z, \retnetOutputVec)} \log p_{\theta}(z | \retnetOutputVec)$ where gradients are computed w.r.t $\theta$.
where
% \begin{equation*}
%     \mathcal{L}_{\text{obs}}(\theta, \tokens_{t}, p_{\theta}(\hat{\tokens}_{t} \vert \retnetOutput^{\text{u}}_{t})) = -\frac{1}{\tokensPerObs} \sum_{(z, \retnetOutputVec) \in (\tokens_{t}, \retnetOutput^{\text{u}}_{t})} \log p_{\theta}(z | \retnetOutputVec)
% \end{equation*}
\begin{equation*}
    \mathcal{L}_{\text{obs}}(\theta, \tokens_{t}, p_{\theta}(\hat{\tokens}_{t} \vert \retnetOutput^{\text{u}}_{t})) = -\frac{1}{\tokensPerObs} \sum_{i=1}^{\tokensPerObs} \log p_{\theta}(z_i | \retnetOutputVec_i)
\end{equation*}
is the average of the cross-entropy losses of the individual tokens, and $\mathcal{L}_{\text{reward}}(\theta, \reward_{t}, \hat{\reward}_{t})$ is the $\mathcal{L}_{\text{HL-Gauss}}$ loss with the respective parameters of the reward head.
Here, $\retnetOutputVec_i$ is the vector of $\retnetOutput^{\text{u}}_{t}$ that corresponds to $\token_i$, the $i$-th token of $\tokens_{t}$.


\subsubsection{Retentive Networks}
% \paragraph{Retentive Networks}
Retentive Networks (RetNet) \cite{sun2023retentive} are sequence model architectures with a Transformer-like structure \cite{NIPS2017_attn_is_all_you_need}.
However, RetNet replaces the self-attention mechanism with a linear-attention \cite{pmlr-v119-katharopoulos20a} based Retention mechanism.
At a high level, given an input sequence $\tknEmbs \in \mathbb{R}^{|\tknEmbs| \times \retnetDmodel}$ of $\retnetDmodel$-dimensional vectors, the Retention operator outputs
\begin{equation*}
    \Retention(\tknEmbs) = (\retnetQ \retnetK^{\Tr} \odot \retnetD) \retnetV
\end{equation*}
where $\retnetQ, \retnetK, \retnetV$ are the queries, keys, and values, respectively, and $\retnetD$ is a causal mask and decay matrix.
Notably, the softmax operation is discarded in Retention and other linear attention methods.
As a linear attention method, the computation can also be carried in a recurrent form:
\begin{equation*}
\begin{split}
    \Retention(\tknEmb_{t}, \retnetState_{t-1}) & = \retnetState_{t} \retnetQVec_{t} \\
    \retnetState_{t} & = \retnetEta \retnetState_{t-1} + \retnetVVec_{t} \retnetKVec_{t}^{\Tr} \in \mathbb{R}^{\retnetDmodel \times \retnetDmodel}
\end{split}
\end{equation*}
where $\retnetEta$ is a decay factor, $\retnetState_{t}$ is a recurrent state, and $\retnetState_{0} = 0$.
In addition, a hybrid form of recurrent and parallel forward computation known as the chunkwise mode allows to balance the quadratic cost of the parallel form and the sequential cost of the recurrent form by processing the input as a sequence of chunks.
We refer the reader to \cite{sun2023retentive} for the full details about this architecture.

% \todo{describe our configurable decay rate}
In our implementation, since inputs are complete observation-action block sequences $\tknEmbs_{1},\ldots, \tknEmbs_{t}$, we configure the decay factors of the multi-scale retention operator in block units:
\begin{gather*}
    % \retnetEta_{\text{min}} = \log(\frac{1}{\tokensPerObs \tilde{\retnetEta}_{\text{min}}} ) \\
    % \retnetEta_{\text{max}} = \log(\frac{1}{\tokensPerObs \tilde{\retnetEta}_{\text{max}} }) \\
    \retnetEta = 1 - 2^{ - \linspace(\log(\tokensPerObs \retnetEta_{\text{min}}), \log(\tokensPerObs  \retnetEta_{\text{max}}  ), N_h )    }
\end{gather*}
where $\linspace(a, b, c )$ is a sequence of $c$ values evenly distributed between $a$ and $b$, $N_h$ is the number of retention heads, and $\retnetEta_{\text{min}}, \retnetEta_{\text{max}}$ are hyperparameters that control the memory decay in observation-action block units.
















\subsubsection{Parallel Observation Prediction (POP)}
POP \cite{cohen2024improving} is a mechanism for parallel generation of non-causal subsequences such as observations in token representation.
It's purpose is to improve generation efficiency by alleviating the sequential bottleneck caused by generating observations a single token at a time (as done in language models).
However, to achieve this goal, POP also includes a mechanism for maintaining training efficiency.
Specifically, POP extends the chunkwise forward mode of RetNet to maintain efficient training of the sequence model.

To generate multiple tokens into the future at once, POP introduces a set of prediction tokens $\predTokens = \predToken_{1}, \ldots, \predToken_{\tokensPerObs}$ and embeddings $\predEmb \in \mathbb{R}^{\tokensPerObs \times \retnetDmodel}$ where $\tokensPerObs$ is the number of tokens in an observation.
Each token in $\predTokens$ corresponds to an observation token in $\tokens$.
These tokens, and their respective learned embeddings, serve as a learned prior.

Let $\tknEmbs_{1}, \ldots, \tknEmbs_{T}$ be a sequence of $T$ observation-action (embeddings) blocks.
Given $\retnetState_{t-1}$ summarizing all key-value outer products of elements of $\tknEmbs_{\leq t-1}$, the outputs $\retnetOutput^{\text{u}}$ from which the next observation tokens are predicted are given by:
\begin{equation*}
    (\cdot, \retnetOutput^{\text{u}}_{t}) = \seqModel(\retnetState_{t-1}, \predEmb)
\end{equation*}
Importantly, the recurrent state is never updated based on the prediction tokens $\predTokens$ (or their embeddings).
The next observation tokens $\hat{\tokens}_{t}$ are sampled from $p_{\theta}(\hat{\tokens}_{t} | \retnetOutput^{\text{u}}_{t})$.
Then, the next action is generated by the controller, and the next observation-action block $\tknEmbs_{t}$ can be processed to predict the next observation $\hat{\tokens}_{t+1}$.

To maintain efficient training, a two step computation is carried at each RetNet layer.
First, all recurrent states $\retnetState_{t}$ for all $1\leq t \leq T$ are calculated in parallel. Although there is an auto-regressive relationship between time steps, the linear structure of $\retnetState$ allows to calculate the compute-intensive part of each state in parallel and incorporate past information efficiently afterwards.
In the second step, all outputs $\retnetOutput^{\text{u}}_{t}$ for all $1 \leq t \leq T$ are computed in parallel, using the appropriate states $\retnetState_{t-1}$ and $\tknEmbs^{\text{u}}$ in batch computation.
Note that this computation involves delicate positional information handling.
We refer the reader to \cite{cohen2024improving} for full details of this computation.

% \begin{equation*}
    
% \end{equation*}

% For every observation-action block $\tknEmbs_{t}$ and recurrent state $\retnetState_{t-1}$ summarizing the first $t-1$ blocks,
% \begin{equation*}
%     \POPRetention(\tknEmbs_{t}, \retnetState_{t-1}, \predEmb) = \retnetOutput_{t}, \retnetOutput_{t}^{\text{u}}  
% \end{equation*}
% \begin{align*}
%     % \retnetState_{[t]} = & \retnetEta^{\tokensPerObs} \retnetState_{[t-1]} + (\retnetK \odot \zeta)^{\Tr} \retnetV \\
%     \retnetOutput_{t} = & \Retention(\tknEmbs_{t}, \retnetState_{t-1}) \\
%     \retnetOutput_{t}^{\text{u}} = & \Retention(\tknEmbs^{\text{u}}, \retnetState_{t-1}) 
%         % \retnetOutput_{[t]} = & (\retnetQ \retnetK^{\Tr} \odot \retnetD) \retnetV + (\retnetQ \retnetState_{[t-1]}) \odot \xi \\
%         % \retnetOutput_{[t]}^{\text{u}} = & (\retnetQ^{\text{u}} \retnetK^{\text{u} \Tr} \odot \retnetD) \retnetV^{\text{u}} + (\retnetQ^{\text{u}} \retnetState_{[t-1]}) \odot \xi
% \end{align*}
% where $\zeta, \xi$ decay factor correction matrices and
% \begin{equation*}
%     \Retention(\tknEmbs_{t}, \retnetState_{t-1}) = (\retnetQ \retnetK^{\Tr} \odot \retnetD) \retnetV + (\retnetQ \retnetState_{t-1}) \odot \xi
% \end{equation*}


































% \pagebreak
\newpage
\subsection{The Controller $\Controller$}
\label{sec:controller-additional-details}

\paragraph{Critic}
The value prediction uses 128 bins in the range $\mathbf{b} = (-15, \ldots, 15)$.


\paragraph{Continuous Action Spaces}
The policy network outputs $m=51$ logits corresponding to $m$ quantization values uniformly distributed in $[-1, 1]$ for each individual action in the action vector. 



% \paragraph{Training Stabilization}
% Sparse high-magnitude rewards can lead to instabilities in policy optimization.
% Specifically, when an entropy regularizer is used, the amount of regularization is determined by the relationship between the scales of the entropy and policy loss terms.
% The latter is determined by the scale of episodic returns.
% When rewards vary between tasks, both in scale and in sparsity, it affects the relationship between the scales.
% Hence, to maintain a stable relationship, one must either dynamically normalize the returns or dynamically adjust the scale of the entropy regularizer.
% Here, we follow DreamerV3 \cite{hafner2023mastering} and normalize the advantage using return scale estimates.
% We use running average estimators of the $2.5$ and $97.5$ return percentiles based on windows of $500$ batches.


\subsubsection{Input Encoding}
\label{sec:appendix-C-input-encoding}
The controller $\Controller$ operates in the latent token space.
Token trajectories $\tau = \tokens_{1}, \actionTokens_{1}, \ldots, \tokens_{H}, \actionTokens_{H}$ are processed sequentially by the LSTM model.
At each time step $t$, the network gets $\tokens_{t}$ as input, outputs $\policy(\action_{t} \vert \tau_{\leq t-1}, \tokens_{t})$ and $\valueFn(\action_{t} \vert \tau_{\leq t-1}, \tokens_{t})$, samples an action $\action_{t}$ and then process the sampled action as another sequence element.

The processing of actions involve embedding the action into a latent vector which is then provided as input to the LSTM.
Embedding of continuous action tokens is performed by first reconstructing the continuous action vector and then computing the embedding using a linear projection.
Discrete tokens are embedded using a dedicated embedding table.

% The embedding of observations involves applying modality-specific encoders which produce a single latent vector for each modality, concatenating the latents, and mapping through a MLP $\CLatentFuser$ to obtain the unified representation.

To embed observation tokens $\tokens$, the tokens of each modality are processed by a modality-specific encoder.
The outputs of the encoders are concatenated and further processed by a MLP $\CLatentFuser$ that combines the information into a single vector latent representation.

The image encoder is a convolutional neural network (CNN). Its architecture is given in \autoref{table:actor-critic-obs-encoder-arch}.

Categorical variables are embedded using a learned embedding table.
For 2D categoricals, shared per-channel embedding tables map tokens to embedding vectors, which are averaged to obtain a single embedding for each multi-channel token vector. 
For both types of categorical inputs we use 64 dimensional embeddings.
The embeddings are concatenated and processed by $\CLatentFuser$.


\begin{table}[h]
\caption{The image observation encoder architecture of the actor-critic controller $\Controller$.}
\label{table:actor-critic-obs-encoder-arch}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lcr}
\toprule
Module & Output Shape \\
\midrule

Input & $256 \times 8 \times 8$ \\
Conv(3, 1, 1) & $128 \times 8 \times 8$ \\
SiLU & $128 \times 8 \times 8$ \\
Conv(3, 1, 1) & $64 \times 8 \times 8$ \\
SiLU & $64 \times 8 \times 8$ \\
Flatten & 4096 \\
Linear & 512 \\
SiLU & 512 \\



\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}





\subsubsection{Optimization}
\label{sec:appendix-controller-optimization}
$\lambda$-returns are computed for each generated trajectory segment $\hat{\tau} = (\tokens_{1}, \action_{1}, \bar{\reward}_{1}, \doneSgnl_{1}, \hat{\tokens}_{2}, \action_{2}, \bar{\reward}_{2}, \doneSgnl_{2}, \ldots, \hat{\tokens}_{H}, \action_{H}, \bar{\reward}_{H}, \doneSgnl_{H})$:
\begin{equation*}
    G_{t} = \begin{cases}
        \bar{\reward}_{t} + \discountF (1-\doneSgnl_t)( (1-\lambda) \valueFn_{t+1} + \lambda G_{t+1} ) & t<H \\
        \valueFn_{H} & t=H
    \end{cases}
\end{equation*}
where $\valueFn_{t} = \valueFn(\hat{\tau}_{\leq t})$.
These $\lambda$-returns are used as targets for critic learning.
For policy learning, a REINFORCE \cite{sutton1999REINFORCE} objective is used, with a normalized $\valueFn$ baseline for variance reduction:
\begin{equation*}
    \mathcal{L}_{\policy}(\CParams) = \E_{\policy}\left[ \sum_{t=1}^{H} \stopGrad\left( \frac{G_{t} - \valueFn_{t}}{\max(1, c)} \right) \log \policy(\action_{t} \vert \hat{\tau}_{\leq t-1}, \hat{\tokens}_{t})  + \CentropyWeight \mathcal{H}(\policy(\action_{t} \vert \hat{\tau}_{\leq t-1}, \hat{\tokens}_{t})) \right]
\end{equation*}
where $c$ is an estimate of the effective return scale similar to DreamerV3 \cite{hafner2023mastering} and $\CentropyWeight$ is a hyperparameter that controls the entropy regularization weight.
$c$ is calculated as the difference between the running average estimators of the $97.5$ and $2.5$ return percentiles, based on a window of return estimates obtained in the last $500$ batches (imagination).


























\newpage
\section{Implementation Details}
\paragraph{Code}
We \href{https://github.com/leor-c/M3}{open-source our code and trained model weights}.
Our code is written in Pytorch \cite{Paszke2019Pytorch}.
All code was developed by Lior Cohen.
Experiments were run by Lior Cohen, Bingyi Kang, and Uri Gadot.

\paragraph{Hardware} 
All Atari and DMC experiments were performed on V100 GPUs, while for Craftax a single RTX 4090 was used.
% We defer the full details of the experimental setup to Appendix \ref{sec:models-and-hyper-params}.
% \todo{move to Appendix?} All experiments were conducted on [hardware setup]. The model was trained using [optimizer] with a learning rate of [value].

\paragraph{Run Times}
Experiments on Atari require approximately 12 hours on an RTX 4090 GPU and around 29 hours on a V100 GPU. For DMC, the runtime is about 40 hours on a V100 GPU. Craftax runs take roughly 94 hours, equivalent to 3.9 days.





\paragraph{Craftax}
The official environment provides the categorical variables in one-hot encoding format.
Our implementation translates these variables to integer values which can be interpreted as tokens.

\paragraph{Setup in Atari Freeway}
% Following prior works \cite{micheli2022transformers, cohen2024improving}, we used a special configuration for Freeway.
% Specifically, the sampling temperature of the agent is modified from 1 to 0.01, a heuristic that guides the agent towards non-zero reward trajectories.
% We highlight that different methods use other mechanisms such as epsilon-greedy schedules and ``argmax" action selection policies to overcome this exploration challenge \cite{micheli2022transformers}.
For the Freeway environment, we adopted a modified sampling strategy where a temperature of 0.01 is used instead of the standard value of 1, following \cite{micheli2022transformers, cohen2024improving}. 
This adjustment helps directing the agent toward rewarding paths. 
Note that alternative approaches in the literature tackle the exploration challenge through different mechanisms, including epsilon-greedy exploration schedules and deterministic action selection via argmax policies \cite{micheli2022transformers}.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
