\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{makecell} 
\usepackage{array}    
\usepackage{hyperref}
\usepackage{enumitem}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\aux}[1]{{\scriptsize{\textcolor{gray}{#1}}}}

\usepackage[accepted]{preprint}
\usepackage{bbm}  

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\input{macros}

\usepackage[capitalize,noabbrev]{cleveref}

\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\newcommand{\redtodo}[1]{\textcolor{red}{#1}}

\newcommand{\lie}[1]{\textcolor{blue}{#1}}
\newcommand{\lichao}[1]{\textcolor{green}{#1}}

\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}
\usepackage{mdframed}

\definecolor{teaserblue}{RGB}{242, 242, 255}

\theoremstyle{plain}

\newtheorem{proposition}[theorem]{Proposition}

\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{graphicx} 
\newcommand{\signal}{s}
\newcommand{\uwsignal}{s_u}
\newcommand{\wsignal}{s_w}
\newcommand{\fsignal}{\hat{s_w}}
\newcommand{\spect}{C}
\newcommand{\wm}{w}
\newcommand{\amp}{a}
\newcommand{\phase}{p}
\newcommand{\encoder}{\texttt{Enc}}
\newcommand{\decoder}{\texttt{Dec}}
\newcommand{\detector}{\texttt{Det}}
\newcommand{\mean}[1]{\overline{#1}}
\newcommand{\ba}{\texttt{BA}}
\newcommand{\stft}{\texttt{STFT}}
\newcommand{\istft}{\texttt{ISTFT}}

\usepackage[textsize=tiny]{todonotes}
\newcommand{\oursPlain}{XAttnMark}
\newcommand{\ours}{\textsc{\oursPlain}\xspace}

\newcommand{\thistitle}{
\raisebox{-0.2em}{\includegraphics[height=1em]{figures/sword.png}}
  \textsc{XAttnMark}: Learning Robust Audio {W}atermarking with Cross-Attention
    }

\icmltitlerunning{ \ours: Learning Robust Audio {W}atermarking with Cross-Attention}

\usepackage{xspace} 

\newcommand{\ploss}{TF-Loudness\xspace} 

\newcommand{\wavmark}{WavMark\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}

\begin{document}

\twocolumn[
\icmltitle{
\thistitle
}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yixin Liu}{lehigh,dolby}
\icmlauthor{Lie Lu}{dolby}
\icmlauthor{Jihui Jin}{dolby}
\icmlauthor{Lichao Sun}{lehigh}
\icmlauthor{Andrea Fanelli}{dolby}
\end{icmlauthorlist}

\icmlaffiliation{lehigh}{Department of Computer Science, Lehigh University, Bethlehem, PA, USA}
\icmlaffiliation{dolby}{Dolby Laboratories Inc., San Francisco, CA, USA}

\icmlcorrespondingauthor{Yixin Liu}{yila22@lehigh.edu; ydliu@dolby.com}
\icmlcorrespondingauthor{Lie Lu}{llu@dolby.com}
\icmlcorrespondingauthor{Jihui Jin}{jihui.jin@dolby.com}
\icmlcorrespondingauthor{Lichao Sun}{lis221@lehigh.edu}
\icmlcorrespondingauthor{Andrea Fanelli}{andrea.fanelli@dolby.com}

\icmlkeywords{Audio Watermarking, Deep Learning, Attribution, Robustness}

\vskip 0.3in
]

\printAffiliationsAndNotice{This work was done during Yixin Liu's internship at Dolby Laboratories Inc.}  

\begin{abstract}
    
The rapid proliferation of generative audio synthesis and editing technologies has raised significant concerns about copyright infringement, data provenance, and the spread of misinformation through deepfake audio. Watermarking offers a proactive solution by embedding imperceptible, identifiable, and traceable marks into audio content. While recent neural network-based watermarking methods like WavMark and AudioSeal have improved robustness and quality, they struggle to achieve both robust detection and accurate attribution simultaneously. This paper introduces \underline{Cross}-\underline{Att}entio\underline{n} Robust Audio Water\underline{mark} (\ours), that bridges this gap by leveraging partial parameter sharing between the generator and the detector, a cross-attention mechanism for efficient message retrieval, and a temporal conditioning module for improved message distribution. Additionally, we propose a psychoacoustic-aligned temporal-frequency masking loss that captures fine-grained auditory masking effects, enhancing watermark imperceptibility. Our approach achieves state-of-the-art performance in both detection and attribution, demonstrating superior robustness against a wide range of audio transformations, including challenging generative editing with strong editing strength. Project webpage is available at \href{https://liuyixin-louis.github.io/xattnmark/}{https://liuyixin-louis.github.io/xattnmark/}. 
\end{abstract}

% \vspace{-3em}
\section{Introduction}
\label{sec:intro}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser-v1.pdf}
    \caption{
        Quality-attribution performance trade-off curve across different watermarking strengths and the overall performance comparison on detection and attribution tasks. 
        Higher values on both axes indicate better performance.
    }
    \label{fig:trade_off}
\end{figure}

With the rapid development of generative audio synthesis and editing techniques, anyone can now easily edit and re-synthesize audio content \cite{openai2024sora,li2024quality,copet2024simple}. While it democratizes the creative process and enables new applications, it also brings serious concerns for copyrighted data misuse, data provenance and authenticity \cite{pan2023risk, shoaib2023deepfakes, park2023ai}. A notable example is the recent surge of deepfake audio and video, where actors have been using deepfake techniques to impersonate and create fake speech and video content of online politicians or public figures, with the malicious intent to spread misinformation and manipulate public opinion \cite{verma2024ai,wenger2021hello,buo2020emerging,bilika2023hello}. Furthermore, beyond deepfake threats, the unauthorized exploitation of copyrighted content is also a growing concern in the AI industry \cite{singer2024deepfake, qiwei2024reporting, brigham2024violation}. These days, many content creators fall victim of copyright infringement due to the unauthorized use of their content for AI training and editing \citep{uscopyright2023ai,abbott2023disrupting}. Original content is now being exploited and modified at scale in a way that makes it hard to track data provenance \cite{cho2024artists, robinson2024umg, vermillio2024wme}. Among the various solutions to track audio provenance and guarantee artists' protection \cite{ren2024copyright,desai2024copyright,Liu_2024_CVPR},  watermarking stands out as one of the most effective proactive approaches.  It involves embedding imperceptible perturbations into the audio that are both identifiable and traceable. Watermarking enables two key processes: \underline{detection}, which verifies the presence of the watermark in an audio file, and \underline{attribution}, which involves decoding a message that uniquely identifies the original creator. 

Initialized by WavMark \cite{chen2023wavmark} and the seminal work of AudioSeal \cite{san2024proactive}, using end-to-end deep neural networks for learning to watermark audio content has demonstrated stronger robustness with minor quality degradation compared to the state-of-the-art hand-crafted watermarking method \cite{Westerfeld_audiowmark}. This is especially true under more challenging audio editing transformations, like EnCodec \citep{defossez2022high}. WavMark proposes to use an invertible neural network architecture with a 16-bit synchronization code and a 16-bit message code to jointly conduct detection and attribution. However, the brute-force decoding approach in WavMark is inefficient, and the invertible architecture limits the watermarking capacity under more challenging transformations \citep{chen2023wavmark}. Further work is represented by AudioSeal \citep{san2024proactive}, which utilizes a disjointed generator-detector framework with a detection head and a message decoding head split to improve robustness against advanced transformations. Nevertheless, AudioSeal improves detection at the cost of reduced accuracy in attribution. In a nutshell, \emph{whether the neural-network-based watermarking can achieve both robust detection and attribution is still an unsolved puzzle}. 

This paper identifies the following key factors to bridge the gap: first, motivated by the shared-parameter architecture advantage of WavMark for boosting learning efficiency, and the disjointed generator-detector architecture of AudioSeal for robust capability, we introduce a blended architecture of partial parameter sharing between generator and detector, which jointly achieve both efficient learning and robustness. Specifically, we propose a cross-attention module that leverages a shared embedding table to facilitate message decoding in the detector part. Secondly, we design a simple yet effective conditioning mechanism that distributes the message temporally before injection, which further improves the learning efficiency. With these two key components, we obverse significant boost in both robust detection and attribution performance. Furthermore, to enforce stronger constraints for watermark imperceptibility, we introduce a new per-tile temporal-frequency (TF) masking loss. Specifically, we first compute masking energy with an asymmetric 2D kernel, identify the masked regions and then use the masking energy as a weighting factor for computing a TF-weighted $\ell_2$ loss in the mel-spectrogram domain. With these efforts, we demonstrate state-of-the-art robustness across a wide range of audio editing transformations, while preserving superior perceptual quality (see Figure~\ref{fig:trade_off}). 
Furthermore, under the more challenging task of generative model editing, we demonstrate that \ours is the only watermarking approach that can conduct watermark detection even when edits of strong strength are applied. We summarize our main contributions as follows:

\begin{itemize}
    \item Blending the architecture advantages of previous works, we introduce partial parameter sharing between the neural generator and the detector, with an embedding table as the bridge and a cross-attention module in the detector as the core, to allow for more efficient learning and accurate message retrieval. Furthermore, we introduce a simple yet effective message conditioning module that distributes the latent message temporally, boosting the attribution learning efficiency. 
    
    \item To increase the watermarking quality, we introduce a new psychoacoustic-inspired temporal-frequency (TF) masking loss that captures per-tile masking effects.  We compute masking energy with an asymmetric 2D kernel, identify the masked TF tiles, and  downweight these tiles with a TF-weighted $\ell_2$ loss, achieving more imperceptible watermarking. 
    \item We empirically show that our approach can achieve state-of-the-art performance in both detection and attribution with comparable perceptual quality and superior robustness. Furthermore, testing in a zero-shot manner on unseen generative editing transformations, we show that our approach is the only one that can still successfully conduct watermark detection even with strong editing strength.
\end{itemize}

\section{Related Work}
\label{sec.related}

\header{Audio Watermarking} Audio watermarking has evolved significantly from traditional signal processing to modern deep learning approaches. Early rule-based methods focused on embedding watermarks in time or frequency domains through hand-crafted techniques \cite{zhang2020time, hu2020selection, zhang2017robust, qin2022lattice}. A notable example is AudiowMark \cite{Westerfeld_audiowmark}, which embeds a 128-bit message using convolutional coding and selective frequency band modifications. While sophisticated, these hand-crafted approaches often struggle with robustness against challenging transformations like neural audio codecs \citep{defossez2022high}. Deep neural networks (DNNs) have enabled more robust end-to-end watermarking systems that can generalize to unseen transformations \citep{san2024proactive,chen2023wavmark,liu2023detecting}. WavMark \cite{chen2023wavmark} introduced an invertible neural architecture for joint detection and attribution with 16-bit synchronization codes. While achieving strong performance, its brute-force decoding and architectural constraints limit scalability. AudioSeal \cite{san2024proactive} addressed these limitations with a generator-detector design with separate detection and message decoding. However, this improved detection came at the cost of attribution accuracy. Our work focuses on enabling both robust detection and accurate attribution through a more efficient architecture design with psychoacoustic-inspired quality loss.

\header{Source Attribution} A central objective in copyright protection is the ability to trace and verify the origin of creative works, which remains challenging in the realm of generative audio. Recent efforts have highlighted the necessity of robust source attribution mechanisms that work reliably across different transformations. For instance, Agnew et al.~\cite{agnew2024sound} performed an extensive audit of popular audio datasets and revealed serious intellectual property infringements, underscoring the urgency for transparent dataset documentation and reliable authorship checks. In the music domain specifically, Barnett et al.~\cite{barnett2024exploring} advanced source attribution by leveraging audio embeddings to identify influential training data in generative music models, enabling a more transparent ``musical roots'' analysis. Such embedding-based similarity checks align with the broader push for dataset auditing, as reflected in Du et al.~\cite{du2024sok}, who argue for holistic copyright auditing mechanisms throughout the existing machine learning processes. In this work, we propose a neural watermarking system that achieves SoTA message decoding performance, which is an essential step toward robust source attribution.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/archs-jan16.pdf}
    \caption{
        \textbf{System Diagram for \ours.} \ours consists of a watermark generator and a watermark detector, with a shared embedding table that facilitates message decoding through a cross-attention module. In the generator part, we first employ an encoder network to encode the audio latent and then apply a temporal modulation to hide the message. The modulated latent is then fed into a decoder to produce the watermark residual. In the detector part, a linear detection head is used for detecting the presence of watermarks, and a cross-attention module with the shared embedding table is used for message decoding. 
    }
    \label{fig:framework}
\end{figure*}

\section{Preliminaries}
\label{sec:prelim}

\header{Audio Watermarking} Audio watermarking systems typically comprise two primary components: a generator \(\mathcal{G}\) to embed watermark, and a detector \(\mathcal{D}\) for recovering them. Let \(\boldsymbol{x} \in \mathbb{R}^T\) be an audio signal of length \(T\), and let \(\boldsymbol{w} \in \{0,1\}^K\) be a binary watermark sequence. The generator \(\mathcal{G}: \mathbb{R}^T \times \{0,1\}^K \rightarrow \mathbb{R}^T\) outputs a watermarked signal \(\boldsymbol{x}_w = \mathcal{G}(\boldsymbol{x}, \boldsymbol{w})\), which should ideally preserve the audio's perceptual quality. The detector \(\mathcal{D}\) is then responsible for two tasks. First, its detection head \(\mathcal{D}^{\mathrm{det}}: \mathbb{R}^T \rightarrow [0,1]\) produces a probability indicating whether an input contains a valid watermark, which yields a final decision of presence or absence with a threshold. Second, its decoder head \(\mathcal{D}^{\mathrm{msg}}: \mathbb{R}^T \rightarrow [0,1]^K\) returns a vector indicating the probability of each bit being 1, which can be thresholded to reconstruct the embedded message. In practical settings, the watermarked audio may undergo various transformations \(\mathcal{T}(\cdot)\) such as compression and cropping, resulting in a distorted signal \(\boldsymbol{x}_w^T = \mathcal{T}(\boldsymbol{x}_w)\). Across different transformations, an ideal detector should be able to robustly detect and decode watermarks. In essence, the detection head should output probabilities close to 1 for watermarked signals (and close to 0 otherwise), while the decoder head should recover a bit sequence \(\tilde{\boldsymbol{w}}\) matching \(\boldsymbol{w}\). In this paper, we focus on the per-sample level detection and attribution, which is a common and practical setting in real-world watermarking applications \cite{liu2024audiomarkbench}.

\header{Learning to Watermark with Two-Headed Detector} We first present the general formulation of a neural watermarking system with detection and decoding separation. 
A neural watermarking system is parameterized by \(\Theta = (\Theta_\mathcal{G}, \Theta_\mathcal{D})\), where \(\Theta_\mathcal{G}\) denotes the generator parameters and \(\Theta_\mathcal{D}\) denotes the detector parameters. During training, the detection head typically first produces a logit for each audio input, which is then passed through a sigmoid function to yield a probability \(p(\boldsymbol{x}) = \mathcal{D}^{\mathrm{det}}_{\Theta}(\boldsymbol{x}) \in [0,1]\). For a watermarked signal \(\boldsymbol{x}_w^T\) and an unwatermarked signal \(\boldsymbol{x}^T\), the detection head's objective is to correctly classify both, often formulated by maximizing the following expected log-likelihood:
\[
    \max_{\Theta_{\mathcal{D}^{\mathrm{det}}}} \Bigl( 
    \underset{\boldsymbol{x}, \boldsymbol{w}, \mathcal{T}}{\mathbb{E}}\bigl[\log \mathcal{D}^{\mathrm{det}}(\boldsymbol{x}_w^T)\bigr] 
    \;+\; 
    \underset{\boldsymbol{x}, \mathcal{T}}{\mathbb{E}}\bigl[\log\bigl(1 - \mathcal{D}^{\mathrm{det}}(\boldsymbol{x}^T)\bigr)\bigr] 
    \Bigr),
\]
where \(\boldsymbol{x}_w = \mathcal{G}(\boldsymbol{x}, \boldsymbol{w})\) and \(\boldsymbol{x}_w^T = \mathcal{T}(\boldsymbol{x}_w)\). During the inference time, the threshold \(\tau\) is usually tuned on the validation set to control a certain level of false positive rate. 

Meanwhile, the decoder head produces \(K\) probabilities \(\mathcal{D}^{\mathrm{msg}}(\boldsymbol{x}_w^T) = [\mathcal{D}^{\mathrm{msg}}(\boldsymbol{x}_w^T)_1, \dots, \mathcal{D}^{\mathrm{msg}}(\boldsymbol{x}_w^T)_K]\), where each entry indicates the likelihood of a particular bit being 1. Its training objective is to maximize the probability of correctly predicting each bit of \(\boldsymbol{w}\), which can be expressed as:
\begin{equation}
    \max_{\Theta_{\mathcal{D}^{\mathrm{msg}}}} \underset{\boldsymbol{x}, \boldsymbol{w_k}, \mathcal{T}}{\mathbb{E}}\bigl[\boldsymbol{w}_{k}\log \mathcal{D}^{\mathrm{msg}}(\boldsymbol{x}^T_w)_{k} + (1-\boldsymbol{w}_{k})\log(1 - \mathcal{D}^{\mathrm{msg}}(\boldsymbol{x}^T_w)_{k})\bigr].
\end{equation}
By thresholding each \(\mathcal{D}^{\mathrm{msg}}(\boldsymbol{x}_w^T)_k\) at around 0.5, the decoded bit string \(\hat{\boldsymbol{w}}\) is obtained, with the learning objective seeking \(\hat{\boldsymbol{w}} = \mathbf{w}\) despite the distortion layer \(\mathcal{T}\) over \(\mathbf{x}_w\). 

\subsection{Overview of AudioSeal \citep{san2024proactive}}
\label{subsec:audioseal-review}

To contextualize our architectural improvements, we first analyze AudioSeal's framework, which pioneered the disjointed generator-detector paradigm for neural watermarking. While it achieves strong detection robustness, its attribution limitations motivate key aspects of \ours.

\header{Disjointed Generator--Detector Architecture} AudioSeal employs two separate networks for watermark generation and detection: given audio $\boldsymbol{x}$, the generator $\mathcal{G}=\{\mathcal{E}_{\text{gen}}, \mathcal{J}_{\text{gen}}\}$ is composed of an encoder $\mathcal{E}_{\text{gen}}$ that encodes the audio into compact latent $\boldsymbol{h_x}$, and a decoder $\mathcal{J}_{\text{gen}}$ that decodes the latent into the watermarked perturbation. The audio latent is obtained via $\boldsymbol{h_x} = \mathcal{E}_{\text{gen}}(\boldsymbol{x}) \in \mathbb{R}^{t' \times H}$, where $t'=\lfloor T/\alpha \rfloor$, $\alpha$ is the temporal downsampling factor, and $H$ is the latent dimension. The secret message $w$ is injected into the latent space with a message encoder $\mathcal{M}$ that maps $\boldsymbol{w}$ to its latent $\boldsymbol{h}_w = \mathcal{M}(\boldsymbol{w})$. The modulated latent waveform is obtained via direct addition $\boldsymbol{h}(\boldsymbol{x}, \boldsymbol{w}) = \boldsymbol{h}_x + \boldsymbol{h}_w$, following with the decoder network $\mathcal{J}_{\text{gen}}$ to produce the predicted watermarked perturbation $\boldsymbol{\delta}_{\boldsymbol{w}} = \mathcal{J}_{\text{gen}}(\boldsymbol{h}(\boldsymbol{x}, \boldsymbol{w})) \in \mathbb{R}^{T \times 1}$, which is later applied on the original audio to produce the watermarked audio $\boldsymbol{x}_w = \boldsymbol{x} + \boldsymbol{\delta}_{\boldsymbol{w}}$. 
Note that the decoder model $\mathcal{J}_{\text{gen}}$ shares a symmetric structure with the encoder $\mathcal{E}_{\text{gen}}$, with layers of residual transposed convolution for temporal upsampling. The watermark detector $\mathcal{D}= \{\mathcal{E}_{\text{det}}, \mathcal{O}^{\mathrm{dec}}\}$ is composed of an encoder $\mathcal{E}_{\text{det}}$ (sharing similar structure with $\mathcal{E}_{\text{gen}}$ but with an additional padding to match original length $T$) and a unified decoding head $\mathcal{O}^{\mathrm{dec}} \in \mathbb{R}^{H \times (1+K)} $ for both detection and message decoding. Given potential watermarked audio $\boldsymbol{x}_w^T$, the encoder $\mathcal{E}_{\text{det}}$ first processes the audio to obtain the latent $\boldsymbol{{\tilde{h}}}_{x} = \mathcal{E}_{\text{det}}(\boldsymbol{x}_w^T) \in \mathbb{R}^{T \times H}$. Then the decoding head $\mathcal{O}^{\mathrm{dec}}$ is applied on the latent to obtain the logits for both detection and message decoding, i.e., $ \mathcal{O}^{\mathrm{dec}}(\boldsymbol{{\tilde{h}}}_{x}) \in \mathbb{R}^{T \times (1+K)}$. 

\header{Multi-bit Message Conditioning $\mathcal{M}$} Similar to the common practice of using learned embeddings for conditional generation \citep{peebles2023scalable}, AudioSeal uses an embedding table of only $2K$ entries, $\mathbf{E} \in \mathbb{R}^{2K \times H}$, to encode the total $2^K$ possible message states, which significantly reduces the storage complexity. Specifically, each bit $w_j \in \{0,1\}$ maps to index $I_j = 2j + w_j$, allowing separate representations for 0/1 states in each bit position with distinct embedding vectors. 
Given the position sequence $\boldsymbol{p}=[I_1, \dots, I_K]$, the retrieved $K$ embedding vector sequence is $\mathbf{V}(\boldsymbol{w}) = [\mathbf{E}_{I_1}, \dots, \mathbf{E}_{I_K}] \in \mathbb{R}^{K \times H}$. Then an average pooling operation on $\mathbf{V}(\boldsymbol{w})$ is applied to obtain the final message latent $\boldsymbol{h}_w = \frac{1}{K} \sum_{j=1}^K \mathbf{E}_{I_j} \in \mathbb{R}^{H}$. The message latent is repeated along the temporal axis and then added to the waveform latent, leading $\boldsymbol{h}(\boldsymbol{x}, \boldsymbol{w}) = \boldsymbol{h}_x + \boldsymbol{h}_w$. 

\section{Methodology}
\label{sec:method}

Despite AudioSeal achieving robust detection, the model struggles to perform accurate message decoding even without distortion (See Table \ref{tab:full_standard_edit_detection}). In this work, following the architecture backbone of AudioSeal, we identify and resolve two architectural limitations: disjoint generator-detector and the information bottleneck caused by embedding mean-pooling. Specifically, we first propose a cross-attention generator-detector watermarking system with a shared embedding table and temporal modulation to improve learning efficiency in message decoding. Then, to further improve watermark quality, we propose a psychoacoustic-aligned temporal-frequency masking $\ell_2$ loss. The overall framework is shown in Figure \ref{fig:framework}.

\subsection{Cross-Attention Generator-Detector Watermarking System with Shared Embedding Table}
\label{sec:architecture}
We observe that the fully disjointed architecture of AudioSeal ($\Theta_\mathcal{G} \neq \Theta_\mathcal{D}$) often converges fast for the watermark detection learning but struggles to learn the message decoding part efficiently and accurately. On the other hand, the full parameter-sharing architecture of WavMark ($\Theta_\mathcal{G} = \Theta_\mathcal{D}$) can achieve superior efficiency in learning both detection and message decoding but lacks enough robustness capability against various distortions (See App. \ref{app:training_dynamics}). This motivates us to explore a blended architecture with partial parameter sharing between the generator and the detector, which has the potential to boost message decoding learning while preserving robust capability. 

Our key design is to share the message conditioning module $\mathcal{M}$ between the generator and the detector, which helps bridge the information flow between how the message is composed and how it can be reconstructed in the detector part. Specifically, the learnable part of the embedding table $\mathbf{E}$ in $\mathcal{M}$, which serves as the fundamental vector set for composing the message latent in the generator part, is now utilized as a reference when decomposing the latent to retrieve the message bits in the detector. To achieve this, we propose leveraging a cross-attention block \citep{vaswani2017attention} to use the embedding table $\mathbf{E}$ as a (key-value) reference for message decoding, given the query audio latent $\boldsymbol{\tilde{h}}_{x}$. We describe this module in detail in the following section. 

\header{Message Decoding with Cross-Attention}
Ideally, for the given embedding table $\mathbf{E}$ with $2K$ entries and a watermarked audio $\boldsymbol{x}_w = \boldsymbol{x} + \mathcal{G}(\boldsymbol{x}, \boldsymbol{w})$, we want to reconstruct the original $K$ embedding vectors $[\boldsymbol{E}_{I_1}, \dots, \boldsymbol{E}_{I_K}]$, that were used to compose the ground-truth message latent $\boldsymbol{h}_w$ and then feed them as context for the message decoding. To achieve this, we use an attention mechanism that transforms the embedding table into key and value using two different linear projections, and does attention-based vector merging with a query from the reconstructed latent representation $\boldsymbol{\tilde{h}}_{x}$. Specifically, since the nearby two entries in $\mathbf{E}$ represent one position with different bit states, merging them into one $2H$-dim vector represents the latent of each position. We transform this reshaped embedding matrix $\mathbf{E}' \in \mathbb{R}^{K \times 2H}$ into $\mathbf{K}, \mathbf{V}$ with two linear projections $\mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{2H \times H}$, that is $\mathbf{K} = \mathbf{E}' \mathbf{W}_K$ and $\mathbf{V} = \mathbf{E}' \mathbf{W}_V$. Then, we ``demodulate'' along the temporal axis the reconstructed waveform latent  $\boldsymbol{\tilde{h}}_{x} \in \mathbb{R}^{T \times H}$, to obtain the first version of raw prediction $\mathbf{\tilde{V}}_x$ for the original $K$ components. To do this, we first use a linear projection $\mathbf{W}_{dem} \in \mathbb{R}^{T \times K}$ for obtaining the query sequence $\mathbf{\tilde{h}}_{x}^{dem} = \mathbf{W}_{dem}^T \mathbf{\tilde{h}}_{x} \in \mathbb{R}^{K \times H}$ and follow with a linear query projection $\mathbf{W}_Q \in \mathbb{R}^{H \times H}$. Then we can utilize the embedding table $\mathbf{E}$ to further refine the final component prediction $\mathbf{\tilde{V}}_{x}$ with the following cross-attention mechanism:
\begin{equation}
\begin{aligned}
    \mathbf{Q} &= \mathbf{\tilde{h}}_{x}^{dem}\,\mathbf{W}_Q \in \mathbb{R}^{K \times H }, \\
    \mathbf{K} &= \mathbf{E}'\,\mathbf{W}_K \in \mathbb{R}^{K \times H }, \\
    \mathbf{V} &= \mathbf{E}'\,\mathbf{W}_V \in \mathbb{R}^{K \times H }, \\
    \mathbf{A} &= \mathrm{softmax}\!\Bigl(\frac{\mathbf{Q}\,\mathbf{K}^\top}{\sqrt{H}}\Bigr) \in \mathbb{R}^{K \times K}, \\
    \mathbf{\tilde{V}}_{x} &= \mathrm{act}\bigl(\mathbf{A}\,\mathbf{V}\bigr) \in \mathbb{R}^{K \times H},
\end{aligned}
\end{equation}
where $H$ is the latent dimension, $K$ is the number of message bits, $t$ is the temporal dimension, and $\mathrm{act}(\cdot)$ can be any activation function. In this study, we use exponential linear units (ELU) \cite{clevert2015fast}. With the predicted $\mathbf{\tilde{V}}_{x}$, we further employ a linear projection layer $\mathbf{W}_{dec} \in \mathbb{R}^{H \times 1}$ with sigmoid activation function for the construction of the message decoding head. The final logit for message prediction bit is then obtained as $\hat{\boldsymbol{w}}_{x} = \sigma\bigl( \mathbf{\tilde{V}}_{x} \mathbf{W}_{dec} \bigr)$, where $\sigma(\cdot)$ is the sigmoid activation function.

\header{Message Embedding via Temporal Modulation} In AudioSeal, the message latent is obtained by a mean-pooling operation with temporal-axis repetition, 
\(
\boldsymbol{h}_w 
= 
\texttt{Repeat}\!\left(\frac{1}{K} \sum_{j=1}^K \boldsymbol{E}_{I_j}, t'\right)
\in \mathbb{R}^{t' \times H}. 
\) 

This approach can be viewed as injecting the message information mostly into the frequency domain, which greatly limits the message hiding capabilities. To improve upon this, knowing the input audio length $T$, we introduce a temporal message conditioning mechanism that employs a linear modulation layer 
\(\mathbf{W}_M \in \mathbb{R}^{K \times t'}\) 
to obtain the message latent:
\begin{equation}
    \boldsymbol{h}_w = \mathbf{W}_M^\top \,\boldsymbol{V}(\boldsymbol{w}) \;\in\; \mathbb{R}^{t' \times H}.
\end{equation}
This simple design not only prevents the sole reliance on the frequency domain for distributing the \(K\)-bit message, but also significantly facilitates the learning process for message decoding (see Figure \ref{fig:ablation}).

\subsection{Psychoacoustic-Aligned Temporal–Frequency Masking Loss}
\label{subsec:quality-loss}

Achieving imperceptibility is a key requirement of any watermarking system. Ideally, a watermark should exploit the perception characteristics of the human auditory system so that artifacts remain imperceptible. Following AudioSeal, we place a \(\ell_1\) constraint on the watermark signal \(\delta_x\) to ensure waveform-domain smoothness, and we incorporate a multi-scale Mel spectrogram loss $\mathcal{L}_{\text{msspec}}$ (e.g., as in \cite{defossez2022high}) to manage frequency-domain fidelity. 

We also adopt adversarial losses $\mathcal{L}_{adv}$ on multi-scale STFT spectrograms for perceptual improvement. However, simply pushing the watermarked audio distribution to be close to the clean audio in an adversarial sense may penalize desirable watermark characteristics (e.g., certain musical ``remixes'' can produce satisfactory imperceptible watermarks, but still yield high adversarial loss). 
Thus, we choose to down-weight $\mathcal{L}_{adv}$ and focus more on the psychoacoustic aspects to develop a human-centric, imperceptible watermark. 

\header{Psychoacoustic Masking Principles} Our design is based on psychoacoustic masking principles \citep{gelfand2017hearing,holdsworth1988implementing}, which describe how listeners are less sensitive to low-level sounds occurring near stronger sounds, both in time (forward/backward masking) and in frequency (simultaneous masking). In other words, when a loud audio component is present at a certain time–frequency location, it can mask weaker signals in its temporal and frequency vicinity \citep{necciari2016auditory}. Inspired by this, we propose a novel $\ell_2$ loss with per-tile TF-penalty. 

\header{Per-Tile Penalty with Asymmetric Temporal–Frequency Decay} Let \(\mathcal{S}_O(m,t)\) denote the magnitude of the original audio's mel-spectrogram, and the watermarked Mel-spectrogram is \(\mathcal{S}_W(m,t) \). To identify those strong ``masker'' tiles $\mathcal{M}$, we apply a magnitude threshold $\alpha_S$ to different timestamps on each frequency band ($\alpha_S=0.8$):
\[
    \mathcal{M}_{\text{masker}} \;=\; \{(m,t) \mid \mathcal{S}_O(m,t) > \alpha_S \,\max_{t'}\,\mathcal{S}_O(m,t')\}.
\]
Each index pair \((m_c,t_c)\in \mathcal{M}_{\text{masker}}\) acts as a masker, and we model its masking energy over the neighboring tiles as a linear decay in the decibel domain. Specifically, given mel-scale frequency radius $r_l^m$, $r_h^m$, and time-axis radius $r_b^t$, $r_f^t$, the local region of a masker is defined as: 
\[
    R(m_c,t_c) = \{(m,t) \mid -r_l^m \leq \Delta m \leq r_h^m, -r^t_{b} \leq \Delta t \leq r^t_{f}\},
\]

where $\Delta m = m - m_c$, and $\Delta t = t - t_c$. 
Aligned with the empirical studies \citep{necciari2016auditory} that the post-masking region (forward masking) is usually longer than the pre-masking region (backward masking) in the temporal axis, we set asymmetric radii $r_b^t$ and $r_f^t$, with forward masking region of 200ms and backward masking region of 20ms. For the frequency-bin radius, we first compute the empirical critical bandwidth for each mel-bin using the formula from \citet{zwicker2013psychoacoustics}, $W(m_c) = 25 + 75 \left(1 + 1.4 \left(\frac{\mathcal{F}(V[m_c])}{1000}\right)^2\right)^{0.69}$, where $\mathcal{F}(m_c)$ is the approximated center frequency obtained by converting each Mel bin value $V[m_c]$ back to Hz using $\mathcal{F}(m_c) = 700 \cdot (10^{\frac{V[m_c]}{2595}} - 1)$. Then the radii for each mel-bin is set as $r_l^m = r_h^m = r_b^m \cdot \gamma $, where $\gamma= W(m_c) / |\mathcal{F}(m_1) - \mathcal{F}(m_0)| $ is an adaptive scaling term, $V[m_i]$ denote the $i$-th mel-bin value, $r_b^m$ is a base radius that is set to be 3. With the dynamic radius scaled by the critical bandwidth -- where a higher frequency component has a wider masking range \citep{holdsworth1988implementing} -- we observe a better performance than using constant $\gamma=1$. 

The physical structure of the cochlea determines that lower frequencies (located at the base of the cochlea) cause broader and stronger excitation patterns compared to the higher frequencies (located at the apex).
To inject this bias of slower decay in the upward spread of masking \citep{zwicker2013psychoacoustics}, we set the decaying term for the upward direction to be larger than the downward direction, i.e., $\alpha_f^+  \leq \alpha_f^-$. For the temporal decay, following \citet{necciari2016auditory}, we set the backward direction to be larger than the forward direction, i.e., $\beta_t^+  \leq \beta_t^-$. 
Given the masker $(m_c,t_c)$, the threshold energy $E_{\text{mask}}(m,t)$ for the maskee tile $(m,t)$ is computed as:

\begin{equation}
\begin{aligned}
    E_{\text{mask}}(m,t; (m_c,t_c))  = 20\log_{10}\mathcal{S}_O(m_c,t_c)  - \Delta E, \\
\text{where} \quad \Delta E  = \alpha_f^+ \max(0, \Delta m) + \alpha_f^- \min(0, \Delta m) \\
+ \beta_t^{+} \max(0, \Delta t) + \beta_t^{-} \min(0, \Delta t), \\
\end{aligned}
\end{equation}

In psychoacoustic modeling, particularly in audio compression \citep{bosi1997iso}, the global masking threshold is determined by considering only the most dominant masker at each frequency component. Following this, the final masking threshold $E^*_{\text{mask}}(m,t)$ is computed as:
\[
   E^*_{\text{mask}}(m,t) = \max_{\substack{(m_c,t_c) \in \mathcal{M}_{\text{masker}}, \\ (m,t) \in R(m_c,t_c)}} E_{\text{mask}}(m,t; (m_c,t_c)).
\]
This ensures each maskee tile receives masking energy from its most perceptually dominant masker, while weaker maskers have negligible impact \citep{gelfand2017hearing}.

\header{Per-Tile-Weighted $\ell_2$ Loss Computation} Psychoacoustic masking effectively applies only when the masker's masking energy surpasses the tile's energy. We therefore apply the masking threshold on watermarked mel-spectrogram at \(20\log_{10}\mathcal{S}_W(m,t)\), filtering the masked tiles set as:
\[
    \mathcal{M}_{\text{maskee}}= \left\{ (m,t) \mid E^*_{\text{mask}}(m,t) > 20\log_{10}\mathcal{S}_W(m,t) \right\}.
\]
To encourage the model to embed the watermark in those maskee regions, we design per-tile penalty terms which weight the $\ell_2$ difference \citep{chen2023wavmark} between the watermarked and original audio in Mel-spectrogram space: 

\begin{align}
    \lambda(m,t) &= 1 + \mathbf{1}_{(m,t) \in \mathcal{M}_{\text{maskee}}} \cdot 10^{E^*_{\text{mask}}(m,t)/20},  \\
    \mathcal{L}_{\text{TF}} &\;=\; \sum_{(m,t)} \frac{\| 
  \mathcal{S}_W(m,t) - \mathcal{S}_O(m,t)
    \|_2^2}{\lambda(m,t)},
\end{align}
where $10^{E^*_{\text{mask}}(m,t)/20}$ is the masking energy, and the weighting term $\lambda(m,t)$ is larger for tiles in the masking region $\mathcal{M}_{\text{maskee}}$, effectively allowing more watermark signal to be embedded in those locations. For non-masked tiles, $\lambda(m,t)=1$ enforces standard $\ell_2$ loss. 

\begin{table*}[thbp]
    \centering
    \caption{
        The accuracy of detection and attribution across different editing operations. For detection, we also report the True Positive Rate (TPR) and False Positive Rate (FPR) where the threshold is selected by Youden's Index \citep{youden1950index} on a balanced validation set. 
    }
    \label{tab:standard_edit_det_att}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l *{2}{l} *{2}{l} *{2}{l} *{2}{l} *{2}{l}}
            \toprule
            & \multicolumn{2}{c}{\textbf{AudiowMark}} & \multicolumn{2}{c}{\textbf{WavMark}} & \multicolumn{2}{c}{\textbf{TimbreWM}} & \multicolumn{2}{c}{\textbf{AudioSeal}} & \multicolumn{2}{c}{\textbf{\ours}} \\
            \cmidrule(rr){2-3} \cmidrule(rr){4-5} \cmidrule(rr){6-7} \cmidrule(rr){8-9} \cmidrule(rr){10-11}
            \textbf{Edit} & \textbf{Det. \aux{(TPR/FPR)}} & \textbf{Att. } & \textbf{Det.  \aux{(TPR/FPR)}} & \textbf{Att. } & \textbf{Det.  \aux{(TPR/FPR)}} & \textbf{Att. } & \textbf{Det.  \aux{(TPR/FPR)}} & \textbf{Att. } & \textbf{Det.  \aux{(TPR/FPR)}} & \textbf{Att. } \\
            \midrule
            Identity      & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 0.93 & 1.00 (\aux{0.99/0.00}) & 0.69 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            Bandpass      & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.985 (\aux{0.97/0.00}) & 0.93 & 1.00 (\aux{0.99/0.00}) & 0.31 & 0.995 (\aux{0.99/0.00}) & 0.99 \\
            Boost         & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.98 (\aux{0.96/0.00}) & 0.91 & 1.00 (\aux{0.99/0.00}) & 0.50 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            Duck          & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 & 0.97 (\aux{0.94/0.00}) & 0.89 & 1.00 (\aux{0.99/0.00}) & 0.56 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            Echo          & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.945 (\aux{0.89/0.00}) & 0.91 & 1.00 (\aux{0.99/0.00}) & 0.38 & 0.995 (\aux{0.99/0.00}) & 0.99 \\
            Highpass      & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.95 (\aux{0.90/0.00}) & 1.00 & 0.985 (\aux{0.97/0.00}) & 0.93 & 1.00 (\aux{0.99/0.00}) & 0.31 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            Lowpass       & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.985 (\aux{0.97/0.00}) & 0.90 & 1.00 (\aux{0.99/0.00}) & 0.56 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            MP3           & 0.94 (\aux{0.88/0.00}) & 1.00 & 0.805 (\aux{0.61/0.00}) & 1.00 & 0.95 (\aux{0.91/0.01}) & 0.86 & 1.00 (\aux{0.99/0.00}) & 0.38 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            Pink Noise    & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 0.92 & 1.00 (\aux{0.99/0.00}) & 0.75 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            Random Noise  & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.975 (\aux{0.95/0.00}) & 0.90 & 1.00 (\aux{0.99/0.00}) & 0.56 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            Smooth        & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.96 (\aux{0.92/0.00}) & 0.88 & 0.98 (\aux{0.96/0.00}) & 0.80 & 1.00 (\aux{0.99/0.00}) & 0.19 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            Speed         & 0.50 (\aux{0.00/0.00}) & 0.00 & 0.50 (\aux{0.00/0.00}) & 0.00 & 0.515 (\aux{0.07/0.04}) & 0.18 & 0.61 (\aux{0.36/0.15}) & 0.00 & 0.995 (\aux{0.99/0.00}) & 0.03 \\
            Resample      & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.98 (\aux{0.96/0.00}) & 0.92 & 1.00 (\aux{0.99/0.00}) & 0.56 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
            AAC           & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.975 (\aux{0.95/0.00}) & 0.90 & 1.00 (\aux{0.99/0.00}) & 0.12 & 0.995 (\aux{0.99/0.00}) & 0.88 \\
            EnCodec (nq=16) & 0.50 (\aux{0.00/0.00}) & 0.00 & 0.805 (\aux{0.61/0.00}) & 0.00 & 0.625 (\aux{0.50/0.25}) & 0.07 & 1.00 (\aux{0.99/0.00}) & 0.31 & 0.965 (\aux{0.93/0.00}) & 0.99 \\
            Crop          & 0.965 (\aux{0.93/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 & 0.96 (\aux{0.92/0.00}) & 0.85 & 1.00 (\aux{0.99/0.00}) & 0.12 & 0.975 (\aux{0.98/0.03}) & 1.00 \\
            \midrule
            
            \textbf{Average} & 0.929 \aux{(0.859/0.000)} & 0.88 & 0.918 \aux{(0.836/0.000)} & 0.87 & 0.925 \aux{(0.869/0.019)} & 0.80 & 0.971 \aux{(0.950/0.010)} & 0.39 & \textbf{0.9919} \aux{(0.9856/0.0019)} & \textbf{0.93} \\
            
            \bottomrule
            \end{tabular}
    }
\end{table*}

\section{Experiments}
\label{sec:results}
\subsection{Experimental Setup}

Following prior works \citep{san2024proactive,chen2023wavmark}, we use a sampling rate of 16 kHz and one-second mono samples for training ($T=16000$) under 16 diverse audio editing transformations. We train the models on a mixed audio dataset of 4100 hours containing speech (3016 hours VoxPopuli \citep{wang2021voxpopuli} and 100 hours LibriSpeech \citep{panayotov2015librispeech}), music (9 hours MusicCap \citep{agostinelli2023musiclm}) and sound effects (98 hours AudioSet \citep{Audioset2017}). For evaluation, we use a held-out test set from MusicCap of size 100. The audio duration is set as 5s by default in evaluation. The loss weights are set as: $\lambda_{\text{TF}}=1$, $\lambda_{\text{adv}}=1$, $\lambda_{\ell_1}=0.1$, $\lambda_{\text{msspec}}=2$, $\lambda_{\text{detect}}=\lambda_{\text{message}}=10$. We use the Adam optimizer \citep{kingma2014adam} with learning rate \texttt{1e-5}, $\beta_1=0.4$, $\beta_2=0.9$, and Exponential Moving Average (EMA) with decay factor of $0.99$ updated every step. The training uses batch size 16 for a total of 73k steps. The model's latent dimension is set to $H=32$. For the attribution experiment, we follow the simulation protocol in \citet{san2024proactive} that defines a message pool of size $N$ ($\in\{100,1000,10000\}$), where each message is uniquely associated to a different user. As $N$ increases, the message length (in bits) also grows, making attribution more challenging due to the increased complexity of distinguishing individual messages. During decoding, the message is retrieved from the pool by selecting the one with the closest Hamming distance. 
We compare to four SoTA baselines: AudiowMark \citep{Westerfeld_audiowmark}, WavMark \citep{chen2023wavmark}, TimbreWM \citep{liu2023detecting}, AudioSeal \citep{san2024proactive}. 
Please refer to App. \ref{app:details} for more details. 

\subsection{Detection and Attribution Effectiveness}

\header{Robustness to Standard Edits}
We present the detection and attribution performance across a comprehensive suite of standard audio editing operations in Table \ref{tab:standard_edit_det_att}. For detection, we report both the overall accuracy and the true/false positive rates \aux{(TPR/FPR)}. For attribution, we report the averaged performance across different user numbers (see Figure. \ref{fig:attribution_acc_diff_users} for decomposed result). Our method achieves new state-of-the-art performance on both tasks, maintaining high detection accuracy (99.19\% average) and attribution accuracy (93\%) on average across transformations. Notably, while AudioSeal demonstrates strong detection performance (97.1\% average), it fails to perform effective attribution (39\% average). The traditional approach AudiowMark exhibits more balanced detection-attribution trade-offs (around 88\% attribution) but lower overall detection performance (92.9\% average). 
On the speeding operation, our method achieves 99.5\% detection accuracy while all other methods degrade to near random-guess levels (50-61\%). Moreover, while all the models currently fail to attribute against speeding transformation, we demonstrate that our model can be further enhanced with a simple speed reversion layer to boost speed robustness while maintaining efficiency in the App. \ref{app:speed_reversion}. Finally, we validate the scalability of our watermarking system in Figure~\ref{fig:attribution_acc_diff_users}, which highlights that \ours can maintain high attribution accuracy even when the users pool becomes larger. In summary, we demonstrate that \ours is more robust under standard edits for both detection and attribution. See App.~\ref{app:more-std-edit} for more details.

\begin{table}[t]
    \centering
    \caption{
        The detection performance (Accuracy \aux{(TPR/FPR)}) under two generative edits applied with different editing strengths $t$. 
    }
    \label{tab:generative_edit_detection}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{ccrrrrrr} 
            \toprule
            \textbf{System}                                                               & $t$ & \multicolumn{1}{c}{\textbf{AudiowMark}} & \multicolumn{1}{c}{\textbf{WavMark}} & \multicolumn{1}{c}{\textbf{TimbreWM}} & \multicolumn{1}{c}{\textbf{AudioSeal}} & \multicolumn{1}{c}{\textbf{\ours}}  \\ 
            \cmidrule{1-2}\cmidrule(l){3-7}
            \multirow{3}{*}{\parbox{1cm}{\centering\textbf{Stable\\Audio}}}                  
                & 10  & 0.50 (\aux{0.50/0.50})                            & 0.50 (\aux{0.50/0.50})                        & 0.47 (\aux{0.81/0.88})                        & 0.69 (\aux{0.63/0.25})                    & \textbf{0.94} (\aux{0.94/0.06})     \\
                                                                                          & 70  & 0.50 (\aux{0.50/0.50})                            & 0.50 (\aux{0.50/0.50})                        & 0.53 (\aux{0.19/0.13})                        & 0.59 (\aux{0.75/0.56})                  & \textbf{0.91} (\aux{0.88/0.06})     \\
                                                                                          & 110  & 0.50 (\aux{0.50/0.50})                            & 0.50 (\aux{0.50/0.50})                        & 0.53 (\aux{0.56/0.50})                        & 0.59 (\aux{0.94/0.75})                    & \textbf{0.91} (\aux{0.88/0.06})    \\ 
            \midrule
            \multirow{3}{*}{\parbox{1cm}{\centering\textbf{Audio\\LDM2}}} & 10  & 0.50 (\aux{0.50/0.50})                            & 0.50 (\aux{0.50/0.50})                        & 0.53 (\aux{0.81/0.75})                        & 0.59 (\aux{0.94/0.75})                   & \textbf{0.94} (\aux{0.94/0.06})    \\
                                                                                          & 70  & 0.50 (\aux{0.50/0.50})                            & 0.50 (\aux{0.50/0.50})                        & 0.50 (\aux{0.13/0.13})                        & 0.59 (\aux{0.94/0.75})                   & \textbf{0.94} (\aux{0.94/0.06})    \\
                                                                                          & 110  & 0.50 (\aux{0.50/0.50})                            & 0.50 (\aux{0.50/0.50})                        & 0.50 (\aux{0.00/0.00})                        & 0.59 (\aux{0.94/0.75})                       & \textbf{0.94} (\aux{0.94/0.06})    \\
            \bottomrule
            \end{tabular}
    
    }
\end{table}

\header{Robustness to Generative Edits}
Beyond the standard audio transformations that are seen during training, audio generative editing is one particularly challenging type of editing that watermarking systems might have to endure at deployment \citep{liu2024audioldm}. To simulate this, we use two state-of-the-art audio generative models, AudioLDM2 \citep{liu2024audioldm} and Stable Audio \citep{evans2024stable}, with a text-guided DDIM inversion method proposed in ZETA \citep{manor2024zero}. We test various editing strength $t \in \{10, 70, 110\}$, which represents the diffusion forward step when using DDIM inversion. 
As shown in Table \ref{tab:generative_edit_detection}, AudiowMark and WavMark degrade to a random-guess level performance, while TimbreWM and AudioSeal show inferior performance across different editing strengths around 50-60\%. In contrast, our method maintains consistently high detection accuracy, averaging 91-94\% across all editing strengths, and consistent across both generative models. This demonstrates that \ours generalize better to unseen generative edits compared to existing methods. Please refer to the App. \ref{app:generative_edit_robustness} for more details.

\begin{table}[t]
    \centering
    \caption{
        Detection performance and audio quality metrics after HSJA-based adversarial attacks on the waveform and spectrogram domains on Audiomark dataset \citep{liu2024audiomarkbench}. 
    }
    \label{tab:adversarial_attack}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l | cccc | cccc}
    \toprule
    \multirow{2}{*}{\textbf{Diff. Setup}}  & \multicolumn{4}{c|}{\textbf{AudioSeal}} & \multicolumn{4}{c}{\textbf{\ours}} \\
    \cmidrule{2-9}
    &  \textbf{Acc.} & \textbf{PESQ} & \textbf{SISNR} & \textbf{ViSQOL} & \textbf{Acc.} & \textbf{PESQ} & \textbf{SISNR} & \textbf{ViSQOL} \\
    \midrule
    Waveform
    & 0.15 & 1.14 & 8.97 & 2.61 & 0.68 & 2.80 & 17.79 & 3.34 \\
    Spectrogram
    & 0.15 & 1.05 & -17.82 & 2.45 & 0.36 & 1.56 & -24.08 & 2.37 \\
    \midrule
    \#Q=100 & 0.15 & 1.14 & 8.97 & 2.61 & 0.68 & 2.80 & 17.79 & 3.34 \\
    \#Q=200 & 0.15 & 1.13 & 8.95 & 2.64 & 0.57 & 2.30 & 6.93 & 2.91 \\
    \#Q=500 & 0.15 & 1.13 & 8.72 & 2.63 & 0.47 & 1.97 & 1.66 & 2.55 \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\header{Robustness to Adversarial Watermark Removal} As highlighted in \citet{liu2024audiomarkbench}, adversarial perturbations can be crafted to make the watermark undetectable, even with only black-box access to the detection model. To test robustness against these attacks, we employ the black-box HopSkipJumpAttack (HSJA) \citep{hopskipjump}, to iteratively craft perturbations in the waveform or spectrogram domains that can fool the detector while minimizing the perturbation residual norm. As shown in Table \ref{tab:adversarial_attack}, we first evaluate robustness against HSJA attacks in the two domains with query budget $Q=100$. The results show that, although the attack effectively decreases the detection accuracy, the quality of the perturbed audio is significantly degraded. Furthermore, with the waveform-based attack, increasing the query budget $Q$ from $100$ to $500$ further decreases the detection accuracy from 0.68 to 0.47. However, across different domains and query budgets, \ours is consistently more robust than AudioSeal, maintaining higher detection accuracy and perceptual quality after the attack.

\subsection{Quality and Stealthiness Assessment}
Following \citet{san2024proactive}, we evaluate the following objective quality metrics of the watermarked audio: Scale Invariant Signal to Noise Ratio (SI-SNR), as well as PESQ~\cite{rix2001perceptual}, ViSQOL~\cite{hines2012visqol} and STOI~\cite{taal2010short}. Furthermore, we also report the loudness of watermark residual $L(\boldsymbol{\delta_w})$ according to ITU-R BS.1770-4 \citep{series2011algorithms}, which is an important metric for watermarking stealthiness, especially under residual-based detection \citep{yang2024can}. As shown in \autoref{tab:audio_quality}, \ours achieves competitive performance across all perceptual quality metrics. While WavMark achieves the highest SI-SNR (36.35 dB), \ours excels in perceptual quality measures, achieving the best STOI score (1.000) and the lowest watermark residual loudness (-54.63 dB). For the PESQ and ViSQOL scores, our method achieves competitive performance, with a PESQ score of 4.43 and a ViSQOL score of 4.56. Additionally, we conducted a subjective listening test of MUSHRA \citep{mushra} with 12 participants, and the results suggest that our method achieves a comparable quality with AudioSeal scoring around 91 (while ground truth scores around 95, please refer to the App. \ref{app:MUSHRA} for more details). In summary, our method guarantees superior detection and attribution performance, while maintaining competitive perceptual quality.
Please refer to the App. \ref{app: watermark-vis} and App. \ref{app: tf-weighted-penalty} for more discussion.

\begin{table}[thbp]
    \centering
    \caption{
        \textbf{Audio Quality Metrics}. 
        Comparison of perceptual quality metrics across different watermarking methods.
    }\label{tab:audio_quality}
    \vspace{0.2cm}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lccccc}
            \toprule
            \textbf{Methods} & \textbf{SI-SNR} $\uparrow$ & \textbf{PESQ} $\uparrow$ & \textbf{STOI} $\uparrow$ & \textbf{ViSQOL} $\uparrow$ & \textbf{L($\delta_x$)} $\downarrow$ \\
            \midrule
            AudiowMark & 27.88 & \textbf{4.59} & 0.988 & \textbf{4.72} & -44.96 \\
            Wavmark & \textbf{36.35} & 4.43 & 0.985 & \underline{4.62} & \underline{-53.01} \\
            TimbreWM & 26.45 & 4.29 & 0.974 & 4.63 & -43.69 \\
            AudioSeal & 25.32 & \underline{4.51} & \underline{0.990} & 4.72 & -44.51 \\
            \midrule
            \ours & \underline{29.00} & 4.43 & \textbf{1.000} & 4.56 & \textbf{-54.63} \\
            \quad w/o $\mathcal{L}_{TF}$ & 19.64 & 4.25 & 0.990 & 4.26 & -52.63 \\
            \quad  w/o Adaptive $\gamma$ & 21.89 & 4.22 & 0.991 & 4.24 & -54.30 \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-8pt}
\end{table}

% \vspace{-2em}
\subsection{Ablation Study}

We first conduct ablation on the cross-attention module and temporal modulation layer to showcase their effectiveness for boosting learning in message decoding. As shown in \autoref{fig:ablation}, our approach demonstrates superior learning efficiency, reaching approximately 98\% accuracy. When removing the modulation component, performance drops significantly to around 60\%. More dramatically, ablating the cross-attention mechanism causes accuracy to drop to random guess levels (50\%), indicating this is a crucial architectural element. These results clearly demonstrate that while both components contribute to model performance, cross-attention is particularly vital for effective message retrieval learning. Furthermore, we ablate the proposed temporal-frequency loss $\mathcal{L}_{\text{TF}}$ and the adaptive bandwidth with constant weight $\gamma=1$. As shown in \autoref{tab:audio_quality}, both of them contribute to improving perceptual quality.

\begin{figure}[thbp]
    \centering
    \begin{minipage}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/attribution_acc_diff_users.pdf}
        \caption{Attribution accuracy with different \#Users. }
        \label{fig:attribution_acc_diff_users}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation.pdf}
        \caption{Ablation study on the proposed architecture.}
        \label{fig:ablation}
    \end{minipage}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}

In this paper, we presented a neural audio watermarking framework that jointly achieves reliable watermark detection and accurate message attribution under challenging transformations, including generative editing. By integrating partial parameter sharing, a cross-attention detector, and a temporal conditioning module, we improved the robustness and decoding accuracy compared to prior neural watermarking methods. Our psychoacoustic-aligned time-frequency masking loss further enables quality-preserving watermarking. Though limitations remain for extreme transformations such as speeding, future work incorporating stronger temporal alignment could further improve real-world applicability.

\section*{Impact Statement}
Our work advances robust audio watermarking to address critical challenges in media authenticity and intellectual property protection arising from generative AI. By enabling reliable detection and attribution of imperceptible watermarks across diverse transformations—including state-of-the-art generative edits—this technology helps combat deepfake misinformation while safeguarding creator rights. The proposed cross-attention architecture and psychoacoustic loss represent technical innovations that balance robustness with perceptual quality, offering practical tools for content provenance tracking and copyright enforcement in real-world applications.

Potential dual-use implications necessitate ethical consideration: while designed to enhance media integrity, similar watermarking systems could theoretically enable covert tracking or censorship if misapplied. We advocate for transparent deployment practices and policy frameworks to prevent abuse, emphasizing watermarking's role in supporting—not undermining—digital rights. Researchers should consider these societal impacts when developing attribution technologies, particularly regarding user consent and detection transparency.

\bibliography{ref}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

\section{Implementation Details}
\label{app:details}
\header{Model Architecture} Following AudioSeal \citep{san2024proactive}, we leverage a pair of convolutional encoder-decoder models that operate on the waveform domain to generate and detect watermarks. The watermark generator consists of a waveform encoder and decoder, both utilizing components from EnCodec~\cite{defossez2022high}. The encoder employs a 1D convolution containing 32 channels and kernel size 7, then uses four convolutional blocks. Each of these blocks contains a residual unit and a down-sampling layer, utilizing convolution with stride $S$ and kernel size $K = 2S$. 
The residual unit contains two kernel-3 convolutions with a skip-connection, with channels doubling during down-sampling. The encoder ends with a two-layer LSTM and final 1D convolution having kernel size 7 and 128 channels.
The stride $S$ values used are (2, 4, 5, 8), and residual units utilize the Exponential Linear Unit (ELU) as nonlinear activation.
The decoder reflects the encoder's structure but employs transposed convolutions, with strides in opposite order.

The detector contains an encoder, transposed convolution, and linear layer for the detection head.
The encoder utilizes the same structure as the one from the generator but with different weights.
The transposed convolution includes $H$ output channels and upscales the activation map to match the original audio dimension, yielding an activation map shaped $(T, H)$. This frame-level latent state serves both the detection head and the message decoding head.
For the \emph{detection head}, the linear layer serves to reduce the dimensionality of $H$ to two, followed by a softmax function that produces per-sample detection probability scores. Additional details on the \emph{message decoding head} are provided in the next section.

\emph{Cross-Attention Message Decoding Head.} The message decoding head aims to decode the embedded message bits by leveraging the shared embedding table from the generator. The key idea is to have a ``message vocabulary'' to look up when decoding the message bits. To this end, we employ a single-head cross-attention layer with the message latent state as a query and the units in the embedding table as key-value pairs. Note that the reconstructed waveform latent is first passed through a fully connected layer to decompose the time dimension and obtain a $K$-dimensional message latent state as a query. For the embedding table part, we reshape the embedding table to a $2H$-dimensional vector by combining the two vicinal units that correspond to the same message bit into one $2H$-dimensional vector. All the linear projections $Q, K, V$ are designed to map the latent dimension to $H$ dimensions. After the cross-attention operation, we apply an ELU activation function followed by a final linear projection $W\in \mathbb{R}^{H \times 1}$ to obtain the final per-bit message logits for the logistic regression.

\header{Training and Inference Details} During the training stage, we employ a curriculum learning method to first disable the gradient-based quality balancing term, guiding the model to focus more on learning watermark decoding at the beginning of training. After 59000 training steps, we enable the quality balancing term to help the model improve the watermark auditory transparency with more dynamic scaling across different perceptual loss terms. To boost the sampling of the transformation efficiency, we update the sampling probability of each transformation every 1000 steps on the validation set, adjusting it based on the validation accuracy of each transformation. In detail, the sampling ratio $p_g$ for transformation $g$ is given by $p_g = \frac{1-\text{acc}_g}{\sum_g 1- \text{acc}_g}+\epsilon$, where $\text{acc}_g$ is the accuracy of the model on transformation $g$ and $\epsilon$ is a small constant for keeping the sampling ratio non-zero. We observe that it boosts the learning efficiency in attribution with at least 2x improvement under the main setup. The training checkpoint at 73000 is selected based on its balanced performance between quality and utility scores on the validation set. During the inference stage, for given audio with arbitrary duration, we first pad and split the input audio into multiple 1s audio segments and then apply the watermarking model on each chunk. Audio chunks are then concatenated to obtain the final watermarked audio. 

\header{Perceptual Loss} Following AudioSeal \citep{san2024proactive}, we employ a pool of perceptual loss terms to guide the model to learn the watermarking task. The perceptual loss terms include: i) multi-scale mel-spectrogram loss $\mathcal{L}_{\text{msspec}}$ that computes both L1 loss on linear-scale mel-spectrograms and MSE loss on log-scale mel-spectrograms across multiple FFT window sizes (from $2^6$ to $2^{11}$), with each scale weighted by $\sqrt{2^i-1}$ for scale $i$; ii) feature matching loss $\mathcal{L}_{\text{feat}}$ that minimizes L1 distance between intermediate feature maps extracted from the adversarial discriminator, averaged across all layers. iii) waveform L1 loss $\mathcal{L}_{\text{waveform}}$ that minimizes L1 distance between the original waveform and the watermarked waveform; iv) our proposed psychoacoustic-aligned time-frequency masking loss $\mathcal{L}_{\text{mask}}$, which captures the weighted $\ell_2$ loss in the mel-spectrograms domain, using a per-tile weight obtained through simulation of the masker energy decaying in time-frequency domain.

For the adversarial discriminator, we adopt a multi-scale STFT discriminator architecture that operates on the complex STFT representations of the audio at different scales. Specifically, it consists of multiple sub-discriminators, each processing the STFT with different FFT sizes (512, 1024, 2048), hop lengths (128, 256, 512), and window sizes (512, 1024, 2048). Each sub-discriminator first computes the STFT, concatenates the real and imaginary components along the channel dimension, and processes them through a series of 2D convolutions with increasing dilation rates (1,2,4). The feature maps from these convolutional layers are used for the feature matching loss. These losses jointly optimize for perceptual quality by matching both time-frequency characteristics and learned audio representations.

\header{Evaluation Setup} For the attribution experiment, following \citet{san2024proactive}, we set up a pool of $N$ potential users, where each user's sequential bit message is derived from their ordinal number, i.e., the $i$-th user's message is given by $w_i = \text{binary}(i)$. This creates a space of all potential messages $E_W$. The attribution is achieved by retrieving the message in $E_W$ with the smallest Hamming distance. Compared to bit-wise message accuracy, this metric provides a more practical measure of message retrieval performance, aligning better with real-world watermarking applications and attribution use cases. The message length (in bits) determines the maximum number of uniquely identifiable users. Specifically, for a message of \(K\) bits, the system can support up to \(2^K\) unique users. In our experiments, we use a 16-bit message, enabling attribution across \(2^{16} = 65,\!536\) users at maximum. 

\subsection{Robustness Augmentations}\label{app:augmentations}
Following AudioSeal \citep{san2024proactive}, we apply these audio editing augmentations during training and evaluation:
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Bandpass Filter:} Simulates frequency-selective audio equipment by allowing only mid-range frequencies. Allows frequencies between 300Hz-8000Hz to pass through.
    \item \textbf{Highpass Filter:} Removes bass frequencies to simulate poor bass response. Cuts frequencies below 500Hz.
    \item \textbf{Lowpass Filter:} Removes high frequencies to simulate muffled audio. Cuts frequencies above 5000Hz.
    \item \textbf{Speed:} Simulates playback speed variations. Changes speed by random factor 0.8-1.2.
    \item \textbf{Resample:} Robustness to sample rate conversion. Upsamples to 32kHz, then downsamples back to the original rate.
    \item \textbf{Boost Audio:} Simulates volume increase. Amplifies by factor 1.2.
    \item \textbf{Duck Audio:} Simulates volume decrease. Reduces volume by a factor of 0.8.
    \item \textbf{Echo:} Simulates room acoustics and reverberation. Adds delayed copy with 0.1-0.5s delay and 0.1-0.5 volume.
    \item \textbf{Pink Noise:} Adds realistic environmental noise. Adds pink noise with std 0.01.
    \item \textbf{White Noise:} Adds random electronic noise. Adds Gaussian noise with std 0.001.
    \item \textbf{Smooth:} Simulates low-quality audio processing. Moving average filter with window size 2-10.
    \item \textbf{AAC:} Robustness to common lossy compression. AAC encoding at 128kbps.
    \item \textbf{MP3:} Robustness to common lossy compression. MP3 encoding at 128kbps.
    \item \textbf{EnCodec:} Tests neural audio codec compression. Resamples to 24kHz, encodes with 16 streams ($nq=16$), resamples to 16kHz.
    \item \textbf{Crop:} Robustness to audio truncation, padding, and in-batch audio mixing. While \citet{san2024proactive} use this to obtain a localization mask, we implement crop as one of the edits to gain cropping-based robustness. Specifically, we first randomly select $k$ starting points, then modify $T/2k$ consecutive samples in one of four ways:
    \begin{itemize}[noitemsep,topsep=0pt]
        \item Revert to original audio (40\% probability)
        \item Replace with zeros (20\% probability) 
        \item Substitute with different audio from the same batch (20\% probability)
        \item Leave unmodified (20\% probability)
    \end{itemize}

\end{itemize}

\subsection{Dataset Details}
\label{app:dataset}
We utilize a mixed dataset of $4100$ hours in total for training, which contains 100.59-hour LibriSpeech~\cite{panayotov2015librispeech}, 98.53-hour AudioSet~\cite{Audioset2017}, 879.29-hour Free Music Archive~\cite{FMA2016}, 9-hour MusicCaps~\cite{agostinelli2023musiclm}, and 3,016.43-hour VoxPopuli~\cite{wang2021voxpopuli}. Each dataset is described as follows:

\header{LibriSpeech} LibriSpeech~\cite{panayotov2015librispeech} is an English speech dataset derived from audiobooks in the LibriVox project. The dataset contains approximately 1000 hours of read English speech sampled at 16 kHz, with careful segmentation and alignment. The audio is paired with transcribed text, making it suitable for speech recognition tasks. We used a 100.59-hour subset of the full 1000-hour dataset for training.

\header{AudioSet} AudioSet~\cite{Audioset2017} is a large-scale dataset containing 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. It consists of 632 audio event classes organized in a hierarchical ontology, covering a wide range of sounds, including human and animal sounds, musical instruments, genres, and common environmental sounds. The dataset contains approximately 5,790 hours of annotated audio. For our experiments, we used a 98.53-hour randomly sampled subset of the full dataset for training.

\header{Free Music Archive} FMA~\cite{FMA2016} is a large-scale, open-source dataset of music tracks with clear licensing. We used the ``large'' subset containing 879.29 hours of audio data with 106,574 30-second tracks sampled at 44.1kHz. The dataset spans multiple genres, including Rock, Electronic, Experimental, Hip-Hop, Folk, and Instrumental, making it suitable for training robust watermarking models across diverse musical styles.

\header{MusicCaps} MusicCaps~\cite{agostinelli2023musiclm} is a dataset of 5.5k music-text pairs (9.47 hours total), with high-quality human-written captions describing musical attributes like genre, mood, instruments, and tempo. The dataset was created by having music experts write detailed captions for a subset of 5.5k music clips from AudioSet, with each caption carefully describing the musical content, instrumentation, style, and other sonic characteristics. We used a subset of 9 hours for training.

\header{VoxPopuli} VoxPopuli~\cite{wang2021voxpopuli} is a large-scale multilingual speech corpus collected from European Parliament event recordings between 2009-2020. It contains approximately 400K hours of unlabeled speech data across 23 languages, 1.8K hours of transcribed speech for 16 languages, and 17.3K hours of speech-to-speech interpretation data. For English specifically, it provides 24.1K hours of unlabeled speech data and 543 hours of transcribed speech from 1,313 speakers. The dataset has been widely used for representation learning, semi-supervised learning, and interpretation tasks. We used a 3,016.43-hour subset that covers balanced speech from 23 languages for training.

\section{Limitations and Broader Impact}
\label{sec:limitations}
While our method demonstrates strong performance across various standard audio transformations, there are still some limitations and areas for improvement. First, while we achieve good robustness against most common audio edits, the attribution robustness against challenging operations like speed changes and generative editing could be further improved. As shown in Table~\ref{tab:full_attribution_table}, for speed changes, our method achieves only 4\% average attribution accuracy across different user setups, though this still outperforms existing baselines which completely fail. One potential direction for further boosting robustness against speed variations is to incorporate a temporal adjustment layer, similar to the one implemented in \citet{Westerfeld_audiowmark}.  This layer aims to search in a black-box manner for the optimal speed factor to reverse the change. We provide a preliminary exploration of this direction in App. \ref{app:speed_reversion}.

From a broader impact perspective, audio watermarking technology plays an important role in protecting intellectual property rights and detecting AI-generated content in a proactive manner. However, our experiments on adversarial perturbations (Table \ref{tab:adversarial_attack}) show that our model remains vulnerable if the adversary has sufficient access and query opportunities to the watermarking system. This highlights the importance of securing the trained watermarking model and preventing public exposure. Overall, our work contributes to the development of a more robust watermarking solution, and we hope it will inspire further research in this field.

\section{More Results}
\label{app:more_results}

\subsection{Subjective Evaluation with MUSHRA}
\label{app:MUSHRA}
The MUSHRA protocol is a crowdsourced test in which participants rate the quality of various samples on a scale of 0 to 100. 
The ground truth is provided for reference. 
We utilized eight randomly sampled speech and music samples from different sources, each lasting 5 seconds. 
As part of the study, we included a low anchor, which is a lowpass-filtered version at 3.5kHz.
Each sample was evaluated by 16 participants, with a post-hoc filtering process to exclude participants who failed to correctly identify more than two hidden reference samples in their scoring, resulting in 12 valid participants at the end.
For comparison, the ground truth samples received an average score of around $98$, while the low anchor's average score was around $25$. 

As shown in Figure \ref{fig:mushra}, our method achieved a score of approximately 91, closely matching AudioSeal. Among all the methods, Audiowmark is the one that achieves the highest score at around $94$. WavMark received a slightly lower score of 88, while TimbreWM scored the lowest, around $80$. These results demonstrate that our watermarking approach maintains a perceptual quality that is competitive with SoTA methods. 

\begin{figure}[t]
    \centering
    \includegraphics[width=.5\linewidth]{figures/mushra.png}
    \caption{MUSHRA subjective listening test results comparing perceptual quality across different watermarking methods. Higher scores indicate better audio quality as rated by human listeners. Our method achieves quality scores comparable to AudioSeal.}
    \label{fig:mushra}
\end{figure}

\subsection{Analysis on the Training Dynamics of Models with Different Architectures}
\label{app:training_dynamics}

We analyze the training dynamics of different architectures under a controlled experimental setup to better understand their inherent learning capabilities. To isolate architectural effects, we remove most auxiliary losses, retaining only the adversarial loss (weight=1) for all methods. Additionally, we include a waveform-domain $\ell_1$ loss (weight=1) for AudioSeal and our method, as we observed it improves the learning efficiency. Figure~\ref{fig:arch-dynamic} illustrates the validation accuracy trajectories and quality for both watermark detection and message-bit decoding during training. The results reveal distinct learning characteristics that highlight the trade-offs between different architectural choices. 

WavMark's fully-shared architecture exhibits remarkably rapid initial learning, converging to high accuracy within just a few hundred steps. However, this early success is followed by a concerning degradation in detection accuracy as training progresses. This pattern suggests that while full parameter sharing enables quick initial learning, it may lead to destructive interference between the detection and the decoding tasks during extended training. 

AudioSeal's fully-disjoint architecture in general show slow convergence in both detection and message-bit decoding. After training for 30k steps, it quickly converge to 99\% detection accuracy but the learning of message-bit decoding progress slowly, achieving around 70\% message bit accuracy at the end of 50k training steps. Moreover, the perceptual quality of AudioSeal is also degraded more than \ours and WavMark. 

Compared to these, our blended architecture demonstrates steady and comprehensive learning - both detection and decoding accuracies improve gradually, ultimately achieving near-perfect performance only with about 19k training steps. Notably, with only 2k training steps, \ours can converge to 99\% detection accuracy. This great boost in learning efficiency suggests that our partial parameter-sharing strategy and cross-attention mechanism enables more efficient learning of both tasks.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/comparison_all.pdf}
    \caption{Validation accuracy and quality curve of different methods over training steps. Using a blended architecture, \ours is able to achieve a better balance in terms of learning efficiency in detection, and message-bit decoding and watermark imperceptibility. Compared to our method, WavMark, the fully-shared architecture, suffers from robustness degradation in detection as the training progresses; the fully-disjoint architecture, AudioSeal, suffers from training efficiency issues in learning message-bit decoding. }
    \label{fig:arch-dynamic}
\end{figure}
\subsection{Robustness to Standard Edits}
\label{app:more-std-edit}
We present the full results summarizing detection and attribution robustness against standard edits in Tables \ref{tab:full_standard_edit_detection} and \ref{tab:full_attribution_table}. For detection, our method achieves consistently high performance across all transformations, with an average accuracy of 99.19\% and AUC of 99.93\%, significantly outperforming all baselines. Even for challenging transformations like speed changes and neural codec compression (EnCodec) where other methods mostly fail (accuracy around 50-60\%), our approach maintains exceptional detection performance with accuracies of 99.5\% and 96.5\%, respectively.

For attribution, the results in Table \ref{tab:full_attribution_table} show that our method maintains high attribution accuracy (92-94\% average) across different user pool numbers (and associated messages), demonstrating strong scalability. The performance is particularly robust for most standard audio transformations like bandpass filtering, boost/duck, echo, and noise addition, achieving near 100\% attribution accuracy. While speed changes remain challenging, our method still achieves better performance than baselines which completely fail. This indicates that while our method handles most common audio edits exceptionally well, speed changes continue to be a challenging transformation for reliable attribution. We also explore a potential simple extension of speed reversion to further boost the performance in App. \ref{app:speed_reversion}.

Our approach shows particularly strong robustness against modern audio codecs - maintaining near-perfect attribution accuracy for both MP3 and AAC compression (98-100\%), while baselines like AudioSeal struggle significantly with these transformations (showing accuracies below 60\%). This demonstrates our method's effectiveness against state-of-the-art audio compression techniques that are increasingly common in real-world applications. Additionally, our method shows outstanding robustness to cropping operations, maintaining 100\% attribution accuracy, while AudioSeal struggles in this task (12\% accuracy).

\begin{table*}[t]
    \centering
    \caption{
        \textbf{The full detection results} of \ours across different standard audio edits on the MusicCaps dataset.
        Acc. (TPR/FPR) is the accuracy (and TPR/FPR) obtained for the optimal threshold from Youden's Index \citep{youden1950index} on a balanced validation set. 
    }
    \label{tab:full_standard_edit_detection}
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{l *{2}{l} *{2}{l} *{2}{l} *{2}{l} *{2}{l}}
        \toprule
        & \multicolumn{2}{c}{\textbf{AudiowMark}} & \multicolumn{2}{c}{\textbf{WavMark}} & \multicolumn{2}{c}{\textbf{TimbreWM}} & \multicolumn{2}{c}{\textbf{AudioSeal}} & \multicolumn{2}{c}{\textbf{\ours}} \\
        \cmidrule(rr){2-3} \cmidrule(rr){4-5} \cmidrule(rr){6-7} \cmidrule(rr){8-9} \cmidrule(rr){10-11}
        \textbf{Edit} & \textbf{Acc. \aux{(TPR/FPR)}} & \textbf{AUC} & \textbf{Acc. \aux{(TPR/FPR)}} & \textbf{AUC} & \textbf{Acc. \aux{(TPR/FPR)}} & \textbf{AUC} & \textbf{Acc. \aux{(TPR/FPR)}} & \textbf{AUC} & \textbf{Acc. \aux{(TPR/FPR)}} & \textbf{AUC} \\
        \midrule
        Identity      & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Bandpass      & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.985 (\aux{0.97/0.00}) & 1.00 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Boost         & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.98 (\aux{0.96/0.00}) & 0.9998 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Duck          & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 0.995 & 0.97 (\aux{0.94/0.00}) & 0.9999 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Echo          & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.945 (\aux{0.89/0.00}) & 0.9965 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Highpass      & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.95 (\aux{0.90/0.00}) & 0.95 & 0.985 (\aux{0.97/0.00}) & 1.00 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Lowpass       & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.985 (\aux{0.97/0.00}) & 0.9986 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        MP3           & 0.94 (\aux{0.88/0.00}) & 0.94 & 0.805 (\aux{0.61/0.00}) & 0.805 & 0.95 (\aux{0.91/0.01}) & 0.9971 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Pink Noise    & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Random Noise  & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.975 (\aux{0.95/0.00}) & 0.9999 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Smooth        & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.96 (\aux{0.92/0.00}) & 0.96 & 0.98 (\aux{0.96/0.00}) & 0.9976 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Speed  & 0.50 (\aux{0.00/0.00}) & 0.50 & 0.50 (\aux{0.00/0.00}) & 0.50 & 0.515 (\aux{0.07/0.04}) & 0.4812 & 0.61 (\aux{0.36/0.15}) & 0.61 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        Resample      & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.98 (\aux{0.96/0.00}) & 1.00 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        AAC           & 1.00 (\aux{1.00/0.00}) & 1.00 & 1.00 (\aux{1.00/0.00}) & 1.00 & 0.975 (\aux{0.95/0.00}) & 0.9999 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.995 (\aux{0.99/0.00}) & 1.00 \\
        EnCodec (nq=16) & 0.50 (\aux{0.00/0.00}) & 0.50 & 0.805 (\aux{0.61/0.00}) & 0.805 & 0.625 (\aux{0.50/0.25}) & 0.7092 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.965 (\aux{0.93/0.00}) & 0.991 \\
        Crop          & 0.965 (\aux{0.93/0.00}) & 0.965 & 0.995 (\aux{0.99/0.00}) & 0.995 & 0.96 (\aux{0.92/0.00}) & 0.9918 & 1.00 (\aux{0.99/0.00}) & 1.00 & 0.975 (\aux{0.98/0.03}) & 0.997 \\
        \midrule

        \textbf{Average} & 0.9294 (\aux{0.8588/0.00}) & 0.9294 & 0.9181 (\aux{0.8363/0.00}) & 0.9181 & 0.925 (\aux{0.8688/0.0188}) & 0.9482 & 0.9706 (\aux{0.95/0.01}) & 0.9757 & 0.9919 (\aux{0.9856/0.0019}) & 0.9993 \\
        
        \bottomrule
        \end{tabular}
    }
\end{table*}

\begin{table*}[t]
\centering
\caption{The full attribution results for all methods, and associated overall average performance, across all transformations on the MusicCaps dataset.}
\label{tab:full_attribution_table}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
\toprule
\multirow{2}{*}{\textbf{Transformation}} & \multicolumn{4}{c|}{\textbf{AudiowMark}} & \multicolumn{4}{c|}{\textbf{Wavmark}} & \multicolumn{4}{c|}{\textbf{TimbreWM}} & \multicolumn{4}{c|}{\textbf{AudioSeal}} & \multicolumn{4}{c}{\textbf{\ours}} \\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}\cmidrule(lr){14-17}\cmidrule(lr){18-21}  & N=100 & 1000 & 10000 & \textbf{Avg} & 100 & 1000 & 10000 & \textbf{Avg} & 100 & 1000 & 10000 & \textbf{Avg} & 100 & 1000 & 10000 & \textbf{Avg} & 100 & 1000 & 10000 & \textbf{Avg} \\
\midrule
\textbf{Identity} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.96 & 0.93 & 0.86 & 0.93 & 0.75 & 0.50 & 0.50 & 0.69 & 1.00 & 1.00 & 1.00 & 1.00 \\
\textbf{Encodec} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.03 & 0.00 & 0.00 & 0.07 & 0.50 & 0.00 & 0.00 & 0.31 & 1.00 & 0.99 & 0.99 & 0.99 \\
\textbf{AAC} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.94 & 0.90 & 0.85 & 0.90 & 0.50 & 0.00 & 0.00 & 0.12 & 0.99 & 0.96 & 0.69 & 0.88 \\
\textbf{Bandpass} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.92 & 0.92 & 0.89 & 0.93 & 0.50 & 0.25 & 0.00 & 0.31 & 0.99 & 0.99 & 0.99 & 0.99 \\
\textbf{Boost} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.91 & 0.91 & 0.87 & 0.91 & 0.25 & 0.50 & 0.25 & 0.50 & 1.00 & 1.00 & 1.00 & 1.00 \\
\textbf{Duck} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.93 & 0.85 & 0.84 & 0.89 & 0.50 & 0.50 & 0.25 & 0.56 & 1.00 & 1.00 & 1.00 & 1.00 \\
\textbf{Echo} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.92 & 0.92 & 0.87 & 0.91 & 0.25 & 0.25 & 0.25 & 0.38 & 1.00 & 0.98 & 1.00 & 0.99 \\
\textbf{Highpass} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.96 & 0.92 & 0.87 & 0.93 & 0.50 & 0.25 & 0.00 & 0.31 & 1.00 & 1.00 & 1.00 & 1.00 \\
\textbf{Lowpass} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.93 & 0.86 & 0.85 & 0.90 & 0.50 & 0.50 & 0.50 & 0.56 & 1.00 & 1.00 & 1.00 & 1.00 \\
\textbf{MP3} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.85 & 0.85 & 0.86 & 0.86 & 0.50 & 0.50 & 0.00 & 0.38 & 1.00 & 1.00 & 1.00 & 1.00 \\
\textbf{Pink Noise} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.93 & 0.91 & 0.87 & 0.92 & 0.75 & 0.75 & 0.75 & 0.75 & 1.00 & 1.00 & 1.00 & 1.00 \\
\textbf{White Noise} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.88 & 0.89 & 0.87 & 0.90 & 0.50 & 0.75 & 0.50 & 0.56 & 1.00 & 1.00 & 1.00 & 1.00 \\
\textbf{Smooth} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.75 & 0.88 & 0.75 & 0.86 & 0.69 & 0.80 & 0.00 & 0.25 & 0.25 & 0.19 & 1.00 & 1.00 & 0.99 & 1.00 \\
\textbf{Speed } & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.20 & 0.04 & 0.19 & 0.18 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.08 & 0.03 \\
\textbf{Resample} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.93 & 0.92 & 0.87 & 0.92 & 0.75 & 0.25 & 0.25 & 0.56 & 1.00 & 1.00 & 1.00 & 1.00 \\
\textbf{Crop} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.93 & 0.76 & 0.80 & 0.85 & 0.00 & 0.00 & 0.00 & 0.12 & 1.00 & 0.99 & 1.00 & 1.00 \\
\midrule
\textbf{Avg}& 0.88 & 0.88 & 0.88 & 0.88 & 0.88 & 0.88 & 0.86 & 0.87 & 0.81 & 0.78 & 0.75 & 0.80 & 0.42 & 0.33 & 0.22 & 0.39 & 0.94 & 0.93 & 0.92 & 0.93 \\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{figure*}[thbp]
    \centering
    \includegraphics[width=.8\linewidth]{figures/diff-trans-transferability-results-full.pdf}
    \caption{Detection and Attribution Accuracy of our method on augmented samples across different augmentation strengths.}
    \label{fig:diff_trans_transferability}
\end{figure*}

\subsubsection{Simple Speed Reversion Layer with Black-box Grid Search}
\label{app:speed_reversion}
We explore a post-hoc simple speed reversion layer with a black-box grid search to further boost the robustness against speed changes. The speed reversion layer is added on top of the watermarking model. The main idea is to reverse the speed change by searching for the optimal speed factor with the guidance of the watermarking model. We obverse that on the unwatermarked audio, changing the speed can not adversarially increase the detection accuracy, while on the watermarked audio, doing speed change will significantly affect the watermark message decoding pattern. We observe that the watermarking model predicts more stable message bits for each 1-second chunk when the speed remains unchanged. However, after applying a speed change, the model predicts more diverse message bits with lower confidence for each chunk. To leverage this insight, given the original speed change space $[\gamma_{\min}, \gamma_{\max}]$ and the subsequent search space $[\frac{1}{\gamma_{\max}}, \frac{1}{\gamma_{\min}}]$, we design the score function  $r(\alpha)$ as the average of the watermarking model's mean detection score over each chunk ${p(\alpha)}$, and the average standard deviation of the predicted message bits ${\bar{s}_{m(\alpha)}}$. Formally, this is expressed as $r(\alpha) = \frac{1}{L} \sum_{i=1}^{L} \left( {p(\alpha)} + {\bar{s}_{m(\alpha)}} \right)$, where $L$ is the number of chunks, $\alpha$ is the speed factor, and $m(\alpha)$ is the predicted message bits. Then we conduct a two-round linear grid search to find the optimal speed factor $\alpha$ that maximizes the score function $r(\alpha)$, with the first round step size as $S_1$, and the second round step size as $S_2$. The speed change parameter $\gamma_{\min}$ and $\gamma_{\max}$ are set to 0.8 and 1.25, respectively. We conduct experiments on both watermarked and unwatermarked audio to investigate whether speed reversion might cause false positives. For the watermarked case, we report the attribution success rate/detection time, and for the unwatermarked case, we report the FPR. As shown by the results in Table~\ref{tab:speed_reversion}, with more fine-grained search space, the speed reversion layer can further improve the robustness against speed changes while maintaining the overall detection efficiency. Meanwhile, for unwatermarked audio, applying the speed reversion layer does not falsely increase the probability of being classified as watermarked by the detection model. 

\begin{table}[t]
    \centering
    \caption{
        Comparison of speed reversion layer performance. We report both the success rate and the average detection time (in seconds) for different user pool sizes $N$. The speed reversion layer significantly improves robustness against speed changes while maintaining efficient detection time.
    }
    \label{tab:speed_reversion}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l|cc|ccc|ccc}
            \toprule
            \multirow{2}{*}{\textbf{\#User}} & \multicolumn{2}{c|}{\textbf{w/o Search}} & \multicolumn{3}{c|}{\textbf{w/ Search ($S_1=0.1, S_2=0.01$)}} & \multicolumn{3}{c}{\textbf{w/ Search ($S_1=0.01, S_2=0.001$)}} \\
            \cmidrule(lr){2-3}\cmidrule(lr){4-6}\cmidrule(lr){7-9}
            & \textbf{Success Rate} & \textbf{Det. Time (s)} & \textbf{Success Rate} & \textbf{Det. Time (s)} & \textbf{FPR} & \textbf{Success Rate} & \textbf{Det. Time (s)} & \textbf{FPR} \\
            \midrule
            $N=2$ & 0.391 & 0.005 & 0.537 & 0.220 & 0.01 & \textbf{1.000} & 0.653 & 0.00 \\
            $N=10$ & 0.069 & 0.006 & 0.237 & 0.229 & 0.00 & \textbf{1.000} & 0.583 & 0.01 \\
            $N=100$ & 0.222 & 0.006 & 0.612 & 0.230 & 0.00 & \textbf{0.990} & 0.545 & 0.00 \\
            $N=1000$ & 0.000 & 0.007 & 0.163 & 0.230 & 0.00 & \textbf{1.000} & 0.497 & 0.00 \\
            
            \bottomrule
        \end{tabular}
    }
\end{table}

\subsection{Robustness to Generative Edits}
\label{app:generative_edit_robustness}
We use the DDIM-inversion-based editing approach of ZETA (Text-Based Audio Editing) introduced in \citet{manor2024zero} to manipulate the audio with text prompts. We briefly summarize the ZETA editing process in the next section. For more details on this editing process, please refer to \citet{manor2024zero}. 

\header{ZETA} Given the original signal $\boldsymbol{x}_0$ (waveform or latent space), this approach first conduct guided forward process $\mathbf{x}_t=\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \tilde{\mathbf{e}}_t, \quad t=1, \ldots, T$, where $\bar{\alpha}_t$ is the cumulative variance schedule, and $\tilde{\mathbf{e}}_t$ is the independent Gaussian noise. Then a sequence of noise vectors that capture the main characteristics of the source audio is extracted using $\mathbf{z}_t=\left(\mathbf{x}_{t-1}-\boldsymbol{\mu}_t\left(\mathbf{x}_t\right)\right) / \sigma_t, \quad t=T, \ldots, 1$, where $\boldsymbol{\mu}_t(\cdot)$ is a denoising neural network trained to predict the mean component of the noise added at time $t$. Note that a source prompt $p_\text{src}$ can be also used at the conditioning layer of the denoising neural network to guide the forward diffusion process. During the editing process, given a target prompt $p_\text{tgt}$ and a noisy state $\mathbf{x}_T$, a guided diffusion backward process is conducted via $\mathbf{x}_{t-1}=\mu_t\left(\mathbf{x}_t\right)+\sigma_t \mathbf{z}_t, t=T, \ldots, 1$, where the noise vector sequence $\mathbf{z}_t$ are the ones extracted from the previous forward process. The target prompt $p_\text{tgt}$ is embedded and applied on the cross-attention part that affects the editing trajectory.

We present the full results of robustness against generative edits in Table~\ref{tab:generative_edit_detection_more}, where we evaluate our method against two state-of-the-art audio generation models: Stable Audio~\cite{evans2024stable} and AudioLDM2~\cite{liu2024audioldm}. 
Different forward diffusion steps $T$ are used to create varying strengths of audio manipulation, ranging from 10 to 110. The source prompt $p_\text{src}$ is set to ``A recording of music'', and the target prompt $p_\text{tgt}$ is set to ``A recording of \underline{EDM} music \underline{with strong rhythm}''. We follow the default settings in the original paper for other hyperparameters.

For Stable Audio, our method achieves consistently high accuracy (0.81-0.94) and AUC scores (0.89-0.99) across different $T$ values. In contrast, AudioSeal shows more variable performance: while it achieves decent accuracy (0.69-0.78) for lower $T$ values (10-30), its performance degrades significantly as $T$ increases, with accuracy dropping to around 0.59 and AUC falling to around 0.50-0.53 for higher $T$ values. For AudioLDM2, our approach maintains strong performance with an accuracy of around 0.94 and an AUC above 0.93 across all diffusion steps. AudioSeal again struggles with higher $T$ values, showing consistent accuracy around 0.59 but poor AUC scores (0.33-0.53).

Furthermore, we also compare against Timbre Watermark \citep{liu2023detecting}, a frequency-based watermarking method that has demonstrated state-of-the-art robustness against voice cloning training pipelines. While this approach could potentially handle generative edits, which also modify audio significantly, our experiments show that it performs poorly on this task. As shown in Table~\ref{tab:generative_edit_detection_more}, Timbre Watermark achieves only random-guess level performance (accuracy around 0.50) for both Stable Audio and AudioLDM2 across different $T$ values, significantly underperforming both AudioSeal and our method.
\begin{table}[t]
    \centering
    \caption{
        We demonstrate that our watermarking approach can generalize better to the unseen, challenging generative edits setting. 
        \textbf{Detection Results} for two generative edits applied with different forward $t$ steps. 
    }
    \label{tab:generative_edit_detection_more}
    \resizebox{.9\linewidth}{!}{
    \begin{tabular}{l c *{3}{r r}}
    \toprule
    \multirow{2}{*}{\textbf{Editing Model}} & \multirow{2}{*}{{$T$}} 
    & \multicolumn{2}{c}{\textbf{AudioSeal}} 
    & \multicolumn{2}{c}{\textbf{TimbreWatermark}}
    & \multicolumn{2}{c}{\textbf{\ours}} \\
    \cmidrule(rr){3-4} \cmidrule(rr){5-6} \cmidrule(rr){7-8}
    & 
    & \textbf{Acc. \aux{(TPR/FPR)}} & \textbf{AUC}
    & \textbf{Acc. \aux{(TPR/FPR)}} & \textbf{AUC}
    & \textbf{Acc. \aux{(TPR/FPR)}} & \textbf{AUC} \\
    \midrule
    \multirow{6}{*}{\textbf{Stable Audio}}
    & 10 & 0.6875 \aux{(0.625/0.25)} & 0.7227 & 0.4688 \aux{(0.8125/0.875)} & 0.4219 & 0.9375 \aux{(0.9375/0.0625)} & 0.9883 \\
    & 30 & 0.7813 \aux{(0.875/0.3125)} & 0.8125 & 0.4063 \aux{(0.5625/0.75)} & 0.3789 & 0.8125 \aux{(0.75/0.125)} & 0.8906 \\
    & 50 & 0.5938 \aux{(0.875/0.75)} & 0.3398 & 0.4688 \aux{(0.0625/0.125)} & 0.4863 & 0.9063 \aux{(0.875/0.0625)} & 0.9688 \\
    & 70 & 0.5938 \aux{(0.75/0.5625)} & 0.5313 & 0.5313 \aux{(0.1875/0.125)} & 0.5547 & 0.9063 \aux{(0.875/0.0625)} & 0.9648 \\
    & 90 & 0.5938 \aux{(0.875/0.6875)} & 0.5469 & 0.5000 \aux{(0.5000/0.5000)} & 0.5996 & 0.9063 \aux{(0.875/0.0625)} & 0.9688 \\
    & 110 & 0.5938 \aux{(0.9375/0.75)} & 0.5039 & 0.5313 \aux{(0.5625/0.5000)} & 0.5488 & 0.9063 \aux{(0.875/0.0625)} & 0.9570 \\
    \midrule\midrule 
    \multirow{6}{*}{\textbf{AudioLDM2\footnote{Music version is used here}}}
    & 10 & 0.5938 \aux{(0.9375/0.75)} & 0.5273 & 0.5313 \aux{(0.8125/0.75)} & 0.4961 & 0.9375 \aux{(0.9375/0.0625)} & 0.9492 \\
    & 30 & 0.5938 \aux{(0.9375/0.75)} & 0.3945 & 0.4375 \aux{(0.0/0.125)} & 0.4570 & 0.9375 \aux{(0.9375/0.0625)} & 0.9414 \\
    & 50 & 0.56 \aux{(0.88/0.75)} & 0.34 & 0.5313 \aux{(0.5625/0.5000)} & 0.5645 & 0.9063 \aux{(0.875/0.0625)} & 0.9336 \\
    & 70 & 0.5938 \aux{(0.9375/0.75)} & 0.3477 & 0.5000 \aux{(0.125/0.125)} & 0.5703 & 0.9375 \aux{(0.9375/0.0625)} & 0.9453 \\
    & 90 & 0.5938 \aux{(0.9375/0.75)} & 0.3945 & 0.5000 \aux{(0.5000/0.5000)} & 0.5723 & 0.9375 \aux{(0.9375/0.0625)} & 0.9414 \\
    & 110 & 0.5938 \aux{(0.9375/0.75)} & 0.3359 & 0.5000 \aux{(0.0/0.0)} & 0.5137 & 0.9375 \aux{(0.9375/0.0625)} & 0.9414 \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Generalization Across Standard Editing with Different Strength}
\label{app: standard_editing_generalization}
We present the generalization of \ours against transformations of different strength in Figure \ref{fig:diff_trans_transferability}. Our method demonstrates remarkable robustness across a wide spectrum of audio transformations with varying intensities. For \textbf{lossy compression operations}, EnCodec shows detection accuracy of 69.5-95.0\% and attribution rates of 70.0-100\% depending on bitrate. AAC maintains strong detection (97.5\%) across all bitrates with attribution ranging from 53.0-98.0\%. MP3 compression achieves consistent detection (97.5\%) and perfect attribution (100\%) across all bitrates.

For \textbf{frequency-domain operations}, our watermark remains highly effective. When filtering out low frequencies (highpass filtering), we maintain excellent performance with 99.0-99.5\% detection accuracy and 99.0-100\% attribution accuracy, even as we vary the cutoff frequency from 500Hz to 4000Hz. Similarly, when filtering out high frequencies (lowpass filtering with cutoffs between 500-2500Hz), we achieve 97.5\% detection accuracy while maintaining 100\% attribution accuracy. For volume changes, our method handles both increases and reductions well - maintaining 97.5\% detection and 100\% attribution for volume boosts up to 90\%, and only showing slight degradation (85.0\% detection) when reducing volume to 10\% of original while still maintaining 100\% attribution.

For \textbf{additive noise}, both Gaussian and pink noise maintains 97.5\% detection accuracy up to 0.1 standard deviations, though attribution degrades from 100\% at low noise levels ($\leq$0.01) to 0\% at high intensities ($\geq$0.5). \textbf{Speed changes} achieve 94.5-97.5\% detection but, as expected, only maintain attribution at normal speed (1.0x). For \textbf{resampling operations} across frequencies from 16kHz to 1024kHz, detection remains strong (97.5\%) with perfect attribution (100\%). Echo effects across different delay times (0.1-0.5) consistently achieve 97.5\% detection and 100\% attribution, demonstrating the watermark's comprehensive resilience to common audio transformations.

\subsection{Generalization Across Different Datasets}
\label{app: dataset_generalization}
To study how \ours generalizes across different domains and datasets, we conduct in-distribution and out-of-distribution tests on multiple audio datasets. We randomly sample 100 audio clips with 5s duration from each dataset. The user size $N$ is set to 1000.

For in-distribution evaluation (Table~\ref{tab:dataset_generalization_in}), we test on FMA-Large (music), LibriSpeech (speech), VoxPopuli-10K (speech), and AudioSet (general audio). Our method achieves consistently high detection accuracy (96-99\%) and attribution accuracy (87-93\%) across all datasets and editing operations. The only exception is speed change, where attribution fails as expected since it fundamentally alters the temporal structure that watermarks rely on. We also observe slightly lower performance on EnCodec compression for FMA-Large and AudioSet, where detection accuracy drops to 72-81\% and attribution accuracy to 70-97\%, reflecting the challenge of handling highly compressed audio. Notably, the average detection accuracy across all datasets remains strong at 96-99\%, with attribution accuracy ranging from 87-93\%.

For out-of-distribution generalization (Table~\ref{tab:dataset_generalization_out}), we evaluate on four unseen datasets: AudioMarkBench (speech subset from Common Voice \citep{ardila2019common}) \citep{liu2024audiomarkbench}, ASVspoof (speech) \citep{liu2023asvspoof}, MusicGen (AI-generated music) \citep{copet2023simple}, and MUSDB18 (music) \citep{musdb18}. Our method maintains robust performance on these challenging OOD datasets, with detection accuracy ranging from 93-99\% and attribution accuracy from 86-94\%. The speech datasets (AudioMarkBench and ASVspoof) show slightly lower but still strong detection rates of 93-97\%, while music datasets (MusicGen and MUSDB18) achieve near-perfect detection at 98-99\%. Attribution performance remains consistently high across all OOD datasets, except for speed changes, which consistently fail as expected. Notably, the average attribution accuracy for OOD datasets ranges from 86-94\%, demonstrating the robustness of our watermarking approach across diverse audio domains.

These results highlight the strong generalization capabilities of our method, both in-distribution and out-of-distribution. The consistent performance across datasets and editing operations, with only expected failures (e.g., speed changes and extreme compression), underscores the practical applicability of our watermarking system in real-world scenarios where diverse audio content and editing operations are encountered.

\subsubsection{Performance across the different audio duration and watermark strength $\alpha$}
\label{app: performance_across_duration_and_strength}
To investigate the performance of our method across different audio durations and watermark strength $\alpha$, we conduct experiments with audio duration ranging from 1s to 10s and watermark strength $\alpha$ ranging from 0.0 to 1.5. As shown in Figure~\ref{fig:performance_across_duration_and_strength}, we analyze the impact of these parameters on detection and attribution accuracy.

For duration analysis, we observe that the detection accuracy remains robust (98.6--99.3\%) across all tested durations from 1--10s, demonstrating our method's effectiveness even with short audio clips. Attribution accuracy shows greater sensitivity, improving from 81.2\% at 1s to a peak of 93.0\% at 5s, then stabilizing at 91.5--92.6\% for longer durations. This suggests 5s segments provide the optimal trade-off between practical usability and attribution performance, though reliable detection ($>$$98.5\%$) is achievable with clips as short as 1s.

For watermark strength analysis, we observe three distinct operational regimes: i) weak watermarks ($\alpha \leq 0.3$), ii) moderate watermarks ($0.4 \leq \alpha \leq 0.7$), and iii) strong watermarks ($\alpha \geq 0.8$); iii) Strong watermarks ($\alpha \geq 0.8$) achieves asymptotic performance with detection $>$98.3\% and attribution $>$92.3\%, showing diminishing returns beyond $\alpha=1.0$.

Our default $\alpha=1.0$ configuration achieves 98.5\% detection and 92.7\% attribution accuracy. While higher $\alpha$ values (up to 1.5) marginally improve detection to 98.9\%, attribution plateaus at 93.1--93.2\%, suggesting an upper bound for useful watermark strength. Notably, the complete absence of watermarking ($\alpha=0$) yields chance-level detection (55.7\%) and zero attribution capability, confirming the system's dependence on the embedded signal.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/performance_analysis.pdf}
    \caption{The detection and attribution accuracy of \ours across different audio duration and watermark strength $\alpha$ on MusicCaps. The performance is averaged over all the standard audio editing. The attribution pool size is set to $\{100,1000,10000\}$.}
    \label{fig:performance_across_duration_and_strength}
\end{figure}

\begin{table*}[thbp]
    \centering
    \caption{
        The performance of detection and attribution of our method on different in-distribution datasets over different editing operations. 
    }
    \label{tab:dataset_generalization_in}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l *{2}{l} *{2}{l} *{2}{l} *{2}{l}}
        \toprule
       Dataset & \multicolumn{2}{c}{\textbf{FMA-Large}} & \multicolumn{2}{c}{\textbf{LibriSpeech}} & \multicolumn{2}{c}{\textbf{VoxPopuli-10K}} & \multicolumn{2}{c}{\textbf{AudioSet}} \\
        \cmidrule(rr){2-3} \cmidrule(rr){4-5} \cmidrule(rr){6-7} \cmidrule(rr){8-9}
        \textbf{Edit} & \textbf{Det. \aux{(TPR/FPR)}} & \textbf{Att.} & \textbf{Det. \aux{(TPR/FPR)}} & \textbf{Att.} & \textbf{Det. \aux{(TPR/FPR)}} & \textbf{Att.} & \textbf{Det. \aux{(TPR/FPR)}} & \textbf{Att.} \\
        \midrule
        Identity      & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 1.000 & 0.990 \aux{(0.980/0.000)} & 0.970 \\
        Bandpass      & 0.995 \aux{(0.990/0.000)} & 0.980 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 0.990 \\
        Boost         & 0.995 \aux{(0.990/0.000)} & 0.990 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 1.000 & 0.985 \aux{(0.980/0.010)} & 0.970 \\
        Duck          & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 0.980 \\
        Echo          & 0.990 \aux{(0.980/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 1.000 & 0.985 \aux{(0.970/0.000)} & 0.960 \\
        Highpass      & 0.995 \aux{(0.990/0.000)} & 0.990 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 \\
        Lowpass       & 0.965 \aux{(0.950/0.020)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.994 \aux{(0.994/0.006)} & 1.000 & 0.955 \aux{(0.950/0.040)} & 0.969 \\
        MP3           & 0.975 \aux{(0.970/0.020)} & 0.990 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.994 \aux{(0.994/0.006)} & 1.000 & 0.955 \aux{(0.960/0.050)} & 0.947 \\
        Pink Noise    & 0.995 \aux{(0.990/0.000)} & 0.990 & 0.995 \aux{(0.990/0.000)} & 0.920 & 0.997 \aux{(0.994/0.000)} & 0.918 & 0.995 \aux{(0.990/0.000)} & 0.940 \\
        Random Noise  & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 0.980 \\
        Smooth        & 0.990 \aux{(0.980/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 1.000 & 0.985 \aux{(0.990/0.020)} & 0.927 \\
        Speed         & 0.905 \aux{(0.830/0.020)} & 0.000 & 0.995 \aux{(0.990/0.000)} & 0.000 & 0.994 \aux{(0.994/0.006)} & 0.000 & 0.935 \aux{(0.920/0.050)} & 0.000 \\
        Resample      & 0.970 \aux{(0.960/0.020)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.994 \aux{(0.987/0.000)} & 1.000 & 0.960 \aux{(0.940/0.020)} & 0.949 \\
        AAC           & 0.990 \aux{(0.980/0.000)} & 0.960 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 1.000 & 0.980 \aux{(0.980/0.020)} & 0.950 \\
        EnCodec (nq=16) & 0.720 \aux{(0.460/0.020)} & 0.970 & 0.810 \aux{(0.730/0.110)} & 0.000 & 0.956 \aux{(0.943/0.031)} & 1.000 & 0.735 \aux{(0.620/0.150)} & 0.706 \\
        Crop          & 0.995 \aux{(0.990/0.000)} & 0.947 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.997 \aux{(0.994/0.000)} & 0.992 & 0.995 \aux{(0.990/0.000)} & 0.964 \\
        \midrule
        \textbf{Average} & 0.967 \aux{(0.939/0.006)} & 0.926 & 0.983 \aux{(0.974/0.007)} & 0.870 & 0.994 \aux{(0.990/0.003)} & 0.932 & 0.965 \aux{(0.952/0.023)} & 0.888 \\
        \bottomrule
        \end{tabular}
    }
\end{table*}

\begin{table*}[thbp]
    \centering
    \caption{
        The performance of detection and attribution of our method on different out-of-distribution datasets (unseen in training) over different editing operations. 
    }
    \label{tab:dataset_generalization_out}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l *{2}{l} *{2}{l} *{2}{l} *{2}{l}}
            \toprule
            Dataset & \multicolumn{2}{c}{\textbf{AudioMarkBench}} & \multicolumn{2}{c}{\textbf{ASVspoof}} & \multicolumn{2}{c}{\textbf{MusicGen}} & \multicolumn{2}{c}{\textbf{MUSDB18}} \\
            \cmidrule(rr){2-3} \cmidrule(rr){4-5} \cmidrule(rr){6-7} \cmidrule(rr){8-9}
            \textbf{Edit} & \textbf{Det. \aux{(TPR/FPR)}} & \textbf{Att.} & \textbf{Det. \aux{(TPR/FPR)}} & \textbf{Att.} & \textbf{Det. \aux{(TPR/FPR)}} & \textbf{Att.} & \textbf{Det. \aux{(TPR/FPR)}} & \textbf{Att.} \\
            \midrule
            Identity      & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 0.917 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            Bandpass      & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            Boost         & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.990 \aux{(0.979/0.000)} & 0.969 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            Duck          & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.990 \aux{(0.979/0.000)} & 0.969 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            Echo          & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.984 \aux{(0.979/0.010)} & 0.938 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            Highpass      & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.995 \aux{(0.990/0.000)} & 1.000 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            Lowpass       & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 0.933 & 0.969 \aux{(0.979/0.042)} & 0.926 & 0.991 \aux{(0.991/0.009)} & 1.000 \\
            MP3           & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.933 \aux{(0.867/0.000)} & 0.867 & 0.969 \aux{(0.969/0.031)} & 0.926 & 0.991 \aux{(0.983/0.000)} & 1.000 \\
            Pink Noise    & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.990 \aux{(0.979/0.000)} & 0.979 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            Random Noise  & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.990 \aux{(0.979/0.000)} & 0.896 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            Smooth        & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 0.867 & 0.974 \aux{(0.979/0.031)} & 0.895 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            Speed         & 0.938 \aux{(0.875/0.000)} & 0.000 & 0.967 \aux{(0.933/0.000)} & 0.000 & 0.911 \aux{(0.896/0.073)} & 0.000 & 0.987 \aux{(0.983/0.009)} & 0.000 \\
            Resample      & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 0.867 & 0.974 \aux{(0.979/0.031)} & 0.860 & 0.991 \aux{(0.991/0.009)} & 1.000 \\
            AAC           & 0.938 \aux{(0.875/0.000)} & 1.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.979 \aux{(0.990/0.031)} & 0.781 & 0.996 \aux{(0.991/0.000)} & 1.000 \\
            EnCodec (nq=16) & 0.875 \aux{(0.750/0.000)} & 1.000 & 0.867 \aux{(0.867/0.133)} & 1.000 & 0.885 \aux{(0.875/0.104)} & 0.833 & 0.957 \aux{(0.957/0.043)} & 1.000 \\
            Crop          & 0.938 \aux{(0.875/0.000)} & 0.000 & 0.967 \aux{(0.933/0.000)} & 1.000 & 0.891 \aux{(0.875/0.094)} & 0.871 & 0.905 \aux{(0.888/0.078)} & 0.988 \\
            \midrule
            \textbf{Average} & 0.934 \aux{(0.867/0.000)} & 0.875 & 0.958 \aux{(0.925/0.008)} & 0.908 & 0.967 \aux{(0.963/0.028)} & 0.860 & 0.986 \aux{(0.982/0.009)} & 0.937 \\
            \bottomrule
        \end{tabular}
    }
\end{table*}

\label{app: generalization_across_transformation_configurations}

\subsection{Watermark Residual Visualization}
\label{app: watermark-vis}
To better understand the characteristics of different watermarking methods, we visualize the watermark residuals in Figure \ref{fig:residual_vis}. The visualization includes waveform, spectrogram, and mel-spectrogram representations for the original audio and four watermarking approaches (\ours, AudioSeal, WavMark, and AudiowMark). Our analysis reveals several key insights: (1) Our method introduces watermarks with the lowest energy, making them less perceptible in both time and frequency domains. (2) The spectral patterns of our watermarks appear more stealthy and natural compared to other methods. (3) Unlike WavMark, which shows discrete interruptions, our watermarking pattern exhibits better continuity, contributing to enhanced robustness against audio transformations while preserving imperceptibility.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/residual_vis.pdf}
    \caption{Visualization of the original audio and watermark residuals across four different methods for a randomly selected sample from MusicCaps dataset, showing waveform (left), spectrogram (middle), and mel-spectrogram (right) representations.}
    \label{fig:residual_vis}
\end{figure}

\subsection{Visualization of TF-weighted Penalty Distribution.} 
\label{app: tf-weighted-penalty}

\header{Comparison with TF-loudness loss in AudioSeal} 
AudioSeal adopts a coarse approach to implement auditory masking, using a naive loudness difference between the noise and the original audio. The main idea is that high loudness regions should mask lower loudness regions in the watermark residual. A positive penalty is assigned when the residual loudness is higher than the one of the original audio for each tile. 

Specifically, the waveform is directly divided into $B \times W$ regions, where each region shares the same weighting term. Here, $B$ represents the number of frequency bands for splitting, and $W$ denotes the number of time tiles. The loudness difference between the watermark residual and the original waveform is then computed as $l^w_b=\text{Loudness}({{\delta}}^{w}_{b}) - \text{Loudness}(x^w_b)$, where $w$ is the window index and $b$ is the frequency band index. The weighting of the penalty term for each tile is computed in a per-band soft-max manner, $w^w_b = \frac{\exp(l^w_b)}{\sum_{b=1}^{B} \exp(l^w_b)}$. The final loundness loss is the weighted sum over all the tiles, $\ell_{Loud} = \sum_{w=1}^{W} \sum_{b=1}^{B} w^w_b \ell^w_b$. 

However, we identify two limitations of this TF-based approach: (1) it lacks consideration of sophisticated auditory masking effects, where the masker and the maskee are not in the same tile; (2) using the loudness difference as the discrepancy measure only provides weak supervision for TF-guided watermarking in the spectrogram domain. To resolve these two issues, we propose to use a more sophisticated TF-weighted $\ell_2$ loss to guide the watermarking process. First, we simulate in the TF domain the 2D energy asymmetric decay. This helps us identifying the potential masker-maskee pairs using filtering rules based on psychoacoustic principles. Secondly, instead of using loudness difference as a discrepancy measure, we compute the mean-square error between the watermarked audio and the original one in the mel-spectrogram domain. This approach provides a more dense and fine-grained guidance signal for the imperceptible watermark. 

\begin{figure*}[thbp]
    \centering
    \includegraphics[width=.7\linewidth]{figures/weighting_comparison.pdf}
    \caption{
        Visualization of the per-tile masking penalty distribution. The blue regions indicate tiles assigned a lower penalty for embedding a watermark. Our weighting scheme provides more fine-grained control. The audio sample is randomly selected from MusicCap \citep{agostinelli2023musiclm}, and the watermarked audio is generated from a model at the very beginning of training.
    }
    \label{fig:tf-weighted-penalty}
\end{figure*}

\header{Visualization of the TF-weighted penalty distribution} 
We further visualize the penalty weight distribution of different TF-based approaches of AudioSeal and \ours with the same spectrogram partitioning configuration. We use a randomly sampled watermarked audio from a model that is at the beginning of training, where the watermark still did not exploit the potential of the TF masking effect for better imperceptibility. As we can see from Figure \ref{fig:tf-weighted-penalty}, our method provides a more fine-grained and dense penalty distribution, while AudioSeal's approach just assigns the penalty weight to one tile (the red tile in the figure). Since we use multiple maskers in the vicinity to match their masked regions, our method is able to provide broader and more dense penalty guidance. This indicates that our method is more effective in exploiting the TF masking effect for better imperceptibility. 

\end{document}

