\section{Related Work: Self-improvement and Data Synthesis}
In the field of LLMs, self-generated data has become a powerful tool for enhancing model performance~\citep{liu2024best}. Researchers have explored various techniques, such as rejection sampling~\citep{touvron2023llama, dubey2024llama}, self-rewarding~\citep{yuan2024self}, and self-play~\citep{chen2024self}, enabling models to improve using synthetic data. Recent studies~\citep{snell2024scaling, brown2024large} have proposed the ``inference-time scaling law'', suggesting that increasing inference samples size boosts the likelihood of generating high-quality data.
Data synthesis techniques have also been applied to LMMs, improving general vision tasks like visual question answering~\citep{zhou2024aligning, deng2024enhancing, gao2023g, zhou2024calibrated} and enhancing instruction-following capabilities~\citep{luo2024mmevol}. Our work extends this line of research by focusing on domain-specific visual classification to enable effective visual assistance in professional tasks. In contrast to existing methods, we address the unique challenges of data synthesis in specialized domains, thereby extending these techniques to support expert-driven applications.