[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2024best",
        "author": "Liu, Ruibo and Wei, Jerry and Liu, Fangyu and Si, Chenglei and Zhang, Yanzhe and Rao, Jinmeng and Zheng, Steven and Peng, Daiyi and Yang, Diyi and Zhou, Denny and others",
        "title": "Best practices and lessons learned on synthetic data for language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yuan2024self",
        "author": "Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason",
        "title": "Self-rewarding language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chen2024self",
        "author": "Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan",
        "title": "Self-play fine-tuning converts weak language models to strong language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters"
      },
      {
        "key": "brown2024large",
        "author": "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "Large language monkeys: Scaling inference compute with repeated sampling"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhou2024aligning",
        "author": "Zhou, Yiyang and Cui, Chenhang and Rafailov, Rafael and Finn, Chelsea and Yao, Huaxiu",
        "title": "Aligning modalities in vision large language models via preference fine-tuning"
      },
      {
        "key": "deng2024enhancing",
        "author": "Deng, Yihe and Lu, Pan and Yin, Fan and Hu, Ziniu and Shen, Sheng and Zou, James and Chang, Kai-Wei and Wang, Wei",
        "title": "Enhancing Large Vision Language Models with Self-Training on Image Comprehension"
      },
      {
        "key": "gao2023g",
        "author": "Gao, Jiahui and Pi, Renjie and Zhang, Jipeng and Ye, Jiacheng and Zhong, Wanjun and Wang, Yufei and Hong, Lanqing and Han, Jianhua and Xu, Hang and Li, Zhenguo and others",
        "title": "G-llava: Solving geometric problem with multi-modal large language model"
      },
      {
        "key": "zhou2024calibrated",
        "author": "Zhou, Yiyang and Fan, Zhiyuan and Cheng, Dongjie and Yang, Sihan and Chen, Zhaorun and Cui, Chenhang and Wang, Xiyao and Li, Yun and Zhang, Linjun and Yao, Huaxiu",
        "title": "Calibrated self-rewarding vision language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "luo2024mmevol",
        "author": "Luo, Run and Zhang, Haonan and Chen, Longze and Lin, Ting-En and Liu, Xiong and Wu, Yuchuan and Yang, Min and Wang, Minzheng and Zeng, Pengpeng and Gao, Lianli and others",
        "title": "MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct"
      }
    ]
  }
]