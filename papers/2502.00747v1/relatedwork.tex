\section{Related Work}
\paragraph{Modularity of Task-Oriented Dialogue Systems}
In typical pipeline task-oriented dialogue systems, dedicated modules for each subtask, such as NLU, DST, policy, and NLG, have been individually developed and optimized~\citep{zhang2020recent}. However, in recent years, methods addressing multiple subtasks using a single model have become common~\citep{ni2022recent}. For example, word-level DST~\citep{wu-etal-2019-transferable, zhao2022description} estimates the dialogue state directly from the dialogue history without requiring user intent estimation using NLU. Similarly, a word-level policy ~\citep{lubis-etal-2020-lava, wang-etal-2020-multi-domain} generates system responses directly from the dialogue state without requiring conversion from DA to system utterances by NLG. Furthermore, end-to-end dialogue systems~\citep{NEURIPS2020_e9462095, He_Dai_Zheng_Wu_Cao_Liu_Jiang_Yang_Huang_Si_Sun_Li_2022, wu-etal-2023-diacttod}, which learn all subtasks using a single model, are becoming popular. Because these end-to-end systems maintain modularity by sequentially executing each subtask, they are often referred to as modularly end-to-end systems ~\citep{qin-etal-2023-end}.

\paragraph{Online RL for Task Completion}
In response generation for task-oriented dialogue systems, it is crucial not only to maximize the probability of reference tokens in corpora but also to maximize task completion capability in actual multi-turn dialogues~\citep{kwan2023survey}. Some studies~\citep{liu-etal-2018-dialogue, tseng-etal-2021-transferable} employed online RL frameworks to train dialogue systems based on experiences obtained online from interactions with users. For example,  research has focused on optimizing DST~\citep{10.1007/978-981-99-2401-1_25} or policy ~\citep{li-etal-2020-guided, deng2024plugandplay} within pipeline systems. Additionally, \citet{zhao-eskenazi-2016-towards} demonstrated that jointly optimizing DST and policy with shared parameters outperforms systems in which the DST and policy are trained separately. Our proposed method also utilizes an online RL framework  to optimize dialogue systems. However, contrary to previous studies that focused on learning the modules, we concentrate on learning to modify the outputs of these modules.

\paragraph{Post-Processing Networks}
Methods that train modules via RL cannot be applied to dialogue systems with non-trainable modules, such as rule-based or API-based modules, as expected in real-world scenarios. To address this issue, \citet{ohashi-higashinaka-2022-post} proposed optimizing BinPPNs instead of the modules. BinPPNs modify the outputs of NLU, DST, and policy, and are optimized using online RL. Specifically, BinPPNs perform post-processing on the set of slot-value pairs output by each module through binary classification to determine whether to delete (0) or maintain (1) each pair. To handle post-processing for NLG, which outputs natural language rather than a set of slot-value pairs, \citet{ohashi-higashinaka-2023-enhancing} introduced GenPPN, which uses LLMs to paraphrase the system utterance output by NLG, to improve task completion by generating system utterances that can be easily understood by users. It is optimized through RL based on  feedback regarding whether DAs are correctly conveyed to users.

Our proposed UniPPN can process arbitrary sequences as both input and output. Therefore, contrary to BinPPN, which is limited to binary decisions of deletes/maintenance, it allows for more flexible post-processing. In addition, contrary to GenPPN, which is optimized using detailed feedback on DAs, UniPPN is optimized solely using the task success/failure signal obtained at the end of the dialogue. This makes it applicable to a wide range of systems, including word-level policies and end-to-end systems.

\begin{figure*}
\centering
\includegraphics[width=1\linewidth]{figures/pseudo_pp_demo_creation.pdf}
\caption{Procedure for creating pseudo-post-processing demonstration data. First, we generate dialogues between the dialogue system and the user simulator. Subsequently, we create pairs of positive and negative outputs, where the output $\text{out}_t$ of module $m$ for context $s_t$ at turn $t$ is positive and the output $\text{out}_u$ at another turn $u$ is negative (i.e., $\text{out}_t^-$). In imitation learning stage, the reconstruction from $\text{out}_t^-$ to $\text{out}_t$ is learned as pseudo-post-processing.}
\label{fig:pseudo_pp_demo_creation}
\end{figure*}