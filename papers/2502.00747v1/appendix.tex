\appendix

\part*{\centering Appendix}

\section{Reinforcement Learning Algorithm}
\label{appendix:sec:rl_algorithm}

In the implementation of our proposed method, we used Proximal Policy Optimization (PPO)~\citep{schulman2017proximal} as the optimization algorithm for $\pi$. In addition, we employed the generalized advantage estimate ~\citep{schulman2015high} to implement $\Psi_{(t,m)}$. The value network $V_\psi$ is trained to minimize the mean squared error with a cumulative reward:
\begin{equation}
\label{equ:value_loss}
\mathcal{L}_V(\psi) = \left( V_\psi(\bm{x}_{(t,m)}) - \sum_{i=0}^T\sum_{j=1}^M \gamma^{(i+1)(j-1)}r_{(t,m)+(i,j)} \right)^2
\end{equation}
Here, $(t,m)+(i,j)$ represents the progression of timesteps in the module-level MDP, similar to Eq. ~(\ref{equ:module_level_timestep_increment}). $V_\psi$ estimates the state value of each module $m$ at each turn $t$. The final policy objective is formulated based on Eq. ~(\ref{equ:module_level_policy_gradient}), and the clipped surrogate objective~\citep{schulman2017proximal}:
\begin{multline}
\label{equ:policy_loss}
\mathcal{L}_\pi(\phi) = -\mathbb{E} \Bigg[\sum_{t=0}^{T} \sum_{m=1}^M \min \bigg( \zeta_{(t,m)}\hat{A}_{(t,m)} ,\\
\text{clip}(\zeta_{(t,m)},1-\epsilon, 1+\epsilon) \hat{A}_{(t,m)} \bigg) \Bigg]
\end{multline}
$\zeta_{(t,m)} = \pi_\phi(\bm{y}_{(t,m)} | \bm{x}_{(t,m)}) / \pi_{\phi_\text{old}}(\bm{y}_{(t,m)} | \bm{x}_{(t,m)})$ is the ratio of probabilities between the policy being updated and the policy before update in each iteration.

Following previous studies~\citep{ziegler2019fine, NEURIPS2020_1f89885d}, to prevent the probability distribution output by the policy network $\pi_\phi^\text{RL}$ from deviating too far from the original $\pi_\phi^\text{IL}$ obtained by IL and breaking the output, we used a penalty based on Kullbackâ€“Leibler (KL) divergence as the final immediate reward $r_{(t,m)}$:
\begin{multline}
\label{equ:immediate_reward}
r_{(t,m)} = R(t,m) - \beta\mathbb{D}_\text{KL}\big[\pi_\phi^\text{RL}(\bm{y}_{(t,m)}|\bm{x}_{(t,m)}) \| \\
\pi_\phi^\text{IL}(\bm{y}_{(t,m)}|\bm{x}_{(t,m)})\big]
\end{multline}
where $\beta$ denotes a hyperparameter indicating the KL penalty coefficient. Algorithm~\ref{alg:ppo_algorithm} summarizes the RL algorithm.

\begin{algorithm}[ht]
\caption{UniPPN Optimization Algorithm with PPO}
\label{alg:ppo_algorithm}
\begin{algorithmic}[1] %[1] enables line numbers
\REQUIRE Policy network $\pi_\phi$, value function $V_\psi$\\
\REQUIRE Dialogue system $\mathcal{F}$, dialogue environment $\mathcal{P}$
\STATE Install $\pi_\phi$ to the dialogue system $\mathcal{F}$\\
\FOR{each iteration}
\STATE Initialize replay buffer $\mathcal{D}$
\WHILE{$|\mathcal{D}| < \text{batch size } B \times M$}
\STATE // $\mathcal{F}$ interacts with $\mathcal{P}$
\FOR{each turn $t$}
\FOR{each $\text{Module}_m$}
\STATE Sample $\bm{y}_{(t,m)}$ $\sim$ $\pi_\phi(\bm{y}_{(t,m)} | \bm{x}_{(t,m)})$ by post-processing the output of $\text{Module}_m$ \\
\STATE Calculate reward $r_{(t,m)}$ by Eq.~(\ref{equ:immediate_reward})\\
\STATE Add $(\bm{x}_{(t,m)}, \bm{y}_{(t,m)}, r_{(t,m)})$ to $\mathcal{D}$
\ENDFOR
\ENDFOR
\ENDWHILE
\STATE Compute advantage estimate $\hat{A}_{(t,m)}$ by Eq.~(\ref{equ:advantage_estimate})
\FOR{each inner epoch}
\STATE Update $\psi$ by Eq.~(\ref{equ:value_loss})
\STATE Update $\phi$ by Eq.~(\ref{equ:policy_loss})
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Details of UniPPN Training}
\label{appendix:sec:details_unippn_training}
We adopted the 355M parameter version of GPT-2 as the backbone model for UniPPN. The outputs of some modules, such as NLU and DST contain redundant occurrences of domain names or other symbols (e.g., \texttt{[restaurant, -, price, range, =, cheap, ; restaurant, -, food, =, italian]} with 12 tokens). This redundancy may complicate LM's generation task and hinder exploration during RL. To address this, we add tokens representing domain and slot name pairs and assign one token to a pair to simplify the sequence format (e.g., \texttt{[restaurant-pricerange=, cheap, restaurant-food=, italian]} with four tokens). This enables the model to focus solely on learning to modify $\text{out}_{(t,m)}$. The embedding parameters for these new vocabularies were randomly initialized. Below, we describe the learning details of IL and RL, as well as the impact of the module-level MDP.

\subsection{Imitation Learning}
In the supervised fine-tuning of $\pi_\theta$ using $D_{1:M}$, we divided $D_m$ into mini-batches of size 64 and trained them for 10 epochs using the AdamW optimizer. For the first five epochs, we trained only the embedding vectors of the newly added vocabulary for the slots with a learning rate of $5 \times 10^{-3}$, whereas for the latter five epochs, we trained the entire $\pi_\theta$ with $5 \times 10^{-5}$. This prevents random gradients caused by randomly initialized embedding parameters from destroying $\theta$ as a whole during the early stages of learning.

\subsection{Reinforcement Learning}
For $\pi$'s generation parameters during RL, we set the maximum number of input and output tokens to 256 and 128, respectively, and both the top $p$ and temperature to 1.0, to promote exploration. The total number of iterations for the entire learning process was set to 200, and the number of turns sampled in each iteration was 1,024. In each iteration, the trajectories of the post-processing outputs from all modules accumulated in the replay buffer were trained for four epochs with a mini-batch size of 128 and gradient accumulation steps $M$. We used the Adam Optimizer with a learning rate of $1 \times 10^{-6}$. $\gamma$ and $\lambda$ were set to 0.99 and 0.95 respectively, and the coefficient $\beta$ of the KL divergence penalty was set to 0.01.

\subsection{Impact of Module-level MDP}
\label{appendix:sec:impact_module_level_mdp}

We examined the contribution of the module-level MDP, introduced in our optimization algorithm, to UniPPN learning. We trained UniPPN for all the modules of SYS$_\text{PPO}$ with and without module-level MDP. During training without module-level MDP, all actions performed by UniPPN in the same turn received the same value estimation (called turn-level MDP). For each setup, the training was conducted using three random seeds.

\begin{figure}[tbp]
\begin{minipage}[t]{1\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/unippn_learning_curve_tasks.pdf}
\subcaption{Task completion metrics}
\label{fig:unippn_learning_curve_tasks}
\end{minipage}
\begin{minipage}[t]{1\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/unippn_learning_curve_advantages.pdf}
\subcaption{Advantage estimates}
\label{fig:unippn_learning_curve_advantages}
\end{minipage} 
\caption{Learning curve where UniPPN was applied to SYS$_{\text{PPO}}$}
\label{fig:unippn_learning_curve}
\end{figure}

Figure~\ref{fig:unippn_learning_curve_tasks} shows the learning curves for the success rate and number of turns. The introduction of the module-level MDP improved the scores. Furthermore, turn-level MDP becomes particularly unstable in the latter half of learning, and the final performance decreases. To investigate this, we plotted the advantage estimates for the post-processing of each module (Figure~\ref{fig:unippn_learning_curve_advantages}). The plots reveal that advantage estimation is significantly destabilized and destroyed in the turn-level MDP. This can be considered an example of a credit assignment problem in RL. Specifically, assigning the same value estimation to all actions (i.e., the post-processing outputs of all modules) performed within the same turn fails to estimate the proper contribution of each action,  resulting in learning instability. This instability is believed to significantly affect the final system performance.

\section{Human Evaluation Details}
\label{appendix:sec:human_evaluation_detail}
The procedure of the human evaluation experiment followed these steps. First, each worker read a user goal randomly created for each dialogue. Next, the worker conducted a dialogue with one of the three systems (SYS$_\text{PPO}$, +BinPPN\&GenPPN, +UniPPN). The maximum number of turns for the dialogue was set to 20, which is the same as that used in the automatic evaluation experiment. Each worker judges whether a goal has been achieved within the maximum number of turns. After the dialogue ended, the participants answered three subjective evaluation questionnaires. To ensure the quality of the subjects, we only recruited workers with masters qualifications\footnote{\url{https://www.mturk.com/worker/help#what_is_master_worker}} from AMT. In addition, we restricted each worker from participating in the experiment only once. The reward for each participant was set to \$2, considering the time required per task (around ten minutes or less) and the minimum wage in the U.S.

\begin{table}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/case_study_nlu_policy.pdf}
\caption{Dialogue history between SYS$_\text{PPO}$ with UniPPN and a crowdworker from the human evaluation experiment. The rows below each module (that is, NLU, DST, policy, and NLG) display the results of post-processing by UniPPN. Text with strikethrough indicates information deleted by UniPPN, while bold text representsand information added.}
\label{tab:case_study}
\end{table}

To examine the performance of UniPPN, we quantitatively analyzed the dialogue history between the systems and crowdworkers. Table~\ref{tab:case_study} displays the response history for SYS$_\text{PPO}$ with UniPPN applied. Initially, for the user's input utterance, the original NLU failed to recognize that the area was ``centre'' and only output that the user was looking for a restaurant. UniPPN corrected the output by adding ``area=centre.'' For the DST output, UniPPN judged that it was not problematic and maintained (copied) the output without modifications. Regarding policy output, UniPPN added new DAs to restaurant names and phone numbers. This is probably because, during RL with the user simulator, UniPPN learned that adding a name and other information increased the likelihood of task success. Note that in dialogues with humans, outputting excessive information not explicitly requested by the user might negatively impact dialogue satisfaction scores. From the analysis, we confirmed cases where UniPPN, developed in a simulation environment, was also effective in dialogue with humans, resulting in improved task completion performance.
