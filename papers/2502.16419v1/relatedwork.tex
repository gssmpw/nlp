\section{Related Work}
\subsection{Multi-view 3D Human Pose Estimation Model}
Most multi-view 3D human pose estimation methods adopt a two-stage processing framework. 
These methods first estimate 2D human poses from images of each view, then map the 2D poses from multiple views into 3D space to construct the overall 3D pose structure. 
The advantage of this approach lies in leveraging existing 2D pose estimation techniques and achieving high initial 2D detection accuracy. 
Shuai~\textit{et al.}~\cite{shuai2022adaptive} introduce the MTF-Transformer, which employs feature extractors, multi-view fusion transformers, and temporal fusion transformers to adaptively handle varying views and video lengths. 
This approach excels in processing uncalibrated multi-view sequences, effectively improving the model's adaptability and accuracy across diverse scenarios.
Similarly, Cai~\textit{et al.}~\cite{cai2024fusionformer} propose FusionFormer, a concise unified feature fusion transformer. 
By leveraging a unified feature fusion scheme to integrate multi-frame and multi-view features, FusionFormer not only reduces the impact of depth uncertainty but also achieves efficient 3D pose estimation with a compact model size and low computational cost, offering new perspectives for model optimization and practical applications. 
Zhao~\textit{et al.}~\cite{zhao2023triangulation} a triangulation residual loss is proposed for multi-view 3D pose estimation. 
By utilizing multi-view geometric consistency for self-supervised training, this approach addresses the challenge of limited annotated data, providing an effective solution to data scarcity.

With the development of deep learning, end-to-end multi-view 3D pose estimation models have become a research hotspot. 
Unlike traditional methods that rely on intermediate 2D pose estimations, end-to-end methods use a unified network to directly estimate 3D poses from multi-view images. 
This approach allows the model to automatically learn the relationships between features from different views, directly optimizing the 3D pose results, thus reducing information loss and error accumulation during data transmission. 
Pavlakos~\textit{et al.}~\cite{pavlakos2017coarse} propose a method for 3D human pose estimation from a single RGB image, formulating it as a keypoint localization problem in discrete space.
A coarse-to-fine prediction scheme is employed to address high-dimensional challenges, achieving superior performance over existing methods.
The MvP model~\cite{zhang2021direct} is introduced for multi-view, multi-person 3D pose estimation. 
This model directly regresses the 3D poses of multiple individuals from multi-view images using a carefully designed query embedding scheme and projection attention mechanism to efficiently fuse multi-view information. 
The approach demonstrates strong performance and efficiency on multiple benchmark datasets, providing a more effective solution for multi-view, multi-person 3D pose estimation.

Despite their achievements, these methods fail to address application scenarios involving occlusions and often feature complex model mechanisms, limiting their practical applicability and broader usage.

\subsection{Multi-view Feature Fusion Strategy}
Multi-view feature fusion is a critical component in 3D human pose estimation. Bartol~\textit{et al.}~\cite{bartol2022generalizable} introduce a generalizable random framework for human pose triangulation. In terms of feature fusion, it integrates multi-view information by generating random hypotheses. 
For each joint, a random subset of views is selected, and 3D joint coordinates are obtained through triangulation, forming a 3D human pose hypothesis. A scoring neural network is then used to evaluate these hypotheses. 
The network takes in 3D pose coordinates that are specially normalized, including selecting three specific points for rotation calculation and applying this to the coordinates, as well as extracting body part lengths and concatenating them into vectors. 
The final pose estimate is determined by a weighted averaging strategy, where the hypotheses with higher scores are given greater weight. 
Jiang~\textit{et al.}~\cite{jiang2023probabilistic} focus on uncalibrated multi-view 3D human pose estimation. 
Its probabilistic triangulation module models camera poses using probability distributions, iteratively updating the distribution via Monte Carlo sampling. During inference, the proposed camera pose distributions are maintained, and the network updates the weights by sampling to learn the camera poses from 2D features, achieving multi-view feature fusion. 
In the 3D pose reconstruction network, the results from multiple iterations are weighted and averaged as inputs to obtain the final 3D pose estimate. 

Ma~\textit{et al.}~\cite{ma2023self} design a factorization network to predict normalized 3D human poses and camera viewpoint coefficients. 
It takes two random views of 2D skeletons as input and introduces multi-view information constraints into pose prediction. 
By calculating reprojection errors and consistent factorization losses, the former measures the difference between the input and reprojected 2D poses, and the latter ensures that the normalized 2D poses from different views are consistently reconstructed. 
This enables the network to learn accurate pose representations from multi-view information, thus achieving multi-view feature fusion. 
Xia~\textit{et al.}~\cite{xia2022vitpose} propose a simple feature fusion network that introduces positional information into the Transformer structure through multi-view geometric calibration, allowing the network to perceive the spatial relationships between views. 
During feature fusion, for corresponding feature points from the source and reference views, a line-based fusion method is adopted due to the depth uncertainty of the reference viewâ€™s feature points. Additionally, a fusion weight adjustment strategy is employed based on the similarity between the fusion heatmap and the ground truth heatmap, effectively integrating multi-view features and improving pose estimation accuracy.

These methods have made significant progress in multi-view feature fusion, effectively utilizing information from different viewpoints to enhance the accuracy of 3D pose estimation. 
However, they still exhibit some limitations when addressing deficiency-aware problems, including loss of feature information, reduced fusion accuracy between viewpoints, and excessive reliance on assumptions, which lead to inaccurate pose estimation when deficiency-aware issues are severe.

\subsection{Methods for Handling Noise and Occlusion Issues}
Occlusion and noise are major challenges in 3D human pose estimation. 
Zhang~\textit{et al.}~\cite{zhang20233d} introduce the 3D-Aware Neural Body Fitting (3DNBF) framework, which achieves 3D human pose estimation through feature-level synthesis and analysis, providing high robustness to occlusion. 
The framework uses Neural Body Volumes (NBV) as the explicit volumetric base model for the human body, composed of Gaussian ellipsoid kernels. 
This method allows for feature-level rendering of the human body and has several advantages over mesh-based representations. 
Additionally, a contrastive learning framework is used to train the feature extractor, addressing the 2D-3D ambiguity problem.
Yu ~\textit{et al.}~\cite{yu2024yolo} designed the SEAM attention module to enhance the feature learning of occluded faces and introduced the Repulsion Loss to address the face occlusion problem. Additionally, they utilized the information of the effective receptive field to design the anchor for improving the detection accuracy.
Lei Ke ~\textit{et al.}~\cite{ke2021deep} proposed the Bilayer Convolutional Network (BCNet). 
They adopted a bilayer graph convolutional network structure to model the occluder and the occludee respectively and took their interactions into account during the mask regression stage, thus effectively handling the occlusion problem.
Cheng~\textit{et al.}~\cite{cheng2019occlusion} introduce an occlusion-aware deep learning framework that utilizes 2D confidence heatmaps and optical flow consistency constraints to filter out unreliable occlusion estimates of keypoints. 
This framework combines 2D and 3D temporal convolutional networks (TCNs) to handle incomplete 2D keypoints. 
The framework consists of 2D pose estimation (using stacked hourglass networks and optical flow to compute confidence scores, followed by 2D TCN to process incomplete keypoints), 3D pose estimation (using 3D TCNs to obtain 3D poses and calculate multiple losses), and a cylindrical human model (for data augmentation and pose regularization, by projecting keypoint visibility and expanding the dataset). 

These methods provide important insights into improving human pose estimation performance in complex scenarios. 
However, since occlusion issues severely impact 2D detection results, they also lead to a significant drop in 3D detection performance.
Therefore, we bypass the 2D detection results through an end-to-end approach and address the deficiency-aware problem by utilizing adaptive multi-view feature fusion. 
This method enables us to effectively integrate information from different viewpoints, reducing the impact of deficiency-aware issues on the final 3D pose estimation, thereby enhancing the robustness and accuracy of the model in complex scenarios.