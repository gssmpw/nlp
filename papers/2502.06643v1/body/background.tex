\section{Background}\label{sec:background}

\subsection{Mixture-of-Experts (MoE)}
Mixture-of-Experts (MoE) models have emerged as a powerful approach for scaling deep learning architectures by partitioning model parameters into specialized subsets, known as “experts”, which process tokens selectively~\cite{gemma, switchtransformer, mixtral,gshard,shazeer2017outrageously,deepspeed-moe}. 
%
This selective routing of tokens to specific experts enables MoE models to achieve high model capacity without a proportional increase in computational costs, making them particularly effective for diverse tasks and datasets.
%
In MoE models, a sparse routing mechanism directs each token to a small fraction of experts—typically one or two per token~\cite{switchtransformer,gshard}.
%
This selective activation allows the model to maintain a low computational footprint while preserving the expressiveness of larger architectures, as only a subset of parameters is activated for each input. 
%
Consequently, MoE models are able to optimize both memory and compute efficiency per token processing, as only the necessary parameters are involved in the processing of each token.

The structure of an MoE model typically involves feed-forward layers interspersed with a router that determines which experts to activate based on token characteristics. 
%
Some models, such as Mixtral, employ a top-2 router that routes each token to the two most relevant experts, balancing model versatility and computational cost~\cite{mixtral}.
%
This routing design allows the model to adaptively distribute workload among experts, enhancing both accuracy and efficiency across varied inputs. 
%
% For smoother transition, bring up the limitations of MoE models (memory capacity requirements), which necessitates the use of parallelization strategies.
%
Despite these advantages, MoE models face significant challenges in deployment as the number of experts increases.
%
While selective activation mitigates computational costs, the excessively scaled number of parameters can result in memory capacity constraints~\cite{mcsmoe,hwang2024pre,huang2023towards,kim2021scalable}.
%
These memory constraints necessitate the use of parallelization strategies to distribute both model parameters and computation across devices.
%
%However, such strategies introduce additional challenges in large-scale setups, including imbalanced workloads across devices and increased communication overhead.
%
%As a result, addressing these issues is essential to ensure the efficient scaling of MoE models while maintaining high throughput and hardware utilization.

% However, the selective activation of experts introduces unique challenges in distributed training and inference setups, where expert parallelism is crucial for efficient scaling. 
%
% Expert parallelism allows the distribution of experts across multiple GPUs to manage memory demands, but as model sizes grow and routing complexity increases, it becomes essential to balance computational workloads and manage memory effectively across devices. 
%
% These factors underscore the need for strategies to optimize expert placement and communication across GPUs, particularly as MoE models scale to handle larger, more complex datasets.

\subsection{Expert Parallelism and Token Dispatching}
%
To manage the massive parameter size of MoE models, distributed systems employ expert parallelism—a model-parallel strategy designed specifically for MoE architectures.
%
Expert parallelism partitions experts across multiple GPUs, distributing computational workloads and reducing the memory footprint on each GPU.
% 
This approach enables the model to scale without requiring an impractically large amount of memory on individual GPUs.
% 
There are two primary techniques for distributing experts across devices to reduce their memory footprint. The first evenly splits the number of experts across devices~\cite{deepspeed, deepspeed-moe, megatron, exflow}, while the second splits each expert and distributes its components across devices~\cite{vllm, megatron,singh2023hybrid}.
% 
Figure~\ref{fig:background-ep}(a) illustrates an example MoE model on a single GPU, while (b) demonstrates expert parallel execution across four devices, which involves all-to-all communication~\cite{nccl} to gather the expert data.
% 
In the second expert parallel setup, tokens are routed to remote GPUs using all-to-all communication based on their assigned experts. This mechanism ensures that tokens are processed by the most relevant experts.
% 
Despite these strategies, both techniques distribute experts evenly based solely on memory footprint, without accounting for the computational load on each expert.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.36\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/moe_single_gpu.pdf}
        \caption{Execution of an MoE model on a single GPU with 8 experts. }
        \label{fig:moe-single-gpu}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.54\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/moe_expert_parallel.pdf}
        \caption{Expert-parallel execution with an expert parallel size of 4. Experts are distributed across GPUs, with all-to-all operations before and after the FFN computations.}
        \label{fig:moe-expert-parallel}
    \end{subfigure}
    \vspace{-0.8em}
    \caption{An example of Mixture-of-Experts (MoE) model execution. (a) Single-GPU execution with all experts local to the GPU. (b) Expert-parallel execution, where experts are distributed across GPUs, requiring inter-GPU communication through all-to-all operations.}
    \label{fig:background-ep}
\end{figure}
\vspace{-1em}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/EP_Detail.pdf}
%     \caption{\seokjin{EP Example. TODO: \seokjin{is this required?}}}
%     \label{fig:EP_Example}
% \end{figure}


\subsection{Related Work}


\niparagraph{Distributing deep learning models across devices.}
% Briefly talk about DP/TP/PP and related works.
Distributing deep learning models across multiple devices has been a common strategy to overcome memory and computational limitations. 
%
Data parallelism~\cite{li2020pytorch,zhao2023pytorch} replicates the model across devices, with each device processing a subset of the input data independently and synchronizing gradients during updates.
%
Pipeline parallelism~\cite{huang2019gpipe,narayanan2019pipedream,narayanan2021memory} partitions a model into sequential stages, assigning each stage to a different device, and enables concurrent processing of inputs for different batches.
%
Model parallelism~\cite{megatron, switchtransformer}, unlike data and pipeline parallelism, partitions the model itself across multiple devices.
%
For instance, in tensor parallelism, large tensor computations are split into smaller sub-operations distributed across devices. 
%
Each device computes a portion of the operation, exchanging intermediate results with others during the forward and backward passes. 
%
This approach enables running models that exceed the memory capacity of a single device but introduces inter-device communication overhead, particularly for large matrix multiplications.
%
Expert parallelism~\cite{switchtransformer,deepspeed-moe,cai2024shortcut,chen2022ta} is a form of model parallelism tailored for MoE models, where the experts are distributed across devices.

\niparagraph{System level optimizations for MoE.}
To enhance scalability and efficiency in distributed training and inference of MoE models, various system level optimizations have been introduced.
%
DeepSpeed-MoE~\cite{deepspeed-moe} integrates multidimensional parallelism and hierarchical all-to-all algorithm to improve the scalability of distributed MoE inference.
%
Tutel~\cite{hwang2023tutel} incorporates adaptive parallelism and pipelining optimization during runtime to accommodate the dynamic nature of MoE models.
%
Other works, such as Switch Transformers~\cite{switchtransformer}, reduce the number of active experts per input to simplify routing and lower memory overhead. 
%
However, this often exacerbates token routing imbalances and leads to suboptimal resource utilization in distributed settings.
%
While these approaches aim to improve throughput and reduce resource overhead, they often fail to address critical challenges such as activation sparsity overheads and the load imbalances inherent in MoE models.
%
As a result, critical bottlenecks like inter-GPU communication and workload imbalances caused by real-world token routing patterns remain unaddressed, limiting their effectiveness in large-scale deployments.

\niparagraph{Expert parallelism to mitigate the memory bottleneck of MoE models.}
%
Several prior works have addressed the challenges of optimizing expert parallelism through techniques that focus on overlapping expert computation with communication or reducing the total number of communication steps.
%~\cite{li2023accelerating,fastermoe,exflow}.
%
Lancet\cite{jiang2024lancet} improves MoE performance by overlapping non-MoE computations with all-to-all through careful partitioning and pipelining of the training graph.
%
FasterMoE\cite{fastermoe} introduces a congestion-avoiding expert selection strategy that dynamically adjusts token assignments to relieve network congestion, thereby reducing training latency in distributed MoE models. 
%
Lina\cite{li2023accelerating} employs all-to-all prioritization and dynamic resource scheduling to mitigate communication bottlenecks and reduce resource contention during both training and inference. 
%
ExFlow~\cite{exflow} exploits token routing dependencies between different layers by strategically placing experts with the highest interdependencies on the same GPU. 
%
These works often aim to maximize GPU utilization by overlapping the computation of one expert with the communication for another, or by reducing the communication frequency through techniques like pipelining. 
%
However, they tend to overlook the impact of communication imbalance on tail latency, which becomes increasingly important in large-scale MoE models. 
%
% This is especially true when tokens are dispatched to GPUs with high communication latency, causing some GPUs to be overburdened with token processing while others remain underutilized.

In contrast, our approach specifically targets tail latency by formulating an ILP-based optimization strategy that balances both computation and communication loads. By considering token processing and communication overhead together, our method ensures more efficient scaling and improved performance under distributed conditions.