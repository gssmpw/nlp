\begin{abstract}
%
Mixture-of-Experts (MoE) model architecture has emerged as a promising solution for scaling transformer models efficiently, offering sparse activation that reduces computational costs while increasing model capacity. 
%
MoE models activate only specific experts per token, unlike dense models that utilize the entire model for every token. This design allows for an increase in the total number of parameters through the inclusion of multiple experts, without a corresponding increase in computational intensity.
%
However, as MoE models scale, they need to be distributed across GPU devices, thus face critical performance bottlenecks due to their large memory footprint.
%
Expert parallelism distributes experts across GPUs, however, faces key challenges including an unbalanced token routing and expert activation, resulting in communication tail latency and processing inefficiencies. 
%
While existing solutions address some of these issues, they fail to resolve the dual challenges of load imbalance and communication skew.
%
The imbalance in token processing load across experts causes uneven processing times on different GPUs, while communication skew between GPUs leads to unbalanced inter-GPU data transfers. These factors degrade the performance of MoE models by increasing tail latency and reducing overall throughput.
%
To address these limitations, we propose an Integer Linear Programming (ILP) formulation to optimize expert placement by jointly considering token load, communication, and computation costs. 
%
We exploit the property that there is a token routing dependency across layers, where tokens routed to a specific expert in one layer are likely to be routed to a limited set of experts in the subsequent layer.
%
Our solution, \expertune, offers an optimal expert-to-GPU assignment that minimizes inter-GPU token routing costs and balances token processing across devices, thereby reducing tail latency and end-to-end execution time.
%
By resolving these systemic inefficiencies, our method significantly enhances the performance and efficiency of MoE models in a distributed setting. 
%
Experimental results demonstrate 9.3\% and 17.5\% of end-to-end speedups for single-node and multi-node inference respectively, showcasing the potential of our ILP-based optimization for offering expert parallel solutions for next-generation MoEs. 

% \todoseokjin{Include numbers here.}
\end{abstract}