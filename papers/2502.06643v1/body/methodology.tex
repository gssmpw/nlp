\section{\expertune~for Expert Placement}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/methodology-overview.pdf}
    \caption{Overview of the \expertune~framework.}
    \label{fig:methodology-overview}
    \vspace{-3ex}
\end{figure*}


Based on the observations presented in Section \ref{sec:background}, we propose an \expertune~ framework a novel way to optimize expert placement for MoE models. This framework is built on the insight that inter-layer token routing exhibits predictable dependencies.
% 
The primary goal of \expertune~is to develop an expert placement strategy that minimizes two critical factors: the imbalance of token processing load across GPUs and the inter-GPU communication overhead.
%
To leverage the expert routing dependency, we need to determine the routing per dataset.
%
However, using the entire dataset to determine the token routing can be prohibitive. 
%
Instead, we observe that we can perform inference on a small subset of the dataset for each task over a predefined number of iterations.
% 
Our experiments in Section~\ref{subsec:designoverview} reveal that the routing patterns of the sampled dataset reliably approximate the overall routing behavior across the full dataset for a given task.
% 
This method significantly reduces computational overhead while still capturing representative routing information.
%
Using the collected routing statistics, \expertune~formulates the expert placement problem as an Integer Linear Programming (ILP) optimization. The ILP optimization integrates the routing data into its constraints and objectives, enabling simultaneous optimization of load balance and communication costs across GPUs.
% 
We use ILP as it offers optimal solutions under the given constraints, ensuring the best expert placement strategies. 
%
The \expertune~framework operates in two key stages, each modeled as an ILP: (1) Load-Balanced Expert Clustering where we group experts based on routing dependencies while maintaining balanced loads, and (2) Cluster-to-GPU Assignment where these are mapped clusters to GPUs to minimize communication overhead while preserving load balance.
%
The ILP is formulated using the following:

\begin{itemize}[leftmargin=*]
    \vspace{-0.5em}
    \item \textbf{Modeling Per-GPU Token Processing Load:}
    Using token routing statistics, we quantify the token processing load for each GPU by assigning a weight to each expert proportional to its token demands. This weight reflects the computational workload associated with processing tokens routed to the expert. To ensure balanced token processing, we introduce constraints in the ILP that evenly distribute these loads across GPUs within their capacity limits.
    \vspace{-0.5em}
    \item \textbf{Modeling Inter-GPU Communication Cost:}
    Inter-GPU communication costs are modeled based on token routing frequency between expert pairs. If two interacting experts are placed on separate GPUs, the communication cost is proportional to the volume of tokens transferred between them. This model drives the optimization process, penalizing placements that increase inter-GPU data exchanges and encouraging configurations that minimize cross-GPU communication.
    \vspace{-0.5em}
    \item \textbf{Optimizing expert placement:}
    The expert placement is formulated as two ILP stages where each aims to: (a) balance token processing loads across GPUs and (b) minimize the maximum inter-GPU communication cost. Each ILP incorporates constraints for GPU memory and processing capacity, as well as routing dependencies captured from the token statistics. By solving the optimization problems, we identify an expert placement strategy that maximizes GPU utilization, minimizes idle time, and reduces interconnect communication overhead across all MoE layers.
    \vspace{-0.5em}
\end{itemize}

Through this formulation, \expertune~delivers an efficient and scalable solution for expert placement. By addressing token processing imbalance and inter-GPU communication, it enhances overall system performance, reducing latency and improving throughput in large-scale MoE deployments.


\subsection{\expertune~Overview}~\label{subsec:designoverview}
\vspace{-3ex}
% \seokjin{TODO: think about how we want to present this part.}

The framework for expert placement in MoE models consists of three stages: Token Routing Profiling, ILP Optimization, and Custom Expert Parallelism Initialization. 
%
These stages are systematically integrated to optimize expert placement, minimizing token processing imbalance and inter-GPU communication costs. 
%
The overall process is summarized in Figure ~\ref{fig:methodology-overview}.
% Brifely go over each stages according to Figure 8.
In (1) Token Routing Profiling, we analyze token routing patterns by profiling a sampled subset of the dataset, leveraging their consistency across batches to estimate routing dependencies and expert loads. 
%
Next, in the (2) ILP Optimization stage, we solve the placement problem in two steps: first, we cluster experts to balance token loads across GPUs; then, we determine the optimal mapping of these clusters to GPUs to minimize communication latency induced by token dispatching.
%
Finally, in the (3) Custom Expert Parallelism Initialization stage, the optimized expert placement is applied to the MoE model, replacing default configurations to improve inference performance by balancing GPU utilization and minimizing communication latency.


\begin{figure}
    \centering
    \includesvg[width=1.0\linewidth]{figures/routing-invariance.svg}
    \vspace{-1em}
    \caption{Token routing statistics to different experts during the inference of Mixtral-8$\times$7B on the WikiText dataset. The results show that token routing statistics remain consistent across different batches within the same task.}
    \label{fig:routing-invariance}
    \vspace{-2ex}
\end{figure}

\niparagraph{Token Routing Profile}
%
In this stage, we execute inference over a sampled subset of the task dataset to gather token routing statistics. For each token, we track its routing path across layers in the MoE model. The statistics capture how tokens are routed from one expert to another between neighboring layers.
%
To better understand the nature of these routing patterns, we analyze their consistency over time. Figure~\ref{fig:routing-invariance} indicates a high degree of invariance in token routing, as illustrated in the accompanying plot. 
%
This invariance suggests that token routing decisions remain stable across iterations, suggesting that we can use a small subset of the task dataset to optimize the expert placement instead of the entire dataset.
%
The gathered routing statistics and their consistent patterns will be utilized in the next stage to formulate the expert placement optimization problem, aiming to minimize communication overhead and balance computational loads effectively.



\niparagraph{Leveraging ILP optimization for Expert Placement}
%
The expert placement in MoE models involves balancing token processing loads and minimizing inter-GPU communication overhead, both of which are highly constrained and interdependent. 
%
ILP is well-suited because it enables precise modeling of these constraints while optimizing for multiple objectives simultaneously. 
%
Unlike heuristic methods such as graph partitioning, ILP guarantees globally optimal solutions under the given constraints, ensuring the best expert placement strategies. 
%
Furthermore, ILP allows for the integration of routing statistics, GPU capacity limitations, and communication costs into a unified optimization problem, making it an ideal choice for solving this complex placement problem.


For expert placement, \expertune~uses the token routing history to construct a routing history table. 
%
This table captures the number of tokens routed between each pair of neighboring layers in the MoE model. 
%
The ILP model is formulated with the objective of minimizing load imbalance and inter-GPU communication costs. 
%
The ILP solver takes routing data, interconnect bandwidth, and resource constraints as inputs and outputs the optimal placement of experts across GPUs. 
%
After solving the ILP, the optimal expert-to-GPU mapping is saved into a PyTorch tensor file for future use during custom expert parallelism initialization.



\niparagraph{Custom Expert Parallelism}
%
Using the optimized expert placement derived from the ILP solvers, we initialize the MoE model with the ILP-optimized custom expert parallelism strategy.
%
The expert-to-GPU mapping file is first loaded from local storage. 
%
For each layer of the model, the corresponding expert-to-GPU assignments are extracted from the mapping. 
%
These assignments are then applied to the model to replace the default placement with the optimized configuration.
%
By ensuring that experts are assigned to GPUs based on the solverâ€™s results, this initialization minimizes communication latency and balances the workload across GPUs.


%
% The optimization workflow is summarized in Algorithm~\ref{alg:ilp_optimization}.

% \begin{algorithm}[H]
% \caption{Two-Stage Expert Placement Optimization}
% \label{alg:ilp_optimization}
% \begin{algorithmic}[1]
%     \STATE \textbf{Input:} Token routing statistics $T$, number of GPUs $G$, interconnect bandwidth $B$, constraints.
%     \STATE \textbf{Output:} Expert-to-GPU mapping file $M$.
%     \STATE \textbf{ILP 1: Load-balancing Expert Clustering}
%     \STATE Formulate and solve an ILP to assign experts to clusters.
%     \STATE Extract expert-to-cluster mapping $X$.
    
%     \STATE \textbf{ILP 2: Communication-aware Cluster Placement}
%     \STATE Formulate and solve an ILP to assign clusters to GPUs.
%     \STATE Extract cluster-to-GPU mapping $Y$.
    
%     \STATE \textbf{Combine Mappings}
%     \STATE Compute expert-to-GPU mapping $M$ based on $X$ and $Y$.
%     \STATE Save $M$ to local directory.
% \end{algorithmic}
% \end{algorithm}



\if 0
\begin{algorithm}[H]
\caption{Custom Expert Parallelism Initialization}
\label{alg:expert_parallelism_init}
\begin{algorithmic}[1]
    \STATE \textbf{Input:} Expert mapping file $M$, model configuration $C$.
    \STATE \textbf{Output:} Initialized MoE model.
    \STATE Load mapping $M$ from file.
    \FOR{each layer $L$ in $C$}
        \STATE Extract expert-to-GPU mapping for $L$ from $M$.
        \STATE Assign experts to GPUs.
    \ENDFOR
    \STATE Initialize model checkpoint with custom mapping.
    \RETURN Initialized MoE model.
\end{algorithmic}
\end{algorithm}

\fi

% Seokjin: OLD MANUSCRIPT
% In this stage, we run inference over a sampled subset of the target task dataset to gather token routing statistics. For each token, we track its routing path across layers in the MoE model. 
% The statistics capture how tokens are routed from one expert to another between neighboring layers. 
% This information will be used in the next stage to formulate the expert placement optimization problem. 
% To collect the token routing decisions during profiling, we modify the forward pass implementation of the Megatron-core transformer models. 
% Specifically, we modify files including \texttt{gpt\_model.py}, \texttt{transformer\_block.py}, \texttt{transformer\_layer.py}, and \texttt{moe\_layer.py} to enable token routing tracking. 
% This profiling functionality is enabled only during this profiling stage and is disabled in later stages to ensure it does not impact performance.
% Listing ~\ref{listing:profiling} illustrates the modification made to the forward function in the MoE layer to capture token routing statistics:

% % Code snippet
% \begin{lstlisting}[language=Python, caption={Code snippet for MoE layer forward pass modification to profile token routing.}, label={listing:profiling}, float=t]
% # MoE Layer forward
% def forward():
%   ...
%   def custom_forward():
%     probs, indices = self.router(hidden_states)
%     ...
%     # Collect token routing statistics
%     if self.is_profiling:
%         self.collect_routing(indices)
%     return (output, mlp_bias)
% \end{lstlisting}

% In this modification, the method \texttt{collect\_routing} is called when the profiling mode is enabled, and it tracks the routing decisions (i.e., the \texttt{indices}) made by the router for each token in the MoE layer. 
% These statistics are then stored and later used for the expert placement optimization problem. 
% The profiling functionality is disabled during subsequent stages to ensure that the performance of the model is not affected by the added tracking.

% \subsubsection{ILP Optimization}
% Once the token routing history is collected, we use it to construct a routing history table. 
% This table captures the number of tokens routed between each pair of neighboring layers in the MoE model. 
% The ILP model is formulated with the objective of minimizing load imbalance and inter-GPU communication costs. 
% The ILP model incorporates routing statistics and pipeline parallel size (i.e., PP\_SIZE) constraints, as optimization occurs within each pipeline parallel domain. 
% The goal is to place experts optimally across GPUs while considering factors such as available inter-GPU bandwidth and token processing load. 
% Listing ~\ref{listing:ilp_optimization} demonstrates how the routing history is used to construct the ILP model, followed by solving the model using Gurobi optimizer.


% \begin{lstlisting}[language=Python, caption={Code snippet for ILP optimization using token routing statistics and Gurobi solver.}, label={listing:ilp_optimization}]
% # Build routing history table
% def build_routing_table(routing_history):
%   table = {}
%   for (src_expert, dst_expert), tokens in routing_history.items():
%     if (src_expert, dst_expert) not in table:
%       table[(src_expert, dst_expert)] = 0
%     table[(src_expert, dst_expert)] += tokens
%   return table

% # ILP formulation
% def solveILP(routing_history, PP_SIZE, ...):
%   solutions = {}
%   # Solve the ILP for each PP domain
%   for pp_id in range(PP_SIZE):
%     model = grb.Model(f"Optimize_PP{pp_id}")
        
%     # Build table for the current PP domain
%     routing_table = build_routing_table(routing_history[pp_id])
        
%     # Variables, obj function, and constraints
%     ...
    
%     # Optimize the model
%     model.optimize()

%   # Return optimized expert placement
%   if model.status == grb.GRB.OPTIMAL:
%     solutions[pp_id] = model.getAttr('X', experts)
%   else:
%     raise Exception("No optimal solution found.")
  
%   return solutions

% # Example routing history and parameters
% routing_history = torch.load("routing_history.pt")
% PP_SIZE = 2
% ...

% # Run ILP optimization
% solution = solveILP(routing_history, PP_SIZE, ...)

% # Save the solution
% torch.save(solution, "optimized_placement.pt")

% \end{lstlisting}



% \subsubsection{Expert Parallelism Initialization}
% With the optimal expert placement obtained from the ILP solver, we initialize the expert parallelism configuration. 
% This step ensures that the experts are placed across GPUs in a way that minimizes communication latency and balances the load. 
% Finally, the inference process is run with the optimized expert placement, leading to improved performance by reducing idle times and communication bottlenecks.

% % Code snippet
% \begin{lstlisting}[language=Python, caption={Code snippet for expert parallelism initialization using custom placement.}, label={lst:expert_parallelism_init}]
% class MoELayer(MegatronModule, ABC):
%   def __init__(self, config: TransformerConfig, layer_num: int = None):
%     super().__init__(config)
%     ...

%     # Read the expert mapping tensor
%     def custom_mapping(experts, EP, path, layer):
%       mapping = torch.load(path)[:, :, layer].argmax(dim=0)
%       return [[e for e, g in enumerate(mapping) if g == ep_rank] for ep_rank in range(EP)]

%     # Custom mapping implementation
%     if os.getenv('CUSTOM_MAPPING') == '1':
%       path = "optimized_placement.pt"
%       mapping = custom_mapping(num_experts, EP, path, layer_num)
%       self.local_ids = mapping[rank]
%       self.local_count = len(self.local_ids)
    
%     # Baseline implementation
%     else:
%       offset = ep_rank * self.local_count
%       self.local_ids = [offset + i for i in range(self.local_count)]
% \end{lstlisting}

\subsection{ILP Formulation}~\label{sec:ilp}
%
%This section details \expertune's novel ILP optimization for expert placement in MoE models across multiple GPUs and layers. 
%
% The ILP is structured in two stages: ILP 1 for clustering experts to balance token processing loads across GPUs, and ILP 2 for assigning clusters to GPUs while minimizing inter-cluster token routing costs.
%
The \expertune~optimization comprises two ILPs: ILP 1 for clustering experts to balance token processing loads across GPUs, and ILP 2 for assigning clusters to GPUs while minimizing inter-GPU token routing costs.
%
%To solve the formulated ILP models, we use the Gurobi solver~\cite{gurobi}. 
%
By solving the ILPs, we obtain an expert placement that optimizes GPU and interconnect utilization, enhancing the efficiency of MoE processing by reducing processing load imbalance and communication latency.

\subsubsection{ILP 1: Clustering Experts within Layers}

\niparagraph{Inputs.}
The input to the ILP formulation includes the token routing statistics, which provide information on how many tokens are routed between experts within each MoE layer. 
%
\begin{itemize}[leftmargin=*]
    \vspace{-0.5em}
    \item \( P_{e,l} \): The number of tokens routed to expert \( e \) in layer \( l \). It indicates the workload associated with each expert and helps calculate the token processing load for each expert cluster during ILP 1.
    \vspace{-0.5em}
    \item \(E\): Number of experts per layer.
    \vspace{-0.5em}
    \item \(L\): Total number of MoE layers.
    \vspace{-0.5em}
    \item \(G\): Total number of GPUs.
\end{itemize}
%
\niparagraph{Variables.}
The variables of this ILP are:
\vspace{-0.5em}
\begin{align*}
    x_{c,e,l} \in \{0, 1\} \quad \quad \quad &\text{for} \quad c \in \{0, \dots, G-1\}, \\
    &\text{for} \quad e \in \{0, \dots, E-1\}, \\
    &\text{for} \quad \hspace{0.12em} l \in \{0, \dots, L-1\}
\end{align*}
\begin{itemize}
\vspace{-1.5em}
\item \( x_{c,e,l} \): Binary decision variable indicating whether expert \( e \) is assigned to cluster \( c \) in layer \( l \).
\end{itemize}


\niparagraph{Objective Function.}
%
In the first ILP, the objective is to cluster experts within each layer that balance the token processing load across all clusters, where each cluster will be mapped to a GPU using ILP 2. 
%
The goal is to distribute the token processing workload as evenly as possible across expert clusters in each layer. 
%
This is accomplished by minimizing the absolute deviation between the load of each cluster and the average load per layer, ensuring efficient resource utilization. The objective function \(O_1\)can be described as:
%
\vspace{-0.5ex}
\begin{equation}
    O_1 = \sum_{c=0}^{G-1} \sum_{l=0}^{L-1} \left| T_{c,l} - \bar{T}_l \right|
\end{equation}
\vspace{-0.5ex}
%
where $T_{c,l}$ is the total token processing load for expert cluster $c$ in layer $l$, and $\bar{T}_l$ is the average token processing load across all clusters in layer $l$.
%
The token processing load $T_{c,l}$ for each expert cluster in layer $l$ is the sum of token routing statistics for each expert assigned to that cluster:
\vspace{-0.5ex}
%
\begin{equation}
    T_{c,l} = \sum_{e=0}^{E-1} \sum_{t=0}^{T-1} P_{e,l} \cdot x_{c,e,l}
\end{equation}
%
\vspace{-0.5ex}
where $P_{e,l}$ is the profiled number of tokens routed to expert $e$ at layer $l$ and $x_{c,e,l}$ is a binary decision variable (1 if expert $e$ is assigned to cluster c , 0 otherwise).
%
Lastly, the average token processing load across all clusters in a layer is computed as:

\vspace{-0.5ex}
\begin{equation}
    \bar{T}_l = \frac{1}{G} \sum_{e=0}^{E-1} P_{e,l}
\end{equation}
%
\vspace{-0.5ex}

This objective minimizes the deviation of the token processing load across expert clusters for each layer. 
%
By minimizing this deviation, we ensure that no cluster is overloaded while others are underutilized.

\niparagraph{Solving the ILP.}
The ILP is executed for every possible expert cluster and for every MoE layer of the model.
The optimization goal is to minimize the deviation in token processing load across expert clusters. The ILP formulation is:
%
\begin{align}
\text{min} \quad & O_1 \\
\text{s.t.} \quad & O_1 = \sum_{c=0}^{G-1} \sum_{l=0}^{L-1} \left| T_{c,l} - \bar{T}_l \right| \\
\quad & T_{c,l} = \sum_{e=0}^{E-1} P_{e,l} \cdot x_{c,e,l} && (\forall c, l) \\
% \quad & \sum_{e=0}^{E-1} \sum_{l=0}^{L-1} x_{c,e,l} = \frac{E}{G} && (\forall c) \\
& \sum_{e=0}^{E-1} x_{c,e,l} \geq 1 && (\forall c, l)
\end{align}

\niparagraph{Constraints.}
% Equation 6 enforces that the total number of experts assigned per cluster \( c \) across all layers is equal to \( \frac{E}{G} \). This guarantees that the number of experts per cluster is proportionate to the number of available GPUs. 
% Removed above line since it doesn't exist anymore; Eq 7 is the only constraint in ILP1
The ILP constraints in Equation 7 that at least one expert is assigned to each cluster \( c \) in each layer \( l \), preventing null cluster assignments and ensuring that every layer has sufficient resources.

\subsubsection{ILP 2: Cluster Placement on GPUs}

\niparagraph{Inputs.}
The input to the ILP formulation includes the precomputed communication cost between clusters, which provides information on how many tokens are routed between experts within neighboring MoE layers.
%
\begin{itemize}[leftmargin=*]
    \vspace{-0.5em}
    \item  \( x_{c,e,l} \) : The binary decision variable indicating whether expert  \( e \)  is assigned to cluster  \( c \)  in layer  \( l \) . This value is determined in ILP 1 and is used to compute the communication cost between clusters for ILP 2.
    \vspace{-0.5em}
    \item \( C_{c_1,c_2,l} \): Number of tokens routed between cluster \( c_1 \) in layer \( l \) and cluster \( c_2 \) in layer \( l+1 \). This represents the number of tokens routed clusters of neighboring layers and is used for balancing the inter-GPU communication load.
    \vspace{-0.5em}
    \item \( R_{e_1, e_2, l} \) : The number of tokens routed between experts \( e_1 \)  and \( e_2 \) in layer \( l \). This value is used to precompute the communication cost between clusters.
    \vspace{-0.5em}
    \item \( B_{g_1,g_2} \): The available bandwidth between GPUs \( g_1 \) and \( g_2 \). This parameter is used to model the bandwidth-aware communication cost by normalizing \( C \) with the available bandwidth, reflecting the relative cost of inter-GPU communication.
    \vspace{-0.5em}
    \item \(E\): Number of experts per layer.
    \vspace{-0.5em}
    \item \(L\): Total number of MoE layers.
    \vspace{-0.5em}
    \item \(G\): Total number of GPUs.
\end{itemize}

\niparagraph{Variables.}
The variables for ILP 2 are defined as follows:
\vspace{-0.5em}
\begin{align*}
    y_{c,g,l} \in \{0, 1\} \quad \quad \quad &\text{for} \quad c \in \{0, \dots, G-1\}, \\
    &\text{for} \quad g \in \{0, \dots, G-1\}, \\
    &\text{for} \quad \hspace{0.12em} l \in \{0, \dots, L-1\}
\end{align*}
\begin{itemize}
\vspace{-0.5em}
\item \( y_{c,g,l} \in \{0, 1\} \) : Binary decision variable indicating whether cluster \( c \) is assigned to GPU \( g \) in layer \( l \) .
\end{itemize}


\niparagraph{Objective Function.}
%
We aim to minimize the communication overhead between GPUs when routing tokens between experts. 
%
This is achieved by strategically placing expert clusters on GPUs to reduce inter-GPU communication, with a particular focus on minimizing the all-to-all tail latency in each layer.
%
The objective function \( O_2 \) directly targets the tail latency per layer by minimizing the maximum communication cost across all GPU pairs, which impacts the latency of token dispatching. The objective function is given by:
%
\vspace{-1ex}
\begin{equation}
    \small
    O_2 = \sum_{l=0}^{L-1} \max \left( \sum_{c_1, c_2 = 0}^{G-1} \sum_{g_1, g_2 = 0}^{G-1} \frac{C_{c_1, c_2, l}}{B_{g_1, g_2}} \cdot y_{c_1, g_1, l} \cdot y_{c_2, g_2, l+1} \right)
\end{equation}
where $C_{c_1, c_2, l}$ is the communication cost between expert clusters  $c_1$ and $c_2$ for layer $l$ and $l+1$, and $B_{g_1, g_2}$ is the bandwidth between GPUs $g_1$ and $g_2$.
%
The term $y_{c_1, g_1}$ is a binary decision variable that indicates whether cluster $c_1$ is assigned to GPU $g_1$.
%
The communication cost $C_{c_1, c_2, l}$ is calculated using the expert assignments from ILP 1, where the $x_{c,e,l}$ values (the binary decision variables indicating whether expert $e$  is assigned to cluster $c$ in layer $l$) have already been determined. 
%
Using these values, we calculate the total communication cost between expert clusters $c_1$ and $c_2$ for each layer $l$ by summing over all pairs of experts $e_1$ and $e_2$. Specifically, the total communication cost $C_{c_1, c_2, l}$ is computed as:
%
\vspace{-1ex}
\begin{equation}
    C_{c_1, c_2, l} = \sum_{e_1=0}^{E-1} \sum_{e_2=0}^{E-1} R_{e_1, e_2, l} \cdot x_{c_1,e_1,l} \cdot x_{c_2,e_2,l}
\end{equation}
%
where $R_{e_1, e_2, l}$ is the number of tokens routed between these experts.
%
Note that we pre-compute these communication costs \( C_{c_1, c_2, l} \) after ILP 1, thereby avoid having to compute them repeatedly during the ILP 2 optimization process. 
%

\niparagraph{Solving the ILP.}
%
The ILP is solved for every possible cluster-GPU mapping across every MoE layer of the model:
%
\begin{small}
\begin{align}
    \text{min} \quad \quad \quad & O_2 \\
    \text{s.t.} \quad \quad \quad & \notag \\
    O_2 = &\sum_{l=0}^{L-1} \max \left( \sum_{c_1, c_2 = 0}^{G-1} \sum_{g_1, g_2 = 0}^{G-1} \frac{C_{c_1, c_2, l}}{B_{g_1, g_2}} \cdot y_{c_1, g_1, l} \cdot y_{c_2, g_2, l+1} \right) \\
    C_{c_1, c_2, l} = &\sum_{e_1=0}^{E-1} \sum_{e_2=0}^{E-1} R_{e_1, e_2, l} \cdot x_{c_1,e_1,l} \cdot x_{c_2,e_2,l} \quad (\forall c_1, c_2, l) \\
    \quad \sum_{l=0}^{L-1} \sum_{c=0}^{G-1} \sum_{e=0}^{E-1} &x_{c,e,l} \cdot y_{c,g,l} = \frac{E\cdot L}{G} \quad \hspace{6em} (\forall g) \\
    \sum_{g=0}^{G-1} y_{c,g,l} = &1 \quad \hspace{12em} (\forall c, l) \\
    \sum_{c=0}^{G-1} y_{c,g,l} = &1 \quad \hspace{12em} (\forall g, l)
\end{align}
\end{small}


\niparagraph{Constraints.}
%
The formulation of ILP 2 includes several constraints to ensure a valid and balanced solution. 
These constraints collectively guarantee the one-on-one assignment of clusters to GPUs while respecting the available GPU capacity.
%
Equation 13 ensures that the total number of experts assigned to each GPU \( g \) across all layers is equal to \( \frac{E \cdot L}{G} \), ensuring a balanced assignment of experts across GPUs. This guarantees a balanced distribution of experts, ensuring that the memory footprint is evenly distributed across GPUs.
%
Equation 14 enforces that each cluster \( c \) is assigned to exactly one GPU \( g \) in each MoE layer \( l \).
%
Equation 15 ensures that each GPU \( g \) is assigned to exactly one cluster \( c \) for each layer \( l \), maintaining a balanced distribution of clusters across the GPUs.


\niparagraph{\expertune~Optimization} \expertune~offers a robust solution for optimizing expert placement in large-scale MoE models.
%
By leveraging routing dependencies and systematically balancing token processing loads, while minimizing inter-GPU communication costs, \expertune~achieves significant improvements in both GPU utilization and overall system throughput.
%
Next, we analyze the performance of \expertune~across diverse configurations and datasets, demonstrating its capability to address key challenges in MoE inference effectively.

% \subsection{Solving Expert Placement with ILP}
% %
% In the previous section, we presented the Integer Linear Programming (ILP) formulation for solving the expert placement problem, highlighting the problem structure and design considerations behind the two-stage approach. 
% %
% In this section, we outline the presented ILP model in detail by defining the specific variables, constants, and constraints that are used in our formulation.

% \noindent \textbf{Input.}
% %
% The input is a 

% \noindent \textbf{Variables.}
% \begin{itemize}
%     \vspace{-0.5em}
%     \item \( x_{c,e,l} \in \{0, 1\} \): Binary variable indicating whether expert \( e \) is assigned to cluster \( c \) in layer \( l \).
%     \vspace{-0.5em}
%     \item \( y_{c,g,l} \in \{0, 1\} \): Binary variable representing the mapping between expert cluster \( c \) and GPU \( g \) in layer \( l \).
%     \vspace{-0.5em}
% \end{itemize}

% \noindent \textbf{Constants.}
% \begin{itemize}
%     \vspace{-0.5em}
%     \item \( E \): Number of experts per layer.
%     \vspace{-0.5em}
%     \item \( L \): Total number of MoE layers in the model. 
%     \vspace{-0.5em}
%     \item \( G \): Total number of GPUs being used.
%     \vspace{-0.5em}
%     \item \( P_{e, l} \): Integer constant representing the number of tokens processed by expert \( e \) in layer \( l \), precomputed based on token routing data.
%     \vspace{-0.5em}
%     \item \( B_{g,g'} \): Integer constant representing the bandwidth between GPUs \( g \) and \( g' \), used to calculate the bandwidth-aware communication cost.
%     \vspace{-0.5em}
% \end{itemize}

% \subsubsection{Constraints}

% \noindent\textbf{Equal Expert Distribution Across Layers.}  
% Each cluster hosts an equal number of experts across layers:

% \begin{equation}
%     \sum_{e=0}^{E-1} \sum_{l=0}^{L-1} x_{c,e,l} = \frac{E}{G} \quad \forall c
% \end{equation}

% \noindent\textbf{Minimum One Expert per Layer per Cluster.}  
% Each cluster has at least one expert assigned in every layer:

% \begin{equation}
%     \sum_{e=0}^{E-1} x_{c,e,l} \geq 1 \quad \forall c, l
% \end{equation}

% \noindent\textbf{Only one Cluster per GPU.}  
% Each cluster is assigned to only one GPU:
% \begin{equation}
%     \sum_{g=0}^{G-1} y_{c,g,l} = 1 \quad \forall c, l
% \end{equation}

% \noindent\textbf{Only one GPU per Cluster.}  
% Each GPU can host to only one cluster:
% \begin{equation}
%     \sum_{c=0}^{G-1} y_{c,g,l} = 1 \quad \forall g, l
% \end{equation}

% \subsubsection{Solving the ILP with Gurobi}
% %
% To solve the formulated ILP model, we used the Gurobi solver, known for its efficiency and optimization capabilities in handling large-scale ILP problems. 
% %
% The ILP was set to run until reaching tolerance of 0.025, meaning the solver iteratively refines the solution by adjusting the values of decision variables to minimize the objective function.
% %
% By solving this ILP, we obtain an expert placement that optimizes GPU and interconnect utilization, enhancing the efficiency of MoE processing by reducing processing load imbalance and communication latency.

% \subsection{Modeling Per-GPU Token Processing Load}

% To minimize load imbalance, we define the token processing load for each GPU \( g \) in layer \( l \) by the maximum processing load across GPUs for that layer, denoted as \( w_l \). Given that \( x_{g, e, l} \) is a binary variable indicating whether expert \( e \) in layer \( l \) is assigned to GPU \( g \), we express \( w_l \) as follows:

% \begin{equation}
%     w_l = \max_g \sum_{e=1}^{E} x_{g, e, l} \cdot t_{e, l}
% \end{equation}

% where \( t_{e, l} \) is the number of tokens processed by expert \( e \) in layer \( l \). To capture the cumulative imbalance across all layers, we define the total processing load imbalance \( I \) as:

% \begin{equation}
%     I = \sum_{l=1}^{L} w_l
% \end{equation}

% This formulation aligns with the ILP objective to minimize \( I \), representing the cumulative processing load imbalance across GPUs in all layers.

% \subsection{Modeling Inter-GPU Communication Cost}

% Since all-to-all communication is synchronous, the latency for each layer is determined by the maximum communication load between any pair of GPUs, adjusted for available bandwidth. Our objective is to minimize the sum of these maximum communication latencies across all layers.

% The communication load \( y_{g,g',l} \) between a pair of GPUs \( g \) and \( g' \) in layer \( l \) depends on the number of tokens routed between experts on these GPUs in consecutive layers. Let \( T_{e_1, e_2, l, l+1} \) denote the number of tokens routed from expert \( e_1 \) in layer \( l \) to expert \( e_2 \) in layer \( l+1 \). Then:

% \begin{equation}
%     y_{g,g',l} = \sum_{e_1=1}^{E} \sum_{e_2=1}^{E} T_{e_1, e_2, l, l+1} \cdot x_{g, e_1, l} \cdot x_{g', e_2, l+1}
% \end{equation}

% To account for bandwidth limitations, we define \( z_l \) as the maximum normalized communication load across GPU pairs in layer \( l \):

% \begin{equation}
%     z_l = \max_{g \neq g'} \frac{y_{g,g',l}}{B_{g,g'}}
% \end{equation}

% where \( B_{g,g'} \) represents the bandwidth between GPUs \( g \) and \( g' \). The total inter-GPU communication cost \( C_{\text{total}} \) across all layers is then:

% \begin{equation}
%     C_{\text{total}} = \sum_{l=1}^{L-1} z_l
% \end{equation}

% Minimizing \( C_{\text{total}} \) reduces synchronization latency across layers, thus optimizing the communication bottlenecks based on token load and bandwidth constraints.

% \subsection{Solving Expert Placement with ILP}

% To determine the optimal expert placement, we formulate an Integer Linear Programming (ILP) model with the objective of minimizing both the load imbalance \( I \) and the communication cost \( C_{\text{total}} \). Our objective function is:

% \begin{equation}
%     \min \lambda \sum_{l=1}^{L-1} z_l + \sum_{l=1}^{L} w_l
% \end{equation}

% where:
% \begin{itemize}
%     \item \( z_l \): the maximum communication latency in the transition from layer \( l \) to \( l+1 \),
%     \item \( w_l \): the maximum token processing load across GPUs in layer \( l \),
%     \item \( \lambda \): a weighting factor to balance routing latency and processing load.
% \end{itemize}

% \subsubsection{Variables and Constants}

% In our Integer Linear Programming (ILP) model, we define the following variables and constants:

% \noindent \textbf{Variables.}
% \begin{itemize}
%     \vspace{-0.5em}
%     \item \( x_{g,e,l} \in \{0, 1\} \): Binary variable indicating whether expert \( e \) is assigned to GPU \( g \) in layer \( l \).
%     \vspace{-0.5em}
%     \item \( y_{g,g',l} \): Integer variable representing the routing load between GPU \( g \) and GPU \( g' \) during the transition from layer \( l \) to \( l+1 \).
%     \vspace{-0.5em}
%     \item \( z_l \): Continuous variable representing the maximum communication latency during the transition between layers \( l \) and \( l+1 \).
%     \vspace{-0.5em}
%     \item \( w_l \): Continuous variable denoting the maximum token processing load across all GPUs in layer \( l \).
%     \vspace{-0.5em}
%     \item \( \mathcal{E}_g^{(l)} \): Set of experts assigned to GPU \( g \) in layer \( l \), determined as a result of the expert placement strategy  \(x_{g,e,l}\) .
% \end{itemize}

% \noindent \textbf{Constants.}
% \begin{itemize}
%     \vspace{-0.5em}
%     \item \( T_{e_1, e_2, l, l+1} \): Integer constant representing the number of tokens routed from expert \( e_1 \) in layer \( l \) to expert \( e_2 \) in layer \( l+1 \), which is predetermined based on token routing data.
%     \vspace{-0.5em}
%     \item \( B_{g,g'} \): Integer constant representing the bandwidth between GPUs \( g \) and \( g' \), used to calculate the communication latency \( z_l \).
%     \vspace{-0.5em}
%     \item \( t_e^{(l)} \): Integer constant representing the number of tokens processed by expert \( e \) in layer \( l \), which is determined based on the data and the expert's processing capacity.
% \end{itemize}

% \subsubsection{Constraints}

% \noindent\textbf{Equal Expert Distribution Across Layers.}  
% Each GPU hosts an equal number of experts across layers:

% \begin{equation}
%     \sum_{e=1}^{E} \sum_{l=1}^{L} x_{g,e,l} = \frac{E}{G}
% \end{equation}

% \noindent\textbf{Minimum One Expert per Layer per GPU.}  
% Each GPU has at least one expert assigned in every layer:

% \begin{equation}
%     \sum_{e=1}^{E} x_{g,e,l} \geq 1 \quad \forall g, l
% \end{equation}

% \noindent\textbf{Routing Load Constraints.}  
% The routing load \( y_{g,g',l} \) between GPU \( g \) and \( g' \) for consecutive layers \( l \) and \( l+1 \) is:

% \begin{equation}
%     y_{g,g',l} = \sum_{e_1=1}^{E} \sum_{e_2=1}^{E} T_{e_1, e_2, l, l+1} \cdot x_{g, e_1, l} \cdot x_{g', e_2, l+1}
% \end{equation}

% \noindent\textbf{Maximum Communication Latency per Layer Transition.}  
% For each layer transition \( l \), \( z_l \) captures the maximum latency:

% \begin{equation}
%     z_l \geq \frac{y_{g,g',l}}{B_{g,g'}} \quad \forall g \neq g', l
% \end{equation}

% \noindent\textbf{Maximum Token Processing Load per Layer.}  
% To ensure load balancing, \( w_l \) represents the maximum processing load for layer \( l \):

% \begin{equation}
%     w_l \geq \sum_{e \in \mathcal{E}_g^{(l)}} t_{e}^{(l)} \quad \forall g, l
% \end{equation}

% where \( \mathcal{E}_g^{(l)} \) is the set of experts assigned to GPU \( g \) in layer \( l \), and \( t_{e}^{(l)} \) is the number of tokens processed by expert \( e \) in layer \( l \).

% \subsubsection{Solving the ILP with Gurobi}
% To solve the formulated ILP model, we used the Gurobi solver, known for its efficiency and optimization capabilities in handling large-scale ILP problems. 
% The ILP was set to run until convergence, meaning the solver iteratively refines the solution by adjusting the values of decision variables to minimize the objective function.
% During each iteration, Gurobi evaluates potential placements and routing configurations to achieve an optimal balance between load imbalance \( I \) and total communication cost \( C_{\text{total}} \). 
% The solver stops once it converges to an optimal or near-optimal solution, which minimizes the objective function to the extent possible under the given constraints.
% By solving this ILP, we obtain an expert placement that optimizes GPU and interconnect utilization, enhancing the efficiency of MoE processing by reducing processing load imbalance and communication latency.