\section{Introduction}
The rapid growth of transformer models~\cite{bert,gpt2,gpt3,gemma,llama,t5} has revolutionized deep learning, driving advancements in natural language processing, computer vision, and other sequence-based tasks.
% 
By leveraging self-attention mechanisms, transformers excel at capturing long-range dependencies and processing large-scale datasets efficiently.
% 
However, as these models scale, their parameter count grows exponentially, significantly increasing computational and memory demands.
% 
For instance, models like LLaMA and GPT, with billions of parameters, push the limits of current hardware, making large-scale training and deployment prohibitively expensive~\cite{llama, gpt3}.

% Introduce the concept of MoE
As such, Mixture-of-Experts (MoE) architecture has emerged to efficiently scale large models~\cite{mixtral, switchtransformer, gshard, cai2024survey}. 
%
MoE models activate only a subset of experts within each layer, significantly reducing the computational burden compared to dense models~\cite{shazeer2017outrageously}.
%
By routing tokens through only a subset of experts at each layer, MoE models reduce computational costs compared to dense models of equivalent capacity.
%
This sparse activation allow for larger models to be trained without a proportional increase in compute requirements. 
%
% Limitations of MoE models
Despite these benefits, MoE models face several challenges as they continue to scale. 
%
While sparse routing helps reduce compute intensity, the parameters still scale and so does the memory capacity requirement~\cite{mcsmoe,deepspeed, switchtransformer, fastermoe, fastmoe, cai2024shortcut,hwang2024pre}.
%
Additionally, the effectiveness of individual experts becomes increasingly unbalanced, with some experts being activated far more frequently than others. 
%
% This skewed activation pattern results in imbalanced hardware utilization and reduced parameter efficiency of MoE models.
%
This skewed activation pattern results in imbalanced hardware utilization of MoE models.

\begin{figure}
    \centering
    \includesvg[width=0.85\linewidth]{figures/intro-tokenrouting.svg}
    \vspace{-1em}
    \caption{Token routing statistics for Mixtral-8x7B. Each colored circle represents an expert in a layer, and the lines connecting them illustrate the number of tokens routed between pairs of experts. Thicker lines indicate a higher volume of routed tokens, highlighting key routing dependencies.
    % \seokjintodo{Fill out this figure with the token routing dependency plot; it could demonstrate both the token processing imbalance and communication skew issues.}
    }
    \label{fig:intro-tokenrouting}
\end{figure}


Expert parallelism distributes experts across multiple GPUs to scale the experts and size of the models~\cite{deepspeed-moe, exflow,  li2023accelerating, fastermoe, fastmoe,cai2024shortcut, singh2023hybrid}.
%
In a typical MoE model, each expert is responsible for processing a subset of tokens. 
%
As the number of experts grows, it becomes impractical to fit all of them onto a single GPU due to memory constraints. 
%
Expert parallelism solves this issue by partitioning the experts evenly across GPUs, ensuring that each GPU processes a smaller subset of experts~\cite{deepspeed-moe, li2023accelerating}.
%
In conventional expert parallelism implemented in popular ML frameworks like Deepspeed and Megatron-LM~\cite{deepspeed,megatron}, each GPU is assigned a contiguous range of experts.
%
For instance, when expert parallelism is deployed across 4 GPUs for an MoE model with 8 experts per layer, as shown in Figure~\ref{fig:intro-tokenrouting}, GPU 0 handles experts 0 and 1, GPU 1 handles experts 2 and 3, and so on.
%
In this setup, tokens are dispatched to remote GPUs via all-to-all communication if the destination expert is not present locally. 
%
This distribution aims to alleviate the memory burden on individual GPUs, enabling the model to scale efficiently as the number of experts increases.

Expert parallelism introduces bottlenecks. 
%
Firstly, communication during token routing, especially all-to-all, can dominate execution time, especially when token routing is imbalanced across GPU pairs. 
%
For example, for the Mixtral-8x7B~\cite{mixtral} model, we observe that all-to-all communication takes up 35.7\% of the end-to-end inference time.
%
Secondly, in existing work~\cite{exflow,li2023accelerating, fastermoe, singh2023hybrid} even though every GPU has equal number of experts, the expert activation is still skewed and activates certain experts more than the others. 
%
Uneven activation of experts across GPUs exacerbates token processing tail latency, where GPUs responsible for processing a larger number of tokens cause other GPUs to stall until all computations are finished. 
%
In summary, while expert parallelism mitigates memory constraints to facilitate scalability, it adds the communication overhead and faces token balance issues.

% Bring up prior works and their limitations
To address inefficiencies in expert parallelism, prior works explore techniques such as overlapping compute and communication to mitigate communication costs~\cite{deepspeed-moe, megatron, li2023accelerating, fastermoe,cai2024shortcut,singh2023hybrid,jiang2024lancet}.
%
Other approaches focus on minimizing communication volume through optimized token dispatch mechanisms or locality-aware expert placement strategies, achieving notable gains in throughput and efficiency~\cite{exflow}. 
%
While these methods achieve notable improvements in throughput and efficiency by reducing the overhead of inter-GPU communication, they often fall short in addressing two critical challenges: load imbalance in token processing and the communication tail latency caused by uneven token distribution across GPU pairs. 
%
These works still distribute the experts evenly across GPUs purely based on parameter size and not based on their token activation. 
%
These systemic inefficiencies remain significant bottlenecks, preventing optimal scalability and performance in large-scale MoE systems.

% Move into how we solve this problem
To address these challenges, we leverage the insight that activation dependencies exist between experts across layers, creating an affinity for activating specific experts based on those activated in the previous layer.
% 
Moreover, certain experts are activated significantly more frequently than others, adding an imbalance to the system.
%
A straightforward approach is to distribute experts based on token routing; however, the challenge with this approach is that distributing with expert parallelism still needs to mitigate the memory bottleneck, thus parameter size is still a factor that determines how experts are distributed.
%
In this work, instead of distributing experts evenly based solely on parameter size, we account for size, token routing patterns, and expert activation chains.
%
To solve this optimization problem, we propose an Integer Linear Programming (ILP) formulation that models the communication and computation costs of token routing and expert activation.
%
By leveraging cross-layer dependencies, our approach derives an optimal expert placement strategy that minimizes inter-GPU communication while balancing token processing workloads and expert size across devices.
% 
This strategy effectively reduces all-to-all communication overhead and mitigates compute-induced tail latency by ensuring even distribution of token dispatching and expert activations, while minimizing the total number of tokens dispatched.
% 
Through ILP-based optimization, we resolve communication skew and token processing load imbalances, ultimately enhancing the scalability and performance of MoE models.

% Summary of our contributions
In summary, we make the following contributions:
\begin{itemize}
    % \item We identify the key bottlenecks in the state-of-the art techniques of expert parallelism, which are limited to distributing experts solely based on reducing the total communication volume that leads to imbalanced token routing and uneven expert activation across devices.
    \item We identify key bottlenecks in existing expert parallelism techniques, which focus primarily on reducing total communication volume but often result in imbalanced token routing and uneven expert activation across devices.
    \vspace{-0.5em}
    % \item We discover opportunities to exploit token routing dependencies to determine an optimal expert placement strategy that effectively resolves these bottlenecks.
    \item We exploit token routing dependencies to design an optimal expert placement strategy that mitigates these bottlenecks, improving load balancing and communication efficiency across GPUs.
    \vspace{-0.5em}
    \item We introduce \expertune~, a novel optimization approach for expert parallelism in MoE models leveraging ILP to minimize both communication and compute-induced tail latency.
    \vspace{-0.5em}
\end{itemize}

% We demonstrate the effectiveness of \expertune~in mitigating the performance degradation caused by these bottlenecks, achieving 9.3\% and 17.5\% of average speedups in single-node and multi-node distributed MoE inference tasks using the Mixtral-8x7B model.
We demonstrate the effectiveness of \expertune~using the Mixtral-8x7B model, achieving 9.3\% and 17.5\% of average speedups in single-node (8 H100 GPUs) and multi-node (16 H200 GPUs) distributed MoE inference tasks, respectively.
%
These improvements are driven by two key factors: (1) load-balanced token processing to minimize GPU idle times and lower token processing latency, and (2) reduced variation in remote token dispatching, ensuring efficient all-to-all communication.
%
Together, these optimizations address inefficiencies in expert computation and token dispatching, enabling consistent performance improvements across distributed settings.

% \todoseokjin{Add results.}



% Seokjin: below is the original Q&A-like version of intro.
% % Q. What is the problem?
% The increasing adoption of Mixture-of-Experts (MoE) models has introduced a new set of performance challenges in distributed training and inference, where token dispatching to remote GPUs often constitutes a significant bottleneck. 
% In large-scale distributed systems, tail latency caused by all-to-all communication critically impacts overall throughput, especially under imbalanced token dispatch patterns. 
% These patterns stem from sparse routing of MoE models, where irregular token distribution can lead to unpredictable delays and inefficiencies.
% \seokjin{TODO: add a smoother transition}

% % Q. Why is this a problem?
% Expert parallelism is crucial for scaling MoE model training and inference as the number of experts continues to increase, yet managing latency and token imbalance remains challenging in these environments. 
% Without addressing these bottlenecks, the performance and cost efficiency of MoE models, which are increasingly deployed in industrial settings, remain suboptimal.

% % Q. Why did the prior work fail?
% Previous efforts have attempted to mitigate these performance challenges by optimizing expert placement across GPUs, leveraging inter-layer token routing statistics to reduce overall communication volume. 
% However, these approaches primarily focus on minimizing token routing costs without fully addressing load imbalances, which can become pronounced during fine-tuning or inference on out-of-distribution datasets. 
% In such scenarios, skewed token distributions may lead to severe tail latencies on specific GPUs or interconnects, as certain GPUs bear disproportionately high token processing loads. 
% We observe that balancing token routing alone is insufficient to achieve optimal MoE performance; both balanced token processing loads and routing are essential to overcoming these bottlenecks and ensuring consistent system performance.

% % Q. How are we different from prior work?
% To address these limitations, we propose a novel Integer Linear Programming (ILP)-based optimization framework that jointly minimizes inter-GPU token routing costs while balancing token processing loads across GPUs. 
% In contrast to prior work, our approach explicitly tackles both computational and communication imbalances that arise in MoE inference. 
% % Q. What are the benefits that we get?
% By strategically placing experts based on token routing history and load distribution, our framework reduces idle times by equalizing token processing load and minimizing tail latency of inter-GPU communication. Additionally, our load-balancing scheme maximizes GPU and interconnect utilization across MoE layers, enhancing overall system efficiency and resource utilization across large-scale distributed setups. 
% These improvements lead to significant performance gains in MoE model inference, making it more scalable and efficient in real-world applications. 