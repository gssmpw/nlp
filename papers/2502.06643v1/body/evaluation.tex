\section{Evaluation}
\subsection{Experimental Setup}~\label{subsec:setup}
\vspace{-2ex}

\niparagraph{MoE Model and Datasets.} We evaluate \expertune~on pre-trained Mixtral8x7B~\cite{mixtral} available on the Huggingface Hub~\cite{huggingface}, benchmarking its performance on a representative selection of language modeling datasets, as shown in Table~\ref{tab:evaluation-datasets}. 
%
As discussed in Section~\ref{subsec:designoverview}, the routing patterns observed with the subset closely match those of the full datasets, providing reliable insights into \expertune's performance. 

\begin{table}
    \centering
    \caption{Evaluation Datasets}
    \vspace{-0.7em}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Dataset} & \textbf{Abbreviation} & \textbf{Type} \\
        \hline
        WikiText-103~\cite{wikitext} & wiki & Language Modeling \\
        \hline
        MiniPile~\cite{minipile} & pile & Language Modeling \\
        \hline
        LAMBADA~\cite{lambada} & lamb & Language Modeling \\
        \hline
        enwik8~\cite{enwik8} & enwi & Language Modeling \\
        \hline
        % OpenWebText & open & Language Modeling \\
        % \hline
        % C4 & c4 & Language Modeling \\
        % \hline
    \end{tabular}
    }
    \label{tab:evaluation-datasets}
\end{table}

\niparagraph{Expert and Tensor Parallel Configurations.}
%
Since Mixtral features eight experts per layer, we limit the size of expert parallelism (EP) to four. 
%
Our methodology remains broadly applicable as the number of experts is expected to scale further in the future to accommodate greater knowledge capacity. To scale beyond four GPUs, we employ a hybrid parallelism strategy combining tensor parallelism (TP) and expert parallelism.
%
For single-node (8 GPUs) and multi-node (2 nodes and 16 GPUs total) experiments, we configured parallelism as 4EP-2TP and 4EP-4TP, respectively.

\niparagraph{Software and Libraries and Setup.}
%
To implement \expertune, we modify the all-to-all communication and expert placement modules in Megatron-LM~\cite{megatron} to allow custom expert mappings across GPUs, supporting variable numbers of experts per layer. Our ILP was optimized using Gurobi~\cite{gurobi} (version 12.0.0). Both ILPs were set to execute until reaching a tolerance of 0.025, meaning the solver iteratively refines the solution by adjusting the values of decision variables to minimize the objective function. The evaluation was conducted with PyTorch 2.5.1~\cite{pytorch}, CUDA Toolkit 12.4~\cite{cuda,nccl}, and RHEL 9 OS~\cite{rhel9}.

\begin{table}
    \centering
    \caption{H100 Server Node Specifications}
    \vspace{-0.7em}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Component} & \textbf{Specification} \\
        \hline
        GPU & 8x NVIDIA H100 SXM5 80GB \\
        \hline
        Interconnect & NVLink Gen4 (900GB/s) \\
        \hline
        CPU & Dual Xeon Platinum 8462Y+ \\
        \hline
        System Memory & 2048 GB DDR5 4800 MHz \\
        \hline
        NIC & NVIDIA ConnectX-7 IB (400Gbps) \\
        \hline
    \end{tabular}}
    \label{tab:hardware-specs}
\end{table}


\niparagraph{Server Architecture.}
%
Experiments were conducted on a high-performance computing node with specifications detailed in Table~\ref{tab:hardware-specs} and Table~\ref{tab:hardware-specs1}. Our evaluations were performed in both single-node (i.e., 1 node with 8 GPUs) and multi-node (i.e., 2 nodes with 8 GPUs each) configurations to assess the impact of hierarchical interconnect topologies, especially in terms of how communication patterns change across nodes. Due to resource availability, single-node experiments were conducted using NVIDIA H100 GPUs~\cite{h100} while multi-node experiments were performed on NVIDIA H200 GPUs~\cite{h200}.

\begin{table}
    \centering
    \caption{H200 Server Node Specifications}
    \vspace{-0.7em}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Component} & \textbf{Specification} \\
        \hline
        GPU & 8x NVIDIA H200 SXM5 142GB \\
        \hline
        Interconnect & NVLink Gen4 (900GB/s) \\
        \hline
        CPU & Dual Xeon Platinum 8562Y \\
        \hline
        System Memory & 2048 GB DDR5 5600 MHz \\
        \hline
        NIC & NVIDIA ConnectX-7 IB (800Gbps) \\
        \hline
    \end{tabular}}
    \label{tab:hardware-specs1}
\end{table}
\vspace{-1em}

\subsection{Baseline and Metrics}

\niparagraph{Baseline and Expert Assignment.}  
We use Megatron-LM~\cite{megatron} as the baseline, which employs a naive expert placement strategy where experts are assigned to GPUs in contiguous blocks (for example, with 8 experts and 4 GPUs, experts 0 and 1 are assigned to GPU 0, experts 2 and 3 to GPU 1, and so on). 
%
This approach ensures an even distribution of experts for memory footprint but does not address load balancing or communication inefficiencies during runtime. 
%
% Previous work~\cite{exflow} use ILP to optimize expert assignment for minimizing inter-GPU communication, but without considering load balancing or communication skew.

\niparagraph{Evaluation Metrics.} 
%
We evaluate \expertune and the baseline using the following metrics: \textit{End-to-End Speedup:} Speedup in absolute time to complete one batch of inference. \textit{Token Processing Time:} Tail latency and average time taken for experts to finish processing tokens at each layer. \textit{All-to-All Time:} Tail latency and average time taken to complete all-to-all communication across GPUs.

\niparagraph{End-to-End Speedup Measurement.}
End-to-end speedup is measured by running 100 inference steps and averaging the results across datasets. To ensure stable measurements, 100 warmup steps are performed prior to recording the timing.

\niparagraph{Tail- and Average- Latency Measurement.}
%
For all latency measurements, we employed PyTorch Profiler~\cite{torchprof}, averaging results over 10 inference steps with an initial 100 warm-up steps on the WikiText-103 dataset.
%
To measure tail latency, we identify the GPU with the longest execution time at each layer for each iteration, then calculate the average of these maximum values across all iterations. 
%
For average latency, we compute the mean execution time across all GPUs for each iteration and then average these means across iterations.

\subsection{Results and Insights}
% \seokjin{TODO: add that compared to prior work where they were kinda effective only for multi-node cases as they were reducing the total communication only, since we are targeting tail latency for both computation and communication, our work provides benefits both single (compute time is relatively more dominant) and multi-node (communication time is more dominant) cases.}


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/speedup.pdf}
    \vspace{-1.5em}
    \caption{End-to-end inference performance, normalized to Megatron-LM's expert parallelism approach.}
    \label{fig:speedup}
\end{figure}




\subsubsection{End-to-End Speedup}
%
Figure~\ref{fig:speedup} compares the end-to-end speedup of \expertune~against the baseline Megatron-LM in both single-node and multi-node settings.
%
\expertune~achieves a 9.3\% speedup in the single-node setup and a 17.5\% speedup in the multi-node setup.
%
These performance improvements are driven by reductions in communication overhead and efficient load balancing of token processing across GPUs.
%
Figure~\ref{fig:mapping} illustrates the token routing statistics of Mixtral-8x7B on the WikiText-103 dataset, with the mapping generated by \expertune. 
%
\expertune~demonstrates significant improvements in token load balancing, as evidenced by the darker expert colors indicating high token load on GPUs with only one expert assigned per layer, while lighter expert colors indicate low token load on GPUs with a larger number of experts.
%
Moreover, \expertune~also effectively manages remote token dispatching, as observed in layers 4 and 5, where experts 7 and 6 are mapped to the same GPU.
%
In the single-node setup, the speedup can primarily be attributed to better resource utilization, with load balancing minimizing GPU idle times.
%
Due to the balanced token processing across GPUs, \expertune~enhances GPU memory utilization by 8.7\% in the single-node setup.

\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{figures/eval_mapping.svg}
    \vspace{-1em}
    \caption{Token routing statistics for Mixtral-8x7B, with custom mapping generated by \expertune. The circle represents the index of expert parallel rank assignment.}
    \label{fig:mapping}
\end{figure}


\begin{figure}
    \centering
    % First subfigure (a)
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includesvg[width=1\linewidth]{figures/tail-latency-computation_1node.svg}  % Update file name if needed
        \vspace{-1.8em}
        \caption{Single node: Average and tail latency of token processing time across GPUs in each layer.}
        \vspace{0.5em}
        \label{fig:tail-latency-computation-1node}
    \end{subfigure}
    % Second subfigure (b)
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includesvg[width=1\linewidth]{figures/tail-latency-computation_2nodes.svg}  % Update file name if needed
        \vspace{-1.8em}
        \caption{Multi node: Average and tail latency of token processing time across GPUs in each layer.}
        \label{fig:tail-latency-computation-2nodes}
    \end{subfigure}
    \vspace{-2em}
    \caption{Comparison of average and tail latency of token processing time across GPUs in each layer for single-node and multi-node setups.}
    \label{fig:compute-tail-latency-comparison}
\end{figure}



In the multi-node setup, reduced inter-node communication overhead leads to a more significant speedup by minimizing communication volume, which is especially costly in multi-node configurations.
%
This is due to the much lower inter-node network bandwidth, 9$\times$ lower than intra-node NVLink bandwidth.
%
The lower inter-node bandwidth results in higher all-to-all communication latencies, but \expertune~effectively reduces inter-node communication volume, leading to a more substantial speedup compared to the single-node scenario.
%
As a result, our dual approach-targeting both computation and communication tail latency-enables consistent performance gains in both single-node and multi-node scenarios.
%
As such this shows the benefits of \expertune~and its scalability to multi-node setting.
%
In the next part of this section, we break down the reasons for this performance benefit through token processing and communication time reduction.



\begin{figure}[h!]
    \centering
    % First subfigure (a)
    \begin{subfigure}[b]{0.485\linewidth}
        \centering
        \includesvg[width=1\linewidth]{figures/processing-load-vanilla.svg}
        \caption{Token processing load distribution (Megatron-LM).}
        \label{fig:processing-load-baseline}
    \end{subfigure}
    % Second subfigure (b)
    \begin{subfigure}[b]{0.485\linewidth}
        \centering
        \includesvg[width=1\linewidth]{figures/processing-load-custom.svg}
        \caption{Token processing load distribution (\expertune).}
        \label{fig:processing-load-expertune}
    \end{subfigure}
    \caption{Distribution of tokens processed by a single GPU per layer. Each box plot summarizes the variation in token processing load across GPUs for a single layer. \expertune~significantly reduces both the variation and peak load, demonstrating improved load balancing across GPUs.}
    \label{fig:processing-load}
\end{figure}


\subsubsection{Token Processing Time}
%
As shown in Figure~\ref{fig:compute-tail-latency-comparison}, \expertune~reduces the tail latency in token processing 36\% in the single-node setup and by 27\% in the multi-node setup.
%
Similarly, the average token processing time is reduced by 34.8\% in the single-node configuration and 22.5\% in the multi-node configuration.
%
These improvements are achieved through improved token dispatching and load balancing across GPUs, which minimizes stragglers during token processing.
%
The benefits are particularly pronounced in layers with high computational loads, such as layer 31, where \expertune~significantly reduces both average and tail processing times. 
%
This is likely due to its capacity to mitigate imbalances in token dispatching, as evidenced by the token routing distributions in Figure~\ref{fig:processing-load}. 
%
By preventing GPUs from being overloaded or underutilized, \expertune~ensures more consistent and efficient computation, even in the most demanding layers.

\begin{figure}
    \centering
    % First subfigure (a)
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includesvg[width=1\linewidth]{figures/tail-latency-communication_1node.svg}  % Update file name if needed
        \vspace{-1.8em}
        \caption{Single node: Average and tail latency of all-to-all time across GPUs in each neighboring layers.}
        \vspace{0.5em}
        \label{fig:tail-latency-communication-1node}
    \end{subfigure}
    % Second subfigure (b)
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includesvg[width=1\linewidth]{figures/tail-latency-communication_2nodes.svg}  % Update file name if needed
        \vspace{-1.8em}
        \caption{Multi node: Average and tail latency of token processing time across GPUs in each neighboring layers.}
        \label{fig:tail-latency-communication-2nodes}
    \end{subfigure}
    \vspace{-2em}
    \caption{Comparison of average and tail latency of all-to-all communication in each layer for single-node and multi-node setups. \expertune~provides substantial reduction in tail latency and average latency.}
    \label{fig:communication-tail-latency-comparison}
\end{figure}

In the multi-node setup, while token processing time is reduced, the improvements are less pronounced compared to the single-node configuration. 
%
This is primarily due to the additional inter-node communication overhead during token processing, specifically the costs of all-gather operations introduced by tensor parallelism, as demonstrated in Figure~\ref{fig:time-distribution}.
%
As discussed in Section~\ref{subsec:setup}, our experimental design keeps the expert parallel size fixed while scaling the tensor parallel size, which increases the all-gather communication volume required to synchronize intermediate activations.
%
Although some layers, such as those in the 12â€“22 range, exhibit fluctuations in latency, this variability is likely due to the short profiling iterations.
%
Despite these anomalies, \expertune~demonstrates consistent benefits across setups, effectively addressing inefficiencies in token processing to deliver predictable and efficient computation.


\begin{figure}
    \centering
    % First subfigure (a)
    \begin{subfigure}[b]{0.485\linewidth}
        \centering
        \includesvg[width=1\linewidth]{figures/comm-volume-vanilla.svg}
        \caption{Token dispatching distribution (Megatron-LM).}
        \label{fig:comm-vanilla}
    \end{subfigure}
    % Second subfigure (b)
    \begin{subfigure}[b]{0.485\linewidth}
        \centering
        \includesvg[width=1\linewidth]{figures/comm-volume-custom.svg}
        \caption{Token dispatching distribution (\expertune).}
        \label{fig:comm-custom}
    \end{subfigure}
    \caption{Distribution of total token dispatching between individual GPU pairs across neighboring layers, measured for a single-node configuration. Each data point in a box plot represents the total number of tokens dispatched between a specific GPU pair (e.g., GPU0-GPU1) across all iterations.
    }
    \vspace{-1em}
    \label{fig:comm-volume}
\end{figure}



\subsubsection{All-to-All Time}
%
As illustrated in Figure~\ref{fig:communication-tail-latency-comparison}, our proposed optimization strategy significantly reduces the tail latency of all-to-all communication, which is critical for improving end-to-end inference time.
%
In the single-node setup, \expertune~reduces the tail latency by 36.3\% and the average latency by 35.4\% compared to the baseline.
%
This reduction is consistent across most layers, with notable improvements in layers 13-17, where both average and tail latencies are particularly low.
%
In the multi-node setup, \expertune~achieves a 30.50\% reduction in tail latency and a 24.7\% reduction in average latency.
%
The reduced impact in this setup is primarily due to the higher inter-node communication overhead, which dominates the latency in multi-node environments.
%
For example, in certain layers (e.g., layers 15 and 30), the improvement in tail latency is less pronounced.
%
This is attributed to high inter-node token dispatching in these layers, which introduces additional communication overhead.
%
The tail latency spikes observed in such layers are consistent with the token dispatching distributions shown in Figure~\ref{fig:comm-volume}, where layers 15 and 30 exhibit relatively higher maximum communication volumes compared to neighboring layers.
%
Despite these spikes, \expertune~still provides measurable benefits over the baseline, maintaining lower latency overall.
%
By reducing the variation in token dispatching and avoiding severe imbalances, our approach ensures more efficient and predictable all-to-all communication.

% In the single-node setup, tail latency is reduced by 38.21\%, while in the multi-node setup, it is reduced by 32.35\%. 
%
% This reduction is achieved through optimized expert placement, ensuring fewer tokens are routed across nodes and preventing bottlenecks caused by unbalanced token dispatching. 
%
% In terms of average all-to-all communication time, there is a reduction of 35.38\% in the single-node setup and 24.18\% in the multi-node setup. 
% %
% While the reduction is less pronounced in the multi-node configuration, this is primarily because our strategy focuses on minimizing the impact of tail latency, rather than aiming to solely minimize the total communication volume. 
% %
% Figure~\ref{fig:comm-volume-latency} demonstrates that \expertune~leads to substantial reductions in both the average and maximum number of remote token dispatches, which directly contribute to the improvements in both the average and tail latency of all-to-all communication discussed above.





% \subsubsection{Tail Latency}
% Our method focuses on minimizing tail latency for both computation and communication.  
% Computation tail latency is reduced by in single and multi-node setups by 35.94\% and 26.89\%, as shown in Figure~\ref{fig:compute-tail-latency-comparison}, due to improved token dispatching and load balancing across GPUs, minimizing stragglers during token processing.  
% On the other hand, communication tail latency is reduced by 38.21\% and 32.35\% (Figure~\ref{fig:communication-tail-latency-comparison}) through optimized expert placement, ensuring that fewer tokens are routed across nodes and avoiding bottlenecks caused by unbalanced token dispatching.  
% By addressing tail latency comprehensively, our work achieves a more consistent and predictable performance, particularly in scenarios with varying workloads.


% \subsubsection{Performance Scaling}
% Our method demonstrates robust performance scaling as the GPU count increases.  
% In single-node setups, the improved load balancing ensures efficient utilization of all GPUs, even as more GPUs are added (Figure~\ref{fig:scaling-efficiency}).  
% In multi-node setups, the reduction in inter-node communication overhead becomes increasingly significant as the number of GPUs grows.  
% For example, adding 8 additional GPUs to a multi-node cluster only increases overall latency by \textcolor{red}{X.XX}, compared to \textcolor{red}{X.XX} in the baseline configuration.  
% This near-linear scaling sets our method apart from prior approaches that struggle with imbalanced workloads in large systems.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/scaling-efficiency.pdf}
%     \caption{Performance scaling of ExPerTune. (Mixtral-8x7B)}
%     \label{fig:scaling-efficiency}
% \end{figure}


% \subsubsection{GPU Utilization.}
% %
% ExPerTune enhances GPU memory utilization by 8.65\% in the single-node setup and \textcolor{red}{X.XX\%} in the multi-node setup.
% %
% Although GPU compute utilization decreases by 3.02\% and \textcolor{red}{X.XX\%} in the single-node and multi-node setup, this reduction is attributed to an increased proportion of inter-GPU communication overhead introduced by tensor parallelism.
% %
% These results highlight the effectiveness of our load-balancing strategy, which optimally distributes tasks across GPUs to minimize idle times and ensure efficient use of both compute and memory resources.

% \begin{table}[h!]
%     \centering
%     \caption{Hardware Utilization}
%     \vspace{0.3em}
%     \begin{tabular}{|l|c|c|c|}
%         \hline
%         \textbf{Metric} & \textbf{Baseline} & \textbf{ExPerTune} \\
%         \hline
%         GPU Utilization (\%) & X\% & Y\% \\
%         \hline
%         Memory Utilization (\%) & X\% & Y\%\\
%         \hline
%         Power Consumption (W) & X\% & Y\% \\
%         \hline
%     \end{tabular}
%     \label{tab:utilization-efficiency}
% \end{table}
