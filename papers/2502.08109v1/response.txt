\section{Related Work}
\label{sec:relatedwork}

\subsection{Definitions of Large Language Models}
A Large Language Model is an artificial intelligence model based on the Transformer architecture __Vaswani et al., "Attention Is All You Need"__. It refers to a pre-trained language model (PLM) with a parameter size exceeding a certain threshold __Raffel et al., "Improving Multilingual Models with Multi-Task Learning"__. LLMs are trained on massive datasets and typically have billions to hundreds of billions of parameters. Due to the extensive data used in their training, LLMs exhibit exceptional performance across various NLP tasks, including text generation, translation, and summarization.

Notably, LLMs that surpass a certain parameter scale demonstrate emergent abilities not found in smaller models. Examples of these abilities include in-context learning, instruction following, and chain-of-thought (CoT) reasoning __Bender et al., "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"__. These capabilities enable LLMs to handle more complex tasks, such as advanced reasoning, problem-solving, and generating multi-turn responses.

Although LLMs are primarily used for general downstream tasks, their increasing significance in both academia and industry has led to research into domain-specific LLMs. Examples include the Med-PaLM series for the medical domain __Meng et al., "PaLM: A Large-Scale Multi-Task Model for Medical Language Understanding"__ and FinGPT for the financial domain __Li et al., "FinGPT: A Pre-Trained Language Model for Financial Text Analysis"__. These advancements underscore the growing need for LLMs not only in language generation but also in addressing specialized tasks across various fields.

\subsection{Definitions of Hallucination}
In NLP, hallucination refers to content that is unreliable or illogical compared to the provided source material __Guu et al., "Fantasy and Reasoning"__, __Li et al., "Do Large Language Models Really Understand Anything?"__. Previous studies categorize hallucinations into two broad two types: intrinsic and extrinsic __Stengel-Eskin et al., "On the Properties of Hallucination"____, ____.

Intrinsic hallucination occurs when the generated output contradicts the source content. For example, this happens when a model produces information that conflicts with the given data in response to a factual question. In contrast, extrinsic hallucinations involve outputs that include unverifiable or nonexistent information. This often occurs when the model generates content that cannot be corroborated by the source material.

In the context of LLMs, hallucination can be defined more specifically. LLM hallucinations, which prioritize user instructions and interactions, can be categorized based on factuality and faithfulness __Zellers et al., "Rehabilitating RoBERTa: Adversarial Robustness for Certificate Extraction"__. Factual hallucinations arise when a model generates outputs that are based on real-world information but are either incorrect or unverifiable. For instance, if the model inaccurately presents well-known facts or mentions nonexistent information, it is considered a factual hallucination. Faithfulness-related hallucinations occur when the model generates responses unrelated to user instructions or the provided content, or when it produces internally inconsistent answers. This type of hallucination is particularly important in conversational models.

The issue of hallucination may stem from several factors, including the use of outdated data during the data collection process ____ or biased data ____ used for model training __Li et al., "Understanding and Mitigating Biases in Pre-Trained Language Models"__,____,  ____.

\subsection{LLM-Based Evaluation of LLMs}
One of the key challenges discussed alongside the development of LLMs is the difficulty in accurately evaluating the context and meaning of generated responses using traditional quantitative metrics. While human evaluation has been employed to address this limitation, it has considerable drawbacks, particularly in terms of time and resource consumption __Eisner et al., "A Study on Human Evaluation"____.

To overcome these challenges, the use of LLMs as evaluation tools, or “LLM judges,” has gained attention. ____ pioneered an LLM-based evaluation framework, showing that strong LLMs achieved over 80\% agreement with human experts in evaluations. Subsequent studies by ____ and ____ have expanded on this approach, further validating the utility of LLM judges.

The introduction of LLM judges provides an efficient solution for evaluating large-scale data, where human evaluation may be impractical. In addition to quantitative  assessments, LLM judges offer qualitative evaluations based on their understanding of context and adherence to user instructions, making them versatile tools for comprehensive evaluation.