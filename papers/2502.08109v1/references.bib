@misc{Bubeck2023,
   author = {Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
   title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
   url = {https://arxiv.org/abs/2303.12712},
   year = {2023},
}

@inproceedings{Chen2023,
   author = {Shiqi Chen and Yiran Zhao and Jinghan Zhang and I-Chun Chern and Siyang Gao and Pengfei Liu and Junxian He},
   booktitle = {Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
   title = {FELM: Benchmarking Factuality Evaluation of Large Language Models},
   url = {http://arxiv.org/abs/2310.00741},
   year = {2023},
}

@inproceedings{Lin2022,
   abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
   author = {Stephanie Lin and Jacob Hilton and Owain Evans},
   city = {Dublin, Ireland},
   doi = {10.18653/v1/2022.acl-long.229},
   editor = {Smaranda Muresan and Preslav Nakov and Aline Villavicencio},
   booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   month = {5},
   pages = {3214-3252},
   publisher = {Association for Computational Linguistics},
   title = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
   url = {https://aclanthology.org/2022.acl-long.229},
   year = {2022},
}

@inproceedings{Fabbri2022,
   abstract = {Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost.},
   author = {Alexander Fabbri and Chien-Sheng Wu and Wenhao Liu and Caiming Xiong},
   city = {Seattle, United States},
   doi = {10.18653/v1/2022.naacl-main.187},
   editor = {Marine Carpuat and Marie-Catherine de Marneffe and Ivan Vladimir Meza Ruiz},
   booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
   month = {7},
   pages = {2587-2601},
   publisher = {Association for Computational Linguistics},
   title = {QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization},
   url = {https://aclanthology.org/2022.naacl-main.187},
   year = {2022},
}

@inproceedings{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and \L\{\}ukasz Kaiser and Illia Polosukhin},
   city = {Red Hook, NY, USA},
   isbn = {9781510860964},
   booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
   pages = {6000-6010},
   publisher = {Curran Associates Inc.},
   title = {Attention is all you need},
   year = {2017},
}

@misc{Zhao2023,
   author = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
   title = {A Survey of Large Language Models},
   url = {https://arxiv.org/abs/2303.18223},
   year = {2023},
}

@misc{Wei2022,
   author = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
   title = {Emergent Abilities of Large Language Models},
   url = {https://arxiv.org/abs/2206.07682},
   year = {2022},
}

@article{Singhal2023,
   abstract = {Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
   author = {Karan Singhal and Shekoofeh Azizi and Tao Tu and S. Sara Mahdavi and Jason Wei and Hyung Won Chung and Nathan Scales and Ajay Tanwani and Heather Cole-Lewis and Stephen Pfohl and Perry Payne and Martin Seneviratne and Paul Gamble and Chris Kelly and Abubakr Babiker and Nathanael Schärli and Aakanksha Chowdhery and Philip Mansfield and Dina Demner-Fushman and Blaise Agüera y Arcas and Dale Webster and Greg S. Corrado and Yossi Matias and Katherine Chou and Juraj Gottweis and Nenad Tomasev and Yun Liu and Alvin Rajkomar and Joelle Barral and Christopher Semturs and Alan Karthikesalingam and Vivek Natarajan},
   doi = {10.1038/s41586-023-06291-2},
   issn = {14764687},
   issue = {7972},
   journal = {Nature},
   month = {8},
   pages = {172-180},
   pmid = {37438534},
   publisher = {Nature Research},
   title = {Large language models encode clinical knowledge},
   volume = {620},
   year = {2023},
}

@article{Yang2023,
   author = {Hongyang Yang and Xiao-Yang Liu and Christina Dan Wang},
   journal = {FinLLM Symposium at IJCAI 2023},
   title = {FinGPT: Open-Source Financial Large Language Models},
   year = {2023},
}

@article{Ji2023,
   abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
   author = {Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Ye Jin Bang and Andrea Madotto and Pascale Fung},
   city = {New York, NY, USA},
   doi = {10.1145/3571730},
   issn = {0360-0300},
   issue = {12},
   journal = {ACM Comput. Surv.},
   keywords = {Hallucination,consistency in NLG,extrinsic hallucination,factuality in NLG,faithfulness in NLG,intrinsic hallucination},
   month = {3},
   publisher = {Association for Computing Machinery},
   title = {Survey of Hallucination in Natural Language Generation},
   volume = {55},
   url = {https://doi.org/10.1145/3571730},
   year = {2023},
}

@inproceedings{Maynez2020,
   abstract = {It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.},
   author = {Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan McDonald},
   city = {Online},
   doi = {10.18653/v1/2020.acl-main.173},
   editor = {Dan Jurafsky and Joyce Chai and Natalie Schluter and Joel Tetreault},
   booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
   month = {7},
   pages = {1906-1919},
   publisher = {Association for Computational Linguistics},
   title = {On Faithfulness and Factuality in Abstractive Summarization},
   url = {https://aclanthology.org/2020.acl-main.173},
   year = {2020},
}

@inproceedings{Dziri2021,
   abstract = {Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.},
   author = {Nouha Dziri and Andrea Madotto and Osmar Zaiane and Avishek Joey Bose},
   city = {Online and Punta Cana, Dominican Republic},
   doi = {10.18653/v1/2021.emnlp-main.168},
   editor = {Marie-Francine Moens and Xuanjing Huang and Lucia Specia and Scott Wen-tau Yih},
   booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
   month = {11},
   pages = {2197-2214},
   publisher = {Association for Computational Linguistics},
   title = {Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding},
   url = {https://aclanthology.org/2021.emnlp-main.168},
   year = {2021},
}

@misc{Huang2023a,
   author = {Yichong Huang and Xiachong Feng and Xiaocheng Feng and Bing Qin},
   title = {The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey},
   url = {https://arxiv.org/abs/2104.14839},
   year = {2023},
}

@misc{Huang2023b,
   author = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
   title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
   url = {https://arxiv.org/abs/2311.05232},
   year = {2023},
}

@inproceedings{Kasai2024,
   abstract = {We introduce REALTIME QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). REALTIME QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open-domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that REALTIME QA will spur progress in instantaneous applications of question answering and beyond. https://realtimeqa.github.io/.},
   author = {Jungo Kasai and Keisuke Sakaguchi and Yoichi Takahashi and Ronan Le Bras and Akari Asai and Xinyan Velocity Yu and Dragomir Radev and Noah A Smith and Yejin Choi and Kentaro Inui},
   city = {Red Hook, NY, USA},
   booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
   publisher = {Curran Associates Inc.},
   title = {REALTIME QA: what's the answer right now?},
   year = {2024},
}

@inproceedings{Bender2021,
   abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
   author = {Emily M. Bender and Timnit Gebru and Angelina McMillan-Major and Shmargaret Shmitchell},
   doi = {10.1145/3442188.3445922},
   isbn = {9781450383097},
   booktitle = {FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
   month = {3},
   pages = {610-623},
   publisher = {Association for Computing Machinery, Inc},
   title = {On the dangers of stochastic parrots: Can language models be too big?},
   year = {2021},
}

@misc{chiang2022overcomingtheoreticallimitationselfattention,
      author={David Chiang and Peter Cholak},
      title={Overcoming a Theoretical Limitation of Self-Attention}, 
      year={2022},
      eprint={2202.12172},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.12172}, 
}

@misc{Li2023a,
   author = {Zuchao Li and Shitou Zhang and Hai Zhao and Yifei Yang and Dongjie Yang},
   title = {BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer},
   url = {https://arxiv.org/abs/2307.00360},
   year = {2023},
}

@inproceedings{Bang2023,
   author = {Yejin Bang and Samuel Cahyawijaya and Nayeon Lee and Wenliang Dai and Dan Su and Bryan Wilie and Holy Lovenia and Ziwei Ji and Tiezheng Yu and Willy Chung and Quyet V Do and Yan Xu and Pascale Fung},
   city = {Nusa Dua, Bali},
   doi = {10.18653/v1/2023.ijcnlp-main.45},
   editor = {Jong C Park and Yuki Arase and Baotian Hu and Wei Lu and Derry Wijaya and Ayu Purwarianti and Adila Alfa Krisnadhi},
   booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
   month = {11},
   pages = {675-718},
   publisher = {Association for Computational Linguistics},
   title = {A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity},
   url = {https://aclanthology.org/2023.ijcnlp-main.45},
   year = {2023},
}

@inproceedings{Zheng2024,
   abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.},
   author = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P Xing and Hao Zhang and Joseph E Gonzalez and Ion Stoica},
   city = {Red Hook, NY, USA},
   booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
   publisher = {Curran Associates Inc.},
   title = {Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
   year = {2024},
}

@misc{Bai2023,
   author = {Yushi Bai and Jiahao Ying and Yixin Cao and Xin Lv and Yuze He and Xiaozhi Wang and Jifan Yu and Kaisheng Zeng and Yijia Xiao and Haozhe Lyu and Jiayin Zhang and Juanzi Li and Lei Hou},
   title = {Benchmarking Foundation Models with Language-Model-as-an-Examiner},
   url = {https://arxiv.org/abs/2306.04181},
   year = {2023},
}

@misc{Li2023b,
   author = {Ruosen Li and Teerth Patel and Xinya Du},
   title = {PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations},
   year = {2023},
}

@inproceedings{Liu2023,
   abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.},
   author = {Yang Liu and Dan Iter and Yichong Xu and Shuohang Wang and Ruochen Xu and Chenguang Zhu},
   city = {Singapore},
   doi = {10.18653/v1/2023.emnlp-main.153},
   editor = {Houda Bouamor and Juan Pino and Kalika Bali},
   booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
   month = {12},
   pages = {2511-2522},
   publisher = {Association for Computational Linguistics},
   title = {G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment},
   url = {https://aclanthology.org/2023.emnlp-main.153},
   year = {2023},
}

@inproceedings{Li2023c,
   abstract = {Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5% user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps.},
   author = {Junyi Li and Xiaoxue Cheng and Xin Zhao and Jian-Yun Nie and Ji-Rong Wen},
   city = {Singapore},
   doi = {10.18653/v1/2023.emnlp-main.397},
   editor = {Houda Bouamor and Juan Pino and Kalika Bali},
   booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
   month = {12},
   pages = {6449-6464},
   publisher = {Association for Computational Linguistics},
   title = {HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},
   url = {https://aclanthology.org/2023.emnlp-main.397},
   year = {2023},
}

@misc{Chen2024,
   author = {Xiang Chen and Duanzheng Song and Honghao Gui and Chenxi Wang and Ningyu Zhang and Yong Jiang and Fei Huang and Chengfei Lv and Dan Zhang and Huajun Chen},
   title = {FactCHD: Benchmarking Fact-Conflicting Hallucination Detection},
   url = {https://arxiv.org/abs/2310.12086},
   year = {2024},
}

@article{Dziri2022a,
   abstract = {The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 12.8 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.},
   author = {Nouha Dziri and Ehsan Kamalloo and Sivan Milton and Osmar Zaiane and Mo Yu and Edoardo M Ponti and Siva Reddy},
   city = {Cambridge, MA},
   doi = {10.1162/tacl_a_00529},
   editor = {Brian Roark and Ani Nenkova},
   journal = {Transactions of the Association for Computational Linguistics},
   pages = {1473-1490},
   publisher = {MIT Press},
   title = {FaithDial: A Faithful Benchmark for Information-Seeking Dialogue},
   volume = {10},
   url = {https://aclanthology.org/2022.tacl-1.84},
   year = {2022},
}

@article{Dziri2022b,
   abstract = {Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (Begin), comprising 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora. We collect human annotations assessing the extent to which the models' responses can be attributed to the given background information. We then use Begin to analyze eight evaluation metrics. We find that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer. Our findings underscore the need for more sophisticated and robust evaluation metrics for knowledge-grounded dialogue. We make Begin publicly available at https://github.com/google/BEGIN-dataset.},
   author = {Nouha Dziri and Hannah Rashkin and Tal Linzen and David Reitter},
   city = {Cambridge, MA},
   doi = {10.1162/tacl_a_00506},
   editor = {Brian Roark and Ani Nenkova},
   journal = {Transactions of the Association for Computational Linguistics},
   pages = {1066-1083},
   publisher = {MIT Press},
   title = {Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark},
   volume = {10},
   url = {https://aclanthology.org/2022.tacl-1.62},
   year = {2022},
}

@misc{Team2024,
   abstract = {A detailed contributor list can be found in the appendix of this paper. Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
   author = {Llama Team and Ai @ Meta},
   title = {The Llama 3 Herd of Models},
   year = {2024},
}

@inproceedings{Hu2022,
   author = {Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
   booktitle = {International Conference on Learning Representations},
   title = {LoRA: Low-Rank Adaptation of Large Language Models},
   url = {https://openreview.net/forum?id=nZeVKeeFYf9},
   year = {2022},
}

@misc{OpenAI2024,
   author = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and C J Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
   title = {GPT-4 Technical Report},
   url = {https://arxiv.org/abs/2303.08774},
   year = {2024},
}
