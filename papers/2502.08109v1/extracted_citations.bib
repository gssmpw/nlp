@misc{Bai2023,
   author = {Yushi Bai and Jiahao Ying and Yixin Cao and Xin Lv and Yuze He and Xiaozhi Wang and Jifan Yu and Kaisheng Zeng and Yijia Xiao and Haozhe Lyu and Jiayin Zhang and Juanzi Li and Lei Hou},
   title = {Benchmarking Foundation Models with Language-Model-as-an-Examiner},
   url = {https://arxiv.org/abs/2306.04181},
   year = {2023},
}

@inproceedings{Bang2023,
   author = {Yejin Bang and Samuel Cahyawijaya and Nayeon Lee and Wenliang Dai and Dan Su and Bryan Wilie and Holy Lovenia and Ziwei Ji and Tiezheng Yu and Willy Chung and Quyet V Do and Yan Xu and Pascale Fung},
   city = {Nusa Dua, Bali},
   doi = {10.18653/v1/2023.ijcnlp-main.45},
   editor = {Jong C Park and Yuki Arase and Baotian Hu and Wei Lu and Derry Wijaya and Ayu Purwarianti and Adila Alfa Krisnadhi},
   booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
   month = {11},
   pages = {675-718},
   publisher = {Association for Computational Linguistics},
   title = {A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity},
   url = {https://aclanthology.org/2023.ijcnlp-main.45},
   year = {2023},
}

@inproceedings{Bender2021,
   abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
   author = {Emily M. Bender and Timnit Gebru and Angelina McMillan-Major and Shmargaret Shmitchell},
   doi = {10.1145/3442188.3445922},
   isbn = {9781450383097},
   booktitle = {FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
   month = {3},
   pages = {610-623},
   publisher = {Association for Computing Machinery, Inc},
   title = {On the dangers of stochastic parrots: Can language models be too big?},
   year = {2021},
}

@misc{Bubeck2023,
   author = {Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
   title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
   url = {https://arxiv.org/abs/2303.12712},
   year = {2023},
}

@inproceedings{Dziri2021,
   abstract = {Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.},
   author = {Nouha Dziri and Andrea Madotto and Osmar Zaiane and Avishek Joey Bose},
   city = {Online and Punta Cana, Dominican Republic},
   doi = {10.18653/v1/2021.emnlp-main.168},
   editor = {Marie-Francine Moens and Xuanjing Huang and Lucia Specia and Scott Wen-tau Yih},
   booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
   month = {11},
   pages = {2197-2214},
   publisher = {Association for Computational Linguistics},
   title = {Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding},
   url = {https://aclanthology.org/2021.emnlp-main.168},
   year = {2021},
}

@misc{Huang2023a,
   author = {Yichong Huang and Xiachong Feng and Xiaocheng Feng and Bing Qin},
   title = {The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey},
   url = {https://arxiv.org/abs/2104.14839},
   year = {2023},
}

@misc{Huang2023b,
   author = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
   title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
   url = {https://arxiv.org/abs/2311.05232},
   year = {2023},
}

@article{Ji2023,
   abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
   author = {Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Ye Jin Bang and Andrea Madotto and Pascale Fung},
   city = {New York, NY, USA},
   doi = {10.1145/3571730},
   issn = {0360-0300},
   issue = {12},
   journal = {ACM Comput. Surv.},
   keywords = {Hallucination,consistency in NLG,extrinsic hallucination,factuality in NLG,faithfulness in NLG,intrinsic hallucination},
   month = {3},
   publisher = {Association for Computing Machinery},
   title = {Survey of Hallucination in Natural Language Generation},
   volume = {55},
   url = {https://doi.org/10.1145/3571730},
   year = {2023},
}

@inproceedings{Kasai2024,
   abstract = {We introduce REALTIME QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). REALTIME QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open-domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that REALTIME QA will spur progress in instantaneous applications of question answering and beyond. https://realtimeqa.github.io/.},
   author = {Jungo Kasai and Keisuke Sakaguchi and Yoichi Takahashi and Ronan Le Bras and Akari Asai and Xinyan Velocity Yu and Dragomir Radev and Noah A Smith and Yejin Choi and Kentaro Inui},
   city = {Red Hook, NY, USA},
   booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
   publisher = {Curran Associates Inc.},
   title = {REALTIME QA: what's the answer right now?},
   year = {2024},
}

@misc{Li2023a,
   author = {Zuchao Li and Shitou Zhang and Hai Zhao and Yifei Yang and Dongjie Yang},
   title = {BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer},
   url = {https://arxiv.org/abs/2307.00360},
   year = {2023},
}

@misc{Li2023b,
   author = {Ruosen Li and Teerth Patel and Xinya Du},
   title = {PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations},
   year = {2023},
}

@inproceedings{Liu2023,
   abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.},
   author = {Yang Liu and Dan Iter and Yichong Xu and Shuohang Wang and Ruochen Xu and Chenguang Zhu},
   city = {Singapore},
   doi = {10.18653/v1/2023.emnlp-main.153},
   editor = {Houda Bouamor and Juan Pino and Kalika Bali},
   booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
   month = {12},
   pages = {2511-2522},
   publisher = {Association for Computational Linguistics},
   title = {G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment},
   url = {https://aclanthology.org/2023.emnlp-main.153},
   year = {2023},
}

@inproceedings{Maynez2020,
   abstract = {It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.},
   author = {Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan McDonald},
   city = {Online},
   doi = {10.18653/v1/2020.acl-main.173},
   editor = {Dan Jurafsky and Joyce Chai and Natalie Schluter and Joel Tetreault},
   booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
   month = {7},
   pages = {1906-1919},
   publisher = {Association for Computational Linguistics},
   title = {On Faithfulness and Factuality in Abstractive Summarization},
   url = {https://aclanthology.org/2020.acl-main.173},
   year = {2020},
}

@article{Singhal2023,
   abstract = {Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
   author = {Karan Singhal and Shekoofeh Azizi and Tao Tu and S. Sara Mahdavi and Jason Wei and Hyung Won Chung and Nathan Scales and Ajay Tanwani and Heather Cole-Lewis and Stephen Pfohl and Perry Payne and Martin Seneviratne and Paul Gamble and Chris Kelly and Abubakr Babiker and Nathanael Schärli and Aakanksha Chowdhery and Philip Mansfield and Dina Demner-Fushman and Blaise Agüera y Arcas and Dale Webster and Greg S. Corrado and Yossi Matias and Katherine Chou and Juraj Gottweis and Nenad Tomasev and Yun Liu and Alvin Rajkomar and Joelle Barral and Christopher Semturs and Alan Karthikesalingam and Vivek Natarajan},
   doi = {10.1038/s41586-023-06291-2},
   issn = {14764687},
   issue = {7972},
   journal = {Nature},
   month = {8},
   pages = {172-180},
   pmid = {37438534},
   publisher = {Nature Research},
   title = {Large language models encode clinical knowledge},
   volume = {620},
   year = {2023},
}

@inproceedings{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and \L\{\}ukasz Kaiser and Illia Polosukhin},
   city = {Red Hook, NY, USA},
   isbn = {9781510860964},
   booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
   pages = {6000-6010},
   publisher = {Curran Associates Inc.},
   title = {Attention is all you need},
   year = {2017},
}

@misc{Wei2022,
   author = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
   title = {Emergent Abilities of Large Language Models},
   url = {https://arxiv.org/abs/2206.07682},
   year = {2022},
}

@article{Yang2023,
   author = {Hongyang Yang and Xiao-Yang Liu and Christina Dan Wang},
   journal = {FinLLM Symposium at IJCAI 2023},
   title = {FinGPT: Open-Source Financial Large Language Models},
   year = {2023},
}

@misc{Zhao2023,
   author = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
   title = {A Survey of Large Language Models},
   url = {https://arxiv.org/abs/2303.18223},
   year = {2023},
}

@inproceedings{Zheng2024,
   abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.},
   author = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P Xing and Hao Zhang and Joseph E Gonzalez and Ion Stoica},
   city = {Red Hook, NY, USA},
   booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
   publisher = {Curran Associates Inc.},
   title = {Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
   year = {2024},
}

@misc{chiang2022overcomingtheoreticallimitationselfattention,
      author={David Chiang and Peter Cholak},
      title={Overcoming a Theoretical Limitation of Self-Attention}, 
      year={2022},
      eprint={2202.12172},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.12172}, 
}

