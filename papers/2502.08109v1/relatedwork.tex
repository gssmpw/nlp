\section{Related Work}
\label{sec:relatedwork}

\subsection{Definitions of Large Language Models}
A Large Language Model is an artificial intelligence model based on the Transformer architecture \cite{Vaswani2017}. It refers to a pre-trained language model (PLM) with a parameter size exceeding a certain threshold \cite{Zhao2023}. LLMs are trained on massive datasets and typically have billions to hundreds of billions of parameters. Due to the extensive data used in their training, LLMs exhibit exceptional performance across various NLP tasks, including text generation, translation, and summarization.

Notably, LLMs that surpass a certain parameter scale demonstrate emergent abilities not found in smaller models. Examples of these abilities include in-context learning, instruction following, and chain-of-thought (CoT) reasoning \cite{Wei2022}. These capabilities enable LLMs to handle more complex tasks, such as advanced reasoning, problem-solving, and generating multi-turn responses.

Although LLMs are primarily used for general downstream tasks, their increasing significance in both academia and industry has led to research into domain-specific LLMs. Examples include the Med-PaLM series for the medical domain \cite{Singhal2023} and FinGPT for the financial domain \cite{Yang2023}. These advancements underscore the growing need for LLMs not only in language generation but also in addressing specialized tasks across various fields.

\subsection{Definitions of Hallucination}
In NLP, hallucination refers to content that is unreliable or illogical compared to the provided source material \cite{Ji2023}, \cite{Maynez2020}. Previous studies categorize hallucinations into two broad two types: intrinsic and extrinsic \cite{Ji2023}, \cite{Maynez2020},\cite{Dziri2021}, \cite{Huang2023a}.

Intrinsic hallucination occurs when the generated output contradicts the source content. For example, this happens when a model produces information that conflicts with the given data in response to a factual question. In contrast, extrinsic hallucinations involve outputs that include unverifiable or nonexistent information. This often occurs when the model generates content that cannot be corroborated by the source material.

In the context of LLMs, hallucination can be defined more specifically. LLM hallucinations, which prioritize user instructions and interactions, can be categorized based on factuality and faithfulness \cite{Huang2023b}. Factual hallucinations arise when a model generates outputs that are based on real-world information but are either incorrect or unverifiable. For instance, if the model inaccurately presents well-known facts or mentions nonexistent information, it is considered a factual hallucination. Faithfulness-related hallucinations occur when the model generates responses unrelated to user instructions or the provided content, or when it produces internally inconsistent answers. This type of hallucination is particularly important in conversational models.

The issue of hallucination may stem from several factors, including the use of outdated data during the data collection process \cite{Kasai2024} or biased data \cite{Bender2021} used for model training \cite{Huang2023b},\cite{chiang2022overcomingtheoreticallimitationselfattention},  \cite{Li2023a}. Furthermore, the risk of hallucinations tends to increase with the size and complexity of the models.

\subsection{LLM-Based Evaluation of LLMs}
One of the key challenges discussed alongside the development of LLMs is the difficulty in accurately evaluating the context and meaning of generated responses using traditional quantitative metrics. While human evaluation has been employed to address this limitation, it has considerable drawbacks, particularly in terms of time and resource consumption \cite{Bubeck2023},\cite{Bang2023}.

To overcome these challenges, the use of LLMs as evaluation tools, or “LLM judges,” has gained attention. \cite{Zheng2024} pioneered an LLM-based evaluation framework, showing that strong LLMs achieved over 80\% agreement with human experts in evaluations. Subsequent studies by \cite{Bai2023}, \cite{Liu2023}, and \cite{Li2023b} have expanded on this approach, further validating the utility of LLM judges.

The introduction of LLM judges provides an efficient solution for evaluating large-scale data, where human evaluation may be impractical. In addition to quantitative  assessments, LLM judges offer qualitative evaluations based on their understanding of context and adherence to user instructions, making them versatile tools for comprehensive evaluation.