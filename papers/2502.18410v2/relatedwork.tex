\section{Related Work}
\subsection{Time-Series Mixer (TSMixer)}
TSMixer is an MLP-based architecture for time series forecasting \cite{chen2023tsmixer}, which analyzes the performance of linear models for time series forecasting rather than RNNs or Transformer-based frameworks and demonstrates its competitive performance on several time series forecasting benchmarks. TSMixer consists of multiple MLP layers across time and feature dimensions (i.e., time-mixing and feature-mixing MLP block) to capture time-domain temporal patterns and feature-domain cross-variate information alternatively with residual connections and batch norm. The residual designs ensure that TSMixer retains the capacity of temporal linear models. In contrast to recent Transformer-based models, the architecture of TSMixer is relatively simple to implement. Despite its simplicity, it demonstrates that TSMixer remains competitive with state-of-the-art models at representative benchmarks \cite{chen2023tsmixer}. The detail of TSMixer architecture is shown in Figure \ref{figure:TSMixer}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{figures/TSKAN_02}
\caption{TSMixer for multivariate time series forecasting \cite{chen2023tsmixer}}
\label{figure:TSMixer}
\end{figure}

\subsection{Kolmogorov-Arnold Network (KAN)}
As MLPs are based on the universal approximation theorem \cite{cybenko1989approximation}, which states that neural networks with a single hidden layer can approximate any continuous function with finite support, KANs rely on the Kolmogorov-Arnold representation theorem \cite{arnold2009functions, arnold2009representation}. The theorem states that any multivariate continuous function $f(x)$ on a bounded domain, where $x=(x_{1}, \dots, x_{n})$, can be written as a finite composition of continuous functions of a single variable and the binary operation of addition. Formally, a multivariate continuous function $f(x) : [0,1]^{n} \rightarrow \mathbb{R}$ can be represented by the finite composition of univariate functions \cite{liu2024kan}:

\begin{align}
f(x) = f(x_{1}, \dots , x_{n}) = \sum_{j=1}^{2n+1} \Phi_j \left( \sum_{i=1}^{n} \phi_{j,i} (x_i) \right) \label{eq:KAN}
\end{align}

where an outer function is $\Phi_j: \mathbb{R} \rightarrow \mathbb{R}$ and an inner function is $\phi_{j,i}: [0,1] \rightarrow \mathbb{R}$. 

As a MLP consists of layers where each layer performs a linear transformation followed by a non-linear activation function, a KAN layer can be defined as a matrix $\mathbf{\Phi}$ of univariate functions:

\begin{align}
\mathbf{\Phi(x)} = \{ \phi_{j,i} \}, \qquad i = \{1, \dots, n_{in}\}, \, j = \{1, \dots, n_{out}\} \label{eq:KAN_layer}
\end{align}

where the univariate functions $\phi_{j,i}$ have trainable parameters and $n_{in}$ is the number of inputs and $n_{out}$ is the number of outputs.

Generally, KANs can be expressed by a composition of multiple KAN layers, $y = \textbf{KAN}(x) = (\mathbf{\Phi_{L}} \circ \cdots \circ \mathbf{\Phi_{1}})(x)$ where $L$ is the number of layers. Then, the equation \ref{eq:KAN} for the Kolmogorov-Arnold representation theorem can be represented by a two-depth KAN layer of shape $[n, 2n + 1, 1]$, consisting of an inner layer with $n_{in} = n$ and $n_{out} = 2n + 1$, and an outer layer with $n_{in} = 2n + 1$ and $n_{out} = 1$ \cite{liu2024kan}.

While MLPs employ fixed activation functions on nodes, KANs employ learnable activation functions on edges \cite{liu2024kan}. Specifically, KANs learn activation patterns dynamically by replacing traditional linear weights on MLPs with univariate functions parameterized as splines, where a spline is defined by the order $k$ (the degree of the polynomial functions used to interpolate the curve between control points), and the number of intervals $G$ (the number of segments between adjacent control points). During spline interpolation, the control points separated by $G$ intervals are connected to form a smooth curve \cite{vaca2024kolmogorov}. Through learnable activation functions, KANs improve accuracy and interpretability while maintaining comparable or superior performance with more compact architectures across various tasks.

Vaca-Rubio et al. \cite{vaca2024kolmogorov} demonstrate that KANs consistently outperform MLPs with lower error metrics while achieving better results with reduced computational resources in time series forecasting. However, due to their intrinsic architecture, KANs have $(k + G)$ times more learnable parameters compared to MLPs \cite{yu2024kan}. To enhance computational efficiency, several regularization techniques have proven effective in optimizing KAN training \cite{cheon2024improving}. Specifically, the incorporation of dropout, weight decay, and batch normalization not only accelerates convergence but also significantly improves the model's generalization capabilities. Additionally, Bayesian optimization can be leveraged to reduce the parameter search space for more efficient training \cite{snoek2012practical}.