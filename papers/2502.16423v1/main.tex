
\documentclass[10pt,journal,compsoc]{IEEEtran}

% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.

\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{color}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{subfigure}
\usepackage{float}
\usepackage{color}
\usepackage{setspace}
%\usepackage{hyperref}
\usepackage{url}
\usepackage{makecell}
% \usepackage{cleveref}

\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=green]{hyperref}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{pifont}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{ulem}
\usepackage[table,xcdraw]{xcolor}
\usepackage{booktabs} % for professional tables
\usepackage{setspace}
\usepackage{wrapfig}

\usepackage{ragged2e}
\usepackage{pifont}

\usepackage[ruled]{algorithm2e}                  %style 1
%\usepackage[ruled,vlined]{algorithm2e}          %style 2
%\usepackage[linesnumbered,boxed]{algorithm2e}   %style 3


% correct bad hyphenation here
% \hyphenation{op-tical net-works semi-conduc-tor}

% \usepackage{bibspacing}
% \setlength{\bibspacing}{0\baselineskip}



\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.



\title{Unified Prompt Attack Against Text-to-Image Generation Models}




\author{Duo Peng, Qiuhong Ke, Mark He Huang, Ping Hu and Jun Liu% <-this % stops a space
\IEEEcompsocitemizethanks{
	\IEEEcompsocthanksitem Corresponding author: Jun Liu.
	\IEEEcompsocthanksitem D. Peng and M. H. Huang are with Singapore University of Technology and Design, Singapore. \protect\\
	E-mail: duo\_peng@mymail.sutd.edu.sg, he\_huang@mymail.sutd.edu.sg.
	\IEEEcompsocthanksitem  Q. Ke is with Monash University, Australia. \protect\\
	E-mail: Qiuhong.Ke@monash.edu.
 	\IEEEcompsocthanksitem  P. Hu is with University of Electronic Science and Technology of China, China. \protect\\
	E-mail: chinahuping@gmail.com.
	\IEEEcompsocthanksitem  J. Liu is with Lancaster University, United Kingdom, and Singapore University of Technology and Design, Singapore. \protect\\
	E-mail: j.liu81@lancaster.ac.uk.}
}


%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}





%% The paper headers
\markboth{Transactions on Pattern Analysis and Machine Intelligence}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.

% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}

% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.




\IEEEtitleabstractindextext{%
	\begin{abstract}
		\justifying Text-to-Image (T2I) models have advanced significantly, but their growing popularity raises security concerns due to their potential to generate harmful images. To address these issues, we propose UPAM, a novel framework to evaluate the robustness of T2I models from an attack perspective. Unlike prior methods that focus solely on textual defenses, UPAM unifies the attack on both textual and visual defenses. Additionally, it enables gradient-based optimization, overcoming reliance on enumeration for improved efficiency and effectiveness. To handle cases where T2I models block image outputs due to defenses, we introduce \textbf{Sphere-Probing Learning (SPL)} to enable optimization even without image results. Following SPL, our model bypasses defenses, inducing the generation of harmful content. To ensure semantic alignment with attacker intent, we propose \textbf{Semantic-Enhancing Learning (SEL)} for precise semantic control. UPAM also prioritizes the naturalness of adversarial prompts using \textbf{In-context Naturalness Enhancement (INE)}, making them harder for human examiners to detect. Additionally, we address the issue of iterative queries--common in prior methods and easily detectable by API defenders--by introducing \textbf{Transferable Attack Learning (TAL)}, allowing effective attacks with minimal queries. Extensive experiments validate UPAM's superiority in effectiveness, efficiency, naturalness, and low query detection rates.
	\end{abstract}
	% Note that keywords are not normally used for peerreview papers.
	\begin{IEEEkeywords}
		Adversarial Prompt Attack, Text-to-Image Model, Black Box, Gradient-Based Optimization, Naturalness.
\end{IEEEkeywords}}






% make the title area
\maketitle

% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle









%==================   1 Introduction =====================
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}


The application of Text-to-Image (T2I) models has become widespread, owing to their exceptional ability to produce high-quality images from text prompts\cite{xu2018attngan,zhang2017stackgan,zhang2018stackgan++}. 
As T2I models are getting increasingly integrated into various online services, the misuse of the T2I technique has raised security concerns and even pressured changes in the regulations/laws in Europe \cite{galetin2022review} and US \cite{daly2020ai}.
In response, service providers are urged to deploy defense mechanisms to prevent harmful generation such as violent or obscene images \cite{birhane2021multimodal,saharia2022photorealistic,yu2022scaling}.



Despite the implementation of defense mechanisms, T2I APIs remain vulnerable to adversarial attacks, as highlighted in prior studies \cite{liu2023riatig,daras2022discovering,milliere2022adversarial}. In this context, ensuring the robustness and security of T2I systems against evolving attacks is crucial. By mimicking attacks \cite{yang2023mma}, it allows researchers to identify weaknesses and inform the development of stronger defenses. As this field is still emerging, our work explores this critical area to test and enhance the robustness of T2I models, contributing to improved security protocols.


% ==============================Figure 1
\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{images/task_new.pdf} 
\vspace{-4mm}
\caption{T2I APIs could incorporate both textual filters and visual checkers for dual defense, which can deny the forward propagation of data when harmful information is detected \cite{schramowski2023safe,rombach2022high}.
}
\vspace{-0mm}
\label{fig:task}
\end{figure}
% ==============================Figure 1


In order to mitigate the generation of harmful content, public APIs typically employ textual filters to block malicious prompts at the input stage \cite{Midjourney,Leonardo}. These filters can easily identify and deny naive prompts containing explicit harmful content, preventing their submission to T2I models. In adversarial attacks \cite{liu2023riatig}, a common approach is to transform such naive prompts into adversarial ones that bypass these filters and induce T2I models to generate harmful content. However, since T2I models and their defense systems are often packaged as black boxes, attackers lack access to internal model structures and parameters, making it challenging to craft effective adversarial prompts using conventional learning algorithms \cite{papernot2017practical}.


Confronted with this black-box scenario, existing attack methods \cite{liu2023riatig,daras2022discovering,milliere2022adversarial,struppek2022biased,jin2020bert,yang2024sneakyprompt} primarily depend on \textit{enumeration}, which involves searching for effective adversarial prompts to bypass the textual filter via attempting various candidates.
However, such enumeration-based methods tend to show limited effectiveness and efficiency, especially when faced with a more practical dual-defense scenario where API providers implement both textual filters and visual checkers \cite{schramowski2023safe,rombach2022high} (see Fig. \ref{fig:task}).
It can be contended that the limited \textit{effectiveness} and \textit{efficiency} demonstrated by enumeration methods stem from their insufficient optimization capabilities in exploring optimal solutions.
Therefore, inspired by the success of gradient-based optimization in handling intricate problems both \textit{effectively} and \textit{efficiently} \cite{daoud2023gradient,zhou2021random}, we propose a novel \textbf{Unified Prompt Attack Model (UPAM)} to address this task in a gradient-based learning manner.
Although some  methods \cite{golovin2017google,oh2023blackvip} can address the learning in the black-box setting, they rely on the output results of the black box to compute losses and estimate gradients. However, in this task, defenses often block outputs (i.e., ``Deny'' in Fig. \ref{fig:task}), making it difficult to calculate losses and discover gradients.



In response to the aforementioned challenge, we introduce a novel \textbf{Sphere-Probing Learning (SPL)} approach designed to estimate gradients when no image results are returned.
Our key insight is that when the attack model invades the black box, its attack case is binary (see Fig. \ref{fig:task}) -- either ``Deny'' (images not returned) or ``Pass'' (images returned).
The goal of SPL is to optimize the parameters of the attack model, aiming to change the model's attack case from ``Deny'' (i.e., unable to bypass defenses) to ``Pass'' (i.e., capable of bypassing defenses).
SPL gets rid of the need for loss-based gradients by sampling in the \textit{parameter space} of the attack model. As illustrated in Fig. \ref{fig_spl_abs}, SPL samples parameter configurations around the current model (gray star) and derives gradients (yellow arrow) from these samples to guide optimization toward the ``Pass'' case. This approach allows gradient-based training even when no image results are returned, enabling the attack to bypass both textual filters and visual checkers.



Following training with SPL, the attack model can effectively prompt the T2I API to generate images.
Nevertheless, in this task, it's also crucial to ensure these returned images align with the intended harmful semantics.
However, simply applying harmful-content gradients can trigger defense mechanisms, leading to the ``Deny'' case again and hindering SPL's learning progress. 



To tackle this challenge, we introduce a novel \textbf{Semantic-Enhancing Learning (SEL)} scheme designed to further enhance the harmful semantics of generated images, working compatibly with SPL.
SEL estimates gradients from the loss function measuring the semantic distance between returned images and harmful content. To safeguard SPL's progress, SEL uses gradient projection to remove components that may trigger defense mechanisms, enabling robust gradient-based learning for harmful semantic enhancement.


Moreover, it is crucial to ensure the naturalness of the attack prompts.
To address this issue, we draw inspiration from advancements in Large Language Models (LLMs) for natural language synthesis \cite{huang2023towards} and propose an \textbf{In-context Naturalness Enhancement (INE)} scheme. Leveraging the in-context learning capability of LLMs, our UPAM generates human-readable, spelling-correct adversarial prompts, significantly enhancing attack naturalness.


Additionally, we devise a \textbf{Transferable Attack Learning (TAL)} scheme to enable our UPAM to work with only few-shot API queries, significantly reducing the number of queries compared to previous enumeration-based methods.
Specifically, TAL trains UPAM using a set of offline T2I models and then transfers the offline-trained UPAM to the online T2I API. During offline learning, TAL focuses on enhancing UPAM's transferability to new T2I models, enabling it to adapt to the online T2I API with minimal queries.



In summary, the contributions of this work include: 
\begin{itemize}

\item To investigate the security issue in T2I models, we present a novel unified attack model, UPAM, with greater effectiveness, efficiency, naturalness and fewer query times than previous enumeration work.
\item To achieve better effectiveness and efficiency, we devise SPL and SEL schemes that collaboratively enable gradient-based optimization. 
\item To enhance the naturalness of our generated adversarial prompts, we propose an INE scheme that harnesses the natural expressive power of the LLM.
\item To enable our UPAM to function with only few-shot queries, we design a TAL scheme that extracts knowledge from offline attacks. 
\item Experiments on various T2I models demonstrates UPAM's superiority over existing methods.

\end{itemize}



% ==============================Figure 2
\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{images/SPL_abs_new.pdf} 
\vspace{-4mm}
\caption{Intuitive illustration of our SPL scheme.
}
\vspace{-0mm}
\label{fig_spl_abs}
\end{figure}
% ==============================Figure 2


This paper is an extension of our preliminary conference version \cite{peng2024UPAM}. In this paper, we incorporate multiple improvements:
(1) In \cite{peng2024UPAM}, we simply utilized a pre-trained LLM to ensure the naturalness of our adversarial prompts. In this paper, we propose an INE scheme to further enhance the natural representation ability of our framework.
(2) Compared to \cite{peng2024UPAM}, this paper additionally considers the issue of API query times, and proposes a TAL scheme to enable our framework to operate effectively with only few-shot queries. The associated experimental results are reported.
(3) In \cite{peng2024UPAM}, we validated the effectiveness of our framework on the COCO dataset, while in this paper, we additionally evaluate our framework on the NSFW dataset which contains truly harmful content.
(4) More detailed ablation analyses of each proposed design are presented in this paper.














%==================   2 Related Work =====================
\vspace{2mm}
\section{Related Work}


%---------------   2 .1  --------------------
\subsection{Adversarial Whitebox and Blackbox Attacks}

Early works \cite{szegedy2013intriguing,goodfellow2014explaining} revealed the susceptibility of Deep Neural Networks (DNNs) to adversarial attacks, which involve crafting input adversarial examples designed to mislead models into making incorrect predictions. 
Such attacks can significantly compromise model performance, leading to issues such as misclassification. 
Adversarial attack strategies are broadly categorized into white-box and black-box settings, depending on the attacker's access to the target model \cite{costa2024deep}.
In the white-box setting, attackers possess complete knowledge of the model, including its architecture and parameters. This allows them to employ gradient-based optimization \cite{amari1993backpropagation} to craft adversarial examples aligned with their attack objectives \cite{carlini2017towards,goodfellow2014explaining,moosavi2016deepfool,wong2020fast,madry2017towards,dong2018boosting}. While highly effective, these methods depend on full model access, an assumption that is rarely feasible in real-world scenarios.
Nevertheless, most current threat models still rely on the white-box setting, which significantly limits their relevance for real-world applications, particularly for text-to-image (T2I) systems that are typically deployed as black-box APIs. 
In recent years, some progress has been made in black-box attacks \cite{andriushchenko2020square,wicker2018feature}, where attackers only have access to model outputs without insight into the architecture or parameters.
These methods rely on querying the model and using its outputs as feedback to iteratively refine adversarial examples.


Unlike most adversarial attack studies, we focus on enabling the gradient-based attack in a more constrained black-box setting, where the defensive mechanisms in T2I models can result in no outputs for feedback (no image results returned).






%---------------   2 .2  --------------------
\subsection{Adversarial Attacks on LLMs and VLMs}


Despite significant progress in safety assurance, large language models (LLMs) and vision-language models (VLMs) remain susceptible to carefully crafted inputs, known as adversarial attacks, which can manipulate them into generating unsafe content \cite{liu2023jailbreaking,tu2025many}. There are several preliminary attempts to explore adversarial attack on LLMs and VLMs.
As for attack against LLMs, Li et al. \cite{li2023multi} demonstrated a multistep adversarial prompting capable of extracting sensitive personal information from LLMs. 
Automated adversarial attacks are also gaining traction, with Deng et al. \cite{deng2023jailbreaker} proposing a time-based reverse-engineering method to uncover defense mechanisms and streamline attack strategies across different LLMs.
Zou et al. \cite{zou2023universal} proposed a simple yet effective attack method that employs greedy and gradient-based techniques to automatically generate adversarial suffixes for attacking open-source (white-box) language models, while also demonstrating the transferability of these adversarial prompts to black-box LLMs.
Additionally, Wei et al. \cite{wei2024jailbroken} highlighted that large language models remain vulnerable to adversarial misuse due to two key safety-training limitations: competing objectives and mismatched generalization. Leveraging these two failure modes, they devised a framework for designing adversarial prompts.
As for attack against VLMs, Shayegani et al. \cite{shayegani2023jailbreak} introduced a method for crafting adversarial images within the visual-textual joint embedding space, aiming to subtly influence model outputs.
Qi et al. \cite{qi2024visual} highlighted the increasing adversarial risks of vision-integrated LLMs, showing how a single visual adversarial example can universally jailbreak aligned models, exposing broader security vulnerabilities tied to multi-modal AI systems.
Hubinger et al. \cite{hubinger2024sleeper} pointed out the ongoing challenge of persistent deceptive behaviors, which remain evident even after the application of safety training techniques. 
Concurrently, Yang et al. \cite{yang2024watch} developed a framework for agent backdoor attacks and demonstrated its effectiveness across two representative agent tasks.



Compared to the rapid progress in attacking LLMs and VLMs, research on attacks against T2I models remains in its early stages, with fewer and less advanced methods. Given the severe potential impact of spreading harmful images, we believe that investigating attacks on T2I models holds substantial research importance.






%---------------   2 .3  --------------------
\subsection{Adversarial Attacks on T2I Models}



Adversarial attack on T2I models remains a relatively under-explored area. The goal of this task is to craft adversarial prompts that bypass the defense mechanisms of T2I models, causing them to generate outputs they are designed to block. Most existing attack methods rely heavily on enumeration, wherein candidate prompts are iteratively generated and tested to identify a successful one. Several prior works have explored this domain, including \cite{daras2022discovering,ramesh2022hierarchical,milliere2022adversarial,struppek2022biased,liu2023riatig,yang2024sneakyprompt,yang2023mma}.
Specifically, Daras et al. \cite{daras2022discovering} proposed a pioneering work that investigated vulnerabilities of DALL·E 2 \cite{ramesh2022hierarchical}. 
They found that it is possible to deceive textual filters by using unnormal text as prompts. 
Later, Milli{\`e}re et al. \cite{milliere2022adversarial} proposed to create adversarial prompts by combining multi-language sub-word segments.
Struppek et al. \cite{struppek2022biased} demonstrated that replacing the original text with non-Latin letters can induce different visual contents in generated images. 
Liu et al. \cite{liu2023riatig} devised a character-level modification strategy (e.g., replacing, deleting, or swapping characters) to transform  the naive prompt into an adversarial one.
Yang et al. \cite{yang2024sneakyprompt} proposed to directly change the word tokens that are identified as sensitive (harmful) into non-sensitive (safe) ones with the help of reinforcement learning.
Yang et al. \cite{yang2023mma} present a multimodal attack, which inputs both an adversarial text prompt and an adversarial image into T2I diffusion models.


In contrast to previous works that focus on deceiving the textual filter only, our paper proposes a unified attack framework UPAM, which effectively deceives both the textual filter and the visual checker simultaneously. 
Unlike previous methods using enumeration for massive attempts, our UPAM allows the training of the parameterized model based on gradients, where the (parameterized) attack model can learn how to adaptively bypass both the textual filters and visual checkers, which brings much effectiveness.
Moreover, unlike previous approaches that need to undertake time-consuming attempts during inference, we enable the gradient-based optimization for parameterized model training. The trained parameterized model allows for fast inference, which brings much efficiency.
Additionally, previous attack methods often use unnatural prompts, like ``Apoploe vesrreaitais'' for bugs'' \cite{daras2022discovering}, reducing naturalness and making attack prompts easier to identify. 
In contrast, our UPAM is designed to enhance the naturalness of the generated adversarial prompts. 
Besides, previous methods mostly overlook a practical issue: they require iteratively querying the target T2I API to gather information for adjusting adversarial prompts. Such numerous queries can easily be detected by the API defender \cite{li2022blacklight}.
Different from these methods, our UPAM efficiently performs the attack requiring only few-shot queries.




% ==============================Figure 3
\begin{figure*}[t]
\centering
\includegraphics[width=0.85\textwidth]{images/pipeline_new.pdf} 
\vspace{-2mm}
\caption{Overview of our UPAM framework. 
Initially, due to the lack of training, our adversarial prompt $\mathbf{T}^{\ast}$ is unable to bypass the API's defenses (i.e., return no image).
At the pre-training phase, we propose an SPL scheme that estimates the gradient $\pmb{g}_\mathrm{spl}$ under the challenging no-result scenario, finally compelling the black-box T2I model to return images.
At the fine-tuning phase, we utilize an SEL scheme to provide the gradient $\pmb{g}_\mathrm{sel}$ to align the returned images $\mathbf{I}^{\ast}$ with the target semantics.
}
\vspace{-1mm}
\label{fig:pipeline}
\end{figure*}
% ==============================Figure 3












%==================   3 Methodology =====================
\section{Methodology}\label{sec:method}


In the task of adversarial attack against T2I models, given the paired data $\{ \mathbf{T}, \mathbf{I} \}$, where $\mathbf{T}$ denotes the naive text prompts containing malicious information, and $\mathbf{I}$ denotes the corresponding target harmful images, the goal of the attacker is to construct an attack model $H$: $\mathbf{T}$ $\rightarrow$ $\mathbf{T}^{\ast}$, where $H$ rewrites the naive text prompt $\mathbf{T}$ into the adversarial prompt $\mathbf{T}^{\ast}$, capable of inducing the T2I API to generate a harmful image $\mathbf{I}^{\ast}$ without triggering the defense mechanism.



Previous attack methods \cite{liu2023riatig,daras2022discovering,milliere2022adversarial,struppek2022biased,jin2020bert,yang2024sneakyprompt} treat $H$ as an enumeration model. Specifically, $H$ is designed to replace words in the original prompt with new ones to create candidates. They iteratively utilize $H$ to generate candidate prompts, querying the API with each one until a candidate successfully induces the black-box T2I API to produce an image with sufficient semantic similarity to the intended harmful content.




Different from previous methods, in this paper, we propose a unified framework UPAM that aims to train a parameterized model $H$: $\mathbf{T}$ $\rightarrow$ $\mathbf{T}^{\ast}$, with only few-shot queries to the T2I API.
In this way, during inference (testing), the trained $H$ can immediately and effectively convert testing data (naive prompts) $\mathbf{T}_t$ into adversarial ones $\mathbf{T}_t^{\ast}$. 



Our UPAM contains several components. To clarify each component, we organize this methodology section as follows: (1) In Sec. \ref{sec:spl} and \ref{sec:sel}, a \textbf{Sphere-Probing Learning (SPL)} scheme and a \textbf{Semantic-Enhancing Learning (SEL)} scheme are proposed to ensure \textit{effectiveness} and \textit{efficiency} of attack; (2) In Sec. \ref{sec:llm}, an \textbf{In-context Naturalness Enhancement (INE)} scheme is introduced to enhance the \textit{naturalness} of the generated prompts; (3) In Sec. \ref{sec:TAL}, a \textbf{Transferable Attack Learning (TAL)} scheme is detailed to reduce API \textit{query times} to a few-shot level.



% ==============================Figure 4
\begin{figure*}[t]
\centering
\includegraphics[width=0.82\textwidth]{images/SPL_new.pdf} 
\vspace{-2mm}
\caption{Illustration of our SPL scheme.  In this figure, each point in the optimization space denotes a specific configuration of model parameters. 
\textbf{(a)} In the first stage of SPL, we optimize the model from ``Deny'' region towards the ``Pass'' region, thereby enabling the attack to bypass the defenses. 
\textbf{(b)} In the second stage of SPL, we start to optimize the model in the opposite direction to approach the boundary, thereby enhancing the likelihood of generating harmful images. 
}
\vspace{-2mm}
\label{fig:spl}
\end{figure*}
% ==============================Figure 4






%---------------   3.1  --------------------
\vspace{-1mm}
\subsection{Pre-training with SPL}\label{sec:spl}


In our UPAM framework, the model $H$ consists of a frozen off-the-shelf LLM and a learnable LoRA adapter \cite{hu2021lora}, as shown in Fig. \ref{fig:pipeline}. We only update the LoRA adapter, which contains a minimal number of parameters. This allows us to preserve the knowledge of the pre-trained LLM, thus maintaining its ability to generate natural $\mathbf{T}^{\ast}$ for attack naturalness.
In the beginning, given an initial attack model $H$ that lacks the ability to bypass defenses, its untrained parameters (weights) typically lead to the ``Deny'' case, due to the language model's tendency to preserve naive textual cues from the input \cite{KALYAN2024100048}. 
Next, we introduce a Sphere-Probing Learning (SPL) scheme. Even without image results, SPL can still search for effective gradients to optimize the model $H$, resulting in the ``Pass'' case (i.e., compelling the black box to return images).



As shown in Fig. \ref{fig:spl}, SPL contains two learning stages: (a) optimizing towards the ``Pass'' region and (b) optimizing towards the boundary. The first stage is to train our attack model to induce the black-box T2I API to return images, while the second stage aims to increase the likelihood of returning harmful images. For brevity, hereafter, we use ``black box'' to refer to the ``black-box T2I API''.
Next, we will describe the two stages of SPL one by one. 



\vspace{2mm}
\noindent\textbf{(a) Optimizing towards the ``Pass'' region}


In the parameter space of the attack model $H$, various configurations can be categorized based on the black box's binary output: ``Deny'' (no images returned) or ``Pass'' (images returned). Accordingly, the parameter space is divided into two regions: the ``Deny'' region and the``Pass'' region (red and green areas in Fig. \ref{fig_spl_abs}).
If the black box does not return images, the model parameters lie in the ``Deny'' region. Conversely, its parameters fall in the ``Pass'' region. Initially, $H$ typically resides in the ``Deny'' region. The first stage of SPL focuses on estimating gradients to iteratively optimize $H$ towards the ``Pass'' region until the model successfully transitions, as illustrated in Fig. \ref{fig:spl} (a).



As only LoRA in the attack model $H$ is learnable, we perform optimization in the parameter space of LoRA, denoted as $\psi$. Each point in the optimization space represents a specific configuration of $\psi$.
As shown in Fig. \ref{fig:spl} (a) left, SPL firstly samples several points on a sphere centered around the current model parameters (marked as a gray star) with $r$ as the radius.
Then, we query the black box's output case for each sampled point.
If all points are predicted as ``Deny'' (meaning no ``Pass'' points are detected in the sphere), we increase the sphere radius $r$ to probe the ``Pass'' points.
Once the sphere encompasses ``Pass'' points, we stop increasing the radius and start optimizing the LoRA parameters $\psi$.
Intuitively, the points predicted as ``Pass'' represent the direction we aim to move towards. Hence, we calculate the average of these points and optimize the model parameters in the direction of this average. 
Here, we define $\Phi$, which use $1$ to mark ``Pass'' points:
% ======================== Eq.1
\begin{equation}
\vspace{-0mm}
\Phi(\psi)= \begin{Bmatrix}
 1 & \mathbf{if} \rm{\; image \;returned \,(``Pass")} \\
 0 & \mathbf{otherwise}
\end{Bmatrix}
\label{eq:phi}
\end{equation}
% ======================== Eq.1
After sampling on the sphere, we can use the function $\Phi$ to mark all ``Pass'' points for gradient calculation. Specifically, given the current model (LoRA) parameters $\psi$, the SPL gradient $\pmb{g}_\mathrm{spl}$ is calculated as:
% ======================== Eq.2
\begin{equation}
\vspace{-0mm}
\pmb{g}_\mathrm{spl}(\psi)=\frac{1}{N}\sum_{n=1}^{N}  \Phi(\psi+r\cdot z_n)\cdot z_n,
\label{eq:grad}
\end{equation}
% ======================== Eq.2
where $\psi \in \mathbb{R}^d$ denotes the current model parameters, $z_n$ represents a unit-length vector randomly sampled from a $d$-dimensional space, $r$ denotes the radius of the sample sphere (as shown in Fig. \ref{fig:spl} a), $\psi+r\cdot z_n$ is the sampling point, and $N$ denotes the total number of points sampled on this sphere.
Note that different vectors $z_n$ possess different directions, yet they share the same length, resulting in all sampling points being located on a sphere.
Leveraging the above sphere-sampling exploration, we can identify an effective optimization direction $\pmb{g}_\mathrm{spl}$.
In this way, even if the current model $\psi$ is incapable of inducing the API to output image results, the gradient $\pmb{g}_\mathrm{spl}$ can still be computed via the above parameter-space sampling.
Next, we use $\pmb{g}_\mathrm{spl}$ to update $\psi$ as follows:
% ======================== Eq.3
\begin{equation}
\vspace{-0mm}
\psi = \psi + \alpha \cdot \pmb{g}_\mathrm{spl}(\psi),
\label{eq:update}
\end{equation}
% ======================== Eq.3
where $\alpha$ is the learning rate, which is related to the current radius $r$, i.e., $\alpha = r/k$, $k$ is a hyper-parameter. 
The role of Eq. \ref{eq:grad} is to compute the gradient, while Eq. \ref{eq:update} is used to update the model parameters based on the computed gradient. When handling Eq. \ref{eq:grad} and Eq. \ref{eq:update} for one time, the model is optimized for one iteration. 
We alternate between Eq. \ref{eq:grad} and Eq. \ref{eq:update} to perform the gradient-based optimization iteratively, until the sphere center (i.e., model parameters) falls into the ``Pass'' region, as shown in Fig. \ref{fig:spl} (a).
After the first stage of SPL, the model parameters are located in  the ``Pass'' region, meaning the current model is capable of deceiving the defenses and getting images returned.


Since SPL employs a large radius $r$ to find ``Pass'' points, the optimized model may land in the ``Pass'' region but remain far from its boundary. This occurs because $r$ is positively correlated with the optimization step size.
However, being deep within the ``Pass'' region increases the likelihood of the black box returning safe images rather than the intended harmful ones. This is because these regions represent the classification tendencies of the defense system: deeper into the ''Pass'' region, the system is more confident the content is safe.


Motivated by the observation that misclassifications often occur near the decision boundaries \cite{finlay2019logbarrier}, we set SPL’s final optimization objective within the ``Pass'' region but close to the boundary. This positioning reduces the black-box defense system’s confidence, increasing the likelihood of generating target harmful images while ensuring the images can still be returned. Below, we detail the second stage of our SPL scheme: optimizing toward the boundary.



\vspace{2mm}
\noindent\textbf{(b) Optimizing towards the boundary}


In the second stage of SPL, the model is further refined to approach the boundary while staying within the ``Pass'' region. As illustrated in Fig. \ref{fig:spl} (b), the radius $r$ is progressively reduced before each iteration to align with the decreasing step size required for boundary proximity. This ensures the model avoids crossing into the ``Deny'' region.


Specifically, before each calculation of the gradient, we decrease the radius $r$ into a smaller one. Then we use the new smaller radius to calculate the SPL gradient $\pmb{g}_\mathrm{spl}$ via Eq. \ref{eq:grad}. After obtaining the gradient $\pmb{g}_\mathrm{spl}$, we optimize the model $\psi$ as follows: 
% ======================== Eq.4
\begin{equation}
\vspace{-0mm}
\psi = \psi - \alpha \cdot \pmb{g}_\mathrm{spl}(\psi),
\label{eq:update_}
\end{equation}
% ======================== Eq.4
Here, we utilize the negative sign ``$-$'', in contrast to Eq. \ref{eq:update}, to signify a reversed optimization direction (towards the boundary).
We alternate between Eq. \ref{eq:grad} and Eq. \ref{eq:update_}, iteratively approaching the boundary. 
During the iterative optimization, we gradually decrease the radius, as shown in Fig. \ref{fig:spl} (b).
Next, we describe how to decrease the radius in the second stage of SPL.


Given the original radius $r$ used in the first stage, the decreased radius used for the $i$-th iteration of this second stage is calculated as:
% ======================== Eq.5
\begin{equation}
\vspace{-0mm}
r_i = r\cdot e^{-\lambda i},
\label{eq:update_r}
\end{equation}
% ======================== Eq.5
where $i$ is iteration index number $i \in \{1, 2, 3, ...\}$ for the second stage of SPL, $\lambda$ is the coefficient of decay. As $i$ increases, $r_i$ gradually decreases and infinitely approaches zero.
To ensure effective boundary approaching, for each decreased radius $r_i$, we need to check if the following conditions are met:
(1) The sphere with this decreased radius $r_i$ encompasses both ``Pass'' and ``Deny'' points.
(2) After optimization with this decreased radius $r_i$, the model parameters are still located in the ``Pass'' case.
If the current $r_i$ does not satisfy the first condition, it indicates that $r_i$ is too small to probe both points. We set the current $i$ to $i-1$ to reuse the larger radius from the previous iteration to solve this issue. If the current $r_i$ does not satisfy the second condition, it indicates that $r_i$ is too large. We set the current $i$ to $i+1$ to use a smaller radius to address this issue.
Finally, we stop the SPL scheme when $r_i$ is %less 
smaller than a pre-defined threshold $r_{min}$.



After pre-training with the SPL scheme, the attack model $H$ learns to bypass black-box defenses, ensuring the API returns images $\mathbf{I}^{\ast}$. In the fine-tuning phase, $H$ is further refined to align the semantics of returned images 
$\mathbf{I}^{\ast}$ with harmful targets $\mathbf{I}$.





%---------------   3.2  --------------------
\vspace{-2mm}
\subsection{Fine-tuning with SEL}\label{sec:sel}



After pre-training with SPL, the attack model $H$ can induce the T2I API to return images  $\mathbf{I}^{\ast}$ to the attacker. 
To ensure the harmful semantics of returned images $\mathbf{I}^{\ast}$, an effective way involves calculating semantic-similarity losses between returned images $\mathbf{I}^{\ast}$ and harmful targets $\mathbf{I}$ to obtain gradients for model fine-tuning.
To avoid pulling the model back into the ``Deny'' case, we propose a Semantic-Enhancing Learning (SEL) scheme to adjust the calculated gradients for fine-tuning the model $H$ (i.e., LoRA parameters) while maintaining the progress achieved by SPL.


Specifically, in our SEL, we propose a Multi-Modal Alignment Loss $\mathcal{L}_{MMA}$ to achieve a comprehensive semantic alignment.
Motivated by the multi-modal semantic understanding capability of CLIP \cite{radford2021learning}, we utilize it to help calculating the semantic alignment loss $\mathcal{L}_{MMA}$ in two perspectives:



(1) Text-Image Alignment, i.e., encoding both the returned image $\mathbf{I}^{\ast}$ and the naive prompt $\mathbf{T}$, then computing the cosine similarity between the encoded vectors, which formulates the loss $\mathcal{L}_{TIA}$:
% ======================== Eq.6
\begin{equation}
\vspace{-0mm}
\mathcal{L}_{TIA} = 1- \frac{E_{img}(\mathbf{I}^{\ast})\cdot E_{text}(\mathbf{T})}{ \left \| E_{img}(\mathbf{I}^{\ast}) \right \| \left \| E_{text}(\mathbf{T}) \right \|  },
\label{eq:l_1}
\end{equation}
% ======================== Eq.6
where $E_{img}(\cdot)$ and $E_{text}(\cdot)$ denote the image encoder and text encoder of CLIP, respectively.

(2) Image-Image Alignment, i.e., encoding both the returned image $\mathbf{I}^{\ast}$ and the target image $\mathbf{I}$ for computing the cosine similarity, which formulates the loss $\mathcal{L}_{IIA}$:
% ======================== Eq.7
\begin{equation}
\vspace{-0mm}
\mathcal{L}_{IIA} = 1- \frac{E_{img}(\mathbf{I}^{\ast})\cdot E_{img}(\mathbf{I})}{ \left \| E_{img}(\mathbf{I}^{\ast}) \right \| \left \| E_{img}(\mathbf{I}) \right \|  }.
\label{eq:l_2}
\end{equation}
% ======================== Eq.7
Based on the above two similarity loss functions, we can formulate our Multi-Modal Alignment Loss $\mathcal{L}_{MMA}$ as:
% ======================== Eq.8
\begin{equation}
\vspace{-0mm}
\mathcal{L}_{MMA} = \mathcal{L}_{TIA} + \mathcal{L}_{IIA}.
\label{eq:lvs}
\end{equation}
% ======================== Eq.8
By minimizing $\mathcal{L}_{MMA}$, we can enhance the target (harmful) semantics of the returned images $\mathbf{I}^{\ast}$.
To this end, we use the calculated $\mathcal{L}_{MMA}$ to obtain gradients and then optimize the LoRA parameters $\psi$.
Since the T2I model is packed into the black box, we cannot directly obtain the oracle precise gradients.
To address this black-box setting, drawing inspiration from \cite{spall1992multivariate,spall1997one}, we adopt zeroth-order optimization to estimate the gradient without accessing the model architecture and model parameters. 
Given the training loss $\mathcal{L}_{MMA}$, the estimated gradient can be formulated as follows:
% ======================== Eq.9
\begin{equation}
\vspace{-0mm}
\pmb{g}(\psi)=\frac{\mathcal{L}_{MMA}(\psi +c\cdot\Delta)-\mathcal{L}_{MMA}(\psi-c\cdot\Delta) }{2c\cdot\Delta},
\label{eq:grad_}
\end{equation}
% ======================== Eq.9
where $c \in (0,1]$ is the scaling coefficient and $\Delta \in \mathbb{R}^d$ is a random perturbation vector, sampled from mean-zero distributions while satisfying the finite inverse momentum condition \cite{spall1992multivariate,nowak2007introduction}.


Although the standard form of zeroth-order optimization is generally effective, it can still encounter poor convergence in practical applications \cite{spall2000adaptive}. This problem, as discussed in \cite{spall1997accelerated}, is primarily attributed to the stochastic nature of gradient estimation, stemming from the random directions of perturbations. 
To overcome this challenge, we draw inspiration from Nesterov's accelerated gradient \cite{nesterov1983method} and improve the gradient calculation rule (Eq. \ref{eq:grad_}). The improved gradient $\pmb{g}_\mathrm{im}$ is defined as follows:
% ======================== Eq.10
\begin{equation}
\vspace{-0mm}
\pmb{g}_\mathrm{im}(\psi) =  \pmb{\bar{g}}_\mathrm{im} + \beta \cdot\pmb{g}(\psi+\pmb{\bar{g}}_\mathrm{im}).
\label{eq:new_gradient}
\end{equation}
% ======================== Eq.10
This formula consists of two gradient terms. The first term, $\pmb{\bar{g}}_\mathrm{im}$, represents the gradient $\pmb{g}_\mathrm{im}$ used for model update in the (previous) last iteration. The second term, $\beta \cdot\pmb{g}(\psi+\pmb{\bar{g}}_\mathrm{im})$, represents the newly estimated gradient in this iteration. In the second term, $\psi+\pmb{\bar{g}}_\mathrm{im}$ is the model parameters after updating along the previous gradient, $\pmb{g}(\cdot)$ represents the gradient calculation formula (Eq. \ref{eq:grad_}), and $\beta$ is the learning rate. 
Inspired by studies in the field of accelerated gradient optimization \cite{spall1997accelerated}, we incorporate the previous gradient (i.e., the first term) into the optimization of current iteration (i.e., the second term) to increase the gradient consistency and reduce incidental randomness in black-box optimization, thus making the training more stable and effective. 




% ==============================Figure 5
\begin{figure*}[t]
\centering
\includegraphics[width=0.85\textwidth]{images/SEL_new.pdf} 
\vspace{-1mm}
\caption{Intuitive illustration of our SEL scheme. \ding{172} Firstly, We compute the gradient $\pmb{g}_\mathrm{im}$ for enhancing target harmful semantics. \ding{173} Then, we calculate negative SPL gradient $-\pmb{g}_\mathrm{spl}$, which points towards the boundary direction. \ding{174} We calculate the component of the gradient $\pmb{g}_\mathrm{im}$ in the boundary direction, i.e., $\pmb{g}\prime_\mathrm{im}$. \ding{175} To prevent from falling into the ``deny'' region, we let $\pmb{g}_\mathrm{im}$ subtract its component 
$\pmb{g}\prime_\mathrm{im}$, obtaining the final SEL gradient $\pmb{g}_\mathrm{sel}$, where $\pmb{g}_\mathrm{sel} = \pmb{g}_\mathrm{im} - \pmb{g}\prime_\mathrm{im}$.
}
\vspace{-2mm}
\label{fig:sel}
\end{figure*}
% ==============================Figure 5


Recall that in Sec. \ref{sec:spl}, the SPL optimization objective is set within the ``Pass'' region near the boundary. However, directly optimizing the model using gradients $\pmb{g}_\mathrm{im}$ (Eq. \ref{eq:new_gradient}) risks pulling the parameters back into the ``Deny'' region, blocking image generation. Thus, the SEL optimization objective focuses on keeping the model within the ``Pass'' region while maximizing semantic alignment between the returned images and the harmful targets.



To prevent SEL from undermining SPL's learning achievements, we propose a gradient harmonization method to ensure compatibility between SEL and SPL.
As shown in Fig. \ref{fig:sel} \ding{172}, we first obtain the gradient $\pmb{g}_\mathrm{im}$ for harmful-semantic enhancement via Eq. \ref{eq:new_gradient}. Then, the SPL gradient $\pmb{g}_\mathrm{spl}$ is calculated through Eq. \ref{eq:grad}. As shown in Fig. \ref{fig:sel} \ding{173}, we adopt the negative SPL gradient $-\pmb{g}_\mathrm{spl}$, where $-\pmb{g}_\mathrm{spl}$ signifies the direction towards crossing the boundary. 
Given the direction provided by $-\pmb{g}_\mathrm{spl}$, we can obtain $\pmb{g}\prime_\mathrm{im}$ (see Fig. \ref{fig:sel} \ding{174}), which is the boundary-directed component of the gradient $\pmb{g}_\mathrm{im}$. 
As shown in Fig. \ref{fig:sel} \ding{175}, we finally obtain the SEL gradient $\pmb{g}_\mathrm{sel}$ by eliminating the component $\pmb{g}\prime_\mathrm{im}$ from $\pmb{g}_\mathrm{im}$, which can be formulated as:
% ======================== Eq.11
\begin{equation}
\vspace{-0mm}
\begin{split}
\pmb{g}_\mathrm{sel}(\psi) &= \pmb{g}_\mathrm{im} (\psi) - \pmb{g}\prime_\mathrm{im} (\psi)\\
&= \pmb{g}_\mathrm{im} (\psi) - \frac{\pmb{g}_\mathrm{spl} (\psi) \cdot \pmb{g}_\mathrm{im} (\psi)} {\left |  \pmb{g}_{spl} (\psi) \right |^{2} } \pmb{g}_\mathrm{spl} (\psi),
\end{split}
\label{eq:sel_gradient}
\end{equation}
% ======================== Eq.11
After obtaining SEL gradient $\pmb{g}_\mathrm{sel}$, we update the LoRA parameters as:
% ======================== Eq.12
\begin{equation}
\vspace{-0mm}
\psi = \psi - \beta \cdot\pmb{g}_\mathrm{sel}(\psi).
\label{eq:final_update}
\end{equation}
% ======================== Eq.12
We alternate between Eq. \ref{eq:sel_gradient} and Eq. \ref{eq:final_update}, iteratively enhancing the target harmful semantics of returned images. In this way, we can avoid the SEL's optimization crossing the boundary, thereby not only aligning towards the target semantics but also maintaining within the ``Pass'' region.








%---------------   3.3  --------------------
\subsection{Training-free Enhancement with INE}\label{sec:llm}

After the above two training phases, the attack model can effectively and efficiently generate adversarial prompts to deceive the T2I API. In this section, we introduce an In-context Naturalness Enhancement (INE) scheme, which further improves prompt naturalness in a training-free manner, complementing the LLM+LoRA structure.

In particular, inspired by the in-context learning \cite{dong2022survey} ability of LLM, we carefully choose a set of natural and effective adversarial prompts, treating them as in-context examples to input into the LLM. 
The goal is to guide the LLM to generate prompts in a natural manner.
The in-context examples are obtained through the following steps: 
(1) After the above two training stages, we feed all training samples into the model $H$ to obtain corresponding adversarial prompts and returned images. 
(2) Then, we use the loss $\mathcal{L}_{MMA}$ (Eqn. \ref{eq:lvs}) to measure the semantics of all returned images.
(3) We rank the adversarial prompts in ascending order based on their corresponding loss values.
(4) Following the ranking, we manually select $M$ adversarial prompts, $\{ \mathbf{T}^{\ast}_1, \  \mathbf{T}^{\ast}_2, \  ...,  \  \mathbf{T}^{\ast}_M\}$ with satisfactory naturalness through human-perceptive evaluation. 
(5) For these selected $M$ adversarial prompts $\{ \mathbf{T}^{\ast}_1, \  \mathbf{T}^{\ast}_2, \  ...,  \  \mathbf{T}^{\ast}_M\}$, we combine them with their corresponding naive prompts $\{ \mathbf{T}_1, \  \mathbf{T}_2, \  ...,  \  \mathbf{T}_M\}$, using a form as: $\{ \mathbf{T}_1 \to \mathbf{T}^{\ast}_1, \  \mathbf{T}_2 \to \mathbf{T}^{\ast}_2, \  ...,  \  \mathbf{T}_M \to \mathbf{T}^{\ast}_M\}$. 
This form outlines the transformations of naive prompts into natural adversarial prompts. 
We treat these $M$ transformation examples as in-context examples, denoted as $\mathbf{U}$.


Next, we use $\mathbf{U}$ to enhance our model $H$ without the need of any additional training.
After enhancement, the enhanced model $H$ can be directly used for inference.
Specifically, given an test-time naive prompt $\mathbf{T}_{test}$, we use $\mathbf{U}$ to enhance our our model $H$ by simply concatenating $\mathbf{U}$ with the input naive prompt $\mathbf{T}_{test}$:
% ======================== Eq.13
\begin{equation}
\vspace{-1mm}
\mathbf{T}_{test}^{\ast} = f_{\mathrm{H}}([\mathbf{U}, \mathbf{T}_{test}]),
\label{eq:h}
\end{equation}
% ======================== Eq.13
where $[\cdot]$ denotes the concatenation, $f_{\mathrm{H}}(\cdot)$ denotes attack model $H$, the test sample $\mathbf{T}_{test}$ is changeable, while $\mathbf{U}$ is fixed.
Based on these $M$ transformation examples (i.e., $\mathbf{U}$), given a naive prompt $\mathbf{T}_{test}$, UPAM can more naturally convert $\mathbf{T}_{test}$ into the adversarial prompt $\mathbf{T}_{test}^{\ast}$.
Note that in this paper, the number of in-context examples $M$ is set to 3, so the human labor cost is almost negligible.








%---------------   3.4  --------------------
\subsection{Few-shot Query Attack with TAL}\label{sec:TAL}


From Sec. \ref{sec:spl} to \ref{sec:llm}, we introduce our UPAM framework, which requires queries to the T2I API to train the attack model $H$. However, in reality, many queries are likely to be detected by the defender. To address this issue, we propose Transferable Attack Learning (TAL) to enable our attack model to require only few-shot queries to the target T2I API.


Instead of directly querying the online T2I API, in the TAL scheme, we first train the attack model $H$ using the offline T2I model(s). Then, the attack model $H$ is connected to the online T2I API and fine-tuned with few-shot queries. The key of TAL is that the offline-trained attack model $H$ should be transferable for it to fit the unseen T2I model with only few-shot queries. 
Overall, given a group of offline T2I models with different model structures, we split all these offline T2I models into training and validation sets, where the transferable attack model $H$ is trained only on the training set.
We use the validation set to control the training process for attack model $H$ that suffices to deceive the validation-set model. This is achieved through an early-stopping mechanism to avoid overfitting to the training-set models.



Algorithm \ref{alg_1} outlines our TAL scheme.
TAL comprises two phases: \textit{offline learning} and \textit{online learning}. The offline learning phase inputs with an attack model $H$ (with parameters $\psi$), a dataset $\mathcal{D}$, a training set of offline T2I models denoted as $p$, and a validation set of offline T2I models denoted as $q$. Offline learning proceeds iteratively, where in each iteration, the paired training data $\{\mathbf{T}, \mathbf{I}\}$ is sampled from the dataset $\mathcal{D}$. In each iteration, we calculate the SPL gradient $\pmb{g}_\mathrm{spl}$ and SEL gradient $\pmb{g}_\mathrm{sel}$ on training models $p$, and use these gradients to update the parameters $\psi$ (i.e., model $H$). The entire offline learning process is monitored by the validation models $q$. Specifically, if the performance (loss value) of the validation models $q$ is not in lowering trend, the offline training process is terminated, and the model parameters $\psi$ are returned. 
For the training (or validation) set consisting of multiple models, we use their average ensemble \cite{liu2016delving}.
During training, the validation loss may briefly increase, but it might still have the potential to decrease further. To avoid premature termination of the algorithm, we set a patience threshold to allow for some extent increase in the validation loss.
Since the T2I models used in the training and validation sets have different structures, the attack model $H$ trained through this process exhibits good transferability to new T2I models. 



The online learning phase inputs with an offline-trained attack model $H$, a dataset $\mathcal{D}$, and an online T2I API. We only perform few-shot queries (i.e., $W$ iterations, where $W$ is small) to the T2I API. We observe that the offline-trained attack model is typically capable to deceive the T2I API to return image results. Therefore, in the online learning phase, we omit the calculation of the SPL gradient and only compute the SEL gradient to update the model parameters $\psi$. Due to the lack of SPL gradient, we simplify the calculation of the SEL gradient to $g_{\text{im}}$ (Eq. \ref{eq:new_gradient}). After $W$ iterations, we conduct the INE process in Sec. \ref{sec:llm} to enhance the naturalness of model $H$ in a training-free manner. Finally, we use the naturalness-enhanced attack model $H$ for inference.



% ======================== Algorithm.1
\begin{algorithm}[ht]
%\SetAlgoNoLine
        \caption{Transferable Attack Learning (TAL)}\label{alg_1}
        $//\ offline \ learning \ phase$\\
        \KwIn{Attack model $\psi$, dataset $\mathcal{D}$, offline (training) T2I models $p$, offline (validation) T2I models $q$, patience threshold $\epsilon$;} 
        \vspace{1mm}
        $\mathcal{L}_{min} =$ inf; \ // $'inf'\ stands\ for\ positive\ infinity$\\
        \For{$ \{\mathbf{T}, \mathbf{I}\} \in \mathcal{D} $}{
        \vspace{0.5mm}
            Compute SPL gradient using $p$: \ $\pmb{g}_\mathrm{spl}$; \ // \ $Eq. \ref{eq:grad}$\\
            Compute SEL gradient using $p$: \ $\pmb{g}_\mathrm{sel}$; \ // \ $Eq. \ref{eq:sel_gradient}$\\
            \uIf{model $\psi$ in the ``Deny'' region}{
            \vspace{0.5mm}
            $\psi$ $\leftarrow$ Update($\psi$, $\alpha$, $\pmb{g}_\mathrm{spl}$); \ // \ $Eq. \ref{eq:update}$}
            \Else{$\psi$ $\leftarrow$ Update($\psi$, $\beta$, $\pmb{g}_\mathrm{sel}$); \ // \ $Eq. \ref{eq:final_update}$}
            Compute validation loss using $q$: \ $\mathcal{L}_{val}$; \ // \ $Eq. \ref{eq:lvs}$\\
            \uIf{$\mathcal{L}_{val} < \mathcal{L}_{min}$}{
            \vspace{0.5mm}
            $\mathcal{L}_{min}$ $\leftarrow$ $\mathcal{L}_{val}$;\\}
            \uElseIf{$\mathcal{L}_{val} > \mathcal{L}_{min} + \epsilon$}{
            \vspace{0.5mm}
            \Return model $\psi$; \ // \ $early\ stopping$\\}
        }
        
        \vspace{2mm}
        $//\ online \ learning \ phase$\\
        \KwIn{offline-trained model $\psi$, dataset $\mathcal{D}$, online T2I API, few-shot iteration number $W$;} 
        \vspace{1mm}
        $Counter = 0$;\\
        \For{$ \{\mathbf{T}, \mathbf{I}\} \in \mathcal{D} $}{
        \vspace{0.5mm}
            Compute gradient using API: \ $g_{\mathrm{im}}$; \ // \ $Eq. \ref{eq:new_gradient}$\\
            $\psi$ $\leftarrow$ Update($\psi$, $\beta$, $\pmb{g}_\mathrm{im}$); \ // \ $Eq. \ref{eq:final_update}$
            $Counter = Counter+1$;\\
            \If{$Counter \ge W$}{
            \vspace{0.5mm}
            \Return model $\psi$; \ // \ $few-shot\ queries$\\}
        }
\end{algorithm} 
\vspace{-4mm}
% ======================== Algorithm.1









%---------------   3.5  --------------------
\subsection{Training and Testing}\label{sec:train_test}


In summary, our UPAM framework can choose to work \textit{with} or \textit{without} TAL based on the requirement for few-shot queries.


As for UPAM \textit{without} the need of few-shot queries (i.e., w/o TAL), we train the attack model $H$ in two phases: pre-training and fine-tuning.
In the pre-training phase, we use the SPL scheme to obtain the gradient $\pmb{g}_\mathrm{spl}$ (Eq. \ref{eq:grad}) and update the model parameters (Eq. \ref{eq:update}). 
This update enables effective optimization under the challenging scenario where no image results are returned, finally compelling the API to reliably return images.
Then, in the fine-tuning phase, we adopt the SEL scheme to obtain the gradient $\pmb{g}_\mathrm{sel}$ (Eq. \ref{eq:sel_gradient}) and update the model parameters (Eq. \ref{eq:final_update}). 
This update ensures that the returned images can accurately show target-aligned semantics.
During the whole training process, we only optimize LoRA parameters, while keeping the LLM frozen.
After training and before testing, we use the INE scheme mentioned in Sec. \ref{sec:llm} to gather in-context examples for naturalness-enhanced UPAM (Eq. \ref{eq:h}). We finally handle the inference (testing) based on the enhanced UPAM.


As for UPAM \textit{with} the need of few-shot queries (i.e., with TAL), we train the attack model $H$ in two phases: offline learning and online learning.
In the offline learning phase, we calculate the SPL gradient $\pmb{g}_\mathrm{spl}$ and SEL gradient $\pmb{g}_\mathrm{sel}$ to update the attack model in a T2I-transferable manner, using a set of training and validation T2I models.
In the online learning phase, we calculate the gradient $g_{\text{im}}$ to further update the offline-trained model for only $W$ iterations. We use the INE scheme (Sec. \ref{sec:llm}) to enhance the attack model's naturalness and then conduct inference (testing).














%==================  4 Experiments =====================
\section{Experiments}\label{sec:experiment}


\textbf{Textual Filter and Visual Checker.}
For the textual filter, we follow previous work \cite{liu2023riatig} employing the same filter model, which operates by utilizing a predefined list of sensitive words, also known as a blacklist, to deny the forward propagation of harmful prompts.
For the visual checker, we follow \cite{rando2022red} to adopt the widely-used visual checking model: using the off-the-shelf CLIP visual encoder to map the generated image to a latent vector, which is then compared with all default embeddings of predefined harmful images using cosine distance. If any cosine value exceeds the manually specified threshold, the further propagation of the generated image is denied.



\textbf{Dataset.}
We follow previous works \cite{liu2023riatig,daras2022discovering,milliere2022adversarial,struppek2022biased,jin2020bert} to deploy defenses and conduct attacks using the paired text-image data from the \underline{Microsoft COCO dataset} \cite{lin2014microsoft}.
In consideration of ethics, privacy, legal compliance, and potential societal impacts, these previous works treat 10 classes from COCO as ``harmful'' classes (i.e., \textit{boat}, \textit{bird}, \textit{clock}, \textit{kite}, \textit{wine glass}, \textit{knife}, \textit{pizza}, \textit{teddy bear}, \textit{vase} and \textit{laptop}). 
Specifically, we train our UPAM using the training set of these 10 classes, and then test UPAM using the test set of the same 10 classes. 
Although the training and testing data contain the same classes, the specific text descriptions and their corresponding image contents are different.
To achieve a comprehensive evaluation, we also follow the previous work \cite{yang2024sneakyprompt} to conduct experiments on the \underline{NSFW dataset} \cite{yang2024sneakyprompt}, which contains truly harmful content, including 6 harmful classes: (i.e., \textit{breasts}, \textit{nipples}, \textit{genitalia}, \textit{nudity}, \textit{sexual acts}, and \textit{bodily fluids})



\textbf{Target T2I APIs.}
Following previous work \cite{liu2023riatig}, we conduct experiments on three large-scale API T2I models: DALL·E \cite{yu2022scaling}, DALL·E 2 \cite{ramesh2022hierarchical}, and Imagen \cite{saharia2022photorealistic}. 
For each of them, we follow \cite{liu2023riatig} to adopt the corresponding released model that achieves comparable performance to API, and treat it as a black box. 



\textbf{Implementation Details.}
In this paper, we use the open-source LLaMA \cite{touvron2023llama} as the LLM. 
In the TAL scheme, we adopt DeepFloyd IF \cite{DeepFloydIF}, StableStudio \cite{stablestudio}, Invoke \cite{InvokeAI}, and Stable Diffusion \cite{rombach2022high} as our training T2I models, while utilizing Dreamlike Photoreal \cite{Dreamlike}, Waifu Diffusion \cite{waifu}, and OpenJourney \cite{openjourney} as our validation T2I models. All these models are offline and open-source.
As for the LoRA adapter \cite{hu2021lora}, we adopt the adaptation matrices with a rank value of 8.
In our SPL scheme, we set $N=10$, $\alpha=r/4$.
The radius $r$ increases from 0 and then decreases to 0.2 (i.e., $r_{min}=0.2$), following the exponential function with a base value of 1.3 (increase) and $-$0.6 (decrease).
In our SEL scheme, we set $\beta = 0.3$. In our TAL scheme, we set $\epsilon = 0.1$.





%---------------   4.1  --------------------
\subsection{Experimental Protocols}\label{protocol}

In this paper, we conduct experiments under two protocols for a comprehensive study.


\textbf{Protocol A:} Given the training data $\{ \mathbf{T}, \mathbf{I} \}$, we train a parameterized model for converting the naive prompts $\mathbf{T}$ into adversarial ones $\mathbf{T}^{\ast}$ with the objective of compelling the black-box T2I model to return the target images $\mathbf{I}$.
Subsequently, given the testing data $\mathbf{T}_t$, the trained model can swiftly convert $\mathbf{T}_t$ into $\mathbf{T}_t^{\ast}$, compelling the black-box T2I model to return the corresponding target images.



\textbf{Protocol B:} Previous enumeration-based methods do not require model training. Instead, they typically generate numerous adversarial prompt candidates and then attempt the candidates one by one until the returned image is semantic similar to the target image.
They conduct experiments on a subset of 10  ``harmful'' images randomly selected from the COCO dataset (or 200 images from the NSFW dataset), employing enumeration to find an adversarial prompt for each image.
To make a fair comparison with previous enumeration-based methods, we slightly modify our UPAM in this protocol. 
This modification enables UPAM to identify adversarial prompts for the 10 (or 200) images without learning on the training set.
The details of modification can be seen in Sec. \ref{modi_B}.






%---------------   4.2  --------------------
\vspace{-2mm}
\subsection{Evaluation Metrics.}\label{sec:evaluation}
\vspace{-0.5mm}



We evaluate attack methods from the following perspectives:


\textbf{Attack Effectiveness.}
Following previous work \cite{liu2023riatig}, we measure the attack effectiveness by evaluating the generated adversarial prompts $\mathbf{T}^{\ast}$ and the returned images $\mathbf{I}^{\ast}$, respectively.
(1) As for the returned images $\mathbf{I}^{\ast}$, we follow \cite{liu2023riatig} to utilize \uline{R-precision} to measure the attack effectiveness. 
Specifically, a retrieval experiment is conducted using the returned image to query against a set of candidate text descriptions. 
This set comprises 1 ground-truth description and 99 randomly chosen mismatched descriptions. 
For this purpose, a state-of-the-art image-text retrieval model \cite{kim2021vilt} is employed here. 
We collect the top $R$ ranked results in the retrieval, deeming our attacks successful if the ground truth description is among them. We follow \cite{liu2023riatig} to conduct evaluation at R-1 precision (i.e., $R=1$) and R-3 precision (i.e., $R=3$).
Note that if no image is returned to the attacker, the R-precision score of this sample is 0.
(2) As for the generated adversarial prompts $\mathbf{T}^{\ast}$, we use \uline{textual similarity} to evaluate the attack effectiveness.
The evaluation of textual similarity, based on how different the adversarial prompt $T^*$ is from the naive prompt $T$, is closely tied to the defense mechanism used in text-to-image (T2I) generation APIs.
As mentioned in previous work \cite{liu2023riatig,jin2020bert}, T2I APIs often incorporate textual filters as a defense mechanism, where these filters are designed to detect and block prompts that are textually similar to harmful naive prompts listed in a blacklist.
That is, if $T^*$, the adversarial prompt, is more similar to the harmful naive prompt $T$ in the blacklist, the textual filter is more likely to flag and reject the generated output.
Therefore, ensuring that the generated $T^*$ is sufficiently dissimilar from the naive harmful $T$ is critical for bypassing the textual filters. By maximizing this dissimilarity, one can improve the likelihood of the adversarial prompt evading detection, thus enhancing the effectiveness of the attack.
Following previous work \cite{liu2023riatig,jin2020bert}, we adopt the Universal Sentence Encoder \cite{cer2018universal} to encode the adversarial prompts $\mathbf{T}^{\ast}$ and naive prompts $\mathbf{T}$ into high dimensional vectors, and then compute their textual similarity score. 
The lower the textual similarity, the better the attack.


\textbf{Attack Efficiency.}
We also evaluate the attack efficiency of the methods. 
Specifically, we present the \uline{average inference time} taken for each testing sample during testing.
Due to the uncertain inference time of enumeration methods, in order to avoid endless waiting periods, we set a criterion: if the inference time exceeds 10 minutes for a given sample, the attack is considered unsuccessful, and the inference process is stopped.



\textbf{Attack Naturalness.} 
To evaluate the attack stealthiness, we follow \cite{liu2023riatig} to evaluate the naturalness of the generated adversarial prompts by computing the \uline{perplexity score (PPL)} using GPT-2 \cite{radford2019language}, a model trained on extensive real-world sentences. 
Generally, the prompt sample with a lower PPL is more natural.


\textbf{Query Times.} 
We record the number of queries each method makes to the T2I API, and report the average number per sample. Besides inference (testing), the method proposed in this paper also queries the T2I API during training. Therefore, we report both the \uline{training query times} and \uline{inference query times} for a comprehensive comparison.





% ==============================Tab.1
\begin{table*}[th]
\centering
\caption{Experimental results on COCO (left) and NSFW (right) under protocol A.}
\vspace{-2mm}
\label{tab:exp_whole}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{cc|ccccccc|ccccccc}
\toprule[0.15em]
                            &                              & \multicolumn{7}{c|}{\textbf{COCO}}                                                                                                                                                             & \multicolumn{7}{c}{\textbf{NSFW}}                                                                                                                                                             \\ 
\cline{3-16} 
\specialrule{0em}{1pt}{1pt}
\multirow{-2}{*}{T2I Model} & \multirow{-2}{*}{Methods}    & R-1 Precision ↑                          & R-3 Precision ↑                          & Text. Sim. ↓                 & Infer. Time (s) ↓                     & PPL ↓         &Train. Query ↓     &Infer. Query ↓                          & R-1 Precision ↑                         & R-3 Precision ↑                        & Text. Sim. ↓                 & Infer. Time (s) ↓                       & PPL ↓            &Train. Query ↓     &Infer. Query ↓        \\ 
\midrule[0.15em]
                            & TextFooler \cite{jin2020bert}   & 0.48\%                                   & 0.98\%                                   & 0.22                         & 589.29                                & 3463.02          & 0    &  1018             &  0.31\%                                    & 0.67\%                                    & 0.20                          & 592.65                                   & 4165.37          &    0        &    1142      \\
                            & HomoSubs \cite{struppek2022biased}  & 0.74\%                                   & 1.12\%                                   & 0.28                         & 562.48                                & 3089.58             &     0       &  769      &  0.67\%                                    &  1.03\%                                  &   0.22                        & 583.73                                   &  3671.58         &    0        &   802         \\
                            & EvoPromp \cite{milliere2022adversarial}  & 0.82\%                                   & 1.37\%                                   & 0.19                         & 579.20                                & 4984.24           &  0   &      734                   &   1.16\%                                   & 1.52\%                               &  0.25                     &  528.54                  &  4316.65            &  0   &       727                \\
                            & HiddVocab \cite{daras2022discovering}          & 3.35\%                                   & 4.21\%                                   & 0.33                         & 457.34                                & 4027.64             & 0    &   742                    &   4.12\%               &  6.75\%                                 &  0.40                 &  561.59                             &  4593.42          &  0   &    688           \\
                            & MacPromp \cite{milliere2022adversarial}      & 5.05\%                                   & 6.80\%                                   & 0.57                         & 452.16                                & 3163.07              &  0   &   561                   &  5.65\%          &   6.97\%                           &   0.48                  & 502.63                            &   2884.06             &  0   &  616               \\
                            & MMA-Diff  \cite{yang2023mma}   & 8.27\%                                   & 9.16\%                                   & 0.34                         & 386.77                                & 3857.29             & 0    &  493                     &   6.73\%                                 & 7.46\%                                 & 0.32                         & 398.13                                   &   3746.08              &  0   &    514                  \\
                            & RIATIG  \cite{liu2023riatig}   & 8.65\%                                   & 9.95\%                                   & 0.18                         & 391.52                                & 1003.27               &  0   &   379                  &   6.55\%                                 & 7.39\%                                 & 0.21                         & 409.72                                   &   1474.30               & 0    & 392                    \\
                            & SneakyPrompt \cite{yang2024sneakyprompt} & 12.68\%                                  & 16.79\%                                   & 0.25                         & 561.75                                & 1457.36                &  0   &  453                  &    9.15\%                               &  12.13\%                                &   0.29                       &   498.38                                &  1574.28           &   0  &   429                      \\
                            & \cellcolor[HTML]{EFEFEF}UPAM (with TAL) & \cellcolor[HTML]{EFEFEF}23.40\% & \cellcolor[HTML]{EFEFEF}25.89\% & \cellcolor[HTML]{EFEFEF}0.21 & \cellcolor[HTML]{EFEFEF}5.14 & \cellcolor[HTML]{EFEFEF}658.18  & \cellcolor[HTML]{EFEFEF} 10      & \cellcolor[HTML]{EFEFEF}  \textbf{1}    & \cellcolor[HTML]{EFEFEF}17.69\% & \cellcolor[HTML]{EFEFEF}20.72\% & \cellcolor[HTML]{EFEFEF}0.23  & \cellcolor[HTML]{EFEFEF}5.62 & \cellcolor[HTML]{EFEFEF}933.76 & \cellcolor[HTML]{EFEFEF}  10     & \cellcolor[HTML]{EFEFEF}  \textbf{1} \\
\multirow{-7}{*}{DALL·E \cite{yu2022scaling}}    & \cellcolor[HTML]{EFEFEF}UPAM (w/o TAL) & \cellcolor[HTML]{EFEFEF}\textbf{38.56\%} & \cellcolor[HTML]{EFEFEF}\textbf{41.92\%} & \cellcolor[HTML]{EFEFEF}\textbf{0.17} & \cellcolor[HTML]{EFEFEF}\textbf{5.11} & \cellcolor[HTML]{EFEFEF}\textbf{641.26}  & \cellcolor[HTML]{EFEFEF}  173     & \cellcolor[HTML]{EFEFEF}   \textbf{1}   & \cellcolor[HTML]{EFEFEF}\textbf{35.82\%} & \cellcolor[HTML]{EFEFEF}\textbf{39.11\%} & \cellcolor[HTML]{EFEFEF}\textbf{0.19}  & \cellcolor[HTML]{EFEFEF}\textbf{5.45} & \cellcolor[HTML]{EFEFEF}\textbf{919.57} & \cellcolor[HTML]{EFEFEF}    185   & \cellcolor[HTML]{EFEFEF} \textbf{1}  \\ \hline
                            & TextFooler \cite{jin2020bert}   & 0.85\%                                   & 0.93\%                                   & 0.18                         & 540.77                                & 3566.32            &  0   & 1009                       &  0.68\%                                   &  0.92\%                                   &    0.25                       & 593.51                                  &   4398.65             &  0   & 1157                     \\
                            & HomoSubs \cite{struppek2022biased} & 0.70\%                                   & 0.89\%                                   & 0.22                         & 568.26                                & 4125.22             & 0    &  791                     & 0.64\%                                    &  0.88\%                                   &  0.26                        & 583.19                                  & 4426.74           & 0    &      797                    \\
                            & EvoPromp \cite{milliere2022adversarial}  & 0.92\%                                   & 1.17\%                                   & 0.37                         & 475.81                                & 5837.06                  &  0   &  662                &   1.54\%                                  & 1.86\%                                    &  0.44                        &   454.78                                &   5396.20             & 0    &  650                    \\
                            & HiddVocab \cite{daras2022discovering}      & 3.21\%                                   & 4.62\%                                   & 0.46                         & 565.33                                & 4125.22             & 0    &  636                     &    3.65\%                               &   5.06\%                               &  0.49                       &  552.97                                 &   3956.72           & 0    &  611                    \\
                            & MacPromp \cite{milliere2022adversarial}     & 4.84\%                                   & 5.92\%                                   & 0.66                         & 523.49                                & 3056.41          &  0   &  702                        &  3.99\%                                 & 5.72\%                                 &   0.52                       &      511.59                             &  2737.15              &  0   &  685                   \\
                            & MMA-Diff  \cite{yang2023mma}  & 7.88\%                                   & 8.93\%                                   & 0.51                         & 361.71                                & 3246.16              &  0   &   467                    & 8.21\%                                  & 9.55\%                                 & 0.42                         & 457.52                                  & 2943.02          &  0   &   493                        \\
                            & RIATIG  \cite{liu2023riatig}  & 8.55\%                                   & 9.19\%                                   & 0.35                         & 353.53                                & 992.54                    & 0    &  354               & 7.66\%                                  & 9.21\%                                 & 0.33                         & 507.62                                  & 1163.71                 & 0    &  416                  \\
                            & SneakyPrompt \cite{yang2024sneakyprompt} & 14.77\%                                  & 18.20\%                                   & 0.31                         & 495.05                                & 1057.19                   & 0    & 406                &  12.68\%                                 &  16.79\%                                &   0.25                       &  561.75                                 &    1457.36                 &  0   &  449               \\
                            & \cellcolor[HTML]{EFEFEF}UPAM (with TAL) & \cellcolor[HTML]{EFEFEF}25.08\% & \cellcolor[HTML]{EFEFEF}28.31\% & \cellcolor[HTML]{EFEFEF}0.21 & \cellcolor[HTML]{EFEFEF}5.77 & \cellcolor[HTML]{EFEFEF}758.42  & \cellcolor[HTML]{EFEFEF} 10      & \cellcolor[HTML]{EFEFEF}  \textbf{1}    & \cellcolor[HTML]{EFEFEF}23.31\% & \cellcolor[HTML]{EFEFEF}24.86\% & \cellcolor[HTML]{EFEFEF}0.24  & \cellcolor[HTML]{EFEFEF}5.51 & \cellcolor[HTML]{EFEFEF}862.07 & \cellcolor[HTML]{EFEFEF}  10     & \cellcolor[HTML]{EFEFEF}  \textbf{1} \\
\multirow{-7}{*}{DALL·E 2 \cite{ramesh2022hierarchical}}  & \cellcolor[HTML]{EFEFEF}UPAM (w/o TAL) & \cellcolor[HTML]{EFEFEF}\textbf{40.71\%} & \cellcolor[HTML]{EFEFEF}\textbf{44.83\%} & \cellcolor[HTML]{EFEFEF}\textbf{0.18} & \cellcolor[HTML]{EFEFEF}\textbf{5.54} & \cellcolor[HTML]{EFEFEF}\textbf{717.69}   & \cellcolor[HTML]{EFEFEF} 168      & \cellcolor[HTML]{EFEFEF} \textbf{1} & \cellcolor[HTML]{EFEFEF}\textbf{42.57\%} & \cellcolor[HTML]{EFEFEF}\textbf{46.33\%} & \cellcolor[HTML]{EFEFEF}\textbf{0.20} & \cellcolor[HTML]{EFEFEF}\textbf{5.32}  & \cellcolor[HTML]{EFEFEF}\textbf{836.49} & \cellcolor[HTML]{EFEFEF}  181     & \cellcolor[HTML]{EFEFEF} \textbf{1} \\ \hline
                            & TextFooler \cite{jin2020bert}  & 0.32\%                                   & 0.54\%                                   & 0.15                         & 527.65                                & 3321.88             & 0    &  974                      &  0.17\%                                    &   0.51\%                                  &   0.19                        &  583.35                                   &    3746.09                &  0   &  1046                 \\
                            & HomoSubs \cite{struppek2022biased}  & 0.35\%                                   & 0.75\%                                   & 0.18                         & 567.82                                & 2850.41               &  0   &   789                   & 0.29\%                                     &  0.61\%                                   &  0.20                        & 588.31                                   &  3003.76              &  0   &  802                    \\
                            & EvoPromp \cite{milliere2022adversarial}  & 0.64\%                                   & 0.92\%                                   & 0.25                         & 398.51                                & 4055.71                      &  0   &   526            &    0.79\%                                &   1.05\%                                & 0.34                         & 476.32                                   & 4876.31              & 0    &  662                      \\
                            & HiddVocab \cite{daras2022discovering}           & 2.37\%                                   & 3.69\%                                   & 0.27                         & 592.47                                & 4153.58              &  0   &    686                   &      2.02\%                                &   2.96\%                                &  0.29                        &   562.38                                 &  3647.57           & 0    &    634                      \\
                            & MacPromp \cite{milliere2022adversarial}      & 4.58\%                                   & 5.52\%                                   & 0.16                         & 524.38                                & 3173.35             &  0   &   704                     &     5.77\%                               &   7.65\%                               &   0.22                        &   519.66                                &  2945.02           & 0    &  690                       \\
                            & MMA-Diff  \cite{yang2023mma}  & 5.87\%                                   & 8.30\%                                   & 0.41                         & 394.84                                & 2576.85               &  0   &  508                     &  9.57\%                                 &  11.54\%                                   &   0.37                       &   476.59                                &   3851.66            & 0    & 536                       \\
                            & RIATIG  \cite{liu2023riatig}  & 7.80\%                                   & 9.22\%                                   & 0.36                         & 388.16                                & 1076.29            & 0    &   387                       &  10.33\%                                 &  13.16\%                                   &   0.28                       &   422.64                                &   1473.49            &  0   &  402                      \\
                            & SneakyPrompt \cite{yang2024sneakyprompt} & 11.61\%                                  & 15.93\%                                   & 0.27                         & 427.43                                & 1028.42              &  0   &  344                     &   16.34\%                                &  18.07\%                                &  0.32                        &   404.53                                &   1342.76           & 0    &  326                       \\
                            & \cellcolor[HTML]{EFEFEF}UPAM (with TAL) & \cellcolor[HTML]{EFEFEF}19.84\% & \cellcolor[HTML]{EFEFEF}21.79\% & \cellcolor[HTML]{EFEFEF}0.16 & \cellcolor[HTML]{EFEFEF}5.38 & \cellcolor[HTML]{EFEFEF}529.43  & \cellcolor[HTML]{EFEFEF} 10      & \cellcolor[HTML]{EFEFEF}  \textbf{1}    & \cellcolor[HTML]{EFEFEF}18.55\% & \cellcolor[HTML]{EFEFEF}22.60\% & \cellcolor[HTML]{EFEFEF}0.19  & \cellcolor[HTML]{EFEFEF}5.57 & \cellcolor[HTML]{EFEFEF}859.78 & \cellcolor[HTML]{EFEFEF}  10     & \cellcolor[HTML]{EFEFEF}  \textbf{1} \\
\multirow{-7}{*}{Imagen \cite{saharia2022photorealistic}}    & \cellcolor[HTML]{EFEFEF}UPAM (w/o TAL) & \cellcolor[HTML]{EFEFEF}\textbf{36.63\%} & \cellcolor[HTML]{EFEFEF}\textbf{41.58\%} & \cellcolor[HTML]{EFEFEF}\textbf{0.14} & \cellcolor[HTML]{EFEFEF}\textbf{5.25} & \cellcolor[HTML]{EFEFEF}\textbf{503.76}  & \cellcolor[HTML]{EFEFEF} 172      & \cellcolor[HTML]{EFEFEF}  \textbf{1}  & \cellcolor[HTML]{EFEFEF}\textbf{40.16\%} & \cellcolor[HTML]{EFEFEF}\textbf{40.16\%} & \cellcolor[HTML]{EFEFEF}\textbf{0.18}  & \cellcolor[HTML]{EFEFEF}\textbf{5.42} & \cellcolor[HTML]{EFEFEF}\textbf{847.37} & \cellcolor[HTML]{EFEFEF}  182     & \cellcolor[HTML]{EFEFEF} \textbf{1} \\ 
\bottomrule[0.15em] 
\vspace{2mm}
\end{tabular}
}
\end{table*}
% ==============================Tab.1




% ==============================Tab.2
\begin{table*}[th]
\centering
\caption{Experimental results on COCO (left) and NSFW (right) under protocol B.}
\vspace{-2mm}
\label{tab:exp_harm}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{cc|ccccccc|ccccccc}
\toprule[0.15em]
                            &                              & \multicolumn{7}{c|}{\textbf{COCO}}                                                                                                                                                             & \multicolumn{7}{c}{\textbf{NSFW}}                                                                                                                                                             \\ 
\cline{3-16} 
\specialrule{0em}{1pt}{1pt}
\multirow{-2}{*}{T2I Model} & \multirow{-2}{*}{Methods}    & R-1 Precision ↑                          & R-3 Precision ↑                          & Text. Sim. ↓                 & Infer. Time (s) ↓                     & PPL ↓             &Train. Query ↓     &Infer. Query ↓                              & R-1 Precision ↑                         & R-3 Precision ↑                        & Text. Sim. ↓                 & Infer. Time (s) ↓                       & PPL ↓                 &Train. Query ↓     &Infer. Query ↓                          \\ 
\midrule[0.15em]
                            & TextFooler \cite{jin2020bert}   & 0/10                                    & 0/10                                   & 0.22                         & 582.37                                  & 3517.03          &  0       &   1121           &    0/200                                 &     0/200                               &   0.21                       &   594.15                               &    4276.43           &   0      &   1136                      \\
                            & HomoSubs \cite{struppek2022biased}  & 0/10                                    & 0/10                                   & 0.16                         & 590.28                                  & 3216.95    &   0      &  772       &        0/200                            &      0.4/200                              &     0.20                     &     586.96                            &  4116.32                &   0      &  805                              \\
                            & EvoPromp \cite{milliere2022adversarial} & 0.2/10                                    & 0.5/10                                 & 0.16                         & 586.47                                  & 4837.63    &  0       &  730     &         3.4/200                           &      4/200                                 & 0.33                          &  577.64                               &  4079.52                         &   0      &  718                             \\
                            & HiddVocab \cite{daras2022discovering}   & 0.3/10                                  & 0.4/10                                 & 0.18                         & 483.27                                  & 4607.93     &  0       &   748      &    10.4/200                                 &   12.4/200                                  &   0.41                        &    517.63                              &   4316.18          &   0      &  682                            \\
                            & MacPromp \cite{milliere2022adversarial}  & 0.7/10                                  & 0.9/10                                 & 0.39                         & 476.46                                  & 3239.56      & 0        &  573     &    14.6/200                            &   17.2/200                                  &   0.37                        &   571.30                               &  3542.85                       &   0      &  622                      \\
                            & MMA-Diff  \cite{yang2023mma}   & 0.6/10                                  & 1/10                                 & 0.30                         & 408.47                                  & 2473.13   &   0      &   502     &  25.4/200                                   &    26.2/200                         &   0.29                       &    486.27               &  3157.26                            &   0      &  518                    \\
                            & RIATIG  \cite{liu2023riatig}   & 0.8/10                                  & 1.1/10                                 & 0.34                         & 425.66                                  & 1026.81    &  0       &  375      &  27.8/200                                   &    29.6/200                         &   0.33                       &    520.79               &  1643.85                              &   0     &   453                 \\
                            & SneakyPrompt \cite{yang2024sneakyprompt} & 4.2/10                                  & 4.4/10                                   & 0.27                         & 496.79                                & 1078.33            &  0       &   418                        &  95.6/200                                &  98.4/200                                &  0.36                         &  553.49                                  &  1251.22      &  0       & 464                                 \\
                            & \cellcolor[HTML]{EFEFEF}UPAM (with TAL) & \cellcolor[HTML]{EFEFEF}5.5/10 & \cellcolor[HTML]{EFEFEF}5.9/10 & \cellcolor[HTML]{EFEFEF}0.17 & \cellcolor[HTML]{EFEFEF}\textbf{56.15} & \cellcolor[HTML]{EFEFEF}603.74  & \cellcolor[HTML]{EFEFEF}0      & \cellcolor[HTML]{EFEFEF}  \textbf{10}    & \cellcolor[HTML]{EFEFEF}113.2/200 & \cellcolor[HTML]{EFEFEF}117.7/200 & \cellcolor[HTML]{EFEFEF}0.20  & \cellcolor[HTML]{EFEFEF}\textbf{54.87} & \cellcolor[HTML]{EFEFEF}904.56 & \cellcolor[HTML]{EFEFEF}0     & \cellcolor[HTML]{EFEFEF}  \textbf{10} \\
\multirow{-7}{*}{DALL·E \cite{yu2022scaling}}    & \cellcolor[HTML]{EFEFEF} UPAM (w/o TAL)  & \cellcolor[HTML]{EFEFEF}\textbf{7.4/10} & \cellcolor[HTML]{EFEFEF}\textbf{8/10} & \cellcolor[HTML]{EFEFEF}\textbf{0.15} & \cellcolor[HTML]{EFEFEF}362.11 & \cellcolor[HTML]{EFEFEF}\textbf{580.62} & \cellcolor[HTML]{EFEFEF}0 & \cellcolor[HTML]{EFEFEF}  72    & \cellcolor[HTML]{EFEFEF}\textbf{136.8/200} & \cellcolor[HTML]{EFEFEF}\textbf{141.4/200} & \cellcolor[HTML]{EFEFEF}\textbf{0.16}  & \cellcolor[HTML]{EFEFEF}376.66 & \cellcolor[HTML]{EFEFEF}\textbf{899.43}  & \cellcolor[HTML]{EFEFEF}0  & \cellcolor[HTML]{EFEFEF} 69 \\ \hline
                            & TextFooler \cite{jin2020bert}  & 0/10                                    & 0/10                                   & 0.19                         & 543.72                                  & 3578.14      &   0      & 1013         &       0/200                              &      0/200                               & 0.22                          &    585.23                              &    4517.39                                     & 0        &   1153                               \\
                            & HomoSubs \cite{struppek2022biased} & 0/10                                    & 0/10                                   & 0.15                         & 589.46                                  & 4374.80     &  0       &   785        &     0/200                               &    0/200                                & 0.23                          &   573.64                               &     4286.74         &  0       &   792                                                        \\
                            & EvoPromp \cite{milliere2022adversarial}  & 0/10                                    & 0/10                                   & 0.21                         & 449.61                                  & 5267.07      &  0       &  659         &      1.6/200                               &       3.2/200                              &    0.38                       &    507.18                              &   5243.60                &   0      &   677                               \\
                            & HiddVocab \cite{daras2022discovering}    & 0.6/10                                  & 0.9/10                                 & 0.22                         & 576.27                                  & 4933.29        &  0       &  642        &    8.8/200                                &      10/200                              &       0.54                   &   563.02                              &    4227.55          &  0       &   633                                 \\
                            & MacPromp \cite{milliere2022adversarial}   & 0.4/10                                  & 0.8/10                                 & 0.30                         & 571.63                                  & 3575.26      &  0       &   716      &     9.8/200                                &       11.4/200                              &   0.49                        &  539.46                               &  2866.81        &  0       &   697                                \\
                            & MMA-Diff  \cite{yang2023mma} & 1/10                                  & 1.4/10                                 & 0.25                         & 413.40          & 2951.67       &  0       &    482     &  24.2/200         &   25/200                &  0.28                        &  492.29                                &  3049.25         &  0       &    533                           \\
                            & RIATIG  \cite{liu2023riatig} & 0.5/10                                  & 0.7/10                                 & 0.29                         & 417.34          & 965.38       &  0       & 376        &  20.6/200         &   23.2/200                &  0.37                        &  485.65                                &  1386.72          &  0       &    402                            \\
                            & SneakyPrompt \cite{yang2024sneakyprompt} & 2.8/10                                  & 3.2/10                                   & 0.26                         & 457.84                                & 1137.69            &  0       &   389          &   85.2/200                        &   89.4/200                               &  0.31               &  506.32                             &  1115.73         &   0      &  406         \\
                            & \cellcolor[HTML]{EFEFEF}UPAM (with TAL) & \cellcolor[HTML]{EFEFEF}3.6/10 & \cellcolor[HTML]{EFEFEF}4.7/10 & \cellcolor[HTML]{EFEFEF}0.18 & \cellcolor[HTML]{EFEFEF}\textbf{52.67} & \cellcolor[HTML]{EFEFEF}762.85  & \cellcolor[HTML]{EFEFEF}0      & \cellcolor[HTML]{EFEFEF}  \textbf{10}    & \cellcolor[HTML]{EFEFEF}105.3/200 & \cellcolor[HTML]{EFEFEF}112.1/200 & \cellcolor[HTML]{EFEFEF}\textbf{0.19}  & \cellcolor[HTML]{EFEFEF}\textbf{55.18} & \cellcolor[HTML]{EFEFEF}816.74 & \cellcolor[HTML]{EFEFEF}0     & \cellcolor[HTML]{EFEFEF}  \textbf{10} \\
\multirow{-7}{*}{DALL·E 2 \cite{ramesh2022hierarchical}}  & \cellcolor[HTML]{EFEFEF}UPAM (w/o TAL) & \cellcolor[HTML]{EFEFEF}\textbf{6.7/10} & \cellcolor[HTML]{EFEFEF}\textbf{7.4/10} & \cellcolor[HTML]{EFEFEF}\textbf{0.16} & \cellcolor[HTML]{EFEFEF}346.46  & \cellcolor[HTML]{EFEFEF}\textbf{725.14}  & \cellcolor[HTML]{EFEFEF}0  & \cellcolor[HTML]{EFEFEF}  66  & \cellcolor[HTML]{EFEFEF}\textbf{132.6/200} & \cellcolor[HTML]{EFEFEF}\textbf{135/200} & \cellcolor[HTML]{EFEFEF}\textbf{0.19}  & \cellcolor[HTML]{EFEFEF}389.50 & \cellcolor[HTML]{EFEFEF}\textbf{796.06} & \cellcolor[HTML]{EFEFEF}0 & \cellcolor[HTML]{EFEFEF} 71  \\ \hline
                            & TextFooler \cite{jin2020bert}  & 0/10                                    & 0/10                                   & 0.23                         & 559.42                                  & 3539.57       &  0       &  1007     &   0/200                                &   0/200                                 &  0.23                        &      592.37                            &  3593.46                             &  0       &  1051                        \\
                            & HomoSubs \cite{struppek2022biased} & 0/10                                    & 0/10                                   & 0.13                         & 586.75                                  & 2674.29         & 0        &   795     &    0/200                                 &    1.2/200                                &       0.20                   &     571.31                           &    3265.13                &  0       &  801                      \\
                            & EvoPromp \cite{milliere2022adversarial}   & 0.3/10                                  & 0.7/10                                 & 0.21                         & 443.39                                  & 4162.18            &  0       & 554      &      3.2/200                              &   3.8/200                                 &   0.45                        &    499.56                              &  4623.07             &  0       &   633                           \\
                            & HiddVocab \cite{daras2022discovering}      & 0.3/10                                    & 0.5/10                                 & 0.18                         & 576.83                                  & 4264.34       &   0      &   661      &       6.8/200                             &    8.6/200                                &   0.31                        &   543.06                              &    3755.64       &   0      &   622                                    \\
                            & MacPromp \cite{milliere2022adversarial}    & 0.5/10                                  & 0.8/10                                 & 0.43                         & 463.52                                  & 3097.13      &   0      &   648      &     14/200                              &     15.4/200                         &      0.25                    &    507.49                             &   3128.42                   &     0    &   676                            \\
                            & MMA-Diff  \cite{yang2023mma}   & 0.6/10                                  & 1/10                                   & 0.30                         & 422.67                                  & 2774.33       &  0       &  530    &  38/200                     &  39.8/200                    &  0.39                &  480.46                               &  3209.41         &   0      &  549                                        \\
                            & RIATIG  \cite{liu2023riatig}   & 0.9/10                                  & 1.2/10                                   & 0.34                         & 485.36                                  & 1057.42      &   0      &   472    &  32.6/200                     &  35.6/200                    &  0.32                &  518.22                               &  1543.76       &   0      &    515                                         \\
                            & SneakyPrompt \cite{yang2024sneakyprompt} & 3.2/10                                  & 3.7/10                                   & 0.29                         & 508.79                                & 1197.57             &   0      &     419                    &  94.6/200                       &  95.6/200                                &  0.29                  &  498.11                            &  1325.86            &   0      &   403        \\
                            & \cellcolor[HTML]{EFEFEF}UPAM (with TAL) & \cellcolor[HTML]{EFEFEF}3.9/10 & \cellcolor[HTML]{EFEFEF}4.8/10 & \cellcolor[HTML]{EFEFEF}0.19 & \cellcolor[HTML]{EFEFEF}\textbf{53.16} & \cellcolor[HTML]{EFEFEF}468.05  & \cellcolor[HTML]{EFEFEF}0      & \cellcolor[HTML]{EFEFEF}  \textbf{10}    & \cellcolor[HTML]{EFEFEF}112.5/200 & \cellcolor[HTML]{EFEFEF}116.2/200 & \cellcolor[HTML]{EFEFEF}0.20  & \cellcolor[HTML]{EFEFEF}\textbf{58.41} & \cellcolor[HTML]{EFEFEF}670.34 & \cellcolor[HTML]{EFEFEF}0     & \cellcolor[HTML]{EFEFEF}  \textbf{10} \\
\multirow{-7}{*}{Imagen \cite{saharia2022photorealistic}}    & \cellcolor[HTML]{EFEFEF}UPAM (w/o TAL) & \cellcolor[HTML]{EFEFEF}\textbf{6.8/10} & \cellcolor[HTML]{EFEFEF}\textbf{7.3/10} & \cellcolor[HTML]{EFEFEF}\textbf{0.17} & \cellcolor[HTML]{EFEFEF}358.29 & \cellcolor[HTML]{EFEFEF}\textbf{411.53}  & \cellcolor[HTML]{EFEFEF}0  & \cellcolor[HTML]{EFEFEF} 67   & \cellcolor[HTML]{EFEFEF}\textbf{138.2/200} & \cellcolor[HTML]{EFEFEF}\textbf{140.4/200} & \cellcolor[HTML]{EFEFEF}\textbf{0.18}  & \cellcolor[HTML]{EFEFEF}393.03 & \cellcolor[HTML]{EFEFEF}\textbf{651.86}  & \cellcolor[HTML]{EFEFEF}0  & \cellcolor[HTML]{EFEFEF} 68   \\ 
\bottomrule[0.15em] 
\end{tabular}
}
\end{table*}
% ==============================Tab.2









%---------------   4.3  --------------------
\subsection{Experimental Results}


\textbf{Experimental Results Under Protocol A.}
We use UPAM (w/o TAL) to represent our method's conventional approach, which does not limit the number of API queries. In contrast, UPAM (with TAL) represents our method that trains the attack model using only few-shot API queries.
As shown in Tab. \ref{tab:exp_whole}, our UPAM (w/o TAL) achieves the best results in terms of the R-1 precision, R-3 precision and textual similarity. 
Specifically, our UPAM (w/o TAL) significantly outperforms other methods by an average of 33.89\% (R-1), 37.01\% (R-3), and 14.79\% (textual similarity) in COCO dataset, and 35.03\% (R-1), 36.11\% (R-3), and 12.42\% (textual similarity) in NSFW dataset, respectively. These results demonstrate the attack effectiveness of our method.
In terms of the attack naturalness, our UPAM (w/o TAL) achieves a significantly lower PPL than other methods, which demonstrates the naturalness of adversarial prompts generated by our method.
Also, we can observe that UPAM (w/o TAL) exhibits a significantly shorter inference time compared to other methods, showcasing superior efficiency of our approach.
In terms of the query times, as shown in Tab. \ref{tab:exp_whole}, different from previous methods that require querying the API during inference, our method primarily relies on querying the API during training. Since we utilize the trained model directly for inference, during inference, our method queries API only once, which is significantly lower than the query times of previous methods. 
Even though our method requires API queries for training, the combined total training and testing query times are still obviously less than the total query times of each previous method.
We can see UPAM (w/o TAL) already performs fewer queries than previous methods. Our TAL scheme can further reduce the number of queries.
As shown in Tab. \ref{tab:exp_whole}, our UPAM (with TAL) queries API for only few-shot (10-shot) times during training, outperforming previous methods requiring hundreds or thousands of quires, demonstrating the effect of our TAL scheme.



\textbf{Experimental Results of Protocol B.}
In protocol B, given a target image, we aim to directly find out the corresponding adversarial prompt for the target image. 
For the COCO dataset, we follow previous methods \cite{liu2023riatig,daras2022discovering,milliere2022adversarial,struppek2022biased,jin2020bert} to randomly choose 10 samples from 10 ``harmful'' classes of COCO (one for each class), and then use our UPAM to find the corresponding adversarial prompts for the 10 samples. Finally, we evaluate the attack performance of these adversarial prompts.
For the NSFW dataset, we follow the previous work \cite{yang2024sneakyprompt} to find adversarial prompts for 200 given samples, and subsequently calculate the attack performance.
We conduct experiments 10 times and show the average results in Tab. \ref{tab:exp_harm}.
We can see that, our UPAM (w/o TAL) significantly outperforms other methods in terms of R-1 precision, R-3 precision, and textual similarity, which demonstrates the effectiveness of our method. Moreover, even in protocol B (where we don't train a parameterized model on the training set), our UPAM (w/o TAL) still achieves the much shorter inference time than others. This is due to our method's ability to use gradients for searching adversarial prompts, which is much more efficient than enumeration-based methods. In Tab. \ref{tab:exp_harm}, we can also see that our UPAM (w/o TAL) exhibits the lowest PPL, indicating the best naturalness of generated adversarial prompts. 
In terms of query times, we can see that our method has zero training queries due to the inference time optimization defined in protocol B. 
We can see that our method has a significant advantage in terms of query times. Particularly, when using TAL, our UPAM can outperform previous methods with only a few (10-shot) queries, demonstrating the effect of TAL.
All these results collectively demonstrate the superiority and applicability of our approach in protocol B.



\textbf{Additional Comparison Results in Efficiency.}
As shown in Tab. \ref{tab:time}, to further demonstrate the efficiency of our method, we additionally present the total training time and total inference time of our approach with comparison to previous enumeration methods under protocol A. It can be observed that, in the setting of attacking DALL$\cdot$E \cite{yu2022scaling}, the total inference time of our method is only 0.17 hours on the COCO dataset (and 0.28 hours on the NSFW dataset), which is significantly lower than the total inference time of enumeration-based methods (e.g., 19.85 hours).

It is noteworthy that although our method introduces a training phase, the combined total training and inference time remains notably less than the total time required by enumeration-based methods. Taking the setting of attacking DALL$\cdot$E \cite{yu2022scaling} as an example, our UPAM (with TAL) requires a total of 9.62 hours for both training and inference on the COCO dataset (and 11.93 hours on the NSFW dataset), while each enumeration-based method requires more than 13 hours on the COCO dataset (and more than 19 hours on the NSFW dataset), demonstrating the efficiency of our method even when considering the training time.


This is because, after training, our parameterized model can efficiently perform fast mapping that translates naive prompts into adversarial prompts. As mentioned earlier, the inference time per sample for our method (around 5.11$\sim$5.57 seconds) is considerably lower than that of enumeration-based approaches (around 353.53$\sim$593.51 seconds). This advantage becomes increasingly evident as the number of testing (inference) samples grows, further widening the efficiency gap between our approach and traditional enumeration-based methods.


To ensure the robustness (general applicability) of our method, we have conducted a comprehensive evaluation under various settings by attacking different T2I models. As shown in Tab. \ref{tab:time}, the results on different T2I models demonstrate that our method consistently achieves strong efficiency superiority than previous methods, even when accounting for training time.


Moreover, we can also observe that removing TAL (i.e., w/o TAL) further reduces the training time of our method. This is because, when not using TAL, our UPAM eliminates the process of adapting the offline model to the online API.




\textbf{Qualitative Results.}  
In this section, we present qualitative results. Specifically, we compare our framework with HiddVocab \cite{daras2022discovering}, MacPromp \cite{milliere2022adversarial}, MMA-Diff  \cite{yang2023mma}, RIATIG  \cite{liu2023riatig}, and SneakyPrompt \cite{yang2024sneakyprompt}, while not showcasing the results of TextFooler \cite{jin2020bert}, HomoSubs \cite{struppek2022biased}, and EvoPromp \cite{milliere2022adversarial}. This is because the latter three methods can hardly compel the black-box API to return images, and thus their results cannot be presented.

In Fig. \ref{fig:qua_2} and \ref{fig:qua_3}, we respectively showcase the attack results on the COCO dataset and the NSFW dataset. We can see that, in order to bypass textual filters, all methods generate adversarial prompts that no longer contain the sensitive words of naive prompts (marked in red). However, existing methods produce adversarial text with noticeably unnatural words (highlighted with blue underlines). In contrast, our method generates adversarial prompts exhibiting superior naturalness. Moreover, compared to other methods, our framework achieves the closest semantic alignment between the generated images and target images. 
We can observe that, our UPAM framework occasionally generates prompts with unnatural semantics. As shown in Fig. \ref{fig:qua_2} and \ref{fig:qua_3}, examples in UPAM outputs sometimes include misspelled words like ``consist e d'', ``ma-mary'', ``a n'', and ``tegument'' (also marked with blue underlines). These instances indicate that there is room for improvement in ensuring completely semantically accurate textual prompts.


Nevertheless, compared to other methods, which often produce highly unnatural and difficult-to-interpret words (e.g., ``Judsonnisi'', ``atoKjbmyith''), our UPAM framework exhibits significantly higher contextual coherence. The generated prompts are generally easier to interpret, and readers can often deduce the intended words even when minor errors are present. This highlights UPAM's advantage in maintaining overall contextual relevance, making its adversarial prompts more understandable and contextually aligned.



% ==============================Tab.3
\begin{table}[t]
\centering
\caption{Performance comparison in terms of total training time and total inference time. The unit for the values in the table is \textit{hours.}}
\vspace{-2mm}
\label{tab:time}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule[0.15em]
\multirow{2}{*}{T2I Model} & \multirow{2}{*}{Methods} & \multicolumn{2}{c}{\textbf{COCO}}                      &           & \multicolumn{2}{c}{\textbf{NSFW}}          \\ \cline{3-4} \cline{6-7}
\specialrule{0em}{1pt}{1pt}
                           &                          & Total Train. Time & Total Infer. Time  &           & Total Train. time & Total Infer. time \\ 
\midrule[0.10em]
\specialrule{0em}{1pt}{1pt}
                           & TextFooler \cite{jin2020bert}      & -                         & 19.85                      &           & -                   & 29.63                \\
                           & HomoSubs \cite{struppek2022biased}        & -                         & 18.74                      &           & -                   & 29.19               \\
                           & EvoPromp \cite{milliere2022adversarial}        & -                         & 19.30                      &           & -                   & 26.43                \\
                           & HiddVocab \cite{daras2022discovering}       & -                         & 15.24                      &           & -                   & 28.08                \\
DALL$\cdot$E \cite{yu2022scaling}             & MacPromp \cite{milliere2022adversarial}        & -                         & 15.07                      &           & -                   & 25.13                \\
                           & MMA-Diff \cite{yang2023mma}        & -                         & 12.89                      &  & -                   & 19.91                \\
                           & RIATIG \cite{liu2023riatig}           & -                         & 13.05                      &           & -                   & 20.49                \\
                           & SneakyPrompt \cite{yang2024sneakyprompt}    & -                         & 18.73                      &           & -                   & 24.92                \\
                           & \cellcolor[HTML]{EFEFEF}UPAM (with TAL)          & \cellcolor[HTML]{EFEFEF}9.45                      & \cellcolor[HTML]{EFEFEF}\textbf{0.17}              &\cellcolor[HTML]{EFEFEF}           & \cellcolor[HTML]{EFEFEF}11.65               & \cellcolor[HTML]{EFEFEF}\textbf{0.28}        \\
                           & \cellcolor[HTML]{EFEFEF}UPAM (w/o TAL)           & \cellcolor[HTML]{EFEFEF}9.41                      & \cellcolor[HTML]{EFEFEF}\textbf{0.17}              & \cellcolor[HTML]{EFEFEF} & \cellcolor[HTML]{EFEFEF}11.58               & \cellcolor[HTML]{EFEFEF}\textbf{0.28}        \\ 
\hline
\specialrule{0em}{1pt}{1pt}
                           & TextFooler \cite{jin2020bert}      & -                         & 18.03                      &           & -                   & 29.68                \\
                           & HomoSubs \cite{struppek2022biased}        & -                         & 18.94                      &           & -                   & 29.16                \\
                           & EvoPromp \cite{milliere2022adversarial}        & -                         & 15.86                      &           & -                   & 22.74                \\
                           & HiddVocab \cite{daras2022discovering}       & -                         & 18.84                      &           & -                   & 27.65                \\
DALL$\cdot$E 2 \cite{ramesh2022hierarchical}          & MacPromp \cite{milliere2022adversarial}        & -                         & 17.45                      &           & -                   & 25.57                \\
                           & MMA-Diff \cite{yang2023mma}        & -                         & 12.06                      &           & -                   & 22.88                \\
                           & RIATIG \cite{liu2023riatig}          & -                         & 11.78                      &           & -                   & 25.38                \\
                           & SneakyPrompt \cite{yang2024sneakyprompt}    & -                         & 16.50                      &           & -                   & 28.09                \\
                           & \cellcolor[HTML]{EFEFEF}UPAM (with TAL)          & \cellcolor[HTML]{EFEFEF}9.58                      & \cellcolor[HTML]{EFEFEF}\textbf{0.19}              & \cellcolor[HTML]{EFEFEF}          & \cellcolor[HTML]{EFEFEF}11.77               & \cellcolor[HTML]{EFEFEF}\textbf{0.27}        \\
                           & \cellcolor[HTML]{EFEFEF}UPAM (w/o TAL)           & \cellcolor[HTML]{EFEFEF}9.53                      & \cellcolor[HTML]{EFEFEF}\textbf{0.19}              & \cellcolor[HTML]{EFEFEF}          & \cellcolor[HTML]{EFEFEF}11.73               & \cellcolor[HTML]{EFEFEF}\textbf{0.27}        \\ 
\hline
\specialrule{0em}{1pt}{1pt}
                           & TextFooler \cite{jin2020bert}      & -                         & 17.59                      &           & -                   & 29.17                \\
                           & HomoSubs \cite{struppek2022biased}        & -                         & 18.93                      &           & -                   & 29.42                \\
                           & EvoPromp \cite{milliere2022adversarial}        & -                         & 13.28                      &           & -                   & 23.81                \\
                           & HiddVocab \cite{daras2022discovering}       & -                         & 19.75                      &           & -                   & 28.12                \\
                           & MacPromp \cite{milliere2022adversarial}        & -                         & 17.48                      &           & -                   & 25.98                \\
Imagen \cite{saharia2022photorealistic}            & MMA-Diff \cite{yang2023mma}        & -                         & 13.16                      &           & -                   & 23.83                \\
                           & RIATIG \cite{liu2023riatig}           & -                         & 12.94                      &           & -                   & 21.13                \\
                           & SneakyPrompt \cite{yang2024sneakyprompt}    & -                         & 14.25                      &           & -                   & 20.23                \\
                           & \cellcolor[HTML]{EFEFEF} UPAM (with TAL)          & \cellcolor[HTML]{EFEFEF}9.27                      & \cellcolor[HTML]{EFEFEF}\textbf{0.18}              & \cellcolor[HTML]{EFEFEF}          & \cellcolor[HTML]{EFEFEF}11.39               & \cellcolor[HTML]{EFEFEF}\textbf{0.28}        \\
                           & \cellcolor[HTML]{EFEFEF} UPAM (w/o TAL)           & \cellcolor[HTML]{EFEFEF}9.21                      & \cellcolor[HTML]{EFEFEF}\textbf{0.18}              &\cellcolor[HTML]{EFEFEF}           & \cellcolor[HTML]{EFEFEF}11.33               & \cellcolor[HTML]{EFEFEF}\textbf{0.28}        \\ 
\bottomrule[0.15em]
\vspace{-3mm}
\end{tabular}}

\end{table}
% ==============================Tab.3






% ==============================Tab.4
\begin{table}[t]
\centering
\caption{Ablation study on each design of our approach.}
\vspace{-2mm}
\label{tab:abl}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule[0.15em] 
&\textbf{Methods}      & \textbf{R-1} ↑    & \textbf{R-3} ↑    & \textbf{Text. Sim.} ↓ & \textbf{Infer. Time} ↓ & \textbf{PPL} ↓  \\ 
\hline
\specialrule{0em}{1pt}{1pt}
\cellcolor[HTML]{EFEFEF}(a) & \cellcolor[HTML]{EFEFEF}UPAM   & \cellcolor[HTML]{EFEFEF}38.56\%       & \cellcolor[HTML]{EFEFEF}41.92\%   & \cellcolor[HTML]{EFEFEF}0.17   & \cellcolor[HTML]{EFEFEF}5.11    & \cellcolor[HTML]{EFEFEF}641.26 \\
\specialrule{0em}{1pt}{1pt}
(b) & UPAM w/o SPL   & 0.22\%  & 0.37\%  & 0.53         & 5.13          & 640.05 \\ 
(c) & UPAM w/o SEL   & 23.25\% & 28.36\% & 0.17         & 5.10          & 643.31 \\
\bottomrule[0.15em] 
\vspace{-4mm}
\end{tabular}
}
\end{table}
% ==============================Tab.4










%==================  5 Ablation Studies =====================
\vspace{2mm}
\section{Ablation Studies}\label{sec:ablation}


In the following content, we use ''UPAM'' to refer to ''UPAM (w/o TAL)'' for clarity.




%---------------   5.1  --------------------
\vspace{2mm}
\subsection{Effect of SPL and SEL}
To demonstrate the effectiveness of our proposed schemes: SPL and SEL, we conduct ablation experiments by removing the specific SPL (or SEL) in our approach. 
Comparing (a) and (b) in Tab. \ref{tab:abl}, it can be seen that when UPAM does not employ SPL, both R-1 precision and R-3 precision drop to almost 0. 
This is because SPL is designed to deceive textual and visual defenses. In the absence of SPL, black-box T2I models hardly return images, resulting in poor R-precision performance. 
Additionally, SPL also influences the Textual Similarity score, as it can reduce the similarity between adversarial prompts and naive ones, aiming to deceive textual filters within the black-box system.
These ablation results demonstrate the effect of SPL.
When comparing (a) and (c) in Tab. \ref{tab:abl}, we can see that our UPAM without SEL leads to a significant R-precision decrease of 15.31\% (R-1) and 15.56\% (R-3), demonstrating the effect of SEL. We can observe that SEL has virtually no impact on Textual Similarity, Inference Time, and PPL. This is because SEL is proposed to enhance the semantic representation of the returned images, thereby only affecting R-precision.
As SPL and SEL do not alter the LLM-based structure of UPAM, they do not impact the Inference Time and PPL.






%---------------   5.2  --------------------
\subsection{Effect of LLM and LoRA}
To ensure the attack stealthiness, we propose to utilize a pre-trained LLM and optimize the LoRA adapter to ensure the naturalness of the generated adversarial prompts.
Here, we conduct two ablation studies: \textbf{(1)} instead of using the pr-trained LLM, we directly employ an untrained transformer \cite{touvron2023llama} (with the same structure as the LLM) and then train it from scratch. 
Comparing (i) and (ii) in Tab. \ref{tab:supp_llm}, we can see the transformer performs much worse in terms of PPL, demonstrating the effect of adopting knowledge of LLM.
\textbf{(2)} Instead of optimizing LoRA, we directly optimize the pre-trained LLM. Comparing (i) and (iii) in Tab. \ref{tab:supp_llm}, the significant change in PPL demonstrates the effect of using LoRA adapter.



% ==============================Tab.5
\begin{table}[t]
\centering
\caption{Ablation study on LLM and LoRA.}
\vspace{-2mm}
\label{tab:supp_llm}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule[0.15em] 
&\textbf{Methods}      & \textbf{R-1} ↑    & \textbf{R-3} ↑    & \textbf{Text. Sim.} ↓ & \textbf{Infer. Time} ↓ & \textbf{PPL} ↓  \\ 
\hline
\specialrule{0em}{1pt}{1pt}
\cellcolor[HTML]{EFEFEF}(i) & \cellcolor[HTML]{EFEFEF}UPAM   & \cellcolor[HTML]{EFEFEF}38.56\%       & \cellcolor[HTML]{EFEFEF}41.92\%   & \cellcolor[HTML]{EFEFEF}0.17   & \cellcolor[HTML]{EFEFEF}5.11    & \cellcolor[HTML]{EFEFEF}641.26 \\
\specialrule{0em}{1pt}{1pt}
(ii) & UPAM w/o LLM   & 38.45\% & 41.68\% & 0.18         & 5.13          & 3954.85 \\
(iii) & UPAM w/o LoRA  & 38.42\% & 41.79\% & 0.17         & 5.11          & 2848.52 \\
\bottomrule[0.15em] 
\end{tabular}
}
\end{table}
% ==============================Tab.5





%---------------   5.3  --------------------
\subsection{Effect of INE} 

In order to harness in-context learning to further unleash the potential of LLM in generating naturally adversarial prompts, we propose an INE scheme which introduces several in-context examples as the input of LLM. With the comparison between (i) and (ii) in Tab. \ref{tab:supp_in_con},  the observed change in PPL clearly demonstrates that INE can enhance the naturalness of adversarial prompts.



% ==============================Tab.6
\begin{table}[ht]
\centering
\caption{Ablation study on INE.}
\vspace{-2mm}
\label{tab:supp_in_con}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule[0.15em] 
&\textbf{Methods}      & \textbf{R-1} ↑    & \textbf{R-3} ↑    & \textbf{Text. Sim.} ↓ & \textbf{Infer. Time} ↓ & \textbf{PPL} ↓  \\ 
\hline
\specialrule{0em}{1pt}{1pt}
\cellcolor[HTML]{EFEFEF}(i) & \cellcolor[HTML]{EFEFEF}UPAM   & \cellcolor[HTML]{EFEFEF}38.56\%       & \cellcolor[HTML]{EFEFEF}41.92\%   & \cellcolor[HTML]{EFEFEF}0.17   & \cellcolor[HTML]{EFEFEF}5.11    & \cellcolor[HTML]{EFEFEF}641.26 \\
\specialrule{0em}{1pt}{1pt}
(ii) & UPAM w/o INE.  & 38.33\% & 41.72\% & 0.17      & 5.08          & 715.32 \\
\bottomrule[0.15em] 
\end{tabular}
}
\end{table}
% ==============================Tab.6






%---------------   5.4  --------------------
\subsection{Effect of the second stage of SPL}
In the second stage of SPL, after optimizing the model parameters into the ``Pass'' region, we gradually reduce the radius, moving the model parameters closer to the boundary, aiming to increase the likelihood of the generation of target ``harmful'' images, as illustrated in Fig. \ref{fig:spl} (b).
To investigate the effect of this design, we conduct ablation by removing this second stage. 
By comparing (i) and (ii) in Tab. \ref{tab:MCB}, when removing the second stage of SPL, obvious performance degradation is observed in terms of R-1, R-3, and Textual Similarity.
This demonstrates the effectiveness of the second stage of SPL.


% ==============================Tab.7
\begin{table}[ht]
\centering
\caption{Ablation study on the second stage of SPL.}
\vspace{-2mm}
\label{tab:MCB}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule[0.15em] 
&\textbf{Methods}      & \textbf{R-1} ↑    & \textbf{R-3} ↑    & \textbf{Text. Sim.} ↓ & \textbf{Infer. Time} ↓ & \textbf{PPL} ↓  \\ 
\hline
\specialrule{0em}{1pt}{1pt}
\cellcolor[HTML]{EFEFEF}(i) & \cellcolor[HTML]{EFEFEF}UPAM   & \cellcolor[HTML]{EFEFEF}38.56\%       & \cellcolor[HTML]{EFEFEF}41.92\%   & \cellcolor[HTML]{EFEFEF}0.17   & \cellcolor[HTML]{EFEFEF}5.11    & \cellcolor[HTML]{EFEFEF}641.26 \\
\specialrule{0em}{1pt}{1pt}
(ii) & UPAM w/o 2nd Stage   & 35.76\% & 39.31\% & 0.22         & 5.11          & 706.10 \\
\bottomrule[0.15em] 
\end{tabular}
}
\end{table}
% ==============================Tab.7









%---------------   5.5  --------------------
\subsection{Effect of gradient harmonization in SEL}
To improve the compatibility of SEL with SPL, we introduce a Gradient Harmonization (GH) method (see Eq. \ref{eq:sel_gradient}) to adaptively adjust SEL's gradients.
With a comparison between (i) and (ii) in Tab. \ref{tab:GH}, we can see that, when removing GH, SEL could disrupt SPL's optimization achievements, thus leading to worse performances in terms of R-1, R-3, and Textual Similarity.
This demonstrates the effectiveness of gradient harmonization in SEL.


% ==============================Tab.8
\begin{table}[ht]
\centering
\caption{Ablation study on Gradient Harmonization (GH) in SEL.}
\vspace{-2mm}
\label{tab:GH}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule[0.15em] 
&\textbf{Methods}      & \textbf{R-1} ↑    & \textbf{R-3} ↑    & \textbf{Text. Sim.} ↓ & \textbf{Infer. Time} ↓ & \textbf{PPL} ↓  \\ 
\hline
\specialrule{0em}{1pt}{1pt}
\cellcolor[HTML]{EFEFEF}(i) & \cellcolor[HTML]{EFEFEF}UPAM   & \cellcolor[HTML]{EFEFEF}38.56\%       & \cellcolor[HTML]{EFEFEF}41.92\%   & \cellcolor[HTML]{EFEFEF}0.17   & \cellcolor[HTML]{EFEFEF}5.11    & \cellcolor[HTML]{EFEFEF}641.26 \\
\specialrule{0em}{1pt}{1pt}
(ii) & UPAM w/o GH    & 33.82\% & 35.08\% & 0.35         & 5.11          & 645.14 \\
\bottomrule[0.15em] 
\end{tabular}
}
\end{table}
% ==============================Tab.8




% ==============================Figure 6
\begin{figure*}[ht]
\centering
\includegraphics[width=0.85\textwidth]{images/pipeline_for_B_new.pdf} 
\caption{Overview of our UPAM framework modified for Protocol B.
}
\label{fig:pipeline_for_B}
\vspace{-2mm}
\end{figure*}
% ==============================Figure 6







%---------------   5.6  --------------------
\subsection{Ablation on using small learning rate in SPL} 
In SPL, our goal is to make the model parameters fall into the boundary region, leaning towards the ``Pass'' side. 
We consider that when the model parameters are optimized from the ``Deny'' case to the ``Pass'' case, if we use a very small learning rate (i.e., optimization step size), whether it is possible to directly locate near the boundary after crossing the boundary.
If this can work, the process of  ``moving closer to the boundary'' in SPL could be skipped.
However, experimental results from Tab. \ref{tab:supp_lr} (ii) show that when using a small learning rate, SPL shows less effective performance in terms of R1, R3, and Textual Similarity.
This is because the model with a small learning rate struggles to transition from the ``Deny'' case to the ``Pass'' case, thus cannot effectively bypass the textual and visual defenses.
We also attempt to set a relatively large initial learning rate and then gradually decrease it.  
However, as shown in  Tab. \ref{tab:supp_lr} (iii), the performance remains unsatisfactory, because it is challenging to manually set the learning rate decay in a way that allows the model to cross the boundary while remaining very close to it.


% ==============================Tab.9
\begin{table}[ht]
\centering
\caption{Ablation study on using small learning rate in SPL.}
\vspace{-2mm}
\label{tab:supp_lr}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule[0.15em] 
&\textbf{Methods}      & \textbf{R-1} ↑    & \textbf{R-3} ↑    & \textbf{Text. Sim.} ↓ & \textbf{Infer. Time} ↓ & \textbf{PPL} ↓  \\ 
\hline
\specialrule{0em}{1pt}{1pt}
\cellcolor[HTML]{EFEFEF}(i) & \cellcolor[HTML]{EFEFEF}SPL   & \cellcolor[HTML]{EFEFEF}38.56\%       & \cellcolor[HTML]{EFEFEF}41.92\%   & \cellcolor[HTML]{EFEFEF}0.17   & \cellcolor[HTML]{EFEFEF}5.11    & \cellcolor[HTML]{EFEFEF}641.26 \\
\specialrule{0em}{1pt}{1pt}
(ii) & SPL (small learning rate)   & 7.34\% & 9.62\% & 0.47         & 5.11          & 645.08 \\
(iii) & SPL (learning rate decay)  & 12.42\% & 13.77\% & 0.45         & 5.09          & 644.11 \\
\bottomrule[0.15em] 
\end{tabular}
}
\end{table}
% ==============================Tab.9



Note that in Tab. \ref{tab:supp_lr}, we present the results obtained after carefully tuning the learning rates. Specifically, in experiment (ii), we adopt a small learning rate of 0.005, and in experiment (iii), we use an initial learning rate of 0.5 with an exponential decay rate of 0.2. These experiments reveal that neither employing a small learning rate nor gradually decaying the learning rate in SPL can achieve effective performance, further highlighting the effectiveness of the SPL's design.

Moreover, it is important to highlight that no modifications were made to the model structure of UPAM or the parameters of the LLM in these ablation studies. Therefore, we can see that the inference time and PPL (naturalness) remain almost unchanged.



% ==============================Tab.10
\begin{table}[ht]
\centering
\caption{Ablation study on calculating gradients from different points in SPL.}
\vspace{-2mm}
\label{tab:supp_points}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule[0.15em] 
&\textbf{Methods}      & \textbf{R-1} ↑    & \textbf{R-3} ↑    & \textbf{Text. Sim.} ↓ & \textbf{Infer. Time} ↓ & \textbf{PPL} ↓  \\ 
\hline
\specialrule{0em}{1pt}{1pt}
\cellcolor[HTML]{EFEFEF}(i) & \cellcolor[HTML]{EFEFEF}SPL (using ``Deny'' points)   & \cellcolor[HTML]{EFEFEF}38.56\%       & \cellcolor[HTML]{EFEFEF}41.92\%   & \cellcolor[HTML]{EFEFEF}0.17   & \cellcolor[HTML]{EFEFEF}5.11    & \cellcolor[HTML]{EFEFEF}641.26 \\
\specialrule{0em}{1pt}{1pt}
(ii) & SPL (using ``Deny'' points)   & 38.45\% & 41.80\% & 0.18         & 5.12          & 641.02 \\
(iii) & SPL (using both ``Pass'' and ``Deny'' points)  & 38.49\% & 41.91\% & 0.17         & 5.10          & 644.26 \\
\bottomrule[0.15em] 
\end{tabular}
}
\end{table}
% ==============================Tab.10








%---------------   5.7  --------------------
\subsection{Ablation on calculating gradients from different points in SPL} 

In SPL, when calculating gradients, we compute the gradients based on the average of ``Pass'' points. Here, we conduct an ablation study by calculating gradients based on the average of ``Deny'' points, which modifies the Eq. \ref{eq:phi} as follows: 
% ======================== Eq.14
\begin{equation}
\vspace{-2mm}
\Phi(\psi)= \begin{Bmatrix}
 0 & \mathbf{if} \rm{\; image \;returned \,(``Pass")} \\
 -1 & \mathbf{otherwise}
\end{Bmatrix}
\end{equation}
% ======================== Eq.14
Additionally, we conduct an ablation experiment using both ``Pass'' points and ``Deny'' points to calculate gradients, reformulating the Eq. \ref{eq:phi} as follows: 
% ======================== Eq.15
\begin{equation}
\vspace{-2mm}
\Phi(\psi)= \begin{Bmatrix}
 +1 & \mathbf{if} \rm{\; image \;returned \,(``Pass")} \\
 -1 & \mathbf{otherwise}
\end{Bmatrix}
\end{equation}
% ======================== Eq.15
The results in Tab. \ref{tab:supp_points} indicate that, regardless of which kind of points are used, the performance remains almost unchanged, demonstrating the robustness of our SPL in using different gradient calculation approaches.



% ==============================Tab.11
\begin{table}[ht]
\centering
\caption{Ablation on using a smaller instruction-tuned language model (Flan-T5-Small) for PPL evaluation.}
\vspace{-2mm}
\label{tab:small}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{ccccc}
\toprule[0.15em] 
\textbf{Methods}        &        & \textbf{PPL (GPT-2) ↓}   & & \textbf{PPL (Flan-T5-Small) ↓} \\ 
\hline
\specialrule{0em}{1pt}{1pt}
TextFooler \cite{jin2020bert} &  & 3463.02      &   & 3876.05             \\
HomoSubs \cite{struppek2022biased} &    & 3089.58     &    & 3663.89             \\
EvoPromp \cite{milliere2022adversarial}   &  & 4984.24     &    & 5197.42             \\
HiddVocab \cite{daras2022discovering}   & & 4027.64     &    & 4175.28            \\
MacPromp \cite{milliere2022adversarial}   &  & 3163.07      &   & 3531.73             \\
MMA-Diff \cite{yang2023mma}    & & 3857.29      &   & 4031.19             \\
RIATIG \cite{liu2023riatig}    &    & 1003.27      &   & 1326.84             \\
SneakyPrompt \cite{yang2024sneakyprompt} & &1457.36     &   & 1584.57             \\
\cellcolor[HTML]{EFEFEF}UPAM          & \cellcolor[HTML]{EFEFEF}      & \cellcolor[HTML]{EFEFEF}\textbf{658.18} & \cellcolor[HTML]{EFEFEF} &\cellcolor[HTML]{EFEFEF}\textbf{823.63}     \\ 
\bottomrule[0.15em] 
\end{tabular}}
\end{table}
% ==============================Tab.11







%---------------   5.8  --------------------
\subsection{Ablation on using a smaller instruction-tuned language model for PPL evaluation} 


To further explore the evaluation of adversarial prompts' naturalness, we selected a smaller, instruction-tuned model, Flan-T5-Small \cite{chung2024scaling}, to compute the perplexity score (PPL). Flan-T5-Small has only 80M parameters, significantly fewer than GPT-2 (more than 117M). Being fine-tuned through multi-task instruction tuning, Flan-T5-Small is better equipped to understand and execute human instructions.


As shown in the Tab. \ref{tab:small}, the results reveal that: (1) even when evaluated with a different language model like Flan-T5-Small, our method continues to exhibit significant advantages in preserving the naturalness of adversarial prompts, outperforming prior approaches. This further validates the robustness of our method in maintaining prompt naturalness.
(2) Both our method and other approaches show higher PPL scores (i.e., generally performing worse) when measured with Flan-T5-Small compared to GPT-2.
We analyze the reason that Flan-T5-Small, as a small instruction-tuned model, is trained with objectives distinct from GPT-2. 
Specifically, Flan-T5-Small is optimized for instruction-based downstream tasks such as question answering and reasoning, while GPT-2 focuses on pure language modeling. Therefore, as PPL reflects a model's language modeling capabilities, Flan-T5-Small which is an instruction-tuned model may yield less favorable PPL scores compared to GPT-2 which is designed for language generation alone.



In summary, the experiment results demonstrate that while the choice of evaluation model impacts PPL scores, our method consistently maintains superior naturalness across models, reinforcing its effectiveness. Additionally, the observed differences underscore the importance of selecting appropriate evaluation metrics and models tailored to the evaluation context.





% ==============================Figure 7
\begin{figure*}[t]
\centering
\includegraphics[width=0.89\textwidth]{images/qualitative_1_new.pdf} 
\caption{Qualitative comparison results with existing methods on COCO dataset. The text above represents the adversarial prompts generated by the corresponding method, while the pictures below show the images returned from the T2I model. The name of each ``harmful'' class is marked in \textcolor{red}{red}. We employ \textcolor{blue}{blue underlines} to show unnatural words within adversarial prompts.
}
\label{fig:qua_2}
\end{figure*}
% ==============================Figure 7

% ==============================Figure 8
\begin{figure*}[t]
\centering
\includegraphics[width=0.89\textwidth]{images/qualitative_2_new.pdf} 
\caption{Qualitative comparison results with existing methods on NSFW dataset. The text above represents the adversarial prompts generated by the corresponding method, while the pictures below show the images returned from the T2I model. The name of each ``harmful'' class is marked in \textcolor{red}{red}. We employ \textcolor{blue}{blue underlines} to show unnatural words within adversarial prompts. We use white mosaics to obscure sensitive areas for ethical considerations.
}
\label{fig:qua_3}
\end{figure*}
% ==============================Figure 8





%=========  6 Modification of UPAM for Protocol B ===========
\section{Modification of UPAM for Protocol B}\label{modi_B}
As for previous enumeration-based methods, given a naive prompt $\mathbf{T}$ and a target image $\mathbf{I}$, instead of training parameterized models, they aim to directly find an adversarial prompt $\mathbf{T}^{\ast}$ based on the given data $\mathbf{T}$ and $\mathbf{I}$. 
We refer to this as Protocol B. 
To ensure a fair comparison with previous enumeration-based methods, we make slight modifications to UPAM to enable it to directly find adversarial prompts without learning from the training set. 
The modified architecture is illustrated in Fig. \ref{fig:pipeline_for_B}. Next, we provide modification details as follows:

1. As for the structure of our UPAM, we no longer utilize the LoRA adapter while only adopting the LLM. The parameters of the LLM remain frozen.

2. Given $\{ \mathbf{T}, \mathbf{I}  \}$, following the processes of SPL and SEL, we still compute gradients $\pmb{g}_\mathrm{spl}$ and $\pmb{g}_\mathrm{sel}$. The difference is that, these gradients no longer optimize LoRA parameters but optimize the input soft embedding $\mathbf{P}$. The soft embedding $\mathbf{P}$ is obtained by feeding the naive $\mathbf{T}$ into the text encoder of LLM. 

In this way, given a naive prompt $\mathbf{T}$ and a target image $\mathbf{I}$, we can directly find a proper adversarial prompt $\mathbf{T}^{\ast}$ by optimizing the input embedding $\mathbf{P}$, without training any model parameters.







%==================  7 Discussion =====================
\section{Discussion}


\textbf{Ethical Implications:}
While our research contributes to understanding model vulnerabilities, we recognize the potential ethical concerns, particularly the misuse of our findings in generating harmful or inappropriate content. Text-to-image models, when misused, have the potential to create images that could cause harm, spread misinformation, or infringe on individual privacy and dignity. Therefore, it is critical to develop not only the technology but also a responsible framework for its use.



\textbf{Regulatory Compliance:}
Researchers and practitioners seeking to implement this work must strictly adhere to existing guidelines and regulations governing AI and content generation, including data protection laws \cite{yan2010personal}, AI ethics principles \cite{fournier2021towards}, and anti-harassment policies \cite{witze2018nsf}. Compliance with these frameworks is crucial to ensure that research does not inadvertently contribute to harmful or illegal activities.


\textbf{Measures to Mitigate Misuse:}
To mitigate the risk of misuse, besides the regulatory compliance, we suggest several measures that should accompany the development of such techniques.
(1) Access control. To further reduce the potential for harmful applications, we propose implementing stricter access controls on our UPAM model. Only authorized and responsible users, such as researchers and organizations committed to ethical AI practices, could be allowed to access and deploy our UPAM model. 
(2) Method transparency. For authorized developers who wants to further improve our method for research, we will urge them to be transparent about the capabilities and limitations of their methods. This includes providing research paper to clearly describe on how the technology works, its potential for harm, and the safeguards implemented to minimize misuse. Such transparency will enable defense teams to develop targeted and effective countermeasures.
(3) Possible defense against our approach. One potential defense is to fine-tune the T2I models using \textit{adversarial training} \cite{shafahi2019adversarial}. Through adversarial training, defenders can enhance the robustness of T2I models, making them unable to generate the target image when provided with adversarial prompts crafted by our approach.
Another effective defense mechanism could be leveraging \textit{model unlearning} \cite{liu2022backdoor}. This technique allows T2I models to ``unlearn'' harmful or sensitive knowledge, effectively reducing their susceptibility to adversarial attacks.
Since our approach specifically targets the textual filters and visual checkers deployed in APIs, another possible defense is to frequently update or rotate the textual filters and visual checkers, i.e., \textit{dynamic defense system} \cite{lee2020dynamic}. This strategy can make it harder for attackers to adapt and craft effective adversarial prompts.



\textbf{Future Beneficial Applications: }
Notably, the technique we explore can be repurposed for future positive applications. For instance, our black-box attack model can serve as a security vulnerability detector to evaluate the robustness and security of T2I APIs. By simulating potential adversarial attacks, developers can proactively uncover weaknesses in their models and implement more effective defense mechanisms, such as refining textual filters and visual checkers.
Furthermore, the capability to effectively optimize prompts through our black-box gradient learning could enhance creative AI systems. That is, our approach could help enable users to craft more effective prompts for generating high-quality, tailored outputs, all without requiring access to the API's internal structure or parameters.


\textbf{Potential Generalizability to Other Tasks:}
Due to the general applicability of our method, the UPAM framework has the potential to be extended to non-T2I tasks with appropriate modifications, so long as a target output image type is defined for optimization. Here, we provide several potential applications where UPAM can be applied beyond T2I systems for future research. These include: (1) Attack on text-to-video generation models \cite{miao2024t2vsafetybench}. The same principles used to attack static image generation could be extended to sequence generation and temporal consistency in videos. (2) Attack on image-to-image translation models \cite{yeh2021attack}. Given the targeted output in image-to-image tasks, our method could be adapted to force the model to produce incorrect or distorted images by introducing adversarial artifacts in the input visual space. (3) Attack on cross-modal retrieval models \cite{li2021adversarial}. UPAM can be adapted to retrieval task to generate adversarial prompts that misguide the retrieval process, highlighting vulnerabilities in systems that depend on cross-modal alignment.









%==================  8 Conclusion =====================
\section{Conclusion}

In this paper, we introduce UPAM, a unified attack framework against T2I models, which outperforms existing enumeration methods by targeting both textual filters and visual checkers through gradient-based optimization.
To achieve this, we propose the SPL scheme for gradient estimation without image feedback, the SEL scheme to ensure harmful semantics, and the INE scheme to enhance prompt naturalness. Additionally, the TAL scheme enables UPAM to operate with minimal queries.
Comprehensive experiments validate the effectiveness, efficiency, naturalness, and low query requirements of our approach.






% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section
\clearpage
\normalem
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,reference}









% that's all folks
\end{document}


