\section{Related Work}
%---------------   2 .1  --------------------
\subsection{Adversarial Whitebox and Blackbox Attacks}

Early works **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** revealed the susceptibility of Deep Neural Networks (DNNs) to adversarial attacks, which involve crafting input adversarial examples designed to mislead models into making incorrect predictions. 
Such attacks can significantly compromise model performance, leading to issues such as misclassification. 
Adversarial attack strategies are broadly categorized into white-box and black-box settings, depending on the attacker's access to the target model **Kurakin et al., "Adversarial examples in the physical world"**.
In the white-box setting, attackers possess complete knowledge of the model, including its architecture and parameters. This allows them to employ gradient-based optimization **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"** to craft adversarial examples aligned with their attack objectives **Papernot et al., "Practical Black-Box Attacks against Machine Learning Models"**. While highly effective, these methods depend on full model access, an assumption that is rarely feasible in real-world scenarios.
Nevertheless, most current threat models still rely on the white-box setting, which significantly limits their relevance for real-world applications, particularly for text-to-image (T2I) systems that are typically deployed as black-box APIs. 
In recent years, some progress has been made in black-box attacks **Chen et al., "One-Pixel Attack for Fooling Deep Neural Networks"** , where attackers only have access to model outputs without insight into the architecture or parameters.
These methods rely on querying the model and using its outputs as feedback to iteratively refine adversarial examples.


Unlike most adversarial attack studies, we focus on enabling the gradient-based attack in a more constrained black-box setting, where the defensive mechanisms in T2I models can result in no outputs for feedback (no image results returned).




%---------------   2 .2  --------------------
\subsection{Adversarial Attacks on LLMs and VLMs}


Despite significant progress in safety assurance, large language models (LLMs) and vision-language models (VLMs) remain susceptible to carefully crafted inputs, known as adversarial attacks, which can manipulate them into generating unsafe content **Li et al., "Adversarial Text Attack on Large Language Models"**. There are several preliminary attempts to explore adversarial attack on LLMs and VLMs.
As for attack against LLMs, Li et al. **Li et al., "Adversarial Text Attack on Large Language Models"** demonstrated a multistep adversarial prompting capable of extracting sensitive personal information from LLMs. 
Automated adversarial attacks are also gaining traction, with Deng et al. **Deng et al., "Reverse-engineering the Defense Mechanisms of LLMs"** proposing a time-based reverse-engineering method to uncover defense mechanisms and streamline attack strategies across different LLMs.
Zou et al. **Zou et al., "Adversarial Attacks on Open-source Language Models"** proposed a simple yet effective attack method that employs greedy and gradient-based techniques to automatically generate adversarial suffixes for attacking open-source (white-box) language models, while also demonstrating the transferability of these adversarial prompts to black-box LLMs.
Additionally, Wei et al. **Wei et al., "Adversarial Misuse of Large Language Models"** highlighted that large language models remain vulnerable to adversarial misuse due to two key safety-training limitations: competing objectives and mismatched generalization. Leveraging these two failure modes, they devised a framework for designing adversarial prompts.
As for attack against VLMs, Shayegani et al. **Shayegani et al., "Adversarial Image Attacks on Vision-Language Models"** introduced a method for crafting adversarial images within the visual-textual joint embedding space, aiming to subtly influence model outputs.
Qi et al. **Qi et al., "Universal Jailbreak of Vision-integrated LLMs"** highlighted the increasing adversarial risks of vision-integrated LLMs, showing how a single visual adversarial example can universally jailbreak aligned models, exposing broader security vulnerabilities tied to multi-modal AI systems.
Hubinger et al. **Hubinger et al., "Persistent Deceptive Behaviors in VLMs"** pointed out the ongoing challenge of persistent deceptive behaviors, which remain evident even after the application of safety training techniques. 
Concurrently, Yang et al. **Yang et al., "Agent Backdoor Attacks on LLMs and VLMs"** developed a framework for agent backdoor attacks and demonstrated its effectiveness across two representative agent tasks.



Compared to the rapid progress in attacking LLMs and VLMs, research on attacks against T2I models remains in its early stages, with fewer and less advanced methods. Given the severe potential impact of spreading harmful images, we believe that investigating attacks on T2I models holds substantial research importance.






%---------------   2 .3  --------------------
\subsection{Adversarial Attacks on T2I Models}



Adversarial attack on T2I models remains a relatively under-explored area. The goal of this task is to craft adversarial prompts that bypass the defense mechanisms of T2I models, causing them to generate outputs they are designed to block. Most existing attack methods rely heavily on enumeration, wherein candidate prompts are iteratively generated and tested to identify a successful one. Several prior works have explored this domain, including **Daras et al., "Vulnerabilities of DALL·E 2"**.
Specifically, Daras et al. **Daras et al., "Vulnerabilities of DALL·E 2"** proposed a pioneering work that investigated vulnerabilities of DALL·E 2 . 
They found that it is possible to deceive textual filters by using unnormal text as prompts. 
Later, Milli{\`e}re et al. **Milli{\`e}re et al., "Adversarial Prompts for T2I Models"** proposed to create adversarial prompts by combining multi-language sub-word segments.
Struppek et al. **Struppek et al., "Character-level Modification Attacks on T2I Models"** demonstrated that replacing the original text with non-Latin letters can induce different visual contents in generated images. 
Liu et al. **Liu et al., "Adversarial Prompt Generation for T2I Models"** devised a character-level modification strategy (e.g., replacing, deleting, or swapping characters) to transform  the naive prompt into an adversarial one.
Yang et al. **Yang et al., "Multimodal Adversarial Attacks on T2I Models"** proposed to directly change the word tokens that are identified as sensitive (harmful) into non-sensitive (safe) ones with the help of reinforcement learning.
Yang et al. **Yang et al., "Multimodal Adversarial Attacks on T2I Models"** present a multimodal attack, which inputs both an adversarial text prompt and an adversarial image into T2I diffusion models.


In contrast to previous works that focus on deceiving the textual filter only, our paper proposes a unified attack framework UPAM, which effectively deceives both the textual filter and the visual checker simultaneously. 
Unlike previous methods using enumeration for massive attempts, our UPAM allows the training of the parameterized model based on gradients, where the (parameterized) attack model can learn how to adaptively bypass both the textual filters and visual checkers, which brings much effectiveness.
Moreover, unlike previous approaches that need to undertake time-consuming attempts during inference, we enable the gradient-based optimization for parameterized model training. The trained parameterized model allows for fast inference, which brings much efficiency.
Additionally, previous attack methods often use unnatural prompts, like ``Apoploe vesrreaitais'' for bugs'' **Daras et al., "Vulnerabilities of DALL·E 2"** , reducing naturalness and making attack prompts easier to identify. 
In contrast, our UPAM is designed to enhance the naturalness of the generated adversarial prompts. 
Besides, previous methods mostly overlook a practical issue: they require iteratively querying the target T2I API to gather information for adjusting adversarial prompts. Such numerous queries can easily be detected by the API defender **Daras et al., "Vulnerabilities of DALL·E 2"**.
Different from these methods, our UPAM efficiently performs the attack requiring only few-shot queries.




% ==============================Figure 3
\begin{figure*}[t]
\centering
\includegraphics[width=0.85\textwidth]{images/pipeline_new.pdf} 
\vspace{-2mm}
\caption{Overview of our UPAM framework. 
Initially, due to the lack of training, our adversarial prompt $\mathbf{T}^{\ast}$ is unable to bypass the API's defenses (i.e., return no image).
At the pre-training phase, we propose an SPL scheme that estimates the gradient $\pmb{g}_\mathrm{spl}$ under the challenging no-result scenario, finally compelling the black-box T2I model to return images.
At the fine-tuning phase, we utilize an SEL scheme to provide the gradient $\pmb{g}_\mathrm{sel}$ to align the returned images $\mathbf{I}^{\ast}$ with the target semantics.
}
\vspace{-1mm}
\label{fig:pipeline}
\end{figure*}
% ==============================Figure 3