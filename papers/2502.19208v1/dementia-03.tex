\section{Dataset Creation}
\label{sec:dataset}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/datasetcreation.jpg} 
    \caption{Flowchart illustrating the process of dataset generation. }
    \label{fig:process}
\end{figure*}




\begin{table}[t]  
  \caption{Distribution of diagnostic categories (AD, MCI, HC) across languages with corresponding train and test splits. } %
  \captionshrink
  \centering
  \begin{tabular}{l|rrr|rrr}
    \toprule
    \multirow{2}{*}{\textbf{Language}} &  \multicolumn{3}{c|}{\textbf{Train Split}} & \multicolumn{3}{c}{\textbf{Test Split}} \\
    &  AD & MCI & HC & AD & MCI & HC \\
    \midrule
    Spanish &   78 & 70 & 156 &  20 & 18 & 39 \\
    Chinese &   93  & 200 & 187 &  23 & 51&47 \\
    Greek   &   266 & 51 & 87 & 66 & 13 & 22 \\
    English &  512 & 239 & 1450 & 63 & 60 & 87 \\
     \midrule
    \textbf{Total} &  \textbf{949} & \textbf{560} & \textbf{1880} & \textbf{172} & \textbf{142} & \textbf{195} \\
    \bottomrule
  \end{tabular}
  \label{tab:statistics_table}
\end{table}


\dataset{} is constructed from 16 individual datasets presented in Table~\ref{tab:datasetdescription}, which vary significantly in format, modality, and associated metadata. To create a unified multilingual dataset, we performed a series of preprocessing and normalization steps. This involved converting data to a standard format, transcribing audio data, extracting relevant metadata, standardizing data structures, and performing language-specific text cleaning. Figure~\ref{fig:process} illustrates the process. The overall goal was to create a consistent and well-structured dataset suitable for training and evaluating multilingual AD detection models.
A detailed breakdown of the number of conversation transcripts in each dataset, along with the train-test split statistics, is provided in Table~\ref{tab:statistics_table}.

\subsection{Preprocessing}

\subsubsection{Data Conversion}

The datasets exist in various formats, reflecting differences in modalities and data sources. All text-based datasets from DementiaBank\footnote{\url{https://dementia.talkbank.org/}} are provided in the CHAT format~\cite{macwhinney2017tools}. The Chinese iFlyTek text-based dataset is available in TSV format. All audio files are provided in WAV format.

\subsubsection{Audio-to-text Transcription}

Several datasets consisted solely of audio recordings. We transcribed them using a state-of-the-art Transformer-based multilingual speech recognition system.
Specifically, we employed the Whisper-Large multilingual model (\texttt{Large-V3})~\cite{radford2023robust} to transcribe recordings in Chinese, Greek, and Spanish. For speaker diarization, we implemented whisper-large-v3 along with NVIDIA's NeMo, as NeMo offers a robust framework for distinguishing speakers within a conversation. However, upon manual inspection, we found that the diarization results were often inaccurate, likely due to background noise or suboptimal audio quality. Consequently, we omitted the speaker diarization step and focused solely on transcribing the audio files. 

\subsubsection{Metadata Extraction}
We extracted all relevant information, including patient IDs, cognitive scores, demographic details, and conversational data—and standardized them into a unified format. Mini-Mental State Examination (MMSE) \cite{woodward2005mini} and MoCA \cite{nasreddine2005montreal} are two commonly reported cognitive scores in the datasets used for this study. MMSE is a widely used cognitive screening tool designed to assess cognitive impairment, particularly in dementia. A score below 24 typically indicates the need for further evaluation, though factors such as age, education, and cultural background can influence interpretation \cite{woodward2005mini}. MoCA is a cognitive screening tool designed to detect MCI by evaluating eight cognitive domains through rapid and sensitive tasks. A score below 26 indicates cognitive impairment, with the MoCA demonstrating higher sensitivity than the MMSE in detecting early cognitive decline \cite{nasreddine2005montreal}.
This information was typically provided in supplementary files, most commonly in Excel format. The dataset metadata, including details on the original data modality, underlying task, dataset name, language, test date, test duration, and testing environment, is also incorporated into the final normalized dataset. Comprehensive documentation of this process can be found in the GitHub repository accompanying this work.


\subsubsection{Data Standardization and Dataset Combination}

To facilitate integration into a single multilingual dataset, all extracted data and transcriptions were stored as JSONL files. Certain dataset-specific considerations were made during standardization: for the Pitt Corpus, only transcripts from the CTP task were included, and for the Chinese NCMMSE dataset, only the training set was used due to test set label unavailability. Datasets within the same language were combined, resulting in four standardized datasets in English, Spanish, Chinese, and Greek. To ensure fair comparisons, an 80\%-20\% train-test split was applied across all datasets, with the exception of the WLS dataset, which was used exclusively for training.

\paragraph{WLS dataset}
Given its extensive number of transcripts and its demonstrated utility in AD detection, as highlighted by \citet{guo2021crossing}, we incorporated the WLS dataset to enhance model performance even if it does not include explicit dementia diagnoses but provides cognitive test scores and health-related responses. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/pipeline.jpg} %
     \caption{Flowchart depicting the experimental scenarios. Monolingual;  Combined-Multilingual; Combined-Translated}
    \label{fig:variants}
\end{figure*}


Participants in the WLS dataset completed two verbal fluency tasks, naming words in the categories of animals and food within one minute. Verbal fluency tests are highly sensitive to AD detection, with a suggested screening cutoff of 15 words in the animal category \cite{guo2021crossing}. To establish a comparable control subgroup within WLS, we applied age- and education-adjusted verbal fluency thresholds: 16 words for participants under 60, 14 words for those aged 60–79, and 12 words for individuals over 79. Prior studies indicate that animal fluency scores below 15 are strongly indicative of AD, with a sensitivity of 0.88 and specificity of 0.96 \cite{guo2021crossing}. Since no normative data exist for the food category, we adopted the same thresholds as the animal category due to their similar score distributions, as suggested by \citet{guo2021crossing}.



\subsection{Language-Specific Text Preprocessing}

We developed four distinct text preprocessing pipelines, one for each language, to process the unified datasets generated during data collection. A major part of the preprocessing effort involved removing unnecessary symbols and marks in the transcripts, such as \texttt{*}, \&=laugh, and \&=nodes. Additionally, we created visualizations of transcript lengths and their frequency distributions to identify and remove potential outliers. The unified dataset comprises both the raw and preprocessed transcripts.



\subsection{Data Augmentation}

To broaden the usability of the dataset and facilitate addressing the research objectives outlined in the introduction, we augment it by including an English translation of Spanish, Greek, and Chinese texts.
We perform the translation with help of GPT-4 model accessed via the OpenAI API. Specifically, we used the prompt: 
\begin{small}
\begin{verbatim}
Translate the following {source_lang} text to {target_lang}: 
{text}
Translation:
\end{verbatim}
\end{small}
To ensure accuracy and consistency, we set the temperature parameter to zero, minimizing variability in responses.




\subsection{Dataset Variants}
\label{sec:dataset:variants}

We define three dataset variants to enable a range of evaluation scenarios, investigating the impact of multilingual data and translation on model performance. Figure~\ref{fig:variants} illustrates the corresponding experimental scenarios using these dataset variants.

\begin{itemize}
    \item \textbf{Monolingual:} Models are trained and evaluated solely on the train and test sets of the \emph{same} language. This represents a baseline scenario where only data from a single language is used.

    \item \textbf{Combined-Multilingual:}  Models are trained on a combination of the training sets from 
    \emph{all} languages.  This allows the model to potentially learn cross-lingual patterns.  Evaluation is performed on a single language's test set at each iteration.

    \item \textbf{Combined-Translated:} We utilize the machine-translated versions of all non-English datasets. The model is then trained on the combined English and English-translated training sets, while evaluation is performed on the translated test sets. This setting tests the effectiveness of using translation to create a unified English-only dataset.
\end{itemize}




