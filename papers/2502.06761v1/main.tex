
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% nico: I am using this for ArXiv
\usepackage[accepted]{arxiv_icml_style}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% nico
% \usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{soul} % highlight
\usepackage{subcaption}
\usepackage{float}

% % nico - colors
% color_nadam = "#2f4b7c"
% color_lawa = "#87CBB9"
% color_ema = "#b77bc9"
\definecolor{color_nadam}{HTML}{2f4b7c}
\definecolor{color_lawa}{HTML}{87CBB9}
\definecolor{color_ema}{HTML}{b77bc9}
\definecolor{color_lawa_no_decay}{HTML}{FFB4A2}
\colorlet{color_nadam_transparent}{color_nadam!50}
\colorlet{color_lawa_transparent}{color_lawa!50}
\colorlet{color_ema_transparent}{color_ema!50}

\definecolor{cornflowerblue}{rgb}{0.39, 0.58, 0.93}
\hypersetup{
    colorlinks=true,
    linkcolor=cornflowerblue,
    filecolor=magenta,      
    urlcolor=color_ema,
    citecolor=cornflowerblue,%teal,
    pdftitle={When, Where and Why to Average Weights?},
    pdfpagemode=FullScreen,
    }

\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Subsection}

%% Default is 50
% \interlinepenalty=500
% \hyphenpenalty=500
% \exhyphenpenalty=500


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{When, Why and How to Average Weights?}
\icmltitlerunning{When, Where and Why to Average Weights?}

\begin{document}

\twocolumn[
\icmltitle{When, Where and Why to Average Weights?}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Niccolò Ajroldi}{ellis,mpi}
\icmlauthor{Antonio Orvieto}{ellis,mpi,tuai}
\icmlauthor{Jonas Geiping}{ellis,mpi,tuai}
\end{icmlauthorlist}

\icmlaffiliation{ellis}{ELLIS Institute Tübingen}
\icmlaffiliation{mpi}{Max-Planck Institute for Intelligent Systems}
\icmlaffiliation{tuai}{Tübingen AI Center}

\icmlcorrespondingauthor{Niccolò Ajroldi}{niccolo@tue.ellis.eu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
  Averaging checkpoints along the training trajectory is a simple yet powerful approach to improve the generalization performance of Machine Learning models and reduce training time. Motivated by these potential gains, and in an effort to fairly and thoroughly benchmark this technique, we present an extensive evaluation of averaging techniques in modern Deep Learning, which we perform using AlgoPerf \citep{dahl_benchmarking_2023}, a large-scale benchmark for optimization algorithms. We investigate whether weight averaging can reduce training time, improve generalization, and replace learning rate decay, as suggested by recent literature. Our evaluation across seven architectures and datasets reveals that averaging significantly accelerates training and yields considerable efficiency gains, at the price of a minimal implementation and memory cost, while mildly improving generalization across all considered workloads.
  Finally, we explore the relationship between averaging and learning rate annealing and show how to optimally combine the two to achieve the best performances.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

% {\color{magenta} This short intro strategy is nice but you do not want to sound naive. you cite a lot in RW but you should also do it here, and emphasize the difference to previous work. Assume people will question your novelty and want to find here something exciting. You should not just phrase questions but answer them. You can phrase these questions as a motivation and in your contribution summarize findings. A point to make is how much compute hours are needed to train with AdamW (sum over all tasks). this gives the reader the concrete idea that we are indeed not toy at all and nobody has come close.}
% {\color{magenta}I would also say that your experiments suggest a new workflow. Also, I would point to the fact that there is no paper comparing fixed budget and target loss. I would also say that this is meant to be the ultimate reference for practitioners and i would write down exactly how many GPU hours this took!}


Training Deep Learning models is both resource-expensive and time-consuming.
A principled and simple approach to speed up training, dating back to \citet{polyak_new_1990} and \citet{david_ruppert_efficient_1988}, involves averaging the model's weights across training iterations. This can either be performed online during training or post hoc by averaging checkpoints, effectively creating an ensemble of models at minimal additional cost. 
Previous studies have shown that weight averaging (WA) techniques can improve generalization \citep{merity2017regularizingoptimizinglstmlanguage, Gupta2020Stochastic, kaddour_stop_2022, melis_two-tailed_2023}, increase robustness \citep{morales_ema}, smooth loss landscapes \citep{izmailov_averaging_2019}, and accelerate convergence \citep{athiwaratkun_there_2018, li_trainable_2022, sanyal_early_2023}.
Recent studies have also explored the connection between learning rate decaying and weight averaging \citep{sandler_training_2023, hagele2024scaling}, and used the latter to develop schedule free algorithms \citep{defazio_road_2024}.

\begin{table}[t]
\caption{Estimated training cost for one run on the AlgoPerf collection of models and tasks. Even for industrial-scale tasks like benchmark workloads, weight averaging reliably reduces compute costs by hundreds of GPU hours.}
\label{tab:small_table}
% \vskip -1in
\begin{center}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{lccc}
\toprule
 & \textcolor{color_nadam}{NadamW} & \textcolor{color_lawa}{+LAWA} & \textcolor{color_ema}{+EMA} \\
\midrule
GPU-Hours  & 636 & 548 & 540 \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-.3cm}
\end{table}

In this work, we present the largest evaluation of averaging techniques in modern Deep Learning, which we perform using AlgoPerf \citep{dahl_benchmarking_2023}, a collection of large-scale workloads and architectures developed to provide a unified benchmark for optimization algorithms.
Framing our analysis in this setting provides a carefully designed evaluation framework, enables comparisons against strong, heavily tuned baselines, and allows drawing broad and robust conclusions.

Building on previous work, we investigate the following questions.
(i) Can weight averaging \textit{reduce training time} across multiple models and tasks? (ii) Does averaging checkpoints \textit{improve the generalization} performance of existing optimization algorithms? (iii) Is weight averaging merely a proxy for a shorter learning rate decay schedule, and can it fully replace learning rate decay?

Our contributions are as follows. 
\vspace{-1mm}
\begin{enumerate}
    \item We show that averaging can significantly accelerate training dynamics across seven different models and architectures and estimate a 15\% reduction in GPU-hours to train the entire AlgoPerf suite up to the validation target compared to our baseline. We find this effect to be consistent across hyperparameters and observe encouraging speed-ups even on second-order optimizers \citep{shi2023distributed_shampoo}. 
    % \vspace{2em}
    \item Beyond efficiency gains, we demonstrate how averaging achieves improved generalization across all the considered workloads, and show that coupling WA with learning rate annealing yields optimal results.%, a novel finding to the best of our knowledge. 
    \item Finally, we demonstrate how averaging checkpoints can act as a proxy for a shorter learning rate decay, but that it \textit{cannot} fully replace learning rate schedules, at least within the explored variants, addressing an important question about the role of these techniques \citep{hagele2024scaling, defazio_road_2024}. 
\end{enumerate}

% Examining seven different models and datasets, we show that (i) averaging can significantly accelerate training dynamics, and estimate a 15\% reduction in GPU-hours to train the entire AlgoPerf suite up to the validation target compared to our baseline. We find this effect to be consistent across hyperparameters and observe encouraging speed-ups even on second-order optimizers \citep{shi2023distributed_shampoo}. 
% Beyond efficiency gains, we (ii) demonstrate how averaging achieves improved generalization across all the considered workloads, and show that coupling WA with learning rate annealing yields optimal results, a novel finding to the best of our knowledge. 
% Finally, we (iii) demonstrate how averaging checkpoints can act as a proxy for a shorter learning rate decay, but that it \textit{cannot} fully replace learning rate schedules, at least within the explored variants.

The paper is organized as follows: \autoref{sec:related_work} reviews related work on weight averaging; \autoref{sec:experimental_setup} discuss the methodology and experimental setup; \autoref{sec:speeding_up_training}, \ref{sec:improve_generalization}, and \ref{sec:averag_vs_lr_decay} presents our findings. \autoref{sec:limitations} addresses the limitation of our analysis and \autoref{sec:conclusions} concludes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[ht]
    \centering
    % Legend
    \begin{minipage}{0.7\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/loss_plot/legend.pdf}
    \end{minipage}
    
    % First row
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/loss_plot/ogbg_fig1.pdf}
        \subcaption{OGBG}
        \label{fig:step_to_target_ogbg}
    \end{minipage}
    % \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/loss_plot/wmt_fig1.pdf}
        \subcaption{WMT}
        \label{fig:step_to_target_wmt}
    \end{minipage}
    
    % \vspace{1em} % Space between rows
    
    % Second row
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/loss_plot/librispeech_conformer_fig1.pdf}
        \subcaption{Librispeech Conformer}
        \label{fig:step_to_target_conformer}
    \end{minipage}
    % \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/loss_plot/imagenet_vit_fig1.pdf}
        \subcaption{Imagenet ViT}
        \label{fig:step_to_target_vit}
    \end{minipage}
    \caption{Weight averaging speeds up training across all considered workloads. The averaged schemes consistently achieve better performance during training, reaching the validation score target faster than the baseline algorithm. We display the validation score against the number of iterations across different workloads; the dotted line represents the target score on each workload.}
    \label{fig:step_to_target}
\end{figure*}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{img/step_to_target_all_workloads_lawa_ema.pdf}
    \caption{LAWA and EMA speed up convergence across several architectures and datasets. Both averaging schemes consistently outperform the baseline, achieving on average the benchmark target score using 78\% of the steps required by NadamW. 
    We estimate a 15\% reduction in GPU-hours to train the entire AlgoPerf suite of workloads with respect to NadamW.
    }
    \label{fig:results_speedup_nadamw}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:related_work}

The idea of averaging iterates along a stochastic optimization trajectory dates back to \citet{polyak_new_1990} and \citet{david_ruppert_efficient_1988}, and is often referred to as Polyak–Ruppert averaging.
It has been extensively studied in the stochastic approximation framework, \citep{polyak_acceleration_1992, bach_non-strongly-convex_2013, neu_iterate_2018, lakshminarayanan_linear_2018}, and is a common technique to derive convergence guarantees in both convex and nonconvex optimization~\citep{garrigos2023handbook}. 

\vspace{-2mm}
\paragraph{Deep Learning applications.}
Weight averaging techniques have seen extensive use in the Deep Learning community, often without explicit recognition or emphasis, despite their effectiveness. Influential works such as \citet{szegedy_2016_rethinking}, \citet{vaswani2017attention}, and \citet{merity2017regularizingoptimizinglstmlanguage} have demonstrated their ability to enhance model performance and mitigate overfitting.


\vspace{-2mm}
\paragraph{SWA.}
The work of \citet{izmailov_averaging_2019} sparked renewed interest in weight averaging by demonstrating how averaging points along the SGD trajectory leads to wider minima and improves generalization performance. Their approach, Stochastic Weight Averaging (SWA), has since been applied to semi-supervised learning \citep{tarvainen2018meanteachersbetterrole, athiwaratkun_there_2018}, low-precision training \citep{yang_swalp_2019}, domain generalization tasks \citep{cha_swad_2021}, and meta-optimization \citep{li_trainable_2022}.

\vspace{-2mm}
\paragraph{LAWA and EMA.} In the original formulation of SWA, a pretrained model is trained with a cyclical or constant learning rate, and multiple checkpoints are collected and later averaged.
\citet{kaddour_stop_2022} proposed Latest Weight Averaging (LAWA), an online algorithm that averages the latest checkpoints in a rolling window, showing significant speed-ups on vision and language tasks. Further modifications of LAWA demonstrated notable boosts in pretraining modern decoder-only language models \citep{sanyal_early_2023}. %, and very recently \citet{DeepSeekV3} preserved an exponential moving average of the model parameters in CPU memory to estimate the model performance after learning rate decay. 
A valuable alternative to moving window averaging techniques like LAWA is Exponential Moving Averaging (EMA) \citep{li_switch_2024, morales_ema}. It retains similar advantages of rolling window averaging, and constitutes an indispensable technique for high-quality image synthesis models such as GANs and diffusion models \citep{yazıcı2019avg_gan, song2021score_based_gen, karras2024nvidia_diffusion_avg}.

\vspace{-2mm}
\paragraph{Connection to learning rate annealing.} 
Classical stochastic smooth convex optimization rates showcase a tight link between WA and learning rate annealing, suggesting a practical interplay between these techniques~(see e.g. Theorem 5.3. in~\citet{garrigos2023handbook}).
Intuitively, averaging weights along the training trajectory reduces noise and might act as a proxy for learning rate decay. In fact, \citet{sandler_training_2023} proved the theoretical and empirical equivalence between WA and decaying learning rate for SGD. 
However, despite this appealing result, 
% a thorough analysis of the interplay between averaging and learning rate scheduling on modern tasks and optimization algorithms is still missing, and bridging the gap between the two is an open research problem \citep{hagele2024scaling}: 
modern Deep Learning models are still predominantly trained with learning rate annealing \citep{hagele2024scaling}, even when maintaining an EMA of model weights \citep{DeepSeekV3}. 
A recent study by \citet{defazio_road_2024} specifically investigates this connection, proposing an approach that fully replaces learning rate schedules with iterate averaging and demonstrating strong performance on the same benchmark used in this analysis.
Whereas \citet{defazio_road_2024} incorporates averaging directly into the optimization procedure, we explore a different flavor of averaging, where the averaged weights do not influence the updates—akin to Polyak averaging, SWA, LAWA, and EMA.
% A recent effort to fully replace learning rate schedules with iterate averaging was developed by \citet{defazio_road_2024}, demonstrating strong performance on the same benchmark used in this analysis.

\vspace{-2mm}
\paragraph{Model soups.} Finally, a different but notable approach that leverages the benefits of averaging is model soups \citep{wortsman_model_2022}. In this case, multiple models are trained with different hyperparameter configurations and later aggregated, resulting in an ensemble with improved accuracy and robustness.

In this work, we demonstrate the benefits of weight averaging techniques on a challenging optimization benchmark \citep{dahl_benchmarking_2023}, hoping to encourage broader adoption of these methods in training large-scale Machine Learning models.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Setup}
\label{sec:experimental_setup}


\paragraph{Comparing optimization algorithms.}
When evaluating optimization algorithms, two strategies are possible:
\vspace{-5pt}
\begin{enumerate}\setlength{\itemsep}{3pt}%\setlength{\parskip}{1pt}\setlength{\topsep}{1pt}
% \begin{enumerate}\setlength{\itemsep}{0pt}
    \item[(A)] Fixing a challenging target loss value and comparing the runtime needed to reach it.
    \item[(B)] Comparing generalization performance within a fixed budget, either specified in number of steps or wall-clock time.
\end{enumerate}
\vspace{-5pt}
In this work, we investigate whether weight averaging improves existing optimization algorithms in \textit{either} of the two described frameworks.

\paragraph{Weight averaging.}
We explore two flavors of weight averaging discussed in \autoref{sec:related_work}: LAWA~\citep{kaddour_stop_2022} and EMA. For both approaches, we update a buffer that stores previous information about the model history. In the first case, we update a circular queue of length $L$, which stores previous checkpoints, while in the latter case, we maintain an exponential moving average of the model parameters, with coefficient $\gamma$. 
To reduce the overhead of the averaging scheme, we update the buffer every $\nu$ optimization steps. 
In most scenarios, we save checkpoints of the baseline algorithm and run averaging schemes offline, but when testing collecting consecutive checkpoints, we use online versions of LAWA and EMA to avoid excessive disk storage (see \autoref{app:experimental_details} for algorithm details). %We report on the computational overhead of using an online averaging algorithm in \autoref{app:overhead}.
%We argue that updating the buffer too often is unnecessary to achieve significant speed-ups and explore the optimal update frequency and its interplay with $L$, and $\gamma$ in \autoref{sec:speeding_up_training} and \autoref{app:ogbg}.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section{Experimental Setup}
% \label{sec:experimental_setup}

% In an attempt to provide a comprehensive benchmark for testing optimization algorithms, \citet{dahl_benchmarking_2023} recently introduced AlgoPerf, a large suite of Deep Learning workloads.

\paragraph{AlgoPerf.}
We conduct our analysis on AlgoPerf \citet{dahl_benchmarking_2023}, a large suite of Deep Learning workloads, which provides a comprehensive benchmark for testing optimization algorithms. The benchmark is composed of eight \textit{workloads}, each defined by a \textit{dataset}, a \textit{model} architecture, and a predefined \textit{target  metric} on a held-out set, designed to represent optimal performance on such a workload.
We note that AlgoPerf has been developed following option (A) and that significant effort has been dedicated to deriving challenging target scores. 
Thus, it provides an ideal setting to evaluate the impact of weight averaging techniques applied to strong optimization algorithms and compare their performance against heavily tuned baselines. We also make use of the same datasets and architectures to score algorithms by means of their generalization capabilities (when following option (B)).
We consider the following workloads from the AlgoPerf suite:
(i) a DLRMsmall model on Criteo 1TB dataset for click-through rate prediction;
(ii) U-Net on FastMRI for medical image reconstruction;
(iii) ViT on ImageNet-1k for image classification;
(iv) a GNN model on OGBG for graph-based prediction;
(v) a Transformer-big on WMT for machine translation;
(vi) a Conformer for speech recognition;
(vii) a DeepSpeech model on LibriSpeech for speech-to-text.
We refer to \autoref{app:experimental_details} for more details on the dataset and the tasks.


\paragraph{Baseline optimizers.}
We build on top of existing optimization algorithms that perform well on AlgoPerf, enhancing them with LAWA or EMA. 
Unless otherwise specified, we use NadamW~\citep{dozat_nadam, Loshchilov2017DecoupledWD} as our baseline and name the resulting algorithms NadamW~+~LAWA and NadamW~+~EMA, respectively.
Additionally, we investigate the potential benefits of combining a second-order optimizer, such as Distributed Shampoo \citep{shi2023distributed_shampoo}, with these averaging methods, exploring how this approach might further improve training efficiency.
Given the high computational cost of the benchmark, we conduct most of the analysis using the best performing hyperparameters in \citet{dahl_benchmarking_2023} and \citet{kasimbeg2025accelerating}: this gives us strong baselines, avoiding the burden of expensive hyperparameter tuning. Additionally, we ablate on the role of the learning rate in each analysis and study in more detail the consequences of changing the learning rate schedule. Given this reference algorithm, we add LAWA or EMA on top of it, tuning only the hyperparameters of the averaging scheme and leaving the other baseline hyperparameters fixed, including learning rate schedule and weight decay. Unless otherwise specified, or when explicitly ablating on it, we train using a cosine learning rate schedule \citep{loshchilov2017sgdr}.

% \vspace{-2mm}
\paragraph{Optimal averaging horizon.} The impact of an averaging scheme largely depends on the \textit{horizon} over which it is applied and on the \textit{frequency} at which checkpoints are collected. A long horizon may prioritize outdated model checkpoints and generalize worse, whereas a short one may be suboptimal or result in a serious computational overhead. At the same time, updating the averaging buffers every step is only possible in the online version of LAWA and EMA, and collecting checkpoints too rarely might reduce the benefits of averaging.
Previous works have collected checkpoints at each training step \citep{morales_ema, hagele2024scaling}, or the end of each epoch \citep{kaddour_stop_2022}, and investigated the optimal horizon and update frequency on a single task \citep{sanyal_early_2023}. We explore combinations and interactions of these variables across architectures and objectives, and compare LAWA with EMA.

To control for variability in model initialization and data shuffling, we repeat experiments for three different seeds and report the mean and standard deviation.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Speeding up Training}
\label{sec:speeding_up_training}

We ask whether equipping a strong baseline with LAWA or EMA can reduce the number of steps required to reach a predefined validation target~(option A in Section~\ref{sec:experimental_setup}). To evaluate this, we track the \textit{number of steps} required to reach the benchmark score on the held-out set. The target scores, derived in \citet{dahl_benchmarking_2023} by heavily tuning and comparing 4 different optimization algorithms, represent desirable performance on each workload.

\paragraph{Efficiency gains of averaging.} When looking for a scheme that reduces training time, we observe that both LAWA and EMA can significantly speed up training with respect to the considered baseline (\autoref{fig:step_to_target}). %Among the two averaging schemes, ... TODO
If a target validation score is known in advance, weight averaging can effectively be employed to trigger early stopping and significantly reduce computational costs. We estimate a 15\% reduction of GPU hours for training AlgoPerf using LAWA on top of NadamW.

% Among the two averaging schemes, we find that EMA compares favorably against N
% Among the two averaging schemes, LAWA is more sensible to hyperparameter tuning, and results in worst performance when collecting checkpoints at high frequency ($\nu=1$, \autoref{fig:optimal_horizon}), whereas EMA emerges as a more attractive solution, exhibiting higher resilience to hyperparameter tuning. %We further observe that EMA benefits greatly from being initialized later in training. Excluding the initial part of training, akin to Latest weight averaging, enhances the performance of the averaged model, achieving the target score sooner. 


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/scatter_horizon.pdf}
    \caption{Impact of averaging horizon on training efficiency.
    % across workloads
    Moderate weight averaging generally reduces the number of steps needed to reach the validation target. However, excessive averaging (rightmost region) or overly small averaging windows (leftmost region) may diminish gains or hinder progress. We use LAWA for this analysis, and define the averaging window proportion as $\frac{\nu \times L}{T}$ ($x$-axis), where $T$ denotes the total training budget, $\nu$ is the number of steps between consecutive checkpoints, and $L$ is the span of the averaging window. For each workload, we fit and display a second order polynomial: $y = a x^2 +b x + c$.}
    \label{fig:scatter_horizon_lawa}
\end{figure}

\begin{figure}[t]
    \centering
    % First row
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/lr_sweep_step_to_target_criteo1tb.pdf}
        \subcaption{Criteo1TB}
    \end{minipage}
    % \hfill
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/lr_sweep_step_to_target_ogbg.pdf}
        \subcaption{OGBG}
    \end{minipage}
    \caption{Averaging weights of a suboptimal baseline. We train using NadamW, varying the top learning rate, and compare the number of steps needed to reach the validation target.}
    \label{fig:lr_sweep_step_to_target}
\end{figure}

\paragraph{Averaging horizon.} We further investigate the relationship between the optimal averaging window and training efficiency. \autoref{fig:scatter_horizon_lawa} reveals a consistent trend across workloads: whereas moderate weight averaging significantly reduces the number of steps required to reach the validation target, both excessively small and excessively large horizons can be suboptimal, resulting in minor speed-ups. Importantly, we find that LAWA is often highly stable and forgiving with respect to the choice of its hyperparameters, considering that for each workload a broad range of horizons are effective. This robustness is particularly evident in OGBG and FastMRI, where performance remains relatively stable across a wide range of averaging windows. We attribute this to their long training horizons, as also noted in \citet{kasimbeg2025accelerating}.
Furthermore, our analysis suggests a generalizable optimal averaging horizon across workloads: an averaging window around 1\% of the total training budget consistently yields optimal results. %This finding provides a practical guide for selecting an averaging horizon without extensive hyperparameter tuning.

\paragraph{Stability to hyperparameter tuning.}
% An important question is whether weight averaging can provide considerable speed-ups \textit{when an optimal baseline is not available}. 
An important question is whether these gains are to be observed only at peak tuning of an optimizer, or if they hold in generic hyperparameter settings. We investigate this circumstance by variating the top learning rate of NadamW and averaging the correspondent checkpoints. 
We notice that LAWA maintains its efficiency gains across all considered learning rates, reliably achieving the target score before the baseline optimizer, as shown in \autoref{fig:lr_sweep_step_to_target}. 
This result shows that averaging remains effective in improving efficiency, even when applied to a suboptimal optimization algorithm. This is promising, as it suggests benefits even in the absence of a strong baseline or when searching for one is too costly.
% Interestingly, we notice that, as long as the baseline algorithm is able to reach the predefined target, weight averaging consistently speeds up convergence (\autoref{fig:lr_sweep_step_to_target_wmt}). If instead the learning rate is too big (or too small) and the optimizer does achieve the target in the maximum number of iterations, neither do the averaged checkpoints. This suggests that while averaging can significantly accelerate training, it has a limited impact on improving generalization, a topic that we explore further in the next section.

\paragraph{Averaging Shampoo.} Finally, we explore whether a more sophisticated optimizer can benefit from weight averaging. We choose Distributed Shampoo \citet{shi2023distributed_shampoo} as the best scoring algorithm on AlgoPerf \citet{kasimbeg2025accelerating}, which provides a significant speed-up over NadamW. We report in \autoref{fig:wmt_shampoo} the validation BLEU when training an Encoder-Decoder Transformer on WMT using Distributed Shampoo equipped with LAWA. We observe that LAWA consistently enhances performance, even when applied to an already highly efficient optimizer like Shampoo. This remarks that the benefits of weight averaging extend beyond compensating for slower or less effective optimizers, and indicates that averaging offers inherent benefits, independent of the adopted optimization algorithm. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{img/wmt_shampoo.pdf}
    \caption{Weight averaging provides significant speed-ups also when applied on top of a second-order optimizer like Shampoo. Averaging checkpoints through LAWA reduces the number of iterations required to reach the target BLEU when training a Transformer on WMT.}
    \label{fig:wmt_shampoo}
\end{figure}



% \begin{table*}[t]
% % \caption{Validation performance across workloads. Both averaging schemes provide little benefit with respect to the baseline. They never perform workse.. slightly better... The only significant improvement is on WMT and OGBG}
% \caption{Validation performance across workloads. Both averaging schemes show minimal but consistent improvement over the baseline, always matching or slightly surpassing its performance. We observe notable gains in the WMT and OGBG workloads. We report the mean and standard deviation across 3 seeds.}
% \label{tab:best_performance}
% \vskip 0.1in
% \begin{center}
% \footnotesize % Shrinks text
% \setlength{\tabcolsep}{2pt} % Reduce column spacing further
% % \renewcommand{\arraystretch}{0.85} % Reduce row spacing
% \begin{sc}
% \begin{tabular}{lcccccccc}
% \toprule
%  & Criteo1TB & FastMRI & ViT & Conformer & DeepSpeech & OGBG & WMT \\
% \midrule
%  & Loss $\downarrow$ & SSIM $\uparrow$ & Accuracy $\uparrow$ & WRT $\downarrow$ & WRT $\downarrow$ & MAP $\uparrow$ & BLEU $\uparrow$ \\
% \midrule
% NadamW & $0.124 \pm 0.00003$ & $0.724 \pm 0.00029$ & $0.778 \pm 0.059$ & $0.092 \pm 0.015$ & $0.120 \pm 0.002$ & $0.283 \pm 0.003$ & $30.73 \pm 0.16$ \\
% LAWA & $\mathbf{0.123} \pm 0.00004$ & $\mathbf{0.727} \pm 0.00035$ & $\mathbf{0.780} \pm 0.010$ & $\mathbf{0.087} \pm 0.009$ & $\mathbf{0.116} \pm 0.001$ & $0.298 \pm 0.017$ & $31.44 \pm 0.06$ \\
% EMA & $0.123 \pm 0.00005$ & $0.725 \pm 0.00169$ & $0.780 \pm 0.011$ & $0.088 \pm 0.011$ & $0.116 \pm 0.002$ & $\mathbf{0.299} \pm 0.012$ & $\mathbf{31.47} \pm 0.10$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{center}
% \vskip -0.1in
% \end{table*}

\begin{table*}[t]
\caption{Validation performance across workloads. Both averaging schemes show minimal but consistent improvement over the baseline, always matching or slightly surpassing its performance. We observe notable gains in the WMT workload. We select the best runs for each algorithm, and report the median and the interquartile range across seeds.}
\label{tab:best_performance}
\vskip 0.1in
\begin{center}
\footnotesize % Shrinks text
% \fontsize{8}{10}\selectfont
\setlength{\tabcolsep}{2pt} % Reduce column spacing further
% \renewcommand{\arraystretch}{0.85} % Reduce row spacing
\begin{sc}
\begin{tabular}{lcccccccc}
\toprule
 & Criteo1TB & FastMRI & ViT & Conformer & DeepSpeech & OGBG & WMT \\
\midrule
 & Loss $\downarrow$ & SSIM $\uparrow$ & Accuracy $\uparrow$ & WRT $\downarrow$ & WRT $\downarrow$ & MAP $\uparrow$ & BLEU $\uparrow$ \\
\midrule
NadamW & $0.1237$ ($0.00003$) & $0.724$ ($0.0003$) & $0.771$ ($0.050$) & $0.083$ ($0.01$) & $0.119$ ($0.002$) & $0.28$ ($0.003$) & $30.71$ ($0.20$) \\
LAWA & $\mathbf{0.1235}$ ($0.00003$) & $\mathbf{0.727}$ ($0.0003$) & $\mathbf{0.774}$ ($0.009$) & $\mathbf{0.082}$ ($0.008$) & $\mathbf{0.115}$ ($0.001$) & $\mathbf{0.29}$ ($0.020$) & $31.43$ ($0.06$) \\
EMA & $\mathbf{0.1235}$ ($0.00005$) & $0.724$ ($0.0011$) & $0.773$ ($0.009$) & $\mathbf{0.082}$ ($0.010$) & $\mathbf{0.115}$ ($0.002$) & $\mathbf{0.29}$ ($0.010$) & $\mathbf{31.44}$ ($0.09$) \\
% \midrule
% \midrule
% NadamW & $0.124 \pm 0.00003$ & $0.724 \pm 0.00029$ & $0.778 \pm 0.059$ & $0.092 \pm 0.015$ & $0.120 \pm 0.002$ & $0.283 \pm 0.003$ & $30.73 \pm 0.16$ \\
%  & $\mathbf{0.123} \pm 0.00004$ & $\mathbf{0.727} \pm 0.00035$ & $\mathbf{0.780} \pm 0.010$ &LAWA $\mathbf{0.087} \pm 0.009$ & $\mathbf{0.116} \pm 0.001$ & $0.298 \pm 0.017$ & $31.44 \pm 0.06$ \\
% EMA & $0.123 \pm 0.00005$ & $0.725 \pm 0.00169$ & $0.780 \pm 0.011$ & $0.088 \pm 0.011$ & $0.116 \pm 0.002$ & $\mathbf{0.299} \pm 0.012$ & $\mathbf{31.47} \pm 0.10$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\vskip -0.1in
\end{table*}





\begin{figure*}[ht]
    \centering
    % First row
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/lr_sweep_generalization_criteo1tb.pdf}
        \subcaption{Criteo1TB}
    \end{minipage}
    % \hfill
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/lr_sweep_generalization_ogbg.pdf}
        \subcaption{OGBG}
    \end{minipage}
    % \hfill
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/lr_sweep_generalization_wmt.pdf}
        \subcaption{WMT}
    \end{minipage}
    \caption{Averaging a suboptimal baseline. We train using NadamW, varying the top learning rate, but still decaying it to zero, and compare the validation performance when averaging weights.}
    \label{fig:lr_sweep_generalization}
\end{figure*}

\section{Improving Generalization}
\label{sec:improve_generalization}

When a predefined target is not known \textit{a priori}, or a computational budget is fixed and one does not need to stop training early, weight averaging might be an appealing option to improve generalization. We investigate this setting~(Option B in Section~\ref{sec:experimental_setup}) by fixing a step budget and comparing the \textit{best validation score} achieved during training by the baseline algorithm and by its averaged version. %value of the validation score \textit{at the end of training}.

\paragraph{WA leads to moderate performance gains.}
We report in \autoref{tab:best_performance} and \autoref{fig:generalization_no_decay_boxplot} the results of averaging in terms of generalization performance.
We find that using LAWA or EMA on top of a learning rate schedule consistently improves over the baseline optimizer, achieving slightly better validation scores across all the considered workloads. We notably observe significant improvements on the WMT workload. 
% We notice that the optimal averaging scheme slightly varies with respect to the previous objective, as the best validation performance is usually achieved by using a shorter averaging window for LAWA and a smaller $\gamma$ for EMA. % TODO: check
As previously reported in \citet{kaddour_stop_2022} and~\citet{sanyal_early_2023}, in the early stage of training, averaging schemes provide considerably better performance than the underlying optimization algorithm, but this gap shrinks towards the end of training. We argue that this behavior is closely linked to the use of WA on top of a learning rate schedule, and that the benefits of averaging diminish later in training due to its similarity to learning rate annealing \citep{sandler_training_2023}. We explore this topic in more details in the following section.

\paragraph{Stability to hyperparameter tuning.}
In line with the previous section, we investigate how averaging affects generalization performance across different hyperparameter configurations. This is a common scenario, that may occur when an optimal baseline is not available, or when it is too expensive to search for one. We variate the baseline learning rate and report the achieved validation score with and without averaging. 
We observe in \autoref{fig:lr_sweep_generalization} that the averaged checkpoints closely track the performance of the baseline algorithms, sometimes providing marginal gains. Unlike \citet{sanyal_early_2023}, we do not notice inherent benefits when training at higher learning rates.
%we find that the optimal performance achieved by the baseline algorithm and by averaging stays roughly the same across learning rates. Crucially, we conduct this experiment using a cosine learning rate schedule that anneals the learning rate to $0$ regardless of its maximum value. We argue that the benefit of averaging is closely related to decaying the learning rate \citep{sandler_training_2023, defazio_road_2024}, and further explore this hypothesis in the next paragraph.

% {\color{magenta} This section is a bit scrumbled. I would say first of all that we consider a fixed number of epochs (as algoperf) and different annealing on top of WA. I would say how the results compare with the findings of LAWA and EWA and discuss the dynamics. I would then say that this setting in practice, while standard, assumes you fix the budget and is not practical -- and then I will go to the next section motivating that WA can give a more practical workflow.}


% At higher learning rate?
% Without a schedule?

% \textbf{Takeaway}: averaging checkpoints provides minimal benefits in terms of improved generalization at the end of training...


% Updating the averaging buffer every iteration is time-consuming. We argue that a frequent update would result in diminishing returns due to the frequent moving of weights from CPU to GPU and vice versa, and the averaging scheme would be effective only if collecting checkpoints over a longer time frame (large $L$ for LAWA, large $\gamma$ for EMA). Figure ?? shows the joint effect of changing the update frequency $\nu$, and the memory span of the buffers (queue length $L$ for LAWA and coefficient $\gamma$ for EMA).

% We also explore whether using an averaging strategy such as LAWA improves the resilience to other hyperparameter tuning. This question is particularly relevant for AlgoPerf, where tuning hyperparameters is extremely expensive and time-consuming due to the extensive number and variety of workloads. We 

% EMA can provide faster convergence in terms of steps, but the additional overhead of averaging might limit the advantage of using an EMA. This can however be mitigated by asynchronously transferring the model parameters to CPU, and offload the averaging to the CPU, as done by \citet{DeepSeekV3}.

% \textbf{Conclusion}. EMA is as effective as LAWA. 


\section{Averaging as a Proxy for LR Decay}
\label{sec:averag_vs_lr_decay}


\begin{figure*}[t]
    \centering
    % First row
    \begin{minipage}{0.485\linewidth}  % Reduce width slightly
        \centering
        \includegraphics[width=\linewidth]{img/librispeech_conformer_reduce_budget.pdf}
        \subcaption{Weight averaging of a long (annealed) training run performs similarly to training with shorter learning rate schedules.}
        \label{fig:sweep_horizon}
    \end{minipage}
    \hspace{0.02\linewidth}  % Add horizontal space
    \begin{minipage}{0.485\linewidth}  % Reduce width slightly
        \centering
        \includegraphics[width=\linewidth]{img/librispeech_conformer_decay_less.pdf}
        \subcaption{Averaging checkpoints of training runs with different annealing strategies. The benefits of WA diminish when annealing the learning rate to zero.}
        \label{fig:decay_less_conformer}
    \end{minipage}
    \caption{Weight averaging on top of different learning rate schedules when training a Conformer model on Librispeech.}
    \label{fig:avg_vs_lr_decay}
\end{figure*}


To achieve optimal performance on most Deep Learning tasks, it is common practice to anneal the learning rate during training \citep{defazio2024optimallineardecaylearning}. Despite its ubiquity, this practice prevents continual learning and significantly increases training costs \citep{hagele2024scaling}. In contrast, averaging weights along the training trajectory intuitively reduces noise and might act as a proxy for learning rate decay \citep{sandler_training_2023, defazio_road_2024}. 

In this section, we ask how averaging influences performance when changing the underlying learning rate schedule, and if it can reliably act as a proxy for short LR decay. 


\begin{figure*}[ht]
  \centering

  % Overall horizontal plot (with legend included)
  \begin{minipage}{\linewidth}
    \centering
    \hspace{1em}
    \includegraphics[width=.225\linewidth]{img/generalization_no_decay/legend_dotted.pdf}
    % \subcaption{Overall Plot with Legend}
  \end{minipage}

  \vspace{-4mm}
  
  % Overall horizontal plot (with legend included)
  \begin{minipage}{\linewidth}
    \centering
    \hspace{1em}
    \includegraphics[width=.9\linewidth]{img/generalization_no_decay/legend.pdf}
    % \subcaption{Overall Plot with Legend}
  \end{minipage}

  % \vspace{-1pt}
    
  % First row of grid
  \begin{minipage}{0.32\linewidth}
  % \begin{minipage}{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{img/generalization_no_decay/ogbg.pdf}
    \subcaption{OGBG}
  \end{minipage}
  % \hfill
  \begin{minipage}{0.32\linewidth}
  % \begin{minipage}{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{img/generalization_no_decay/fastmri.pdf}
    \subcaption{FastMRI}
  \end{minipage}
  % \hfill
  \begin{minipage}{0.32\linewidth}
  % \begin{minipage}{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{img/generalization_no_decay/wmt.pdf}
    \subcaption{WMT}
  \end{minipage}

  % \vspace{0.05cm} % Space between rows

  % Second row of grid
  \begin{minipage}{0.32\linewidth}
  % \begin{minipage}{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{img/generalization_no_decay/librispeech_conformer.pdf}
    \subcaption{Conformer}
  \end{minipage}
  % \hfill
  \begin{minipage}{0.32\linewidth}
  % \begin{minipage}{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{img/generalization_no_decay/librispeech_deepspeech.pdf}
    \subcaption{DeepSpeech}
  \end{minipage}
  % \hfill
  \begin{minipage}{0.32\linewidth}
  % \begin{minipage}{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{img/generalization_no_decay/criteo1tb.pdf}
    \subcaption{Criteo1TB}
  \end{minipage}

  \caption{Generalization performance when \textcolor{color_nadam}{(i)} training with a learning rate schedule, \textcolor{color_lawa}{(ii)} training with a learning rate schedule and weight averaging, \textcolor{color_lawa_no_decay}{(iii)} training with weight averaging and without a scheduler. Combining learning decay with weight averaging (LAWA) consistently yields optimal performances. Instead, replacing the learning rate schedule with averaging leads to worse result across all workloads, except FastMRI, which however exhibits high variability, as noted also in \citep{kasimbeg2025accelerating} and \citep{defazio_road_2024}. The red dotted line represents the benchmark target score.}
  \label{fig:generalization_no_decay_boxplot}
\end{figure*}


\paragraph{Averaging vs LR scheduling.}
In line with previous work, we compare checkpoint averaging over a long traning run against training with a \textit{shorter learning rate schedule}. We systematically observe that the validation performance of averaged checkpoints closely tracks a training run with a faster learning rate decay (\autoref{fig:sweep_horizon}). %, approaching the Pareto frontier of loss versus training time.
This behavior, although pointed out in earlier studies, consistently emerges across all the diverse model architectures and datasets considered for this analysis, and explains the considerable speed-ups observed in \autoref{sec:speeding_up_training}. Since checkpoint averaging acts as a proxy for a cooled-down learning rate, WA enables access to a better model during training, thus achieving the target score faster.
We note that more sophisticated annealing strategies are possible and could achieve better results than averaging \citep{hagele2024scaling}, and while the averaged model consistently approaches the Pareto frontier of loss versus training time, it may not always lie on it.
Nevertheless, its simplicity and effectiveness make averaging a highly practical tool in large-scale training, providing access to a stronger model with minimal computational overhead.
% Finally, we acknowledge previous a SCHEDULEFREE>>


Given the promising effect of averaging as an alternative to learning rate annealing, we examine its impact under different annealing strategies.
We use a cosine learning rate schedule with a maximum value of $\eta_{max}$ and explore three variations: no annealing, annealing the learning rate to half of $\eta_{max}$, and annealing to zero (\autoref{fig:decay_less_conformer}). 
Despite yielding significantly better validation scores during training across all learning rate schedules, the final validation performance of WA is strongly influenced by the annealing strategy. When little or no annealing is applied, WA provides substantial improvements; however, when the learning rate is fully annealed to zero, WA converges closely to the annealed model, suggesting that its benefits diminish as optimization naturally reaches a well-converged solution. 
This observation is consistent with our earlier findings, where averaging improves performance during training (\autoref{sec:speeding_up_training}) but provides minimal generalization gains when a learning rate schedule is used (\autoref{sec:improve_generalization}), and further supports the previously discussed relation between averaging and learning rate scheduling.

\vspace{-0.8mm}
\paragraph{Can averaging replace LR decay on AlgoPerf?}
We thoroughly compare averaging with learning rate scheduling across the entire AlgoPerf suite, reporting our findings in \autoref{fig:generalization_no_decay_boxplot}. 
Despite heavily tuning the averaging algorithms, we find that fully replacing the schedule with averaging consistently yields inferior performances than annealing the learning rate. We note that on Citeo1TB and OGBG, WA can achieve the target without lowering the learning rate, but it still reaches it later than the other approaches.
This result suggests that \textit{offline} averaging schemes, which do not use the average to compute optimization updates, cannot fully replace a learning rate schedule, and more advanced techniques like the one introduced by \citet{defazio_road_2024} are necessary.
Nevertheless, we argue that these averaging schemes can still be used effectively and cheaply to materialize better-performing models during training, as shown in \autoref{sec:speeding_up_training}, with notable applications, including recent work by \citet{DeepSeekV3} and \citet{geiping2025scalingtesttimecomputelatent}. 
Additionally, applying them alongside a learning rate schedule can further enhance performance and improve generalization, all at minimal cost (\autoref{sec:improve_generalization}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations}
\label{sec:limitations}

We limit our analysis primarily to NadamW as the baseline optimization algorithm. However, we believe that our conclusions are broadly applicable, and demonstrate how averaging can provide substantial benefits also on top of a second order optimizer like Shampoo.
Nevertheless, it would be interesting to compare averaging schemes across a broader range of optimization algorithms, and against schedule-free methods \citep{defazio_road_2024}.

% Finally, deterrent averaging sc


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusions}

Through a comprehensive empirical investigation, we study the effects of weight averaging on a challenging optimization benchmark \citep{dahl_benchmarking_2023}, consisting of several diverse Deep Learning architectures and datasets. %, and providing a refined framework for fair comparisons. % of averaging methods.
We first evaluate averaging methods both by means of the required time to reach a reasonable validation target and by comparing the best achieved validation performance, and conclude that averaging yields considerable efficiency gains, while consistently achieving better validation performance.
% We attribute this behavior to the resemblance between averaging and learning rate annealing, and provide evidence supporting its role as a proxy for early learning rate decay.
We investigate the relation between averaging and learning rate annealing, and show how to optimally combine the two to achieve better performances.
% We hope that this work further encourages the adoption of averaging techniques by offering a clearer understanding of their benefits, when to use them, and why they are effective.
We hope that this work further encourages the adoption of averaging techniques.
Although such methods are not directly involved in optimizing updates, their significant improvements may dramatically reduce training time. 
In the field of efficient training, we believe this is a promising study with potential for practical applications \citep{shen_efficency}.

% Indeed, although such methods are not directly involved in optimizing updates, their significant improvements in the generality help to early stop the training with the comparable parameters. It dramatically reduces the required training epochs. In the field of efficient training, it is a promising study in practical applications. \citet{shen_efficency}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Acknowledgments}
The authors would like to acknowledge the Hector Foundation for their support and funding, which made this research possible.

% \clearpage
% \newpage
% \clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section*{Impact Statement}
% Our paper presents a thorough analysis of weight averaging for large-scale Machine Learning optimization, we acknowledge its potential impact in improving efficiency when training complex Deep Learning systems.

\bibliography{bibliography}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
% \newpage
% \clearpage

\appendix
% \onecolumn
\section{Experimental Details}
\label{app:experimental_details}

\subsection{Datasets and Architectures}
\label{sec:datasets_archs}

We report further details about the workloads considered for the analysis and provide the correspondence references. We consider the following architectures and tasks:

\begin{itemize}
    \item DLRMsmall model on the Criteo 1TB dataset \citep{naumov2019deep, criteo2014dataset}: a deep learning recommendation model optimized for large-scale industrial recommendation tasks.
    \item U-Net \citet{unet_2015} on the FastMRI dataset \citep{unet_2015, fastMRI_dataset}: a popular architecture for medical image segmentation.
    \item ViT (Vision Transformer) on ImageNet-1k \citep{dosovitskiy2020image, deng2009imagenet}: a transformer-based model for image classification.
    \item GNN model on OGBG dataset \citep{battaglia2018relational, hu2020open}: a graph neural network for learning over graph-structured data, used to test performance on graph-based machine learning tasks.
    \item Transformer-big on WMT dataset \citep{vaswani2017attention, bojar2017findings}: a large-scale transformer model applied to machine translation.
    \item Conformer on LibriSpeech dataset \citep{gulati2020conformer, librispeech}: a hybrid architecture combining CNNs and transformers, used for speech recognition tasks.
    \item  DeepSpeech \citet{deepspeech_amodei16} on LibriSpeech dataset \citep{librispeech}: a recurrent model for speech-to-text, tested on a popular speech recognition dataset to assess its performance in transcribing audio.
\end{itemize}



These workloads span various domains, including recommendation systems, image and speech processing, machine translation, and graph-based learning. Each tests different model architectures under real-world data conditions.


\subsection{Offline vs Online averaging}
\label{sec:offline_online_avg}
We perform most of the experiments using offline versions of LAWA and EMA. This means that we first run a baseline algorithm (NadamW), frequently saving checkpoints to disk. This allows us to then average and evaluate the model weights, exploring different averaging windows and hyperparameters, without retraining the model from scratch.

However, in practice, it might be beneficial to use online averaging schemes. Indeed, as showcased throughout this work, averaging can operate as a proxy for a shorter learning rate schedule and provide significantly better performance than the current available original model. 
Since this aspect has interesting implications for large-scale efficient training \citep{shen_efficency}, it is worth considering the possible computational overhead introduced by averaging schemes.
When running an online version of EMA or LAWA, we offload the averaging buffer to the CPU.
We implement the online schemes naively in \texttt{PyTorch}, defaulting to blocking communication. However, we note that it is possible to offload the model weights to the CPU asynchronously, using nonblocking communications to update an EMA or a moving average of model weight. We acknowledge a first adoption of this approach in \citet{DeepSeekV3}, and we hope that this work encourages a wider usage of it.

\subsection{Computational Resources}
\label{sec:computational_resources}

We conducted experiments on 4xA100-SXM4-40GB and 4xH100-HBM3-80GB GPUs, and occasionally resort to 8xV100-32GB machines. We use Distributed Data Parallel to parallelize training across devices. Training is done in full precision, enabling TF32-matmul for faster computations.

We make use of the original AlgoPerf repository\footnote{\url{https://github.com/mlcommons/algorithmic-efficiency}}, with minimal modifications, using the \texttt{PyTorch} \citep{paszke2017pytorch} implementation of the corresponding algorithms. For the Distributed Shampoo implementation, we resort to the original submission to the AlgoPerf competition\footnote{\url{https://github.com/mlcommons/algorithms_results_v0.5/tree/main/AlgoPerf_Team_21/external_tuning/shampoo_submission}}. We select the optimal hyperparameters for NadamW and Shampoo on each workload as the ones providing faster convergence to the benchmark target in the first iteration of the AlgoPerf competition\footnote{\url{https://github.com/mlcommons/algorithms_results_v0.5/tree/main/logs/algoperf_scoring_v05/external_tuning/AlgoPerf/prize_qualification_baseline}}.




% \begin{algorithm}[t]
% \caption{Latest weight averaging (\colorbox{color_lawa_transparent}{LAWA})}
% \State Initialize circular queue $Q$ of max length $L$
% \For{t, batch in enumerate(trainloader)}
%     \State Optimization step
%     \If{t mod $\nu = 0$}
%         \State Store current parameters $\theta_t$ in $Q$
%     \EndIf
%     \If{is eval time}
%         \State Compute \colorbox{color_lawa_transparent}{$\bar{\theta}_Q = \frac{1}{L} \sum_{\theta \in Q} \theta$}
%         \State Eval $\bar{\theta}_Q$ on held-out data
%     \EndIf
% \EndFor
% \end{algorithm}

% \begin{algorithm}[t]
% \caption{Exponential Moving Averaging (\colorbox{color_ema_transparent}{EMA})}
% \State Initialize EMA parameters $\theta_{\text{EMA}} = \theta_0$
% \For{t, batch in enumerate(trainloader)}
%     \State Optimization step
%     \If{t mod $\nu = 0$}
%         \State \colorbox{color_ema_transparent}{$\theta_{\text{EMA}} \gets \gamma \theta_{\text{EMA}} + (1 - \gamma) \theta_t$}
%     \EndIf
%     \If{is eval time}
%         \State Eval $\theta_{\text{EMA}}$ on held-out data
%     \EndIf
% \EndFor        
% \end{algorithm}

% \section{Optimal Averaging Horizon}

% \begin{figure*}[t]
%   \centering
%   \begin{minipage}{0.45\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{img/ogbg_sweep_lawa.pdf}
%     \subcaption{}
%   \end{minipage}
%   \hfill
%   \begin{minipage}{0.45\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{img/ogbg_sweep_ema.pdf}
%     \subcaption{}
%   \end{minipage}
%   \caption{Trash it and put a scatterplot instead}
%   \label{fig:optimal_horizon}
% \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}