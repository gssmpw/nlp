\section{Related Work}
\label{sec:related_work}

The idea of averaging iterates along a stochastic optimization trajectory dates back to ____ and ____, and is often referred to as Polyak–Ruppert averaging.
It has been extensively studied in the stochastic approximation framework, ____, and is a common technique to derive convergence guarantees in both convex and nonconvex optimization____. 

\vspace{-2mm}
\paragraph{Deep Learning applications.}
Weight averaging techniques have seen extensive use in the Deep Learning community, often without explicit recognition or emphasis, despite their effectiveness. Influential works such as ____, ____, and ____ have demonstrated their ability to enhance model performance and mitigate overfitting.


\vspace{-2mm}
\paragraph{SWA.}
The work of ____ sparked renewed interest in weight averaging by demonstrating how averaging points along the SGD trajectory leads to wider minima and improves generalization performance. Their approach, Stochastic Weight Averaging (SWA), has since been applied to semi-supervised learning ____, low-precision training ____, domain generalization tasks ____, and meta-optimization ____.

\vspace{-2mm}
\paragraph{LAWA and EMA.} In the original formulation of SWA, a pretrained model is trained with a cyclical or constant learning rate, and multiple checkpoints are collected and later averaged.
____ proposed Latest Weight Averaging (LAWA), an online algorithm that averages the latest checkpoints in a rolling window, showing significant speed-ups on vision and language tasks. Further modifications of LAWA demonstrated notable boosts in pretraining modern decoder-only language models ____. %, and very recently ____ preserved an exponential moving average of the model parameters in CPU memory to estimate the model performance after learning rate decay. 
A valuable alternative to moving window averaging techniques like LAWA is Exponential Moving Averaging (EMA) ____. It retains similar advantages of rolling window averaging, and constitutes an indispensable technique for high-quality image synthesis models such as GANs and diffusion models ____.

\vspace{-2mm}
\paragraph{Connection to learning rate annealing.} 
Classical stochastic smooth convex optimization rates showcase a tight link between WA and learning rate annealing, suggesting a practical interplay between these techniques~(see e.g. Theorem 5.3. in____).
Intuitively, averaging weights along the training trajectory reduces noise and might act as a proxy for learning rate decay. In fact, ____ proved the theoretical and empirical equivalence between WA and decaying learning rate for SGD. 
However, despite this appealing result, 
% a thorough analysis of the interplay between averaging and learning rate scheduling on modern tasks and optimization algorithms is still missing, and bridging the gap between the two is an open research problem ____: 
modern Deep Learning models are still predominantly trained with learning rate annealing ____, even when maintaining an EMA of model weights ____. 
A recent study by ____ specifically investigates this connection, proposing an approach that fully replaces learning rate schedules with iterate averaging and demonstrating strong performance on the same benchmark used in this analysis.
Whereas ____ incorporates averaging directly into the optimization procedure, we explore a different flavor of averaging, where the averaged weights do not influence the updates—akin to Polyak averaging, SWA, LAWA, and EMA.
% A recent effort to fully replace learning rate schedules with iterate averaging was developed by ____, demonstrating strong performance on the same benchmark used in this analysis.

\vspace{-2mm}
\paragraph{Model soups.} Finally, a different but notable approach that leverages the benefits of averaging is model soups ____. In this case, multiple models are trained with different hyperparameter configurations and later aggregated, resulting in an ensemble with improved accuracy and robustness.

In this work, we demonstrate the benefits of weight averaging techniques on a challenging optimization benchmark ____, hoping to encourage broader adoption of these methods in training large-scale Machine Learning models.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%