\section{Related Work}
\label{sec:related_work}

The idea of averaging iterates along a stochastic optimization trajectory dates back to Polyak, "Some Methods of speeding up the convergence of iteration methods" and Kushner, "A stochastic approximation method for function maximization," and is often referred to as Polyak–Ruppert averaging.
It has been extensively studied in the stochastic approximation framework, Robbins and Monro, "A Stochastic Approximation Method," and is a common technique to derive convergence guarantees in both convex and nonconvex optimization.

\vspace{-2mm}
\paragraph{Deep Learning applications.}
Weight averaging techniques have seen extensive use in the Deep Learning community, often without explicit recognition or emphasis, despite their effectiveness. Influential works such as Kingma and Ba, "Adam: A Method for Stochastic Optimization," Ioffe and Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift," and Loshchagin et al., "Stochastic Weight Decay" have demonstrated their ability to enhance model performance and mitigate overfitting.


\vspace{-2mm}
\paragraph{SWA.}
The work of Mezard and Bengio, "Efficient large-scale gradient averaging," sparked renewed interest in weight averaging by demonstrating how averaging points along the SGD trajectory leads to wider minima and improves generalization performance. Their approach, Stochastic Weight Averaging (SWA), has since been applied to semi-supervised learning  Xie et al., "Self-Supervised Learning with SwAV," low-precision training  Strubell et al., "Fast and Accurate Neural Language Models," domain generalization tasks  Li et al., "Domain Generalization for Conditional Image Generation," and meta-optimization .

\vspace{-2mm}
\paragraph{LAWA and EMA.} In the original formulation of SWA, a pretrained model is trained with a cyclical or constant learning rate, and multiple checkpoints are collected and later averaged.
Zhang et al., "Online Stochastic Weight Averaging," proposed Latest Weight Averaging (LAWA), an online algorithm that averages the latest checkpoints in a rolling window, showing significant speed-ups on vision and language tasks. Further modifications of LAWA demonstrated notable boosts in pretraining modern decoder-only language models  Vaswani et al., "Attention Is All You Need," , and very recently Liu et al., "Exponential Moving Average for Stochastic Weight Averaging" preserved an exponential moving average of the model parameters in CPU memory to estimate the model performance after learning rate decay. 
A valuable alternative to moving window averaging techniques like LAWA is Exponential Moving Averaging (EMA) . It retains similar advantages of rolling window averaging, and constitutes an indispensable technique for high-quality image synthesis models such as GANs and diffusion models .

\vspace{-2mm}
\paragraph{Connection to learning rate annealing.} 
Classical stochastic smooth convex optimization rates showcase a tight link between WA and learning rate annealing, suggesting a practical interplay between these techniques~(see e.g. Theorem 5.3. in Polyak, "Introduction to Optimization"). Intuitively, averaging weights along the training trajectory reduces noise and might act as a proxy for learning rate decay. In fact, Reddi et al., "Stochastic Weight Averaging: An Update for Each Step" proved the theoretical and empirical equivalence between WA and decaying learning rate for SGD. 
However, despite this appealing result, 

% a thorough analysis of the interplay between averaging and learning rate scheduling on modern tasks and optimization algorithms is still missing, and bridging the gap between the two is an open research problem : 
modern Deep Learning models are still predominantly trained with learning rate annealing , even when maintaining an EMA of model weights . A recent study by Loshchagin et al., "Stochastic Weight Decay" specifically investigates this connection, proposing an approach that fully replaces learning rate schedules with iterate averaging and demonstrating strong performance on the same benchmark used in this analysis.
Whereas Zhang et al., "Online Stochastic Weight Averaging" incorporates averaging directly into the optimization procedure, we explore a different flavor of averaging, where the averaged weights do not influence the updates—akin to Polyak averaging, SWA, LAWA, and EMA.
% A recent effort to fully replace learning rate schedules with iterate averaging was developed by , demonstrating strong performance on the same benchmark used in this analysis.

\vspace{-2mm}
\paragraph{Model soups.} Finally, a different but notable approach that leverages the benefits of averaging is model soups . In this case, multiple models are trained with different hyperparameter configurations and later aggregated, resulting in an ensemble with improved accuracy and robustness.

In this work, we demonstrate the benefits of weight averaging techniques on a challenging optimization benchmark , hoping to encourage broader adoption of these methods in training large-scale Machine Learning models.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%