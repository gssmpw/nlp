@article{DeepSeekV3, 
    title={DeepSeek-V3: A Strong Mixture-of-Experts Language Model}, author={DeepSeekAI}, year={2024},
}

@inproceedings{athiwaratkun_there_2018,
	title = {There {Are} {Many} {Consistent} {Explanations} of {Unlabeled} {Data}: {Why} {You} {Should} {Average}},
	shorttitle = {There {Are} {Many} {Consistent} {Explanations} of {Unlabeled} {Data}},
	language = {en},
	urldate = {2024-12-19},
	author = {Athiwaratkun, Ben and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew Gordon},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/Users/nico/Zotero/storage/MD9KYGRU/Athiwaratkun et al. - 2018 - There Are Many Consistent Explanations of Unlabele.pdf:application/pdf},
}

@inproceedings{bach_non-strongly-convex_2013,
	title = {Non-strongly-convex smooth stochastic approximation with convergence rate {O}(1/n)},
	volume = {26},
	urldate = {2024-12-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bach, Francis and Moulines, Eric},
	year = {2013},
	file = {Full Text PDF:/Users/nico/Zotero/storage/S6B9ZTS2/Bach and Moulines - 2013 - Non-strongly-convex smooth stochastic approximatio.pdf:application/pdf},
}

@inproceedings{cha_swad_2021,
	title = {{SWAD}: {Domain} {Generalization} by {Seeking} {Flat} {Minima}},
	volume = {34},
	shorttitle = {{SWAD}},
	abstract = {Domain generalization (DG) methods aim to achieve generalizability to an unseen target domain by using only training data from the source domains. Although a variety of DG methods have been proposed, a recent study shows that under a fair evaluation protocol, called DomainBed, the simple empirical risk minimization (ERM) approach works comparable to or even outperforms previous methods. Unfortunately, simply solving ERM on a complex, non-convex loss function can easily lead to sub-optimal generalizability by seeking sharp minima. In this paper, we theoretically show that finding flat minima results in a smaller domain generalization gap. We also propose a simple yet effective method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima. SWAD finds flatter minima and suffers less from overfitting than does the vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy. SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large margins of +1.6\% averagely on out-of-domain accuracy. We also compare SWAD with conventional generalization methods, such as data augmentation and consistency regularization methods, to verify that the remarkable performance improvements are originated from by seeking flat minima, not from better in-domain generalizability. Last but not least, SWAD is readily adaptable to existing DG methods without modification; the combination of SWAD and an existing DG method further improves DG performances. Source code is available at https://github.com/khanrc/swad.},
	urldate = {2024-12-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cha, Junbum and Chun, Sanghyuk and Lee, Kyungjae and Cho, Han-Cheol and Park, Seunghyun and Lee, Yunsung and Park, Sungrae},
	year = {2021},
	pages = {22405--22418},
	file = {Full Text PDF:/Users/nico/Zotero/storage/K5NB8U7C/Cha et al. - 2021 - SWAD Domain Generalization by Seeking Flat Minima.pdf:application/pdf},
}

@misc{dahl_benchmarking_2023,
	title = {Benchmarking {Neural} {Network} {Training} {Algorithms}},
	language = {en},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Dahl, George E. and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and Bae, Juhan and Gilmer, Justin and Peirson, Abel L. and Khan, Bilal and Anil, Rohan and Rabbat, Mike and Krishnan, Shankar and Snider, Daniel and Amid, Ehsan and Chen, Kongtao and Maddison, Chris J. and Vasudev, Rakshith and Badura, Michal and Garg, Ankush and Mattson, Peter},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07179 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Dahl et al. - 2023 - Benchmarking Neural Network Training Algorithms.pdf:/Users/nico/Zotero/storage/H9DBW6SN/Dahl et al. - 2023 - Benchmarking Neural Network Training Algorithms.pdf:application/pdf},
}

@misc{david_ruppert_efficient_1988,
	title = {Efficient {Estimations} from a {Slowly} {Convergent} {Robbins}-{Monro} {Process}},
	author = {David Ruppert},
	year = {1988},
	file = {ruppert_avg.pdf:/Users/nico/Zotero/storage/BASCEFRR/ruppert_avg.pdf:application/pdf},
}

@misc{defazio_road_2024,
	title = {The {Road} {Less} {Scheduled}},
	abstract = {Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T . We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available1.},
	language = {en},
	urldate = {2024-05-28},
	publisher = {arXiv},
	author = {Defazio, Aaron and Xingyu and Yang and Mehta, Harsh and Mishchenko, Konstantin and Khaled, Ahmed and Cutkosky, Ashok},
	month = may,
	year = {2024},
	note = {arXiv:2405.15682 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {Defazio et al. - 2024 - The Road Less Scheduled.pdf:/Users/nico/Zotero/storage/DW7D2XJ9/Defazio et al. - 2024 - The Road Less Scheduled.pdf:application/pdf},
}

@article{garrigos2023handbook,
  title={Handbook of convergence theorems for (stochastic) gradient methods},
  author={Garrigos, Guillaume and Gower, Robert M},
  journal={arXiv preprint arXiv:2301.11235},
  year={2023}
}

@article{hagele2024scaling,
  author  = {Alexander H\"agele and Elie Bakouch and Atli Kosson and Loubna Ben Allal and Leandro Von Werra and Martin Jaggi},
  title   = {{Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations}},
  year    = {2024},
  journal = {Advances in Neural Information Processing Systems},
}

@misc{izmailov_averaging_2019,
      title={Averaging Weights Leads to Wider Optima and Better Generalization}, 
      author={Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
      year={2019},
      eprint={1803.05407},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{kaddour_stop_2022,
	title = {Stop {Wasting} {My} {Time}! {Saving} {Days} of {ImageNet} and {BERT} {Training} with {Latest} {Weight} {Averaging}},
	abstract = {Training vision or language models on large datasets can take days, if not weeks. We show that averaging the weights of the k latest checkpoints, each collected at the end of an epoch, can speed up the training progression in terms of loss and accuracy by dozens of epochs, corresponding to time savings up to 68 and 30 GPU hours when training a ResNet50 on ImageNet and RoBERTa-Base model on WikiText-103, respectively. We also provide the code and model checkpoint trajectory to reproduce the results and facilitate research on reusing historical weights for faster convergence1.},
	language = {en},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Kaddour, Jean},
	month = oct,
	year = {2022},
	note = {arXiv:2209.14981 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Kaddour - 2022 - Stop Wasting My Time! Saving Days of ImageNet and .pdf:/Users/nico/Zotero/storage/S2N3B43W/Kaddour - 2022 - Stop Wasting My Time! Saving Days of ImageNet and .pdf:application/pdf},
}

@misc{karras2024nvidia_diffusion_avg,
      title={Analyzing and Improving the Training Dynamics of Diffusion Models}, 
      author={Tero Karras and Miika Aittala and Jaakko Lehtinen and Janne Hellsten and Timo Aila and Samuli Laine},
      year={2024},
      eprint={2312.02696},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@inproceedings{lakshminarayanan_linear_2018,
	title = {Linear {Stochastic} {Approximation}: {How} {Far} {Does} {Constant} {Step}-{Size} and {Iterate} {Averaging} {Go}?},
	shorttitle = {Linear {Stochastic} {Approximation}},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lakshminarayanan, Chandrashekar and Szepesvari, Csaba},
	month = mar,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1347--1355},
	file = {Full Text PDF:/Users/nico/Zotero/storage/RPD7T4EP/Lakshminarayanan and Szepesvari - 2018 - Linear Stochastic Approximation How Far Does Cons.pdf:application/pdf;Supplementary PDF:/Users/nico/Zotero/storage/3MXKLU3M/Lakshminarayanan and Szepesvari - 2018 - Linear Stochastic Approximation How Far Does Cons.pdf:application/pdf},
}

@misc{li_switch_2024,
	title = {Switch {EMA}: {A} {Free} {Lunch} for {Better} {Flatness} and {Sharpness}},
	shorttitle = {Switch {EMA}},
	abstract = {Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.},
	language = {en},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Li, Siyuan and Liu, Zicheng and Tian, Juanxi and Wang, Ge and Wang, Zedong and Jin, Weiyang and Wu, Di and Tan, Cheng and Lin, Tao and Liu, Yang and Sun, Baigui and Li, Stan Z.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09240 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Li et al. - 2024 - Switch EMA A Free Lunch for Better Flatness and S.pdf:/Users/nico/Zotero/storage/DKGCF85E/Li et al. - 2024 - Switch EMA A Free Lunch for Better Flatness and S.pdf:application/pdf},
}

@inproceedings{li_trainable_2022,
	title = {Trainable {Weight} {Averaging}: {Efficient} {Training} by {Optimizing} {Historical} {Solutions}},
	shorttitle = {Trainable {Weight} {Averaging}},
	abstract = {Stochastic gradient descent (SGD) and its variants are considered as the de-facto methods to train deep neural networks (DNNs). While recent improvements to SGD mainly focus on the descent algorithm itself, few works pay attention to utilizing the historical solutions---as an iterative method, SGD has gone through substantial explorations before convergence. Recently, an interesting attempt is stochastic weight averaging (SWA), which significantly improves the generalization by simply averaging the solutions at the tail stage of training. In this paper, we realize that the averaging coefficients could be determined in a trainable manner and propose Trainable Weight Averaging (TWA), a novel optimization method in the reduced subspace spanned by historical solutions. TWA has much greater flexibility and can be applied to the head stage of training to achieve training efficiency while preserving good generalization capability. Further, we propose a distributed training scheme to resolve the memory burden of large-scale training with efficient parallel computation. In the extensive numerical experiments, (i) TWA achieves consistent improvements over SWA with less sensitivity to learning rate; (ii) applying TWA in the head stage of training largely speeds up the convergence, resulting in over \$40{\textbackslash}\%\$ time saving on CIFAR and \$30{\textbackslash}\%\$ on ImageNet with improved generalization compared with regular training.},
	language = {en},
	urldate = {2024-12-19},
	author = {Li, Tao and Huang, Zhehao and Tao, Qinghua and Wu, Yingwen and Huang, Xiaolin},
	month = sep,
	year = {2022},
	file = {Full Text PDF:/Users/nico/Zotero/storage/8JPJPI2I/Li et al. - 2022 - Trainable Weight Averaging Efficient Training by .pdf:application/pdf},
}

@misc{merity2017regularizingoptimizinglstmlanguage,
      title={Regularizing and Optimizing LSTM Language Models}, 
      author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
      year={2017},
      eprint={1708.02182},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{morales_ema,
      title={Exponential Moving Average of Weights in Deep Learning: Dynamics and Benefits}, 
      author={Daniel Morales-Brotons and Thijs Vogels and Hadrien Hendrikx},
      year={2024},
      eprint={2411.18704},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{neu_iterate_2018,
	title = {Iterate {Averaging} as {Regularization} for {Stochastic} {Gradient} {Descent}},
	abstract = {We propose and analyze a variant of the classic  Polyak–Ruppert averaging scheme, broadly used in stochastic gradient methods.  Rather than a uniform average of the iterates, we consider a weighted average, with weights decaying in a geometric fashion. In the context of linear least-squares regression, we show that this averaging scheme has the same regularizing effect, and indeed is asymptotically equivalent, to ridge regression. In particular, we derive finite-sample bounds for the proposed approach that match the best known results for regularized stochastic gradient methods.},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the 31st  {Conference} {On} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Neu, Gergely and Rosasco, Lorenzo},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {3222--3242},
	file = {Full Text PDF:/Users/nico/Zotero/storage/HT3VP5I7/Neu and Rosasco - 2018 - Iterate Averaging as Regularization for Stochastic.pdf:application/pdf},
}

@article{polyak_acceleration_1992,
	title = {Acceleration of {Stochastic} {Approximation} by {Averaging}},
	volume = {30},
	issn = {0363-0129, 1095-7138},
	doi = {10.1137/0330046},
	abstract = {A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.},
	language = {en},
	number = {4},
	urldate = {2024-12-17},
	journal = {SIAM Journal on Control and Optimization},
	author = {Polyak, B. T. and Juditsky, A. B.},
	month = jul,
	year = {1992},
	pages = {838--855},
	file = {Polyak and Juditsky - 1992 - Acceleration of Stochastic Approximation by Averag.pdf:/Users/nico/Zotero/storage/4A8V42LF/Polyak and Juditsky - 1992 - Acceleration of Stochastic Approximation by Averag.pdf:application/pdf},
}

@article{polyak_new_1990,
	title = {New stochastic approximation type procedures},
	volume = {7},
	journal = {Avtomatica i Telemekhanika},
	author = {Polyak, B. T.},
	year = {1990},
	pages = {98--107},
	file = {New stochastic approximation type procedures:/Users/nico/Zotero/storage/W5YAE2VZ/Polyak and Juditsky - 1992 - New stochastic approximation type procedures.pdf:application/pdf},
}

@misc{sandler_training_2023,
	title = {Training trajectories, mini-batch losses and the curious role of the learning rate},
	doi = {10.48550/arXiv.2301.02312},
	abstract = {Stochastic gradient descent plays a fundamental role in nearly all applications of deep learning. However its ability to converge to a global minimum remains shrouded in mystery. In this paper we propose to study the behavior of the loss function on fixed mini-batches along SGD trajectories. We show that the loss function on a fixed batch appears to be remarkably convex-like. In particular for ResNet the loss for any fixed mini-batch can be accurately modeled by a quadratic function and a very low loss value can be reached in just one step of gradient descent with sufficiently large learning rate. We propose a simple model that allows to analyze the relationship between the gradients of stochastic mini-batches and the full batch. Our analysis allows us to discover the equivalency between iterate aggregates and specific learning rate schedules. In particular, for Exponential Moving Average (EMA) and Stochastic Weight Averaging we show that our proposed model matches the observed training trajectories on ImageNet. Our theoretical model predicts that an even simpler averaging technique, averaging just two points a many steps apart, significantly improves accuracy compared to the baseline. We validated our findings on ImageNet and other datasets using ResNet architecture.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Sandler, Mark and Zhmoginov, Andrey and Vladymyrov, Max and Miller, Nolan},
	month = feb,
	year = {2023},
	note = {arXiv:2301.02312 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/nico/Zotero/storage/5ZYVTQBV/Sandler et al. - 2023 - Training trajectories, mini-batch losses and the c.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/G9HQJ3UP/2301.html:text/html},
}

@misc{sanyal_early_2023,
	title = {Early {Weight} {Averaging} meets {High} {Learning} {Rates} for {LLM} {Pre}-training},
	doi = {10.48550/arXiv.2306.03241},
	abstract = {Training Large Language Models (LLMs) incurs significant cost; hence, any strategy that accelerates model convergence is helpful. In this paper, we investigate the ability of a simple idea checkpoint averaging along the trajectory of a training run to improve both convergence and generalization quite early on during training. Here we show that models trained with high learning rates observe higher gains due to checkpoint averaging. Furthermore, these gains are amplified when checkpoints are sampled with considerable spacing in training steps. Our training recipe outperforms conventional training and popular checkpoint averaging baselines such as exponential moving average (EMA) and stochastic moving average (SWA). We evaluate our training recipe by pre-training LLMs, where high learning rates are inherently preferred due to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2 models of varying sizes, small (125M), medium (335M), and large (770M)on the OpenWebText dataset, comprised of 9B tokens. Additionally, we present results for publicly available Pythia LLMs, ranging from 1B to 12B, which were trained on the PILE-deduped dataset containing 207B tokens.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Sanyal, Sunny and Neerkaje, Atula and Kaddour, Jean and Kumar, Abhishek and Sanghavi, Sujay},
	month = dec,
	year = {2023},
	note = {arXiv:2306.03241 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/nico/Zotero/storage/UTM7E45K/Sanyal et al. - 2023 - Early Weight Averaging meets High Learning Rates f.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/6YU7ZDNP/2306.html:text/html},
}

@misc{song2021score_based_gen,
      title={Score-Based Generative Modeling through Stochastic Differential Equations}, 
      author={Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
      year={2021},
      eprint={2011.13456},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@INPROCEEDINGS{szegedy_2016_rethinking,
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Rethinking the Inception Architecture for Computer Vision}, 
  year={2016},
  volume={},
  number={},
  pages={2818-2826},
  keywords={Convolution;Computer architecture;Training;Computational efficiency;Computer vision;Benchmark testing;Computational modeling},
  doi={10.1109/CVPR.2016.308}}

@misc{tarvainen2018meanteachersbetterrole,
      title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results}, 
      author={Antti Tarvainen and Harri Valpola},
      year={2018},
      eprint={1703.01780},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  year={2017}
}

@inproceedings{wortsman_model_2022,
	title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
	shorttitle = {Model soups},
	abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs—we call the results “model soups.” When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {23965--23998},
	file = {Full Text PDF:/Users/nico/Zotero/storage/GMVINC97/Wortsman et al. - 2022 - Model soups averaging weights of multiple fine-tu.pdf:application/pdf},
}

@inproceedings{yang_swalp_2019,
	title = {{SWALP} : {Stochastic} {Weight} {Averaging} in {Low} {Precision} {Training}},
	shorttitle = {{SWALP}},
	abstract = {Low precision operations can provide scalability, memory savings, portability, and energy efficiency. This paper proposes SWALP, an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule. SWALP is easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the optimal solution for quadratic objectives, and to a noise ball asymptotically smaller than low precision SGD in strongly convex settings.},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Guandao and Zhang, Tianyi and Kirichenko, Polina and Bai, Junwen and Wilson, Andrew Gordon and Sa, Chris De},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7015--7024},
	file = {Full Text PDF:/Users/nico/Zotero/storage/MIT9IBCT/Yang et al. - 2019 - SWALP  Stochastic Weight Averaging in Low Precisi.pdf:application/pdf;Supplementary PDF:/Users/nico/Zotero/storage/LWRUP3ZD/Yang et al. - 2019 - SWALP  Stochastic Weight Averaging in Low Precisi.pdf:application/pdf},
}

@misc{yazıcı2019avg_gan,
      title={The Unusual Effectiveness of Averaging in GAN Training}, 
      author={Yasin Yazıcı and Chuan-Sheng Foo and Stefan Winkler and Kim-Hui Yap and Georgios Piliouras and Vijay Chandrasekhar},
      year={2019},
      eprint={1806.04498},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
}

