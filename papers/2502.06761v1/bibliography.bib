@article{garrigos2023handbook,
  title={Handbook of convergence theorems for (stochastic) gradient methods},
  author={Garrigos, Guillaume and Gower, Robert M},
  journal={arXiv preprint arXiv:2301.11235},
  year={2023}
}
@misc{kaddour_when_2023,
	title = {When {Do} {Flat} {Minima} {Optimizers} {Work}?},
	abstract = {Recently, ﬂat-minima optimizers, which seek to ﬁnd parameters in low-loss neighborhoods, have been shown to improve a neural network’s generalization performance over stochastic and adaptive gradient-based optimizers. Two methods have received signiﬁcant attention due to their scalability: 1. Stochastic Weight Averaging (SWA), and 2. Sharpness-Aware Minimization (SAM). However, there has been limited investigation into their properties and no systematic benchmarking of them across different domains. We ﬁll this gap here by comparing the loss surfaces of the models trained with each method and through broad benchmarking across computer vision, natural language processing, and graph representation learning tasks. We discover several surprising ﬁndings from these results, which we hope will help researchers further improve deep learning optimizers, and practitioners identify the right optimizer for their problem.},
	language = {en},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J.},
	month = jan,
	year = {2023},
	note = {arXiv:2202.00661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Kaddour et al. - 2023 - When Do Flat Minima Optimizers Work.pdf:/Users/nico/Zotero/storage/QQMIL7VX/Kaddour et al. - 2023 - When Do Flat Minima Optimizers Work.pdf:application/pdf},
}

@misc{izmailov_averaging_2019,
      title={Averaging Weights Leads to Wider Optima and Better Generalization}, 
      author={Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
      year={2019},
      eprint={1803.05407},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{melis_two-tailed_2023,
	title = {Two-{Tailed} {Averaging}: {Anytime}, {Adaptive}, {Once}-in-a-{While} {Optimal} {Weight} {Averaging} for {Better} {Generalization}},
	shorttitle = {Two-{Tailed} {Averaging}},
	abstract = {Tail Averaging improves on Polyak averaging's non-asymptotic behaviour by excluding a number of leading iterates of stochastic optimization from its calculations. In practice, with a finite number of optimization steps and a learning rate that cannot be annealed to zero, Tail Averaging can get much closer to a local minimum point of the training loss than either the individual iterates or the Polyak average. However, the number of leading iterates to ignore is an important hyperparameter, and starting averaging too early or too late leads to inefficient use of resources or suboptimal solutions. Our work focusses on improving generalization, which makes setting this hyperparameter even more difficult, especially in the presence of other hyperparameters and overfitting. Furthermore, before averaging starts, the loss is only weakly informative of the final performance, which makes early stopping unreliable. To alleviate these problems, we propose an anytime variant of Tail Averaging intended for improving generalization not pure optimization, that has no hyperparameters and approximates the optimal tail at all optimization steps. Our algorithm is based on two running averages with adaptive lengths bounded in terms of the optimal tail length, one of which achieves approximate optimality with some regularity. Requiring only the additional storage for two sets of weights and periodic evaluation of the loss, the proposed Two-Tailed Averaging algorithm is a practical and widely applicable method for improving generalization.},
	language = {en},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Melis, Gábor},
	month = apr,
	year = {2023},
	note = {arXiv:2209.12581 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {Melis - 2023 - Two-Tailed Averaging Anytime, Adaptive, Once-in-a.pdf:/Users/nico/Zotero/storage/9CNJYBT2/Melis - 2023 - Two-Tailed Averaging Anytime, Adaptive, Once-in-a.pdf:application/pdf},
}

@misc{kaddour_stop_2022,
	title = {Stop {Wasting} {My} {Time}! {Saving} {Days} of {ImageNet} and {BERT} {Training} with {Latest} {Weight} {Averaging}},
	abstract = {Training vision or language models on large datasets can take days, if not weeks. We show that averaging the weights of the k latest checkpoints, each collected at the end of an epoch, can speed up the training progression in terms of loss and accuracy by dozens of epochs, corresponding to time savings up to 68 and 30 GPU hours when training a ResNet50 on ImageNet and RoBERTa-Base model on WikiText-103, respectively. We also provide the code and model checkpoint trajectory to reproduce the results and facilitate research on reusing historical weights for faster convergence1.},
	language = {en},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Kaddour, Jean},
	month = oct,
	year = {2022},
	note = {arXiv:2209.14981 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Kaddour - 2022 - Stop Wasting My Time! Saving Days of ImageNet and .pdf:/Users/nico/Zotero/storage/S2N3B43W/Kaddour - 2022 - Stop Wasting My Time! Saving Days of ImageNet and .pdf:application/pdf},
}

@misc{li_switch_2024,
	title = {Switch {EMA}: {A} {Free} {Lunch} for {Better} {Flatness} and {Sharpness}},
	shorttitle = {Switch {EMA}},
	abstract = {Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.},
	language = {en},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Li, Siyuan and Liu, Zicheng and Tian, Juanxi and Wang, Ge and Wang, Zedong and Jin, Weiyang and Wu, Di and Tan, Cheng and Lin, Tao and Liu, Yang and Sun, Baigui and Li, Stan Z.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09240 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Li et al. - 2024 - Switch EMA A Free Lunch for Better Flatness and S.pdf:/Users/nico/Zotero/storage/DKGCF85E/Li et al. - 2024 - Switch EMA A Free Lunch for Better Flatness and S.pdf:application/pdf},
}

@misc{sanyal_early_2023,
	title = {Early {Weight} {Averaging} meets {High} {Learning} {Rates} for {LLM} {Pre}-training},
	doi = {10.48550/arXiv.2306.03241},
	abstract = {Training Large Language Models (LLMs) incurs significant cost; hence, any strategy that accelerates model convergence is helpful. In this paper, we investigate the ability of a simple idea checkpoint averaging along the trajectory of a training run to improve both convergence and generalization quite early on during training. Here we show that models trained with high learning rates observe higher gains due to checkpoint averaging. Furthermore, these gains are amplified when checkpoints are sampled with considerable spacing in training steps. Our training recipe outperforms conventional training and popular checkpoint averaging baselines such as exponential moving average (EMA) and stochastic moving average (SWA). We evaluate our training recipe by pre-training LLMs, where high learning rates are inherently preferred due to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2 models of varying sizes, small (125M), medium (335M), and large (770M)on the OpenWebText dataset, comprised of 9B tokens. Additionally, we present results for publicly available Pythia LLMs, ranging from 1B to 12B, which were trained on the PILE-deduped dataset containing 207B tokens.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Sanyal, Sunny and Neerkaje, Atula and Kaddour, Jean and Kumar, Abhishek and Sanghavi, Sujay},
	month = dec,
	year = {2023},
	note = {arXiv:2306.03241 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/nico/Zotero/storage/UTM7E45K/Sanyal et al. - 2023 - Early Weight Averaging meets High Learning Rates f.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/6YU7ZDNP/2306.html:text/html},
}

@misc{sandler_training_2023,
	title = {Training trajectories, mini-batch losses and the curious role of the learning rate},
	doi = {10.48550/arXiv.2301.02312},
	abstract = {Stochastic gradient descent plays a fundamental role in nearly all applications of deep learning. However its ability to converge to a global minimum remains shrouded in mystery. In this paper we propose to study the behavior of the loss function on fixed mini-batches along SGD trajectories. We show that the loss function on a fixed batch appears to be remarkably convex-like. In particular for ResNet the loss for any fixed mini-batch can be accurately modeled by a quadratic function and a very low loss value can be reached in just one step of gradient descent with sufficiently large learning rate. We propose a simple model that allows to analyze the relationship between the gradients of stochastic mini-batches and the full batch. Our analysis allows us to discover the equivalency between iterate aggregates and specific learning rate schedules. In particular, for Exponential Moving Average (EMA) and Stochastic Weight Averaging we show that our proposed model matches the observed training trajectories on ImageNet. Our theoretical model predicts that an even simpler averaging technique, averaging just two points a many steps apart, significantly improves accuracy compared to the baseline. We validated our findings on ImageNet and other datasets using ResNet architecture.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Sandler, Mark and Zhmoginov, Andrey and Vladymyrov, Max and Miller, Nolan},
	month = feb,
	year = {2023},
	note = {arXiv:2301.02312 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/nico/Zotero/storage/5ZYVTQBV/Sandler et al. - 2023 - Training trajectories, mini-batch losses and the c.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/G9HQJ3UP/2301.html:text/html},
}

@inproceedings{wortsman_model_2022,
	title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
	shorttitle = {Model soups},
	abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs—we call the results “model soups.” When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {23965--23998},
	file = {Full Text PDF:/Users/nico/Zotero/storage/GMVINC97/Wortsman et al. - 2022 - Model soups averaging weights of multiple fine-tu.pdf:application/pdf},
}

@inproceedings{lakshminarayanan_linear_2018,
	title = {Linear {Stochastic} {Approximation}: {How} {Far} {Does} {Constant} {Step}-{Size} and {Iterate} {Averaging} {Go}?},
	shorttitle = {Linear {Stochastic} {Approximation}},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lakshminarayanan, Chandrashekar and Szepesvari, Csaba},
	month = mar,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1347--1355},
	file = {Full Text PDF:/Users/nico/Zotero/storage/RPD7T4EP/Lakshminarayanan and Szepesvari - 2018 - Linear Stochastic Approximation How Far Does Cons.pdf:application/pdf;Supplementary PDF:/Users/nico/Zotero/storage/3MXKLU3M/Lakshminarayanan and Szepesvari - 2018 - Linear Stochastic Approximation How Far Does Cons.pdf:application/pdf},
}

@article{polyak_acceleration_1992,
	title = {Acceleration of {Stochastic} {Approximation} by {Averaging}},
	volume = {30},
	issn = {0363-0129, 1095-7138},
	doi = {10.1137/0330046},
	abstract = {A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.},
	language = {en},
	number = {4},
	urldate = {2024-12-17},
	journal = {SIAM Journal on Control and Optimization},
	author = {Polyak, B. T. and Juditsky, A. B.},
	month = jul,
	year = {1992},
	pages = {838--855},
	file = {Polyak and Juditsky - 1992 - Acceleration of Stochastic Approximation by Averag.pdf:/Users/nico/Zotero/storage/4A8V42LF/Polyak and Juditsky - 1992 - Acceleration of Stochastic Approximation by Averag.pdf:application/pdf},
}

@misc{noauthor_gergely_nodate,
	title = {Gergely {Neu} and {Lorenzo} {Rosasco}. {Iterate} averaging as regularization for stochastic gradient descent. {In} {Conference} {On} {Learning} {Theory}, pp. 3222–3242. {PMLR}, 2018 - {Google} {Search}},
}

@inproceedings{neu_iterate_2018,
	title = {Iterate {Averaging} as {Regularization} for {Stochastic} {Gradient} {Descent}},
	abstract = {We propose and analyze a variant of the classic  Polyak–Ruppert averaging scheme, broadly used in stochastic gradient methods.  Rather than a uniform average of the iterates, we consider a weighted average, with weights decaying in a geometric fashion. In the context of linear least-squares regression, we show that this averaging scheme has the same regularizing effect, and indeed is asymptotically equivalent, to ridge regression. In particular, we derive finite-sample bounds for the proposed approach that match the best known results for regularized stochastic gradient methods.},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the 31st  {Conference} {On} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Neu, Gergely and Rosasco, Lorenzo},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {3222--3242},
	file = {Full Text PDF:/Users/nico/Zotero/storage/HT3VP5I7/Neu and Rosasco - 2018 - Iterate Averaging as Regularization for Stochastic.pdf:application/pdf},
}

@inproceedings{yang_swalp_2019,
	title = {{SWALP} : {Stochastic} {Weight} {Averaging} in {Low} {Precision} {Training}},
	shorttitle = {{SWALP}},
	abstract = {Low precision operations can provide scalability, memory savings, portability, and energy efficiency. This paper proposes SWALP, an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule. SWALP is easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the optimal solution for quadratic objectives, and to a noise ball asymptotically smaller than low precision SGD in strongly convex settings.},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Guandao and Zhang, Tianyi and Kirichenko, Polina and Bai, Junwen and Wilson, Andrew Gordon and Sa, Chris De},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7015--7024},
	file = {Full Text PDF:/Users/nico/Zotero/storage/MIT9IBCT/Yang et al. - 2019 - SWALP  Stochastic Weight Averaging in Low Precisi.pdf:application/pdf;Supplementary PDF:/Users/nico/Zotero/storage/LWRUP3ZD/Yang et al. - 2019 - SWALP  Stochastic Weight Averaging in Low Precisi.pdf:application/pdf},
}

@inproceedings{cha_swad_2021,
	title = {{SWAD}: {Domain} {Generalization} by {Seeking} {Flat} {Minima}},
	volume = {34},
	shorttitle = {{SWAD}},
	abstract = {Domain generalization (DG) methods aim to achieve generalizability to an unseen target domain by using only training data from the source domains. Although a variety of DG methods have been proposed, a recent study shows that under a fair evaluation protocol, called DomainBed, the simple empirical risk minimization (ERM) approach works comparable to or even outperforms previous methods. Unfortunately, simply solving ERM on a complex, non-convex loss function can easily lead to sub-optimal generalizability by seeking sharp minima. In this paper, we theoretically show that finding flat minima results in a smaller domain generalization gap. We also propose a simple yet effective method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima. SWAD finds flatter minima and suffers less from overfitting than does the vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy. SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large margins of +1.6\% averagely on out-of-domain accuracy. We also compare SWAD with conventional generalization methods, such as data augmentation and consistency regularization methods, to verify that the remarkable performance improvements are originated from by seeking flat minima, not from better in-domain generalizability. Last but not least, SWAD is readily adaptable to existing DG methods without modification; the combination of SWAD and an existing DG method further improves DG performances. Source code is available at https://github.com/khanrc/swad.},
	urldate = {2024-12-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cha, Junbum and Chun, Sanghyuk and Lee, Kyungjae and Cho, Han-Cheol and Park, Seunghyun and Lee, Yunsung and Park, Sungrae},
	year = {2021},
	pages = {22405--22418},
	file = {Full Text PDF:/Users/nico/Zotero/storage/K5NB8U7C/Cha et al. - 2021 - SWAD Domain Generalization by Seeking Flat Minima.pdf:application/pdf},
}

@inproceedings{athiwaratkun_there_2018,
	title = {There {Are} {Many} {Consistent} {Explanations} of {Unlabeled} {Data}: {Why} {You} {Should} {Average}},
	shorttitle = {There {Are} {Many} {Consistent} {Explanations} of {Unlabeled} {Data}},
	language = {en},
	urldate = {2024-12-19},
	author = {Athiwaratkun, Ben and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew Gordon},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/Users/nico/Zotero/storage/MD9KYGRU/Athiwaratkun et al. - 2018 - There Are Many Consistent Explanations of Unlabele.pdf:application/pdf},
}

@article{polyak_new_1990,
	title = {New stochastic approximation type procedures},
	volume = {7},
	journal = {Avtomatica i Telemekhanika},
	author = {Polyak, B. T.},
	year = {1990},
	pages = {98--107},
	file = {New stochastic approximation type procedures:/Users/nico/Zotero/storage/W5YAE2VZ/Polyak and Juditsky - 1992 - New stochastic approximation type procedures.pdf:application/pdf},
}

@misc{david_ruppert_efficient_1988,
	title = {Efficient {Estimations} from a {Slowly} {Convergent} {Robbins}-{Monro} {Process}},
	author = {David Ruppert},
	year = {1988},
	file = {ruppert_avg.pdf:/Users/nico/Zotero/storage/BASCEFRR/ruppert_avg.pdf:application/pdf},
}

@misc{li_training_2017,
	title = {Training {Quantized} {Nets}: {A} {Deeper} {Understanding}},
	shorttitle = {Training {Quantized} {Nets}},
	doi = {10.48550/arXiv.1706.02379},
	abstract = {Currently, deep neural networks are deployed on low-power portable devices by first training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Li, Hao and De, Soham and Xu, Zheng and Studer, Christoph and Samet, Hanan and Goldstein, Tom},
	month = nov,
	year = {2017},
	note = {arXiv:1706.02379 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/nico/Zotero/storage/ZULXHP3K/Li et al. - 2017 - Training Quantized Nets A Deeper Understanding.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/D7U4UCPH/1706.html:text/html},
}

@misc{keskar_large-batch_2017,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	shorttitle = {On {Large}-{Batch} {Training} for {Deep} {Learning}},
	doi = {10.48550/arXiv.1609.04836},
	abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	month = feb,
	year = {2017},
	note = {arXiv:1609.04836 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Preprint PDF:/Users/nico/Zotero/storage/K7PZ9A6T/Keskar et al. - 2017 - On Large-Batch Training for Deep Learning General.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/BA47US4S/1609.html:text/html},
}

@inproceedings{bach_non-strongly-convex_2013,
	title = {Non-strongly-convex smooth stochastic approximation with convergence rate {O}(1/n)},
	volume = {26},
	urldate = {2024-12-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bach, Francis and Moulines, Eric},
	year = {2013},
	file = {Full Text PDF:/Users/nico/Zotero/storage/S6B9ZTS2/Bach and Moulines - 2013 - Non-strongly-convex smooth stochastic approximatio.pdf:application/pdf},
}

@article{gyorfi_averaged_1996,
	title = {On the {Averaged} {Stochastic} {Approximation} for {Linear} {Regression}},
	volume = {34},
	issn = {0363-0129},
	doi = {10.1137/S0363012992226661},
	abstract = {A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.},
	number = {1},
	urldate = {2024-12-19},
	journal = {SIAM Journal on Control and Optimization},
	author = {Györfi, László and Walk, Harro},
	month = jan,
	year = {1996},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {31--61},
}

@inproceedings{li_trainable_2022,
	title = {Trainable {Weight} {Averaging}: {Efficient} {Training} by {Optimizing} {Historical} {Solutions}},
	shorttitle = {Trainable {Weight} {Averaging}},
	abstract = {Stochastic gradient descent (SGD) and its variants are considered as the de-facto methods to train deep neural networks (DNNs). While recent improvements to SGD mainly focus on the descent algorithm itself, few works pay attention to utilizing the historical solutions---as an iterative method, SGD has gone through substantial explorations before convergence. Recently, an interesting attempt is stochastic weight averaging (SWA), which significantly improves the generalization by simply averaging the solutions at the tail stage of training. In this paper, we realize that the averaging coefficients could be determined in a trainable manner and propose Trainable Weight Averaging (TWA), a novel optimization method in the reduced subspace spanned by historical solutions. TWA has much greater flexibility and can be applied to the head stage of training to achieve training efficiency while preserving good generalization capability. Further, we propose a distributed training scheme to resolve the memory burden of large-scale training with efficient parallel computation. In the extensive numerical experiments, (i) TWA achieves consistent improvements over SWA with less sensitivity to learning rate; (ii) applying TWA in the head stage of training largely speeds up the convergence, resulting in over \$40{\textbackslash}\%\$ time saving on CIFAR and \$30{\textbackslash}\%\$ on ImageNet with improved generalization compared with regular training.},
	language = {en},
	urldate = {2024-12-19},
	author = {Li, Tao and Huang, Zhehao and Tao, Qinghua and Wu, Yingwen and Huang, Xiaolin},
	month = sep,
	year = {2022},
	file = {Full Text PDF:/Users/nico/Zotero/storage/8JPJPI2I/Li et al. - 2022 - Trainable Weight Averaging Efficient Training by .pdf:application/pdf},
}

@inproceedings{zhang_lookahead_2019,
	title = {Lookahead {Optimizer}: k steps forward, 1 step back},
	volume = {32},
	shorttitle = {Lookahead {Optimizer}},
	abstract = {The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights" generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.},
	urldate = {2024-12-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Michael and Lucas, James and Ba, Jimmy and Hinton, Geoffrey E},
	year = {2019},
	file = {Full Text PDF:/Users/nico/Zotero/storage/6F8U5X6D/Zhang et al. - 2019 - Lookahead Optimizer k steps forward, 1 step back.pdf:application/pdf},
}


@misc{dahl_benchmarking_2023,
	title = {Benchmarking {Neural} {Network} {Training} {Algorithms}},
	language = {en},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Dahl, George E. and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and Bae, Juhan and Gilmer, Justin and Peirson, Abel L. and Khan, Bilal and Anil, Rohan and Rabbat, Mike and Krishnan, Shankar and Snider, Daniel and Amid, Ehsan and Chen, Kongtao and Maddison, Chris J. and Vasudev, Rakshith and Badura, Michal and Garg, Ankush and Mattson, Peter},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07179 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Dahl et al. - 2023 - Benchmarking Neural Network Training Algorithms.pdf:/Users/nico/Zotero/storage/H9DBW6SN/Dahl et al. - 2023 - Benchmarking Neural Network Training Algorithms.pdf:application/pdf},
}


@article{hagele2024scaling,
  author  = {Alexander H\"agele and Elie Bakouch and Atli Kosson and Loubna Ben Allal and Leandro Von Werra and Martin Jaggi},
  title   = {{Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations}},
  year    = {2024},
  journal = {Advances in Neural Information Processing Systems},
}

@article{DeepSeekV3, 
    title={DeepSeek-V3: A Strong Mixture-of-Experts Language Model}, author={DeepSeekAI}, year={2024},
}

@inproceedings{dozat_nadam,
    title = {Incorporating {Nesterov Momentum into Adam}},
    author = {Dozat Timothy},
    year={2016},
    booktitle    = {Proceedings of the 4th International Conference on Learning Representations},
    pages = {1--4},
    date = 2016
}

@inproceedings{Loshchilov2017DecoupledWD,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={International Conference on Learning Representations},
  year={2017},
}

@article{Kingma2014AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6980}
}


@misc{defazio_optimal_2024,
	title = {Optimal {Linear} {Decay} {Learning} {Rate} {Schedules} and {Further} {Refinements}},
	doi = {10.48550/arXiv.2310.07831},
	urldate = {2025-01-12},
	publisher = {arXiv},
	author = {Defazio, Aaron and Cutkosky, Ashok and Mehta, Harsh and Mishchenko, Konstantin},
	month = oct,
	year = {2024},
	note = {arXiv:2310.07831 [cs]
version: 2},
}


@misc{defazio_road_2024,
	title = {The {Road} {Less} {Scheduled}},
	abstract = {Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T . We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available1.},
	language = {en},
	urldate = {2024-05-28},
	publisher = {arXiv},
	author = {Defazio, Aaron and Xingyu and Yang and Mehta, Harsh and Mishchenko, Konstantin and Khaled, Ahmed and Cutkosky, Ashok},
	month = may,
	year = {2024},
	note = {arXiv:2405.15682 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {Defazio et al. - 2024 - The Road Less Scheduled.pdf:/Users/nico/Zotero/storage/DW7D2XJ9/Defazio et al. - 2024 - The Road Less Scheduled.pdf:application/pdf},
}

@article{shen_efficency,
author = {Shen, Li and Sun, Yan and Yu, Zhiyuan and Ding, Liang and Tian, Xinmei and Tao, Dacheng},
title = {On Efficient Training of Large-Scale Deep Learning Models},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {3},
issn = {0360-0300},
doi = {10.1145/3700439},
abstract = {The field of deep learning has witnessed significant progress in recent times, particularly in areas such as computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. However, it suffers extremely from the unstable training process and stringent requirements of computational resources. With the increasing demands on the adaption of computational capacity, though numerous studies have explored the efficient training field to a certain extent, a comprehensive summarization/guideline on those general acceleration techniques of training large-scale deep learning models is still much anticipated. In this survey, we present a detailed review of the general techniques for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) “data-centric,” including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) “model-centric,” including acceleration of basic modules, compression training, model initialization, and model-centric curriculum learning techniques, which focus on accelerating the training via reducing the calculations on parameters and providing better initialization; (3) “optimization-centric,” including the selection of learning rate, the employment of large batch size, the designs of efficient objectives, and model average techniques, which pay attention to the training policy and improving the generality for the large-scale models; (4) “budgeted training,” including some distinctive acceleration methods on source-constrained situations, e.g., for limitation on the total iterations; and (5) “system-centric,” including some efficient distributed frameworks and open source libraries that provide adequate hardware support for the implementation of the above-mentioned acceleration algorithms. By presenting this comprehensive taxonomy, our survey presents a comprehensive review to understand the general mechanisms within each component and their joint interaction. Meanwhile, we further provide a detailed analysis and discussion of future works on the development of general acceleration techniques, which could inspire us to re-think and design novel efficient paradigms. Overall, we hope that this survey will serve as a valuable guideline for general efficient training.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {57},
numpages = {36},
keywords = {Efficient training, large-scale models, general acceleration techniques}
}

@misc{morales_ema,
      title={Exponential Moving Average of Weights in Deep Learning: Dynamics and Benefits}, 
      author={Daniel Morales-Brotons and Thijs Vogels and Hadrien Hendrikx},
      year={2024},
      eprint={2411.18704},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{defazio2024optimallineardecaylearning,
      title={Optimal Linear Decay Learning Rate Schedules and Further Refinements}, 
      author={Aaron Defazio and Ashok Cutkosky and Harsh Mehta and Konstantin Mishchenko},
      year={2024},
      eprint={2310.07831},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{
li2023trainableWA,
title={Trainable Weight Averaging: Efficient Training by Optimizing Historical Solutions},
author={Tao Li and Zhehao Huang and Qinghua Tao and Yingwen Wu and Xiaolin Huang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@misc{tarvainen2018meanteachersbetterrole,
      title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results}, 
      author={Antti Tarvainen and Harri Valpola},
      year={2018},
      eprint={1703.01780},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
}

@misc{yazıcı2019avg_gan,
      title={The Unusual Effectiveness of Averaging in GAN Training}, 
      author={Yasin Yazıcı and Chuan-Sheng Foo and Stefan Winkler and Kim-Hui Yap and Georgios Piliouras and Vijay Chandrasekhar},
      year={2019},
      eprint={1806.04498},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
}

@misc{karras2024nvidia_diffusion_avg,
      title={Analyzing and Improving the Training Dynamics of Diffusion Models}, 
      author={Tero Karras and Miika Aittala and Jaakko Lehtinen and Janne Hellsten and Timo Aila and Samuli Laine},
      year={2024},
      eprint={2312.02696},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@misc{song2021score_based_gen,
      title={Score-Based Generative Modeling through Stochastic Differential Equations}, 
      author={Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
      year={2021},
      eprint={2011.13456},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{shi2023distributed_shampoo,
      title={A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale}, 
      author={Hao-Jun Michael Shi and Tsung-Hsien Lee and Shintaro Iwasaki and Jose Gallego-Posada and Zhijing Li and Kaushik Rangadurai and Dheevatsa Mudigere and Michael Rabbat},
      year={2023},
      eprint={2309.06497},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{
    anonymous2025_algoperf_results,
    title={Accelerating neural network training: An analysis of the AlgoPerf competition},
    author={Anonymous},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
}

@article{paszke2017pytorch,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017},
  journal={NIPS 2017 Workshop Autodiff}
}

@article{hu2020open,
  title={Open graph benchmark: Datasets for machine learning on graphs},
  author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  year={2020}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  year={2009},
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{criteo2014dataset,
    author = {Criteo A. I. Lab},
    title = {Criteo 1TB Click Logs dataset},
    year = {2014}
}

@article{naumov2019deep,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}


@inproceedings{unet_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
}-+9

@article{fastMRI_dataset,
  author = {Jure Zbontar and
                  Florian Knoll and
                  Anuroop Sriram and
                  Matthew J. Muckley and
                  Mary Bruno and
                  Aaron Defazio and
                  Marc Parente and
                  Krzysztof J. Geras and
                  Joe Katsnelson and
                  Hersh Chandarana and
                  Zizhao Zhang and
                  Michal Drozdzal and
                  Adriana Romero and
                  Michael G. Rabbat and
                  Pascal Vincent and
                  James Pinkerton and
                  Duo Wang and
                  Nafissa Yakubova and
                  Erich Owens and
                  C. Lawrence Zitnick and
                  Michael P. Recht and
                  Daniel K. Sodickson and
                  Yvonne W. Lui},
  title        = {fastMRI: An Open Dataset and Benchmarks for Accelerated {MRI}},
  journal      = {CoRR},
  volume       = {abs/1811.08839},
  year         = {2018},
  eprinttype    = {arXiv},
  eprint       = {1811.08839},
  timestamp    = {Mon, 09 Nov 2020 08:50:24 +0100},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  year={2017}
}

@inproceedings{bojar2017findings,
  title={Findings of the 2017 conference on machine translation (wmt17)},
  author={Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huang, Shujian and Huck, Matthias and Koehn, Philipp and Liu, Qun and Logacheva, Varvara and others},
  year={2017},
  organization={Association for Computational Linguistics}
}

@article{battaglia2018relational,
      title={Relational inductive biases, deep learning, and graph networks}, 
      author={Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
      year={2018},
      journal={arXiv preprint arXiv: 1806.01261}
}


@InProceedings{deepspeech_amodei16,
  title = 	 {Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin},
  author = 	 {Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and Chen, Jie and Chen, Jingdong and Chen, Zhijie and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Ding, Ke and Du, Niandong and Elsen, Erich and Engel, Jesse and Fang, Weiwei and Fan, Linxi and Fougner, Christopher and Gao, Liang and Gong, Caixia and Hannun, Awni and Han, Tony and Johannes, Lappi and Jiang, Bing and Ju, Cai and Jun, Billy and LeGresley, Patrick and Lin, Libby and Liu, Junjie and Liu, Yang and Li, Weigao and Li, Xiangang and Ma, Dongpeng and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Peng, Yiping and Prenger, Ryan and Qian, Sheng and Quan, Zongfeng and Raiman, Jonathan and Rao, Vinay and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Srinet, Kavya and Sriram, Anuroop and Tang, Haiyuan and Tang, Liliang and Wang, Chong and Wang, Jidong and Wang, Kaifu and Wang, Yi and Wang, Zhijian and Wang, Zhiqian and Wu, Shuang and Wei, Likai and Xiao, Bo and Xie, Wen and Xie, Yan and Yogatama, Dani and Yuan, Bin and Zhan, Jun and Zhu, Zhenyao},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {173--182},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
}

@misc{gulati2020conformer,
      title={Conformer: Convolution-augmented Transformer for Speech Recognition}, 
      author={Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},
      year={2020},
      eprint={2005.08100},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
}

@inproceedings{librispeech,
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Librispeech: An ASR corpus based on public domain audio books}, 
  year={2015},
  volume={},
  number={},
  pages={5206-5210},
  keywords={Resource description framework;Genomics;Bioinformatics;Blogs;Information services;Electronic publishing;Speech Recognition;Corpus;LibriVox},
  doi={10.1109/ICASSP.2015.7178964}}


@misc{garrigos2024handbookconvergence,
      title={Handbook of Convergence Theorems for (Stochastic) Gradient Methods}, 
      author={Guillaume Garrigos and Robert M. Gower},
      year={2024},
      eprint={2301.11235},
      archivePrefix={arXiv},
      primaryClass={math.OC},
}

@inproceedings{
Gupta2020Stochastic,
title={Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well},
author={Vipul Gupta and Santiago Akle Serrano and Dennis DeCoste},
booktitle={International Conference on Learning Representations},
year={2020},
}

@misc{schaipp2025surprising_lr,
      title={The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training}, 
      author={Fabian Schaipp and Alexander Hägele and Adrien Taylor and Umut Simsekli and Francis Bach},
      year={2025},
      eprint={2501.18965},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{merity2017regularizingoptimizinglstmlanguage,
      title={Regularizing and Optimizing LSTM Language Models}, 
      author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
      year={2017},
      eprint={1708.02182},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{
kasimbeg2025accelerating,
title={Accelerating neural network training: An analysis of the AlgoPerf competition},
author={Priya Kasimbeg and Frank Schneider and Runa Eschenhagen and Juhan Bae and Chandramouli Shama Sastry and Mark Saroufim and BOYUAN FENG and Less Wright and Edward Z. Yang and Zachary Nado and Sourabh Medapati and Philipp Hennig and Michael Rabbat and George E. Dahl},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
}

@INPROCEEDINGS{szegedy_2016_rethinking,
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Rethinking the Inception Architecture for Computer Vision}, 
  year={2016},
  volume={},
  number={},
  pages={2818-2826},
  keywords={Convolution;Computer architecture;Training;Computational efficiency;Computer vision;Benchmark testing;Computational modeling},
  doi={10.1109/CVPR.2016.308}}

@misc{geiping2025scalingtesttimecomputelatent,
      title={Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach}, 
      author={Jonas Geiping and Sean McLeish and Neel Jain and John Kirchenbauer and Siddharth Singh and Brian R. Bartoldson and Bhavya Kailkhura and Abhinav Bhatele and Tom Goldstein},
      year={2025},
      eprint={2502.05171},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{loshchilov2017sgdr,
      title={SGDR: Stochastic Gradient Descent with Warm Restarts}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2017},
      eprint={1608.03983},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}