\section{Related Work}
\label{sec:related_work}

The idea of averaging iterates along a stochastic optimization trajectory dates back to \citet{polyak_new_1990} and \citet{david_ruppert_efficient_1988}, and is often referred to as Polyak–Ruppert averaging.
It has been extensively studied in the stochastic approximation framework, \citep{polyak_acceleration_1992, bach_non-strongly-convex_2013, neu_iterate_2018, lakshminarayanan_linear_2018}, and is a common technique to derive convergence guarantees in both convex and nonconvex optimization~\citep{garrigos2023handbook}. 

\vspace{-2mm}
\paragraph{Deep Learning applications.}
Weight averaging techniques have seen extensive use in the Deep Learning community, often without explicit recognition or emphasis, despite their effectiveness. Influential works such as \citet{szegedy_2016_rethinking}, \citet{vaswani2017attention}, and \citet{merity2017regularizingoptimizinglstmlanguage} have demonstrated their ability to enhance model performance and mitigate overfitting.


\vspace{-2mm}
\paragraph{SWA.}
The work of \citet{izmailov_averaging_2019} sparked renewed interest in weight averaging by demonstrating how averaging points along the SGD trajectory leads to wider minima and improves generalization performance. Their approach, Stochastic Weight Averaging (SWA), has since been applied to semi-supervised learning \citep{tarvainen2018meanteachersbetterrole, athiwaratkun_there_2018}, low-precision training \citep{yang_swalp_2019}, domain generalization tasks \citep{cha_swad_2021}, and meta-optimization \citep{li_trainable_2022}.

\vspace{-2mm}
\paragraph{LAWA and EMA.} In the original formulation of SWA, a pretrained model is trained with a cyclical or constant learning rate, and multiple checkpoints are collected and later averaged.
\citet{kaddour_stop_2022} proposed Latest Weight Averaging (LAWA), an online algorithm that averages the latest checkpoints in a rolling window, showing significant speed-ups on vision and language tasks. Further modifications of LAWA demonstrated notable boosts in pretraining modern decoder-only language models \citep{sanyal_early_2023}. %, and very recently \citet{DeepSeekV3} preserved an exponential moving average of the model parameters in CPU memory to estimate the model performance after learning rate decay. 
A valuable alternative to moving window averaging techniques like LAWA is Exponential Moving Averaging (EMA) \citep{li_switch_2024, morales_ema}. It retains similar advantages of rolling window averaging, and constitutes an indispensable technique for high-quality image synthesis models such as GANs and diffusion models \citep{yazıcı2019avg_gan, song2021score_based_gen, karras2024nvidia_diffusion_avg}.

\vspace{-2mm}
\paragraph{Connection to learning rate annealing.} 
Classical stochastic smooth convex optimization rates showcase a tight link between WA and learning rate annealing, suggesting a practical interplay between these techniques~(see e.g. Theorem 5.3. in~\citet{garrigos2023handbook}).
Intuitively, averaging weights along the training trajectory reduces noise and might act as a proxy for learning rate decay. In fact, \citet{sandler_training_2023} proved the theoretical and empirical equivalence between WA and decaying learning rate for SGD. 
However, despite this appealing result, 
% a thorough analysis of the interplay between averaging and learning rate scheduling on modern tasks and optimization algorithms is still missing, and bridging the gap between the two is an open research problem \citep{hagele2024scaling}: 
modern Deep Learning models are still predominantly trained with learning rate annealing \citep{hagele2024scaling}, even when maintaining an EMA of model weights \citep{DeepSeekV3}. 
A recent study by \citet{defazio_road_2024} specifically investigates this connection, proposing an approach that fully replaces learning rate schedules with iterate averaging and demonstrating strong performance on the same benchmark used in this analysis.
Whereas \citet{defazio_road_2024} incorporates averaging directly into the optimization procedure, we explore a different flavor of averaging, where the averaged weights do not influence the updates—akin to Polyak averaging, SWA, LAWA, and EMA.
% A recent effort to fully replace learning rate schedules with iterate averaging was developed by \citet{defazio_road_2024}, demonstrating strong performance on the same benchmark used in this analysis.

\vspace{-2mm}
\paragraph{Model soups.} Finally, a different but notable approach that leverages the benefits of averaging is model soups \citep{wortsman_model_2022}. In this case, multiple models are trained with different hyperparameter configurations and later aggregated, resulting in an ensemble with improved accuracy and robustness.

In this work, we demonstrate the benefits of weight averaging techniques on a challenging optimization benchmark \citep{dahl_benchmarking_2023}, hoping to encourage broader adoption of these methods in training large-scale Machine Learning models.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%