\section{Related Work}
\label{sec:related-work}

The quality and debugging of rules in knowledge-based systems have been studied in several contexts, with efforts ranging from foundational correctness principles to advanced explanation frameworks. This section reviews key contributions and situates our work within this landscape.

\subsection{Rule Quality Principles}

\citet{landauer1990correctness} propose a set of acceptability principles—Consistency, Completeness, Irredundancy, Connectivity, and Distribution—to guide the construction and validation of rule bases. These principles provide foundational criteria for evaluating rule systems, emphasizing logical soundness, efficiency, and simplicity. 
% (WV) I think the work is rather complementary here
%However, their approach primarily focuses on technical correctness and identifying specification errors, without addressing how the behavior of the rules can be effectively communicated to knowledge engineers or end-users. 
In this vein, we focus on communicating the behavior of rules to knowledge engineers or end-users by employing user-centric explanations— trace-based, contextual, contrastive, and counterfactual—to concretely help with identifying inconsistencies, incompleteness, accuracy, and fairness of decision-making.

% make rule validation not only systematic but also explainable and actionable. By bridging technical correctness with user understanding, our approach empowers engineers to debug and refine rules effectively.

% OS: Addressed WV comment: ``I think we need to have a closer look at Gebser et al. as I don't think the ASP focus is strong enough of an argument. But, if we are the first ones (to the best of our knowledge, and if we are sure enough :-) to investigate these explanations to refine rules, we could mention that too."
\subsection{Debugging and Validation Techniques}

In the domain of declarative programming, \citet{gebser2008meta} introduces a meta-programming technique for debugging answer-set programming (ASP). Their approach allows querying why a single interpretation, or class of interpretations, is not an answer set for a program.
% focuses on analyzing why certain expected interpretations fail to be answer sets, using a formal error classification scheme and leveraging ASP solvers for debugging queries. 
This method identifies semantical errors within the context of ASP and can be used for debugging answer-set programs.
However, their work is inherently tied to the semantics of ASP and does not extend to broader rule-based reasoning systems. % or knowledge graphs. (WV) don't think we've mentioned KG until now
In contrast, our framework generalizes debugging and validation techniques through the use of a number of explanation types.
% (WV) think we've said this already :-)
%—trace-based, contextual, contrastive, and counterfactual. 
% These explanations provide a structured mechanism to evaluate and refine rules in knowledge-based systems, making them applicable across domains and accessible to a broader range of users, including non-technical stakeholders.
By focusing on explanation-driven rule refinement rather than program-level debugging, our work moves from identifying errors within a specific programming paradigm to enhancing the quality of rules that underpin knowledge-based reasoning in general. 
% (WV) idem
% This approach aligns rule validation with practical usability, trust, and transparency—key attributes of modern, user-centric AI systems.

\subsection{Explanation Frameworks}

Explanation frameworks have emerged as critical tools for enhancing the interpretability and usability of AI systems. Explanation Ontology (EO)~\cite{chari2020explanation, chari2023explanation} provides a general-purpose semantic framework for representing explanations, supporting 15 distinct explanation types with clear definitions and logical formalizations using Web Ontology Language (OWL). EO enables system designers to connect explanations to underlying data and reasoning processes, ensuring that AI systems address user-centered needs effectively. Practical applications of EO include contextual explanations in healthcare, such as helping clinical practitioners understand AI-driven risk predictions for Type-2 Diabetes and Chronic Kidney Disease (CKD)~\cite{chari2023informing}. While EO provides a robust theoretical foundation for explainability, our framework focuses on practical implementation within the MIT App Inventor Punya platform, demonstrating its utility through real-world applications and diverse explanation types.

Similarly, XAIN (eXplanations for AI in Notation3)~\cite{vanwoensel2023explanations} supports trace-based, contrastive, and counterfactual explanations, specifically focusing on healthcare applications. For example, XAIN explains recommendations for Chronic Obstructive Pulmonary Disease (COPD) patients in order to effect understanding, persuasion, and behavior change. 
% (WV) don't think that XAIN was domain-specific per se (?)
While XAIN showcases the power of symbolic reasoning for generating explanations, it does not address rule quality as a central theme. Our work builds upon these advancements by implementing explanations that explicitly target rule validation and debugging, % as part of the explanation process, 
and integrating these capabilities into a lightweight reasoning architecture for mobile and resource-constrained environments, as described in the following section.

% (WV) it's perhaps a bit dangerous to say that we specifically build on these methods (e.g., we don't address those quality criteria directly, or allow requiring for non-matching interpretations)
% Building on foundational principles for rule quality~\cite{landauer1990correctness}, program-level debugging techniques~\cite{gebser2008meta}, and user-centric explanation frameworks like EO~\cite{chari2020explanation} and XAIN~\cite{vanwoensel2023explanations}, our framework unifies these efforts into an integrated system for rule validation and quality enhancement. Our work bridges the gap between system-level correctness and user-centered transparency by focusing on diverse explanation types and their application in several use cases.