@inproceedings{chari2020explanation,
author="Chari, Shruthi
and Seneviratne, Oshani
and Gruen, Daniel M.
and Foreman, Morgan A.
and Das, Amar K.
and McGuinness, Deborah L.",
editor="Pan, Jeff Z.
and Tamma, Valentina
and d'Amato, Claudia
and Janowicz, Krzysztof
and Fu, Bo
and Polleres, Axel
and Seneviratne, Oshani
and Kagal, Lalana",
title="Explanation Ontology: A Model of Explanations for User-Centered AI",
booktitle="The Semantic Web -- ISWC 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="228--243",
abstract="Explainability has been a goal for Artificial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system's AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users' needs and a system's capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.",
isbn="978-3-030-62466-8"
}

@article{chari2023explanation, 
    author = {Shruthi Chari and Oshani Seneviratne and Mohamed Ghalwash and Sola Shirai and Daniel M. Gruen and Pablo Meyer and Prithwish Chakraborty and Deborah L. McGuinness},
    title ={Explanation Ontology: A general-purpose, semantic representation for supporting user-centered explanations},
    journal = {Semantic Web},
    volume = {15},
    number = {4},
    pages = {959-989},
    year = {2024},
    doi = {10.3233/SW-233282},
    URL = {https://doi.org/10.3233/SW-233282},
    eprint = {https://doi.org/10.3233/SW-233282},
    abstract = { In the past decade, trustworthy Artificial Intelligence (AI) has emerged as a focus for the AI community to ensure better adoption of AI models, and explainable AI is a cornerstone in this area. Over the years, the focus has shifted from building transparent AI methods to making recommendations on how to make black-box or opaque machine learning models and their results more understandable by experts and non-expert users. In our previous work, to address the goal of supporting user-centered explanations that make model recommendations more explainable, we developed an Explanation Ontology (EO). The EO is a general-purpose representation that was designed to help system designers connect explanations to their underlying data and knowledge. This paper addresses the apparent need for improved interoperability to support a wider range of use cases. We expand the EO, mainly in the system attributes contributing to explanations, by introducing new classes and properties to support a broader range of state-of-the-art explainer models. We present the expanded ontology model, highlighting the classes and properties that are important to model a larger set of fifteen literature-backed explanation types that are supported within the expanded EO. We build on these explanation type descriptions to show how to utilize the EO model to represent explanations in five use cases spanning the domains of finance, food, and healthcare. We include competency questions that evaluate the EO’s capabilities to provide guidance for system designers on how to apply our ontology to their own use cases. This guidance includes allowing system designers to query the EO directly and providing them exemplar queries to explore content in the EO represented use cases. We have released this significantly expanded version of the Explanation Ontology at https://purl.org/heals/eo and updated our resource website, https://tetherless-world.github.io/explanation-ontology, with supporting documentation. Overall, through the EO model, we aim to help system designers be better informed about explanations and support these explanations that can be composed, given their systems’ outputs from various AI models, including a mix of machine learning, logical and explainer models, and different types of data and knowledge available to their systems. }
}

@article{chari2023informing,
title = {Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes},
journal = {Artificial Intelligence in Medicine},
volume = {137},
pages = {102498},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102498},
url = {https://www.sciencedirect.com/science/article/pii/S093336572300012X},
author = {Shruthi Chari and Prasant Acharya and Daniel M. Gruen and Olivia Zhang and Elif K. Eyigoz and Mohamed Ghalwash and Oshani Seneviratne and Fernando Suarez Saiz and Pablo Meyer and Prithwish Chakraborty and Deborah L. McGuinness},
keywords = {User-driven, Clinical explainability, Contextual explanations, Question-answering approach, Type-2 diabetes comorbidity risk prediction},
abstract = {Medical experts may use Artificial Intelligence (AI) systems with greater trust if these are supported by ‘contextual explanations’ that let the practitioner connect system inferences to their context of use. However, their importance in improving model usage and understanding has not been extensively studied. Hence, we consider a comorbidity risk prediction scenario and focus on contexts regarding the patients’ clinical state, AI predictions about their risk of complications, and algorithmic explanations supporting the predictions. We explore how relevant information for such dimensions can be extracted from Medical guidelines to answer typical questions from clinical practitioners. We identify this as a question answering (QA) task and employ several state-of-the-art Large Language Models (LLM) to present contexts around risk prediction model inferences and evaluate their acceptability. Finally, we study the benefits of contextual explanations by building an end-to-end AI pipeline including data cohorting, AI risk modeling, post-hoc model explanations, and prototyped a visual dashboard to present the combined insights from different context dimensions and data sources, while predicting and identifying the drivers of risk of Chronic Kidney Disease (CKD) - a common type-2 diabetes (T2DM) comorbidity. All of these steps were performed in deep engagement with medical experts, including a final evaluation of the dashboard results by an expert medical panel. We show that LLMs, in particular BERT and SciBERT, can be readily deployed to extract some relevant explanations to support clinical usage. To understand the value-add of the contextual explanations, the expert panel evaluated these regarding actionable insights in the relevant clinical setting. Overall, our paper is one of the first end-to-end analyses identifying the feasibility and benefits of contextual explanations in a real-world clinical use case. Our findings can help improve clinicians’ usage of AI models.}
}

@inproceedings{gebser2008meta,
      title={A meta-programming technique for debugging answer-set programs.},
  author={Gebser, Martin and P{\"u}hrer, J{\"o}rg and Schaub, Torsten and Tompits, Hans},
  booktitle={AAAI},
  volume={8},
  pages={448--453},
  year={2008}
}

@article{landauer1990correctness,
  title={Correctness principles for rule-based expert systems},
  author={Landauer, Christopher},
  journal={Expert Systems with Applications},
  volume={1},
  number={3},
  pages={291--316},
  year={1990},
  publisher={Elsevier}
}

@inproceedings{vanwoensel2023explanations,
author="Van Woensel, William
and Scioscia, Floriano
and Loseto, Giuseppe
and Seneviratne, Oshani
and Patton, Evan
and Abidi, Samina",
editor="Juarez, Jose M.
and Fernandez-Llatas, Carlos
and Bielza, Concha
and Johnson, Owen
and Kocbek, Primoz
and Larra{\~{n}}aga, Pedro
and Martin, Niels
and Munoz-Gama, Jorge
and {\v{S}}tiglic, Gregor
and Sepulveda, Marcos
and Vellido, Alfredo",
title="Explanations of Symbolic Reasoning to Effect Patient Persuasion and Education",
booktitle="Explainable Artificial Intelligence and Process Mining Applications for Healthcare",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="62--71",
abstract="Artificial Intelligence (AI) models can issue smart, context-sensitive recommendations to help patients self-manage their illnesses, including medication regimens, dietary habits, physical activity, and avoiding flare-ups. Instead of merely positing an ``edict,'' the AI model can also explain why the recommendation was issued: why one should stay indoors (e.g., increased risk of flare-ups), why further calorie intake should be avoided (e.g., met the daily limit), or why the care provider should be contacted (e.g., prescription change). The goal of these explanations is to achieve understanding and persuasion effects, which, in turn, targets education and long-term behavior change. Symbolic AI models facilitate explanations as they are able to offer logical proofs of inferences (or recommendations) from which explanations can be generated. We implemented a modular framework called XAIN (eXplanations for AI in Notation3) to explain symbolic reasoning inferences in a trace-based, contrastive, and counterfactual way. We applied this framework to explain recommendations for Chronic Obstructive Pulmonary Disease (COPD) patients to avoid flare-ups. For evaluation, we propose a questionnaire that captures understanding, persuasion, education, and behavior change, together with traditional XAI metrics including fidelity (soundness, completeness) and interpretability (parsimony, clarity).",
isbn="978-3-031-54303-6"
}

