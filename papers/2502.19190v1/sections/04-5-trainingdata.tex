A related point to the previous provocation has to do with the nature of training data. Up until very recently, advances in generative AI have relied upon on vast, heterogeneous pretraining datasets. These draw from a wide range of sources: internet content, copyrighted books, scientific articles, and now even synthetic data from other AI models, not to mention a variety (but to be clear, not the entirety) of languages, time periods, communities, and genres \cite{Anil_Dai_Firat_Johnson_Lepikhin_Passos_Shakeri_Taropa_Bailey_Chen_etal._2023,brown_language_2020,nostalgebraist_2022}. Despite this range, pretraining data is often treated as a homogenous, undifferentiated resource—often likened to ``oil''—where individual pieces of data are assumed to be interchangeable and valued primarily for their downstream utility. When individual points within these massive datasets are considered, they are typically evaluated through metrics like ``toxicity'' or ``quality'' \citep{Lucy_Gururangan_Soldaini_Strubell_Bamman_Klein_Dodge_2024}, framings that prioritize how the data will impact a model’s performance. This shallow approach to pretraining data is further compounded by  ``documentation debt'' \cite{Bandy_Vincent_2021}; the actual contents of pretraining data and why specific sources are chosen are rarely disclosed. 

This reduction and obfuscation of training data is now widely recognized as an ethical problem as well as a technical one. Inspired by practices in the electronics industry, the influential ``Datasheets for Datasets'' \cite{Gebru_Morgenstern_Vecchione_Vaughan_Wallach_III_Crawford_2021} paradigm introduced a structured approach to documenting machine learning datasets, which has been implemented for high-profile data repositories such as HuggingFace. We have also increasingly seen empirical tests that assess how different factors such as dataset age, degree of toxicity, and quality level, impact model performance \cite{longpre-etal-2024-pretrainers}. " While these are urgent and necessary interventions, approaching pretraining data from a humanistic perspective can offer additional affordances. For example, the development of rich metadata can help to ensure that datasets more accurately represent the contents they seek to include. For a dataset of books, this might include both computationally and manually derived metadata like the book’s title, publication date, genre, author, and perhaps even demographic information about the author like race, gender, and geography, as in a recent project aimed at improving the analysis of the more than 17 million volumes of digitized text in the HathiTrust Digital Library \cite{underwood_page-level_2014,Underwood_Kimutis_Witte_2020}. 

Humanities researchers are also keenly aware that both metadata and data curation choices radically impact any downstream task, and have developed several detailed strategies for documenting these choices and their potential impact. For example, digital archeologies of datasets place specific data sources in the broader context of the conditions of their creation, revealing biases and limitations \cite{lee_compounded_2021,fyfe_archaeology_2016}. Data narratives seek to situate datasets in human contexts and social histories beyond their immediate technical use \cite{pouchard_data_2014}. Another recent approach used by initiatives such as Post45 Data Collective, the Nineteenth Century Data Collective, and Responsible Datasets in Context Project, is the data essay. These longform works describe the data’s historical context, collection and curation, limitations, and ethical considerations, build on important prior work like ``datasheets''  but expand to include more abstract considerations of how any dataset is shaped by, and must be considered in light of, history, society, power, and specific cultural phenomena.

What these examples all point towards is how a consideration of a dataset’s conceptual characteristics, in addition to its technical characteristic and downstream utility, can improve the transparency and accountability of AI models and have other benefits.  Knowing more about training data can help model developers avoid the perpetual cycle of after-the-fact fixes, such as guardrails, RLHF, post-hoc content moderation, and other patchwork solutions. Knowing more about where data actually came from and who it belongs to can help developers address ongoing intellectual property issues. And knowing more about training data can even lead to innovation. Because most developers have not cared to understand their training data in great depth, how models are influenced by finer features of training data remains underexplored.
