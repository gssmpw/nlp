As boyd and Crawford assert in their original provocations, ``bigger data is not always better data'' \cite{boyd_Crawford_2012}. This applies to models as well. For the first several years of LLM development, the growth in numbers of parameters was rivaled only by the growth in the number of charts documenting the rising number of those parameters. But the axiom that bigger is better has now met its match. Increasingly, studies have found that more data, more parameters, and more compute no longer lead to more accurate output (e.g. (\cite{Zhou_Schellaert_Martínez-Plumed_Moros-Daval_Ferri_Hernández-Orallo_2024,mckenzie_inverse_2024}. These findings have raised interesting technical questions, as well anxieties that we are "running out of data" \cite{noauthor_openai_nodate}. But to humanities researchers, these findings serve to prove a more philosophical point: that the goal of a single universal source of ``intelligence'' is a fraught endeavor. More than that, it will never lead to complete knowledge, as feminist philosophers as early as the seventeenth century have shown \citep{Newcastle_2019, Emecheta_1983, Haraway_1988}. 

Arguably, the humanities’ most universal commitment is to the diversity and complexity of the human experience. More than that, centuries of humanities scholarship has confirmed the asymptotic relationship between increased understanding and complete knowledge. The idea that we could ever build a model of artificial ``general'' intelligence is not only a fool’s errand; it is uninformed by how intelligence actually works. Somewhat paradoxically, this same body of work---that is to say, humanities scholarship---offers the clearest path forward. Smaller models, with intentionally-curated datasets, and trained or fine-tuned for precise tasks, promise to bring us closer to a goal we all share: of more informed, more accurate, and more precise knowledge. In the field of computational humanities, this work has already begun. For example, Giselle Gonzalez Garcia and Christian Weilbach brought their respective backgrounds in the fields of history and computer science to customize an LLM for research assistant in the field of Irish migration studies \cite{Garcia_Weilbach}. Ted Underwood and collaborators at UIUC are currently training an LLM from scratch on only nineteenth-century English-language writing, so as to be able to run counterfactual experiments on the past.

The promise of smaller, more bespoke models also points to the need for a shift in thinking. Rather than being driven by the ``bigger is better'' agenda that characterizes much of the current corporate research landscape, we might reestablish a research agenda born of hundreds of years of humanities scholarship, and informed by the evidence of thousands of years of actual human experience. This will lead us to value the expertise of domain experts---not only as prompt engineers, as has been the case thus far, but as collaborators in model development. And it will lead to a goal we all share: of more informed, more accurate, and more precise knowledge.
