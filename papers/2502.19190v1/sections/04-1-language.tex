Humanists have wrestled for decades with the interplay between language and intention.
What is the implication of language that is produced in a way that is---by construction---devoid of human intention?
The field of natural language processing has been shaped by decades of research in both machine learning and linguistics. This has led to explanations (and rejections) of the language generated by LLMs and related AI systems that rely on linguistic theories of ``communicative intent'' \cite{Bender_Gebru_McMillan-Major_Shmitchell_2021}. This framing remains essential for understanding the output of these models in conversational contexts, but there are additional theories from the humanities that can be used to understand the broader significance (or lack thereof) of the language these models produce. 

More concretely, the theories of meaning-making first developed by literary scholars in the 1960s and 1970s offer ways of making sense of AI-generated language that are not dependent upon being able to identify any particular speaker or source. As early as 1967, philosopher Roland Barthes proposed that the meaning of any particular text is determined not by the author’s intention, but by the reader’s interpretation \cite{barthes1977image}. Literary theorist Jacques Derrida \citeyear{Derrida_1998} articulated a similar sentiment in his critique of logocentrism, or the idea that words are bound to a static meaning in the world. Scholars in the fields of Black studies, ethnic studies, gender and sexuality studies, and more, see the literal grammar of the English language, which has long functioned to deny people their humanity, as something that must be challenged wholesale. Far from enabling a descent into relativism, theories such as these become the basis for an expansion of the sources of meaning, as well as the techniques we can employ (or ourselves develop) for making sense of the range of writing we encounter in the world. 

Consider how LLMs generate text by predicting sequences of words. These predictions are based on both observed patterns and on human preferences and feedback. The result is often output that is factually wrong yet linguistically fluent and seemingly coherent---these are the ``hallucinations'' that have become a topic of research interest \cite{koenecke_careless_2024} and (justifiable) public concern. The theories cited above allow us to understand how the human tendency to create meaning through the interpretive expectations of intention and care can confer the illusion that the model ``knows'' something or someone. This is compounded by our current climate of ``AI hype'' and the nonconsensual infusion of AI into our lives, which have together naturalized artificial ``knowing'' over the complicated, messy, and polyrhythmic meaning-making conducted by humans when using language (oral, written, and otherwise) in social contexts. Humanities scholars have already been at the forefront of analyzing the output of genAI models, both text and image, not for what they ``know'' but for what meaning their output elicits about the cultures from which they emerged. 

Meaning is always people-fueled, socially-driven, irreverent, and impossible to map consistently. Especially in the context of the Global Minority, making meaning from language also requires an awareness of the history of evading and erasing dominant meanings, and extracting additional meaning from guarded words. Take for example the work of the Translation Lab, which set out to translate a famed interview with Ousmane Sembène, the Senegalese filmmaker, using SmartCat, a professional AI translation platform. The team quickly found that there were few Twi and Wolof terms on suggestion, and the terms that were suggested varied widely from the meaning expressed in the original French. As humanists, we are not surprised. AI ``knowing'' is an easier solution to a complicated reality of meaning forged between language, place, time, and social relations. 