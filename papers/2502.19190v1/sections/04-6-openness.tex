As an increasing number of open and open-source models have been released for public use, many researchers in both technical and humanistic fields have moved on from the issues surrounding black-boxed, proprietary, and pay-for-access models. But the question of openness---both what it means and what it implies---remains unresolved. In short: there are no easy fixes when working with objects of culture (which include both training data and models, as has been explained). In this section we introduce some key considerations from the humanities aimed at illustrating how questions of openness, ownership, and access, rarely have yes-or-no answers and must be consistently reevaluated as contexts and conditions change.   

It may be obvious at this point to assert that Meta’s ``open'' models, while available for use, remain closed in terms of information about their underlying data and training processes. They also still require significant computational resources, which becomes economically restrictive as discussed more in the next section. This limitation also holds for open-source models such as AI2’s OLMo models, and while we welcome additional entries in this area, as well as the proliferation of smaller desktop models, the questions of which and whose data and models should be open, for which purposes, and for whose goals, remains unresolved. Consider current debates over the inclusion of copyrighted content in training data. On the surface, this would seem to have an easy answer: individuals should control access to their data, including their creative content, and if they do not want it included in training data, it should not be. But consider the downstream effects on scholarship aimed at understanding those people and their cultures. If their data is not included in the model, future researchers cannot employ it to learn about the past. Set against the backdrop of corporate extraction, this use case may not be the most important one to consider. (And to be clear, preserving the ability of writers and artists to create their art and be fairly compensated for it should remain paramount). But there are examples from computational research in the humanities that point to how this work can be done on a case-by-case basis. For example, \cite{Bamman_Samberg_So_Zhou_2024} use fair use exceptions and an exemption to the Digital Millennium Copyright Act (DMCA) to create one of the largest digitized collections of copyrighted films. The HathiTrust Digital Library, which consists of over 18 million digitized books, offers derivative data and virtual environments that enable researchers to access and analyze copyright-restricted materials for educational purposes \cite{hathitrust}. However, the time, labor, and cost involved in maintaining this environment is substantial, and it is unclear if it will be able to continue into the future. 

In addition, there remain questions of access to data from communities that have not or cannot provide consent. These questions apply to community data that might even technically be ``open'' and scrapeable from the web. For example, fanfiction writers and readers often operate with an expectation of privacy, participating in intimate, close-knit communities, despite their work being openly published online. But because of the ease of collecting data from the web, users’ stories and interactions can be gathered and shared even without their knowledge or consent, violating their expectations and community norms. According to \citet{Dym_Fiesler_2020}, members of fanfiction communities have expressed concerns about the potential negative repercussions of their data being shared with broader audiences, such as being outed or facing professional consequences. Similarly, the Documenting the Now project has highlighted the potential harms that can arise when social media data, especially data related to protests like the Black Lives Matter movement, is archived or shared. These concerns must also be placed in the larger context of the history of data, and of data extraction, both of which can be traced to specific and violent pasts. We discuss this further in the section below.

There are also deeper questions that emerge from the use of synthetic data, as well as from the ability of models to generate ``new'' content that would seem to circumvent these real-world harms. Beyond banning specific keywords, we must consider the impact of our ability to prompt models to generate what Saidiya Hartman would describe as ``scenes of subjection''---text and images that, in restaging scenes of historical violence, even through computational means, introduces questions of our own complicity---even with respect to cultures or phenomena that we might simply seek to learn more about. We must also contend with the underlying motivation---and very often, the final use case---of much of the research on minoritized groups. As we survey the devastation that has been brought upon Ukraine and Gaza, in part due to AI technologies \cite{noauthor_tech_nodate,tharoor_analysis_2024}, we must remind ourselves that the end goal of much of AI research is the surveillance of people and communities---and at times, their outright destruction---even if these are not the goals that compel us forward in our research.  