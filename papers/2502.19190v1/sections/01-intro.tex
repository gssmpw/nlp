As humanities researchers with long histories of engaging with computational technologies, who are currently witnessing the proliferation of AI (and AI “hype”) across nearly every aspect of society, we care deeply about its uses, impact, and harms. Whether we must learn to inhabit physical and social landscapes forever changed by AI, or whether we will soon be required to contend with the residue of a burst AI bubble, many important questions have come to the fore: What are the social, political, and historical conditions that have brought us to this point? What are the boundaries between social, political, economic, and technical systems? How are the ideas of ``culture'' and ``community'' being redefined by AI researchers or the models they develop? How can we ensure that communities in the world are helped and not harmed by these models? Is scholar-driven or community-led work even possible in a research environment dominated by corporations? And what of the environmental impact of these (and our) research efforts? Is any future that might be enhanced by new developments in AI doomed to fail because of outsized corporate power? Because of the extreme environmental footprint of data centers or the exploitation of data workers? Because of narrow or unreflective assumptions about what human intelligence truly entails?

Many AI researchers feel little obligation to truly grapple with questions of this level of complexity and scope, choosing instead to enact what philosopher Charles Mills has described as an ``epistemology of ignorance'' \cite{Mills_1999}. This is the worldview that results from an intentional avoidance of frameworks that challenge normative beliefs, and serves to reinforce dominant perspectives. In the context of AI research, we see this especially in papers making broad claims about language, culture, and values, as well as in systems designed to serve a universal user or ``fix'' a complex social issue. Even work that purports to engage with humanistic ways of thinking tends to demonstrate a limited engagement with work coming out of actual humanities disciplines---English and other languages, history and art history, American, African American, and ethnic studies, musicology and philosophy, women’s and gender studies, and more. As a result, the deep expertise that humanities researchers possess---our knowledge about the past, our ability to conduct detailed analyses within and between cultures, and our command of meaning-making practices past and present, among others---has not been leveraged in this pivotal field. 

This may be the result of disciplinary differences in our research methods and outputs. In the humanities, we build up our systems-level analyses from specific objects of culture. A poem or an artwork, a community on Twitter, an interview with a local elder, a historical newspaper or court transcript, or even a large language model, might function as the focal point of an analysis that leads to an interpretive model. We call these models ``theories,'' but not in the sense that technical disciplines use this term. Our theories, grounded in specificity, help us better understand, navigate, and posit potential answers to questions that are otherwise too big, too complex, or too intractable to be able to resolve with any precision. 

In this paper, we enlist this expertise in the service of the big questions about AI posed just above. Our contribution is a set of eight provocations about AI---and about generative AI models in particular---as they are currently developed, employed, and discussed in technical fields. We draw from the framework employed by danah boyd and Kate Crawford \citeyear{boyd_Crawford_2012} in ``Critical Questions for Big Data,'' in which they present a similar set of provocations, drawing from their own expertise in the humanities and social sciences, for what was then the new landscape of Big Data. Inspired by this work and its transformative impact, we offer a new set of provocations that encompass the broader set of questions engaged by generative AI. Our provocations are as follows: 1) Language makes meaning in multiple ways; 2) Generative AI requires an expanded definition of culture; 3) Generative AI can never be representative; 4) Bigger models are not always better models; 5) Not all training data is equivalent; 6) Openness is not an easy fix; 7) Limited access to compute enables corporate capture; and 8) AI universalism creates narrow human subjects. Each provocation consists of an assertion that is elaborated through current humanities research, and is intended to encourage technical AI researchers to more fully consider how their work could be enriched by incorporating humanistic expertise. 

Before moving forward, it is important to clarify that the end result of humanities research---including the work presented here---is very rarely a simple conclusion. Rather, the goal is a deeper understanding of the ``big question'' that frames the work. This is what classicist Gregory Crane, in the context of a discussion of the relationship between AI and the humanities, describes as an augmentation of our own intelligence \citeyear{Crane_2019}. Put more directly,  what humanities researchers can contribute to the current conversation about AI, its present uses, its incontrovertible harms, and its future possibilities, is additional clarity about the stakes of engaging AI in an age of techno-capital. This contribution enables our work with (or against) AI to progress because it enables us to conduct more informed, more accurate, and more humane research (in the sense of ``people-centered''). In this spirit, we position our provocations as bridges and not walls. It is our collective belief that we need a more complex understanding of AI and we also need a more complex understanding of human culture. 
