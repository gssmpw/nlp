A recognition of the cultural complexity of historical sources, including training data, also offers a fundamentally different way to approach the issue of bias. As is widely recognized, gaps in training data, coupled with the range of biases embedded in existing training datasets, have contributed to a range of harms perpetuated by AI systems \citep{Buolamwini_Gebru_2018, Noble_2018, Eubanks_2018}. In response, researchers and government agencies alike often call for strategies of bias mitigation, ranging from attempts to ``de-bias'' datasets and AI systems \citep{Bolukbasi_Chang_Zou_Saligrama_Kalai_2016}, to improved documentation of both datasets \cite{ Gebru_Morgenstern_Vecchione_Vaughan_Wallach_III_Crawford_2021} and models \cite{Mitchell_Wu_Zaldivar_Barnes_Vasserman_Hutchinson_Spitzer_Raji_Gebru_2019}, to research into model interpretability \cite{Bhatt_Ravikumar_Moura_2019} and explainability \cite{Danilevsky_Qian_Aharonov_Katsis_Kawas_Sen_2020}. Each of these interventions are necessary, but the issue of bias reflects a deeper structural problem, one that will never be fixed unless the power differentials that cause structural inequalities are challenged at their source. 

Here is where additional concepts and strategies from the humanities enter in. Among them, we might draw from theories and practices that emerge from feminist, Black feminist, and archival theory, which offer examples of how to engage with historical sources that contain biases–what humanities researchers would call silences \cite{trouillot_silencing_2015} or absences \cite{Hartman_2008}---that can never be ``de-biased'' or modeled away \cite{bode_why_2020}. Reframing bias not as "bad data" but as an effect of unequal structural power redirects our attention to the social, historical, and political conditions that gave rise to the biased data, as well as to the spaces of indeterminacy, and to the irrecoverably missing parts, of any dataset past or present \cite{Sherman_Morrison_Klein_Rosner_2024}. In an algorithmic context, this raises new research questions: what new methods might be required in order to mark missing data, or intentionally amplify the significance of sparse data, rather than merely pass over these silences or gaps? This view also raises questions of personal responsibility: are we informed enough about, or connected enough to the people or cultures that the data represents in order to make these decisions? Have we read until we understand? \cite{Griffin_2021}. 

Humanities researchers devote significant amounts of time and energy to supplementing our own knowledge, expanding our shared archives, and contributing to a collective understanding, but we do not deceive ourselves into thinking that there is ever an end-point to this process. We know we will never have all of the ``data,'' so to speak, so as to fully understand the past. How do we grapple with this lack of complete representativeness? What are some lessons we might carry over to our work with AI? We might seek to learn from scholars of the archive of slavery who have spent decades developing strategies for making meaning from damaged archival records. We might look to the technique of critical fabulation as conceived by Saidiya Hartman, for example, which seeks to amplify the significance of sparse and at times violent archival records with narrative detail \cite{Hartman_2020}; we might model our work after the mode of historically-informed speculation as illustrated by Marisa Fuentes, who posits questions about what might have happened on the basis of historical fact \cite{fuentes_2016}; or we might look to Jessica Marie Johnson’s theorization of the ``null value,''---the same concept as in the relational database---which she employs to hold space for the people whose stories that cannot be recovered at all \cite{Johnson_2020}. We can see some of these approaches already being applied to the output of biased models in the present, as in Curry Hackett’s use of text-to-image models to generate images of city streetscapes inspired by examples of twentieth-century African American art \cite{noauthor_architect_2023}. But we are eager to see them applied to the construction and framing of models themselves. 

There is an additional, broader set of lessons to be learned about the impossibility of complete knowledge and the need for technical researchers to come to terms with that fact: any act of dataset creation is a political act. This is true because our datasets represent languages, peoples, and cultures, which are in themselves embedded in larger structures of power. Here we can look to the work of statistician Taylor Arnold and visual culture scholar Lauren Tilton and their approach of distant viewing \cite{Arnold_2023}. Using AI to analyze photographs from the US National Archives, they show how the very process of assigning an annotation to an image entails a set of political as well as social and cultural decisions. Put another way, there is always a perspective that is shaped by training data, annotations, parameters, and prompts. Rather than chasing a ``representative'' dataset---which, of course, can never be achieved---we would be better served by acknowledging that there are always perspectives encoded in our models, and by taking the time to document and name them. 