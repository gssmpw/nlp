\section{Related Work}
\label{sec:relatedwork}
In this section we discuss the theoretical concepts and literature that are relevant to a recommender system which interacts with reinforcement learners that play a repeated congestion game with a large Price of Anarchy.

\subsection{Modeling Congestion}

Traffic is a ubiquitous complex system driven by the interactions of vehicles on road networks. Congestion is the particular state of traffic where the road network is `saturated' in that it reaches its maximum capacity, the limit of vehicles that can simultaneously use the roads. When traffic is not congested, the individual choices of vehicles are less influential to traffic flows. Conversely, when traffic is congested the influence of individual vehicle choices is heightened. 

It is well understood that the complex physical interactions between vehicles can create worsen congestion \cite{bando1995dynamical, helbing2002micro}. For example, traffic phenomena like phantom jams and green waves could be explained by these microscopic interactions between vehicles \cite{helbing2001traffic}. Vehicles are modeled as pursuing a target speed and slowing down when they approach a neighboring vehicle, but with a reaction delay. 

In this paper, we focus on the effects of route choice on congestion. Which paths do vehicles pick in a traffic network \cite{wardrop1952road}? We start by assuming that each vehicle prefers a shorter travel time. Then we conclude that each vehicle will select the shortest path available to it. In so doing we enter the dynamic realm of game theory where individual path choices may influence all travel times; reasonably, if all vehicles pick the same shortest path it may stop being the fastest. This gives rise to the Traffic Assignment Problem which requires picking a path between origins and destinations for the vehicles in a road network. Wardrop's seminal work on the subject \cite{wardrop1952road} formalized a traffic assignment reached by selfish vehicles as the User Equilibrium, and one coordinated by a central planner as the Wardrop Equilibrium.

% While the dynamics of traffic congestion driven by route choices are known to depend largely on the road network topology. Despite this, most roads in cities were not designed with widespread GPS routing recommendations and their effects in mind. It is known that congestion may worsen, when all users selfishly choose the same shortest paths.

\subsection{Congestion Games}

Routing, route choice, and congestion are a prominent part of game theory \cite{rosenthal1973class}. These `congestion games' assume that $N$ players share a set $A$ of roads where selecting a resource $a$ yields a utility $u_a(f(a))$, which depends on the number of players $f(a)$ that selected road $a$ and $u_a$ is a non-increasing function (more players on the same road lead to less utility per player). When we assume that players are rational utility maximizers (travel time minimizers), game theory defines solution concepts like Nash equilibria to characterize the road network (equivalent to the Nash Equilibrium (NE) for non-atomic players \cite{wie1998relationship}). Congestion games are a special case of convex potential games \cite{monderer1996potential} --- games that admit a convex potential function whose maximization leads to the NE --- and as such have a unique pure strategy NE. The unique pure NE tells us that if each player picks the NE path, no individual player is incentivized to pick a different path. 

As formulated, the NE may not be the best description of the state of a real traffic system, as it may be reasonable to assume that the vehicles are unaware of the NE strategy, or that they have partial information about what other vehicles are doing. Perhaps it is reasonable to assume that each vehicle receives some additional information on which they can decide what to do, such as a route recommendation. This situation has a more suitable economic solution concept, known as a correlated equilibrium. A correlated equilibrium is defined as an NE with additional information given to all players, just like recommendations. In a correlated equilibrium, no player is incentivized to change their action given their recommendation and given that they know what all other players are recommended. Strikingly, for non-atomic congestion games, it is proven that correlated equilibria correspond exactly with the Nash Equilibria \cite{koessler2024correlated}. If this result were a general fact of route choice in real-world traffic systems it would entail that: route recommendations can not change the route choice of vehicles in a road network. Such a prediction is hardly justified. Route recommendations from modern platforms like Google Maps and Waze clearly influence vehicles in traffic. The kind of player that is unaffected by a route recommendation is one who ignores it. 

A key feature of this work is the introduction of recommendations as an input to route choices.
% In this paper, this will be a central feature of the route choices in our model: they will be affected by recommendations. 
Furthermore, agents will repeatedly play a congestion game and learn to do so over time. In repeated congestion games, selfish behavior can sustain outcomes that are better than NE in terms of social welfare (Pareto-optimal) \cite{scarsini2012repeated}. At the same time, Best-Response dynamics are known to converge in congestion games \cite{candogan2013near}. It is relevant, therefore, to look beyond Best-Response dynamics if we wish to study behavior away from the equilibrium. 
% Correlated equilibria are a well-known generalization of NE \cite{aumann1987correlated}, which contain equilibria that are possibly better than NE. A correlated equilibrium is achieved with a correlation device, meaning that all players are given additional information, which can change their best responses. A correlation device can be understood as a RS. For rational players and Best-Response dynamics in convex potential games, there are expected to be little possible improvements with correlation devices \cite{roughgarden2015intrinsic, koessler2023correlated}.


\subsection{Braess's Paradox}

In practice, vehicles do not use roads in the most efficient way. In fact, most concern stems from the realization that self-interested choices can worsen congestion. In economic terms there are road networks which have a Pareto inefficient NE. The Pareto inefficiency entails that there exists an assignment of traffic which improves the travel time for all vehicles without worsening the travel time for any vehicle. This inefficiency is exemplified in the Braess Paradox, whereby the capacity of a road network can be increased while simultaneously worsening the NE social welfare, and subsequently creating a network where self-interested vehicles make congestion worse than when there was less road capacity. This counter-intuitive phenomenon is also found to have real-life relevance for the routing of packets on the Internet~\cite{korilis1999avoiding, tumer2000collective}, the flow of energy on power grids \cite{schafer2022understanding}, and the path choice of drivers in urban road networks \cite{argota4291171less}. In this paper we will be using the Braess Network for our investigations as captures in \autoref{fig:braess_network}.

\begin{figure*}[!ht]
\centering
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/recommender/braessNetworkInitial.pdf}
  \caption{Initial Network}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/recommender/braessNetwork.pdf}
  \caption{Augmented Network}
  \label{fig:sub2}
\end{subfigure}
\caption{Illustration of the initial network (a), and the augmented network (b) in the Braess Paradox. Agents start in the ``S'' state and pick a path to reach state ``t''. The numbers represent the cost of traveling over a link. A cost of $x$ is the ratio of agents that choose that link. Two actions are possible in (a), $\mathit{up}$ takes the upper edges, and $\mathit{down}$ takes the lower edges. In (b) an additional action $\mathit{cross}$ is possible, which takes the first upper edge, crosses to the lower section at the middle, and finishes on the second lower edge. Rational and fully-informed agents all pick the crossing link in the augmented network (Nash equilibrium), which leads to high congestion and the worst possible social welfare.}
\label{fig:braess_network}
\end{figure*}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/braess_with_eq.pdf}
%     \caption{Illustration of the Braess Paradox network. Agents start in the ``s'' state and pick a path to reach state ``t''. The numbers represent the cost of traveling over a link. A cost of $x$ is the ratio of agents that choose that link. Rational and fully-informed agents all pick the crossing link in the augmented network (NE), which leads to high congestion and the worst possible social welfare.}
%     \label{fig:braess_network}
% \end{figure}

The Braess Paradox has a structure similar to other popular games in game theory, like the Prisoner's Dilemma, the Public Goods game and the Bertrand Duopoly. These games share the property that the Nash Equilibrium is inefficient, meaning that there are better collusive that the players could establish if they could convince each other not to be selfish, or alternatively if they were forced not to be.


\subsection{Price of Anarchy}

This aforementioned property of an inefficient Nash Equilibrium which many games share is also called the Price of Anarchy \cite{koutsoupias1999worst}, and was first explored for congestion games. First, a suitable system wide metric is defined, like social welfare, which is taken to be the average (or sum) of all the utilities of the players. Then, the Price of Anarchy is defined as the ratio of the social welfare at the worst Nash Equilibrium (NE) and the socially optimal solution. Intuitively, it is referred to as the cost of having self-interested decision makers rather than coordinated social control. If systems like traffic road networks did not have a Price of Anarchy, it may the theoretically justifiable not to intervene at all. However, given the widespread identification of theoretical networks with a Price of Anarchy larger than 1, there is a theoretical `room for improvement'. The question is how much. For linear congestion games the Price of Anarchy is upper bounded to $\frac{4}{3}$, where the augmented network of the Braess Paradox is the network which saturates this upper bound. In other words, in the worst case road network self-interested route choice is $33\%$ worse than the social optimum.

Now, depending on the field of interest a worst case inefficiency of $33\%$ may have different interpretations. How bad is this, really? Within the congestion game literature, this inefficiency is interpreted with varying severity. The authors of these bounds viewed them optimistically as proof of a limit to the inefficiency of self-interested behavior, at least for congestion games \cite{roughgarden2002bad}. Furthermore, if the assumption of linear costs seems unreasonable (which it certainly will to traffic scientists which more commonly use a BPR cost-function \cite{us1964traffic}) the same paper established an upper bound of $2$ for the Price of Anarchy when the costs functions were only restricted to be increasing in the number of players, once again in the very worst case.

If this is `as bad as it gets', then what features of vehicle behavior may reduce the Price of Anarchy? After all, the assumptions of rational behavior required to derive these worst-case bounds are easily criticized for their stringency. Recent work has challenged the Price of Anarchy in the Braess Paradox by deploying reasonable reinforcement learning algorithms in the road network and demonstrating that even their learning dynamics --- though self-interested --- lead to a significantly better social welfare than rational agents \cite{carissimo2024counter}. This paper constitutes an extension of that work, the discussion of which we defer to the next sections.

The upper bounds on the Price of Anarchy remain to be interpreted. In this paper, as we seek to mitigate the Price of Anarchy with a coordination algorithm, we will frequently refer to the mirrored perspective which can be though of as a `Cost of Control'. How costly --- to solve, to implement, in its externalities --- is the coordination solution required to mitigate the inefficiencies of self-interested behavior? It is often the case that, while possibly imperfect, coordination solutions via self-organization mechanisms are most efficient.


\subsection{Road Pricing}

A common approach to improving the efficiency of traffic and reducing congestion is to charge drivers for their road choices \cite{morrison1986survey}. By making roads more expensive it is believed that drivers choices can be shifted to the more efficient roads. Marginal cost pricing is an economically effective way of achieving these results. They rest on assumptions that drivers will be impacted by these costs. Algorithmic drivers which act in approximately rational ways may well conform to these models. Comprehensive reviews of road pricing literature find the benefits to exceed the costs \cite{anas2011reducing}. An oft raised issue with road pricing is the potential un-fairness of their results when applied to systems with unequal initial distributions of capital, where few agents may be less affected by an increase in price that others \cite{levinson2010equity}. Furthermore, the evaluation of road pricing is conducted empirically but is primarily due to reductions in overall demand, rather than the re-allocation of resource use to more efficient paths. In fact, the success of road pricing is greatly influenced by the availability of public transport options \cite{anas2011reducing}. As such, the success of road pricing is less due to a reduction of the Price of Anarchy, as it is to the reduction of demand. It is less clear and a challenge to evaluate whether road pricing is an effective method to reduce the Price of Anarchy.


% \subsection{Cybernetic Societies}

Is there a way to manage road networks in congested states without the detour of money? Road pricing schemes feature money as the social coordination tool that can re-distribute vehicles away from inefficient path choices. As such, road pricing acts on the 
incentives of agents. Would it be enough to provide agents with additional information? 
% This information should be calibrated just right to produce a desired action. From this approach we can ask whether there exists an individualized distribution of information such that the vehicles pick different paths. Cybernetics recognizes complex dynamics and feedback loops present in systems that govern their behavior, and asks whether one can intervene to steer a system. If vehicles react to perceptual inputs, which inputs lead to the best social welfare outcomes? 

% From such a perspective, each vehicle is represented as a function, a mapping from perceptions to actions. If this map is known it is possible to determine \textit{a priori} which perceptions lead to a desired path choice. If this function is not known, it may be learned by interaction with the vehicle. Over time, an observer may notice all the ways in which a vehicle responds to inputs, and learn the vehicles mapping of inputs to paths. Then, if the observer can influence the perceptual inputs of the vehicle it can influence the paths that the vehicle will choose. If the observer can do so for all vehicles it can influence the social welfare of the traffic system. 

In this paper we investigate such an approach and method when applied to a system of learning algorithms. To this end, we will use reinforcement learning as the constrained choice model employed by the vehicles.


\subsection{Reinforcement Learning Agents}
\label{sec:stateless}
A reinforcement learning agent is an agent which learns a reward function while interacting with an environment. The environment is characterized as a state space $\mathcal{S}$ and an action space $\mathcal{A}$ which represent everything that can happen. The reward function is feedback that a learner gets for performing an action in a given state and ending up in a new state. Then a reinforcement learner, in particular a model-free reinforcement learner, can try to maximize the rewards it receives from the environment using a behavioral policy which explores its available actions and exploits those actions which provide the highest rewards \cite{sutton2018reinforcement}.

% Reinforcement learning has led to breakthrough performances like algorithms which beat humans at the games of Go and Starcraft. 
In this paper we will use $Q$-learning, a reinforcement learning algorithm useful for its simplicity and broad application potential. $Q$-learning is known to converge to `optimal behavior' in finite Markovian environments (Markov Decision Processes) when all states are visited infinitely many times \cite{watkins1992q}. Optimal behavior in this case means that the agent learns how to maximize the rewards it accumulates by interacting with the environment. In multi-agent settings, however, there are no convergence guarantees for $Q$-learning.

As we will be interested to use $Q$-learning in a multi-agent game, the lack of convergence guarantees may at first seem to be a hindrance. However, this is precisely the opposite, because agents which converge may have the same behavior regardless of the inputs they receive, just like rational self-interested agents which converge to Nash Equilibria in non-atomic congestion games no matter what additional information they are given. We will explore this feature further in this paper, but for now it suffices to claim that convergence is not the only interesting behavior that arises from the interactions of agents. Here, as we wish to study Machine Behavior, we interest ourselves in those dynamics which lead us away from equilibrium, as was previously explored in the Braess Paradox in \cite{carissimo2024counter}.

Our methodological perspective has some similarities with other literature on steering in Reinforcement Learning. Steering refers to influencing agents during their learning to guide their convergence to desired policies. For example, single agent steering with multiple criteria \cite{mannor2001steering}, steering Markovian agents \cite{huang2024learning}, and  steering no-regret learners \cite{zhang2023steering}. Steering typically involves adjusting the reward function during learning. Our approach differs in that we adjust the state information that reinforcement learners observe during learning.

We note that no-regret learning is a popular model for dynamic agents which interact with an environment with appealing properties \cite{hart2000simple}. A no-regret learner is defined as an agent which, in the limit of infinite time, will converge to playing the actions which are optimal. This agent may explore during the course of its learning, in the beginning for example, but is assumed to eventually play the optimal actions. We do not assume no-regret learners, or analyze our $Q$-learners against regret. This is because in our game setting, a no-regret learner converges to the Nash Equilibrium behavior. Furthermore, we hypothesize that a no-regret can not be steered with recommendations, even though they can be steered with rewards \cite{zhang2023steering}. Therefore, to study steering with recommendations we look beyond no-regret learning and settle on $Q$-learning.

% \subsection{Congestion Games with Learning}\label{sec:stateless}
This paper analyses a repeated congestion game with $Q$-learning agents \footnote{The $Q$-learning agents can also be seen as weakly rational agents, as agents with bounded rationality}. The congestion game can be framed as a Markov Decision Process (MDP), with the actions corresponding to the set of resources $\mathcal{A}$ and the reward function of the MDP being equal to the utilities experienced by the agents $R(\va) = \left(u_{a_1}\left(f(a_1)\right), \dots, u_{a_n}\left(f(a_n)\right)\right)$. The $Q$-learners are assumed to have an $\epsilon$-greedy policy, $\argmax$ with probability $1-\epsilon$ and uniform random with probability $\epsilon$, and update their $q$-values with the Bellman update rule. The $Q$-learners have their own state--action value function $Q_{i}: \mathcal{S} \times \mathcal{A} \rightarrow \R$, a policy function $\pi_i: \mathcal{S} \rightarrow \mathcal{A}$, and an update rule $U_i: \mathcal{S} \times \mathcal{A} \times \R \times \mathcal{S} \rightarrow \R$ parametrized with learning rate $\alpha$ and discount factor $\gamma$. The environment runs for $\tau$ steps, and at each step $t$ $Q$-learners apply a policy $\pi_i$ to determine their next action $a_{i,t}$, observe $(s_{i,t}, a_{i,t}, r_{i,t}, s'_{i,t})$ and then update $Q_{i,t}$ using their update rule $U_i$ to obtain $Q_{i,t+1}$.

Independent reinforcement learners that apply incremental updates to their policies are drawn towards the NE, but the NE may not be a stable equilibrium \cite{kleinberg2011beyond, bielawski2021follow, bielawski2022route}. Additionally, the NE may not be the optimum of the social welfare function defined as $W=\frac{1}{n}\sum_{i}^n r_i$. It has already been shown, for some games (pricing game \cite{calvano2020artificial, klein2021autonomous}, prisoner's dilemma \cite{schaefer2022emergence, dolgopolov2022reinforcement}), that $Q$-learners are able to perform better social outcomes than the NE through implicit coordination. These same effects have been shown in the Braess Paradox \cite{carissimo2024counter}, which we replicate in \autoref{fig:braess_network_learning}. It was demonstrated that $Q$-learning in the Braess Paradox could be chaotic and oscillatory while leading to better social welfare \cite{carissimo2024counter}. In this paper we seek to amplify these effects by steering with strategic state recommendations.

\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{figures/small_braess_100_qlearn_epsilonDecayed.pdf} \\
\caption{Learning of 100 $\epsilon$-greedy tabular $Q$-learners \mbox{($\alpha=0.1$, $\gamma=0.8$)} in the Braess Paradox converges to social welfare values much higher than the NE (0). Values of social welfare were rescaled from \mbox{$[-2,-1.5]\rightarrow[0,1]$}, higher social welfare is better. Results replicated from \cite{carissimo2024counter}.}
\label{fig:braess_network_learning}
\end{figure}

Braess's Paradox has a Nash Equilibrium where all agents cross, which leads to an average latency of $2$. However, the Social Welfare optimizing solution is for half of the agents to go up and the other half to go down, for an average latency of $1.5$. The latencies experienced by the agents are linear in the fraction of agents that choose the actions, $\frac{n_{u}}{N}, \frac{n_{d}}{N}$. Specifically, the latencies, $l(a)$, are: $l(u) = 1 + \frac{n_{u}+n_{c}}{N}, l(d) = 1 + \frac{n_{d}+n_{c}}{N}, l(c) = \frac{n_{u}+n_{c}}{N} + \frac{n_{d}+n_{c}}{N}$. The rewards for agents are then the negative of the latency ($r_i=-l(a_i)$). \autoref{fig:braess} in the appendix gives additional information about the learning dynamics in the augmented network of the Braess paradox, which are not the focus of this paper.


\subsection{Recommender Systems}

Route recommender systems are a prominent example of recommender systems (RS) that promise to reduce average travel times for drivers. In the simplest case, shortest path algorithms are applied directly to networking and traffic \cite{abolhasan2004review, zeng2009finding}, and extended to user platforms such as Google Maps and Waze \cite{luxen2011real, wang2014r3, dai2015personalized}. This approach, however, runs into trouble when all drivers receive the same shortest path recommendation and follow it: then, the recommended path will often get congested and slow drivers down. If drivers realize this, they might `learn' not to follow the recommendations \cite{helbing2002volatile}. 

Therefore, good route recommender systems (RRSs) must account for the collective effects they trigger when recommending routes. While it is widely accepted that RRSs influence the systems they interact with and that users are not static entities with fixed preferences \cite{Nguyen2014, Stocker2020, Stray2020, stray2021you, stray2021designing, Hazrati2022}, it is still not entirely clear how users should be modeled dynamically \cite{jiang2019degenerate, chen2019generative, kalimeris2021preference}. Furthermore, it is recognized that the rules of digital platforms create incentives for agents \cite{benporat2018gametheoretic, hron2023modeling}. It is an active area of research to model the effects that RRSs have on the social welfare of the systems they interact with \cite{helbing2004dynamic}. For example, by establishing metrics that can predict actually resulting performance \cite{krauth2020offline}, creating game-theoretic models where users are utility maximizers rather than static entities with fixed preferences \cite{chen2019generative}, allowing user preferences to be shaped by recommendations \cite{kalimeris2021preference}, and understanding the emergence of `echo chambers' and `filter bubbles' \cite{jiang2019degenerate}. 

Route recommendation methods are increasingly common \cite{su2014crowdplanner, cui2018personalized, su2009survey}. They have the advantage of being able to rely on traffic flow models and congestion games to estimate their validity. With game-theoretic solution concepts, researchers have shown that ubiquitous shortest path planning may cause or worsen traffic congestion \cite{thai2016negative, cabannes2017impact, macfarlane2019apps}, and possibly affect cities' economies \cite{sweet2011does}. 

Finally, the model of recommendation which we explain in the next section is novel but has some similarity to previous work on machine teaching \cite{zhu2015machine, zhu2018overview}. The goal of machine teaching is to select the optimal dataset that will allow a learner to learn the optimum. Recent work has extended machine teaching to reinforcement learning \cite{lewandowski2022reinforcement}. Our work is similar to machine teaching because we assume knowledge of a target distribution (like an optimum) to which we would like the system of $Q$-learning agents to converge on. It is also similar to machine teaching because the recommender in our model is capable of influencing the experiences of agents (picking data). Our model crucially differs from machine teaching because it involves picking the states for $Q$-learners rather than feeding them data (a state, action, reward, next state tuple for a reinforcement learner). Furthermore, our model has many $Q$-learners interacting in a congestion game such that what is optimal for an individual learner may not be socially optimal, leading to pluralistic notions of optimality.