\section{Evaluation}

We evaluate \method \ on text-to-texture generation both quantitatively and qualitatively. In the following sections, we explain the experimental setup for evaluating our approach, including  the evaluation dataset and metrics (Section 
\ref{subsec:experiment_setup}). We then compare \method\,  against competing 
text-to-texture generation methods (Section \ref{subsec:comparisons}). We also analyze the impact 
of pixel neighborhood sizes, geodesic distances and number of input views on performance in an ablation study (Section \ref{subsec:ablation}).


\subsection{Experimental setup}
\label{subsec:experiment_setup}


\paragraph{Test dataset}  
We evaluate \method \ on the test split provided by
Text2Tex~\cite{Chen:2023:Text2tex}. The split includes
$410$ textured meshes from Objaverse~\cite{Deitke:2023:Objaverse} across $225$ categories. All competing methods are trained on the same training split, as discussed in Section \ref{sec:training}, and evaluated on the same above test Objaverse split. We also note that all competing methods use the same UV maps and surface parametrization.

\paragraph{Metrics}
For quantitative evaluation, we use standard image quality metrics for generative image models. Specifically, we report the Fréchet Inception Distance 
(\textbf{FID})~\cite{Heusel:2017:GANs} and Kernel Inception Distance 
(\textbf{KID})~\cite{Bińkowski:2018d:Demystifying}. 
The FID compares the mean and standard deviation of the deepest layer features in the Inception v3 network between the set of real and generated images. The KID calculates the maximum mean discrepancy between the real and generated images. In practice, the MMD is calculated over a number of subsets to obtain a mean and standard deviation measurement.
Additionally, we measure 
alignment, or similarity, of the generated images with the input text prompt using the \textbf{CLIP score}~\cite{Radford:2021:CLIP}. To compute these metrics, following \cite{Zeng:2024:Paint3D}, we 
render each mesh with the generated textures from $20$ fixed viewpoints at a resolution of 
$512\times512$. The reference distribution consists of renders of the same meshes using the textures found 
in the Objaverse dataset, under identical lighting settings.


\subsection{Comparisons}
\label{subsec:comparisons}

Our main finding -- replacing traditional backprojection with our neural module -- is numerically examined in Table~\ref{tab:ablation_backbones}. Our neural module improves both the original Paint3D backbone as well as our implemented MatAtlas backbone. The improvements are consistent across all three evaluation metrics (FID, KID, and CLIP score). Our neural backprojection improves the FID distance by $6.1\%$ for the Paint3D backbone, and $6.9\%$ for the MatAtlas backbone. The improvements are more prominent in terms of the KID score ($19.1\%$ relative reduction for the Paint3D backbone, and $29.2\%$ relative reduction for the MatAtlas backbone). The KID score is more sensitive to fine-grained texture variations due to the use of Maximum Mean Discrepancy (MMD) with a polynomial kernel when comparing distributions. As a result, when a model improvement primarily reduces local inconsistencies -- such as texture artifacts and fine details, KID tends to exhibit a more substantial improvement than FID. In terms of CLIP score, all methods seem to generate images that are similarly aligned with the text prompt, yet our module still maintains a small edge over traditional backprojection. 


Figure \ref{fig:paint3d_comparison} and \ref{fig:matatlas_comparison} 
provide comparisons of our module against the Paint3D and MatAtlas respectively. Overall, we observe that our texture results have less artifacts and seams, while preserving a similar level of texture detail. We also refer readers to the supplementary material for more results. 

In Table \ref{tab:comparisons_t2t}, we include quantitative comparisons of the best variant of our method (based on the MatAtlas backbone) with other state-of-the-art models for text-to-texture generation. Here we also include a comparison with the recent method of TEXGen\cite{Yu:2024:TEXGen}.
According to all the evaluation metrics, our method provides the best performance in terms of FID \& KID distances as well as CLIP score.  

In Figure \ref{fig:texgencomparison}, we show qualiitative comparisons with 
TEXGen's released implementation \cite{Yu:2024:TEXGen}. We observe that TEXGen often leads to global texture inconsistencies on the output shapes, while our method is  more view-consistent.



\begin{table}[!t]
    \centering
    \begin{tabular}{c|cccccccc}
        \toprule
         \textbf{Model} & \textbf{FID} $\downarrow$ & \textbf{KID} $\downarrow$ & \textbf{CLIP Score} 
         $\uparrow$  \\ 
         \midrule
        Paint3D & $29.13$ & $2.62$ $\pm$ $0.3$ & $29.45$ \\ 
        Im2SurfTex$_{paint3d}$ & \textbf{$27.34$} & \textbf{$2.12$} $\pm$ \textbf{$0.2$} & \textbf{$29.63$} \\
         \midrule
        MatAtlas & $28.68$ & $2.16$ $\pm$ $0.2$ & $29.65$ \\
        Im2SurfTex$_{matatlas}$ & \textbf{26.68} & \textbf{1.53} $\pm$ \textbf{0.2} & \textbf{29.76} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation using different backbones for viewpoint selection and image generation. Note that the KID metric includes a mean and standard deviation measurement.}
    \label{tab:ablation_backbones}
    \vspace*{-1mm}
\end{table}

\begin{table}[!t]
    \centering
    \begin{tabular}{c|cccccccc}
        \toprule
         \textbf{Model} & \textbf{FID} $\downarrow$ & \textbf{KID} $\downarrow$ & \textbf{CLIP Score} 
         $\uparrow$ \\ 
         \midrule
        Text2Tex & $34.89$ & $4.82$ $\pm$ $0.3$ & $29.65$  \\ 
        Paint3D & $29.13$ & $2.62$ $\pm$ $0.3$ & $29.45$ &  \\ 
        MatAtlas & $28.68$ & $2.16$ $\pm$ $0.2$ & $29.65$ \\
        TEXGen & $27.41$ & $2.42$ $\pm$ $0.2$ & $29.23$ \\
        Im2SurfTex & \textbf{26.68} & \textbf{1.53} $\pm$ \textbf{0.2} & \textbf{29.76} \\
        \bottomrule
    \end{tabular}
    \caption{Comparisons with other text-to-texture methods.}
    \label{tab:comparisons_t2t}
    \vspace*{-1mm}
\end{table}

\begin{figure}[!t]
    \includegraphics[width=0.48\textwidth]{figures/evaluation/figure_texgen.pdf}
    \caption{Comparison between \method \ and TEXGen \cite{Yu:2024:TEXGen}. We show  two different views of the textured objects. Our method produces more coherent and view-consistent textures.}
    \label{fig:texgencomparison}
    \vspace*{-4mm}
\end{figure}


\subsection{Ablation}
\label{subsec:ablation}

We provide an ablation study where we vary the input pixel neighborhood size extracted from the generated images
for each texel. Results are shown in  Table~\ref{tab:ablation-neighborhood} for neighborhoods $1 \times 1$ , $3 \times 3$ , $5 \times 5$, and $7 \times 7$. Best performance is achieved under the $3 \times 3$ neighborhood setting. 

Table \ref{tab:ablation-geodesics} provides another ablation where we compare using absolute versus relative coordinates in the positional encodings of the Eq. \ref{eq:positional_encodings}, and also examine whether using geodesic distance as additional feature in the positional encodings helps. Relative coordinates enhance performance compared to absolute coordinates, as they provide a more effective encoding for processing the local interactions between neighboring points, regardless of their actual 3D locations. With repect to the use of geodesic distances, we observe rather minor improvements in terms of the numerical scores. We suspect that the small differences are due to the fact that the improvements happen only in small image regions for the shapes of our dataset,
where the surface changes rapidly (e.g., folds, handles, high curvature regions), as shown in Figure \ref{fig:surfconsistency}. These small regions seem to have a relatively small effect on the established image quality metrics. 
Figure \ref{fig:surfconsistency} demonstrates  that adding geodesic distances as features in our module leads to fewer texture artifacts and diminished color bleeding in these regions e.g., see the color bleeding between the bed mattress and wooden frame, or the green leaf and the apple when geodedic distances are not used. 

Figure \ref{fig:fig_reconstruction} demonstrates a visual comparison between reference textured meshes from our dataset, and reconstructed textures by our method, when we pass as input the rendered images from the original textures. This comparison aims to show whether our method causes any significant color shifting or bleeding while aggregating information from different views. We see that demonstrating our method does not introduce any such discrepancies during neural backprojection.
\begin{figure}[!t]
    \includegraphics[width=0.49\textwidth]{figures/evaluation/figure_reconstruction.pdf}
     \vspace*{-5mm}
    \caption{Our neural backprojection can closely reconstruct challenging textures of target objects in our dataset without  causing noticeable color shifting or discrepancies between target and decoded textures.}
    \label{fig:fig_reconstruction}
    \vspace*{-5mm}
\end{figure}

\begin{figure}[!t]
    \includegraphics[width=0.45\textwidth]{figures/evaluation/geod_vs_euclidean.pdf}
    \caption{Using geodesic distances in the positional encodings of texels promote texture consistency. On the left, Im2SurfTex operates without geodesic information, resulting in less coherent textures in areas with rapidly changing local geometry
    (e.g., surface regions with folds, handles, or high curvature). On the right, incorporating geodesic information improves texture quality in these regions.}
    \label{fig:surfconsistency}
    \vspace*{-4mm}
\end{figure}

\begin{table}[!t]
\centering
\begin{tabular}{c|cccc}
\toprule
Window size  & FID $\downarrow$ & KID $\downarrow$ & CLIPscore $\uparrow$ \\
\midrule
1 $\times$ 1 & $27.65$    & $2.31$ $\pm$ $0.2$ & $29.59$ \\
3 $\times$ 3 & \textbf{27.35}  & \textbf{2.15} $\pm$ \textbf{0.2}  & \textbf{29.61}\\
5 $\times$ 5 & $27.43$ & $2.26$ $\pm$ $0.2$ & $29.60$  \\
7 $\times$ 7 & $28.12$ & $2.36$ $\pm$ $0.3$ & $29.54$  \\
\bottomrule
\end{tabular}
\caption{Ablation study results wrt texel neighborhood size (no geodesic distances are used in this experiment)}
\label{tab:ablation-neighborhood}
\vspace*{-1mm}
\end{table}


\begin{table}[!t]
\centering
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{cc|ccc}
\toprule
Rel Coords. & Geod. Distances & FID $\downarrow$ & KID $\downarrow$ & CLIPscore $\uparrow$ \\
\midrule
- & -  & 27.86  & 2.32 $\pm$ 0.2  & 29.62\\
\checkmark & -  &  27.35  & 2.15 $\pm$ 0.2  & 29.61\\
\checkmark & \checkmark & \textbf{27.34} & \textbf{2.12} $\pm$ \textbf{0.3} & \textbf{29.63} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Ablation study results wrt using geodesic distances or not in the cross-attention operation of our backprojection module. Note that this experiment uses $3 \times 3$ pixel neighborhoods.}
\label{tab:ablation-geodesics}
\vspace*{-1mm}
\end{table}

\begin{table}[!t]
\centering
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{cc|ccc}
\toprule
 \# of Views &  Method &  FID $\downarrow$ &  KID $\downarrow$ &  CLIPscore $\uparrow$ \\
\midrule
\multirow{2}{*}{ 4} &  Paint3D &  29.41  &  2.71 $\pm$ 0.3  &  29.40\\
 &  Im2SurfTex &  \textbf{28.51}  &  \textbf{2.42 $\pm$ 0.3}  &  \textbf{29.62}\\
\hline
\multirow{2}{*}{ 6} &  Paint3D &  29.13 &  2.62 $\pm$ 0.2  &  29.45\\
&  Im2SurfTex &  \textbf{27.34}  &  \textbf{2.12 $\pm$ 0.2} &  \textbf{29.63}\\
\hline
\multirow{2}{*}{ 8} &  Paint3D &  29.20  &  2.75 $\pm$ 0.3  &  29.48\\
 &  Im2SurfTex &  \textbf{27.86} &  \textbf{2.32 $\pm$ 0.2}  &  \textbf{29.62}\\

\bottomrule
\end{tabular}
\end{adjustbox}
\caption{ Ablation study results wrt using different number of views.}
\label{tab:ablation-number-views}
\vspace*{-4mm}
\end{table}

Table \ref{tab:ablation-number-views} presents the impact of using different number of views on our evaluation metrics for both Paint3D and our method (using the Paint3D backbone). Increasing the number of views from $4$ to $6$ views results in improvements for the FID, KID, and CLIP cores. Yet, for the maximum number of views ($8$) in this experiments, we see that the FID and KID scores do not further improve. As shown in Figure \ref{fig:numverofviewsnumber}, we observe more artifacts appearing in Paint3D, probably due to its use of mere backprojection which often leads to more seams when more views are backprojected. Our method scores with $8$ views are affected less; our neural backprojection produces smoother results, yet we do notice a bit more oversmoothing in our case, which is a limitation our our method.

\begin{figure}[!t]
    \includegraphics[width=0.45\textwidth]{figures/evaluation/figure_view_number.pdf}
    \caption{Results for Paint3D and our method for $4$, $6$, and $8$ input views. \method\ generates smoother surfaces.}
    \label{fig:numverofviewsnumber}
    \vspace*{-1mm}
\end{figure}

