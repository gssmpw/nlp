\section{Related Work}

\paragraph{Early Works on Texture Synthesis}
Classic texture generation methods mostly focused on example-based approaches~\cite{Wei:2009:STARtexture}, 
including region-growing techniques~\cite{Efros:1999:texture, Wei:2000:Fast} and strategies leveraging local
coherence~\cite{Ashikhmin:2001:Synthesizing, Tong:2002:BTF} to maintain consistency across synthesized 
textures. Patch-based methods~\cite{Efros:2001:Quilting, Kwatra:2003:Graphcuttex} 
synthesized textures using patch patterns from reference images, while other
techniques~\cite{Kwatra:2005:TextureOptim, Han:2006:FastOptim} progressively refined 
synthesized texture based on optimization procedures. Another significant research direction involved directly generating textures on 3D 
surfaces~\cite{Turk:2001:Surfaces, Zhang:2006:vector, Fisher:2007:Design} by exploiting vector 
fields defined over the surface to seamlessly map textures onto complex geometries. All these example-based approaches were unable to capture texture variability, generate diverse textures, or handle diverse shapes.


\paragraph{Text-Guided Diffusion Models for Image Synthesis}
Our method builds upon diffusion models~\cite{Sohl:2015:Deep, Ho:2020:Denoising, Zhang:2023:T2I}, which have demonstrated superior performance compared to GANs~\cite{Goodfellow:2014:GAN, Zhu:2017:unpaired} in image generation tasks~\cite{Rombach:2022:High, Nichol:2021:glide, Saharia:2022:imagen, Ramesh:2022:dalle, 
Zhang:2023:ControlNet}. 
Closely related to our approach are text-guided generation models for 3D object synthesis, 
where text-to-image diffusion models are used for distilling 3D objects as neural radiance 
fields~\cite{Mildenhall:2020:nerf, Kerbl:2023:3Dgaussians} via Score Distillation 
Sampling~\cite{Poole:2022:Dreamfusion, Wang:2023:score}. Following DreamFusion~\cite{Poole:2022:Dreamfusion}, several approaches have been proposed~\cite{Lin:2023:Magic3D, 
Metzer:2023:latentnerf, Chen:2024:text2gs, Wang:2024:Prolificdreamer, Tsalicoglou:2024:textmesh, 
Shi:2024:mvdream}. However, these methods do not specifically target the task of texture generation.

\paragraph{Texture Generation via T2I Diffusion Models}
Initial efforts in texture generation via text-to-image diffusion models, such as 
Text2Tex~\cite{Chen:2023:Text2tex} and TEXTure~\cite{Richardson:2023:Texture}, employed depth-conditioned 
diffusion models~\cite{Rombach:2022:High, Zhang:2023:ControlNet} to iteratively inpaint and refine the 
textures of 3D objects. Both methods start with a preset viewpoint, generating texture updates for 
corresponding regions of the 3D object by back-projecting depth-guided views. In Text2Tex, a coarse texture 
is progressively created by iterating over multiple viewpoints and refining the texture map based on high-
surface-coverage viewpoints. This refinement applies a denoising diffusion process of moderate strength to 
preserve the textureâ€™s original appearance while enhancing details. Similarly, TEXTure divides the texture 
map into distinct regions labeled as \textit{keep}, \textit{refine}, or \textit{generate}, enabling 
selective refinement or generation of textures. Despite these efforts to achieve global consistency, these methods employ ad hoc thresholds to define the different shape regions and hand-engineered strategies for backprojection, often leading to seams between texture regions synthesized from different viewpoints.  


To address these issues, other methods such as TexFusion~\cite{Cao:2023:Texfusion} leverage 
\textit{latent} diffusion to interlace diffusion and back-projection steps, producing 3D-aware 
latent images that are subsequently decoded and merged into a texture map. Similarly, 
SyncMVD~\cite{Liu:2024:SyncMVD} employs a latent texture map where all views are encoded at each 
denoising step, further enhancing consistency in geometry and appearance. TexGen~\cite{Huo:2024:TexGen} introduced a multi-view sampling and resampling framework 
that updates a UV texture map iteratively during denoising, aiming to reduce view discrepancies.
Still, these methods rely on ad hoc blending masks or heuristics for aggregating texture information from different views, such as mere averaging or using the most front-facing view information for each texel.
 In contrast, our approach learns to aggregate color information from multiple views based on both geometry and texture information, promoting the generation of more coherent and seamless surface texture maps.


A more recent approach, Paint3D~\cite{Zeng:2024:Paint3D}, achieves impressive texture generation results, by adopting a two-stage texture generation strategy. 
In the first stage, a coarse texture is created by backprojecting views to texture space via the heuristic of
using the most front-facing view information for each texel, as in previous methods. The second stage involves a 
refinement and inpainting process that utilizes a diffusion model in UV texture space, conditioned on a UV position map 
encoding the 3D adjacency information of texels. While this method directly encodes 3D geometric 
information into the texture map, it can still result in misaligned textures due to its employed heuristic during its coarse stage. 
The subsequent texture refinement steps often fail to fix the artifacts of the coarse stage, as demonstrated in our experiments.
Another related method, MatAtlas~\cite{Ceylan:2024:Matatlas}, incorporates a three-step denoising process 
with sequential operations and line conditions to preserve geometry and style consistency. However, 
MatAtlas employs an averaging heuristic for blending the final texture from the generated views, leading to inconsistencies or overly smooth surfaces, as also demonstrated in our experiments. TEXGen~\cite{Yu:2024:TEXGen} takes a different approach by directly training a large-scale diffusion model in the UV texture 
space, and integrating convolution operations 
in UV space with 3D-aware attention layers in their denoising network to achieve high-resolution texture synthesis. However, their method still faces challenges in maintaining cross-view consistency since the 
generated textures are conditioned on single-view images which are merely backprojected to the UV space to derive the initial partial texture maps used in their diffusion.

In a concurrent work, MVPaint~\cite{Cheng:2024:MVPaint} introduces a multi-stage texture generation framework. In the 
initial stage, a latent texture map is employed during multi-view projection to create a synchronized texture 
across multiple views, similar to SyncMVD~\cite{Liu:2024:SyncMVD}. This is followed by an inpainting stage, where 
uncovered texture regions are filled using a dense colored point cloud extracted from the generated texture map. 
Colors are propagated to empty texels in a spatially aware manner using inverse distance weighting and normal 
similarity between neighboring points. Finally, a refinement stage upscales the texture map and smooths out seams 
through weighted color averaging among k-nearest neighbors in 3D space. Still, MVPaint relies on an averaging scheme to
aggregate view information into texture space. In contract, our approach learns this aggregation by encoding both 
geometric and appearance information from multiple views to produce textures with greater  consistency.

Expanding beyond single object texture generation, 
InstanceTex~\cite{Yang:2024:Instancetex} focuses on texture generation for 3D scenes, employing a local synchronized multi-view diffusion strategy to improve local texture consistency across multiple objects. 
3D Paintbrush~\cite{Cecatur:2024:Paintbrush} 
 specializes in localized stylization of single objects, using cascaded score distillation to refine textures within specific object regions. These approaches differ from our 
method in scope: InstanceTex is tailored for stylistic consistency in large environments, while 3D Paintbrush targets localized edits.