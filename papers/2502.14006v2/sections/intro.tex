\section{Introduction}
\label{sec:intro} 

\begin{figure*}[!t]
    \includegraphics[width=\textwidth]{figures/intro/gallery.png}
    \vspace*{-7mm}
    \caption{A gallery of 3D shapes from various categories, textured by \method.}
    \label{fig:gallery}
    \vspace*{-5mm}
\end{figure*}

Producing compelling 3D assets has become an increasingly active area of research in the field of generative AI. 
Despite the significant progress in training large-scale generative models of 3D geometry~\cite{Lin:2023:Magic3D, 
Liu:2023:Zero, Wang:2024:Prolificdreamer, Gao:2022:Get3d, Jun:2023:Shape, Νichol:2022:Pointe, Vahdat:2022:Lion, 
Liu:2024:One, Liu:2023:One2345plus, Shi:2023:Zero123plus, Zhang:2024:CLAY}, synthesizing compelling, seamless, and 
high-quality textures for 3D shapes and scenes still remains challenging. One major obstacle is the limited 
availability of training 3D asset datasets with high-quality textures. Several recent 
approaches~\cite{Chen:2023:Text2tex, Richardson:2023:Texture, 
Cao:2023:Texfusion, Ceylan:2024:Matatlas, Zeng:2024:Paint3D, Liu:2024:SyncMVD, Cheng:2024:MVPaint}  have resorted 
to leveraging powerful generative 2D image models, such as denoising diffusion models pre-trained on massive 2D 
datasets, to guide surface texture generation. These approaches generate images across different views conditioned 
on text prompts and depth maps rendered from the given 3D shapes or scenes, then attempt to combine the generated 
images onto the surface.

Unfortunately, these methods suffer from a number of limitations. First, merely projecting the generated images 
back to the surface tends to generate texture distortions in areas of rapid depth changes or high curvature
surface regions (Figure \ref{fig:teaser}, left). Second, the images generated from different views are often combined 
with ad-hoc criteria to create a single surface texture. For example, many methods~\cite{Chen:2023:Text2tex, Richardson:2023:Texture}
simply transfer the colors from the most front-facing view to each texel (i.e., pixel in the texture map), causing 
visible seams in the texture maps especially in areas where colors of neighboring texels are copied from different 
views (Figure \ref{fig:teaser}, middle \& right). Other methods rely on simple averaging schemes of RGB 
colors~\cite{Ceylan:2024:Matatlas} or image latents, causing significant color bleeding. 
Others resort to global texture optimization techniques
\cite{Cao:2023:Texfusion, Liu:2024:SyncMVD}, which are slow and can still fail to generate 
coherent textures since their initialization is still based on ad-hoc thresholds for view 
selection and color averaging. 

Our approach, named \method, addresses the above limitations with the introduction of a novel, optimization-free module 
that can be easily integrated to existing texture generation approaches. The module is trained to combine color 
information from multiple  viewpoints to textures through a cross-attention mechanism, where for each surface 
point, several candidate image neighborhoods across different views are examined and combined back in texture 
space depending on local surface geometry, 
as encoded in 3D positions, normals and geodesic distances within these patches. 
In this manner, the attention mechanism captures local context based 
on surface (geodesic) proximity rather than relying solely on 3D Euclidean proximity that might correlate surface region textures far from each other 
in geodesic sense. Our module yields coherent textures efficiently, without requiring any slow optimization procedures. Our experiments indicate significant improvements in generated texture quality, measured by different scores, including 
FID~\cite{Heusel:2017:GANs}, KID~\cite{Bińkowski:2018d:Demystifying}, CLIP~\cite{Schuhmann:2022:Laion5b}
metrics, when compared to alternatives. 

In summary, our method introduces the following contributions:
\begin{itemize}
    \item a cross-attention mechanism that learns how to wrap generated images from different views onto a single, coherent surface texture map.
    \item we integrate this modules with multiple alternative backbones based on 
	texture map diffusion showing consistent improvements in synthesized texture quality.
\end{itemize}
