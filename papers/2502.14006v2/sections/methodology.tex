
\begin{figure*}[!t]
    \includegraphics[width=\textwidth]{figures/methodology/network.pdf}
    \vspace*{-2mm}
    \caption{(Top) The\method, pipeline utilizes depth images and a text prompt to generate a number of candidate views (RGB images) for a given shape. The views are aggregated through a learned backprojection module that incorporates geometric information, such as 3D location, normals, angles between normals, and view vectors, as well as geodesic neighborhood information (bottom right) of shape points corresponding to pixels of the generated RGB images.
 The backprojection module integrates several cross-attention blocks
 (bottom left) used
 to infer texel features and colors from the appearance and geometric information gathered from relevant, non-background pixels across all
 available views.  
 As some texels may remain uncolored, an inpainting and high-definition (HD) module is applied to refine the texture map following Paint3D \cite{Zeng:2024:Paint3D}. }
    \label{fig:architecture}
    \vspace*{-5mm}
\end{figure*}

\section{Method}

Given an untextured 3D shape $\bS$, represented as a polygon mesh, along with its surface parametrization in terms of UV coordinates and a text prompt $\bt$ describing its intended texture,
the goal of our method is to generate an albedo texture map i.e., the base RGB color of the object. The texture map
$\bT$
is 
 stored as a high-res $H \times W \times 3$ atlas in UV space
 ($H=W=1024$ in our implementation). Our overall pipeline is illustrated in Figure \ref{fig:architecture}. Its stages involve: (a) rendering depth maps for the input shape from a set of viewpoints, (b) generating RGB views for these viewpoints through a diffusion model  conditioned on the input depth maps, (c) backprojecting the RGB images to the shape's texture space, (d) inpainting and upsampling the map in UV space. 
  In the following sections, we discuss the steps of our pipeline, and in particular the learned backprojection stage, which is our main contribution. 

\subsection{Depth/edge map rendering \& viewpoint selection} 
\label{subsec:backbones}
As done in several recent texture generation approaches \cite{Zeng:2024:Paint3D, Liu:2024:SyncMVD, Ceylan:2024:Matatlas, Cheng:2024:MVPaint}, the first step in our pipeline is to render the mesh into a set of depth maps
$\{\bD_c\}_{c=1}^C$ from various viewpoints,
where $C$ is the total number of viewpoints. These maps are used as conditioning to guide the diffusion process to generate images consistent with the depth cues.
There have been various strategies for viewpoint selection and diffusion model conditioning -- in our paper, we experimented with two backbones: one based on Paint3D \cite{Zeng:2024:Paint3D}, and another based on MatAtlas \cite{Ceylan:2024:Matatlas}, briefly described below. 

\paragraph{Paint3D backbone}
 Paint3D follows an iterative strategy of viewpoint selection, image generation, and backprojection of the generated images to texture space.  
  First, a couple of $1024 \times 512$ depth images are generated from the frontal and rear views of the shape and are concatenated in a $2\times1$ grid. The grid is passed as input to a diffusion process that generates a corresponding $2\times1$ grid of RGB images. The use of both views as input to the diffusion model
  helps with the view consistency \cite{Zeng:2024:Paint3D}.
  The generated images are backprojected to the shape texture through a simple inverse UV mapping strategy -- we discuss backprojection strategies, including ours, in Section \ref{sec:backprojection}. 
The next iteration proceeds with two side-wise viewpoints, from which both depth images and partially colored RGB images are rendered from the partially textured mesh. These are provided as a grid to another diffusion process, whose generated images are again backprojected to UV space. The process repeats for one more step where two other top- and bottom-wise viewpoints are used. In total, three iterations (total $C=6$ views), 
with two viewpoints processed at a time, yielded the best results in Paint3D. Our experiments with this backbone follow the same iterative procedure and viewpoints -- we only modify the backprojection. 

\paragraph{MatAtlas backbone}
MatAtlas \cite{Ceylan:2024:Matatlas} follows a different viewpoint selection, diffusion conditioning,
 and view generation strategy. Initially, a set  
 $400  \times 400$ depth maps are rendered from viewpoints  uniformly sampled from the viewing sphere, and are arranged in a $4 \times 4$ grid.
  In addition, $16$ edge maps are created using the shape's occluding and suggestive contours and are placed also in a grid. These two grids are used as input to the diffusion process that generates a $4 \times 4$ grid of RGB images. These are backprojected and blended into the shape's texture space (discussed in Section \ref{sec:backprojection}). The resulting partially textured shape is rendered from the same viewpoints, and the rendered RGB images along with added partial noise, are passed to a second diffusion process yielding an updated set of RGB views. These are backprojected to the shape's texture space, yielding a sharper texture \cite{Ceylan:2024:Matatlas}. In a third step, additional viewpoints are selected accessing shape regions not textured yet. The textured shape is rendered from these viewpoints and the rendered images are arranged in a grid processed through another diffusion process, which generates another set of RGB images. These are again backprojected to the final shape's texture. In our implementation, we use $6$ initial viewpoints to render depth maps at resolution $512 \times 512$, arranged in a $3 \times 2$ grid (we do not make use of edge maps). The used viewpoints are the same as the ones used in Paint3D for more fair comparisons across the two backbones, and also because we observed that texture details are better preserved from the higher resolution
  depth maps. We replace the MatAtlas backprojection with ours -- the rest of the pipeline follows
  MatAtlas.
  
\subsection{View generation}
Both backbones use a text-to-image  stable diffusion model \cite{Rombach:2022:High} to generate candidate RGB images based on the input grids.
The stable diffusion model gradually denoises a random normal noise image in latent space
$\bz \in \real^{h\times w \times l}$, where ${h=w=64}$ and $l=4$ are the stable diffusion's latent space dimensions. The outputs of the diffusion model blocks are modulated by a ControlNet network branch \cite{Zhang:2023:ControlNet}, which is conditioned on the encoded text, depth map grid, and, depending on the specific backbone and iteration, on the rendered maps derived from partially textured shapes.
The denoised latent is decoded into a grid of images $\{\bI_c\}_{c=1}^C$ for the selected viewpoints:
\begin{equation}
\{\bI_c\}_{c=1}^C = \mD( \bz, \bt, \{\bD_c\}_{c=1}^C, \{\bG_c\}_{c=1}^C
; \tau_{t}, \tau_{d}, \tau_{g})
\end{equation}
where $\bz$ are noisy latents, $\bt$ is the input text, $\{\bD_c\}_{c=1}^C$ are depth maps, $\{\bG_c\}_{c=1}^C$ are rendered images from the partially textured shape (used in Paint3D and MatAtlas after the first iteration), 
$\tau_{t}, \tau_{d}, \tau_{g}$ are encoder networks that produced text, depth, and image representations used as control guidance for the diffusion process.

\subsection{Backprojection}
\label{sec:backprojection}

The goal of the backprojection is to transfer the generated image colors from all used viewpoints 
back to the shape's texture map. We first describe how backprojection has been implemented in previous methods, then we discuss our neural approach. 

\subsubsection{Traditional backprojection}
\paragraph{Inverse UV mapping}
Previous methods use an inverse UV mapping procedure for backprojection. 
Specifically, given each texel in the texture map $\bu=(u,v) \in \bT$, its corresponding 3D surface point $\bs_{\bu}=(x_{\bu},y_{\bu},z_{\bu}) \in \bS$ is first estimated. Practically, this can be implemented by rendering a flattened version of the input polygon mesh $\bS$ with its vertex coordinates replaced with its texture coordinates. Then for each rendered pixel, its barycentric coordinates are calculated within the flattened triangle it belongs to. These are used to interpolate the 3D vertex positions of this triangle in the original mesh to acquire the corresponding 3D point $\bs_{\bu}$ for that texel. 
The procedure assumes that each texture coordinate maps to a single 3D face -- if a texture coordinate is re-used by multiple faces, the texture can be unwrapped to avoid this \cite{Levy:2002:LSCM}. 


\paragraph{Backprojection via most front-facing view}
Most previous methods, such as Paint3D \cite{Zeng:2024:Paint3D}, Text2Tex \cite{Chen:2023:Text2tex}, 
TEXTure~\cite{Richardson:2023:Texture}, 
 find the view where the 3D point appears to be the most front-facing i.e., the dot product between its normal $\bn_{\bu}$ and the view vector $\bv_c$ is maximized,  and simply copy the color from the generated image pixel where the 3D point is projected onto under that view:
\begin{equation}
 \bT[\bu] = \bI_{c'}[ \mR_{c'}( \bs_{\bu} ) ], \;    \text{where}  \;
 c' = argmax_c \big( \bn_{\bu} \cdot \bv_c \big)
\label{eq:backprojection_paint3d}
\end{equation}
where $\mR_{c'}$ returns the 2D pixel coordinates of the point $\bs_{\bu}$ rendered onto the image $\bI_{c'}$ under the most front-facing viewpoint $c'$ for this point. It is also common to employ a hand-tuned threshold 
$\bn_{\bu} \cdot \bv_c > thr$ to avoid copying colors from  obscure views. We note some texels may not acquire any color, if their corresponding points are not accessible by any acceptable views -- texture inpainting is used to fill such texels with color \cite{Zeng:2024:Paint3D}. Unfortunately, this strategy can easily lead to inconsistencies e.g., texels of neighboring 3D points might acquire colors from different views that may not blend well together. 

\paragraph{Backprojection via blending views}
An alternative strategy, followed by MatAtlas \cite{Ceylan:2024:Matatlas} in its first diffusion iteration, 
is to average colors from the pixels of all views accessing the texel's corresponding point to blends any small inconsistencies:
\begin{equation}
 \bT[\bu] = \avg_c \, \bI_{c}[ \mR_{c}( \bs_{\bu} ) ] , 
 \label{eq:backprojection_matatlas}
\end{equation}
Other approaches \cite{zhang2024texpainter, Cheng:2024:MVPaint} implement a weighted averaging scheme, where the weights are the dot product between the 3D point normals $\bn_{\bu}$ and view vectors $\bv_c$.
Unfortunately, averaging schemes can yield blurry texture results, as also noted in \cite{Ceylan:2024:Matatlas}.

\subsubsection{Neural backprojection} 

Instead of relying on ad hoc, hand-tuned schemes for backprojecting and blending colors from
the generated views, we instead propose a learned backprojection scheme. We utilize a neural 
module based on attention \cite{Vaswani:2017:Attenion} to assign appropriate colors to each 
texel by comparing its features with those of pixels gathered from image neighborhoods 
related to this texel across all views. The texel and pixels features are learned based on   positional encodings of the underlying 3D points corresponding to these texels and pixels
 as well as their underlying appearance (color). The positional encodings incorporate information about their 3D position, normals, angles between normals and view vectors, and surface coordinates encoded in geodesic distances -- the reason for using all this information is that the texel color should not be determined by a pixel from a single view, or by merely averaging pixels, but instead by considering
 broader pixel neighborhoods across all views to maximize view consistency,
 and by   
 considering texture correlations in local surface neighborhoods according to the underlying 3D geometry
 to promote texture consistency.

\paragraph{Pixel neighborhoods}
For each texel $\bu$ and each input view, we collect the $K \times K$ pixel neighborhood
centered around the pixel $\mR_{c}( \bs_{\bu} )$, where the texel's corresponding point $\bs_{\bu}$ is projected onto. 
We discard any pixels that lie outside the shape's silhouette, i.e., those in the background. 
The remaining pixels from neighborhoods across all views are then gathered to form a set of pixels
$\mN(\bs_{\bu})$.
The features from these pixels are used as input to our neural module, which learns to determine the texel's color by identifying relevant pixels from this set.
We discuss the choice of $K$ in our experiments.
 While one could theoretically use a very large $K$ (even the entire image), this would be inefficient and degrade performance. 
 Limiting $K$ to $1$, which only includes pixels where the 3D point projects, results in less view-consistent textures in our experiments. 
  We found that smaller neighborhoods ($K=3$) yield the most consistent textures.
 
\paragraph{Positional encodings}
For each pixel $\bp \in \mN(\bs_u)$, we determine the corresponding 3D surface point projected onto this pixel based on the view the pixel originated from. 
We then compute a feature vector that encodes the 3D position $\bs_{\bp}$ and normal $\bn_{\bp}$ of this surface point relative to the texel's corresponding surface point.
Pixels whose 3D locations are closer to the texel's point, or have more similar normals, are expected to have a stronger influence on its color.
Additionally, we encode the geodesic distance $\delta_{\bp, \bu}$ between the pixel's surface point and the texel's 3D point.
Geodesic distances refine pixel contributions by accounting for true surface proximity unlike Euclidean distances, which may misleadingly suggest closeness e.g., in regions with folds and high-curvature regions (Figure \ref{fig:surfconsistency}). Geodesic distances are computed using the method in \cite{Melvaer:2012:GPC}. The encoding is obtained via a trained MLP using the following features:
\begin{equation}
\bh_{\bp} = MLP( \bs_{\bp} - \bs_{\bu}, \bn_{\bp} - \bn_{\bu}, \bn_{\bp} \cdot \bv_{\bc}, \delta_{\bp, \bu} )
\label{eq:positional_encodings}
\end{equation}
The texel's encoding  $\bh_{\bu}$ is also computed using the same MLP. Since we encode relative positions and normals, the texel itself is represented by zero vectors for position and normal differences, and a geodesic distance of zero. We note that absolute 3D positions and normals are not included in our encodings, as they were found to degrade performance.

\paragraph{Appearance encodings}
The texel color should be determined as a function of the pixel color in the extracted neighborhoods, thus we also encode color features used as input to our backprojection module.
For each pixel $\bp \in \mN(\bs_u)$, we use an MLP  to encode its RGB color into a feature vector $\bff_{\bp}$. The same MLP is used to encode the texel's current color into $\bff_{\bu}$, provided it has been initialized from a previous backprojection step. If the texel is empty, we use black color as the input to the MLP.

\begin{figure*}[!t]
    \includegraphics[width=\textwidth]{figures/evaluation/paint3d_comparisons.pdf}
    \vspace*{-5mm}
    \caption{Comparisons between our method,\method, and Paint3D \cite{Zeng:2024:Paint3D}. Paint3D suffers from view projection artifacts when there are steep depth changes or occluded regions in the input views, as its heuristic best view selection strategy leads to texture discontinuities and inconsistencies. In contrast, our approach generates more seamless and coherent textures.}
    \label{fig:paint3d_comparison}
    \vspace*{-5mm}
\end{figure*}

\paragraph{Cross attention}
To compute the texel color, our module employs a cross-attention mechanism that compares the texel's position and color encoding with those of neighboring pixels to determine their contribution towards the texel color. Specifically, we treat the texel as the query and each pixel as a key, applying the following query-key-value transformations:
\begin{align}
\bq_{\bu} = \bQ \cdot ( \bff_{\bu} + \bh_{\bu} ) \\
\bk_{\bp} = \bK \cdot ( \bff_{\bp} + \bh_{\bp} ) \\
\bv_{\bp} = \bV \cdot  \bff_{\bp} 
\end{align}
where $\bQ, \bK, \bV$ are learned transformations. 
Note that the positional encodings are added to the rest of the features, as also done in 
\cite{Vaswani:2017:Attenion}. Note that in our case, the value transformation involves only the color 
encodings, as our end goal is to transform pixel colors (rather than position) to  texel 
colors. Based on the query and key transformations, we compute the attention weights, which 
represent the importance of each pixel in contributing to the texel's color:
\begin{equation}
a_{\bu, \bp} = softmax( \bq_{\bu} \cdot \bk_{\bp} / \sqrt{D} )
\end{equation}
where $D$ is the dimensionality of the feature vectors ($D=64$ in our implementation). Finally  texel features are updated based on the computed attention weights and a residual block: 
\begin{equation}
\bff_{\bu}' = \sum_{\bp} a_{\bu, \bp} \bv_{\bp} + \bff_{\bu}
\end{equation}

The computed texel features serve as input to a subsequent cross-attention block -- our module applies a total of three attention blocks. The final texel features are then decoded into RGB colors using a trained MLP. Each texel with a non-empty pixel neighborhood is processed through this pipeline. Texels without detected pixel neighborhoods, corresponding to regions inaccessible from any view, remain empty (non-colored); we discuss inpainting for these cases in the next section.


\subsection{Texture inpainting and refinement}
After backprojection and the final iteration of either backbone, some texels may still remain empty. For 
texture inpainting, we follow Paint3D's approach: a trained diffusion model fills any texture holes 
within the UV plane. Additionally, Paint3D's high-definition (HD) diffusion model is subsequently used 
to further enhances the visual quality of the texture map in UV space. We refer readers to Paint3D 
\cite{Zeng:2024:Paint3D} for more details, and the authors' implementation for these trained modules.
We note that we apply the same inpainting and HD processing for both backbone implementations. 
Unfortunately, as shown in our experiments, these post-processing modules often fail to correct the 
artifacts introduced by traditional backprojection.

\subsection{Training}
\label{sec:training}
We train the parameters of our MLPs and cross-attention module based on supervision from 
Objaverse \cite{Deitke:2023:Objaverse}. We use the training split from Paint3D, a subset of 
the Objaverse dataset containing approximately 100K shapes, each paired with a target 
texture image. We preprocess the data by computing geodesic neighborhoods for each object, 
storing the resulting tensors as additional shape-specific information.
The network renders input views, which are then used to reconstruct the target texture 
during training.  Our network is trained with a 
batch size of $4$ for $10$ epochs on four NVIDIA A6000 GPUs, taking approximately five days. 
To make our model more robust to any view inconsistencies,
we employ a mixed batch approach where some renders are re-generated using a pretrained 
Stable Diffusion 1.5 model~\cite{Rombach:2022:High} with partial noise levels ranging from 
$0.2$ to $0.7$. Samples with $0.2$ noise introduce minor variations, while those with $0.7$ 
noise introduce significant deviations from the target texture. For training, we optimize 
the model’s weights using an L1 loss function between the generated and target texture 
images. \vspace*{-4mm}

\subsection{Implementation details} 
During inference, our approach follows either backbone described in Section \ref{subsec:backbones}, yet 
incorporating the learned backprojection module instead of their heuristic backprojection. Since some 
texels remain unfilled after backprojection, they are subsequently inpainted and refined using 
pretrained Stable Diffusion 1.5 and the Paint3D's 3D-aware ControlNet module. The final output textures 
have a resolution of $1024\times1024$. The entire texturing process takes one to two minutes on a single 
NVIDIA A6000 GPU to texture an input shape. Each iteration of view generation and neural backprojection 
takes approximately $35$ seconds, while the inpainting and high-definition (HD) modules require around 
$20$ seconds each. For preprocessing, our approach employs a one-time procedure to compute geodesic 
information metadata, which takes approximately $30$ minutes when processing a new object for the first 
time. This part can be significantly accelerated with more efficient techniques for computation of 
geodesics \cite{Crane:2013:Heat,Zhang:2023:NeuroGF}. We also refer readers to our project page with 
source code for more details. \footnote{\emph{Project page (with code): 
\href{https://ygeorg01.github.io/Im2SurfTex/}{ygeorg01.github.io/Im2SurfTex}}} 

\begin{figure*}[!t]
    \includegraphics[width=\textwidth]{figures/evaluation/matatlas_comparisons.pdf}
    \vspace*{-5mm}
    \caption{Comparisons between our method and MatAtlas \cite{Ceylan:2024:Matatlas}. MatAtlas struggles with
    inconsistencies in the output texture, particularly in regions with high curvature, where misalignments 
    become more apparent. In contrast, as shown in the figure, \method\, tends to produce more coherent 
    textures.}
    \label{fig:matatlas_comparison}
    \vspace*{-5mm}
\end{figure*}

