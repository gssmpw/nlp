\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{fontawesome}


\usepackage{hyperref}
\usepackage[many]{tcolorbox}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[accepted]{preprint}

\usepackage{enumitem}
\setlist{nosep}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tabularray} 
\UseTblrLibrary{booktabs}
\usepackage{wrapfig}
\usepackage{xspace}
\newcommand{\ours}{\texttt{\textbf{DuoGuard}}\xspace}
\allowdisplaybreaks
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Spacing adjustments
\setlength{\textfloatsep}{4pt}
\setlength\floatsep{2pt}
\setlength\intextsep{2pt}
\setlength{\abovecaptionskip}{0.2em}
\setlength{\belowcaptionskip}{0.2em}
\setlength{\parskip}{0.2em}
\usepackage[compact]{titlesec}
\usepackage{sidecap}
\titlespacing*{\section}{0.5pt}{*0}{*0}
\titlespacing*{\subsection}{0.5pt}{*0}{*0}

\newtcolorbox{example}[1]{
    enhanced,
    drop shadow=black!10!white,
    left=4mm,
    right=4mm,
    top=2mm,
    bottom=2mm,
    boxsep=0mm,
    rounded corners,
    title=#1,
    fontupper=\footnotesize\linespread{0.9}\fontfamily{lmr}\selectfont,}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{mylatexstyle}

\usepackage[textsize=tiny]{todonotes}

\preprinttitlerunning{\ours: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails}

\begin{document}

\twocolumn[
\preprinttitle{\ours: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails \\
\normalsize
\textbf{{\color{red} \faWarning This paper contains model outputs that may be offensive in nature.}}
\vspace{-3mm}
}

\preprintsetsymbol{equal}{*}

\begin{preprintauthorlist}
\preprintauthor{Yihe Deng}{equal,univ}
\preprintauthor{Yu Yang}{equal,univ,comp}
\preprintauthor{Junkai Zhang}{equal,univ}
\preprintauthor{Wei Wang}{univ}
\preprintauthor{Bo Li}{comp,univ2}
\end{preprintauthorlist}

\preprintaffiliation{univ}{University of California, Los Angeles}
\preprintaffiliation{univ2}{University of Illinois at Urbana-Champaign}
\preprintaffiliation{comp}{VirtueAI}

\preprintcorrespondingauthor{Yihe Deng}{yihedeng@cs.ucla.edu}

\vskip 0.3in
]

\printAffiliationsAndNotice{\preprintEqualContribution} % otherwise use the standard text.

\begin{abstract}
The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages.  
To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium.  
Empirical evaluations show that our model \ours outperforms state-of-the-art models, achieving nearly \textbf{10\%} improvement over LlamaGuard3 (8B) on English benchmarks while being \textbf{4.5$\times$ faster} at inference with a significantly smaller model (0.5B). 
We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. 
Code, model, and data will be open-sourced at \url{https://github.com/yihedeng9/DuoGuard}.
\end{abstract}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/demo-guard.pdf}
    \vspace{-2mm}
    \caption{Illustration of the use-case of a guardrail model for LLMs, which functions as moderation between the user-LLM conversation.}
    \label{fig:demo-guard}
\end{figure}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/parallel_main.pdf}
    \vspace{-7mm}
    \caption{\textbf{Overview of our main results.} In the left figure, we demonstrate a consistently superior performance of average f1 score across 6 benchmarks in the four languages. In the right figure, we show that our model maintains the lowest inference cost while achieving superior average performance across languages. We note that, although we focus on the four languages to demonstrate the two-player data synthesis framework, \ours retains its base model Qwen-2.5â€™s capacity to support all 29 languages.}
    \vspace{-3mm}
    \label{fig:main-res}
\end{figure*}

\section{Introduction}
While LLMs have become increasingly effective at assisting with human queries, their outputs can pose risks of harm to users if not properly safeguarded~\citep{zou2023universal,qi2023fine,wei2024jailbroken,shen2024anything}. Consequently, substantial research has focused on developing LLM moderation models that implement guardrails for both user inputs and LLM-generated outputs~\citep{inan2023llama,dubey2024llama,han2024wildguard,zeng2024shieldgemma,ghosh2024aegis,li2024salad}, as illustrated in Figure~\ref{fig:demo-guard}. Guardrail models designed for harmlessness, similar to reward models for helpfulness~\citep{ouyang2022training,lambert2024rewardbench}, typically function as smaller, more inference-efficient models than the larger LLMs, providing binary responses or ratings for their inputs.

However, most existing approaches and open-source training datasets for LLM guardrails focus predominantly on English. Recent research has highlighted that safety-aligned models in English exhibit performance declines when applied to other languages~\citep{de2024rtp,jain2024polyglotoxicityprompts,yang2024benchmarking,shen2024language}. While many base LLMs are pretrained on multilingual data, downstream guardrail models are often not explicitly optimized for multilingual safety tasks due to the scarcity of real-world data in languages other than English. 

The scarcity of data is not unique to multilingual model training, and synthetic data has played a crucial role in addressing this issue~\citep{aryabumi2024aya}. Ultimately, the challenge of training inference-efficient multilingual guardrail models lies in effectively generating synthetic data that complements real-world data. Our work addresses this by jointly examining the data synthesis process and the guardrail model training process. Specifically, we ask: 
can we develop a self-improving system in which the guardrail model actively guides the synthetic data generation process to enhance its own training? In response, we propose an iterative two-player RL framework involving a data generator and a guardrail classifier, enabling continuous improvement of both synthetic data generation and classifier training.

We formulate and analyze the two-player game in a theoretical setting, demonstrating that it constitutes a minimax game with a Nash equilibrium, and prove that our algorithm converges linearly to the equilibrium. Building on this theoretical foundation, we implement practical techniques, such as data filtering and self-judgment, to ensure stability and robustness within the framework. Additionally, we carefully curate the seed dataset to provide a strong foundation for the iterative process.
Our model, \ours, is evaluated across six multilingual safety benchmarks, including four originally in English that were translated into the languages under consideration. The results show that \ours consistently outperforms baselines of similar scale by more than $20\%$ on average. Even when compared to larger-scale guardrail baselines, \ours achieves an average improvement of approximately $10\%$ across languages. Our contributions are listed as follows,
\begin{itemize}[nosep,leftmargin=*]
\item We propose a two-player RL framework for multilingual guardrail model training, grounded in theoretical analysis of convergence to Nash equilibrium.
\item Addressing the lack of open-source multilingual safety data, our framework enables the generation of synthetic data in any language supported by the generator.
\item Through extensive empirical evaluation, we demonstrate that our 0.5B classifier significantly outperforms state-of-the-art guardrails of similar scale across diverse datasets and consistently surpasses larger models. 
\item We show that synthetic data generated under the guidance of the 0.5B classifier generalizes effectively to train both larger classifiers (1.5B) and different architectures (Llama-3.2-1B), resulting in superior performance.
\end{itemize}

\section{Related Work}
\input{related}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/gen-guard-demo.pdf}
    \vspace{-2mm}
    \caption{\textbf{Overview of the two-player training pipeline.} The generator produces synthetic data from seed data. The classifier makes predictions and we measure these examples as being predicted correctly or incorrectly based on their seed data label. We train the generator with DPO to create increasingly challenging examples, which in turn improve the classifier through iterative training.}
    \vspace{-3mm}
    \label{fig:demo}
\end{figure*}

\section{Problem Setting and Preliminaries}
\input{prelim}

\section{Method}
\input{method}

\section{Experiments}
\input{experiment}

\section{Conclusion and Discussion}
In summary, our work addresses the data scarcity challenge of multilingual LLM safety through a self-improving framework that combines synthetic data generation with guardrail training. Our two-player reinforcement learning approach, theoretically grounded as a min-max game with proven convergence properties, enables joint optimization of data quality and classifier performance. Empirical evaluation across six languages shows our model outperforming similarly-sized baselines by over 20\% and larger models by 10\%, with a 0.5B model size and 4.5$\times$ speedup comparing to existing guardrails. 

A key limitation of synthetic data generation is reliance on an LLM: stronger models naturally yield better outcomes. In multilingual settings, the generatorâ€™s pre-training data dictates which languages it can effectively produce. However, pre-training data is generally easier to obtain than high-quality post-training data for specific downstream tasks. Many modern LLMs, such as Qwen-2.5, already support over 29 languages. The challenge lies in leveraging these models to generate high-quality post-training data. Our work thus focuses on the contribution toward better post-training synthetic data generation.
Lastly, although \ours focuses on English, French, German, and Spanish to demonstrate the two-player data synthesis framework, it retains Qwen-2.5â€™s capacity to support all 29 languages.


\nocite{langley00}

% \clearpage
\section*{Acknowledgment}
We thank Yi Zeng for providing early constructive suggestions on candidate models for the generator and one of the source dataset SCOPE~\citep{zeng2024scope}.

\section*{Impact Statement}
This work enhances moderation capabilities across languages while addressing the scarcity of multilingual safety data. Theoretical guarantees on convergence and empirical gains across six multilingual safety benchmarks demonstrate the effectiveness and robustness of our approach.

From an ethical standpoint, our method inherits common risks associated with LLM moderation, such as potential biases in training data and potential overreliance on certain shortcuts. Ensuring responsible synthetic data curation and evaluation is crucial for minimizing unintended harms. 

Furthermore, while our approach improves multilingual safety alignment, it does not address all possible risks related to adversarial attacks or nuanced cultural contexts in safety assessments. Future research should explore techniques for refining synthetic data generation, incorporating human oversight, and ensuring that moderation models remain robust across diverse linguistic and sociocultural settings. Our work underscores the importance of scalable, multilingual safety solutions and provides a foundation for further advancements in responsible LLM alignment.
\bibliography{main}
\bibliographystyle{preprint}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


