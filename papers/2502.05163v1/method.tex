We propose an iterative two-player framework involving a generator and a guardrail classifier to synthesize multilingual training data and enhance the classifier’s ability to distinguish harmful content from benign content. The process begins with a seed dataset containing labeled safe and unsafe examples collected from open-source datasets. The generator proposes new samples in a target language, and both the generator and classifier are iteratively updated. This framework establishes a dynamic interaction: 
\begin{itemize}[nosep,leftmargin=*]
    \item \textbf{Generator's Objective:} Generate samples in the target language that challenge the classifier, reinforcing on the misclassified samples.  
    \item \textbf{Classifier's Objective:} Improve robustness by minimizing errors on previously misclassified samples proposed by the generator.
\end{itemize}
Figure~\ref{fig:demo} provides an overview of our approach.


\subsection{The Two-Player Game: Theoretical Convergence}\label{sec:method}
We formalize the interaction between the adversarial generator and the defensive classifier as a two-player game. The process begins with a seed dataset $\mathcal{S}=\{(\xb_i,y_i)\}_{i\in\cI}$ of labeled real data, where $\xb_i$ is an input text sequence and $y_i\in\{-1,1\}$ is its toxicity label. Let $\mathcal{G}_{\bphi}$ denote the adversarial generator parameterized by $\bphi$. The generator takes a sample from the seed dataset $\mathcal{S}$ and a specified language $\ell$ as input and outputs a sample text sequence $\tilde{\xb}_i$ in that language that preserves the toxicity label $y_i$ of $\xb_i$. Formally,
\vspace{-1mm}
\begin{align*}
    \mathcal{G}_{\bphi}: (\xb, y, \ell) \rightarrow \tilde{\xb}, \quad \tilde{\xb} \in \cX_{\ell}.
\end{align*}
In the following narrative, we fix a target language and deprecate $\ell$ for simplicity. Let $\mathcal{C}_{\btheta}: \cX \rightarrow y$ denote the defensive classifier parameterized by $\btheta$, which takes the generated query as input and outputs the probability of toxicity. 

\textbf{Classifier Update.} 
At iteration $t$, for a given input $(\xb,y) \in \mathcal{S}$, the generator $\mathcal{G}_{\bphi_t}$ samples a new sequence $\tilde\xb$ from its conditional probability distribution $p_{\bphi_t}(\tilde{\xb} | \xb, y)$.
The classifier is then updated by minimizing the negative log-likelihood of the true labels over the generator’s distribution $p_{\bphi_t}(\tilde{\xb} | \xb,y)$:
\begin{align}
    \btheta_{t+1} & = \argmax_{\btheta} L_{\mathcal{C}}^t(\btheta), \nonumber\\
    L_{\mathcal{C}}^t(\btheta) & =  \EE_{\tilde\xb\sim p_{\bphi_t}(\tilde\xb|\xb,y)} \big[-\log p_{\btheta}(y|\tilde\xb)\big], \label{eq:update_classifier}
\end{align}
where $p_\theta (y|\tilde\xb)$ is the conditional distribution of the classifier. 

\textbf{Generator Update.}
Simultaneously, the generator $\mathcal{G}_{\bphi}$ is aimed to produce samples that cause the classifier to make incorrect predictions. 
Therefore, we define the reward signal with the negative log-likelihood: \vspace{-2mm}
\begin{align}\label{eq:reward}
r_t\big((\xb,y), \tilde\xb \big) = - \log p_{\btheta_t} (y | \tilde\xb).
\end{align}
Equation \eqref{eq:reward} computes the negative log-likelihood of the correct label for generated samples under the current classifier, where a higher value indicates greater vulnerability of the classifier to these adversarial samples. 
Many RL algorithms can be used to maximize the reward. For training stability and computational efficiency, we choose the offline RL algorithm DPO over the online RL algorithm PPO~\citep{schulman2017proximal}. We thus model the preference between two generated samples, $\tilde\xb_w$ and $\tilde\xb_l$, given input $(\xb,y)$, using the Bradley-Terry framework:\vspace{-0.1mm}
\begin{align*} 
\PP_t(\tilde \xb_w \succ \tilde\xb_l | \xb,y) = \sigma\Big(r_t\big((\xb,y), \tilde\xb_w\big) - r_t\big((\xb,y), \tilde\xb_l\big)\Big).
\end{align*}
Based on these preferences, the generator $\mathcal{G}_{\bphi}$ is updated by minimizing the DPO objective:
\begin{align}
& \bphi_{t+1} = \argmax_{\bphi} L^t_{\mathcal{G}}(\bphi, \bphi_{\text{ref}})\nonumber\\
    &L_{\mathcal{G}}(\bphi, \bphi_{\text{ref}}) = \EE_{\tilde \xb_w, \tilde\xb_l \sim p_{\bphi_t}(\tilde \xb|\xb,y)} \PP(\tilde \xb_w \succ \tilde\xb_l | \xb,y), \nonumber \\
    &\quad \bigg[\ell\bigg(\beta \log \frac{p_{\bphi}(\tilde{\xb}_w | \xb, y)}{p_{\bphi_{\text{ref}}}(\tilde{\xb}_w | \xb, y)} - \beta \log \frac{p_{\bphi}(\tilde{\xb}_l | \xb, y)}{p_{\bphi_{\text{ref}}}(\tilde{\xb}_l | \xb, y)}\bigg)\bigg],\label{eq:population_dpo}
\end{align}
where $\bphi_{\text{ref}}$ is the reference generator model and $\beta$ is a regularization parameter controlling the deviation from the reference generator model. 

\textbf{Min-max Game Equilibrium Analysis.} The DPO objective shares the same minimizer as the corresponding PPO training objective, which is defined as:
\begin{align*}
    & L_{\text{PPO}}^t(\bphi,\bphi_{\text{ref}}) = \underbrace{\EE_{\tilde{\xb} \sim p_{\bphi}}[r_t((\xb,y), \tilde{\xb})]}_{\text{I}} - \underbrace{\beta D_{\text{KL}}(p_{\bphi} || p_{\text{ref}})}_{\text{II}}.
\end{align*}
Here, term I in $L_{\text{PPO}}^t$ is indeed the same as the training objective of the classifier $L_{\mathcal{C}}^t(\btheta)$, while the regularization term II is independent of the classifier. This connection demonstrates that our algorithm optimizes a minimax game with the following objective:
\begin{align}
     \min_{p_{\btheta}} \max_{p_{\bphi}}   \mathbb{E}_{\substack{\tilde{\xb} \sim p_{\phi}}}\big[- \log p_{\theta} (y | \tilde \xb) \big] - 
     \beta  D_{\text{KL}}(p_{\phi} || p_{\text{ref}}). \label{eq:minmax}
\end{align}
In this game, the iterative update rules for each player, as defined in Equations~\eqref{eq:update_classifier} and \eqref{eq:population_dpo}, represent their best response to the current opponent policy. As a result, the generator and classifier are guaranteed to converge to a Nash equilibrium.  
\begin{theorem}\label{thm:main}
    The minimax game defined in Equation~\eqref{eq:minmax} admits a Nash equilibrium. In addition, with an appropriately chosen regularization parameter $\beta$, the iterative updates in \eqref{eq:update_classifier} and \eqref{eq:population_dpo} converge linearly to the Nash equilibrium. 
\end{theorem}
The detailed proof is provided in Appendix~\ref{app:proof}.

\subsection{The Two-Player Game: Practical Algorithm}\label{sec:practical}
While our method is conceptually framed as the minimax game in \eqref{eq:minmax}, additional implementation details are introduced to ensure feasibility, efficiency, and performance. 
First, the generator produces $k$ new queries $\{\tilde{\xb}_{j}^{(i)}\}_{j=1}^k$ 
for a given input query $\xb^{(i)}$. To preserve the original label of the seed data, we use two distinct prompts $\bc_{y:y=\pm 1}$ for generating samples, based on whether the input is safe or unsafe: 
$\tilde{\xb}^{(i)} \sim p_{\bphi_{t-1}}(\tilde{\xb} | \xb^{(i)}, \bc_{y^{(i)}})$. 
We detail the prompts used for the generator in Appendix~\ref{app:exp}. 
The training data $\mathcal{S}^{(t)}$ at iteration $t$ is augmented exclusively with misclassified synthetic samples, defined as: $\mathcal{S}^{(t)} = \mathcal{S}^{(t-1)} \cup \tilde{\mathcal{S}}_{\text{mis}}$, where $\tilde{\mathcal{S}}_{\text{mis}} = \{\tilde{\xb}_j^{(i)}: \hat{y}_j^{(i)} \neq y^{(i)}\}$ and $\mathcal{S}^{(0)}=\mathcal{S}$.

% Instead of computing the expectation in \eqref{eq:update_classifier}, the classifier’s training objective is practically formulated as $- \frac{1}{N_1}\sum_{\tilde{\xb}_j \in \mathcal{S}} \log p_{\btheta}(y^{(j)} | \tilde{\xb}_j)$, where $N_1$ is the total number of misclassified samples. 
% The expectation in the classifier’s training objective \eqref{eq:update_classifier} is approximated by the average over the dataset $\mathcal{S}^{(t)}$: $- \frac{1}{|\mathcal{S}^{(t)}|}\sum_{\tilde{\xb}_j \in \mathcal{S}} \log p_{\btheta}(y^{(j)} | \tilde{\xb}_j)$, where $N_1$ is the total number of misclassified samples.
To further enhance performance, we adopt a fine-grained multi-label classification setup similar to \citet{dubey2024llama}, where harmful inputs can have multiple labels (e.g., hate, violence), and safe content is labeled with all zeros. The classifier’s objective is modified to a multi-label classification loss using binary cross-entropy loss (equivalent to the negative log-likelihood minimization) for each of the 12 defined harmful classes (detailed in Appendix~\ref{app:data}): \vspace{-1mm}
% \begin{align}
% L_{\mathcal{C}}(\btheta) = &- \frac{1}{N_1} \sum_{\big(\tilde{\xb},\{y_c\}\big) \in \mathcal{S}} \sum_{c=1}^{12} \bigg[y_c^{(j)} \log p_{\btheta}(y_c^{(j)} | \tilde{\xb}_j) \nonumber \\
% &+ (1 - y_{c}^{(j)}) \log (1 - p_{\btheta}(y_{c}^{(j)} | \tilde{\xb}_{j})) \bigg]. \label{eq:entropy_loss}
% \end{align}
\begin{align}
L^{(t)}_{\mathcal{C}}(\btheta) = &- \frac{1}{|\mathcal{S}^{(t)}|} \sum_{\left(\tilde{\xb},\{y_c\}\right) \in \mathcal{S}^{(t)}} \sum_{c=1}^{12} \bigg[y_c \log p_{\btheta}(y_c | \tilde{\xb}) \nonumber \\
&+ (1 - y_{c}) \log (1 - p_{\btheta}(y_{c} | \tilde{\xb})) \bigg]. \label{eq:entropy_loss}
\end{align}
To maintain stability, we retrain the classifier from scratch at each iteration using the evolving dataset, similar to iterative approaches in mathematical reasoning~\citep{hosseini2024v}.

For the generator, the DPO training objective increases the likelihood of preferred data, which are samples that cause incorrect prediction of the classifier. Therefore, we consider the correctly classified ones as the dispreferred generation samples in preference learning. The correctly classified samples are defined as $\tilde{\mathcal{S}}_{\text{cor}} = \{\tilde{\xb}_j^{(i)} : \hat{y}_j^{(i)} = y^{(i)}\}$. 
The generator's loss is then given by:
\begin{align}
    &L^{(t)}_{\mathcal{G}}(\bphi, \bphi_{\text{ref}}) = \frac{1}{N}\sum_{\xb\in\mathcal{S}^{(t)}, \tilde{\xb}_w \in \tilde{\mathcal{S}}_{\text{mis}}, \tilde{\xb}_l \in \tilde{\mathcal{S}}_{\text{cor}}} \nonumber \\
    &\quad \bigg[\ell\bigg(\beta \log \frac{p_{\bphi}(\tilde{\xb}_w | \xb)}{p_{\bphi_{\text{ref}}}(\tilde{\xb}_w | \xb)} - \beta \log \frac{p_{\bphi}(\tilde{\xb}_l | \xb)}{p_{\bphi_{\text{ref}}}(\tilde{\xb}_l | \xb)}\bigg)\bigg],\label{eq:dpo_loss}
\end{align}
where $N < |\mathcal{S}^{(t)}|$ is the number of preference pairs that we were able to construct. We summarize the practical algorithm in Algorithm~\ref{alg:adversarial-training}. 

\begin{algorithm}[ht]
\caption{Two-Player Training}
\label{alg:adversarial-training}
\textbf{Require:} Initial generator $\mathcal{G}_{\bphi_0}$ and classifier $\mathcal{C}_{\btheta_0}$; maximum iteration $T$. \\
\textbf{Input:} Seed training dataset $\mathcal{S} = \{(\xb^{(i)}, y^{(i)})\}_{i=1}^N$. Prompt $\bc_{y=-1}$ and $\bc_{y=1}$.\\
\textbf{Output:} Final generator $\mathcal{G}_{\bphi_T}$ and classifier $\mathcal{C}_{\btheta_T}$.
\begin{algorithmic}[1]
\For{$t = 1, \dots, T$}
    \State \textbf{Sample Queries:}
    \For{$(\xb^{(i)}, y^{(i)}) \in \mathcal{S}^{(t-1)}$}
        \State Sample $\{\tilde{\xb}_j^{(i)}\}_{j=1}^k \sim p_{\bphi_{t-1}}(\tilde{\xb} | \bc_{y^{(i)}},\xb^{(i)})$.
        \State Assign $\hat{y}_j^{(i)} = \mathcal{C}_{\btheta_{t-1}}(\tilde{\xb}_j^{(i)})$.
        \State Partition into:
        \[
        \tilde{\mathcal{S}}_{\text{mis}}^{(i)} = \{\tilde{\xb}_j^{(i)} : \hat{y}_j^{(i)} \neq y^{(i)}\},\]
        \[\tilde{\mathcal{S}}_{\text{cor}}^{(i)} = \{\tilde{\xb}_j^{(i)} : \hat{y}_j^{(i)} = y^{(i)}\}.
        \]
    \EndFor
    \State \textbf{Update Training Dataset:
    }
    \begin{align*}
        \mathcal{S}^{(t)} = \mathcal{S}^{(t-1)} \cup \left( \bigcup_{i}\tilde{\mathcal{S}}^{(i)}_{\text{mis}}\right).
    \end{align*}
    \State \textbf{Update Classifier According to \eqref{eq:entropy_loss}:}
    \[
    \btheta_t \gets \argmin_{\btheta} L^{(t)}_{\mathcal{C}}(\btheta).
    \]

    \State \textbf{Update Generator According to \eqref{eq:dpo_loss}:}
    \begin{align*}
        \bphi_t \gets \argmin_{\bphi} L^{(t)}_{\mathcal{G}}(\bphi, \bphi_{\text{ref}}).
    \end{align*}

\EndFor
\State \Return $\mathcal{C}_{\btheta_T}$
\end{algorithmic}
\end{algorithm}

\subsection{Data Curation}
\textbf{Data Filtering.}
A filtering process was applied during synthetic data generation to retain only high-quality, relevant proposals from the generator. First, the base model (without further fine-tuning) of the generator 
was used to assign each proposal a \textit{harmfulness score} on a scale of 1 to 5, with the prompt detailed in Appendix~\ref{app:exp}. Proposals were retained only if their scores roughly matched the seed label (e.g., scores $\leq 2$ for safe seeds and $\geq 3$ for harmful seeds). To maintain alignment with the original seed’s context, a \textit{length constraint} was enforced: proposals differing by more than 200 characters from the seed were discarded.  Furthermore, outputs that contain refusal phrases, such as ``I apologize'' or ``I cannot comply'' in any language, were excluded, as the generator fails to produce meaningful samples due to internal censorship. Finally, all retained proposals were evaluated with the current guardrail classifier. Proposals that led to misclassifications were selected for training the classifier. 


\textbf{Preference Data Construction.}
To enhance the generator within the two-player game, we construct preference data for DPO. For each seed instance, the $k$ generated proposals are categorized into one of four levels based on two key criteria: whether the proposal causes the classifier to misclassify and whether its harm rating \textit{matches} the seed label. 

\begin{itemize}[nosep,leftmargin=*]
    \item \textbf{Level~1 (Best, Preferred):} The proposal causes the classifier to misclassify. The proposal's generator-assigned rating \textit{matches} the seed label (e.g., rating $\leq 2$ for safe, $\geq 3$ for harmful).
    
    \item \textbf{Level~2 (Dispreferred):} The proposal \textit{does not} cause the classifier to misclassify. The rating \textit{matches} the seed label.
    
    \item \textbf{Level~3 (Dispreferred):} The proposal causes the classifier to misclassify. The rating \textit{does not} match the seed label. 
    
    \item \textbf{Level~4 (Unsure):} The proposal \textit{does not} cause the classifier to misclassify. The rating \textit{does not} match the label.
\end{itemize}
Preference pairs are derived by comparing proposals across these categories. For each seed instance, Level 1 data are prioritized as the \textit{preferred} option, with Level 2 serving as the \textit{dispreferred} reference. If no Level 1 examples are available, the instance is excluded from preference pairing. Alternatively, if no Level 2 examples exist, Level 3 may be used to form a weaker preference signal, since it improves the generator towards better instruction-following ability.
