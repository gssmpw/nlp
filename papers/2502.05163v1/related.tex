\noindent\textbf{Guardrail Models for LLM Safety.}
The rapid advancement of LLM capabilities~\citep{touvron2023llama,touvron2023llama2,openai2023gpt4} has underscored the need for robust safeguards to ensure responsible use~\citep{yao2024survey,dong2024building}. While safety mechanisms remain less developed than LLMs themselves, early efforts introduced models such as LlamaGuard~\citep{inan2023llama}, followed by LlamaGuard2, based on Llama3~\citep{dubey2024llama}, and LlamaGuard3, built on Llama3.1~\citep{dubey2024llama}. More recent advancements include WildGuard~\citep{han2024wildguard}, Aegis~\citep{ghosh2024aegis}, MD-Judge~\citep{li2024salad}, and ShieldGemma~\citep{zeng2024shieldgemma}.  
While the F1 score is a key metric for guardrail performance, the practical deployment also demands models that are small in scale and inference-efficient. In this regard, state-of-the-art small-scale models include LlamaGuard3 (1B), built on Llama-3.2 (1B), and ShieldGemma (2B), based on Gemma 2 (2B).  

\noindent\textbf{Benchmarks for Multilingual Safety.}
Extending safety mechanisms to multilingual settings remains challenging due to the scarcity of open-source datasets in low-resource languages~\citep{deng2024multilingual}. While many base LLMs are pretrained on multilingual corpus, most guardrail models are not explicitly fine-tuned for multilingual data, limiting their effectiveness~\citep{de2024rtp}.  
To examine this gap, early works introduced multilingual toxicity detection benchmarks by translating English datasets~\citep{wang2023all} or sourcing from Reddit~\citep{ye2023multilingual}. Recently, \citet{de2024rtp} proposed RTP-LX, focusing on evaluating guardrails in low-resource languages. Other notable contributions include PolyglotToxicityPrompts (PTP)~\citep{jain2024polyglotoxicityprompts}, which examines toxic degeneration in multilingual outputs, and a test suite by \citet{yang2024benchmarking} to assess guardrails on toxicity detection and resistance to adversarial prompts across resource levels.

\noindent\textbf{Multilingual Synthetic Data Generation.} In recent years, synthetic data generated by LLMs has emerged as a valuable tool for augmenting training datasets, particularly in scenarios where real-world data is scarce or sensitive. Among the most widely used techniques is translation, which creates synthetic parallel datasets by translating monolingual text from the target language back into the source language~\citep{bi-etal-2021-data, caswell-etal-2019-tagged, liao-etal-2021-back, marie-etal-2020-tagged, pham2021metaback, sennrich-etal-2016-improving, xu-etal-2022-synthetic}. This method has shown significant success in neural machine translation tasks, with strategies such as beam search and constrained sampling further improving data quality and diversity~\citep{sennrich-etal-2016-improving, edunov-etal-2018-understanding, xu-etal-2022-synthetic}.


\noindent\textbf{Fine-tuning LLMs via Two-player RL.} 
Recent research on improving LLM reasoning has been exploring various two-player RL frameworks. \citet{zhou2024reflect} and \citet{ma2024coevolving} employ online RL to fine-tune two LLM agents for collaborative task-solving.  
Unlike these approaches, our method, while also leveraging a two-player RL framework, focuses on data synthesis and model training rather than real-time collaboration between LLM agents during inference.  
More relevantly, recent work has adopted adversarial approaches where two players pursue opposing objectives. Among these, \citet{cheng2024self, chen2024self, wu2024self, munos2023nash, swamy2024minimaximalist} employ a self-play framework, where LLMs iteratively optimize themselves to outperform previous versions on generation tasks such as math reasoning or instruction following. We defer the detailed discussion of more classical adversarial training schemes to Appendix~\ref{app:related}.
