An LLM is represented by the probability distribution $p_{\btheta}$, parameterized by the model weight $\btheta$. Given a sequence $\xb = [x_1, \ldots, x_n]$ as the prompt, the model generates response $\yb = [y_1, \ldots, y_m]$, where $x_i$ and $y_j$ denote individual tokens. The response $\yb$ is treated as a sample from the conditional probability distribution $p_{\btheta}(\cdot|\xb)$.
The conditional probability $p_{\btheta}(\yb|\xb)$ can be factorized as $p_{\btheta}(\yb|\xb) = \prod_{j=1}^{m} p_{\btheta}(y_{j} | \xb, y_1, \ldots, y_{j-1})$.

\textbf{Preference Optimization.} To improve LLM alignment with human preferences, reinforcement learning with human feedback (RLHF) is commonly applied. This approach optimizes the LLM using human preference data modeled under the Bradley-Terry framework~\citep{dong2024rlhf,shao2024deepseekmath,ahmadian2024back}:
\begin{align*} \PP(\yb_w \succ \yb_l | \xb) = \sigma\big(r(\xb, \yb_w) - r(\xb, \yb_l)\big), \end{align*}
where $\yb_w$ is the preferred response, $\yb_l$ is the dispreferred response, and $\sigma(t) = 1 / (1 + \exp(-t))$ is the sigmoid function. The reward function $r(\xb, \yb)$ is designed to assign higher values to preferred responses.

However, training a reward model can be computationally expensive and operationally challenging. To address this, Direct Preference Optimization (DPO)~\citep{rafailov2023direct} offers a simplified alternative by leveraging an implicit reward function defined by the LLM itself. Specifically, the DPO objective is formulated as:
% \allowdisplaybreaks 
\begin{align*} 
&L_{\mathrm{DPO}}(\btheta, \btheta_{\mathrm{ref}}) = \frac{1}{|S_{\text{pref}}|} \sum_{(\xb, \yb_w, \yb_l) \in S_{\text{pref}}} \\
& \bigg[\ell\bigg(\beta \log \frac{p_{\btheta}(\yb_w | \xb)}{p_{\btheta_{\mathrm{ref}}}(\yb_w | \xb)} - \beta \log \frac{p_{\btheta}(\yb_l | \xb)}{p_{\btheta_{\mathrm{ref}}}(\yb_l | \xb)}\bigg)\bigg], 
\end{align*}
where $\btheta_{\mathrm{ref}}$ is the reference model that the policy model should not deviate too much from.

\textbf{Guardrail Models.} A guardrail model acts as a function $f: \mathcal{X} \rightarrow \{-1,1\}$ that evaluates an input text sequence, which may be either user input or an LLM-generated response, and determines whether the content is harmful. In practice, guardrail models are typically built upon pre-trained LLMs, parameterized by $\btheta$, and generate discrete outputs such as \textit{``safe''} or \textit{``unsafe''}. Some models further provide explanations for their classifications, improving performance at the cost of increased inference time. In our setting, we prioritize inference efficiency in model architecture by modifying the final layer of a pre-trained LLM and converting it to a binary classification model.

