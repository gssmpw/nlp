@article{owt2020pile,
    title={{OpenWebText2} dataset, as part of `The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling`},
    author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
    journal={arXiv preprint arXiv:2101.00027},
    year={2020}
}

@inproceedings{VeitWB16,
  author       = {Andreas Veit and
                  Michael J. Wilber and
                  Serge J. Belongie},
  title        = {Residual Networks Behave Like Ensembles of Relatively Shallow Networks},
  booktitle    = {{NIPS}},
  pages        = {550--558},
  year         = {2016}
}

@article{hutter2012human,
  title={The human knowledge compression contest},
  author={Hutter, Marcus},
  journal={URL http://prize. hutter1. net},
  volume={6},
  year={2012}
}

@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{LeCunBBH98,
  author       = {Yann LeCun and
                  L{\'{e}}on Bottou and
                  Yoshua Bengio and
                  Patrick Haffner},
  title        = {Gradient-based learning applied to document recognition},
  journal      = {Proc. {IEEE}},
  volume       = {86},
  number       = {11},
  pages        = {2278--2324},
  year         = {1998}
}

@inproceedings{MichelLN19,
  author       = {Paul Michel and
                  Omer Levy and
                  Graham Neubig},
  title        = {Are Sixteen Heads Really Better than One?},
  booktitle    = {NeurIPS},
  pages        = {14014--14024},
  year         = {2019}
}

@inproceedings{VoitaTMST19,
  author       = {Elena Voita and
                  David Talbot and
                  Fedor Moiseev and
                  Rico Sennrich and
                  Ivan Titov},
  title        = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
                  Lifting, the Rest Can Be Pruned},
  booktitle    = {{ACL} {(1)}},
  pages        = {5797--5808},
  publisher    = {Association for Computational Linguistics},
  year         = {2019}
}

@inproceedings{BehnkeH20,
  author       = {Maximiliana Behnke and
                  Kenneth Heafield},
  title        = {Losing Heads in the Lottery: Pruning Transformer Attention in Neural
                  Machine Translation},
  booktitle    = {{EMNLP} {(1)}},
  pages        = {2664--2674},
  publisher    = {Association for Computational Linguistics},
  year         = {2020}
}

@inproceedings{RaganatoST20,
  author       = {Alessandro Raganato and
                  Yves Scherrer and
                  J{\"{o}}rg Tiedemann},
  title        = {Fixed Encoder Self-Attention Patterns in Transformer-Based Machine
                  Translation},
  booktitle    = {{EMNLP} (Findings)},
  series       = {Findings of {ACL}},
  volume       = {{EMNLP} 2020},
  pages        = {556--568},
  publisher    = {Association for Computational Linguistics},
  year         = {2020}
}

@inproceedings{BudhrajaPNKK20,
  author       = {Aakriti Budhraja and
                  Madhura Pande and
                  Preksha Nema and
                  Pratyush Kumar and
                  Mitesh M. Khapra},
  title        = {On the weak link between importance and prunability of attention heads},
  booktitle    = {{EMNLP} {(1)}},
  pages        = {3230--3235},
  publisher    = {Association for Computational Linguistics},
  year         = {2020}
}

@inproceedings{Spatten21,
  author       = {Hanrui Wang and
                  Zhekai Zhang and
                  Song Han},
  title        = {SpAtten: Efficient Sparse Attention Architecture with Cascade Token
                  and Head Pruning},
  booktitle    = {{HPCA}},
  pages        = {97--110},
  publisher    = {{IEEE}},
  year         = {2021}
}

@inproceedings{PengSLS20,
  author       = {Hao Peng and
                  Roy Schwartz and
                  Dianqi Li and
                  Noah A. Smith},
  title        = {A Mixture of h - 1 Heads is Better than h Heads},
  booktitle    = {{ACL}},
  pages        = {6566--6577},
  publisher    = {Association for Computational Linguistics},
  year         = {2020}
}

@inproceedings{XiaZC22,
  author       = {Mengzhou Xia and
                  Zexuan Zhong and
                  Danqi Chen},
  title        = {Structured Pruning Learns Compact and Accurate Models},
  booktitle    = {{ACL} {(1)}},
  pages        = {1513--1528},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}

@article{LiCS21,
  author       = {Jiaoda Li and
                  Ryan Cotterell and
                  Mrinmaya Sachan},
  title        = {Differentiable Subset Pruning of Transformer Heads},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {9},
  pages        = {1442--1459},
  year         = {2021}
}

@inproceedings{RamsauerSLSWGHA21,
  author       = {Hubert Ramsauer and
                  Bernhard Sch{\"{a}}fl and
                  Johannes Lehner and
                  Philipp Seidl and
                  Michael Widrich and
                  Lukas Gruber and
                  Markus Holzleitner and
                  Thomas Adler and
                  David P. Kreil and
                  Michael K. Kopp and
                  G{\"{u}}nter Klambauer and
                  Johannes Brandstetter and
                  Sepp Hochreiter},
  title        = {Hopfield Networks is All You Need},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2021}
}


@inproceedings{ZhouGW0020,
  author       = {Wangchunshu Zhou and
                  Tao Ge and
                  Furu Wei and
                  Ming Zhou and
                  Ke Xu},
  title        = {Scheduled DropHead: {A} Regularization Method for Transformer Models},
  booktitle    = {{EMNLP} (Findings)},
  series       = {Findings of {ACL}},
  volume       = {{EMNLP} 2020},
  pages        = {1971--1980},
  publisher    = {Association for Computational Linguistics},
  year         = {2020}
}

@inproceedings{DaoFERR22,
  author       = {Tri Dao and
                  Daniel Y. Fu and
                  Stefano Ermon and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  booktitle    = {NeurIPS},
  year         = {2022}
}


@inproceedings{ZaheerGDAAOPRWY20,
  author       = {Manzil Zaheer and
                  Guru Guruganesh and
                  Kumar Avinava Dubey and
                  Joshua Ainslie and
                  Chris Alberti and
                  Santiago Onta{\~{n}}{\'{o}}n and
                  Philip Pham and
                  Anirudh Ravula and
                  Qifan Wang and
                  Li Yang and
                  Amr Ahmed},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Big Bird: Transformers for Longer Sequences},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html},
  timestamp    = {Tue, 19 Jan 2021 15:57:14 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/ZaheerGDAAOPRWY20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{longformer,
  author       = {Iz Beltagy and
                  Matthew E. Peters and
                  Arman Cohan},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {CoRR},
  volume       = {abs/2004.05150},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05150},
  eprinttype    = {arXiv},
  eprint       = {2004.05150},
  timestamp    = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vyas-et-al-2020,
  author={Vyas, A. and Katharopoulos, A. and Fleuret, F.},
  title={Fast Transformers with Clustered Attention},
  booktitle={Proceedings of the international conference on Neural Information Processing Systems (NeurIPS)},
  year={2020},
  pages = {21665--21674},
  url={https://fleuret.org/papers/vyas-et-al-neurips2020.pdf},
}

@inproceedings{katharopoulos-et-al-2020,
  author={Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.},
  title={Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  year={2020},
  url={https://fleuret.org/papers/katharopoulos-et-al-icml2020.pdf},
  pages={5294--5303}
}

@inproceedings{courdier-et-al-2022,
  author={Courdier, E. and Prabhu, T. and Fleuret, F.},
  title={PAUMER: Patch Pausing Transformer for Semantic Segmentation},
  booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
  year={2022},
  note={To appear},
}

@misc{andoni2015practical,
      title={Practical and Optimal LSH for Angular Distance}, 
      author={Alexandr Andoni and Piotr Indyk and Thijs Laarhoven and Ilya Razenshteyn and Ludwig Schmidt},
      year={2015},
      eprint={1509.02897},
      archivePrefix={arXiv},
      primaryClass={cs.DS}
}

@article{childsparse,
  author       = {Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Ilya Sutskever},
  title        = {Generating Long Sequences with Sparse Transformers},
  journal      = {CoRR},
  volume       = {abs/1904.10509},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.10509},
  eprinttype    = {arXiv},
  eprint       = {1904.10509},
  timestamp    = {Thu, 02 May 2019 15:13:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-10509.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{triton,
author = {Tillet, Philippe and Kung, H. T. and Cox, David},
title = {Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations},
year = {2019},
isbn = {9781450367196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3315508.3329973},
doi = {10.1145/3315508.3329973},
abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {10–19},
numpages = {10},
keywords = {compiler, GPU, neural networks},
location = {Phoenix, AZ, USA},
series = {MAPL 2019}
}

@misc{jung2019compactassessingcompactnessrepresentations,
      title={How Compact?: Assessing Compactness of Representations through Layer-Wise Pruning}, 
      author={Hyun-Joo Jung and Jaedeok Kim and Yoonsuck Choe},
      year={2019},
      eprint={1901.02757},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.02757}, 
}

@misc{gromov2024unreasonableineffectivenessdeeperlayers,
      title={The Unreasonable Ineffectiveness of the Deeper Layers}, 
      author={Andrey Gromov and Kushal Tirumala and Hassan Shapourian and Paolo Glorioso and Daniel A. Roberts},
      year={2024},
      eprint={2403.17887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.17887}, 
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@misc{mihaylov2018suitarmorconductelectricity,
      title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}, 
      author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
      year={2018},
      eprint={1809.02789},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1809.02789}, 
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@misc{bisk2019piqareasoningphysicalcommonsense,
      title={PIQA: Reasoning about Physical Commonsense in Natural Language}, 
      author={Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
      year={2019},
      eprint={1911.11641},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.11641}, 
}

@misc{hendrycks2021measuringmassivemultitasklanguage,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

@misc{shoeybi2020megatronlmtrainingmultibillionparameter,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08053}, 
}

@misc{cai2024medusasimplellminference,
      title={Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads}, 
      author={Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
      year={2024},
      eprint={2401.10774},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.10774}, 
}

@inproceedings{Tay0ASBPRYRM21,
  author       = {Yi Tay and
                  Mostafa Dehghani and
                  Samira Abnar and
                  Yikang Shen and
                  Dara Bahri and
                  Philip Pham and
                  Jinfeng Rao and
                  Liu Yang and
                  Sebastian Ruder and
                  Donald Metzler},
  title        = {Long Range Arena : {A} Benchmark for Efficient Transformers},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@inproceedings{QinSDLWLYKZ22,
  author       = {Zhen Qin and
                  Weixuan Sun and
                  Hui Deng and
                  Dongxu Li and
                  Yunshen Wei and
                  Baohong Lv and
                  Junjie Yan and
                  Lingpeng Kong and
                  Yiran Zhong},
  title        = {cosFormer: Rethinking Softmax In Attention},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2022}
}

@inproceedings{ZhengWK22,
  author       = {Lin Zheng and
                  Chong Wang and
                  Lingpeng Kong},
  title        = {Linear Complexity Randomized Self-attention Mechanism},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {27011--27041},
  publisher    = {{PMLR}},
  year         = {2022}
}

@inproceedings{Peng0Y0SK21,
  author       = {Hao Peng and
                  Nikolaos Pappas and
                  Dani Yogatama and
                  Roy Schwartz and
                  Noah A. Smith and
                  Lingpeng Kong},
  title        = {Random Feature Attention},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@inproceedings{TayBMJZZ21,
  author       = {Yi Tay and
                  Dara Bahri and
                  Donald Metzler and
                  Da{-}Cheng Juan and
                  Zhe Zhao and
                  Che Zheng},
  title        = {Synthesizer: Rethinking Self-Attention for Transformer Models},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {139},
  pages        = {10183--10192},
  publisher    = {{PMLR}},
  year         = {2021}
}



@inproceedings{GoyalCRCSV20,
  author       = {Saurabh Goyal and
                  Anamitra Roy Choudhury and
                  Saurabh Raje and
                  Venkatesan T. Chakaravarthy and
                  Yogish Sabharwal and
                  Ashish Verma},
  title        = {PoWER-BERT: Accelerating {BERT} Inference via Progressive Word-vector
                  Elimination},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {3690--3699},
  publisher    = {{PMLR}},
  year         = {2020}
}

@inproceedings{LoshchilovH19,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = {{ICLR} (Poster)},
  publisher    = {OpenReview.net},
  year         = {2019}
}

@inproceedings{KimSTGKHK22,
  author       = {Sehoon Kim and
                  Sheng Shen and
                  David Thorsley and
                  Amir Gholami and
                  Woosuk Kwon and
                  Joseph Hassoun and
                  Kurt Keutzer},
  title        = {Learned Token Pruning for Transformers},
  booktitle    = {{KDD}},
  pages        = {784--794},
  publisher    = {{ACM}},
  year         = {2022}
}

@article{linformer,
  author       = {Sinong Wang and
                  Belinda Z. Li and
                  Madian Khabsa and
                  Han Fang and
                  Hao Ma},
  title        = {Linformer: Self-Attention with Linear Complexity},
  journal      = {CoRR},
  volume       = {abs/2006.04768},
  year         = {2020}
}

@article{vaswani,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{performer,
  author       = {Krzysztof Choromanski and
                  Valerii Likhosherstov and
                  David Dohan and
                  Xingyou Song and
                  Andreea Gane and
                  Tam{\'{a}}s Sarl{\'{o}}s and
                  Peter Hawkins and
                  Jared Davis and
                  Afroz Mohiuddin and
                  Lukasz Kaiser and
                  David Belanger and
                  Lucy J. Colwell and
                  Adrian Weller},
  title        = {Rethinking Attention with Performers},
  journal      = {CoRR},
  volume       = {abs/2009.14794},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.14794},
  eprinttype    = {arXiv},
  eprint       = {2009.14794},
  timestamp    = {Wed, 23 Jun 2021 10:58:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-14794.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{efficient_survey,
  author       = {Yi Tay and
                  Mostafa Dehghani and
                  Dara Bahri and
                  Donald Metzler},
  title        = {Efficient Transformers: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2009.06732},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.06732},
  eprinttype    = {arXiv},
  eprint       = {2009.06732},
  timestamp    = {Fri, 18 Sep 2020 15:17:35 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-06732.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{wu2022memorizing,
      title={Memorizing Transformers}, 
      author={Yuhuai Wu and Markus N. Rabe and DeLesley Hutchins and Christian Szegedy},
      year={2022},
      eprint={2203.08913},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{retro,
  author       = {Sebastian Borgeaud and
                  Arthur Mensch and
                  Jordan Hoffmann and
                  Trevor Cai and
                  Eliza Rutherford and
                  Katie Millican and
                  George van den Driessche and
                  Jean{-}Baptiste Lespiau and
                  Bogdan Damoc and
                  Aidan Clark and
                  Diego de Las Casas and
                  Aurelia Guy and
                  Jacob Menick and
                  Roman Ring and
                  Tom Hennigan and
                  Saffron Huang and
                  Loren Maggiore and
                  Chris Jones and
                  Albin Cassirer and
                  Andy Brock and
                  Michela Paganini and
                  Geoffrey Irving and
                  Oriol Vinyals and
                  Simon Osindero and
                  Karen Simonyan and
                  Jack W. Rae and
                  Erich Elsen and
                  Laurent Sifre},
  title        = {Improving language models by retrieving from trillions of tokens},
  journal      = {CoRR},
  volume       = {abs/2112.04426},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.04426},
  eprinttype    = {arXiv},
  eprint       = {2112.04426},
  timestamp    = {Mon, 13 Dec 2021 17:51:48 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-04426.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{KitaevKL20,
  author       = {Nikita Kitaev and
                  Lukasz Kaiser and
                  Anselm Levskaya},
  title        = {Reformer: The Efficient Transformer},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2020}
}


@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

## beyond scale: diversity coefficient ##
@misc{lee2023scale,
      title={Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data}, 
      author={Alycia Lee and Brando Miranda and Sanmi Koyejo},
      year={2023},
      eprint={2306.13840},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

## scaling law: performance & data mixture ##
@inproceedings{Hashimoto2021ModelPS,
  title={Model Performance Scaling with Multiple Data Sources},
  author={Tatsunori Hashimoto},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235826265}
}

@misc{longpre2023pretrainers,
      title={A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity}, 
      author={Shayne Longpre and Gregory Yauney and Emily Reif and Katherine Lee and Adam Roberts and Barret Zoph and Denny Zhou and Jason Wei and Kevin Robinson and David Mimno and Daphne Ippolito},
      year={2023},
      eprint={2305.13169},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

# The Pile #
@misc{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lad2024remarkablerobustnessllmsstages,
      title={The Remarkable Robustness of LLMs: Stages of Inference?}, 
      author={Vedang Lad and Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2406.19384},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.19384}, 
}

@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}

@misc{szegedy2014goingdeeperconvolutions,
      title={Going Deeper with Convolutions}, 
      author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
      year={2014},
      eprint={1409.4842},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.4842}, 
}

@inproceedings{LeCun1989OptimalBD,
  title={Optimal Brain Damage},
  author={Yann LeCun and John S. Denker and Sara A. Solla},
  booktitle={Neural Information Processing Systems},
  year={1989},
  url={https://api.semanticscholar.org/CorpusID:7785881}
}

@misc{han2015learningweightsconnectionsefficient,
      title={Learning both Weights and Connections for Efficient Neural Networks}, 
      author={Song Han and Jeff Pool and John Tran and William J. Dally},
      year={2015},
      eprint={1506.02626},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1506.02626}, 
}

@inproceedings{zhang2020sparch,
  title={Sparch: Efficient architecture for sparse matrix multiplication},
  author={Zhang, Zhekai and Wang, Hanrui and Han, Song and Dally, William J},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={261--274},
  year={2020},
  organization={IEEE}
}

@misc{cutler2025stagformertimestaggeringtransformer,
      title={StagFormer: Time Staggering Transformer Decoding for RunningLayers In Parallel}, 
      author={Dylan Cutler and Arun Kandoor and Nishanth Dikkala and Nikunj Saunshi and Xin Wang and Rina Panigrahy},
      year={2025},
      eprint={2501.15665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.15665}, 
}

@article{gholami2024ai,
  title={AI and memory wall},
  author={Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W and Keutzer, Kurt},
  journal={IEEE Micro},
  year={2024},
  publisher={IEEE}
}

@misc{li2024tpillmserving70bscalellms,
      title={TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices}, 
      author={Zonghang Li and Wenjiao Feng and Mohsen Guizani and Hongfang Yu},
      year={2024},
      eprint={2410.00531},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2410.00531}, 
}

@misc{narayanan2021efficientlargescalelanguagemodel,
      title={Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM}, 
      author={Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick LeGresley and Mostofa Patwary and Vijay Anand Korthikanti and Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
      year={2021},
      eprint={2104.04473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.04473}, 
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@inproceedings{horowitz20141,
  title={1.1 computing's energy problem (and what we can do about it)},
  author={Horowitz, Mark},
  booktitle={2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC)},
  pages={10--14},
  year={2014},
  organization={IEEE}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@inproceedings{wang2021spatten,
  title={Spatten: Efficient sparse attention architecture with cascade token and head pruning},
  author={Wang, Hanrui and Zhang, Zhekai and Han, Song},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={97--110},
  year={2021},
  organization={IEEE}
}

@misc{han2016deepcompressioncompressingdeep,
      title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}, 
      author={Song Han and Huizi Mao and William J. Dally},
      year={2016},
      eprint={1510.00149},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1510.00149}, 
}

@misc{srivastava2015highwaynetworks,
      title={Highway Networks}, 
      author={Rupesh Kumar Srivastava and Klaus Greff and Jürgen Schmidhuber},
      year={2015},
      eprint={1505.00387},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1505.00387}, 
}

@article{Pinkus_1999, title={Approximation theory of the MLP model in neural networks}, volume={8}, DOI={10.1017/S0962492900002919}, journal={Acta Numerica}, author={Pinkus, Allan}, year={1999}, pages={143–195}}

@misc{wu2022sustainableaienvironmentalimplications,
      title={Sustainable AI: Environmental Implications, Challenges and Opportunities}, 
      author={Carole-Jean Wu and Ramya Raghavendra and Udit Gupta and Bilge Acun and Newsha Ardalani and Kiwan Maeng and Gloria Chang and Fiona Aga Behram and James Huang and Charles Bai and Michael Gschwind and Anurag Gupta and Myle Ott and Anastasia Melnikov and Salvatore Candido and David Brooks and Geeta Chauhan and Benjamin Lee and Hsien-Hsin S. Lee and Bugra Akyildiz and Maximilian Balandat and Joe Spisak and Ravi Jain and Mike Rabbat and Kim Hazelwood},
      year={2022},
      eprint={2111.00364},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.00364}, 
}

@misc{xu2024surveyresourceefficientllmmultimodal,
      title={A Survey of Resource-efficient LLM and Multimodal Foundation Models}, 
      author={Mengwei Xu and Wangsong Yin and Dongqi Cai and Rongjie Yi and Daliang Xu and Qipeng Wang and Bingyang Wu and Yihao Zhao and Chen Yang and Shihe Wang and Qiyang Zhang and Zhenyan Lu and Li Zhang and Shangguang Wang and Yuanchun Li and Yunxin Liu and Xin Jin and Xuanzhe Liu},
      year={2024},
      eprint={2401.08092},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.08092}, 
}

@misc{singh2025surveysustainabilitylargelanguage,
      title={A Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges}, 
      author={Aditi Singh and Nirmal Prakashbhai Patel and Abul Ehtesham and Saket Kumar and Tala Talaei Khoei},
      year={2025},
      eprint={2412.04782},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.04782}, 
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori et. al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

# GLAM #
@misc{du2022glam,
      title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts}, 
      author={Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and Liam Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and Kathleen Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V Le and Yonghui Wu and Zhifeng Chen and Claire Cui},
      year={2022},
      eprint={2112.06905},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

## Chinchila Scaling Law ##
@misc{hoffmann2022chinchila,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

## scaling law between loss and scale factors (model, dataset, compute) ##
@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

#RedPajama#
@software{together2023redpajama,
  author = {{Together Computer}},
  title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}


@misc{mindermann2022prioritized,
      title={Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt}, 
      author={Sören Mindermann and Jan Brauner and Muhammed Razzak and Mrinank Sharma and Andreas Kirsch and Winnie Xu and Benedikt Höltgen and Aidan N. Gomez and Adrien Morisot and Sebastian Farquhar and Yarin Gal},
      year={2022},
      eprint={2206.07137},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xie2023doremi,
      title={DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining}, 
      author={Sang Michael Xie and Hieu Pham and Xuanyi Dong and Nan Du and Hanxiao Liu and Yifeng Lu and Percy Liang and Quoc V. Le and Tengyu Ma and Adams Wei Yu},
      year={2023},
      eprint={2305.10429},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2023skillit,
      title={Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models}, 
      author={Mayee F. Chen and Nicholas Roberts and Kush Bhatia and Jue Wang and Ce Zhang and Frederic Sala and Christopher Ré},
      year={2023},
      eprint={2307.14430},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%  ======== Methodology ========  %

# TracIN #
@misc{pruthi2020tracin,
      title={Estimating Training Data Influence by Tracing Gradient Descent}, 
      author={Garima Pruthi and Frederick Liu and Mukund Sundararajan and Satyen Kale},
      year={2020},
      eprint={2002.08484},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{raecompressive2019,
  author = {Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and
            Hillier, Chloe and Lillicrap, Timothy P},
  title = {Compressive Transformers for Long-Range Sequence Modelling},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/1911.05507},
  year = {2019},
}

@inproceedings{dehghaniunivtrans,
  author       = {Mostafa Dehghani and
                  Stephan Gouws and
                  Oriol Vinyals and
                  Jakob Uszkoreit and
                  Lukasz Kaiser},
  title        = {Universal Transformers},
  booktitle    = {{ICLR} (Poster)},
  publisher    = {OpenReview.net},
  year         = {2019}
}


# TracIN-WE #
@misc{yeh2022better,
      title={First is Better Than Last for Language Data Influence}, 
      author={Chih-Kuan Yeh and Ankur Taly and Mukund Sundararajan and Frederick Liu and Pradeep Ravikumar},
      year={2022},
      eprint={2202.11844},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

# SlimPajama #
@misc{cerebras2023slimpajama,
  author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
  title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
  month = June,
  year = 2023,
  howpublished = {\url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}},
  url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B},
}


@inproceedings{parrots,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{deepmind_risks,
  author       = {Laura Weidinger and
                  John Mellor and
                  Maribeth Rauh and
                  Conor Griffin and
                  Jonathan Uesato and
                  Po{-}Sen Huang and
                  Myra Cheng and
                  Mia Glaese and
                  Borja Balle and
                  Atoosa Kasirzadeh and
                  Zac Kenton and
                  Sasha Brown and
                  Will Hawkins and
                  Tom Stepleton and
                  Courtney Biles and
                  Abeba Birhane and
                  Julia Haas and
                  Laura Rimell and
                  Lisa Anne Hendricks and
                  William Isaac and
                  Sean Legassick and
                  Geoffrey Irving and
                  Iason Gabriel},
  title        = {Ethical and social risks of harm from Language Models},
  journal      = {CoRR},
  volume       = {abs/2112.04359},
  year         = {2021}
}


@misc{gpt2,
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  title = {Language Models are Unsupervised Multitask Learners},
  pdf = {https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
}


@misc{gpt3,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	annote = {Comment: 40+32 pages},
}


@misc{gpt4,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	annote = {Comment: 100 pages},
}


@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	doi = {10.48550/arXiv.2206.07682},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	annote = {Comment: Transactions on Machine Learning Research (TMLR), 2022},
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@misc{cobbe_training_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	doi = {10.48550/arXiv.2110.14168},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]},
}

@inproceedings{pg19,
  author       = {Jack W. Rae and
                  Anna Potapenko and
                  Siddhant M. Jayakumar and
                  Chloe Hillier and
                  Timothy P. Lillicrap},
  title        = {Compressive Transformers for Long-Range Sequence Modelling},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2020}
}

@article{rope,
  author       = {Jianlin Su and
                  Yu Lu and
                  Shengfeng Pan and
                  Bo Wen and
                  Yunfeng Liu},
  title        = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  journal      = {CoRR},
  volume       = {abs/2104.09864},
  year         = {2021}
}

@misc{gptj,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{Hesimplifying,
  author       = {Bobby He and
                  Thomas Hofmann},
  title        = {Simplifying Transformer Blocks},
  journal      = {CoRR},
  volume       = {abs/2311.01906},
  year         = {2023}
}

@article{flashattn2,
  author       = {Tri Dao},
  title        = {FlashAttention-2: Faster Attention with Better Parallelism and Work
                  Partitioning},
  journal      = {CoRR},
  volume       = {abs/2307.08691},
  year         = {2023}
}

@article{multiqueryattn,
  author       = {Noam Shazeer},
  title        = {Fast Transformer Decoding: One Write-Head is All You Need},
  journal      = {CoRR},
  volume       = {abs/1911.02150},
  year         = {2019}
}

@article{falcon,
  author       = {Guilherme Penedo and
                  Quentin Malartic and
                  Daniel Hesslow and
                  Ruxandra Cojocaru and
                  Alessandro Cappelli and
                  Hamza Alobeidli and
                  Baptiste Pannier and
                  Ebtesam Almazrouei and
                  Julien Launay},
  title        = {The RefinedWeb Dataset for Falcon {LLM:} Outperforming Curated Corpora
                  with Web Data, and Web Data Only},
  journal      = {CoRR},
  volume       = {abs/2306.01116},
  year         = {2023}
}

@article{swiglu,
  author       = {Noam Shazeer},
  title        = {{GLU} Variants Improve Transformer},
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  year         = {2020}
}

@article{impactofdepth,
  author       = {Jackson Petty and
                  Sjoerd van Steenkiste and
                  Ishita Dasgupta and
                  Fei Sha and
                  Dan Garrette and
                  Tal Linzen},
  title        = {The Impact of Depth and Width on Transformer Language Model Generalization},
  journal      = {CoRR},
  volume       = {abs/2310.19956},
  year         = {2023}
}

@inproceedings{Huangdensenet,
  author       = {Gao Huang and
                  Zhuang Liu and
                  Laurens van der Maaten and
                  Kilian Q. Weinberger},
  title        = {Densely Connected Convolutional Networks},
  booktitle    = {{CVPR}},
  pages        = {2261--2269},
  publisher    = {{IEEE} Computer Society},
  year         = {2017}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{elbayad2019depth,
  title={Depth-adaptive transformer},
  author={Elbayad, Maha and Gu, Jiatao and Grave, Edouard and Auli, Michael},
  journal={arXiv preprint arXiv:1910.10073},
  year={2019}
}
@article{takase2021lessons,
  title={Lessons on parameter sharing across layers in transformers},
  author={Takase, Sho and Kiyono, Shun},
  journal={arXiv preprint arXiv:2104.06022},
  year={2021}
}

@inproceedings{liu2021faster,
  title={Faster depth-adaptive transformers},
  author={Liu, Yijin and Meng, Fandong and Zhou, Jie and Chen, Yufeng and Xu, Jinan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={13424--13432},
  year={2021}
}

@article{fedus2022switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}



@article{liang2023encouraging,
  title={Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate},
  author={Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Tu, Zhaopeng and Shi, Shuming},
  journal={arXiv preprint arXiv:2305.19118},
  year={2023}
}

@inproceedings{mohtashami2023cotformer,
  title={CoTFormer: More Tokens With Attention Make Up For Less Depth},
  author={Mohtashami, Amirkeivan and Pagliardini, Matteo and Jaggi, Martin},
  booktitle={Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)},
  year={2023}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{petty2023impact,
  title={The Impact of Depth and Width on Transformer Language Model Generalization},
  author={Petty, Jackson and van Steenkiste, Sjoerd and Dasgupta, Ishita and Sha, Fei and Garrette, Dan and Linzen, Tal},
  journal={arXiv preprint arXiv:2310.19956},
  year={2023}
}
@inproceedings{he2015convolutional,
  title={Convolutional neural networks at constrained time cost},
  author={He, Kaiming and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5353--5360},
  year={2015}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{srivastava2015highway,
  title={Highway networks},
  author={Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1505.00387},
  year={2015}
}

@article{elnokrashy2022depth,
  title={Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient Classification},
  author={ElNokrashy, Muhammad and AlKhamissi, Badr and Diab, Mona},
  journal={arXiv preprint arXiv:2209.15168},
  year={2022}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@article{villalobos2022will,
  title={Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning},
  author={Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
  journal={arXiv preprint arXiv:2211.04325},
  year={2022}
}

@article{briesch2023large,
  title={Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop},
  author={Briesch, Martin and Sobania, Dominik and Rothlauf, Franz},
  journal={arXiv preprint arXiv:2311.16822},
  year={2023}
}


@misc{men2024shortgptlayerslargelanguage,
      title={ShortGPT: Layers in Large Language Models are More Redundant Than You Expect}, 
      author={Xin Men and Mingyu Xu and Qingyu Zhang and Bingning Wang and Hongyu Lin and Yaojie Lu and Xianpei Han and Weipeng Chen},
      year={2024},
      eprint={2403.03853},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.03853}, 
}