\section{Related work}
\textbf{The effective depth of Deep Networks.} Theoretically, any feed-forward network with at least one hidden layer can model any function, given enough width ____. In practice, it is easier to achieve high expressivity by increasing the model's depth. But naively increasing the depth can make things more difficult for the optimizer, since the gradients now have to flow through many layers. To alleviate this problem, ResNets ____ introduced skip connections at regular intervals to allow an easy flow of the gradient to the first layers. Alternatively, in Inception ____, the researchers investigated ways to boost computational power by adding additional processing units along different parallel pathways in the computational network, rather than just along a single sequential path. A unification of both methods can be found in the Highway Networks ____, where the skip connection of the residual blocks consists of another block of compute. Nowadays, residual connections are ubiquitous in large models.


\begin{figure*}[h!]
  \centering
  \begin{subfigure}[b]{0.212\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/matrix_shuffle.pdf}
  \caption{Shuffling.}
  \label{fig:matrix-shuffling}
  \end{subfigure}
  \begin{subfigure}[b]{0.18\linewidth}
  \includegraphics[width=\textwidth,trim={1.3cm 0cm 0cm 0cm},clip]{./figs/matrix_identity.pdf}
  \caption{Pruning.}
  \label{fig:matrix-pruning}
  \end{subfigure}
  \begin{subfigure}[b]{0.18\linewidth}
  \includegraphics[width=\textwidth,trim={1.3cm 0cm 0cm 0cm},clip]{./figs/matrix_merge.pdf}
  \caption{Merging.}
  \label{fig:matrix-merge}
  \end{subfigure}
  \begin{subfigure}[b]{0.18\linewidth}
  \includegraphics[width=\textwidth,trim={1.3cm 0cm 0cm 0cm},clip]{./figs/matrix_parallel.pdf}
  \caption{Parallel.}
  \label{fig:matrix-parallel}
  \end{subfigure}
  \begin{subfigure}[b]{0.215\linewidth}
  \includegraphics[width=\textwidth,trim={1.3cm 0cm 0cm 0cm},clip]{./figs/matrix_parallel2_w_colorbar.pdf}
  \caption{2-Parallel.}
  \label{fig:matrix-2parallel}
  \end{subfigure}
\caption{\textbf{Changes in perplexity when applying transformations on contiguous stretches of layers.} Each one of the five heatmaps above correspond to a transformation of a group of consecutive layer, where the row index $s$ corresponds to the first layer of the group, and the column index $e$ to the last. The color coding indicates how the perplexity---estimated on a subset of RedPajama ____---is impacted by the corresponding modification of the model. The perplexity for the base Llama 2 7B model is $6.2$. In \textbf{(a)}, we shuffle---for each forward---the layers from $s$ to $e$. We can see that many consecutive layers can be shuffled with little impact on the overall perplexity. For instance, shuffling layers $15$ to $25$---$10$ layers in total---raises the perplexity only to $9.1$. In \textbf{(b)}, we prune contiguous stretches of layers. We can see that not many blocks can be removed without starting to significantly degrade the perplexity. In \textbf{(c)} we merge contiguous layers. The results with merging are near identical to those for pruning. This reveals there is no advantage in merging layers, most likely a results of averaging matrices not originating from the same initial values. In \textbf{(d)} we run contiguous blocks in parallel. Given the success of shuffling, it makes sense that this approach works well. Running blocks $17$ to $27$ raises the perplexity to $9.3$. Finally, in \textbf{(e)} we run \emph{pairs of consecutive layers} in parallel. As a result, we can parallelize much longer stretches of layers. For instance, we can apply this transformation from layer $4$ to $29$ and only increase the perplexity to $9.1$. This reduces the depth of the model from $32$ to $19$. This result makes it possible for us to leverage this parallelism for faster inference as we discuss in \S~\ref{sec:efficiency}.      
}
\label{fig:matrices}
\end{figure*}

% \textbf{The effective depth of DNNs.} Work on ResNet, work on LLMs.

\textbf{Efficient inference of LLMs.} Several complementary approaches exist for enhancing the computational efficiency of large-scale models, primarily through pruning and sparsity, quantization, and parallelism. Pruning____ constitutes a dimensional reduction methodology that systematically eliminates redundant parameters while preserving model performance, thereby introducing architectural sparsity. This methodology is founded on empirical evidence demonstrating that neural networks frequently exhibit overparameterization, containing numerous weights with negligible contribution to the output. Through sophisticated pruning strategies, the inherent sparsity support in contemporary accelerators can be leveraged to enhance both memory utilization and computational efficiency ____. In contrast, quantization encompasses the transformation of floating-point numerical representations (predominantly FP32) into reduced-precision integer formats, such as INT8 or INT4 ____. When implemented on hardware accelerators, these lower-precision representations facilitate superior memory bandwidth utilization, addressing a primary bottleneck in modern large-scale models ____; moreover, integer-based computations yield enhanced processing speed and substantially improved energy efficiency ____. Finally, parallelization techniques during inference, such as tensor and pipeline parallelism, enable the distribution of computational workload across multiple devices, thereby reducing latency and increasing throughput, although this often requires careful consideration of communication overhead and load balancing ____.





\textbf{Parallelism via Computational Graph Optimization.} Recent research has investigated architectural layer-level optimization strategies to enhance transformer model inference efficiency. The Staircase Transformer ____ implements parallel layer execution with dynamic recurrent computation based on model requirements. Similarly, the Staggering Transformer ____ achieves layer parallelization by connecting layer $l_k$ at time step $t$ to both the $(t-1)$ output of layer $l_{k-1}$ and the $t$ output of layer $l_{k-2}$. To the best of our knowledge, no research has addressed the fusion of consecutive layers through tensor parallelism.