@inproceedings{LeCun1989OptimalBD,
  title={Optimal Brain Damage},
  author={Yann LeCun and John S. Denker and Sara A. Solla},
  booktitle={Neural Information Processing Systems},
  year={1989},
  url={https://api.semanticscholar.org/CorpusID:7785881}
}

@article{Pinkus_1999, title={Approximation theory of the MLP model in neural networks}, volume={8}, DOI={10.1017/S0962492900002919}, journal={Acta Numerica}, author={Pinkus, Allan}, year={1999}, pages={143–195}}

@inproceedings{Spatten21,
  author       = {Hanrui Wang and
                  Zhekai Zhang and
                  Song Han},
  title        = {SpAtten: Efficient Sparse Attention Architecture with Cascade Token
                  and Head Pruning},
  booktitle    = {{HPCA}},
  pages        = {97--110},
  publisher    = {{IEEE}},
  year         = {2021}
}

@misc{cai2024medusasimplellminference,
      title={Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads}, 
      author={Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
      year={2024},
      eprint={2401.10774},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.10774}, 
}

@misc{cutler2025stagformertimestaggeringtransformer,
      title={StagFormer: Time Staggering Transformer Decoding for RunningLayers In Parallel}, 
      author={Dylan Cutler and Arun Kandoor and Nishanth Dikkala and Nikunj Saunshi and Xin Wang and Rina Panigrahy},
      year={2025},
      eprint={2501.15665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.15665}, 
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{gholami2024ai,
  title={AI and memory wall},
  author={Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W and Keutzer, Kurt},
  journal={IEEE Micro},
  year={2024},
  publisher={IEEE}
}

@misc{han2015learningweightsconnectionsefficient,
      title={Learning both Weights and Connections for Efficient Neural Networks}, 
      author={Song Han and Jeff Pool and John Tran and William J. Dally},
      year={2015},
      eprint={1506.02626},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1506.02626}, 
}

@misc{han2016deepcompressioncompressingdeep,
      title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}, 
      author={Song Han and Huizi Mao and William J. Dally},
      year={2016},
      eprint={1510.00149},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1510.00149}, 
}

@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}

@inproceedings{horowitz20141,
  title={1.1 computing's energy problem (and what we can do about it)},
  author={Horowitz, Mark},
  booktitle={2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC)},
  pages={10--14},
  year={2014},
  organization={IEEE}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@misc{li2024tpillmserving70bscalellms,
      title={TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices}, 
      author={Zonghang Li and Wenjiao Feng and Mohsen Guizani and Hongfang Yu},
      year={2024},
      eprint={2410.00531},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2410.00531}, 
}

@misc{narayanan2021efficientlargescalelanguagemodel,
      title={Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM}, 
      author={Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick LeGresley and Mostofa Patwary and Vijay Anand Korthikanti and Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
      year={2021},
      eprint={2104.04473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.04473}, 
}

@misc{srivastava2015highwaynetworks,
      title={Highway Networks}, 
      author={Rupesh Kumar Srivastava and Klaus Greff and Jürgen Schmidhuber},
      year={2015},
      eprint={1505.00387},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1505.00387}, 
}

@misc{szegedy2014goingdeeperconvolutions,
      title={Going Deeper with Convolutions}, 
      author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
      year={2014},
      eprint={1409.4842},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.4842}, 
}

@software{together2023redpajama,
  author = {{Together Computer}},
  title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@inproceedings{zhang2020sparch,
  title={Sparch: Efficient architecture for sparse matrix multiplication},
  author={Zhang, Zhekai and Wang, Hanrui and Han, Song and Dally, William J},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={261--274},
  year={2020},
  organization={IEEE}
}

