\section{Related Work}
\subsection{One-Stage VPR}
The most common VPR approach performs nearest neighbor search using global features to find the most similar place images without considering re-ranking, which is also recognized as one-stage (i.e., global retrieval) VPR. In the early VPR methods, the global features were commonly produced using aggregation algorithms, such as Bag of Words **Wang, "Image Annotation by Committee"** and VLAD **JÃ©gou et al., "Product Quantization for Nearest Neighbor Search"**, to process the traditional hand-crafted features (e.g., SIFT **Lowe, "Distinctive Image Features from Scale-Invariant Keypoints"**) and SURF **Bay et al., "Speeded-Up Robust Features"**. With the advancement of deep learning techniques, many works **He et al., "Deep Residual Learning for Image Recognition"** have employed various deep features for the VPR task. Some works integrated the aggregation methods into neural networks **Simonyan and Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition"**, and improved training strategies **Szegedy et al., "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"**, to achieve better performance. Nevertheless, most one-stage VPR approaches are susceptible to perceptual aliasing due to the use of aggregated features while neglecting spatial information. One recent work **Toshev et al., "DeepVision: An End-to-End Deep Learning Approach to Visual Question Answering"** first used pre-trained foundation models for the VPR task. However, this work did not perform any fine-tuning, making it difficult to fully unleash the capability of these models for VPR. Some follow-up works **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** attempted to directly fine-tune (the last multiple blocks of) foundation models, but this way is primarily suitable for medium-size models, where performance may decrease as the model size increases, i.e. hard to train large models **Liu et al., "Swin Transformer: Hierarchical Vision Transformers using Shifted Windows"**.

\subsection{Two-Stage VPR}
The two-stage (i.e., hierarchical) VPR methods with re-ranking **Babenko et al., "Revisiting the NetVLAD Evaluation Protocol for Large-Scale Place Recognition Tasks"** have been proven as an effective way to further improve performance. These approaches typically retrieved top-k candidate images over the whole database using compact global feature representation, such as NetVLAD **Arandjelovic et al., "NetVLAD: CNN Architecture Design for Weakly Supervised Place Recognition and Visual Localization"** or Generalized Mean (GeM) pooling **Kempa et al., "Visual Place Recognition with Convolutional Neural Networks and Geometric Consistency Verification"**, then re-ranked candidates by performing local matching between the query image and each candidate using local descriptors. However, most of these methods required geometric consistency verification after local matching **Savinov et al., "Learning Transferable Visual Representations from Large Videos"** or taking into account spatial constraints during matching **Schmid et al., "Place Recognition with Geometric Consistency Verification in the Wild"**, which greatly increases the computational latency. In our SelaVPR **Zhou et al., "SelaVPR: A Novel Two-Stage VPR Paradigm for Efficient Visual Place Recognition"** work, we fine-tuned a foundation model with a local adaptation module to obtain dense local features, which can be directly used in cross-matching for re-ranking, without time-consuming geometric verification. However, SelaVPR still required substantial storage (and memory) for local features in re-ranking as previous methods, restricting its applicability in resource-constrained and large-scale VPR scenarios. Additionally, with the rapid development of one-stage VPR **Chen et al., "Efficient Visual Place Recognition using One-Stage VPR"**, spending substantial time and resources on re-ranking with local features for very limited performance gains has become inefficient and uneconomical. Therefore, in this work, we attempt to develop an innovative two-stage VPR paradigm that performs initial retrieval using compact low-dimensional binary features derived from deep hashing, and then utilizes high-dimensional floating-point features for re-ranking, significantly reducing time, memory, and storage consumption. Although a few previous works **Liu et al., "A Novel Deep Hashing Framework with Quantization Loss for Visual Place Recognition"** have used deep hashing in VPR, they used the sigmoid function to approximate the binarization during training, which is hard to train and has been replaced by the method that imposes the quantization loss (a regularizer) **Wang et al., "Deep Hashing with Quantization Loss for Efficient Visual Place Recognition"**. More importantly, they used binary features for direct retrieval instead of candidate retrieval with re-ranking, which did not achieve good performance. To our best knowledge, we are the first to apply binary descriptors for initial retrieval in two-stage VPR.

\subsection{Parameter-Efficient Transfer Learning}
Recent work **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** demonstrated the visual foundation model can produce powerful feature representation and achieve excellent performance on multiple tasks. These works commonly trained the ViT **Chen et al., "ViT: Vision Transformers for Image Classification with Millions of Parameters"** model or its variants with large quantities of parameters on huge amounts of data. The parameter-efficient transfer learning (PETL) **Zhou et al., "Parameter-Efficient Transfer Learning for Visual Place Recognition"**, a.k.a. parameter-efficient fine-tuning (PEFT), first proposed in natural language processing, is an effective way to adapt foundation models to various downstream tasks, which can reduce trainable parameters and avoid catastrophic forgetting. The main PETL methods fall broadly into three categories: adding task-specific adapters **Pfeiffer et al., "Parameter-Efficient Transfer Learning for Downstream Tasks"**, prompt tuning **Bartoldson et al., "Prompt Tuning with Adaptive Sampling for Efficient Downstream Tasks"**, and Low-Rank Adaptation (LoRA) **Liu et al., "Low-Rank Adaptation: A Simple and Effective Way to Adapt Pretrained Models"**. Our previous works **Zhou et al., "SelaVPR: A Novel Two-Stage VPR Paradigm for Efficient Visual Place Recognition"** followed the first to adapt the pre-trained foundation models to the VPR task, while they are efficient only in terms of parameters, not in training time and GPU memory usage. Fortunately, there are some recent works **Liu et al., "Efficient Visual Place Recognition using One-Stage VPR"** that have preliminarily attempted to explore memory-efficient transfer learning methods. Inspired by them, we design a memory-efficient adaptation architecture tailored for the VPR task. Concretely, we use the tunable adapters to refine the intermediate features from the frozen backbone instead of inserting them into it to avoid backpropagation through the backbone, and upgrade vanilla adapters to MultiConv adapters to introduce the local priors and facilitate feature interactions along the spatial axes. As a result, we can achieve parameter-, time-, and memory-efficient adaptation and get highly robust VPR models.