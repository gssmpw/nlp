\section{Related Work}
\subsection{One-Stage VPR}
The most common VPR approach performs nearest neighbor search using global features to find the most similar place images without considering re-ranking, which is also recognized as one-stage (i.e., global retrieval) VPR. In the early VPR methods, the global features were commonly produced using aggregation algorithms, such as Bag of Words ____ and VLAD ____, to process the traditional hand-crafted features (e.g., SIFT ____, SURF ____). With the advancement of deep learning techniques, many works ____ have employed various deep features for the VPR task. Some works integrated the aggregation methods into neural networks ____, and improved training strategies ____, to achieve better performance. Nevertheless, most one-stage VPR approaches are susceptible to perceptual aliasing due to the use of aggregated features while neglecting spatial information. One recent work ____ first used pre-trained foundation models for the VPR task. However, this work did not perform any fine-tuning, making it difficult to fully unleash the capability of these models for VPR. Some follow-up works ____ attempted to directly fine-tune (the last multiple blocks of) foundation models, but this way is primarily suitable for medium-size models, where performance may decrease as the model size increases, i.e. hard to train large models ____.

\subsection{Two-Stage VPR}
\label{sec:2stagevpr}
The two-stage (i.e., hierarchical) VPR methods with re-ranking ____ have been proven as an effective way to further improve performance. These approaches typically retrieved top-k candidate images over the whole database using compact global feature representation, such as NetVLAD ____ or Generalized Mean (GeM) pooling ____, then re-ranked candidates by performing local matching between the query image and each candidate using local descriptors. However, most of these methods required geometric consistency verification after local matching ____ or taking into account spatial constraints during matching ____, which greatly increases the computational latency. In our SelaVPR ____ work, we fine-tuned a foundation model with a local adaptation module to obtain dense local features, which can be directly used in cross-matching for re-ranking, without time-consuming geometric verification. However, SelaVPR still required substantial storage (and memory) for local features in re-ranking as previous methods, restricting its applicability in resource-constrained and large-scale VPR scenarios. Additionally, with the rapid development of one-stage VPR ____, spending substantial time and resources on re-ranking with local features for very limited performance gains has become inefficient and uneconomical. Therefore, in this work, we attempt to develop an innovative two-stage VPR paradigm that performs initial retrieval using compact low-dimensional binary features derived from deep hashing, and then utilizes high-dimensional floating-point features for re-ranking, significantly reducing time, memory, and storage consumption. Although a few previous works ____ have used deep hashing in VPR, they used the sigmoid function to approximate the binarization during training, which is hard to train and has been replaced by the method that imposes the quantization loss (a regularizer) ____. More importantly, they used binary features for direct retrieval instead of candidate retrieval with re-ranking, which did not achieve good performance. To our best knowledge, we are the first to apply binary descriptors for initial retrieval in two-stage VPR.

\subsection{Parameter-Efficient Transfer Learning}
Recent work ____ demonstrated the visual foundation model can produce powerful feature representation and achieve excellent performance on multiple tasks. These works commonly trained the ViT ____ model or its variants with large quantities of parameters on huge amounts of data. The parameter-efficient transfer learning (PETL) ____, a.k.a. parameter-efficient fine-tuning (PEFT), first proposed in natural language processing, is an effective way to adapt foundation models to various downstream tasks, which can reduce trainable parameters and avoid catastrophic forgetting. The main PETL methods fall broadly into three categories: adding task-specific adapters ____, prompt tuning ____, and Low-Rank Adaptation (LoRA) ____. Our previous works ____ followed the first to adapt the pre-trained foundation models to the VPR task, while they are efficient only in terms of parameters, not in training time and GPU memory usage. Fortunately, there are some recent works ____ that have preliminarily attempted to explore memory-efficient transfer learning methods. Inspired by them, we design a memory-efficient adaptation architecture tailored for the VPR task. Concretely, we use the tunable adapters to refine the intermediate features from the frozen backbone instead of inserting them into it to avoid backpropagation through the backbone, and upgrade vanilla adapters to MultiConv adapters to introduce the local priors and facilitate feature interactions along the spatial axes. As a result, we can achieve parameter-, time-, and memory-efficient adaptation and get highly robust VPR models.
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/MultiConv.pdf}
    \vspace{-0.2cm}
    \caption{
        Illustration of the difference between our memory-efficient MultiConv adaptation network, i.e. (c), and the global adaptation in SelaVPR, i.e. (b). (a) is a transformer block in ViT. Instead of inserting the adapter into the block as (b), we train a parallel side adaptation network as (c), which consists of a series of MultiConv adapters (abbreviated as MCA) to progressively refine the intermediate features from the transformer blocks of the frozen backbone.
		}
    \vspace{-0.3cm}
    \label{MultiConv}
\end{figure*}