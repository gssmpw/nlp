\section{System Design and Implementation}

We developed \SB{},  an AI-powered platform for creating sports video highlights, featuring intuitive visualization tools designed for effective insight communication.
\SB{} system comprises three core components:
% 
\textbf{1) AI-powered Video Processing Pipeline.} A server dedicated to processing all uploaded videos using advanced computer vision techniques.
\textbf{2) Web-based Video Editor.} A React.js-based interface where users can perform video editing and explore visualizations.
% 
\textbf{3) Cloud-Based Distributed Server.} A Python Flask server that facilitates seamless data communication between the user interface and the backend.

In this section, we detail the video processing pipeline and the methods used to present visualizations in the web app. We also discuss how we have streamlined the creation process to enhance user experience, and how AI is leveraged to facilitate content creation. Lastly, we address the current limitations of our system.

\subsection{Video Processing Pipeline}
\label{sec:video-processing}
We use a video processing pipeline to pre-process sports videos for editing. 
The pipeline includes three parallel branches: 

\begin{itemize}[leftmargin=*, topsep=1pt]
    \item \textbf{Player Detection.}
This branch performs two tasks: (1) player tracking and (2) pose estimation. For player tracking, we employ MixSort~\cite{cui2023sportsmotlargemultiobjecttracking}, a customized tracker optimized for sports datasets. For pose estimation, we use MMPose~\cite{mmpose2020}, a top-down approach that utilizes bounding boxes from player tracking to predict key points for each player. Together, these methods ensure accurate and efficient tracking of player movements and poses.

    \item \textbf{Foreground-Background Segmentation.}
This branch uses Mask R-CNN~\cite{he2018maskrcnn} to segment human figures (foreground) from the court (background). The resulting segmentation masks are exported in MP4 format for use in the web app. 

    \item \textbf{AI Captioning.}
The AI Captioning branch leverages GPT-4o, a large language model (LLM), to process video content and generate detailed descriptions. Frames are sampled at a rate of one frame every 30, encoded, and then processed by GPT-4o to extract relevant insights, such as team and player details. The insights are formatted into a JSON file with second-by-second annotations and subsequently refined by another LLM to synthesize captions tailored to user needs.
\end{itemize}


The pipeline's parallel architecture operates asynchronously, ensuring efficient video processing. Each branch demonstrates robust performance, making the system well-suited for production environments. \revision{For detailed accuracy and performance evaluations of these methods, readers are referred to the original works~\cite{cui2023sportsmotlargemultiobjecttracking, mmpose2020, he2018maskrcnn}. Our primary contribution is integrating these methods into a unified visualization system and applying them in real-world, user-centered scenarios.}


% This AI Captioning branch uses a large language model (LLM), GPT-4o, to process video and generate detailed descriptions. 
% To generate captions, the system samples one frame every 30 frames from the video, encodes these frames, and prompts GPT-4o to extract relevant insights. 
% The extracted insights, such as team and player information, are formatted into a JSON file with second-by-second annotations, and then are provided to another LLM to sythesis captions based on the user's need.




% The selected frame rate balances knowledge representation and computational efficiency: lower sampling rates increase hallucinations, while higher rates result in longer processing times. 

% On the \SB{} web app, users can generate captions by retrieving stored data from the backend. Based on the user's preferred caption length, the system combines captions and creates Caption render objects. These objects are displayed on the video timeline and within the video itself. Users can further customize captions by adjusting font style, text size, color, and other properties.


% The backend pipeline leverages multimodal and few-shot prompting techniques to synthesize meaningful insights from the video.
% In addition to player tracking and visualization features, we leverage OpenAI GPT-4o to implement AI narrative features. 
% The \SB{} AI captioning feature incorporates a LLM system to process video data and generate detailed video descriptions. Within the backend video processing pipeline, multimodal and few-shot prompting techniques combined with OpenAI API calls are used to synthesize meaningful information from the video. These insights are then consolidated to create detailed captions. 

% The goal of the LLM system is to convert textual information about the video into a render object with captions. The system selects one frame for every 30 or 60 frames in the video, encodes these, and prompts GPT-4o to extract insights from the video. While the selected frames may limit the knowledge of GPT-4o, lower sampling frame rates led to model hallucinations, and higher rates led to increasingly long response times. The insights extracted from the selected frames are formatted in a json file, with information, such as teams and player numbers, separated by second. Using the json file, the system then generates captions for each second of the video and sends this, via HTTP request, to the backend. 

% On the \SB{} web app, when the user chooses to generate AI Captions, the system retrieves the stored captions from the backend. Depending on the user's chosen caption length, the web app combines the captions accordingly and creates \textit{Caption} render objects of the specified length for each. These render objects are then displayed in the timeline and on the video as captions. The user can customize these captions by changing the font style, text size, color, etc.

% AI Captioning
% In addition to tracking and visualization features, we integrate AI-powered narrative capabilities through OpenAI . 



\subsection{Web-based Video Editor}

\subsubsection{User Interfaces}
\SB{} interface, as shown in Fig.~\ref{fig:teaser}, consists of 
(a) a feature menu with a collapsible panel for the selected feature list, 
(b) a video canvas featuring an interactive video view and a timeline, and 
(c) a top menu bar for video export and account setting. 
To support an end-to-end highlight creation workflow, users can upload their clips, edit, and share the created sports highlight all in one place (\textbf{G4}). 

\vspace{1mm}
\noindent\textbf{Feature Menu.}
Three key feature categories are provided on the menu sidebar (a1), including \Media{}, \Highlight{}, and \Narrative{}. Users can click on each tab to see the respective feature list (a2):

\begin{itemize}[leftmargin=*]
    \item \emph{Media Tab.} A user can upload and manage video clips under \Media{} tab, as shown in Fig.~\ref{fig:user_journey} (a). They can select sports types from five supported sports, including basketball, soccer, volleyball, lacrosse, and tennis, and update clip through drag-and-drop, or paste a URL of an online video link. They can retrieve  clips by selecting from the recent upload list or clicking ``View All'' to search for clips in the media library. 
    
    \item  \emph{Highlight Tab.} Once a clip is loaded into the video canvas, the user can select from various sports highlight effects to turn their insights into engaging visualizations (\textbf{G1}). The highlight features are grouped by the type of sports insights, including players, tactics, or actions, as shown in Fig.~\ref{fig:teaser} (a2). When selecting a highlight feature, the user will see the corresponding feature panel (Fig.~\ref{fig:teaser} (a3)) to create the visualization or customize the color, shape or other settings. After the visualization is created, each element is mapped to an interactive track shown on video timeline (Fig.~\ref{fig:teaser} (b3)). These effect tracks are encoded with colors, orange for player features, green for tactic features, and blue for the action features.
    Detailed visualization design considerations are described in Sec~\ref{sec:vis-feature}.
    
    \item  \emph{Narrative Tab.} The user can add captions to the video under \Narrative{}, as shown in Fig.~\ref{fig:narrative}. Captions can be typed directly, with each line mapped to a corresponding subtitle track on the video timeline. The user can customize the font style and color, with real-time updates displayed on the video. Additionally, the user can select \textit{AI Captions} feature to automatically generate subtitles, with options to create sentences from 1- to 5-second video segments.
The user can interactively adjust the duration of each subtitle track on the video timeline. 
\end{itemize}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{pictures/narrative.png}
     \vspace{-6mm}
    \caption{\Narrative{} Tab provides captioning features. Users can (a) input captions manually, (b) create AI-generated captions, and (c) see instant caption update in the video.} 
    \label{fig:narrative}
    \vspace{-3mm}
\end{figure}

\vspace{1mm}
\noindent\textbf{Video Canvas.}
The video canvas consists of a video preview (b1), an editing toolbar (b2), and a video timeline (b3):

\begin{itemize}[leftmargin=*]
    \item \emph{Video Preview.} Apart from playing the video preview, the user can directly interact with the video to apply visualization effects (\textbf{G2}), as depicted in Fig.~\ref{fig:vis-feature} (a)-(d). For player and action features, the user clicks on the player in the video to create visualizations or zoom in to the selected player. For tactic features, the user draws path, zone, or marker position on the canvas. They can also preview the video in full screen mode. 

    \item \emph{Editing Toolbar.} Five common editing features are supported, including \textit{Reset}, \textit{Undo}, \textit{Redo}, \textit{Freeze}, and \textit{Split}. \textit{Freeze} allows users to add a static frame to the selected video frame, commonly used in explaining tactics in game breakdown. \textit{Split} separates the video track into two, allowing duplication or adjusting speed for individual tracks in the video timeline. Each editing tool has a shortcut key to support fast video editing. 

    \item \emph{Video Timeline.} Each track on the video timeline represent an added highlight or narrative effect, with the original video track at the bottom (Fig.~\ref{fig:teaser} (b3)). Users can change the duration and timing of the effects by moving the tracks or deleting them, providing a comprehensive overview of all visuals and narratives at one place (\textbf{G3}).
    On top of it, right click on the video track will show a track submenu for users to \textit{Mute}, \textit{Duplicate}, or change \textit{Speed} of the video easily, as shown in Fig.~\ref{fig:teaser} (b4).
\end{itemize}

% need a vis feature image to show all vis feature & canvas feature

\vspace{1mm}
\noindent\textbf{Top Menu.}
The top menu bar provides convenient access to export videos, open tutorial, and additional settings.
\begin{itemize}[leftmargin=*]
    \item \emph{Export.} To streamline the video creation process, users can export their highlights directly as a video file, a shareable link, or post them to popular social media platforms for sports content, like X, Facebook, or Reddit, as shown in Fig.~\ref{fig:teaser} (c1).
    If specified, exported videos can be stored in publicly accessible cloud storage in combination with social media friendly HTML tags. The two setups making it simpler and faster to direct share to social media platforms such as Facebook and X, streamlining content distribution for users.

    \item \emph{Additional Settings.}  To ensure future scalability, additional setting options are available.
Users can manage their account information and language preferences. Currently, \SB{} supports English, Simplified, and Traditional Chinese, with the flexibility to expand to more languages for the global sports community.
\end{itemize}


% \textbf{2. Tutorial \& Additional Settings.} 
% To ease the learning curve, \SB{} provides a tutorial that demonstrates the end-to-end workflow. The tutorial is loaded for the first time user, and can be accessed from the top menu at anytime.
% The user can load a demo video and follow steps to create and export the highlight. 
% \textbf{3. Additional settings.} 



\subsubsection{Canvas-based Video Rendering}
Visual effects in the \SB{} web app are rendered using an HTML canvas.
To enable object-level control over visualizations (\textbf{G2}), 
we introduce the concept of a \textit{render object}.
A render object is a structured data entity that encapsulates all the necessary information for rendering a specific visual effect.
Each render object includes key attributes for UI and rendering, such as its \emph{type}, \emph{start frame}, \emph{end frame}, and \emph{effect parameters}.
\SB{} supports a total of 13 render objects, including 9 visualizations described in Sec.\ref{sec:vis-feature}, 
as well as \textit{Caption}, \textit{Freeze Frame}, \textit{Background}, and \textit{Foreground}.

% \emph{Type}: Specifies the type of visualization, such as \textit{Spotlight} or \textit{Circle}.
%  and \textbf{End Frame.} Defines the time range for rendering the effect.
% Contains unique parameters for each visualization type. For instance, a \textit{Spotlight} render object requires a player ID to associate the effect with a specific player.

\setlength{\itemsep}{0pt} % Space between items
\setlength{\parskip}{0pt} % Space between paragraphs within the list
\setlength{\topsep}{0pt}  % Space above and below the list

\vspace{1mm}
\noindent\textbf{Rendering Strategy.}
Most render objects represent individual visual effects, positioned within the canvas coordinate system with their shapes and positions calculated in real time. Different types of render objects employ distinct rendering strategies:
%%% Circle 
\begin{itemize}[leftmargin=*]
    \item \emph{Player highlights}. 
    Render objects like \textit{Circle} and \textit{Spotlight} are dynamically rendered at the coordinates of the selected player's bounding box in the current frame, allowing them to move with the player.

    \item \emph{Tactic highlights}. 
    Render objects such as \textit{Path} and \textit{Zone} are rendered at fixed positions based on user-provided coordinates to highlight tactical elements.

    \item \emph{Action highlights}. Render objects for action highlights modify the canvas attributes or states rather than representing direct visual effects. Key render objects include: \emph{BG Filter} -- Adds a filter effect to the canvas; \Zoom{} -- Applies a scaling transformation to the canvas; and \emph{Freeze Frame} -- Provides a static image source to represent a frozen moment in the video.
    
\end{itemize}

Additionally, for all visual effects that need to appear on the ground, such as \textit{Circle} and \textit{Area}, a transformation matrix is applied to the original coordinates. 
This transformation creates a realistic three dimensional effect that aligns the visualizations with the court's perspective.

\vspace{1mm}
\noindent\textbf{Rendering Order.} 
To embed the visualizations into the scene, we place visualizations in two key locations:
\begin{itemize} [leftmargin=*]
    \item \emph{Between the Background and Foreground.} Suitable for effects interacting with the court, such as areas and markers.
    \item \emph{Above the Foreground.} Designed for overlays that highlight players or events, such as \textit{Spotlight} and \textit{Text} annotations.
    
\end{itemize}

To ensure proper stacking and organization, we introduce a \textit{layer} concept. Each rendering order corresponds to a specific layer, helping to manage the visual hierarchy. Render objects are processed and displayed at their assigned layer during each frame. Smaller numbers indicate layers that are rendered first.

\vspace{1mm}
\noindent\textbf{Rendering Efficiency.}
For all the render objects rendering on canvas, we are able to control the overall rendering time within 10 milliseconds for each frame. This means we support a 60 FPS video playing smoothly.



% Sports visualizations often include a combination of overlays and embedded effects~\cite{chen2021augmenting, lin2022quest}. With the foreground and background separation achieved in the video processing pipeline, 



% Table~\ref{tab:render_obj} lists all mentioned render objects,  the associated layer names, and their corresponding rendering order.
% \begin{table}[h]
%     \centering
%     \renewcommand{\arraystretch}{1.3} 
%     \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{|p{0.3\linewidth}|p{0.3\linewidth}|p{0.3\linewidth}|}
%         \hline
%         \textbf{Type} & \textbf{Layer} & \textbf{Rendering Order} \\ 
%         \hline
%         \textbf{Background} & \textbf{\texttt{bg}} & \textbf{-1} \\
%         \hline
%         \multicolumn{3}{|c|}{\textbf{Between Background and Foreground}} \\ 
%         \hline
%         \BGFilter{} & \texttt{filter} & 0 \\
%         \Circle{} & \texttt{vis} & 1 \\
%         \Connector{} & \texttt{vis} & 1 \\
%         \Path{} & \texttt{vis} & 1 \\
%         \Zone{} & \texttt{vis} & 1 \\
%         \Marker{} & \texttt{vis} & 1 \\
%         \Zoom{} & \texttt{vis} & 1 \\
%         \hline
%         \textbf{Foreground} & \textbf{\texttt{fg}} & \textbf{2} \\ \hline
%         \multicolumn{3}{|c|}{\textbf{Above Foreground}} \\ 
%         \hline
%         \Spotlight{} & \texttt{overlay} & 3 \\
%         \Text{} & \texttt{overlay} & 3 \\
%         \Caption{} & \texttt{annotation} & 4 \\
%         \hline
%     \end{tabular}
%     \vspace{0.5em} % Add vertical space between table and caption
%     \caption{Render object type, layer names, and rendering order, categorized by layers positioned behind or above the foreground.}
%     \label{tab:render_obj}
%     \vspace{-4mm}
% \end{table}




% how do we prompt gpt, what’s our input, what’s our strategy to balance gpt response time and performance
%How does the gpt response look like and how we use the response, i.e. how does the front end get the data and convert to the render object finally

% add what prompt strategy we used and what's the goal (e.g., convert textual information to render object)
% briefly mention the experiment results of different sampling framerate, caption curation
% add one line about performance

% Within the \Narrative{} tab on the web-based user interface, users can select the \textit{AI Captions} button, which will begin the caption generation process, as shown in Fig.~\ref{fig:narrative}(b). Our system allows caption generation for full videos as well as specific video clips, and allow various caption length, from 1 to 5 seconds. 
% catering to diverse user needs.
% Once the user specifies the desired caption length, the system will group and process the captions. Each line of caption will create a corresponding \textit{Caption} render object on timeline. These objects are layered within the video editing timeline as well as displayed over the video for instant preview. 
% For ease of use, the system prevents overlap of and displays all AI generated subtitles in one timeline layer. To provide greater customization, users can modify the caption length and regenerate the captions as needed.

% To ensure the captions are accessible and visually compatible with the video content, the system includes robust customization features. Users can adjust text size, apply bold formatting, and modify font and background colors. These options enhance readability and make the captions more inclusive to viewers with visual impairments or specific accessibility needs. 

% This AI Caption feature allows users to more efficiently create captions for their videos, which is particularly benefical for content creators, educators, and sports analysts who need to quickly generate detailed subtitles. By leveraging the power of LLMs and AI, we significantly reduce the manual effort typically required for captioning, enabling users to focus more on content enhancement and storytelling. By automating the labor-intensize captioning process, users can effortlessly generate subtitles for their videos. 

\subsection{Cloud-Based Distributed Server}
To efficiently handle user requests and enable scalable video processing, we implement a distributed server architecture. At the core of the system is a Python Flask server, which facilitates communication between the frontend and backend. User requests are logged into a distributed message queue system, such as Kafka, ensuring reliable and asynchronous task management.
Multiple GPU server nodes are deployed to pull requests from the queue and process the video tasks. Once processing is complete, the results are stored in a centralized database, which is accessible to the Flask server for seamless integration with the frontend.

The entire architecture is deployed on the Azure cloud platform, leveraging elastic scaling to dynamically allocate resources based on user demand. This setup ensures robust performance, high availability, and efficient handling of variable workloads.


\subsection{System Limitation}
The current system has limitations affecting both the web app and the video processing pipeline.

\vspace{1mm}
\noindent\textbf{Video Processing.}
The primary limitation of the video processing pipeline lies in the player tracking algorithm. It can fail under common conditions such as occlusion, changes in player's gesture, or player disappearance and reappearance—issues frequently encountered in sports videos. To address these challenges, improvements to the tracking algorithm are required, along with fine-tuning it on our specific dataset for more robust performance.

\vspace{1mm}
\noindent\textbf{Visualization Rendering.}
The current implementation lacks parameters of the camera, which impacts certain visual effects, such as \textit{Path} and \textit{Zone} that require a realistic 3D perspective. Currently, we apply a fixed transformation matrix across all scenarios, which can result in suboptimal visual quality in specific cases. Additionally, as the camera angle changes and the court shifts, the visualizations do not dynamically move with the court as the render objects are positioned in the static canvas coordinate system. Without proper coordinate mapping to the actual court, there could potentially be misalignment between visualizations and the court’s position.

\vspace{1mm}
\noindent\textbf{Web App Performance.}
Since the application runs in a web browser, its performance is inherently tied to browser memory usage. For instance, rendering a 15-second, 1080p video clip typically uses around 200MB of memory, which is currently acceptable. However, supporting longer or higher-resolution videos will require us to implement dynamic loading and compression mechanisms to maintain optimal performance.




% speed
% accuracy of player tracking
% future work : fine tuning...