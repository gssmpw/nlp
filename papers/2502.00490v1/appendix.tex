\section{Appendix}

\subsection{2-layer with single weights}
\label{appendix:multi_layer_qat}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/2weights_toy.pdf}
    \hfill
    \caption{We repeat the toy model experiments, but this time with 2 weights, taking into account that the linear term is no longer 0 in the gradient. We notice at epoch 15 and 18 where the prediction of the quantized model is greater than y, the effect of the terms flip for $w_2$.}
\end{figure}
Consider a linear model \( f(x) = w_2 w_1 x \), with \( w_1, w_2 \), input \( x \), and target \( y \in \mathbb{R} \). The quantized version of this model is defined as \( f_q(x) = q(w_2) q(w_1) x \), where \( q(\cdot) \) is the quantizer from Eq.~\ref{eq:quantizer}. The quadratic loss for the model is given by
\[
   \loss(f(x)) = \frac{1}{2} \bigl( w_2 w_1 x - y \bigr)^2
\]
The difference compared to full-precision optimization is then given as
\begin{align}
   \delta_{\loss} &= \loss(f_q(x)) - \loss(f(x)) \\
   &= \frac{1}{2} \Bigl[ \bigl( q(w_2) q(w_1) x - y \bigr)^2 
            - \bigl( w_2 w_1 x - y \bigr)^2 \Bigr] \\[6pt]
   &= \frac{1}{2} \Bigl[ \bigl( q(w_2) q(w_1) x \bigr)^2
            - \bigl( w_2 w_1 x \bigr)^2
            - 2y \bigl( q(w_2) q(w_1) x - w_2 w_1 x \bigr) \Bigr] \\
    &= \frac{1}{2} x^2 \Bigl[ q(w_2)^2 q(w_1)^2 - w_2^2 w_1^2 \Bigr]
    + yx \Bigl[ w_2 w_1 - q(w_2) q(w_1) \Bigr]
\end{align}

The loss difference decomposes into:
\[
   \underbrace{\frac{1}{2} x^2 \Bigl( q(w_2)^2 q(w_1)^2 - w_2^2 w_1^2 \Bigr)}_{\text{quadratic term}}
   \quad+\quad
   \underbrace{yx \Bigl( w_2 w_1 - q(w_2) q(w_1) \Bigr)}_{\text{linear term}}
\]

Taking the derivative of \(\loss\) with respect to \(w_1\):
\begin{align}
   \frac{\partial \delta_{\loss}}{\partial w_1}
   &= \frac{\partial}{\partial w_1}
      \Bigl( \loss(f_q(x)) - \loss(f(x)) \Bigr) \\
   &= \frac{\partial}{\partial w_1}
      \biggl[
         \frac{1}{2} x^2 \Bigl( q(w_2)^2 q(w_1)^2 - w_2^2 w_1^2 \Bigr)
         +
         yx \Bigl( w_2 w_1 - q(w_2) q(w_1) \Bigr)
      \biggr] \\
   &= x^2 \Bigl[
      q(w_2)^2 q(w_1) \frac{\partial q(w_1)}{\partial w_1}
      - w_2^2 w_1
   \Bigr]
   + yx \Bigl[
      w_2
      - q(w_2) \frac{\partial q(w_1)}{\partial w_1}
   \Bigr]
\end{align}

Using the STE approximation from Eq.~\ref{eq:qat_ste}, we get:
\begin{equation}
\frac{\partial \Hat{\delta}_{\loss}}{\partial w_1}
   =
   x^2 \Bigl[
      q(w_2)^2 q(w_1)
      - w_2^2 w_1
   \Bigr]
   + yx \Bigl[
      w_2 - q(w_2)
   \Bigr]
\end{equation}
We note that the linear term is no longer zero in the gradient and thus for a model consisting of 2 single weight layers we see that there is additional effects from QAT other than oscillations. Additionally because of the non-linearity of the rounding operation, even with the absence of a non-linear activation function, we can no longer reduce the model to a single weight.
\subsection{Hyperparameters}
\label{appendix:hyperparameters}
\subsubsection{ResNet-18}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.50\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ResNet_scratch_lr.pdf}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \label{tab:best_val_accuracies}
        \begin{tabular}{|c|c|c|}
    \hline
    \textbf{$\lambda$} & \textbf{3-bit (\%)} & \textbf{Ternary (\%)} \\ \hline
    0.25 & 68.77 $\pm$ 0.19 & 47.85 $\pm$ 5.51 \\ \hline
    0.50 & 69.47 $\pm$ 1.11 & 46.77 $\pm$ 4.83 \\ \hline
    0.75 & 70.08 $\pm$ 0.40 & 46.86 $\pm$ 3.01 \\ \hline
    1.00 & 66.20 $\pm$ 4.05 & 47.33 $\pm$ 2.06 \\ \hline
    1.25 & 69.31 $\pm$ 0.32 & 43.14 $\pm$ 6.62 \\ \hline
    1.50 & 68.96 $\pm$ 0.30 & 46.73 $\pm$ 3.91 \\ \hline
    1.75 & 69.92 $\pm$ 0.11 & 47.02 $\pm$ 4.19 \\ \hline
\end{tabular}
    \end{minipage}
    \caption{Mean over 3 runs of the best validation accuracy for different lambdas. Training a ResNet-18 from scratch. Both ternary and 3-bit is at $10^{-3}$ learning rate and 50\% of the data used for training. The plot shows three learning rates, where we for each learning ratue evaluate with the $\lambda$s in the rhs. table. The colored background covers the range between the maximum and minimum value of the quantized validation accuracy with the given $\lambda$s.}
    \label{fig:ResNet_scratch_hyperparam}
\end{figure}
In Fig.~\ref{fig:ResNet_scratch_hyperparam} we see the results of a hyperparameter search over different learning rates and $\lambda$s for a ResNet-18 model. There is a clear trend of seeing the best performance at a learning rate of $10^{-3}$. We note that interestingly there is a comparable performance for a wide range of $\lambda$s, indicating that it is the presence of oscillations which is important for quantization robustness, and not the exact frequency of oscillations. 

\subsubsection{Tiny ViT}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.50\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/lambda_vit.pdf}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \label{tab:best_val_accuracies}
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{$\lambda$} & \textbf{3-bit (\%)} & \textbf{Ternary (\%)} \\ \hline
            0.01 & 18.85 & - \\ \hline
            0.5  & 85.21 & 15.10 \\ \hline
            0.75 & 87.68 & - \\ \hline
            1.0  & 90.29 & 13.04 \\ \hline
            2.0  & 89.31 & 14.16 \\ \hline
            2.5  & -     & 13.70 \\ \hline
            5.0  & -     & 14.20 \\ \hline
        \end{tabular}
    \end{minipage}
    \caption{Validation accuracy at different $\lambda$ values and the corresponding best validation accuracies for 3-bit and 2-bit configurations for a single run. Learning rate is set to 1e-4 for fine-tuning. For the 2-bit we test higher $\lambda$ but still see no improvemenet in accuracy. We note how all the $\lambda$s lies close to each other, except for the low of $10^{-2}$}
    \label{fig:vit_lambda}
\end{figure}
Fig.~\ref{fig:vit_lambda} We note how also the Tiny Vit seems to allow for a wide range of $\lambda$s even though we this time note that $\lambda=1$ performs significantly better than the others.

%\subsection{Weight distribution}
%Fig.~\ref{fig:weight_distributions}


\subsection{Epochs and cross-bit robustness}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/robust_qat.png}
    \hfill
    \includegraphics[width=0.48\textwidth]{figures/robust_reg.png}
    \caption{Left is the validation accuracy during training of a ViT with QAT at different bits, right is for our regularization. Both QAT and regularization is trained with a 3-bit quantizer. We note how the order of convergences for cross-bit changes between QAT and our model and that QAT cross-bit robustness especially depends on number of epochs trained.}
    \label{fig:robustness_during_training}
\end{figure}
There is an interesting interaction between number of epochs trained and robustness both of our method and QAT. We note how QAT converges first for the target-bit and then over time also converges for the 4 and 8-bit. Additionally we see that QAT seems upper-bounded by the target-bit performance, while this is not the case for our metho.
Fig.~\ref{fig:robustness_during_training}, 

%\subsection{Weight trajectory during OsciQuant}
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.55\textwidth]{figures/weight_trajectory_reg.pdf}
%    \hfill
%    \caption{ResNet-18 first conv layer after 50 epochs, scratch.}
%    \label{fig:weight_trajectories}
%\end{figure}
%Fig.~\ref{fig:weight_trajectories} Showing per epoch might be misleading wrt to the frequency of the oscillations, we might have oscillations at each gradient step

\subsection{Convergence behaviour of Tiny ViT}
\label{appendix:robustness_convergence}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{figures/convergence.png}
    \hfill
    \caption{Regularization with a 3-bit quantizer on a Tiny ViT. We note the peculiar behaviour of the orange line, which is the validation accuracy on the target-bit performance. The performances cycles between $\approx 90\%$ and $10\%$, while the full precision accuracy (The model evaluated without quantized weights) stays some-what stable.}
    \label{fig:convergence_plot}
\end{figure}
Fig.~\ref{fig:convergence_plot} shows the convergence behaviour of the full precision weights and the quantized weights at target-bit. We note how the Tiny ViT displays a peculiar convergence behaviour, where the accuracy will break, only to go up again. In the Tiny Vit model we quantize the self-attention layers, it is already noted in ~\citet{vitoscillations} that ViTs are especially vulnerable to quantization of the query and key of a self-attention layer, which might be related to the convergence behaviour we see.

%\subsection{Wider layers less affected by oscillations}
%\label{appendix:wide_layers}
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.32\textwidth]{figures/qat_2bit_50epochs_layer1conv1.png}
    \hfill
%    \includegraphics[width=0.32\textwidth]{figures/qat_3bit_50epochs_layer1conv1.png}
    \hfill
%    \includegraphics[width=0.32\textwidth]{figures/baseline_50epochs_layer1conv1.png}
%    \caption{}
%    \label{fig:wide_layers_osc}
%\end{figure}
%In Figure ~\ref{fig:wide_layers_osc} we look at the distribution of weights for layer1[0].conv1 $\approx$ 40k in ResNet-18 after 30 epochs of training. The weights seems to cluster less at thresholds

% \subsection{Stochastic rounding and oscillations}
% \label{appendix:stochastic_rounding_appendix}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.48\textwidth]{figures/3bit2.png}
%     \hfill
%     \includegraphics[width=0.48\textwidth]{figures/stoch_3bit2.png}
%     \caption{left First layer, block 0 conv 1, 9408 weights, right is 36864 weights}
%     %\label{fig:}
% \end{figure}
% The role of the rounding operation is not mentioned in earlier works on oscillations [ADD SOURCES]. While we usually associate oscillations with a periodic change in $q(w)$, that is because the oscillation pivot is the quantization threshold. If we instead use stochastic rounding, the pivot is  $w*$ \ref{appendix:stochastic_rounding_appendix}. This means that with SR a change in $q(w)$ because of oscillations is much more unlikely than if we were using regular rounding, where we instead oscillate around thresholds.

% With SR oscillation pivot is no longer the threshold, but seems to be w* instead. Hence we see no spikes around thresholds for SR. But the weights are still being pertubed by q-w.

% with SR E(q(w)) = w* but also E[w] = w*
    
% [add toy experiment with SR]

% [Old experiments from bachelor. Redo experiments with ResNet]

% \subsection{E[q(w)] in toy model}
% \label{appendix:oscillate_expected_value}

\subsection{Counting oscillations}
\label{appendix:count_oscillations}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.23\textwidth]{figures/osc_count_nagel_qat.pdf}
    \hfill
    \includegraphics[width=0.23\textwidth]{figures/osc_count_nagel_l0.pdf}
    \hfill
    \includegraphics[width=0.23\textwidth]{figures/osc_count_nagel_l1.pdf}
    \hfill
    \includegraphics[width=0.23\textwidth]{figures/osc_count_nagel_l10.pdf}
    \hfill
    \caption{The plots shows the weights with a total oscillation count $>0$ in a baseline model, a QAT model and a model regularized with $\lambda=1$ respectively. The y-axis is the percentage of the total weights in the first convolutional layer in a ResNet-18 trained from scratch for 50 epochs. For the baseline model we log the full precision weights at each epoch and then apply the quantizer after training, where as for the regularized model we simply log both the full precision and quantized weights at each epoch.}
    \label{}
\end{figure}
Using the counting method proposed by \citet{nagel2022overcoming}, we notice the biased odd distribution of the bins. In the plot used for the main paper, we count each bin the histogram in iterations of 2. Additionally we note that  the proposed method misses the first oscillation count in each weight history.

% IF we instead use the following counting methods, which counts oscillation cycles, ie a cycle first ends when it has returned to the start state, we get the following plots
%We count an oscillation at epoch $i > 1$ if
%\[
%\text{Oscillation}(i) = 
%\begin{cases}
%1, & \text{if } q_i \neq q_{i-1} \text{ and } q_i = q_{\text{prev}}, \\
%0, & \text{otherwise}.
%\end{cases}
%\]
%Where \( q_{\text{prev}} \) is set in the following manner
%\[
%q_{\text{prev}}(i) =
%\begin{cases}
%q_{\text{prev}}(i-1), & \text{if } q_i = q_{i-1}, \\[6pt]
%q_i, & \text{if } \bigl(q_i \neq q_{i-1}\bigr) \wedge \bigl(q_i = %q_{\text{prev}}(i-1)\bigr), \\[6pt]
%q_{i-1}, & \text{if } \bigl(q_i \neq q_{i-1}\bigr) \wedge \bigl(q_i \neq %q_{\text{prev}}(i-1)\bigr).
%\end{cases}
%\]
