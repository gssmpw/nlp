\section{Related Work}
The most used strategy to minimize the impact of quantization on model accuracy is to minimize the quantization error. This can be achieved by adjusting the granularity of the quantizerâ€”for instance, using per-channel~\cite{Nagel_2019_ICCV} or block-wise quantization~\cite{dettmers2022bit} instead of per-tensor quantization. While these methods reduce quantization error without additional training, they come with increased storage requirements due to extra quantization parameters and may still fall short at very low bit widths, necessitating the combination with other approaches.

Consequently, extensive research has been dedicated to developing techniques that explicitly minimize the quantization error during optimization \cite{BridgeDeepLearning, minimizeQuantError, APTQ, Choi_2020, ImprovingLowBit, zhong2024mbquantnovelmultibranchtopology}. Given a model $\w$ the hope is that by ensuring \( q(\w) \approx \w \), we likely also have \( \loss(q(\w)) \approx \loss(\w) \), thereby preserving model accuracy after quantization.

 
An alternative and less explored approach involves training models to be robust to quantization perturbations without necessarily minimizing the quantization error itself. This means finding weights \( \w \) such that \( \loss(q(\w)) \approx \loss(\w) \) even if \( q(\w) \) is not close to \( \w \) \cite{alizadeh2020gradient, OneModelRobust}. Such methods focus on enhancing the robustness of the model to the quantization error, leading to better performance at bits different than the ones used in the quantizer, which we will refer to as cross-bit quantization.

A third approach is to train supernets on the desired configurations of the quantizers \cite{ijcai2022p504, Xu_2023_ICCV}. This approach increases the training complexity and cost, which is not incurred by explicit regularization. 

Despite these efforts, the aforementioned strategies often fall short of the accuracy obtained with QAT~\cite{jacob2017quantization} at individual bits or indirectly rely upon QAT themselves. In short, QAT integrates the quantization process into the training loop allowing the model to adapt to the quantization effects directly. This is done by quantizing the weights during the forward pass and using techniques like the Straight-Through-Estimator (STE) to approximate the gradient of the quantizer (Which has a derivative of zero almost everywhere) during backpropagation~\cite{bengio2013ste}.

Yet, there is limited understanding of how QAT affects model optimization and why it outperforms other methods. One phenomenon observed during QAT is weight oscillations \cite{pseudoQuantNoise, nagel2022overcoming}, which are periodic changes in the value of the quantized weight between two adjacent quantization levels. It is speculated in these works that that the abrupt changes in values caused by oscillations can interfere negatively with optimization. Oscillations are assumed to be undesirable side effects caused by the use of the STE during backpropagation, as the STE allows gradients to pass through the rounding operation in the quantizer, which has a gradient of zero almost everywhere \cite{pseudoQuantNoise, nagel2022overcoming}.

Several approaches have been suggested to mitigate oscillations, such as dampening or freezing the oscillating weights, which have shown improved accuracy \cite{nagel2022overcoming, gupta2023reducing}. However, the reported gains are sometimes marginal, and these methods may inadvertently also hinder the optimization process. For instance, \citet{nagel2022overcoming} notes that freezing or dampening weights too early during training can hurt optimization, indicating that oscillations might contribute to finding better quantized minima. \citet{vitoscillations} propose that weights with low oscillation frequency should be frozen, where as high-frequency ones should be left unfrozen, under the rational that high frequency means the network has little confidence in what value to quantize the weight to, where as low frequency means the optimal weight lies close to a quantization level.

Though QAT often provides the best accuracy for a given target bit, degradation to a lesser or greater extent exists when the bit of the quantizer is different to the one seen during training, ie. cross-bit quantization \cite{alizadeh2020gradient, OneModelRobust}. This means QAT requires training and storing of weights for each desired bit width. This specialization can also pose challenges when deploying models across different hardware platforms, each potentially using different quantization schemes~\cite{inferenceBenchmark}, making it difficult to develop models which can be easily quantized at deployment according to end-user requirements.

This makes robust quantization methods an interesting research avenue, especially if they could be improved to match the individual bit performance of QAT. In this work, we aim to deepen the understanding of how QAT influences model optimization, particularly focusing on the role of weight oscillations and their relation to robustness.