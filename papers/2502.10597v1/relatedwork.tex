\section{Related work \label{sec-realted-work}
}
This section will first outline the evolution from read-only to updatable learned indexes, then discuss the main operations in updatable learned indexes, and briefly compare existing methods with BLI.

\subsection{Evolution from read-only to updatable learned indexes}
The first learned index, Recursive Model Index (RMI)\cite{kraska2018case}, utilizes a hierarchy of models to improve prediction accuracy. RMI starts with the root learned model that covers the entire key range and then branches the root into child models that each handle a subset of the key range. This hierarchical division continues through several iterations, ultimately creating multiple levels of models with good prediction accuracy. At the last level, a leaf model makes predictions only on the keys within its corresponding sub-key range.

However, RMI was read-only since it was built atop a static set of data and a densely packed array. Later, \textit{updatable learned indexes} were introduced to accommodate new key-value pair insertions. In updatable learned indexes, the global sorted array can further split into multiple sub-arrays corresponding to the key ranges of their leaf models. These sub-arrays of key-value pairs are stored along with their leaf models for ease of maintenance. Existing approaches typically utilize the following three techniques \cite{wongkham2022updatable} to accommodate insertions. 1) \textbf{Merge Trees}: PGM-Index\cite{ferragina2020pgm} employs a collection of static sub-learned indexes. During insertions, the newly inserted key-value pair, along with a subset of sub-learned indexes, are merged in a write-optimized LSM-tree style, which compromises the lookup performance since all sub-learned indexes need to be visited. 2) \textbf{Delta Buffers}: XIndex\cite{tang2020xindex} and FINEdex\cite{li2021finedex} place new insertions temporarily to some separate area called delta buffers. However, these designs also degrade lookup speeds as it is necessary to check across both the major learned index and the delta buffer. 3) \textbf{Gapped Arrays}: ALEX\cite{ding2020alex} and LIPP\cite{wu2021updatable} incorporate pre-allocated gaps (i.e., empty slots) to integrate incoming key-value pairs. Typically, these in-place approaches perform better since the merge-tree and delta-buffer-based methods involve accessing multiple structures (sub-indexes or delta buffers) during a lookup. According to GRE, two state-of-the-art gapped-array-based learned indexes, ALEX \cite{ding2020alex} and LIPP \cite{wu2021updatable}, generally outperform other existing updatable learned indexes, and we mainly focus our comparisons with these two systems. Subsequent sub-sections will detail the main operations of existing updatable learned indexes, especially ALEX and LIPP, and discuss how our BLI design can offer improvements.

\subsection{Lookup: predicting an individual key vs. a key range}
The lookup process can typically be split into a \textit{traverse-to-leaf lookup}, which selects the appropriate sub-key range and traverses to the corresponding leaf model, and a \textit{leaf lookup}, which uses the leaf model to predict the position of key-value pairs. Existing learned indexes \cite{ferragina2020pgm, tang2020xindex, li2021finedex, galakatos2019fiting, ding2020alex, wongkham2022updatable} predict the offset of a target key within the corresponding array of the model.

Unlike most existing updatable learned indexes, our proposed BLI focuses on a different dimension: prediction granularity.
BLI predicts a key range, which is represented by a Bucket rather than an individual key. The Bucket layout allows for efficient last-mile search by bypassing all entries from non-target Buckets. Furthermore, the prediction model is more linear since Buckets simplifies the key distribution, which will be proved in Section \ref{sec-resilience-model-inaccuracy}.

\subsection{Insertion: strictly sorted order vs. "globally sorted, locally unsorted" order }
We focus on in-place updatable learned indexes, especially ALEX and LIPP, since they generally outperform other updatable learned indexes. Specifically, ALEX reserves empty slots in arrays corresponding to its leaf models to facilitate insertions. When a new key-value pair is inserted using a leaf model, if the model-predicted slot is occupied, ALEX shifts neighboring pairs to nearby gaps to maintain a strict sorted order. % as shown in Figure \ref{fig-SOTA-insert-op} (a).
Conversely, LIPP does not distinguish between non-leaf and leaf models, allowing the placement of key-value pairs, child pointers, and empty slots throughout any model in its hierarchy. During insertion, if a predicted slot is occupied, LIPP resorts to chaining, %as illustrated in Figure \ref{fig-SOTA-insert-op} (b), 
by creating a new child model containing both the existing and new pairs, subsequently replacing the existing pair with a pointer to this new child model.

To ensure high lookup and insertion performance, our proposed learned index, BLI, also supports in-place insertions through pre-allocated gaps in arrays corresponding to their leaf models. Unlike ALEX, which requires data insertion into strictly sorted linear arrays, BLI incorporates empty slots within each Bucket. Since keys within a Bucket are unsorted, BLI avoids the overhead associated with shifting operations. Moreover, while LIPP requires chaining for every occupied predicted position, BLI simply searches forward to find the next available empty slot, benefiting from the unsorted nature of Buckets.

Notably, insertions in all updatable learned indexes necessitate Structure-Modification Operations (SMOs), such as model splitting, scaling, or retraining, triggered by capacity limits or performance degradation. Efficient execution and the trigger conditions for these SMOs are crucial for maintaining system performance. Moreover, the way SMOs are implemented also affects the three aforementioned metrics affecting performance (i.e., the model fanout $N$, in-model lookup efficiency $E_{\text{lookup}}$, and in-model insert efficiency($E_{\text{insert}}$).\


\subsection{Bulk loading: top-down vs. bottom-up}
Bulk loading is a process that builds the whole learned index given a sorted array of key-value pairs. % The resulting indexes can still accommodate new insertions, growing or updating their index structures after the bulk loading phase. Bulk loading typically results in a better index layout (e.g., less number of models or smaller prediction errors) compared to a sequence of random insertions without bulk loading. This is because the models are aware of the data distribution during bulk loading. In contrast, random insertions to learned indexes need to dynamically update the models, which is often sub-optimal due to limited awareness of future insertions. % For example, once a learned model is split into two due to performance degradation, it can be hard to merge the two new models back in the future.
Bulk loading can be implemented using a top-down or bottom-up method. Given a sorted linear array of key-value pairs, top-down methods\cite{ding2020alex, wu2021updatable} start by generating the root model and then recursively branching into child models. In contrast, bottom-up bulk loading methods \cite{ferragina2020pgm, tang2020xindex, li2021finedex} first partition all key-value pairs into disjoint key ranges and train leaf models for each key range. After that, each key range is typically represented by its minimum key and a pointer to this key range. The (\texttt{min key}, \texttt{pointer}) pairs propagate upwards in the same way until a single model (i.e., the root) is generated.

Both ALEX and LIPP adopt a top-down method. To train the root model, they transverse every key-value pair in the sorted array. The traversing of the training process happens in each newly-generate sub-key range. As a result, they traverse the data multiple passes, which can typically be avoided in a bottom-up approach.

BLI employs a bottom-up approach to construct the tree. In a single pass, BLI segments key-value pairs into Buckets and then operates on the (\texttt{min key}, \texttt{pointer}) iteratively. Similar to the existing bottom-up approaches, although the upper-level models also need to transverse the (\texttt{min key}, \texttt{pointer}) pairs for training, the number of such pairs decreases exponentially after each iteration. As a result, only a single pass of data traversal is required in BLI. After bulk loading, the BLI structure has no long or skewed paths.

\subsection{Concurrency: use locking to ensure conrrectness}
Previous work has relied on locking mechanisms to support concurrency. XIndex \cite{tang2020xindex} and FINEdex \cite{li2021finedex} use temporary buffers to accommodate new key-value pairs, periodically merging the buffered data into the main index structure. XIndex applies locks to individual records within the data array and uses a per-group lock for sequential insertions. FINEdex avoids locks during reads by employing version control, but it still requires locks during insertions. SALI \cite{ge2023sali} employs per-node write locks on LIPP. The locking mechanism blocks user requests in the event of read or write conflicts. In contrast, thanks to the unsortedness of Buckets, BLI is able to support lock-free concurrency by utilizing valid bits and Read-Copy-Updates.