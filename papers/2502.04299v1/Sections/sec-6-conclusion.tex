\section{Conclusion}
\label{sec-6-conclusion}

% We present \MOCA, a novel image-to-video (I2V) synthesis system that brings cinematic shot design to I2V generation, empowering users with flexible controls over both camera and object motion. \MOCA addresses the key challenge of translating high-level user intent, expressed through intuitive 3D scene-space motion planning on the initial image, into precise 2D screen-space control signals that guide a motion-conditioned, DiT-based video diffusion model.  To this end, we develop the Motion Signal Translation module to bridge this gap by decomposing user-defined motions into hierarchical representations, resolving ambiguities and enabling accurate translation into bounding box and point trajectory control signals. This approach not only facilitates intuitive user interaction but while bypassing the need for costly 3D labeled data, relying instead on readily obtainable 2D annotations. Through comprehensive evaluations, we demonstrated \MOCA's effectiveness in generating diverse, high-quality animations that faithfully reflect user intent, highlighting its potential to significantly enhance creative workflows in filmmaking and content creation by enabling precise, scene-aware manipulation of motion in I2V synthesis.


We presented \MOCA, a unified I2V synthesis system that enables cinematic shot design with flexible control of camera and object motion. Using a Motion Signal Translation module, 
\MOCA converts intuitive 3D motion planning into precise 2D control signals for training video models without relying on 3D annotations, and hence broadens the source of training data. Comprehensive evaluations showed \MOCA's effectiveness in generating diverse, high-quality videos that faithfully reflect user motion intent.

