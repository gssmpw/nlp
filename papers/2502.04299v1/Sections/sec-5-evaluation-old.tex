
\section{Experiments}
\label{sec-5-evaluation}
\subsection{Implementation Details}
%data
\noindent\textbf{Data.} We collect $\sim$1.1M high-quality videos and extract the bounding boxes and sparse point tracking annotations using DEVA~\cite{cheng2023tracking} and RAFT~\cite{teed2020raft}. To ensure reliable motions, we filter videos using keyword-based criteria (e.g., $\texttt{vector}$, $\texttt{animation}$) and a threshold on valid tracking length. Bounding boxes are further filtered based on thresholds for adjacent-frame IoU, size change ratio, position change (Euclidean distance), and whether the object associated with the bounding box belongs to our interested moving object list. In the end, we obtained $\sim$600K videos with good motion quality and high-fidelity annotations. During training, with 80\% probability, we randomly select $N$ point trajectories, where $N \sim \mathcal{U}(0, 100)$. With 10\% probability, points are selected only from the moving object regions, and with another 10\% probability, points are selected from the non-moving object regions.

%% TODO: reason for this sampling
% model training
\noindent\textbf{Model.} Our \MOCA is fine-tuned based on a pre-trained image-to-video DiT model for 100K steps with a batch size of 256 using AdamW~\cite{loshchilov2017decoupled}, on the learning rate 1e-5 with a weight decay of 0.1. It is mainly trained on 32 and 64 frames with 12 and 24 FPS at a resolution of 640$\times$352. The results can be upscaled  to 720p using a super-resolution model. 
% TODO: model inference
During inference, we apply classifier-free guidance for text condition~\cite{ho2022classifier}.




\subsection{Camera Motion Control Quality}
To evaluate the quality of camera motion control, following~\cite{he2024cameractrl,wang2024motionctrl}, we employ rotation error (RotErr.), translation error (TransErr.), and CamMC as the metrics. Additionally, we compute Fr\'echet inception distance (FID)~\cite{heusel2017gans} to measure the quality and fidelity of the generated frames, and Fr\'echet Video Distance (FVD)~\cite{unterthiner2019fvd} to measure the temporal motion dynamics of the generated videos in both spatial and temporal domains. We compute these metrics on 1K videos randomly sampled from RealEstate-10K~\cite{zhou2018stereo} test set (@640$\times$352 with 14 frames).

We compare our method against two state-of-the-art camera-motion-controlled image-to-video methods: MotionCtrl~\cite{wang2024motionctrl} and CameraCtrl~\cite{he2024cameractrl}. The quantitative results are presented in Table~\ref{tab:quan_compare_camera}. It is worth noting that both MotionCtrl and CameraCtrl are trained on RealEstate10K train set, which provides them with a significant advantage on the in-domain test set.
Nevertheless, according to the results, our proposed method still outperforms previous approaches in all metrics under the zero-shot setting. 
%% Thanks to xxx, reason
The visual comparison is shown in Fig.~\ref{fig:eval-camera-control}, where the motion generated by MotionCtrl and CameraCtrl exhibits low quality, due to their necessity of training on video datasets (RealEstate10K) with 3D camera pose labels, which lacks diversity and contains only static estate scene. Additionally, our method supports controlling intrinsic parameters to produce more advanced cinematic shots like dolly zoom (see Fig.~\ref{fig:eval-camera-control}(right)).

%%%%%%%%%%% Figure: Evalulation -- Camera Control %%%%%%%%%%%%
\input{Figures/fig-eval-camera-control}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Tables/tab-camera-motion}

% [Analyze the results.]
% [Refer to figure \ref{fig:eval-camera-control}. Discuss the ability of our method in handling the change in intrinsic as well as extrinsic camera parameters. Compare our methods and baselines for complex camera patterns such as Dolly-Zoom and Spiraling.]



\subsection{3D-Aware Object Motion Control Quality}
% To measure the object motion control performance, we adopt the Euclidean distance between the predicted~\cite{karaev2024cotracker3} and ground-truth object central trajectories (ObjMC)~\cite{wu2025draganything}. Additionally, we employ FID to compare the generated frame quality.
Following~\cite{wu2025draganything,wang2024motionctrl}, to measure the object motion control performance, we adopt ObjMC and FID as the metrics, and compute on
VIPSeg~\cite{miao2022large} filtered val set with 116 samples by excluding videos without moving object (@640$\times$352 with 14 frames). We compare our approach with DragAnything~\cite{wu2025draganything}, MOFA-Video~\cite{niu2025mofa}, and TrackDiffusion~\cite{li2023trackdiffusion}, and report the quantitative results in Table~\ref{tab:quan_compare_object}.
Our method demonstrates significant superiority over other baselines in both object control accuracy (ObjMC) and frame quality (FID), which is further evidenced in Fig.~\ref{fig:eval-object-control}. 
%% TODO: analysis a little bit...
%Emphasize that existing baseline cannot enable 3D-aware controls.]


%%%%%%%%%%% Figure: Evalulation -- Object Motion Control %%%%%%%%%%%%
\input{Figures/fig-eval-object-control}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Tables/tab-object-global-motion}

% [Motivate the experiments. We want to evaluate whether our method can implement pure bounding-box based object motion controls without camera motion controls.]
% [Describe the evaluation protocol]
% [Describe the baseline methods]
% [Describe the evaluation metrics]

% [Analyze the results. Emphasize the fact that our method performs on par or better than others in bbox following. Also analyze the effectiveness of our bbox conditioning mechanism compared to baseline strategies.]

% [Refer to the figure \ref{fig:eval-object-control} and explain the results. Emphasize that existing baseline cannot enable 3D-aware controls.]

\subsection{Joint Camera and Object Control}
\input{Tables/tab-user-study}
%%%%%%%%%%% Figure: Evalulation -- Shot Design %%%%%%%%%%%%
\input{Figures/fig-eval-shot-design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% [Focus directly on the task of joint camera and object control in 3D-Aware manner. For this we only need to compare to MotionCtrl and our adopted version of DragNUWA]

% [For this, we need to derive an evaluation protocol with a user study. Consider two versions for each baseline: 1. The original implementation, 2. Incorporating our motion signal synthesizer module.]

% [Discuss the user study results]

% [Refer to figure \ref{fig:eval-shot-design} and discuss the results]
% Table~\ref{tab:user_study}
We conducted a user study (detailed in the supplementary) to evaluate the perceptual quality of the joint camera and object motion control on real-case images in a 3D-scene-aware manner. We compare with drag-based I2V methods DragAnything~\cite{wu2025draganything} and MOFA-Video~\cite{niu2025mofa}. In addition to the point trajectory for object local motion control, we feed point trajectory from our depth-based warping and bbox sequence to them for camera and object global motion control. The participants were asked to choose the best result in terms of motion adherence, motion quality, and frame fidelity. The statistics from the responses of 35 participants are presented in Table~\ref{tab:user_study}. Our method demonstrates significant superiority over other competitors in all aspects. The visual results in Fig.~\ref{fig:eval-shot-design} xxx


\input{Figures/fig-eval-camera-ablation}
\subsection{Ablation Studies}
\input{Tables/tab-ablation-study}
\paragraph{\textbf{Camera Motion Representation.}} We construct the following baselines to investigate the effectiveness of our camera motion representation: Gaussian map: 2D gaussian blurred sparse optical-flow map, which is widely adopted~\cite{wang2024motionctrl,wu2025draganything,mou2024revideo}, Plucker embedding~\cite{he2024cameractrl,xu2024camco,bahmani2024vd3d}, and Trajectory coefficients (Ours). The quantitative comparison is tabulated in Table~\ref{tab:ablation_study_camera}. Noted that we plucker embedding variant necessitate training on video dataset with 3D camera pose labels, we select RealEstate10K training set following~\cite{wang2024motionctrl,he2024cameractrl}. Fig.~\ref{fig:eval-camera-ablation}.

\paragraph{\textbf{Bbox Conditioning.}}
We further evaluate the effectiveness of our bbox conditioning design. We apply the closed-source conditioning design from~\cite{wangboximator} to our base model, by concatenating bbox coordinates to visual tokens. The results in the last two columns in Table~\ref{tab:quan_compare_object} demonstrate the superiority of our color-coding map design.



