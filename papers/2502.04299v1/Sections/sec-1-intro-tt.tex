

\section{Introduction}
\label{sec-1-intro}

{\bf [TT version:]} Image animation, also known as image-to-video generation (I2V), offers a powerful tool for video creation, allowing filmmakers and content creators 
to bring static images to life with dynamic motion. 
Over the years, image animation has attracted significant research efforts~\cite{horrytour, xuanimating,niklaus3d,liuinfinite, holynski2021animating}, exploring various approaches to animate still images.
Recent advances in modern video generation models have transformed the landscape of I2V 
%image-to-video 
synthesis~\cite{xing2023dynamicrafter,yang2024cogvideox}. These models utilize generative techniques 
%modeling techniques to generate 
to produce videos from textual descriptions, opening up new creative possibilities. However, the limitations of pure I2V approaches become clear when considering the user-intent dynamic content, as textual information alone falls short of 
%fails to 
capturing the intricate details of the motion, leading to uncertainty and a lack of control in generating the animations.

In content creation and filmmaking, motion design of the shot is crucial. It involves planning the camera movements and object motions to create a cohesive visual narrative. Despite its importance, current video generation models rely primarily on textual inputs for motion control, which largely limits the precision and expressiveness of the resulting animations. Some existing works~\cite{wang2024motionctrl,wu2025draganything} attempt to incorporate camera and object motion controls in video diffusion models.
However, they cannot fully address the challenges come from the intertwining nature of camera and object motions, and hence struggle to interpret the ambiguous user intention. For instance, when user drags the arm of a character in the image, it could mean panning the camera, moving the character, or just moving the arm. 

In our work, we aim at enabling the precise user control of camera motion, object global motion (moving the character), and object local motion (moving the arm). 
A straightforward approach is to prepare video training data owning all required labels (e.g. camera extrinsic parameters, object motion, etc). 
But this requires labor-intensive and possibly unreliable labeling, and eventually limits the variety and quantity of video data for training~\cite{wang2024motionctrl,he2024cameractrl}. 
Instead, we propose to represent motions that do not rely on such intensive labels, but on simple 2D bounding boxes and 2D point trajectories that can be automatically and reliably estimated. 
This allows us to take almost any kind of videos for training, and enriches our generation variety.

However, we still have one major problem left. 
Users tend to think and plan the motion in 3D scene space, but the video generation model is trained in 2D screen space. 
Moreover, users have to specify the motion plan on the 2D image canvas. 
In other words, we need an effective translation from the user motion plan in 3D scene space to control signals (bounding boxes and point trajectories) in 2D screen space. 
Based on the classical scene modeling in computer graphics, we propose the motion signal translation module that allows users to plan motion in 3D scene space, and then accurately interpret the user intention. 
This module can decompose the user motion into hierarchical transformation and rectify the potentially erroneous user input before feeding the interpreted 2D control signals to the video generation model.



%Specifically, they face two main limitations: the lack of joint modeling for camera and object motions, and the absence of 3D-aware motion conditioning signals.
%\phil{Specifically, the challenges come from (i)} the lack of joint modeling for camera and object motions, and (ii) the absence of 3D-aware motion conditioning signals.

%\phil{The next sentence is important. Make sure the sentence clearly describes this work without overly claiming something or misleading, e.g., did you relate camera movements with object motion?}
%To define our problem, we focus on the relationship between camera movement and scene-aware object motion within the temporal range of a shot. This involves representing each motion signal in a way that accurately reflects user intentions while remaining suitable for training video generation models. The key challenges include the following:

%\textbf{Designing Control Signals}: Creating control signals that effectively communicate \phil{describe? model? capture?} user intentions for both camera and object motions.

%\textbf{Generating Video with User Intentions}: Ensuring that the generated video aligns with the specified motion intentions.

%\textbf{Separating Camera and Object Motion}: Distinguishing between camera movements and object motions to achieve coherent animations \phil{natural video generation?}.

We called our streamlined video synthesis system \MOCA,
allowing for the joint manipulation of both camera and object motions in a scene-aware manner during the shot design and video synthesis processes. 
We develop dedicated motion-conditioning mechanisms that guide DiT-based~\cite{peebles2023scalable} video diffusion models to follow control signals capturing camera and object motions. 
In summary, this paper makes the following contributions.

\begin{itemize} 
%
%\item We introduce the problem of cinematic shot design in image-to-video synthesis. 
\item We introduce cinematic shot design to the process of image-to-video synthesis. 
%
\item We present \MOCA, a streamlined video synthesis system for cinematic shot design, 
%enabling 
\phil{offering} holistic motion control\phil{s} 
%that allows 
for joint manipulation of camera and object motions in a scene-aware manner. 
%
\item 
%To this end, 
Further, we design dedicated motion conditioning mechanisms to guide the DiT-based video diffusion models with 
%in following 
the control signals that capture the camera and object motions. We couple that with a Motion Signal Translation module to translate the depicted scene-space motion intent into the screen-space conditioning signal for the Video Generation module. 
%
\item Evaluations on diverse 
%range of 
real-world photos 
%input images 
demonstrate the effectiveness of \MOCA for cinematic shot design, highlighting its flexibility and potential for various creative applications. 
%
\end{itemize}
