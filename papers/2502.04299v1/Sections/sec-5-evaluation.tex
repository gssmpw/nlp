\section{Experiments}
\label{sec-5-evaluation}

\subsection{Implementation Details}

\input{Tables/tab-camera-motion}

\noindent\textbf{Data.} We collected $\sim$1.1M high-quality videos from our internal dataset. We extracted bounding boxes from the videos by performing panoptic segmentation with DEVA~\cite{cheng2023tracking}, followed by fitting the bounding boxes to the extracted masks. We compute sparse point tracking annotations by chaining optical flow from RAFT~\cite{teed2020raft}. To ensure reliable motion data, we established a threshold for valid tracking length. 
We also filtered out a subset of videos based on keywords such as $\texttt{vector}$, $\texttt{animation}$ to focus on natural video data. Bounding boxes were further refined using thresholds for adjacent-frame Intersection over Union (IoU), size change ratio, position change (Euclidean distance), and the relevance of the associated object to our list of moving objects. In the end, we obtained around 600K videos with good motion quality and high-fidelity annotations. During training, we randomly selected $N$ point trajectories with an 80\% probability, where $N \sim \mathcal{U}(0, 100)$. Additionally, there was a 10\% probability of selecting points exclusively from moving object regions, and another 10\% probability for points from non-moving object regions.

\noindent\textbf{Model.} Our video generation module is fine-tuned from a pre-trained image-to-video DiT model for 100K steps, utilizing a batch size of 256 and the AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of $1 \times 10^{-5}$ and a weight decay of 0.1. The training primarily involved videos with 32 and 64 frames, sampled at 12 and 24 FPS with a resolution of 640$\times$352. During inference, we apply classifier-free guidance for text condition~\cite{ho2022classifier}.

\input{Tables/tab-object-global-motion}

\subsection{Camera Motion Control Quality}

%To evaluate the quality of camera motion control, w
We adopt rotation error (RotErr.), translation error (TransErr.), and CamMC as metrics, following~\cite{he2024cameractrl,wang2024motionctrl}. Additionally, we compute the Fréchet Inception Distance (FID)~\cite{heusel2017gans} and the Fréchet Video Distance (FVD)~\cite{unterthiner2019fvd} to assess the quality of the generated videos. These metrics are computed on 1K videos randomly sampled from the RealEstate-10K~\cite{zhou2018stereo} test set (@640$\times$352 with 14 frames).

We compare our method with two state-of-the-art camera-motion-controlled image-to-video methods: MotionCtrl~\cite{wang2024motionctrl} and CameraCtrl~\cite{he2024cameractrl}. The quantitative results are presented in Table~\ref{tab:quan_compare_camera}. Note that both MotionCtrl and CameraCtrl were trained on the RealEstate10K training set, which contains videos within the same domain as the test set. Nevertheless, our method outperforms them across all metrics in a zero-shot setting. 

The visual comparison in Fig.~\ref{fig:eval-camera-control} illustrates that the motion generated by MotionCtrl and CameraCtrl exhibits lower quality, primarily due to their reliance on training with video datasets (RealEstate10K) that include 3D camera pose labels, which lack diversity and consist solely of static scenes. Furthermore, our method allows for the control of intrinsic parameters, enabling the production of more advanced cinematic shots, such as dolly zoom (see Fig.~\ref{fig:eval-camera-control} (right)) which is difficult to achieve with existing methods.

\input{Tables/tab-user-study}

\subsection{3D-Aware Object Motion Control Quality}

Following~\cite{wu2025draganything}, we compute ObjMC and FID on the VIPSeg~\cite{miao2022large} filtered validation set, which includes 116 samples after excluding videos without moving objects (@640$\times$352 with 14 frames). We compare with DragAnything~\cite{wu2025draganything}, MOFA-Video~\cite{niu2025mofa}, and TrackDiffusion~\cite{li2023trackdiffusion}, with the quantitative results reported in Table~\ref{tab:quan_compare_object}. 
Our method outperforms the other baselines in both control accuracy (ObjMC) and frame quality (FID), as further evidenced in Fig.~\ref{fig:eval-object-control}. The explicit warping in DragAnything and MOFA-Video introduces object distortion while the reliance on Euclidean coordinates in TrackDiffusion hinders convergency, resulting in inaccuracy. By incorporating a spatiotemporal representation for bbox, our method enables the precise object motion controls (\eg, position, size, and pose).


%%%%%%%%%%% Figure: Evaluation -- Shot Design %%%%%%%%%%%%
\input{Figures/fig-eval-shot-design}


\subsection{Joint Camera and Object Control}

We conducted a user study (detailed in section~\ref{sec:user} in the supplement) to evaluate the perceptual quality of joint camera and object motion control in a 3D-scene-aware context. We compared our method with drag-based I2V methods: DragAnything~\cite{wu2025draganything} and MOFA-Video~\cite{niu2025mofa}. 
Note that none of the existing methods are designed for 3D-aware control, thus we directly take scene-space point trajectories as input to the baselines, following their original setting.
In addition to the point trajectory for object local motion control, we provided point trajectories from bounding box sequences and depth-based warping for global motion control of both object and camera. 
Participants were asked to select the best result based on motion adherence, motion quality, and frame fidelity. The statistics from the responses of 35 participants are summarized in Table~\ref{tab:user_study}. Our method consistently outperformed the competitors across all evaluated aspects. The visual results are presented in Fig.~\ref{fig:eval-shot-design}, where both baselines fail to jointly capture complex object global motion (\ie, the body's movement), local motion (\ie, putting down hands), and camera motion in a 3D-aware manner. In contrast, our \MOCA generates motions following all types of controls, thanks to its unified framework and design of motion representations.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ablation Studies}

\input{Tables/tab-ablation-study}
\input{Figures/fig-eval-camera-ablation}
\textbf{Camera Motion Representation.} We constructed several baselines to investigate the effectiveness of our camera motion representation: a Gaussian map (a 2D Gaussian blurred sparse optical-flow map), Plucker embedding~\cite{he2024cameractrl,xu2024camco,bahmani2024vd3d}, and our proposed DCT-coefficient-based trajectory encoding. The quantitative comparison is presented in Table~\ref{tab:ablation_study_camera}. The Gaussian map variant is inferior in accurate camera control due to its inherent ambiguity (especially under more dense controls), tending to generate static camera motions (high FVD). It is noteworthy that the Plucker embedding variant requires training on a video dataset with 3D camera pose labels, (\ie, RealEstate10K training set, following~\cite{he2024cameractrl}). It performs well on this in-domain static test set, but fails to generate object motions (Fig.~\ref{fig:eval-camera-ablation} `cat') and lacks generalizability. In addition, our trajectory coding is highly efficient, introducing only a few coefficient tokens while delivering robust performance for camera intrinsic and extrinsic controls.

\noindent\textbf{Bounding Box Conditioning.} We further evaluated our bounding-box conditioning. We applied an alternative conditioning design proposed in~\cite{wang2024boximator} that concatenates bounding-box coordinates to visual tokens (Ours$_\text{coord}$). The results in the last two columns of Table~\ref{tab:quan_compare_object} demonstrate the superiority of our spatiotemporal color-coding map conditioning. The difficulty of fusing Euclidean coordinate tokens with visual tokens leads to low ObjMC.