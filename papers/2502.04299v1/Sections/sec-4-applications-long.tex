\section{Applications}
\label{sec-4-applications}

\MOCA provides a solution for flexible controls of both camera and object motion in a scene-aware manner. This enables our main application of cinematic shot design framework, allowing users to interactively manage key motion aspects of a shot. 
Additionally, the flexibility of our motion representations makes it possible to naturally apply our framework on a variety of simple video-based editing tasks, including motion transfer and video-to-video transforms such as localized edit propagation (e.g., adding, removing, or replacing elements) and global appearance transform (e.g., style modifications).

\input{Figures/fig-app-shot-design}
\subsection{Shot Design with Joint Camera and Object Control}

Our shot design pipeline begins with an input image, where users specify 3D camera motion and object motion, and aligning them to a timeline. These user-defined intentions are transformed into screen-space conditioning signals, which are input into the video generation model. As illustrated in Fig.~\ref{fig:app-shot-design}, our framework enables precise and independent control of object and camera motion in a scene-aware manner, allowing for the design of highly dynamic and visually compelling shots while closely following the provided motion design. 

In Fig.~\ref{fig:app-shot-design}, it's worth noting that in both examples the results in each column follow the same camera motion while the object motions change according to the corresponding specified object controls. By placing the bounding box in a scene-aware manner, users can achieve various scene-space effect. For example, this makes it possible to induce the car in the bottom example to stay still (first row) or moving forward (second row) and backward (third row) on the road. Importantly, such scene-anchored motion is preserve while the camera motion change independently. This highlights the importance of scene-aware object motion controls.

\input{Figures/fig-app-shot-design-longvid}
\textbf{Long Videos with Complex Trajectories.}
To generate extended videos featuring complex camera and object motions, \MOCA employs a ``specification-generation'' loop. This approach allows users to define motion signals for each segment and subsequently generate video chunks in an auto-regressive manner. Inspired by animation workflows~\cite{xing2024tooncrafter,tang2025generativeaicelanimationsurvey}, \MOCA combines keyframing and interpolation to create intricate motion paths. Specifically, users can set keyframes for object and camera motions, and the system interpolates between these keyframes to produce smooth and coherent trajectories. 

As demonstrated in Fig.~\ref{fig:app-shot-design-longvid}, our method can generate long videos with complex sequence of camera motion controls. We showed two video results for each input image, resulting from the same camera controls (note that almost identical camera motion were generated for the two results) while intentionally using different object global motion control.

\input{Figures/fig-app-shot-design-local-motion}
\subsection{Object Local Motion Control.}

\MOCA also enables controlling local object motion to potentially support drag-based editing and generation. 
Users can define local object motions by directly specifying drag-based trajectories within the object's own coordinates. These point trajectories are then transformed to the appropriate screen-space point trajectories for conditioning the video generation model, taking into account both the camera and the object global motion. 
As illustrated in Fig.~\ref{fig:app-shot-design-local-motion}, our method can generate diverse and fine-grained local motions, making it possible to generate different variations of the object movements (\eg the different ways the arms of the baby move)

In addition, thanks to our dedicated motion translation module that accounts for the coordination between local motion and camera motion as well as global object motion,
consistent local object motion control can be achieved with different camera and object dynamics (bottom row of Fig.~\ref{fig:app-shot-design-local-motion}). This opens the possibility of incorporating local object motion control into the shot design framework described above.
%Our framework extends previous drag-based editing and generation approaches by incorporating local object motion control in a 3D-aware manner, surpassing the capabilities of prior works~\cite{pan2023drag,niu2025mofa}. While traditional drag-based methods permit manipulation of object trajectories in 2D space, our approach supports both drag-based and 3D-aware local object motion control, enabling users to create more sophisticated and spatially consistent motions.

\input{Figures/fig-additional-apps}
\subsection{Additional Applications: Simple Video-based Editing Tasks}

\textbf{Motion Transfer.}
Our method can be adapted to perform motion transfer from an input video to an input image that shares a structural similarity with the initial frames of the reference video. By leveraging the versatile screen-space conditioning representation, our framework effectively captures and transfers both object and camera motions, even for cases that involves 3D transform, without the need for explicit 3D camera pose extraction from the reference video. As shown in Fig.~\ref{fig:additional-apps}, a rotating movement of an apple was transferred to rotate the lion's head. 

\textbf{Video Editing.}
The concept of motion transfer can be extended to facilitate video editing, where the input image is derived from the first frame of the video through image editing~\cite{brooks2023instructpix2pix}. Utilizing the versatile screen-space conditioning representation, our method propagates extracted object and camera motions to the derived image, ensuring consistent and realistic dynamics, similar to~\cite{liu2024generative}. Fig.~\ref{fig:additional-apps} shows two examples where the edits performed on the initial frame of a video is propagated to results in a fully edited video, using the motion signals extracted from the original videos. 