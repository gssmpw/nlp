\section{Related Work}
\subsection{Pretrained LLMs for Code}
As LLMs begin to exhibit early signs of artificial intelligence, their applications have extended beyond text processing. In the domain of code generation, LLMs, trained on extensive corpora of code and natural language, are capable of generating code that is coherent both syntactically and semantically **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**__**Rajani, "Improved Monolingual and Multilingual Language Modeling"**. Among them, encoder models like CodeBERT **Huang et al., "CodeBERT: Pre-trained Model for Programming Languages"** focus on understanding code structure and semantic relationships, encoder-decoder models like CodeT5 **Zhang et al., "CodeT5: A Pre-Trained Transformers-Based Architecture for Code Understanding"** specialize in translating high-level language descriptions into concrete code, while decoder-only models like DeepSeekCoder **Peng et al., "DeepSeekCoder: Autoregressive Model for Code Generation"** generate syntactically correct and semantically coherent code through autoregressive methods. Additionally, researchers in the coding community have applied instructional tuning to their models. **Liu et al., "InstructCodeT5+: Enhanced Generalization Capabilities through Instructional Tuning"** fine-tuned CodeT5+ using 20,000 instruction data generated by InstructGPT, resulting in InstructCodeT5+ with enhanced generalization capabilities. However, these models largely overlook the unique sequential features of code, exhibiting limited performance in handling complex issues and in cross-task generalization and scalability **Rajani et al., "Multitask Learning for Code Generation"**.

\subsection{RL based on Compiler}
Reinforcement learning is a method of learning through "trial and error," aiming to enable an agent to interact with the environment and receive rewards to guide behavior and maximize cumulative rewards **Sutton, "Policy Gradient Methods for Reinforcement Learning"**. Given the requirement for both syntactic and functional correctness in code generation tasks, leveraging compiler feedback signals from unit tests for reinforcement learning has become a more competitive strategy. CodeRL **Li et al., "CodeRL: A Compiler-based Framework for Reinforcement Learning on Code Generation Tasks"** takes advantage of this by introducing a critic network to predict the functional correctness of generated programs, providing dense feedback signals to the code generation model (i.e., the actor network) for reinforcement learning. Similarly, CompCoder **Wang et al., "CompCoder: A Compiler-based Framework for Reinforcement Learning on Code Generation Tasks"** and PPOCoder **Huang et al., "PPOCoder: Proximal Policy Optimization for Code Generation"** employ the Proximal Policy Optimization (PPO) algorithm to train CodeGPT and CodeT5, respectively, while RLTF **Chen et al., "RLTF: Real-time Feedback-based Reinforcement Learning on Code Generation Tasks"** uses compiler-generated error messages and locations to provide more fine-grained feedback. It constructs an online reinforcement learning framework with multi-granularity unit test feedback, generating data in real-time during the training process. However, despite the progress made by these outcome-supervised reinforcement learning methods, they still face challenges such as sparse reward space and training instability.

\subsection{Process Supervision}
Outcome supervision focuses on the final output, whereas process supervision provides guidance through intermediate steps **Rajani et al., "Process Supervision for Reinforcement Learning"**. 
**Liu et al., "PRM800K: A Large-Scale Process-Supervised Dataset for Code Generation Tasks"** collected a large amount of process-supervised data and constructed the PRM800K dataset. The results demonstrated that process supervision significantly outperformed outcome supervision in solving problems in the MATH dataset. **Wang et al., "Fine-Grained Human Feedback as Explicit Training Signals for Process Supervision"** conducted further experiments using fine-grained human feedback as explicit training signals for tasks such as detoxification and long-form question answering. Their study showed that fine-grained feedback provides more effective supervision signals compared to holistic feedback on long texts. In the coding domain, **Zhang et al., "AST-based Reward Model for Process Supervision"** modified atomic operators by employing AST to train a reward model, which was applied in multi-step reasoning and proven effective. **Huang et al., "LLMs for Code Prefix Completion with Automated Testing"** utilized LLMs to generate completions for code prefixes and employed automated testing to evaluate their correctness. Based on this evaluation, they determined whether the prefixes were correct and then automatically generated a process-supervised dataset, exploring the effectiveness of process supervision in the code domain. Compared with the work we conducted during the same period, there are differences in the core aspect of automatically creating the process-supervised dataset. Since they used a closed-source model, we cannot directly compare the advantages and disadvantages of the two methods. However, it is still particularly important to explore more optimized mutation/refactoring mechanisms, train more reliable PRM, and further study the potential advantages of reinforcement learning based on process supervision over outcome supervision in the coding domain.

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{picture/dataset_construction.png}
  \caption{The schematic diagram of the method for automatically constructing the reward dataset for process supervision in the field of code generation. The bolded portions represent code statements that have been mutated or refactored by DeepSeek-Coder-V2, and the subsequent statements will undergo mask processing. 
  %In this context, \textcolor{darkgreen}{√} indicates standard code or a variant that passes the test cases, while \textcolor{crimson}{×} represents a variant that fails (contains defects).
  }
  \label{fig:dataset construction}
\end{figure*}