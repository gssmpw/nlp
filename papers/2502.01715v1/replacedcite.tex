\section{Related Work}
\subsection{Pretrained LLMs for Code}
As LLMs begin to exhibit early signs of artificial intelligence, their applications have extended beyond text processing. In the domain of code generation, LLMs, trained on extensive corpora of code and natural language, are capable of generating code that is coherent both syntactically and semantically ____. Among them, encoder models like CodeBERT ____ focus on understanding code structure and semantic relationships, encoder-decoder models like CodeT5 ____ specialize in translating high-level language descriptions into concrete code, while decoder-only models like DeepSeekCoder ____ generate syntactically correct and semantically coherent code through autoregressive methods. Additionally, researchers in the coding community have applied instructional tuning to their models. ____ fine-tuned CodeT5+ using 20,000 instruction data generated by InstructGPT, resulting in InstructCodeT5+ with enhanced generalization capabilities. However, these models largely overlook the unique sequential features of code, exhibiting limited performance in handling complex issues and in cross-task generalization and scalability ____.

\subsection{RL based on Compiler}
Reinforcement learning is a method of learning through "trial and error," aiming to enable an agent to interact with the environment and receive rewards to guide behavior and maximize cumulative rewards ____. Given the requirement for both syntactic and functional correctness in code generation tasks, leveraging compiler feedback signals from unit tests for reinforcement learning has become a more competitive strategy. CodeRL ____ takes advantage of this by introducing a critic network to predict the functional correctness of generated programs, providing dense feedback signals to the code generation model (i.e., the actor network) for reinforcement learning. Similarly, CompCoder ____ and PPOCoder ____ employ the Proximal Policy Optimization (PPO) algorithm to train CodeGPT and CodeT5, respectively, while RLTF ____ uses compiler-generated error messages and locations to provide more fine-grained feedback. It constructs an online reinforcement learning framework with multi-granularity unit test feedback, generating data in real-time during the training process. However, despite the progress made by these outcome-supervised reinforcement learning methods, they still face challenges such as sparse reward space and training instability.

\subsection{Process Supervision}
Outcome supervision focuses on the final output, whereas process supervision provides guidance through intermediate steps ____. 
 ____ collected a large amount of process-supervised data and constructed the PRM800K dataset. The results demonstrated that process supervision significantly outperformed outcome supervision in solving problems in the MATH dataset. ____ conducted further experiments using fine-grained human feedback as explicit training signals for tasks such as detoxification and long-form question answering. Their study showed that fine-grained feedback provides more effective supervision signals compared to holistic feedback on long texts. In the coding domain, ____ modified atomic operators by employing AST to train a reward model, which was applied in multi-step reasoning and proven effective. ____ utilized LLMs to generate completions for code prefixes and employed automated testing to evaluate their correctness. Based on this evaluation, they determined whether the prefixes were correct and then automatically generated a process-supervised dataset, exploring the effectiveness of process supervision in the code domain. Compared with the work we conducted during the same period, there are differences in the core aspect of automatically creating the process-supervised dataset. Since they used a closed-source model, we cannot directly compare the advantages and disadvantages of the two methods. However, it is still particularly important to explore more optimized mutation/refactoring mechanisms, train more reliable PRM, and further study the potential advantages of reinforcement learning based on process supervision over outcome supervision in the coding domain. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{picture/dataset_construction.png}
  \caption{The schematic diagram of the method for automatically constructing the reward dataset for process supervision in the field of code generation. The bolded portions represent code statements that have been mutated or refactored by DeepSeek-Coder-V2, and the subsequent statements will undergo mask processing. 
  %In this context, \textcolor{darkgreen}{√} indicates standard code or a variant that passes the test cases, while \textcolor{crimson}{×} represents a variant that fails (contains defects).
  }
  \label{fig:dataset construction}
\end{figure*}