[
  {
    "index": 0,
    "papers": [
      {
        "key": "jiang2024survey",
        "author": "Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun",
        "title": "A Survey on Large Language Models for Code Generation"
      },
      {
        "key": "guo2020graphcodebert",
        "author": "Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others",
        "title": "Graphcodebert: Pre-training code representations with data flow"
      },
      {
        "key": "li2022competition",
        "author": "Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others",
        "title": "Competition-level code generation with alphacode"
      },
      {
        "key": "nijkamp2022codegen",
        "author": "Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming",
        "title": "Codegen: An open large language model for code with multi-turn program synthesis"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "feng2020codebert",
        "author": "Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others",
        "title": "Codebert: A pre-trained model for programming and natural languages"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wang2021codet5",
        "author": "Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH",
        "title": "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "guo2024deepseek",
        "author": "Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2023codet5+",
        "author": "Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi DQ and Li, Junnan and Hoi, Steven CH",
        "title": "Codet5+: Open code large language models for code understanding and generation"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2024deep",
        "author": "Huangzhao Zhang and Kechi Zhang and Zhuo Li and Jia Li and Yongmin Li and Yunfei Zhao and Yuqi Zhu and Fang Liu and Ge Li and Zhi Jin",
        "title": "Deep learning for code generation: a survey"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "mnih2013playing",
        "author": "Mnih, Volodymyr",
        "title": "Playing atari with deep reinforcement learning"
      },
      {
        "key": "mnih2015human",
        "author": "Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "key": "van2016deep",
        "author": "Van Hasselt, Hado and Guez, Arthur and Silver, David",
        "title": "Deep reinforcement learning with double q-learning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "le2022coderl",
        "author": "Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven Chu Hong",
        "title": "Coderl: Mastering code generation through pretrained models and deep reinforcement learning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2022compilable",
        "author": "Wang, Xin and Wang, Yasheng and Wan, Yao and Mi, Fei and Li, Yitong and Zhou, Pingyi and Liu, Jin and Wu, Hao and Jiang, Xin and Liu, Qun",
        "title": "Compilable neural code generation with compiler feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shojaee2023execution",
        "author": "Shojaee, Parshin and Jain, Aneesh and Tipirneni, Sindhu and Reddy, Chandan K",
        "title": "Execution-based code generation using deep reinforcement learning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2023rltf",
        "author": "Liu, Jiate and Zhu, Yiqin and Xiao, Kaiwen and Fu, Qiang and Han, Xiao and Yang, Wei and Ye, Deheng",
        "title": "Rltf: Reinforcement learning from unit test feedback"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "uesato2022solving",
        "author": "Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina",
        "title": "Solving math word problems with process-and outcome-based feedback"
      },
      {
        "key": "luo2024improve",
        "author": "Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others",
        "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision"
      },
      {
        "key": "wang2024math",
        "author": "Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang",
        "title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wu2024fine",
        "author": "Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh",
        "title": "Fine-grained human feedback gives better rewards for language model training"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "ma2023let",
        "author": "Ma, Qianli and Zhou, Haotian and Liu, Tingkai and Yuan, Jianbo and Liu, Pengfei and You, Yang and Yang, Hongxia",
        "title": "Let's reward step by step: Step-Level reward model as the Navigators for Reasoning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dai2024process",
        "author": "Dai, Ning and Wu, Zheng and Zheng, Renjie and Wei, Ziyun and Shi, Wenlei and Jin, Xing and Liu, Guanlin and Dun, Chen and Huang, Liang and Yan, Lin",
        "title": "Process supervision-guided policy optimization for code generation"
      }
    ]
  }
]