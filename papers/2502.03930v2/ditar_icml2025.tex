%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{graphicx} % 用于包含图像
\usepackage{subcaption} % 用于创建子图形
\usepackage{bm}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation}

\begin{document}

\twocolumn[
%\icmltitle{DiTAR: Bridging Autoregression and Diffusion for Continuous Speech Synthesis}

\icmltitle{DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation}
%\icmltitle{DiTAR: Continuous Speech Synthesis with Diffusion Transformer Autoregressive Modeling}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dongya Jia}{yyy}
\icmlauthor{Zhuo Chen}{yyy}
\icmlauthor{Jiawei Chen}{yyy}
\icmlauthor{Chenpeng Du}{yyy}
\icmlauthor{Jian Wu}{yyy}
\icmlauthor{Jian Cong}{yyy}
\icmlauthor{Xiaobin Zhuang}{yyy}
\icmlauthor{Chumin Li}{yyy}
\icmlauthor{Zhen Wei}{yyy}
\icmlauthor{Yuping Wang}{yyy}
\icmlauthor{Yuxuan Wang}{yyy}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{ByteDance Seed}
\icmlcorrespondingauthor{Dongya Jia}{jiadongya@bytedance.com}
\icmlcorrespondingauthor{Zhuo Chen}{zhuo.chen1@bytedance.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes.
In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands.
DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model.
For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.

\end{abstract}

\section{Introduction}
\label{introduction}

Recently, the autoregressive language model (LM) has demonstrated strong generative capabilities and scaling properties\cite{achiam2023gpt,touvron2023llama,yang2024qwen2,team2023gemini}, where discrete tokenization is commonly used. While discretization is natural for text, it is not as straightforward for other modalities, such as images, video, and audio.
Due to bitrate limitations, discrete representation often fails to reconstruct complex modalities with high fidelity. 
In comparison, continuous tokens can better reconstruct the original information\cite{fan2024fluid,rombach2022high}. To alleviate this challenge, a two-stage strategy is commonly applied for autoregressive multi-media generation, where a lossy discrete token is first generated by an LM, followed by a token-based diffusion for detail enrichment. However, such a cascaded design suffers from error accumulation and limits the scalability of the language model. 
Therefore, autoregressive modeling of continuous representations is meaningful.

The diffusion model has been proven effective at modeling continuous representations\cite{peebles2023scalable,rombach2022high,anastassiou2024seed}, and integrating it with autoregressive models is a major research trend. 
\cite{li2024autoregressive}  proposes incorporating a diffusion head into a language model for image generation, pioneering a new approach. However, the performance significantly falls short when a causal-attention language model forms the backbone.
Another approach, such as ARDiT\cite{liu2024autoregressive} or Transfusion\cite{zhou2024transfusion}, repurposes the language model’s parameters for diffusion, leading to substantial computational demands.
This raises the question of whether we can autoregressively predict continuous tokens while ensuring high-quality generation and maintaining reasonable computational demands.




To tackle this challenge, we introduce the Diffusion Transformer AutoRegressive (DiTAR) modeling, a framework that seamlessly combines transformer diffusion with a language model.
We attribute the subpar performance of a language model with a diffusion head to the unidirectional dependency imposed by causal attention, which conflicts with the close inter-frame correlations characteristic of continuous tokens.
In DiTAR, we propose a divide-and-conquer strategy that breaks continuous tokens into multiple patches. A language model is responsible for inter-patch prediction, while a diffusion transformer handles intra-patch prediction. 
Specifically, a diffusion transformer with bidirectional attention (DiT), noted for its state-of-the-art performance across various generative fields\cite{liu2024sora,peebles2023scalable,anastassiou2024seed}, is employed to predict localized patches, named LocDiT. Furthermore, we propose a broadly applicable guidance method and introduce historical contexts to LocDiT to further enhance its generative capabilities.
After patchification, the causal language model handling long contexts receives shorter sequences, further reducing computational load.







For inference, temperature-based sampling balances exploration and exploitation and is crucial in discrete-valued language models. However, Its application in continuous-valued LMs remains underexplored.
We define temperature as the point at which noise is introduced along the reverse ODE trajectory and propose a temperature-based sampling method for the continuous-valued AR model. Distinct from previous SDE-based approaches\cite{dhariwal2021diffusion,li2024autoregressive} that require numerous steps, our method suits quicker ODE solvers and adeptly balances diversity with determinism.

DiTAR, rooted in a language model, excels at zero-shot generation tasks.
We apply DiTAR to zero-shot text-to-speech (TTS) that aims to generate speech from unseen speakers' prompts. 
Unlike systems\cite{chen2024vall,chen2025neural,anastassiou2024seed,du2024cosyvoice} using a coarse-to-fine pipeline, DiTAR simplifies the process by having the language model directly predict final features, obviating the need for multiple stages. It achieves state-of-the-art results in robustness, speaker similarity, and naturalness while demanding far less computational power than competing models.

In summary, our contributions to the community include:
\begin{itemize}
\item We introduce DiTAR, a patch-based autoregressive framework that seamlessly blends LM and DiT, maintaining their strengths while offering superb generative capabilities and reduced computational demands.
\item In inference, we propose a new definition of temperature and a fast temperature-based sampling method tailored for autoregressive models with diffusion loss.
\item We apply DiTAR to zero-shot speech generation and achieve SOTA performance with a much lower computational load. 
\end{itemize}


\section{Related Works}




\textbf{Integrating Autoregressive Language Model and Diffusion.} 
Language models are primarily used for discrete representations, while diffusion excels in modeling continuous distributions. Integrating them for multimodal modeling is a crucial research direction.
Some efforts \cite{wu2023ar,liu2024autoregressive,chen2024diffusion} enable diffusion to have autoregressive capabilities by varying the denoising rates between consecutive tokens to achieve earlier predictions for preceding tokens.
Transfusion\cite{zhou2024transfusion} utilizes a shared transformer for both diffusion and language models, employing causal attention for discrete tokens and bidirectional attention for continuous tokens. However, it does not naturally support the autoregressive generation of continuous tokens.
These approaches repurpose language model parameters for diffusion, which significantly increases computational demands as the sequence lengthens and the language model size grows.
Most relevant to our work, \citet{li2024autoregressive} proposes a diffusion head for next-token prediction. However, its application in causal language models results in relatively poor performance.

\textbf{Patchification in Generative Modeling.}
In speech\cite{wang2017tacotron,meng2024autoregressive,chen2024vall}, image\cite{peebles2023scalable,li2024imagefolder}, and video generation\cite{liu2024sora}, the patchification technique is widely applied. In these works, patchification primarily aims to reduce the computational load by shortening the sequence length. However, in this paper, patchification not only lowers computational demands but also enables bidirectional modeling on patches within our autoregressive framework, further improving modeling effectiveness.


\textbf{Zero-Shot Text-to-Speech.} Zero-shot TTS aims to generate speech with prompts from unseen speakers.
Existing works can be divided into two categories: multi-stage and single-stage.
Multi-stage approaches\cite{chen2024vall,chen2025neural,lee2023hierspeech++,jiang2023mega,jiang2023boosting,anastassiou2024seed}, represent speech using various representations, primarily divided into coarse and fine categories. 
The autoregressive language model is often adopted to predict the coarse representations, usually in discrete value and low information, such as semantics\cite{kharitonov2023speak,anastassiou2024seed,hsu2021hubert,chung2021w2v,baevski2020wav2vec} and prosody\cite{jiang2023mega,jiang2023boosting,ju2024naturalspeech}. And then another model is adopted to conduct coarse-to-fine. 
Single-stage methods focus on generating high-information continuous representations, such as Mel spectrograms or latents of auto-encoders, which can directly reconstruct audio waveforms.
Diffusion based on the non-causal attention transformer is the primary method employed\cite{chen2024f5,eskimez2024e2,gao2023e3,le2024voicebox}.
Additionally, some approaches\cite{meng2024autoregressive,wang2017tacotron,shen2018natural,li2019neural} directly use AR to model Mel-spectrograms of speech but often involve using Dropout\cite{srivastava2014dropout} at the input stage to guarantee generation robustness, resulting in weaker in-context learning capabilities. These methods are better suited for in-set speaker TTS.
This paper introduces a single-stage autoregressive method for zero-shot TTS achieving SOTA generation robustness and voice similarity.


\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{framework.pdf}
  \caption{DiTAR is composed of an aggregation encoder for input, a causal language model backbone, and a diffusion decoder, LocDiT, predicting local patches of tokens.}
  \label{fig:ditar}
  \vspace{-0.3cm}
\end{figure*}


\section{Method}

In a nutshell, We propose DiTAR, a patch-based autoregressive system based on continuous representation. This system amalgamates the strengths of the causal-attention AR and bidirectional-attention transformer diffusion.  
%We deploy it to zero-shot text-to-speech tasks in a simple yet effective manner, achieving state-of-the-art performance.

\subsection{Overview}

\subsubsection{Formulation}
DiTAR is an autoregressive model via next-token prediction. Consider a sequence of continuous tokens $\bm{x}=(\bm{x}_1, \bm{x}_2, ..., \bm{x}_N)$, we can factorize the joint distribution of the sequence by the chain rule of probability: 
\begin{equation}
    p_{\theta} \lparen \bm{x}_1, \bm{x}_2, ...,\bm{x}_N\rparen = 
    \prod_{i=1}^{N}p_{\theta} \lparen \bm{x}_{i}|\bm{x}_1, \bm{x}_2, ...,\bm{x}_{i-1}\rparen
\end{equation}
where $\theta$ denotes the parameters of an AR generative model. Noting the high similarity among adjacent continuous tokens, it is evident that a bidirectional dependency exists within local regions. Based on this discovery, we aggregate local $\bm{x}_i$ into patches with a size of $P$, and then employ bidirectional attention to model the tokens inside each patch. We can divide the model into two parts, $\theta_a$ and $\theta_b$: $\theta_a$ denotes the autoregressive model responsible for long context learning via $p_{\theta_a}(\bm{h}_i|\bm{x}_1,\bm{x}_2,...,\bm{x}_{i})$, while $\theta_b$ denotes a bidirectional-attention diffusion transformer executing next-patch prediction via $p_{\theta_b}(\bm{x}_{i+1},...,\bm{x}_{i+P}|\bm{h}_i)$, where $\bm{h}_i$ is the output of language model and condition for diffusion. 

We validate the effectiveness of DiTAR on the zero-shot text-to-speech task. Based on the formulation, we regard zero-shot TTS as a conditional continuation task for the AR model like\cite{chen2024vall}, where prompting texts, target text, and prompting speech are concatenated and fed into the model as prefix context, then the model autoregressively generates the target speech given the context.


\subsubsection{Overall architecture} 

\citet{li2024autoregressive} finds that the performance of the causal-attention autoregressive model combined with diffusion loss is significantly inferior to systems utilizing full attention.
To address the issue, we propose a divide-and-conquer strategy, where a long sequence of continuous tokens is divided into multiple patches. A language model is responsible for inter-patch prediction, while a diffusion transformer handles intra-patch prediction. 
As shown in Figure \ref{fig:ditar}, the backbone of our system is a causal-attention transformer with next-token prediction. Each patch of continuous tokens is processed with an aggregation encoder into a single vector, which is then fed into the AR model to get the output embedding $\bm{h}_t$. $\bm{h}_t$ serves as the condition of the following diffusion decoder, LocDiT. Following \cite{li2024autoregressive}, a diffusion loss is used for the output continuous tokens at training time.


\subsection{LocDiT: Next-Patch Bidirectional Modeling}

The diffusion transformer\cite{peebles2023scalable,liu2024sora} has achieved success across numerous generative fields. These approaches leverage the full receptive field of the bidirectional transformer to generate entire samples. In our work, we propose using a bidirectional transformer diffusion, called Local Diffusion Transformer (LocDiT), to generate localized continuous token patches. 

LocDiT generates the next patch of speech given the AR's output. However, we have found that diffusion struggles to predict the next patch under these conditions. To capitalize on the context-learning potential of the diffusion transformer, we propose a context-aware diffusion approach. Specifically, as illustrated in the right half of Fig \ref{fig:ditar}, historical patches of tokens are utilized as prefix inputs for the LocDiT, thereby aligning the task more closely with outpainting and significantly improving the generation performance. Detailed quantitative results are discussed in \cref{ablation}.

Furthermore, the design also implies an inherent coarse-to-fine manner. Current approaches\cite{anastassiou2024seed,du2024cosyvoice,chen2024vall} commonly explicitly delineate both coarse and fine features. A language model typically predicts the coarse feature, while another network achieves the transition from coarse to fine. 
However, multi-stage methods are prone to cumulative errors. In contrast, DiTAR functions in a seamless end-to-end manner, condensing each patch of tokens into an implicit lower-dimensional feature space that LocDiT subsequently expands into high-fidelity continuous tokens.


\subsection{LM Guidance}

Classifier-free guidance (CFG)\cite{ho2022classifier} is extensively employed to enhance the condition adherence of generative models. In diffusion, the unconditional and conditional models share parameters and are jointly trained by intermittently omitting the condition during training. At inference, the outputs from these two models are merged with a parameter $w$ balancing the trade-off between diversity and fidelity. This is equivalent to sampling under such a distribution $\tilde{p}_\theta\left(\bm{z}_t, c\right) \propto p_\theta(\bm{z}_{t}|c)p_\theta(c|\bm{z}_t)^{w}$, where $\theta$ denotes the model parameters, $c$ denotes the condition and $\bm{z}_t$ denotes the noisy sample at the time of $t$.

In the continuous-token LMs for conditional image generation, \citet{li2024autoregressive} incorporates the class label into the diffusion head and applies CFG which significantly enhances performance.
However, such an approach lacks generality. In DiTAR, we propose a more generalized approach: we place all conditions within the LM prefix inputs and only apply CFG to the diffusion decoder via the AR's on-the-fly output.
The $i$th output $\bm{h}_i$ of the AR essentially represents all historical inputs $(\bm{x}_0,\bm{x}_1,...,\bm{x}_{i})$.
Specifically, we randomly replace the $\bm{h}_i$ with a null embedding along the sequence in training. At inference time, we samples from the distribution $p_\theta(\bm{z}_{i,t}|\bm{x}_1,...,\bm{x}_{i-1})p_\theta(\bm{x}_1,...,\bm{x}_{i-1}|\bm{z}_{i,t})^{w}$ by:

\begin{equation}
    \tilde{\pmb{\epsilon}}_\theta(\pmb{z}_{i,t}, \pmb{h}_i)=(1+w)\pmb{\epsilon}_{\theta}(\pmb{z}_{i,t},\pmb{h}_i)-w\pmb{\epsilon}_\theta(\pmb{z}_{i,t})
\end{equation}

where $z_{i,t}$ denotes the $i$th noisy sample in the sequence at the time of $t$ for diffusion, $\epsilon_{\theta}$ denotes the score estimated by the LocDiT. Operating in the velocity space with a conditional flow-matching target is also equivalent. We show that the approach significantly enhances the model's performance in \cref{ablation}.

%Additionally, the proposed method has wide applicability as the diffusion decoder operates independently of specific conditions, mirroring the output layer of the discrete-valued AR used in LLMs. This characteristic enables continuous-valued AR to undertake unsupervised autoregressive learning akin to discrete-valued ARs.


\subsection{Temperature for Continuous-Valued LMs}
Temperature-based sampling, which strikes a balance between exploration and exploitation, is fundamental in discrete-valued LMs. However, its application in continuous-valued LMs has been less extensively studied.
\citet{li2024autoregressive} proposes a sampling method based on the DDPM reversed process\cite{ho2020denoising}, which is a first-order SDE solver, where the temperature is defined as the noise scaling factor at each step. 
Due to the stochastic nature of the Wiener process, SDE solvers require small step sizes,  necessitating a higher number of steps for convergence\cite{song2020score}. 
Additionally, this concept of temperature cannot be readily extended to the more widely used and faster ODE solvers prevalent in contemporary applications\cite{lu2022dpm,lu2022dpm+,song2020denoising}.
To address these issues, we propose a new sampling method with a new definition of temperature, which is compatible with the ODE solvers. 

We define the temperature $\tau \in [0,1]$ as \emph{the time point to introduce noise} while solving the reverse ODE of diffusion. 
Specifically, 
consider a Gaussian diffusion forward process defined on per-patch by $\bm{x}_t=\alpha_t \bm{x}_0+\sigma_t\bm{\varepsilon}$, where $\bm{x}_0\sim q_{data}(\bm{x}_0)$, $\bm{\varepsilon} \sim \mathcal{N}(\bm{0},\bm{I})$, $t \in [0,1]$, $\alpha_t$ and $\sigma_t$ collectively defines the flow path.
At $\tau=1$, the sampling process is equivalent to the standard ODE sampling process, where we solve the reverse ODE $d{\bm{x}_t}=\bm{v}_{\theta}(\bm{x}_t,t)dt$ with respect to $dt$ from $1$ to $0$, and $\bm{x}_1\sim \mathcal{N}(\bm{0},\bm{I})$.
At $\tau=0$, no random noise is introduced leading to a completely deterministic process.
Considering that $0$ is the value with the highest likelihood in standard Gaussian distribution, we define the greedy sampling by sampling from $x_1 \equiv \bf{0}$, thus ensuring determinism. 

When $0<\tau <1$, we introduce random noise at $\tau$ by using the forward process to diffuse an estimated $\bm{x}_0$.
Eqn \ref{eq:x1} and \ref{eq:xt} summarize the iterative process, where the Euler method is used as the default ODE solver for illustration.


\begin{align} 
        \bm{x}_1 &\sim \begin{cases}
  \mathcal{N}(\bm{0},\bm{I}) &\text{if } \tau=1 \\
   \bf{0} &\text{if } 0 \leq \tau < 1
     \end{cases}  \label{eq:x1}  \\
    \bm{x}_{t} &= \begin{cases}
   \bm{x}_{t+\Delta t} - \bm{v}_\theta(\bm{x}_{t+\Delta t},t+\Delta t)\Delta t &\text{if } t \neq\tau \\
   \alpha_{t} \bm{x}_\theta(\bm{x}_{t+\Delta t},t+\Delta t)+\sigma_{t}\bm{\varepsilon} &\text{if } t=\tau
     \end{cases}  \label{eq:xt}
\end{align}

where $\bm{x}_\theta$ represents the estimated $\bm{x}_0$, which can be derived from the estimated velocity or score through a linear transformation. The derivation process is detailed in \cref{app_derivation}.
In \cref{ablation}, we demonstrate that $\tau$ effectively balances diversity and stability. The sampling process is summarized in Algorithm \ref{alg:temperature}.

\newlength{\oldtabcolsep}
\setlength{\oldtabcolsep}{\tabcolsep}
\setlength{\tabcolsep}{5pt}
\begin{table*}[ht]
\caption{
Objective evaluation results of DiTAR and other systems on two subsets of LibriSpeech test-clean. Specifically, subset A is used and reported by NaturalSpeech3, while subset B is released by F5TTS. $\blacklozenge$ denotes the scores reported in NaturalSpeech3. $\clubsuit$ means the results obtained from the authors of F5TTS. $\spadesuit $ means the results are obtained via released checkpoints.
The boldface and underline indicate the best and the second-best result, respectively. 
$\uparrow$ and $\downarrow$ indicate lower or higher values are better. Abbreviation: Disc.(discrete), Cont.(Continuous), AR(autoregressive model), NAR(non-autoregressive model), NFE(number of function evaluation)}
\label{tts_overall}
\centering
\begin{small}
\begin{tabular}{l l c c c c c l }
\hline
\textbf{Type} & \textbf{System} &  \textbf{\#{Params}} & \textbf{{Training Data}} & \textbf{WER(\%)$\downarrow$} & \textbf{SIM$\uparrow$} & \textbf{UTMOS$\uparrow$} & \textbf{TFLOPs$\downarrow$} \\\hline
\multicolumn{7}{c}{\textbf{LibriSpeech test-clean A}} \\\hline
- & Human &  - & - & 0.34 & 0.68 & 4.14 & -\\
- & Vocoder &  - & - & 0.34 & 0.63 & 4.03 & -\\ \hline
Disc. AR + Disc. NAR & VALL-E $^\blacklozenge$ & 0.4B & Librilight & 6.11 & 0.47 & 3.68 & $\sim2.99$\\
Disc. AR + Cont. NAR & MegaTTS 2 $^\blacklozenge$ &  0.5B & Librilight & 2.32 & 0.53 & 4.02 & $\sim0.06$\\
Disc. NARs & NaturalSpeech 3 $^\blacklozenge$ & 0.5B & Librilight & \underline{1.81} & \textbf{0.67} & \textbf{4.30} & $\sim8.92$ \\
Cont. NAR & NaturalSpeech 2  $^\blacklozenge$ &  0.4B & Librilight & 1.94 & 0.55 & 3.88 & $\sim 12.89$\\
Cont. NAR & Voicebox (NFE=32) $^\blacklozenge$ &  0.4B & Librilight & 2.14 & 0.48 & 3.73 & $\sim 60.89$\\ \hline
Cont. AR & DiTAR (NFE=10) &  0.6B & Librilight & \textbf{1.78} & \underline{0.64} & \underline{4.15} & $\sim2.75$ \\ \hline
\midrule
\multicolumn{7}{c}{\textbf{LibriSpeech test-clean B}} \\\hline
- & Human &  - & - & 2.23 & 0.69 & 4.10 & -\\
- & Vocoder resynthesized & - & - & 2.38 & 0.66 & 3.97 & -\\ \hline
Disc. NARs & MaskGCT (NFE=50) $^\spadesuit$ &  1.1B & Emilia & 2.72 & \textbf{0.69} & 3.90 & $\sim116.66$ \\
Cont. NAR & E2TTS (NFE=32) $^\clubsuit$ &  0.3B & Emilia & 2.95 & \textbf{0.69} & 3.56 & $\sim56.46$ \\
Cont. NAR & F5TTS (NFE=32) $^\clubsuit$ &  0.3B & Emilia & \underline{2.42} & 0.66 & 3.88 & $\sim37.36$ \\ \hline
Cont. AR & DiTAR (NFE=10) &  0.6B & Emilia & \textbf{2.39} & \underline{0.67} & \textbf{4.22} & $\sim 2.75$
\\ \hline
\end{tabular}
\end{small}
\end{table*}
\setlength{\tabcolsep}{\oldtabcolsep}


\subsection{Implemenations}

\subsubsection{Continuous speech tokenization}

Following LDM\cite{rombach2022high}, we use a variational auto-encoder (VAE)\cite{kingma2013auto} to convert the waveform into the distribution of latent $z$, represented by mean and variance. The encoder of the VAE consists of multiple layers of the convolutional network, and the decoder's architecture follows BigVGAN\cite{lee2022bigvgan}. The adversarial training scheme also follows \citet{lee2022bigvgan}. We adopt the multi-period discriminator (MPD) and multi-scale discriminator (MSD) proposed by \citet{kong2020hifi} as our discriminators. In our setup, the 24000hz waveform is compressed into 40hz latent with a dimension of 64.

\subsubsection{Model}

DiTAR consists of three modules: aggregation encoder, language model, and decoder (LocDiT). 
In our implementation, all of them are based on a transformer architecture. Specifically, both the encoder and decoder employ bidirectional attention masks, while the LM utilizes a causal attention mask. All transformers adopt the Pre-Norm\cite{xiong2020layer} architecture and utilize RMSNorm\cite{zhang2019root} and RoPE \cite{su2024roformer}. 
Each patch of continuous tokens, together with a learnable special token positioned at the beginning of the sequence similar to \cite{devlin2018bert}, is fed into the aggregation encoder. 
The output corresponding to the special token's position serves as the aggregation embedding. Aggregation embeddings from different patches form a sequence that is processed by the LM.
In LocDiT, LM's outputs and the time embedding are added, along with historical context patches and noisy target tokens, forming a new sequence that serves as the input of LocDiT. 
When calculating the loss, we only consider the output corresponding to the position of noisy target tokens. During training, LM's output is randomly replaced by a vector of all zeros with a probability of 0.1 to enable LM guidance for LocDiT.


%\vspace{-0.2cm}
\begin{table}[t]
\caption{Subjective evaluation results on LibriSpeech test-clean subset B. We compare DiTAR with several leading NAR systems.}
%\vspace{-0.1cm}
\label{tab:subjective}
\centering
\begin{tabular}{l |c c c | c}
\hline
\textbf{System} & N-MOS & Q-MOS & S-MOS & CMOS \\\hline
Human &  3.89 & 3.61 & 3.56 & +0.18 \\ \hline
E2TTS & 3.27 & 3.44 & 3.15 & -0.32\\
F5TTS &  3.36  & 3.58 & 3.33 & -0.04 \\ \hline
DiTAR  & \textbf{3.69} & \textbf{3.87} & \textbf{3.55} & \textbf{0.00} \\ \hline
\end{tabular}
\end{table}
 
\subsubsection{Diffusion formulation} 
Following \citet{song2020score,lu2024simplifying}, we adopt a variance-preserving diffusion process defined by 
\begin{align}
   \bm{x}_t  &=\alpha_t \bm{x}_0 + \sigma_t \bm{\varepsilon} \\
    &= \cos{\left( \frac{\pi t}{2} \right)} \bm{x}_0 + \sin{\left( \frac{\pi t}{2} \right)} \bm{\varepsilon}
\end{align}
where $\bm{x}_0\thicksim q(\bm{x}_0)$ denotes the data, $\bm{\varepsilon} \sim \mathcal{N}(\bm{0}, \bm{I})$ denotes the standard Gaussian noise, $t\in[0,1]$. 
We employ a conditional flow-matching loss\cite{lipman2022flow}:
\begin{equation}
    L_{diff}= \mathbb{E}_{t, \bm{x}_0,\bm{\varepsilon}} 
    \left[ \lVert \bm{v}_{\theta}(\bm{x}_t, t) - \bm{v}(\bm{x}_t, t)
    \rVert^2_2  \right] 
\end{equation}

where the velocity is defined as:
    $\bm{v}(\bm{x}_t, t)=
    \dot{\bm{x}_t}=\dot{\alpha}_t \bm{x}_t+\dot{\sigma}_t \bm{\varepsilon}$.
At inference time, We employed the DDIM sampler\cite{song2020denoising}, which is essentially an Euler ODE sampler with respect to signal-to-noise ratio $\frac{\alpha_t^2}{\sigma_t^2}$ instead of $t$, proved to better align with the semi-linear property of the diffusion ODE\cite{lu2022dpm}.

\subsubsection{Zero-shot TTS system} 

The text sequence is converted into phonemes and processed through a lookup table to obtain text embeddings. Speech tokens are processed by the aggregation encoder to produce speech embeddings, which are then concatenated with the text embeddings. The embedding sequence serves as the input of the LM of DiTAR. During training, the loss of the text is not included. Additionally, we introduce a binary classifier with a fully connected layer at LM's output to predict when to stop following \citet{wang2017tacotron,li2019neural}.
The loss function for zero-shot TTS can be summarized as $L=L_{diff}+L_{\text{stop}}$. During inference, prompting audio, text, and target text are fed as prefix input to DiTAR's LM, which then autoregressively generates the target audio.



\section{Experiments}

\subsection{SOTA Performance in Zero-Shot TTS}

In this subsection, we benchmark DiTAR against leading systems and demonstrate its state-of-the-art performance.

\subsubsection{Setup}
Many studies evaluate zero-shot TTS with inconsistent training sets, prompt audios, texts, and tools, diluting the value of comparisons. We standardize these variables for a more objective and fair assessment across systems.

\textbf{Training and Evaluation Dataset.}
We consider two open-source datasets as our training dataset. 1) Librilight\cite{kahn2020libri}, containing 60K hours of English speech data from LibriVox audiobooks. 2) Emilia\cite{he2024emilia}, a multilingual dataset containing around 100k hours of speech data. 

We adopt three open-source datasets for evaluation: 1) LibriSpeech(PC)\cite{panayotov2015librispeech,meister2023librispeech} test-clean, containing 40 distinct English speakers and a 5.4-hour speech. 
We employ two established subsets: subset A from NaturalSpeech3, featuring 40 three-second speech prompts and 40 target samples, and subset B from F5TTS, which includes 40 prompts and 1127 samples.
2) Seed-ZH: a subset from DiDiSpeech 2\cite{guo2021didispeech}, a 
Chinese speech dataset, containing 1088 prompts and targets. 3) Seed-EN: a subset from 
Common Voice\cite{ardila2019common}, a crowdsourcing English speech dataset with diverse accents, containing 2020 prompts and targets.

\textbf{Evaluation Metrics.}
We evaluate four objective metrics: 1) Word Error Rate (WER), which assesses generation robustness. For consistency, we use the same ASR setups as previous studies to transcribe generated speech across different test sets. Specifically, we adopt a Hubert-based model\footnote{\href{https://huggingface.co/facebook/hubert-large-ls960-ft}{https://huggingface.co/facebook/hubert-large-ls960-ft}} and Faster-whisper-large-v3 \footnote{\href{https://huggingface.co/Systran/faster-whisper-large-v3}{https://huggingface.co/Systran/faster-whisper-large-v3},version:0.10.1}\cite{radford2022whisper} for the subset A and B of Librispeech test-clean, respectively, Whisper-large-v3\footnote{\href{https://huggingface.co/openai/whisper-large-v3}{https://huggingface.co/openai/whisper-large-v3}} for Seed-EN and Paraformer-zh for Seed-ZH. 2) Speaker similarity to the prompt audio (SIM). Specifically, we employed WavLM-large\cite{chen2022wavlm} to compute the cosine distance between the generated and reference speech. 3) UTMOS\cite{saeki2022utmos}, an automatic predicted mean opinion score (MOS) to evaluate speech quality. 4) Floating point operations (FLOPs), measuring the computational load of models. The calculation process is detailed in \ref{flops}.

For subjective evaluation, we employ four MOS metrics: 1) N-MOS for naturalness,  2) Q-MOS for sound quality 3) S-MOS for speaker voice similarity 4) CMOS for side-by-side comparison with human audios.


\textbf{Model Setup and Baselines.}
We benchmark DiTAR against diverse zero-shot TTS systems with varying architectures, including both multi-stage and single-stage schemes that operate in discrete or continuous value spaces. We conduct comparisons using DiTAR with 0.6 billion parameters and a patch size of 4. During inference, DiTAR's LocDiT uses an NFE (Number of Function Evaluations) of 10. Specific details about the parameters of DiTAR are provided in \ref{app_conf}.

\subsubsection{Experimental results}
We conduct a multi-dimensional comparison of DiTAR with other baseline works.
For objective metrics, Table \ref{tts_overall} presents the evaluation results on LibriSpeech test-clean, while Table \ref{tab:tts_sup} in \ref{app_seed} shows the results for Seed-ZH and Seed-EN. 
For subjective evaluation, we invite 10 English experts to rate the generated audio. For N-MOS, Q-MOS, and S-MOS metrics, the generated audios are rated on a scale of 1 to 5. For CMOS, experts compare the generated audio against the ground truth (GT) and assign scores from -2 to 2. Subjective results are detailed in Table \ref{tab:subjective}.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{scaling.pdf}
  \caption{The performance of DiTAR consistently improves with increases in either training data or model size. The star marker indicates performance that surpasses human levels.}
  \label{fig:Scaling}
  \vspace{0cm}
\end{figure}

\textbf{Generation Robustness.} 
As shown in Table \ref{tts_overall}, under two different training data configurations and test sets, DiTAR consistently delivered the best WER, showcasing robust synthesis performance matched by NAR systems with phone-level duration models.
Table \ref{tab:tts_sup} in \ref{app_seed} further details DiTAR's comparison with models trained on proprietary data, highlighting its superior synthesis stability.

\textbf{Speaker Similarity.} We perform both objective and subjective assessments of speaker similarity. Objectively, as Table \ref{tts_overall} illustrates, DiTAR delivers strong SIM scores, on par with NAR systems. Subjectively, as detailed in Table \ref{tab:subjective}, DiTAR outperforms leading NAR systems in S-MOS scores, demonstrating excellent in-context learning capabilities.


\textbf{Naturalness.}
We assess speech naturalness using the subjective metrics N-MOS and CMOS. As detailed in Table \ref{tab:subjective}, DiTAR excels over leading NAR systems, achieving the highest scores for naturalness.

\textbf{Audio Quality.}
We use the objective metric UTMOS and the subjective metric Q-MOS to evaluate audio quality. 
Table \ref{tts_overall} shows that DiTAR scores highly on UTMOS in LibriSpeech test-clean subset A, just behind NaturalSpeech3, and leads in subset B. Subjectively, as shown in  Table \ref{tab:subjective}, DiTAR exceeds even the ground truth in Q-MOS scores, highlighting the superior quality of its outputs.

\textbf{Computaional Load.}
Table \ref{tts_overall} indicates that DiTAR not only ensures high-quality audio generation but also dramatically cuts computational demands by approximately $3\sim43\times$ compared to other NAR systems. Although hybrid systems require less computational power, they tend to produce lower-quality outputs. Further details on inference efficiency are discussed in \ref{efficiency}.


\subsection{Scaling Behaviors}
In this subsection, we explore DiTAR's scaling properties for zero-shot speech generation, focusing on model architecture and training data, using Seed-EN as the test set.

\textbf{The performance of DiTAR consistently enhances as either data size or model size increases.}  We conduct scaling experiments concerning both data and model size.  We expand the training data from 20k to 280k hours with a 0.6B model to assess performance changes.
For model size, we scale from 0.1B to 1B parameters using 280k hours of training data, simultaneously increasing the parameters of the encoder, language model, and LocDiT. For the detailed setups see Table \ref{tab:app_conf}. As shown in Figure \ref{fig:Scaling}, the model's WER and SIM consistently improve as training data and model parameters increase.


\textbf{The language model and diffusion decoder benefit more from scaling.}
We scale the encoder, language model, and decoder (LocDiT) individually to assess their impacts, starting with a 0.4B parameter model. Given that the encoder and decoder process shorter sequences, we allocated more parameters to the language model empirically. Table \ref{tab:CompScale} demonstrates that enlarging the language model and LocDiT enhances performance, whereas increasing the encoder size has little effect on outcomes.

\vspace{-0.2cm}
\begin{table}[t]
\caption{Scaling behavior of different components. }
\vspace{-0.1cm}
\label{tab:CompScale}
\centering
\begin{tabular}{l|c|c}
\hline
\textbf{System} & \textbf{WER(\%)$\downarrow$} & \textbf{SIM$\uparrow$}\\ \hline
DiTAR (0.4B)              & 1.876 & 0.716 \\ \hline
Encoder size   $\times4$    & 1.821 & 0.72 \\ 
Language model size $\times4$    & \textbf{1.695} & \textbf{0.727} \\ 
LocDiT size $\times4$      & \textbf{1.785} & \textbf{0.726} \\  \hline
\end{tabular}
\end{table}



\begin{table}[t]
\caption{The impact of historical context in LocDiT. $^\star$ indictes that many samples fail to stop during generation.}
%\begin{small}
\vspace{-0.1cm}
\label{tab:context}
\centering
\begin{tabular}{c|c|c|c}
\hline
\textbf{Patch Size} & \textbf{Historical contexts} & \textbf{WER(\%)$\downarrow$} & \textbf{SIM$\uparrow$} \\ \hline
\multirow{3}{*}{1} &   2  & 3.334 & 0.716  \\ 
& 1  & 6.131 & 0.692  \\ 
& 0 (not used)      & 53$^\star$ & 0.34$^\star$   \\ \hline 
\multirow{3}{*}{4} &   2  & \textbf{1.809} & \textbf{0.73}  \\ 
& 1    & \textbf{1.736} & \textbf{0.72}  \\ 
& 0 (not used)      & 22.874$^\star$ & 0.56$^\star$   \\ \hline 
\end{tabular}
%\end{small}
\vspace{-0.3cm}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{group.pdf}
  \caption{The impact of patch size of LocDiT. }
  \label{fig:group}
  \vspace{0cm}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{guidance.pdf}
  \caption{The impact of LM guidance under different NFE setups. $w=1$ indicates that guidance is not used.}
  \label{fig:guidance}
  \vspace{-0.3cm}
\end{figure}



\begin{figure}[t]
  \centering
  \includegraphics[width=0.92\linewidth]{throughput.pdf}
  \caption{Throughput variations across different models as batch size changes.}
  \label{fig:throughput}
  \vspace{0cm}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{variance.pdf}
  \caption{The impact of temperature on generation diversity.}
  \label{fig:variance}
  \vspace{-0.3cm}
\end{figure}



\subsection{Method Analysis and Ablation Study}
\label{ablation}
In this subsection, we conduct a detailed analysis of the various components of DiTAR. Unless specified otherwise, we default to using DiTAR with 0.6 billion parameters, a patch size of 4, and NFE=10, tested on the Seed-EN dataset.

\textbf{Patch Size.} 
LocDiT utilizes bidirectional attention to generate the next patch. To investigate the impact of patch size, we vary it while keeping the model's total parameter count constant. 
As illustrated in Figure \ref{fig:group}, performance declines when patch sizes are either too large or too small. 
Excessively small patches diminish the model's bidirectional attention capability, forcing reliance on causal-attention AR and degrading performance. This may explain the poor performance of causal AR with diffusion loss as noted in \citet{li2024autoregressive}. Conversely, overly large patches turn LocDiT into a bottleneck, necessitating increased parameters.

\textbf{The Historical Context of LocDiT.} We experiment with different numbers of historical patches and confirm the critical role of LocDiT's context.
Table \ref{tab:context} indicates that without historical context, the model's performance drastically worsens. Integrating historical context shifts LocDiT's function from mere generation to outpainting, markedly improving synthesis results. 
For larger patch sizes, such as 4, choosing a single historical context strikes the best balance between computational efficiency and performance.

\textbf{LM Guidance.} 
Figure \ref{fig:guidance} illustrates that LM guidance significantly enhances the diffusion decoder's inference process. Without guidance ($w=1$), both WER and SIM deteriorate significantly. Conversely, an excessively large guidance scale can also impair outcomes.
Additionally, even with extremely low NFE (such as 2), DiTAR still performs well on WER and SIM when combined with LM guidance.



\subsection{Inference Analysis}
\subsubsection{The impact of temperature}
Temperature is vital for balancing diversity and determinism during language model inference, yet its definition for continuous-valued LMs remains underexplored. In this study, we define temperature $\tau$ as the point at which random sampling is introduced during the reverse ODE solution.

To explore how $\tau$ balances diversity and stability, we synthesize the same text 500 times under different $\tau$ settings to assess voice diversity. We use only the text as DiTAR's prefix input prompting the model to generate speech in random voices autoregressively. We then use WavLM-large to extract speaker embeddings from these samples. Following this, we conduct Principal Component Analysis (PCA), previously fitted on the training set, on the embeddings, and visualize the first two components.
Figure \ref{fig:variance} demonstrates that as $\tau$ increases, so does the diversity of the speaker voices. We also assess WER and SIM for zero-shot speech synthesis across different temperatures $\tau$, observing consistently excellent performance. Notably, higher temperatures slightly improve SIM scores, while lower temperatures enhance WER outcomes. Check more details in \ref{app_temp}.

\subsubsection{Efficiency}
\label{efficiency}
DiTAR, blending a language model with a diffusion transformer, inherits features from both components.
DiTAR drastically cuts computational demands by allocating the majority of its parameters to the language model and fewer to LocDiT.

We compare the inference performance of NAR (a diffusion transformer) and DiTAR under the same parameter sizes. During inference, all systems use CFG with NFE = 10. We evaluate throughput, defined as the duration of audio generated per unit time, across various batch sizes by inferring 10 seconds of audio on an A100 GPU.
As shown in Fig \ref{fig:throughput}, with small batch sizes,  NAR achieves higher throughput due to lacking autoregressive computations. 
As batch sizes grow, NAR's high FLOPs demands hinder throughput gains, while DiTAR's throughput increases rapidly and is significantly superior to NAR.


\section{Conclusion}

In this work, we propose DiTAR which utilizes the diffusion transformer’s capacity for high-quality generation to create localized patches while maintaining the core autoregressive features of language models. For inference, we introduce temperature as the introduction 
time point for noise while solving the reverse diffusion ODE. Applied to zero-shot speech synthesis, DiTAR achieves SOTA robustness, speaker similarity, and naturalness with substantially lower computational requirements.






% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{ditar_icml2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Implementation Details}

\subsection{Training Details}
Regarding data processing for LibriLight, since it lacks transcriptions, we use our internal ASR system to transcribe this dataset and then convert the text into phonemes for use. For Emilia, we perform Grapheme-to-Phoneme (G2P) conversion on the official transcriptions provided.

We utilize 16 A100 GPUs, each processing a batch size of 15K tokens, and train DiTAR for 0.5M steps. The AdamW optimizer is employed with a constant learning rate of 1e-4, 
$\beta_t=0.9$, and $\beta_2=0.99$. For DiTAR with 1B parameters, we utilize 32 A100 GPUs with a batch size of 7.5k per GPU.


\subsection{Model Configuration for Scaling}
\label{app_conf}
During the validation of DiTAR's scaling behavior, we trained models of four different sizes, ranging from 0.1 billion to 1 billion parameters. Specific hyperparameter configurations are detailed in Table \ref{tab:app_conf}.

\begin{table}[ht]
\caption{Configurations of DiTAR with different sizes.}
\label{tab:app_conf}
\centering
\begin{tabular}{l l c c c c}
\hline
\multicolumn{2}{c}{Model size} & $\sim0.1$B & $\sim0.4$B & $\sim0.6$B & $\sim1$B \\ \hline
\multicolumn{2}{c}{Hyper-parameters} & \multicolumn{4}{c}{Value}  \\ \hline
\multirow{4}{*}{Encoder} & Number of layers & 4 & 4 & 6 & 8\\
& Hidden dim & 512 & 1024 & 1024 & 1024 \\
& Number of heads & 8 & 16 & 16 & 16 \\
& FFN dim & 2048 & 4096 & 4096 & 4096 \\ \hline
\multirow{4}{*}{Language Model} & Number of layers & 24 & 24 & 36 & 24 \\
& Hidden dim & 512 & 1024 & 1024 & 1536 \\
& Number of heads & 8 & 16 & 16 & 24 \\
& FFN dim & 1024 & 4096 & 4096 & 6144 \\ \hline
\multirow{4}{*}{LocDiT} & Number of layers & 4 & 4 & 6 & 8\\
& Hidden dim & 512 & 1024 & 1024 & 1024 \\
& Number of heads & 8 & 16 & 16 & 16 \\
& FFN dim & 2048 & 4096 & 4096 & 4096 \\ \hline
\end{tabular}
\end{table}

\subsection{Derivation of $x_\theta$ in temperature sampling for different parameterized diffusion}
\label{app_derivation}
As previously discussed in Eq 5, $x_\theta$ denotes the predicted data, which can be derived under different parameterizations of diffusion. Consisdering a diffusion process defined by $x_t=\alpha_t x_0 + \sigma_t \varepsilon$, where $x_0\thicksim q(x_0)$ denotes the data, $\varepsilon \thicksim N(0, \textbf{I})$ denotes the standard Gaussian noise, $t\in[0,1]$. 

For $\epsilon$-prediction mode,
\begin{equation}
       x_\theta(x_{t},t) = \frac{x_t-\sigma_t\epsilon_\theta(x_t,t)}{\alpha_t}
\end{equation}
For $x_0$-prediction mode, $x_\theta(x_t, t)$ is exactly model's prediction. 

For $v$-prediction mode, also known as flow-matching. Next, we will proceed with a step-by-step deduction. Based on the definition of $v$, we can derive the following:
\begin{align} 
     v &= \dot{\alpha_t}x_0 + \dot{\sigma_t}\varepsilon \\
       &= \dot{\alpha_t}x_0 +  \frac{\dot{\sigma_t}(x_t-\alpha_tx_0)}{\sigma_t} \\
       & = \lparen \dot{\alpha_t}-\frac{\dot{\sigma_t}\alpha_t}{\sigma_t} \rparen x_0 + 
       \frac{\dot{\sigma_t}}{\sigma_t}x_t
\end{align}
After rearanging $v$ and $x_0$, we get:
\begin{equation}
    x_0 =  \frac{\dot{\sigma_t}x_t - \sigma_tv}
    { \sigma_t\dot{\alpha_t}- \dot{\sigma_t}\alpha_t}
\end{equation} 
Thus, 
\begin{equation}
  x_\theta(x_t,t) =  \frac{\dot{\sigma_t}x_t - \sigma_tv_\theta(x_t,t)}
    { \sigma_t\dot{\alpha_t}- \dot{\sigma_t}\alpha_t}
\end{equation} 

\subsection{The process of temperature sampling}
Here, we present our temperature-based sampling process in the form of an algorithm flowchart, shown in Algorithm \ref{alg:temperature}.

\begin{algorithm}[ht]
   \caption{Temperature sampling}
   \label{alg:temperature}
\begin{algorithmic}
   \STATE {\bfseries Input:} $v$-prediction model $\bm{f_\theta}(.,.)$, discretized time points $t_1<t_2<...<t_{N-1}\in[0,1)$, $t_N=1$,ODE solver $\bm{\Psi}$(.,.,.), transformation function $\mathcal{F}$, temperature $\tau$

   \STATE $\eta \gets\underset{n\in[1,N]}{\operatorname{argmax}}  |t_n - \tau|$
   \IF{$\eta = N$}
        \STATE Sample initial noise $\bm{x}_{t_N} \sim  \mathcal{N} (\bm{0},\bm{I})$ 
   \ELSE
        \STATE  initial noise $\bm{x}_{t_N} \gets \bm{0}$
   \ENDIF
   
   \FOR{$n=N-1$ {\bfseries to} $1$}
    \STATE $\hat{\bm{v}} \gets \bm{f_\theta}(\bm{x}_{t_N},t_N)$
    \IF{$t_n=\eta$}
    \STATE $\hat{\bm{x}} \gets \mathcal{F}(\hat{\bm{v}})$
    \STATE Sample $\bm{z} \sim  \mathcal{N} (\bm{0},\bm{I})$
    \STATE $\bm{x} \gets \alpha_{t_n} \hat{\bm{x}} + \beta_{t_n}\bm{z}$
    \ELSE
    \STATE $\bm{x} \gets \bm{\Psi}(\bm{v}, n+1, n)$
    \ENDIF
   \ENDFOR
   
   \STATE {\bfseries Output:} $\bm{x}$
\end{algorithmic}
\end{algorithm}

%\subsection{The training of Speech VAE}
%We use a VAE to convert the waveform into the distribution of latent $z$, represented by mean and variance. 
%The adversarial training scheme follows \cite{kong2020hifi}.

\subsection{Calculation of FLOPs}
\label{flops}
When calculating FLOPs for all systems, we use a 3-second audio prompt to synthesize 10 seconds of audio. Assuming a text frame rate of 7Hz, the prompt's text length is 21, and the target text length is 70. 
We focus solely on the computationally dominant components, including causal transformers, non-causal transformers, and convolutional layers. For causal transformers, calculations account for the use of KV cache. The multi-head setup does not impact FLOPs, so we uniformly assume that the number of heads is 1. Considering that bias and normalization layers contribute minimally to the overall FLOPs of the transformer, we will temporarily disregard them.
Matrix multiplication involves an equal number of additions and multiplications, so the result should be multiplied by 2. For the diffusion part, the corresponding FLOPs needs to be multiplied by the Number of Function Evaluations (NFE). Additionally, Classifier-Free Guidance (CFG) incurs double the computational cost.

For a one-dimensional convolution network with $N$ layers, a hidden channel size of $C$, a kernel size of $K$, and the input length of T, the FLOPs can be calculated as follows:
\begin{align}
     \text{FLOPs} &= [C, KT] \times[KT, C] \times N= 2C^2KTN
\end{align}

For a non-causal transformer with $N$ layers, a hidden size of $C$, an FFN intermediate hidden size of $C_\text{mid}$, and the input length of T, the FLOPs can be calculated as follows:
\begin{align}
    \text{QKV} &= [T, C] \times [C, 3C] = [T,3C]=6TC^2 \\
    \text{Attention} &= Q K^T = [T, C] \times [C, T] = [T, T]= 2T^2C \\
    \text{V} &= [T, T] \times [T, C] = [T, C] = 2T^2C \\
     \text{FC} &= [T, C] \times [C, C] = [T, C] = 2TC^2 \\
    \text{FFN\_FC1}  &= [T, C] \times [C, C_\text{mid}]  = [T, C_\text{mid}] = 2TCC_\text{mid} \\
     \text{FFN\_FC2}  &= [T, C_\text{mid}] \times [C_\text{mid},C]  = [T, C] = 2TCC_\text{mid} \\
     \text{Transformer\_FLOPs}(N,C,T,C_\text{mid}) &= (\text{QKV} + \text{Attention} + \text{V} + \text{FC} +  \text{FFN\_FC1} +  \text{FFN\_FC2}) \times N \\
\end{align}

For a causal transformer with $N$ layers, a hidden size of $C$, an FFN intermediate hidden size of $C_\text{mid}$, a prefix input length of $T_{\text{pre}}$, and the input length of T, the FLOPs can be calculated as follows:
\begin{align}
    \text{Prefix\_FLOPs} &=  \text{Transformer\_FLOPs}(N,C,T_\text{pre},C_\text{mid}) \times N \\
    \text{QKV} &= [1, C] \times [C, 3C] = [1,3C]=6C^2 \\
    \text{Attention} &= Q K^T = [1, C] \times [C, t] = [1, t]= 2t^2C \\
     \text{V} &= [1, t] \times [t, C] = [1, C] = 2t^2C \\
    \text{FC} &= [1, C] \times [C, C] = [1, C] = 2C^2 \\
    \text{FFN\_FC1}  &= [1, C] \times [C, C_\text{mid}]  = [1, C_\text{mid}] = 2CC_\text{mid} \\
    \text{FFN\_FC2}  &= [1, C_\text{mid}] \times [C_\text{mid},C]  = [1, C] = 2CC_\text{mid} \\
     \text{AR\_FLOPs}(N,C,T,T_\text{pre},C_\text{mid}) &= \sum_{t=1+T_\text{pre}}^{T+T_\text{pre}}{(\text{QKV} + \text{Attention} + \text{V} + \text{FC} +  \text{FFN\_FC1} +  \text{FFN\_FC2})} \times N \\
     \text{AR\_Transformer\_FLOPs}(N,C,T,T_\text{pre},C_\text{mid}) &=  \text{Prefix\_FLOPs} +  \text{AR\_FLOPs}
\end{align}


\newpage
\section{Experimental Result Supplements}

\subsection{Comparison with additional models}
\label{app_seed}
We further compared DiTAR against a wider range of models, including some close-source ones, such as $\text{Seed-TTS}$. We conduct tests on datasets for two languages, namely Seed-EN and Seed-ZH. As shown in Fig \ref{tab:tts_sup}, DiTAR achieves the best generation robustness and excellent speaker similarity. All results of other systems are reported in their papers.

\begin{table}[ht]
\caption{
Objective evaluation results of DiTAR and various systems on Seed-EN and Seed-ZH. }
\label{tab:tts_sup}
\centering
\begin{tabular}{l c c c c}
\hline
\textbf{System}& \multicolumn{2}{c}{\textbf{Seed-EN}} & \multicolumn{2}{c}{\textbf{Seed-ZH}}\\\hline
 & \textbf{WER(\%)$\downarrow$} & \textbf{SIM$\uparrow$}& \textbf{WER(\%)$\downarrow$} & \textbf{SIM$\uparrow$}\\ \hline
Human    & 2.06 & 0.73 & 1.254 & 0.750 \\ \hline
$\text{Seed-TTS}_\text{DiT}$    & \underline{1.733} & \textbf{0.790} & \underline{1.178} & \textbf{0.809} \\
CosyVoice    & 4.29& 0.609 & 3.63 & 0.723\\
CosyVoice 2    & 2.57 & 0.652 & 1.45 & 0.748\\
CosyVoice 2-S & 2.38 & 0.654 & 1.45 & 0.753 \\
FireRedTTS  & 3.82 & 0.46 & 1.51 & 0.63 \\
MaskGCT & 2.623 & 0.717 & 2.273 & \underline{0.774} \\
E2TTS    & 2.19 & 0.71 & 1.97 & 0.73 \\
F5TTS    & 1.83 & 0.67 & 1.56 & 0.76 \\ \hline
DiTAR       & \textbf{1.685} & \underline{0.735} & \textbf{1.023} & 0.753 \\ \hline

\end{tabular}
\end{table}

\subsection{The impact of temperature during inference}
\label{app_temp}

We further test how the model's objective metrics change under various temperatures.
As shown in Table \ref{tab:app_temperature}, across different temperatures, the model consistently achieves favorable objective metrics on a large-scale test set.
There is a trend that higher temperatures yield slightly better SIM scores, whereas lower temperatures result in better WER scores. The underlying reason may be that simulating the voice of unseen speakers requires greater diversity from the model, while pronunciation robustness demands more determinacy and stability from the model.

\begin{table}[ht]
\caption{Objective evaluation results under different temperature values and NFE.}
\label{tab:app_temperature}
\centering
\begin{tabular}{l c c c c}
\hline
$\tau$ & \multicolumn{2}{c}{NFE=2} & \multicolumn{2}{c}{NFE=10}\\ \hline
 & \textbf{WER(\%)$\downarrow$} & \textbf{SIM$\uparrow$}& \textbf{WER(\%)$\downarrow$} & \textbf{SIM$\uparrow$}\\ \hline
0 & 1.666 & 0.717 & 1.623 & 0.719\\
0.5 & 1.669 & 0.722 & 1.699 & 0.727 \\
1 & 1.686 & 0.72 & 1.689 & 0.727 \\ \hline
\end{tabular}
\end{table}

\newpage
\section{Subjective evaluation}
Apart from objective metrics, we employ many subjective metrics to comprehensively evaluate the generation ability of different systems. In the process of evaluation, raters are presented with 2 audios for comparison and 1 audio as a reference. Each rater is asked to score after comparing the two audios (CMOS) and to also rate each audio individually (N-MOS, Q-MOS, S-MOS). The user interface is shown in Fig \ref{fig:mos_ui} and \ref{fig:mos_Q}. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\linewidth]{mos_ui.png}
  \caption{The user interface for subjective evaluation. }
  \label{fig:mos_ui}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\linewidth]{mos_Q.png}
  \caption{The rating interface and questions presented to raters.}
  \label{fig:mos_Q}
\end{figure}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
