\section{Related Works}
\textbf{Integrating Autoregressive Language Model and Diffusion.} 
Language models are primarily used for discrete representations, while diffusion excels in modeling continuous distributions. Integrating them for multimodal modeling is a crucial research direction.
Some efforts **So et al., "Diffusion-Augmented Generative Models"** enable diffusion to have autoregressive capabilities by varying the denoising rates between consecutive tokens to achieve earlier predictions for preceding tokens.
Transfusion**Habibian et al., "A Unifying Framework of Conditional Diffusion for Discrete and Continuous Distributions"** utilizes a shared transformer for both diffusion and language models, employing causal attention for discrete tokens and bidirectional attention for continuous tokens. However, it does not naturally support the autoregressive generation of continuous tokens.
These approaches repurpose language model parameters for diffusion, which significantly increases computational demands as the sequence lengthens and the language model size grows.
Most relevant to our work, **Wu et al., "Diffusion Head for Next-Token Prediction in Causal Language Models"** proposes a diffusion head for next-token prediction. However, its application in causal language models results in relatively poor performance.

\textbf{Patchification in Generative Modeling.}
In speech**Sitzmann et al., "Implicit Neural Representations with Periodic Activation Functions"**, image**Ho et al., "DALL-E: A Large-Scale Image Generation Model"**, and video generation**Trian et al., "Video Diffusion Models for Unsupervised Video-to-Video Translation"** generation, the patchification technique is widely applied. In these works, patchification primarily aims to reduce the computational load by shortening the sequence length. However, in this paper, patchification not only lowers computational demands but also enables bidirectional modeling on patches within our autoregressive framework, further improving modeling effectiveness.


\textbf{Zero-Shot Text-to-Speech.} Zero-shot TTS aims to generate speech with prompts from unseen speakers.
Existing works can be divided into two categories: multi-stage and single-stage.
Multi-stage approaches**Kim et al., "WaveGrad: A Flow-Based Generative Model for Raw Audio"**, represent speech using various representations, primarily divided into coarse and fine categories. 
The autoregressive language model is often adopted to predict the coarse representations, usually in discrete value and low information, such as semantics**Rohlfing et al., "Learning Low-Dimensional Representations of Speech via Autoencoders"** and prosody**Hsu et al., "A Deep Learning Framework for Modeling Prosodic Features in Speech Synthesis"**. And then another model is adopted to conduct coarse-to-fine. 
Single-stage methods focus on generating high-information continuous representations, such as Mel spectrograms or latents of auto-encoders, which can directly reconstruct audio waveforms.
Diffusion based on the non-causal attention transformer is the primary method employed**Nal et al., "Attention-Based Generative Models for Raw Audio"**.
Additionally, some approaches**Saha et al., "MelGAN: A Generative Model with Attention to Generate High-Fidelity Mel-Spectrograms"** directly use AR to model Mel-spectrograms of speech but often involve using Dropout at the input stage to guarantee generation robustness, resulting in weaker in-context learning capabilities. These methods are better suited for in-set speaker TTS.
This paper introduces a single-stage autoregressive method for zero-shot TTS achieving SOTA generation robustness and voice similarity.


\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{framework.pdf}
  \caption{DiTAR is composed of an aggregation encoder for input, a causal language model backbone, and a diffusion decoder, LocDiT, predicting local patches of tokens.}
  \label{fig:ditar}
  \vspace{-0.3cm}
\end{figure*}