\section{Related Works}
\textbf{Integrating Autoregressive Language Model and Diffusion.} 
Language models are primarily used for discrete representations, while diffusion excels in modeling continuous distributions. Integrating them for multimodal modeling is a crucial research direction.
Some efforts \cite{wu2023ar,liu2024autoregressive,chen2024diffusion} enable diffusion to have autoregressive capabilities by varying the denoising rates between consecutive tokens to achieve earlier predictions for preceding tokens.
Transfusion\cite{zhou2024transfusion} utilizes a shared transformer for both diffusion and language models, employing causal attention for discrete tokens and bidirectional attention for continuous tokens. However, it does not naturally support the autoregressive generation of continuous tokens.
These approaches repurpose language model parameters for diffusion, which significantly increases computational demands as the sequence lengthens and the language model size grows.
Most relevant to our work, \citet{li2024autoregressive} proposes a diffusion head for next-token prediction. However, its application in causal language models results in relatively poor performance.

\textbf{Patchification in Generative Modeling.}
In speech\cite{wang2017tacotron,meng2024autoregressive,chen2024vall}, image\cite{peebles2023scalable,li2024imagefolder}, and video generation\cite{liu2024sora}, the patchification technique is widely applied. In these works, patchification primarily aims to reduce the computational load by shortening the sequence length. However, in this paper, patchification not only lowers computational demands but also enables bidirectional modeling on patches within our autoregressive framework, further improving modeling effectiveness.


\textbf{Zero-Shot Text-to-Speech.} Zero-shot TTS aims to generate speech with prompts from unseen speakers.
Existing works can be divided into two categories: multi-stage and single-stage.
Multi-stage approaches\cite{chen2024vall,chen2025neural,lee2023hierspeech++,jiang2023mega,jiang2023boosting,anastassiou2024seed}, represent speech using various representations, primarily divided into coarse and fine categories. 
The autoregressive language model is often adopted to predict the coarse representations, usually in discrete value and low information, such as semantics\cite{kharitonov2023speak,anastassiou2024seed,hsu2021hubert,chung2021w2v,baevski2020wav2vec} and prosody\cite{jiang2023mega,jiang2023boosting,ju2024naturalspeech}. And then another model is adopted to conduct coarse-to-fine. 
Single-stage methods focus on generating high-information continuous representations, such as Mel spectrograms or latents of auto-encoders, which can directly reconstruct audio waveforms.
Diffusion based on the non-causal attention transformer is the primary method employed\cite{chen2024f5,eskimez2024e2,gao2023e3,le2024voicebox}.
Additionally, some approaches\cite{meng2024autoregressive,wang2017tacotron,shen2018natural,li2019neural} directly use AR to model Mel-spectrograms of speech but often involve using Dropout\cite{srivastava2014dropout} at the input stage to guarantee generation robustness, resulting in weaker in-context learning capabilities. These methods are better suited for in-set speaker TTS.
This paper introduces a single-stage autoregressive method for zero-shot TTS achieving SOTA generation robustness and voice similarity.


\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{framework.pdf}
  \caption{DiTAR is composed of an aggregation encoder for input, a causal language model backbone, and a diffusion decoder, LocDiT, predicting local patches of tokens.}
  \label{fig:ditar}
  \vspace{-0.3cm}
\end{figure*}