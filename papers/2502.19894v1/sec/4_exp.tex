\section{Experiments}
\subsection{Implementation Details}
\textbf{Datasets.} We train our model on the CelebV-HQ \cite{zhu2022celebv} and VFHQ \cite{xie2022vfhq} datasets. Since the backbone of SVD \cite{blattmann2023stable} is sensitive to video quality, we first evaluate each video in two datasets with the video quality assessment method FasterVQA \cite{wu2023neighbourhood}, and remove videos with scores lower than 0.6. In the end, 37,644 videos remain for training. To ensure a fair comparison in experiments, we evaluate our method on the portrait video dataset HDTF \cite{zhang2021flow} and FFHQ \cite{karras2019style}.

\noindent\textbf{Training Details.} During the training phase, for the temporal attention layers of the SVD, we sample 16-frame video sequences to establish temporal consistency, with each frame at a resolution of $512\times512$. Unlike methods such as \cite{hu2024animate,ma2024follow}, which require two separate training stages, we update all the weights of both the SVD and two adapters simultaneously. The model is trained for 30,000 steps with a batch size of 8 using gradient accumulation, optimized by 8bit-Adam \cite{kingma2014adam} with a learning rate of $1\times10^{-5}$. 
\subsection{Metrics and Comparisons}
\textbf{Evaluation Metrics.} To evaluate the performance of our method, following \cite{cai2024real}, we relight the first 100 frames of each video in the HDTF dataset. Each video is rendered with four distinct lighting conditions derived from four different lighting-effect reference faces, resulting in a total of 44,000 frames for comprehensive comparison. Following \cite{nerffacelighting}, we use an off-the-shelf estimator \cite{feng2021learning} to calculate the Lighting Error (LE). Arcface \cite{deng2019arcface} is used to measure Identity Preservation (ID) between the relit results and the original images. To assess temporal consistency, we compute LPIPS \cite{zhang2018perceptual} between adjacent frames. We further employ an image quality assessment model \cite{pyiqa} and a video quality assessment model \cite{wu2023neighbourhood} to evaluate Image Quality (IQ) and Video Quality (VQ), respectively. Additionally, Fréchet Inception Distance (FID) \cite{heusel2017gans} and Fréchet Video Distance (FVD) \cite{skorokhodov2022stylegan} are used to measure video fidelity. In addition to objective evaluation, we conduct a user study in which 17 participants rate the videos based on three criteria: Lighting Accuracy (LA-User), Identity Similarity (ID-User), and Video Quality (VQ-User). Each criterion is rated on a scale of 1 to 5: poor, fair, average, good, and excellent. Finally, we calculate the average score for each criterion across participants.

\noindent\textbf{Comparative Methods. }For the portrait relighting task, we conduct a comparative analysis between LCVD and five state-of-the-art portrait relighting methods: DPR \cite{zhou2019deep}, SMFR \cite{hou2021towards}, NFL \cite{nerffacelighting}, StyleFlow \cite{10.1145/3447648}, and DiFaReli \cite{ponglertnapakorn2023difareli}, evaluating performance on both the HDTF and FFHQ datasets. For the portrait animation task, we compare LCVD with three state-of-the-art portrait animation methods: DaGAN \cite{hong2022depth}, StyleHEAT \cite{yin2022styleheat}, and AnimateAnyone \cite{hu2024animate}, using the HDTF dataset for evaluation.
\begin{table*}[t!]
    \centering
    \caption{Quantitative comparison of portrait relighting with DPR, SMFR, NFL, StyleFlow, and DiFaReli based on objective evaluation and user study on the HDTF video dataset. The best scores are highlighted in bold, and the second-best are underlined.}
    \vspace{-2mm}
    \label{tab:compare}
    \scalebox{1.0}
    {\begin{tabular}{cccccccc||ccc}
        \hline
        &\multicolumn{7}{c}{Objective Evaluation}&\multicolumn{3}{c}{User Study} \\
        \cmidrule(r){2-8} \cmidrule(r){9-11}
        Methods & LE$\downarrow$ & ID$\uparrow$ & LPIPS$\downarrow$ & IQ$\uparrow$ & VQ$\uparrow$ & FID$\downarrow$ & FVD$\downarrow$ & LA-User$\uparrow$ & ID-User$\uparrow$ & VQ-User$\uparrow$\\
        \hline\hline
        %\multicolumn{11}{c}{\textbf{HDTF} \cite{zhang2021flow}} \\ \hline
        DPR \cite{zhou2019deep} & 0.768 & \textbf{0.730} & \underline{0.0295} & \underline{2.646} & 0.734 & \underline{44.57} & \underline{403.0} & \underline{3.423} & \underline{3.462} &\underline{3.125}\\
        SMFR \cite{hou2021towards} & \underline{0.747} & \underline{0.601} & 0.0333 & 1.057 & 0.588 & 60.50 & 551.6 & 3.047 & 2.877 & 2.604\\
        NFL \cite{nerffacelighting} & 0.784 & 0.199 & 0.0823 & 2.586 & \underline{0.766} & 96.17 & 819.3 & 2.894 & 2.553 & 2.398\\
        StyleFlow \cite{10.1145/3447648} & 0.932 & 0.474 & 0.1088 & 2.614 & 0.746 & 161.3 & 900.6 & 2.103 & 1.929 &1.563\\
        DiFaReli \cite{ponglertnapakorn2023difareli} & 0.783 & 0.531 & 0.1152 & 1.103 & 0.458 & 57.49 & 743.2 & 3.141 & 2.592 & 2.284\\
        \hline
        Ours & \textbf{0.738} & 0.585 & \textbf{0.0282} & \textbf{3.034} & \textbf{0.775} & \textbf{37.46} & \textbf{273.3} & \textbf{3.534} & \textbf{4.000} & \textbf{3.398}\\
        \hline\hline
    \end{tabular}}
    \vspace{-4mm}
\end{table*}
%\iffalse
%\begin{table}
%    \centering
%    \caption{Quantitative comparison with DPR and SMFR on the synthesized portrait videos, which are generated using our method (Ours w/o Rel.) with the lighting from the reference image itself. The best scores are highlighted in bold, and the second-best scores are underlined.}
%    \vspace{-2mm}
%    \scalebox{0.95}
%    {\begin{tabular}{cccccc}
%        \hline
%        Methods & LE\downarrow & ID\uparrow & LPIPS\downarrow & IQ\uparrow & FID\downarrow\\
%        \hline\hline
%        DPR\cite{zhou2019deep} & 0.770 & \underline{0.695} & 0.032 & 2.63 & 48.2 \\
%        SMFR\cite{hou2021towards} & \underline{0.750} & 0.581 & 0.039 & 1.05 & 63.9 \\
%        \hline
%        Ours w/o Reli. & -- & \textbf{0.837} & \textbf{0.027} & \textbf{3.04} & \textbf{34.8} \\
%        Ours & \textbf{0.738} & 0.585 & \underline{0.028} & \underline{3.03} & \underline{37.5} \\
%        \hline
%    \end{tabular}}
%    \vspace{-4mm}
%    \label{tab:self}
%\end{table}
%\fi

\begin{table}
    \centering
    \caption{Quantitative comparison of portrait relighting with NFL, StyleFlow and DiFaReli on the FFHQ dataset. The best scores are highlighted in bold, and the second-best are underlined.}
    \vspace{-2mm}
    \begin{tabular}{ccccc}
        \hline
        Methods & LE$\downarrow$ & ID$\uparrow$ & IQ$\uparrow$ & FID$\downarrow$\\
        \hline\hline
        NFL\cite{nerffacelighting} & \underline{0.892} & 0.253 & 3.020 & 118.9\\
        StyleFlow\cite{10.1145/3447648} & 1.042 & 0.485 & \underline{3.846} & 102.7\\
        DiFaReli\cite{ponglertnapakorn2023difareli} & \textbf{0.749} & \underline{0.687} & 1.591 & \textbf{25.98}\\
        \hline
        Ours & 0.938 & \textbf{0.765} & \textbf{4.465} & \underline{26.71}\\
        \hline
    \end{tabular}
    \vspace{-4mm}
    \label{tab:ffhq}
\end{table}
\begin{table}
    \centering
    \caption{Quantitative comparison of cross-identity portrait animation with DaGAN, StyleHEAT, and AnimateAnyone on the HDTF dataset. The best scores are highlighted in bold, and the second-best scores are underlined.}
    \vspace{-2mm}
    \scalebox{0.95}
    {\begin{tabular}{cccccc}
        \hline
        Methods & ID$\uparrow$ & POSE$\downarrow$ & IQ$\uparrow$ & VQ$\uparrow$ & FID$\downarrow$\\
        \hline\hline
        DaGAN\cite{hong2022depth} & 0.645 & \underline{3.935} & 1.005 & 0.528 & 107.4 \\
        StyHE.\cite{yin2022styleheat} & 0.201 & 34.58 & 1.554 & 0.612 & 149.9 \\
        AniAny.\cite{hu2024animate} & \underline{0.806} & 5.086 & \underline{2.744} & \underline{0.706} & \underline{69.85} \\
        \hline
        Ours & \textbf{0.876} & \textbf{3.805} & \textbf{3.021} & \textbf{0.717} & \textbf{49.11} \\
        \hline
    \end{tabular}}
    \vspace{-4mm}
    \label{tab:animate}
\end{table}
\subsection{Quantitative Evaluation}
In portrait video relighting, Table \ref{tab:compare} shows that our method outperforms other state-of-the-art methods in all metrics except for ID. Specifically, it improves video fidelity (FVD) by 32\%, image fidelity (FID) by 16\%, and image quality (IQ) by 14.6\% compared to the second-best method, demonstrating excellent video quality. While our method does not achieve the highest ID performance, this is because relighting in our method is applied during portrait animation, where ID information is derived only from the reference, unlike other methods that relight each frame individually. However, our method achieves the best ID performance in the user study, likely due to its higher-quality, more stable video synthesis, which visually aligns with better ID preservation. This also proves that the ID loss in our method is within an acceptable range for human perception.

Since NFL \cite{nerffacelighting}, StyleFlow \cite{10.1145/3447648}, and DiFaReli \cite{ponglertnapakorn2023difareli} are trained on the aligned FFHQ facial dataset, we compare our method on 500 FFHQ images for a fair evaluation. As shown in Table \ref{tab:ffhq}, our method outperforms the second-best method in identity preservation (ID) by 11.4\% and image quality (IQ) by 16.1\%. However, it does not achieve the best performance in lighting error (LE) and image fidelity (FID) because these methods are trained on FFHQ, while our model is trained on different video datasets, resulting in slightly lower lighting and fidelity performance. Notably, since our method is designed for video sequences and FFHQ is an image dataset, we replicate each image 16 times to form a video sequence in order to adapt the method for image testing.

In addition to portrait relighting, we use the lighting and shape from the reference image and the pose from the driving image to render shading hints, guiding our model to achieve cross-identity portrait animation, which we then evaluate. Beyond the previously mentioned metrics, we incorporate a POSE metric to assess the pose accuracy of the animated portraits, ensuring alignment with the poses in the driving video. The POSE evaluation method follows that of \cite{siarohin2021motion}, using a facial landmark detection model \cite{bulat2017far} to measure the pose error between the animated portraits and the driving portraits based on facial keypoints. As shown in Table \ref{tab:animate}, our method outperforms the other methods in all metrics, particularly achieving a 29.7\% improvement in image fidelity (FID), a 10.1\% improvement in image quality (IQ), and an 8.7\% improvement in identity preservation (ID) compared to the second-best method.
\begin{figure*}[!htbp]
	\centering
	\includegraphics[width=0.85\textwidth]{resources/exp_compare_V2.pdf}
	\caption{Qualitative comparisons with DPR \cite{zhou2019deep}, SMFR \cite{hou2021towards}, StyleFlow \cite{10.1145/3447648}, NFL \cite{nerffacelighting}, and DiFaReli \cite{ponglertnapakorn2023difareli}. The first column shows the input video frames, and the remaining columns present relighted results under various lighting conditions. Our method demonstrates more realistic performance, particularly in challenging cases such as side lighting.}
    \label{fig:compare_hdtf}
    \vspace{-0.18in}
\end{figure*}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{resources/exp_compare_ffhq_V2.pdf}
	\caption{Qualitative comparison of portrait relighting with NFL \cite{nerffacelighting}, StyleFlow \cite{10.1145/3447648}, and DiFaReli \cite{ponglertnapakorn2023difareli} on the FFHQ dataset \cite{karras2019style}. The first column shows the input FFHQ portrait images, and the remaining column display the relighted results under various lighting conditions. Our method demonstrates more realistic results.}
    \label{fig:compare_ffhq}
    \vspace{-0.15in}
\end{figure}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{resources/exp_animate.pdf}
	\caption{Qualitative comparison of cross-identity portrait animation with DaGAN \cite{hong2022depth}, StyleHEAT \cite{yin2022styleheat} and AnimateAnyone \cite{hu2024animate} on the HDTF dataset. Our method demonstrates more lifelike results.}
    \label{fig:compare_animate}
    \vspace{-0.18in}
\end{figure}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{resources/exp_ablation_module.pdf}
	\caption{Ablation study comparing the performance of our model in portrait generation under different adapter combinations. $F_s$ represents using only the shading adapter, $F_r$ represents using only the reference adapter, and $F_s + F_r$ represents using both adapters together.}
    \label{fig:ablation_module}
    \vspace{-0.25in}
\end{figure}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{resources/ablationstudy_w_V2.pdf}
	\caption{Ablation study comparing our model with varying strengths of multi-condition classifier-free guidance $\omega$. As $\omega$ increases, the relighting effect increasingly aligns with the target lighting; however, this comes at the cost of some loss of identity information and a decrease in image quality.}
    \label{fig:ablation}
    \vspace{-0.18in}
\end{figure}
\subsection{Qualitative Evaluation}
We compare our approach with previous portrait relighting methods on the HDTF dataset, including state-of-the-art face alignment-based approaches such as StyleFlow \cite{10.1145/3447648}, NFL \cite{nerffacelighting}, and DiFaReli \cite{ponglertnapakorn2023difareli}. Additionally, we compare our method with face alignment-free methods like DPR \cite{zhou2019deep} and SMFR \cite{hou2021towards}. The results are shown in Fig. \ref{fig:compare_hdtf}. We find that face alignment-based methods easily suffer from background detail loss and identity degradation, especially in pre-trained StyleGAN-based \cite{karras2020analyzing} methods like StyleFlow and NFL (e.g., see the results in the fourth and fifth columns, where the background details are completely lost, and the facial identity is inconsistent with the input). On the other hand, DiFaReli, based on a pre-trained diffusion model \cite{preechakul2021diffusion}, benefits from the DDIM inverse \cite{song2020denoising} method, which successfully reconstructs background details and preserves identity; however, it introduces noticeable artifacts on the face.

Although face alignment-free methods like DPR and SMFR achieve relighting without losing background and facial identity, the trade-off is a significant reduction in image quality, with the lighting appearing unnatural, as if a shadow has been cast over the image (e.g., in the first and second rows of the third column for SMFR). In contrast, our method in the final column greatly outperforms others in both image quality and the realism of the lighting effects. Notably, our approach accurately renders specular reflections on the face and eyes, as well as realistic shadows cast by facial muscles, while keeping identity loss within acceptable limits. The background details are also largely preserved. Overall, our approach demonstrates superior capability.

Since NFL, StyleFlow, and DiFaReli are trained on the aligned FFHQ dataset, we visualize the relighting results on FFHQ for a fair comparison. As shown in Fig. \ref{fig:compare_ffhq}, NFL and StyleFlow lose background details and alter the portrait identity. DiFaReli preserves background details but introduces facial artifacts, lowering image quality. In contrast, our method maintains background details and identity consistency, achieving optimal image quality.

Additionally, we compare our method with DaGAN, StyleHEAT, and AnimateAnyone for portrait animation. As shown in Fig. \ref{fig:compare_animate}, while DaGAN preserves the pose from the driving frame, the portrait identity differs significantly from the reference, and the image quality is low. StyleHEAT introduces distortions in cross-identity portrait animation, and although AnimateAnyone, a diffusion model guided by a reference-net, generates higher image quality, it still suffers from identity loss and occasional facial artifacts.
\subsection{Ablation Study}
\textbf{Effectiveness of Adapters.} Our method constructs intrinsic and extrinsic feature subspaces using the reference and shading adapters, respectively, enabling relightable portrait animation by merging these subspaces. We conduct an ablation study with different adapter combinations. First, when retaining only the shading adapter as shown in Fig. \ref{fig:ablation_module}, the column labeled $F_s$  illustrates that the generated portrait’s pose and lighting align with the shading hints, indicating that only the extrinsic features are transferred. When only the reference adapter is used, the column labeled $F_r$ shows that the generated portrait closely resembles the reference with only minor variations, such as blinking, indicating intrinsic feature preservation. When both adapters are used, the column labeled $F_s + F_r$ demonstrates that the generated portrait not only matches the pose and lighting of the shading hints but also maintains the identity and appearance of the reference.

\noindent\textbf{Effectiveness of Guidance Strength.} In Fig. \ref{fig:ablation}, we visualize the relighting results for different $\omega$ values. When $\omega = 2$, the lighting effect is minimal, with only small differences from the input image, resulting in good identity retention. In contrast, when $\omega = 8$, the lighting effect closely aligns with the target lighting, but this also leads to reduced image quality and some loss of identity retention. The primary reason for this phenomenon is that as $\omega$ increases, the proportion of extrinsic features grows, while the proportion of intrinsic features diminishes, resulting in a degradation of identity information from the reference image. Consequently, higher values of $\omega$ enhance lighting effects but lead to greater identity loss.

