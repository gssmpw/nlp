\section{Preliminaries}
The Latent Diffusion Model (LDM) \cite{rombach2022high} is designed to generate high-quality, diverse images based on text prompts. It performs the denoising process within the latent space of a Variational Autoencoder (VAE) \cite{esser2021taming}. During training, the input image $\mathbf{x}_0$ is first encoded into its latent representation $\mathbf{z}_0 = \mathcal{E}(\mathbf{x}_0)$, where $\mathcal{E}(\cdot)$ represents the frozen encoder. The resulting latent code $\mathbf{z}_0$ is then perturbed as follows:
\begin{equation}
	\mathbf{z}_{t}=\sqrt{\bar{\alpha}_t}\mathbf{z}_0+\sqrt{1-\bar{\alpha}_t}\bm{\epsilon},\bm{\epsilon}\in \mathcal{N}(0,\bf{\textit{I}}),
	\label{eq:diffusion}
\end{equation}
where \( \bar{\alpha}_t = \prod_{i=1}^{t} (1 - \beta_t) \) with \( \beta_t \) is the noise strength at step \( t \), and \( t \) is sampled uniformly from \( \{1, \dots, T\} \). This process is a Markov chain that adds Gaussian noise to the latent code $\mathbf{z}_0$. The denoising model \( \bm{\epsilon}_\theta \) learns the latent space distribution by optimizing the objective function using \( \mathbf{z}_t \) as input,
\begin{equation}
	\mathcal{L}_{LDM}=\mathbb{E}_{\mathbf{z}_0,\mathbf{c},\bm{\epsilon}\in \mathcal{N}(0,\mathbf{\textit{I}})}\left[ \|\bm{\epsilon}-\bm{\epsilon}_{\theta}(\mathbf{z}_t,t,\mathbf{c}) \|^2_2\right],
	\label{eq:ldm}
\end{equation}
where $\mathbf{c}$ represents the condition, which is the text embedding encoded by the CLIP \cite{radford2021learning} text encoder provided by the user.
\section{Methodology}
\label{sec:method}
\subsection{Overview}
Our pipeline for lighting controllable portrait animation consists of two stages. First, in the training phase, we construct portrait intrinsic and extrinsic feature subspaces within a pre-trained I2V model’s feature space using two adapters. Then, in the relighting and animation stage, we modify the extrinsic subspace and merge it with the intrinsic subspace to achieve relightable portrait animation, as illustrated in Fig. \ref{fig:overview}.
\begin{enumerate}
    \item \textit{Portrait Attributes Subspace Modeling Stage:} We employ an off-the-shelf model DECA \cite{feng2021learning} to encode each frame of the input video, extracting key parameters such as lighting, pose, and shape, which are then rendered as shading hints. After the shading hints and reference image are processed through the shading adapter and reference adapter, they are randomly selected, with each training iteration potentially including either one, both, or neither for composition (Sec. \ref{sec:facm}). The composed features are subsequently fed into the Stable Video Diffusion Model \cite{blattmann2023stable} for self-supervised training (Sec. \ref{sec:agvdm}). The goal of this stage is to model both the extrinsic and intrinsic feature subspaces through the joint optimization of two adapters.
    \item \textit{Relighting and Animation Stage:} We render the shading hints using the pose of the portrait from the video, the shape from the reference image, and the spherical harmonics coefficients of the target lighting. Then, we combine the outputs of the shading adapter and the reference adapter to form the conditional set and employ multi-condition classifier-free guidance to adjust the magnitude of the extrinsic feature guidance direction by modifying the strength of the guidance, thereby generating results for lighting controllable portrait animation (Sec. \ref{sec:lcpa}).
\end{enumerate}

\begin{figure*}[!htbp]
	\centering
	\includegraphics[width=1\textwidth]{resources/overview_V6.pdf}
	\caption{Overview of our pipeline for lighting controllable portrait animation. It consists of two main stages: (1) Portrait Attributes Subspace Modeling Stage: We use DECA to encode video frames and extract lighting, pose, and shape parameters, which are rendered as shading hints. After processing the shading hints and reference image through the shading adapter and reference adapter, the two features are randomly selected and fused as guidance to guide the Stable Video Diffusion Model in generating denoised video frames with consistent lighting, pose, identity, and appearance. (2) Relighting and Animation Stage: We render the shading hints using the pose of the portrait from the video, the shape from the reference image, and the spherical harmonics coefficients of the target lighting. After processing the shading hints and reference image through two adapters, we employ multi-condition classifier-free guidance to adjust the magnitude of the extrinsic feature guidance direction, enabling the generation of lighting controllable portrait animations.}
    \label{fig:overview}
    \vspace{-0.18in}
\end{figure*}
%-------------------------------------------------------------------------
\subsection{Portrait Attributes Subspace Modeling}
\label{sec:facm}
Current portrait animation methods can be driven by user-provided pose information. However, since portrait intrinsic and extrinsic features are entangled during the self-supervised training process, manipulating lighting requires modifying extrinsic features. This entanglement makes it difficult to adjust lighting independently during portrait animation. Therefore, separating portrait intrinsic and extrinsic features is a significant challenge for enabling effective lighting control in portrait animation. 

To address this, we design a shading adapter and a reference adapter to construct an extrinsic feature subspace, as well as an intrinsic feature subspace within the SVD feature space during the training phase. First, we use the parametric model $\mathbf{FLAME}$ \cite{li2017learning} as a prior to model the shape and pose attributes of human portraits.
\begin{equation}
	\mathbf{FLAME}(s,p,e)=\mathbb{R}^{|s|\times |p|\times |e|} \rightarrow \mathbb{R}^{m\times 3},
	\label{eq:flame}
\end{equation}
which takes shape coefficients $s\in \mathbb{R}^{|s|}$, pose $p\in \mathbb{R}^{|p|}$, and expression $e\in \mathbb{R}^{|e|}$ as inputs to generate the corresponding 3D face mesh. We use DECA to estimate these parameters, with the added benefit of DECA’s ability to predict second-order spherical harmonics lighting coefficients $l\in \mathbb{R}^{|l|}$. We then render the 3D face mesh using the spherical harmonics to produce a lighting-shaded face, referred to as shading hints. 

We process the input video fragment into a sequence of shading hints, which, together with the reference image, are independently transformed by the shading and reference adapters into features $\mathbf{F}_s$ and $\mathbf{F}_r$. To establish these two feature subspaces, $\mathbf{F}_s$ and $\mathbf{F}_r$ are recombined with random coefficients $\left\{\alpha, \beta|\alpha,\beta\in \left\{0,1\right\}\right\}$ and input into the SVD. This method effectively enables the SVD to explore both the extrinsic and intrinsic feature subspaces within its feature space.

\subsection{Lighting-Guided Video Diffusion Model}
\label{sec:agvdm}
We choose the Stable Video Diffusion Model (SVD) \cite{blattmann2023stable} as the prior model for our LCVD method. However, SVD is an image-guided video generation model that takes an input image $I \in \mathbb{R}^{H \times W \times 3}$. This image is first encoded by CLIP’s vision encoder \cite{radford2021learning} and passed into SVD’s cross-attention module. At the same time, the image is encoded by a Variational Autoencoder (VAE) \cite{esser2021taming} into a latent representation $\mathbf{z}_0 = \mathcal{E}(I) \in \mathbb{R}^{h \times w \times c}$. This latent $\mathbf{z}_0$ is then replicated $T$ times and concatenated along the channel dimension with noise $\mathbf{\hat{z}} \in \mathbb{R}^{T \times h \times w \times c}$, resulting in $\mathbf{z}_t \in \mathbb{R}^{T \times h \times w \times 2c}$. The resulting $\mathbf{z}_t$ is then input into a 3D UNet \cite{ronneberger2015u}, which progressively denoises the input to generate a video of $T$ frames. Here, we set $h = H/8$, $w = W/8$, and $c = 4$.

For instance, when an image of a dog walking on the street is input into the SVD, the model predicts the next $T$ frames of the dog based on the original image. As a result, the subsequent frames generated by SVD inherit the objects and lighting conditions from the original image. To eliminate the influence of the original image’s lighting on the relighting results, as shown in Fig. \ref{fig:overview}, we use a mask to remove the portrait from the latent space of the reference image.

During the training phase, we use a mask $\mathcal{M}$ to remove the portrait, compensating for the loss of identity and appearance information using the reference adapter. Additionally, we incorporate each frame’s portrait mask into the loss function, encouraging the model to focus more on the portrait region. The loss function is defined as follows:
\begin{equation}
	\mathcal{L}_p = \mathbb{E}\left[\|\left(1-\mathcal{M}\right)\left(\bm{\epsilon} -\bm{\epsilon}_\theta\left(\mathbf{z}_t,t,\mathbf{c}\right)\right) \|\right],
	\label{eq:Lp}
\end{equation}
where \(\bm{\epsilon} \sim \mathcal{N}(0,I) \in \mathbb{R}^{T\times h\times w\times c}\), and the portrait mask \(\mathcal{M} \in \left\{0,1\right\}^{T\times h\times w\times c}\). Finally, the total loss is formulated as:
\begin{equation}
	\mathcal{L} = \mathcal{L}_p + \mathcal{L}_{LDM}.
	\label{eq:Ltotal}
\end{equation}
\subsection{Lighting Controllable Portrait Animation}
\label{sec:lcpa}
%\textbf{Motion Alignment} As shown in Fig. \ref{fig:overview}, during the relighting and animation stages, we use a video to animate the reference image, ensuring that the lighting effect of the relit portrait is consistent with that of the lighting map. In the inference stage, since the portrait in the video and the reference image come from different identities, directly using the shading hints of the portrait from the video to animate the reference image would cause the generated portrait to resemble the one from the driving video. This leads to identity leakage during animation, degrading the animation quality.

%To address the issue of identity leakage in cross-identity animation, we propose a motion alignment method. First, we use DECA to extract the pose sequence \(\mathbf{P} = \{p^v_1, p^v_2, \dots, p^v_n\}\) from each frame of the driving video, along with the pose \(p^R\) and shape \(s^R\) from the reference image. Next, we calculate the relative pose offsets \(\bm{\Delta} \mathbf{P} = \{0, p^v_2 - p^v_1, \dots, p^v_n - p^v_1\}\) for each frame with respect to the first frame. Using the reference image’s pose \(p^R\) as the base pose, we then apply these relative offsets to obtain an aligned pose sequence \(\mathbf{P}^{align} = \{p^R, p^R + (p^v_2 - p^v_1), \dots, p^R + (p^v_n - p^v_1)\}\). Finally, we extract the expression coefficients \(e^v\) from the driving video and combine them with the reference image’s shape \(s^R\) and the aligned pose sequence \(\mathbf{P}^{align}\). These parameters are then input into Eq. \ref{eq:flame} to obtain \(\mathbf{FLAME}(s^R, p^{align}, e^v)\), which, along with the spherical harmonic lighting coefficients \(l\) from the lighting map, is used to render the shading hints for each frame.
%\noindent\textbf{Portrait Relighting} 
In the relighting and animation stage, we incorporate the reference image, video fragment, and target lighting. When the portrait in the reference image corresponds to the same individual as that in the video fragment, we utilize DECA to extract the pose information from the video portrait and the shape information from the reference image, subsequently rendering a sequence of shading hints based on the lighting coefficients derived from the target lighting. However, in cases where the portraits in the video and reference image do not represent the same individual, we introduce a motion alignment module to mitigate the risk of identity leakage from the video portrait, which could compromise the quality of the generated output (for further details of motion alignment, please refer to the supplementary materials).

Following this, we input the shading hints and the reference image into the shading adapter and reference adapter. Given that the portrait in the reference image inherently contains its own lighting information, directly combining features may result in the original lighting dominating the shading hints, leading to ineffective relighting. To solve this, we adopt the concept of Composer \cite{huang2023composer}. This allows us to achieve portrait lighting manipulation by adjusting the direction of the lighting guidance within the set of conditions. The formula is as follows:
\begin{equation}
\hat{\bm{\epsilon}}_\theta(\mathbf{z}_t, \mathbf{c}) = \omega \left( \bm{\epsilon}_\theta(\mathbf{z}_t, \mathbf{c}_2) - \bm{\epsilon}_\theta(\mathbf{z}_t, \mathbf{c}_1) \right) +\bm{\epsilon}_\theta(\mathbf{z}_t, \mathbf{c}_1),
\label{eq:mccfg}
\end{equation}
here, \(\mathbf{c}_1\) and \(\mathbf{c}_2\) are two sets of conditions. If a condition exists in \(\mathbf{c}_2\) but not in \(\mathbf{c}_1\), its strength is enhanced by a weight \(\omega\). The larger \(\omega\), the stronger the condition. If a condition exists in both \(\mathbf{c}_1\) and \(\mathbf{c}_2\), \(\omega\) has no effect, and the condition strength defaults to 1.0.
In this way, we can set \(\mathbf{c}_2 = \mathbf{F}_s+ \mathbf{F}_r\) and \(\mathbf{c}_1 = \mathbf{F}_r\), where \(\mathbf{F}_s\) and \(\mathbf{F}_r\) are the features from the shading adapter and reference adapter. Since \(\mathbf{F}_s\) is present in \(\mathbf{c}_2\) but not in \(\mathbf{c}_1\), we can enhance the strength of the extrinsic feature by adjusting \(\omega\). At the same time, because both \(\mathbf{c}_1\) and \(\mathbf{c}_2\) contain \(\mathbf{F}_r\), the reference image’s portrait features remain intact. This allows us to achieve relightable portrait animation by utilizing classifier-free guidance for condition combination.
 
%\subsection{Lighting Adaptive Portrait Animation}
%\label{sec:lapa}
%In addition to achieving relightable portrait animation, LCVD is also capable of lighting-adaptive portrait animation, where the lighting on the animated portrait matches that of the reference image. While it is possible to extract spherical harmonic lighting coefficients from the reference image, as described in the previous section, to ensure consistent lighting, there are often prediction errors with these coefficients, such as with highlights and shadows. These inaccuracies lead to discrepancies between the shading hint and the actual lighting of the reference image. Furthermore, adjusting \(\omega\) alone makes it difficult to accurately control the lighting intensity of the reference image, which can result in inconsistencies in the final animated lighting.

%To address this issue, we are using facial contour maps as pose information to directly animate the reference image, ensuring that the lighting on the animated portrait matches the lighting of the reference image. First, we are using MediaPipe to extract facial landmarks from each frame of video \(V\) and from the reference image. We align these landmarks using the keypoint alignment method from Follow-Your-Emoji. After alignment, we generate facial contour maps, which are then fed into the FACM. The Contour Encoder processes these maps and outputs feature maps \(F_c\).
%We are also applying **multi-condition classifier-free guidance** for this step. Unlike the previous method, which requires suppressing the lighting information from the reference image and enhancing the shading hint, lighting-adaptive portrait animation focuses on expressing both the pose information from the facial contour map and the appearance information from the reference image simultaneously. We achieve this by using \(F = \gamma F_c + \beta F_a\). We set \(c_2 = \{F_c, F_a\}\) and \(c_1 = \{\text{null}\}\) for conditional guidance, which allows us to adjust the intensities of \(F_c\) and \(F_a\) by controlling \(\omega\).This approach successfully achieves lighting-adaptive portrait animation, ensuring that the lighting of the animated portrait is consistent with that of the reference image.
