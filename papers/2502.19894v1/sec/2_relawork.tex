\section{Related Work}
\label{sec:relatedwork}
\subsection{Portrait Relighting}
Portrait relighting involves adjusting the lighting of an image or video while preserving the subjectâ€™s identity and appearance. Previous methods \cite{pandey2021total, sun2019single, zhang2021neural} utilized One-Light-at-a-Time (OLAT) systems to capture detailed geometry and reflectance, achieving realistic relighting. However, OLAT data is expensive and difficult to acquire, limiting its practicality. To overcome this, recent approaches \cite{zhou2019deep, hou2021towards} simulate multi-lighting data and train networks for relighting. Despite these efforts, simulated methods still lag behind the realism achieved by OLAT-based techniques. Additionally, learning 3D face representations from 2D images without explicit 3D supervision has now become feasible. Recent method \cite{nerffacelighting} combines neural radiance fields (NeRF) \cite{mildenhall2021nerf} with generative models like GANs \cite{karras2020analyzing} and diffusion models \cite{rombach2022high} to generate high-resolution, multi-view consistent face images. 

Another simplified strategy \cite{nestmeyer2020learning, qiu2024relitalk} involves capturing a selfie video or a sequence of images to obtain multi-view information. However, the rendering quality is highly dependent on the accuracy of the geometry, requiring sufficient viewpoints from the video. Additionally, these methods often need to be retrained for each new video, which makes them impractical. In contrast, our method achieves high-fidelity and temporally stable video portrait relighting, requiring only a single portrait image and a target lighting.


\subsection{Diffusion-based Portrait Animation}
Denoising Diffusion Models \cite{ho2020denoising,song2020denoising} are based on the idea of Markov diffusion and fits the distribution of real samples by approximating a standard normal distribution. They outperform GANs \cite{goodfellow2014generative} in sample diversity and quality, and have been successfully applied to image synthesis \cite{kawar2023imagic, avrahami2023spatext, zhang2023adding, kumari2023multi, ruiz2023dreambooth}, image editing \cite{avrahami2023blended, brooks2023instructpix2pix}, and video synthesis \cite{yang2024cogvideox, chen2024videocrafter2, blattmann2023stable}. In portrait animation, FADM \cite{zeng2023face} refines coarsely animated portraits generated by previous methods \cite{siarohin2019first, siarohin2021motion, hong2022depth} by combining 3DMM \cite{blanz1999morphable} parameters with a diffusion model to improve appearance. Follow-Your-Emoji \cite{ma2024follow} instead uses expression-aware landmarks within the Animate-Anyone framework \cite{hu2024animate} to guide the animation process.

However, while diffusion-based portrait animation methods effectively transfer poses from driving images to animate the reference image, they cannot simultaneously manipulate the lighting of the portrait in the reference image during the animation.