\section{Introduction}
\label{sec:intro}
Portrait animation has wide applications in video conferencing, virtual reality, and the film industry. With the rapid advancement of GANs \cite{goodfellow2014generative, karras2019style,karras2020analyzing} and Diffusion Models \cite{ho2020denoising,song2020denoising,rombach2022high}, existing portrait animation methods \cite{zhao2022thin, zhang2023sadtalker, guo2024liveportrait, xie2024x, chen2024echomimic} have demonstrated remarkable capabilities in generating talking faces. For instance, the state-of-the-art LivePortrait \cite{guo2024liveportrait} achieves real-time, high-fidelity portrait animation by designing better motion transformation and scaling up the talking head datasets. However, the ability of manipulating lighting during portrait animation remains under-explored, which is highly important for seamlessly blending the generated foreground portrait with backgrounds under varying lighting conditions.

In this paper, we focus on relightable portrait animation. We aim to animate a portrait in a still reference image, matching the head movement and facial expression of a driving video, at the same time, matching the lighting condition provided by users or extracted from another given portrait image. From the perspective of face attributes, we can reduce the task to preserving the intrinsic features (identity and appearance) of the reference portrait while effectively transferring the extrinsic ones (given pose and lighting) to the reference portrait. Obviously, the exact separation between intrinsic features and extrinsic features is crucial to reach our goal. A main reason why existing portrait animation methods can’t manipulate lighting is that they can't separately manipulate these two kinds of features.

In order to achieve high-fidelity relightable portrait animation, our key idea is to distinguish these two types of features by learning their feature subspaces, then maintain the intrinsic facial features and transfer external features. We observe that the image-to-video (I2V) diffusion model \cite{blattmann2023stable}, trained on a large-scale dataset encompassing a variety of portraits with different lighting, poses, identities and appearances, provides a foundation for learning the two feature subspaces. Specifically, we represent the portrait’s extrinsic attributes using shading hints rendered with the reference image’s 3D mesh, target lighting, and pose, while the intrinsic attributes are represented by the reference image. During the self-supervised training process, we design a shading adapter to map the shading hints into the extrinsic feature subspace and a reference adapter to map the reference image into the intrinsic feature subspace. By merging the features from these two subspaces, the model generates portraits with specified lighting, pose, identity, and appearance.

With the I2V diffusion model, we propose a novel \textbf{L}ighting \textbf{C}ontrollable \textbf{V}ideo \textbf{D}iffusion model (\textbf{LCVD}) to achieve high-fidelity, relightable portrait animation. First, we use an off-the-shelf model \cite{feng2021learning} to extract the 3D mesh, pose, and spherical harmonics lighting coefficients of the target portrait, which are rendered into shading hints containing lighting and pose information. In the training stage, to enable pose alignment and lighting control, we use a shading adapter to map these shading hints to the extrinsic feature subspace, representing external portrait attributes by establishing a mapping between the shading hints and the target portrait. For identity and appearance preservation, we use a reference adapter to map the reference image to the intrinsic feature subspace, representing internal portrait attributes by creating a mapping between an initial frame and subsequent frames. Finally, during the inference stage, we merge the features from the two subspaces and input them into the I2V diffusion model to generate portraits with specified lighting, pose, identity, and appearance. To further control the lighting magnitude, we employ multi-condition classifier-free guidance to emphasize the influence of the shading adapter and reduces the reference’s impact on relighting.

Our main contributions are listed as follows:
\begin{itemize}
\item We introduce the Lighting Controllable Video Diffusion model, a diffusion-based framework for relightable portrait animation, which overcomes the limitations of current portrait animation methods that fail to manipulate lighting while animate the portrait.
\item We propose a shading adapter and a reference adapter to construct feature subspaces for extrinsic and intrinsic facial features. By merging these two subspaces, the I2V model is guided to achieve relightable portrait animation.
\item Extensive experiments demonstrate that LCVD surpasses state-of-the-art methods, showing significant improvements in metrics related to lighting effects, image quality, and video consistency.
\end{itemize}
