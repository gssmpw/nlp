\section{Related Work}
\subsection{Multimodal Information Retrieval}
Early Multimodal Information Retrieval tasks focused on cross-modal retrieval of text and image~\citep{DBLP:conf/ijcai/CaoLLNZ22}, where the goal is simply to retrieve captions of everyday images \cite{lin2014microsoft,young-etal-2014-image}.
The scope has been extended to more complex scenarios, such as composed image retrieval \cite{liu2021image}, scientific contents \cite{wu-etal-2024-scimmir}, and visual documents \cite{ma-etal-2024-unifying,faysse2024colpali}.
Recent studies have been progressively exploring unified MIR settings \cite{zhou-etal-2024-marvel}.
For instance, M-BEIR \cite{DBLP:conf/eccv/WeiCCHZFRC24} integrates various image and text-related retrieval tasks, while UMRB \cite{zhang2024gme} further extends the evaluation to encompass more textual datasets and visual document retrieval \cite{faysse2024colpali}.
However, these benchmarks are constrained by their limitation to single-image queries or texts \cite{zhang2024gme}, lacking support for multi-image and interleaved contents.
We construct a new text-image interleaved retrieval benchmark to meet the demands of complex multimodal RAG scenarios.

Current strong multimodal retrievers predominantly adopt the dense retrieval paradigm, which can be categorized into two main approaches: CLIP-style dual-stream models \cite{liu2023universal,koukounas2024jinaclipv2,nussbaum2024nomic} and language model-centric architectures \cite{lin-etal-2024-preflmr,zhou-etal-2024-vista,jiang2024e5}.
\citet{wang-etal-2024-unified} proposed to compute unified multimodal embeddings from frozen LLM, which is not specifically designed for TIIR but shows potential in the multimodal context to image search task.
A concurrent work \cite{lee2024unified} also explores interleaved embeddings for multimodal document retrieval, where a task-specific hierarchical encoder is suggested to retrieve interleaved documents parsed from Wikipedia webpage.
In this work, we introduce the more generalized MLLM-based embedding model and propose a novel Matryoshka Multimodal Embedder to address the challenge of excessive visual tokens, which is crucial for TIIR.


\subsection{Multimodal Interleaved Modeling}
The modeling of interleaved text and image has been explored in various aspects, such as pre-training models \cite{NEURIPS2022_960a172b,laurenccon2024building} and corpus \cite{laurenccon2023obelics,zhu2023multimodal}.
Notably, there exists a parallel line of research focusing on unified models that simultaneously handle both interleaved representation and generation tasks \cite{pmlr-v202-koh23a,li2024improving,zou2024interfacing}.
Their experimental datasets are converted from existing multimodal generation datasets with interleaved context, \eg Visual Storytelling \cite{huang-etal-2016-visual}, and less retrieval-oriented.
Additionally, general interleaved corpus typically suffers from low knowledge density and logical coherence in image
sequence \cite{zhang20252}, which might not be suitable for constructing an interleaved retrieval benchmark.
In contrast, we build the TIIR dataset from human-curated high-quality tutorials (from wikiHow) for everyday skills, which are naturally interleaved and more informative for retrieval.