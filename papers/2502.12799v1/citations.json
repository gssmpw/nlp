[
  {
    "index": 0,
    "papers": [
      {
        "key": "DBLP:conf/ijcai/CaoLLNZ22",
        "author": "Min Cao and\nShiping Li and\nJuntao Li and\nLiqiang Nie and\nMin Zhang",
        "title": "Image-text Retrieval: {A} Survey on Recent Research and Development"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lin2014microsoft",
        "author": "Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\\'a}r, Piotr and Zitnick, C Lawrence",
        "title": "Microsoft coco: Common objects in context"
      },
      {
        "key": "young-etal-2014-image",
        "author": "Young, Peter  and\nLai, Alice  and\nHodosh, Micah  and\nHockenmaier, Julia",
        "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "liu2021image",
        "author": "Liu, Zheyuan and Rodriguez-Opazo, Cristian and Teney, Damien and Gould, Stephen",
        "title": "Image retrieval on real-life images with pre-trained vision-and-language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wu-etal-2024-scimmir",
        "author": "Wu, Siwei  and\nLi, Yizhi  and\nZhu, Kang  and\nZhang, Ge  and\nLiang, Yiming  and\nMa, Kaijing  and\nXiao, Chenghao  and\nZhang, Haoran  and\nYang, Bohao  and\nChen, Wenhu  and\nHuang, Wenhao  and\nAl Moubayed, Noura  and\nFu, Jie  and\nLin, Chenghua",
        "title": "{S}ci{MMIR}: Benchmarking Scientific Multi-modal Information Retrieval"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ma-etal-2024-unifying",
        "author": "Ma, Xueguang  and\nLin, Sheng-Chieh  and\nLi, Minghan  and\nChen, Wenhu  and\nLin, Jimmy",
        "title": "Unifying Multimodal Retrieval via Document Screenshot Embedding"
      },
      {
        "key": "faysse2024colpali",
        "author": "Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre",
        "title": "Colpali: Efficient document retrieval with vision language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhou-etal-2024-marvel",
        "author": "Zhou, Tianshuo  and\nMei, Sen  and\nLi, Xinze  and\nLiu, Zhenghao  and\nXiong, Chenyan  and\nLiu, Zhiyuan  and\nGu, Yu  and\nYu, Ge",
        "title": "{MARVEL}: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "DBLP:conf/eccv/WeiCCHZFRC24",
        "author": "Cong Wei and\nYang Chen and\nHaonan Chen and\nHexiang Hu and\nGe Zhang and\nJie Fu and\nAlan Ritter and\nWenhu Chen",
        "title": "UniIR: Training and Benchmarking Universal Multimodal Information Retrievers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2024gme",
        "author": "Zhang, Xin and Zhang, Yanzhao and Xie, Wen and Li, Mingxin and Dai, Ziqi and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Li, Wenjie and Zhang, Min",
        "title": "GME: Improving Universal Multimodal Retrieval by Multimodal LLMs"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "faysse2024colpali",
        "author": "Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre",
        "title": "Colpali: Efficient document retrieval with vision language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhang2024gme",
        "author": "Zhang, Xin and Zhang, Yanzhao and Xie, Wen and Li, Mingxin and Dai, Ziqi and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Li, Wenjie and Zhang, Min",
        "title": "GME: Improving Universal Multimodal Retrieval by Multimodal LLMs"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2023universal",
        "author": "Liu, Zhenghao and Xiong, Chenyan and Lv, Yuanhuiyi and Liu, Zhiyuan and Yu, Ge",
        "title": "Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval"
      },
      {
        "key": "koukounas2024jinaclipv2",
        "author": "Andreas Koukounas and Georgios Mastrapas and Bo Wang and Mohammad Kalim Akram and Sedigheh Eslami and Michael G\u00fcnther and Isabelle Mohr and Saba Sturua and Scott Martens and Nan Wang and Han Xiao",
        "title": "jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images"
      },
      {
        "key": "nussbaum2024nomic",
        "author": "Nussbaum, Zach and Duderstadt, Brandon and Mulyar, Andriy",
        "title": "Nomic Embed Vision: Expanding the Latent Space"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lin-etal-2024-preflmr",
        "author": "Lin, Weizhe  and\nMei, Jingbiao  and\nChen, Jinghong  and\nByrne, Bill",
        "title": "{P}re{FLMR}: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers"
      },
      {
        "key": "zhou-etal-2024-vista",
        "author": "Zhou, Junjie  and\nLiu, Zheng  and\nXiao, Shitao  and\nZhao, Bo  and\nXiong, Yongping",
        "title": "{VISTA}: Visualized Text Embedding For Universal Multi-Modal Retrieval"
      },
      {
        "key": "jiang2024e5",
        "author": "Jiang, Ting and Song, Minghui and Zhang, Zihan and Huang, Haizhen and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and Zhuang, Fuzhen",
        "title": "E5-v: Universal embeddings with multimodal large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wang-etal-2024-unified",
        "author": "Wang, Ziyang  and\nElfardy, Heba  and\nDreyer, Markus  and\nSmall, Kevin  and\nBansal, Mohit",
        "title": "Unified Embeddings for Multimodal Retrieval via Frozen {LLM}s"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lee2024unified",
        "author": "Lee, Jaewoo and Ko, Joonho and Baek, Jinheon and Jeong, Soyeong and Hwang, Sung Ju",
        "title": "Unified Multi-Modal Interleaved Document Representation for Information Retrieval"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "NEURIPS2022_960a172b",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bi\\'{n}kowski, Miko\\l aj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Kar\\'{e}n",
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning"
      },
      {
        "key": "laurenccon2024building",
        "author": "Lauren{\\c{c}}on, Hugo and Marafioti, Andr{\\'e}s and Sanh, Victor and Tronchon, L{\\'e}o",
        "title": "Building and better understanding vision-language models: insights and future directions"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "laurenccon2023obelics",
        "author": "Lauren{\\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, Leo and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander M and Kiela, Douwe and others",
        "title": "OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"
      },
      {
        "key": "zhu2023multimodal",
        "author": "Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin",
        "title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "pmlr-v202-koh23a",
        "author": "Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel",
        "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs"
      },
      {
        "key": "li2024improving",
        "author": "Li, Wei and Fan, Hehe and Wong, Yongkang and Yang, Yi and Kankanhalli, Mohan",
        "title": "Improving Context Understanding in Multimodal Large Language Models via Multimodal Composition Learning"
      },
      {
        "key": "zou2024interfacing",
        "author": "Zou, Xueyan and Li, Linjie and Wang, Jianfeng and Yang, Jianwei and Ding, Mingyu and Wei, Junyi and Yang, Zhengyuan and Li, Feng and Zhang, Hao and Liu, Shilong and others",
        "title": "Interfacing Foundation Models' Embeddings"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "huang-etal-2016-visual",
        "author": "Huang, Ting-Hao Kenneth  and\nFerraro, Francis  and\nMostafazadeh, Nasrin  and\nMisra, Ishan  and\nAgrawal, Aishwarya  and\nDevlin, Jacob  and\nGirshick, Ross  and\nHe, Xiaodong  and\nKohli, Pushmeet  and\nBatra, Dhruv  and\nZitnick, C. Lawrence  and\nParikh, Devi  and\nVanderwende, Lucy  and\nGalley, Michel  and\nMitchell, Margaret",
        "title": "Visual Storytelling"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "zhang20252",
        "author": "Zhang, Wenqi and Zhang, Hang and Li, Xin and Sun, Jiashuo and Shen, Yongliang and Lu, Weiming and Zhao, Deli and Zhuang, Yueting and Bing, Lidong",
        "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining"
      }
    ]
  }
]