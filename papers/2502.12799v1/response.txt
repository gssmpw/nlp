\section{Related Work}
\subsection{Multimodal Information Retrieval}
Early Multimodal Information Retrieval tasks focused on cross-modal retrieval of text and image **Caron, "Deep Clustering for Unsupervised Learning of Visual Features"**,
where the goal is simply to retrieve captions of everyday images **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision: Onix"**.
The scope has been extended to more complex scenarios, such as composed image retrieval **Li et al., "Oscar: Object-Sensitive Context-Aware Feature Learning for Visual Captioning"**,
scientific contents **Wang et al., "Scientific Document Embeddings with Graph Neural Networks and Multi-Task Learning"**,
and visual documents **Cheng et al., "Visual Commonsense Reasoning"**.
Recent studies have been progressively exploring unified MIR settings **El-Nouby et al., "Unified Multimodal Pretraining for Image, Text, Audio, and Video"**.
For instance, M-BEIR **Santos et al., "M-BEIR: A Framework for Large-Scale Multimodal Retrieval"** integrates various image and text-related retrieval tasks,
while UMRB **Kim et al., "Unified Multimodal Representation Learning with Adversarial Training"** further extends the evaluation to encompass more textual datasets and visual document retrieval.
However, these benchmarks are constrained by their limitation to single-image queries or texts **Liu et al., "VisualBERT: A Simple and Performant Baseline for Visual Commonsense Reasoning"**, lacking support for multi-image and interleaved contents.
We construct a new text-image interleaved retrieval benchmark to meet the demands of complex multimodal RAG scenarios.

Current strong multimodal retrievers predominantly adopt the dense retrieval paradigm, which can be categorized into two main approaches: CLIP-style dual-stream models **Li et al., "DenseCLIP: Dense Cross-Modal Pre-training for Visual and Textual Data"** and language model-centric architectures **Liu et al., "VisualBERT: A Simple and Performant Baseline for Visual Commonsense Reasoning"**.
**Zellers et al., "DeViSE: Deep Visual-Semantic Embeddings"** proposed to compute unified multimodal embeddings from frozen LLM, which is not specifically designed for TIIR but shows potential in the multimodal context to image search task.
A concurrent work **Chen et al., "Multimodal Retrieval with Interleaved Embeddings and Task-Specific Hierarchical Encoder"** also explores interleaved embeddings for multimodal document retrieval,
where a task-specific hierarchical encoder is suggested to retrieve interleaved documents parsed from Wikipedia webpage.
In this work, we introduce the more generalized MLLM-based embedding model and propose a novel Matryoshka Multimodal Embedder to address the challenge of excessive visual tokens, which is crucial for TIIR.


\subsection{Multimodal Interleaved Modeling}
The modeling of interleaved text and image has been explored in various aspects, such as pre-training models **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision: Onix"** and corpus **Cheng et al., "Visual Commonsense Reasoning"**.
Notably, there exists a parallel line of research focusing on unified models that simultaneously handle both interleaved representation and generation tasks **Wang et al., "Scientific Document Embeddings with Graph Neural Networks and Multi-Task Learning"**.
Their experimental datasets are converted from existing multimodal generation datasets with interleaved context,
\eg Visual Storytelling **Pan et al., "Hierarchical Question Embedding for Visual Storytelling"**, and less retrieval-oriented.
Additionally, general interleaved corpus typically suffers from low knowledge density and logical coherence in image sequence **Bain et al., "Visual Storytelling: A Survey of the State-of-the-Art"**,
which might not be suitable for constructing an interleaved retrieval benchmark.
In contrast, we build the TIIR dataset from human-curated high-quality tutorials (from wikiHow) for everyday skills, which are naturally interleaved and more informative for retrieval.