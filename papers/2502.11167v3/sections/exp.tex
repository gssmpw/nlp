\section{Experiments}



\subsection{Setup}

\paragraph{Models.} We tested \bench on $17$ open-source and $4$ closed-source models of different sizes, including both chat models and code models. The closed-source models include \codefont{GPT-4o}~(2024-08-06)~\citep{openai2024gpt4o}, \codefont{GPT-4o-mini}~(2024-07-18)~\citep{openai2024gpt4omini}, \codefont{Claude-3.5-Sonnet}~(2024-10-22)~\citep{anthropic2024claude35}, and \codefont{Qwen-Max}~(2025-01-25)~\citep{qwen25}. The open-source models include \codefont{LLaMA-3.1-\{8, 70\}B-Instruct}\footnote{\url{https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f}}, \codefont{LLaMA-3.3-70B-Instruct}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct}}, \codefont{Qwen-2.5-\{0.5, 1.5, 3, 7, 14, 32, 72\}B-Instruct}\footnote{\url{https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e}}, \codefont{Qwen-2.5-Coder-\{0.5, 1.5, 3, 7, 14, 32\}B-Instruct}\footnote{\url{https://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f}} and \codefont{DeepSeek-V3}\footnote{\url{https://huggingface.co/deepseek-ai/DeepSeek-V3}}~(671B).

\paragraph{Settings.} We tested the above models on \bench under 3 settings: 0-shot w/o CoT, 0-shot w/ CoT, and few-shot w/ CoT. CoT here means whether we use Chain-of-Thought~\citep{wei2022chain} prompting, allowing the models to think step by step, or ask the models to answer directly. We set the temperature to $0$, i.e. employing greedy decoding.

\subsection{Results}

Table~\ref{tab:main} presents the performance of 21 models across 8 sub-datasets of \bench under 3 different settings. The best performance for each sub-dataset in each setting is highlighted in bold. In this table, we provide a detailed breakdown of the models' performance on the ML and BG sub-datasets.

From the experimental results, several notable findings emerge:

\textbf{\bench demonstrates strong discriminative ability}, especially in certain subsets. Even the strongest models perform only moderately well, highlighting the value of our benchmark. The models exhibit significant performance differences across different subsets, reflecting the comprehensiveness of our dataset. Additionally, models that perform well in other tasks (e.g. code generation), such as \texttt{Claude-3.5-Sonnet}, also achieve substantial overall results on \bench. This demonstrates the reasonableness of our dataset and its effectiveness in benchmarking LLMs as general-purpose surrogate code executors.

\textbf{Different prompting strategies lead to varying model performance and have different effects across subsets.} We found that for the task of code execution surrogacy, both Chain-of-Thought prompting and few-shot learning can enhance model performance.

\textbf{Larger model sizes tend to yield better performance on \bench.} From the results, we observe that regardless of whether it is a Qwen-Chat model or a Qwen-Coder model, performance improves as the parameter size increases.

\textbf{Chat models and coder models exhibit different performance patterns and are affected differently by prompting strategies}. We observed that for chat and code models of the same size (e.g. \texttt{Qwen-2.5-32B-Instruct} and \texttt{Qwen-2.5-Coder-32B-Instruct}), code models outperform chat models in the zero-shot setting \bench. However, in the other two settings, chat models perform better. This suggests that code models have stronger zero-shot surrogate capabilities, whereas chat models excel in reasoning and imitation abilities.

\textbf{On some sub-datasets of \bench, stronger models perform worse than smaller, weaker ones.} For example, in the FL dataset, we found that this occurs because stronger models tend to actively look for errors in the code, often misidentifying correct code as incorrect. In contrast, smaller models are more inclined to assume that all code is error-free. Since half of the samples in this subset are indeed correct, the smaller models end up achieving better performance.

