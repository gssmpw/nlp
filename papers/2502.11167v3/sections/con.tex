\section{Conclusion}

In this paper, we introduce \bench, a holistic benchmark for evaluating LLMs as general-purpose surrogate code executors. The curated dataset spans multiple domains, bridging theoretical tasks with real-world applications. Through extensive empirical study, we argue that while current large language models possess the power to predict code execution results to a certain extent, there remains significant room for further improvements on grounding LLMs to facilitate general surrogate model. 