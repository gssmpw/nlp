\section{\bench}

To comprehensively evaluate the potential of LLMs as surrogate code executors, we construct a diverse benchmark, \bench, that covers various code execution scenarios. Our dataset is designed to assess the modelâ€™s ability to approximate execution results across multiple dimensions, including multi-lingual diversity, repository-level complexity, computational intensity, error handling, and scenario-dependent variability. Below, we describe each component of \bench, explaining the motivation behind its inclusion.

\subsection{Dataset Construction}
\label{sec:dc}

As illustrated in Figure~\ref{fig:construction}, the construction of \bench involves four distinct methodologies, each applied to specific components of our eight subsets. For \textbf{ML, CL, BG} components, we employ the Iterative Refactor methodology, where we refine the code through an interactive process involving LLM assistance and human verification. The \textbf{RL} component is constructed through Repository Sampling, where we extract and construct test cases from both public and custom code repositories. For \textbf{SC, TC, DR} components, we utilize Manual Implementation, carefully handcrafting code based on selected textbook materials and question collections. Finally, the \textbf{FL} component is developed using the Inference \& Verification approach, leveraging formal mathematical provers to generate proofs and validate them through compiler verification. These methodologies are strategically applied to their respective components to ensure a comprehensive and rigorous benchmark construction.


\subsection{Dataset Components}

\paragraph{\underline{M}ulti-\underline{l}ingual Code~(ML).}

A fundamental nature of a general-purpose surrogate executor is its ability to handle multiple programming languages. Since we are exploring the model's execution capabilities, we do not focus on rendering languages such as \codefont{HTML}, but concentrate on computational languages. Our dataset covers 7 such languages, including \codefont{C}, \codefont{C++}, \codefont{C\#}, \codefont{Java}, \codefont{Rust}, \codefont{Python}, and \codefont{Julia}. Our dataset is adapted from McEval~\citep{chai2024mcevalmassivelymultilingualcode}. The original dataset does not provide executable code, so we used an LLM to generate executable code by providing it with prompts, ground truth, and test cases in the original dataset. This generated code was then manually processed. We manually modified code that failed to compile, such as adding missing headers. And then we took precautions to prevent answers from being leaked in assert statements or comments.

\paragraph{\underline{C}ompetition-\underline{l}evel Code~(CL).}

Next, we consider competition-level code, which presents a higher level of coding difficulty. We collect these tasks from 2 public repositories~\footnote{\url{https://github.com/azl397985856/leetcode}}\footnote{\url{https://gitee.com/shanire/OJCode}}, which contain problems from open coding platforms~(e.g. LeetCode~\footnote{\url{https://leetcode.com}}, Luogu~\footnote{\url{https://www.luogu.com.cn}}). The dataset includes problems in 3 languages, \codefont{C++}, \codefont{Java}, and \codefont{JavaScript}.
%
Since the original repositories only provide partial solutions, we first use an LLM to generate complete, executable code that prints the expected output. This generated code is then manually verified. To investigate whether problem difficulty affects the performance of surrogate models, we further employ an LLM to automatically classify problems into $5$ different difficulty levels, followed by human verification to ensure accuracy.

\paragraph{\underline{R}epository-\underline{l}evel Code~(RL).}

In real-world scenarios, most code exists at the repository level, making repository-level code execution prediction equally important for a general-purpose surrogate model. We manually collect computational repositories that fit within the input length constraints of LLMs. These repositories include tasks such as solving the 24-point problem, Sudoku solving, and converting Python code to LaTeX. These tasks exhibit complex logic but do not rely on sophisticated models or external inputs. To assess the model's ability to understand multi-file structures, we also manually construct two repositories containing advanced C++ syntax and multiple files.

\paragraph{\underline{S}cientific \underline{C}omputing~(SC).}

Scientific computing has long been adopting neural surrogate models. It's crucial to investigate whether LLMs can approximate execution results for these non-trivial tasks and hold the potential to serve as efficient surrogate models. We introduced tasks ranging from solving ordinary differential equations~(ODEs) to optimization problems and signal processing. These tasks are motivated by and widely used in real-world scientific challenges, including areas where increasing research has been done on solving these computational tasks through building efficient surrogate models~\citep{DBLP:journals/natmi/WuZLW23, journals/nature/0004LCXJJ023}..

\paragraph{\underline{T}ime-\underline{C}onsuming Algorithms~(TC).}
Surrogate models were originally motivated by real-world applications where program execution is high-cost and time-consuming. It's a necessity for LLms to generalize well to strongly computation-power-dependent and time-consuming tasks. We include examples from linear algebra, sorting, searching, Monte Carlo simulations, and string matching programs, ensuring a broad representation of computationally intense tasks. These tasks cover various complexity classes, including P (e.g. sorting an array), NP (e.g. Hamiltonian Cycle), and NP-Hard (e.g. Traveling Salesman's Problem). Additionally, we record the CPU execution time for each program under consistent environments individually to support subsequent studies.

\paragraph{\underline{B}u\underline{g}gy Code~(BG).}

Real-world code execution often encounters errors due to syntax mistakes, logical flaws, or runtime exceptions. Code errors pose risks in sensitive scenarios, therefore, we aim for the code surrogate model to recognize the presence of bugs. This dataset is adapted from DebugBench~\citep{tian2024debugbenchevaluatingdebuggingcapability}, which extracted Java, Python, and C++ code from LeetCode and manually inserted errors from 4 major bug categories and 18 minor types.
%
Since DebugBench only provides erroneous code snippets rather than complete executable programs, we first used an LLM to automatically complete the error-free code into fully runnable versions. After verifying their correctness, we replaced relevant parts with buggy code and executed them again to capture the corresponding error outputs. Some errors resulted in infinite loops causing timeouts, so we set a $30$-second execution filter out such cases.

\paragraph{Code with \underline{D}ifferential \underline{R}esults under Different Scenarios~(DR).}

Various contextual factors, such as compiler versions, optimization levels, and language standards often influence code execution. These variations can lead to different outputs for the same code snippet. It is crucial for surrogate models to recognize such discrepancies and adapt to different settings.
%
We focus specifically on C++ and manually collect code snippets from textbooks and online sources~\citep{10.5555/1841497, 10.5555/2423877} that exhibit different behaviors under varying compilation settings. We consider multiple compilers (\codefont{g++}, \codefont{clang++}), C++ standards (\codefont{03}, \codefont{11}, \codefont{14}, \codefont{17}), and optimization levels (\codefont{-O0}, \codefont{-O1}, \codefont{-O2}, \codefont{-O3}, \codefont{-Os}). Each snippet is executed across these different settings, and we retain only those that produce varying outputs through different configurations while discarding cases that yield identical results across all settings.

\paragraph{Mathematics \underline{F}ormal \underline{L}anguage~(FL).}

Math-Proving Formal Languages are specialized programming languages designed for mathematical proof verification through compilers~\citep{de2015lean, moura2021lean, paulson1994isabelle, barras1997coq}. These compilers can determine whether a proof is correct and identify specific errors if present. Formal theorem proving is gaining increasing attention, as constructing a valid proof requires extensive trial and verification, which can be highly time-consuming. We aim to explore whether a surrogate model can assist in this verification process. Unlike conventional code execution, formal language verification follows a distinct paradigm, therefore we categorize this task separately.
%
In this study, we focus on Lean 4 which is the most widely used proof assistant language. To build our dataset, we use state-of-the-art formal math prover, Goedel-Prover~\citep{lin2025goedelproverfrontiermodelopensource}, to conduct large-scale reasoning on Lean-Workbook~\citep{ying2024lean}, extracting an equal proportion of correct and incorrect proofs. This balanced dataset allows us to evaluate the surrogate model's ability to assess proof validity effectively.

\begin{table*}[!ht]
\centering
\caption{Statistics of \bench, including construction methods of 8 subsets, problem sources, quantities in our dataset, number of examples in few-shot scenarios, evaluation metrics, and criteria for further classification of these subsets. In the table, ``custom" refers to customized approaches, and ``mixed" indicates multiple methods, which are elaborated in detail in the text.}
\label{tab:1}
\resizebox{\textwidth}{!}{\begin{tabular}{@{}ccccccc@{}}
\toprule
Subset & Construction Method & Source & Quantity & Example Num. & Metric & Categories \\ \midrule
ML & Iterative Refactor & McEval~\citep{chai2024mcevalmassivelymultilingualcode} & 150 & 3 & Exact Match & Languages \\
CL & Iterative Refactor & GitHub & 150 & 3 & Exact Match & Difficulties \& Languages \\
RL & Repository Sampling & GitHub \& Custom & 60 & 3 & Mixed & - \\
SC & Manual Implementation & Custom & 150 & 3 & Mixed & Scenarios \\
TC & Manual Implementation & Custom & 150 & 3 & Mixed & Scenarios, CPU Time \\
BG & Iterative Refactor & DebugBench~\citep{tian2024debugbenchevaluatingdebuggingcapability} & 150 & 4 & Jaccard similarity & Error Type \& Languages \\
DR & Manual Implementation & Custom & 200 & 3 & Jaccard similarity & Variable Type \\
FL & Inference \& Verification & Lean-Workbook~\citep{ying2024lean} & 150 & 3 & Custom & - \\ \bottomrule
\end{tabular}}
\end{table*}

\subsection{Evaluation Metrics}

We design different evaluation metrics tailored to each subset of \bench to ensure accurate evaluation. 

In \textbf{ML} and \textbf{CL}, the outputs are simple numerical values or formatted strings and contain no error or warning message, we employ exact string matching to measure correctness.

For \textbf{RL}, we employ different evaluation methods for different tasks. For structured C repositories, we use exact character matching to compare outputs. For Sudoku and 24-point problems, we use edit distance to compare results. For other types of repositories, we apply the Ratcliff/Obershelp~\citep{ratcliff1988pattern} algorithm, which measures the similarity between two sequences by finding the longest common subsequence and computing a similarity ratio.

For \textbf{SC} and \textbf{TC}, various tasks necessitate distinct evaluation methods. Specifically, (1) numerical simulations are evaluated using the average Relative Absolute Error (RAE), a widely used metric that measures the deviation between the estimated values and the ground truth values; (2) position-based tasks, such as binary search, are assessed through exact string matching; and (3) sorting tasks are evaluated by the rank correlation coefficient~\citep{spearman1904proof}, which quantifies the similarity in the ordering of elements.

For \textbf{BG}, where outputs contain error messages, we use the Jaccard similarity~\citep{jaccard1901etude} between predicted and ground truth error messages. Jaccard similarity measures the overlap between two sets and is defined as $J(A, B) = \frac{|A \cap B|}{|A \cup B|}$, where \( A \) and \( B \) represent the sets of words in the predicted and ground truth error messages respectively. Jaccard similarity well captures keywords in strings, thus suitable for error message comparison.

For \textbf{DR}, since the same code can produce different outputs in varying settings, which sometimes include warnings or errors, we again utilize Jaccard similarity. This metric appropriately handles cases where the ground truth is not an error message.


For \textbf{FL}, the results consist of two parts: (1) whether the proof passes or not, and (2) if it fails, the associated error message. The evaluation proceeds as follows. If both predicted and ground truth results indicate a successful proof, the prediction is considered correct. If one is passed and the other is not, the prediction is incorrect. If both fail, we evaluate the accuracy of the error message. The error message consists of a list containing the error locations and descriptions. We compute the score of a prediction as $\frac{1}{N} \sum_{j=1}^{N} \mathbbm{1}[{\hat{p}_j \in P}] \cdot J(\hat{m}_j, m_j)$, where \( N \) is the number of errors in the ground truth, \( P \) is the set of predicted error positions, \( p_j \) represents the \( j \)-th ground truth error position, \( \hat{p}_j \) represents the predicted error position corresponding to \( p_j \), $\mathbbm{1}[{\hat{p}_j \in P}]$ is the indicator function which equals to $1$ when there exists $\hat{p}_j \in P$ and equals to $0$ when there's not, \( m_j \) is the ground truth error message for position \( p_j \), \( \hat{m}_j \) is the predicted error message for position \( \hat{p}_j \), and \( J\) is the Jaccard similarity function.

\subsection{Dataset Statistics}

Except for RL, which has 60 questions, and DR, which has 200 questions, all other subsets have 150 questions. Our dataset contains a total of 1,160 questions. Table \ref{tab:1} presents detailed statistics of \bench, including the construction methods, problem sources, dataset quantities, number of examples in few-shot scenarios, evaluation metrics, and classification criteria for each subset.

\input{sections/maintab}