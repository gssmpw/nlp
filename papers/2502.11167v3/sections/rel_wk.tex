\section{Related Works}

\begin{figure*}[!ht]
\centering
\includegraphics[width=\linewidth]{figs/SURGE.pdf}
\caption{The Construction of \bench employs $4$ methodologies: 1. Iterative Refactor: Interactively refactoring based on original datasets through LLM and human checks, 2. Repository Sampling: Creating test cases from public/custom code repositories, 3. Manual Implementation: Handcrafting code based on textbooks and question collections, 4. Inference \& Verification: Using formal math provers to generate proofs and verify them with compilers.}
\label{fig:construction}
\end{figure*}

\paragraph{Neural Surrogate Models.} Neural surrogate models are neural network-based approximations used to replace computationally expensive simulations in various scientific and engineering domains~\citep{zhang2024artificialintelligencesciencequantum, sun2019review}. These models act as efficient emulators by learning complex input-output relationships from high-fidelity data, significantly reducing computational costs while maintaining accuracy \citep{raissi2020hidden, sun2020surrogate, bessa2017framework, thuerey2020deep, raissi2019physics, willard2022integrating}. Recently, generative models (e.g. pre-trained language models) have been incorporated into surrogate modeling. Some equip language models with traditional surrogate models to facilitate iterative optimization~\citep{ma2024llmsimulationbileveloptimizers, lyu2025adaptinglearninggroundingllms}, and some use generative models to realize the end-to-end surrogate process~\citep{gruver2024largelanguagemodelszeroshot, hao2024largelanguagemodelssurrogate, wimmer2023leveragingvisionlanguagemodelsgranular, che2024gamegenxinteractiveopenworldgame}. While these studies primarily focus on natural sciences, time series, and multimodal gaming, the application of surrogate modeling to code execution, where both input and output exist in the modality of language, remains unexplored.

\paragraph{LLMs for Code.} LLMs are widely used in code-related tasks~\citep{lu2021codexglue, zheng2023codegeex, luo2023wizardcoder, codeqwen, guo2024deepseek}, which can be fundamentally categorized into code understanding and code generation. Code understanding tasks include code summarization~\cite{hu2018deep, harer2019tree, ahmad2020transformer}, bug detection~\cite{li2018vuldeepecker, russell2018automated, zhou2019devign, chakraborty2020deep}, duplication detection~\cite{zhang2019novel, yu2019neural, wang2020detecting}, code retrieval~\citep{husain2020codesearchnetchallengeevaluatingstate, lu2021codexglue}, etc. Code generation tasks include code completion~\cite{li2017code, parvez2018building}, code repair~\cite{chen2019sequencer, chakraborty2020codit, lutellier2020coconut}, test generation~\citep{Watson_2020, Siddiq_2024, sch√§fer2023empiricalevaluationusinglarge}, etc.
%
When evaluating LLMs on these tasks, some datasets provide broad evaluations across general tasks~\citep{chen2021evaluating, austin2021program, liu2024your, muennighoff2023octopack}, some focus on specific dimensions such as multi-lingual capabilities~\citep{athiwaratkun2022multi, zheng2023codegeex, cassano2022scalable, khan2023xcodeeval}, competition problems~\citep{hendrycks2021measuring, li2022competition}, data science tasks~\citep{chandel2022training, lai2023ds, huang2022execution}, repository-level understanding~\citep{zhang2023repocoder, shrivastava2023repofusion, liu2023repobench, li2024evocodebench, jimenez2023swe, ding2024crosscodeeval, zan2024codes}, and reasoning abilities~\citep{athiwaratkun2022multi, austin2021program, cobbe2021training, gao2023pal, gu2024cruxeval}.
%
However, while the potential execution result of code is important for both code understanding and generation, this aspect remains largely unexplored~\citep{weber2024learningcompileprogramsneural}. 