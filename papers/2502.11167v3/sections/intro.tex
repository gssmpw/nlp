\section{Introduction}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/performance_radar_chart.pdf}
    \caption{Performance of $6$ typical models on $8$ subsets of \bench.}
    \label{fig:case1}
\end{figure}

Neural surrogate models~\citep{zhang2024artificialintelligencesciencequantum, sun2019review} have emerged as powerful tools in data mining and machine learning, efficiently approximating complex computational processes. Meanwhile, Large language models (LLMs)~\citep{reid2024gemini, llama3, claude3, hui2024qwen2, bi2024deepseek} have demonstrated remarkable capabilities in code-related tasks~\citep{lu2021codexglue, zheng2023codegeex, luo2023wizardcoder, codeqwen, guo2024deepseek}, including code understanding~\citep{ahmad2020transformer, chakraborty2020deep} and code generation~\citep{li2017code, parvez2018building}. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, which predict the output and behavior of a program without actually running it. Recent study~\citep{lyu2024largelanguagemodelscode} acknowledges its importance, however, it focuses on a case study rather than a systematic analysis.

The ability to predict code execution outcomes without execution has tremendous siginficance. In scientific computing, running simulations often requires substantial computational resources and is time-consuming, making it impractical to test every possible configuration~\citep{lu2019efficientsurrogatemodelingmethods, Non-intrusive, doi:10.1137/130932715}. In security-sensitive environments, executing untrusted code poses inherent risks, necessitating alternative mechanisms for assessing program behavior without exposing the system to potential vulnerabilities~\citep{nebbione2023methodological, ccs, wang2024uniquesecurityprivacythreats}. Additionally, some code requires highly specific execution environments, which may not always be available, making surrogate execution a valuable alternative~\citep{10.1145/3617591, gu2025softwaretestingextendedreality}. Moreover, accurately predicting a model’s potential outputs or errors is crucial for improving traditional tasks such as code understanding, code generation, and even math reasoning~\citep{li2025codeiocondensingreasoningpatterns}. Lastly, many works use LLMs as reward models (RMs) in reinforcement learning. For code tasks, accurate execution prediction is key to a reliable RM~\citep{ouyang2022traininglanguagemodelsfollow}.

Traditional approaches to surrogate code executing~\citep{10.1145/360248.360252, 10.1145/2408776.2408795} struggle to generalize across languages and suffer from scalability issues when applied to complex real-world codebases. Containerized environments~\citep{10.5555/2600239.2600241} mitigate dependency issues but still require full code execution. Recent efforts to train neural executors~\citep{yan2020neuralexecutionengineslearning} focus on narrow tasks and lack the generality needed for real-world code. In contrast, LLMs’ capacity to internalize patterns from vast code corpora~\citep{CodeXGLUE, codealpaca} suggests a path toward general-purpose surrogate code execution, though their limits remain unquantified. 

Therefore, it's crucial to understand the potential of LLMs as general-purpose surrogate code executors. In this paper, we try to figure out \textbf{how well current LLMs function as general-purpose surrogate code executors and whether this ability can be improved through training}.

To systematically investigate LLMs as \textbf{GE}neral-purpose \textbf{SUR}rogate code executors, we introduce \textbf{\bench}. It includes $8$ components: (1) fundamental programming tasks in multiple languages, (2) competition programming problems requiring deep logical inference, (3) repository-level codebases that test long-range dependencies, (4) scientific simulations and optimizations where direct execution is high-cost, (5) time-consuming logical algorithms that have high time-complexity, (6) buggy code that examines LLMs' ability to predict runtime errors, (7) programs whose behavior depends on specific compiler versions or execution environments and (8) math theorem proving in formal language~(LEAN4)~\citep{de2015lean, moura2021lean} which expects compilers to testify the proofs.

Through extensive empirical evaluation of 21 open-source and proprietary LLMs on \bench, we provide the first large-scale study of LLMs' capabilities as computational surrogates. Additionally, we investigate the impact of various factors including prompt engineering strategies, programming language characteristics, computational complexity, and execution time requirements on surrogate performance. Our findings reveal both the promising potential and current limitations of LLMs as general-purpose code execution surrogates. The performance of typical models on \bench is shown in Figure~\ref{fig:case1}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/performance_comparison.pdf}
    \caption{Performance scaling across model sizes and training steps.}
    \label{fig:case2}
\end{figure}

Beyond evaluating existing models, we conduct a scaling law study to determine whether models' performance improves with increased model size and data availability. Specifically, we train models with sizes ranging from \codefont{\{0.5/1.5/3/7\}B} on different scales of training data from the formal language subset of \bench and analyze how model capacity and training data scale affect surrogate execution accuracy. Our scaling experiments demonstrate that performance consistently improves with both model size (from 0.5B to 7B parameters) and training steps, with larger models showing stronger learning capacity and higher performance ceilings throughout the training process~(Figure~\ref{fig:case2}).

In short, our work makes the following key contributions:

\begin{itemize}
    \item We introduce \bench, the first but holistic benchmark for evaluating LLMs as general-purpose surrogate code executors. It consists of $8$ subsets and $1160$ problems.
    \item We conduct an extensive evaluation of $21$ open-source and proprietary LLMs on \bench, providing the first large-scale study assessing their capabilities.
    \item We present a scaling law study with models of varying sizes and data of different scales, providing empirical insights into whether LLMs systematically improve with more capacity and training data.
\end{itemize}