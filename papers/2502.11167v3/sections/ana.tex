\section{Analysis}


\begin{table*}[!t]
\caption{Model Performance Across Different Variables in DR}
\label{tab:dr}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Compiler} & \multicolumn{3}{c}{Standard} & \multicolumn{3}{c}{Optimization} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& Zero-shot & Zero-shot CoT & Few-shot CoT & Zero-shot & Zero-shot CoT & Few-shot CoT & Zero-shot & Zero-shot CoT & Few-shot CoT \\
\midrule
\texttt{Claude-3.5} & 12.92 & 18.19 & 25.70 & 14.21 & 16.75 & 23.59 & 16.06 & 1.23 & 12.90 \\
\texttt{GPT-4o} & 14.75 & 14.21 & 22.08 & 15.33 & 14.61 & 20.02 & 7.66 & 1.63 & 1.22 \\
\texttt{LLaMA-3.1-8B} & 3.97 & 10.12 & 12.05 & 4.29 & 10.60 & 13.17 & 1.96 & 3.91 & 8.04 \\
\texttt{LLaMA-3.1-70B} & 15.73 & 16.86 & 13.27 & 16.36 & 16.27 & 13.24 & 15.04 & 2.24 & 3.73 \\
\bottomrule
\end{tabular}}
\end{table*}

\subsection{The Impact of Language Type}

In Table~\ref{tab:main}, we compare model performance across different programming languages.  

In the ML subset, models perform best in Python, followed by C++. Python’s simple syntax and dynamic typing make it easier to process, while C++ benefits from its strong presence in competitive programming and system-level tasks. Models also perform well in Julia, likely due to its clean syntax and similarity to Python. However, performance drops significantly in Rust, where strict ownership and lifetime rules introduce complexity, making code harder to predict.  

In the BG subset, when predicting the output of buggy code, models excel in Python but struggle with C++. Python’s clear error messages and interpreted nature aid prediction, whereas C++'s static typing, manual memory management, and potential for undefined behavior make error handling more difficult.

\subsection{The Impact of Problem Difficulty}

To analyze how problem difficulty affects model performance, we examine results in the CL subset, as shown in Table~\ref{tab:cl}.

Across all settings, model performance generally declines as problem difficulty increases. However, at the highest difficulty level, performance improves slightly. This anomaly arises because difficulty levels are categorized based on coding complexity, but the hardest problems often involve implementing simple functionality using complex, optimized algorithms. In such cases, models can generate correct answers through end-to-end reasoning without fully understanding the underlying code logic. In contrast, levels 1–4 require logical reasoning based on code execution, making prediction harder as complexity increases.

Furthermore, while Chain-of-Thought prompting improves performance, few-shot learning does not and may even degrade results. This is likely because buggy code varies widely, causing few-shot examples to mislead rather than aid the model.

\begin{table}[!h]
\caption{Model Performance by Difficulty Level in the CL Subset}
\label{tab:cl}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
Difficulty & 1 & 2 & 3 & 4 & 5 \\
\midrule
\multicolumn{6}{c}{\textit{Zero-shot}} \\ \midrule
\texttt{Claude-3.5-Sonnet} & $90.91$ & $81.25$ & $82.05$ & $72.00$ & $79.49$ \\
\texttt{GPT-4o} & $72.73$ & $56.25$ & $58.97$ & $84.00$ & $61.54$ \\
\texttt{LLaMA-3.1-8B-Instruct} & $24.24$ & $18.75$ & $7.69$ & $4.00$ & $12.82$ \\
\texttt{LLaMA-3.1-70B-Instruct} & $96.97$ & $93.75$ & $64.10$ & $64.00$ & $79.49$ \\
\midrule
\multicolumn{6}{c}{\textit{Zero-shot  Chain-of-Thought}} \\ \midrule
\texttt{Claude-3.5-Sonnet} & $96.97$ & $87.50$ & $76.92$ & $72.00$ & $79.49$ \\
\texttt{GPT-4o} & $96.97$ & $81.25$ & $74.36$ & $80.00$ & $69.23$ \\
\texttt{LLaMA-3.1-8B-Instruct} & $57.58$ & $43.75$ & $30.77$ & $40.00$ & $38.46$ \\
\texttt{LLaMA-3.1-70B-Instruct} & $90.91$ & $56.25$ & $48.72$ & $36.00$ & $56.41$ \\
\midrule
\multicolumn{6}{c}{\textit{Few-shot Chain-of-Thought}} \\ \midrule
\texttt{Claude-3.5-Sonnet} & $96.97$ & $81.25$ & $76.92$ & $68.00$ & $84.62$ \\
\texttt{GPT-4o} & $93.94$ & $81.25$ & $71.79$ & $60.00$ & $71.79$ \\
\texttt{LLaMA-3.1-8B-Instruct} & $39.39$ & $18.75$ & $33.33$ & $16.00$ & $33.33$ \\
\texttt{LLaMA-3.1-70B-Instruct} & $87.88$ & $87.50$ & $48.72$ & $48.00$ & $76.92$ \\
\bottomrule
\end{tabular}}
\end{table}

\subsection{The Impact of Program's CPU Execution Time}

We explore the relationship between the execution time of the given code through running with CPUs and the accuracy of using large language models as surrogate models to acquire the output. As introduced in Section~\ref{sec:dc}, we recorded the CPU execution times of the programs in \textbf{TC}. We categorize problems in TC into distinct bins based on their execution times. Next, we calculated the average accuracy for each model across all samples within the same bin, as shown in Figure~\ref{fig:time}. We observed the trend of prediction accuracy of the model falling as the actual execution time required for the corresponding program prolonged. It's especially worth noting that for computational tasks that require execution time longer than 1 second, even state-of-the-art models struggle to obtain even one correct answer.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figs/time_comparison.pdf}
    \caption{Model's Performance on \textbf{TC} subset across programs with different run time on CPU.}
    \label{fig:time}
\end{figure}

\subsection{The Impact of Variable Type}

The DR subset in \bench examines model performance in predicting code behavior under different environmental factors. Specifically, DR includes variations in C++ compiler version, C++ standard, and compilation optimization settings. Table~\ref{tab:dr} detailedly presents model performance across different prompting strategies in DR.

In the zero-shot setting, model accuracy improves sequentially across the three factors, suggesting that compiler version differences are harder to predict than standard variations, which in turn are harder than optimization settings. However, with Chain-of-Thought reasoning, performance declines across all factors, with the sharpest drop for optimization settings. This indicates that while CoT aids reasoning for compiler versions and standards, it adds unnecessary complexity for optimizations, ultimately reducing accuracy.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{figs/lzc.pdf}
    \caption{Breakdown of error types across different language models and prompting methods.}
    \label{fig:ea}
\end{figure*}

\subsection{Error Analysis}

To further understand the model's performance and limitations regarding serving as general surrogate models, we categorize the Errors made by \codefont{Claude-3.5-Sonnet}, \codefont{GPT-4o} and \codefont{Llama-3.1-8B-Instruct} and \codefont{Llama-3.1-70B-Instruct}in the \textbf{CL} subset of \bench.

We first combined machine-assisted annotation and manual verification to identify $7$ most typical error types: 1. Code Language Knowledge: evaluating foundational programming language proficiency, 2. Context Awareness: measuring understanding of long text and repository-level code, 3. Conceptual Understanding: assessing comprehension of programming concepts, 4. Calculation Process: verifying computational step accuracy, 5. Code Understanding: testing comprehension of code logic and structure, 6. Data Interpretation: evaluating data processing and analysis capabilities, and 7. Calculation Accuracy: measuring precision in scientific computations. We then use LLM to label errors in the model's responses. The categorized error statistics are demonstrated in Figure ~\ref{fig:ea}.

As models prompted with CoT exhibit fewer instances of most error types compared to zero-shot, especially for the Code Understanding capability of Llama, which shows a significant improvement over zero-shot, CoT prompting results in a universal performance gain on the overall performance. The main error types are Code Understanding, Calculation Process, and Calculation Accuracy. For zero-shot, the primary error is accuracy, but for CoT, the most frequent error is Calculation Process. This suggests that CoT can better grasp the overall code logic and produce more correct results, but it may still make mistakes in the chain of thought process. In general, CoT has fewer and smaller errors. From the model perspective, Llama has a clear lead in Conceptual Understanding errors, indicating its weaker ability to understand concepts.
The Claude model has the fewest errors, showing better performance under our criteria. Moreover, the error distribution of Claude and GPT is quite similar, which may suggest they share a similar way of thinking.


\subsection{Training Scale Analysis}

Apart from benchmarking, we systematically investigate whether training scaling can affect LLMs' surrogate execution capabilities, we conducted comprehensive scaling experiments by training models of varying sizes on different amounts of training data. We conducted comprehensive scaling experiments with a dataset of 260K formal proofs
and a fixed training batch size of 128.

As demonstrated in Figure~\ref{fig:case2}, both model size and training steps are crucial factors in determining surrogate execution accuracy. As we scale from 0.5B to 7B parameters, models consistently show improved learning efficiency and higher performance ceilings throughout the training process. Larger models not only learn faster in the early stages but also continue to improve for longer before plateauing, suggesting better utilization of the training data.

These empirical observations align with established scaling laws in language modeling, indicating that surrogate execution capabilities follow similar scaling patterns as other language tasks. The consistent performance improvements with increased model size and training steps suggest that larger language models have significant potential to evolve into general-purpose surrogate code executors.