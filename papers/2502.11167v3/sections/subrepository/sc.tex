\subsection{SC}

\subsubsection{Tasks Descriptions}
\label{sec:appendix1}

The scientific computing component of \bench consists of 4 carefully curated areas, aiming to evaluate model performance on computational tasks that exhibit a time-consuming nature as well as applicational values in scientific computing areas. In this section, we provide a detailed description of each component.

\paragraph{Numerical Optimization.} In this task, the model is given a program that solves an optimization problem through gradient descent. The query may be the optimized value (\textit{min}) or the optimal point (\textit{argmin}). We carefully select four functions, which consist of: a simple quadratic function, Rosenbrock Function, Himmelblauâ€™s Function, and a polynomial function with linear constraints. For each function, we will select multiple different hyperparameter configurations to assess the model's performance. These four functions provide a systematic evaluation of the model's potential to serve as a surrogate model in this field. As the quadratic function is solvable without need the to run the gradient descent, the model may solve it through world knowledge. The Rosenbrock function is known for its narrow, curved valley containing the global minimum, making it difficult for optimization algorithms to converge. Therefore the output is highly dependent on hyperparameters (initial point, learning rate, maximum steps), thus the model must execute code in its reasoning process to acquire the answer. Himmelblau's function has multiple local minima, also posing sensitivity to hyperparameters.

\paragraph{PDE Solving.} We consider three types of Partial Differential Equations: the 1D Heat Equation, the 2D Wave Equation, and the 2D Laplace Equation. For the 1D Heat Equation, we focus on solving the following equation:
\begin{equation}
    \frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2}.
\end{equation}
For the 2D Laplace Equation, we aim to solve the equation:
\begin{equation}
    \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0.
\end{equation}
Lastly, for the 2D Wave Equation, we work on solving the following equation:
\begin{equation}
    \frac{\partial^2 u}{\partial t^2} = c^2 \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right).
\end{equation}
We solve 1D Heat Equation and 2D Wave Equation using the Explicit Finite Difference Method. For the 2D Laplace Equation, we solve it using the Gauss-Seidel Method. The model is then queried on the values of $u$ and $x$.
\paragraph{Fourier Transform (FFT)} We implement FFT using the Cooley-Tukey Algorithm and query the model to give the magnitude of the top 10 values.
\paragraph{ODE Solving} For solving ordinary differential equations, we constructed three different equations and implemented the Euler Method and the Runge-Kutta Method so solve these equations.

\subsubsection{Evaluation Metrics}
\label{app:metric}
\paragraph{Relative Absolute Error (RAE).} 
Given a scalar ground truth value \( p \) and a model prediction \( \hat{p} \), the Relative Absolute Error (RAE) is defined as:
\begin{equation}
    \text{RAE}(\hat{p}, p) = \frac{|p - \hat{p}|}{|p|}.
\end{equation}
For cases involving multiple entries, such as tensors or vectors, the following alignment procedure is applied: (1) if the prediction contains fewer elements than the ground truth, the prediction is padded with zeros until it matches the length of the ground truth; (2) if the prediction has more elements than the ground truth, it is truncated to match the ground truth length. The average RAE is then computed by averaging the RAE for each corresponding element.

\paragraph{Exact Matching.} 
For tasks involving position-based predictions, such as binary search, we adapt exact matching, as the accuracy of the algorithm is determined by comparing the exactness of the estimated result to the true result. This evaluation method checks if the estimated solution matches the ground truth exactly, typically using string or sequence matching. For such tasks, an exact match is considered a success, and any discrepancy between the ground truth and the estimate results in failure. Formally, given a string $s$ and the model's prediction $\hat{s}$, the Exact Matching is given by:
\begin{equation}
    \text{EM}(s,\hat{s})=\mathbbm{1}[s=\hat{s}]
\end{equation}
where $\mathbbm{1}[\cdot]$ is the indicator function.


\subsubsection{System Prompts}


\paragraph{Zero-shot Chain-of-Thought:}

\begin{tcolorbox}[left=0mm,right=0mm,top=0mm,bottom=0mm,boxsep=1mm,arc=0mm,boxrule=0pt, frame empty, breakable]
    \small
    \begin{lstlisting}
You are an expert in gradient_descent programming.
Please execute the above code with the input provided and return the output. You should think step by step.
Your answer should be in the following format:
Thought: <your thought>
Output: <execution result>
Please follow this format strictly and ensure the Output section contains only the required result without any additional text.
\end{lstlisting}
\end{tcolorbox}




\paragraph{Zero-shot:}

\begin{tcolorbox}[left=0mm,right=0mm,top=0mm,bottom=0mm,boxsep=1mm,arc=0mm,boxrule=0pt, frame empty, breakable]
    \small
    \begin{lstlisting}
You are an expert in gradient_descent programming.
Please execute the given code with the provided input and return the output.
Make sure to return only the output in the exact format as expected.

Output Format:
Output: <result>
\end{lstlisting}
\end{tcolorbox}




\paragraph{Few-shot Chain-of-Thought:}

\begin{tcolorbox}[left=0mm,right=0mm,top=0mm,bottom=0mm,boxsep=1mm,arc=0mm,boxrule=0pt, frame empty, breakable]
    \small
    \begin{lstlisting}
You are an expert in gradient_descent programming.
Please execute the above code with the input provided and return the output. You should think step by step.
Your answer should be in the following format:
Thought: <your thought>
Output: <execution result>
Please follow this format strictly and ensure the Output section contains only the required result without any additional text.

Here are some examples:
{{examples here}}

\end{lstlisting}
\end{tcolorbox}





\subsubsection{Demo Questions}

\begin{tcolorbox}[left=0mm,right=0mm,top=0mm,bottom=0mm,boxsep=1mm,arc=0mm,boxrule=0pt, frame empty, breakable]
    \small
    \begin{lstlisting}
code:```
import numpy as np
import argparse

def f(t, y):
    """dy/dt = -y"""
    return -y

def euler_method(f, y0, t0, t_end, h, additional_args=None):
    t_values = np.arange(t0, t_end, h)
    y_values = [y0]
    v_values = [additional_args] if additional_args is not None else [None]
    
    for t in t_values[:-1]:
        if additional_args:
            y_next, v_next = y_values[-1] + h * f(t, y_values[-1])[0], v_values[-1] + h * f(t, y_values[-1], v_values[-1])[1]
            y_values.append(y_next)
            v_values.append(v_next)
        else:
            y_next = y_values[-1] + h * f(t, y_values[-1])
            y_values.append(y_next)
    
    return t_values, np.array(y_values), np.array(v_values) if v_values[0] is not None else None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--y0", type=float, default=1.0)
    parser.add_argument("--t0", type=float, default=0.0)
    parser.add_argument("--t_end", type=float, default=10.0)
    parser.add_argument("--h", type=float, default=0.1)
    args = parser.parse_args()

    y0_1 = args.y0
    t0 = args.t0
    t_end = args.t_end
    h = args.h

    t_values, y_values, _ = euler_method(f, y0_1, t0, t_end, h)
    print(f"{y_values[-1]:.4f}")

if __name__ == "__main__":
    main()

```
command:```
python euler_3.py --y0 12 --t0 0.0 --t_end 74 --h 0.36
```
\end{lstlisting}
\end{tcolorbox}



\begin{tcolorbox}[left=0mm,right=0mm,top=0mm,bottom=0mm,boxsep=1mm,arc=0mm,boxrule=0pt, frame empty, breakable]
    \small
    \begin{lstlisting}
code:```
import numpy as np
import argparse

def gradient_descent(func, grad_func, initial_guess, learning_rate=0.1, tolerance=1e-6, max_iter=1000):
    x = initial_guess
    for _ in range(max_iter):
        grad = grad_func(x)
        x = x - learning_rate * grad
        if np.abs(grad) < tolerance:
            break
    return x, func(x)

# Function and its gradient
def func(x):
    return (x - 3)**2 + 5

def grad_func(x):
    return 2 * (x - 3)

def main(): 
    parser = argparse.ArgumentParser()
    parser.add_argument("--initial_guess", type=float, default=0.0)
    parser.add_argument("--learning_rate", type=float, default=0.1)
    parser.add_argument("--tolerance", type=float, default=1e-6)
    parser.add_argument("--max_iter", type=int, default=1000)
    args = parser.parse_args()

    # Test with initial guess
    initial_guess = args.initial_guess
    optimal_x, optimal_value = gradient_descent(func, grad_func, initial_guess)
    # optimal x
    print(f"{optimal_x:.3f}")

if __name__ == "__main__":
    main()
```
command:```
python gd_ox.py --initial_guess -5.0 --learning_rate 0.01 --max_iter 5000
```
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[left=0mm,right=0mm,top=0mm,bottom=0mm,boxsep=1mm,arc=0mm,boxrule=0pt, frame empty, breakable]
    \small
    \begin{lstlisting}
code:```
import numpy as np
import argparse

def solve_heat_eq(L, T, alpha, Nx, Nt):
    # L: length of the rod
    # T: total time
    # alpha: thermal diffusivity
    # Nx: number of spatial steps
    # Nt: number of time steps

    dx = L / (Nx - 1)
    dt = T / Nt
    r = alpha * dt / dx**2

    # Initial condition: u(x, 0) = sin(pi * x)
    x = np.linspace(0, L, Nx)
    u = np.sin(np.pi * x)

    # Time stepping
    for n in range(Nt):
        u_new = u.copy()
        for i in range(1, Nx - 1):
            u_new[i] = u[i] + r * (u[i-1] - 2*u[i] + u[i+1])
        u = u_new
    return x, u

def parse_input():
    parser = argparse.ArgumentParser(description="Solve the 1D Heat Equation")
    parser.add_argument('--L', type=float, required=True, help="Length of the rod")
    parser.add_argument('--T', type=float, required=True, help="Total time")
    parser.add_argument('--alpha', type=float, required=True, help="Thermal diffusivity")
    parser.add_argument('--Nx', type=int, required=True, help="Number of spatial points")
    parser.add_argument('--Nt', type=int, required=True, help="Number of time steps")
    return parser.parse_args()

def main():
    args = parse_input()
    x, u = solve_heat_eq(args.L, args.T, args.alpha, args.Nx, args.Nt)
    np.set_printoptions(threshold=np.inf, linewidth=np.inf)
    formatted_x = np.vectorize(lambda x: f"{x:.4e}")(x)
    print(f"{formatted_x}")

if __name__ == "__main__":
    main()

```
command:```
python heat_eq_x.py --L 36 --T 62 --alpha 91 --Nx 170 --Nt 860
```
\end{lstlisting}
\end{tcolorbox}


\begin{tcolorbox}[left=0mm,right=0mm,top=0mm,bottom=0mm,boxsep=1mm,arc=0mm,boxrule=0pt, frame empty, breakable]
    \small
    \begin{lstlisting}
code:```
import numpy as np
import argparse

def gradient_descent(func, grad_func, initial_guess, learning_rate=0.1, tolerance=1e-6, max_iter=1000):
    x = initial_guess
    for _ in range(max_iter):
        grad = grad_func(x)
        x = x - learning_rate * grad
        if np.abs(grad) < tolerance:
            break
    return x, func(x)

# Function and its gradient
def func(x):
    return (x - 3)**2 + 5

def grad_func(x):
    return 2 * (x - 3)

def main(): 
    parser = argparse.ArgumentParser()
    parser.add_argument("--initial_guess", type=float, default=0.0)
    parser.add_argument("--learning_rate", type=float, default=0.1)
    parser.add_argument("--tolerance", type=float, default=1e-6)
    parser.add_argument("--max_iter", type=int, default=1000)
    args = parser.parse_args()

    # Test with initial guess
    initial_guess = args.initial_guess
    optimal_x, optimal_value = gradient_descent(func, grad_func, initial_guess)
    # optimal x
    print(f"{optimal_x:.3f}")

if __name__ == "__main__":
    main()
```
command:```
python gd_ox.py --initial_guess -10.0 --learning_rate 0.001 --max_iter 100
```
\end{lstlisting}
\end{tcolorbox}



\begin{tcolorbox}[left=0mm,right=0mm,top=0mm,bottom=0mm,boxsep=1mm,arc=0mm,boxrule=0pt, frame empty, breakable]
    \small
    \begin{lstlisting}
code:```
import numpy as np
import argparse

# Objective function: f(x, y) = x^2 + y^2
def objective(x, y):
    return x**2 + y**2

# Gradient of the objective function: âˆ‡f(x, y) = (2x, 2y)
def gradient(x, y):
    return np.array([2 * x, 2 * y])

# Projection function onto the constraint x + y = 1
def projection(x, y):
    # Since the constraint is x + y = 1, we can project the point (x, y) onto the line
    # by solving the system: x' + y' = 1
    # Let x' = x - (x + y - 1)/2, and y' = y - (x + y - 1)/2
    adjustment = (x + y - 1) / 2
    return np.array([x - adjustment, y - adjustment])

def projected_gradient_descent(learning_rate=0.1, max_iter=1000, tolerance=1e-6, initial_guess=(0.0, 0.0)):
    x, y = initial_guess
    
    for _ in range(max_iter):
        # Compute the gradient of the objective function
        grad = gradient(x, y)
        
        # Update the variables by moving in the opposite direction of the gradient
        x, y = np.array([x, y]) - learning_rate * grad
        
        # Project the updated point onto the constraint set (x + y = 1)
        x, y = projection(x, y)
        
        # Check if the gradient is small enough to stop
        if np.linalg.norm(grad) < tolerance:
            break
    
    return x, y, objective(x, y)

def main(): 
    parser = argparse.ArgumentParser()
    parser.add_argument("--initial_guess_x", type=float, default=0.0)
    parser.add_argument("--initial_guess_y", type=float, default=0.0)
    parser.add_argument("--learning_rate", type=float, default=0.1)
    parser.add_argument("--tolerance", type=float, default=1e-6)
    parser.add_argument("--max_iter", type=int, default=1000)
    args = parser.parse_args()
    
    initial_guess = (args.initial_guess_x, args.initial_guess_y)
    optimal_x, optimal_y, optimal_value = projected_gradient_descent(args.learning_rate, args.max_iter, args.tolerance, initial_guess)
    print(f"{optimal_x:.4e}, {optimal_y:.4e}")

if __name__ == "__main__":
    main()

```
command:```
python gd_pgdx.py --initial_guess_x 32.14 --initial_guess_y 46.04 --learning_rate 0.01 --max_iter 1000
```
\end{lstlisting}
\end{tcolorbox}