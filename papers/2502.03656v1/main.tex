% Template for ICIP-2025 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf}
\usepackage{graphicx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{microtype}
%\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{colortbl}
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\DeclareMathOperator*{\argmin}{arg\,min}\let\subsubsectionautorefname\sectionautorefname
% --------------------

% Redefine names for autoref
\renewcommand{\figureautorefname}{Fig.}       % For figures
\renewcommand{\equationautorefname}{Eq.}      % For equations
\renewcommand{\sectionautorefname}{Sec.}      % For sections
\renewcommand{\subsectionautorefname}{Sec.}   % For subsections
\renewcommand{\tableautorefname}{Tab.}        % For tables

% Title.
% ------
\title{A Study in Dataset Distillation for Image Super-Resolution}
%
% Single address.
% ---------------
% 著者情報

\newcommand{\dummyfig}[1]{
  \centering
  \fbox{
    \begin{minipage}[c][0.33\textheight][c]{0.5\textwidth}
      \centering{#1}
    \end{minipage}
  }
}

  
% \name{Author(s) Name(s)}
% \address{Author Affiliation(s)}


\name{Tobias Dietz$^{\dagger}$, Brian B. Moser$^{\star,\dagger}$, Tobias Nauen$^{\star,\dagger}$, Federico Raue$^{\star}$, Stanislav Frolov$^{\star,\dagger}$, Andreas Dengel$^{\star,\dagger}$
}
\address{
$^{\dagger}$RPTU Kaiserslautern-Landau, Germany \\
$^{\star}$German Research Center for Artificial Intelligence, Germany 
}


\begin{document}
%\ninept
%
\maketitle

% -----------------------
% Abstract
% -----------------------
\begin{abstract}
Dataset distillation is the concept of condensing large datasets into smaller but highly representative synthetic samples.
While previous research has primarily focused on image classification, its application to image Super-Resolution (SR) remains underexplored. 
This exploratory work studies multiple dataset distillation techniques applied to SR, including pixel- and latent-space approaches under different aspects. 
Our experiments demonstrate that a 91.12\% dataset size reduction can be achieved while maintaining comparable SR performance to the full dataset. 
We further analyze initialization strategies and distillation methods to optimize memory efficiency and computational costs. 
Our findings provide new insights into dataset distillation for SR and set the stage for future advancements.
\end{abstract}

\begin{keywords}
Image Super-Resolution, Dataset Distillation, Dataset Pruning
\end{keywords}


\section{Introduction}
High-Resolution (HR) image reconstruction from Low-Resolution (LR) inputs, known as Super-Resolution (SR), plays a crucial role in applications such as medical imaging, surveillance, and satellite imagery \cite{moser2023hitchhiker, liu2022blind, moser2024diffusion}. 
However, training SR models requires large-scale, high-quality datasets, leading to significant storage and computational overhead \cite{ganguli2022predictability}. 
This challenge has driven interest in dataset distillation - a technique that aims to synthesize compact datasets while preserving model performance \cite{moser2024latent, cazenavette2023generalizing, cazenavette2022dataset}.

While dataset distillation has been extensively explored in classification tasks, its potential for SR remains largely unexplored, i.e., previous works focus on using synthetic images rather than distilling them explicitly for SR \cite{zhang2024gsdd, aiello2024synthetic, liu2022c}. 
Unlike classification, where distilled samples can be interpreted as condensed representations of class features, SR distillation must retain fine-grained spatial details necessary for high-quality image reconstruction. This distinction raises fundamental questions about how synthetic datasets can effectively capture the necessary information for training SR models.

This work provides an in-depth, exploratory investigation of dataset distillation in SR, comparing both pixel- and latent-space approaches to significantly reduce training data without compromising reconstruction quality. We analyze the impact of initialization and optimization strategies, examine the trade-offs between dataset compression and image fidelity, and demonstrate that a carefully designed distillation process can achieve substantial data reduction while maintaining competitive SR performance. By bridging the gap between dataset distillation and SR, this study lays the initial groundwork for future research in memory-efficient SR model training.
\section{Background}

\subsection{Image SR}
A trained Super-Resolution (SR) model $\psi_\theta: \mathbb{R}^{\text{H} \times \text{W} \times \text{C}} \rightarrow \mathbb{R}^{s\cdot\text{H} \times s\cdot\text{W} \times \text{C}}$ is optimized to approximate the inverse of a degradation process that maps a High-Resolution (HR) image $\mathbf{y} \in \mathbb{R}^{s\cdot\text{H} \times s\cdot\text{W} \times \text{C}}$ to its corresponding Low-Resolution (LR) counterpart $\mathbf{x} \in \mathbb{R}^{\text{H} \times \text{W} \times \text{C}}$, where $s > 1$ represents the scaling factor \cite{moser2023hitchhiker}.
The model parameters $\theta$ are optimized using a dataset $\mathcal{T} = (X_r, Y_r)$, a collection of LR-HR image pairs. 
The objective function for training the SR model is
\begin{equation}
    \theta^{*} = \argmin_\theta \mathbb{E}_{\mathbf{x}_i \in X_r, \mathbf{y}_i \in Y_r}\lVert \psi_\theta (\mathbf{x}_i) - \mathbf{y}_i\rVert^2.
\end{equation}

\begin{figure*}[t!]
    \begin{center}
        \includegraphics[width=.95\textwidth]{figures/main.pdf}
        \caption{\label{fig:main}Outline of DC that is adapted to work with image SR. It includes a generative model to allow for latent distillation and proposes to distill large images from a SR dataset into smaller synthetic images.}
    \end{center} 
\end{figure*}

\subsection{Dataset Distillation}
The goal of dataset distillation is to compress a real dataset $\mathcal{T} = (X_r, Y_r)$, where $X_r \in \mathbb{R}^{N \times H \times W \times C}$ and $N$ is the number of samples, into a significantly smaller synthetic dataset $\mathcal{S}=(X_s, Y_s)$ with $X_s \in \mathbb{R}^{M \times H \times W \times C}$ and $M \ll N$. 
However, the goal is not only to reduce the dataset size but also to retain the essential training quality of $\mathcal{T}$ \cite{cazenavette2023generalizing}.
In this setting, $M$ is defined as $M=\mathcal{C} \cdot IPC$, $\mathcal{C}$, where $\mathcal{C}$ is the number of classes and $IPC$ represents the Images Per Classes (IPC). 
The overall optimization for dataset distillation is formulated as
\begin{equation}\small
    \text{\small $\mathcal{S}^* = \mathop{\arg\min}\limits_{\mathcal{S}}\mathcal{L}(\mathcal{S}, \mathcal{T})$}, \label{eq:def}
\end{equation}
where $\mathcal{L}$ is a predefined objective for dataset distillation.
One objective is Dataset Condensation (DC), which aligns the gradients of classification errors \cite{zhao2020dataset}.
It computes the loss on real data ($\ell^\mathcal{T}$) and the corresponding synthetic data ($\ell^\mathcal{S}$), then minimizes the discrepancy between the gradients of both networks:
\begin{equation}
    \mathcal{L}_{DC} =1- \frac{\nabla_\theta\ell^\mathcal{S}(\theta) \cdot \nabla_\theta\ell^\mathcal{T}(\theta)}{\left\|\nabla_\theta\ell^\mathcal{S}(\theta)\right\|\left\|\nabla_\theta\ell^\mathcal{T}(\theta)\right\|}.
\end{equation}



\section{Methodology}
The first challenge is to adapt the loss function to the domain of image SR.
For our study, we propose to modify DC, as outlined in \autoref{fig:main}: Instead of using the classification error, we use the classical SR $L_2$ loss term (MSE).
More concretely, 
\begin{equation} \label{eq:mse_loss}
    \ell_{MSE} = \frac{1}{n} \sum_{i=1}^{n} \left\| \psi_{\theta}(X_{r}^{(i)}) - Y_{r}^{(i)} \right\|^2,
\end{equation}
where $\psi_{\theta}$ is an arbitrary reference SR model. 

\subsection{Adapting Class Labels for SR}
With the adapted loss function in place, another key challenge is adapting the class labels for image SR (i.e., the distillation targets). 
In classification-based distillation, class labels are not only used to compute loss terms but also to structure the data, determining how samples are grouped and synthesized. 
This dependency complicates direct application to image SR, where a class-based organization is less well-defined.

In this work, we propose to exploit the standard procedure of extracting sub-images of the respective images as part of the traditional generation of training data, as shown in \autoref{fig:datapreparation}.
Consequently, we group each of these sub-images with a label corresponding to the large original image.
All of the sub-images 1) have the same dimensions and 2) evidently belong to the same ``class''. 
We can, therefore, group multiple sub-images based on these labels and distill one synthetic sub-image with high information density, which allows for distilling one large real image into a smaller synthetic one.

In conclusion, we distill 800 synthetic images from roughly 32.000 sub-images, which results in 91.12\% dataset size reduction.

\subsection{Latent Distillation for SR}
In addition to pixel-space distillation, we investigate the use of latent dataset distillation with Generative Latent Distillation (GLaD) \cite{cazenavette2023generalizing}. 
Instead of optimizing synthetic images directly in pixel space, GLaD operates in the latent space of a generative model (e.g., StyleGAN-XL \cite{sauer2022stylegan}). 
This approach enables distillation at a higher level of abstraction, capturing complex image features while reducing the need for direct pixel-wise optimization. 
Specifically, GLaD learns latent codes that, when passed through the generator, reconstruct high-fidelity synthetic images. This method is particularly beneficial for SR, as it allows the distillation of large-scale image structures while maintaining fine-grained details.

%By integrating GLaD with traditional DC, we enhance the ability to compress large SR datasets efficiently while preserving the most relevant features necessary for high-quality SR model training. 
%The combination of pixel-space and latent-space distillation provides a flexible and scalable solution for dataset distillation in SR.
 

\begin{figure}[t!]
    \centering
    \includegraphics[width=.95\columnwidth]{figures/VisualizationDataPreparationLR.pdf}
    \caption{Illustration of the dataset preparation process. HR images are divided into overlapping sub-images using a stride to preserve edge details. During training, random patches are extracted from these sub-images to construct training batches. }
    \label{fig:datapreparation}
\end{figure}

\section{Experiments}

\subsection{Setup}
For the reference SR model $\psi_\theta$ used during dataset distillation, we chose the lightweight SRCNN \cite{dong2015image} model (randomly initialized if not stated otherwise). 
As a baseline comparison, we test the SR models SRCNN \cite{dong2015image}, VDSR \cite{kim2016accurate}, and EDSR \cite{lim2017enhanced}, trained on the distilled images, also on the full dataset.
For testing, we used the classical datasets Set5, Set14, and DIV2K (validation set) \cite{moser2023hitchhiker}.

\subsection{Pixel-Space Results}

\begin{table}[t!]
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccccc}
                &     & \multicolumn{3}{c}{Set5}      & \multicolumn{3}{c}{Set14}     & \multicolumn{3}{c}{DIV2K}                                                \\ 
                &     & \cellcolor[HTML]{C2C2C2}PSNR $\uparrow$ & \cellcolor[HTML]{C2C2C2}SSIM $\uparrow$            & \cellcolor[HTML]{C2C2C2}LPIPS $\downarrow$  & 
                        \cellcolor[HTML]{C2C2C2}PSNR $\uparrow$ & \cellcolor[HTML]{C2C2C2}SSIM $\uparrow$            & \cellcolor[HTML]{C2C2C2}LPIPS $\downarrow$  & 
                        \cellcolor[HTML]{C2C2C2}PSNR $\uparrow$    & \cellcolor[HTML]{C2C2C2}SSIM $\uparrow$            & \cellcolor[HTML]{C2C2C2}LPIPS $\downarrow$  \\ 
                        
    & \cellcolor[HTML]{C2C2C2}SRCNN & 34.8370   & 0.9394    & 0.0930    & \cellcolor[HTML]{EEEEEE}30.6087   & \cellcolor[HTML]{EEEEEE}0.8845    & \cellcolor[HTML]{EEEEEE}0.1529    & 33.3828   & 0.9269    & 0.1488 \\ 
    & \cellcolor[HTML]{C2C2C2}VDSR  & 32.0620   & 0.9127    & 0.1293    & \cellcolor[HTML]{EEEEEE}28.7188   & \cellcolor[HTML]{EEEEEE}0.8483    & \cellcolor[HTML]{EEEEEE}0.1896    & 31.2650   & 0.8970    & 0.1889 \\ 
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}Baseline\\(original)\end{tabular}}    
    & \cellcolor[HTML]{C2C2C2}EDSR  & \textbf{35.7720}      & \textbf{0.9442}   & \textbf{0.0886}   & \cellcolor[HTML]{EEEEEE}\textbf{31.5102}  & \cellcolor[HTML]{EEEEEE}\textbf{0.8937}   & \cellcolor[HTML]{EEEEEE}\textbf{0.1400} & \textbf{34.6383}     & \textbf{0.9374}   &  \textbf{0.1328}  \\ \hline
    
    & \cellcolor[HTML]{C2C2C2}SRCNN & \cellcolor[HTML]{EEEEEE}15.4093 & \cellcolor[HTML]{EEEEEE}0.4574 & \cellcolor[HTML]{EEEEEE}0.3176 & 14.9583 & 0.4890 & 0.3326 & \cellcolor[HTML]{EEEEEE}15.3261 & \cellcolor[HTML]{EEEEEE}0.4726  & \cellcolor[HTML]{EEEEEE}0.4151 \\
    & \cellcolor[HTML]{C2C2C2}VDSR  & \cellcolor[HTML]{EEEEEE}15.4176 & \cellcolor[HTML]{EEEEEE}0.4787 & \cellcolor[HTML]{EEEEEE}0.3709 & 14.9669 & 0.4739 & 0.3848 & \cellcolor[HTML]{EEEEEE}15.4559 & \cellcolor[HTML]{EEEEEE}0.4715  & \cellcolor[HTML]{EEEEEE}0.4587 \\ 
\multirow{-3}{*}{Syn. IPC=1}  & \cellcolor[HTML]{C2C2C2}EDSR  
    & \cellcolor[HTML]{EEEEEE}16.4814 & \cellcolor[HTML]{EEEEEE}\textbf{0.5132} & \cellcolor[HTML]{EEEEEE}0.4705 & 16.6783 & 0.4943 & 0.4714 & \cellcolor[HTML]{EEEEEE}17.8057 & \cellcolor[HTML]{EEEEEE}\textbf{0.5539} & \cellcolor[HTML]{EEEEEE}0.5375 \\ 
    
    & \cellcolor[HTML]{C2C2C2}SRCNN & 15.8993 & 0.4768 & \textbf{0.3110} & \cellcolor[HTML]{EEEEEE}15.5450 & \cellcolor[HTML]{EEEEEE}\textbf{0.5154} & \cellcolor[HTML]{EEEEEE}\textbf{0.3239} & 15.8124 & 0.4898 & \textbf{0.4088} \\
    & \cellcolor[HTML]{C2C2C2}VDSR  & 14.8428 & 0.4284 & 0.4090 & \cellcolor[HTML]{EEEEEE}13.4794 & \cellcolor[HTML]{EEEEEE}0.4116 & \cellcolor[HTML]{EEEEEE}0.4492 & 13.6287 & 0.4074 & 0.5181                         \\
\multirow{-3}{*}{Syn. IPC=10} & \cellcolor[HTML]{C2C2C2}EDSR  
    & \textbf{17.1089} & 0.4706 & 0.4519 & \cellcolor[HTML]{EEEEEE}\textbf{17.2534} & \cellcolor[HTML]{EEEEEE}0.4739 & \cellcolor[HTML]{EEEEEE}0.4601 & \textbf{18.2324} & 0.5005 & 0.5389   
    
\end{tabular}
}
\caption{Quantitative $2\times$ results using random initializations for dataset distillation with SRCNN. Best values for original data and synthetic data are
marked with bold letters, respectively.}
\label{table:noiseResultsSRCNN}
\end{table}


\textbf{Initialization Matters.} 
As shown in \autoref{table:noiseResultsSRCNN}, starting the distillation process with random noise images fails to produce meaningful training images for SR models. 
As a result, training on them leads to substantial drops in PSNR/SSIM scores across all datasets. 
Notably, with IPC=1, the performance degrades severely, with PSNR dropping by more than 15 dB compared to the baseline. 
Even with IPC=10, the improvement remains marginal, indicating that naive random initialization is insufficient for learning effective SR training images.

\begin{table}[t]
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccccc}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{Set5} & \multicolumn{3}{c}{Set14} & \multicolumn{3}{c}{DIV2K} \\

\multicolumn{1}{c}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c}{\cellcolor[HTML]{B2B2B2}PSNR $\uparrow$} & \multicolumn{1}{c}{\cellcolor[HTML]{B2B2B2}SSIM $\uparrow$} & \multicolumn{1}{c}{\cellcolor[HTML]{B2B2B2}LPIPS $\downarrow$} & \cellcolor[HTML]{B2B2B2}PSNR $\uparrow$ & \cellcolor[HTML]{B2B2B2}SSIM $\uparrow$ & \cellcolor[HTML]{B2B2B2}LPIPS $\downarrow$ & \cellcolor[HTML]{B2B2B2}PSNR $\uparrow$ & \cellcolor[HTML]{B2B2B2}SSIM $\uparrow$ & \cellcolor[HTML]{B2B2B2}LPIPS $\downarrow$ \\ 

\multicolumn{1}{c}{} & \multicolumn{1}{c}{\cellcolor[HTML]{C2C2C2}SRCNN} & \multicolumn{1}{c}{34.8370} & \multicolumn{1}{c}{0.9394} & \multicolumn{1}{c}{0.0930} & \cellcolor[HTML]{EEEEEE}30.6087 & \cellcolor[HTML]{EEEEEE}0.8845 & \cellcolor[HTML]{EEEEEE}0.1529 & 33.3828 & 0.9269 & 0.1488 \\ 

\multicolumn{1}{c}{} & \multicolumn{1}{c}{\cellcolor[HTML]{C2C2C2}VDSR}  & \multicolumn{1}{c}{32.0620} & \multicolumn{1}{c}{0.9127} & \multicolumn{1}{c}{0.1293} & \cellcolor[HTML]{EEEEEE}28.7188 & \cellcolor[HTML]{EEEEEE}0.8483 & \cellcolor[HTML]{EEEEEE}0.1896 & 31.2650 & 0.8970 & 0.1889 \\ 

\multicolumn{1}{c}{\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}Baseline\\(original)\end{tabular}}} & \cellcolor[HTML]{C2C2C2}EDSR & \textbf{35.7720} & \textbf{0.9442} & \textbf{0.0886} & \cellcolor[HTML]{EEEEEE}\textbf{31.5102} & \cellcolor[HTML]{EEEEEE}\textbf{0.8937} & \cellcolor[HTML]{EEEEEE}\textbf{0.1400} & \textbf{34.6383} & \textbf{0.9374} & \textbf{0.1328} \\ 

& \cellcolor[HTML]{C2C2C2}SRCNN & \cellcolor[HTML]{EEEEEE}34.7127 & \cellcolor[HTML]{EEEEEE}0.9385 & \cellcolor[HTML]{EEEEEE}0.0942 & 30.6389 & 0.8844 & 0.1532 & \cellcolor[HTML]{EEEEEE}33.3287 & \cellcolor[HTML]{EEEEEE}0.9262 & \cellcolor[HTML]{EEEEEE}0.1522 \\

& \cellcolor[HTML]{C2C2C2}VDSR & \cellcolor[HTML]{EEEEEE}32.0619 & \cellcolor[HTML]{EEEEEE}0.9127 & \cellcolor[HTML]{EEEEEE}0.1293 & 28.7188 & 0.8483 & 0.1896 & \cellcolor[HTML]{EEEEEE}31.2650 & \cellcolor[HTML]{EEEEEE}0.8970 & \cellcolor[HTML]{EEEEEE}0.1889 \\

\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}Baseline\\(downscaled)\end{tabular}} & \cellcolor[HTML]{C2C2C2}EDSR & \cellcolor[HTML]{EEEEEE}35.2664 & \cellcolor[HTML]{EEEEEE}0.9421 & \cellcolor[HTML]{EEEEEE}0.0898 & 30.8937 & 0.8853 & 0.1482 & \cellcolor[HTML]{EEEEEE}33.8184 & \cellcolor[HTML]{EEEEEE}0.9312 & \cellcolor[HTML]{EEEEEE}0.1388 \\\hline 

& \cellcolor[HTML]{C2C2C2}SRCNN & \textbf{33.9813} & 0.9349 & 0.0973 & \cellcolor[HTML]{EEEEEE}\textbf{30.2628} & \cellcolor[HTML]{EEEEEE}0.8812 & \cellcolor[HTML]{EEEEEE}0.1561 & \textbf{32.8649} & 0.9230 & 0.1544 \\

& \cellcolor[HTML]{C2C2C2}VDSR & 33.6614 & \textbf{0.9361} & 0.1002 & \cellcolor[HTML]{EEEEEE}30.1338 & \cellcolor[HTML]{EEEEEE}\textbf{0.8832} & \cellcolor[HTML]{EEEEEE}0.1579 & 32.7802 & \textbf{0.9256} & 0.1525 \\

\multirow{-3}{*}{Syn. IPC=1} & \cellcolor[HTML]{C2C2C2}EDSR & 33.6500 & 0.9352 & \textbf{0.0920} & \cellcolor[HTML]{EEEEEE}29.9498 & \cellcolor[HTML]{EEEEEE}0.8803 & \cellcolor[HTML]{EEEEEE}\textbf{0.1526} & 32.6784 & 0.9235 & \textbf{0.1465}                                 
\end{tabular}
}
\caption{Quantitative $2\times$ results using downscaled images to initialize dataset distillation with SRCNN. Best values for original data (downscaled) and synthetic data are marked with bold letters, respectively.}
\label{table:downscaledInitResults}
\end{table}

\textbf{Initialize With Downscaled Images.} 
By downscaling the original images for initialization, we provide a more structured starting point for dataset distillation. 
As shown in \autoref{table:downscaledInitResults}, models trained on downscaled images achieve performance closer to the original dataset than randomly initialized synthetic datasets. 
This suggests that random initialization leads to suboptimal local minima, limiting the effectiveness of the distilled dataset. 
Interestingly, while downscaled images alone yield reasonable results, their distilled counterparts do not outperform this baseline. 
This highlights the need for more refined distillation methods to leverage downscaled initializations better while preserving high-frequency details.

\textbf{Pre-Trained Network During Distillation.}
To better capture the underlying data distribution, we explore using a pre-trained SR model instead of a randomly initialized network during dataset distillation.
The rationale is that a pre-trained model, having already learned relevant image structures and features, may lead to more informative synthetic images.
However, as shown in \autoref{table:downscaledInitPretrainedResults}, the results are mixed. 
While the synthetic dataset distilled with a pre-trained model achieves higher PSNR, better SSIM and LPIPS values are scored with a randomly initialized model.
These findings indicate that using a pre-trained model does not consistently outperform a randomly initialized model regarding final SR quality. 
Despite the ambiguous results, further experiments incorporate a pre-trained SRCNN to investigate its long-term advantages in distillation.

\begin{table}[t]
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccccc}
    & & \multicolumn{3}{c}{Set5} & \multicolumn{3}{c}{Set14} & \multicolumn{3}{c}{DIV2K} \\
    & & \cellcolor[HTML]{B2B2B2}PSNR $\uparrow$ & \cellcolor[HTML]{B2B2B2}SSIM $\uparrow$ & \cellcolor[HTML]{B2B2B2}LPIPS $\downarrow$ & \cellcolor[HTML]{B2B2B2}PSNR $\uparrow$ & \cellcolor[HTML]{B2B2B2}SSIM $\uparrow$ & \cellcolor[HTML]{B2B2B2}LPIPS $\downarrow$ & \cellcolor[HTML]{B2B2B2}PSNR $\uparrow$ & \cellcolor[HTML]{B2B2B2}SSIM $\uparrow$ & \cellcolor[HTML]{B2B2B2}LPIPS $\downarrow$ \\

    & \cellcolor[HTML]{C2C2C2}SRCNN & 34.8370 & 0.9394 & 0.0930 & \cellcolor[HTML]{EEEEEE}30.6087 & \cellcolor[HTML]{EEEEEE}0.8845 & \cellcolor[HTML]{EEEEEE}0.1529 & 33.3828 & 0.9269 & 0.1488 \\
    & \cellcolor[HTML]{C2C2C2}VDSR  & 32.0620 & 0.9127 & 0.1293 & \cellcolor[HTML]{EEEEEE}28.7188 & \cellcolor[HTML]{EEEEEE}0.8483 & \cellcolor[HTML]{EEEEEE}0.1896 & 31.2650 & 0.8970 & 0.1889 \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}Baseline\\(original)\end{tabular}}
    & \cellcolor[HTML]{C2C2C2}EDSR  & \textbf{35.7720} & \textbf{0.9442} & \textbf{0.0886} & \cellcolor[HTML]{EEEEEE}\textbf{31.5102} & \cellcolor[HTML]{EEEEEE}\textbf{0.8937} & \cellcolor[HTML]{EEEEEE}\textbf{0.1400} & \textbf{34.6383} & \textbf{0.9374} & \textbf{0.1328} \\\hline

    & \cellcolor[HTML]{C2C2C2}SRCNN & \cellcolor[HTML]{EEEEEE}33.9813 & \cellcolor[HTML]{EEEEEE}0.9349 & \cellcolor[HTML]{EEEEEE}0.0973 & 30.2628 & 0.8812 & 0.1561 & \cellcolor[HTML]{EEEEEE}32.8649 & \cellcolor[HTML]{EEEEEE}0.9230 & \cellcolor[HTML]{EEEEEE}0.1544 \\
    & \cellcolor[HTML]{C2C2C2}VDSR  & \cellcolor[HTML]{EEEEEE}33.6614 & \cellcolor[HTML]{EEEEEE}\textbf{0.9361} & \cellcolor[HTML]{EEEEEE}0.1002 & 30.1338 & \textbf{0.8832} & 0.1579 & \cellcolor[HTML]{EEEEEE}32.7802 & \cellcolor[HTML]{EEEEEE}\textbf{0.9256} & \cellcolor[HTML]{EEEEEE}0.1525 \\ 
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}Syn. IPC=1\\(random)\end{tabular}}   
    & \cellcolor[HTML]{C2C2C2}EDSR  & \cellcolor[HTML]{EEEEEE}33.6500 & \cellcolor[HTML]{EEEEEE}0.9352 & \cellcolor[HTML]{EEEEEE}\textbf{0.0920} & 29.9498 & 0.8803 & \textbf{0.1526} & \cellcolor[HTML]{EEEEEE}32.6784 & \cellcolor[HTML]{EEEEEE}0.9235 & \cellcolor[HTML]{EEEEEE}\textbf{0.1465} \\

    & \cellcolor[HTML]{C2C2C2}SRCNN & 33.9001 & 0.9321  & 0.1026 & \cellcolor[HTML]{EEEEEE}30.1633 & \cellcolor[HTML]{EEEEEE}0.8795 & \cellcolor[HTML]{EEEEEE}0.1623 & 32.7271 & 0.9198 & 0.1591 \\
    & \cellcolor[HTML]{C2C2C2}VDSR  & \textbf{33.9951}  & 0.9336 & 0.0995  & \cellcolor[HTML]{EEEEEE}\textbf{30.4137} & \cellcolor[HTML]{EEEEEE}0.8818 & \cellcolor[HTML]{EEEEEE}0.1559 & \textbf{32.9882} & 0.9228 & 0.1534 \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}Syn. IPC=1\\(pre-trained)\end{tabular}}
    & \cellcolor[HTML]{C2C2C2}EDSR  & 33.9310 & 0.9332 & 0.0961 & \cellcolor[HTML]{EEEEEE}30.2509 & \cellcolor[HTML]{EEEEEE}0.8805 & \cellcolor[HTML]{EEEEEE}0.1530 & 32.8899  & 0.9212 & 0.1529 
    
\end{tabular}
}
\caption{Quantitative $2\times$ results using a pre-trained SRCNN during dataset distillation. Best values for original data and synthetic data are
marked with bold letters, respectively.}
\label{table:downscaledInitPretrainedResults}
\end{table}

\begin{table}[t]
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccccccccc}
    & & \multicolumn{3}{c}{Set5}    & \multicolumn{3}{c}{Set14} & \multicolumn{3}{c}{DIV2K}                                                                     \\
    & & \cellcolor[HTML]{B2B2B2}PSNR $\uparrow$ & \cellcolor[HTML]{B2B2B2}SSIM $\uparrow$   & \cellcolor[HTML]{B2B2B2}LPIPS $\downarrow$  
      & \cellcolor[HTML]{B2B2B2}PSNR $\uparrow$ & \cellcolor[HTML]{B2B2B2}SSIM $\uparrow$            & \cellcolor[HTML]{B2B2B2}LPIPS $\downarrow$          
      & \cellcolor[HTML]{B2B2B2}PSNR $\uparrow$    & \cellcolor[HTML]{B2B2B2}SSIM  $\uparrow$   & \cellcolor[HTML]{B2B2B2}LPIPS $\downarrow$  \\
      
    & \cellcolor[HTML]{C2C2C2}SRCNN & 34.8370   & 0.9394    & 0.0930    & \cellcolor[HTML]{EEEEEE}30.6087   & \cellcolor[HTML]{EEEEEE}0.8845    & \cellcolor[HTML]{EEEEEE}0.1529   & 33.3828   & 0.9269    & 0.1488    \\
    & \cellcolor[HTML]{C2C2C2}VDSR  & 32.0620   & 0.9127    & 0.1293    & \cellcolor[HTML]{EEEEEE}28.7188   & \cellcolor[HTML]{EEEEEE}0.8483    & \cellcolor[HTML]{EEEEEE}0.1896   & 31.2650   & 0.8970    & 0.1889    \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}l@{}}Baseline \\ (original)\end{tabular}}     
    & \cellcolor[HTML]{C2C2C2}EDSR  & \textbf{35.7720} & \textbf{0.9442}   & \textbf{0.0886} & \cellcolor[HTML]{EEEEEE}\textbf{31.5102} & \cellcolor[HTML]{EEEEEE}\textbf{0.8937} & \cellcolor[HTML]{EEEEEE}\textbf{0.1400} & \textbf{34.6383}  & \textbf{0.9374}   & \textbf{0.1328}   \\ 
    
    & \cellcolor[HTML]{C2C2C2}SRCNN & \cellcolor[HTML]{EEEEEE}34.2725 & \cellcolor[HTML]{EEEEEE}0.9361 & \cellcolor[HTML]{EEEEEE}0.0966 & 30.2008 & 0.8800 & 0.1590 & \cellcolor[HTML]{EEEEEE}32.9720 & \cellcolor[HTML]{EEEEEE}0.9225 & \cellcolor[HTML]{EEEEEE}0.1560 \\
    & \cellcolor[HTML]{C2C2C2}VDSR  & \cellcolor[HTML]{EEEEEE}32.0619 & \cellcolor[HTML]{EEEEEE}0.9127 & \cellcolor[HTML]{EEEEEE}0.1293 & 28.7188 & 0.8483 & 0.1896 & \cellcolor[HTML]{EEEEEE}31.2650 & \cellcolor[HTML]{EEEEEE}0.8970 & \cellcolor[HTML]{EEEEEE}0.1889 \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}l@{}}Baseline \\ (GAN inversion)\end{tabular}} 
    & \cellcolor[HTML]{C2C2C2}EDSR  & \cellcolor[HTML]{EEEEEE}34.9169 & \cellcolor[HTML]{EEEEEE}0.9399 & \cellcolor[HTML]{EEEEEE}0.0903 & 30.5112 & 0.8834 & 0.1477 & \cellcolor[HTML]{EEEEEE}33.4698 & \cellcolor[HTML]{EEEEEE}0.9272 & \cellcolor[HTML]{EEEEEE}0.1435 \\\hline
    
    & \cellcolor[HTML]{C2C2C2}SRCNN & 34.3706  & 0.9360  & 0.0941 & \cellcolor[HTML]{EEEEEE}30.3550 & \cellcolor[HTML]{EEEEEE}0.8817 & \cellcolor[HTML]{EEEEEE}0.1553 & 33.0254 & 0.9231 & 0.1534 \\
    & \cellcolor[HTML]{C2C2C2}VDSR  & 32.0621  & 0.9127  & 0.1293 & \cellcolor[HTML]{EEEEEE}28.7188 & \cellcolor[HTML]{EEEEEE}0.8483 & \cellcolor[HTML]{EEEEEE}0.1896 & 31.2650 & 0.8970 & 0.1889 \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}l@{}} Latent \\Distillation \end{tabular}}   
    & \cellcolor[HTML]{C2C2C2}EDSR  & \textbf{34.9590} & \textbf{0.9399} & \textbf{0.0889} & \cellcolor[HTML]{EEEEEE}\textbf{30.7149} & \cellcolor[HTML]{EEEEEE}\textbf{0.8846} & \cellcolor[HTML]{EEEEEE}\textbf{0.1458} & \textbf{33.5222} & \textbf{0.9274} & \textbf{0.1429}               
\end{tabular}
}
\caption{Quantitative results of SR models trained on the GAN inversion baseline and a synthetic dataset distilled with latent space distillation for x2 upscaling. For latent distillation, latent codes from GAN inversion are used as initialization and further optimized using a pretrained SRCNN to match its gradients. We compare both evaluations with the original baseline. Best values for original data and synthetic data are respectively marked with bold letters.}
\label{table:latentdistillationResults}
\end{table}

\subsection{Latent-Space Results}
In this experiment, we apply latent distillation using a generative model, requiring a different setup from previous experiments. 
Unlike pixel-based distillation, which operates on 192×192 patches, latent distillation is constrained by the resolution of the generative model.
Here, we use StyleGAN-XL \cite{sauer2022stylegan} to generate 512×512 synthetic sub-images, shifting the objective from learning synthetic patches to learning optimal synthetic latent codes \cite{moser2024latent, cazenavette2023generalizing}.

To initialize the latent codes, we employ GAN Inversion with Pivotal Tuning \cite{roich2022pivotal} to leverage the internal knowledge of a pre-trained StyleGAN-XL model trained on ImageNet. 
This approach enables meaningful initialization without requiring a dedicated DIV2K-trained StyleGAN. 
Although GAN Inversion does not yield perfect reconstructions, it provides a strong starting point for the distillation process. 
To make computation feasible, we reduce the number of inversion iterations, ensuring efficiency while retaining high-quality initializations.

As shown in \autoref{table:latentdistillationResults}, models trained on latent-distilled synthetic datasets using GLaD and DC outperform those trained on direct GAN inversions and previous synthetic datasets. 
This demonstrates that latent-space distillation retains key features necessary for SR training, bridging the gap between synthetic and real datasets, outperforming the downscaled baseline for SRCNN and VDSR. 

\begin{table}[t]
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccccc}
    & \multicolumn{3}{c}{Set 5} & \multicolumn{3}{c}{Set 14} & \multicolumn{3}{c}{DIV2K} \\
    \multicolumn{1}{c|}{}   & \cellcolor[HTML]{B2B2B2}PSNR  & \cellcolor[HTML]{B2B2B2}SSIM & \cellcolor[HTML]{B2B2B2}LPIPS 
                            & \cellcolor[HTML]{B2B2B2}PSNR  & \cellcolor[HTML]{B2B2B2}SSIM & \cellcolor[HTML]{B2B2B2}LPIPS  
                            & \cellcolor[HTML]{B2B2B2}PSNR  & \cellcolor[HTML]{B2B2B2}SSIM & \cellcolor[HTML]{B2B2B2}LPIPS   \\ 
    
    \multicolumn{1}{c|}{\cellcolor[HTML]{B2B2B2}\begin{tabular}[c]{@{}c@{}}Baseline \\ (original)\end{tabular}} 
        & 30.0909  & 0.8640 & 0.2095 & \cellcolor[HTML]{EEEEEE}26.5687 & \cellcolor[HTML]{EEEEEE}0.7425 & \cellcolor[HTML]{EEEEEE}0.3059 & 28.9269 & 0.8175 & 0.3050 \\ 
        
    \multicolumn{1}{c|}{\cellcolor[HTML]{C2C2C2}\begin{tabular}[c]{@{}c@{}}Baseline \\ (downscaled)\end{tabular}} 
        & \cellcolor[HTML]{EEEEEE}29.0669 & \cellcolor[HTML]{EEEEEE}0.8474 & \cellcolor[HTML]{EEEEEE}\textbf{0.2171} & 25.8882 & 0.7246 & 0.3204 & \cellcolor[HTML]{EEEEEE}28.0390 & \cellcolor[HTML]{EEEEEE}0.7994 & \cellcolor[HTML]{EEEEEE}0.3190 \\ 
        
    \multicolumn{1}{c|}{\cellcolor[HTML]{B2B2B2}\begin{tabular}[c]{@{}c@{}}Baseline \\ (GAN inversion)\end{tabular}} 
        & 29.3426 & 0.8508 & 0.2181 & \cellcolor[HTML]{EEEEEE}26.0815 & \cellcolor[HTML]{EEEEEE}0.7278 & \cellcolor[HTML]{EEEEEE}0.3169 & 28.3216 & 0.8024 & 0.3161 \\\hline
        
    \multicolumn{1}{c|}{\cellcolor[HTML]{C2C2C2}\begin{tabular}[c]{@{}c@{}}Syn. IPC=1\\ (downscaled)\end{tabular}} & \cellcolor[HTML]{EEEEEE}27.9465 & \cellcolor[HTML]{EEEEEE}0.8281 & \cellcolor[HTML]{EEEEEE}0.2435 & 25.2692 & 0.7093 & 0.3331 & \cellcolor[HTML]{EEEEEE}27.4346 & \cellcolor[HTML]{EEEEEE}0.7849 & \cellcolor[HTML]{EEEEEE}0.3285 \\
    
    \multicolumn{1}{c|}{\cellcolor[HTML]{B2B2B2}\begin{tabular}[c]{@{}c@{}}Syn. IPC=1\\ (pre-trained)\\ IPC=1\end{tabular}} 
        & 27.4473 & 0.8098 & 0.2574 & \cellcolor[HTML]{EEEEEE}24.9738 & \cellcolor[HTML]{EEEEEE}0.6964 & \cellcolor[HTML]{EEEEEE}0.3405 & 27.2203 & 0.7752 & 0.3353 \\ 
        
    
        
    \multicolumn{1}{c|}{\cellcolor[HTML]{C2C2C2}\begin{tabular}[c]{@{}c@{}}Syn. IPC=1\\ (latent)\end{tabular}}
        & \cellcolor[HTML]{EEEEEE}\textbf{29.4424} & \cellcolor[HTML]{EEEEEE}\textbf{0.8521} & \cellcolor[HTML]{EEEEEE}0.2188 & \textbf{26.1528} & \textbf{0.7307} & \textbf{0.3149} & \cellcolor[HTML]{EEEEEE}\textbf{28.3765} & \cellcolor[HTML]{EEEEEE}\textbf{0.8036} & \cellcolor[HTML]{EEEEEE}\textbf{0.3152}
    
\end{tabular}
}
\caption{Quantitative results of EDSR networks trained on synthetic datasets from previous datasets for x4 upscaling. We observe latent distillation to be consistently better, except for LPIPS on Set5. Best values for original data and synthetic data are marked with bold letters.}
\label{table:x4upscalingResults}
\end{table}

\textbf{From 2$\times$ to 4$\times$.} 
Concluding this section, we assess how previously distilled datasets perform when used for a different SR upscaling factor than the one applied during distillation. 
In this study, the distillation loss is consistently computed for 2$\times$  upscaling. 
By training SR models on these synthetic datasets for 4$\times$  upscaling, we examine whether a single upscaling factor during distillation suffices or if each target resolution requires a separate synthetic dataset. 
The latter scenario would contradict the core objective of dataset distillation, as it would lead to redundancy and unnecessarily large synthetic datasets.

Furthermore, the results in \autoref{table:x4upscalingResults} support the premise that distillation at a single upscaling factor is sufficient for latent distillation. 
The performance of latent distillation surpasses all baselines and generally outperforms all other distillation methods, except for LPIPS on Set5. 
Both GAN inversion and the downscaled baseline dataset also yield results comparable to latent distillation and, by extension, to the original baseline. 

\begin{figure}[t!]
    \begin{center}
        \includegraphics[width=\columnwidth]{figures/evolution.pdf}
        \caption{\label{fig:evolution}Illustrated evolution of synthetic HR samples over distillation steps. Each row displays how the distillation process optimizes synthetic samples for different experiments over 1,000 distillation iterations. It becomes apparent that artifacts do not suddenly appear but develop gradually. Further, for latent distillation, only minor changes happen in later iterations, indicating that fewer iterations might suffice for a similarly well-performing synthetic dataset.}
    \end{center} 
\end{figure}

\begin{figure*}[t!]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/illustration_eval_results.pdf}
        \caption{\label{fig:reconstructions}Comparison of qualitative results of the images generated by evaluating the different distillation experiments. Qualitatively, there is no significant difference between SR models trained on real or distilled images.}
    \end{center} 
\end{figure*}

\subsection{Qualitative Results}
\autoref{fig:evolution} illustrates how latent distillation refines synthetic samples over time. In the fourth row, we observe a clear shift from initialization to the first synthetic sample, likely due to the generative model and intermediate latent space. Subsequent iterations show only minor changes, indicating faster convergence compared to prior experiments - suggesting fewer steps may suffice for comparable performance. In contrast, pixel-space experiments (first three rows) exhibit no such trend and given their inferior performance, increasing distillation steps would be ineffective. Notably, artifacts in pixel-space distillation emerge gradually, reinforcing that they are an inherent byproduct of the process.

For the qualitative evaluation, we compare the upscaled images generated by EDSR models trained on different datasets. The results for pixel-space distillation with random initialization are omitted, as the model fails to reconstruct color accurately, making it unsuitable for SR.
We focus on all other synthetic datasets, along with the downscaled baseline, and compare their outputs to ground truth HR images and an EDSR model trained on real data. The models were trained for  4$\times$  upscaling, as  2$\times$  showed negligible differences. As illustrated in \autoref{fig:reconstructions}, even the EDSR model trained on real data struggles to recover fine textures and details from the HR images fully. Nonetheless, models trained on synthetic datasets achieve visually comparable results. While minor differences can be spotted upon closer inspection, no single distillation approach clearly outperforms the others based solely on qualitative evaluation.

\section{Limitation \& Future Work}
While the proposed method eliminates the need for labeled datasets, it still relies on class-based grouping to associate real and synthetic samples. This dependency arises from the design of most existing dataset distillation approaches, which optimize synthetic images per class rather than jointly across all samples. While this class-wise optimization improves computational efficiency and facilitates feature aggregation in classification tasks, it is not inherently meaningful for SR, where the focus is on reconstructing fine details rather than class-specific attributes. Future research should explore class-agnostic distillation methods that remain computationally efficient while generating diverse and representative synthetic datasets for SR.
Additionally, the impact of using class-unconditional generative models on distillation performance remains an open question and warrants further exploration.
Previous work suggests that other generative models, including alternative unconditional GANs and Latent Diffusion Models (LDMs), can be leveraged for dataset distillation \cite{moser2024latent}.

\section{Conclusion}
This exploratory work explores the adaptation of dataset distillation for image SR, demonstrating that synthetic datasets can effectively train SR models while achieving performance comparable to those trained on real data. By leveraging generative models for latent-space distillation via DC, we successfully distill the DIV2K dataset into a significantly smaller yet effective training set. Our results highlight the limitations of pixel-space distillation, while latent-space distillation enables a remarkable 91.12\% dataset reduction without compromising quality.
%
Beyond reducing storage and computational demands, this study provides new insights into different aspects of dataset distillation for SR. %The findings suggest that further optimizations in generative modeling and loss design could push dataset compression even further while maintaining fidelity.

\section*{Acknowledgements}
%Will be in the final...
This work was supported by the BMBF projects SustainML (Grant 101070408) and Albatross (Grant 01IW24002).

\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
