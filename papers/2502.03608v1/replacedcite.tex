\section{Related Work}
\label{related}

\subsection{Tabular Deep Learning}
Although it is theoretically proven that feedforward neural networks can approximate functions from a wide family with arbitrary accuracy ____, in practice, they often underperform compared to GBDT methods in the tabular domain ____. To improve performance, extensive research has been conducted. Here, we highlight three main directions.

The first direction focuses on improving feature preprocessing ____ or enhancing the training process ____. The second direction attempts to adapt more advanced neural network architectures, such as transformer-based models ____. Although these architectures show promising results on specific datasets, they often fail to consistently outperform vanilla MLPs across a wide range of datasets while requiring significantly more computational resources ____. 

The third line of research explores neural network ensembles, which generate multiple predictions for each data point and aggregate them into a final scalar prediction. The most straightforward ensembling approach involves training multiple neural networks independently and averaging their results ____. While this improves performance, it demands significantly more computational resources. Recent studies have investigated more efficient ensembling methods, such as partially sharing weights across different neural networks ____. Although ensembles with shared weights tend to improve performance, they still require significantly more computational resources than GBDT and MLP models ____. In this paper, we investigate more efficient architectures.

\subsection{Mixture of Experts}
Mixture of Experts (MoE) is not a new architecture, and extensive research has been conducted on it. We encourage readers to refer to the comprehensive survey by ____ for an overview. MoE consists of two main components: a gating function and expert functions. The experts can be considered an ensemble of different models, which are aggregated into a final prediction using a gating function.\footnote{While a gating function and expert functions do not necessarily have to be neural networks, in this paper, we assume that they are neural networks when referring to MoE.} A detailed description of the MoE architecture is provided in \cref{moe}.

MoE was not a widely adopted choice in deep learning architectures until its recent application to natural language processing (NLP) ____ and computer vision (CV) ____. Various MoE architectures have been developed and tested for these domains ____. However, to the best of our knowledge, few studies have evaluated the performance of MoE across a broad range of tabular datasets. In this paper, we aim to address this gap.

\subsection{Gumbel-Softmax Distribution}
The Gumbel-Softmax distribution is widely used in deep learning for its ability to produce differentiable samples that approximate non-differentiable categorical distributions ____. In this paper, we utilize this distribution for a different purposeâ€”primarily to regularize the gating neural network in MoE (see \cref{gg_moe}).