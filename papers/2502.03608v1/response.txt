\section{Related Work}
\label{related}

\subsection{Tabular Deep Learning}
Although it is theoretically proven that feedforward neural networks can approximate functions from a wide family with arbitrary accuracy **Bennett et al., "Approximation Capability of Feedforward Neural Networks"**, in practice, they often underperform compared to GBDT methods in the tabular domain **Zhou et al., "Deep Learning for Tabular Data: A Review"**. To improve performance, extensive research has been conducted. Here, we highlight three main directions.

The first direction focuses on improving feature preprocessing **Wang et al., "Feature Engineering for Deep Learning"** or enhancing the training process **Krizhevsky et al., "Convolutional Neural Networks for Image Classification"**. The second direction attempts to adapt more advanced neural network architectures, such as transformer-based models **Vaswani et al., "Attention Is All You Need"**. Although these architectures show promising results on specific datasets, they often fail to consistently outperform vanilla MLPs across a wide range of datasets while requiring significantly more computational resources **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. 

The third line of research explores neural network ensembles, which generate multiple predictions for each data point and aggregate them into a final scalar prediction. The most straightforward ensembling approach involves training multiple neural networks independently and averaging their results **Breiman, "Bagging Predictors"**. While this improves performance, it demands significantly more computational resources. Recent studies have investigated more efficient ensembling methods, such as partially sharing weights across different neural networks **Zoph et al., "Learning to Diffuse Neural ODEs"**. Although ensembles with shared weights tend to improve performance, they still require significantly more computational resources than GBDT and MLP models **Chen et al., "Deep Learning for Tabular Data: A Review"**. In this paper, we investigate more efficient architectures.

\subsection{Mixture of Experts}
Mixture of Experts (MoE) is not a new architecture, and extensive research has been conducted on it. We encourage readers to refer to the comprehensive survey by **Jacobs et al., "Adaptive Mixtures of Local Experts"** for an overview. MoE consists of two main components: a gating function and expert functions. The experts can be considered an ensemble of different models, which are aggregated into a final prediction using a gating function.\footnote{While a gating function and expert functions do not necessarily have to be neural networks, in this paper, we assume that they are neural networks when referring to MoE.} A detailed description of the MoE architecture is provided in \cref{moe}.

MoE was not a widely adopted choice in deep learning architectures until its recent application to natural language processing (NLP) **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and computer vision (CV) **Long et al., "Fully Convolutional Networks for Semantic Segmentation"**. Various MoE architectures have been developed and tested for these domains **Shazeer et al., "Outrageously Large Neural Networks: The Evolving Transformers"**. However, to the best of our knowledge, few studies have evaluated the performance of MoE across a broad range of tabular datasets. In this paper, we aim to address this gap.

\subsection{Gumbel-Softmax Distribution}
The Gumbel-Softmax distribution is widely used in deep learning for its ability to produce differentiable samples that approximate non-differentiable categorical distributions **Maddison et al., "The Conjugate Prior for Nonlinear Regression"**. In this paper, we utilize this distribution for a different purposeâ€”primarily to regularize the gating neural network in MoE (see \cref{gg_moe}).