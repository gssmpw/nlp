\section{Related Work}
\label{related}

\subsection{Tabular Deep Learning}
Although it is theoretically proven that feedforward neural networks can approximate functions from a wide family with arbitrary accuracy \cite{cybenko1989approximation,hornik1991approximation}, in practice, they often underperform compared to GBDT methods in the tabular domain \cite{gorishniy2021revisiting,shwartz2022tabular,grinsztajn2022tree}. To improve performance, extensive research has been conducted. Here, we highlight three main directions.

The first direction focuses on improving feature preprocessing \cite{gorishniy2022embeddings,guo2021embedding} or enhancing the training process \cite{bahri2021scarf,gorishniy2021revisiting,jeffares2023tangos,holzmuller2024better,kadra2021well}. The second direction attempts to adapt more advanced neural network architectures, such as transformer-based models \cite{huang2020tabtransformer,somepalli2021saint,song2019autoint}. Although these architectures show promising results on specific datasets, they often fail to consistently outperform vanilla MLPs across a wide range of datasets while requiring significantly more computational resources \cite{gorishniy2024tabm}. 

The third line of research explores neural network ensembles, which generate multiple predictions for each data point and aggregate them into a final scalar prediction. The most straightforward ensembling approach involves training multiple neural networks independently and averaging their results \cite{lakshminarayanan2017simple}. While this improves performance, it demands significantly more computational resources. Recent studies have investigated more efficient ensembling methods, such as partially sharing weights across different neural networks \cite{gorishniy2024tabm,wen2020batchensemble}. Although ensembles with shared weights tend to improve performance, they still require significantly more computational resources than GBDT and MLP models \cite{gorishniy2024tabm}. In this paper, we investigate more efficient architectures.

\subsection{Mixture of Experts}
Mixture of Experts (MoE) is not a new architecture, and extensive research has been conducted on it. We encourage readers to refer to the comprehensive survey by \cite{yuksel2012twenty} for an overview. MoE consists of two main components: a gating function and expert functions. The experts can be considered an ensemble of different models, which are aggregated into a final prediction using a gating function.\footnote{While a gating function and expert functions do not necessarily have to be neural networks, in this paper, we assume that they are neural networks when referring to MoE.} A detailed description of the MoE architecture is provided in \cref{moe}.

MoE was not a widely adopted choice in deep learning architectures until its recent application to natural language processing (NLP) \cite{du2022glam} and computer vision (CV) \cite{puigcerver2023sparse,riquelme2021scaling}. Various MoE architectures have been developed and tested for these domains \cite{fedus2022review}. However, to the best of our knowledge, few studies have evaluated the performance of MoE across a broad range of tabular datasets. In this paper, we aim to address this gap.

\subsection{Gumbel-Softmax Distribution}
The Gumbel-Softmax distribution is widely used in deep learning for its ability to produce differentiable samples that approximate non-differentiable categorical distributions \cite{jang2016categorical}. In this paper, we utilize this distribution for a different purposeâ€”primarily to regularize the gating neural network in MoE (see \cref{gg_moe}).