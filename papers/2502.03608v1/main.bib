
@article{gorishniy2024tabm,
  title={TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling},
  author={Gorishniy, Yury and Kotelnikov, Akim and Babenko, Artem},
  journal={arXiv preprint arXiv:2410.24210},
  year={2024}
}

%embeddings
@article{gorishniy2022embeddings,
  title={On embeddings for numerical features in tabular deep learning},
  author={Gorishniy, Yury and Rubachev, Ivan and Babenko, Artem},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24991--25004},
  year={2022}
}

%embeddings?
@inproceedings{guo2021embedding,
  title={An embedding learning framework for numerical features in ctr prediction},
  author={Guo, Huifeng and Chen, Bo and Tang, Ruiming and Zhang, Weinan and Li, Zhenguo and He, Xiuqiang},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={2910--2918},
  year={2021}
}

%MLP regularization
@article{kadra2021well,
  title={Well-tuned simple nets excel on tabular datasets},
  author={Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={23928--23941},
  year={2021}
}

%MLP regularization
@article{jeffares2023tangos,
  title={TANGOS: Regularizing tabular neural networks through gradient orthogonalization and specialization},
  author={Jeffares, Alan and Liu, Tennison and Crabb{\'e}, Jonathan and Imrie, Fergus and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2303.05506},
  year={2023}
}

%MLP regularization
@article{holzmuller2024better,
  title={Better by default: Strong pre-tuned mlps and boosted trees on tabular data},
  author={Holzm{\"u}ller, David and Grinsztajn, L{\'e}o and Steinwart, Ingo},
  journal={arXiv preprint arXiv:2407.04491},
  year={2024}
}

%MLP regularization
@article{bahri2021scarf,
  title={Scarf: Self-supervised contrastive learning using random feature corruption},
  author={Bahri, Dara and Jiang, Heinrich and Tay, Yi and Metzler, Donald},
  journal={arXiv preprint arXiv:2106.15147},
  year={2021}
}

%GBDT
@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{prokhorenkova2018catboost,
  title={CatBoost: unbiased boosting with categorical features},
  author={Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

%mlp
@article{chen2023trompt,
  title={Trompt: Towards a better deep neural network for tabular data},
  author={Chen, Kuan-Yu and Chiang, Ping-Han and Chou, Hsin-Rung and Chen, Ting-Wei and Chang, Tien-Hao},
  journal={arXiv preprint arXiv:2305.18446},
  year={2023}
}
%mlp
@article{klambauer2017self,
  title={Self-normalizing neural networks},
  author={Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

%gbdt is better + transformer? mlp like?
@article{gorishniy2021revisiting,
  title={Revisiting deep learning models for tabular data},
  author={Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18932--18943},
  year={2021}
}

%gbdt is better
@article{shwartz2022tabular,
  title={Tabular data: Deep learning is not all you need},
  author={Shwartz-Ziv, Ravid and Armon, Amitai},
  journal={Information Fusion},
  volume={81},
  pages={84--90},
  year={2022},
  publisher={Elsevier}
}

%gbdt is better
@article{grinsztajn2022tree,
  title={Why do tree-based models still outperform deep learning on typical tabular data?},
  author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={507--520},
  year={2022}
}

%transformer
@article{huang2020tabtransformer,
  title={Tabtransformer: Tabular data modeling using contextual embeddings},
  author={Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
  journal={arXiv preprint arXiv:2012.06678},
  year={2020}
}

%transformer
@article{somepalli2021saint,
  title={Saint: Improved neural networks for tabular data via row attention and contrastive pre-training},
  author={Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C Bayan and Goldstein, Tom},
  journal={arXiv preprint arXiv:2106.01342},
  year={2021}
}

%transformer
@inproceedings{song2019autoint,
  title={Autoint: Automatic feature interaction learning via self-attentive neural networks},
  author={Song, Weiping and Shi, Chence and Xiao, Zhiping and Duan, Zhijian and Xu, Yewen and Zhang, Ming and Tang, Jian},
  booktitle={Proceedings of the 28th ACM international conference on information and knowledge management},
  pages={1161--1170},
  year={2019}
}

%MLP- universal approx
@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}
%MLP- universal approx
@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

%deep ensembles
@article{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

%%batch ensembles
@article{wen2020batchensemble,
  title={Batchensemble: an alternative approach to efficient ensemble and lifelong learning},
  author={Wen, Yeming and Tran, Dustin and Ba, Jimmy},
  journal={arXiv preprint arXiv:2002.06715},
  year={2020}
}

%optuna
@inproceedings{akiba2019optuna,
  title={Optuna: A next-generation hyperparameter optimization framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2623--2631},
  year={2019}
}

%gumbel
@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

%Monte Carlo estimation
@book{graham2013stochastic,
  title={Stochastic simulation and Monte Carlo methods: mathematical foundations of stochastic simulation},
  author={Graham, Carl and Talay, Denis},
  volume={68},
  year={2013},
  publisher={Springer Science \& Business Media}
}

%MoE survey
@article{yuksel2012twenty,
  title={Twenty years of mixture of experts},
  author={Yuksel, Seniha Esen and Wilson, Joseph N and Gader, Paul D},
  journal={IEEE transactions on neural networks and learning systems},
  volume={23},
  number={8},
  pages={1177--1193},
  year={2012},
  publisher={IEEE}
}
%MoE NLP
@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}

%MoE CV
@article{puigcerver2023sparse,
  title={From sparse to soft mixtures of experts},
  author={Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Houlsby, Neil},
  journal={arXiv preprint arXiv:2308.00951},
  year={2023}
}
%% MoE CV
@article{riquelme2021scaling,
  title={Scaling vision with sparse mixture of experts},
  author={Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8583--8595},
  year={2021}
}

%MoE collapse - load balancing
@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

%moe deep learning overview.
@article{fedus2022review,
  title={A review of sparse expert models in deep learning},
  author={Fedus, William and Dean, Jeff and Zoph, Barret},
  journal={arXiv preprint arXiv:2209.01667},
  year={2022}
}

%entropy
@article{conrad2004probability,
  title={Probability distributions and maximum entropy},
  author={Conrad, Keith},
  journal={Entropy},
  volume={6},
  number={452},
  pages={10},
  year={2004}
}


%quantline normalisation
@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}

%%AdamW
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
