\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}

\clearpage
\appendix

\onecolumn 

\section{Appendix}
\tableofcontents 
\clearpage


\begin{table*}[t]
\centering
\small
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} 
  >{\centering\arraybackslash}m{3cm}  % Sampling Mode
  >{\centering\arraybackslash}m{1.5cm} % Stride
  >{\centering\arraybackslash}m{3cm}   % Extrapolation Factor
  >{\centering\arraybackslash}m{2cm}   % Total NFE
  >{\centering\arraybackslash}m{3cm}   % Sampling Time [s]
  >{\centering\arraybackslash}m{1cm}   % FVD subcolumn 1
  >{\centering\arraybackslash}m{1cm}}  % FVD subcolumn 2
\toprule
\makecell{\textbf{Sampling}\\\textbf{Mode}} & 
\makecell{\textbf{Stride}} & 
\makecell{\textbf{Extrapolation}\\\textbf{Factor}} & 
\makecell{\textbf{Total}\\\textbf{NFE}} & 
\makecell{\textbf{Sampling}\\\textbf{Time [s]}} & 
\multicolumn{2}{c}{\textbf{FVD$\downarrow$}} \\
\cmidrule(rr){6-7}
 & & & & & \textbf{DMLab} & \textbf{FFS} \\
 \midrule
\rowcolor{gray!8}Diffusion Forcing~\cite{chen2024diffusionforcing} & $s=k-m$ & $1\times$ & $286 / 266$ & $45.32$ / $52.26$ & $60.30$ & $51.90$ \\
Rolling Diffusion~\cite{ruhe2024rollingdiffusionmodels} & $s=k-m$ & $1\times$ & $500$ / $500$ & $79.24$ / $98.23$ & \textbf{52.43} & \textbf{45.51} \\
\rowcolor{gray!8}\textit{MaskFlow} (MGM-Style) & $s=k-m$ & $1\times$ & \textbf{20} / \textbf{20} & \textbf{3.17 / 3.93} & $53.17$ & $45.92$ \\
\midrule
Diffusion Forcing~\cite{chen2024diffusionforcing} & $s=k-m$ & $2\times$ & $858$ / $798$ & $135.97$ / $156.78$ & $175.01$ & $144.43$ \\
\rowcolor{gray!8}Rolling Diffusion~\cite{ruhe2024rollingdiffusionmodels} & $s=k-m$ & $2\times$ & $896$ / $788$ & $141.99 / 154.81$ & 201.70 & 72.49 \\
\textit{MaskFlow} (MGM-Style) & $s=k-m$ & $2\times$ & \textbf{60} / \textbf{60} & \textbf{9.51} / \textbf{9.30} & $188.02$ &  $59.93$ \\
\rowcolor{gray!8}\textit{MaskFlow} (MGM-Style) & $s=1$ & $2\times$ & $740$ / $340$ & 117.27 / 66.80 & \textbf{50.87} & \textbf{30.43} \\
\midrule
Diffusion Forcing~\cite{chen2024diffusionforcing} & $s=k-m$ & $5\times$ & $2{,}002$ / $1{,}596$ & $317.27$ / $313.56$ & $232.89$ & $272.14$ \\
\rowcolor{gray!8}Rolling Diffusion~\cite{ruhe2024rollingdiffusionmodels} & $s=k-m$ & $5\times$ & $2{,}084$ / $1{,}652$ & $330.27$ / $324.56$ & $338.34$ & $248.13$ \\
\textit{MaskFlow} (MGM-Style) & $s=k-m$ & $5\times$ & \textbf{140} / \textbf{120} & \textbf{22.19 / 23.58} & $334.15$ & $108.74$ \\
\rowcolor{gray!8}\textit{MaskFlow} (MGM-Style) & $s=1$ & $5\times$ & $2{,}900$ / $1{,}300$ & 100.09/379.91 & \textbf{181.11} & \textbf{103.69} \\
\bottomrule
\end{tabular*}
\caption{\textbf{MGM Style sampling is much faster without sacrificing quality.} We report the total number of function evaluations (NFE), sampling time (in seconds), and FVD for various sampling methods and extrapolation factors across both datasets.}
\label{tab:speed_comparison}
\end{table*}



\subsection{Additional Related Work}

\paragraph{Masked Diffusion Models.} 
Limitations of autoregressive models for probabilistic language modeling have recently sparked increasing interest in masked diffusion models. Recent works like \cite{shi2024simplifiedgeneralizedmaskeddiffusion} and \cite{sahoo2024simpleeffectivemaskeddiffusion} have aligned masked generative models with the design space of diffusion models by formulating continuous-time forward and sampling processes. Works like \cite{nie2024scalingmaskeddiffusionmodels} and \cite{gong2024scalingdiffusionlanguagemodels} also demonstrate the significant scaling potential of MDM for language tasks, indicating that this masked modeling paradigm can rival autoregressive approaches for modalities beyond language such as protein co-design \cite{campbell2024generative} and vision.

\subsection{Computation of NFE for Different Sampling Methods}

Our sampling speed evaluations are determined by computing the required number of chunks 
\[
\ell = \left\lceil \frac{L - k}{s} \right\rceil + 1,
\]
to generate a video of total length \(L\), where \(k\) is the chunk size and \(s\) is the stride with which the chunk start is shifted. The overall number of function evaluations (NFEs) is then obtained by multiplying \(\ell\) with the number of sampling steps required to generate one chunk. We apply this methodology for all chunkwise-autoregressive approaches.

\begin{itemize}
    \item \textbf{MGM-Style Sampling:} In this method each chunk is generated in $20$ forward passes, so that the total NFE is
    \[
    \text{NFE}_{\mathrm{MGM}} = \ell \times 20.
    \]

    \item \textbf{FM-Style Sampling:} Here we generate each chunk in $250$ forward passes:
    \[
    \text{NFE}_{\mathrm{FM}} = \ell \times 250.
    \]
    
    \item \textbf{Diffusion Forcing with Pyramid Scheduling:} Here, we apply $250$ sampling timesteps per frame but begin unmasking earlier frames as the denoising process proceeds. For a chunk of \(k\) frames, we generate a scheduling matrix with 
    \[
    H = 250 + (k-1) + 1 = k + 250
    \]
    rows and \(k\) columns. Each entry in the scheduling matrix is computed as
    \[
    \text{scheduling\_matrix}[i,j] = 250 + j - i,\quad \text{for } i=0,\ldots,H-1 \text{ and } j=0,\ldots,k-1,
    \]
    and then clipped to the interval \([0,249]\). Since we iterate through each of the $H$ rows of the denoising matrix in each chunk we effectively compute
    \[
    \text{NFE}_{\text{DiffusionForcing}} = k + 250.
    \] 
    
    \item \textbf{RDM Sampling:} This approach proceeds in three stages:
    \begin{enumerate}
        \item \textit{Initialization (Init-Schedule):} The initial window of \(k\) frames is processed using a fixed schedule that applies $T=250$ forward passes to bring the window to its rolling state.
        
        \item \textit{Sliding Window Handling:} After initialization, the window is shifted by one frame at a time. For each shift, an inner loop is executed that updates the denoising levels until the first non-context frame (i.e., the frame immediately following the \(m\) context frames) is fully denoised (i.e., reaches a value of 1). This inner loop requires $\left\lceil \frac{T}{k-m} \right\rceil$ forward passes per window shift. As the window is shifted \((L - k)\) times, this stage contributes roughly \((L - k) \times \left\lceil \frac{T}{k-m} \right\rceil\) forward passes.
        
        \item \textit{Final Window Processing:} Once the sliding window stage is complete, the final (partial) window is further refined until all frames are fully denoised. This final stage requires additional $250$ forward passes.
    \end{enumerate}
    
    Thus, the total NFE for RDM is given by
    \[
    \text{NFE}_{\mathrm{Rolling}} = 250 \; (\text{init-schedule}) + (L - k) \times \left\lceil \frac{T}{k-m} \right\rceil\ \; (\text{sliding}) + 250 \; (\text{final window}).
    \]
\end{itemize}




\subsection{Training \& Implementation Details}

All FFS models were trained on 4 H100 GPUs with a local batch size of $4$. We run training for a total of $200{,}000$ steps and use a sigmoid scheduler that determines the per-frame masking ratio for a sampled masking level $t^k$. We use an AdamW optimizer with a learning rate of $1e-4$ and $\beta_1 = 0.9$ and $\beta_2 = 0.999$. We additionally incorporate a frame-level loss weighting mechanism based that is also based on \(t^k\). We adopt \emph{fused}-SNR loss weighting from \cite{hang2023efficient,chen2024diffusionforcing} and derive it for discrete flow matching. Let

\[
\text{SNR}(t) \;=\; \frac{\kappa(t)^2}{\,1 - \kappa(t)^2\,},
\]

where \(\kappa(t)\) is the masking schedule. The \emph{fused}-SNR mechanism smoothes SNR values across time steps in a video by computing an exponentially decaying SNR from previous frames (or tokens). We refer the reader to~\cite{chen2024diffusionforcing} for full details.


\begin{algorithm}[!ht]
\caption{\textbf{FM-Style Sampling with Context Frames for a Single Chunk}}
\label{alg:fmsampling}
\begin{algorithmic}[1]
\REQUIRE 
   $p(\mathbf{x}_1 | \mathbf{x}_t, \mathbf{t};\theta)$, 
   $t$, 
   context frames $\mathbf{c} = (c^1,\dots,c^m)$, 
   fully masked frame \([M]\) (i.e., a frame where every token equals the mask token \(M\)),
   $t \in [0,1]$, 
   $\Delta t$

\STATE $\mathbf{x}_t \,\gets\, (\,c^1,\dots,c^m,\,[M],\dots,[M])$
\STATE $t \,\gets\, 0$
\STATE $\mathbf{t} \gets (1,\dots,1,0,\dots0)$

\WHILE{$t \,\le\, 1 - \Delta t$}
    \STATE $u_t(\mathbf{x}_t) 
        \;=\; 
        \frac{t}{1-t}
        \Bigl[
          p_\theta(\mathbf{x}_1 \mid \mathbf{x}_t,\,\mathbf{t}) 
          \;-\; 
          \delta_{\mathbf{x}_t}
        \Bigr]$
    \STATE $p_\theta\!\bigl(\mathbf{x}_1 \mid \mathbf{x}_{t+\Delta t},\,\mathbf{t}+\Delta t\bigr)
        \;=\;
        \mathrm{Cat}\!\Bigl[\,
          \delta_{\mathbf{x}_t}
          \;+\; 
          u_t(\mathbf{x}_t)\,\Delta t
        \Bigr]$
    \STATE \textbf{For each token} $n$ in $\mathbf{x}_t$: 
    \STATE \quad 
    $
       x_{t+\Delta t}^{n} \gets
       \begin{cases}
          x_t^{n}, & \text{if } x_t^{n} \neq M,\\
          p(\cdot | \mathbf{x}_{t+\Delta t},\,\mathbf{t}+\Delta t; \theta), & \text{if } x_t^{n} = M.
       \end{cases}
    $
\STATE $t \gets t + \Delta t$
\STATE $\mathbf{t} \gets \mathbf{t} + \Delta t$

\ENDWHILE
\STATE \textbf{return} $\mathbf{x}_t$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{\textbf{MGM-Style Sampling for a Single Chunk}}
\label{alg:mgm_chunk_unmasking_revised}
\begin{algorithmic}[1]
\REQUIRE 
  Network $p(\mathbf{x}_1 \mid \mathbf{x}_t, \mathbf{t}; \theta)$,  
  context frames $\mathbf{c} = (c^1,\dots,c^m)$,  
  masked frame $[M]$ (i.e., every token equals $M$),    
  total unmasking steps $T$
\STATE \textbf{Initialize:}\\
$\mathbf{x}_t \;\leftarrow\; (\mathbf{c},\, [M],\dots,[M])$\\
$\mathbf{t} \;\leftarrow\; (\underbrace{1,\dots,1}_{m},\, \underbrace{0,\dots,0}_{k-m})$
\STATE Define the set of masked token indices in $\mathbf{x}_t$:\\
$\mathcal{M} \;\triangleq\; \{\, n \mid x_t^n = M \,\}.$
\FOR{$i=1$ \textbf{to} $T$}
    \STATE Compute token-wise logits:\\
    $\boldsymbol{\lambda} \;\leftarrow\; p(\mathbf{x}_1 \mid \mathbf{x}_t, \mathbf{t}; \theta).$
    \STATE \textbf{For each token} $n \in \mathcal{M}$: \\
    sample $\hat{x}_t^n \sim \mathrm{Cat}\Bigl(\mathrm{Softmax}\bigl(\boldsymbol{\lambda}^n\bigr)\Bigr)$ \\
    and compute the confidence score 
    $C_n \;=\; \mathrm{Softmax}\bigl(\boldsymbol{\lambda}^n\bigr)_{\hat{x}_t^n}.$ \\
    \STATE \textbf{Define the confidence threshold:}\\
    Let $\alpha$ denote the desired fraction of masked tokens to update in each iteration (e.g. $\alpha = 1/T$). \\
    
    Then set 
    $\tau_c \;=\; \min\Bigl\{ c \in [0,1] \;\Bigm|\; \Bigl|\{ j \in \mathcal{M} \mid C_j \ge c \}\Bigr| \ge \Bigl\lceil \alpha\,|\mathcal{M}| \Bigr\rceil \Bigr\}.$ \\
    
    (That is, $\tau_c$ is chosen as the minimum confidence such that at least $\lceil \alpha\,|\mathcal{M}| \rceil$ tokens have confidence scores at or above $\tau_c$, thereby selecting the top $\lceil \alpha\,|\mathcal{M}| \rceil$ tokens.)
    \STATE \textbf{For each token} $n \in \mathcal{M}$ with $C_n \ge \tau_c$, update:\\
    $x_t^n \;\leftarrow\; \hat{x}_t^n.$
    \STATE Update the set of masked indices:\\
    $\mathcal{M} \;\leftarrow\; \{\, n \mid x_t^n = M \,\}.$
    \IF{$\mathcal{M} = \varnothing$}
         \STATE \textbf{break}
    \ENDIF
\ENDFOR
\STATE \textbf{return} $\mathbf{x}_t$.
\end{algorithmic}
\end{algorithm}


\subsection{Baseline Details}

The two most comparable works to our method are \citet{chen2024diffusionforcing} and \citet{ruhe2024rollingdiffusionmodels}. Both of these techniques propose novel sampling methods that can be rolled out to long video lengths, and also apply frame-specific noise levels. Both of these approaches are diffusion-based and operate on continuous representations, whereas we operate on discrete tokens and use masking. We re-implement both the pyramid sampling scheme proposed in Diffusion Forcing and the Rolling Diffusion sampling method in our discrete setting. This allows us to compare the baseline sampling methods to MaskFlow on the same model backbones. 
%
To isolate the effect of our chunkwise autoregressive sampling methodology on performance from the effects of tokenization, we reimplement both the pyramid sampling scheme proposed in Diffusion Forcing and the Rolling Diffusion sampling method for our discrete setting. This allows us to compare the baseline sampling methods on the same timestep-dependent model backbone. 
%
Although it is conceivable that Rolling Diffusion sampling may perform better when applied to a model explicitly trained using the progressive noise schedule suggested in \citet{ruhe2024rollingdiffusionmodels}, we believe this comparison is still fair. Our training methodology does not inject any inductive bias by way of the masking level into the model, so there is no obvious advantage that our sampling should have over other methods. 
We provide a comprehensive evaluation of performance and sampling efficiency across both datasets and different sampling modes.


\subsection{Dataset Details}

\paragraph{Deepmind Lab.} The Deepmind Lab (DMLab) navigation dataset contains $64 \times 64$ resolution videos of random walks in a 3D maze environment. We use the total 625 videos with frame length 300 frames, and randomly sample sequences of 36 consecutive frames from each video during training. We upscale video frames to a resolution of $256 \times 256$ before tokenizing them similar to our approach for FaceForensics. We disregard the provided actions, focusing on action-unconditional video generation. We use $m=12$ and $s=24$ for the DMLab full sequence generation experiments unless stated otherwise.

\paragraph{FaceForensics.} FaceForensics (FFS) is a dataset that contains $150\times150$ images of deepfake faces, totaling 704 videos with varying number of frames at 8 frames-per-second. We upsample the resolution to $256 \times 256$, before encoding individual frames using the image-based tokenizer SD-VQGAN \cite{rombach2022high_latentdiffusion_ldm}. While image-based tokenizers have shown to lead to flickering issues, we observe high-reconstruction quality (reconstruction FVD $\approx 8$ on FFS) on our datasets and thus leave work on video tokenization to other works. After tokenization, we train on encoded frame sequences of 16 frames, each consisting of token grids with dimensionality $32 \times 32$. We generally use $m=2$ ground-truth context frames for conditioning, and $s=14$.

\subsection{Further Quantitative Results}

\paragraph{Our chunkwise autoregressive MGM-style sampling is preferable to full sequence training in settings with limited hardware.} To evaluate our method for long video generation against a longer training window baseline, we compare the performance of a frame-level masking model trained on $16$ frames with full sequence generation of a constant-masking level model trained on $32$ frames with similar batch size and on similar hardware. In Table ~\ref{tab:longer_train_window_baseline} we show that iterative rollout of our MGM-style sampling outperforms full sequence generation even when the full sequence model is trained on a longer window.

\begin{table}[ht]
    \centering
    \normalsize
    \resizebox{0.48\textwidth}{!}{%
    \begin{tabular}{l|cccc}
    \toprule
    \makecell{\textbf{Sampling} \\ \textbf{Mode}} 
    & \makecell{\textbf{Training} \\ \textbf{Window}} 
    & \makecell{\textbf{Sampling} \\ \textbf{Window}} 
    & \makecell{\textbf{Total} \\ \textbf{NFE}}
    & \makecell{\textbf{FVD} $\downarrow$} \\
    \midrule
    FM-Style (bs=2) & 32 & 32 & 250 & 253.08 \\
    \midrule
    \textit{MaskFlow} (MGM-Style) (bs=2) & 16 & 32 & 60 & 192.76 \\
    \rowcolor{gray!8}\textit{MaskFlow} (MGM-Style) (bs=4) & 16 & 32 & 60 & \textbf{59.93} \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Our MGM-style sampling is more efficient and generates better results over baseline for larger training windows}. We train a constant masking ratio model on larger window sizes with similar batch size on similar hardware, and compare full sequence generation to generating the same length using our chunkwise MGM-style sampling.}
    \label{tab:longer_train_window_baseline}
\end{table}

\begin{table}[ht]
    \centering
    \normalsize
    \resizebox{0.48\textwidth}{!}{%
    \begin{tabular}{l|ccrr}
        \toprule
        & \makecell{\textbf{Extrapolation} \\ \textbf{Factor}}
        & \makecell{\textbf{Sampling} \\ \textbf{Stride}}
        & \makecell{\textbf{Total} \\ \textbf{NFE}}
        & \makecell{\textbf{FVD} $\downarrow$} \\
        \midrule
        FaceForensics   & $2\times$  & $s=14$ (\textit{full sequence}) & \textbf{60} & 59.93 \\
        \rowcolor{gray!8}FaceForensics   & $2\times$  & $s=1$ (\textit{autoregressive})  & 340 & \textbf{30.43} \\
        \midrule
        FaceForensics   & $5\times$  & $s=14$ (\textit{full sequence}) & \textbf{120} & 108.74 \\
        \rowcolor{gray!8}FaceForensics & $5\times$  & $s=1$ (\textit{autoregressive})  & 1,300 & \textbf{103.69} \\
        \midrule
        FaceForensics   & $10\times$ & $s=14$ (\textit{full sequence}) & \textbf{240} & 214.39 \\
       \rowcolor{gray!8} FaceForensics   & $10\times$ & $s=1$ (\textit{autoregressive})  & 2,900 & \textbf{165.02} \\
        \midrule
        \midrule
        DMLab & $2\times$  & $s=24$ (\textit{full sequence}) & \textbf{60} & 188.22 \\
        \rowcolor{gray!8}DMLab & $2\times$  & $s=1$ (\textit{autoregressive})  & 740  & \textbf{50.87} \\
        \midrule
        DMLab & $5\times$  & $s=24$ (\textit{full sequence}) & \textbf{140}  & 334.15 \\
       \rowcolor{gray!8} DMLab & $5\times$  & $s=1$ (\textit{autoregressive})  & 2,900 & \textbf{181.11} \\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Autoregressive sampling outperforms full sequence sampling on timestep-dependent models at the cost of higher NFE.}}
    \label{tab:autoregression_dependent}
\end{table}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.7\textwidth]{figpaper/realestate.pdf}\hfill
    \caption{\textbf{Further visualizations on the Realestate10K \cite{zhou2018stereo} dataset.} Models trained on chunk size $k = 16$ with $4$ H100 GPUs. Due to computational limitations, we cannot  provide further analyses on this larger, more compute intensive dataset.}
    \label{fig:faces_comparison}
\end{figure*}




\begin{table}[ht]
    \centering
    \normalsize
    \resizebox{0.58\textwidth}{!}{%
    \begin{tabular}{l|ccrr}
        \toprule
        & \makecell{\textbf{Extrapolation} \\ \textbf{Factor}}
        & \makecell{\textbf{Sampling} \\ \textbf{Stride}}
        & \makecell{\textbf{Total} \\ \textbf{NFE}}
        & \makecell{\textbf{FVD} $\downarrow$} \\
        \midrule
        FaceForensics   & $2\times$  & $s=14$ (\textit{full sequence}) & \textbf{60} & 109.96 \\
        FaceForensics   & $2\times$  & $s=1$ (\textit{autoregressive}) & 340 & \textbf{43.91} \\
        \midrule
        FaceForensics   & $5\times$  & $s=14$ (\textit{full sequence}) & \textbf{120} & \textbf{137.66} \\
        FaceForensics & $5\times$  & $s=1$ (\textit{autoregressive})  & 1,300 & 193.90 \\
        \midrule
        FaceForensics   & $10\times$ & $s=14$ (\textit{full sequence}) & \textbf{240} & \textbf{174.92} \\
        FaceForensics   & $10\times$ & $s=1$ (\textit{autoregressive})  & 2,900 & 293.16 \\
        \midrule
        \midrule
        DMLab & $2\times$  & $s=24$ (\textit{full sequence}) & \textbf{60} & 219.33 \\
        DMLab & $2\times$  & $s=1$ (\textit{autoregressive})  & 740  & \textbf{42.53} \\
        \midrule
        DMLab & $5\times$  & $s=24$ (\textit{full sequence}) & \textbf{140}  & 402.73 \\
        DMLab & $5\times$  & $s=1$ (\textit{autoregressive})  & 2,900 & \textbf{80.56} \\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Autoregressive sampling outperforms full sequence sampling on timestep-independent models at the cost of higher NFE.} Performance improvement on DMLab is substantial.}
    \label{tab:autoregression_independent}
\end{table}

\newpage

\subsection{Further Qualitative Results}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.48\textwidth]{figpaper/faces1.pdf}\hfill
    \includegraphics[width=0.48\textwidth]{figpaper/faces2.pdf}
    \caption{\textbf{Visualizations of FaceForensics generation results with different context frames.}}
    \label{fig:faces_comparison}
\end{figure*}














