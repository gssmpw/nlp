\section{Method}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\linewidth]{figpaper/maskflow_sampling.pdf}
    \vspace{-7pt}
    \caption{\textbf{MaskFlow Sampling:} Given $m=2$ \textcolor{red}{context frames} used to initialize generation, we unmask the current window and use \textcolor{color_generated_frame}{newly generated frames} as new context frames in the next chunk of size $k=5$, using stride $s=3$. (\textit{Tokenization omitted here to simplify understanding}) .
    }
    \label{fig:sampling}
    \vspace{-10pt}
\end{figure*}

\subsection{Task formulation: Long video generation}
\label{subsec:long_vid}

 There are, generally, three distinct approaches to long video generation. The first is the naive approach of training on long video sequences. This is challenging due to the quadratic complexity in attention mechanisms with respect to token numbers. Although works like\cite{tan2024video,harvey2022flexible} address this by distributing the generative process or by generating every \(n\)-th frame and subsequently infilling the remaining frames, the approach remains fundamentally resource-intensive. The second approach is a \emph{rolling} (or ``sliding-window'') approach, which applies monotonically increasing noise dependent on a frame's position in the sliding window. This process can be rolled out indefinitely, removing frames from the window when they are fully denoised and appending random noise frames at the end of the window. Works such as \cite{ruhe2024rollingdiffusionmodels, wu2023ar, xie2024progressive} belong to this paradigm. The third approach is \emph{chunkwise-autoregression}, also referred to as blockwise-autoregression \cite{ruhe2024rollingdiffusionmodels}. Here, the video of length $L$ is divided into overlapping \emph{chunks} of length $k \ll L$, where each chunk overlaps by $m$ frames, which we refer to as context frames. Concretely, we define a video and its frames as

\begin{equation}
   \mathbf{v} = (v^1, v^2, \dots, v^L)
\end{equation}

which we divide into overlapping chunks of length $k$. Let $\ell\;=\;\left\lceil \frac{L - k}{s} \right\rceil + 1$ denote the number of chunks needed to cover the video of length $L$, and we further define each chunk $\mathbf{v}^{(i)}$ as

\begin{equation}
   \mathbf{v}^{(i)} = \bigl(v^{(i-1)\,s + 1}, \dots, v^{(i-1)\,s + k}\bigr),
\end{equation}

where $s \le k$ is the sampling window stride, i.e., how far the context start shifts at each step. Often, one sets $s = k - m$, but this is not strictly required. The video distribution then factorizes as

\begin{equation}
\label{eq:markov}
   p(\mathbf{v};{\theta})
   \;=\;
   p(\mathbf{v}^{(1)};\theta)
   \prod_{i=2}^{\ell}
      p
         \bigl(
            \mathbf{v}^{(i)} 
            \;\bigm|\;
            \mathbf{v}^{(i-1)};\theta
         \bigr).
\end{equation}

Because each $\mathbf{v}^{(i)}$ overlaps the previous chunk by $m$ frames, the context frames feed into the next chunk's generation, ensuring smooth transitions and continuity between chunks. To enable such Markovian temporal dependencies during sampling, it is crucial to train a flexible backbone model \(p(\mathbf{v};\theta)\) that can generalize across different sampling schemes, such as the one defined in Equation~\eqref{eq:markov}.


\subsection{Preliminary: Flow Matching for Videos}
\label{subsec:flow}

Our masking flow matching approach, named \emph{MaskFlow}, draws inspiration from previous works that apply individual noise levels to individual frames in a sequence \cite{chen2024diffusionforcing, ruhe2024rollingdiffusionmodels}. These works operate in a continuous space, and use diffusion processes to corrupt data. MaskFlow operates in a discrete token space and uses \emph{masking} to corrupt data. We seek to learn a continuous transition process in ``time'' \(t\) that moves from a purely masked sequence at \(t=0\) to the unmasked token sequence at \(t=1\). In our method, the timestep $t$ corresponds to the masking ratio, and represents the frame-level probability of a token being masked. Consider a video consisting of \(L\) frames, where each frame is mapped to a discrete latent space using a vector-quantized (VQ) tokenizer \cite{esser2021taming_vqgan}. This tokenizer encodes each frame in the video $\mathbf{v}$ to a set of discrete latent indices $\mathbf{x}_{\text{latent}} \in [K]^N$, which consists of $N$ tokens drawn from the tokenizer vocabulary of size $K$. Let \(\mathcal{F}\) denote the VQ encoder-decoder, i.e., the function that maps a video in pixel space to its tokenized representation. Then, we have

\begin{equation}
    \mathbf{x} = \mathcal{F}(\mathbf{v}) \in [K]^{L \times N},
\end{equation}

where \([K] = \{1, 2, \ldots, K\}\) is the set of all possible token indices which includes a special ``mask token" $M \in [K]$. The choice of tokenization is essential here, since it compresses spatial dimensions of $\mathbf{x}$ compared to $\mathbf{v}$ and allows us to employ discrete flow matching, which we outline in further detail in the following section.

\begin{algorithm}[!ht]
\caption{\textbf{Training with Frame-level Masking}}
\label{alg:training}
\begin{algorithmic}[1]
\REQUIRE 
  Dataset of tokenized video clips $\mathcal{D}$, 
  network $p(\mathbf{x}_1 \mid \mathbf{x}_t, \mathbf{t};\theta)$, 
  chunk size $k$

\WHILE{not converged}
    \STATE \textbf{Sample} a chunk of $k$ frames  from $\mathcal{D}$, denoted 
      $\mathbf{x}_1 = (x_1^1, x_1^2, \dots, x_1^k)$
    %\STATE \textbf{Initialize} fully-masked chunk: 
      %$\mathbf{x}_0 = (\, [M], [M], \dots, [M] \,)$
    \FOR{$f = 1, \dots, k$}
        \STATE $t_f \sim \mathcal{U}(0,1)$
        \STATE 
           $x_{t^f} \;\sim\; p_{t^f \mid 0,1}\bigl(\,\cdot \mid x_0^f,\,x_1^f\bigr)$,
        where $p_{t^f \mid 0,1}$ follows 
        $
           (1 - t^f)\,\delta_{x_0^f} 
           \;+\; 
           t^f\,\delta_{x_1^f}.
        $
    \ENDFOR
    \STATE $\mathbf{x}_t = (x_{t^1}^1,\, x_{t^2}^2,\, \dots,\, x_{t^k}^k)$
    \STATE 
    $
       \hat{\mathbf{x}}_1 \;=\; p\bigl(\mathbf{x}_1 \mid \mathbf{x}_t,\, \mathbf{t};\theta\bigr),
    $
    where $\mathbf{t} = (t^1,\ldots,t^k)$
    \STATE \textbf{Backpropagate} $\mathcal{L}_\theta(\mathbf{x}_1, \hat{\mathbf{x}}_1)$ and \textbf{update} $\theta$.
\ENDWHILE
\end{algorithmic}
\end{algorithm}
\vspace{-10pt}

\paragraph{Discrete Flow Matching.}  
Discrete flow matching \cite{gat2024discrete} defines a vector field \(u_t\) in a discrete space that can be traversed to yield a smooth probability transition between our source distribution of fully masked frame sequences $p(\mathbf{x}_0)$ and the distribution of unmasked sequences $p(\mathbf{x}_1)$. This vector field defines an optimal transport path between the two distributions. Concretely, we construct the conditional probability path:
\begin{equation}
    p_{t \,\vert\, 0,1}\bigl(\mathbf{x} \,\vert\, \mathbf{x}_0, \mathbf{x}_1\bigr)
    \;=\; (1-t)\,\delta_{\mathbf{x}_0}(\mathbf{x})
    \;+\; t\,\delta_{\mathbf{x}_1}(\mathbf{x}),
\end{equation}

where \(\delta_{\mathbf{x}_0}(\mathbf{x})\) and \(\delta_{\mathbf{x}_1}(\mathbf{x})\) are Dirac delta functions (analogous to one hot encodings) in the discrete space that allocate all probability mass to the fully masked and fully unmasked sequences at $t=0$ and $t=1$, respectively. For any intermediate value \(t \in (0,1)\), the interpolation governed by the weights \((1-t)\) and \(t\) yields a new video sequence \(\mathbf{x}_t\) that represents a partially corrupted sequence. This is achieved by sampling each token from a mixture distribution where $1-t$ represents the probability of a token being masked.
\vspace{-10pt}
\paragraph{Kolmogorov Equation in Discrete State Spaces.}
In continuous-state models, one leverages the Continuity Equation \cite{song2021scorebased_sde} to ensure that a vector field \(u(\mathbf{x}_t, t)\) induces the desired probability transition between \(p(\mathbf{x}_0)\) and \(p(\mathbf{x}_1)\). The discrete counterpart is given by the Kolmogorov Equation \cite{campbell2024generative}, which similarly characterizes how a probability distribution evolves in time over discrete states. To achieve a transition between the fully masked and fully unmasked video distributions, we define the vector field:

\begin{equation}
    u_t(\mathbf{x}_t) 
    \;=\; 
    \frac{t}{\,1 - t} 
    \Bigl[
    p_{1 \,\vert\, t}(\mathbf{x}_1 \mid \mathbf{x}_t, t; \theta) 
    \;-\; 
    \delta_{\mathbf{x}_t}(x)
    \Bigr],
\end{equation}

where \(p_{1|t}(\mathbf{x}_1 \mid \mathbf{x}_t, t; \theta)\) is the model-predicted distribution of clean tokens \(\mathbf{x}_1\) given a partially corrupted sequence \(\mathbf{x}_t\) at time \(t\). Here, \(\delta_{\mathbf{x}_t}(x)\) represents the discrete Dirac delta centered at \(\mathbf{x}_t\). By following \(u_t\) through time, we recover a path that transforms \(p(\mathbf{x}_0)\) into \(p(\mathbf{x}_1)\).

\subsection{Training with Frame-Level Masking}
\label{subsec:training}

The flow matching formulation introduced in Sections~\ref{subsec:long_vid} and \ref{subsec:flow} employs a single scalar timestep $t$ to interpolate between the fully masked and fully unmasked video distributions. Our training procedure uses a reparametrization of this timestep. In our method, videos are generated in chunks, and only a subset of the frames (the non-context frames) are sampled from a fully masked initial state. To better simulate this process during training, we reparametrize the global timestep $t$ into a per-frame timestep vector $\mathbf{t}=(t^1,\dots, t^k)$ where each timestep $t^f$ specifies the masking ratio applied to frame $f$. In our setup, the context frames are assigned $t^f = 1$ (i.e. fully unmasked) while the new frames receive a masking level sampled from $\mathcal{U}(0,1)$.
By training the model to unmask frames with varying masking ratios per frame, we ensure that the network can effectively handle unmasked context frames while still learning a continuous transition from $p(\mathbf{x}_0)$ to $p(\mathbf{x}_1)$. To emphasize the reconstruction of masked tokens, we follow \cite{hu2024maskneed} in applying a masking operation on the cross-entropy loss. This results in the following objective:
\begin{multline}
\label{eq:ce_loss_masked}
    \mathcal{L}_{\theta} \;=\; 
    \E_{p(\mathbf{x}_1)\,p(\mathbf{x}_0)\,\mathcal{U}(\mathbf{t};0,1)\,p_{t|0,1}(\mathbf{x}_t \,\vert\, \mathbf{x}_0,\mathbf{x}_1)} \\
    \Bigl[\;\underbrace{\delta_{[M]}(\mathbf{x}_t)\,( \mathbf{x}_1 )^\top}_{\text{Loss Masking}}
    \;\log \denoise(\mathbf{x}_1 \,\vert\, \mathbf{x}_t,\mathbf{t};\theta) 
    \Bigr],
\end{multline}

where $\delta_{[M]}(\mathbf{x}_t)$ indicates that only masked tokens are used in the cross-entropy computation. The choice of frame-level masking is essential because it aligns the task of generating chunks of size $k$ conditioned on $m$ clean context frames with our training task. In both scenarios, our models are tasked with unmasking frame sequences with varying masking levels across frames. We show that compared to a constant masking level baseline, this training choice enables chunkwise autoregressive rollout to long sequence lengths. Our training algorithm is shown in detail in Algorithm~\ref{alg:training}.

\subsection{Chunkwise Autoregression for Long Videos}
\label{sec:flex_long}

 To generate a coherent video of length \(L \gg k\), we employ the chunkwise autoregressive approach as described previously. Let \(m\) be the number of context frames provided to the model (drawn initially from ground-truth, later from previous generated frames). In each iteration, we pass \(k\) frames to the model, where the first \(m\) of these frames are context and the remaining \((k - m)\) frames are fully masked. The model unmasks these frames. Afterwards, we shift the context window forward by \(s\) and repeat this process, until we have generated \(L\) total frames. Figure~\ref{fig:sampling} illustrates this pipeline. Note that we dynamically increase the number of context frames $m$ in the final chunk in case there are less than $s$ frames left to generate. In those cases we set $m = k - R$ where $R$ is the remaining number of frames, giving the final chunk a larger context. We do this to avoid generating video lengths beyond $L$ which would result in either discarding generated frames or generating videos longer than $L$. This is shown in detail in Algorithm~\ref{alg:chunkwise}.

\paragraph{Autoregressive \textit{v.s.} Full-Sequence Generation.}
By varying the stride \(s\), we can interpolate between (i) a fully autoregressive mode (\(s = 1\)) with $m = k - s$, where we generate a single new frame per chunk, and (ii) a full-sequence mode (\(s = k - m\)), where we generate $k - m$ new frames simultaneously in each chunk. Smaller \(s\) increases compute cost but may yield higher frame quality, whereas larger \(s\) is more efficient, but may result in a drop in frame quality. Our experimental results shown in Table~\ref{tab:autoregression} support this intuition.

\paragraph{FM-Style \textit{v.s.} MGM-Style Sampling.}
MaskFlow supports two distinct sampling modes. In FM-style sampling, we gradually traverse the probability path from the fully masked sequence $\mathbf{x}_0$ to the final unmasked sequence $\mathbf{x}_1$. A smaller step size yields smoother transitions at the cost of more denoising steps. Alternatively, in MGM-style sampling, we apply confidence-based heuristic sampling similar to \citet{chang2022maskgit}. In each sampling step, the model computes token-wise confidence scores for each predicted token and selects a fraction of the most confident tokens to unmask. This sampling process allows us to generate video chunks efficiently in much fewer sampling steps.
\vspace{-10pt}

\paragraph{Timestep-dependent models and timestep-independent sampling.}

By default, our model backbones are timstep-dependent, meaning each forward pass receives a timestep vector $\mathbf{t}\in[0,1]^k$ that indicates the masking ratio of each frame. Internally, we embed $\mathbf{t}$ through a learnable mapping to produce conditioning vectors that modulate various layers (e.g., via layer norm shifts/scales). Interestingly, we can still sample these models timstep-independently. Concretely, when using MGM-style sampling, we iteratively unmask a chunk of tokens while simply passing $\mathbf{t}=\mathbf{0}$ at each iteration, effectively treating our timestep-dependent model as if it were timestep-independent:

\begin{equation}
    p(\mathbf{x}_1|\mathbf{x}_t;\theta) \approx p(\mathbf{x}_1|\mathbf{x}_{t}, \mathbf{t}=\mathbf{0};\theta).
\end{equation}

This works, since the learned network can infer the corruption state (mask ratio) from the input tokens alone. Thus, in practice, \emph{a single} trained model can serve both as a standard time-dependent (flow-matching) generator \emph{and} as a time-independent (MGM-style) sampler, providing greater flexibility at inference time.

\begin{algorithm}[ht!]
\caption{Chunkwise Autoregression for Long Videos}
\label{alg:chunkwise}
\begin{algorithmic}[1]
\REQUIRE Video length \(L\), context frames \(\mathbf{x}^{1:m} = (x^1,\dots,x^m)\), chunk size \(k\), stride \(s\), fully masked frame \([M]\), network \(p(\mathbf{x}_1 \mid \mathbf{x}_t,\mathbf{t};\theta)\)
\STATE \textbf{Initialize:} \(\hat{\mathbf{x}}_{1} \leftarrow (x^1,\dots,x^m)\); \(c \leftarrow m\) \COMMENT{current frame}
\WHILE{\(c < L\)}
    \STATE \(R \leftarrow L - c\) \COMMENT{remaining frames}
    \STATE \(h \leftarrow \min(R,\, s)\) \COMMENT{frames to generate this chunk}
    \IF{\(R \le s\)}
        \STATE \(m \leftarrow k - R\)
    \ENDIF
    \STATE \(\mathbf{x}_{\mathrm{context}} \leftarrow (x^{\,c-m+1}, \dots, x^c)\)
    \STATE \(\mathbf{x}_{\mathrm{mask}} \leftarrow (\underbrace{[M], \dots, [M]}_{h\text{ times}})\)
    \STATE \(\mathbf{x}_{\mathrm{out}} \sim p\Bigl(\mathbf{x}_1 \mid (\mathbf{x}_{\mathrm{context}},\, \mathbf{x}_{\mathrm{mask}}), \mathbf{t};\theta\Bigr)\)
    \STATE \(\mathbf{x}_{\mathrm{new}} \leftarrow (x_{\mathrm{out}}^{\,m+1}, \dots, x_{\mathrm{out}}^{\,m+h})\)
    \STATE \(\hat{\mathbf{x}}_1 \leftarrow (\hat{\mathbf{x}}_1,\, \mathbf{x}_{\mathrm{new}})\)
    \STATE \(c \leftarrow c + h\)
\ENDWHILE
\RETURN \(\hat{\mathbf{x}}_1\)
\end{algorithmic}
\end{algorithm}

