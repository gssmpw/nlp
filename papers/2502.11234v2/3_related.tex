\section{Related Work}

\paragraph{Long Video Generation.} 
The training dynamics and the sampling methodology in this work are inspired by works like Diffusion Forcing \cite{chen2024diffusionforcing,song2025historyguidedvideodiffusion}, Rolling Diffusion Models \cite{ruhe2024rollingdiffusionmodels} and AR-Diffusion \cite{wu2023ar}. The main motivation behind these works is to unify the benefits of autoregression and full sequence diffusion by applying token-specific noise levels during training, which allows the model to generate future frames without fully denoising past frames in a sequence. \citet{xie2024progressive} is a similar work that prescribes a progressive sampling schedule for increased smoothness of transitions between generation windows. FIFO-Diffusion is a training-free inference approach for infinite text-to-video generation that uses a similar progressive denoising schedule and latent partitioning to reduce the training-inference gap with pre-trained video diffusion models. Other methods like \cite{gao2024vid, zheng2024open} and \cite{blattmann2023stable} use context frame conditioning similar to our method, but do not focus on long video generation. The closest to our work is \citet{zhou2025taming}, who also employ a masking-based design to generate arbitrary-length videos autoregressively. There are two key differences in our approach: We do not condition frame generation on any previous ground truth frames during training, but adopt a frame-level masking approach that is more flexible. We also employ confidence-based MGM-style sampling, which lets us sample entire training windows in very few sampling steps, whereas \citet{zhou2025taming} employs MAR-style \cite{li2024autoregressive} sampling that requires a higher amount of sampling steps per individual frame and does not use vector quantization.
\vspace{-10pt}

\paragraph{Discrete Representations in Video Generation.}
There are several previous works that investigate the use of discrete representations for video diffusion. MaskGIT \cite{chang2022maskgit} is a generative transformer that uses a bidirectional transformer decoder to predict randomly masked tokens in an input sequence of image patches. This idea is extended to videos in MAGVIT \cite{yu2023magvit}, which tokenizes video pixel space inputs into spatial-temporal visual tokens and uses a masked auto-regressive approach to predict masked input tokens. Similar approaches like Muse \cite{Chang2023MuseTG} and MAGVIT-v2\cite{yu2023language_magvit2} have shown promise in scaling up image and video generation tasks, but suffer from training instabilities. Latte \cite{ma2024latte} is a latent diffusion transformer model that uses a pre-trained VAE-based tokenizer to reduce the dimensions of frame sequences as well as a mixture of spatial and temporal attention blocks designed to decompose spatial and temporal dimensions of input sequences. We adapt this backbone to handle frame-level timestep conditioning to denoise frame sequences with independent masking levels. Unlike previous discrete methods~\cite{hu2024maskneed,ma2024latte} that do not explicitly consider frame dependence in the noise schedule, we investigate how combining multiple sampling styles and leveraging guidance from previously generated frames can yield an efficient and flexible long-video generation paradigm.

\paragraph{Discrete Flow Matching.}
Flow matching \cite{lipman2022flow} is an emerging generative modeling paradigm that generalizes common formulations of diffusion models and offers more freedom in the choice of the source distribution. Flow matching models have seen wide adoption in speech \cite{liu2023generative}, image generation \cite{hu2024zigma,hulfm,dao2023flow, lipman2022flow}, super-resolution \cite{schusterbauer2024boosting}, depth estimation \cite{gui2024depthfm} and video generation \cite{jin2024pyramidal}, but their application in high-dimensional discrete domains is still limited. Discrete flow matching \cite{gat2024discrete,campbell2024generative,shi2024simplifiedgeneralizedmaskeddiffusion,sahoo2024simpleeffectivemaskeddiffusion} addresses this limitation, introducing a novel discrete flow paradigm designed for discrete data generation. Building on this, \citet{hu2024maskneed} validates the efficacy of discrete flow matching in the image domain and bridges the connection between Discrete Diffusion and Masked Generative Models \cite{chang2022maskgit}. %
In contrast, we explore vectorizing timesteps across frames for memory-efficient long-video generation with improved extrapolation to long sampling horizons while also analyzing the impact of sampling styles on video quality.


