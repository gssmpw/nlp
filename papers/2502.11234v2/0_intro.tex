\begin{figure}
    \centering
    \begin{tikzpicture}[font=\footnotesize]
        \node (img) {\includegraphics[width=0.7\columnwidth]{figpaper/nfe_vs_fvd_vs_ep_ffs_teaser.pdf}};
            \node[anchor=north west, xshift=25pt, yshift=-5pt] at (img.north west) {
                \begin{tabular}{ll}
                \scriptsize
                    \textcolor[HTML]{A0A0A0}{\rule{6pt}{6pt}} &Rolling Diffusion \cite{ruhe2024rollingdiffusionmodels} \\
                    \textcolor[HTML]{e9cbc4}{\rule{6pt}{6pt}} &Diffusion Forcing \cite{chen2024diffusionforcing} \\
                    \textcolor[HTML]{F4A700}{\rule{6pt}{6pt}} &MaskFlow (\textit{Ours})
                \end{tabular}
            };
    \end{tikzpicture}
    \vspace{-7pt}
    \caption{\textbf{Our method (MaskFlow) improves video quality compared to baselines while simultaneously requiring fewer function evaluations (NFE)} when generating videos $2\times$, $5\times$, and $10\times$ longer than the training window.
}
    \label{fig:teaser}
    \vspace{-10pt}
\end{figure}

\section{Introduction}

Due to the high computational demands of both training and sampling processes, long video generation remains a challenging task in computer vision. Many recent state-of-the-art video generation approaches train on fixed sequence lengths \cite{blattmann2023stable,blattmann2023align_videoldm,ho2022video} and thus struggle to scale to longer sampling horizons. Many use cases not only require long video generation, but also require the ability to generate videos with varying length. A common way to address this is by adopting an autoregressive diffusion approach similar to LLMs \cite{gao2024vid}, where videos are generated frame by frame. This has other downsides, since it requires traversing the entire denoising chain for every frame individually, which is computationally expensive. Since autoregressive models condition the generative process recursively on previously generated frames, error accumulation, specifically when rolling out to videos longer than the training videos, is another challenge.
\par
Several recent works \cite{ruhe2024rollingdiffusionmodels, chen2024diffusionforcing} have attempted to unify the flexibility of autoregressive generation approaches with the advantages of full sequence generation. These approaches are built on the intuition that the data corruption process in diffusion models can serve as an intermediary for injecting temporal inductive bias. Progressively increasing noise schedules \cite{xie2024progressive,ruhe2024rollingdiffusionmodels} are an example of a sampling schedule enabled by this paradigm. These works impose monotonically increasing noise schedules w.r.t. frame position in the window during training, limiting their flexibility in interpolating between fully autoregressive, frame-by-frame generation and full-sequence generation. This is alleviated in \cite{chen2024diffusionforcing}, where independent, uniformly sampled noise levels are applied to frames during training, and the diffusion model is trained to denoise arbitrary sequences of noisy frames. All of these works use continuous representations.
\par
We transfer this idea to a discrete token space for two main reasons: First, it allows us to use a masking-based data corruption process, which enables confidence-based heuristic sampling that drastically speeds up the generative process. This becomes especially relevant when considering frame-by-frame autoregressive generation. Second, it allows us to use discrete flow matching dynamics, which provide a more flexible design space and the ability to further increase our sampling speed. Specifically, we adopt a \emph{frame-level masking} scheme in training (versus a \emph{constant-level masking} baseline, see Figure~\ref{fig:training}), which allows us to condition on an arbitrary number of previously generated frames while still being consistent with the training task. This makes our method inherently versatile, allowing us to generate videos using both full-sequence and autoregressive frame-by-frame generation, and use different sampling modes. We show that confidence-based masked generative model (MGM) style sampling is uniquely suited to this setting, generating high-quality results with a low number of function evaluations (NFE), and does not degrade quality compared to diffusion-like flow matching (FM)-style sampling that uses larger NFE. 
Combining frame-level masking during training with MGM-style sampling enables highly efficient long-horizon rollouts of our video generation models beyond $10 \times$ training frame lengths without degradation. We also demonstrate that this sampling method can be applied in a timestep-\emph{independent} setting that omits explicit timestep conditioning, even when models were trained in a timestep-dependent manner, which further underlines the flexibility of our approach. In summary, our contributions are the following:

\begin{itemize}
    \item To the best of our knowledge, we are the first to unify the paradigms of discrete representations in video flow matching with rolling out generative models to generate arbitrary-length videos. 
    \item We introduce MaskFlow, a frame-level masking approach that supports highly flexible sampling methods in a single unified model architecture.
    \item We demonstrate that MaskFlow with MGM-style sampling generates long videos faster while simultaneously preserving high visual quality (as shown in Figure~\ref{fig:teaser}).
    \item Additionally, we demonstrate an additional increase in quality when using full autoregressive generation or partial context guidance combined with MaskFlow for very long sampling horizons.
    \item We show that we can apply MaskFlow to both timestep-dependent and timestep-independent model backbones without re-training.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figpaper/training.pdf}
    \caption{\textbf{MaskFlow Training:} For each video, Baseline training applies a single masking ratios to all frames, whereas our method samples masking ratios independently for each frame.}
    \vspace{-10pt}
    \label{fig:training}
\end{figure}
















