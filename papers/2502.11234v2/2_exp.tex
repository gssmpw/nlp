\vspace{-1pt}
\section{Experiments}

\subsection{Datasets and Evaluation Metrics}

\paragraph{Datasets.} We mainly consider two datasets: Deepmind Lab (DMLab) for evaluating performance in diverse ego-centric views and FaceForensics (FFS) for assessing video fluency. DMLab contains videos of random walks in a 3D maze, while FFS consists of deepfakes. Both datasets are preprocessed and tokenized using SD-VQGAN \cite{rombach2022high_latentdiffusion_ldm} for training. Further details are provided in the Appendix.

\paragraph{Evaluation metrics: FVD for video quality, NFE for sampling efficiency.}  For video generation, we use Fr√©chet Video Distance (FVD) \cite{unterthiner2018towards} as our main evaluation metric. For FVD, we adhere to the evaluation guidelines introduced by StyleGAN-V \cite{ma2024latte,skorokhodov2022stylegan_v}.
%
For all generation experiments requiring context frames, we randomly sample consecutive context frames from each ground-truth video in the dataset, and generate a corresponding generated video using our trained models. 
%
To compute FVD, we use a randomly sampled window of $L$ frames from the ground-truth videos, and sample the same number of generated videos using our models. This amounts to 704 videos for FFS, and 625 videos for DMLab FVD calculation across different sampling horizons $L$. 
%
We additionally evaluate the sampling efficiency of our method against various baselines by comparing the required number of function evaluations (NFE) and sampling wall clock times using identical compute resources.

\subsection{Training details}

We use a vocabulary size $K=16{,}384$ and token length $1{,}024$ to compress video frames by a compression factor of $8$. We then train on a small subset of training sequences of $k=16$ frames for FFS and $k=36$ frames for DMLab. We use a Latte XL2 \cite{ma2024latte} backbone with 760M parameters for all FFS experiments, and a smaller Latte B2 backbone architecture with 129M parameters for DMLab, and train it using discrete flow matching dynamics. Please refer to the Appendix for more detailed information about the training recipe and hyperparameters.


\subsection{Main Results}

\begin{table}[ht!]
    \centering
    \normalsize
    \resizebox{0.47\textwidth}{!}{
    \begin{tabular}{l|crr}
    \toprule
    \textbf{Sampling Mode} & \makecell{\textbf{Extrapolation} \\ \textbf{Factor}} 
    & \makecell{\textbf{Total} \\ \textbf{NFE}} 
    & \makecell{\textbf{FVD} $\downarrow$} \\
    \midrule
        Diffusion Forcing~\cite{chen2024diffusionforcing} & $2\times$ & $798$ & 144.43 \\ 
        Rolling Diffusion~\cite{ruhe2024rollingdiffusionmodels} & $2\times$ & $750$ & 72.49 \\
        \hline
        \textit{MaskFlow} (FM-Style) & $2\times$ & $788$ & $66.94$ \\
        \rowcolor{gray!8}\textit{MaskFlow} (MGM-Style) & $2\times$ & \textbf{60}  & \textbf{59.93} \\
        \midrule
        \midrule
        Diffusion Forcing~\cite{chen2024diffusionforcing} & $5\times$ & $1{,}596$ & 272.14 \\
        Rolling Diffusion~\cite{ruhe2024rollingdiffusionmodels} & $5\times$ & $1{,}652$ & 248.13 \\ 
        \hline
        \textit{MaskFlow} (FM-Style) & $5\times$ & $1{,}500$ & $118.81$ \\
        \rowcolor{gray!8}\textit{MaskFlow} (MGM-Style) & $5\times$ & \textbf{120}  & \textbf{108.74} \\
        \midrule
        \midrule
        Diffusion Forcing~\cite{chen2024diffusionforcing} & $10\times$ & $3{,}192$ & 306.31 \\
        Rolling Diffusion~\cite{ruhe2024rollingdiffusionmodels} & $10\times$ & $3{,}092$ & 451.38 \\ 
        \hline
        \textit{MaskFlow} (FM-Style) & $10\times$ & $3{,}000$ & \textbf{174.85} \\
        \rowcolor{gray!8}\textit{MaskFlow} (MGM-Style) & $10\times$ & \textbf{240}  & $214.39$ \\
        \bottomrule
    \end{tabular}}
    \vspace{-7pt}
    \caption{\textbf{Both MGM-style and FM-style sampling extrapolate to longer sequences with similar FVD, but MGM-style is much faster.} Performance deteriorates for larger extrapolation factors, but MaskFlow consistently outperforms Diffusion Forcing and Rolling Diffusion. Results are on timestep-dependent FaceForensics models with full sequence generation ($s = k - m$).}
    \label{tab:extrapolation}
\end{table}


\begin{comment}
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.495\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figpaper/nfe_vs_fvd_vs_ep_ffs.pdf}
        \caption{\textbf{FaceForensics}}
        \label{fig:ffs}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.495\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figpaper/nfe_vs_fvd_vs_ep_dmlab.pdf}
        \caption{\textbf{DMLab}}
        \label{fig:dmlab}
    \end{subfigure}
    \caption{\textbf{MaskFlow performance scales favorably across NFE for different extrapolation factors across both datasets.} Shows a comparison between MaskFlow full sequence and MaskFlow autoregressive modes and other baselines across extrapolation factors.}
    \vspace{-10pt}
    \label{fig:maskflow_scaling}
\end{figure}
\end{comment}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figpaper/nfe_vs_fvd_vs_ep_dmlab_legendright.pdf}
    %\label{fig:dmlab}
    \vspace{-18pt}
    \caption{\textbf{MaskFlow performance scales favorably across NFE for different extrapolation factors.} Shows a comparison between MaskFlow full sequence and MaskFlow autoregressive modes and other baselines across extrapolation factors on DMLab.}
    \vspace{-10pt}
    \label{fig:maskflow_scaling}
\end{figure}


\paragraph{Baselines.}
The two most comparable works to our method are \citet{chen2024diffusionforcing} and \citet{ruhe2024rollingdiffusionmodels}. Both of these techniques propose novel sampling methods that can be rolled out to long video lengths, and also apply frame-specific noise levels. Both of these approaches are diffusion-based and operate on continuous representations, whereas we operate on discrete tokens and use masking. We re-implement both the pyramid sampling scheme proposed in Diffusion Forcing and the Rolling Diffusion sampling method in our discrete setting. This allows us to compare the baseline sampling methods to MaskFlow on the same model backbones. We also compare MaskFlow to a constant masking level baseline from \citet{hu2024maskneed} to evaluate the design choice of frame-level masking.

\paragraph{Our MGM-style sampling approach can generate long videos efficiently with minimal degradation.} 
Table~\ref{tab:extrapolation} shows the ability of our model to generate long videos. We define the \emph{extrapolation factor} as the ratio of sampling and training window lengths, so an extrapolation factor of $2 \times$ means we generate videos twice as long as the training videos, e.g. $32$ frames for FFS on a training window size of $k=16$ frames. The experiments in Table~\ref{tab:longer_train_window_baseline} of the Appendix all use full sequence generation with $s = k - m$. While video quality deteriorates for longer extrapolation factors due to error accumulation, our method is able to maintain visual quality for large extrapolation factors. This ability is enabled by our training approach, which ensures that our models are able to unmask arbitrary mixtures of low and high masking ratio frames. This allows us to condition each chunk on arbitrary numbers of previously generated frames, which is consistent with the training task. A detailed qualitative overview is shown in Figure~\ref{fig:faces1}. Both FM-style and MGM-style sampling modes retain this ability, but our MGM-style sampling generates high-quality results with lower NFE. We also show that MaskFlow outperforms both Rolling Diffusion~\cite{ruhe2024rollingdiffusionmodels} and Diffusion Forcing~\cite{chen2024diffusionforcing} with pyramid noise schedule in discrete settings.

\paragraph{Frame-level masking does not reduce performance on original training window length generation.} Table~\ref{tab:baseline_training_window} shows that our frame-level masking approach does not reduce performance for a single chunk compared to a constant masking baseline. We compare a frame-level masking DMLab model trained on $k=36$ frames with a constant masking baseline and show that our frame-level masking models outperform the constant masking baseline across two sampling modes. This demonstrates that our frame-level masking training does not trade off quality on training window length generation for the ability to generate longer videos.


\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figpaper/dmlab_autoreg.pdf}
    \vspace{-20pt}
    \caption{\textbf{Fully autoregressive sampling stabilizes DMLab videos beyond extrapolation factor $10 \times$.} All examples use fully autoregressive MaskFlow (MGM-style) sampling with $s=1$ and 6,500 NFE in total. The final context frame is shown in red.}
    \vspace{-10pt}
    \label{fig:dmlabauto}
\end{figure}

\paragraph{Fully Autoregressive Sampling increases video quality at the cost of inference speed.} To further illustrate the flexibility of our method, we run a series of experiments using a sampling stride of $s=1$ with $m = k-1$. We thus initialize the generative process by conditioning on almost a full training clip, and then generating new frames frame by frame using our existing sampling approaches. This requires us to traverse the entire unmasking chain for each generated frame, making this sampling method slower than the sampling approach employed in Table~\ref{tab:extrapolation}. Specifically on DMLab, which is more dynamic than FFS, this substantially improves results, enabling extremely long high-quality rollouts (see Figure~\ref{fig:dmlabauto}. The findings in Table ~\ref{tab:autoregression} thus demonstrate that for certain datasets, such as FFS, iterative full sequence generation already works very well, whereas autoregressive sampling is more suitable for more dynamic datasets, such as DMLab. Since our MGM-style sampling is able to generate new frames in very few NFE, autoregressive frame-by-frame generation actually requires a similar NFE than the baselines that do full sequence generation with FM-style sampling. Figure~\ref{fig:maskflow_scaling} highlights this, showing that MaskFlow scales favorably compared to other methods in terms of NFE for $s=1$ and $s=k-m$. A more detailed comparison of autoregressive and full sequence sampling in terms of wall clock sampling speed can be found in Table~\ref{tab:speed_comparison} of the Appendix.

\begin{table}[ht]
    \centering
    \normalsize
    \resizebox{0.47\textwidth}{!}{%
    \begin{tabular}{l|ccrr}
        \toprule
        & \makecell{\textbf{Extrapolation} \\ \textbf{Factor}}
        & \makecell{\textbf{Sampling} \\ \textbf{Stride}}
        & \makecell{\textbf{Total} \\ \textbf{NFE}}
        & \makecell{\textbf{FVD} $\downarrow$} \\
        \midrule
        FaceForensics   & $2\times$  & $s=14$ (\textit{full sequence}) & \textbf{60} & 59.93 \\
        FaceForensics   & $2\times$  & $s=1$ (\textit{autoregressive})  & 340 & \textbf{30.43} \\
        \midrule
        FaceForensics   & $5\times$  & $s=14$ (\textit{full sequence}) & \textbf{120} & 108.74 \\
        FaceForensics & $5\times$  & $s=1$ (\textit{autoregressive})  & 1,300 & \textbf{103.69} \\
        \midrule
        FaceForensics   & $10\times$ & $s=14$ (\textit{full sequence}) & \textbf{240} & 214.39 \\
        FaceForensics   & $10\times$ & $s=1$ (\textit{autoregressive})  & 2,900 & \textbf{165.02} \\
        \midrule
        \midrule
        DMLab & $2\times$  & $s=24$ (\textit{full sequence}) & \textbf{60} & 195.84 \\
        DMLab & $2\times$  & $s=1$ (\textit{autoregressive})  & 740  & \textbf{42.53} \\
        \midrule
        DMLab & $5\times$  & $s=24$ (\textit{full sequence}) & \textbf{140}  & 334.15 \\
        DMLab & $5\times$  & $s=1$ (\textit{autoregressive})  & 2,900 & \textbf{80.56} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-7pt}
    \caption{\textbf{Fully autoregressive sampling significantly improves performance on DMLab but also increases the required NFE.} Results are obtained using best-performing models with MGM-style sampling mode.}
     \vspace{-10pt}
    \label{tab:autoregression}
\end{table}

\begin{table}
    \centering
    \scriptsize
    \begin{tabular}{c c r}
        \toprule
          \multirow{2}{*}{\textbf{Extrapolation}} & \multirow{2}{*}{\textbf{Guidance}} 
          & \textbf{FVD} $\downarrow$ \\
        \cmidrule(lr){3-3}
          \textbf{Factor} & \textbf{Level $\omega$} 
          & DMLab \\
        \midrule
        $1\times$
        & $0$
        & \textbf{45.84} \\
        $1\times$
        & $1.0$
        & \text{49.76} \\
       $1\times$
       & $1.5$
       & \text{47.25} \\
       $1\times$
        & $2.0$
         & \text{46.29} \\
       \hline
        $2\times$
          & $0$
          & \text{219.33} \\
        $2\times$
          & $1.0$
          & \text{189.48} \\
        $2\times$
          & $1.5$
          & \text{167.80} \\
        \rowcolor{gray!8}$2\times$
          & $2.0$
          & \textbf{141.94} \\
        \hline
        $5\times$
          & $0$
          & \text{402.73} \\
        $5\times$
          & $1.0$
          & \text{403.32} \\
        $5\times$
          & $1.5$
          & \text{315.26} \\
        \rowcolor{gray!8} $5\times$
          & $2.0$
          & \textbf{281.20} \\
        \bottomrule
    \end{tabular}
    %}
    \vspace{-7pt}
    \caption{\textbf{Scaling partial context guidance $\omega$ can substantially improve performance for longer extrapolation factors.} Results use MaskFlow with MGM-Style sampling and $s = k - m$.}
    \vspace{-10pt}
    \label{tab:partial_context}
\end{table}


\paragraph{Scaling partial context guidance further improves performance on full sequence generation.}
Inspired by classifier-free guidance \cite{ho2021classifier} and history guidance in Diffusion Forcing \cite{chen2024diffusionforcing,song2025historyguidedvideodiffusion}, we propose a training-free sampling method that fuses multiple model predictions of \(\,p(x_{1}| x_{t};\theta)\) using different levels of conditioning on past frames. 
Concretely, we run forward passes where \(x_t\) contains: 
(i) \emph{no} context frames (unconditional) , 
(ii) \emph{partially masked} context frames (partial conditioning), 
and (iii) \emph{fully clean} context frames (fully conditional). 
We then fuse the predicted logits with a guidance scale $\omega$. 
By using \emph{partially masked} rather than fully clean context frames for some of these passes, the model is encouraged to preserve global movement and dynamics without strictly copying the observed context. Formally, if \(z_{\mathrm{uncond}} (i),\ z_{\mathrm{partial}}(ii),\ z_{\mathrm{cond}} (iii)\) denotes logits from the three forward passes, one can construct a composite logit distribution via
$z_{\mathrm{cond}} + \omega \cdot (z_{\mathrm{partial}} - z_{\mathrm{uncond}})$ that balances sample variety (unconditional) with temporal coherence (partial and full context). Partial context guidance requires no re-training and can yield improved fidelity and motion consistency. Table~\ref{tab:partial_context} shows performance improvements achieved on timestep-independent DMLab models.



\begin{table}[t]
    \centering
    \normalsize
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l c c}
        \toprule
          \multirow{2}{*}{\textbf{Training}} & \multirow{2}{*}{\textbf{Sampling}} 
          & \textbf{FVD} $\downarrow$ \\
        \cmidrule(lr){3-3}
          \textbf{Mode} & \textbf{Mode (NFE)} 
          & DMLab \\
        \midrule
        Constant Masking \cite{hu2024maskneed}$^\dagger$ 
          & FM-Style
          & \text{53.31} \\
       
          Frame-level Masking 
          & Diffusion Forcing~\cite{chen2024diffusionforcing} 
          & 60.30 \\
          Frame-level Masking 
          & Rolling Diffusion~\cite{ruhe2024rollingdiffusionmodels}
          & 52.43 \\
          \hline 
          \hline 
        Frame-level Masking 
          & \textit{MaskFlow} (MGM-Style)  
          & 53.17 \\
           Frame-level Masking 
          & \textit{MaskFlow} (FM-Style)
          & \textbf{49.62} \\
        \bottomrule
    \end{tabular}
    }
    {\fontsize{6}{7}\selectfont ($\dagger$) denotes pretrained by us using their official implementation.}
    \vspace{-7pt}
    \caption{\textbf{Frame-level masking performs on par with constant masking when sampling window equals training window length videos}. MGM-style sampling performs well with only 20 NFE.}
    \label{tab:baseline_training_window}
\end{table}

\subsection{Ablations}

\begin{table}[t]
\centering
\normalsize
\resizebox{\columnwidth}{!}{
\begin{tabular}{cc|cccc}
\toprule
\multirow{2}{*}{\textbf{Sampling}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Sampling-}} & \multirow{2}{*}{\textbf{Extrap.}} & \multicolumn{2}{c}{\textbf{FVD} $\downarrow$} \\
\cmidrule(lr){5-6}
\textbf{Mode} & \textbf{Time Dep.} & \textbf{Time Indep.} & \textbf{Factor} & DMLab & FaceForensics \\
\midrule
FM-Style    & \textcolor{green}{\cmark} & \textcolor{red}{\xmark} & 1√ó & 55.19 & 48.98 \\
MGM-Style   & \textcolor{red}{\xmark} & \textcolor{green}{\cmark} & 1√ó & \textbf{45.84} & 77.04 \\
\rowcolor{gray!8}MGM-Style   & \textcolor{green}{\cmark} & \textcolor{red}{\textcolor{green}{\cmark}} & 1√ó & 53.17 & \textbf{45.92} \\
\midrule
FM-Style    & \textcolor{green}{\cmark} & \textcolor{red}{\xmark} & 2√ó & 267.80 & 66.94 \\
MGM-Style   & \textcolor{red}{\xmark} & \textcolor{green}{\cmark} & 2√ó & 219.33 & 109.96 \\
\rowcolor{gray!8}MGM-Style   & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} & 2√ó & \textbf{188.22} & \textbf{59.93} \\
\midrule
FM-Style    & \textcolor{green}{\cmark} & \textcolor{red}{\xmark} & 5√ó & 360.61 & 118.81 \\
MGM-Style   & \textcolor{red}{\xmark} & \textcolor{green}{\cmark} & 5√ó & 402.73 & 137.66 \\
\rowcolor{gray!8}MGM-Style   & \textcolor{green}{\cmark} & \textcolor{green}{\cmark} & 5√ó & \textbf{334.15} & \textbf{108.74} \\
\bottomrule
\end{tabular}}
\vspace{-7pt}
\caption{\textbf{Timestep-dependent models can generate high-quality results with timestep-independent sampling.} Timestep-dependent models with timestep-independent sampling show best results across various extrapolation factors.}
\vspace{-10pt}
\label{tab:time_condition}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{figpaper/faces.pdf}
    \vspace{-17pt}
    \caption{\textbf{MGM-style sampling generates visually pleasing videos with two \textcolor{red}{context frames} beyond $10\times$ training frame length with only 20 sampling steps.} Shows sampling mode and total NFE in brackets, and frame indices $f$. The left and right subfigures show distinct videos obtained with identical sampling modes and context frames.
    }
    \vspace{-10pt}
    \label{fig:faces1}
\end{figure}




\paragraph{Timestep-dependent models can be sampled in a time-independent training-free manner.}
An additional interesting observation is that MGM-style sampling without explicit timestep conditioning is able to generate high-quality results in the full-sequence case. We thus compare timestep-dependent and timestep-independent models under different sampling modes in Table ~\ref{tab:time_condition}. Our results demonstrate that the timestep-dependent models when sampled with MGM-style sampling actually perform best. We hypothesize that this is due to the more explicit inductive bias of timestep conditioning during training, and that this guides the learning process towards improved unmasking irrespective of the actual timesteps passed during inference. We are thus able to apply our sampling modes across timestep-dependent and independent models without requiring any re-training, which further underlines the flexibility of our approach.
\vspace{-10pt}

\paragraph{MGM-style and FM-style NFE choices minimize visual quality and sampling efficiency tradeoffs.}
The choice of NFE in our work is driven empirically. We compare generation quality when generating a single chunk $k$ on both datasets and tune our NFE accordingly for FM-style and MGM-style sampling modes. We are aware that our observations regarding sampling speeds depend on the choice of NFE, so we compare video quality for a lower number of sampling steps for both sampling modes on both datasets. In Figure ~\ref{fig:nfe_fvd}, we show that our choices of 20 for MGM-style and 250 for FM-style sampling achieve the best trade-off between sampling efficiency and quality, since video quality saturates for higher NFE in both modes across both datasets.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.495\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figpaper/fvd_vs_nfe_mg.pdf}
        \caption{MGM-style sampling}
        \label{fig:nfe_fvd_mgm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figpaper/fvd_vs_nfe_fm.pdf}
        \caption{FM-style sampling}
        \label{fig:nfe_fvd_fm}
    \end{subfigure}
    %\vspace{-7pt}
    \caption{\textbf{NFE choices for both MGM-style ($20$) and FM-style ($250$) suitably trade off sampling speed with visual quality.} Figures show FVD on a single chunk of size $k$ for timestep-dependent frame-level masking models.}
    \vspace{-10pt}
    \label{fig:nfe_fvd}
\end{figure}













