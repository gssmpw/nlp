\section{Background and Related Work}
\label{sec:related}
\textbf{Hand prehension analysis.}
Human-Computer Interaction (HCI) is the research field that focuses on making technological interaction easy for the user**Shneiderman, "The Limits of Attention"**. It has been getting a lot of focus by emerging new technologies in the modern world. Hands play an important role in user interactions, and hand prehension is one of the most important domains in HCI, psychology, and neuroscience**Castellucci, "Hand Kinematics for Grasping and Manipulation"**. Some studies that focused on hand preshaping in R2G tasks proved that grasping information depends on the object's size, shape, and intended action**Ruffaldi, "Hand Preshaping During Reach-to-Grasp Movements"**. Some papers studied the relationship between hand transport and grasp formation. They found that, while hand transport and grasp formation are largely independent, pre-shaping is typically completed before the hand reaches the object**Zaal, "The Role of Hand Transport in Grasping"**. Most of these work rescale, trim, or resample the motion sequences which is not practical in real-time. Valkov et al.**Valkov, "Classifying Intended Actions from Reach-to-Grasp Movements"**, attempted to classify the intended object using reach-to-grasp hand kinematics. However, their network failed to reliably discriminate the synthetic object, achieving performance close to chance level. In this paper, we predict the intended object in the reach-to-grasp task, before grasping the object and without synchronizing or trimming motion sequences.

\noindent
\textbf{Machine learning for MTSC.}
While a wide range of Machine-learning approaches classify univariate time series, multivariate time series have received less focus. We refer to recent surveys in the field**Gupta, "A Survey on Time Series Classification"**. Middlehurst et al.**Middlehurst, "Time Series Classification: A Review and Comparison of Methods"**, categorize the different approaches into methods based on distance, feature, interval, shapelet, dictionary, convolution, and deep learning as well as hybrid ones. As the best approach may vary from application to application, meta-ensemble approaches have been introduced that combine several approaches and weigh them, where the weights are learned. \textit{Hierarchical Vote Collective of Transformation-based Ensembles v2 (HIVE-COTE2)} is a state-of-the-art meta ensemble for time series classification consisting of four classifiers of diverse categories identified by Middlehurst et al. It includes phase-independent shapelets, bag-of-words-based dictionaries, phase-dependent intervals, and convolution-based classifiers. Each classifier is trained independently to estimate the probability of class membership for unseen data~\cite{middlehurst2021hive}. We follow this approach by employing HIVE-COTE2 to our data. However, due to the large computation times, we investigated the learned weights and then focused on the most influential classifier only, which was possible without a loss of accuracy.

\noindent
\textbf{Explainable AI.}
Advances in machine-learning algorithms have significantly enhanced predictive power and accuracy. However, this progress has come at the cost of increasing complexity, presenting a challenging trade-off between performance and transparency. The field of Explainable AI (XAI) addresses the explainability and interpretability of machine learning and deep learning algorithms**Gunning, "Explainable Artificial Intelligence (XAI)"**.
For recent surveys in XAI, see, e.g.,**Lipton, "The Mythos of Model Interpretability"**, and, for the recent surveys in XAI for time series, see, e.g.,**Kersting, "Time Series Forecasting with Explainable Models"**.

Kamath et al.**Kamath, "Explainability Methods for Machine Learning"**, categorize XAI methods based on their scope, stage, and model type. 
%
XAI methods can be categorized as global or local in \textit{scope}. Global methods are valuable for interpreting the overall, macro-level behavior of models, while local methods are useful for understanding their behavior at a micro-level, focusing on individual predictions.
%
XAI methods can be divided into three categories based on \textit{stage}: pre-model, intrinsic, or post-hoc methods. This is based on whether they are applied before, during, or after a model makes its prediction. 
When investigating the \textit{model type}, methods can be model-specific or model-agnostic. Model-specific methods utilize unique characteristics or the architecture of a particular model to achieve explainability. While many pre-model and post-hoc explainability techniques are model-agnostic and can be applied to a broad range of models, some are tailored to specific types of models (e.g., convolutional neural networks) and are limited to their respective architectures.
Model-agnostic methods have the advantage that they can be implemented and used for any model and can compare different models together. The advantage of post-hoc methods is that they can be applied to any trained black-box model, i.e., they can work for a wide variety of model algorithms, without the need for understanding their internal structure.
We conclude that a global, post-hoc, and model-agnostic method is most desirable for our purposes, as it gives the user some global insights and is comparable and applicable to any other trained models. 

Some well-established post-hoc methods are Shapely values, Local Interpretable Model-Agnostic Explanations (LIME), Partial Dependence Plots (PDP) , confusion matrices, and global surrogates.
Shapely values are based on the game theory and compute the (weighted) average of the marginal contributions of each player (i.e., feature value) across all possible coalitions**Shapley, "A Value for n-Person Games"**. 
Local Interpretable Model-Agnostic Explanations (LIME) is a model-agnostic technique for generating interpretable explanations of individual predictions in the vicinity of the prediction. This approach operates by randomly perturbing instances and fitting a local surrogate model to these perturbations**Ribeiro, "Model-Agnostic Interpretability of Machine Learning"**.
Partial Dependence Plots (PDP), as introduced by Friedman**Friedman, "Greedy Function Approximation: A Gradient Boosting Machine"**, quantify the marginal impact of one or more features on a machine-learning model's predicted outcome. It is among the most widely used model-agnostic techniques**Strobl, "Conditional Variable Importance for Random Forests"**.
Confusion matrices are commonly used to visualize classification results on the test dataset. They provide quantitative metrics to assess the model's generalization performance and serve as a diagnostic tool to analyze its behavior on individual classes**Bzdok, "A Toolbox for Load-Independent Assessment of Intrinsic Connectivity Maps from Resting-State fMRI Data"**.
The global surrogate model aims to explain a complex model by approximating the predictions with a simpler, more interpretable model. The simpler model provides insights into the global behavior of the complex model. Global surrogate models typically lack the ability to provide precise predictions for individual data instances. Even when a surrogate model accurately mirrors the predictions of a black-box model, it does not guarantee that these predictions correspond to real-world outcomes**Louppe, "Understanding Black-Box Predictions via Influence Functions"**.

In this paper, we require a global, post-hoc, and model-agnostic approach that fulfills a number of tasks. For detecting the trade-off point between prediction earliness and accuracy, we develop a temporal plot, which is combined with confusion matrix visualizations to investigate how the models evolve with increasing time-series lengths. Confusion matrices provide a good global overview of the classification output. For the investigation of the influence of individual features on the classification output, Partial Dependency Plots have been developed, as explained above, and we follow this approach. For the detailed investigation of the classification evolution for individual multi-variate time series, we adopt the heatmap visualization proposed by Nourani et al.**Nourani, "Multi-Modal Time Series Forecasting"**.