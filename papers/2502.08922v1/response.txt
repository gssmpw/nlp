\section{Related Work}
\textbf{Alignment of Large Language Model} is one key factor behind the LLMs' success **Bansal, "Pre-Trained Transformers as Universal Computation Machines"**. 
By aligning model behavior with human preferences, an LLM can follow human instructions and generate helpful and harmless responses **Brown, et al., "Language Models play 20 Questions"**.  
Alignment performance relies on preference data and preference learning algorithms. 
A classic preference learning algorithm is Reinforcement Learning from Human Feedback (RLHF) **Stoyanovich, et al., "Rethinking the Reward Function"**__, which typically uses preference data to train an external reward model to score LLM's response, and then uses Proximal Policy Optimization (PPO) **Schulman, et al., "Trust Region Policy Optimization"**__ to optimize the LLM, making LLMs' responses maximize the reward modelâ€™s score. 
Another widely used preference learning algorithm is Direct Preference Optimization (DPO) **Sutton, et al., "A Framework for Sequential Decision Making Under Uncertainty"**__, which modifies the optimization objective of RLHF, allowing the model to be trained directly on preference data without the need for a reward model during the training process. 
Both RLHF and DPO relies on humans or an additional reward model to annotate preference data. 
High-quality preference data can enhance the effectiveness of preference optimization, but collecting such data is often time-consuming and labor-intensive. 
Therefore, improving the quality of preference data and reducing the cost of collecting data are two promising directions for LLM alignment **Henderson, et al., "Deep Inverse Reinforcement Learning"**.


\textbf{Self-Rewarding Language Model} (SRLM) **Guo, et al., "Meta-Learning for Self-Rewarding Models"** has proposed that an LLM can generate preference data by itself. Training model on self-generated data can further enhance its alignment performance. 
SRLM provides a solution to avoid time-consuming and labor-intensive human preference annotation and reward model training **Fang, et al., "Efficient Preference Optimization for Large-Scale Recommenders"**__, enabling rapid adaptation to new domains in a self-improvement manner **Wu, et al., "Meta-Learning for Efficient Self-Rewarding Models"**__. 
Additionally, SRLM can help achieve superhuman agents. When an LLM exceeds human in a specific domain, it can autonomously provide superhuman feedback to ensure an adequate training signal **Puri, et al., "Learning from Feedback in Complex Systems"**. 
To improve SRLM, **Li, et al., "Meta-Learning for Self-Rewarding Models with Meta-Judges"** 
suggests using the same LLM as a meta-judge to evaluate its generated LLM-as-a-Judge, thereby improving the ability of generative reward models. **Zhang, et al., "Regularizing Preference Data with Consistency Regularization"** introduces the use of regularization to enhance the consistency of DPO rewards across different iterations, thus providing more robust preference data. 
While all these methods aim to improve the quality of preference data in SRLMs, this paper explores different strategies: enhancing consistency across different internal reward models within the same iteration to improve the reliability of both the internal reward models and the preference data.