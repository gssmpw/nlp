[
  {
    "index": 0,
    "papers": [
      {
        "key": "bommasani2021opportunities",
        "author": "Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others",
        "title": "On the opportunities and risks of foundation models"
      },
      {
        "key": "wang2023aligninglargelanguagemodels",
        "author": "Yufei Wang and Wanjun Zhong and Liangyou Li and Fei Mi and Xingshan Zeng and Wenyong Huang and Lifeng Shang and Xin Jiang and Qun Liu",
        "title": "Aligning Large Language Models with Human: A Survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "bai2022traininghelpfulharmlessassistant",
        "author": "Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bai2022training",
        "author": "Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "key": "bai2022constitutional",
        "author": "Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "schulman2017proximalpolicyoptimizationalgorithms",
        "author": "John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov",
        "title": "Proximal Policy Optimization Algorithms"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "kaufmann2024surveyreinforcementlearninghuman",
        "author": "Timo Kaufmann and Paul Weng and Viktor Bengs and Eyke H\u00fcllermeier",
        "title": "A Survey of Reinforcement Learning from Human Feedback"
      },
      {
        "key": "casper2023openproblemsfundamentallimitations",
        "author": "Stephen Casper and Xander Davies and Claudia Shi and Thomas Krendl Gilbert and J\u00e9r\u00e9my Scheurer and Javier Rando and Rachel Freedman and Tomasz Korbak and David Lindner and Pedro Freire and Tony Wang and Samuel Marks and Charbel-Rapha\u00ebl Segerie and Micah Carroll and Andi Peng and Phillip Christoffersen and Mehul Damani and Stewart Slocum and Usman Anwar and Anand Siththaranjan and Max Nadeau and Eric J. Michaud and Jacob Pfau and Dmitrii Krasheninnikov and Xin Chen and Lauro Langosco and Peter Hase and Erdem B\u0131y\u0131k and Anca Dragan and David Krueger and Dorsa Sadigh and Dylan Hadfield-Menell",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yuanself",
        "author": "Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Li, Xian and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason E",
        "title": "Self-Rewarding Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "bai2022training",
        "author": "Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "huang2022largelanguagemodelsselfimprove",
        "author": "Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han",
        "title": "Large Language Models Can Self-Improve"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "burns2023weak",
        "author": "Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others",
        "title": "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "anonymous2024metarewarding",
        "author": "Anonymous",
        "title": "Meta-Rewarding Language Models: Self-Improving Alignment with {LLM}-as-a-Meta-Judge"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2024creamconsistencyregularizedselfrewarding",
        "author": "Zhaoyang Wang and Weilei He and Zhiyuan Liang and Xuchao Zhang and Chetan Bansal and Ying Wei and Weitong Zhang and Huaxiu Yao",
        "title": "CREAM: Consistency Regularized Self-Rewarding Language Models"
      }
    ]
  }
]