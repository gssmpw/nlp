\section{Related Work}
\textbf{Alignment of Large Language Model} is one key factor behind the LLMs' success \cite{bommasani2021opportunities,wang2023aligninglargelanguagemodels}. 
By aligning model behavior with human preferences, an LLM can follow human instructions and generate helpful and harmless responses \cite{bai2022traininghelpfulharmlessassistant}.  
Alignment performance relies on preference data and preference learning algorithms. 
A classic preference learning algorithm is Reinforcement Learning from Human Feedback (RLHF) \cite{bai2022training,bai2022constitutional,ouyang2022training}, which typically uses preference data to train an external reward model to score LLM's response, and then uses Proximal Policy Optimization (PPO) \cite{schulman2017proximalpolicyoptimizationalgorithms} to optimize the LLM, making LLMs' responses maximize the reward modelâ€™s score. 
Another widely used preference learning algorithm is Direct Preference Optimization (DPO) \cite{rafailov2024direct}, which modifies the optimization objective of RLHF, allowing the model to be trained directly on preference data without the need for a reward model during the training process. 
Both RLHF and DPO relies on humans or an additional reward model to annotate preference data. 
High-quality preference data can enhance the effectiveness of preference optimization, but collecting such data is often time-consuming and labor-intensive. 
Therefore, improving the quality of preference data and reducing the cost of collecting data are two promising directions for LLM alignment \cite{kaufmann2024surveyreinforcementlearninghuman,casper2023openproblemsfundamentallimitations}.


\textbf{Self-Rewarding Language Model} (SRLM) \cite{yuanself} has proposed that an LLM can generate preference data by itself. Training model on self-generated data can further enhance its alignment performance. 
SRLM provides a solution to avoid time-consuming and labor-intensive human preference annotation and reward model training \cite{bai2022training}, enabling rapid adaptation to new domains in a self-improvement manner \cite{huang2022largelanguagemodelsselfimprove}. 
Additionally, SRLM can help achieve superhuman agents. When an LLM exceeds human in a specific domain, it can autonomously provide superhuman feedback to ensure an adequate training signal \cite{burns2023weak}. 
To improve SRLM, \citet{anonymous2024metarewarding} 
suggests using the same LLM as a meta-judge to evaluate its generated LLM-as-a-Judge, thereby improving the ability of generative reward models. \citet{wang2024creamconsistencyregularizedselfrewarding} introduces the use of regularization to enhance the consistency of DPO rewards across different iterations, thus providing more robust preference data. 
While all these methods aim to improve the quality of preference data in SRLMs, this paper explores different strategies: enhancing consistency across different internal reward models within the same iteration to improve the reliability of both the internal reward models and the preference data.