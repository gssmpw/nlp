\section{Related works}
In the realm of vision-based tactile images, the application of computer vision models and algorithms has become common practice due to the visual nature of the data these sensors capture~\citep{dong2021tactile,li2019connecting,calandra2018more}. 
Researchers have adapted mature representation learning methods from the vision community to tactile images. 
One popular approach is contrastive learning.
Both tactile and visual-tactile representations have been explored for specific tasks~\citep{yuan2017connecting,yang2022touch,tian2020contrastive,kerr2022self,guzey2023dexterity,grill2020bootstrap,zambelli2021learning}. 
Another technical approach is based on auto-encoding representation. 
\citet{cao2023learn} and \citet{xu2024unit} leveraged Masked Auto-Encoder (MAE) to learn tactile representations.

However, many works that directly apply existing representation learning methods to the tactile modality ignore the significant domain gap seen between sensors. 
Representations trained on one sensor may work well on the exact same sensor or the same type, but the domain gap between different sensors makes models based on such representation fail to generalize to other sensors. 
To address this, \citet{zhao2024transferable} trained individual encoder-decoder pairs for different sensor-task combinations, focusing on learning the shared features and improving fine-tuned performance on new sensor-task combinations. 
\citet{yang2024binding} sought to address this by proposing a general-purpose multimodal representation for vision-based tactile sensors.
By integrating multiple tactile datasets into a large language model (LLM) framework and encoding sensor types as tokens, they try to inform the LLM explicitly of the domain gap among different types of sensors.
\citet{higuera2024sparshselfsupervisedtouchrepresentations} introduced a family of self-supervised models for vision-based tactile sensing that learns general-purpose representations by leveraging masking and self-distillation pretraining across multiple sensors. They aim to improve tactile perception and downstream task performance under a limited labeled data budget.
However, these methods often depend on large datasets and treat sensor types as fixed categories, failing to account for variations within the same sensor type and lacking the flexibility to generalize zero-shot to unseen sensors.

Our framework introduces a novel combination of geometry-preserving supervision, supervised contrastive learning, and sensor-specific calibration images. 
The calibration images capture sensor-specific domain features, such as optical properties unique to each sensor, which help the encoder adapt to these characteristics. 
By accounting for subtle variations both within the same sensor type and across different types, our method enhances zero-shot generalization across tactile tasks and demonstrates strong transferability to new sensors.

%