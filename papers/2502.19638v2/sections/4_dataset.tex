\section{Datasets}

We collect three datasets for model training and evaluation. 
The first dataset contains purely synthetic data and is used to train the encoder for \modelname. 
The other two datasets are collected across 7 real sensors on two specific tactile applications: object classification and contact localization. These two datasets are used to evaluate the zero-shot transferability of \modelname for downstream tasks.

\subsection{Simulated tactile dataset}
\label{sec:simulated_daaset}

We construct a large-scale synthetic dataset that spans a wide range of tactile sensor configurations, providing tactile signals of contact geometries along with their corresponding normal maps. 
The sensor's configuration is defined by its optical design, such as the location and optical properties of the lights, cameras, and reflective surfaces. 
These attributes quantify the major variances seen in real tactile sensors.
The core idea is to train SITR with a large distribution of simulated sensors so SITR can generalize to, and be aligned across, real-world sensors.
This dataset is designed to be sensor-aligned, where each contact geometry is sampled across all sensor configurations for SCL. 

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-0.4cm}
    \centering
    \includegraphics[width=\linewidth]{fig/BlenderScreenshotPlacebo.png}
    \caption{Demonstration of our physics-based rendering (PBR) model to simulate GelSight sensors. We parameterize the sensor's optical design in the environment. 
    }
    \label{fig:dataset_blender_environment}
\end{wrapfigure}


We use Physics-based Rendering (PBR) ~\citep{pharr2023physically} to simulate GelSight sensors~\citep{agarwal2021simulation} and implement the algorithm in Blender. 
PBR simulates the camera images by tracing the path of light rays traveling in the scene and how they interact with optical components. 
Therefore, the technology models the physical behavior of the optical system and can simulate a GelSight sensor's reading with parameterized optical settings. 
We design the simulator platform to customize the sensors by modulating the locations and characteristics of each optical component. 
Fig. \ref{fig:dataset_blender_environment} illustrates an example setup where three light sources surround a deformable surface. 
A camera positioned above captures the change of color on the surface caused by object contact.
 

\textbf{Sensor variation}:
To mimic the variance across real-world tactile sensors, we identify key parameters that highlight the differences between real-world tactile sensors. Specifically, we look at the differences among GelSight Mini~\citep{gsmini_website}, GelSight Hex~\citep{yuan2017gelsight}, GelSight Wedge~\citep{wang2021gelsight}, GelSlim 3.0~\citep{taylor2022gelslim}, GelSight Finray~\citep{liu2022gelsight}, and DIGIT~\citep{lambeta2020digit}.
This includes light properties (shape, orientation, angle, color), gel properties (stiffness, specularity), and camera properties (FOV, sensing area).
In total, we generate 100 unique simulated sensor configurations. 
More details on the sensor configurations and examples of rendered images for the same contact object can be found in \Secref{sec:appendix_sensor}. 
For each sensor, we also collect a set of calibration images as described in Section \ref{subsec:method_domain_knowledge}. We introduce random variability in the calibration positions to make the training more robust to the real-world setting.

\textbf{Object diversity}: 
To enable \modelname~to generalize across diverse contact geometries, we utilize 50 high-resolution 3D meshes of common household objects. 
These meshes include tools, kitchenware, toys, and clothing items, which are often used in robotics research. 
During simulation, the objects are randomly scaled, rotated, and placed at varying locations on the gel pad. 
For each contact geometry, we render tactile images using all sensor configurations and pair them with ground-truth surface normal maps. 
We generate a total of 10K contact configurations through this process.

With 10K unique contact configurations across 100 different sensor configurations, we pre-train \modelname~encoder solely using our 1M synthetic dataset. 

\subsection{Real-world tactile dataset}
\label{sec:real_world_dataset}

We collect real-world datasets for training and evaluating downstream tasks across different baselines.
Compared to the synthetic dataset we used for \modelname~encoder pre-training phase, we keep \modelname~encoder frozen and train only the corresponding task-specific decoder head for downstream tasks.
We use seven different sensors for our datasets: four GelSight Minis~\citep{gsmini_website} with varying sensor bodies and in-house gel pad modifications, GelSight Hex~\citep{yuan2017gelsight}, GelSight Wedge~\citep{wang2021gelsight}, and DIGIT~\citep{lambeta2020digit}.

For the classification task, we select 16 objects and press them against the sensor in various poses and depths, recording 1K tactile images for each object. 
We repeat this process for all 16 objects across the 7 sensors, resulting in a dataset with 112K tactile images, with 16K samples per sensor.
\Secref{sec:appendix_samples}
shows that tactile signals vary even when using the same object across different sensor configurations. 

For the pose estimation task, we modify an Ender-3 Pro 3D printer by replacing its extruder with 3D-printed indenters and mount the tactile sensors onto the print bed. 
This setup provides the accurate ground truth pose of each contact, including metric $x$, $y$, and $z$ values. 
During the data collection process, we press indentors at various locations and depths on the sensor surface. 
We collected 1K samples per indentor for 6 different indentors across 4 sensors.
This results in a dataset of 24K tactile images with precise pose labels, with 6K samples per sensor. 
More details can be found in \Secref{sec:appendix_poe_samples}.
