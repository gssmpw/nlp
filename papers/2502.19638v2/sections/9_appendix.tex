\clearpage
\section{Appendix}
\label{sec:appendix}

\subsection{Implementation Details}
\label{sec:appendix_implementation_details}

This section outlines the detailed implementation steps, including pre-processing, architecture, training settings, and decoder choices for all models.

\subsubsection{Pre-processing} 
For SITR, we apply the following pre-processing steps across real and simulated sensors:

\begin{enumerate}
    \item All input images are resized to $224 \times 224$. For the GelSight Wedge sensor, an affine transformation is applied to correct distortions in the tactile images.
    \item Batched data augmentations are applied during training to both the tactile input and calibration images, including color jitter and Gaussian blur.
    \item Background subtraction is performed on each image to isolate the tactile signal. All images are then normalized based on the mean and standard deviation calculated from the simulated dataset.
\end{enumerate}

\subsubsection{Architecture} 

\textbf{Encoders:} Table \ref{tab:model_parameters_comparison} shows the number of parameters used in each encoder.

\begin{table}[htbp]
\centering
\begin{tabular}{lc}
\toprule
Model & Number of Parameters \\
\midrule
ViT-Base & 86M \\
ViT-Large & 307M \\
T3-Medium & 173M \\
UniT & 25M \\
\midrule
SITR (Ours) & 96M \\
\bottomrule
\end{tabular}
\caption{Comparison of model parameters.}
\label{tab:model_parameters_comparison}
\end{table}

Our SITR model is derived from the ViT-Base architecture. The key modification is in the patch embedding, where we tokenize the tactile input and calibration images separately and add a positional embedding before passing them through the transformer. 

\paragraph{SITR Training Decoders:}

During the pre-training phase for SITR, we use two decoders:

\begin{itemize}
    \item \textbf{Normal Map Reconstruction Decoder:} We apply a simple linear projection to the output tactile image tokens from SITR. We reshape and unpatchify the output to create a feature image map. We supervise with MSE loss $\lambda_{\mathrm{normal}}$ against the ground truth normal map.
    \item \textbf{Class Token Decoder:} The class token is passed through a linear projection to a 128-dimensional embedding. We then supervise this embedding with SCL loss $\lambda_{\mathrm{SCL}}$.
    \item \textbf{Loss Terms} The total loss during training is a weighted sum of these two loss terms: 
    $\mathcal{L} = \lambda_{\mathrm{normal}} \cdot \mathcal{L}_{\mathrm{normal}} + \lambda_{\mathrm{SCL}} \cdot \mathcal{L}_{\mathrm{SCL}}$
We set both loss weighting hyperparameters $\lambda_{\mathrm{normal}}$ and $\lambda_{\mathrm{SCL}}$ to 1. 
\end{itemize}

\paragraph{Downstream Task Decoders:}

We try several decoders for downstream tasks for each baseline and task and report the best-performing ones here. 

\begin{enumerate}
    \item \textbf{Classification Decoders} We use Cross Entropy Loss for this task.
    \begin{itemize}
        \item \textbf{SITR:} We unpatchify the output tokens $x_i$ to a feature map and pass it through a ResNet-18 network. 
        The resulting feature vector is concatenated with the class token $z_i$. We then apply a 3-layer MLP decoder with dimensions [256, 128, 16]. The SITR encoder is frozen during this process.
        
        \item \textbf{ViT:} For all ViT encoders, we linearly project the class token to an output of 16 dimensions. We also find that unfreezing the ViT pre-trained weights during training improves performance.
        
        \item \textbf{T3:} We unpatchify the output tokens to a feature map and pass it through a ResNet-18 network with an output dimension of 16. The T3 encoder is frozen for this process.
        
        \item \textbf{UniT:} We directly apply their proposed pooling and MLP decoder blocks to an output dimension of 16. We find that unfreezing the UniT encoder provides better results.
    \end{itemize}

    \item \textbf{Pose Estimation Decoders} We use MSE loss for this task.
    \begin{itemize}
        \item \textbf{SITR:} We pass 2 tactile images $x_1$ and $x_2$ into the network separately.
        We unpatchify the output tokens from  $x_1$ and $x_2$ and concatenate their feature maps. We pass the concatenated feature maps into a modified ResNet-18 with a 6-channel input.
        We then linearly project the resulting feature vector to an output dimension of 3. 
        The SITR encoder is frozen during this process.
        
        \item \textbf{ViT:} For all ViT encoders, we pass 2 tactile images $x_1$ and $x_2$ into a modified ViT network allowing 6 channel input. We then linearly project the resulting class token to an output dimension of 3. 
        We unfreeze the ViTs when training.
        
        \item \textbf{T3:} We follow the same procedure described in SITR's pose estimation decoder. 2 tactile images $x_1$ and $x_2$ are passed into the network separately.  We unpatchify the output tokens from  $x_1$ and $x_2$ and concatenate their feature maps. We pass this feature into a modified ResNet-18 and linearly project the resulting feature vector to an output dimension of 3. We keep the T3 encoder frozen during this training process.
    \end{itemize}
\end{enumerate}

\clearpage
\subsection{Simulated Dataset}
\label{sec:appendix_sensor}
\begin{table}[htbp]
\centering
\begin{tabular}{>{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1cm} >{\centering\arraybackslash}m{1cm} >{\centering\arraybackslash}m{1.8cm} >{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{1.7cm} >{\centering\arraybackslash}m{1.7cm} }
\toprule
Parameter & Lower bound & Upper bound & Lower bound vis. & Upper bound vis. & Lower bound env. & Upper bound env.\\
\midrule

Light shape & point & area & {\includegraphics[width=0.15\textwidth]{fig/simvar/11.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/10.png}}   & {\includegraphics[width=0.15\textwidth]{fig/simvar/sim00.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/sim01.png}}  \\ \midrule

Light orientation & sides & corners & {\includegraphics   [width=0.15\textwidth]{fig/simvar/60.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/61.png}} & {\includegraphics[width=0.15\textwidth]{fig/simvar/sim01.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/sim11.png}}     \\ \midrule

Light angle & 5$^\circ$  & 30$^\circ$ & {\includegraphics[width=0.15\textwidth]{fig/simvar/40.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/41.png}}& {\includegraphics[width=0.15\textwidth]{fig/simvar/sim20.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/sim21.png}}  \\ \midrule

Light color & rand & rand & {\includegraphics[width=0.15\textwidth]{fig/simvar/20.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/21.png}} & {\includegraphics[width=0.15\textwidth]{fig/simvar/sim30.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/sim31.png}} \\ \midrule

 Gel stiffness & low  & high & {\includegraphics[width=0.15\textwidth]{fig/simvar/00.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/01.png}} & {\includegraphics[width=0.14\textwidth]{fig/simvar/sim40.png}}  & {\includegraphics[width=0.14\textwidth]{fig/simvar/sim41.png}} \\ \midrule

Gel specularity & low  & high & {\includegraphics[width=0.15\textwidth]{fig/simvar/51.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/50.png}} & {\includegraphics[width=0.11\textwidth]{fig/simvar/sim50.png}}  & {\includegraphics[width=0.11\textwidth]{fig/simvar/sim51.png}} \\ \midrule

Camera FOV & 40$^\circ$  & 90$^\circ$ & {\includegraphics[width=0.15\textwidth]{fig/simvar/31.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/30.png}}& {\includegraphics[width=0.15\textwidth]{fig/simvar/sim60.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/sim61.png}}  \\ \midrule

Sensing area & 4cm$^2$ & 16cm$^2$ & {\includegraphics[width=0.15\textwidth]{fig/simvar/81.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/80.png}}& {\includegraphics[width=0.15\textwidth]{fig/simvar/sim70.png}}  & {\includegraphics[width=0.15\textwidth]{fig/simvar/sim71.png}}  \\


\bottomrule
\end{tabular}
\caption{Visualization of parameter bounds in the simulated dataset.}
\label{table:appendix_vis_sim_dataset}
\end{table}

\clearpage
As discussed in \Secref{sec:simulated_daaset}, we construct a large-scale simulated dataset that includes a wide range of tactile sensor configurations.
Figure \ref{fig:appendix_sim_dataset_sample1} illustrates a sample of tactile images from different simulated sensor configurations and contact geometry within the dataset.
The samples can be retrieved from our dataset with the sensor IDs and contact IDs provided.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig/sim_dataset.png}
    \caption{Samples from the simulation dataset.}
    \label{fig:appendix_sim_dataset_sample1}
\end{figure}

\clearpage
\subsection{Classification Dataset Samples}
Figure \ref{fig:appendix_real_dataset_sample1} and \ref{fig:appendix_real_dataset_sample2} show the real-world classification dataset that we used to generate the result discussed in \Secref{subsec:classification}.
Each row corresponds to a different object class, and each column represents a different sensor.
\label{sec:appendix_samples}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig/real_dataset_small.png}
    \caption{Samples from the classification dataset. (Part 1)}
    \label{fig:appendix_real_dataset_sample1}
\end{figure}
\clearpage
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig/real_dataset2_small.png}
    \caption{Samples from the classification dataset. (Part 2)}
    \label{fig:appendix_real_dataset_sample2}
\end{figure}

\clearpage
\subsection{Pose estimation dataset samples}
Figure \ref{fig:appendix_pose_dataset_sample} shows the real-world classification dataset that we used to generate the result discussed in \Secref{subsec:pose_estimation}.
Each row corresponds to a different object class, and each column represents a different sensor.
\label{sec:appendix_poe_samples}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{fig/pose_dataset.png}
    \caption{Samples from pose estimation dataset.}
    \label{fig:appendix_pose_dataset_sample}
\end{figure}


Figure \ref{fig:appendix_pose_dataset_ender3} shows the modified Ender-3 3D printer.
We mount indentors and collect the pose estimation dataset for multiple sensors.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{fig/Ender3.png}
    \caption{Modified Ender-3 Pro 3D printer }
    \label{fig:appendix_pose_dataset_ender3}
\end{figure}

\clearpage

\subsection{Transferability Details}
In this section, we present the full results of the sensor transfer downstream experiments. Details of experiments can be found in \Secref{subsec:classification} and \ref{subsec:pose_estimation}. Figure \ref{fig:transferability_cls1} and \ref{fig:transferability_cls2} show the classification results and Figure \ref{fig:transferability_pose} shows pose estimation results. 
\subsubsection{Classification }
\label{sec:transfer_details}
\begin{figure}[htbp]
\begin{center}
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/vit_scratch_intra.png}
        \subcaption{ViT-Base Scratch Intra-sensor}
        \label{fig:class_vit_b_scratch_intra}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/vit_scratch_inter.png}
        \subcaption{ViT-Base Scratched Inter-sensor}
        \label{fig:class_vit_b_scratch_inter}
    \end{minipage}
    \vfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/vitb_pt_intra.png}
        \subcaption{ViT-Base Pre-trained Intra-sensor}
        \label{fig:class_vit_b_pt_intra}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/vitb_pt_inter.png}
        \subcaption{ViT-Base Pre-trained Inter-sensor}
        \label{fig:class_vit_b_pt_inter}
    \end{minipage}
    \vfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/vitl_pt_intra.png}
        \subcaption{ViT-Large Pre-trained Intra-sensor}
        \label{fig:class_vit_l_pt_intra}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/vitl_pt_inter.png}
        \subcaption{ViT-Large Pre-trained Inter-sensor}
        \label{fig:class_vit_l_pt_inter}
    \end{minipage}
    
\end{center}
\caption{Transferability on classification tasks. (Part 1) }
\label{fig:transferability_cls1}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/T3_intra.png}
        \subcaption{T3 Intra-sensor}
        \label{fig:class_T3_intra}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/T3_inter.png}
        \subcaption{T3 Inter-sensor}
        \label{fig:class_T3_inter}
    \end{minipage}
    \vfill
    \begin{minipage}[b]{1.0\linewidth}
        \centering
        \includegraphics[width=0.45\linewidth]{fig/heatmap/UniT_unfreeze.png}
        \subcaption{UniT Intra-sensor}
        \label{fig:class_UniT_intra}
    \end{minipage}
    
    \vfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/sitr_intra.png}
        \subcaption{SITR Intra-sensor}
        \label{fig:class_sitr_intra}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/sitr_inter.png}
        \subcaption{SITR Inter-sensor}
        \label{fig:class_sitr_inter}
    \end{minipage}
    
\end{center}
\caption{Transferability on classification tasks. (Part 2) }
\label{fig:transferability_cls2}
\end{figure}
\clearpage
\subsubsection{Pose Estimation}
\begin{figure}[htbp]
\begin{center}
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/vitb_scratch_pose.png}
        \subcaption{ViT-Base Scratch Inter-sensor}
        \label{fig:vit_scratch_pose}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/vitb_pt_pose.png}
        \subcaption{ViT-Base Pre-trained Inter-sensor}
        \label{fig:vitb_pt_pose}
    \end{minipage}
    \vfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/vitl_pt_pose.png}
        \subcaption{ViT-Large Pre-trained Inter-sensor}
        \label{fig:vitl_pt_pose}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/heatmap/t3_pose.png}
        \subcaption{T3 Inter-sensor}
        \label{fig:t3_pose}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{\linewidth}
        \centering
        \includegraphics[width=0.45\linewidth]{fig/heatmap/sitr_pose.png}
        \subcaption{SITR Inter-sensor}
        \label{fig:sitr_pose}
    \end{minipage}
    
\end{center}
\caption{Transferability on pose estimation tasks. }
\label{fig:transferability_pose}
\end{figure}


\clearpage
\subsection{Additional Ablations}
\label{sec:additional}

This section presents ablation experiments to evaluate the impact of loss terms, alternative supervision signals, and dataset size on SITRâ€™s performance.

\subsubsection{Contribution of loss terms}

We conduct an ablation study to evaluate the contributions of the normal map loss and SCL loss to SITR's performance. As shown in Table~\ref{table:ablation_loss_term}, either loss term independently serves as an effective supervision signal. However, their combination yields the strongest results. This evaluation is conducted on the dataset visualized in Figure~\ref{fig:discussion_tsne}, further highlighting how these two loss terms synergize to improve representation learning. 

\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\toprule
Method & Classification (\%) \\
\midrule
Normal loss only & $84.21$ {\scriptsize $\pm$ $14.01$} \\
SCL loss only & $78.86$ {\scriptsize $\pm$ $18.72$} \\
Normal + SCL losses & $\boldsymbol{91.43}$ {\scriptsize $\pm$ $9.88$} \\
\bottomrule
\end{tabular}
\caption{Ablation study showing the impact of different loss terms on classification accuracy transferability.}
\label{table:ablation_loss_term}
\end{table}

\subsubsection{Choice of supervision signal}
There are alternative supervisions to our normal map, such as using MAE or VQGAN to reconstruct tactile images, as employed in T3 and UniT. To evaluate the effectiveness of SITR, we adapt these supervisions to train representations using our simulated dataset. 
We evaluate the models' transferability as described in \Secref{subsec:classification} and \Secref{subsec:pose_estimation}. SITR consistently outperforms MAE and VQGAN, highlighting the benefits of SITR's architecture and training pipeline.

\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\toprule
 & \multicolumn{2}{c}{Classification (\%)} & Pose estimation (mm)\\
 Method & Intra-sensor set $\uparrow$ & Inter-sensor set $\uparrow$ & Inter-sensor set $\downarrow$\\
\midrule
MAE & $45.81$ {\scriptsize $\pm$ $21.44$} & $26.46$ {\scriptsize $\pm$ $19.54$} & $1.13$ {\scriptsize $\pm$ $0.19$}\\
VQGAN & $59.41$ {\scriptsize $\pm$ $19.50$} & $31.02$ {\scriptsize $\pm$ $22.01$} & $1.18$ {\scriptsize $\pm$ $0.14$}\\
 \midrule
SITR (Ours) & $\boldsymbol{90.23}$ {\scriptsize $\pm$ $8.16$} & $\boldsymbol{81.94}$ {\scriptsize $\pm$ $12.92$} & $\boldsymbol{0.80}$ {\scriptsize $\pm$ $0.21$} \\
\bottomrule
\end{tabular}
\caption{Comparison of MAE, VQGAN, and SITR performance on intra-sensor and inter-sensor classification tasks (\%) and inter-sensor pose estimation (mm)}
\label{table:ablation_method}
\end{table}

\subsubsection{Effect of simulation dataset size}

We evaluate how the size of the simulation dataset and the variety of sensor configurations impact classification transfer performance on inter-set classification. Table~\ref{table:ablation_dataset} shows that increasing the number of samples per sensor and the number of sensor variations lead to increases in performance. This demonstrates the benefit of a diverse and large-scale training dataset. 

\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\toprule
 \multirow{2}{15mm}{Sensor Variations}& \multicolumn{3}{c}{Samples per sensor} \\
\cmidrule(lr){2-4}
 & 1K & 5K & 10K \\
\midrule
10 & $45.82$ {\scriptsize $\pm$ $21.12$} & $57.00$ {\scriptsize $\pm$ $21.55$} & $61.44$ {\scriptsize $\pm$ $22.81$} \\
50 & $55.86$ {\scriptsize $\pm$ $25.04$} & $68.55$ {\scriptsize $\pm$ $11.96$} & $76.78$ {\scriptsize $\pm$ $13.91$} \\
100 & ${62.85}$ {\scriptsize $\pm$ $16.45$} & ${73.71}$ {\scriptsize $\pm$ $14.27$} & $\boldsymbol{81.94}$ {\scriptsize $\pm$ $12.92$} \\
\bottomrule
\end{tabular}
\caption{Transfer classification accuracy (\%) on the inter-set dataset across different sensor variations and samples per sensor. }
\label{table:ablation_dataset}
\end{table}

