\section{Introduction}

Tactile sensing is a crucial modality for intelligent systems to perceive the physical world. 
Among the various tactile technologies, the GelSight sensor~\citep{yuan2017gelsight} and its variants~\citep{wang2021gelsight,gsmini_website,zhao2023gelsight} have recently emerged as one of the most influential tactile technologies, offering rich and detailed information on contact surfaces.
GelSight captures fine contact geometries through an optical system that transforms tactile data into visual images. 
This enables robots to precisely detect object shapes, recognize materials, and perform fine-grained manipulations with a high degree of accuracy
~\citep{yuan2018clothes,Dong19slip,hogan2020tactile,ota2023tactile,yang2023seq2seq,shirai2023tactile}. 

Despite their advantages, GelSight-like sensors, and vision-based tactile sensing in a more general sense, still face a key challenge: sensor variance.
Differences in the optical design or manufacturing process can result in significant discrepancies in sensor output. 
Consequently, machine learning models trained on data from one sensor often fail to generalize to other sensors. 
This challenge is further compounded by the high cost and effort of collecting tactile datasets, creating a major barrier to sensor transferability in tactile perception.

In this paper, we address the challenge of data transferability between GelSight sensors by tackling sensor variance arising from optical design and manufacturing differences. 
The key issue lies in enabling generalization to new sensors as the domain gap between individual sensors is substantial and unpredictable. 
Previous methods, such as \citet{yuan2018clothes,calandra2018more}, attempted to improve generalization by using multiple GelSight sensors to gather diverse tactile datasets, but this approach offered limited gains. 
More recently, {\tt T3}~\citep{zhao2024transferable} sought to improve transferability by pre-training a transformer model across multiple sensors and tasks. 
However, their reliance on category-specific encoders limited the ability of their model to generalize to unseen sensors.

In contrast, we propose that achieving sensor transferability requires learning effective sensor-invariant representations by ensuring the model is trained on sufficiently diverse sensor variations.
We introduce a novel framework for generating sensor-invariant feature representations from high-resolution tactile readings, enabling zero-shot transfer to unseen sensors across multiple downstream tasks. 
Our framework incorporates three core innovations: 

\begin{enumerate}
    \item We utilize a small set of easy-to-acquire calibration images to characterize individual sensors. We then use a transformer model as the encoder to effectively combine the calibration images with the tactile reading.
    \item We employ supervised contrastive learning (SCL)~\citep{khosla2020supervised} to emphasize the geometric aspects of tactile data, encouraging the clustering of similar contact geometries across multiple sensors. This training is further supervised by measuring geometric accuracy.
    \item We develop a large-scale synthetic dataset using a physics-based simulator that models sensor optical systems, capturing variations in both sensor characteristics and contact geometries. This dataset, consisting of 1M examples across 100 sensor configurations, provides the diversity necessary for robust model training using precise ground truth of the contact geometries.
\end{enumerate}

Our motivation comes from the belief that contact geometry is one of the most critical features for most tactile-driven tasks, including shape recognition, texture classification, and contact localization. 
By focusing on geometric accuracy and using calibration to remove sensor-specific variations, we ensure the development of robust, sensor-invariant representations. 
Leveraging physics-based simulations allows us to efficiently generate diverse tactile datasets, reducing the time and cost of real-world data collection.

We evaluate the generalizability of our method across various downstream tasks using multiple real-world GelSight sensors. 
Our results demonstrate that models trained on one sensor can be seamlessly transferred to others in a zero-shot manner, significantly outperforming existing approaches. 
This framework paves the way for easier transferability of machine learning models and datasets between different sensors, thereby enhancing the future development of the tactile-sensing community.