\section{Experiments}
\label{sec:experiments}

In this section, we show several experiments to evaluate the zero-shot transferability of our model to different real sensors. We evaluate model performance on three downstream tasks: shape reconstruction, object classification, and contact localization. 

\subsection{Experiment setting}

We conduct experiments with multiple real tactile sensors that can be divided into two groups: 

\begin{itemize}
    \item Intra-sensor set: GelSight Mini 1 to 4 of different gel pads. These sensors have the same optical design, i.e., placement of camera and light sources, but differ in brightness and color of tactile signals due to manufacturing differences and choice of coating materials. 
    \item Inter-sensor set: GelSight Mini 1, GelSight Wedge, GelSight Hex, and DIGIT. These sensors are designed with very different optical structures and, therefore, generate tactile signals that are significantly different from each other. 
\end{itemize}

For each downstream task, we freeze the \modelname~encoder and only train the downstream task-specific decoder on a single sensor.  We evaluate this model using the rest of the sensors in the set. Formally, let \( S = \{S_1, S_2, \dots, S_n\} \) be the set of sensors. Let \( A_{ij} \) represent the performance (e.g., classification accuracy or pose estimation error) when trained on \( S_i \) and evaluated on \( S_j \). The transfer performance across all sensors in the set is computed as

\[
\text{Transfer Performance} = \frac{1}{n(n-1)} \sum_{i=1}^{n} \sum_{\substack{j=1 \\ j \neq i}}^{n} A_{ij}
\]

We also compute the score when training and testing on the same sensor $i=j$ acting as an upper bound of the performance: $\text{No Transfer Performance} = \frac{1}{n} \sum_{i=1}^{n} A_{ii}$.

\textbf{Baseline}: 
We compare our \modelname~with ViTs that are either trained from scratch or fine-tuned from ImageNet weights to show the effectiveness of our method. 
As there is no previous work that directly focuses on transferable tactile representations, we also compare against {\tt T3}~\citep{zhao2024transferable} and UniT~\citep{xu2024unit}.
{\tt T3} focuses on improving few-shot fine-tuning results across different sensors and has the potential for zero-shot transfer.
UniT learns dense representations for various downstream tasks and shows preliminary results on transferring among GelSight Mini sensors.
We evaluate their available models for our experiments to compare the transferability of these representations. 
Additionally, we ablate the method used in {\tt T3} (MAE) and UniT (VQGAN) when trained on our synthetic dataset to test the effectiveness of our architecture in \Secref{sec:additional}.
We describe model configurations and decoders for each task in~\Secref{sec:appendix_implementation_details}.

\subsection{Zero-shot Transfer for Shape Reconstruction}


\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig/reconstruction.png}
\caption{
Reconstruction examples for various sensors. The top row shows input tactile images, the middle row presents 3D reconstructions, and the bottom row shows the contact objects. Simulated sensors (Simulation 1 and 2) are in the training set, while real sensors (GelSight Mini, DIGIT, Hex, Wedge) are not.
}
\label{fig:reconstruction_examples}
\end{center}
\vspace{-5pt}
\end{figure}

We qualitatively evaluate how \modelname~preserves geometry and texture information by reconstructing the contact height map. 
As shown in Fig. \ref{fig:reconstruction_examples}, we reconstruct normal maps for objects in our real-world classification dataset and integrate them to generate their corresponding height maps. 
These 3D reconstructions capture fine-grained geometry and texture details of the contact surface. Though, these reconstructions are naturally constrained by the resolution and sensitivity limitations of the sensors. 
Despite these limitations, the preservation of dense surface features demonstrates the robustness of \modelname~in accurately modeling the contact geometry across varying sensor inputs.

\subsection{Object Classification}
\label{subsec:classification}
We compare \modelname~with baselines using our real-world classification dataset from~\Secref{sec:real_world_dataset} and report top-1 accuracy. 
We freeze our \modelname~encoder and train the downstream classifier using cross-entropy loss. 
For {\tt T3}, we use their released GelSight Mini encoder weights for intra-sensor experiments. 
Since {\tt T3} does not provide encoder weights for GelSight Hex or DIGIT, we report inter-sensor results only for the GelSight Wedge and Mini. 
Note that {\tt T3}'s encoders were trained on marked sensors, so the results in our unmarked evaluations may not reflect their full potential. 
UniT demonstrates transferability only within GelSight Minis, so we exclude it from inter-sensor experiments. 
We train a UniT encoder on our unmarked real-world dataset and evaluate its intra-sensor transfer performance.

\begin{table}[htbp]
\centering
\begin{tabular}{lcccc}
\toprule
Method & Intra-sensor set $\uparrow$ & Inter-sensor set $\uparrow$ & Wedge-Mini $\uparrow$ & No transfer $\uparrow$\\
\midrule
ViT-Base Scratch& $36.90$ {\scriptsize $\pm$ $22.19$} & $24.02$ {\scriptsize $\pm$ $14.83$} & $52.56$ {\scriptsize $\pm$ $4.95$} & $96.76$ {\scriptsize $\pm$ $1.41$}\\
ViT-Base Pre-trained & $73.22$ {\scriptsize $\pm$ $22.42$} & $48.10$ {\scriptsize $\pm$ $22.82$} & $76.28$ {\scriptsize $\pm$ $17.06$} & $99.01$ {\scriptsize $\pm$ $1.14$}\\
ViT-Large Pre-trained & $78.38$ {\scriptsize $\pm$ $17.79$} & $54.34$ {\scriptsize $\pm$ $23.04$} & $79.04$ {\scriptsize $\pm$ $16.44$} & $99.44$ {\scriptsize $\pm$ $0.43$} \\
{\tt T3}-Medium & $38.66$ {\scriptsize $\pm$ $20.63$} & $-$ $-$ & $17.02$ {\scriptsize $\pm$ $8.55$} & $93.77$ {\scriptsize $\pm$ $2.87$} \\
UniT & $46.39$ {\scriptsize $\pm$ $23.30$} & $-$ $-$ & $-$ $-$ & $92.53$ {\scriptsize $\pm$ $4.19$}  \\ \midrule
SITR (Ours) & $\boldsymbol{90.23}$ {\scriptsize $\pm$ $8.16$} & $\boldsymbol{81.94}$ {\scriptsize $\pm$ $12.92$} & $\boldsymbol{90.80}$ {\scriptsize $\pm$ $2.85$} & $\boldsymbol{99.72}$ {\scriptsize $\pm$ $0.22$}  \\
\bottomrule
\end{tabular}
\caption{
Results of object classification accuracy on 16 classes for model transfer and no-transfer performance.
We report the mean and standard deviation of transfer accuracy percent among the sensor sets specified. 
Random guess classification accuracy corresponds to $6.67\%$.}
\label{table:result_classification}
\end{table}

As shown in Table \ref{table:result_classification}, \modelname~outperforms all baselines by a large margin regarding classification accuracy when transferred across sensors. 
Note that most models perform well under the no-transfer setting, but fail to generalize when tested on a different sensor.
This indicates that baselines can understand tactile features learned in the same domain, but SITR can capture meaningful features that are robust to changes in the sensor domain.  
We also find that the ViT pre-trained on ImageNet performs better than that trained from scratch, which indicates the effectiveness of pre-training on the image domain. 

Moving to feature-level analysis, Fig. \ref{fig:discussion_tsne} presents the t-SNE visualization of the \modelname~features for the contacts in our real-world classification dataset. 
The visualization illustrates that the use of contrastive learning significantly improves feature clustering, bringing together samples of the same object across different sensors. 
This indicates that \modelname~successfully aligns the tactile signals from different sensors, highlighting its capacity to eliminate sensor-variant features.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig/tsne_placebo.png}
    \caption{t-SNE visualization of the feature space. We qualitatively show that our contrastive loss term helps cluster those similar contacts from different sensors together.}
    \label{fig:discussion_tsne}
\end{figure}

However, the results also reveal some challenges. 
The features from the DIGIT sensor are somewhat more difficult to cluster with those from other sensors. 
This is better demonstrated in our detailed transfer results in Sec. \ref{sec:transfer_details}, where we see relatively worse classification transferability to and from the DIGIT sensor.
We attribute this to DIGITâ€™s distinct optical design, which differs from the GelSight designs in our simulation dataset. We believe the result will be improved in the future if we extend our synthetic dataset to cover optical designs similar to the DIGIT sensor. 

\subsection{Pose estimation}
\label{subsec:pose_estimation}
In this task, we try to estimate the 3-DoF ($x$, $y$, $z$) position change of the object in contact using an initial and final tactile image. 
We separately feed 2 tactile images of the same object into the frozen \modelname~encoder, concatenate their features, and train a decoder to learn the pose change with mean square error (MSE) loss. 
For baseline models, we use similar pipelines as detailed in  \Secref{sec:appendix_implementation_details}.
We evaluate this task on the inter-sensor set to see how each model handles differences in scale across sensors. 
Each sensor in this set has a different physical design, meaning they capture tactile signals at varying scales. Variations in object size may create significant challenges for zero-shot transfer tasks like pose estimation. 

\begin{table}[htbp]
\centering
\begin{tabular}{lccc}                                                                                                                       
\toprule
Method & Inter-sensor set $\downarrow$ & Wedge-Mini $\downarrow$ & No transfer $\downarrow$\\
\midrule
ViT-Base Scratch & $1.63$ {\scriptsize $\pm$ $0.20$} & $1.69$ {\scriptsize $\pm$ $0.13$} & $0.56$ {\scriptsize $\pm$ $0.02$}\\
ViT-Base Pre-trained & $1.58$ {\scriptsize $\pm$ $0.22$} & $1.65$ {\scriptsize $\pm$ $0.13$} & $\boldsymbol{0.49}$ {\scriptsize $\pm$ $0.01$}\\
ViT-Large Pre-trained & $1.49$ {\scriptsize $\pm$ $0.25$} & $1.45$ {\scriptsize $\pm$ $0.01$} & $0.50$ {\scriptsize $\pm$ $0.02$} \\
{\tt T3}-Medium & $-$ $-$ & $1.7$ {\scriptsize $\pm$ $0.07$} & $0.51$ {\scriptsize $\pm$ $0.02$}\\ \midrule
SITR (Ours) & $\boldsymbol{0.80}$ {\scriptsize $\pm$ $0.21$} & $\boldsymbol{0.62}$ {\scriptsize $\pm$ $0.11$} & $0.51$ {\scriptsize $\pm$ $0.01$}\\

\bottomrule
\end{tabular}
\caption{Results of pose estimation with 6 objects. 
We report the mean and standard deviation of transfer pose estimation root mean square error (RMSE) in $mm$ among the sensor sets specified. 
Random guess pose estimation RMSE corresponds to $2.52 mm$.}
\label{table:result_pose_estimation}
\end{table}

As shown in Table \ref{table:result_pose_estimation}, \modelname~demonstrates strong performance on the pose estimation when tested on a different sensor, reducing the RMSE by about 50\% compared to baselines. 
Remarkably, all models have similar RMSE errors for the no-transfer setting. This may suggest sub-millimeter inaccuracies present in our data collection process. Nonetheless, the no-transfer setting serves as the upper bound for our transfer setting. 
We also find that compared to ViT trained from scratch, the ViT pre-trained on ImageNet only marginally improves this task. This indicates that features learned from natural images may not transfer adequately to the tactile domain for accurate regression tasks like pose estimation.