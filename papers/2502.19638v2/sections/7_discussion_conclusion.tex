\vspace{-1.5mm}
\section{Discussion}
\vspace{-2mm}
Our qualitative and quantitative results indicate that \modelname~can generalize across sensors while preserving key geometric and texture features from tactile interactions.  
Our model has been largely trained and evaluated on optical tactile sensors with flat gel pads within the GelSight family.
Despite this, \modelname~can be adapted to a broader range of sensors.
Our PBR environment can be easily expanded to accommodate new parameters to explore distinct optical properties in flat tactile sensors. 
For more complex optical sensors like GelSight Svelte~\citep{zhao2023gelsight} or DIGIT 360~\citep{lambeta2024digitizingtouchartificialmultimodal}, adaptation remains feasible using an appropriate PBR model and contact surface mapping. 

One future direction of our framework is to generalize to traditional array-based tactile sensors.
The challenge lies in bridging the signal modalities of low-resolution normal force distribution to the high-resolution contact geometry from GelSight sensors.
One possible approach is to downsample vision-based tactile sensors' depth maps to approximate low-resolution tactile signals while establishing a meaningful invariant relationship between depth and force.
While this approach provides a step towards a unified tactile modality, its effectiveness in maintaining transferability requires further validation and exploration.

Another direction of future work is incorporating marker-based tactile information to \modelname. Many variations of GelSight are equipped with markers—distinct patterns embedded within the gel surface—that provide force and torque information. Currently, these markers are reconstructed using simple computer vision techniques to generate a marker motion field. We believe that unifying marker motion fields between sensors may be possible with adaptations to calibration in \modelname. This extension would broaden the applicability of our model to a wider range of tactile sensing tasks. 

\vspace{-1.5mm}
\section{Conclusion}
\vspace{-2mm}
In this paper, we introduced \modelname, a tactile representation that transfers across various vision-based tactile sensors in a zero-shot manner.
We build large-scale, sensor-aligned datasets using synthetic and real-world data, and propose a method to train \modelname~to capture dense, sensor-invariant features. 
Our experimental results demonstrate that \modelname~outperforms baseline models and other related tactile representations in different downstream tasks, showcasing robust transferability and effectiveness. \modelname represents a step towards a unified approach to tactile sensing, where models can generalize seamlessly across different sensor types, facilitating advancements in robotic and tactile research.
