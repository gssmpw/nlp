\section{Sensor-Invariant Representation Learning}
\begin{figure}[ht]
\begin{center}
\includegraphics[width=1\linewidth]{fig/SITR_architecture.png}
\caption{Our %\textbf{\modelname} 
sensor-invariant representation learning framework.
Each tactile image $x$ is paired with a set of calibration images $c$.
We patchify and linearly project $x$ and $c$ to tokens. Additionally, the $c$ patches are region-wise stacked before projection. We concatenate the input tokens with a class token $z$ and pass it through a transformer encoder. 
The class token $z$ is trained with SCL, while patch tokens are supervised by normal map reconstruction loss.
We highlight in grey the concatenation of the output class token and patch tokens as our Sensor-Invariant Tactile Representation (\modelname) for downstream tasks.}
\label{fig:architecture}
\end{center}
\end{figure}

In this section, we introduce our framework for training Sensor-Invariant Tactile Representation (SITR). 
We explain how calibration images capture sensor-specific information and use normal maps to preserve contact features. 
We introduce our implementation of SCL to align tactile features across sensor domains.
 We provide details on the role of calibration in \Secref{subsec:method_domain_knowledge}, followed by the network architecture and training process in \Secref{subsec:method_network_architecture}.

\subsection{Calibration Images for Tactile Sensors}
\label{subsec:method_domain_knowledge}

GelSight-like sensors map RGB values at each pixel to the local surface gradient, enabling the reconstruction of the contact surface. 
However, these sensors exhibit variations in physical properties that introduce sensor-specific artifacts in tactile images.
A widely adopted calibration technique involves pressing a ball of known radius onto the sensor pad at various points.
The tactile images captured during this process, combined with the known geometry of the ball, establish the correspondence between RGB changes and the local surface gradient at different locations. 
This method generates a sensor-specific look-up table. 
While traditional techniques assume pixel-invariant projection for simplicity,  neural networks can further learn precise and pixel-dependent projections. 

In the pre-training stage of \modelname~we adopt these steps to inform the model of sensor characteristics.
We include a cube in our calibration to inform SITR about how the gel deforms around edges and corners. 
Thus, we press two objects—a 4mm diameter ball and a cube corner—at nine locations each, roughly arranged in a $3\times 3$ grid pattern across the sensor surface as seen in Fig. \ref{fig:calibration}. 
These calibration images guide the encoder to identify and factor out sensor-specific features.

\begin{wrapfigure}[12]{r}{0.45\textwidth}
    \vspace{-0.4cm}
    \centering
    \includegraphics[width=\linewidth]{fig/calibration.png}
    \caption{Calibration images used in SITR, obtained by pressing two objects—a 4mm ball and a cube corner—at nine different locations each in a $3\times 3$ grid.
    }
    \label{fig:calibration}
\end{wrapfigure}

Formally, given a tactile image $ x_i \in \mathbb{R}^{H \times W \times C} $, we select $K$ calibration images $c_{i,k} \in \mathbb{R}^{H \times W \times C}$, where ($H,W$) is the resolution of the original image and $C$ is the number of channels. 
To efficiently encode multiple calibration images we reshape $c_{i,k}$ into the form $ c_{i} \in \mathbb{R}^{H \times W \times KC}$, in effect stacking the patches. 
We then linearly project $x_i$ and $c_i$ into a sequence of 
$N$ flattened 2D patches $x_{p} \in \mathbb{R}^{N \times P^2C}$ and $c_{p} \in \mathbb{R}^{N \times P^2C}$ similar to a standard ViT, where $N=HW/P^2$.
The resulting token sequences from $x_p$, $c_p$, and a class token $z_i$ are concatenated as input to the transformer encoder.

\subsection{Network architecture}
\label{subsec:method_network_architecture}

\textbf{Input}: We use the tactile image and a set of calibration images for the sensor as inputs for the network.
We subtract the sensor background from all the input images to get the pixel-wise color change as described in~\Secref{subsec:method_domain_knowledge}.
Following the process described in Vision Transformer (ViT)~\citep{ViT} and~\Secref{subsec:method_domain_knowledge}, we linearly project the input and calibration images to tokens. Note that calibration images need only be tokenized once per sensor.

\textbf{Encoder}: 
We modify a ViT to process both image and calibration tokens. 
Adapted from ViT, we add positional encoding to them based on their 2D coordinates and then pass them into the encoder.
We apply two supervision signals to train this encoder. 
One is the pixel-wise normal map reconstruction loss for the output patch tokens, and an additional contrastive loss for the class token.

\textbf{Normal map reconstruction}: 
During the \modelname~pre-training phase, we apply a lightweight decoder to reconstruct the contact surface as a normal map from the encoder output.
Normal maps record the orientation of each 3D point on the contact surface.
This feature is invariant to the variance across different sensors, contains rich geometry information for downstream tasks, and is viable for many GelSight-like vision-based tactile sensors.
Therefore, we apply a pixel-wise MSE loss $\mathcal{L}_{\text{normal}}$ between predicted normal map $\hat{n}$ and ground truth normal map $n$.

\textbf{Supervised contrastive learning}: 
SCL is an extension of contrastive learning that leverages label information to learn more effective representations.
Traditional contrastive learning aims to pull together similar samples and push apart dissimilar ones in the embedding space, typically relying on data augmentations to create positive pairs. 
SCL enhances this approach by utilizing class labels to define similarity, allowing for more semantically meaningful contrasts. 

We employ SCL to create sensor-invariant representations from our labeled simulated tactile dataset. 
We label positive pairs from tactile images with the same contact geometry across multiple sensors, while negative pairs are labeled from images of different contact geometries or locations.
In our batched implementation, we include two views for each sample: tactile images of the same contact captured by two different sensors. 
This approach allows us to learn discriminative features for downstream tasks while being robust to variations in sensor characteristics. 

Formally, given a batch of $N$ samples, let class token $z_i \in \mathbb{R}^d$ represent the encoded feature vector for sample $i$, where $d$ is the dimension of the embedding space. 
Let $y_i$ denote its corresponding contact label.
Let $A(i)$ denote the set of all samples in the batch except for sample $i$ itself. 
For each anchor sample $i$, we define the set of positive samples as
$P(i) = {p \in A(i) : y_p = y_i},$ with $|P(i)|$ being its cardinality. The supervised contrastive loss for a batch of samples is then formulated as
$$\mathcal{L}_{\text{SCL}} = \sum_{i \in I} \frac{-1}{|P(i)|} \sum_{p \in P(i)} \log \frac{\exp \left( \frac{z_i \cdot z_p}{\tau} \right)}{\sum_{a \in A(i)} \exp \left( \frac{z_i \cdot z_a}{\tau} \right)}$$

where $\tau$ is a temperature parameter that scales the similarity values to control the concentration of the distribution.

In summary, the total loss for \modelname~is defined as
$\mathcal{L} = \lambda_{\mathrm{normal}}\cdot\mathcal{L}_{\mathrm{normal}} + \lambda_{\mathrm{SCL}}\cdot\mathcal{L}_{\mathrm{SCL}}
$
where $\lambda_{\mathrm{normal}}$ and $\lambda_{\mathrm{SCL}}$ are loss weighting hyperparameters. 
Refer to \Secref{sec:appendix_implementation_details} for more implementation details.