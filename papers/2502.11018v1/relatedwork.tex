\section{Related Work}
Speculative decoding~\cite{sun2024spectr, miao2024specinfer, chen2023cascade, kim2024speculative, liu2023online} accelerates LLM inference by dividing each decoding step into a draft stage and a verification stage. Existing methods differ primarily in their draft model architectures or strategies, each addressing specific challenges in speculative decoding.  

Several approaches focus on retrieving candidate predictions from similar contexts, such as PLD~\cite{saxena2023prompt}, Lookahead~\cite{fu2024break}, and CLLMs~\cite{kou2024cllms}, which rely on prompt-based context retrieval. However, their dependence on contextual similarity limits generalization to tasks with insufficient or unavailable context. Tree-based verification mechanisms, such as Sequoia~\cite{chen2024sequoia} and SpecExec~\cite{svirschevski2024specexec}, improve verification via hierarchical structures but introduce computational overhead, making them less suitable for latency-critical tasks.
Other works leverage databases or prior outputs for drafting, such as REST~\cite{he2023rest} and Ouroboros~\cite{zhao2024ouroboros}, which reuse previous outputs to improve draft coherence. However, these methods are constrained by the quality and availability of external resources. Chimera~\cite{zeng2024chimera} and Glide~\cite{du2024glide} integrate the target model into the draft model to improve token quality, but this increases computational costs.  

Lightweight draft models have also been explored to improve efficiency. Medusa~\cite{cai2024medusa} employs MLPs for parallel candidate prediction, while Hydra~\cite{ankner2024hydra} and Recurrent Drafter~\cite{cheng2024recurrent} use RNN-based models for regressive generation. EAGLE~\cite{li2024eagle} and EAGLE2~\cite{li2024eagle2} introduces a transformer decoder for autoregression over feature sequences, balancing accuracy and complexity. FSPAD~\cite{gui2024boosting} constructs input sequences tailored for lightweight draft model predictions and introduces specialized training methods to improve draft quality. Additionally, methods like HASS~\cite{zhang2024learning} address feature misalignment during training and decoding but do not fully resolve token-level misalignment.  
In contrast, this work focuses on addressing token misalignment, a critical challenge in speculative decoding. We propose the GRIFFIN framework, which introduces a token-alignable training strategy and a token-alignable draft model. By tackling this issue, GRIFFIN improves both acceptance length and speedup ratio, offering a complementary perspective to existing methods.