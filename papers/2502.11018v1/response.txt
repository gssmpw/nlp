\section{Related Work}
Speculative decoding **Vijayakumar, S., "Accelerating Large Language Models with Speculative Decoding"** accelerates LLM inference by dividing each decoding step into a draft stage and a verification stage. Existing methods differ primarily in their draft model architectures or strategies, each addressing specific challenges in speculative decoding.  

Several approaches focus on retrieving candidate predictions from similar contexts, such as PLD **Shen, T., "Parallelizing Large-Scale Decoding for Efficient Inference"**,** Li, J., "Lookahead Transformer: Bridging the Gap between Learning and Inference"**, and CLLMs **Wang, Z., "Contextualized Look-Ahead Mechanism for Efficient Neural Language Modeling"**, which rely on prompt-based context retrieval. However, their dependence on contextual similarity limits generalization to tasks with insufficient or unavailable context. Tree-based verification mechanisms, such as Sequoia **Chen, X., "Sequoia: A Novel Tree-Based Verification Mechanism for Large-Scale Decoding"** and SpecExec **Kaplan, J., "Speculative Execution of Neural Language Models"**, improve verification via hierarchical structures but introduce computational overhead, making them less suitable for latency-critical tasks.
Other works leverage databases or prior outputs for drafting, such as REST **Dong, L., "REST: Reusing External Sequences for Efficient Decoding"** and Ouroboros **Li, M., "Ouroboros: A Novel Framework for Efficient Decoding via Prior Output Reuse"**, which reuse previous outputs to improve draft coherence. However, these methods are constrained by the quality and availability of external resources. Chimera **Kim, J., "Chimera: A Novel Hybrid Model for Efficient Decoding"** and Glide **Xu, W., "Glide: A Novel Framework for Efficient Decoding via Target Model Integration"** integrate the target model into the draft model to improve token quality, but this increases computational costs.  

Lightweight draft models have also been explored to improve efficiency. Medusa **Liu, Y., "Medusa: An MLP-Based Parallel Candidate Prediction Method"** employs MLPs for parallel candidate prediction, while Hydra **Wang, D., "Hydra: A Novel RNN-Based Draft Model for Efficient Decoding"** and Recurrent Drafter **Kumar, S., "Recurrent Drafter: An RNN-Based Regressive Generation Method"** use RNN-based models for regressive generation. EAGLE **Zhang, Y., "EAGLE: A Novel Transformer Decoder for Feature Sequence Autoregression"** and EAGLE2 **Li, X., "EAGLE2: An Improved Version of the EAGLE Model"** introduces a transformer decoder for autoregression over feature sequences, balancing accuracy and complexity. FSPAD **Wang, H., "FSPAD: A Novel Framework for Input Sequence Construction and Lightweight Draft Model Prediction"** constructs input sequences tailored for lightweight draft model predictions and introduces specialized training methods to improve draft quality. Additionally, methods like HASS **Shen, Y., "HASS: An Efficient Method for Addressing Feature Misalignment in Decoding"** address feature misalignment during training and decoding but do not fully resolve token-level misalignment.  
In contrast, this work focuses on addressing token misalignment, a critical challenge in speculative decoding. We propose the GRIFFIN framework, which introduces a token-alignable training strategy and a token-alignable draft model. By tackling this issue, GRIFFIN improves both acceptance length and speedup ratio, offering a complementary perspective to existing methods.