%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow} 
\usepackage{caption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{arxiv_griffin}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\M}{\mathcal{M}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Hm}{\mathcal{H}}
\newcommand{\xdi}[1]{\bar{\mathbf{x}}_{#1}}
\newcommand{\xmi}[1]{\mathbf{x}_{#1}}
%\newcommand{\fdi}[1]{\widehat{\mathbf{F}}_{#1}}
\newcommand{\fdi}[1]{\bar{\mathbf{F}}_{#1}}
\newcommand{\fmi}[1]{\mathbf{F}_{#1}}
\newcommand{\mmi}[1]{\mathbf{m}_{#1}}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\newcommand{\zp}[1]{\textcolor{blue}{#1}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{GRIFFIN: Effective Token Alignment for Faster Speculative Decoding}

\begin{document} 

\twocolumn[
\icmltitle{GRIFFIN: Effective Token Alignment for Faster Speculative Decoding}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\begin{icmlauthorlist}
\icmlauthor{Shijing Hu}{fdu}
\icmlauthor{Jingyang Li}{nus}
\icmlauthor{Xingyu Xie}{nus}
\icmlauthor{Zhihui Lu}{fdu}
\icmlauthor{Kim-Chuan Toh}{nus}
\icmlauthor{Pan Zhou}{smu}
\end{icmlauthorlist}

\icmlaffiliation{fdu}{Fudan University}
\icmlaffiliation{nus}{National University of Singapore}
\icmlaffiliation{smu}{Singapore Management University}

\icmlcorrespondingauthor{Zhihui Lu}{lzh@fudan.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Large Language Model, Speculative Decoding}

% \begin{figure*}[t]
% 	\includegraphics[width=2.1\columnwidth]{figures/motivation.png}
% 	\caption{Comparison between our GRIFFIN, EAGLE2, and HASS. (a) Speedup ratio compression on various models. (b) Acceptance length of different methods in various train steps consistent with HASS;(c) Misaligned token rate of the  methods with different forward passes, the multiple forward passes enable HASS to generate its own feature and align it during training;  }
% 	\label{token-disalignment}
% \end{figure*}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment.
The training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features.
Experiments on LLaMA-series and Vicuna models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 7\% and a speedup ratio exceeding 8\%, outperforming current SoTAs as shown in Fig.~\ref{motivation-fig} (a) and (b).
\end{abstract}

\section{Introduction}
\label{introduction}

\begin{figure*}[h!p!b!t]
	%\vskip 0.01in
	\begin{center}
		\centerline{\includegraphics[width=2.1\columnwidth]{figures/motivation.png}}
		\caption{Comparison between our GRIFFIN, EAGLE2, and HASS. (a) Speedup ratio comparison on various models. 
        (b) Acceptance length of different methods in various training steps, in which ``Step k" denotes forwarding $k$ passes to generate $k$ draft tokens and then building loss for network training. This is a progressive training in HASS.  
        (c) Misaligned token rate under different forward passes in each drafting-verification cycle, where  ``Forward  k" denotes forwarding $k$ passes to generate $k$ draft tokens.   
        }
		\label{motivation-fig}
	\end{center}
	%\vskip -0.1in
        \vspace{-7mm}
\end{figure*}

Large language models (LLMs), such as GPT-4~\cite{achiam2023gpt} and LLaMAs~\cite{touvron2023llama, touvron2023llama2}, have demonstrated extraordinary capabilities and potential in various fields, such as dialogue~\cite{zheng2023judging}, code generation~\cite{chen2021evaluating}, and mathematical reasoning~\cite{cobbe2021training}. However, autoregressive decoding, the standard approach for LLMs, generates tokens sequentially, with each token requiring a full forward pass through the entire model. Given the large size of LLMs, this process is both computationally expensive and time-consuming, limiting their practicality in time-sensitive applications.

To address the high computational cost of generation, speculative decoding~\cite{leviathan2023fast,chen2023accelerating} has become  widely adopted and demonstrated significant improvements in inference speed. Its core mechanism involves generating draft tokens with an efficient draft model, verifying them in parallel using the target LLM, and selecting tokens that match the target LLM's output distribution. This process allows multiple-token generation in a single forward pass of the target LLM, substantially reducing latency.

However, the efficiency of speculative decoding depends critically on achieving a high acceptance rate for draft tokens, while also minimizing the computational cost of generating them. Recent mixing-based methods, such as EAGLE~\cite{li2024eagle}, EAGLE2~\cite{li2024eagle2}, and Medusa~\cite{cai2024medusa}, address this by utilizing shallow-layer hidden states of the target LLM to guide the draft model’s token predictions. These methods improve computational efficiency and align draft and target models more closely by using target features to initialize the draft model.  


Despite these advances, existing mixing-based methods like EAGLE series~\cite{li2024eagle,li2024eagle2} face a fundamental limitation: misalignment between the training and decoding processes. During training, the draft model uses features from the target model and ground-truth tokens from training data, whereas in decoding, it relies on its own generated features and previously generated draft tokens.  This discrepancy introduces two key issues: (1) feature misalignment, where the features generated by the draft model during decoding diverge from those features used during training, and (2) token misalignment, where ground-truth tokens are replaced by draft tokens, often compounding errors over multiple steps. These misalignments, akin to exposure bias~\cite{bengio2015scheduled, schmidt2019generalization}, significantly degrade  the acceptance rate of draft tokens and thus  impair the overall speedup performance.  

Efforts to address feature misalignment like HASS~\cite{zhang2024learning} use the draft model's features to replace the target model's features during training. %wherein the draft model generates its own features over multiple forward passes.
While this aligns training with decoding, it neglects token misalignment, which is particularly problematic in autoregressive decoding. Errors from earlier decoding steps propagate and amplify, further exacerbating token misalignment. For instance, as shown in Fig.~\ref{motivation-fig} (c), EAGLE2 suffers from a token misalignment rate of 30\% during training, resulting in suboptimal acceptance lengths and limiting its effectiveness.  Similarly, while mitigating feature misalignment,  HASS sees token misalignment escalate to 35\% in later training steps, rendering it ineffective for deeper multi-forward harmonized   training, e.g., forward number $ \geq 3$, as shown in Fig.~\ref{motivation-fig} (b).  

\noindent{\textbf{Contributions.}} To solve  token misalignment, we propose a novel GRIFFIN  framework that contains a token-alignable training strategy and a token-alignable draft model.
 
Firstly, GRIFFIN develops a  \textit{token-alignable training strategy} to mitigate the adverse effects of token misalignment. It employs  a dynamic loss masking mechanism during training that focuses only on aligned tokens. Tokens are considered aligned if their corresponding ground-truth tokens appear in the top-$k$ predictions of the draft model. This approach excludes highly misaligned tokens from the loss computation during multiple forward passes, preventing their negative influence on model optimization.   Additionally, the top-$k$ criterion ensures consistency between training and decoding, as draft trees in the decoding phase are also constructed based on top-$k$ predictions rather than requiring exact matches to the highest-probability token. 

Secondly, to reduce token misalignment,  GRIFFIN designs a \textit{token-alignable draft model} by incorporating the architectural innovation of Token-Guided Fusion (TGF) into draft model in EAGLE~\cite{li2024eagle}. TGF performs a two-step fusion to refine feature representations and mitigate inconsistencies between the draft and target models. By incorporating input tokens twice—initially with features and later to refine them—TGF ensures that the draft model produces features more closely aligned with the target model, reducing feature and token misalignment.   

Together, these two innovations allow GRIFFIN to achieve complementary improvements. The token-alignable draft model reduces token misalignment, enabling more aligned tokens to contribute effectively to training. Meanwhile, the token-alignable training strategy ensures that the draft model operates on inputs that are closely aligned with the ground truth, enhancing the corrective power of TGF.  

Extensive experimental results demonstrate GRIFFIN's superior performance over state-of-the-arts (SoTAs) across diverse tasks, including dialogue (MT-Bench~\cite{zheng2023judging}), code generation (HumanEval~\cite{chen2021evaluating}), and mathematical reasoning (GSM8K~\cite{cobbe2021training}). For example,   Fig.~\ref{motivation-fig} (a) and (b) show that on LLaMA2-7/13B, LLaMA3-8B, and Vicuna-7B, GRIFFIN achieves an average acceptance length improvement of 18\% over EAGLE2 and 7\% over HASS, while delivering a speedup ratio of over 20\% over EAGLE2 and 8\% over HASS. 

\section{Related Work}
Speculative decoding~\cite{sun2024spectr, miao2024specinfer, chen2023cascade, kim2024speculative, liu2023online} accelerates LLM inference by dividing each decoding step into a draft stage and a verification stage. Existing methods differ primarily in their draft model architectures or strategies, each addressing specific challenges in speculative decoding.  

Several approaches focus on retrieving candidate predictions from similar contexts, such as PLD~\cite{saxena2023prompt}, Lookahead~\cite{fu2024break}, and CLLMs~\cite{kou2024cllms}, which rely on prompt-based context retrieval. However, their dependence on contextual similarity limits generalization to tasks with insufficient or unavailable context. Tree-based verification mechanisms, such as Sequoia~\cite{chen2024sequoia} and SpecExec~\cite{svirschevski2024specexec}, improve verification via hierarchical structures but introduce computational overhead, making them less suitable for latency-critical tasks.
Other works leverage databases or prior outputs for drafting, such as REST~\cite{he2023rest} and Ouroboros~\cite{zhao2024ouroboros}, which reuse previous outputs to improve draft coherence. However, these methods are constrained by the quality and availability of external resources. Chimera~\cite{zeng2024chimera} and Glide~\cite{du2024glide} integrate the target model into the draft model to improve token quality, but this increases computational costs.  

Lightweight draft models have also been explored to improve efficiency. Medusa~\cite{cai2024medusa} employs MLPs for parallel candidate prediction, while Hydra~\cite{ankner2024hydra} and Recurrent Drafter~\cite{cheng2024recurrent} use RNN-based models for regressive generation. EAGLE~\cite{li2024eagle} and EAGLE2~\cite{li2024eagle2} introduces a transformer decoder for autoregression over feature sequences, balancing accuracy and complexity. FSPAD~\cite{gui2024boosting} constructs input sequences tailored for lightweight draft model predictions and introduces specialized training methods to improve draft quality. Additionally, methods like HASS~\cite{zhang2024learning} address feature misalignment during training and decoding but do not fully resolve token-level misalignment.  
In contrast, this work focuses on addressing token misalignment, a critical challenge in speculative decoding. We propose the GRIFFIN framework, which introduces a token-alignable training strategy and a token-alignable draft model. By tackling this issue, GRIFFIN improves both acceptance length and speedup ratio, offering a complementary perspective to existing methods. 

\section{Motivation: Token Misalignment}
\label{pre}
    \begin{figure}[t]
   	\centering
   	\centerline{\includegraphics[width=1.05\columnwidth]{figures/train-decode-align.png}}
   	\caption{Token and feature misalignment in EAGLE.}
   	\label{train-decode}
   	\vspace{-3mm}
   \end{figure}

Speculative decoding~\cite{leviathan2023fast,chen2023accelerating} accelerates text generation through a ``draft-and-verify” mechanism. It uses two language models: a smaller  efficient draft model $\M$ to generate draft tokens via multiple forward passes per cycle, and a target model $\T$ to verify and accept tokens based on their alignment with the target distribution in a single forward pass per cycle.  

Unlike standard speculative decoding which autoregressively predicts tokens, EAGLE~\cite{li2024eagle} shifts autoregression to the feature level. Instead of generating tokens directly, it predicts hidden state features from the final layer of target model $\T$ before the LM head  $\Hm$.  
Let $\xmi{t}$ and $\xdi{t}$ represent the $t$-th ground-truth token from the training dataset and the $t$-th draft token generated by $\M$, respectively. Their corresponding hidden state features from $\T$ and $\M$ are $\fmi{t}$ and $\fdi{t}$.  
 
 
As shown in Fig.~\ref{train-decode}, during training, at time step $t$, the target model produces features $\fmi{t}$ for ground truth $\xmi{t}$, allowing the draft model to use $\fmi{t}$ and $\xmi{t}$ to generate the next features $\fdi{t+1}$ and tokens $\xdi{t+1}$.  However, during decoding, the draft-and-verify mechanism of speculative decoding ensures that the target model performs only a single forward pass during verification, but only after the draft model has generated all draft tokens in a cycle through multiple forward passes. %\zp{during decoding, verification occurs only after all draft tokens in a cycle are generated. not clear, need more explanation.} 
Thus, the draft model cannot access $\fmi{t}$ or $\xmi{t}$ at time step $t$ and instead relies solely on its own previously generated features $\fdi{t}$ and tokens $\xdi{t}$. This discrepancy introduces two key issues.  \textbf{1)  Feature Misalignment.}  During decoding, the features $\fdi{t}$ used by the draft model differ from the features $\fmi{t}$ it relied on during training.   \textbf{2) Token Misalignment.}  Similarly, during decoding, the tokens $\xdi{t}$ differ from the ground-truth tokens $\xmi{t}$ used in training.  


   

As illustrated in Fig.~\ref{motivation-fig} (c), we analyze the misaligned token rate of the draft model across multiple forward passes in each drafting-verification cycle. Here misaligned token rate refers to the frequency with which draft tokens $\xdi{t}$ differ from their corresponding ground-truth tokens $\xmi{t}$. Our results indicate that as the number of forward passes increases to generate more draft tokens, the misalignment rate also rises significantly, as evidenced by EAGLE2 and HASS in Fig.~\ref{motivation-fig} (c). This occurs because errors from earlier passes propagate and accumulate, exacerbating token misalignment in later passes. For example, EAGLE2 exhibits a misalignment rate of 49\% when predicting five draft tokens across multiple forward passes. While HASS partially mitigates feature misalignment, its token misalignment rate still reaches 35\%.  Such a high misalignment rate hampers training efficiency, particularly when the number of forward passes exceeds three in HASS. This is demonstrated in Fig.~\ref{motivation-fig} (b), where the acceptance length—the number of draft tokens accepted by the target LLM—fails to improve despite continued training. Thus, addressing token misalignment between the training and decoding phases is both crucial and  necessary.  

 


A straightforward solution would be to replace $\xmi{t}$ in the training data with $\xdi{t}$ from the draft model. However, this naive approach introduces feature inconsistency. Speculative decoding frameworks such as EAGLE and HASS rely on a fixed dataset to train the draft model, avoiding the computational burden of regenerating training data using the target LLM. Since the target model precomputes and stores features for all ground-truth tokens before training, substituting $\xmi{t}$ with $\xdi{t}$ disrupts alignment: the draft model then generates $\fdi{t}$ from different inputs than those used to generate $\fmi{t}$, making them incompatible for loss computation.  As a result, directly replacing ground-truth tokens with draft tokens not only fails to resolve token misalignment but can also degrade performance. This issue is evident in Appendix B of HASS~\cite{zhang2024learning}, where such substitution significantly reduces the acceptance length. In the next section, we introduce our effective solution to address this token misalignment challenge.

\section{GRIFFIN}
To address the token misalignment  challenge in Sec.~\ref{pre}, we propose GRIFFIN, a novel framework designed to mitigate token misalignment through two key components: token-alignable training elaborated in Sec.~\ref{sec-train} and a token-alignable draft model elaborated in Sec.~\ref{sec-draft-model}. %We elaborate on token-alignable training in Sec.~\ref{sec-train} and the design of the token-alignable draft model in Sec.~\ref{sec-draft-model}.  

\begin{figure}[t]
	\centering
	\centerline{\includegraphics[width=\columnwidth]{figures/TAD.png}}
	\caption{Structure of Token-Alignable Draft Model.}
	\label{TAD}
	\vspace{-4mm}
\end{figure}

\begin{figure*}[t!]
\begin{center}
\centerline{\includegraphics[width=2.1\columnwidth]{figures/TGF.png}}
\caption{Structure of Token-Guided Fusion (TGF).}
\label{feature-fusion}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Token-Alignable Training}
\label{sec-train} 

To resolve the token misalignment issue, we propose a token-alienable training strategy. More specifically,  our training consists of multiple steps,  with the number of forward passes performed by the draft model increasing at each step. That is, at training step $k$, the draft model performs $k$ forward passes to generate $k$ subsequent features and their corresponding draft tokens. The losses from these $k$ forward passes are averaged, and a single backward pass is performed to update the parameters of the draft model.   
The primary benefit of performing multiple forward passes is that the draft model can utilize the tokens and features generated by itself during previous forward passes, rather than relying on the ground-truth tokens and features generated by the target model. This approach effectively aligns the training process with the decoding phase, as in training phase, the draft model could simulate the similar input conditions encountered during decoding. %\zp{[not clear? e.g.,  performs $k$ forward passes  for predicting $k$ draft tokens? why $k$ forward passes and a single backward pass?  can explain more. Moreover, why need to train draft model for 1 forward pass and then k forward pass? what is the benefits? ]}. 
Then we elaborate on the first training step and its subsequent training step for   token alignment.  
	
\textbf{First Forward Pass}: Same as the conventional autoregressive generation, draft model $\M$  needs to predict the draft tokens which are then fed into target model $\T$ for verification and acceptance.  Specifically, at the $t$-th time step, we need to use draft model $\M$ and the LM head $\Hm$ in the target model to predict the $t$-th feature embedding $\fdi{t}$ and the $t$-th draft token $\xdi{t}$:

\begin{equation}
	\fdi{t}=\M(\xmi{1:t-1}, \fmi{1:t-1}), \quad 
\xdi{t} = \Hm(\fdi{t}).
\end{equation}
where    $\xmi{1:t-1}$ denotes the  token sequence $\{\xmi{i}\}_{i=1}^{t-1}$ from training dataset and $\fmi{1:t-1}$ are the  feature embedding sequence $\{\fmi{i}\}_{i=1}^{t-1}$ generated by target model  $\T$. 
 
 
 Then we  resolve the token misalignment caused by the inconsistency between training and decoding phases as introduced in Sec.~\ref{pre}. %To this end, we align the training criterion with the decoding phase. 
 Specifically, during decoding, EAGLE employs a tree-structured draft, where nodes at the same level of the draft tree represent different top-$k$ draft tokens generated during the same forward pass. This tree structure enables alternative branches of top-$k$ draft tokens to be explored if the top-1 draft token is rejected.   
 %\zp{[need to introduce more, not so clear.]}
 For consistency, during training, a draft token $\xdi{t}$ is considered aligned if its ground-truth token $\xmi{t}$ appears within the top-$k$ predictions of the draft model, rather than relying solely on the top-1 prediction. This ensures that tokens beyond the top-1 prediction also contribute to the training process, aligning with the decoding phase, where top-$k$ tokens can be explored if the top-1 draft token is rejected. 
 %\zp{[still unclear why this can make training and decoding consistent. need more explanation]} 
 Accordingly, we introduce a mask $\mmi{t}=1$ to mark this consistency. For these misaligned draft tokens like $\xmi{s}$, their masks like $\mmi{s}=0$.  This ensures  the alignment criterion during   both training and decoding  phases.  
 In this way, we can use the following training loss to train the draft model $\M$:
  \begin{equation}\label{afsafs}
 	\begin{aligned}
 		\mathcal{L}_{\M}^{(1)} = \frac{1}{\sum_{t=1}^{l} \mmi{t}} \sum_{t=1}^{l} \mmi{t} \ell(\xdi{t}, \xmi{t}, \fdi{t}, \fmi{t}), 
 	\end{aligned}
 \end{equation}
 where the loss function $\ell(\xdi{t}, \xmi{t}, \fdi{t}, \fmi{t})$ combines two components: a feature-level loss, computed as the \( \ell_1 \) loss between $\fmi{t}$ and $\fdi{t}$, and a token-level loss, computed as the cross-entropy loss between $\xmi{t}$ and $\xdi{t}$. 
 %\zp{[introduce it in detail]}
 
 
\noindent\textbf{$k$-th Forward Pass $(k\geq 2)$.} Draft model $\M$ would predict $k$ draft tokens at  the $k$-th forward pass.  Similarly, we also align the training criterion with the decoding phase. Like the first forward pass, at the $t$ timestep of the $k$-th forward pass,  during training, a draft token $\xdi{t}$ is considered aligned if its ground-truth token $\xmi{t}$ appears in the top-$k$ predictions of the draft model, and its mask is set $\mmi{t}=1$. Since the current draft token $\xmi{t}$  is decided by previous predicted draft tokens $\xmi{t-k:t-1}$ predicted in earlier $(k-1)$ forward passes, then if  any draft token in $\xmi{t-k:t-1}$ is misalignment,  the draft token $\xmi{t}$ would likely be misalignment. So we further adjust the current mask  $\mmi{t}$ with the   alignment masks  $\mmi{t-k+1:t-1}$ of draft tokens $\xmi{t-k+1:t-1}$:
\begin{equation}
    \mmi{t} = \prod\nolimits_{i=t-k+1}^{t-1} \mmi{i}.
\end{equation}
Next, to further ensure alignment between training and decoding, we replace the   features $\fmi{t-k+1:t}$ generated by the target model  with the features $\fdi{t-k+1:t}$   generated by the draft model in the previous $(k-1)$ forward passes. Then the draft model $\M$ and the LM head $\Hm$ are used to generate the feature $\fdi{t}$  and the draft token $\xdi{t}$:  
\begin{equation}
	\begin{aligned}
		\fdi{t}= \M(\xmi{1:t}, \fmi{1:t-k}, \fdi{t-k+1:t}),  \quad 
		 \xdi{t}  = \Hm(\fdi{t}). \\
	\end{aligned}
\end{equation}
In this way, we can also use $(\xdi{t}, \xmi{t}, \fdi{t}, \fmi{t})$ to build the loss $ 		\mathcal{L}_{\M}^{(k)} $ in Eqn.~\eqref{afsafs}.

\noindent{\textbf{Training loss.}} We use the above method to construct the training loss $ 	\{\mathcal{L}_{\M}^{(k)}\}_{k=1}^{K} $ for all $K$ forward passes, and average all  training losses to train the draft model $\M$:
\begin{equation}
	\begin{aligned}
\min_{\M} \frac{1}{K} \sum_{k=1}^{K} \mathcal{L}_{\M}^{(k)}. 
	\end{aligned}
\end{equation}
This means that we  forward $K$ passes and then backward one time to train the draft model $\M$. In all experiments, we set $K=3$ which works well in practice as shown in Sec.~\ref{exp}.   
 
	\subsection{Token-Alignable Draft Model}
	\label{sec-draft-model}
	To enhance the  accuracy of  draft tokens and mitigate token misalignment, we introduce a token-alignable draft model that systematically addresses the feature inconsistency issues in existing draft models by introducing Token-Guided Fusion (TGF).
        As shown in Fig.~\ref{TAD}, we introduce  TGF module before the auto-regressive layer to fuse the input features $\fmi{t}$ and tokens $\xmi{t}$. After processing through the auto-regressive layer, following prior work~\cite{gui2024boosting}, we also adopt a dual-head design (TEH) to decouple the conflicting objectives of token prediction and feature generation within the draft model. Specifically, TEH generates a \textit{predict feature} $\fdi{t+1}^P$ for next-token prediction and a \textit{regress feature} $\fdi{t+1}^R$ to serve as input for subsequent forward passes.  
	%\zp{need to briefly introduce what positions these two components are in the draft model. It is better to have an overall figure to show.}
	
	Now we introduce   TGF module  more. It is  designed to improve feature consistency.  Feature representations in the draft model, being continuous and derived from a high-dimensional space, often fail to fully align the ones in the target model. This misalignment persists even with extensive training because the feature-level loss cannot be reduced to zero. As a result, the draft model struggles to generate features that accurately reflect the target model’s feature distribution, leading to the misalignment. 
	
	To address this, TGF enhances feature fusion by prioritizing token embeddings, ensuring that the draft model generates feature representations that better reflect the target model’s feature distribution. As illustrated in Fig.~\ref{feature-fusion}, TGF follows a three-step process. 1) \textit{Initial Fusion.} The input feature and token embedding (both of embedding dimension \( d \)) are concatenated along the embedding dimension and passed through a lightweight Multi-Layer Perceptron (MLP) which compresses the representation back to \( d \), forming the initial fused features.  2) \textit{Feature Normalization and Expansion.} To further align distributions, layer normalization is applied separately to the fused features and token embeddings. The outputs are concatenated again and processed by an Up Projector (lightweight MLP), expanding the embedding dimension to \(3 \times d \). This higher-dimensional space allows the model to separate essential information from noise.  3) \textit{Refinement and Stability.} The expanded features undergo activation via SiLU and are then compressed back to \( d \) using a Down Projector lightweight MLP). Moreover, a residual connection is applied by adding the refined features back to the initial fused features, ensuring stable optimization.  
	
	By emphasizing token embeddings during feature fusion, TGF aligns feature representations with token distributions, reducing inconsistencies and improving the draft model’s token prediction accuracy.  


\section{Experiments}\label{exp}

\begin{table*}[ht]
\caption{Comparison of different speculative decoding methods. This table presents evaluation results on standard LLM benchmarks with temperature $T \in \{0, 1\}$, including speedup ratio $SR$ and acceptance lengths $\tau$. Higher values indicate better performance.} %SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methods like Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration. Therefore, we do not compare GRIFFIN with these methods.}
\label{main-result}
\vskip 0.01in
\begin{center}
\begin{small}
%\begin{sc}
\resizebox{2.1\columnwidth}{!}{
\setlength{\tabcolsep}{4pt}
\begin{tabular}{ll||cccccc|cc||cccccc|cc}
        \toprule
        &  & \multicolumn{8}{c||}{Temperature = 0} & \multicolumn{8}{c}{Temperature = 1} \\
       \midrule
        Model & Method & \multicolumn{2}{c}{MT-bench} & \multicolumn{2}{c}{HumanEval} & \multicolumn{2}{c|}{GSM8K} & \multicolumn{2}{c||}{Mean} & \multicolumn{2}{c}{MT-bench} & \multicolumn{2}{c}{HumanEval} & \multicolumn{2}{c|}{GSM8K} & \multicolumn{2}{c}{Mean} \\
        \midrule
        & & $SR$ & $\tau$ & $SR$ &  $\tau$ & $SR$ &  $\tau$ & $SR$ &  $\tau$ & $SR$ &  $\tau$ & $SR$ &  $\tau$ & $SR$ &  $\tau$ & $SR$ &  $\tau$ \\
        \midrule
        \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}LLaMA2 \\ Chat \\ 7B\end{tabular}} & PLD & 1.38 & 1.43 & 1.52 & 1.59 & 1.32 & 1.37 & 1.41 & 1.46 & \multicolumn{8}{c}{\multirow{2}{*}{N/A, since the acceptance conditions are relaxed}} \\
        & Lookahead & 1.61 & 1.66 & 1.72 & 1.77 & 1.58 & 1.65 & 1.64 & 1.69 & \\
        & EAGLE & 1.90 & 3.68 & 2.10 & 3.90 & 2.04 & 3.77 & 2.01 & 3.78 & 1.50 & 3.45 & 1.91 & 3.67 & 1.87 & 3.62 & 1.76 & 3.58 \\
        & EAGLE-2 & 2.66 & 4.44 & 3.06 & 4.78 & 2.72 & 4.60 & 2.81 & 4.61 & 2.39 & 4.23 & 2.87 & 4.47 & 2.54 & 4.50 & 2.71 & 4.40 \\
        & FSPAD & 2.89 & 4.82 & 3.38 & 5.62 & 2.95 & 4.99 & 3.07 & 5.14 & 2.61 & 4.53 & 3.14 & 5.35 & 2.84 & 4.88 & 2.86 & 4.92 \\
        & HASS & 2.99 & 4.89 & 3.41 & 5.59 & 3.04 & 5.11 & 3.15 & 5.20 & 2.70 & 4.58 & 3.13 & 4.91 & 2.87 & 5.21 & 2.90 & 4.93 \\
        & GRIFFIN & \textbf{3.12} & \textbf{5.11} & \textbf{3.61} & \textbf{5.93} & \textbf{3.10} & \textbf{5.27} & \textbf{3.28} & \textbf{5.44} & \textbf{2.81} & \textbf{4.81} & \textbf{3.33} & \textbf{5.63} & \textbf{3.06} & \textbf{5.26} & \textbf{3.17} & \textbf{5.23} \\
        \midrule
         \multirow{8}{*}{\begin{tabular}[c]{@{}c@{}}LLaMA3 \\ Instruct\\ 8B\end{tabular}} & PLD & 1.32 & 1.37 & 1.48 & 1.56 & 1.34 & 1.44 & 1.38 & 1.46 & \multicolumn{8}{c}{\multirow{3}{*}{N/A, since the acceptance conditions are relaxed}} \\
        & Lookahead & 1.35 & 1.49 & 1.61 & 1.72 & 1.45 & 1.71 & 1.47 & 1.64 &  \\
        & Medusa & 1.68 & 1.73 & 2.11 & 2.32 & 2.35 & 2.42 & 2.05 & 2.16 &  \\
        & EAGLE & 1.29 & 2.91 & 2.00 & 3.66 & 1.85 & 3.57 & 1.71 & 3.38 & 1.25 & 2.67 & 1.41 & 3.35 & 1.67 & 3.30 & 1.44 & 3.11 \\
        & EAGLE-2 & 2.64 & 4.21 & 3.31 & 4.93 & 2.54 & 4.42 & 2.83 & 4.52 & 2.39 & 3.90 & 2.54 & 4.73 & 2.48 & 4.30 & 2.47 & 4.31 \\
        & FSPAD & 2.72 & 4.52 & 3.40 & 5.39 & 2.95 & 4.77 & 3.02 & 4.89 & 2.43 & 4.09 & 3.04 & 5.18 & 2.75 & 4.60 & 2.74 & 4.62 \\
        & HASS & 2.78 & 4.68 & 3.43 & 5.54 & 3.06 & 5.02 & 3.09 & 5.08 & 2.49 & 4.26 & 3.05 & 5.30 & 2.89 & 4.85 & 2.81 & 4.80 \\
        & GRIFFIN & \textbf{3.09} & \textbf{4.85} & \textbf{3.65} & \textbf{5.97} & \textbf{3.30} & \textbf{5.31} & \textbf{3.35} & \textbf{5.38} & \textbf{2.62} & \textbf{4.35} & \textbf{3.31} & \textbf{5.62} & \textbf{3.07} & \textbf{5.08} & \textbf{3.00} & \textbf{5.20} \\
        \midrule
        \multirow{10}{*}{\begin{tabular}[c]{@{}c@{}}Vicuna1.5 \\ 7B\end{tabular}} & SPS & 1.82 & 2.36 & 1.99 & 2.61 & 1.71 & 2.26 & 1.84 & 2.41 & 1.50 & 1.87 & 1.55 & 1.95 & 1.53 & 1.82 & 1.53 & 1.88 \\
        & PLD & 1.61 & 1.68 & 1.82 & 1.87 & 1.82 & 1.99 & 1.75 & 1.85 & \multicolumn{8}{c}{\multirow{4}{*}{N/A, since the acceptance conditions are relaxed}} \\
        & Medusa & 1.91 & 2.52 & 2.02 & 2.67 & 1.89 & 2.59 & 1.94 & 2.59 &  \\
        & Lookahead & 1.63 & 1.69 & 1.72 & 1.77 & 1.84 & 1.99 & 1.73 & 1.82 &  \\
        & Hydra & 2.69 & 3.60 & 2.98 & 3.79 & 2.73 & 3.66 & 2.80 & 3.68 & \\
        & EAGLE & 2.90 & 3.94 & 3.33 & 4.29 & 3.08 & 3.97 & 3.10 & 4.07 & 2.13 & 3.17 & 2.39 & 3.43 & 2.34 & 3.29 & 2.29 & 3.30 \\
        & EAGLE-2 & 3.56 & 4.74 & 3.95 & 5.33 & 3.69 & 5.03 & 3.73 & 5.03 & 3.15 & 4.20 & 3.33 & 4.65 & 3.41 & 4.65 & 3.30 & 4.50 \\
        & FSPAD & 3.71 & 5.12 & 4.12 & 5.74 & 3.82 & 5.32 & 3.88 & 5.39 & 3.26 & 4.51 & 3.45 & 5.11 & 3.55 & 4.95 & 3.42 & 4.86 \\
        & HASS & 3.91 & 5.15 & 4.22 & 5.86 & 3.97 & 5.41 & 4.03 & 5.47 & 3.34 & 4.52 & 3.62 & 5.16 & 3.70 & 5.03 & 3.55 & 4.90 \\
        & GRIFFIN & \textbf{4.02} & \textbf{5.36} & \textbf{4.53} & \textbf{6.29} & \textbf{4.14} & \textbf{5.63} & \textbf{4.23} & \textbf{5.76} & \textbf{3.38} & \textbf{4.64} & \textbf{4.12} & \textbf{5.68} & \textbf{3.88} & \textbf{5.29} & \textbf{3.79} & \textbf{5.20} \\
        \midrule
        \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}LLaMA2 \\ Chat \\ 13B\end{tabular}} & PLD & 1.42 & 1.46 & 1.63 & 1.70 & 1.41 & 1.44 & 1.49 & 1.53 & \multicolumn{8}{c}{\multirow{2}{*}{N/A, since the acceptance conditions are relaxed}} \\
        & Lookahead & 1.58 & 1.64 & 1.80 & 1.85 & 1.65 & 1.69 & 1.68 & 1.73 & \\
        & EAGLE & 1.80 & 3.86 & 2.46 & 4.5 & 2.41 & 4.17 & 2.22 & 4.18 & 1.84 & 3.62 & 2.10 & 4.27 & 2.21 & 3.98 & 2.05 & 3.96 \\
        & EAGLE-2 & 3.02 & 4.74 & 3.64 & 5.57 & 3.23 & 5.17 & 3.30 & 5.16 & 3.04 & 4.60 & 3.45 & 5.41 & 3.13 & 5.03 & 3.21 & 5.01 \\
        & FSPAD & 3.09 & 5.05 & 3.91 & 5.98 & 3.32 & 5.35 & 3.44 & 5.46 & 3.12 & 4.85 & 3.61 & 5.81 & 3.24 & 5.25 & 3.32 & 5.30 \\
        & HASS & 3.14 & 5.07 & 4.17 & 6.01 & 3.41 & 5.48 & 3.57 & 5.52 & 3.21 & 4.90 & 3.71 & 5.83 & 3.32 & 5.34 & 3.41 & 5.36 \\
        & GRIFFIN & \textbf{3.33} & \textbf{5.27} & \textbf{4.29} & \textbf{6.26} & \textbf{3.61} & \textbf{5.56} & \textbf{3.74} & \textbf{5.70} & \textbf{3.36} & \textbf{5.07} & \textbf{3.94} & \textbf{6.13} & \textbf{3.61} & \textbf{5.49} & \textbf{3.64} & \textbf{5.56} \\
        \bottomrule
    \end{tabular}
}
%\end{sc}
\end{small}
\end{center}
%\vskip -0.1in
\vspace{-3mm}
\end{table*}

\subsection{Experimental Setup}

\textbf{Target LLMs.} We evaluate our approach on a diverse set of LLMs, including LLaMA2-Chat 7B/13B, LLaMA3-Instruct 8B~\cite{touvron2023llama2}, and Vicuna-1.5 7B~\cite{leng2023chinese-vicuna}. To ensure consistency and fairness, all inference processes are conducted on a single NVIDIA A100 GPU.  


\textbf{Tasks.} We conduct evaluations across three representative generation tasks: multi-turn conversation, code generation, and mathematical reasoning. For these tasks, we utilize the MT-Bench~\cite{zheng2023judging}, HumanEval~\cite{chen2021evaluating}, and GSM8K~\cite{cobbe2021training} datasets, respectively. To ensure consistency and comparability with prior works such as DistillSpec~\cite{zhou2023distillspec} and EAGLE~\cite{li2024eagle}, we set the batch size to 1, and set the temperature $T \in \{0,1\}$ for all experiments.  

\textbf{Metrics.} GRIFFIN is a lightweight acceleration method that neither fine-tunes the target LLMs’ weights during training nor relaxes the acceptance conditions during decoding. As a result, the generation results remain unchanged, eliminating the need for additional quality evaluation. To measure the acceleration performance, we adopt  two metrics:  (1) \textbf{Speedup Ratio ($SR$):} The actual test speedup ratio relative to vanilla auto-regressive decoding; (2) \textbf{Acceptance Length~($\tau$):} The average number of tokens generated per drafting-verification cycle, indicating the number of tokens accepted by the target LLM from the draft model.  

\textbf{Comparisons.} Vanilla auto-regressive decoding is used as the baseline, serving as the benchmark for speedup ratios (1.00x). %\zp{not sure to use $\times$ or x. Reply: EAGLE and HASS all use x}
We compare GRIFFIN against recent SoTA speculative decoding methods, including SPS (standard speculative sampling with its draft model being Vicuna-68M)~\cite{leviathan2023fast}, PLD~\cite{saxena2023prompt}, Lookahead~\cite{fu2024break}, Medusa~\cite{cai2024medusa}, Hydra~\cite{ankner2024hydra}, EAGLE~\cite{li2024eagle}, EAGLE-2~\cite{li2024eagle2}, FSPAD~\cite{gui2024boosting}, and HASS~\cite{zhang2024learning}.  

\textbf{Implementation.} Our implementation is based on the open-source repository of EAGLE-2. We use the ShareGPT dataset to train the draft model. In token-alignable training, we set $k=3$ for aligning top-$k$ tokens, training the draft model for $N=3$ steps. %For the dynamic tree structure \zp{never mention before}, we set the total number of draft tokens to 60 across all experiments, with a draft tree depth of 6. 
Other settings, such as optimizer, are kept consistent with EAGLE-2 to ensure comparability.  

\begin{table}[t]
\caption{Ablation study on TAT and TAD. This table presents the evaluation of acceptance lengths on LLM benchmarks with temperature $T \in \{0, 1\}$. Higher values indicate better performance.}
\label{ablation-overall}
%\vskip 0.1in
\vspace{-2mm}
\begin{center}
\begin{small}
%\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll||ccc|c}
        \toprule
        %\multicolumn{5}{c}{Temperature = 0} \\
        %\midrule
        Temp & Method & MT-bench & HumanEval & GSM8K & Mean \\
        \midrule
        \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}} $T=0$ \end{tabular}} & GRIFFIN & \textbf{5.11} & \textbf{5.93} & \textbf{5.27} & \textbf{5.44}  \\
        & w/o both & 4.44 & 4.78 & 4.60 & 4.61  \\
        & w/o TAT & 4.85 & 5.65 & 5.04 & 5.18  \\
        & w/o TAD & 4.94 & 5.68 & 5.14 & 5.25  \\
        %\midrule
        %\multicolumn{5}{c}{Temperature = 1} \\
        %\midrule
        % Method & MT-bench & HumanEval & GSM8K & Mean \\
        \midrule
        \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}} $T=1$ \end{tabular}} & GRIFFIN &  \textbf{4.81} & \textbf{5.63} & \textbf{5.26} & \textbf{5.23} \\
        & w/o both &  4.23 & 4.47 & 4.50 & 4.40 \\
        & w/o TAT & 4.56 & 5.34 & 4.96 & 4.95 \\
        & w/o TAD & 4.65 & 5.31 & 5.05 & 5.00 \\
        \bottomrule
    \end{tabular}
}
%\end{sc}
\end{small}
\end{center}
%\vskip -0.1in
\vspace{-2mm}
\end{table}

\subsection{Comparison with SoTAs}
We present the acceptance lengths and speedup ratio of various methods across three datasets in Table~\ref{main-result}. GRIFFIN consistently achieves the highest acceptance length and speedup ratio across all datasets and LLMs tested. %GRIFFIN demonstrates the best performance on code generation tasks(HumanEval dataset), achieving a speedup ratio of up to 4.5x, as the fixed templates in the code generation task simplify drafting and acceleration. \zp{seems no relation between these 2 sentences here}
Each GRIFFIN drafting-verification cycle generates approximately 5–6 tokens, significantly exceeding other methods. This is roughly three times the amount of standard speculative sampling and 1.5 times the amount of EAGLE.   

For the multi-round conversation task (MT-Bench) with LLaMA3 8B (temperature $T=0$), GRIFFIN achieves an 11.1\% higher speedup ratio compared to HASS. Even for temperature $T=1$, GRIFFIN maintains a 5.2\% improvement over HASS.   
For the code generation task (HumanEval) with Vicuna 7B, GRIFFIN demonstrates a 7.3\% increase in speedup ratios compared to HASS at a temperature of 0, and a 13.8\% improvement when the temperature is set to 1.  
For the mathematical reasoning task (GSM8K with LLaMA2 13B), GRIFFIN achieves a 5.9\% increase in speedup ratios compared to HASS  with temperature $T=0$, and an 8.7\% improvement at a temperature of 1.

The results across diverse tasks and models highlight the versatility and effectiveness of GRIFFIN. The consistent improvements over HASS, even at different temperatures, underscore GRIFFIN's robustness in handling varying levels of uncertainty in token predictions.   
Moreover, the performance gains in tasks like code generation and mathematical reasoning suggest that GRIFFIN's token-alignable speculative decoding framework is particularly advantageous for applications requiring high precision and reasoning capabilities. These findings position GRIFFIN as a strong candidate for accelerating LLM inference in real-world scenarios, where both speed and accuracy are critical.   
%\zp{add more numerical detailed comparison and discussion, the current parts for the core results are too short.}

\subsection{Ablation Study}
\paragraph{Components} In this experiment, we conduct the ablation study using LLaMA2-Chat 7B. We first conduct an ablation study to analyze the impact of token-alignable training (TAT) and the token-alignable draft Model (TAD) in GRIFFIN. As shown in Table~\ref{ablation-overall}, removing either component leads to a substantial reduction in acceptance length. %, demonstrating their essential contributions to the overall performance of GRIFFIN. \zp{can add more diss for the components here.}
Specifically, removing TAT results in a noticeable drop in performance across all benchmarks, with a mean acceptance length reduction of 0.26 at $T=0$ and 0.28 at $T=1$. This highlights the importance of TAT in aligning draft tokens during the training process.  
Similarly, removing TAD also results in significant performance degradation, with a mean acceptance length reduction of 0.19 at $T=0$ and 0.23 at $T=1$. This demonstrates the critical role of TAD in mitigating the misaligned token rate. 
Finally, the results show that removing both components (w/o both) leads to the most severe performance drop, with mean acceptance lengths reduced by 0.83 at  $T=0$ and 0.83 at  $T=1$ compared to the full GRIFFIN model. This suggests that TAT and TAD work synergistically to maximize the acceptance length, with TAT aligning draft tokens during training process and TAD mitigating the misaligned token rate. The combination of these two components ensures that GRIFFIN achieves SoTA performance across diverse benchmarks.  

\paragraph{Hyper-param. for TAT}
We first analyze the effect of the hyper-parameter $k$, which determines the number of top-$k$ tokens to align. As shown in Table~\ref{ablation-topk}, aligning the top-$k$ tokens (for $k$ ranging from 1 to 10) consistently improves the acceptance length compared to not aligning tokens. Notably, aligning only the top-1 token is less effective, as it neglects many other tokens that could benefit from alignment. The acceptance length achieves its peak when $k=3$, suggesting that aligning a small but sufficient number of tokens provides the optimal trade-off between alignment and generalization.  

\begin{table}[t]
\caption{Comparison of different top-$k$ parameter for GRIFFIN. This table presents evaluation of acceptance lengths on standard LLM benchmarks with temperature $T \in \{0, 1\}$. Higher values indicate better performance. NA represents do not align token.}
\label{ablation-topk}
\vskip 0.1in
\vspace{-2mm}
\begin{center}
\begin{small}
%\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lc||ccc|c}
        \toprule
        %\multicolumn{5}{c}{Temperature = 0} \\
        %\midrule
        Temp & top-$k$ & MT-bench & HumanEval & GSM8K & Mean  \\
        \midrule
        \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}} $T=0$ \end{tabular}} & 1 & 5.03 & 5.85 & 5.17 & 5.35  \\
        & 3 & \textbf{5.11} & \textbf{5.93} & \textbf{5.27} & \textbf{5.44} \\
        & 5 & 5.09 & 5.91 & 5.23 & 5.41  \\
        & 10 & 5.05 & 5.84 & 5.19 & 5.36  \\
        & NA & 4.95 & 5.76 & 5.12 & 5.28  \\
        \midrule
        %\multicolumn{5}{c}{Temperature = 1} \\
        %\midrule
        %top-$k$ & MT-bench & HumanEval & GSM8K & Mean \\
        %\midrule
        \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}} $T=1$ \end{tabular}} & 1 & 4.73 & 5.52 & 5.10 & 5.12  \\
        & 3 & \textbf{4.81} & \textbf{5.63} & \textbf{5.26} & \textbf{5.23} \\
        & 5 & 4.78 & 5.61 & 5.19 & 5.19  \\
        & 10 & 4.72 & 5.60 & 5.12 & 5.15  \\
        & NA & 4.65 & 5.47 & 5.01 & 5.04  \\
        \bottomrule
    \end{tabular}
}
%\end{sc}
\end{small}
\end{center}
%\vskip -0.1in
\vspace{-2mm}
\end{table}

We further analyze the effect of increasing the number of training steps N in \textbf{TAT}. As shown in Table~\ref{ablation-align-step}, increasing the training steps steadily improves GRIFFIN's acceptance length during the first 5 steps. In contrast to HASS, which plateaus after step 3 (as shown in Fig.~\ref{motivation-fig}b), GRIFFIN continues to improve due to its token alignment mechanism. However, as the number of aligned tokens decreases with each additional training step, the improvements become less pronounced at steps 4 and 5. To ensure a fair comparison with HASS, we choose the number of training steps $N=3$ in our experiments.  

\begin{table}[t]
\caption{Comparision of varied training steps for GRIFFIN. This table presents evaluation results of acceptance lengths on standard LLM benchmarks with temperature $T \in \{0, 1\}$. Higher values indicate better performance.}
\label{ablation-align-step}
\begin{center}
\begin{small}
%\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lc||ccc|c}
        \toprule
        %\multicolumn{5}{c}{Temperature = 0} \\
        %\midrule
        Temp & Step & MT-bench & HumanEval & GSM8K & Mean  \\
        \midrule
        \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}} $T=0$ \end{tabular}} & 1 & 4.85 & 5.65 & 5.04 & 5.18  \\
        & 2 & 5.02 & 5.81 & 5.15 & 5.33  \\
        & 3 & 5.11 & 5.93 & 5.27 & 5.44  \\
        & 4 & 5.13 & 5.96 & 5.31 & 5.46  \\
        & 5 & \textbf{5.14} & \textbf{5.98} & \textbf{5.33} & \textbf{5.48}  \\
        \midrule
        %\multicolumn{5}{c}{Temperature = 1} \\
        %\midrule
        %Step & MT-bench & HumanEval & GSM8K & Mean \\
        %\midrule
        \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}} $T=1$ \end{tabular}} & 1 & 4.56 & 5.34 & 4.96 & 4.95 \\
        & 2 & 4.73 & 5.49 & 5.14 & 5.12 \\
        & 3 & 4.81 & 5.63 & 5.26 & 5.23 \\
        & 4 & 4.84 & 5.66 & 5.31 & 5.27 \\
        & 5 & \textbf{4.86} & \textbf{5.68} & \textbf{5.34} & \textbf{5.29} \\
        \bottomrule
    \end{tabular}
}
%\end{sc}
\end{small}
\end{center}
%\vskip -0.1in
\vspace{-4mm}
\end{table}


\begin{figure}[ht]
\vskip 0.1in
%\vspace{-4mm}
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/ablation-draft-model.png}}
\caption{Ablation study of misaligned token rate for TAD.}
\label{ablation-draft}
\end{center}
%\vskip -0.1in
\vspace{-6mm}
\end{figure}

\paragraph{Components of TAD}
We analyze the effectiveness of {TGF} and {TEH} in reducing the misaligned token rate within TAD. As shown in Fig.~\ref{ablation-draft}, the misaligned token rate increases significantly when either TGF or TEH is removed. Furthermore, when both TGF and TEH are removed, the misaligned token rate increases even more dramatically, indicating that these components work synergistically to mitigate misaligned token rate for draft model.  


\begin{table}[t]
\caption{Ablation study on TGF. This table presents evaluation results of acceptance lengths on standard LLM benchmarks with temperature $T \in \{0, 1\}$. Higher values indicate better performance. Feature represents using feature to replace token embedding in secondary fusion of TGF, fused represents using initial fused feature to replace token embedding in secondary fusion of TGF.}
\label{ablation-TGF}
%\vskip 0.1in
%\vspace{-4mm}
\begin{center}
\begin{small}
%\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll||ccc|c}
        \toprule
        %\multicolumn{5}{c}{Temperature = 0} \\
        %\midrule
        Temp & Method & MT-bench & HumanEval & GSM8K & Mean \\
        \midrule
        \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} $T=0$ \end{tabular}} & GRIFFIN & \textbf{5.11} & \textbf{5.93} & \textbf{5.27} & \textbf{5.44}  \\
        & feature & 4.44 & 4.78 & 4.60 & 4.61  \\
        & fused & 4.85 & 5.65 & 5.04 & 5.18  \\
        \midrule
        %\multicolumn{5}{c}{Temperature = 1} \\
        %\midrule
        %Method & MT-bench & HumanEval & GSM8K & Mean \\
        %\midrule
        \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} $T=0$ \end{tabular}} & GRIFFIN &  \textbf{4.81} & \textbf{5.63} & \textbf{5.26} & \textbf{5.23} \\
        & feature &  4.23 & 4.47 & 4.50 & 4.40 \\
        & fused & 4.56 & 5.34 & 4.96 & 4.95 \\
        \bottomrule
    \end{tabular}
}
%\end{sc}
\end{small}
\end{center}
%\vskip -0.1in
\vspace{-4mm}
\end{table}

We evaluate whether the effectiveness of using tokens to correct inconsistent features in \textit{Token-Guided Fusion (TGF)} stems from their inherent utility or merely from an increase in model parameters. To investigate this, we conduct an ablation study (Table~\ref{ablation-TGF}) by replacing the token embeddings used in the secondary fusion step of TGF with either raw features or the initial fused features, while keeping the model architecture and training process unchanged.  
When the token embeddings are replaced with raw features, the acceptance length decreases by 0.83, indicating that relying solely on features is insufficient to correct inconsistent features in TGF. This demonstrates the critical role of token embeddings in addressing feature inconsistencies.   
When the token embeddings are replaced with initial fused features, the acceptance length is higher compared to using raw features. This is because the initial fused features incorporate some token information, which partially helps in correcting inconsistent features in TGF. However, the acceptance length still decreases by 0.26 compared to GRIFFIN, highlighting that the full effectiveness of TGF relies on the explicit use of token embeddings in the secondary fusion step.  
These results demonstrate the critical role of TGF in addressing feature inconsistencies and confirm that the improvements in TGF are not merely attributable to parameter scaling but stem from the inherent utility of token embeddings in correcting inconsistent features.

\section{Conclusion}
In this paper, we introduce \textbf{GRIFFIN}, a token-alignable speculative decoding framework. Previous works have largely overlooked the issue of token misalignment between training and decoding. To address this, GRIFFIN incorporates a token-alignable training strategy to excludes misaligned tokens from the loss computation. Furthermore, GRIFFIN adopts a token-alignable draft model, which significantly reduces the token misalignment rate. We conducted extensive evaluations across various LLMs and datasets, comparing GRIFFIN against several SOTA speculative decoding methods. In all experiments, GRIFFIN consistently achieved the highest speedup ratios and acceptance length, demonstrating its effectiveness and efficiency.

\noindent{\textbf{Limitations.}} GRIFFIN adopts a multi-step training process for token-alignable training, which incurs additional training overhead compared to EAGLE. However, since the draft model is trained only once, real-world applications prioritize decoding efficiency over training overhead, as inference is the primary bottleneck. GRIFFIN improves the speedup ratio by over 20\% compared to EAGLE2, making the extra training cost a worthwhile trade-off for the significant inference acceleration it delivers.  Furthermore, GRIFFIN’s overall training overhead remains comparable to that of HASS. Under the same training cost, GRIFFIN achieves an over 8\% improvement in speedup ratio compared to HASS, further highlighting its effectiveness.

%GRIFFIN employs a multi-step training process during token-alignable training, which introduces additional training overhead compared to EAGLE. However, the overall training overhead of GRIFFIN is comparable to that of HASS. Given the significant acceleration benefits brought by GRIFFIN, the additional training overhead is acceptable. %\zp{please discuss your one to two limitations. please do not give  fatal limitations.}


\section*{Impact Statement}
This paper presents work whose goal is to advance the field of LLM acceleration. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{griffin.bib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
