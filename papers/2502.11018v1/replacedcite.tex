\section{Related Work}
Speculative decoding____ accelerates LLM inference by dividing each decoding step into a draft stage and a verification stage. Existing methods differ primarily in their draft model architectures or strategies, each addressing specific challenges in speculative decoding.  

Several approaches focus on retrieving candidate predictions from similar contexts, such as PLD____, Lookahead____, and CLLMs____, which rely on prompt-based context retrieval. However, their dependence on contextual similarity limits generalization to tasks with insufficient or unavailable context. Tree-based verification mechanisms, such as Sequoia____ and SpecExec____, improve verification via hierarchical structures but introduce computational overhead, making them less suitable for latency-critical tasks.
Other works leverage databases or prior outputs for drafting, such as REST____ and Ouroboros____, which reuse previous outputs to improve draft coherence. However, these methods are constrained by the quality and availability of external resources. Chimera____ and Glide____ integrate the target model into the draft model to improve token quality, but this increases computational costs.  

Lightweight draft models have also been explored to improve efficiency. Medusa____ employs MLPs for parallel candidate prediction, while Hydra____ and Recurrent Drafter____ use RNN-based models for regressive generation. EAGLE____ and EAGLE2____ introduces a transformer decoder for autoregression over feature sequences, balancing accuracy and complexity. FSPAD____ constructs input sequences tailored for lightweight draft model predictions and introduces specialized training methods to improve draft quality. Additionally, methods like HASS____ address feature misalignment during training and decoding but do not fully resolve token-level misalignment.  
In contrast, this work focuses on addressing token misalignment, a critical challenge in speculative decoding. We propose the GRIFFIN framework, which introduces a token-alignable training strategy and a token-alignable draft model. By tackling this issue, GRIFFIN improves both acceptance length and speedup ratio, offering a complementary perspective to existing methods.