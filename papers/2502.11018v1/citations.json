[
  {
    "index": 0,
    "papers": [
      {
        "key": "sun2024spectr",
        "author": "Sun, Ziteng and Suresh, Ananda Theertha and Ro, Jae Hun and Beirami, Ahmad and Jain, Himanshu and Yu, Felix",
        "title": "Spectr: Fast speculative decoding via optimal transport"
      },
      {
        "key": "miao2024specinfer",
        "author": "Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others",
        "title": "Specinfer: Accelerating large language model serving with tree-based speculative inference and verification"
      },
      {
        "key": "chen2023cascade",
        "author": "Chen, Ziyi and Yang, Xiaocong and Lin, Jiacheng and Sun, Chenkai and Chang, Kevin Chen-Chuan and Huang, Jie",
        "title": "Cascade speculative drafting for even faster llm inference"
      },
      {
        "key": "kim2024speculative",
        "author": "Kim, Sehoon and Mangalam, Karttikeya and Moon, Suhong and Malik, Jitendra and Mahoney, Michael W and Gholami, Amir and Keutzer, Kurt",
        "title": "Speculative decoding with big little decoder"
      },
      {
        "key": "liu2023online",
        "author": "Liu, Xiaoxuan and Hu, Lanxiang and Bailis, Peter and Cheung, Alvin and Deng, Zhijie and Stoica, Ion and Zhang, Hao",
        "title": "Online speculative decoding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "saxena2023prompt",
        "author": "Apoorv Saxena",
        "title": "Prompt Lookup Decoding"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "fu2024break",
        "author": "Fu, Yichao and Bailis, Peter and Stoica, Ion and Zhang, Hao",
        "title": "Break the sequential dependency of llm inference using lookahead decoding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kou2024cllms",
        "author": "Kou, Siqi and Hu, Lanxiang and He, Zhezhi and Deng, Zhijie and Zhang, Hao",
        "title": "Cllms: Consistency large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2024sequoia",
        "author": "Chen, Zhuoming and May, Avner and Svirschevski, Ruslan and Huang, Yuhsun and Ryabinin, Max and Jia, Zhihao and Chen, Beidi",
        "title": "Sequoia: Scalable, robust, and hardware-aware speculative decoding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "svirschevski2024specexec",
        "author": "Svirschevski, Ruslan and May, Avner and Chen, Zhuoming and Chen, Beidi and Jia, Zhihao and Ryabinin, Max",
        "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "he2023rest",
        "author": "He, Zhenyu and Zhong, Zexuan and Cai, Tianle and Lee, Jason D and He, Di",
        "title": "Rest: Retrieval-based speculative decoding"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhao2024ouroboros",
        "author": "Zhao, Weilin and Huang, Yuxiang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan and Sun, Maosong",
        "title": "Ouroboros: Speculative Decoding with Large Model Enhanced Drafting"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zeng2024chimera",
        "author": "Zeng, Ziqian and Yu, Jiahong and Pang, Qianshi and Wang, Zihao and Zhuang, Huiping and Shao, Hongen and Zou, Xiaofeng",
        "title": "Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "du2024glide",
        "author": "Du, Cunxiao and Jiang, Jing and Yuanchen, Xu and Wu, Jiawei and Yu, Sicheng and Li, Yongqi and Li, Shenggui and Xu, Kai and Nie, Liqiang and Tu, Zhaopeng and others",
        "title": "Glide with a cape: A low-hassle method to accelerate speculative decoding"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "cai2024medusa",
        "author": "Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri",
        "title": "Medusa: Simple llm inference acceleration framework with multiple decoding heads"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ankner2024hydra",
        "author": "Ankner, Zachary and Parthasarathy, Rishab and Nrusimha, Aniruddha and Rinard, Christopher and Ragan-Kelley, Jonathan and Brandon, William",
        "title": "Hydra: Sequentially-dependent draft heads for medusa decoding"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "cheng2024recurrent",
        "author": "Cheng, Yunfei and Zhang, Aonan and Zhang, Xuanyu and Wang, Chong and Wang, Yi",
        "title": "Recurrent drafter for fast speculative decoding in large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "li2024eagle",
        "author": "Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang",
        "title": "Eagle: Speculative sampling requires rethinking feature uncertainty"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2024eagle2",
        "author": "Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang",
        "title": "Eagle-2: Faster inference of language models with dynamic draft trees"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "gui2024boosting",
        "author": "Gui, Lujun and Xiao, Bin and Su, Lei and Chen, Weipeng",
        "title": "Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment Distillation"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zhang2024learning",
        "author": "Zhang, Lefan and Wang, Xiaodan and Huang, Yanhua and Xu, Ruiwen",
        "title": "Learning Harmonized Representations for Speculative Sampling"
      }
    ]
  }
]