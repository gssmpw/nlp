%\newpage
\section*{\Large Appendix}

\section{Proofs}

    We will now derive identifiability guarantees for the global attribution map under the model described in the main paper. Given a data generating process and a ground truth global attribution map of the data generating process, we aim for a guarantee of the form
    \begin{equation}
        \hat{\JJ}_\gg  = \JJ_\gg \odot \LL
    \end{equation}
    for a suitable estimator $\hat{\JJ}_\gg$ up to a matrix $\LL$ that scales the ground truth derivatives in $\JJ_\gg$ point-wise and will hence not affect the ``real zeros'' in the Jacobians relevant for Def.~\ref{def:gt-attribution}.

    We use contrastive learning to obtain a feature encoder $\ff$ which identifies the ground-truth latents up to a linear indeterminacy. We structure this feature encoder to reconstruct different parts of the latent representation in different dimensions of the reconstructed latent space.

    Then, we estimate the attribution map by computing the pseudo-inverse of the feature encoder's Jacobian, which is directly related to the Jacobian of the mixing function.
    To obtain the correct pseudo-inverse, we need to obtain a minimum-Jacobian solution of the feature encoding network. We hence introduce a new regularized contrastive learning objective. 

    The underlying constrained optimization problem is:
    \begin{equation}
        \min_\ff \| \JJ_\ff(\xx) \|_F^2
        \quad
        \text{ s.t. }
        \quad
        \phi_i(\ff_i(\xx), \ff_i(\yy)) = \log \frac{p_i(\yy | \xx)}{q(\yy | \xx)} + C_i(\xx) \quad \forall i \in [G],
        \label{eq:exact-objective}
    \end{equation}
    with the positive sample distribution $p_i$ and the negative sample distribution $q$. We call $(\xx, \yy_+)$ the positive pair, and all $(\xx, \yy^-_i)$ negative pairs. 
    In the following we define $\psi_i(\xx, \yy) := \phi_i(\ff_i(\xx), \ff_i(\yy))$ where $\ff := [\ff_1; \dots; \ff_G]$ is the feature encoder and $\phi_i$ are similarity metrics.
    We re-state the regularized contrastive learning objective function which is a relaxation of Eq.~\ref{eq:exact-objective}:
    \begin{equation}
        \mathcal L_N[\psi; \lambda] = \mathop{\mathbb{E}}\limits_{{\substack{
            \xx \sim p(\xx),\\ 
            \yy^+ \sim p_i(\yy|\xx) \ \forall i\in[G]\\
            \yy^-_1\dots\yy^-_N \sim q(\yy|\xx)\\
        }}}
        \hspace{-.25em}
        \left[ \sum_{i=1}^{G} \Big( - \psi_i(\xx, \yy^+_i)
        +  \log \sum_{j=1}^{N} e^{\psi_i(\xx, \yy^-_j)} \Big)
        + 
        \lambda \| \JJ_\ff(\xx) \|_F^2
        \right].
    \end{equation}
    
    In principle, this objective is able to identify an arbitrary amount of separate factor groups ($G$), given sufficient capacity of the model.
    The choice of $\psi_i$ for the individual parts of the feature representation depends on the exact distribution underlying data generation, and is discussed below.

\subsection{Preliminaries}
\label{label:prelims-part2}
    Before proving our results on identifiable attribution maps, it is useful to restate a few known results from the literature, concerning properties of the InfoNCE loss.
    \citet{hyvarinen2019nonlinear} showed that contrastive learning with auxiliary variables is identifiable up to permutations or linear transformations for conditionally exponential distributions. \citet{zimmermann2021contrastive} related this to identifiability for models trained with the InfoNCE loss, and showed that assumptions about the data-generating process can be incorporated into the choice of loss function.
    \citet{schneider2023cebra} then formulated a supervised contrastive learning objective based on selecting the positive and negative distributions in the generalized InfoNCE objective.

    We will first re-state the minimizer of the InfoNCE loss (Def.~\ref{def:infonce}) used in our algorithm:
   \begin{proposition}[restated from \citet{schneider2023cebra}]\label{prop:infonce-minimizer}
    Let $p(\cdot | \cdot)$ be the conditional distribution of the positive samples, $q(\cdot | \cdot)$ the conditional distribution of the negative samples and $p(\cdot)$ the marginal distribution of the reference samples. 
    The generalized InfoNCE objective (Def.~\ref{def:infonce}) is convex in $\psi$ with the unique minimizer
    \begin{equation}
        \psi^*(\xx,\yy) = \log{\frac{p(\yy | \xx )}{q(\yy | \xx)}} + C(\xx), \quad \text{with} \quad
        \mathcal{L}_N[\psi^*] = \log N - \DKL(p(\cdot|\cdot) \| q(\cdot|\cdot))
    \end{equation}
    for $N \rightarrow \infty$ on the support of $p(\xx)$, where $C: \Ru \to \R$ is an arbitrary mapping. 
    \begin{proof}
            See \cite{schneider2023cebra}, but note that we added the batch size $N$.
        \end{proof}
    \end{proposition}

    We also re-state:
    \begin{proposition}[restated from Proposition~6 in \citet{schneider2023cebra}]\label{prop:discovery-driven-identifiability}
        Assume the learning setup in Def.~1 \citep{schneider2023cebra}, and that the ground-truth latents $\uu_1,\dots,\uu_T$ for each time point follow a uniform marginal distribution and the change between subsequent time steps is given by the conditional distribution of the form
        \begin{equation}
            p(\uu_{t + \Delta t} | \uu_{t}) = \frac{1}{Z(\uu_{t})}  \exp \delta (\uu_{t + \Delta t}, \uu_{t})
        \end{equation}
        where $\delta$ is either a (scaled) dot product (and $\uu_t \in \mathcal{S}^{n-1} \subset \R^\dimu$ lies on the $(n-1)$-sphere $\mathcal{S}^{n-1}$) or an arbitrary semi-metric (and $\uu_t \in \mathcal{U} \subset \Ru$ lies in a convex body $\mathcal{U}$).
        Assume that the data generating process $\gg$ with $\mathbf{s}_t = \gg(\uu_t)$ is injective.
        Assume we train a symmetric CEBRA \citep{schneider2023cebra} model with encoder $\ffx=\ffy$ and the similarity measure including a fixed temperature $\tau > 0$ is set to or sufficiently flexible such that $\phi = \delta$ for all arguments.
        Then $\hh = \hh' = \gg \circ \ff$ is affine.
        \begin{proof}
            For $\delta$ being the dot product, the result follows from the proof of Theorem 2 in \citet{zimmermann2021contrastive}.
            For $\delta$ being a semi-metric, the result follows from the proof of Theorem 5 in \citet{zimmermann2021contrastive}. 
        \end{proof}
    \end{proposition} 

\subsection{Positive distributions for self-supervised and supervised contrastive learning}\label{app:positive-distributions}

    \paragraph{Self-supervised contrastive learning}

        Up to one of the parts in the latent representation $\zz$ can be estimated using self-supervised learning by leveraging time information in the signal. The underlying assumption is that latents vary over time according to a distribution we can model with $\psi$.
        For instance, Brownian motion $p(\zz^{(t+1)} | \zz^{(t)}) = \mathcal{N}(\zz^{(t+1)} - \zz^{(t)} | 0, \sigma^2 \mathbf{I})$ can be estimated by selecting $\phi(\xx, \yy) = -\|\xx - \yy \|^2$. On the hypersphere with a vMF conditional across timesteps, the dot product is a suitable choice for $\phi(\xx, \yy) = \xx^\top \yy$. 
        Due to Proposition~\ref{prop:discovery-driven-identifiability}, this training scheme is able to identify the ground truth latents up to a linear indeterminacy.

    
    \paragraph{Supervised contrastive learning}

        For supervised contrastive learning, we uniformly sample a timestep (and hence, a sample $\xx$) from the dataset. This timestep is associated to the label $\cc$, and we then sample $\cc'$ from the conditional distribution $p(\cc' | \cc)$. We select the nearest neighbour to $\cc'$ with the corresponding sample $\xx'$.

        The conditional distribution $p(\cc' | \cc)$ can be constructed as an \emph{empirical} distribution: For instance, if we assume non-stationarity, $\cc^{(t)} - \cc^{(t-1)}$ can be computed across the dataset. Let us call this distribution $\hat{p}(\cc' - \cc)$. Then, sampling from $p(\cc' | \cc)$ can take the form of sampling $\cc' = \cc + \Delta$ with $\Delta \sim \hat{p}(\cc' - \cc)$.

        If this approximation is correct under the underlying latent distribution, have we have $p(\cc' | \cc) \det \JJ_\gamma^{-1}(\cc') = p(\zz' | \zz)$. This means that the solutions of the supervised and self-supervised contrastive learning solutions coincide.

    \paragraph{Superposition of self-supervised and supervised contrastive learning}

        Depending on the assumptions about the ground truth data distribution, different estimation schemes can be combined to obtain a latent representation. In the end, the feature encoder $\ff$ should identify the original latents $\zz$ up to a linear transformation,
        \begin{equation}
            \ff(\gg(\zz)) = \LL \zz.
        \end{equation}
        Our goal is to obtain block-structure in $\LL$, with zeros in the lower block triangular part of the matrix.
        
        This is possible by simultaneously solving multiple contrastive learning objectives, which requires
        \begin{equation}
            \ff_i(\gg(\zz)) = \LL_i \zz.
        \end{equation}
        for each part $i$ of the latent representation.
        Assume without loss of generality that we apply self-supervised contrastive learning to the $G$-th part, and supervised contrastive learning to all remaining parts.
        For supervised contrastive learning we then obtain
        \begin{equation}
            \ff_i(\gg(\zz)) = \LL_i \zz = \LL'_i \zz_i.
        \end{equation}
        \emph{If} all latents $\zz$ satisfy the conditions for time-contrastive learning, we can then also apply time-contrastive learning to the full representation, which gives us the following constraints:
        \begin{align}
            \ff_i(\gg(\zz)) &= \LL_i \zz = \LL'_i \zz_i \quad \forall i\in[G-1] \\
            \ff(\gg(\zz)) &= \LL \zz
        \end{align}
        from which we can follow the matrix structure
        \begin{equation}
            \ff(\gg(\zz)) = \text{diag}(\LL_1, \dots, \LL_G)
        \end{equation}

        In cases where this is not possible, note that it is always possible to treat all contrastive learning problems separately, and learn separate regions of the feature space in $\ff$. This gives the same result, but re-uses less of the representation (e.g., the self-supervised part of the representation would be learned separately from the supervised part).

        Consider a time-series dataset where $p(\zz_t | \zz_{t-1})$, i.e., all latents, follow Brownian motion.
        We can then produce the solution
        \newcommand{\muteddetjac}{\textcolor{gray}{|\JJ^{-1}_{\gamma_i}(\zz'_i)|}}
        \begin{align}
            \psi_i(\xx, \xx') :=& \phi_i(\ff_i(\xx), \ff_i(\xx')) = \log \frac{p(\cc_i' | \cc_i)}{q(\cc_i' | \cc_i)} \quad i \in \{1,\dots,G-1\} \\
            \psi_G(\xx, \xx') :=& \sum_{i=1}^G \phi_i(\ff_i(\xx), \ff_i(\xx'))  
                = \log \frac{p(\zz' | \zz)}{q(\zz' | \zz)} 
                = \log \frac{p(\zz_G' | \zz_G)}{q(\zz_G' | \zz_G)} 
                   + \sum_{i=1}^{G-1} \log \frac{p(\cc'_i | \cc_i)\muteddetjac }{q(\cc'_i | \cc_i) \muteddetjac}
        \end{align}
        in case our training distributions for supervised contrastive learning, $p(\cc_i | \cc_i)$ are a sufficiently good approximation of the variation in the ground truth latents, we can select $\psi_G(\xx, \yy) := \phi(\ff(\xx), \ff(\yy))$ to be trained on the whole feature space using self-supervised learning, while all other objectives on $\psi_i$ would solve supervised contrastive losses. If this training setup is not possible, it would be required to parametrize $\psi_G(\xx, \yy) := \phi(\ff(\xx), \ff(\yy))$ as a separate part of the feature space.
    
        While it is beyond the scope of the current work to thoroughly investigate the trade-offs between the two methods, our verification experiments assume the former case: The time contrastive objective is applied to the whole objective function, and the behavior contrastive objective to the previous latent factors.


\subsection{Proof of Theorem~\ref{thm:overfitting}}\label{sec:proof-prop-overfitting}

    An interesting property of contrastive learning algorithms is the natural definition of a ``goodness of fit'' metric for the model. This goodness of fit can be derived from the value of the InfoNCE metric which is bounded from below and above as follows \citep{schneider2023cebra}:
    \begin{equation}
       \log N - D_\text{KL}(p || q) \le \mathcal{L}_N[\psi] \le \log N.
    \end{equation}

    In scientific applications, we can leverage the distance to the trivial solution $\log N$ as a quality measure for the model fit.
    Theorem~\ref{thm:overfitting} states that if during supervised contrastive learning with labels $\cc$ there is no meaningful relation between $\cc$ and $\xx$, we will observe a trivial solution with loss value at $\log N$.

    For the following proof, let us recall from Def.~\ref{def:data-generator} that we can split the latents $\zz$ that fully define the data through the mixing function, $\xx = \gg(\zz)$. We can split $\zz$ into different parts, $\zz = [\zz_1, \dots, \zz_G]$ and assume that $\cc_i$ is the observable factor corresponding to the $i$-th part. For notational brevity, we omit the $i$ in the following formulation of the proof without loss of generality.
    
    \paragraph{Proof of Theorem~\ref{thm:overfitting}}

    \begin{proof}
    Assume that the distribution $p$ is informed by labels.
    In the most general case, we can depict the sampling scheme for supervised contrastive learning with continuous labels $\cc$ and $\cc'$ and latents $\zz$ and $\zz'$ with the following graphical model:
    
    \begin{center}
    \begin{tikzpicture}[scale=1.5]
        \node[latent] (x) at (0,0) {$\zz$};
        \node[latent] (y) at (1,0) {$\zz'$};
        \node[obs] (u) at (0,1) {$\cc$};
        \node[obs] (v) at (1,1) {$\cc'$};
        \edge {x} {u};
        \edge {u} {v};
        \edge {v} {y};
    \end{tikzpicture}
    \end{center}

    The reference sample $\xx$ is linked to the observable factor/label $\cc$, and the conditional $p(\cc' | \cc)$ links both samples. In particular, $\zz'$ and hence $\xx'$ are selected based on $\cc'$ in the dataset.

    The distributions for positive and negative samples then factorize into
    \begin{align}
        p(\zz' | \zz) = \int\int d\cc' d\cc p(\zz' | \cc') p(\cc' | \cc) p(\cc | \zz)\\
        q(\zz' | \zz) = \int\int d\cc' d\cc p(\zz' | \cc') q(\cc' | \cc) p(\cc | \zz)
    \end{align}
    and note that only $p(\cc' | \cc)$ and $q(\cc' | \cc)$ are selected by the user of the algorithm, the remaining distributions are empirical properties of the dataset.

    We can compute the density ratio
    \begin{align}
    \frac{p(\zz' | \zz)}{q(\zz' | \zz)} &= \frac{\int\int d\cc' d\cc p(\zz' | \cc') p(\cc' | \cc) p(\cc | \zz)}%
                          {\int\int d\cc' d\cc p(\zz' | \cc') q(\cc' | \cc) p(\cc | \zz)}\\
        \intertext{%
            In the case where latents and observables are independent variables, we have $p(\zz' | \cc') = p(\zz')$ and $p(\cc | \zz) = p(\cc)$. The equation then reduces to
        }
        &= \frac{\int\int d\cc' d\cc p(\zz') p(\cc' | \cc) p(\cc)}%
               {\int\int d\cc' d\cc p(\zz') q(\cc' | \cc) p(\cc)} \\
        &= \frac{p(\zz') \int\int d\cc' d\cc p(\cc' | \cc) p(\cc)}%
               {p(\zz') \int\int d\cc' d\cc q(\cc' | \cc) p(\cc)} = 1.
    \end{align}
    Consequently, the minimizer is $\psi(\xx, \yy) = C(\xx)$ and we obtain the maximum value of the loss with $\mathcal{L}[\psi] = \log N$ in the limit of $N \rightarrow \infty$.
    Note, for any symmetrically parametrized similarity metric (like the cosine or Euclidean loss), it follows that $\psi(\xx, \yy) = \psi$ is constant, i.e., the function collapses onto a single point.

\end{proof}

\subsection{Proof of Theorem~\ref{thm:main-theorem}}\label{sec:proof-main-theorem}


        \begin{proof}
            For the first part of the proof, we invoke Proposition 2. For training multiple encoders, for each latent factor $\zz_i$ and the corresponding part of the feature encoder $\ff_i$, we obtain at the minimizer of the contrastive loss, 
            \begin{equation}
                \forall i \in [G]: \quad \ff_i(\gg(\zz)) = \LL_i \zz_i.
            \end{equation}
            Assume without loss of generality that we apply self-supervised contrastive learning to the $G$-th part, and supervised contrastive learning to all remaining parts.
            For supervised contrastive learning we then obtain
            \begin{equation}
                \ff_i(\gg(\zz)) = \LL_i \zz = \LL'_i \zz_i.
            \end{equation}
            \emph{If} all latents $\zz$ satisfy the conditions for time-contrastive learning, we can then also apply time-contrastive learning to the full representation, which gives us the following constraints:
            \begin{align}
                \ff_i(\gg(\zz)) &= \LL_i \zz = \LL'_i \zz_i \quad \forall i\in[G-1] \\
                \ff(\gg(\zz)) &= \LL \zz
            \end{align}
            from which we can follow the matrix structure
            \begin{equation}
                \ff(\gg(\zz)) = \text{diag}(\LL_1, \dots, \LL_G)
            \end{equation}
            In cases where this is not possible, note that it is always possible to treat all contrastive learning problems separately. We then still get a block diagonal structure because all latents are independent, and no mapping can exist between separate latent spaces.
            Hence, if $\ff$ is a minimizer of the InfoNCE loss under the assumed generative model, it follows that we part-wise identify the underlying latents,
            \begin{equation}
                \ff(\gg(\zz)) = \BB \zz
            \end{equation}
            with some block diagonal matrix $\BB$.
            
            By taking the derivative w.r.t. $\zz$ it follows that
            \begin{align}
               \JJ_\ff(\xx) \JJ_\gg(\zz) &= \BB. \\
               \intertext{We need to show that at each point $\zz$ in the factor space, we can recover $\JJ_g$ up to some indeterminacy.
                    We will re-arrange the equation to obtain
               }
               \JJ_\ff(\xx) \JJ_\gg(\zz) \BB^{-1} &= \mathbf{I}, \\
               \JJ_\ff(\xx) \tilde{\JJ}_\gg(\zz) &= \mathbf{I}.
            \end{align}
            It is clear that for each point in the support of $p$, $\JJ_\ff(\xx)$ is a left inverse of $\tilde{\JJ}_\gg(\zz)$. 
            \begin{equation}
                \JJ_\ff(\xx) = \tilde{\JJ}^+_\gg(\zz) + \VV, \vv_{i} \in \ker \tilde{\JJ}_\gg(\zz)
                \label{eq:jf-jg-v}
            \end{equation}
            Among these solutions, it is well-known that the minimum norm solution $\JJ^*$ to
            \begin{equation}
                \min_{\JJ(\zz)} \| \JJ(\zz) \|_F^2 \text{ s.t. } \JJ(\zz) \JJ_\gg(\zz)= \mathbf{I}
            \end{equation}
            is the Moore-Penrose inverse, $\JJ^*(\zz) = \tilde{\JJ}^+_\gg(\zz)$. By invoking assumption (2), we arrive at this solution and have
            \begin{align}
                \JJ_\ff(\xx) &= \tilde{\JJ}^+_\gg(\zz) \\
                \JJ^+_\ff(\xx) &= \tilde{\JJ}_\gg(\zz) \\
                \JJ^+_\ff(\xx) &= \JJ_\gg(\zz) \BB^{-1}
            \end{align}
            Because $\BB$ is block-diagonal with zeros in the off-diagonal blocks, this also applies to $\BB^{-1}$.
            It follows that
            %Depending on the choice of training scheme, $\BB$ can have different forms:
            %    If all parts of $\zz$ are inferred in separate groups, $\BB$ will be block-diagonal with zeros in all off-diagonal blocks.
            %    If the latent part of $\zz$ is inferred on the full embedding, and all observable parts are inferred on the the respective parts, $\BB$ will have block structure with its lower triangular block set to 0.
            %    In both cases, the inverse of $\BB$ will have its zeros as the same location as $\BB$, and hence multiplying by $\BB^{-1}$ hence does not affect the locations of the zeros in $\JJ_\gg$, and hence
            \begin{equation}
                \JJ^+_\ff(\xx) = \JJ^+_\ff(\gg(\zz)) \propto \JJ_\gg(\zz)
            \end{equation}
            concluding the proof.
        \end{proof}


\section{Detailed experimental methods}

    \subsection{Synthetic finite time-series data design}
    \label{app:synthetic-data}
    
        \begin{figure*}[t]
            \centering
            \vspace{-6pt}
            \includegraphics[width=\textwidth]{Suppl_gdp.png}
            \caption{\textbf{Synthetic Data Generation Process.} We generate two sets of latent variables, $z_1$ and $z_2$, each consisting of 100,000 samples drawn from Brownian motion within a box $[-1,1]^d$. In this example, $z_1$ is connected to both $x_1$ and $x_2$, while $z_2$ is connected only to $x_2$. Additionally, we use an injective mixing function consisting of $g_1$ and $g_2$. Function $g_1$ takes 3 (denoted $d_1$) latent variables as input and outputs 25 neurons (denoted $n_1$), whereas $g_2$ takes 6 ($d_1+d2$) latent variables as input and outputs 25 neurons (denoted $n_2$). The final data $x$ is constructed by concatenating $x_1$ and $x_2$, resulting in a data matrix $x$ with a shape of 100,000 by 50.}
            \label{fig:synthetic}
        \end{figure*}
        

        We sample 10 different datasets with 100,000 samples, each with a different mixing function $\gg$.
        All latents of the dataset are chosen to lie within the box $[-1, 1]^D$.
        We sample the dataset by selecting $\zz_1$ from a uniform distribution over $[-1, 1]^D$.
        The following time steps are generated by Brownian motion, $\zz_{t} = \mathcal{N}_{[-1, 1]}(\zz_{t-1}, \sigma^2 I)$ where $\mathcal{N}_{[-1, 1]}$ is a truncated normal distribution clipped to the bounds of the box. All other latent factors are sampled accordingly. The process is outlined in Figure~\ref{fig:synthetic}.


        Similar to \citet{schneider2023cebra}, the feature encoder $\ff$ is an MLP with three layers followed by GELU activations \citep{hendrycks2016gaussian}, and one layer followed by a scaled $\tanh$ to decode the latents. We train on batches with 5,000 samples each. The first 2,500 training steps minimize the InfoNCE or supervised loss with $\lambda=0$; we then ramp up $\lambda$ to its maximum value over the following 2,500 steps, and continue to train until 20,000 total steps.
        % 
        We compute the $R^2$ for predicting the auxiliary variable $\cc$ from the feature space after a linear regression, and ensure that this metric is close to $100\%$ for both our baseline and contrastive learning models to remove performance as a potential confounder.
        %
  

        To compare to previous works, we vary the training method (hybrid contrastive, supervised contrastive, standard supervised) and consider baseline methods for estimating the attribution maps (Neuron gradients~\citep{Simonyan2013DeepIC}, Integrated gradients~\citep{shrikumar2018computationally,Sundararajan2017AxiomaticAF}, Shapley values~\citep{shapley1953value,lundberg2017unified}, and Feature ablation \citep{molnar2022}), which are commonly used algorithms in scientific applications \citep{samek2019explainable,molnar2022}. To compute these attribution maps, we leveraged the open source library Captum \citep{kokhlikyan2020captum}. We also compare regularized and non-regularized training. Hyperparameters are identical between training setups, the regularizer $\lambda$, and number of training steps are informed by the training dynamics.

        We evaluate the identification of the attribution map at different decision thresholds $\epsilon$ similar to a binary classification problem: namely, for each decision threshold, we binarize the inferred map, and compute the binary accuracy to the ground truth map. We compute the ROC curve as we vary the threshold for each method, and use area under ROC (auROC) as our main metric. In practice where a single threshold needs to be picked, we found z-scoring of the attribution score an effective way to set $\epsilon$ correspondig to a z-score of 0.

        In our synthetic experiments, we consider variations of three model properties. Our theory predicts that the combination of estimating the inverse of the feature encoder Jacobian with regularized training allows us to identify the ground truth attribution map. We test the following, and underline our proposed methods: \textbf{Training mode:} Supervised, Supervised contrastive, \underline{Hybrid contrastive}.
        \textbf{Regularization:}  Off ($\lambda = 0$), \underline{On ($\lambda = 0.1$)}.
        \textbf{Attribution map estimation:} Feature ablation, Shapley values (zeros, shuffles), Integrated gradients, Neuron gradient, \underline{Inverted neuron gradient}.
        Our theory predicts that any deviation from the underlined settings will yield a drop in AUC score (empirical identifiability of the attribution map).
        We validated this claim by running all combinations with 10 seeds (i.e., different latents \& mixing functions) across different numbers of latent dimensions and ran a statistical analysis to test the influence of the different factors.

  
        

\subsection{Simulated (RatInABox) neural data.}
    \label{app:ratinabox}
    
    As an application to a neuroscientific use case, we generate synthetic neural data during navigation using RatInABox \citep{george2022ratinabox}, a toolbox that simulates spatial trajectories and neural firing patterns of an agent in an enclosed environment. We generate a trajectory with a duration of 2000 seconds and sample every $\delta t=0.1s$, resulting in 20000 time steps. We use the default environment and simulate place, two modules of grid, head direction, and speed cells (n=100 neurons each, 400 neurons in total). Place cells are modeled as a difference of Gaussians with width=0.2m; grid cells are modeled as three rectified cosines with two grid modules with module scales set to 0.3 and 0.4; for all other cells, we use the RatInABox default values. As all neurons within RatInABox are rate-based we use the firing rate of the cells for all subsequent analysis. For all cells we then calculate the spatial information criteria $SI = \sum_{i} P_i \frac{r_i}{\bar{r}} \log_2 \left( \frac{r_i}{\bar{r}} \right)$
    where \( P_i \) is the probability of the stimulus being in the \(i^{th}\) spatial bin, \( r_i \) is the estimated firing rate in the \(i^{th}\) spatial bin and \( \bar{r} \) is the overall average estimated firing rate \citep{skaggs1996theta}. 

    To calculate the grid scores we used the method described by \citet{sargolini2006conjunctive}. Briefly, we first calculate ratemaps for each cell, which we use to calculate Spatial Auto-Correlograms (SAC). We then rotate the SAC at multiple angles and determine the correlation coefficients in comparison with the unaltered SAC. The highest correlation score obtained at rotations of 30, 90, and 150 degrees is deducted from the lowest score observed at 60, 90, and 120 degrees rotation. This value is denoted as the grid score. 

    The purpose of this dataset is to model properties of real place, grid, head direction, and speed cells. Due to the simulation environment, at least three properties (position, speed, and head direction) are encoded by these neurons, and represented in the ground truth latents. Speed information is incorporated only in speed cells, head direction information only in head direction cells, and position information is coded by both position and grid cells, by design. We design the attribution map accordingly (Appendix Figure~\ref{fig:synth-gt-graph}) --- for models trained with position information, we would expect to discover grid and place cells, but not the other types.
    
\begin{figure}[th]
    \centering
    \includegraphics[width=\textwidth]{Suppl_dimensionality.pdf}
    \caption{\textbf{Correct Dimensionality of regularized contrastive learning models.} Panel a, b and c show the consistency scores for time contrastive (a), supervised contrastive (b) and hybrid contrastive (c) respectively. We see that the optimal consistency is 5-6 (panels a, b) and (3,2), (2,3) and (2,2) in panel c. Panels d and e show the AUC scores for behavior and hybrid contrastive. We see that consistency scores and AUC scores are highly correlated.
    }
    \label{fig:dimensionality}
\end{figure}

\section{Additional Experimental Results}

\paragraph{Uncovering the Correct Dimensionality in regularized contrastive learning.} We conducted experiments aimed at identifying the correct dimensionality in our regularized contrastive learning algorithm, \raisebox{-0.3ex}{\scriptsize{x}}CEBRA. The experimental setup follows the procedure detailed in Appendix~\ref{app:synthetic-data}, where the true dimensionality is 6D (3D+3D). Instead of also fixing the dimensionality of our model to 6D, we vary the model dimensionality from 2D to 10D. We run time contrastive, supervised contrastive, and hybrid contrastive models, each with 10 independent seeds.

The selection protocol uses the \emph{consistency} of models across different runs. If the model dimensionality is larger than the true underlying data dimensionality, the identifiability guarantee does not hold, and the model behavior is not clearly defined. We compare the $R^2$ value between embeddings derived from two model seeds after affine alignment. Note, this metric does not require access to the ground truth latents, and can also be computed in practice.

We first consider the time-contrastive case in Figure \ref{fig:dimensionality}(a), where we successively increase dimensionality and see an increase in consistency from 80-85\% (for 2D) to almost 100\% for 5D and 6D embeddings. Afterwards, performance drops, potentially due to overfitting effects as the embedding dimensionality gets too large.
For supervised contrastive training (b), we observe a similar effect with a drop in $R^2$ after 3D embeddings, which is again the correct dimensionality.
Finally, we combine both results for hybrid contrastive learning (c), where we repeat the experiment for all combinations of dimensionality for the time-contrastive and supervised contrastive part, and again see optimal solutions for (3D,2D), (2D,3D) and (2D,2D) embeddings.
Selecting the correct dimensionality accordingly yields high AUC for both the supervised contrastive (d) and hybrid contrastive (e) models, corroborating our results from the main paper.


\paragraph{Application to real neural data: grid cells.}

Grid cells~\citep{hafting2005microstructure} display a hexagonal firing pattern across the environment (Figure~\ref{fig:real_data}a) and the combined activity of several grid cells provides a powerful neural code to map space that scales exponentially in the number of neurons \citep{fiete2008grid,mathis2012resolution}. 
To quantify if a neuron is a grid cell, one uses the ``gridness'' score, which quantifies the six-fold rotational symmetry of the firing pattern 
\citep{sargolini2006conjunctive, brandon2011reduction}.

We aimed to see if our attribution method aligned with the field-norm grid score. We trained \raisebox{-0.3ex}{\scriptsize{x}}CEBRA (and baselines) with 2D position as the auxiliary variable and computed the attribution score over time. As a control, we shuffled the neurons.
%
We also provide the visualization of the learned latent embeddings (Figure~\ref{fig:real_data}c), which nicely shows the time-associated vs. auxiliary (position) associated latents.

%Lastly, we examined the stability of the score over time as it is not \textit{a priori} true that every grid cell in every time window would map position, but given the small(er) size of the area vs. the ratemap we postulated that indeed position decodability should be relatively stable (Figures~\ref{fig:real-dataoverview} and \ref{figure:across-time-omd}).

\begin{figure*}[th]
    \centering
    \includegraphics[width=\textwidth]{S2.pdf}
    %\renewcommand{\thefigure}{S2}
    \caption{\textbf{Real Neural data and behavior \citep{Gardner2022}}.
        (a) spiking of 128 grid cells with example ratemaps.
        (b) Bottom: behavioral trajectory over the 2D arena, and speed
            and heading of the rat. Red line in each panel denotes the
            same time step. 
        (c) Visualization of a converged embedding on the real grid cell dataset. The embedding space is jointly trained with behavioral information about animal position (first 4 dimensions, top) and additional time-varying latent information (the remaining 10 dimensions) with our regularized contrastive learning hybrid contrastive learning setting (\raisebox{-0.3ex}{\scriptsize{x}}CEBRA). The position information was decoded as indicated by cross-validated R2 score on held-out data. Training embedding is shown.
        (d) Attribution map across time \& Position Attribution vs. Grid Score. Scores are centered and standardized. Grid score vs. attribution score shows separation of the cell types.
    }
    \label{fig:real_data}
\end{figure*}


\begin{table*}[t]
    \centering
        \caption{%
        \textbf{Synthetic (RatInABox) neural data.} Experiment replicates are re-inits of models, mean plus 95\% CI is shown in relation to auROC to position (covers synthetic place and grid cells).
    }
    \small
    \label{tbl:summary-scd}
    \vspace{-5pt}
    \begin{tabular}{lllllll}
    \toprule
     & \multicolumn{2}{c}{supervised} & \multicolumn{2}{c}{supervised contrastive} & \multicolumn{2}{c}{hybrid contrastive} \\
     & none & regularized & none & regularized & none & regularized \\
    attribution method &  &  &  &  \textbf{(ours)} &  &  \textbf{(ours)}\\
    \midrule
    Feature Ablation & $45.8_{44.3}^{47.2}$ & $96.2_{95.3}^{97.0}$ & $96.1_{95.3}^{97.2}$ & $95.8_{90.7}^{98.6}$ & $86.5_{85.3}^{87.5}$ & $78.3_{70.5}^{82.6}$ \\
    Shapley, shuffled & $61.1_{59.1}^{63.1}$ & $94.6_{90.4}^{97.2}$ & $99.5_{99.2}^{99.8}$ & $100.0_{100.0}^{100.0}$ & $99.2_{99.0}^{99.4}$ & $99.2_{98.9}^{99.6}$ \\
    Shapley, zeros & $46.3_{44.9}^{47.7}$ & $62.8_{58.4}^{69.0}$ & $88.0_{86.3}^{89.7}$ & $97.7_{97.2}^{98.1}$ & $83.5_{82.3}^{84.9}$ & $85.8_{82.5}^{88.6}$ \\
    Integrated Gradients & $45.7_{44.5}^{47.1}$ & $62.2_{58.1}^{68.6}$ & $86.4_{84.4}^{88.1}$ & $96.3_{95.6}^{96.7}$ & $81.5_{80.0}^{83.3}$ & $84.7_{82.0}^{87.6}$ \\
    \midrule
    Neuron Gradient & $63.7_{61.8}^{65.6}$ & $100.0_{100.0}^{100.0}$ & $100.0_{100.0}^{100.0}$ & $100.0_{99.9}^{100.0}$ & $100.0_{99.9}^{100.0}$ & $99.0_{97.1}^{100.0}$ \\
    Inverted Neuron Gradient & $68.4_{66.6}^{70.0}$ & $100.0_{100.0}^{100.0}$ & $100.0_{100.0}^{100.0}$ & $100.0_{100.0}^{100.0}$ & $100.0_{99.9}^{100.0}$ & $99.9_{99.8}^{100.0}$ \\
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[ht]
    \centering
        \caption{\textbf{Timing information for attribution analysis on the RatInABox dataset}. Depicted are times in seconds, with 95\% CI for estimating the attribution map on an A5000 GPU.}
    \label{tbl:timing-scd}
    \small
    \begin{tabular}{lllllll}
    \toprule
 & \multicolumn{2}{r}{ supervised} & \multicolumn{2}{r}{ supervised contrastive} & \multicolumn{2}{r}{hybrid contrastive} \\
 & none & regularized & none & regularized & none & regularized \\
 attribution method &  &  &  &  \textbf{(ours)} &  &  \textbf{(ours)}\\
\midrule
Feature Ablation & $124.9_{52.8}^{268.9}$ & $125.5_{52.9}^{270.3}$ & $279.0_{126.8}^{552.6}$ & $213.1_{133.5}^{369.5}$ & $757.4_{460.2}^{1185.6}$ & $659.8_{568.3}^{739.7}$ \\
Shapley, shuffle & $6.7_{6.4}^{7.0}$ & $7.0_{6.4}^{7.8}$ & $17.8_{14.2}^{20.5}$ & $15.6_{12.4}^{18.0}$ & $38.6_{31.9}^{44.7}$ & $39.3_{29.1}^{47.9}$ \\
Shapley, zeros & $3.6_{2.5}^{4.8}$ & $5.1_{2.4}^{10.2}$ & $9.2_{7.4}^{11.5}$ & $8.0_{5.0}^{12.6}$ & $34.9_{23.7}^{49.9}$ & $31.5_{20.2}^{46.1}$ \\
Integrated Gradients & $19.2_{17.0}^{23.5}$ & $19.6_{17.0}^{24.4}$ & $67.1_{60.3}^{74.2}$ & $65.8_{52.8}^{76.5}$ & $212.3_{184.4}^{233.2}$ & $199.5_{155.1}^{225.0}$ \\
\midrule
Neuron Gradient & $2.9_{2.8}^{3.0}$ & $2.9_{2.8}^{3.0}$ & $9.0_{6.8}^{10.5}$ & $7.9_{5.8}^{9.9}$ & $23.2_{18.4}^{26.5}$ & $26.9_{19.0}^{33.7}$ \\
Inverted Neuron Gradient & $3.7_{3.6}^{3.8}$ & $3.7_{3.5}^{3.7}$ & $10.8_{8.0}^{12.7}$ & $9.6_{7.2}^{11.7}$ & $29.6_{23.5}^{34.1}$ & $32.5_{23.1}^{40.9}$ \\
\bottomrule
    \end{tabular} \\
    \end{table*}




 
 \begin{table}[ht]
    \small
    \centering
    \caption{\textbf{Timing information for the model training phase for RatInABox}. These are times in seconds (s) on an A5000 GPU.
    }
    \label{tbl:timing-compute}
\begin{tabular}{llll}
\toprule
\textbf{}              & {regularizer} & {output dim} & {time (s)} \\
\midrule
supervised             & none                 & 2                  & 142        \\
                       & regularized          & 2                  & 307        \\
supervised contrastive & none                 & 4                  & 458        \\
                       & regularized          & 4                  & 614        \\
hybrid contrastive     & none                 & 14                 & 657        \\
                       & regularized          & 14                 & 996 \\
\bottomrule
\end{tabular}
\end{table}




\begin{figure*}[b]
    %\vspace{-25pt}
    \centering
    \includegraphics[width=\textwidth]{synthetic.jpg}
    \vspace{-5pt}
    \caption{\textbf{Visualization of the synthetic data and learned embedding}.
    Left: First three dimensions of ground truth latent variables. Each dot denotes one sample in time, and we show 300 samples in total for clarity.
    Middle: First three dimensions of the data, after passing the latent variables through the mixing function $\gg$.
    Right: First three dimensions of the recovered latents after linear alignment to the ground truth space.
    }
    \label{figure:synthetic-data}
\end{figure*}

\begin{figure*}
\centering
\begin{tikzpicture}

    \node[latent] (h_t) at (2.5,-1) {$h(t)$};
    \node[latent] (theta_t) at (1,-1) {$\theta(t)$};
    \node[latent] (v_t) at (1, -4) {$\vv(t)$};
    \node[obs] (x_t) at (1, -6) {$\xx(t)$};
    \node[latent] (v2d) at (2.5, -3) {$v(t)$};
    
    \node[latent] (nGC) at (2.5, -5) {$\bm{\epsilon}(t)$};
    \node[latent] (nPC) at (2.5, -7) {$\bm{\eta}(t)$};


    \node[factor, above = .5 of theta_t] (n) [label=$$] {n};
    \node[factor, above = .5 of v2d] (x) [label=$$] {};

    % cells
    \node[obs] (hdcell) at (4,-1) {HD};
    \node[obs] (speedcell) at (4,-3) {SC};
    \node[obs] (gridcell) at (4,-5) {GC};
    \node[obs] (placecell) at (4,-7) {PC};
    % \node[factor] [label=$\mathcal{N}$] {}; %

    \edge {nGC} {gridcell};
    \edge {nPC} {placecell};

    \edge {theta_t} {h_t};
    \edge {theta_t} {v_t};
    \edge {n} {theta_t};
    \edge {x} {v2d};
    \edge {v2d} {v_t} ;
    \edge {v2d} {speedcell};
    \edge[dashed] {v_t} {x_t};

    \draw[->,dashed] (x_t.90) arc (0:264:4mm) ;
    %\draw[->,dashed] (v_t.90) arc (0:264:4mm) ;
    \draw[->,dashed] (theta_t.90) arc (0:264:4mm) ;
    \draw[->,dashed] (h_t.90) arc (0:264:4mm) ;
    
    \edge {x_t} {gridcell};
    \edge {x_t} {placecell};
    \edge {h_t} {hdcell};
\end{tikzpicture}
\caption{%
    \textbf{Graphical model for the motion model in RatInABox} \citep{george2022ratinabox}, used to generate synthetic grid cell data.
    Direction $\theta(t)$ and speed $v(t)$ are derived from Ornstein-Uhlenbeck processes. Head direction $h(t)$ is computed by smoothing vectors derived from $\theta$ across time, and used to compute firing rates of head direction (HD) cells. Velocity in 2D $\vv(t)$ is computed from direction and speed. Speed is directly encoded in speed cells (SC). Velocity and past position information is used to calculate current position $\xx(t)$ by integrating, and position is used to compute firing rates of both grid-cells (GC) and place cells (PC). Dashed arrows denote connectivity across time, e.g., $\xx(t)$ depends on $\xx(t-1)$ and $\vv(t-1)$. In our experiments, we use $\xx(t)$ as the observable auxiliary behavior variable. $\bm{\epsilon}(t)$ and $\bm{\eta}(t)$ denote noise variables. Note that for simplicity, the diagram ignores handling of borders during trajectory simulation.
}
\label{fig:synth-gt-graph}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{scd_noise.pdf}
    \caption{\textbf{Performance on RatInABox dataset}. Across all training types we show the performance across six different noise levels ($\sigma$, Gaussian noise) for both our method (Inverted Neuron Gradient) and all other baselines. }
    \label{fig:noise}
\end{figure*}


\clearpage
\section{Statistical Analysis}\label{app:statistical_analysis}

Here we provide statistical tests for the auROC metric from Table~\ref{tbl:summary}. We fit an ANOVA on an ordinary least squares model using combinations of all latent factors, see Table~\ref{tbl:anova}.
As a post-hoc test, we use a Tukey HSD test on the statistically significant model properties.
See Table~\ref{tbl:attr-training} we show that hybrid contrastive learning computing followed by computing the pseudo-inverse significantly outperforms all other methods, and in Table~\ref{tab:attr-reg} we show that combining the pseudo-inverse on regularized trained models also significantly outperforms all other methods.
Statistical analysis is implemented using \texttt{statsmodels}\footnote{https://github.com/statsmodels/statsmodels/}.


\begin{table*}[ht]
\centering
\caption{Results for fitting an ANOVA on the auROC results for all combination of model properties.}
\label{tbl:anova}
\footnotesize
\centering
\begin{tabular}{lrrrr}
\toprule
 & sum sq & df & F & PR($>$F) \\
\midrule
C(attribution method name) & 807.50 & 5 & 6.14 & 0.00 \\
C(dim Z1) & 3286.82 & 5 & 24.98 & 0.00 \\
C(method name) & 1505.46 & 2 & 28.60 & 0.00 \\
C(extension) & 15722.40 & 1 & 597.37 & 0.00 \\
C(attribution method name):C(dim Z1) & 456.86 & 25 & 0.69 & 0.85 \\
C(attribution method name):C(method name) & 8747.26 & 10 & 33.24 & 0.00 \\
C(dim Z1):C(method name) & 270.05 & 10 & 1.03 & 0.41 \\
C(attribution method name):C(extension) & 6661.36 & 5 & 50.62 & 0.00 \\
C(dim Z1):C(extension) & 2647.68 & 5 & 20.12 & 0.00 \\
C(method name):C(extension) & 2813.94 & 2 & 53.46 & 0.00 \\
C(attribution method name):C(dim Z1):C(method name) & 463.75 & 50 & 0.35 & 1.00 \\
C(attribution method name):C(dim Z1):C(extension) & 672.62 & 25 & 1.02 & 0.43 \\
C(attribution method name):C(method name):C(extension) & 177.68 & 10 & 0.68 & 0.71 \\
C(dim Z1):C(method name):C(extension) & 237.40 & 10 & 0.90 & 0.51 \\
C(attribution method name):C(dim Z1):C(method name):C(extension) & 932.86 & 50 & 0.71 & 0.93 \\
Residual & 50059.50 & 1902 & NaN & NaN \\
\bottomrule
\end{tabular}
\end{table*}



\begin{table*}[ht]

\footnotesize
    \caption{Post-hoc test for the combination of attribution method and training method.}
    \label{tbl:attr-training}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llrrrrr}
\toprule
 group1 & group2 & meandiff & p-adj & lower & upper & reject \\
\midrule
 Inverted Neuron Gradient:hybrid contrastive & Neuron Gradient:behavior contrastive & 9.41 & 0.00 & 5.83 & 12.98 & True \\
 Inverted Neuron Gradient:hybrid contrastive & Neuron Gradient:hybrid contrastive & 9.51 & 0.00 & 5.93 & 13.08 & True \\
 Inverted Neuron Gradient:hybrid contrastive & Neuron Gradient:supervised & 6.97 & 0.00 & 3.40 & 10.55 & True \\
 Inverted Neuron Gradient:hybrid contrastive & Inverted Neuron Gradient:behavior contrastive & 11.29 & 0.00 & 7.71 & 14.87 & True \\
 Inverted Neuron Gradient:hybrid contrastive & integrated-gradients:hybrid contrastive & -7.67 & 0.00 & -11.75 & -3.59 & True \\
 Inverted Neuron Gradient:hybrid contrastive & feature-ablation:supervised & -7.23 & 0.00 & -10.81 & -3.66 & True \\
 Inverted Neuron Gradient:hybrid contrastive & feature-ablation:hybrid contrastive & -9.00 & 0.00 & -12.58 & -5.42 & True \\
 Inverted Neuron Gradient:hybrid contrastive & feature-ablation:behavior contrastive & -8.74 & 0.00 & -12.32 & -5.16 & True \\
 Inverted Neuron Gradient:hybrid contrastive & Inverted Neuron Gradient:supervised & -8.14 & 0.00 & -11.72 & -4.57 & True \\
 Inverted Neuron Gradient:hybrid contrastive & shapley-zeros:hybrid contrastive & -10.68 & 0.00 & -14.26 & -7.10 & True \\
 Inverted Neuron Gradient:hybrid contrastive & shapley-shuffle:hybrid contrastive & -9.71 & 0.00 & -13.29 & -6.13 & True \\
 Inverted Neuron Gradient:hybrid contrastive & shapley-shuffle:supervised & -7.48 & 0.00 & -11.06 & -3.90 & True \\
 Inverted Neuron Gradient:hybrid contrastive & shapley-zeros:behavior contrastive & -10.90 & 0.00 & -14.47 & -7.32 & True \\
 Inverted Neuron Gradient:hybrid contrastive & shapley-zeros:supervised & -10.09 & 0.00 & -13.66 & -6.51 & True \\
 Inverted Neuron Gradient:hybrid contrastive & integrated-gradients:behavior contrastive & -10.96 & 0.00 & -14.54 & -7.38 & True \\
 Inverted Neuron Gradient:hybrid contrastive & integrated-gradients:supervised & -10.14 & 0.00 & -13.72 & -6.57 & True \\
 Inverted Neuron Gradient:hybrid contrastive & shapley-shuffle:behavior contrastive & -9.13 & 0.00 & -12.71 & -5.55 & True \\
 Neuron Gradient:supervised & Inverted Neuron Gradient:behavior contrastive & -4.31 & 0.00 & -7.89 & -0.74 & True \\
 Neuron Gradient:supervised & shapley-zeros:hybrid contrastive & -3.70 & 0.03 & -7.28 & -0.13 & True \\
 Neuron Gradient:supervised & shapley-zeros:behavior contrastive & -3.92 & 0.02 & -7.50 & -0.35 & True \\
 Neuron Gradient:supervised & integrated-gradients:behavior contrastive & -3.99 & 0.01 & -7.56 & -0.41 & True \\
 feature-ablation:supervised & Inverted Neuron Gradient:behavior contrastive & 4.05 & 0.01 & 0.48 & 7.63 & True \\
 feature-ablation:supervised & integrated-gradients:behavior contrastive & -3.73 & 0.03 & -7.30 & -0.15 & True \\
 feature-ablation:supervised & shapley-zeros:behavior contrastive & -3.66 & 0.04 & -7.24 & -0.09 & True \\
 shapley-shuffle:supervised & Inverted Neuron Gradient:behavior contrastive & 3.81 & 0.02 & 0.23 & 7.39 & True \\
\bottomrule
\end{tabular}
}
\end{table*}




\newpage

\begin{table}[thb]
    \centering
    \footnotesize
    \caption{Posthoc test for the combination of attribution method and regularization (REG) scheme.}
    \label{tab:attr-reg}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{llrrrrr}
\toprule
group1 & group2 & meandiff & p-adj & lower & upper & reject \\
\midrule
 Inverted Neuron Gradient:REG & Neuron Gradient:REG & 3.16 & 0.00 & 0.58 & 5.75 & True \\
 Inverted Neuron Gradient:REG & shapley-zeros:REG & -8.91 & 0.00 & -11.50 & -6.32 & True \\
 Inverted Neuron Gradient:REG & Inverted Neuron Gradient:none & -11.61 & 0.00 & -14.20 & -9.03 & True \\
 Inverted Neuron Gradient:REG & feature-ablation:REG & -6.25 & 0.00 & -8.84 & -3.67 & True \\
 Inverted Neuron Gradient:REG & feature-ablation:none & -9.06 & 0.00 & -11.64 & -6.47 & True \\
 Inverted Neuron Gradient:REG & integrated-gradients:REG & -8.02 & 0.00 & -10.70 & -5.35 & True \\
 Inverted Neuron Gradient:REG & integrated-gradients:none & -10.38 & 0.00 & -13.06 & -7.70 & True \\
 Inverted Neuron Gradient:REG & shapley-shuffle:REG & -6.11 & 0.00 & -8.69 & -3.52 & True \\
 Inverted Neuron Gradient:REG & shapley-shuffle:none & -10.10 & 0.00 & -12.69 & -7.51 & True \\
 Inverted Neuron Gradient:REG & Neuron Gradient:none & 12.75 & 0.00 & 10.17 & 15.34 & True \\
 Inverted Neuron Gradient:REG & shapley-zeros:none & -10.86 & 0.00 & -13.44 & -8.27 & True \\
 Neuron Gradient:REG & shapley-zeros:REG & -5.75 & 0.00 & -8.33 & -3.16 & True \\
 Neuron Gradient:REG & shapley-shuffle:REG & -2.94 & 0.01 & -5.53 & -0.35 & True \\
 Neuron Gradient:REG & integrated-gradients:none & -7.21 & 0.00 & -9.89 & -4.53 & True \\
 Neuron Gradient:REG & integrated-gradients:REG & -4.86 & 0.00 & -7.53 & -2.18 & True \\
 Neuron Gradient:REG & feature-ablation:none & -5.89 & 0.00 & -8.48 & -3.30 & True \\
 Neuron Gradient:REG & feature-ablation:REG & -3.09 & 0.01 & -5.68 & -0.50 & True \\
 Neuron Gradient:REG & Inverted Neuron Gradient:none & -8.45 & 0.00 & -11.04 & -5.86 & True \\
 Neuron Gradient:REG & Neuron Gradient:none & -9.59 & 0.00 & -12.18 & -7.00 & True \\
 Neuron Gradient:REG & shapley-shuffle:none & -6.93 & 0.00 & -9.52 & -4.35 & True \\
 Neuron Gradient:REG & shapley-zeros:none & -7.69 & 0.00 & -10.28 & -5.10 & True \\
 shapley-shuffle:REG & Neuron Gradient:none & 6.65 & 0.00 & 4.06 & 9.24 & True \\
 shapley-shuffle:REG & shapley-zeros:none & -4.75 & 0.00 & -7.34 & -2.16 & True \\
 shapley-shuffle:REG & shapley-zeros:REG & -2.81 & 0.02 & -5.39 & -0.22 & True \\
 shapley-shuffle:REG & Inverted Neuron Gradient:none & 5.51 & 0.00 & 2.92 & 8.10 & True \\
 shapley-shuffle:REG & integrated-gradients:none & 4.27 & 0.00 & 1.59 & 6.95 & True \\
 shapley-shuffle:REG & feature-ablation:none & 2.95 & 0.01 & 0.36 & 5.54 & True \\
 shapley-shuffle:REG & shapley-shuffle:none & -3.99 & 0.00 & -6.58 & -1.41 & True \\
 feature-ablation:REG & feature-ablation:none & -2.80 & 0.02 & -5.39 & -0.21 & True \\
 feature-ablation:REG & shapley-shuffle:none & -3.84 & 0.00 & -6.43 & -1.26 & True \\
 feature-ablation:REG & Neuron Gradient:none & 6.50 & 0.00 & 3.91 & 9.09 & True \\
 feature-ablation:REG & shapley-zeros:REG & -2.66 & 0.04 & -5.24 & -0.07 & True \\
 feature-ablation:REG & integrated-gradients:none & -4.12 & 0.00 & -6.80 & -1.44 & True \\
 feature-ablation:REG & Inverted Neuron Gradient:none & 5.36 & 0.00 & 2.77 & 7.95 & True \\
 feature-ablation:REG & shapley-zeros:none & -4.60 & 0.00 & -7.19 & -2.01 & True \\
 integrated-gradients:REG & Inverted Neuron Gradient:none & 3.59 & 0.00 & 0.92 & 6.27 & True \\
 integrated-gradients:REG & shapley-zeros:none & -2.83 & 0.03 & -5.51 & -0.16 & True \\
 integrated-gradients:REG & Neuron Gradient:none & 4.73 & 0.00 & 2.06 & 7.41 & True \\
 shapley-zeros:REG & Neuron Gradient:none & 3.84 & 0.00 & 1.26 & 6.43 & True \\
 shapley-zeros:REG & Inverted Neuron Gradient:none & 2.70 & 0.03 & 0.11 & 5.29 & True \\
 feature-ablation:none & Neuron Gradient:none & 3.70 & 0.00 & 1.11 & 6.29 & True \\
 shapley-shuffle:none & Neuron Gradient:none & 2.66 & 0.04 & 0.07 & 5.24 & True \\
\bottomrule
\end{tabular}
    }
\end{table}


\section{Implementation}\label{app:implementation}

We built our implementation on top of the open source CEBRA package (\citealp{schneider2023cebra}; available at \url{https://github.com/AdaptiveMotorControlLab/cebra}, with the Apache 2.0 license), and our code has been integrated as of version v0.6.0.




