\begin{abstract}
    Gradient-based attribution methods aim to explain decisions of deep learning models but so far lack identifiability guarantees. Here, we propose a method to generate attribution maps with identifiability guarantees by developing a regularized contrastive learning algorithm trained on time-series data plus a new attribution method called Inverted Neuron Gradient (collectively named \raisebox{-0.3ex}{\scriptsize{x}}CEBRA). We show theoretically that \raisebox{-0.3ex}{\scriptsize{x}}CEBRA has favorable properties for identifying the Jacobian matrix of the data generating process. Empirically, we demonstrate robust approximation of zero vs. non-zero entries in the ground-truth attribution map on synthetic datasets, and significant improvements across previous attribution methods based on feature ablation, Shapley values, and other gradient-based methods. Our work constitutes a first example of identifiable inference of time-series attribution maps and opens avenues to a better understanding of time-series data, such as for neural dynamics and decision-processes within neural networks.
\end{abstract}

\section{Introduction}

    The distillation of knowledge from data is a core tenet of science. In neuroscience, where high-dimensional and large-scale data are becoming increasingly available, a better understanding of how the input data is shaping the distilled knowledge is a key challenge. Modern approaches for extracting information for neural time-series data are leveraging deep learning models to extract latent dynamics. Yet, the nature of how individual neurons can be mapped to these population-level latents is unknown. Similarly to computer vision, where pixels are attributed to classification decisions, our aim is to understand how individual neurons contribute to the neural code over time.

    
    In machine learning, especially in computer vision, many algorithms exist for explaining the decisions of trained (non-linear) neural networks, often on static-image classification tasks~\citep{samek2019explainable,Ancona2017TowardsBU,Shrikumar2016NotJA,Sundararajan2017AxiomaticAF,Montavon2015ExplainingNC,Simonyan2013DeepIC,lundberg2017unified}. In particular, gradient-based attribution methods have shown empirical success, but can be computationally costly and/or lack theoretical grounding~\citep{Simonyan2013DeepIC,lundberg2017unified}, which ultimately limits their utility and scope in scientific applications that benefit from theoretical guarantees.
    

    We consider the problem of estimating time-series attribution maps for the purpose of scientific, neural data analysis.
    Concretely, in neuroscience, various populations of neurons are recorded over time, and one aims to understand how these neurons relate to observable behaviors or internal states (Figure~\ref{fig:fig1}). For interpretability, linear methods (such as PCA or linear regression) are often used, even though the underlying data did not necessarily arise from linear processes. However, non-linear methods are difficult to interpret~\citep{breen2018interpreting,samek2019explainable}.
    Emerging approaches leverage latent variable models, which are particularly well suited to extract the underlying dynamics, but how these abstract latent factors map onto neurons remains an open challenge. 
    
    Here, we build on recent advances using time contrastive learning with auxiliary variables, as it showed considerable promise in its performance for recovering latent spaces with identifiability guarantees, both theoretically and empirically~\citep{hyvarinen2016unsupervised,hyvarinen2019nonlinear,schneider2023cebra,zimmermann2021contrastive}. 
    %Moreover, the use of auxiliary variables is attractive, as it forces the recovered factors to be conditionally independent, allowing for hypothesis-guided latents to be extracted.

    Our work connects recent advances in identifiable representation learning with the estimation of attribution maps for scientific data analysis. Specifically,
    \vspace{-5pt}
    \begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
        \item We formalize properties of time-series attribution maps based on the causal connectivity between latents and input data in Section~\ref{sec:attribution-maps}. Moving towards such a formalism will help align goals of future estimation algorithms, as the derivated theoretical properties are necessary for successful application of attribution methods in scientific inference. 
        \item We propose a regularized contrastive learning algorithm in Section~\ref{sec:regcl}, and theoretically show that this algorithm recovers the essential graph structure of these ground truth attribution maps in Section~\ref{sec:identifiability}.
        \item We verify our algorithm on multiple synthetic datasets in Section~\ref{sec:simulations} and show applicability to neural data in Section~\ref{sec:application}. Critically, we show that our unsupervised regularized contrastive learning and Inverted Neuron Gradient (\raisebox{-0.3ex}{\scriptsize{x}}CEBRA) method can outperform supervised baselines.
    \end{enumerate}
    

    
\begin{figure*}[t]
        \centering
        \includegraphics[width=\textwidth]{FIG1.jpg}
        \vspace{-5pt}
        \caption{\textbf{Identifiable attribution maps for time-series data.} Using time-series data (such as neural data recorded during navigation, as depicted), our inference framework estimates the ground-truth Jacobian matrix $\JJ_\gg$ (i.e., $\xx$ is the observed neural data linked to latents $\zz$ and $\cc$, where $\cc$ is the explicit [auxiliary] behavioral variable that would be linked to grid cells) by identifying the inverse data generation process up to a linear indeterminacy $\LL$. Then, we estimate the Jacobian $\JJ_\ff$ of the encoder model ($\ff$) by minimizing a generalized InfoNCE objective. Inverting this Jacobian $\JJ_\ff^+$, which approximates $\JJ_\gg$, allows us to construct the attributions.
        }
        \label{fig:fig1}
        \vspace{-7pt}
    \end{figure*}

\vspace{-10pt}
\paragraph{Related Works.}

Interpretable machine learning stems from either \textit{ante hoc} designing interpretable models, i.e., linear models or de-correlated design matrices, or \textit{post hoc} using attribution methods such as perturbations or gradient methods. In the presence of non-linear relationships in the data, the first approach is often not feasible. Thus, the field of Explainable AI aims to design algorithms for understanding ``black-box models'', which facilitates comprehension and refinement of complex models and/or data.

Depending on the type of explanation one aims to obtain, there are different \textit{post hoc} interpretability methods \citep{samek2019explainable}. We can differentiate between local and global explanations. \emph{Global explanations} provide an interpretable description of the behavior of the model as a whole. \emph{Local explanations} provide a description of the model behavior in a specific neighborhood/for an individual prediction.

Local explanations, which we consider most critical for time-series, assign a weight to each feature in the input space that indicates its importance or effect.
\emph{Perturbation-based} methods compute a relevance score by removing, masking, or altering the input, running a forward pass on the new input, and measuring the difference with the original input. Methods include LIME or the highly popular Shapley values \citep{ribeiro2016should, lundberg2017unified}.
\emph{Gradient-based} methods locally evaluate the gradient $\partial{f} / \partial{x_i}$ or variations of it (e.g., the absolute value of the gradient). Methods include Integrated Gradients, SmoothGrad, or Grad-CAM \citep{Sundararajan2017AxiomaticAF, smilkov2017smoothgrad, selvaraju2017grad}.



\section{Identifiable attribution maps} \label{sec:attribution-maps}

    A critical application of attribution methods is to investigate properties of a trained neural network, e.g., a computer vision model classifying images. In many scientific domains, data comes in the form of time-series (videos, neural recordings, etc.). Therefore, in this setting, we are interested in a notion of attribution grounded in the ground truth connectivity -- ``ground truth map' -- between the recorded time-series data and the underlying data-generating process, i.e., latent factors, at each time step.
    Such a view on attribution methods allows us to connect the attribution map to the causal structure of the data generating process, which we outline in the following:

    \begin{definition}[Data generating process] \label{def:data-generator}
        We assume that data is generated from a set of latent factors $\zz_1 \in \R^{d_1}, \dots, \zz_{G} \in \R^{d_G}$.
        For brevity, the vector $\zz \in \R^d$ denotes the concatenation of all factors, and $d = \sum_i d_i$.
        Their distribution for the timestep $t$ factorizes to
        \begin{align}
            p(\zz^{(t)} | \zz^{(t-1)}) = %p(\zz_1,\dots,\zz_G) =
            \prod_{i=1}^G p(\zz^{(t)}_i | \zz_i^{(t-1)}), 
        \end{align}
        i.e., factors are conditionally independent given their value at the previous time step $\zz^{(t-1)}$.
        The support of the resulting marginal distributions $p(\zz_i)$ is assumed to be a convex body or the hypersphere embedded in $\R^{d_i}$.
        The conditional distribution is assumed to take the form

        \begin{equation}
            \forall i\in[G]:
                p(\zz^{(t)}_i | \zz_i^{(t-1)})
                %p(\zz_i | \zz_i')
                \propto \exp{(-\delta(\zz^{(t)}_i, \zz^{(t-1)}_i))}
        \end{equation}
        for each latent factor,
        based on the negative dot-product or a semi-metric $\delta: \mathcal Z \times \mathcal Z \mapsto \R$. 
        %
        An injective mixing function $\gg: \mathcal{Z} \mapsto \mathcal{X}$ with $\mathcal{X} \subseteq \R^D$ maps latent factors to observations,
        \begin{equation}\label{eq:mixing}
            \forall i \in [D]: \quad
                x_i = g_i([\zz_j]_{j\in P_i}).
        \end{equation}
        $P_i$ is an index set, and $j \in P_i$ implies that factor $\zz_j \in \mathbb{R}^{d_j}$ is used to generate the output $x_i$.
      
        Some factors are connected to auxiliary variables $\cc_i$ through bijective maps $\gamma_i: \R^{d_i} \mapsto \R^{d_i}$ s.t. $\zz_i = \gamma_i(\cc_i)$, as exemplified in Figures~\ref{fig:fig1} \& \ref{fig:mixing_function_exp2}.
    \end{definition} 

    We proceed with a rigorous definition of identifiability for time-series attribution maps. Identifiability in the context of deep learning models is commonly studied in terms of indeterminacies in the inferred latent space \citep{khemakhem2020variational, roeder2021linear}.
    Under the data-generating framework defined above, consider a feature encoder $\ff: \mathcal{X} \mapsto \mathcal{Z}$ which maps observable data to an embedding space. The feature encoder is part of a probabilistic model with density\footnote{The definition of this density depends on the model type and would e.g., vary between an iVAE \citep{khemakhem2020variational} and a contrastive learning model. The following definitions are independent of this model choice.} $p_\ff$.
    We define:
    \begin{definition}[Subspace Identifiability]
        Feature encoders $\ff', \ff^*: \mathcal X \mapsto \mathcal Z$ are identifiable up to subspaces if matching distributions $p_{\ff'} = p_{\ff^*}$ imply that the following equivalence relation holds (the label ``$B$'' denotes ``blockwise''):
        \begin{equation}
            \ff' \mathrel{\mathop{\sim}^B} \ff^*
            \iff
            \ff'(\mathbf x) = \mathbf{B} \ff^*(\mathbf x),
        \end{equation}
        for the block-diagonal matrix $\mathbf{B} \in \R^{d \times d}$ with blocks of sizes $d_1 \times d_1, \dots, d_G \times d_G$.
    \end{definition}
    %Other notions of identifiability have been considered: ... TODO ...

    Next, we extend the concept of identifiability to attribution maps.
    An attribution map $\AA \in \mathcal{A} \subseteq \R^{D \times d}$ contains scores $A_{ij}$. If the $i$-th latent is connected to the $j$-th output, we expect a high score, otherwise a low score -- our definitions below are invariant to scaling and shifting of the scores. 
    For two attribution methods generating attribution maps $\AA', \AA^* \in \mathcal{A}$, we define the following equivalence relation on $\mathcal{A}$:
    \begin{definition}[Identifiability of connectivity in attribution maps] \label{def:ident-attr}
        Let $\AA',\AA^* \in \mathcal A$ be attribution maps for the feature encoders $\ff',\ff^*$. Let $\mathrel{\mathop{\sim}^C}$ be a pairwise relation on $\mathcal{A}$ defined as:
        \begin{equation}
        \AA' \mathrel{\mathop{\sim}^C} \AA^* \iff
        \forall i,j:
            (A_{ij}' \neq 0 \Leftrightarrow A^*_{ij} \neq 0)
        \end{equation}
        An attribution method is identifiable if the following relation holds (the label ``$C$'' denotes ``connectivity''):
        %An attribution method identifies theof connectivity if the following relation holds:
        %An attribution method is identifiable of connectivity if the following relation holds:
        \begin{equation}
            \ff' \mathrel{\mathop{\sim}^B} \ff^* \implies \AA' \mathrel{\mathop{\sim}^C} \AA^*.
        \end{equation}
    \end{definition}
    The relation describes how to match the locations of the ``zero entries'' in $\AA'$ and $\AA^*$. For scientific discovery, obtaining this relation is already of high value: It addresses the question of how inferred latent factors are related (i.e.,``connected'') to parts of the observable data. It also avoids conflicts with other definitions of attribution values discussed in the current literature \citep{Sundararajan2017AxiomaticAF,afchar2021nonzero}. 

    We now have all the necessary definitions to establish a ground-truth attribution map for the mixing function $\gg$. Specifically, we are interested in how the factors $\zz$ are connected to the generated data $\xx$ by means of any non-linear mapping. The connectivity defined in Eq.~\ref{eq:mixing} can be read out by considering the Jacobian matrix of $\gg$, which lets us define the ground truth attribution map as follows:
    \begin{definition}[Ground truth attribution map of the mixing function] \label{def:gt-attribution}
        The ground-truth attribution map $\AA_\gg \in \mathcal{A}$ of the mixing function $\gg$ is defined via the following relationship to the Jacobian matrix $\JJ_\gg$:
        \begin{equation}
        \forall \zz \in \mathcal{Z}: 
            \AA_\gg \mathrel{\mathop{\sim}^C}
            \JJ_\mathbf{\gg}(\zz). 
        \end{equation}
    \end{definition}
    Intuitively, zero-valued derivatives of the observable data with respect to a latent defines non-connectivity.

    This definition of the ``ground-truth map'' is intentionally quite flexible. It does not conflict with existing approaches (for example, see~\citealp{Sundararajan2017AxiomaticAF}), and does not imply a particular form of the ground-truth map $\AA_\gg$ besides the locations of zeros. 
    
\section{Regularized Contrastive Learning}
\label{sec:regcl}

    We now propose a new estimation algorithm for time-series attribution maps under the data generating process in Def.~\ref{def:data-generator}, and later show that it satisfies the notion of identifiability in Def.~\ref{def:ident-attr}.
    We introduce a new variant of contrastive learning
    for estimation of time-series attribution maps, which we call \raisebox{-0.3ex}{\scriptsize{x}}CEBRA (e\raisebox{-0.3ex}{\scriptsize{x}}plainable). Specifically, we build on our previous work CEBRA~\cite{schneider2023cebra}.
    As we show in our theoretical results, this extended algorithm identifies latent factors underlying the dataset, and then attributes them to the input data conditioned on observable, auxiliary variables.

    In the following, we call $p(\cdot | \cdot)$ the positive and $q(\cdot | \cdot)$ the negative sample distribution.
    We call $(\xx, \xx^+)$ a positive pair, and all $(\xx, \xx^-_i)$ \rev{for $i \in [N]$} negative pairs.
    The auxiliary variables shape the positive distribution, and hence the positive pairs.
    $\xx$ is the input time-series data, for example neural activity recorded from the brain (Figure~\ref{fig:fig1}).
  
    We define a feature encoder $\ff := [\ff_1; \dots; \ff_G]$, with $\ff_i: \mathcal{X} \mapsto \R^{d_i}$ that maps samples into an embedding space partitioned into $G$ groups. In practice, we parameterize $\ff$ as a single neural network and only split the last layer into $G$ different parts.
    For training, we apply similarity metrics $\phi_i: \R^{d_i} \times \R^{d_i} \mapsto \R$ to the different parts of this feature encoder,
    abbreviated as $\psi_i(\xx, \yy) := \phi_i(\ff(\xx), \ff(\yy))$. We then leverage the generalized InfoNCE loss~\citep{schneider2023cebra},
    \begin{align}\label{def:infonce}
        \mathcal L_N[\psi] = \mathop{\mathbb{E}}\limits_{{\substack{
            \xx \sim p(\xx),\  
            \xx^+ \sim p_i(\xx^+|\xx),\\
            \xx^-_1 \dots \xx^-_N \sim q(\xx^-|\xx)\\
        }}}
        \hspace{-.25em}
        \left[ \ell(\xx,\xx^+\!, \{\xx^-_j\}_{j=1}^{N})\right], \\
        \intertext{using the loss function}
        \ell(\xx,\xx^+\!, S) = - \psi(\xx, \xx^+)
                    + \log \sum_{\xx^- \in S} e^{\psi(\xx, \xx^-)},
    \end{align}
    where $S$ denotes a set of negative examples.
    In addition, we regularize the Jacobian matrix of the feature encoder by minimizing its Frobenius norm~\citep{hoffman2019robust}. With these constraints, we propose our modified objective function, which we call \emph{Regularized Contrastive Learning}, for all parts of the representation:
    \begin{equation}
    \begin{aligned}
        &\mathcal L_N[\psi; \lambda] = \hspace{-2.5em}
        \mathop{\mathbb{E}}\limits_{{\substack{
            \xx \sim p(\xx),\\ 
            \xx_i^+ \sim p_i(\xx^+|\xx) \ \forall i\in[G]\\
            \xx^-_1{,}\dots {,} \xx^-_N \sim q(\xx^-|\xx)
        }}} \hspace{-2.5em}
        \big[ \sum_{i=1}^{G} \ell(\xx,\xx^+_i,\{\xx^-_i\}_{i=1}^{N})
        + 
        \lambda \| \JJ_\ff(\xx) \|_F^2
        \big],
        \label{eq:generalized-infonce}
        \end{aligned}
    \end{equation}
    where $\JJ_\ff(\xx)$ is the Jacobian of the feature encoder $\ff$ optimized as part of $\psi$, $\|\cdot\|_F$ denotes the Frobenius norm and $\lambda$ is a hyperparameter tuned based on the learning dynamics. $\lambda$ is \rev{set} to the highest value possible that still allows the InfoNCE component of the loss to stay at its minimum.

    In this work, we use this method in two ways: ``supervised contrastive'' and ``hybrid contrastive'' both with ($\lambda > 0$) or without regularization ($\lambda = 0$). Supervised means the auxiliary information is used for all latent dimensions. Hybrid means some latent dimensions are specifically reserved for unaccounted for latent factors (i.e., unsupervised ``time-only'; factors that we do not explicitly test with auxiliary data but want to account for) and others tied to auxiliary variables~\citep{schneider2023cebra}.


    \paragraph{Model fitting.}

        To optimize Eq.~\ref{eq:generalized-infonce}, we need to sample from suitable positive distributions $p_1,\dots,p_G$ for each group and a negative distribution $q$.
        If a latent factor $\zz$ is connected to an observable $\cc$, we use a variant of supervised contrastive learning with continuous labels \citep{schneider2023cebra}: We uniformly sample a timestep $t$ (and hence, a sample $\xx^{(t)}$) from the dataset. This timestep is associated to the label $\cc^{(t)}$. 
        We consider the changes of $\cc$ across the dataset, $\Delta_t = \cc^{(t+1)} - \cc^{(t)}$. We sample a timestep $\tau$ uniformly, and then find the timestep $t'$ for which $\|\cc^{(t')} - \cc^{(t)} - \Delta_{\tau}\|$ is minimized.
        This yields a positive pair $(\xx^{(t)}, \xx^{(t')})$ to feed to the model.

        If a latent factor is not connected to an observable, we leverage the time structure only \citep{hyvarinen17pcl,hyvarinen2019nonlinear} and use adjacent timesteps as positive pairs: $(\xx^{(t)}, \xx^{(t+1)})$.
        More details about sampling are provided in the Appendix. 
    
    \paragraph{Obtaining attribution maps.}

        Our attribution map $\AA$ is a $D \times d$-dimensional matrix and its entry $A_{ij}$ denotes if the latent at dimension $j$ is related to input dimension $i$. We can compute such a map for every timepoint in the dataset or aggregate multiple timepoints into a global map.

        After training $\ff$ using our regularized contrastive learning method, we obtain attribution maps by computing the Jacobian matrix $\JJ_\ff(\xx)$. We then consider its pseudo-inverse $\JJ_\ff^+(\xx)$ at every timestep,  which we name the ``inverted neuron gradient''. The estimation coincides with the ``neuron gradient'' attribution method \citep{Simonyan2013DeepIC}, however this has not been paired with identifiable regularized contrastive learning as proposed here. 

        Similar to \citet{afchar2021nonzero}, our work focuses on the problem of clearly delineating the \emph{binary} relationship between latents and input data. For this, we threshold the attribution map with a variable decision threshold $\epsilon$, 
        $\hat \AA(\xx) := \mathbf{1}\{| \JJ_\ff^+(\xx) |  > \epsilon \}$
        for inverted neuron gradient, and analogously for our baseline methods. 

        To obtain a global attribution map from local attribution maps, we additionally improve the signal-to-noise ratio by averaging multiple maps. In practice, we found that the operation
        \begin{equation}\label{eq:estimation}
            \hat{\AA} = \mathbf{1}\{\sum_\xx | \JJ_\ff^+(\xx) |  > \epsilon\}
        \end{equation}
        yields even better performance, which we used for all experiments and baselines. An alternative, which we considered but did not further explore due to seeing considerably worse performance, is to leverage a $\max$ operation instead of the sum. Taking the median is possible, and performs roughly on par with the mean.
       

    \section{Identifiability of \raisebox{-0.3ex}{\scriptsize{x}}CEBRA}
    \label{sec:identifiability}

    We now derive two new results relevant for the application to the generation of attribution maps. Firstly, we want to ensure a goodness of fit criterion for distinguishing meaningful fits of the model, both in the time contrastive and supervised contrastive case (Theorem 1).
    Secondly, we extend identifiability of the latent space to identifiability of the Jacobian (Theorem 2). 
    

    
    \begin{theorem}\label{thm:overfitting}
        Assume $\psi^*$ is a minimizer of the generalized InfoNCE loss (Eq.~\ref{def:infonce}) under the non-linear ICA problem in Def.~\ref{def:data-generator} for $N \rightarrow \infty$. Assume that the model is trained on auxiliary variables $\cc$ which are independent of $\zz$.
        Then, $\psi^*=const.$ is the trivial solution with $\lim_{N \rightarrow \infty} \mathcal{L}_N[\psi^*] = \log N$ and the embedding collapses. 
        \begin{proof}%

        The full proof is given in Appendix~\ref{sec:proof-prop-overfitting}.
        \end{proof}
    \end{theorem}
    This result ensures that if an auxiliary variable $\cc$ is not related to the data but still used during training, the loss remains at change level $\log N$.
    Hence, we can rule out auxiliary variables not useful for subspace identification, and sort them out for model fitting.
     
    We proceed with studying the attribution map. The loss in Eq.~\ref{eq:generalized-infonce} intuitively solves $G$ non-linear demixing problems using the single feature encoder $\ff$. By applying time contrastive and supervised contrastive learning to structure the embedding space, we can show:
    %
    \begin{theorem}\label{thm:main-theorem}
        Assume
        \begin{itemize}
            \item A mixing function $\gg$ with ground truth map $\AA_\gg$ maps latent factors $\zz$ to a signal space such that $\xx = \gg(\zz)$ according to Def.~\ref{def:data-generator}.
            \item The differentiable feature encoder $\ff$ minimizes the regularized contrastive loss (Eq.~\ref{eq:generalized-infonce}) on \rev{the support of} $p(\zz)$.
        \end{itemize}
        Then, in the limit of infinite samples $N \rightarrow \infty$,
        \begin{itemize}
            \item the model identifies the latent subspaces of the ground truth process, i.e., $\gg(\ff(\xx)) = \BB \xx$
                with a block diagonal matrix $\BB$. 
            \item we identify zero-entries of the ground truth attribution map $\AA_\gg$ (Def.~\ref{def:gt-attribution})
                through the pseudo-inverse $\JJ_\ff^+(\xx)$,
                \begin{equation}
                    \JJ_\ff^+ (\xx) \mathrel{\mathop{\sim}^C} \AA_\gg.
                \end{equation}
        \end{itemize}
        \vspace{-3pt}
        \begin{proofsketch}
            The individual parts of the loss function result in $\psi(\xx, \xx') = \log p_i(\zz'_i | \zz_i) / q(\zz'_i)$ from which a linear indeterminacy follows, $\ff_i(\gg(\zz)) = \LL_i \zz$. We can express the result as $\ff(\gg(\zz)) = \LL \zz$ where $\LL$ is a block-diagonal matrix with zeros in its lower block triangular part. Hence, $\LL^{-1}$ will have the same property. It then follows that $\JJ_\ff(\xx) \JJ_\gg(\zz) = \LL$ and since $\JJ_\ff$ has minimum norm everywhere, $\JJ_\ff^+(\xx)$ is the Moore-Penrose pseudo-inverse of $\JJ_\gg(\zz) \LL^{-1}$. Multiplication with $\LL^{-1}$ does not alter the location of zero entries in $\JJ_\gg(\zz)$, and hence thresholding $\JJ_\ff^+(\xx)$ across samples $\xx$ in the dataset is an estimator of the ground-truth attribution map.
            The full proof is given in Appendix~\ref{sec:proof-main-theorem}.
        \end{proofsketch}
    \end{theorem}
    Theorem 1 justifies the use of the InfoNCE loss as a ``goodness of fit''. We leverage this property during model training of our regularized contrastive learning model, where we set $\lambda = 0$ for the first steps to determine the value of $\mathcal{L}_N(\psi^*; 0)$. If this value converges to a minimum that is meaningfully below chance level ($\log N$), we proceed by raising $\lambda$, while ensuring that the InfoNCE loss stays constant. 
    Once the second component of the loss also converges, Theorem 2 guarantees identifiability of zero/non-zero entries in the attribution map.

    Note, while both Theorems are stated in the limit of infinite data, \citet{wang2020understanding} show that the deviation of the contrastive loss and its asympotic limit decays with $\mathcal{O}(N^{-1/2})$.
    The empirical verification below also confirms that our identifiability guarantee holds up well in practice, with limited $N$.

    \begin{figure}[t]
  \centering
    \begin{tabular}{ccc}
    \begin{tikzpicture}[scale=1.5]
      \node[latent] (z1) at (0,1) {$\zz_1$};
      \node[latent] (z2) at (0,0) {$\zz_2$};
      \node[obs] (c) at (1,-1) {$\cc_2$};
      \node[obs] (x1) at (1,1) {$\xx_1$};
      \node[obs] (x2) at (1,0) {$\xx_2$};
      \edge {z1} {x1};
      \edge {z1} {x2};
      \edge {z2} {x2};
      \edge {z2} {c};
      \draw[dashed,red] (x2) -- (c);
    \end{tikzpicture}
    &
    \hspace{3em}
    &
    \begin{tikzpicture}[scale=1.5]
      \node[latent] (z1) at (0,1) {$\zz_1$};
      \node[latent] (z2) at (0,0) {$\zz_2$};
      \node[obs] (c) at  (1,2) {$\cc_1$};
      \node[obs] (x1) at (1,1) {$\xx_1$};
      \node[obs] (x2) at (1,0) {$\xx_2$};
      \edge {z1} {x1};
      \edge {z1} {x2};
      \edge {z2} {x2};
      \edge {z1} {c};
      \draw[dashed,red] (x1) -- (c);
    \end{tikzpicture} \\
    (a) && (b)
    \end{tabular}
    \caption{\textbf{Left:} Graphical model for the data generating process where $\zz_2$ is observed through $\cc_2$. The attribution map needs to be computed with respect to $\zz_2$, which is inferred with supervised (contrastive) learning. Note, practically, this means $\xx_2$ is behaviorally linked to $\cc_2$ (denoted by dashed line). Related to Table~\ref{tbl:summary}. \textbf{Right:} Graphical model for the data generating process where $\zz_1$ is observed through $\cc_1$. Since $\zz_2$ is not observed, the attribution map can only be estimated through the time-contrastive component in \raisebox{-0.3ex}{\scriptsize{x}}CEBRA. Related to Table~\ref{table:latent-attribution}.}
    \label{fig:mixing_function_exp2}
\end{figure}

    \begin{table*}[t]
    \centering
    \small
        \caption{%
        \textbf{Verification of the theory.} auROC comparison of attribution methods (rows), and combinations of training/regularization schemes (columns). Our proposed method is regularized contrastive learning, with the Jacobian (Neuron gradient) or pseudo-inverse Jacobian (Inverted Neuron Gradient). Numbers average across different total latent dimensions ($d=4$ to $d=9$), for 10 different datasets. Sub- and superscript values denote the 95\% confidence interval obtained through bootstrapping (n=1,000).
    }
    \label{tbl:summary}
    %\small
    \begin{tabular}{lcccccc}
    \toprule
     & \multicolumn{2}{c}{supervised} & \multicolumn{2}{c}{supervised contrastive} & \multicolumn{2}{c}{hybrid contrastive} \\
     & none & regularized & none & regularized & none & regularized \\
    attribution method &  &  &  & \textbf{(Ours)} &  &  \textbf{(Ours)} \\
    \midrule
    Feature Ablation & $83.1_{81.3}^{84.8}$ & $88.5_{87.0}^{90.0}$ & $84.0_{82.1}^{85.6}$ & $84.7_{82.8}^{86.5}$ & $82.9_{81.3}^{84.5}$ & $85.2_{83.4}^{86.9}$ \\
    Shapley, shuffled & $82.0_{80.3}^{83.7}$ & $89.2_{87.6}^{90.8}$ & $83.3_{81.4}^{84.9}$ & $84.6_{82.6}^{86.6}$ & $81.6_{80.1}^{83.2}$ & $85.1_{83.0}^{87.1}$ \\
    Shapley, zeros & $81.0_{79.3}^{82.8}$ & $84.9_{83.1}^{86.8}$ & $82.0_{80.2}^{83.7}$ & $82.4_{80.4}^{84.3}$ & $81.6_{79.9}^{83.4}$ & $83.2_{81.2}^{85.0}$ \\
    Integrated Gradients & $81.0_{79.2}^{82.7}$ & $84.9_{83.1}^{86.6}$ & $81.9_{80.2}^{83.7}$ & $82.3_{80.5}^{84.3}$ & $83.9_{82.1}^{85.6}$ & $86.9_{84.9}^{88.8}$ \\
    Neuron Gradient & $79.2_{77.4}^{81.0}$ & $93.0_{91.5}^{94.5}$ & $80.6_{78.8}^{82.4}$ & $86.7_{84.6}^{89.0}$ & $79.2_{77.5}^{81.0}$ & $88.0_{85.8}^{90.1}$ \\
    \midrule
    \textbf{Inverted Neuron Gradient (Ours)}& $76.9_{74.9}^{78.7}$ & $92.9_{91.5}^{94.5}$ & $77.5_{75.5}^{79.4}$ & $86.1_{83.8}^{88.3}$ & $87.9_{86.3}^{89.5}$ & $\mathbf{98.2_{97.4}^{98.9}}$ \\
    \bottomrule
    \end{tabular}
    \end{table*}
    
%\vspace{-2pt}    
\section{Experimental Methods}
\label{sec:methods}

\paragraph{Synthetic finite time-series data design.}
    To verify our theory, we generated a synthetic dataset following Def.~\ref{def:data-generator}. An essential aspect of our synthetic design lies in the definition of the mixing function $\gg$ which, consequently, defines the ground truth attribution map. We split $\zz$ into the factors $\zz_1$ and $\zz_2$ (Appendix Figure~\ref{fig:synthetic}). Figure \ref{fig:mixing_function_exp2} illustrates the two experimental synthetic data-generation configurations employed in this work, and Appendix Figure~\ref{figure:synthetic-data} shows the learned embedding. 
    
    In both settings, $\zz_1$ is connected both to $\xx_1$ and $\xx_2$ whereas $\zz_2$ is only connected to $\xx_2$. The main difference is that in the first setting $\zz_2 = \gamma_2(\cc_2)$ whereas in the second setting $\zz_1 = \gamma_1(\cc_1)$.
    We sample 10 different datasets with 100,000 samples, each with a different mixing function $\gg$. All latents of the dataset are chosen to lie within the box $[-1, 1]^D$. The following timesteps are generated by Brownian motion.

\paragraph{Model fitting.}
    The feature encoder $\ff$ is an MLP with three layers followed by GELU activations \citep{hendrycks2016gaussian}, and one layer followed by a scaled $\tanh$ to decode the latents~\cite{schneider2023cebra}. We train on batches with 5,000 samples each. The first 2,500 training steps minimize the InfoNCE or supervised loss with $\lambda=0$; we ramp up $\lambda$ to its maximum value over the following 2,500 steps, and train until 20,000 total steps.
    We compute the $R^2$ for predicting the auxiliary variable $\cc$ from the feature space after a linear regression and ensure that this metric is close to $100\%$ for both our baseline and contrastive learning models to remove performance as a potential confounder.
    Hyperparameters are identical between training setups, the regularizer $\lambda$, and number of training steps are informed by the training dynamics.
        
\paragraph{Baselines.}
    To compare to previous works, we vary the training method (hybrid contrastive, supervised contrastive, standard supervised) and consider baseline methods for estimating the attribution maps (Neuron gradients,~\citealp{Simonyan2013DeepIC}; Integrated gradients,~\citealp{shrikumar2018computationally,Sundararajan2017AxiomaticAF}; Shapley values,~\citealp{shapley1953value,lundberg2017unified}; and Feature ablation, \citealp{molnar2022}), which are commonly used algorithms in scientific applications \citep{samek2019explainable,molnar2022}. To compute these attribution maps, we leveraged the open source library Captum~\citep{kokhlikyan2020captum}. We also compare regularized and non-regularized training. 

\paragraph{Evaluation.}
    We evaluate the identification of the attribution map at different decision thresholds $\epsilon$ similar to a binary classification problem: namely, for each decision threshold, we binarize the inferred map, and compute the binary accuracy to the ground truth map. We compute the ROC curve as we vary the threshold for each method, and use area under ROC (auROC) as our main metric. In practice where a single threshold needs to be picked, we found z-scoring of the attribution score an effective way to set $\epsilon$ corresponding to a z-score of 0.

\paragraph{Synthetic (RatInABox) neural data.}
    
    As an application to a neuroscientific use case, we generate synthetic neural data during navigation using RatInABox \citep{george2022ratinabox}, a toolbox that simulates spatial trajectories and neural firing patterns of an agent in an enclosed environment.
    We generate firing rates of place, two modules of grid, head direction, and speed cells (n=100 neurons each, 400 neurons in total) for 20,000 time steps.
    To calculate the grid scores we used the method described by \citet{sargolini2006conjunctive}.
    
    With these cell types, at least three properties (position, speed, and head direction) are encoded by these neurons, and represented in the ground truth latents. Speed information is incorporated only in speed cells, head direction information only in head direction cells, and position information is coded by both position and grid cells, by design. We design the attribution map accordingly (Appendix Figure~\ref{fig:synth-gt-graph}) -- for models trained with position information, we would expect to discover grid and place cells, but not the other types.
    Further details are outlined in Appendix~\ref{app:ratinabox}.




\section{Simulations}\label{sec:simulations}
    %\label{sec:results_setup}       

    \paragraph{Regularized, hybrid contrastive learning identifies the ground truth attribution map.}

    We begin by experimentally testing our theory that regularized hybrid contrastive learning allows for causal discovery of time-series attribution maps. To quantify this, we first consider an average auROC score across time for recovering the ground truth graph structure (as shown in Figure~\ref{fig:mixing_function_exp2}(a)).

    Concretely, Table~\ref{tbl:summary} shows the auROC for recovering $\AA$ using combinations of training schemes.
    We investigate the effect of the different model properties with an ordinary least squares (OLS) ANOVA ($F=17.0, p<10^{-5}$) followed by a Tukey HSD posthoc test, see Appendix~\ref{app:statistical_analysis} for statistical methods and full results.
    Both the combination of regularized training followed by estimating the pseudo-inverse ($p<0.01$), and combining regularized training with hybrid contrastive learning ($p<0.001$) significantly outperform all considered baselines, validating the claims made in Theorem 2 empirically.



    \paragraph{Contrastive learning is critical for large numbers of latent factors.}
        The importance of using hybrid contrastive learning (which can identify the latent factors) becomes most apparent with an increasing number of latent factors, as we would expect in a realistic dataset.
        Figure~\ref{fig:latent-factors} shows the variation in performance as we keep the dimension of observable factors fixed at 2 and vary the latent dimension from 4 to 9. % variables. 
        %Beyond this value, the drop in $R^2$ becomes too large, prohibiting a meaningful attribution map. 
        Performance scales with the number of available training samples, and we observed that increasing dataset size beyond 100,000 samples allows the use of even higher numbers of latents.

    \begin{figure}[t]
        \begin{center}
        \includegraphics[width=0.4\textwidth]{num_factors.pdf}
        \end{center}
        \vspace{-1em}        
        \caption{Hybrid Regularized Contrastive Learning+Inverted Neuron Gradient (\raisebox{-0.3ex}{\scriptsize{x}}CEBRA; Ours, black) and supervised baselines auROC vs. dimension of latent factors. Two latent factors are observable as auxiliary variables in all experiments.
        }
        \label{fig:latent-factors}
    \end{figure}
    
\begin{table}[t]
    \caption{\textbf{Estimating attribution maps w.r.t. latent factors}: Results for identifying the attribution map, avg. across 10 seeds and 4--9 latents.}
    \label{table:latent-attribution}
    \small
    \begin{center}
    \addtolength{\tabcolsep}{-3pt}
    \vspace{-8pt}
    \begin{tabular}{lll}
    \toprule
     & CL, no reg. & \textbf{CL + reg.} \\
    \midrule
    Feature Ablation & $77.1_{73.8}^{80.4}$ & $86.7_{83.1}^{89.9}$ \\
    Shapley shuffled & $74.4_{71.2}^{77.5}$ & $87.5_{84.0}^{91.0}$ \\
    Shapley, zeros & $75.8_{72.6}^{78.7}$ & $85.3_{82.0}^{88.6}$ \\
    Integrated Gradient & $77.5_{75.4}^{79.6}$ & $86.8_{83.4}^{89.9}$ \\
    Neuron Gradient & $69.3_{66.0}^{72.4}$ & $91.9_{88.3}^{95.3}$ \\
    \midrule
    \textbf{Inverted Neuron Gradient} & $84.2_{81.2}^{86.7}$ & $\mathbf{99.2_{98.4}^{99.8}}$ \\
    \bottomrule
    \end{tabular}
    \end{center}
    \vspace{-5pt}
\end{table}

%\vspace{-8pt}
    \paragraph{Hybrid contrastive learning allows attribution computation with latent factors.}
        In contrast to supervised algorithms, hybrid contrastive learning allows us to estimate the attribution map with respect to latent factors, i.e., we treat $\zz_1$ as the observable, and $\zz_2$ as the latent factor. With hybrid contrastive learning, we can continue to estimate the attribution map at auROC=99.2\% (Table~\ref{table:latent-attribution}).

    \paragraph{Estimation of the correct dimensionality.}
        It is interesting to consider the case where the dimensionality of the underlying latent space, and the dimensionality of the feature encoder do not match. In these cases, the correct dimensionality can be inferred by starting at a low embedding dimension, and increasing the dimension until the empirical identifiability between pairs of models peaks. The aforementioned results also hold if the true latent dimensionality is unknown (see Appendix).
        
\vspace{-5pt}
\section{Application to neural data analysis}
\label{sec:application}
% with artificial neurons.}

\begin{figure*}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=\textwidth]{embeddings.jpg}
    \caption{\textbf{Attribution scores of synthetic cell types.} \textbf{a}, the synthetic 4-cell type neural data, the simulated navigation and computed speed/head direction. \textbf{b}, embedding space is jointly trained with behavioral information about animal position (first 4 dimensions, top) and additional time-varying latent information (the remaining 10 dimensions) with our regularized hybrid contrastive learning setting. The position information was decoded as indicated by cross-validated $R^2$ score on held-out data. Training embedding is shown. \textbf{c}, time-series attribution map, showing high scores (lighter) for position. \textbf{d}, Attribution scores, zero-centered \& standardized across cells. \textbf{e}, auROC across training.}
    \label{fig:sync-cell-att}
    \vspace{-5pt}
\end{figure*}

We next tested the combinations of supervised (baseline), supervised-contrastive, and hybrid contrasting learning with or without regularization using the attribution method we propose (and compare to the described baselines) on synthetic neural data for benchmarking using RatInABox (Figure~\ref{fig:sync-cell-att}a; see Experimental Methods). 
On this data, multiple combinations of methods reach the maximum possible performance (100\% auROC), but importantly this means that our \raisebox{-0.3ex}{\scriptsize{x}}CEBRA method still performs very well under more realistic (time and neuron number) settings (Appendix Table~\ref{tbl:summary-scd}, Figure~\ref{fig:sync-cell-att}b).

We then examined the position attribution scores for each cell type. Specifically, we measured whether place and grid cells had a higher attribution to speed or head direction (as would be desired from the ground truth graph (see Appendix Figure~\ref{fig:synth-gt-graph}). \raisebox{-0.3ex}{\scriptsize{x}}CEBRA could indeed nicely segment neurons into different types (Figure~\ref{fig:sync-cell-att}c-e). We also carried out experiments where we increased the noise within the input data and show excellent results with \raisebox{-0.3ex}{\scriptsize{x}}CEBRA (Appendix Figure~\ref{fig:noise}).

Notably, our attribution method is computationally faster than integrated gradients and non-gradient based approaches like feature ablation, and of comparable speed as Shapley values (see Appendix Table~\ref{tbl:timing-scd}). Contrastive model training adds a 2x computational overhead for behavior contrastive learning and a 3x computational overhead for the hybrid mode (Appendix Table~\ref{tbl:timing-compute}). This overhead comes with the ability to attribute inputs to latent factors and clearly defined behavior of the goodness-of-fit if no connection exists between input data and auxiliary variables (Theorem 1), i.e., visible as an embedding collapse.

Lastly, we show that our method is applicable to real-world neural data recorded in rats~\citep{Gardner2022}. We trained \raisebox{-0.3ex}{\scriptsize{x}}CEBRA (and baselines) with 2D position as the auxiliary variable. We compute the attribution score over time and show that our method can be used to attribute cells to known cell types (e.g., a grid cell); see Appendix C for full results.

\section{Discussion}

Our presented approach differs from other time-series attribution methods by considering the attribution map of the data-generating process, which is particularly relevant for applications in scientific data analysis. In contrast to previous work, our attribution map is not with respect to a particular model, but rather the data generating process itself. 

\paragraph{Time-series attribution.}
    \citealp{ismail2021improvingdeeplearninginterpretability} discuss multiple attribution methods in the context of time-series attribution and point to their potential limitations. An early work trying to address these limitations is Dynamax~\cite{crabbÃ©2021explainingtimeseriespredictions}. Dynamax is a perturbation-based approach: Given a trained time-series model, it learns a binary mask which, when applied to the input, does not meaningfully change the prediction of that model. While the context is slightly different, the authors similarly to us define the correct masking values through non-zero gradients (see their Def. 2). However, unlike our notion, the definition here is with respect to the model trained on the data, without a defined connection to the ground truth process underlying the dataset. 
    
    \citealp{liu2024explaining} recently combined Dynamax-like training of an attribution mask with contrastive learning and the proposed ContraLSP. ContraLSP uses both a learned mask (like in Dynamax) and the inverted mask to provide a stronger regularization signal to the mask, resulting in substantially improved performance on several downstream tasks. \citealp{leung2023temporal} propose WinIT which uses perturbation-based time series attribution across temporal dependencies. This extended the capabilities of Dynamax across multiple time steps, which is relevant in a range of real-world tasks. However, these developments are orthogonal to our approach discussed here, as their main focus is on the computation of the mask value, rather than its theoretical connection to the ground truth process. We anticipate that incorporating advanced mask learning methods into the parameterization of our attribution map might yield further improvements over our naive averaging method to obtain a stable attribution map.

\paragraph{Contrastive surrogates.}

    Another interesting development is CoRTX~\cite{chuang2023cortx} train CoRTX which can be considered a ``surrogate'' model for generating explanations: Given an existing model to investigate, CoRTX trains a second model which mimics the sensitivity to perturbations using contrastive learning. This is an interesting connection to our supervised contrastive mode, as this sensitivity is an auxiliary variable influencing the selection of positive pairs. However, while~\citealp{chuang2023cortx} provide error bounds between the surrogate and investigate model, no connection to the ground-truth generating process is given, as in our work. It would be interesting to discuss whether this method gets conceptually similar to ours as we consider the inverted data generating process $\gg^{-1}$ as the ``model'' under investigation; however, this still requires an approach like our proposed \raisebox{-0.3ex}{\scriptsize{x}}CEBRA, specifically the regularized contrastive learning, for identifying this model in the first place.

\paragraph{Gradient based techniques.} 

    \citealp{ismail2021improvingdeeplearninginterpretability} discuss the performance of gradient-based techniques by altering the training process of the model that is supposed to be explained. This is quite orthogonal to our approach, and could be seen as an alternative for the Jacobian regularizer we developed. Note that the unique property of our approach is that we aim to find a ``ground truth attribution map'' of the underlying data-generating process.
    

\balance
\section{Conclusions}
\label{sec:discussion}


    We proposed a theoretically grounded approach for estimating attribution maps in time-series data based on a newly formalized method: regularized contrastive learning with inverted neuron gradients. 
    We theoretically and empirically showed that this approach can outperform supervised baselines.
    Our theoretical results hold for fully converged contrastive learning models with infinite data, yet our finite data experiments show the effectiveness of our approach in limited data settings. 
    Although theoretically connecting the attribution score to model fit in limited data is complex, our work shows that the measured $R^2$ of recovering observable factors aligns with theory.
    %
    In neural recordings, many behaviors and sensory inputs -- such as animal motion, stimuli, and rewards -- are measurable, leading the field to focus on mapping neural dynamics to these behaviors. Our work considers a single truly latent (but potentially multi-dimensional) factor for attribution, while also supporting multiple latents that can be mapped to observable auxiliary variables. Notably, our method outperforms supervised baselines in this task (Figure~\ref{fig:latent-factors}). 
    
    Adopting the contrastive learning algorithm from \citet{hyvarinen2019nonlinear} could theoretically improve results by achieving identifiability up to permutations and point-wise bijective transforms, yet it requires stricter conditions and more complex training with a non-linear projection head. 
      
    Lastly, for practical applications, our chosen setup is quite versatile. During analysis it is always possible to break up the linear ambiguity between different latent factors by specifying the dimensions (or more broadly, the basis vectors of a latent subspace) to attribute to. This possibility exists with our inference framework, and allows attribution to multiple latent factors with this form of weak supervision, i.e., user input.

    Overall, our new method, \twemoji{1f993}\raisebox{-0.3ex}{\scriptsize{x}}CEBRA, demonstrates a significant advancement in time-series attribution, and we hope future work can leverage it to find biological insights -- how inputs concretely map to hidden underlying factors in neural dynamics.

    
\newpage
\balance
\section*{Acknowledgments}
    Funding was provided by a Google PhD Fellowship to StS and NIH 1UF1NS126566-01 and SNSF Grant No. TMSGI3\_226525 to MWM. StS acknowledges the EPFL EDNE, the IMPRS-IS T\"ubingen, and ELLIS PhD programs. MWM is the Bertarelli Foundation Chair of Integrative Neuroscience. The authors thank Celia Benquet for code testing and development.

\section*{Author contributions}

        Conceptualization: StS, MWM;
        Methodology: StS, MWM, RG, MF;
        Software: RG, StS, AF, MWM;
        Theory: StS;
        Formal analysis: StS, AF, RG, MF;
        Experiments: AF, RG, StS, MF;
        Writing--Original Draft: StS, MWM;
        Writing--Editing: all authors;
        Funding and supervision: MWM, StS.

