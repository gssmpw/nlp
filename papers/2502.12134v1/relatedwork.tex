\section{Related Works}
Early research on chain-of-thought (CoT) reasoning can be traced back to \citet{DBLP:conf/nips/Wei0SBIXCLZ22}, who first introduced a prompting strategy that guides LLMs through decomposed intermediate reasoning steps using few-shot exemplars. Concurrently, \citet{DBLP:conf/nips/KojimaGRMI22} demonstrated that LLMs are capable of zero-shot CoT reasoning by simply appending the phrase ``Let's think step by step'' to the prompt template. This discovery underscored the latent reasoning abilities of LLMs, even in the absence of explicit demonstrations.

Building upon these foundational works, the NLP community has extensively explored the potential of CoT reasoning. As summarized by \citet{DBLP:conf/acl/ChuCCYH0P00L24}, recent advancements in CoT methods can be broadly categorized into three areas: (1) {\it Prompt Construction}, which aims to optimize prompts for improved CoT reasoning \cite{DBLP:conf/nips/Wei0SBIXCLZ22,DBLP:conf/nips/KojimaGRMI22,DBLP:conf/iclr/0001Z0S23}; (2) {\it Topological Variants}, which leverage structured representations such as trees and graphs to enhance CoT reasoning~\cite{DBLP:conf/nips/YaoYZS00N23,DBLP:conf/aaai/BestaBKGPGGLNNH24}; and (3) {\it Enhancement Methods}, which introduce external strategies to further improve CoT reasoning, such as question decomposition~\cite{DBLP:conf/iclr/ZhouSHWS0SCBLC23} and self-consistency decoding \cite{DBLP:conf/iclr/0002WSLCNCZ23}. Despite the effectiveness of these approaches, the majority of existing CoT methods rely on discrete token-by-token generation, which imposes inherent constraints and limits their expressiveness.

To address the limitations of discrete language space, an effective approach is to leverage continuous representation space for reasoning. Coconut~\cite{DBLP:journals/corr/abs-2412-06769} introduces a Chain-of-Continuous-Thought, while CCoT~\cite{DBLP:journals/corr/abs-2412-13171} employs Compressed Chain-of-Thought, generating content-rich and continuous contemplation tokens. Heima~\cite{DBLP:journals/corr/abs-2501-19201} further advances this idea by utilizing a single continuous vector to represent compressed reasoning tokens in multi-modal tasks. However, both Coconut and CCoT rely on a language modeling objective for supervised fine-tuning, which is infeasible for state-of-the-art LLMs due to the catastrophic forgetting problem. Moreover, Heima underperforms compared to its backbone model, LLaVA-CoT~\cite{DBLP:journals/corr/abs-2411-10440}. These challenges underscore the need to develop methodologies that mitigate catastrophic forgetting in the application of continuous-space CoT reasoning.