@inproceedings{3dvista,
    title={3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment},
    author={Ziyu, Zhu and Xiaojian, Ma and Yixin, Chen and Zhidong, Deng and Siyuan, Huang and Qing, Li},
    booktitle={ICCV},
    year={2023}
}

@inproceedings{achlioptas2020referit3d,
  title={{ReferIt3D: Neural Listeners for Fine-grained 3D Object Identification in Real-world Scenes}},
  author={Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas},
  booktitle={ECCV},
  pages={422--440},
  year={2020},
  organization={Springer}
}

@inproceedings{chen2020scanrefer,
  title={{ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language}},
  author={Chen, Dave Zhenyu and Chang, Angel X and Nie{\ss}ner, Matthias},
  booktitle={ECCV},
  pages={202--221},
  year={2020},
  organization={Springer}
}

@article{chen2022language,
  title={Language conditioned spatial relation reasoning for 3d object grounding},
  author={Chen, Shizhe and Guhur, Pierre-Louis and Tapaswi, Makarand and Schmid, Cordelia and Laptev, Ivan},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={20522--20535},
  year={2022}
}

@article{chen2023teaching,
  title={Teaching large language models to self-debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}

@inproceedings{cheng2022binding,
  title={{Binding Language Models in Symbolic Languages}},
  author={Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and others},
  booktitle={ICLR},
  year={2023}
}

@article{csvg,
      title={Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction Problems}, 
      author={Yuan, Qihao and Zhang, Jiaming and Li, Kailai and Stiefelhagen, Rainer},
      journal={arXiv preprint arXiv:2411.14594},
      year={2024}
}

@inproceedings{dai2017scannet,
  title={{ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes}},
  author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
  booktitle={CVPR},
  pages={5828--5839},
  year={2017}
}

@article{fang2024transcrib3d,
  title={Transcrib3D: 3D Referring Expression Resolution through Large Language Models},
  author={Fang, Jiading and Tan, Xiangshan and Lin, Shengjie and Vasiljevic, Igor and Guizilini, Vitor and Mei, Hongyuan and Ambrus, Rares and Shakhnarovich, Gregory and Walter, Matthew R},
  journal={arXiv preprint arXiv:2404.19221},
  year={2024}
}

@inproceedings{feng2024naturally,
  title={Naturally supervised 3d visual grounding with language-regularized concept learners},
  author={Feng, Chun and Hsu, Joy and Liu, Weiyu and Wu, Jiajun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13269--13278},
  year={2024}
}

@inproceedings{gupta2023visual,
  title={Visual Programming: Compositional visual reasoning without training},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={14953--14962},
  year={2023},
  organization={IEEE}
}

@inproceedings{hsu2023ns3d,
  title={{NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations}},
  author={Hsu, Joy and Mao, Jiayuan and Wu, Jiajun},
  booktitle={CVPR},
  pages={2614--2623},
  year={2023}
}

@inproceedings{jain2022bottom,
  title={{Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds}},
  author={Jain, Ayush and Gkanatsios, Nikolaos and Mediratta, Ishita and Fragkiadaki, Katerina},
  booktitle={ECCV},
  pages={417--433},
  year={2022},
  organization={Springer}
}

@article{le2022coderl,
  title={Coderl: Mastering code generation through pretrained models and deep reinforcement learning},
  author={Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven Chu Hong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21314--21328},
  year={2022}
}

@article{li2023chain,
  title={Chain of code: Reasoning with a language model-augmented code emulator},
  author={Li, Chengshu and Liang, Jacky and Zeng, Andy and Chen, Xinyun and Hausman, Karol and Sadigh, Dorsa and Levine, Sergey and Fei-Fei, Li and Xia, Fei and Ichter, Brian},
  journal={arXiv preprint arXiv:2312.04474},
  year={2023}
}

@article{li2023scallop,
  title={Scallop: A language for neurosymbolic programming},
  author={Li, Ziyang and Huang, Jiani and Naik, Mayur},
  journal={Proceedings of the ACM on Programming Languages},
  volume={7},
  number={PLDI},
  pages={1463--1487},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{li2024r2g,
  title={R2G: Reasoning to Ground in 3D Scenes},
  author={Li, Yixuan and Wang, Zan and Liang, Wei},
  journal={arXiv preprint arXiv:2408.13499},
  year={2024}
}

@article{li2024seeground,
  title   = {SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding},
  author  = {Rong Li and Shijie Li and Lingdong Kong and Xulei Yang and Junwei Liang},
  journal = {arXiv preprint arXiv:2412.04383},
  year    = {2024},
}

@inproceedings{liang2023code,
  title={Code as policies: Language model programs for embodied control},
  author={Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={9493--9500},
  year={2023},
  organization={IEEE}
}

@article{ma2023eureka,
  title={Eureka: Human-level reward design via coding large language models},
  author={Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2310.12931},
  year={2023}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@inproceedings{xuvlm,
  title={VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding},
  author={Xu, Runsen and Huang, Zhiwei and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
  booktitle={8th Annual Conference on Robot Learning}
}

@inproceedings{yang2024llm,
  title={Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent},
  author={Yang, Jianing and Chen, Xuweiyi and Qian, Shengyi and Madaan, Nikhil and Iyengar, Madhavan and Fouhey, David F and Chai, Joyce},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7694--7701},
  year={2024},
  organization={IEEE}
}

@inproceedings{yuan2024visual,
  title={Visual programming for zero-shot open-vocabulary 3d visual grounding},
  author={Yuan, Zhihao and Ren, Jinke and Feng, Chun-Mei and Zhao, Hengshuang and Cui, Shuguang and Li, Zhen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20623--20633},
  year={2024}
}

@article{zhou2024programming,
  title={Programming every example: Lifting pre-training data quality like experts at scale},
  author={Zhou, Fan and Wang, Zengzhi and Liu, Qian and Li, Junlong and Liu, Pengfei},
  journal={arXiv preprint arXiv:2409.17115},
  year={2024}
}

