% This class has a lot of options, so please check deepmind.cls for more details.
% This is a minimal set for most needs.
\documentclass[11pt, a4paper, twocolumn, external, copyright]{dm}

% Omit dates for reproducibility.
\pdfinfoomitdate 1
\pdftrailerid{redacted}

% This avoids duplicate hyperref bookmark entries when using \bibentry (e.g. via \citeas).
\makeatletter
\renewcommand\bibentry[1]{\nocite{#1}{\frenchspacing\@nameuse{BR@r@#1\@extra@b@citeb}}}
\makeatother
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{kantlipsum, lipsum}
\usepackage{dsfont}
\usepackage{gdm-colors}
\usepackage{comment}
\usepackage{wrapfig}
\usepackage[absolute,overlay]{textpos}
\usepackage{amsmath, amssymb}
% \usepackage{algorithm}
% \usepackage{algorithmic}
\newcommand{\algorithmautorefname}{Algorithm}
\usepackage{wrapfig}
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}{Claim}
% \newtheorem{definition}[theorem]{Definition}
\newtheorem{definition}{Definition}
\renewcommand{\thedefinition}{D.\arabic{definition}}
% \newtheorem{conj}[theorem]{Conjecture}
\newtheorem{conj}{Conjecture}
\renewcommand{\theconj}{C.\arabic{conj}}
% Define a new page style (Flipbook)
\usepackage{fancyhdr}
\setlength{\footskip}{40pt} % Increase the footskip
\addtolength{\textheight}{-10pt} 
\renewcommand{\footrulewidth}{0pt} % horizontal footer
\pagestyle{fancy}
\usepackage{bm}
% \usepackage[outputdir=./]{minted}
% \newsavebox{\mintedbox}
\usepackage{listings}
\usepackage{xcolor}

% Define Python syntax highlighting style
\lstdefinestyle{pythonstyle}{
    language=Python,
    basicstyle=\ttfamily\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numberstyle=\tiny\color{gray},
    % numbers=left,
    stepnumber=1,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    showstringspaces=false,
    frame=single
}


\definecolor{burgundy}{RGB}{128, 0, 32} % RGB values for burgundy
\definecolor{jhublue}{RGB}{0, 45, 114}

\usepackage[authoryear, sort&compress, round]{natbib}
\hypersetup{
    colorlinks=true,      % Enable colored links
    citecolor=dmblue400,       % Color for citations
    linkcolor=dmblue400,        % Color for internal links
    urlcolor=black       % Color for URLs
}

\graphicspath{{figures/}}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}

\newcommand{\x}{\bm{x}}
\newcommand{\z}{\bm{z}}
\renewcommand{\a}{\bm{a}}
\renewcommand{\L}{\mathcal{L}}

\definecolor{gray}{gray}{.75}
\definecolor{humancolor}{gray}{.95}
\newcommand{\human}[1]{\cellcolor{humancolor}{#1}}

\definecolor{lightblue}{rgb}{0.85,0.9,1}
\colorlet{transparentblue}{lightblue!30}
\newcommand{\gpt}[1]{\cellcolor{transparentblue}{#1}}

\newcommand{\psnrdiff}{\ensuremath{\Delta_t\text{PSNR}}}
\newcommand{\jc}[1]{{\color{red}[JC: #1]}}
\newcommand{\chen}[1]{{\color{ForestGreen}[Chen : #1]}}
\newcommand{\daniel}[1]{{\color{red}[DK : #1]}}
\newcommand{\tlu}[1]{{\color{blue}[TaiMing : #1]}}
\newcommand{\ourmethod}{\textsc{EaSe}\hspace{0.1cm}}
\title{Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection}

\keywords{\small 3D Visual Grounding, Large Language Model, Code Generation}

\author{Boyu Mi}
\author{Hanqing Wang}
\author{Tai Wang}
\author{Yilun Chen}
\author{Jiangmiao Pang}
\affil{Shanghai AI Laboratory}


\begin{abstract}
{
\vspace{-0.3cm}
\small
{\hskip 2em} 3D visual grounding (3DVG) is challenging due to the need to understand 3D spatial relations.
While supervised approaches have achieved superior performance, they are constrained by the scarcity and high cost of 3D vision-language datasets.
Training-free approaches based on LLM/VLM agents eliminate the need for training data, but they incur prohibitive grounding time and token costs.
To address the challenges, we introduce a novel training-free symbolic framework for 3D visual grounding, namely \underline{E}volv\underline{a}ble \underline{S}ymbolic Visual Ground\underline{e}r (\ourmethod).
\ourmethod uses LLM-generated code to determine 3D spatial relations among objects and integrates VLMs to process their visual information.
\ourmethod also implements an automatic pipeline that evaluates and optimizes the quality of generated code.
Experimental results demonstrate that \ourmethod achieves 52.9\% accuracy on Nr3D dataset and 49.2\% Acc@0.25 on ScanRefer, ranking among the best training-free methods.
Moreover, it substantially reduces the grounding time and token cost, offering a balanced trade-off between performance and efficiency.
Code is available at \url{https://github.com/OpenRobotLab/EaSe}.
}

\end{abstract}

\banner{figures/teaser.pdf}
{
\small{
Comparision between two previous training-free 3DVG methods and our method(\ourmethod).
For a query,
agent based methods employ multimodal LLM to process scene information. They are more accurate, but their online LLM generation increases time consumption.
Visual programming (Visprog.) method uses offline annoated relation functions, thus reduces grounding time, but it doesn't perform well.
In contrast, \ourmethod utilizes offline LLM generation and optimization before grounding and improves relation functions to relation encoders. 
As a result, \ourmethod's accuracy is close to agents but it's  consumption is much lower.
}
}
\vspace{-1cm}


\begin{document}
\maketitle
\newpage
\clearpage
\twocolumn

\section{1. Introduction}
\label{sec:intro}
The 3D visual grounding (3DVG) task focuses on locating an object in a 3D scene based on a referring utterance. 
Numerous supervised methods have been proposed for 3DVG~\citep{hsu2023ns3d, jain2022bottom, huang2022multi, chen2022language, huang2024chat, 3dvista, bakr2023cot3dref, wu2023eda}. 
These methods learn representations of referring utterances, object attributes and spatial relations from large scale 3D vision-language training data with high-quality annotations and achieve state-of-the-art performances on 3DVG.
However, the scarcity of 3D vision-language datasets~\citep{chen2020scanrefer, achlioptas2020referit3d}, coupled with the high cost of their annotations, limits their applicability.
Furthermore, some supervised methods are trained on these closed-vocabulary datasets, restricting their applicability in open-vocabulary scenarios.

In recent years, large language models (LLMs) and vision-language models (VLMs) have shown remarkable capabilities in reasoning, code generation, and visual perception. 
Building on these advancements, open-vocabulary and zero-shot 3DVG agents~\citep{yang2023llmgrounder, xuvlm, fang2024transcrib3d, li2024seeground} are proposed.
These methods let LLMs perform numerical computing and reasoning on object locations in the text modality~\citep{yang2023llmgrounder, fang2024transcrib3d}, or let VLMs locate targets from scene scan images in the visual modality~\citep{xuvlm}.
Leveraging the reasoning and perceptual abilities of advanced LLMs and VLMs, these agents achieve superior accuracy compared to other training-free methods.
However, they rely on LLMs to produce lengthy responses (containing planning, reasoning, or self-debugging processes) for every referring utterance.
This online generation style results in significant costs in terms of grounding time and token usage (see \textcolor{red}{\texttt{Agents}} block in \autoref{fig:banner}).
In contrast, the visual programming method~\citep{yuan2024visual} utilizes LLM to generate a program which uses annotated relation functions and outputs the target object by executing the program. 
The generated program is short so its time and token consumption are much lower.
However, it has trouble considering many spatial relations in the referring utterance simultaneously~\citep{csvg}. 
This results in relatively low accuracy (see \textcolor{blue}{\texttt{Visprog.}} in \autoref{fig:banner}). 

To address these dual challenges of accuracy and cost, we propose \textbf{\ourmethod}, a novel training-free symbolic framework that integrates LLMs and VLMs for 3D visual grounding, balancing both accuracy and inference cost.
\ourmethod builds upon previous neuro-symbolic frameworks~\citep{hsu2023ns3d, feng2024naturally} and uses Python code generated and optimized by LLMs as spatial relation encoders (see ~\autoref{fig:framework}, block (b)). 
\ourmethod also employs a VLM to distinguish objects that differ only in visual appearance.
Specifically, \ourmethod parses the referring utterance into a symbolic expression which encapsulates all mentioned object categories and their spatial relations. 
Given positions of all objects in the scene, the spatial relation encoders generate relation features which can represent spatial relations between them.
Then an executor aggregates the symbolic expression, relation features and object categories to exclude most objects that do not match the referring utterance.
Finally, the VLM identifies the target object from images of the remaining candidate objects (see \autoref{fig:framework}, block (c)). 
For more accurate spatial relation encoders, we generate them through iterative optimization processes instead of directly prompting the LLM. 
We introduce test suites to evaluate spatial relation encoders. 
The test suites not only enable us to select better relation encoders from LLM responses but also allow the LLM to leverage failed test cases to optimize its code.
In contrast to the \textbf{online} generation of agent-based methods, our generation and optimization are performed \textbf{offline}, avoiding per-utterance code generation.
The spatial relation encoders are reused in all grounding processes, so \ourmethod has advantages on time and token cost (see \textcolor{green}{\texttt{Ours}} in \autoref{fig:banner}). 

We evaluate \ourmethod on the widely used ScanRefer~\citep{chen2020scanrefer} and Nr3D~\citep{achlioptas2020referit3d} datasets.
Experiment results show that {\ourmethod} achieves 52.9\% accuracy on Nr3D and 49.2\% Acc@0.25 on ScanRefer, matching the performance of agent-based approaches~\citep{xuvlm, fang2024transcrib3d} while offering significant advantages in grounding time and token cost. 
On Nr3D, {\ourmethod} outperforms VLMGrounder~\citep{xuvlm} in accuracy while requiring less than $1/20$ of the grounding time and less than $1/6$ of the token usage.
In addition,  {\ourmethod} significantly outperforms ZSVG3D~\citep{yuan2024visual} in accuracy with comparable grounding time and token cost. 
In conclusion, among various training-free 3DVG methods, {\ourmethod} strikes an excellent balance between accuracy and efficiency.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/framework.pdf}
    \caption{\textbf{Overview of \ourmethod}. Block (a) and (b) compare the difference of spatial relation encoders between \ourmethod and previous neuro-symbolic approaches. Spatial relation encoders compute relation features using object positions from the scene. 
    The executor executes the symbolic expression with relation features and gets candidate objects.
    Block (c) demonstrates a VLM selects the target from candidate objects given scan images containing them.
    }
    \label{fig:framework}
\end{figure*}

\section{2. Related Work}
\paragraph{3D Visual Grounding}
3D visual grounding (3DVG) aims to localize objects in 3D scenes based on natural language descriptions of appearance and spatial relations. 
Two dominant benchmarks, ScanRefer~\citep{chen2020scanrefer} and ReferIt3D~\citep{achlioptas2020referit3d}, leverage ScanNet~\citep{dai2017scannet} scenes to provide diverse object-utterance pairs. 
Supervised Methods typically train end-to-end models on annotated 3D vision-language data. 
\cite{jain2022bottom} integrates bottom-up object detection with transformer-based grounding, 
\cite{chen2022language} designs fine-grained neural networks to encode spatial relations.
Though achieving promising accuracy, these methods suffer from expensive data annotation dependency~\citep{3dvista}.
Neuro-Symbolic Methods~\citep{hsu2023ns3d, feng2024naturally, li2024r2g} attempt to mitigate data reliance by combining symbolic parsing with neural components. 
They parse referring utterances into symbolic expressions using LLMs and train neural networks as spatial relation encoders.
Unlike these approaches, \ourmethod completely avoids training on large-scale 3D datasets.
Training-free methods exploit pre-trained LLMs / VLMs for open-vocabulary 3DVG. 
\cite{yuan2024visual} uses LLMs to generate programs that call predefined functions to find the target object.
\cite{yang2024llm, fang2024transcrib3d} deploy LLM/VLM-based agents that analyze object appearances and locations and find the target.
\cite{xuvlm} uses VLMs and images from the scene to figure out the target object. 
Concurrently, \cite{csvg} proposes to replace the programming of \cite{yuan2024visual} by constraint satisfaction solving.
\cite{li2024seeground} parses landmark and perspective of the referring utterance and then uses VLMs to find the target object from a rendered image.
Compared to these methods, \ourmethod offers a superior balance of accuracy and efficiency, outperforming ~\cite{yang2024llm, xuvlm, fang2024transcrib3d} in grounding time and token cost while surpassing \cite{yuan2024visual} in accuracy.

\paragraph{LLM Programming}
LLMs demonstrate growing proficiency in generating executable code~\citep{roziere2023code} for precise mathematical reasoning~\citep{li2023chain}, robotics control~\citep{liang2023code}, tool use~\citep{gupta2023visual, yuan2024visual} or data cleaning~\citep{zhou2024programming}.
Recent work further explores code refinement via environmental feedback, such as RL training trajectories~\citep{ma2023eureka} or real-world execution errors~\citep{le2022coderl, chen2023teaching}.
In the 3DVG area, \cite{yuan2024visual, fang2024transcrib3d} also uses code to process spatial relations, but \ourmethod advances this paradigm by introducing test suites to automatically optimize code.

% \paragraph{Neuro-symbolic Reasoning}
% Neuro-symbolic systems integrate symbolic rule execution with neural perception, applied in domains from visual QA~\citep{li2023scallop} to SQL generation~\citep{cheng2022binding}. 
% For 3DVG, \cite{hsu2023ns3d} defines domain-specific languages with neural spatial relation encoders, \cite{feng2024naturally} further regularize the encoders by auxiliary losses.
% \ourmethod has the similar framework, but uses Python code as spatial relation encoders and avoids training.

\section{3. Method}
\label{sec:method}


\subsection{Problem Statement}

3D visual grounding tasks involve a scene, denoted as $\mathcal{S}$, represented by an RGB-colored point cloud containing $C$ points, where $\mathcal{S} \in \mathbb{R}^{C \times 6}$. Associated with this is an utterance $\mathcal{U}$ that describes an object within the scene $\mathcal{S}$. The objective is to identify the location of the target object $\mathcal{T}$ in the form of a 3D bounding box. In the ReferIt3D dataset~\citep{achlioptas2020referit3d}, bounding boxes for all objects are provided, making the visual grounding process a task of matching these bounding boxes to the scene $\mathcal{S}$. In contrast, the ScanRefer dataset~\citep{chen2020scanrefer} provides only the point cloud of the scene, requiring additional detection or segmentation modules to accomplish the grounding task.
\vspace{-0.3cm}


\subsection{Grounding Pipeline}
\label{sec:pipeline}
We adhere to the previous SOTA neuro-symbolic framework for 3DVG~\citep{hsu2023ns3d, feng2024naturally}. 
A semantic parser (LLM) converts $\mathcal{U}$ into a symbolic expression $\mathcal{E}$ in JSON. 
Spatial relation encoders compute the relation features such as \texttt{under} and \texttt{right} within $\mathcal{E}$.
Finally, the relation features, along with category features of object categories in $\mathcal{E}$, such as \texttt{chair} and \texttt{desk} are subsequently used to calculate the matching scores between $\mathcal{S}$ and the objects based on $\mathcal{E}$.

\paragraph{Semantic Parsing}
We employ GPT-4o~\citep{gpt4o} to parse $\mathcal{U}$ into JSON expression $\mathcal{E}$, which contains the categories and spatial relations in $\mathcal{U}$.
For example, the utterance ``chair near the table'' can be represented as: 
\begin{verbatim}
{"category": "chair", "relations":
[{"relation_name": "near", 
"objects": [{"category": "table"}]}]}
\end{verbatim}


Human-annotated natural language expressions exhibit diverse descriptions of relations, leading to a long-tail distribution of \textbf{relation\_name} in parsed expressions. 
To mitigate this, we define a set of common relation names and prompt LLM to select from them for $\mathcal{E}$ instead of using the original terms from $\mathcal{U}$. 

Based on the number of associated objects, the relations can be categorized into \texttt{unary}, \texttt{binary}, and \texttt{ternary}~\citep{feng2024naturally}. 
For simplicity, attributes that describe properties of a single object, such as ``large'' or ``at the corner'' are treated as special types of unary relations. We present our predefined set of relations along with their classifications in \autoref{tab:relation_cls}.

\begin{table}[h!]
\centering
\caption{Classification of all relations.}
\label{tab:relation_cls}
\begin{tabular}{@{}l l@{}} 
\toprule
Classification & Relations \\
\midrule
unary & large, small, high, low, on the floor, \\
& against the wall, at the corner \\
\hline 
binary & near, far, above, below, \\ 
 & left, right, front, behind \\
\hline 
ternary & between \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Feature Computing}
Our spatial relation encoders are Python code generated by LLMs. 
They take the objects' point clouds and positions as input and compute spatial relation features by explicit geometric calculations.
The category features are from a pretrained point cloud classifier.
Unary relation features $f_{\text{unary}} \in \mathbb{R}^{N}$ and category features $f_{\text{category}} \in \mathbb{R}^{N}$ measure the similarity between objects and their respective relations or categories, where $N$ is the number of objects in the scene. 
The features of the binary relation $f_{\text{binary}} \in \mathbb{R}^{N \times N}$ represent the likelihood that there are binary relations between all possible pairs of objects. 
For example, $f_{\text{near}}^{(i,j)}$ quantifies the probability that the $i$-th object is near the $j$-th object. 
Ternary features follow an analogous pattern for relations involving three objects.
% For instance, the element $f_{\text{near}}^{(i,j)}$ quantifies the probability that the $i$-th object is ``near" the $j$-th object. 
% The structure of ternary features follows a similar pattern, capturing ternary relations involving three objects.


\paragraph{Executor}
Our executor follows a similar design to \cite{hsu2023ns3d}. 
Given the symbolic expression $\mathcal{E}$ and features, the executor computes the matching score between objects and referring utterance $\mathcal{U}$.
Specifically, the category features $f_{\text{category}}$ are computed by an object classifier, and relation features are from our spatial relation encoders.
For each relation in $\mathcal{E}$'s \texttt{relations} field, the corresponding relation feature $f_{\text{relation}}$ is multiplied with $f_{\text{category}}$ of its related objects, yielding intermediate features $\{f_i \in \mathbb{R}^{N}\}_{i=1}^K$ (where $K$ is the number of relations). 
Finally, all intermediate features and $f_{\text{category}}$ are aggregated via the element-wise product to compute the final matching score. See Algorithm \ref{alg:execute} for more details.


% \paragraph{Execution}

% Our executor is similar to \cite{hsu2023ns3d}.
% The executor utilizes the symbolic expression $\mathcal{E}$ and the associated relation features to identify the target object $\mathcal{T}$. Since the elements in the features represent probabilities or corresponding relations/categories, logical conjunctions within $\mathcal{E}$ are modeled using product operations.

% For a given symbolic expression $\mathcal{E}$:
% The executor use $\mathcal{E}$ and associated features to identify $\mathcal{T}$.
% Since elements in the features represent the probabilities or corresponding relation or category, the logical conjunction in $\mathcal{E}$ can be represented through the product operation.
% For a symbolic expression $\mathcal{E}$, its category feature $f_{\text{category}}$ of calculated initially by a classifier. 
% The executor utilizes the symbolic expression $\mathcal{E}$ and the associated relation features to identify the target object $\mathcal{T}$. 
% Since the elements in the features represent probabilities of relations or categories, logical conjunctions in $\mathcal{E}$ are modeled through product operations. 
% Specifically, the category feature $f_{\text{category}}$ is computed by a pretrained object classifier. 
% Subsequently, the executor processes each relation individually of the \texttt{relations} field. 
% For the $i$-th relation, the relation feature $f_{\text{relation}}$ is computed and multiplied with $f_{\text{category}}$ of its related objects, resulting in $f_{i} \in \mathbb{R}^{N}$.
% After processing all relations, we obtain multiple relation features, each of size $\mathbb{R}^{N}$. They are aggregated with $f_{\text{category}}$ through an element-wise product to compute the final matching degree.


% \subsection{Relation Encoders}
% The sizes and positions of objects in 3D scenes are mathematically related to specific relations. For example, ``near" is related to the distance between objects, and ``large" is related to the volume of an object. The probabilities of these relations can be computed by code. For each spatial relationship, {\ourmethod} use Python code as its encoder.
% The generation and optimization processes of the codes are illustrated in part (a) of \autoref{fig:refinement}.
% In the initial generation of the code, the optimized code of relevant relation is retrieved from the code library as an in-context example(\cref{sec:prompt}). Multiple codes are sampled from the LLM, relation probability matrix from \texttt{forward} function (\cref{sec:schema}) is tested in the test suite (\cref{sec:training_data}). Test suite synthesis feedback according to each failure cases for LLM to optimize codes(\cref{sec:generation}). 
% Following sections explain more details about this process.

\subsection{Spatial Relation Encoders}
Sizes and positions of objects in 3D scenes inherently determine spatial relations. For example, the \texttt{near} relation depends on pairwise distances, while \texttt{large} is determined by object volumes. 
In \ourmethod, each spatial relation is handled by a dedicated Python class ($\S$~\ref{sec:schema}) that can compute its associated features given the object bounding boxes.

As illustrated in \autoref{fig:refinement}, our encoder generation involves three phases: (1) Retrieving in-context examples from semantically similar relations ($\S$~\ref{sec:prompt}), (2) Sampling multiple code implementations from LLMs, and (3) Validating candidates through unit tests ($\S$~\ref{sec:training_data}). When test failures occur, we automatically synthesize error messages that guide iterative refinement ($\S$~\ref{sec:generation}).

\subsubsection{Class Schema}
\label{sec:schema}
\ourmethod uses Python classes as spatial relation encoders. A class is initialized with the scene's point cloud data and object segmentation and provides two key methods: 
\texttt{\_init\_param}, which computes the necessary parameters for feature derivation. For instance, in the ``near'' class, it calculates distances between each pair of objects; \texttt{forward}, which performs numerical operations such as inversion or exponentiation and returns the relation feature.

\subsubsection{In Context Example}
\label{sec:prompt}

Adding in-context examples into the prompt can significantly improve the accuracy of responses from LLMs~\citep{brown2020language}.
To reduce human effort and provide suitable ICE for different relation encoders' generation, we introduce a semantic-based retrieval strategy. 
For example, relation encoders for ``near'' and ``far'' may both compute pairwise distances but differ only in the numerical processing, so the well-optimized relation encoder for ``near'' can serve as an in-context example for the generation of ``far''. We show the retrieval details in \autoref{fig:dag}.


\subsubsection{Unit Test}
\label{sec:training_data}
Since LLMs may not always generate correct code in a single attempt \citep{olausson2023self}, inspired by \cite{wu2024inference}'s finding that increased sampling enhances success probability, we design unit test suites to select the plausible relation encoders from multiple LLM responses. 

Take the binary relation ``above'' as an example. We collect a few triplets in the format of \texttt{target object, same class distractor, anchor object} from the training set, with each triplet serving as a unit test case.
The scale for each relation is small (less than 50 for most relations). 
In the generated relation feature for ``above'' $f$, if $f^{(distractor,anchor)}$ is larger than  $f^{(target,anchor)}$, the test is deemed to have failed, and an error message like \texttt{[target bbox] is above [anchor bbox] So metric value of [target bbox] "above" [anchor bbox] should be larger than the metric value of [distractor bbox] "above" [anchor bbox].} is synthesized. \autoref{feedback} provides an example of such an error message.

\subsubsection{Code Generation and Optimization}
\label{sec:generation}

As illustrated in \autoref{fig:refinement}, for any relation, we begin by prompting the LLM with a high-level task description, the relation name, and an in-context example retrieved from codes that are already optimized as $\S$~\ref{sec:prompt}. 
Then we sample $N_{\text{sample}}$ candidate encoders from LLM, where $N_{\text{sample}}$ is a configurable hyperparameter. 
Next, each generated code is tested using the unit tests defined in \S~\ref{sec:training_data}. We select the $top_k$ codes that pass the most test cases and subject them to an optimization phase. 
During the optimization phase, the LLM receives the initial prompt, the code to be optimized, and the error message produced by the test suite. It then revises the code according to these errors.
This test and optimization procedure is repeated for up to $N_{\text{iter}}$ iterations. Ultimately, we adopt the code that achieves the highest pass rate across all test cases. In the event of a tie, we select the code that underwent more optimization steps. The optimization and selection algorithm is shown in Algorithm \ref{alg:code_optimization}.




\begin{algorithm}[ht!]
\caption{Code Generation and Optimization}
\label{alg:code_optimization}
\DontPrintSemicolon
\SetAlgoVlined
\SetKwInOut{Input}{\textbf{Require}}
\SetKwInOut{Output}{\textbf{Output}}
\SetKwInOut{Hyperparameters}{\textbf{Hyperparameters}}

\Input{relation name $R$, relation name $G$, code library $L$, test cases $C$, LLM $\texttt{LLM}$, test suites $T$, initial prompt $\texttt{prompt}$}
\Output{\texttt{best\_code}}

\Hyperparameters{search iteration $N$, sample number $M$, optimizing example number $top_k$}

\BlankLine
\setcounter{AlgoLine}{0}

% --------------------------------------------------------------
% Step 1: Retrieve in-context example and initialize prompt
% --------------------------------------------------------------
$\texttt{example} \leftarrow \texttt{retrieve}(G, R)$ 

$\texttt{init\_prompt} \leftarrow \texttt{prompt} + \texttt{example}$

% --------------------------------------------------------------
% Step 2: Sample initial candidates from the LLM
% --------------------------------------------------------------
$F_1, \dots, F_M \leftarrow \texttt{LLM}(R, \texttt{init\_prompt})$

% --------------------------------------------------------------
% Step 3: Evaluate all candidate codes
% --------------------------------------------------------------
\For{$j \leftarrow 1 \dots M$}{
    $acc_j, err_j \leftarrow T(F_j)$  
    \tcp*[r]{Test each code.}
}

$\texttt{max\_acc} \leftarrow \max \bigl(\{acc_1, \dots, acc_M\}\bigr)$ 

$\texttt{best\_code} \leftarrow F_{\arg\max(\{acc_1, \dots, acc_M\})}$

% --------------------------------------------------------------
% Step 4: Select top-k candidates for refinement
% --------------------------------------------------------------
$\texttt{TopK} \leftarrow \text{SelectTopK}\bigl(\{(F_j, acc_j)\}_{j=1}^M, K\bigr)$

% --------------------------------------------------------------
% Step 5: Iterative refinement of top-k candidates
% --------------------------------------------------------------
\For{$i \leftarrow 2 \dots N$}{
    
    $\texttt{results} \leftarrow []$
    
    \For{$j \leftarrow 1 \dots K$}{
        $(F_{\text{old}}, err_{\text{old}}) \leftarrow \texttt{TopK}[j]$
        
        % Combine code-to-refine with error info
        $\texttt{prompt}_{\text{ref}} \leftarrow \texttt{init\_prompt} + F_{\text{old}} + err_{\text{old}}$
        
        % Sample M new candidates using refined prompt
        $F_1, \dots, F_M \leftarrow \texttt{LLM}(R, \texttt{prompt}_{\text{ref}})$
        
        \For{$k \leftarrow 1 \dots M$}{
            $\texttt{results}.\texttt{append}(F_k)$
        }
    }
    
    % ----------------------------------------------------------
    % Step 6: Evaluate newly refined candidates
    % ----------------------------------------------------------
    $\texttt{eval\_results} \leftarrow []$
    
    \ForEach{$F_k \in \texttt{results}$}{
        $acc_k, err_k \leftarrow T(F_k)$
        
        \If{$acc_k = 1$}{
            \Return $F_k$ 
        }
        
        \If{$acc_k > \texttt{max\_acc}$}{
            $\texttt{max\_acc} \leftarrow acc_k$

            $\texttt{best\_code} \leftarrow F_k$
        }
        
        $\texttt{eval\_results}.\texttt{append}\bigl((F_k, acc_k, err_k)\bigr)$
    }
    
    % ----------------------------------------------------------
    % Step 7: Update TopK for the next iteration
    % ----------------------------------------------------------
    $\texttt{TopK} \leftarrow \text{SelectTopK}(\texttt{eval\_results}, K)$
}

% --------------------------------------------------------------
% Step 8: Store and output the best code
% --------------------------------------------------------------
$L \leftarrow L \cup \{\texttt{best\_code}\}$ \\
\Return{\texttt{best\_code}}
\end{algorithm}


\subsection{VLM Decision}
\label{sec:vlm}
The visual information, like descriptions on color or shape in utterances, can be essential for accurate grounding, particularly for natural datasets like Nr3D~\citep{achlioptas2020referit3d} and ScanRefer~\citep{chen2020scanrefer}.
When two candidate objects share a similar class and spatial position, visual perception is required to distinguish between them.

Following \citet{xuvlm}, we incorporate GPT-4o to identify the target object from a set of candidates by utilizing 2D images from ScanNet~\citep{dai2017scannet} as additional context. 

Specifically, we select the top five objects based on the scores from the relation encoders and retain those whose logits exceed a chosen threshold as final candidates. Next, we retrieve the corresponding images from ScanNet~\citep{dai2017scannet} by matching each candidate's point cloud to camera parameters, thereby finding images that include the candidate objects. Out of these, we select eight images that contain the largest projected bounding box area of each candidate. We then annotate each image with object IDs and stitch them together in a $4 \times 2$ grid. Finally, we prompt GPT-4o to identify the target object from stitched images. By integrating these visual cues, the VLM decision step effectively disambiguates candidates that appear similar in terms of class and spatial attributes, yielding more accurate results.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{ figures/new_framework.pdf}
    \vspace{-0.3cm}
    \caption{Overview of the generation and optimization process of relation encoders.}
    \label{fig:refinement}
\end{figure}

\section{4. Experiments}
\label{sec:exp}
\subsection{Experimental Settings}
\paragraph{Dataset}
We conduct experiments on the Nr3D subset of ReferIt3D~\citep{achlioptas2020referit3d} dataset and ScanRefer~\citep{chen2020scanrefer}. ReferIt3D has 2 subsets: Nr3D and Sr3D. The Nr3D subset utterances contain human-annotated utterances and the Sr3D contains synthesized ones. 
Based on the number of same-class distractors, the dataset can be categorized into ``easy'' and ``hard'' subsets. The easy subset has a single distractor, and the hard subset has multiple distractors.
The dataset can also be split into ``view dependent'' and ``view independent'' subsets according to the referring utterance. 
Ground truth object bounding boxes are given in the ReferIt3D default evaluation setting. 
Therefore, the metric is an exact match between the predicted bounding box and the target bounding box.
In ScanRefer, no GT object mask is provided; the evaluation metric is the intersection over union (IoU) value between the predicted bounding box and the GT bounding box. We use the Acc@0.25 metric here. 


\paragraph{Implementation Details}
For code optimization (\S\ \ref{sec:generation}), we set $N_{sample}$ and $N_{iter}$ to 5, $top_k$ to 3.
We mainly use \texttt{gpt-4o-2024-08-06} model with a temperature of 1.0 and top\_p of 0.95. 
For a fair comparison, we use the object classification results from ~\cite{yuan2024visual} for the evaluation of ReferIt3D.
For evaluation of ScanRefer, we use the object detection and classification results of~\citep{jain2024odin}. (Experiments show that \ourmethod has similar accuracy when using object detection and classification results from~\cite{yuan2024visual}.)
For VLM decision-making, we use the same temperature and top\_p values as in~\cite{xuvlm}. The threshold (\S\ \ref{sec:vlm} is set to 0.9 for Nr3D and 0.1 for ScanRefer.

\paragraph{Baselines}
We compare \ourmethod against both supervised and training-free methods, evaluating accuracy, grounding time, and token cost. The supervised baselines include BUTD-DETR~\citep{jain2022bottom}, Vil3DRel~\citep{chen2022language}, 3D-VisTA~\citep{3dvista}, and CoT3DRef~\citep{bakr2023cot3dref}, while the training-free approaches include ZSVG3D~\citep{yuan2024visual}, VLM-Grounder~\citep{xuvlm}, and Transcrib3D~\citep{fang2024transcrib3d}. 

On the Nr3D dataset, Transcrib3D~\citep{fang2024transcrib3d} uses ground-truth object labels, providing an advantage over methods which rely on predicted labels. Additionally, the neuro-symbolic method NS3D~\citep{hsu2023ns3d} operates under a more limited evaluation protocol; therefore, we compare \ourmethod with them under their specific settings.

\subsection{Quantitative Results}

\paragraph{ReferIt3D}
~\autoref{tab:nr3d} presents the results on Nr3D. Compared to other training-free baselines, \ourmethod (without VLM) achieves higher overall accuracy than both ZSVG3D~\citep{yuan2024visual} and VLM-Grounder~\citep{xuvlm}. When integrated with VLM, \ourmethod further narrows the gap in overall performance relative to the supervised BUTD-DETR~\citep{jain2022bottom}, especially on the view-dependent (VD) subset. However, it still lags behind other more recent supervised methods, which are trained on large-scale task-specific 3D datasets.
We further evaluate \ourmethod under the experimental settings of \cite{fang2024transcrib3d} and \cite{hsu2023ns3d} respectively. In \cite{fang2024transcrib3d}'s setting, the ground truth (GT) object labels are utilized for more accurate category-level object recognition. \ourmethod slightly underperforms \cite{fang2024transcrib3d} by 2.4\% (67.8\% vs. 70.2\%), but \ourmethod has significant advantages in grounding time and token cost.
Under the setting of ~\cite{hsu2023ns3d}, \ourmethod outperforms it by 7.4\% without requiring any training.

\paragraph{ScanRefer}
Table~\ref{tab:scanrefer} shows the results on ScanRefer. \ourmethod outperforms ZSVG3D~\citep{yuan2024visual} by $12.8\%$ (49.2\% vs.\ 36.4\%).
Moreover, the performance gap between \ourmethod (49.2\%) and \cite{xuvlm} (51.6\%), \cite{fang2024transcrib3d} (51.3\%) is relatively small, demonstrating that \ourmethod balances cost with competitive accuracy.

% \paragraph{Grounding Cost}

% ~\autoref{tab:efficiency} compares the average grounding time and token costs of various training-free methods on randomly sampled a subset of the Nr3D.
% Agent-based methods~\citep{xuvlm, fang2024transcrib3d} consume significantly more time and tokens (27.0s and 50.3s respectively) compared to \ourmethod (w/o VLM) (2.1s).

% In contrast, methods like \ourmethod (w/o VLM) and ZSVG3D~\citep{yuan2024visual} only require a single call to the LLM for semantic parsing with a short prompt, leading to lower time and token consumption during inference, and \ourmethod significantly outperforms ZSVG3D on accuracy. 
% When integrated VLMs, \ourmethod still achieves more than three times faster grounding and significantly reduced token consumption compared to agent-based methods~\citep{xuvlm, fang2024transcrib3d}.

% In conclusion, the above quantitative results highlight that \ourmethod effectively balances performance and efficiency, achieving leading accuracy alongside exceptional computational cost savings among training-free methods.

\paragraph{Grounding Cost}

~\autoref{tab:efficiency} compares the average grounding time and token costs of various training-free methods on a randomly sampled subset of the Nr3D dataset. Agent-based methods~\citep{xuvlm, fang2024transcrib3d} exhibit significantly higher time and token consumption (27.0s and 50.3k tokens, respectively) compared to \ourmethod (without VLM) (2.1s and 3.2k tokens).
\ourmethod (without VLM) and ZSVG3D~\citep{yuan2024visual} have much lower costs and \ourmethod demonstrates a significant improvement in accuracy over ZSVG3D~\citep{yuan2024visual}. 
Even when integrated with a VLM, \ourmethod maintains a more than threefold reduction in grounding time and token consumption compared to agent-based methods~\citep{xuvlm, fang2024transcrib3d}.

These quantitative results underscore the ability of \ourmethod to effectively balance performance and efficiency, achieving competitive accuracy while offering substantial computational cost savings compared to other training-free methods.

\begin{table}[t]
\caption{Grounding time and token costs on Nr3D. 
\ourmethod has significant advantage, especially when compared to agent-based methods (VLM-Grounder and Transcrib3D).
$\dagger$: evaluated on a subset having 250 samples. }
  \centering
  \label{tab:efficiency}
  \begin{tabular}{lcc}
    \hline
    \hline
    \multicolumn{1}{l}{Method}  & \multicolumn{1}{c}{\bf Time/s} & \multicolumn{1}{c}{\bf Token}
    \\ \hline
    ZSVG3D$^*$  & 2.4 & 2.5k\\ 
    VLM-Grounder$\dagger$  & 50.3 & 8k\\
    Transcrib3D & 27.0 & 12k\\
    \ourmethod (w/o VLM) & 2.1 & 1.2k \\
    \ourmethod  & 7.7 (+5.6) & 3.1k (+1.9k)\\ 
    \hline
  \end{tabular}
\end{table}



\begin{table}[t]
\caption{Performances on ScanRefer. \ourmethod has close performance with VLM-Grounder and BUTD-DETR. $\dagger$: evaluated on a subset having 250 samples. }
  \centering
  \label{tab:scanrefer}
  \begin{tabular}{lc}
    \hline
    Method & Acc@0.25 \\
    \hline
    BUTD-DETR & 52.2 \\
    VLM-Grounder$\dagger$ & 51.6 \\
    Transcrib3D$^*$ & 51.3 \\
    \ourmethod & 49.2 \\
    ZSVG3D & 36.4 \\
    \hline
  \end{tabular}
\end{table}

\begin{table*}[h]
\centering
\caption{Performances on Nr3D. VD and VID stand for view-dependent and view-independent, respectively. We only report time and token consuming for training-free methods which involve LLM or VLM. \ourmethod outperforms other training-free baselines in overall performance, and has significant advantages in terms of time and token consumptions.
For comparison with Transcrib3D, we use GT object labels. For comparison with NS3D, we evaluate \ourmethod on the same subset as NS3D.
$\dagger$: VLM-Grounder is evaluated on a subset having 250 samples.
* : we re-run ZSVG3D using GPT-4o.
}
\begin{center}
\label{tab:nr3d}
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{l}{Method}  &\multicolumn{1}{c}{\bf Overall} &\multicolumn{1}{c}{\bf Easy} &\multicolumn{1}{c}{\bf Hard} &\multicolumn{1}{c}{\bf VD} &\multicolumn{1}{c}{\bf VID} 
\\ \hline
ViL3DRel & 64.4 & 70.2 & 57.4 & 62.0 & 64.5\\ 
CoT3DRef &  64.4 & 70.0 & 59.2 & 61.9 & 65.7\\ 
3D-VisTA & 64.2 & 72.1 & 56.7 & 61.5 & 65.1\\ 
BUTD-DETR & 54.6 & 60.7 & 48.4 & 46.0 & 58.0\\ 
\hline
ZSVG3D$^*$  & 40.2 & 49.1 & 31.1 & 37.8 & 41.6\\ 
VLM-Grounder$\dagger$  & 48.0 & 55.2 & 39.5 & 45.8 & 49.4\\

\ourmethod (w/o VLM) & 50.7 & 58.7 & 43.0 & 45.6 & 53.2\\
\ourmethod  & 52.9 & - & - & - & - \\ 
\hline
Transcrib3D  & 70.2 & 79.7 & 60.3 & 60.1 & 75.4\\
\ourmethod (w/o VLM) &  65.7 & 75.6 & 56.2 & 58.7 & 69.1\\ 
\ourmethod  & 67.8 & - & - & - & -\\ 
\hline
NS3D & 52.8 & - & - & - & - \\ 
\ourmethod (w/o VLM) & 60.2  & - & - & - & - \\
\hline
\hline
\end{tabular}
\end{center}
\end{table*}



\subsection{Qualitative Results}
\paragraph{Scene Visualization}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{ figures/qualitive.pdf}
    \caption{ Visualization of the grounding process. Anchor (the door) are marked with \textbf{red circles}. Objects that strongly match the conditions are highlighted in \textbf{green}, with brighter shades indicating higher matching scores. }
    \label{fig:scene_vis}
\end{figure*}

% In \autoref{fig:scene_vis}, we present two step-by-step grounding process of \ourmethod, illustrating how the final grounding results are constructed through combinition of multiple conditions within the referring utterance. 
\autoref{fig:scene_vis} illustrates a grounding process of \ourmethod, demonstrating how the final grounding result is constructed through the combination of multiple conditions within the referring utterance.
For example, the utterance ``When facing the door, it’s the shelf above the desk on the right'' can be understood as following four steps, progressing from left to right in the figure.
First, objects \texttt{right of the door} are identified using the category feature ``door'' and the relation feature ``right''.
Next, the \texttt{desk right of the door} is located from this set using the category feature ``desk''.
Subsequently, objects satisfying the joint condition \texttt{above desk and on door’s right} are identified.
Finally, the target ``shelf'' is grounded from them.
More visualization results can be found in the \autoref{fig:more_vis}.

\subsection{Ablation Study}
\label{sec:ablation}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{ figures/ablation.pdf}
    \caption{The accuracy curves of different variants. The x-axis is the generation number of the code. The y-axis is the normalized accuracy. }
    \label{fig:ablation}
    % \vspace{-10pt}
\end{figure*}

We conduct ablation studies to investigate the impact of various components during the code generation and optimization processes, evaluating three different variants.
We choose to analyze six relations that required multiple optimization iterations. (For relatively simple relations like ``small'', the generated code passes all unit tests in the first generation, so there is no optimization.)

The three variants are:
Variant 1 direct prompts LLMs for multiple code, selecting the one with the highest pass rate.
Variant 2 replaces the error message (see \S \ref{sec:generation}), which includes specific failure cases, with a general optimization instruction that omits failure cases.
Variant 3 keeps all components except in-context examples.
For the relations that no in-context example is used (``left'', ``above'', and ``corner''), variant 3 is identical to \ourmethod, so we only plot variant 1 and 2 on the corresponding subplots.
To control for the impact of the generation results of the first iteration, we use the same results of iteration 0 across variant 2 and variant 3.
In all three variants, no in-context example ($\S$\ref{sec:prompt}) is used.

\autoref{fig:ablation} illustrates the results of the ablation study; different variants are represented by lines of different colors. 
The horizontal axis represents the number of iterations.
The vertical axis shows the normalized accuracy on test examples associated with the relation.
The effect of optimization is evident in variant 1: without optimization, LLMs fail to produce plausible spatial relation encoders for most relations, except ``corner'' and ``between''. Variant 2 demonstrates the effect of optimization: by incorporating simple optimization, there is a noticeable performance improvement compared to variant 1. However, LLMs still struggle with relations except ``above'' Variant 3 highlights the effect of error messages (see \S\ \ref{sec:training_data}). 
By using specific failure cases in error messages, LLMs are able to generate plausible spatial relation encoders for most relations. 
For relations like ``right'', ``between'' and ``below'' which use in-context examples, variant 3 shows a significant performance gap in initial iterations, underscoring the impact of in-context examples.

\section{5. Conclusion}
In this work, we propose \ourmethod, a training-free 3DVG method that uses Python code to encode spatial relations and an automatic pipeline for their  generation and optimization. We leverage knowledge from LLMs to create spatial relation encoders, circumventing the need for human annotation or supervised learning. 
\ourmethod eliminates the need for large-scale data and offers promising advantages in accuracy and grounding cost compared to other training-free methods.

There are some limitations in \ourmethod. In referring utterances, there are diverse linguistic elements beyond object categories and spatial relations. Our method, focusing primarily on these core components, may struggle with non-relation words like ``second from left'' in complex cases.
Empirically, we observe that object bounding boxes alone are insufficient for precise spatial encoding. For simplicity, we currently ignore object shapes, orientations, and different areas within the scene such as bathroom. Integrating richer scene information into LLMs and VLMs is an important avenue for future exploration.


\clearpage
{
\small
\bibliographystyle{abbrvnat}
\nobibliography*
\bibliography{refs}
}
\input{appendix}
\end{document}
