\section{Related Work}
\paragraph{3D Visual Grounding}
3D visual grounding (3DVG) aims to localize objects in 3D scenes based on natural language descriptions of appearance and spatial relations. 
Two dominant benchmarks, ScanRefer~\citep{chen2020scanrefer} and ReferIt3D~\citep{achlioptas2020referit3d}, leverage ScanNet~\citep{dai2017scannet} scenes to provide diverse object-utterance pairs. 
Supervised Methods typically train end-to-end models on annotated 3D vision-language data. 
\cite{jain2022bottom} integrates bottom-up object detection with transformer-based grounding, 
\cite{chen2022language} designs fine-grained neural networks to encode spatial relations.
Though achieving promising accuracy, these methods suffer from expensive data annotation dependency~\citep{3dvista}.
Neuro-Symbolic Methods~\citep{hsu2023ns3d, feng2024naturally, li2024r2g} attempt to mitigate data reliance by combining symbolic parsing with neural components. 
They parse referring utterances into symbolic expressions using LLMs and train neural networks as spatial relation encoders.
Unlike these approaches, \ourmethod completely avoids training on large-scale 3D datasets.
Training-free methods exploit pre-trained LLMs / VLMs for open-vocabulary 3DVG. 
\cite{yuan2024visual} uses LLMs to generate programs that call predefined functions to find the target object.
\cite{yang2024llm, fang2024transcrib3d} deploy LLM/VLM-based agents that analyze object appearances and locations and find the target.
\cite{xuvlm} uses VLMs and images from the scene to figure out the target object. 
Concurrently, \cite{csvg} proposes to replace the programming of \cite{yuan2024visual} by constraint satisfaction solving.
\cite{li2024seeground} parses landmark and perspective of the referring utterance and then uses VLMs to find the target object from a rendered image.
Compared to these methods, \ourmethod offers a superior balance of accuracy and efficiency, outperforming ~\cite{yang2024llm, xuvlm, fang2024transcrib3d} in grounding time and token cost while surpassing \cite{yuan2024visual} in accuracy.

\paragraph{LLM Programming}
LLMs demonstrate growing proficiency in generating executable code~\citep{roziere2023code} for precise mathematical reasoning~\citep{li2023chain}, robotics control~\citep{liang2023code}, tool use~\citep{gupta2023visual, yuan2024visual} or data cleaning~\citep{zhou2024programming}.
Recent work further explores code refinement via environmental feedback, such as RL training trajectories~\citep{ma2023eureka} or real-world execution errors~\citep{le2022coderl, chen2023teaching}.
In the 3DVG area, \cite{yuan2024visual, fang2024transcrib3d} also uses code to process spatial relations, but \ourmethod advances this paradigm by introducing test suites to automatically optimize code.

% \paragraph{Neuro-symbolic Reasoning}
% Neuro-symbolic systems integrate symbolic rule execution with neural perception, applied in domains from visual QA~\citep{li2023scallop} to SQL generation~\citep{cheng2022binding}. 
% For 3DVG, \cite{hsu2023ns3d} defines domain-specific languages with neural spatial relation encoders, \cite{feng2024naturally} further regularize the encoders by auxiliary losses.
% \ourmethod has the similar framework, but uses Python code as spatial relation encoders and avoids training.