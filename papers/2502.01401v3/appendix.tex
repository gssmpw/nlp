\newpage
\appendix
\twocolumn

\section{Prompts}

In this section, we show the prompts we used.

\setcounter{lstlisting}{0}
\lstinputlisting[basicstyle=\fontfamily{\ttdefault}\scriptsize, breaklines=true,caption={Prompt for semantic parsing}, label={prompt:semantic_parsing}]{prompts/parsing.txt}

\lstinputlisting[basicstyle=\fontfamily{\ttdefault}\scriptsize, breaklines=true,caption={An example of initial prompt for relation encoder generation}, label={prompt:initprompt}]{prompts/init.txt}

\lstinputlisting[basicstyle=\fontfamily{\ttdefault}\scriptsize, breaklines=true, caption={An example for feedback message}, label={feedback}]{prompts/feedback.txt}

\lstinputlisting[basicstyle=\fontfamily{\ttdefault}\scriptsize, breaklines=true,caption={The general prompt for self-reflection in variant 2 of ablation studies.}, label={prompt:selfoptim}]{prompts/self-refine.txt}



\section{Implementation Details}
\subsection{Definition of Symbolic Expression}
\label{sec:symbolic_expression}

The semantic parser translates the referring utterance $\mathcal{U}$ into a JSON-based symbolic expression $\mathcal{E}$ comprising the following elements:

\begin{itemize}
    \item \textbf{Category}: A string indicating the category of the target object referenced in $\mathcal{U}$.

    \item \textbf{Relations}: A list specifying spatial constraints relative to the target object. Each entry in this list contains:
    \begin{itemize}
        \item \textbf{relation\_name}: A string identifying the spatial relation in $\mathcal{U}$ (e.g., ``near,'' ``above'').
        \item \textbf{anchors}: A list of objects that share the given spatial relation with the target object. Each object is represented as its own JSON entity.
        \item \textbf{negative}: A boolean value which, if set to \textit{true}, denotes that the target object \textbf{should not} exhibit the specified spatial relation.
    \end{itemize}
\end{itemize}

\section{Definition of Symbolic Expression}
\label{sec:retreiver}
\subsection{Example of Relation Encoder}

An example of relation encoder for relation ``Near" is shown in \autoref{fig:encoder}. 
The class is initialized using the bounding boxes of all objects in the scene.  
he function \texttt{\_init\_params} computes the distances between all object pairs. The \texttt{forward} function calculates the relation feature for "Near" by performing numerical operations on these distances.

\begin{figure}
\centering
\begin{lstlisting}[style=pythonstyle]

class Near:
    def __init__(
        self, 
        object_locations: torch.Tensor) -> None:
        """
        Args:
            object_locations: torch.Tensor, shape (N, 6), 
            N is the number of objects in the scene.

        """
        self.object_locations = object_locations.to(DEVICE)
        self._init_params()

    def _init_params(self) -> None:
        """
        Computing some necessary parameters about `Near` relation and 
        initialize `self.param`.
        """
        # Based on average size to calculate a baseline distance
        sizes = self.object_locations[:, 3:]
        self.avg_size_norm = sizes.mean(dim=0).norm().to(DEVICE)

    def forward(self) -> torch.Tensor:
        """
        Return a tensor of shape (N, N), 
        where element (i, j) is the metric value  
        of the `Near` relation between object  
        i and object j. 
        """
        centers = self.object_locations[:, :3]
        sizes = self.object_locations[:, 3:]
        # Calculate the pairwise distances between object 
        centers in a vectorized manner
        diff = centers.unsqueeze(1) - centers.unsqueeze(0)
        distances = diff.norm(dim=2)
        # Calculate the "nearness" metric based 
        on distances and average size norm
        nearness_metric = torch.exp(-distances / 
                        (self.avg_size_norm + 1e-6))
        
        # Set diagonal to zero since an object 
        cannot be near itself
        nearness_metric.fill_diagonal_(0)
        return nearness_metric.to(DEVICE)
\end{lstlisting}
\caption{Example relation encoder for ``Near".}
\label{fig:encoder}
\end{figure}




% \subsection{Executor Details}
% \paragraph{relation lookup table}
% Some relations are not in the pre-defined set, but they have similar meaning with some relations in the set. So we build an lookup table from relation names in the set to parsed relation names for relation translation and ignore all undefined relations.
\subsubsection{In Context Example Selection}
The graph used for select in the context example for relation encoder generation is shown in \autoref{fig:dag}. 


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{dag_graph.pdf}
    \caption{The graph for in context example selection. An edge from node A to node B means that the encoder for relation A is used as an in context example when generating for relation B.}
    \label{fig:dag}
\end{figure*}

\paragraph{Category Features}
\cite{yuan2024visual} provides the predicted classified label for each object. For the category feature $f_{\text{category}} \in \mathbb{R}^{N}$ ($N$ is the number of objects), we compute the cosine similarity $sim \in \mathbb{R}^{N}$ between the \texttt{category} and the predicted labels using CLIP \citep{radford2021learning}.
Subsequently, we define the category feature as:
$$
f_{\text{category}} = softmax(100 \times sim)
$$
To optimize computation, we cache features for different categories in a dictionary $C$.
\vspace{-0.3cm}


\paragraph{Relation Features}
We use the object bounding boxes in the scene to initialize the relation encoders and then call the \texttt{forward()} function to compute the corresponding relation feature, \texttt{f\_rel}. These relation features are also cached in a dictionary $R$ for efficient reuse.

\paragraph{Execution Details}
The detailed execution algorithm is presented in Algorithm \ref{alg:execute}, utilizing the precomputed category features and relation features. The \texttt{Execute} function runs recursively to compute the $\texttt{matching\_score} \in \mathbb{R}^{N}$ ($N$ is the number of objects).

\section{More Quantitative Results}
\label{appendix:sr3d_results}
\paragraph{NS3D}
We show evaluation results in NS3D\citep{hsu2023ns3d} in \autoref{tab:nr3d}
.
NS3D can only learn concepts (\textit{e.g.} relation name, category name) from the training set and its parsing results of Nr3D contain more than 5,000 concepts, resulting in a long-tailed problem. So it selects a subset containing 1,041 test examples, which only contains the same concepts as Sr3D, the dataset it is mainly trained on. On the NS3D subset, \ourmethod has a large improvement over Ns3D (more than 7\%), which shows the advantage of \ourmethod for processing natural grounding tasks.

\paragraph{Sr3D}
In \autoref{tab:sr3d}, we show the evaluation results on NS3D, a subset of Nr3D, proposed by~\cite{hsu2023ns3d}. 
Even not using training data of Sr3D, \ourmethod still achieves comparable performance with~\cite{hsu2023ns3d} on both settings (w/ and w/o GT labels).
We show evaluation results on Sr3D, a subset of \cite{achlioptas2020referit3d} in \autoref{tab:sr3d}.
In the standard setting, 
if using GT object labels, the accuracy of our method (w/o VLM) on Sr3D is 95.3\%, the performance of NS3D and \cite{fang2024transcrib3d} are 96.9\% and 98.4\%. So \textbf{we believe that the bottleneck of Sr3D performance is object detection and classification rather than spatial relation understanding} because its relation annotation is synthesized by relatively simple functions. 
So we mainly focus on natural benchmarks (Nr3D and Scanrefer) which have complex and real spatial relations. 

\begin{table*}[t]
\centering
\caption{Performance on Sr3D.}
\begin{center}
\label{tab:sr3d}
\begin{tabular}{lccc}
\hline
\multicolumn{1}{l}{Method}  &\multicolumn{1}{c}{\bf Overall}   &\multicolumn{1}{c}{\bf NS3D} 
\\ \hline \\
BUTD-DETR& 67.0  & -\\ 
NS3D \citep{hsu2023ns3d} & 62.7 & 52.6\\ 
NS3D \citep{hsu2023ns3d} (w/ GT Label) & 96.9  & -\\ 
Transcrib3D \citep{fang2024transcrib3d} (w/ GT Label) & 98.4 & -\\
Ours (w/o VLM)& 62.0  & \textbf{58.6}\\ 
Ours (w/o VLM, w/ GT Label) & 95.1 & -\\
\hline
\end{tabular}
\end{center}
\end{table*}


\section{More Qualitative Results}
\subsection{Effect of Code Optimization}
We show the change between the initial response and the final code after multiple rounds of sampling and iterative refinement. By transitioning from a strict geometric overlap calculation to a continuous, exponential-based measure for both vertical and horizontal distances, the optimized code now captures nuances in ``above'' relation more robustly. This improved formulation inherently handles scenarios where objects are close but not strictly overlapping, and it provides a more stable gradient for training. Consequently, the updated model passes all test cases by offering a smoother, more differentiable metric that better aligns with real-world spatial relations and passes more test cases.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.45\textwidth} 
       \begin{lstlisting}[style=pythonstyle]
class Above:
    def __init__(self, object_locations: torch.Tensor) -> None:
        """
        Args:
        object_locations: torch.Tensor, shape (N, 6), 
        N is the number of objects in the scene.
        The first three columns are the center of the object (x, y, z),
        the last three columns are the size of the object (width, height, depth).
        """
        self.object_locations = object_locations.to(DEVICE)
        self._init_params()
    
    def _init_params(self) -> None:
        """
        Compute necessary parameters about `OnTopOf` 
        relation and initialize self.param if needed.
        """
        # shape (N, 3)
        self.centers = self.object_locations[:, :3] 
        # shape (N, 3)
        self.sizes = self.object_locations[:, 3:]   
    
    def forward(self) -> torch.Tensor:
        """
        Return a tensor of shape (N, N), where element (i, j) is the 
        metric value of the `OnTopOf` relation between object i and object j.
        """
        N = self.object_locations.shape[0]
        
        # Expand dimensions for broadcasting
        # Shape (N, 1, 3)
        centers_i = self.centers.unsqueeze(1)  
        # Shape (N, 1, 3)
        sizes_i = self.sizes.unsqueeze(1)  
        
        # Shape (1, N, 3)
        centers_j = self.centers.unsqueeze(0)
         # Shape (1, N, 3)
        sizes_j = self.sizes.unsqueeze(0)     

        # Calculate vertical distances
        vertical_distance = centers_i[..., 2] - centers_j[..., 2]
        - (sizes_i[..., 1] + sizes_j[..., 1]) / 2

        # Calculate horizontal overlaps
        overlap_x = torch.clamp((sizes_i[..., 0] + sizes_j[..., 0]) / 2 
        - torch.abs(centers_i[..., 0] - centers_j[..., 0]), min=0)
        overlap_y = torch.clamp((sizes_i[..., 2] + sizes_j[..., 2]) / 2 
        - torch.abs(centers_i[..., 1] - centers_j[..., 1]), min=0)

        # Calculate the overlap area in the x-y plane
        horizontal_overlap_area = overlap_x * overlap_y

        # Calculate the metric value, only where object i is above object j
        metric_value = torch.where(vertical_distance > 0,
        horizontal_overlap_area / (1 + vertical_distance), 
        torch.tensor(0.0, device=DEVICE))

        # Mask out the diagonal (i == j) to ensure 
        self-relations are not considered
        metric_value.fill_diagonal_(0)

        return metric_value
        \end{lstlisting}
        \caption{Initial generation of relation encoder of ``above".}
        \label{fig:subfig-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
       \begin{lstlisting}[style=pythonstyle]
class Above:
    def __init__(
        self, 
        object_locations: torch.Tensor) -> None:
        """
        Args:
        object_locations: torch.Tensor, shape (N, 6), 
        N is the number of objects in the scene.
        The first three columns are the center of the object (x, y, z),
        the last three columns are the size of the object (width, height, depth).
        """
        self.object_locations = object_locations
        self._init_params()
    
    def _init_params(self) -> None:
        """
        Compute necessary parameters about `OnTopOf` 
        relation and initialize parameters.
        """
        self.center = self.object_locations[:, :3]  # (x, y, z)
        self.size = self.object_locations[:, 3:]    # (width, depth, height)
    
    def forward(self) -> torch.Tensor:
        """
        Return a tensor of shape (N, N), where element (i, j) 
        is the metric value of the `OnTopOf` 
        relation between object i and object j.
        """
        N = self.object_locations.shape[0]

        # Calculate bottom of i and top of j
        height_i = self.size[:, 2].view(N, 1)
        bottom_of_i = self.center[:, 2].view(N, 1) - height_i / 2
        top_of_j = self.center[:, 2].view(1, N) 
                        + self.size[:, 2].view(1, N) / 2
        # Vertical proximity
        vertical_proximity = torch.exp(-torch.abs(bottom_of_i - top_of_j) 
                        / (height_i / 2))

        # Center distances
        center_dist_x = torch.abs(self.center[:, 0].view(N, 1) 
                        - self.center[:, 0].view(1, N))
        center_dist_y = torch.abs(self.center[:, 1].view(N, 1) 
                        - self.center[:, 1].view(1, N))
    
        # Combined sizes
        combined_size_x = (self.size[:, 0].view(N, 1) + self.size[:, 0].view(1, N)) / 2
        combined_size_y = (self.size[:, 1].view(N, 1) + self.size[:, 1].view(1, N)) / 2

        # Horizontal alignment
        horizontal_alignment = torch.exp(-(center_dist_x / combined_size_x) - (center_dist_y / combined_size_y))
        # Combine the metrics with a weight emphasizing more on vertical proximity
        relation_metric = vertical_proximity * horizontal_alignment
        
        # Remove self-relations by zeroing the diagonal
        relation_metric.fill_diagonal_(0)
        return relation_metric
\end{lstlisting}
        \caption{The relation encoder of ``above" after multi-turn sampling and optimization}
        \label{fig:subfig-b}
    \end{subfigure}
    
    \caption{Example of code optimization result on relation encoder of ``above". The left output is from the first iteration, which only pass 18 test cases out of 37. After 3 iterations of sampling and optimization, we get the the relation encoder as the right one. It passes all 37 test cases.}
    \label{fig:main}
\end{figure*}

\subsection{Relation Constraints}
\cite{feng2024naturally} propose that certain spatial relations are symmetric, like ``near" or ``far", meaning that if object A is "near" B, then B should also be "near" A. 
Consequently, the features of these relations should be symmetric.
Conversely, other relations are inherently asymmetric, such as ``left" or ``right". For these relations, if a feature element is positive (indicating the presence of the spatial relation), its corresponding symmetric element should be negative (indicating the absence of the reverse spatial relation).

\cite{feng2024naturally} leverage large language models (LLMs) to annotate these constraints and apply an auxiliary loss to enforce them during training. In contrast, while \ourmethod does not explicitly train or use specialized instructions to create these constraints, we observe that some LLM-generated relation encoders inherently produce relation features that satisfy these constraints. Moreover, for certain relations, these constraints are guaranteed due to the deterministic execution of the code. In \autoref{fig:heatmap}, we present four relation features for a scene:

\begin{itemize} 
    \item Features for ``near" and ``far" are guaranteed to be symmetric.
    \item For asymmetric features such as ``left" and ``right," if $f_{i, j} > 0$, it is guaranteed that $f_{j, i} = 0$.
\end{itemize}

\subsection{Condition Level Accuracy}

% Our parsed symbolic expressions actually contain one or more spatial conditions of the target object. However, there may be some redundant condition in the utterance. 
% For example, if the referring utterance is ``find the monitor on the floor and under desk."
% , and all monitors ``on the floor" are ``under the desk", so one of these two conditions are redundant, which means even if the method can not process one condition of them, it can still give the correct grounding result. 
% So we test \ourmethod on utterances containing single condition for a better understanding of its ability.

% We categorize objects of same class to groups. With in a group, we collect the conditions for each object from parsed expressions. Each condition is a JSON format like 
% \texttt{\{\{"relation": ..., "anchors": [...]\}\}} and can be executed seamlessly to find best matching object. We calculate the average precision and recall for all condition level matches.
% \ourmethod has 67.5\% average precision and 66.9\% average recall.
Our parsed symbolic expressions typically include one or more spatial conditions for the target object. However, some conditions in the referring utterance may be redundant.

For instance, if the referring utterance is "find the monitor on the floor and under the desk," and all monitors "on the floor" are also "under the desk," then one of these two conditions is redundant. This means that even if the method fails to process one of the conditions, it can still provide the correct grounding result.

To better understand \ourmethod's capability, we evaluate it on utterances containing a single condition.

We categorize objects of the same class into groups. Within each group, we collect the conditions for each object from the parsed expressions. Each condition is represented in JSON format, such as:
\texttt{{"relation": ..., "anchors": [...]}}.

These conditions are executed seamlessly to identify the best-matching object. We compute the average precision and recall for all condition-level matches. \ourmethod achieves an average precision of 67.5\% and an average recall of 66.9\%.

\begin{algorithm}[ht!]
\DontPrintSemicolon
\SetAlgoVlined
\caption{Execute}
\label{alg:execute}
\SetKwInOut{Input}{\textbf{Require}}
\SetKwInOut{Output}{\textbf{Output}}
\Input{symbolic expression $E$, category features $C$, relation features $R$}

\Output{\texttt{matching\_score}}
\setcounter{AlgoLine}{0}

% Initialize the category feature
$\texttt{f\_category} \gets C[E[\texttt{"category"}]]$ 

% Initialize an empty set for logits
$\texttt{matching\_score} \gets \texttt{f\_category}$ 

% Iterate over each relation in the symbolic expression
\ForEach{$\texttt{item\_rel} \in E[\texttt{"relations"}]$}{
    % Extract relation name and corresponding features
    $\texttt{n\_rel} \gets \texttt{item\_rel}[\texttt{"name"}]$ \;
    
    $\texttt{f\_rel} \gets R[\texttt{n\_rel}]$ \;
    
    % Extract anchors from the relation item
    $\texttt{anchors} \gets \texttt{item\_rel}[\texttt{"anchors"}]$ \;

    % Check the category of the relation name
    \If{$\texttt{n\_rel} \in \texttt{Unary\_Relations}$}{
        % Unary relation: relation_feature is (N,)
        
        $\texttt{f} \gets \texttt{f\_rel}$ \;
    }
    
    \ElseIf{$\texttt{n\_rel} \in \texttt{Binary\_Relations}$}{
        % Binary relation: relation_feature is (N, N)
        % anchors length is 1
        $\texttt{a} \gets \texttt{Execute}(\texttt{anchors}[0])$ \;
        
        $\texttt{f} \gets \texttt{f\_rel} \cdot \texttt{a}$ \; % Matrix-vector multiplication
    }
    \ElseIf{$\texttt{n\_rel} \in \texttt{Ternary\_Relations}$}{
        $\texttt{a\_1} \gets \texttt{Execute}(\texttt{anchors}[0])$ \;
        
        $\texttt{a\_2} \gets \texttt{Execute}(\texttt{anchors}[1])$ \;

        % Einsum notation for tensor computation
        $\texttt{pattern} \gets \texttt{"ijk,j,k}\to\texttt{i"}$ \;
        
        $\texttt{f} \gets \texttt{einsum}(\texttt{pattern}, \texttt{f\_rel}, \texttt{a\_1}, \texttt{a\_2})$ \;
    }
    
    $\texttt{f} \gets \texttt{softmax}(\texttt{f})$ \;

    \If{$E[\texttt{"negative"}]$}{
        % Unary relation: relation_feature is (N,)
        
        $\texttt{f} \gets \texttt{max}(\texttt{f}) - \texttt{f}$ \;
    }
    % Append concept-related matching\_score or other computations (optional)
    $\texttt{matching\_score} \gets \texttt{matching\_score} \cdot \texttt{f}$ \;
}

\Output{\texttt{matching\_score}}
\end{algorithm}





\subsection{More Visualization Results}
\label{appendix:more_vis_results}

We visualize more grounding examples in \autoref{fig:appendix}.
% The first row shows the process of grounding \texttt{the kitchen cabinet close to fridge and beside the stove.}
% The second row is the grounding process of \texttt{trash can on right below the sink.}
The first row illustrates the grounding process for \texttt{the kitchen cabinet close to the fridge and beside the stove}. In the process, the stove, objects beside the stove, and the objects near the fridge are sequentially grounded, culminating in the target kitchen cabinet highlighted in green.
The second row shows the grounding process for \texttt{right trash can below the sink}. Starting with the objects below the sink, followed by the objects on the right of the sink, and finally combining these conditions to highlight the target trash can in green.


\subsection{Effect of Unit tests}
% To demonstrate the effect of filtering generated code by its accuracy on training cases, we choose 6 relations and plot the pass rate on training cases on the x-axis and the number of passed examples in all relative test examples. For some easy relations like ``near" or ``far", GPT-4o can pass all the tests at once, so we only show the cases having multiple refinement steps.

% The result is shown in \autoref{fig:train_acc_test_acc}. For 5 of 6 relations (except relation \texttt{behind}), the code having the highest performance on the training cases can have the top-tier performance on the test set. As for relation \texttt{behind}, using the best code on training cases causes about 15 cases loss on the test case compared to using about 70 percent accurate code. But it's still better than using most of the codes whose accuracy is less than 0.5. This might be caused by the bias of training data collection. But in general, choosing codes according to performance on the training set is useful for overall performance on the test set.
% \subsection{Test case numbers}
% We list the number of test cases we used for each relation:
To demonstrate the impact of filtering generated code based on its accuracy on training cases, we selected six relations and plotted their performance. The x-axis represents the pass rate on training cases, while the y-axis shows the number of passed examples in all relevant test cases.

For straightforward relations such as ``near" or ``far", GPT-4o can pass all unit tests on the first attempt, so we focus on cases requiring multiple refinement steps.

The results, shown in \autoref{fig:train_acc_test_acc}, indicate that for five out of six relations (excluding \texttt{behind}), the code with the highest pass rate on training cases achieves top-tier performance on the test set. However, for the \texttt{behind} relation, using the best-performing code on the training cases results in about 15 fewer passed test cases compared to using code with approximately 70\% accuracy. Despite this, it still outperforms code with accuracy below 0.5.

This discrepancy for \texttt{behind} may stem from biases in the training data collection process. Overall, selecting code based on its performance on the training set is effective for achieving strong test set performance.






\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/more-vis.pdf}
    \caption{The target objects are: ``Stove next to another stove and close to the fridge" (top row) and ``Trashcan to the right of and below the sink" (bottom row).}
    \label{fig:more_vis}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/scatter_plots.pdf}
    \caption{Corresponding relation between the unit test pass rate and number of correct examples on test set.}
    \label{fig:train_acc_test_acc}
\end{figure*}





\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/heatmap.pdf}
    \caption{Relation feature examples. The features of ``near" and ``far" are symmetric, meaning mutual relationships hold true in both directions. For ``left" and ``right," if an element is positive, its corresponding symmetric element is zero, ensuring asymmetry. Additionally, all diagonal elements are zero, as self-relations are not considered.}
    \label{fig:heatmap}
\end{figure*}


