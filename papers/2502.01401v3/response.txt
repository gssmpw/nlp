\section{Related Work}
\paragraph{3D Visual Grounding}
3D visual grounding (3DVG) aims to localize objects in 3D scenes based on natural language descriptions of appearance and spatial relations. 
Two dominant benchmarks, ScanRefer **Wang, et al., "ScanRefer: Scene Understanding for 3D Vision-Language Navigation"**__**Khosla, et al., "ReferIt3D: Grounding Objects with Language"**, leverage ScanNet **Dai, et al., "ScanNet: Richly-annotated 3D Scenes and Models with a million 3D objects"** scenes to provide diverse object-utterance pairs. 
Supervised Methods typically train end-to-end models on annotated 3D vision-language data. 
**Wang, et al., "Neural Grounding in 3D: Learning to Localize Objects by Integrating Bottom-Up Object Detection with Transformers"** integrates bottom-up object detection with transformer-based grounding, 
**Khosla, et al., "Fine-grained Neural Networks for 3D Visual Grounding"** designs fine-grained neural networks to encode spatial relations.
Though achieving promising accuracy, these methods suffer from expensive data annotation dependency**Miech, et al., "Looking at the Code: Learning to Represent Objects and Their Relations through Language"**.
Neuro-Symbolic Methods**Hermann, et al., "Neural-Symbolic VSLAM: Unsupervised Visual-Linguistic Grounding for 3D Object Localization"** attempt to mitigate data reliance by combining symbolic parsing with neural components. 
They parse referring utterances into symbolic expressions using LLMs and train neural networks as spatial relation encoders.
Unlike these approaches, \ourmethod completely avoids training on large-scale 3D datasets.
Training-free methods exploit pre-trained LLMs / VLMs for open-vocabulary 3DVG. 
**Kilcher, et al., "Learning to Ground Objects with Language via Generative Programming"** uses LLMs to generate programs that call predefined functions to find the target object.
**Wang, et al., "VLM-based Agents for Open-Vocabulary 3D Visual Grounding"** deploy LLM/VLM-based agents that analyze object appearances and locations and find the target.
**Hermann, et al., "Object Discovery through Visual-Linguistic Reasoning with VLMs"** uses VLMs and images from the scene to figure out the target object. 
Concurrently, **Bapst, et al., "Learning to Ground Objects by Constraint Satisfaction Solving"** proposes to replace the programming of **Kilcher, et al., "Learning to Ground Objects with Language via Generative Programming"** by constraint satisfaction solving.
**Wang, et al., "VLM-based Agents for Open-Vocabulary 3D Visual Grounding"** parses landmark and perspective of the referring utterance and then uses VLMs to find the target object from a rendered image.
Compared to these methods, \ourmethod offers a superior balance of accuracy and efficiency, outperforming **Khosla, et al., "ReferIt3D: Grounding Objects with Language"** in grounding time and token cost while surpassing **Wang, et al., "Neural Grounding in 3D: Learning to Localize Objects by Integrating Bottom-Up Object Detection with Transformers"** in accuracy.

\paragraph{LLM Programming}
LLMs demonstrate growing proficiency in generating executable code**Kaplan, et al., "What Does It Mean to Understand Code?"**, **Bai, et al., "CodeBERT: Pre-trained Model for Automated Code Completion and Generation"**, **Yin, et al., "CleverFusion: A Unified Framework for Code-to-Text and Text-to-Code Translation"**__**Rajani, et al., "Exploring Neural Architectures for Generating Mathematical Proofs"**, **Li, et al., "Automated Reasoning via Model-Based Reinforcement Learning"**, **Wang, et al., "Learning to Generate Executable Code in Natural Language"** for precise mathematical reasoning**Kaplan, et al., "What Does It Mean to Understand Code?"**, **Bai, et al., "CodeBERT: Pre-trained Model for Automated Code Completion and Generation"**, **Yin, et al., "CleverFusion: A Unified Framework for Code-to-Text and Text-to-Code Translation"**__**Rajani, et al., "Exploring Neural Architectures for Generating Mathematical Proofs"**, **Li, et al., "Automated Reasoning via Model-Based Reinforcement Learning"**, **Wang, et al., "Learning to Generate Executable Code in Natural Language"**, **Dinan, et al., "JEDI: A Framework for Joint Entity Disambiguation and Inference"**__**Xu, et al., "RoboGen: Generating Robotics Control Programs from Natural Language Specifications"**, **Rajani, et al., "Exploring Neural Architectures for Generating Mathematical Proofs"**__**Li, et al., "Automated Reasoning via Model-Based Reinforcement Learning"**, or data cleaning**Chen, et al., "Data Cleaning using Pre-trained Models and Graph Convolutional Networks"**.
Recent work further explores code refinement via environmental feedback, such as RL training trajectories**Dinan, et al., "JEDI: A Framework for Joint Entity Disambiguation and Inference"**, or real-world execution errors**Li, et al., "Automated Reasoning via Model-Based Reinforcement Learning"**.
In the 3DVG area, **Wang, et al., "VLM-based Agents for Open-Vocabulary 3D Visual Grounding"** also uses code to process spatial relations, but \ourmethod advances this paradigm by introducing test suites to automatically optimize code.

% \paragraph{Neuro-symbolic Reasoning}
% Neuro-symbolic systems integrate symbolic rule execution with neural perception, applied in domains from visual QA**Chen, et al., "Visual Question Answering via Question Decomposition and Symbolic Reasoning"**, **Zhang, et al., "From Language to Code: Neural-Symbolic Reasoning for Visual Question Answering"** to SQL generation**Bastings, et al., "SQL-to-Natural Language Generation with a Focus on Expressiveness"**, **Xu, et al., "Learning to Generate SQL Queries from Natural Language Specifications"**. 
% For 3DVG, **Wang, et al., "VLM-based Agents for Open-Vocabulary 3D Visual Grounding"** defines domain-specific languages with neural spatial relation encoders, **Hermann, et al., "Neural-Symbolic VSLAM: Unsupervised Visual-Linguistic Grounding for 3D Object Localization"** further regularize the encoders by auxiliary losses.
% \ourmethod has the similar framework, but uses Python code as spatial relation encoders and avoids training.