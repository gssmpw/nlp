\section{Proofs of Theorems \ref{compare} and \ref{small}}
\label{sec:problem_setup_additional}
% Here we present the differential equations that we study in our experiments.

% \subsection{Convection}
% The one-dimensional convection problem is a hyperbolic PDE that can be used to model fluid flow, heat transfer, and biological processes.
% The convection PDE we study is
% \begin{align*}
%     \frac{\partial u}{\partial t} + \beta \frac{\partial u}{\partial x} = 0, & \quad x \in (0, 2\pi), t \in (0, 1), \\
%     u(x, 0) = \sin(x), & \quad x \in [0, 2\pi], \\
%     u(0, t) = u(2 \pi, t), & \quad t \in [0, 1]. 
% \end{align*}

% The analytical solution to this PDE is $u(x, t) = \sin(x - \beta t)$.
% We set $\beta = 40$ in our experiments.

% \subsection{Reaction}
% The one-dimensional reaction problem is a non-linear ODE which can be used to model chemical reactions.
% The reaction ODE we study is
% \begin{align*}
%     \frac{\partial u}{\partial t} - \rho u (1 - u) = 0, & \quad x \in (0, 2\pi), t \in (0, 1) \\
%     u(x, 0) = \exp \left( -\frac{(x - \pi)^2}{2 (\pi / 4)^2} \right), & \quad x \in [0, 2\pi], \\
%     u(0, t) = u(2 \pi, t), & \quad t \in [0, 1].
% \end{align*}

% The analytical solution to this ODE is $u(x, t) = \frac{h(x) e^{\rho t}}{h(x) e^{\rho t} + 1 - h(x)}$, where $h(x) = \exp \left( -\frac{(x - \pi)^2}{2 (\pi / 4)^2} \right)$.
% We set $\rho = 5$ in our experiments.

% % \subsubsection{Reaction-diffusion}
% % The one-dimensional reaction-diffusion problem is a non-linear PDE that can be used to model the concentration of chemical substances.
% % The reaction-diffusion PDE we study is
% % \begin{align*}
% %     \frac{\partial u}{\partial t} - \nu \frac{\partial^2 u}{\partial x^2} - \rho u (1 - u) = 0, & \quad x \in (0, 2\pi), t \in (0, 1), \\
% %     u(x, 0) = \exp \left( -\frac{(x - \pi)^2}{2 (\pi / 4)^2} \right), & \quad x \in [0, 2\pi], \\
% %     u(0, t) = u(2 \pi, t), & \quad t \in [0, 1].
% % \end{align*}

% % This PDE has no analytical solution; instead we calculate the solution using Strang splitting, as done in \cite{krishnapriyan2021characterizing}.
% % We set $\rho = 5$ and vary $\nu \in \{2, 3, 4, 5, 6\}$ in our experiments.

% \subsection{Wave}
% The one-dimensional wave problem is a hyperbolic PDE that often arises in acoustics, electromagnetism, and fluid dynamics.
% The wave PDE we study is
% \begin{align*}
%     \frac{\partial^2 u}{\partial t^2} - 4 \frac{\partial^2 u}{\partial x^2} = 0, & \quad x \in (0, 1), t \in (0, 1), \\
%     u(x, 0) = \sin(\pi x) + \frac{1}{2} \sin(\beta \pi x), & \quad x \in [0, 1], \\
%     \frac{\partial u(x, 0)}{\partial t} = 0, & \quad x \in [0, 1], \\
%     u(0, t) = u(1, t) = 0, & \quad t \in [0, 1].
% \end{align*}

% The analytical solution to this PDE is $u(x, t) = \sin(\pi x) \cos(2 \pi t) + \frac{1}{2} \sin(\beta \pi x) \cos(2 \beta \pi t)$.
% We set $\beta = 5$ in our experiments.
\subsection{\( \lambda_{\text{min}}(\vS\vS^{\top}) > 0 \)}\label{ss}
In this subsection, we consider a two-layer neural network defined as follows:
\begin{equation}
    \phi(\boldsymbol{x};\boldsymbol{\theta}) := \frac{1}{\sqrt{m}} \sum_{k=1}^{m} a_k \sigma (\boldsymbol{\omega}_k^{\top} \boldsymbol{x}),
\end{equation}
where the activation function is given by
\begin{equation}
    \sigma(z) = \text{ReLU}(z) = \max\{z,0\}.
\end{equation}
We assume that the weights and biases are sampled as follows: 
\begin{equation}
    \boldsymbol{\omega}_k \sim N\left(0, \boldsymbol{I}_d\right), \quad a_k \sim N(0,1),
\end{equation}
where \( N(0,1) \) denotes the standard Gaussian distribution.


The kernels characterizing the training dynamics take the following form:\begin{align}
    k^{[a]}(\boldsymbol{x},\boldsymbol{x}'):=&\mathbf{E}_{\boldsymbol{\omega}}\sigma(\boldsymbol{\omega}^{\top}\boldsymbol{x})\sigma(\boldsymbol{\omega}^{\top}\boldsymbol{x}')\notag\\k^{[\boldsymbol{\omega}]}(\boldsymbol{x},\boldsymbol{x}'):=&\mathbf{E}_{(a,\boldsymbol{\omega})}a^2\sigma'(\boldsymbol{\omega}^{\top}\boldsymbol{x})\sigma'(\boldsymbol{\omega}^{\top}\boldsymbol{x}')\boldsymbol{x}\cdot\boldsymbol{x}'.
\end{align} The Gram matrices, denoted as $\boldsymbol{K}^{[a]}$ and $\boldsymbol{K}^{[\boldsymbol{\omega}]}$, corresponding to an infinite-width two-layer network with the activation function $\sigma$, can be expressed as follows:\begin{align}
    &K_{ij}^{[a]}=k^{[a]}(\boldsymbol{x}_i,\boldsymbol{x}_j),~\boldsymbol{K}^{[a]}=(K_{ij}^{[a]})_{n\times n},\notag\\& K_{ij}^{[\boldsymbol{\omega}]}=k^{[\boldsymbol{\omega}]}(\boldsymbol{x}_i,\boldsymbol{x}_j),~\boldsymbol{K}^{[\boldsymbol{\omega}]}=(K_{ij}^{[\boldsymbol{\omega}]})_{n\times n}.
\end{align}

\begin{lemma}[\cite{allen2019convergence}]
\label{positive}    The matrices \(\boldsymbol{K}^{[\boldsymbol{\omega}]}\) and \(\boldsymbol{K}^{[a]}\) are strictly positive.
\end{lemma}
It is easy to check that
\begin{equation}
    \boldsymbol{K}^{[\boldsymbol{\omega}]} + \boldsymbol{K}^{[a]} = \lim_{m\to \infty} \mathbf{S} \mathbf{S}^{\top}
\end{equation}
based on the law of large numbers. Furthermore, we can show that the accuracy decreases exponentially as the width of the neural network increases.

\begin{definition}[\cite{vershynin2018high}]
    A random variable $X$ is sub-exponential if and only if its sub-exponential norm is finite i.e.\begin{equation}
        \|X\|_{\psi_1}:=\inf\{s>0\mid\mathbf{E}_X[e^{|X|/s}\le 2.]
    \end{equation} Furthermore, the chi-square random variable $X$ is a sub-exponential random variable and $C_{\psi,d}:=\|X\|_{\psi_1}$.
\end{definition}

\begin{lemma}\label{matrice sub}
    Suppose that $\boldsymbol{w} \sim N\left(0, \boldsymbol{I}_d\right), a \sim N(0,1)$ and given $\boldsymbol{x}_i, \boldsymbol{x}_j \in \Omega$. Then we have
    
(i) if $\mathrm{X}:=\sigma\left(\boldsymbol{w}^{\top} \boldsymbol{x}_i\right) \sigma\left(\boldsymbol{x} \cdot \boldsymbol{x}_j\right)$, then $\|\mathrm{X}\|_{\psi_1} \leq d C_{\psi, d}$.

(ii) if $\mathrm{X}:=a^2 \sigma^{\prime}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_i\right) \sigma^{\prime}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_j\right) \boldsymbol{x}_i \cdot \boldsymbol{x}_j$, then $\|\mathrm{X}\|_{\psi_1} \leq d C_{\psi, d}$.
\end{lemma}
\begin{proof}
(i) $|\mathrm{X}| \leq d\|\boldsymbol{w}\|_2^2=d \mathrm{Z}$ and
$$
\begin{aligned}
\|\mathrm{X}\|_{\psi_1} & =\inf \left\{s>0 \mid \mathbf{E}_{\mathrm{X}} \exp (|\mathrm{X}| / s) \leq 2\right\} \\
& =\inf \left\{s>0 \mid \mathbf{E}_{\boldsymbol{w}} \exp \left(\left|\sigma\left(\boldsymbol{w}^{\top} \boldsymbol{x}_i\right) \sigma\left(\boldsymbol{w}^{\top} \boldsymbol{x}_j\right)\right| / s\right) \leq 2\right\} \\
& \leq \inf \left\{s>0 \mid \mathbf{E}_{\boldsymbol{w}} \exp \left(d\|\boldsymbol{w}\|_2^2 / s\right) \leq 2\right\} \\
& =\inf \left\{s>0 \mid \mathbf{E}_{\mathrm{Z}} \exp (d|\mathrm{Z}| / s) \leq 2\right\} \\
& =d \inf \left\{s>0 \mid \mathbf{E}_{\mathrm{Z}} \exp (|\mathrm{Z}| / s) \leq 2\right\} \\
& =d\left\|\chi^2(d)\right\|_{\psi_1} \\
& \leq d C_{\psi, d}
\end{aligned}
$$
(ii) $|\mathrm{X}| \leq d|a|^2 \leq d \mathrm{Z}$ and $\|\mathrm{X}\|_{\psi_1} \leq d C_{\psi, d}$.
\end{proof}

\begin{proposition}[sub-exponential Bernstein's inequality \cite{vershynin2018high}]\label{vershynin}
    Suppose that $\mathrm{X}_1, \ldots, \mathrm{X}_m$ are i.i.d. sub-exponential random variables with $\mathbf{E} \mathrm{X}_1=\mu$, then for any $s \geq 0$ we have
$$
\mathbf{P}\left(\left|\frac{1}{m} \sum_{k=1}^m \mathrm{X}_k-\mu\right| \geq s\right) \leq 2 \exp \left(-C_0 m \min \left(\frac{s^2}{\left\|\mathrm{X}_1\right\|_{\psi_1}^2}, \frac{s}{\left\|\mathrm{X}_1\right\|_{\psi_1}}\right)\right),
$$
where $C_0$ is an absolute constant.
\end{proposition}

\begin{proposition}\label{eig pos}
    Given $\delta \in(0,1)$, $\boldsymbol{w} \sim N\left(0, \boldsymbol{I}_d\right), a \sim N(0,1)$ and the sample set $S=\left\{\boldsymbol{x}_i \right\}_{i=1}^n \subset \Omega$ with $\boldsymbol{x}_i$ 's drawn i.i.d. with uniformly distributed. If $m \geq \frac{16 n^2 d^2 C_{\psi, d}}{C_0 \lambda^2} \log \frac{4 n^2}{\delta}$ then with probability at least $1-\delta$ over the choice of $\boldsymbol{\theta}(0)$, we have
$$
\lambda_{\min }\left(\vS\vS^\top\right)\geq\frac{3}{4}(\lambda_{\text{min}}(\vK^{[a]})+\lambda_{\text{min}}(\vK^{[\boldsymbol{\omega}]})).
$$
\end{proposition}
\begin{proof}
    For any $\varepsilon>0$, we define \begin{align}
        \Omega_{ij}^{[a]}&:=\left\{\boldsymbol{\theta}\mid \left|(\vS\vS^\top)_{ij}(\boldsymbol{\theta})-K_{ij}^{[a]}-K_{ij}^{[\boldsymbol{\omega}]}\right|\le\frac{\varepsilon}{n}\right\}.
    \end{align}

    Setting $\varepsilon\le ndC_{\psi,d}$, by Proposition \ref{vershynin} and Lemma \ref{matrice sub}, we have \begin{align}
        \mathbf{P}(\Omega_{ij})&\ge 1-2\exp\left(-\frac{mC_0\varepsilon^2}{n^2d^2C_{\psi,d}}\right).
    \end{align}

    Therefore, with probability at least \[\left[1-2\exp\left(-\frac{mC_0\varepsilon^2}{n^2d^2C_{\psi,d}^2}\right)\right]^{2n^2}\ge 1-4n^2\exp\left(-\frac{mC_0\varepsilon^2}{n^2d^2C_{\psi,d}^2}\right)\] over the choice of $\boldsymbol{\theta}$, we have \begin{align}
        &\left\|\vS\vS^\top(\boldsymbol{\theta})-\vK^{[a]}-\vK^{[\boldsymbol{\omega}]}\right\|_F\le \varepsilon.
    \end{align}
Hence by taking $\varepsilon=\frac{\lambda_1}{4}$ and $\delta=4n^2\exp\left(-\frac{mC_0\lambda_1^2}{16n^2d^2C_{\psi,d}^2}\right)$, where $\lambda_1=\min\{\lambda_{\text{min}}(\vK^{[a]}),\lambda_{\text{min}}(\vK^{[\boldsymbol{\omega}]})\}$\begin{align}
       \lambda_{\min }\left(\vS\vS^\top\right)\geq\frac{3}{4}(\lambda_{\text{min}}(\vK^{[a]})+\lambda_{\text{min}}(\vK^{[\boldsymbol{\omega}]})).
    \end{align}
\end{proof}

Combining Lemma \ref{positive} and Proposition \ref{eig pos}, we obtain that under the conditions stated in Proposition \ref{eig pos}, the following holds with high probability:
\begin{equation}
    \lambda_{\text{min}}(\vS \vS^{\top}) > 0.
\end{equation}






\subsection{Proof of Theorem \ref{compare}}
We can analysis the smallest eigenvalue of the problems based on the following lemma:
\begin{lemma}[\cite{li1999lidskii}]\label{comparelink}
   Let $\vA$ be an $n \times n$ Hermitian matrix and let $\tilde{\vA}=\vT^* \vA \vT$. Then we have
$$
\lambda_{\text{min}}\left(\vT^* \vT\right) \leq  \frac{\lambda_{\text{min}}(\tilde{\vA})}{\lambda_{\text{min}}(\vA)} \leq \lambda_{\text{max}}\left(\vT^* \vT\right).
$$

\end{lemma}
\begin{proof}[Proof of Theorem \ref{compare}]
    We first show that \( \lambda_{\text{min}}(\vK_\varepsilon) > 0 \), which follows directly from Lemma~\ref{comparelink}:
    \[
    \lambda_{\text{min}}(\vK_\varepsilon) \geq \lambda_{\text{min}}(\vS\vS^\top) \cdot \lambda_{\text{min}}(\vD_\varepsilon\vD_\varepsilon^\top) > 0.
    \]
    Therefore, at the beginning of gradient descent, the kernel of the gradient descent step is strictly positive. We then define \( T \) as
    \begin{equation}
        T := \inf\{t \mid \boldsymbol{\theta}(t) \not\in N(\boldsymbol{\theta}(0))\},\label{t_1}
    \end{equation}
    where
    \[
    N(\boldsymbol{\theta}) := \left\{\boldsymbol{\theta} \mid \|\boldsymbol{K}_\varepsilon(\boldsymbol{\theta}(t)) - \boldsymbol{K}_\varepsilon(\boldsymbol{\theta}(0))\|_F \leq \frac{1}{2} \lambda_{\text{min}}(\vK_\varepsilon) \right\}.
    \]

    We now analyze the evolution of the loss function:
    \begin{align}
    \frac{\D L(\vtheta(t))}{\D t} &= \nabla_{\vtheta} L(\vtheta) \frac{\D \vtheta}{\D t} \notag \\
    &= -\frac{1}{n^2} \vl \vD_\varepsilon \vS \vS^{\top} \vD_\varepsilon^{\top} \vl^{\top} \notag \\
    &\leq -\frac{2}{n} \lambda_{\text{min}}(\vK_\varepsilon(\vtheta(t))) L(\vtheta(t)),
    \end{align}
    where we use the fact that \( \vl \cdot \vl^\top = 2n L(\vtheta(t)) \).

    Furthermore, for \( t \in [0,T] \), we have 
    \[
    \|\boldsymbol{K}_\varepsilon(\boldsymbol{\theta}(t)) - \boldsymbol{K}_\varepsilon(\boldsymbol{\theta}(0))\|_F \leq \frac{1}{2} \lambda_{\text{min}}(\vK_\varepsilon).
    \]
    This implies
    \[
    \lambda_{\text{min}}(\vK_\varepsilon(\vtheta(t))) \geq \frac{1}{2} \lambda_{\text{min}}(\boldsymbol{K}_\varepsilon(\boldsymbol{\theta}(0))).
    \]
    Therefore, we obtain
    \begin{align}
    \frac{\D L(\vtheta(t))}{\D t} \leq -\frac{1}{n} \lambda_{\text{min}}(\boldsymbol{K}_\varepsilon(\boldsymbol{\theta}(0))) L(\vtheta(t)),
    \end{align}
    for \( t \in [0,T] \). Solving this differential inequality yields
    \begin{equation}
    L(\vtheta(t)) \leq L(\vtheta(0))\exp\left(-\frac{\lambda_{\text{min}}(\vK_{\varepsilon})}{n} t\right)
    \end{equation}
    for all \( t \in [0, T] \).

    Finally, for the inequality
    \begin{equation}
    \lambda_{\text{min}}(\vK_{\varepsilon}) \leq \lambda_{\text{min}}(\vS\vS^{\top}) \lambda_{\text{max}}(\vD_{\varepsilon}\vD_{\varepsilon}^\top),
    \end{equation}
    it follows directly from Lemma~\ref{comparelink}.
\end{proof}
\subsection{Proof of Theorem \ref{small}}
\begin{proof}[Proof of Theorem \ref{small}]
    First, we have  
    \begin{align}
        u(\varepsilon_{k+1}) &= u(\varepsilon_k) + (\varepsilon_{k+1}-\varepsilon_k)u'(\varepsilon_k) + \frac{1}{2}(\varepsilon_{k+1}-\varepsilon_k)^2u''(\xi_k) \notag \\
        &= u(\varepsilon_k) + (\varepsilon_{k+1}-\varepsilon_k)h(\varepsilon_k,u(\varepsilon_k)) + \frac{1}{2}(\varepsilon_{k+1}-\varepsilon_k)^2u''(\xi_k),
    \end{align}
    where \( \xi_k \) lies between \( \varepsilon_{k+1} \) and \( \varepsilon_k \) and depends on \( \vx \). Therefore, we obtain  
    \begin{equation}
        e(\varepsilon_{k+1}) = e(\varepsilon_k) + (\varepsilon_{k+1}-\varepsilon_k)(h(\varepsilon_k,u(\varepsilon_k)) - h(\varepsilon_k,U(\varepsilon_k))) + \frac{1}{2}(\varepsilon_{k+1}-\varepsilon_k)^2u''(\xi_k),
    \end{equation}
    where \( e(\varepsilon_k) = u(\varepsilon_k) - U(\varepsilon_k) \). Then, we have  
    \begin{align}
     &\|e(\varepsilon_{k+1})\|_{H^2(\Omega)} \notag \\
     =&\|e(\varepsilon_k)\|_{H^2(\Omega)} + (\varepsilon_{k+1}-\varepsilon_k)\|h(\varepsilon_k,u(\varepsilon_k)) - h(\varepsilon_k,U(\varepsilon_k))\|_{H^2(\Omega)} \notag \\
     &+ \frac{1}{2}(\varepsilon_{k+1}-\varepsilon_k)^2\|u''(\xi_k)\|_{H^2(\Omega)} \notag \\
     \leq& \|e(\varepsilon_k)\|_{H^2(\Omega)} + (\varepsilon_{k+1}-\varepsilon_k)K_{\varepsilon_k}\|e(\varepsilon_k)\|_{H^2(\Omega)} + \frac{1}{2} \frac{\varepsilon_0-\varepsilon_n}{n} \tau \notag \\
     \leq& \|e(\varepsilon_k)\|_{H^2(\Omega)} + K \cdot \frac{\varepsilon_0-\varepsilon_n}{n} \|e(\varepsilon_k)\|_{H^2(\Omega)} + \frac{1}{2} \frac{\varepsilon_0-\varepsilon_n}{n} \tau.
    \end{align}
    Recalling that \( e_0 = \|u (\varepsilon_0)-U(\varepsilon_0)\|_{H^2(\Omega)}\), we obtain  
    \begin{align}
    \|e(\varepsilon_{n})\|_{H^2(\Omega)} 
        &\leq e_0\left(1+K\cdot \frac{\varepsilon_0-\varepsilon_n}{n}\right)^n+\frac{\tau}{2} \frac{\varepsilon_0-\varepsilon_n}{n} \sum_{n=0}^{n-1} \left(1+K\cdot \frac{\varepsilon_0-\varepsilon_n}{n}\right)^n \notag \\
        &= e_0\left(1+K\cdot \frac{\varepsilon_0-\varepsilon_n}{n}\right)^n+\frac{\tau}{2} \frac{\left(1+K\cdot \frac{\varepsilon_0-\varepsilon_n}{n}\right)^n-1}{K} \notag \\
        &\leq \frac{\tau(e^{K(\varepsilon_0-\varepsilon_n)}-1)}{2K}+e_0e^{K(\varepsilon_0-\varepsilon_n)},
    \end{align}
    where the last step follows from the inequality  
    \begin{equation}
    (1+a)^m \leq e^{m a}, \notag
    \end{equation}
    for \( a>0 \).
\end{proof}

\begin{corollary}[Convergence of Homotopy Functions]\label{cosmall}
    Suppose the assumptions in Theorem \ref{small} hold, and \( H(\varepsilon_n, u) \) is Lipschitz continuous in \( H^2(\Omega) \), i.e.,
    \[
    \|H( u_1,\varepsilon_n) - H( u_2,\varepsilon_n)\|_{H^2(\Omega)} \leq L \|u_1 - u_2\|_{H^2(\Omega)}.
    \]
    Then, we have  
    \begin{align}
        &\|H( U(\varepsilon_n),\varepsilon_n)\|_{H^2(\Omega)} \notag\\\leq& L\left[e_0e^{K(\varepsilon_0-\varepsilon_n)}+\frac{\tau(e^{K(\varepsilon_0-\varepsilon_n)}-1)}{2K}\right] \ll 1.
    \end{align}
\end{corollary}

\begin{proof}
    The proof follows directly from the result in Theorem \ref{small}.
\end{proof}

\subsection{Discussion on \( e_0 \)}\label{e0}

In Theorem~\ref{small} and Corollary~\ref{cosmall}, one important assumption is that we assume \( e_0 \) is small. Here, we discuss why this assumption is reasonable. 

First, we use physics-informed neural networks (PINNs) to solve the following equations:
\begin{equation}
\left\{
\begin{array}{ll}
\mathcal{L}_\varepsilon u = f(u), & \text{in } \Omega, \\
\mathcal{B} u = g(x), & \text{on } \partial \Omega,
\end{array}
\right.
\label{eq:gen_pde1}
\end{equation}
where \( \mathcal{L}_\varepsilon \) is a differential operator defining the PDE with certain parameters, \( \mathcal{B} \) is an operator associated with the boundary and/or initial conditions, and \( \Omega \subseteq \mathbb{R}^d \).

The corresponding continuum loss function is given by:
\begin{align}
   L_c(\boldsymbol{\theta}) \coloneqq  \frac{1}{2} \int_\Omega \left( \mathcal{L}_\varepsilon u(\boldsymbol{x}; \boldsymbol{\theta}) - f(u) \right)^2 \D \vx
   + \frac{\lambda}{2} \int_{\partial\Omega} \left( \mathcal{B} u(\boldsymbol{x}; \boldsymbol{\theta}) - g(\boldsymbol{x}) \right)^2 \D \vx. 
   \label{loss1}
\end{align}

We assume this loss function satisfies a regularity condition:
\begin{assumption}
   Let \( u_* \) be the exact solution of Eq.~(\ref{eq:gen_pde1}). Then, there exists a constant \( C \) such that
   \begin{equation}
       \| u(\boldsymbol{x}; \boldsymbol{\theta}) - u_*(\boldsymbol{x}) \|_{H^2(\Omega)} \leq C L_c(\boldsymbol{\theta}).
   \end{equation}
\end{assumption}

The above assumption holds in many cases. For example, based on \cite{grisvard2011elliptic}, when \( \mathcal{L} \) is a linear elliptic operator with smooth coefficients, and \( f(u) \) reduces to \( f(\boldsymbol{x}) \in L^2(\Omega) \), and if \( \Omega \) is a polygonal domain (e.g., \( [0,1]^d \)), then, provided the boundary conditions are always satisfied, the assumption holds.

Therefore, we only need to ensure that \( L_c(\boldsymbol{\theta}_s) \) is sufficiently small, where \( \boldsymbol{\theta}_s \) denotes the learned parameters at convergence. Here, \( L_c(\boldsymbol{\theta}_s) \) can be divided into three sources of error: approximation error, generalization error, and training error:

\begin{align}
    \boldsymbol{\theta}_c &= \arg \min_{\boldsymbol{\theta}} L_c(\boldsymbol{\theta}) 
    = \arg \min_{\boldsymbol{\theta}} \frac{1}{2} \int_\Omega \left( \mathcal{L}_\varepsilon u(\boldsymbol{x}; \boldsymbol{\theta}) - f(u(\boldsymbol{x})) \right)^2 \D \vx
   + \frac{\lambda}{2} \int_{\partial\Omega} \left( \mathcal{B} u(\boldsymbol{x}; \boldsymbol{\theta}) - g(\boldsymbol{x}) \right)^2 \D \vx, \notag\\
    \boldsymbol{\theta}_d &= \arg \min_{\boldsymbol{\theta}} L(\boldsymbol{\theta}) 
    = \arg \min_{\boldsymbol{\theta}} \frac{1}{2n_r} \sum_{i=1}^{n_r} \left( \mathcal{L}_\varepsilon u(\boldsymbol{x}_r^i; \boldsymbol{\theta}) - f(u(\boldsymbol{x}_r^i; \boldsymbol{\theta})) \right)^2
    + \frac{\lambda}{2n_b} \sum_{j=1}^{n_b} \left( \mathcal{B} u(\boldsymbol{x}_b^j; \boldsymbol{\theta}) - g(\boldsymbol{x}_b^j) \right)^2,
\end{align}
where \( \boldsymbol{x}_r^i, \boldsymbol{x}_b^j \) are sampled points as defined in Eq.~(\ref{loss}).

The error decomposition can then be expressed as:
\begin{align}
    \mathbb{E} L_c(\boldsymbol{\theta}_s) 
    &\leq L_c(\boldsymbol{\theta}_c) + \mathbb{E} L(\boldsymbol{\theta}_c) - L_c(\boldsymbol{\theta}_c) 
    + \mathbb{E} L(\boldsymbol{\theta}_d) - \mathbb{E} L(\boldsymbol{\theta}_c) 
    + \mathbb{E} L(\boldsymbol{\theta}_s) - \mathbb{E} L(\boldsymbol{\theta}_d) 
    + \mathbb{E} L_c(\boldsymbol{\theta}_s) - \mathbb{E} L(\boldsymbol{\theta}_s) \notag \\
    &\leq \underbrace{L_c(\boldsymbol{\theta}_c)}_{\text{approximation error}}
    + \underbrace{\mathbb{E} L(\boldsymbol{\theta}_c) - L_c(\boldsymbol{\theta}_c) 
    + \mathbb{E} L_c(\boldsymbol{\theta}_s) - \mathbb{E} L(\boldsymbol{\theta}_s)}_{\text{generalization error}}
    + \underbrace{\mathbb{E} L(\boldsymbol{\theta}_s) - \mathbb{E} L(\boldsymbol{\theta}_d)}_{\text{training error}},
\end{align}where the last inequality is due to $\mathbb{E} L(\boldsymbol{\theta}_d) - \mathbb{E} L(\boldsymbol{\theta}_c) \le 0$ based on the definition of $\vtheta_d$.

The approximation error describes how closely the neural network approximates the exact solution of the PDEs. If \( f \) is a Lipschitz continuous function, \( \mathcal{L}_\varepsilon \) is Lipschitz continuous from \( W^{2,1}(\Omega) \to L^1(\Omega) \), and \( \mathcal{B} \) is Lipschitz continuous from \( L^1(\partial\Omega) \to L^1(\partial\Omega) \), with \( u(\boldsymbol{x}; \boldsymbol{\theta}), u_* \in W^{2,\infty}(\bar{\Omega}) \) and \( \partial \Omega \in C^1(\Omega) \), then we have
\begin{align}
    L_c(\boldsymbol{\theta}) &= \int_\Omega \left( \mathcal{L}_\varepsilon u(\boldsymbol{x}; \boldsymbol{\theta}) - f(u(\boldsymbol{x})) \right)^2 - \left( \mathcal{L}_\varepsilon u_* - f(u_*) \right)^2 \, d\boldsymbol{x} + \frac{\lambda}{2} \int_{\partial\Omega} \left( \mathcal{B} u(\boldsymbol{x}; \boldsymbol{\theta}) - g(\boldsymbol{x}) \right)^2 - \left( \mathcal{B} u_* - g(\boldsymbol{x}) \right)^2 \, d\boldsymbol{x} \notag\\&\leq C_1 \left( \|\mathcal{L}_\varepsilon (u(\boldsymbol{x}; \boldsymbol{\theta}) - u_*)\|_{L^1(\Omega)} + \|f( u(\boldsymbol{x}; \boldsymbol{\theta})) - f(u_*)\|_{L^1(\Omega)} \right)  + C_2 \|\mathcal{B} (u(\boldsymbol{x}; \boldsymbol{\theta}) - u_*)\|_{L^1(\partial\Omega)} \notag \\
    &\leq C_3 \|u(\boldsymbol{x}; \boldsymbol{\theta}) - u_*\|_{W^{2,1}(\Omega)} + C_4 \|u(\boldsymbol{x}; \boldsymbol{\theta}) - u_*\|_{W^{1,1}(\Omega)} \notag \\
    &\leq C \|u(\boldsymbol{x}; \boldsymbol{\theta}) - u_*\|_{W^{2,1}(\Omega)},
\end{align}
where the second inequality follows from the trace theorem \cite{evans2022partial}. Therefore, we conclude that \( L_c(\boldsymbol{\theta}) \) can be bounded by \( \|u(\boldsymbol{x}; \boldsymbol{\theta}) - u_*\|_{W^{2,1}(\Omega)} \), which has been widely studied in the context of shallow neural networks \cite{siegel2020approximation} and deep neural networks \cite{yang2023nearly1}. These results show that if the number of neurons is sufficiently large, the error in this part becomes small.

For the generalization error, it arises from the fact that we have only a finite number of data points. This error can be bounded using Rademacher complexity \cite{yang2023nearly1,luo2020two}, which leads to a bound of \( \mathcal{O}\left(n_r^{-\frac{1}{2}}\right) + \mathcal{O}\left(n_b^{-\frac{1}{2}}\right) \). In other words, this error term is small when the number of sample points is large.

For the training error, Theorem~\ref{compare} shows that when \( \varepsilon \) is large in certain PDEs, the loss function can decay efficiently, reducing the training error to a small value.

