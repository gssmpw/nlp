\section{Homotopy Dynamics}
\label{sec: Method}
We introduce homotopy dynamics to improve the training of neural networks.

\subsection{Homotopy Path Tracking}

First, we introduce the homotopy function below:
\begin{equation}
    H(u,\varepsilon) = \mathcal{L}_\varepsilon u - f(u) \equiv 0,
\end{equation}
where $\varepsilon$ is the parameter in the PDEs. Specifically, this formulation represents the PDE problem $\mathcal{L}_\varepsilon u = f(u)$.  In this context, $\varepsilon$ is treated as a path-tracking parameter. At $\varepsilon = \varepsilon_0$, we assume that the solutions to \( H(u_0, \varepsilon_0) = 0 \) are either known or can be easily approximated by neural networks. These solutions are referred to as the starting points. At $\varepsilon = \varepsilon_1$, the original system we aim to solve is recovered, which is referred to as the target system. Therefore, solving the target system involves tracking the solutions of \( H(u, \varepsilon) = 0 \) from \( \varepsilon = \varepsilon_0 \), where the solutions are known, to \( \varepsilon = \varepsilon_1 \), where the solutions are sought.

The process of path tracking between $\varepsilon_0$ and $\varepsilon_1$ is governed by solving the Davidenko differential equation:
\begin{equation}
\frac{\D H(u(\varepsilon), \varepsilon)}{\D \varepsilon}=\frac{\partial H(u(\varepsilon),\varepsilon)}{\partial u} \frac{\D u(\varepsilon)}{\D \varepsilon}+\frac{\partial H(u(\varepsilon), \varepsilon)}{\partial \varepsilon}= 0,
\label{eq:homotopy_dynamics}
\end{equation}
with the initial condition $u(\varepsilon_0)=u_0$.
Thus, path tracking reduces to numerically solving an initial value problem, with the starting points acting as the initial conditions. Additionally, the boundary condition in \eqref{eq:gen_pde} should be taken into account when solving the initial value problem numerically.



\subsection{Incorporating Homotopy Dynamics into Neural Network Training}

To enhance the neural network training process, we incorporate homotopy dynamics by gradually transitioning the network from an easier problem (with a larger \( \varepsilon_0 \)) to the original target problem (with \( \varepsilon_1 \)). This approach helps mitigate the challenges associated with training networks for problems involving small values of \( \varepsilon \), where solutions become increasingly sharp and harder to compute. Specifically, 
 we denote the neural network solution for Eq. \eqref{eq:gen_pde} as $u(x;{\vtheta}(\varepsilon))$. The homotopy path tracking for training neural networks can then be refined as:
\begin{equation}
    H_{u}\nabla_{{\vtheta}}u \cdot \frac{\D{\vtheta}(\varepsilon)}{\D \varepsilon} + H_{\varepsilon} = 0,
\label{eq.homo_dynamics}
\end{equation}
where $H_{u} = \frac{\partial H}{\partial u}$, $H_{\varepsilon} = \frac{\partial H}{\partial \varepsilon}$ and $\nabla_{{\vtheta}}u$ represents the Jacobian with respect to the neural network parameters ${\vtheta}$. Thus we can derive the homotopy dynamics system as:
\begin{equation}
 \frac{\D {\vtheta}(\varepsilon)}{\D \varepsilon} = -(H_{u}\nabla_{{\vtheta}}u)^{\dagger}H_{\varepsilon}, \quad \varepsilon \in [\varepsilon_0,\varepsilon_n],
\label{eq:homotopy_pinn}
\end{equation}
with the initial condition $\vtheta(\varepsilon_0)=\vtheta_0$ and ${\dagger}$ stands for Mooreâ€“Penrose inverse \cite{ben2006generalized}.
%\begin{equation}
%\frac{\D u_{{\vtheta}(\varepsilon)}}{\D \varepsilon} = \nabla_{{\vtheta}}u \cdot \frac{\D {\vtheta}(\varepsilon)}{\D \varepsilon} = - H_{u}^{-1}H_{\varepsilon}, \quad \varepsilon \in [\varepsilon_0,\varepsilon_n],
%\label{eq:homotopy_pinn}
%\end{equation}
%where the start point can be $u_{{\vtheta}(\varepsilon_0)}$. 
Thus, to solve the sharp interface problem \eqref{eq:gen_pde} where \( \varepsilon \) is small, we can first solve \eqref{eq:gen_pde} with a large \( \varepsilon \) using the loss function \eqref{loss}. Then, by following the homotopy dynamics path tracking \eqref{eq:homotopy_pinn}, we can progressively obtain the solution for smaller values of \( \varepsilon \), ultimately solving the sharp interface problem.

In particular, path tracking in homotopy dynamics reduces to solving initial value problems numerically, with the start points serving as the initial conditions. For different neural network architectures, we propose two strategies, which are summarized in {\bf Algorithm 1.}

One is to solve the initial value problem by using the forward Euler method, as follows:
\begin{equation}
{\vtheta}(\varepsilon_k) = 
          {\vtheta}(\varepsilon_{k-1})-\Delta \varepsilon_k\nabla_{{\vtheta}}u(\varepsilon_{k-1})^{\dagger}H_u^{-1}H_{\varepsilon},
\end{equation}
where $\Delta \varepsilon_k = \varepsilon_{k-1}-\varepsilon_{k}$. This approach is effective for small neural networks, as the pseudo-inverse is easy to compute.

The other approach is to introduce the Homotopy Loss in the optimization, formulated as:
\begin{align}
\underset{\boldsymbol{{\vtheta}({\varepsilon_k})} \in \R^p}{\mbox{min}}~L_{\text{Hom}}(\boldsymbol{{\vtheta}}({\varepsilon_k})) \coloneqq &  L_{H} +\lambda L_{\text{bc}}+\alpha L_{H_{\varepsilon}},
\end{align}
where $L_{H}$ is defined in Eq.~(\ref{eq:hom_or_loss}), and $L_{H_{\varepsilon}}$ is the loss function from Homotopy Dynamics, which is \[L_{H_{\varepsilon}} =  \textstyle\frac{1}{2\nres}\textstyle\sum^{\nres}\limits_{i=1}\left(H_{u}(u_{{\vtheta}({\varepsilon_k})}(\vx_r^i),\varepsilon)\frac{\Delta u_k}{\Delta \varepsilon_k} + H_{\varepsilon}(u_{\vtheta}(\vx_r^i),\varepsilon)\right)^2.\]
This approach is suitable for large neural networks, as it does not require the computation of the pseudo-inverse.






% \begin{align}
%  L_H &=  \textstyle\frac{1}{2\nres} \textstyle\sum\limits_{i=1}^{\nres} \left(H(u_{{\vtheta}({\varepsilon_k})}(\vx_r^i),\varepsilon)\right)^2,\notag\\
%   L_{H_{\varepsilon}} &=  \textstyle\frac{1}{2\nres}\textstyle\sum^{\nres}\limits_{i=1}\left(H_{u}(u_{{\vtheta}({\varepsilon_k})}(\vx_r^i),\varepsilon)\frac{\Delta u_k}{\Delta \varepsilon_k} + H_{\varepsilon}(u_{\vtheta}(\vx_r^i),\varepsilon)\right)^2, 
% \notag\\
%  L_{\text{bc}}  &=      \textstyle\frac{1}{2\nbc}\textstyle\sum^{\nbc}\limits_{i=1}\left(\Bc u(\vx_b^j;{\vtheta}({\varepsilon_k}))-g(\vx_b^j)\right)^2,\notag
% \end{align}
and $\Delta u_k = u_{{\vtheta}({\varepsilon_k})} - u_{{\vtheta}({\varepsilon_{k-1}})}$.



\begin{algorithm}[H]
	\centering
    \footnotesize
	\caption{Homotopy Dynamics Path Tracking}
	\label{alg1-FEuler}
	\begin{algorithmic}
	\INPUT{tolerance $\tau$, list of parameter $\varepsilon_0,\varepsilon_1,\ldots,\varepsilon_n$
        \STATE{\textbf{Phase I}: \textbf{Directly train NN for large $\varepsilon_0$}} 
        % \COMMENT{Get close to a minimizer with gradient descent}
        \WHILE{$L({\vtheta}(\varepsilon_0)) > \tau$}
            \STATE{$\min L({\vtheta}(\varepsilon_0))$}
        \ENDWHILE
        \STATE{\textbf{Phase II}: \textbf{Homotopy dynamics path tracking}} 
        % \COMMENT{Obtain high accuracy solution with Damped Newton's method}
        %\STATE{Set ${\vtheta}(\varepsilon_)$}
		\FOR{$k = 1,\dots,n$}
            \STATE{$\Delta \varepsilon_k = \varepsilon_{k-1}-\varepsilon_{k}$}
		  \STATE{Strategy 1. Numerical Solution via Forward Euler Evolution:
          
          ${\vtheta}(\varepsilon_k) = 
          {\vtheta}(\varepsilon_{k-1})-\Delta \varepsilon_k\nabla_{{\vtheta}}u(\varepsilon_{k-1})^{\dagger}H_u^{-1}H_{\varepsilon}$} \\
          Strategy 2. Optimization using Homotopy Loss:
          \WHILE{$L_{\text{Hom}}(\boldsymbol{{\vtheta}}(\varepsilon)) > \tau$}
            \STATE{$\Delta \varepsilon_k = \varepsilon_{k-1}-\varepsilon_{k}$}
		  \STATE{$\min L_{\text{Hom}}(\boldsymbol{{\vtheta}}(\varepsilon_k))$}
          \ENDWHILE
	\ENDFOR
	\OUTPUT{$u_{{\vtheta}(\varepsilon_n)}$}}
	\end{algorithmic}
\end{algorithm}

\paragraph{Example: 1D Allen-Cahn steady-state equation.}
We demonstrate our proposed method on the one-dimensional Allen-Cahn steady-state equation by defining the following homotopy function:
\begin{equation}
    H(u_{\vtheta},\varepsilon) = \varepsilon^2 u_{\vtheta}^{\prime \prime}(x) + {u_{\vtheta}^3 - u_{\vtheta}} \equiv 0.
\end{equation}
Following the homotopy dynamics in Eq. \eqref{eq:homotopy_pinn}, we set the initial value at $\varepsilon = 0.1$ and gradually decrease it to the final value $\varepsilon_n = 0.01$. The initial solution, ${\vtheta(\varepsilon_0)}$, is obtained using the standard training process by directly minimizing \eqref{loss}. The results and the evolution process are presented in \cref{tab:1D_Allen_loss_l2re_comparison} and \cref{fig:1d_allen_cahn_homo_result}. These results show that when $\varepsilon$ is large, the original training method achieves a relatively small error, leading to an accurate solution. However, as $\varepsilon$ decreases, the error increases, which reduces the accuracy of the solution. In contrast, the homotopy dynamics-based approach maintains accuracy effectively as $\varepsilon$ decreases.

\begin{table}[t]
    \caption{Training losses for both the classical training and homotopy dynamics with different $\varepsilon$. The homotopy dynamics approach achieves both a smaller loss and a lower L2RE compared to the classical method.
    }
    \vskip 0.15in
    \centering
    \tiny
    \begin{tabular}{|c|c|c|c|c|c|c|c|} 
    \hline 
    \multirow{2}{*}{} & \multicolumn{2}{c|}{$\varepsilon = 0.1$} & \multicolumn{2}{c|}{$\varepsilon = 0.03$} & \multicolumn{2}{c|}{$\varepsilon = 0.01$} \\ \cline{2-7}
                               & Loss & L2RE & Loss & L2RE & Loss & L2RE \\ \hline 
    Classical training                        & 5.00e-6     & 1.71e-2     & 7.76e-4     & 1.11    & 7.21     & 8.17e-1     \\ \hline 
    Homotopy dynamics                     & 5.00e-6    & 1.71e-2     & 7.45e-8    & 9.83e-3     & \textbf{4.63e-8}     & \textbf{8.08e-3}     \\ \hline
    \end{tabular}
    % On convection, \al{} provides 14.2$\times$ and 1.97$\times$ improvement over Adam or \lbfgs{} on L2RE. 
    % On reaction, \al{} provides 1.10$\times$ and 1.99$\times$ improvement over Adam or \lbfgs{} on L2RE.
    % On wave, \al{} provides 6.32$\times$ and 6.07$\times$ improvement over Adam or \lbfgs{} on L2RE.}
    \label{tab:1D_Allen_loss_l2re_comparison}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.45]{figs/1D_AC_Homo_result.png}
    \caption{Evolution of the Homotopy dynamics for steady state 1D Allen-Cahn equation. The L2RE for $\varepsilon=0.01$ is $8.08e-3$.}
    \label{fig:1d_allen_cahn_homo_result}
\end{figure}
