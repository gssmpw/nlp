\section{Introduction}
The study of Partial Differential Equations (PDEs) serves as a cornerstone for numerous scientific and engineering disciplines. In recent years, leveraging neural network architectures to solve PDEs has gained significant attention, particularly in handling complex domains and incorporating empirical data. Theoretically, neural networks have the potential to overcome the curse of dimensionality when solving PDEs \cite{han2018solving,siegel2020approximation,lu2021priori,yang2022approximation}. However, despite these advancements, numerically solving such fundamental physical equations remains a challenging task.
Existing neural network-based methods for solving PDEs can be broadly categorized into two classes: PDE solution approximation \cite{e2018deep,raissi2019unified,wan,karniadakis2021physicsinformed,cuomo2022scientific,chen2022bridging,dong2023method,sun2024local,chen2024quantifying} and operator learning \cite{lu2021deeponet,li2021fourier,haonewton}.
% PINNs parameterize the solution to a PDE with a neural network, and are often fit by minimizing a least-squares loss involving the PDE residual, boundary condition(s), and initial condition(s).
% The promise of PINNs is the potential to obtain solutions to PDEs without discretizing or meshing the space,
% enabling scalable solutions to high-dimensional problems that currently require weeks on advanced supercomputers.
% This loss is typically minimized with gradient-based optimizers such as Adam \cite{kingma2014adam}, L-BFGS \cite{liu1989limited}, or a combination of both.


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.35]{figs/Homo_framework2.png}
    \caption{Framework of homotopy dynamics for solving sharp interface problem.}
    \label{fig:homotopy_dynamics_framework}
\end{figure}


The optimization challenges in solving PDEs significantly limit the applicability and development of neural network-based methods. Studies have shown that the loss functions for solving PDEs are often difficult to minimize, even in simple scenarios \cite{krishnapriyan2021characterizing,rathore2024challenges,xu2024overview,chen2024quantifying,chen2024automatic}. This difficulty is particularly pronounced in sharp interface problems, such as the Allen-Cahn equation \cite{allen1975coherent} and Burgers' equation \cite{burgers1948mathematical}, where solutions with small diffusion coefficients often exhibit localized regions of low regularity. These features introduce near-singularities in the loss function, causing the neural network optimization solvers to struggle with convergence, potentially leading to inaccurate solutions or even divergence.


The root of this challenge lies in the highly complex energy landscape of the loss function near singularities, which exacerbates optimization difficulties \cite{karniadakis2021physicsinformed,xu2024overview}. To address these issues, two main strategies have been proposed. The first is resampling, which involves adding more collocation points in regions of low regularity to better capture solution complexity \cite{wight2020solving,gao2024failure,ZHANG2025113561}. The second is designing multiscale neural network structures \cite{wang2020multi,liu2020multi,LIU2024112944,hao2024multiscale}, which aim to rescale the energy landscape and accelerate convergence. However, both approaches are computationally expensive, requiring a large number of collocation points or significantly increasing the network size.


In this paper, we introduce a novel approach based on homotopy dynamics to gradually reshape the complex energy landscape with respect to a specific coefficient. Rather than directly computing solutions near singularities, we leverage homotopy dynamics to trace a solution path that approximates them more effectively.
More specifically, we investigate the training challenges introduced by a parameter $\varepsilon$ in the PDE residual term within the loss functions.
As $\varepsilon$ decreases, the problem becomes more significantly difficult to solve. To understand this effect, we provide a theoretical analysis of how $\varepsilon$ influences the convergence of the training process. To address this issue, we propose a novel method called \textit{Homotopy Dynamics}. The key idea is to first train the neural network on PDEs with a large $\varepsilon$, where the problem is easier to learn and training is more efficient. Then, we gradually and adaptively adjust the neural network according to the evolution of the homotopy dynamics, guiding $\varepsilon$ toward its target value (as illustrated in \cref{fig:homotopy_dynamics_framework}). Although the homotopy approach has been used to train neural networks \cite{chen2019homotopy,yang2025homotopy}, this work is the first to apply homotopy dynamics to sharp interface problems in PDEs through the parameter $\varepsilon$.

%In \cite{wang2020multi,hao2024multiscale,liu2020multi}, the authors propose the use of multiscale neural networks to rescale the energy landscape. However, for such methods to be effective, one needs prior knowledge of the exact large-scale high-frequency components in the solution. This is particularly challenging, as the frequency content of the solution depends not only on the PDE itself but also on the regularity of the boundary conditions. If the boundary is complex, accurately determining the frequency characteristics of the solution becomes even more difficult. The only viable approach in such cases is to incorporate a sufficiently broad range of scales within the neural network to account for the unknown frequency components. However, this leads to a significant increase in the network's size, making the approach computationally expensive.


%In this paper, we propose a novel approach to solving PDEs with certain parameters. Our key observation is that, for most PDEs, when a given parameter is relatively large, the corresponding sharp interface solution remains smooth enough to be effectively learned by neural networks. However, as this parameter becomes extremely small, the solution develops very sharp regions, leading to near-singularities in the loss function, which significantly complicates the learning process. To support this observation, we provide both theoretical analysis and experimental validation. Based on this insight, we introduce a method called \textit{Homotopy Dynamics}. The core idea of this method is to first train the neural network on PDEs with a large parameter, where the solution is easier to learn. Then, we gradually and adaptively adjust the parameter according to the evolution of the dynamics with respect to the parameter, which follows a standard ordinary differential equation. In other words, we transform the complex training process of a neural network for sharp interface PDEs into a simpler training problem, which is then refined through a standard dynamical evolution. This \textit{Homotopy Dynamics} approach significantly outperforms traditional training algorithms in solving PDEs with sharp interface solutions, leading to much faster convergence and improved accuracy in capturing sharp features. 

\textbf{Contributions.} We highlight our key contributions in this paper as follows:

\begin{itemize}%[itemsep=0pt]
\setlength\itemsep{-1pt}

\item We propose a novel method, \textit{Homotopy Dynamics}, for using neural networks to solve sharp interface problems. Our results demonstrate that this method significantly improves the training process for such problems (\cref{sec: Method}).
\item  We theoretically analyze the impact of the parameter $\varepsilon$ in PDEs on training difficulty for interface problems and prove the convergence of the proposed \textit{Homotopy Dynamics} (\cref{sec:theory}).
\item We conduct extensive experiments on various problem settings, including sharp interface problems, high-frequency function approximation, and operator learning, showcasing the potential of our method to be applied to a broader range of problems (\cref{sec:Experiments}).

\end{itemize}
% As a result, the PINN often fails to learn the solution.
% Furthermore, optimization challenges can obscure the effectiveness of new neural network architectures for PINNs, 
% as an apparently inferior performance may stem from insufficient loss function optimization 
% rather than inherent limitations of an architecture.
% A simple, reliable training paradigm is critical to enable wider adoption of PINNs.

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.6]{figs/under_optimization_intro.pdf}
%     \caption{On the wave PDE, Adam converges slowly due to ill-conditioning and the combined \al{} optimizer stalls after about 40000 steps. Running NNCG (our method) after \al{} provides further improvement.}
%     \label{fig:under_optimization_intro}
% \end{figure}

% This work explores the loss landscape of PINNs and the challenges this landscape poses for gradient-based optimization methods.
% We provide insights from optimization theory that explain slow convergence of first-order methods such as Adam and show how ill-conditioned differential operators make optimization difficult.
% We also use our theoretical insights to improve the PINN training pipeline by combining existing and new optimization methods.


%The most closely related works to ours are \cite{chen2019homotopy,yang2025homotopy}, where the homotopy idea is utilized to design neural network structures. While these methods improve the training process for PDEs, they are not specifically tailored to address PDEs with sharp interface solutions. In contrast, the \textit{Homotopy Dynamics} approach proposed in this paper applies the homotopy concept directly to certain parameters within the PDE itself, rather than altering the neural network architecture. This novel perspective enables us to effectively tackle PDEs with sharp interface solutions.

%Additionally, the idea of using a homotopy approach to transition from solving simpler problems to more challenging ones is also evident in \cite{chen2019homotopy,yang2025homotopy}. In our work, we implement this homotopy strategy through the parameter $\varepsilon$ in the sharp interface problem, allowing for a systematic and adaptive training process.

% and also studies the empirical loss function instead of the population loss.
% \pnote{Make this comparison stronger}

% \begin{itemize}
%     \item Physics-informed ML is growing in popularity
%     \item PINNs are a particular example; they have been applied to blah, blah, and blah
%     \item However, PINNs are hard to train (elaborate on why). Elaborate on why optimization is the bottleneck
%     \item Talk about existing work on training of PINNs, discuss how L-BFGS seems to work better (cite some stuff here)
%     \item Talk about why we want (near-)zero loss for the forward PDE problem -- there is no noise in the training set, so interpolation makes sense
%     \item We want to understand the loss landscape, and use this understanding to propose better optimization strategies
%     \item It's difficult to determine whether improvements in architectures are real, or an artifact of insufficient optimization
%     \item Contributions of our work
%     \item Provide notation
% \end{itemize}

% \textbf{Contributions.} We highlight contributions of this paper:
% \begin{itemize}%[itemsep=0pt]
% \setlength\itemsep{-1pt}
%     % \item We show both theoretically and empirically why second-order methods are essential for obtaining good solutions with PINNs.
%     \item We demonstrate that the loss landscape of PINNs is ill-conditioned due to differential operators in the residual term and show that quasi-Newton methods improve the conditioning by 1000$\times$ or more (\cref{sec:loss_landscape}).
%     % We compare the loss landscape of PINNs to that of image classification tasks to show the difference in optimization difficulty.
%     \item We compare three optimizers frequently used for training PINNs: (i) Adam, (ii) \lbfgs{}, and (iii) Adam followed by L-BFGS (referred to as \al). 
%     We show that \al{} is superior across a variety of network sizes (\cref{sec:opt_comparison}).
%     % We also show the importance of using line search with L-BFGS for obtaining good performance.
    
%     % \item We show that PINN solutions resemble the true PDE solution only for extremely small loss values 
%     % (often $<1e^{-3})$ is needed.
%     % \al{} does not provide this level of convergence and so often yields nonsense solutions.

%     \item We show the PINN solution
%     resembles the true PDE solution only for extremely small loss values (\cref{sec:near_zero_loss}).
%     However, we find that the loss returned by \al{} can be improved further, which also improves the PINN solution (\cref{sec:under_optimized}).

%     \item Motivated by the ill-conditioned loss landscape, we introduce a novel second-order optimizer, NysNewton-CG (NNCG). 
%     %which uses Nystr\"{o}mPCG \citep{frangella2023randomized} to compute the Newton step (\cref{sec:under_optimized}). 
%     We show NNCG  can significantly improve the solution returned by \al{} (\cref{fig:under_optimization_intro,sec:under_optimized}). 
%     % which inspires several directions for future work.
%     % We show that PINNs training often suffers from under-optimization, which leads to subpar solutions.
%     % \item \textbf{Make Newton-CG separate from underoptimization. Say that optimizers are not giving sufficient convergence} We find that PINNs often suffer from under-optimization, which leads to subpar solutions. 
%     \item We prove that ill-conditioned differential operators lead to an ill-conditioned PINN loss (\cref{sec:theory}). We also prove that combining first- and second-order methods (e.g., \al) leads to fast convergence, providing justification for the importance of the combined method (\cref{sec:theory}).
%     % \item We give best practices for training PINNs that have not been clearly described in the literature.
% \end{itemize}



% \textbf{Notation.}
% We denote the Euclidean norm by $\|\cdot\|_2$ and use $\|M\|$ to denote the operator norm of $M\in \R^{m\times n}$.
% % For $M\in \R^{m\times n}$, we use $\|M\|$ to denote the operator norm of $M$.  
% For a smooth function $f: \R^p\rightarrow \R$, we denote its gradient at $w \in \R^p$ by $\nabla f(w)$ and its Hessian by $H_f(w)$.
% We write $\partial_{w_i}f$ for $\partial f/\partial w_i$.
% For $\Omega \subset \R^d$, we denote its boundary by $\partial \Omega$.
% For any $m\in \mathbb{N}$, we use $I_m$ to denote the $m\times m$ identity matrix. 
% Finally, we use $\preceq$ to denote the Loewner ordering on the convex cone of positive semidefinite matrices.
% %$\bigO(\cdot)$ and $\Omega(\cdot)$ are used to denote asymptotic upper and lower bounds respectively.   