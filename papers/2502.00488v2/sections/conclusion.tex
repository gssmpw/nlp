\section{Conclusion}

In this work, we explore the challenges of using neural networks to solve sharp interface problems. Specifically, we analyze the training difficulties caused by certain parameters in the PDEs. To overcome these challenges, we propose a training method based on homotopy dynamics to accelerate the training process for sharp interface problems. Our theoretical analysis supports the convergence of the proposed homotopy dynamics. Experimental results demonstrate that our method not only effectively captures the sharp interfaces but also facilitates the training of neural networks for approximating high-frequency functions, highlighting the potential of our approach for broader applications.

% Going forward, we can use the insights in this work to improve the PINN training pipeline.
% The techniques in this paper can be used to investigate whether existing PINN training strategies reduce ill-conditioning, which can help determine which strategies are the most effective and why.
% Furthermore, we could use the insights of this paper to craft new preconditioners for the PINN loss that will lead to faster convergence.
% Finally, since the PINN loss is challenging to optimize, we could apply higher-order optimization methods such as cubic regularized Newton \cite{nesterov2006cubic}, which may lead to faster convergence.

% Finally, we can extend our investigation of PINNs to physics-informed DeepONet, which incorporates differential operators in the loss, and is likely to suffer from the same ill-conditioning issues as PINNs.

% Our experiments and theory reveal the ill-conditioned nature of the PINN loss due to the differential operator in the loss, a key factor hindering effective training. By comparing Adam, L-BFGS, and Adam+L-BFGS optimizers, and introducing the novel NysNewton-CG, we made substantial strides in overcoming these challenges. The improved understanding and optimization strategies presented here not only enhance the current methodologies in PINN training but also set a foundation for future research to build upon.

% \begin{itemize}
%     \item Future work: superlinear convergence; Gauss-Newton approaches; investigating other architectures from the PINNs literature; do BatchNorm/LayerNorm improve problem conditioning; preconditioned stochastic optimization in place of Adam; automatic switching between Adam and \lbfgs; cubic-regularized Newton; randomized subspace methods; multiobjective optimization; Physics-informed DeepONet; float32 vs. float64; Note possible improvements to NysNewton-CG: Damping could be made automatic, optimizer is also quite slow due to hvps
% \end{itemize}