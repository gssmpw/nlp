\section{Ill-conditioned Differential Operators Lead to Difficult Optimization Problems}
In this section, we state and prove the formal version of \cref{thm:informal_ill_cond}.
The overall structure of the proof is based on showing the conditioning of the Gauss-Newton matrix of the population PINN loss is controlled by the conditioning of the differential operator.
We then show the empirical Gauss-Newton matrix is close to its population counterpart by using matrix concentration techniques. 
Finally, as the conditioning of $H_L$ at a minimizer is controlled by the empirical Gauss-Newton matrix, we obtain the desired result. 
% Part of the first portion of the analysis is similar to \citet{de2023operator}, but we perform no linearization. 
% So the quantities 

\label{section:ill-cond-D}
\subsection{Preliminaries}
Similar to \citet{de2023operator}, we consider a general linear PDE with Dirichlet boundary conditions:
\[
\begin{array}{ll}
    & \Dc[u](x) = f(x),\quad x\in \Omega, \\
    & u(x) = g(x), \quad x\in \partial \Omega,
\end{array}
\]
where $u: \R^d \mapsto \R$, $f:\R^d \mapsto \R$ and $\Omega$ is a bounded subset of $\R^d$.
The ``population'' PINN objective for this PDE is
\[
L_\infty(w) = \frac{1}{2}\int_{\Omega}\left(\Dc[u(x;w)]-f(x)\right)^2d\mu(x)+\frac{\lambda}{2} \int_{\partial \Omega}\left(u(x; w)-g(x)\right)^2d\sigma(x).
\]
$\lambda$ can be any positive real number; we set $\lambda = 1$ in our experiments.
Here $\mu$ and $\sigma$ are probability measures on $\Omega$ and $\partial \Omega$ respectively, from which the data is sampled. 
The empirical PINN objective is given by
\[
L(w) = \frac{1}{2\nres}\sum_{i=1}^{\nres}\left(\Dc[u(x^i_r;w)]-f(x_i)\right)^2+\frac{\lambda}{2\nbc}\sum_{j=1}^{\nbc}\left(u(x_b^j;w)-g(x_j)\right)^2.
\]
Moreover, throughout this section we use the notation $\langle f,g\rangle_{L^{2}(\Omega)}$ to denote the standard $L^2$-inner product on $\Omega$:
\[
\langle f,g\rangle_{L^{2}(\Omega)} = \int_{\Omega}fg d\mu(x).
\]


\begin{lemma}
    The Hessian of the $L_\infty(w)$ is given by
    \begin{align*}
    H_{L_\infty}(w) & = \int_{\Omega}\Dc[\nabla_w u(x;w)]\Dc[\nabla_w u(x;w)]^{T}d\mu(x)+\int_{\Omega}\Dc[\nabla^2_w u(x;w)]\left(\Dc[\nabla_w u(x;w)]-f(x)\right)d\mu(x)\\
    & + \lambda\int_{\partial \Omega}\nabla_w u(x; w)\nabla_w u(x; w)^{T}d\sigma(x) + \lambda\int_{\partial \Omega}\nabla^2_w u(x; w)\left(u(x;w)-g(x)\right)d\sigma(x). 
    \end{align*}
    The Hessian of $L(w)$ is given by
    \begin{align}
       H_L(w) & = \frac{1}{n_{\textup{res}}}\sum^{n_{\textup{res}}}_{i=1}\Dc[\nabla_w u(x_r^i; w)]\Dc[\nabla_w u(x_r^i; w)]^{T}+\frac{1}{n_{\textup{res}}}\sum^{n_{\textup{res}}}_{i=1}\Dc[\nabla^2_w u(x^r_i;w)]\left(\Dc[\nabla_w u(x_r^i;w)]-f(x_r^i)\right)\\
       & +\frac{\lambda}{n_{\textup{bc}}}\sum_{j=1}^{n_{\textup{bc}}}\nabla_w u(x_b^j;w)\nabla_w u(x_b^j;w)^{T} + \frac{\lambda}{n_{\textup{bc}}}\sum^{n_{\textup{bc}}}_{j=1}\nabla^2_w u(x_b^j;w)\left(u(x_b^j;w)-g(x_j)\right). \nonumber
    \end{align}
    In particular, for $w_\star\in \Wstar$
    \begin{align*}
        H_L(w_\star) = G_r(w)+ G_b(w).
    \end{align*}
    Here 
    \[
    G_r(w) \coloneqq \frac{1}{n_{\textup{res}}}\sum^{n_{\textup{res}}}_{i=1}\Dc[\nabla_w u(x_i; w_\star)]\Dc[\nabla_w u(x_i; w_\star)]^{T},\quad G_b(w) = \frac{\lambda}{n_{\textup{bc}}}\sum_{j=1}^{n_{\textup{bc}}}\nabla_w u(x_b^j;w_\star)\nabla_w u(x_b^j;w_\star)^{T}.
    \]
\end{lemma}
Define the maps $\F_{\textup{res}}(w) = \begin{bmatrix}
    \Dc[u(x_r^1;w)] \\
    \vdots \\
    \Dc[u(x_r^{\nres};w)]
\end{bmatrix}$,
and $\F_{\textup{bc}}(w) = \begin{bmatrix}
    u(x_b^1;w) \\
    \vdots \\
    u(x_b^{\nbc};w)]
\end{bmatrix}$.
We have the following important lemma, which follows via routine calculation. 
\begin{lemma}
\label{lemma:jac_ntk}
    Let $n = \nres+\nbc$. Define the map $\mathcal F:\R^p\rightarrow \R^{n}$, by stacking $\F_{\textup{res}}(w), \F_{\textup{bc}}(w)$.
    Then, the Jacobian of $\F$ is given by
    \[
    J_\F(w) = \begin{bmatrix}
        J_{\F_{\textup{res}}}(w) \\
        J_{\F_{\textup{bc}}}(w).
    \end{bmatrix}
    \]
    Moreover, the tangent kernel $K_\F(w) = J_\F(w)J_\F(w)^{T}$ is given by
    \[ K_\F(w) = 
    \begin{bmatrix}
        J_{\F_{\textup{res}}}(w)J_{\F_{\textup{res}}}(w)^{T} & J_{\F_{\textup{res}}}(w)J_{\F_{\textup{bc}}}(w)^{T}  \\
        J_{\F_{\textup{bc}}}(w)J_{\F_{\textup{res}}}(w)^{T} & J_{\F_{\textup{bc}}}(w)J_{\F_{\textup{bc}}}(w)^{T} 
    \end{bmatrix} =
    \begin{bmatrix}
        K_{\F_{\textup{res}}}(w) & J_{\F_{\textup{res}}}(w)J_{\F_{\textup{bc}}}(w)^{T}  \\
        J_{\F_{\textup{bc}}}(w)J_{\F_{\textup{res}}}(w)^{T} & K_{\F_{\textup{bc}}}(w) 
    \end{bmatrix}.
    \]
\end{lemma}

\subsection{Relating $G_{\infty}(w)$ to $\mathcal D$}
Isolate the population Gauss-Newton matrix for the residual:
\[
G_{\infty}(w) = \int_{\Omega}\Dc[\nabla_w u(x;w)]\Dc[\nabla_w u(x;w)]^{T}d\mu(x).
\]
Analogous to \citet{de2023operator} we define the functions $\phi_i(x;w) = \partial_{w_i}u(x;w)$ for $i\in\{1\dots,p\}$.
From this and the definition of $G_{\infty}(w)$, it follows that $\left(G_\infty(w)\right)_{ij} = \langle \Dc[\phi_i], \Dc[\phi_j]\rangle_{L^2(\Omega)}$.

Similar to \citet{de2023operator} we can associate each $w\in\R^p$ with a space of functions $\mathcal H(w) = \textup{span}\left(\phi_1(x;w),\dots,\phi_p(x;w)\right)\subset L^2(\Omega).$
We also define two linear maps associated with $\mathcal H(w)$:
\[
T(w)v = \sum_{i=1}^{p}v_i\phi_i(x;w),
\]
\[
T^{*}(w)f = \left(\langle f,\phi_1\rangle_{L^2(\Omega)},\dots,\langle f,\phi_p\rangle_{L^2(\Omega)}\right).
\]
From these definitions, we establish the following lemma. 
\begin{lemma}[Characterizing $G_{\infty}(w)$]
\label{lemma:Pop-GN}
Define $\mathcal A = \Dc^{*}\Dc$. 
Then the matrix $G_{\infty}(w)$ satisfies
    \[
    G_{\infty}(w) = T^{*}(w)\mathcal A T(w). 
    \]
\end{lemma}
\begin{proof}
Let $e_i$ and $e_j$ denote the $ith$ and $jth$ standard basis vectors in $\R^p$. 
Then,
\begin{align*}
(G_{\infty}(w))_{ij} &= \langle \Dc[\phi_i](w), \Dc[\phi_j](w)\rangle_{L^2(\Omega)} = \langle \phi_i(w),\Dc^{*}\Dc[\phi_j(w)] \rangle_{L^2(\Omega)} = \langle Te_i, \Dc^{*}\Dc[Te_j]\rangle_{L^2(\Omega)} \\
&= \langle e_i, (T^{*}\Dc^{*}\Dc T)[e_j]\rangle_{L^2(\Omega)},
\end{align*}
where the second equality follows from the definition of the adjoint. 
Hence, using $\mathcal A = \Dc^{*}\Dc$, we conclude $G_{\infty}(w) = T^{*}(w)\mathcal A T(w)$.
\end{proof}

Define the kernel integral operator $\mathcal K_{\infty}(w):L^2(\Omega)\rightarrow \mathcal H$ by
\begin{equation}
\label{eq:kern_op}
\mathcal K_{\infty}(w)[f](x) = T(w)T^{*}(w)f = \sum_{i=1}^{p}\langle f,\phi_i(x;w)\rangle \phi_i(x;w),
\end{equation}
and the kernel matrix $A(w)$ with entries $A_{ij}(w) = \langle \phi_{i}(x;w),\phi_{j}(x;w)\rangle_{L^2(\Omega)}$. 

Using \cref{lemma:Pop-GN} and applying the same logic as in the proof of Theorem 2.4 in \citet{de2023operator},
we obtain the following theorem. 
% We define the following weighted-inner product on $L^{2}(\Omega)$:
% \[
% \langle f,g\rangle = \langle f, \Kc_{\infty}g\rangle_{L_2(\Omega)}. 
% \]
\begin{theorem}
\label{thm:pop-gn-eigvals}
    Suppose that the matrix $A(w)$ is invertible.
    Then the eigenvalues of $G_{\infty}(w)$ satisfy
    \[
    \lambda_j(G_\infty(w)) = \lambda_j(\mathcal A\circ \Kc_\infty(w)),\quad \text{for all}~j\in[p].
    \]
\end{theorem}
% \begin{proof}
% \begin{align*}
%     \lamMax(G_{\infty}) &= \sup_{\|v\|=1}v^{T}G_{\infty}(w)v = \sup_{f\in \mathcal H, \|T^{*}f\| = 1}(T^{*}f)^{T}G_{\infty}(w)(T^{*}f) \\
%     &= \sup_{f\in \mathcal H,\|T^{*}f\| = 1}(T^{*}f)^{T}(T^{*}\mathcal A T)(T^{*}f) = \sup_{f\in \mathcal H, \|T^{*}f\| = 1}\langle f, TT^{*}(\mathcal ATT^{*}f)\rangle_{L^2(\Omega)}\\
%     & = \sup_{f\in \mathcal H,\|f\|_{\mathcal K_{\infty}} = 1}\langle f, \mathcal A\circ TT^{*}f\rangle_{\mathcal K_{\infty}} = \lamMax(\mathcal A\circ\Kc_{\infty})
% \end{align*}
% \end{proof}

% \begin{align*}
%     \langle f, (\mathcal A\circ TT^{*})g\rangle &= \langle f, TT^{*}(\mathcal A \circ TT^{*})g\rangle_{L^{2}(\Omega)} = \langle \mathcal A\circ TT^{*}f, TT^{*}g\rangle_{L^{2}(\Omega)}\\
%     &= \langle  (\A\circ TT^{*})f,g \rangle
% \end{align*}
% \[
% \kappa(G_{\infty}) = \kappa(\mathcal A\circ\Kc_{\infty})
% \]

\subsection{$G_r(w)$ Concentrates Around $G_{\infty}(w)$}
In order to relate the conditioning of the population objective to the empirical objective, we must relate the population Gauss-Newton residual matrix to its empirical counterpart. 
We accomplish this by showing $G_r(w)$ concentrates around $G_{\infty}(w)$. 
%Here we assume $\mu$ is a probability measure on $\Omega$, from which the training set is sampled.
To this end, we recall the following variant of the intrinsic dimension matrix Bernstein inequality from \citet{tropp2015introduction}.
\begin{theorem}[Intrinsic Dimension Matrix Bernstein]
 \label{thm:int_bern}
    Let $\{X_i\}_{i\in [n]}$ be a sequence of independent mean zero random matrices of the same size. 
    Suppose that the following conditions hold:
    \begin{align*}
        &\|X_i\|  \leq B,~\sum^{n}_{i=1}\mathbb E[X_i X_i^{T}]\preceq V_1,~\sum^{n}_{i=1}\mathbb E[X_i^{T} X_i]\preceq V_2.
    \end{align*}
    Define 
    \[\mathcal V = 
    \begin{bmatrix}
        V_1 & 0 \\
        0   &  V_2
    \end{bmatrix},~ \varsigma^2 = \max\{\|V_1\|,\|V_2\|\},
    \]
    and the \emph{intrinsic dimension} $d_{\textup{int}} = \frac{\textup{trace}(\mathcal V)}{\|\mathcal V\|}$.
    \newline
    Then for all $t\geq \varsigma+\frac{B}{3}$, 
    \[
    \mathbb P\left(\left\|\sum^{n}_{i=1}X_i\right\|\geq t\right) \leq 4d_{\textup{int}}\exp\left(-\frac{3}{8}\min\left\{\frac{t^2}{\varsigma^2},\frac{t}{B}\right\}\right).
    \]   
\end{theorem}

Next, we recall two key concepts from the kernel ridge regression literature and approximation via sampling literature: $\gamma$-\emph{effective dimension} and $\gamma$-\emph{ridge leverage coherence} \cite{bach2013sharp,cohen2017input,rudi2017falkon}. 
\begin{definition}[$\gamma$-Effective dimension and $\gamma$-ridge leverage coherence]
    Let $\gamma>0$. 
    Then the $\gamma$-effective dimension of $G_{\infty}(w)$ is given by
    \[
    d^{\gamma}_{\textup{eff}}(G_{\infty}(w)) = \textup{trace}\left(G_{\infty}(w)(G_{\infty}(w)+\gamma I)^{-1}\right).
    \]
    The $\gamma$-ridge leverage coherence is given by
    \[
    \chi^\gamma(G_{\infty}(w)) = \sup_{x\in \Omega}\frac{\left\|(G_{\infty}(w)+\gamma I)^{-1/2}\Dc[\nabla_w u(x;w)]\right\|^2}{\mathbb E_{x\sim \mu}\left\|(G_{\infty}(w)+\gamma I)^{-1/2}\Dc[\nabla_w u(x;w)]\right\|^2} = \frac{\sup_{x\in \Omega}{\left\|(G_{\infty}(w)+\gamma I)^{-1/2}\Dc[\nabla_w u(x;w)]\right\|^2}}{{d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}}.
    \]
\end{definition}
Observe that $d^{\gamma}_{\textup{eff}}(G_{\infty}(w))$ only depends upon $\gamma$ and $w$, while $\chi^\gamma(G_{\infty}(w)) $ only depends upon $\gamma, w,$ and $\Omega$. 
Moreover, $\chi^\gamma(G_{\infty}(w))<\infty$ as $\Omega$ is bounded. 

We prove the following lemma using the $\gamma$-effective dimension and $\gamma$-ridge leverage coherence in conjunction with \cref{thm:int_bern}.
\begin{lemma}[Finite-sample approximation]
\label{lemma:sampling}
Let $0<\gamma<\lambda_1(G_{\infty}(w))$. 
If $\nres\geq 40\chi^\gamma(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))\log\left(\frac{8d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\delta}\right)$, then with probability at least $1-\delta$
    \[
    \frac{1}{2}\left[G_\infty(w)-\gamma I\right] \preceq G_{r}(w)\preceq \frac{1}{2}\left[3 G_\infty(w)+\gamma I.\right]
    \]
\end{lemma}
\begin{proof}
    Let $x_i = (G_{\infty}(w)+\gamma I)^{-1/2}\Dc[\nabla_w u(x_i;w)]$, and $X_i = \frac{1}{\nres}\left(x_ix_i^{T}-D_\gamma\right)$, where $D_\gamma = G_{\infty}(w)\left(G_{\infty}(w)+\gamma I\right)^{-1}$.
    Clearly, $\mathbb E[X_i] = 0$. 
    Moreover, the $X_i$'s are bounded as
    \begin{align*}
    \|X_i\| & = \max\left\{\frac{\lamMax(X_i)}{\nres},-\frac{\lamMin(X_i)}{\nres}\right\} \leq \max\left\{\frac{\|x_i\|^2}{\nres}, \frac{\lamMax(-X_i)}{\nres}\right\} \leq \max\left\{\frac{\chi^{\gamma}(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\nres},\frac{1}{\nres}\right\} \\
    & = \frac{\chi^{\gamma}(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\nres}.
    \end{align*}
    Thus, it remains to verify the variance condition. 
    We have
    \begin{align*}
    \sum^{\nres}_{i=1}\mathbb E[X_iX_i^{T}] & = \nres\mathbb E[X_1^2] = \nres\times \frac{1}{\nres^2}\mathbb E[(x_1x_1^{T}-D_\gamma)^{2}]\preceq \frac{1}{\nres}\mathbb E[\|x_1\|^2 x_1 x_1^{T}] \\ 
    & \preceq \frac{\chi^{\gamma}(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\nres} D_\gamma. 
    \end{align*}
    Hence, the conditions of \cref{thm:int_bern} hold with $B = \frac{\chi^{\gamma}(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\nres}$ and $V_1 = V_2 = \frac{\chi^{\gamma}(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\nres} D_\gamma$.
    Now $1/2 \leq \|\mathcal V\|\leq 1$ as $\nres\geq \chi^{\gamma}(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))$ and $\gamma\leq \lambda_1\left(G_\infty(w)\right)$.
    Moreover, as $V_1 = V_2$ we have $d_{\textup{int}} \leq 4 d^{\gamma}_{\textup{eff}}(G_{\infty}(w))$. 
    So, setting 
    \[
    t = \sqrt{\frac{8\chi^\gamma(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))\log\left(\frac{8d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\delta}\right)}{3\nres}}+\frac{8\chi^\gamma(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))\log\left(\frac{8d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\delta}\right)}{3\nres}
    \]
    and using $\nres\geq 40\chi^\gamma(G_{\infty}(w)) d^{\gamma}_{\textup{eff}}(G_{\infty}(w))\log\left(\frac{8d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\delta}\right)$, we conclude
    \[\mathbb P\left(\left\|\sum_{i=1}^{\nres}X_i\right\|\geq \frac{1}{2}\right)\leq \delta.\]
    Now, $\left\|\sum_{i=1}^{\nres}X_i\right\|\leq \frac{1}{2}$ implies
    \[
    -\frac{1}{2}\left[G_{\infty}(w)+\gamma I\right]\preceq G_r(w)-G_{\infty}(w)\preceq \frac{1}{2}\left[G_{\infty}(w)+\gamma I\right].
    \]
    The claim now follows by rearrangement. 
\end{proof}

By combining \cref{thm:pop-gn-eigvals} and \cref{lemma:sampling}, we show that if the spectrum of $\A\circ \Kc_{\infty}(w)$ decays, then the spectrum of the empirical Gauss-Newton matrix also decays with high probability.  
\begin{proposition}[Spectrum of empirical Gauss-Newton matrix decays fast]
\label{prop:emp_gn_spectrum}
Suppose the eigenvalues of $\A\circ \Kc_{\infty}(w)$ satisfy $\lambda_j(\mathcal A\circ \Kc_{\infty}(w))\leq Cj^{-2\alpha}$, where $\alpha>1/2$ and $C>0$ is some absolute constant.
Then if $\sqrt{\nres}\geq 40C_1\chi^{\gamma}(G_{\infty}(w))\log\left(\frac{1}{\delta}\right)$, for some absolute constant $C_1$, it holds that
\[
  \lambda_{\nres}(G_r(w))\leq \nres^{-\alpha}
\]
with probability at least $1-\delta$.
         
\end{proposition}
\begin{proof} 
    The hypotheses on the decay of the eigenvalues implies $d^{\gamma}_{\textup{eff}}(G_{\infty}(w)) \leq C_1\gamma^{-\frac{1}{2\alpha}}$ (see Appendix C of \citet{bach2013sharp}).
    Consequently, given $\gamma = \nres^{-\alpha}$, we have $d^{\gamma}_{\textup{eff}}(G_{\infty}(w)) \leq C_1\nres^{\frac{1}{2}}$. 
    Combining this with our hypotheses on $\nres$, it follows $\nres\geq 40 C_1\chi^{\gamma}(G_{\infty}(w))d^{\gamma}_{\textup{eff}}(G_{\infty}(w))\log\left(\frac{8d^{\gamma}_{\textup{eff}}(G_{\infty}(w))}{\delta}\right)$.
    Hence \cref{lemma:sampling} implies with probability at least $1-\delta$ that 
    \[
    G_r(w)\preceq \frac{1}{2}\left(3 G_\infty(w)+\gamma I\right),
    \]
    which yields for any $1\leq r\leq n$
    \[
    \lambda_{\nres}(G_r(w))\leq \frac{1}{2}\left(3\lambda_r(G_\infty(w))+\gamma\right).
    \]
    Combining the last display with $\nres\geq 3d^{\gamma}_{\textup{eff}}(G_{\infty}(w))$, 
    Lemma 5.4 of \citet{frangella2023randomized} guarantees $\lambda_r(G_\infty(w))\leq \gamma/3$, and so 
    \[
    \lambda_{\nres}(G_r(w))\leq \frac{1}{2}\left(3\lambda_r(G_\infty(w))+\gamma\right)\leq \gamma \leq \nres^{-\alpha}.
    \]
\end{proof}

\subsection{Formal Statement of \cref{thm:informal_ill_cond} and Proof}
\begin{theorem}[An ill-conditioned differential operator leads to hard optimization]
    Fix $w_\star \in \Wstar$, and let $\mathcal S$ be a set containing $w_\star$ for which $\mathcal S$ is $\mu$-\PL.
    Let $\alpha>1/2$.
    If the eigenvalues of $\A\circ \Kc_{\infty}(w_\star)$ satisfy $\lambda_j(\mathcal A\circ \Kc_{\infty}(w_\star))\leq C j^{-2\alpha}$ and $\sqrt{\nres}\geq 40 C_1\chi^{\gamma}(G_{\infty}(w_\star))\log\left(\frac{1}{\delta}\right)$, then 
    \[
            \kappa_L(\mathcal S) \geq C_2\nres^{\alpha},
    \]
    with probability at least $1-\delta$.
    Here $C, C_1,$ and $C_2$ are absolute constants. 
        
\end{theorem}

\begin{proof}
    By the assumption on $\nres$, the conditions of \cref{prop:emp_gn_spectrum} are met, so, 
    \[
    \lambda_{\nres}(G_r(w_\star))\leq \nres^{-\alpha}.
    \] 
    with probability at least $1-\delta$.
    By definition $G_r(w_\star) = J_{\F_{\textup{res}}}(w_\star)^{T}J_{\F_{\textup{res}}}(w_\star)$, consequently,
    \[
    \lambda_{\nres}(K_{\F_{\textup{res}}}(w_\star)) = \lambda_{\nres}(G_r(w_\star)) \leq \nres^{-\alpha}.
    \] 
    Now, the \PL-constant for $\mathcal S$, satisfies $\mu = \inf_{w \in \mathcal S}\lambda_{n}(K_{\F}(w))$ \cite{liu2022loss}. 
    Combining this with the expression for $K_\F(w_\star)$ in \cref{lemma:jac_ntk}, we reach 
    \[
    \mu\leq \lambda_n(K_\F(w_\star))\leq \lambda_{\nres}(K_{\F_{\textup{res}}}(w_\star))\leq \nres^{-\alpha},
    \]
    where the second inequality follows from Cauchy's Interlacing theorem. 
    Recalling that $\kappa_L(\mathcal S) = \frac{\sup_{w \in \mathcal S}\|H_L(w)\|}{\mu}$, and $H_L(w_\star)$ is symmetric psd, we reach
    \begin{align*}
        \kappa_L(\mathcal S) \geq \frac{\lambda_1(H_L(w_\star))}{\mu}\overset{(1)}{\geq} \frac{\lambda_1(G_r(w_\star))+\lambda_p(G_b(w_\star))}{\mu} \overset{(2)}{=} \frac{\lambda_1(G_r(w_\star))}{\mu} \overset{(3)}{\geq} C_3\lambda_1(G_\infty(w_\star))\nres^{\alpha}. 
    \end{align*}
    Here $(1)$ uses $H_L(w_\star) = G_r(w_\star)+G_b(w_\star)$ and Weyl's inequalities, $(2)$ uses $p\geq \nres+\nbc$, so that $\lambda_p(G_b(w_\star)) = 0$.
    Inequality $(3)$ uses the upper bound on $\mu$ and the lower bound on $G_r(w)$ given in \cref{lemma:sampling}. 
    Hence, the claim follows with $C_2 = C_3\lambda_1(G_\infty(w_\star))$.
    %As $\mu = \inf_{w \in \mathcal S}\lambda_n(K(w))\leq n^{-\beta}$, and $\kappa_L(\mathcal S) = \sup_{w \in \mathcal S}\frac{\lambda_1(H_L(w))}{\mu}$, we have
    %\[
    %\kappa_L(\mathcal S)\geq \frac{\lambda_1(H_L(w_\star))}{\mu} = \frac{\lambda_1(K(w_\star))}{\mu} \geq C_{2}\nres^{\beta}.
    %\]
\end{proof}
\subsection{$\kappa$ Grows with the Number of Residual Points}
\label{subsec:kappa_grows}
\begin{figure*}
    \centering
    \includegraphics[scale=0.45]{figs/condition_number_bound.pdf}
    \caption{Estimated condition number after 41000 iterations of \al{} with different number of residual points from $255 \times 100$ grid on the interior. Here $\lambda_i$ denotes the $i$th largest eigenvalue of the Hessian. The model has $2$ layers and the hidden layer has width $32$. The plot shows $\kappa_L$ grows polynomially in the number of residual points.}
    \label{fig:condition_number_bound}
\end{figure*}
\Cref{fig:condition_number_bound} plots the ratio $\lambda_1(H_L)/\lambda_{129}(H_L)$ near a minimizer $w_\star$. This ratio is a lower bound for the condition number of $H_L$, and is computationally tractable to compute. 
We see that the estimate of the $\kappa$ grows polynomially with $\nres$, which provides empirical verification for \cref{thm:informal_ill_cond}.

% Suppose, $n\geq 4Cd^{\lambda}_{\textup{eff}}(G_{\infty}(w))\log\left(\frac{1}{\delta}\right)$
% which implies
% \[
% \frac{1}{2}\left[G_\infty(w)-\lambda I\right]\preceq G(w)\preceq \frac{1}{2}\left[3 G_\infty(w)+\lambda I\right].
% \]
% Thus, $\lambda_{r_\star}(G(w))\leq 2\lambda_{r_\star}(G_{\infty}(w)) = 2 \lambda_{r_\star}\left(\A\circ K_{\infty}\right)$.
% As $n\geq 4d^{\lambda}_{\textup{eff}}(G_{\infty}(w))$ it follows that
% \[
% \lambda_{n}(G(w))\leq \lambda.
% \]
% With $n \geq C\sqrt{n}\log\left(\frac{1}{\delta}\right)$, for fast poly-decay this becomes 
% \[
% \lambda_{n}(G(w))\leq n^{-\frac{\beta}{2}},
% \]
% while for exponential
% \[
% \lambda_{n}(G(w))\leq \exp(-\sqrt{n}).
% \]
% So GD will converge in $\mathcal O\left(n^{\beta/2}\log(1/\epsilon)\right)$, $\mathcal O\left(\exp{(\sqrt{n})}\log(1/\epsilon)\right)$

\section{Convergence of GDND (\cref{alg-GDND})}
\label{section:GDND_conv}
In this section, we provide the formal version of \cref{thm:GDND_informal} and its proof. 
However, this is delayed till \cref{subsec:GDND_conv}, as the theorem is a consequence of a series of results.
Before jumping to the theorem, we recommend reading the statements in the preceding subsections to understand the statement and corresponding proof. 
\subsection{Overview and Notation}
Recall, we are interested in minimizing the objective in \eqref{eq:pinn_prob_gen}:
\[
L(w) = \frac{1}{2\nres}\sum_{i=1}^{\nres}\left(\Dc[u(x_r^i;w)]\right)^2+\frac{1}{2\nbc}\sum_{j=1}^{\nbc}\left(\Bc[u(x_b^j;w)]\right)^2, 
\]
where $\Dc$ is the differential operator defining the PDE and $\Bc$ is the operator defining the boundary conditions. 
Define
\[
\F(w) = \begin{bmatrix}
    \frac{1}{\sqrt{\nres}}\Dc[u(x^1_r;w)] \\
    \vdots \\
    \frac{1}{\sqrt{\nres}}\Dc[u(x_r^{\nres};w)]\\
    \frac{1}{\sqrt{\nbc}}\Bc[u(x^1_b;w)] \\
    \vdots \\
    \frac{1}{\sqrt{\nbc}}\Bc[u(x_b^{\nbc};w)]
\end{bmatrix},~ y = 0
\]
Using the preceding definitions, our objective may be rewritten as:
\[
L(w) = \frac{1}{2}\|\F(w)-y\|^2.
\]
Throughout the appendix, we work with the condensed expression for the loss given above.
We denote the $(\nres+\nbc)\times p$ Jacobian matrix of $\mathcal F$ by $J_\F(w)$. 
%Note that $D\mathcal F(w) \in \mathbb{R}^{n\times p}$.
The tangent kernel at $w$ is given by the $n\times n$ matrix $K_\F(w) = J_\F(w) J_\F(w)^{T}$.
The closely related Gauss-Newton matrix is given by $G(w) = J_\F(w)^{T} J_\F(w)$.


\subsection{Global Behavior: Reaching a Small Ball About a Minimizer}
We begin by showing that under appropriate conditions, gradient descent outputs a point close to a minimizer after a fixed number of iterations.
We first start with the following assumption which is common in the neural network literature \cite{liu2022loss,liu2023aiming}.
\begin{assumption}
\label{assp:loss_reg}
     The mapping $\mathcal F(w)$ is $\mathcal L_\F$-Lipschitz, and the loss $L(w)$ is $\beta_{L}$-smooth.
\end{assumption}

% \begin{assumption}
%     Let $w_0$ denote the network weight at initialization, then such $L(w)$ is $\mu$-\PL in $B(w_0,2R)$, with $$.
% \end{assumption}

Under \Cref{assp:loss_reg} and a \PL-condition, we have the following theorem of \citet{liu2022loss}, which shows gradient descent converges linearly. 
\begin{theorem}
\label{thm:grad_dsct_conv}
    Let $w_0$ denote the network weights at initialization. 
    Suppose \Cref{assp:loss_reg} holds, and that $L(w)$ is $\mu$-P\L$^{\star}$ in $B(w_0,2R)$ with $R = \frac{2\sqrt{2\beta_{L}L(w_0)}}{\mu}$.
    Then the following statements hold:
    \begin{enumerate}
        \item The intersection $B(w_0,R)\cap\Wstar$ is non-empty.
        \item Gradient descent with step size $\eta = 1/\beta_L$ satisfies:
        \begin{align*}
        &w_{k+1} = w_k-\eta \nabla L(w_k)\in B(w_0,R)~ \text{for all } k\geq 0,\\
        &L(w_k)\leq \left(1-\frac{\mu}{\beta_L}\right)^kL(w_0).
        \end{align*}
    \end{enumerate}
\end{theorem}
For wide neural neural networks, it is known that the $\mu$-\PL condition in \cref{thm:grad_dsct_conv} hold with high probability, see \citet{liu2022loss} for details.

We also recall the following lemma from \citet{liu2023aiming}.
    \begin{lemma}[Descent Principle]
    \label{lemma:descent_principle}
        Let $L:\R^p\mapsto [0,\infty)$ be differentiable and $\mu$-\PL in the ball $B(w,r)$. 
        Suppose $L(w)<\frac{1}{2}\mu r^2$.
        Then the intersection $B(w,r)\cap \Wstar$ is non-empty, and
        \[
        \frac{\mu}{2}\dist^2(w,\Wstar)\leq L(w).
        \]
    \end{lemma}
        Let $\Lc_{H_L}$ be the Hessian Lipschitz constant in $B(w_0,2R)$, and $\Lc_{J_\F} = \sup_{w\in B(w_0,2R)}\|H_\F(w)\|$, where $\|H_\F(w)\| = \max_{i\in [n]}\|H_{\F_i}(w)\|$. 
        Define $M = \max\{\mathcal L_{\HL},\Lc_{J_\F},\mathcal \Lc_\F \Lc_{J_\F},1\}$,  $\epsLoc = \frac{\varepsilon \mu^{3/2}}{4M}$, where $\varepsilon\in (0,1)$.
        By combining \cref{thm:grad_dsct_conv} and \cref{lemma:descent_principle}, we are able to establish the following important corollary, which shows gradient descent outputs a point close to a minimizer.
    \begin{corollary}[Getting close to a minimizer]
        \label{corr:close_to_min}
        Set $\rho =  \min\left\{\frac{\epsLoc}{19\sqrt{\frac{\beta_L}{\mu}}},\sqrt{\mu}R,R\right\}$.
        Run gradient descent for $k = \frac{\beta_L}{\mu}\log\left(\frac{4\max\{2\beta_{L},1\}L(w_0)}{\mu\rho^2}\right)$ iterations, 
        gradient descent outputs a point $\wloc$ satisfying 
        \[
        L(\wloc) \leq \frac{\mu \rho^2}{4}\min\left\{1,\frac{1}{2\beta_L}\right\},
        \]
        \[
        \|\wloc-w_\star\|_{H_{L}(w_\star)+\mu I}\leq \rho,~ \text{for some}~ w_\star\in \Wstar.
        \]
    \end{corollary}
\begin{proof}
    The first claim about $L(\wloc)$ is an immediate consequence of \Cref{thm:grad_dsct_conv}.
    For the second claim, consider the ball $B(\wloc,\rho)$.
    Observe that $B(\wloc,\rho)\subset B(w_0,2R)$, so $L$ is $\mu$-\PL~in $B(\wloc,\rho)$.
    Combining this with $L(\wloc) \leq \frac{\mu \rho^2}{4}$, \Cref{lemma:descent_principle} guarantees the existence of $w_\star\in B(\wloc,\rho)\cap\Wstar$, with 
    $\|\wloc-w_\star\|\leq \sqrt{\frac{2}{\mu}L(\wloc)}$.
    Hence Cauchy-Schwarz yields 
    \begin{align*}
        \|\wloc-w_\star\|_{H_L(w_\star)+\mu I} & \leq \sqrt{\beta_L+\mu}\|\wloc-w_\star\| \leq \sqrt{2\beta_L}\|\wloc-w_\star\|\\
        & \leq 2\sqrt{\frac{\beta_L}{\mu}L(\wloc)} \leq 2 \times \sqrt{\frac{\beta_L}{\mu}\frac{\mu \rho^2}{8{\beta_L}}}\leq \rho,
    \end{align*}
   which proves the claim.
\end{proof}

% \begin{lemma}[Local quadratic growth and error bound]
%     Let $\wloc$ and $\bar w$ be as in blah, and suppose $B(\bar w,2\epsLoc)\subset B(w_0,R)$. 
%     Then the following statements hold:
%     \begin{enumerate}
%         %\item (Existence of local minimizer) The intersection $B(w_\star,\epsLoc)\cap \Wstar$ is non-empty.
%         \item (Local quadratic growth) $L(w)\geq \frac{\mu}{8}\dist^2(w,\Wstar)\quad \forall w\in B(\bar w,\epsLoc)$.
%         \item (Local error bound) $\|\nabla L(w)\|\geq \frac{\mu}{2}\dist(w,\Wstar)\quad \forall w\in B(\bar w,\epsLoc)$.
%     \end{enumerate}
% \end{lemma}


\subsection{Fast Local Convergence of Damped Newton's Method}
% Let $\delta\in (0,1)$.
% Define $g_k = \nabla L(\tilde w_k)$, $H_k = \nabla^2 L(\tilde w_k)$, $p_k = (H_k+\mu I+\|g_k\|^{\delta}I)^{-1}g_k$, and consider
% the iteration 
% \[\tilde w_{k+1} = \tilde w_k+p_k, \quad \tilde w_0 = \wloc.\]

% \begin{lemma}
%     For all $w\in B(w_\star, \epsLoc)$, it holds that
%     \[
%     \lambda_{\textup{min}}(H(w))\geq -\frac{\varepsilon\mu}{2}.
%     \]
%     Consequently, 
%     \[
%     (H(w)+\mu I+\|g(w)\|^{\delta})\succ 0, \quad \forall w\in B(w_\star,\epsLoc).
%     \]
% \end{lemma}

% \begin{lemma}
%     Suppose $\tilde w_k \in B(w_\star,\epsLoc/2)$. Then
%     \[
%     \|d_k\| \leq \lambda ~\dist(\tilde w_k,\Wstar),
%     \]
%     where $\lambda = \left(\max\{1,\varepsilon/(2-\varepsilon)\}+\frac{L_{\HL}}{\mu^{\delta}}\right).$
% \end{lemma}

% \begin{proof}
%     \begin{align*}
%         \|d_k\| & = \left\|(H_k+\mu I+\|g_k\|^{\delta})^{-1}g_k\right\| \\
%         &= \left\|(H_k+\mu I+\|g_k\|^{\delta})^{-1}\left[g_k-g(\bar w_k)+H_k(w_k-\bar w_k)-H_k(w_k-\bar w_k)\right]\right\|\\
%         &\leq \left\|(H_k+\mu I+\|g_k\|^{\delta})^{-1}\left[g_k-g(\bar w_k)+H_k(w_k-\bar w_k)\right]\right\|+\left\|(H_k+\mu I+\|g_k\|^{\delta})^{-1}H_k(w_k-\bar w_k)\right\|.
%     \end{align*}
%     For term $T_1$, observe that
%     \begin{align*}
%         &\left\|(H_k+\mu I+\|g_k\|^{\delta})^{-1}\left[g_k-g(\bar w_k)+H_k(w_k-\bar w_k)\right]\right\| \leq \|g_k\|^{-\delta}\frac{L_{\HL}}{2}\|w_k-\bar w_k\|^2\\
%         &= \|g_k\|^{-\delta}\frac{L_{\HL}}{2}\dist^2(w_k,\Wstar) \leq (2/\mu)^{\delta}\frac{L_{\HL}}{2}\dist^{2-\delta}(w_k,\Wstar) \leq \frac{L_{\HL}}{\mu^{\delta}}\dist(w_k,\Wstar).
%     \end{align*}
%     For term $T_2$, note that
%     \[
%     \left\|(H_k+\mu I+\|g_k\|^{\delta})^{-1}H_k\right\|
%     \]
% \end{proof}

% \begin{lemma}[Staying in the ball]
%     Suppose that $\tilde w_k \in B(\bar w,\epsLoc/(1+\lambda))$, then
%     \[
%     \tilde w_k+\eta p_k \in B(\bar w, \epsLoc),\quad \forall \eta \in [0,1]. 
%     \]
% \end{lemma}
% \begin{proof}
%     \begin{align*}
%         \|\tilde w_k+\eta p_k-\bar w\| &\leq \|\tilde w_k-\bar w\|+\|p_k\|\leq \|\tilde w_k-\bar w\|+\lambda \dist(\tilde w_k,W_\star) = (1+\lambda)\|\tilde w_k-\bar w\|
%         \\ &\leq \epsLoc.
%     \end{align*}
% \end{proof}

% \begin{lemma}[Sufficient descent]
    
% \end{lemma}
In this section, we show damped Newton's method with fixed stepsize exhibits fast linear convergence in an appropriate region about the minimizer $w_\star$ from \cref{corr:close_to_min}. 
Fix $\varepsilon \in (0,1)$, then the region of local convergence is given by:
\[
\Neps = \left\{w\in \R^p: \|w-w_\star\|_{H_L(w_\star)+\mu I}\leq \epsLoc\right\},
\]
where $\epsLoc = \frac{\varepsilon \mu^{3/2}}{4M}$ as above. 
Note that $\wloc \in \Neps$.

We now prove several lemmas, that are essential to the argument. 
We begin with the following elementary technical result, which shall be used repeatedly below.  
\begin{lemma}[Sandwich lemma]
\label{lemma:sandwich}
    Let $A$ be a symmetric matrix and $B$ be a symmetric positive-definite matrix.
    Suppose that $A$ and $B$ satisfy $\|A-B\|\leq \varepsilon \lambda_{\textup{min}}(B)$ where $\varepsilon \in (0,1)$.
    Then
    \[
    (1-\varepsilon)B\preceq A\preceq (1+\varepsilon) B. 
    \]
\end{lemma}
\begin{proof}
    By hypothesis, it holds that
    \[
    -\varepsilon \lambda_{\textup{min}}(B)I\preceq A-B \preceq \varepsilon\lambda_{\textup{min}}(B) I.
    \]
    So using $B\succeq \lambda_{\textup{min}}(B) I$, and adding $B$ to both sides, we reach
    \[
    (1-\varepsilon)B \preceq A\preceq (1+\varepsilon) B.
    \]
\end{proof}


%Define $P = \HL(\wloc)+\lambda I$, the following lemma shows $P$ is positive definite. 
The next result describes the behavior of the damped Hessian in $\Neps$.
\begin{lemma}[Damped Hessian in $\Neps$]
\label{lemma:local_hess}
Suppose that $\gamma \geq \mu$ and $\varepsilon\in (0,1)$. 
\begin{enumerate}
    \item (Positive-definiteness of damped Hessian in $\Neps$) For any $w\in \Neps$, 
    \[
    \HL(w)+\gamma I \succeq \left(1-\frac{\varepsilon}{4}\right)\gamma I.
    \]
    \item (Damped Hessians stay close in $\Neps$)
    For any $w,w' \in \Neps$,
    \[
    (1-\varepsilon)\left[\HL(w)+\gamma I\right] \preceq \HL(w')+\gamma I \preceq (1+\varepsilon) \left[\HL(w)+\gamma I\right].
    \]
\end{enumerate}
% Then it holds that 
% \[
% P\succ \frac{\mu}{2} I.
% \]
\end{lemma}
\begin{proof}
    We begin by observing that the damped Hessian at $w_\star$ satisfies
    \begin{align*}
        \HL(w_\star)+\gamma I & = G(w_\star)+\gamma I+\frac{1}{n}\sum_{i=1}^{n}\left[\F(w_\star)-y\right]_{i}H_{\mathcal F_i}(w_\star)\\
        &= G(w_\star)+\gamma I \succeq \gamma I.
    \end{align*}
    Thus, $\HL(w_\star)+\gamma I$ is positive definite. 
    Now, for any $w\in \Neps$, it follows from Lipschitzness of $\HL$ that
    \begin{align*}
        \left\|\left(\HL(w)+\gamma I\right)-\left(\HL(w_\star)+\gamma I\right)\right\|\leq \Lc_{\HL}\|w-w_\star\|\leq \frac{\Lc_{\HL}}{\sqrt{\gamma}}\|w-w_\star\|_{\HL(w_\star)+\gamma I} \leq \frac{\varepsilon \mu}{4}.
    \end{align*}
    As $\lamMin\left(\HL(w_\star)+\gamma I\right)\geq \gamma>\mu$, we may invoke \Cref{lemma:sandwich} to reach 
    \[
        \left(1-\frac{\varepsilon}{4}\right)\left[\HL(w_\star)+\gamma I\right]\preceq \HL(w)+\gamma I \preceq \left(1+\frac{\varepsilon}{4}\right)\left[\HL(w_\star)+\gamma I\right].
    \]
    This immediately yields 
    \[
    \lamMin\left(\HL(w)+\gamma I\right)\geq \left(1-\frac{\varepsilon}{4}\right)\gamma \geq \frac{3}{4}\gamma,
    \]
    which proves item 1. 
    To see the second claim, observe for any $w,w'\in \Neps$ the triangle inequality implies
    \[
    \left\|\left(\HL(w')+\gamma I\right)-\left(\HL(w)+\gamma I\right)\right\|\leq \frac{\varepsilon \mu}{2} \leq \frac{2}{3}\varepsilon\left(\frac{3}{4}\gamma\right).
    \]
    As $\lamMin\left(\HL(w)+\gamma I\right)\geq \frac{3}{4}\gamma $, it follows from \Cref{lemma:sandwich} that
    \[
    \left(1-\frac{2}{3}\varepsilon\right)\left[\HL(w)+\gamma I\right]\preceq \HL(w')+\gamma I \preceq \left(1+\frac{2}{3}\varepsilon\right)\left[\HL(w)+\gamma I\right],
    \]
    which establishes item 2. 
\end{proof}

The next result characterizes the behavior of the tangent kernel and Gauss-Newton matrix in $\Neps$.
\begin{lemma}[Tangent kernel and Gauss-Newton matrix in $\Neps$]
\label{lemma:local_gn}
    Let $\gamma \geq \mu$. Then for any $w,w'\in \Neps$, the following statements hold:
    \begin{enumerate}
        %\item $(1-\varepsilon)\left[G(\wloc)+\gamma I\right] \preceq G(w)+\gamma I\preceq (1+\varepsilon)\left[G(\wloc)+\gamma I\right] $.
        \item (Tangent kernels stay close) 
        \[
        \left(1-\frac{\varepsilon}{2}\right)K_\F(w_\star)\preceq K_\F(w) \preceq \left(1+\frac{\varepsilon}{2}\right) K_\F(w_\star)
        \]
        \item (Gauss-Newton matrices stay close)
        \[
         \left(1-\frac{\varepsilon}{2}\right)\left[G(w)+\gamma I\right]\preceq G(w_\star)+\gamma I \preceq \left(1+\frac{\varepsilon}{2}\right) \left[G(w)
        +\gamma I\right]\]
        \item (Damped Hessian is close to damped Gauss-Newton matrix) 
        \[
        (1-\varepsilon)\left[G(w)+\gamma I\right] \preceq \HL(w)+\gamma I \preceq (1+\varepsilon)\left[G(w)+\gamma I\right].
        \]
        \item (Jacobian has full row-rank) The Jacobian satisfies $\textup{rank}(J_{\F}(w)) = n$.
    \end{enumerate}
\end{lemma}
\begin{proof}
\begin{enumerate}
    \item Observe that
    \begin{align*}
        \|K_\F(w)-K_\F(w_\star)\| &= \|J_{\F}(w)J_{\F}(w)^{T}-J_{\F}(w_\star)J_{\F}(w_\star)^{T}\| \\
                  &= \left\|\left[J_{\F}(w)-J_{\F}(w_\star)\right]J_{\F}(w)^{T}+J_{\F}(w_\star)\left[J_{\F}(w)-J_{\F}(w_\star)\right]^{T}\right\| \\
                  &\leq 2 \Lc_\F \Lc_{J_\F}\|w-w_\star\| \leq \frac{2 \Lc_\F \Lc_{J_\F}}{\sqrt{\gamma}}\|w-w_\star\|_{H_L(w_\star)+\gamma I} \leq \frac{\varepsilon\mu^{3/2}}{\sqrt{\gamma}} \leq \frac{\varepsilon}{2} \mu,
    \end{align*}
    where in the first inequality we applied the fundamental theorem of calculus to reach 
    \[
    \|J_{\F}(w)-J_{\F}(w_\star)\|\leq \Lc_{J_{\F}}\|w-w_\star\|.
    \]
    Hence the claim follows from \Cref{lemma:sandwich}.
    \item By an analogous argument to item 1, we find
    \[
    \left\|\left(G(w)+\gamma I\right)-\left(G(w_\star)+\gamma I\right)\right\| \leq \frac{\varepsilon}{2}\mu,
    \]
    so the result again follows from \Cref{lemma:sandwich}.
    \item First observe $\HL(w_\star)+\gamma I = G(w_\star)+\gamma I$. Hence the proof of \Cref{lemma:local_hess} implies,
    \[
    \left(1-\frac{\varepsilon}{4}\right)\left[G(w_\star)
        +\gamma I\right]\preceq \HL(w)+\gamma I\preceq \left(1+\frac{\varepsilon}{4}\right)\left[G(w_\star)
        +\gamma I\right].
    \]
    Hence the claim now follows from combining the last display with item 2. 
    \item This last claim follows immediately from item 1, as for any $w\in \Neps$,
    \[
    \sigma_{n}\left(J_{\F}(w)\right) = \sqrt{\lamMin(K_\F(w))}\geq \sqrt{\left(1-\frac{\varepsilon}{2}\right)\mu}>0.
    \]   
    Here the last inequality uses $\lamMin(K_\F(w_\star))\geq \mu$, which follows as $w_\star\in B(w_0,2R)$.
\end{enumerate}

\end{proof}

The next lemma is essential to proving convergence. It shows in $\Neps$ that $L(w)$ is uniformly smooth with respect to the damped Hessian, with nice smoothness constant $(1+\varepsilon)$. 
Moreover, it establishes that the loss is uniformly \PL with respect to the damped Hessian in $\Neps$. 
\begin{lemma}[Preconditioned smoothness and \PL]
\label{lemma:local-sm-pl}
    Suppose $\gamma \geq \mu$. Then 
    for any $w,w',w''\in \Neps$, the following statements hold:
    \begin{enumerate}
        \item $L(w'')\leq L(w')+\langle \nabla L(w'),w''-w'\rangle +\frac{1+\varepsilon}{2}\|w''-w'\|_{H_L(w)+\gamma I}^2$.
        \item $\frac{\|\nabla L(w)\|_{(\HL(w)+\gamma I)^{-1}}^2}{2}\geq \frac{1}{1+\varepsilon}\frac{1}{\left(1+\gamma/\mu\right)}L(w)$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    \begin{enumerate}
        \item By Taylor's theorem
        \begin{align*}
        L(w'') = L(w')+\langle \nabla L(w'),w''-w'\rangle+\int_{0}^{1}(1-t)\|w''-w'\|_{\HL(w'+t(w''-w'))}^2 dt
        \end{align*}
        Note $w'+t(w''-w')\in \Neps $ as $\Neps$ is convex.
        Thus we have,
        \begin{align*}
            L(w'') & \leq L(w')+\langle \nabla L(w'),w''-w'\rangle+\int_{0}^{1}(1-t)\|w''-w'\|_{\HL(w'+t(w''-w'))+\gamma I}^2dt \\
            & \leq L(w')+\langle \nabla L(w'),w''-w'\rangle+\int_{0}^{1}(1-t)(1+\varepsilon)\|w''-w'\|_{\HL(w)+\gamma I}^2dt \\
            & = L(w')+\langle \nabla L(w'),w''-w'\rangle+\frac{(1+\varepsilon)}{2}\|w''-w'\|_{\HL(w)+\gamma I}^2.  
        \end{align*}
    
        \item Observe that
        \begin{align*}
            \frac{\|\nabla L(w)\|_{(\HL(w)+\gamma I)^{-1}}^2}{2} = \frac{1}{2}(\F(w)-y)^{T}\left[J_{\F}(w)\left(\HL(w)+\gamma I\right)^{-1}J_{\F}(w)^{T}\right](\F(w)-y).
        \end{align*}
        Now,
        \begin{align*}
            %J_{\F}(w)P^{-1}J_{\F}(w)^{T} 
            J_{\F}(w)\left(\HL(w)+\gamma I\right)^{-1}J_{\F}(w)^{T} & \succeq \frac{1}{(1+\varepsilon)}J_{\F}(w)\left(G(w)+\gamma I\right)^{-1}J_{\F}(w)^{T}\\ 
             &= \frac{1}{(1+\varepsilon)}J_{\F}(w)\left(J_{\F}(w)^{T}J_{\F}(w)+\gamma I\right)^{-1}J_{\F}(w)^{T}\\ 
        \end{align*}
        \Cref{lemma:local_gn} guarantees $J_{\F}(w)$ has full row-rank, so the SVD yields
        \[
        J_{\F}(w)\left(J_{\F}(w)^{T}J_{\F}(w)+\gamma I\right)^{-1}J_{\F}(w)^{T} = U\Sigma^2(\Sigma^2+\gamma I)^{-1}U^{T}\succeq \frac{\mu}{\mu+\gamma} I.
        \]
        Hence
        \[
          \frac{\|\nabla L(w)\|_{(\HL(w)+\gamma I)^{-1}}^2}{2}\geq \frac{\mu}{(1+\varepsilon)(\mu+\gamma)}\frac{1}{2}\|\F(w)-y\|^2 = \frac{\mu}{(1+\varepsilon)(\mu+\gamma)}L(w).
        \]
    \end{enumerate}
\end{proof}

% \begin{lemma}
%     Let $w_k \in \Neps$. Then 
%     \[
%     \|p_k\|_{P}\leq
%     \]
% \end{lemma}
% \begin{proof}
%     \begin{align*}
%         \|p_k\|_{P} & = \|\nabla L(w_k)\|_{P^{-1}}\leq \|\nabla L(w_k)-\nabla L(\bar w_k)-\nabla^2L(\bar w_k)(w_k-\bar w_k)\|_{P^{-1}}+\|\nabla^2L(\bar w_k)(w_k-\bar w_k)\|_{P^{-1}} \\
%         &= \|\nabla L(w_k)-\nabla L(\bar w_k)-\nabla^2L(\bar w_k)(w_k-\bar w_k)\|_{P^{-1}}+\|\nabla^2L(\bar w_k)^{1/2}(w_k-\bar w_k)\|_{\nabla^2L(\bar w_k)^{1/2}P^{-1}\nabla^2L(\bar w_k)^{1/2}} \\
%         &\leq \frac{1}{\sqrt{1-\varepsilon}}\|\nabla L(w_k)-\nabla L(\bar w_k)-\nabla^2L(\bar w_k)(w_k-\bar w_k)\|_{(\nabla^2 L(\bar w_k)+\rho I)^{-1}}\\
%         &+\|\nabla^2L(\bar w_k)^{1/2}(w_k-\bar w_k)\|_{\nabla^2L(\bar w_k)^{1/2}P^{-1}\nabla^2L(\bar w_k)^{1/2}}. 
%     \end{align*}
%     Now,
%     \begin{align*}
%         & \|\nabla L(w_k)-\nabla L(\bar w_k)-\nabla^2L(\bar w_k)(w_k-\bar w_k)\|_{(\nabla^2 L(\bar w_k)+\rho I)^{-1}} = \\
%         & \left\|\int_{0}^{1}\left[\nabla^2 L(\bar w_k+t(w_k-\bar w_k))-\nabla^2L(\bar w_k)\right](w_k-\bar w_k)\right\|_{(\nabla^2 L(\bar w_k)+\rho I)^{-1}}
%     \end{align*}
% \end{proof}
\begin{lemma}[Local preconditioned-descent]
\label{lemma:local_descent}
    Run Phase II of \cref{alg-GDND} with $\eta_{\textup{DN}} = (1+\varepsilon)^{-1}$ and $\gamma = \mu$. 
    Suppose that $\tilde w_{k}, \tilde w_{k+1}\in \Neps$, then
    \[
     L(\tilde w_{k+1})\leq \left(1-\frac{1}{2(1+\varepsilon)^2}\right)L(\tilde w_k).
    \]
\end{lemma}
\begin{proof}
    As $\tilde w_k, \tilde w_{k+1}\in \Neps$, item 1 of \Cref{lemma:local-sm-pl} yields
    \[
    L(\tilde w_{k+1})\leq L(\tilde w_k)-\frac{\|\nabla L(\tilde w_k)\|^2_{(\HL(\tilde w_k)+\mu I)^{-1}}}{2(1+\varepsilon)}.
    \]
    Combining the last display with the preconditioned \PL condition, 
    we conclude
    \[
    L(\tilde w_{k+1})\leq \left(1-\frac{1}{2(1+\varepsilon)^2}\right)L(\tilde w_k).
    \]
\end{proof}

The following lemma describes how far an iterate moves after one-step of Phase II of \cref{alg-GDND}.
\begin{lemma}[1-step evolution]
\label{lemma:one_step_evol}
    Run Phase II of \cref{alg-GDND} with $\eta_{\textup{DN}} = (1+\varepsilon)^{-1}$ and $\gamma \geq \mu$.
    Suppose $\tilde w_k \in \N_{\frac{\varepsilon}{3}}(w_\star)$, then $\tilde w_{k+1}\in \Neps$.
\end{lemma}

\begin{proof}
    Let $P = H_L(\tilde w_k)+\gamma I$.
    We begin by observing that 
    \begin{align*}
        \|\tilde w_{k+1}-w_\star\|_{\HL(w_\star)+\mu I}\leq \sqrt{1+\varepsilon}\|\tilde w_{k+1}-w_\star\|_{P}.
    \end{align*}
    Now,
    \begin{align*}
        \|\tilde w_{k+1}-w_\star\|_P & = \frac{1}{1+\varepsilon}\|\nabla L(\tilde w_{k})-\nabla L(w_\star)-(1+\varepsilon)P(w_\star-\tilde w_k)\|_{P^{-1}} \\ &=
        \frac{1}{1+\varepsilon}\left\|\int_{0}^{1}\left[\nabla^2 L(w_\star+t(w_k-w_\star))-(1+\varepsilon)P\right]dt(w_\star-\tilde w_k)\right\|_{P^{-1}} \\
        & =   \frac{1}{1+\varepsilon}\left\|\int_{0}^{1}\left[P^{-1/2}\nabla^2 L(w_\star+t(w_k-w_\star))P^{-1/2}-(1+\varepsilon)I\right]dtP^{1/2}(w_\star-\tilde w_k)\right\|\\
        &\leq \frac{1}{1+\varepsilon}\int_{0}^{1}\left\|P^{-1/2}\nabla^2 L(w_\star+t(w_k-w_\star))P^{-1/2}-(1+\varepsilon)I\right\|dt\|\tilde w_k-w_\star\|_{P}
    \end{align*}
    We now analyze the matrix $P^{-1/2}\nabla^2 L(w_\star+t(w_k-w_\star))P^{-1/2}$. 
    Observe that
    \begin{align*}
        & P^{-1/2}\nabla^2 L(w_\star+t(w_k-w_\star))P^{-1/2} = P^{-1/2}(\nabla^2 L(w_\star+t(w_k-w_\star))+\gamma I-\gamma I)P^{-1/2} \\
        & = P^{-1/2}(\nabla^2 L(w_\star+t(w_k-w_\star))+\gamma I)P^{-1/2}-\gamma P^{-1} \succeq (1-\varepsilon)I-\gamma P^{-1} \succeq -\varepsilon I.
        %&= I-\rho P^{-1}+P^{-1/2}EP^{-1/2}\succeq -\|E\|P^{-1} \succeq -(\varepsilon \mu)P^{-1}\succeq -\varepsilon I.
    \end{align*}
    Moreover,
    \[
    P^{-1/2}\nabla^2 L(w_\star+t(w_k-w_\star))P^{-1/2}\preceq P^{-1/2}(\nabla^2 L(w_\star+t(w_k-w_\star))+\gamma I)P^{-1/2}\preceq (1+\varepsilon)I.
    \]
    Hence, 
    \[0 \preceq (1+\varepsilon)I-P^{-1/2}\nabla^2 L(w_\star+t(w_k-w_\star))P^{-1/2}\preceq (1+2\varepsilon)I,\] 
    and so
    \[
    \|\tilde w^{k+1}-w_\star\|_P\leq \frac{1+2\varepsilon}{1+\varepsilon}\|\tilde w_k-w_\star\|_{P}.
    \]
    Thus,
    \[
    \|\tilde w^{k+1}-w_\star\|_{\HL(w_\star)+\mu I}\leq \frac{1+2\varepsilon}{\sqrt{1+\varepsilon}}\|\tilde w_k-w_\star\|_P \leq(1+2\varepsilon)\|\tilde w_k-w_\star\|_{\HL(w_\star)+\mu I}\leq \epsLoc.
    \]
\end{proof}

The following lemma is key to establishing fast local convergence; it shows that the iterates produced by damped Newton's method remain in $\Neps$, the region of local convergence. 
\begin{lemma}[Staying in $\Neps$]
\label{lemma:trapped}
    Suppose that $\wloc \in \mathcal N_{\rho}(w_\star)$, where $\rho = \frac{\epsLoc}{19\sqrt{\beta_L/\mu}}$.
    Run Phase II of \cref{alg-GDND} with $\gamma = \mu$ and $\eta = (1+\varepsilon)^{-1}$, then $\tilde w_{k+1} \in \Neps$ for all $k\geq 1$.
\end{lemma}
\begin{proof}
  In the argument that follows $\kappa_P = 2(1+\varepsilon)^2$.
  The proof is via induction. 
  Observe that if $\wloc \in \mathcal N_{\varrho}(w_\star)$ then by \Cref{lemma:one_step_evol}, $\tilde w_1 \in \Neps$.  
  %\[
  %\|\tilde w_1-w_\star\|_{\HL(w_\star)+\mu I}\leq 3\|\wloc-w_\star\|_{\HL(w_\star)+\mu I} \leq \epsLoc,
  %\]
  Now assume $\tilde w_j \in \Neps$ for $j = 2,\dots, k$. 
  We shall show $\tilde w_{k+1}\in \Neps$.
  To this end, observe that
  \begin{align*}
  \|\tilde w_{k+1}-w_\star\|_{\HL(w_\star)+\mu I} & \leq \|\wloc-w_\star\|_{\HL(w_\star)+\mu I}+\frac{1}{1+\varepsilon}\sum_{j=1}^{k}\|\nabla L(w_j)\|_{\left(\HL(w_\star)+\mu I\right)^{-1}} \\
  % &\leq \varrho+\sqrt{\frac{2}{1+\varepsilon}}\sum_{j=1}^{k}\sqrt{L(\tilde w_{j})} \leq \varrho+\sqrt{\frac{2}{1+\varepsilon}}\sum_{j=1}^{k}\left(1-\frac{\mu_P}{L_P}\right)^{t/2}\sqrt{L(\tilde w_0)} \\
  % &\leq \varrho+\|\wloc - w_\star\|_{\HL(w_\star)+\mu I}\sum_{j=1}^{k}\left(1-\frac{\mu_P}{L_P}\right)^{t/2}\leq \left(1+\sum_{k=0}^{\infty}\left(1-\frac{\mu_P}{L_P}\right)^{t/2}\right)\varrho\\
  % & = \left(1+\frac{1}{{1-\sqrt{1-\frac{\mu_P}{L_P}}}}\right)\varrho\leq \epsLoc.
  \end{align*}
  Now,
  \begin{align*}
      \|\nabla L(w_j)\|_{\left(\HL(w_\star)+\mu I\right)^{-1}} &\leq \frac{1}{\sqrt{\mu}}\|\nabla L(w_j)\|_2 \leq \sqrt{\frac{2\beta_L}{\mu}L(w_j) }\\
      &\leq \sqrt{\frac{2\beta_L}{\mu}}\left(1-\frac{1}{\kappa_P}\right)^{j/2}\sqrt{L(\wloc)},
  \end{align*}
  Here the second inequality follows from $\|\nabla L(w)\| \leq \sqrt{2\beta_L L(w)}$, and the last inequality follows from \Cref{lemma:local_descent}, which is applicable as $\tilde w_{0},\dots,\tilde w_k \in \Neps$. 
  Thus,
  \begin{align*}
  \|\tilde w_{k+1}-w_\star\|_{\HL(w_\star)+\mu I} &\leq \rho+\sqrt{\frac{2\beta_L}{\mu}}\sum_{j=1}^{k}\left(1-\frac{1}{\kappa_P}\right)^{j/2}\sqrt{L(\tilde w_0)} \\
  &\leq \rho+\sqrt{\frac{(1+\varepsilon)\beta_L}{2\mu}}\|\wloc - w_\star\|_{\HL(w_\star)+\mu I}\sum_{j=1}^{k}\left(1-\frac{1}{\kappa_P}\right)^{j/2}\\
  &\leq \left(1+\sqrt{\frac{\beta_L}{\mu}}\sum_{j=0}^{\infty}\left(1-\frac{1}{\kappa_P}\right)^{j/2}\right)\rho\\
  & = \left(1+\frac{\sqrt{\beta_L/\mu}}{{1-\sqrt{1-\frac{1}{\kappa_P}}}}\right)\rho\leq \epsLoc.
  \end{align*}
  Here, in the second inequality we have used $L(\tilde w_0)\leq 2(1+\varepsilon)\|\wloc - w_\star\|^2_{\HL(w_\star)+\mu I}$, which is an immediate consequence of \cref{lemma:local-sm-pl}.
  Hence, $\tilde w_{k+1}\in \Neps$, and the desired claim follows by induction. 
\end{proof}

\begin{theorem}[Fast-local convergence of Damped Newton]
\label{thm:dn_fast_loc}
    Let $\wloc$ be as in \cref{corr:close_to_min}. 
    Consider the iteration 
    \[
    \tilde w_{k+1} = \tilde w_k-\frac{1}{1+\varepsilon}(\HL(\tilde w_k)+\mu I)^{-1}\nabla L(\tilde w_k),\quad \text{where}~\tilde w_0 = \wloc.\] 
    Then, after $k$ iterations, the loss satisfies
        \[
        L(\tilde w_k) \leq \left(1-\frac{1}{2(1+\varepsilon)^2}\right)^{k}L(\wloc).
        \]
        Thus after $k = \mathcal O\left(\log\left(\frac{1}{\epsilon}\right)\right)$ iterations
        \[
        L(\tilde w_k)\leq \epsilon.
        \]
    \begin{proof}
        \cref{lemma:trapped} ensure that $\tilde w^{k} \in \Neps$ for all $k$.
         Thus, we can apply item $1$ of \Cref{lemma:local-sm-pl} and the definition of $\tilde w^{k+1}$, to reach
         \[
         L(\tilde w_{k+1})\leq L(\tilde w_{k})-\frac{1}{2(1+\varepsilon)}\|\nabla L(\tilde w_k)\|_{P^{-1}}^2. 
         \]
         Now, using item $2$ of \Cref{lemma:local-sm-pl} and recursing yields  
         \[
         L(\tilde w_{k+1})\leq \left(1-\frac{1}{2(1+\varepsilon)^2}\right)L(\tilde w_k)\leq \left(1-\frac{1}{2(1+\varepsilon)^2}\right)^{k+1}L(\wloc).
         \]
         The remaining portion of the theorem now follows via a routine calculation.
    \end{proof}
\end{theorem}

% \subsection{Fast local-convergence of Gauss-Newton with Levenberg-Marquardt regularization}
\subsection{Formal Convergence of \cref{alg-GDND}}
\label{subsec:GDND_conv}
Here, we state and prove the formal convergence result for \cref{alg-GDND}.
\begin{theorem}
\label{thm:GDND}
    Suppose that \cref{assp:interpolation} and \cref{assp:loss_reg} hold, and that the loss is $\mu$-\PL in $B(w_0,2R)$, where $R = \frac{2\sqrt{2\beta_L L(w_0)}}{\mu}$.
    Let $\epsLoc$ and $\rho$ be as in \cref{corr:close_to_min}, and set $\varepsilon = 1/6$ in the definition of $\epsLoc$. 
    Run \cref{alg-GDND} with parameters: $\eta_{\textup{GD}} = 1/\beta_L, K_{\textup{GD}} = \frac{\beta_L}{\mu}\log\left(\frac{4\max\{2\beta_{L},1\}L(w_0)}{\mu\rho^2}\right), \eta_{\textup{DN}} = 5/6, \gamma = \mu$ and $K_{\textup{DN}}\geq 1$.
    Then Phase II of \cref{alg-GDND} satisfies
    \[
    L(\tilde w_{k})\leq \left(\frac{2}{3}\right)^{k}L(w_{K_{\textup{GD}}}).
    \]
    Hence after $K_{\textup{DN}} \geq 3\log\left(\frac{L(w_{K_{\textup{GD}}})}{\epsilon}\right)$ iterations, Phase II of \cref{alg-GDND} outputs a point satisfying
    \[
     L(\tilde w_{K_{\textup{DN}}})\leq \epsilon.
    \]
\end{theorem}

\begin{proof}
    By assumption the conditions of \cref{corr:close_to_min} are met, therefore $w_{K_{\textup{GD}}}$ satisfies $\|w_{K_{\textup{GD}}}-w_\star\|_{H_L(w_\star)+\mu I}\leq \rho$, for some $w_\star \in \Wstar$.
    Hence, we may invoke \cref{thm:dn_fast_loc} to conclude the desired result. 
\end{proof}
