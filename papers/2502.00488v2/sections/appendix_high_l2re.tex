\section{Details on Experiments}
\label{sec:apendix_experiments}
\subsection{Overall Experiments Settings}
\textbf{Examples.} We conduct experiments on function learning case: 1D Allen-Cahn equation, 2D Allen-Cahn equation, high frequency function approximation and operator learning for Burgers' equation. These equations have been studied in previous works investigating difficulties in solving numerically; we use the formulations in \citet{xu2020variational, ZHANG2024112638,hao2019convergence} for our experiments. 

\textbf{Network Structure.} We use multilayer perceptrons (MLPs) with tanh activations and three hidden layers with width 30.
We initialize these networks with the Xavier normal initialization \cite{glorot2010understanding} and all biases equal to zero.


\textbf{Training.} We use Adam to train the neural network and we tune the learning rate by a grid search on $\{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\}$. All iterations continue until the loss stabilizes and no longer decreases significantly. %We use 10000 residual points randomly sampled from a $255 \times 100$ grid on the interior of the problem domain. 
%We use 257 equally spaced points for the initial conditions and 101 equally spaced points for each boundary condition.


\textbf{Device.} We develop our experiments in PyTorch 1.12.1 \cite{paszke2019pytorch} with Python 3.9.12.
Each experiment is run on a single NVIDIA 3070Ti GPU using CUDA 11.8.
%The code for our experiments is available at \href{https://github.com/pratikrathore8/opt_for_pinns}{https://github.com/pratikrathore8/opt\_for\_pinns}.

\subsection{1D Allen-Cahn Equation}

Number of residual points $\nres = 200$ and number of boundary points $\nbc=2$. In this example, we use forward Euler method to numerically solve the homotopy dynamics. And $\varepsilon_{0} = 0.1$ and $\varepsilon_n = 0.01$, here we choose $\Delta \varepsilon_k = 0.001$.

The results for using original training for this example \cref{fig:1d_allen_cahn_origin_result}. As shown in the figure, the original training method results in a large training error, leading to poor accuracy.

\begin{figure}[htp!]
    \centering
    \includegraphics[scale=0.5]{figs/1D_AC_Origin_result.png}
    \caption{Solution for 1D Allen-Cahn equation for origin training.}
    \label{fig:1d_allen_cahn_origin_result}
\end{figure}

\subsection{2D Allen-Cahn Equation}

Number of residual points $\nres = 50\times50$ and number of boundary points $\nbc=198$.In this example, we optimize using the Homotopy Loss. We set $s_{0} = 1.0$ and $s_n=0$, initially choosing $\Delta s= 0.1$, and later refining it to $\Delta t= 0.01$.    
When $s=0.05, \varepsilon(s) = 0.05$ we fix $\varepsilon = 0.05$ and gradually decrease $s$ to $0$.

The reference ground truth solution is obtained using the finite difference method with $N = 1000 \times 1000$  grid points. The result is shown below.
\begin{figure}[htp!]
    \centering
    \includegraphics[scale=0.5]{figs/2D_AD_Reference.png}
    \caption{Reference Solution for 2D Allen-Cahn equation.}
    \label{fig:2d_allen_cahn_reference}
\end{figure}




The result obtained using PINN is shown in the figure below. It is evident that the solution still deviates significantly from the ground truth solution.
\begin{figure}[htp!]
    \centering
    \includegraphics[scale=0.5]{figs/2D_AD_PINN.png}
    \caption{Solution for 2D Allen-Cahn equation for origin training.}
    \label{fig:2d_allen_cahn_origin_result}
\end{figure}
\subsection{High Frequency Function Approximation}

Number of residual points $\nres = 300$. In this example, we optimize using the Homotopy Loss. We set $\varepsilon_{0} = \frac{1}{15}$ and $\varepsilon_n=\frac{1}{50}$, the list for $\{\varepsilon_i\}$ is $[\frac{1}{15},\frac{1}{20},\frac{1}{25},\frac{1}{30},\frac{1}{35},\frac{1}{40},\frac{1}{45},\frac{1}{50}]$. From this example, we observe that the homotopy dynamics approach can also mitigate the slow training issue caused by the Frequency Principle (F-Principle) when neural networks approximate high-frequency functions.

\subsection{Operator Learning 1D Burgers' Equation}
\label{Ap:operator}
In this example, we apply homotopy dynamics to operator learning. The neural network architecture follows the DeepONet structure: \begin{equation}
\mathcal{G}_{\vtheta}(v)(y) = \sum_{k=1}^p \sum_{i=1}^n a_i^k \sigma\left(\sum_{j=1}^m \xi_{i j}^k v\left(x_j\right)+c_i^k\right) \sigma\left(w_k \cdot y+b_k\right).
\end{equation}

Here, $\sigma\left(w_k \cdot y+b_k\right)$ represents the trunk net, which takes the coordinates $y \in D^{\prime}$ as input, and $\sigma\left(\sum_{j=1}^m \xi_{i j}^k u\left(x_j\right)+c_i^k\right)$ represents the branch net, which takes the discretized function $v$ as input. We can interpret the trunk net as the basis functions for solving PDEs. For this example, the input is $u_0$ and the output is $u_{\infty}$. We still train using the homotopy loss. It is important to emphasize that, unlike conventional operator learning, which typically follows a supervised learning strategy, our approach adopts an unsupervised learning paradigm. This makes the training process significantly more challenging. The initial condition $u_0(x)$ is generated from a Gaussian random field with a Riesz kernel, denoted by $\text{GRF} \sim 
\mathcal{N}\left(0,49^2(-\Delta+49I)^{-4}\right)$ and $\Delta$ and $I$ represent the Laplacian and the identity. We utilize a spatial resolution of $128$ grids to represent both the input and output functions.

We want to find the steady state solution for this equation and $\epsilon = 0.05$. The homotopy is:
\begin{equation}
    H(u,s,\varepsilon) = (1-s)\left(\left(\frac{u^2}{2}\right)_x - \varepsilon(s) u_{xx} -\pi \sin (\pi x) \cos (\pi x)\right) + s(u-u_0),
\end{equation}
where $s \in [0,1]$. In particular, when $s = 1$, the initial condition $u_0$ automatically satisfies and when $s = 0$ becomes the steady state problem. And $\varepsilon(s)$ can be set to




\begin{equation}
\varepsilon(s) = 
\left\{\begin{array}{l}
s, \quad s \in [0.05,1],\\
0.05 \quad s\in [0,0.05].
\end{array}\right.\label{eq:epsilon_t}
\end{equation}

Here, $\varepsilon(s)$ varies with $s$ during the first half of the evolution. Once $\varepsilon(s)$ reaches $0.05$, it is fixed at $\varepsilon(s) = 0.05$, and only $s$ continues to evolve toward $0$.






