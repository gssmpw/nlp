\section{Theory}
\label{sec:theory}
In this section, we provide theoretical support for homotopy dynamics. In the first part, we demonstrate that for certain PDEs with small parameters, direct training using PINN methods is highly challenging. This analysis is based on the neural tangent kernel (NTK) framework \cite{allen2019convergence}. In the second part, we show that homotopy dynamics will converge to the solution with a small parameter \( \varepsilon \), provided that the dynamic step size is sufficiently small and the initial solution has been well learned by the neural network.
\subsection{Challenges in Training Neural Network with Small Certain Parameters}
Let us consider training neural networks without homotopy dynamics. The corresponding loss function can be expressed as  
\begin{align}
    L_H(\vtheta) = \frac{1}{2n} \sum_{i=1}^n H^2(u_{\vtheta}(\vx_i),\varepsilon),
\label{eq:hom_or_loss}
\end{align}  
where $\{\vx_i\}_{i=1}^n$ represents the training data used to optimize the neural network. Here, we assume that the parameter $\varepsilon$ in the PDE appears only in the interior terms and not in the boundary conditions. Therefore, in this section, we omit the effect of boundary conditions, as the behavior at the boundary remains unchanged for any given $\varepsilon$.  

Furthermore, to simplify the notation, we use $n$ instead of $\nres$ and denote $\vx_r^i$ simply as $\vx_i$ comparing with Eq. \eqref{loss}.

In the classical approach, such a loss function is optimized using gradient descent, stochastic gradient descent, or Adam. Considering the training process of gradient descent in its continuous form, it can be expressed as:
\begin{align}
    \frac{\D \vtheta}{\D t} 
    &= -\nabla_{\vtheta}L(\vtheta) 
    \notag\\&= -\frac{1}{n} \sum_{i=1}^n H(u_{\vtheta}(\vx_i),\varepsilon)\delta_{\phi}H(u_{\vtheta}(\vx_i),\varepsilon)\nabla_{\vtheta}u_{\vtheta}(\vx_i), \notag \\
    &= -\frac{1}{n} \vH(u_{\vtheta}(\vx),\varepsilon) \cdot \vS,
\end{align}
where $t$ in this section is the time of the gradient decent process instead of the time in PDEs, and
\begin{align}
    \vH(u_{\vtheta}(\vx),\varepsilon) 
    &:= \big[ H(u_{\vtheta}(\vx_i),\varepsilon)\delta_{\phi}H(u_{\vtheta}(\vx_i),\varepsilon)\big]_{i=1}^n \notag \\
    &= \vl \cdot \vD_\varepsilon,
\end{align}
and
\begin{align}
    \vl := \big[ H(\phi(\vx_i,\vtheta),\varepsilon) \big]_{i=1}^n \in \sR^{1 \times n},  \vD_\varepsilon \in \sR^{n \times n}
\end{align}where $\vD_\varepsilon$ represents the discrete form of the variation of PDEs in different scenarios. Furthermore,
\begin{align}
    \vS = \big[ \nabla_{\vtheta}u_{\vtheta}(\vx_1),\dots, \nabla_{\vtheta}u_{\vtheta}(\vx_n) \big].
\end{align}

Therefore, we obtain  
\begin{align}
    \frac{\D L(\vtheta)}{\D t} &= \nabla_{\vtheta}L(\vtheta) \frac{\D \vtheta}{\D t} \notag \\
    &= -\frac{1}{n^2} \vH(u_{\vtheta}(\vx),\varepsilon) \vS \vS^{\top} \vH^{\top}(u_{\vtheta}(\vx),\varepsilon) \notag \\
    &= -\frac{1}{n^2} \vl \vD_\varepsilon \vS \vS^{\top} \vD_\varepsilon^{\top} \vl^{\top}.
\end{align}
Hence, the kernel of the gradient descent update is given by  
\begin{align}
    \vK_\varepsilon := \vD_\varepsilon \vS \vS^{\top} \vD_\varepsilon^{\top}.
\end{align}

The following theorem provides an upper bound for the smallest eigenvalue of the kernel and its role in the gradient descent dynamics:
\begin{theorem}[Effectiveness of Training via the Eigenvalue of the Kernel]\label{compare}
   Suppose \( \lambda_{\text{min}}(\vS\vS^{\top}) > 0 \) and \( \vD_\varepsilon \) is non-singular, and let \( \varepsilon \geq 0 \) be a constant. Then, we have \( \lambda_{\text{min}}(\vK_{\varepsilon}) > 0 \), and there exists \( T > 0 \) such that  
\begin{equation}
    L(\vtheta(t)) \leq L(\vtheta(0))\exp\left(-\frac{\lambda_{\text{min}}(\vK_{\varepsilon})}{n} t\right)\label{speed}
\end{equation}
for all \( t \in [0, T] \). Furthermore,  
\begin{equation}
    \lambda_{\text{min}}(\vK_{\varepsilon}) \leq \lambda_{\text{min}}(\vS\vS^{\top}) \lambda_{\text{max}}(\vD_{\varepsilon}\vD_{\varepsilon}^\top).
    \label{mineigen}
\end{equation}
\end{theorem}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.48]{figs/AD_operator_eigenvalue.png}
    \caption{Largest eigenvalue of \(\vD_\varepsilon\) \eqref{eq:discete_operator} for different $\varepsilon$. A smaller $\varepsilon$ results in a smaller largest eigenvalue of \eqref{eq:discete_operator}, leading to a slower convergence rate and increased difficulty in training.}
    \label{fig:1d_allen_cahn_eigen_value}
\end{figure}
\begin{figure*}[htbp!]
    \centering
    \includegraphics[scale=0.40]{figs/2D_AC_results.pdf}
    \caption{2D Allen Cahn Equaiton. (Top) Evolution of the Homotopy Dynamcis. (Bottom) Plot for Cross-section of $u(x,y)$ at $y = 0.5$ i.e., $u(x,y=0.5)$. The reference solution $u_{\infty}(x)$ represents the ground truth steady-state solution. The L2RE is $8.78e-3$. Number of residual points is $\nres = 50\times50$. }
\label{fig:2D_Allen_Cahn_Equation}
\end{figure*}


\begin{remark}\label{hard}
    For \( \vS\vS^{\top} \), previous works such as \cite{luo2020two, allen2019convergence, arora2019exact, cao2020generalization, yang2025homotopy} demonstrate that it becomes positive when the width of the neural network is sufficiently large with ReLU activation functions. Additionally, \cite{gao2023gradient} discusses the positivity of the gradient kernel in PINNs for solving heat equations. Therefore, we can reasonably assume that \( \vS\vS^{\top} \) is a strictly positive matrix. In Appendix~\ref{ss}, we present a specific scenario where \( \lambda_{\text{min}}(\vS\vS^{\top}) > 0 \) holds with high probability.


   This theorem demonstrates that the smallest eigenvalue of the kernel directly affects the training speed. Equation \eqref{mineigen} shows that the upper bound of \( \lambda_{\text{min}}(\vK_{\varepsilon}) \) can be influenced by \( \lambda_{\text{max}}(\vD_{\varepsilon}\vD_{\varepsilon}^\top) \). In many PDE settings, the maximum eigenvalue \( \lambda_{\text{max}}(\vD_{\varepsilon}\vD_{\varepsilon}^\top) \) tends to be small when \( \varepsilon \) is small. For example, in this paper, we consider the Allen–Cahn equation, given by
\[
-\varepsilon^2\Delta u + f(u) = 0,
\]
where \( f(u) = u^3 - u \). In this case, \( \vD_\varepsilon \) corresponds to the discrete form of the operator \( -\varepsilon^2\Delta + f'(u) \), which can be written as
\begin{equation}
   \vD_\varepsilon= -\varepsilon^2\Delta_{\text{dis}} + \text{diag} \big(f'(u(\vx_1)), \dots, f'(u(\vx_n)) \big).
    \label{eq:discete_operator}
\end{equation}
According to \cite{morton2005numerical}, the discrete Laplacian \( -\varepsilon^2\Delta_{\text{dis}} \) is strictly positive. Specifically, in the one-dimensional case, its largest eigenvalue is given by
\[
4\varepsilon^2 n^2 \cos^2 \frac{ \pi}{2n+1},
\]
which is close \( 4\varepsilon^2n^2 \) as \( n  \) is large enough. 

Moreover, since \( f'(u(\boldsymbol{x}_i)) \) ranges between \(-1\) and \(2\), when \( \varepsilon \) is large (close to 1), the largest eigenvalue of  
\( \vD_\varepsilon \)
becomes very large, regardless of the sampling locations \( \{\boldsymbol{x}_i\}_{i=1}^n \), as shown in \cref{fig:1d_allen_cahn_eigen_value} for the case \( n=200 \). Therefore, according to Theorem~\ref{compare}, the upper bound of the smallest eigenvalue of \( \vK_\varepsilon \) will also be large, specifically of order \( n^4 \) with respect to \( n \) in this case due to Weyl’s inequalities. Consequently, the training speed can reach \( \exp(-Cn^3t) \) based on Eq. \eqref{speed}, which is fast and implies that training is easy.

However, when \( \varepsilon \) is small (close to 0), the largest eigenvalue of  
\( \vD_\varepsilon \)
is only of order \( 1 \) with respect to \( n \), which implies that the upper bound of the smallest eigenvalue of \( \vK_\varepsilon \) will no longer be of order \( 1 \) with respect to \( n \). Therefore, the training speed can reach \( \exp(-Ct/n) \) based on Eq. \eqref{speed}, which is slow and indicates that training is difficult in this case.
\end{remark}


\subsection{Convergence of Homotopy Dynamics}
In this section, we aim to demonstrate that homotopy dynamics is a reasonable approach for obtaining the solution when \( \varepsilon \) is small. For simplicity of notation, we denote \( u(\varepsilon) \) as the exact solution of \( H(u,\varepsilon) = 0 \) and \( U(\varepsilon) \) as its numerical approximation in the simulation. Suppose \( H(u(\varepsilon),\varepsilon) = 0 \), and assume that \( \frac{\partial H(u(\varepsilon),\varepsilon)}{\partial u} \) is invertible. Then, the dynamical system (\ref{eq:homotopy_dynamics}) can be rewritten as  
\begin{equation}
\frac{\D u}{\D \varepsilon} = -\left(\frac{\partial H(u(\varepsilon),\varepsilon)}{\partial u}\right)^{-1} \frac{\partial H(u(\varepsilon),\varepsilon)}{\partial \varepsilon} =: h(u(\varepsilon),\varepsilon).\label{dym}
\end{equation}
Applying Euler’s method to this dynamic system, we obtain  
\begin{align}
    U(\varepsilon_{k+1}) = U(\varepsilon_k) + (\varepsilon_{k+1} - \varepsilon_k) h( U(\varepsilon_k),\varepsilon_k).
\end{align} 

The following theorem shows that if \( u(\varepsilon_0)-U(\varepsilon_0) \) is small and the step size \( (\varepsilon_{k+1} - \varepsilon_k) \) is sufficiently small at each step, then \( u(\varepsilon_k)- U(\varepsilon_k) \) remains small.

\begin{theorem}[Convergence of Homotopy Dynamics]\label{small}
    Suppose \( h(\varepsilon,u) \) is a continuous operator for \( 0 \leq \varepsilon_n \leq \varepsilon_0 \) and \( u \in H^2(\Omega) \), and 
    \[
    \|h(u_1,\varepsilon)-h(u_2,\varepsilon)\|_{H^2(\Omega)}\le K_\varepsilon\|u_1-u_2\|_{H^2(\Omega)}.
    \]
    Assume there exists a constant \( K \) such that 
    \[
    (\varepsilon_k-\varepsilon_{k+1})K_{\varepsilon_k}\le K\cdot \frac{\varepsilon_0-\varepsilon_n}{N}
    \]
    and 
    \begin{align}
    \tau&:=\frac{n}{\varepsilon_0-\varepsilon_n} \sup_{0\leq k\leq n} (\varepsilon_k-\varepsilon_{k+1})^2\|u(\varepsilon_k)\|_{H^4(\Omega)}\ll 1,\notag\\e_0&:=\|u(\varepsilon_0)-U(\varepsilon_0)\|_{H^2(\Omega)}  \ll 1 \notag
    \end{align}
    then we have 
    \begin{align}
        &\|u(\varepsilon_n)- U(\varepsilon_n)\|_{H^2(\Omega)}\notag\\\le& e_0e^{K(\varepsilon_0-\varepsilon_n)}+\frac{\tau(e^{K(\varepsilon_0-\varepsilon_n)}-1)}{2K} \ll 1.
    \end{align}
\end{theorem}

\begin{figure*}[htbp!]
    \centering
    \includegraphics[scale=0.40]{figs/High_frequncy_aproximation.pdf}
    \caption{High-frequency function $\sin(50\pi x)$ approximation: Comparison of loss curves between original evolution and homotopy evolution. The comparison shows that homotopy evolution effectively reduces the loss, successfully approximating the high-frequency function, while the original evolution fails. The number of residual points is $\nres = 300$. }
\label{fig:high_frequency_result}
\end{figure*}


   The proof of Theorem~\ref{small} is inspired by \cite{antonakopoulos2022adagrad}.  

Theorem~\ref{small} shows that if \( e_0 \) is small and the step size \( (\varepsilon_{k+1} - \varepsilon_k) \) is sufficiently small at each step and satisfies  
\[
(\varepsilon_k - \varepsilon_{k+1}) K_{\varepsilon_k} \leq K \cdot \frac{\varepsilon_0 - \varepsilon_n}{n},
\]
i.e., the training step size should depend on the Lipschitz constant of \( h(u,\varepsilon) \), ensuring stable training, then \( u(\varepsilon_k) - U(\varepsilon_k) \) remains small. The initial error \( e_0 \) can be very small since we use a neural network to approximate the solution of PDEs for large \( \varepsilon \), where learning is effective.  

The error \( e_0 \) consists of approximation, generalization, and training errors. The approximation error reflects the gap between the exact PDE solution and the neural network's hypothesis space, the generalization error arises from the challenges of learning with finite samples, and the training error results from optimizing the neural network's loss function. The training error can be well controlled by Theorem~\ref{compare} when \( \varepsilon \) is large, while the approximation and generalization errors can be small if the sample size is sufficiently large and the neural network is expressive enough.  

The theoretical support for this result can be found in \cite{yang2023nearly,yang2024deeper}, which we discuss further in Appendix~\ref{e0}.




% \begin{figure*}[t]
%     \centering
%     \includegraphics[scale=0.33]{figs/scatter_multi_pde.pdf}
%     \caption{We plot the final L2RE against the final loss for each combination of network width, optimization strategy, and random seed.
%     Across all three PDEs, a lower loss generally corresponds to a lower L2RE.}
%     \label{fig:l2re_vs_loss}
% \end{figure*}

% First, we show that PINNs must be trained to near-zero loss to obtain a reasonably low L2RE.
% This phenomenon can be observed in \cref{fig:l2re_vs_loss}, demonstrating that a lower loss generally corresponds to a lower L2RE.
% For example, on the convection PDE, a loss of $10^{-3}$ yields an L2RE around $10^{-1}$, but decreasing the loss by a factor of $100$ to $10^{-5}$ yields an L2RE around $10^{-2}$, a 10$\times$ improvement.
% This relationship between loss and L2RE in \cref{fig:l2re_vs_loss} 
% is typical of many PDEs \cite{lu2022multifidelity}. 

% The relationship in \cref{fig:l2re_vs_loss} underscores that high-accuracy optimization is required for a useful PINN.
% There are instances (especially on the reaction ODE), where the PINN solution has a L2RE around 1, despite a near-zero loss; we provide insight into why this is occurring in \cref{sec:low_loss_high_l2re}. 
% In \cref{sec:loss_landscape,sec:under_optimized}, we show that ill-conditioning and under-optimization make reaching a solution with sufficient accuracy difficult.

% \pnote{Cite Mishra paper about training error and quadrature error}

% \begin{itemize}
    % \item Show empirically that near-zero loss is need for a good solution. This can be achieved by plotting l2re against loss for a few runs
    % \item Compare and contrast to vision tasks in deep learning. We do not need zero loss to get decent test accuracy
% \end{itemize}