\section{Experiments}
\label{sec:opt_comparison}






We perform numerical experiments in the contexts of function approximation, physics-informed neural networks (PINNs), and operator learning, applying the proposed Homotopy Evolution Training Strategy and presenting the associated results.


We conduct experiments on optimizing PINNs for convection, wave PDEs, and a reaction ODE. 
These equations have been studied in previous works investigating difficulties in training PINNs; we use the formulations in \citet{krishnapriyan2021characterizing, wang2022when} for our experiments. 
The coefficient settings we use for these equations are considered challenging in the literature \cite{krishnapriyan2021characterizing, wang2022when}.
\cref{sec:problem_setup_additional} contains additional details.

We compare the performance of Adam, \lbfgs{}, and \al{} on training PINNs for all three classes of PDEs. 
For Adam, we tune the learning rate by a grid search on $\{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$.
For \lbfgs, we use the default learning rate $1.0$, memory size $100$, and strong Wolfe line search.
For \al, we tune the learning rate for Adam as before, and also vary the switch from Adam to \lbfgs{} (after 1000, 11000, 31000 iterations).
These correspond to \al{} (1k), \al{} (11k), and \al{} (31k) in our figures.
All three methods are run for a total of 41000 iterations.

We use multilayer perceptrons (MLPs) with tanh activations and three hidden layers. These MLPs have widths 50, 100, 200, or 400.
We initialize these networks with the Xavier normal initialization \cite{glorot2010understanding} and all biases equal to zero.
Each combination of PDE, optimizer, and MLP architecture is run with 5 random seeds.

We use 10000 residual points randomly sampled from a $255 \times 100$ grid on the interior of the problem domain. 
We use 257 equally spaced points for the initial conditions and 101 equally spaced points for each boundary condition.

We assess the discrepancy between the PINN solution and the ground truth using $\ell_2$ relative error (L2RE), a standard metric in the PINN literature. Let $y = (y_i)_{i = 1}^n$ be the PINN prediction and $y' = (y'_i)_{i = 1}^n$ the ground truth. Define
\begin{align*}
    \mathrm{L2RE} = \sqrt{\frac{\sum_{i = 1}^n (y_i - y'_i)^2}{\sum_{i = 1}^n y'^2_i}} = \sqrt{\frac{\|y - y'\|_2^2}{\|y'\|_2^2}}.
\end{align*}
We compute the L2RE using all points in the $255 \times 100$ grid on the interior of the problem domain, along with the 257 and 101 points used for the initial and boundary conditions.

We develop our experiments in PyTorch 2.0.0 \cite{paszke2019pytorch} with Python 3.10.12.
Each experiment is run on a single NVIDIA Titan V GPU using CUDA 11.8.
The code for our experiments is available at \href{https://github.com/pratikrathore8/opt_for_pinns}{https://github.com/pratikrathore8/opt\_for\_pinns}.


\subsection{Function Approximation}
\cref{fig:opt_comparison} in \cref{sec:opt_comparison_additional} compares \al, Adam, and \lbfgs{} on the convection, reaction, and wave problems at difficult coefficient settings noted in the literature \cite{krishnapriyan2021characterizing, wang2022when}.
Across each network width, the lowest loss and L2RE is always delivered by \al.
Similarly, the lowest median loss and L2RE are almost always delivered by \al{} (\cref{fig:opt_comparison}).
The only exception is the reaction problem, where Adam outperforms \al{} on loss at width = 100 and L2RE at width = 200 (\cref{fig:opt_comparison}).

%The best performance of each optimizer across all hyperparameters and architectures is shown in \cref{tab:loss_l2re_comparison}.
\cref{tab:loss_l2re_comparison} summarizes the best performance of each optimizer.
Again, \al{} is better than running either Adam or L-BFGS alone.
Notably, \al{} attains 14.2$\times$ smaller L2RE than Adam on the convection problem and 6.07$\times$ smaller L2RE than \lbfgs{} on the wave problem.

\begin{table}[t]
    \caption{Lowest loss for Adam, \lbfgs, and \al{} across all network widths after hyperparameter tuning. 
    \al{} attains both smaller loss and L2RE vs. Adam or \lbfgs. 
    }
    \vskip 0.15in
    \centering
    \tiny
    \begin{tabular}{|c|c|c|c|c|c|c|c|} 
    \hline 
    \multirow{2}{*}{Optimizer} & \multicolumn{2}{c|}{Convection} & \multicolumn{2}{c|}{Reaction} & \multicolumn{2}{c|}{Wave} \\ \cline{2-7}
                               & Loss & L2RE & Loss & L2RE & Loss & L2RE \\ \hline 
    Adam                        & 1.40e-4     & 5.96e-2     & 4.73e-6     & 2.12e-2     & 2.03e-2     & 3.49e-1     \\ \hline 
    L-BFGS                      & 1.51e-5     & 8.26e-3     & 8.93e-6     & 3.83e-2     & 1.84e-2     & 3.35e-1     \\ \hline 
    \al                         & \textbf{5.95e-6}     & \textbf{4.19e-3}     & \textbf{3.26e-6}     & \textbf{1.92e-2}     & \textbf{1.12e-3}     & \textbf{5.52e-2}      \\ \hline
    \end{tabular}
    % On convection, \al{} provides 14.2$\times$ and 1.97$\times$ improvement over Adam or \lbfgs{} on L2RE. 
    % On reaction, \al{} provides 1.10$\times$ and 1.99$\times$ improvement over Adam or \lbfgs{} on L2RE.
    % On wave, \al{} provides 6.32$\times$ and 6.07$\times$ improvement over Adam or \lbfgs{} on L2RE.}
    \label{tab:loss_l2re_comparison}
\end{table}


% \textbf{Convection.} Across each network width, the lowest loss and L2RE is always attained by one of the three \al{} strategies. 
% The lowest L2REs attained by Adam, \lbfgs, and \al{} are 5.96e-2, 8.26e-3, 4.19e-3, respectively, i.e., \al{} provides 14.2 and 1.97 times improvement over Adam and \lbfgs{} on L2RE.  
% Similarly, the lowest median loss and L2RE is always attained by one of the three \al{} strategies.

% \textbf{Reaction.} Similar to convection, the lowest loss and L2RE is always attained by one of the three \al{} strategies.
% The lowest L2RE attained by Adam, \lbfgs, and \al{} are 2.12e-2, 3.83e-2, 1.92e-2, respectively, i.e., \al{} provides 1.10 and 1.99 times improvements over Adam and \lbfgs{} on L2RE.
% The lowest median loss and L2RE is attained by one of the three \al{} strategies, except for the width = 100 setting for loss and width = 200 setting for L2RE.

% \textbf{Wave.} Similar to both convection and reaction, the lowest loss and L2RE is always attained by one of the three \al{} strategies.
% The lowest L2REs attained by Adam, \lbfgs, and \al{} are 3.49e-1, 3.35e-1, 5.52e-2, respectively, i.e., \al{} provides 6.32 and 6.07 times improvement over Adam and \lbfgs{} on L2RE.  
% Again, the lowest median loss and L2RE is always attained by one of the three \al{} strategies.

% \pnote{Express this stuff in a table}

% \begin{itemize}
    % \item Show that \al{} is better in practice. Demonstrate that this holds up across a wide range of network widths. The loss is improved over other methods -- but this doesn't always mean better L2RE. Leave this is a question for future work.
    % \item Tell reader that this suggests any results using just \lbfgs{} should be viewed with skepticism
% \end{itemize}

\subsection{Intuition From Optimization Theory}
The success of \al{} over Adam and \lbfgs{} can be explained by existing results in optimization theory.
In neural networks, saddle points typically outnumber local minima \cite{dauphin2014identifying,lee2019firstorder}.
% We must retain convergence speed near saddle points to converge to a global minimizer efficiently.
A saddle point can never be a global minimum. 
We want to reach a global minimum when training PINNs.

Newton's method (which \lbfgs{} attempts to approximate) is attracted to saddle points \cite{dauphin2014identifying},  and quasi-Newton methods such as \lbfgs{} also converge to saddle points since they ignore negative curvature \cite{dauphin2014identifying}.
On the other hand, first-order methods such as gradient descent and AdaGrad \cite{duchi2011adaptive} avoid saddle points \cite{lee2019firstorder,antonakopoulos2022adagrad}.
We expect that (full-gradient) Adam also avoids saddles for similar reasons, although
we are not aware of such a result.

Alas, first-order methods converge slowly when the problem is ill-conditioned. 
This result generalizes the well-known slow convergence of conjugate gradient (CG)
for ill-conditioned linear systems:
 $\mathcal O (\sqrt \kappa \log(\frac{1}{\epsilon}))$ iterations to converge to an $\epsilon$-approximate solution of a system with condition number $\kappa$.
In optimization, an analogous notion of a condition number in a set $\mathcal S$ near a global minimum is given by $\kappa_{f}(\mathcal S) \coloneqq \sup_{w \in \mathcal S} \| H_f(w) \| / \mu$, where $\mu$ is the \PL-constant (see \cref{sec:theory}).
Then gradient descent requires $\mathcal O (\kappa_{f}(\mathcal S) \log(\frac{1}{\epsilon}))$ iterations to converge to an $\epsilon$-suboptimal point.
% Under the $\mu$-\PL condition , gradient descent requires $\mathcal O (\kappa \log(\frac{1}{\epsilon}))$ iterations to converge to an $\epsilon$-suboptimal point, where $\kappa \coloneqq \max /\mu$
% For example, suppose $f(w)$ is a convex function for which $\mu I \preceq \nabla^2 f(w) \preceq L I$, where $L > \mu > 0$ \footnote{In the optimization literature, the upper and lower bounds on the Hessian are called strong convexity and smoothness assumptions.}.
% Then gradient descent requires $\mathcal O (\kappa \log(\frac{1}{\epsilon}))$ iterations to converge to an $\epsilon$-suboptimal point, where $\kappa \coloneqq L/\mu$.
For PINNs, the condition number near a solution is often $> 10^{4}$ (\cref{fig:spectral_density_multi_pde_convection_combined}), which leads to slow convergence of first-order methods. 
% \pnote{Ask Zach if we should talk about AGD or rates for adaptive methods}
However, Newton's method and L-BFGS can significantly reduce the condition number (\cref{fig:spectral_density_multi_pde_convection_combined}), which yields faster convergence. 

% \pnote{Make explanation more careful. We are not actually in a convex setting, but we do make the $\mu$-\PL assumption later}

\al{} combines the best of both first- and second-order/quasi-Newton methods. 
By running Adam first, we avoid saddle points that would attract L-BFGS.
By running L-BFGS after Adam, 
we can reduce the condition number of the problem, which leads to faster local convergence.
\cref{fig:under_optimization_intro} exemplifies this, showing faster convergence of \al{} over Adam on the wave equation.
%We provide an example showing faster convergence of \al{} compared to Adam for the wave equation (the most ill-conditioned problem in \cref{sec:loss_landscape}) in \cref{fig:under_optimization_intro}.

This intuition also explains why Adam sometimes performs as well as \al{} on the reaction problem.
\cref{fig:spectral_density_multi_pde_convection_combined} shows the largest eigenvalue of the reaction problem is around $10^{3}$, 
while the largest eigenvalues of the convection and wave problems are around $10^{4}$ and $10^{5}$, suggesting the reaction problem is less ill-conditioned.
%Therefore, we would expect a first-order method (i.e., Adam) to be more competitive with \al{}.

% \begin{itemize}
    % \item Explain why we don't expect Adam or \lbfgs{} alone to work well. GD has condition number dependence and AGD has square root condition number dependence. The theoretical rate of convergence for Adam is $\mathcal{O}(\log T / \sqrt{T})$ and would require decaying stepsizes to get to minimum. \lbfgs{} on its own could easily converge to a saddle point. Adam is probably helping avoid saddle points; following this by \lbfgs{} leads to a much better solution
    % \item Try L-BFGS in float32 and float64. Does float64 give better results?
% \end{itemize}
