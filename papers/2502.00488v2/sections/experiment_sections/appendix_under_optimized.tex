\section{Additional details on Under-optimization}
\label{sec:under_optimization_additional}

\subsection{Early Termination of \lbfgs{}}
\cref{fig:line_search_multi_pde} explains why \lbfgs{} terminates early for the convection, reaction, and wave problems.
We evaluate the loss at $10^{4}$ uniformly spaced points in the interval $[0, 1]$.
The orange stars in \cref{fig:line_search_multi_pde} are step sizes that satisfy the strong Wolfe conditions and the red dots are step sizes that \lbfgs{} examines during the line search.

\begin{figure}
    \centering
    \includegraphics[scale=0.45]{figs/line_search_multi_pde.pdf}
    \caption{Loss evaluated along the \lbfgs{} search direction at different stepsizes after 41000 iterations of \al{}. For convection and wave, the line search does not find a stepsize that satisfies the strong Wolfe conditions, even though there are plenty of such points. For reaction, the slope of the objective used in the line search procedure at the current iterate is less than a pre-defined threshold $10^{-9}$, so \lbfgs{} terminates without performing any line-search.}
    \label{fig:line_search_multi_pde}
\end{figure}

\subsection{NysNewton-CG (NNCG)}
Here we present the NNCG algorithm (\cref{alg-NNCG}) introduced in \cref{subsec:NNCG} and its associated subroutines RandomizedNystr{\"o}mApproximation (\cref{alg-RNA}), Nystr\"{o}mPCG (\cref{alg-nyspcg}), and Armijo (\cref{alg-armijo}).
At each iteration, NNCG first checks whether the Nystr\"{o}m preconditioner (stored in $U$ and  $\hat{\Lambda}$) for the Nystr\"{o}mPCG method needs to be updated.
If so, the preconditioner is recomputed using the RandomizedNystr{\"o}mApproximation subroutine.
From here, the Newton step $d_k$ is computed using Nystr\"{o}mPCG; we warm start the PCG algorithm using the Newton step $d_{k - 1}$ from the previous iteration.
After computing the Newton step, we compute the step size $\eta_k$ using Armijo line search --- this guarantees that the loss will decrease when we update the parameters.
Finally, we update the parameters using $\eta_k$ and $d_k$.

In our experiments, we set $\eta = 1, K = 2000, s = 60, F = 20, \epsilon = 10^{-16}, M = 1000, \alpha = 0.1$, and $\beta = 0.5$.
We tune $\mu \in [10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}]$; we find that $\mu = 10^{-2}, 10^{-1}$ work best in practice.
\cref{fig:under_optimization_intro,fig:under_optimization} show the NNCG run that attains the lowest loss after tuning $\mu$.

\begin{algorithm}[H]
	\centering
	\caption{NysNewton-CG (NNCG)}
	\label{alg-NNCG}
	\begin{algorithmic}
	\INPUT{Initialization $w_0$, max. learning rate $\eta$, number of iterations $K$, preconditioner sketch size $s$, preconditioner update frequency $F$, damping parameter $\mu$, CG tolerance $\epsilon$, CG max. iterations $M$, backtracking parameters $\alpha, \beta$}
    \STATE{$d_{-1} = 0$}
    \FOR{$k = 0, \dots, K - 1$}
        \IF{$k$ is a multiple of $F$} 
            \STATE{$[U, \hat{\Lambda}] = \textrm{RandomizedNystr{\"o}mApproximation}(H_{L}(w_k), s)$} \COMMENT{Update Nystr{\"o}m preconditioner every $F$ iterations}
        \ENDIF
        \STATE{$d_k = \textrm{Nystr{\"o}mPCG}(H_{L}(w_k), \nabla L(w_k), d_{k - 1}, U, \hat{\Lambda}, s, \mu, \epsilon, M)$} \COMMENT{Damped Newton step $(H_L(w_k) + \mu I)^{-1} \nabla L(w_k)$}
        \STATE{$\eta_k = \textrm{Armijo}(L, w_k, \nabla L(w_k), -d_k, \eta)$} \COMMENT{Compute step size via line search}
        \STATE{$w_{k+1} = w_k - \eta_k d_k$} \COMMENT{Update parameters}
    \ENDFOR
	\end{algorithmic}
\end{algorithm}

The RandomizedNystr{\"o}mApproximation subroutine (\cref{alg-RNA}) is used in NNCG to compute the preconditioner for Nystr\"{o}mPCG.
The algorithm returns the top-$s$ approximate eigenvectors and eigenvalues of the input matrix $M$.
Within NNCG, the sketch computation $Y = MQ$ is implemented using Hessian-vector products.
The portion in red is a fail-safe that allows for the preconditioner to be computed when $H$ is an indefinite matrix.
For further details, please see \citet{frangella2023randomized}.


\begin{algorithm}[H] 
   \caption{RandomizedNystr{\"o}mApproximation}
   \label{alg-RNA}
    \begin{algorithmic}
       % \INPUT{sketch $Y\in \R^{p\times r_j}$ of $H_{S_j}$, orthogonalized test matrix $Q\in \R^{p\times r_j}$}
       \INPUT{Symmetric matrix $M$, sketch size $s$}
       \STATE{$S = \textrm{randn}(p, s)$} \COMMENT{Generate test matrix}
       \STATE{$Q = \textrm{qr\_econ}(S)$}
       \STATE{$Y = M Q$} \COMMENT{Compute sketch}
       \STATE $\nu = \sqrt{p} \text{eps}(\text{norm}(Y, 2))$ \hfill \COMMENT{Compute shift}
       \STATE $Y_{\nu} = Y + \nu Q$ \hfill \COMMENT{Add shift for stability}
       \STATE $\lambda = 0$ \hfill \COMMENT{Additional shift may be required for positive definiteness}
       \STATE $C = \text{chol}(Q^TY_\nu)$ \hfill \COMMENT{Cholesky decomposition: $C^{T}C = Q^{T}Y_\nu$}
       \textcolor{red}{
       \IF {chol fails}
        \STATE Compute $[W, \Gamma] = \mathrm{eig}(Q^T Y_\nu)$ \hfill \COMMENT{$Q^T Y_\nu$ is small and square}
        \STATE Set $\lambda = \lambda_{\min}(Q^T Y_\nu)$
        \STATE $R = W(\Gamma + |\lambda| I)^{-1/2} W^T$
        \STATE $B = YR$ \hfill \COMMENT{$R$ is psd}
       \ELSE
        \STATE $B = YC^{-1}$ \hfill \COMMENT{Triangular solve}
       \ENDIF
       }
       \STATE $[\hat V, \Sigma, \sim] = \text{svd}(B, 0)$ \hfill \COMMENT{Thin SVD}
       \STATE $\hat \Lambda = \text{max}\{0, \Sigma^2 - (\nu + |\lambda| I)\}$ \hfill \COMMENT{Compute eigs, and remove shift with element-wise max}
       \STATE {\bfseries Return:} $\hat V, \hat \Lambda$
    \end{algorithmic}
\end{algorithm} 

The Nystr\"{o}mPCG subroutine (\cref{alg-nyspcg}) is used in NNCG to compute the damped Newton step.
The preconditioner $P$ and its inverse $P^{-1}$ are given by 
\begin{align*}
    P &= \frac{1}{\hat{\lambda}_s + \mu} U (\hat{\Lambda} + \mu I) U^T + (I - UU^T) \\
    P^{-1} &= (\hat{\lambda}_s + \mu) U (\hat{\Lambda} + \mu I)^{-1} U^T + (I - UU^T).
\end{align*}
Within NNCG, the matrix-vector product involving the Hessian (i.e., $A = H_L(w_k)$) is implemented using Hessian-vector products.
For further details, please see \citet{frangella2023randomized}.

\begin{algorithm}[H] 
   \caption{Nystr\"{o}mPCG}
   \label{alg-nyspcg}
    \begin{algorithmic}
       \INPUT{Psd matrix $A$, right-hand side $b$, initial guess $x_0$, approx. eigenvectors $U$, approx. eigenvalues $\hat{\Lambda}$, sketch size $s$, damping parameter $\mu$, CG tolerance $\epsilon$, CG max. iterations $M$}
       \STATE{$r_0 = b - (A + \mu I) x_0$}
       \STATE{$z_0 = P^{-1} r_0$}
       \STATE{$p_0 = z_0$}
       \STATE{$k = 0$} \COMMENT{Iteration counter}
       \WHILE{$\|r_0\|_2 \geq \eps$ and $k < M$}
           \STATE{$v = (A + \mu I) p_0$}
           \STATE{$\alpha = (r_0^T z_0) / (p_0^T v_0)$} \COMMENT{Compute step size}
           \STATE{$x = x_0 + \alpha p_0$} \COMMENT{Update solution}
           \STATE{$r = r_0 - \alpha v$} \COMMENT{Update residual}
           \STATE{$z = P^{-1}r$}
           \STATE{$\beta = (r^T z) / (r_0^T z_0)$}
           \STATE{$x_0 \gets x, r_0 \gets r, p_0 \gets z + \beta p_0, z_0 \gets z, k \gets k + 1$}
       \ENDWHILE
       \STATE {\bfseries Return:} $x$
    \end{algorithmic}
\end{algorithm} 

The Armijo subroutine (\cref{alg-armijo}) is used in NNCG to guarantee that the loss decreases at every iteration.
The function oracle is implemented in PyTorch using a \textit{closure}.
At each iteration, the subroutine checks whether the \textit{sufficient decrease condition} has been met; if not, it shrinks the step size by a factor of $\beta$.
For further details, please see \citet{nocedal2006numerical}.

\begin{algorithm}[H] 
   \caption{Armijo}
   \label{alg-armijo}
    \begin{algorithmic}
       \INPUT{Function oracle $f$, current iterate $x$, current gradient $\nabla f(x)$, search direction $d$, initial step size $t$, backtracking parameters $\alpha, \beta$}
       \WHILE{$f(x + td) > f(x) + \alpha t (\nabla f(x)^T d)$} 
        \STATE{$t \gets \beta t$} \COMMENT{Shrink step size}
       \ENDWHILE
       \STATE {\bfseries Return:} $t$
    \end{algorithmic}
\end{algorithm} 

\subsection{Wall-clock Times for \lbfgs{} and NNCG}

\begin{table}[t]
    \caption{Per-iteration times (in seconds) of \lbfgs{} and NNCG on each PDE.}
    \vskip 0.15in
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Optimizer & Convection & Reaction & Wave \\
        \hline
        \lbfgs{} & 4.6e-2 & 3.6e-2 & 9.0e-2 \\
        \hline
        NNCG & 2.5e-1 & 7.2e-1 & 2.9e1 \\
        \hline
        Time Ratio & 5.43 & 20 & 322.22 \\
        \hline
    \end{tabular}
    \label{tab:wall_clock_time_comparison}
\end{table}

\cref{tab:wall_clock_time_comparison} summarizes the per-iteration wall-clock times of \lbfgs{} and NNCG on each PDE. The large gap on wave (compared to reaction and convection) is because NNCG has to compute hessian-vector products involving second derivatives, while this is not the case for the two other PDEs. 

% NNCG can be improved in a number of ways to lower its runtime in comparison to \lbfgs{}. We can leverage existing auto-differentiation strategies in the literature and use a faster strategy to compute the hessian-vector product. We can further close the gap between NNCG and \lbfgs{} using several techniques: (i) subsampled Hessians (which would reduce $n_{\mathrm{res}} + n_{\mathrm{bc}}$ to some smaller value, leading to smaller wall-clock time per iteration), (ii) adaptive damping (which would lead to faster convergence with respect to iterations), and (iii) early termination when computing the damped Newton step via PCG. For example, if we subsample Hessians using $10\%$ of the data, then we would expect the wall-clock time per iteration to decrease by a factor of $10$. For PCG, early termination could probably lead to a 2-3 times speedup, since we run $1000$ PCG iterations per damped Newton step, which is rather conservative. In total, we could easily get a 20-30 times speedup per iteration; we leave the implementation of these improvements to future work.
