\section{The Loss is Often Under-optimized}
\label{sec:under_optimized}
In \cref{sec:opt_comparison}, we show that \al{} improves on running Adam or \lbfgs{} alone.
However, even \al{} does not reach a critical point of the loss: the loss is still under-optimized.
We show that the loss and L2RE can be further improved by running a damped version of Newton's method.
% , where we solve for the Newton step using Nystr\"{o}mPCG.

\subsection{Why is the Loss Under-optimized?}
\cref{fig:under_optimization} shows the run of \al{} with smallest L2RE for each PDE.
For each run, \lbfgs{} stops making progress before reaching the maximum number of iterations.
\lbfgs{} uses \textit{strong Wolfe line search}, as it is needed to maintain the stability of \lbfgs{} \cite{nocedal2006numerical}.
\lbfgs{} often terminates because it cannot find a positive step size satisfying these conditions---we have observed several instances where \lbfgs{} picks a step size of zero (\cref{fig:line_search_multi_pde} in \cref{sec:under_optimization_additional}), leading to early stopping.
Perversely, \lbfgs{} stops in these cases without reaching a critical point: 
the gradient norm is around $10^{-2}$ or $10^{-3}$ 
(see the bottom row of \cref{fig:under_optimization}).
The gradient still contains useful information for improving the loss.

\subsection{NysNewton-CG (NNCG)}
\label{subsec:NNCG} 
We can avoid premature termination by using a damped version of Newton's method with \textit{Armijo line search}.
The Armijo conditions use only a subset of the strong Wolfe conditions.
Under only Armijo conditions, \lbfgs{} is unstable; we require a different 
approximation to the Hessian ($p\times p$ for a neural net with $p$ parameters) that does not require storing ($\mathcal O(p^2)$) or inverting ($\mathcal O(p^3)$) the Hessian.
% We do not explicitly form and invert the Hessian, which would take $\mathcal{O}(p^2)$ memory and $\mathcal{O}(p^3)$ time, where $p$ is the number of parameters in the neural network.
Instead, we run a Newton-CG algorithm that solves for the Newton step using preconditioned conjugate gradient (PCG).
This algorithm can be implemented efficiently with Hessian-vector products. These can be computed $\mathcal{O}\left((\nres+\nbc)p\right)$ time \cite{pearlmutter1994fast}.
\cref{sec:loss_landscape} shows that the Hessian is ill-conditioned with fast spectral decay, so CG without preconditioning will converge slowly.
Hence we use Nystr\"{o}mPCG, a PCG method that is designed to solve linear systems with fast spectral decay \cite{frangella2023randomized}.
The resulting algorithm is called NysNewton-CG (abbreviated NNCG); a full description of the algorithm appears in \cref{sec:under_optimization_additional}.

\subsection{Performance of NNCG}
\begin{figure*}
    \centering
    \includegraphics[scale=0.33]{figs/under_optimization.pdf}
    \caption{Performance of NNCG and GD after \al. 
    (Top) NNCG reduces the loss by a factor greater than 10 in all instances, while GD fails to make progress. (Bottom) Furthermore, NNCG significantly reduces the gradient norm on the convection and wave problems, while GD fails to do so.}
    \label{fig:under_optimization}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.33]{figs/solution_evolutions.pdf}
    \caption{Absolute errors of the PINN solution at optimizer switch points. 
    The first column shows errors after Adam, the second column shows errors after running \lbfgs{} following Adam, and the third column shows the errors after running NNCG folllowing \al{}.
    \lbfgs{} improves the solution obtained from first running Adam, and NNCG further improves the solution even after \al{} stops making progress. 
    Note that Adam solution errors (left-most column) are presented at separate scales as these solutions are far off from the exact solutions. }
    \label{fig:solution_evolutions}
\end{figure*}

% \begin{table}[t]
%     \caption{L2RE after fine-tuning by NNCG and GD. NNCG outperforms both GD and the original \al{} results.}
%     \vskip 0.15in
%     \centering
%     \scriptsize
%     \begin{tabular}{|c|c|c|c|}
%         \hline
%         Optimizer & Convection & Reaction & Wave \\
%         \hline
%         \al{} & 4.19e-3 & 1.92e-2 & 5.52e-2 \\
%         \hline
%         \aln{} & \textbf{1.94e-3} & \textbf{9.92e-3} & \textbf{1.27e-2} \\
%         \hline
%         \alg{} & 4.19e-3 & 1.92e-2 & 5.52e-2 \\
%         \hline
%     \end{tabular}
%     \label{tab:nncg_l2re_improvement}
% \end{table}

\begin{table*}[t]
    \caption{Loss and L2RE after fine-tuning by NNCG and GD. NNCG outperforms both GD and the original \al{} results.}
    \vskip 0.15in
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|c|c|c|c|c|} 
    \hline 
    \multirow{2}{*}{Optimizer} & \multicolumn{2}{c|}{Convection} & \multicolumn{2}{c|}{Reaction} & \multicolumn{2}{c|}{Wave} \\ \cline{2-7}
                               & Loss & L2RE & Loss & L2RE & Loss & L2RE \\ \hline 
        \al{} & 5.95e-6 & 4.19e-3 & 5.26e-6 & 1.92e-2 & 1.12e-3 & 5.52e-2 \\
        \hline
        \aln{} & \textbf{3.63e-7} & \textbf{1.94e-3} & \textbf{2.89e-7} & \textbf{9.92e-3} & \textbf{6.13e-5} & \textbf{1.27e-2} \\
        \hline
        \alg{} & 5.95e-6 & 4.19e-3 & 5.26e-6 & 1.92e-2 & 1.12e-3 & 5.52e-2 \\
        \hline
    \end{tabular}
    \label{tab:nncg_l2re_improvement}
\end{table*}

\cref{fig:under_optimization} shows that NNCG significantly improves both the loss and gradient norm of the solution when applied after \al{}, while \cref{fig:solution_evolutions} visualizes how NNCG improves the absolute error (pointwise) of the PINN solution when applied after \al{}.
Furthermore, \cref{tab:nncg_l2re_improvement} shows that NNCG also improves the L2RE of the PINN solution.
In contrast, applying gradient descent (GD) after \al{} improves neither the loss nor the L2RE. This result is unsurprising, as our theory predicts that NNCG will work better than GD for an ill-conditioned loss (\cref{sec:theory}). 

\subsection{Why Not Use NNCG Directly After Adam?}
\label{subsec:why_not_nncg}
Since NNCG improves the PINN solution and uses simpler line search conditions than \lbfgs, it is tempting to replace \lbfgs{} with NNCG entirely.
However, NNCG is slower than \lbfgs{}: the \lbfgs{} update can be computed in $\mathcal O(mp)$ time, where $m$ is the memory parameter, while just a single Hessian-vector product for computing the NNCG update requires $\mathcal{O}\left((\nres+\nbc)p\right)$ time. \cref{tab:wall_clock_time_comparison} shows NNCG takes 5, 20, and 322 more times per-iteration as \lbfgs{} on convection, reaction, and wave respectively. 
% However, the time to compute the NNCG update is much longer than that of the L-BFGS update (\pnote{add theoretical complexities to appendix}).
Consequently, we should run \al{} to make as much progress as possible before switching to NNCG.

% \begin{itemize}
    % \item Show the gradient norms of the best runs for each PDE -- they are not close to zero. This means the loss can be optimized further, which is especially important for PINNs since high-precision is needed.
    % \item For proof-of-concept, we apply an additional damped Newton-CG fine-tuning step after \al. This should work if \lbfgs{} has gotten us to a (nearly)-convex region.
    % \item Since the loss landscape has fast spectral decay, we should not use vanilla CG. However, Nystr\"{o}m preconditioning is appropriate for this exact scenario. Consequently, we develop NysNewton-CG. 
    % \item Show improvements from NysNewton-CG in both gradient norm and loss. We can get XXX times improvement in l2re for each problem on the hardest coefficient setting.
    % \item Demonstrate that NysNewton-CG is better than no preconditioning. This can be done by looking at a semilog plot of the loss when using NysNewton-CG and gradient descent, and comparing the slopes.
    % \item Note possible improvements to NysNewton-CG: Damping could be made automatic, optimizer is also quite slow due to hvps
    % \item Improve on state-of-the-art results that are using Adam + L-BFGS (look at PINNacle for ideas)
% \end{itemize}

% \pnote{Add wall-clock times to L2RE table}