\section{Computing the Spectral Density of the \lbfgs{}-preconditioned Hessian}
\label{sec:lbfgs_spectral_info}

\subsection{How L-BFGS Preconditions} 
\label{subsec:how_lbfgs_preconditions}
To minimize \eqref{eq:pinn_prob_gen}, L-BFGS uses the update
\begin{equation}
\label{eq:l-bfgs}
w_{k+1} = w_k-\eta H_k \nabla L(w_k), 
\end{equation}
where $H_k$ is a matrix approximating the inverse Hessian.
We now show how \eqref{eq:l-bfgs} is equivalent to preconditioning the objective \eqref{eq:pinn_prob_gen}.
Define the coordinate transformation $w = H_k^{1/2}z$. 
By the chain rule, $\nabla L(z) = H_k^{1/2}\nabla L(w)$ and $H_L(z) = H^{1/2}_k H_L(w)H_k^{1/2}$. 
Thus, \eqref{eq:l-bfgs} is equivalent to
\begin{align}
\label{eq:precond}
    & z_{k+1} = z_k-\eta \nabla L(z_k), \\
    & w_{k+1} = H^{1/2}_kz_{k+1}. \nonumber
\end{align}

\cref{eq:precond} reveals how L-BFGS preconditions \eqref{eq:pinn_prob_gen}.
L-BFGS first takes a step in the \emph{preconditioned} $z$-space, where the conditioning is determined by $H_L(z)$, the preconditioned Hessian. 
Since $H_k$ approximates $H_L^{-1}(w)$, $H^{1/2}_k H_L(w) H_k^{1/2} \approx I_p$, so the condition number of $H_L(z)$ is much smaller than that of $H_L(w)$. 
Consequently, L-BFGS can take a step that makes more progress than a method like gradient descent, which performs no preconditioning at all. 
In the second phase, L-BFGS maps the progress in the preconditioned space back to the original space.
Thus, L-BFGS is able to make superior progress by transforming \eqref{eq:pinn_prob_gen} to another space where the conditioning is more favorable, which enables it to compute an update that better reduces the loss in \eqref{eq:pinn_prob_gen}.

\subsection{Preconditioned Spectral Density Computation}
Here we discuss how to compute the spectral density of the Hessian after preconditioning by L-BFGS.
This is the procedure we use to generate the figures in \cref{subsec:lbfgs_improvement}. 

\lbfgs{} stores a set of vector pairs given by the difference in consecutive iterates and gradients from most recent $m$ iterations (we use $m = 100$ in our experiments).
To compute the update direction $H_k \nabla f_k$, \lbfgs{} combines the stored vector pairs with a recursive scheme \cite{nocedal2006numerical}.
Defining
\[
    s_{k} = x_{k+1} - x_{k}, 
    \quad y_k = \nabla f_{k+1} - \nabla f_{k}, 
    \quad \rho_{k} = \frac{1}{y_{k}^{T}s_{k}}, 
    \quad \gamma_{k} = \frac{s_{k-1}^{T}y_{k-1}}{y_{k-1}^{T}y_{k-1}}, 
    \quad V_k = I - \rho_{k} y_{k} s_{k}^{T}, 
    \quad H_k^{0} = \gamma_{k} I,
\]
the formula for $H_k$ can be written as
\[
    H_{k} 
    = (V_{k-1}^{T} V_{k-m}^{T}) H_{k}^{0} (V_{k-m} V_{k-1}) 
    + \sum_{l=2}^{m} \rho_{k-l} (V_{k-1}^{T} \cdots V_{k-l+1}^{T}) s_{k-l} s_{k-l}^{T} (V_{k-l+1} \cdots V_{k-1})
    + \rho_{k-1} s_{k-1} s_{k-1}^{T}.
\]
Expanding the terms, we have for $j \in \{1, 2, \ldots, i\}$,
\[
    V_{k-i} \cdots V_{k-1} = I - \sum_{j=1}^{i} \rho_{k-j} y_{k-j} \tilde{v}_{k-j}^{T}
    \quad \text{where} \quad \tilde{v}_{k-j} = s_{k-j} - \sum_{l=1}^{j-1} (\rho_{k-l} y_{k-l}^{T} s_{k-j}) \tilde{v}_{k-l}.
\]
It follows that
\[
    H_{k} 
    = (I - \tilde{Y}\tilde{V}^{T})^{T} \gamma_{k} I (I - \tilde{Y}\tilde{V}^{T}) + \tilde{S} \tilde{S}^{T}
    = 
    \begin{bmatrix*}
        \sqrt{\gamma_k} (I - \tilde{Y}\tilde{V}^{T})^{T} & \tilde{S}
    \end{bmatrix*}
    \begin{bmatrix*}
        \sqrt{\gamma_k} (I - \tilde{Y}\tilde{V}^{T}) \\ 
        \tilde{S}^{T}.
    \end{bmatrix*}
    = \tilde{H}_k \tilde{H}_k^{T},
\]
where
\[
\begin{aligned}
    & \tilde{Y} = 
    \begin{bmatrix*} 
        \vert & & \vert \\
        \rho_{k-1} y_{k-1} & \cdots & \rho_{k-m} y_{k-m} \\
        \vert & & \vert \\
    \end{bmatrix*}, \\
    & \tilde{V} = 
    \begin{bmatrix*} 
        \vert & & \vert \\
        \tilde{v}_{k-1} & \cdots & \tilde{v}_{k-m} \\
        \vert & & \vert \\
    \end{bmatrix*}, \\
    & \tilde{S} = 
    \begin{bmatrix*} 
        \vert & & \vert \\
        \tilde{s}_{k-1} & \cdots & \tilde{s}_{k-m} \\
        \vert & & \vert \\
    \end{bmatrix*},
    \quad \tilde{s}_{k-1} = \sqrt{\rho_{k-1}} s_{k-1}, 
    ~ \tilde{s}_{k-l} = \sqrt{\rho_{k-l}} (V_{k-1}^{T} \cdots V_{k-l+1}^{T}) s_{k-l} ~ \text{for} ~ 2 \leq l \leq m.
\end{aligned}
\]
We now apply \cref{alg-unrolling-lbfgs} to unroll the above recurrence relations to compute columns of $\tilde Y, \tilde S$ and $\tilde V$. 

\begin{algorithm}[H]
  \centering
  \caption{Unrolling the \lbfgs{} Update}
  \label{alg-unrolling-lbfgs}
  \begin{algorithmic}
  \INPUT{saved directions $\{y_i\}_{i=k-1}^{k-m}$, saved steps $\{s_i\}_{i=k-1}^{k-m}$, saved inverse of inner products $\{\rho_i\}_{i=k-1}^{k-m}$}
    \STATE {$\tilde{y}_{k-1} = \rho_{k-1} y_{k-1}$}
    \STATE {$\tilde{v}_{k-1} = s_{k-1}$}
    \STATE {$\tilde{s}_{k-1} = \sqrt{\rho_{k-1}} s_{k-1}$}
    \FOR{$i = k-2, \dots, k-m$}
        \STATE {$\tilde{y}_i = \rho_i y_i$}
        \STATE {Set $\alpha = 0$}
        \FOR{$j = k-1, \dots, i+1$}
          \STATE {$\alpha = \alpha + (\tilde{y}_j^{T} s_i) \tilde{v}_j$}
        \ENDFOR
        \STATE {$\tilde{v}_i = s_i - \alpha$}
        \STATE {$\tilde{s}_i = \sqrt{\rho_i} (s_i - \alpha)$}
    \ENDFOR
  \OUTPUT{vectors $\{\tilde{y}_i, \tilde{v}_i, \tilde{s}_i\}_{i=k-1}^{k-m}$}
  \end{algorithmic}
\end{algorithm}

Since (non-zero) eigenvalues of $\tilde{H}_{k}^{T} H_L(w)\tilde{H}_{k}$ equal the eigenvalues of the preconditioned Hessian $H_{k} H_L(w) = \tilde{H}_k \tilde{H}_k^{T} H_L(w)$ (Theorem 1.3.22 of \citet{horn2012matrix}), we can analyze the spectrum of $\tilde{H}_{k}^{T}H_L(w)\tilde{H}_{k}$ instead.
This is advantageous since methods for calculating the spectral density of neural network Hessians are only compatible with symmetric matrices.

Since $\tilde{H}_{k}^{T} H_L(w)\tilde{H}_{k}$ is symmetric, we can use stochastic Lanczos quadrature (SLQ) \cite{golub2009matrices,lin2016approximating} to compute spectral density of this matrix.
SLQ only requires matrix-vector products with $\tilde H_k$ and Hessian-vector products, the latter of which may be efficiently computed via automatic differentiation; this is precisely what PyHessian does to compute spectral densities \cite{yao2020pyhessian}.

\begin{algorithm}[H]
  \centering
  \caption{Performing matrix-vector product}
  \label{alg-mvp}
  \begin{algorithmic}
  \INPUT{matrices $\tilde{Y}$, $\tilde{V}$, $\tilde{S}$ formed from resulting vectors from unrolling, vector $v$, and saved scaling factor for initializing diagonal matrix $\gamma_k$}
    \STATE {Split vector $v$ of length $\mathrm{size}(w) + m$ into $v_1$ of size $\mathrm{size}(w)$ and $v_2$ of size $m$}
    \STATE {$v' = \sqrt{\gamma_k}(v_1 - \tilde{V}\tilde{Y}^{T}v_1) + \tilde{S} v_2$}
    \STATE {Perform Hessian-vector-product on $v'$, and obtain $v''$}
    \STATE {Stack $\sqrt{\gamma_k}(v'' - \tilde{Y}\tilde{V}^{T}v'')$ and $\tilde{S}^{T}v''$, and obtain $v'''$}
  \OUTPUT{resulting vector $v'''$}
  \end{algorithmic}
\end{algorithm}

By combining the matrix-vector product procedure described in \cref{alg-mvp} with the Hessian-vector product operation, we are able to obtain spectral information of the preconditioned Hessian. 

\begin{figure*}
    \centering
    \includegraphics[trim={0 1.25cm 0 0}, clip, scale=0.45]{figs/spectral_density_reaction.pdf}
    \includegraphics[scale=0.45]{figs/spectral_density_wave.pdf}
    \caption{Spectral density of the Hessian and the preconditioned Hessian of each loss component after 41000 iterations of \al{} for the reaction and wave problems. The plots show the loss landscape of each component is ill-conditioned, and the conditioning of each loss component is improved by \lbfgs{}.}
    \label{fig:spectral_density_reaction_wave}
\end{figure*}