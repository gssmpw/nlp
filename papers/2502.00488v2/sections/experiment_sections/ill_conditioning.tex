\section{Experiments}
\label{sec:loss_landscape}

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.33]{figs/spectral_density_multi_pde_convection_combined.pdf}
    \caption{(Top) Spectral density of the Hessian and the preconditioned Hessian after 41000 iterations of \al{}. 
    The plots show that the PINN loss is ill-conditioned and that \lbfgs{} improves the conditioning, reducing the top eigenvalue by $10^3$ or more. \\
    (Bottom) Spectral density of the Hessian and the preconditioned Hessian of each loss component after 41000 iterations of \al{} for convection. The plots show that each component loss is ill-conditioned and that the conditioning is improved by \lbfgs{}.}
    \label{fig:spectral_density_multi_pde_convection_combined}
\end{figure*}

% In this section, %we empirically study the conditioning of the PINN loss. 
We show empirically that the ill-conditioning of the PINN loss is mainly due to the residual loss, which contains the differential operator.
We also show that quasi-Newton methods like \lbfgs{} improve the conditioning of the problem. 

\subsection{The PINN Loss is Ill-conditioned}

% \begin{itemize}
%     \item Say that we use PyHessian for obtaining spectral densities
%     \item Show loss landscapes for each PDE at the most difficult setting (could place landscapes for other coefficient settings in the appendix)
%     \item Explain why this is bad. Appeal to convergence of gradient descent depending on the condition number
% \end{itemize}

% The conditioning of the loss plays a significant role in optimization and affects the performance of optimization algorithms. 
The conditioning of the loss $L$ plays a key role in the performance of first-order optimization methods \cite{nesterov2018lectures}.
% This property is closely related to the curvature of the loss landscape. 
We can understand the conditioning of an optimization problem through the eigenvalues of the Hessian of the loss, $H_L$. 
Intuitively, the eigenvalues of $H_L$ provide information about the local curvature of the loss function at a given point along different directions. 
The condition number is defined as the ratio of the largest magnitude's eigenvalue to the smallest magnitude's eigenvalue.
A large condition number implies the loss is very steep in some directions and flat in others, making it difficult for first-order methods to make sufficient progress toward the minimum. 
When $H_L(w)$ has a large condition number (particularly, for $w$ near the optimum), the loss $L$ is called \emph{ill-conditioned}.
For example, the convergence rate of gradient descent (GD) depends on the condition number \cite{nesterov2018lectures}, which results in GD converging slowly on ill-conditioned problems.

% and ill-conditioned problems--characterized by a large condition number--lead to slow convergence of this algorithm. 

To investigate the conditioning of the PINN loss $L$, 
we would like to examine the eigenvalues of the Hessian. 
For large matrices, it is convenient to visualize the set of eigenvalues via  \emph{spectral density}, which approximates the distribution of the eigenvalues.
Fast approximation methods for the spectral density of the Hessian are available for deep neural networks \cite{ghorbani2019an, yao2020pyhessian}. 
% Specifically, we apply stochastic Lanczos quadrature to compute empirical spectral density of the Hessian eigenvalues. 
\cref{fig:spectral_density_multi_pde_convection_combined} shows the estimated Hessian spectral density (solid lines) of the PINN loss for the convection, reaction, and wave problems after training with \al{}. 
For all three problems, we observe large outlier eigenvalues ($> 10^4$ for convection, $> 10^3$ for reaction, and $> 10^5$ for wave) in the spectrum, and a significant spectral density near $0$, implying that the loss $L$ is ill-conditioned.
The plots also show how the spectrum is improved by preconditioning (\cref{subsec:lbfgs_improvement}).
% Since convection and wave have larger outlier eigenvalues, we expect first-order methods to perform worse than on the reaction 
% We observe large estimated condition numbers for all three problems. 
% This provides an indication that the PINN loss is ill-conditioned, and we should expect very slow convergence if we were to use a first-order method like GD. 
% Moreover, the reaction problem appears to have noticeably large negative eigenvalues that signify saddle point, further illustrating the difficult loss landscape that optimization algorithms have to deal with. 
% \textcolor{blue}{\textbf{[WL: need comparison with loss landscape of ResNet (loss surface plot as seen in the Goldstein's)?]}}

\subsection{The Ill-conditioning is Due to the Residual Loss}

% \begin{itemize}
%     \item Look at the spectrum of each component of the loss separately.
%     \item Show that this is the case due to the presence of the differential operator in the residual loss 
% \end{itemize}

We use the same method to study the conditioning of each component of the PINN loss. \cref{fig:spectral_density_multi_pde_convection_combined,fig:spectral_density_reaction_wave} show the estimated spectral density of the Hessian of the residual, initial condition, and boundary condition components of the PINN loss for each problem after training with \al. 
We see residual loss, which contains the differential operator $\mathcal D$, is the most ill-conditioned among all components.
Our theory (\cref{sec:theory}) shows this ill-conditioning is likely due to the ill-conditioning of $\mathcal D$.
% implies the differential operator that defines the residual loss is ill-conditioned. 
% We explain consequences of an ill-conditioned differential operator in \cref{sec:theory}.

\subsection{\lbfgs{} Improves Problem Conditioning}
\label{subsec:lbfgs_improvement}

% \begin{itemize}
%     \item Show preconditioning effect of \lbfgs{} on the Hessian of the entire loss and the Hessian of each loss component. Explain the notion of the preconditioned Hessian, but leave mathematical details of unrolling the \lbfgs{} recursion for the appendix
%     \item Remind reader that improving condition number leads to faster convergence
%     \item Compare to loss landscape of a vision task?
% \end{itemize}

Preconditioning is a popular technique for improving conditioning in optimization. 
A classic example is Newton's method, which uses second-order information (i.e., the Hessian) to (locally) transform an ill-conditioned loss landscape into a well-conditioned one.
% It aims to transform the loss landscape so that it becomes well-conditioned, thus improving convergence by making local minima more accessible to gradient updates in the transformed space. 
\lbfgs{} is a quasi-Newton method that improves conditioning without explicit access to the problem Hessian. 
%Instead, it implicitly constructs and stores a symmetric positive definite preconditioner that approximates the Hessian over the optimization trajectory.
% \lbfgs{}, one of the classic deterministic quasi-Newton methods, implicitly stores a symmetric positive definite preconditioner that satisfies the secant equation and represent the preconditioner as a low-rank update to a diagonal matrix at each iteration \cite{nocedal2006numerical}. 
To examine the effectiveness of quasi-Newton methods for optimizing $L$, 
we compute the spectral density of the Hessian after \lbfgs{} preconditioning. (For details of this computation and how L-BFGS preconditions, see \cref{sec:lbfgs_spectral_info}.)
\cref{fig:spectral_density_multi_pde_convection_combined} shows this preconditioned Hessian spectral density (dashed lines). 
%For all three problems, both the spread and magnitude of eigenvalues has decreased by $10^3$ or more, 
%improving the condition number by at least $10^3$. 
For all three problems, the magnitude of eigenvalues and the condition number has been reduced by at least $10^3$. 
%or more, 
%improving the condition number by at least $10^3$. 
In addition, the preconditioner improves the conditioning of each individual loss component of $L$ (\cref{fig:spectral_density_multi_pde_convection_combined,fig:spectral_density_reaction_wave}). 
These observations offer clear evidence that quasi-Newton methods improve the conditioning of the loss, and show the importance of quasi-Newton methods in training PINNs, which we demonstrate in \cref{sec:opt_comparison}. 