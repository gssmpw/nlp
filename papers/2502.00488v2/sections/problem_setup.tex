\section{Problem Setup}

In this section, we first introduce the setting of the sharp interface problem investigated in this study, followed by the methodology for solving the sharp interface problem using neural networks. Finally, we discuss the training difficulties encountered when applying neural networks to solve this problem, which is the motivation for our work.
\subsection{Sharp Interface Problem}

The form of the sharp interface problem is defined as follows: 
\begin{equation}
\left\{\begin{array}{l}
\mathcal{L}_\varepsilon u=f(u), \quad  \text{in } \Omega, \\
\Bc u = g(x), \quad \text{on } \partial \Omega,
\end{array}\right.\label{eq:gen_pde}
\end{equation}
where $\mathcal{L}_\varepsilon$ is a differential operator defining the PDE with certain parameters, $\Bc$ is an operator associated with the boundary and/or initial conditions, and $\Omega \subseteq \R^d$.  Here, \(\varepsilon\) is the PDE parameter that quantifies how singular the system is. Specifically, as \(\varepsilon \to 0\), the PDE becomes increasingly singular, whereas, for \(\varepsilon \to \infty\), the solution is easier to compute.
 For example, in the Allen-Cahn equation \eqref{eq:1d_allen_cahn}, \( \varepsilon \) represents the interfacial width parameter, while in Burgers' equation  \eqref{eq:1D_Burgers}, \( \varepsilon \) corresponds to the viscosity coefficient. More details will be provided in Section~\ref{sec:Experiments}.
\subsection{Neural Networks for
Solving PDEs}

In this section, we focus on solution approximation rather than operator learning for simplicity, specifically using a neural network to approximate the PDE solution. In Section \ref{sec:Experiments}, we will demonstrate that our \textit{Homotopy Dynamic} can also generalize to the operator learning case.  the PDE problem is typically reformulated as the following non-linear least-squares problem, aiming to determine the parameters $\boldsymbol{\theta}$ of the neural network $u(x;{\boldsymbol{\theta}})$ (commonly a multi-layer perceptron, MLP):
\begin{align}
    \underset{\boldsymbol{\theta} \in \R^p}{\mbox{min}}~L(\boldsymbol{\theta}) \coloneqq  & \underbrace{\frac{1}{2\nres}\sum_{i=1}^{\nres}\left(\Lc_\varepsilon u(\vx_r^i; \boldsymbol{\theta})-f(u(\vx_r^i;\theta))\right)^2}_{L_{\text{res}}}\nonumber \\ &+\lambda\underbrace{\frac{1}{2\nbc}\sum^{\nbc}_{i=1}\left(\Bc u(\vx_b^j;\vtheta)-g(\vx_b^j)\right)^2}_{ L_{\text{bc}}}. \label{loss}
\end{align}

Here $L_{\text{res}}$ is the PDE residual loss, $ L_{\text{bc}}$ is the boundary loss and $\lambda$ is a constant used to balance these two terms. The sets $\{\vx_r^i\}^{\nres}_{i=1}$ represent  represent the interior sample points, and $\{\vx^j_b\}^{\nbc}_{j=1}$ represent  the boundary sample points. %Hence a neural network-based approach requires selection of a suitable neural network architecture $u(\vx,\boldsymbol{\theta})$ and an optimizer to solve the above optimization problem. The process of solving this optimization problem is referred to as training, while the magnitude of the loss is referred to as the training error.
We also introduce the \(\ell_2\) relative error (L2RE) to evaluate the discrepancy between the neural network solution and the ground truth, defined as
\begin{align*}
    \mathrm{L2RE} = \frac{\|u_{\vtheta} - u^{*}\|_2}{\|u^{*}\|_2},
\end{align*}where \(u_{\boldsymbol{\theta}}\) is the neural network solution and \(u^{*}\) is the ground truth.

% Smaller training error corresponds to better accuracy between the PINN solution and the ground truth, better generalization for PINNs \cite{mishra2023estimates}. Therefore, designing neural networks and optimization algorithms to minimize the training error is a critical research topic.
% Unlike traditional deep learning, there is no reason \textit{a priori} to use more sophisticated networks such as convolutional networks \cite{lecun1995convolutional}, residual networks \cite{he2016deep} or transformers \cite{vaswani2017attention}, as it is not clear they yield any benefit. 
% Thus, $u(x;w)$ is commonly taken to be a multilayer perceptron (MLP) \cite{krishnapriyan2021characterizing,hao2023pinnacle}.

% \begin{itemize}
%     \item Describe the PINN problem (what is the purpose, what is the mathematical formulation)
%     % \item Show a diagram of a PINN?
%     \item Usually use MLPs
%     \item The forward PDE problem does not have noise, so overfitting is not an issue -- We want interpolation
% \end{itemize}

\subsection{Challenges in Training Neural Networks}

To illustrate the challenges in training, we consider the following one-dimensional Allen-Cahn steady-state equation as an example:
\begin{equation}
\left\{\begin{array}{l}
\varepsilon^2 u^{\prime \prime}(x) + {u^3 - u}=0, \quad x \in[0,1], \\
u(0)=-1, \quad u(1)=1,
\end{array}\right.
\label{eq:1d_allen_cahn}
\end{equation}
where $\varepsilon$ governs the thickness of the interface in the steady-state solution. 
A decrease in 
$\varepsilon$ results in a thinner interface, leading to a sharper interface. 
The steady-state solution takes the following closed-form expression:\begin{equation}
    u(x) = \tanh\left(\frac{x-0.5}{\sqrt{2}\varepsilon}\right),
\end{equation}
where the interface is located at $x = 0.5$, and as $\varepsilon$ decreases, the solution becomes sharper (\cref{fig:1d_allen_cahn_homo_result}). %To solve this problem, the loss can be defined as
%\begin{align}
%\label{eq:pinn_prob_gen}
%   L(\boldsymbol{\theta}) = & \frac{1}{2\nres}\sum_{i=1}^{\nres}\left(\varepsilon u_{\boldsymbol{\theta}}^{\prime \prime}(x_r^{i}) + \frac{(u_{\boldsymbol{\theta}}(x_r^{i})^{3}-u_{\boldsymbol{\theta}}(x_r^{i}))}{\varepsilon})\right)^2\\ 
%    &+\frac{\lambda}{2\nbc}\sum^{\nbc}_{i=1}\left(u_{\boldsymbol{\theta}}(x_b^j))\right)^2, \nonumber
%\end{align}
%where $x_r^{i} \in [0,1]$ and $x_b^{j} \in \{0,1\}$. 

To show the challenges in the optimization problem defined in (\ref{loss}), we present the training curves for varying values of \(\varepsilon\) in \cref{fig:1d_allen_cahn_pinn_loss}. As \(\varepsilon\) decreases, training errors increase. This is due to the significantly increased training difficulty and slower convergence for smaller \(\varepsilon\), as the solution becomes sharper. In the subsequent sections, we analyze the underlying reasons for this phenomenon and introduce a homotopy dynamics-based approach to address the challenge.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.45]{figs/Loss_curve_1DAC.png}
    \caption{Training curves for different values of \(\varepsilon\) in solving the 1D Allen-Cahn steady-state equation. As \(\varepsilon\) decreases, the training error increases, indicating that the training process becomes progressively more difficult.}
    \label{fig:1d_allen_cahn_pinn_loss}
\end{figure}


% \pnote{State equation for L2RE}
% \pnote{State deep learning framework + hardware}

% \begin{itemize}
    % \item PDEs studied: Convection, wave, reaction-diffusion (failure modes of PINNs + gradient pathologies by Perdikaris). Say what coefficients we use (DONE)
    % \item We compare Adam, L-BFGS, and Adam + L-BFGS. Give hyperparameters we use for each one + number of epochs (DONE)
    % \item Talk about \# of random seeds, network architectures (DONE)
    % \item Compare commonly used optimization approaches: Adam, L-BFGS, Adam + L-BFGS. Compare using line search to no line search. Is there an implicit regularization effect by using Adam followed by L-BFGS?
    % \item Use spectral density to understand the loss landscape, before and after preconditioning. Could also contrast with a ``classic'' problem, like CIFAR10 on ResNet
    % \item Try Gauss-Newton approach for preconditioning
    % \item L-BFGS often terminates without reaching a gradient norm close enough to zero. How can we lower it? We should try restarting L-BFGS or using preconditioned Newton-CG.
    % \item Study how widening the networks changes the optimization. Does it help -- why or why not?
    % \item Since we want to interpolate, try (preconditioned) Polyak step sizes?
% \end{itemize}