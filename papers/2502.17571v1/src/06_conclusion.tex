
\section{Conclusion and Future Work}
In this work, we explored strategies for conditioning LLMs to give clinicians control over both content and style in clinical text generation. Using the BioNLP ACL'24 Shared Task Discharge Me! as a case study, we demonstrated that augmenting datasets with authoring guidelines and topic segmentation significantly improves accuracy, relevance, and factual consistency. Notably, our findings raise concerns about metrics exhibiting significant sensitivity to stylistic deviations, even when fact-based, warranting further research.

Our preliminary human evaluation suggests that LLMs can serve as proxies for expert annotations, enabling dataset augmentation at scale. By introducing a separation of content and style, we extended the traditional clinical text generation paradigm to facilitate the integration of clinical communication and authoring guidelines. Since such guidelines are crafted once per task, they offer a low-cost enhancement to clinical text generation without adding cognitive burden.

We also establish a new state-of-the-art for conventional clinical text generation on Discharge Me!, surpassing prior submissions while using fewer parameters and significantly lower computational costs. To support further research and real-world adoption, we disclose our methods, allowing hospitals and clinical institutions to adapt these augmentations to their own data and workflows.

While preliminary human evaluation validates the effectiveness of our approach, a systematic study is needed to identify which specific components of authoring guidelines contribute most to downstream performance. Future work should also focus on scaling human evaluation, assessing generalization across diverse clinical datasets, and refining LLM conditioning techniques to improve adaptability to real-world medical documentation workflows. Additionally, user studies should evaluate interactivity and its impact on clinician oversight.


\section{Limitations}
While our approach demonstrates strong performance in clinical text generation, several limitations remain. Our findings rely primarily on automated metrics, with only preliminary human evaluation, making a larger-scale, clinician-in-the-loop assessment essential to validate practical usability and real-world adoption. Additionally, this study does not yet evaluate how interactive clinician involvement impacts cognitive workload and oversight burden. Future work should investigate whether LLM-conditioned generation can reduce verification effort and how user feedback can further refine dataset augmentation to better align with clinical workflows. Lastly, as with all large-scale pre-trained models, our approach inherits biases from its training data, potentially affecting fairness and reliability in clinical decision-making. No work was done to mitigate such bias and assess the clinical implications of these biases to ensure responsible AI deployment in healthcare, and the effects remain unknown.