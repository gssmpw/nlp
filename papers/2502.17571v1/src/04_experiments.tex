\section{Experiments and Evaluation}
\vspace{-0.4em}
\subsection{Experimental Setup and Baselines}

We fine-tune \textit{Llama 3 8B Instruct} on the training split of the \textit{Discharge Me!} challenge dataset with instruction tuning using $prompt_i(c, g)$ for all possible configurations (see Section~\ref{subsec:instruction-tuning-for-controlled-clinical-text-generation}). See Appendix~\ref{subsec:Training-Details} for training details. This model is chosen to maintain a fair comparison with~\citet{damm-etal-2024-wispermed}, who placed first on the leaderboard by employing a Dynamic Expert Selection (DES) system that included \textit{Llama 3 8B Instruct} as one of its smaller models. We evaluate on the test-phase-2 split used to determine the final leaderboard rankings. The Top 3 leaderboard entries serve as the state-of-the-art baseline for the Brief Hospital Course (BHC) and Discharge Instructions (DI) generation tasks.

\textbf{BASE} denotes our model which is trained without any data augmentations ($c = \texttt{none}$, $g = \texttt{none}$). It serves as a baseline for our other models. Models trained with authoring guidelines ($g \in \{\texttt{style}, \texttt{instr}\})$ are indicated with \textbf{\textsc{w/STYLE}} or \textbf{\textsc{w/INSTR}} respectively. Similarly, models trained on structured output ($c = \texttt{topics}$) are indicated with \textbf{\textsc{w/TOPICS}}.

In addition, we prompt the stronger base model \textit{Llama 3.3 70B Instruct} with user messages $user_i(c, g)$ zero-shot and three-shot to assess the gains provided by dataset augmentations without any fine-tuning.

\subsection{Automated Evaluation}
\label{sec:automated-evaluation}
All our models are evaluated using the code provided by the \textit{Discharge Me!} challenge\footnote{https://github.com/Stanford-AIMI/discharge-me/scoring}, which employs a comprehensive set of metrics (see Appendix~\ref{sec:automated-metrics-explanation}) to assess lexical similarity (\textbf{BLEU\nobreakdash-4}, \textbf{ROUGE\nobreakdash-1}, \textbf{ROUGE\nobreakdash-2}, \textbf{ROUGE\nobreakdash-L}, \textbf{METEOR}), semantic similarity (\textbf{BERTScore}), factual consistency (\textbf{AlignScore}), and the clinical relevance and correctness (\textbf{MEDCON}) of the generated texts $\hat{t}_i$ in comparison to the gold-standard target texts $t_i$. It was reported, that this ensemble resulted in rankings that aligned well with clinician evaluation \cite{xu-etal-2024-overview}.
For evaluation, we first complete the BHC task and then use the output to generate the DI section. Greedy decoding is used for inference. For models w/\textsc{topics}, which generate XML-structured outputs $seg(\hat{t}_i)$, the output is parsed into plain text by joining the spans $\hat{t}_i^1, \dots, \hat{t}_i^n$ with white spaces to retrieve the final model output.


To simulate user-control, we adopt a methodology (see Appendix~\ref{sec:automated-metrics-explanation}) inspired by prior work~\citep{10.1145/3660810,10606356}, leveraging LLMs as proxies for human evaluators to automate evaluation on existing benchmarks. Specifically, an LLM acts as a proxy for the original authors of the DI and BHC sections by generating authoring guidelines and providing topic guidance.
For simplicity, topic guidance is provided indirectly and non-interactively, without refining outputs to match the target text, establishing a lower-bound baseline for performance.

\subsection{Human Evaluation}
The evaluation of interactive, user-controlled models would ideally involve a user study, where users engage with the models to generate DI and BHC sections. However, conducting such a study at scale is beyond the scope of this exploratory study, as it is too resource-intensive and time-consuming. We therefore complement our automatic evaluation with two preliminary human evaluations to assess the effectiveness of our approach and the quality of the dataset augmentations.


The first evaluation assessed whether our models generate clinically appropriate outputs when provided with human-written guidelines and whether automatic evaluation metrics align with human judgment. An advanced medical student in his final clinical year, serving as a domain expert, dedicated 95 hours and 13 minutes to manually authoring 200 guidelines for the DI and BHC sections of 100 randomly sampled discharge summaries from the test-phase-2 split of the 'DischargeMe!' dataset.  
While no fixed template was imposed, the expert was encouraged to consider elements such as document type, content coverage, structure, formatting, tone, use of language, complexity, and technicality.
To ensure the guidelines captured clinically relevant stylistic and structural directives, while authentically reflecting human-written guidelines, the expert was instructed to: (1)~Write naturally, following personal preferences, rather than adhering to rigid templates.
(2)~Provide guidance enabling a non-medical layman to write the target text solely based on the discharge summary.
(3)~Avoid medical jargon and patient-specific details, while capturing key clinical writing conventions.

The second evaluation assessed the quality of LLM-generated topic segmentations, specifically the topic accuracy, question validity, and text block appropriateness. 500 discharge summaries were sampled from the post-processed subset of the training split of the 'DischargeMe!' dataset for this purpose, yielding a total of 1000 segmentations $seg(t_i)$. For each $t_i$,  one segment $[h_i^j, q_i^j, t_i^j]$ was randomly selected for assessment. The same medical expert then dedicated 26 hours and 27 minutes to evaluating each segment through a two-step process (see Appendix~\ref{sec:human-evaluation-of-topic-segmentations}).