
\begin{table}[t!]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccr}
\hline
 & 0-shot & 3-shot & RI \\
\hline
Llama 3.3 70B Instruct & 0.175 & \textbf{0.210} & +20\% \\
\hspace{3mm} w/\textsc{style} & 0.184 & \textbf{0.215} & +17\% \\
\hspace{3mm} w/\textsc{instr} & 0.210 & \textbf{0.223} & +6\% \\
\hspace{3mm} w/\textsc{topics} & 0.226 & \textbf{0.227} & +0\% \\
\hspace{3mm} w/\textsc{style} w/\textsc{topics} & \textbf{0.225} & \textbf{0.225} & +0\% \\
\hspace{3mm} w/\textsc{instr} w/\textsc{topics} & \textbf{0.230} & \textbf{0.230} & +0\% \\
\hline
\end{tabular}
}
\caption{Overall evaluation results of Llama 3.3 70B Instruct with zero-shot and three-shot prompting. Relative Improvements (RI) are rounded to integers. See Tab.~\ref{table:task_performance_llama} for detailed results. \vspace{-0.2cm}}
\label{table:llama_results}
\end{table}



\begin{table*}[ht!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\hline
 & Overall & BLEU & R-1 & R-2 & R-L & BS & METEOR & AS & MEDCON \\
\hline
WisPerMed& \textbf{0.332} &\textbf{ 0.124} & \textbf{0.453} & \textbf{0.201} & \textbf{0.308} & \textbf{0.438 }& \textbf{0.403} & \textbf{0.315} & \textbf{0.411} \\
HarmonAiLab@Yale & 0.300 & 0.106 & 0.423 & 0.180 & 0.284 & 0.412 & 0.381 & 0.265 & 0.353 \\
aehrc & 0.297 & 0.097 & 0.414 & 0.192 & 0.284 & 0.383 & 0.398 & 0.274 & 0.332 \\
\hline
BASE & 0.363 & 0.168 & 0.483 & 0.255 & 0.345 & 0.472 & 0.362 & 0.359 & 0.460 \\
\hspace{3mm} w/\textsc{style} & 0.399 & 0.202 & 0.526 & 0.289 & 0.382 & 0.508 & 0.404 & 0.383 & 0.495 \\

\hspace{3mm} w/\textsc{instr} & 0.420 & 0.224 & 0.547 & 0.310 & 0.404 & 0.527 & 0.428 & 0.403 & 0.515 \\

\hspace{3mm} w/\textsc{topics} & 0.403 & 0.195 & 0.524 & 0.287 & 0.384 & 0.503 & 0.414 & 0.402 & 0.517 \\

\hspace{3mm} w/\textsc{style} w/\textsc{topics} & 0.436 & 0.226 & 0.562 & 0.319 & 0.421 & 0.539 & 0.444 & \underline{\textbf{0.429}} & 0.548 \\

\hspace{3mm} w/\textsc{instr} w/\textsc{topics} & \underline{\textbf{0.445}} & \underline{\textbf{0.238}} & \underline{\textbf{0.571}} & \underline{\textbf{0.327}} & \underline{\textbf{0.429}} & \underline{\textbf{0.548}} & \underline{\textbf{0.463}} & 0.426 & \underline{\textbf{0.556}} \\


\hline
\end{tabular}
}
\caption{Evaluation results of the \textit{Discharge Me!} leaderboard leaders WisPerMed~\citep{damm-etal-2024-wispermed}, HarmonAiLab@Yale~\citep{socrates-etal-2024-yale} and aehrc~\citep{liu-etal-2024-e}, and our instruction-tuned models on the test set (phase 2). Bold indicates best scores in each block. In addition, underscoring indicates the overall best score. Figure~\ref{fig:relative_improvments_heatmap} shows relative improvements. Table~\ref{table:task_performance} breaks down performance by task. Abbreviations: BERTScore (BS), AlignScore (AS).}
\label{table:main_results}
\end{table*}



\section{Results and Discussion}

In this section, we analyze the impact of our data augmentation strategies on general instruction-tuned LLMs, evaluate the efficiency of our state-of-the-art training approach, and assess how conditioning text generation for user control enhances clinical text generation. We further present human evaluation results, validating the effectiveness of our approach.

\subsection{Impact of Data Augmentations on General Instruction-Tuned LLMs} 
\label{subsec:user-control-in-general-instruction-tuned-llms}
Llama 3.3 70B Instruct performs significantly worse than previous submissions on the DischargeMe! leaderboard (cf. Tab.~\ref{table:llama_results} vs. Tab.~\ref{table:main_results}). Nonetheless, augmenting the input with authoring guidelines and topic guidance yields a $31\%$ relative improvement in the best configuration ($c = \texttt{topics}, g = \texttt{instr})$ over using no data augmentations.
Notably, zero-shot Llama 3.3 70B Instruct w/\textsc{instr} performs on par with the three-shot setting without any dataset augmentations. This suggest that in-context learning effects can be replicated using explicit authoring guidelines with a lot less context tokens (cf. Tab.~\ref{table:dataset_statistics}). Further supporting this, three-shot prompting provides no additional gains over zero-shot prompting when topic guidance is provided (w/\textsc{topics}).

\textbf{Conclusion:} Overall, zero-shot Llama 3.3 70B Instruct achieves only about half of the performance of our models based on Llama 3 8B Instruct (cf. Tab.\ref{table:main_results}), underscoring the importance of augmenting datasets for user control during training.

\subsection{State-of-the-Art Performance with Efficient Training}
Our instruction-tuned baseline model (BASE), trained without dataset augmentations, achieves a new state-of-the-art on the BioNLP ACL'24 \textit{DischargeMe!} leaderboard, outperforming prior submissions across all metrics except METEOR (Tab.~\ref{table:main_results}). BASE achieves a score of $0.363$, surpassing WisPerMed ($0.332$), HarmonAiLab@Yale ($0.300$), and aehrc ($0.297$) --- a relative improvement of $9\%$ over the previous best model. Compared to them, BASE is more efficient:

\textbf{Smaller Trainable Parameter Size.} BASE has only 169M trainable parameters, which is 5-6× fewer than WisPerMed (1046M), Yale (812M), and aehrc (894M).

\textbf{Simpler Methodology.} We instruction-tune Llama 3 8B Instruct, while WisPerMed employs an ensemble of instruction-tuned Llama 3 8B \& 70B Instruct, OpenBioLLM 70B, Mistral 7B Instruct (v0.2), and Phi 3 Mini 128K Instruct. Yale uses an extended training dataset, while aehrc optimizes the clinical input context for downstream tasks. In addition, other submissions use nucleus sampling or 4-beam search, while we decode greedly.

\textbf{Lower Computational Cost.} Considering all individual training setups, our training requires only $56\%$ of Yale’s compute budget, $23\%$ of aehrc’s, and $32\%$ of WisPerMed’s.

Notably, WisPerMed and aehrc also instruction-tuned Llama 3 8B Instruct using similar approaches, yet reported significantly lower scores ($0.253$ and $0.235$, respectively). Our model achieves relative improvements of $43\%$ over WisPerMed's and $54\%$ over aehrc's fine-tuning attempts. A detailed comparative analysis (Appendix~\ref{sec:comparative-analysis-with-existing-approaches}) suggests that our superior performance stems from more efficient training, which includes higher learning rates, rank-stabilized LoRA and SVD-based PISSA.

\textbf{Conclusion:} Our findings demonstrate that more efficient training strategies can yield substantial improvements, even with fewer parameters and lower computational costs, achieving a new state-of-the-art for clinical text generation.

\subsection{Conditioning Text Generation for User Control}
\label{sec:conditioning-text-generation-for-user-control}
\textbf{Authoring Guidelines.} Augmenting datasets with authoring guidelines significantly improves model performance (cf. Table~\ref{table:main_results}, Fig.~\ref{fig:relative_improvments_heatmap}). BASE w/\textsc{style} ($0.399$, $+10\%$) and BASE w/\textsc{instr} ($0.420$, $+16\%$) outperform BASE ($0.363$), demonstrating the potential of augmenting datasets with explicit guidelines. 

Style guidelines enhance lexical similarity (BLEU, ROUGE, METEOR) with $9$–$21\%$ relative improvements, compensating for BASE’s METEOR deficit. Surprisingly, even semantic and fact-based metrics (BERTScore, AlignScore, MEDCON) improve by $7$–$8\%,$ suggesting either (i) these metrics are sensitive to stylistic variances or (ii) automatically generated style guidelines contain spurious features that reinforce factual  and clinical alignment --- an area requiring further research.

Writing instructions consistently outperform style guidelines and match BASE w/\textsc{topics} on fact-based metrics (AlignScore, MEDCON: $+12\%$), despite the latter being conditioned for and provided with topic-level guidance.

\textbf{Topic Guidance.} Providing LLMs conditioned for topic-level control with topic guidance yields overall improvements ($+11\%$) similar to style guidelines, but with fact-based metrics (AlignScore, MEDCON: $+12\%$) contributing more. 
Although BASE w/\textsc{style} w/\textsc{topics} ($+20\%$) performs slightly worse, integrating both authoring guidelines and topic guidance yields further performance gains across all metrics, showing that these strategies are complementary, and evidencing the need for both style- and  content-aware conditioning. Notably, our best model BASE w/\textsc{instr} w/\textsc{topics} ($+22\%$) excels in DI generation, achieving high ROUGE-1 ($0.612$), BERTScore ($0.587$), and MEDCON ($0.594$) scores. (cf. Table~\ref{table:task_performance}).

\textbf{Conclusion:} Overall, we observe that stylistic and content-related guidance is complementary, and that all metrics, even fact-based ones, appear sensitive to stylistic deviations to different degrees. Furthermore, clear instructions, expressing the purpose of stylistic features and the document clearly outperform simple stylistic descriptions.

\subsection{Human Evaluation Results}
We evaluate BASE w/\textsc{instr} on a sample of 100 discharge summaries using human-written authoring guidelines (cf. Tab.~\ref{table:human_evaluation_results}). For cross-validation, we also assess BASE and BASE w/\textsc{instr} using automated evaluation (Sec.~\ref{sec:automated-evaluation}). Results (cf. Tab.~\ref{table:human_evaluation_results}) indicate that the sampled dataset is not significantly easier than the original test set (BASE: $0.365,$ BASE w/\textsc{instr}: $0.415$) . When prompted with human-written guidelines, BASE w/\textsc{instr} ($0.403$) retained $97\%$ of its performance, while maintaining clinical accuracy \& relevance (MEDCON: $-0.8\%$) and improving factual consistency (AlignScore: $+4\%$), underscoring the promise of adopting LLMs for expert annotations in clinical practice.

Evaluating the topic segmentations (cf. Appendix~\ref{sec:human-evaluation-of-topic-segmentations}) reveals that $91.9\%$ of LLM-generated headings ($\mathring{h}_i^j$) correctly match their corresponding text blocks ($t_i^j$). Similarly, $88.4\%$  of the generated questions ($\mathring{q}_i^j$) are appropriately answered by the text block and effectively inquire it's content. The selected text range ($t_i^j$) is deemed accurate in $75.2\%$ of cases, meaning it accurately aligns with the optimal segment boundaries for the suggested heading $\mathring{h}_i^j$ and question $\mathring{q}_i^j$. These results suggest that expert-level accuracy may already be within reach with stronger models or a secondary validation pass to refine the segmentation, in particular segment boundaries. 

\textbf{Conclusion:} Our findings reinforce the idea that LLMs can effectively act as human proxies, even for complex multi-step tasks like topic segmentation,  bringing automation closer to expert-level performance.