\begin{figure*}[t!]
   \centering
   \includegraphics[width=1.5\columnwidth]{images/relative_improvements_heatmap.pdf}
  \caption{Relative improvement of augmented models against the traditionally instruction-tuned BASE model (cf. Tab.~\ref{table:main_results}).}
  \label{fig:relative_improvments_heatmap}
\end{figure*}



\begin{table*}[t!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{llccccccccc}
\hline
  &  & Overall & BLEU & R-1 & R-2 & R-L & BS & Meteor & AS & MEDCON \\
\hline
\multirow{6}{*}{\rotatebox{90}{\textsc{BHC}}} & BASE  & 0.333 & 0.142 & 0.465 & 0.228 & 0.313 & 0.460 & 0.335 & 0.290 & 0.435 \\
 & \hspace{3mm} w/\textsc{style} & 0.350 & 0.158 & 0.488 & 0.242 & 0.330 & 0.477 & 0.356 & 0.297 & 0.452 \\
 & \hspace{3mm} w/\textsc{instr}  & 0.364 & 0.171 & 0.505 & 0.255 & 0.343 & 0.489 & 0.376 & 0.304 & 0.470 \\
 & \hspace{3mm} w/\textsc{topics}  & 0.356 & 0.149 & 0.487 & 0.239 & 0.333 & 0.471 & 0.369 & 0.317 & 0.482 \\
 & \hspace{3mm} w/\textsc{style} w/\textsc{topics}& 0.385 & 0.178 & 0.524 & 0.268 & 0.367 & 0.504 & 0.396 & \textbf{0.332} & 0.513 \\
& \hspace{3mm} w/\textsc{instr} w/\textsc{topics} & \textbf{0.390} & \textbf{0.183} & \textbf{0.530} & \textbf{0.271} & \textbf{0.370} & \textbf{0.509} & \textbf{0.410} & 0.327 & \textbf{0.517} \\
\hline
\multirow{6}{*}{\rotatebox{90}{\textsc{DI}}} & BASE  & 0.393 & 0.193 & 0.502 & 0.283 & 0.377 & 0.484 & 0.390 & 0.428 & 0.484 \\
 & \hspace{3mm} w/\textsc{style} & 0.448 & 0.247 & 0.565 & 0.337 & 0.434 & 0.539 & 0.452 & 0.469 & 0.538 \\
 & \hspace{3mm} w/\textsc{instr} & 0.476 & 0.277 & 0.589 & 0.366 & 0.464 & 0.565 & 0.480 & 0.503 & 0.560 \\
 & \hspace{3mm} w/\textsc{topics}  & 0.451 & 0.240 & 0.561 & 0.336 & 0.436 & 0.535 & 0.459 & 0.487 & 0.552 \\
 & \hspace{3mm} w/\textsc{style} w/\textsc{topics} & 0.487 & 0.273 & 0.600 & 0.370 & 0.474 & 0.574 & 0.491 & \textbf{0.526} & 0.584 \\
 & \hspace{3mm} w/\textsc{instr} w/\textsc{topics} & \textbf{0.500} & \textbf{0.292} & \textbf{0.612} & \textbf{0.383} & \textbf{0.488} & \textbf{0.587} & \textbf{0.517} & 0.525 & \textbf{0.594} \\
\hline
\end{tabular}
}
\caption{The average scores per metric of our evaluation (Sec.~\ref{sec:conditioning-text-generation-for-user-control}), broken down by the two tasks: discharge instructions (DI) generation and brief hospital course (BHC) generation.}
\label{table:task_performance}
\end{table*}

\begin{table*}[t!]
\centering
\begin{tabular}{lccccccccc}
\hline
 & Overall & BLEU & R-1 & R-2 & R-L & BS & METEOR & AS & MEDCON \\
\hline
BASE & 0.365 & 0.169 & 0.484 & 0.252 & 0.345 & 0.478 & 0.360 & 0.364 & 0.467 \\

\hspace{3mm} w/\textsc{instr} (A) & \textbf{0.415} & \textbf{0.215} & \textbf{0.546} & \textbf{0.303} & \textbf{0.397} & \textbf{0.525} & \textbf{0.419} & 0.400 & \textbf{0.513}  \\
\hspace{3mm} w/\textsc{instr} (H) & 0.403 & 0.193 & 0.524 & 0.288 & 0.390 & 0.514 & 0.392 & \textbf{0.415} & 0.509 \\
\hline
\end{tabular}
\caption{The results of BASE and BASE w/\textsc{instr} evaluated on 100 discharge summaries randomly sampled from the \textit{Discharge Me!} test phase 2 split, once with augmented authoring guidelines (A) and once with human-written authoring guidelines (H).}
\label{table:human_evaluation_results}
\end{table*}

\section{Training Details}
\label{subsec:Training-Details}

We fine-tune \textit{Llama~3~8B~Instruct} on 8~H100 GPUs for 3,000 steps ($\approx 2.8$ epochs) with the AdamW 8\nobreakdash-bit optimizer~\citep{dettmers20228bitoptimizersblockwisequantization} ($\beta = (0.9, 0.999), \epsilon=1e^{-8}$) and a batch size of 128 on completions only. We use gradient clipping with a maximum gradient norm of $1$ and weight decay is set to $1e^{-4}$.  Furthermore, our models are fine-tuned with instruction-tuning on completions only with rank-stabilized LoRA~\citep{kalajdzievski2023rankstabilizationscalingfactor} targeting all linear layers with $\alpha_{\text{LoRA}} = 64$, $\text{dropout}_{\text{LoRA}} = 0.1$, $r_{\text{LoRA}} = 64$, and fast SVD-based PISSA~\citep{meng2024pissaprincipalsingularvalues} with 32 iterations to initialize adapter weights. 

Inspired by~\citet{hu2024minicpmunveilingpotentialsmall}, who proposed to replace linear decay with a cosine cyclic decay to increase the duration of higher learning rates, we adopt a learning rate scheduler with a stable phase of 1,000 steps with learning rate $\alpha_1 = 1e^{-4}$, a decay phase of 1,800 steps corresponding to 0.25 cosine cycles to reduce the learning rate to $\alpha_2 = 5e^{-6}$, and another smoothing decay phase of 200 steps corresponding to the remaining 0.25 cosine cycles to reduce the learning rate to $\alpha_3 = 1e^{-6}$. This increases the duration of high learning rates even further.


\section{Automated Evaluation}
\label{sec:automated-metrics-explanation}
For evaluation, we use the code provided by the \textit{Discharge Me!} challenge, which employs a comprehensive set of metrics to assess lexical and semantic similarity, factual consistency, as well as the clinical relevance and correctness.

The metrics include: \textbf{BLEU-4}~\citep{papineni-etal-2002-bleu}, which measures the precision of four-word sequences (4-grams) in the generated text against reference texts, capturing the overlap of these sequences.
\textbf{ROUGE-1, ROUGE-2, ROUGE-L}~\citep{lin-2004-rouge}, which evaluate the recall of unigrams, bigrams, and the longest common subsequence between the generated and reference texts, indicating the similarity of content.
\textbf{BERTScore}~\citep{zhang2020bertscoreevaluatingtextgeneration}, which uses contextual embeddings from BERT to evaluate the semantic similarity between the generated and reference texts.
\textbf{Meteor}~\citep{banerjee-lavie-2005-meteor}, which considers synonyms and stemming to compare the generated text with reference texts, providing a more flexible measure of similarity. 
\textbf{AlignScore}~\citep{zha-etal-2023-alignscore}, which aligns generated and reference texts to measure the quality of the alignment, reflecting the coherence and consistency of the generation.
\textbf{MEDCON}~\citep{Yim2023AcibenchAN}, which is specifically designed for medical contexts, and evaluates the clinical relevance and correctness of the generated text.

To simulate user control for automated evaluation on the \textit{DischargeMe!} dataset, we employ Llama 3.1 70B Instruct again to automatically generate authoring guidelines (Sec.~\ref{sec:authoring-guidelines}) and provide topic-level control. The LLM serves as proxy for the original authors based on the assumption that their generated output approximates the input and feedback the authors would have provided if they had originally used our methods to write the target texts $t_i$.

While authoring guidelines can be seamlessly incorporated at inference time for \textbf{w/\textsc{style}} and \textbf{w/\textsc{instr}}, simulating granular, interactive topic-level control for \textbf{w/\textsc{topics}}, that iteratively refines model output, is more complex. However, while increased user interaction generally improves output quality, it also amplifies user contribution, making it less reflective of the model's standalone performance. To minimize user contribution, we provide topic guidance indirectly and non-interactively, and effectively establish a lower-bound baseline for performance. 

Specifically, we extend the user prompt $user_i(c,g)$ with an instruction to cover a predefined list of topics (cf. Fig.~\ref{fig:evaluation_prompt_template}). This list is derived from topic segmentations (Sec.~\ref{sec:topic-level-generation-control}) for each target text $t_i$ by extracting and concatenating the headings $\mathring{h}_i^1$, ..., $\mathring{h}_i^n$ into an unnumbered bullet list.


\begin{figure}[t!]
\centering
\begin{tabular}{p{0.9\columnwidth}}
\hline
User Message \\
\hline
\vspace{-1em}
\begin{lstlisting}
{{discharge summary}}
{{radiology report 1}}
         ...
{{radiology report n}}
{{brief hospital course}}
{{authoring guidelines}}
{{instructions}}
{{topics}}
\end{lstlisting} \vspace{-1.5em} \\
\hline
\end{tabular}

\caption{The user prompt used for evaluation.}
\label{fig:evaluation_prompt_template}
\end{figure}




\section{Comparative Analysis with Existing Approaches}
\label{sec:comparative-analysis-with-existing-approaches}


\begin{table*}[ht]
\centering
\begin{tabular}{lccc}
\hline
 & BASE (ours) & WisPerMed & aehrc \\
Score & \underline{\textbf{0.363}} & \textbf{0.253} & \textbf{0.235} \\
\hline
Clinical Context & $ds + rr$ & $ds$ & optimized \\
Models trained & 1 & 2 & 2 \\
Decoding Strategy & greedy & optimized nucleus sampling & 4-beam search \\
\hline
Optimizer & AdamW 8-Bit & AdamW 8-Bit & Adam \\
Epochs & 2.8 & 3 & 5 \\
Batch Size & 128 & 16 & 16 \\
Learning Rate $\alpha$ & $1e^{-4}$ & $2e^{-4}$ & $2e^{-4}$ \\
Weight Decay & $1e^{-4}$  & $1e^{-2}$  & N/A \\
Learning Rate Scheduler & optimized WSD & linear &  linear \\
Warmup Steps & 0 & 5  & 3\% \\
\hline
LoRA Type & rank-stabilized LoRA & LoRA & QLoRA \\
Layers & all linear & all linear & all linear \\
Total Trainable Parameters & 168M & 84M & 336M \\
Weight Initialization & fast SVD-PISSA ($n=32$) & N/A & QLoRA \\
$r_{LoRA}$ & 64 & 16 & 64 \\
$\alpha_{LoRA}$ & 64 & 16 & 16 \\
dropout$_{LoRA}$ & 0.1 & 0 & N/A \\
\hline
\end{tabular}
\caption{Detailed comparison of training configurations, decoding strategies, and scores for instruction-tuned Llama 3 8B Instruct models, highlighting the key differences among our, WisPerMed's and aehrc's approach. $ds$ = Discharge Summary. $rr$ = Radiology Reports. N/A = Not Available.}
\label{tab:method-comparison}
\end{table*}


We present a detailed comparison of our instruction-tuned Llama 3 8B Instruct BASE model against the three top-performing systems on the \textit{DischargeMe!} leaderboard. We also include a detailed comparison of other instruction-tuned Llama 3 8B Instruct models evaluated during experimentation by leaderboard participants but ultimately dropped due to suboptimal performance. Table~\ref{tab:method-comparison} summarizes the primary distinctions across these methods.

\textbf{WisPerMed} \citep{damm-etal-2024-wispermed} achieved the highest leaderboard score of $0.332$, surpassing other submissions by a notable margin. This success was attributed to its Dynamic Expert Selection (DES) strategy, which combines predictions from five instruction-tuned models: Llama 3 8B and 70B Instruct, OpenBioLLM 70B, Mistral 7B Instruct (v0.2), and Phi 3 Mini 128K Instruct.  Notably, the standalone Llama 3 8B Instruct model within this ensemble achieved the lowest score ($0.253$), marginally underperforming the Phi 3 Mini model.

All models in WisPerMed's ensemble were fine-tuned using the entire discharge summary as input and LoRA with a rank of $r_{LoRA} = 16$, applied to all linear layers. In addition, some models were fine-tuned on Asclepius~\citep{kweon-etal-2024-publicly}.  For inference, they employed optimized nucleus sampling to enhance output quality. This ensemble approach enabled WisPerMed to leverage complementary model strengths, albeit at the cost of increased complexity and resource demands.

\textbf{aehrc}~\citep{liu-etal-2024-e}, similarly, fine-tuned the Llama 3 8B Instruct model using LoRA ($r_{\text{LoRA}} = 64$), but introduced notable variations in preprocessing and decoding strategies. Discharge summaries were partitioned into: (1) the text preceding the Brief Hospital Course (BHC) section for BHC generation, and (2) the text between the BHC and Discharge Instructions (DI) sections, joined with the BHC section, for DI generation. This design was motivated by their observation that longer input contexts negatively impacted model performance. They also reported that providing the entire discharge summary, including all radiology reports (as used in our setting), yielded the lowest results. For decoding, aehrc employed a 4-beam search strategy. Their leaderboard submission leveraged PRIMERA~\citep{xiao-etal-2022-primera}, a specialized instruction-tuned summarization model with 447M parameters.

\textbf{HarmonAiLab@Yale}~\citep{socrates-etal-2024-yale} has not experimented with Llama 3 8B Instruct, but GPT-3 and GPT-4 instead. 
They ultimately submitted a fine-tuned clinical model (BioBART-Large, 406M parameters) trained on an extended dataset that reportedly included samples from the validation and phase 1 test splits for a total of 83.475 ($+21.4\%$) training samples for the BHC task. HarmonAiLab@Yale also employed a 4-beam search strategy for generation, but blocking repeats with an n-gram size of 3.

\textbf{All teams} (WisPerMed, aehrc, and HarmonAiLab@Yale) trained separate models for BHC and DI tasks, effectively doubling their total trainable parameter size.

\textbf{In contrast}, we adopt a unified strategy, training a single Llama 3 8B Instruct model to jointly handle both BHC and DI tasks. The input includes the entire discharge summary and all radiology reports. Similar to aehrc, we applied LoRA ($r_{\text{LoRA}} = 64$) to all linear layers during fine-tuning, resulting in a total trainable parameter count of 168M -- double that of WisPerMed but only half that of aehrc when comparing the fine-tuned Llama 3 8B Instruct models (rather than final submissions). For decoding, we employed a greedy decoding strategy. See Table~\ref{tab:method-comparison} for a more detailed comparison.

Despite the less favorable input context and decoding strategy, our model achieved a leaderboard score of $0.363$ --- a 43\% improvement over WisPerMed's attempts, and a 54\% improvement over aehrc's attempts with instruction-tuned Llama 3 8B Instruct models (cf. Tab.~\ref{tab:method-comparison}). Moreover, our method~(168M) has significantly fewer total trainable parameters than the final submissions of WisPerMed~(1.046B), HarmonAiLab@Yale's~(812M) and aehrc's~(894M), requires less training~(2.8~epochs) than WisPerMed~(3~epochs) and aehrc~(5~epochs), and no additional data~(WisPerMed), nor an extended dataset~(HarmonAiLab@Yale). Considering all individual training setups, our training also requires only $56\%$ of Yale’s compute budget, $23\%$ of aehrc’s, and $32\%$ of WisPerMed’s

The results underscore the efficiency and effectiveness of our approach, demonstrating that instruction-tuning a single general-purpose model can achieve state-of-the-art performance without the complexity of ensembles or reliance on domain-specific models and architectures.



\section{Topic Segmentation Post-Processing}
\label{sec:topic-segmentation-post-processing}


\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\hline
  & Overall & BLEU & R-1 & R-2 & R-L & BS & Meteor & AS & MEDCON \\
\hline
Llama 3.3 70B Instruct (0-shot) & 0.175 & 0.012 & 0.286 & 0.069 & 0.142 & 0.209 & 0.205 & 0.242 & 0.239 \\
\hspace{3mm} w/\textsc{style} & 0.184 & 0.014 & 0.300 & 0.070 & 0.148 & 0.231 & 0.220 & 0.243 & 0.246 \\
\hspace{3mm} w/\textsc{instr} & 0.210 & 0.021 & 0.333 & 0.087 & 0.170 & 0.276 & 0.246 & 0.269 & 0.279 \\
\hspace{3mm} w/\textsc{topics} & 0.226 & 0.027 & 0.352 & 0.095 & 0.183 & 0.292 & 0.283 & 0.260 & 0.320 \\
\hspace{3mm} w/\textsc{style} w/\textsc{topics} & 0.225 & 0.027 & 0.348 & 0.094 & 0.182 & 0.291 & 0.281 & 0.259 & 0.319 \\
\hspace{3mm} w/\textsc{instr} w/\textsc{topics} & 0.230 & 0.027 & 0.352 & 0.097 & 0.185 & 0.296 & 0.287 & 0.270 & 0.322 \\
\hline
Llama 3.3 70B Instruct (3-shot) & 0.210 & 0.025 & 0.336 & 0.094 & 0.177 & 0.281 & 0.232 & 0.246 & 0.285 \\
\hspace{3mm} w/\textsc{style} & 0.215 & 0.027 & 0.342 & 0.097 & 0.183 & 0.296 & 0.235 & 0.254 & 0.289 \\
\hspace{3mm} w/\textsc{instr} & 0.223 & 0.029 & 0.345 & 0.104 & 0.193 & 0.317 & 0.237 & 0.259 & 0.299 \\
\hspace{3mm} w/\textsc{topics} & 0.227 & 0.027 & 0.352 & 0.095 & 0.183 & 0.292 & 0.283 & 0.261 & 0.321 \\
\hspace{3mm} w/\textsc{style} w/\textsc{topics} & 0.225 & 0.027 & 0.348 & 0.094 & 0.182 & 0.291 & 0.281 & 0.259 & 0.319 \\
\hspace{3mm} w/\textsc{instr} w/\textsc{topics} & 0.230 & 0.028 & 0.352 & 0.097 & 0.185 & 0.296 & 0.287 & 0.271 & 0.322 \\
\hline
\end{tabular}
}
\caption{The average scores per metrics for our evaluations, broken down by the two tasks: discharge instructions (DI) generation and brief hospital course (BHC) generation.}
\label{table:task_performance_llama}
\end{table*}


This step is applied only when the segmentation introduces minor alterations to the original text to avoid introducing inconsistencies between headings, questions, and text blocks through such replacements.  To achieve this, we use diff methods to identify word-level differences --- defined as whitespace-delimited character sequences --- between the generated text $\mathring{t}_i = (\mathring{t}_i^1, \dots, \mathring{t}_i^n)$ and original text $t_i$. Segmentations containing consecutive differences are then filtered out, ensuring that segmentations involving only minor differences, such as the spelling or formatting, are considering for this post-processing step. This leaves us with 93.61\% of the DI, and 81.15\% of the BHC segmentations, whose blocks $t_i^k$ are then mapped back to the original text $t_i$ to retrieve the original character sequences corresponding to each block.


\section{Human Evaluation of Topic Segmentations}
\label{sec:human-evaluation-of-topic-segmentations}

\begin{table*}[ht]
\centering
\begin{tabular}{l|ccc}
\hline
 & DS + RRs & DI & BHC  \\
\hline
\texttt{\#tokens} & 3883.76 ($\pm$ 2262.69) & 278.68  ($\pm$ 220.91) & 525.05 ($\pm$ 386.98) \\
\texttt{\#tokens(SG)} & --- & 330.62 ($\pm$ 34.51) & 325.54 ($\pm$ 29.73) \\
\texttt{\#tokens(WI)} & --- & 470.58 ($\pm$ 42.21) & 460.46 ($\pm$ 37.19) \\
\hline
\end{tabular}
\caption{Averages (and standard deviations) of token counts  for various quantities of the augmented DischargeMe! training split. Abbreviations: SG = Style Guidelines. WI = Writing Instructions. DS = Discharge Summary. RRs = Radiology Reports. DI = Discharge Instructions. BHC = Brief Hospital Course.}
\label{table:dataset_statistics}
\end{table*}

\begin{table}[ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc}
\hline
 & DI & BHC  \\
\hline
\texttt{\#segments} & 6.25  ($\pm$ 2.09) & 8.25 ( $\pm$ 4.03) \\
\texttt{\#tokens($\mathring{h}_i^k$)} & 3.82  ($\pm$ 2.00) & 4.65  ($\pm$ 2.95) \\
\texttt{\#tokens($\mathring{q}_i^k$)}  & 9.60  ($\pm$ 2.64) & 11.46 ($\pm$ 2.97) \\
\texttt{\#tokens($t_i^k$)}  & 44.57 ($\pm$ 46.16 )& 62.51 ($\pm$ 61.43) \\

\hline
\end{tabular}
}
\caption{Averages (and standard deviations) of various quantities of topic segmentations for DI and BHC sections of the DischargeMe! training split. The statistics for the number of segments \texttt{\#segments} and the token counts \texttt{\#tokens($\cdot$)} of headings $\mathring{h}_i^k$, questions $\mathring{q}_i^k$ and text blocks $t_i^k$ are consistently larger for BHC sections.}
\label{table:topic_segmentations_statistics}
\end{table}

We conducted a human evaluation of the topic segmentations (Sec.~\ref{sec:topic-level-generation-control}) generated using Llama 3.1 70B Instruct. Specifically, 500 DI and BHC sections were randomly sampled from the post-processed subset of the training split of the \textit{DischargeMe!} dataset, resulting in a total of 1000 target texts $t_i$. For each $t_i$, one segment $seg_i^j = (h_i^j, q_i^j, t_i^j)$ was randomly selection for assessment. A human expert then evaluated the selected segment through a two-step process, details in Section~\ref{sec:human-evaluation-of-topic-segmentations}.

\textbf{Step 1.} The expert was presented with the target text $t_i$, where the text range from the start of the selected text block $t_i^j$ to the end of $t_i$ was highlighted. The expert was instructed to annotate the next topic beginning within the highlighted range by identifying the heading, question, and corresponding text block. This step ensured that the expert interacted thoroughly with the target text and independently assessed and annotated the next segment without being influenced by the LLM-generated output.

\textbf{Step 2.} The expert was then provided with the LLM-generated annotation $\mathring{seg}_i^j$, which included the heading $\mathring{h}_i^j$, question $\mathring{q}_i^j$, and text block $t_i^j$. The expert evaluated the appropriateness of the generated heading, the quality of the question, and the accuracy of the text block boundaries.  While the expert could refer to their own annotations for comparison, they were instructed to assess the LLM-generated segment for correctness without imposing personal preferences, given the inherent subjectivity of the topic segmentation task and the existence of multiple competing solutions. 

The heading $\mathring{h}_i^j$ was considered appropriate only if it effectively encapsulated the content and focus of the corresponding text block $t_i^j$. The question $\mathring{q}_i^j$ was considered high quality only if it was directly answerable by the selected text block $t_i^j$ and accurately reflected and inquired the central issue or information addressed within that range.

For evaluating the text range, the expert was tasked with envisioning the optimal segment boundaries, aligning with both the heading $\mathring{h}_i^j$ and the Question Under Discussion (QUD) $\mathring{q}_i^j$, within the entire target text $t_i$. The text range was considered accurate only when the start and end points coincided with the hypothesized segment boundaries.

\textbf{Results} The evaluation revealed that the LLM-generated headings ($\mathring{h}_i^j$) aligned with the corresponding text blocks ($t_i^j$) in the majority of cases ($91.9\%$). Similarly, the generated questions ($\mathring{q}_i^j$) were well-formulated in $88.4\%$ of instances, effectively inquiring about the content of the text block and being answerable by it. In $87.5\%$ cases, both the heading and question was deemed appropriate. The accuracy of the selected text range ($t_i^j$) was confirmed in $75.2\%$ of all cases. Notably, in instances where both the headings and questions were appropriate, the accuracy of the text ranges increased to $80.91\%$.



\section{Dataset \& Annotation Statistics, Prompts and Examples}
In this section, we present examples of data augmentations, showcasing annotation samples alongside corresponding LLM prompts.
Table~\ref{table:dataset_statistics} summarizes token length statistics for the DischargeMe! training split. We find that style guidelines and writing instructions have similar average lengths across tasks (DI vs. BHC), but writing instructions are nearly 1.5× longer than style guidelines. Additionally, BHC sections are, on average, twice as long as DI sections, and BHC topic segmentations consistently contain slightly more segments, longer headings, and extended text blocks, as detailed in Table~\ref{table:topic_segmentations_statistics}.
 
\begin{table*}[ht]
\centering
\vspace{-3em}
\resizebox{0.96\textwidth}{!}{
\begin{tabular}{p{\textwidth}}
\hline
 \textbf{Synthetic Clinical Document} \\
\hline
\textcolor{MutedCoral}{The 75-year-old male patient with multi-organ sarcoidosis, diabetes mellitus, and chronic renal failure was admitted due to fatigue, dyspnea, lower limb edema, and pain.}
\textcolor{SeafoamGreen}{He received corticosteroid therapy for two years but experienced a bloodstream infection caused by Pseudomonas aeruginosa, which was successfully treated with levofloxacin. The patient's dosage of methylprednisolone was increased, leading to him being transferred to ICU and intubated due to worsening functional status.}
\textcolor{DustyBlue}{He was diagnosed with Candida albicans on Day +3 and started antifungal therapy with fluconazole (400 mg daily) and then later found to have disseminated cryptococcal disease on Day +5, leading to antifungal therapy with liposomal amphotericin B (80 mg daily).}
\textcolor{GoldenSand}{Unfortunately, the patient died from septic shock on Day +10. The laboratory findings indicated lymphocytopenia of 900 cells/µL, creatinine of 1.73 mg/dL, C-reactive protein of 83 mg/L, procalcitonin of 2.5 ng/L, increased C-reactive protein to 160 mg/L, increased procalcitonin to 14 ng/mL, and serum positive titers for CrAg ($\geq 1:4096$).}
\textcolor{BlushPink}{The diagnostic findings included pulmonary infiltration with lymphadenopathy, multiple nodules within the lung parenchyma, and disseminated cryptococcal disease.}
\textcolor{TealBreeze}{The treatment consisted of broad-spectrum antibiotic therapy with meropenem and teicoplanin, antifungal therapy with fluconazole (Day +3), and antifungal therapy with liposomal amphotericin B (Day +6).}
\textcolor{MistyLavender}{There is no follow-up information available.} 
\\
\hline
\textbf{Topic Segmentation Prompt}\\
\hline
You are tasked with fine-grained topic segmentation. Given this formatted text, segment the paragraphs into as many short blocks as sensible, each with a distinct topic. Give each block a meaningful, short topic heading, summarizing the most important information from the beginning of the block for the intended audience, and a subtitle, which reformulates the topic as a question that is answered by the block.\newline Guidelines:\newline  - Segment everything from the very first to the very last word/character/symbol.\newline - Terminate spans and insert new headings, whenever the upcoming text does not match the current running topic anymore, e.g. whenever the medical, clinical or healthcare focus changes.\newline - When formulating questions, do not use pronouns as the subjects, and do not use possessive pronouns.\newline - Do not alter the text. Copy typos, errors, mistakes and formatting from the original text.\newline  - Include headings, symbols, separators, vertical/horizontal spacing, empty lines and other formattings with their associated blocks.\newline Answer format: '<split-text>\textbackslash n\allowbreak<topic>...\allowbreak</topic>\textbackslash n\allowbreak<question>...\allowbreak</question>\textbackslash n\allowbreak<span>...\allowbreak</span>\textbackslash n\textbackslash n\allowbreak<topic>...\allowbreak</topic>\textbackslash n\allowbreak<question>...\allowbreak</question>\textbackslash n\allowbreak<span>...\allowbreak</span>\textbackslash n\textbackslash n\allowbreak...\allowbreak</split-text>' \\
\hline
 \textbf{Segmented Text} \\
\hline
\textbf{<topic>}Initial Patient Information\textbf{</topic>}

\textbf{<question>}What is the patient's initial condition and medical history?\textbf{</question>}

\textbf{<span>}\textcolor{MutedCoral}{The 75-year-old male patient with [...] fatigue, dyspnea, lower limb edema, and pain.}\textbf{</span>}

\textbf{<topic>}Initial Treatment and Complications\textbf{</topic>}

\textbf{<question>}What treatment did the patient receive initially, and what complications arose?\textbf{</question>}

\textbf{<span>}\textcolor{SeafoamGreen}{He received corticosteroid therapy for  [...] intubated due to worsening functional status.}\textbf{</span>}

\textbf{<topic>}Antifungal Therapy and Diagnosis\textbf{</topic>}

\textbf{<question>}What antifungal therapy was administered, and what diagnoses were made?\textbf{</question>}

\textbf{<span>}\textcolor{DustyBlue}{He was diagnosed with Candida albicans [...] liposomal amphotericin B (80 mg daily).}\textbf{</span>}

\textbf{<topic>}Outcome and Laboratory Findings\textbf{</topic>}

\textbf{<question>}What was the patient's outcome, and what laboratory findings were reported?\textbf{</question>}

\textbf{<span>}\textcolor{GoldenSand}{Unfortunately, the patient died from septic shock [...] positive titers for CrAg ($\geq 1:4096$).}\textbf{</span>}

\textbf{<topic>}Diagnostic Findings\textbf{</topic>}

\textbf{<question>}What diagnostic findings were reported?\textbf{</question>}

\textbf{<span>}\textcolor{BlushPink}{The diagnostic findings included [...] parenchyma, and disseminated cryptococcal disease.}\textbf{</span>}

\textbf{<topic>}Treatment Summary\textbf{</topic>}

\textbf{<question>}What treatment did the patient receive?\textbf{</question>}

\textbf{<span>}\textcolor{TealBreeze}{The treatment consisted of broad-spectrum [...] liposomal amphotericin B (Day +6).}\textbf{</span>}

\textbf{<topic>}Follow-up Information\textbf{</topic>}

\textbf{<question>}Is there any follow-up information available?\textbf{</question>}

\textbf{<span>}\textcolor{MistyLavender}{There is no follow-up information available.}\textbf{</span>}\\
\hline
\end{tabular}
}

\caption{Topic segmentation of our framework of an arbitrary clinical document retrieved from the synthetic Asclepius dataset~\cite{kweon-etal-2024-publicly} for demonstration purposes, as the \textit{DischargeMe!} dataset cannot be used directly due to privacy restrictions and access limitations. The control sequences to initiate and stop generation of specific elements are indicated in bold.}
\label{table:topic_segmentation_annotation_example}
\end{table*}



\begin{table*}[ht]
\centering
\resizebox{0.96\textwidth}{!}{
\begin{tabular}{p{\textwidth}}

\hline
\textbf{Prompt for Style Guideline Generation}\\
\hline
<text>\texttt{\{\{target text\}\}}</text>

Describe the text's tone, writing style, document format, layout, composition, textual structure, use of language, use of abbreviations, use of medical jargon, the intendened audience and anything else noteworthy. Write full sentences and paragraphs.\newline \newline Guidelines:\newline - Do not use the terms from the text.\newline  - Do not quote the text.\newline - Do not give examples from the text.\newline - Do not reveal details about the patient.\\
\hline
\textbf{Style Guideline}\\
\hline
The original text's tone is formal, objective, and neutral, indicating a professional and clinical approach to documenting a patient's medical history and treatment. The writing style is concise, direct, and informative, suggesting a focus on conveying complex medical information in a clear and efficient manner.

The document format appears to be a medical case report or a patient summary, likely used for educational, research, or clinical purposes. The layout is structured and organized, with a logical flow of information that facilitates easy comprehension. The composition is dense and detailed, indicating a high level of technical expertise and a focus on accuracy.

The textual structure is chronological, with events and treatments presented in a linear sequence. This structure allows the reader to follow the patient's progression and the corresponding medical interventions. The use of language is technical and specialized, with a high degree of medical terminology and jargon. This suggests that the intended audience is composed of medical professionals or individuals with a strong background in healthcare.

The use of abbreviations is frequent, indicating a familiarity with medical shorthand and a desire to convey complex information concisely. The text assumes a high level of prior knowledge in medicine, with minimal explanation of technical terms or concepts. This suggests that the intended audience is composed of specialists or experts in the field.

The text's use of medical jargon and technical terminology creates a sense of authority and expertise, but may also limit its accessibility to non-medical readers. The language is precise and formal, with a focus on conveying complex information in a clear and concise manner.

Overall, the text appears to be written for an audience of medical professionals, researchers, or students in the healthcare field. The tone, writing style, and use of language all suggest a focus on conveying complex medical information in a clear and efficient manner, with a high degree of technical expertise and accuracy.
\\
\hline
\end{tabular}
}

\caption{The Style Guideline generated for the synthetic clinical document from Tab.~\ref{table:topic_segmentation_annotation_example}, constituting of a list of descriptions of stylistic features.}
\label{table:style_guideline_example}
\end{table*}


\begin{table*}[ht]
\centering
\vspace{-3em}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{p{1.1\textwidth}}
\hline
\textbf{Prompt for  Writing Instructions Generation}\\
\hline
<text>\texttt{\{\{target text\}\}}</text>

Please provide detailed and comprehensive writing instructions for a non-specialist to exactly reproduce the text above. The instructions should include details on:\newline - the purpose and intent of the text, including how it is achieved\newline  - the intended audience, including how the audience's needs are met\newline  - the tone of text, including how to achieve it\newline  - the text structure and outline\newline  - the text disposition\newline  - the text formatting (not typographical), such as (but not only) the use of paragraphs, subheadings, introductions, closings, bullet points, list, including any apparent rules and patterns\newline  - the use of language, including the use of abbreviations and medical jargon (if it is used), with respect to the audience\newline  - and any other noteworty features.\newline\newline Guidelines:\newline - Use an instructive tone for writing.\newline - Consider that the non-specialist will not see the original text.\newline  - Do not use the terms from the text.\newline  - Do not quote the text.\newline  - Do not give examples from the text.\newline  - Do not reveal details about the patient.\newline\newline Answer format: '\#\# Writing Instructions\textbackslash n\textbackslash n...'
\\
\hline
 \textbf{Writing Instructions} \\
\hline
\textbf{\#\# Writing Instructions}

\textbf{\#\#\# Purpose and Intent}

The purpose of this text is to document a patient's medical history, diagnosis, treatment, and outcome in a clear and concise manner. The intent is to provide a comprehensive record of the patient's case, which can be used for medical reference, education, or research purposes. To achieve this, focus on presenting the information in a logical and chronological order, using specific details and medical terminology.

\textbf{\#\#\# Intended Audience}

The intended audience appears to be medical professionals, researchers, or students who require in-depth information about the patient's case. To meet the audience's needs, use technical language and assume a certain level of medical knowledge. However, avoid using overly complex terminology or jargon that might be unfamiliar to non-specialists in the field.

\textbf{\#\#\# Tone}

The tone of the text is formal, objective, and informative. To achieve this tone, maintain a neutral and detached perspective, focusing on presenting the facts of the case without emotional or personal commentary. Use a professional and respectful language throughout the text.

\textbf{\#\#\# Text Structure and Outline}

The text follows a chronological structure, starting with the patient's admission and progressing through their diagnosis, treatment, and outcome. The outline appears to be:

1. Introduction: Brief overview of the patient's condition and admission

2. Medical history: Description of the patient's pre-existing conditions and previous treatments

3. Diagnosis: Presentation of the patient's symptoms, laboratory findings, and diagnostic results

4. Treatment: Description of the treatments administered, including medications and therapies

5. Outcome: Report of the patient's outcome, including any complications or results

\textbf{\#\#\# Text Disposition}

The text is written in a straightforward and factual style, with no apparent attempt to persuade or entertain the reader. The focus is on conveying information in a clear and concise manner.

\textbf{\#\#\# Text Formatting}

The text appears to be written in a single block of paragraphs, with no subheadings, bullet points, or lists. The paragraphs are relatively short, with each one focusing on a specific aspect of the patient's case. To replicate this format, use a single block of text, dividing it into paragraphs as needed to separate distinct sections of information.

\textbf{\#\#\# Language and Terminology}

The text uses medical terminology and abbreviations, assuming a certain level of familiarity with medical concepts. To replicate this, use technical language and abbreviations commonly used in medical contexts. However, avoid using overly complex or specialized terminology that might be unfamiliar to non-specialists in the field.

\textbf{\#\#\# Noteworthy Features}

The text includes specific details about the patient's laboratory findings, medications, and treatments, which suggests a focus on accuracy and precision. To replicate this, include specific details and data wherever possible, using precise language and terminology.\\
\hline
\end{tabular}
}

\caption{The Writing Instructions generated for the synthetic clinical document from Tab.~\ref{table:topic_segmentation_annotation_example} alongside the respective prompt. Writing Instructions are structured and more comprehensive than Style Guidelines. They also feature a slight instructional tone.}
\label{table:writing_instructions_example}
\end{table*}