\section{Datasets}\label{app:datasets}
\subsection{Multi-hop QA Datasets}\label{app:multi_hop_qa}
Three multi-hop QA datasets are used in our experiments: HotpotQA \cite{yang2018hotpotqa}, MuSiQue \cite{trivedi2022musique}, and 2WikiMultiHopQA (2Wiki) \cite{ho2020constructing}. We provide a brief overview of these datasets below.
\begin{itemize}
    \item HotpotQA \cite{yang2018hotpotqa} is a multi-hop QA dataset that requires reasoning over multiple documents to answer questions. The dataset consists of 97k question-answer pairs, where each question is associated with up to 2 supporting and several distracting documents. The questions are designed to be answerable using multiple pieces of information from the supporting documents.
    \item MuSiQue \cite{trivedi2022musique} is a challenging multi-hop QA dataset with 25k 2-4 hop questions. It requires coherent multi-step reasoning to answer questions that span multiple documents. 
    \item 2WikiMultiHopQA (2Wiki) \cite{ho2020constructing} is a multi-hop QA dataset that requires reasoning over multiple Wikipedia articles to answer questions. The dataset consists of 192k questions, which are designed to be answerable using information from 2 or 4 articles.
\end{itemize}
In experiments, we follow existing methods \cite{trivedi2023interleaving,gutiérrez2024hipporag} to use the same 1,000 samples from each validation set and merge the candidate passages as the document corpus for KG-index construction, whose statistics are presented in \Cref{tab:test_data}.

\subsection{Domain-specific RAG Datasets}\label{app:domain_specific}
To test the generalizability of \ourmethod, we evaluate it on seven domain-specific RAG datasets \cite{friel2024ragbench} including, (1) \emph{biomedical}: PubMedQA \cite{jin-etal-2019-pubmedqa}; (2) \emph{customer support}: DelucionQA \cite{sadat-etal-2023-delucionqa}, TechQA \cite{castelli-etal-2020-techqa}, ExpertQA \cite{malaviya2023expertqa}, EManual \cite{nandy-etal-2021-question-answering}; (3) \emph{general knowledge}: MS Marco \cite{nguyen2016ms}, HAGRID \cite{kamalloo2023hagrid}. We provide a brief overview of these datasets below.

\begin{itemize}
    \item PubMedQA \cite{jin-etal-2019-pubmedqa} is a collection of PubMed research abstracts with corresponding questions paired with 4 abstract chunks.
    \item  DelucionQA \cite{sadat-etal-2023-delucionqa} is a domain-specific RAG dataset leveraging Jeep’s 2023 Gladiator model manual as the source of knowledge, where each question is associated with 4 context documents and only 1 relevant passage.
    \item TechQA \cite{castelli-etal-2020-techqa} is a collection of real-world user questions posted on IBMDeveloper and DeveloperWorks forums, along with 10 technical support documents relating to each question.
    \item ExpertQA \cite{malaviya2023expertqa} is a collection of curated questions from domain experts in various fields of science, arts, and law. The dataset also contains expert-curated passages relevant to each question.
    \item EManual \cite{nandy-etal-2021-question-answering} is a question-answering dataset comprising consumer electronic device manuals and realistic questions about them composed by human annotators, where each question is related with up to 3 context documents.
     \item MS Marco \cite{nguyen2016ms} is an open-domain question-answering dataset sourced from Bing search engine user query logs. Each question is associated with 10 context passages retrieved via Bing web search.
    \item HAGRID \cite{kamalloo2023hagrid} is a multi-lingual information retrieval dataset with questions and passages from MIRACL \cite{zhang2022making}.
\end{itemize}
In experiments, we use test sets constructed by RAGBench \cite{friel2024ragbench} and merge all the candidate passages as document corpus for KG-index construction. The statistics of the test dataset are detailed in \Cref{tab:test_data}.

\input{tables/test_data}

\section{Baselines}\label{app:baselines}
In experiments, we compare with several widely used retrieval methods under three categories: (1) \emph{single-step naive methods}: BM25 \cite{robertson1994some}, Contriever \cite{izacardunsupervised}, GTR \cite{ni2022large}, ColBERTv2 \cite{santhanam2022colbertv2}, RAPTOR \cite{sarthiraptor}, Proposition \cite{chen-etal-2024-dense}; (2) \emph{graph-enhanced methods}: LightRAG \cite{guo2024lightrag}, HippoRAG \cite{gutiérrez2024hipporag}; (3) \emph{multi-step methods}: IRCoT \cite{trivedi2023interleaving}. The detailed introduction of the baselines is as follows.

\noindent\textbf{Single-step Naive Methods} are widely adopted in real-world applications due to their great efficiency and generalizability. 
\begin{itemize}
    \item BM25 \cite{robertson1994some} is a classic information retrieval method based on the probabilistic model that ranks a set of documents based on the query terms frequency appearing in each document.
    \item Contriever \cite{izacardunsupervised} trains a dense retriever with contrastive learning on a large-scale corpus to retrieve relevant documents for a given query.
    \item GTR \cite{ni2022large} develops a scale-up T5-based dense retriever that could generalize across different datasets and domains.
    \item ColBERTv2 \cite{santhanam2022colbertv2} is a state-of-the-art dense retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the retrieval quality.
    \item RAPTOR \cite{sarthiraptor} is an LLM-augmented retriever that recursively embeds, clusters, and summarizes chunks of text, constructing a tree with differing levels of summarization to enable accurate retrieval.
    \item Proposition \cite{chen-etal-2024-dense} enhances the performance of dense retrievers by leveraging LLMs to generate a natural language proposition that captures the essential information of the document.
\end{itemize}

\noindent\textbf{Graph-enhanced Methods} design a retriever that is built upon a graph structure to conduct effective retrieval and reasoning.
\begin{itemize}
    \item LightRAG \cite{guo2024lightrag} is an innovative graph-enhanced RAG method that incorporates graph structures into text indexing and retrieval, enabling efficient retrieval of entities and their relationships. It employs a dual-level retrieval system to gather both low-level and high-level knowledge for LLM generation.
    \item HippoRAG \cite{gutiérrez2024hipporag} is a state-of-the-art, training-free graph-enhanced retriever that uses the Personalized PageRank algorithm to assess entity relevance to a query and performs multi-hop retrieval on a document-based knowledge graph. It can be directly applied to various datasets.
\end{itemize}

\noindent\textbf{Multi-step Methods} are designed to conduct multi-hop reasoning by iteratively retrieving and reasoning over documents, which can be integrated with arbitrary retrieval methods.
\begin{itemize}
    \item IRCoT \cite{trivedi2023interleaving} is a powerful multi-step retrieval pipeline that integrates the retrieval with the chain-of-thought (CoT) reasoning of LLMs. It guides the retrieval with CoT and in turn using retrieved documents to improve CoT. IRCoT can be compatible with arbitrary retrievers to conduct multi-step retrieval and reasoning.
\end{itemize}

\section{Implementations and Training Details}\label{app:implementation}

\subsection{Training Data Construction}\label{app:kg_index}
We extract 60,000 samples from the training set of HotpotQA, MuSiQu, and 2Wiki to construct KG-indexes and conduct large-scale training. Specifically, we merge the candidate passages as the document corpus. In the KG-index construction, we use the GPT-4o-mini \cite{gpt4o} with the OpenIE prompts described in HippoRAG \cite{gutiérrez2024hipporag} to extract the entities, relations, and triples from the document corpus. Then, we use the ColBERTv2 \cite{santhanam2022colbertv2} to conduct the entity resolution by computing the similarity between entities as
\begin{equation}
    s(e_i, e_j) = \text{Emb.}(e_i)^{\top} \text{Emb.}(e_j), 
\end{equation}
where a new triple $(e_i, \texttt{equivalent}, e_j)$ is generated if $s(e_i, e_j) > \tau$ and $e_i \neq e_j$. We set the threshold $\tau$ as 0.8 in our experiments. We divide the samples into groups of approximately 1k questions and 10k documents each to control the constructed KG-index size. In the end, we obtain 60 different KG-indexes and associated question-document pairs for model training.

\subsection{Model Settings}\label{app:model_settings}
In \ourmethod, the GFM is implemented as a 6-layer query-dependent GNN with the hidden dimension of 512, DistMult message function, and sum aggregation. The relation update function $g^{l}(\cdot)$ is implemented as a 2-layer MLP. We use the all-mpnet-v2 as the sentence embedding model with a dimension of 768. The total training parameters of the GFM is 8M. In the retrieval stage, we select top $T=20$ entities for the document ranker.

\subsection{Training Settings}\label{app:training_settings}
In the unsupervised KG completion pre-training, the GFM is trained on the mixture of 60 constructed KG-indexes for 30,000 steps. Then, we conduct the supervised document retrieval fine-tuning on the labeled question-document pairs for 5 epochs. The weight $\alpha$ between losses is set to 0.3. We use AdamW optimizer, learning rate of 5e-4 with batch sizes of both training stages set to 4. Each batch contains only one KG-index and training samples associated to it, where we randomly sample from different KG-indexes during training. The model is trained on 8 NVIDIA A100s (80G) with 14 hours pre-training and 5 hours fine-tuning. The detailed settings are summarized in \Cref{tab:settings}.

\input{tables/settings}

\section{Additional Experiments}\label{app:additional_experiments}

\input{tables/text_emb.tex}
\subsection{Effectiveness of Different Sentence Embeddings}\label{app:text_embeddings}

In this section, we study the effectiveness of different sentence embeddings in the GFM. We compare the all-mpnet-v2 \cite{all-mpnet-v2}, bge-large-en \cite{bge_embedding}, gte-Qwen2-1.5B-instruct and gte-Qwen2-7B-instruct \cite{li2023towards} as well as NV-Embed-v2 \cite{lee2024nv}. We download the official pre-trained model from the Huggingface\footnote{\url{https://huggingface.co/}}. The details of the models are shown in \Cref{tab:text-emb}. From the results, we can  observe that the performance variance between different sentence embeddings is relatively small, where the all-mpnet-v2 achieves the best performance with respect to 3 metrics. This indicates that \ourmethod is not sensitive to the choice of sentence embedding models. In experiments, we use the all-mpnet-v2 as the default sentence embedding model due to its efficiency. However, it has relative smaller context-size (512) which limits the length of input text. We leave the exploration of larger context-size sentence embedding models (e.g., NV-Embed-v2 with 32k context) for future work.

\input{tables/pre-train.tex}
\subsection{Effectiveness of Different Training Strategies}\label{app:pre-training}
In this section, we study the effectiveness of the two training tasks used in \ourmethod. We compare the performance by only conducting the unsupervised KG completion pre-training (\ourmethod $w/o$ Fine-tune) and supervised document retrieval fine-tuning (\ourmethod $w/o$ Pre-train). The results are shown in \Cref{tab:pre-train}. The results show that removing the supervised document retrieval fine-tuning significantly decreases the performance of \ourmethod. This highlights the importance of supervised fine-tuning, as it enables the model to understand users' queries and better capture the relevance between questions and knowledge for improved retrieval. Meanwhile, the unsupervised KG completion pre-training also plays a crucial role in enhancing the model's reasoning ability by learning the knowledge graph structure and reasoning patterns from the large-scale triples in KG-indexes.

\input{tables/loss_weight.tex}
\subsection{Effectiveness of Loss Weights}\label{app:loss_weight}
In this section, we examine the effectiveness of the weights assigned to the BCE loss and ranking loss in training \ourmethod. We compare performance by varying the weight $\alpha$ between the two losses: $\gL=\alpha\gL_{\text{BCE}} + (1-\alpha)\gL_{\text{RANK}}$, with results presented in \Cref{tab:loss_weight}. The findings indicate that using only either the BCE loss or ranking loss leads to suboptimal performance ($\alpha=0~\text{or}~1$). The best performance occurs when $\alpha$ is set to 0.3, which aligns with previous studies \cite{lin2024understanding} suggesting that a smaller weight for BCE loss is preferable when positive samples are rare in the training data.

\subsection{Model Transferability}\label{app:trans}
\input{tables/transferability}
In this section, we evaluate \ourmethod's transferability by fine-tuning on the training split of each domain-specific dataset. As shown in \ref{tab:trans}, \ourmethod performs well in zero-shot generalization, with further improvements achieved through fine-tuning. This highlights its transferability when adapted to domain-specific datasets.

\input{tables/model_size.tex}
\input{figures/scaling_separate.tex}
\subsection{Details of Model Neural Scaling}\label{app:scaling}
In this section, we provide more details on the neural scaling experiments. We evaluate the changes of the model performance with respect to different parameter sizes and training data sizes. In \ourmethod, the model parameter sizes are primarily influenced by the hidden dimension of the GFM. Thus, we vary the dimension from 32 to 512 which results in the model parameter sizes ranging from 0.08M to 8M. The detailed settings are shown in \Cref{tab:model_size}. We test models with different sizes on different scales of training data ranging from 3k to 45k samples. We separately report the fitted trend line of performance changing with model parameter size and training data size in \Cref{fig:scaling_separate}. From the trend line, we can observe that the performance of \ourmethod increases with the model parameter size and training data size. Meanwhile, with the larger model parameter size a larger training data size is required to achieve the best performance. This indicates that the performance of \ourmethod can be further improved by scaling up the model size and training data simultaneously.

\input{figures/vis_hops.tex}
\subsection{Visualization of the Distribution of Multi-hop Prediction}\label{app:multi_hop}
In this section, we visualize the distribution of the number of hops in the multi-hop reasoning process of \ourmethod. We calculate the number of hops in the ground-truth reasoning path required for each question in the test set of HotpotQA, MuSiQue, and 2Wiki. Then, we compare the distribution of the number of hops in the reasoning path of the ground-truth and the predicted reasoning path by \ourmethod as well as HippoRAG.
%
The results are shown in \Cref{fig:vis_hops}. We can observe that the distribution of \ourmethod is closely aligned to the ground-truth, which indicates that \ourmethod can effectively conduct the multi-hop reasoning within a single step. Meanwhile, the distribution of HippoRAG is relatively different from the ground-truth, especially in 2Wiki dataset. This indicates that HippoRAG may not be able to effectively capture the complex relationship to conduct multi-hop reasoning on graphs.