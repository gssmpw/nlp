\vspace{-0.8cm}
\section{Experiment}\label{sec:experiment}
In experiments, we aim to address the following research questions: (1) How does \ourmethod perform in multi-hop retrieval and QA tasks? (\Cref{sec:retrieval,sec:qa}); (2) What are the efficiency and effectiveness of \ourmethod in multi-hop retrieval? (\Cref{sec:efficiency}); (3) How well does \ourmethod generalize to unseen datasets as a foundation model? (\Cref{sec:generalizability}); (4) How does the performance of \ourmethod scale with training as a foundation model? (\Cref{sec:scaling}); (5) How to interpret \ourmethod in multi-hop reasoning? (\Cref{sec:interpretation}).

\subsection{Experimental Setup}\label{sec:setup}
\noindent\textbf{Datasets.} We first evaluate the effectiveness of \ourmethod on three widely-used multi-hop QA datasets, including HotpotQA \cite{yang2018hotpotqa}, MuSiQue \cite{trivedi2022musique}, and 2WikiMultiHopQA (2Wiki) \cite{ho2020constructing}. For a fair comparison \cite{trivedi2023interleaving,gutiérrez2024hipporag}, we use 1,000 samples from each validation set for testing. We merge the candidate passages into the document corpus and extract 20,000 samples from each training set for GFM training. We also evaluate the performance of \ourmethod on seven RAG datasets from three domains, including biomedical \cite{jin-etal-2019-pubmedqa}, custom support \cite{sadat-etal-2023-delucionqa,nandy-etal-2021-question-answering,malaviya2023expertqa,castelli-etal-2020-techqa}, and general knowledge \cite{nguyen2016ms,kamalloo2023hagrid}, to demonstrate the generalizability of \ourmethod as the foundation model. The detailed statistics of the test datasets are shown in the \Cref{app:datasets}.

\noindent\textbf{Baselines.} We compare against several widely used retrieval methods under three categories: (1) \emph{single-step naive methods}: BM25 \cite{robertson1994some}, Contriever \cite{izacardunsupervised}, GTR \cite{ni2022large}, ColBERTv2 \cite{santhanam2022colbertv2}, RAPTOR \cite{sarthiraptor}, Proposition \cite{chen-etal-2024-dense}; (2) state-of-the-art \emph{graph-enhanced methods}: LightRAG \cite{guo2024lightrag}, HippoRAG \cite{gutiérrez2024hipporag}; (3) \emph{multi-step methods}: IRCoT \cite{trivedi2023interleaving}, which can be integrated with arbitrary retrieval methods to conduct multi-step retrieval and reasoning. The detailed introduction of the baselines are shown in the \Cref{app:baselines}.

\noindent\textbf{Metrics.} For retrieval performance, we use recall@2 (R@2) and recall@5 (R@5) as evaluation metrics. For the final QA performance, we use the EM score and F1 score following previous works \cite{gutiérrez2024hipporag}.

\noindent\textbf{Implementation Details.} The GFM retriever is implemented with 6 query-dependent message passing layers with the hidden dimension set to 512. The pre-trained all-mpnet-v2 \cite{all-mpnet-v2} is adopted as the sentence embedding model and is frozen during training. The total parameters of the GFM retriever are 8M, which is trained on 8 NVIDIA A100s (80G) with batch size 4, learning rate 5e-4, and loss weight $\alpha=0.3$. The training data contains 60 KGs with over 14M triples constructed from 700k documents extracted from the training set. The statistics are shown in \Cref{tab:training_data}, and the detailed data construction process, model settings, and training process are shown in \Cref{app:implementation}.

\input{tables/training_data.tex}

\input{tables/retrieval_performance.tex}
\subsection{Retrieval Performance}\label{sec:retrieval}
We first evaluate the retrieval performance of \ourmethod against the baselines on three multi-hop QA datasets. As shown in \Cref{tab:retrieval}, \ourmethod achieves the best performance on all datasets, outperforming the SOTA IRCoT + HippoRAG by 16.8\%, 8.3\%, 19.8\% in R@2 on HotpotQA, MuSiQue, and 2Wiki, respectively. The results demonstrate the effectiveness of \ourmethod in multi-hop retrieval. From the result, we can observe that the naive single-step retrievers (e.g., BM25, RAPTOR) are outperformed by graph-enhanced HippoRAG, which highlights the significance of graph structure in multi-hop retrieval. Although LightRAG uses the graph structure, it struggles with multi-hop QA tasks because its retriever lacks multi-hop reasoning capability. 
With the help of LLMs, the multi-step retrieval pipeline IRCoT improves the performance of all single-step methods through iterative reasoning and retrieval. However, \ourmethod still outperforms the multi-step IRCoT + HippoRAG by a large margin even with a single-step retrieval. This indicates that the \ourmethod can effectively conduct the multi-hop reasoning in a single step (detailed in \Cref{sec:interpretation} and \Cref{app:multi_hop}), which is more efficient and effective than the multi-step retrieval pipeline (detailed in \Cref{sec:efficiency}).

\input{tables/qa_performance.tex}
\subsection{Question Answering Performance}\label{sec:qa}
We then evaluate the QA performance of \ourmethod, as it is directly influenced by retrieval quality. We adopt the GPT-4o-mini \cite{gpt4o} as LLM and use the top-$5$ retrieved documents for generating answers. From the results shown in \Cref{tab:qa}, the single-step \ourmethod has already achieved state-of-the-art performance against all other baselines. Meanwhile, we also integrate \ourmethod with IRCoT to conduct multi-step retrieval and reasoning, which further improves the performance by 8.5\%, 21.2\%, 3.9\% in EM on three datasets, respectively. The results demonstrate the effectiveness and great compatibility of \ourmethod with an arbitrary multi-step framework in multi-hop reasoning tasks.

\input{tables/efficiency.tex}
\subsection{Efficiency Analysis}\label{sec:efficiency}
\ourmethod achieves great efficiency in performing multi-step reasoning in a single step. As shown in \Cref{tab:efficiency}, while the naive single-step methods get the best efficiency whose performance is not satisfying. Admittedly, the multi-step framework IRCoT could improve the performance, but it suffers from high computational costs due to the iterative retrieval and reasoning with LLMs. In contrast, \ourmethod conducts multi-hop reasoning within a single-step GNN reasoning, which is more effective than single-step methods and more efficient than multi-step ones.

\subsection{Ablation Study}\label{sec:ablation}
We conduct ablation studies to investigate the effectiveness of different components in \ourmethod, including different sentence embedding models (\Cref{app:text_embeddings}), pre-training strategies (\Cref{app:pre-training}), and loss weighting strategies (\Cref{app:loss_weight}). The results show that \ourmethod is not sensitive to different sentence embedding models, and the pre-training strategy, as well as the loss weighting strategy, are both crucial for the performance of \ourmethod.

\input{figures/radar.tex}
\subsection{Model Generalizability}\label{sec:generalizability}
To demonstrate the generalizability of \ourmethod as a foundation model, we test the performance (R@5) of \ourmethod on seven domain-specific RAG datasets without any fine-tuning. Specifically, we first build the KG-index from the documents in each dataset. Then, given the query, we use the pre-trained GFM retriever to retrieve the top-$K$ documents with the help of the corresponding KG-index. 
%
As shown in \Cref{fig:radar}, \ourmethod achieves the best performance on all datasets, outperforming the SOTA HippoRAG by 18.9\% on average. The results demonstrate the generalizability of \ourmethod as the foundation model which can be directly applied to various unseen datasets without any fine-tuning. Additionally, results in \Cref{app:trans} demonstrate \ourmethod's strong transferability for further performance improvement when fine-tuned on domain-specific datasets.


\input{figures/scaling.tex}
\input{tables/path_cases.tex}
\subsection{Model Neural Scaling Law}\label{sec:scaling}
We further investigate the neural scaling law of \ourmethod, which quantifies how model performance grows with the scale of training data and model parameter size. It has been validated in the recent foundation models \cite{kaplan2020scaling,dehghani2023scaling}. As shown in \Cref{fig:scaling}, the performance of \ourmethod (MRR: $z$) scales well with the training data ($x$) and the model size ($y$), which can be fitted by the power-law scaling law $z \propto 0.24x^{0.05} + 0.11y^{0.03}$. The results demonstrate the scalability of \ourmethod as the foundation model and potential for further improvement. The detailed analysis of the neural scaling law is shown in \Cref{app:scaling}.

\subsection{Path Interpretations}\label{sec:interpretation}
\ourmethod exhibits the multi-hop reasoning ability powered by the multi-layer GFM. We provide path interpretations of \ourmethod for multi-hop reasoning in \Cref{tab:cases}. Inspired by NBFNet \cite{zhu2021neural}, the paths' importance to the final prediction can be quantified by the partial derivative of the prediction score with respect to the triples at each layer (hop), defined as: 
\begin{equation}
    \setlength\abovedisplayskip{0pt}%shrink space
    \setlength\belowdisplayskip{0pt}
    s_1,s_2,\ldots,s_L=\arg\mathop{\text{top-}}k\frac{\partial p_e(q)}{\partial s_l}.
\end{equation}
The top-$k$ path interpretations can be obtained by the top-$k$ longest paths with beam search. We illustrate the path interpretations in \Cref{tab:cases}. In the first example, \ourmethod successfully deduces that the singer of the song has a football club named after him and that he owned it. In the second example, \ourmethod identifies two paths related to the murder case and the judge presiding over the trial. These interpretations show that \ourmethod exhibits the ability of multi-hop reasoning within single-step retrieval. We also illustrate the distribution the multi-hop prediction in \Cref{app:multi_hop}.