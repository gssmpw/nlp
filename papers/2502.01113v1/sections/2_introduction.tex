\section{Introduction}\label{sec:introduction}
\input{figures/intro.tex}

Recent advancements in large language models (LLMs) \cite{gpt4o,llama3,qwen2} have greatly propelled the evolution of natural language processing, positioning them as foundational models for artificial general intelligence (AGI). Despite the remarkable reasoning ability \cite{openai_o1}, LLMs are still limited in accessing real-time information and lack of domain-specific knowledge, which is outside the pre-training corpus. To address these limitations, retrieval-augmented generation (RAG) \cite{gao2023retrieval} has become a popular paradigm in adding new knowledge to the static LLMs by retrieving relevant documents into the context of LLM generation. 

Existing RAG methods typically retrieve documents independently, making it difficult to capture complex relationships between pieces of knowledge \cite{karpukhin2020dense,bge_m3,moreira2024nv}. This limitation hampers the performance of LLMs in integrating knowledge across document boundaries, particularly in multi-hop reasoning tasks \cite{yang2018hotpotqa,trivedi2022musique} and real-world applications like legal judgment \cite{kang2024bridging} and medical diagnoses \cite{jin-etal-2019-pubmedqa}, which require reasoning over multiple sources. Although recent methods have expanded the retrieval process into multiple steps and incorporate LLM reasoning, they still encounter high computational costs due to iterative retrieval and reasoning with LLMs \cite{trivedi2023interleaving,sunthink,joshi2024reaper}.

Recently, graph-enhanced retrieval augmented generation (GraphRAG) \cite{peng2024graph,han2024retrieval} has emerged as a novel solution that builds a graph structure to explicitly model the intricate relationships between knowledge. This enables the development of a graph-enhanced retriever to identify relevant information using graphs. The structural nature of graphs allows GraphRAG to capture global context and dependencies among documents, significantly improving reasoning across multiple sources \cite{edge2024local}. 
Methods like HippoRAG \cite{gutiérrez2024hipporag} enhance retrieval by employing a personalized PageRank algorithm to locate relevant knowledge with graphs. However, these algorithms rely solely on the graph structure, which is often noisy or incomplete, limiting their overall performance.
%However, the graph structure in GraphRAG may contain noise or be incomplete, which hampers the retriever's ability to locate relevant knowledge solely based on graph structure \cite{gutiérrez2024hipporag}. 
Alternative methods \cite{mavromatis2024gnn,he2024g} incorporate graph neural networks (GNNs) into the retrieval process. These methods have shown impressive performance due to GNNs' powerful reasoning capabilities in handling incomplete graphs \cite{galkintowards}. Nevertheless, they still face limitations in generalizability since they require training from scratch on new datasets.

% GFM
Nowadays, the search for a foundation GNN model that can transfer and generalize across different datasets has been an active research topic. Ideally, a foundation GNN or graph foundation model (GFM) can benefit from large-scale training and generalize across diverse graphs \cite{maoposition,liu2023towards}. Efforts have been made to identify transferable graph tokens (e.g., motifs, sub-trees, and relation graphs) \cite{galkintowards,wang2024gft,xia2024opengraph} that can be shared among different graphs for GFM design. However, these methods primarily focus on graph-related tasks (e.g., node classification and link prediction), leaving the design of a GFM to enhance LLMs' reasoning ability unexplored.
%How to design a GFM to enhance the reasoning of LLM remains an open question.

% Foundation models (FMs) \cite{bommasani2021opportunities} renovate the landscape of AI by providing a large-scale model that can be directly applied to diverse datasets and applications without expensive training. FMs, trained on massive datasets, are expected to fit the neural scaling law \cite{hestness2017deep} as the scale of training data and model size grows, which has been validated in recent foundation models in computer vision \cite{dehghani2023scaling} and natural language processing \cite{kaplan2020scaling}. However, how to design a general and scalable graph foundation model (GFM) that can be directly applied to various unseen datasets to power GraphRAG remains an open challenge \cite{maoposition,liu2023towards}.

To bridge the gap, in this paper, we propose an effective, efficient, and general graph foundation model for retrieval augmented generation (\ourmethod), thereby enhancing LLMs' reasoning ability.
%
As shown in \Cref{fig:intro}, we create a \emph{knowledge graph index} (KG-index) from documents in each dataset. The KG-index consists of interconnected factual triples pointing to the original documents, which serves as a structural knowledge index across multiple sources, enhancing the integration of diverse knowledge for complex reasoning tasks \cite{gutiérrez2024hipporag}. 
%This aligns with the hippocampal memory indexing theory \cite{teyler1986hippocampal}, where the KG-index functions like an artificial hippocampus to store associations between knowledge memories, enhancing the integration of diverse knowledge for complex reasoning tasks\cite{gutiérrez2024hipporag}.
% As shown in \Cref{fig:intro}, we begin by creating a \emph{knowledge graph index} (KG-index) from documents in each dataset. The KG-index holds a set of interconnected triples pointing to the original documents, serving as a structural index of knowledge across multiple documents. The KG-index 
% aligns with the human hippocampal memory indexing theory \cite{teyler1986hippocampal} where the KG-index acts like an artificial hippocampal index to store associations between knowledge memory, enhancing the integration of diverse knowledge for complex reasoning tasks \cite{gutiérrez2024hipporag}. 
%
Then, we present the \emph{graph foundation model retriever} (GFM retriever), driven by a query-dependent GNN that captures complex query-knowledge relationships in a unified,  transferable space of semantics and graph structure. Through multi-layer message passing, the GFM retriever enables efficient multi-hop retrieval in a single step, surpassing previous multi-step methods.
%
The GFM retriever, with 8M parameters, undergoes a two-stage training: \emph{unsupervised KG completion pre-training} and \emph{supervised document retrieval fine-tuning} on large-scale datasets, including 60 knowledge graphs with over 14M triples and 700k documents. This large-scale training ensures the generalizability of GFM retriever to be applied directly to unseen datasets without further training.

In experiments, \ourmethod achieves state-of-the-art performance across three multi-hop QA datasets, demonstrating its effectiveness and efficiency in multi-hop reasoning. It also generalizes well across seven RAG datasets from diverse domains, such as biomedical, customer service, and general knowledge, without requiring additional training. Furthermore, \ourmethod follows the neural scaling law \cite{hestness2017deep}, whose performance benefits from training data and model size scaling, emphasizing its potential as a foundational model for future improvements.

The main contributions of this paper are as follows:
\begin{itemize}
\item We introduce a graph foundation model for retrieval augmented generation (\ourmethod), powered by a novel query-dependent GNN to enable efficient multi-hop retrieval within a single step.
\item We train a large-scale model with 8M parameters, marking the first graph foundation model (GFM) that can be applied directly to various unseen datasets for retrieval augmented generation.
\item We evaluate \ourmethod on three multi-hop QA datasets and seven domain-specific RAG datasets, achieving state-of-the-art performance across all, demonstrating its effectiveness, efficiency, generalizability, and potential as a foundational model for further enhancement.
\end{itemize}
