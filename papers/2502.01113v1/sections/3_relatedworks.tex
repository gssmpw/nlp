\input{figures/framework.tex}
\section{Related Work}

\noindent\textbf{Retrieval-augmented generation (RAG)} \cite{gao2023retrieval} provides an effective way to integrate external knowledge into large language models (LLMs) by retrieving relevant documents to facilitate LLM generation. Early works adopt the pre-trained dense embedding model to encode documents as separate vectors \cite{karpukhin2020dense,bge_m3,li2023towards,moreira2024nv}, which are then retrieved by calculating the similarity to the query. Despite efficiency and generalizability, these methods struggle to capture complex document relationships. Subsequent studies have explored multi-step retrieval, where LLMs guide an iterative process to retrieve and reason over multiple documents \cite{trivedi2023interleaving,jiang2023active,su-etal-2024-dragin}. However, this approach is computationally expensive. 

\noindent\textbf{Graph-enhanced retrieval augmented generation (GraphRAG)} \cite{peng2024graph,han2024retrieval} is a novel approach that builds graphs to explicitly model the complex relationships between knowledge, facilitating comprehensive retrieval and reasoning. Early research focuses on retrieving information from existing knowledge graphs (KGs), such as WikiData \cite{vrandevcic2014wikidata} and Freebase \cite{bollacker2008freebase}, by identifying relevant facts or reasoning paths \cite{li2023graph,luoreasoning}. Recent studies have integrated documents with KGs to improve knowledge coverage and retrieval \cite{edge2024local,liang2024kag}. A graph structure is built from these documents to aid in identifying relevant content for LLM generation \cite{dong2024don}. Based on graphs, LightRAG \cite{guo2024lightrag} incorporates graph structures into text indexing and retrieval, enabling efficient retrieval of entities and their relationships.
HippoRAG \cite{guti√©rrez2024hipporag} enhances multi-hop retrieval by using a personalized PageRank algorithm to locate relevant knowledge with graphs. However, the graph structure can be noisy and incomplete, leading to suboptimal performance. Efforts to incorporate GNNs into graph-enhanced RAG \cite{mavromatis2024gnn,he2024g} have shown impressive results due to the strong graph reasoning capabilities of GNNs in handling incomplete graphs \cite{galkintowards}. Nonetheless, these methods still limit in generalizability due to the lack of a graph foundational model.

\noindent\textbf{Graph Foundation models (GFM)} aims to be a large-scale model that can generalize to various datasets \cite{maoposition,liu2023towards}. The main challenge in designing GFMs is identifying graph tokens that capture invariance across diverse graph data. For instance, ULTRA \cite{galkintowards} employs four fundamental relational interactions in knowledge graphs (KGs) to create a GFM for link prediction. OpenGraph \cite{xia2024opengraph} develops a graph tokenizer that converts graphs into a unified node token representation, enabling transformer-like GFMs for tasks such as link prediction and node classification. GFT \cite{wang2024gft} introduces a transferable tree vocabulary to construct a GFM that demonstrates effectiveness across various tasks and domains in graph learning. Despite these successful efforts, most methods primarily focus on conventional graph-related tasks. How to design a GFM to enhance the reasoning of LLM remains an open question.

% \noindent\textbf{Foundation models (FMs)} revolutionize AI research by offering a large-scale model that can be directly applied to various datasets. FMs have shown impressive effectiveness in many fields, including computer vision \cite{dehghani2023scaling}, natural language processing \cite{kaplan2020scaling}, audio \cite{borsos2023audiolm}, and video \cite{zhao2023learning}. To enable generalizability, FMs generally undergo large-scale training on diverse datasets, adhering to the neural scaling law \cite{hestness2017deep} as performance improves with increasing training data and model size. The exploration of graph foundation models (GFMs) is also a trending topic in the graph learning community \cite{maoposition,liu2023towards}. However, designing GFMs remains a challenge due to the complexity of graph structures and the diversity of features present in different datasets.