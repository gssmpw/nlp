\section{Approach}\label{sec:approach}
% In this section, we will introduce the proposed \ourmethod, which consists of three main components: (1) \emph{KG-index construction}, which constructs a knowledge graph index from the document corpus; (2) \emph{graph foundation model retriever} (GFM retriever), which is pre-trained on large-scale datasets and could retrieve documents based on any user query and knowledge graph index; and (3) \emph{documents ranking and answer generation}, which ranks the retrieved documents and generates the final answer. The overall framework of \ourmethod is illustrated in \Cref{fig:framework}.

The proposed \ourmethod essentially implements a GraphRAG paradigm by constructing graphs from documents and using a graph-enhanced retriever to retrieve relevant documents.

\noindent\textbf{GFM-RAG Overview.}
Given a set of documents $\gD=\{D_1,D_2,\ldots,D_{|\gD|}\}$, we construct a knowledge graph $\gG=\{(e,r,e')\in \gE\times\gR\times\gE\}$, where $e,e'\in\gE$ and $r\in\gR$ denote the set of entities and relations extracted from $\gD$, respectively. 
%
For a user query $q$, we aim to design a graph-enhanced retriever to obtain relevant documents from $\gD$ by leveraging the knowledge graph $\gG$. The whole \ourmethod process can be formulated as:
\begin{gather}
    \gG = \text{KG-index}(\gD), \\ \gD^K = \text{GFM-Retriever}(q,\gD,\gG),\\
    a = \text{LLM}(q,\gD^K).
\end{gather}
In the first step, KG-index($\cdot$)  constructs a knowledge graph index $\gG$ from the document corpus $\gD$, followed by our proposed \emph{graph foundation model retriever} (GFM-Retriever), which is pre-trained on large-scale datasets. It retrieves top-$K$ documents based on any user query $q$ and knowledge graph index $\gG$. The retrieved documents $\gD^K$, along with the query $q$, are then input into a large language model (LLM) to generate the final answer $a$.
% The final step ranks the retrieved documents and generates the final answer $a$ for the query $q$ with LLM based on the retrieved documents $\gD^K$.
%where $\gD^K$ denotes the top-$K$ retrieved documents and $a$ denotes the final answer generated by large language models (LLMs). 
%
These three main components in \ourmethod are illustrated in \Cref{fig:framework} and will be detailed next.

% These three main components in \ourmethod: (1) \emph{KG-index construction}, which constructs a knowledge graph index from the document corpus; (2) \emph{graph foundation model retriever} (GFM retriever), which is pre-trained on large-scale datasets and could retrieve documents based on any user query and knowledge graph index; and (3) \emph{documents ranking and answer generation}, which ranks the retrieved documents and generates the final answer, are illustrated in \Cref{fig:framework} and will be detailed next.


\subsection{KG-index Construction}\label{sec:kg-construction}
Conventional embedding-based index methods encode documents as separate vectors \cite{karpukhin2020dense,bge_m3,moreira2024nv}, which are limited in modeling the relationships between them. Knowledge graphs (KGs), on the other hand, explicitly capturing the relationships between millions of facts, can provide a structural index of knowledge across multiple documents \cite{edge2024local,gutiérrez2024hipporag}. The structural nature of the KG-index aligns well with the human hippocampal memory indexing theory \cite{teyler1986hippocampal}, where the KG-index functions like an artificial hippocampus to store associations between knowledge memories, enhancing the integration of diverse knowledge for complex reasoning tasks \cite{gutiérrez2024hipporag}.

To construct the KG-index, given a set of documents $\gD$, we first extract entities $\gE$ and relations $\gR$ to form triples $\gT$ from documents. Then, the entity to document inverted index $M \in \{0,1\}^{|\gE|\times|\gD|}$ is constructed to record the entities mentioned in each document. Such a process can be achieved by existing open information extraction (OpenIE) tools \cite{angeli2015leveraging,ijcai2022p793,pai2024survey}. To better capture the connection between knowledge, we further conduct the entity resolution \cite{gillick2019learning,zeakis2023pre} to add additional edges $\gT^{\texttt{+}}$ between entities with similar semantics, e.g., (\texttt{USA}, \texttt{equivalent}, \texttt{United States of America}). Therefore, the final KG-index $\gG$ is constructed as $\gG=\{(e,r,e')\in\gT\cup\gT^{\texttt{+}}\}$. In implementation, we leverage an LLM \cite{gpt4o} as the OpenIE tool and a pre-trained dense embedding model \cite{santhanam2022colbertv2} for entity resolution.

\subsection{Graph Foundation Model (GFM) Retriever}\label{sec:gfm-retriever}
The GFM retriever is designed to retrieve relevant documents based on any user query and the constructed KG-index. While the KG-index offers a structured representation of knowledge, it still suffers from incompleteness and noise, resulting in suboptimal retrieval performance when solely relying on its structure \cite{gutiérrez2024hipporag}. Recently, graph neural networks (GNNs) \cite{wu2020comprehensive} have shown impressive graph reasoning ability by capturing the complex relationships between knowledge for retrieval or question answering \cite{mavromatis2024gnn,he2024g}. However, existing GNNs are limited in generalizability, as they are usually trained on specific graphs \cite{maoposition,liu2023towards}, which limits their application to unseen corpora and KGs. Therefore, there is still a need for a graph foundation model that can be directly applied to unseen datasets and KGs without additional training.

% What a GFM for graph retriever should be like?
To address these issues, we propose the first graph foundation model-powered retriever (GFM retriever), which harnesses the graph reasoning ability of GNNs to capture the complex relationships between queries, documents, and knowledge graphs in a unified and transferable space. The GFM retriever employs a query-dependent GNN to identify relevant entities in graphs that will aid in locating pertinent documents. After pre-training on large-scale datasets, the GFM retriever can be directly applied to new corpora and KGs without further training.

\subsubsection{Query-dependent GNN}\label{sec:message-passing}
Conventional GNNs \cite{gilmer2017neural} follow the message passing paradigm, which iteratively aggregates information from neighbors to update entity representations. Such a paradigm is not suitable for the GFM retriever as it is graph-specific and neglects the relevance of queries. Recent query-dependent GNNs \cite{zhu2021neural,galkintowards} have shown promising results in capturing query-specific information and generalizability to unseen graphs, which is essential for the GFM retriever and can be formulated as: 
% The comparison between the conventional GNN and query-dependent GNN is as follows:
% \begin{align}
%     & \text{\textbf{Conventional GNN: }} 
%         H^L = \text{GNN}(\gG,H^0),
%     \\
%     & \text{\textbf{Query-dependent GNN: }}
%         H_q^L = \text{GNN}_q(q,\gG,H^0),
%         % p(e|q) = \sigma(\text{MLP}(h_{e|q})),~h_{e|q} \in H_q^T, 
%     % \end{gathered}
% \end{align}
\begin{equation}
    \setlength\abovedisplayskip{2pt}%shrink space
    \setlength\belowdisplayskip{2pt}
    % \text{\textbf{Query-dependent GNN: }}
        H_q^L = \text{GNN}_q(q,\gG,H^0),
\end{equation}
where $H^0\in\sR^{|\gE|\times d}$ denotes initial entity features, and $H_q^L$ denotes the updated entity representations conditioned on query $q$ after $L$ layers of query-dependent message passing. 
% and $p(e|q)$ denotes the probability of entity $e$ being relevant to query $q$, and $\sigma$ denotes the sigmoid function.

The query-dependent GNN exhibits better expressively \cite{you2021identity} and logical reasoning ability \cite{qiuunderstanding}, which is selected as the backbone of our GFM retriever. It allows the GFM retriever to dynamically adjust the message passing process based on user queries and find the most relevant information on the graph.

\noindent\textbf{Query Initialization.} Given a query $q$, we first encode it into a query embedding with a sentence embedding model:
\begin{equation}
    \setlength\abovedisplayskip{2pt}%shrink space
    \setlength\belowdisplayskip{2pt}
    \vq = \text{SentenceEmb}(q),~\vq\in\mathbb{R}^d,
\end{equation}
where $d$ denotes the dimension of the query embedding. Then, for all the entities mentioned in the query $e_q\in\gE_q\subseteq\gE$, we initialize their entity features as $\vq$ while others as zero vectors:
\begin{equation}
    \setlength\abovedisplayskip{1pt}%shrink space
    \setlength\belowdisplayskip{1pt}
    H^0 = \begin{cases}
        \vq, & e\in\gE_q, \\
        \vzero, & \text{otherwise}.
    \end{cases}
\end{equation}

\noindent\textbf{Query-dependent Message Passing.} The query-dependent message passing will propagate the information from the question entities to other entities in the KG to capture their relevance to the query. The message passing process can be formulated as:
%
\begin{flalign}
    & \text{\textbf{Triple-level}: } \nonumber && \\
    & h^0_r = \text{SentenceEmb}(r),~h^0_r\in\mathbb{R}^d, && \\
    & \resizebox{.8\columnwidth}{!}{$m_e^{l+1} = \text{Msg}(h_e^l,g^{l+1}(h_r^l),h_{e'}^l), (e,r,e')\in\gG,$} && \\
    & \text{\textbf{Entity-level}: } \nonumber && \\
    & \resizebox{.89\columnwidth}{!}{$h_e^{l+1} = \text{Update}(h_e^l,\text{Agg}(\{m_{e'}^{l+1} | e'\in\gN_r(e),r\in\gR\})),$} &&
\end{flalign}
where $h^l_e, h^l_r$ denote the entity and relation embeddings at layer $l$, respectively. The relation embeddings $ h^0_r$ are also initialized using the same sentence embedding model as the query, reflecting their semantics (e.g., ``$\texttt{born\_in}$''), and updated by a layer-specific function $g^{l+1}(\cdot)$, implemented as a 2-layer MLP.
The $\text{Msg}(\cdot)$ is operated on all triples in the KG to generate messages, which is implemented with a non-parametric DistMult \cite{yang2015embedding} following the architecture of NBFNet \cite{zhu2021neural}. For each entity, we aggregate the messages from its neighbors $\gN_r(e)$ with relation $r$ using sum and update the entity representation with a single linear layer.


After $L$ layers message passing, a final MLP layer together with a sigmoid function maps the entity embeddings to their relevance scores to the query:
\begin{equation}
    \setlength\abovedisplayskip{2pt}%shrink space
    \setlength\belowdisplayskip{2pt}
    P_q = \sigma(\text{MLP}(H_q^L)),~P_q\in\mathbb{R}^{|\gE|\times 1}.
\end{equation}

\noindent\textbf{Generalizability.} Since the query, entity, and relation embeddings are initialized using the same sentence embedding model with identical dimensions, the query-dependent GNN can be directly applied to different queries and KGs. This allows it to learn complex relationships between queries and entities by taking into account both the semantics and structure of the KG through training on large-scale datasets.

\subsubsection{Training Process}\label{sec:training}

\noindent\textbf{Training Objective.} The training objective of the GFM retriever is to maximize the likelihood of the relevant entities to the query, which can be optimized by minimizing the binary cross-entropy (BCE) loss:
\begin{equation}
    \setlength\abovedisplayskip{3pt}%shrink space
    \setlength\belowdisplayskip{3pt}
    \resizebox{1\columnwidth}{!}{$
    \gL_{\text{BCE}} = -\frac{1}{|\gA_q|}\sum_{e\in\gA_q} \log P_q(e) - \frac{1}{|\gE^{\texttt{-}}|}\sum_{|\gE^{\texttt{-}}|} \log (1-P_q(e)), $}
\end{equation}
where $\gA_q$ denotes the set of target relevant entities to the query $q$, and $\gE^{\texttt{-}}\subseteq \gE\setminus \gA_q$ denotes the set of negative entities sampled from the KG. However, due to the sparsity of the target entities, the BCE loss may suffer from the gradient vanishing problem \cite{lin2024understanding}. To address this issue, we further introduce the ranking loss \cite{bai2023regression} to maximize the margin between the positive and negative entities:
\begin{equation}
    \setlength\abovedisplayskip{2pt}%shrink space
    \setlength\belowdisplayskip{2pt}
    \gL_{\text{RANK}} = - \frac{1}{|\gA_q|}\sum_{e\in\gA_q} \frac{P_q(e)}{\sum_{e'\in\gE^{\texttt{-}}} P_q(e')}.
\end{equation}
The final training objective is the weighted combination of the BCE loss and ranking loss:
\begin{equation}
    \setlength\abovedisplayskip{2pt}%shrink space
    \setlength\belowdisplayskip{2pt}
    \gL = \alpha\gL_{\text{BCE}} + (1-\alpha) \gL_{\text{RANK}}.\label{eq:training}
\end{equation}

\noindent\textbf{Unsupervised KG Completion Pre-training.} To enhance the graph reasoning capability of the GFM retriever, we first pre-train it on a large-scale knowledge graph (KG) completion task. We sample a set of triples from the KG index and mask either the head or tail entity to unsupervisedly create synthetic queries in the form $q=(e,r,?)~\text{or}~(?, r, e')$, with the masked entity serving as the target entity $\gA_q = \{e\}~\text{or}~\{e'\}$. The GFM retriever is then trained to predict the masked entity using both the query and the KG, as outlined in \eqref{eq:training}.

\noindent\textbf{Supervised Document Retrieval Fine-tuning.} After unsupervised pre-training, we fine-tune the GFM retriever on a supervised document retrieval task. In this task, queries $q$ are natural language questions, and target entities $\gA_q$ are extracted from labeled supporting documents $\gD_q$. The GFM retriever is trained to retrieve relevant entities from the KG index using the same training objective as in \eqref{eq:training}. 

\subsection{Documents Ranking and Answer Generation}\label{sec:ranking}
Given the entity relevance scores $P_q\in\sR^{|\gE|\times 1}$ predicted by the GFM retriever, we first retrieve the top-$T$ entities $\gE_q^{T}$ with the highest relevance scores as:
\begin{equation}
    \gE_q^{T} = \arg\text{top-}T(P_q),~\gE_q^{T}=\{e_1,\ldots,e_T\}.
\end{equation}
These retrieved entities are then used by the document ranker to obtain the final documents. To diminish the influence of popular entities, we weigh the entities by the inverse of their frequency as entities mentioned in the document inverted index $M \in \{0,1\}^{|\gE|\times|\gD|}$ and calculate the final document relevance scores by summing the weights of entity mentioned in documents:
{
\begin{gather}
    \setlength\abovedisplayskip{2pt}%shrink space
    \setlength\belowdisplayskip{2pt}
    F_e = \begin{cases}
        \frac{1}{\sum_{d\in\gD} M[e,d]}, & e\in\gE_q^{T}, \\
        0, & \text{otherwise},
    \end{cases} \\
    P_d = M^{\top}F_e,~P_d\in\sR^{|\gD|\times 1}.
\end{gather}
The top-$K$ documents are retrieved based on the document relevance scores $P_d$ and fed into the context of LLMs, with a retrieval augmented generation manner, to generate the final answer:
\begin{gather}
    \setlength\abovedisplayskip{1pt}%shrink space
    \setlength\belowdisplayskip{1pt}
    \gD^K = \arg\text{top-}K(P_d),~\gD^K=\{D_1,\ldots,D_K\}, \\
    a = \text{LLM}(q,\gD^K).
\end{gather}
}