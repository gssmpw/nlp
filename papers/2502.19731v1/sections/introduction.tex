\section{Introduction}
According to the World Mental Health Report\footnote{\url{https://www.who.int/teams/mental-health-and-substance-use/world-mental-health-report}}, the global demand for mental health support far exceeds the availability of accessible, affordable, and timely care. Millions of individuals struggle with mental health challenges, yet many face a severe shortage of trained professionals, particularly in low- and middle-income countries. The emergence of Large Language Models (LLMs) has accelerated the integration of AI into psycho-counseling, thanks to their remarkable ability to comprehend human intent and provide effective responses~\citep{Ouyang2022-ic}. However, due to the complexity of clients’ situations and the professional skills required, current LLMs still face challenges in consistently providing effective responses to client speeches during counseling sessions~\citep{Na2025-fs,Zhang2024-ma,Chung2023-jk}.

The sensitive and private nature of counseling sessions poses significant challenges for obtaining publicly available datasets that accurately reflect real-world interactions~\citep{Stade2024-dn}. This scarcity of resources hinders efforts to train LLMs in effectively understanding and responding to client speech within counseling contexts. Moreover, the quality of responses provided by therapists can vary widely, influenced by their level of professional training and experience~\citep{Rocco2019-jn}. This variability impacts the consistency of counseling effects and underscores the importance of standardizing and assessing therapist responses.

To address these gaps, we collaborated with professors in social work and psychiatry to develop a set of professional and comprehensive principles for evaluating therapists’ responses to client speeches. These principles assess not only the fundamental aspects of a response in a counseling session, such as empathy, relevance, conciseness, and safety, but also extend the effectiveness of a response based on professional psycho-counseling theory. This includes evaluating whether the response promotes clients’ self-exploration, enhances their autonomy, and effectively identifies the mind changing stages.

Using these principles, we extract high-quality responses from the generations of a pool of popular LLMs and construct a large psycho-counseling preference dataset, \dataset{}. The dataset comprises 26,483 unique client speeches spanning 8 coarse-grained and 42 fine-grained topics. To the best of our knowledge, \dataset{} is the first large-scale and comprehensive psycho-counseling preference dataset. We hired professional psychotherapists for verification, and their annotations exhibit strong agreement within \dataset{}, ensuring the dataset’s reliability and consistency.

Experiments show that our reward models trained with \dataset{} show an excellent ability of evaluating responses to clients while previous start-of-the-art reward models lag behind. Moreover, we apply both online and offline preference learning on \dataset{} or the trained reward models. Our best resulting model, \policyl{}, achieves the state-of-the-art performance on the testing set of \dataset{}, with an impressive win rate of 87\% against GPT-4o. Feedback from professional psychologists shows that \policyl{} could give more balanced and desirable responses under length constraint during the inference stage. Through further analysis and case study, we demonstrate the advantage of training online over offline and provide insights into how to improve the model performance in the future.