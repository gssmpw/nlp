\section{\dataset{}}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/pipline-cropped.pdf}
    \caption{\dataset{} Construction Pipeline. 1) We first collect over 26k client speeches covering a wide range of topics from various sources, applying necessary data cleaning. 2) 20 popular LLMs are sampled and prompted to roleplay as psychotherapists and give responses to these client speeches. 3) GPT-4o is instructed to evaluate the responses based on our proposed PsychoCounsel Principles, and preference pairs with substantial score gaps are incorporated into \dataset{}.}
    \label{fig:pipline}
\end{figure}

\subsection{Client Speech Collection}
We collect client speeches from various data sources: counsel-chat\footnote{\url{https://github.com/nbertagnolli/counsel-chat}}, MentalAgora~\citep{Lee2024-ly}, TherapistQA~\citep{Shreevastava2021-eh}, Psycho8k~\citep{Liu2023-ub}, and several huggingface datasets (amod-counsel\footnote{\url{https://huggingface.co/datasets/Amod/mental_health_counseling_conversations}}, MentalChat16K\footnote{\url{https://huggingface.co/datasets/ShenLab/MentalChat16K}}, and phi2Mental\footnote{\url{https://huggingface.co/datasets/saxenaindresh681/microsoft-phi2-mental-health}}). Client speeches with number of characters more than 1,000 and less than 100 are discarded to ensure a proper length of context. After an additional step of de-duplication, the resulting data contains 26,483 client speeches with average length of 366 characters covering a wide range of topics including 8 coarse topics: Core Mental Health Issues (9,054), Emotional Well-being and Coping Strategies (5,717), Relationships and Interpersonal Dynamics (6,483), Life Transitions and Challenges (934), Social Issues (667), Youth and Development (1,175), Crisis and Safety Concerns (529) and Special Topics (1,924). Under these 8 topics are 42 fine-grained topics (see Table~\ref{tab:topic_distributions} in the appendix for the detailed topic distribution).

\subsection{PsychoCounsel Principles}\label{sec:principle}
To answer the question \textit{what is a good response to a client speech in psycho-counseling}, we collaborate with professors in social work and psychiatry (our co-authors) and propose a set of professional principles to measure the response to a client speech from seven different dimensions: \input{tables/principles}

Please refer to Box~\ref{box:priciple} for the complete definition of the principles. Among these seven principles, \textbf{Facilitation of Self-Exploration}, \textbf{Promotion of Autonomy and Confidence}, and \textbf{Identifying Stages and Reasons for Change} emphasize a client-centered approach, which is recognized as a hallmark of effective psycho-counseling~\citep{Miller-and-Stephen-RollnickUnknown-oo}. We use these three principles to measure the \textit{effectiveness} of a response to a client speech, complementary to the other four principles, which are more basic, requiring the response to be \textit{empathy}, \textit{relevant}, \textit{concise}, and \textit{safe}. Evaluating therapist responses using these fine-grained principles provides a more structured and nuanced assessment of their effectiveness. Unlike general evaluations that focus solely on overall quality, this detailed approach allows for a deeper understanding of how well a response supports the client’s emotional and psychological needs.

\subsection{Preference Generation}
We apply the generate-score-pair pipeline as ~\cite{Cui2023-ui} to construct the \dataset{} dataset. For each client speech, we randomly sample four off-the-shelf LLMs from a model pool to give the response and instruct GPT-4o to annotate each response with 5-Likert scores for each principle defined in Section~\ref{sec:principle}; higher scores mean more alignment with the principles. Then scores of the principles are averaged to get the overall score for a response and preference pairs are generated based on the overall scores. The whole pipeline is illustrated in Figure~\ref{fig:pipline}. To increase the diversity of the model responses, we initialize the model pool with 20 popular LLMs of a range of sizes developed by different organizations shown in Table~\ref{tab:model-pool}. We also include LLMs with different architectures other than pure transformers like AI21-Jamba-1.5-Mini~\citep{Jamba-Team2024-vt}, which is a hybrid transformer-mamba model. We randomly held out 3,291 client speeches for testing and the remaining 23,192 for training. After obtaining the scores of principles, for training, we extract response pairs with the overall score gap larger than or equal to 1 as the preference pairs, and for testing, we only extract the ([highest score response], [lowest score response]) pairs and pairs with the score gap less than 1 are discarded. In this way, we could exclude response pairs with similar scores, whose quality may be hard to differentiate.

Ultimately, \dataset{} includes 34,329 training preference pairs and 2,324 testing pairs. The models most likely to be chosen and those most likely to be rejected vary significantly in size (see Figures~\ref{fig:response-dist-chosen} and~\ref{fig:response-dist-rejected} for the distributions of chosen and rejected models). This suggests that simply scaling model size is not a decisive factor in making LLMs effective responders in psycho-counseling. We also observe that LLMs developed by non-English-speaking institutions are more likely to be rejected compared to those from English-speaking countries. This may suggest that non-English-speaking institutions have a greater need to enhance the capabilities of LLMs in their respective languages, potentially leading to less emphasis on developing psycho-counseling skills in English.


\subsection{Preference Validation}
To validate the quality of synthetic human preferences in \dataset{}, we hired two professional psychotherapists through Upwork\footnote{\url{https://www.upwork.com/}} and instructed them to annotate preferences based on each principle and give the overall preference. The annotation set consists of 200 preference pairs randomly sampled from \dataset{}. The two therapists agree on 174 out of 200 samples. Additionally, one expert’s annotations align with the preference labels in \dataset{} for 184 out of 200 samples, while the other aligns for 170 out of 200 samples. These results indicate a high level of agreement between the experts (87\%) and demonstrate strong alignment between the expert annotations and the preference labels in \dataset{} (88.5\%). This strongly suggests that the labels in \dataset{} are reliable and trustworthy.



