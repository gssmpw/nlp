\section{Related Work}
\subsection{LLMs Assisting Psychotherapy}
Integrating LLMs into Psychotherapy is not a trivial process which could articulated as a continuum of stages of assistive AI, collaborated AI, and fully autonomous AI~\citep{Stade2024-dn}. Currently, we are still in the first two stages where models operating tasks need human supervision. Related tasks include cognitive disorder detection~\citep{Shreevastava2021-eh,Chen2023-oi}, negative thoughts recognition and reframing~\citep{Maddela2023-yb,Sharma2024-jy}, and patient simulation~\citep{Chen2023-dk} or therapist simulation~\cite{Liu2023-ub}, among which therapist simulation is the primary goal across the stages. However, \cite{Zhang2024-ma} found that due to the lack of public high-quality data in psychotherapy and the complexity of clients' situation, LLMs still are not able to give effective responses to a client's speech consistently in a therapy session. Our work focuses on psycho-counseling, which is a short-term, supportive process for helping individuals cope with life challenges and emotional distress, and constructs a high-quality preference dataset that aligns with the preference of professional psychotherapists and uses preference learning to train helpful and reliable assistants for psycho-counseling.


\subsection{Human Preference Alignment}
Human preference alignment has been shown to be a critical step in making LLMs helpful, harmless, and honest~\citep{Ouyang2022-ic,Bai2022-yy}. Current methods can be broadly categorized into offline and online approaches. Offline methods optimize the model using a pre-annotated set of preference data with objectives such as DPO~\citep{Rafailov2023-jm}. Online methods, on the other hand, generate outputs during training and utilize a reward function to score them. High-scoring generations are encouraged, while low-scoring ones are discouraged through policy gradient methods such as PPO~\citep{Schulman2017-rh}.
% and SimPO~\citep{Meng2024-uh}
% and GRPO~\citep{Shao2024-dp}

Compared to offline alignment, online methods are more computationally expensive and require careful hyperparameter tuning to ensure stable training~\citep{Xu2024-jb}. Offline methods, which frame alignment as optimizing a classification loss, eliminate the need for a reward model, making them more stable and efficient. However, they are susceptible to distribution shifts~\citep{Marks2023-gv}. Furthermore, \cite{Tang2024-ac} found that optimizing with online preferences instead of offline data can lead to better model performance. Iterative direct preference learning combines the strengths of both offline and online methods. In this approach, preference data is generated online and used to optimize an offline learning objective~\citep{Pang2024-ag}, which has been demonstrated as a strong baseline in both academia~\citep{Xu2024-jb} and industry~\citep{Yang2024-nl}.
