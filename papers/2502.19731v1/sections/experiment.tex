\section{Experiments}
\subsection{Reward Model}
\input{tables/rm-main}
Following \cite{Ouyang2022-ic} and \cite{Bai2022-yy}, we train Bradley-Terry (BT) style reward models $r_{\theta}(\cdot)$ where a linear head added on the top of LLMs outputs a scalar reward. Given a pair of preference data $\{y_c,y_r\}$ to a prompt $x$, the objective is to optimize the reward gap between chosen response $y_c$ and rejected response $y_r$:
\begin{equation}\nonumber
    \mathcal{L} = -\log \left( \sigma \big( r_{\theta}(x, y_c) - r_{\theta}(x, y_r) \big) \right),
\end{equation}
where the sigmoid function $\sigma(\cdot)$ generates the probability of $y_c$ preferred than $y_r$.

We use Llama3.2-3B-Instruct and Llama3.1-8B-Instruct (abbreviated as Llama3-3B and Llama3-8B) to initialize the BT models, training them on \dataset{} for 2 epochs with a batch size of 128 and a learning rate of 9e-6. To evaluate our reward models, we compare them against three state-of-the-art reward models that rank highly on RewardBench~\citep{Lambert2024-hl}, as well as three popular LLMs, which are prompted to rank responses (see Box~\ref{box:apx-rm-eval-prompt} for the prompt). 

The overall results on \dataset{} testing response pairs are shown in Table~\ref{tab:acc}. Our reward models significantly outperform all other reward models and generative LLMs, achieving notably high accuracy and ROC AUC Score~\citep{Bradley1997-zg} on the \dataset{} testing set. These results suggest that \dataset{} provides robust supervision for training powerful reward models capable of effectively ranking responses to client speeches. We also calculate the Expected Calibration Error (ECE)~\citep{Naeini2015-ak} and Brier Score\footnote{\url{https://en.wikipedia.org/wiki/Brier_score}} to assess the calibration level of the models. The results demonstrate that our reward models have comparable and low ECE values to the state-of-the-art reward model, Llama-3.1-Nemotron-70B-Reward, while achieving significantly better Brier Scores. This indicates that our reward models could give more reliable rewards and preference probability. 

\subsection{Policy Model}
\input{tables/rlhf-main}
To further verify the effectiveness of \dataset{} and the trained reward models, we employ two preference alignment methods to optimize base models. \textbf{1) DPO}: we directly optimize the DPO~\citep{Rafailov2023-jm} objective on \dataset{}:

\begin{equation}\nonumber
\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}}) = 
-\mathbb{E}_{(x, y_c, y_r) \sim \mathcal{D}} 
\left[ 
\log \sigma \left( 
\beta \log \frac{\pi_{\theta}(y_c \mid x)}{\pi_{\text{ref}}(y_c \mid x)}
- \beta \log \frac{\pi_{\theta}(y_r \mid x)}{\pi_{\text{ref}}(y_r \mid x)}
\right) 
\right].    
\end{equation}

\textbf{2) DPO-Iter}: we follow an iterative approach~\citep{Pang2024-ag}, where, in each iteration, 8 responses are generated for each client speech and ranked by the reward model of the same size as the base model. The responses with the highest and lowest rewards are then annotated as online preference pairs, which are used to train the base model with the DPO objective. The client speeches for each iteration are 6400 sampled from the train set of \dataset{}. We use Llama3.2-3B-Instruct and Llama3.1-8B-Instruct as the base models. The training configuration includes a batch size of 64, a learning rate of 5e-7, and a total of 1,600 training steps. A development set comprising 10\% of the training set from \dataset{} is used to select the best checkpoints. We set the value of $\beta$ as 0.1 for DPO across all the experiments.

\noindent\textbf{Evaluation} We use LLM-as-judge~\citep{Zheng2023-df} to effectively approximate human preferences (validated by the human experts) for evaluation. We prompt the model to generate responses for the testing client speeches in \dataset{} and leverage GPT-4o to compare these responses against those of GPT-4o using the proposed PsychoCounsel Principles. Specifically, we prompt the model in two settings: \textbf{1) w/o Length Constraint}: The models are instructed to act as therapists and respond to the given client speech without any restrictions on response length. \textbf{2) w/ Length Constraint}: To ensure a fairer comparison with GPT-4o, we impose a length constraint, requiring the models to generate responses of similar length to those produced by GPT-4o. The overall win rates of the models against GPT-4o are calculated for comparison. We also show the win rates for the coarse topic categories.

\noindent\textbf{Main Results} As shown in Table~\ref{tab:win_rate}, in the w/o Length Constraint setting, the base models have low probabilities of outperforming GPT-4o. However, the models after alignment demonstrate significantly higher win rates against GPT-4o, indicating that supervision from \dataset{} effectively guides the models in learning how to respond to client speeches. Notably, Llama3-8B(+DPO-Iter) achieves the best performance, with a high overall win rate of 87.0\% against GPT-4o. This result suggests that online training and larger model sizes can potentially enhance generation quality, and models with approximately 8B parameters can effectively develop the skills to respond to client speeches under the guidance of reward models trained on \dataset{}.
Compared to models in the w/o Length Constraint setting, those in the w/ Length Constraint setting generally have lower win rates against GPT-4o. We attribute this to the stricter generation constraint, which requires our models to align their response length with that of GPT-4o. However, our model, Llama3-8B (+DPO-Iter), still achieves a high win rate of 77\% against GPT-4o, demonstrating that with proper training, the model can develop a robust ability to effectively respond to clients, regardless of generation constraints such as response length. We refer to the best model Llama3-8B (+DPO-Iter) as \policyl{}.

\noindent\textbf{Human Evaluation}\label{human-eval} We instruct the hired psychotherapists to provide preference judgments between the 200 randomly sampled response pairs generated by \policyl{} and GPT-4o, among which 100 for w/o Length Constraint setting and 100 for w/ Length Constraint. The provided order is shuffled to eliminate any position bias in the evaluation. In 82.5\% of cases, GPT-4o and human experts made the same judgments, indicating that \textbf{GPT-4o serves as a reliable evaluator for assessing psycho-counseling responses}. Figure~\ref{fig:human-win-rate} presents the human experts' comparison between the two models based on the annotation principles. Overall, \textbf{real experts clearly prefer the outputs of \policyl{} across both evaluation settings and nearly all principles}. Only if no length constraint is applied, \policyl{} exhibits lower clarity compared to GPT-4o. This is primarily because \policyl{} tends to generate longer responses, which aligns with the observed phenomenon that as LLMs develop more complex capabilities through reinforcement learning (RL), they tend to produce more tokens~\citep{DeepSeek-AI2025-te}. However, in the w/ Length Constraint setting, where models generate responses of similar length, \policyl{} demonstrates better performance in \textit{Clarity}, \textit{Safety} and \textit{Staging}. This suggests that applying a length constraint after RL training is a promising approach to obtain more balanced and desirable generations.

Additionally, higher win rates only indicate the \textbf{relative} quality of responses. To provide an \textbf{absolute} assessment of the responses generated by \policyl{}, we instructed experts to assign fine-grained scores based on the PsychoCounsel Principles. Figure~\ref{fig:human-score} presents the average scores of 100 randomly sampled responses, evaluated by experts under two different settings. We observe that, except for \textit{Clarity} in the w/ Length Constraint setting, \policyl{} achieves consistently high scores ($>$4) across all principles, indicating a strong alignment with the criteria for effective responses in psycho-counseling.


\begin{figure}[tbp]
    \centering  % Centers the entire figure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/win-rate-batch-2-2-ori.pdf}
        \caption{w/o Length Constraint}
        \label{}
    \end{subfigure}
    \hspace{0.02\textwidth}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/win-rate-batch-2-1-ori.pdf}
        \caption{w/ Length Constraint}
        \label{}
    \end{subfigure}

    \caption{Experts' Comparison between GPT-4o and PychoChat-Llama3-8B in Two Settings}
    \label{fig:human-win-rate}
\end{figure}


\subsection{Ablation Study}
To explore the differences between training on offline vs online data and base models with different sizes, we set up a controlled experimental group. In this setup, base models are trained by DPO with two different sets of preference data, one is offline preferences from \dataset{}, and the other is trained using online preferences generated by the base model. Responses are selected by the reward model of the same size trained on \dataset{}. All the other experimental settings are kept identical with the training epoch as 1, learning rate as 5e-7, and global batch size as 64. Figure~\ref{fig:online-offline} illustrates the win rates of checkpoints against GPT-4o on the testing client speeches of \dataset{}. 

In general, training on online samples demonstrates clear advantages over offline training: \textbf{1) Better Performance}: For Llama3-3B, training with online data (green line) consistently achieves a higher win rate compared to training with offline data (orange line). Similarly, for Llama3-8B, training with online data (blue line) delivers performance comparable to the best checkpoints of training with offline data (red line). \textbf{2) Greater Stability}: Both offline training curves exhibit a pronounced hump-shaped pattern, a known indicator of reward hacking~\citep{Rafailov2024-ll}. In contrast, the performance of online training models remains more stable across training steps. \textbf{3) Enhanced Compatibility with Smaller LLMs}: Online samples enable Llama3-3B to perform on par with, and sometimes even surpass, Llama3-8B models. This highlights the significant potential of combining online training with smaller LLMs. We can also observe that in this setting, training online can be viewed as DPO-Iter with only one round of updates on the policy model. However, its performance lags significantly behind DPO-Iter (shown in Table~\ref{tab:win_rate}), indicating that utilizing online generations from the latest updated policy is crucial for more effective online preference learning.

\begin{figure}[htbp]
    \centering  % Centers the entire figure
    \begin{minipage}[b]{0.36\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/human-score.pdf}
        \caption{Absolute Scores}
        \label{fig:human-score}
    \end{minipage}
    \hspace{0.02\textwidth}
    \begin{minipage}[b]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/online-offline.pdf}
        \caption{Comparison of Training Online or Offline}
        \label{fig:online-offline}
    \end{minipage}
\end{figure}

\subsection{Case Study}
We sample testing cases and compare the outputs of GPT-4o with those of our best policy model, \policyl{}. Generally, \policyl{} provides better responses than GPT-4o. Table~\ref{tab:case1} provides a typical example in the w/ Length Constraint setting. We could see that the response of \policyl{}, not only \darkred{validates the client’s distress with deep empathy}—acknowledging both her emotional burden and the courage it took to share—but also \blue{engages her in a detailed, collaborative exploration of her experiences}. By inviting her to pinpoint specific patterns and triggers behind her fears, Response 1 promotes self-exploration and empowerment, making it particularly effective for someone in the early stages of considering change. In contrast, the response of GPT-4o is \red{general and less detailed}, which can make the client feel less deeply understood. We provide more cases in Appendix~\ref{sec:apx-case}.

\input{tables/cases}