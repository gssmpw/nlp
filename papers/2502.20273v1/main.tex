
% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{makecell}
\usepackage{array}
\usepackage{multirow}
\usepackage{dirtytalk} % quotation marks

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage[final,nopatch=footnote]{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{rotating}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{soul}
\usepackage{url}
\usepackage{float}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{listings}
\usepackage{tabularray}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{lscape}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage{subcaption}    % For subfigures


% For column spacing
\newcommand\Tstrut{\rule{0pt}{1.5em}}       % "top" strut
\newcommand\Bstrut{\rule[-1.5em]{0pt}{0pt}} % "bottom" strut
\newcommand{\TBstrut}{\Tstrut\Bstrut} % top&bottom struts
% For cell spacing
\newcommand\TstrutCell{\rule{0pt}{1.0em}}       % "top" strut
\newcommand\BstrutCell{\rule[-0.5em]{0pt}{0pt}} % "bottom" strut
\newcommand{\TBstrutCell}{\TstrutCell\BstrutCell} 


\lstdefinelanguage{PythonRegex}{
    morekeywords={r},
    morestring=[b]',
    morestring=[b]",
    morecomment=[l]{\#}
}

\lstset{
    language=PythonRegex,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    stringstyle=\color{black},
    commentstyle=\color{gray},
    frame=single,
    breaklines=true,
    captionpos=b
}

% If the title and author information does not fit in the area allocated, uncomment the following
% \setlength\titlebox{7cm}
% and set <dim> to something 5cm or larger.

\title{How Much is Enough?\\The Diminishing Returns of Tokenization Training Data}



\author{
\vspace{1.3mm}
Varshini Reddy\textsuperscript{\dag} 
\quad Craig W. Schmidt\textsuperscript{\dag} 
\quad Yuval Pinter\textsuperscript{\S} 
\quad Chris Tanner\textsuperscript{\dag,\P} \\
\begin{tabular}{ccc}
      \textsuperscript{\dag}Kensho Technologies & \textsuperscript{\S}Ben-Gurion University &  \textsuperscript{\P}MIT \\
     Cambridge, MA  & Beer Sheva, Israel &  Cambridge, MA \\
\end{tabular} \\
\texttt{\small\{varshini.bogolu,craig.schmidt,chris.tanner\}@kensho.com},\,
\texttt{\small uvp@cs.bgu.ac.il} \\
}

\newcommand{\todot}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\cws}[1]{\textcolor{blue}{[Craig: #1]}}
\newcommand{\uvp}[1]{\textcolor{cyan}{[Yuval: #1]}}
\newcommand{\ct}[1]{\textcolor{green}{[Chris: #1]}}
\newcommand{\vr}[1]{\textcolor{orange}{[Varshini: #1]}}  

\date{}

\begin{document}
\maketitle

\begin{abstract}


Tokenization, a crucial initial step in natural language processing, is often assumed to benefit from larger training datasets. This paper investigates the impact of tokenizer training data sizes ranging from 1GB to 900GB. Our findings reveal diminishing returns as the data size increases, highlighting a practical limit on how much further scaling the training data can improve tokenization quality. We analyze this phenomenon and attribute the saturation effect to the constraints imposed by the pre-tokenization stage of tokenization. These results offer valuable insights for optimizing the tokenization process and highlight potential avenues for future research in tokenization algorithms.
\end{abstract}


\newcommand{\mtr}[2]{\multirow{#1}{*}{\textbf{#2}}}
\newcommand{\mtc}[2]{\multicolumn{#1}{c}{\textbf{#2}}}
\newcommand{\mtcb}[2]{\multicolumn{#1}{|c|}{\textbf{#2}}} % with border
\newcommand{\mrt}[2]{\multirow{#1}{*}{\rotatebox{90}{#2}}}

\definecolor{C_SECOND_SOFT}{HTML}{e2a8d6}
\definecolor{C_BASE_SOFT}{HTML}{66adbf}

\definecolor{C_BASE}{HTML}{a2c4c9}
\definecolor{C_SECOND}{HTML}{B62699}
\definecolor{C_THIRD}{HTML}{00B9E8}
\definecolor{C_FOURTH}{HTML}{0B008F}
\definecolor{C_BACKGROUND}{HTML}{EDF5FC}

\section{Introduction}
\label{sec:intro}

Tokenizers are a foundational component of any NLP pipeline, as they are responsible for converting raw text into useful sequences of indexed tokens.
Practitioners often default to standard tokenization algorithms such as Byte-Pair Encoding~\citep[BPE;][]{sennrich-etal-2016-neural}, UnigramLM~\citep{kudo-2018-subword} or WordPiece~\citep{wordpiece,devlin-etal-2019-bert}, sourced directly from libraries such as Hugging Face.\footnote{\scriptsize\url{https://github.com/huggingface/tokenizers}}
The training process of a tokenizer involves using a corpus of training data and a specific tokenization algorithm to generate a fixed-size vocabulary, usually containing between 32,000 and 128,000 tokens.

Extensive research explores the influence of a \textit{model's} training data on LLM performance~\citep{scalinglawspretrainingagents,zhang2024when,hoffmann2022trainingcomputeoptimallargelanguage,kaplan2020scalinglawsneurallanguage}. However, the impact of a \emph{tokenizer's} training data remains relatively unexplored. Recent work has begun to address the importance of tokenizers' vocabulary sizes and the training data domains~\citep{gettingtokenizerpretrainingdomain}. Prior work has also explored various aspects of tokenization, including the influence of different tokenization algorithms~\citep{schmidt-etal-2024-tokenization,ali-etal-2024-tokenizer,wordscharactersbriefhistory,survey-tok-algo}, vocabulary size optimization~\citep{gowda-may-2020-finding}, and the interplay between data type and tokenization strategy, especially in multi-lingual applications~\citep{limisiewicz-etal-2023-tokenization,rust-etal-2021-good}.
Yet, to the best of our knowledge, we are the first to investigate how much training data is needed for a tokenizer, and how this affects performance.


%tRecent work has begun to address the importance of vocabulary size and training data domain for tokenizers~\citep{gettingtokenizerpretrainingdomain}, but, to the best of our knowledge, we are the first to investigate the 
%the question of scaling the data for tokenizer training remains open.

%However, the specific effect of training data size on tokenizer performance has yet to be thoroughly investigated.

We address this by examining the impact of scaling tokenizer training data with sizes ranging from 1GB to 900GB. We use English BPE, UnigramLM, and WordPiece tokenizers with vocabulary sizes of 40,960, 64,000, 128,000, and 256,000. For each tokenization, we examine the proportion of vocabulary tokens that are shared with the 900GB reference case, and we use intrinsic metrics to measure the quality of tokenization on a fixed evaluation corpus.

Our results demonstrate that increasing the amount of tokenizer training data leads to diminishing returns, indicating a saturation point where further data provides minimal to no improvements in tokenization quality. Finally, we examine the proportion of pre-tokenization chunks that exactly match a single token in the tokenizer vocabulary, and suggest that this very high proportion is a possible explanation for these diminishing returns. 

\input{scaling_section_2}

% the old parked version
% \input{old_pretokenization_section_3}

\section{The Limiting Role of Pre-Tokenization}
\label{sec:pretokenization}

Pre-tokenization is the initial step in tokenization, which uses regular expressions to split a document into chunks, which are then tokenized separately. \citet{velayuthan-sarveswaran-2025-egalitarian} recently noted that pre-tokenization can have a greater effect on the resulting tokenization than the choice of tokenization algorithm. They call the chunks resulting from the pre-tokenization phase \emph{pre-tokens}.

The weighted Jaccard results in the previous section show that tokenizers produce a core set of common tokens across the range of tokenizer training data. We hypothesize that this is due to pre-tokenization. The highest frequency pre-tokens are prevalent enough that all three tokenization algorithms have a strong incentive to find a single token that exactly matches the pre-token. 
To investigate this, we go back to the aggregated pre-tokens with their associated counts.\footnote{See \cref{app:scaling-pretokenization} for more details on the pre-tokenization aggregation process.}
We calculate the proportion of pre-tokens represented as a single token within each tokenizer's vocabulary. \cref{fig:pre-tokenization} displays this proportion for BPE, UnigramLM, and WordPiece tokenizers, for varying vocabulary sizes.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/Pre-tokenization/pre-tokenization-combined.png} 
    \caption{Proportion of pre-tokens represented as single tokens in BPE, UnigramLM and WordPiece vocabularies of varying sizes (40,960, 64,000, 128,000, and 256,000) with increasing training data.}
    \label{fig:pre-tokenization}
\end{figure}



The prevalence of these common pre-tokens within the tokenizers' vocabularies is remarkably high across all algorithms, increasing with vocabulary size. This proportion remains relatively stable with more training data at smaller vocabulary sizes and is essentially overlapping at vocabulary sizes of 128,000 and 256,000. The curves are flat because they represent frequent pre-tokens, which are easily found even with very little training data. Larger vocabularies naturally have more capacity to include a greater number of pre-tokens as single tokens, accounting for the overlapping curves.

These observations support our hypothesis that the plateauing intrinsic metrics (\cref{fig:intrinsic_bpe}) and high weighted Jaccard (\cref{fig:avg_downstream_jaccard_40960}) observed with varying training data can at least be partially attributed to the constraints imposed by pre-tokenization. The pre-tokenization step prioritizes the inclusion of commonly occurring pre-tokens as single tokens in the training process. The tokenizers are limited to optimizing the smaller remaining fraction of the tokenized corpus. This limits the tokenizers' ability to fully leverage larger training datasets, as the core vocabulary is largely predetermined by the pre-tokenization process. The additional tokens produced from larger datasets are primarily low-frequency items (i.e., rare words), which have a smaller overall impact on the vocabulary composition and tokenization quality as measured by our intrinsic metrics.

\section{Conclusion}
\label{sec:conclusion}

In this work, we have systematically investigated the impact of tokenizer training data size on the characteristics of trained tokenizers.
Our findings reveal diminishing returns as the tokenizer training data increases beyond 150GB to 180GB.
Our analysis indicates that tokenization algorithms incorporating pre-tokenization may be fundamentally limited in their ability to fully leverage extremely large datasets.
Therefore, rather than focusing solely on \emph{more data}, we advocate for a shift towards developing and employing better vocabulary training methods that are less susceptible to the limitations of pre-tokenization.
Methods that incorporate contextual signals, such as SaGe~\cite{yehezkel-pinter-2023-incorporating}, offer a promising direction for future research.

\section*{Limitations}
This study has several limitations.
First, both our training corpus and most of our evaluation data are English-centric.
This monolingual focus restricts the generalizability of our findings, as the observed trends in tokenizer performance with increasing data size may not be extrapolated to other languages with different morphological structures or tokenization needs.
Future work should explore the impact of data scaling on tokenizers trained and evaluated on diverse language corpora.
Second, our reliance on intrinsic tokenizer metrics as a primary means of evaluation, while enabling efficient analysis, may not fully capture the complex interplay between tokenizer characteristics and downstream LLM performance.
While these metrics provide valuable insights into tokenizer quality, they serve as a proxy for true downstream effectiveness.
Future research should investigate the correlation between intrinsic metrics and downstream task performance across a wider range of language models and tasks to establish a more comprehensive evaluation framework.

\section*{Ethics Statement}
We train our tokenizers on the commonly used public datasets The Pile \cite{pile} and RedPajama \cite{redpajama}, which have not undergone a formal ethics review. While our evaluation set was manually anonymized and checked for abusive language, it may still contain personal opinions that reflect cultural, political, or demographical biases.

\section*{Acknowledgments}
We thank Seth Ebner for many notes and discussions.
This research was supported in part by the Israel Science Foundation (grant No. 1166/23).


% \bibliographystyle{acl_natbib}
\bibliography{anthology,references}

\appendix
\input{appendix}

\end{document}