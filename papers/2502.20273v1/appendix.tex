\clearpage

\section{Scaling Tokenizer Training using Pre-tokenization}
\label{app:scaling-pretokenization}

To enable training tokenizers on datasets ranging to hundreds of gigabytes, we implemented a parallel pre-tokenization step, which used a regular expression to break the documents into chunks. Then we aggregated the counts of each pre-tokenized chunk over all the documents. This pre-tokenized corpus of chunks and counts served as the input for all subsequent tokenizer training.  This approach significantly reduces the computational overhead of tokenizer training by avoiding tokenizing common chunks more than once. 

We based our BPE implementation on MinBPE,\footnote{\url{https://github.com/karpathy/minbpe}}\label{fn:minbpe} modified to work with aggregate chunk counts. It was also modified to efficiently compute the change in pairwise counts, rather than recomputing them from scratch after each merge. We used these same aggregate chunks and counts in training UnigramLM and WordPiece, rather than their native pre-tokenization routines.

\autoref{lst:gpt4} is the regular expression used by GPT-4 for pre-tokenization\addtocounter{footnote}{-1}\footnotemark, which we also used for our parallel pre-tokenization.

\begin{lstlisting}[language=PythonRegex, caption={GPT-4 pre-tokenizer regular expression}, label={lst:gpt4}]
r"(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]|\s[\r\n]|\s+(?!\S)|\s+"
\end{lstlisting}

Here is an explanation of each branch of the regular expression:

\begin{itemize}
    \item\begin{verbatim}(?i:[sdmt]|ll|ve|re)\end{verbatim}  Matches English language contractions or possessives
    \item\begin{verbatim}[^\r\n\p{L}\p{N}]?+\p{L}+\end{verbatim} Matches one or more Unicode letters, optionally preceded by a punctuation or symbol character
    \item\begin{verbatim}\p{N}{1,3}\end{verbatim} Matches one to three Unicode digits (some languages such as Thai and Tamil use different Unicode symbols for digits)
    \item\begin{verbatim} ?[^\s\p{L}\p{N}]++[\r\n]*\end{verbatim} Matches one or more punctuation or symbol characters, optionally preceded by a space, and succeeded by zero or more carriage returns or linefeeds
    \item\begin{verbatim}\s*[\r\n]\end{verbatim}  Matches a single carriage return or linefeed, optionally preceded by zero or more characters of whitespace
    \item\begin{verbatim}\s+(?!\S)\end{verbatim} Matches one or more characters of whitespace not immediately followed by non-whitespace, thus matching trailing whitespace of a line, up to the line break
    \item\begin{verbatim}\s+\end{verbatim} Finally, this matches one or more characters of whitespace
\end{itemize}

This regular expression requires the more flexible \verb|regex| package in Python rather than the default \verb|re| package, in order to support Unicode character groups and the possessive quantifiers \verb|?+| and \verb|++|.

Note that this expression has the necessary property to match all characters in any valid UTF-8. This is because \verb|[^\s\p{L}\p{N}]| in the fourth branch will match any character that is not a letter, number, or whitespace, which are all handled by other parts of the regex. \citet{gettingtokenizerpretrainingdomain} had a similar explanation of this same regex, plus other possible regular expression choices.\footnote{See \url{https://tokencontributions.substack.com/p/pre-tokenization-on-punctuation-in} for other discussions on peculiarities of this particular regular expression.}

\section{Analysis of Common Vocabulary}
\label{app:heatmaps-vocab-sizes}

As seen in \cref{fig:common_vocab_40960,fig:common_vocab_rest}, as the vocabulary size increases from 40,960 to 256,000, we observe a consistent trend where the proportion of common vocabulary stabilizes at a higher value for larger datasets.
BPE and Unigram tokenizers exhibit similar convergence patterns, with BPE maintaining a slightly higher proportion of common vocabulary across all vocabulary sizes.
WordPiece shows a more gradual increase, particularly at larger vocabulary sizes, suggesting a higher sensitivity to increasing data volume.
In particular, at a vocabulary size of 256,000, initial fluctuations are more pronounced, but all tokenizers eventually converge as data scales.


\begin{figure}[!ht]
    \centering
    % First subfigure (occupies the full width)
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Common_Vocab_Plot/Common_vocab_64000.png}
        \caption{Vocabulary size 64,000}
        \label{fig:common_vocab_64000}
    \end{subfigure}
    
    \vspace{1em}  % Add some vertical space between subfigures (optional)

    % Second subfigure (occupies the full width)
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Common_Vocab_Plot/Common_vocab_128000.png}
        \caption{Vocabulary size 128,000}
        \label{fig:common_vocab_128000}
    \end{subfigure}
    
    \vspace{1em}  % More vertical space

    % Third subfigure (occupies the full width)
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Common_Vocab_Plot/Common_vocab_256000.png}
        \caption{Vocabulary size 256,000}
        \label{fig:common_vocab_256000}
    \end{subfigure}

    \caption{Proportion of common vocabulary for BPE, Unigram, and WordPiece tokenizers, trained with cumulatively increasing data, relative to the vocabulary of the corresponding tokenizer trained with 900GB of data.}
    \label{fig:common_vocab_rest}
\end{figure}



Figures \ref{fig:heatmap_bpe}, \ref{fig:heatmap_unigram}, and \ref{fig:heatmap_wordpiece} present heatmaps visualizing the proportion of shared vocabulary among all trained tokenizers for BPE, Unigram, and WordPiece, respectively.  Within each figure, subplots correspond to different vocabulary sizes: 40,960, 64,000, 128,000, and 256,000.

A key observation for both BPE (Figure \ref{fig:heatmap_bpe}) and UnigramLM (Figure \ref{fig:heatmap_unigram}) is the decreasing proportion of shared vocabulary between consecutively trained tokenizers as the vocabulary size increases.  For instance, with BPE and a 40,960 vocabulary, the vocabulary overlap between tokenizers trained on 30GB and 900GB of data is 0.57. This overlap decreases to 0.49, 0.41, and 0.23 for vocabulary sizes of 64,000, 128,000, and 256,000, respectively.  UnigramLM tokenizers exhibit a similar trend, with overlap decreasing from 0.58 to 0.51, 0.34, and 0.2 across the same vocabulary sizes.

This trend suggests that as vocabulary size increases for a fixed training data size (e.g., 30GB), the added tokens become progressively less frequent and more specialized.  These less frequent or more niche tokens are also more susceptible to variation between tokenizers trained on slightly different subsets of data drawn from the same overall distribution.  Essentially, with limited training data, larger vocabularies are populated with progressively less consequential tokens, leading to lower agreement between trained tokenizers.  However, WordPiece (Figure \ref{fig:heatmap_wordpiece}) does not exhibit any trend with a change in vocabulary size.


\begin{figure*}[!ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/BPE-40960.png}
        % \caption{40,960}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/BPE-64000.png}
        % \caption{64,000}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/BPE-128000.png}
        % \caption{128,000}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/BPE-256000.png}
        % \caption{256,000}
    \end{subfigure}
    \caption{Heatmaps showing the proportion of common vocabulary across all \textbf{BPE} tokenizers as a function of cumulatively increasing training data. Each heatmap, in the grid from the top left, represents a different vocabulary size (40,960, 64,000, 128,000, 256,000).}
    \label{fig:heatmap_bpe}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/Unigram-40960.png}
        % \caption{40,960}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/Unigram-64000.png}
        % \caption{64,000}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/Unigram-128000.png}
        % \caption{128,000}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/Unigram-256000.png}
        \caption{256,000}
    \end{subfigure}
    \caption{Heatmaps showing the proportion of common vocabulary across all \textbf{UnigramLM} tokenizers as a function of cumulatively increasing training data. Each heatmap, in the grid from the top left, represents a different vocabulary size (40,960, 64,000, 128,000, 256,000).}
    \label{fig:heatmap_unigram}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/WordPiece-40960.png}
        % \caption{40,960}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/WordPiece-64000.png}
        % \caption{64,000}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/WordPiece-128000.png}
        % \caption{128,000}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/Heatmaps/WordPiece-256000.png}
        % \caption{256,000}
    \end{subfigure}
    \caption{Heatmaps showing the proportion of common vocabulary across all \textbf{WordPiece} tokenizers as a function of cumulatively increasing training data. Each heatmap, in the grid from the top left, represents a different vocabulary size (40,960, 64,000, 128,000, 256,000).}
    \label{fig:heatmap_wordpiece}
\end{figure*}


\section{Performance Analysis using Intrinsic Metrics}
\label{app:intrinsic-vocab-sizes}

Figure~\ref{fig:intrinsic_unigram} displays the intrinsic metrics for the Unigram tokenizers trained with varying data sizes and vocabulary sizes. Similar to the observations with BPE, the Unigram tokenizers do not show a substantial and consistent improvement in intrinsic scores with increasing data size. While the Cognitive Score score exhibits a slight upward trend initially, it plateaus beyond approximately 180GB, suggesting that further increases in training data do not significantly enhance the tokenizer's alignment with morphological segmentations. The Average F1, calculated over the morphological benchmarks, while showing some fluctuations, does not demonstrate a clear and sustained improvement with larger datasets. The Entropy Score, similar to the other metrics, plateaus relatively early, indicating that the balance of token frequencies stabilizes with increasing data size.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/Intrinsic_Metrics/Unigram.png} 
    \caption{Intrinsic measures of \textbf{Unigram} tokenizers trained for each of the four vocabulary sizes with scaled training data}
    \label{fig:intrinsic_unigram}
\end{figure}

For WordPiece (Figure~\ref{fig:intrinsic_wordpiece}), a similar pattern emerges. While slightly higher variations exist, particularly in Cognitive Score, the intrinsic scores do not substantially improve with increasing data size.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/Intrinsic_Metrics/WordPiece.png} 
    \caption{Intrinsic measures of \textbf{WordPiece} tokenizers trained for each of the four vocabulary sizes with scaled training data}
    \label{fig:intrinsic_wordpiece}
\end{figure}


\section{Analysis on Evaluation Data}
\label{app:downstream-others}

\subsection{Data Source}
Our tokenizer training data spans from 2021 to early 2023. To prevent data leakage into our evaluation set, we sourced documents from various domains specifically dated 2024. We manually de-anonymized and removed any offensive language from this dataset. The following describes the composition of our evaluation data for each domain:

\begin{itemize}
    \item \textit{Biology}: We utilized research papers and clinical trial data from the National Library of Medicine.\footnote{\scriptsize\url{https://www.ncbi.nlm.nih.gov/}}
    \item \textit{Code}: Code was generated using GPT-4 for randomly selected programming problems drawn from IIT\footnote{\scriptsize\url{https://www.cse.iitk.ac.in/users/nitin/courses/CS681-2019-20-II/problemsets.html}} homework assignments and LeetCode.\footnote{\scriptsize\url{https://leetcode.com/}} The code corpus contains a mixture of popular programming languages including Python, Java, JavaScript, Ruby, and Go, further enhancing diversity.
    \item \textit{Finance}: This dataset comprises SEC filings\footnote{\scriptsize\url{https://www.sec.gov/}} from various companies filed in 2024, downloaded and extracted from PDF to text format.
    \item \textit{History}: We used papers from Oxford Academic Historical Research. \footnote{\scriptsize\url{https://academic.oup.com/histres/}}
    
    \item \textit{Legal}: This dataset consists of 2024 - Opinions of the Court released by the Supreme Court of the United States.\footnote{\scriptsize\url{https://www.supremecourt.gov/opinions/slipopinion/24}}
    \item \textit{Math}: Mathematics papers from arXiv,\footnote{\scriptsize\url{https://arxiv.org/archive/math}} all released in 2024, were used.
    \item \textit{General Conversation}: We employed general conversation data from the same population as our tokenizer training data, ensuring no overlap between training and evaluation sets.
\end{itemize}

\subsection{Analysis}

As mentioned in \cref{subsec:jaccard-analysis}, we assess the impact of scaling training data for tokenizer training on the evaluation set by computing the Jaccard Index and weighted version to the Jaccard Index between the actual tokens used in the evaluation text using each trained tokenizer and the same text tokenized with the 900GB-trained reference tokenizer. Within the evaluated vocabulary size of 40,960 (see \cref{fig:downstream_jaccard_40960}), slightly different trends emerge across different tasks. However, both Byte Pair Encoding (BPE) and Unigram tokenizers exhibit a near-immediate plateau in performance, around 120GB to 180GB. This suggests that, at this vocabulary size, frequent tokens contribute substantially to the overall token overlap and that these tokenizers quickly converge to a stable segmentation of these frequent terms.  This early plateau indicates that further increases in training data provide diminishing returns for these tokenization algorithms. 

Although WordPiece demonstrates a similar trend of a much lower Jaccard Index compared to its weighted counterpart, it exhibits the widest gap between the two Jaccard Index values. A large difference between the Jaccard Index and the Weighted Jaccard Index indicates that the overlap between vocabulary usage is primarily due to less frequent tokens, while the more frequent tokens are not consistently shared. This could mean that the change in vocabulary might have higher implications for the tokenization of text, relative to BPE and UnigramLM. These patterns remain consistent across WordPiece for the remaining vocabulary sizes, as seen in \cref{fig:downstream_jaccard_40960,fig:downstream_jaccard_64000,fig:downstream_jaccard_128000,fig:downstream_jaccard_256000}.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/Jaccard/Jaccard_Index_Compiled_40960.png} 
    \caption{Jaccard Index (open markers) and Weighted Jaccard Index (WTD; filled markers) for BPE, UnigramLM, and WordPiece tokenizers (vocab size \textbf{40,960}) across varying data sizes, for different domains.}
    \label{fig:downstream_jaccard_40960}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/Jaccard/Jaccard_Index_Compiled_64000.png} 
    \caption{Jaccard Index (open markers) and Weighted Jaccard Index (WTD; filled markers) for BPE, UnigramLM, and WordPiece tokenizers (vocab size \textbf{64,000}) across varying data sizes, for different domains.}
    \label{fig:downstream_jaccard_64000}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/Jaccard/Jaccard_Index_Compiled_128000.png} 
    \caption{Jaccard Index (open markers) and Weighted Jaccard Index (WTD; filled markers) for BPE, UnigramLM, and WordPiece tokenizers (vocab size \textbf{128,000}) across varying data sizes, for different domains.}
    \label{fig:downstream_jaccard_128000}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/Jaccard/Jaccard_Index_Compiled_256000.png} 
    \caption{Jaccard Index (open markers) and Weighted Jaccard Index (WTD; filled markers) for BPE, UnigramLM, and WordPiece tokenizers (vocab size \textbf{256,000}) across varying data sizes, for different domains.}
    \label{fig:downstream_jaccard_256000}
\end{figure*}


% lets leave this out for now
% \input{ablation_study}





