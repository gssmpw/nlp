\section{Effect of Training Corpus Size on Tokenization}
\label{sec:scaling}

% In our experiments, we focus on the effect of the tokenizer training corpus size, over various vocabulary sizes and tokenizers.

Our training corpus combines the de-duplicated PILE~\citep{pile} and RedPajama~\citep{redpajama} datasets, totaling 900GB of text. We train three tokenizers -- BPE,\footnote{We use a custom BPE tokenizer based on \scriptsize \url{https://github.com/karpathy/minbpe} \footnotesize as a starting point.} UnigramLM,\footnote{\scriptsize \url{https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py}} and WordPiece\footnote{\scriptsize{\url{https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/bert_wordpiece.py}}} -- on progressively larger subsets of the randomly shuffled 900GB corpus.

For smaller dataset sizes, we used 1GB, 5GB, and 15GB subsets. For larger scales, we started with a 30GB subset of our 900GB corpus and cumulatively increased the training data in 30GB intervals up to the full 900GB. This resulted in 33 distinct training runs for each of the four vocabulary sizes for each of the three tokenizers, leading to a total of 396 trained tokenizers. See Appendix~\ref{app:scaling-pretokenization} for a complete description of the pre-tokenization process and our scaling methodology. 


\subsection{Vocabulary analysis}
\label{sec:analysis-vocab}


% This systematic variation in training data size enables us to precisely observe the effect of scale on the intrinsic metrics and Jaccard Index. 

\cref{fig:common_vocab_40960} shows the fraction of shared vocabulary between tokenizers trained on varying amounts of data and a 900GB-trained reference tokenizer using the same algorithm, for a vocabulary size of 40,960.
Vocabulary similarity increases with training data, rising from approximately 58\% to 97\% for BPE, from 40\% to 97\% for UnigramLM, and from 4\% to 92\% for WordPiece.
This trend is consistent across the other vocabulary sizes of 64,000, 128,000, and 256,000, as shown in \cref{app:heatmaps-vocab-sizes}.
Thus, larger training datasets consistently yield more similar vocabularies across algorithms, though BPE and UnigramLM exhibit a more rapid convergence than WordPiece. These results suggest that a substantial portion of the vocabulary learned from the full 900GB dataset can be obtained from tokenizers trained on significantly smaller fractions of the data.  For example, a tokenizer trained on roughly 150-180GB of data captures over 90\% of the vocabulary present in the 900GB-trained counterpart for BPE and UnigramLM, and over 80\% for WordPiece. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/Common_Vocab_Plot/Common_vocab_40960.png}
    \caption{Proportion of common vocabulary for BPE, UnigramLM, and WordPiece tokenizers (vocabulary size: 40,960) trained with cumulatively increasing data, relative to the vocabulary of the corresponding tokenizer trained with 900GB of data.}
    \label{fig:common_vocab_40960}
\end{figure}


\subsection{Intrinsic metrics analysis}
\label{sec:analysis-metrics}

While we have seen that there are significant differences in the vocabularies of tokenizers trained on, for example, 30GB versus 900GB of data, it is important to examine these differences to determine whether they translate into meaningful improvements in tokenization quality.
To evaluate the trained tokenizers without the additional computational overhead of training full LLMs, we use the intrinsic tokenizer metrics collected by \citet{uzan-etal-2024-greed}, summarized below: 

\begin{itemize}
    \item \textbf{Morphological alignment:} %(Average F1):}
    Measures how well a tokenizer's word segmentations match gold-standard morphological segmentations. Higher scores indicate a greater ability to capture word structure.
    \item \textbf{Cognitive score:} Assesses the correlation between a tokenizer's output and human performance in lexical decision tasks, evaluating how well the tokenizer's behavior aligns with human lexical processing.
    \item \textbf{Entropy score:} Evaluates token distribution quality by penalizing excessively high- or low-frequency tokens, aiming for a balanced representation.
\end{itemize}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/Intrinsic_Metrics/BPE.png} 
    \caption{Intrinsic measures of BPE tokenizers trained for each of the four vocabulary sizes with scaled training data. `Average F1' is calculated over morphological benchmarks.}
    \label{fig:intrinsic_bpe}
\end{figure}

\cref{fig:intrinsic_bpe} gives results on intrinsic metrics for BPE for each of the four vocabulary sizes included in our study.
Contrary to our expectations, these intrinsic measures do not reveal substantial performance gains with increasing data size.
In fact, for BPE, performance plateaus in the 150GB to 180GB range. Despite the observed vocabulary shifts, the core properties reflected by these metrics remain relatively stable across the range of training data volumes. 
% This suggests that while the vocabularies of the trained tokenizers evolve with increasing training data, the fundamental characteristics of the tokenizers remain largely consistent. 
Thus, simply increasing the training data size may not inherently lead to substantial improvements in a tokenizer's effectiveness. A discussion of the observed patterns for the remaining two kinds of tokenizer is presented in \cref{app:intrinsic-vocab-sizes}.

\subsection{Evaluation dataset analysis}
\label{subsec:jaccard-analysis}

To reconcile the apparent discrepancy between observed vocabulary shifts and stable intrinsic scores, we analyzed the impact of our trained tokenizers on a diverse, multi-domain evaluation dataset. This dataset spans several domains: biology, code, finance, history, legal, mathematics, and general text. This diverse composition mitigates potential tokenizer biases that arise from the influence of domain-specific vocabulary prevalence. Each domain-specific corpus contains 1.5 million characters. The sources of the evaluation set are discussed in \cref{app:downstream-others}.

We assessed the impact of scaling training data for vocabulary construction on the evaluation set by computing the Jaccard Index between the actual tokens used in the evaluation text using each trained tokenizer and the same text tokenized with the 900GB-trained reference tokenizer:
\begin{equation}
    J(U,V) = \frac{|U \cap V|}{|U \cup V|},
    \label{eq:jaccard}
\end{equation}
where $U$ and $V$ are the vocabularies of the reference tokenizer and the current tokenizer.

We also computed a weighted version of the Jaccard Index using the normalized token counts over the evaluation data, to account for the differences in the training set sizes:
\begin{equation}
    J_w(U, V) = \frac{\sum_{t \in U \cap V} \min(w_U(t), w_V(t))}{\sum_{t \in U \cup V} \max(w_U(t), w_V(t))},
    \label{eq:weighted_jaccard}
\end{equation}
where $w_U(t)$ and $w_V(t)$ are the normalized token frequencies for token $t$ in vocabularies $U$ and $V$.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/Jaccard/Average_Jaccard_Index_Compiled_40960.png} 
    \caption{Jaccard Index (open markers) and Weighted Jaccard Index (WTD; filled markers) for BPE, UnigramLM, and WordPiece tokenizers (vocabulary size \textbf{40,960}) across varying data sizes, averaged over all evaluation domains.}
    \label{fig:avg_downstream_jaccard_40960}
\end{figure}

\cref{fig:avg_downstream_jaccard_40960} presents the standard (open markers) and weighted (filled markers) Jaccard Index for tokenizers of 40,960 vocabulary size. The weighted scores consistently exceed the unweighted scores, highlighting the significant influence of token frequency on vocabulary overlap. This suggests that high-frequency tokens exhibit greater consistency across varying data sizes, and they account for a significant fraction of the training token counts.

Our analysis revealed that over 80\% of our evaluation text is represented by approximately 20\% of the tokenizer vocabulary (see Appendix~\ref{app:downstream-others} for detailed results across individual domains of our evaluation dataset and vocabulary sizes).
This finding suggests that the majority of tokens that are added with increasing training data are low-frequency and thus less consequential. While the specific composition of the vocabulary evolves with an increase in training data, the core set of tokens responsible for representing the majority of the text remains relatively stable. This observation is consistent with Zipf's law~\citep{zipf1949human}, which postulates an inverse relationship between word frequency and rank in natural language corpora. Thus, increasing a tokenizer's training data beyond a certain point primarily adds low-frequency tokens to the vocabulary, which has a limited impact on the overall tokenization characteristics captured by the intrinsic metrics. This is one explanation as to why the intrinsic metrics are largely unaffected by increases in training data.
