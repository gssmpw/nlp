The remarkable capabilities of large language models (LLMs) \cite{vaswani_attention_2017,devlin_bert_2019,raffel2020exploring,brown2020language,chowdhery2023palm} have raised fundamental questions about how these models acquire, represent, and utilize knowledge \cite{ju-etal-2024-large,meng2022locating,turpin2024language,zhang2024knowledge}. While LLMs demonstrate impressive few-shot and zero-shot learning abilities, understanding their knowledge application process remains challenging. \textit{Prompting} \cite{liu2023pre} has emerged as a crucial technique for accessing and directing LLMs' knowledge, leading to extensive research in both manual \cite{schick-schutze-2021-just,reynolds2021prompt} and automated prompting approaches \cite{gao-etal-2021-making,shin-etal-2020-autoprompt}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\columnwidth]{images/inter_front_example.png}
\label{fig:iao_front}
\caption{Illustration of IAO prompting demonstrating how knowledge is structured and applied through explicit Input-Action-Output steps. Each step's output becomes verified knowledge for subsequent reasoning.}
\end{figure}

Chain-of-Thought (CoT) prompting \cite{wei2022chain,wang-etal-2022-iteratively} represents a significant advancement in accessing LLMs' knowledge by making intermediate reasoning steps explicit. This approach guides LLMs to decompose complex knowledge application into sequential steps, similar to human reasoning processes. In zero-shot settings, simple prompts like "\texttt{let's think step by step}" have proven effective in activating LLMs' inherent reasoning capabilities \citep{kojima2022large}, suggesting that these models possess substantial implicit knowledge that can be systematically accessed.

However, a critical challenge persists: ensuring that LLMs' knowledge utilization is both interpretable and verifiable. As noted by \citet{singh2024rethinking}, understanding an LLM's knowledge application requires extracting and validating the relationships learned by the model. While CoT improves reasoning performance, it often fails to provide a clear mapping of how stored knowledge is accessed and applied in generating outputs step by step.

\begin{figure*}[ht]
\includegraphics[width=\textwidth]{images/inter_example.png}
\caption{Comparison of knowledge application between IAO prompting and zero-shot CoT using PALM-2 on GSM8k. IAO's structured format reveals how knowledge is accessed and applied at each step, while CoT misses crucial information.}
\label{fig:iao_example_fig}
\end{figure*}

This opacity in knowledge utilization poses significant challenges for verifying factual accuracy, identifying knowledge gaps, and ensuring reliable reasoning \cite{chen2024survey}. To address these challenges, we introduce IAO (Input-Action-Output) prompting, a structured template that explicitly models how LLMs access and apply their knowledge. Each reasoning step clearly delineates the input knowledge being used, the knowledge-based action being performed, and the resulting output, creating a transparent chain of knowledge utilization.

Our main contributions are:
(i) we propose IAO prompting, a template-based approach that makes explicit how LLMs access and apply their stored knowledge during reasoning;
(ii) we demonstrate through experiments across various reasoning tasks how IAO enhances knowledge verification through intermediate steps; (iii) through systematic human evaluation, we show that IAO's structured format significantly improves evaluators' ability to identify knowledge gaps, verify reasoning steps, and detect potential hallucinations compared to traditional prompting methods.

