\begin{table*}[!t]
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccc@{}}
\toprule
\multirow{2}{*}{Knowledge Domain} &
  \multicolumn{2}{c}{\textit{Arithmetic Knowledge}} &
  \multicolumn{2}{c}{\textit{Logical Knowledge}} &
  \multicolumn{2}{c}{\textit{Commonsense Knowledge}} &
  \textit{Symbolic Knowledge} \\ \cmidrule(l){2-8} 
 &
  \multicolumn{1}{l}{AQUA} &
  \multicolumn{1}{l}{GSM8K} &
  Date Understanding &
  Object Tracking &
  \multicolumn{1}{l}{StrategyQA} &
  \multicolumn{1}{l}{CommonsenseQA} &
  Last Letter \\ \midrule
zero-shot CoT        & 61.8 & 76.4 & 85.3 & 61.2 & 72.8 & 78.4 & 75.6 \\
zero-shot IAO (ours) & 63.9 & \textbf{82.3} & \textbf{88.1} & {67.1} & \textbf{76.9} & \textbf{83.1} & \textbf{88.8} \\ \midrule
L2M              & \textbf{64.2} & 77.5 & 86.4 & 63.8 & 73.5 & 79.2 & 83.2 \\
P\&S             & 62.7 & 79.3 & 87.2 & \textbf{67.5} & 74.8 & 80.6 & 84.5 \\  \bottomrule

\end{tabular}%
}
\caption{Evaluation results for PALM-2. Bold denotes best result. All methods use the same answer extraction prompt in a single stage for fair comparison. All methods are evaluated under the zero-shot setting.}

\label{tab:palm_results}
\end{table*}

\begin{table*}[!t]
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Knowledge Domain}} &
  \multicolumn{2}{c}{\textit{Arithmetic Knowledge}} &
  \multicolumn{2}{c}{\textit{Logical Knowledge}} &
  \multicolumn{2}{c}{\textit{Commonsense Knowledge}} &
  \textit{Symbolic Knowledge} \\ \cmidrule(l){2-8} 
\multicolumn{1}{c}{} & AQUA          & GSM8K         & Date Understanding & Object Tracking & StrategyQA     & CommonsenseQA & Last Letter   \\ \midrule
zero-shot CoT        & 68.4 & 90.1 & 81.5 & 97.8 & 73.2 & 79.5 & 90.3 \\
zero-shot IAO (ours) & 70.2 & \textbf{94.2} & {83.2} & {100} & \textbf{76.3} & \textbf{84.8} & \textbf{94.7} \\ \midrule
L2M              & 69.1 & 91.8 & 82.4 & 100 & 74.8 & 81.2 & 92.6 \\
P\&S             & \textbf{73.1} & 92.4 & \textbf{83.8} & 100 & 75.1 & 83.7 & 93.8 \\
\bottomrule
\end{tabular}%
}
\caption{Evaluation results for GPT-4. Bold denotes best result. All methods use the same answer extraction prompt in a single stage for fair comparison. All methods are evaluated under the zero-shot setting.}

\label{tab:gpt4_results}
\end{table*}

\paragraph{Tasks}

To evaluate IAO, we use common datasets across four different reasoning tasks: (a) \textit{Mathematical reasoning} (using GSM8k \cite{cobbe2021training} and AQuA \cite{ling2017program}) for testing numerical computation and mathematical reasoning capabilities; 
(b) \textit{Commonsense reasoning} (using StrategyQA \cite{geva2021did} and  CommonsenseQA \cite{talmor2019commonsenseqa}); (c) \textit{Symbolic reasoning} (using Last Letter \cite{wei2022chain}) for  evaluating abstract pattern recognition and manipulation; (d) \textit{Logical Reasoning} using Date Understanding and Shuffled Object Tracking \cite{srivastava2023beyond}) for testing temporal reasoning and spatial relationship understanding.

Dataset statistics are presented in \Cref{tab:data_statistics}, with additional details in \Cref{tab:more_data_stats}.

\paragraph{Models}

We conduct the experiments using (a) PALM-2 \cite{anil2023palm} (\texttt{text-unicorn}) which provides insights into knowledge utilization in mid-size models and GPT-4 (\texttt{gpt-4-1106-preview}) \cite{achiam2023gpt} that represents advanced knowledge capabilities.

Initial experiments with GPT-3.5 revealed limitations in following structured knowledge templates, highlighting how model scale affects knowledge organization capabilities. Our template-based approach demonstrates the ability to guide knowledge application without task-specific demonstrations, suggesting effective knowledge structuring across different model scales.

\paragraph{Baselines}


As a baseline, we compare our approach to chain-of-thought (CoT) \cite{wei2022chain}, in particular zero-shot-CoT \cite{kojima2022large} with the prompt \texttt{Let's think step by step} appended to the question. We use two different settings: the single step where the reasoning prompt and answer extraction prompt are in a single API call; and the two-step setting.

Additionally, we compare IAO to relevant prompting frameworks focusing on problem decomposition and planning before solving the problem to substantiate IAO's validity. In particular, we compare to \cite{zhou2022least} as \textbf{L2M} and \cite{wang2023plan} as \textbf{P\&S}.  
It is worth mentioning that these methods are multi-prompting approaches which is not the case for IAO. 
