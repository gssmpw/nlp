

%\Cref{tab:palm_results} and \Cref{tab:gpt4_results} summarize the results for  Palm-2 and GPT-4, respectively. From these results, we observe performance improvements consistent with our original hypothesis

\subsection{Arithmetic Reasoning} 

%AQUA and GSM8k are more challenging arithmetic datasets that require multi-step reasoning to solve.
\paragraph{GPT-4} As shown in \Cref{tab:gpt4_results}, IAO demonstrates notable improvements in arithmetic reasoning tasks, particularly on the GSM8K dataset where it achieves 94.2\% accuracy, outperforming the zero-shot CoT baseline (90.1\%) by 4.1 percentage points. This significant improvement suggests that IAO's structured decomposition particularly benefits complex multi-step arithmetic problems, which are characteristic of GSM8K. For the AQUA dataset, IAO achieves 70.2\% accuracy, showing moderate improvement over the zero-shot CoT baseline (68.4\%). While P\&S achieves the highest performance on AQUA (73.1\%), IAO consistently outperforms both zero-shot CoT and L2M baselines across both arithmetic datasets, demonstrating the effectiveness of explicit input-action-output structuring in mathematical reasoning tasks.
 Moreover, we observe that most cases where the IAO prompt fails are due to inherent arithmetic calculation failures. Loosely speaking, while the ``action'' planned is correct and the function is also correct, the LLM does calculation errors or fails to report the correct output as a final answer. We discuss this further in the following sections. 

\paragraph{PALM-2} We observe the same trend when using PALM-2.
There is a decrease in terms of percentage points (p.p), 2.1\%, for the AQuA dataset (where the zero-shot CoT baseline accuracy is of 61.8\%) but improves over the zero-shot CoT baseline for the GSM8k dataset (5.9 p.p). The same observations about the errors and failures for the IAO prompting apply to PALM-2 too.

\subsection{Logical Reasoning}
The datasets studied are Date Understanding and Object tracking from \cite{srivastava2023beyond}. The former asks the models
to infer the date from a context. Tracking Shuffled Objects tests a modelâ€™s ability to infer the final state of objects given its initial state and a sequence of object shuffling. 
\paragraph{GPT-4} GPT-4 achieved perfect scores for Object tracking task for almost all baselines settings with the exception of zero-shot CoT. However, that is not the case for the Date Understanding task where the baseline accuracy for zero-shot CoT is of 81.5 while IAO achieves 83.2 p.p. The challenge in this task is to correctly interpret the question and understand the temporal setting. Even by forcing the LLM to interrogate itself about what it knows (\textit{Input} field) it fails to solve this issue.  
\paragraph{PALM-2} IAO prompting achieves 67.1 p.p (over a zero-shot CoT baseline of 61.2) for the Object Tracking dataset. On the other hand, it achieves and accuracy of 88.1 from 85.3 for the same baseline prompt for the Date Understanding task. 

\subsection{Commonsense Reasoning}
CommonsenseQA asks questions with complex semantics that often require reasoning based on prior knowledge \cite{talmor2019commonsenseqa}. StrategyQA  dataset have implicit multi-step questions requiring a wide-range
of reasoning skills to answer them \cite{geva2021did}.
\paragraph{GPT-4} For the StrategyQA dataset, IAO improves the accuracy metric over all baselines but by modest margin. This is likely because the dataset contains a number of ambiguous questions, which if read verbatim may have many plausible answers but the ground truth contains only one answer. GPT-4 will in such cases refrain from giving a final answer, which is counted as an incorrect answer. This lead to a task accuracy of 76.3 p.p in StrategyQA and 84.8 in CommonsenseQA where the zero-shot CoT prompt accuracy are 73.2 and 79.5 respectively.

\paragraph{PALM-2} For StrategyQA, we see an improvement from 72.8 p.p to 76.9. For CommonsenseQA, we observe an increase in accuracy over the the zero-shot CoT baseline of 4.7 p.p.

\subsection{Symbolic Reasoning} We use the Last Letter Concatenation \cite{wei2022chain} dataset which contains questions asking the model to concatenate the last letters of each word. 
\paragraph{GPT-4} GPT-4 reaches 90.3 p.p in the zero-shot CoT setting. IAO prompting improves over all the baselines reaching 94.7 p.p in accuracy.  
\paragraph{PALM-2} Here the gains are more substantial. IAO prompting improves the accuracy by 11.6 p.p reaching 88.8 p.p over a baseline of 75.6 p.p. From the examples we observe how dividing the problem into multiple subproblems and solving each one in a structured way increases the model's reasoning abilities. 


\begin{comment}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
Method     & \textbf{AQUA} & \textbf{GSM8k} & \textbf{SQA} & \textbf{CSQA} & \textbf{LL} \\ \midrule
\textbf{L2M} & 69.1          & 89.5 & 73.4 & 81.2 & 92.6 \\
\textbf{P\&S} & \textbf{73.1} & 91.7 & 75.1 & 83.7 & 94.1 \\
\textbf{IAO} & 70.2          & \textbf{94.2}  & \textbf{76.3}       & \textbf{84.8}          & \textbf{94.7}        \\ \bottomrule
\end{tabular}%
}
\caption{Accuracy comparison to L2M and P\&S in zero-shot setting using GPT-4.}
\label{tab:baseline_gpt4}
\end{table}
\vspace{-0.3cm}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
Method     & \textbf{AQUA} & \textbf{GSM8k} & \textbf{SQA} & \textbf{CSQA} & \textbf{LL} \\ \midrule
\textbf{L2M}  & 57.4 & 77.5 & 61.5 & 71.9 & 83.2 \\
\textbf{P\&S} & 55.9 & 79.3 & 62.8 & 68.8 & 70.6 \\
\textbf{IAO} & \textbf{63.9} & \textbf{82.3}  & \textbf{76.9}       & \textbf{83.1}          & \textbf{88.8}        \\ \bottomrule
\end{tabular}%
}
\caption{Accuracy comparison to L2M and P\&S in zero-shot setting using PALM-2.}
\label{tab:baseline_palm}
\end{table}
\end{comment}

\paragraph{Comparison to L2M and P\&S} As shown in row 3 and 4 \Cref{tab:gpt4_results} and \Cref{tab:palm_results} IAO shows improvement over these baselines approaches on both GPT-4 and PaLM-2 across almost all the evaluated tasks with the additional benefit of added transparency and interpretability of the reasoning steps in a single-step prompting approach.


\subsection{Ablation studies}

We additionally perform ablation studies by removing parts of the template and assessing performance on specific tasks. Through this process, we aim to achieve three key objectives: (i) isolate the impact of each field in the proposed template, (ii) identify redundancies and (iii) enhance interpretability. The results are presented in \Cref{tab:ablation_results} and \Cref{tab:ablation_results_appendix}.

\begin{table}[h]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lc@{}}
\toprule
Prompt                                              & \multicolumn{1}{c}{Average} \\ \midrule
\texttt{[Step, Input, Action, Output]}              & 68.9                        \\
\texttt{[Step, Subquestion, Action, Output]}         & 77.7                        \\
\texttt{[Step, Subquestion, Input, Output]}         & 76.3                        \\
\texttt{[Step, Subquestion, Input, Action]}         & 59.0                        \\ \midrule
\texttt{[Step, Subquestion, Input, Action, Output]} & 80.9                        \\ \bottomrule
\end{tabular}%
}
\caption{Performance in a zero-shot setting when one of the field of the prompt is removed.}
\label{tab:ablation_results}
\end{table}

First, we observe that the prompt with all fields (\texttt{[Step, Subquestion, Input, Action, Output]}) achieves the highest average performance (80.9 p.p), indicating that including all available information is beneficial. Including the \texttt{Subquestion} field consistently improves performance compared to excluding it, suggesting that it is important for the model to understand the context of the task. Removing the \texttt{Output} from the prompt leads to a significant drop in performance (12.0 p.p). This suggests that the model is able to use the output information from previous steps to improve its performance on subsequent steps. Removing the \texttt{Action} also results in a noticeable decrease in performance (3.2 p.p). This suggests that the model is able to use the action information to better understand the context of the task.

