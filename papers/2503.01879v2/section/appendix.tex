\section*{Contributions}

\begin{enumerate}[leftmargin=0pt,label={--},itemindent=0pt]
    \item Che Liu: Exploration of network architecture, code development, and ablation studies.
    \item Yingji Zhang: Representation analysis and report writing.
    \item Dong Zhang: Data organization, code development, partial ablation studies, model evaluation and report writing.
    \item Weijie Zhang: Experiments and training optimization.
    \item Chenggong Gong: Speech data and speech evaluation.
    \item Haohan Li: Testing code and model evaluation.
    \item Yu Lu: Survey of multimodal work and writing of related sections.
    \item Shilin Zhou: Lead of Text-to-Speech (TTS) section.
    \item Yue Lu: Responsible for video data and algorithms.
    \item Ziliang Gan: Testing and optimization of the model in the financial sub-domain.
    \item Ziao Wang: Assisted in data cleaning and dataset construction.
    \item Junwei Liao: Assisted in speech-related strategies and manuscript writing.
    \item Haipang Wu: Provided organizational support and departmental coordination.
    \item Ji Liu: Manuscript revision, participated in discussions, and provided feedback and suggestions.
    \item Andr√© Freitas: Participated in methodological discussions and provided feedback and suggestions.
    \item Qifan Wang: Provided consultation and discussion on training strategies.
    \item Zenglin Xu: Manuscript revision and suggestions on training strategies.
    \item Rongjuncheng Zhang: Responsible for the testing platform, strategy suggestions, and resource coordination.
    \item Yong Dai: Oversaw all aspects of the project, and major contributor to manuscript writing.
\end{enumerate}

\section*{Future Experiments}
\begin{enumerate}[leftmargin=0pt,label={--},itemindent=0pt]
    \item \model-audio testbed: we will further incorporate various audio-modal tasks to evaluate its robustness in real scenarios.
    \item Speech-to-Text Translation: \model~falls short of the performance of its contemporary competitor, a shortcoming we intend to address in future work.
    \item TTS evaluation: the current version does not include TTS experiments; we plan to release this evaluation in a future update.
    \item tri-modal evaluation: we aim to extend our evaluation to encompass real-life, tri-modal tasks.
    \item Multiple rounds of conversation ability: we are working towards enabling multiple rounds of conversation.
    \item Pre-train Data Scale: future work will focus on further increasing the scale of the training dataset.
\end{enumerate}

\section{Experimental Details} \label{sec:app_res} \label{sec:app_details}
The following tables describe the template we used for each pre-training task:
% \documentclass{article}
% \usepackage{tcolorbox} % For tcolorbox
% \usepackage{graphicx}  % For including images

% \begin{tcolorbox}[title=Visual Audio QA Example]
%     \centering
%     \includegraphics[width=11cm]{figure/en_audio_qa.pdf} % Replace 'example-image' with your image file name
%     \medskip
    
%     Audio Query: \textit{How much did the risk-free interest rate change from 2003 to 2004?}
% \end{tcolorbox}
\begin{tcolorbox}[title=ASR] 
{<|im\_start|>}system

You are a helpful assistant.<|im\_end|>

<|im\_start|>user

<|audio\_bos|>audio.wav<|audio\_eos|>Please help me transcribe my current speech into text. <|im\_end|>

<|im\_start|>assistant

[text] <|im\_end|>

\end{tcolorbox}
\begin{tcolorbox}[title=Speech-to-Text Chat] 
{<|im\_start|>}system

You are a helpful assistant.<|im\_end|>

<|im\_start|>user

<|audio\_bos|>audio.wav<|audio\_eos|> <|im\_end|>

<|im\_start|>assistant

[response] <|im\_end|>
\end{tcolorbox}
\begin{tcolorbox}[title=Speech-to-Text Translation] 
{<|im\_start|>}system

You are a helpful assistant.<|im\_end|>

<|im\_start|>user

<|audio\_bos|>audio.wav<|audio\_eos|> What is the English equivalent of this Chinese speech? <|im\_end|>

<|im\_start|>assistant

[response] <|im\_end|>
\end{tcolorbox}
% \begin{tcolorbox}[title=Speech-Visual-to-Text Chat] 
% {<|im\_start|>}system

% You are a helpful assistant.<|im\_end|>

% <|image\_bos|>image.png<|image\_eos|> <|audio\_bos|>audio.wav<|audio\_eos|> <|im\_end|>

% <|im\_start|> [response] <|im\_end|>
% \end{tcolorbox}
\begin{tcolorbox}[title=Speech Synthesis] 
{<|im\_start|>}system

You are a helpful assistant.<|im\_end|>

<|im\_start|>user

<|audio\_bos|>text<|audio\_eos|> <|im\_end|>

<|im\_start|>assistant

[audio.wav] <|im\_end|>
\end{tcolorbox}
\begin{tcolorbox}[title=Speech-to-Speech Chat] 
{<|im\_start|>}system

You are a helpful assistant.<|im\_end|>

<|im\_start|>user

<|audio\_bos|>audio.wav<|audio\_eos|> <|im\_end|>

<|im\_start|>assistant

<|audio\_bos|>audio.wav<|audio\_eos|> <|im\_end|>
\end{tcolorbox}
\begin{tcolorbox}[title=Automatic Audio Caption] 
{<|im\_start|>}system

You are a helpful assistant.<|im\_end|>

<|im\_start|>user

<|audio\_bos|>audio.wav<|audio\_eos|> please describe the speech.
<|im\_end|>

<|im\_start|>assistant

[text] <|im\_end|>
\end{tcolorbox}
\begin{tcolorbox}[title=Speech Emotion Recognition] 
{<|im\_start|>}system

You are a helpful assistant.<|im\_end|>

<|im\_start|>user

<|audio\_bos|>audio.wav<|audio\_eos|> what is the emotion in this speech?
<|im\_end|>

<|im\_start|>assistant

[text] <|im\_end|>
\end{tcolorbox}

% \section{Experimental Results}
% \begin{tcolorbox}[title=Visual Audio QA Task Example]
%     \centering
%     \includegraphics[width=11cm]{figure/en_audio_qa.pdf}
%     \medskip
    
%     Audio Query: \textit{How much did the risk-free interest rate change from 2003 to 2004?}
% \end{tcolorbox}

\section{Representational Alignment} \label{sec:rep_align}
\paragraph{Preliminaries.} 
Kernels, characterising the distance metrics between points in a representation space, are commonly used to assess vector space \citep{pmlr-v235-huh24a}. Typically, the similarity between two kernels derived from different spaces indicates a higher degree of alignment between those spaces. This similarity can be quantified via \textbf{\textit{kernel-alignment metrics}}, such as Centered Kernel Distance (cka) \citep{kornblith2019similarity}. Next, we formally define the target of our evaluation.

Given a data $x^{i} \in \mathcal{X}$ where $i=(1,\dots,N)$, each data can be represented as a corresponding tri-modal tuple $(x^{i}_{\text{language}}, x^{i}_{\text{vision}}, x^{i}_{\text{audio}}) \in (\mathcal{X}_{\text{language}}, \mathcal{X}_{\text{vision}}, \mathcal{X}_{\text{audio}})$. For each data $x^{i}$, its vector representation can be mapped via feature function $f: \mathcal{X} \rightarrow \mathbb{R}^n$. At the unembedding stage, $f$ for different modalities are the corresponding encoders (i.e., $f_{\text{language}}$, $f_{\text{vision}}$, and $f_{\text{audio}}$). At the embedding stage, $f$ is the unified LLMs with different modal inputs, denoted by $f_{\text{LLM}}$.

For each modality, a kernel, which measures the distance/similarity between datapoints, can be denoted by $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$. The measurement of a single modal space can be described as:
$$K(\mathcal{X}) = K(x^{i}, x^{j})=\langle f(x^{i}), f(x^{j}) \rangle $$ 
where $\langle , \rangle$ denotes the inner product operation, $x^{i}$ and $x^{j}$ are different input points. In this experiment, we specifically assess the alignment between language-vision modalities by measuring the alignment between kernels through kernel-alignment metrics, $m$: 
$$m(K(\mathcal{X}_{\text{language}}), K(\mathcal{X}_{\text{vision}}))$$
To further evaluate whether audio modality can assist vision-language representational alignment at each stage, we fuse vision and audio spaces and examine whether this fusion leads to better alignment to language space. In detail, for the unembedding stage, we concatenate $f_{\text{vision}}(x^{i}_{\text{vision}})$ and $f_{\text{audio}}(x^{i}_{\text{audio}})$ as the fused feature space. The kernel can be described as:
\begin{equation} \label{eq:unembed_kernel}
    K(\mathcal{X}_{\text{vision}+\text{audio}}) =\langle [f(x_{\text{vision}}^{i}); f(x_{\text{audio}}^{i})],[f(x_{\text{vision}}^{j}); f(x_{\text{audio}}^{j})] \rangle 
\end{equation}
For the embedding stage, we feed both vision and audio inputs into the model. The kernel can be described as:
\begin{equation} \label{eq:embed_kernel}
K(\mathcal{X}_{\text{vision}+\text{audio}}) =\langle f_{\text{LLM}}(x^{i}_{\text{vision}}, x^{i}_{\text{audio}}), f_{\text{LLM}}(x^{j}_{\text{vision}}, x^{j}_{\text{audio}}) \rangle 
\end{equation}
The final kernel-alignment scores can be described as:
$$m(K(\mathcal{X}_{\text{language}}), K(\mathcal{X}_{\text{vision}+\text{audio}}))$$
\input{section/appendix_exp_res}




