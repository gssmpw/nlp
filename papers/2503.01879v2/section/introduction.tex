\section{Introduction}
\begin{figure}[t]
    \centering
    \includegraphics[width=11cm]{figure/radar.pdf}
    \caption{An overview of the model's performance. \model~is capable of handling a wide range of tasks, demonstrating strong performance across various benchmarks. For \model-audio (EN) and (CN), the scores are expressed as reciprocal (1/scores).}
    \label{fig:radar}
\end{figure}
Since the mid-20th century, significant efforts have been devoted to designing systems of the capability to simulate human cognition, with the ultimate goal of developing Artificial General Intelligence (AGI)~\citep{feng2024far,morris2023levels}. Notable algorithms include Rule-based systems, Neuro-symbolic AI systems \citep{sheth2023neurosymbolicaiwhy}, Bayesian nets systems \citep{heckerman2022tutoriallearningbayesiannetworks}, Neural networks systems \citep{hinton2006fast}, etc. Among them, neural networks systems have emerged as a cornerstone thanks to their remarkable generalization capabilities. In this context, various neural architectures have been proposed, such as CNN \citep{lecun1995convolutional} and Transformer \citep{vaswani2017attention}. The discovery of neural scaling laws~\citep{kaplan2020scalinglawsneurallanguage} has further advanced the field, paving the way for Transformer-based Large Language Models (LLMs), such as ChatGPT~\citep{kaplan2020scalinglawsneurallanguage}, which centre on language as the medium for perceiving and interacting with both human society and the physical world. By aligning multiple modalities around the language, Multimodal Large Language Models (MLLMs) have gained widespread recognition as a promising pathway toward AGI \citep{liang2024survey, zhang2024mm,nvidia2025cosmosworldfoundationmodel,ma2025visual,hong2025worldsense}.

% MLLMs, involving language(text), visual, and auditory modalities, can generally be divided into two categories. The structure for visual processing is relatively uniform across omni-style MLLMs, typically involving a visual encoder that uniformly encodes images and videos before feeding them into a large language model. Thus, the handling of audio becomes the primary differentiator among these models.

% \begin{itemize}
%     \item \textbf{Cascaded Models}: These models directly encode the wavlet or Mel-spectrogram features of the speech \junwei{through speech encodes such as Whisper \citep{radford2023robust}, with trainable encoder adapters, allowing the speech input to be interpreted as a specialised form of text by the large language model. Due to the output of the model still text, a Text-to-Speech (TTS) module is cascaded for converting the output text to speech \citep{fu2024vita,li2024baichuan}}.
    
%     \item \textbf{End-to-End Models}: \junwei{These models can directly understand and generate speech representation and do not rely on the text as the intermediate. To align with the autoregressive language models, End-to-End speech models use the discrete audio representation derived from the quantization of self-supervised learning models or neural audio codecs \citep{xie2024mini,xie2024mini2}}.
% \end{itemize}

MLLMs, involving language, vision, and auditory modalities, can generally be divided into three categories: vision-language model \citep{wang2024qwen2}, audio-language model \citep{chu2024qwen2}, and vision-language-audio model \citep{fu2025vita}. While alignment among three modalities offers a more comprehensive simulation of human cognitive processes than two-modality alignment, it presents significantly greater challenges. These challenges include limited accessibility to tri-modal datasets, higher computational resource requirements, and the complexity of aligning features across three modalities. These limitations hinder the advancement of omni-perceptive and interactive AGI systems. To address these challenges, we explores the development of a fully end-to-end tri-modal system capable of omni-perception and seamless interaction with the world in this work.

% To achieve this goal, several key research questions need to be addressed. Firstly, the immense computational power required makes it highly unlikely for most research institutions or even companies to train from scratch. Secondly, when integrating capabilities from different models, how can we avoid forgetting while maintaining their individual strengths? Thirdly, how can we enhance the model's robustness so that it can be effectively deployed in real-world scenarios? This involves considerations on how to train and how to evaluate the model.
To achieve this goal, several key research questions need to be addressed. First, what methodologies and strategies can be employed to curate and obtain high-quality, real-scenario speech datasets? Second, how can models be efficiently designed and trained to achieve tri-modal alignment, understanding and reasoning capabilities across multiple modalities? Third, what approaches can be implemented to evaluate audio-modal robustness, ensuring reliable performance and applicability in real-world scenarios?

% In response to these questions, we propose \model~which adopts the following strategies: (1) To avoid the pre-training over large-scale vision-language data, we start by pre-training our model based on the vision-language model, especially Qwen-VL~\citep{wang2024qwen2}, rather than a language model, as is typically chosen in most omni models~\citep{fu2024vita,xie2024mini,xie2024mini2,li2024baichuan}. By pre-training the model over high-quality synthetic audio data and fine-tuning it with a small amount of visual data, our model is capable of tri-modal perception and interaction. This strategy offers high training efficiency while reducing the computational resource burden, which meets the requirements of most institutions with low computational resources. (2) We systematically assess our model in under-explored tri-modal tasks, including audio-visual question answering (AVQA), audio-audio question answering(AAQA), and tri-modal audio-visual-text question answering (AVTQA). (3) To facilitate the rapid deployment and fair evaluation of the model in real-world scenarios, we introduce a new audio testbed, \model-audio, comprising diverse ASR and QA audio samples, spanning various real-world scenarios, such as corporate meetings, live broadcasting scenarios, etc.
In response to these questions, we propose \model~with the following three approaches: (1) We systematically design the speech data synthesis pipeline to obtain high-quality speech training datasets, spanning various real-world scenarios, such as corporate meetings, live broadcasting scenarios, etc. (2) We start by designing and pre-training \model~based on the vision-language model, especially Qwen2.5-VL~\citep{bai2025qwen25vltechnicalreport}\footnote{In this report, Qwen2.5-VL refers to the instruction version.}, rather than a language model, as is typically chosen in most omni models~\citep{fu2024vita,xie2024mini,xie2024mini2,li2024baichuan}. By pre-training the model over high-quality synthetic audio data, \model~is capable of tri-modal perception and interaction. This strategy offers high training efficiency while reducing the computational resource burden derived from the training vision-language alignment stage, which is an efficient and effective option for most institutions with low computational resources. (3) For a fair evaluation of the model in real-world scenarios, we introduce a new audio testbed, i.e., \model-audio, comprising diverse Automatic Speech Recognition (ASR) samples, spanning various real-world scenarios.
% , and propose under-explored tri-modal tasks, including Audio-Visual Question Answering (AVQA) and Audio Question Answering (AQA).

The experimental results demonstrate the following five key findings: 
(1) In the visual understanding task, \model~exhibits superior performance compared with its backbone model - Qwen2.5-VL-7B, indicating the effectiveness of our training strategy. 
(2) Within the Audio English QA benchmark, the model achieves better accuracy than the same-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark \citep{nachmani2024spokenquestionansweringspeech}. 
(3) In our real-world ASR benchmarks, \model{} achieves outstanding performance, which underscores its robustness in real scenarios. 
(4) In the Speech-to-Text Translation benchmark, our model outperforms Qwen2-Audio-Instruct-7B. 
% Besides, while \model{} trails behind MiniCPM-o2.6-7B and MinMo-7B in the Speech-to-Text Translation benchmark, it is noteworthy that \model{} was pretrained on a dataset approximately 20 times smaller than that for MinMo-7B, indicating the efficiency of our training strategy.
(5) Furthermore, an in-depth analysis of tri-modal alignment reveals that incorporating the audio modality enhances representational alignment. Even in the absence of audio input, our model delivers performance comparable to the backbone model. The main contributions of this paper are summarized as follows:

\begin{itemize}
    \item We systematically design the speech data synthesis pipeline to obtain high-quality speech datasets to facilitate the rapid deployment of the model in real-world scenarios.
    
    \item We propose an omni-perceive and -interactive tri-modal model, i.e., \model, which supports any combination of audio, image/video, and text inputs. Different from existing approaches that treat audio as an auxiliary, our model enables a joint understanding of these modalities.
    
    \item To evaluate the performance of the tri-modal model, we propose the \model-audio testbed, covering various real-world scenarios, such as corporate meetings, live broadcasting scenarios, etc. We intend to release the \model-audio testbed to establish a new standard for fostering innovation in addressing complex real-world applications.

    \item We comprehensively evaluate the performance of \model~on various benchmarks and conduct an in-depth analysis of tri-modal alignment from the perspective of representation space. Experimental results illustrate the efficiency and the effectiveness of our tri-modal model. 
\end{itemize}

The structure of this work is organised as follows:
In Section \ref{sec:relate}, we first identify the research gap between our model and other contemporaneous approaches, and then discuss the limitations of current speech benchmarks.
In Section \ref{sec:model}, we present our tri-modal model architecture, the composition of training data and audio data synthesis pipeline, pre-training strategies, and the \model-audio testbed.
In Section \ref{sec:eval}, we first address the alignment of downstream tasks, including vision-language evaluation and audio-language evaluation, and then evaluate representational alignment.
Finally, we conclude with a summary of our findings and outline our future research directions in Section \ref{sec:conclusion}.


