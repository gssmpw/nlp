% Thus far, we have evaluated the alignment of our model on downstream tasks. In the next section, we focus on the representation space and explore the representational alignment between vision-language modalities.

\subsection{Representational Alignment Analysis} \label{sec:eval_embed_align}

In this section, we evaluate the alignment from the perspective of representation space to offer a deep evaluation beyond conventional downstream task alignment.
We formalise MLLM within the framework of an \textbf{\textit{unembedding-embedding}} architecture. In this framework, the unembedding stage is responsible for learning transformations between observations (e.g., text, vision, audio) and latent spaces through encoders, while the embedding stage captures the complex interactions among latent variables within the latent space of the hidden layers in MLLMs. Each stage serves distinct functions and yields representations with different properties \citep{pmlr-v235-park24c,zhang-etal-2024-improving}. Consequently, by focusing on each stage independently, we can have a systematical evaluation of model behaviours in representation spaces. To assess the representational alignment among vision-language modalities at each stage, we next measure the geometrical similarity among them via the \textbf{\textit{kernel-alignment metrics}}. More information is provided in Appendix \ref{sec:rep_align}.

\paragraph{Unembedding stage.} In the unembedding stage, we evaluate representational alignment using two language-vision datasets: ImageNet \citep{5206848} and Wikipedia Caption (WIT) \citep{srinivasan2021wit}. These datasets offer varying levels of fine granularity in language-vision alignment, enabling a comprehensive assessment of representational performance. To get the audio input for each language-vision pair, we apply the pretrained Text-to-Speech LLMs, cosyvoice-300M-SFT checkpoint, \citep{du2024cosyvoice}. As for the baseline, we compare our model with MLLMs after SFT, including Qwen2.5-VL 
 \citep{bai2025qwen25vltechnicalreport}, LLaMA3.2-Vision \citep{llama3.2}, LLaVA family \cite{liu2023llava,liu2023improvedllava,liu2024llavanext}, Phi-3.5-Vision \cite{abdin2024phi}, and InternVL-8B \citep{chen2024internvl}. The kernel-alignment metrics involve cka \citep{kornblith2019similarity}, cknna \citep{pmlr-v235-huh24a}, svcca \citep{raghu2017svcca}, and $k$-nearest neighbours (knn)-based metrics: cycle knn, mutual knn, lcs knn, and edit knn. More information for those metrics can be found in \citep{pmlr-v235-huh24a}.

As shown in Table \ref{tab:eval_unembed_align}, our tri-modal model incorporating audio inputs can generally outperform other models across both corpora. This demonstrates that the inclusion of the audio modality enhances representational alignment between the language and vision modalities. Additionally, Figure \ref{fig:pca_vis} provides a Principal Component Analysis (PCA) visualisation of the vector spaces for different modalities. The visualisation highlights that each modality exhibits distinct latent space geometries, and concatenating the vision and audio spaces results in a fusion of previously separated latent space structures.
\input{table/eval_unembed_align}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{figure/pca.pdf}
    \caption{The unembedding space is visualized using Principal Component Analysis (PCA), with distinct colours representing clusters identified by k-means clustering. The visualisation reveals that, despite being trained within a unified model, the different modalities exhibit distinct latent space geometries.}
    \label{fig:pca_vis}
\end{figure}

\paragraph{Embedding stage.} Next, we focus on the vision-language alignment at the embedding stage. We restrict our attention to the Wikipedia-caption corpus \citep{srinivasan2021wit}. 
% Regarding the baselines, since most vision-language MLLMs do not support single-modality inputs and our model is pretrained over Qwen2.5-VL-7B, 
We compare our model exclusively with Qwen2.5-VL-7B. For each sample, the representation is the averaged token embeddings or the last token embeddings.
Table \ref{tab:eval_embed_align} illustrates the kernel-alignment scores at the last hidden layer, we can observe that incorporating audio into the vision modality improves the language-vision alignment of MLLMs, indicating the audio modality can provide complementary information to vision and language modalities. 
\input{table/eval_embed_align}

Additionally, we present a comparative analysis between Qwen2.5-VL-7B and our model across all hidden layers. As illustrated in Figure \ref{fig:kernel_bar}, the incorporation of the audio modality significantly enhances vision-language alignment, particularly in the intermediate hidden layers. Furthermore, we also provide a qualitative evaluation of language and vision alignment via PCA visualisation. In Figure \ref{fig:embed_pca_compare}, the observation reveals that the vision-audio fusion (depicted in the two images on the right) demonstrates a greater degree of overlap compared to the language-vision-audio separation (illustrated in the two images on the left). This suggests that the integration of audio enhances the alignment between language and vision modalities.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{figure/kernel_align_layers_bar.pdf}
    \caption{Averaged kernel-alignment score across different hidden layers, we can observe that audio modality (green bar) can result in better vision-language alignment at most hidden layers.}
    \label{fig:kernel_bar}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{figure/embed_modalities_compare_pca.pdf}
    \caption{Qualitative evaluation for language-vision alignment at embedding hidden layers. We can observe that the vision-audio fusion (right two) has a larger overlap than language-vision-audio separation (left two), indicating incorporating audio leads to better language-vision alignment. we also present the PCA visualisation at all hidden layers in Figure \ref{fig:embed_modalities_sep_pca} and \ref{fig:embed_modalities_fuse_sep_pca}.}
    \label{fig:embed_pca_compare}
\end{figure}

% \paragraph{Information bottleneck.} Thus far, we can conclude that audio modality can assist vision-language alignment at both stages. We conjecture that this phenomenon is derived from the fact that audio can provide additional mutual information between language and vision. From the perspective of the information theoretical framework, 
% (explanations from information theory): 
% $$I(audio+vision, text) = I(audio, text) + I(vision, text) - I(audio, vision|text) > I(vision, text)$$

% \begin{tikzpicture}
%     % Define colors
%     \definecolor{visioncolor}{RGB}{255,200,150} % Light orange
%     \definecolor{audiocolor}{RGB}{200,150,255} % Light purple
%     \definecolor{textcolor}{RGB}{150,200,255} % Light blue
%     \definecolor{highlightcolor}{RGB}{255,255,255} % Light yellow for intersection highlight

%     % Draw circles with transparency
%     \fill[visioncolor,opacity=0.5] (-1.5,0) circle (2cm);
%     \fill[audiocolor,opacity=0.5] (1.5,0) circle (2cm);
%     \fill[textcolor,opacity=0.5] (0,-1.5) circle (2cm);

%     % Highlight the intersection
%     \begin{scope}
%         \clip (0,-1.5) circle (2cm); % Clip to the text circle
%         \fill[highlightcolor,opacity=1] (-1.5,0) circle (2cm); % Vision
%         \fill[highlightcolor,opacity=1] (1.5,0) circle (2cm); % Audio
%     \end{scope}

%     % Draw outlines
%     \draw[thick] (-1.5,0) circle (2cm); % Vision
%     \draw[thick] (1.5,0) circle (2cm);  % Audio
%     \draw[thick] (0,-1.5) circle (2cm); % Text

%     % Add labels
%     \node at (-1.8,1) {vision};
%     \node at (1.8,1) {audio};
%     \node at (0,-2.5) {text};

%     \node[align=center, font=\small] at (0,-0.5) {$I(audio+vision; text)$};
% \end{tikzpicture}



% \subsubsection{Functional Space Analysis}


