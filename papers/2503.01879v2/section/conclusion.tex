\section{Conclusion and Future Work} \label{sec:conclusion}

In this work, we propose an omni-perceive and -interactive tri-modal model, \model, that supports any combination of audio, image/video, and text inputs, and the \model-audio testbed. Unlike existing approaches that treat audio as an auxiliary, our model enables a joint understanding of these modalities. To facilitate the rapid deployment of the model in real-world scenarios, we investigate the synthesis of high-quality speech dialogue data augmented with actual environmental noise derived from public datasets. This approach substantially improves the performance and robustness of MLLMs in practical applications. To better understand the characteristics of diverse modalities throughout the integration process, we conducted an in-depth analysis from the perspective of representation space. As a cutting-edge contribution, we aim to establish a new standard for open-source multimodal foundation models and drive innovation in complex real-world applications.

In the future, we aim to enhance the capability of MLLMs to handle audio in a streaming manner, enabling real-time processing and generation of audio inputs and outputs for dynamic, time-sensitive applications. Additionally, we intend to integrate advanced Artificial Intelligence-Generated Content (AIGC) capabilities to allow the model to seamlessly generate high-quality, contextually coherent multimodal outputs, further broadening its applicability in creative and industrial domains.