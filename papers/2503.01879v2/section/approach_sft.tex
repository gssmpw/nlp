
% To facilitate a comprehensive alignment between audio and other modalities on human preference dialogue pattern, we propose two training objectives: (i) an audio understanding task, where the goal is to generate text based on an audio query and other modalities as sources, (ii) an audio generation task, which focuses on generating audio using multimodal inputs as queries and sources. Moreover, we also design a vision-language task to further avoid forgetting issues.

% \paragraph{Low-resources audio capabilities.} 
% For the instruction tuning stage, as for the adapter, we utilise the UltraChat World corpus \citep{ding2023enhancing} and the Voice Assistant corpus \citep{xie2024miniomnilanguagemodelshear, xie2024miniomni2opensourcegpt4ovision}. From the UltraChat World corpus, a multi-round textual dialogue dataset, we select only the samples where the query is less than 50 tokens and the response is less than 150 tokens, retaining only the first five rounds of each dialogue. The textual queries are converted into corresponding audio queries using the StyleTTS2 model \citep{li2023styletts2humanleveltexttospeech}. To introduce diversity in the tone characteristics of the audio queries, distinct tones are randomly sampled from the LibriTTS corpus \citep{zen2019librittscorpusderivedlibrispeech}. For the Voice Assistant corpus, an open-source dialogue dataset, we exclusively sample single-round dialogues to serve as the training data. Both preprocessed datasets are utilised for training the adapter in the speech encoder, enabling comprehensive instruction tuning across diverse dialogue scenarios and equipping the model with the capability to effectively respond to audio queries.

% For the speech decoder, we selected dialogue where the duration of both input and output speech does not exceed 90 seconds in the Voice Assistant corpus to accelerate the training speed. ...

% \paragraph{Vision-language forgetting issues.} 