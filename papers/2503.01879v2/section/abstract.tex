\begin{abstract}
% Human beings perceive the real world through a spectrum of sensory modalities, encompassing auditory, visual, and linguistic faculties. The journey towards achieving Artificial General Intelligence (AGI) necessitates the development of models that can emulate these multifaceted perceptual capabilities and comprehensively understand these diversified data. To this end, we introduce \textbf{\model}, an industry-level \textbf{omni-perceptive and -interactive} model capable of efficiently processing Audio, Image, Video, and Text data in any combination and output audio/text in an end-to-end way.
% In this paper, we demonstrate four key features of our exploration for multi-modal information processing.
% \textbf{First}, we explore a pathway for embedding strong speech capabilities into visual capabilities under conditions of limited data and computational resources, which endows the trained model with omni-perceptive and -interactive ability and circumvents associated forgetting issues. \textbf{Second}, we conduct pioneering research on several underexplored tasks, including Audio-Visual Question Answering (AVQA), Audio-Audio Question Answering(AAQA), and tri-modal Audio-Visual-Text Question Answering (AVTQA), to create a model that truly omni-perceives and -interacts with the world.
% \textbf{Third}, for industrial-grade applications, we synthesise a large amount of high-quality dialogue data that closely aligns with the distribution of real-world applications. The data are user-focused, scenario-diverse, and voluminous. Additionally, we specifically synthesize Chinese scenario data, significantly enhancing the capabilities in Chinese and enabling it to be rapidly deployed in Chinese application scenarios. 
% \textbf{Finally}, we validate the effectiveness of \model on an extensive benchmark and conduct an in-depth analysis of feature alignment. 
Human beings perceive the real world through a spectrum of sensory modalities, encompassing auditory, visual, and linguistic faculties. The journey towards achieving Artificial General Intelligence (AGI) necessitates the development of models that can emulate these multifaceted perceptual capabilities and comprehensively understand these diversified data. To this end, we introduce \textbf{\model}, an industry-level \textbf{omni-perceptive and -interactive} model capable of efficiently processing Audio, Image, Video, and Text data in any combination and output audio/text in an end-to-end way.
We systematically investigate \model~by addressing three key research questions: First, how can models be efficiently designed and trained to achieve tri-modal alignment, understanding and reasoning capabilities across multiple modalities? 
Second, what approaches can be implemented to evaluate tri-modal model robustness, ensuring reliable performance and applicability in real-world scenarios? 
Third, what strategies can be employed to curate and obtain high-quality, real-life scenario speech datasets? 
For the first question, we design and pre-train \model \ based on the vision-language model, rather than the language model. By pre-training the model over high-quality synthetic audio data, our model is capable of tri-modal perception and interaction. 
For the second question, we introduce a new audio testbed, Nexus-O-audio, comprising diverse Automatic Speech Recognition (ASR) samples, spanning various real-world scenarios, such as corporate meetings and live stream.
For the third question, we design the speech data synthesis pipeline to obtain high-quality speech training datasets, covering various real-world scenarios. Comprehensive experimentation and an in-depth analysis of tri-modal alignment over latent space demonstrate the advantages of our model on downstream tasks.
\end{abstract}