\section{Related Work} \label{sec:relate}

In this section, we present the existing works of Multimodal Large Language Models (MLLMs) in Section \ref{sec:relate_mllms}, and analyse existing audio modality evaluation approaches in Section \ref{sec:relate_evaluation}.

\subsection{MLLMs} \label{sec:relate_mllms}

Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and reasoning over textual knowledge \citep{touvron2023llama, touvron2023llama2, dubey2024llama, yang2024qwen2, jiang2023mistral}. Building on these advancements, a bunch of studies \citep{chen2024expanding, chen2024far, dai2022one} have extended the understanding and alignment capabilities of LLMs to visual knowledge, addressing challenges such as vision-language alignment and instruction-following, by introducing Multimodal Large Language Models (MLLMs). Notable models include Qwen-VL \citep{wang2024qwen2, yang2024qwen2}, InternVL \citep{chen2024internvl}, etc. However, in complex visual reasoning tasks, such as video analysis, MLLMs typically struggle to perform well due to the heterogeneity among diverse modalities, where different modalities have distinct feature granularities, representations, and geometries in a shared latent space \citep{zhang-etal-2024-graph,zhang-etal-2024-learning}.
To further improve their performance in visual understanding tasks, recent works have expanded MLLMs by incorporating audio capabilities, such as LLaMA-Omni \citep{fang2024llama}, VITA-1.0 \citep{fu2024vita}, Mini-Omni2 \citep{xie2024mini2}, Baichuan-Omni \citep{li2024baichuan}, and MinMo \citep{chen2025minmomultimodallargelanguage}. However, these models exhibit certain limitations: LLaMA-Omni and MinMo lacks visual capabilities, Baichuan-Omni does not have end-to-end Text To Speech (TTS) capabilities, and VITA-1.0 is constrained by the limited capability of its backbone model.

Recent models, such as VITA-1.5~\citep{fu2025vita}, MiniCPM-o2.6~\footnote{https://github.com/OpenBMB/MiniCPM-o}, Baichuan-omni-1.5\footnote{\url{https://github.com/baichuan-inc/Baichuan-Omni-1.5}}, and OpenOmni \citep{luo2025openomnilargelanguagemodels}, have been proposed and have achieved state-of-the-art performance. However, these are pretrained based on the LLMs rather than MLLMs. While pre-training from scratch can enhance multimodal alignment, this strategy significantly increases training time and resource demands due to the necessity of additional visual-language alignment, which relies on large-scale pre-training datasets. To improve training efficiency and to reduce resource burden, we start by pre-training over MLLM, specifically Qwen2.5-VL-7B, and introduce \model, an advanced unified tri-modal LLM designed to enable audio understanding and generation across arbitrary combinations of modalities. This strategy is friendly with institutions with limited computation resources.

\subsection{Audio Modality Evaluation} \label{sec:relate_evaluation}

With the rapid advancement of MLLMs, a wide range of multimodal benchmarks has been proposed and widely adopted for evaluation, ranging over vision \citep{xie2023tencentllmeval,gan2024mme,fu2024videommefirstevercomprehensiveevaluation,yue2024mmmumassivemultidisciplinemultimodal,yang2025doestablesourcematter}, audio \citep{conneau2023fleurs,yang2024air,fu2024mmecomprehensiveevaluationbenchmark}, omni\citep{li2024omnibenchfutureuniversalomnilanguage}, etc. In the speech domain, existing benchmarks, e.g., Fleurs \citep{conneau2023fleurs}, Aishell2 \citep{du2018aishell}, LibriSpeech \citep{pratap2020mls}, and Common Voice \citep{ardila2019common}, provide linguistically diverse datasets under varying acoustic conditions, facilitating Automatic Speech Recognition (ASR) evaluation. Additionally, AIR-Bench \citep{yang2024air} introduces a pioneering generative evaluation framework encompassing speech, natural sounds, and music, employing novel audio mixing strategies and GPT-4 for unified, objective, and reproducible assessments.

However, the assessment of speech models should be closely aligned with real-world environments to ensure that the evaluation results faithfully represent the demands and requirements of practical and industrial applications. We observe that these existing speech benchmarks present three limitations. First, the small data scale of these benchmarks typically results in significant performance variance. Second, the benchmarks primarily focus on controlled scenarios, making it difficult to evaluate models in dynamic and complex real-world settings, e.g., meetings or live broadcast environments. Third, scores from the existing ASR benchmarks generally fail to align closely with the actual user interaction experience, thereby limiting their practical utility. To address these limitations, we propose \model-audio testbed, {\color{black}comprising 8.1k and 2.8k of Chinese and English ASR samples}, spanning various application domains, such as corporate meetings and live broadcasting scenarios.

In addition to assessing audio downstream task alignment, we conduct an in-depth evaluation of whether audio modality can enhance vision-language alignment, as well. Intuitively, the audio modality can provide complementary information to the visual and language modalities, enhancing mutual information and feature alignment, thereby improving the performance on visual understanding tasks \citep{song2023bridge,choi2024av2av,zhang2024deep}.

% Intuitively, the audio modality can provide complementary information to the visual and language modalities, enhancing mutual information and feature alignment, thereby improving performance on visual understanding tasks \citep{song2023bridge,choi2024av2av,zhang2024deep}. To explore this, we conducted an in-depth analysis from the perspective of representation learning. 

% Video-MME \citep{fu2024videommefirstevercomprehensiveevaluation}: evaluating video QA reasoning capabilities of MLLMs over 6 domains, diverse time range, etc. \textbf{(5)} MMMU \citep{yue2024mmmumassivemultidisciplinemultimodal}: evaluate MLLMs on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. \textbf{(6)} DocVQA-val \citep{mathew2021docvqadatasetvqadocument}: evaluate MLLMs on textual (handwritten, typewritten or printed) content of the document images. \textbf{(7)} AI2D: AI2 Diagrams (AI2D) is a dataset of over 5000 grade school science diagrams. \textbf{(8)} MMVet \citep{yu2024mmvetevaluatinglargemultimodal}: evaluate the integrated Capabilities of MLLMs, including recognition, OCR, knowledge, language generation, spatial awareness, and math. \textbf{(9)} MME \citep{fu2024mmecomprehensiveevaluationbenchmark}: measures both perception and cognition abilities on a total of 14 subtasks.