\subsection{Architecture} \label{sec:arc}

\paragraph{Visual encoder.} 
% {\color{blue}Following Qwen2.5-VL \citep{wang2024qwen2}, \model~takes the ViT part of the Data Filtering Network (DFN) \citep{fang2023data} as the visual encoder, which utilizes 2D-RoPE position embedding in place of the fixed position embedding. It can process images of any resolution, dynamically converting them into a variable number of visual tokens. In addition, to reduce the visual tokens of each image, a simple MLP layer is exploited after the ViT to aggregate $2 \times 2$ adjacent tokens into a single token. For videos, \model~samples 2 frames per second and then process them using 3D convolutions \citep{carreira2017quo} with a depth of two, allowing \model~to handle more video frames at the same computational cost \citep{arnab2021vivit}.}
Following Qwen2.5-VL \citep{bai2025qwen25vltechnicalreport}, the vision encoder employs a re-designed Vision Transformer (ViT) architecture by incorporating M-RoPE and window attention to support native input resolutions while accelerating the computation of the entire visual encoder. During both training and inference, the height and width of the input images are resized to multiples of 28 before being fed into the ViT. The vision encoder processes images by splitting them into patches with a stride of 14, generating a set of image features. 

\paragraph{Audio encoder/decoder.} To enable audio capabilities in the vision-language MLLM, we incorporate an audio encoder-decoder architecture. In this setup, the encoder is responsible for mapping the speech features into the semantic space of the MLLM, while the decoder transforms the semantic code back into speech, as shown in Figure \ref{fig:architecture}. Specifically, the audio encoder comprises a pre-trained Whisper-large-v3 \citep{radford2023robust}, and a two-layer MLP adapter. Specifically, the first layer of the MLP concatenates five consecutive speech feature vectors into a single vector, which is subsequently processed by the second MLP layer.

The auto-regressive audio decoder is a 6-layer decoder-only transformer designed to generate discrete speech codes, which are then passed to a pre-trained generator to produce the final waveforms. Specifically, we use the audio codebook from the FireFly GAN in Fish-Speech \citep{liao2024fishspeechleveraginglargelanguage} as the audio codebook. Following MusicGen \citep{copet2023simple}, the audio decoder predicts sequences of eight speech codes in a delayed manner. At each time step, the decoder averages the embeddings of the eight speech codes generated in the previous time step with the hidden state of the LLM at the current time step. The resulting averaged embedding is then fed into the audio transformer to predict the next eight speech tokens. Finally, the predicted speech tokens are input into the generator of the FireFly GAN to synthesise the final waveforms.

\paragraph{Language model.} \model~utilises the language model component of Qwen2.5-VL-7B \citep{bai2025qwen25vltechnicalreport} as the foundational language model, consisting of 28 causal Transformer layers.

\subsection{Training Data} \label{sec:data}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figure/dataset.pdf}
    \caption{Audio dataset synthesis pipeline. In the current version, our testbed only supports the ASR task. We will further incorporate various audio-modal tasks, including the AQA and AVQA tasks. Both are currently under development.}
    \label{fig:data}
\end{figure}

\paragraph{Datasets composition.} As detailed in Table~\ref{tab:dataset}, the training data for \model{} encompasses a wide range of domains within speech processing. These domains include recognition tasks such as ASR and Speech Emotion Recognition (SER), reasoning tasks include Automatic Audio Caption and Speech Emotion Recognition, translation tasks like Speech-to-Text Translation (S2TT), generation tasks such as Speech Synthesis, and interactive tasks involving Speech-to-Speech Chat and Speech-to-Text Chat. The pretrained datasets include (1) public corpora (50\%): Aishell1 \citep{bu2017aishell1opensourcemandarinspeech}, Wenetspeech \citep{zhang2022wenetspeech}, Librispeech \citep{7178964}, Kensho \citep{oneill2021spgispeech5000hourstranscribed}, PeopleSpeech \citep{galvez2021peoplesspeechlargescalediverse}, MLS \citep{Pratap2020MLSAL}, and Gigast \citep{,ye2023gigast10000hourpseudospeech} and (2) real-life in-house data (50\%).

\input{table/datasets}

\paragraph{Audio dataset synthesis pipeline.} 
% Obtaining speech conversation data is typically challenging. In contrast, textual dialogue data is more easily accessible on the internet. Therefore, 
For the non-speech datasets, we employ the zero-shot in-context generation technique proposed by CosyVoice \citep{du2024cosyvoice} to convert the text into natural-sounding speech. To enhance the diversity of voices, we randomly select a speaker from a pool of two thousand individuals for each corresponding language to synthesise the speech, ensuring that the voice aligns with the language of the text. However, many conversational and instruction datasets are not ideal for speech synthesis due to the nature of the tasks, which do not facilitate effective speech interaction. To address this issue, we enhance the quality of our synthesised speech data by implementing the following optimisation measures:

\begin{itemize}
	\item Length Filtering: texts exceeding a predefined length threshold are excluded from the dataset. Specifically, texts longer than 200 Chinese characters or 200 English words are removed.
	\item Non-text Element Filtering: texts containing an excessively high proportion of non-text elements are excluded. Specifically, texts with a non-text character ratio exceeding 0.2 are removed. Text characters in this context include punctuation marks (e.g., !, ?), alphabetic characters ([a-z]), and Chinese characters.
	\item Pattern Matching Filtering: texts that included URLs, file paths, or common LaTeX formulas were identified and removed using regular expressions.
\end{itemize}

We further enhance the data quality with the help of a large model through the following steps:
\begin{itemize}
	\item Filter out questions that are not suitable for oral QA.
	\item Rewrite the command in a conversational tone and generate a conversational response.
\end{itemize}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{figure/train_stage.pdf}
%     \caption{Overview of the training stage in \model. The first stage aims to map the speech features into semantic space, the second stage aims to enable the audio instruction-following capability, and the last stage aims to enable speech generation capability.}
%     \label{fig:stage}
% \end{figure}

\subsection{Training Strategies} \label{sec:tr}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figure/train_stage.pdf}
    \caption{Overview of the training stage in \model. The first stage aims to map the speech features into semantic space, the second stage aims to enable the audio instruction-following capability, and the last stage aims to enable speech generation capability.}
    \label{fig:stage}
\end{figure}
\paragraph{Stage 1: audio alignment.}
The first stage aims to facilitate the capacity of LLMs to comprehend and process input speech features. {\color{black}To accomplish this, we utilise 8 million bilingual speech-text pairs for alignment training, covering five training objectives, as illustrated in Table \ref{tab:dataset}.} Consistent with the methodology employed in Qwen2.5-VL-7B, our training data is structured in the ChatML format. An example of this format is illustrated as below:

% colback=black!5!white,colframe=yellow!75!black,
\begin{tcolorbox}[title=The Dataset Format Example of ChatML] 
{<|im\_start|>}system

You are a helpful assistant.<|im\_end|>

<|im\_start|>user

<|audio\_bos|>audio.wav<|audio\_eos|>Please help me transcribe my current speech into text. <|im\_end|>

<|im\_start|>assistant

\textit{hat had its source away back in the woods of the old cuthbert place it was reputed to be an intricate headlong brook in its earlier course through those woods with dark secrets of pool and cascade but by the time it reached lynde's hollow it was a quiet well conducted little stream} <|im\_end|>
\end{tcolorbox}

\paragraph{Stage 2: audio SFT.}
% After the previous stage of training, the model can align the visual input features and the speech input features, but it cannot follow human instructions very well. Therefore, 
The second stage aims to enable the model to follow speech instructions by incorporating speech-based questions and text-based answers. In this stage, we freeze the visual and audio encoders while unfreezing the visual and audio adapters, and backbone LLM. This strategy is designed to enhance the capability of \model{} to respond accurately to multimodal instructions, thereby improving its performance in understanding and processing both visual and auditory information.

By keeping the core encoder parameters fixed, we ensure that the foundational representations remain stable while allowing the adapters to fine-tune and specialize in the nuances of specific tasks or instructions. This strategic adjustment aims to refine the sensitivity of \model{} to multimodal cues without disrupting the robustness of its underlying architecture.

\paragraph{Stage 3: audio output.}
Different from the previous large speech model connected to the TTS module \citep{fu2024vita,li2024baichuan}, we exploit a decoder to generate speech end-to-end. First, we train \model{} on the Text-to-Speech dataset to make the model learn text-to-speech alignment. Then, we utilize the speech-to-speech dataset to train the model to learn speech understanding and speech synthesis capabilities. At this stage, only the parameters of the audio decoder are trainable.

\subsection{\model-audio testbed} \label{sec:syn_audio_data}

Our \model-audio testbed comprises audio samples collected from real-world scenarios, including video meetings, live streaming, voice-based customer service QA interactions, and real-life conversations. We apply an off-the-shelf ASR model to convert each speech into corresponding text as our testbed. More information is provided in Figure \ref{fig:data}. In addition to the ASR task, we plan to introduce Audio QA, Audio Visual QA tasks, and tri-modal tasks, which are currently under development.