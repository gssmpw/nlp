\subsection{Multimodal Tasks Alignment Analysis} \label{sec:eval_task_align}

In this section, we evaluate the performance of \model{} on the following four tasks: vision understanding, audio English tasks, ASR, and S2TT. These tasks cover a wide range of modalities and capabilities, providing a comprehensive assessment of the versatility and effectiveness of MLLMs across various domains.

\subsubsection{Vision-Language Evaluation} \label{sec:vision_language}

We first quantitatively evaluate \model~on various vision understanding tasks, including the following benchmarks: \textbf{(1)} HallusionBench (Hal) \citep{guan2023hallusionbench}: evaluating the hallucination and visual illusion. \textbf{(2)} MathVista (MathV) \citep{lu2023mathvista}: evaluating mathematical reasoning in visual contexts. \textbf{(3)} OCRBench (OCR) \citep{fu2024ocrbenchv2improvedbenchmark}: evaluating Optical Character Recognition (OCR) capabilities on text-intensive images. \textbf{(4)} Video-MME \citep{fu2024videommefirstevercomprehensiveevaluation}: evaluating video QA reasoning capabilities of MLLMs over 6 domains, diverse time range, etc. \textbf{(5)} MMMU \citep{yue2024mmmumassivemultidisciplinemultimodal}: evaluating MLLMs on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. 
% \textbf{(6)} DocVQA-val \citep{mathew2021docvqadatasetvqadocument}: evaluating MLLMs on textual (handwritten, typewritten or printed) content of the document images. 
\textbf{(6)} AI2D: AI2 Diagrams (AI2D) is a dataset of over 5000 grade school science diagrams. \textbf{(7)} MMVet \citep{yu2024mmvetevaluatinglargemultimodal}: evaluating the integrated capabilities of MLLMs, including recognition, OCR, knowledge, language generation, spatial awareness, and math. \textbf{(8)} MME \citep{fu2024mmecomprehensiveevaluationbenchmark}: measuring both perception and cognition abilities on a total of 14 subtasks.

As illustrated in Table \ref{fig:image}, we observe that \model\ achieves strong performance to current popular vision-language MLLMs on the MathV, OCR, and Hal benchmarks. 
% Furthermore, when compared to proprietary models such as GPT-4V and Claude 3.5 Sonnet, \model\ demonstrates similar levels of performance, highlighting its competitive capabilities in vision understanding and reasoning tasks, indicating our model can effectively maintain vision-language alignment capability. 
Furthermore, when compared to Qwen2.5-VL, \model\ demonstrates superior performance, highlighting its competitive capabilities in vision understanding and reasoning tasks, indicating our model can effectively maintain vision-language alignment capability after auditory pretraining stage. 
\input{table/vision_bench}

\subsubsection{Audio-Language Evaluation} \label{sec:audio_language}

Next, we evaluate the audio-language alignment on English QA, ASR, and S2TT tasks. 

\paragraph{English QA task.} 
% We evaluate the performance on the following 3 spoken QA benchmarks: Web Q. \citep{berant-etal-2013-semantic}, LLaMA Q. \citep{nachmani2024spokenquestionansweringspeech}, and Trivia QA \citep{joshi2017triviaqalargescaledistantly}. Note that since Web Q. and Trivia QA benchmarks do not provide audio queries, the comparison among models may be influenced by the performance of different TTS models. This could introduce variability in results, as the quality and characteristics of the TTS model used for generating audio queries may impact overall performance. Therefore, we do not provide the analysis for both benchmarks.
We evaluate the performance on the spoken QA benchmark: LLaMA Q. \citep{nachmani2024spokenquestionansweringspeech}. As presented in Table \ref{tab:eqa}, our model achieves top performance, higher than same-period competitor MiniCPM-o2.6-7B (highlighted in blue), demonstrating its competitive capabilities in this task. 
% Besides, we also provide the performance of our model, i.e., \model$\dagger$, evaluated by LLM (Qwen2-Instruct-72B) as the model might generate answers that are semantically similar but not an exact match (e.g., \textit{``China''} and \textit{``People's Republic of China''}).
\input{table/eqa_bench}

% ----------------------------------------------------------
% For the Chinese QA, we focus on the following benchmarks:
% C-Eval \citep{huang2023ceval}: C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels.
% ant-fineval \citep{}: \textcolor{blue}{\textbf{TODO}}.
% mmlu \citep{hendrycks2021measuringmassivemultitasklanguage}: measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more.
% \input{table/cqa_bench}
% ----------------------------------------------------------

\paragraph{ASR task.} In the ASR task, we focus on both Mandarin (cn) and English (eng), evaluating performance on the following benchmarks: AIShell-2 \citep{du2018aishell2transformingmandarinasr}, Librispeech \citep{panayotov2015librispeech}, and our real-scenario benchmark. As demonstrated in Table \ref{tab:asr}, our model achieves the best performance on the real scenario benchmarks, indicating the robustness of our model.
% competitive performance, surpassing most baselines on the Librispeech and real scenario benchmarks, indicating the robustness of our model.
\input{table/asr_bench}

\paragraph{Speech-to-Text translation task.}

As for the S2TT, we exploit CoVoST2 \citep{wang2020covost2massivelymultilingual}, which provides massive multilingual S2TT datasets. As shown in Table \ref{tab:speech2text}, \model~demonstrates superior performance compared to specialised speech LLM, Qwen2-Audio-7B-Instruct, in both translation tasks. Nevertheless, our model still falls short of the performance of its contemporary competitor, MiniCPM-o2.6-7B. However, we pre-train our model on only 8 million audio samples, which is a relatively modest dataset at the pretraining stage.
% Although our model left behind the same-period competitor (MinMo-7B and MiniCPM-02.6-7B), our model is pretrained over only 20,000 translation training samples. In contrast, MinMo-7B relies on a pretrain dataset 20 times larger, indicating the high training efficiency of our model.
% Unlike MinMo-7B, which uses hundreds of thousands of training data, we only use 20,000 translation data (1/20 of it) to achieve very good translation results. 
\input{table/speech2text_bench}
