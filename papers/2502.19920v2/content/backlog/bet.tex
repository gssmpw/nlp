(Addis/BET 11/29/24) When ‘reinforcement learning’ (RL) and ‘video games’ find their way into the same sentence, images are conjured of DeepMind’s Q-learning RL agent playing Pong or Breakout (Mnih et al. 2015), OpenAI’s DOTA 2 agent storming a contested lane (OpenAI et al. 2019), or DeepMind’s AlphaStar trouncing Grand-Master-level Starcraft players (Vinyals et al. 2019). Each of these accomplishments was dramatic and ground-breaking in its own right, and in playing these games, insight can be provided into applications of RL to solve more-complex, real-world problems (need specifics?) (OpenAI et al. 2019). While the current climate is dominated by large language models (LLMs) (citation?), the viral video “Training AI to Play Pokemon With Reinforcement Learning,” which has, at the time of writing, 6.6 million views on YouTube (Whidden 2024), shows that there remains significant interest in this line of research in the general public. Despite this, little has been done to explore game-playing in the Pokemon Red environment (Flaherty et al. 2021). Pokemon Red is a very human game that presents many challenges for RL that are applicable to real-world problems, a few being a complex state-action space, sparse rewards, partially-observable states, and very long time horizons (phrase better?)(citation?). Here, we present an application of Proximal Policy Optimization (PPO) (Schulman et al. 2017) to train an agent, on a single desktop computer, using pixel data and memory (RAM) values as inputs, that plays Pokemon Red and progresses through beating Gym 2 in Cerulean City. Due to the low compute requirements, simplicity of the training setup, and relative complexity of the environment, this PPO agent represents an exploration of efficient ways to approach solving a difficult human task.

Building generally capable artificial intelligence, which can plan, explore, and act continuously, represents a formidable challenge in the AI community. Deploying such agents in real-world scenarios, like robotics, involves numerous complexities, including mechanics, data collection, and control algorithms. These challenges make direct application difficult and potentially prolong breakthroughs. To concentrate exclusively on the problem of intelligence learning, researchers increasingly utilize simulated game environments. This approach allows for the development of algorithms that, while inspired by simulations, are transferable to real-world applications. Currently, a popular method involves using reinforcement learning to train agents specialized in specific games.

%current challenges
However, much of the existing research focuses on optimizing agents to excel in particular games such as Go or StarCraft, which more closely resembles developing a specialized skill rather than a general capability. Conversely, training agents in high-level simulator games like GTA, Minecraft, or Red Dead Redemption introduces problems related to complex action spaces (e.g., free mouse interactions) and high operational costs, which hinder rapid evaluation and separate intelligence learning from action control issues.

%our main idea
In this paper, we propose a reinforcement learning environment on Pokémon-Red. Pokémon-Red combines a human-like environment with a simple action space (seven buttons), making it ideal for both rapidly deploying and testing reinforcement learning algorithms at a low cost and for researching general intelligence learning capabilities in a simulated gaming system. Specifically, the Pokémon series of games incorporates a wealth of human prior knowledge, such as using hospitals to restore health, shops to purchase goods, and obstacles like trees and rocks that must be navigated. This design necessitates that agents develop general capabilities to address challenges that mimic those in the human world. What further enhances its suitability as a benchmark environment is the game’s naturally limited and discrete set of action options: UP, DOWN, LEFT, RIGHT, A, B, and START. This simplicity prevents the complexity of the action space from overshadowing the learning of intelligence by complicating action control.

%our baseline and implementation method
Alongside the Pokémon reinforcement learning environment, we propose a strong baseline for agents to engage deeply with the game narrative, requiring comprehensive intelligence to explore maps, capture Pokémon, win battles and gym challenges, engage in NPC dialogues, and solve puzzles.
To achieve this, we've designed a nuanced reward system that includes Pokémon Level Rewards to incentivize training stronger Pokémon, Pokémon Capture Rewards to encourage more captures, Health Rewards to ensure timely visits to hospitals for health recovery, and Map Exploration Rewards to foster extensive exploration of the game environment.
This system not only triggers more in-game events but also significantly advances the narrative.
For map exploration, we've developed a method using KNN to cluster past screenshots, which efficiently conserves memory and aids in discovering new scenes.
Additionally, to optimize agent training, we've implemented a parallel training framework utilizing the Proximal Policy Optimization (PPO) strategy, thereby enhancing the agent’s performance and interaction within the game.

%results
Our Pokémon reinforcement learning environment has proven to be a fertile ground for cultivating reinforcement learning agents adept at exploring and learning within the game, with our baseline algorithm effectively advancing the gameplay akin to human players.
Utilizing this baseline, we observed that our agents can successfully capture Pokémon, purchase Pokéballs in shops, win battles and gym challenges, and drive the narrative forward.
Notably, our baseline demonstrated its capability by progressing the game up to Mount Moon, navigating through the maze-like Viridian Forest, and consistently securing victory challenging Gym 1.
We also observed several intriguing behaviors in the agents post-training, which highlight distinct features developed in this unique environment.
Analyzing these behaviors has provided valuable insights into both research trends and academic development. 

%impact
The project has generated considerable interest and discussion within the community, underscoring the utility and innovative nature of the environment.
This environment is not only popular and unique, but also readily adaptable for future performance metrics, such as the speed of narrative progression or completion of Pokémon collections.
Its flexibility allows for easy modifications on the maps to test more specific objectives, and the tasks within are inherently human-like, which facilitates interaction where agents complete tasks via natural language instructions.
This adaptability make our environment an exceptional tool for both testing and advancing reinforcement learning applications.