%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results and Discussion}
\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The previous sections introduced the MDP behind Pokémon Red and the training algorithm.  
These establish our minimal baseline, which we analyze through ablation experiments.  
We vary the agent's starting state by selecting Squirtle, Charmander, or Bulbasaur as the starter Pokémon.  
Additional experiments ablate individual reward signals and compare the results to 30 playthroughs by skilled human players.

Each experiment is repeated five times with different seeds.  
During training, we evaluate 25 time points using 30 episodes per repetition.  
We report the mean performance across runs and measure variability with the standard deviation.  

\begin{figure*}[t] % Use figure* for spanning both columns
    \centering

    % Subfigure for Legend
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim={0 20 0 26}, clip]{figures/performance/legend_horizontal_plot.pdf}
    \end{subfigure}

    % Subfigure 1
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim={0 42 0 0}, clip]{figures/performance/performance_vermilion.pdf}
    \end{subfigure}

    \vspace{0.25em} % Add vertical spacing between subfigures

    % Subfigure 2
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim={0 42 0 0}, clip]{figures/performance/performance_bill.pdf}
    \end{subfigure}

    \vspace{0.25em}

    % Subfigure 3
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim={0 42 0 0}, clip]{figures/performance/performance_misty.pdf}
    \end{subfigure}

    \vspace{0.25em}

    % Subfigure 4
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim={0 42 0 0}, clip]{figures/performance/performance_cerulean.pdf}
    \end{subfigure}

    \vspace{0.25em}

    % Subfigure 5
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim={0 0 0 0}, clip]{figures/performance/performance_mt_moon.pdf}
    \end{subfigure}

    \caption{Mean completion curves for distinct milestones. Beating Misty can be skipped to reach Vermilion City.}
    \label{fig:performance_overview}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Baseline Performance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure \ref{fig:performance_overview} shows sample efficiency curves, depicting the mean proportion of milestones completed.
The experiments vary by starter Pokémon, an agent using memory via GRU, and a \textit{Fast} setting with increased text speed and disabled battle animations.

The first milestone is reaching Mt. Moon after defeating Brock.
\textit{Fast} performs best, peaking at 98\% completion rate, followed by the baseline at 97\%.
With Bulbasaur, the agent reaches 94\% but fails entirely at the second milestone: arriving in Cerulean City.
This outcome is due to exploiting the heal reward as discussed in Section \ref{sec:heal_exploit}.
Other agents achieve between 53\% and 93\%.

Beating Misty and completing Bill's quest can be done in any order, but only the latter unlocks Vermilion City.
Agents may skip Misty since her water-type Pokémon pose a challenge, especially for the fire-type Charmander, which has just a 2\% completion rate.
\textit{Fast} (33\%), baseline (27\%), and GRU (21\%) perform better, but the rates are low compared to reaching Cerulean City.
While \textit{Fast} and baseline excel early on, the GRU agent performs better on Bill’s quest and on reaching Vermilion City.  
Bill's quest is not trivial, requiring three basic interactions, in a specified order, without leaving his house. The events reset if the house is exited before completion of the full sequence. Despite the confined space of the house, and an event flags observation that should convey quest progression to agents, completion still proves too complex for most:  
for example, 71\% of \textit{Fast} experiments visited Bill's house, yet only 19\% of all \textit{Fast} completed his quest.
In contrast, 48\% of GRU agents completed the quest, down just 1\% from the 49\% arriving at Cerulean; this suggests an advantage in retaining task-relevant memory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussing Ablations and Human Performance}
\label{sec:ablations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Table \ref{tab:performance} adds the following experiments to those previously described: individual rewards ablations, starting the agent from a state where it can choose its starter Pokémon, and the results from human playthroughs through Cerulean City.
Next, we discuss the most salient findings.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Distinct Exploration Strategies are Indispensable}
\label{sec:ablations}
%%%%%%%%%%%%%%%%%%%%%%%%%%

Exploring the world of Pokémon is tremendously difficult.  
To address this, a naive navigation reward is introduced.  
Without it, the agent fails to achieve any milestones, yet, when this reward is scaled up by a factor of 10, the agent rarely defeats Brock. Instead, such agents explore almost exclusively, to the detriment of all other aspects of the game. Even battles and menuing are avoided to conserve steps and thus maximize navigation reward.
%However, the agent shown in the video mentioned in the introduction performs better under this setting.  
%The key difference is that the agent in the video always runs episodes with a fixed length of 81,920 steps, whereas our agent starts with fewer steps and gradually increases them based on success.  
%When trained under the same fixed-length condition, our agent also reaches Cerulean City.  
%However, it avoids battles and menu interactions to conserve steps and maximize the navigation reward.  

Since this reward signal is so simplistic, yet navigation is so crucial, it would be interesting to investigate more advanced exploration methods, such as curiosity-driven rewards \cite{Pathak2017} or Random Network Distillation \cite{Burda2019}. Such methods could provide robustness via reduced reward sensitivity, and by potentially allowing the visited-coordinates binary observation to be obviated.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Coping with Extremely Long Horizons}
\label{sec:horizons}
%%%%%%%%%%%%%%%%%%%%%%%%%%

Most MDPs have a terminal state, but in Pokémon Red, we must define this ourselves due to the game's open-ended nature.
With a fixed episode length, all data-collecting workers reset simultaneously, repeatedly sampling similar training data.
Since the game's episodes are significantly longer than the sampled trajectories, there is a high risk of catastrophic forgetting, where learned policies are lost.
To mitigate forgetting, we allowed the episode length to grow dynamically based on progress.
Because the agent's decisions are stochastic, workers quickly fall out of sync in their resets, with some agents restarting in Pallet Town, while others continue advancing. While this increases sample diversity, training remains unstable: if Figure \ref{fig:performance_overview} rendered standard deviations, the performances across different run types would be difficult to distinguish.  
Similarly, Table \ref{tab:performance} hints on this instability, showing large standard deviations.  

Although agents are competitive in defeating Brock, they fall significantly behind in reaching Cerulean City: humans require about 11,000 steps on average to reach Cerulean City, while an agent requires effectively double that. The \textit{Fast} agent stood out among agents as the quickest, requiring just 18,500 steps on average. Since \textit{Fast} greatly speeds the passage of the dialogs that appear upon NPC or object interaction, and when in battle, and clearing such dialogs requires multiple successive, correct inputs, it is likely that action efficiency plays an important role in completion speed. A short dialog takes at least 10 steps; evolution, another dialog, takes about 15 steps. Here, steps can be saved by pressing B, which aborts the evolution. Although human players can use this advantageously, agents only seem to stochastically abort, suggesting an insouciance towards wasted steps.
Since addressing such inefficiency would require additional engineering, such as reading memory locations of the game to determine when decision-making is relevant, while our action execution method is precise for overworld navigation, it remains coarse for battles and dialogues.
One approach to handle unnecessary actions could be to add a time dimension to the action space.  
In addition to selecting a button to press, the agent would decide when to act or how long to keep the button pressed.  
This would allow the agent to learn when to act and when to observe by itself \cite{zhou2024}.

Finally, the suitability of the training method itself is questionable.
Maximizing the discounted cumulative reward encourages minimizing time, introducing bias.
This bias is evident in the \textit{Choose Starter} experiment, where the agent consistently picks Charmander, despite Squirtle being the most effective choice against Brock.
The agent favors Charmander because Squirtle and Bulbasaur require one and two extra actions, respectively, to reach in the overworld compared to Charmander.
Since picking a starter immediately grants an event and level reward, Charmander is chosen simply because its reward is triggered sooner.
By the time the agent reaches Brock, it becomes clear that Charmander is an inferior choice; however, defeating Brock takes over 3,000 steps on average—vastly exceeding both the sampled trajectory length of 2,048 steps and the effective horizon of approximately $333$ steps (calculated as $\frac{1}{1-0.997}$).
To address this, hierarchical training methods, such as Director \cite{hafner2022deep}, are promising.
In this context, a sequence of primitive actions (i.e., button presses) can be combined to form high-level actions.
Choosing the starter Pokémon could be such an action, which would remove the undesirable time bias.
However, learning high-level actions from scratch remains a significant challenge.


%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Heal Reward Exploitation}
\label{sec:heal_exploit}
%%%%%%%%%%%%%%%%%%%%%%%%%%

The heal reward encourages exploitation, especially when Bulbasaur is chosen as the starter.
The agent's policy converges to a strategy of battling wild Zubats in Mt. Moon, using Bulbasaur's Leech Seed to restore health while Zubat heals with Leech Life.
This results in nearly endless battles, providing frequent heal rewards.
Before Bulbasaur faints, the agent visits the Poké Center for another heal reward.

Similar exploitation occurs in the Charmander, GRU, and \textit{Choose Starter} experiments, where at least one out of five runs involves battling wild Pokémon near Pallet Town and using the player's mother to heal.
This strategy leads to suboptimal performance, with runs failing to exceed an 80\% success rate against Brock (Table \ref{tab:performance}) and high standard deviations in heals and Poké Center visits (Table \ref{tab:policy}).
Removing the heal reward generally reduces performance, as the Poké Center is no longer used as a respawn point.
With an average of 8–9 blackouts, the agent requires additional steps to retrace its path from Pallet Town to the point of fainting (e.g., Cerulean City).

Bulbasaur is an exception, as the agent reaches Cerulean City without the heal reward, though it is less effective than Charmander or Squirtle.
Bulbasaur may appear advantageous due to its grass-type advantage against Brock, but Bulbasaur learns Vine Whip, its first contact Grass-type attack, comparatively late, at level 15. And, this move has a pitiful 10 PP.
In contrast, Squirtle learns Bubble at level 8, and Charmander learns Ember at level 9; both moves have higher PP than Vine Whip, making them more suitable for the lengthy, battle-heavy traversal of Mt. Moon. But such nuance is lost on the agent: the starter Pokémon is chosen very early in the episode, and connecting this choice to distal outcomes would require significant engineering.



%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Reward Shaping Introduces Vulnerabilities}
\label{sec:reward_shaping}
%%%%%%%%%%%%%%%%%%%%%%%%%%

Beyond healing and navigation, event and level rewards also shape the agent’s behavior in unintended ways.

Event rewards serve as breadcrumbs, guiding the agent through the storyline.  
Some, like retrieving a potion from an NPC, are optional, while others, such as earning gym badges, are mandatory.  
That said, the agent frequently skips Misty's badge in Cerulean City while progressing to Vermilion City, suggesting that critical events may require stronger feedback.  

Ablating the level reward yields the best ablation performance (Table \ref{tab:performance}), but introduces side effects as well.  
The agent’s highest-level Pokémon averages 27.45—suprisingly comparable to the baseline, and just two levels below the \textit{Fast} run.  
To explain this performance, we hypothesize that ablating level reward might allow the agent to `focus' on other sources of rewards, namely, event rewards. Since most events are trainer battles, which are early-game dense and XP-rich, this agent progresses readily within the scope of this paper by having sufficient level and incentive to readily complete the early game objectives. 
    While all reward sources influence the agent's performance, other rewards have a debatably-positive effect on the agent. 

% On average, the agent catches a second Pokémon in 58\% of runs, which helps prevent blackouts caused by poison damage.  
% For instance, if Squirtle is the only Pokémon and gets poisoned, it loses one HP every few steps in the overworld.  
% If Squirtle faints, the player blacks out and respawns.  
An agent starting with Squirtle (baseline) typically avoids healing exploits, whereas the Charmander agent heals heavily. But, ablate the heal reward, and the agent explores fecklessly, visiting 5637 coordinates on average. Despite varying positive and side effects, no reward nor ablation manages to address the challenge of reaching the third gym in Vermilion City, which is blocked by a cuttable tree (Figure \ref{fig:vermilion_cut}).
This presents a significant exploration challenge:  
the agent must obtain a Pokémon capable of learning Cut, acquire the 
necessary item, HM01, navigate the UI to use it, and correctly execute the move in the overworld.
Until to this point in the game, none of these actions were necessary. Clearly, solving this requires
\input{tables/large_tables}
\clearpage
\noindent
an additional reward signal to incentivize all the behaviors associated with Cut. Indeed, many such challenges exist in Pokémon Red, and further refining the reward function risks introducing other vulnerabilities to be exploited and side effects to be realized, highlighting the environment's complexity. 
% This highlights the game as a uniquely complex environment for reward shaping.  


%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Limitations}
\label{sec:reward_shaping}
%%%%%%%%%%%%%%%%%%%%%%%%%%

While Pokémon Red serves as a valuable research environment, it comes with several limitations.  
First, the game is closed-source and requires owning a legal copy.  
Once a digital copy is available for training, additional engineering effort is needed to extract relevant information from the Game Boy emulator's RAM.

Second, the prolonged episode length significantly increases evaluation time.  
Running evaluation episodes takes as much wall-time as the training runs themselves, with a single training run lasting approximately 36 hours.  
This extended horizon also presents challenges for agents utilizing Recurrent Neural Networks.  
Processing long observation sequences—2048 steps in our case—leads to substantial VRAM overhead.
As such, in lieu of additional optimization, the GRU experiments were run on CPU only, with an average runtime of 24 days per training run.  

Furthermore, the vast number of possible gameplay strategies contributes to the curse of dimensionality.  
Comparing agents across numerous factors makes comprehensive evaluation labor-intensive.  
For example, analyzing item usage and move selection falls outside the scope of this paper, as does manually reviewing thousands of episodes.

Addressing these limitations may require refining the toolset, which has been effective for other environments.  


% Also a limitation: we do not examine or work towards generalization