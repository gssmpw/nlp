%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Training Method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

All agents are trained using Proximal Policy Optimization (PPO) \cite{Schulman2017}.
We leverage the clipped surrogate loss to optimize the policy at time $t$:
\begin{equation}
\begin{aligned}
    L^{C}_t(\theta) = -\mathbb{E}_t \Bigg[ &\min \Big( q_t(\theta) A_\pi^{\text{GAE}}(s_t, a_t), \\
    &\quad \text{clip}(q_t(\theta),1- \epsilon,1+\epsilon) A_\pi^{\text{GAE}}(s_t, a_t) \Big) \Bigg]
\end{aligned}
\label{eq:policy_loss}
\end{equation}
\begin{equation*}
    \text{where} \quad q_t(\theta) = \frac{\pi(a_t|s_t,\theta)}{\pi(a_t|s_t, \theta_{\text{old}})}
\end{equation*}
$s_t$ denotes the observed state, $a_t$ is the chosen action, $\theta$ defines the parameters of the policy, $\epsilon$ is the clip range, and $A_\pi^{\text{GAE}}$ is the generalized advantage estimate.
The value function shares parameters with the policy and undergoes clipping:
\begin{equation}
\begin{aligned}
    L^{VClip}_t(\theta) = \Big( &\text{clip} \big( V(s_t, \theta), \\
    &V(s_t, \theta_{\text{old}}) - \epsilon, \\
    &V(s_t, \theta_{\text{old}}) + \epsilon \big) - \hat{V}_t \Big)^2
\end{aligned}
\end{equation}
\begin{equation}
    L_t^{V}(\theta) = \max\left[\left(V(s_t, \theta) - \hat{V}_t\right)^2,~L^{VClip}_t(\theta)\right]
    \label{eq:value_clip}
\end{equation}
The final combined loss is depicted by $L^{C+V}_t(\theta)$:

\begin{equation}
	L^{C+V}_t(\theta)=\mathbb{E}_t [L^{C}_t(\theta)+vL^{V}_t(\theta)]
\end{equation}
\noindent
where $v$ is the value function's coefficient.

We intentionally leave out the commonly used entropy bonus, as tuning its weighting across \(10^{-3}\), \(10^{-4}\), \(10^{-5}\), and \(10^{-6}\) yielded no significant improvements.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Network Architecture}
% GRU: 3.87M
% No GRU: 2.03M
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/ann.pdf}
    \caption{Actor-Critic Network (2M parameters, GRU: 4M).}
    \label{fig:ann}
\end{figure}

The general network architecture of the agent is illustrated in Figure \ref{fig:ann}.
Each vision modality within the observation space is encoded using a classic Nature Convolutional Neural Network (CNN) \cite{mnih2015DQN}.
The game state vector is processed separately through a single fully connected layer.
The outputs of all encoders are then flattened, concatenated, and projected to match the dimensionality of the network’s body, which consists of a fully connected layer as standard, or, to facilitate recurrence, a Gated Recurrent Unit (GRU).
Pleines et al. \cite{pleines2023memory} describe how the GRU enables memory capabilities by maintaining a hidden state over time.
The hidden features computed in the body are used to construct both a policy head and a value head, which respectively output the discrete policy (actions) and state value estimate (via the value function).
The ReLU activation function is applied throughout the network.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hyperparameters}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{tables/hyperparameters}

Table \ref{tab:hyperparameters} presents the hyperparameters used across all experiments.
Since Pokémon Red features episodes lasting tens of thousands of steps, each environment instance (i.e., worker) samples a horizon of 2048 steps, which we found to be more effective than horizons of 512 or 1024 steps.
This longer horizon increases the likelihood of capturing more reward signals compared to the more commonly used shorter horizons, such as 512 steps or fewer.
To leverage all available cores of our training hardware, we use 32 workers, resulting in a total batch size of 65,536.
\(\gamma\) was tuned over \(\{0.99, 0.997, 0.999, 0.9995\}\).