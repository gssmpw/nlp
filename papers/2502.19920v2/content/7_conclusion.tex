%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Outlook}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our simple DRL baseline trains agents to complete an initial portion of Pokémon Red, serving as stepping stone for tackling its extensive challenges.
Key tasks like cutting trees and solving puzzles remain unexplored by our agents but pose significant hurdles, potentially requiring engineered solutions such as reward shaping or curriculum learning, thus further diverging from human perception and play.
We did not address generalization, as agents started from a fixed state while influencing RNG through their actions.
ROM randomizers could improve robustness by varying start conditions, such as the choice of starter Pokémon.
For handling long horizons, hierarchical DRL may reduce unnecessary actions, while advanced exploration methods could replace naive navigation rewards and binary image observations.
Overall, we anticipate Pokémon Red will play an essential role in future research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgment}
% without s, bacause it is defined this way in the template
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This research is supported by computing time from the Paderborn Center for Parallel Computing and the Linux-HPC-Cluster at TU Dortmund.
We also thank Keelan Donovan and Sky (Discord username) for their discussions and reviews, PufferLib \cite{pufferlib2024} for RL expertise, and Ryan Sullivan and Joseph Suarez for manuscript review.