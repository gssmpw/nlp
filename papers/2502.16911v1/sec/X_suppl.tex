\clearpage
\setcounter{page}{1}
\maketitlesupplementaryonecolumn

\section{Supplementary Overview}
We organize our Supplementary Material as follows:
\begin{itemize}
    \item Sec.~\ref{sec:compound_prompt_generation_pseudocode} provides pseudocode for our compound prompt generation method.
    \item Sec.~\ref{sec:expanded_noise_model_results} extends the noise model analysis from Sec.~\ref{sec:noisemodel} of the main paper to all datasets and CLIP backbones.
    \item Sec.~\ref{sec:per_class_APs_all_datasets} shows the per-class performance of our method on all classes from all three datasets.
    \item Sec.~\ref{sec:compound_prompt_ablation_results} shows the results of different ablations on the compound prompts in our method, including randomized prompts, cooccurrence filtration, and compound prompt templates.
    \item Sec.~\ref{sec:theoretical_explanation_for_weakened_max} offers a theoretical justification for use of a ``weakened max'' and use of an adaptive fusion strategy. This includes a proof of the theorem that was informally stated at the end of Sec.~\ref{sec:noisemodel} in the main paper.
\end{itemize}

\section{Compound Prompt Generation Pseudocode}
\label{sec:compound_prompt_generation_pseudocode}

We provide pseudocode for our compound prompt generation method below. Note that it only requires \emph{coarse} knowledge of the cooccurrence probabilities, specifically knowledge of which pairs and triplets have low probability of cooccurring.

\input{sec/compound_prompt_generation_pseudocode}

\section{Expanded Noise Model Results}
\label{sec:expanded_noise_model_results}

\input{sec/tables/noise_model_tables_all}

We use this section to show the results of running the noise model analysis from Tab.~\ref{tab:NoiseModelTable} on CLIP similarity scores from all three datasets computed with all nine CLIP backbones. Each colummn of each of Tab.~\ref{tab:NoiseModelExtendedCOCO}, Tab.~\ref{tab:NoiseModelExtendedVOC}, Tab.~\ref{tab:NoiseModelExtendedNUSWIDE} represents a separate fit of the noise models. As in the main paper, we report fraction of variance unexplained (FVU), as well as the fitted $\delta$ strength of the static AND-bonus and the lower and upper quartiles of the $\delta_{i,j}$ strengths of the variable AND-bonus. We see similar trends to those discussed in the main paper; the OR-only noise model explains significantly more variance than the AND-only model, and the OR-with-AND-bonus models explain most of the variance gap between the ``constant'' upper-bound and ``look-up table'' lower-bound. Hence, we find that CLIP scores tend to behave like an OR-gate with an AND-gate ``bonus'' for many different backbones on multiple datasets.

\section{Per-class performance of SPARC vs vanilla ZSCLIP on all datasets}
\label{sec:per_class_APs_all_datasets}

\begin{figure}[t]
  \centering
    \includegraphics[width=1\linewidth]{sec/figs/COCO2014_test_class_APs.png}
    \includegraphics[width=1\linewidth]{sec/figs/VOC2007_test_class_APs.png}
    \includegraphics[width=1\linewidth]{sec/figs/nuswideTheirVersion_test_class_APs.png}
    \caption{Per-class APs (averaged over all CLIP backbones) for our method vs vanilla ZSCLIP on all three datasets. Our method consistently improves over ZSCLIP for almost every class in all datasets.}
    \label{fig:PerClassAPsAllDatasets}
\end{figure}

We showed in Fig.~\ref{fig:PerClassAPsCOCO} in the main paper that SPARC \emph{consistently} improves over vanilla ZSCLIP across all the classes in COCO. We now show in Fig.~\ref{fig:PerClassAPsAllDatasets} that the improvement is consistent across all the classes in all three datasets. In fact, we see that there is only one class out of all the classes in all the datasets (181 classes total) where our method does notably worse than ZSCLIP. That class is the ``earthquake'' class of the NUSWIDE dataset. For all other classes, our method is either the same or (in most cases) notably better than ZSCLIP. 

\section{Compound prompt ablation results}
\label{sec:compound_prompt_ablation_results}


\input{sec/tables/waffleclip_ablation_table}

We describe a few different ablations on the compound prompts used by our method. We start by comparing the performance of our compound prompts with ``randomized'' prompts in which the cooccurrent classes are replaced by random characters. We do this in light of the findings of WaffleCLIP \cite{WaffleCLIP}, which found that randomized prompt ensembles could perform as well as descriptive ones due to the inherent statistical benefits of ensembling. We find that this is not the case for our problem - randomized compound prompts offer no benefit over debiased singletons. We show our results in Tab.~\ref{tab:WaffleCLIPAblationTable}.

\input{sec/tables/compound_prompt_filtration_ablation_table}

Our next ablation takes a closer look at the \textit{composition} of our set of compound prompts. We start by including \textit{all} pairs of classes in our set of formulaic pairwise prompts (i.e. of the form ``A and B'') and not including any other kind of compound prompt. We find that this gives us a 1.3\% average boost over debiased singletons. Filtering these pairwise prompts by cooccurrence, as our proposed method does, yields a further 0.5\% boost, and adding triplet and descriptive prompts gives an additional 0.3\% boost. It makes sense that using the full set of class pairs would still have some benefit, without any cooccurrence filtering, if all the classes as a whole tend to cooccur positively more often than negatively, as that would mean that they are on average a helpful signal for predicting each other's presence. It might also be the case that related classes provide a useful context signal to CLIP.

\input{sec/tables/compound_prompt_template_ablation_table}

Finally, we consider alternative templates for the formulaic pair prompts. In addition to the ``A and B'' template used in the main paper, we also try ``A or B'', ``A with B'', ``A next to B'', ``A and not B'' (alongside ``A and B''), and the combination of all templates. For simplicity, we remove the triplet and descriptive compound prompts during this analysis. We report the results in Tab.~\ref{tab:compoundPromptTemplateAblation}. We find that our original template ``A and B'' performs the best, although other conjunctive templates do get quite close, while ``A or B'' does considerably worse. This latter finding suggests that perhaps CLIP does interpret ``and'' and ``or'' differently, even if it treats ``A and B'' primarily as an OR-gate. 

\section{Theoretical Explanation for Weakened Max and Adaptive Fusion}
\label{sec:theoretical_explanation_for_weakened_max}

\subsection{Theory Overview}

We introduced a theoretical justification at the end of Sec.~\ref{sec:noisemodel} for the use of a ``weakened max'' instead of an outright maximum of compound scores. We will now explain that justification in full detail.

Recall that the goal was to predict the presence or absence of target class $0$ given a set of $m$ compound prompts pairing class $0$ with each of cooccurring classes $1,...,m$. We informally stated in Sec.~\ref{sec:noisemodel} that the second-max will outperform the first-max for sufficiently large $m$. We define our settings and assumptions more precisely in Sec.~\ref{subsec:theory_preliminaries} and then formally state this theorem as Theorem \ref{thm:theorem1}, which we prove in Sec.~\ref{sec:theorem1_proof}.

We make a further claim in Theorem \ref{thm:theorem2}, which states that there are settings for which a sufficiently \emph{small} $m$ will cause the first-max to outperform the second-max, and that the boundary between ``sufficiently large'' and ``sufficiently small'' depends on data statistics that are unknowable in any practical setting, even one where exact cooccurrence statistics are available. We prove this theorem in Sec.~\ref{sec:theorem2_proof}. We suspect that we would find similar behavior for other pairs of statistics, such as second-max vs third-max, third vs fourth, fourth vs median, median vs min, etc. In general, \textbf{fixed fusion rules are suboptimal} for combining the compound prompt scores.

Together, these theorems justify not only the use of a \textbf{``weakened max''}, but also the use of an \textbf{adaptive fusion strategy} such as Rank Fusion, which can use the direction of highest variance to figure out which order statistics are most useful for the setting at hand.

\subsection{Preliminaries}
\label{subsec:theory_preliminaries}

Suppose we have target class $0$ and cooccurring classes $1,..,m$. These have ground-truth presences $y_0, y_1,...,y_m \in \{0,1\}$. We make some assumptions about their distribution.

\begin{assumption}
\label{assumption:assumption1}
The ground-truth distribution has the following properties:
\begin{align}
Pr(y_i = 1 \ |\  y_0 = 1) &= \rho \qquad\quad \forall i \in [m]\\
Pr(y_i = 1\ |\ y_0 = 0) &= q \qquad\quad \forall i \in [m]\\
1 > \rho &> q > 0\\
y_j &\perp y_i\ |\ y_0\qquad\!\!\!\!\!\!\!\ \forall i \neq j \in [m]
\end{align}
\end{assumption}

We also introduce variables $\tilde{y}_0, \tilde{y}_1,...,\tilde{y}_m \in \{0,1\}$ which are ``noisy'' versions of the ground-truth. Think of these as indicating whether each object is visible to the VLM. E.g. we might have $y_i = 1$ and $\tilde{y}_i = 0$ if object $i$ was occluded, or we might have $y_i = 0$ and $\tilde{y}_i = 1$ if an spurious object in the image resembled $i$. For each $i \in [m]$ we have:
\[
\tilde{y}_i = 
\begin{cases} 
1 - y_i & \text{with probability } \nu, \\
y_i & \text{with probability } 1 - \nu.
\end{cases}
\]

We make some assumptions about the distribution of these variables.

\begin{assumption}
\label{assumption:assumption2}
The distribution of $\tilde{y}_0, \tilde{y}_1,...,\tilde{y}_m$ has the following properties:
\begin{align}
\tilde{y}_i & \ \textrm{depends only on}\ y_i\\
\nu &< \frac{1}{2}
\end{align}
\end{assumption}

As mentioned on the main paper, we assume that the score for the prompt ``$\{0\}$ and $\{i\}$'' is distributed as follows:
\begin{align}
s_{0,i} &= \max(\tilde{y}_0, \tilde{y}_i) + \delta \min(\tilde{y}_0, \tilde{y}_i) + \varepsilon
\end{align}
where $\delta$ is the strength of the ``AND-bonus'' described in the main paper, and $\varepsilon \sim \mathcal{W}(\sigma)$ is symmetric, zero-centered, additive noise whose scale is controlled by $\sigma$.\\

From the set $\{s_{0,1},...,s_{0,m}\}$ we compute order statistics $r_1, r_2$, which are the first and second highest elements, respectively.\\

Now, suppose we independently draw a ground-truth positive sample with $y_0^{+} = 1$ and a ground-truth negative sample with $y_0^{-} = 0$ and compute order statistics $r_1^{+}, r_2^{+}$ and $r_1^{-}, r_2^{-}$. We define "win" events $W_1$ and $W_2$ as the events where $r_1^{+} > r_1^{-}$ and $r_2^{+} > r_2^{-}$, respectively.\\

We will now make an assumption about $\varepsilon \sim \mathcal{W}(\sigma)$ in order to simplify our analysis. In order to state our assumption, we will need a bit more notation.
\begin{align}
\bar{s}_{0,i} &= \max(\tilde{y}_0, \tilde{y}_i) + \delta \min(\tilde{y}_0, \tilde{y}_i)\\
\bar{r}_1, \bar{r}_2\ &\textrm{are the first and second highest elements of}\ \{\bar{s}_{0,1},...,\bar{s}_{0,m}\}
\end{align}

We are now ready to state our assumption.
\begin{assumption}
\label{assumption:assumption3}
Assume that $\sigma$ is small enough for the following to approximately hold for each $k \in \{1,2\}$
\[
\textrm{Pr}(W_k) \approx 
\begin{cases} 
1 & \text{if }\ \bar{r}_k^{+} > \bar{r}_k^{-}, \\
0 & \text{if }\ \bar{r}_k^{+} < \bar{r}_k^{-}, \\
\frac{1}{2} & \text{if }\ \bar{r}_k^{+} = \bar{r}_k^{-},
\end{cases}
\]
\end{assumption}

We have now stated all of our assumptions.\\

Before making our formal theorem statements, we define some shorthand that we will use throughout the proof. First, we note that if we hold $\tilde{y}_0$ fixed, then $\bar{r}_1$ and $\bar{r}_2$ can each take on one of two values. For example, if $\tilde{y}_0 = 0$ then the possible values are $\{0,1\}$, and if $\tilde{y}_0 = 1$ then the possible values are $\{1, 1 + \delta\}$. As such, we define pairs of complementary events $(H_1, L_1)$ and $(H_2, L_2)$ to denote that $\bar{r}_1$ or $\bar{r}_2$ took the higher or lower of its possible values.\\

We define some additional shorthand:
\begin{align}
\rho^{\prime} &:= (1 - \nu) \rho\ +\ \nu (1 - \rho)\\
q^{\prime} &:= (1 - \nu) q\ +\ \nu (1 - q)\\
a &:= 1 - \rho^{\prime}\\
\gamma &:= \frac{1 - q}{1 - \rho^{\prime}}\\
A &:= m (1 - a) a^{m-1}\\
G &:= m (1 - \gamma a) (\gamma a)^{m-1}
\end{align}

We are now ready to formally state our theorems.

\subsection{Formal Theorem Statements}

\begin{theorem}
\label{thm:theorem1}
Given the assumptions above, plus the additional assumption that $\textrm{Pr}(\tilde{y}_0^{+} \neq y_0^{+} \bigvee \tilde{y}_0^{-} \neq y_0^{-}) > 0$, we can guarantee that $\textrm{Pr}(W_2) > \textrm{Pr}(W_1)$ for sufficiently large $m$.
\end{theorem}

\begin{theorem}
\label{thm:theorem2}
There are values of $\rho, q, \nu$ and distributions of $(\tilde{y}_0^{+}, \tilde{y}_0^{-})$ which satisfy all the requirements of Theorem \ref{thm:theorem1}, for which $\textrm{Pr}(W_2) < \textrm{Pr}(W_1)$ for sufficiently small $m$. In fact, the value of $m$ at which the inequality reverses depends on label-flip probability $\nu$.
\end{theorem}

\subsection{Proof of Theorem 1}
\label{sec:theorem1_proof}

We start by proving that $\rho^{\prime} > q^{\prime}$, i.e. $\textrm{Pr}(\tilde{y}_i = 1\ |\ y_0 = 1) > \textrm{Pr}(\tilde{y}_i = 1\ |\ y_0 = 0)$.

\begin{lemma}
\label{lemma:lemma1}
$1 > \rho^{\prime} > q^{\prime} > 0$ given the above assumptions on $\rho$, $q$, and $\nu$.
\end{lemma}

\begin{proof}
We can use some algebra to prove this from Assumptions~\ref{assumption:assumption1} and~\ref{assumption:assumption2}.
\begin{align}
\rho^{\prime} &= \rho + \nu - 2 \nu \rho\\
q^{\prime} &= q + \nu - 2 \nu q\\
\rho^{\prime} - q^{\prime} &= 2 (\frac{1}{2} - \nu) (\rho - q)\\
&> 0
\end{align}
It is trivial to show that $\rho^{\prime}, q^{\prime} \in (0,1)$ because they are both mixtures of quantities in that range.
\end{proof}

Our next lemma will derive some probability differences that will be important for our proof.

\begin{lemma}
\label{lemma:lemma2}
Consider the following probability differences:
\begin{align}
d^{HH} &:= \textrm{Pr}(H_2^{+}, H_2^{-}) - \textrm{Pr}(H_1^{+}, H_1^{-})\\
d^{HL} &:= \textrm{Pr}(H_2^{+}, L_2^{-}) - \textrm{Pr}(H_1^{+}, L_1^{-})\\
d^{LH} &:= \textrm{Pr}(L_2^{+}, H_2^{-}) - \textrm{Pr}(L_1^{+}, H_1^{-})\\
d^{LL} &:= \textrm{Pr}(L_2^{+}, L_2^{-}) - \textrm{Pr}(L_1^{+}, L_1^{-})
\end{align}
We claim that, regardless of the values or distribution of $\tilde{y}_0^{+}$ and $\tilde{y}_0^{-}$, the following is true:
\begin{align}
d^{HH} &= AG - (1 - a^m) G - (1 - (\gamma a)^m) A\\
d^{HL} &= (1 - a^m) G - AG - (\gamma a)^m A\\
d^{LH} &=  (1 - (\gamma a)^m) A - AG - a^m G\\
d^{LL} &= AG + a^m G + (\gamma a)^m A
\end{align}
\end{lemma}

\begin{proof}
We start by noting that $d^{HH}, d^{HL}, d^{LH}, d^{LL}$ do not depend on $\tilde{y}_0^{+}, \tilde{y}_0^{-}$. Although $\tilde{y}_0^{+}$ and $\tilde{y}_0^{-}$ affect the specific values that $\bar{r}_1^{+}, \bar{r}_2^{+}, \bar{r}_1^{-}, \bar{r}_2^{-}$ can take, they do not affect the probabilities of events $H_1^{+}, L_1^{+}, H_2^{+}, L_2^{+}, H_1^{-}, L_1^{-}, H_2^{-}, L_2^{-}$. This is because these events only depend on $\tilde{y}_1^{+},...,\tilde{y}_m^{+}$ and $\tilde{y}_1^{-},...,\tilde{y}_m^{-}$, which in turn depend on ground truths $y_1^{+},...,y_m^{+}$ and $y_1^{-},...,y_m^{-}$, which all depend on $y_0^{+}$ and $y_0^{-}$, which are fixed, so there is no dependency on $\tilde{y}_0^{+}$ or $\tilde{y}_0^{-}$.

We also note that we can factor out the joint probabilities in $d^{HH}, d^{HL}, d^{LH}, d^{LL}$ because the positive and negative samples were drawn independently, hence:
\begin{align}
d^{HH} &= \textrm{Pr}(H_2^{+}) \textrm{Pr}(H_2^{-}) - \textrm{Pr}(H_1^{+}) \textrm{Pr}(H_1^{-})\\
d^{HL} &= \textrm{Pr}(H_2^{+}) \textrm{Pr}(L_2^{-}) - \textrm{Pr}(H_1^{+}) \textrm{Pr}(L_1^{-})\\
d^{LH} &= \textrm{Pr}(L_2^{+}) \textrm{Pr}(H_2^{-}) - \textrm{Pr}(L_1^{+}) \textrm{Pr}(H_1^{-})\\
d^{LL} &= \textrm{Pr}(L_2^{+}) \textrm{Pr}(L_2^{-}) - \textrm{Pr}(L_1^{+}) \textrm{Pr}(L_1^{-})
\end{align}

We can work out the probability for event $H_1^{+}$, which occurs iff at least one of $\tilde{y}_1^{+},...,\tilde{y}_m^{+}$ is $1$:
\begin{align}
\textrm{Pr}(H_1^{+}) &= 1 - (1 - \rho^{\prime})^m\\
&= 1 - a^m
\end{align}

By similar reasoning, we can say:
\begin{align}
\textrm{Pr}(H_1^{-}) &= 1 - (\gamma a)^m
\end{align}

Next, we work out the probability for the event $H_2^{+}$, which which occurs iff at least two of $\tilde{y}_1^{+},...,\tilde{y}_m^{+}$ are $1$:
\begin{align}
\textrm{Pr}(H_2^{+}) &= 1 - (1 - \rho^{\prime})^m - m \rho^{\prime} (1 - \rho^{\prime})^{m-1}\\
&= 1 - a^m - m (1 - a)a^{m-1}\\
&= 1 - a^m - A
\end{align}

Similar reasoning can be used for $H_2^{-}$:
\begin{align}
\textrm{Pr}(H_2^{-}) &= 1 - (\gamma a)^m - m (1 - \gamma a)(\gamma a)^{m-1}\\
&= 1 - (\gamma a)^m - G
\end{align}

Putting these all together, we get:
\begin{align}
d^{HH} &= \textrm{Pr}(H_2^{+}) \textrm{Pr}(H_2^{-}) - \textrm{Pr}(H_1^{+}) \textrm{Pr}(H_1^{-})\\
&= (1 - a^m - A) (1 - (\gamma a)^m - G) - (1 - a^m) (1 - (\gamma a)^m)\\
&= (1 - a^m) (1 - (\gamma a)^m) - (1 - a^m) G - (1 - (\gamma a)^m) A + AG - (1 - a^m) (1 - (\gamma a)^m)\\
&= AG - (1 - a^m) G - (1 - (\gamma a)^m) A\\
d^{HL} &= \textrm{Pr}(H_2^{+}) \textrm{Pr}(L_2^{-}) - \textrm{Pr}(H_1^{+}) \textrm{Pr}(L_1^{-})\\
&= (1 - a^m - A) ((\gamma a)^m + G) - (1 - a^m) (\gamma a)^m\\
&= (1 - a^m) (\gamma a)^m + (1 - a^m) G - (\gamma a)^m A - AG - (1 - a^m) (\gamma a)^m\\
&= (1 - a^m) G - AG - (\gamma a)^m A\\
d^{LH} &= \textrm{Pr}(L_2^{+}) \textrm{Pr}(H_2^{-}) - \textrm{Pr}(L_1^{+}) \textrm{Pr}(H_1^{-})\\
&= (a^m + A) (1 - (\gamma a)^m - G) - (a^m) (1 - (\gamma a)^m)\\
&= (a^m) (1 - (\gamma a)^m) + (1 - (\gamma a)^m) A - a^m G - AG - (a^m) (1 - (\gamma a)^m)\\
&= (1 - (\gamma a)^m) A - AG - a^m G\\
d^{LL} &= \textrm{Pr}(L_2^{+}) \textrm{Pr}(L_2^{-}) - \textrm{Pr}(L_1^{+}) \textrm{Pr}(L_1^{-})\\
&= (a^m + A) ((\gamma a)^m + G) - (a^m) (\gamma a)^m\\
&= (a^m) (\gamma a)^m + a^m G + (\gamma a)^m A + AG - (a^m) (\gamma a)^m\\
& = AG + a^m G + (\gamma a)^m A
\end{align}
\end{proof}

Now that we have derived these probability differences, we can use them to work out $\textrm{Pr}(W_2) - \textrm{Pr}(W_1)$ for the four possible settings of $(\tilde{y}_0^{+}, \tilde{y}_0^{-})$. We address these case-by-case.

\paragraph{Case 1: $(\tilde{y}_0^{+}, \tilde{y}_0^{-}) = (0,0)$} We can think of this as the \textbf{``TN-vs-FN'' case}, where object $0$ is not visible in either of the ground-truth positive or ground-truth negative images. In this case, we can derive the following expression:

\begin{lemma}
\label{lemma:lemma3}
If $(\tilde{y}_0^{+}, \tilde{y}_0^{-}) = (0,0)$ then $\textrm{Pr}(W_2) - \textrm{Pr}(W_1) = \frac{1}{2} (G - A)$.
\end{lemma}

\begin{proof}
In this case, we have $\bar{r}_1^{+}, \bar{r}_2^{+}, \bar{r}_1^{-}, \bar{r}_2^{-} \in \{0, 1\}$, so we can say:
\begin{align}
\textrm{Pr}(W_1) &= \frac{1}{2} \textrm{Pr}(H_1^{+}, H_1^{-}) + \frac{1}{2} \textrm{Pr}(L_1^{+}, L_1^{-}) + \textrm{Pr}(H_1^{+}, L_1^{-})\\
&= \frac{1}{2} \Big( \textrm{Pr}(H_1^{+}) \textrm{Pr}(H_1^{-}) + \textrm{Pr}(L_1^{+}) \textrm{Pr}(L_1^{-}) + 2 \textrm{Pr}(H_1^{+}) \textrm{Pr}(L_1^{-}) \Big)\\
\textrm{Pr}(W_2) &= \frac{1}{2} \Big( \textrm{Pr}(H_2^{+}) \textrm{Pr}(H_2^{-}) + \textrm{Pr}(L_2^{+}) \textrm{Pr}(L_2^{-}) + 2 \textrm{Pr}(H_2^{+}) \textrm{Pr}(L_2^{-}) \Big)
\end{align}

Hence, we can express the win-rate difference using $d^{HH}, d^{HL}, d^{LH}, d^{LL}$ as follows:
\begin{align}
\textrm{Pr}(W_2)\! -\! \textrm{Pr}(W_1) &= \frac{1}{2} \Big(  d^{HH} + 2 d^{HL} + d^{LL}  \Big)\\
&= \frac{1}{2} \Big( AG\! -\! (1\! -\! a^m) G\! -\! (1\! -\! (\gamma a)^m) A + 2 ( (1\! -\! a^m) G\! -\! AG\! -\! (\gamma a)^m A ) + AG\! +\! a^m G\! +\! (\gamma a)^m A \Big)\\
&= \frac{1}{2} (G - A)
\end{align}
We note that this quantity is positive for sufficiently large $m$ because $\frac{G}{A} = \frac{1-\gamma a}{1 - a} \gamma^{m-1} = \frac{q^{\prime}}{\rho^{\prime}} (\frac{1 - q^{\prime}}{1 - \rho^{\prime}})^{m-1}$, which grows with $m$ because $\rho^{\prime} > q^{\prime}$ per Lemma~\ref{lemma:lemma1}.
\end{proof}

\paragraph{Case 2: $(\tilde{y}_0^{+}, \tilde{y}_0^{-}) = (1,1)$} We can think of this as the \textbf{``TP-vs-FP'' case}, where object $0$ is visible (correctly or spuriously) in both the ground-truth positive and ground-truth negative images. This leads to the same win-rate difference as the previous case.

\begin{lemma}
\label{lemma:lemma4}
If $(\tilde{y}_0^{+}, \tilde{y}_0^{-}) = (1,1)$ then $\textrm{Pr}(W_2) - \textrm{Pr}(W_1) = \frac{1}{2} (G - A)$.
\end{lemma}

\begin{proof}
In this case, we have $\bar{r}_1^{+}, \bar{r}_2^{+}, \bar{r}_1^{-}, \bar{r}_2^{-} \in \{1, 1 + \delta\}$, so we can say:
\begin{align}
\textrm{Pr}(W_1) &= \frac{1}{2} \Big( \textrm{Pr}(H_1^{+}) \textrm{Pr}(H_1^{-}) + \textrm{Pr}(L_1^{+}) \textrm{Pr}(L_1^{-}) + 2 \textrm{Pr}(H_1^{+}) \textrm{Pr}(L_1^{-}) \Big)\\
\textrm{Pr}(W_2) &= \frac{1}{2} \Big( \textrm{Pr}(H_2^{+}) \textrm{Pr}(H_2^{-}) + \textrm{Pr}(L_2^{+}) \textrm{Pr}(L_2^{-}) + 2 \textrm{Pr}(H_2^{+}) \textrm{Pr}(L_2^{-}) \Big)
\end{align}

Hence, the win-rate difference is the same as before, via the same steps as the previous lemma (Lemma~\ref{lemma:lemma3}):
\begin{align}
\textrm{Pr}(W_2) - \textrm{Pr}(W_1) &= \frac{1}{2} (G - A)
\end{align}
\end{proof}

\paragraph{Case 3: $(\tilde{y}_0^{+}, \tilde{y}_0^{-}) = (0,1)$} We can think of this as the \textbf{``FP-vs-FN'' case}, where object $0$ is occluded or obscured in the ground-truth positive image and spuriously visible in the ground-truth negative image. This is the most ``difficult'' case to rectify.

\begin{lemma}
\label{lemma:lemma5}
If $(\tilde{y}_0^{+}, \tilde{y}_0^{-}) = (0,1)$ then $\textrm{Pr}(W_2) - \textrm{Pr}(W_1) = \frac{1}{2} G \Big(1 - a^{m-1}(1 - \rho^{\prime}  + m \rho^{\prime} +  \rho^{\prime} \frac{1 - q^{\prime}}{q^{\prime}})  \Big)$.
\end{lemma}

\begin{proof}
In this case, we have $\bar{r}_1^{+}, \bar{r}_2^{+} \in \{0, 1\}$ and $\bar{r}_1^{-}, \bar{r}_2^{-} \in \{1, 1 + \delta\}$. The best we can hope for is a tie, where the positive example takes on its higher value \textit{and} the negative example takes on its lower value. Hence:
\begin{align}
\textrm{Pr}(W_1) &= \frac{1}{2} \textrm{Pr}(H_1^{+}, L_1^{-}) = \frac{1}{2} \textrm{Pr}(H_1^{+}) \textrm{Pr}(L_1^{-})\\
\textrm{Pr}(W_2) &= \frac{1}{2} \textrm{Pr}(H_2^{+}, L_2^{-}) = \frac{1}{2} \textrm{Pr}(H_2^{+}) \textrm{Pr}(L_2^{-})
\end{align}

Hence, we can express the win-rate difference as:
\begin{align}
\textrm{Pr}(W_2) - \textrm{Pr}(W_1) &= \frac{1}{2} d^{HL}\\
&= \frac{1}{2} \Big(  (1 - a^m) G - AG - (\gamma a)^m A  \Big)\\
&= \frac{1}{2} G \Big(  (1 - a^m)  - A - (\gamma a)^m \frac{A}{G}  \Big)\\
&= \frac{1}{2} G \Big(  (1 - a^m)  - A -  a^m \gamma \frac{1 - a}{1 - \gamma a}  \Big)\\
&= \frac{1}{2} G \Big(  (1 - a^m)  - m (1 - a) a^{m-1} -  a^m \gamma \frac{1 - a}{1 - \gamma a}  \Big)\\
&= \frac{1}{2} G \Big(1 - a^{m-1}(a  + m (1 - a) +  a \gamma \frac{1 - a}{1 - \gamma a})  \Big)\\
&= \frac{1}{2} G \Big(1 - a^{m-1}(1 - \rho^{\prime}  + m \rho^{\prime} +  \rho^{\prime} \frac{1 - q^{\prime}}{q^{\prime}})  \Big)
\end{align}
We once again note that this quantity is positive for sufficiently large $m$ because $a^{m-1}(1\! -\! \rho^{\prime}\!  +\! m \rho^{\prime}\! +\!  \rho^{\prime} \frac{1\! -\! q^{\prime}}{q^{\prime}}) = o(m a^m)$\\ since $a = (1 - \rho^{\prime}) < 1$ (per Lemma~\ref{lemma:lemma1}).
\end{proof}

\paragraph{Case 4: $(\tilde{y}_0^{+}, \tilde{y}_0^{-}) = (1,0)$} We can think of this as the \textbf{``TP-vs-TN'' case}, where there is no occlusion or spurious cue for object $0$ in either image. This is the ``easiest'' case to deal with, and it turns out to be the one case where a first-max is actually better than a second-max.

\begin{lemma}
\label{lemma:lemma6}
If $(\tilde{y}_0^{+}, \tilde{y}_0^{-}) = (1,0)$ then $\textrm{Pr}(W_2) - \textrm{Pr}(W_1) = \frac{1}{2} A \Big(G + a (1 - q^{\prime})^{m-1} (\frac{q^{\prime}}{\rho^{\prime}} + \frac{1 - q^{\prime}}{1 - \rho^{\prime}}) - 1 \Big)$.
\end{lemma}

\begin{proof}
In this case, we have $\bar{r}_1^{+}, \bar{r}_2^{+} \in \{1, 1+\delta\}$ and $\bar{r}_1^{-}, \bar{r}_2^{-} \in \{0, 1\}$. The worst thing that can happen is a ``tie'', in the event that the positive example gets its lower value \textit{and} the negative example gets its higher one, otherwise we get an outright ``win''. Hence:
\begin{align}
\textrm{Pr}(W_1) &= \textrm{Pr}(H_1^{+}, H_1^{-}) + \textrm{Pr}(H_1^{+}, L_1^{-}) + \frac{1}{2} \textrm{Pr}(L_1^{+}, H_1^{-}) + \textrm{Pr}(L_1^{+}, L_1^{-})\\
&= 1 - \frac{1}{2} \textrm{Pr}(L_1^{+}, H_1^{-})\\
&= 1 - \frac{1}{2} \textrm{Pr}(L_1^{+}) \textrm{Pr}(H_1^{-})\\
\textrm{Pr}(W_2) &= 1 - \frac{1}{2} \textrm{Pr}(L_2^{+}) \textrm{Pr}(H_2^{-})
\end{align}

Hence, we can express the win-rate difference as:
\begin{align}
\textrm{Pr}(W_2) - \textrm{Pr}(W_1) &= -\frac{1}{2} d^{LH}\\
&= -\frac{1}{2} \Big( (1 - (\gamma a)^m) A - AG - a^m G \Big)\\
&= \frac{1}{2} \Big( AG + a^m G - (1  - (\gamma a)^m) A \Big)\\
&= \frac{1}{2} A \Big(G + a^m \frac{G}{A} - (1  - (\gamma a)^m) \Big)\\
&= \frac{1}{2} A \Big(G + a^m \frac{1 - \gamma a}{1 - a} \gamma^{m-1} + (\gamma a)^m - 1 \Big)\\
&= \frac{1}{2} A \Big(G + a (\gamma a)^{m-1} (\frac{1 - \gamma a}{1 - a} + \gamma) - 1 \Big)\\
&= \frac{1}{2} A \Big(G + a (1 - q^{\prime})^{m-1} (\frac{q^{\prime}}{\rho^{\prime}} + \frac{1 - q^{\prime}}{1 - \rho^{\prime}}) - 1 \Big)
\end{align}
We note that in this particular case, $\textrm{Pr}(W_2) - \textrm{Pr}(W_1)$ actually becomes \textit{negative} as $m$ grows large, because both $G$ and $a (1 - q^{\prime})^{m-1} (\frac{q^{\prime}}{\rho^{\prime}} + \frac{1 - q^{\prime}}{1 - \rho^{\prime}})$ shrink exponentially with $m$, and so $\frac{1}{2} A$ is eventually multiplied by a negative number. However, we will soon see that this negative win-rate difference is outweighed by the positive ones from Lemmas~\ref{lemma:lemma3},~\ref{lemma:lemma4}, and~\ref{lemma:lemma5}, leading to an overall positive difference.
\end{proof}

We have now worked out the win-rate differences for all four possible values of $(\tilde{y}_0^{+}, \tilde{y}_0^{-})$. We are finally ready to prove our main theorem, which we restate below.\\

\restate{thm:theorem1}{If $\textrm{Pr}(\tilde{y}_0^{+} \neq y_0^{+} \bigvee \tilde{y}_0^{-} \neq y_0^{-}) > 0$, then $\textrm{Pr}(W_2) > \textrm{Pr}(W_1)$ for sufficiently large $m$.}

\begin{proof}
We start by defining a distribution over $(\tilde{y}_0^{+}, \tilde{y}_0^{-})$:
\begin{align}
\pi^{(0,0)} &:= \textrm{Pr}(\tilde{y}_0^{+} = 0,\ \tilde{y}_0^{-} = 0)\\
\pi^{(1,1)} &:= \textrm{Pr}(\tilde{y}_0^{+} = 1,\ \tilde{y}_0^{-} = 1)\\
\pi^{(0,1)} &:= \textrm{Pr}(\tilde{y}_0^{+} = 0,\ \tilde{y}_0^{-} = 1)\\
\pi^{(1,0)} &:= \textrm{Pr}(\tilde{y}_0^{+} = 1,\ \tilde{y}_0^{-} = 0)
\end{align}

We can combine the results of Lemmas~\ref{lemma:lemma3},~\ref{lemma:lemma4},~\ref{lemma:lemma5}, and~\ref{lemma:lemma6}, to get the overall win-rate difference:
\begin{align}
\label{supeq:theorem1_exact_difference}
\textrm{Pr}(W_2) - \textrm{Pr}(W_1)\quad = \qquad &\frac{1}{2} (\pi^{(0,0)} + \pi^{(1,1)}) (G - A)\\
+ &\frac{1}{2} \pi^{(0,1)}  G \Big(1 - a^{m-1}(1 - \rho^{\prime}  + m \rho^{\prime} +  \rho^{\prime} \frac{1 - q^{\prime}}{q^{\prime}})  \Big)\\
+ &\frac{1}{2} \pi^{(1,0)} A \Big(G + a (1 - q^{\prime})^{m-1} (\frac{q^{\prime}}{\rho^{\prime}} + \frac{1 - q^{\prime}}{1 - \rho^{\prime}}) - 1 \Big)
\end{align}

We can lower-bound the last term of this sum with $-A$ to get:
\begin{align}
\textrm{Pr}(W_2) - \textrm{Pr}(W_1)\quad \geq \qquad &\frac{1}{2} (\pi^{(0,0)} + \pi^{(1,1)}) (G - A)\\
+ &\frac{1}{2} \pi^{(0,1)}  G \Big(1 - a^{m-1}(1 - \rho^{\prime}  + m \rho^{\prime} +  \rho^{\prime} \frac{1 - q^{\prime}}{q^{\prime}})  \Big)\\
- &\frac{1}{2} \pi^{(1,0)} A
\end{align}

If we pick $m > 1 + \frac{\log(2) + \log(1 - \rho^{\prime}  + m \rho^{\prime} +  \rho^{\prime} \frac{1 - q^{\prime}}{q^{\prime}})}{-\log(1 - \rho^{\prime})}$ then we can lower-bound the second term to get:
\begin{align}
\textrm{Pr}(W_2) - \textrm{Pr}(W_1)\quad \geq \qquad &\frac{1}{2} (\pi^{(0,0)} + \pi^{(1,1)}) (G - A)\\
+ &\frac{1}{4} \pi^{(0,1)}  G\\
- &\frac{1}{2} \pi^{(1,0)} A
\end{align}

which simplifies to:
\begin{align}
\textrm{Pr}(W_2) - \textrm{Pr}(W_1) &\geq \frac{1}{2} \Big(  (\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)}) G - (\pi^{(0,0)} + \pi^{(1,1)} + \pi^{(1,0)}) A \Big)\\
&= \frac{1}{2} \Big(  (\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)}) G - (1 - \pi^{(0,1)}) A \Big)
\end{align}

We note that $\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)} > 0$ due to our assumption that $\textrm{Pr}(\tilde{y}_0^{+} \neq y_0^{+} \bigvee \tilde{y}_0^{-} \neq y_0^{-}) > 0$. Hence, we can do some rearrangement to get:
\begin{align}
\textrm{Pr}(W_2) - \textrm{Pr}(W_1) &\geq \frac{1}{2} (\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)}) G \Big(1 - (\frac{1 - \pi^{(0,1)}}{\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)}}) (\frac{A}{G}) \Big)\\
&= \frac{1}{2} (\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)}) G \Big(1 - (\frac{1 - \pi^{(0,1)}}{\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)}}) (\frac{1 - a}{1 - \gamma a}) \gamma^{-(m-1)})\\
&= \frac{1}{2} (\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)}) G \Big(1 - (\frac{1 - \pi^{(0,1)}}{\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)}}) (\frac{\rho^{\prime}}{q^{\prime}}) (\frac{1 - \rho^{\prime}}{1 - q^{\prime}})^{m-1})
\end{align}

We can make this expression positive by picking $m > 1 + \frac{\log(\rho^{\prime}) - \log(q^{\prime}) + \log(1 - \pi^{(0,1)}) - \log(\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)})}{\log(1 - q^{\prime}) - \log(1 - \rho^{\prime})}$.\\

Thus, the second-max will be advantageous over the first-max as long as $m$ satisfies the following lower bounds:
\begin{align}
m &> 1 + \frac{\log(2) + \log(1 - \rho^{\prime}  + m \rho^{\prime} +  \rho^{\prime} \frac{1 - q^{\prime}}{q^{\prime}})}{-\log(1 - \rho^{\prime})}\\
m &> 1 + \frac{\log(\rho^{\prime}) - \log(q^{\prime}) + \log(1 - \pi^{(0,1)}) - \log(\pi^{(0,0)} + \pi^{(1,1)} + \frac{1}{2} \pi^{(0,1)})}{\log(1 - q^{\prime}) - \log(1 - \rho^{\prime})}
\end{align}

In fact, in the special case where $\pi^{(1,0)} = 0$, these bounds are ``tight'' in the sense that an $m$ that violates both bounds will lead to the first-max being advantageous over the second-max. Theorem~\ref{thm:theorem2} gives further insight into this dependency on $m$.

\end{proof}

\subsection{Proof of Theorem 2}
\label{sec:theorem2_proof}
\begin{proof}
We prove Theorem \ref{thm:theorem2} by example. We give two example settings that fulfill the requirements of Theorem \ref{thm:theorem1} and for which $\textrm{Pr}(W_2) < \textrm{Pr}(W_1)$ for sufficiently small $m$. Furthermore, these two example settings differ only by $\nu$ and have different values of $m$ at which the inequality reverses, and so we know that this reversal point depends on $\nu$. Some further examples show that the reversal point also depends on other setting variables such as $\rho, q, \pi^{(0,0)}, \pi^{(1,1)},\pi^{(0,1)}, \pi^{(1,0)}$, but we limit our analysis to $\nu$ for the sake of brevity.

\begin{figure}[t]
  \centering
    \includegraphics[width=1\linewidth]{sec/figs/theory_example.png}
    \caption{$\textrm{Pr}(W_2) - \textrm{Pr}(W_1)$ via Eq.~\eqref{supeq:theorem1_exact_difference} for different values of $m$, proving by example that first-max can be advantageous for sufficiently small $m$, and that the point of advantage depends on label-flip probability $\nu$.}
    \label{fig:theorem2_example}
\end{figure}

Our two example settings share $\rho = 0.15,\ q = 0.01,\ \pi^{(1,0)} = 0.55^2,\ \pi^{(0,1)} = (1 - 0.55)^2$, and $\pi^{(0,0)} = \pi^{(1,1)} = 0.55 \cdot (1 - 0.55)$. They differ only in their values of $\nu$ which are $0.05$ and $0.2$. We evaluate $\textrm{Pr}(W_2) - \textrm{Pr}(W_1)$ via Eq.~\eqref{supeq:theorem1_exact_difference} for multiple values of $m$ under these settings and plot the resulting probability differences in Fig.~\ref{fig:theorem2_example}. We see that the claims in Theorem \ref{thm:theorem2} follow from these examples.
\end{proof}
