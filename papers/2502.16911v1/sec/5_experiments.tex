\section{Experiments}
\label{sec:experiments}

\paragraph{Datasets}
We benchmark our method on 3 different MLR datasets. They are COCO2014 \cite{COCO2014}, which has 40,504 test images with 80 classes; VOC2007 \cite{VOC2007}, which has 4.952 test images with 20 classes; and NUSWIDE \cite{nuswide}., which has 107,859 test images with 81 classes. We use only test, and not train, images from these datasets. We do use their conditional ground-truth label distributions, computed from the training set, to choose compound prompts, but note that our method requires only rough information about which classes are more or less likely to cooccur.\\

\noindent\textbf{CLIP Backbones}
In order to showcase the generality of our method, we apply it to 9 different CLIP backbones, which are ViT-L/14@336px, ViT-L/14, ViT-B/16, ViT-B/32, RN50x64, RN50x16, RN50x4, RN101, RN50. It is worth noting that from our model's perspective, each backbone could be seen as a dataset of its own, providing a different source of scores for each image set. We use the default 224x224 resolution, resizing without center crop, for all ViT models. For ResNet models, we use the same resolution as \cite{DualCoOp++} except when otherwise stated.

\subsection{Our method improves over vanilla ZSCLIP across many datasets and models}

\input{sec/tables/main_tables_pcaidea_tersified}

\begin{figure}[t]
  \centering
    \includegraphics[width=1.05\linewidth]{sec/figs/COCO2014_test_class_APs.png}
    \vspace{-20pt}
   \caption{\captionPerClassAPsCOCO}
   \label{fig:PerClassAPsCOCO}
\end{figure}

As a baseline, we compute singleton scores using prompt-ensembling with the 80 templates from the CLIP paper \cite{CLIP}. We then compute compound prompt scores and apply our method to the singletons and compounds. We compare our results to this vanilla ZSCLIP baseline.

Tab.~\ref{tab:MainTableThreeDatasets} shows our results over three different datasets and nine different CLIP backbones. We see that our method is able improve over vanilla ZSCLIP by 12.6\% on COCO, 8.8\% on VOC, and 7.9\% on NUSWIDE. Moreover, we see that our method acheives consistent improvements across all datasets and backbones, showcasing its generality. For example, the improvements range from 11.4-15.0\% for COCO, 7.4-10\% for VOC, and 6.1-10.8\% for NUSWIDE. In fact, we see in Fig.~\ref{fig:PerClassAPsCOCO} that SPARC consistently improves the AP of almost every class in COCO. We show similar improvements on VOC and NUSWIDE in Supplementary.

\subsection{Our method has complementary strengths}\label{sec:experiment_complementarity}

\input{sec/tables/competition_tables_pcaidea_tersified}

Several existing methods adapt CLIP to MLR tasks in an image-free (\cite{TaI-DPT}, \cite{DataFreeMLR}, \cite{TaiPlusPlus}, \cite{CoMC}, \cite{TaI-Adapter}) or training-free (\cite{CLIPSurgery}, \cite{TagCLIP}) manner. These approaches lie outside our scope, as we focus on methods that are deployable `out-of-the-box' on new datasets and VLMs, without requiring dataset-specific training or modifications to the VLM. Our black-box method allows seamless 'plug-and-play' integration, using scores from existing methods in place of the vanilla ZSCLIP singleton scores.

We combined our method with three existing methods that have complete, publicly-available codebases: (1) TagCLIP \cite{TagCLIP}, a training-free method that benchmarks on COCO and VOC with a ViT-B/16 backbone; (2) TaI-DPT \cite{TaI-DPT}, an image-free method that requires training, which benchmarks on COCO, VOC, and NUSWIDE with a RN50 backbone; (3) CoMC \cite{CoMC}, also an image-free method that requires training, which has a publicly-available RN50-based model for COCO with a single training seed.

There are some nuances regarding image preprocessing and score postprocessing. TaI-DPT and CoMC use different image preprocessing settings than us - theirs are lower resolution and take a center crop. For a fair comparison, we use these same preprocessing settings when getting compound prompt scores to combine with TaI-DPT and CoMC. We also try running both our and their method with our preprocessing setting, which we denote as ``RN50*''. We do not change our settings for TagCLIP, as they rely on a less conventional preprocessing setting for ViT-B/16 that would be somewhat difficult to translate between methods. As for score postprocessing, TagCLIP uses softmax, which we have found to have a debiasing effect, so we omit our image-level debiasing on TagCLIP's scores, and take the log of their scores to make them compatible with ours. We make no modifications to postprocessing for TaI-DPT or CoMC.

We show the results of these plug-and-play combinations in Tab.~\ref{tab:CompetitionTagCLIPlogYESrowcalibbaseNO} and Tab.~\ref{tab:CompetitionTaIDPTandCoMC}. We find that method is able to improve on TagCLIP by on average 2.6\% and on TaI-DPT by on average 1.7\%. We do not improve on CoMC, but the degradation is only 0.3\%. These results suggest that our method's signal complements those obtained through training and architectural manipulation by other methods.

\subsection{Ablations}

\subsubsection{Debias Module}

\input{sec/tables/debias_ablation_table}
We take a closer look at the Debias module by seeing how it impacts the performance of both singleton prompts and our full pipeline. We see in Tab.~\ref{tab:DebiasAblationTable} that the Debias module provides significant gains in both cases (7.7\% with singletons, 8.6\% full pipeline), confirming that image-level bias significantly impacts MLR when left unaddressed. %

\subsubsection{Rank Fusion Module}
\label{subsubsec:RankFusionAblation}
Our next ablation takes a closer look at the role of the Rank Fusion module. We consider some possible handcrafted strategies for combining compound prompt scores:
\begin{itemize}
    \item ``$k$max'': Use the $k$-th highest compound score, i.e. $r_{i,k}^t$
    \item ``mean$>=k$'': Use partial avg $\frac{1}{m_i-K+1} \sum_{k=K}^{m_i-K+1} r_{i,k}^t$
\end{itemize}
\begin{figure}[h]
  \centering
   \includegraphics[width=0.73\linewidth, trim=0 0 0 15, clip]{sec/figs/comprehensive_plot_compoundonly.png}
   \includegraphics[width=0.73\linewidth, trim=0 0 0 15, clip]{sec/figs/comprehensive_plot_avg.png}
   \caption{\captionRankFusionAblation}
   \label{fig:RankFusionAblation}
\end{figure}
Fig.~\ref{fig:RankFusionAblation} shows the average mAP of these strategies, as well as ``maxVariance'' \eqref{eq:maxVariance} and singleton-only, across all datasets and backbones, with and without the last ``merge'' step \eqref{eq:merge}. We see that the 1st-max does much worse than the 2nd-max, which does worse than the 3rd-max, and so on, up until a plateau somewhere around 5th-max. This lends credence to the idea that a weakened max is a more useful \mbox{signal} than outright max. We also see that the mean of all compound prompts (i.e. ``mean$>=\!1$'') does surprisingly well. This is actually consistent with our observation about weakened maxes - the mean weakens the 1st- and 2nd-max by averaging them with the lower ranks. We can improve further on the mean, to the point where we surpass the kmaxes, by excluding high ranks in the ``mean$>=k$'' strategy. However, we can surpass all of these options by adaptively weighting the scores using the max-variance direction.\\
Comparing the two figures also reveals that the ``merge'' step in \eqref{eq:merge} is critical and confirms that the score from ``maxVariance'' is indeed complementary to the singleton score.

We note that these results use CLIP's 80-prompt ensemble for singleton scores, indicating gains from compound prompting beyond basic prompt diversity.

\subsubsection{Compound Prompts}


The WaffleCLIP paper \cite{WaffleCLIP} found that prompt ensembles made of random texts could perform just as well as descriptive ensembles generated by LLMs, suggesting that the performance gains of the latter came not from semantics, but from the nature of ensembling itself. Given this result, we think it is always worth checking whether any prompt-enhancement method is actually benefiting from semantics.

To answer this question, we try using WaffleCLIP-style prompts in place of the compound prompts in our method. Specifically, for each classname $c_i$ we create 30 prompts of the form ``A photo of a $c_i$, which is [RAND]'', where ``[RAND]'' is 10 random characters. We try both the ``maxVariance'' strategy with merging and also ``mean$>=\!1$'' with merging, as the latter most closely resembles an ensemble.

We show the results of this ablation in the Supplementary. We see that randomized compound prompts do not offer any complementary signal to the singleton prompts. Thus, any gain from the compound prompts must come from their semantics. Just as our gains over random prompts come from semantics, they also extend beyond simple prompt diversity - our baseline singleton scores already incorporate CLIP's ensemble template.

\subsection{A qualitative look at the weakened max}
\label{subsec:qualitative}

\begin{figure}[t]
  \centering
   \includegraphics[width=0.8\linewidth]{sec/figs/histograms_coco_cat_vitL14336px.png}
   \caption{\captionQualitativeHist}
   \label{fig:QualitativeHist}
\end{figure}

We saw in Sec.~\ref{subsubsec:RankFusionAblation} that the second-highest compound prompt score is a much better signal than the highest one. This counterintuitive finding can be intuitively understood through object co-occurrence. When CLIP processes a compound prompt like 'cat and sofa', it has OR-like behavior - giving high scores if either object is present. The maximum score often comes from such prompts where only one object is present but strongly detected. In contrast, the second-highest scores tend to come from prompts where both objects have moderate confidence, providing a more reliable signal for true multi-label detection. This explains why fusing second-highest scores with singleton scores produces better separation between positive and negative cases.


Fig.~\ref{fig:QualitativeHist} shows the distributions of singleton, 1st-max, and 2nd-max scores for ``cat'' in COCO, as well as the uniform average of singleton and 2nd-max. We see in the second row that the 1st-max lifts a considerable number of negative examples, creating overlap between the negative and positive distributions. The third row shows that 2nd-max causes less overlap, lifting fewer negative examples without adversely impacting positive examples. The last row shows that fusing 2nd-max with singleton leads to good separation.

Fig.~\ref{fig:exampleFigure} shows an example of what happens at an image level. On the right is an image that contains a cat (curled up behind the two dogs on the bed); on the left is an image without a cat. The highest-scoring compound prompt is ``Cat and Dog'' for both images. This prompt gives a high score to the right image by detecting the dogs, but it also detects the dog in the left image for the same reason, and so it also misorders the images. But the second-highest-scoring prompts - ``Bed and Cat'' for right image, ``Cat and Potted plant'' for left - correctly order the images.
