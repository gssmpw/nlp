\section{Method}
\label{sec:method}


We detail our method, SPARC (see Alg.~\ref{alg:sparc} and Fig.~\ref{fig:methodFigure}).


\input{sec/pipeline_pseudocode}
\noindent \emph{Setup.} %
Suppose we have $M$ images and $N$ target classes, with classnames $c_1,...,c_N$. We assume that we have a set of ``singleton'' prompts describing each of these classes in isolation (e.g. ``picture of a cat''). Given an image $t$, let $s_1^t,...,s_N^t$ denote the similarity scores produced by a VLM when comparing the singleton prompts to that image.

A na\"{i}ve approach is to directly use these singleton scores to perform MLR. However, since the presence of a particular class alters the conditional probability of the remaining classes in images, it should be possible to further refine these scores by accounting for the multi-label structure of the images. In order to do this, we use the classnames to generate a further set of ``compound'' prompts $P$ which mention multiple classes. For instance, the classes ``cat'' and ``sofa'' may be compounded to ``cat and sofa''. Let $\{s_p^t : p \in P\}$ denote the scores given by the VLM when comparing these compound prompts to image $t$.

Using the singleton as well as the compound prompts above, SPARC produces a vector of refined class-wise scores $\zeta_1^t, \cdots, \zeta_N^t,$ for each image $t$. Given this structure, SPARC has three main components: (i) compound prompt generation to enable the use of CLIP scoring for MLR (ii) image and prompt level debiasing to allow scores to be directly compared, and (iii) Rank Fusion to combine singleton and compound prompt scores into a single score per class. We now discuss each of these aspects in detail. 

\subsection{Compound Prompt Generation}
Our compound prompt generation method relies on contextual associations from commonly observed visual patterns to inform prompt structure. Specifically, we remove object pairs and triplets that are implausible based on low-frequency observations in the data, retaining only combinations that align with realistic visual scenes. Leveraging these contextual associations, an off-the-shelf LLM generates descriptive prompts that expand beyond the target class names, enriching the diversity of object scenarios. While we explored various prompt templates ("A OR B", "A next to B", "A with B"), "A and B" performed best in initial experiments. We focus on this template to establish our core method, leaving systematic prompting as future work.
Pseudocode and details are available in the Supplementary.

\subsection{Debiasing}
Once we have obtained compound prompts $P$, we can query the VLM to obtain singleton scores $s_1^t,...,s_N^t$ and compound scores $\{s_p^t : p \in P\}$ for each image $t$. However, these scores contain both image- and prompt-specific biases. The former in particular is a problem, as it can change the order of scores between negative and positive images.

We address image-level bias for singleton and compound prompts, respectively, as follows:
\begin{align}\label{eqn:debias_images}
\tilde{s}_i^t := \frac{s_i^t - \hat{\mu}(s_{\cdot}^t)}{\hat{\sigma}(s_{\cdot}^t)} \quad \textit{and} \quad
\tilde{s}_p^t := \frac{s_p^t - \hat{\mu}(\check{s}_{\cdot}^t)}{\hat{\sigma}(\check{s}_{\cdot}^t)},
\end{align}
where $\hat{\mu}(s_{\cdot}^t)$, $\hat{\mu}(\check{s}_{\cdot}^t)$, $\hat{\sigma}(s_{\cdot}^t)$, $\hat{\sigma}(\check{s}_{\cdot}^t)$ are sample means and standard deviations across the prompt dimension for a single image. $\check{s}_1^t,...,\check{s}_N^t$ are the scores of ``auxiliary'' prompts that only mention the classnames $c_1,...,c_N$ and are used only to obtain statistics for standardizing the compound prompt scores. We use these instead of singleton prompts for this purpose as the latter might have different statistical properties depending on the nature of the singleton prompting method.

We also standardize across images to quash prompt bias:
\begin{align}\label{eqn:debias_classes}
\overline{s}_i^t := \frac{\tilde{s}_i^t - \hat{\mu}(\tilde{s}_i^{\cdot})}{\hat{\sigma}(\tilde{s}_i^{\cdot})} \quad \textit{and} \quad
\overline{s}_p^t := \frac{\tilde{s}_p^t - \hat{\mu}(\tilde{s}_p^{\cdot})}{\hat{\sigma}(\tilde{s}_p^{\cdot})},
\end{align}
where $\hat{\mu}(\tilde{s}_i^{\cdot})$, $\hat{\mu}(\tilde{s}_p^{\cdot})$, $\hat{\sigma}(\tilde{s}_i^{\cdot})$, and $\hat{\sigma}(\tilde{s}_p^{\cdot})$ now denote sample means and SDs across the images. Removing prompt-level bias makes scores from different prompts more compatible with each other, which is important for fusing them.

\subsection{Rank Fusion}
Our goal now is to find a way to use the compound scores to strengthen the singleton scores. Let us define a bit more notation here. Let $C(p)$ be a function that maps each compound prompt to the set of classes in $[N]$ that are mentioned by that prompt. Let $r_{i,k}^t$ denote the $k$-th largest element of the set $\{\overline{s}_p^t : i \in C(p)\}$.

A natural choice might be to choose $r_{i,1}^t$, i.e. the highest scoring prompt. After all, we would expect this to be the prompt that most closely describes the image, and if the image did not contain $c_i$ then we would expect none of the prompts containing $c_i$ to score high. However we find that $r_{i,1}^t$ does not offer a good detection of classes. The mechanism behind this, which is explored in detail in \S\ref{sec:noisemodel}, is that when using a compound prompt of the form `A and B' to detect the class $A$, the score sees a large increase if the object $B$ truly occurs in the image. This `OR'-like behaviour (\S\ref{sec:noisemodel}) means that maximum score $r_{i,1}^t$ typically captures the effect of other classes than $A$ being present in the image, and thus leads to very poor separation between ground truth positive and negative images for any class. Counterintuitively, then, we find that `\emph{weakened maxes}', i.e., lower order statistics like $r_{i,2}^t$ and $r_{i,3}^t$ yield much better separation. 

The above fact necessitates that we need to deal with the entirety of the order statistics $\{r_{i,k}^t\}$ in order to generate an effective fusion rule. We fuse these scores by projecting the vector of order statistics along the direction of highest variance, i.e., we compute the fused compound score $\tilde{\zeta}_i^t$ as
\begin{align}
\label{eq:maxVariance}
w^{i*} &:= \argmax_{w^i} \textrm{Var}_t(w_0^i \bar{s}_i^t + \sum_{k} w_k^i r_{i,k}^t )\\
\tilde{\zeta}_i^t &:= w_0^{i*} \bar{s}_i^t + \sum_{k} w_k^{i*} r_{i,k}^t. \label{eqn:pca_projection}
\end{align}
Notice that the variance above is computed across all images. The main idea is similar to linear discriminant analysis. Indeed, if we assume that the mean compound score are when class $i$ is actually present or absent differ from one another, then the collection of the compound scores across all images can be modeled as a mixture of two noisy distributions, with some mean separation. If we further assume that the noise in these scores are roughly homoskedastic in either component, then, as long as the `signal' induced by the above mean shift is large compared to the noisy variation, highest variance direction $w^{i*}$ captures precisely the direction of the mean shift, and thus $\tilde{\zeta_i^t}$ effectively computes the component of the score vector that lies along this signal direction. While principled, this max-variance strategy is of course not necessary to execute, and we compare it to alternative fusion strategies in \S\ref{subsubsec:RankFusionAblation}. We add (i.e. ``merge'') this fused score into the original singleton score to get our final score:
\begin{equation}
\label{eq:merge}
\zeta_i^t := s_i^t + \tilde{\zeta}_i^t
\end{equation}
In the next section we probe CLIP scores through a statistical lens to build intuition on our approach.
