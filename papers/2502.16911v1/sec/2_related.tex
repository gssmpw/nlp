\subsection{Related Work}
\label{sec:related}




\noindent \textbf{Supervised Methods.} 
Prior methods improve MLR performance through various approaches. DualCoOp \cite{DualCoOp,DualCoOp++} trains prompts for text-guided spatial attention. Subsequent works use class co-occurrences to refine CLIP logits \cite{CoocGCN,HSPNet}. Hierarchical structuring \cite{HSPNet} encourages related classes to learn similar prompts. SSPA \cite{SSPA} combines embeddings from learned and LLM-generated prompts, and TRM-ML \cite{TRM-ML} uses pseudolabels to match texts to image regions. While effective, these methods require annotated training data. In contrast, ours is unsupervised.



\noindent \textbf{Unsupervised Training-Based Methods.} 
TaI-DPT \cite{TaI-DPT} uses caption-only data to adapt CLIP to MLR by training a prompt-generation network using caption embeddings. Extensions include LLM-generated captions \cite{DataFreeMLR}, pseudo-visual prompts \cite{TaiPlusPlus}, and lightweight classifiers on caption embeddings \cite{CoMC, TaI-Adapter}. Separately, CDUL \cite{CDUL} leverages CLIP scores with spatial aggregation to create training pseudolabels. While these approaches demonstrate shared vision-language embedding power, they require significant training and CLIP internal access. In contrast, our black-box method uses crude cooccurrence-based compound prompts (essentially deleting implausible contextual associations) to probe multilabel information, fusing scores for effective MLR.








\noindent\textbf{Unsupervised Training-free methods.} The recent literature has proposed methods to adapt CLIP to the MLR taks in a training-free manner via architectural changes. CLIPSurgery \cite{CLIPSurgery} alters the architecture to increase the coherence of attention masks with in the image backbone, and debiases features on a patch-by-patch basis. TagCLIP \cite{TagCLIP} uses masks from within the CLIP image backbone to refine scores at a patch level based on regional coherence, and aggregate queries over crops of the image. Again, both of these methods require white-box access to the internals of the VLM model, while ours works in a black-box manner. %

\noindent\textbf{Prompt Enhancement for Single-Label Recognition.} Several methods use LLM-generated descriptive prompts for single-label recognition, combining them via max or mean fusion \cite{CuPL,DCLIP,FuDD,CHiLS}. While related, our compound prompts specifically target MLR by modeling class relationships rather than enriching individual class descriptions.

\noindent \textbf{Complementarity of SPARC to Unsupervised Methods.} We explicitly note that our methodology is complementary to much of the work on unsupervised VLM-based MLR. For instance, since we assume only black-box access to VLM scores, our crude compound prompts can be refined via prompt tuning in the manner of TaI-DPT~\cite{TaI-DPT}, and can be applied at a patch-by-patch level and integrated \`{a} la TagCLIP~\cite{TagCLIP}. The resulting scores can then directly be combined via our debiasing and rank-fusion approach, and thus yield improvements to these prior approaches. \S\ref{sec:experiment_complementarity} investigates the resulting improvements. 















