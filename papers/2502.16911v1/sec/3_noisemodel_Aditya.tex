\section{Noise Model of a VLM}
\label{sec:noisemodel}



Our methodological choices are strongly driven by the behaviour of CLIP scores under compound prompts, and this leads to both the debiasing approach taken, as well as our counterintuitive choice of exploiting the weakened maxes rather than just fusing the compound prompts via the maximum score. In order to justify these choices, we probe the structure of CLIP scores under compound prompts by constructing a simple model of the same.

Given an image $t$ and a prompt with classes $i$ and $j$, we model the score of compound prompt `$i$ and $j$' as \mbox{$s_{i,j}^t := \theta_1^t \cdot f(y_i^t, y_j^t) + \theta_0^t + \varepsilon$}
where $y_i^t, y_j^t \in \{0,1\}$ indicate ground-truth presence of of the two classes, $\theta_1^t$ and $\theta_0^t$ represent image-level bias (e.g. due to visual domain-shifts), and $\varepsilon$ is a zero-centered Gaussian noise.
Notice that our Debias module is well-suited to mitigate variations in $\theta_0^t$ and $\theta_1^t$, at least when $\varepsilon$ is not strongly dominating. 

Now, the core object of interest is the map $f : \{0,1\}^2 \to \mathbb{R},$ which is responsible for modeling class pair interactions and prompt-level biases (see Table~\ref{tab:NoiseModelTable}). In an ideal situation for MLR, $f$ would behave roughly as an `AND' function, so that compound prompts would let us directly hone in on the presence of pairs of classes. However, prior work has observed that instead, for CLIP-type models, $f$ tends to predominantly behave as an `OR' function, in that $f$ is high if either $i$ or $j$ is present. Notice that if $f$ were to exactly behave as an OR, compound prompts would not be effective in detecting classes that were missed by the singleton scores.

In experimentation, we found that the behaviour of $s_{i,j}^t$ is intermediary to the above extremes: CLIP scores for compound prompts behave qualitatively as an OR function with a small AND `bonus' when both classes are present. This bonus, as well as the particular scores, are modulated by the classes mentioned (a consequence of prompt-level bias for each class) which we model as linear effects, leading to an overall model of the signal term of the CLIP score as \begin{align}
f(y_i^t, y_j^t) &:= \max(a_0^i + y_i^t a_1^i, a_0^j + y_j^t a_1^j) \nonumber \\ &+ \delta_{ij} \min(a_0^i + y_i^t a_1^i, a_0^j + y_j^t a_1^j),
\end{align} where the $a$ terms are class-wise linear effect coefficients, and $\delta_{ij}$ captures the strength of this AND bonus (which depends on which pair is being queried).

Note that this AND bonus is a critical feature of CLIP scores in their use for MLR problems, since this bump allows us to both infer the presence of classes that singleton prompts have missed, and indirectly to filter false positives (since the true positives are further elevated by the bonus).

\input{sec/tables/noise_model_table_aditya}

Table~\ref{tab:NoiseModelTable} shows the results of a systematic evaluation of the AND-OR structure of the compound CLIP scores. We evaluate a variety of models of $f$, ranging from simple constant, `only OR,' and `only AND' models to additive models and OR+AND-bonus models with both a `static' $\delta$ shared by all class pairs, and a `variable' $\delta_{i,j}$ that is specific to each class pairs. The final entry is a `look-up table' (LUT) model wherein all four values of $f$ are allowed to vary arbitrarily, which serves as the most expressive baseline in this model class. These are evaluated w.r.t. the fraction of variance unexplained (i.e., $1-R^2,$ where $R$ is the residual). Observe that while the only OR model has remarkable fidelity, the additive and AND-bonus models significantly improve upon this, and allowing $\delta$ to vary with $i,j$ yield a further significant improvement to within a $6\%$ extra loss from the LUT model. This justifies our above model, and shows that it captures a significant amount of the qualitative behaviour of CLIP scores for compound prompts. 

A closer look at these fitted models reveals the $\delta$ values in these models are both small, and relatively stable: for the static $\delta,$ the recovered value is $0.56$, and the interquantile range of the varying $\delta_{ij}$ is $(0.44, 0.57)$.  %

We find similar behavior when examining other backbones and datasets, which we detail in the Supplementary.

We now use this noise model to explain the strengths of the weakened max from a theoretical perspective.

\noindent \textbf{Theoretical Explanation for Weakened Max.}
We observe empirically in Sec.~\ref{subsubsec:RankFusionAblation} that using the second-max score outperforms the first-max in discriminative ability. %
Consider a target class \(0\) with cooccurring classes \(1, \dots, m\) and assume that each cooccurrence is governed by a simple distribution with probabilities \(\rho\) and \(q\) as follows:
\[
    \Pr(y_i = 1 \mid y_0 = 1) = \rho, \quad \Pr(y_i = 1 \mid y_0 = 0) = q < \rho.
\]
Let us now assume that the scores for these compound prompts are derived from the following simplified noise model:
$s_{0,i} = \max(\tilde{y}_0, \tilde{y}_i) + \delta \min(\tilde{y}_0, \tilde{y}_i) + \varepsilon,$
where \(\varepsilon \sim \mathcal{W}(\sigma)\) is additive noise, and $\tilde{y}_0,\, \tilde{y}_i \in \{0,1\}$ are noisy, potentially flipped versions of ground truth $y_0, y_i,$  respectively. This label-flip noise models the effects of real-world conditions such as occlusion or partial visibility of each object in the input to the VLM. We informally state our theoretical finding. See Supplementary for full details:\\
\textbf{Theorem (informal)} For any distribution of label noise on \(y_0\), the second-max consistently improves discrimination over the first-max, given enough cooccurring classes.
\subsection{Methodological Implications}

The qualitative structure of the OR+AND-bonus behaviour observed in the previous section offers a robust justification for our methodological choices in \S\ref{sec:method}. Naturally, image level debiasing reduces the effect of variation in $\theta_0^t$ (via the mean subtraction) and $\theta_1^t$ (via the scaling by standard deviation). Further, prompt level debiasing reduces the effect of variations in $a_0$ and $a_1$ across classes. 

We can also qualitatively observe that due to the fact that the OR structure of the scores dominates the AND structure, the behaviour of the largest score $r_{i,1}^t$ should often reflect the presence of a related class $j$ instead of that of $i$, and suggest that subsequent order statistics are more informative. In fact, this property can be theoretically derived under mild assumptions, as stated above. 





















