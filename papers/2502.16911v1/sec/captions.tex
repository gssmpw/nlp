\newcommand{\captionMainTableCOCO}{Main results for COCO.}
\newcommand{\captionMainTableVOC}{Main results for VOC.}
\newcommand{\captionMainTableNUSWIDE}{Main results for NUSWIDE.}
\newcommand{\captionMainTableAvgArch}{Results over three datasets, averaged over backbones.}
\newcommand{\captionCompetitionTagCLIPlogNOrowcalibbaseNO}{Results from combining our method with TagCLIP.}
\newcommand{\captionCompetitionTagCLIPlogNOrowcalibbaseYES}{Results from combining our method with TagCLIP.}
\newcommand{\captionCompetitionTagCLIPlogYESrowcalibbaseNO}{Shows complementarity with architectural approaches, improving TagCLIP by 2.6\%}
\newcommand{\captionCompetitionTagCLIPlogYESrowcalibbaseYES}{Results from combining our method with TagCLIP.}
\newcommand{\captionCompetitionTaIDPTrowcalibbaseNO}{TaI-DPT results}
\newcommand{\captionCompetitionTaIDPTrowcalibbaseYES}{TaI-DPT results}
\newcommand{\captionCompetitionCoMCrowcalibbaseNO}{CoMC results}
\newcommand{\captionCompetitionCoMCrowcalibbaseYES}{CoMC results}
\newcommand{\captionComphrehensiveCompoundOnly}{Average mAP (across all datasets and architectures) for different Rank Fusion strategies, without ``merge'' step.}
\newcommand{\captionComphrehensiveAvg}{Average mAP (across all datasets and architectures) for different Rank Fusion strategies, with ``merge'' step.}
\newcommand{\captionComphrehensivePCA}{Average mAP (across all datasets and architectures) for different selection strategies, PCA ensemble.}
\newcommand{\captionNoiseModelResults}{Comparing different noise models for pair prompt scores.}
\newcommand{\captionDebiasAblationTable}{Ablations on Debias module. Quantifies impact of debiasing both with and without compound prompts.}
\newcommand{\captionQualitativeHist}{Histograms for singleton, 1st max, and 2nd max scores for ``cat'' in the COCO dataset. We see that 1st max creates overlap by lifting the scores of some ground-truth negatives. 2nd max does not create these issues and performs well when fused with singleton scores.}
\newcommand{\captionQualitativeExample}{An example of why 2max works better than 1max. The left image gets a lower singleton score than the right image, resulting in a misordering. The 1max strategy lifts the scores of both images and thus fails to correct the misordering. The 2max strategy successfully corrects the misordering by lifting only the left image's score.}
\newcommand{\captionOurMethodSchematic}{A schematic of our method. \textbf{Top:} We use the target classnames along with cooccurrence stats to create compound prompts that mention multiple classes together. \textbf{Bottom:} At inference time, we query the VLM with both singleton and compound prompts, and use debiasing, selection, and fusion to come up with refined scores.}
\newcommand{\captionIntroFigure}{\textbf{Top Left:} Motivating example where the second-highest-scoring compound prompt does a better job than the highest-scoring one at correcting a misordering. \textbf{Top Right:} Our method uses classnames and cooccurrence info to generate compound prompts that mention multiple objects. \textbf{Bottom Left:} We use Debias and Rank Fusion modules to refine singleton scores by combining them with compound scores. \textbf{Bottom Right:} Rank Fusion module sorts the compound scores weights them based on max variation direction.}
\newcommand{\captionMethodFigure}{(top) Vision Language models(VLMs) like CLIP can be used for zero-shot classification with image-text similarity scores. While this works fairly well for single-class labels, they can struggle in the multi-label scenario. (bottom) In this paper, we instroduce SPARC, our solution that functions on top of an existing VLM, treating it simply as a black-box score generator. Using class names, SPARC first creates compound prompts for additional queries to the VLM. It then debiases, ranks and appropriately fuses them to generate final scores for the original classes.}
\newcommand{\captionExampleFigure}{A motivating example with an image where class ``cat'' is absent (left) and one where it is present (right). The highest compound prompt score is an unhelpful signal because it gives a high score to both negatives and positives, while the second-highest is more discriminative. Our method adaptively fuses the most informative order statistics, resulting in a strong signal.}
\newcommand{\captionCompetitionTaIDPTandCoMC}{Results from combining our method with TaI-DPT (left) and CoMC (right) showing compatibility with training-based methods. Our method improves TaI-DPT by 1.7\% while preserving the existing strong signal of CoMC, degrading it by only 0.3\%.}%
\newcommand{\captionMainTableThreeDatasets}{Results over three datasets and nine CLIP backbones. Our proposed method consistently outperforms the ZSCLIP baseline across all datasets and architectures exhibiting its effectivness on multi-label recognition tasks.}
\newcommand{\captionRankFusionAblation}{Average mAP for different Rank Fusion strategies, without (top) and with (bottom) the ``merge'' step \eqref{eq:merge} demonstrates superiority of adaptive fusion over fixed strategies.}
\newcommand{\captionPerClassAPsCOCO}{Per-class APs (averaged over all CLIP backbones) for our method vs vanilla ZSCLIP on the COCO dataset. Our method consistently improves over ZSCLIP for almost every class. Plots for VOC and NUSWIDE are shown in the Supplementary.}
