\section{Related Works}
\label{sec:works}

In this section, we first discuss the existing works which conduct theoretical analysis of semi-dual OT losses (\S \ref{subsec:works-OT}). Since minimax objective \eqref{minimax} resembles adversarial training, we additionally discuss some of the literature dealing with theoretical aspects of Generative Adversarial Neural Networks (GAN) losses and how they relate to ours (\S \ref{subsec:works-gans}). 

\subsection{Theoretical analysis of continuous semi-dual OT solvers}\label{subsec:works-OT}

The current progress in theoretical analysis of semi-dual OT could be subdivided into two branches.

\textbf{Non-minimax semi-dual OT.} The first branch of works ____ analyze the non-minimax losses \eqref{dualconvex} and \eqref{dualconvexcvx} and develop error bounds \eqref{eq-obj-of-study} for pushforward maps $T$ given by the gradient of dual potentials, i.e., $\EXY \Vert \nabla \overline{\phi}^* - \nabla \widehat{\overline{\phi}}_{\omega} \Vert^2_{L^2(p)}$. To achieve particular statistical rates, the authors place certain restrictions on the considered problem setup and deal with specific classes of functions (maps), e.g., wavelet expansions or (kernel-smoothed) plug-in estimators. Recent studies in this direction ____ extend the analysis of error rates for the class of potentials $\phi$ given by neural networks, e.g., input convex neural networks. Meanwhile, none of the mentioned works treats the learned map $T$ separately from dual potential $\phi$. Importantly, the analysis of minimax objective \eqref{minimax} is considerably more challenging than \eqref{dualconvex} due to the additional ``degree of freedom'' given by the optimized map $T$. Furthermore, saddle-point problems such as \eqref{minimax} are known to be more tricky for theoretical investigation than usual minimization. At the same time, the practical demand stemming from the recent proliferation of minimax-based OT solvers, see \S \ref{background}, makes such an investigation highly desirable. All of this necessitates separate study of minimax OT solvers; the adaptation of existing non-minimax results is questionable, if that is even possible.

\textbf{Minimax semi-dual OT.} The estimation of \eqref{eq-obj-of-study} in case of minimax OT \eqref{minimax_emp} is a much less explored task. In fact, there are no studies at all that provide statistical bounds for recovered minimax OT map $\widehat{T}_\omega$. The existing works ____ only conduct an analysis of $\Vert T^* - \widehat{T}_{\omega} \Vert^2_{L_2(p)}$ in the sense of \textit{duality gaps}. That is, the error between the true OT map and recovered map is upper-bounded by values of functional $L(\phi, T)$. The duality gaps analysis helps to validate minimax methodology; it is common for more broad class of (non-quadratic) minimax OT solvers ____. However, we again emphasize that it does not reveal particular statistical rates and guarantees for \eqref{eq-obj-of-study}. In ____, the authors establish promising results for the error of recovered approximate OT map. However, their problem formulation of min-max OT differs from ours, eq. \eqref{minimax}, as they aim to treat OT as the limit of regularized GAN objectives. 

\textbf{Theoretical analysis of other OT formulations.} For the completeness of exposition, we also mention several studies that develop statistical learning analysis of recovered OT map (plan) error for non semi-dual OT formulations. The works ____ deal with Entropy-regularized OT; ____ investigate unbalanced OT versions.
Although these works are interesting and insightful, their object of study is different from ours, making them less relevant.

\subsection{Theoretical analysis of adversarial Generative Models}\label{subsec:works-gans}
The objective of the semi-dual minimax continuous OT solvers \eqref{minimax} resembles that of GANs ____. This fact motivates us to review some of the existing theoretical results on GANs below. Still, while there are many papers on GANs ____ and a relatively large number of works studying the theoretical aspects of their objectives, they are not relevant to us for two reasons. First, OT solvers and GANs pursue different goals. The main goal of OT solvers is to approximate true OT maps $T^*$, i.e., to yield specific generators which satisfy the optimality condition; accordingly, our theoretical work focuses on the error of this approximation. Meanwhile, GANs objectives usually do not have a unique optimal generator $G^*$; thus, existing theoretical results mostly investigate the error of approximating the ground-truth distribution $q$ by the generated $p_{gen}$. Second, OT solvers and GANs have an evident difference corresponding to the order of 
optimization over generator $G$ (map $T$) and discriminator $D$ (potential $\phi$) in their objectives. In particular, for GANs, the optimization over the generator $G$ is done in the outer problem of their objective, while for OT solvers, the optimization over OT maps is conducted in the inner problem, i.e., the OT map $T^*$ corresponds to the solution of the inner problem. We emphasize the importance of this difference $-$ for OT solvers, OT map $T^*$ corresponds to the solution of the inner problem, which makes the task of theoretical investigation of the error of $T^*$ approximation even more difficult.
Below we discuss two groups of theoretical results on GANs which are slightly relevant to us.


\textbf{Estimation error of distance between generated and true distributions}. The early work of ____ introduces a notion of an $\mathcal{F}$-\textit{distance}
between the pair of measures w.r.t. a given class of discriminators $\mathcal{F}$:
$$
d_{\mathcal{F}}(p,q)\!=\! \sup_{D\in \calF} \bbE_{x\sim p} \psi(D(x)) + \bbE_{y\sim q} \psi(1-D(y))-2\psi(\frac{1}{2}).
$$
Here $\psi:[0,1]\rightarrow\bbR$ is any monotone concave function.
For certain classes of functions, these distances define an inner optimization problem for well-known GANs, e.g., Wasserstein, Vanilla, MMD, Sobolev, etc. Class $\mathcal{F}_{nn}$ corresponding to neural networks with a bound on a number of parameters yields a so-called \textit{neural net (NN) distance}. 
The paper ____ establishes an upper bound on the estimation error of this distance, i.e., difference of NN distances $d_{\mathcal{F}_{nn}}(q, p_{gen})$ for empirical and continuous distributions. Subsequent work ____ extends this analysis by providing a tighter upper bound and lower bound for this difference. In some sense, these works analyze the error in GANs inner optimization problem coming from the usage of empirical measures $\widehat{p}$, $\widehat{q}$ instead of the real ones.
Still, these works ignore the errors rooted in outer optimization problem of GANs. 

\textbf{Estimation error of GANs.} Other class of papers takes into account errors in GANs' \textit{outer} optimization problem. They commonly consider the solutions of GANs' empirical optimization problem:
$\widehat{w}^* = \arg\min_{w\in \Omega} d_{\mathcal{F}_{nn}}(\widehat{q},\widehat{p}_{gen(w)})
$ where $\widehat{p}_{gen(w)}$ denotes the distribution generated from empirical samples of distribution $\widehat{p}$ by a generator $G_{w}$ (with parameters $w$ from the class $\Omega$). Then the papers derive upper bounds for the estimation error defined as
$$
d_{\mathcal{F}}(q, p_{gen(\widehat{w}^*)}) - \inf_{w\in \Omega} d_{\mathcal{F}}(q, p_{gen(w)})
$$
for different variants of function classes $\calF$.
For example, ____ derives an upper and minimax lower bound for this error and $\calF_{nn}$ class; ____ considers the same class but a different type of error using $w^*=\arg\min_{w\in \Omega} d_{\mathcal{F}}(\widehat{q},p_{gen(w)})$ where $p_{gen(w)}$ is the distribution generated by $G_w$ from $p$; ____ and ____ study the same type of error but consider the discriminator class in Sobolev space; ____ extends these result by modeling the discriminator and generator as Besov function classes. In general, these bounds are constructed based on Rademacher complexities of function spaces and are utilized to construct the bounds on the distance between $p_{gen}$ and $q$.

For completeness, we mention several other works which investigate theoretical properties of specific GANs, e.g., bidirectional ____, Wasserstein ____ and CycleGAN ____, ____, ____.