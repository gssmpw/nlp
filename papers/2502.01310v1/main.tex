%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}
\usepackage{soul}
\usepackage{ulem}
\usepackage{cancel}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\input{math_symbols}

\renewcommand{\phi}{\varphi}
\let\hat\widehat % always use widehat

\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers}

\begin{document}

\twocolumn[
\icmltitle{A Statistical Learning Perspective on \\Semi-dual Adversarial Neural Optimal Transport Solvers}

\begin{icmlauthorlist}
\icmlauthor{Roman Tarasov}{sk}
\icmlauthor{Petr Mokrov}{sk}
\icmlauthor{Milena Gazdieva}{sk,airi}
\icmlauthor{Evgeny Burnaev}{sk,airi}
\icmlauthor{Alexander Korotin}{sk,airi}
\end{icmlauthorlist}

\icmlaffiliation{sk}{Skolkovo Institute of Science and Technology, Moscow, Russia}
\icmlaffiliation{airi}{Artificial Intelligence Research Institute, Moscow, Russia}

\icmlcorrespondingauthor{Roman Tarasov}{roman.tarasov@skoltech.ru}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]


\printAffiliationsAndNotice{} 

\begin{abstract}
Neural network based Optimal Transport (OT) is a recent and fruitful direction in the generative modeling community. It finds its applications in various fields such as domain translation, image super-resolution, computational biology and others. Among the existing approaches to OT, of considerable interest are adversarial minimax solvers based on semi-dual formulations of OT problems. While promising, these methods lack theoretical investigation from a statistical learning perspective. Our work fills this gap by establishing upper bounds on the generalization error of an approximate OT map recovered by the minimax quadratic OT solver. Importantly, the bounds we derive depend solely on some standard statistical and mathematical properties of the considered functional classes (neural networks). While our analysis focuses on the quadratic OT, we believe that similar bounds could be derived for more general OT formulations, paving the promising direction for future research. 
\end{abstract}

\vspace{-4mm}\section{Introduction}
\label{intro}
In recent years, there has been a boom in the development of computational Optimal Transport (OT) which has been identified as a powerful tool of solving various machine learning problems, e.g., biological data transfer \citep{bunne2023learning,koshizukaneural,vargas2021solving}, image generation \citep{wang2021deep,de2021diffusion,chenlikelihood} and domain translation \citep{xie2019scalable,fan2023neural} tasks. The first works in the OT field had proposed methods for solving OT problems between discrete distributions \citep{cuturi2013sinkhorn, peyre2019computational}. The next milestone was the emergence of OT-based neural methods using the OT cost as a loss function for updating the generator in generative models \citep{sanjabi2018convergence,gulrajani2017improved, petzka2018regularization, liu2019wasserstein}. Only recently, ML community has experienced an explosion of interest in developing scalable neural methods which compute OT map (or plan) and use it directly as the generative model, see, e.g., \citep{rout,daniels2021score}. Methods of this kind are usually attributed as \textit{continuous OT solvers} and typically have min-max optimization objectives based on semi-dual formulations of OT problems. 

Although a large number of neural network based min-max OT solvers have already been developed, e.g., \citep{fan2023neural,not,knot,genconddist, enot, domain}, there is a relatively limited understanding of the theoretical aspects of their losses. Existing theoretical investigations \citep{makkuva, rout, fan2023neural,gnot,barycenters} do not reveal practical convergence guarantees, i.e., statistical rates, of such min-max OT solvers. 

\textbf{Contributions.} In our paper, we fill the aforementioned gap by conducting a thorough theoretical investigation of min-max quadratic OT solvers. \textit{First}, we show that for the min-max OT solvers, the \textit{generalization error}, i.e., the difference between true and approximate OT maps, can be upper bounded by the sum of estimation and approximation errors (Theorem \ref{thm:decomp}). \textit{Second}, we specify the bounds for the estimation errors by showing that they depend only on the Rademacher complexities of the classes of the neural networks used (Theorem \ref{thm:estim_error}). \textit{Third}, we show that the approximation errors can be made arbitrarily small by choosing the appropriate classes of neural networks (Theorems \ref{thm:approx_inner}, \ref{thm:approx_outer}). As a result, we establish the \textit{learnability guarantees} for min-max OT solvers by showing that their generalization errors also can be made arbitrarily small by choosing the sufficient number of samples and appropriate classes of neural networks (Theorem \ref{thm:main}, Corollary \ref{corollary-rademacher}).

\textbf{Notations.}
In the manuscript, $\calX, \calY \subset \bbR^D$ are compact subsets; continuous scalar-valued functions on $\calX$ are $\calC(\calX)$; absolute continuous probability distributions on $\calX$ are $\Pac(\calX)$. We use the same symbol ``$p$'' to denote a continuous distribution $p \in \Pac(\calX)$ and its density $p(x)$ at some point $x \in \calX$. For a measurable map $T : \calX \rightarrow \calY$, we use $T_{\#}$ to denote the associated push-forward operator. Throughout the text, $\Vert \cdot \Vert_{L^2(\rho)}$ denotes the $L^2$ norm w.r.t probability measure $p$. For a function $\phi \in \calC(\calY)$, its convex conjugate is $\ol{\phi}(x) \eqdef \max_{y\in\calY} \{ \langle x, y\rangle - \phi(y)\}$. Note that convex conjugate is always a convex function.


\vspace{-2mm}\section{Background and object of study}
\label{background}

In this section we provide key concepts of OT theory that are used in our paper, a comprehensive introduction could be found in \cite{santambrogio, villani}. Throughout the paper, $p \in \Pac(\calX)$ and $q \in \Pac(\calY)$ are absolute continuous source and target distributions. 

\textbf{Monge's OT problem.} Given $p, q$ and a continuous cost function $c : \calX \times \calY \rightarrow \bbR$, Monge's primal OT prescribes to find a measurable transport map $T : \mathcal{X} \rightarrow \mathcal{Y}$ which minimizes OT cost:
\vspace{-2mm}
\begin{equation}\label{primal}
  \mathrm{Cost}(p, q) \eqdef \inf_{T_{\#}p=q} \int_{\mathcal X} c(x,T(x))p(x)dx.\vspace{-2mm}
\end{equation}
%
In other words, we want to find a map $T$ that transports probability mass from $p$ to $q$ in the cheapest way with respect to the given cost function $c(x, y)$, see Fig. \ref{fig:ot-map-def}. I.e., the average transport expenses when moving $p$ to $q$ should be minimal. 
\begin{figure}[!h]
\centering
\includegraphics[width=0.65\linewidth]{pics/OT_map_def.png}
\caption{\centering Monge's formulation of optimal transport.}
\label{fig:ot-map-def}\vspace{-3mm}
\end{figure}\vspace{-4mm}

\textbf{Wasserstein-2 distance.} A popular example of an OT cost is the (squared) Wasserstein-2 distance ($\mathbb W^2_2$), for which the cost function is quadratic, i.e., $c(x,y)=\frac{1}{2}\|x-y\|^2$. In this case, the corresponding OT objective \eqref{primal} is known to permit the unique minimizer $T^*$.

\ul{\textit{Remark}.} Note that:
%
\begin{align}\label{w2decomp}
\begin{aligned}
    \int_{\mathcal X} \frac{1}{2}\|x-T(x)\|^2p(x)dx = \int_{\mathcal X} \frac{1}{2}\|x\|^2p(x)dx &\,\\
    \hspace*{3mm} \!+\! \int_{\mathcal X} \!\!-\langle x, T(x) \rangle p(x)dx +\! \int_{\mathcal X} \!\frac{1}{2}\|T(x)\|^2p(x)dx&.
\end{aligned}
\end{align}
%
Since $T$ maps distribution $p$ to $q$, then $\int_{\mathcal X} \frac{1}{2}\|T(x)\|^2p(x)dx = \int_{\mathcal Y} \frac{1}{2}\|y\|^2q(y)dy$. Therefore, the first and the third terms in \eqref{w2decomp} do not depend on $T$ and minimizations over the quadratic cost and scalar product cost $c(x,y) = -\langle x, y \rangle$ in \eqref{primal} are equivalent, i.e., result in the same OT map $T^*$.

\textbf{Semi-dual OT problem.} Primal OT problem \eqref{primal} has a tight dual counterpart\footnote{Strictly speaking, \eqref{dual} appears as the dual for \textit{Kantorovich's} relaxation of \eqref{primal}, \citep[\S 1.1]{santambrogio}. However, \eqref{primal} = \eqref{dual} still holds, see \citep[Theorem 1.33]{santambrogio}} \citep[Theorem 1.39]{santambrogio}:
%
\begin{equation}\label{dual}
    \mathrm{Cost}(p, q) \!=\!\! \max_{f\in\mathcal C(\mathcal Y)}\!  \int_{\mathcal{X}}\! f^c(x)p(x)dx +\!\! \int_{\mathcal{Y}}\! f(y)q(y)dy,
\end{equation}
%
where $f^c \in \calC(\calX)$ is called $c$-transform and defined as ${f^c(x) \eqdef \min_{y\in \mathcal Y} \{c(x,y)-f(y)\}}$. Potential $f^*$ which delivers maximum to \eqref{dual} is called \textit{Kantorovich} potential. It always could be chosen to be $c$-concave, i.e., $f^* = g^c$ for some $g \in \calC(\calY)$, see \citep[Remark 1.13]{santambrogio}.

\textbf{Semi-dual OT for quadratic cost .} For the quadratic cost, according to \citep[proposition 1.21]{santambrogio}, optimal potential and its $c$-transform have the form $f^*(y) = \frac{1}{2}\|y\|^2-\phi^*(y)$ and $(f^*)^c(x) \!=\! \frac{1}{2}\|x\|^2-\overline {\phi^*}(x)$, where $\phi^*$ is a continuous convex function and $\overline{\phi^*}$ is its convex conjugate. 
Thus, we can rewrite the problem \eqref{dual} by substituting $f(y) = \frac{1}{2}\|y\|^2-\phi(y)$: 
%
\begin{align*}\label{eq:dual_phi}
\begin{aligned}
    \max_{f\in\mathcal C(\mathcal Y)}  \int_{\mathcal{X}} f^c(x)p(x)dx + \int_{\mathcal{Y}} f(y)q(y)dy &= \\
    \int_{\calX} \frac{1}{2} \Vert x \Vert^2 p(x) dx + \int_{\calY} \frac{1}{2} \Vert y \Vert^2 q(y) dy &\,\\
    - \min_{\phi \in \calC(\calY)}\Big\{ \int_{\calX} \overline{\phi}(x) p(x) dx + \int_{\calY} \phi(y) q(y) dy \Big\} &\, 
\end{aligned}
\end{align*}
%
Removing terms that do not depend on $\phi$, we get an equivalent formulation of \eqref{dual}, which has a convex solution ${\phi^*(y) = \frac{1}{2}\|y\|^2-f^*(y)}$:
%
\begin{equation}\label{dualconvex}
    \L(\phi)\eqdef\min_{\phi\in\mathcal C(\mathcal Y)}  \int_{\mathcal{X}} \overline {\phi}(x)p(x)dx + \int_{\mathcal{Y}} \phi(y)q(y)dy.
\end{equation}
%
Since the optimal potential $\phi^*$ which solves \eqref{dualconvex} is convex, we can optimize \eqref{dualconvex} with respect to the set of the continuous convex potentials $\phi \in \text{Cvx}(\calY) \subset \calC(\calY)$:
%
\begin{equation}\label{dualconvexcvx}
    \L(\phi)=\min_{\phi\in\text{Cvx}(\mathcal Y)}  \int_{\mathcal{X}} \overline {\phi}(x)p(x)dx + \int_{\mathcal{Y}} \phi(y)q(y)dy.
\end{equation}
%
Furthermore, we can recover the OT map $T^*$ which solves \eqref{primal} for quadratic (scalar product) cost from optimal dual potential 
$T^*(x) \!=\! \nabla \overline{\phi^*}(x)$ \citep[Thm. 1.17]{santambrogio}.

\textbf{Continuous OT problem.} Analytical solution for problem \eqref{dual} is, in general, not known. In real-world scenarios, the measures $p, q$ are typically not available explicitly but only through their empirical samples $X=\{x_1, \dots, x_N \} \sim p$, $Y=\{y_1, \dots, y_M\} \sim q$. To approximate the desired solution for primal (dual) OT, two setups are possible. In the first setup called \textbf{discrete} \citep{peyre2019computational}, one aims to establish optimal matching (point-to-point correspondence) between the empirical distributions $\hat p=\frac{1}{N}\sum_{i=1}^N\delta_{x_i}$, $\hat q=\frac{1}{M}\sum_{i=1}^M\delta_{y_i}$. In turn, our study focuses on the second \textbf{continuous} setup. Under this setup, the aim is to recover some parametric approximation $\widehat{T}$ of OT map $T^*$ based on available training samples from the reference distributions $p, q$. Compared to discrete OT, continuous OT allows out-of-the-box estimation of the recovered map $\widehat{T}(x^{\text{new}})$ for new (unseen) samples $x^{\text{new}} \sim p$, see Fig. \ref{fig:ot-cont-setup}. It may be tricky for discrete OT.
\begin{figure}[!h]
\centering
\includegraphics[width=0.85\linewidth]{pics/continuous_OT_brown.png}
\caption{\centering Continuous setup of OT problem.}
\label{fig:ot-cont-setup}\vspace{-2mm}
\end{figure}

\textbf{Continuous OT solvers for quadratic cost.} 
In recent years, numerous algorithms have been developed to solve the continuous optimal transport problem for the quadratic cost. The primal problem is challenging to solve due to the difficulty in satisfying the constraint \( T_\# p = q \). For this reason, semi-dual problem \eqref{dualconvex} or \eqref{dualconvexcvx} is solved instead and optimal transport map $T$ is recovered from the optimal dual potential. 
One popular strategy \cite{taghvaei} is to consider \eqref{dualconvexcvx} and parameterize learned $\phi$ as \textit{input convex neural network} (ICNN) \cite{amos2017input}. ICNNs are neural networks which place certain restrictions on their weights and activations to ensure the convexity w.r.t. the input. Given proper parametrization of dual potential $\phi_{\theta}, \theta \in \Theta$, one can directly minimize:
%
\begin{equation*}
    \L(\phi_{\theta})=\int_{\mathcal{X}} \overline {\phi_{\theta}}(x)p(x)dx + \int_{\mathcal{Y}} \phi_{\theta}(y)q(y)dy.
\end{equation*}
%
Since only empirical samples are known, the integrals are replaced by (Monte-Carlo) sum 
%
\begin{equation*}
    \hat \L(\phi_{\theta})=\frac{1}{N}\sum_{n=1}^N \overline {\phi_{\theta}}(x_n) + \frac{1}{M}\sum_{m=1}^M \phi_{\theta}(y_m)
\end{equation*}
%
or its minibatch version. 
Such methods, however, require additional optimization steps to compute $\overline{\phi_{\theta}}$, which may be undesirable from the computational efficiency perspectives.
To overcome the issue, researchers have explored ways to simplify the task. Several studies propose methods that approximate the conjugate potential using another neural network \cite{makkuva,w2gn, amos2023on}. Such methods, however, may suffer from \textit{gradient deviation} issue \citep[\S 2]{korotin2021neural} and fail to recover OT mappings sufficiently well. 

\textbf{Continuous min-max OT solvers for quadratic cost.} Recently, minimax semi-dual approaches have been actively explored, focusing on learning both the dual potential and the primal transport map. This formulation can be derived by applying the interchange theorem \citep[Theorem 3A]{rockafellar}:  
%
\begin{align}
    \int_{\calX} \overline{\phi}(x) p(x) dx &= 
    \int_{\mathcal{X}} \max_{y\in\mathcal Y}\{ \langle x, y \rangle - \phi(y)\}p(x)dx  \nonumber \\ &\hspace*{-8mm}= \max_{T} \int_{\mathcal{X}} [\langle x, T(x) \rangle - \phi(T(x))]p(x)dx \label{eq:rock-interchange},
\end{align}
%
the outer $\max_{T}$ is taken w.r.t. measurable maps ${T: \calX \rightarrow \calY}$. 
\ul{\textit{Remark}.} The existence of a map which maximizes the above problem follows from the measurable selection theory. For $\phi \in \calC(\calY)$, the sets $\argmax_{y} \{\langle x, y \rangle - \phi(y)\}$, $x \in \calX$ are nonempty and closed. From \citep[Thm. 8.1.3]{aubin2009set}, it follows that there exist a measurable map $T$ with values in such sets. This map delivers maximum to \eqref{eq:rock-interchange}.

In light of \eqref{eq:rock-interchange}, problem \eqref{dualconvex} is thus reformulated as a minimax optimization problem:
%
\begin{align}
    \text{\eqref{dualconvex}} &= \min_{\phi}\max_{T} \L(\phi, T) ; \label{minimax} \\
    \L(\phi, T) &\eqdef \!\!\int_{\mathcal X}\!\![\langle x,T(x) \rangle \!-\! \phi(T(x))]p(x)dx \!+ \!\!\int_{\mathcal Y}\!\! \phi(y)q(y)dy. \nonumber
\end{align}
Under certain assumptions one may guarantee that if the values $\L(\phi, T)$ and $\L(\phi^*,
T^*)$ are close, then $T$ is close to OT map $T^*$. And it is our work which establishes \textbf{practical convergence guarantees} for \eqref{minimax} under continuous OT setup with $\phi, T$ given by neural networks, see \S \ref{seq:results}.

Importantly, the use of min-max semi-dual OT losses like \eqref{minimax} is an emerging direction in generative modeling  \cite{korotin2021neural, rout, gazdieva2022optimal, knot, not, fan2023neural, domain, choi, choi2024analyzing, enot, gnot, genconddist, carrasco2024uncovering, barycenters}, making the analysis of \eqref{minimax} highly beneficial and desirable to the community. In particular, some of the min-max OT works, e.g., \cite{korotin2021neural, rout}, explore quadratic (scalar product) cost functions, i.e., deal exactly with  \eqref{minimax}. The others consider more general cost functions \cite{fan2023neural} and OT formulations \cite{gnot}. Extending the results of our study for such setups is a fruitful direction of future research.


\textbf{Object of study.} In practice, $T$ and $\phi$ are parametrized as neural networks $T_{\omega},\, \omega \in \Omega$ and $\phi_{\theta},\, \theta \in \Theta$. Besides, following our continuous OT setup, the (unknown) reference distributions $p, q$ are replaced by their empirical counterparts $\widehat{p}, \widehat{q}$. This yields the optimization problem typically solved in practice:
\begin{align}
    &\min_{\theta\in\Theta}\max_{\omega\in\Omega} \hat \L(\phi_{\theta}, T_{\omega}) ;\label{minimax_emp}\\
    \hat \L(\phi_{\theta},\!T_{\omega}\!) \!\eqdef& \sum_{n=1}^N \!\frac{\langle x_n,\!T_{\omega}(x_n)\!\rangle \!-\! \phi_{\theta}(T_{\omega}(x_n)\!)}{N}\! + \!\!\sum_{m=1}^M\!\! \frac{\phi_{\theta}(y_m)}{M}.\nonumber
\end{align}
ML practitioners and researchers naturally wonder how different are the practical map $\widehat{T}_{\omega^*}$ that solves \eqref{minimax_emp} and OT map $T^*$. Formally speaking, the problem is to estimate the \textit{generalization error}
%
\begin{align}
    \EXY \norm{ T^* - \hat{T}_{\omega^*}}_{L^2(\rho)}^2,\label{eq-obj-of-study}
\end{align}
%
where the expectation is taken w.r.t. empirical training samples $X \sim p, Y\sim q$. Remarkably, the presence of the error \eqref{eq-obj-of-study} comes from two sources: a) reliance on the restricted families of optimized potentials and maps (\textit{approximation error}); b) usage of empirical measures $\widehat{p}, \widehat{q}$ instead of original ones (\textit{estimation error}). 
Establishing the particular bounds for \eqref{eq-obj-of-study} is the primal focus of our research, see \S \ref{seq:results}.

\section{Related Works}\label{sec:works}

In this section, we first discuss the existing works which conduct theoretical analysis of semi-dual OT losses (\S \ref{subsec:works-OT}). Since minimax objective \eqref{minimax} resembles adversarial training, we additionally discuss some of the literature dealing with theoretical aspects of Generative Adversarial Neural Networks (GAN) losses and how they relate to ours (\S \ref{subsec:works-gans}). 

\subsection{Theoretical analysis of continuous semi-dual OT solvers}\label{subsec:works-OT}

The current progress in theoretical analysis of semi-dual OT could be subdivided into two branches.

\textbf{Non-minimax semi-dual OT.} The first branch of works \cite{hutter2021minimax, gunsilius2022convergence} analyze the non-minimax losses \eqref{dualconvex} and \eqref{dualconvexcvx} and develop error bounds \eqref{eq-obj-of-study} for pushforward maps $T$ given by the gradient of dual potentials, i.e., $\EXY \Vert \nabla \overline{\phi}^* - \nabla \widehat{\overline{\phi}}_{\omega} \Vert^2_{L^2(p)}$. To achieve particular statistical rates, the authors place certain restrictions on the considered problem setup and deal with specific classes of functions (maps), e.g., wavelet expansions or (kernel-smoothed) plug-in estimators. Recent studies in this direction \cite{divol2022optimal, ding2024statistical} extend the analysis of error rates for the class of potentials $\phi$ given by neural networks, e.g., input convex neural networks. Meanwhile, none of the mentioned works treats the learned map $T$ separately from dual potential $\phi$. Importantly, the analysis of minimax objective \eqref{minimax} is considerably more challenging than \eqref{dualconvex} due to the additional ``degree of freedom'' given by the optimized map $T$. Furthermore, saddle-point problems such as \eqref{minimax} are known to be more tricky for theoretical investigation than usual minimization. At the same time, the practical demand stemming from the recent proliferation of minimax-based OT solvers, see \S \ref{background}, makes such an investigation highly desirable. All of this necessitates separate study of minimax OT solvers; the adaptation of existing non-minimax results is questionable, if that is even possible.

\textbf{Minimax semi-dual OT.} The estimation of \eqref{eq-obj-of-study} in case of minimax OT \eqref{minimax_emp} is a much less explored task. In fact, there are no studies at all that provide statistical bounds for recovered minimax OT map $\widehat{T}_\omega$. The existing works \citep{makkuva, rout} only conduct an analysis of $\Vert T^* - \widehat{T}_{\omega} \Vert^2_{L_2(p)}$ in the sense of \textit{duality gaps}. That is, the error between the true OT map and recovered map is upper-bounded by values of functional $L(\phi, T)$. The duality gaps analysis helps to validate minimax methodology; it is common for more broad class of (non-quadratic) minimax OT solvers \cite{fan2023neural, gnot, barycenters}. However, we again emphasize that it does not reveal particular statistical rates and guarantees for \eqref{eq-obj-of-study}. In \cite{gonzalez2022gan}, the authors establish promising results for the error of recovered approximate OT map. However, their problem formulation of min-max OT differs from ours, eq. \eqref{minimax}, as they aim to treat OT as the limit of regularized GAN objectives. 

\textbf{Theoretical analysis of other OT formulations.} For the completeness of exposition, we also mention several studies that develop statistical learning analysis of recovered OT map (plan) error for non semi-dual OT formulations. The works \cite{genevay2019sample, rigollet2022sample, gonzalez2023weak, mokrov2024energyguided, goldfeld2024limit, korotin2024light} deal with Entropy-regularized OT; \cite{vacher2022stability, vacher2023semi, gazdieva2024light} investigate unbalanced OT versions.
Although these works are interesting and insightful, their object of study is different from ours, making them less relevant.

\subsection{Theoretical analysis of adversarial Generative Models}\label{subsec:works-gans}
The objective of the semi-dual minimax continuous OT solvers \eqref{minimax} resembles that of GANs \citep{goodfellow2014generative}. This fact motivates us to review some of the existing theoretical results on GANs below. Still, while there are many papers on GANs \citep{pan2019recent} and a relatively large number of works studying the theoretical aspects of their objectives, they are not relevant to us for two reasons. First, OT solvers and GANs pursue different goals. The main goal of OT solvers is to approximate true OT maps $T^*$, i.e., to yield specific generators which satisfy the optimality condition; accordingly, our theoretical work focuses on the error of this approximation. Meanwhile, GANs objectives usually do not have a unique optimal generator $G^*$; thus, existing theoretical results mostly investigate the error of approximating the ground-truth distribution $q$ by the generated $p_{gen}$. Second, OT solvers and GANs have an evident difference corresponding to the order of 
optimization over generator $G$ (map $T$) and discriminator $D$ (potential $\phi$) in their objectives. In particular, for GANs, the optimization over the generator $G$ is done in the outer problem of their objective, while for OT solvers, the optimization over OT maps is conducted in the inner problem, i.e., the OT map $T^*$ corresponds to the solution of the inner problem. We emphasize the importance of this difference $-$ for OT solvers, OT map $T^*$ corresponds to the solution of the inner problem, which makes the task of theoretical investigation of the error of $T^*$ approximation even more difficult.
Below we discuss two groups of theoretical results on GANs which are slightly relevant to us.


\textbf{Estimation error of distance between generated and true distributions}. The early work of \citep{arora2017generalization} introduces a notion of an $\mathcal{F}$-\textit{distance}
between the pair of measures w.r.t. a given class of discriminators $\mathcal{F}$:
$$
d_{\mathcal{F}}(p,q)\!=\! \sup_{D\in \calF} \bbE_{x\sim p} \psi(D(x)) + \bbE_{y\sim q} \psi(1-D(y))-2\psi(\frac{1}{2}).
$$
Here $\psi:[0,1]\rightarrow\bbR$ is any monotone concave function.
For certain classes of functions, these distances define an inner optimization problem for well-known GANs, e.g., Wasserstein, Vanilla, MMD, Sobolev, etc. Class $\mathcal{F}_{nn}$ corresponding to neural networks with a bound on a number of parameters yields a so-called \textit{neural net (NN) distance}. 
The paper \citep{arora2017generalization} establishes an upper bound on the estimation error of this distance, i.e., difference of NN distances $d_{\mathcal{F}_{nn}}(q, p_{gen})$ for empirical and continuous distributions. Subsequent work \citep{ji2018minimax} extends this analysis by providing a tighter upper bound and lower bound for this difference. In some sense, these works analyze the error in GANs inner optimization problem coming from the usage of empirical measures $\widehat{p}$, $\widehat{q}$ instead of the real ones.
Still, these works ignore the errors rooted in outer optimization problem of GANs. 

\textbf{Estimation error of GANs.} Other class of papers takes into account errors in GANs' \textit{outer} optimization problem. They commonly consider the solutions of GANs' empirical optimization problem:
$\widehat{w}^* = \arg\min_{w\in \Omega} d_{\mathcal{F}_{nn}}(\widehat{q},\widehat{p}_{gen(w)})
$ where $\widehat{p}_{gen(w)}$ denotes the distribution generated from empirical samples of distribution $\widehat{p}$ by a generator $G_{w}$ (with parameters $w$ from the class $\Omega$). Then the papers derive upper bounds for the estimation error defined as
$$
d_{\mathcal{F}}(q, p_{gen(\widehat{w}^*)}) - \inf_{w\in \Omega} d_{\mathcal{F}}(q, p_{gen(w)})
$$
for different variants of function classes $\calF$.
For example, \citep{ji2021understanding} derives an upper and minimax lower bound for this error and $\calF_{nn}$ class; \citep{zhang2018discrimination} considers the same class but a different type of error using $w^*=\arg\min_{w\in \Omega} d_{\mathcal{F}}(\widehat{q},p_{gen(w)})$ where $p_{gen(w)}$ is the distribution generated by $G_w$ from $p$; \citep{liang2017well} and \citep{singh2018nonparametric} study the same type of error but consider the discriminator class in Sobolev space; \citep{uppal2019nonparametric} extends these result by modeling the discriminator and generator as Besov function classes. In general, these bounds are constructed based on Rademacher complexities of function spaces and are utilized to construct the bounds on the distance between $p_{gen}$ and $q$.

For completeness, we mention several other works which investigate theoretical properties of specific GANs, e.g., bidirectional \citep{liu2021non}, Wasserstein \citep{biau2021some} and CycleGAN \citep{chakrabarty2022translation}, \citep{sun2024theoretical}, \citep{moriakov2020kernel}.

\section{Results}\label{seq:results}

In real-world use cases, OT practitioners are given (i.i.d.) training samples $X \!\sim\! p$ and $Y\!\sim\! q$ and optimize empirical min-max objective \eqref{minimax_emp} with respect to restricted classes of functions $\phi \in \cF$, $T \in \cT$, e.g., neural networks. Below we denote the solutions of the problem which we have in practice.
%
\vspace*{-2mm}\begin{align*}
& \hat {\phi}^{R}=\underset{{\phi} \in \mathcal F}{\operatorname{argmin}} \max_{T \in \mathcal T} \hat \L({\phi}, T); \\
& \hat T^{R}=\underset{T \in \mathcal T}{\operatorname{argmax}} \hat{\L}\left(\hat {\phi}^{R}, T\right).
\end{align*}
%
Note that in these equations we implicitly assume the \textit{existence} of optimizers $\hphiR \in \cF, \hTR \in \cT$. While in general this may not always hold true, some natural practical choices of $\cF$ and $\cT$, e.g., neural network architectures $T_{w}$, $\phi_\theta$ with bounded set of parameters $\Omega, \Theta$ guarantee the existence. 

Our goal is to estimate the \textit{generalization error}, i.e., the (average) error between OT map $T^*$ and empirical map $\hTR$:
\begin{align}
{\EXY \big\Vert \hTR-T^*\big\Vert_{L^2(p)}}. \label{eq-error}
\end{align}
We subdivide the problem into three steps. 

\ul{In the first step}, we upper-bound the error using differences in the functional $\L(\phi, T)$ values (\S \ref{subsec:err-decomp}). The obtained upper bound decomposes into several terms: \textit{estimation} and \textit{approximation} errors that occur in both the \textit{inner} and \textit{outer} optimization problems within our min-max objective \eqref{minimax}. 

\ul{In the second step}, we estimate each term individually using suitable techniques from statistical learning theory (\S \ref{subsec:approx-estim}).

\ul{Finally}, we bring it all together and formulate our main theoretical result (\S \ref{subsec:main-res}).

\subsection{Error decomposition}\label{subsec:err-decomp}

Our starting point is the introduction of four components which will appear when upper-bounding \eqref{eq-error}. % in our error decomposition. 
The primary quantity which we analyze in this section is the \textit{error} between values of functional $\L$, i.e.:
\begin{align}
    \L(\phi, T) - \L(\phi', T'). \label{L-error}
\end{align}
Depending on the context, the plug-in arguments $\phi, \phi', T, T'$ of the expression above may be ``optimal'' in some sense and connected to each other. In particular, the potentials $\phi, \phi'$ may solve the outer ($\min_{\phi}$) optimization for the original \eqref{minimax} or empirical \eqref{minimax_emp} objective in certain class of functions, i.e., $\cC(\cY)$ or $\cF$. In turn, the maps $T, T'$ can be optimal in the inner ($\max_T$) optimization of the min-max objectives for certain potentials $\phi$ and also belong to certain classes of functions. 
These choices give rise to several options, which are discussed below.

Typically, quantities (errors, bounds, etc.) that appear in statistical learning theory allow decomposition into approximation and estimation components. Our analyzed quantity \eqref{L-error} could also be seen in this framework.

The \textit{approximation error} arises because we are not optimizing over the full space of continuous functions $\phi \in \cC(\cY)$ and measurable maps $T$, but over the restricted classes $\calF$ and $\calT$ of neural networks.
Since neural networks have a limited capacity, they may not be able to represent the true minimizer $(\phi^*,T^*)$ of $\L(\phi,T)$ exactly. 
%While dealing with this error, we  
We consider two components of this error, focusing respectively on the outer and inner optimization in \eqref{minimax}.

The outer component assumes that the inner maximization problem ($\max_T$) is solved exactly. We want to identify to which extend the restricted outer minimization ($\min_{\phi \in \cF}$) affects the min-max objective:
\vspace*{-1mm}\begin{equation}
    \mathcal{E}_{Out}^{A}(\mathcal F) \eqdef \big\vert \min_{\phi\in\calF}\max_T \L(\phi,T) - \min_{\phi}\max_T \L(\phi,T) \big\vert. \nonumber\vspace*{-2mm}
\end{equation}
%
Note that $\max_T \L(\phi,T) = \L(\phi)$. Then the outer approximation error could be reformulated as follows:
%
\begin{equation}\label{def:e_a_out}
    \mathcal{E}_{Out}^{A}(\mathcal F) =  \big\vert \min_{\phi\in\mathcal F}\L({\phi}) - \min_{\phi}\L({\phi})\big\vert .
\end{equation}

The inner approximation error, in turn, measures the looseness of inner maximization in the restricted class $\calT$, i.e., the gap $\vert\max_T\L(\phi, T) - \max_{T\in\calT}\L(\phi,T)\vert$. To have $\phi$-independent characteristic, we further bound the gap uniformly w.r.t. $\phi\in\calF$. This yields  
\begin{equation}\label{def:e_a_in}
    \mathcal{E}_{In}^{A}(\mathcal F, \mathcal T) = \max_{{\phi}\in\mathcal F}\left\vert\max_{T} \L({\phi}, T) \!-\! \max_{T\in\mathcal T} \L({\phi}, T)\right\vert.
\end{equation}
In practice, we do not have direct access to $\L(\phi,T)$ but instead minimize the empirical functional $\hat\L(\phi,T)$ based on finite training data. Thus, the optimal potential and map which deliver optimal value to $\hat{\L}$, i.e., $\hphiR$ and $\hTR$, may be different from those which deliver optimal value to $\L$, even within classes $\cF$, $\cT$.
This introduces \textit{estimation error}, which quantitatively measures the behaviour of functional $\L$ which takes ``empirically-optimal'' functions $\hphiR, \hTR$ as the input. By design, the estimation error is evaluated within classes $\cF$, $\cT$. Also, since $\hphiR, \hTR$ are (indirectly) based on the empirical samples $\{x_1, \dots x_N\} = X \sim p$ and $\{y_1, \dots, y_M\} = Y \sim q$, the estimation error is averaged w.r.t. them and depend on the sample sizes $N$, $M$.

The inner estimation error is defined as:
\begin{equation}\label{def:e_e_in}
    \hspace*{-3mm}\mathcal{E}_{In}^{E}(\mathcal F, \mathcal T, N, M)\! \eqdef \!\!
    \EXY\!
    \left\vert\max_{T\in\mathcal T} \L(\hat {\phi}^R, T)\!-\!\! \L(\hat {\phi}^R, \hat T^R)\right\vert.\!\!
\end{equation}
%
For the fixed potential $\hphiR$ it shows how different are the optimal value of the inner ($\max_{T\in\cT}$) optimization and the ``empirically-induced'' one $\L(\hphiR, \hTR)$.

Similarly, the outer estimation error deals with the outer minimization ($\min_{\phi\in\cF}$) while assuming exact solution of inner maximization in class $\cT$:
%
\begin{eqnarray}\label{def:e_e_out}
    \mathcal{E}_{Out}^{E}(\mathcal F\!, \cT\!, N\!, M) \!=  \!\!\EXY\!
    \left\vert \max_{T\in\calT}\L({\hphiR}\!, T) \!-\! \min_{\phi\in\calF}\max_{T\in\calT}\L({\phi}, T) \right\vert.
\end{eqnarray}
Now we are ready to formulate an important theorem \ref{thm:decomp} about the error decomposition.
\begin{theorem}[\normalfont{Error decomposition [\hyperref[proof:decomp]{proof ref.}]}]\label{thm:decomp}

Let $\calF$ be a class of $\beta$-strongly convex functions, then
\begin{align}\label{eq:main}
\begin{aligned}
    &\bbE\left\|\hat T^{R}-T^*\right\|^2_{L_2(p)} \leq \frac{4}{\beta}\left(\mathcal{E}_{In}^{E}(\mathcal F, \mathcal T, N, M) + \right.\\
    & \left. + 3\mathcal{E}_{In}^{A}(\mathcal F, \mathcal T) + \mathcal{E}_{Out}^{E}(\mathcal F, \mathcal T, N, M) + \mathcal{E}_{Out}^{A}(\mathcal F) \right).
\end{aligned}
\end{align}\vspace*{-3mm}
%
\end{theorem}

The theorem shows that the (averaged) $L^2$ gap between the empirical and true OT maps \eqref{eq-error} could be upper-bounded by our introduced errors. Importantly, the latter compare the values of functional $\L$ and thus easier for the analysis (conducted in the next subsection \S \ref{subsec:approx-estim}) then the former. 

\subsection{Bounds on the Approximation and Estimation Errors}\label{subsec:approx-estim}

In this section, we establish the bounds on the estimation and approximation errors of the minimax OT solvers defined in \S \ref{subsec:err-decomp}. In the theorem below, we establish the bounds on the total estimation error, i.e., the sum of inner \eqref{def:e_e_in} and outer errors \eqref{def:e_e_out}.

\begin{theorem}[\normalfont{Rademacher Bound on the Estimation Error [\hyperref[proof:estim_error]{proof ref.}]}]\label{thm:estim_error}

    Let $\mathcal{E}^{E} = \mathcal{E}_{In}^{E} + \mathcal{E}_{Out}^{E}$. Then
%
    \begin{align}\label{eq:estim}
    \begin{aligned}
        \mathcal{E}^{E} \leq 8\mathcal R_{p, N}(\mathcal H)+8\mathcal R_{q, M}(\mathcal F),
    \end{aligned}\vspace{-1mm}
    \end{align}
%
where $\mathcal H(\calF,\calT) \eqdef \{h: h(x) = \langle x,T(x) \rangle - \phi(T(x)),T\in\calT,\phi\in\calF\}$ and $\mathcal R_{p, N}(\mathcal H)$ is the Rademacher complexity of the function class 
$\mathcal H$ with respect to probability density $p$ for sample size $N$.
\end{theorem}

Now we proceed to the approximation errors \eqref{def:e_a_in} and \eqref{def:e_a_out}. In what follows, we prove that under proper choice of classes $\mathcal{F}, \mathcal T$ the errors could be done arbitrarily small.

First we look at with the inner approximation error \eqref{def:e_a_in}. Note that it depends on both classes $\mathcal{F}$ and $ \mathcal T$.
The following theorem states that by imposing certain restrictions on class $\calF$ and by choosing an appropriate class of neural networks $\calT$, we can control the inner approximation error. 

\begin{theorem}[\normalfont{Inner approximation error [\hyperref[proof:approx_inner]{proof ref.}]}]\label{thm:approx_inner}

Let $\mathcal{F}$ be a class of Lipschitz $\beta$-strongly convex functions, compact with respect to the Lipschitz norm. Then for all $\varepsilon > 0$ there exists such class of neural networks $\mathcal{T} = \calT( \varepsilon, \cF)$, that
\vspace{-1mm}\begin{equation}
\mathcal{E}_{In}^{A}(\mathcal F, \mathcal T) < \varepsilon. \label{eq:err-in-a-est}
\end{equation}
\end{theorem}

Our following Remark \ref{remark} elaborates on the functional class $\cF$ appearing in the theorem above.

\vspace{1mm}
\begin{remark}\label{remark}
    Let $\calF_{icnn}$ be some class of $D$-layer ICNN with ReLU activations, bounded width and bounded weights matrices ${\norm{W_d}_2\leq K,\ d=1,\dots,D}$, defined in proof of \citep[Theorem 1]{icnn_approx}. 
    Then $\calF = \{ \phi + \beta\frac{\|.\|^2_2}{2}, \phi\in\calF_{icnn} \}$ is class of $\beta$-strongly convex functions. It is expected to be compact with respect to the lipschitz norm.
\end{remark}

From the proof of Theorem \ref{thm:approx_inner} one can notice that the class of neural networks $\cT$ for which \eqref{eq:err-in-a-est} holds could be chosen to be finite. This naturally leads to another practically-important (cf. Corollary \ref{corollary-rademacher}) Remark \ref{remark-T}:
\vspace{1mm}
\begin{remark}\label{remark-T}
    In conditions of Theorem \ref{thm:approx_inner}, the class of maps $\cT$ could be chosen in such a way that it contains neural networks with (uniformly) bounded width and height; weights of bounded norms and ReLU activations.
\end{remark}

Now we move to the analysis of the outer approximation error \eqref{def:e_a_out}. Similar to the inner case, under an additional assumption that the optimal potential $\phi^*$ is $\beta$-strongly convex for some $\beta > 0$, we show that there exist Neural Network architecture for the dual potential $\phi$, which makes the error arbitrarily small.


\begin{theorem}[\normalfont{Outer Approximation Error [\hyperref[proof:approx_outer]{proof ref.}]}]\label{thm:approx_outer}

    Let the optimal dual potential $\phi^*$ which solves \eqref{dualconvex} be $\beta$-strongly convex. Then for any $\varepsilon > 0$ there exist a lipschitz $\beta$-strongly convex function $\phi_L^{\beta}$ such that $\mathcal{L}(\phi_L^{\beta}) -\min_{\phi} \mathcal{L}(\phi)
     < \varepsilon$. And $\phi_L^{\beta}$ has the form $\phi_L + \beta\frac{\|.\|^2}{2}$, where $\phi_L$ is an ICNN.
    
\end{theorem}

Theorem \eqref{thm:approx_outer} immediately yields the following

\begin{corollary}\label{corollary_f}
    There exists a class of Lipschitz $\beta$-strongly convex functions, compact with respect to the Lipschitz norm, such that
    \begin{equation}
        \mathcal{E}_{Out}^{A}(\mathcal F) \leq \varepsilon.
    \end{equation}
\end{corollary}


\subsection{Main Result}\label{subsec:main-res}

The main goal of our paper is the establishment of bounds on \textit{generalization error}, i.e., the difference in true OT map and its empirical approximation defined in \eqref{eq-error}. In the theorem below, we use the previously obtained bounds on the estimation (Theorem \ref{thm:estim_error}) and approximation errors (Theorems \ref{thm:approx_inner}, \ref{thm:approx_outer}) to derive a bound on the generalization error.

\begin{theorem}[Bound on the Generalization Error\normalfont{ [\hyperref[proof-main-thm]{proof ref.}]}]\label{thm:main}
    Let the optimal dual potential $\phi^*$ be $\beta$-strongly convex. Then for any $\varepsilon > 0$ there exist such classes $\mathcal F = \mathcal F(\varepsilon, \beta), \mathcal T = \mathcal T(\varepsilon, \cF)$ that
    \begin{align}\label{eq:main}
    \begin{aligned}
        \hspace*{-3mm}\EXY \norm{T^*\!\!-\!\hat T^{R}}^2_{L_2(p)}\!\! \leq \varepsilon \!+\! \frac{32}{\beta}\left(\mathcal R_{p,N}(\mathcal H)\!+\!\mathcal R_{q,M}(\mathcal F)\right).
    \end{aligned}
    \end{align}
    In particular, $\forall \phi \in \mathcal F$ it has the form $\phi_{\theta} + \beta\frac{\|.\|^2}{2}$, where $\phi_{\theta}$ is an ICNN and $\mathcal T$ is some class of neural networks.
\end{theorem}

Theorem \ref{thm:main} shows that a practitioner can make the generalization error \eqref{eq-error} arbitrarily small by choosing appropriate classes of functions $\calF$ and $\calH$. However, the particular choice of these functional classes and the convergence rates remain unclear. The next corollary shows for the particular choice of functional classes (neural network), the obtained bound can be specified by replacing the Rademacher complexities with their upper bounds depending only on the number of empirical samples. 

\begin{corollary}[\normalfont{Generalization Error for the Specific Classes of Neural Networks [\hyperref[proof:corollary_rademacher]{proof ref.}]}]\label{corollary-rademacher}
    Let $\calT$ be a class of neural networks with limited width and height, weights with bounded norms and Lipschitz activation functions. Let $\calF$ be a class of functions satisfying the assumptions of Theorem \ref{thm:main}. For any $\varepsilon > 0$ there exist class $\calT$ of neural networks with limited width and height, weights with bounded norms and ReLU activation functions, and class $\calF$ of functions satisfying the assumptions of Theorem \ref{thm:main} such that
    \begin{align}\label{eq:main}
    \begin{aligned}
        \EXY \norm{T^*-\hat T^{R}}^2_{L_2(p)} \leq \varepsilon + O(\frac{1}{\sqrt{N}}) + O(\frac{1}{\sqrt{M}}).
    \end{aligned}
    \end{align}
    
\end{corollary}

This corollary allows us to conclude that the generalization error can be made arbitrary small if one selects the appropriate classes of neural networks and sufficient number of empirical samples during their training.

\section{Discussion}

Our paper performs theoretical investigation of semi-dual min-max OT solvers, i.e., a popular and fruitful branch of generative models based on Optimal Transport. While these solvers show impressive results in various machine learning problems, theoretical investigation of their practical convergence guarantees is rather limited. 
We address this problem by presenting the first theoretical analysis of these \textit{minimax} solvers from a \textit{statistical learning} perspective. Most importantly, our paper provides \textit{learnability guarantees} for the solvers which justify their practical usability.
We believe that our research will advance the development of minimax adversarial OT solvers by mitigating potential concerns regarding their theoretical validity. The only \textit{limitation} of our study corresponds to the focus on a popular case of a quadratic OT cost function. Generalization of the established bounds for the general OT formulations represents a promising avenue for future work.

\textbf{Impact statement.} This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\onecolumn
\section{Proofs.}
\vspace{-1mm}
In this section, we provide the proofs of our theoretical results:
\vspace{-2mm}
\begin{itemize}
    \item[-] Theorem \ref{thm:decomp} (\S \ref{proof:decomp}) which decomposes the recovered map $\hat{T}^R$ error into several approximation and estimation subterms which depend on the values of the optimized functional $\L$.
    \item[-] Theorem \ref{thm:estim_error} (\S \ref{proof:estim_error}) which upper-bounds the \textit{estimation} error with Rademacher complexities of function classes $\calF$, $\calH$. 
    \item[-] Theorem \ref{thm:approx_inner} (\S \ref{proof:approx_inner}) and Theorem \ref{thm:approx_outer} (\S \ref{proof:approx_outer}) which demonstrate that the approximation error could be done arbitrary small under properly chosen classes of Neural Networks.  
    \item[-] Theorem \ref{thm:main} (\S \ref{proof:corollary_rademacher}) and Corollary \ref{corollary-rademacher} (\S \ref{proof:corollary_rademacher}) which establish upper bounds for the generalization error \eqref{eq-error} and show that it can be made arbitrarily small by choosing the appropriate classes of functions and sufficient number of samples.
\end{itemize}

Before starting the main proofs, we state the following auxiliary lemma

\begin{lemma}\label{lem-aux}
Let $A, B : \cG \rightarrow \bbR$ be some functionals on a functional space $\cG$. Then
\begin{align}
\big| \inf_{g\in\cG} A(g)-\inf_{g\in\cG} B(g) \big|  \leq \sup_{g\in\cG}|A(g)-B(g)|\nonumber; \\
\big| \sup_{g\in\cG} A(g)-\sup_{g\in\cG} B(g) \big|  \leq \sup_{g\in\cG}|A(g)-B(g)|\nonumber.
\end{align}
\end{lemma}
%
\begin{proof}

In what follows, we prove the first inequality in the statement of the lemma. The second inequality could be derived in a similar way.

Without loss of generality, assume that
\begin{align*}
\inf_gA(g) \geqslant \inf_gB(g) \Longrightarrow \big| \inf_{g\in\cG} A(g)-\inf_{g\in\cG} B(g) \big| = \inf_{g\in\cG} A(g)-\inf_{g\in\cG} B(g).
\end{align*}
Let us pick $\epsilon > 0$ and consider $g^*_{\epsilon} \in \cG$ such that $B(g^*_{\epsilon}) \leq \inf_{g} B(g) + \epsilon$. Note that $\inf_{g} A(g) \leq A(g^*_{\epsilon})$. Therefore:
\begin{align}
    \inf_{g\in\cG} A(g)-\inf_{g\in\cG} B(g) &\leq A(g^*_{\epsilon}) - B(g^*_{\epsilon}) + \epsilon \nonumber \\
    &\leq \big| A(g^*_{\epsilon}) - B(g^*_{\epsilon}) \big| + \epsilon. \nonumber \\
    &\leq \sup_{g\in\cG} \big| A(g) - B(g) \big| + \epsilon. \nonumber
\end{align}
Taking the limit $\epsilon \rightarrow 0$ in the last inequality finishes the proof.
\end{proof}

\subsection{Proof of theorem \ref{thm:decomp}}\label{proof:decomp}

First, we upper-bound the error in estimating transport map via duality gaps analysis. Our theorem below borrows the main ideas from \citep[Theorem 3.6]{makkuva}, but has its own specificity, since it deals with transport maps that are not necessarily given by gradients of convex functions.

\begin{theorem}\label{thm:decomp-aux}
    Under the condition of theorem \ref{thm:decomp} it holds:
    \begin{equation}
        \norm{\hat T^R-T^*}^2_{L^2(p)} \leq \frac{4}{\beta}\left( \calE_{1}(\hat{\phi}^R, \hat T^R) + \calE_{2}(\hat{\phi}^R) \right), \nonumber
    \end{equation}
    %
    where $\displaystyle \calE_{1}(\hat{\phi}^R, \hat T^R) \eqdef \max_T \L(\hat{\phi}^R, T) - \L(\hat{\phi}^R, \hat T^R)$ is the inner error, and $\displaystyle \calE_{2}(\hat{\phi}^R) \eqdef \max_T \L(\hat{\phi}^R, T) - \min_{\phi}\max_T \L(\phi, T)$ is the outer error.
\end{theorem}

\begin{proof}
    We introduce $T_{\hphiR} \eqdef \argmax_{T} \L(\hphiR, T)$. Then,
    \begin{align*}
    \begin{aligned} 
    \calE_{1}(\hat{\phi}^R, \hat T^R) & = \int_{\cX} \left[\langle x,T_{\hphiR}(x) \rangle - \hphiR(T_{\hphiR}(x)) \right] p(x) dx  -
    \int_{\calX}\left[\langle x,\hTR(x) \rangle - \hphiR(\hTR(x)) \right] p(x)dx .
    \end{aligned}
    \end{align*}

    Consider $\beta$-strongly concave $f_x(y) \eqdef \langle x,y \rangle - \hphiR(y)$. Then, 
    \begin{align}
        \calE_{1}(\hat{\phi}^R, \hat T^R) = \int_{\cX} \big[f_x(T_{\hphiR}(x)) - f_x(\hTR(x))\big] p(x) dx. \label{E1-fx}
    \end{align}
    
    Note that $T_{\hphiR}(x)$ is the maximizer of $f_x$. Thanks to $\beta$-strong concavity of $f_x$ it holds (for any $T$): 
    \begin{align}\label{eq:16}
    f_x(T_{\hphiR}(x)) - f_x(T(x)) \geq \frac{\beta}{2} \norm{T(x) - T_{\hphiR}(x)}_2^2 .
    \end{align}
    %
    Combining \eqref{E1-fx} and \eqref{eq:16}, we get:
    %
    \begin{align}
    \calE_{1}(\hat{\phi}^R, \hat T^R) \geq \frac{\beta}{2} \norm{\hTR-T_{\hphiR}}_{L^2(p)}^2. \label{E1-est}
    \end{align}
    %
    Now we move to $\calE_{2}(\hat{\phi}^R)$. We have the following:
    \begin{align}
        \cE_2(\hphiR) &= \L(\hphiR, T_{\hphiR}) - \L(\phi^*, T^*)  \nonumber \\ &=\int_{\calX}\langle x,T_{\hphiR}(x) \rangle p(x)dx \!-\!\int_{\cX} \hphiR(T_{\hphiR}(x)) p(x)dx + \int_{\cY} \hphiR(y) q(y)dy   \nonumber \\
        &\hspace*{4mm} - \int_{\calX}\langle x,T^*(x) \rangle p(x)dx \!+\underbrace{\!\int_{\cX} \phi^*(T^*(x)) p(x)dx - \int_{\cY} \phi^*(y) q(y)dy}_{= 0 \text{, since } T^*_{\#} p = q}   \nonumber \\
        &= \int_{\calX}\big[\langle x,T_{\hphiR}(x) \rangle \!-\!\hphiR(T_{\hphiR}(x))\big] p(x)dx + \int_{\cY} \hphiR(y) q(y)dy - \int_{\calX}\langle x,T^*(x) \rangle p(x)dx \label{E2-1} \\
        &= \int_{\calX}\big[\langle x,T_{\hphiR}(x) \rangle \!-\!\hphiR(T_{\hphiR}(x))\big] p(x)dx + \int_{\cX} \hphiR(T^*(x)) p(x)dx - \int_{\calX}\langle x,T^*(x) \rangle p(x)dx \label{E2-2} \\
        &= \int_{\calX}\underbrace{\big[\langle x,T_{\hphiR}(x) \rangle \!-\!\hphiR(T_{\hphiR}(x))\big]}_{=f_x(T_{\hphiR}(x))} p(x)dx - \int_{\cX} \underbrace{\big[\langle x,T^*(x) \rangle - \hphiR(T^*(x))\big]}_{=f_x(T^*(x))} p(x)dx, \nonumber
        % &\geq \frac{\beta}{2} \int_{\calX} \norm{T^*(x) - T_{\phi}(x)}_2^2p(x)dx.
    \end{align}
    where in transition from \eqref{E2-1}
 to \eqref{E2-2} we use  $T^*_{\#} p = q$ which yields $\int_{\cY}\hphiR(y) q(y) dy = \int_{\cX} \hphiR(T^*(x)) p(x) d x$.
 
 Now we apply property \eqref{eq:16} for $T$ given by true OT map $T^*$:
    %
    \begin{align}
    \cE_2(\hphiR) &= 
        \int_{\calX}\Big\{\!\!\left[\langle x,T_{\hphiR}(x) \rangle \!-\! \hphiR(T_{\hphiR}(x)) \right] \!-\!
    \left[\langle x, T^*(x) \rangle \!-\! \hphiR(T^*(x)) \right]\!\!\Big\}p(x)dx \nonumber \\ &= \int_{\cX} [f_x(T_{\hphiR}(x)) - f_x(T^*(x))] p(x) dx \nonumber \\
    &\geq  \frac{\beta}{2} \int_{\calX} \norm{T^*(x) - T_{\hphiR}(x)}_2^2p(x)dx. \label{E2-est}
    \end{align}
%
    Application of the triangle inequality for \eqref{E1-est} and \eqref{E2-est} finishes the proof:
    \begin{align}
        \norm{T^* - \hTR}_{L_2(p)}^2 \leq \frac{2}{\beta} \left( \sqrt{\calE_{1}(\hphiR, \hTR)} + \sqrt{\calE_{2}(\hphiR)} \right)^2 \leq \frac{4}{\beta}\left( {\calE_{1}(\hphiR, \hTR)} + {\calE_{2}(\hphiR)} \right). \nonumber
    \end{align}
    %
\end{proof}
To get the final result, it remains to decompose both errors to estimation and approximation parts. 

Now we proceed to the proof of the main theorem. To get the final result, it remains to decompose both $\cE_1$ and $\cE_2$ errors from the Theorem \ref{thm:decomp-aux} into estimation and approximation parts. 

\begin{proof}[Proof of Theorem \ref{thm:decomp}]

From Theorem \ref{thm:decomp-aux} we have:
%
\begin{align}
    \EXY \lpnorm{\hTR - T^*} \leq \frac{4}{\beta}\Big(\EXY \cE_1(\hphiR, \hTR) + \EXY \cE_2(\hphiR)\Big). \label{err-bound-E1-E2}
\end{align}
%
For the inner error $\cE_1$ we have:
%
\begin{align*}
    \calE_{1}(\hat{\phi}^R, \hat T^R) &=\max_T \L(\hat{\phi}^R, T) - \L(\hat{\phi}^R, \hat T^R) \\ 
    &\leq \left|\max_{T} \L(\hat{\phi}^R, T) - \max_{T\in \calT}\L(\hat{\phi}^R, T)\right| + \left|\max_{T\in \calT} \L(\hat{\phi}^R, T) - \L(\hat{\phi}^R, \hat T^R)\right| \\
    &\leq \max_{\phi\in\calF}\left|\max_{T} \L(\phi, T) - \max_{T\in \calT}\L(\phi, T)\right| + \left|\max_{T\in \calT} \L(\hat{\phi}^R, T) - \L(\hat{\phi}^R, \hat T^R)\right|.
\end{align*}
%
Taking the expectation of the both sides in the inequality above yields:
%
\begin{align}
    \EXY \cE_1(\hphiR, \hTR) \leq \cE_{In}^{A}(\cF, \cT) + \cE_{In}^{E}(\cF, \cT, N, M). \label{E1-decomp}
\end{align}
%
% Minimization duality gap is decomposed into four terms:
\vspace{-2mm}
For the outer error $\cE_2$ we have:
%
\begin{align}
    \calE_{2}(\hat{\phi}^R) &=\max_T \L(\hat{\phi}^R, T) - \min_{\phi}\max_T \L(\phi, T)  \nonumber \\
    &=\dotuline{\max_T \L(\hat{\phi}^R, T) - \max_{T\in\calT} \L(\hat{\phi}^R, T)} + \uwave{\max_{T\in\calT} \L(\hat{\phi}^R, T) - \min_{\phi\in\calF} \max_{T\in\calT} \L({\phi}, T)} \nonumber \\
    &\hspace*{4mm}+\dashuline{\min_{\phi\in\calF} \max_{T\in\calT} \L({\phi}, T) - \min_{\phi\in\calF} \max_{T} \L({\phi}, T)}  + \uline{\min_{\phi\in\calF} \max_{T} \L({\phi}, T) - \min_{\phi} \max_{T} \L({\phi}, T)} \label{E2-decomp-1} \\
    &\leq \dotuline{\max_{{\phi}\in\mathcal F}\left|\max_{T\in\mathcal T} \L({\phi}, T)-\max_{T} \L({\phi}, T)\right|} + \uwave{\max_{T\in\calT} \L(\hat{\phi}^R, T) - \min_{\phi\in\calF} \max_{T\in\calT} \L({\phi}, T)} \nonumber \\
    &\hspace*{4mm}+\dashuline{\max_{{\phi}\in\mathcal F}\left|\max_{T\in\mathcal T} \L({\phi}, T)-\max_{T} \L({\phi}, T)\right|} + \uline{\min_{\phi\in\cF} \L(\phi) - \L(\phi^*)} \label{E2-decomp-2} \\
    &\leq \dotuline{\cE_{In}^{A}(\cF, \cT)} + \uwave{\left|\max_{T\in\calT} \L(\hat{\phi}^R, T) - \min_{\phi\in\calF} \max_{T\in\calT} \L({\phi}, T)\right|} + \dashuline{\cE_{In}^{A}(\cF, \cT)} + \uline{\cE_{Out}^{A}(\cF)}, \label{E2-decomp-no-exp}
\end{align}
%
where the transition from \eqref{E2-decomp-1} to \eqref{E2-decomp-2}, namely, the inequality between \dashuline{dashed} terms, is thanks to Lemma \ref{lem-aux}.

Taking the expectation of the inequality \eqref{E2-decomp-no-exp} results in the following:
\begin{align}
    \EXY \cE_2(\hphiR) \leq \cE_{In}^{A}(\cF, \cT) + \cE_{Out}^{E}(\cF, \cT, N, M) + \cE_{In}^{A}(\cF, \cT) + \cE_{Out}^{A}(\cF). \label{E2-decomp}
\end{align}
%
The combination of \eqref{err-bound-E1-E2}, \eqref{E1-decomp} and \eqref{E2-decomp} finishes the proof.
\end{proof}

\subsection{Proof of theorem \ref{thm:estim_error}}\label{proof:estim_error}
Our theorem \ref{thm:estim_error} uses some standard notions from learning theory, see, e.g.~\citep[\S 26]{shalev2014understanding}. We recall them for the convenience. For a class $\cF$ of functions $\phi: \cY \rightarrow \bbR$ and probability distribution $q$, the \textbf{representativeness} of a sample $Y = \{y_1, \dots, y_{M}\} \sim q$ of size $M$ is:
\begin{equation}
    \text{Rep}_{\cF, q}(Y)\eqdef \sup_{\phi\in\cF} \left|\int \phi(y) q(y)dy -\frac{1}{M} \sum_{m=1}^M \phi(y_m)\right|.
        \label{representativeness-def}
\end{equation}
In turn, the \textbf{Rademacher complexity} of the class $\cF$ w.r.t. the probability distribution $q$ and sample size $M$ is given by:
\begin{equation}
    \cR_{q, M}( \cF) \eqdef \frac{1}{M}\E{\sigma, Y} \bigg\{ \sup_{\phi \in \mathcal{F}} 
        % \bigg|
            \sum_{m = 1}^{M} \phi(y_m) \sigma_m
        % \bigg|
    \bigg\},
\label{rademcaher-def}
\end{equation}
where $Y = \{y_m\}_{m = 1}^{M} \sim q$ are mutually independent, $\sigma = \{\sigma_m\}_{m = 1}^M$ are mutually independent Rademacher random variables, i.e., $\text{Prob}\big(\sigma_m = 1\big) = \text{Prob}\big(\sigma_m = -1\big) = 0.5$, and the expectation is taken with respect to both $Y$, $\sigma$. The quantities \eqref{rademcaher-def} and $\eqref{representativeness-def}$ have a well-known relation \citep[Lemma 26.2]{shalev2014understanding}:
\begin{equation}
    \E{Y}\text{Rep}_{\cF, q}(Y)\leq 2 \cR_{q, M}( \cF),
    \label{rademacher-bound}
\end{equation}
where the expectation is taken w.r.t. random i.i.d. sample $Y\sim q$ of size $M$.

%%%%%%%%%%%%%%%

Now we proceed to an auxiliary statement needed for the proof of our Theorem \ref{thm:estim_error}. In the next lemma, we upper-bound the difference between the original and sample-based  functionals $\L$ and $\hL$ with representativeness numbers w.r.t. classes $\cF$, $\cH$.

\begin{lemma}\label{lemma:lhatl}
Let $X$ and $Y$ be training samples from $p$ and $q$, respectively. Under the conditions of Theorem \ref{thm:estim_error} it holds:
\begin{align}
\sup _{\phi \in \calF} \sup_{T \in \calT}\big|\L(\phi,T)-\hat \L(\phi,T)\big| \leq \operatorname{Rep}_{\calH, p}( X)+\operatorname{Rep}_{ \calF, q}(Y), \label{ldiff-with-reps}
\end{align}
where $\operatorname{Rep}_{\calH, p}(X)$ and $\operatorname{Rep}_{\cF, q}(Y)$ are representativeness \eqref{representativeness-def} of the corresponding samples w.r.t. corresponding classes.
\end{lemma}
%
\begin{proof}
Let us write out in detail the right-hand-side of \eqref{ldiff-with-reps} and regroup the terms:
%
\begin{align*}
\sup _{\phi \in \calF} \sup_{T \in \calT} \big|\L(\phi,T)-&\hat \L(\phi,T)\big| = \\
&= \sup_{\phi\in \calF\ T\in \calT} \bigg| \int \left[ \langle x, T(x))-\phi(T(x)\rangle \right] p(x)dx+\int \phi(y) q(y)dy  \\
&\hspace*{4mm}-\frac{1}{N} \sum_{n=1}^N\left[ \langle
x_n, T(x_n)\rangle- \phi(T(x_n))\right]-\frac{1}{M} \sum_{m=1}^M \phi\left(y_m\right) \bigg|
\\ &\leq 
\sup_{\phi\in \calF\ T\in \calT}\left\{\left|\int \underbrace{\left[ \langle x, T(x)\rangle-\phi(T(x)) \right]}_{=h(x), h \in \cH} p(x)dx -\frac{1}{N} \sum_{n=1}^N\underbrace{\left[\langle x_n, T(x_n)\rangle-\phi(T(x_n))\right]}_{=h(x_n)}\right| \right.\\
&\hspace*{4mm}\left.+\left| \int \phi(y) q(y)dy-\frac{1}{M} \sum_{m=1}^M \phi(y_m) \right|\right\} 
\\ &\leq
 \sup_{h\in\calH} \left|\int h(x) p(x)dx -\frac{1}{N} \sum_{n=1}^N h(x_n)\right| + \sup_{\phi\in\calF} \left| \int \phi(y) q(y)dy-\frac{1}{M} \sum_{m=1}^M \phi(y_m) \right| \\ &=
\operatorname{Rep}_{\calH, p}( X)+\operatorname{Rep}_{\calF, q}(Y).
\end{align*}
\end{proof}
%
Now we proceed to the proof of Theorem \ref{thm:estim_error}
%
\begin{proof}[Proof of theorem \ref{thm:estim_error}]
We start by decomposing $\cE_{In}^{E}$ and $\cE_{Out}^{E}$ into subterms:
\begin{align}
    \mathcal{E}_{In}^{E}(\calF, \calT, N, M) & = 
    \EXY
     \left| \L(\hat{\phi}^R, \hat T^R) - \max_{T\in\calT}\L(\hat {\phi}^R, T)\right| \nonumber\\
    &\leq \EXY
    \left[ \left| \L(\hat{\phi}^R, \hat T^R) -  \hat \L(\hat {\phi}^R, \hat T^R)\right| \right. \label{eq:26} \\
    &\hspace*{4mm} + \left. \left| \hat \L(\hat {\phi}^R, \hat T^R)- \max_{T\in\calT}\L(\hat {\phi}^R, T)\right| \right]; \label{eq:27} \\
    \mathcal{E}_{Out}^{E}(\calF, \calT, N, M) & = \EXY
    \left[ \max_{T\in\calT} \L(\hat{\phi}^R, T) - \min_{\phi\in\calF} \max_{T\in\calT} \L({\phi}, T) \right] \nonumber \\
    & \leq \EXY
    \left[ \left| \max_{T\in\calT} \L(\hat{\phi}^R, T) - \hat \L(\hat{\phi}^R, \hat T^R) \right| \right. \label{eq:28}\\
    &\hspace*{4mm} + \left. \left| \hat \L(\hat{\phi}^R, \hat T^R) - \min_{\phi\in\calF} \max_{T\in\calT} \L({\phi}, T) \right| \right]. \label{eq:29}
\end{align}
%
Now we analyze the subterms from the equations above separately. For simplicity, we omit the expectation ``$\EXY$'' sign.

\uline{Term \eqref{eq:29}}
\begin{equation}
    \left| \hat \L(\hat {\phi}^R, \hat T^R)- \min_{\phi\in\calF}\max_{T\in\calT}\L(\phi, T)\right| = \left|\min_{\phi\in\calF}\max_{T\in\calT} \hat \L( {\phi}, T)- \min_{\phi\in\calF}\max_{T\in\calT}\L(\phi, T)\right|.
\end{equation}
%
Notice that $\max_{T\in\calT}\L(\phi, T)$ and $\max_{T\in\calT}\hat\L(\phi, T)$ are some functionals, depending on $\phi$ (denote them as $l(\phi)$ and $\hat l(\phi)$). Then we can apply Lemma \ref{lem-aux}:
%
\begin{equation}
    \left| \min_{\phi\in\calF}l(\phi)-\min_{\phi\in\calF}\hat l(\phi) \right| \leq \max_{\phi\in\calF}\left| l(\phi)-\hat l(\phi) \right| = \max_{\phi\in\calF} \left| \max_{T\in\calT} \hat \L( {\phi}, T)- \max_{T\in\calT}\L(\phi, T) \right|.
\end{equation}
%
Applying Lemma \ref{lem-aux} again to the internal expression, we get: 

\begin{equation}
    \max_{\phi\in\calF} \left| \max_{T\in\calT} \hat \L( {\phi}, T)- \max_{T\in\calT}\L(\phi, T) \right| \leq \max_{\phi\in\calF} \max_{T\in\calT} \left|  \hat \L( {\phi}, T)- \L(\phi, T) \right|.
\end{equation}

\uline{Terms \eqref{eq:27}, \eqref{eq:28}.}

Similarly, we apply Lemma \ref{lem-aux}:
%
\begin{equation}
    \left| \hat \L(\hat {\phi}^R, \hat T^R)- \max_{T\in\calT}\L(\hat {\phi}^R, T)\right| = \left|\max_{T\in\calT} \hat \L(\hat {\phi}^R, T)- \max_{T\in\calT}\L(\hat {\phi}^R, T)\right| \leq
    \max_{T\in\calT} \left| \hat \L(\hat {\phi}^R, T)- \L(\hat {\phi}^R, T) \right|.
\end{equation}
%
\uline{Finally}, we get that all four terms \eqref{eq:26}, \eqref{eq:27}, \eqref{eq:28}, \eqref{eq:29} are bounded by $\max_{\phi\in\calF} \max_{T\in\calT} \left|  \hat \L( {\phi}, T)- \L(\phi, T) \right|$. Therefore, by applying Lemma \ref{lemma:lhatl} we get:
%
\begin{align*}
    \mathcal{E}_{In}^{E}(\calF, \calT, N, M) + \mathcal{E}_{Out}^{E}(\calF, \calT, N, M) &\leq \EXY\left[ 4\max_{\phi\in\calF} \max_{T\in\calT} \left|  \hat \L( {\phi}, T)- \L(\phi, T) \right| \right] \\ &\leq 4 \E{X}\operatorname{Rep}_{\calH, p}( X)+4 \E{Y}\operatorname{Rep}_{\calF, q}(Y) \\
    &\overset{\text{\eqref{rademacher-bound}}}{\leq} 8 \mathcal R_{p, N}(\mathcal H) + 8 \mathcal R_{q, M}(\mathcal F),
\end{align*}
which finishes the proof.
\end{proof}

\subsection{Proof of theorem \ref{thm:approx_inner}}\label{proof:approx_inner}
To begin with, we recall some standard definitions and notions used in this subsection. Given function $\phi \in \cC(\cY)$, its \textbf{Lipschitz (semi)norm} is defined as:
\begin{align*}
    \Vert \phi \Vert_{Lip} \eqdef \sup_{\scriptscriptstyle \substack{y \neq y'\\ y,y'\in\cY}} \frac{\vert \phi(y) - \phi(y')\vert}{\Vert y - y'\Vert_2}.
\end{align*}
(Continuous) functions with bounded Lipschitz norm are called \textbf{lipschitz continuous} and denoted as $Lip(\cY) \subset \cC(\cY)$.

Now we proceed to the main content of the subsection. The next auxiliary lemma states that the differences in the functional $\L$ values could be controlled by the properties of its arguments. 

\begin{lemma}\label{LleqT}
    Let $\phi\in Lip(\mathcal{Y})$ and $T_1,T_2 : \cX \rightarrow \cY$ be measurable mappings. Then,
\[
|\L(\phi,T_1) - \L(\phi,T_2)| \leq (\sup_{x\in \mathcal{X}} \|x\|_2 + \|\phi\|_{Lip})  \lpnorm{T_1-T_2}.
\]
\end{lemma}
\begin{proof} 

Consider the right-hand-side of the desired inequality:
%
\begin{align}
    \big\vert \L(\phi, T_1) - \L(\phi, T_2) \big\vert \hspace{3mm}&=\hspace{3mm} \bigg\vert \int_{\cX} \Big\{\big(\langle x, T_1(x)\rangle - \phi(T_1(x))\big) - \big( \langle x, T_2(x)\rangle - \phi(T_2(x))\big)\Big\}p(x)dx \bigg\vert \nonumber \\
    &\hspace{-6mm}\overset{\text{Jensen ineq.}}{\leq} \int_{\cX} \Big\vert\big(\langle x, T_1(x)\rangle - \phi(T_1(x))\big) - \big(\langle x, T_2(x)\rangle - \phi(T_2(x))\big)\Big\vert p(x)dx. \label{ldiff-1}
\end{align}
%
Consider the expression under integral sign:
\begin{align}
    \Big\vert\big(\langle x,T_1(x) \rangle - \phi(T_1(x))\big) - \big(\langle (x,T_2(x)\rangle  - \phi(T_2(x))\big)\Big\vert &\leq \dotuline{\big\vert \langle x,T_1(x)\rangle -\langle x,T_2(x)\rangle \big|} + \dashuline{\big|\phi(T_1(x))-\phi(T_2(x))\big|} \nonumber \\
    &\leq \dotuline{\|x\|_2  \norm{T_1(x)-T_2(x)}_2} + \dashuline{\|\phi\|_{Lip} \norm{T_1(x)-T_2(x)}_2}. \label{ldiff-2} 
\end{align}
The substitution of \eqref{ldiff-2} into \eqref{ldiff-1} finishes the proof.
\end{proof}

\begin{corollary}\label{cor1}
Let $ T_{\phi}^{R} = \argmax_{T\in\mathcal{T}}\L(\phi,T),\ T_{\phi} = \argmax_{T}\L(\phi,T)$. Then,
\[
 \max_{T} \L(\phi,T) - \max_{T\in\mathcal{T}} \L(\phi,T)\leq \big(\sup_{x\in \mathcal{X}} \|x\|_2 + \|\phi\|_{Lip}\big)  \lpnorm{T_{\phi}^{R}-T_{\phi}}.
\]
\end{corollary}
The next auxiliary lemma upper-bounds the $L^2$ norm between maps which are optimal for given dual potentials.

\begin{lemma}\label{lemma 2.3}
    %(lemma 2.3) from Note "Error bounds for NOT"
    Let $\phi_1,\phi_2 \in Lip(\cY)$ be $\beta$-strongly convex. Then:
    \begin{align*}
        \lpnorm{T_{\phi_1}-T_{\phi_2}}\leq \frac{1}{\beta} \|\phi_1-\phi_2\|_{Lip},
    \end{align*}
    where $T_{\phi_1} = \argmax_{T}\L(\phi_1,T)$; $T_{\phi_2} = \argmax_{T}\L(\phi_2,T)$.
\end{lemma}
\begin{proof}

We pick $x \in \cX$ and consider function $y \mapsto \langle x, y\rangle-\phi_1\left(y\right)$. Then:
\vspace{-2mm}
\begin{itemize}
    \item $T_{\phi_1}(x)$ is the maximizer of $y \mapsto \langle x, y\rangle-\phi_1\left(y\right)$;
    \vspace{-2mm}
    \item  $y \mapsto \langle x, y\rangle-\phi_1\left(y\right)$ is a $\beta$-strongly concave function.
\end{itemize}
\vspace{-2mm}
By the property of strongly-concave function, we have:
%
\begin{align}
\langle x, T_{\phi_1}(x)\rangle-\phi_1\left(T_{\phi_1}(x)\right) - \frac{\beta}{2}\left\|T_{\phi_1}(x)-T_{\phi_2}(x)\right\|^2_2 &\geq \langle x, T_{\phi_2}(x)\rangle-\phi_1\left(T_{\phi_2}(x)\right) \Longrightarrow \nonumber \\
\frac{\beta}{2}\left\|T_{\phi_1}(x)-T_{\phi_2}(x)\right\|^2_2 &\leq\phi_1\left(T_{\phi_2}(x)\right)-\phi_1\left(T_{\phi_1}(x)\right) + \langle x, T_{\phi_1}(x)\rangle - \langle x, T_{\phi_2}(x)\rangle. \label{T-through-phi-1}
\end{align}
Similar inequality holds for $\phi_2$:
\begin{align}
 \langle x, T_{\phi_2}(x)\rangle -\phi_2\left(T_{\phi_2}(x)\right) -\frac{\beta}{2}\left\|T_{\phi_1}(x)-T_{\phi_2}(x)\right\|^2_2 &\geq \langle x, T_{\phi_1}(x)\rangle - \phi_2\left(T_{\phi_1}(x)\right) \Longrightarrow \nonumber \\
 \frac{\beta}{2}\left\|T_{\phi_1}(x)-T_{\phi_2}(x)\right\|^2_2 &\leq \phi_2\left(T_{\phi_1}(x)\right) - \phi_2\left(T_{\phi_2}(x)\right) + \langle x, T_{\phi_2}(x)\rangle -\langle x, T_{\phi_1}(x)\rangle. \label{T-through-phi-2}
\end{align}
%
Combining inequalities \eqref{T-through-phi-1} and \eqref{T-through-phi-2} we get:
\begin{align*}
    \beta\left\|T_{\phi_1}(x)-T_{\phi_2}(x)\right\|^2_2 &\leq\left[\phi_1-\phi_2\right]\left(T_{\phi_2}(x)\right)-\left[\phi_1-\phi_2\right] \left(T_{\phi_2}(x)\right) \\ &\leq \|\phi_1-\phi_2\|_{Lip} \left\|T_{\phi_1}(x)-T_{\phi_2}(x)\right\|_2,
\end{align*}
i.e., $\left\|T_{\phi_1}(x)-T_{\phi_2}(x)\right\|^2_2 \leq \frac{1}{\beta^2} \|\phi_1-\phi_2\|_{Lip}^2$. Taking the expectation w.r.t. probability distribution $p(x)$ and then extracting the roots of the obtained expressions completes the proof.
\end{proof}
%
Now we proceed to the proof of the main theorem statement.

\begin{proof}[Proof of theorem \ref{thm:approx_inner}]

First notice, that from compactness of $\mathcal{F}$ it follows that $\calF$ is uniformly bounded, i.e, $\exists L \in \mathbb{R}:\ \forall \phi\in\mathcal{F}\ \|\phi\|_{Lip} \leq L$.

For a fixed arbitrary small $\varepsilon$ pick $\delta_1 = \frac{\beta \varepsilon}{2\left(\sup_{x\in \mathcal{X}} \|x\| + L\right)}$. From the definition of compactness, it follows that there exists a finite \( \delta_1 \)-covering of \( \mathcal{F} \): 

\begin{equation}
\forall \delta_1 > 0\ \exists n=n(\delta_1),\ \exists \phi_1, \dots, \phi_n\in \mathcal{F} :\ \mathcal{F} \subset \bigcup_{i=1}^n \mathcal{F}_i,
\end{equation}

where $\mathcal{F}_i = \{\phi:\ \phi\in \mathcal{F}, \|\phi-\phi_i\|_{Lip}\leq \delta_1\}$

Pick $\delta_2 = \frac{\varepsilon}{2\left(\sup_{x\in \mathcal{X}} \|x\| + L\right)}$. According to \citep[Theorem 4.16]{kidger2020universal}  for  each $T_{\phi_k}$ there exists neural network $T_{\phi_k}^{NN(\delta_2)} $, such that $\norm{T_{\phi_k}^{NN(\delta_2)}-T_{\phi_k}}_{L_2(p)} \leq \delta_2$ for arbitrary small $\delta_2$. Let's choose class of neural networks $\mathcal{T}(\delta_2)$ such that it includes all the functions $T_{\phi_1}^{NN(\delta_2)}, \dots, T_{\phi_n}^{NN(\delta_2)}$. Then for such class



\begin{equation}
\max_{T} \L(\phi_k,T) - \max_{T\in\mathcal{T}_{NN}(\delta_2)} \L(\phi_k,T) \leq \max_{T} \L(\phi_k,T) -  \L(\phi_k,T_{\phi_k}^{NN(\delta_2)}) .
\end{equation}

Now we are ready to bound inner approximation error.

Consider $\phi\in\mathcal{F}$. Then $\phi$ belongs to one of the sets of a finite coverage $\mathcal{F}_k$. According to lemma \ref{lemma 2.3} we have $\norm{T_{\phi_k}-T_{\phi}}_{L_2(p)} \leq \frac{1}{\beta}\|\phi_k-\phi\|_{Lip} \leq \frac{\delta_1}{\beta}$

We also can approximate $T_{\phi_k}$ by the neural network, so we can do it with $T_{\phi}$:

\begin{equation}
 \norm{T_{\phi_k}^{NN(\delta_2)}-T_{\phi}}_{L_2(p)} \leq 
 \norm{T_{\phi_k}^{NN(\delta_2)}-T_{\phi_k}}_{L_2(p)} +  \norm{T_{\phi_k}-T_{\phi}}_{L_2(p)} \leq  \\
\delta_2 + \frac{\delta_1}{\beta}.
\end{equation}

Then it remains to apply Lemma \ref{LleqT} and use the uniform boundness of $\mathcal{F}$:

\begin{equation}
\max_{T} \L(\phi,T) - \L(\phi,T_{\phi_k}^{NN(\delta_2)}) \leq \left(\sup_{x\in \mathcal{X}} \|x\| + \|\phi\|_{Lip}\right)  \norm{T_{\phi}^{{NN}(\delta_2)}-T_{\phi}}_{L_2(p)} \leq 
 \left(\sup_{x\in \mathcal{X}} \|x\| + L\right) \left( \delta_2 + \frac{\delta_1}{\beta} \right) = \varepsilon.
\end{equation}

Due to the arbitrariness of the choice of $\phi$, we obtain

\begin{equation}
    \max_{\phi\in\calF}\left[\max_{T} \L(\phi,T) - \max_{T\in\calT(\delta_2)}\L(\phi,T)\right] \leq \varepsilon
\end{equation}

\end{proof}

\subsection{Proof of theorem \ref{thm:approx_outer}}\label{proof:approx_outer}

In what follows, we will use the notion of $\mathcal{B}$-smoothness. Function $\phi: \cY \rightarrow \bbR$ is called \textbf{$\mathcal B$-smooth} if it is continuously differentiable on $\calY$, and its gradient
is Lipschitz continuous with Lipschitz constant $\mathcal B$:
%
\begin{equation}
    \norm{\nabla \phi(y_1)-\nabla \phi(y_2)}_2 \leq \mathcal B\norm{y_1-y_2}_2, \quad \forall y_1,y_2\in\calY. \nonumber
\end{equation}
%
We proceed with several auxiliary results needed for the main proof.

\begin{lemma}\label{lemma:smooth_lipsch}
    Let $\phi$ is $\mathcal B$-smooth on $\calY$. Then there exists $L>0$ such that $\phi$ is $L$-lipschitz, i.e., $\Vert \phi \Vert_{Lip} \leq L$.
\end{lemma}
%
\begin{proof}
At first, we show that $\|\nabla \phi(y)\|_2$ is bounded on $\cY$. Fix $y_0\in \calY$. Then $\forall y \in \cY$:
%
\begin{align}
\|\nabla \phi(y)\|_2 \leq \|\nabla \phi(y) - \nabla \phi(y_0)\|_2 + \|\nabla \phi(y_0)\|_2 \leq \mathcal B\|y-y_0\|_2 + \|\nabla \phi(y_0)\|_2 \leq 2\mathcal B\sup_{\calY}\|y\|_2 + \|\nabla \phi(y_0)\|_2. \label{smooth-vs-lip-1}
\end{align}
%
By the property of $\mathcal B$-smooth functions \citep[\S 12.1.3]{shalev2014understanding}, $\forall y_1, y_2 \in \cY$: 
%
\begin{align*}
\phi(y_2) - \phi(y_1) &\leq \langle \nabla \phi(y_1), y_2-y_1 \rangle + \frac{\mathcal B}{2}\|y_2-y_1\|_2\\ &\leq \|\nabla \phi(y_1)\|_2 \|y_2-y_1\|_2 +  \frac{\mathcal B}{2}\|y_2-y_1\|_2 \\&\leq \left( \|\nabla \phi(y_1)\|_2 + \frac{\mathcal B}{2} \right) \|y_2-y_1\|_2 \\
&\overset{\text{\eqref{smooth-vs-lip-1}}}{\leq} \left( 2\mathcal B\sup_{\calY}\|y\|_2 + \|\nabla \phi(y_0)\|_2 + \frac{\mathcal B}{2} \right) \|y_2-y_1\|_2.
\end{align*}
Picking $L \eqdef 2\mathcal B\sup_{\calY}\|y\|_2 + \|\nabla \phi(y_0)\|_2 + \frac{\mathcal B}{2}$ finishes the proof.
\end{proof}

\begin{lemma}[Property of convex conjugate w.r.t. sup norm]\label{lemma-conj-sup-norm}
    Let $\phi_1, \phi_2 \in \cC(\cY)$. Then,
    \begin{align*}
        \Vert \ol{\phi_1} - \ol{\phi_2} \Vert_{\infty} \leq \Vert \phi_1 - \phi_2 \Vert_{\infty}.
    \end{align*}
    If $\phi_1$ and $\phi_2$ are convex, then the inequality becomes an equality, i.e., $\Vert \ol{\phi_1} - \ol{\phi_2} \Vert_{\infty} = \Vert \phi_1 - \phi_2 \Vert_{\infty}$.
\end{lemma}
%
\begin{proof}
    We pick $x \in \cX$. Then,
    \begin{align*}
        \vert \ol{\phi_1}(x) - \ol{\phi_2}(x) \vert &\hspace{4mm}=\hspace{4mm} \vert\max_{y \in \cY} [ \langle x, y \rangle - \phi_1(y)] - \max_{y \in \cY} [\langle x, y\rangle - \phi_2(y)]\vert \\
        &\overset{\text{Lemma \ref{lem-aux}}}{=} \max_{y \in \cY} \vert \cancel{\langle x, y \rangle} - \phi_1(y) - \cancel{\langle x, y \rangle} + \phi_2(y) \vert \\
        &\hspace{4mm}\leq\hspace{4mm} \Vert \phi_1 - \phi_2 \Vert_{\infty}.
    \end{align*}
    Taking the supremum w.r.t. $x$ in the inequality above yields $\Vert \ol{\phi_1} - \ol{\phi_2} \Vert_{\infty} \leq \Vert \phi_1 - \phi_2 \Vert_{\infty}$.

    If $\phi_1$ and $\phi_2$ are convex, then:
    \begin{gather*}
        \Vert \phi_1 - \phi_2 \Vert_{\infty} = \Vert \ol{\ol{\phi_1}} - \ol{\ol{\phi_2}} \Vert_{\infty} \leq \Vert \ol{\phi_1} - \ol{\phi_2} \Vert_{\infty},
    \end{gather*}
    which proves the remaining statement.
\end{proof}

\begin{lemma}\label{lemma:conv_approx_by_lipsch}
    Let $\phi \in \cC(\cY)$ be a convex function. Then $\phi$ can be arbitrarily closely approximated by a Lipschitz function, i.e., for any $\varepsilon > 0$ one can choose a lipschitz continuous function $\phi_L \in Lip(\cY)$ such that
    \[
    \|\phi-\phi_L\|_{\infty} \leq \varepsilon.
    \]
     %
\end{lemma}
%
\begin{proof}

Pick $\varepsilon > 0$ and take $\mathcal B = \frac{\sup_{\cY} \Vert y\Vert_2^2}{2 \varepsilon}$. Consider the following functions (recall that $\ol{\phi}$ is convex conjugate of $\phi$):
%
\begin{gather*}
    \psi(x) \eqdef \overline \phi(x) + \frac{\|x\|^2_2}{2\mathcal B} \quad ; \quad \phi_L \eqdef \overline \psi = \overline {\overline \phi + \frac{\|.\|^2_2}{2\mathcal B}}.
\end{gather*}
%
Note that $\psi$ is $\frac{1}{\mathcal B}$-strongly convex. Therefore, by \citep[Theorem 6]{kakade} $\phi_L$ is $\mathcal B$-smooth.
Applying Lemma \ref{lemma:smooth_lipsch} we get that $\phi_L$ is also Lipschitz with some constant $L$.

Finally,
\begin{gather*}
    \|\phi-\phi_L\|_{\infty} \overset{\text{Lemma \ref{lemma-conj-sup-norm}}}{=} \norm{{\overline{\phi}} -  \ol{\phi_L}}_{\infty} = \norm{{\overline{\phi}} -  {\overline \phi + \frac{\|.\|^2_2}{2L_s}}}_{\infty} = \sup_{\calY} \frac{\|y\|^2_2}{2\mathcal B} = \varepsilon,
\end{gather*}
which completes the proof.
%
\end{proof}

Now we are ready to prove the main Theorem.
\begin{proof}[Proof of theorem \ref{thm:approx_outer}]

By the assumption of the theorem $\phi^* = \argmin_{\phi} \L(\phi)$ is $\beta$-strongly convex. Consider
%
\begin{align*}
    \phi^{*, -\beta} \eqdef \phi^* - \beta \frac{\Vert\cdot\Vert_2^2}{2}.
\end{align*}
%
Note that $\phi^{*, -\beta}$ is convex. From Lemma \ref{lemma:conv_approx_by_lipsch}, we can find $\phi_L \in Lip(\cY)$ such that
\begin{gather*}
    \|\phi^{*, -\beta}-\phi_L\|_{\infty} < \frac{\varepsilon}{4}.
\end{gather*}
Thanks to \citep[Theorem 1]{icnn_approx} we can find input convex neural network $\phi_L^{nn}$ with ReLU activations such that
\begin{gather*}
    \|\phi_L-\phi_L^{nn}\|_{\infty} < \frac{\varepsilon}{4}.
\end{gather*}
Obviously, $\phi_L^{nn}$ is also Lipschitz since it is a composition of piecewise linear functions. Consider
%
\begin{gather*}
    \phi_L^{\beta} \eqdef \phi_L^{nn}+\beta\frac{\|.\|_2^2}{2}.
\end{gather*}
Note that $\phi_L^{\beta}$ is $\beta$-strongly convex. Then,
%
\begin{align*}
\|\phi^*-\phi_L^{\beta}\|_{\infty} = \Vert \phi^{*, -\beta} - \phi_L^{nn}\Vert_{\infty} \leq  \|\phi^{*, -\beta}-\phi_L\|_{\infty} + \|\phi_L-\phi_L^{nn}\|_{\infty} \leq \frac{\varepsilon}{2}.
\end{align*}
%
Lets check that $\phi_L^{\beta}$ delivers the desired bound.
%
\begin{align*}
\L(\phi_L^{\beta}) - \L(\phi^*) &= \int_{\calX} \overline {\phi_L^{\beta}}(x) p(x) d x + \int_{\calY} \phi_L^{\beta}(y) q(y) d y - \Big(\int_{\calX} \overline {\phi^{*}}(x) p(x) d x + \int_{\calY} \phi^*(y) q(y) d y \Big) \\
&\leq \int_{\calX} \big\vert \overline {\phi^*}(x)-\overline {\phi_L^{\beta}}(x)\big\vert p(x) d x + 
\int_{\calY}\big\vert \phi^*(y)-\phi_L^{\beta}(y)\big\vert q(y) d y \\
&\leq \int_{\calX} \!\!\!\!\!\!\!\underbrace{\norm{\overline {\phi^*}-\overline {\phi_L^{\beta}}}_{\infty}}_{= \norm{\phi^*-\phi_L^{\beta}}_{\infty}\text{ by Lemma \ref{lemma-conj-sup-norm}}}\!\!\!\!\!\!\!\!\!\!p(x) d x + 
\int_{\calY}\norm{\phi^*-\phi_L^{\beta}}_{\infty} q(y) d y \\ &= \varepsilon,
\end{align*}
which finishes the proof.
\end{proof}

\begin{proof}[Proof of corollary \ref{corollary_f}]
    It remains to provide some class $\calF$, which contains $\phi_L^{\beta}$. We can choose sufficiently expressive class $\calF_{icnn}$ and then define $\calF$ such that $\phi\in\calF$ has the form $x\mapsto \phi_{\theta}(x)+\beta \frac{x^T x}{2}$ where $\phi_{\theta}\in\calF_{icnn}$. For compactness of $\calF$ by lipschitz norm we can bound all weight matrices by second norm.
\end{proof}

\subsection{Proof of theorem \ref{thm:main} and corollary \ref{corollary-rademacher}}

\begin{proof}[Proof of theorem \ref{thm:main}]\label{proof-main-thm}

    Let's bound all the terms from decomposition in theorem \ref{thm:decomp} step by step.
    
    From corollary \ref{corollary_f} we get such class $\calF = \calF(\frac{\varepsilon}{4}, \beta)$ that $\mathcal{E}_{Out}^{A}(\mathcal F) < \frac{\varepsilon}{4}$.  
    
     The second step is to apply theorem \ref{thm:approx_inner}, which gives us  $\calT=\calT(\frac{\varepsilon}{12},\calF)$, such that $3\mathcal{E}_{In}^{A}(\mathcal F, \calT) < \frac{\varepsilon}{4}$.

     Finally, applying theorem \ref{thm:estim_error}, we bound remaining two terms  $\mathcal{E}_{In}^{E}(\mathcal F, \mathcal T, N, M) + \mathcal{E}_{Out}^{E}(\mathcal F, \mathcal T, N, M) \leq 8\mathcal R_{p, N}(\mathcal H)+8\mathcal R_{q, M}(\mathcal F)$
     
\end{proof}

\begin{proof}[Proof of corollary \ref{corollary-rademacher}]
\label{proof:corollary_rademacher}
     Our goal is to obtain bounds depending on sample sizes for Rademacher complexities of function classes $\calH$ and $\calF$ introduced in Theorem \ref{thm:estim_error}. 
     Let $\calT$ be a class of feedforward neural network with Lipschitz activation functions, e.g., ReLU, and bounded norms of weights. Note that the Rademacher complexity of class $\calT$ is known to attain an upper bound: $\mathcal{R}_{p,N}\leq O(\frac{1}{\sqrt{N}})$ where $O(\cdot)$ hides the constants which do not depend on the sample size $N$, see, e.g., \citep{golowich2018size}.
     
     \underline{\textit{Step 1}}. We start with obtaining an upper bound for $\mathcal{R}_{q,M}(\calF)$. Recall that every function $\phi\in\calF$ has the form $x\mapsto \phi_{\theta}(x)+\beta \frac{x^T x}{2}$ where $\phi_{\theta}$ belongs to the class of input convex neural networks $\calF_{icnn}$. Here we additionally assume that the neural nets from $\calF_{icnn}$ have bounded norms  and Lipschitz activation functions. Then from the well-known monotonicity property of Rademacher complexity, we get that $$\calF_{icnn}\subset\calT\Longrightarrow\mathcal{R}_{p,N}(\calF_{icnn})\leq \mathcal{R}_{q,M} (\calT)\leq O(\frac{1}{\sqrt{M}}).$$
    The complexity of the constrained quadratic functions $x\mapsto \frac{\beta}{2}x^Tx$ is also $O(\frac{1}{\sqrt{M}})$. It follows from their representation using the Reproducing Kernel Hilbert spaces (RKHS), see, e.g., \citep[Theorem 6.12]{mohri2018foundations}, \citep[Lemma 5 \& Eq. 24]{latorre2021effect}. Then, from the additivity of Rademacher complexities it follows that the complexity of functional class $\calF$ is also bounded by $O(\frac{1}{\sqrt{M}})$.

    \underline{\textit{Step 2}}. Below we prove that $\mathcal{R}_{p,N}(\calH)$ attains an upper bound $O(\frac{1}{\sqrt{N}})$. Recall that class $\calH$ consists of functions $h$ which map $x\mapsto \langle x,T(x)\rangle -\phi(T(x))$ where $T\in\calT,\phi\in\calF$. Note that the function $\phi \circ T$ is a composition of functions from $\calT$ and, thus, also belongs to the class $\calT$ of neural networks. Thus, by monotonocity property, the complexity of this class of functions is bounded by $O(\frac{1}{\sqrt{N}})$ which follows from the first step of this proof. It remains to show that the complexity of the class of functions $x\mapsto \langle x, T(x) \rangle$ for $T\in\calT$ is also bounded by $O(\frac{1}{\sqrt{N}})$. Hopefully, for the considered class of neural networks $\calT$, this fact immediatelly follows from \citep[Proposition 5.5]{lei2023generalization} and \citep[Lemma B.1]{maurer2016vector}.
    Then the upper bound for $\mathcal{R}_{p,N}(\calH)$ follows from the additivity property of Rademacher complexity. It completes the proof.
\end{proof}
\end{document}