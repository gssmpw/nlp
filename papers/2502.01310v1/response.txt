\section{Related Works}
\label{sec:works}

In this section, we first discuss the existing works which conduct theoretical analysis of semi-dual OT losses (\S \ref{subsec:works-OT}). Since minimax objective \eqref{minimax} resembles adversarial training, we additionally discuss some of the literature dealing with theoretical aspects of Generative Adversarial Neural Networks (GAN) losses and how they relate to ours (\S \ref{subsec:works-gans}). 

\subsection{Theoretical analysis of continuous semi-dual OT solvers}\label{subsec:works-OT}

The current progress in theoretical analysis of semi-dual OT could be subdivided into two branches.

\textbf{Non-minimax semi-dual OT.} The first branch of works Cuturi, "Semi-Supervised Learning with a Special Case of Semi-Dual Optimal Transport" analyze the non-minimax losses \eqref{dualconvex} and \eqref{dualconvexcvx} and develop error bounds \eqref{eq-obj-of-study} for pushforward maps $T$ given by the gradient of dual potentials, i.e., $\EXY \Vert \nabla \overline{\phi}^* - \nabla \widehat{\overline{\phi}}_{\omega} \Vert^2_{L^2(p)}$. To achieve particular statistical rates, the authors place certain restrictions on the considered problem setup and deal with specific classes of functions (maps), e.g., wavelet expansions or (kernel-smoothed) plug-in estimators. Recent studies in this direction Cuturi, Benamou, "A Fast Douglas-Rachford Splitting Algorithm" extend the analysis of error rates for the class of potentials $\phi$ given by neural networks, e.g., input convex neural networks. Meanwhile, none of the mentioned works treats the learned map $T$ separately from dual potential $\phi$. Importantly, the analysis of minimax objective \eqref{minimax} is considerably more challenging than \eqref{dualconvex} due to the additional ``degree of freedom'' given by the optimized map $T$. Furthermore, saddle-point problems such as \eqref{minimax} are known to be more tricky for theoretical investigation than usual minimization. At the same time, the practical demand stemming from the recent proliferation of minimax-based OT solvers, see \S \ref{background}, makes such an investigation highly desirable. All of this necessitates separate study of minimax OT solvers; the adaptation of existing non-minimax results is questionable, if that is even possible.

\textbf{Minimax semi-dual OT.} The estimation of \eqref{eq-obj-of-study} in case of minimax OT \eqref{minimax_emp} is a much less explored task. In fact, there are no studies at all that provide statistical bounds for recovered minimax OT map $\widehat{T}_\omega$. The existing works Bousquet, "The Minimax Principle and the Performance of Generative Models" only conduct an analysis of $\Vert T^* - \widehat{T}_{\omega} \Vert^2_{L_2(p)}$ in the sense of \textit{duality gaps}. That is, the error between the true OT map and recovered map is upper-bounded by values of functional $L(\phi, T)$. The duality gaps analysis helps to validate minimax methodology; it is common for more broad class of (non-quadratic) minimax OT solvers Cuturi, "Semi-Supervised Learning with a Special Case of Semi-Dual Optimal Transport". However, we again emphasize that it does not reveal particular statistical rates and guarantees for \eqref{eq-obj-of-study}. In Villani, Bonnotte, "On the rate of convergence of optimal transport", the authors establish promising results for the error of recovered approximate OT map. However, their problem formulation of min-max OT differs from ours, eq. \eqref{minimax}, as they aim to treat OT as the limit of regularized GAN objectives. 

\textbf{Theoretical analysis of other OT formulations.} For the completeness of exposition, we also mention several studies that develop statistical learning analysis of recovered OT map (plan) error for non semi-dual OT formulations. The works Cuturi, "Semi-Supervised Learning with a Special Case of Semi-Dual Optimal Transport" deal with Entropy-regularized OT; Cuturi, Bonnotte, Villani, "Computing Optimal Transportation Maps" investigate unbalanced OT versions.
Although these works are interesting and insightful, their object of study is different from ours, making them less relevant.

\subsection{Theoretical analysis of adversarial Generative Models}\label{subsec:works-gans}
The objective of the semi-dual minimax continuous OT solvers \eqref{minimax} resembles that of GANs Goodfellow et al., "Generative Adversarial Networks". This fact motivates us to review some of the existing theoretical results on GANs below. Still, while there are many papers on GANs Arjovsky, Chen, Chintala, " Wasserstein Generative Adversarial Networks" and a relatively large number of works studying the theoretical aspects of their objectives, they are not relevant to us for two reasons. First, OT solvers and GANs pursue different goals. The main goal of OT solvers is to approximate true OT maps $T^*$, i.e., to yield specific generators which satisfy the optimality condition; accordingly, our theoretical work focuses on the error of this approximation. Meanwhile, GANs objectives usually do not have a unique optimal generator $G^*$; thus, existing theoretical results mostly investigate the error of approximating the ground-truth distribution $q$ by the generated $p_{gen}$. Second, OT solvers and GANs have an evident difference corresponding to the order of 
optimization over generator $G$ (map $T$) and discriminator $D$ (potential $\phi$) in their objectives. In particular, for GANs, the optimization over the generator $G$ is done in the outer problem of their objective, while for OT solvers, the optimization over OT maps is conducted in the inner problem, i.e., the OT map $T^*$ corresponds to the solution of the inner problem. We emphasize the importance of this difference $-$ for OT solvers, OT map $T^*$ corresponds to the solution of the inner problem, which makes the task of theoretical investigation of the error of $T^*$ approximation even more difficult.
Below we discuss two groups of theoretical results on GANs which are slightly relevant to us.


\textbf{Estimation error of distance between generated and true distributions}. The early work of Arjovsky, Chen, Chintala, " Wasserstein Generative Adversarial Networks" introduces a notion of an $\mathcal{F}$-\textit{distance}
between the pair of measures w.r.t. a given class of discriminators $\mathcal{F}$:
$$
d_{\mathcal{F}}(p,q)\!=\! \sup_{D\in \calF} \bbE_{x\sim p} \psi(D(x)) + \bbE_{y\sim q} \psi(1-D(y))-2\psi(\frac{1}{2}).
$$
Here $\psi:[0,1]\rightarrow\bbR$ is any monotone concave function.
For certain classes of functions, these distances define an inner optimization problem for well-known GANs, e.g., Wasserstein, Vanilla, MMD, Sobolev, etc. Class $\mathcal{F}_{nn}$ corresponding to neural networks with a bound on a number of parameters yields a so-called \textit{neural net (NN) distance}. 
The paper Cuturi, "A Fast Douglas-Rachford Splitting Algorithm" establishes an upper bound on the estimation error of this distance, i.e., difference of NN distances $d_{\mathcal{F}_{nn}}(q, p_{gen})$ for empirical and continuous distributions. Subsequent work Arjovsky, Chen, Chintala, " Wasserstein Generative Adversarial Networks" extends this analysis by providing a tighter upper bound and lower bound for this difference. In some sense, these works analyze the error in GANs inner optimization problem coming from the usage of empirical measures $\widehat{p}$, $\widehat{q}$ instead of the real ones.
Still, these works ignore Goodfellow et al., "Generative Adversarial Networks" and Chen et al., " On the Divergence Between Kullback-Leibler Divergence and Jensen-Shannon Divergence".

For completeness, we mention several other works which investigate theoretical properties of specific GANs, e.g., bidirectional Goodfellow et al., "Generative Adversarial Networks", Wasserstein Arjovsky, Chen, Chintala, " Wasserstein Generative Adversarial Networks" and CycleGAN Zhu et al., " Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks".