\section{Background and Related Work}
Understanding the intersection of trust, privacy, and social media design is crucial for developing platforms that effectively support adolescent users in regulating their boundaries. Existing research highlights the limitations of current social media privacy models and underscores the importance of trust in shaping adolescent disclosure and privacy behaviors. This section explores the literature on the kind of privacy adolescents seek, reviews existing social media designs, and the role of trust in privacy management.

For our study, we focus on strengthening existing ties \eg{developing a closer relationship with an acquaintance added as a Follower on Instagram}, especially those ties that extend beyond the online contexts into real-life relationships~\cite{Nguyen2012-cc}. We also draw upon Altman's Social Penetration Theory~\cite{altman1973social} and view relationship-building from the perspective of repeated (reciprocal) exchanges of increasingly deepening and broadening self-disclosure. We take the definition of self-disclosure from Derlega et al.~\cite{derlega1993self}, i.e., \textit{``a deliberate or voluntary activity whereby people reveal information, thoughts, and feelings about themselves to at least one other person during an interaction.''} 

We focus on platforms where people share content with multiple others (1:N) with some degree of permanence, a focus that emerged naturally from our interviews rather than one-to-one messaging or ephemeral group chats. We refer to these as \textit{broadcast social media} for convenience. In these platforms, challenges around self-disclosure are more prevalent and harder to address, partly due to context collapse. 

We also scope trust to \textit{interpersonal trust}---the belief that others have one's best interests at heart---aligning with relational trustworthiness factors such as benevolence and integrity~\cite{Nguyen2009-ri}. Within the scope of teen social media privacy that we examine in this paper, trust manifests as confidence that viewers will not misuse disclosed information and will respond to or judge what is shared with skepticism.

Lastly, we acknowledge that throughout the paper, we alternate between specifying teen users and users in general. This is because much of what is relevant to teens is also applicable to the general population~\cite{Christofides-2012-HeyMomAdults-o}.



\subsection{Teens Privacy Concerns and Awareness}
Adolescents' developmental characteristics make online sharing particularly high-stakes. They experience heightened significance of peer acceptance and sensitivity to social rejection~\cite{Somerville2013-zf}; to maintain social status, teens often expand their ``Friend'' networks beyond the circle of friends they trust~\cite{Yau2019-ab}, intensifying context collapse challenges. Real-world social pressures further complicate privacy management~\cite{Weinstein2022-rh}, as peers' digital documentation and phenomena like \inlinequote{cancel culture} \ie{the practice of publicly shaming and boycotting individuals for perceived wrongdoings} and \inlinequote{receipts} \ie{proof screenshots, screen recordings, or messages saved for evidence of an indiscretion} leave teens feeling vulnerable and disempowered about their privacy~\cite{Weinstein2022-rh, BoydDanah2014ICTS}. 

These challenges are not exclusive to adolescents, but they are particularly relevant due to the public nature of social media platforms, which exacerbates context collapse~\cite{BoydDanah2014ICTS}. As a result, users often feel compelled to share only the \inlinequote{lowest common denominator}~\cite{Hogan2010-oh}, content deemed universally appropriate. Furthermore, the hyperpersonal nature of online communication~\cite{walther2011introduction} often leads to exaggerated or inaccurate understandings of individuals based on limited cues, which makes curated and infrequent posting more high-stakes.

Contrary to the common perception that teenagers are indifferent to or oblivious of online privacy risks, research consistently demonstrates that they are acutely aware of the potential dangers of disclosure online and actively engage in privacy management~\cite{Cho-2018-CollectivePrivacyValidation-h} employing various tactics such as social steganography---embedding hidden meanings in posts---or manually filtering audiences~\cite{DeWolf-2014-ManagingPrivacyFacebook-c}. They recognize that online disclosure can lead to unforeseen consequences, from various cyber risks like cyberstalking and identity theft to exposure caused by people they know~\cite{Agha2023-mu, zhao2022understanding}. Teens also understand that once information is shared online, it is difficult to control its spread given the networked nature of social media platforms~\cite{Dienlin-2015-PrivacyParadoxBehaviors-w, DeWolf-2014-ManagingPrivacyFacebook-c}.

However, such an awareness does not eliminate their anxiety~\cite{kim2025privacysocialnormsystematically}. Many teen users experience stress and uncertainty about their digital footprint, fearing unintended consequences of their disclosures~\cite{Williams2023-jm}. They understand that privacy is not solely an individual concern but is shaped by the actions of others in their social networks~\cite{marwick2014networked}. Given the risks of privacy breaches that can occur even with careful disclosure, teens often experience resigned pragmatism---engaging in privacy protection measures while simultaneously feeling a sense of resignation~\cite{Acquisti-2017-NudgesPrivacyOnline-y, Weinstein2022-rh}. In worse cases, this can lead to outright privacy resignation~\cite{Malkin-2022-RuntimePermissionsAssistants-p}, dysfunctional fear that affects quality of life~\cite{kim2025privacysocialnormsystematically}, and fatalistic \inlinequote{network defeatism}~\cite{de2020contextualizing}. Recognizing the contextual and relational nature of their privacy~\cite{zhao2022understanding}, teens desire to have more agency and feel more empowered in managing their online presence~\cite{kim2025privacysocialnormsystematically, kim2025socialmediaisntjust}. 


\subsection{Privacy as Dynamic Boundary Regulation}
Privacy in social media, especially for teens, is not solely about restricting access to personal information. It is an ongoing process of interpersonal boundary regulation~\cite{Page-2019-PragmaticToolFeatures-p, Barkhuus-2012-MismeasurementPrivacyHCI-d, DeWolf-2014-ManagingPrivacyFacebook-c}. However, instead of supporting adolescents in dynamically managing their boundaries in ways that align with their social needs, social media privacy narratives and platform designs often frame privacy as a trade-off---forcing users to relinquish control in order to engage socially~\cite{Wisniewski-2015-GiveSocialWant-z, Wisniewski-2016-FramingMeasuringUsers-w}. Further, social media safety measures often prioritize surveillance and prevention-based approaches over resilience-based ones~\cite{Wisniewski-2018-PrivacyParadox-l}---for example, privacy-preserving tools that encourage open dialogue~\cite{Chouhan-2019-Co-designingCommunityTogether-x}. Similarly, the concept of group privacy~\cite{Choksi-2024-PrivacyGroupsMatters-b} is often neglected, often emphasizing individual control rather than collective boundary-setting mechanisms. Such views and designs that frame self-disclosure as inherently unsafe largely ignore the social benefits of self-disclosure that are especially critical to adolescents~\cite{Davis2012-bq, Weinstein2022-rh}. 

Privacy is also context- and norm-dependent~\cite{Nissenbaum-2004-PrivacyContextualIntegrity-j}. It is dynamic and situational, influenced by factors such as the intended audience and perceived long-term potential risks~\cite{Barkhuus-2012-MismeasurementPrivacyHCI-d}. Furthermore, privacy preferences shift over time~\cite{Barkhuus-2012-MismeasurementPrivacyHCI-d}, necessitating adaptable privacy tools that reflect evolving needs. However, there is often a mismatch between users' privacy expectations and needs versus the actual control they have over the data they disclose~\cite{Wisniewski-2015-GiveSocialWant-z}. Default privacy settings often fail to provide meaningful protection necessary while navigating the complex and nuanced \textit{contexts}~\cite{Acquisti2017-ez}. This shapes platform norms that are not conducive to co-owners of boundaries upholding their privacy expectations~\cite{Choksi-2024-PrivacyGroupsMatters-b, Abaquita-2020-PrivacyNormsIntegrity-m}. 

Compounding this issue, the often-found one-size-fits-all approach to privacy settings does not account for the diverse needs of users---especially those of adolescents---who navigate varying levels of social exposure and relationships~\cite{Wisniewski-2015-GiveSocialWant-z}. In addition, many platforms prioritize public interaction over privacy, making it difficult for users to navigate context collapse and manage audience segmentation effectively~\cite{10.1145/3686909, Yau2019-ab}. The interplay of these hurdles results in teens expressing concerns about privacy but still engaging in self-disclosure due to social benefits or peer influence~\cite{kim2025privacysocialnormsystematically, Wisniewski2018-rc}. 

\subsection{Effective Privacy Management via Trust}
Trust and boundary regulation are deeply interconnected~\cite{derlega1979self}. Communication Privacy Management (CPM) theory~\cite{Petronio-2002-BoundariesPrivacyDisclosure-m}, for example, highlights how privacy rules and co-ownership of information depend on trust. Trust is fundamental in an environment where users rely on each other to uphold privacy norms~\cite{Waldman2016-em, Waldman2018-dc}. A good example of this is trust in \inlinequote{Bounded Social Media Places} (BSMPs)~\cite{Malhotra-2024-"whatPostPlaces-u}, such as private groups or direct messages, where users assume their information remains within a limited audience. Empirical research has also demonstrated that the relationship between privacy concerns and self-disclosure is significantly mediated by trust~\cite{Joinson2010-cf}. While individuals with higher privacy concerns may be less likely to disclose information, this effect is substantially reduced when they trust the recipient of the information.

Conversely, breaches of trust---whether through breaches, unintended exposure, or platform failures---create \inlinequote{boundary turbulence}~\cite{Petronio2002-ce}, leading to reduced self-disclosure, blocking, and stricter regulations. However, current social media platforms often fail to reinforce privacy responsibilities with clear expectations or reliable controls~\cite{Choksi-2024-PrivacyGroupsMatters-b, Paci-2019-SurveyAccessSystems-a}. Moreover, social influence plays a crucial role in shaping privacy behaviors---users are significantly more likely to adopt security features when they see their peers doing the same~\cite{Akter-2023-EvaluatingImpactSecurity-h}. However, most platforms fail to harness these collective privacy dynamics, instead burdening individual users with the full responsibility of managing their privacy~\cite{Schnitzler-2020-SoKManagingData-h, Humbert-2019-SurveyInterdependentPrivacy-f, hargittai2016can, kim2025privacysocialnormsystematically}. This lack of support not only increases the complexity of privacy management but also exacerbates the pervasive negativity in social media~\cite{kim2025privacysocialnormsystematically}.

\subsection{Existing Design Approaches} %for Trust and Boundary Regulation}
To address these issues around boundary regulation in social media, usable privacy researchers have proposed several design interventions to enhance trust and privacy while accommodating the social needs of users. The overarching theme around the spectrum ideas is the recognition that privacy is not merely about limiting access but rather about enabling users to negotiate social boundaries dynamically. 

One approach is community-based privacy management mechanisms~\cite{Akter-2023-EvaluatingImpactSecurity-h}. These designs acknowledge that users co-manage information and establish shared privacy norms with their friends. However, existing platforms often lack sufficient support for collaborative privacy management mechanisms~\cite{Cho-2018-CollectivePrivacyValidation-h}. To address this gap, tools were designed to enable users to negotiate privacy boundaries together, such as features that allow co-owners of a post to jointly decide its audience or require mutual approval for tags and shares~\cite{Humbert-2019-SurveyInterdependentPrivacy-f, Wisniewski-2016-FramingMeasuringUsers-w}. Another important approach is \inlinequote{community oversight}~\cite{Akter-2024-ExaminingCaregivingSecurity-n, Akter-2023-EvaluatingImpactSecurity-h}, a mechanism that leverages trusted social groups to help individuals manage their digital safety regarding mobile app use. The approach is based on the principles of transparency, making community members' app installations and permissions visible, and awareness, keeping members informed of each other's practices and changes. It is intended to build trust and foster a sense of shared responsibility for privacy and security within the group.

Another key design principle is based on the contextual integrity framework~\cite{nissenbaum2004privacy} where system design would align with users' expectations of information flow. One example is adaptive privacy defaults, which dynamically adjust visibility settings based on contexts such as content sensitivity, audience type, or prior user preferences~\cite{Acquisti2017-ez, Wisniewski-2015-GiveSocialWant-z}. For example, platforms could suggest context-sensitive audience suggestions recommending different levels of visibility for casual updates versus personal reflections~\cite{Mondal-2019-MovingBeyondMedia-p, Sleeper-2013-PostWasntFacebook-m}. Another approach is \inlinequote{role- and relationship-based access control} (RBAC \& ReBAC)~\cite{Paci-2019-SurveyAccessSystems-a}, where access is determined not just by predefined categories (e.g., ``Friends'' vs. ``Public'') but by the relational context between the user and their audience. Real-time audience feedback---such as a preview of who will see a post---could help users make more informed decisions before sharing~\cite{Acquisti-2017-NudgesPrivacyOnline-y, Paci-2019-SurveyAccessSystems-a}. Further, temporary access controls could let users set expiration dates on posts or limit visibility based on contextual factors, like whether a viewer has interacted with them recently~\cite{Ayalon-2013-RetrospectivePrivacyNetworks-t, Schnitzler-2020-SoKManagingData-h, Mondal-2019-MovingBeyondMedia-p}. These design ideas enable users to regulate disclosure dynamically in ways that better align with their evolving social contexts.

Lastly, platforms could move beyond one-size-fits-all approaches to accommodate different attitudes toward boundary regulation~\cite{Wisniewski-2015-GiveSocialWant-z, Stutzman2012-ev}. Given that privacy needs vary widely across users---some prioritize risk management, while others may be more concerned with relational dynamics~\cite{Page-2019-PragmaticToolFeatures-p, Wisniewski-2015-GiveSocialWant-z}---platforms could offer customizable privacy models, letting users choose different levels of privacy strictness depending on their current preferences~\cite{Colnago-2023-ThereReverseBehaviors-j}. For example, discretionary privacy features like a ``hide'' option (instead of blocking or unfriending) could reduce social friction~\cite{Page-2019-PragmaticToolFeatures-p}. Nudging mechanisms, such as gentle reminders to review audience settings, can encourage safer practices without disrupting user autonomy~\cite{Acquisti-2017-NudgesPrivacyOnline-y, Agha2023-dv, Agha2023-mu}. By recognizing and accommodating these individual differences, social media systems can provide more flexible, user-centered privacy solutions that support both nuanced relational concerns and pragmatic risk avoidance.