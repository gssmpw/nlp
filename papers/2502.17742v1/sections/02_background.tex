%!TEX root = 2024_auv_mola_drl6dof_main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
\label{sec:background}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Reinforcement learning
\subsection{Deep Reinforcement learning}

\ac{drl} \cite{RichardSutton20} is a method that aims to train an agent's policy $\pi$ to map states into actions by interacting with the environment. This is achieved by maximizing a numerical reward signal and using a \ac{mdp} framework to regulate the interaction between the \ac{rl} agentâ€™s policy and the environment. At each time step, the agent observes a state $\bm{s}$, takes an action $\bm{a}$, and upon transitioning to the next state, receives a reward $r$. Once the episode (i.e., process) is complete, the accumulated reward is calculated as the sum of all time steps rewards in that episode.

\ac{drl} methods can be model-based or model-free. Model-based methods use a model to predict the next state and reward, while model-free methods learn solely from experiencing the unmodeled and unknown consequences of an action. While learning from trial and error may result in less efficient learning, model-free methods have the advantage when a model is unavailable or inaccurate.

\begin{figure}[t!]
\centering
\includegraphics[width=0.45\textwidth]{figures/reward_vs_step.pdf}%
\caption{Average reward per episode over a moving window of 100 episodes obtained by the TQC, SAC, and TD3 algorithms during a $2.5\times10^6$ step training, equivalent to 3125 episodes.}
\label{fig:rewards}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 6DOF Error Computation
\subsection{\ac{6dof} Error Computation}

The position errors are determined by the difference between the current position $(x, y, z)$ and the goal position $(x_d, y_d, z_d)$ following the North-East-Down (NED) convention, computed as
%%%
% Keep to remove space between equations and paragraph
%%%
\begin{equation}
    e_x(t) = x^t - x_d^t,\; e_y(t) = y^t - y_d^t,\; e_z(t) = z^t - z_d^t.
\label{eq:errors}
\end{equation}

To compute the error in attitude, we will evaluate the difference between the current orientation and the goal attitude, both with respect to the fixed world frame. This involves representing both poses as rotation matrices ($\bm{R}\in SO(3)$) and converting their difference to exponential coordinates $[\bm{{e_\theta}}]\in so(3)$ through the matrix logarithm:
%%%
% Keep to remove space between equations and paragraph
%%%
\begin{equation}
     [\bm{{e_\theta}}(t)] = \log(\bm{R}(t)^T \cdot \bm{R}_d)
\end{equation}

Then, the skew-symmetric matrix $[\bm{{e_\theta}}(t)]$ is converted into its vector representation $\bm{{e_\theta}}(t) \in \mathbb{R}^3$, where its entries correspond to the element-wise error for the attitude, defined as
%%%
% Keep to remove space between equations and paragraph
%%%
\begin{equation}
    \begin{bmatrix} \theta_{x}^t & \theta_{y}^t & \theta_{z}^t \end{bmatrix} = \bm{{e_\theta}}(t).
    \label{eq:attitude_error}
\end{equation}

Furthermore, to provide a single metric for attitude error evaluation, we compute $\theta^t$ based on the axis-angle representation for $\bm{{e_\theta}}(t)$, as described in \eqref{eq:theta_error}. By using this metric, we obtain a global evaluation of orientation, which aligns the controller's performance with practical manual navigation comparisons.
%%%
% Keep to remove space between equations and paragraph
%%%
\begin{equation}
    \theta^t = ||\bm{{e_\theta}}(t)||
    \label{eq:theta_error}
\end{equation}
