%!TEX root = 2024_auv_mola_drl6dof_main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}
\label{sec:proposed_approach}

In this work, we introduce \acf{tqc-hp} and \acf{tqc-ea}: two end-to-end \ac{drl}-based approaches for the low-level control of a holonomic \ac{6dof} \ac{auv} using the \ac{tqc} algorithm. These methods require neither manual tuning nor prior knowledge of the thruster configuration.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Action Space
\subsection{Action space}

The action space $\bm{a}(t) \in \mathbb{R}^{8}$, normalized between $[-1, 1]$, is designed to enable the agent to generate precise control commands for the vehicle’s thrusters, ensuring the desired movement and orientation based on the given state inputs. The action space is defined as:
%%%
% Keep to remove space between equations and paragraph
%%%
\begin{equation}
\bm{a}(t) = \begin{bmatrix}
T_{1}^t & T_{2}^t & T_{3}^t & T_{4}^t & T_{5}^t & T_{6}^t & T_{7}^t & T_{8}^t
\end{bmatrix}^T,
\label{eq:action_space}
\end{equation}

\noindent where $\bm{a}(t)$ represents the normalized \ac{pwm} signals sent to the eight thrusters at time $t$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Observation Space
\subsection{Observation space}

The observation space $\bm{s}(t) \in \mathbb{R}^{20}$, normalized between $[-1, 1]$, is designed to capture all relevant environmental information necessary for the agent’s decision-making, enabling an understanding of the system’s current state relative to the desired goal. The observation space is defined as:
%%%
% Keep to remove space between equations and paragraph
%%%
\begin{equation}
\bm{s}(t) = \begin{bmatrix}
\bm{e}(t) & \bm{v}(t) & \bm{a}(t-1)
\end{bmatrix}^T,
\label{eq:observation_space}
\end{equation}

\noindent where $\bm{e}(t)=[e_{x}^t, e_{y}^t, e_{z}^t, \theta_{x}^t, \theta_{y}^t, \theta_{z}^t]^T$ represents the error vector at time $t$ between the current pose and the goal pose for each \ac{dof}, $ \bm{v}(t)=[v_{x}^t, v_{y}^t, v_{z}^t, \omega_{x}^t, \omega_{y}^t, \omega_{z}^t]^T$ corresponds to the spatial twist of the vehicle at time $t$, composed by the linear and angular velocities, and $\boldsymbol{a}(t-1)$ the action in the previous time step.

\begin{table}[b!]
\centering
\caption{Weights $\alpha_i$ for reward functions for \ac{tqc-hp} and \ac{tqc-ea}}
\label{tab:weights}
\begin{tabular}{ccccccccc}
\toprule
 & $\bm{\alpha_1}$ & $\bm{\alpha_2}$ & $\bm{\alpha_3}$ & $\bm{\alpha_4}$ & $\bm{\alpha_5}$ & $\bm{\alpha_6}$  \\ \midrule
\textbf{\ac{tqc-hp}} & $-4$ & $-4$ & $-3$ & $-1.8$ & $-1$ & $0$ \\
\textbf{\ac{tqc-ea}} & $-4$ & $-4$ & $-3$ & $-1.7$ & $-0.8$ & $-0.3$  \\ \bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Reward Function
\subsection{Reward function}

To evaluate policy performance and provide feedback during the training stage, an optimal reward function must be defined to enable the model to learn the desired behavior. In this work, we define the reward function \eqref{eq:reward_fx} that aims to bring the \ac{auv} closer to the target position in each of the \ac{6dof}, linearly penalizing the agent for positional \eqref{eq:reward_position} and angular \eqref{eq:reward_attitude} errors over time. Additionally, the reward function aims to generate smoother commands for the thrusters by penalizing signal fluctuations \eqref{eq:reward_smoothness} and to minimize the utilization of each thruster, thereby reducing the energy consumption of the \ac{auv} \eqref{eq:reward_power}. Each term is weighted with the parameter  $\alpha_i \leq 0$ to denote the importance of each term within \eqref{eq:reward_fx}.
%%%
% Keep to remove space between equations and paragraph
%%%
\begin{subequations}
\begin{align}
r(t) &= \sum\nolimits_{i=1}^{4} r_i(t) \label{eq:reward_fx} \\
r_1(t) &= \alpha_1 \cdot{|e_x^t|} + \alpha_2 \cdot{|e_y^t|} + \alpha_3 \cdot{|e_z^t|}\label{eq:reward_position} \\
r_2(t) &= \alpha_4 \cdot{|\theta^t|}
\label{eq:reward_attitude} \\
r_3(t) &= \sum\nolimits_{i=1}^{8} \alpha_5 \cdot \left|{T_{i}^t - T_{i}^{t-1}} \right| \label{eq:reward_smoothness} \\
r_4(t) &= \sum\nolimits_{i=1}^{8} \alpha_6 \cdot \left|{T_i^t} \right| \label{eq:reward_power}
\end{align}
\end{subequations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% DRL Algorithm
\subsection{Proposed \ac{drl} algorithms}

The proposed approaches are based on a \ac{drl} framework using a \ac{tqc} algorithm \cite{kuznetsov2020}. The selection of this algorithm was guided by an experiment conducted to compare the performance of the \ac{tqc}, \ac{sac}, and \ac{td3} algorithms in a \ac{6dof} \ac{auv} position control problem. The methodology described in Section \ref{sec:metodology} was used, and the three methods were trained using the same reward function to compare which of the aforementioned models obtained the highest average reward during the training episodes, indicating superior performance in the specific control task.

Fig. \ref{fig:rewards} shows the average reward of an episode over a moving window of 100 episodes obtained by the three \ac{drl} algorithms during training. It is evident that the \ac{tqc} algorithm converges in a smaller number of episodes compared to \ac{sac} and \ac{td3}, as well as reaching higher reward values than the other two algorithms. This experiment correlates with the results obtained by Lidtke et al. \cite{lidtke2024} for a 3-\acs{dof} \ac{auv} and confirms our decision to use the \ac{tqc} algorithm for the task of controlling an \ac{auv} in \ac{6dof}.

In this work, two different approaches of \ac{tqc} models are proposed: \ac{tqc-hp} aims to reach the goal position in the \ac{6dof} using smooth commands to the thrusters, i.e., its reward function includes the components $r_1$, $r_2$, and $r_3$, while \ac{tqc-ea} has the same objectives as \ac{tqc-hp} but adds energy-awareness, minimizing the energy consumption of the \ac{auv} by incorporating the $r_4$ component in its reward function. Table \ref{tab:weights} presents the $\alpha_i$ weights used in the reward function for both approaches. Both approaches are implemented in \textit{Python3} using the \textit{Stable Baselines3} library, which is based on principles used in both \ac{sac} and \ac{td3} algorithms but utilizes the distributed representation of a critic, truncation of critic predictions, and a set of multiple critics.
