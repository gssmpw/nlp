@article{barocas2016,
    title={Big Data's Disparate Impact},
    author={Barocas, S. and Selbst, A. D.},
    journal={California Law Review},
    volume={104},
    page={671},
    year={2016}
}

@article{chouldechova2017,
    title={Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
    author={Chouldechova, A.},
    journal={Big Data},
    volume={5},
    number={2},
    pages={153--163},
    year={2017}
}

@article{corbettdavies2018,
    title={The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning},
    author={Corbett-Davies, S. and Goel, S.},
    journal={arXiv preprint arXiv:1808.00023},
    year={2018}
}

@inproceedings{dwork2012,
    title={Fairness Through Awareness},
    author={Dwork, C. and Hardt, M. and Pitassi, T. and Reingold, O. and Zemel, R.},
    booktitle={Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
    pages={214--226},
    year={2012},
    organization={ACM},
    address={Cambridge, Massachusetts}
}

@inproceedings{feldman2015,
    title={Certifying and Removing Disparate Impact},
    author={Feldman, M. and Friedler, S. A. and Moeller, J. and Scheidegger, C. and Venkatasubramanian, S.},
    booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages={259--268},
    year={2015},
    organization={ACM},
    address={Sydney, NSW, Australia}
}

@inproceedings{gitiaux2021,
  title={Fair Representations by Compression},
  author={Gitiaux, Xavier and Rangwala, Huzefa},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={13},
  pages={11506--11515},
  year={2021},
  doi={https://doi.org/10.1609/aaai.v35i13.17370}
}

@inproceedings{grgichlaca2018,
    title={Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning},
    author={Grgić-Hlača, N. and Zafar, M. B. and Gummadi, K. P. and Weller, A.},
    booktitle={AAAI},
    pages={51--60},
    year={2018}
}

@inproceedings{hardt2016,
    title={Equality of Opportunity in Supervised Learning},
    author={Hardt, M. and Price, E. and Srebro, N.},
    booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems},
    pages={3315--3323},
    year={2016},
    organization={Curran Associates Inc.},
    address={Barcelona, Spain}
}

@article{hardt2016equality,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{louizos2016,
  title={The Variational Fair Autoencoder},
  author={Louizos, Christos and Swersky, Kevin and Li, Yujia and Welling, Max and Zemel, Richard},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@inproceedings{madras2018,
  title={Learning adversarially fair and transferable representations},
  author={Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
  booktitle={International Conference on Machine Learning},
  pages={3381--3390},
  year={2018}
}

@article{quan2022,
title = {Learning fair representations by separating the relevance of potential information},
journal = {Information Processing \& Management},
volume = {59},
number = {6},
pages = {103103},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103103},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322002047},
author = {Tangkun Quan and Fei Zhu and Xinghong Ling and Quan Liu},
keywords = {Fair representation, Fair classification, Interpretability of representations, Fair machine learning},
abstract = {Representation learning has recently been used to remove sensitive information from data and improve the fairness of machine learning algorithms in social applications. However, previous works that used neural networks are opaque and poorly interpretable, as it is difficult to intuitively determine the independence between representations and sensitive information. The internal correlation among data features has not been fully discussed, and it may be the key to improving the interpretability of neural networks. A novel fair representation algorithm referred to as FRC is proposed from this conjecture. It indicates how representations independent of multiple sensitive attributes can be learned by applying specific correlation constraints on representation dimensions. Specifically, dimensions of the representation and sensitive attributes are treated as statistical variables. The representation variables are divided into two parts related to and unrelated to the sensitive variables by adjusting their absolute correlation coefficient with sensitive variables. The potential impact of sensitive information on representations is concentrated in the related part. The unrelated part of the representation can be used in downstream tasks to yield fair results. FRC takes the correlation between dimensions as the key to solving the problem of fair representation. Empirical results show that our representations enhance the ability of neural networks to show fairness and achieve better fairness-accuracy tradeoffs than state-of-the-art works.}
}

@article{quan2023,
title = {Learning fair representations for accuracy parity},
journal = {Engineering Applications of Artificial Intelligence},
volume = {119},
pages = {105819},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.105819},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623000039},
author = {Tangkun Quan and Fei Zhu and Quan Liu and Fanzhang Li},
keywords = {Fair machine learning, Accuracy parity, Fair measures, Fair classification},
abstract = {With the application of artificial intelligence in various fields of society, algorithm fairness has become a concern in social decision-making. Researchers have proposed various fair algorithms to achieve statistical parity and equal opportunity of outputs. However, the accuracy parity of algorithms has not been fully discussed. Existing fair algorithms either ignore the accuracy parity or sacrifice the joint accuracy of the model while achieving accuracy parity. A novel fairness algorithm referred to as balanced fair representation for accuracy parity (BFA) is proposed to reduce the bias on the accuracy among sensitive attribute groups while maintaining their joint accuracy. BFA uses representation learning methods to learn similar representations from different sensitive attribute groups. The learned representations are used as training data to learn a fair classification model. The classification error of the model is constrained to be independent of the sensitive attributes. To obtain similar representations, BFA calculates the characteristic scores of different sensitive attribute groups, respectively. BFA takes the difference between scores of different sensitive attribute groups from the same label as the constraint objective of learning the representation. To achieve accuracy parity, BFA calculates the cross entropy loss of neural networks from the learned similar representations and reduces the correlation between the loss and sensitive information. Empirical results show that the proposed representations provide a better trade-off between accuracy parity and joint accuracy than state-of-the-art works.}
}

@inproceedings{zhao2020,
  title={Conditional Learning of Fair Representations},
  author={Zhao, Han and Coston, Amanda and Adel, Tameem and Gordon, Geoffrey J.},
  booktitle={8th International Conference on Learning Representations},
  pages={1--17},
  year={2020}
}

