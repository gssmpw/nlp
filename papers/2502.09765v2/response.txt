\section{Background and Related Work}
\label{sec:background}
Below we will first discuss modern adversarial techniques for debiasing. We will then cover the non-adversarial techniques which are more closely related to this paper. This is followed by a formalisation of the various definitions of fairness found in the literature.

\subsection{Adversarial Debiasing}

Adversarially Learned Fair Representations, ALFR (Edwards and Storkey, 2016), is one of the earliest models developed to reduce bias and learn fair informative representations. It utilises an adversary which tries to predict the sensitive feature from the representations. The autoencoder tries to make these predictions as difficult as possible. It employs task and reconstruction losses to ensure the representation are informative for the downstream task, and an additional sensitive feature loss to remove sensitive feature related information.

The Conditional Fair Representations, CFair, approach Hardt et al., "Equality of Opportunity in Supervised Learning" stands as a seminal method aiming for accuracy parity. Within the confines of conventional fair adversarial networks, CFair augments the original adversarial constraints and adopts conditional error constraints. 
%The objective function for \textit{CFair} is expressed as:
%\begin{align}
% \mathcal{L} = &\min_{h, g} \max_{h', h''} \text{BER}_{\mathcal{D}}(h(g(X)) || Y) \notag \\
% &- \lambda \left( \text{BER}_{\mathcal{D}_0}(h'(g(X)) || S) + \text{BER}_{\mathcal{D}_1}(h''(g(X)) || S) \right)
% \end{align}
%where \(\text{BER}\) signifies the Balanced Error Rate (Menon and Williamson, 2018), while \(\mathcal{D}_k\) represents instances associated with the sensitive attribute \(k\).

Learning Adversarially Fair and Transferable Representation, LAFTR (Madras et al., "Fairness through Awareness" and Zhang et al., "Mitigating Unwanted Biases with Adversarial Learning"), is similar to CFAIR but it uses one adversary instead of two and an L1 instead of a cross entropy loss to debias the representations. It still utilises a global cross-entropy loss for the target variable.


\subsection{Non-adversarial Debiasing}
Outside of adversarial learning, Fairness by Compression (FBC) (Menon et al., "Optimizing F1 Score via Logit Adjustment on Imbalanced Text Classification") advocates for the use of binary compression to mitigate sensitive elements in representations. They establish that the cross-entropy between \( P(z) \) and \( Q \) stands as the upper bound for the entropy \( H(z) \). In this context, \( P(z) \) delineates the distribution of a factorized representation, and \( Q \) is utilized to predict \( z_i \) based on \( \{ z_0, z_1, \dots , z_{i-1} \} \). The FRC model (Hardt et al., "Equalization of Opportunity") aims to mitigate the influence of sensitive factors in the data representation by adjusting the correlation between the representation and the sensitive vectors.
% Their proposed objective function is
% \vspace{-0.1cm}\begin{equation}
% \mathcal{L} = \min_{z} \mathop{\mathbb{E}}_{x,z,a}[-\log(P(x|z, a))] + \beta \textit{CE}(P, Q)
% \vspace{-0.2cm}\end{equation}
%where \( \textit{CE} \) signifies the cross-entropy function, and \( \beta \) modulates the compression rate.


% The objective function for FRC is formulated as
% \begin{align}
%     \mathcal{L} = &\mathop{\mathbb{E}}_{q(Z|X)}[\log p(X|Z)] - \beta \text{DKL}(q(Z|X) || p(Z))\notag \\ 
%     & + \gamma (\text{Corr}(S, Z_P) - \text{Corr}(S, Z_N))
% \end{align}
%where \(Z_P\) and \(Z_N\) denote the representation vectors that are respectively associated and dissociated with sensitive information.

BFA (Shimodaira et al., "Covariate Shift by Kernel Mean Matching Under Various Definitions of Fairness") draws inspiration from the correlation coefficient constraints used in FRC. The primary ambition is to minimize the correlation between sensitive information and prediction error (as opposed to minimising correlation between sensitive information and representations in FRC), aiming to maintain high predictive accuracy.

The Variational Fair Autoencoder, VFAE (Mescheder et al., "Adversarial Variational Bayes: Unifying Variance-Based and Minimum-Probability-Framework Approaches to Generative Models"), comprises of a variational autoencoder (VAE) instead of an autoencoder and employs an addititional maximum mean discrepancy (MMD) loss to ensure less sensitive information, which may be correlated to the target task, leaks into the learned representation. MMD minimises the mismatch in moments between the marginal posterior distributions for the different sensitive features.

These techniques exhibit greatly improved training stability compared to the adversarial training approaches. However, they struggle to achieve good task performance (i.e. developing useful representations). It can be observed that many of these techniques experience a degeneracy where the loss function can be satisfied by performing equally poorly across all sensitive domains. In contrast our proposed approach maintains the benefits of non-adversarial training, while removing this degeneracy.

\subsection{Measures of Fairness}
Fairness in machine learning has been extensively studied, and there exist a range of metrics which measure different aspects of it. Our primary contribution is the introduction of a new differentiable fairness metric, thus necessitating a brief overview of commonly used metrics in our model evaluation. %Thus we will first briefly formalize the metrics that are commonly used in the field and that we will use in the evaluation of our model.
%These metrics include demographic parity, equalized odds, equal opportunity, and disparate impact.

Demographic parity, also known as statistical parity or group fairness, requires that the selection rate (the rate at which individuals are positively classified) should be the same across all demographic groups. Mathematically, if \( Y \) is the predicted label and \( A \) denotes the demographic group, demographic parity is defined as:
%
\vspace{-0.1cm}\begin{equation}
P(Y=1|A=0) = P(Y=1|A=1).
\end{equation}
%While these metrics provide valuable tools for assessing fairness, they are not panaceas. There are trade-offs and tensions among them, and it is generally impossible to satisfy all these conditions simultaneously when the base rates differ across groups ____. Thus, the selection of appropriate fairness metrics depends heavily on the specific context and ethical considerations.

%When assessing the fairness of machine learning models, these metrics primarily focus on ensuring parity in prediction outcomes across different demographic groups. These metrics do not explicitly take into consideration the downstream task's classification accuracy ____.

%Specifically, demographic parity and disparate impact concentrate on the balanced positive prediction rates across various groups, disregarding the actual labels of the instances. Similarly, equalized odds and equal opportunity are concerned with similar true positive rates or error rates across groups but do not explicitly account for overall accuracy. Therefore, while these metrics ensure that models do not disproportionately disadvantage any particular group, they may do so at the cost of reducing the model's overall predictive performance ____.

Recognizing the need to balance accuracy and fairness, there's a growing emphasis on metrics that integrate classification performance, creating more robust and equitable machine learning systems suitable for real-world applications ____. This unified approach fosters a comprehensive evaluation and comparison of debiasing models, ensuring effectiveness in predictions while maintaining fairness.

%In the real world, accuracy and fairness are both critically important. Decisions based on machine learning models often have significant consequences, such as granting loans, admitting students to universities, or diagnosing diseases. In these situations, sacrificing accuracy can lead to suboptimal outcomes, while disregarding fairness can perpetuate systemic bias.

%Thus, there is a growing need for a unified metric that balances both classification accuracy and fairness. This approach allows for a more comprehensive comparison and evaluation of debiasing models, ensuring not just fairness but also effectiveness in prediction. By incorporating classification accuracy into fairness metrics, we can build more robust and equitable machine learning systems that are better suited for real-world applications ____