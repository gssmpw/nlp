\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[preprint]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}



\title{Differential Adjusted Parity for Learning Fair Representations}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Bucher Sahyouni\\
  Centre for Vision, Speech and Signal Processing\\
  University of Surrey\\
  Guildford, Surrey GU2 7XH\\
  \texttt{bs00826@surrey.ac.uk} \\
  % examples of more authors
  \And
  Matthew Vowels \\
  University of Lusanne \\
  CH-1015 Lausanne  \\
  \texttt{matthew.vowels@unil.ch} \\
  \AND
  Liqun Chen \\
  Unviersity of Surrey \\
  Guildford, Surrey GU2 7XH \\
  \texttt{liqun.chen@surrey.ac.uk} \\
  \And
  Simon Hadfield \\
  Centre for Vision, Speech and Signal Processing\\
  University of Surrey\\
  Guildford, Surrey GU2 7XH \\
  \texttt{s.hadfield@surrey.ac.uk}\\
}


\begin{document}


\maketitle


\begin{abstract}
The development of fair and unbiased machine learning models remains an ongoing objective for researchers in the field of artificial intelligence. We introduce the Differential Adjusted Parity (DAP) loss to produce unbiased informative representations. It utilises a differentiable variant of the adjusted parity metric to create a unified objective function. By combining downstream task classification accuracy and its inconsistency across sensitive feature domains, it provides a single tool to increase performance and mitigate bias. A key element in this approach is the use of soft balanced accuracies. In contrast to previous non-adversarial approaches, DAP does not suffer a degeneracy where the metric is satisfied by performing equally poorly across all sensitive domains. It outperforms several adversarial models on downstream task accuracy and fairness in our analysis. Specifically, it improves the demographic parity, equalized odds and sensitive feature accuracy by as much as 22.5\%, 44.1\% and 40.1\%, respectively, when compared to the best performing adversarial approaches on these metrics. Overall, the DAP loss and its associated metric can play a significant role in creating more fair machine learning models.
\end{abstract}

\section{Introduction} \noindent As artificial intelligence (AI) and machine learning (ML) models become increasingly prevalent, the need for responsible, fair, and unbiased machine learning is critical \cite{barocas2016}. Many machine learning models heavily rely on learned representations which distill complex data into compressed forms, capturing key patterns for efficient learning and prediction \cite{bengio2013}. However, they may also encode sensitive attributes like gender or race, thus leading to biased decisions that can perpetuate societal inequities. 

The recognition of these biases has spurred the development of debiasing techniques. These methods aim to remove sensitive information while maintaining task performance, a balance that is challenging to achieve. Among various approaches, adversarial training has become popular, introducing an adversarial model during training to learn sensitive features. This process, often a min-max game, where the main model competes with the adversarial model making sensitive features more difficult to learn while focusing on the main task \cite{zhang2018}, is conceptually appealing but unstable and computationally intensive, requiring careful hyperparameter tuning and rigorous experimentation \cite{arjovsky2017}.

In response to these challenges, this paper introduces a new approach to debiasing. We propose a differentiable variant of the adjusted parity metric as a single objective function that considers both task classification accuracy and its invariance across sensitive feature domains, without an adversarial componenet. This approach simplifies the learning process by focusing on cooperative elements of the loss that improve performance across protected characteristics. Our technique extends to multi-class problems, ensuring fairness across various sensitive features. Our approach aims to simplify the debiasing process and enhance the stability during training, contributing to the ongoing efforts to foster ethical AI \cite{crawford2016}.

The rest of the paper is structured as follows: subsequent sections cover popular debiasing techniques, the adjusted parity metric, our experimental methodology, and a discussion of the results, concluding with insights into potential future work.

%\noindent As artificial intelligence (AI) and machine learning (ML) models permeate our lives in increasingly intricate ways, the call for responsible, fair, and unbiased machine learning has never been more critical \cite{barocas2016}. These models often rely on learned representations.

%These representations form the foundation of many ML models. They distill complex and high-dimensional raw data into a more manageable form, capturing essential patterns and structures that facilitate efficient learning and accurate prediction \cite{bengio2013}. However, this distillation process may inadvertently encode sensitive information such as gender, race, or other protected attributes. The retention of these sensitive features within models can lead to biased decisions, which, in turn, can perpetuate societal inequities and reinforce harmful stereotypes.


%The recognition of these biases embedded in machine learning systems has spurred the development of various debiasing techniques. These models attempt to remove sensitive information from the representations while ensuring the learned representations are as informative as possible to maintain overall task performance. Yet, striking this balance remains a significant challenge. Current debiasing techniques often involve a trade-off between the removal of bias and the preservation of useful, task-specific information \cite{zhang2018}.


%Among the various techniques proposed, adversarial training has emerged as a popular debiasing approach. Here, an adversarial model is introduced during training to specifically learn the sensitive features. The main model competes with this adversarial model, making sensitive features difficult to learn while focusing on the main task, which could involve the reconstruction of the original input data or the maximisation of classification accuracy for some downstream task \cite{zhang2018}. Adversarial training is a dynamic process, often viewed as a min-max game. 
%The model is continually trying to minimize its loss by incorporating as much of the input information (or information relevant for the downstream task), while the adversarial component is trying to remove sensitive information which may be relevant for the reconstruction or downstream task classification. The adversarial network is essentially ``attacking'' the main network, finding and exploiting weaknesses in its learning.
%This tension can lead to instability during the training process, making it difficult to achieve convergence. The balance between the two competing goals can often lead to a situation known as ``mode collapse'', where the generator network is only able to produce limited varieties of samples.
%Conceptually appealing as it may be, adversarial training is often unstable, posing difficulties for optimization. In particular, it is susceptible to the problem of vanishing or exploding gradients. This can either stall the learning process or cause it to diverge.
%It is also computationally expensive, requiring more time and resources compared to traditional training methods
%, which can be a hindrance, especially when working with large and complex datasets .
%Finally, the delicate nature of adversarial training demands careful hyperparameter tuning and rigorous experimentation, which can be labor-intensive and time-consuming. This often leads to a complex and fragile learning process, requiring meticulous tuning and monitoring.
%, and achieving the right equilibrium is not an easy task

%In light of these challenges, this paper introduces a new approach to debiasing. We propose a differentiable variant of the adjusted parity metric (Vowels et al., 2020) as a single objective function that simultaneously considers downstream task classification accuracy and its invariance across sensitive features. 
%Importantly there is no adversarial objective. All elements of the loss co-operatively improve performance on the downstream task, with their influence weighted adaptively across the protected characteristics.
%As such, we no longer need to find a stable point between competing objectives.
%This single, unifying metric provides a clear and robust criterion for comparing different debiasing models, and adds a valuable tool for evaluating fairness in AI models.

%Additionally, in this work, we provide a generalisation of the adjusted parity metric to multi-class problems. Rather than confining our technique to ensure fairness across binary protected characteristics, we also include sensitive categorical features with multiple domains.
%By doing so, we evolve the metric to function effectively in a more complex, multi-dimensional space.

%Building on the foundation of the adjusted parity metric, we further develop a novel differentiable loss function for debiasing latent representations. Designed minimise the downstream task accuracy invariance while maintaining downstream task performance, our method diverges from the adversarial path, thereby offering more stable training and simpler model architecture.

%The promise of our work lies in its practicality and potentially broad impact. By presenting a model that effectively debiases while remaining stable throughout training, we offer a contribution to the ongoing discussion on ethical AI. This work is in line with the growing emphasis on transparency, fairness, and accountability in AI\cite{crawford2016}, reflecting the pressing need to mitigate the societal implications of biased AI systems.

%The rest of the paper is structured as follows: Section \ref{sec:background} covers some popular debiasing techniques and fairness metrics. Section \ref{sec:AP} provides a detailed exposition of the adjusted parity metric and the corresponding differentiable loss function is described in Section \ref{sec:DAP}. The experimental methodology used to validate our approach is detailed in Section \ref{sec:experiments}, followed by a discussion of the results in Section \ref{sec:results}. We then wrap up with Section \ref{sec:conclusion}, summarizing our findings and offering insights into potential future work.

\section{Background and Related Work}
\label{sec:background}
Below we will first discuss modern adversarial techniques for debiasing. We will then cover the non-adversarial techniques which are more closely related to this paper. This is followed by a formalisation of the various definitions of fairness found in the literature.

\subsection{Adversarial Debiasing}

Adversarially Learned Fair Representations, ALFR (Edwards and Storkey, 2016), is one of the earliest models developed to reduce bias and learn fair informative representations. It utilises an adversary which tries to predict the sensitive feature from the representations. The autoencoder tries to make these predictions as difficult as possible. It employs task and reconstruction losses to ensure the representation are informative for the downstream task, and an additional sensitive feature loss to remove sensitive feature related information.

The Conditional Fair Representations, CFair, approach \cite{zhao2020} stands as a seminal method aiming for accuracy parity. Within the confines of conventional fair adversarial networks, CFair augments the original adversarial constraints and adopts conditional error constraints. 
%The objective function for \textit{CFair} is expressed as:
%\begin{align}
% \mathcal{L} = &\min_{h, g} \max_{h', h''} \text{BER}_{\mathcal{D}}(h(g(X)) || Y) \notag \\
% &- \lambda \left( \text{BER}_{\mathcal{D}_0}(h'(g(X)) || S) + \text{BER}_{\mathcal{D}_1}(h''(g(X)) || S) \right)
% \end{align}
%where \(\text{BER}\) signifies the Balanced Error Rate (Menon and Williamson, 2018), while \(\mathcal{D}_k\) represents instances associated with the sensitive attribute \(k\).

Learning Adversarially Fair and Transferable Representation, LAFTR \cite{madras2018}, is similar to CFAIR but it uses one adversary instead of two and an L1 instead of a cross entropy loss to debias the representations. It still utilises a global cross-entropy loss for the target variable.


\subsection{Non-adversarial Debiasing}
Outside of adversarial learning, Fairness by Compression (FBC) \cite{gitiaux2021} advocates for the use of binary compression to mitigate sensitive elements in representations. They establish that the cross-entropy between \( P(z) \) and \( Q \) stands as the upper bound for the entropy \( H(z) \). In this context, \( P(z) \) delineates the distribution of a factorized representation, and \( Q \) is utilized to predict \( z_i \) based on \( \{ z_0, z_1, \dots , z_{i-1} \} \). The FRC model \cite{quan2022} aims to mitigate the influence of sensitive factors in the data representation by adjusting the correlation between the representation and the sensitive vectors.
% Their proposed objective function is
% \vspace{-0.1cm}\begin{equation}
% \mathcal{L} = \min_{z} \mathop{\mathbb{E}}_{x,z,a}[-\log(P(x|z, a))] + \beta \textit{CE}(P, Q)
% \vspace{-0.2cm}\end{equation}
%where \( \textit{CE} \) signifies the cross-entropy function, and \( \beta \) modulates the compression rate.


% The objective function for FRC is formulated as
% \begin{align}
%     \mathcal{L} = &\mathop{\mathbb{E}}_{q(Z|X)}[\log p(X|Z)] - \beta \text{DKL}(q(Z|X) || p(Z))\notag \\ 
%     & + \gamma (\text{Corr}(S, Z_P) - \text{Corr}(S, Z_N))
% \end{align}
%where \(Z_P\) and \(Z_N\) denote the representation vectors that are respectively associated and dissociated with sensitive information.

BFA \cite{quan2023} draws inspiration from the correlation coefficient constraints used in FRC. The primary ambition is to minimize the correlation between sensitive information and prediction error (as opposed to minimising correlation between sensitive information and representations in FRC), aiming to maintain high predictive accuracy.

The Variational Fair Autoencoder, VFAE \cite{louizos2016}, comprises of a variational autoencoder (VAE) instead of an autoencoder and employs an addititional maximum mean discrepancy (MMD) loss to ensure less sensitive information, which may be correlated to the target task, leaks into the learned representation. MMD minimises the mismatch in moments between the marginal posterior distributions for the different sensitive features.

These techniques exhibit greatly improved training stability compared to the adversarial training approaches. However, they struggle to achieve good task performance (i.e. developing useful representations). It can be observed that many of these techniques experience a degeneracy where the loss function can be satisfied by performing equally poorly across all sensitive domains. In contrast our proposed approach maintains the benefits of non-adversarial training, while removing this degeneracy.

\subsection{Measures of Fairness}
Fairness in machine learning has been extensively studied, and there exist a range of metrics which measure different aspects of it. Our primary contribution is the introduction of a new differentiable fairness metric, thus necessitating a brief overview of commonly used metrics in our model evaluation. %Thus we will first briefly formalize the metrics that are commonly used in the field and that we will use in the evaluation of our model.
%These metrics include demographic parity, equalized odds, equal opportunity, and disparate impact.

Demographic parity, also known as statistical parity or group fairness, requires that the selection rate (the rate at which individuals are positively classified) should be the same across all demographic groups. Mathematically, if \( Y \) is the predicted label and \( A \) denotes the demographic group, demographic parity is defined as:
%
\vspace{-0.1cm}\begin{equation}
P(Y=1|A=0) = P(Y=1|A=1).
\end{equation}
%
This implies that the algorithm should be independent of the sensitive attribute \( A \) \cite{dwork2012}, which can be limiting if the attribute is relevant to the outcome \cite{hardt2016equality}.% However, demographic parity can be limiting in situations where the sensitive attribute is indeed relevant to the outcome. This and other flaws are discussed in detail in the seminal work \cite{hardt2016equality}.

The equalized odds fairness metric demands true positive rates and false positive rates to be equal across demographic groups. Mathematically, if $\hat{Y}$ is the true label:
%
\vspace{-0.1cm}\begin{equation}
P(Y\!=\!1|\hat{Y}\!=\!1, A\!=\!0) \!=\! P(Y\!=\!1|\hat{Y}\!=\!1, A\!=\!1)
\end{equation}
%
and
%
\vspace{-0.1cm}\begin{equation}
P(Y\!=\!1|\hat{Y}\!=\!0, A\!=\!0) \!=\! P(Y\!=\!1|\hat{Y}\!=\!0, A\!=\!1).
\end{equation}
%
Equalized odds aims for outcome independence across demographic groups when conditioned on the true label \cite{hardt2016}. %This means the classifier’s outcomes, both in terms of correctly identified positive cases and incorrectly identified negative cases, should be independent of the demographic group when conditioned on the true label \cite{hardt2016}. However, maintaining equalized odds can often be challenging in the presence of significant imbalances in the underlying data distributions among different groups.

%The equal opportunity metric, a subset of equalized odds, solely focuses on the true positive rates. In essence, equal opportunity requires the algorithm to have similar true positive rates across different demographic groups. This is mathematically represented as:
%
%\vspace{-0.1cm}\begin{equation}
%P(Y\!=\!1|\hat{Y}\!=\!1, A\!=\!0) \!=\! P(Y\!=\!1|\hat{Y}\!=\!1, A\!=\!1).
%\vspace{-0.2cm}\end{equation}
%
%Equal opportunity thus ensures that the proportion of correctly classified positive outcomes is the same across all demographic groups \cite{barocas2016}.

%Disparate impact refers to a scenario where a decision, rule, or policy has a discriminatory effect on a protected class, even though the explicit rules appear to be neutral. In terms of machine learning, disparate impact can be quantified as the ratio of the probabilities of positive outcomes in the protected group to that in the non-protected group. Thus, disparate impact is defined as:
%
%\vspace{-0.1cm}\begin{equation}
%\frac{P(Y=1|A=1)}{P(Y=1|A=0)}.
%\vspace{-0.2cm}\end{equation}
%
%A value of 1 indicates no disparate impact, and U.S. legal guidelines suggest that a value below 0.8 may be indicative of significant disparate impact \cite{feldman2015}.

These metrics, while essential for assessing fairness, have inherent trade-offs and limitations. It is generally impossible to satisfy all these conditions simultaneously when the base rates differ across groups \cite{chouldechova2017}. They focus on ensuring parity in predictions without necessarily considering the impact on overall task accuracy \cite{dwork2012,hardt2016equality}.

%While these metrics provide valuable tools for assessing fairness, they are not panaceas. There are trade-offs and tensions among them, and it is generally impossible to satisfy all these conditions simultaneously when the base rates differ across groups \cite{chouldechova2017}. Thus, the selection of appropriate fairness metrics depends heavily on the specific context and ethical considerations.

%When assessing the fairness of machine learning models, these metrics primarily focus on ensuring parity in prediction outcomes across different demographic groups. These metrics do not explicitly take into consideration the downstream task's classification accuracy \cite{dwork2012,hardt2016equality}.

%Specifically, demographic parity and disparate impact concentrate on the balanced positive prediction rates across various groups, disregarding the actual labels of the instances. Similarly, equalized odds and equal opportunity are concerned with similar true positive rates or error rates across groups but do not explicitly account for overall accuracy. Therefore, while these metrics ensure that models do not disproportionately disadvantage any particular group, they may do so at the cost of reducing the model's overall predictive performance \cite{corbettdavies2018}.

Recognizing the need to balance accuracy and fairness, there's a growing emphasis on metrics that integrate classification performance, creating more robust and equitable machine learning systems suitable for real-world applications \cite{grgichlaca2018}. This unified approach fosters a comprehensive evaluation and comparison of debiasing models, ensuring effectiveness in predictions while maintaining fairness.

%In the real world, accuracy and fairness are both critically important. Decisions based on machine learning models often have significant consequences, such as granting loans, admitting students to universities, or diagnosing diseases. In these situations, sacrificing accuracy can lead to suboptimal outcomes, while disregarding fairness can perpetuate systemic bias.

%Thus, there is a growing need for a unified metric that balances both classification accuracy and fairness. This approach allows for a more comprehensive comparison and evaluation of debiasing models, ensuring not just fairness but also effectiveness in prediction. By incorporating classification accuracy into fairness metrics, we can build more robust and equitable machine learning systems that are better suited for real-world applications \cite{grgichlaca2018}.


\section{The Adjusted Parity Metric}
\label{sec:AP}
As an initial step towards solving these issues, \cite{vowels2020} introduced a parity metric for evaluating domain invariance. This metric accommodates both discrepancies in accuracy across domains and normalised classifier or regressor performance to provide a single unified value for comparing debiasing models.

The adjusted parity metric was originally expressed for binary domains as:
\vspace{-0.1cm}\begin{equation}
    \Delta_{\text{adj}} = \bar{S} (1 - 2\sigma),
\end{equation}
where \( \sigma \) represents the standard deviation of the normalised classifier accuracy across the domains,  and\( \bar{S} \) denotes the average accuracy over the domains.

We extend this definition to an arbitrary number of domains:
\vspace{-0.1cm}\begin{equation}
    \Delta_{\text{adj}} = \frac{\bar{S}-S^R}{1-S^R}\left(1 - \frac{\sigma}{\gamma}\right),
\label{eq:diffpar2}
\end{equation}
where $S^R$ is the baseline accuracy of a random predictor, and $\gamma$ is the maximum standard deviation across domains. This serves to normalise the metric between [0,1].
%To illustrate, consider a scenario where there's an equal chance of predicting any of the 10 digits in the MNIST dataset by random chance. Here, the baseline would be 0.1.

The introduction of $\gamma$ in this paper extends the metric to sensitive characteristics with more than two domains. For any even number of domains ($N$) the value remains at $\gamma=0.5$ as in the original formulation. However for an odd number of domains: 
\vspace{-0.1cm}\begin{equation}
    \gamma = \sqrt{\frac{1}{4} \left(1 - \frac{1}{N^2}\right)}.
    \label{eq:gamma}
\end{equation}
Please see the appendix for a full derivation.

The implications of this metric are twofold. Firstly, any classifier that exhibits either minimal consistency or accuracy will yield \(\Delta_{\text{adj}} = 0\). Conversely, only a classifier that demonstrates maximal consistency and accuracy will result in \(\Delta_{\text{adj}} = 1\). Figure \ref{fig6} in Appendix shows how \(\Delta_{\text{adj}}\) changes with $\overline{S}$ for various $\sigma$. The motivation behind developing this metric stems from an essential understanding that invariance to a domain or attribute does not necessarily equate to reliable classification. A representation must also be informative for the intended task to be deemed effective. 

% The adjusted parity metric requires that \(1-2\sigma = 0\) when maximal inconsistency in normalised accuracy is observed across domains, and similarly \(1-2\sigma = 1\) for the case of minimal inconsistency. The maximum standard deviation for \(N\) classes should be calculated such that this criterion is met.

% The standard deviation of values, \( \sigma \), is defined as:
% \vspace{-0.1cm}\begin{equation}
%     \sigma = \sqrt{\frac{1}{N}\sum_{S \in \mathcal{S}} (S - \bar{S})^2}
% \vspace{-0.2cm}\end{equation}
% where \( \bar{S} \) is the mean of the values in \( \mathcal{S} \), representing the mean of the accuracies.

% The formulation for the maximum standard deviation, \( \sigma^* \), is:
% \begin{align}
%     \sigma^* &= \arg \max_{\mathcal{S} \in \mathbb{R}^N} \sqrt{\frac{1}{N}\sum_{S \in \mathcal{S}} (S - \bar{S})^2} \\
%     & \text{s.t. } S \in [\alpha..\omega]
% \end{align}

% The derivation for \( \sigma^* \) is grounded on the idea that for bounded values, the mean \( \bar{S} \) should be as centered as possible within the range, while all the values are on the extremes of the range. When \( N \) is even, this can be perfectly achieved by placing half of the items at each end. In such a scenario, \( \bar{S} = \sigma = \frac{\alpha + \omega}{2} \). Hence, the maximum standard deviation for an even number of classes is 0.5.

% However, for odd \( N \), the mean has to be slightly off-center from the range, leading to a lower possible \( \sigma^* \). Considering that \( \mathrm{floor}(N/2) \) items are valued at \( \alpha \) and \( \mathrm{ceil}(N/2) \) items are at \( \omega \), the mean can be derived as:
% \vspace{-0.1cm}\begin{equation}
%     \bar{S} = \frac{\alpha\frac{N-1}{2} + \omega\frac{N+1}{2}}{N}
% \vspace{-0.2cm}\end{equation}

% Given that the minimum normalized accuracy, \( \alpha \), is 0 and the maximum normalized accuracy, \( \omega \), is 1, the simplified mean is:
% \begin{align}
%     \bar{S} &= \frac{1+\frac{1}{N}}{2}
%     &= \frac{1}{2} + \frac{1}{2N}
% \end{align}
% Consequently, as \( N \to \infty \), \( \bar{S} \) approaches 0.5.

% The maximum standard deviation for odd \( N \) classes can be expressed as:
% \vspace{-0.1cm}\begin{equation}
%     \sigma^* = \sqrt{\frac{1}{4} \left(1 - \frac{1}{N^2}\right)}
% \vspace{-0.2cm}\end{equation}

% Consequently, the adjusted parity metric for odd \( N \) classes is:
% \vspace{-0.1cm}\begin{equation}
%     \Delta_{\text{adj}} = \bar{S} (1 - \gamma \sigma)
% \vspace{-0.2cm}\end{equation}
% with \( \gamma = \frac{1}{\sigma^*} \).

% From this definition, it is obvious that:
% \vspace{-0.1cm}\begin{equation}
%     \lim_{N \to \infty} \bar{S} = \frac{1}{2}
% \vspace{-0.2cm}\end{equation}

\section{Differential Adjusted Parity}
\label{sec:DAP}
%\subsection{Soft Accuracies}
In order to propose a differentiable variant of the adjusted parity metric from equation \ref{eq:diffpar2}, we must first rely on a differentiable variant of the accuracy measure $S$.
%The conventional accuracy of a classifier is a widely used evaluation metric, but it suffers from two major issues: it is non-differentiable, and it can yield misleading results when the classes are imbalanced. This necessitates the use 
In this paper, we propose the use of  ``Soft Balanced Accuracy''. This is a differentiable form of balanced accuracy, which allows us to simultaneously deal with the challenges common in imbalanced problems.


% Let's consider a binary classification task where the output probability vector from a classifier is of size two, denoting the probabilities of the two classes. We denote these probabilities as $\text{probs} = [p_0, p_1]$ where $p_0$ and $p_1$ are the predicted probabilities of the first and second class respectively. Let's also assume that the true labels for the instances are represented as $\text{labels}$, where $\text{labels}[i]$ is 1 if instance $i$ belongs to the second class and 0 otherwise.

% In conventional accuracy calculation, we apply an argmax operation on $\text{probs}$ or by thresholding them to get the predicted class and compare it with $\text{labels}$. However, this operation is non-differentiable, i.e., it does not allow backpropagation of gradients during training.

% Instead, we can compute a soft version of accuracy that uses the probabilities directly without needing to apply the argmax function. For instance, the Soft True Positive (TP) for class 1 can be computed as the sum of the predicted probabilities for instances that truly belong to class 1, or mathematically: $\text{TP\_soft} = \sum((\text{labels} == 1) \times p_1)$. Similarly, for class 0, $\text{TP\_soft} = \sum((\text{labels} == 0) \times p_0)$. This directly corresponds to correctly predicting instances of each class.

The soft accuracy is computed by omitting the ``argmax'' function from a standard classification accuracy metric. In other words, for a vector of predicted class probabilities $P(x)$ given input $x$ and a one-hot encoded label vector $L_x$, the vector of soft True Positive rates for all classes is:
\vspace{-0.1cm}\begin{equation}
\text{TP} = \sum_x P(x) \odot L_x,
\end{equation}
where $\odot$ represents the Hadamard product.

Analogously, the vector of Soft False Positives (FP) for each class would be computed as the sum of the predicted probabilities for instances that are incorrectly predicted as that. Given the inverted (one-cold) label vector $\bar{L}_x$:
\vspace{-0.1cm}\begin{equation}
\text{FP} = \sum_x P(x) \odot \bar{L}_x.
\end{equation}
% : $\text{FP\_soft} = \sum((\text{labels} == 0) \times p_1)$. Similarly, for class 0, $\text{FP\_soft} = \sum((\text{labels} == 1) \times p_0)$.

We can similarly compute the vectors of Soft True Negatives (TN) and Soft False Negatives (FN) as
\vspace{-0.1cm}\begin{equation}
\text{TN} = \sum_x (1-P(x)) \odot \bar{L}_x,
\vspace{-0.2cm}\end{equation}
\vspace{-0.1cm}\begin{equation}
\text{FN} = \sum_x (1-P(x)) \odot L_x.
\end{equation}
%
%using the probabilities. For class 1, $\text{TN\_soft} = \sum((\text{labels} == 0) \times (1 - p_1))$ and $\text{FN\_soft} = \sum((\text{labels} == 1) \times (1 - p_1))$. Similarly, for class 0, $\text{TN\_soft} = \sum((\text{labels} == 1) \times (1 - p_0))$ and $\text{FN\_soft} = \sum((\text{labels} == 0) \times (1 - p_0))$.
%
Although it may be obvious, it is worth pointing out that these soft variants of the TP, TN, FP, and FN are easily differentiable, as they are computed directly from the predicted probabilities. It is also worth pointing out that the classes referred to here are based on the output task, and differ from the sensitive characteristic domains of equation \ref{eq:gamma}.
%This allows us to backpropagate gradients during training, enabling the model to learn from its errors.

Given the above, we could compute the per-class accuracy vector as
%Now, accuracy for a single class can be computed as 
\vspace{-0.1cm}\begin{equation}
\text{Acc} = \frac{(\text{TP} + \text{TN}) }{ (\text{TP} + \text{TN} + \text{FP} + \text{FN})}.
\end{equation}
However, this measure can be misleading when classes are imbalanced. For instance, in a dataset where a single class represents 90\% of the data, a naive classifier that always predicts the dominant class will have an accuracy of 90\%.

To address this, we calculate the Soft Balanced Accuracy, which is the average of the per-class recall. In other words
\vspace{-0.1cm}\begin{equation}
S = \frac{1}{C} \left|\frac{\text{TP} }{ (\text{TP} + \text{FN})}\right|^1,
\end{equation}
where $C$ is the number of classes and $|.|^1$ represents the L1 norm.
%Recall for a class is computed as $\text{TP} / (\text{TP} + \text{FN})$, and hence, $\text{Balanced Accuracy} = 0.5 \times (\text{Recall\_class0} + \text{Recall\_class1})$. In the case of Soft Balanced Accuracy, it becomes $0.5 \times ((\text{TP\_soft0} / (\text{TP\_soft0} + \text{FN\_soft0})) + (\text{TP\_soft1} / (\text{TP\_soft1} + \text{FN\_soft1})))$. 
This gives equal weighting to all classes irrespective of their prevalence in the dataset.%, thereby offering a more honest measure of the classifier's performance.

%Soft Balanced Accuracy provides a differentiable metric for evaluating and training classifiers, especially when dealing with imbalanced data. It allows for gradient-based optimization and provides an incentive for the model to perform equally well on all classes, enhancing the overall fairness of the model.

To return to the definition of $\Delta_{adj}$ in Section \ref{sec:AP}: we propose computing this soft balanced accuracy $S$ 
independently on subsets of the dataset corresponding to each sensitive domain. The mean and standard deviation of $S$ across these domains can then be substituted for $\bar{S}$ and $\sigma$ in equation \ref{eq:diffpar2} (once again noting that the set of task labels is not the same as the set of sensitive characteristics). By substituting the balanced soft accuracy into the adjusted parity metric, we can obtain a differential adjusted parity (DAP) loss which we can use to train a model. This loss encourages improvements in task accuracy across all labels, weighted by their prevalence and the current relative task performance, with no adversarial component.
%to improve the average soft balanced accuracy and reduce the standard deviation of the soft balanced accuracies across domains, thus improving task performance, while minimising the inconsistency and differences for different sensitive feature groups. 
Intuitively, minimising task prediction inconsistency across sensitive domains would minimise the mutual information between the representations and the sensitive feature. This leads to less information regarding the sensitive feature being encoded in the representations and thus better demographic parity and equalised odds scores.

% \subsection{DAP Model}
% Next we will formulate a neural network predictor that is built within the above framework.
% We assume that each data sample is characterized by non-sensitive input attributes (\( x \)) and sensitive attributes (\( A \)). Additionally, we have access to ground-truth labels for a given task (\( L_x \)).

% Our network comprises two modules: First an encoder \( F \) which produces latent embeddings $z$ from inputs $x$, parameterised by weights $\omega_f$. Second a classifier \( C \) which produces predicted task labels $P(x)$ that are both fair and accurate, based on latent embeddings, parameterised by weights $\omega_c$.
% %Given a dataset, we can characterize its instances with the following attributes:
% %
% %\begin{itemize}
% %    \item \( X \): Non-sensitive attributes of the data.
% %    \item \( A \) (belongs to the set \( \{0, 1\} \)): Sensitive attributes.
% %    \item \( Y \) (also belongs to the set \( \{0, 1\} \)): Ground-truth labels.
% %\end{itemize}
% %
% %Our primary objective is to train a classifier, denoted as \( c \), which operates on latent representations generated by an encoder function, \( f \). Specifically:
% In  other words
% \begin{align}
%     z &= f(X|\omega_f)\\
%     P(x) &= c(z|\omega_c) = c(f(X|\omega_f)|\omega_c).
% \end{align}

% The classifier \( c \) aims to produce with \( Z \) as input predictions \( \hat{Y} \) that are both accurate in relation to \( Y \) and fair concerning the sensitive attribute \( A \).

% Given the above, We seek a mechanism to compute the adjusted parity loss which accounts for gender-specific performance disparities in classification outcomes along with ensuring proper classification of income. We aim to do that by calculation of soft balanced accuracies.

% \begin{itemize}
%     \item Probabilities Calculation:
%     \vspace{-0.1cm}\begin{equation}
%         P = \text{softmax}(c(z))
%     \vspace{-0.2cm}\end{equation}
%     Where \( P \) is the output probability matrix from the classifier \( c \) acting on \( z \).
    
%     \item Income Cross-Entropy Loss:
%     \vspace{-0.1cm}\begin{equation}
%         L_{\text{income}} = -\sum_i \left[ y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i) \right]
%     \vspace{-0.2cm}\end{equation}
%     Where \( \hat{y}_i \) is the predicted probability of the income being \( 1 \) for the \( i^{\text{th}} \) instance.
    
%     \item Soft Prediction Metrics:
    
%     For gender \( a=0 \):
%     \begin{align}
%         \text{softTP}_0 &= y \times P_{y=1|a=0} \\
%         \text{softFN}_0 &= y \times (1-P_{y=1|a=0}) \\
%         \text{softTN}_0 &= (1-y) \times P_{y=0|a=0} \\
%         \text{softFP}_0 &= (1-y) \times (1-P_{y=0|a=0})
%     \end{align}
    
%     For gender \( a=1 \):
%     \begin{align}
%         \text{softTP}_1 &= y \times P_{y=1|a=1} \\
%         \text{softFN}_1 &= y \times (1-P_{y=1|a=1}) \\
%         \text{softTN}_1 &= (1-y) \times P_{y=0|a=1} \\
%         \text{softFP}_1 &= (1-y) \times (1-P_{y=0|a=1})
%     \end{align}
    
%     \item Soft Balanced Accuracy Calculation:
    
%     For gender \( a=0 \):
%     \vspace{-0.1cm}\begin{equation}
%         \text{softBA}_0 = \frac{\text{softTP}_0}{\text{softTP}_0 + \text{softFN}_0 + \epsilon} + \frac{\text{softTN}_0}{\text{softTN}_0 + \text{softFP}_0 + \epsilon}
%     \vspace{-0.2cm}\end{equation}
    
%     For gender \( a=1 \):
%     \vspace{-0.1cm}\begin{equation}
%         \text{softBA}_1 = \frac{\text{softTP}_1}{\text{softTP}_1 + \text{softFN}_1 + \epsilon} + \frac{\text{softTN}_1}{\text{softTN}_1 + \text{softFP}_1 + \epsilon}
%     \vspace{-0.2cm}\end{equation}
    
%     Where \( \epsilon \) is a small constant to prevent division by zero.
    
%     \item Overall Balanced Accuracy and Standard Deviation:
%     \begin{align}
%         S &= \frac{\text{softBA}_0 + \text{softBA}_1}{2} \\
%         \sigma &= \beta \times \text{std}(\text{softBA}_0, \text{softBA}_1)
%     \end{align}
    
%     \item Adjusted Parity:
%     \vspace{-0.1cm}\begin{equation}
%         \text{AdjustedParity} = S \times (1-2 \times \sigma)
%     \vspace{-0.2cm}\end{equation}
    
%     \item Final Loss:
%     \vspace{-0.1cm}\begin{equation}
%         \text{Loss} = L_{\text{income}} - \text{AdjustedParity}
%     \vspace{-0.2cm}\end{equation}
% \end{itemize}
% Cross-entropy loss and soft accuracy, though conceptually related, emphasize different facets of a model's performance, and combining them can offer a more holistic training regimen. 

% Cross-entropy loss focuses on penalizing wrong predictions, especially those made with high confidence. It pushes the model to produce sharper probability distributions over classes and aids in making more decisive predictions. By amplifying the penalties for confident but wrong predictions, it nudges the model towards achieving higher confidence in correct predictions. 

% Soft accuracy, on the other hand, serves as a more direct reflection of the model's classification performance. It rewards the model based on how aligned the predicted probabilities are with the true labels. Incorporating this loss offers a nuanced incentive: it motivates the model to not just predict accurately, but also calibrate its confidence in those predictions. 

% By combining these two losses, we can achieve a more balanced training process. While cross-entropy ensures that the model makes confident decisions, soft accuracy ensures that these decisions align closely with the ground truth. This fusion can potentially lead to a model that's both decisive in its predictions and accurate in its classifications. Nevertheless, a careful balance is needed to ensure that the two objectives complement and don't counteract each other and we do so by including another hyperparametre scaling factor [omega]. 

% Furthermore, while our foundational setup assumes \( A \) to be binary, our model is readily extendable to handle multi-label scenarios. We would compute soft TP, FN, TN and FP for all the sensitive feature domains, from which we can compute the soft balanced accuracies for all the domains. We would then calculate the mean and standard deviation of the balanced soft accuracies across the domains similar to the binary scenario above. Importantly, when adapting to cases with an odd number of classes \( N \), it is crucial to adjust the factor \( \gamma \) by which we multiply \( \sigma \) (the standard deviation), as outlined in the previous section. This not absolutely necessary during training because we’re multiplying \( \sigma \) by the hyperparameter scaling factor beta, but it would be when calculating the final adjusted parity metric for the model.

\section{Evaluation protocol}
\label{sec:experiments}

In our experiments, we've chosen to focus on two widely-acknowledged datasets in fairness research: the Adult dataset and the COMPAS dataset. In both cases we train a network using a combination of our DAP metric and the standard task cross-entropy loss ($\mathcal{L}_{ce}$). We also introduce weighting hyperparameters $\beta$ and $\Omega$ which control the contribution of standard deviation term and $\mathcal{L}_{ce}$ respectively.

\subsection{Adult Dataset}

The Adult dataset, often referred to as the ``Census Income'' dataset, originates from the UCI Machine Learning Repository \cite{dua2017}. It comprises demographic data extracted from the 1994 Census Bureau database. The primary task for this dataset is binary classification: predicting whether an individual earns more than \$50k annually based on attributes like age, occupation, education, and marital status.

One notable characteristic of the Adult dataset is its inherent imbalance. Specifically, a substantial proportion of individuals in the dataset have incomes below \$50k (around 75.4\%). The dataset contains several sensitive attributes such as race and gender. We opt to use gender as the sensitive feature in this evaluation. This is also imbalanced with roughly 67.3\% of the data being male. Such imbalances could mislead naive classifiers into an unwanted bias towards the dominant class. Both the gender feature and target income variable are binary. We attempt to eliminate disparities in income predictions across gender groups.

\subsection{COMPAS Dataset}
The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) dataset became notably popular following an investigation by ProPublica in 2016 \cite{dieterich2016}. COMPAS is a risk assessment tool used in the U.S. legal system to assess the likelihood that a defendant will re-offend. Each instance in the dataset contains 12 features like age, gender, criminal history, and risk scores. The primary task is to predict if an individual will re-offend within two years.

ProPublica's analysis notably highlighted racial disparities in the predictions, where African-American defendants were more likely to be falsely classified as high risk compared to white defendants. We therefore opt to use race as the sensitive feature. The COMPAS dataset is balanced in terms of both the sensitive feature and target variable. The COMPAS dataset one-hot encodes ethnicity into five categories: African American, Asian, Hispanic, Native American, and Other. Studies often reduce this multi-class feature into a binary one distinguishing only between African-American and all other ethnicities, overshadowing the multi-class complexity. To enable comparison against models that do not have multi-class sensitive feature debiasing capability, we perform experiments with this binary simplification. However, we also evaluate our approach with the true multi-class problem. It is important to note that the COMPAS dataset has been heavily cirticised for its use in fairness research due to its inherent measurement biases and errors, its disconnection from real-world criminal justice outcomes and its lack of consideration for the complex normative issues related to fairness, justice and equality \cite{bao2021s}. We use it here only to support comparison against previous works.

\subsection{Data Preprocessing}
For both datasets, we performed standard preprocessing, mapping categorical features to numerical indices, normalization of continuous variables, and handling missing values by replacing them with -1. We split the datasets into training and test sets in a 175:25 ratio. We also drop redundant features, that are either repeated or with mostly missing values. %We end up with each instance having 12 input features (excluding the sensitive feature and target variable) for both datasets being passed as input to the encoder head.

\subsection{Hyperparameter and Model Training}
We employed a learning rate of $0.005$ and $0.01$ and a batch size of $64$ and $32$ for the Adult and COMPAS datasets respectively. The models were trained for $20$ epochs.

For our hyperparameter sensitivity study, we chose values $\Omega \in \{0...100\}$ and $\beta \in \{0.1...100\}$
%$\omega \in \{0, 0.01, 0.1, 1, 3, 5, 10, 15, 20, 50, 100\}$ and $\beta \in \{0.1, 1, 3, 5, 10, 15, 20, 50, 100\}$
, resulting in $100$ tested combinations of $\Omega$ and $\beta$. All models were trained through $5$ distinct runs, and we report the median as well as standard deviation of their performance across the runs. Given the stochastic nature of neural network training, this ensured robustness in our findings. 
%The median grants a central tendency less prone to outliers compared to a simple mean.

\begin{figure*}[t]
\centering
\includegraphics[width=0.43\textwidth]{sota_adult1.png}
\includegraphics[width=0.43\textwidth]{sota_adult2.png} \\
\includegraphics[width=0.43\textwidth]{sota_adult3.png}
\includegraphics[width=0.43\textwidth]{sota_adult4.png}
\caption {Comparing performance and fairness of all 4 models against DAP on Adult dataset. Top-left, top-right, bottom-left and bottom-right graph show how adjusted parity, equalised odds difference (EOD), gender classification accuracy and demographic parity difference (DPD) change with $\beta$. Higher adjusted parity and lower EOD, DPD and gender accuracy are favourable. DAP has higher adjusted parity and lower gender classification accuracy at all $\beta$. Lowest EOD and DPD are obtained by DAP at $\beta$=100.}
\label{fig1}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.45\textwidth]{beta_omega_adult_1.png}
\includegraphics[width=0.45\textwidth]{beta_omega_adult_2.png} \\
\includegraphics[width=0.45\textwidth]{beta_omega_adult_3.png}
\includegraphics[width=0.45\textwidth]{beta_omega_adult_4.png}
%\includegraphics[width=0.9\textwidth]{Figure2} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Effect of altering $\Omega$ and $\beta$ on adjusted parity (top-left), EOD (top-right), gender accuracy (bottom-left), and DPD (bottom-right) on Adult dataset. Higher adjusted parity and lower EOD, DPD and gender accuracy are favourable. Increasing $\beta$ lowers adjusted parity but improves all other metrics. Effect more pronounced at lower $\Omega$}
\label{fig2}
\end{figure*}

\begin{table*}[t!]
\centering
\begin{tabular}{lcccccl} \hline 
DAP Model  & $\Omega$   & $\beta$ & \begin{tabular}[c]{@{}l@{}} Adjusted \\ Parity\end{tabular} & EOD         & DPD         & \begin{tabular}[c]{@{}l@{}}Gender \\ Accuracy (\%)\end{tabular} \\ \hline 
Balanced   & 100 & 5 & 0.632±0.014                                                & 0.126±0.075 & 0.265±0.028 & 60.9±7.2                                                        \\
Balanced   & 10  & 3 & 0.617±0.005                                                & 0.090±0.005 & 0.233±0.011 & 62.3±4.2                                                        \\
Balanced   & 1   & 1 & 0.593±0.008                                                & 0.052±0.052 & 0.185±0.016 & 59.7±5.5                                                        \\
Balanced   & 15  & 5 & 0.621±0.009                                                & 0.111±0.010 & 0.242±0.018 & 61.1±1.9                                                        \\
Balanced   & 3   & 1 & 0.622±0.006                                                & 0.113±0.038 & 0.244±0.011 & 59.3±4.4                                                        \\
Unbalanced & 100 & 5 & 0.635±0.013                                                & 0.122±0.059 & 0.271±0.025 & 62.5±5.9                                                        \\
Unbalanced & 10  & 3 & 0.618±0.011                                                & 0.096±0.020 & 0.236±0.022 & 61.4±3.3                                                        \\
Unbalanced & 1   & 1 & 0.596±0.010                                                & 0.053±0.033 & 0.193±0.019 & 61.0±4.1                                                        \\
Unbalanced & 15  & 5 & 0.615±0.010                                                & 0.094±0.009 & 0.230±0.020 & 59.9±3.8                                                        \\
Unbalanced & 3   & 1 & 0.619±0.005                                                & 0.114±0.045 & 0.239±0.011 & 60.7±5.2  \\   \hline  \hline         
\end{tabular}
\caption{Comparing the adjusted parity, EOD, DPD, gender accuracy when balanced and unbalanced accuracies are used during training. This table shows a few high performing combinations of $\beta$ and $\Omega$ on the Adult dataset. No significant difference is observed between Balanced and Unbalanced.
}
\label{table1}
\end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.43\textwidth]{sota_compas1.pdf}
\includegraphics[width=0.43\textwidth]{sota_compas2.pdf} \\
\includegraphics[width=0.43\textwidth]{sota_compas3.pdf}
\includegraphics[width=0.43\textwidth]{sota_compas4.pdf}
%\includegraphics[width=0.9\textwidth]{Figure2} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Comparing all 4 models performance and fairness against DAP on the COMPAS dataset. Top-left, top-right, bottom-left and bottom-right graph show how adjusted parity, EOD, race classification accuracy and DPD change with $\beta$. Higher adjusted parity and lower EOD, DPD and gender accuracy are favourable. DAP has lower gender classification accuracy at all $\beta$. Highest adjusted parity and lowest EOD and DPD are obtained by DAP}
\label{fig3}
\end{figure*}
\subsection{Evaluation Framework}
To evaluate our models' performance, we trained 2 balanced random forest classifiers to predict the sensitive and target features from the encodings in the testing phase. This allowed us to measure balanced classification accuracies on the task variable and sensitive feature from the embeddings produced during testing. On the fairness front, we obtain fairness metrics like demographic parity and equalised odds, using the predicted target from the random forest classifier. We also obtain an adjusted parity metric for comparing models and selecting best performing hyperparameters.

We compare our model with CFAIR, LAFTR and FAIR (ALFR), which we covered in \ref{sec:background}. We also compare against  NODEBIAS which is reference network devoid of fairness constraints. The implementation for these models was adapted from \cite{taejunkim}. We evaluate them using the same evaluation protocol as for our model with balanced random forest classifier to obtain target and sensitive feature classification accuracy, DP, EO and adjusted parity metrics. We also contrast our DAP model using balanced soft accuracies against a variant using unbalanced soft accuracies.

\begin{figure*}[t]
\centering
\begin{minipage}{0.43\textwidth}
    \centering
    \textbf{Adult} \par\medskip
    \includegraphics[width=\textwidth]{EODvsIncome.png} \\
    \includegraphics[width=\textwidth]{DPDvsIncome.png} \\
    \includegraphics[width=\textwidth]{GendervsIncome.png}
\end{minipage}
\hspace{0.05\textwidth} % Add horizontal space between columns
\begin{minipage}{0.43\textwidth}
    \centering
    \textbf{COMPAS} \par\medskip
    \includegraphics[width=\textwidth]{EODvsRecid.png} \\
    \includegraphics[width=\textwidth]{DPDvsRecid.png} \\
    \includegraphics[width=\textwidth]{RacevsRecid.png}
\end{minipage}
\caption{Showing how the fairness metrics vary with changes in task accuracy from baseline values (Income Accuracy: 0.8294 for Adult, Recidivism Accuracy: 0.584 for COMPAS; EOD: 0.1429/0.310; DPD: 0.2759/0.173; Gender/Race Accuracy: 0.637/0.581) for each metric for Adult (left) and COMPAS (right). Changes were binned into 0.005 intervals, and averages for EOD, DPD, and gender/race accuracy were computed for each interval. Obtaining the largest negative change in EOD, DPD and sensitive feature accuracy for the least drop in task accuracy is favorable.}
\label{fig5}
\end{figure*}

\label{sec:results}
As depicted in Figure \ref{fig1}, DAP consistently outperforms NODEBIAS, LAFTR, CFAIR, and FAIR in terms of adjusted parity and gender classification balanced accuracy on the Adult dataset. Specifically, DAP demonstrates a superior adjusted parity (which is notably higher) and a more favorable gender classification balanced accuracy (which is significantly lower). When benchmarked against EO and DP metrics, DAP either achieves equivalent or better than the other 4 models. The lowest EOD and DPD are achieved with DAP. With \( \beta = 100 \), DAP obtains 44.1\% and 18.6\% lower EOD and DPD than the next best performer FAIR.

Figure \ref{fig2} delves into the effects of tweaking $\beta$ and $\Omega$ on the performance of DAP. When $\Omega$ is held constant and $\beta$ is gradually increased, there is a notable decrease in the adjusted parity. Concurrently, other fairness metrics such as EOD, DPD, and gender classification balanced accuracy witness marked improvements. This effect is particularly evident at lower $\Omega$ values. Conversely, when $\beta$ remains static and $\Omega$ is increased, the outcomes typically include a surge in adjusted parity, gender classification balanced accuracy, and DPD metrics. Interestingly, EOD doesn't seem to follow a discernible trend in relation to changing $\Omega$ values. It's important to highlight that specific pairings of $\beta$ and $\Omega$ can optimize EOD values, indicating the delicate interplay between these parameters.

In an evaluative comparison between using balanced and unbalanced soft accuracies during training, Table \ref{table1} underscores that there are negligible differences in the adjusted parity. Moreover, the differences in adjusted parity, gender classification accuracy, EOD, and DPD are all insignificant as emphasized by the overlap of the standard deviations.

Figure \ref{fig3} presents the experimental outcomes on the COMPAS dataset. Here DAP exhibits consistent improvements over previous state-of-the-art. In particular for high values of  \( \beta \) (meaning a high fairness weighting), DAP outperforms all competing approaches. 
%It increases adjusted parity and reduces DPD, EOD, and race classification accuracy (all with reduced values). More specifically, 
DAP achieves an adjusted parity that is improved by 45.9\% and an EOD that is reduced by 12.4\% when compared with its nearest competitor, CFAIR. Similarly, it registers a 22.5\% improvement in DPD and a substantial 40.1\% reduction in race classification accuracy when compared to FAIR, the latter being the second-best performer for these metrics. Unlike with the Adult dataset, the performance of DAP on COMPAS does not seem to be very sensitive to the value of $\Omega$. Because the performance is roughly similar across all values, the results are omitted here and can be found in section Appendix A. However, they can be found in the supplementary material.

Figure \ref{fig5} demonstrates the interplay between improving fairness metrics and declining task performance. Ideally, we aim to minimize the impact on task accuracy while maximizing the reduction in fairness-related metrics. With less than a 2.5\% decrease in income classification accuracy, a reduction of 0.08, 0.10, and 4.93\% is achieved on EOD, DPD, and gender classification accuracy, respectively, on the Adult dataset. Similarly, a decline of less than 2\% in recidivism accuracy results in a decrease of 0.14, 0.06, and 3.41\% in EOD, DPD, and race classification accuracy on the COMPAS dataset.

\begin{figure}[t]
\centering
\includegraphics[width=0.6\columnwidth]{multiclass_compas.png}
%\includegraphics[width=0.9\textwidth]{Figure2} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Demonstrating the performance of DAP with multi-class sensitive features at $\Omega$=20. DPD and adjusted parity decrease and race classification accuracy approaches 0.33 with increasing $\beta$, as desired. EOD shows no significant trend. %Higher adjusted parity and lower DPD and EOD are favourable. Race classification accuracy close to 0.33 (random chance) is preferred.
}
\label{fig4}
\end{figure}
%Figure \ref{fig4} illustrates the effectiveness of DAP when applied to multi-class sensitive attributes. With increasing $\beta$, EOD remains constant, starting and ending at 0.5, with no percentage change and an insignificant negative trend (slope: -0.0000132, p-value: 0.9292). In contrast, DPD experiences a significant decrease of 68.18\%, dropping from 0.4889 to 0.1556, alongside a statistically significant negative trend (slope: -0.0002808, p-value: 0.0350). Balanced race classification accuracy shows an increase of 28.82\%, rising from 0.2570 to 0.3311, with a statistically significant positive trend (slope: 0.0000839, p-value: 0.0018). With three race classes, random chance balanced accuracy would be 0.33. An accuracy closer to this value indicates less race information present in the embeddings. Finally, increases slightly upto 0.27 at $\beta$=5, but overall shows a substantial decrease of 92.14\%, falling from 0.2100 to 0.0165.
Figure \ref{fig4} illustrates the effectiveness of DAP for multi-class sensitive attributes. As $\beta$ increases, EOD remains constant, starting and ending at 0.5 with no significant negative trend. In contrast, DPD significantly decreases by 68.18\%, dropping from 0.4889 to 0.1556. Balanced race classification accuracy improves by 28.82\%, rising from 0.2570 to 0.3311, nearing the random chance level of 0.33, indicating reduced race information in the embeddings. Finally, adjusted parity increased slightly up to 0.27 at $\beta=5$, but overall shows a substantial drop of 92.14\%, from 0.2100 to 0.0165.

%DAP significantly improves over previous SOTA in terms of race classification accuracy. Here, it consistently surpasses the other four models by maintaining a race classification accuracy that's lower by approximately 40\%, highlighting the model's robustness in this particular aspect. The optimal performance metrics of DAP are attained through specific combinations of \( \beta \) and \( \Omega \).

\section{Conclusion}
\label{sec:conclusion}
The results unequivocally position DAP as a highly effective approach for achieving fairness across different datasets, significantly outclassing established models like FAIR, CFAIR, and LAFTR. 
These adversarial approaches can prove challenging to train but DAP remains stable, even for complex under-explored problems like those with multi-class sensitive features.
%This distinction becomes particularly important when considering the mechanism of these models. While FAIR, CFAIR, and LAFTR primarily leverage adversarial components that can be unstable and challenging to train, DAP presents an alternative.

%We also demonstrated DAP's effectiveness when working with multi-class sensitive features on the COMPAS dataset. DAP's performance is on par with, or even surpasses, its effectiveness on binary sensitive features. This capability is significant as it inherently integrates multi-class sensitive features without necessitating modifications to the model architecture.

We formulate a differentiable variant of the adjusted parity metric, which includes only adaptively weighted positive learning signals with no adversarial tension. At its core, this involves the innovative use of the ``Soft Balanced Accuracy'' to provide a metric which is smoothly differentiable and agnostic to dataset biases (either in terms of sensitive characteristics or end-task labels).
%
Unlike other non-adversarial approaches to fairness, DAP also does not suffer from degenerate solutions. The metric cannot be satisfied by performing equally poorly across all sensitive domains.

%Unlike the traditional accuracy metrics that can be skewed by imbalanced datasets, soft balanced accuracy provides a holistic, balanced view of classification performance. By removing the \textit{argmax} function from the standard classification accuracy metric and introducing the soft variants of TP, TN, FP, and FN, DAP ensures a measure that's robust, differentiable, and unbiased towards any specific class.

%What becomes evident from this design choice is the model's ability to handle challenges inherent in imbalanced problems. The soft balanced accuracy not only offers a fairer reflection of a model's performance, but its differentiable nature also means it can be incorporated into a loss function for optimization purposes. This approach allows DAP to ensure consistent and unbiased performance across all classes.

%It is also imperative to acknowledge the implications of DAP's emphasis on ensuring equal weighting to all classes, regardless of their prevalence. This counters the potential pitfalls of traditional accuracy metrics, which can be misleadingly high in the face of class imbalances. For instance, if a class dominated 90\% of the dataset, traditional metrics might praise a naive classifier for its ostensibly high accuracy. DAP’s use of soft balanced accuracy mitigates such misleading representations, providing a truer image of the model's performance.

%Moreover, the differentiable nature of DAP's metrics implies that, during model training, the gradients provide direct feedback on how to improve fairness metrics. This contrasts with adversarial-based approaches where the model and adversary often end up playing a zero-sum game, leading to potential convergence issues.

%It is worth noting that DAP's design choice to minimize task prediction inconsistency across sensitive domains, intuitively corresponds to the reduction of mutual information between representations and sensitive features. The immediate consequences of this are twofold: first, it results in representations that carry minimal information about the sensitive attribute, and second, it enhances demographic parity and equalized odds scores. Both these outcomes serve the broader objective of fairness in machine learning models.

In terms of the limitations of DAP, there is some sensitivity to the hyperparameters $\beta$ and $\Omega$ as illustrated in our sensitivity studies. The effects of $\beta$ and $\Omega$ are nuanced and interconnected, making it necessary to calibrate them specifically depending on the application and dataset characteristics.
%While increasing $\beta$ notably diminishes the adjusted parity, it simultaneously enhances other fairness metrics. However, the effects are more pronounced at lower $\Omega$ values. The subtle interplay between these parameters makes it necessary to calibrate them specifically depending on the application requirements and dataset characteristics.

Future work in this area should focus on techniques to automatically calibrate the hyperparameters for a given problem. There may be some benefits in developing a scheme to adapt the hyperparameters throughout the training process based on current performance. This would be a natural extension of DAP's current implicit approach to adaptive loss weighting based on fairness measures.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{neurips_2024}
\bibliographystyle{plainnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section{Derivation of maximum standard deviation.}

We define our Differential Adjusted Parity as
\vspace{-0.1cm}\begin{equation}
    \Delta_{\text{adj}} = (\bar{S} - S^{r})\left(1 - \frac{\sigma}{\gamma}\right)
\label{eq:diffpar}
\end{equation}
where $S^r$ is the baseline accuracy of a random predictor, $\bar{S}$ is the average Soft Balanced Accuracy across sensitive domains, $\sigma$ is the standard deviation of SAB across these domains and $\gamma$ is the maximum possible standard deviation. The term $\gamma$ serves to normalise the metric between [0,1].

For any even number of domains ($N$) $\gamma=0.5$. However for an odd number of domains 
\vspace{-0.1cm}\begin{equation}
    \gamma = \sqrt{\frac{1}{4} \left(1 - \frac{1}{N^2}\right)}.
    \label{eq:gamma2}
\end{equation}
Below we present the derivation of this rule.

The standard deviation of values, \( \sigma \), is defined as:
\vspace{-0.1cm}\begin{equation}
    \sigma = \sqrt{\frac{1}{N}\sum_{s \in S} (s - \bar{S})^2}
\end{equation}
where \( \bar{S} \) is the mean of the values in \( S \), representing the mean of the soft balanced accuracies.

The formulation for the maximum standard deviation, \( \gamma \), is:
\vspace{-0.1cm}\begin{equation}
    \gamma = \arg \max_{S \in \mathbb{R}^N} \sqrt{\frac{1}{N}\sum_{s \in S} (s - \bar{S})^2} 
     \quad  \quad \text{s.t. } s \in [\alpha..\Omega]
\end{equation}
where $\alpha$ and $\Omega$ are the upper and lower bounds on the values of $s$. In other words we use soft balanced accuracy vector $S$ which leads to maximal standard deviation from the mean.

We can see by inspection that when the values of $s$ are bounded, we achieve maximial standard deviation by making \( \bar{S} \) as centered as possible within the range, while all the values in $s$ are on the extremes of the range. When \( N \) is even, this can be perfectly achieved by placing half of the items at each end. In such a scenario, \( \bar{S} = \sigma = \frac{\alpha + \Omega}{2} \). Hence, if the soft balanced accuracies are bounded between [0..1] then the maximum standard deviation for any even number of sensitive domains is 0.5.

However, for odd \( N \), the mean will necessarily be slightly off-center from the range, leading to a lower maximal \( \gamma \). Considering that \( \mathrm{floor}(N/2) \) items are placed at \( \alpha \) and \( \mathrm{ceil}(N/2) \) items are placed at \( \Omega \), the mean can be derived as:
\begin{align}
    \bar{S} &= \frac{\alpha\mathrm{floor}\left(\frac{N}{2}\right) + \Omega\mathrm{ceil}\left(\frac{N}{2}\right)}{N} \\
    & = \frac{\alpha\frac{N-1}{2} + \Omega\frac{N+1}{2}}{N}
\end{align}

Similarly, the summation inside the definition of $\sigma$ can be split into 2 parts and resolved
\begin{align}
    \gamma & = 
    \sqrt{\frac{1}{N}\left(
    \sum_{i=1}^{\frac{N-1}{2}}
    (\bar{S} - \alpha)^2
    +
    \sum_{i=1}^{\frac{N+1}{2}}
    (\bar{S} - \omega)^2
    \right)}
    \\ & = 
    \sqrt{\frac{1}{N}\left(
    \frac{N-1}{2}
    (\bar{S} - \alpha)^2
    +
    \frac{N+1}{2}
    (\bar{S} - \omega)^2
    \right)}
    \label{eq:stdev}
\end{align}

Given that the minimum normalized accuracy, \( \alpha \), is 0 and the maximum normalized accuracy, \( \Omega \), is 1, the simplified mean is:
\begin{align}
    \bar{S} &= \frac{1+\frac{1}{N}}{2} \\
    &= \frac{1}{2} + \frac{1}{2N}
\label{eq:mean}
\end{align}
Consequently, we note that as \( N \to \infty \), \( \bar{S} \) approaches 0.5.

Similarly, substituting $\alpha=0$, $\Omega=1$ into equation \ref{eq:stdev} gives
\begin{align}
    \gamma & = 
    \sqrt{\frac{1}{N}\left(
    \frac{N-1}{2}
    (\bar{S} - 0)^2
    +
    \frac{N+1}{2}
    (\bar{S} - 1)^2
    \right)}
    \\ & = 
    \sqrt{\frac{1}{N}\left(
    \frac{N-1}{2}
    \bar{S}^2
    +
    \frac{N+1}{2}
    (\bar{S}^2 - 2\bar{S} +1)
    \right)}
    \\ & = 
    \sqrt{\left(
    \frac{1-\frac{1}{N}}{2}
    \bar{S}^2
    +
    \frac{1+\frac{1}{N}}{2}
    (\bar{S}^2 - 2\bar{S} +1)
    \right)}
    \\ & = 
    \sqrt{\left(
    \frac{\bar{S}^2}{2} - \frac{\bar{S}^2}{2N}\right)
    +
    \left(
    \frac{1}{2}+\frac{1}{2N}\right)
    (\bar{S}^2 - 2\bar{S} +1)
    }
    \\ & = 
    \sqrt{\left(
    \frac{\bar{S}^2}{2} - \frac{\bar{S}^2}{2N}\right)
    +
    \left(\frac{\bar{S}^2}{2} - \bar{S} +\frac{1}{2}\right)
    +
    \left(\frac{\bar{S}^2}{2N} - \frac{\bar{S}}{N} + \frac{1}{2N}\right)
    }
    \\ & = 
    \sqrt{\bar{S}^2\left(
    \frac{1}{2} - \frac{1}{2N} + \frac{1}{2} + \frac{1}{2N}\right)
    -
    \bar{S}\left(1 + \frac{1}{N}\right)
    +
    \left(\frac{1}{2} + \frac{1}{2N}\right)
    }
    \\ & = 
    \sqrt{\bar{S}^2
    -
    \bar{S} - \frac{\bar{S}}{N}
    +
    \frac{1}{2} + \frac{1}{2N}
    }
    % \\ & = 
    % \sqrt{\frac{1}{N}\left(
    % \frac{N-1}{2}
    % \left(\frac{1}{2} + \frac{1}{2N}\right)^2
    % +
    % \frac{N+1}{2}
    % \left(\frac{1}{2} + \frac{1}{2N} - 1\right)^2
    % \right)}
    \label{eq:stdev2}
\end{align}

Finally, substituting equation \ref{eq:mean} into equation \ref{eq:stdev2} we can fully simplify:
\begin{align}
    \gamma & =  
    \sqrt{\left(\frac{1}{2} + \frac{1}{2N}\right)^2
    -
    \left(\frac{1}{2} + \frac{1}{2N}\right) - \frac{\left(\frac{1}{2} + \frac{1}{2N}\right)}{N}
    +
    \frac{1}{2} + \frac{1}{2N}
    }
    \\ & = 
    \sqrt{
    \left(\frac{1}{4} + \frac{1}{4N^2} + \frac{1}{2N}\right)
    -
    \left(\frac{1}{2} + \frac{1}{2N}\right) - \left(\frac{1}{2N} + \frac{1}{2N^2}\right)
    +
    \frac{1}{2} + \frac{1}{2N}
    }
    \\ & = 
    \sqrt{
    \frac{1}{N^2}
    \left(\frac{1}{4} - \frac{1}{2}\right)
    +
    \frac{1}{4}
    }
    \\ & = 
    \sqrt{
    \frac{1}{4}
    \left(
    1 - \frac{1}{N^2}
    \right)
    }
    % \sqrt{\frac{1}{N}\left(
    % \frac{N-1}{2}
    % \left(\frac{1}{2} + \frac{1}{2N}\right)^2
    % +
    % \frac{N+1}{2}
    % \left(\frac{1}{2} + \frac{1}{2N} - 1\right)^2
    % \right)}
    \label{eq:stdev3}
\end{align}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.6\textwidth]{adjustedparitytoy.png}
\caption{Adjusted Parity Metric $\Delta_{\text{adj}}$ as a function of the Average Accuracy $\overline{S}$ for various values of the Standard Deviation $\sigma$ across domains. The baseline accuracy of a random predictor $S^R$ is set to 0.1, and the maximum standard deviation $\gamma$ is fixed at 0.5. The metric demonstrates how increasing performance inconsistency (higher $\sigma$) across domains reduces $\Delta_{\text{adj}}$, even when average accuracy is high.}
\label{fig6}
\end{figure*}

\section{Hyperparameter sensitivity on the COMPAS dataset}
Figure \ref{fig4b} shows the sensitivity of the DAP system to the $\beta$ and $\Omega$ hyperparameters on the COMPAS dataset. The results do not show the same trend witnessed on the Adult dataset with varying $\beta$ and $\Omega$. Increasing$\beta$ for a given $\Omega$ does not lower adjusted parity and improve fairness metrics as observed on the Adult dataset. The full set of results used to obtain graphs here and in the anonymous submission are placed with the zip file containing the code.

\begin{figure*}[t!]
\centering
\includegraphics[width=0.47\textwidth]{beta_omega_compas_1.png}
\includegraphics[width=0.47\textwidth]{beta_omega_compas_2.png} \\
\includegraphics[width=0.47\textwidth]{beta_omega_compas_3.png}
\includegraphics[width=0.47\textwidth]{beta_omega_compas_4.png}
%\includegraphics[width=0.9\textwidth]{Figure2} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Effect of altering $\Omega$ and $\beta$ on adjusted parity (top-left), EOD (top-right), gender accuracy (bottom-left), and DPD (bottom-right) on COMPAS dataset. Higher adjusted parity and lower EOD, DPD and gender accuracy are favourable.}
\label{fig7}
\end{figure*}


%\newpage
%\section*{NeurIPS Paper Checklist}
%
%%%% BEGIN INSTRUCTIONS %%%
%The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page
%limit. 
%
%Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
%\begin{itemize}
%    \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
%    \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). 
%   % \item {\bf The papers not including the checklist will be desk rejected.}
%\end{itemize}
%
%{\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.
%
%The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.
%
%IMPORTANT, please:
%\begin{itemize}
%    \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},
%    \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
%    \item {\bf Do not modify the questions and only use the provided macros for your answers}.
%\end{itemize} 
% 
%
%%%% END INSTRUCTIONS %%%
%
%
%\begin{enumerate}
%
%\item {\bf Claims}
%    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The abstract and introduction clearly state the development of the Differential Adjusted Parity (DAP) loss, which enhances performance and mitigates bias without adversarial components. These claims are consistent and well-supported throughout the paper with theoretical and experimental evidence, specifically highlighted results on the Adult dataset.
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
%        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
%        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
%        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
%    \end{itemize}
%
%\item {\bf Limitations}
%    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The paper outlines the limitations regarding the sensitivity of the Differential Adjusted Parity (DAP) model to hyperparameters, the need for specific calibrations based on datasets, and potential issues with computational efficiency and scalability
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
%        \item The authors are encouraged to create a separate "Limitations" section in their paper.
%        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
%        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
%        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
%        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
%        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
%        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
%    \end{itemize}
%
%\item {\bf Theory Assumptions and Proofs}
%    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The paper clearly states all assumptions alongside the theoretical results and provides complete proofs within the supplemental material, while the main text includes intuitive sketches for reader comprehension. 
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper does not include theoretical results. 
%        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
%        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
%        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
%        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
%        \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
%    \end{itemize}
%
%    \item {\bf Experimental Result Reproducibility}
%    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The paper provides detailed descriptions of the experimental setups, including hyperparameter settings, dataset sources, and preprocessing steps. It also includes discussions on model architecture and training procedures, allowing for reproducibility of the results. The actual code will also be made available.
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper does not include experiments.
%        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
%        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
%        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
%        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
%        \begin{enumerate}
%            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
%            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
%            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
%            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
%        \end{enumerate}
%    \end{itemize}
%
%
%\item {\bf Open access to data and code}
%    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: Code will be made publicly available. This will include detailed instructions on data access, preparation, and the exact commands required to reproduce the experimental results, facilitating full reproducibility of the studies conducted.
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that paper does not include experiments requiring code.
%        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%        \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
%        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
%        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
%        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
%        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
%    \end{itemize}
%
%
%\item {\bf Experimental Setting/Details}
%    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The paper provides comprehensive details on training and test settings, including data splits, hyperparameters, the selection process for these parameters, and the type of optimizer used. These details are sufficiently detailed to replicate the experiments and understand the results presented.
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper does not include experiments.
%        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
%        \item The full details can be provided either with the code, in appendix, or as supplemental material.
%    \end{itemize}
%
%\item {\bf Experiment Statistical Significance}
%    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The paper includes error bars/standard deviation where necessary.
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper does not include experiments.
%        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
%        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
%        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
%        \item The assumptions made should be given (e.g., Normally distributed errors).
%        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
%        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
%        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
%        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
%    \end{itemize}
%
%\item {\bf Experiments Compute Resources}
%    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: Computational resources are detailed in the paper.
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper does not include experiments.
%        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
%        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
%        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
%    \end{itemize}
%    
%\item {\bf Code Of Ethics}
%    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The research adheres to the NeurIPS Code of Ethics, ensuring anonymity, ethical consideration in the use of data, and compliance with legal standards. No deviations from the Code of Ethics are noted in the paper.
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
%        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
%        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
%    \end{itemize}
%
%
%\item {\bf Broader Impacts}
%    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The paper highlights positive impacts, such as improving fairness in AI models, and acknowledges negative impacts, including potential reduced performance in critical applications like loan approvals and university admissions. It notes that the model could be tuned to minimize or disregard fairness considerations, leading to ethical concerns. 
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that there is no societal impact of the work performed.
%        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
%        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
%        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
%        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
%        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
%    \end{itemize}
%    
%\item {\bf Safeguards}
%    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
%    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: 
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper poses no such risks.
%        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
%        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
%        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
%    \end{itemize}
%
%\item {\bf Licenses for existing assets}
%    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The creators of the models, which we compare against, and code framework used to obtain results for these models are correctly cited and referenced.
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper does not use existing assets.
%        \item The authors should cite the original paper that produced the code package or dataset.
%        \item The authors should state which version of the asset is used and, if possible, include a URL.
%        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
%        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
%        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
%        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
%        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
%    \end{itemize}
%
%\item {\bf New Assets}
%    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
%    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: The code base that will be provided with the anonymised submission is documented properly.
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper does not release new assets.
%        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
%        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
%        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
%    \end{itemize}
%
%\item {\bf Crowdsourcing and Research with Human Subjects}
%    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
%    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification:
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
%        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
%    \end{itemize}
%
%\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
%    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
%    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item[] Justification: 
%    \item[] Guidelines:
%    \begin{itemize}
%        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
%        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
%        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
%    \end{itemize}
%
%\end{enumerate}


\end{document}