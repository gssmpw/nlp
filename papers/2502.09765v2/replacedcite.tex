\section{Background and Related Work}
\label{sec:background}
Below we will first discuss modern adversarial techniques for debiasing. We will then cover the non-adversarial techniques which are more closely related to this paper. This is followed by a formalisation of the various definitions of fairness found in the literature.

\subsection{Adversarial Debiasing}

Adversarially Learned Fair Representations, ALFR (Edwards and Storkey, 2016), is one of the earliest models developed to reduce bias and learn fair informative representations. It utilises an adversary which tries to predict the sensitive feature from the representations. The autoencoder tries to make these predictions as difficult as possible. It employs task and reconstruction losses to ensure the representation are informative for the downstream task, and an additional sensitive feature loss to remove sensitive feature related information.

The Conditional Fair Representations, CFair, approach ____ stands as a seminal method aiming for accuracy parity. Within the confines of conventional fair adversarial networks, CFair augments the original adversarial constraints and adopts conditional error constraints. 
%The objective function for \textit{CFair} is expressed as:
%\begin{align}
% \mathcal{L} = &\min_{h, g} \max_{h', h''} \text{BER}_{\mathcal{D}}(h(g(X)) || Y) \notag \\
% &- \lambda \left( \text{BER}_{\mathcal{D}_0}(h'(g(X)) || S) + \text{BER}_{\mathcal{D}_1}(h''(g(X)) || S) \right)
% \end{align}
%where \(\text{BER}\) signifies the Balanced Error Rate (Menon and Williamson, 2018), while \(\mathcal{D}_k\) represents instances associated with the sensitive attribute \(k\).

Learning Adversarially Fair and Transferable Representation, LAFTR ____, is similar to CFAIR but it uses one adversary instead of two and an L1 instead of a cross entropy loss to debias the representations. It still utilises a global cross-entropy loss for the target variable.


\subsection{Non-adversarial Debiasing}
Outside of adversarial learning, Fairness by Compression (FBC) ____ advocates for the use of binary compression to mitigate sensitive elements in representations. They establish that the cross-entropy between \( P(z) \) and \( Q \) stands as the upper bound for the entropy \( H(z) \). In this context, \( P(z) \) delineates the distribution of a factorized representation, and \( Q \) is utilized to predict \( z_i \) based on \( \{ z_0, z_1, \dots , z_{i-1} \} \). The FRC model ____ aims to mitigate the influence of sensitive factors in the data representation by adjusting the correlation between the representation and the sensitive vectors.
% Their proposed objective function is
% \vspace{-0.1cm}\begin{equation}
% \mathcal{L} = \min_{z} \mathop{\mathbb{E}}_{x,z,a}[-\log(P(x|z, a))] + \beta \textit{CE}(P, Q)
% \vspace{-0.2cm}\end{equation}
%where \( \textit{CE} \) signifies the cross-entropy function, and \( \beta \) modulates the compression rate.


% The objective function for FRC is formulated as
% \begin{align}
%     \mathcal{L} = &\mathop{\mathbb{E}}_{q(Z|X)}[\log p(X|Z)] - \beta \text{DKL}(q(Z|X) || p(Z))\notag \\ 
%     & + \gamma (\text{Corr}(S, Z_P) - \text{Corr}(S, Z_N))
% \end{align}
%where \(Z_P\) and \(Z_N\) denote the representation vectors that are respectively associated and dissociated with sensitive information.

BFA ____ draws inspiration from the correlation coefficient constraints used in FRC. The primary ambition is to minimize the correlation between sensitive information and prediction error (as opposed to minimising correlation between sensitive information and representations in FRC), aiming to maintain high predictive accuracy.

The Variational Fair Autoencoder, VFAE ____, comprises of a variational autoencoder (VAE) instead of an autoencoder and employs an addititional maximum mean discrepancy (MMD) loss to ensure less sensitive information, which may be correlated to the target task, leaks into the learned representation. MMD minimises the mismatch in moments between the marginal posterior distributions for the different sensitive features.

These techniques exhibit greatly improved training stability compared to the adversarial training approaches. However, they struggle to achieve good task performance (i.e. developing useful representations). It can be observed that many of these techniques experience a degeneracy where the loss function can be satisfied by performing equally poorly across all sensitive domains. In contrast our proposed approach maintains the benefits of non-adversarial training, while removing this degeneracy.

\subsection{Measures of Fairness}
Fairness in machine learning has been extensively studied, and there exist a range of metrics which measure different aspects of it. Our primary contribution is the introduction of a new differentiable fairness metric, thus necessitating a brief overview of commonly used metrics in our model evaluation. %Thus we will first briefly formalize the metrics that are commonly used in the field and that we will use in the evaluation of our model.
%These metrics include demographic parity, equalized odds, equal opportunity, and disparate impact.

Demographic parity, also known as statistical parity or group fairness, requires that the selection rate (the rate at which individuals are positively classified) should be the same across all demographic groups. Mathematically, if \( Y \) is the predicted label and \( A \) denotes the demographic group, demographic parity is defined as:
%
\vspace{-0.1cm}\begin{equation}
P(Y=1|A=0) = P(Y=1|A=1).
\end{equation}
%
This implies that the algorithm should be independent of the sensitive attribute \( A \) ____, which can be limiting if the attribute is relevant to the outcome ____.% However, demographic parity can be limiting in situations where the sensitive attribute is indeed relevant to the outcome. This and other flaws are discussed in detail in the seminal work ____.

The equalized odds fairness metric demands true positive rates and false positive rates to be equal across demographic groups. Mathematically, if $\hat{Y}$ is the true label:
%
\vspace{-0.1cm}\begin{equation}
P(Y\!=\!1|\hat{Y}\!=\!1, A\!=\!0) \!=\! P(Y\!=\!1|\hat{Y}\!=\!1, A\!=\!1)
\end{equation}
%
and
%
\vspace{-0.1cm}\begin{equation}
P(Y\!=\!1|\hat{Y}\!=\!0, A\!=\!0) \!=\! P(Y\!=\!1|\hat{Y}\!=\!0, A\!=\!1).
\end{equation}
%
Equalized odds aims for outcome independence across demographic groups when conditioned on the true label ____. %This means the classifierâ€™s outcomes, both in terms of correctly identified positive cases and incorrectly identified negative cases, should be independent of the demographic group when conditioned on the true label ____. However, maintaining equalized odds can often be challenging in the presence of significant imbalances in the underlying data distributions among different groups.

%The equal opportunity metric, a subset of equalized odds, solely focuses on the true positive rates. In essence, equal opportunity requires the algorithm to have similar true positive rates across different demographic groups. This is mathematically represented as:
%
%\vspace{-0.1cm}\begin{equation}
%P(Y\!=\!1|\hat{Y}\!=\!1, A\!=\!0) \!=\! P(Y\!=\!1|\hat{Y}\!=\!1, A\!=\!1).
%\vspace{-0.2cm}\end{equation}
%
%Equal opportunity thus ensures that the proportion of correctly classified positive outcomes is the same across all demographic groups ____.

%Disparate impact refers to a scenario where a decision, rule, or policy has a discriminatory effect on a protected class, even though the explicit rules appear to be neutral. In terms of machine learning, disparate impact can be quantified as the ratio of the probabilities of positive outcomes in the protected group to that in the non-protected group. Thus, disparate impact is defined as:
%
%\vspace{-0.1cm}\begin{equation}
%\frac{P(Y=1|A=1)}{P(Y=1|A=0)}.
%\vspace{-0.2cm}\end{equation}
%
%A value of 1 indicates no disparate impact, and U.S. legal guidelines suggest that a value below 0.8 may be indicative of significant disparate impact ____.

These metrics, while essential for assessing fairness, have inherent trade-offs and limitations. It is generally impossible to satisfy all these conditions simultaneously when the base rates differ across groups ____. They focus on ensuring parity in predictions without necessarily considering the impact on overall task accuracy ____.

%While these metrics provide valuable tools for assessing fairness, they are not panaceas. There are trade-offs and tensions among them, and it is generally impossible to satisfy all these conditions simultaneously when the base rates differ across groups ____. Thus, the selection of appropriate fairness metrics depends heavily on the specific context and ethical considerations.

%When assessing the fairness of machine learning models, these metrics primarily focus on ensuring parity in prediction outcomes across different demographic groups. These metrics do not explicitly take into consideration the downstream task's classification accuracy ____.

%Specifically, demographic parity and disparate impact concentrate on the balanced positive prediction rates across various groups, disregarding the actual labels of the instances. Similarly, equalized odds and equal opportunity are concerned with similar true positive rates or error rates across groups but do not explicitly account for overall accuracy. Therefore, while these metrics ensure that models do not disproportionately disadvantage any particular group, they may do so at the cost of reducing the model's overall predictive performance ____.

Recognizing the need to balance accuracy and fairness, there's a growing emphasis on metrics that integrate classification performance, creating more robust and equitable machine learning systems suitable for real-world applications ____. This unified approach fosters a comprehensive evaluation and comparison of debiasing models, ensuring effectiveness in predictions while maintaining fairness.

%In the real world, accuracy and fairness are both critically important. Decisions based on machine learning models often have significant consequences, such as granting loans, admitting students to universities, or diagnosing diseases. In these situations, sacrificing accuracy can lead to suboptimal outcomes, while disregarding fairness can perpetuate systemic bias.

%Thus, there is a growing need for a unified metric that balances both classification accuracy and fairness. This approach allows for a more comprehensive comparison and evaluation of debiasing models, ensuring not just fairness but also effectiveness in prediction. By incorporating classification accuracy into fairness metrics, we can build more robust and equitable machine learning systems that are better suited for real-world applications ____.