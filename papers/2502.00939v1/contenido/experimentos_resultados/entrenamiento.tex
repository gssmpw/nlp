\subsection{Training.}

As described in \cite{20Simonyan2015}, the VGG16 and VGG19 models were originally trained with images of 224 x 224 pixels. On the other hand, \cite{21Szegedy2016} mentions that the Inception-v3 model was trained with images of 299 x 299 pixels, although it can also process images of lower resolution. While training with lower-resolution images may increase the required time, the quality of the final result is quite close to that obtained with the original resolution.

Based on the information provided by \cite{20Simonyan2015, 21Szegedy2016}, we decided to resize our dataset to images of 224 x 224 pixels and three color channels (224 x 224 x 3), maintaining input data homogeneity for the three models. This way, we maintain the proper resolution for the VGG16 and VGG19 models and ensure acceptable performance for the Inception-v3 model. Additionally, we used the specific preprocessing functions for each model to standardize the inputs.

The \textit{preprocess\_input()} function of VGG16 \cite{25tensorflowvgg16} and VGG19 \cite{26tensorflowvgg17} converts the image array to the \textit{float32} format, changes the color space from RGB to BGR, and performs zero-centering, i.e., subtracts the mean of each color channel from the corresponding pixel values.

In the case of Inception-v3, the \textit{preprocess\_input()} function \cite{27tensorflowinceptionv3} also converts the image array to \textit{float32}, but additionally scales the pixel values in a range from -1 to 1.

\textbf{Dataset Split.} The dataset used in this study consists of a total of 472 images. It was divided into three subsets: 70\% for training, 20\% for validation, and 10\% for testing. The images that had incorrect segmentations were included in the test set in order to evaluate the model's ability to generalize and correctly classify both well-segmented images and those with segmentation errors. This detailed split is presented in Table \ref{tab:datosentrenamiento}.

\begin{table}[htbp]
	\centering
	\caption{Dataset Partitioning.}
	\label{tab:datosentrenamiento}
	\begin{tabular}{|l|c|c|c|}
		\hline
		& \textbf{Training} & \textbf{Validation} & \textbf{Test} \\
		\hline
		\textbf{\textit{A. fraterculus}} & 165 & 47 & 24 + 50 \\
		\hline
		\textbf{\textit{C. capitata}} & 165 & 47 & 24 + 50 \\
		\hline
		\textbf{Total} & 330 & 94 & 148 \\
		\hline
	\end{tabular}
\end{table}

\textbf{Hyperparameter Definition.} For each model, we used the following hyperparameters.

\begin{itemize}
	\item Learning rate: 0.0001
	\item Number of epochs: 100
	\item Loss function: categorical\_crossentropy
	\item Optimizer: SGD with momentum of 0.8
	\item Batch size: 16
\end{itemize}

\textbf{Callback Definitions.} In our training approach, we used two callbacks: EarlyStopping and ModelCheckpoint.

\begin{itemize}
	\item   EarlyStopping. 
	This callback stops the training of a model when the selected metric stops improving for a specified number of epochs. This strategy is essential to prevent overfitting the model.
	\item   ModelCheckpoint. 
	This callback saves the model at periodic checkpoints, either after each epoch or when a new best performance on a specific metric is reached. This allows us to preserve the best model obtained during training.
\end{itemize}

By combining these two callbacks in our training strategy, we achieved greater robustness and efficiency in the training process of our models. We prevent overfitting by stopping training when no improvement in a specific metric is observed over a period, while simultaneously allowing us to preserve the best model obtained so far.

For our research, we set the number of epochs for EarlyStopping to 25 and "val\_accuracy" as the monitored metric for both callbacks.

