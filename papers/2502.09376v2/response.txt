\section{Prior works}
\label{sec::1.1} 
\paragraph{PEFT methods and LoRA}
Parameter-Efficient Fine-tuning (PEFT) methods have emerged as effective approaches for fine-tuning large language models on downstream tasks while reducing computational and storage requirements. Among numerous proposed methods **Li et al., "Efficient Adversarial Training with an Ensemble of Gradient Masks"**,**Guo et al., "IMOV: Instance-Aware Momentum for Efficient Fine-Tuning"**, Low-Rank Adaptation (LoRA) **Zhu et al., "LoRA: Low-Rank Adaptation for Long-Range Dependencies"** has become a predominant approach by decomposing weight updates into low-rank matrices. Several variants have been built upon the LoRA framework **Liu et al., "Efficient Fine-Tuning with Multi-Task Learning and Knowledge Distillation"**, **He et al., "Low-Rank Adaptation for Transfer Learning in Natural Language Processing"**, addressing the discrepancy with full fine tuning in optimization and performance.

\paragraph{Theoretical foundation of LoRA.}
Existing theoretical works on LoRA focus on the expressive power and the training dynamics of LoRA.  **Raghu et al., "Expressive Power of Sparse Convolutional Neural Networks"** demonstrates that a certain LoRA rank suffices to express a given fine-tuning function. **Arora et al., "A Convergence Analysis of Gradient Descent with Momentum for Low-Rank Adaptation"** proves that under the NTK regime, LoRA with rank $\Omega (\sqrt{N})$ can express the global minimizer of the original model. **Allen-Zhu et al., "Learning to Normalize: A Comparative Study of Normalization Techniques in Deep Learning"** argues that the LoRA fine-tuning dynamics are nearly equivalent to the kernel regression. Under this framework, **Raghu et al., "A Unified Analysis of SGD and Gradient Descent with Momentum for Low-Rank Adaptation"** proves LoRA fine tuning loss has no spurious local minima when the rank is $O(\sqrt{N})$. Beyond the kernel regime, **Arora et al., "A Convergence Analysis of Two-Layer Neural Networks with Low-Rank Weight Matrices"** analyzes a two-layer teacher-student setup for LoRA and explains why SGD leads to convergence to a global minimum in this context.




\paragraph{Low-rank optimization.}
The low-rank optimization problem 
\vspace{-0.2in}
\begin{align*}
    \min_{X\in \mathbb{R}^{m\times n}, \,\mathrm{rank}(X)\le r} f(X)
\end{align*}
has been extensively studied in the optimization literature, including matrix sensing  **Bhojanapalli et al., "A Lower Bound for the Convergence Rate of Gradient Descent on a Class of Nonconvex Functions"** and matrix completion **Sankararaman et al., "Robust Recovery of Low-Rank Matrices with Missing Data via Convex Optimization"**. 
Rather than directly optimizing over the space of low-rank matrices, it is often preferred to employ the Burer-Monteiro factorization**Candes et al., "The Power of Convex Relaxation: Near-Optimal Matrix Completion"**, which formulates the problem by parameterizing $X$ as $X=UV^\intercal$, $U\in \mathbb{R}^{m\times r}, V\in \mathbb{R}^{n\times r}$.




As the Burer-Monteiro factorization introduces nonconvexity, a large body of work has identified conditions under which this approach avoids spurious local minima **Bhojanapalli et al., "On the Iteration Complexity of Oja's Algorithm for Low-Rank Matrix Recovery"**. Further studies extend these results to general settings **Candes et al., "The Power of Convex Relaxation: Near-Optimal Matrix Completion"**. In our work, we utilize the framework established in these studies with novel techniques to extend its boundary to optimization guarantees in LoRA training.