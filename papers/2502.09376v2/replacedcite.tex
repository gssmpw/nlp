\section{Prior works}
\label{sec::1.1} 
\paragraph{PEFT methods and LoRA}
Parameter-Efficient Fine-tuning (PEFT) methods have emerged as effective approaches for fine-tuning large language models on downstream tasks while reducing computational and storage requirements. Among numerous proposed methods ____, Low-Rank Adaptation (LoRA) ____ has become a predominant approach by decomposing weight updates into low-rank matrices. Several variants have been built upon the LoRA framework ____, addressing the discrepancy with full fine tuning in optimization and performance.

\paragraph{Theoretical foundation of LoRA.}
Existing theoretical works on LoRA focus on the expressive power and the training dynamics of LoRA.  ____ demonstrates that a certain LoRA rank suffices to express a given fine-tuning function. ____ proves that under the NTK regime, LoRA with rank $\Omega (\sqrt{N})$ can express the global minimizer of the original model. ____ argues that the LoRA fine-tuning dynamics are nearly equivalent to the kernel regression. Under this framework, ____ proves LoRA fine tuning loss has no spurious local minima when the rank is $O(\sqrt{N})$. Beyond the kernel regime, ____ analyzes a two-layer teacher-student setup for LoRA and explains why SGD leads to convergence to a global minimum in this context.








\paragraph{Low-rank optimization.}
The low-rank optimization problem 
\vspace{-0.2in}
\begin{align*}
    \min_{X\in \mathbb{R}^{m\times n}, \,\mathrm{rank}(X)\le r} f(X)
\end{align*}
has been extensively studied in the optimization literature, including matrix sensing  ____ and matrix completion ____. 
Rather than directly optimizing over the space of low-rank matrices, it is often preferred to employ the Burer-Monteiro factorization____, which formulates the problem by parameterizing $X$ as $X=UV^\intercal$, $U\in \mathbb{R}^{m\times r}, V\in \mathbb{R}^{n\times r}$.




As the Burer-Monteiro factorization introduces nonconvexity, a large body of work has identified conditions under which this approach avoids spurious local minima ____. Further studies extend these results to general settings ____. In our work, we utilize the framework established in these studies with novel techniques to extend its boundary to optimization guarantees in LoRA training.