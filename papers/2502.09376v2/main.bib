@article{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
journal={International Conference on Learning Representations},
year={2022}
}

@inproceedings{zhang2024riemannian,
      title={Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models}, 
      author={Fangzhao Zhang and Mert Pilanci},
      year={2024},
      booktitle = {International Conference of Machine Learning} 
}

@article{
zeng2024the,
title={The Expressive Power of Low-Rank Adaptation},
author={Yuchen Zeng and Kangwook Lee},
journal={International Conference on Learning Representations},
year={2024}
}

@article{
jang2024lora,
title={Lo{RA} Training in the {NTK} Regime has No Spurious Local Minima},
author={Uijeong Jang and Jason D. Lee and Ernest K. Ryu},
journal={International Conference on Machine Learning},
year={2024}
}


@article{pmlr-v202-malladi23a,
  title = 	 {A Kernel-Based View of Language Model Fine-Tuning},
  author =       {Malladi, Sadhika and Wettig, Alexander and Yu, Dingli and Chen, Danqi and Arora, Sanjeev},
  journal= 	 {International Conference on Machine Learning},
  year = {2023}
}

@article{dayi2024gradient,
  title={Gradient dynamics for low-rank fine-tuning beyond kernels},
  author={Dayi, Arif Kerem and Chen, Sitan},
  journal={arXiv preprint arXiv:2411.15385},
  year={2024}
}

@article{
li2018measuring,
title={Measuring the Intrinsic Dimension of Objective Landscapes},
author={Chunyuan Li and Heerad Farkhoor and Rosanne Liu and Jason Yosinski},
journal={International Conference on Learning Representations},
year={2018}
}

@article{aghajanyan-etal-2021-intrinsic,
    title = "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    author = "Aghajanyan, Armen  and
      Gupta, Sonal  and
      Zettlemoyer, Luke",
    journal = "Association for Computational Linguistics",
    year = 2021
}

@article{NIPS1991_8eefcfdf,
 author = {Krogh, Anders and Hertz, John},
 journal = {Neural Information Processing Systems},
 title = {A Simple Weight Decay Can Improve Generalization},
 year = {1991}
}

@article{MacKay1995ProbableNA,
  title={Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks},
  author={David John Cameron MacKay},
  journal={Network: Computation In Neural Systems},
  year={1995},
  volume={6},
  pages={469-505},
}




@article{doi:10.1137/070697835,
author = {Recht, Benjamin and Fazel, Maryam and Parrilo, Pablo A.},
title = {Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization},
journal = {SIAM Review},
volume = {52},
number = {3},
pages = {471-501},
year = {2010}}

@InProceedings{10.1007/11503415_37,
author="Srebro, Nathan
and Shraibman, Adi",
editor="Auer, Peter
and Meir, Ron",
title="Rank, Trace-Norm and Max-Norm",
booktitle="Learning Theory",
year="2005",
publisher="Springer Berlin Heidelberg"
}

@article{fazel2001rank,
  title={A rank minimization heuristic with application to minimum order system approximation},
  author={Fazel, Maryam and Hindi, Haitham and Boyd, Stephen P},
  journal={American Control Conference},
  year={2001}
}

@article{cabral2013unifying,
  title={Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition},
  author={Cabral, Ricardo and De la Torre, Fernando and Costeira, Jo{\~a}o P and Bernardino, Alexandre},
  journal={International Conference on Computer Vision},
  year={2013}
}

@misc{xiao2024stochasticsubgradientmethodsguaranteed,
      title={Stochastic Subgradient Methods with Guaranteed Global Stability in Nonsmooth Nonconvex Optimization}, 
      author={Nachuan Xiao and Xiaoyin Hu and Kim-Chuan Toh},
      year={2023},
      journal={arXiv preprint arXiv:2307.10053}
}

@article{hu_low_2021,
	title = {Low {Rank} {Regularization}: {A} review},
	volume = {136},
	issn = {0893-6080},
	author = {Hu, Zhanxuan and Nie, Feiping and Wang, Rong and Li, Xuelong},
	year = {2021},
	keywords = {Low rank, Optimization, Regularization},
	pages = {218--232},
        journal = {Neural Networks}
}

@article{
kobayashi2024weight,
title={Weight decay induces low-rank attention layers},
author={Seijin Kobayashi and Yassir Akram and Johannes Von Oswald},
journal={Neural Information Processing Systems},
year={2024}
}

@inproceedings{DBLP:conf/acl/RenS0ZRRCP24,
  author={Pengjie Ren and Chengshun Shi and Shiguang Wu and Mengqi Zhang and Zhaochun Ren and Maarten de Rijke and Zhumin Chen and Jiahuan Pei},
  title={MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning},
  year={2024},
  cdate={1704067200000},
  pages={3052-3064},
  booktitle={ACL (1)},
  crossref={conf/acl/2024-1}
}


@article{pmlr-v40-Ge15,
  title = 	 {Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition},
  author = 	 {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  journal = 	 {Conference on Learning Theory},
  year = 	 {2015}
}

@inproceedings{NIPS2016_7fb8ceb3,
 author = {Ge, Rong and Lee, Jason D and Ma, Tengyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 title = {Matrix Completion has No Spurious Local Minimum},
 year = {2016}
}

@article{10.1145/2184319.2184343,
author = {Cand\`{e}s, Emmanuel and Recht, Benjamin},
title = {Exact matrix completion via convex optimization},
year = {2012},
journal = {Foundations of Computational Mathematics},
volume = {55},
number = {6},
pages = {111--119}
}
@article{Burer2003ANP,
  title={A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization},
  author={Samuel Burer and Renato D. C. Monteiro},
  journal={Mathematical Programming},
  year={2003},
  volume={95},
  pages={329-357}
}

@article{NIPS2016_b139e104,
 author = {Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
 journal = {Neural Information Processing Systems},
 title = {Global Optimality of Local Search for Low Rank Matrix Recovery},
 year = {2016}
}

@article{10.5555/3305381.3305509,
author = {Ge, Rong and Jin, Chi and Zheng, Yi},
title = {No spurious local minima in nonconvex low rank problems: A unified geometric analysis},
year = {2017},
journal = {International Conference on Machine Learning}
}


@article{pmlr-v54-park17a,
  title = 	 {{Non-square matrix sensing without spurious local minima via the {B}urer--{M}onteiro approach}},
  author = 	 {Park, Dohyung and Kyrillidis, Anastasios and Carmanis, Constantine and Sanghavi, Sujay},
  journal = 	 {International Conference on Artificial Intelligence and Statistics},
  year = 	 {2017}
}
@article{zhang2021sharpglobalguaranteesnonconvex,
      title={Sharp Global Guarantees for Nonconvex Low-Rank Matrix Recovery in the Overparameterized Regime}, 
      author={Richard Y. Zhang},
      year={2021},
      journal ={arXiv preprint arXiv: 2104.10790}
}
@article{doi:10.1137/18M1231675,
author = {Ha, Wooseok and Liu, Haoyang and Barber, Rina Foygel},
title = {An Equivalence between Critical Points for Rank Constraints Versus Low-Rank Factorizations},
journal = {SIAM Journal on Optimization},
volume = {30},
number = {4},
pages = {2927-2955},
year = {2020}
}
@article{zhang2024improved,
  title={Improved global guarantees for the nonconvex {B}urer--{M}onteiro factorization via rank overparameterization},
  author={Zhang, Richard Y},
  journal={Mathematical Programming},
  pages={1--30},
  year={2024}
}

@article{mordukhovich1995nonconvex,
  title={On nonconvex subdifferential calculus in Banach spaces},
  author={Mordukhovich, Boris S and Shao, Yongheng},
  journal={Journal of Convex Analysis},
  volume={2},
  number={1--2},
  pages={211--227},
  year={1995}
}

@article{
wang2024implicit,
title={Implicit bias of {SGD} in {$L_2$}-regularized linear {DNN}s: One-way jumps from high to low rank},
author={Zihan Wang and Arthur Jacot},
journal={International Conference on Learning Representations},
year={2024}
}

@article{
galanti2024sgd,
title={{SGD} and Weight Decay Secretly Minimize the Rank of Your Neural Network},
author={Tomer Galanti and Zachary S Siegel and Aparna Gupte and Tomaso A Poggio},
journal={NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning},
year={2024}
}

@article{Liu2019RoBERTaAR,
  title={{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={arXiv preprint arXiv: 1907.11692},
  year = {2019},
}

@article{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
journal={International Conference on Learning Representations},
year={2021}
}

@mastersthesis{Krizhevsky2009LearningML,
  author       = {Krizhevsky, Alex},
  title        = {Learning Multiple Layers of Features from Tiny Images},
  school       = {University of Toronto},
  type         = {Master's Thesis},
  year         = {2009},
}


@article{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
journal = "Empirical Methods in Natural Language Processing",
    year = 2013
}


@article{pmlr-v49-lee16,
  title = 	 {Gradient Descent Only Converges to Minimizers},
  author = 	 {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
  journal = 	 {Conference on Learning Theory},
  year = 	 {2016}
}

@ARTICLE{8357489,
  author={Zhu, Zhihui and Li, Qiuwei and Tang, Gongguo and Wakin, Michael B.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Global Optimality in Low-Rank Matrix Optimization}, 
  year={2018},
  volume={66},
  number={13},
  pages={3614-3628}}

@article{ben-zaken-etal-2022-bitfit,
    title = "{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    author = "Ben Zaken, Elad  and
      Goldberg, Yoav  and
      Ravfogel, Shauli",
    journal = "Association for Computational Linguistics",
    year = "2022",
}

@article{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    journal = "Association for Computational Linguistics",
    year = 2021
}

@article{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    journal = "Empirical Methods in Natural Language Processing",
    year = 2021
}

@article{
liu2024dora,
title={Do{RA}: Weight-Decomposed Low-Rank Adaptation},
author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
journal={International Conference on Machine Learning},
year={2024}
}

@article{wang2024loraprolowrankadaptersproperly,
      title={{LoRA-Pro}: Are Low-Rank Adapters Properly Optimized?}, 
      author={Zhengbo Wang and Jian Liang and Ran He and Zilei Wang and Tieniu Tan},
      year={2024},
      journal ={arXiv preprint arXiv: 2407.18242},
}

@article{
dettmers2023qlora,
title={{QL}o{RA}: Efficient Finetuning of Quantized {LLM}s},
author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
journal={Neural Information Processing Systems},
year={2023}
}




@article{xiong2024how,
  title   = {How over-parameterization slows down gradient descent in matrix sensing: The curses of symmetry and initialization},
  author  = {Xiong, N. and Ding, L. and Du, S.},
  journal = {International Conference on Learning Representations},
  year    = {2024}
}


@article{xu2023over,
  title   = {Over-parameterization exponentially slows down gradient descent for learning a single neuron},
  author  = {Xu, W. and Du, S.},
  journal = {Conference on Learning Theory},
  year    = {2023}
}

@article{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    journal = "EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    year = "2018"
}
@article{10.1561/2400000003,
author = {Parikh, Neal and Boyd, Stephen},
title = {Proximal Algorithms},
year = {2014},
volume = {1},
number = {3},
journal = {Foundations and Trends in Optimization},
pages = {127--239},
}

@article{
tomihari2024understanding,
title={Understanding Linear Probing then Fine-tuning Language Models from {NTK} Perspective},
author={Akiyoshi Tomihari and Issei Sato},
journal={Neural Information Processing Systems},
year={2024},
}