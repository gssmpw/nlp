\label{sec:intro}

The perception stage is crucial for numerous real-world applications, including robotics navigation and autonomous driving. As LiDAR sensors can provide precise depth information compared to RGB cameras, they have gained increasing popularity in the field of perception. Among those perception tasks, 3D object detection aims to detect and localize objects of interest in the surrounding environment. With the advent of 3D point cloud deep learning technologies~\cite{qi2017pointnet, qi2017pointnet++} and the release of several 3D real-world human-annotated datasets~\cite{geiger2012we, caesar2020nuscenes, sun2020scalability}, numerous research studies related to 3D object detection models~\cite{zhou2018voxelnet, yan2018second, yang2018pixor, lang2019pointpillars, shi2019pointrcnn, shi2020pv, shi2020points, shi2020point, yin2021center, deng2021voxel, shi2023pv} have emerged recently in the pursuit of achieving more accurate results. However, most of those 3D object detection works focus on training in a specific domain. They might experience a significant performance drop when used in an unknown domain because they may not account for different sensor settings and environments during the training phase, thereby limiting their applicability for real-world applications.

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\columnwidth, clip]{./Figure/fig1.pdf}
\caption{Traditional 3D detectors, trained directly on the source domain, often experience a significant performance drop when the point clouds become sparser. In contrast, our method empowers the 3D detector to learn sparsity-invariant features through training with our proposed augmentation and feature alignment techniques. Note that in the detection results, blue boxes represent the ground truth annotations, while the green boxes indicate the predicted boxes.
}

\vspace{-.5cm}
% \vspace{-10pt}
\label{fig1}
\end{figure}

% Discribe the domain gap in 3D
As mentioned in~\cite{wang2020train}, the domain gap in 3D can be categorized into two main reasons. The first reason is that the LiDAR sensor configurations for each dataset are different, leading to significant differences in the sensed point clouds. The other reason arises from scene distribution. Public datasets collect their data from different environments. All these factors contribute to varying statistical distributions, including the distribution of each class, the shape, and the size of each object. Table~\ref{dataset_overview} summarizes the collection details for each dataset.

% Introduce UDA and why DG
To mitigate the domain gap between various 3D datasets, some works~\cite{wang2020train, saltori2020sf, yang2021st3d, xu2021spg, yihan2021learning, wei2022lidar, hu2023density, tsai2023viewer} have proposed unsupervised domain adaptation (UDA) methods for 3D object detection, which aims to transfer the detector trained on source domain to a new domain. Since these methods primarily focus on enhancing performance in the target domain, they cannot ensure high performance in unseen domains, rendering them less applicable for real-world usage. To this end, domain generalization (DG) aims to improve the generalization ability of 3D detectors, ensuring performance in unseen domains and robustness against domain gaps. Existing works~\cite{lehner20223d} use data augmentation to enhance model robustness against damaged or unusually shaped cars. However, DG techniques addressing other general domain issues, such as sensor configuration, remain relatively unexplored.

% Introduce our method
In this work, we propose a novel method to improve the generalization ability of a 3D object detector on a single source domain. As shown in Fig.~\ref{fig1}, the detector directly trained on the source domain experiences a significant performance drop when the point clouds density changes. Consequently, our work primarily focuses on mitigating the domain shift caused by different sensor configurations. Additionally, due to the efficiency and practicality of low-beam LiDAR for real-world applications, we generalize the detector from a high-beam source domain to low-beam target domains. We first selectively downsample the source data to a specific density, using confidence scores determined by the current detector to identify the density that holds utmost importance for the detector. With the augmented domain and the original domain, we employ the student-teacher framework with our proposed graph-based embedding relationship alignment (GERA) and feature content alignment (FCA) to instruct the model to overlook the density difference between the augmented domain and the original domain. GERA ensures that the high-level pairwise relationships remain consistent, while FCA serves to maintain low-level content consistency. Through training with these techniques, the 3D object detector becomes more robust to domain shifts.

% Experiments intro
% Not sure if we need this part now?
To assess the generalization ability of the detector trained with our method, we conduct experiments across various datasets, including Waymo~\cite{sun2020scalability}, KITTI~\cite{geiger2012we}, and nuScenes~\cite{caesar2020nuscenes}. Experimental results demonstrate that the detector trained using our method exhibits superior generalization ability. Remarkably, even without access to any target domain data, our method shows comparable performance to UDA methods. Another experiment also indicates that model trained with our method can also collaborate with UDA methods to achieve better performance on the target domain.

% Summarize contribution
Our contributions are summarized as follows:
\begin{itemize}
  \item We propose a method to improve generalization ability for 3D object detection on a single source domain.
  \item We introduce an augmentation strategy to downsample the point cloud to a specific sparsity based on the confidence of the detector.
  \item We develop graph-based embedding relationship alignment along with feature content alignment to maintain high-level relationship and low-level content consistency for domain-invariant representation learning.
  \item Extensive experiments show that our method outperforms all the baselines in terms of generalization. Furthermore, it can be compatible with UDA techniques to achieve even better performance on the target domain.
\end{itemize}

% Dataset overview table
\begin{table}[t]
    \scriptsize
    \centering

    \caption{Datasets overview. The dataset size refers to the total number of annotated frames.}
    \resizebox{\columnwidth}{!}{\begin{tabular}{ c | c c c c c}
        \hline
        Dataset & Size & \begin{tabular}[x]{@{}c@{}}LiDAR\\beams\end{tabular} & \begin{tabular}[x]{@{}c@{}}Points Per\\Beam\end{tabular} & \begin{tabular}[x]{@{}c@{}}Vertival\\ FOV(°)\end{tabular} & Location \\
        \hline \hline
        Waymo~\cite{sun2020scalability} & 230K & 64 & 2258 & [-17.6°, 2.4°] & USA\\ 
        KITTI~\cite{geiger2012we} & 15K & 64 & 1863 & [-23.6°, 3.2°] & Germany\\  
        nuScenes~\cite{caesar2020nuscenes} & 40K & 32 & 1084 & [-30.0°, 10.0°] & USA/Singapore \\   
        \hline
    \end{tabular}}

    \label{dataset_overview}
\end{table}