\label{experiment}

\begin{table*}[t!]
    \small
    \centering
    % \resizebox{\textwidth}{!}{
        \caption{Generalization results from the Waymo dataset using two different detection backbones. The reported AP is the moderate case for KITTI and the overall result for other datasets. The best score across all baselines is underlined, while the best score excluding domain adaptation (DA) methods is indicated in \textbf{bold}. }
        % Note that W, N, K denote Waymo, nuScenes, KITTI, respectively, and $\rightarrow$ signifies transfer or adapt to a specific dataset.}

        \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}
            { l | c | c c | c | c | c c | c }
            \toprule[1.5pt]
             \multirow{2}{*}{Methods} & 
             \multicolumn{4}{c|}{SECOND-IoU}  & 
             \multicolumn{4}{c}{PV-RCNN}
             \\
             
             \cline{2-9}  &
             W$^{1}$ & $\rightarrow$N$^{2}$ & $\rightarrow$K$^{3}$ & Avg. & 
             W & $\rightarrow$N & $\rightarrow$K & Avg.   
             \\ \hline
    
             Source Only & \textbf{\underline{67.75}/\underline{55.68}} & 32.91/17.24 & 67.64/27.48 & 56.10/33.47
             & \textbf{\underline{70.11}/\underline{58.56}} & 34.50/21.47 & 61.18/22.01 & 55.26/34.11 \\ \hline
             
             ROS~\cite{yang2021st3d} & 65.36/51.60 & 30.86/17.28 & 77.93/51.40 & 58.05/40.09
             & 62.24/47.23 & 29.79/18.85 &  78.82/\textbf{56.01} & 56.95/40.70 \\

             % TODO: Weird performance, retraining
             PA-AUG~\cite{choi2021part} & 67.50/53.98 & 28.79/17.47 & 77.23/46.12 & 57.84/39.19 
             & 68.01/56.76 & 32.11/20.06 & 71.91/38.39 & 57.34/38.40 \\
             
             RBRS~\cite{hu2023density} & 67.25/52.13 & 38.82/21.76 & 76.41/43.32 & 60.83/39.07
             & 68.08/56.10 & 41.58/25.12 & 77.31/34.72 & 62.32/38.65 \\ 
             
             Ours & 66.58/51.27 & \textbf{39.45/22.30} & \textbf{79.96/53.60} & \textbf{\underline{62.00}/\underline{42.39}}
             & 67.68/56.03 & \textbf{41.88/25.83} & \textbf{80.03}/48.25 & \textbf{\underline{63.20}/\underline{43.37}} \\ \hline
             
            ST3D($\rightarrow$N)~\cite{yang2021st3d} & 61.94/46.80 & 35.92/20.19 & 69.57/39.14 & 55.81/35.38 
            & 65.53/54.04 & 36.42/22.99 & 70.49/41.19 & 57.48/39.41 \\
            
            ST3D($\rightarrow$K)~\cite{yang2021st3d} & 53.22/34.13 & 25.54/10.19 & 82.19/61.83 & 53.65/35.38 
            & 59.37/42.34 & 30.59/18.44 & 84.10/64.78 & 58.02/41.85 \\
    
            DTS($\rightarrow$N)~\cite{hu2023density} & 42.67/30.56 & \underline{41.20}/\underline{23.00} & 66.45/30.32 & 50.11/27.96 
            & 46.10/39.99 & \underline{44.00}/\underline{26.20} & 77.69/39.27 & 55.93/35.15 \\
    
            DTS($\rightarrow$K)~\cite{hu2023density} & 49.18/31.38 & 27.43/18.48 & \underline{85.80}/\underline{71.50} & 54.14/40.45
            & 54.37/38.27 & 27.52/18.37 & \underline{86.40}/\underline{68.10} & 56.10/41.58 \\
            \bottomrule[1.5pt]
        \end{tabular}
        \end{adjustbox}
    % }
        \\
$^{1}$ denotes Waymo, $^{2}$ denotes transfer or adapt to nuScenes, and $^{3}$ denotes transfer or adapt to KITTI.
    \vspace{-.2cm}
   

    \label{table:MainResult}
\end{table*}

% Datasets, Implementation Details, Metrics
\subsection{Experiment Details}
% Datasets
\textbf{Datasets.} We conduct our experiments on three well-known autonomous driving datasets: Waymo~\cite{sun2020scalability}, KITTI~\cite{geiger2012we}, and nuScenes~\cite{caesar2020nuscenes}. As shown in Table~\ref{dataset_overview}, Waymo is the most dense and largest dataset. 
We choose to generalize mainly from the Waymo dataset for most of our experiments, while also utilizing the KITTI dataset to demonstrate generalization results by only altering the LiDAR beams for the original point clouds.

% Implementation Details
% \textbf{Implementation Details.} We validate our method using two detection backbones, namely SECOND-IOU~\cite{yang2021st3d} and PV-RCNN~\cite{shi2020pv}, to demonstrate our method is suitable for different detection architectures. We adopt the widely-used point cloud detection codebase OpenPCDet~\cite{openpcdet2020} for training the detector. Specifically, we train the detector for 30 epochs when Waymo is set as the source domain and 80 epochs when KITTI is set as the source domain.  During the augmentation phase, We typically downsample the original point clouds $2^k$ times, since most of LiDAR sensor contains $2^k$ beams. Additionally, we set the IoU threshold to 0.7 for confidence-based selection. To obtain instance proposal features through ROI-pooling from both the teacher's BEV feature and the student's BEV feature, we utilize the teacher's ROI map as the shared-ROI since the teacher model preserves better precision on the original data. The importance factor $\lambda$ for our relationship matrix $R$ in graph-based embedding relationship alignment is set to 1.0. The balance factors $\alpha$ and $\beta$ are set to 1.0 and 5.0, respectively.

% Metrics
\textbf{Metrics.} Following previous UDA work~\cite{yang2021st3d}, we adopt the KITTI evaluation metric for assessing our methods on the commonly used car category in KITTI, nuScenes, and the vehicle category in Waymo. We report the average precision (AP) over 40 recall positions under the IoU threshold of 0.7 for both the birdâ€™s eye view (BEV) IoUs and 3D IoUs. To demonstrate the generalization ability, we also compute the average of AP over all datasets. We also report \textbf{Closed Gap}~\cite{yang2021st3d} to demonstrate how much our method can help close the performance gap between Source Only and Oracle under UDA settings, which is defined as: $\textbf{Closed Gap} = \frac{AP_{model} - AP_{source~only}}{AP_{oracle} - AP_{source~only}} \times 100\% $. Note that all experiments evaluate the methods on the validation set. 


% Baselines, Main Results
\subsection{Comparison with State-of-the-art Methods}
We benchmark our method against both DG and UDA baselines as shown in Table~\ref{table:MainResult}. For DG baselines, we compare our method with augmentation strategies designed to enhance the detector's robustness to domain shifts, including ROS \textbf{(augmentation strategy of ST3D~\cite{yang2021st3d})}, RBRS \textbf{(augmentation strategy of DTS~\cite{hu2023density})}, and PA-AUG~\cite{choi2021part}. For DA baselines, we compare our method with ST3D~\cite{yang2021st3d} and DTS~\cite{hu2023density}.

We observe that previous augmentation techniques achieve better performance compared to Source only. ROS excels in the Waymo$\rightarrow$KITTI setting, addressing the primary domain gap related to size distribution. PA-AUG exhibits limited performance improvement, primarily benefiting objects with rare shapes. RBRS enhances the performance in the Waymo$\rightarrow$nuScenes scenario due to significant differences in point cloud density. Our method incorporates our proposed augmentation  and alignments strategies, which facilitates the learning of domain-invariant features, resulting in superior performance across unseen domains.

When comparing with UDA baselines, it is evident that while they perform well in their target domains, their performance across other domains is not guaranteed. In the Waymo$\rightarrow$nuScenes task, our method demonstrates comparable performance to UDA methods and even outperforms ST3D, which employs self-training for target domain data. This highlights the significance of improving the generalization ability of the detector. A robust generalization ability is also crucial for adapting to new domains, even when access to them is available.

We also validate our method with different detection backbones. This indicated that our method compatible with both the lightweight detector SECOND-IoU~\cite{yang2021st3d} and the two-stage detector PV-RCNN~\cite{shi2020pv}, which provides better precision.

\subsection{Ablation Study}
\begin{table}[t!]
    \centering
    \caption{Ablation study for our method. BDS refers to beam-based downsampling and CBS is the confidence-based selection strategy. The best score is \textbf{bolded}.}
    \begin{tabular}{c c | c c | c | c c | c}
    \toprule[1.5pt]

    \multicolumn{2}{c|}{Augmentation} & \multicolumn{2}{c|}{Alignments}
    & \multicolumn{4}{c}{$\SI{}{AP_{3D}}$} \\ \hline

    BDS & CBS & FCA & GERA & W$^{1}$ & $\rightarrow$N$^{2}$ & $\rightarrow$K$^{3}$ & Avg. \\ \hline

    & & & & \textbf{55.68} & 17.24 & 27.48 & 33.47 \\

    $\checkmark$ & & & & 52.87 & 20.15 & 43.92 & 38.98 \\
    $\checkmark$ & $\checkmark$ & & & 51.97 & 20.83 & 46.82 & 39.87 \\ \hline
    $\checkmark$ & $\checkmark$ & $\checkmark$ & & 50.90 & 21.50 & 51.96 & 41.45 \\
    $\checkmark$ & $\checkmark$ & & $\checkmark$ & 53.36 & 21.04 & 48.65 & 41.02 \\
    $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 51.27 & \textbf{22.30} & \textbf{53.60} & \textbf{42.39} \\
    
    \bottomrule[1.5pt]
    \end{tabular}

    
$^{1}$ denotes Waymo , $^{2}$ denotes transfer to nuScenes, and $^{3}$ denotes transfer to KITTI.
    \label{tab:ablation}
\end{table}

% \begin{table*}[ht!]
%     \centering
%     \caption{Generalization for different densities from KITTI dataset. We report $\SI{}{AP_{BEV}}$/$\SI{}{AP_{3D}}$ for KITTI validation set and the improvement of our method.}
%     \begin{tabular}{l | c | c c c c}
%     \toprule[1.5pt]
%          Methods & 64-beam & 32-beam & 32*-beam & 16-beam & 16*-beam \\ \hline
%          Source Only & \textbf{88.04/78.91} & 79.67/67.79 & 72.21/60.06 & 54.58/41.67 & 47.70/34.15 \\
%          Ours & 86.62/77.02 & \textbf{84.67/73.47} & \textbf{81.05/70.05} & \textbf{76.35/63.27} & \textbf{71.10/58.10} \\ \hline
%          \emph{Improvement} & -1.42/-1.89 & +5.00/+5.68 & +8.84/+9.99 & +21.77/+21.60 & +23.40/+23.95 \\
%     \bottomrule[1.5pt]
%     \end{tabular}
    
%     \label{tab:SDG}
% \end{table*}

To validate the effectiveness of each component for our method, we conduct the ablation study experiment as shown in Table~\ref{tab:ablation}. All experiments are conducted with the detector SECOND-IoU and use Waymo as the source domain.  

In the augmentation phase, applying only the beam-based downsampling significantly enhances overall generalization ability. However, this approach leads to unstable performance in unseen domains since the density used for training is randomly selected. By employing downsampling with the confidence selection strategy, the current detector dynamically chooses the density it needs to learn, leading to improved generalization ability by sacrificing a little performance on the original source domain. 

In our two alignments, FCA enforces the model to generate similar BEV features regardless of point cloud density, while GERA preserves high-level relationship consistency for instance pairs. Training with only FCA yields significant improvement for unseen domains; however, it struggles to preserve the performance on the original domain. On the other hand, the GERA loss enables the detector to learn how to maintain consistency in the relationships of instance embeddings, preserving more performance on the original domain but showing limited improvement in unseen domains.

Combining both alignment losses, our method achieves a balance between low-level and high-level consistency, resulting in the best generalization ability across all domains.

\begin{figure}[h!]
\centering
\includegraphics[width=1\columnwidth, clip]{./Figure/fig4.pdf}
\caption{Performance on KITTI dataset~\cite{geiger2012we} across various point cloud densities. Source Only indicates the direct evaluation of the model trained on the original dataset (64-beam) on other low-beam validation sets.  Ours denotes that the detector is trained using the proposed method. 
% For $32^{*}$-beam and $16^{*}$-beam, we subsample half of points in each beam from 32-beam and 16-beam respectively.
}
\label{fig4}
\end{figure}

\subsection{Single Dataset Generalization}
To validate the generalization ability of our method across different densities, we only modify the beam type of the original point clouds in this experiment. Note that we not only decrease the number of LiDAR beams but also reduce the number of points for each beam, \emph{i.e.}, 32*-beam means we subsample half of the points of the 32-beam data. In this experiment, we use SECOND-IoU as our backbone model. As shown in Fig.~\ref{fig4}, Source Only model suffers from a severe performance drop as the point cloud density becomes sparser, while our method maintains acceptable performance even when transferring to 16*-beam data. This experiment highlights that the model trained with our method becomes more robust to changes in point cloud density.


\subsection{Compatible with Domain Adaptation Methods}
\begin{table}[t!]
    \centering
    \caption{Domain adaptation of Waymo$\rightarrow$nuScenes setting. The best score is indicated in \textbf{bold}.}
    \begin{tabular}{l | c | c}
    \toprule[1.5pt]
         Methods & $\SI{}{AP_{BEV}}$/$\SI{}{AP_{3D}}$ & Closed Gap \\ \hline
         Source only & 32.91/17.24 & - \\
         Ours & 39.45/22.30 & +34.48\%/+28.70\% \\ \hline
         ST3D & 35.92/20.19 & +15.87\%/+16.73\% \\
         Ours(w/ ST3D) & \textbf{40.63/24.04} & \textbf{+40.70\%/+38.57\%} \\ \hline
         Oracle & 51.88/34.87 & - \\

    \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:With_DA}
\end{table}

Since our method doesn't use any target domain data during training, we can further compatible our method with DA techniques. We conduct this experiment by utilizing SECOND-IoU as our backbone model and evaluate the performance in Waymo$\rightarrow$nuScenes adaptation setting. In this experiment, we also report \textbf{Closed Gap} as mentioned in experiment details. As shown in Table~\ref{tab:With_DA}, our method significantly boosts ST3D~\cite{yang2021st3d} by 4.71/3.85 and 24.83\%/21.84\% for $\SI{}{AP_{BEV}}$/$\SI{}{AP_{3D}}$ and Closed Gap, respectively. This underscores the importance of our approach even when target domain data is accessible.