\label{method}

\subsection{Point Clouds Downsampling} 
Learning generalizable features from a single source domain is a challenging task, as we can only acquire knowledge within point clouds captured under same sensor configurations and similar scene distribution. To address this challenge, we can leverage our prior knowledge of LiDAR sensors to simulate point clouds across various densities.
\begin{enumerate}[label=\emph{\arabic*)}, wide]
    \item \emph{Beam-based Downsampling:} To downsample point clouds into specific beam types, we adopt the approach from \cite{wei2022lidar}. We first convert the Cartesian coordinates $(x, y, z)$ of points to spherical coordinates using the equations:
    \begin{equation}
        \begin{split}
        \theta = \arctan \frac{z}{\sqrt{x^2 + y^2}},\\
        \phi = \arctan \frac{y}{\sqrt{x^2 + y^2}},
        \end{split}
        \label{coord_trans}
    \end{equation}
    where $\theta$ and $\phi$ represent zenith and azimuth angles. For beam-based downsampling, it is necessary to split the points from different beams. Since some datasets lack beam labels for each LiDAR beam, the K-means algorithm is then applied on the zenith angle $\theta$ to assign beam labels to each point. With the obtained labels for each LiDAR beam, downsampling the original point clouds to specific types of point clouds corresponding to different beams becomes straightforward.
    
    \item \emph{Confidence-based Selection:} After downsampling original point clouds to point clouds with distinct densities, we then require a strategy to select one of them. We observe that randomly choosing one may result in an imbalanced performance across various densities. This is because random selection may sometimes lead to choosing a density that the model is already familiar with, diverting the model from the intended goal of generalization. To address this issue, we propose confidence-based selection to choose the density that that holds utmost importance for the current detector. Given the original point clouds $P^{s}$ and sets of augmented point clouds $\{P^{a}_{i}\}_{i=1}^{N}$, where $N$ is the number of augmented point clouds with different beam types, we can inference our current detector on  $\{P^{a}_{i}\}_{i=1}^{N}$ to obtain the matched objects $\{O_{i, j}\}_{j=1}^{N_{a}}$ for the $i$-th augmented point cloud by computing the Intersection over Union (IoU) between each predicted object and the ground truth object, where $N_{a}$ is the number of matched objects. Note that the matching process associates each predicted object with a ground truth object having the maximum IoU. With matched objects $\{O_{i, j}\}_{j=1}^{N_{a}}$, we can calculate the confidence for each augmented data using the following equation:
    \begin{equation}
        S{i} = \frac{\sum_{j=1}^{N_{a}}{\mathds{1}_{IoU>IoU_{th}} C_{i,j}}}{\sum_{j=1}^{N_{a}}{\mathds{1}_{IoU>IoU_{th}}}},
    \end{equation}
    where $C_{i,j}$ represents the confidence for the $j$-th matched object in the $i$-th augmented point cloud, and $S_{i}$ is the final confidence score for the $i$-th augmented point cloud. The term $IoU$ denotes the matched IoU for the object, and $IoU_{th}$ indicates the threshold for considering a matched object in the computation. To achieve a balance between the quantity and confidence for various densities, we additionally weighted the confidence score $S_{i}$ with the proportion of data we have already selected for each beam type. Subsequently, we select the augmented point cloud with the lowest score as our augmented domain. Fig.~\ref{fig3} illustrate the pipeline for our confidence-based selection strategy.
\end{enumerate}

With the augmented domain and the original domain, we employ the student-teacher framework for sparsity-invariant features learning. To ensure the accuracy of our teacher model on the original domain and guide the student model to generalize across various density levels, we feed the original data to the teacher model and the augmented data to the student model. Notice that the initial teacher model and the student model are pretrained on the original source domain with our proposed augmentation.

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\columnwidth, clip]{./Figure/fig3.pdf}
\caption{Illustration of our confidence-based selection strategy. $P^{s}$ and $P^{a'}$ denote the original point clouds and the final augmented point clouds, respectively, while $\{P^{a}_{i}\}_{i=1}^{N}$ indicates $N$ augmented point clouds with different beam types.}

\vspace{-.5cm}
% \vspace{-10pt}
\label{fig3}
\end{figure}

\subsection{Feature Content Alignment}
The purpose of employing the student-teacher framework is to train the detector to learn domain-invariant features. Since the only difference between the original domain and the augmented domain is the LiDAR beam number, the features generated by the teacher model fed with the original data and the student model fed with the augmented data should be similar. To this end, we can directly align the feature content between the student detector and the teacher detector.

Given the BEV features generated by both the student and teacher models, we use the shared-ROI map to extract instance proposal features from both sets of BEV features. This results in student instance proposal features $F^{a} \in \mathbb{R}^{N_{r} \times H \times W \times M}$ and teacher instance proposal features $F^{s} \in \mathbb{R}^{N_{r} \times H \times W \times M}$, where $N_{r}$ is the number of region proposals. $H$ and $W$ represent the height and width for a region proposal, and $M$ is the feature dimension. We then easily align these instance proposal features by the following equation:
\begin{equation}
    \mathcal{L}_{FCA} = \frac{\sum_{i=1}^{N_{r}}{\|f^{s}_{i} - f^{a}_{i}\|_{2}}}{N_{r}},
\end{equation}
where $f^{s}_{i}$ and $f^{a}_{i}$ are the $i$-th feature of $F^{s}$ and $F^{a}$, respectively.

\subsection{Graph-based Embedding Relationship Alignment}
By employing feature content alignment, we enforce the model to generate similar features regardless of the density change. However, it is also crucial for the relationship between each instance proposal feature pair to remain consistent, given that we only alter the beam type of the original point clouds.

To measure the discrepancy of the pairwise relationships, we can transform the features for each proposal into a fully-connected graph. This allows us to focus on computing the edge difference between two graphs constructed by the student instance proposal features and the teacher instance proposal features, respectively.

To construct a graph from the instance proposal features, we first convert the three-dimensional features $f^{s}_{i}$ and $f^{a}_{i}$ into one-dimensional embeddings by passing them through a multi-perception layer (MLP) to capture the high-level meanings for each proposal. Here, $f^{s}_{i}$ and $f^{a}_{i}$ represent the $i$-th feature of $F^{s}$ and $F^{a}$ mentioned above, respectively. The graph is then built, with the edges representing the cosine similarity between embeddings. The process can be described as:
\begin{equation}
    E^{s}_{i, j} = \frac{\Phi(f^{s}_{i}) \cdot \Phi(f^{s}_{j})}{\|\Phi(f^{s}_{i})\| \|\Phi(f^{s}_{j})\|},
\end{equation}
where $E^{s}_{i, j}$ denotes the edge between $i$-th embedding and $j$-th embedding for teacher and $\Phi$ is a MLP utilized to extract high-level embeddings from the three-dimensional features.

Given the edges $E^{s}_{i, j}$ and $E^{a}_{i, j}$ for teacher and student, respectively, we then utilize Gromov-Wasserstein discrepancy~\cite{peyre2016gromov} to define our graph-based embedding relationship alignment loss as the following equation:
\begin{equation}
    \mathcal{L}_{GERA} = \sum_{i, j, m, n} KL(E^{a}_{i,j}, E^{s}_{m, n})R_{i, m}R_{j, n},
\end{equation}
where $KL$ indicates the Kullback-Leibler divergence to measure the distance of the edges across graphs. $R$ is the relationship matrix to represent the difference distance between each proposal.

In addition to the completely matched edge pairs, the relationship between edge pairs containing similar instance proposals should also remain consistent across graphs. To address this, we initially construct a discrepancy matrix by calculating the differences in center, size, and orientation between each instance proposal. The discrepancy matrix $D$ is then formulated as below:
\begin{equation}
    D_{i,m} = \frac{1}{\|c_{i}-c_{m}\|_{2}+\|d_{i}-d_{m}\|_{2}+\|r_{i}-r_{m}\|_{1}+\epsilon},
\end{equation}
where $c_{i}$, $d_{i}$ and $r_{i}$ denote the center, size, and orientation for the $i$-th proposal, respectively. $\epsilon$ is a factor used to scale the discrepancy value and prevent zero division errors.

After computing the discrepancy matrix, we define the relationship matrix $R$ as:
\begin{equation}
    R = I + \lambda D,
\end{equation}
where $I$ is the identity matrix, preserving the weights for the totally matched edge pairs, and  $\lambda$ controls the importance of the discrepancy matrix.

\subsection{Overall Loss Function}
In the student-teacher framework, we freeze the parameters of the teacher model, while the parameters of the student model are updated by the original detection loss $\mathcal{L}_{det}$, feature content alignment loss $\mathcal{L}_{FCA}$, and graph-based embedding relationship alignment loss $\mathcal{L}_{GERA}$. This can be expressed as the following equation:
\begin{equation}
\mathcal{L}_{overall} = \mathcal{L}_{det} + \alpha \mathcal{L}_{FCA} + \beta \mathcal{L}_{GERA},
\end{equation}
Where $\alpha$ and $\beta$ are hyperparameters to balance two alignment losses. 

With these two alignments, our goal is to guide the student detector to align both the low-level content and the high-level relationship between the augmented domain and the original domain for sparsity-invariant feature learning.