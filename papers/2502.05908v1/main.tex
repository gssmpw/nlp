%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
 
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

\usepackage{amsfonts}       % blackboard math symbols

\usepackage{microtype}      % microtypography
\usepackage{mathtools}
\usepackage{amsmath, amsthm, amssymb, bm, bbm}
\input{math_commands}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{soul}
\usepackage{wrapfig}
\usepackage{adjustbox}

% \usepackage{algorithm}
% \usepackage{algpseudocode}

% For theorems and such

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{enumerate}% http://ctan.org/pkg/enumerate

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% authors
\newcommand{\MN}{LD-SMC}
\newcommand\ia[1]{\textcolor{orange}{[IA: #1]}}
\newcommand\an[1]{\textcolor{blue}{[AN: #1]}}
\newcommand\ef[1]{\textcolor{red}{[EF: #1]}}
\newcommand\id[1]{\textcolor{purple}{[ID: #1]}}
\newcommand\ignore[1]{{}}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Inverse Problems as State-Space Models}
\newcounter{phase}[algorithm]
\newlength{\phaserulewidth}
\newcommand{\setphaserulewidth}{\setlength{\phaserulewidth}}
\setphaserulewidth{.7pt}
\begin{document}
\newcommand{\phase}[1]{%
  % 
  % Top phase rule
  \STATE\leavevmode\llap{\rule{\dimexpr\labelwidth+\labelsep}{\phaserulewidth}}\rule{\linewidth}{\phaserulewidth}
  \vspace{-3ex}
  \STATE\strut\refstepcounter{phase}\textit{Stage~\thephase~--~#1}% Phase text
  % Bottom phase rule
  \vspace{-1.25ex}\STATE\leavevmode\llap{\rule{\dimexpr\labelwidth+\labelsep}{\phaserulewidth}}\rule{\linewidth}{\phaserulewidth}}

\twocolumn[
\icmltitle{Inverse Problem Sampling in Latent Space Using Sequential Monte Carlo}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Idan Achituve}{Sony}
\icmlauthor{Hai Victor Habi}{Sony}
\icmlauthor{Amir Rosenfeld}{Sony}
\icmlauthor{Arnon Netzer}{Sony}
\icmlauthor{Idit Diamant}{equal,Sony}
\icmlauthor{Ethan Fetaya}{equal,barilan}
\end{icmlauthorlist}

\icmlaffiliation{Sony}{
Sony Semiconductor Israel (SSI), Israel}
\icmlaffiliation{barilan}{Faculty of Engineering, Bar-Ilan University, Israel}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Idan Achituve}{Idan.Achituve@sony.com}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In image processing, solving inverse problems is the task of finding plausible reconstructions of an image that was corrupted by some (usually known) degradation model.
Commonly, this process is done using a generative image model that can guide the reconstruction towards solutions that appear natural. The success of diffusion models over the last few years has made them a leading candidate for this task. However, the sequential nature of diffusion models makes this conditional sampling process challenging. Furthermore, since diffusion models are often defined in the latent space of an autoencoder, the encoder-decoder transformations introduce additional difficulties. Here, we suggest a novel sampling method based on sequential Monte Carlo (SMC) in the latent space of diffusion models. We use the forward process of the diffusion model to add additional auxiliary observations and then perform an SMC sampling as part of the backward process. Empirical evaluations on ImageNet and FFHQ show the benefits of our approach over competing methods on various inverse problem tasks.
% In recent years, diffusion models have been utilized as powerful priors for solving inverse problems on image data. The goal in inverse problems is to find plausible reconstructions of an image that was corrupted by some known degradation model. Given the sequential nature of diffusion models, it is sensible to view this task as performing Bayesian inference in a state-space model. The observed variable is the corrupted image, the diffusion model defines a Markovian dynamics, and the aim is to infer a latent quantity (i.e., a clean image). However, when the diffusion process is defined in the latent space of an auto-encoder, tractable Bayesian inference becomes more challenging. Here, we suggest a simple solution based on sequential Monte Carlo (SMC) sampling. We use the forward process of the diffusion model to augment the model with additional observations and then perform posterior sampling by solving an SMC sampling problem. We name our method Gibbs Diffusion Sampling (\MN). \id{need to change Gibbs Diffusion sampling.}
%  We use the forward process of the diffusion model to augment the model with additional observations and then perform posterior sampling by solving an SMC sampling problem. We name our method Gibbs Diffusion Sampling (\MN).
\end{abstract}

\section{Introduction}
\label{Introduction}
Many important signal processing tasks can be viewed as inverse problems \cite{song2021scorebased,Moliner2023audio,Daras2024survey,chungdiffusion,cardoso2023monte}. In inverse problems, the objective is to obtain a clean signal $\rvx \in \R^n$ from a degraded observation $\rvy=\gA(\rvx)+\rvpsi$, where $\gA$ is usually a known irreversible mapping and $\rvpsi$ is a Gaussian noise vector. Common applications that fit this framework include image deblurring, super-resolution, inpainting, and Gaussian denoising. 
The broad applicability of inverse problems makes them highly significant, as they encompass numerous real-world challenges, such as those found in digital image processing \cite{blackledge2005digital}, wireless communication \cite{Chen2021}, seismology \cite{Virieux2009seismology}, medical imaging \cite{song2021scorebased,chung2023solving}, and astronomy \cite{Craig1986book}.
% The wide-ranging applicability of inverse problems underscores their importance, as they address a multitude of practical challenges, including those found in wireless communication, seismology, medical imaging, and astronomy.

%Inverse problems are common in many scientific fields, such as, wireless communication, seismology, medical imaging, and astronomy \ia{[.... Idit - can you please add/alter the text here and add references?]}. 
% In inverse problems, one is given an observation $\rvy \in \R^m$

% a clean signal $\rvx \in \R^n$ undergoes a transformation by a 

% one is given a degraded signal $\rvy$ of some clean, unknown, signal $\rvx$. 

%Many important signal-processing and image-processing tasks can be viewed as inverse problems. 
%In inverse problems, the objective is to retrieve a signal $\rvx\in\R^d$ from a degraded observation $\rvy=\gA(x)+\rvpsi$, where $\gA$ is a known mapping and $\rvpsi$ is a Gaussian noise.

%there is a clean signal $\rvx\in\R^d$ and you are given a corrupted version of it with a known corruption model $\rvy=\gA(x)+\rvpsi$ where $\gA$ is a corruption operator and $\rvpsi$ is a Gaussian noise. 
%\id{suggestion: In inverse problems, the objective is to retrieve a signal $\rvx\in\R^d$ from a degraded observation $\rvy=\gA(x)+\rvpsi$, where $\gA$ is a known mapping and $\rvpsi$ is a Gaussian noise.}

% Common applications that align with this framework encompass tasks such as image deblurring, super-resolution, inpainting, and Gaussian denoising. 

A major challenge in solving inverse problems is the existence of multiple plausible solutions. For example, in image inpainting, the likelihood $p(\rvy|\rvx)$ remains constant regardless of how the absent pixels are filled. However, the desired solution is one that not only fits the observation, but also appears natural, which corresponds to having a high probability under a natural image prior $p(\rvx)$. This insight naturally leads to the approach of sampling from the posterior distribution $p(\rvx|\rvy)\propto p(\rvy|\rvx)p(\rvx)$, combining the data likelihood and the prior to achieve realistic and data-consistent solutions.

%For example, in image inpainting, any way we fill the missing pixels will have the same likelihood $p(\rvy|\rvx)$ value. 
%\id{suggestion: For example, in image in-painting, the likelihood $p(\rvy|\rvx)$ remains constant regardless of how the absent pixels are filled.}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/overview_fixres.jpg}
    \caption{\MN{} solves inverse problems in the latent space by first augmenting the model with auxiliary observations using the forward diffusion process. Then, sampling from the posterior distribution $p_\theta(\rvz_0 | \rvy_0)$ is done based on the backward diffusion process using sequential Monte Carlo. In the figure, $\gD$ and $\gA$ denote the decoder and the corruption operator respectively.}
    \label{fig:enter-label}
\end{figure}

With the impressive recent advances in diffusion models \cite{sohl2015deep,ho2020denoising,SongME21}, there has been a significant interest in leveraging them as prior image models to solve inverse problems. However, integrating diffusion models into this context is not straightforward because of their sequential sampling process. Specifically, diffusion sampling involves iterative drawing from $p(\rvx_{t-1}|\rvx_t)$, while the conditioning on the corrupted image $\rvy$ is defined only in the final step, namely, through $p(\rvy|\rvx_0)$. This mismatch makes direct sampling from the joint posterior $p(\rvx_0,...,\rvx_T|\rvy)$ particularly challenging.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/imagenet_reconstructions.jpg}
\caption{Comparison between \MN{} and baseline methods on inpainting of images from the ImageNet dataset.}
\label{fig:in_ip_recon}
\end{figure*}

A simple solution proposed in \cite{chungdiffusion} is to approximate $p(\rvy|\rvx_t)$ with $p(\rvy|\mathbb{E}[\rvx_0|\rvx_t])$ for efficient sampling. Further work in \cite{wu2024practical} applied a sequential Monte Carlo (SMC) process to correct for this approximation. Another recent approach proposed in \cite{dou2024diffusion} connected $\rvx_t$ to $\rvy$ by introducing a sequence of latent variables $\rvy_t$ and sampling sequentially from  $p(\rvx_{t:T}|\rvy_{t:T})$. While this approach has shown great potential, it is limited to linear corruption models. As such, it is not applicable for non-linear mappings $\gA$ or common Latent Diffusion Models (LDMs) \cite{rombach2022high} due to the nonlinearity of the decoder. This is a serious restriction, as many of the recent powerful and efficient models are LDMs \cite{esser2024scaling}. 


% Since pixel-space diffusion process can be costly, developing models that effectively address inverse problems with latent diffusion models holds significant value. However, compared to pixel-space diffusion models, latent-space diffusion models have been less studied in the context of inverse problems. 

% The latter approach has shown great potential; however, it is limited to linear corruption models only, and as such, cannot be used with common latent diffusion models \cite{rombach2022high} due to the involvement of a non-linear decoder.

 
%And indeed recently, there have been attempts in that direction (e.g., \cite{song2023solving, rout2024beyond}); however, as we will show in the experimental section, they can often achieve sub-par performance. 

%Furthermore, recently there have been attempts to solve inverse problems using latent diffusion models \cite{song2023solving, rout2024beyond}. 
%We emphasize here that due to the nonlinear decoder in latent diffusion, some previous studies that also used SMC sampling \cite{cardoso2023monte, dou2024diffusion}, cannot be applied as they are suited for problems with a linear corruption model only.

Both existing approaches have pros and cons. Using the $p(\rvy|\mathbb{E}[\rvx_0|\rvx_t])$ approximation can be helpful in capturing the large scale semantics of the image, but it might not be well suited for capturing the small details. On the other hand, the auxiliary $\rvy_{1:T}$ can aid in the finer details and could be used with LDMs. Here, we propose a method that combines these two approaches and strives to achieve the best of both worlds. Specifically, we augment the model with additional latent variables $\rvy_t$, one for each time step, and then apply posterior inference over the latent diffusion variables $\rvz_t$. To obtain a tractable sampling procedure, we derive a novel posterior approximation and define a new proposal distribution for the SMC sampling process \cite{doucet2001sequential, del2012adaptive}. Hence, we name our method Latent Diffusion Sequential Monte Carlo, or more concisely \MN. Importantly, as our approach utilizes these auxiliary variables in the sampling process, we found that it was better suited to more challenging inverse problems such as inpainting. An illustration of our approach is shown in \Figref{fig:enter-label}.

% because our model relies on all $\rvy_t$ through an SMC procedure, and not only on $\rvy_0$ through $\mathbb{E}[\rvx_0|\rvx_t]$, it is able to generate fine details in the resulting image, a property often missing in baseline methods.


% Here, we propose a novel sampling algorithm which is applicable on latent diffusion models. We define a generative model for the data based on the forward diffusion process and perform posterior inference using sequential Monte Carlo (SMC) \cite{doucet2001sequential, del2012adaptive} as part of the backward diffusion process. Specifically, we augment the model with additional latent variables $\rvy_t$, one for each time step, and then apply posterior inference over the latent diffusion variables $\rvz_t$. To obtain a tractable sampling procedure, we derive a novel posterior approximation and define a novel proposal distribution for the SMC sampling process. Hence, we name our method Latent Diffusion sequential Monte Carlo, or more concisely \MN. Importantly, because our model relies on all $\rvy_t$ through an SMC procedure, and not only on $\rvy_0$ through $\mathbb{E}[\rvx_0|\rvx_t]$, it is able to generate fine details in the resulting image, a property often missing in baseline methods.

 We empirically validated our approach on the ImageNet \cite{ILSVRC15} and FFHQ \cite{KarrasLA19} datasets. \MN{} usually outperforms or is comparable to baseline methods on image deblurring and super-resolution tasks, and can significantly improve over baseline methods on inapainting tasks, especially on ImageNet which has more diversity in it.


%that defines different latent variables $\rvy_t$ to help better guide the sampling process and design a new sampling algorithm based on Gibbs sampling and importance sampling to generate samples from the posterior. 

%Moreover, we found that using \cite{chungdiffusion} as the proposal distribution significant improved performance, allowing us to get the best of both approaches. 

To conclude, in this study we make the following contributions: (1) A novel SMC procedure for solving inverse problems using latent diffusion models; (2) A novel posterior approximation and proposal distribution to perform approximate posterior sampling; (3) \MN{} outperforms baseline methods, especially on challenging inpainting tasks.


% Diffusion models \cite{sohl2015deep, ho2020denoising} have made significant progress in data synthesis in recent years, transforming areas such as image synthesis, video generation and audio synthesis. These models, known for their exceptional sample quality, offer robust data priors capable of capturing the complexities of high-dimensional data distributions. By leveraging these priors, it is possible to recover data from incomplete or noisy measurements using Bayesian posterior sampling. This approach has led to a range of diffusion-based techniques for solving linear inverse problems without the need for task-specific training. Notable applications include image inpainting, colorization, super-resolution and motion deblurring.

% Although diffusion models have shown impressive success in solving inverse problems, obtaining exact Bayesian posterior samples for these models remains computationally intractable, necessitating approximation methods. Various methods have been proposed to address this problem \cite{}. One strategy revises the conventional diffusion sampling procedure by maintaining data consistency at each time interval, thereby guaranteeing that all intermediate samples are consistent with the observed noisy measurements \cite{}. An alternative method is to approximate the score function of the Bayesian posterior and leverage it to direct each step in the diffusion sampling process \cite{}. The third strategy involves training a neural network to reduce the statistical divergence between the distribution of its samples and the true Bayesian posterior of the diffusion model. For the first two strategies, while each sampling iteration is theoretically grounded, assessing the cumulative errors introduced over multiple iterations remains a difficult theoretical challenge. The third approach, on the other hand, requires the training of separate neural networks for each specific task, leading to considerable computational overhead, particularly when applying the same diffusion prior across a range of inverse problems.

% Given the sequential nature of diffusion models, it is sensible to view this task as performing Bayesian inference in a state-space model. 
% Several approaches \cite{dou2024diffusion,wu2024practical,cardoso2023monte} have modeled inverse problems in this way, treating them as state-space models. 
% However, these methods were designed for diffusion models in pixel space, and with the exception of \cite{cardoso2023monte} they assume a linear degradation model.

% % The observed variable is the corrupted image, the diffusion model defines a Markovian dynamics, and the aim is to infer a latent quantity (i.e., a clean image). 
% When the diffusion process is defined in the latent space of an auto-encoder, tractable Bayesian inference becomes more challenging. 
% Here, we suggest a simple solution to address this issue using Gibbs sampling. We use the forward process of the diffusion model to augment the model with additional observations and then perform posterior sampling by solving a filtering problem.
% We demonstrate our method effectiveness on standard inverse problem tasks such as in-painting and super-resolution on the ImageNet and FFHQ datasets.


\section{Background}
\label{sec:background}
% \textbf{Notations.} Scalars are denoted with lower-case letters (e.g., $x$), vectors with bold lower-case letters (e.g., $\rvx$), and matrices with bold upper-case letters (e.g., $\rmX$).

\textbf{Inverse Problems.} In inverse problems one would like to recover a sample $\rvx \in \sR^{n}$ from a corrupted version of it $\rvy \in \sR^{m}$. Usually, the corruption model that acted on $\rvx$ is known, but the operation is irreversible \cite{tarantola2005inverse}. For instance, restoring a high-quality image from a low-quality one.
%For instance, cleaning an image from an additive noise sampled from a known noise distribution. 
We denote the corruption operator by $\gA(\cdot)$, %we assume it is governed by a set of parameters $\phi$.
and assume that $\rvy = \gA(\rvx) + \rvpsi$, where $\rvpsi \sim \normal(0, \tau^2\rmI)$ has a known standard deviation $\tau$. In a more concise way, $p(\rvy | \rvx) =\normal(\gA(\rvx), \tau^2\rmI)$. Common examples of inverse problems are inpainting, colorization, and deblurring. In general, solving inverse problem tasks is considered an ill-posed problem with many possible solutions $\rvx$ with equally high $p(\rvy|\rvx)$ values. Given a prior distribution $p(\rvx)$ over natural images, one standard approach to solving the inverse problem is to sample the posterior distribution $p(\rvx | \rvy) \propto p(\rvy | \rvx) p(\rvx)$.
%Given the ill-posed nature of this problem, an appealing option is to sample from the posterior distribution $p(\rvx | \rvy) \propto p(\rvy | \rvx) p(\rvx)$. Hence, different prior specifications can lead to different recoveries and may have detrimental effect on the quality of the recovered image.


\textbf{Diffusion Models.} Owing to their high-quality generation capabilities, in recent years diffusion models \cite{sohl2015deep, ho2020denoising} have been leveraged as priors in inverse problems \cite{jalal2021robust, songscore}. Here, we adopt the DDIM formulation \cite{SongME21} for the prior model, although our approach can work with other diffusion model formulations as well. Furthermore, since it is costly to apply the diffusion process in the pixel space, a common approach is to apply the diffusion model in the latent space given by an auto-encoder \cite{rombach2022high}. %Albeit, our method is suited for the pixel space as well.
Applying diffusion models in the latent space allows us to sample high-quality images while reducing the computational resources needed by the model. Hence, designing models that effectively solve inverse problems using latent diffusion models is of great importance.

Denote by $\rvz_{1:T}$ the random variables in the latent space.  % In what follows, we use DDPM notations \cite{ho2020denoising}. 
Let $\alpha_{1:T}$, $\beta_{1:T}$ be the variance schedule of the diffusion process with $\beta_t \coloneqq 1 - \alpha_t$. Also, denote by $\bar{\alpha}_t \coloneqq \prod_{j=1}^t \alpha_j$. Then, the DDIM sampling is done according to $p_{\theta}(\rvz_{t-1} | \rvz_{t}) = \normal(\rvz_{t-1} | \rvmu_\theta(\rvz_t, t), \rmSigma(t))$, where $\theta$ are the parameters of the neural network and, 
\begin{equation}
\begin{aligned} 
    \rmSigma(t) &= \sigma_t^2 \rmI\\
    \rvmu_\theta(\rvz_t, t) &= \sqrt{\bar{\alpha}_{t-1}}\left(\frac{\rvz_t - \sqrt{1 - \bar{\alpha}_t} \cdot \rvepsilon_\theta (\rvz_t, t)}{\sqrt{\bar{\alpha}_t}}\right) + \\
    & ~~~~ \sqrt{1 - \bar{\alpha}_{t-1} - \sigma^2_t} \cdot \rvepsilon_\theta (\rvz_t, t).
\end{aligned}
\label{eq:ddim_sampling}
\end{equation}
As in \cite{dou2024diffusion} we fix $\sigma_t = \eta \cdot
\sqrt{\beta_t \cdot \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}}$ with $\eta$ being a hyper-parameter. We denote the approximate posterior mean of $\E[\rvz_0 | \rvz_t]$ by $\bar{\rvz}_0(\rvz_t) \coloneqq \frac{1}{\sqrt{\bar{\alpha}_t}} (\rvz_t - \sqrt{1 - \bar{\alpha}_t} \cdot \rvepsilon_\theta (\rvz_t, t))$ \cite{robbins1956empirical, efron2011tweedie, chungdiffusion}. % = \frac{1}{\sqrt{\bar{\alpha}_t}} (\rvz_t + (1 - \bar{\alpha}_t) \cdot \rvs_\theta (\rvz_t, t))$, where $\rvs_\theta = \nabla_{\rvz_t} \log(p_{\theta}(\rvz_t))$ is an approximation of the score function. 

\textbf{Sequential Monte Carlo (SMC).} SMC is an important technique for sampling in probabilistic graphical models in which exact posterior inference is intractable. The SMC breaks the sampling process down to intermediate steps, allowing efficient sampling through a recursive procedure \cite{doucet2001sequential, del2012adaptive, naesseth2019elements, chopin2020introduction}. 

One family of probabilistic models for which SMC is especially known is state-space models (SSMs), also known as Hidden Markov Models (HMMs). In general, the following quantities need to be defined in SSMs, (1) a prior distribution over the initial state $p(\rvz_T)$, (2) a transition distribution that defines the dynamics between states $p(\rvz_t | \rvz_{t+1}) ~ \forall t < T$, and (3) a measurement model $p(\rvy_t | \rvz_t) ~ \forall t < T$. The goal is to sample from the posterior distribution $p(\rvz_{t:T} | \rvy_{t:T-1})$. To do so, SMC starts by sampling $N$ particles $\{\rvz^{(i)}_T\}_{i=1}^N$ from the prior distribution. Then, at each step, given the previous particle set $\{\rvz^{(i)}_t\}_{i=1}^N$ new samples are taken from a proposal distribution $\rvz^{(i)}_{t-1} \sim \pi(\rvz^{(i)}_{t-1}|\rvz^{(i)}_t)~\forall i \in \{1, ..., N\}$. The particles are then weighted and resampled according to the new proposed sequences $\{\rvz^{(i)}_{t-1:T}\}_{i=1}^{N}$.
The proposal distribution serves as an approximation to the posterior distribution. Its support needs to contain the support of the posterior density. The weighting function corrects the approximation by assigning a weight to each particle to adjust its probability. And the resampling step aims to remove unlikely particles according to the model \cite{sarkka2013bayesian}.
 
%the particles are replaced by new particles that are informed with the data from the current step. Importantly, the sampling process relies on two elements, a proposal distribution and a weighting function. 

%\ef{Given $z^{(i)}_t$ sample from proposal $\pi(z_{t-1}|z^{(i)}_t$)   then given our proposed new sequences $z_{t-1},z^{(i)}_{t:T}$ resample }


\section{Related Work}
\label{sec:related}

Inverse problems have a long and evolving history, with methodologies that have undergone significant advances over the years \cite{Daras2024survey}.
Recently, diffusion models \cite{sohl2015deep, ho2020denoising,songscore} have emerged as effective priors for solving inverse problems in image data \cite{wangzero,kawar2022denoising,chungdiffusion,dou2024diffusion,rout2024solving,song2023pseudoinverse,sun2024provable,Choi2021ILVRCM,chung2023parallel}. 
%We divide our literature review to inverse problem methods in the pixel space and inverse problem methods in the latent space, and note here that several methods can be applied in both settings.
% Another perspective approaches inverse problems from the viewpoint of Bayesian inference. 
% Certain techniques utilize diffusion models as priors to generate plausible reconstructions by sampling from the posterior distribution, for example.


% \textbf{Diffusion-based Inverse Problems in Pixel Space.}
% DDRM \cite{kawar2022denoising} and DDNM \cite{wangzero} utilize diffusion models as prior solving linear inverse problems in pixel space by approximating the measurement matching score, $\nabla \log p(\rvy|\rvx_t)$. 
In \cite{songscore} it was shown that to sample from the posterior distribution, $p(\rvx_0 | \rvy)$, one can solve a stochastic differential equation based on the prior score, $\nabla_{\rvx_t} \log~p_{\theta}(\rvx_t)$, and the conditional score, $\nabla_{\rvx_t} \log~p_{\theta}(\rvy | \rvx_t)$. Although the first term is easy to compute, the latter term requires integration over the full diffusion path from time $t$ to $0$. A useful and easy-to-calculate approximation found in several studies is $p_{\theta}(\rvy | \rvx_t) \approx p_{\theta}(\rvy | \E[\rvx_0 | \rvx_t])$, which is readily available at each step \cite{chungdiffusion, song2023pseudoinverse, wu2024practical}.
Specifically, Diffusion Posterior Sampling (DPS) \cite{chungdiffusion} uses this approximation for linear and non-linear inverse problems with Gaussian and Poisson likelihood models.
%explicit approximations for the measurement matching term with $\mathbb{E}[\rvx_0|\rvx_t]$, 
% approximating $\nabla \log p(\rvy|\rvx_t)$ with $\nabla \log p(\rvy|\mathbb{E}[\rvx_0|\rvx_t])$, 
%addressing non-linear inverse problem scenarios.
$\Pi$GDM \cite{song2023pseudoinverse} introduces pseudoinverse guidance by matching the denoising output and the corrupted image $\rvy$, via transformation of both through a 'pseudoinverse' of the corruption model. %However, it was observed that relying on that approximation alone may miss fine details in the image \cite{rout2024beyond}.
%  guidance-based approach for inverse problem solving that handles measurements with Gaussian noise, as well as some non-linear, non-differentiable measurement models
DDNM \cite{wangzero} suggested to refine only the contents of the null space during the backward diffusion process. As such it is suited only for linear inverse problems. 
%utilizes diffusion models as prior for solving linear inverse problems by decomposing the linear operator $\sA$ into t%in pixel space by approximating the conditional score, $\nabla_{\rvx_t} \log p_{\theta}(\rvy|\rvx_t)$, using $\mathbb{E}[\rvx_0|\rvx_t,\rvy]$.
% Asymptotically Exact Methods
An additional category of inverse problem approaches that use diffusion models is designed with the objective of asymptotic exactness \cite{cardoso2023monte, trippe2023diffusion, wu2024practical, dou2024diffusion}. 
%SMC-Diff \cite{trippe2023diffusion}, MCGDiff \cite{cardoso2023monte}, and TDS \cite{wu2024practical} 
These methods utilize SMC techniques targeting exact sampling from the posterior distribution $p(\rvx_0 | \rvy)$. Specifically, 
% \ia{for the first two please add why they are not relevant/why we didn't compre to them}.
SMC-Diff \cite{trippe2023diffusion} applies particle filtering for inpainting in motif scaffolding, and
% offers asymptotic guarantees solely under the assumption that the trained diffusion model perfectly aligns with the forward noising process, a condition rarely met in practical situations.
MCGDiff \cite{cardoso2023monte} is designed for linear inverse problems only. Hence, both approaches are not suited for inverse problems with latent-space diffusion models. %, with inpainting serving as a specific example.
TDS \cite{wu2024practical}, a recent SMC-based method, solves general inverse problem tasks using the twisting technique. This method also uses the approximation of DPS, but by applying SMC sampling it can correct for it.


%Importantly, the connection to the observation is made only through $\E[\rvx_0 | \rvx_t]$. 
FPS \cite{dou2024diffusion} is also a recent method based on SMC with auxiliary variables. FPS generates a sequence of observations $\rvy_{1:T}$ based on a duplex diffusion process, one process at the $\rvx$ space and the other process at the $\rvy$ space. Since this method is designed for linear inverse problems only, it permits tractable Bayesian inference. Our method combines the ideas of both TDS and FPS to obtain the best of both. Namely, we use the posterior mean approximation and $\rvy_{1:T}$ in our SMC sampling process. As we will show, this combination can be helpful in both understanding the general semantics of an image and capturing fine details. 

% demonstrated considerable potential linking posterior sampling with Bayesian filtering and adeptly tackling the filtering problem utilizing sequential Monte Carlo methods. However, this approach is confined to linear corruption models. As we will show in \Secref{sec:method} our method combines TDS and FPS to solve general inverse problems.
% The FPS methodology \cite{dou2024diffusion} links posterior sampling with Bayesian filtering and adeptly tackles the filtering problem utilizing sequential Monte Carlo methods. This approach demonstrates considerable potential, but it is confined to linear corruption models.

% Recently, several approaches \cite{dou2024diffusion,wu2024practical,cardoso2023monte} have modeled inverse problems as Bayesian inference in a state-space model.
% However, these methods were designed for diffusion models in pixel space.
% and with the exception of \cite{cardoso2023monte} they assume a linear degradation model.

Several inverse sampling methods were specifically tailored for latent diffusion models. 
PSLD \cite{rout2024solving} extend DPS \cite{chungdiffusion} by incorporating an additional gradient update step to guide the diffusion process to sample latent representations that maintain the integrity of the decoding-encoding transformation, ensuring it remains non-lossy.
% for which the decoding-encoding map is not lossy.
STSL \cite{rout2024beyond} presents a novel sampler with a tractable reverse process using an efficient second-order approximation. 
Comparative analysis with STSL was not feasible due to the absence of publicly available code, making replication challenging.
% a Second-order Tweedie sampler from Surrogate Loss.  
Resample \cite{song2023solving}, a contemporary method alongside PSLD, introduces a strategy for addressing general inverse problems using pretrained latent diffusion models, tackling the complexities posed by encoder and decoder nonlinearity.
Resample algorithm includes hard data consistency to obtain latent variable that is consistent with the observed measurements, and then employs a resampling scheme to map the sample back onto the correct noisy data manifold and continue the reverse sampling process. 
Concurrent to this study \citet{nazemi2024particle} proposed a particle filtering approach. Their method builds on PSLD and DPS update in the proposal distribution. Similarly to TDS \cite{wu2024practical} the connection to the labels is only through $\rvz_0$ using the approximate mean estimator. Since these methods share commonalities, we compare only to the latter in the experimental section. 


% refined based on a proposal distribution and a weighting function. The proposal distribution serves as an approximation to the posterior distribution and should uphold two conditions: (1) its support contains the support of the posterior density and (2) it is easy to sample from. The weighting function corrects the approximation by assigning a weight for each particle.
%Let $\pi(\rvz_t | \rvz_{t+1})$ denote the proposal density at time step $t$


%which corresponds to a Markovian forward process as in \cite{dou2024diffusion} 

% \textbf{Particle Filtering.} Particle filtering is a technique to sample from a posterior distribution in state space models \cite{sarkka2013bayesian}. The key idea   

\begin{figure}[!t]
\centering
\scalebox{0.8}{
    \begin{tikzpicture}[
        scale=1.0, % Adjust the scale to shrink the entire figure
        node distance=0.4cm and 0.8cm,
        observed/.style={circle, draw, fill=gray!20, minimum size=1.2cm},
        latent/.style={circle, draw, minimum size=1.2cm},
        arrow/.style={-{Stealth[scale=1.2]}}
    ]
    
    % Nodes for observations # , opacity=0
    \node[latent, opacity=0] (yT) {$\rvy_T$};
    \node[latent, right=of yT] (yT_1) {$\rvy_{T-1}$};
    \node[right=of yT_1] (dots) {$\cdots$};
    \node[latent, right=of dots] (y1) {$\rvy_1$};
    \node[observed, right=of y1] (y0) {$\rvy_0$};
    
    % Nodes for intermidiate states below observations
    \node[latent, below=of yT, opacity=0] (xT) {$\rvx_T$};
    \node[latent, below=of yT_1] (xT_1) {$\rvx_{T-1}$};
    \node[below=of dots, yshift=-0.8cm] (dots2) {$\cdots$};
    \node[latent, below=of y1] (x1) {$\rvx_1$};
    \node[latent, below=of y0] (x0) {$\rvx_0$};
    
    % Nodes for states
    \node[latent, below=of xT] (zT) {$\rvz_T$};
    \node[latent, below=of xT_1] (zT_1) {$\rvz_{T-1}$};
    \node[below=of dots2, yshift=-0.8cm] (dots3) {$\cdots$};
    \node[latent, below=of x1] (z1) {$\rvz_1$};
    \node[latent, below=of x0] (z0) {$\rvz_0$};
    
    % Arrows between states
    % \draw[arrow] (zT_1) -- (zT);
    % \draw[arrow] (dots3) -- (zT_1);
    % \draw[arrow] (z1) -- (dots3);
    % \draw[arrow] (z0) -- (z1);
    % \draw[dashed, arrow] (z1.south) to [out=-60,in=-120] (z0.south);
    % %\draw[dashed, arrow] (dots3.south) to [out=-60,in=-120] (z1.south);
    % \draw[dashed, arrow] (4,-4) to [out=-60,in=-120] (z1.south);
    % %\draw[dashed, arrow] (zT_1.south) to [out=-60,in=-120] (dots3.south);
    % \draw[dashed, arrow] (zT_1.south) to [out=-60,in=-120] (3.5,-4);
    % \draw[dashed, arrow] (zT.south) to [out=-60,in=-120] (zT_1.south);
    \draw[arrow] (zT) -- (zT_1);
    \draw[arrow] (zT_1) -- (dots3);
    \draw[arrow] (dots3) -- (z1);
    \draw[arrow] (z1) -- (z0);

    % Arrows from states to intermidiate states
    \draw[arrow] (z0) -- (x0);
    \draw[arrow] (z1) -- (x1);
    \draw[arrow] (zT_1) -- (xT_1);
    %\draw[arrow] (zT) -- (xT);
    
    % Arrows from states to observations
    \draw[arrow] (x0) -- (y0);
    \draw[arrow] (x1) -- (y1);
    \draw[arrow] (xT_1) -- (yT_1);
    %\draw[arrow] (xT) -- (yT);
    
    % Add legend for Markovian dynamics
    % \node[below=1cm of x3, align=center] (note) {Graphical model of a state-space model \\ with Markovian dynamics};
    
    \end{tikzpicture}
}
\caption{The graphical model of \MN. In gray observed variables and in white are latent variables.} %We use the forward process of the diffusion model to sample auxiliary observations $\{\rvy_t\}_{t=1}^T$ and then use the backward process for posterior inference.}
\label{fig:graphical_model}
\end{figure}

\section{Method}
\label{sec:method}
%We first provide an overview of our approach and then we will describe each element of it in detail. 
Given a corrupted image $\rvy_0$, the goal is to sample $\hat{\rvz}_0 \sim p_{\theta}(\rvz_0 | \rvy_0)$ using a pre-trained latent diffusion model as prior. Then, we can transform this sample into an image by applying a pre-trained decoder $\gD$, i.e. $\rvx_0 \coloneqq \gD(\hat{\rvz}_0)$. We first define a generative model for the data and then apply Bayesian inference on all latent variables using blocked Gibbs sampling and SMC. Specifically, we use the forward diffusion process to augment the model with additional auxiliary observations, and then apply posterior inference using a sequential Monte Carlo (SMC) approach based on the backward diffusion process. The corresponding graphical model can be seen in \Figref{fig:graphical_model}. %Due to the iterative nature of Gibbs sampling, this procedure can be applied multiple times, yet in our experiments, we found that one forward-backward pass suffices to achieve good results.
% \input{full_alg}
\subsection{The Generative Model}
\label{method:gen_model}
We now explicitly define the data generation model based on the forward diffusion process of DDIM \cite{SongME21},
%We add an observation for Augmenting the model with additional random variables $\rvz_{1:T} \coloneqq \{\rvz_t\}_{t=1}^{T}$ and $\rvy_{1:T-1} \coloneqq \{\rvy_t\}_{t=1}^{T-1}$ and define the following generative process:
\begin{equation*}
\begin{aligned}
    &1.~ \rvz_0 \sim p(\rvz_0)\\
    &2.~ \rvz_T | \rvz_0 \sim \normal(\sqrt{\bar{\alpha}_T}\rvz_0, (1 - \bar{\alpha}_T) \rmI)),\\
    &3.~ \rvz_{t-1} | \rvz_{t}, \rvz_0 \sim p(\rvz_{t-1} | \rvz_{t}, \rvz_0) \quad \forall t \in \{2, ..., T\},\\
    &4.~ \rvy_t | \rvz_t \sim \normal( \gA(\underbrace{\gD(\rvz_t))}_{\rvx_t}, \tau^2\rmI) \quad \forall t \in \{0, ..., {T-1}\}.
    % &4. \hat{\rvz}_T \sim \normal(0, \rmI)\\
    % &5.~ \hat{\rvz}_t | \hat{\rvz}_{t+1}, \rvy_t \sim p_\theta(\rvz_t | \hat{\rvz}_{t+1}, \rvy_t) \quad \forall t \in \{{T-1}, ..., 0\}\\
    % &6.~ \rvy_0 | \hat{\rvz}_0 \sim \normal(\rmA\gD(\hat{\rvz}_0), \sigma^2\rmI)
\end{aligned}
\label{gen_proces}
\end{equation*}

Here $p(\rvz_0)$ is a prior distribution over $\rvz_0$, $p(\rvy_t | \rvz_t)$ is defined by the corruption model, and
\small
\begin{equation}
\begin{aligned}
    &p(\rvz_{t-1} | \rvz_{t}, \rvz_0) =  \\
    &\normal \left( \rvz_{t-1}| \sqrt{\bar{\alpha}_{t-1}}\rvz_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma^2_{t}} \cdot \frac{\rvz_{t} - \sqrt{ \bar{\alpha}_{t}} \rvz_0}{\sqrt{1 - \bar{\alpha}_t}}, \sigma_t^2 \rmI \right),
\end{aligned}
\label{eq:ddim_forward}
\end{equation}
\normalsize
is defined by the forward diffusion process. 
Introducing unobserved data is a known technique in statistics for conducting effective Markov chain Monte Carlo (MCMC) sampling \cite{van2001art, dou2024diffusion}. In our case, we can use it while leveraging the dependencies between the variables in order to build an efficient SMC sampling procedure, as described in the next section.
%Note that we intentionally write the forward diffusion with dependence between subsequent time steps (instead of $\rvz_0$), it will be made clear why in a few moments. 
%, and $p_\theta(\rvz_t|\rvz_{t+1}, \rvy_t)$ models the dynamics.

\subsection{Sampling Procedure}
\label{sec:sampling_procedure}
Given the generative model defined in \Secref{method:gen_model}, our aim is to apply Bayesian inference over the latent variables. In broad strokes, to obtain a sample $\hat{\rvz}_0 \sim p(\rvz_0 | \rvy_0)$ we use blocked Gibbs sampling to sample in turns from $p(\rvy_{1:T-1}|\rvz_{0:T},\rvy_{0})$ and then use SMC to sample from $p(\rvz_{0:T}|\rvy_{0:T-1})$. Specifically, we propose the following procedure:
\begin{enumerate}
    \item Obtain an initial guess for $\hat{\rvz}_0$ (detailed in Sec. \ref{sec:init}),
    \item Repeat for some fixed number of steps:
    \begin{enumerate}
        \item Sample,
        $\rvz_{1:T} \sim p(\rvz_{1:T} | \hat{\rvz}_0, \rvy_0) = p(\rvz_{1:T} | \hat{\rvz}_0)$ according to the forward process of DDIM (\Eqref{eq:ddim_forward}).
        %\prod_{t=1}^{T}\normal(\rvz_t|\sqrt{\alpha_t}\rvz_{t-1}, \beta_t\rmI)$,
        %$\rvz_{1:T} | \hat{\rvz}_0 \sim p(\rvz_{1:T} | \hat{\rvz}_0, \rvy_0) = \prod_{t=1}^{T}\normal(\rvz_t|\sqrt{\alpha_t}\rvz_{t-1}, \beta_t\rmI)$, 
        \item Sample, 
        $\rvy_{1:T-1} \sim p(\rvy_{1:T-1}|\hat{\rvz}_0, \rvz_{1:T}, \rvy_0) = p(\rvy_{1:T-1}|\rvz_{1:T-1}) = \prod_{t=1}^{T-1} \normal(\rvy_t | \gA(\gD(\rvz_t)), \tau^2\rmI)$,
        \item Sample N particles $\{\hat{\rvz}_T^{(i)}\}_{i=1}^{N} \sim \normal (\rvz_T, \xi^2 \rmI)$,
        \item Sample $\hat{\rvz}_{0:T-1} \sim p_{\theta}(\rvz_{0:T-1}|\rvy_{0:T-1}, \{\hat{\rvz}_T^{(i)}\}_{i=1}^{N})$ using SMC based on a pre-trained diffusion model and select one particle $\hat{\rvz}_0$ for the next iteration (detailed in Sec. \ref{sec:post_sampling}).
    \end{enumerate}
\end{enumerate}
% $p(\rvz_{1:T}|\rvz_0,\rvy_{0:T-1})=p(\rvz_{1:T}|\rvz_0)$, the auxiliary observations $p(\rvy_{1:T-1}|\rvz_{0:T},\rvy_0)$, and the backward diffusion variables $p({\rvz}_{0:T}|\rvy_{0:T-1})$.
% from $p_{\theta}(\rvz_{1:T}, \rvy_{1:T-1}, \hat{\rvz}_{0:T-1} | \rvy_0)$. Then, we can take samples of $\hat{\rvz}_0$ only. 
Here we use the dynamics of the forward process and the graphical model dependencies in steps (a) and (b). In step (c) we sample from a Gaussian centered at the final $\rvz_T$  having a small std $\xi$ instead of the prior distribution as commonly done and described in \Secref{sec:background}. Although empirically we found that this step did not have a significant effect on the final model performance, we did it to be better aligned with the generated $\rvy_t$ from the forward process. In addition, we found that one forward-backward pass suffices to achieve good results. Hence, in our empirical evaluations, we apply Step 2 only once. In conclusion, steps (a), (b) and (c) are fully defined. The two steps that are not straightforward are how to obtain an initial guess for $\hat{\rvz}_0$ (step 1) and how to perform the sampling process in step (d). We discuss both next. 

%from the forward process having a small std $\xi$. In our experiments, we found that \MN{} was robust to the std choice; however, having a small std helped to obtain a higher likelihood in the early stages of the backward sampling procedure. 

%, but before that we would like to make two comments regarding the sampling procedure. First, in our experiments, we found that one forward-backward pass suffices to achieve good results. Hence, unless otherwise stated, we apply Step 2 only once. Second, the backward process in step (d) starts with $\rvz_T$ that was obtained from the forward process. 
%We note here that in practice in step (a) we used the DDIM sampler. \ia{should we say that here? Is it relevant?}\ef{then write the DDIM equation in step 1a}
% since it allowed us to control the variance of the forward process as well using the hyper-parameter $\eta$. 

\subsubsection{Initial Guess for $\hat{\rvz}_0$}
\label{sec:init}
The first challenge is to obtain an initial $\hat{\rvz}_0$ (step 1. in the sampling procedure). To reduce the variance in the process and accelerate convergence, we performed the following optimization procedure in pixel space:
% (1) Solve the following optimization problem in latent space:
% \begin{equation}
% \begin{aligned}
%     \hat{\rvz}_0 = \argmax_\rvz log~p_{\theta}(\rvy_0 | \rvz) = \argmin_\rvz || \rvy_0 - \gA(\gD(\rvz))||^2_2
% \end{aligned}
% \label{optimize_z}
% \end{equation}
% (2) Solve the following optimization problem in pixel space and then apply the encoder on it:
\begin{equation}
\begin{aligned} 
    \hat{\rvx}_0 &= \argmax_\rvx log~p_{\theta}(\rvy_0 | \rvx) = \argmin_\rvx || \rvy_0 - \gA(\rvx)||^2_2,
\end{aligned}
\label{optimize_x}
\end{equation}
and then we applied the encoder on the outcome, namely $\hat{\rvz}_0 = \gE(\hat{\rvx}_0)$.
For linear inverse problems, this optimization problem can be solved in closed form \cite{ song2023solving, wangzero}, although it can be costly, as it requires inverting the linear operator $\gA$. An alternative for this procedure is to apply the optimization process directly in the latent space. However, in our experiments we found that the former option worked better and it did not involve expensive gradient propagation through the decoder.

\subsubsection{Posterior Sampling}
\label{sec:post_sampling}
We now move on to explain step (d) of the sampling procedure.
Due to the non-linearity of the decoder, even for linear inverse problems, finding the exact posterior is intractable. One option to overcome this difficulty is to use SMC sampling. In what follows, we first describe an approximate posterior for $p_{\theta}(\rvz_{0:T}|\rvy_{0:T-1})$, then we suggest an iterative procedure based on SMC sampling. %For notational convenience we drop the dependence in $\{\hat{\rvz}_T^{(i)}\}_{i=1}^{N}$ in $p_\theta$, as it is only relevant at the initial SMC step.

First, notice that because of the structure of the model, the posterior density of the r.v. $\rvz_t$ at each step $t$ depends only on $\rvz_{t+1:T}$. Hence, only $p_{\theta}(\rvz_{t:T}| \rvy_{0:T-1})$ needs to be computed at each time step $t$. However, even computing an unnormalized quantity of that posterior can be costly. Therefore, we make the following assumption $p_{\theta}(\rvz_{t:T}| \rvy_{0:T-1}) \approx p_{\theta}(\rvz_{t:T}| \rvy_{t:T-1}, \rvy_{0})$. We assume this as $\rvy_0$ stores all the input information.
Now we can arrive at the following recursive formula (to prevent cluttered notations we omit here the subscript of $\theta$ from the probability densities):
\begin{equation*}
    \begin{aligned} 
        &p(\rvz_{t:T} | \rvy_{t:T}, \rvy_0)  \\
        &\propto p(\rvy_t | \rvz_{t:T}, \rvy_{t+1:T}, \rvy_0) p(\rvz_{t:T} | \rvy_{t+1:T}, \rvy_0) \\
        &= p(\rvy_t | \rvz_t) p(\rvz_t | \rvz_{t+1:T}, \rvy_{t+1:T}, \rvy_0)p(\rvz_{t+1:T} | \rvy_{t+1:T}, \rvy_0) \\
        &= p(\rvy_t | \rvz_t) p(\rvz_t | \rvz_{t+1}, \rvy_0) p(\rvz_{t+1:T} | \rvy_{t+1:T}, \rvy_0) \\
        &= p(\rvy_t | \rvz_t) \frac{p(\rvy_0 | \rvz_t)}{p(\rvy_0 | \rvz_{t+1})}p(\rvz_t|\rvz_{t+1})p(\rvz_{t+1:T} | \rvy_{t+1:T}, \rvy_0) \\
        & \approx \frac{p(\rvy_t | \rvz_t) p(\rvy_0 | \bar{\rvz}_0(\rvz_t))}{p(\rvy_0 | \bar{\rvz}_0(\rvz_{t+1}))}p(\rvz_t|\rvz_{t+1})p(\rvz_{t+1:T} | \rvy_{t+1:T}, \rvy_0).
    \end{aligned}
\end{equation*}
%, and $\rvy_{0:t}$ $p_{\theta}(\rvz_{0:T}|\rvy_{0:T-1})$  
% can be factorized according to,
% \begin{equation*}
%     \begin{aligned} 
%         &p_{\theta}(\rvz_{0:T}|\rvy_{0:T-1}) = \prod_{t=0}^{T-1} p_{\theta}(\rvz_{t}| \rvz_{t+1}, \rvy_{0:t})
%     \end{aligned}
% \end{equation*}
Where, in the first transition we used Bayes rule, in the third transition we used the Markovian assumption, in the forth transition we used Bayes rule again, and in the last transition we make an additional approximation and condition on the posterior mean estimator for both time $t$ and time $t+1$.
We define $p(\rvy_0 | \bar{\rvz}_0(\rvz_t)) = \normal(\rvy_0 | \gA(\gD(\bar{\rvz}_0(\rvz_t))), (1 - \bar{\alpha}_t)\rmI))$ and similarly for $p(\rvy_0 | \bar{\rvz}_0(\rvz_{t+1}))$, where as in \cite{wu2024practical} the variance term is taken to be the variance of the forward diffusion process.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/ImageNet_debluring_comparison.jpg}
\caption{Comparison between \MN{} and baseline methods on Gaussian deblurring of images from the FFHQ dataset.}
\label{fig:ffhq_gd_recon_comp}
\end{figure*}


Now, we can derive an SMC procedure using the proposed posterior. Specifically, we used sequential importance sampling with resampling (SISR). 
The general prescription (adapted to our setting) is as follows \cite{sarkka2013bayesian}:
\begin{enumerate}
    \item Collect N particles as detailed in step (c) in \Secref{sec:sampling_procedure}. %$\{\hat{\rvz}_T^{(i)}\}_{i=1}^{N} \sim \normal (\rvz_T, \xi^2 \rmI)$.   %and set their weights to $w_T^{(i)} = \frac{1}{N} \quad \forall i \in \{1, ..., N\}$.\\
    \item For $t = T-1, ..., 0$:
    \begin{enumerate}[i.]
        \item Draw $\{\tilde{\rvz}_{t}^{(i)}\}_{i=1}^{N}$ samples from a proposal distribution (detailed in Sec. \ref{sec:proposal_distirbution}): 
        \begin{equation*}
        \begin{aligned} 
        \tilde{\rvz}_{t}^{(i)} \sim \pi(\tilde{\rvz}_t | \hat{\rvz}_{t+1}^{(i)}, \rvy_{0:T-1}) \quad i = 1, ..., N.
        \end{aligned}
        \end{equation*}
        
        \item Compute the proposal weights for all $i \in \{1, ..., N\}$ according to:
        \begin{equation*}
        \begin{aligned} 
        w_t^{(i)} &\propto \frac{p_{\theta}(\rvy_t | \tilde{\rvz}_{t}^{(i)}) p_{\theta}(\rvy_0 | \bar{\rvz}_0(\tilde{\rvz}_{t}^{(i)})) p_{\theta}(\tilde{\rvz}_t^{(i)}|\hat{\rvz}_{t+1}^{(i)})}{\pi(\tilde{\rvz}^{(i)}_t | \hat{\rvz}_{t+1}^{(i)}, \rvy_{0:T-1}) p_{\theta}(\rvy_t | \bar{\rvz}_0(\hat{\rvz}_{t+1}^{(i)}))},
        \end{aligned}
        \end{equation*}
        and then normalize the weights to sum to one.
    
        \item Resample $N$ particles (with replacements) from the discrete distribution:
        $\hat{\rvz}_{t}^{(i)} \sim \{\tilde{\rvz}_{t}^{(1)}, ..., \tilde{\rvz}_{t}^{(N)}\}$ 
        with weights $\{w_t^{(1)}, ..., w_t^{(N)}\}$.
        \end{enumerate}
\end{enumerate}

Here, $p_{\theta}(\tilde{\rvz}_t^{(i)}|\hat{\rvz}_{t+1}^{(i)})$ is defined according to \Eqref{eq:ddim_sampling} and we assume that the resampling step is performed at each iteration. While the derivation is different, the resulting weighting scheme of \MN{} bears some resemblance to the weighting of TDS \cite{wu2024practical}. The main difference between the two methods is the dependence on the auxiliary variables $\rvy_{1:T}$ in the proposal distribution and resampling weights.  Empirically, we observed that this additional conditioning helped to better align the sampling with the corrupted image $\rvy_0$ compared to using the posterior mean approximation as in \cite{chungdiffusion} and \cite{wu2024practical}. Finally, to select one sample at time $t=0$ for terminating the algorithm in \Secref{sec:sampling_procedure} or the next iteration in the Gibbs sampling procedure, we take the particle that has the highest weight. %Namely, $\hat{\rvz}_0 = \hat{\rvz}_0^{(\argmax_i \{w_t^{(1)}, ..., w_t^{(N)}\})}$.



%we observed that this additional information helps capture fine-detail information in inpainting tasks that can be lost when the connection to the labels is only thorough the posterior mean approximation as in \cite{chungdiffusion} and \cite{wu2024practical}.



% Two crucial differences between the two methods are (1) the dependence on $\{\rvy_t\}_{t=1}^{T}$ in the proposal distribution and (2) an additional multiplicative term $p(\rvy_t | \rvz_t)$ in the weighting function. Both were derived as part of our model design and formulation. In that sense \MN{} is a strict generalization of their approach.


%we observed that when the dependence on $\rvy_0$ is only through the posterior mean estimator, as in \cite{wu2024practical}, the generated image loses fine-detail information. Others in the literature also observed this phenomenon (e.g., \cite{rout2024beyond}). In our model, the dependence on $\rvy_t$ enables us to mitigate this problem.

% The SISR algorithm requires access to $\pi(\rvz_t | \hat{\rvz}_{t+1}^{(i)}, \rvy_{0:T-1})$, a proposal distribution. The optimal choice in the sense of minimizing the variance of the proposal weights is $\pi(\rvz_t | \rvz_{t+1:T}, \rvy_{0:T-1}) = p_\theta(\rvz_t | \rvz_{t+1}, \rvy_{0:t})$ \cite{murphy2001rao, sarkka2013bayesian}. However, it cannot be obtained in closed form. Hence, we design a novel proposal distribution as described in the next section.  

\begin{table*}[!t]
%\small
\caption{Quantitative results on $1024$ examples of size $256 \times 256$ from ImageNet test set. All methods were evaluated under the same experimental setup using LDM.} %Per method, best hyper-parameter configuration was chosen based on the FID.}
\begin{adjustbox}{width=\linewidth,center}
%\centering
\begin{tabular}{l ccc c ccc c ccc}
    \toprule
    & \multicolumn{3}{c}{Inpainting (Box)} && \multicolumn{3}{c}{Gaussian Deblur} &&
    \multicolumn{3}{c}{Super Resolution ($8\times$)}\\
    \cmidrule(l){2-4}  \cmidrule(l){5-8} \cmidrule(l){9-12}
    & FID ($\downarrow$) & NIQE ($\downarrow$) & LPIPS ($\downarrow$) && FID ($\downarrow$) & NIQE ($\downarrow$) & LPIPS ($\downarrow$) && FID ($\downarrow$) & NIQE ($\downarrow$) & LPIPS ($\downarrow$) \\
    \midrule
    Latent DPS & 65.45 & 7.918 & 0.407 && 52.48 & 6.855 & 0.383 && 61.02 & \underline{6.514} & \underline{0.439}\\
    Latent TDS & \underline{65.03} & 7.872 & 0.406 && \underline{50.82} & \textbf{6.695} & \underline{0.379} && \textbf{58.73} & 7.157 & 0.454\\
    Resample & 90.32 & 8.464 & \textbf{0.318} && \textbf{46.45} & 7.411 & \textbf{0.353} && 87.65 & 8.290 & 0.491\\
    PSLD & 79.90 & 9.268 & 0.410 && 79.31 & 7.972 & 0.474 && 78.56 & 7.000 & 0.467\\
    \midrule
    \MN{} (Ours)  & \textbf{51.81} & \textbf{5.103} & \underline{0.355} && 52.17 & 6.789 & 0.382 && \underline{59.27} & \textbf{6.423} & \textbf{0.437}\\
    \bottomrule
    \end{tabular}
\label{tab:imagenet}
\end{adjustbox}
\end{table*}

\subsubsection{Proposal Distribution}\label{sec:proposal_distirbution}
The SISR algorithm requires access to $\pi(\tilde{\rvz}_{t-1} | \hat{\rvz}_{t}^{(i)}, \rvy_{0:T-1})$, a proposal distribution. The optimal choice in the sense of minimizing the variance of the proposal weights is $\pi(\tilde{\rvz}_{t-1} | \hat{\rvz}_{t:T}^{(i)}, \rvy_{0:T-1}) = p_\theta(\tilde{\rvz}_{t-1} | \hat{\rvz}_{t}^{(i)}, \rvy_{0:t})$ \cite{murphy2001rao, sarkka2013bayesian}. However, it cannot be obtained in closed form. Hence, we design an alternative proposal distribution, which we will now describe.

%In general, the importance density should uphold two conditions: (1) its support contains the support of the posterior density, and (2) it is easy to sample from. 
For clarity, we drop here the index notation of the particles. 
We define the proposal distribution to be a Gaussian $\pi(\tilde{\rvz}_{t-1} | \hat{\rvz}_{t:T}, \rvy_{0:T-1}) = \normal(\rvm_t, \rmS_t)$ with parameters:
\begin{equation}
\begin{aligned} 
    \rmS_t &= \sigma_t^2 \rmI\\
    \rvm_t &= \rvmu_\theta(\hat{\rvz}_t, t) - (\gamma_t \nabla_{\hat{\rvz}_t}||\rvy_0 - \gA(\gD(\bar{\rvz}_0(\hat{\rvz}_t)))||_2^2 \\
    &\qquad\qquad~ + \lambda_t \nabla_{\rvmu_\theta(\hat{\rvz}_t, t)}||\rvy_t - \gA(\gD(\rvmu_\theta(\hat{\rvz}_t, t)))||_2^2).
\end{aligned}
\label{eq:proposal}
\end{equation}

Here we set the variance to be the variance of the prior diffusion model; however, other choices are applicable as well. The idea behind our proposal mean is to correct the prior mean estimation by shifting it towards latents that agree more strongly with both $\rvy_t$ and $\rvy_0$. The second term can be seen as making one gradient update step starting from the current prior mean location.

The $\gamma_t$ and $\lambda_t$ parameters control the effect of the correction terms to the prior mean. In practice, it is challenging to control the trade-off between the two correction terms. Hence, during the sampling process we start only with the first term and then at some predefined point, $t = s$, we switch to the second term. The intuition here is that during the initial sampling steps the quality of the labels $\rvy_t$ may not be good. Therefore, we rely on the first term through the posterior mean estimator to capture the general semantics of the image. However, in later sampling stages the quality of the labels increases (see \Figref{fig:in_forward_process} in the Appendix) and the latter correction term can add fine details to the image. This intuition also relates to the three-stage phenomenon in the diffusion sampling process witnessed in the literature \cite{yu2023freedom}. Setting $\lambda_t = 0$ for all time steps reduces the \MN{} proposal update to that of TDS \cite{wu2024practical}. We show in \Appref{sec:scaling_coefs} an instantiation of $\gamma_t$ and $\lambda_t$ used in this study.

 
% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}


% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% \section{Theoretical Motivation}
% \ia{Can we prove some asymptotic result along the lines of TDS? I believe so.}


% They propose an alternative way to improve the performance of Latent DPS.


% \textbf{Diffusion-based methods.}
% Recently, several approaches have suggested two-stage pipeline algorithms. DifFace \cite{yue2024difface} suggested such a method for blind face restoration,  performing sampling from a transition distribution followed by a diffusion process. DiffBIR \cite{lin2023diffbir} proposed to solve blind image restoration by first applying a restoration module for degradation removal and then generating the lost content using a latent diffusion model.


\section{Experiments}

% \begin{table*}[!t]
% \small
% \centering
% \caption{\textit{ImageNet}. Box in-painting on $128$ test examples.}
% \begin{tabular}{l ccc c ccc c ccc}
%     \toprule
%     & \multicolumn{3}{c}{Inpainting (Box)} && \multicolumn{3}{c}{Gussian Deblur} &&
%     \multicolumn{3}{c}{Super Resolution ($\times 8$)}\\
%     \cmidrule(l){1-4}  \cmidrule(l){5-8} \cmidrule(l){9-12}
%     & FID ($\downarrow$) & NIQE ($\downarrow$) & PSNR ($\uparrow$) && FID ($\downarrow$) & NIQE ($\downarrow$) & PSNR ($\uparrow$) && FID ($\downarrow$) & NIQE ($\downarrow$) & PSNR ($\uparrow$) \\
%     \midrule
%     DPS & 139.2 & 8.244 & 19.52 && 139.2 & 8.244 & 19.52 && 139.2 & 8.244 & 19.52 \\
%     TDS & 138.8 & 8.307 & 19.45 && 138.8 & 8.307 & 19.45 && 138.8 & 8.307 & 19.45 \\
%     Resample & 173.6 & 8.530 & 18.35 && 173.6 & 8.530 & 18.35 && 173.6 & 8.530 & 18.35 \\
%     PSLD & 131.3 & 7.002 & 16.62 && 131.3 & 7.002 & 16.62 && 131.3 & 7.002 & 16.62 \\ 
%     \midrule
%     \MN (Ours) & 111.9 & 5.240 & 19.19 && 111.9 & 5.240 & 19.19 && 111.9 & 5.240 & 19.19 \\ 
%     \bottomrule
%     \end{tabular}
% \label{tab:imagenet}
% \end{table*}


\begin{table*}[!t]
% \small
% \centering
\caption{Quantitative results on $1024$ examples of size $256 \times 256$ from FFHQ test set. All methods were evaluated under the same experimental setup using LDM.} %Per method, best hyper-parameter configuration was chosen based on the FID.}
\begin{adjustbox}{width=\linewidth,center}
\begin{tabular}{l ccc c ccc c ccc}
    \toprule
    & \multicolumn{3}{c}{Inpainting (Box)} && \multicolumn{3}{c}{Gaussian Deblur} &&
    \multicolumn{3}{c}{Super Resolution ($8\times$)}\\
    \cmidrule(l){2-4}  \cmidrule(l){5-8} \cmidrule(l){9-12}
    & FID ($\downarrow$) & NIQE ($\downarrow$) & LPIPS ($\downarrow$) && FID ($\downarrow$) & NIQE ($\downarrow$) & LPIPS ($\downarrow$) && FID ($\downarrow$) & NIQE ($\downarrow$) & LPIPS ($\downarrow$) \\
    \midrule
    Latent DPS & 39.81 & 7.592 & 0.236 && \textbf{31.81} & 6.813 & 0.285 && \textbf{29.64} & \underline{6.412} & \textbf{0.282}\\
    Latent TDS & \underline{39.57} & 7.602 & 0.236 && 33.19 & 6.879 & 0.288 && 30.45 & \textbf{6.411} & \underline{0.284}\\
    Resample & 86.79 & \textbf{7.142} & \underline{0.230} && 39.80 & 7.441 & \textbf{0.275} && 59.23 & 7.307 & 0.356\\
    PSLD & 47.51 & \underline{7.480} & 0.312 && 36.31 & \underline{6.802} & 0.341 && 40.33 & 6.803 & 0.347\\
    \midrule
    \MN{} (Ours)  & \textbf{37.14} & 7.520 & \textbf{0.224} && \underline{32.18} & \textbf{6.566} & \underline{0.280} && \underline{30.37} & 6.456 & \underline{0.284}\\
    \bottomrule
    \end{tabular}
\label{tab:ffhq}
\end{adjustbox}
\end{table*}




\label{experiment}
\subsection{Experimental Setting}
We evaluated \MN{} on ImageNet \cite{ILSVRC15} and FFHQ \cite{KarrasLA19}; both are common in the literature of inverse problems (e.g., \cite{chungdiffusion, dou2024diffusion}). In ImageNet samples were conditioned on the class label. The guidance scale was fixed to $1.0$ in all our experiments. Results can be improved by adjusting it \cite{rombach2022high}. %Results can be improved for all methods by adjusting the scale, nevertheless any performance gaps between the methods should remain irrespective of that.
%\id{suggestion - By adjusting the scale, results for all methods can be enhanced; however, any differences in performance among the methods are likely to persist regardless.}\ef{just say it was fixed and not fine-tuned to any methods}
Images were resized to $3 \times 256 \times 256$ and normalized to the range $[0, 1]$. 
We used the latent diffusion model (LDM) VQ-4 \cite{rombach2022high} for the prior model with the DDIM diffusion sampler \cite{SongME21}, according to the data split in \cite{esser2021taming}. We sampled $1024$ random images from the validation set of each dataset which were used to evaluate all methods. 
We followed the protocol of \cite{song2023solving} and added Gaussian noise with zero mean and standard deviation $\tau = 0.01$ to the corrupted images. 
Full experimental details are provided in \Appref{sec:full_exp_details}.

\textbf{Compared methods.} We compared \MN{} with several recent SoTA baseline methods; the first two methods were designed for general inverse problems, but were evaluated using pixel space diffusion models, and the latter two were designed specifically for inverse problems in the latent space. Nevertheless, all methods were evaluated under a similar experimental setup using latent diffusion to ensure fairness in the comparisons. \textbf{(1) Diffusion Posterior Sampling (DPS)} \cite{chungdiffusion}, which introduces correction to the sampling process of the diffusion through the posterior mean estimator; \textbf{(2) Twisted Diffusion Sampling (TDS)} \cite{wu2024practical}, which uses the twisting technique for approximate sequential Monte Carlo sampling;  \textbf{(3) Resample } \cite{song2023solving}, which applies an optimization procedure during the sampling process to match the approximate posterior mean to the label, and then performs resampling;
\textbf{(4) Posterior Sampling with Latent Diffusion (PSLD)} \cite{rout2024solving}, which introduces a correction term to the DPS step to ``glue" $\rvz_0$. For our method and TDS we used $N = 5$ particles.

\textbf{Evaluation metrics.} We report the following metrics in the main text, FID \cite{heusel2017gans}, NIQE \cite{mittal2012making}, and LPIPS \cite{zhang2018unreasonable}. Full results with the PSNR and SSIM metrics \cite{wang2004image} are deferred to \Appref{sec:full_results}. The first two are considered perceptual metrics, lower values in them indicate higher perceptual quality. The other metrics are considered as distortion metrics, which quantify some discrepancy between the generated images and the ground-truth values. Since perceptual metrics and distortion metrics can be in conflict with each other \cite{blau2018perception}, we put more emphasis on perceptual quality. Hence, for all methods, we performed grid search over hyper-parameters and chose the best configuration according to the FID. 

\subsection{Experimental Results}
Quantitative results are shown in Tables \ref{tab:imagenet} and \ref{tab:ffhq}. From the tables, \MN{} is usually the best or second best among all the comparisons. 
Specifically, on inpainting where extrapolation is needed inside the box and details should be preserved outside the box, \MN{} can greatly improve over baseline methods, improving the FID score by up to $\sim 13$  points. This property is also manifested in \Figref{fig:in_ip_recon} and \Figref{fig:ffhq_ib_recon} in the appendix. \MN{} manages to produce plausible reconstructions while maintaining fine details. This is in contrast to baseline methods, which mainly rely on the posterior mean approximation for reconstruction. The differences are especially highlighted on the ImageNet dataset which has more diversity in it. Also, as is clear from the figures, Resample images suffer from significant artifacts. We speculate that it partly stems from the complete optimization process performed in every few sampling steps according to this method. We observe that this method is well suited for some tasks, such as Gaussian deblurring, but does not perform well on others such as box inpainting.
%was designed specifically for Gaussian deblurring, on which it has the best results, but it doesn't transfer well to image inpainting. % As a result, more emphasis is given to the label and less for the prior diffusion model to generate plausible, natural-looking images. 


In \Figref{fig:ffhq_gd_recon_comp} we show qualitative results for Gaussian deblurring of images by \MN{} and baseline methods. From the figure, all methods are able to generate plausible reconstructions on this task. Additional quantitative and qualitative results can be found in \Appref{sec:full_results} and \ref{sec:img_rec} .

\subsection{Analysis}
%\textbf{Number of particles.}
%\textbf{Proposal update.} 
Recall that in our proposal update we suggested correcting the prior mean using two terms, one that involves $\rvy_0$ and another that involves $\rvy_t$. In practice during the sampling process, we first use the former one and then switch to the latter at some fixed time step $s$. This is a hyperparameter of our approach. Table \ref{tab:switch_step} compares the FID and PSNR on ImageNet inpainting task as a function of the diffusion step $s$ in which the switch is made. 
From the table, when the switch is done at earlier stages of the sampling process, the FID improves, but at the same time, the PSNR degrades. We chose to use $s = 333$ since it balances well between the two metrics while giving more emphasis to the perceptual quality.
\begin{table}[!h]
\centering
\caption{Tradeoff between the proposal update correction terms for $T=1000$ steps.}
\begin{tabular}{l cc}
    \toprule
    & FID ($\downarrow$) & PSNR ($\uparrow$) \\
    \midrule
    $s = 0$ & 65.76 & 19.61 \\
    $s = T / 6$ & 61.09 & 19.25 \\
    $s = T / 3$ & 51.81 & 18.87 \\
    $s = T / 2$ & 48.77 & 18.59 \\
    \bottomrule
    \end{tabular}
\label{tab:switch_step}
\end{table}



% \Tableref{tab:inpainting_imagenet} presents the results on image inpainting task, and \Tableref{tab:gauss_deblur_imagenet} presents results on Gaussian deblurring task. On the challenging task of inpainting, where extrapolation is needed \MN outperforms all baselines by a large margin in perceptual metrics. On Gaussian deblurring task \MN is second best.

% \subsection{Qualitative Results}
% In \Figref{fig:in_recon} we present qualitative results. As seen from the figure, the completion of \MN is not only sensible but it is able to reconstruct fine details of the image. 


\section{Limitations}
Although our approach has strong empirical results,
one limitation of our approach is related to computational demand. The sampling time and the memory demand increase with the number of particles. In addition, compared to TDS, in the resampling step, we need to use the decoder one more time to compute $p(\rvy_t|\rvz_t)$ (only forward pass), which can also affect the sampling time. This effect can be mitigated by taking fewer particles or by parallelizing \MN{} between GPUs. Furthermore, for more challenging tasks like box inpainting, competing algorithms tend to exhibit noticeable artifacts, which limits their applicability.



\section{Conclusion}
\label{conclusion}
In this study, we presented \MN, a novel method for solving inverse problems in the latent space of diffusion models using SMC. Specifically, we leveraged the forward process of the diffusion process to augment the model with auxiliary observations, one per each timestep, and used these observations to guide the sampling process as part of the backward diffusion process. This framework can be seen as applying one step of blocked Gibbs sampling. To perform SMC sampling, we suggested a novel weighing scheme and a novel proposal distribution. Both are based on information from the auxiliary labels and the true label $\rvy_0$.  Empirically, we validated \MN{} against strong baseline methods on common benchmarks. The results suggest that \MN{} can improve the performance over baseline methods, especially in cases where extrapolation is needed (e.g., in inpainting). 
% \section*{Acknowledgements}

%\newpage
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{Ref}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\input{appendix}
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
