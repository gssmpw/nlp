\section{Related Work}
\label{sec:related}

Inverse problems have a long and evolving history, with methodologies that have undergone significant advances over the years \cite{Daras2024survey}.
Recently, diffusion models \cite{sohl2015deep, ho2020denoising,songscore} have emerged as effective priors for solving inverse problems in image data \cite{wangzero,kawar2022denoising,chungdiffusion,dou2024diffusion,rout2024solving,song2023pseudoinverse,sun2024provable,Choi2021ILVRCM,chung2023parallel}. 
%We divide our literature review to inverse problem methods in the pixel space and inverse problem methods in the latent space, and note here that several methods can be applied in both settings.
% Another perspective approaches inverse problems from the viewpoint of Bayesian inference. 
% Certain techniques utilize diffusion models as priors to generate plausible reconstructions by sampling from the posterior distribution, for example.


% \textbf{Diffusion-based Inverse Problems in Pixel Space.}
% DDRM \cite{kawar2022denoising} and DDNM \cite{wangzero} utilize diffusion models as prior solving linear inverse problems in pixel space by approximating the measurement matching score, $\nabla \log p(\rvy|\rvx_t)$. 
In \cite{songscore} it was shown that to sample from the posterior distribution, $p(\rvx_0 | \rvy)$, one can solve a stochastic differential equation based on the prior score, $\nabla_{\rvx_t} \log~p_{\theta}(\rvx_t)$, and the conditional score, $\nabla_{\rvx_t} \log~p_{\theta}(\rvy | \rvx_t)$. Although the first term is easy to compute, the latter term requires integration over the full diffusion path from time $t$ to $0$. A useful and easy-to-calculate approximation found in several studies is $p_{\theta}(\rvy | \rvx_t) \approx p_{\theta}(\rvy | \E[\rvx_0 | \rvx_t])$, which is readily available at each step \cite{chungdiffusion, song2023pseudoinverse, wu2024practical}.
Specifically, Diffusion Posterior Sampling (DPS) \cite{chungdiffusion} uses this approximation for linear and non-linear inverse problems with Gaussian and Poisson likelihood models.
%explicit approximations for the measurement matching term with $\mathbb{E}[\rvx_0|\rvx_t]$, 
% approximating $\nabla \log p(\rvy|\rvx_t)$ with $\nabla \log p(\rvy|\mathbb{E}[\rvx_0|\rvx_t])$, 
%addressing non-linear inverse problem scenarios.
$\Pi$GDM \cite{song2023pseudoinverse} introduces pseudoinverse guidance by matching the denoising output and the corrupted image $\rvy$, via transformation of both through a 'pseudoinverse' of the corruption model. %However, it was observed that relying on that approximation alone may miss fine details in the image \cite{rout2024beyond}.
%  guidance-based approach for inverse problem solving that handles measurements with Gaussian noise, as well as some non-linear, non-differentiable measurement models
DDNM \cite{wangzero} suggested to refine only the contents of the null space during the backward diffusion process. As such it is suited only for linear inverse problems. 
%utilizes diffusion models as prior for solving linear inverse problems by decomposing the linear operator $\sA$ into t%in pixel space by approximating the conditional score, $\nabla_{\rvx_t} \log p_{\theta}(\rvy|\rvx_t)$, using $\mathbb{E}[\rvx_0|\rvx_t,\rvy]$.
% Asymptotically Exact Methods
An additional category of inverse problem approaches that use diffusion models is designed with the objective of asymptotic exactness \cite{cardoso2023monte, trippe2023diffusion, wu2024practical, dou2024diffusion}. 
%SMC-Diff \cite{trippe2023diffusion}, MCGDiff \cite{cardoso2023monte}, and TDS \cite{wu2024practical} 
These methods utilize SMC techniques targeting exact sampling from the posterior distribution $p(\rvx_0 | \rvy)$. Specifically, 
% \ia{for the first two please add why they are not relevant/why we didn't compre to them}.
SMC-Diff \cite{trippe2023diffusion} applies particle filtering for inpainting in motif scaffolding, and
% offers asymptotic guarantees solely under the assumption that the trained diffusion model perfectly aligns with the forward noising process, a condition rarely met in practical situations.
MCGDiff \cite{cardoso2023monte} is designed for linear inverse problems only. Hence, both approaches are not suited for inverse problems with latent-space diffusion models. %, with inpainting serving as a specific example.
TDS \cite{wu2024practical}, a recent SMC-based method, solves general inverse problem tasks using the twisting technique. This method also uses the approximation of DPS, but by applying SMC sampling it can correct for it.


%Importantly, the connection to the observation is made only through $\E[\rvx_0 | \rvx_t]$. 
FPS \cite{dou2024diffusion} is also a recent method based on SMC with auxiliary variables. FPS generates a sequence of observations $\rvy_{1:T}$ based on a duplex diffusion process, one process at the $\rvx$ space and the other process at the $\rvy$ space. Since this method is designed for linear inverse problems only, it permits tractable Bayesian inference. Our method combines the ideas of both TDS and FPS to obtain the best of both. Namely, we use the posterior mean approximation and $\rvy_{1:T}$ in our SMC sampling process. As we will show, this combination can be helpful in both understanding the general semantics of an image and capturing fine details. 

% demonstrated considerable potential linking posterior sampling with Bayesian filtering and adeptly tackling the filtering problem utilizing sequential Monte Carlo methods. However, this approach is confined to linear corruption models. As we will show in \Secref{sec:method} our method combines TDS and FPS to solve general inverse problems.
% The FPS methodology \cite{dou2024diffusion} links posterior sampling with Bayesian filtering and adeptly tackles the filtering problem utilizing sequential Monte Carlo methods. This approach demonstrates considerable potential, but it is confined to linear corruption models.

% Recently, several approaches \cite{dou2024diffusion,wu2024practical,cardoso2023monte} have modeled inverse problems as Bayesian inference in a state-space model.
% However, these methods were designed for diffusion models in pixel space.
% and with the exception of \cite{cardoso2023monte} they assume a linear degradation model.

Several inverse sampling methods were specifically tailored for latent diffusion models. 
PSLD \cite{rout2024solving} extend DPS \cite{chungdiffusion} by incorporating an additional gradient update step to guide the diffusion process to sample latent representations that maintain the integrity of the decoding-encoding transformation, ensuring it remains non-lossy.
% for which the decoding-encoding map is not lossy.
STSL \cite{rout2024beyond} presents a novel sampler with a tractable reverse process using an efficient second-order approximation. 
Comparative analysis with STSL was not feasible due to the absence of publicly available code, making replication challenging.
% a Second-order Tweedie sampler from Surrogate Loss.  
Resample \cite{song2023solving}, a contemporary method alongside PSLD, introduces a strategy for addressing general inverse problems using pretrained latent diffusion models, tackling the complexities posed by encoder and decoder nonlinearity.
Resample algorithm includes hard data consistency to obtain latent variable that is consistent with the observed measurements, and then employs a resampling scheme to map the sample back onto the correct noisy data manifold and continue the reverse sampling process. 
Concurrent to this study \citet{nazemi2024particle} proposed a particle filtering approach. Their method builds on PSLD and DPS update in the proposal distribution. Similarly to TDS \cite{wu2024practical} the connection to the labels is only through $\rvz_0$ using the approximate mean estimator. Since these methods share commonalities, we compare only to the latter in the experimental section. 


% refined based on a proposal distribution and a weighting function. The proposal distribution serves as an approximation to the posterior distribution and should uphold two conditions: (1) its support contains the support of the posterior density and (2) it is easy to sample from. The weighting function corrects the approximation by assigning a weight for each particle.
%Let $\pi(\rvz_t | \rvz_{t+1})$ denote the proposal density at time step $t$


%which corresponds to a Markovian forward process as in \cite{dou2024diffusion} 

% \textbf{Particle Filtering.} Particle filtering is a technique to sample from a posterior distribution in state space models \cite{sarkka2013bayesian}. The key idea   

\begin{figure}[!t]
\centering
\scalebox{0.8}{
    \begin{tikzpicture}[
        scale=1.0, % Adjust the scale to shrink the entire figure
        node distance=0.4cm and 0.8cm,
        observed/.style={circle, draw, fill=gray!20, minimum size=1.2cm},
        latent/.style={circle, draw, minimum size=1.2cm},
        arrow/.style={-{Stealth[scale=1.2]}}
    ]
    
    % Nodes for observations # , opacity=0
    \node[latent, opacity=0] (yT) {$\rvy_T$};
    \node[latent, right=of yT] (yT_1) {$\rvy_{T-1}$};
    \node[right=of yT_1] (dots) {$\cdots$};
    \node[latent, right=of dots] (y1) {$\rvy_1$};
    \node[observed, right=of y1] (y0) {$\rvy_0$};
    
    % Nodes for intermidiate states below observations
    \node[latent, below=of yT, opacity=0] (xT) {$\rvx_T$};
    \node[latent, below=of yT_1] (xT_1) {$\rvx_{T-1}$};
    \node[below=of dots, yshift=-0.8cm] (dots2) {$\cdots$};
    \node[latent, below=of y1] (x1) {$\rvx_1$};
    \node[latent, below=of y0] (x0) {$\rvx_0$};
    
    % Nodes for states
    \node[latent, below=of xT] (zT) {$\rvz_T$};
    \node[latent, below=of xT_1] (zT_1) {$\rvz_{T-1}$};
    \node[below=of dots2, yshift=-0.8cm] (dots3) {$\cdots$};
    \node[latent, below=of x1] (z1) {$\rvz_1$};
    \node[latent, below=of x0] (z0) {$\rvz_0$};
    
    % Arrows between states
    % \draw[arrow] (zT_1) -- (zT);
    % \draw[arrow] (dots3) -- (zT_1);
    % \draw[arrow] (z1) -- (dots3);
    % \draw[arrow] (z0) -- (z1);
    % \draw[dashed, arrow] (z1.south) to [out=-60,in=-120] (z0.south);
    % %\draw[dashed, arrow] (dots3.south) to [out=-60,in=-120] (z1.south);
    % \draw[dashed, arrow] (4,-4) to [out=-60,in=-120] (z1.south);
    % %\draw[dashed, arrow] (zT_1.south) to [out=-60,in=-120] (dots3.south);
    % \draw[dashed, arrow] (zT_1.south) to [out=-60,in=-120] (3.5,-4);
    % \draw[dashed, arrow] (zT.south) to [out=-60,in=-120] (zT_1.south);
    \draw[arrow] (zT) -- (zT_1);
    \draw[arrow] (zT_1) -- (dots3);
    \draw[arrow] (dots3) -- (z1);
    \draw[arrow] (z1) -- (z0);

    % Arrows from states to intermidiate states
    \draw[arrow] (z0) -- (x0);
    \draw[arrow] (z1) -- (x1);
    \draw[arrow] (zT_1) -- (xT_1);
    %\draw[arrow] (zT) -- (xT);
    
    % Arrows from states to observations
    \draw[arrow] (x0) -- (y0);
    \draw[arrow] (x1) -- (y1);
    \draw[arrow] (xT_1) -- (yT_1);
    %\draw[arrow] (xT) -- (yT);
    
    % Add legend for Markovian dynamics
    % \node[below=1cm of x3, align=center] (note) {Graphical model of a state-space model \\ with Markovian dynamics};
    
    \end{tikzpicture}
}
\caption{The graphical model of \MN. In gray observed variables and in white are latent variables.} %We use the forward process of the diffusion model to sample auxiliary observations $\{\rvy_t\}_{t=1}^T$ and then use the backward process for posterior inference.}
\label{fig:graphical_model}
\end{figure}