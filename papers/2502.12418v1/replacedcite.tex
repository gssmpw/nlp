\section{Related Work}
\label{sec2}
% \par The primary objective of color constancy is to estimate the chromaticity of the illuminant. Subsequently, image color correction is performed using the estimated illuminant chromaticity within the existing transformation framework, most commonly the von Kries model____. In the following section, we will first analyze the imaging equation. Then, we will review the research focus of this paper—deep neural networks-driven color constancy and the data augmentation strategy employed.
\subsection{Image Formation for Color Constancy} \label{sec2.1}
\par Based on the Dichromatic Reflection Model____, the image $\boldsymbol{f}(\boldsymbol{x}) = \left( f_R(x), f_G(x), f_B(x) \right)^\top$ is determined by the scene illuminant $I(\boldsymbol{x}, \lambda)$, the camera sensor response function $\boldsymbol{\rho}(\lambda) = \left( \rho_R(\lambda), \rho_G(\lambda), \rho_B(\lambda) \right)^\top$, the surface reflectance $S(\boldsymbol{x}, \lambda)$, and the parameters $m_b(\boldsymbol{x})$ and $m_s(\boldsymbol{x})$ which represent body reflection and surface reflection, respectively____, such that:
\begin{equation} \label{eq1}
\begin{aligned}
  f_c(\boldsymbol{x}) &= m_b(\boldsymbol{x}) \int_\omega I(\boldsymbol{x}, \lambda) \rho_c(\lambda) S(\boldsymbol{x}, \lambda) \, d\lambda \\
  &\quad\quad + m_s(\boldsymbol{x}) \int_\omega I(\boldsymbol{x}, \lambda) \rho_c(\lambda) \, d\lambda,
\end{aligned}
\end{equation}
where $c \in \{R, G, B\}$, $\lambda$ denotes the illuminant wavelength, and $\omega$ represents the visible wavelength range. The scaling factors $m_b(\boldsymbol{x})$ and $m_s(\boldsymbol{x})$ depend on the viewing angle, illuminant direction, and surface orientation____. 
The objective of the color constancy model is to estimate the illuminant chromaticity from the color-biased image $\boldsymbol{f}(\boldsymbol{x})$, denoted as $\boldsymbol{e}(\boldsymbol{x})$:
\begin{equation} \label{eq2}
\boldsymbol{e}(\boldsymbol{x}) = \left(e_R(\boldsymbol{x}), e_G(\boldsymbol{x}), e_B(\boldsymbol{x})\right)^\top = \int_\omega I(\boldsymbol{x}, \lambda) \rho_c(\lambda) \, d\lambda.
\end{equation}

\subsection{Color Constancy} \label{sec2.2}
Color constancy methods typically assume that the scene is illuminated by a single illuminant. Under this assumption, the objective of color constancy model is to predict the $R$, $G$, and $B$ values for each image. In this paper, we follow the assumption and categorize these methods into two groups—DNN-based and non-DNN-based—providing an overview of each.
% Single-illuminant color constancy, based on the assumption that a scene is illuminated by a single illuminant, is the most widely studied tasks within the field of color constancy. In this task, color constancy methods are required to predict the $R$, $G$, and $B$ values of the single illuminant $\boldsymbol{e}$ for each image, thereby simplifying the challenges associated with the under-constrained problem of color constancy.
% \begin{equation} \label{eq3}
% \boldsymbol{e} = \int_\omega I(\lambda) \rho_c(\lambda) \, d\lambda
% \end{equation}
% In this paper, we categorize single-illuminant color constancy methods into non-DNN and DNN-based types, reviewing each separately.

\subsubsection{Non-DNN Methods} \label{sec2.2.1}
Early non-DNN color constancy methods typically estimate the illuminant through statistical assumptions, including the gray-world____, white-patch____, and gray-edge assumptions____, among others. These statistical assumptions can be encompassed within a unified framework____:
\begin{equation} \label{eq3}
\left( \int \left| \frac{\partial^n \boldsymbol{f}^{\sigma}(\boldsymbol{x})}{\partial \boldsymbol{x}^n} \right|^p \, d\boldsymbol{x} \right)^{\frac{1}{p}} = k \cdot \boldsymbol{e}^{n,p,\sigma},
\end{equation}
where the variable \( n \) denotes the order of the derivative, \( \sigma \) represents the standard deviation of a Gaussian filter, with \( \boldsymbol{f}^{\sigma}(\boldsymbol{x}) = G^{\sigma} * \boldsymbol{f}(\boldsymbol{x}) \), and \( p \) specifies the order of the Minkowski norm. 
Eq.~\ref{eq3} assumes a fixed ratio \( k \) between image statistics and the illuminant, allowing the illuminant value to be estimated directly from image statistics.
Similarly, some learning-based non-DNN methods____ also build machine learning models based on images statistical assumptions. Moreover, additional methods seek to develop models through the lenses of gamut mapping____, Bayesian inference____, regression trees____, frequency domain____, and so on.

\subsubsection{DNN-based Methods} \label{sec2.2.2}
\par With the advancement of deep learning, Deep Neural Network-driven Color Constancy~(DNNCC) has attracted significant attention from researchers.
Bianco et al.____ first introduced CNN for illuminant chromaticity estimation. Given the limited number of training samples typically found in color constancy datasets, they divided images into patches to alleviate data insufficiency.
Additionally, Yu et al.____ proposed a cascading architecture to capture dependencies between light source hypotheses, facilitating a coarse-to-fine estimation. Furthermore, both IGTN____ and CLCC____ emphasized that the performance of DNNCC is highly sensitive to variations in scene content. They utilize metric learning and contrastive learning, respectively, to focus on illuminant-dependent features. 
\par Given the ill-posed nature of the color constancy problem, researchers have explored various strategies to handle the inherent ambiguities. For instance, Shi et al.____ developed DS-Net to mitigate ambiguities from unknown reflections and object appearances. Similarly, Song et al.____ framed color constancy as a grouped regression problem, generating multiple possible illuminant solutions to tackle these ambiguities. Moreover, Hu et al.____ introduced a confidence-weighted layer to identify key regions crucial for illuminant estimation, thereby reducing the influence of uncertainty. Recently, Buzzelli et al.____ proposed three uncertainty estimation strategies for color constancy and developed cascaded methods based on the estimated uncertainties.
\par Unlike the aforementioned studies that directly estimate illuminant chromaticity, Hernandez-Juarez et al.____ define color constancy as a classification task instead of a regression task, determining whether candidate illuminant correctly adjusts the image. 
Additionally, Bianco et al.____ employed a brightness map to identify achromatic regions, directly inferring the illuminant chromaticity.
\par Alongside the utilization of RGB images as input, recent studies have investigated the integration of additional data sources, such as the degree of linear polarization____, point clouds____, and multiple rear-facing cameras____, to improve illuminant estimation accuracy. These methods have shown significant improvements in accuracy, attributed to the integration of additional data sources. However, reliance on specialized hardware limits their accessibility in broader applications.
\par Beyond improving the accuracy of DNNCC models, some studies have focused on reducing their computational complexity. Buzzelli et al.____ proposed a convolutional model based on low-level image features to enable efficient illuminant estimation through image statistics. Similarly, Domislović et al.____ introduced One-Net, a lightweight network with five 1$\times$1 kernel convolutional layers, designed to leverage low-level image statistics. Moreover, Laakom et al.____ introduced BoCF, which achieves a reduction in the number of model parameters required for illuminant estimation through bag-of-features pooling.
\par Aside from illuminant estimation, another key challenge in color constancy is generalizing across varied camera sensors, as spectral sensitivity differences cause shifts in illuminant distributions. Xiao et al.____ and Zhang et al.____ addressed this with multi-domain and domain-adversarial learning to derive shared features from different sensors. Alternatively, Afifi et al.____ advanced this by leveraging unlabeled test-phase images for dynamic inference of sensor spectral sensitivity profiles. Tang et al.____ introduced a statistical approach to convert sensor-specific illuminant labels to a sensor-agnostic format, promoting model generalization across diverse sensors.

% \par Color constancy models have traditionally relied on RGB image inputs to estimate illuminants. More recently, advancements have sought to overcome the limitations of RGB-only data by integrating additional data sources to improve estimation.
% For example, Ono et al.____ leveraged the degree of linear polarization to effectively identify achromatic pixels in images, using these pixels to perform illuminant estimation. Additionally, Xing et al.____ introduced the Point Cloud Color Constancy (PCCC) model, which synthesizes spatial and chromatic data in a six-dimensional point cloud that combines spatial coordinates with RGB intensities, leveraging the PointNet architecture for precise illuminant estimation. Similarly, Abdelhamed et al.____ utilized spectral measurement results from multiple rear-facing cameras, commonly found in modern smartphones. By applying a 3×3 linear transformation between these spectral measurements, they trained a lightweight neural network to enable robust illuminant estimation.
% These methods demonstrate notable accuracy improvements due to their use of additional data sources. However, their reliance on specialized hardware, such as polarization sensors or multi-camera arrays, limits their accessibility for broader applications.

% \par Beyond improving illuminant estimation accuracy, some studies have focused on reducing the computational complexity of color constancy models. For example, Buzzelli et al.____ introduced a general convolutional model rooted in low-level image feature frameworks, leveraging image statistics to achieve efficient illuminant estimation. Additionally, Domislović et al.____ proposed One-Net, a lightweight convolutional neural network consisting of five convolutional layers with (1,1) kernel sizes. Instead of extracting complex high-level semantic information from images, One-net is designed to focus on low-level image statistics to achieve efficient illuminant estimation. Moreover, Laakom et al.____ introduced BoCF, which achieves parameter efficiency in illuminant estimation through Bag-of-Features pooling.

% \par Alongside improving illuminant estimation accuracy, another significant challenge in color constancy is achieving generalization across diverse camera sensors. Variations in spectral sensitivity among camera sensors lead to shifts in illuminant distributions, which limit the effectiveness of models trained on data from a single camera. To address this challenge, Xiao et al.____ and Zhang et al.____ utilized multi-domain learning and domain-adversarial learning, respectively, to derive shared features across datasets originating from disparate sensors. Afifi et al.____ advanced this area by proposing a method that leverages unlabeled images from the same sensor during the testing phase to infer the sensor's spectral characteristics, thus allowing the model to dynamically adapt to the spectral profile of the test image's sensor. Moreover, Tang et al.____ introduced a statistical framework to transform sensor-specific illuminant labels into a sensor-agnostic representation, thereby enhancing the model's capacity for generalization across images captured by diverse sensor.


\subsection{Data Augmentation for DNNCC} \label{sec2.3}
\par Data augmentation diversifies the training data through various transformations. Common geometric transformations, such as random flipping, cropping, and rotating, are also used for color constancy____. Nevertheless, due to the high sensitivity to color variations, most color transformations, such as channel dropping and swapping, are unsuitable for color constancy. The most prevalent color augmentation technique for color constancy involves linearly scaling the illuminant and adjusting image colors simultaneously____, thus creating richer scene-light combinations. A more precise method____ identifies 24-color swatch values and uses a transformation matrix to swap illuminants between images.
Although the aforementioned data augmentation significantly enhanced the performance of DNNCC, these strategies did not explicitly consider brightness transformations. Our study reveals a general issue with the brightness robustness of DNNCC models. To tackle this challenge, we propose an adversarial brightness augmentation strategy. This approach effectively mitigates performance degradation caused by brightness sensitivity by learning from high-risk brightness transformations, demonstrating its effectiveness as an innovative augmentation method for DNNCC models.
% dropping____ and swapping____,
\subsection{Adversarial Attack and Defense} \label{sec2.4}
DNN have achieved state-of-the-art performance across a wide range of tasks. However, extensive research has revealed that DNN are highly susceptible to adversarial examples. Szegedy et al.____ were the first to introduce the concept of adversarial examples, employing the L-BFGS method to generate small perturbations that, when added to clean samples, cause DNN to produce incorrect predictions with high confidence. Subsequently, Goodfellow et al.____ proposed the Fast Gradient Sign Method (FGSM) as a more efficient approach to generating adversarial examples:
\begin{equation}~\label{eq4}
\begin{gathered}
    \delta = \varepsilon sign\left( {{\nabla }_{\boldsymbol{x}}}\mathcal{J} \left( \mathcal{M}\left( \boldsymbol{f}(\boldsymbol{x}) \right),y \right) \right),
\end{gathered}
\end{equation}
where \(\delta\) is an additive perturbation, defined as the product of the scalar \(\varepsilon\) and the sign of the gradient \(\nabla_{\boldsymbol{x}}\) of the loss function \(\mathcal{J}(\mathcal{M}(\boldsymbol{f}(\boldsymbol{x})), y)\), where $\mathcal{J}$, $\mathcal{M}$, $y$, and $\boldsymbol{x}$ represent the loss function, DNN model, ground truth, and clean sample, respectively, with $\boldsymbol{f}(\boldsymbol{x}) = \left( f_R{(\boldsymbol{x})}, f_G{(\boldsymbol{x})}, f_B{(\boldsymbol{x})} \right)^\top$. The scalar \(\varepsilon\) ensures the perturbation to be minimal but effective to mislead the model. Similar to FGSM, numerous attack variants such as PGD____ and C\&W____ have been proposed. Additionally, some research has shifted focus from pursuing powerful attacks to expanding perturbation types through techniques such as altering color temperature____ and retouching____
% \subsection{Adversarial Attack Defense}
\par The success of adversarial attacks has driven advancements in defense research. To improve the robustness of DNN against adversarial examples, several methods have been proposed, including adversarial training____, defensive distillation____, and gradient regularization____, etc. Additionally, some defences methods focus on preprocessing to eliminate adversarial perturbations, such as JPEG compression____, as well as denoising using diffusion models____.
% random resizing and scaling____
Additionally, some research has focused on detecting adversarial samples during the testing phase. For instance, Metzen et al.____ trained auxiliary models to identify adversarial inputs by using intermediate layer outputs as features. Gao et al.____ developed an autoencoder-based model robust to adversarial perturbations, detecting adversarial samples by comparing prediction consistency with the protected model.
Gao et al.____ developed an autoencoder-based model that is robust to adversarial perturbations, detecting adversarial samples by comparing prediction consistency with the protected model.

% Universal Adversarial Perturbations
% 传统的对抗攻击主要依赖于单一输入图像，即为每个图像生成一个特定的扰动，使其能迷惑模型预测。然而，这种扰动针对性较强，难以泛化到不同的图像。这导致训练后的模型无法很好的应对在各种图像内容上的亮度变化。
% 导致模型仅能改善训练集图像的亮度鲁棒性。