\section{Outlook}
Our study offers significant potential for further exploration in self-supervised learning. First, replacing the Mallows' distance with alternative divergences may yield more efficient representations. Second, as highlighted by \citet{wang2020understanding}, \citet{Awasthi2022DoMN}, and \citet{duan2024unsupervisedtransferlearningadversarial}, different self-supervised learning losses can lead to distinct structures in the representation space. While analyzing the structure of some existing losses is challenging due to their specialized design, recovering these structures via DM and examining the hyperparameters of the reference distribution may provide valuable insights into their interpretation. Lastly, the condition in Definition~\ref{def: (σ,δ)-augmentation} represents a crucial factor in advancing self-supervised learning methods. Random augmentation compositions may be too disruptive for handling more complex real-world tasks. How to derive more effective augmentations that align with the requirements in Definition~\ref{def: (σ,δ)-augmentation} remains an open question for future investigation.