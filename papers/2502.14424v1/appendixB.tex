\section{Sample Theorem}\label{section: Appendix B}
The sample theorem in this study mainly draws on the technique used in \citet{duan2024unsupervisedtransferlearningadversarial}.
\subsection{Error Decomposition}
Note that $\mathcal{L}(f) = \sup_{g \in \mathcal{G}}\mathcal{L}(f, g)$, define the stochastic error $\mathcal{E}_{\mathrm{sta}}$, the encoder approximation error $\mathcal{E}_{\mathcal{F}}$ and the discriminator approximation error, $\mathcal{E}_{\mathcal{G}}$ respectively as follows
\begin{gather*}
    \mathcal{E}_{\mathrm{sta}} := \sup_{f \in \widehat{\mathcal{F}}, g \in \widehat{\mathcal{G}}}\abs{\mathcal{L}(f, g) - \widehat{\mathcal{L}}(f, g)},\\ 
    \mathcal{E}_{\mathcal{F}} := \inf_{f \in \widehat{\mathcal{F}}}\{\mathcal{L}(f) - \mathcal{L}(f^*)\}, \\
    \mathcal{E}_{\mathcal{G}} := \sup_{f \in \widehat{\mathcal{F}}}\big\vert\sup_{g \in \mathcal{G}}\mathcal{W}(f, g) - \sup_{g \in \widehat{\mathcal{G}}}\mathcal{W}(f, g)\big\vert.
\end{gather*}

Then we have following relationship.
\begin{lemma}
$\mathcal{L}(\hat{f}_{n_S}) \leq \mathcal{L}(f^*) +  2\mathcal{E}_{\mathrm{sta}} + \mathcal{E}_{\mathcal{F}} + 2\mathcal{E}_{\mathcal{G}}$.
\end{lemma}
\begin{proof}
    For any $f \in \widehat{\mathcal{F}}$,
    \begin{align*}
        \mathcal{L}(\hat{f}_{n_S}) &= \big\{\mathcal{L}(\hat{f}_{n_S}) - \sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(\hat{f}_{n_S}, g)\big\} + \big\{\sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(\hat{f}_{n_S}, g) - \sup_{g \in \widehat{\mathcal{G}}}\widehat{\mathcal{L}}(\hat{f}_{n_S}, g)\big\} \\
        &\qquad +\big\{\sup_{g \in \widehat{\mathcal{G}}}\widehat{\mathcal{L}}(\hat{f}_{n_S}, g) - \sup_{g \in \widehat{\mathcal{G}}}\widehat{\mathcal{L}}(f, g)\big\} + \big\{\sup_{g \in \widehat{\mathcal{G}}}\widehat{\mathcal{L}}(f, g) - \sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(f, g)\big\} \\
        &\qquad +\big\{\sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(f, g) - \mathcal{L}(f)\big\} + \big\{\mathcal{L}(f) - \mathcal{L}(f^*)\big\} + \mathcal{L}(f^*)
    \end{align*}
    For the second term and the forth term, we can conclude
    \begin{align*}
        \sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(\hat{f}_{n_S}, g) - \sup_{g \in \widehat{\mathcal{G}}}\widehat{\mathcal{L}}(\hat{f}_{n_S}, g) \leq \sup_{g \in \widehat{\mathcal{G}}}\{\mathcal{L}(\hat{f}_{n_S}, g) - \widehat{\mathcal{L}}(\hat{f}_{n_S}, g)\} \leq  \mathcal{E}_{\mathrm{sta}}
    \end{align*}
    and
    \begin{align*}
        \sup_{g \in \widehat{\mathcal{G}}}\widehat{\mathcal{L}}(f, g) - \sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(f, g) &\leq \sup_{g \in \widehat{\mathcal{G}}}\{\widehat{\mathcal{L}}(f, g) - \mathcal{L}(f, g)\}\leq \mathcal{E}_{\mathrm{sta}}
    \end{align*}
    The first and the fifth terms both can be bounded $\mathcal{E}_{\mathcal{G}}$. For the first term:
    \begin{align*}
        \mathcal{L}(\hat{f}_{n_S}) - \sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(\hat{f}_{n_S}, g) &\leq \sup_{f \in \widehat{\mathcal{F}}}\{\mathcal{L}(f) - \sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(f, g)\} = \sup_{f \in \widehat{\mathcal{F}}}\{\sup_{g \in \mathcal{G}}\mathcal{W}(f, g) - \sup_{g \in \widehat{\mathcal{G}}}\mathcal{W}(f, g)\} \\
        &\leq \sup_{f \in \widehat{\mathcal{F}}}\big\vert\sup_{g \in \mathcal{G}}\mathcal{W}(f, g) - \sup_{g \in \widehat{\mathcal{G}}}\mathcal{W}(f, g)\big\vert = \mathcal{E}_{\mathcal{G}}.
    \end{align*}
    Similar for the fifth term, 
    \begin{align*}
        \sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(f, g) - \mathcal{L}(f) &\leq \sup_{f \in \widehat{\mathcal{F}}}\{\sup_{g \in \widehat{\mathcal{G}}}\mathcal{L}(f,g) - \mathcal{L}(f)\} = \sup_{f \in \widehat{\mathcal{F}}}\{\sup_{g \in \widehat{\mathcal{G}}}\mathcal{W}(f,g) - \sup_{g \in \mathcal{G}}\mathcal{W}(f,g)\} \\
        &\leq \sup_{f \in \widehat{\mathcal{F}}}\abs{\sup_{g \in \widehat{\mathcal{G}}}\mathcal{W}(f,g) - \sup_{g \in \mathcal{G}}\mathcal{W}(f,g)} = \mathcal{E}_{\mathcal{G}}
    \end{align*}
    Finally, taking infimum over all $f \in \widehat{\mathcal{F}}$ yields
    \begin{align*}
        \mathcal{L}(\hat{f}_{n_S}) \leq \mathcal{L}(f^*) + 2\mathcal{E}_{\mathrm{sta}} + \mathcal{E}_{\mathcal{F}} + 2\mathcal{E}_{\mathcal{G}}.
    \end{align*}
\end{proof}

\subsection{The Stochastic Error}\label{subsection: The Stochastic Error}
Let $\ell(\bm{v}_1, \bm{v}_2, v_3, v_4, v_5) = \norm{\bm{v}_1 - \bm{v}_2}_2^2 + v_3 - \frac{1}{2}\{v_4 + v_5\}$, where $\bm{v}_1,\bm{v}_2 \in \R^{d^*}$ and $v_3, v_4, v_5 \in \R$. It immediately follows that 
\begin{align*}
    \widehat{\mathcal{L}}(f,g) = \frac{1}{n_S}\sum_{i=1}^{n_S}\ell\big(f(\mathtt{X}^{(i)}_{S,1}), f(\mathtt{X}^{(i)}_{S,2}), g(\mathcal{R}^{(i)}), g(f(\mathtt{X}^{(i)}_{S,1})), g(f(\mathtt{X}^{(i)}_{S,2}))\big).
\end{align*}
Let $\widetilde{\mathcal{D}}^\prime_S = \{(\mathtt{X}_{S,1}^{\prime(i)}, \mathtt{X}_{S,2}^{\prime(i)}, \mathcal{R}^\prime_i): 1 \leq i \leq n_S\}$ be a random copy of $\widetilde{\mathcal{D}}_S$, which follows that 
\begin{align*}
    \mathcal{L}(f,g) = \frac{1}{n_S}\sum_{i=1}^{n_S}\E_{\widetilde{\mathcal{D}}^\prime_s}\big\{\ell\big(f(\mathtt{X}_{S,1}^{\prime(i)}), f(\mathtt{X}_{S,2}^{\prime(i)}), g(\mathcal{R}^\prime_i), g(f(\mathtt{X}_{S,1}^{\prime(i)})), g(f(\mathtt{X}_{S,2}^{\prime(i)}))\big)\big\}
\end{align*}
Plugging this equation into the definition of $\mathcal{E}_{\mathrm{sta}}$ yields
\begin{align*}
    \E_{\widetilde{\mathcal{D}}_S}\big\{\mathcal{E}_{\mathrm{sta}}\big\} &= \E_{\widetilde{\mathcal{D}}_S}\big\{\sup_{f \in \widehat{\mathcal{F}}, g \in \widehat{\mathcal{G}}}\abs{\mathcal{L}(f, g) - \widehat{\mathcal{L}}(f, g)}\big\} \\
    &\leq \E_{\widetilde{\mathcal{D}}_S}\Big[\sup_{(f, g) \in \widehat{\mathcal{F}} \times \widehat{\mathcal{G}}}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\E_{\widetilde{\mathcal{D}}^\prime_S}\big\{\ell\big(f(\mathtt{X}_{S,1}^{\prime(i)}), f(\mathtt{X}_{S,2}^{\prime(i)}), g(\mathcal{R}^\prime_i), g(f(\mathtt{X}_{S,1}^{\prime(i)})), g(f(\mathtt{X}_{S,2}^{\prime(i)}))\big)\big\} \\
    &\qquad - \frac{1}{n_S}\sum_{i=1}^{n_S}\ell\big(f(\mathtt{X}_{S,1}^{(i)}), f(\mathtt{X}_{S,2}^{(i)}), g(\mathcal{R}^{(i)}), g(f(\mathtt{X}_{S,1}^{(i)})), g(f(\mathtt{X}_{S,2}^{(i)}))\big) \Big\vert\Big] \\
    &\leq \E_{\widetilde{\mathcal{D}}_S, \widetilde{\mathcal{D}}_S^\prime}\Big\{\sup_{(f, g) \in \widehat{\mathcal{F}} \times \widehat{\mathcal{G}}}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\ell\big(f(\mathtt{X}_{S,1}^{\prime(i)}), f(\mathtt{X}_{S,2}^{\prime(i)}), g(\mathcal{R}^{(i)}), g(f(\mathtt{X}_{S,1}^{\prime(i)})), g(f(\mathtt{X}_{S,2}^{\prime(i)}))\big) \\
    &\qquad - \ell\big(f(\mathtt{X}_{S,1}^{(i)}), f(\mathtt{X}_{S,2}^{(i)}), g(\mathcal{R}^{(i)}), g(f(\mathtt{X}_{S,1}^{(i)})), g(f(\mathtt{X}_{S,2}^{(i)}))\big) \Big\vert\Big\} \\
    &\leq \E_{\widetilde{\mathcal{D}}_S, \widetilde{\mathcal{D}}_S^\prime, \bm{\xi}}\Big\{\sup_{(f, g) \in \widehat{\mathcal{F}} \times \widehat{\mathcal{G}}}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\xi_i\Big(\ell\big(f(\mathtt{X}_{S,1}^{\prime(i)}), f(\mathtt{X}_{S,2}^{\prime(i)}), g(\mathcal{R}^{(i)}), g(f(\mathtt{X}_{S,1}^{\prime(i)})), g(f(\mathtt{X}_{S,2}^{\prime(i)}))\big) \\
    &\qquad - \ell\big(f(\mathtt{X}_{S,1}^{(i)}), f(\mathtt{X}_{S,2}^{(i)}), g(\mathcal{R}^{(i)}), g(f(\mathtt{X}_{S,1}^{(i)})), g(f(\mathtt{X}_{S,2}^{(i)}))\big)\Big)\Big\vert\Big\},
    \end{align*}
    where the last inequality stems from the standard randomization techniques in empirical process theory, as detailed in \citet{gine2016mathematical}. Moreover, since $\widetilde{\mathcal{D}}^\prime_S$ is a random copy of $\mathcal{D}_S$, we have
    \begin{align}\label{eq: Esta < complexity}
    \E_{\widetilde{\mathcal{D}}_S}\big\{\mathcal{E}_{\mathrm{sta}}\big\}&\leq 2\E_{\widetilde{\mathcal{D}}_S, \bm{\xi}}\Big\{\sup_{(f, g) \in \widehat{\mathcal{F}} \times \widehat{\mathcal{G}}}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\xi_i\ell\big(f(\mathtt{X}_{S,1}^{(i)}), f(\mathtt{X}_{S,2}^{(i)}), g(\mathcal{R}^{(i)}), g(f(\mathtt{X}_{S,1}^{(i)})), g(f(\mathtt{X}_{S,2}^{(i)}))\big)\Big\vert\Big\} \nonumber \\
    &\lesssim \E_{\widetilde{\mathcal{D}}_S, \bm{\xi}}\Big[\sup_{(f, g) \in \widehat{\mathcal{F}} \times \widehat{\mathcal{G}}}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\sum_{j=1}^{d^*}\big\{\xi_{i,j,1}f_j(\mathtt{X}_{S,1}^{(i)})+ \xi_{i,j,2}f_j(\mathtt{X}_{S,2}^{(i)})\big\} + \xi_{i,1}g(\mathcal{R}^{(i)})  \nonumber\\
    &\qquad +\xi_{i,2}g(f(\mathtt{X}_{S,1}^{(i)})) + \xi_{i,3}g(f(\mathtt{X}_{S,2}^{(i)}))\Big\vert\Big] \nonumber\\
    &\lesssim \E_{\widetilde{\mathcal{D}}_S, \bm{\xi}}\Big\{\sup_{f \in \widehat{\mathcal{F}}}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\sum_{j=1}^{d^*}\xi_{i,j}f_j(\mathtt{X}_{S,1}^{(i)})\Big\vert\Big\}+  \E_{\widetilde{\mathcal{D}}_S, \bm{\xi}}\Big\{\sup_{g \in \widehat{\mathcal{G}}}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\xi_{i}g(\mathcal{R}^{(i)})\Big\vert\Big\} \nonumber\\ 
    &\qquad+ \E_{\widetilde{\mathcal{D}}_S, \bm{\xi}}\Big\{\sup_{(f,g) \in \widehat{\mathcal{F}} \times \widehat{\mathcal{G}}}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\xi_{i}g(f(\mathtt{X}_{S,1}^{(i)}))\Big\vert\Big\},
    \end{align}
    where the second inequality follows from the vector-contraction principle, derived by combining \citet{maurer2016vectorcontraction} with Theorem 3.2.1 in \citet{gine2016mathematical}.
    \begin{lemma}[Vector-contraction principle]
    \label{lemma: vector-contraction principle}
    Let $\mathcal{X}$ be any set, $(x_1, \ldots, x_n) \in \mathcal{X}^n$, let $\mathcal{H}$ be a class of functions $f:\mathcal{X} \rightarrow \ell_2$ and let $h_i: \ell_2 \rightarrow \R$ have Lipschitz norm $L^\prime$. Then
    \begin{align*}
    \E\sup_{f \in \mathcal{H}}\Big\vert\sum_i\xi_ih_i(f(x_i))\Big\vert \leq 2\sqrt{2}L^\prime\E\sup_{f \in \mathcal{H}}\Big\vert\sum_{i,k}\xi_{ik}f_k(x_i)\Big\vert,
    \end{align*}
    where $\xi_{ik}$ is an independent doubly indexed Rademacher sequence and $f_k(x_i)$ is the $k$-th component of $f(x_i)$.
    \end{lemma}
    To deal with three terms concluded in~\eqref{eq: Esta < complexity}, it is necessary to introduce several definitions and lemmas below.
    \begin{definition}[Covering number]\label{def: covering number}
        Let $n \in \N$, $\mathcal{S} \subseteq \R^n$, and $\rho > 0$. A set $\mathcal{N} \subseteq \mathcal{S}$ is called a $\varrho$-net of $\mathcal{S}$ with respect to a metric $d$ if for every $\bm{u} \in \mathcal{S}$, there exists $\bm{v} \in \mathcal{N}$ such that $d(\bm{u}, \bm{v}) \leq \varrho$. The covering number of $\mathcal{S}$ is defined as
        \begin{align*}
        \mathcal{N}(\varrho, \mathcal{S}, d) := \min\big\{\abs{\mathcal{Q}}: \mathcal{Q} \text{ is an $\varrho$-cover of }\mathcal{S}\big\},
        \end{align*}
        where $\abs{\mathcal{Q}}$ is the cardinality of the set $\mathcal{Q}$.
    \end{definition}
    
    \begin{definition}[Uniform covering number]\label{def: uniform covering number}
        Let $\mathcal{H}$ be a class of functions from $\mathcal{X}$ to $\R$. Given a sequence $x = (x_1, x_2, \ldots, x_k) \in \mathcal{X}^k$, define $\mathcal{H}_{\vert_x}$ be the subset of $\R^n$ given by $\mathcal{H}_{\vert_x} = \{(f(x_1), f(x_2), \ldots, f(x_k)): f \in \mathcal{H}\}$. For a positive number $\varrho$, the uniform covering number is given by
        \begin{align*}
            \mathcal{N}_\infty(\varrho, \mathcal{H}, k) = \max\big\{\mathcal{N}(\varrho, \mathcal{H}_{\vert_x}, d): x \in \mathcal{X}^k\big\}.
        \end{align*}
    \end{definition}

    \begin{lemma}[Lemma 10.5 of \citet{anthony1999neural}]\label{lemma: N1 < N2 < N∞}
        Let $\mathcal{H}$ is a class of functions from $\mathcal{X}$ to $\R$. For any $\varrho > 0$ and $x \in \mathcal{X}^k$, we have the following inequality for the covering numbers:
        \begin{align*}
            \mathcal{N}(\varrho, \mathcal{H}_{\vert_x}, d_1) \leq \mathcal{N}(\varrho, \mathcal{H}_{\vert_x}, d_2) \leq \mathcal{N}(\varrho, \mathcal{H}_{\vert_x},d_\infty),
        \end{align*}
        where $d_1(\bm{x}, \bm{y}) := \frac{1}{n}\sum_{i=1}^n\abs{x_i - y_i}, d_2(\bm{x}, \bm{y}) := \big(\frac{1}{n}\sum_{i=1}^n(x_i - y_i)^2\big)^{1/2}$ and $d_\infty(\bm{x}, \bm{y}) :=\max_{1\leq i\leq n}|x_i - y_i|$.
    \end{lemma}

    \begin{definition}[Sub-Gaussian process]
        A centred stochastic process $X(t), t \in T$, is sub-Gaussian with respect to a distance or pseudo-distance $d$ on $T$ if its increments satisfy the sub-Gaussian inequality, that is, if
        \begin{align*}
            \E[e^{\varsigma\{X(t) - X(s)\}}] \leq e^{\varsigma^2d^2(s,t)/2}, \varsigma \in \R, s,t \in T.
        \end{align*}
    \end{definition}

    The following lemma are derived from Theorem 2.3.7 in \citet{gine2016mathematical}:
    \begin{lemma}[Dudley's entropy integral]\label{lemma: dudley's entropy integral}
        Let $(T,d)$ be a separable pseudo-metric space, and let $X(t), t \in T$, be a sub-Gaussian process relative to $d$. Then
        \begin{align*}
            \E\sup_{t \in T}\abs{X(t)} \leq \E\abs{X(t_0)} + 4\sqrt{2}\int_0^{D/2}\sqrt{\log 2\mathcal{N}(\varrho, T, d)}d\varrho.
        \end{align*}
        where $t_0$ is any point in $T$ and $D$ is the diameter of $(T,d)$.
    \end{lemma}
    \begin{proof}
        It is remarkable to note that the essence of the entropy condition $\int_0^\infty \log\mathcal{N}(\rho, T, d)d\varrho < \infty$ in the proof of Theorem 2.3.7 in \citet{gine2016mathematical} is to establish the separability of $(T,d)$.
    \end{proof}
    Based on Lemma~\ref{lemma: dudley's entropy integral} and~\eqref{eq: Esta < complexity}, we can conclude
    \begin{align}\label{eq: Esta < entropy}
        \E_{\widetilde{\mathcal{D}}_S}\big\{\mathcal{E}_{\mathrm{sta}}\big\} &\lesssim \frac{1}{\sqrt{n_S}}\E_{\widetilde{\mathcal{D}}_S}\Big\{\int_0^{\mathtt{B}_1}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)_{\vert_{\{\mathtt{X}^{(i)}_{S,1}\}_{i=1}^{n_S}}}, d_2\big)}d\varrho \nonumber\\
        &\qquad+ \int_0^{\mathtt{B}_2}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d^*,1}(\mathtt{W}_2, \mathtt{D}_2, \mathtt{B}_2)_{\vert_{\{\mathcal{R}^{(i)}\}_{i=1}^{n_S}}}, d_2\big)}d\varrho \nonumber\\
        &\qquad+ \int_0^{\mathtt{B}_2}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d,1}(\max\{\mathtt{W}_1, \mathtt{W}_2\}, \mathtt{D}_1 + \mathtt{D}_2, \mathtt{B}_2)_{\vert_{\{\mathcal{R}^{(i)}\}_{i=1}^{n_S}}}, d_2\big)}d\varrho
        \Big\}.
    \end{align}
    We exemplify the first term in~\eqref{eq: Esta < complexity}. By the fact that $f \in \widehat{\mathcal{F}} \Rightarrow f_j \in \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)$ for any $1 \leq j \leq d^*$, along with Fubini theorem, we have 
    \begin{align*}
        &\E_{\widetilde{\mathcal{D}}_S, \bm{\xi}}\Big\{\sup_{f \in \widehat{\mathcal{F}}}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\sum_{j=1}^{d^*}\xi_{i,j}f_j(\mathtt{X}_{S,1}^{(i)})\Big\vert\Big\} \leq d^* \E_{\widetilde{\mathcal{D}}_S,\bm{\xi}}\Big[\E_{\bm{\xi}}\Big\{\sup_{f \in \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\xi_{i}f(\mathtt{X}_{S,1}^{(i)})\Big\vert\Big\} \\
        &= d^* \E_{\widetilde{\mathcal{D}}_S}\Big[\E_{\bm{\xi}}\Big\{\sup_{f \in \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)}\Big\vert\frac{1}{n_S}\sum_{i=1}^{n_S}\xi_{i}f(\mathtt{X}_{S,1}^{(i)})\Big\vert \Big\vert \mathtt{X}_{S,1}^{(i)}, 1\leq i\leq n_S\Big\}\Big].    
    \end{align*}
    Therefore, it suffices to show   
    \begin{align*}
        \E_{\bm{\xi}}\Big\{\sup_{f \in \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)}\Big\vert\frac{1}{\sqrt{n_S}}\sum_{i=1}^{n_S}\xi_{i}f(\mathtt{X}_{S,1}^{(i)})\Big\vert \Big\vert \widetilde{\mathcal{D}}_S\Big\} \leq \int_0^{\mathtt{B}_1}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)_{\vert_{\{\mathtt{X}^{(i)}_{S,1}\}_{i=1}^{n_S}}}, d_2\big)}d\varrho.
    \end{align*}
    In fact, conditioned on $\widetilde{\mathcal{D}}_S$, which implies that $\mathtt{X}_{S,1}^{(i)}, 1\leq i\leq n_S$ are fixed, the stochastic process $\{\frac{1}{\sqrt{n_S}}\sum_{i=1}^{n_S}\xi_{i}f(\mathtt{X}_{S,1}^{(i)}): f \in \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B_1})\}$ is a sub-Gaussian process, as $\xi_i, 1\leq i\leq n_S$ are independent Rademacher variables (see page 40 in \citet{gine2016mathematical}). Let $f_{\vert_{\{\mathtt{X}_{S,1}^{(i)}\}_{i=1}^{n_S}}} = (f(\mathtt{X}_{S,1}^{(1)}), \ldots, f(\mathtt{X}_{S,1}^{(n_S)})) \in \R^{n_S}$ for any $f \in \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)$, and define the distance on the index set $\mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)$ as 
    \begin{align*}
        d_{\mathcal{NN}}(f_1, f_2) &:= \sqrt{\E\Big\{\Big\vert\frac{1}{\sqrt{n_S}}\sum_{i=1}^{n_S}\xi_if_1(\mathtt{X}_{S,1}^{(i)}) - \frac{1}{\sqrt{n_S}}\sum_{i=1}^{n_S}\xi_if_2(\mathtt{X}_{S,1}^{(i)})\Big\vert\Big\}} \\
        &= \sqrt{\frac{1}{n_S}\sum_{i=1}^{n_S}\big(f_1(\mathtt{X}_{S,1}^{(i)}) - f_2(\mathtt{X}_{S,1}^{(i)})\big)^2}  = d_2( {f_1}_{\vert_{\{\mathtt{X}_{S,1}^{(i)}\}_{i=1}^{n_S}}},{f_2}_{\vert_{\{\mathtt{X}_{S,1}^{(i)}\}_{i=1}^{n_S}}}),
    \end{align*}
    we know that $(\mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)_{\vert_{\{\mathtt{X}^{(i)}_{S,1}\}_{i=1}^{n_S}}}, d_2)$ is a separable subset of $\R^{n_S}$ due to the existence of networks with rational parameters, satisfying the condition of Lemma~\ref{lemma: dudley's entropy integral}. Let $f_{\bm{0}} \in \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)$ be the network with all zero parameters. Setting $t_0$ in Lemma~\ref{lemma: dudley's entropy integral} as $f_{\bm{0}}$ yields $\E\abs{X(t_0)} = 0$. Furthermore, for any $f \in \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)$:
    \begin{align*}
        d_{\mathcal{NN}}(f, f_{\bm{0}}) = d_2(f_{\vert_{\{\mathtt{X}_{S,1}^{(i)}\}_{i=1}^{n_S}}},{f_{\bm{0}}}_{\vert_{\{\mathtt{X}_{S,1}^{(i)}\}_{i=1}^{n_S}}}) = \sqrt{\frac{1}{n_S}\sum_{i=1}^{n_S}f^2(X_{S,1}^{(i)})} \leq \mathtt{B}_1,
    \end{align*}
    hence the triangular inequality immediately follows that $D/2 \leq \mathtt{B}_1$. Combining all facts turns out what we desire. The second and the third terms in~\ref{eq: Esta < entropy} can be obtained similarly.

    We now introduce several definitions and lemmas to address the terms in~\eqref{eq: Esta < entropy}.
    \begin{definition}[VC-dimension] Let $\mathcal{H}$ denote a class of functions from $\mathcal{X}$ to $\{0,1\}$. For any non-negative integer $m$, we define the growth function of $\mathcal{H}$ as
    \begin{align*}
        \Pi_\mathcal{H}(m) := \max_{x_1, \ldots, x_m \in \mathcal{X}}\big\vert\{(h(x_1), \ldots, h(x_m)): h \in \mathcal{H}\} \big\vert.
    \end{align*}
    If $\abs{\{(h(x_1), \ldots, h(x_m)): h \in \mathcal{H}\}} = 2^m$, we say $\mathcal{H}$ shatters the set $\{x_1, \ldots, x_m\}$. The Vapnik-Chervonenkis dimension of $\mathcal{H}$, denoted $\mathrm{VCdim}(\mathcal{H})$, is the size of the largest shattered set, i.e. the largest $m$ such that $\Pi_\mathcal{H}(m) = 2^m$. If there is no largest $m$, we define $\mathrm{VCdim}(\mathcal{H}) = \infty$. Moreover, for a class $\mathcal{H}$ of real-valued functions, we may define $\mathrm{VCdim}(\mathcal{H}):=\mathrm{VCdim}(\mathrm{sgn}(\mathcal{H}))$, where $\{\mathrm{sgn}(f): f \in \mathcal{H}\}$ and $\mathrm{sgn}(x) = \1\{x > 0\}$.
    \end{definition}

    \begin{definition}[pseudodimension]
    Let $\mathcal{H}$ be a class of functions from $\mathcal{X}$ to $\R$. The pseudodimension of $\mathcal{H}$, written $\mathrm{Pdim}(\mathcal{H})$, is the largest integer $m$ for which there exists $(x_1, \ldots, x_m,y_1, \ldots, y_m) \in \mathcal{X}^m \times \R^m$ such that for any $(b_1, \ldots, b_m) \in \{0,1\}^m$ there exists $f \in \mathcal{H}$ such that $\forall i: f(x_i) > y_i \Leftrightarrow b_i = 1$.
    \end{definition}

    \begin{lemma}[Theorem 12.2 in \citet{bartlett2019nearly}]\label{lemma: covering number < Pdim}
        Let $\mathcal{H}$ be a set of real functions from a domain $\mathcal{X}$ to the bounded interval $[0,\mathtt{B}]$. Then for any $\varrho > 0$, the uniform covering number
        \begin{align*}
            \mathcal{N}_\infty(\varrho, \mathcal{H}, m) \leq \sum_{i=1}^{\mathrm{Pdim}(\mathcal{H})}\dbinom{m}{i}\dbinom{\mathtt{B}}{\varrho}^i,
        \end{align*}
        which is less than $\big(em\mathtt{B}/(\varrho\mathrm{Pdim}(\mathcal{H}))\big)^{\mathrm{Pdim}(\mathcal{H})}$ for $m \geq \mathrm{Pdim}(\mathcal{H})$. 
    \end{lemma}

    \begin{lemma}[Theorem 14.1 in \citet{anthony1999neural}]\label{lemma: Pdim < VCdim}
        For any $\mathtt{d}, \mathtt{W}, \mathtt{D} \in \N$,
        \begin{align*}
            \mathrm{Pdim}\big(\mathcal{NN}_{d,1}(\mathtt{W},\mathtt{L})\big) \leq \mathrm{VCdim}\big(\mathcal{NN}_{d,1}(\mathtt{W},\mathtt{L})\big).
        \end{align*}
    \end{lemma}
    \begin{lemma}[Theorem 6 in \citet{bartlett2019nearly}]\label{lemma: VCdim < DSlogS}
    For any $\mathtt{d}, \mathtt{W}, \mathtt{D} \in \N$, let $\mathtt{S}$ be the total number of parameters of $\mathcal{NN}_{\mathtt{d},1}(\mathtt{W}, \mathtt{D})$, we have
        $\mathrm{VCdim}\big(\mathcal{NN}_{\mathtt{d},1}(\mathtt{W}, \mathtt{D})\big) \lesssim \mathtt{D}\mathtt{S}\log_2\mathtt{S}$.
    \end{lemma}

    Let $\mathcal{NN}_{d_1,d_2}(\mathtt{W}, \mathtt{D})$ be the ReLU network class without the constraint $\sup_{\bm{x} \in \R^p} \Vert f_{\bm{\theta}}(\bm{x}) \Vert_{\infty} \le \mathtt{B}$ in Definition~\ref{def: Deep ReLU networks}, it immediately follows that $\mathcal{NN}_{d_1,d_2}(\mathtt{W}, \mathtt{D}, \mathtt{B}) \subseteq \mathcal{NN}(\mathtt{W}, \mathtt{D})$, implying following Lemma
    \begin{lemma}\label{lemma: Pdim(NN(W,D,B)) < Pdim(NN(W,D))}
        For any $\mathtt{d}, \mathtt{W}, \mathtt{D} \in \N$, we have  $\mathrm{Pdim}\big(\mathcal{NN}_{\mathtt{d},1}(\mathtt{W}, \mathtt{D}, \mathtt{B})\big) \leq \mathrm{Pdim}\big(\mathcal{NN}_{\mathtt{d},1}(\mathtt{W}, \mathtt{D})\big)$.
    \end{lemma}
    Following above preliminaries, we are now further processing~\eqref{eq: Esta < entropy}. 

    \begin{align*}
   \E_{\widetilde{\mathcal{D}}_S}\big\{\mathcal{E}_{\mathrm{sta}}\big\} &\lesssim \frac{1}{\sqrt{n_S}}\E_{\widetilde{\mathcal{D}}_S}\Big\{\int_0^{\mathtt{B}_1}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)_{\vert_{\{\mathtt{X}^{(i)}_{S,1}\}_{i=1}^{n_S}}}, d_2\big)}d\varrho \nonumber\\
    &\qquad+ \int_0^{\mathtt{B}_2}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d^*,1}(\mathtt{W}_2, \mathtt{D}_2, \mathtt{B}_2)_{\vert_{\{\mathcal{R}^{(i)}\}_{i=1}^{n_S}}}, d_2\big)}d\varrho \nonumber\\
    &\qquad+ \int_0^{\mathtt{B}_2}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d,1}(\max\{\mathtt{W}_1, \mathtt{W}_2\}, \mathtt{D}_1 + \mathtt{D}_2, \mathtt{B}_2)_{\vert_{\{\mathcal{R}^{(i)}\}_{i=1}^{n_S}}}, d_2\big)}d\varrho
    \Big\}\\
    &\leq \frac{1}{\sqrt{n_S}}\E_{\widetilde{\mathcal{D}}_S}\Big\{\int_0^{\mathtt{B}_1}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)_{\vert_{\{\mathtt{X}^{(i)}_{S,1}\}_{i=1}^{n_S}}}, d_\infty\big)}d\varrho \nonumber\\
    &\qquad+ \int_0^{\mathtt{B}_2}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d^*,1}(\mathtt{W}_2, \mathtt{D}_2, \mathtt{B}_2)_{\vert_{\{\mathcal{R}^{(i)}\}_{i=1}^{n_S}}}, d_\infty\big)}d\varrho \nonumber\\
    &\qquad+ \int_0^{\mathtt{B}_2}\sqrt{\log2\mathcal{N}\big(\varrho, \mathcal{NN}_{d,1}(\max\{\mathtt{W}_1, \mathtt{W}_2\}, \mathtt{D}_1 + \mathtt{D}_2, \mathtt{B}_2)_{\vert_{\{\mathcal{R}^{(i)}\}_{i=1}^{n_S}}}, d_\infty\big)}d\varrho
    \Big\} \tag{Lemma~\ref{lemma: N1 < N2 < N∞}}\\
    &\leq \frac{1}{\sqrt{n_S}}\Big\{\int_0^{\mathtt{B}_1}\sqrt{\log2\mathcal{N}_\infty\big(\varrho, \mathcal{NN}_{d,1}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1), n_S\big)}d\varrho \nonumber\\
    &\qquad+ \int_0^{\mathtt{B}_2}\sqrt{\log2\mathcal{N}_\infty\big(\varrho, \mathcal{NN}_{d^*,1}(\mathtt{W}_2, \mathtt{D}_2, \mathtt{B}_2), n_S\big)}d\varrho \nonumber\\
    &\qquad+ \int_0^{\mathtt{B}_2}\sqrt{\log2\mathcal{N}_\infty\big(\varrho, \mathcal{NN}_{d,1}(\max\{\mathtt{W}_1, \mathtt{W}_2\}, \mathtt{D}_1 + \mathtt{D}_2, \mathtt{B}_2), n_S\big)}d\varrho\Big\} \tag{Definition~\ref{def: uniform covering number}}\\
    &\lesssim  \Big(\frac{\mathrm{Pdim}(\mathcal{NN}_{d,1}\big(\mathtt{W}_1, \mathtt{D}_1)\big)\log n_S}{n_S}\Big)^{1/2} + \Big(\frac{\mathrm{Pdim}(\mathcal{NN}_{d^*,1}\big(\mathtt{W}_2, \mathtt{D}_2)\big)\log n_S}{n_S}\Big)^{1/2} \nonumber\\
    &\qquad+ \Big(\frac{\mathrm{Pdim}(\mathcal{NN}_{d,1}\big(\max\{\mathtt{W}_1,\mathtt{W}_2\}, \mathtt{D}_1+\mathtt{D}_2)\big)\log n_S}{n_S}\Big)^{1/2} \tag{Lemma~\ref{lemma: covering number < Pdim} and \ref{lemma: Pdim(NN(W,D,B)) < Pdim(NN(W,D))}}\\
    &\lesssim \Big(\frac{\mathrm{VCdim}(\mathcal{NN}_{d,1}\big(\mathtt{W}_1, \mathtt{D}_1)\big)\log n_S}{n_S}\Big)^{1/2} + \Big(\frac{\mathrm{VCdim}(\mathcal{NN}_{d^*,1}\big(\mathtt{W}_2, \mathtt{D}_2)\big)\log n_S}{n_S}\Big)^{1/2} \\
    &\qquad + \Big(\frac{\mathrm{VCdim}(\mathcal{NN}_{d,1}\big(\max\{\mathtt{W}_1,\mathtt{W}_2\}, \mathtt{D}_1+\mathtt{D}_2)\big)\log n_S}{n_S}\Big)^{1/2} \tag{Lemma~\ref{lemma: Pdim < VCdim}}\\
    &\leq \mathcal{O}\Big(\sqrt{\frac{(\mathtt{D}_1 + \mathtt{D}_2)^2\max\{\mathtt{W_1}, \mathtt{W}_2\}^2}{n_S}}\Big) \tag{Lemma~\ref{lemma: VCdim < DSlogS} and $\mathtt{S} \leq \mathtt{W}^2\mathtt{D}$}\\
    &\lesssim \frac{\mathtt{D}_1\mathtt{W_1}}{\sqrt{n_S}} \tag{$\mathtt{W}_1 \geq \mathtt{W}_2$ and $\mathtt{D}_1 \geq \mathtt{D}_2$}.
\end{align*}
We ignore the logarithmic term when deriving the penultimate inequality, as its impact on polynomial growth is negligible.


\subsection{The Approximation Error}\label{section: The Approximation Error}
In this section, we aim to determine the upper bounds for $\mathcal{E}_{\mathcal{F}}$ and $\mathcal{E}_{\mathcal{G}}$, following the approach outlined in \citet{yang2023nearly} and \citet{gao2024convergencecontinuousnormalizingflows}. To this end, we need to introduce serval definitions and lemmas in advance. Let $d \in \N$ and $U$ be an open subset of $\R^d$. We denote $L^\infty(U)$ as the standard Lebesgue space on $U$ with $L^\infty$ norm.
\begin{definition}[Sobolev space]
    Let $n \in \{0\}\cup\N$, the Sobolev space $W^{n, \infty}(U)$ is defined by
    \begin{align*}
        W^{n, \infty}(U) := \{f \in L^\infty(U): D^{\bm{\alpha}} f \in L^{\infty}(U) \text{ for all } \bm{\alpha} \in \N_0^{d} \text{ with } \norm{\bm{\alpha}}_1 \leq n\}.
    \end{align*}
    Moreover, for any $f \in W^{n, \infty}(U)$, we define the Sobolev norm $\norm{\cdot}_{W^{n, \infty}(U)}$ by
    \begin{align*}
        \norm{f}_{W^{n, \infty}(U)} := \max_{0 \leq \norm{\bm{\alpha}}_1 \leq n}\norm{D^{\bm{\alpha}} f}_{L^\infty(U)}.
    \end{align*}
\end{definition}

\begin{lemma}[Characterization of $W^{1, \infty}$ in \citet{evans2010partial}]
    \label{lemma: lipschitz v.s sobolev}
    Let $U$ be open and bounded, with $\partial U$ of class $C^1$. Then $f : U \to \R$ is Lipschitz continuous if and only if $f \in W^{1, \infty}(U)$
\end{lemma}
\begin{lemma}[Corollary B.2 in \citet{gao2024convergencecontinuousnormalizingflows}]
    \label{lemma: approximation with lipschitz regularity}
    For any $f \in W^{1, \infty}((0, 1)^d)$ such that  $\|f\|_{W^{1, \infty}((0,1)^d)} < \infty$, and $\mathtt{N}, \mathtt{L} \in \N$, there exists a function $f_{\bm{\theta}}$ implemented by a deep ReLU network with width $\widetilde{\mathcal{O}}(\mathtt{N})$, depth $\widetilde{\mathcal{O}}(\mathtt{L})$ and $\mathtt{B} \geq \norm{f}_\infty$ such that $\|f_{\bm{\theta}}\|_{W^{1, \infty}((0, 1))^d} \lesssim \|f\|_{W^{1, \infty}((0,1)^d)}$ and
    \begin{align*}
        \norm{f_{\bm{\theta}} - f}_{L^\infty([0,1]^d)} \lesssim \norm{f}_{W^{1, \infty}((0,1)^d)}(\mathtt{N}\mathtt{L})^{-2/d}.
    \end{align*}
\end{lemma}
\subsubsection{The Encoder Approximation Error \texorpdfstring{$\mathcal{E}_{\mathcal{F}}$}{EF}}\label{subsection: encoder approximation error}
Lemma~\ref{lemma: approximation with lipschitz regularity} and Lemma~\ref{lemma: lipschitz v.s sobolev} together demonstrate that the approximation capacity of $\mathcal{NN}_{d, 1}(\mathtt{W},\mathtt{D}, \mathtt{B})$ to Lipschitz functions can be made arbitrarily precise by increasing the scale of the neural network. Consequently, the function. $\hat{f}_{n_S}$ retains the property $R_1 \leq \norm{\hat{f}_{n_S}}_2 \leq R_2$  for some constants $R_1$ and $R_2$ close to  $R$, allowing us to directly apply Theorem~\ref{theorem: general version of the pop theorem}.

Recall the $\mathcal{L}(f)$ is defined as follow:
\begin{align*}
    \mathcal{L}(f)=\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2}) \Big\Vert_2^2 + \lambda \sup_{g \in \mathcal{G}} \mathop{\E}_{Z \sim \P_f} g(Z)  - \mathop{\E}_{\mathcal{R} \sim \P_\mathcal{R}} g(\mathcal{R}).
\end{align*}
For any $f$ with $B_1\leq \norm{f}_2 \leq B_2$, we know that
\begin{align*}
    \mathcal{E}_{\mathcal{F}} &:= \inf_{f \in \widehat{\mathcal{F}}}\big\{\mathcal{L}(f) - \mathcal{L}(f^*)\big\} \\
    &\lesssim \inf_{f \in \widehat{\mathcal{F}}}\Big[\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2}) \Big\Vert_2^2 -\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big\Vert f^*(\mathtt{X}_{S,1}) - f^*(\mathtt{X}_{S,2}) \Big\Vert_2^2 \\
    &\qquad+ \lambda \sup_{g\in\mathcal{G}}\E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\big\{g(f(\mathtt{X}_S)\big\} - \lambda \sup_{g\in\mathcal{G}}\E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\big\{g(f^*(\mathtt{X}_S)\big\} \Big]  \\
    &\lesssim \inf_{f \in \widehat{\mathcal{F}}}\Big\{\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2}) \Big\Vert_2^2 -\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big\Vert f^*(\mathtt{X}_{S,1}) - f^*(\mathtt{X}_{S,2}) \Big\Vert_2^2 \\
    &\qquad+ \lambda \sup_{g\in\mathcal{G}}\big[\E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\big\{g(f(\mathtt{X}_S)\big\} - \E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\big\{g(f^*(\mathtt{X}_S)\big\}\big] \Big\}  \\
    &\leq \inf_{f \in \widehat{\mathcal{F}}}\Big\{\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2}) \Big\Vert_2^2 -\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big\Vert f^*(\mathtt{X}_{S,1}) - f^*(\mathtt{X}_{S,2}) \Big\Vert_2^2 \\
    &\qquad+ \lambda\E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\big\|f(\mathtt{X}_S) - f^*(\mathtt{X}_S)\big\|_2 \Big\}  \tag{$g \in \mathrm{Lip}(1)$} \\
    &= \inf_{f \in \widehat{\mathcal{F}}}\Big[\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big\{\Big(\Big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2}) \Big\Vert_2 +  \Big\Vert f^*(\mathtt{X}_{S,1}) - f^*(\mathtt{X}_{S,2}) \Big\Vert_2\Big)\Big(\Big\Vert f(\mathtt{X}_{S,1}) \\
    &\qquad - f(\mathtt{X}_{S,2}) \Big\Vert_2 - \Big\Vert f^*(\mathtt{X}_{S,1}) - f^*(\mathtt{X}_{S,2}) \Big\Vert_2\Big)  \Big\}+ \lambda\E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\big\|f(\mathtt{X}_S) - f^*(\mathtt{X}_S)\big\|_2 \Big] \\
    &\lesssim \inf_{f \in \widehat{\mathcal{F}}}\Big\{\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big(\Big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2}) \Big\Vert_2 - \Big\Vert f^*(\mathtt{X}_{S,1}) - f^*(\mathtt{X}_{S,2}) \Big\Vert_2\Big) \\
    &\qquad + \E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\big\|f(\mathtt{X}_S) - f^*(\mathtt{X}_S)\big\|_2 \Big\} \tag{$\norm{f}_2 \leq R_2, \norm{f^*}_2 \leq R$}\\
    &\lesssim \inf_{f \in \widehat{\mathcal{F}}}\Big\{\mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big(\Big\Vert f(\mathtt{X}_{S,1}) - f^*(\mathtt{X}_{S,1}) \Big\Vert_2 + \Big\Vert f(\mathtt{X}_{S,2}) - f^*(\mathtt{X}_{S,2}) \Big\Vert_2\Big) \\
    &\qquad + \E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\big\|f(\mathtt{X}_S) - f^*(\mathtt{X}_S)\big\|_2 \Big\} \tag{Triangle inequlity} \\
    &\lesssim \inf_{f \in \widehat{\mathcal{F}}}\E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\big\|f(\mathtt{X}_S) - f^*(\mathtt{X}_S)\big\|_2 \\
    &= \inf_{f \in \widehat{\mathcal{F}}}\E_{X_S\sim\P_S}\E_{\mathtt{X}_S\sim\mathcal{A}(X_S)}\sqrt{\sum_{i=1}^{d^*}\{f_i(\mathtt{X}_S) - f_i^*(\mathtt{X}_S)\}^2} \\
    &\lesssim  \inf_{f \in \widehat{\mathcal{F}}}\sqrt{\sum_{i=1}^{d^*}\norm{f_i - f_i^*}_\infty^2}\\
    &\leq \sqrt{\sum_{i=1}^{d^*}\inf_{f_i \in \mathcal{NN}_{d, 1}(\floor{\mathtt{W}_1 / d^*}, \mathtt{D}_1, \mathtt{B}_1)}\norm{f_i - f^*_i}_\infty^2} \tag{$*$}\\
    &\lesssim \sup_{f \in W^{1, \infty}((0 ,1)^d)}\inf_{f_i \in \mathcal{NN}(\floor{\mathtt{W}_1/d^*}, \mathtt{D}_1, \mathtt{B}_1)}\norm{f_i - f}_\infty \\
    & \lesssim (\mathtt{D}_1\mathtt{W}_1)^{-2/d}. \tag{Lemma~\ref{lemma: approximation with lipschitz regularity}}
\end{align*}
The inequality $(*)$ follows from the fact that $f_i \in \mathcal{NN}_{d, 1}(\floor{\mathtt{W}_1/ d^*}, \mathtt{D}_1, \mathtt{B}_1)$ for $i\in [d^*]$, with independent parameters, then their concatenation $f = (f_1, f_2, \cdots, f_{d^*})^\top$ is an element of $\mathcal{NN}_{d, d^*}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)$ with specific parameters. This is due to $\sup_{\bm{x} \in \R^{d}}\norm{f(\bm{x})}_\infty = \sup_{\bm{x} \in \R^{d}}\max_{i \in d^*}\abs{f_i(\bm{x})} \leq \mathtt{B}_1$. We ignore the logarithmic term when deriving the last inequality, as its impact on polynomial term is negligible.

% \begin{definition}[Vector-valued Sobolev space]
%     Let $n, k \in \N_0$ with $k \leq n$, and $m \in \N$. Then the vector-valued Sobolev space $W^{n, \infty}(U; \R^m)$ is defined by
%     \begin{align*}
%         W^{n, \infty}(U; \R^m) := \{(f_1, f_2, \cdots, f_m): f_i \in W^{n, \infty}(U), 1 \leq i \leq m\}.
%     \end{align*}
%     Moreover, the Sobolev norm $\norm{\cdot}_{W^{n, \infty}(U; \R^m)}$ is defined by
%     \begin{align*}
%         \norm{f}_{W^n, \infty}(U; \R^m) := \max_{1 \leq i \leq m}\norm{f_i}_{W^{n,\infty}(U)}.
%     \end{align*}
% \end{definition}

% \begin{definition}
%     \label{def: partition of (0, 1)^d}
%     Given $\mathtt{K}, d \in \N$, and for any $\bm{m} = [m_1, m_2, \cdots, m_d]^{\top} \in \{1, 2\}^d$, we define $U_{\bm{m}} := \Cross_{j=1}^{d}U_{m_j}$ where $U_1 := \bigcup_{i=1}^{\mathtt{K}-1}[\frac{i}{\mathtt{K}}, \frac{i}{\mathtt{K}} + \frac{3}{4\mathtt{K}}]$ and $U_2 := \bigcup_{i=0}^\mathtt{K}[\frac{i}{\mathtt{K}} - \frac{1}{2\mathtt{K}}, \frac{i}{\mathtt{K}} + \frac{1}{4\mathtt{K}}] \cap [0, 1]$.
% \end{definition}
% It is easy to see the definition \ref{def: partition of (0, 1)^d} is a partition of $(0, 1)^d$ for any $\mathtt{K}, d \in \N$. Furthermore, we can construct a partition of unity $\{g_{\bm{m}}\}_{\bm{m} \in \{1, 2\}^d}$ on $(0, 1)^d$ with $\supp(g_{\bm{m}}) \cap (0, 1)^d \subset U_{\bm{m}}$ as definition \ref{def: partition of unity}.
% \begin{definition}
%     \label{def: partition of unity}
%     Given $\mathtt{K}, d \in \mathbb{N}$, for any integer $i \in \Z$, we define
%     \begin{align*}
%         g_1(x) :=
%         \begin{cases}
%             1, & x \in \left[ \frac{i}{\mathtt{K}} + \frac{1}{4\mathtt{K}}, \frac{i}{\mathtt{K}} + \frac{1}{2\mathtt{K}} \right], \\
%             0, & x \in \left[ \frac{i}{\mathtt{K}} + \frac{3}{4\mathtt{K}}, \frac{i}{\mathtt{K}} + \frac{1}{\mathtt{K}} \right], \\
%             4\mathtt{K} \left( x - \frac{i}{\mathtt{K}} \right), & x \in \left[ \frac{i}{\mathtt{K}}, \frac{i}{\mathtt{K}} + \frac{1}{4\mathtt{K}} \right], \\
%             -4\mathtt{K} \left( x - \frac{i}{\mathtt{K}} - \frac{3}{4\mathtt{K}} \right), & x \in \left[ \frac{i}{\mathtt{K}} + \frac{1}{2\mathtt{K}}, \frac{i}{\mathtt{K}} + \frac{3}{4\mathtt{K}} \right],
%         \end{cases}
%         \qquad g_2(x) := g_1\left( x + \frac{1}{2\mathtt{K}} \right).
%     \end{align*}
%     For any $\bm{m} = [m_1, m_2, \cdots, m_d]^{\top} \in \{1, 2\}^d$, we further define $g_{\bm{m}}(\bm{x}) := \prod_{j=1}^d g_{m_j}(x_j)$ where $\bm{x} = [x_1, x_2, \cdots, x_d]^{\top}$.
% \end{definition}

% Following lemma reveals that $g_{\bm{m}}$ can be approximated by deep network with arbitrary precision as your desired.
% \begin{lemma}
%     Given any $\mathtt{N}, \mathtt{L} \in \N$ and any $\bm{m} \in \{1, 2\}^d$, for $\mathtt{K} = \floor{\mathtt{N}^{1/d}}^2\floor{\mathtt{L}^{2/d}}$, there exists a function $\phi_{\bm{m}}$ implemented by a deep ReLU with width $\mathcal{O}(d\mathtt{N})$ and depth $\mathcal{O}(d^2\mathtt{L})$ such that
%     \begin{align*}
%         \norm{\phi_{\bm{m}} - g_{\bm{m}}}_{W^{1, \infty}}((0, 1)^d) \leq 50d^{5/2}(\mathtt{N} + 1)^{-4dL}.
%     \end{align*}
% \end{lemma}

% \begin{lemma}
%     Let $K \in \N$. For any $f \in W^{1, \infty}((0,1)^d)$ with $\norm{f}_{W^{1, \infty}((0,1)^d)} \leq 1$ and $\bm{m} \in \{1, 2\}^d$, there exists a piecewise constant function $f_{\mathtt{K}, \bm{m}}$ on $U_{\bm{m}}$ satisfying
%     \begin{align*}
%         \norm{f_{\mathtt{K}, \bm{m}}  - f}_{W^{1, \infty}(U_{\bm{m}})} \lesssim 1, \qquad \norm{f_{K,\bm{m}} - f}_{L^\infty(U_{\bm{m}})} \lesssim 1 / \mathtt{K}.
%     \end{align*}
% \end{lemma}

\subsubsection{The Critic Approximation Error \texorpdfstring{$\mathcal{E}_{\mathcal{G}}$}{EG}}
The main goal of this section is to bound $\mathcal{E}_{\mathcal{G}}$. The key idea is based on the approach presented in \citep{liu2021non}.
\begin{definition}[IPM, \citet{Muller_1997}]
    For any probability distribution $\mu$ and $\nu$ and symmetric function class $\mathcal{H}$, define
    \begin{align*}
        d_{\mathcal{H}}(\mu, \nu) = \sup_{h \in \mathcal{H}}\E_{X_1\sim\mu}\big\{h(X_1)\big\} - \E_{X_2 \sim \nu}\big\{h(X_2)\big\}
    \end{align*}
    \begin{remark}
         We focus on the scenario that $\mathcal{H} = \mathrm{Lip}(1)$, implying $d_\mathcal{H}(\mu, \nu) = \mathcal{W}(\mu,\nu)$.
    \end{remark}
\end{definition}

\begin{definition}[Approximation error of $\mathcal{H}_1$ to $\mathcal{H}_2$]
    Define the approximation error of a function class $\mathcal{H}_1$ to another function class $\mathcal{H}_2$
    \begin{align*}
        \mathcal{E}(\mathcal{H}_2, \mathcal{H}_1) = \sup_{h_2 \in \mathcal{H}_2}\inf_{h_1 \in \mathcal{H}_1}\norm{h_2 - h_1}_\infty
    \end{align*}
\end{definition}

\begin{lemma}\label{lemma: diffenerce between IPMs < 2 approx error}
For any probability distributions $\mu$ and $\nu$ and symmetric function classes $\mathcal{H}_1$ and $\mathcal{H}_2$, the difference in IPMs with two distinct evaluation classes will not exceed $2$ times the approximation error between the two evaluation classes, that is $d_{\mathcal{H}_2}(\mu, \nu) - d_{\mathcal{H}_1}(\mu, \nu)\leq 2\mathcal{E}(\mathcal{H}_2, \mathcal{H}_1)$.
\end{lemma}
\begin{proof}
     \begin{align*}
        &d_{\mathcal{H}_2}(\mu, \nu) - d_{\mathcal{H}_1}(\mu, \nu) \\
        &=   \sup_{h_2 \in \mathcal{H}_2}\big[\E_{X_1 \sim \mu} \{h_2(X_1)\} - \E_{X_2\sim\nu} \{h_2(X_2)\}\big] - \sup_{h_1 \in \mathcal{H}_1}\big[\E_{X_1 \sim \mu} \{h_1(X_1)\} - \E_{X_2\sim\nu} \{h_1(X_2)\}\big]\\
        &=\sup_{h_2 \in \mathcal{H}_2}\inf_{h_1 \in \mathcal{H}_1}\big[\E_{X_1 \sim \mu} \{h_2(X_1) - h_1(X_1)\} + \E_{X_2\sim\nu}\{h_1(X_2) - h_2(X_2)\}\big] \leq 2\mathcal{E}(\mathcal{H}_2, \mathcal{H}_1)
    \end{align*}
\end{proof}
Applying Lemma~\ref{lemma: diffenerce between IPMs < 2 approx error} to $\mathcal{E}_{\mathcal{G}}$ transforms the problem of bounding $\mathcal{E}_{\mathcal{G}}$  into estimating the approximation error between $\mathcal{G}$ and $\widehat{\mathcal{G}}$, as shown in Corollary~\ref{corollary}. This allows for the direct application of Lemma~\ref{lemma: approximation with lipschitz regularity}.
\begin{corollary}\label{corollary}
    The discriminator approximation error, $\mathcal{E}_{\mathcal{G}}$, will not exceed $2$ times the approximation error between the two evaluation classes, that is $\mathcal{E}_{\mathcal{G}} \leq 2\mathcal{E}(\mathcal{G}, \widehat{\mathcal{G}})$.
\end{corollary}

Recall we have assumed $\mathtt{D}_2\mathtt{W}_2\lesssim \mathtt{D}_1\mathtt{W}_1$. Combining Corollary~\ref{corollary} and Lemma~\ref{lemma: approximation with lipschitz regularity} yields
\begin{align*}
    2\mathcal{E}_{\mathcal{G}} \lesssim (\mathtt{D}_2\mathtt{W}_2)^{-2/d} \lesssim (\mathtt{D}_1\mathtt{W}_1)^{-2/d}.
\end{align*}

\subsection{Trade-off between Statistic Error and Approximation Error}\label{subsection: Trade off between statistic error and approximation error}
By setting $\mathtt{D}_1\mathtt{W}_1 = n_S^{\frac{d}{2d+4}}, \mathtt{D}_2\mathtt{W}_2 \lesssim \mathtt{D}_1\mathtt{W}_1$ yields
\begin{align*}
    \E_{\widetilde{\mathcal{D}}_S}[\mathcal{L}(\hat{f}_{n_S})] &\lesssim  \mathcal{L}(f^*) + \frac{\mathtt{D}_1\mathtt{W}_1}{\sqrt{n_S}} + (\mathtt{D}_1\mathtt{W}_1)^{-2/d} \lesssim \mathcal{L}(f^*) + n_S^{-\frac{1}{d+2}}.
\end{align*}

\subsection{Vanish \texorpdfstring{$\mathcal{L}(f^*)$}{Vanish L(f*)}}\label{subsection: vanish L(f*)}
In this section, we focus on constructing an encoder $\tilde{f} \in \mathcal{F}$ making $\mathcal{L}(\tilde{f})$ vanish. This follows that $\mathcal{L}(f^*) = 0$ by the definition of $f^*$, further providing an end-to-end theoretical guarantee for DM. To this end, we introduce following well-known lemma, the Kirszbraun theorem. as stated in page 21 of \citet{schwartz1969nonlinear}.  
\begin{lemma}[Kirszbraun theorem]\label{lemma: kirszbraun theorem}
    if $U$ is a subset of some Hilbert space $\mathcal{H}_1$, and $\mathcal{H}_2$ is another Hilbert space, and $f: U \to \mathcal{H}_2$ is a Lipschitz-continuous map, then there is a Lipschitz-continuous map $F: \mathcal{H}_1 \to \mathcal{H}_2$ that extends $f$ and has the same Lipschitz constants as $f$.
\end{lemma}


We first construct a function $\tilde{f}_1$ such that $\mathcal{L}_{\mathrm{align}}(\tilde{f}_1) = 0$, and subsequently identify an injection $\tilde{f}_2$. The composition $\tilde{f} := \tilde{f}_2 \circ \tilde{f}_1$ is shown to satisfy $\mathcal{W}(\P_{\tilde{f}}, \P_\mathcal{R}) = 0$, while maintaining $\mathcal{L}_{\mathrm{align}}(\tilde{f}) = 0$.


By the definition of $\mathcal{L}_{\mathrm{align}}(f)$, 
$\tilde{f}_1$ satisfies $\mathcal{L}_{\mathrm{align}}(\tilde{f}_1) = 0$ if and only if, for all $\bm{x} \sim \P_S$, any $\mathtt{x}_1, \mathtt{x}_2 \in \mathcal{A}(\bm{x})$, we have $\tilde{f}_1(\mathtt{x}_1) = \tilde{f}_1(\mathtt{x}_2)$. This implies that $\tilde{f}_1$ must encode all augmented views of the same $\bm{x} \sim \P_S$ as the same representation. To achieve this, we modify $f$ from Assumption~\ref{assumption: lipschitz transportation}. Specifically, for any $\mathtt{x} \in \mathcal{A}(\mathcal{X}_S)$, where $\mathtt{x} = A(\bm{x})$ for some $\bm{x} \in \mathcal{X}_S$ and $A \in \mathcal{A}$, we define $\tilde{f}_1(\mathtt{x}) = f(\bm{x})$. 

It follows that $\tilde{f}_1$ is a Lipschitz map on $\mathcal{A}(\mathcal{X}_S)$, as both $f$ and $A \in \mathcal{A}$ are Lipschitz continuous. Specifically, for any $\mathtt{x}_1 = A_1(\bm{x}_1)$ and $\mathtt{x}_2 = A_2(\bm{x}_2)$, we have:
\begin{align*}
    \norm{\tilde{f}_1(\mathtt{x}_1) - \tilde{f}_1(\mathtt{x}_2)}_2 &=
    \norm{\tilde{f}_1(A_1(\bm{x}_1)) - \tilde{f}_1(A_2(\bm{x}_2))}_2 \lesssim \norm{A_1(\bm{x}_1) - A_2(\bm{x}_2)}_2\\
    &\leq \norm{A_1(\bm{x}_1) - A_1(\bm{x}_2)}_2 + \norm{A_2(\bm{x}_1) - A_2(\bm{x}_2)}_2 \leq 2M\norm{\bm{x}_1 - \bm{x}_2}_2.
\end{align*}
We next extend $\tilde{f}_1$ to $[0,1]^d$ using Kirszbraun theorem (Lemma~\ref{lemma: kirszbraun theorem}). It is easy to verify that $\norm{\tilde{f}_1(\mathtt{x}_1) - \tilde{f}_1(\mathtt{x}_2)}_2 = 0$ when $\mathtt{x}_1$ and $\mathtt{x}_2$ are augmented views of the same $\bm{x} \in \mathcal{X}_S$. Moreover, since the distribution on $\mathcal{A}$ is uniform, it is evident that $f_\sharp\P_S = (\tilde{f}_1)_\sharp\P_S$. Therefore, according to Assumption~\ref{assumption: lipschitz transportation}, the optimal transport map $T$ between $(\tilde{f}_1)_\sharp\P_S$ and $\P_\mathcal{R}$ is a Lipschitz bijection, so we set $\tilde{f}_2 = T$ to obtain the desired $\tilde{f}$.

In fact, $\tilde{f}_2$ being the optimal transport map ensures that $\tilde{f}_\sharp\P_S = (\tilde{f}_2\circ \tilde{f_1})_\sharp\P_S = (\tilde{f}_2)_\sharp (\tilde{f}_1)_\sharp\P_S= \P_\mathcal{R}$, implying $\mathcal{W}(\P_{\tilde{f}}, \P_\mathcal{R}) = 0$. Furthermore, since both $\tilde{f}_1$ and $\tilde{f}_2$ is Lipschitz continuous, $\tilde{f}$ is Lipschitz continuous, ensuring that $\tilde{f} \in \mathcal{F}$ with an appropriate Lipschitz constant $L$ in~\eqref{eq: definition of F}. Finally, the bijectivity of $\tilde{f}_2$ guarantees that $\mathcal{L}_{\mathrm{align}}(\tilde{f})= 0$. Therefore, we have constructed an encoder $\tilde{f} \in \mathcal{F}$ such that $\mathcal{L}(\tilde{f}) = 0$, further concluding $\mathcal{L}(f^*)=0$ under Assumption~\ref{assumption: lipschitz transportation}.

\subsection{Proof of Theorem~\ref{theorem: sample theorem}}
\begin{mythm}{\ref{theorem: sample theorem}}
    Suppose Assumptions \ref{assumption: Q* does not vanish}-\ref{assumption: lipschitz transportation} hold. Set the widths and depths of the encoder and critic networks satisfying $\mathtt{D}_2\mathtt{W}_2\lesssim\mathtt{D}_1\mathtt{W}_1 = \mathcal{O}\big(n_S^{-\frac{d}{2d+4}}\big)$, and set the augmentation as $\mathcal{A}_{n_S}$, then with probability at least $1 - \mathcal{O}\big(n_S^{-\min\{\frac{1}{d+2}, \alpha\}}\big) - \mathcal{O}\big(\frac{1}{\min_{k}\sqrt{n_T(k)}}\big)$,
    \begin{align*}
        \E_{\widetilde{\mathcal{D}}_S}\big\{\mathrm{Err}(G_{\hat{f}_{n_S}})\big\} \leq \big(1 - \sigma_{n_S}\big) + \mathcal{O}\Big(n_S^{-\min\{\frac{1}{2d+4}, \frac{\alpha}{4}, \frac{\beta}{4}\}}\Big)
    \end{align*}
holds for sufficiently large $n_S$.
\end{mythm}
\begin{proof}
    We have established that $R_1 \leq \norm{\hat{f}_{n_S}}_2 \leq R_2$ with $R_1 \approx R_2$ in Section~\ref{subsection: encoder approximation error}, in Section~\ref{subsection: encoder approximation error}, allowing us to apply Theorem~\ref{theorem: general version of the pop theorem} to $\hat{f}_{n_S}$. Taking the expectation with respect to $\widetilde{\mathcal{D}}_S$ on both sides yields:
    \begin{align}\label{eq: E[divergence] < E[L]}
    \E_{\widetilde{\mathcal{D}}_S}\big\{\max_{i \neq j}\mu_T(i)^{\top}\mu_T(j)\big\} \lesssim \E_{\widetilde{\mathcal{D}}_S}\big\{\mathcal{L}(\hat{f}_{n_S})\big\} + \epsilon_1
    \end{align}
    \begin{align}\label{eq: E[Err(G)] < (1 - sigma) + E[L]}
    \E_{\widetilde{\mathcal{D}}_S}\big\{\mathrm{Err}(G_{\hat{f}_{n_S}})\big\} \lesssim \big(1 - \sigma_{n_S}\big) + \eps^{-1}\Big(\E_{\widetilde{\mathcal{D}}_S}\big\{\mathcal{L}(\hat{f}_{n_S})\big\} + \epsilon_1 + \epsilon_2\Big)^{\frac{1}{2}},
    \end{align}
    The right-hand side of~\eqref{eq: E[Err(G)] < (1 - sigma) + E[L]} follows from Jensen's inequality. Furthermore, substituting $\E_{\widetilde{\mathcal{D}}_S}\{\mathcal{L}(\hat{f}_{n_S})\}\lesssim n_S^{-\frac{1}{d+2}}$ into~\eqref{eq: E[divergence] < E[L]} and~\eqref{eq: E[Err(G)] < (1 - sigma) + E[L]}, we obtain:
    \begin{align}\label{eq: E[divergence] < n_S}
    \max_{i \neq j}\E_{\widetilde{\mathcal{D}}_S}\big\{\mu_T(i)^{\top}\mu_T(j)\big\} \lesssim n_S^{-\frac{1}{d+2}} + \epsilon_1
    \end{align}
    and
    \begin{align}\label{eq: E[Err(G)] < (1 - sigma) + n_S}
    \E_{\widetilde{\mathcal{D}}_S}\big\{\mathrm{Err}(G_{\hat{f}_{n_S}})\big\}\leq (1 - \sigma_{n_S}) + \mathcal{O}\Big(\eps^{-1}\big(n_S^{-\frac{1}{d+2}} + \epsilon_1 + \epsilon_2\big)^{\frac{1}{2}}\Big).
    \end{align}
    To ensure~\eqref{eq: E[Err(G)] < (1 - sigma) + n_S} holds, recall the corresponding requirement stated in Theorem~\ref{theorem: general version of the pop theorem}:
    \begin{align*}
        \mu_T(i)^{\top}\mu_T(j) < R_2^2\psi(\sigma_{n_S}, \delta_{n_S}, \eps, \hat{f}_{n_S}),
    \end{align*}
    where $\psi(\sigma_{n_S}, \delta_{n_S}, \eps,\hat{f}_{n_S}) = \Gamma_{\min}(\sigma_{n_S}, \delta_{n_S}, \eps, \hat{f}_{n_S}) - \sqrt{2 - 2\Gamma_{\min}(\sigma, \delta, \eps, f)} - \frac{1}{2}\Big(1 - \frac{\min_k\norm{\hat{\mu}_T(k)}_2^2}{R_2}\Big)
  - \frac{2\max_k\norm{\hat{\mu}_T(k) - \mu_T(k)}_2}{R_2}$, in here, $\Gamma_{\min}(\sigma_{n_S}, \delta_{n_S}, \eps, \hat{f}_{n_S}) = 
    \big(\sigma_{n_S} - \frac{U_T(\eps, \hat{f}_{n_S})}{\min_ip_t(i)}\big)\big(1 + \big(\frac{R_2}{R_1}\big)^2 - \frac{L\delta_{n_S}}{R_2} - \frac{2\eps}{R_2}\big) - 1$.
    
    Therefore, if $\psi(\sigma_{n_S}, \delta_{n_S}, \eps, \hat{f}_{n_S}) > 0$, by Markov inequality, we can ensure~\eqref{eq: E[Err(G)] < (1 - sigma) + n_S} holds with probability at least $1 - \mathcal{O}\Big(\frac{n_S^{-1/(d+2)} + \epsilon_1}{\psi(\sigma_{n_S}, \delta_{n_S}, \eps, \hat{f}_{n_S})}\Big)$.
    
    For the scenario where the distribution shift satisfies $\epsilon_1 \lesssim n_S^{-\alpha}, \epsilon_2 \lesssim n_S^{-\beta}$ for sufficiently large $n_S$, as stated in Assumption~\ref{assumption: distributions shift < n}, and data augmentation in Assumption~\ref{assumption: existence of augmentation} (i.e., $\sigma_{n_S} \to 1$ and $\delta_{n_S} \to 0$), setting $\eps = \eps_{n_S} = n_S^{-\min\{\frac{1}{4(d + 2)},\frac{\alpha}{4}, \frac{\beta}{4}\}}$ yields $\E_{\widetilde{\mathcal{D}}_S}\big\{U_T^2(\eps_{n_S}, \hat{f}_{n_S})\big\} \lesssim n_S^{-\min\{\frac{1}{2d+4}, \frac{\alpha}{4}, \frac{\beta}{4}\}}$ by~\eqref{eq: U_T < L}. This implies $\Gamma_{\min}(\sigma_{n_S}, \delta_{n_S}, \eps_{n_S}, \hat{f}_{n_S}) \approx 1$ for sufficiently large $n_S$.  Furthermore, since $\frac{1}{2}\big(1 - \min_k\norm{\hat{\mu}_T(k)}_2^2/R_2\big) \leq \frac{1}{2}$, we conclude $\psi(\sigma_{n_S}, \delta_{n_S}, \eps_{n_S}, \hat{f}_{n_S}) \geq \frac{1}{2} - \frac{2\max_{k}\norm{\hat{\mu}_T(k) - \mu_T(k)}_2}{R_2}$. According to Multidimensional Chebyshev's inequality,
    \begin{align*}
        \P_T\Big(\norm{\hat{\mu}_T(k) - \mu_T(k)}_2 \geq \frac{R_2}{8}\Big) &\leq \frac{64\sqrt{\E_{X_T\sim\P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\big\{\norm{f(\mathtt{X}_T) - \mu_T(k)}_2^2\vert X_T\in \widetilde{C}_T(k)\big\}}}{R_2^2\sqrt{2n_T(k)}} \\
        &\leq \frac{128}{R_2\sqrt{n_T(k)}},
    \end{align*}
    we have $\psi(\sigma_{n_S}, \delta_{n_S}, \eps_{n_S}, \hat{f}_{n_S}) \geq 1/4$ with probability at least $1 - \mathcal{O}\big(\frac{1}{\min_{k}\sqrt{n_T(k)}}\big)$ when $n_S$ is large enough.

    By combining above conclusions, we know with probability at least $1 - \mathcal{O}\big(n_S^{-\min\{\frac{1}{d+2}, \alpha\}}\big) - \mathcal{O}\big(\frac{1}{\min_{k}\sqrt{n_T(k)}}\big)$,
    \begin{align*}
        \E_{\widetilde{\mathcal{D}}_S}\big\{\mathrm{Err}(G_{\hat{f}_{n_S}})\big\} \leq \big(1 - \sigma_{n_S}\big) + \mathcal{O}\Big(n_S^{-\min\{\frac{1}{2d+4}, \frac{\alpha}{4}, \frac{\beta}{4}\}}\Big)
    \end{align*}
    when $n_S$ is sufficiently large.
\end{proof} 