\section{Experiment}
The PyTorch implementation of DM can be found in \href{https://github.com/vincen-github/DM}{https://github.com/vincen-github/DM}.
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        Method & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{STL-10} \\
        & Linear & $k$-nn & Linear & $k$-nn & Linear & $k$-nn \\
        \midrule
        Barlow Twins \citep{zbontar2021barlow} & 83.96 & 81.18 & 56.75 & 47.91 & 81.41 & 76.41 \\
        SimCLR \citep{chen2020simclr} & 90.23 & 87.57 & 64.16 & \textbf{53.65} & 87.44 & 82.68 \\
        Haochen22 \citep{haochen2022beyond} & 86.95 & 82.83 & 53.75 & 48.40 & 81.44 & 77.31 \\
        Vicreg \citep{adrien2022vicreg} & 87.16 &  85.10 & 56.63 & 49.59 & 84.63 & 81.13 \\
        \midrule
        DM & \textbf{91.10} & \textbf{88.17} & \textbf{64.49} & 53.11 & \textbf{88.65} & \textbf{84.21} \\
        \bottomrule
    \end{tabular}
    \caption{Classification accuracy (top 1) of a linear classifier and a $k$-nearest neighbors classifier ($k = 5$) for different loss functions and datasets.}
    \label{tab:experiment_result}
\end{table}
\subsection{Experiment details}
\paragraph{Datasets}
Following prior self-supervised learning works \citep{chen2020simclr, ermolov2021whitening, zbontar2021barlow, haochen2021spectral, adrien2022vicreg}, we evaluate our method on three widely used image datasets: CIFAR-10 \citep{Krizhevsky09cifar}, CIFAR-100 \citep{Krizhevsky09cifar}, and STL-10 \citep{coates2011stl10}. Each dataset is split into three parts: an unsupervised set for training the encoder and critic via DM, a supervised set for training the linear classifier, and a testing set to assess the error.

\paragraph{Experimental Pipeline}
During training, we randomly crop and resize images to 32$\times$32 (CIFAR-10, CIFAR-100) or 64$\times$64 (STL-10) before feeding them into the encoder. In the pretraining phase, we use the Adam optimizer to update both the encoder and the projection head based on the unlabeled dataset. After pretraining, the encoder is frozen, and the projection head is removed. We then train a linear classifier on top of the frozen encoder using another Adam optimizer, with the classifier represented as a linear transformation from $\R^{d^*}$ to $\R^K$.  followed by a softmax layer. The classification loss is cross-entropy. We evaluate the classifier's accuracy on the testing dataset and also report the performance of a $k$-nearest neighbors classifier ($k$=5) without fine-tuning.

\paragraph{Network Architecture}
The encoder backbone is ResNet-18 \citep{he2016resent}, while the critic network consists of three smaller layers, each followed by layer normalization \citep{ba2016layernormalization} and a LeakyReLU activation with a slope of 0.2. The critic's dimensionality transformation follows $d^* \to 128 \to d^* \to 1$. Notably, an overly complex critic may impair the learned representation's performance. Following \citet{chen2020simclr}, we train a projection head alongside the encoder during the self-supervised task. The projection head is a two-layer ReLU network with a hidden size of 1000.

\paragraph{Estimating Mallows' Distance with Gradient Penalty}
Mallow's distance in Equation \eqref{eq: risk at pop level (specific version)} involves a minimization problem with the constraint $g \in \mathrm{Lip}(1)$, which is difficult to optimize directly due to the challenging nature of searching the $\mathrm{Lip}(1)$ set. To address this, \citet{gulrajani2017wgangp} reformulate Mallow's distance in Equation \eqref{eq: mini-max} as an unconstrained optimization problem,
\begin{align}\label{eq: gp}
    \mathcal{W}_{\mathrm{gp}}(f, g) = \E_{Z \sim \P_f} \big\{g(Z)\big\} - \E_{\mathcal{R} \sim \P_\mathcal{R}} \big\{g(\mathcal{R})\big\} + \eta \cdot \E_{\bar{X} \sim \P_{\bar{X}}} \Big[\big\{\big\Vert\nabla_{\bar{X}}g(\bar{X}) \big\Vert - 1\big\}^2\Big],
\end{align}
where $\eta > 0$ is a tuning parameter referred to as the ``plenty weight", typically set to 1 during DM training. Let $U[0,1]$ be the uniform distribution on the interval $[0,1]$ The random variable $\bar{X}\sim \P_{\bar{X}}$ is defined by $\bar{X} = uZ + (1 - u)\mathcal{R}$, where $Z \sim \P_f$, $\mathcal{R} \sim \P_\mathcal{R}$, and $u \sim U[0,1]$. In practice, the encoder is updated at each step, while the critic is updated every five steps.

\paragraph{Hyperparameters}
We set $K^\prime = 384$, $R=1$ and $\epsilon = 10^{-3}$ across all datasets. The encoder's output dimension $d^*$ is set to be 384.  The learning rates for the encoder and critic are $3 \times 10^{-5}$ and $10^{-3}$, respectively, with weight decay of $5 \times 10^{-7}$ and $10^{-4}$. A learning rate warm-up is applied for the first 500 iterations of the encoder optimizer. The weight parameter $\lambda$ is set as 1 for all datasets. is set to 1 for all datasets. The batch sizes are 512 for CIFAR-10 and CIFAR-100, and 384 for STL-10, with training for 1000 epochs on the unsupervised dataset. During testing, a linear classifier is trained for 500 epochs using the Adam optimizer with an exponentially decaying learning rate from $10^{-2}$ to $10^{-6}$, and a weight decay of $5 \times 10^{-6}$.

\paragraph{Data Augmentations}
We randomly extract crops ranging in size from 0.2 to 1.0 of the original area, with aspect ratios varying from 3/4 to 4/3 of the original aspect ratio. Horizontal mirroring is applied with a probability of 0.5. Additionally, color jittering is configured with parameters 0.4, 0.4, 0.4, 0.1 and a probability of 0.8, while grayscaling is applied with a probability of 0.2. During testing, only randomly crop and resize are utilized for evaluation.

\paragraph{Platform}
All experiments were conducted using a single Tesla V100 GPU unit. The torch version is 2.2.1+cu118 and the CUDA version is 11.8.

\subsection{Ablation Experiment: Finer-Grained Concept} \label{subsection: finer-grained concept}
As shown in Figure~\ref{fig: reference}, some samples (e.g., orange and gray points) share similar semantic meaning (both represent ``dog") but are distant in the representation space due to the existence of finer-grained classes (e.g., ``black dog" and ``orange dog"). Whether self-supervised representation learning methods can effectively capture such subclass structures remains an open question, particularly in real-world applications \citep{haochen2021spectral, haochen2022beyond, haochen2023theoretical}.

A key distinction between our theoretical framework and experiments lies in the optimization of $\widehat{W}$. In theory, $\widehat{W}$ can be directly calculated, whereas in practice, it is updated via gradient descent. This approach relaxes the constraint $K^\prime = K$ discussed in Section~\ref{section: theory} and provides greater flexibility. Additionally, $K^\prime$ offers significant interpretability, reflecting the number of concepts within the data. Intuitively, as the number of learned concepts were to increase within a certain range, more fine-grained concepts would be captured, and the transferability of the representations would improve. We validate this through the following ablation experiments.


\begin{table}[ht]
    \centering
    \begin{tabular}{cccccc}
    \toprule
    Concept number ($K^\prime$) & 64 & 128 & 256 & 384 & 512 \\ 
    \midrule
    Linear & 49.83 & 55.93 & 61.13 & \textbf{64.49} & 63.87 \\
    $k$-nn & 32.73 & 44.17 & 51.00 & \textbf{53.11} & 52.45 \\
    \bottomrule
    \end{tabular}
    \caption{The influence of concept number on representation performance. All experiments are conducted on CIFAR-100. The parameter $k$ is set to be $5$ and the representation dimension $d^*$ is set as $K^\prime$.}
    \label{tab:ablation_study}
\end{table}
