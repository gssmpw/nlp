\section{Methodology}\label{section: methodology}
Let $\bm{x} = (x_1, 
\cdots, x_d)^{\top} \in \R^d$ be an arbitrary $d$-dimensional vector, we define $\norm{\bm{x}}_p = (\sum_i\abs{x_i}^p)^{\frac{1}{p}}$ be its $p$-norm with $p \in \{1, 2, \infty\}$.  In particular, for $p = \infty$, $\norm{\bm{x}}_\infty = \max_i\abs{x_i}$. Let $f$ be a function from $\R^{d_1}$ to $\R^{d_2}$, and let $\mathrm{dom}(f)$ represent the domain of $f$. For a constant $c \geq 0$, we say that $f$ satisfies $\norm{f}_2 = c$ if $\norm{f(\bm{x})}_2 = c$ holds for any $\bm{x} \in \mathrm{dom}(f)$. Additionally, we define the functional set as:
\begin{align}\label{eq: L-Lipschitz function}
    \mathrm{Lip}(L) = \Big\{f:\R^{d_1} \to \R^{d_2} \Big\vert \sup_{\substack{\bm{x}_1, \bm{x}_2\in \mathrm{dom}(f)\\\bm{x}_1 \neq \bm{x}_2}}\frac{\norm{f(\bm{x}_1)-f(\bm{x}_2)}_2}{\norm{\bm{x}_1 - \bm{x}_2}_2} \leq L\Big\}.
\end{align}
Let $f$ and $g$ be two functions defined on $\N = \{1, 2, \cdots\}$. We say that $f(n) \leq \mathcal{O}(g(n))$ if and only if there exist two fixed constants $0 < c_1 \leq  c_2$ and a positive integer $n_0 \in \N$, such that for all $n \geq n_0, c_1 g(n)\leq f(n) \leq c_2 g(n)$. It immediately follows that $c_2^{-1}f(n) \leq g(n) \leq c_1^{-1}f(n)$ for any $n \geq n_0$. Therefore, the statement $f(n) = \mathcal{O}(g(n))$ implies that $g(n) = \mathcal{O}(f(n))$. Given two quantities $X$ and $Y$, we use $X \lesssim Y$ or $Y \gtrsim X$ to denote $X \leq cY$ for some constant $c > 0$.

Assume a source dataset containing a total of $n_S$ unlabeled image instances, denoted by $\mathcal{D}_S = \{X_S^{(i)}: 1 \leq i \leq n_S\}$, Here $X_S^{(i)} \in \mathcal{X}_S \subseteq [0,1]^d$ represents the $i$-th instance, which are independently and identically generated from a source distribution $\P_S$ on the source domain $\mathcal{X}_S$. To fix the idea, consider the ImageNet dataset as an example for $\mathcal{D}_S$. We then have a total of $n_S = 1.28 \times 10^{6}$ instances \citep{deng2009imagenet}. Since ImageNet instance are of $224\times 224\times 3$ resolution, we thus have $d = 150,528 = 224 \times 224 \times 3$. Next, assume a target dataset as $\mathcal{D}_T = \{(X_T^{(i)}, Y_i): 1 \leq i \leq n_T \}$ with $X_T^{(i)} \in [0,1]^d$ and $Y_i \in \{1,2,\cdots, K\}$ being the class label. Assume $(X_T^{(i)}, Y_i)$s are independently and identically generated from a target distribution $\P_T$. For most real applications, we typically have $n_S \gg n_T$. How to leverage $\mathcal{D}_S$ so that a model with excellent classification accuracy on $\mathcal{D}_T$ is a problem of great intent. 

\subsection{Data Representation}
Pixel images pose significant challenges for statistical learning for at least two reasons. First, their high dimensionality, as exemplified by ImageNet with 150,528 dimensions per image, complicates statistical modeling. Second, pixel images are inherently noisy. For example, consider Figure~\ref{fig:semantic_dist}, where the left panel ($\bm{x}_1$) shows a photo of a dog, the middle panel ($\bm{x}_2$) shows a different image, and the right panel ($\bm{x}_3$) shows a cropped version of $\bm{x}_1$. Intuitively, $\bm{x}_1$ and $\bm{x}_3$ should be more similar, yet Euclidean distance calculations reveal $\norm{\bm{x}_1 - \bm{x}_2}_2 < \norm{\bm{x}_1- \bm{x}_3}_2$. This counterintuitive result highlights that pixel vectors encode both useful semantic information and significant noise, making the transformation to a lower-dimensional, less noisy representation crucial.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{semantic_dist.png}
    \caption{The semantic meaning of $\bm{x}_1$ and $\bm{x}_2$ are almost same since $\bm{x}_3$ is a cropped version of $\bm{x}_1$. However, we have $\norm{\bm{x}_1 - \bm{x}_2}_2 < \norm{\bm{x}_1- \bm{x}_3}_2$.}
    \label{fig:semantic_dist}
\end{figure}

This leads to the concept of data representation \citep{rumelhart1986learning, Bengio2012unsupervised, lecun2015deep}. By ``data representation", we refer to mapping an original image $X\in \R^{d}$ to a lower-dimensional space $f(X) \in \R^{d^*}$, where $d^* \ll d$. Here $f$ is typically a nonlinear function from $\R^{d}$ to $\R^{d^*}$. We refer to $f$ as an encoder and the range of $f$ as the representation space. A crucial question is: what defines a useful encoder? Intuitively, an effective encoder should map semantically similar images to nearby points in the representation space, while images with distinct semantic content should be well-separated. This principle has inspired many supervised representation learning methods \citep{hoffer2015deep, chopra2005learning, zhai2018classification}, which rely on accurately annotated labels. Instances with the same label are treated as semantically similar, while those with different labels are considered distinct.

These methods excel in preserving similarity among instances with the same label, but they have notable limitations. First, annotation is costly, particularly for large datasets \citep{albelwi2022survey}. Second, they fail to fully capture the richness of semantic meanings. For instance, an image labeled as \textit{toilet paper} in the ImageNet dataset (Figure~\ref{fig: toilet paper imagenet}) could also be labeled as \textit{bike}, \textit{man}, \textit{road}, and others. By assigning a single label, we lose the opportunity to capture these additional semantic meanings, leading to significant information loss. Thus, developing efficient representation learning methods that minimize this loss is a key research challenge.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{toliet_paper_imagenet.png}
    \caption{Images labeled by \textit{toilet paper} in ImageNet}
    \label{fig: toilet paper imagenet}
\end{figure}

\subsection{Self-Supervised Contrastive Learning}\label{subsection: self-supervised contrastive learning}
In the absence of labeled data, the need for effective representations has driven the development of contrastive learning. The core idea is to learn representations invariant to augmentations. By augmentation, we refer to a predefined function that transforms an image $A(\cdot)$ into a similar, but not identical image $A(X) \in \R^{d}$. In practice, $X$ and $A(X)$ might be of different dimensions. For notation simplicity, we assume they share the same dimension in this work. Since $A(X)$ is derived from $X$, they are expected to share similar semantic meanings. Commonly used augmentation include random cropping, flipping, translation, rescaling, color distortion, grayscale, normalization, and their compositions (see \citet{chen2020simclr} and \citet{wang2024comprehensive} for details). We define the set of augmentations as $\mathcal{A} = \{A_{m}(\cdot): 1\leq m \leq M\}$, where $M$ represents the total number of augmentations. While $M$ could be infinite, we consider a sufficiently large finite $M$ for theoretical convenience. With a large enough $M$, any augmentation can be well-approximated by some $A \in \mathcal{A}$. For convenience in derivation, we assume the identity transformation is included in $\mathcal{A}$.

We now introduce the concept of augmentation invariance, which means that $\|f(\mathtt{X}_{1}) - f(\mathtt{X}_{2})\|_2^2$ should be minimized, where $\mathtt{X}_1$ and $\mathtt{X}_2$ are augmented from the same original image. Let $\mathcal{A}(X) = \{A(X): A  \in \mathcal{A}\}$ be the set of all augmented views of $X$, and let $\mathtt{X} \sim \mathcal{A}(X)$ indicates that $\mathtt{X}$ is sample uniformly from $\mathcal{A}(X)$ according to a uniform distribution. Following \citep{huang2023towards, duan2024unsupervisedtransferlearningadversarial}, we define the alignment loss function $\mathcal{L}_{\mathrm{align}}(f)$ as:
\begin{align}\label{eq: augmentaion invariant}
    \mathcal{L}_{\mathrm{align}}(f) = \E_{X_S \sim \P_S}\E_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)}\Big\{\Big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2})\Big\Vert_2^2\Big\}.
\end{align}
Furthermore, given $L > 0, R > 0$, we define a functional class as
\begin{align}
    \label{eq: definition of F}
    \mathcal{F} = \big\{f: [0,1]^d \to \R^{d^*} \big\vert f \in \mathrm{Lip}(L) \text{ and }\norm{f}_2 = R\big\}.
\end{align}
Theoretically, the optimal encoder is $f_{\mathrm{opt}} \in \argmin_{f \in \mathcal{F}}\mathcal{L}_{\mathrm{align}}(f)$. However, this  results in an trivial solution where $f_{\mathrm{opt}} \equiv \bm{p} \in \R^{d^*}$, a fixed point with $\norm{\bm{p}}_2 = R$. which is ineffective for the learning task. This issue is referred to as model collapse \citep{jing2021understanding, zbontar2021barlow}.

To prevent model collapse, several effective techniques have been developed. The fundamental idea behind \citet{ye2019invarspread, he2020momentum, chen2020simclr, haochen2021spectral} is to identify an encoder that pushes the augmented views of different images far apart while minimizing~\eqref{eq: augmentaion invariant}. Therein, the augmented views of different images are dubbed as negative samples. Nevertheless, as noted by \citet{chuang2020debiased, chuang2022robust}, brutally pushing far apart negative samples can hinder representation learning, as these samples may share similar or even identical semantic meaning. Consequently, efficient representation learning without negative samples has become a significant research focus. Methods like \citet{zbontar2021barlow, ermolov2021whitening, adrien2022vicreg, haochen2022beyond} propose regularization techniques on $f$ to ensure non-degenerate representation variability. A common approach ~\citep{haochen2022beyond, haochen2023theoretical, duan2024unsupervisedtransferlearningadversarial} constrains $\E_{X_S \sim \P_S}\E_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2}  \sim \mathcal{A}(X_S)}\{f(\mathtt{X}_{S,1})f(\mathtt{X}_{S,2})^\top\}$ to be close to the identity matrix, demonstrating effectiveness but lacking interpretability. As an alternative, we propose distribution matching (DM), which defines a reference distribution in the representation space and minimizes the Mallows' distance to align the learned distribution with this reference, offering a clear geometric interpretation.



\subsection{Distribution Matching}\label{subsection: DM}
Before introducing the DM method, we briefly review the Mallows' distance \citep{mallows1972note, shao2012jackknife}, also known as the Wasserstein distance \citep{Villani}. To do so, we first define some key concepts. Let $\nu$ be a measure on $\R^{d_1}$ and $f : \R^{d_1} \rightarrow \R^{d_2}$ a measurable function. The push-forward measure $f_{\sharp}\nu$ is defined as $f_{\sharp}\nu(E) = \nu(f^{-1}(E))$ for any $f_{\sharp}\nu$-measurable set $E \subseteq \R^{d_2}$. In this context, the Mallows' distance is defined as:
\begin{definition}[Mallows' distance]\label{def: Wasserstein-1 distance}
    Let $(\mathcal{X}_1, \nu_1)$ and $(\mathcal{X}_2, \nu_2)$ are two probability spaces with $\mathcal{X}_1, \mathcal{X}_2 \subseteq \R^k$ for some positive integer $k$. Then the Mallow's distance is defined as
    \begin{align}\label{eq: formulation of wasserstein distance}
        \mathcal{W}(\nu_1, \nu_2) = \inf_{(X_1, X_2) \in \Pi(\nu_1, \nu_2)}\E_{(X_1, X_2)}\Big(\norm{X_1 - X_2}_1\Big),
    \end{align}
    where $\Pi(\nu_1, \nu_2)$ denotes the collection of all possible joint distributions of the pairs $(X_1, X_2)$ with marginal distributions given by $\nu_1$ and $\nu_2$, respectively. Here we implicitly assume that there exists a probability space $(\Omega, P)$ such that $X_1 : \Omega \to \mathcal{X}_1$ and $X_2: \Omega \to \mathcal{X}_2$ are measurable and satisfy $(X_1)_\sharp P = \nu_1$ and $(X_2)_\sharp P = \nu_2$.
\end{definition}
To better understand the Definition~\ref{def: Wasserstein-1 distance},  we explore a special case in detail. Let $\mathcal{X}_1 = \{\bm{x}_{1,1}, \bm{x}_{1,2}, \cdots, \bm{x}_{1,n_1}\} \subseteq \R^{k}$ and $\mathcal{X}_2 = \{\bm{x}_{2,1},\bm{x}_{2,2},\cdots, \bm{x}_{2,n_2}\} \subseteq \R^{k}$, where $k, n_1$ and $n_2$ are positive integers. Suppose $\nu_1$ and $\nu_2$ are discrete probability distributions on $\mathcal{X}_1$ and $\mathcal{X}_2$, respectively. Then each element in $\Pi(\nu_1, \nu_2)$ can be completely determined by a discrete probability distribution on the cartesian product $\mathcal{X}_1\times \mathcal{X}_2$, represented by $\Psi(\bm{x}_1, \bm{x}_2) \in \R^{n_1 \times n_2}$. Accordingly, it should satisfy that (\romannumeral1) $\Psi(\bm{x}_1, \bm{x}_2) \geq 0$ for any $\bm{x}_1 \in \mathcal{X}_1$ and $\bm{x}_2 \in \mathcal{X}_2$, (\romannumeral2) $\sum_{\bm{x}_1,\bm{x}_2}\Psi(\bm{x}_1, \bm{x}_2) = 1$, (\romannumeral3) $\sum_{\bm{x}_2 \in \mathcal{X}_2}\Psi(\bm{x}_1,\bm{x}_2) = \nu_1(\bm{x}_1)$ for any $\bm{x}_1 \in \mathcal{X}_1$ and (\romannumeral4) $\sum_{\bm{x}_1 \in \mathcal{X}_1}\Psi(\bm{x}_1,\bm{x}_2) = \nu_2(\bm{x}_2)$ for any $ \bm{x}_2 \in \mathcal{X}_2$. The Mallows' distance between $\nu_1$ and $\nu_2$ is then given by: $\mathcal{W}(\nu_1, \nu_2) = \inf_{\Psi \in \Pi(\nu_1, \nu_2)}\sum_{\bm{x}_1 \in \mathcal{X}_1, \bm{x}_2 \in \mathcal{X}_2}\Psi(\bm{x}_1, \bm{x}_2)\cdot\norm{\bm{x}_1 - \bm{x}_2}_1$. Intuitively, $\nu_1$ and $\nu_2$ can be regarded as two piles of probability masses, with $\nu_1(\bm{x}_1)$ and $\nu_2(\bm{x}_2)$  indicating the mass at $\bm{x}_1$ and $\bm{x}_2$, respectively. The transport plan $\Psi(\bm{x}_1, \bm{x}_2)$ can be thought of as the amount of mass transported from $\bm{x}_1$ and $\bm{x}_2$, while the term $\norm{\bm{x}_1 - \bm{x}_2}_1$ represents the transportation cost. Thus, the Mallows' distance quantifies the minimal cost to transport one probability distribution to another.

Although Definition~\ref{def: Wasserstein-1 distance} is intuitive, computing it is challenging due to the difficulty of finding the optimal coupling in $\Pi(\nu_1, \nu_2)$. To address this, a dual formulation is provided in Remark 6.5 of \citet{Villani}:
    \begin{align}\label{eq: dual form of wasserstein distance}
        \mathcal{W}(\nu_1, \nu_2) = \sup_{g \in \mathrm{Lip}(1)}\E_{X_1 \sim \nu_1}\Big\{g(X_1)\Big\} - \E_{X_2 \sim \nu_2}\Big\{g(X_2)\Big\},
    \end{align}
where the task reduces to finding the optimal function $g$ in $\mathrm{Lip}(1)$, , a problem that can be solved using a neural network with gradient penalty \citep{gulrajani2017wgangp}, as detailed in~\eqref{eq: gp}. Notably, the Mallows' distance remains effective even when $\nu_1$ and $\nu_2$ have different supports, unlike many other divergence measures (e.g., Kullback-Leibler and Jensen-Shannon divergence), which either diverge to infinite or become constant in such cases.  Furthermore, the Mallows' distance satisfies the triangle inequality, making it a true distance metric, an important property not shared by many other divergence measures. For a thorough theoretical treatment of Mallows' distance, we refer to \citet{Villani}.

With the Mallows' distance defined, we can now proceed to develop the DM method. The key idea is to prevent model collapse by minimizing the Mallows' distance between the representation distribution and the predefined reference distribution. As a result, constructing the reference distribution becomes the most crucial step, which can be broken down into three sub-steps. In the first sub-step, we design $K^\prime$ centers in $\R^{d^*}$ , where $K^\prime \leq d^*$. The $i$-th center $\bm{c}_i$ is chosen to be either $\bm{e}_i$ or $-\bm{e}_i$ with equal probability, where $\bm{e}_i$ is the standard basis vector in $\R^{d^*}$ with the $i$-th component equal to $1$ and all others components equal to $0$. In the second sub-step, we define the $i$-th reference part a random vector as:
\begin{align}\label{eq: P_i}
    \mathcal{P}_i =  R\frac{\bm{c}_i + \epsilon\frac{\bm{\gamma}_{d^*}}{\norm{\bm{\gamma}_{d^*}}_2}}{\big\Vert\bm{c}_i + \epsilon\frac{\bm{\gamma}_{d^*}}{\norm{\bm{\gamma}_{d^*}}_2}\big\Vert_2},
\end{align}
where $\epsilon > 0$ is a tuning parameter, and $\bm{\gamma}_{d^*}$ is a standard Gaussian random vector in $\R^{d^*}$. To gain an intuitive understanding of $\mathcal{P}_i$, let $\mathcal{B}(\bm{a}, r)$ denote the ball centered at $\bm{a} \in \R^{d^*}$ with radius $r > 0$. It is straightforward to observe that the vector $\bm{\gamma}_{d^*}/\norm{\bm{\gamma}_{d^*}}_2$  follows a uniform distribution on the surface of the unit ball $\mathcal{B}(\bm{0}, 1)$. We then scale and translate this vector to lie within the ball $\mathcal{B}(\bm{c}_i, \epsilon)$ by multiplying by $\epsilon$ and adding the center $\bm{c}_i$. To ensure that the resulting random variable $\mathcal{P}_i$ lies on the surface of the ball $\mathcal{B}(\bm{0}, R)$, we normalize the vector and scale it by $R$. As shown in the left-hand side of Figure~\ref{fig: reference}, the process results in $\mathcal{P}_i$ follows a uniform distribution over the orange region of the sphere. Next, we define a categorical random variable $\mathcal{C} \in \{1, 2, \cdots, K^\prime\}$ with $\P(\mathcal{C} = i) = \alpha_i$, where $\alpha_i$ are the probabilities associated with $i$-th part, and $\mathcal{C}$ is independent of $\mathcal{P}_i$ for all $1 \leq i \leq K^\prime$. We then construct a new random variable $\mathcal{R}$ as $\mathcal{R} = \sum_{i=1}^{K^\prime}\1(\mathcal{C} = i)\mathcal{P}_i$. The distribution of $\mathcal{R}$ is referred to as the reference distribution, denoted by $\P_\mathcal{R}$. DM aim to cluster augmented views with similar semantic meaning according to the same part of the reference distribution by minimizing $\mathcal{W}(\P_f, \P_\mathcal{R})$, as illustrated on the right hand side of Figure~\ref{fig: reference}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{reference.png}
    \caption{\textbf{Left. Generative process of $\mathcal{P}_i$} ($R = 1$). Given $\bm{c}_i$ (red point) and $\epsilon$, the black point is obtained by adding $\bm{c}_i$ and $\epsilon \bm{\gamma}_{d^*}/\norm{\bm{\gamma}_{d^*}}_2$. Normalizing this into the sphere of radius $R$ yields a sample of $\mathcal{P}_i$ (purple point). This process results in $\mathcal{P}_i$ following a uniform distribution on the orange region of the sphere. \textbf{Right: The key idea of DM}. Augmented views with similar semantic meaning are mapped to the same region of the reference distribution.}
    \label{fig: reference}
\end{figure}

We define the representation distribution $\P_f = f_\sharp\P_\mathcal{A}$, where $\P_\mathcal{A}$ is the distribution of augmented views. This is rigorously given by $\P_\mathcal{A}(E) = \int\frac{1}{M}\sum_{A \in \mathcal{A}}\1\{A(\bm{x}) \in E\}\P_S(d\bm{x})$ for any measurable set $E$. The DM learning problem is then formulated as the following minimization problem:  
\begin{align}\label{eq: risk at pop level}
    f^* \in \argmin_{f \in \mathcal{F}}\mathcal{L}(f) := \mathcal{L}_{\mathrm{align}}(f) + \lambda \cdot \mathcal{W}(\P_f, \P_\mathcal{R}).
\end{align}
where $\mathcal{L}(f)$ is the objective function that consists of alignment loss and the Wasserstein distance between $\P_f$ and $\P_\mathcal{R}$. The tuning parameter $\lambda > 0$ balances the relative importance of $\mathcal{L}_{\mathrm{align}}(f)$ and $\mathcal{W}(\P_f, \P_\mathcal{R})$. The function class $\mathcal{F}$ is defined in~\eqref{eq: definition of F}. It is important to note that the solution $f^*$ to this minimization problem may bot be unique, as $f\in \min_{f \in \mathcal{F}}\mathcal{L}(f)$ implies multiple possible minimizer. Let $\mathcal{G} := \mathrm{Lip(1)}$ and plug~\eqref{eq: augmentaion invariant} and~\eqref{eq: dual form of wasserstein distance} into~\eqref{eq: risk at pop level} gives the following formulation of the DM learning problem:
\begin{align}
\label{eq: risk at pop level (specific version)}
f^* \in \argmin_{f \in \mathcal{F}} \mathop{\E}_{X_S \sim \P_S} \mathop{\E}_{\mathtt{X}_{S,1}, \mathtt{X}_{S,2} \sim \mathcal{A}(X_S)} \Big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2}) \Big\Vert_2^2 + \lambda \sup_{g \in \mathcal{G}} \mathop{\E}_{Z \sim \P_f} g(Z)  - \mathop{\E}_{\mathcal{R} \sim \P_\mathcal{R}} g(\mathcal{R}).
\end{align}
It is evident that~\eqref{eq: risk at pop level (specific version)} can be interpreted as a mini-max optimization problem. To emphasize this, we rewrite it as follows:
\begin{align}\label{eq: mini-max}
    (f^*, g^*) \in \arg\min_{f \in \mathcal{F}}\max_{g \in \mathcal{G}}\mathcal{L}(f,g) :=\mathcal{L}_{\mathrm{align}}(f) + \lambda \cdot \mathcal{W}(f, g),
\end{align}
where $\mathcal{W}(f, g) = \mathop{\E}_{Z \sim \P_f}\{g(Z)\} - \mathop{\E}_{\mathcal{R} \sim \P_\mathcal{R}}\{g(\mathcal{R})\}$, $g$ is referred to as a critic. It immediately follows that $\mathcal{W}(\P_f, \P_\mathcal{R}) = \sup_{g \in \mathcal{G}}\mathcal{W}(f, g)$ and $\mathcal{L}(f) = \sup_{g \in \mathcal{G}}\mathcal{L}(f, g)$.

To solve~\eqref{eq: mini-max} in practice, we face two challenges. The first challenge is the population distribution of the original images $\P_S$ is unknown. We therefore have to replace it by its finite sample counterpart. Specifically, for each instance $X_S^{(i)}$, we sample two augmentations $A_{i,1}$ and $A_{i,2}$ from $\mathcal{A}$ uniformly. These augmentations produce two views, $\tilde{\mathtt{X}}_S^{(i)} = (\mathtt{X}^{(i)}_{S,1}, \mathtt{X}^{(i)}_{S,2}) = (A_{i,1}(X_S^{(i)}), A_{i,2}(X_S^{(i)})) \in \R^{2d}$. Simultaneously, we independently collect $n_S$ instances $\{\mathcal{R}^{(i)}: 1\leq i \leq n_S\}$ from $\P_\mathcal{R}$. The resulting augmentation-reference dataset is $\widetilde{\mathcal{D}}_S = \{(\tilde{\mathtt{X}}_S^{(i)}, \mathcal{R}^{(i)}): 1 \leq i \leq n_S\}$. The finite sample approximation of $\mathcal{L}(f,g)$ is then:
\begin{gather}\label{eq: hatW(f,g)}
    \widehat{\mathcal{L}}(f, g) := \widehat{\mathcal{L}}_{\mathrm{align}}(f) + \lambda \cdot \widehat{\mathcal{W}}(f, g), \nonumber\\
    \widehat{\mathcal{L}}_{\mathrm{align}}(f)=\frac{1}{n_S}\sum_{i=1}^{n_S}\norm{f(\mathtt{X}^{(i)}_{S,1}) - f(\mathtt{X}^{(i)}_{S,2})}_2^2, \nonumber \\
    \widehat{\mathcal{W}}(f, g) = \frac{1}{n_S}\sum_{i=1}^{n_S}\Big[g(\mathcal{R}^{(i)}) - \frac{1}{2}\Big\{g\big(f(\mathtt{X}^{(i)}_{S,1})\big) + g\big(f(\mathtt{X}^{(i)}_{S,2})\big)\Big\}\Big].
\end{gather}
It is evident that $\mathcal{W}(f, g) = \E_{\widetilde{\mathcal{D}}_S}\{\widehat{\mathcal{W}}(f, g)\}$ and $\mathcal{L}(f, g) = \E_{\widetilde{\mathcal{D}}_S}\{\widehat{\mathcal{L}}(f, g)\}$, which justifies calling $\widehat{\mathcal{L}}(f,g)$ the finite sample counterpart of $\mathcal{L}(f,g)$.

The second challenge stems from the complexity of the functional spaces $\mathcal{F}$ and $\mathcal{G}$, which complicates practical search. To overcome this, we parametrize them using deep ReLU networks. Specifically, we define a class of deep ReLU networks as follows:
\begin{definition}[Deep ReLU network class]
\label{def: Deep ReLU networks}
The function $f_{\bm{\theta}}(\bm{x}): \R^p \to \R^q$ implemented by a deep ReLU network with parameter $\bm{\theta}$ is expressed as composition of a sequence of functions
    \begin{align*}
        f_{\bm{\theta}}(\bm{x}) := l_{\mathtt{D}} \circ \varrho \circ l_{\mathtt{D}-1} \circ \varrho \circ \cdots \circ l_1 \circ \varrho \circ l_0(\bm{x})
    \end{align*}
    for any $\bm{x} \in \R^{p}$, where $\varrho(\bm{x})$ is the ReLU activation function and the depth $\mathtt{D}$ is the number of hidden layers. For $1\leq i \leq \mathtt{D}$, the $i$-th layer is represented by $l_i(\bm{x}) := A_i \bm{x} + \bm{b}_i$, where $A_i \in \R^{d_{i+1} \times d_i}$ is the weight matrix, $\bm{b}_i \in \R^{d_{i+1}}$ is the bias vector, $d_i$ is the width of the $i$-th layer and $\bm{\theta} = ((A_0,\bm{b}_0), \cdots,(A_{\mathtt{D}},\bm{b}_{\mathtt{D}}))$.
    The network $f_{\bm{\theta}}$ contains $(\mathtt{D}+1)$ layers in all.
    We use a $(\mathtt{D} + 1)$-dimension vector $(d_0, d_1, \cdots, d_{\mathtt{D}})^{\top}$ to describe the width of each layer.
    In particular, $d_0 = p$ is the dimension of the domain and $d_{\mathtt{D}} = q$ is the dimension of the codomain.
    The width $\mathtt{W}$ is defined as the maximum width of hidden layers, that is, $\mathtt{W} = \max \left\{d_1, d_2, \cdots, d_{\mathtt{D}} \right\}$.
    The bound $\mathtt{B}$ denotes the $L^{\infty}$ bound of $f_{\bm{\theta}}(\cdot)$, that is, $\sup_{\bm{x} \in \R^p} \Vert f_{\bm{\theta}}(\bm{x}) \Vert_{\infty} \le \mathtt{B}$.
    We denote the function class $\{f_{\bm{\theta}}: \R^p \to \R^q\}$ implemented by deep ReLU network class with width $\mathtt{W}$, depth $\mathtt{D}$, and bound $\mathtt{B}$ as $\mathcal{NN}_{p, q}(\mathtt{W}, \mathtt{D}, \mathtt{B})$.
\end{definition}
By parametrizing $\mathcal{F}$ and $\mathcal{G}$ as two deep ReLU network classes, the optimization problem in \eqref{eq: mini-max} is reformulated as:
\begin{align}\label{eq: (hatf, hatg) ∈ argminmaxhatL(f,g)}
    (\hat{f}_{n_S}, \hat{g}_{n_S}) \in \arg\min_{f \in \widehat{\mathcal{F}}}\max_{g \in \widehat{\mathcal{G}}}\widehat{\mathcal{L}}(f, g),
\end{align}
where $\widehat{\mathcal{F}} = \mathcal{NN}_{d, d^*}(\mathtt{W}_1, \mathtt{D}_1, \mathtt{B}_1)$ and $\widehat{\mathcal{G}} = \mathcal{NN}_{d^*, 1}(\mathtt{W}_2, \mathtt{D}_2, \mathtt{B}_2)$. In practice, we set $\mathtt{W}_1 \gtrsim \mathtt{W}_2$ and $\mathtt{D}_1 \gtrsim \mathtt{D}_2$, ensuring that $\mathtt{W}_1\mathtt{D}_1 \gtrsim \mathtt{W}_2\mathtt{D}_2$ in subsequent analysis.

\subsection{Transfer Learning}\label{subsection: transfer learning}
One significant application of learned representations is transfer learning. Recall $\mathcal{D}_T = \{(X_T^{(i)}, Y_i): 1 \leq i \leq n_T\}$ denotes the target dataset. For each $X_T^{(i)} \in \mathcal{D}_T$, we sample two augmentations $A_{i,1}, A_{i,2}$ from $\mathcal{A}$ uniformly, resulting in $\tilde{\mathtt{X}}_T^{(i)} = (\mathtt{X}_{T,1}^{(i)}, \mathtt{X}_{T, 2}^{(i)}) =(A_{i,1}(X_T^{(i)}), A_{i,2}(X_T^{(i)}))$. The augmented dataset is then $\widetilde{\mathcal{D}}_T=\{(\tilde{\mathtt{X}}_T^{(i)}, Y_i): 1 \leq i \leq n_T\}$. We next consider a linear classifier
\begin{equation}\label{eq: linear probe}
G_f(\bm{x})=\mathop{\arg\max}_{1\leq k \leq K}\big(\widehat{W}f(\bm{x})\big)_k,
\end{equation}
where $(\cdot)_k$ denotes the $k$-th entry of the vector, and $\widehat{W}$ is a $K \times d^*$ matrix with its $k$-th row given by 
\begin{align}\label{eq: hatμT(k)}
    \widehat{\mu}_T(k)=\frac{1}{2n_{T}(k)}\sum_{i=1}^{n_T}\{f(\mathtt{X}_{T,1}^{(i)}) + f(\mathtt{X}_{T,2}^{(i)})\}\1\{Y_i = k\},
\end{align}
where $n_T(k) = \sum_{i=1}^{n_T}\1\{Y_i = k\}$ represents the sample size of the $k$-th class. It is evident that $\widehat{\mu}_T(k)$ serves as an unbiased estimator of $\mu_T(k) = \E_{(X_T, Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\{f(\mathtt{X}_T) \vert Y = k\}$, which denotes the center of the $k$-th class in the representation space. To evaluate its performance, we examine its misclassification rate by
\begin{align*}
    \mathrm{Err}\big(G_f\big) = \P_T\big\{G_f(X_T) \neq Y\big\},
\end{align*}
where $(X_T, Y)$ represents an independent copy of $(X_T^{(i)}, Y_i)$.