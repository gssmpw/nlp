\section{Theoretical Guarantee}\label{section: theory}
\subsection{Population Theorem}
\label{subsection: pop theorem}
We assume that any upstream data $X_S \sim \P_S$ can be categorized into categorized into some of $K$ latent classes, each corresponding to a distinct downstream class. The term ``latent" implies that these classes are not directly observable to us, but do exist. For $1 \leq k \leq K$, we define $C_S(k)$ as the set of data points belonging to the $k$-th latent class. The conditional probability distribution $\P_f(k)$ is given by $\P_f(k)(\cdot) = \P_f\{\cdot \vert X_S \in C_S(k)\}$, with its population center $\mu_S(k) = \E_{X_S \sim \P_S}\E_{\mathtt{X}_S \sim \mathcal{A}(X_S)}\{f(\mathtt{X_S})\vert X_S \in C_S(k)\}$. 

The goal of DM is to render source data well-separated. Specifically, we aim to drive $\mu_S(i)^\top\mu_S(j)$ as close to zero as possible for any $i \neq j$. To accomplish this, we aim to push $\P_f(k)$ towards distinct parts of the reference distribution through Mallows' distance (which we refer to as ``pushing $\P_f$ in parts"), thereby inheriting the characteristics of $\P_\mathcal{R}$. However, since $\P_f(k)$ is inaccessible, we must instead minimize the overall Mallows' distance $\mathcal{W}(\P_f, \P_\mathcal{R})$, and explore its effect on achieving the objective of separating the classes.

We begin by assigning labels to each part of the reference distribution. Let the predefined reference consist of $K$ disjoint parts $\{C_k: 1\leq k \leq K\}$. Let $\Q^*$ represent the joint distribution of $(Z^*, \mathcal{R}^*)$, where $(Z^*, \mathcal{R}^*)= \arg\min_{(Z, \mathcal{R}) \in \Pi(\P_f, \P_\mathcal{R})}\E_{(Z, \mathcal{R})}(\norm{Z - \mathcal{R}}_1)$.  Denote the set of permutations on $\{1, 2,\cdots, K\}$ by $P_K$. The $k$-th class of the reference, $C_\mathcal{R}(k)$, corresponding to the $k$-th latent class $C_S(k)$, is defined as $C_\mathcal{R}(k) := C_{\tau^*(k)}$, where $\tau^* = \argmax_{\tau \in P_K}\sum_{k=1}^K\Q^*(C_S(k) \to C_{\tau(k)})$. Therein, $\Q^*(C_S(k) \to C_{\tau(k)})$ represents the transport mass from $C_S(k)$ to $C_{\tau(k)}$ according to $\Q^*$. To better understand this assignment, consider an example with $K = 3$ and $\Q^*$ such that $\Q^*(C_S(1) \to C_1) = 1/5,\Q^*(C_S(1) \to C_2) = 0,  \Q^*(C_S(1) \to C_3) = 2/15; \Q^*(C_S(2) \to C_1) = 1/15, \Q^*(C_S(2) \to C_2) = 1/30, \Q^*(C_S(2) \to C_3) = 7/30$ and $\Q^*(C_S(3) \to C_1) = 4/15,  \Q^*(C_S(3) \to C_2) = 1/30, \Q^*(C_S(3) \to C_3) = 1/30$. In this context, we for example evaluate two permutations: $\tau_1: (1, 2, 3) \mapsto (3, 2, 1)$ and $\tau_2: (1, 2, 3) \mapsto (2, 3, 1)$. For the permutation $\tau_1$, we have $\sum_{k=1}^3\Q^*(C_S(k) \to C_{\tau_1(k)}) = \Q^*(C_S(1) \to C_3) + \Q^*(C_S(2) \to C_2) + \Q^*(C_S(3) \to C_1) = 2/15 + 1/30 + 4/15 = 13 / 30$, while $\sum_{k=1}^3\Q^*(C_S(k) \to C_{\tau_2(k)}) = \Q^*(C_S(1) \to C_2) + \Q^*(C_S(2) \to C_3) + \Q^*(C_S(3) \to C_1) = 0 + 7/30 + 4/15 = 1/2$. After comparisons across all permutations, we obtain $C_\mathcal{R}(1) = C_2, C_\mathcal{R}(2) = C_3, C_\mathcal{R}(3) = C_1$. In summary, for given $C_i$, we tend to assign the label $\arg\max_k \Q^*(C_S(k) \to C_i)$. However, it may lead to non-unique assignments. We resolve this by introducing optimal permutation.

Let $Z \in C_S(i)$ denote the event $Z = f(A(X_S))$ for some $X_S \in C_S(i)$ and $A \in \mathcal{A}$, $\P_\mathcal{R}(j)$ be the uniform distribution on $C_\mathcal{R}(j)$. We can yield
\begin{align}
    \label{eq: ovarall Wasserstein vs conditional Wasserstein}
    \mathcal{W}(\P_f, \P_\mathcal{R}) &= \int_{(Z, \mathcal{R})}\norm{Z - \mathcal
    R}_1 d\Q^*(Z, \mathcal
    R) \nonumber \\
    &= \sum_{i,j=1}^K\Big\{\int_{(Z, \mathcal{R})}\norm{Z - \mathcal
    R}_1d\Q^*\big(Z, \mathcal{R} \vert Z \in C_S(i), \mathcal{R}\in \P_\mathcal{R}(j)\big)\Big\}\Q^*\big(C_S(i) \to \P_\mathcal{R}(j)\big) \nonumber \\
    &\geq \sum_{i,j=1}^K\mathcal{W}\big(\P_f(i), \P_\mathcal{R}(j)\big)\Q^*\big(C_S(i) \to C_\mathcal{R}(j)\big) \nonumber \\
    &\geq \Q^*\big(C_S(k) \to C_\mathcal{R}(k)\big)\mathcal{W}\big(\P_f(k), \P_\mathcal{R}(k)\big),
\end{align}
where the first inequality follows from $\Q^*(Z, \mathcal{R} \vert Z \in C_S(i), \mathcal{R} \in \P_\mathcal{R}(j)) \in \Pi(\P_f(i), \P_\mathcal{R}(j))$. Therefore, under Assumption~\ref{assumption: Q* does not vanish}, we know that $\mathcal{W}(\P_f(k), \P_\mathcal{R}(k)) \lesssim \mathcal{W}(\P_f, \P_\mathcal{R}) \leq \mathcal{L}(f)$.
\begin{assumption}
    \label{assumption: Q* does not vanish}
    Assume $\Q^*\big(C_S(k) \to C_\mathcal{R}(k)\big) > 0$ for any $k : 1 \leq k \leq K$.
\end{assumption}
Assumption~\ref{assumption: Q* does not vanish} essentially indicates that, in contrast to the example above regarding label assignments, we do not desire $C_2$ to be labeled as $C_\mathcal{R}(1)$ while $\Q^*(C_S(1) \to C_2) = 0$. 

Furthermore, by utilizing $\bm{c}_i^\top \bm{c}_j = 0$ and $\norm{\bm{c}_i}_2 = R$, we yield
\begin{align}\label{eq: μ(i)μ(j) < ||μ - c||}
    \mu_S(i)^{\top}\mu_S(j) &= (\mu_S(i) - \bm{c}_i)^{\top}\mu_S(j) + \bm{c}_i^{\top}\mu_S(j) + \bm{c}_i^\top \bm{c}_j + \bm{c}_i^\top(\mu_S(j) - \bm{c}_j)\nonumber \\
    &\leq \Vert\mu_S(i) - \bm{c}_i\Vert_2\Vert\mu_S(j)\Vert_2 + \Vert \bm{c}_i\Vert_2\Vert\mu_S(j) - \bm{c}_j\Vert_2 \nonumber\\
    &\lesssim \Vert\mu_S(i) - \bm{c}_i\Vert_2 + \Vert\mu_S(j) - \bm{c}_j\Vert_2,
\end{align}
where the last inequality stems from $\norm{\mu_S(j)}_2 = \big\Vert\E_{X_S \sim \P_S}\E_{\mathtt{X}_S \sim \mathcal{A}(X_S)}\{f(\mathtt{X}_S)\vert X_S \in C_S(j)\}\big\Vert_2 \leq \E_{X_S \sim \P_S}\E_{\mathtt{X}_S \sim \mathcal{A}(X_S)}\{\norm{f(\mathtt{X}_S)}_2\big\vert X_S \in C_S(j)\} = R$. Moreover, regarding $\norm{\mu_S(k) - \bm{c}_k}_2$,
\begin{align}\label{eq: 2-norm of μ - c}
    \norm{\mu_S(k) - \bm{c}_k}^2_2 &=\sum_{l=1}^{d^*}\Big[\mathop{\E}_{X_S \in C_S(k)}\mathop{\E}_{\mathtt{X}_S \in \mathcal{A}(X_S)}\big\{f_l(\mathtt{X_S})\big\} - \mathop{\E}_{\mathcal{R}_k \in C_\mathcal{R}(k)}\big\{\mathcal{R}_{k,l}\big\}\Big]^2 \nonumber\\
    &\leq \sum_{l=1}^{d^*}\mathcal{W}^2\big(\P_f(k), \P_\mathcal{R}(k)\big) = d^*\mathcal{W}^2\big(\P_f(k), \P_\mathcal{R}(k)\big) \nonumber\\
    &\lesssim \mathcal{W}^2\big(\P_f(k), \P_\mathcal{R}(k)\big),
\end{align}
where the first inequality follows from~\eqref{eq: dual form of wasserstein distance}. Plugging~\eqref{eq: 2-norm of μ - c} into~\eqref{eq: μ(i)μ(j) < ||μ - c||} yields $\mu_S(i)^{\top}\mu_S(j) \lesssim \mathcal{L}(f)$, implying that minimizing the loss function of DM can indeed reduce $\mu_S(i)^{\top}\mu_S(j)$.

We now show that minimizing $\mathcal{L}(f)$ also reduces $\mu_T(i)^\top\mu_T(j)$. It suffices to explore the relationship between $\mu_S(i)^\top\mu_S(j)$ and $\mu_T(i)^\top\mu_T(j)$. Let $\P_T(k)$ be the distribution defined by $\P_T(k)(E) = \P_T(X_S \in E \vert Y = k)$ for any measurable set $E$, with $p_S(k) = \P_S\{X_S \in C_S(k)\}$ and $p_T(k) = \P_T(Y = k)$. To quantify the distribution shift, we define 
\begin{align}\label{eq: distribution shift}
\epsilon_1 = \max_k\mathcal{W}\big(\P_S(k), \P_T(k)\big),\quad \epsilon_2 = \max_k\abs{p_S(k)- p_T(k)}.    
\end{align}
Thus, we have 
\begin{align*}
    \mu_T(i)^\top\mu_T(j) - \mu_S(i)^\top\mu_S(j) &= \mu_T(i)^\top\mu_T(j) - \mu_T(i)^\top\mu_S(j) + \mu_T(i)^\top\mu_S(j) - \mu_S(i)^\top\mu_S(j) \\
    &=\mu_T(i)^\top\{\mu_T(j) - \mu_S(j)\} + \{\mu_T(i) - \mu_S(i)\}^\top\mu_S(j) \\
    &\leq \norm{\mu_T(i)}_2\norm{\mu_T(j) - \mu_S(j)}_2 + \norm{\mu_T(i) - \mu_S(i)}_2\norm{\mu_S(j)}_2 \\
    &\leq R\big\{\norm{\mu_T(j) - \mu_S(j)}_2 + \norm{\mu_T(i) - \mu_S(i)}_2\big\}.
\end{align*}
Moreover, for any $1 \leq k \leq K$, we have:
\begin{align*}
&\norm{\mu_S(k) - \mu_T(k)}_2^2 = \sum_{l=1}^{d^*}\Big[\big\{\mu_S(k)\big\}_l - \big\{\mu_T(k)\big\}_l\Big]^2 \\
&= \sum_{l=1}^{d^*}\Big[\E_{X_S \sim \P_S}\E_{\mathtt{X}_S \sim \mathcal{A}(X_S)}\big\{f_l(\mathtt{X}_S) \vert X_S \in C_S(k)\big\} - \E_{(X_T, Y) \sim \P_T}\E_{\mathtt{X}^T \sim \mathcal{A}(X_T)}\big\{f_l(\mathtt{X}^T) \vert Y = k\big\}\Big]^2 \\
&= \frac{1}{M}\sum_{i=1}^M\sum_{l=1}^{d^*}\Big[\E_{X_S \sim \P_S}\big\{f_l(A_i(X_S) \vert X_S \in C_S(k)\big\} - \E_{(X_T, Y) \sim \P_T}\big\{f_l(A_i(X_T)) \vert Y = k\big\}\Big]^2.
\end{align*}
If we assume any $A_i \in \mathcal{A}$ is $Q$-Lipschitz function as Assumption~\ref{assumption: Lip augmentation}, and given that $f \in \mathrm{Lip}(L)$, we find that $f_l(A_\gamma(\cdot))$ is $LQ$-Lipschitz continuous for every $1 \leq l \leq d^*$. Furthermore, with $\epsilon_1 = \max_k\mathcal{W}\big(\P_S(k), \P_T(k)\big)$ and equation~\eqref{eq: dual form of wasserstein distance}, we can yield $\norm{\mu_S(k) - \mu_T(k)}_2^2 \lesssim \epsilon_1^2$. Consequently, for any $i \neq j$,
\begin{align}\label{eq : downstream divergence ≤ L + ɛ}
    \mu_T(i)^\top\mu_T(j) \lesssim \mu_S(i)^\top\mu_S(j) + \epsilon_1 \lesssim \mathcal{W}(\P_f, \P_\mathcal{R}) + \epsilon_1 \leq \mathcal{L}(f) + \epsilon_1,
\end{align}
which implies that minimizing $\mathcal{L}(f)$ can indeed reduce $\mu_T(i)^\top\mu_T(j)$, which intuitively measures the distinguishability between different target classes in the representation space.
\begin{assumption}\label{assumption: Lip augmentation}
    There exists a $Q > 0$ satisfying $\norm{A_i(\bm{x}_1) - A_i(\bm{x}_2)}_2 \leq Q\norm{\bm{x}_1 - \bm{x}_2}_2$ for for any $\bm{x}_1, \bm{x}_2 \in [0,1]^d$ and $1\leq i \leq M$.
\end{assumption}
This Assumption is highly realistic. A typical example is that the resulting augmented data obtained through cropping would not undergo drastic changes when minor perturbations are applied to the original image.

Next, we introduce the $(\sigma, \delta)$-augmentation to quantify the quality of data augmentation, inspired by \citet{huang2023towards}. Let $C_T(k)$ denote the set such that for the target data $(\bm{x}_T,y)$, $\bm{x}_T \in C_T(k)$ if and only if $y = k$. The $(\sigma, \delta)$-augmentation is then defined as 
\begin{definition}
\label{def: (σ,δ)-augmentation}
We refer to a collection of data augmentations $\mathcal{A}$ as $(\sigma, \delta)$-augmentation, if for each $1 \leq k \leq K$, there exists a subset $\widetilde{C}_T(k) \subseteq C_T(k)$, such that: (\romannumeral1) $\P_T\big\{X_T \in \widetilde{C}_T(k)\big\} \geq \sigma \P_T\big\{X_T \in C_T(k)\big\}$, (\romannumeral2) $\sup_{\bm{x}_{T, 1},\bm{x}_{T, 2} \in \widetilde{C}_T(k)}\min_{\mathtt{x}_{T, 1} \in \mathcal{A}(\bm{x}_{T, 1}), \mathtt{x}_{T, 2} \in \mathcal{A}(\bm{x}_{T, 2})}\norm{\mathtt{x}_{T, 1} - \mathtt{x}_{T, 2}}_2 \leq \delta$, and (\romannumeral3) 
$\P_T\{\cup_{k=1}^K\widetilde{C}_T(k)\} \geq \sigma$, where $\sigma \in (0,1]$ and $\delta \geq 0$. Moreover, $\widetilde{C}_T(k)$ is referred to as the main part of $C_T(k)$.
\end{definition}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{augmentation.png}
    \caption{Illustration of $(\sigma, \delta)$-augmentation}
    \label{fig: (σ,δ)-augmentation}
\end{figure}
We present Figure~\ref{fig: (σ,δ)-augmentation} to illustrate the motivation behind Definition~\ref{def: (σ,δ)-augmentation}. Consider the task of classifying \textit{dog} and \textit{cat}. Although the images $\bm{x}_1$ and $\bm{x}_2$ are semantically similar, their difference, $\norm{\bm{x}_1 - \bm{x}_2}_2$, can be large due to background variations. Through data augmentation, we can find $\mathtt{x}_1^* \in \mathcal{A}(\bm{x}_1)$ and $\mathtt{x}_2^* \in \mathcal{A}(\bm{x}_2)$ such that $\norm{\mathtt{x}_1^* - \mathtt{x}_2^*}_2$ is sufficiently small. In this regard, the quantity $d_{\mathcal{A}}(\bm{x}_1, \bm{x}_2):=\min_{\mathtt{x}_1 \in \mathcal{A}(\bm{x}_1), \mathtt{x}_2 \in \mathcal{A}(\bm{x}_2)}\norm{\mathtt{x}_1 - \mathtt{x}_2}_2$ can indeed capture the semantic similarity. Furthermore, the supremum over $C_T(k)$, $\sup_{\bm{x}_{T, 1},\bm{x}_{T, 2} \in C_T(k)}d_\mathcal{A}(\bm{x}_{T,1}, \bm{x}_{T,2})$ serves as a criterion for evaluating the quality of data augmentation. However, problematic pairs $(\breve{\bm{x}}_{T,1}, \breve{\bm{x}}_{T,2})$ such that $d_{\mathcal{A}}(\breve{\bm{x}}_1, \breve{\bm{x}}_2)$ is significantly larger than that of other pairs, causing the supremum to be disproportionately large, leading to unreliable results. To fix this issue, we replace $C_T(k)$ with its subset $\widetilde{C}_T(k)$ satisfying condition (\romannumeral1), improving the robustness of the definition. Moreover, condition (\romannumeral3) implies that the augmentation should be sufficiently effective to correctly recognize the objects that align with the image label. Specifically, consider the image presented in Figure~\ref{fig: toilet paper imagenet}, this condition necessitates that the data augmentations can accurately recognize the patch of \textit{toilet paper} rather than the other objects, as this image has been labeled as \textit{toilet paper} in ImageNet. A simpler alternative to this condition is to assume that different classes $C_T(k)$ are pairwise disjoint, i.e., $\forall i \neq j, C_T(i) \cap C_T(j) = \emptyset$, which implies $\P_T\{\cup_{k=1}^K\widetilde{C}_T(k)\big\} = \sum_{k=1}^K\P_T\{\widetilde{C}_T(k)\} \geq \sigma\sum_{k=1}^K\P_T\{C_T(k)\} = \sigma$. With these, we are now are ready to present the population theorem.
\begin{theorem}
\label{theorem: pop theorem}
Given a $(\sigma, \delta)$-augmentation, if the encoder $f$ with $\norm{f}_2=R$ is $L$-Lipschitz, and if Assumption~\ref{assumption: Q* does not vanish} and~\ref{assumption: Lip augmentation} hold, then for any $\eps > 0$, $\max_{i \neq j}\mu_T(i)^{\top}\mu_T(j) \lesssim \mathcal{L}(f) + \epsilon_1$. Furthermore, if $\max_{i\neq j}\mu_T(i)^{\top}\mu_T(j) < R^2\psi(\sigma, \delta, \eps, f)$, then the downstream misclassification rate of $G_f$
\begin{align*}
\mathrm{Err}(G_f) \leq (1 - \sigma) + \mathcal{O}\Big(\eps^{-1}\big\{\mathcal{L}(f) + \epsilon_1 + \epsilon_2\big\}^{\frac{1}{2}}\Big),
\end{align*}
where the specific formulation of $\psi(\sigma, \delta, \eps, f)$ can be found in Lemma~\ref{lemma: sufficient condition of small Err}.
\end{theorem}
Theorem~\ref{theorem: pop theorem} demonstrates that minimizing the loss function of DM results in a well-separated representation space for downstream task. Specifically, once the quantity $\mu_T(i)^{\top}\mu_T(j)$ falls below the critical threshold $\psi(\sigma, \delta, \eps, f)$, minimizing $\mathcal{L}(f)$ significantly reduces the downstream misclassification rate. The error bound is composed of three factors: the quality of augmentation $\sigma$, the loss function of DM, $\mathcal{L}(f)$, and the distribution shift $\epsilon_1, \epsilon_2$.

While Theorem~\ref{theorem: pop theorem} highlights the effectiveness of DM, several questions remain unresolved. First, can the sample-level minimizer $\hat{f}_{n_S}$ satisfy the conditions of Theorem~\ref{theorem: pop theorem}? Second, is it possible to establish an end-to-end error bound for DM to analyze the impact of $n_S$ and $n_T$ on the misclassification rate,
thereby elucidating the success of few-shot learning?
\subsection{Sample Theorem}\label{subsection: sample theorem}
\begin{assumption}\label{assumption: existence of augmentation}
    Assume there exists a sequence of $(\sigma_n, \delta_n)$-augmentations $\mathcal
    {A}_n$ such that both $\sigma_n \to 1$ and $\delta_n \to 0$.
\end{assumption}
We note that, in contrast to Assumption 4.3 in \citet{duan2024unsupervisedtransferlearningadversarial}, which requires the convergence rate of $\delta_n$ to be faster than $\mathcal{O}(n^{-(d + 1)/2(\gamma + d + 1)})$ for $f^*$ in the H{\"o}lder class with parameter $\gamma$, our Assumption~\ref{assumption: existence of augmentation} only requires that $\delta_n \to 1$ without any constraint on the convergence rate. Hence, Assumption~\ref{assumption: existence of augmentation} is notably milder.

\begin{assumption}\label{assumption: distributions shift < n}
    Assume there exists $\alpha > 0$ and $\beta > 0$ such that $\epsilon_1 = \mathcal{O}(n_S^{-\alpha})$ and $\epsilon_2 =\mathcal{O}(n_S^{-\beta})$ for sufficiently large $n_S$.
\end{assumption}
This assumption implies that the distribution shift should not be excessive. Intuitively, a model that distinguishes between cats and dogs is largely ineffective for identifying whether a patient is ill based on X-ray images, due to the significant domain shift between the tasks.

Before presenting the final assumption, we introduce Lemma~\ref{lemma: Existence of optimal transport map}, known as Brenier's theorem in optimal transport theory. Its proof can be found in Theorem 1 of \citet{ball2004transport}.
\begin{lemma}[Existence of Optimal Transport Map]\label{lemma: Existence of optimal transport map}
If $\nu_1$ and $\nu_2$ are probability measures on $\R^k$, $\nu_2$ has compact support and $\nu_1$ assigns no mass to any set of Hausdorff dimension $(k - 1)$, then there exists a optimal transport map $T: \mathbb{R}^k \to \mathbb{R}^k$ transporting $\nu_1$ to $\nu_2$, i.e. $T_{\sharp}(\nu_1) = \nu_2$. Moreover, $T$ is bijective.
\end{lemma}
We now introduce Assumption~\ref{assumption: lipschitz transportation}, which justifies that $\mathcal{L}(f^*) = 0$, a crucial step for extending our theory to the sample level. Further details are provided in Section~\ref{subsection: vanish L(f*)}.
\begin{assumption}\label{assumption: lipschitz transportation}
    Suppose there exists a Lipschitz map $f : \mathcal{X}_S \to \R^{d^*}$ satisfying (\romannumeral1) $f_{\sharp}\P_S$ assigns no mass to any set of Hausdorff dimension $(d^*-1)$ and (\romannumeral2) the optimal transport map transporting $f_{\sharp}\P_S$ to $\P_\mathcal{R}$ is also Lipschitz continuous.
\end{assumption}
The Lipschitz continuity of optimal transport maps has long been a key yet challenging problem, with numerous studies demonstrating this property under specific classes of distributions \citep{caffarelli2000monotonicity, kim2012generlization, carlier2024optimal, fathi2024transportation}. Therefore, Assumption~\ref{assumption: lipschitz transportation} essentially concerns the data distribution, where the variability of $f$ may allow a broad range of distributions to satisfy this condition.
\begin{theorem}\label{theorem: sample theorem}
Suppose Assumptions \ref{assumption: Q* does not vanish}-\ref{assumption: lipschitz transportation} hold. Set the widths and depths of the encoder and critic networks satisfying $\mathtt{D}_2\mathtt{W}_2\lesssim\mathtt{D}_1\mathtt{W}_1 = \mathcal{O}\big(n_S^{-\frac{d}{2d+4}}\big)$, and set the augmentation as $\mathcal{A}_{n_S}$, then with probability at least $1 - \mathcal{O}\big(n_S^{-\min\{\frac{1}{d+2}, \alpha\}}\big) - \mathcal{O}\big(\frac{1}{\sqrt{\min_{k}n_T(k)}}\big)$,
\begin{align*}
    \E_{\widetilde{\mathcal{D}}_S}\big\{\mathrm{Err}(G_{\hat{f}_{n_S}})\big\} \leq \big(1 - \sigma_{n_S}\big) + \mathcal{O}\Big(n_S^{-\min\{\frac{1}{2d+4}, \frac{\alpha}{4}, \frac{\beta}{4}\}}\Big)
\end{align*}
holds for sufficiently large $n_S$.
\end{theorem}
We defer the proof to Section~\ref{section: Appendix B}. Theorem~\ref{theorem: sample theorem} reveals that appropriately setting the widths and depths of encoder and critic ensures that the downstream misclassification rate of $G_{\hat{f}_{n_S}}$ is controlled by the quality of data augmentation $\sigma_{n_S}$ and the source sample size, with high probability. The convergence rate of the downstream misclassification rate is jointly determined by the original data dimension $d$ and the extent of the distribution shift, $\alpha$ and $\beta$. This probability approaches $1$ as $n_S$ or $n_T(k)$ increases. Notably, the convergence rate regarding $\min_k n_T(k)$ is $1/2$, implying even with a few downstream samples, as long as the unlabeled sample size $n_S$ is sufficiently large, the misclassification rate can still be maintained at a sufficiently low level, which coincides with empirical observations in practice.

