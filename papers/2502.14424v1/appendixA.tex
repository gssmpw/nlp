\section{Population theorem}\label{section: Appendix A}
The population theorem in this study mainly builds upon the technique used in \citet{huang2023towards} and \citet{duan2024unsupervisedtransferlearningadversarial}.
\begin{lemma}
\label{lemma: sufficient condition of small Err}
Given a $(\sigma, \delta)$-augmentation, if the encoder $f$ with $R_1 \leq \norm{f}_2 \leq R_2$ is $L$-Lipschitz and
\begin{align*}
\mu_T(i)^\top\mu_T(j) < R_2^2\psi(\sigma, \delta, \eps, f),
\end{align*}
holds for any pair of $(i,j)$ with $i \neq j$, then for any $\eps > 0$, the downstream misclassification rate of $G_f$
\begin{align*}
\text{\rm Err}(G_f) \leq (1 - \sigma) + U_T(\eps, f),
\end{align*}
where $U_T(\eps, f) = \P_T\big\{X_T: \sup_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\norm{f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})}_2 > \eps\big\}$ and
\begin{align}\label{eq: Ψ(σ, δ, ε, f)}
    \psi(\sigma, \delta, \eps,f) &= \Gamma_{\min}(\sigma, \delta, \eps, f) - \sqrt{2 - 2\Gamma_{\min}(\sigma, \delta, \eps, f)} - \frac{1}{2}\Big(1 - \frac{\min_k\norm{\hat{\mu}_T(k)}_2^2}{R_2}\Big) \nonumber\\
    &\qquad - \frac{2\max_k\norm{\hat{\mu}_T(k) - \mu_T(k)}_2}{R_2},
\end{align}
here $\Gamma_{\min}(\sigma, \delta, \eps, f)$ is given by
\begin{align*}
    \Gamma_{\min}(\sigma, \delta, \eps, f) = \begin{cases}
    (2\sigma - 1) - \frac{U_T(\eps, f)}{\min_ip_T(i)} - \Big(\sigma - \frac{U_T(\eps, f)}{\min_ip_T(i)}\Big)\Big(\frac{L\delta}{B} + \frac{2\eps}{B}\Big), &R_1 = R_2 = R \\
    \big(\sigma - \frac{U_T(\eps, f)}{\min_ip_T(i)}\big)\big(1 + \big(\frac{R_2}{R_1}\big)^2 - \frac{L\delta}{R_2} - \frac{2\eps}{R_2}\big) - 1, & R_1 < R_2.
\end{cases}
\end{align*}
\end{lemma}

\begin{proof}
For any encoder $f$, let $V_T(\eps, f):= \{X_T: \sup_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)} \norm{f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})}_2 \leq \eps\}$, if any $X_T \in \widetilde{C}_T(1) \cup \cdots \cup \widetilde{C}_T(K)\cap V_T(\eps, f)$ can be correctly classified by $G_f$, it turns out that $\mathrm{Err}(G_f)$ can be bounded by $(1 - \sigma) + U_T(\eps, f)$. In fact,
\begin{align*}
\mathrm{Err}(G_f) &= \P_T\big\{G_f(X_T) \neq k, Y = k\big\} \leq \P_T\Big[\big\{\widetilde{C}_T(1) \cup \cdots \cup \widetilde{C}_T(K) \cap V_T(\eps, f)\big\}^c\Big] \\
&= \P_T\Big[\big(\widetilde{C}_T(1) \cup \cdots \cup \widetilde{C}_T(K)\big)^c \cup \big\{V_T(\eps, f)\big\}^c\Big] \leq (1 - \sigma) + \P_T\big[\{V_T(\eps, f)\}^c\big] \\
&= (1 - \sigma) + U_T(\eps, f),
\end{align*}
where the last row is due to the fact $U_T(\eps, f) = \{V_T(\eps, f)\}^c$.

Hence it suffices to show for given $1 \leq i \leq K$, $X_T \in \widetilde{C}_T(i) \cap V_T(\eps, f)$ can be correctly classified by $G_f$ if for any $j \neq i$,
\begin{align*}
\mu_T(i)^\top\mu_T(j) < R_2^2\psi(\sigma, \delta, \eps, f).
\end{align*}

To this end, without losing generality, consider the case $i = 1$. To turn out $X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)$ can be correctly classified by $G_f$ under given condition, by the definition of $\widetilde{C}_T(1)$ and $V_T(\eps, f)$, It suffices to show for any $k \neq 1, \norm{f(X_T) - \hat{\mu}_T(1)}_2 < \norm{f(X_T) - \hat{\mu}_T(k)}_2$, which is equivalent to
\begin{align}\label{eq: sufficient condition to be classified as 1}
f(X_T)^{\top}\hat{\mu}_T(1) - f(X_T)^\top\hat{\mu}_T(k) - \Big(\frac{1}{2}\norm{\hat{\mu}_T(1)}^2_2 - \frac{1}{2}\norm{\hat{\mu}_T(k)}^2_2\Big) > 0.
\end{align}
We will firstly deal with the term $f(X_T)^\top\hat{\mu}_T(1)$,
\begin{align}
\label{eq: f(x)μ1 (1)}
&f(X_T)^{\top}\hat{\mu}_T(1) = f(X_T)^\top\mu_T(1) + f(X_T)^\top\{\hat{\mu}_T(1) - \mu_T(1)\} \nonumber\\
&\geq f(X_T)^{\top}\E_{(X_T, Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\{f(\mathtt{X}_T) \vert Y = 1\} - \norm{f(X_T)}_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2 \nonumber\\
&\geq \frac{1}{p_T(1)}f(X_T)^\top\E_{(X_T, Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\Big[f(\mathtt{X}_T)\1\big\{X_T \in C_T(1)\big\}\Big] - R_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2 \nonumber\\
&= \frac{1}{p_T(1)}f(X_T)^{\top}\E_{(X_T, Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\Big[f(\mathtt{X}_T)\1\big\{X_T \in C_T(1) \cap \widetilde{C}_T(1) \cap V_T(\eps, f)\big\}\Big] \nonumber\\
&\qquad + \frac{1}{p_T(1)}f(X_T)^\top\E_{(X_T, Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\Big[f(\mathtt{X}_T)\1\big\{X_T \in C_T(1) \cap \big(\widetilde{C}_T(1) \cap V_T(\eps, f)\big)^c\big\}\Big] \nonumber \\
&\qquad - R_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2 \nonumber\\
&= \frac{\P_T\big\{\widetilde{C}_T(1) \cap V_T(\eps, f)\big\}}{p_T(1)}f(X_T)^\top\E_{(X_T ,Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\Big\{f(\mathtt{X}_T) \big\vert X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)\Big\} \nonumber \\
&\qquad + \frac{1}{p_T(1)}\E_{(X_T, Y) \sim \P_T}\Big[\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\big\{f(X_T)^\top f(\mathtt{X}_T)\big\}\1\big\{X_T \in C_T(1) \backslash \big(\widetilde{C}_T(1) \cap V_T(\eps, f)\big)\big\}\Big] \nonumber \\
&\qquad - R_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2 \nonumber\\
&\geq \frac{\P_T\big\{\widetilde{C}_T(1) \cap V_T(\eps, f)\big\}}{p_T(1)}f(X_T)^{\top}\E_{(X_T ,Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\big\{f(\mathtt{X}_T) \big\vert X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)\big\} \nonumber\\
&\qquad- \frac{R_2^2}{p_T(1)}\P_T\Big[C_T(1) \backslash \big\{\widetilde{C}_T(1) \cap V_T(\eps, f)\big\}\Big] - R_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2,
\end{align}
where the second and the third inequalities are both due to the $\norm{f}_2 \leq R_2$. 

Furthermore, we note that
\begin{align}
\label{eq: P ≤ (1 - σ)p + U}
\P_T\Big[C_T(1) \backslash \big\{\widetilde{C}_T(1) \cap V_T(\eps, f)\big\}\Big] &= \P_T\Big[\{C_T(1) \backslash \widetilde{C}_T(1)\} \cup \big\{\widetilde{C}_T(1) \cap \big(V_T(\eps, f)\big)^c\}\Big] \nonumber \\ 
&\leq (1 - \sigma)p_T(1) + U_T(\eps, f),
\end{align}
and
\begin{align}
\label{eq : P ≥ σp - U}
\P_T\big\{\widetilde{C}_T(1) \cap V_T(\eps, f)\big\} &= \P_T\{C_T(1)\} - \P_T\Big[C_T(1) \backslash \big\{\widetilde{C}_T(1) \cap V_T(\eps, f)\big\}\Big] \nonumber \\
&\geq p_T(1) - \big\{(1 - \sigma)p_T(1) + U_T(\eps, f)\big\} \nonumber \\ 
&= \sigma p_T(1) - U_T(\eps, f).
\end{align}
Plugging \eqref{eq: P ≤ (1 - σ)p + U}, \eqref{eq : P ≥ σp - U} into \eqref{eq: f(x)μ1 (1)} yields
\begin{align}
\label{eq: f(x)μ1 (2)}
f(X_T)^{\top}\hat{\mu}_T(1) &\geq \Big(\sigma - \frac{U_T(\eps, f)}{p_T(1)}\Big)f(X_T)^\top\E_{(X_T ,Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\big\{f(\mathtt{X}_T) \big\vert X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)\big\}  \nonumber\\ 
&\qquad - R_2^2\Big(1 - \sigma + \frac{U_T(\eps, f)}{p_T(1)}\Big) - R_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2.
\end{align}
Notice that $X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)$. Thus for any $X_T^\prime \in \widetilde{C}_T(1)\cap V_T(\eps, f)$, by the definition of $\widetilde{C}_T(1)$, we have $\min_{\mathtt{X}_T \sim \mathcal{A}(X_T), \mathtt{X}_T^\prime \sim \mathcal{A}(X_T^\prime)}\norm{\mathtt{X}_T - \mathtt{X}_T^\prime}_2 \leq \delta$. Further denote $(\mathtt{X}_T^*, \mathtt{X}_T^{\prime*}) = \arg\min_{\mathtt{X}_T \sim \mathcal{A}(X_T), \mathtt{X}_T^\prime \sim \mathcal{A}(\mathtt{X}_T^\prime)}\norm{\mathtt{X}_T - \mathtt{X}_T^\prime}_2$, then $\norm{\mathtt{X}_T^* - \mathtt{X}_T^{\prime*}}_2 \leq \delta$, combining $L$-Lipschitz property of $f$ to yield $\norm{f(\mathtt{X}_T^*) - f(\mathtt{X}_T^{\prime*})}_2 \leq L \norm{\mathtt{X}_T^* - \mathtt{X}_T^{\prime*}}_2 \leq L\delta$. Besides that, since $X_T^\prime \in V_T(\eps, f)$, for any $\mathtt{X}_T^\prime \sim \mathcal{A}(X_T^\prime), \norm{f(\mathtt{X}_T^\prime) - f(\mathtt{X}_T^{\prime*})}_2 \leq \eps$. Similarly, as $X_T \in V_T(\eps, f)$ and $\mathtt{X}_T, \mathtt{X}_T^* \sim \mathcal{A}(X_T)$, we know $\norm{f(\mathtt{X}_T) - f(\mathtt{X}_T^*)}_2 \leq \eps$. Therefore,
\begin{align}
\label{eq: fEf}
&f(X_T)^{\top}\E_{(X_T ,Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\big\{f(\mathtt{X}_T) \big\vert X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)\big\} \nonumber \\
&= \E_{(X_T ,Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\big\{f(X_T)^\top f(\mathtt{X}_T)\big\vert X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)\big\} \nonumber \\
&= \E_{(X_T ,Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\Big[f(X_T)^\top\big\{f(\mathtt{X}_T) - f(X_T^\prime) + f(X_T^\prime)\big\} \big\vert X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)\Big] \nonumber \\
&\geq R_1^2 + \E_{(X_T ,Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\Big[f(X_T)^\top\big\{f(\mathtt{X}_T) - f(X_T^\prime)\big\} \big\vert X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)\Big] \nonumber \\
&= R_1^2 + \E_{(X_T ,Y) \sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\Big[f(X_T)^\top\big\{f(\mathtt{X}_T) - f(\mathtt{X}_T^*) + f(\mathtt{X}_T^*) - f(\mathtt{X}_T^{\prime*}) \nonumber \\
&\qquad +  f(\mathtt{X}_T^{\prime*}) - f(X_T^\prime)\big\} \big\vert X_T \in \widetilde{C}_T(1) \cap V_T(\eps, f)\Big] \nonumber \\
&\geq R_1^2 - (R_2\eps + R_2L\delta + R_2\eps) \nonumber \\
&= R_1^2 - R_2(L\delta + 2\eps),
\end{align}
where the first inequality is derived from the fact that $\norm{f}_2 \geq R_1$. Subsequently, plugging \eqref{eq: fEf} to the inequality \eqref{eq: f(x)μ1 (2)} yields
\begin{align}\label{eq: f(x)μ1 ≥ Γ}
f(X_T)^{\top}\hat{\mu}_T(1) &\geq \Big(\sigma - \frac{U_T(\eps, f)}{p_T(1)}\Big)f(X_T)^{\top}\mathop{\E}\limits_{(X_T ,Y) \sim \P_T}\mathop{\E}\limits_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\{f(\mathtt{X}_T)\} - R_2^2\Big(1 - \sigma + \frac{U_T(\eps, f)}{p_T(1)}\Big)  \nonumber \\
&\qquad - R_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2 \nonumber\\
&\geq \Big(\sigma - \frac{U_T(\eps, f)}{p_T(1)}\Big)\big(R_1^2 - R_2(L\delta + 2\eps)\big) - R_2^2\Big(1 - \sigma + \frac{U_T(\eps, f)}{p_T(1)}\Big) \nonumber\\
&\qquad - R_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2 \nonumber\\
&= R_2^2\Gamma_1(\sigma, \delta, \eps, f) - R_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2,
\end{align}
where $\Gamma_1(\sigma, \delta, \eps, f)$ is defined as
\begin{align*}
    \Gamma_1(\sigma, \delta, \eps, f) = \begin{cases}
    (2\sigma - 1) - \frac{U_T(\eps, f)}{p_T(1)} - \Big(\sigma - \frac{U_T(\eps, f)}{p_T(1)}\Big)\Big(\frac{L\delta}{R_2} + \frac{2\eps}{R_2}\Big), & R_1 = R_2 = R \\
    \big(\sigma - \frac{U_T(\eps, f)}{p_T(1)}\big)\big(1 + \big(\frac{R_2}{R_1}\big)^2 - \frac{L\delta}{R_2} - \frac{2\eps}{R_2}\big) - 1. & R_1 < R_2    
\end{cases}
\end{align*}

As for the term $f(X_T)^\top\hat{\mu}_T(k)$ in~\eqref{eq: sufficient condition to be classified as 1}, we note that similar deduction process as above can also turns out $f(X_T)^{\top}\mu_T(1) \geq R_2^2\Gamma_1(\sigma, \delta, \eps, f)$, along with the fact: any $1 \leq k \leq K$, $\Vert\mu_T(k)\Vert_2 = \Vert\E_{(X_T, Y)\sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\{f(\mathtt{X}_T)\vert Y = k\}\Vert_2 \leq \E_{(X_T, Y)\sim \P_T}\E_{\mathtt{X}_T \sim \mathcal{A}(X_T)}\{\Vert f(\mathtt{X}_T)\Vert_2 \vert Y = k\}\leq R_2$, we have
\begin{align}\label{eq: f(x)μk ≤ Γ}
&f(X_T)^\top\hat{\mu}_T(k) \leq f(X_T)^{\top}\mu_T(k) + f(X_T)^{\top}(\hat{\mu}_T(k) - \mu_T(k)) \nonumber \\
&\leq f(X_T)^{\top}\mu_T(k) + \norm{f(X_T)}_2\norm{\hat{\mu}_T(k) - \mu_T(k)}_2 \nonumber\\
&\leq f(X_T)^{\top}\mu_T(k) + R_2\norm{\hat{\mu}_T(k) - \mu_T(k)}_2 \nonumber \\
&= (f(X_T) - \mu_T(1))^{\top}\mu_T(k) + \mu_T(1)^\top\mu_T(k) + R_2\norm{\hat{\mu}_T(k) - \mu_T(k)}_2 \nonumber\\
&\leq \norm{f(X_T) - \mu_T(1)}_2 \cdot \norm{\mu_T(k)}_2 + \mu_T(1)^\top\mu_T(k) + R_2\norm{\hat{\mu}_T(k) - \mu_T(k)}_2 \nonumber\\
&\leq R_2\sqrt{\norm{f(X_T)}^2_2 - 2f(X_T)^{\top}\mu_T(1) + \norm{\mu_T(1)}_2^2} + \mu_T(1)^\top\mu_T(k) + R_2\norm{\hat{\mu}_T(k) - \mu_T(k)}_2 \nonumber\\
&\leq R_2\sqrt{2R_2^2 - 2f(X_T)^{\top}\mu_T(1)} + \mu_T(1)^\top\mu_T(k) + R_2\norm{\hat{\mu}_T(k) - \mu_T(k)}_2 \nonumber\\
&\leq R_2\sqrt{2R_2^2 - 2R_2^2\Gamma_1(\sigma, \delta, \eps, f)} + \mu_T(1)^\top\mu_T(k) + R_2\norm{\hat{\mu}_T(k) - \mu_T(k)}_2 \nonumber\\
&= \sqrt{2}R_2^2\sqrt{1 - \Gamma_1(\sigma, \delta, \eps, f)} + \mu_T(1)^\top\mu_T(k) + R_2\norm{\hat{\mu}_T(k) - \mu_T(k)}_2.
\end{align}
Plugging~\eqref{eq: f(x)μ1 ≥ Γ} and~\eqref{eq: f(x)μk ≤ Γ} into~\eqref{eq: sufficient condition to be classified as 1} concludes
\begin{align*}
&f(X_T)^{\top}\hat{\mu}_T(1) - f(X_T)^{\top}\hat{\mu}_T(k) - \Big(\frac{1}{2}\norm{\hat{\mu}_T(1)}_2^2 - \frac{1}{2}\norm{\hat{\mu}_T(k)}^2\Big) \\
&= f(X_T)^{\top}\hat{\mu}_T(1) - f(X_T)^{\top}\hat{\mu}_T(k) - \frac{1}{2}\norm{\hat{\mu}_T(1)}^2_2 + \frac{1}{2}\norm{\hat{\mu}_T(k)}^2_2 \\
&\geq f(X_T)^{\top}\hat{\mu}_T(1) - f(X_T)^{\top}\hat{\mu}_T(k) - \frac{1}{2}R_2^2 + \frac{1}{2}\min_{k}\norm{\hat{\mu}_T(k)}_2^2 \\
&= f(X_T)^{\top}\hat{\mu}_T(1) - f(X_T)^{\top}\hat{\mu}_T(k) - \frac{1}{2}R_2^2\big(1 - \min_{k}\norm{\hat{\mu}_T(k)}_2^2 / R_2^2\big) \\
&\geq R_2^2\Gamma_1(\sigma, \delta, \eps, f) - R_2\norm{\hat{\mu}_T(1) - \mu_T(1)}_2 - \sqrt{2}R_2^2\sqrt{1 - \Gamma_1(\sigma, \delta, \eps, f)} \\
&\qquad - \mu_T(1)^\top\mu_T(k) - R_2\norm{\hat{\mu}_T(k) - \mu_T(k)}_2 - \frac{1}{2}R_2^2\big(1 - \min_{k}\norm{\hat{\mu}_T(k)}_2^2 / R_2^2\big) > 0,
\end{align*}
where the last inequality follows from the condition provided in Lemma~\ref{lemma: sufficient condition of small Err}.
\end{proof}

% \begin{lemma}
% \label{lemma: difference between mu_s and mu_t}
% If the encoder $f$ is $L$-Lipschitz and Assumption \ref{assumption: distributions shift} holds, then for any $k$,
% \begin{align*}
% \norm{\mu_s(k) - \mu_T(k)}_2 \lesssim \epsilon_1.
% \end{align*}
% \end{lemma}

% \begin{proof}
% For all $k$,
% \begin{align*}
% \norm{\mu_s(k) - \mu_T(k)}_2^2 &= \sum_{l=1}^{d^*}\big((\mu_s(k))_l - (\mu_T(k))_l\big)^2 \\
% &= \sum_{l=1}^{d^*}(\E_{X_S \in C_S(k)}\E_{X_S^\prime \sim \mathcal{A}(X_S)}[f_l(X_S^\prime)] - \E_{\mathtt{X}_T \in C_t(k)}\E_{\mathtt{X}_T\sim \mathcal{A}(X_T)}[f_l(\mathtt{X}_T)])^2 \\
% &= \sum_{l=1}^{d^*}\big[\frac{1}{m}\sum_{i=1}^m\big(\E_{X_S \in C_S(k)}[f_l(A_i(X_S))] - \E_{\mathtt{X}_T \in C_t(k)}[f_l(A_i(X_T))]\big)\big]^2 \\
% &\lesssim \epsilon_1^2
% \end{align*}
% The final inequality is obtained by Assumption \ref{assumption: distributions shift} along with the fact that $f(A_i(\cdot))$ is $ML$-Lipschitz continuous. In fact, as $f \in \mathrm{Lip}(L)$, then for every $l \in [d^*], f_l \in \mathrm{Lip}(L)$, combining the property that $A_i(\cdot) \in \mathrm{Lip}(M)$, we can turn out $f(A_i(\cdot))$ is $ML$-Lipschitz continuous.
% \end{proof}

\begin{lemma}
\label{lemma: the effect of minimaxing our loss}
Given a $(\sigma, \delta)$-augmentation, if the encoder $f$ with $R_1 \leq \norm{f}_2 \leq R_2$ is $L$-Lipschitz continuous, then for any $\eps > 0$,
\begin{align}\label{eq: U_T < L}
U_T^2(\eps, f) \lesssim \eps^{-2}\big\{\mathcal{L}_{\mathrm{align}}(f) + \epsilon_1 + \epsilon_2\big\},
\end{align}
and
\begin{align}\label{eq: downstream divergence ≤ W + ɛ}
\max_{i \neq j}\mu_T(i)^\top\mu_T(j) \lesssim \mathcal{W}(\P_f, \P_T) + \epsilon_1.
\end{align}
\end{lemma}

\begin{proof}
The inequality in~\eqref{eq: downstream divergence ≤ W + ɛ} has been established according to~\eqref{eq : downstream divergence ≤ L + ɛ}. Therefore, we will focus on proving~\eqref{eq: U_T < L} in this lemma. Since the distribution on $\mathcal{A}$ is uniform distribution, we have
\begin{align*}
    \E_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\big\Vert f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})\big\Vert_2 = \frac{1}{M^2}\sum_{i = 1}^M\sum_{j = 1}^M\big\Vert f\big(A_i(X_T)\big) - f\big(A_j(X_T)\big)\big\Vert_2.
\end{align*}
Hence,
\begin{align*}
    \sup_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\big\Vert f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})\big\Vert_2 &= \sup_{i, j}\big\Vert f\big(A_i(X_T)\big) - f\big(A_j(X_T)\big)\big\Vert_2 \\
    &\leq \sum_{i = 1}^M\sum_{j = 1}^M\big\Vert f\big(A_i(X_T)\big) - f\big(A_j(X_T)\big)\big\Vert_2 \\
    &= M^2\E_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\norm{f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})}_2,
\end{align*}
which implies that
\begin{align*}
    \Big\{X_T: \sup_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\norm{f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})}_2 > \eps\Big\} \subseteq \Big\{X_T:\mathop{\E}_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\norm{f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})}_2 > \frac{\eps}{M^2}\Big\}.
\end{align*}
Recall the definition $U_T(\eps, f) = \P_T\{X_T: \sup_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\norm{f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})}_2 > \eps\big\}$, by Markov inequality, we know that
\begin{align}
\label{eq : U ≤ alignment}
U^2_T(\eps, f) &\leq \P^2_T\Big(\E_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\big\Vert f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})\big\Vert_2 > \frac{\eps}{M^2}\Big) \nonumber \\
&\leq \Big(\frac{\E_{(X_T, Y) \sim \P_T}\E_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\big\Vert f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})\big\Vert_2}{\frac{\eps}{M^2}}\Big)^2 \nonumber \\
&\leq \frac{\E_{(X_T, Y) \sim \P_T}\E_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\big\Vert f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})\big\Vert^2_2}{\frac{\eps^2}{M^4}} \nonumber \\
&\lesssim \eps^{-2}\E_{(X_T, Y) \sim \P_T}\E_{\mathtt{X}_{T,1}, \mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\big\Vert f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})\big\Vert^2_2.
\end{align}
Moreover,
\begin{align}
\label{eq: downstream alignment = upstream alignment + radius}
&\mathop{\E}_{(X_T, Y) \sim \P_T}\mathop{\E}_{\mathtt{X}_{T,1},\mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\big\Vert f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})\big\Vert_2^2 \nonumber\\
&=  \mathop{\E}_{X_S \sim \P_S}\mathop{\E}_{\mathtt{X}_{S,1},\mathtt{X}_{S,2} \sim \mathcal{A}(X_S)}\big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2})\big\Vert_2^2 + \mathop{\E}_{(X_T, Y) \sim \P_T}\mathop{\E}_{\mathtt{X}_{T,1},\mathtt{X}_{T,2} \sim \mathcal{A}(X_T)}\big\Vert f(\mathtt{X}_{T,1}) - f(\mathtt{X}_{T,2})\big\Vert_2^2 \nonumber \\
&\qquad - \mathop{\E}_{X_S \sim \P_S}\mathop{\E}_{\mathtt{X}_{S,1},\mathtt{X}_{S,2} \sim \mathcal{A}(X_S)}\big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2})\big\Vert_2^2 \nonumber \\ 
&= \mathop{\E}_{X_S \sim \P_S}\mathop{\E}_{\mathtt{X}_{S,1},\mathtt{X}_{S,2} \sim \mathcal{A}(X_S)}\big\Vert f(\mathtt{X}_{S,1}) - f(\mathtt{X}_{S,2})\big\Vert_2^2 + \frac{1}{M^2}\sum_{i,j}\Big\{\mathop{\E}_{(X_T, Y) \sim \P_T}\big\Vert f\big(A_i(X_T)\big) - f\big(A_j(X_T)\big)\big\Vert_2^2  \nonumber\\
&\qquad - \mathop{\E}_{X_S \sim \P_S}\big\Vert f(A_i(X_S)) - f(A_j(X_S))\big\Vert_2^2\Big\} \nonumber \\
&=  \mathcal{L}_{\mathrm{align}}(f) + \frac{1}{M^2}\sum_{i=1}^M\sum_{j=1}^M\sum_{l=1}^{d^*}\Big[\E_{(X_T, Y) \sim \P_T}\big\{f_l(A_i(X_T)) - f_l(A_j(X_T))\big\}^2 \nonumber \\
&\qquad - \E_{X_S \sim \P_S}\big\{f_l\big(A_i(X_S)\big) - f_l\big(A_j(X_S)\big)\big\}^2\Big].
\end{align}
Since for all $i \in [m], j \in [m]$ and $l \in [d^*]$, we have
\begin{align}
\label{eq: radius ≤ ɛ}
&\E_{(X_T, Y) \sim \P_T}\big\{f_l\big(A_i(X_T)\big) - f_l\big(A_j(X_T)\big)\big\}^2 - \E_{X_S \sim \P_S}\big\{f_l\big(A_i(X_S)\big) - f_l\big(A_j(X_S)\big)\big\}^2 \nonumber \\
&= \sum_{k=1}^{K}\Big[p_T(k)\mathop{\E}_{(X_T, Y) \sim \P_T}\big\{f_l(A_i(X_T)) - f_l(A_j(X_T))\vert Y = k\big\}^2 \nonumber \\
&\qquad -  p_S(k)\mathop{\E}_{X_S\sim \P_S}\big\{f_l\big(A_i(X_S)\big) - f_l\big(A_j(X_S)\big)\big\vert X_S \in C_S(k)\big\}^2\Big] \nonumber \\
&= \sum_{k=1}^{K}\Big[p_T(k)\Big\{\mathop{\E}_{(X_T, Y)\sim \P_T}\big\{f_l\big(A_i(X_T)\big) - f_l\big(A_j(X_T)\big)\big\vert Y = k\big\}^2 \nonumber \\ 
&\qquad - \mathop{\E}_{X_S \sim \P_S}\underbrace{\big\{f_l\big(A_i(X_S)\big) - f_l\big(A_j(X_S)\big)\big\vert X_S \in C_S(k)\big\}^2}_{h(X_S)}\Big\} \nonumber\\
&\qquad + \big\{p_T(k) - p_S(k)\big\}\mathop{\E}_{X_S\sim \P_S}\big\{f_l\big(A_i(X_S)\big) - f_l\big(A_j(X_S)\big)\big\vert X_S \in C_S(k)\big\}^2\Big] \nonumber \\
&\lesssim \epsilon_1 + \epsilon_2,
\end{align}
where the last inequality arises from $\epsilon_2 = \max_k\abs{p_S(k) - p_T(k)}$ and $\epsilon_1 = \max_k\mathcal{W}(\P_S(k), \P_T(k))$, $\epsilon_2 = \max_k\abs{p_S(k) - p_T(k)}$, along with the dual formulation of Mallows' distance~\eqref{eq: dual form of wasserstein distance}. In fact, since $f$ and any $A \in \mathcal{A}$ are Lipschitz continuous, and given that fact $R_1\leq \norm{f}_2 \leq R_2$, it follows that $h$ is also a Lipschitz function.

Combining~\eqref{eq : U ≤ alignment}~\eqref{eq: downstream alignment = upstream alignment + radius}~\eqref{eq: radius ≤ ɛ} yields $U_T^2(\eps, f) \lesssim \eps^{-2}\big(\mathcal{L}_{\mathrm{align}}(f) + \epsilon_1 + \epsilon_2\big)$.
\end{proof}
Next we represent Theorem~\ref{theorem: pop theorem} and give out its proof.
\begin{theorem}[General version of Theorem~\ref{theorem: pop theorem}] \label{theorem: general version of the pop theorem}
Given a $(\sigma, \delta)$-augmentation, if the encoder $f$ with $R_1 \leq \norm{f}_2 \leq R_2$ is $L$-Lipschitz and Assumption~\ref{assumption: Q* does not vanish},~\ref{assumption: Lip augmentation} both hold, then for any $\eps > 0$, $\max_{i \neq j}\mu_T(i)^\top\mu_T(j) \lesssim \mathcal{L}(f) + \epsilon_1$. Furthermore, if $\max_{i\neq j}\mu_T(i)^\top\mu_T(j) < R_2^2\psi(\sigma, \delta, \eps, f)$, then the downstream misclassification rate of $G_f$
\begin{align*}
\mathrm{Err}(G_f) \leq (1 - \sigma) + \mathcal{O}\Big(\eps^{-1}\big\{\mathcal{L}(f) + \epsilon_1 + \epsilon_2\big\}^{\frac{1}{2}}\Big),
\end{align*}
\end{theorem}
\begin{proof}
    Combining Lemma~\ref{lemma: sufficient condition of small Err} and Lemma~\ref{lemma: the effect of minimaxing our loss} yields this result. It is evident that Theorem~\ref{theorem: pop theorem} is a direct conclusion when setting $R_1 = R_2 = R$.
\end{proof}