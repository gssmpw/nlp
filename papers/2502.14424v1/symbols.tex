\section{Notation List}
Given the large number of symbols in this paper, consolidating them in this section offers readers a convenient reference. This structure reduces confusion and enhances comprehension by guiding readers to the first occurrence of each symbol in the relevant sections or equations.

\begin{longtable}{@{}ccc@{}}
    \toprule
    Symbol       & Description                           & Reference \\ \midrule
    \endfirsthead
    
    \toprule
    Symbol       & Description                           & Reference \\ \midrule
    \endhead
    
    \midrule
    \multicolumn{3}{r}{\textit{Continued on next page}} \\ 
    \endfoot
    
    \bottomrule
    \caption{Summary of Symbols}
    \endlastfoot

    $\mathcal{D}_S$ & source dataset                     &   Section~\ref{section: methodology}       \\
    $\mathcal{D}_T$ & target dataset                     &   Section~\ref{section: methodology}       \\
    $\widetilde{\mathcal{D}}_S$ & augmentation-reference dataset &  Section~\ref{subsection: DM}        \\
    $\widetilde{\mathcal{D}}_T$ & augmented target dataset  &    Section~\ref{subsection: transfer learning}      \\
    $C_S(k)$  & $k$-th source latent class            &   Section~\ref{subsection: pop theorem}      \\
    $C_T(k)$  & $k$-th target class                   &   Definition~\ref{def: (σ,δ)-augmentation}       \\
    $\widetilde{C}_T(k)$ & main part of $C_T(k)$   &      Definition~\ref{def: (σ,δ)-augmentation}    \\
    $C_k$     & $k$-th unlabeled reference part &  Definition~\ref{def: (σ,δ)-augmentation}      \\
    $C_\mathcal{R}(k)$ & $k$-th labeled reference part &   Section~\ref{subsection: pop theorem}    \\
    $n_T(k)$  & sample size of $k$-th target class  &     Equation~\eqref{eq: hatμT(k)}     \\
    $n_S$     & sample size of source dataset       &   Section~\ref{section: methodology}       \\
    $n_T$     & sample size of target dataset   &   Section~\ref{section: methodology}       \\
    $\P_S$    & source distribution        &  Section~\ref{section: methodology}        \\
    $\P_T$    & target distribution &    Section~\ref{section: methodology}      \\
    $\P_S(k)$ & distribution conditioned on $X^S \in C_S(k)$ &  Section~\ref{subsection: pop theorem}       \\
    $\P_T(k)$ & distribution conditioned on $Y = k$ &  Section~\ref{subsection: pop theorem}     \\
    $p_S(k)$  & probability of $X^S \in C^S_k$ & Section~\ref{subsection: pop theorem}          \\
    $p_T(k)$  & probability of $Y = k$ & Section~\ref{subsection: pop theorem}       \\
    $\mu_S(k)$ & center of $k$-th latent class & Section~\ref{subsection: pop theorem}       \\
    $\mu_T(k)$ & center of $k$-th target class &  Section~\ref{subsection: transfer learning}        \\
    $\mathcal{P}_i$ & random variable of $i$-th reference part &  Equation~\eqref{eq: P_i}  \\
    $\bm{c}_i$      & center of $i$-th reference part &   Section~\ref{subsection: DM} \\
    $\epsilon$      & range of reference part   &  Equation~\eqref{eq: P_i} \\
    $\epsilon_1, \epsilon_2$  & distribution shift &   Equation~\eqref{eq: distribution shift} \\
    $K^\prime$      & number of reference parts    &   Section~\ref{subsection: DM}       \\
    $K$             & the number downstream classes  &    Section~\ref{section: methodology}   \\
    $\P_\mathcal{R}$  & reference distribution     &   Section~\ref{subsection: DM}       \\
    $\P_f$           & representation distribution &    Section~\ref{subsection: DM}      \\
    $\mathcal{R}$   &  random vector of reference   &    Section~\ref{subsection: DM}      \\
    $R$             & range constraint of encoder  &   Equation~\eqref{eq: definition of F} \\
    $\mathcal{F}$   & feasible set of encoder     &   Equation~\eqref{eq: definition of F}       \\
    $\mathcal{G}$   & feasible set of critic &  Equation~\eqref{eq: risk at pop level (specific version)}      \\
    $\widehat{\mathcal{F}}$ & space for approximating $\mathcal{F}$ &  Equation~\eqref{eq: (hatf, hatg) ∈ argminmaxhatL(f,g)}        \\
    $\widehat{\mathcal{G}}$ & space for approximating $\mathcal{G}$ &  Equation~\eqref{eq: (hatf, hatg) ∈ argminmaxhatL(f,g)}        \\
    $f^*$           & population optimal encoder  &  Equation~\eqref{eq: risk at pop level}       \\
    $\hat{f}_{n_S}$ & empirical optimal encoder  &   Equation~\eqref{eq: (hatf, hatg) ∈ argminmaxhatL(f,g)}      \\
    $L$             & Lipschitz constant of encoder &  Equation~\eqref{eq: definition of F}  \\
    $Q$             & Lipschitz constant of augmentations &  Assumption~\ref{assumption: Lip augmentation} \\
    $M$             & number of augmentations &  Section~\ref{subsection: self-supervised contrastive learning} \\
    $\mathcal{W}(\P_f, \P_\mathcal{R})$  & Mallows' distance between $\P_f,\P_\mathcal{R}$ & Equation~\eqref{eq: risk at pop level}         \\
    $\mathcal{W}(f,g)$ & $\E_{Z\sim \P_f}\{g(Z)\} - \E_{\mathcal{R}\sim \P_\mathcal{R}} \{g(\mathcal{R})\}$ & Equation~\eqref{eq: hatW(f,g)}        \\
    $\mathcal{L}(f)$ & $\mathcal{L}_{\mathrm{align}}(f) + \lambda\cdot\mathcal{W}(\P_f, \P_\mathcal{R})$ &    Equation~\eqref{eq: risk at pop level}      \\
    $\mathcal{L}(f,g)$ & $\mathcal{L}_{\mathrm{align}}(f) + \lambda\cdot\mathcal{W}(f, g)$ &         Equation~\eqref{eq: mini-max} \\
\end{longtable}
