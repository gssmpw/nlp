\section{Related Works}
\textbf{In-Context Learning.} Since its introduction by \citet{ref:brown2020language}, ICL in LM has been studied extensively in various directions. For example, \citet{ref:reynolds2021prompt,ref:yoo2022ground} analyzed the role of prompts in improving the ICL performance. Theoretical analysis of how LMs perform ICL has been proposed by \citet{ref:akyurek2022learning,ref:dai2023can,ref:von2023transformers,ref:sander2024transformers}. These works study the internal mechanism -- either with regularized linear regression or gradient descent -- of the transformer architecture, which is the workhorse behind most current state-of-the-art LMs. 

\textbf{Language model intervention.} Intervening on the hidden states of transformer-based LMs, or activations editing, has recently emerged as an efficient method for controllable text generation. Contrasting to weights editing, activations editing refers to modifying the output of attention heads on one or several layer(s) of the transformer architecture, ultimately steering the generated text to desirable outcomes. Initially proposed to perform text style transfer, this method has been extended to improve the performance of few shots / zero shots of ICL, such as in \citet{ref:todd2023function,ref:liu2023context,ref:hendel2023incontext,ref:li2024context,ref:hernandez2024linearity}. Our work follows this direction but improved upon them by using only a fewer number of prompt inputs. As such, the aforementioned works, most notably by \citet{ref:todd2023function}, are directly related to our work.