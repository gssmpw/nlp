[
  {
    "index": 0,
    "papers": [
      {
        "key": "ref:brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ref:reynolds2021prompt",
        "author": "Reynolds, Laria and McDonell, Kyle",
        "title": "Prompt programming for large language models: Beyond the few-shot paradigm"
      },
      {
        "key": "ref:yoo2022ground",
        "author": "Yoo, Kang Min and Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-Goo and Kim, Taeuk",
        "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ref:akyurek2022learning",
        "author": "Aky{\\\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny",
        "title": "What learning algorithm is in-context learning? {I}nvestigations with linear models"
      },
      {
        "key": "ref:dai2023can",
        "author": "Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu",
        "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"
      },
      {
        "key": "ref:von2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      },
      {
        "key": "ref:sander2024transformers",
        "author": "Sander, Michael E and Giryes, Raja and Suzuki, Taiji and Blondel, Mathieu and Peyr{\\'e}, Gabriel",
        "title": "How do Transformers perform In-Context Autoregressive Learning?"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ref:todd2023function",
        "author": "Todd, Eric and Li, Millicent L and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C and Bau, David",
        "title": "Function vectors in large language models"
      },
      {
        "key": "ref:liu2023context",
        "author": "Liu, Sheng and Xing, Lei and Zou, James",
        "title": "In-context vectors: Making in context learning more effective and controllable through latent space steering"
      },
      {
        "key": "ref:hendel2023incontext",
        "author": "Hendel, Roee  and\nGeva, Mor  and\nGloberson, Amir",
        "title": "In-Context Learning Creates Task Vectors"
      },
      {
        "key": "ref:li2024context",
        "author": "Li, Dongfang and Liu, Zhenyu and Hu, Xinshuo and Sun, Zetian and Hu, Baotian and Zhang, Min",
        "title": "In-Context Learning State Vector with Inner and Momentum Optimization"
      },
      {
        "key": "ref:hernandez2024linearity",
        "author": "Evan Hernandez and Arnab Sen Sharma and Tal Haklay and Kevin Meng and Martin Wattenberg and Jacob Andreas and Yonatan Belinkov and David Bau",
        "title": "Linearity of Relation Decoding in Transformer Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ref:todd2023function",
        "author": "Todd, Eric and Li, Millicent L and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C and Bau, David",
        "title": "Function vectors in large language models"
      }
    ]
  }
]