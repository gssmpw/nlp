
@article{ref:team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}


@article{ref:hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{ref:lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{ref:liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@Misc{ref:mangrulkar2022peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}

@article{ref:dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{nguyen2025risk,
  title={Risk-Aware Distributional Intervention Policies for Language Models},
  author={Nguyen, Bao and Nguyen, Binh and Nguyen, Duy and Nguyen, Viet Anh},
  journal={arXiv preprint arXiv:2501.15758},
  year={2025}
}
@misc{jiang2025probefreelowrankactivationintervention,
      title={Probe-Free Low-Rank Activation Intervention}, 
      author={Chonghe Jiang and Bao Nguyen and Anthony Man-Cho So and Viet Anh Nguyen},
      year={2025},
      eprint={2502.04043},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.04043}, 
}
@article{ref:turner2023activation,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv preprint arXiv:2308.10248},
  year={2023}
}


@article{ref:li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ref:ghandeharioun2024patchscope,
  title={Patchscope: A unifying framework for inspecting hidden representations of language models},
  author={Ghandeharioun, Asma and Caciularu, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor},
  journal={arXiv preprint arXiv:2401.06102},
  year={2024}
}

@article{ref:chen2024selfie,
  title={Selfie: Self-interpretation of large language model embeddings},
  author={Chen, Haozhe and Vondrick, Carl and Mao, Chengzhi},
  journal={arXiv preprint arXiv:2403.10949},
  year={2024}
}

@article{ref:hernandez2023inspecting,
  title={Inspecting and editing knowledge representations in language models},
  author={Hernandez, Evan and Li, Belinda Z and Andreas, Jacob},
  journal={arXiv preprint arXiv:2304.00740},
  year={2023}
}


@inproceedings{ref:von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}


@inproceedings{ref:dai2023can,
  title={Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={4005--4019},
  year={2023}
}


@article{ref:bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}


@inproceedings{ref:akyurek2022learning,
  title={What learning algorithm is in-context learning? {I}nvestigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@inproceedings{ref:yoo2022ground,
  title={Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations},
  author={Yoo, Kang Min and Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-Goo and Kim, Taeuk},
  booktitle={2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022},
  pages={2422--2437},
  year={2022},
  organization={Association for Computational Linguistics (ACL)}
}


@inproceedings{ref:reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}


@article{ref:subramani2022extracting,
  title={Extracting latent steering vectors from pretrained language models},
  author={Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E},
  journal={arXiv preprint arXiv:2205.05124},
  year={2022}
}

@article{ref:hernandez2023measuring,
  title={Measuring and manipulating knowledge representations in language models},
  author={Hernandez, Evan and Li, Belinda Z and Andreas, Jacob},
  journal={arXiv preprint arXiv:2304.00740},
  year={2023}
}



@article{ref:zhang2023survey,
  title={A survey of controllable text generation using transformer-based pre-trained language models},
  author={Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
  journal={ACM Computing Surveys},
  volume={56},
  number={3},
  pages={1--37},
  year={2023},
  publisher={ACM New York, NY}
}

@article{ref:anthropic2024claude,
  title={The {C}laude 3 model family: Opus, {S}onnet, {H}aiku},
  author={Anthropic, AI},
  journal={Claude-3 Model Card},
  year={2024}
}


@article{ref:abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}



@article{ref:jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@article{ref:touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@article{ref:achiam2023gpt,
  title={GPT-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@article{ref:radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{ref:von2023uncovering,
  title={Uncovering mesa-optimization algorithms in transformers},
  author={von Oswald, Johannes and Niklasson, Eyvind and Schlegel, Maximilian and Kobayashi, Seijin and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and Pascanu, Razvan and others},
  journal={arXiv preprint arXiv:2309.05858},
  year={2023}
}

@article{ref:sander2024transformers,
  title={How do Transformers perform In-Context Autoregressive Learning?},
  author={Sander, Michael E and Giryes, Raja and Suzuki, Taiji and Blondel, Mathieu and Peyr{\'e}, Gabriel},
  journal={arXiv preprint arXiv:2402.05787},
  year={2024}
}

@inproceedings{ref:wang2023label,
  title={Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning},
  author={Wang, Lean and Li, Lei and Dai, Damai and Chen, Deli and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9840--9855},
  year={2023}
}


@article{ref:todd2023function,
  title={Function vectors in large language models},
  author={Todd, Eric and Li, Millicent L and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C and Bau, David},
  journal={arXiv preprint arXiv:2310.15213},
  year={2023}
}


@inproceedings{ref:shin2020autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    pages = "4222--4235",
}


@inproceedings{ref:wen2023hard,
 author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {51008--51025},
 publisher = {Curran Associates, Inc.},
 title = {Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery},
 volume = {36},
 year = {2023}
}

@article{ref:zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@inproceedings{ref:hendel2023incontext,
    title = "In-Context Learning Creates Task Vectors",
    author = "Hendel, Roee  and
      Geva, Mor  and
      Globerson, Amir",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pages = "9318--9333",
}


@article{ref:thompson2024fluent,
  title={Fluent dreaming for language models},
  author={Thompson, T Ben and Straznickas, Zygimantas and Sklar, Michael},
  journal={arXiv preprint arXiv:2402.01702},
  year={2024}
}


@article{ref:takemoto2024all,
  title={All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks},
  author={Takemoto, Kazuhiro},
  journal={Applied Sciences},
  volume={14},
  number={9},
  pages={3558},
  year={2024},
  publisher={MDPI}
}


@article{ref:fan2024not,
  title={Not all layers of {LLM}s are necessary during inference},
  author={Fan, Siqi and Jiang, Xin and Li, Xiang and Meng, Xuying and Han, Peng and Shang, Shuo and Sun, Aixin and Wang, Yequan and Wang, Zhongyuan},
  journal={arXiv preprint arXiv:2403.02181},
  year={2024}
}

@inproceedings{ref:hernandez2024linearity,
    title={Linearity of Relation Decoding in Transformer Language Models}, 
    author={Evan Hernandez and Arnab Sen Sharma and Tal Haklay and Kevin Meng and Martin Wattenberg and Jacob Andreas and Yonatan Belinkov and David Bau},
    booktitle={Proceedings of the 2024 International Conference on Learning Representations},
    year={2024},
}


@article{ref:jorgensen2023improving,
  title={Improving activation steering in language models with mean-centring},
  author={Jorgensen, Ole and Cope, Dylan and Schoots, Nandi and Shanahan, Murray},
  journal={arXiv preprint arXiv:2312.03813},
  year={2023}
}

@article{ref:li2024context,
  title={In-Context Learning State Vector with Inner and Momentum Optimization},
  author={Li, Dongfang and Liu, Zhenyu and Hu, Xinshuo and Sun, Zetian and Hu, Baotian and Zhang, Min},
  journal={arXiv preprint arXiv:2404.11225},
  year={2024}
}

@article{ref:bai2024analyzing,
  title={Analyzing Task-Encoding Tokens in Large Language Models},
  author={Bai, Yu and Huang, Heyan and Piano, Cesare Spinoso-Di and Rondeau, Marc-Antoine and Chen, Sanxing and Gao, Yang and Cheung, Jackie Chi Kit},
  journal={arXiv preprint arXiv:2401.11323},
  year={2024}
}


@article{ref:liu2023context,
  title={In-context vectors: Making in context learning more effective and controllable through latent space steering},
  author={Liu, Sheng and Xing, Lei and Zou, James},
  journal={arXiv preprint arXiv:2311.06668},
  year={2023}
}

@article{ref:brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{ref:ferrando2024primer,
  title={A primer on the inner workings of transformer-based language models},
  author={Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\`a}, Marta R},
  journal={arXiv preprint arXiv:2405.00208},
  year={2024}
}

@article{ref:santurkar2023whose,
    title={Whose Opinions Do Language Models Reflect?},
    author={Shibani Santurkar and Esin Durmus and Faisal Ladhak and Cinoo Lee and Percy Liang and Tatsunori Hashimoto},
    year={2023},
    journal={arXiv preprint arXiv:2303.17548},
}

@article{ref:zhao2023group,
  title={Group preference optimization: Few-shot alignment of large language models},
  author={Zhao, Siyan and Dang, John and Grover, Aditya},
  journal={arXiv preprint arXiv:2310.11523},
  year={2023}
}



@article{shefi2016rate,
  title={On the rate of convergence of the proximal alternating linearized minimization algorithm for convex problems},
  author={Shefi, Ron and Teboulle, Marc},
  journal={EURO Journal on Computational Optimization},
  volume={4},
  pages={27--46},
  year={2016},
  publisher={Springer}
}


@article{yu2013decomposing,
  title={On decomposing the proximal map},
  author={Yu, Yao-Liang},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{shi2016primer,
  title={A primer on coordinate descent algorithms},
  author={Shi, Hao-Jun Michael and Tu, Shenyinying and Xu, Yangyang and Yin, Wotao},
  journal={arXiv preprint arXiv:1610.00040},
  year={2016}
}

@article{zhang2020efficient,
  title={An efficient Hessian based algorithm for solving large-scale sparse group Lasso problems},
  author={Zhang, Yangjing and Zhang, Ning and Sun, Defeng and Toh, Kim-Chuan},
  journal={Mathematical Programming},
  volume={179},
  pages={223--263},
  year={2020},
  publisher={Springer}
}
@article{xu2017globally,
  title={A globally convergent algorithm for nonconvex optimization based on block coordinate update},
  author={Xu, Yangyang and Yin, Wotao},
  journal={Journal of Scientific Computing},
  volume={72},
  number={2},
  pages={700--734},
  year={2017},
  publisher={Springer}
}
