@inproceedings{ref:akyurek2022learning,
  title={What learning algorithm is in-context learning? {I}nvestigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{ref:brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{ref:dai2023can,
  title={Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={4005--4019},
  year={2023}
}

@inproceedings{ref:hendel2023incontext,
    title = "In-Context Learning Creates Task Vectors",
    author = "Hendel, Roee  and
      Geva, Mor  and
      Globerson, Amir",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pages = "9318--9333",
}

@inproceedings{ref:hernandez2024linearity,
    title={Linearity of Relation Decoding in Transformer Language Models}, 
    author={Evan Hernandez and Arnab Sen Sharma and Tal Haklay and Kevin Meng and Martin Wattenberg and Jacob Andreas and Yonatan Belinkov and David Bau},
    booktitle={Proceedings of the 2024 International Conference on Learning Representations},
    year={2024},
}

@article{ref:li2024context,
  title={In-Context Learning State Vector with Inner and Momentum Optimization},
  author={Li, Dongfang and Liu, Zhenyu and Hu, Xinshuo and Sun, Zetian and Hu, Baotian and Zhang, Min},
  journal={arXiv preprint arXiv:2404.11225},
  year={2024}
}

@article{ref:liu2023context,
  title={In-context vectors: Making in context learning more effective and controllable through latent space steering},
  author={Liu, Sheng and Xing, Lei and Zou, James},
  journal={arXiv preprint arXiv:2311.06668},
  year={2023}
}

@inproceedings{ref:reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}

@article{ref:sander2024transformers,
  title={How do Transformers perform In-Context Autoregressive Learning?},
  author={Sander, Michael E and Giryes, Raja and Suzuki, Taiji and Blondel, Mathieu and Peyr{\'e}, Gabriel},
  journal={arXiv preprint arXiv:2402.05787},
  year={2024}
}

@article{ref:todd2023function,
  title={Function vectors in large language models},
  author={Todd, Eric and Li, Millicent L and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C and Bau, David},
  journal={arXiv preprint arXiv:2310.15213},
  year={2023}
}

@inproceedings{ref:von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@inproceedings{ref:yoo2022ground,
  title={Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations},
  author={Yoo, Kang Min and Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-Goo and Kim, Taeuk},
  booktitle={2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022},
  pages={2422--2437},
  year={2022},
  organization={Association for Computational Linguistics (ACL)}
}

