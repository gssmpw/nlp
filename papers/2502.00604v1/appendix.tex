
\section{Nomenclature}

\begin{table}[H]
    \renewcommand{\arraystretch}{1.4}
\centering
\caption{Notation used throughout the paper. We use uppercase letters for matrices and lowercase letters for their vectorized forms. All gradients and Hessians are with respect to the loss function $\mathcal{L}$.}
\label{tab:notation}
\begin{tabular}{c|l}
\toprule
\textbf{Symbol} & \textbf{Description} \\
\midrule
$\mathcal{L}$ & Loss function \\
$\theta$ & Neural network parameters \\
$W$ & Weight matrix for a given layer \\
$w$ & Vectorized weight matrix, $w = \text{Vec}(W)$ \\
$G$ & Gradient matrix, $G = \nabla_W \mathcal{L}$ \\
$g$ & Vectorized gradient, $g = \text{Vec}(G)$ \\
$H$ & Hessian matrix, $H = \nabla^2_\theta \mathcal{L}$ \\
$H_{\text{Ada}}$ & Full Adagrad preconditioner matrix \\
$H_{\text{GN}}$ & Gauss-Newton approximation of the Hessian \\
$\mathcal{A}$ & Gradient alignment score between loss components \\
\bottomrule
\end{tabular}

\end{table}


\section{Related Work} 
\label{sec: related_work}

Physics-informed neural networks (PINNs) have emerged as a powerful paradigm in scientific machine learning by incorporating physical principles through carefully designed loss functions. These loss functions act as soft constraints, guiding neural networks to learn solutions that respect underlying physical laws while simultaneously fitting experimental data. The elegance and versatility of PINNs have led to their widespread adoption in solving both forward and inverse problems involving partial differential equations (PDEs). Their success spans numerous domains in computational science, from fluid mechanics \cite{raissi2020hidden,almajid2022prediction,eivazi2022physics,cao2024surrogate}, heat transfer \cite{xu2023physics,bararnia2022application,gokhale2022physics} to bio-engineering \cite{kissas2020machine,zhang2023physics,caforio2024physics} and materials science \cite{zhang2022analyses,jeong2023physics, hu2024physics}. The impact of PINNs extends even further, with applications in electromagnetics \cite{kovacs2022conditional,khan2022physics,baldan2023physics}, geosciences \cite{smith2022hyposvi, song2023simulating,ren2024seismicnet}, etc.


Despite their broad applications, PINNs currently face limitations in convergence speed and accuracy that affect their reliability as forward PDE solvers. This has motivated extensive research efforts to enhance their performance through various methodological innovations.  One prominent line of research focuses on developing self-adaptive loss weighting schemes to address unbalanced back-propagated gradients during training \cite{wang2021understanding,wang2022and}. While various strategies have been proposed \cite{wang2023expert,wang2021understanding,wang2022and,li2022revisiting,chen2024self,anagnostopoulos2024residual,liu2024discontinuity,song2025vw}, they primarily address gradient magnitude imbalances rather than directional conflicts, with a recent exception being \cite{liu2024config} whose effectiveness is hereby shown to be limited. Other advances include architectural innovations \cite{wang2021understanding,sitzmann2020implicit,fathony2021multiplicative,moseley2021finite,kang2022pixel,cho2024separable,wang2024piratenets}, improved training processes \cite{nabian2021efficient,daw2022rethinking,wu2023comprehensive,muller2023achieving,jnini2024gauss,song2024admm,urban2025unveiling,wight2020solving,krishnapriyan2021characterizing,cao2023tsonn}, and alternative learning objectives \cite{chiu2022can,huang2024efficient,kharazmi2021hp,patel2022thermodynamically,yu2022gradient,son2021sobolev}. However, the fundamental challenge of resolving directional gradient conflicts remains largely unaddressed.




\section{Connection between SOAP and Newton's method}  \label{app:soap_newton}

For our purpose, we begin by noting that there exists a one-to-one correspondence between the original parameter space and the rotated space that preserves the matrix-vector multiplication. 
% This relationship is formalized in the following lemma:
\begin{lemma}
\label{lemma: equivalence}
Let $Q_L \in \R^{m \times m}$ and $Q_R \in \R^{n \times n}$ be two orthogonal matrices. For any matrix $A \in \R^{mn \times mn}$ and vector $v \in \R^{m n}$, define $\widetilde{v} := (Q_L \otimes Q_R)v$ and $\widetilde{A} := (Q_L\otimes Q_R)A(Q_L^T \otimes Q_R^T)$. Then there holds 
\begin{equation*}
    \widetilde{A v} = (Q_L \otimes Q_R) A v = \widetilde{A} \widetilde{v}\,.
\end{equation*}
% the following equivalence holds:
% $Av = b$ if and only if $\widetilde{A}\widetilde{v} = \widetilde{b}$.
\end{lemma}
The proof follows directly by applying the transformation $Q_L \otimes Q_R$ to $A v$ and the definitions of $\widetilde{A}$ and $\widetilde{v}$. Building on the above lemma, one can easily transform the preconditioned gradient descent in the original space to the rotated one and vice versa. 



\begin{corollary} 
\label{corollary: rotated}
Let $W_t, G_t \in \R^{m \times n}$ be the weight matrix and gradient matrix for a layer at iteration $t$, respectively, with vectorizations $w_t = \Vec(W_t)$ and $g_t = \Vec(G_t)$. The preconditioned gradient descent update:
\begin{align}
w_{t+1} = w_t - \eta H^{-1}g_t\,,
\end{align}
is equivalent to performing preconditioning in the rotated space:
\begin{align}
\widetilde{w}_{t+1} = \widetilde{w}_t - \eta \widetilde{H}^{-1}\widetilde{g}_t\,,
\end{align}
where $H \in \R^{mn \times mn}$ is the preconditioner, and $\widetilde{w}$, $\widetilde{g}$, and $\widetilde{H}$ are the rotated weight, gradient, and preconditioner defined by the transformations in \cref{lemma: equivalence}. 
\end{corollary}

\begin{proposition}
\label{prop2}
Let $L_t = \mathbb{E}\left[G_tG_t^T\right]$ and $R_t = \mathbb{E}\left[G_t^T G_t\right]$ have eigendecompositions
$L_t = Q_L \Lambda_L Q_L^T$ and $R_t = Q_R \Lambda_R Q_R^T$. Under the assumption of \cref{lemma: exact_h_gn},
 the equivalent preconditioner in the rotated space is diagonal, i.e., $\widetilde{H}_{\text{GN}} = \operatorname{diag}(\widetilde{H}_{\text{GN}})$.
\end{proposition}

\begin{proof}
    The proof follows from the combination of Lemma \ref{lemma: equivalence} and Lemma \ref{lemma: exact_h_gn}. First, we express $H_{\text{GN}}$ using the Kronecker approximation:
\begin{align}
H_{\text{GN}} = L_t^{1/2} \otimes R_t^{1/2} / \operatorname{Tr}\left(\mathbb{E}\left[GG^T\right]\right).
\end{align}
Then, we derive the rotated preconditioner:
\begin{align*}
\widetilde{H}_{\text{GN}} &= \left(Q_L \otimes Q_R\right) H_{\text{GN}} \left(Q_L^T \otimes Q_R^T\right) \\
&= \left(Q_L \otimes Q_R\right) (L_t^{1/2} \otimes R_t^{1/2}) \left(Q_L^T \otimes Q_R^T\right) / \operatorname{Tr}\left(\mathbb{E}\left[GG^T\right]\right) \\
&= (Q_L L_t^{1/2} Q_L^T) \otimes (Q_R R_t^{1/2} Q_R^T) / \operatorname{Tr}\left(\mathbb{E}\left[\Lambda_L\right]\right) \\
&= \Lambda_L^{1/2} \otimes \Lambda_R^{1/2} / \operatorname{Tr}\left(\mathbb{E}\left[\Lambda_L\right]\right).
\end{align*}
The final expression shows that $\widetilde{H}_{\text{Ada}}$ is diagonal, as it is the Kronecker product of diagonal matrices scaled by a scalar factor.
\end{proof}



Finally, we connect our analysis to Adam's update rule by adapting the following result from Molybog et al \cite{molybog2023theory}: 
\begin{proposition}[Adapt from \cite{molybog2023theory}]
\label{prop:adam_hessian}
Suppose that $\theta^*$ is a local minimum and assume that $\theta - \theta^* \sim \mathcal{N}(0, \sigma^2 I)$. For Adam update rule denoted by $\theta_{t+1} = \theta_t - \eta \operatorname{Adam}(g_t)$, 
we have
\begin{align}
    \operatorname{Adam}(g_t) \approx \operatorname{diag}\left( H \right)^{-1} g_t.
\end{align}
\end{proposition}


\begin{proof}
The Adam optimizer follows the update rule:
\begin{align*}
&m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t, \\
&v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t \odot g_t ,\\
&\hat{m}_t = m_t/(1-\beta_1^t), \\
&\hat{v}t = v_t/(1-\beta_2^t), \\
&\theta_t = \theta{t-1} - \eta \hat{m}_t/(\sqrt{\hat{v}_t}+\epsilon).
\end{align*}
Taking a first-order Taylor expansion of the gradient around a local minimum $\theta^*$:
\begin{align*}
    g_\theta \approx g_{\theta^*}+   H_{\theta^*}\left(\theta-\theta^*\right) \approx    H_{\theta^*}\left( {\theta}- {\theta}^*\right).
\end{align*}
This yields
\begin{align*}
      {g}_\theta   {g}_{  {\theta}}^{\top} \approx    H_{\theta^*}\left(  {\theta}-  {\theta}^*\right)\left(  {\theta}-  {\theta}^*\right)^{\top}    H_{  {\theta}^*}^{\top}.
\end{align*}

Under our assumption that $\theta - \theta^* \sim \mathcal{N}(0, \sigma^2 I)$,
\begin{align*}
    \mathbb{E}\left[   {g}_\theta    {g}_{   {\theta}}^{\top}\right] \approx    H_{   {\theta}} \mathbb{E}\left[\left(   {\theta}-   {\theta}^*\right)\left(   {\theta}-   {\theta}^*\right)^{\top}\right]    H_{   {\theta}^*}^{\top}=\sigma^2    H_{\theta^*}    H_{   {\theta}^*}^{\top}.
\end{align*}

By construction, $v_t$ approximates the diagonal of $\mathbb{E}_{\theta \sim \theta\tau}[g_\theta g_\theta^T]$, where $\theta_\tau$ represents the distribution of model weights over the past $O(1/(1-\beta_2))$ steps:
\begin{align*}
    v_t \approx \operatorname{diag}\left(\mathbb{E}_{\theta \sim \theta_\tau}  \left[ g_\theta g_\theta^T \right] \right) \approx \sigma^2  \text{diag}(   H_{\theta^*}^2).
\end{align*}
Finally, assuming $m_t \approx g_t$:
\begin{align*}
    \operatorname{Adam}\left(g_t\right) \approx \frac{m_t}{\sqrt{v_t}+\varepsilon} \approx \frac{m_t}{\sqrt{v_t}}
    \approx \operatorname{diag}(H)^{-1} g_t.
\end{align*}
\end{proof}

\begin{theorem} Under Assumption \ref{assumption} and the conditions of Proposition \ref{prop:adam_hessian}, 
% \label{thm: soap}
\begin{align}
    w_{t+1} = w_{t} - \eta \operatorname{Soap}(g_t) \approx w_t - \eta H^{-1} g_t.
\end{align}
\end{theorem}
\begin{proof}
    Combining Propositions \ref{prop2} and \ref{prop:adam_hessian}, we obtain
\begin{align}
\operatorname{Adam}(\widetilde{G}_t) \approx \operatorname{diag}(\widetilde{H})^{-1} \widetilde{g}_t \approx \operatorname{diag}(\widetilde{H}_{\text{GN}})^{-1} \widetilde{g}_t = \widetilde{H}_{\text{GN}}^{-1} \widetilde{g}_t \approx \widetilde{H}^{-1} \widetilde{g}_t.
\end{align}
By  Corollary \ref{corollary: rotated}, this is equivalent to  the Newton update in the original space:
\begin{align}
w_{t+1} = w_t - H^{-1}g_t.
\end{align}
As a direction implication, the Hessian matrix is approximately diagonal in the rotated space.
\end{proof} 

% \subsection{Proof of \cref{thm: soap}}
% \begin{proof}
%     Combining  Lemma \ref{lemma: equivalence}, and Propositions \ref{prop2} and \ref{prop:adam_hessian}, we obtain
% \begin{align}
% \operatorname{Adam}(\widetilde{G}_t) \approx \operatorname{diag}(\widetilde{H})^{-1} \widetilde{g}_t \approx \operatorname{diag}(\widetilde{H}_{\text{GN}})^{-1} \widetilde{g}_t = H_{\text{GN}}^{-1} \widetilde{g}_t \approx \widetilde{H}^{-1} \widetilde{g}_t.
% \end{align}
% This is equivalent to  the Newton update in the original space:
% \begin{align}
% w_{t+1} = w_t - H^{-1}g_t.
% \end{align}
% As a corollary, the Hessian matrix is approximately diagonal in the rotated space.
% \end{proof}






\section{Analysis of Intra-step Gradient Alignment}  \label{sec:gradientconflict}
We present some preliminary analysis to understand intra-step gradient conflicts in training PINNs via standard gradient descent, Adam \cite{kingma2014adam}, and Shampoo algorithms \cite{gupta2018shampoo}, and how SOAP can effectively resolve them during training. For simplicity, we consider the simplest case of using PINNs with the two-layer NN to solve the one-dimensional Laplace equation and 
focus on the analysis of the intra-step gradient alignment \eqref{eq: intra_align} 
with small initialization. The analysis can be easily extended to other types of PDEs. 
Following the general setup in \cref{subsec: pinns}, without loss of generality, we consider 1D Laplace equation as follows
\begin{align}
    \left\{
\begin{array}{ll}
\Delta u = u'' = 0 & \text{on } [-1,1], \\
u(\pm 1) = g_{\pm 1}. & 
\end{array}
\right.
\end{align}
We approximate the solution $u(x)$ by a two-layer network with width $N$: 
\begin{equation} 
\label{eq:twolayersol}
    u(x, \theta) = \sum_{i = 1}^N a_i \si ( w_i x) = \ba \dd \si(\bw x)\,,
\end{equation}
where $\ba = (a_1, \ldots, a_N), \bw = (w_1, \ldots, w_N)  \in \R^N$, and $\theta = (\ba,\bw) \in \R^{2N}$. Moreover, we limit ourselves to the activation function $\si(x) = \tanh(x)$. In this case, the loss \eqref{eq: PINN_loss} reduces to 
\begin{equation} \label{eq:losseg}
    \min_{\theta = (\ba, \mathbf{w})} \mc{L}(\theta) = \underbrace{\frac{1}{N_r} \sum_{p = 1}^{N_r} |u''(x_p, \theta)|^2}_{\mc{L}_{r}(\theta)} + \underbrace{\frac{1}{2} \sum_{s = \pm 1} |u(s, \theta) - g_{s}|^2}_{\mc{L}_{bc}(\theta)}\,.
\end{equation}
To analyze the gradient conflict phenomenon in training PINNs, we consider the \emph{small initialization} regime. 
\begin{assumption} \label{assp1}
    The weights $a_i, w_i$ are initialized by i.i.d. Gaussian $\mc{N}(0,\ep^2)$ with small $\ep = o(1)$. 
\end{assumption}
This allows us to introduce the normalized parameters:  
\begin{equation*}
    \bar{\ba} = \ep^{-1} \ba\,, \q \bar{\bw} = \ep^{-1} \bw\,, 
\end{equation*}
initialized as standard Gaussian.

\begin{lemma} \label{lemma51}
Under small initialization, the gradients of the residual and boundary loss terms can be approximated as:
\begin{align}
\nabla_\theta \mathcal{L}_r(\theta) &= \ep^7 G^r(\bar{\mathbf{a}},\bar{\mathbf{w}}) + O(\ep^9), \\
\nabla\theta \mathcal{L}_{bc}(\theta) &= \ep G^{bc}(\bar{\mathbf{a}},\bar{\mathbf{w}}) + O(\ep^3),
\end{align}
where
\begin{align} \label{eq:effgrad1}
    G^r(\wb{\ba},\wb{\bw}) &=  c_r  \wb{\ba} \dd \wb{\bw}^{\odot 3} \left(  \wb{\bw}^{\odot 3},   3  \wb{\ba} \odot \wb{\bw}^{\odot 2}  \right), \\
  \label{eq:effgrad2}
    G^{bc}(\wb{\ba},\wb{\bw}) &= (g_{-1} - g_1) \left(\wb{\bw}, \wb{\ba} \right).
\end{align}
Here $\bar{\mathbf{a}} = \ep^{-1}\mathbf{a}$, $\bar{\mathbf{w}} = \ep^{-1}\mathbf{w}$ are the normalized parameters, and $G^r$, $G^{bc}$ are the effective gradient terms.
\end{lemma}
We remark that these elementary computations also provide insights into the gradient magnitude imbalance discussed in \cref{subsec:gradconflict}, noting $\|\na_\theta \mc{L}_r(\theta) \| = O(\ep^7)$ while $\| \na_{\theta} \mc{L}_{bc}(\theta) \| = O(\ep)$. 

\begin{proof}
    We recall the Taylor expansions of the activation function $\si(x) = \tanh(x)$ and its derivatives for later use:
\begin{equation} \label{eq:taylorexp}
    \begin{aligned}
         &\si(x) = x - \frac{x^3}{3} + O(x^5)\,, \quad \si'(x) = 1 - x^2 + O(x^4)\,, \\ & \si''(x) = - 2 x + \frac{8 x^3}{3} + O(x^5)\,, \quad \si'''(x) = -2 + 8 x^2 + O(x^4)\,.
    \end{aligned}
\end{equation}
The gradient of loss function $\mc{L}(\theta)$ consists of two parts computed as follows:
\begin{equation*}
    \na_{\theta = (\ba, \bw)} \mc{L}_r(\theta) = \frac{2}{N_r} \sum_{p = 1}^{N_r} u_{xx}(x_p, \theta) \na_{\theta = (\ba, \bw)} u_{xx}(x_p\,, \theta)\,,
\end{equation*}
and 
\begin{equation*}
    \na_{\theta = (\ba, \bw)} \mc{L}_{bc}(\theta) = \sum_{s = \pm 1} (u(s, \theta) - g(s)) \na_{\theta = (\ba, \bw)} u(s, \theta)\,,
\end{equation*}
with, thanks to \eqref{eq:twolayersol} and \eqref{eq:taylorexp}, 
\begin{align*}
     \na_{\ba}u(x,\theta) &= \si(\mathbf{w} \x) = \ep \wb{\bw} x + O(\ep^3)\,,\\
     \na_{w_i} u(x,\theta) &= a_i \si'(w_i x) x = \ep \wb{a_i} x - \ep^3 \wb{a_i} \wb{w_i}^2 x^3 + O(\ep^4)\,,
\end{align*}
and 
\begin{align*}
     \na_{a_i} u_{xx}(x, \theta) &= \si''(w_i x) |w_i|^2 = - 2 \ep^3 \wb{w_i}^3 x + O(\ep^5)\,, \\ 
     \na_{w_i} u_{xx}(\x, \theta) & = a_i \si'''(w_i x) |w_i|^2 x + 2 a_i \si''(w_i x) w_i \\
     & = - 6 \ep^3 \wb{a_i} \wb{w_i}^2 x + O(\ep^5) \,.
\end{align*}
We also compute 
\begin{align*}
    u(x,\theta) = \ep^2 \wb{\ba} \dd \wb{\bw} x + O(\ep^4)\,,
\end{align*}
and 
\begin{equation*}
    u_{xx}(x, \theta) = \sum_i a_i \si''(w_i x) |w_i|^2 = - 2 \ep^4\sum_i \wb{a_i} \wb{w_i}^3 x + O(\ep^6)\,.
\end{equation*}
For convenience, we define the componentwise power $\x^{\odot k} = (x_1^k,\ldots, x_N^k)$ and product $ \x \odot \mathbf{y} = (x_1y_1, \ldots, x_N y_N)$ for $\x, \mathbf{y} \in \R^N$.
By the above computation, it follows that at the initialization, there hold
\begin{align*}
      \na_{\theta = (\ba, \bw)} \mc{L}_r(\theta) & = \frac{2}{N_r} \sum_{p = 1}^{N_r} \left(- 2 \ep^4  \wb{\ba} \dd \wb{\bw}^{\odot 3} x_p \right) \left(- 2 \ep^3 \wb{\bw}^{\odot 3} x_p,   - 6 \ep^3 \wb{\ba} \odot \wb{\bw}^{\odot 2} x_p   \right) + O(\ep^9)  \\ 
      & = \ep^7 \frac{8}{N_r} \sum_{p = 1}^{N_r} \wb{\ba} \dd \wb{\bw}^{\odot 3} \left(  \wb{\bw}^{\odot 3},   3  \wb{\ba} \odot \wb{\bw}^{\odot 2}  \right) x_p^2 + O(\ep^9)\,,
\end{align*}
and 
\begin{align*}
        \na_{\theta = (\ba, \bw)} \mc{L}_{bc}(\theta) & = \sum_{s = \pm 1} \left(\ep^2 \wb{\ba} \dd \wb{\bw} s + O(\ep^4) - g(s)\right) \left(\ep \wb{\bw} s + O(\ep^3), \ep \wb{\ba} s - \ep^3 \wb{\ba} \odot \wb{\bw}^{\odot 2} s^3 + O(\ep^4) \right) \\
        & = - \ep \sum_{s = \pm 1} s g(s) \left(\wb{\bw}, \wb{\ba} \right) + O(\ep^3) =  \ep (g_{-1} - g_1) \left(\wb{\bw}, \wb{\ba} \right) + O(\ep^3)\,.
\end{align*}
We then define constant $c_r = 8 N_r^{-1} \sum_{p = 1}^{N_r} x_p^2 > 0$ and the effective gradients 
\begin{align} 
    G^r(\wb{\ba},\wb{\bw}) = \left(G^r_a(\wb{\ba},\wb{\bw}), G^r_w(\wb{\ba},\wb{\bw})\right) =  c_r  \wb{\ba} \dd \wb{\bw}^{\odot 3} \left(  \wb{\bw}^{\odot 3},   3  \wb{\ba} \odot \wb{\bw}^{\odot 2}  \right)\,,
\end{align}
and 
\begin{align}  
    G^{bc}(\wb{\ba},\wb{\bw}) = \left(G^{bc}_a(\wb{\ba},\wb{\bw}), G^{bc}_w(\wb{\ba},\wb{\bw})\right) = (g_{-1} - g_1) \left(\wb{\bw}, \wb{\ba} \right)\,,
\end{align}
enabling us to write 
\begin{equation} \label{eq:effecgrad}
    \na_\theta \mc{L}_r(\theta) = \ep^7 G^r(\wb{\ba},\wb{\bw}) + O(\ep^9)\,, \q \na_{\theta} \mc{L}_{bc}(\theta) = \ep G^{bc}(\wb{\ba},\wb{\bw}) + O(\ep^3)\,.
\end{equation}
\end{proof}



We are now ready to understand the gradient conflict for various optimizers applied to the residual and boundary loss terms separately. 
\begin{proposition} 
\label{prop: grad_align}
At initialization, the alignment score converges to a binary random variable in the infinite width limit:
\begin{align}
    \lim_{N \rightarrow \infty }\mathcal{A}(\square(\nabla \mathcal{L}_b),\square(\nabla \mathcal{L}_r)) = O(\ep^2) + C_\square \begin{cases}\operatorname{sgn}\left(g_{-1}-g_1\right) & \text { with prob. } \frac{1}{2}, \\ -\operatorname{sgn}\left(g_{-1}-g_1\right) & \text { with prob. } \frac{1}{2} .\end{cases}
\end{align}
where $\square = \operatorname{GD}, \operatorname{Adam}, \operatorname{Shampoo}, \text{ or } \operatorname{Soap}$  
 denotes the optimizer update rule, and
$C_\square$ is a constant depending on the optimizer. 
% Particularly, for plain gradient descent, $C_\operatorname{GD} = \frac{3}{\sqrt{21}}$, for Adam, Shampoo, and SOAP $C_{\operatorname{Adam}} = C_{\operatorname{Shamp}} = C_{\operatorname{Soap}} = 1$.
\end{proposition}

 We can see that these optimizers fail to resolve intra-step gradient conflicts in the initialization, aligning with the near-zero initial intra-step gradient scores shown in Figure \ref{fig:grad_align_score}. 


\begin{proof}
    \noindent 
{\bf Gradient descent.} We start with the standard continuous-time gradient descent: 
\begin{align} \label{eq:gradientflow}
    \frac{\rd \theta}{\rd t} = - \na_{\theta} \mc{L}(\theta)\,.
\end{align}
Motivated by \cite{zhou2022towards,chen2024dynamics}, under small initialization \cref{assp1}, in the \emph{initial stage} of training dynamics where the leading-order expansion \eqref{eq:effecgrad} holds for the weights $\ba, \bw$, the gradients $ \na_\theta \mc{L}_r(\theta)$ and $\na_{\theta} \mc{L}_{bc}(\theta)$ can be effectively described by  $G^r(\wb{\ba},\wb{\bw})$ and $G^{bc}(\wb{\ba},\wb{\bw})$, respectively, up to some scaling factors, then the gradient flow \eqref{eq:gradientflow} can be approximated by the effective dynamics for the normalized parameter $\wb{\theta} = (\wb{\ba},\wb{\bw})$: 
\begin{align} \label{eq:effdym}
    \ep \frac{\rd \wb{\theta}}{\rd t} = - \left(\ep^7 G^r(\wb{\theta}) + \ep G^{bc}(\wb{\theta}) \right)\,.    
\end{align}
Recalling \cref{def:gradalign}, under our assumptions, we have the intra alignment score:
\begin{align*}
    \mc{A}\left(\na_\theta \mc{L}_r(\theta), \na_\theta \mc{L}_{bc}(\theta) \right) = \mc{A}\left( G^r(\wb{\theta}) , G^{bc}(\wb{\theta})  \right) + O(\ep^2)\,,
\end{align*}
where 
\begin{align*}
    \mc{A}\left( G^r(\wb{\theta}) , G^{bc}(\wb{\theta})  \right) = \sgn (\wb{\ba} \dd \wb{\bw}^{\odot 3}) \sgn (g_{-1} - g_1) \frac{\sum_i \wb{w_i}^4 + 3 \sum_i \wb{a_i}^2 \wb{w_i}^2}{\sqrt{\sum_i \wb{w_i}^6 + 9 \wb{a_i}^2 \wb{w_i}^4} \sqrt{\sum_i \wb{w_i}^2 + \wb{a_i}^2}}\,.
\end{align*}
Then we find that at the initialization, by
$\wb{a_i}, \wb{w_i} \sim \mc{N}(0,1)$ from \cref{assp1} and the law of large numbers, 
\begin{equation*}
    \frac{\frac{1}{N}\sum_i \wb{w_i}^4 + 3 \frac{1}{N} \sum_i \wb{a_i}^2 \wb{w_i}^2}{\sqrt{\frac{1}{N}\sum_i \wb{w_i}^6 + 9 \wb{a_i}^2 \wb{w_i}^4} \sqrt{\frac{1}{N}\sum_i \wb{w_i}^2 + \wb{a_i}^2}} \longrightarrow \frac{6}{\sqrt{15 + 27} \sqrt{2}} = \frac{3}{\sqrt{21}}\,,\q \text{almost surely}.
\end{equation*}
Also, by the symmetry of Gaussian, there holds 
$\mathbb{P}(\sum_i \wb{a_i} \wb{w_i}^3 > 0) = \frac{1}{2}$. It follows that the alignment score $\mc{A}( G^r(\wb{\theta}) , G^{bc}(\wb{\theta}))_{t = 0}$ converges to a binary random variable with expectation zero in the infinite width limit: 
\begin{equation*}
    \mc{A}\left( G^r(\wb{\theta}) , G^{bc}(\wb{\theta})  \right)_{t = 0} \longrightarrow  A =  \begin{cases}
        \sgn (g_{-1} - g_1)  \frac{3}{\sqrt{21}}  & \text{with prob. $\frac{1}{2}$}\,, \\
       - \sgn (g_{-1} - g_1)  \frac{3}{\sqrt{21}} & \text{with prob. $\frac{1}{2}$}\,.
      \end{cases}
\end{equation*}
% The intrinsic oscillatory behavior of binary randomness indicates unstable gradient alignments during gradient descent at the initial training stage, which could potentially slow down convergence. We acknowledge that the above argument is heuristic, and the detailed examination of $\mc{A}( G^r(\wb{\theta}) , G^{bc}(\wb{\theta}))$ along the effective gradient flow \eqref{eq:effdym} (and the more challenging one $\mc{A}\left(\na_\theta \mc{L}_r(\theta), \na_\theta \mc{L}_{bc}(\theta) \right)$ along \eqref{eq:gradientflow}) is beyond the scope of this work and left for future investigation. 















\noindent 
{\bf Adam.} We now consider the deterministic version of the Adam optimizer \cite{kingma2014adam}, recalled below for completeness. Let $f(x)$ be a differentiable objective function on $\R^d$. The Adam iteration is defined by $z_n = T_{\gamma ,\alpha ,\beta}(n,z_{n - 1})$ 
for $z_n = (x_n, m_n,v_n) \in \R^d \times \R^d \times \R^d$ with $z_0 = (x_0, 0, 0)$, where 
\begin{equation*}
    T_{\gamma, \alpha, \beta}(n,z) = \begin{pmatrix}
        x - \frac{\gamma (1 - \alpha^n)^{- 1} ( \alpha m+ (1 - \alpha )\nabla f(x))}{\epsilon +(1 - \beta^n)^{- 1/2} \sqrt{\beta v+(1 - \beta )\nabla f(x)^{\odot 2}}} \\
        \alpha m+ (1 - \alpha )\nabla f(x) \\
        \beta v+ (1 - \beta )\nabla f(x)^{\odot 2}
        \end{pmatrix}\,.
\end{equation*}
% where the default hyperparameters are set as $\gamma = 0.001$, $\alpha = 0.9$, $\beta = 0.999$, and $\eps = 10^{-8}$. 
% It is worth mentioning that \cite{barakat2021convergence} derived the continuous-time limit of Adam as a nonautonomous ordinary differential equation, in contrast to \eqref{eq:gradientflow}, which was further exploited to show the convergence of Adam.  
We still consider the gradient conflict at the initialization, since the Adam dynamics is more complicated than the gradient flow one. From \cite{barakat2021convergence}, we have that starting from $(x_0,0,0) \in \R^{3d}$, the Adam dynamics at $t = 0$ satisfies $\dot{m}(0) \propto \na f(x_0)$, $\dot{v}(0) \propto \na f(x_0)^{\odot 2}$, and 
\begin{align*}
    \dot{x}(0) = - \frac{\na f(x_0)}{\eps + \sqrt{\na f(x_0)^{\odot 2}}} \underset{\eps = o(1)}{\approx}  - \frac{\na f(x_0)}{\sqrt{\na f(x_0)^{\odot 2}}}\,,
\end{align*}
indicating that at early iterations of Adam, 
the algorithm performance would be similar to the \emph{sign gradient
descent} \cite{balles2018dissecting}. Back to our problem \eqref{eq:losseg}, by the above discussion, if we apply Adam to the loss functions $\mc{L}_r(\theta)$ and $\mc{L}_{bc}(\theta)$, respectively, at the initialization, 
the normalized weights $\wb{\theta} = \ep^{-1} \theta$ will be updated along the directions:
\begin{align*}
    \frac{\na \mc{L}_r(\theta)}{\sqrt{\na \mc{L}_r(\theta)^{\odot 2}}} = \frac{G^r(\wb{\theta})}{\sqrt{G^r(\wb{\theta})^{\odot 2}}} + O(\ep^2)\,, \q  \frac{\na \mc{L}_{bc}(\theta)}{\sqrt{\na \mc{L}_{bc}(\theta)^{\odot 2}}} = \frac{G^{bc}(\wb{\theta})}{\sqrt{G^{bc}(\wb{\theta})^{\odot 2}}} + O(\ep^2)\,.
\end{align*}
Then, by \eqref{eq:effgrad1} and \eqref{eq:effgrad2}, one can compute 
\begin{align*}
    G^r_{\rm Adam}(\wb{\theta}) = \frac{G^r(\wb{\theta})}{\sqrt{G^r(\wb{\theta})^{\odot 2}}} = \sgn(\wb{\ba} \dd \wb{\bw}^{\odot 3})\left(\sgn(\wb{\bw}),\sgn(\wb{\ba}) \right),
\end{align*}
and 
\begin{align*}
    G^{bc}_{\rm Adam}(\wb{\theta}) = \frac{G^{bc}(\wb{\theta})}{\sqrt{G^{bc}(\wb{\theta})^{\odot 2}}} = \sgn(g_{-1} - g_1)\left(\sgn(\wb{\bw}),\sgn(\wb{\ba}) \right).
\end{align*}
It follows that the  alignment score is 
\begin{align*}
    \mc{A}\left( G^r_{\rm Adam}(\wb{\theta}) , G^{bc}_{\rm Adam}(\wb{\theta})  \right)  = \sgn(\wb{\ba} \dd \wb{\bw}^{\odot 3}) \sgn(g_{-1} - g_1) =  \begin{cases}
        \sgn (g_{-1} - g_1)  & \text{with prob. $\frac{1}{2}$}\,, \\
       - \sgn (g_{-1} - g_1) & \text{with prob. $\frac{1}{2}$}\,,
      \end{cases}
\end{align*}
which holds for any two-layer NN with width $N$. 

% Again, the binary randomness of the score $\mc{A}$ suggests the unstable gradient alignments during the early training process via Adam, as observed in \cref{fig:grad_align_score}.

\noindent 
{\bf Shampoo and SOAP.} We proceed to consider Shampoo \cite{gupta2018shampoo}, which is a second-order optimizer with 
Kronecker product preconditioners. 
% The recent work \cite{morwani2024new} shows that this preconditioner can be viewed as either an approximate Gauss-Newton component of Hessian or an approximation of the full-matrix Adagrad preconditioner \cite{duchi2011adaptive}. 
For the reader's convenience, we recall the Shampoo iterations for training neural networks. Following the notation in Section \ref{sec:2ndopt}, let $W_t, G_t \in \R^{m \times n}$ be the weight matrix and gradient matrix for a layer at time step $t$, respectively. Shampoo generates left and right preconditioners: 
\begin{align*}
    L_t = L_{t - 1} + G_t G_t^T\,, \quad R_t = R_{t - 1} + G_t^T G_t\,,
\end{align*}
and then updates the weight matrix by 
\begin{align*}
    W_{t + 1} = W_t - \eta L_t^{-1/4} G_t R_t^{-1/4}\,,
\end{align*}
with step size $\eta > 0$. If we disable the accumulation in the preconditioners and set $ L_t =  G_t G_t^T$ and $R_t = G_t^T G_t$, then the Shampoo optimizer is simplified to
\begin{align*}
    W_{t + 1} = W_t - \eta\, {\rm Shampoo}(G_t)\,, \q {\rm Shampoo}(G_t): = (G_t G_t^T)^{-1/4} G_t (G_t^T G_t)^{-1/4}\,,
\end{align*}
with ${\rm Shampoo}(G_t) = U_t V_t^T$, where $U_t$ and $V_t$ are from the reduced singular value decomposition of $G_t = U_t \Sigma_t V_t^T$. It is clear that if we apply Shampoo to $\mc{L}_r(\theta)$ or $\mc{L}_{bc}(\theta)$ with the two-layer NN \eqref{eq:twolayersol}, then under small initialization \cref{assp1}, at the initialization, the updates of the normalized weights $\wb{\theta} = (\wb{\ba},\wb{\bw}) = \ep^{-1} \theta$ would be 
\begin{align*}
   \wb{\ba} \leftarrow \wb{\ba} - \eta \, {\rm Shampoo}\left(G_a^{\textit{r or bc}}(\wb{\ba},\wb{\bw})\right),\q   \wb{\bw} \leftarrow \wb{\bw} - \eta \, {\rm Shampoo}\left(G_w^{\textit{r or bc}}(\wb{\ba},\wb{\bw})\right).
\end{align*}
Here $G_a^{\textit{r or bc}}, G_w^{\textit{r or bc}} \in \R^N$ are given in \eqref{eq:effgrad1} and \eqref{eq:effgrad2}. Moreover, note that for any vector $x \in \R^d$, ${\rm Shampoo}(x)$ is simply $x/\|x\|$. Therefore, we can compute the (effective) initial Shampoo gradient directions for $\mc{L}_r(\theta)$ and $\mc{L}_{bc}(\theta)$:
\begin{align*}
    G_{\rm Shampoo}^r(\wb{\theta}): = \left({\rm Shampoo}\left(G_a^{r}(\wb{\theta})\right),{\rm Shampoo}\left(G_a^r(\wb{\theta})\right)\right) = \sgn(\wb{\ba} \dd \wb{\bw}^{\odot 3}) \left(\frac{\wb{\bw}^{\odot 3}}{\|\wb{\bw}^{\odot 3}\|},\frac{ \wb{\ba} \odot \wb{\bw}^{\odot 2}}{\| \wb{\ba} \odot \wb{\bw}^{\odot 2}\|} \right)\,.
\end{align*}
and 
\begin{align*}
    G_{\rm Shampoo}^{bc}(\wb{\theta}): = \left({\rm Shampoo}\left(G_a^{bc}(\wb{\theta})\right),{\rm Shampoo}\left(G_a^{bc}(\wb{\theta})\right)\right) = \sgn(g_{-1} - g_1)\left(\frac{\wb{\bw}}{\|\wb{\bw}\|},\frac{\wb{\ba}}{\|\wb{\ba}\|}\right)\,.
\end{align*}
It follows that the  alignment score is, as $N \to \infty$, 
\begin{align*}
    \mc{A}\left( G^r_{\rm Shampoo}(\wb{\theta}) , G^{bc}_{\rm Shampoo}(\wb{\theta})  \right) \longrightarrow C_{{\rm Shampoo}}   \begin{cases}
        \sgn (g_{-1} - g_1)  & \text{with prob. $\frac{1}{2}$}\,, \\
       - \sgn (g_{-1} - g_1) & \text{with prob. $\frac{1}{2}$}\,.
      \end{cases}
\end{align*}

We finally consider SOAP. Following the notations in \cref{sec:soap}, if $G_t$ is a vector, then $\widetilde{G}_t = [1,0,\cdots, 0]^\top$ and $\operatorname{Adam}(\widetilde{G}_t) = \widetilde{G}_t$. We transform this gradient back and obtain $G_t/\norm{G_t}$. It means that at initialization, 
\begin{equation*}
    {\rm Shampoo}\left(G_a^{\textit{r or bc}}(\wb{\ba},\wb{\bw})\right) =  {\rm SOAP}\left(G_a^{\textit{r or bc}}(\wb{\ba},\wb{\bw})\right)\,.
\end{equation*}
Therefore, the initial gradient conflict of SOAP follows from the case of Shampoo. 

\end{proof}


\section{Additional Lemmas and Proofs}

\begin{lemma}[\cite{anil2020scalable}, Lemma 1]
    \label{lemma: hessian}
    Let $G_1, \ldots, G_t \in \mathbb{R}^{m \times n}$ be matrices of rank at most $r$. Let $g_s=\operatorname{vec}\left(G_s\right)$ and define $\widehat{H}_t=\epsilon I_{m n}+\sum_{s=1}^t g_s g_s^{\top}$. Define $L_t, R_t$ as above: $L_t=\epsilon I_m+\sum_{s=1}^t G_s G_s^{\top}, R_t=\epsilon I_n+\sum_{s=1}^t G_s^{\top} G_s$. Then for any $p, q>0$ such that $1 / p+1 / q=1$, we have $\widehat{H}_t \leq r L_t^{1 / p} \otimes R_t^{1 / q}$.
\end{lemma}
It follows from the above lemma that 
for any $p, q>0$ with $1 / p+1 / q=1$,
the full AdaGrad preconditioned gradient $\widehat{H}_t^{-1 / 2}g_t$ can be approximated by $(L_t^{1 / p} \otimes R_t^{1 / q})^{-1 / 2} g_t = \Vec(L_t^{-1 / 2p} G_t R_t^{-1 / 2q})$. In particular, the case of $p=q=2$ yields the standard Shampoo update. 
Moreover, \cite{morwani2024new} explores the Hessian approximation perspective of Shampoo, showing that the preconditioner in Shampoo is a Kronecker product approximation of the Gauss-Newton component of layerwise Hessian (see \cref{rem1}). Similar arguments will be used below to understand the second-order nature of SOAP.  



\begin{lemma} [\cite{morwani2024new}, Corollary 2]
\label{lemma: exact_h_gn}
Under the assumption that the reshaping of the Hessian tensor $H_{GN}$ is rank-1,
$$
\hat{H}_{GN} =\left(\mathbb{E}\left[G G^{\top}\right] \otimes \mathbb{E}\left[G^{\top} G\right]\right) / \operatorname{Tr}\left(\mathbb{E}\left[G G^{\top}\right]\right).
$$
\end{lemma}


\subsection{Proof of \cref{prop1}}
\label{proof: prop1}
\begin{proof}
    When $n=2$, we note 
\begin{align*}
\mathcal{A}(v_1, v_2) &= 2\left\|\frac{\frac{v_1}{\|v_1\|} + \frac{v_2}{\|v_2\|}}{2}\right\|^2 - 1 \notag \\
% &= \frac{1}{2}\left\|\frac{v_1}{\|v_1\|} + \frac{v_2}{|v_2|}\right\|^2 - 1 \\
&= \frac{1}{2}\left(\left\|\frac{v_1}{\|v_1\|}\right\|^2 + 2\frac{v_1 \dd v_2}{\|v_1\|\|v_2\|} + \left|\frac{v_2}{\|v_2\|}\right|^2\right) - 1  \notag \\
&= \frac{1}{2}(1 + 2\cos(v_1, v_2) + 1) - 1  = \cos(v_1, v_2)\,. \qedhere
\end{align*} 
\end{proof}

\subsection{Proof of \cref{lemma: inter_step_gradient_align}} \label{proof: lemma_inter_step_gradient_align}
\begin{proof}
By Taylor expansion, the gradient at step $t+1$ can be expressed as:
\begin{align*}
g_{t+1} &= g_t + H(\theta_t)(\theta_{t+1} - \theta_t) + O(\|\theta_{t+1} - \theta_t\|^2) \\
&= g_t - \eta H(\theta_t)H^{-1}(\theta_t) g_t + O(\eta^2\|g_t\|^2) \\
&= (1-\eta)g_t + O(\eta^2\|g_t\|^2).
\end{align*}    
Then, it is easy to compute 
\begin{align*}
    \mathcal{A}(g_t, g_{t+1}) &= \frac{g_{t+1} \dd g_t}{\|g_{t+1}\|\|g_t\|} \\
    % &= \frac{((1-\eta)g_t + O(\eta^2\|g_t\|^2))^T g_t}{\|g_{t+1}\|\|g_t\|} \\
&= \frac{(1-\eta)\|g_t\|^2 + O(\eta^2\|g_t\|^3)}{\|g_{t+1}\|\|g_t\|}  \\
&= \frac{(1-\eta)\|g_t\|^2 + O(\eta^2\|g_t\|^3)}{((1-\eta)\|g_t\| + O(\eta^2\|g_t\|^2))\|g_t\|} \\
&= \frac{1-\eta + O(\eta^2\|g_t\|)}{1-\eta + O(\eta^2\|g_t\|)} \\
&= 1 + O(\eta^2\|g_t\|). \qedhere
\end{align*}
\end{proof}

\subsection{Proof of \cref{prop:soap}}
\begin{proof}
\label{proof:soap}
For each layer-weight matrix $W$ with $w = \Vec(W)$, the initial, boundary and PDE losses can be approximated via second-order Taylor expansion around $w^*$:
\begin{align*}
    & \mc{L}_{ic}(W) \approx \mc{L}_{ic}\left(w^*\right)+\frac{1}{2}\left(w-w^*\right)^T H_{ic} \left(w-w^*\right), \\
    & \mc{L}_{bc}(W) \approx \mc{L}_b\left(w^*\right)+\frac{1}{2}\left(w-w^*\right)^T H_{bc} \left(w-w^*\right), \\
& \mc{L}_r(w) \approx \mc{L}_r\left(w^*\right)+\frac{1}{2}\left(w-w^*\right)^T H_r\left(w-w^*\right),
\end{align*}
where $H_b$ and $H_r$ denote the Hessians of $\mathcal{L}_b$ and $\mathcal{L}_r$ at $w^*$, respectively. Taking gradients with respect $w$ we have
\begin{align*}
    g_{ic} \approx H_{ic}(w-w^*), \quad,  g_{bc} \approx H_{bc}(w-w^*), \quad g_r \approx H_b(w-w^*).
\end{align*}
Recall from Theorem \ref{thm: soap}, SOAP approximates $H^{-1}$ as the preconditioner. When applying SOAP optimizers to $\mathcal{L}_{ic}, \mathcal{L}_{bc}$ and $\mathcal{L}_r$ separately, we obtain:
\begin{align*}
    \mathcal{A}(\operatorname{Soap}(G_{ic}), \operatorname{Soap}(G_{bc}), \operatorname{Soap}(G_r)) &\approx  \mathcal{A}(H_{ic}^{-1}g_{ic}, H_{bc}^{-1}g_{bc}, H_r^{-1}g_r  ) \\
    &\approx \mathcal{A}(H_{ic}^{-1}H_{ic}(w-w^*), H_{bc}^{-1}H_{bc}(w-w^*), H_r^{-1}H_r(w-w^*)) \approx 1.
\end{align*}
\end{proof}


\section{Experimental Details}
\label{appendix: experiments}

\subsection{Architectures} 
\label{appendix:arch}

This section outlines the network architectures employed in our work, along with the enhancements introduced to improve their performance.



\paragraph{Modified MLP.} The modified MLP architecture is proposed by \cite{wang2021understanding}, which has been extensively used in the literature \cite{daw2022rethinking,wang2022respecting, anagnostopoulos2023residual,chen2024self,karakonstantis2024room,zhangphysics} due to its improved capability in learning complex PDE solutions. The network processes input coordinates through two parallel encoders:
\begin{align}
    \mathbf{U}=\sigma\left(\mathbf{W}_1 \mathbf{x}+\mathbf{b}_1\right), \quad \mathbf{V}=\sigma\left(\mathbf{W}_2 \mathbf{x}+\mathbf{b}_2\right).
\end{align}
Then, for $l=1,2, \ldots, L$, the forward pass is defined as:
\begin{align}
    & \mathbf{f}^{(l)}(\mathbf{x})=\mathbf{W}^{(l)} \cdot \mathbf{g}^{(l-1)}(\mathbf{x})+\mathbf{b}^{(l)}, \\
& \mathbf{g}^{(l)}(\mathbf{x})=\sigma\left(\mathbf{f}_\theta^{(l)}(\mathbf{x})\right) \odot \mathbf{U}+\left(1-\sigma\left(\mathbf{f}_\theta^{(l)}(\mathbf{x})\right)\right) \odot \mathbf{V}
\end{align}
The final network output is given by
\begin{align}
    \mathbf{f}_\theta(\mathbf{x})=\mathbf{W}^{(L+1)} \cdot \mathbf{g}^{(L)}(\mathbf{x})+\mathbf{b}^{(L+1)}.
\end{align}
where $\sigma$ is a nonlinear activation function, $\odot$ denotes element-wise multiplication, and the trainable parameters are:
\begin{align}
    \theta=\left\{\mathbf{W}_1, \mathbf{b}_1, \mathbf{W}_2, \mathbf{b}_2,\left(\mathbf{W}^{(l)}, \mathbf{b}^{(l)}\right)_{l=1}^{L+1}\right\}.
\end{align}
This architecture extends the standard MLP by incorporating dual input encoders and merging their features through point-wise multiplication at each hidden layer. While computationally more demanding, this modification demonstrates superior performance in minimizing PDE residuals compared to standard MLPs.

\paragraph{PirateNet.}  PirateNet is proposed by   \cite{wang2024piratenets}, which aims to enable stable and efficient training of deep PINN models.  The architecture first transforms input coordinates $\mathbf{x}$ into a high-dimensional feature space using random Fourier features \cite{tancik2020fourier}:
\begin{align*}
    \Phi(\mathbf{x})= \begin{bmatrix}
    \cos (\mathbf{B x} ) \\
    \sin (\mathbf{B x} )
    \end{bmatrix},
\end{align*}
where $\mathbf{B} \in \R^{m \times d}$ has entries sampled i.i.d. from $\mathcal{N}(0, s^2)$ with user-specified $s > 0$.  This embedding mitigates spectral bias in PINNs by improving the eigenfunction frequency of the Neural Tangent Kernel, enabling better learning of high-frequency components and multiscale features \cite{wang2021eigenvector}.

The embedded coordinates are processed through two dense layers that act as gates:
\begin{align*}
\mathbf{U} &= \sigma(\mathbf{W}_1 \Phi(\mathbf{x}) + \mathbf{b}_1  ), \quad
\mathbf{V} = \sigma(\mathbf{W}_2 \Phi(\mathbf{x}) + \mathbf{b}_2  ),
\end{align*}
where $\sigma$ is a point-wise activation function. This gating mechanism is essentially the same as in modified MLP.

Let $\mathbf{x}^{(1)} = \Phi(\mathbf{x})$ and $\mathbf{x}^{(l)}$ be the input to the $l$-th block ($1 \le l \le L$). Each block performs:
\begin{align}
    \mathbf{f}^{(l)}  &= \sigma\big(\mathbf{W}^{(l)}_1 \mathbf{x}^{(l)} + \mathbf{b}^{(l)}_1\big)\,, \label{eq: step1}  \\
    \mathbf{z}^{(l)}_1 &= \mathbf{f}^{(l)} \odot \mathbf{U} + (1 - \mathbf{f}^{(l)}) \odot \mathbf{V}\,,  \label{eq: gate1} \\
     \mathbf{g}^{(l)}  &= \sigma\big(\mathbf{W}^{(l)}_2 \mathbf{z}_1^{(l)} + \mathbf{b}^{(l)}_2\big)\,, \\
     \mathbf{z}^{(l)}_2 &= \mathbf{g}^{(l)} \odot \mathbf{U} + (1 - \mathbf{g}^{(l)}) \odot \mathbf{V}\,,  \label{eq: gate2} \\
      \mathbf{h}^{(l)}  &= \sigma\big(\mathbf{W}^{(l)}_3 \mathbf{z}_2^{(l)} + \mathbf{b}^{(l)}_3\big)\,, \\
    \mathbf{x}^{(l+1)} &= \alpha^{(l)}  \mathbf{h}^{(l)} + (1 - \alpha^{(l)})   \mathbf{x}^{(l)}\,,
    \label{eq: skip}
\end{align}
Each block comprises three dense layers with dual gating operations and an adaptive residual connection. The trainable $\alpha^{(l)}$ parameters control block nonlinearity: $\alpha^{(l)}=0$ yields an identity mapping, while $\alpha^{(l)}=1$ produces fully nonlinear transformation. 

The final output of a PirateNet of $L$ residual blocks is given by
\begin{align}
    \mathbf{u}_{\mathbf{\theta}} = \mathbf{W}^{(L+1)} \mathbf{x}^{(L)}\,.
\end{align}

Importantly, we initialize $\alpha^{(l)}=0$, making the initial output a linear combination of first-layer embeddings. This initialization strategy mitigates training difficulties in deep networks by starting with effectively shallow architecture and gradually increasing depth through learned $\alpha$ values. Additionally, the linear structure at initialization enables direct integration of prior solution data through least squares fitting:
\begin{align}
\label{eq: lq}
\min_{\mathbf{W}} \left\| \mathbf{W} \Phi  - \mathbf{Y} \right\|_2^2,
\end{align}
where $\mathbf{Y}$ represents available measurements. This approach provides an optimal initial guess based on various data sources, including experimental measurements, boundary conditions, or linearized PDE solutions.

\paragraph{Exact imposition of periodic boundary conditions.} We adopt the approach of \cite{dong2021method} to enforce periodic boundary conditions as hard constraints, improving both training convergence and accuracy. Consider a one-dimensional periodic function with period $P$ satisfying:
\begin{align}\label{eq:periodic_constraint}
    u^{(l)}(a) = u^{(l)}(a + P), \quad l=0, 1, 2, \dots.
\end{align}
We construct a Fourier feature embedding:
\begin{align}
    \label{eq: 1D_Fourier}
    \mathbf{v}(x) = \left(\cos (\omega x), \sin (\omega x) \right),
\end{align}
where $\omega = \frac{2 \pi}{L}$. Any network $u_{\mathbf{\theta}}(v(x))$ using this embedding inherently satisfies the periodic boundary condition.

The same idea can be directly extended to higher-dimensional domains. For two-dimensional domains, the periodicity constraints are:
\begin{align}
    &\frac{\partial^{l}}{\partial x^{l}} u\left(a, y\right)=\frac{\partial^{l}}{\partial x^{l}} u\left(a + P_x, y\right), \quad  y \in\left[b, b + P_y\right], \\
    &\frac{\partial^{l}}{\partial y^{l}} u\left(x, a\right)=\frac{\partial^{l}}{\partial y^{l}} u\left(x, b + P_y\right), \quad  x \in\left[a, a + P_x\right],
\end{align}
for $l=0, 1, 2, \dots$, where  $P_x$  and $P_y$ are the periods in the $x$ and $y$ directions, respectively. Similarly, these constraints are encoded using the embedding:
\begin{align}
    \mathbf{v}(x, y) = \begin{bmatrix}
    \cos \left(\omega_{x} x\right), \sin \left(\omega_{x} x\right), \cos \left(\omega_{y} y\right),  \sin \left(\omega_{y} y\right)
    \end{bmatrix}
\end{align}
with $w_x = \frac{2 \pi}{P_x}, w_y = \frac{2 \pi}{P_y}$.

For time-dependent problems, we concatenate time coordinates $t$ with spatial embeddings: $u_{\mathbf{\theta}}([t, \mathbf{v}(x)])$ or $u_{\mathbf{\theta}}([t, \mathbf{v}(x, y)])$. 

% When temporal periodicity is desired, we use:
% \begin{align}
%     \mathbf{w}(t, x) = [\cos(\omega_t t), \sin(\omega_t t), \mathbf{v}(t, x)]
% \end{align}
% where $\omega_t = \frac{2 \pi}{P_t}$ and $P_t$ is trainable. We typically initialize $P_t$ to the temporal domain length, allowing the network to learn the correct period. This approach remains applicable even for non-periodic systems by setting initial $P_t$ greater than the temporal domain length.

% Lastly, other types of boundary conditions, including  Dirichlet, Neumann, Robin, etc., can also be enforced in a ``hard'' manner by modifying the network outputs, see \cite{sukumar2021exact, lu2021physics} for more details.



\paragraph{Random weight factorization.} We implement random weight factorization (RWF) \cite{wang2022random} to enhance PINN performance. RWF decomposes each neuron's weight vector as:
\begin{align}
    \mathbf{w}^{(k, l)}=s^{(k, l)} \cdot \mathbf{v}^{(k, l)},
\end{align}
where $k=1,\ldots,d_l$, $l=1,\ldots,L+1$, $\mathbf{w}^{(k, l)} \in \mathbb{R}^{d_{l-1}}$ is the $k$-th row of weight matrix $\mathbf{W}^{(l)}$, $s^{(k, l)} \in \mathbb{R}$ is a trainable scale factor, and $\mathbf{v}^{(k, l)} \in \mathbb{R}^{d_{i-1}}$. This factorization can be expressed in matrix form as:
\begin{align}
    \mathbf{W}^{(l)}=\operatorname{diag}\left(\mathbf{s}^{(l)}\right) \cdot \mathbf{V}^{(l)}, \quad l=1,2, \ldots, L+1
\end{align}
with $\mathbf{s}^{(l)} \in \mathbb{R}^{d_t}$.

Implementation involves: (1) initializing MLP parameters using the Glorot scheme \cite{glorot2010understanding}, (2) initializing scale vectors $\exp(s)$ where $s \sim \mathcal{N}(\mu, \sigma \mathrm{I})$, (3) factorizing each weight matrix as $\mathbf{W}=\operatorname{diag}(\exp (\mathbf{s})) \cdot \mathbf{V}$, and (4) optimizing parameters $\mathbf{s}, \mathbf{V}$ directly. We employ exponential parameterization following Weight Normalization \cite{salimans2016weight} to ensure non-zero scale factors across varied magnitudes. We recommend $\mu=0.5$ or 1 and $\sigma=0.1$, as these values consistently improve convergence and accuracy while avoiding the instability of larger values or the diminished effect of smaller ones.



\subsection{Training pipeline}
\label{appendix:training}

This section details the methodologies and strategies used to train PINN models.

\paragraph{Causal training.}
Recent work by \cite{wang2022respecting} shows that PINNs may violate temporal causality when solving time-dependent PDEs, as they tend to minimize residuals at later times before correctly solving earlier times. To address this, we introduce a causality-aware training approach.
We partition the temporal domain into $M$ equal segments and denote the PDE residual loss within the $i$-th segment as $L_r^i$. The modified residual loss becomes:
\begin{align}
       \mathcal{L}_r(\mathbf{\theta}) = \frac{1}{M} \sum_{i=1}^M w_i \mathcal{L}_r^i(\mathbf{\theta}).
\end{align}
We compute the temporal weights as
\begin{align}
      \label{eq: temporal_update}
    w_i = \exp\left(- \epsilon \sum_{k=1}^{i-1}  \mathcal{L}_r^k( \mathbf{\theta})\right)    , \text{ for } i = 2, 3, \dots, M.
  \end{align}
Then,
\begin{align}
     \mathcal{L}_r(\mathbf{\theta}) = \frac{1}{M} \sum_{i=1}^M     \exp\left(- \epsilon \sum_{k=1}^{i-1}  \mathcal{L}_r^k( \mathbf{\theta})\right) \mathcal L_r^i(\mathbf{\theta}).
\end{align}
The weight $w_i$ decreases exponentially with the cumulative residual loss from previous time steps. This ensures that $\mathcal{L}_r^i(\mathbf{\theta})$ is minimized only after previous residuals $\{\mathcal{L}_r^k(\mathbf{\theta})\}_{k=1}^{i-1}$ become sufficiently small, enforcing temporal causality in the optimization process.

The causality parameter $\epsilon$ requires careful tuning: small values may insufficiently enforce causality, while large values can create optimization difficulties by requiring extremely small early-time residuals before later times are considered. We recommend selecting a moderate $\epsilon$ that allows all temporal weights to converge to 1 by training completion, reducing it if necessary.




\paragraph{Learning rate annealing.} 

A key challenge in training PINNs is handling multi-scale losses from PDE residuals that cannot be normalized during preprocessing process. While loss weighting can address this, manual weight selection is impractical due to their problem-dependent nature and the absence of validation data for hyperparameter tuning in PDE solving.


We implement a self-adaptive learning rate annealing algorithm \cite{wang2021understanding} that automatically balances the weighted loss:
\begin{align}
    \mathcal{L}(\mathbf{\theta}) =  \lambda_{ic} \mathcal{L}_{ic}(\mathbf{\theta}) + \lambda_{bc} \mathcal{L}_{bc}(\mathbf{\theta}) +  \lambda_r \mathcal{L}_r(\mathbf{\theta}), 
\end{align}
The global weights are computed as:
  \begin{align}
  \label{eq: lambda_ic_update}
     \hat{\lambda}_{ic} &=  \frac{  \|\nabla_{\theta}  \mathcal{L}_{ic}(\theta)\| +  \|\nabla_{\theta}  \mathcal{L}_{bc}(\theta)\| +  \|\nabla_{\theta}  \mathcal{L}_{r}(\theta)\|   } {\|\nabla_{\theta}  \mathcal{L}_{ic}(\theta)\|}, \\
     \label{eq: lambda_bc_update}
      \hat{\lambda}_{bc} &= \frac{  \|\nabla_{\theta}  \mathcal{L}_{ic}(\theta)\| +  \|\nabla_{\theta}  \mathcal{L}_{bc}(\theta)\| +  \|\nabla_{\theta}  \mathcal{L}_{r}(\theta)\|   } {\|\nabla_{\theta}  \mathcal{L}_{bc}(\theta)\|},  \\
      \label{eq: lambda_r_update}
       \hat{\lambda}_{r} &=  \frac{  \|\nabla_{\theta}  \mathcal{L}_{ic}(\theta)\| +  \|\nabla_{\theta}  \mathcal{L}_{bc}(\theta)\| +  \|\nabla_{\theta}  \mathcal{L}_{r}(\theta)\|   } {\|\nabla_{\theta}  \mathcal{L}_{r}(\theta)\|}, 
  \end{align}
where $\|\cdot\|$ denotes the $L^2$ norm.
Then we obtain
\begin{align}
  \| \hat{\lambda}_{ic} \nabla_\theta \mathcal{L}_{ic} (\theta) \| =   \| \hat{\lambda}_{bc} \nabla_\theta \mathcal{L}_{ic} (\theta) \| = \| \hat{\lambda}_{r} \nabla_\theta \mathcal{L}_{ic} (\theta) \| = \| \nabla_\theta \mathcal{L}_{ic} (\theta) \| +  \| \nabla_\theta \mathcal{L}_{bc} (\theta) \|  +  \| \nabla_\theta \mathcal{L}_{r} (\theta) \|. 
\end{align}
This formulation equalizes the gradient norms of weighted losses, preventing bias toward any particular term during training. The weights are updated as running averages of their previous values, stabilizing stochastic gradient descent. Updates occur at user-specified intervals (typically every 100-1000 iterations), leading to minimal computational overhead.


\paragraph{Curriculum training and time-marching.} Despite the improvements described above, PINNs still face challenges in complex domains requiring high accuracy, such as chaotic systems like high Reynolds number Navier-Stokes equations where error accumulation can cause trajectory divergence. We address these challenges using curriculum training \cite{krishnapriyan2021characterizing}, which decomposes the optimization into more manageable sub-tasks.

An effective approach we employ is the curriculum training strategy introduced by \cite{krishnapriyan2021characterizing}. The core idea involves decomposing the entire optimization task for PINNs into a sequence of more manageable sub-tasks. In this work, we mainly focus on  integrating this strategy into our training pipeline for solving time-dependent PDEs and singular perturbation problems. 

For time-dependent PDEs, we implement temporal domain decomposition: the time domain is divided into smaller intervals. After the first window, initial conditions for subsequent windows are set using predictions from the final step of the  previous window.  This approach reduces the difficulty of the optimization task of learning full system dynamics, though at an increased computational cost due to per-window model retraining.

While we also partition the temporal domain to compute causal weights within each window, this differs from the time-marching strategy. Both techniques promote learning solutions sequentially along the time axis to respect causality, but causal weighting complements rather than replaces time-marching, as causality violations may still occur within individual time windows.

\subsection{Data Generation}
We generate our reference dataset using two numerical packages: Chebfun \cite{driscoll2014chebfun} in MATLAB and IncompressibleNavierStokes \cite{Agdestein_IncompressibleNavierStokes_jl_2024} in Julia. The data generation process employs a time step of $dt=10^{-4}$, followed by temporal downsampling to construct the final dataset. Table \ref{tab: data_gen} summarizes the PDE parameters and dataset details.

\begin{table}
\renewcommand{\arraystretch}{1.2}
\centering
\caption{Parameter settings and numerical configurations for generating the reference solution across PDE benchmarks.}
\label{tab: data_gen}
\vspace{1mm}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l c c}
\toprule
\textbf{PDE} & Parameter & Package & Resolution  \\
\midrule
\textbf{Wave} & $c=4$   & N/A & $200 \times 128$\\
\textbf{Burgers} & $\nu = 0.01 \ \pi$ & Chebfun & $200 \times 512$\\
\textbf{AC} & $\epsilon = 10^{-4}, a=5$ & Chebfun & $200 \times 512$ \\
\textbf{KdV} & $\eta=1, \mu=0.022$ & Chebfun & $200 \times 512$\\
\textbf{KS} & $\alpha = 100/ 16, \beta=100 / 16^2, \gamma=100 / 16^4$ & Chebfun & $250 \times 512$\\
\textbf{GS} & $\epsilon_1=0.2, \epsilon_2=0.1, b_1=40, b2=100, c_1=c_2=1,000$ & Chebfun & $100 \times 200 \times 200$\\
\textbf{GL} & $\epsilon=0.004, \mu=10, \gamma=10 + 15i$ & Chebfun & $100 \times 200 \times 200$\\
\textbf{LDC} & $\text{Re}{=}5{\times}10^3$ & IncompressibleNavierStokes  & $128 \times 128$\\
\textbf{KF} & $\text{Re}{=}10^4$ & IncompressibleNavierStokes & $50 \times 512 \times 512$\\
\textbf{RT} & $\text{Ra}{=}10^6, \text{Pr}=0.71$ & IncompressibleNavierStokes & $40 \times 100 \times 200$\\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Hyper-parameters}
The complete set of hyperparameters is detailed in Table \ref{tab: hyper-parameters}, largely following the configurations established in \cite{wang2023expert,wang2024piratenets}. The decay step is adapted for each benchmark to ensure the learning rate reaches a sufficiently small value (e.g., $10^{-7}$) by the end of training. The number of time windows is determined empirically based on problem complexity, with fine-tuning guided by the loss convergence behavior during preliminary experiments. 

\begin{table}
\renewcommand{\arraystretch}{1.2}
\centering
\caption{{\em Hyperparameter configurations for benchmark PDEs.} Hyperparameter settings used to reproduce our experimental results. The backbone architecture is PirateNet, where Depth indicates the number of adaptive residual blocks, and Width denotes the number of neurons per hidden layer. RFF and RWF represent Random Fourier Features and Random Weight Factorization, respectively.}
\label{tab: hyper-parameters}
\vspace{1mm}
\resizebox{\textwidth}{!}{
\begin{tabular}{l cccccccccc}
\toprule
\textbf{Parameter} & \textbf{Wave} & \textbf{Burgers} & \textbf{AC} & \textbf{KdV} & \textbf{KS} & \textbf{GS} & \textbf{GL} & \textbf{LDC} & \textbf{KF} & \textbf{RT} \\
\midrule
\textbf{Architecture} & & & & & & & & & & \\
\midrule
Depth & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 4 & 3 & 3 \\
Width & 256 & 256 & 256 & 256 & 256 & 256 & 256 & 256 & 384 & 384 \\
Activation & Tanh & Tanh & Tanh & Tanh & Tanh & Swish & Swish & Tanh & Tanh & Tanh \\
RFF scale & 10.0 & 2.0 & 2.0 & 2.0 & 2.0 & 2.0 & 2.0 & 10.0 & 2.0 & 2.0 \\
RWF & \multicolumn{10}{c}{$\mu{=}1.0, \sigma{=}0.1$} \\
\midrule
\textbf{Learning rate schedule} & & & & & & & & & & \\
\midrule
Initial learning rate &  \multicolumn{10}{c}{ $10^{-3}$ } \\
Decay rate &  \multicolumn{10}{c}{ 0.9 } \\
Decay steps & $2 \times 10^3$ & $2 \times 10^3$ & $5 \times 10^3$ & $2 \times 10^3$ & $2 \times 10^3$ & $2 \times 10^3$ & $2 \times 10^3$ & $2 \times 10^3$ & $2 \times 10^3$ & $2 \times 10^3$ \\
Warmup steps &  \multicolumn{10}{c}{ $5 \times 10^{3}$ } \\
\midrule
\textbf{Training} & & & & & & & & & & \\
\midrule
Iters (per time window) & $10^5$ &  $10^5$ & $3{\times}10^5$ & $10^5$ & $10^5$ & $10^5$ & $10^5$ & $2{\times}10^5$ & $2{\times}10^4$ & $10^5$ \\
Batch size &  \multicolumn{10}{c}{ 8,192 } \\
\# Time windows & 1& 1 & 1& 1& 10& 10 & 5 & N / A & 25 & 4 \\
\midrule
\textbf{Weighting Scheme} &  \multicolumn{10}{c}{ Grad Norm } \\
\midrule
\textbf{Causal weighting} & & & & & & & & & & \\
\midrule
Tolerance & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & N / A & 1.0 & 1.0 \\
\# Chunks & 16 & 16 & 16  & 16  & 16  & 16  &16  & N / A  & 16  & 16  \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Computational Cost}
Our implementation is based on JAX-PI \cite{wang2023expert} and we conducted all experiments on a single NVIDIA A6000 GPU, with detailed runtime benchmarks reported in Table \ref{tab: cost}. 
% While our SOAP implementation exhibits higher computational overhead - requiring approximately 2x longer training time compared to baselines - this study primarily explores the performance frontier of PINNs approaches through extended training until completely convergence.
% Nevertheless, examining the error and loss convergence curves in Appendix \ref{appendix:benchmarks} shows that our method achieves descent accuracy (approximately $10^{-4}$) within the first 10,000 training iterations, followed by gradual error reduction in subsequent iterations. This suggests the potential for a 10x reduction in training time while maintaining competitive performance. 
% These empirical findings motivate future research into computationally efficient optimization algorithms and training strategies for PINNs.

\begin{table}
\centering
\renewcommand{\arraystretch}{1.4}
\caption{Computational runtime (in hours) comparison of different methods across various PDEs. All experiments are performed on an Nvidia A6000 GPU, reporting the total training time needed to achieve convergence using PINNs with Adam and SOAP, respectively}
\label{tab: cost}
\begin{tabular}{l cc}
\toprule
\textbf{Benchmark} & \multicolumn{1}{c}{\textbf{Adam}  } & \multicolumn{1}{c}{\textbf{SOAP}  } \\
\midrule
Wave  & 2.80 &  4.35\\
Burgers  & 1.18  & 4.05 \\
Allen-Cahn  & 1.48 & 5.83 \\
Korteweg–De Vries  & 1.61 & 3.90 \\
Kuramoto-Sivashinsky & 19.51 & 34.16\\
Grey-Scott & 19.52 & 40.01\\
Ginzburg-Landau  & 15.98 & 23.75 \\
Lid-driven cavity ($\text{Re}=5 \times 10^3$) & 5.67 & 8.25 \\
Kolmogorov flow ($\text{Re}=10^4$) & 9.56 & 11.00 \\
Rayleigh-Taylor instability ($\text{Pr}=0.71, \text{Ra}=10^6$) & 20.23 & 21.73 \\
\bottomrule
\end{tabular}
\vspace{1mm}
\end{table}



% \subsection{Computational Costs}
\subsection{Benchmarks}
\label{appendix:benchmarks}

\paragraph{Wave equation.}  We consider a one-dimensional wave equation in the domain $\Omega=[0,1] \times[0,1]$ taking the form
\begin{align*}
    & u_{t t}(x, t)-4 u_{x x}(x, t)=0, \quad(x, t) \in(0,1) \times(0,1), \\
& u(0, t)=u(1, t)=0, \quad t \in[0,1], \\
& u(x, 0)=\sin (\pi x)+\frac{1}{2} \sin (4 \pi x), \quad x \in[0,1], \\
& u_t(x, 0)=0, \quad x \in[0,1].
\end{align*}
where  $u$ represents the wave amplitude, and $c$ is the wave propagation speed, determined by the medium's physical properties.

By d'Alembert's formula, the solution $u(x, t)$ is given by
\begin{align*}
    u(x, t)=\sin (\pi x) \cos (2 \pi t)+\frac{1}{2} \sin (4 \pi x) \cos (8 \pi t).
\end{align*}


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/wave/wave.png}
\caption{{\em Wave equation.}  Top: Comparison between the reference solution and the model predictions. Bottom: Training loss and test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:wave}
\end{figure}


\paragraph{Burgers equation.}  The 1D Burgers equation is defined as:
\begin{align*}
u_t + u u_x = \nu u_{xx},
\end{align*}
where $u$ represents the velocity field, and $\nu$ is the kinematic viscosity coefficient controlling the diffusion strength. Here we set  $(x, t) \in \Omega = [-1, 1] \times [0, 1]$, with initial and boundary conditions:
\begin{align*}
u(x, 0) &= -\sin(\pi x), \\
u(-1, t) &= u(1, t) = 0,
\end{align*}
and viscosity parameter $\nu = 0.01/\pi$.







\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/burgers/burgers.png}
\caption{{\em Burgers' equation.}  Top: Comparison between the reference solution and  model predictions. Bottom: Training loss and test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:burgers}
\end{figure}


\paragraph{Allen-Cahn equation.} We investigate the one-dimensional Allen-Cahn equation with periodic boundary conditions:
\begin{align*}
    &u_{t}-0.0001 u_{x x}+5 u^{3}-5 u=0\,, \quad t \in[0,1]\,,\  x \in[-1,1]\,, \\
    &u(0, x)=x^{2} \cos (\pi x)\,, \\
    &u(t, -1)=u(t, 1)\,, \quad u_{x}(t, -1)=u_{x}(t, 1)\,.
\end{align*}
where $u$ represents the order parameter (e.g., concentration difference between two phases),  $\epsilon$ controls the interfacial width, $a$ is the reaction rate coefficient, and the term $({u}-{u}^3)$ drives the phase separation.

It is worth noting that this benchmark has been extensively used to validate the effectiveness of PINNs methodologies. In Table \ref{tab: AC}, we compare the test errors across different PINNs advancements, demonstrating that our approach achieves state-of-the-art performance with an improvement of up to one order of magnitude in accuracy.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/ac/ac.png}
\caption{{\em Allen-Cahn equation.}  Top: Comparison between the reference solution and model predictions. Bottom: Training loss and test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:ac}
\end{figure}


\begin{table}
    \renewcommand{\arraystretch}{1.4}
    \centering
    \caption{{\em Allen-Cahn equation:} Relative $L^2$ test errors obtained by different PINNs variants.}
    \label{tab: AC}
    \begin{tabular}{l|c}
    \hline
    \textbf{Method}   & \textbf{Relative $L^2$ error}  \\
     \hline
      Original formulation of Raissi {\it et al.} \cite{raissi2019physics}    &  $4.98 \times 10^{-1}$ \\
      Adaptive time sampling \cite{wight2020solving} & $2.33 \times 10^{-2}$ \\
       Self-attention \cite{mcclenny2020self} & $2.10 \times 10^{-2}$  \\
       Time marching \cite{mattey2022novel}  & $1.68 \times 10^{-2}$ \\
       Causal training \cite{wang2022respecting} & $1.39 \times 10^{-4}$ \\
       Dirac delta function causal training \cite{es2023optimal}  & $6.29  \times 10^{-5}$ \\
       JAX-PI \cite{wang2023expert} & $5.37 \times 10^{-5}$ \\
        RBA-PINNs  \cite{anagnostopoulos2023residual}  & $4.55 \times 10^{-5}$ \\
    PirateNet \cite{wang2024piratenets} & ${2.24 \times 10^{-5}}$ \\
         BRDR-PINNs  \cite{chen2024self}  & $1.45 \times 10^{-5}$ \\
        \textbf{Ours} & $\mathbf{3.48 \times 10^{-6}}$ \\
    \hline
    \end{tabular}

\end{table}


\paragraph{Korteweg–De Vries equation.} The one-dimensional KdV equation is expressed as follows:
\begin{align*}
& u_t + \eta u u_x + \mu^2 u_{x x x} = 0\,, \quad t \in(0,1), \quad x \in(-1,1)\,, \\
& u(x, 0) = \cos (\pi x)\,, \\
& u(t,-1) = u(t, 1)\,,
\end{align*}
where $u$ represents the wave amplitude or water surface elevation, and $\eta$ governs the strength of the nonlinearity, while $\mu$ controls the dispersion level. Under the KdV dynamics,  this initial wave evolves into a series of solitary-type waves.

For our study, we adopt the classical parameters of the KdV equation, setting $\eta = 1$ and $\mu = 0.022$ \cite{zabusky1965interaction}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/kdv/kdv.png}
\caption{{\em Korteweg–De Vries equation.}  Top: Comparison between the reference solution and model predictions. Bottom: Training loss and test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:kdv}
\end{figure}


\paragraph{Kuramoto-Sivashinsky equation.} The one-dimensional equation takes the form:
\begin{align*}
    &u_t+\alpha u u_x+\beta u_{x x}+\gamma u_{x x x x}=0, \quad t \in[0,T], x \in[0,2 \pi], \\
    & u(0, x)=u_0(x),
\end{align*}
where $u$ represents the height of a thin film or flame front. This equation arises in various physical contexts, including flame front propagation, thin film flows, and plasma instabilities.


In this example, we take  $T=0.8$, $\alpha=100 / 16, \beta=100 / 16^2, \gamma=100 / 16^4$ and $u_0(x)=\cos (x)(1+\sin (x))$. 


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/ks/ks_loss.png}
\caption{{\em Kuramoto-Sivashinsky equation.} Training loss and test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:ks_loss}
\end{figure}



\paragraph{Grey-Scott equation.} The system is described by the following coupled PDEs:
\begin{align*}
    u_t &=\epsilon_1 \Delta u + b_1(1-u) - c_1 u v^2,  \quad t \in (0, 2)\,, \ (x, y) \in (-1, 1)^2\,, \\
    v_t &=\epsilon_2 \Delta v - b_2 v + c_2 u v^2\,, \quad t \in (0, 2)\,, \ (x, y) \in (-1, 1)^2\,,
\end{align*}
With periodic boundary conditions, the initial conditions are:
\begin{align*}
    &u_0(x, y) = 1 - \exp(-10 ((x + 0.05)^2 + (y + 0.02)^2))\,, \\
    &v_0(x, y) = 1 - \exp(-10 ((x - 0.05)^2 + (y - 0.02)^2))\,.
\end{align*}
where $u$ and $v$ represent activator and inhibitor concentrations respectively, $\varepsilon_1$ and $\varepsilon_2$ are diffusion coefficients, and $(b_1, b_2, c_1, c_2)$ control reaction kinetics. This system generates diverse spatial patterns including spots and stripes. 


We set parameters $\epsilon_1=0.2$, $\epsilon_2=0.1$, $b_1=40$, $b_2=100$, and $c_1=c_2=1,000$, which generates characteristic pattern formations. Due to the similar behavior of $u$ and $v$, we report only the relative $L^2$ error of $u$ in Table \ref{tab: sota}.



\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/gs/gs_pred_u.png}
\caption{{\em Grey-Scott equation.} Comparison between reference solution and model predictions.}
    \label{fig:gs_pred_u}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figures/gs/gs_pred_v.png}
%     \caption{Caption}
%     \label{fig:gs_pred_v}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/gs/gs_error.pdf}
\caption{{\em Grey-Scott equation.} Test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:gs_error}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/gs/gs_loss.pdf}
\caption{{\em Grey-Scott equation.} Training loss and test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:gs_loss}
\end{figure}


\paragraph{Ginzburg-Landau equation.} The complex Ginzburg-Landau equation in 2D takes the form
\begin{align*}
    \frac{\partial A}{\partial t}= \epsilon \Delta A + \mu A - \gamma  A|A|^2\,, \quad t \in (0, 1)\,,
    \ (x, y) \in (-1, 1)^2\,,
\end{align*}
with periodic boundary conditions, an initial condition
\begin{align*}
    A_0(x, y) = (10y + 10 i  x) \exp\left(-0.01 (2500 x^2 + 2500 y^2)\right)\,,
\end{align*}
where  $A$ is the complex amplitude representing the envelope of oscillations, $\epsilon$ represents the diffusion coefficient, $\mu$ is the linear growth rate, and $\gamma$ controls the nonlinear saturation. For this example, we set $\epsilon=0.004$, $\mu = 10$ and $\gamma= 10 + 15i$.


By denoting $A = u + i v$, we can decompose the equation into real and imaginary components, resulting in the following system of PDEs,
\begin{align*}
     \frac{\partial u}{\partial t} &= \epsilon \Delta u + \mu (u - (u-1.5 v) (u^2 + v^2))\,, \\
      \frac{\partial v}{\partial t} &= \epsilon \Delta v + \mu (v - (v + 1.5 u) (u^2 + v^2))\,.
\end{align*}

Given the coupled dynamics of $u$ and $v$, we present the relative $L^2$ error of $u$ in Table \ref{tab: sota}.


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/gl/gl_pred_u.png}
\caption{{\em Ginzburg-Landau equation.} Comparison between the reference solution and model predictions.}
    \label{fig:gl_pred_u}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/gl/gl_error.pdf}
\caption{{\em Ginzburg-Landau equation.} Test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:gl_error}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/gl/gl_loss.pdf}
 \caption{{\em Ginzburg-Landau equation.} Training loss trajectories for the  Adam and SOAP optimizers.}
    \label{fig:gl_loss}
\end{figure}


\paragraph{Lid-driven Cavity.} We study the incompressible Navier-Stokes equations in non-dimensional form for a two-dimensional domain:
\begin{align*}
    \mathbf{u} \cdot \nabla \mathbf{u}+\nabla p-\frac{1}{R e} \Delta \mathbf{u}&=0\,, \quad  (x,y) \in (0,1)^2\,, \\
    \nabla \cdot \mathbf{u}&=0\,, \quad  (x,y) \in (0,1)^2\,,
\end{align*}
where $\mathbf{u} = (u,v)$ represents the steady-state velocity field, $p$ is the pressure field, and $Re$ is the Reynolds number which characterizes the ratio of inertial to viscous forces.  This system models the equilibrium state of the flow, which is driven by the top boundary moving at a constant velocity while the other walls are stationary, leading to the formation of characteristic vortical structures whose complexity increases with the Reynolds number.


To ensure continuity at the corner boundaries, we implement a smoothed top-lid boundary condition:
\begin{align}
& u(x, y)=1-\frac{\cosh \left(C_0(x-0.5)\right)}{\cosh \left(0.5 C_0\right)}\,, \quad v(x, y)=0\,,
\end{align}
where $x \in [0, 1], y=1, C_0 = 50$. For the other three walls, we enforce a no-slip boundary condition. Our goal is to obtain the velocity and pressure field corresponding to a Reynolds number of $5,000$.  The accuracy of our method is evaluated using the velocity magnitude $\sqrt{u^2 + v^2}$, with results presented in Table \ref{tab: sota}.



\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/ldc/ldc_loss.png}
  \caption{{\em Lid-driven Cavity.} Training loss and test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:ldc_loss}
\end{figure}







\paragraph{Kolmogorov flow.}  We study the two-dimensional Kolmogorov flow governed by the incompressible Navier-Stokes equations:
\begin{align*}
\mathbf{u}_t + \mathbf{u} \cdot \nabla \mathbf{u} & =- \nabla p + \frac{1}{R e} \Delta \mathbf{u} + \mathbf{f}, \\
\nabla \cdot \mathbf{u} & =0,
\end{align*}
on the unit square domain $(x,y) \in [0, 1]^2$.  

Here $\mathbf{u} = (u,v)$ represents the time-varying velocity field, and $\mathbf{f}$ denotes the external forcing term that maintains the flow structure. The system evolves from a random initial state and develops characteristic flow patterns, where energy transfers between different spatial scales through nonlinear interactions and viscous dissipation.

For our study, the system is driven by a sinusoidal forcing $\mathbf{f} =(2 \sin(4 \pi y), 0)$. The numerical experiment initializes with a random initial condition and evolves until $T=2$. The model's performance is quantified by the relative $L^2$ error of vorticity (Table \ref{tab: sota}).





% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{Figures/ns_tori/ns_tori_pred_u.png}
% \caption{{\em Kolmogorov flow.} Comparison between the reference solution and model predictions.}
%     \label{fig:ns_tori_pred_u}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{Figures/ns_tori/ns_tori_pred_v.png}
% \caption{{\em Kolmogorov flow.} Comparison between reference solution and model predictions.}
%     \label{fig:ns_tori_pred_v}
% \end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Figures/kf/kf_pred_u.png}
    % \caption{Caption for figure 1}
  \end{subfigure}
  \hspace{2mm}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Figures/kf/kf_pred_v.png}
    % \caption{Caption for figure 2}
  \end{subfigure}
\caption{{\em Kolmogorov flow.} Comparison between reference solution and model predictions.}
  \label{fig:ns_tori_uv_preds}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/kf/kf_error.pdf}
 \caption{{\em Kolmogorov flow.} Test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:ns_tori_error}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/kf/kf_loss.pdf}
 \caption{{\em Kolmogorov flow.} Training loss trajectories for the Adam and SOAP optimizers.}
    \label{fig:ns_tori_loss}
\end{figure}



\paragraph{Rayleigh-Taylor instability.} We investigate a coupled flow-temperature system that models buoyancy-driven instability in a rectangular domain $(x, y) \in [0, 1] \times [0, 2]$:
    \begin{align}
        \mathbf{u}_t + \mathbf{u} \cdot \nabla \mathbf{u}  & = - \nabla p +  \sqrt{\frac{Pr}{Ra} }\Delta \mathbf{u} + T \mathbf{e}_y,  \\
        \nabla \cdot \mathbf{u} & =0,  \\
        T_t + \nabla \cdot (\mathbf{u} T)  & =  \frac{1}{\sqrt{Pr Ra}} T_{tt} 
    \end{align}
where $T$ is the temperature field (acting as a density proxy through the Boussinesq approximation),  Pr is the Prandtl number (ratio of momentum to thermal diffusivity), and Ra is the Rayleigh number (measuring buoyancy-driven flow strength). This system captures the characteristic mushroom-shaped plumes that develop as the heavier fluid penetrates into the lighter fluid below.


We set the Prandtl number $\text{Pr}=0.71$ and Rayleigh number $\text{Ra}=10^6$. The boundary conditions are periodic in the horizontal direction for both $\mathbf{u}$ and $T$, with  Dirichlet conditions $\mathbf{u} = T = 0$ imposed on the top and bottom boundaries. The accuracy of our method is evaluated using the temperature field, with results presented in Table \ref{tab: sota}.



% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{Figures/rayleigh_taylor/rayleigh_taylor_pred_u.png}
%     \caption{Caption}
%     \label{fig:rayleigh_taylor_pred}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{Figures/rayleigh_taylor/rayleigh_taylor_pred_v.png}
%     \caption{Caption}
%     \label{fig:rayleigh_taylor_pred_v}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{Figures/rayleigh_taylor/rayleigh_taylor_pred_temp.png}
%     \caption{{\em Raleigh-Taylor instability (Pr=0.71, Ra=$10^6$).} Comparison between reference numerical solutions and the model predictions trained with SOAP.}
%     \label{fig:rayleigh_taylor_pred_temp}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/rayleigh_taylor/rayleigh_taylor_pred_uv.png}
\caption{{\em Rayleigh-Taylor instability.} Comparison between reference solution and model predictions.}
    \label{fig:rayleigh_taylor_pred_uv}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{Figures/rayleigh_taylor/rayleigh_taylor_error.pdf}
 \caption{{\em Rayleigh-Taylor instability.} Test error trajectories for the Adam and SOAP optimizers.}
    \label{fig:rayleigh_taylor_error}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/rayleigh_taylor/rayleigh_taylor_loss.pdf}
 \caption{{\em Rayleigh-Taylor instability.} Training loss trajectories for the Adam and SOAP optimizers.}
    \label{fig:rayleigh_taylor_loss}
\end{figure}









