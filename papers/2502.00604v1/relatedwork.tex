\section{Related Work}
\label{sec: related_work}

Physics-informed neural networks (PINNs) have emerged as a powerful paradigm in scientific machine learning by incorporating physical principles through carefully designed loss functions. These loss functions act as soft constraints, guiding neural networks to learn solutions that respect underlying physical laws while simultaneously fitting experimental data. The elegance and versatility of PINNs have led to their widespread adoption in solving both forward and inverse problems involving partial differential equations (PDEs). Their success spans numerous domains in computational science, from fluid mechanics \cite{raissi2020hidden,almajid2022prediction,eivazi2022physics,cao2024surrogate}, heat transfer \cite{xu2023physics,bararnia2022application,gokhale2022physics} to bio-engineering \cite{kissas2020machine,zhang2023physics,caforio2024physics} and materials science \cite{zhang2022analyses,jeong2023physics, hu2024physics}. The impact of PINNs extends even further, with applications in electromagnetics \cite{kovacs2022conditional,khan2022physics,baldan2023physics}, geosciences \cite{smith2022hyposvi, song2023simulating,ren2024seismicnet}, etc.


Despite their broad applications, PINNs currently face limitations in convergence speed and accuracy that affect their reliability as forward PDE solvers. This has motivated extensive research efforts to enhance their performance through various methodological innovations.  One prominent line of research focuses on developing self-adaptive loss weighting schemes to address unbalanced back-propagated gradients during training \cite{wang2021understanding,wang2022and}. While various strategies have been proposed \cite{wang2023expert,wang2021understanding,wang2022and,li2022revisiting,chen2024self,anagnostopoulos2024residual,liu2024discontinuity,song2025vw}, they primarily address gradient magnitude imbalances rather than directional conflicts, with a recent exception being \cite{liu2024config} whose effectiveness is hereby shown to be limited. Other advances include architectural innovations \cite{wang2021understanding,sitzmann2020implicit,fathony2021multiplicative,moseley2021finite,kang2022pixel,cho2024separable,wang2024piratenets}, improved training processes \cite{nabian2021efficient,daw2022rethinking,wu2023comprehensive,muller2023achieving,jnini2024gauss,song2024admm,urban2025unveiling,wight2020solving,krishnapriyan2021characterizing,cao2023tsonn}, and alternative learning objectives \cite{chiu2022can,huang2024efficient,kharazmi2021hp,patel2022thermodynamically,yu2022gradient,son2021sobolev}. However, the fundamental challenge of resolving directional gradient conflicts remains largely unaddressed.