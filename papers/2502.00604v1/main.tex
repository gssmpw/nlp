\documentclass{article}

\usepackage{PRIMEarxiv, cite}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}   

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{wrapfig}
\usepackage{siunitx }
\usepackage{bigints}
\usepackage{bm}
\usepackage{diagbox}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}

% \include{pythonlisting}


\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{enumitem}

\usepackage{authblk}  % For author/affiliation blocks
\usepackage{textcomp} % For \textsuperscript
\usepackage{ragged2e} % For \raggedright


\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor=black,
    linkcolor=black,
    filecolor=black,
    urlcolor=black,
}

\NewDocumentCommand\pirateflag{}{
    \includegraphics[scale=0.1]{pirate-flag.png}
}

\usepackage{pifont}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}

\usepackage[capitalise]{cleveref}

%A bunch of definitions that make my life easier
\newcommand{\bproof}{\bigskip {\bf Proof. }}
\newcommand{\eproof}{\hfill\qedsymbol}
\newcommand{\matlab}{{\sc Matlab} }
\newcommand{\cvec}[1]{{\mathbf #1}}
\newcommand{\rvec}[1]{\vec{\mathbf #1}}
\newcommand{\ihat}{\hat{\textbf{\i}}}
\newcommand{\jhat}{\hat{\textbf{\j}}}
\newcommand{\khat}{\hat{\textbf{k}}}
\newcommand{\minor}{{\rm minor}}
\newcommand{\trace}{{\rm trace}}
\newcommand{\spn}{{\rm Span}}
\newcommand{\rem}{{\rm rem}}
\newcommand{\ran}{{\rm range}}
\newcommand{\range}{{\rm range}}
\newcommand{\mdiv}{{\rm div}}
\newcommand{\proj}{{\rm proj}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\attn}[1]{\textbf{#1}}

\newcommand{\Disp}{\displaystyle}
\newcommand{\qe}{\hfill\(\bigtriangledown\)}


\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}


% add
\newcommand{\x}{\mathbf{x}}
\newcommand{\xx}{\mathbf{X}}
\newcommand{\z}{\mathbf{z}}
\def \ba {\mathbf{a}}
\def \bw {\mathbf{w}}
\def \bww {\mathbf{W}}
\def \si {\sigma}
\newcommand{\eps}{\epsilon}
\newcommand{\lad}{\lambda}
\newcommand{\vr}{\varrho}
\newcommand{\dd}{\cdot}
\newcommand{\ti}{\times}

\def \ep {\varepsilon}
\def \ww {\omega}
% \def \l {\langle} 
% \def \r {\rangle}
\def \d {\delta}
\def \vp {\varphi}

\def \mc {\mathcal}
\newcommand{\norm}[1]{{\left\lVert #1 \right\lVert}}
\def \rd {{\rm d}}
\def \h {\hat}
\def \p {\partial}
\def \bb {\mathbf}
\def \q {\quad}
\newcommand{\na}{\nabla}

\IfFileExists{mathabx.sty}%
  {\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}%
   \DeclareFontShape{U}{mathx}{m}{n}{<->mathx10}{}%
   \DeclareSymbolFont{mathx}{U}{mathx}{m}{n}%
   \DeclareFontSubstitution{U}{mathx}{m}{n}%
   \DeclareMathAccent{\widebar}{0}{mathx}{"73}%
}{%
  \PackageWarning{mathabx}{%
    Package mathabx not available, therefore\MessageBreak substituting
    widebar with overline\MessageBreak }%
  \newcommand{\widebar}[1]{\overline{#1}}%
}
\newcommand{\wb}[1]{\widebar{#1}}
\DeclareMathOperator{\sgn}{sgn}

\def \Vec {{\rm vec}}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}


\newtheorem{remark}{Remark}

\numberwithin{equation}{section}

\newcommand{\email}[1]{\texttt{{#1}}}

\usepackage{etoolbox}
\makeatletter
\patchcmd{\maketitle}
 {\def\@makefnmark}
 {\def\@makefnmark{}\def\useless@macro}
 {}{}
\makeatother

% \usepackage{emoji}
  
%% Title
\title{Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective
}

% \author{
%   Sifan Wang\textsuperscript{1,*} \and
%   Ananyae Kumar Bhartari\textsuperscript{2,*} \and
%   Bowen Li\textsuperscript{3,*} \and
%   Paris Perdikaris\textsuperscript{2}
% }

% \affil{
%   \textsuperscript{1}Institution for Foundation of Data Science, Yale University\\
%   \textsuperscript{2}Department of Mechanical Engineering and Applied Mechanics, University of Pennsylvania\\
%   \textsuperscript{3}Department of Mathematics, City University of Hong Kong
% }




\author[1,*]{Sifan Wang}
\author[2, *]{Ananyae Kumar Bhartari}
\author[3, *]{Bowen Li}
\author[2]{Paris Perdikaris}
\affil[1]{Institution for Foundation of Data Science, Yale University}
\affil[2]{Department of Mechanical Engineering and Applied Mechanics, University of Pennsylvania}
\affil[3]{Department of Mathematics, City University of Hong Kong}

\renewcommand\Authands{ and }

\renewcommand\Authands{ }
\renewcommand\Authand{ }
\renewcommand\Authfont{\bfseries}
\renewcommand\Affilfont{\mdseries}
% \texttt{bowen.li@cityu.edu.hk}, \texttt{\{ananyae,pgp\}@seas.upenn.edu}}



\begin{document}
\maketitle


% \thanks{These authors contributed equally to this work.}

\vspace{-15mm}

\email{
  \texttt{sifan.wang@yale.edu, ananyae@seas.upenn.edu, bowen.li@cityu.edu.hk, pgp@seas.upenn.edu}
}
\vspace{10mm}



\blfootnote{* These authors contributed equally to this work.}









\begin{abstract}
Multi-task learning through composite loss functions is fundamental to modern deep learning, yet optimizing competing objectives remains challenging. We present new theoretical and practical approaches for addressing directional conflicts between loss terms, demonstrating their effectiveness in physics-informed neural networks (PINNs) where such conflicts are particularly challenging to resolve. Through theoretical analysis, we demonstrate how these conflicts limit first-order methods and show that second-order optimization naturally resolves them through implicit gradient alignment. We prove that SOAP, a recently proposed quasi-Newton method, efficiently approximates the Hessian preconditioner, enabling breakthrough performance in PINNs: state-of-the-art results on 10 challenging PDE benchmarks, including the first successful application to turbulent flows with Reynolds numbers up to 10,000, with 2-10x accuracy improvements over existing methods. We also introduce a novel gradient alignment score that generalizes cosine similarity to multiple gradients, providing a practical tool for analyzing optimization dynamics. Our findings establish frameworks for understanding and resolving gradient conflicts, with broad implications for optimization beyond scientific computing.

% Training physics-informed neural networks (PINNs) remains challenging due to competing optimization objectives that often impede convergence. While existing research has primarily focused on balancing gradient magnitudes, we identify and analyze a more fundamental challenge: directional conflicts between gradients of different loss terms that force the optimization to follow inefficient trajectories. We introduce a novel gradient alignment score that quantifies these conflicts and reveals their pervasive nature in PINNs training. Our theoretical analysis shows that second-order optimization is necessary to effectively resolve these conflicts through implicit gradient alignment via preconditioned updates. Among scalable quasi-second-order methods, we demonstrate that SOAP offers a particularly effective plug-and-play solution, supported by both theoretical guarantees and empirical evidence. Our approach achieves state-of-the-art accuracy on 10 challenging PDE benchmarks, including turbulent flows with Reynolds numbers up to 10,000, delivering about 2-10x improvement in predictive accuracy over existing methods. Beyond PINNs, our findings provide new insights into the optimization dynamics of multi-task learning problems, paving the way for more reliable deployment of deep learning in scientific and engineering applications.




\end{abstract}

\section{Introduction}




% Introduction

% -- Start with the broad impact and applications of PINNs
% -- Identify the key challenge: Gradient conflicts in PINNs training
% Preview our main contributions:
%   -- Novel gradient alignment metrics
%   -- Theoretical analysis linking second-order optimization to gradient alignment
%   -- Empirical demonstration of SOAP's effectiveness
%   -- State-of-the-art results on challenging PDEs

Multi-task learning through composite loss functions has become a cornerstone of modern deep learning, from computer vision to scientific computing. However, when different loss terms compete for model capacity, they can generate conflicting gradients that impede optimization and degrade performance. While this fundamental challenge is known to the multi-task learning literature \cite{yu2020gradient,liu2021conflict,shi2023recon}, several challenges remain open, particularly in settings where objectives are tightly coupled through complex physical constraints.

In this work, we examine gradient conflicts through the lens of physics-informed neural networks (PINNs), where the challenge manifests acutely due to the inherent coupling between physical constraints and data-fitting objectives. Our key insight is that while first-order optimization methods struggle with competing objectives, appropriate preconditioning can naturally align gradients to enable efficient optimization. While our findings on gradient alignment and second-order preconditioning have broad implications for multi-task learning, here we focus on PINNs as they provide an ideal testbed: their physically-constrained objectives are mathematically precise, their solutions can be rigorously verified, and their performance bottlenecks are well-documented. Through theoretical analysis and extensive experiments on challenging partial differential equations (PDEs), we demonstrate breakthrough results in problems ranging from basic wave propagation to turbulent flows.

% To understand why PINNs provide such a compelling case study, consider their fundamental formulation: they incorporate physical principles through carefully designed loss functions that act as soft constraints, guiding neural networks to learn solutions that respect underlying physical laws while simultaneously fitting experimental data. The simplicity and versatility of this approach has led to their widespread adoption in solving both forward and inverse problems involving PDEs. Their success spans numerous domains in computational science, from fluid mechanics \cite{raissi2020hidden,almajid2022prediction,eivazi2022physics,cao2024surrogate}, heat transfer \cite{xu2023physics,bararnia2022application,gokhale2022physics} to bio-engineering \cite{kissas2020machine,zhang2023physics,caforio2024physics} and materials science \cite{zhang2022analyses,jeong2023physics, hu2024physics}. The impact of PINNs extends even further, with applications in electromagnetics \cite{kovacs2022conditional,khan2022physics,baldan2023physics}, geosciences \cite{smith2022hyposvi, song2023simulating,ren2024seismicnet}, etc.

To better motivate our approach, consider training a PINN to solve the Navier-Stokes equations. The model must simultaneously satisfy boundary conditions, conservation laws, and empirical measurements -- objectives that often push a neural network's parameters in opposing directions. Traditional methods like Adam or gradient descent struggle as they can only follow the average gradient direction, leading to slow convergence or poor solutions. In contrast, second-order methods can identify and resolve these conflicts through implicit gradient alignment, enabling more efficient optimization.
To this end, the key contributions of this work are:
\begin{itemize}[itemsep=1mm, topsep=1mm, parsep=1mm, leftmargin=*]
    \item A novel gradient alignment metric that extends cosine similarity to quantify directional conflicts between multiple loss terms, providing a systematic tool for analyzing multi-task optimization dynamics.
    \item Systematic analysis demonstrating that gradient conflicts are a fundamental barrier in PINNs training, with quantitative evidence linking higher conflict scores to slower convergence across diverse PDE systems.
    \item Theoretical characterization of how different optimizers handle gradient conflicts, proving that second-order methods inherently promote gradient alignment through implicit preconditioning of the loss landscape.
    \item Analysis showing that SOAP \cite{vyas2024soap}, a scalable quasi-Newton method, provides an efficient approximation to the optimal Newton preconditioner while remaining computationally tractable, explaining its effectiveness in resolving gradient conflicts.
    \item Breakthrough experimental results across 10 challenging PDE benchmarks, including the first successful application of PINNs to turbulent flows with Reynolds numbers up to 10,000, achieving 2-10x improvement in accuracy over existing methods.
\end{itemize}
Taken together, this work advances our understanding of optimization dynamics in PINNs while demonstrating how quasi second-order methods can enable more reliable neural PDE solvers for solving complex physical systems. These insights pave the way for developing next-generation optimizers for physics-informed machine learning, and beyond.

% Despite their broad applications, PINNs currently face limitations in convergence speed and accuracy that affect their reliability as forward PDE solvers. This has motivated extensive research efforts to enhance their performance through various methodological innovations. Significant advances have emerged in neural architecture design, including novel network structures \cite{wang2021understanding,sitzmann2020implicit,fathony2021multiplicative,moseley2021finite,kang2022pixel,cho2024separable,wang2024piratenets}, improved activation functions \cite{jagtap2020adaptive,abbasi2024physical}, and effective positional embeddings \cite{wang2021eigenvector,costabal2024delta,zeng2024rbf}.
% Other improvements have focused on optimizing the training process through enhanced collocation point sampling strategies \cite{nabian2021efficient,daw2022rethinking,wu2023comprehensive}, more efficient optimizers \cite{muller2023achieving,jnini2024gauss,song2024admm,urban2025unveiling}, and advanced training strategies such as sequential training \cite{wight2020solving,krishnapriyan2021characterizing,cao2023tsonn} and transfer learning \cite{desai2021one,goswami2020transfer,chakraborty2021transfer}.



% Physics-informed neural networks (PINNs) have emerged as a powerful paradigm in scientific machine learning by incorporating physical principles through carefully designed loss functions. These loss functions act as soft constraints, guiding neural networks to learn solutions that respect underlying physical laws while simultaneously fitting experimental data. The elegance and versatility of PINNs have led to their widespread adoption in solving both forward and inverse problems involving partial differential equations (PDEs). Their success spans numerous domains in computational science, from fluid mechanics \cite{raissi2020hidden,almajid2022prediction,eivazi2022physics,cao2024surrogate}, heat transfer \cite{xu2023physics,bararnia2022application,gokhale2022physics} to bio-engineering \cite{kissas2020machine,zhang2023physics,caforio2024physics} and materials science \cite{zhang2022analyses,jeong2023physics, hu2024physics}. The impact of PINNs extends even further, with applications in electromagnetics \cite{kovacs2022conditional,khan2022physics,baldan2023physics}, geosciences \cite{smith2022hyposvi, song2023simulating,ren2024seismicnet}, etc.


% Despite their broad applications, PINNs currently face limitations in convergence speed and accuracy that affect their reliability as forward PDE solvers. This has motivated extensive research efforts to enhance their performance through various methodological innovations. Significant advances have emerged in neural architecture design, including novel network structures \cite{wang2021understanding,sitzmann2020implicit,fathony2021multiplicative,moseley2021finite,kang2022pixel,cho2024separable,wang2024piratenets}, improved activation functions \cite{jagtap2020adaptive,abbasi2024physical}, and effective positional embeddings \cite{wang2021eigenvector,costabal2024delta,zeng2024rbf}.
% Other improvements have focused on optimizing the training process through enhanced collocation point sampling strategies \cite{nabian2021efficient,daw2022rethinking,wu2023comprehensive}, more efficient optimizers \cite{muller2023achieving,jnini2024gauss,song2024admm,urban2025unveiling}, and advanced training strategies such as sequential training \cite{wight2020solving,krishnapriyan2021characterizing,cao2023tsonn} and transfer learning \cite{desai2021one,goswami2020transfer,chakraborty2021transfer}.
% Researchers have also explored alternative formulations of the learning objective, incorporating numerical differentiation \cite{chiu2022can,huang2024efficient}, variational principles inspired by Finite Element Methods \cite{kharazmi2021hp,patel2022thermodynamically}, and specialized regularization terms \cite{yu2022gradient,son2021sobolev}.


% Particularly, a significant line of research in improving PINNs has focused on developing self-adaptive loss weighting schemes to address gradient pathologies during training  \cite{wang2021understanding,wang2022and}. These pathologies manifest as unbalanced backpropagated gradients of individual losses and large discrepancies in convergence rates, which significantly impact the convergence and accuracy of PINNs in solving complex physical systems.
% While various adaptive weighting strategies have been proposed \cite{wang2021understanding,wang2022and,li2022revisiting,chen2024self,anagnostopoulos2024residual,liu2024discontinuity,song2025vw}, they primarily address gradient magnitude imbalances, leaving the critical issue of directional gradient conflicts largely unexplored. 
% Our work aims to bridge this gap by investigating, analyzing, and resolving these directional conflicts in PINNs training. The key contributions of this work are summarized as follows:
% \begin{itemize}[leftmargin=*]
%     \item We introduce a novel metric, the gradient alignment score, to quantify directional gradient conflicts in multi-task learning, which naturally extends cosine similarity to multiple gradients.

%     \item We reveal that directional gradient conflicts are a widespread phenomenon in PINNs training, occurring across various optimizers and PDEs, and  their strong correlation with slow convergence in test error.

%     \item We theoretically analyze popular optimizers and their connections to second-order methods, showing their susceptibility to gradient conflicts, particularly during early training stages.

%     \item We demonstrate that SOAP approximates the block-diagonal Hessian as a preconditioner for gradient descent, effectively leveraging second-order information to resolve directional gradient conflicts during training.

%     \item We validate the effectiveness of  SOAP  through comprehensive experiments, achieving state-of-the-art results across 10 challenging PDE benchmarks.
% \end{itemize}


% The remainder of this paper is organized as follows. Section \ref{sec: method_analysis} introduces the PINN framework and identifies a critical challenge: directional gradient conflicts, which we quantify using a novel gradient alignment score. We then analyze several popular optimizers through the lens of second-order methods, demonstrating that SOAP approximates Newton's method and naturally promotes gradient alignment. In Section \ref{sec:results}, we present extensive experimental results across a wide range of PDE benchmarks, showing SOAP's significant improvements over existing methods. Finally, Section \ref{sec: conclusion} summarizes our findings and discusses broader impacts.


\section{Problem Formulation}
\label{sec: background}

\paragraph{Overview.} \label{subsec: pinns}
Multi-task learning in deep neural networks requires simultaneously minimizing multiple competing objectives -- a challenge that manifests acutely in physics-informed neural networks (PINNs). Building upon the work of \cite{raissi2019physics}, PINNs approximate solutions to partial differential equations by minimizing a composite loss function that enforces both physical constraints and data-fitting objectives. Consider a general PDE system: 
% \vspace{-10mm}
\begin{align}
\label{eq: PDE}
     &\mathbf{u}_t +  \mathcal{N}[\mathbf{u}] = 0, \ \  t \in [0, T],  \ \mathbf{x} \in \Omega, 
\end{align}
with inital and boundary conditions 
\begin{align}
 \label{eq: IC}
     &\mathbf{u}( 0, \mathbf{x})=\mathbf{g}(\mathbf{x}), \ \ \mathbf{x} \in \Omega, \\
  \label{eq: BC}
 &\mathcal{B}[\mathbf{u}] = 0,  \ \   t\in [0, T], \  \mathbf{x} \in  \partial \Omega,
\end{align}
where $\mathcal{N}[\cdot]$ represents a differential operator and $\mathcal{B}[\cdot]$ denotes boundary conditions. The core idea of PINNs is to approximate the solution $\mathbf{u}(t, \mathbf{x})$ using a neural network $\mathbf{u}_{\mathbf{\theta}}(t, \mathbf{x})$. Through automatic differentiation \cite{griewank2008evaluating}, we can compute the PDE residual: 
\begin{align}
    \label{eq: pde_residual}
    \mathcal{R}[u_{\mathbf{\theta}}](t, \mathbf{x}) = \frac{\partial \mathbf{u}_{\mathbf{\theta}}}{\partial t}(t_r, \mathbf{x}_r) + \mathcal{N}[\mathbf{u}_{\mathbf{\theta}}](t_r, \mathbf{x}_r).
\end{align} 
This leads to a composite loss function that encapsulates multiple competing objectives:
\begin{align}
\label{eq: PINN_loss}
\mathcal{L}(\theta)=\underbrace{\frac{1}{N_{i c}} \sum_{i=1}^{N_{i c}}\left|\mathbf{u}_\theta\left(0, \mathbf{x}_{i c}^i\right)-\mathbf{g}\left(\mathbf{x}_{i c}^i\right)\right|^2}_{\mathcal{L}_{i c}(\theta)} 
+\underbrace{\frac{1}{N_{b c}} \sum_{i=1}^{N_{b c}}\left|\mathcal{B}\left[\mathbf{u}_\theta\right]\left(t_{b c}^i, \mathbf{x}_{b c}^i\right)\right|^2}_{\mathcal{L}_{b c}(\theta)}
+\underbrace{\frac{1}{N_r} \sum_{i=1}^{N_r}\left|\mathcal{R}\left[\mathbf{u}_\theta\right]\left(t_r^i, \mathbf{x}_r^i\right)\right|^2}_{\mathcal{L}_r(\theta)}.
\end{align}
% \begin{align} 
% \label{eq: PINN_loss}
% \mathcal{L}(\theta) = \mathcal{L}_{i c}(\theta) + \mathcal{L}_{b c}(\theta) + \mathcal{L}_{r}(\theta)
% \end{align}
% where  
% \begin{align}
%     \mathcal{L}_{i c}(\theta) &=\frac{1}{N_{i c}} \sum_{i=1}^{N_{i c}}\left|\mathbf{u}_\theta\left(0, \mathbf{x}_{i c}^i\right)-\mathbf{g}\left(\mathbf{x}_{i c}^i\right)\right|^2, \\
%     \mathcal{L}_{b c}(\theta) &=\frac{1}{N_{b c}} \sum_{i=1}^{N_{b c}}\left|\mathcal{B}\left[\mathbf{u}_\theta\right]\left(t_{b c}^i, \mathbf{x}_{b c}^i\right)\right|^2, \\ 
%     \mathcal{L}_{r}(\theta) &=\frac{1}{N_r} \sum_{i=1}^{N_r}\left|\mathcal{R}\left[\mathbf{u}_\theta\right]\left(t_r^i, \mathbf{x}_r^i\right)\right|^2.
% \end{align}  
These loss functions aim to fit the initial, boundary conditions and the PDE residuals, respectively. And 
$\{\mathbf{x}_{ic}^i\}_{i=1}^{N_{ic}}$, $\{t_{bc}^i, \mathbf{x}_{bc}^i\}_{i=1}^{N_{bc}}$ and $\{t_{r}^i, \mathbf{x}_{r}^i\}_{i=1}^{N_{r}}$ may be selected either as fixed mesh vertices or through random sampling during each training iteration.

Here lies the fundamental challenge: these different loss terms frequently work against each other during optimization. Consider the Navier-Stokes equations -- enforcing no-slip boundary conditions requires precise control of velocity gradients near walls, which can conflict with maintaining conservation of mass and momentum in the bulk flow. When such conflicts occur, first-order methods like gradient descent or Adam can only follow the average gradient direction, leading to inefficient optimization trajectories that zigzag between competing objectives. The severity of these conflicts increases with problem complexity, becoming particularly acute for turbulent flows where maintaining physical constraints across multiple scales is crucial.

% \paragraph{Related Work.} 
% The challenges in training PINNs have motivated extensive methodological innovations. One prominent line of research focuses on developing self-adaptive loss weighting schemes to address unbalanced back-propagated gradients during training \cite{wang2021understanding,wang2022and}. While various strategies have been proposed \cite{wang2023expert,wang2021understanding,wang2022and,li2022revisiting,chen2024self,anagnostopoulos2024residual,liu2024discontinuity,song2025vw}, they primarily address gradient magnitude imbalances rather than directional conflicts, with a recent exception being \cite{liu2024config} whose effectiveness is hereby shown to be limited. Other advances include architectural innovations \cite{wang2021understanding,sitzmann2020implicit,fathony2021multiplicative,moseley2021finite,kang2022pixel,cho2024separable,wang2024piratenets}, improved training processes \cite{nabian2021efficient,daw2022rethinking,wu2023comprehensive,muller2023achieving,jnini2024gauss,song2024admm,urban2025unveiling,wight2020solving,krishnapriyan2021characterizing,cao2023tsonn}, and alternative learning objectives \cite{chiu2022can,huang2024efficient,kharazmi2021hp,patel2022thermodynamically,yu2022gradient,son2021sobolev}. However, the fundamental challenge of resolving directional gradient conflicts remains largely unaddressed.

\paragraph{Main Idea.} Here we resolve these challenges following three key steps. First, we develop a gradient alignment score that quantifies conflicts between different loss terms, providing a systematic way to analyze optimization dynamics. Second, we prove that second-order optimization methods can naturally resolve these conflicts through implicit gradient alignment via preconditioned updates. Finally, we demonstrate that SOAP \cite{vyas2024soap} offers an elegant and computationally tractable approximation to the optimal preconditioner, enabling breakthrough performance on challenging PDEs including turbulent flows.


% Background and Problem Formulation

% -- Brief overview of PINNs framework
% -- Clear mathematical setup of the optimization problem

% Introduce the two types of gradient conflicts:
% -- Type I: Magnitude imbalance (known issue)
% -- Type II: Directional conflicts (novel insight)

% \subsection{Overview of Physics-informed Neural Networks} \label{subsec: pinns}

\section{Methods}

% \subsection{Overview and Problem Formulation} \label{subsec: pinns}

% This section develops our analysis formally. First, we introduce a gradient alignment score that quantifies conflicts between different loss terms, providing a systematic way to analyze optimization dynamics. Second, we prove that second-order optimization methods can naturally resolve these conflicts through implicit gradient alignment via preconditioned updates. Finally, we demonstrate that SOAP offers an elegant and computationally tractable approximation to the optimal preconditioner, enabling breakthrough performance on challenging PDEs including turbulent flows.



\paragraph{Gradient Alignment in PINNs.} \label{subsec:gradconflict}



% Gradient Alignment Analysis

% -- Formal definition of gradient alignment score
% Properties and interpretation
%   -- Two variants: intra-step and inter-step alignment
%   -- Empirical evidence showing prevalence of conflicts in PINNs


\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-7mm}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{Figures/loss_landscape.png}
  \end{center}
  \vspace{-1mm}
\caption{{Gradient conflicts and their impact on PINNs optimization. The irregular green trajectory illustrates how the optimization struggles when facing two types of gradient conflicts: Type I, where gradients have similar directions but vastly different magnitudes, and Type II, where gradients have similar magnitudes but opposing directions. The red trajectory shows how appropriate preconditioning through second-order information could mitigate these conflicts by aligning gradients both within and between optimization steps, enabling efficient convergence.}}
  \label{fig:loss_landscape}
\vspace{-5mm}
\end{wrapfigure}

PINNs face a fundamental challenge of competing gradients during training, which manifests in two distinct modes, see Figure \ref{fig:loss_landscape}. The first mode, identified by \cite{wang2021understanding,wang2022and}, involves back-propagated gradients of significantly different magnitudes. When these magnitude imbalances occur, certain loss terms dominate the optimization process, leading to model failure. While this challenge has been partially addressed through self-adaptive weighting schemes \cite{li2022revisiting,chen2024self,anagnostopoulos2024residual,liu2024discontinuity,song2025vw}, a second, more fundamental mode of gradient conflict remains less explored.

This second mode occurs when gradients from different loss terms point in opposing directions, forcing the optimization to follow inefficient trajectories. Traditional scaling-based approaches cannot resolve these directional conflicts, which become particularly severe in complex PDE systems where multiple physical constraints must be simultaneously satisfied. To systematically study and address this challenge, here we introduce a new metric called the {\em alignment score}, defined as follows.
%
\begin{definition} \label{def:gradalign}
Suppose that $v_1, v_2, \dots, v_n$ are vectors, then the  alignment score is defined as
    \begin{align}
       \mathcal{A}(v_1, v_2, \dots, v_n)=2 \left\|\frac{\sum_{i=1}^n \frac{v_i}{\left\|v_i\right\|}}{n}\right\|^2 - 1.
    \end{align}
\end{definition}
This score ranges from $[-1,1]$ and naturally extends the concept of cosine similarity to multiple vectors. As illustrated in Proposition \ref{prop1}, for the special case of $n=2$, our score exactly recovers the standard cosine similarity $\text{cos}(v_1, v_2) = \frac{v_1 \cdot v_2}{\|v_1\| \|v_2\|}$, where 1 indicates perfect alignment, 0 suggests orthogonal directions, and -1 represents complete opposition.

\begin{proposition}
    \label{prop1}
     For n=2, the alignment score $\mathcal{A}(v_1, v_2)$ equals the cosine similarity between $v_1$ and $v_2$:
\begin{align}
\mathcal{A}(v_1, v_2) = \cos(v_1, v_2) = \frac{v_1 \dd v_2}{\|v_1\|\|v_2\|}\,.
\end{align}
\end{proposition} 
The proof is provided in Appendix \ref{proof: prop1}. The alignment score enables us to quantify both the local conflicts between individual loss terms within each gradient descent step and the global conflicts across consecutive steps. Formally:
\begin{definition}
Let $\mathcal{L} = \sum_{i=1}^n \mathcal{L}_i$ be a composite loss function. At the $k$-th step of gradient descent, let $g^k$ denote the full gradient and $g_1^k, g_2^k, \dots, g_n^k$ denote the gradients of individual loss terms. We define:
\begin{enumerate}[label=(\alph*),itemsep=1mm, topsep=1mm, parsep=1mm]
    \item The intra-step gradient alignment score:
\begin{align}
\label{eq: intra_align}
\mathcal{A}_{intra}^k = \mathcal{A}(g_1^k, g_2^k, \dots, g_n^k).
\end{align}
  \vspace{-2em}
\item The inter-step gradient alignment score:
\begin{align}
\label{eq: inter_align}
\mathcal{A}_{inter}^k = \mathcal{A}(g^{k-1}, g^k).
\end{align} 
   \vspace{-2em}
\end{enumerate}
\end{definition}
In the following, we will demonstrate that gradient direction conflicts widely exist in training PINNs, especially in the early stages of training.  To this end, we conduct experiments on five representative PDEs spanning from linear wave propagation to reaction-diffusion systems like the Ginzburg-Landau equation and fluid dynamics governed by the Navier-Stokes equations.  The detailed experimental setup is provided in Appendix \ref{appendix: experiments}.


% Our experimental setup follows the training pipeline established by Wang \emph{et al.}  \cite{wang2023expert}. We employ a random Fourier feature network with four hidden layers, each containing 128 neurons and hyperbolic tangent (Tanh) activation functions. The models are trained with a batch size of $1,024$ over $10^4$ steps of gradient descent. We set the initial learning rate to $10^{-3}$, and use an exponential decay rate of $0.9$  every $2,000$ steps. Particularly, we incorporate learning rate annealing \cite{wang2021understanding,wang2023expert} and causal training algorithms \cite{wang2022respecting, wang2023expert} to address the gradient scale conflicts and enhance model performance and robustness. 


Figure \ref{fig:grad_align_score} visualizes the evolution of inter-step and intra-step gradient alignment scores alongside the error convergence during training with different common optimizers.
In all cases except SOAP, we observe that both scores oscillate significantly near or below zero, providing strong evidence for persistent directional conflicts between gradients throughout the training process. Intuitively, these conflicting gradients force the network parameters to follow an inefficient zigzag trajectory in the loss landscape, significantly impeding convergence speed.

In contrast, the SOAP optimizer consistently maintains the highest positive values for both inter-step and intra-step gradient alignment scores throughout training. This effective resolution of gradient direction conflicts directly corresponds to significantly faster convergence in test error.

The following section analyzes SOAP's theoretical foundations, focusing on two critical aspects that illuminate the gradient conflict problem in PINNs. We first demonstrate why conventional first-order optimizers fundamentally struggle with directional conflicts, then show how SOAP's second-order preconditioning naturally resolves these challenges. For ease of exposition, all the proofs supporting this analysis can be found in Appendices \ref{app:soap_newton} and \ref{sec:gradientconflict}.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/grad_align_score.png}
     \vspace{-7mm}
    \caption{\small{Gradient alignment scores and test errors obtained by training PINNs with different optimizers across different PDEs. From left to right: ground truth PDE solution, intra-step gradient alignment scores (Eq. \eqref{eq: intra_align}), inter-step gradient alignment scores (Eq. \eqref{eq: inter_align}), and test error convergence during training.}}
    \label{fig:grad_align_score}
     \vspace{-2mm}
\end{figure}

\paragraph{Inter-step Gradient Alignment.} \label{sec:2ndopt}

% Second-Order Optimization and Gradient Alignment

% -- Theoretical connection between Newton's method and gradient alignment
% -- Analysis of how second-order methods promote positive alignment
% -- Comparison of different approximation strategies:
%   -- Full Newton (impractical but ideal)
%   -- Adagrad/Adam (diagonal approximation)
%   -- Shampoo (Kronecker factored)
%   -- SOAP (our focus)

Having preliminarily demonstrated SOAP's empirical effectiveness, we now analyze its mechanism through the lens of second-order optimization.  Given the model parameters $\theta_t \in \mathbb{R}^p$ at iteration $t$, and the gradient $g_t \in \mathbb{R}^p$ of the loss function with respect to $\theta_t$,
the Newton update is $\theta_{t+1} = \theta_t - H^{-1} g_t$, where $H \in \R^{p \times p}$ is the Hessian matrix of the loss function, providing a preconditioner that scales the gradient by the local curvature information to achieve faster convergence than first-order methods. By the following lemma, we point out that second-order methods, through their preconditioning matrices, naturally induce positive inter-gradient alignment.
\begin{lemma}
\label{lemma: inter_step_gradient_align}
Let $\mathcal{L}(\theta): \mathbb{R}^p \rightarrow \mathbb{R}$ be a twice differentiable loss function with Hessian $H(\theta)$. Assume that the inverse of Hessian is uniformly bounded. 
For the Newton iteration with a learning rate $\eta > 0$:
\begin{align}
\theta_{t+1} = \theta_t - \eta H^{-1}(\theta_t) g_t\,,
\end{align}
with $g_t$ being the gradient at $\theta_t$, the inter-step gradient alignment score satisfies
\begin{align}
    \mathcal{A}(g_t, g_{t+1}) =
    1 + O(\eta^2\|g_t\|)\,.
\end{align}
\end{lemma}
%
For sufficiently small learning rates, Lemma \ref{lemma: inter_step_gradient_align} shows that Hessian preconditioning naturally promotes gradient alignment across optimization steps. This theoretical result aligns with our empirical observations of SOAP's behavior in Figure \ref{fig:grad_align_score}, suggesting a deeper connection between SOAP and Newton's method that we will explore in subsequent sections.

Recent work on practical second-order optimization has focused on developing efficient preconditioners for large neural networks trained with mini-batches. Adagrad \cite{duchi2011adaptive} introduced the fundamental idea of using accumulated gradient covariance as a preconditioner $H_{\text {Ada }}=\sum_{s = 1}^t g_s g_s^{\top}$, leading to the update rule:
\begin{align}
\theta_{t+1}=\theta_t-\eta H_{\mathrm{Ada}}^{-1 / 2} g_t.
\end{align}
This approach has theoretical connections to the Hessian matrix \cite{hazan2007logarithmic,duchi2011adaptive} and has inspired several extensions including K-FAC \cite{martens2015optimizing}, GGT \cite{agarwal2018case}, and Shampoo \cite{gupta2018shampoo}.

Shampoo \cite{gupta2018shampoo} further develops this idea by approximating $H_{\text {Ada }}$ through Kronecker factorization. For a layer with weight matrix $W_t$ and gradient $G_t \in \R^{m \times n}$, Shampoo maintains two preconditioners:
\begin{align*}
    L_t = L_{t - 1} + G_t G_t^T \in \mathbb{R}^{m \times m}\,, \\
    \quad R_t = R_{t - 1} + G_t^T G_t \in \mathbb{R}^{n \times n}\,,
\end{align*}
initialized as $\epsilon I_m$ and $\epsilon I_n$. The weight update then becomes:
\begin{align} \label{alg:shampooiter}
    W_{t + 1} = W_t - \eta L_t^{-1/4} G_t R_t^{-1/4}\,.
\end{align}
This formulation not only approximates the full Adagrad preconditioner \cite{anil2020scalable,morwani2024new}, but also provides a Kronecker product approximation to the Gauss-Newton component of the layerwise Hessian \cite{morwani2024new} (see \cref{rem1}). This connection to second-order information will prove crucial for understanding SOAP's effectiveness.


% \begin{lemma}[Anil \emph{et al.} \cite{anil2020scalable}, Lemma 1]
%     \label{lemma: hessian}
%     Let $G_1, \ldots, G_t \in \mathbb{R}^{m \times n}$ be matrices of rank at most $r$. Let $g_s=\operatorname{vec}\left(G_s\right)$ and define $\widehat{H}_t=\epsilon I_{m n}+\sum_{s=1}^t g_s g_s^{\top}$. Define $L_t, R_t$ as above: $L_t=\epsilon I_m+\sum_{s=1}^t G_s G_s^{\top}, R_t=\epsilon I_n+\sum_{s=1}^t G_s^{\top} G_s$. Then for any $p, q>0$ such that $1 / p+1 / q=1$, we have $\widehat{H}_t \leq r L_t^{1 / p} \otimes R_t^{1 / q}$.
% \end{lemma}

% It follows from the above lemma that 
% for any $p, q>0$ with $1 / p+1 / q=1$,
% the full AdaGrad preconditioned gradient $\widehat{H}_t^{-1 / 2}g_t$ can be approximated by $(L_t^{1 / p} \otimes R_t^{1 / q})^{-1 / 2} g_t = \Vec(L_t^{-1 / 2p} G_t R_t^{-1 / 2q})$. In particular, the case of $p=q=2$ yields the standard Shampoo update. 
% Moreover, \cite{morwani2024new} explores the Hessian approximation perspective of Shampoo, showing that the preconditioner in Shampoo is a Kronecker product approximation of the Gauss-Newton component of layerwise Hessian (see \cref{rem1}). Similar arguments will be used below to understand the second-order nature of SOAP.  

% The aim of algorithms such as K-FAC and Shampoo (when viewed from the Hessian perspective) is
% to do a layerwise Kronecker product approximation of the Fisher matrix HGN. The following lemma


% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}[b]{0.58\textwidth}
%     \includegraphics[width=\textwidth]{Figures/kf/kf_pred_w_no_error.png}
%     % \caption{Caption for figure 1}
%   \end{subfigure}
%   % \hspace{2mm}
%   \begin{subfigure}[b]{0.38\textwidth}
%     \includegraphics[width=\textwidth]{Figures/kf/kolmogorov_spectrum.png}
%     % \caption{Caption for figure 2}
%   \end{subfigure}
%   \caption{{\em Kolmogorov flow ($\text{Re} =10^4$).} Left: Comparison between reference numerical solutions and the model predictions trained with SOAP. Right: Spectral energy distribution comparing PINN predictions and numerical solutions computed at varying grid resolutions.}
%   \label{fig:ns_tori_rayleigh_taylo}
% \end{figure*}




\paragraph{SOAP for PINNs.}
\label{sec:soap}

% SOAP for PINNs
% -- Detailed analysis of why SOAP is particularly effective
% -- Theoretical guarantees on gradient alignment at initialization
% -- Computational advantages over other second-order methods

Before diving into the formal analysis, let us  build some intuition for why SOAP is particularly effective at resolving gradient conflicts. The key insight comes from understanding how second-order information captures interactions between different loss terms. When gradients conflict, it typically indicates that improving one objective requires coordinated changes across multiple parameters -- information that is encoded in the Hessian matrix's off-diagonal elements.

SOAP approximates this second-order information in two complementary ways:
(i) Its block-diagonal structure naturally captures parameter interactions within each network layer; (ii) Its adaptive preconditioner accumulates information about gradient correlations across training steps.
This allows SOAP to implicitly identify and exploit parameter update directions that simultaneously improve multiple objectives. Rather than simply following the average gradient, SOAP can utilize the local loss landscape geometry to find more direct paths to good solutions. The following sections make this intuition precise through formal analysis of SOAP's convergence properties and gradient alignment characteristics.


We now show that SOAP can be interpreted as an approximation to Newton's method, providing theoretical justification for the high inter-step gradient alignment observed in Figure \ref{fig:grad_align_score}. The key insight comes from analyzing how SOAP modifies Shampoo's preconditioner structure.

SOAP \cite{vyas2024soap} enhances Shampoo's efficiency by performing optimization in a transformed space aligned with the preconditioner's principal directions. For each layer's weight matrix and gradient $G_t \in \R^{m \times n}$, SOAP maintains two covariance matrices using exponential moving averages:
\begin{align}
    L_{t} = \beta_2 L_{t-1}+\left(1-\beta_2\right) G_{t} G_t^T, \\
    R_{t} =  \beta_2 R_{t-1} +\left(1-\beta_2\right) G_t^T G_t\,.
\end{align}
These matrices are then eigendecomposed as $L_t = Q_L \Lambda_L Q_L^T$ and $R_t = Q_R \Lambda_R Q_R^T$, where $\Lambda_L$ and $\Lambda_R$ contain the eigenvalues that capture the principal curvature directions of the loss landscape.

At each iteration $t$, SOAP updates each layer's weight matrix $W_t$ using its corresponding gradient $G_t$ as follows:
\begin{enumerate}[itemsep=1mm, topsep=1mm, parsep=1mm, leftmargin=*]
    \item Project the gradient into the eigenspace: 
    \vspace{-0.5em}
    \[\widetilde{G}_t = Q_L^T G_t Q_R.\]
    \vspace{-2em}
    \item Apply the Adam update in the \emph{rotated} space:
    \vspace{-0.5em}
    \[\widetilde{W}_{t+1} = \widetilde{W}_{t} - \eta \, \operatorname{Adam}(\widetilde{G}_t).\]
    \vspace{-2em}
    \item Transform back to the original parameter space:
    \vspace{-0.5em}
    \[W_{t+1} = Q_L \widetilde{W}_{t+1} Q_R^T.\]
    \vspace{-2em}
\end{enumerate}
To reduce computational overhead, the preconditioners $L_t$ and $R_t$ are updated with frequency $f$ in practice. We will analyze the impact of update frequency and momentum parameters through ablation studies in Section \ref{sec:results}.

% We now demonstrate that SOAP can be interpreted as an approximation to using the Gauss-Newton component of the Hessian as a preconditioner. For a neural network with parameters $W$, the Gauss-Newton matrix is given by:
% \begin{align}
% H_{\mathrm{GN}}=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\frac{\partial f}{\partial W} \frac{\partial^2 \mathcal{L}}{\partial f^2} \frac{\partial f^{\top}}{\partial W}\right]=\mathbb{E}_{\substack{x \sim \mathcal{D}_x \\ s \sim f(x)}}\left[g_{x, s} g_{x, s}^{\top}\right]
% \end{align}
% where $f(x)$ refers to the networkâ€™s output, and $D_x$ represents the training distribution of $x$

% The following proposition shows that performing a preconditioned gradient descent step is approximately equivalent to preconditioned gradient descent step in the rotated space.

% \begin{lemma}[Adapted from Anil, et al. (2022); Morwani, et al. (2024).]
% Assume that $G_{x, s}$ are matrices of rank at most $r$. Let $g_{x, s}=\operatorname{vec}\left(G_{x, s}\right)$. Then for any $p, q>0$ such that $1 / p+1 / q=1$, we have 
% \begin{align}
%     H_{\text{GN}} = \mathbb{E}_{x, s \sim f(x)}\left[g_{x, s} g_{x, s}^{\top}\right] \leqslant r\left(\mathbb{E}_{x, s \sim f(x)}\left[G_{x, s} G_{x, s}^{\top}\right]\right)^{1 / p} \otimes\left(\mathbb{E}_{x, s \sim f(x)}\left[G_{x, s}^{\top} G_{x, s}\right]\right)^{1 / q}
% \end{align}

% \end{lemma}

% For our purpose, we begin by noting that there exists a one-to-one correspondence between the original parameter space and the rotated space that preserves the matrix-vector multiplication. 
% % This relationship is formalized in the following lemma:
% \begin{lemma}
% \label{lemma: equivalence}
% Let $Q_L \in \R^{m \times m}$ and $Q_R \in \R^{n \times n}$ be two orthogonal matrices. For any matrix $A \in \R^{mn \times mn}$ and vector $v \in \R^{m n}$, define $\widetilde{v} := (Q_L \otimes Q_R)v$ and $\widetilde{A} := (Q_L\otimes Q_R)A(Q_L^T \otimes Q_R^T)$. Then there holds 
% \begin{equation*}
%     \widetilde{A v} = (Q_L \otimes Q_R) A v = \widetilde{A} \widetilde{v}\,.
% \end{equation*}
% % the following equivalence holds:
% % $Av = b$ if and only if $\widetilde{A}\widetilde{v} = \widetilde{b}$.
% \end{lemma}
% The proof follows directly by applying the transformation $Q_L \otimes Q_R$ to $A v$ and the definitions of $\widetilde{A}$ and $\widetilde{v}$. Building on the above lemma, one can easily transform the preconditioned gradient descent in the original space to the rotated one and vice versa. 

% we can now establish the equivalence between the preconditioned gradient descent in the original and rotated spaces. 

% \begin{proof}
% The proof follows directly by applying the transformation $Q_L \otimes Q_R$ to $A v$ and the definitions of $\widetilde{A}$ and $\widetilde{v}$.  
% % both sides of the equation $Av = b$:
% %     \begin{align}
% %         (Q_L\otimes Q_R)A(Q_L^T \otimes Q_R^T) (Q_L \otimes Q_R)v =  (Q_L \otimes Q_R) b
% %     \end{align}
% % It implies
% %     \begin{align}
% %         \widetilde{A} \widetilde{v} = \widetilde{b}
% %     \end{align}
% % The reverse direction follows from the invertibility of orthogonal matrices.
% \end{proof}


% \begin{corollary} 
% \label{corollary: rotated}
% Let $W_t, G_t \in \R^{m \times n}$ be the weight matrix and gradient matrix for a layer at iteration $t$, respectively, with vectorizations $w_t = \Vec(W_t)$ and $g_t = \Vec(G_t)$. The preconditioned gradient descent update:
% \begin{align}
% w_{t+1} = w_t - \eta H^{-1}g_t\,,
% \end{align}
% is equivalent to performing preconditioning in the rotated space:
% \begin{align}
% \widetilde{w}_{t+1} = \widetilde{w}_t - \eta \widetilde{H}^{-1}\widetilde{g}_t\,,
% \end{align}
% where $H \in \R^{mn \times mn}$ is the preconditioner, and $\widetilde{w}$, $\widetilde{g}$, and $\widetilde{H}$ are the rotated weight, gradient, and preconditioner defined by the transformations in \cref{lemma: equivalence}. 
% \end{corollary}


To establish SOAP's connection to Newton's method, we begin by examining how the Hessian matrix can be approximated in neural networks. For networks trained with cross-entropy loss, the Gauss-Newton approximation takes the form:
\begin{align}
H_{\text{GN}} = \mathbb{E}\left[\frac{\partial f}{\partial W} \frac{\partial^2 \mathcal{L}}{\partial f^2} \frac{\partial f^T}{\partial W}\right] = \mathbb{E}\left[g g^T\right],
\end{align}
where $\mathcal{L}$ denotes the loss function, $f$ represents network outputs, and $G = \frac{\partial \mathcal{L}}{\partial W}$ is the gradient matrix with vectorization $g = \Vec(G)$. Empirical evidence from \cite{sankar2021deeper} supports a key simplifying assumption:
%
\begin{assumption}
\label{assumption}
The Gauss-Newton component provides a good approximation to the true Hessian: $H_{\text{GN}} \approx H$. 
\end{assumption}
%
This approximation leads to our main theoretical result connecting SOAP to Newton's method:
%
\begin{theorem} 
\label{thm: soap}
Under Assumption \ref{assumption} and the conditions of Proposition \ref{prop:adam_hessian}, SOAP's update approximates Newton's method:
\begin{align}
    w_{t+1} = w_{t} - \eta \operatorname{Soap}(g_t) \approx w_t - \eta H^{-1} g_t.
\end{align}
\end{theorem}
%
The complete proof is provided in Appendix \ref{app:soap_newton}. The key insight is that SOAP effectively approximates the block-diagonal Hessian in a rotated space, with each block corresponding to a layer-wise Kronecker factorization. This structure naturally promotes gradient alignment across optimization steps, as we demonstrated theoretically in Lemma \ref{lemma: inter_step_gradient_align} and observed empirically in Figure \ref{fig:grad_align_score}.


\begin{remark} \label{rem1}
While SOAP effectively approximates Newton's method through its block-diagonal structure, other optimizers make different compromises in their approximations. Adam can approximate Newton's method, but requires the highly restrictive assumption that the Hessian matrix is diagonal. Similarly, Shampoo takes a different approach by using the square root of the Gauss-Newton component as its preconditioner \cite{morwani2024new}:
\begin{align}
    \operatorname{Shampoo}(g_t) \approx H_{\text{GN}}^{-1/2} g_t \approx H^{-1/2} g_t.
\end{align}
\end{remark}
These structural differences help explain why SOAP achieves better gradient alignment than both Adam and Shampoo.


% Deep learning optimization remains a critical challenge in achieving efficient model training and convergence. While recent advances have introduced various optimization methods, the choice of optimizer can significantly impact training efficiency and model performance. In this work, we propose leveraging SOAP \cite{vyas2024soap} (ShampoO with Adam in the Preconditioner's eigenbasis), a recently developed optimization algorithm that has shown promising results in large-scale language model training, to address gradient conflict issue in training PINNs. SOAP combines the benefits of second-order preconditioning from Shampoo with the adaptivity of Adam, potentially offering faster convergence and improved stability compared to traditional optimizers.  


% \begin{algorithm}[h]
% \caption{Single step of SOAP for a $m \times n$ layer}
% \label{alg:soap}
% \begin{algorithmic}[1]
% \Require Layer matrices: $L \in \mathbb{R}^{m \times m}$, $R \in \mathbb{R}^{n \times n}$, $V, M \in \mathbb{R}^{m \times n}$
% \Require Hyperparameters: Learning rate $\eta$, betas $=(\beta_1, \beta_2)$, epsilon $\epsilon$, preconditioning frequency $f$
% \State Sample batch $B_t$
% \State $G \in \mathbb{R}^{m \times n} \leftarrow -\nabla_W \phi_{B_t}(W_t)$
% \State $G' \leftarrow Q_L^T G Q_R$
% \State $M \leftarrow \beta_1 M + (1-\beta_1)G$
% \State $M' \leftarrow Q_L^T M Q_R$
% \Comment{Run Adam on $G'$}
% \State $V \leftarrow \beta_2 V + (1-\beta_2)(G' \odot G')$
% \Comment{Elementwise multiplication}
% \State $N' \leftarrow \frac{M'}{\sqrt{\hat{v}_r + \epsilon}}$
% \Comment{Elementwise division and square root}
% \State $N \leftarrow Q_L N' Q_R^T$ \Comment{Return to original space after Adam preconditioning}
% \State $W \leftarrow W - \eta N$
% \Comment{Update $L$ and $R$, possibly update $Q_L$ and $Q_R$}
% \State $L \leftarrow \beta_2 L + (1-\beta_2)GG^T$
% \State $R \leftarrow \beta_2 R + (1-\beta_2)G^TG$
% \If{$t \bmod f = 0$}
%     \State $Q_L \leftarrow \text{Eigenvectors}(L, Q_L)$
%     \State $Q_R \leftarrow \text{Eigenvectors}(R, Q_R)$
% \EndIf
% \end{algorithmic}
% \end{algorithm}




% \paragraph{Analysis of intra-step gradient alignment.}  \label{sec:gradientconflict}
% Next, we present some preliminary analysis to understand intra-step gradient conflicts in training PINNs via standard gradient descent, Adam \cite{kingma2014adam}, and Shampoo algorithms \cite{gupta2018shampoo}, and how SOAP can effectively resolve them during training. For simplicity, we consider the simplest case of using PINNs with the two-layer NN to solve the one-dimensional Laplace equation and 
% focus on the analysis of the intra-step gradient alignment \eqref{eq: intra_align} 
% with small initialization. The analysis can be easily extended to other types of PDEs. 
% Following the general setup in \cref{subsec: pinns}, without loss of generality, we consider 1D Laplace equation as follows
% \begin{align}
%     \left\{
% \begin{array}{ll}
% \Delta u = u'' = 0 & \text{on } [-1,1], \\
% u(\pm 1) = g_{\pm 1}. & 
% \end{array}
% \right.
% \end{align}
% We approximate the solution $u(x)$ by a two-layer network with width $N$: 
% \begin{equation} 
% \label{eq:twolayersol}
%     u(x, \theta) = \sum_{i = 1}^N a_i \si ( w_i x) = \ba \dd \si(\bw x)\,,
% \end{equation}
% where $\ba = (a_1, \ldots, a_N), \bw = (w_1, \ldots, w_N)  \in \R^N$, and $\theta = (\ba,\bw) \in \R^{2N}$. Moreover, we limit ourselves to the activation function $\si(x) = \tanh(x)$. In this case, the loss \eqref{eq: PINN_loss} reduces to 
% \begin{equation} \label{eq:losseg}
% \small
%     \min_{\theta = (\ba, \mathbf{w})} \mc{L}(\theta) = \underbrace{\frac{1}{N_r} \sum_{p = 1}^{N_r} |u''(x_p, \theta)|^2}_{\mc{L}_{r}(\theta)} + \underbrace{\frac{1}{2} \sum_{s = \pm 1} |u(s, \theta) - g_{s}|^2}_{\mc{L}_{bc}(\theta)}\,.
% \end{equation}
% To analyze the gradient conflict phenomenon in training PINNs, we consider the \emph{small initialization} regime. 
% \begin{assumption} \label{assp1}
%     The weights $a_i, w_i$ are initialized by i.i.d. Gaussian $\mc{N}(0,\ep^2)$ with small $\ep = o(1)$. 
% \end{assumption}
% This allows us to introduce the normalized parameters:  
% \begin{equation*}
%     \bar{\ba} = \ep^{-1} \ba\,, \q \bar{\bw} = \ep^{-1} \bw\,, 
% \end{equation*}
% initialized as standard Gaussian.

% % \begin{lemma} \label{lemma51}
% % Under small initialization, the gradients of the residual and boundary loss terms can be approximated as:
% % \begin{align}
% % \nabla_\theta \mathcal{L}_r(\theta) &= \ep^7 G^r(\bar{\mathbf{a}},\bar{\mathbf{w}}) + O(\ep^9), \\
% % \nabla\theta \mathcal{L}_{bc}(\theta) &= \ep G^{bc}(\bar{\mathbf{a}},\bar{\mathbf{w}}) + O(\ep^3),
% % \end{align}
% % where
% % \begin{align} \label{eq:effgrad1}
% %     G^r(\wb{\ba},\wb{\bw}) &=  c_r  \wb{\ba} \dd \wb{\bw}^{\odot 3} \left(  \wb{\bw}^{\odot 3},   3  \wb{\ba} \odot \wb{\bw}^{\odot 2}  \right), \\
% %   \label{eq:effgrad2}
% %     G^{bc}(\wb{\ba},\wb{\bw}) &= (g_{-1} - g_1) \left(\wb{\bw}, \wb{\ba} \right).
% % \end{align}
% % Here $\bar{\mathbf{a}} = \ep^{-1}\mathbf{a}$, $\bar{\mathbf{w}} = \ep^{-1}\mathbf{w}$ are the normalized parameters, and $G^r$, $G^{bc}$ are the effective gradient terms.
% % \end{lemma}
% % The proof is given in \cref{proof51}. We remark that these elementary computations also provide insights into the gradient magnitude imbalance discussed in \cref{subsec:gradconflict}, noting $\|\na_\theta \mc{L}_r(\theta) \| = O(\ep^7)$ while $\| \na_{\theta} \mc{L}_{bc}(\theta) \| = O(\ep)$. 

% We are now ready to understand the gradient conflict for various optimizers applied to the residual and boundary loss terms separately. 
% \begin{proposition} 
% \label{prop: grad_align}
% At initialization, the alignment score converges to a binary random variable in the infinite width limit:
% \begin{align}
%     &\lim_{N \rightarrow \infty }\mathcal{A}(\square(\nabla \mathcal{L}_b),\square(\nabla \mathcal{L}_r)) \\
%     &= O(\ep^2) + C_\square \begin{cases}\operatorname{sgn}\left(g_{-1}-g_1\right) & \text { with prob. } \frac{1}{2}, \\ -\operatorname{sgn}\left(g_{-1}-g_1\right) & \text { with prob. } \frac{1}{2} .\end{cases}
% \end{align}
% where $\square = \operatorname{GD}, \operatorname{Adam}, \operatorname{Shampoo}, \text{ or } \operatorname{Soap}$  
%  denotes the optimizer update rule, and
% $C_\square$ is a constant depending on the optimizer. 
% % Particularly, for plain gradient descent, $C_\operatorname{GD} = \frac{3}{\sqrt{21}}$, for Adam, Shampoo, and SOAP $C_{\operatorname{Adam}} = C_{\operatorname{Shamp}} = C_{\operatorname{Soap}} = 1$.
% \end{proposition}

% The proof is provided in Appendix \ref{proof: grad_align}.  We can see that these optimizers fail to resolve intra-step gradient conflicts in the initialization, aligning with the near-zero initial intra-step gradient scores shown in Figure \ref{fig:grad_align_score}. 
In the following proposition we show that SOAP's approximation of Newton's method enables it to resolve such conflicts during training.
The proof can be found in Appendix \ref{proof:soap}. 
Note that this result is general, not limited to types of PDEs and network architectures, and can be extended to other multi-task learning problems in principle.


\begin{proposition} \label{prop:soap} 
Assume that there exists a global minima $\theta^* \in \R^p$ corresponding to the true PDE solution such that $\mathcal{L}_{ic}(\theta^*) = \mathcal{L}_{bc}(\theta^*) = \mathcal{L}_r(\theta^*) = 0$. When applying SOAP optimizers to $\mathcal{L}_{ic}, \mathcal{L}_{bc}$ and $\mathcal{L}_r$ separately, for each weight matrix $W$  we have
\begin{align}
    \lim_{\theta \rightarrow \theta^*} \mathcal{A}(\operatorname{Soap}(G_{ic}), \operatorname{Soap}(G_{bc}), \operatorname{Soap}(G_r)) \approx 1,
\end{align}
with $G_{ic} = \frac{\partial \mathcal{L}_{ic}}{\partial W}, G_{bc} = \frac{\partial \mathcal{L}_{bc}}{\partial W}$ and $G_r = \frac{\partial \mathcal{L}_r}{\partial W}$. 
\end{proposition}


\begin{remark}[Limitation of Shampoo]
An important limitation of Shampoo arises from its use of $H^{-1/2}$ rather than $H^{-1}$ in its approximation. This square root term prevents cancellation of the Hessian components in our theoretical analysis, making it impossible to establish gradient alignment guarantees similar to those we proved for SOAP. Our empirical investigation supports this theoretical insight. Using Muon \cite{jordan2024muon}, a computationally efficient variant of Shampoo, we observe that despite achieving reasonable convergence in the loss function, the optimizer maintains consistently low gradient alignment scores throughout training (Figure \ref{fig:grad_align_score}). This evidence reinforces our theoretical understanding of the Hessian approximation, as used in SOAP, is crucial for resolving gradient conflicts.
\end{remark}



% \begin{figure*}[ht]
% \vspace{-1mm}
%     \centering
%     \includegraphics[width=1.0\linewidth]{Figures/master_figure.pdf}
%     \caption{Simulating complex fluid dynamics using PINNs with SOAP optimization. (a) Kolmogorov flow at Re=10,000: comparison between reference solution and PINN predictions demonstrates accurate capture of turbulent structures across multiple time steps. (b) Spectral energy distribution showing PINN's superior resolution of fine-scale dynamics compared to traditional numerical solutions at various grid resolutions. (c) Lid-driven cavity flow at Re=5,000: streamlines and centerline velocity profiles show excellent agreement with benchmark data from \cite{GHIA1982387}. (d) Kuramoto-Sivashinsky equation: PINNs accurately predicts complex spatiotemporal patterns and chaotic dynamics. (e) Rayleigh-Taylor instability (Pr=0.71, Ra=$10^6$): evolution of temperature field shows precise capture of interface dynamics and mushroom-shaped structures characteristic of this flow.}
%     \label{fig:master_figure}
% \end{figure*}

    
\section{Experiments}
\label{sec:results}

% Experimental Results
% -- Comprehensive benchmarks (10 PDEs)
% -- Careful ablation studies
% -- Highlight turbulent flow results
% -- Comparison with state-of-the-art



To rigorously evaluate SOAP's performance, we examine a diverse set of 10 representative and challenging PDEs that govern fundamental physical phenomena. These equations span wave propagation, shock formulation, chaotic systems, reaction-diffusion processes,  fluid dynamics, and heat transfer. The detailed description of the problem setup, including the PDE parameters, initial and boundary conditions, numerical implementations, and supplementary visualizations, are presented in Appendix \ref{appendix: experiments}. 




% \begin{itemize}[leftmargin=*]
%     \item \textbf{Wave equation.} A second-order hyperbolic PDE that describes the propagation of waves:
%     \begin{align}
%     u_{tt} - c u_{xx} = 0,
%     \end{align}
%     where  $u$ represents the wave amplitude, and $c$ is the wave propagation speed, determined by the medium's physical properties.
    
%     \item \textbf{Burgers equation.} A fundamental nonlinear PDE that serves as a prototype for conservation laws and exhibits shock wave formation:
%     \begin{align}
%        u_t +u u_x =\nu u_{xx},
%     \end{align}
%     where $u$ represents the velocity field, and $\nu$ is the kinematic viscosity coefficient controlling the diffusion strength.
    
%     \item \textbf{Allen-Cahn equation.} A reaction-diffusion equation widely used in materials science to model phase separation and interface dynamics:
%     \begin{align}
%         {u}_{{tt}}=\epsilon {u}_{xx}+ a ({u}-{u}^3),
%     \end{align}
%     where $u$ represents the order parameter (e.g., concentration difference between two phases),  $\epsilon$ controls the interfacial width, $a$ is the reaction rate coefficient, and the term $({u}-{u}^3)$ drives the phase separation.

%     \item \textbf{Kortewegâ€“De Vries (KDV) equation.} A third-order nonlinear PDE that models one-dimensional nonlinear dispersive waves:
%     \begin{align}
%          u_t + \eta u u_x + \mu^2 u_{x x x} = 0\,
%     \end{align}
%     where $u$ represents the wave amplitude or water surface elevation, and $\eta$ governs the strength of the nonlinearity, while $\mu$ controls the dispersion level. Under the KdV dynamics,  this initial wave evolves into a series of solitary-type waves.


%     \item \textbf{Kuramoto-Sivashinsky equation.} A fourth-order nonlinear PDE that exhibits spatiotemporal chaos and complex pattern formation:
%     \begin{align}
%         u_t+\alpha u u_x+\beta u_{x x}+\gamma u_{x x x x}=0,
%     \end{align}
%     where $u$ represents the height of a thin film or flame front. This equation arises in various physical contexts, including flame front propagation, thin film flows, and plasma instabilities.

%     \item \textbf{Grey-Scott equation}: A reaction-diffusion system that exhibits pattern formation:
%     \begin{align}
%     u_t &=\epsilon_1 \Delta u + b_1(1-u) - c_1 u v^2,  \\
%     v_t &=\epsilon_2 \Delta v - b_2 v + c_2 u v^2,
%     \end{align}
%     where $u$ and $v$ represent activator and inhibitor concentrations respectively, $\varepsilon_1$ and $\varepsilon_2$ are diffusion coefficients, and $(b_1, b_2, c_1, c_2)$ control reaction kinetics. This system generates diverse spatial patterns including spots and stripes. 

%     \item \textbf{Ginzburg-Landau equation}: A nonlinear PDE that describes the evolution of small perturbations near bifurcation points:
%     \begin{align}
%     \frac{\partial A}{\partial t}= \epsilon \Delta A + \mu A - \gamma  A|A|^2,
%     \end{align}
%     where  $A$ is the complex amplitude representing the envelope of oscillations, $\epsilon$ represents the diffusion coefficient, $\mu$ is the linear growth rate, and $\gamma$ controls the nonlinear saturation.

%     \item \textbf{Lid-driven cavity.} A classical benchmark problem in computational fluid dynamics that describes incompressible fluid flow in a square cavity with a moving top wall:
%     \begin{align}
%         \mathbf{u} \cdot \nabla \mathbf{u}& =- \nabla p + \frac{1}{R e} \Delta \mathbf{u} ,  (x, y) \in(0,1)^2, \\
%         \nabla \cdot \mathbf{u} &=0, (x, y) \in(0,1)^2,
%     \end{align}
%     where $\mathbf{u} = (u,v)$ represents the steady-state velocity field, $p$ is the pressure field, and $Re$ is the Reynolds number which characterizes the ratio of inertial to viscous forces.  This system models the equilibrium state of the flow, which is driven by the top boundary moving at a constant velocity while the other walls are stationary, leading to the formation of characteristic vortical structures whose complexity increases with the Reynolds number.

%     \item \textbf{Kolmogorov flow.} The two-dimensional Kolmogorov flow is described by the unsteady incompressible Navier-Stokes equations:
%         \begin{align}
%         \mathbf{u}_t + \mathbf{u} \cdot \nabla \mathbf{u} & =- \nabla p + \frac{1}{R e} \Delta \mathbf{u} + \mathbf{f}, \\
%         \nabla \cdot \mathbf{u} & =0, 
%     \end{align}
%     where $\mathbf{u} = (u,v)$ represents the time-varying velocity field, and $\mathbf{f}$ denotes the external forcing term that maintains the flow structure. The system evolves from a random initial state and develops characteristic flow patterns, where energy transfers between different spatial scales through nonlinear interactions and viscous dissipation.

%     \item \textbf{Rayleigh-Taylor instability.}  A coupled flow-temperature system modeling buoyancy-driven instability:
%     \begin{align}
%         \mathbf{u}_t + \mathbf{u} \cdot \nabla \mathbf{u}  & = - \nabla p +  \sqrt{\frac{Pr}{Ra} }\Delta \mathbf{u} + T \mathbf{e}_y,  \\
%         \nabla \cdot \mathbf{u} & =0,  \\
%         T_t + \nabla \cdot (\mathbf{u} T)  & =  \frac{1}{\sqrt{Pr Ra}} T_{tt}, 
%     \end{align}
%     where $T$ is the temperature field (acting as a density proxy through the Boussinesq approximation),  Pr is the Prandtl number (ratio of momentum to thermal diffusivity), and Ra is the Rayleigh number (measuring buoyancy-driven flow strength). This system captures the characteristic mushroom-shaped plumes that develop as the heavier fluid penetrates into the lighter fluid below. 
    
% \end{itemize}

% The detailed description of the problem setup, including the PDE parameters, initial and boundary conditions, numerical implementations, and supplementary visualizations, are presented in Appendix \ref{appendix: experiments}.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figures/ks/ks_pred.png}
%     \caption{{\em Kuramoto-Sivashinsky equation.} Left: Comparison between reference numerical solution and the model predictions trained with SOAP. Right: Evolution of relative $L^2$ error over time for Adam and SOAP optimizers. }
%     \label{fig:ks_pred}
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figures/ldc/ldc_pred.png}
% \caption{{\em Lid-driven Cavity (Re=5,000).} Left: Velocity field predicted by SOAP. Right: Comparison of velocity profiles at centerlines ($u(0.5,y)$ and $v(x,0.5)$) between SOAP predictions and benchmark results from Ghia \emph{et al.}  \cite{GHIA1982387}.}
%     \label{fig:ldc_pred}
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.4\linewidth]{Figures/kf/kolmogorov_spectrum.png}
%     \caption{{\em Kolmogorov flow ($\text{Re} =10^4$).} Spectral energy distribution comparing PINN predictions and numerical solutions computed at varying grid resolutions.}
%     \label{fig:kolmogorov_spectrum}
% \end{figure}


% \begin{figure}[t]
%   \centering
%   \begin{subfigure}[b]{0.61\textwidth}
%     \includegraphics[width=\textwidth]{Figures/kf/kf_pred_w.png}
%     % \caption{Caption for figure 1}
%   \end{subfigure}
%   \hspace{2mm}
%   \begin{subfigure}[b]{0.36\textwidth}
%     \includegraphics[width=\textwidth]{Figures/rayleigh_taylor/rayleigh_taylor_pred_temp.png}
%     % \caption{Caption for figure 2}
%   \end{subfigure}
%   \caption{Left: {\em Kolmogorov flow ($\text{Re} =10^4$).} Right: {\em Raleigh-Taylor instability (Pr=0.71, Ra=$10^6$).} Comparison between reference numerical solutions and the model predictions trained with SOAP.}
%   \label{fig:ns_tori_rayleigh_taylo}
% \end{figure}


% \begin{figure}[t]
%   \centering
%   \begin{subfigure}[b]{0.58\textwidth}
%     \includegraphics[width=\textwidth]{Figures/kf/kf_pred_w_no_error.png}
%     % \caption{Caption for figure 1}
%   \end{subfigure}
%   \hspace{2mm}
%   \begin{subfigure}[b]{0.38\textwidth}
%     \includegraphics[width=\textwidth]{Figures/kf/kolmogorov_spectrum.png}
%     % \caption{Caption for figure 2}
%   \end{subfigure}
%   \caption{{\em Kolmogorov flow ($\text{Re} =10^4$).} Left: Comparison between reference numerical solutions and the model predictions trained with SOAP. Right: Spectral energy distribution comparing PINN predictions and numerical solutions computed at varying grid resolutions.}
%   \label{fig:ns_tori_rayleigh_taylo}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\linewidth]{Figures/rayleigh_taylor/rayleigh_taylor_pred_temp.png}
%     \caption{{\em Raleigh-Taylor instability (Pr=0.71, Ra=$10^6$).} Comparison between reference numerical solutions and the model predictions trained with SOAP.}
%     \label{fig:rayleigh_taylor_pred_temp}
% \end{figure}


\begin{figure}[ht]
\vspace{-1mm}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/master_figure.pdf}
    \caption{Simulating complex fluid dynamics using PINNs with SOAP optimization. (a) Kolmogorov flow at Re=10,000: comparison between reference solution and PINN predictions demonstrates accurate capture of turbulent structures across multiple time steps. (b) Spectral energy distribution showing PINN's superior resolution of fine-scale dynamics compared to traditional numerical solutions at various grid resolutions. (c) Lid-driven cavity flow at Re=5,000: streamlines and centerline velocity profiles show excellent agreement with benchmark data from \cite{GHIA1982387}. (d) Kuramoto-Sivashinsky equation: PINNs accurately predicts complex spatiotemporal patterns and chaotic dynamics. (e) Rayleigh-Taylor instability (Pr=0.71, Ra=$10^6$): evolution of temperature field shows precise capture of interface dynamics and mushroom-shaped structures characteristic of this flow.}
    \label{fig:master_figure}
\end{figure}



\paragraph{Baselines.}   We focus our comparisons on PINN approaches that have the potential to scale to training large neural networks, as these are promising for solving realistic large-scale physical problems in the future. That said, we do not compare against PINNs trained with natural gradients \cite{muller2023achieving,jnini2024gauss} and L-BFGS variants \cite{urban2025unveiling}, since these methods are limited to small networks and full-batch gradient descent, making them impractical for the challenging PDEs we consider.

Our baselines follow the current state-of-the-art training pipeline established by \cite{wang2023expert}. Unless otherwise specified, we use PirateNet \cite{wang2024piratenets} as the backbone architecture,  with three residual blocks, a hidden dimension of 256, and \texttt{Tanh} activation functions. All weight matrices are enhanced  using random weight factorization (RWF) \cite{wang2022random}, with $\mu=1.0, \sigma=0.1$.
Exact periodic boundary conditions are strictly enforced when applicable \cite{dong2021method}. 

For model training, we use mini-batch gradient descent with the Adam optimizer \cite{kingma2014adam}, which has become the de facto standard for training PINNs due to its robust performance and computational efficiency.  We randomly sample 8,192 collocation points at each iteration. The learning rate schedule begins with a linear warm-up phase over the first 5,000 steps, increasing from 0 to 0.001, followed by an exponential decay with a factor of 0.9. 
To improve training efficiency and robustness, we use
learning rate annealing \cite{wang2021understanding,wang2023expert} for loss balancing. The loss weights are updated every 1,000 iterations with a moving average. In addition, we employ causal training \cite{wang2022respecting,wang2023expert} to address causality violations when solving time-dependent PDEs.  The causal tolerance is set to 1.0.
For challenging benchmarks, we implement time-marching and curriculum learning strategies \cite{krishnapriyan2021characterizing}.
Details of the aforementioned techniques and hyper-parameters are summarized in Appendix \ref{appendix:arch} and \ref{appendix:training}.
\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.4}
    \caption{Comparison of optimizer performance obtained by training PINNs with Adam and SOAP, respectively, across various PDEs, following the training pipeline described in Section \ref{sec:results}. The evaluation metric is the relative $L^2$ error over the entire spatial temporal domain.
}
    \label{tab: sota}
    \begin{tabular}{l cc} 
        \toprule
    \textbf{Benchmark} & \multicolumn{1}{c}{\textbf{Adam}  } & \multicolumn{1}{c}{\textbf{SOAP}} \\
        \midrule
        Wave  & ${5.15 \times 10^{-5}}$ & $\mathbf{8.05 \times 10^{-6}} $\\ 
         Burgers  & ${8.20 \times 10^{-5}}$ & $\mathbf{4.03\times 10^{-5}}$  \\   
        Allen-Cahn  & ${2.24 \times 10^{-5}}$ & $\mathbf{3.48 \times 10^{-6}}$ \\ 
        Kortewegâ€“De Vries  & ${7.04 \times 10^{-4}}$ & $\mathbf{3.40 \times 10^{-4}}$ \\
        Kuramoto-Sivashinsky & ${7.48 \times 10^{-2}}$ & $\mathbf{3.86 \times 10^{-2}}$ \\
        Grey-Scott & ${3.61 \times 10^{-3}}$ & $\mathbf{1.18 \times 10^{-4}}$ \\
        Ginzburg-Landau  & ${1.49 \times 10^{-2}}$ & $\mathbf{4.78 \times 10^{-3}}$  \\
        Lid-driven cavity ($\text{Re}=5 \times 10^3$) & $3.24 \times 10^{-1}$  & $\mathbf{3.99 \times 10^{-2}}$ \\
        Kolmogorov flow ($\text{Re}=10^4$) & $2.04 \times 10^{-1}$  & $\mathbf{6.99 \times 10^{-2}}$ \\
        Rayleigh-Taylor instability ($\text{Pr}=0.71, \text{Ra}=10^6$) & $7.32 \times 10^{-2}$ & $\mathbf{5.22 \times 10^{-3}}$ \\
        \bottomrule 
    \end{tabular}
\end{table}

\paragraph{State-of-the-art results.} 
Table \ref{tab: sota} demonstrates SOAP's consistent performance improvements across diverse test cases. For the simple wave equation, SOAP achieves a 6.4x reduction in relative error compared to the baseline. For nonlinear 1D problems, our approach yields a 6.9x reduction for the Allen-Cahn equation and about 2x for both Kortewegâ€“De Vries and Kuramoto-Sivashinsky problems.
The performance gains become particularly pronounced for coupled diffusion-reaction systems. The Grey-Scott and Ginzburg-Landau equations exhibit an order of magnitude reduction in error. On challenging Navier-Stokes benchmarks, including the lid-driven cavity and Rayleigh-Taylor instability problems, SOAP demonstrates a more than 10x error reduction. We highlight and discuss these substantial improvements in detail below.


\paragraph{Complex Fluid Dynamics.}

Our most significant achievement is successfully applying PINNs to complex fluid dynamics problems that were previously considered beyond their capabilities. In particular, we demonstrate breakthrough results in three challenging cases that combine multiple physical constraints and have historically proven difficult for PINNs, see Figure \ref{fig:master_figure}.

For the lid-driven cavity flow at Reynolds number 5,000, SOAP enables a dramatic improvement in accuracy, reducing the relative $L^2$ error from 32.4\% to 3.99\%. As shown in Figure \ref{fig:master_figure}c, our model successfully captures intricate flow features including secondary and tertiary corner vortices, showing excellent agreement with the benchmark results of \cite{GHIA1982387}. This level of accuracy was previously unattainable with PINNs at such high Reynolds numbers.

The Rayleigh-Taylor instability presents an even more demanding test, requiring simultaneous handling of interface dynamics and coupled velocity-density evolution. SOAP enables accurate prediction of the characteristic mushroom-shaped structures that develop as heavier fluid penetrates into lighter fluid, achieving a relative $L^2$ error of 0.52\% - nearly an order of magnitude improvement over the best baseline's 7.32\%. Figure \ref{fig:master_figure}e demonstrates excellent agreement with reference solutions across multiple time steps, capturing both the initial linear growth phase and subsequent nonlinear development.

Our most impressive result comes from the turbulent Kolmogorov flow at Reynolds number 10,000 -- marking the first time PINNs have successfully captured turbulent dynamics at such high Reynolds numbers. Our model achieves a relative $L^2$ error of 6.99\%, compared to 20.4\% with our baseline. Figure \ref{fig:master_figure}a shows that our predictions accurately reproduce both the large-scale flow structures and the complex cascade of smaller eddies characteristic of turbulent flows. Moreover, spectral analysis reveals that our PINN solution maintains higher spectral energy at high wavenumbers compared to traditional numerical solvers, even those using a $1024 \times 1024$ grid resolution. This demonstrates PINNs' potential advantage in resolving fine-scale dynamics without requiring explicit grid discretization -- a key capacity for turbulence modeling.


These results represent more than just incremental improvements in accuracy. They demonstrate that PINNs, when properly optimized, can handle complex multi-physics problems previously considered beyond their capabilities. The success in these challenging cases validates our theoretical analysis showing that gradient alignment becomes increasingly critical as physical constraints become more tightly coupled.


\paragraph{On the Convergence of PINNs Solutions.}
An intriguing observation emerges when examining our convergence trajectories in detail. While the PINNs' loss functions continue to decrease throughout training, indicating increasingly better satisfaction of physical and boundary constraints, the relative $L^2$ test error against reference numerical solutions plateaus for most complex cases (notably for turbulent flows and the Rayleigh-Taylor instability, among other cases, see Figures \ref{fig:kdv}, \ref{fig:gs_error}, \ref{fig:ns_tori_error}, \ref{fig:rayleigh_taylor_error}). This behavior warrants careful consideration.

Classical numerical solvers, despite their remarkable accuracy, ultimately rely on spatial and temporal discretization that introduces some degree of numerical error. In contrast, PINNs approximate solutions in a continuous space-time domain without explicit discretization. Our results suggest an interesting possibility: as second-order optimization enables more stable optimization of PINNs toward exact satisfaction of the governing equations, the resulting solutions might be approaching the true analytical solutions more closely than previously thought possible.

This observation is particularly relevant for turbulent flows, where our spectral analysis reveals that PINNs maintain higher spectral energy content at fine scales compared to numerical solutions, even at high grid resolutions (see Figure \ref{fig:master_figure}b). 
While our spectral analysis provides compelling evidence for this phenomenon, several important questions remain open for future investigation. A rigorous theoretical framework will be needed to precisely characterize the relationship between PINNs' continuous approximations and discretized numerical solutions. Additionally, careful analysis of the asymptotic convergence properties of PINNs solutions as training losses approach zero could provide valuable insights into their fundamental approximation capabilities. Understanding these aspects could open new directions for solving challenging PDEs where discretization effects significantly impact solution accuracy.


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/ablation_studies.png}
\caption{Optimizer performance comparison and ablation studies.  Top: Relative $L^2$ error across PDE benchmarks using different optimizers. Bottom left: Relative $L^2$ error for varying preconditioner update frequencies in SOAP optimizer. Bottom right: Relative $L^2$ error with different momentum values in SOAP optimizer.}
    \label{fig:ablation}
\end{figure}

\paragraph{Ablation studies.} We conduct systematic experiments to evaluate SOAP's performance across different architectures and hyperparameter settings, establishing the robustness of our approach. Our first investigation examines SOAP's effectiveness across three representative architectures: standard MLP, modified MLP \cite{wang2021understanding}, and PirateNet \cite{wang2024piratenets}. As shown in the top panel of Figure \ref{fig:ablation}, testing each architecture on four benchmark PDEs (Wave, Burgers, Allen-Cahn, and KdV equations), we find that SOAP consistently improves accuracy compared to Adam optimization regardless of the underlying network backbones. In particular, PirateNet seems to be the most effective architecture across all test cases, leading to its selection for our main experiments.

As illustrated in the bottom panel of Figure \ref{fig:ablation}, our results of SOAP's hyperparameters reveal two critical factors affecting performance. The preconditioner update frequency presents a clear trade-off between accuracy and computational cost. While more frequent updates yield better results, the improvements diminish beyond an update frequency of 2, which we selected as the optimal balance for our experiments. The momentum parameter $\beta_1$ proved especially crucial: high momentum ($\beta_1=0.99$) consistently achieves the best results, while low momentum ($\beta_1$=0.01) significantly degrades performance across all test cases.

For completeness, we also compared SOAP against ConFiG \cite{liu2024config}, a recently proposed method for addressing gradient conflicts in PINNs that has demonstrated relative good performance compared to established baselines such as PCGrad \cite{yu2020gradient} and  IMTL-G  \cite{liu2021towards} in multi-task learning. Despite its promising theoretical foundations, ConFiG showed significant sensitivity to hyperparameter settings in our experiments, often resulting in unstable training and inconsistent performance. These results highlight the practical advantages of SOAP's more robust optimization approach. 

% This empirical finding aligns with our theoretical analysis in Section \ref{sec: method_analysis}, which demonstrates that momentum plays a fundamental role in resolving gradient conflicts. 



% mention we don't compare with L-BFGS is becasue stamdard L-BGFS only support full-batch and tend to perform poorly for mini-batch, not scalable. 



\paragraph{Computational Cost.} While SOAP requires approximately 2x longer training time compared to baselines (Table \ref{tab: cost}), our focus is exploring the performance frontier of PINNs through extended training to full convergence. Impressively, error and loss convergence curves (Appendix \ref{appendix:benchmarks}) indicate that SOAP typically achieves rapid initial convergence, reaching a reasonable accuracy (approximately $10^{-4}$)  within the first 10,000 iterations, followed by gradual error reduction in subsequent iterations. This suggests the potential for reducing training time by up to 10x while maintaining competitive performance.
These findings motivate future research into designing computationally efficient optimization algorithms and training strategies for PINNs, paving the way for practical and scalable applications in complex physics simulations.

% These results demonstrate how adaptive second-order optimization can dramatically expand PINNs' capabilities in solving complex physical systems. The consistent performance improvements across such a diverse range of PDEs, from simple wave equations to turbulent flows, suggest that appropriate optimization techniques are crucial for unleashing the full potential of physics-informed deep learning.















% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\linewidth]{Figures/ns_tori/ns_tori_pred_w.png}
%     \caption{Caption}
%     \label{fig:ns_tori_pred_w}
% \end{figure}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{Figures/rayleigh_taylor/rayleigh_taylor_pred_temp.png}
%     \caption{Caption}
%     \label{fig:rayleigh_taylor_pred_temp}
% \end{figure}

\section{Conclusion and Discussion}
\label{sec: conclusion}



% Conclusion and Discussion
% -- Summary of theoretical and empirical findings
% -- Broader implications for machine learning (e.g. multi-task learning)
% -- Future research directions




% Conclusion and Discussion
% -- Summary of theoretical and empirical findings
% -- Broader implications for machine learning (e.g. multi-task learning)
% -- Future research directions

This work advances our understanding of gradient conflicts in multi-task learning through the lens of physics-informed neural networks. Our theoretical analysis reveals fundamental challenges that arise when different loss terms push parameters in opposing directions -- a situation also common across a broad spectrum of deep learning applications from computer vision to natural language processing. We show that appropriate preconditioning through second-order information naturally aligns these conflicting gradients, providing a general principle for multi-task optimization. Our implementation through SOAP offers both theoretical guarantees and practical efficiency, leading to breakthrough results including the first successful application of PINNs to high Reynolds number turbulent flows with 2-10x accuracy improvements over existing methods.

An intriguing observation emerged from our investigation of complex PDE systems: while training losses continue to decrease, prediction accuracy against high-resolution numerical solutions tends to plateau. Our spectral analysis of turbulent flows provides compelling evidence that PINNs can capture fine-scale dynamics more accurately than traditional numerical solvers, even at very high grid resolutions. This suggests that PINNs, by learning continuous representations unconstrained by discretization, might be approaching more accurate solutions to the underlying physical equations than previously possible. These findings could have broad implications beyond scientific computing to areas like continuous representation learning.

Building on these insights, several promising research directions emerge. A rigorous theoretical framework characterizing how PINNs approximate solutions in the continuous domain could fundamentally change our understanding of neural networks in scientific computing. While SOAP demonstrates the power of gradient alignment in handling coupled physical constraints, opportunities exist for more efficient preconditioned algorithms that maintain their effectiveness with reduced computational cost. More broadly, our work suggests that the principles of gradient alignment and second-order preconditioning could benefit many deep learning applications involving competing objectives, though challenges remain in scaling these approaches to larger systems. Success in these directions could transform both scientific computing and multi-task optimization.




\section*{Acknowledgment}
B.L. would like to acknowledge support from National Key R$\&$D Program of China Grant No. 2024YFA1016000. P.P. and S.W. acknowledge support from the US Department of Energy under the Advanced Scientific Computing Research program (grant DE-SC0024563), the Nvidia Academic Grant Program, and the Institute for Foundations of Data Science at Yale University. 
We also thank the developers of the software that enabled our research, including JAX \cite{jax2018github},  Matplotlib \cite{hunter2007matplotlib}, and NumPy \cite{harris2020array}.


%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\appendix

\input{appendix}

\end{document}
