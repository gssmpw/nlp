\section{Related work}
Intent classification is a well-studied problem in natural language understanding (NLU). While intent classification is a sentence-level classification task, slot filling is a more challenging task that deals with classifying the type of each word. These problems can be solved independently \citet{raymond2007generative}, but they are generally solved jointly to optimize performance \citet{chen2019bert,qin2021co}. Our primary focus here is to investigate and improve the model's reasoning for intent classification.

In explainability, post-hoc explanation techniques are popular and useful for feature attribution. This includes perturbation-based methods \citep{ribeiro2016should,ribeiro2018anchors}, Gradient-based methods like integrated gradient \cite{sundararajan2017axiomatic} and attention-based methods \citep{wu2021explaining, wang-etal-2016-attention, pmlr-v37-xuc15}. For natural language processing (NLP) applications, feature importance is measured by the attribution score assigned to every token. These methods explainform of saliency maps, which is a suitable form for NLP due to well-defined features like words and phrases.

Many methods go beyond evaluation and ingest feature attribution priors during training. Some methods use attribution scores derived from post-hoc explanation techniques to train a hate-speech classifier under a scarce data scenario like \citet{liu2019incorporating}. \citet{zhong2019fine} and \citet{mathew2021hatexplain} supervised the model's attention using human-annotated rationale. \citet{jayaram-allaway-2021-human} incorporates feature attribution for documents from the legal domain. However, these methods primarily focus on tasks like hate speech or sentiment classification. We instead explore the area of intent classification.

Regarding explainability in intent classification, \citet{joshi2021towards} uses Layer wise relevance propagation (LRP) \citep{montavon2019layer} to investigate the deep learning model's reasoning over the ATIS dataset. However, this study only evaluates qualitatively based on some examples due to a lack of ground truth explanation signal data. To solve this problem, we introduce a benchmark dataset using the novel silver annotation technique and evaluate the models' reasoning quantitatively based on multiple metrics. \citet{gunaratna2022explainable} focuses on explaining slot classification, not intent.