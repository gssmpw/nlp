@article{chen2019bert,
  title={Bert for joint intent classification and slot filling},
  author={Chen, Qian and Zhuo, Zhu and Wang, Wen},
  journal={arXiv preprint arXiv:1902.10909},
  year={2019}
}

@article{gunaratna2022explainable,
  title={Explainable slot type attentions to improve joint intent detection and slot filling},
  author={Gunaratna, Kalpa and Srinivasan, Vijay and Yerukola, Akhila and Jin, Hongxia},
  journal={arXiv preprint arXiv:2210.10227},
  year={2022}
}

@inproceedings{jayaram-allaway-2021-human,
    title = "Human Rationales as Attribution Priors for Explainable Stance Detection",
    author = "Jayaram, Sahil  and
      Allaway, Emily",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.450",
    doi = "10.18653/v1/2021.emnlp-main.450",
    pages = "5540--5554",
    abstract = "As NLP systems become better at detecting opinions and beliefs from text, it is important to ensure not only that models are accurate but also that they arrive at their predictions in ways that align with human reasoning. In this work, we present a method for imparting human-like rationalization to a stance detection model using crowdsourced annotations on a small fraction of the training data. We show that in a data-scarce setting, our approach can improve the reasoning of a state-of-the-art classifier{---}particularly for inputs containing challenging phenomena such as sarcasm{---}at no cost in predictive performance. Furthermore, we demonstrate that attention weights surpass a leading attribution method in providing faithful explanations of our model{'}s predictions, thus serving as a computationally cheap and reliable source of attributions for our model.",
}

@inproceedings{joshi2021towards,
  title={Towards explainable dialogue system: Explaining intent classification using saliency techniques},
  author={Joshi, Ratnesh and Chatterjee, Arindam and Ekbal, Asif},
  booktitle={Proceedings of the 18th International Conference on Natural Language Processing (ICON)},
  pages={120--127},
  year={2021}
}

@article{liu2019incorporating,
  title={Incorporating priors with feature attribution on text classification},
  author={Liu, Frederick and Avci, Besim},
  journal={arXiv preprint arXiv:1906.08286},
  year={2019}
}

@inproceedings{mathew2021hatexplain,
  title={Hatexplain: A benchmark dataset for explainable hate speech detection},
  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={17},
  pages={14867--14875},
  year={2021}
}

@article{montavon2019layer,
  title={Layer-wise relevance propagation: an overview},
  author={Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Explainable AI: interpreting, explaining and visualizing deep learning},
  pages={193--209},
  year={2019},
  publisher={Springer}
}

@InProceedings{pmlr-v37-xuc15,
  title = 	 {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  author = 	 {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2048--2057},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/xuc15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/xuc15.html},
  abstract = 	 {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.}
}

@inproceedings{qin2021co,
  title={A co-interactive transformer for joint slot filling and intent detection},
  author={Qin, Libo and Liu, Tailu and Che, Wanxiang and Kang, Bingbing and Zhao, Sendong and Liu, Ting},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8193--8197},
  year={2021},
  organization={IEEE}
}

@inproceedings{raymond2007generative,
  title={Generative and discriminative algorithms for spoken language understanding},
  author={Raymond, Christian and Riccardi, Giuseppe},
  booktitle={Interspeech 2007-8th Annual Conference of the International Speech Communication Association},
  year={2007}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{ribeiro2018anchors,
  title={Anchors: High-precision model-agnostic explanations},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{sundararajan2017axiomatic,
  title={Axiomatic attribution for deep networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle={International conference on machine learning},
  pages={3319--3328},
  year={2017},
  organization={PMLR}
}

@inproceedings{wang-etal-2016-attention,
    title = "Attention-based {LSTM} for Aspect-level Sentiment Classification",
    author = "Wang, Yequan  and
      Huang, Minlie  and
      Zhu, Xiaoyan  and
      Zhao, Li",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1058",
    doi = "10.18653/v1/D16-1058",
    pages = "606--615",
}

@article{wu2021explaining,
  title={On explaining your explanations of bert: An empirical study with sequence classification},
  author={Wu, Zhengxuan and Ong, Desmond C},
  journal={arXiv preprint arXiv:2101.00196},
  year={2021}
}

@article{zhong2019fine,
  title={Fine-grained sentiment analysis with faithful attention},
  author={Zhong, Ruiqi and Shao, Steven and McKeown, Kathleen},
  journal={arXiv preprint arXiv:1908.06870},
  year={2019}
}

