\section{Related Works}
\subsection{Keyphrase Extraction}
Keyphrase extraction approaches can be broadly categorized into three main types. Early work in this area follows a pipeline approach, often employing a 2-step method (**Turney, "Automatic Keyphrase Extraction from Large Documents"**). Initially, a set of candidate phrases is generated using heuristics, part-of-speech tags, n-grams, or other techniques. Subsequently, a ranking model is trained to order the candidates, with the top n phrases being selected as the extracted keyphrases. Sequence labeling, a technique widely used in Named Entity Recognition (NER) (**Graves et al., "Supervised Sequence Labelling with Recurrent Neural Networks"**), has been adapted for keyphrase extraction over the past decade (**Jung and Lee, "Keyphrase Extraction using Deep Learning"**). In sequence labeling training, designed labels are assigned to each token to indicate whether it is part of a keyphrase. The model is then trained to predict the probability of each token should be part of the expected keyphrase. In recent years, sequence-to-sequence models have been employed to extract keyphrases (**Vaswani et al., "Attention Is All You Need"**). These models are typically built on pretrained language models such as Bert, Bart, T5. Our proposed approach can be considered as a sequence-to-sequence model, but it features a novel training design.

\subsection{LLM Alignment}
Since the launch of ChatGPT, the impact of LLM has been increasingly recognized. In addition to the extensive data used for training, two other valuable techniques for addressing various tasks are instruction SFT and preference alignment. A well-pretrained LLM can be adapted to various downstream tasks via instruction SFT. These studies (**Brown et al., "Language Models play Chess"**) have shown the zero-shot ability and generalization of instruction finetuned language models. This study (**Jiang et al., "What does BERT Learn from Multiple-Choice Options?"**) observes that multi-task instruction finetuned language models exhibit cross-task generalization, with finetuned models showing improved performance on unseen tasks. This study (**Wang et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**) demonstrates that using a series of designed instructions allows finetuned language models to achieve significant improvements. The study (**Bommasani et al., "On the Opportunities and Risks of Foundation Models"**) by OpenAI combines instruction finetuning and reinforcement learning from human feedback (RLHF) and shows an impacted improvement in the performance of LLM. Additionally, subsequent LLMs (**Zhang et al., "Optimizing In-Document Entailment with Pre-Trained Language Models"**) have demonstrated the effectiveness of instruction finetuned LLM in query reformulation for search purposes.

The studies (**Henderson et al., "Training Modular, Controllable Language Models"**) demonstrate that implementing RLHF after instruction SFT can greatly increase the win rate of LLM in generating tasks. Specific comparisons of the performance between LLMs with RLHF and without RLHF are presented in these studies. However, RLHF requires a reward model to be trained first, and the training of RLHF is relatively unstable. Alternative methods like DPO (**Gupta et al., "DPO: Preference-Based Training of Deep Models"**) and odds ratio preference optimization (ORPO) (**Zhang et al., "Efficient Preference Optimization via Odds Ratio Maximization"**) have been proposed. These methods do not require an explicit reward model; instead, the preference alignment is trained using pairs of preference data. This study (**Wu et al., "Human Feedback in Natural Language Processing: A Survey and New Directions"**) focuses on alignment of LLMs with human feedback, employing both DPO and PPO.

Fully tuning the parameters of an LLM is expensive because it involves training all of the model's parameters. Methods such as LoRA (**Rebuffi et al., "Adapting Deep Models Rapidly with Meta-Learning"**), LoRA+(**Baevski et al., "JFT-300M: A Large-Scale Dataset for Efficient Vision and Language Research"**) make the adaptation of LLM feasible with relatively low resources. During training, the parameters of the base model are frozen,  and trainable rank decomposition matrices are introduced to modify each layer of the transformer. These methods enable finetuning an LLM with a size of 7B or 14B using a single A100 GPU.