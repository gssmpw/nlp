\section{Related Works}
\subsection{Keyphrase Extraction}
Keyphrase extraction approaches can be broadly categorized into three main types. Early work in this area follows a pipeline approach, often employing a 2-step method (____). Initially, a set of candidate phrases is generated using heuristics, part-of-speech tags, n-grams, or other techniques. Subsequently, a ranking model is trained to order the candidates, with the top n phrases being selected as the extracted keyphrases. Sequence labeling, a technique widely used in Named Entity Recognition (NER) (____), has been adapted for keyphrase extraction over the past decade (____). In sequence labeling training, designed labels are assigned to each token to indicate whether it is part of a keyphrase. The model is then trained to predict the probability of each token should be part of the expected keyphrase. In recent years, sequence-to-sequence model have been employed to extract keyphrases (____). These models are typically built on pretrained language models such as Bert, Bart, T5. Our proposed approach can be considered as a sequence-to-sequence model, but it features a novel training design.

\subsection{LLM Alignment}
Since the launch of ChatGPT, the impact of LLM has been increasingly recognized. In addition to the extensive data used for training, two other valuable techniques for addressing various tasks are instruction SFT and preference alignment. A well pretrained LLM can be adapted to various downstream tasks via instruction SFT. These studies (____) have shown the zero-shot ability and generalization of instruction finetuned language models. This study (____) observes that multi-task instruction finetuned language models exhibit cross-task generalization, with finetuned models showing improved performance on unseen tasks. This study (____) demonstrates that using a series of designed instructions allows finetuned language models to achieve significant improvements. The study (____) by OpenAI combines instruction finetuning and reinforcement learning from human feedback (RLHF) and shows an impacted improvement in the performance of LLM. Additionally, subsequent LLMs (____) have demonstrated the effectiveness of instruction finetuned LLM in query reformulation for search purposes.

The studies (____) demonstrate that implementing RLHF after instruction SFT can greatly increase the win rate of LLM in generating tasks. Specific comparisons of the performance between LLMs with RLHF and without RLHF are presented in these studies. However, RLHF requires a reward model to be trained first, and the training of RLHF is relatively unstable. Alternative methods like DPO (____) and odds ratio preference optimization (ORPO) (____) have been proposed. These methods do not require an explicit reward model; instead, the preference alignment is trained using pairs of preference data. This study (____) focuses on alignment of LLMs with human feedback, employing both DPO and PPO. DPO exhibits a promising performance in preference alignment, benefiting from a simpler design and requiring fewer resources during training.

Fully tuning the parameters of an LLM is expensive because it involves training all of the model's parameters. Methods such as LoRA (____), LoRA+(____) make the adaptation of LLM feasible with relatively low resources. During training, the parameters of the base model are frozen,  and trainable rank decomposition matrices are introduced to modify each layer of the transformer. These methods enable finetuning an LLM with a size of 7B or 14B using a single A100 GPU.