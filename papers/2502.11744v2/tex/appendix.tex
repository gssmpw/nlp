\clearpage
% \appendix
\onecolumn

\section{Appendix}
\noindent \textbf{A. FUNCTO} \\
In this section, we provide additional qualitative results of the function-centric correspondences established by FUNCTO across five functions.

\begin{figure}[h]
  \centering
  % \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=7in,clip=true,trim=0in 0in 0in 0in]{imgs/FUNCTOs-2.png}};
  \end{tikzpicture}
    % \vspace*{-0.2in}
  \caption{Function-centric correspondences established by FUNCTO across five functions.}
  \label{fig:functos}
  % \vspace*{-0.3in}
\end{figure}

% % TODO: collect tools, three more each function

\newpage


\noindent \textbf{B. Real-Robot Experiment}

\noindent \textbf{Experimental Setup.} All experiments are conducted on the platform depicted in Figure \ref{fig:exp_setup}. The platform consists of a Kinova Gen3 7-DoF robotic arm and an Azure Kinect RGB-D camera. During each trial, a tool object and a target object are placed within the robot’s workspace, which is a 50cm × 30cm region.

% platform, camera, Demo and test objects

\begin{figure}[h]
  \centering
  \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=2.3in,clip=true,trim=0in 0in 0in 0in]{imgs/exp_setup.png}};
  \end{tikzpicture}
    \vspace*{-0.1in}
  \caption{Experimental platform}
  \label{fig:exp_setup}
  % \vspace*{-0.3in}
\end{figure}

\noindent The tool and target objects used in the experiments are shown in Figure \ref{fig:exp_tool} and Figure \ref{fig:exp_target}, respectively. The leftmost tool of each function is used for human demonstration.

\begin{figure}[h]
  \centering
  % \vspace*{-0.2in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=3in,clip=true,trim=0in 0in 0in 0in]{imgs/exp_tool.png}};
  \end{tikzpicture}
    \vspace*{-0.1in}
  \caption{Tool objects used in the real-robot experiments.}
  \label{fig:exp_tool}
  % \vspace*{-0.3in}
\end{figure}

\begin{figure}[h]
  \centering
  % \vspace*{-0.2in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=1.2in,clip=true,trim=0in 0in 0in 0in]{imgs/exp_target.png}};
  \end{tikzpicture}
    \vspace*{-0.1in}
  \caption{Target objects used in the real-robot experiments.}
  \label{fig:exp_target}
  % \vspace*{-0.3in}
\end{figure}

\newpage


\noindent \textbf{Task Success Conditions.} 

\begin{itemize}
    \item \textbf{Pour}: The particles within the tool are transferred into the target container.
    \item \textbf{Cut}: The blade of the tool makes contact with the target from above.
    \item \textbf{Scoop}: The tool collects and securely holds particles from the target container.
    \item \textbf{Pound}: The bottom of the tool head strikes the nail head.
    \item \textbf{Brush}: The tool moves across the target’s surface, displacing particles with its bristle. \\
\end{itemize}


\noindent \textbf{Qualitative Results.} 

\begin{figure}[h]
  \centering
  % \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=6.8in,clip=true,trim=0in 0in 0in 0in]{imgs/qualitative_1.png}};
  \end{tikzpicture}
    \vspace*{-0.2in}
  \caption{Qualitative results of predicted functional keypoints, trajectories, and real-robot executions (pour, cut, scoop).}
  \label{fig:qualitative_1}
  % \vspace*{-0.3in}
\end{figure}

\newpage

\begin{figure}[h]
  \centering
  % \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=4.6in,clip=true,trim=0in 0in 0in 0in]{imgs/qualitative_2.png}};
  \end{tikzpicture}
    \vspace*{-0.2in}
  \caption{Qualitative results of predicted functional keypoints, trajectories, and real-robot executions (brush, pound).}
  \label{fig:qualitative_2}
  % \vspace*{-0.3in}
\end{figure}





% demo: 2D tool + keypoints - 3D tool + keypoints - trajectory - frame (5) 

% test: demo tool - test tool  - trajectory - 4 keyframes (10) 


\newpage

\noindent \textbf{C. Functional Keypoint Transfer Experiment} \\
In addition to the real-robot experiments, we compare the performance of different functional keypoint transfer strategies, with a focus on the function point transfer. \\

\noindent \textbf{Baselines.} We evaluate four function point transfer strategies: 
\begin{itemize}
    \item Demo+VLM+DSC (proposed), which utilizes demonstration functional keypoints as references to prompt the VLM for region proposal, followed by point transfer through a dense semantic correspondence model;
    \item Demo+VLM, which removes the dense semantic correspondence model from the proposed implementation;
    \item Demo+DSC, which relies solely on a dense semantic correspondence model for functional keypoint transfer;
    \item VLM (zero-shot), which directly prompts the VLM to propose functional keypoints in a zero-shot manner. \\
\end{itemize}

\noindent \textbf{Experimental Setup.} For each test tool used in the real-robot experiment, we capture RGB images from 6 different views, covering various positions and orientations within the workspace. Each image has a resolution of 1280*720. A total of 150 images are used for evaluation. A set of examples is shown in Figure \ref{fig:keypoint_view}. 

\begin{figure}[h]
  \centering
  % \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=1.3in,clip=true,trim=0in 0in 0in 0in]{imgs/keypoint_view.png}};
  \end{tikzpicture}
    \vspace*{-0.2in}
  \caption{Examples of collected images for function point transfer evaluation.}
  \label{fig:keypoint_view}
  % \vspace*{-0.3in}
\end{figure}


\noindent \textbf{Evaluation Protocol.} To collect ground truth for function point transfer evaluation,  five volunteers were asked to annotate keypoints on test images using demonstration function points as references. Two evaluation metrics are used: (1) Average Keypoint Distance (AKD), which measures the average pixel distance between ground truth and detected keypoints. (2) Average Precision (AP), which represents the proportion of correctly detected keypoints under various thresholds. AP is evaluated under three thresholds: 15, 30, and 45 pixels. \\

\begin{table}[h]
\centering
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{3pt}%调列距
  \vspace*{-0.1in}
\caption{Quantitative results of Function Point Transfer}
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{AKD (pixel) $\downarrow$} & \multirow{2}{*}{AP@15 (\%) $\uparrow$} & \multirow{2}{*}{AP@30 (\%)$\uparrow$} & \multirow{2}{*}{AP@45 (\%)$\uparrow$} \\
                                 &                              &                            &                            &                            \\ \hline
Demo+VLM                         & 26.42                        & 38.89                      & 68.44                      & 83.56                      \\
Demo+DSC                         & 33.54                        & 47.11                      & 68.67                      & 78.67                      \\
VLM (zero-shot)                  & 56.09                        & 15.56                      & 36.22                      & 52.67                      \\
Demo+VLM+DSC (proposed)          & \textbf{18.54}                        & \textbf{51.33}                      & \textbf{85.78}                      & \textbf{94.44}                      \\ \bottomrule
\end{tabular}
\label{tab:func_transfer}
\end{table}

\noindent \textbf{Quantitative results.} The quantitative results of function point transfer are presented in Table \ref{tab:func_transfer}. The proposed Demo+VLM+DCS consistently outperforms the ablated strategies in both AKD and AP metrics. Demo+VLM achieves reasonable performance by leveraging the rich commonsense knowledge embedded in VLMs. However, VLMs alone struggle to provide precise point-level correspondences, which limits the effectiveness of Demo+VLM compared to the proposed strategy. Meanwhile, relying solely on the dense semantic correspondence model (i.e., Demo+DSC) often fails when faced with large intra-function variations. The performance gap between Demo+VLM and VLM (zero-shot) highlights the importance of demonstration functional keypoints, which serve as valuable references for proposing test functional keypoints.

\newpage




\noindent \textbf{Qualitative results.} 
\begin{figure}[h]
  \centering
  % \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=5in,clip=true,trim=0in 0in 0in 0in]{imgs/keypoint_qualitative.png}};
  \end{tikzpicture}
    \vspace*{-0.1in}
  \caption{Qualitative results of function point transfer.}
  \label{fig:keypoint_qualitative}
  % \vspace*{-0.3in}
\end{figure}
\newpage

% baselines + Experimental Setup + Experimental Protocol + quantitative qualitative results

\noindent \textbf{D. Function-Centric Correspondence Implementation Detail}\\
This section provides the implementation details for functional keypoint transfer and function-centric correspondence. \\

\noindent \textbf{Function Keypoint Transfer.} The pseudo-code for functional keypoint transfer is illustrated in Algorithm \ref{alg:3d_keypoint_transfer}.

\begin{algorithm}[h]
\caption{Functional Keypoint Transfer.}
\label{alg:3d_keypoint_transfer}
\textbf{Input:} \\
\hspace{1em} Demo functional keypoints \( K_H^0 = [p_{\text{func}}^0, p_{\text{grasp}}^0, p_{\text{center}}^0] \), Initial keyframe \( I_0 \), Robot observation \( o_R \), Test tool mask \( M \), \\
\hspace{1em} Vision-language model (VLM), Dense semantic correspondence model \( \Phi \), \\
\hspace{1em} 3D-2D projection \( P_{\text{3D-2D}} \), 2D-3D projection \( P_{\text{2D-3D}} \), 3D center computation \( F_{\text{center}} \) \\
\textbf{Output:} Test functional keypoints \( K_R^0 = [q_{\text{func}}^0, q_{\text{grasp}}^0, q_{\text{center}}^0] \)

\begin{algorithmic}[1]
    \State \( K_R \gets \emptyset \)

    \State \textbf{1. Coarse-Grained Region Proposal:}
    \For{each \( k \in \{\text{func}, \text{grasp}\} \)}
        \State \( p_k^{2D} \gets P_{\text{3D-2D}}(p_k^0, I_0) \)
        \State \( r_k \gets \text{VLM}(p_k^{2D}, I_0, o_R, M) \) \Comment{Region proposal}
    \EndFor

    \State \textbf{2. Fine-Grained Point Transfer:}
    \For{each \( k \in \{\text{func}, \text{grasp}\} \)}
        \State \( q_k^{2D} \gets \Phi(p_k^{2D}, r_k, I_0, o_R) \) \Comment{Point transfer}
        \State \( q_k^0 \gets P_{\text{2D-3D}}(q_k^{2D}, o_R) \)
    \EndFor

    \State \textbf{3. 3D Center Computation:}
    \State \( q_{\text{center}}^0 \gets F_{\text{center}}(M, o_R) \)

    \State \textbf{4. Functional Keypoint Transfer Output:}
    \State \( K_R^0 \gets [q_{\text{func}}^0, q_{\text{grasp}}^0, q_{\text{center}}^0] \)
\end{algorithmic}
\end{algorithm}

\noindent \textbf{Function Plane Construction.} We aim to construct function planes $\Pi_H^{t_f}$  and  $\Pi_R^0$  based on the functional keypoints $ K_H^{t_f} = [p_{\text{func}}^{t_f}, p_{\text{grasp}}^{t_f}, p_{\text{center}}^{t_f}]$  and  $ K_R^0 = [q_{\text{func}}^0, q_{\text{grasp}}^0, q_{\text{center}}^0]$. $\Pi_H^{t_f}$ are defined by the following vectors:


\begin{enumerate}
    \item \textbf{Function Axis} 
    \begin{itemize}
        \item Definition: \[
    \mathbf{u}_H^{t_f} = \frac{p_{\text{func}}^{t_f} - p_{\text{center}}^{t_f}}{\|p_{\text{func}}^{t_f} - p_{\text{center}}^{t_f}\|}
    \]
    \item Description:  $\mathbf{u}_H^{t_f}$  is a normalized vector that defines the function axis. It points from the center point $p_{\text{center}}^{t_f}$ to the function point $p_{\text{func}}^{t_f}$ at $t_f$. This axis represents the primary direction along which the function operates.    
    \end{itemize}
    \item \textbf{Grasp Vector}
    \begin{itemize}
        \item Definition: \[  
    \mathbf{v}_H^{t_f} = \frac{p_{\text{grasp}}^{t_f} - p_{\text{func}}^{t_f}}{\|p_{\text{grasp}}^{t_f} - p_{\text{func}}^{t_f}\|}
        \]
    \item Description: $\mathbf{v}_H^{t_f}$  is a normalized vector that points from the function point $p_{\text{func}}^{t_f}$ to the grasp point $p_{\text{grasp}}^{t_f}$ at $t_f$.
    \end{itemize}
    \item \textbf{Normalized Normal Vector}
    \begin{itemize}
        \item Definition:
        \[ 
    \mathbf{n}_H^{t_f} = \frac{\mathbf{u}_H^{t_f} \times \mathbf{v}_H^{t_f}}{\| \mathbf{u}_H^{t_f} \times \mathbf{v}_H^{t_f} \|}
        \]
        \item Description: $\mathbf{n}_H^{t_f}$ is the unit normal vector of the function plane $\Pi_H^{t_f}$.
    \end{itemize}
    \item \textbf{Function Plane}
    \begin{itemize}
        \item Definition:
        \[   
    \Pi_H^{t_f}: (\mathbf{p} - p_{\text{func}}^{t_f}) \cdot \mathbf{n}_H^{t_f} = 0
        \]
        \item Description: The function plane is defined by the function point and its normal vector, describing the tool’s orientation and spatial configuration at $t_f$. 
    \end{itemize}
\end{enumerate}
Similarly, \(\mathbf{u}_R^0\), \(\mathbf{v}_R^0\), and \(\mathbf{n}_R^0\) are defined for \( \Pi_R^0 \).\\


\noindent \textbf{Function Axis Alignment.} Simply aligning the function axes of the demonstration and test tools may not yield a feasible function keyframe pose for the test tool. This is due to substantially different relative locations of the three functional keypoints (particularly for cross-category generalization). As is shown in Figure \ref{fig:axis_example}, the two function keyframe poses in Step 3 may fail to achieve successful task executions, as the teapot and axe are not tilted sufficiently.

\begin{figure}[h]
  \centering
  % \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=1.1in,clip=true,trim=0.2in 0in 0in 0in]{imgs/func_axis_align_example.png}};
  \end{tikzpicture}
    \vspace*{-0.1in}
  \caption{Examples of function axis alignment.}
  \label{fig:axis_example}
  % \vspace*{-0.3in}
\end{figure}


\noindent To address this issue, we refine the function axis alignment using the VLM. Specifically, in Step 3, we rotate the function plane with the function point as the origin and the normal vector as the rotation axis. Seven angle offsets ranging from $[-45^{\circ}, -45^{\circ}]$ are applied , including $-45^{\circ}$, $-30^{\circ}$, $-10^{\circ}$, $0^{\circ}$, $10^{\circ}$, $30^{\circ}$, $45^{\circ}$. Next, the combined point cloud of each rotated test tool and the target are back-projected onto the camera planes using the camera intrinsic matrix, rendering seven synthetic 
function keyframes. Examples of rendered synthetic function keyframes are presented in Figure \ref{fig:axis_render}. Then, we prompt the VLM, using the demonstration function keyframe as the reference, to  identify the image that represents the optimal state conducive to the task success. The detailed prompt is given in Appendix G. Finally, the rotation transformation corresponding to the optimal function keyframe state is recorded for function axis alignment (Step 4). 

\begin{figure}[h]
  \centering
  % \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=0.7in,clip=true,trim=0in 0in 0in 0in]{imgs/func_axis_align_rendering.png}};
  \end{tikzpicture}
    % \vspace*{-0.1in}
  \caption{Examples of rendered synthetic function keyframes. The demonstration function keyframe is highlighted in yellow, and the selected test function keyframe is highlighted in green.}
  \label{fig:axis_render}
  % \vspace*{-0.3in}
\end{figure}

\newpage






% % detail + 5 examples (demo + test + selected)

% \textbf{D.2 Qualitative Result}



% demo function frame + points -> test initial frame + points -> predicted frame + points (5)



\noindent \textbf{E. Trajectory Optimization Implementation Detail} \\
In the section, we provide implementation details for trajectory optimization, complementing the constrained optimization problem  formulated in the manuscript.  \\

\noindent \textbf{Demonstration Trajectory and Pose Wrapping.} As shown in Figure \ref{fig:init_align}, Step 1, the demonstration and test tools are positioned on opposite sides of the target. While the demonstration trajectory requires the test tool to approach from the left side for pouring, the target object’s rotational symmetry about the z-axis allows approaching from the right side as well. This symmetry, common among target objects in the experiment, enables us to warp the demonstration trajectory and function keyframe pose to generate shorter and easier-to-execute test tool trajectories.

\begin{figure}[h]
  \centering
  % \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=2.5in,clip=true,trim=0in 0in 0in 0in]{imgs/init_state_align.png}};
  \end{tikzpicture}
    \vspace*{-0.1in}
  \caption{Demonstration trajectory and function keyframe pose wrapping.}
  \label{fig:init_align}
  % \vspace*{-0.3in}
\end{figure}

\noindent In Figure \ref{fig:init_align}, Step 2, we align the demonstration function frame pose with the test tool by rotating the demonstration functional keypoints around the z-axis. The rotation angle is computed based on the angular difference between the demonstration and test function points. Similarly, in Step 3, we wrap the demonstration trajectory through two operations:  (1) aligning it with the test tool by rotating around the z-axis and (2) scaling its translational component based on the test tool’s function point. Finally, the wrapped demonstration trajectory and function keyframe pose serve as the reference and constraint for trajectory optimization (Step 4).  \\

\noindent \textbf{Optimization Constraints and Costs.} In addition to the trajectory cost and the keyframe pose constraints described in the manuscript, we incorporate the following adjustments: 

\begin{itemize}
    \item Early Trajectory Cost Relaxation. The trajectory cost is omitted for the initial 30\% of the trajectory, as the interaction primarily occurs during the later phases. This approach also allows the optimizer to explore more feasible paths and ensure smoother transitions to the interaction phase, particularly when the initial states of the demonstration and test tools differ significantly.
    \item Velocity Constraint. We impose limits on the translational and angular velocities of the test tool to ensure smooth and physically feasible trajectory generation.
    \item Collision Avoidance Constraint. This constraint enforces a minimum Euclidean distance between the test tool and the 3D bounding box of the obstacle to prevent collisions during trajectory execution. 
\end{itemize}
\noindent We employ CasADi as the optimization framework for symbolic modeling and automatic differentiation. IPOPT is used as the solver to efficiently handle the nonlinear programming problem with constraints.
\newpage




\noindent \textbf{F. PD Controller Implementation Detail} \\
This section details the practical implementation, control architecture, and parameter selection for the implemented velocity-based PD controller.\\

\noindent \textbf{Controller Architecture.} The controller implements joint velocity control with null space optimization.
The input commands are received at $10\text{ Hz}$ while the controller operates at a higher frequency of $200\text{ Hz}$. To ensure smooth motion, we implement trajectory interpolation between commanded poses. \\

\noindent The PD control law is formulated separately for translation and rotation. 

\begin{enumerate}
    \item For the translational motion: 
    \[
        \mathbf{v}_{\text{lin}} = \begin{bmatrix}
            v_x \\
            v_y \\
            v_z
        \end{bmatrix} = \underbrace{\mathbf{K}_p}_{\text{proportional}} \underbrace{\begin{bmatrix}
            e_{p,x} \\
            e_{p,y} \\
            e_{p,z}
        \end{bmatrix}}_{\text{position error}} + \underbrace{\mathbf{K}_d}_{\text{derivative}} \underbrace{\begin{bmatrix}
            \dot{e}_{p,x} \\
            \dot{e}_{p,y} \\
            \dot{e}_{p,z}
        \end{bmatrix}}_{\text{velocity error}}
    \]
    \item For the rotational motion:
    \[
        \boldsymbol{\omega} = \begin{bmatrix}
            \omega_x \\
            \omega_y \\
            \omega_z
        \end{bmatrix} = \underbrace{\mathbf{K}_{p,\text{rot}}}_{\text{proportional}} \underbrace{\begin{bmatrix}
            e_{\theta,x} \\
            e_{\theta,y} \\
            e_{\theta,z}
        \end{bmatrix}}_{\text{orientation error}} + \underbrace{\mathbf{K}_{d,\text{rot}}}_{\text{derivative}} \underbrace{\begin{bmatrix}
            \dot{e}_{\theta,x} \\
            \dot{e}_{\theta,y} \\
            \dot{e}_{\theta,z}
        \end{bmatrix}}_{\text{angular velocity error}}
    \]
\end{enumerate}

\noindent where the control gains are represented by the following  matrices:
\[
    \mathbf{K}_p = \begin{bmatrix} 
        3.0 & 0 & 0 \\
        0 & 3.0 & 0 \\
        0 & 0 & 3.0
    \end{bmatrix} \in \mathbb{R}^{3 \times 3}, \quad
    \mathbf{K}_d = \begin{bmatrix}
        0.001 & 0 & 0 \\
        0 & 0.001 & 0 \\
        0 & 0 & 0.001
    \end{bmatrix} \in \mathbb{R}^{3 \times 3}
\]
\[
    \mathbf{K}_{p,\text{rot}} = \begin{bmatrix}
        3.0 & 0 & 0 \\
        0 & 3.0 & 0 \\
        0 & 0 & 3.0
    \end{bmatrix} \in \mathbb{R}^{3 \times 3}, \quad
    \mathbf{K}_{d,\text{rot}} = \begin{bmatrix}
        0.01 & 0 & 0 \\
        0 & 0.01 & 0 \\
        0 & 0 & 0.01
    \end{bmatrix} \in \mathbb{R}^{3 \times 3}
\]
\\
\noindent \textbf{Trajectory Interpolation.} To address the frequency mismatch between command inputs and controller execution, we implement a trajectory interpolation scheme. Given two consecutive desired poses at times $t_k$ and $t_{k+1}$:
\[
    \mathbf{X}_k = \begin{bmatrix} \mathbf{p}_k \\ \mathbf{q}_k \end{bmatrix}, \quad \mathbf{X}_{k+1} = \begin{bmatrix} \mathbf{p}_{k+1} \\ \mathbf{q}_{k+1} \end{bmatrix}
\]

\noindent where $\mathbf{p}$ represents position and $\mathbf{q}$ represents orientation in quaternion form. 

\noindent The number of interpolation points is determined by:
\[
    N = \max(1, \lfloor (t_{k+1} - t_k) f_c \rfloor)
\]

\noindent For position interpolation, we use linear interpolation:
\[
    \mathbf{p}(s) = (1-s)\mathbf{p}_k + s\mathbf{p}_{k+1}, \quad s \in [0,1]
\]

\noindent For orientation interpolation, we use spherical linear interpolation (SLERP):
\[
    \mathbf{q}(s) = \frac{\sin((1-s)\Omega)}{\sin(\Omega)}\mathbf{q}_k + \frac{\sin(s\Omega)}{\sin(\Omega)}\mathbf{q}_{k+1}
\]

\noindent where $\Omega = \arccos(\mathbf{q}_k \cdot \mathbf{q}_{k+1})$ is the angle between quaternions. 

\noindent The interpolation parameter $s$ is discretized as:
\[
    s_i = \frac{i}{N-1}, \quad i = 0,\ldots,N-1
\]
\\
\noindent \textbf{Error Computation.} The translational error is computed directly in Cartesian space:
\[
    \mathbf{e}_p = \mathbf{x}_{\text{des}} - \mathbf{x}_{\text{cur}}
\]

\noindent For safety, the controller implements a position error threshold:
\[
    \|\mathbf{e}_p\| \leq 0.5 \text{ m}
\]

\noindent The rotational error is computed using rotation matrices:
\[
    \mathbf{R}_{\text{error}} = \mathbf{R}_{\text{des}} \mathbf{R}_{\text{cur}}^{-1}
\]

\noindent The error is converted to axis-angle representation and normalized to ensure the rotation angle remains within $[-\pi, \pi]$:
\[
    \mathbf{e}_\theta = \begin{cases}
        \mathbf{e}_{\text{axis-angle}} & \text{if } \|\mathbf{e}_{\text{axis-angle}}\| \leq \pi \\
        \mathbf{e}_{\text{axis-angle}} \frac{\|\mathbf{e}_{\text{axis-angle}}\| - 2\pi}{\|\mathbf{e}_{\text{axis-angle}}\|} & \text{otherwise}
    \end{cases}
\]
\\
\noindent \textbf{Joint Space Control.} For the joint velocity control mode, the Cartesian velocities are mapped to joint space using the manipulator Jacobian:
\[
    \dot{\mathbf{q}} = \mathbf{J}^\dagger \begin{bmatrix} \mathbf{v}_{\text{lin}} \\ \boldsymbol{\omega} \end{bmatrix} + \mathbf{N}\dot{\mathbf{q}}_0
\]

\noindent where:
\begin{itemize}
    \item $\mathbf{J}^\dagger$ is the Moore-Penrose pseudoinverse of the Jacobian
    \item $\mathbf{N} = \mathbf{I} - \mathbf{J}^\dagger\mathbf{J}$ is the null space projector
    \item $\dot{\mathbf{q}}_0$ is the null space velocity\\
\end{itemize}

\noindent \textbf{Null Space Optimization.} The null space velocity combines two objectives:
\[
    \dot{\mathbf{q}}_0 = K_{\text{home}}\left(\mathbf{q}_{\text{home}} - \mathbf{q}_{\text{cur}}\right) - K_{\text{min}}\mathbf{q}_{\text{cur}}
\]

\noindent where:
\begin{itemize}
    \item $K_{\text{home}} = 0.1$ is the gain for home configuration attraction
    \item $K_{\text{min}} = 0.05$ is the gain for joint velocity minimization
    \item $\mathbf{q}_{\text{home}}$ is the preferred home configuration
\end{itemize}

\newpage




\noindent \textbf{G. VLM Prompting Implementation Detail}


\begin{center}
\begin{tcolorbox}[colback=gray!5, colframe=black!40, sharp corners=south, title= Function Point Detection Prompt]\small

Given an interaction frame between two objects, select pre-defined keypoints. \\

The input request contains:  
\begin{itemize}
    \item The task information as dictionaries. The dictionary contains these fields: 
    \begin{itemize}
        \item `\textbf{instruction}': The task in natural language forms.
        \item `\textbf{object\_grasped}': The object that the human holds in hand while executing the task.
        \item `\textbf{object\_unattached}': The object that the human will interact with `object\_grasped' without holding it in hand.
    \end{itemize}
    \item  An image of the current table-top environment captured from a third-person view camera, annotated with a set of visual marks:
    \begin{itemize}
        \item \textbf{candidate keypoints on `object\_grasped'}: Red dots marked as `P\textsubscript{i}' on the image, where [i] is an integer. \\
    \end{itemize}
\end{itemize}

The interaction is specified by `function\_keypoint' on the `object\_grasped':
\begin{itemize}
    \item The human hand grasps `object\_grasped' and moves the `function\_keypoint' to approach `object\_unattached'.
    \item `\textbf{function\_keypoint}': The point on `object\_grasped' indicating the part that will contact `object\_unattached'. \\
\end{itemize}

The response should be a dictionary in JSON form, which contains:
\begin{itemize}
    \item `\textbf{function\_keypoint}': Selected from candidate keypoints marked as `P\textsubscript{i}' on the image.\\
\end{itemize}

Think about this step by step:
\begin{enumerate}[leftmargin=*, label=\arabic*.]
    \item Describe the region where interaction between `object\_grasped' and `object\_unattached' happens.
    \item Select `function\_keypoint' on the `object\_grasped' within the interaction region.
\end{enumerate}
\end{tcolorbox}
\end{center}


\begin{center}
\begin{tcolorbox}[colback=gray!5, colframe=black!40, sharp corners=south, title= Function Point Transfer Prompt]\small

Refer to the position of red keypoint on the first example image, select corresponding pre-defined keypoints on the second test image. \\

The input request contains:  
\begin{itemize}
    \item The task information as dictionaries. The dictionary contains these fields: 
    \begin{itemize}
        \item `\textbf{instruction}': The task in natural language forms.
        \item `\textbf{object\_grasped}': The object that the human holds in hand while executing the task.
        \item `\textbf{object\_unattached}': The object that the human will interact with `object\_grasped' without holding it in hand.
    \end{itemize}
    \item An example image annotated with a red keypoint.
    \item A test image of the current table-top environment captured from a third-person view camera, annotated with a set of visual marks:
    \begin{itemize}
        \item \textbf{candidate keypoints on `object\_grasped'}: Red dots marked as `P\textsubscript{i}' on the image, where [i] is an integer. \\
    \end{itemize}
\end{itemize}

The interaction is specified by `function\_keypoint' on the `object\_grasped':
\begin{itemize}
    \item Select the candidate keypoint on the test image corresponds to the red keypoint annotated on the example image.
    \item `\textbf{function\_keypoint}': The point on `object\_grasped' indicating the part that will contact `object\_unattached'. \\
\end{itemize}

The response should be a dictionary in JSON form, which contains:
\begin{itemize}
    \item `\textbf{function\_keypoint}': Selected from candidate keypoints marked as `P\textsubscript{i}' on the image. \\
\end{itemize}

Think about this step by step:
\begin{enumerate}[leftmargin=*, label=\arabic*.]
    \item Describe the object part where keypoint is located on the example image.
    \item Describe the region where interaction between `object\_grasped' and `object\_unattached' happens.
    \item Select `function\_keypoint' on the `object\_grasped' within the interaction region on the test image.
\end{enumerate}
\end{tcolorbox}
\end{center}

\newpage


\begin{center}
\begin{tcolorbox}[colback=gray!5, colframe=black!40, sharp corners=south, title= Grasp Point Transfer Prompt]\small

Refer to the position of red keypoint on the first example image, select corresponding pre-defined keypoints on the second test image. \\

The input request contains:  
\begin{itemize}
    \item The task information as dictionaries. The dictionary contains these fields: 
    \begin{itemize}
        \item `\textbf{instruction}': The task in natural language forms.
        \item `\textbf{object\_grasped}': The object that the human holds in hand while executing the task.
        \item `\textbf{object\_unattached}': The object that the human will interact with `object\_grasped' without holding it in hand.
    \end{itemize}
    \item An example image annotated with a red keypoint.
    \item A test image of the current table-top environment captured from a third-person view camera, annotated with a set of visual marks:
    \begin{itemize}
        \item \textbf{candidate keypoints on `object\_grasped'}: Red dots marked as `P\textsubscript{i}' on the image, where [i] is an integer. \\
    \end{itemize}
\end{itemize}

The interaction is specified by `grasp\_keypoint' on the `object\_grasped':
\begin{itemize}
    \item Select the candidate keypoint on the test image corresponds to the red keypoint annotated on the example image.
    \item The human hand grasps the `object\_grasped' at the `grasp\_keypoint’.
    \item `\textbf{grasp\_keypoint}': The point on `object\_grasped' indicates the part where the hand should hold. \\
\end{itemize}

The response should be a dictionary in JSON form, which contains:
\begin{itemize}
    \item `\textbf{grasp\_keypoint}': Selected from candidate keypoints marked as `P\textsubscript{i}' on the image. \\
\end{itemize}

Think about this step by step:
\begin{enumerate}[leftmargin=*, label=\arabic*.]
    \item Describe the object part where keypoint is located on the example image.
    \item Find the part on `object\_grasped’ where humans usually grasp.
    \item Select `grasp\_keypoint' on the `object\_grasped' within the interaction region on the test image.
\end{enumerate}
\end{tcolorbox}
\end{center}



\begin{center}
\begin{tcolorbox}[colback=gray!5, colframe=black!40, sharp corners=south, title= Function Axis Alignment Prompt]

From a list of interaction frames between the tool and target objects, select the image that represents the state most conducive to completing the task. \\


The input request contains:  
\begin{itemize}
    \item The task information as dictionaries. The dictionary contains these fields: 
    \begin{itemize}
        \item `\textbf{instruction}': The task in natural language forms.
        \item `\textbf{object\_grasped}': The object that the human holds in hand while executing the task.
        \item `\textbf{object\_unattached}': The object that the human will interact with `object\_grasped' without holding it in hand.
    \end{itemize}
    \item A list of interaction frames between the tool and target objects. \\
\end{itemize} 

The response should be a dictionary in JSON form, which contains:
\begin{itemize}
    \item `\textbf{selected\_idx}': the idx of the selected image.
\end{itemize}
\end{tcolorbox}
\end{center}

\newpage
\noindent \textbf{H. Q\&A Section} \\
In this section, we address common questions about our method, clarifying potential concerns, discussing limitations, and providing insights into its design, implementation, future improvements, and broader applicability. \\

\begin{itemize}
    \item \textbf{Q1: Can this method be applied to a wider range of tool manipulation tasks?} \\
    \textbf{A1:} FUNCTO is generally applicable to tool manipulation tasks involving two-object interactions (tool and target), where object dynamics are less critical to task success. Examples include peeling, sweeping, stirring, picking, placing, mixing, inserting, stacking, and flipping.\\

    \item \textbf{Q2: Can the proposed functional keypoint transfer strategy be extended to a broader range of applications beyond those demonstrated in the paper} \\
    \textbf{A2:} Yes, the proposed functional keypoint transfer strategy can be applied to other tasks involving semantic correspondences. Notably, we have adapted this approach for semantic keypoint transfer in cloth manipulation. \\

    \item \textbf{Q3: How can this method leverage additional demonstrations for improved performance?} \\
    \textbf{A3:} As discussed in the Limitations subsection, a function is inherently multi-modal. Therefore, FUNCTO can leverage few-shot human demonstrations for multi-modal modeling. During inference, the robot can retrieve the ``best" demonstration from the database to enhance task execution. \\

    \item \textbf{Q4: What are the benefits of explicitly representing skill-use keypoints and trajectories?} \\
    \textbf{A4:} The interpretable explicit representation can be integrated with existing task and motion planning algorithms, enabling the execution of long-horizon tool manipulation tasks. \\


    \item \textbf{Q5: What is the role of foundation models in this approach, and why are they essential?} \\
    \textbf{A5:} Foundation models provide commonsense knowledge, enabling the inference of information that cannot be directly extracted from geometric or visual cues. For instance, tools with the same function may exhibit significant intra-function variations. Transferring functional keypoints based solely on geometric or visual similarities prone to failures. Additionally, without the commonsense reasoning embedded in foundation models, the robot may struggle to accurately infer the correct functional axis alignment transformation. \\
\end{itemize}





