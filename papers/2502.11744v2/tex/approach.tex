\begin{figure*}[t]
  \centering
  \vspace*{-0.2in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=5.5in,clip=true,trim=0in 0in 0in 0in]{imgs/pipeline-4.png}};
  \end{tikzpicture}
    \vspace*{-0.1in}
  \caption{An overview of the FUNCTO framework. The pipeline consists of three stages: (1) Functional keypoint extraction, where functional keypoints and their trajectories are extracted from the human demonstration video; (2) Function-centric correspondence establishment, where function-centric correspondences between demonstration and test tools are established using geometric constraints on the functional keypoints; and (3) Functional keypoint-based action planning, where the test tool trajectory is synthesized and executed to accomplish a functionally equivalent task.}
  \label{fig:pipeline}
  \vspace*{-0.2in}
\end{figure*} 


\section{Problem Formulation}
We consider the problem of enabling the robot to imitate the tool manipulation behavior demonstrated in a single human video to accomplish functionally equivalent tasks using novel tools with the same function. Specifically, each task involves a robot grasping a tool (object) with a parallel-jaw gripper to interact with a target (object) in a tabletop environment. The task is defined by a list of spatiotemporal constraints between the tool and the target.



% Task success is defined by satisfying spatiotemporal constraints between the tool and the target. 

% The key challenge lies in establishing accurate correspondences between demonstration and test tools, despite large intra-function variations.

During the demonstration phase, a human performs a tool manipulation task, recording a sequence of RGB-D images, $\mathcal{V}_H = \{I_t\}_{t=0}^{N-1}$, with a stationary camera, where $N$ denotes a finite task horizon. The sequence $\mathcal{V}_H$ is paired with a natural language task description $l_H$ (e.g., \textit{``use the \underline{mug} to \underline{pour} contents into the \underline{bowl}"}) that specifies three elements: a tool (e.g., \textit{mug}), a target (e.g., \textit{bowl}), and a function (e.g., \textit{pour}). During inference, given a robot observation $o_R$ and a corresponding task description $l_R$, the objective is to develop an OSIL policy $\pi$, mapping $o_R$ and $l_R$ to a robot end-effector trajectory $\tau_R = \{a_t\}_{t=0}^{N-1}$ that maximizes the likelihood of task success. Here, $a_t = (R_t, T_t) \in \text{SE(3)}$ represents the 6-DoF end-effector pose at timestep $t$, where $R_t \in \text{SO(3)}$ and $T_t \in \mathbb{R}^3$ denote 3D orientation and translation, respectively. 

\noindent \textbf{Assumptions.} During the implementation, we have made the following assumptions: (1) Visual observations are single-view and do not contain any action annotations. (2) The robot has no object/task-specific prior knowledge, such as 3D object models or manual task constraints, but has access to commonsense knowledge embedded in foundation models. (3) No in-domain pre-training is conducted. (4) Tools are modeled as rigid objects and can be manipulated by the designated gripper. 


\section{FUNCTO}


\subsection{Overview}

In this section, we describe FUNCTO, a function-centric OSIL framework for tool manipulation. FUNCTO consists of three stages: (1) functional keypoint extraction, (2) function-centric correspondence establishment, and (3) functional keypoint-based action planning. An overview of the proposed framework is presented in Figure \ref{fig:pipeline}. Specifically, in the first stage (Section \ref{detection}), FUNCTO detects 3D functional keypoints $K_H$ and track their motions $\{K_H^t\}_{t=0}^{N-1}$ from $\mathcal{V}_H$. In the second stage (Section \ref{correspondence}), $K_H$ are transferred from the demonstration tool to the test tool to obtain test functional keypoints $K_R$. Subsequently, FUNCTO establishes function-centric correspondences with $K_H$ and $K_R$. In the final stage (Section \ref{planning}), FUNCTO generates a robot end-effector trajectory $\tau_R$, using the reference $\{K_H^t\}_{t=0}^{N-1}$ and the established function-centric correspondences, for execution.


\subsection{Functional Keypoint Extraction}\label{detection}

\noindent \textbf{Demonstration Tool Tracking.} FUNCTO starts by localizing and segmenting the tool and the target in the first frame of $\mathcal{V}_H$ (i.e., $I_0$) using Grounding-SAM \cite{ren2024grounded}. Subsequently, $N_k$ keypoints are uniformly sampled within the tool mask. We use Cotracker \cite{karaev2025cotracker} to capture their 3D motions (using the depth information) for the rest of $\mathcal{V}_H$, yielding their 3D trajectories in the camera frame. To ensure the extracted motion is independent of the absolute locations of the tool and target, FUNCTO transforms the 3D keypoint trajectories from the camera frame to the target (object) frame by estimating the target object pose in the camera frame. The origin of the target frame, denoted as the target point $p_{\text{target}}$, is defined as the 3D center of the target. Finally, FUNCTO utilizes rigid body transformation \cite{bottema1990theoretical} to calculate the relative transformations of the tool between consecutive timesteps based on the 3D keypoint trajectories. Throughout this section, all 3D elements are represented in the target frame unless otherwise specified.  \\

% \noindent \textbf{Demonstration Tool Tracking.} FUNCTO starts by localizing the tool and the target in the human demonstration video. Given $\mathcal{V}_H$ and $l_H$, an LLM is first prompted to parse the tool label (e.g., \textit{mug}) and the target label (\textit{bowl}) from $l_H$. These labels are then used to segment the tool and target in the first frame of $\mathcal{V}_H$ (i.e., $I_0$) using Grounding-SAM \cite{ren2024grounded}. Subsequently, $N_k$ keypoints are uniformly sampled within the tool mask. We use Cotracker \cite{karaev2025cotracker} to capture their 3D motions (using the depth information) for the rest of $\mathcal{V}_H$, yielding their 3D trajectories in the camera frame. To ensure the extracted motion is independent of the absolute locations of the tool and target, FUNCTO transforms the keypoint trajectories from the camera frame to the target (object) frame by estimating the target object pose in the camera frame. The origin of the target frame, denoted as the target point $p_{\text{target}}$, is defined as the 3D center of the target. By modeling the relative spatial relationship between the tool and the target, FUNCTO inherently facilitates spatial generalization to unseen layouts. Finally, FUNCTO utilizes rigid body transformation \cite{bottema1990theoretical} to calculate the relative transformations of the tool between consecutive timesteps based on the keypoint trajectories. Throughout this section, all 3D elements are represented in the target frame unless otherwise specified.  \\

\noindent \textbf{Keyframe Discovery.} Due to the partial observability of functional keypoints in the human demonstration video, FUNCTO discovers keyframes where functional keypoints on the demonstration tool can be effectively identified. Specifically, FUNCTO discovers four keyframes: (1) the initial keyframe $I_0$ ($t=0$), where the tool is in its initial state; (2) the grasping keyframe $I_g$ ($t=t_g$), where the hand grasps the tool; (3) the function keyframe $I_f$ ($t=t_f$), where the interaction between the tool and target starts; (4) the pre-function keyframe $I_{p}$ ($t=t_{p}$), where the interaction is about to start while both the tool and target remain clearly visible. These keyframes satisfy $0 < t_g < t_p < t_f < N-1$. Note that $I_{p}$ is essential for function point detection before heavy occlusion associated with the interaction occurs. For detecting $I_g$, we use an off-the-shelf hand-object detector from \cite{shan2020understanding}. Following \cite{zhu2024vision}, an unsupervised change point detection method \cite{killick2012optimal} is employed to identify $I_f$ based on velocity statistics derived from the 3D keypoint trajectories. Lastly, we backtrack through $\mathcal{V}_H$ to locate a preceding frame (i.e., $I_{p}$) where the occlusion between the tool and the target, measured by IoU, is below a pre-defined threshold. Examples of discovered keyframes are presented in Figure \ref{fig:pipeline} (stage 1).\\

\noindent \textbf{Functional Keypoint Detection.} Once the keyframes are identified, FUNCTO proceeds to detect 3D functional keypoints and track their motions, denoted as $\{K_H^t\}_{t=0}^{N-1} = \{[p_{\text{func}}^t, p_{\text{grasp}}^t, p_{\text{center}}^t]\}_{t=0}^{N-1}$. $p_{\text{func}}^t$, $p_{\text{grasp}}^t$, and $p_{\text{center}}^t$ represent the 3D locations of the function point, grasp point, and center point at timestep $t$, respectively, with $p \in \mathbb{R}^3$.

% $p_{\text{grasp}}^{t_g}$ 
% $p_{\text{center}}^{0}$
% $p_{\text{func}}$
Three functional keypoints are detected in the respective keyframes. The grasp point is determined by computing the intersection between the hand mask and the tool mask in $I_g$, while the center point is computed as the 3D center of the tool in $I_0$. Detecting the function point is non-trivial, as there may be no physical contact between the tool and the target (e.g., pouring), and it also requires commonsense knowledge about tool usage. Therefore, FUNCTO leverages the multi-modal reasoning capabilities of VLMs and employs mark-based visual prompting \cite{nasirianypivot}. The visual prompting process involves two steps: (1) task-agnostic meta-prompt definition and (2) task-specific prompting. In the first step, we provide a meta-prompt with task-agnostic context information to the VLM, including the definition of the function point, the expected behavior of the VLM, and the desired response format. In the second step, we sample and annotate $N_c$ candidate points on the tool's boundary in $I_{p}$ with Farthest Point Sampling \cite{qi2017pointnet}, assigning each point a unique index. Guided by visual cues from the human demonstration indicating where interaction occurs, $I_{p}$ and $l_H$ are then used to prompt the VLM with a multi-choice problem among $N_c$ candidates to determine the function point.

\begin{figure}[t]
  \centering
  \vspace*{-0.2in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=1.5in,clip=true,trim=0.2in 0in 0in 0in]{imgs/point_frame_def-1.png}};
  \end{tikzpicture}
    \vspace*{-0.2in}
  \caption{A graphical illustration of the function point, grasp point, center point, effect point, target point, and target frame.}
  \label{fig:point_frame_def}
  \vspace*{-0.2in}
\end{figure}

After identifying three functional keypoints in their respective keyframes, we track their motions using the previously computed sequence of relative transformations, resulting in $\{K_H^t\}_{t=0}^{N-1}$. Additionally, $p_{\text{func}}^{t_f}$ is attached to the target, referred to as the effect point $p_{\text{eff}}$, to represent the 3D location where the interaction occurs on the target object. Figure \ref{fig:point_frame_def} provides a graphical illustration of the function point, grasp point, center point, effect point, target point, and target frame.


\subsection{Function-Centric Correspondence Establishment}\label{correspondence}
% $K_H^0$
\noindent \textbf{Functional Keypoint Transfer.} At the core of FUNCTO lies the function-centric correspondence establishment using functional keypoints. FUNCTO achieves this by first transferring the functional keypoints from the demonstration tool to the test tool, obtaining $K_R^0 = [q_{\text{func}}^0, q_{\text{grasp}}^0, q_{\text{center}}^0]$, where $q \in \mathbb{R}^3$. 

The functional keypoint transfer process, illustrated in Figure \ref{fig:pipeline} (stage 2),  consists of a coarse-grained region proposal and a fine-grained point transfer. In the first step, we begin by projecting $p_{\text{func}}^0$ and $p_{\text{grasp}}^0$ from the 3D space onto the image plane $I_0$. The marked $I_0$ serves as a reference for identifying test tool functional keypoints. Providing such a reference is essential, as the functional keypoint location is coupled with the demonstrated human action. When the test tool has multiple possible functional keypoints, selecting a functional keypoint not matching the intended action can lead to failure. Figure \ref{fig:keypoint_proposal} provides a qualitative comparison of function point transfer with and without using the reference.

Similar to Section \ref{detection}, FUNCTO first utilizes mark-based visual prompting to propose coarse candidate regions on the test tool for 2D function and grasp points. Compared to functional keypoint detection in the previous stage, two key differences are: (1) the marked  $I_0$  is additionally provided to the VLM as a reference, and (2) the selected candidate point is expanded into a candidate region, with its size adaptively adjusted based on the 2D dimensions of the test tool. In the second step, we employ a pre-trained dense semantic correspondence model \cite{zhang2024tale} to precisely transfer 2D function and grasp points from $I_0$ to candidate regions in $o_R$, resulting in  $q_{\text{func}}^0$  and $q_{\text{grasp}}^0$. The dense semantic correspondence model provides finer point-level correspondences compared to using the VLM alone. $q_{\text{center}}^0$ is computed as the 3D center of the test tool. For the target object, we scale $p_{\text{eff}}$  based on the 3D dimension ratio between the demonstration and test targets to obtain  $q_{\text{eff}}$. $K_R$ and  $q_{\text{eff}}$  are expressed in the test target frame. \\

 

% The detailed implementation is illustrated in Algorithm~\ref{alg:3d_keypoint_transfer}. 

%, which integrates features from DINOv2 and Stable Diffusion \cite{rombach2022high}, 

\begin{figure}[t]
  \centering
  \hspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=1.0in,clip=true,trim=0in 0in 0in 0in]{imgs/keypoint_proposal.png}};
  \end{tikzpicture}
    \vspace*{-0.3in}
  \caption{Qualitative results of function point transfer. (a) shows the function point extracted from the human demonstration. Function points in (b) and (c) are proposed by the VLM in a zero-shot manner. (d) shows the transferred function point using (a) as a reference.}
  \label{fig:keypoint_proposal}
  \vspace*{-0.2in}
\end{figure}





\noindent \textbf{Function-Centric Correspondence.} FUNCTO formulates the function-centric correspondence as geometric constraints on 3D functional keypoints, inspired by \cite{manuelli2019kpam}. Specifically, this step specifies the function keyframe constraint (i.e., the desired test tool state at $t_f$) for trajectory generation.

 % \( p_{\text{func}}^{t_f} \) and \( q_{\text{func}}^0 \) 
 % $\mathbf{n}_H^{t_f}$ and $\mathbf{n}_R^0$
 % $\mathbf{u}_H^{t_f}$ and $\mathbf{u}_R^0$
Formally, the function keyframe constraint can be represented by a rigid transformation $\mathbf{T}_{\text{func}} \in \text{SE(3)}$ that aligns $K_H^{t_f}$ and $K_R^0$. This process, illustrated in Figure \ref{fig:pipeline} (stage 2), can be divided into the following steps:
\begin{enumerate}[label=\arabic*., start=0]
    \item \textbf{Function Plane Construction}: Given $K_H^{t_f}$ and $K_R^0$, function planes \( \Pi_H^{t_f} \) and \( \Pi_R^0 \) are constructed as follows:
\begin{itemize}
    \item \(\mathbf{u}_H^{t_f}\) (function axis): A normalized vector pointing from the center to the function point at $t_f$.
    \item \(\mathbf{v}_H^{t_f}\): A normalized vector pointing from the function to the grasp point at $t_f$.
    \item \(\mathbf{n}_H^{t_f}\): The unit normal vector at $t_f$.
\end{itemize}
Similarly, \(\mathbf{u}_R^0\), \(\mathbf{v}_R^0\), and \(\mathbf{n}_R^0\) are defined for \( \Pi_R^0 \). \\

    \item \textbf{Function Point Alignment}: The function points should be aligned to ensure that the interaction occurs at the desired location of the test tool. The function point alignment is defined by the following constraint:
    \begin{align*}
        \| \mathbf{T}_{\text{point}} \begin{bmatrix} q_{\text{func}}^0 \\ 1 \end{bmatrix} - \begin{bmatrix} p_{\text{func}}^{t_f} \\ 1 \end{bmatrix} \| = 0
    \end{align*}  

    \item \textbf{Function Plane Alignment}: The normal vectors should be aligned to ensure that function planes have the same orientation. The function plane alignment is defined by the following constraint:
    \begin{align*}                
    \mathbf{n}_H^{t_f} \cdot (\text{rot}(\mathbf{T}_{\text{plane}})\mathbf{n}_R^0) = 1
    \end{align*}
    where $\text{rot}(\mathbf{T})$ denotes the rotation component of a rigid transformation $\mathbf{T}$. \\
    
    \item \textbf{Function Axis Alignment}: The function axes, which are function-relevant operational vectors, should be aligned to ensure that the test tool is properly tilted relative to the target (e.g., pitch angle for pouring). The function axis alignment is defined by the following constraint:
    \begin{align*}    
    \mathbf{u}_H^{t_f} \cdot (\text{rot}(\mathbf{T}_{\text{axis}}) \mathbf{u}_R^0) = 1
    \end{align*}
    However, due to structural differences between the demonstration and test tools  (i.e., differences in relative locations of functional keypoints), directly applying $\mathbf{T}_{\text{axis}}$ to the test tool may not result in successful task execution. For instance, a mug and a teapot may require different pouring angles. To address this issue, $\mathbf{T}_{\text{axis}}$ is further refined using the VLM. Specifically, a pre-defined set of angle offsets, ranging from $[-45^{\circ}, -45^{\circ}]$, is applied to $\mathbf{T}_{\text{axis}}$. For each offset, the combined point cloud of the test tool and target is back-projected onto the camera plane. The VLM is then prompted to identify the rendered image that represents the optimal state conducive to the task success, given the demonstration function keyframe $I_f$ as a reference. The transformation corresponding to the optimal state is recorded as  $\mathbf{T}^{*}_{\text{axis}}$. Qualitative results of function axis alignment are illustrated in Figure \ref{fig:func_axis_align}.
\end{enumerate}
Finally, the geometric constraints from each step are combined to compute  $\mathbf{T}_{\text{func}}$ and $K_R^{t_f}$ :
\begin{align*}
\mathbf{T}_{\text{func}} = \mathbf{T}_{\text{point}} \cdot \mathbf{T}_{\text{plane}} \cdot \mathbf{T}^{*}_{\text{axis}}, \
K_R^{t_f} = \mathbf{T}_{\text{func}} K_R^0
\end{align*}
To ensure that the function point interacts with the effect point at $t_f$, $q_{\text{func}}^{t_f}$ is further adjusted to align with  $q_{\text{eff}}$. Meanwhile, the same adjustment is applied to  $q_{\text{grasp}}^{t_f}$ and  $q_{\text{center}}^{t_f}$. The resulting $K_R^{t_f}$ represents the predicted test tool state at $t_f$.

 % $K_R^{t_f}$

 \begin{figure}[t]
  \centering
  \hspace*{-0.3in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=1.2in,clip=true,trim=0in 0in 0in 0in]{imgs/func_axis_align-1.png}};
  \end{tikzpicture}
    \vspace*{-0.3in}
  \caption{An illustration of function axis alignment process: (1) test function plane \( \Pi_R^0 \), (2) demonstration function plane  \( \Pi_H^{t_f} \), (3) initially aligned test function plane  \( \Pi_R^{t_f} \), and (4) VLM refined test function plane  \( \Pi_R^{t_f} \).}
  \label{fig:func_axis_align}
  \vspace*{-0.25in}
\end{figure}


\subsection{Functional Keypoint-Based Action Planning}\label{planning}
\noindent \textbf{Tool Pose Representation.} In the final stage, FUNCTO computes the test tool pose at each timestep and generates a robot end-effector trajectory $\tau_R$ for task execution. Specifically, the demonstration tool pose at timestep $t$ can be represented, using $K_H^t$, as:
\begin{align*}
     \mathbf{T}_H^t =
\begin{bmatrix}
\mathbf{R}_H^t & p_{\text{func}}^t \\
\mathbf{0} & 1
\end{bmatrix}\end{align*}
where $\mathbf{R}_H^t$ is the rotation matrix derived from the function axis $\mathbf{u}_H^t$ and the normal vector $ \mathbf{n}_H^t$. With such a pose representation, demonstration functional keypoint trajectory $\{K_H^t\}_{t=0}^{N-1}$ can be transformed to a sequence of $\text{SE(3)}$ poses $\{\mathbf{T}_H^t\}_{t=0}^{N-1}$. Similarly, the test tool pose at $t$ is represented as:
\begin{align*}
\mathbf{T}_R^t =
\begin{bmatrix}
\mathbf{R}_R^t & q_{\text{func}}^t \\
\mathbf{0} & 1
\end{bmatrix}
\end{align*}
where $\mathbf{R}_R^t$ is similarly defined. An example of the tool pose representation is illustrated in Figure \ref{fig:pipeline} (stage 3). The function keyframe state $K_R^{t_f}$ and the initial keyframe state $K_R^{0}$ are transformed to the pose representations $\mathbf{T}_{\text{func}}$ and $\mathbf{T}_{\text{init}}$, respectively. \\

% \begin{algorithm}[t]
% \caption{Functional Keypoint Transfer}
% \label{alg:3d_keypoint_transfer}
% \textbf{Input:} \\
% \hspace{1em} Demo functional keypoints \( K_H^0 = [p_{\text{func}}^0, p_{\text{grasp}}^0, p_{\text{center}}^0] \), Initial keyframe \( I_0 \), Robot observation \( o_R \), Test tool mask \( M \), \\
% \hspace{1em} Vision-language model (VLM), Dense semantic correspondence model \( \Phi \), 3D-2D projection \( P_{\text{3D-2D}} \), 2D-3D projection \( P_{\text{2D-3D}} \), 3D center computation \( F_{\text{center}} \) \\
% \textbf{Output:} Test functional keypoints \( K_R^0 = [q_{\text{func}}^0, q_{\text{grasp}}^0, q_{\text{center}}^0] \)

% \begin{algorithmic}[1]
%     \State \( K_R \gets \emptyset \)

%     \State \textbf{1. Coarse-Grained Region Proposal:}
%     \For{each \( k \in \{\text{func}, \text{grasp}\} \)}
%         \State \( p_k^{2D} \gets P_{\text{3D-2D}}(p_k^0, I_0) \)
%         \State \( r_k \gets \text{VLM}(p_k^{2D}, I_0, o_R, M) \) \Comment{Region proposal}
%     \EndFor

%     \State \textbf{2. Fine-Grained Point Transfer:}
%     \For{each \( k \in \{\text{func}, \text{grasp}\} \)}
%         \State \( q_k^{2D} \gets \Phi(p_k^{2D}, r_k, I_0, o_R) \) \Comment{Point transfer}
%         \State \( q_k^0 \gets P_{\text{2D-3D}}(q_k^{2D}, o_R) \)
%     \EndFor

%     \State \textbf{3. 3D Center Computation:}
%     \State \( q_{\text{center}}^0 \gets F_{\text{center}}(M, o_R) \)

%     \State \textbf{4. Functional Keypoint Transfer Output:}
%     \State \( K_R^0 \gets [q_{\text{func}}^0, q_{\text{grasp}}^0, q_{\text{center}}^0] \)
% \end{algorithmic}
% \end{algorithm}


\noindent \textbf{Tool Trajectory Optimization.} Given the function keyframe pose $\mathbf{T}_{\text{func}}$, the initial keyframe pose $\mathbf{T}_{\text{init}}$, and the reference pose trajectory $\{\mathbf{T}_H^t\}_{t=0}^{N-1}$, the optimization problem for solving the test tool trajectory $\{\mathbf{T}_R^t\}_{t=0}^{N-1}$ can be formulated as:
\begin{align*}
\min_{\{\mathbf{T}_R^t \}_{t=0}^{N-1}} & \sum_{t=0}^{N-1} \left(
\| q_{\text{func}}^t - p_{\text{func}}^t \|_2^2  + \| \text{Log}(\mathbf{R}_R^t (\mathbf{R}_H^t)^\top)\|^2 \right) \\
\text{s.t.}  \quad & \mathbf{T}_R^0  = \mathbf{T}_{\text{init}} \\
 \quad & \mathbf{T}_R^{t_f}  = \mathbf{T}_{\text{func}}
\end{align*}
where $\text{Log}:\text{SO(3)} \rightarrow \mathbb{R}^3$\cite{sola2018micro}. This formulation can flexibly incorporate additional costs and constraints, such as smoothness costs and collision avoidance constraints. Implementation details can be found in Appendix E. \\


\noindent \textbf{Tool Trajectory Execution.} 
%Finally, we use GraspGPT \cite{tang2023graspgpt} to sample a 6-DoF grasp pose around $q_{\text{grasp}}^0$ on the test tool. \ax{change the grasping to contactgraspnet}
The test tool trajectory in test target frame $\{\mathbf{T}_R^t\}_{t=0}^{N-1}$ is first transformed into the robot base frame $\{\mathbf{T}_{R_\text{base}}^t\}_{t=0}^{N-1}$. Then, we use GraspGPT \cite{tang2023graspgpt} to sample a 6-DoF task-oriented grasp pose around $q_{\text{grasp}}^0$ on the test tool. Assuming the gripper is rigidly attached to the test tool after grasping, the robot end-effector trajectory $\tau_R$ can be computed with the sampled grasp pose and $\{ \mathbf{T}_{R_\text{base}}^t\}_{t=0}^{N-1}$.
% Assuming the gripper is rigidly attached to the test tool after grasping, $\{\mathbf{T}_R^t\}_{t=0}^{N-1}$ is first transformed from the test target frame into the robot frame and subsequently converted to a robot end-effector trajectory $\tau_R$. 
This trajectory is tracked and executed using operational space control.

% $q_{\text{grasp}}^k$ 

% We implement a PD controller with differentiable inverse kinematics (IK) to move the gripper to the sampled grasp pose and execute the predicted trajectory.


% Contact-GraspNet \cite{sundermeyer2021contact} 
























% construct function frame



% trajectory optimization





