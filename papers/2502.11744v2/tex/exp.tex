
\begin{figure*}[t]
  % \centering
  \vspace*{-0.2in}
  \hspace*{-0.3in} % Shift left by 0.5 inches
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (-0in, 0in)
      {\includegraphics[height=2.3in,clip=true,trim=0in 0in 0in 0in]{imgs/osil_exp_main-4.png}};
  \end{tikzpicture}
    \vspace*{-0.3in}
  \caption{Quantitative comparison to one-shot imitation learning baselines. The first tool of each function (highlighted) is used for demonstration.}
  \label{fig:osil_exp}
  % \vspace*{-0.2in}
\end{figure*} 

% \begin{table*}[th] \small
% \centering
% \caption{Quantitative comparison to Behavioral Cloning baselines}
%   \vspace*{-0.1in}
% \renewcommand\arraystretch{1.6}
% \setlength\tabcolsep{4pt}%调列距
% \begin{tabular}{ccclcclcclcclcclcc}
% \toprule
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Pour}} &  & \multicolumn{2}{c}{\textbf{Cut}} &  & \multicolumn{2}{c}{\textbf{Scoop}} &  & \multicolumn{2}{c}{\textbf{Brush}} &  & \multicolumn{2}{c}{\textbf{Pound}} &  & \multicolumn{2}{c}{Success Rate}                    \\ \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12} \cline{14-15} \cline{17-18} 
%                                  & Seen           & Unseen           &  & Seen           & Unseen          &  & Seen            & Unseen           &  & Seen            & Unseen           &  & Seen            & Unseen           &  & \multicolumn{1}{l}{Seen} & \multicolumn{1}{l}{Unseen} \\ \midrule
% ACT (50 demos)                   & 6/10             & 7/40                 &  & 7/10             & 16/40                &  & \textbf{7/10}                & 19/40                 &  & 5/10                & 13/40                 &  & 5/10                & 10/40                &  & 60.00\%                        & 32.50\%                           \\
% DP (50 demos)                    & 5/10               & 8/40                 &  & 5/10              & 9/40                &  & 4/10                & 11/40                 &  & 5/10               & 14/40                 &  & 4/10               & 11/40                &  & 57.50\%                        & 26.50\%                            \\
% DP3 (50 demos)                   & 4/10              & 8/40                 &  & 4/10               & 10/40                &  & 2/10                & 6/40                 &  & 2/10               & 5/40                &  & 4/10               & 6/40                 &  & 32.00\%                          & 17.50\%                            \\ 
% FUNCTO (1 demo)                  & \textbf{9/10}               & \textbf{32/40}                 &  & \textbf{9/10}               & \textbf{33/40}                &  & \textbf{7/10}                & \textbf{31/40}                 &  & \textbf{8/10}                & \textbf{30/40}                 &  & \textbf{9/10}                & \textbf{33/40}                 &  & \textbf{84.00\%}                          & \textbf{79.50\%}                            \\ \bottomrule
% \end{tabular}
%   \vspace*{-0.2in}
% \label{tab:bc_exp}
% \end{table*}

\begin{table*}[th] \small
\centering
\caption{Quantitative comparison to Behavioral Cloning baselines}
  \vspace*{-0.1in}
\renewcommand\arraystretch{1.8}
\setlength\tabcolsep{3pt}%调列距
\begin{tabular}{ccclcclcclcclcclcc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Pour}} &  & \multicolumn{2}{c}{\textbf{Cut}} &  & \multicolumn{2}{c}{\textbf{Scoop}} &  & \multicolumn{2}{c}{\textbf{Brush}} &  & \multicolumn{2}{c}{\textbf{Pound}} &  & \multicolumn{2}{c}{Overall}                    \\ \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12} \cline{14-15} \cline{17-18} 
                                 & Seen           & Unseen           &  & Seen           & Unseen          &  & Seen            & Unseen           &  & Seen            & Unseen           &  & Seen            & Unseen           &  & \multicolumn{1}{l}{Seen} & \multicolumn{1}{l}{Unseen} \\ \midrule
ACT (50 Demos)                   & 60.0\%             & 17.5\%                 &  & 70.0\%             & 40.0\%                &  & \textbf{70.0\%}                & 47.5\%                 &  & 50.0\%                & 32.5\%                 &  & 50.0\%                & 25.0\%                &  & 60.0\%                        & 32.5\%                           \\
DP (50 Demos)                    & 50.0\%              & 20.0\%                &  & 50.0\%             & 22.5\%                &  & 40.0\%                & 27.5\%                &  & 50.0\%               & 35.0\%                 &  & 40.0\%               & 27.5\%                &  & 57.50\%                        & 26.50\%                            \\
DP3 (50 Demos)                   & 40.0\%              & 20.0\%                 &  & 40.0\%               & 25.0\%               &  & 20.0\%                & 15.0\%                 &  & 20.0\%               & 12.5\%                &  & 40.0\%              & 15.0\%                 &  & 32.00\%                          & 17.50\%                            \\ 
FUNCTO (1 Demo)                  & \textbf{90.0\%}               & \textbf{80.0\%}                 &  & \textbf{90.0\%}               & \textbf{82.5\%}                &  & \textbf{70.0\%}                & \textbf{77.5\%}                 &  & \textbf{80.0\%}                & \textbf{75.0\%}                 &  & \textbf{90.0\%}                & \textbf{82.5\%}                 &  & \textbf{84.00\%}                          & \textbf{79.50\%}                            \\ \bottomrule
\end{tabular}
  \vspace*{-0.2in}
\label{tab:bc_exp}
\end{table*}



\begin{figure*}[t]
  \centering
  \vspace*{-0.2in}
  \hspace*{-0.1in} % Shift left by 0.5 inches
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (-0in, 0in)
      {\includegraphics[height=5.6in,clip=true,trim=0in 0in 0in 0in]{imgs/qualitative-1.png}};
  \end{tikzpicture}
    \vspace*{-0.2in}
  \caption{Qualitative results of predicted functional keypoints, test tool trajectories, and real-robot executions across five functions.}
  \label{fig:qualitative}
  \vspace*{-0.2in}
\end{figure*} 





\section{Experiments}
In this section, we conduct real-robot experiments to validate FUNCTO's effectiveness and analyze key design choices. Specifically, we answer the following questions: (1) How well does FUNCTO generalize from a single human demonstration to novel tools? (2) How does FUNCTO perform compared to existing OSIL methods under the same setup? (3) How does FUNCTO compare to state-of-the-art BC methods? (4) How does each design choice in function-centric correspondence establishment (Section \ref{correspondence}) affect the overall performance?

\subsection{Experimental Setup}

\noindent \textbf{Baselines.} We compare FUNCTO against the following OSIL baselines: (1) \textbf{\textsc{DinoBot}} \cite{vitiello2023one, di2024dinobot}, which leverages the visual correspondence capability of the vision foundation model DINO to perform semantic feature extraction and correspondence. (2) \textbf{\textsc{Ditto}} \cite{heppert2024ditto}, which employs a pre-trained visual correspondence model LOFTR to estimate the relative pose transformations between demonstration and test tools. (3) \textbf{\textsc{Orion}} \cite{zhu2024vision, li2024okami}, which extracts geometric features with Fast-Point Feature Histograms and performs a global-local registration. We adopt the original correspondence implementations of these baselines. The low-level execution components remain consistent with FUNCTO. OSIL methods with different setups, such as those requiring in-domain pre-training or object/task-specific prior knowledge, are excluded for a fair comparison.

In addition to OSIL baselines, we also compare FUNCTO with BC baselines, which represent more typical imitation learning approaches in recent works. Specifically, the following BC baselines are compared: (1) \textbf{\textsc{Action Chunking Transformer}} (ACT) \cite{zhao2023learning}, a transformer-based BC method with action chunking and temporal ensemble. (2) \textbf{\textsc{Diffusion Policy}} (DP) \cite{chi2023diffusion}, a diffusion-based BC method that models a visuomotor policy as a conditional denoising diffusion process. For both ACT and DP, we use the pre-trained DINO-ViT as the backbone for visual feature extraction. (3) \textbf{\textsc{3D Diffusion Policy}} (DP3) \cite{ze20243d}, a more recent diffusion-based BC method that operates on 3D visual representations extracted from sparse point clouds with a lightweight point encoder. \\

\noindent \textbf{Task Description.} We evaluate FUNCTO and baselines on five tool manipulation functions: \texttt{pour}, \texttt{cut}, \texttt{scoop}, \texttt{brush}, and \texttt{pound}. A tool manipulation task is defined by pairing a function with a tool and a target  (e.g., \texttt{mug-pour-bowl}). In this work, we primarily focus on addressing intra-function variations between tools, placing less emphasis on target variations. For each function, we design five tasks using different tools, divided into three levels of generalization: (1) spatial generalization (seen), where the demonstration tool is randomly positioned in the workspace; (2) instance generalization (unseen), where the demonstration and test tools are different instances from the same category; (3) category generalization (unseen), where demonstration and test tools are from different categories with the same function. In the context of this paper, ``same function" refers to imitating the tool manipulation behavior demonstrated by the human to accomplish a functionally equivalent task. 

\noindent \textbf{Experimental Protocol.} During the demonstration phase, a single-view, actionless video of a human performing a tool manipulation task is recorded with a stationary RGB-D camera, accompanied by a task description, for each function. During the testing phase, an RGB-D image of the workspace is captured using a similar camera setup and sent to the robot for action planning and execution. For training the BC baselines, we collect 50 human demonstrations with teleportation for each function. For testing the BC baselines, we use the same targets as those in the demonstrations to emphasize the impact of variations in the test tools. In terms of performance evaluation, each method is tested with 10 trials per task, resulting in a total of 50 trials per function and 250 trials across all five functions. The detailed task success conditions are described in Appendix B. The average success rate is used as the evaluation metric.


\subsection{Experimental Results}
\noindent \textbf{Quantitative Comparison to OSIL Baselines.} The detailed quantitative evaluation results are reported in Figure \ref{fig:osil_exp}. Each function is evaluated with five tasks. The first task tests spatial generalization, the next two tasks evaluate instance generalization, and the final two tasks assess category generalization. 

All methods perform well in spatial generalization with the seen demonstration tool, achieving success rates above 70\%. However, all baselines exhibit significant performance drops (from 20\% to 40\%) when generalizing to novel tool instances and categories, especially for those with substantial differences in shape, size, or topology. For instance, in \texttt{teapot-pour-pot}, the teapot and the demonstrated mug differ in both shape and part topology. The teapot has a conical body, a long neck, and a handle positioned on top, whereas the mug features a cylindrical body with a side-mounted handle. We also observe that variations in size and scale negatively affect the performance of the baselines. In \texttt{hammer-pound-nail}, the red hammer is much smaller than the demonstrated mallet, causing inaccurate pounding point alignment in 3D. This results in infeasible contact between the tool and the target, highlighting the significance of the proposed function-centric correspondence. 

Among all baselines, ORION relies solely on geometric features, rendering it ineffective at handling large geometric variations. DINOBot outperforms both DITTO and ORION, achieving an average success rate of 57.5\% when generalizing to novel tools. This performance can be attributed to DINO's strong visual correspondence capability. However, DINOBot still struggles to establish correspondences between visually distinct tools due to intra-function variations. In contrast, FUNCTO significantly outperforms the OSIL baselines, achieving a high success rate of 79.5\% across five functions for novel tool generalization. Figure \ref{fig:qualitative} visualizes the qualitative results of real-robot executions across five functions. \\

% \noindent \textbf{Qualitative Results.} \textcolor{red}{need a big figure here} 

\noindent \textbf{Quantitative Comparison to BC Baselines.} The quantitative evaluation results are summarized in Table \ref{tab:bc_exp}. We divide the results into Seen and Unseen categories to emphasize the performance gap between demonstration and test tools. For seen tools, BC baselines trained on 50 demonstrations exhibit some level of generalization to different spatial layouts, with the two leading baselines, ACT and DP, achieving success rates ranging from 50\% to 60\%.  By modeling the relative spatial relationship between the tool and the target, FUNCTO inherently supports spatial generalization.  

However, all BC baselines struggle with unseen tool generalization, primarily due to intra-function variations, with success rates approximately half those of the Seen category. In contrast, FUNCTO achieves a significantly higher success rate of 79.5\%, attributed to the proposed function-centric correspondence. We also evaluate BC baselines trained with 10 and 1 demonstration(s). However, they fail to produce meaningful results and are therefore excluded from the report. \\


\noindent \textbf{Failure Analysis.} The modular design of FUNCTO facilitates the interpretation and in-depth analysis of failure cases. The result of the failure analysis is reported in Figure \ref{fig:error}. The identified failure sources are categorized into: (1) function-centric correspondence, (2) functional keypoint transfer, (3) trajectory planning, (4) grasping, and (5) others (e.g., segmentation, detection). 

The primary failures arise from (4) and (3). Specifically, failures in grasping often occur when the tool flips or slips due to unstable contact between the tool and the gripper, preventing the robot from completing the task. Incorporating an online state tracking module (probably using multiple calibrated cameras) and closed-loop execution could potentially mitigate this issue. For trajectory planning, failures primarily result from unexpected collisions between the tool and the target, particularly in contact-rich tasks (e.g., \texttt{scrubber-brush-plate}). Providing visual-tactile feedback is essential for successfully accomplishing such tasks. Functional keypoint transfer errors are mainly caused by incorrect candidate region proposals for function points but contribute less significantly to overall failures. These errors may be mitigated as VLMs continue to improve. Correspondence errors are mainly attributed to inaccurate depth information of the functional keypoints. Empirically, the function-centric correspondence works well with accurate 3D functional keypoint locations.


\begin{figure}[t]
  \centering
  % \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=1.5in,clip=true,trim=0in 0in 1in 0in]{imgs/error-1.png}};
  \end{tikzpicture}
    \vspace*{-0.1in}
  \caption{Failure analysis of system components}
  \label{fig:error}
  \vspace*{-0.3in}
\end{figure}


% add instruction

% qualitative restuls (demo tool w/ keypoint -> test tool w/ keypoint -> test trajectory -> 3/4 keyframes execution)

\subsection{Ablation study}
To gain further insights into the design choices behind the core component of FUNCTO, function-centric correspondence establishment, we conduct two sets of ablation studies: one on functional keypoint transfer and another on function-centric correspondence. Performance is evaluated on five tool manipulation tasks: \texttt{teapot}-\texttt{pour}-\texttt{pot}, \texttt{knife}-\texttt{cut}-\texttt{burger}, \texttt{hammer}-\texttt{pound}-\texttt{nail}, \texttt{scoop}-\texttt{scoop}-\texttt{bowl}, \texttt{scrubber}-\texttt{brush}-\texttt{plate}. \\


\begin{figure*}[t]
  % \centering
  \hspace*{0in}
  \vspace*{-0.1in}
  \begin{tikzpicture}[inner sep = 0pt, outer sep = 0pt]
    \node[anchor=south west] (fnC) at (0in,0in)
      {\includegraphics[height=2.3in,clip=true,trim=0in 0in 0in 0in]{imgs/ablation-1.png}};
  \end{tikzpicture}
    % \vspace*{-0.1in}
  \caption{Ablation studies on functional keypoint transfer (left) and function-centric correspondence (right).}
  \label{fig:ablation}
  \vspace*{-0.2in}
\end{figure*}


\noindent \textbf{Ablation on Functional Keypoint Transfer.} We evaluate four functional keypoint transfer strategies: (1) Demo+VLM+DSC (proposed), which utilizes demonstration functional keypoints as references to prompt the VLM for region proposal, followed by point transfer through a dense semantic correspondence model; (2) Demo+VLM, which removes the dense semantic correspondence model from the proposed implementation; (3) Demo+DSC, which relies solely on a dense semantic correspondence model for functional keypoint transfer; (4) VLM (zero-shot), which directly prompts the VLM to propose functional keypoints in a zero-shot manner, as in MOKA \cite{liu2024moka} and ReKep \cite{huangrekep}. The quantitative results are reported in Figure \ref{fig:ablation} (left). The proposed Demo+VLM+DSC consistently outperforms ablated versions. Demo+VLM performs reasonably well, benefiting from the rich commonsense knowledge embedded in VLMs. However, as indicated in \cite{rahmanzadehgervi2024vision}, VLMs struggle to provide precise point-level correspondences, particularly for tasks requiring high precision (e.g., \texttt{hammer}-\texttt{pound}-\texttt{nail}). On the other hand, solely relying on the dense semantic correspondence model often fails when dealing with large intra-function variations. The performance gap between Demo+VLM and VLM (zero-shot) justifies the point that demonstration functional keypoints provide valuable references for test functional keypoint proposal. Additional quantitative and qualitative evaluations are provided in Appendix C.\\

\noindent \textbf{Ablation on Function-Centric Correspondence.} In this ablation study, we investigate two questions: (1) Is aligning the function point more effective than aligning other functional keypoints? (2) Is VLM refinement necessary for function axis alignment? As shown in Figure \ref{fig:ablation} (right), function point alignment achieves the optimal performance in most cases by ensuring interactions occur at the desired location of the tool, regardless of variations in shape, topology, or size. When comparing strategies with and without VLM refinement, the latter rigidly aligns the function axes, ignoring changes in the relative locations of the three functional keypoints. This strategy may produce infeasible function keyframe poses. Incorporating commonsense knowledge from VLMs for function axis refinement statistically improves the performance. 

% keypoint transfer

% center point alignment (center-func), grasp point alignment (grasp-func)

% w/o. function axis refinement




