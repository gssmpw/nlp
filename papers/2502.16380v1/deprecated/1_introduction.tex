\section{Introduction}

Machine Learning is increasingly used in high-stakes settings to decide who receives a loan \citep{hurley2016credit}, a job interview~\citep{bogen2018help}, or even organ transplants ~\citep{murgia2023algorithms}. Models are designed to predict outcomes using features about individuals, but fail to take into account constraints on how individuals can change them~\citep[][]{liu2024actionability}. Consequently, models may assign an individual a \emph{fixed prediction} which is not \emph{responsive} to the individual's actions.

The responsiveness of a model is integral to its safety in settings where predictions map to people. In domains such as healthcare, recent results have highlighted how young patients may be inadvertently precluded from receiving a liver transplant as a result fixed predictions~\cite{murgia2023algorithms}. In settings like content moderation, a fixed prediction can ensure that malicious actors are unable to evade detection by manipulating their features. %Across these settings, it is vital to understand what individuals do not have \emph{recourse}, the ability to change the decision of the model through their actions. %Before deploying a model, it is critical that stakeholders understand and characterize what individuals affected by their model are precluded from receiving a desirable outcome. 

In this paper, we study the problem of understanding whether any individuals are assigned a fixed prediction in an \emph{entire region} of the feature space (e.g., all plausible job applicants). 
While prior work has focused on verifying the existence of recourse for individuals, these individualized approaches do not provide any guarantees on the responsiveness of the model for out-of-sample data. In settings like lending and hiring, this means that critical issues can only be identified \emph{after} a model has been deployed. Moreover, even when individuals without recourse can be identified during model development, determining the root cause of fixed predictions can be challenging. Recent work has highlighted that individualized recourse approaches alone are insufficient to facilitate broader model understanding and debugging~\citep{rawal2020algorithmic}. Auditing the responsiveness of a model with individualized approaches requires access to a representative dataset of individuals affected by the model. This requirement is demanding for external audits where gaining access to a model is easy but getting a representative sample of the population is difficult. For instance, many interpretable medical and criminal justice scoring systems are available publicly~\citep[see e.g.,][]{morrison2022optimized, yamga2023optimized, ribeiro2023use, PennSentence}, but gaining access to a representative dataset is difficult due to privacy concerns. %Here, external auditors are either have to rely on small datasets to audit responsiveness or try to create representative synthetic datasets. 

In response to these challenges, we introduce a new tool to verify recourse over a region of the feature space. We do so for linear classification models, a broad function class encompassing popular methods such as logistic regression, linearizable rule-based models (e.g., rule sets, decision lists), and concept-bottleneck models \cite{koh2020concept, sun2024concept}. Previous work has highlighted that recourse verification for a \emph{single individual} is a non-trivial combinatorial problem that requires performing exhaustive search over a subset of the feature space that captures both the model as well as actionability constraints \cite{ustun2019actionable, kothari2023prediction}. Our approach generalizes this problem to an even more intensive setting that requires searching over an entire region of individuals. Despite these challenges, our approach is able to verify recourse over regions within seconds on real-world datasets. Moreover, our tool is designed to find \emph{static regions} within the feature space (i.e., regions where individuals are assigned a fixed prediction), which can help model developers determine the root causes of fixed predictions. Our tool can be run in settings without a public dataset, enabling practitioners to audit model responsiveness with only access to the model itself and a description of the region on which it is being deployed. 

%This work studies a new paradigm in algorithmic recourse that aims to characterize fixed predictions.% by verifying recourse over an \emph{entire region} of the feature space (e.g., all plausible job applicants). 
%We introduce a new tool that formally verifies recourse over an \emph{entire region} of the feature space (e.g., all plausible job applicants). Our tool can be used to discover interpretable \emph{static regions} (i.e., where all individuals are assigned a fixed prediction) within this space, or provide a formal certification of model responsiveness over the entire region. Our approach is robust to distribution shifts and runs in seconds on real-world datasets.  Moreover, our tool can be used without the need for available datasets, enabling practitioners to audit model responsiveness in settings with only model access and a description of the region on which it is being deployed. For instance, many interpretable medical and criminal justice scoring systems are available publicly~\citep[see e.g.,][]{morrison2022optimized, yamga2023optimized, ribeiro2023use, PennSentence}, but gaining access to a representative dataset is difficult due to privacy concerns. 

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/overview_figure.pdf}
    \vspace{-3em}
    \caption{Sample recourse verification task over a region. The feature space contains a continuous feature \textds{Years of Experience (YoE)}, and a binary feature \textds{Male} that encodes gender. We denote actions on each feature ($a_{\textds{YoE}}, a_{\textds{Male}}$) and specify constraints on feasible actions in the \emph{action set}. The task is to verify the responsiveness of a linear classifier over a \emph{region}, an area of the feature space defined by a set of constraints. Under this classifier there exists a \emph{static region}, an area where all applicants are assigned a fixed prediction. }
    \label{fig:summary}
    \vspace{-1em}
\end{figure*}

The main contributions of this work include:
\begin{enumerate}[leftmargin=*,itemsep=0]
    % Conceptual Contributions
    \item We introduce a new approach to formally verify recourse for linear classifiers over entire regions of the feature space. This tool can be used certify the responsiveness of classifiers beyond data present in a training dataset and can provide stronger guarantees for out-of-sample data. We also present tools to find (or enumerate all) static regions in the feature space, providing an intuitive tool for model developers to audit and correct problems with model responsiveness.

    % Method Constribution
    \item We develop fast methods that are able to find static regions within the feature space via Mixed-Integer Quadratically Constrained Programming (MIQCP). Our approach handles a broad class of actionability constraints, and can verify recourse within seconds on real-world datasets.
    
    % Empirical Contribution
    \item We evaluate our approach on applications in consumer finance, content moderation, and criminal justice. Our results show that existing approaches fail to verify model responsiveness over regions, emphasizing the need for tools that audit recourse beyond individual data points. We also showcase the ability of our approach to audit model responsiveness in settings with no public datasets via a case-study on the Pennsylvania criminal justice sentencing risk assessment instrument.


    % Software
    %\item We include a Python implementation of our approach that builds upon existing APIs for actionable recourse, allowing practitioners to easily specify complex actionability constraints and audit recourse.
    
\end{enumerate}


