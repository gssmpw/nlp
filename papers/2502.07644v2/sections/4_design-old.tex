\section{\Tool{} Design \songlh{XXX}}

\Tool{} utilizes an LLM to verify whether a contract implementation 
adheres to ERC requirements. 
Consequently, the design of \Tool{} primarily centers around 
creating appropriate LLM prompts, including how to specify ERC requirements 
in prompts, how to supply contract code to the LLM, 
and strategies to enhance the effectiveness and accuracy of violation detection.
This section commences with an overview of \Tool{}, 
followed by a detailed presentation of the three design aspects of prompt construction. 


\subsection{Overview}

%As shown by Figure~\ref{fig:overflow}, 
\Tool{} takes a startup phase to analyze ERCs and a subsequent working phase dedicated to inspecting individual contracts.
\commentty{An overview figure
could be helpful.}

We observe that an LLM is more effective when assessing the 
satisfaction of individual rules rather than concurrently checking multiple rules. 
\commentty{possibly doing an ablation study.}
Consequently, we adopt a rule sequentialization policy, 
wherein we iteratively guide the LLM through all rules in multiple prompts, 
checking only one rule in each prompt. 
\commentty{Need to specify
the prompts.}
To facilitate this process, during the startup phase, \Tool{} employs 
the LLM to extract rules from an ERC and express them in a specific YAML format to 
streamline prompt construction during the working phase. 
\commentty{Need more details
on how the prompts are streamlined.}
\Tool{} achieves this by first instructing the LLM to enumerate all public 
APIs of an ERC and subsequently requesting the LLM 
to list rules for each API. For example, \Tool{} identifies nine functions and 
two events for ERC20 and extracts five rules for 
the function \texttt{transferFrom()} in Figure~\ref{fig:20-high}.



Similarly, we notice \Tool{} exhibits better performance 
when scrutinizing smaller code segments separately
compared to assessing the entire contract implementation. 
Consequently, during the working phase, \Tool{} individually 
inspects each public function. For every public function, \Tool{} apply code 
slicing to compute 
its associated code and then employs individual prompts 
to inquire whether the public function, along with its related code, 
adheres to each rule extracted for the function during the startup phase. 
For instance, \Tool{} generates five prompts to 
ascertain whether function \texttt{transferFrom()} 
in Figure~\ref{fig:20-high} 
satisfies the five rules. 
Moreover, \Tool{} employs mechanisms like prompt specialization 
and one shot~\cite{duan2017one} to enhance its effectiveness. 
For example, after employing these mechanisms, an additional 4 prompts are generated for \texttt{transferFrom()} in Figure~\ref{fig:20-high}.



\subsection{ERC Rule Extraction}

\commentty{Need to talk about
prompt design.}

\Tool{} follows a three-step process to extract rules from an ERC and stores them in a YAML file.
\commentty{Why storing
in a YAM file.}

First, \Tool{} supplies the ERC to its LLM and 
instructs it to enumerate all functions in the ERC, presenting the information in a YAML array. 
Each element of the array possesses three properties: the function name, a list containing all 
the function’s parameter names and their corresponding types, and the return type.
\commentty{why this format.}
After that, \Tool{} instructs the ERC to extract all event declarations from the ERC using a 
similar approach.
\commentty{why extracting
declarations.}

Second, \Tool{} iterates through each extracted function 
to extract rules for them individually. 
To prevent scenarios where a function contains numerous rules, 
and the LLM overlooks some when using only one prompt, 
\Tool{} employs multiple prompts for each function to extract different types of rules. 
As \Tool{} utilizes these extracted rules to analyze contract implementations, it selects implementation categories in Table~\ref{tab:linguistic}. 
Specifically, \Tool{} employs four prompts for each function, corresponding to the four pattern groups.
In each prompt, \Tool{} provides the entire ERC document, the function’s declaration, and a concise explanation of the linguistic group. Additionally, \Tool{} presents all identified patterns for the group as one-shot examples in the prompt. 
Moreover, \Tool{} instructs the LLM to present the extracted rule in a YAML format tailored to each group.
For the CP group, \Tool{} instructs the LLM to extract the condition, the condition type, and the action. For the EP group, 
\Tool{} directs the LLM to extract the condition and the event name. For the RP group, \Tool{} specifies that the LLM should extract the method for generating the return. Lastly, for the AP group, \Tool{} instructs the LLM to extract the action.

Take the rule violated in Figure~\ref{fig:20-high} as an example, 
the extracted condition is `` the \_from account has deliberately authorized the sender of the message via some mechanism'', the condition type is ``unless'', and the action is ``throw''.  

Third, \Tool{} examines each extracted event and extracts 
rules regarding when it should or should not be emitted, 
using a prompt similar to those designed for extracting rules in a linguistic pattern within the EP group from a function.


Following this process, \Tool{} successfully extracts 212 rules out of the 222 rules, misses 10 
rules, and mistakenly extracts four extra rules. 
%One missed rule is required by ERC20 to inspect 
%whether the returned Boolean value is \texttt{false} and take proper actions if so. This rule is 
%not associated with a function or an event and thus is not extracted by \Tool{}. For other 
%missed rules, we suspect they are due to the long ERC document provided in the prompt. We leave 
%the removal of unrelated parts of an ERC document to improve the effectiveness of rule 
%extraction as a future work.
%The four mistakenly extracted rules are caused by the same reason.
%Taking the one from ERC20 as an example, \Tool{} extracts a rule that \texttt{transferFrom()} should throw if the message caller’s account balance does not have enough tokens to spend. 
%ERC20 does not have such a requirement for \texttt{transferFrom()}, while it has this requirement for \texttt{transfer()}. We suspect \Tool{} makes this mistake because the two functions’ names are too similar. 
%The remaining three mistakenly extracted rules are from ERC3525.
%They are all caused by similar reasons.
%
We manually correct the errors that occurred during rule extraction to set up \Tool{} for 
subsequent violation detection. We emphasize the irreplaceable value of automated rule 
extraction due to two main reasons: 1) The LLM efficiently processes and condenses ERC documents 
into a concise format, and manual efforts are inherently constrained. 2) Extracted rules are 
reusable across all smart contracts implementing the ERC, reducing manual efforts to a one-time 
occurrence.

\if 0

\shihao{

  Sometimes, the result of the rule extraction can be more specific than the description in the ERC. For example, when asking LLM to list all the conditions to emit Transfer in ERC20, except for the original description "emit when tokens are transferred", LLM can also list some specific examples like "emit when tokens are minted" and "emit when tokens are burn". These concrete descriptions can help reduce the false negative, but might also increase the cost of auditing a contract since more prompts will be generated.

  Given the example of ERC20, regarding the 31 implementation rules out of 35 rules in total, this methodology can successfully extract all of them, with only 2 acceptable extra rules: 1) transferFrom should throw if the message caller’s account balance does not have enough tokens to spend 2) approve should allow \_spender to withdraw from your account multiple times, up to the \_value amount. The 11 function and event interfaces, 9 return semantics, 6 event emission rules, and 8 throw conditions are all precisely extracted by LLM without any issue. Similar situation for the ERC721, ERC1155, and ERC3525. This methodology successfully covered almost all the 66 rules with only 2 rules needing manual fixing in ERC721. For larger ERC1155 and ERC3525, it covers 54 out of 59 rules and 57 out of 60 rules. The rules that LLM fails to extract are related to their unique linguistic patterns and purposes. For example, in ERC1155, "Missing  Balance changes and events MUST follow the ordering of the arrays (\_ids[0]/\_values[0] before \_ids[1]/\_values[1], etc)." in \texttt{safeBatchTransferFrom()} is failed to extract since it neither explicitly reflect any Solidity concepts like throwing or assignment nor the expected action after violation. LLM has trouble recognizing it as a rule.   
In ERC3525, one extra rule "caller is the current owner, an authorized operator, or the approved address for `\_tokenId`" is extracted for event ApprovalValue. The reason is due to the complexity of the full ERC(this condition is for a throw rule in the function \texttt{approve} and there is an event rule "emit ApprovalValue" right next to it) mislead the LLM. 

One possible way to overcome the issue caused by the long ERC is to slice the ERC and segment it by the contract interfaces. Some quick experiments show that this can achieve 100\% precision and recall.
  

  While LLMs are powerful, they are not infallible. Human oversight is necessary to ensure the accuracy and relevance of the extracted rules, especially in such smart contract auditing contexts where precision is critical. 

Although it may still require human oversight, the value of rule extraction is irreplaceable: 1) LLMs can process and summarize large volumes of documentation quickly into a specific format, which is much faster than manual summarization. 2)  Humans only spend very little time examining the concise formatted result instead of the tedious full ERC documentation. 3) LLM can use more intuitive or natural ways to conclude the rules or requirements in the ERC.

  The result of the rule extraction is reusable for all the other smart contracts implementing the same ERC. Since we only have limited ERCs compared to the numerous smart contracts, human review is necessary and critical for the fully automatic auditing process.

}

\fi

\subsection{Code Slicing}
A contract file might be excessively large, 
surpassing the input token limit of the LLM, 
making it impractical to input the entire file in a single prompt. 
Even if a contract file can fit within a prompt, supplying an extensive amount of code to 
the LLM could result in violations being buried in the input code, complicating the identification of the violations. Additionally, there is a risk that even for detected violations, the LLM may not report them due to the word limit of output. 
Consequently, 
\Tool{} divides each input contract file into smaller 
pieces and analyzes them individually.

We separate each contract 
based on its \texttt{public} functions as defined in the corresponding ERC 
for two reasons. First, after deployment, 
interactions with the contract occur through its \texttt{public} functions, and rule violations within 
the contract are exploited through those functions. 
Second, our study finds that, for the majority of rules, their scopes are confined to a \texttt{public} function (Insight 2), and even for those rules whose scopes are the 
entire contract, they can be verified 
by examining all \texttt{public} functions.

For each \texttt{public} function, \Tool{} calculates its associated code and 
mandates the LLM to analyze the related code together with the function. This approach is necessary because understanding the semantics of a function may be 
impractical solely by reading its code. We define a function's direct or 
indirect callees as its related code. Additionally, we consider contract 
fields accessed by the function or any of its callees as part of the related 
code. To distinguish functions defined in different contracts, we include the 
contract declaration of the \texttt{public} function or any of its callees. 
Lastly, given the LLM's proficiency in understanding natural languages, we 
incorporate comments associated with the function and its callees, both 
preceding and inside the function and its callees.

Taking the contract in Figure~\ref{fig:20-high} as an example, 
the entire contract contains 127 lines of code in total. 
We conduct a separate analysis of its nine \texttt{public} functions. 
When analyzing \texttt{transferFrom()}, we consider \texttt{\_transfer()} 
as its related code since it is called by \texttt{transferFrom()}. 
We also include line 2 as related code, 
as this field is accessed by \texttt{\_transfer()} in lines 14 and 15. 
Conversely, we do not consider line 3 as related code since the field is 
not accessed by \texttt{transferFrom()} and its callee. 
Furthermore, we incorporate the contract declarations in lines 1 and 18. 
Ultimately, the code analyzed by \Tool{} for \texttt{transferFrom()} 
contains 26 lines.

%\shihao{Specifically for interface-related auditing,  implementation code can be optimized out but only leave the interfaces as the context.}



\subsection{Optimization Mechanisms}

We have developed three mechanisms to enhance the LLM's understanding of ERC rules and rule violations. 
%by designing specialized 
%questions for each type of ERC rules, 
%breaking down compound rules, and providing 
%examples for rule violations.





\italicparagraph{Prompt Specialization}
Rather than issuing general inquiries to the LLM to determine if the input code violates a 
given rule, we craft specific questions 
tailored to the information stored in YAML format for each rule. 
This approach enhances the LLM's understanding of the rule, 
thereby improving effectiveness. For example, utilizing the information extracted for the violated rule in Figure~\ref{fig:20-high}, 
we instruct the LLM to examine whether each transferFrom() function 
throws unless "the \_from account has deliberately authorized the sender of the message via some mechanism."


% The quality of the LLM's response highly depends on the instructions in the prompt. The more specific the instruction is, the more precise the reply is. The worst instruction under the auditing context is simply feeding LLM with full ERC and full code. The logic behind this kind of prompt at a high level is "find out all the requirements in the ERC and check whether the code violates or not". Some requirements are explicitly mentioned in the ERC, such as the function should throw if from does have enough balance. Some requirements need to be deducted from the description. For example, ERC usually well says that "every ERC-xxx compliant contract must implement the xxx interface". Implementing the interfaces can infer many standards like "must have every required function", "their parameter types should match", "their return types should match", "must have every required event", etc. Simply asking "whether the code violates the rule" is ambiguous on the violation standard. LLM needs to figure out the standards by itself, which is quite unstable if the context is complex.
%Clarifying the standards of violation is the key to improving the precision of the LLM response. Instead of asking whether the code violates the rule, \Tool{} used a specialized way to generate instructions for each type of rule to help LLM clarify the violation standards. For example, instead of asking "whether the code violates the rule \'emit Transfer event\'", we 
% ask "whether the code emits the Transfer event". Instead of asking "whether the code violates the interface", we ask "whether the code contains the interface and parameter types and return types are matched". 

\italicparagraph{One shot.}
To enhance the LLM's understanding of certain 
rule descriptions and Solidity program semantics, 
we employ one-shot learning. Particularly we incorporate a single example 
in the prompts for 36 rules. These examples are specifically designed for the rules 
and remain the same across various contracts.

The rules with a one-shot example can be categorized 
into three scenarios. First, we employ an example to elucidate a specific Solidity 
language feature for 30 rules. For instance, post version 0.5, 
Solidity introduced keyword \texttt{emit} for event emission. Prior to 
version 0.5, events were emitted akin to function calls. Consequently, we 
demonstrate how to emit events before version 0.5 
for rules necessitating event emission. %when checking contracts
%implemented in a Solidity version prior version 0.5.
Second, we utilize examples to guide the LLM's understanding of
implementing specific semantics for five rules. 
For example, ERC20 mandates the emission of a \texttt{Transfer} event for 
``initial token distribution.'' We include an example of an ERC20 contractor 
constructor to aid the LLM's comprehension. Additionally, we devise examples 
for rules requiring a function to return \texttt{false} when it fails to 
complete 
its functionality, preventing the LLM from erroneously equating throwing an exception with returning \texttt{false}.
Third, 
the remaining rule contains obscure descriptions. 
ERC20 stipulates that \texttt{transferFrom()} should verify whether 
the message sender 
has been authorized ``via some mechanism.'' 
We construct an example to illustrate that this mechanism should utilize the 
\texttt{\_allowances} field, like line 10 of Figure~\ref{fig:20-high}.




\italicparagraph{Breaking down compound rules.}
There are 24 rules structured in a way that dictates an action should be taken 
when a condition is met. 
One such rule is mandated by ERC20, 
specifying that a caller must verify the return value is equal to 
\texttt{false}, when a function returns a Boolean value. The remaining rules 
dictate that when a certain condition is met, 
a specific event must be emitted.

When directly querying the LLM about whether a function 
violates one of those compound rules, 
the LLM often interprets the absence of the 
prerequisite as a rule violation, resulting in a false positive. To 
address this, we break down each compound rule into two prompts.
The former prompt prompts the LLM to assess the existence of the condition. 
If the LLM confirms its presence, then \Tool{} sends to latter prompt to 
instruct the LLM to examine 
whether the action exists. \Tool{} reports a rule violation when the LLM 
considers the action is absent. 




