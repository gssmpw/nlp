\subsection{Monetary and Time Costs}

\input{tables/tbl-cost}

%\bolditalicparagraphnospace{Methodology.}
\subsubsection{Methodology}
We evaluate the time and monetary costs using the ground-truth dataset. 
Since \Tool{}'s analysis of an ERC 
applies to all contracts of the ERC, its execution time is 
calculated as the sum of the analysis time for three ERCs and the 
code analysis time for all contracts. Measurements are performed 
three times, with the average reported.
For baseline techniques, each tool is executed on every contract 
file three times, and the average execution time is recorded. The 
time for ECSD is measured only for contracts collected from ECSDâ€™s GitHub. 
It is measured as the time between when an auditing request is 
submitted on GitHub and when a security expert provides the 
result, reflecting the waiting period required for a Solidity 
programmer to receive feedback. 


The monetary costs for \Tool{}, GPT-All, and GPT-O are the fees charged by OpenAI. 
GPT-A requires a \$20 monthly subscription, 
while analyzing the ground-truth dataset with ERCx costs \$100 for a team plan. 
For ECSD, we have cost data for one contract, amounting to \$1,000. 
To estimate costs for other ECSD contracts, we calculate the average hourly 
rate by dividing \$1,000 by the hours spent on the contract 
and then apply this rate to other contracts. For ERCx contracts, 
we cannot estimate auditing costs by ECSD experts due to the lack of information 
on how many hours ECSD experts would spend on the contracts. 
SCE and ZS are open-source, so they incur no monetary costs.





\subsubsection{Experimental Results}
As shown in Table~\ref{tab:cost}, \Tool{} analyzes the ground-truth dataset in 
under 18 minutes, costing just three dollars. The costs for each stage are as 
follows: 215 seconds and \$0.92 for rule extraction, 324 seconds and \$2.02 for rule translation, 
and 498 seconds and \$0 for symbolic execution.

\Tool{} interacts with ChatGPT, leading to longer processing time than 
SCE and ZS, which use only static analysis. 
However, \Tool{} is faster than ERCx, which likely runs its unit tests 
on a cloud. While \Tool{}'s costs are higher than GPT-All's, 
they are lower than GPT-O's, because \Tool{} requires more 
interactions with ChatGPT than GPT-All but fewer than GPT-O, 
and these interactions take longer than static analysis and are charged by OpenAI.
It's important to note that \Tool{} uses ChatGPT exclusively to analyze ERCs, 
so its monetary cost does not increase with the number of contract files. 
In contrast, the monetary costs for GPT-All and GPT-O rise linearly with the contract number. 
For example, to analyze the large dataset, \Tool{}'s monetary cost remains the same, but the estimated 
monetary costs for GPT-All and GPT-O would increase to \$387 and \$3,768, 
respectively.
Compared to the automated techniques,
the manual service provided by ECSD incurs significantly higher time and monetary costs.

\begin{tcolorbox}[size=title]
{\textbf{Answer to cost:} 
\Tool{} incurs minimal costs and offers superior cost efficiency compared to the manual service.}
\end{tcolorbox}

%Particularly, \emph{\Tool{} offers superior cost efficiency compared to the manual service provided by ECSD}, 
%with time reduced 
%by 103 times and monetary costs by 128,204 times. 
%\emph{Overall, \Tool{} offers superior cost efficiency compared to manual services.}

%SCE incurs the smallest cost among the five solutions.
%It analyzes the 30 contract files in the ground-truth %dataset in less than 0.1 
%seconds without any charges. 
%However, due to its limited detection capability, 
%SCE is inadequate in identifying ERC rule violations.

%\songlh{XXX: Estimate the cost of the large dataset}

