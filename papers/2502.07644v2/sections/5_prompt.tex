
\section{Prompt Design}




% To answer RQ1, use XXX metrics
% => setting, exp, result
% dataset (TODO)
% differnet prompt has big effect on the response
% exp5 and exp6 can be reported together


% exp10 => handle how many # of errors in the code
% => code complexity

We aim to build an auditing tool using ChatGPT, 
which can inspect whether a smart contract adheres to the requirements specified 
in the corresponding ERC description. 
Considering this specific task, we break down each prompt into three components:
1) ERC requirements in natural language, 2) Solidity code for ChatGPT to examine, 
and 3) textual instructions that explain the task. 
%As the chosen contracts do not contain many implementation errors that violate ERC requirements, we modify the contracts to introduce violations for the purpose of our experiments. 
%This section first explains the process of injecting ERC requirement violations. 
This section discusses how to construct the three components 
to ensure ChatGPT performs well in auditing. 




\subsection{ERC Requirement Settings}
\label{sec:requirement}

\input{tables/tbl-requirement}

As discussed in Section~\ref{sec:meth}, the training set of ChatGPT
includes ERC-20, ERC-721, and ERC777. 
Consequently, the first question we 
explore is whether it is necessary to include the requirements in these ERCs when constructing prompts. 
Since there is a strict limit of 4097 tokens for each prompt, by simplifying the requirement part, we can allocate more space for the code and instruction parts. 
Moreover, 
ChatGPT does not know ERC3525, and given its length exceeds the prompt limit, 
we need to devise a method to input ERC3525 for ChatGPT to audit ERC3525 contracts. 



%We design three experiments to examine whether 
%requirement descriptions in ERC20, ERC721, and ERC777 are necessary for
%ChatGPT to audit the corresponding contracts. 
%The instruction is "Given an ERC20 specification and a solidity code.
%The function transfer might have errors like missing emitting required events.
%Check whether the function transfer emits events required by the ERC20 specification correctly?". The instruction might not be the most effective one since we have not yet conducted experiments about instruction. It is used simply for comparing the ChatGPT capability of auditing with or without specification.

\italicparagraph{Known ERCs.}


We design \textit{Experiment-1} to explore whether we 
still need to provide requirements in ERC20, ERC721, and ERC777 
in prompts for ChatGPT to audit the corresponding contracts.

For \textit{setting-1}, we construct one prompt for each contract, 
with the complete ERC description as the requirement component and the entire contract as 
the code part. We require ChatGPT
to examine whether the contract is compliant with the ERC.
\textit{Setting-2} is the same as the first one, except that we omit the requirement part. 
These two settings simulate the scenario where a programmer 
relies on ChatGPT to examine an entire contract.

\input{snippets/inj-emit-err}

For the remaining three settings, our focus shifts to the token-transfer function 
in each contract. Those
functions are required to emit an event following a transfer action by the ERCs. {\color{red} As shown in the Listing \ref{lst:rm-emit},}  we remove 
the emit instruction for each function to inject an error in the function.
We use the token-transfer function (without anything outside the function scope)
as the code part. We require ChatGPT to inspect whether 
the function fails to emit any event required by the ERC to
simulate the situation where ChatGPT solely inspects a function body for auditing purposes. 
The three settings differ from each other in the requirement part. 
We use the complete ERC description as the requirement part for \textit{setting-3}, 
ignore the requirement part for \textit{setting-4}, 
and only use the descriptions of the token-transfer functions in the ERCs for \textit{setting-5}. 

{\color{red} For ERC20, the description of the token-transfer function is indeed the requirement of the function, a paragraph on top of the function signature. 
However, in ERC721 and ERC777, the requirements are grouped by functionality in the specification. The requirements of either emitting events or sending tokens are far from the description of the function. We treat the requirements of sending tokens and related event emissions as part of the description of the token transfer function and put them into the prompt for \textit{setting-5} after grouping them manually.

For \textit{setting-1} and \textit{setting-2}, the results cannot lead to any conclusion: 1) ChatGPT does not identify the two violations in the ERC20 contract. 2) The requirements of ERC721 and ERC777 are too long to fit into the prompt. This also affects them in \textit{setting-3} but is mitigated by \textit{setting-5}.
For \textit{setting-3}, \textit{setting-4} and \textit{setting-5}, ChatGPT successfully identified all three manually injected violations about missing required event emission with and without the requirements. 
The results show that ChatGPT can audit the contract without explicitly mentioning the requirements in the prompt if it "learns" the requirements before (a.k.a, in the model's training set). 
}



\subsection{Solidity Code Settings}

\input{tables/tbl-code}

We conduct \textit{Experiment-3} to investigate the proper way of inputting code
to ChatGPT. This is necessary because many contracts exceed ChatGPT’s token limit 
and cannot be directly used as the code part. Furthermore, including unnecessary information 
in the input may divert ChatGPT’s focus and reduce its detection accuracy. 

We create two variations for each contract, with one error injected in each variation.
When creating a prompt, we focus on one variation.  
As what we did in Section~\ref{sec:requirement}, we delete the event-emit 
instruction in the token transfer function to create the first variation.
For the second variation,  
we select the {\color{red}authorizing} function in each contract. 
As all the ERCs mandate the {\color{red}authorizing} function to 
throw an exception when a specific condition is not met, 
we remove the source code lines responsible for throwing the exception and 
checking the condition.  

We explore four different settings in this experiment with progressively 
less code used as the code part for the latter settings. 
In \textit{setting-1}, we use the complete contract as the code component. 
For the other three settings, our focus narrows down to the function with the injected error. 
In \textit{setting-2}, we input not only the function and also 
all program elements beyond the function
that are used by it, including the function’s callees, the declarations of the contracts 
containing the function and its callees, and all contract fields used by the function. 
In \textit{setting-3}, we use the function, its callees, and the declarations of the related contracts, 
since we observe that sometimes a function and an event
share the same name, which may confuse ChatGPT.
Lastly, in \textit{setting-4}, we only input the function with the injected error, without anything 
outside the function, the same as what we did in Section~\ref{sec:requirement}. 

We ignore the requirement part for ERC20, ERC721, and ERC777 contracts, and {\color{red} adding the function level requirements for the ERC3525}.

For instructions, {\color{red}we choose a working one but might be the best since we have not conducted the instruction experiments yet.

As shown in Table \ref{tab:exp-3}, \textit{setting-1}, putting full contract code in the prompt has the worst result.  For \textit{setting-4}, although it does have many true positives, it is impractical to use it in reality since most of the large Solidity implementations will separate the complex transferring token logic into multiple functions. ChatGPT cannot audit the function by merely function calls without their implementation detail. For the last two settings, the results of \textit{setting-3} are better than the \textit{setting-2} with much more true positives. This is due to \textit{setting-3} omitting the texts that are either  unrelated to the violations or misleading. For example, the event declaration can mislead the ChatGPT to answer "the given contract indeed emits the event". The false positives, after analyzing the responses, are related to the "check" rule. One function usually has only one or two event emission requirements but can easily have more than five or more "check" rules. A vague instruction in the prompt will lead to an imprecise answer from ChatGPT. We will explore this more in the next section.
}




\subsection{Textual Instruction Settings}
We design three experiments to examine how textual instruction affects ChatGPT to audit the corresponding contracts.

\input{tables/tbl-instruction-part1}

\input{snippets/exp-inst-st1}

In \textit{setting-1}, we construct the prompt with the instruction "whether the function <name> is compliant with the <ERC> specification", as shown in the Listing \ref{lst:exp-inst-st1}. In the ideal scenario, we want the ChatGPT to find out all inconsistencies by merely providing simple instructions in the prompt.

{
    \color{red} The results of \textit{setting-1} show that ChatGPT does not really understand what is the meaning of compliance. From a contract auditing perspective, the compliance infers following every rule specified in the ERC specification. Clearly, ChatGPT does not have the same definition of compliance as our understanding. We need to provide clearer instructions to guide ChatGPT to audit.
}

\input{snippets/exp-inst-st2}

In \textit{setting-2}, we construct the prompt with the clear auditing target, as shown in Listing \ref{lst:exp-inst-st2}, to check whether the function violates a specific type of rule in the specification. Unlike the \textit{setting-1}, which requests ChatGPT to audit the contract in one sentence, \textit{setting-2} breaks the whole auditing task into auditing six types of rules individually. 

{  \color{red}
    Compared to the \textit{setting-1}, \textit{setting-2} provides a clearer auditing target for ChatGPT.
    The results of \textit{setting-2} show ChatGPT can find three "event" rule violations on the ERC20 contracts. 
    It is better than the \textit{setting-1} but still has much room to improve. After reviewing the responses containing false positives and false negatives, we noticed that the response usually will try to follow the intent of the prompt. "Whether function emits" and "Whether function throws" implicitly hint at ChatGPT to generate a "yes", which is opposed to a contract auditing tool.
    \textit{setting-3} is designed to utilize this pattern.
}

\input{snippets/exp-inst-st3}

In \textit{setting-3}, we construct the prompt with the opposite inclination  in \textit{setting-2}, as shown in the Listing \ref{lst:exp-inst-st3} to check whether the function has a specific type of rule violation. 

{  \color{red}
The results of \textit{setting-3} confirm that we successfully utilized this pattern. 
ChatGPT finds many more true positives in the ERC20 and ERC777.
But the zero true positives on ERC721 still have no improvements compared to the previous settings.
Reviewing the results from the previous settings, the quality of response from ChatGPT improves as more precise instruction is given. Maybe ChatGPT cannot identify the violation because the instruction is not precise enough. For example, for auditing the "check" rule, instead of using the word "throw", maybe "check" is a more general word for ChatGPT to understand, since other programming languages might have different names for the "throw" mechanism. Same thing for the "event" rule, instead of using "emitting event", "event emission" might be a more understandable word for ChatGPT. \textit{setting-4} is designed to verify the effectiveness of the wording used to describe the violation.

}

\input{snippets/exp-inst-st4}

In \textit{setting-4}, we maintain the inclination used in \textit{setting-3} but instead use some different words closer to people's common sense or what ChatGPT used in its responses to describe the error, as shown in the Listing \ref{lst:exp-inst-st4}.

{  \color{red}
The results turn out that not only the inclination, the error-descriptive words can also profoundly impact the quality of the ChatGPT response. From Table \ref{tab:exp-4p1}, \textit{setting-4} has the most true positives and least false positives and false negatives. However, the all experiment settings in  Table \ref{tab:exp-4p1} only cover the situation that the error we ask ChatGPT to check is aligned with the actual error in the code. In real world scenario, the users do not have such information without auditing the contract.
}


\input{tables/tbl-instruction-part2}

\subsection{Real World Scenario}
{  \color{red}
The current SOTA instruction has a strong inclination to induce ChatGPT replies with confirmed violations. In a real-world scenario, we do not know what exactly the violations the code has and how many are they.  We want to see whether this instruction works on code without violation and with other types of violation not in the scope of instruction. We also want to know, instead of asking ChatGPT to audit six times for six types of violations, if it is possible to put all six instructions into one prompt to simplify the auditing process.

We will manually inject all six types of rule violations into token-transfer functions from ERC20, ERC721 and ERC777 that have the corresponding rules required in their ERCs. Each type of violation will be injected into three different contract functions and each of them is from different ERCs to cover the diversity. The token-transfer function in ERC20 does not have a "call" rule and "assign rule". The token-transfer function in ERC721 and ERC777 do not have a "return" rule since they do not have a return value. The token-transfer function in ERC721 does not have an "assign" rule.

As shown in Table \ref{tab:exp-4p2}, we can see that although asking ChatGPT in a negative way can find many true positives, it also brings many false positives when the error asked to ChatGPT is different from the actual error in code. A reasonable conclusion is ChatGPT will try its best to follow your inclination in the prompt and generate the result. If the prompt has a strong inclination, the response from ChatGPT will most likely follow the way. Another reason to cause many false positives is the vague question. When a function has many checks mentioned in the specification, asking ChatGPT like "missing any check" is not precise enough for ChatGPT to find out the correct missing one. It either will response 
they all exist or none of them exist. The reason for false negatives from "signature" and "assign" are similar, the description of the errors is too general. A valid signature infers the correct function name, correct parameter types, and correct return types. ChatGPT need to infer these rules from a simple sentence "whether the function has the incorrect signature", which will increase the false negatives.

\input{snippets/exp-inst-detail-ask-event}

Clearly, we need to avoid asking negatively and increase the precision of the error description in the instruction to reduce false positives and false negatives. As Listing \ref{lst:exp-inst-detail-ask-event} shown, the instruction includes the specific event name instead of "any" in previous settings. Notices that there is an extra "and all its
callees" after the function name, this is because ChatGPT sometime only checks the scope of the target function "transfer", not including its callees. Without including this note, ChatGPT will have many false positives. Including this note in the instruction clarifies our asking and reduces the potential false positives.


\input{tables/tbl-instruction-part3}

As shown in Table \ref{tab:exp-4p3}, after changing the instruction to explicit asking style, the number of false positives dramatically reduced. 

 }

\if 0

\subsection{Prompt Settings }
In order to answer the research questions mentioned earlier, two kinds of information are necessary: the instruction (what the GPT model should do), and context (specification and code).
The prompt sent to the GPT model can be roughly divided into the following three parts:
\begin{enumerate}
    \item Instruction
    \item Specification
    \item Code
\end{enumerate}

And each part has the following different choices:

\subsubsection{Instruction}
\begin{enumerate}
    \item General asking for the whole code/contract
    \item General asking for a single function
    \item Specific asking a violation type for the whole code/contract
    \item Specific asking a violation type for a function
    \item Specific asking a violation type for a function + hint 
\end{enumerate}
\subsubsection{Specification}
\begin{enumerate}
    \item Full spec
    \item No spec
\end{enumerate}
\subsubsection{Code}
\begin{enumerate}
    \item Full code
    \item Target function and its callees
    \item Target function, its callees, and their contract declaration
    \item Interface only
\end{enumerate}


This list contains all the combinations of parts we used in the later experiments.


\subsubsection{ Setting 1 }
Instruction(General Asking for code) + Specification(Full) + Code(Full)

Example Prompts: 
\begin{enumerate}
\item "Whether the code is compliant with the specification" + <full specification> + <full code>
\item "Find out any consistency between the code and specification" + <full specification> + <full code>
\end{enumerate}

\subsubsection{ Setting 2 }
Instruction(General Asking for code) + Code(Full)

Example Prompts: 
\begin{enumerate}
\item Whether the code is compliant with the specification + <full code>
\item Find out any consistency between the code and specification  + <full code>
\end{enumerate}

\subsubsection{ Setting 3 }
Instruction(General Asking for a function) + Code(function and its callees)

\begin{enumerate}
\item "Whether the function is compliant with the specification" + <target function and its callees>
\end{enumerate}


\subsubsection{ Setting 4 }
Instruction(Specific Asking for a function) + Code(function and its callees)

\begin{enumerate}
\item "Whether the function miss any required event" + <target function and its callees>
\end{enumerate}

\subsubsection{ Setting 5 }
Instruction(Specific asking a violation type for a function + hint ) + Code(function and its callee + Contract Decl)
\begin{enumerate}
\item "The function might contain errors like XXX, whether the function has XXX" + <target function and its callees>
\end{enumerate}

\subsubsection{ Setting 6 }
TODO
\subsubsection{ Setting 7 }
TODO
\subsubsection{ Setting 8 }
TODO


\subsection{ Experiment Settings and Results }
In order to comprehensively evaluate the GPT capabilities in various environments, multiple different environment settings are necessary to be conducted. Each experiment might involve one or more different prompt settings regarding different findings or purposes. Some scenarios might require some violations or even some specific violations present in the code. Due to the dataset limitation, some violations are injected into the code if the code does not have corresponding violations.


\subsubsection{ Experiment 1: Naive try }
In the beginning, we hope the GPT model identifies all the violations with context information as much as  possible in order to find out every inconsistency between the specification and the code.
We start with the prompt with the full context information: complete specification and code, which is prompt setting 1.

This experiment runs on all 3 ERC20 contracts since all the other contracts exceed the GPT model limitation. 
Every contract contains at least one violation.

\subsubsection{ Experiment 1 result }
The GPT model cannot find any violation in the code. 
After analyzing the violation, we found that each violation actually is only highly related to a few sentences regarding all the context information in the prompt: a sentence of rule in the context and a few sentences of the Solidity code. 
It is intuitive to make a guess that the GPT model cannot find out any violations due to massive unrelated information.
There are several possible places that can be improved: 1. Specification is too long 2. The code is too long.
We can start with the specification part.

\subsubsection{ Experiment 2: Does specification must present if it is in the GPT model's training set? }
Due to the design of the LLM model, a model will remember much information from its training set. Since the GPT model 3.5 used almost all the internet information before Sept. 2021, it is highly possible that it already learned many popular specifications in its training set. The specification usually consumes a large number of tokens in the prompt. It might either block the space for putting a large contract with long code in the prompt or might distract the GPT's attention. 

This experiment runs on all 3 ERC20 contracts with prompt setting 2 to see whether GPT can give an equal or better response compared to setting 1.


\subsubsection{ Experiment 2 result}
The GPT model still cannot find any violation in the code.
% Although the GPT model cannot find any issue, considering the response and other experiments about ERC understanding, we confirmed that the GPT model has a memory of the popular ERCs. 
% But whether we can find out inconsistencies without mentioning the spec in the prompt need to be further confirmed

\subsubsection{ Experiment 3: Does code length matter? }
Experiment 3 intends to let the GPT model find a violation for a specific function.
For ERC20, we choose one of the most important functions "transfer" as our target function.
Each function is injected with all 5 different violations.
The prompt setting 3 is used in this experiment. 

\subsubsection{ Experiment 3 result }
The GPT model still cannot find any violation in the code.

\input{tables/exp4_table}

\subsubsection{ Experiment 4: Does instruction matter? }
The prompt setting 4 is used in this experiment.

\subsubsection{ Experiment 4 result }
The GPT model can find some violations in the code. But there are still many violations that cannot be found.

\input{tables/exp5_table}
\subsubsection{ Experiment 5: Does add "hint" in the instruction help? }
% TODO

\subsubsection{ Experiment 5 result}
Yes
%TODO

\subsubsection{ Experiment 6: Does "hint" cause false positive? }
\subsubsection{ Experiment 6 result }
No
\subsubsection{ Experiment 7: Does "additional rule" help reduce the false positives and false negatives? }
\subsubsection{ Experiment 7 result }
No

\subsubsection{ Experiment 8: Does current SOTA instruction have false positive on code without error or with out-of-scope error  }
\subsubsection{ Experiment 8 result }
Around 18\% for ERC20 3 contracts 61 tries

\subsubsection{ Experiment 9: Can the GPT model handle obfuscation }
\subsubsection{ Experiment 9 result }
\begin{enumerate}
\item Good at mangled variable names.
\item Can identify simple logic conditions like "if(true){...}", "while(cond){ ...;cond = false;}"
\item Combined with previous experiment (4,5,6,7), the GPT cannot handle complex logic like "if(a == b \&\& c == d \&\& e == f)"
\item Combined with previous experiments (4,5,6,7), the GPT can handle the required action in other functions with long distance of call graph or control flow graph.
\end{enumerate}

\subsubsection{ Experiment 10: Can the GPT support finding multiple violation types using current SOTA instruction? }
\subsubsection{ Experiment 10 result }
No. It is better to use 1 violation type at a time.

\subsubsection{ Experiment 11: Can SOTA be used for whole code }
Since the GPT model can handle fairly long code(from experiment 9), combined with the previous conclusion from experiment 3 that code length matters, it is better to say content in the code matters. 
In order to verify the SOTA cannot be used for the whole code and the necessity of slicing the whole Solidity contract into functions, the SOTA instruction is used with the full code.
\subsubsection{ Experiment 11 result }
No.


\subsection{ Findings }

\begin{enumerate}

    \item The GPT model can find XXX, XXX, XXX violation types on XXX conditions

    Experiment 4,5,6,7
    
    \item The GPT model can find violations without including specifications in the prompt if it is in the training set

    Experiment 2
    
    % TODO: what is the meaning of simple
    \item The GPT model cannot handle the complex logic conditions

    Experiment 4,5,6,7,9
    
    % TODO: what is the meaning of complex
    \item The GPT model can handle mangled variable names

    Experiment 9
    
    \item The GPT model can handle long(cannot longer its token limitation) Solidity code after "slicing"

    Experiment 9

    \item The GPT model will 100\% trust what you provide in the prompt. It is critical to point out the code might contain errors.

    Experiment 5
    
    \item The GPT model can reduce FP/FN by providing the fine-grained description in the first or following prompt

    Experiment 7
    
    \item The GPT model will have more FN/FP as more violation types you ask it to check.

    Experiment 10

    \item The GPT model cannot identify violation type if many related  words existed
     % need to clarify here what is related (for example, missing XXX but interface has XXX)
     
    
\end{enumerate}

\fi






