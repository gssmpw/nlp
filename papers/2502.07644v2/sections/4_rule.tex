\section{Requirement Comprehension}

\subsection{Background.}

\subsection{Experiment setup.} 
In our experiment, we approach the task as a standard machine reading comprehension test. 
Common reading comprehension assessments involve evaluating an individual or system's capability to answer questions based on a provided text. 
As part of this experiment, we create various questions related to ERC requirements and present them to GPT. 
Subsequently, we manually verify each answer provided by GPT against the corresponding ground-truth answer. 
This process allows us to gauge GPT's comprehension and accuracy in addressing the specific ERC-related questions. 

\boldparagraph{Passage.} 
The passage functions as the contextual information from which we extract details to formulate answers for the provided questions. 
In our experiment, we designate the ERC specification as the passage, as shown in \ref{a}. 
Nevertheless, GPT has a restriction on the number of input tokens it can process, with a maximum token limit of 4080. 
Consequently, if the entire specification exceeds this limit, 
we divide it into several smaller chunks that adhere to the token limit. This ensures that each chunk can be processed by GPT effectively. 

\boldparagraph{Question\&Answer.} 
In the evaluation of ERC requirements, we utilize two types of questions: "Yes/No" and "Fact-Based" questions, as depicted in \ref{b}. 
The first "Yes/No" question queries whether the transfer function in ERC20 should emit an event, with the possible responses being "Yes" or "No." 
The purpose of this question is to assess GPT's understanding of a fundamental function requirementâ€”whether a specific action, 
in this case, emitting an event, should be triggered or not. 
The significance of this question type lies in its ability to gauge GPT's grasp of basic function requirements. 
By correctly answering "Yes" or "No," GPT demonstrates its comprehension of whether the transfer function is expected to emit an event as specified in the ERC standard. 
This comprehension is valuable in the context of auditing, as it aids in streamlining the process. 
If GPT understands that a function doesn't need to emit an event, auditing efforts can focus on other aspects without the necessity to verify the emit action, 
saving time and resources during the auditing process. 
Indeed, the "Fact-Based" question in \ref{b} with the corresponding answer in \ref{c} serves a crucial role in improving the accuracy and efficiency of auditing ERC requirements. 
The detailed response in \ref{c} provides the specific event name and elaborates on its attributes or parameters. 
This level of clarity ensures that auditors have accurate information about the expected event emission, enabling them to verify the implementation precisely. 
Having access to the exact event name and its relevant details helps auditors focus their attention on validating the emission of the specific event, 
reducing the chances of overlooking critical aspects during the auditing process. 
With this enhanced comprehension, auditors can efficiently assess whether the transfer function adheres to the ERC requirements concerning event emission. 
Consequently, the auditing process becomes more accurate and streamlined, leading to a higher level of confidence in the smart contract's compliance with ERC standards.

\boldparagraph{Ground-truth.} 
As of our last knowledge, no official centralized repository exists to provide ground-truth answers for ERC requirements. 
Therefore, to establish the ground-truth, we manually review and verify the information from credible sources 
such as the official GitHub repository for Ethereum Improvement Proposals (EIPs), 
which serves as a primary and trustworthy source for ERC specifications. 
Additionally, information can be sought from the official Ethereum Foundation website, Ethereum community forums like Ethereum Stack Exchange and Ethereum's subreddit, 
and relevant research papers related to specific ERCs. When further clarification is needed, 
we communicated with the authors or contributors of specific Ethereum Improvement Proposals to resolve ambiguities. 
We remain updated with the latest developments in the Ethereum ecosystem to ensure the accuracy and reliability of the ground-truth information. 

\begin{enumerate}[label=(\alph*)]
    \item \label{a} \textbf{Passage:} ... 
    Transfers \_value amount of tokens to address \_to, and MUST fire the Transfer event. 
    The function SHOULD throw if the message caller's account balance does not have enough tokens to spend. 
    Note Transfers of 0 values MUST be treated as normal transfers and fire the Transfer event. 
    function transfer(address \_to, uint256 \_value) public returns (bool success) ...
    \item \label{b} \textbf{Question:} Does the transfer function in ERC20 should emit an event? If yes, can you provide guidance on which events should be emitted? 
    \item \label{c} \textbf{Answer:} Yes, according to the ERC20 specification, the transfer function in an ERC20 token contract should emit a Transfer event. 
    The Transfer event should be triggered whenever tokens are transferred, including zero value transfers. 
    The Transfer event has three parameters: \_from (the address tokens are transferred from), \_to (the address tokens are transferred to), 
    and \_value (the amount of tokens being transferred).
\end{enumerate}

\boldparagraph{Metrics.} 
Exact Match (EM) denotes the proportion of questions for which the answer generated by the system perfectly aligns with the ground-truth answer. 
The EM score is calculated as follows, M refers to the count of exact matches, while N represents the total number of questions in the evaluation set:

$$EM=\frac{M}{N}$$

In addition, we also utilize the question-level F1 score as another evaluation metric. 
The F1 score assesses the model's precision and recall in providing answers. It is computed based on the intersection between the system-generated answer and the ground-truth answer. 
By incorporating both EM and F1 score, we gain a comprehensive understanding of the model's performance in Machine Reading Comprehension (MRC) tasks. 

\subsection{Prompts design.} 
The concept behind the designation of prompts is to provide developers with a generic instruction. 
When developers encounter an ERC requirement document, the primary objective is to identify the functions and events that require implementation. 
Therefore, \textbf{\textit{Prompt-1}} is designed to prompt GPT to list all the functions and events mentioned in a given ERC document. 
By using \textbf{\textit{Prompt-1}}, we aim to elicit a comprehensive response from GPT that outlines all the functions and events specified in the ERC document. 
By focusing on listing functions and events, developers can efficiently address the essential aspects of the ERC standard, 
making it easier to meet the required specifications and improve the smart contract's compliance with the ERC guidelines. 
\textbf{\textit{Prompt-2$\sim$7}} are designed to target different ERC requirements, each serving a specific purpose. 
\textbf{\textit{Prompt-2}} focuses on event rules, aiming to extract information about emitted events and the circumstances triggering them. 
\textbf{\textit{Prompt-3}} pertains to checking rules, seeking conditions that cause functions to throw or revert. \textbf{\textit{Prompt-4}} addresses call rules, 
identifying functions to be called when using specific functions. 
\textbf{\textit{Prompt-5}} involves value assignment rules, while \textbf{\textit{Prompt-6}} delves into return rules, 
and \textbf{\textit{Prompt-7}} is dedicated to signature rules. 

\begin{itemize}
    \item \textbf{\textit{Prompt-1}(F+E)}: Can you list all functions/events in ERC[ID]? 
    \item \textbf{\textit{Prompt-2}(RE)}: Does [name] function in ERC[ID] should emit an event? If yes, which events should be emitted?
    \item \textbf{\textit{Prompt-3}(RK)}: Does the [name] function in ERC[ID] revert or throw? If yes, 
    \item \textbf{\textit{Prompt-4}(RC)}: Is there other functions should be called when using [name] function in ERC[ID]?
    \item \textbf{\textit{Prompt-5}(RA)}: If yes, what values should be assigned when calling the [name] function?
    \item \textbf{\textit{Prompt-6}(RR)}: Does [name] function in ERC[ID] have return value? If yes, what is the expected return value when invoking the [name] function? 
    \item \textbf{\textit{Prompt-7}(Total)}: What is the function signature of the [name] function?
  \end{itemize}


\subsection{RQ1: How well does ChatGPT comprehend implementation requirements conveyed in natural language?}

Table~\ref{compre} presents the results obtained by inputting the ERC specification as the passage and using seven prompts for evaluation. 
Almost all prompts achieved a 100\% score on both Exact Match (EM) and F1 metrics, except for prompts related to event rules and checking rules. 
Regarding event rules, GPT exhibited some comprehension limitations, leading to certain misinterpretations. 
For example, it misidentified "isApprovedForSlot" as an "Approved" function, "slotOf" as a changing slot function, and "allowance" as an "approve" function. 
This limitation is likely due to the lack of sufficient description in ERC specifications for these simple functions, such as "isApprovedForSlot" and "slotOf". Consequently, 
GPT attempted to find relevant information for these functions and, in the absence of detailed descriptions, 
it resorted to referencing the descriptions of other functions like "Approved". 
For checking rules,GPT exhibits similar comprehension limitations, such as misinterpreting "isApprovedForAll" as an "Approved" function and "allowance" as an "approve" function. 
While GPT showcases a commendable overall understanding of ERC requirements, 
these limitations are observed primarily with specific "getting" and "checking" methods like "slotOf" and "isApprovedForSlot." 

The impact of these limitations on auditing might be relatively minor. 
Since these methods are primarily used for read operations and validation checks, 
any potential misinterpretations are unlikely to affect the critical functionality or security of smart contracts significantly. 
The read-only nature of "getting" methods ensures that GPT's understanding of these functions would have less influence on the auditing process.
In conclusion, while GPT may have some limitations on certain "getting" and "checking" methods in ERC requirements, 
these limitations are unlikely to have a significant impact on auditing, given the read-only nature of these methods. 
Overall, GPT demonstrates the ability to comprehend ERC requirements effectively, 
and any limitations can be mitigated by providing additional contextual information to enhance its understanding. 

\begin{table}[] \scriptsize
\setlength{\tabcolsep}{3pt} 
\caption{Results of ERC Requirements Comprehension.}\label{compre}
    \begin{tabular}{|l|ll|ll|ll|ll|ll|ll|ll|}
    \hline
    \multicolumn{1}{|c|}{\multirow{2}{*}{ERCs}} & \multicolumn{2}{c|}{\textbf{F+E}}                 & \multicolumn{2}{c|}{\textbf{RE}}                  & \multicolumn{2}{c|}{\textbf{RK}}                  & \multicolumn{2}{c|}{\textbf{RC}}                  & \multicolumn{2}{c|}{\textbf{RA}}                  & \multicolumn{2}{c|}{\textbf{RR}}                  & \multicolumn{2}{c|}{\textbf{Total}}               \\ \cline{2-15} 
    \multicolumn{1}{|c|}{}                      & \multicolumn{1}{c|}{EM} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c|}{EM} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c|}{EM} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c|}{EM} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c|}{EM} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c|}{EM} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c|}{EM} & \multicolumn{1}{c|}{F1} \\ \hline
    ERC20                                       & \multicolumn{1}{l|}{1.0}   &  1.0                 & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   &                      & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   & 1.0                        \\ \hline
    ERC721                                      & \multicolumn{1}{l|}{1.0}   &  1.0                 & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   &                      & \multicolumn{1}{l|}{1.0}   &  1.0                 & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   &  1.0                 & \multicolumn{1}{l|}{1.0}   &  1.0                       \\ \hline
    ERC777                                      & \multicolumn{1}{l|}{1.0}   &  1.0                 & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   &                      & \multicolumn{1}{l|}{1.0}   &  1.0                 & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   &  1.0                       \\ \hline
    ERC3525                                     & \multicolumn{1}{l|}{1.0}   & 1.0                & \multicolumn{1}{l|}{0.64}  & 0.66                 & \multicolumn{1}{l|}{}   &                         & \multicolumn{1}{l|}{1.0}   &  1.0                 & \multicolumn{1}{l|}{1.0}   &  1.0                 & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{}   &                         \\ \hline
    \textbf{All}                                & \multicolumn{1}{l|}{1.0}   & 1.0                & \multicolumn{1}{l|}{0.90}   & 0.96                & \multicolumn{1}{l|}{}   &                         & \multicolumn{1}{l|}{1.0}   &  1.0                 & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{1.0}   & 1.0                  & \multicolumn{1}{l|}{}   &                         \\ \hline
    \end{tabular}
    \end{table}


\label{sec:understanding}
