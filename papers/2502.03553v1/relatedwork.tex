\section{Related Work}
In this work, we reemphasize the automation of the network design process, therefore our work is closely related to the first work \cite{Macro} that revisits NAS for end-to-end discovery with minimal human intervention. This line of work is further emphasized by the more recent work of \cite{Macro-Neural}, which proposes automatically generated search spaces from existing architectures. Another recent work is that of \cite{AGNAS} as it focuses on both micro and macro search; however, it still manually stacks up the blocks to decide the final architecture. Further, we divide the related work into two parts; (1) neural architecture search and (2) face recognition applications.
 

\subsection{Neural Architecture Search}


\subsubsection{Search Space} End-to-end NAS methods search for network depth, width, and convolutional kernel size \cite{zoph2016RL,MetaQNN,Real_evolution,NASH,EAS,NASBOT} except \cite{GeneticProgrammingCNN}. In addition, all these works search for skip connections except \cite{MetaQNN,EAS}. Some works follow the VGG-like CONV-POOL-FC design paradigm \cite{MetaQNN,EAS,NASBOT}. We analyze these existing works in depth in Section \ref{SSDesign} and propose search space optimizations.

 
\subsubsection{Search Strategy}

The algorithms most commonly used by the research community are based on reinforcement learning (RL) \cite{zoph2016RL,MetaQNN,EAS}, evolutionary algorithms \cite{Real_evolution,GeneticProgrammingCNN,NASH,Macro-Neural}, and sequential model-based optimization (SMBO) \cite{NASBOT,NSGA} methods. Due to the huge search spaces, it is computationally expensive for these methods to accurately evaluate a large number of candidates. In Section \ref{OurAlgorithm}, we propose an algorithm that can discover architectures efficiently.

\subsubsection{Performance Evaluation}


Early NAS works \cite{zoph2016RL,MetaQNN,Real_evolution,GeneticProgrammingCNN} fully train candidates to establish their relative ranking and employ speed-up strategies, but an excessively large number of architectures to be evaluated leads to enormous compute costs. Hence, a significant body of NAS research lies in the speed of candidate evaluation. Model-based accuracy predictors are fairly simple \cite{modelbasedpredictor} but require architecture-accuracy paired data for every new dataset, which itself is expensive to obtain, and hence are not generalizable. \cite{EarlyTermination} and \cite{MDENAS} use learning curve prediction, and estimate the final rankings by training for fewer epochs, respectively. Although these methods may perform better for modular search spaces, our experiments in a global space show low correlation between early and final performance of the networks. The most widely adopted technique is perhaps parameter sharing in a supernet \cite{ENAS}, where subgraphs can inherit weights from a large over-parameterized network. \cite{EvaluatingSearch} shows that weight sharing leads to inaccurate rankings. Recently, there has been a substantial body of research using zero-cost proxies \cite{zerocostproxies}, which require no training or very little training. However, works such as \cite{antizeroshot,antizeroshotNIPS} show that proxies developed for modular search spaces are not transferable to global spaces and existing zero-shot proxies cannot outperform simple baseline such as number of parameters and FLOPS. In general, excessive focus on modular search has lead to a substantial research gap for efficient evaluation methods for global NAS.

\subsection{NAS for Face Recognition}

To demonstrate the transferability of our framework to face recognition applications, we use ResNets \cite{ResNet}, as modified in \cite{Adaface,Arcface}, as our baseline architectures. \cite{FairFace} applies joint NAS and hyperparameter optimization (HPO) to mitigate bias in face recognition. \cite{TeachNAS,PocketNet} combine knowledge distillation and NAS to achieve low-complexity, high-accuracy networks. However, our method discovers high-accuracy low-complexity networks solely based on the NAS framework. Although relevant, these methods are highly specialized for face recognition and hence are not directly comparable to our NAS only method.    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%