[
  {
    "index": 0,
    "papers": [
      {
        "key": "kim2022learned",
        "author": "Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt",
        "title": "Learned token pruning for transformers"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chenwalking",
        "author": "Chen, Howard and Pasunuru, Ramakanth and Weston, Jason E and Celikyilmaz, Asli",
        "title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "jiangllmlingua",
        "author": "Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili",
        "title": "Llmlingua: Compressing prompts for accelerated inference of large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2023compressing",
        "author": "Li, Yucheng and Dong, Bo and Guerin, Frank and Lin, Chenghua",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "jiang2023longllmlingua",
        "author": "Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili",
        "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2024adapting",
        "author": "Wang, Cangqing and Yang, Yutian and Li, Ruisi and Sun, Dan and Cai, Ruicong and Zhang, Yuzhu and Fu, Chengqian and Floyd, Lillian",
        "title": "Adapting LLMs for efficient context processing through soft prompt compression"
      },
      {
        "key": "bulatov2022recurrent",
        "author": "Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail",
        "title": "Recurrent memory transformer"
      },
      {
        "key": "chevalier2023adapting",
        "author": "Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi",
        "title": "Adapting Language Models to Compress Contexts"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "xu2024recomp",
        "author": "Xu, Fangyuan and Shi, Weijia and Choi, Eunsol",
        "title": "RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "labankeep",
        "author": "Laban, Philippe and Schnabel, Tobias and Bennett, Paul and Hearst, Marti A",
        "title": "Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jung2024discrete",
        "author": "Jung, Hoyoun and Kim, Kyung-Joong",
        "title": "Discrete prompt compression with reinforcement learning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "pan2024llmlingua",
        "author": "Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and R{\\\"u}hle, Victor and Yang, Yuqing and Lin, Chin-Yew and others",
        "title": "Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "CPC",
        "author": "Liskavets, Barys and Ushakov, Maxim and Roy, Shuvendu and Klibanov, Mark and Etemad, Ali and Luke, Shane",
        "title": "Prompt compression with context-aware sentence encoding for fast and improved LLM inference"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "pan2024llmlingua",
        "author": "Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and R{\\\"u}hle, Victor and Yang, Yuqing and Lin, Chin-Yew and others",
        "title": "Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "pennington2014glove",
        "author": "Pennington, Jeffrey and Socher, Richard and Manning, Christopher D",
        "title": "Glove: Global vectors for word representation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "church2017word2vec",
        "author": "Church, Kenneth Ward",
        "title": "Word2Vec"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "reimers2019sentence",
        "author": "Reimers, Nils and Gurevych, Iryna",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "li2023towards",
        "author": "Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan",
        "title": "Towards general text embeddings with multi-stage contrastive learning"
      },
      {
        "key": "wang2022text",
        "author": "Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu",
        "title": "Text embeddings by weakly-supervised contrastive pre-training"
      },
      {
        "key": "beltagy2020longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "behnamghader2024llm2vec",
        "author": "BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva",
        "title": "Llm2vec: Large language models are secretly powerful text encoders"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "CPC",
        "author": "Liskavets, Barys and Ushakov, Maxim and Roy, Shuvendu and Klibanov, Mark and Etemad, Ali and Luke, Shane",
        "title": "Prompt compression with context-aware sentence encoding for fast and improved LLM inference"
      }
    ]
  }
]