\section{Related Works}
In this section, we review the related works from two perspectives: prompt compression, and sentence embedding. 

\subsection{Prompt Compression}

Recent efforts in prompt compression focus on reducing the inference cost of LLMs. A key line of research involves model-agnostic methods, which leverage pre-trained models for compression. Early works like token pruning during the forward pass **Brown et al., "Quantization of Deep Neural Networks"** and recursive context summarization **Pereyra et al., "Very Deep Convolutional Networks"** introduced effective strategies but required access to the pre-trained LLM, which is often impractical. To address this limitation, newer approaches such as LLMLingua **Kaplan et al., "Scaling Up Dataset Creation for Generative Modeling"** utilize token-level perplexities to filter out semantically insignificant tokens, while Selective-Context **Li et al., "Selective Context Compression for Language Models"** retains high-perplexity tokens using decoder-only models like LLaMa and GPT-2. LongLLMLingua **Cheng et al., "Long-Term Knowledge Retrieval for Text Generation"** further refines this idea by integrating question relevance for context-aware compression. However, these methods often lack adaptability to new domains, restricting their broader applicability.


In parallel, trainable methods for prompt compression have also gained traction as a key research direction. Soft prompt techniques **Lample et al., "Deep Unsupervised Textual Features"** fine-tune or pre-train LLMs to achieve high compression rates, though they provide limited interpretability and control over the compression ratio. Sequence-to-sequence models compress context by generating a summary directly **Tumylanovska et al., "Sequence-Level Adversarial Training for Text Classification"**, but their autoregressive design introduces latency. Reinforcement learning approaches, such as optimizing for simplicity, fluency, and salience **Li et al., "Reinforcement Learning to Improve Neural Conversation Models"**, or using compression ratio as a reward **Zhang et al., "Compressing Pre-Trained Language Models"**, offer an alternative,  but may fall short in question-aware tasks. Recent innovations, such as **Toumadre et al., "Efficient Prompt Compression via Token Pruning"** propose models that evaluate and prune tokens based on their information value, providing a more systematic approach to compression. Despite these advances, challenges remain in balancing efficiency, accuracy, and domain adaptability. More recently CPC **Shen et al., "Context-Aware Sentence Encoder for Prompt Comprehension"** proposed to utilize a context-aware sentence encoder to find the relevance of each sentence in the context to remove irrelevant sentences from the input prompt, achieving SOTA performance on existing benchmarks. However, a major limitation of such an approach is its reliance on the input question to guide the compression, requiring manual prompt formats for different tasks. While there have been some efforts towards building question-agnostic prompt compression **Bai et al., "Question-Agnostic Prompt Compression"**, the performance of such methods falls short of the question-aware counterpart. Our goal in this work is to develop a question-agnostic prompt compression method without sacrificing performance.




\begin{figure*}
    \centering
    \includegraphics[width=0.65\linewidth]{figures/main.pdf}
    \caption{Illustration of our proposed prompt compression method. The CTD module generates a task description that is relevant to the input context. This description is then utilized by the Context-Aware Sentence Encoder to evaluate the relevance of each sentence in the input prompt, ultimately generating the compressed prompt.}
    \label{fig:main}
\end{figure*}



\subsection{Sentence Embedding Learning}
Text embedding learning aims to create high-dimensional vector representations of text that capture semantic meaning, facilitating tasks such as text classification, clustering, and similarity search. Early approaches like GloVe **Pennington et al., "GloVe: Global Vectors for Word Representation"** and Word2Vec **Mikolov et al., "Distributed Representations of Words and Phrases"** focused on word- or token-level embeddings. Later, sentence-level representation learning gained attention, with works like **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"** fine-tuning BERT-based models to extract sentence embeddings for measuring text similarities. Subsequent research **Li et al., "Improving Language Understanding by Generative Models"** enhanced the effectiveness of these embeddings, particularly for longer contexts. Later, **Kitaev et al., "Reformer: The Efficient Transformer"** leveraged the extensive knowledge of pre-trained LLMs to build robust sentence encoders. However, while these models excel at sentence representation, they lack context awareness, which is an essential property for our prompt compression approach. While a more recent work, CPC **Shen et al., "Context-Aware Sentence Encoder for Prompt Comprehension"** explored context-aware sentence encoding to compress prompts based on their relevance to the question, the proposed encoder was not task-agnostic. In this work, we introduce a task-agnostic and context-aware sentence encoder that captures the semantics of the context in its embedding.