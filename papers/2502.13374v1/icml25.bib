@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@inproceedings{shaham2023zeroscrolls,
  title={ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding},
  author={Shaham, Uri and Ivgi, Maor and Efrat, Avia and Berant, Jonathan and Levy, Omer},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
 year={2023}
}



@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{CPC,
  title={Prompt compression with context-aware sentence encoding for fast and improved LLM inference},
  author={Liskavets, Barys and Ushakov, Maxim and Roy, Shuvendu and Klibanov, Mark and Etemad, Ali and Luke, Shane},
  journal={AAAI Conference on Artificial Intelligence},
  year={2025}
}


@article{jiangllmlingua,
  title={Llmlingua: Compressing prompts for accelerated inference of large language models},
  author={Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.05736},
  year={2023}
}

@article{pan2024llmlingua,
  title={Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression},
  author={Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and R{\"u}hle, Victor and Yang, Yuqing and Lin, Chin-Yew and others},
  journal={arXiv preprint arXiv:2403.12968},
  year={2024}
}

@inproceedings{jiang2023longllmlingua,
author = {Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
title = {LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression},
booktitle = {Annual Meeting of the Association for Computational Linguistics},
year = {2023},
}

@inproceedings{li2023compressing,
  title={Compressing Context to Enhance Inference Efficiency of Large Language Models},
  author={Li, Yucheng and Dong, Bo and Guerin, Frank and Lin, Chenghua},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  pages={6342--6353},
  year={2023}
}

@article{wang2024loma,
  title={LoMA: Lossless Compressed Memory Attention},
  author={Wang, Yumeng and Xiao, Zhenyang},
  journal={arXiv preprint arXiv:2401.09486},
  year={2024}
}

@article{bulatov2022recurrent,
  title={Recurrent memory transformer},
  author={Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail},
  journal={Advances in Neural Information Processing Systems},
  pages={11079--11091},
  year={2022}
}

@inproceedings{gecontext,
  title={In-context Autoencoder for Context Compression in a Large Language Model},
  author={Ge, Tao and Jing, Hu and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@article{gpt_2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  pages={9},
  year={2019}
}

@article{touvronllama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothee and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
    journal={arXiv preprint arXiv:2302.13971}, 
    year={2023} 
}



@inproceedings{chevalier2023adapting,
  title={Adapting Language Models to Compress Contexts},
  author={Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  pages={3829--3846},
  year={2023}
}

@article{ghalandariefficient,
  title={Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning},
  author={Ghalandari, Demian Gholipour and Hokamp, Chris and Ifrim, Georgiana},
  journal={arXiv preprint arXiv:2205.08221},
  year={2022}
}


@inproceedings{labankeep,
  title={Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text},
  author={Laban, Philippe and Schnabel, Tobias and Bennett, Paul and Hearst, Marti A},
  booktitle={ International Joint Conference on Natural Language Processing},
  pages={6365--6378},
  year={2021}
}
@article{wang2024adapting,
  title={Adapting LLMs for efficient context processing through soft prompt compression},
  author={Wang, Cangqing and Yang, Yutian and Li, Ruisi and Sun, Dan and Cai, Ruicong and Zhang, Yuzhu and Fu, Chengqian and Floyd, Lillian},
  journal={arXiv preprint arXiv:2404.04997},
  year={2024}
}

@inproceedings{xu2024recomp,
  title={RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation},
  author={Xu, Fangyuan and Shi, Weijia and Choi, Eunsol},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@article{chenwalking,
  title={Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading},
  author={Chen, Howard and Pasunuru, Ramakanth and Weston, Jason E and Celikyilmaz, Asli},
  journal={arXiv preprint arXiv:2310.05029},
  year={2023}
}

@article{packer2023memgpt,
  title={Memgpt: Towards llms as operating systems},
  author={Packer, Charles and Fang, Vivian and Patil, Shishir G and Lin, Kevin and Wooders, Sarah and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2310.08560},
  year={2023}
}

@inproceedings{kim2022learned,
  title={Learned token pruning for transformers},
  author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
  booktitle={ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={784--794},
  year={2022}
}

@article{jung2024discrete,
  title={Discrete prompt compression with reinforcement learning},
  author={Jung, Hoyoun and Kim, Kyung-Joong},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE}
}

@inproceedings{reimers2019sentence,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={International Joint Conference on Natural Language Processing},
  pages={3982--3992},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{church2017word2vec,
  title={Word2Vec},
  author={Church, Kenneth Ward},
  journal={Natural Language Engineering},
  pages={155--162},
  year={2017},
  publisher={Cambridge University Press}
}


@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  pages={1532--1543},
  year={2014}
}

@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}

@article{wang2022text,
  title={Text embeddings by weakly-supervised contrastive pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{behnamghader2024llm2vec,
  title={Llm2vec: Large language models are secretly powerful text encoders},
  author={BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
  journal={arXiv preprint arXiv:2404.05961},
  year={2024}
}


@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}



@inproceedings{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
    year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  pages={9},
  year={2019}
}


@inproceedings{merity2016pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}
 

@article{kovcisky2018narrativeqa,
  title={The NarrativeQA Reading Comprehension Challenge},
  author={Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
  journal={Transactions of the Association for Computational Linguistics},
  pages={317--328},
  year={2018}
}


@inproceedings{dasigi2021dataset,
  title={A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
  author={Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A and Gardner, Matt},
  booktitle={North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4599--4610},
  year={2021}
}

@article{krapivin2009large,
  title={Large dataset for keyphrases extraction},
  author={Krapivin, Mikalai and Autaeu, Aliaksandr and Marchese, Maurizio and others},
  year={2009},
  publisher={University of Trento-Dipartimento di Ingegneria e Scienza dell'Informazione}
}

@article{hu2023meetingbank,
  title={MeetingBank: A benchmark dataset for meeting summarization},
  author={Hu, Yebowen and Ganter, Tim and Deilamsalehy, Hanieh and Dernoncourt, Franck and Foroosh, Hassan and Liu, Fei},
  journal={arXiv preprint arXiv:2305.17529},
  year={2023}
}

@article{chen2021summscreen,
  title={Summscreen: A dataset for abstractive screenplay summarization},
  author={Chen, Mingda and Chu, Zewei and Wiseman, Sam and Gimpel, Kevin},
  journal={arXiv preprint arXiv:2104.07091},
  year={2021}
}

@article{cohan2018discourse,
  title={A discourse-aware attention model for abstractive summarization of long documents},
  author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  journal={arXiv preprint arXiv:1804.05685},
  year={2018}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text Summarization Branches Out},
  pages={74--81},
  year={2004}
}

@article{yujian2007normalized,
  title={A normalized Levenshtein distance metric},
  author={Yujian, Li and Bo, Liu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages={1091--1095},
  year={2007},
}

@article{huang2021efficient,
  title={Efficient attentions for long document summarization},
  author={Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
  journal={arXiv preprint arXiv:2104.02112},
  year={2021}
}

@article{zhong2021qmsum,
  title={QMSum: A new benchmark for query-based multi-domain meeting summarization},
  author={Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Awadallah, Ahmed Hassan and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and others},
  journal={arXiv preprint arXiv:2104.05938},
  year={2021}
}

@article{fabbri2019multi,
  title={Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model},
  author={Fabbri, Alexander R and Li, Irene and She, Tianwei and Li, Suyi and Radev, Dragomir R},
  journal={arXiv preprint arXiv:1906.01749},
  year={2019}
}

@article{wang2022squality,
  title={Squality: Building a long-document summarization dataset the hard way},
  author={Wang, Alex and Pang, Richard Yuanzhe and Chen, Angelica and Phang, Jason and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2205.11465},
  year={2022}
}


@article{trivedi2022musique,
  title={MuSiQue: Multihop Questions via Single-hop Question Composition},
  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  journal={Transactions of the Association for Computational Linguistics},
  pages={539--554},
  year={2022}
}

@article{ho2020constructing,
  title={Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps},
  author={Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},
  journal={arXiv preprint arXiv:2011.01060},
  year={2020}
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@inproceedings{li2002learning,
  title={Learning question classifiers},
  author={Li, Xin and Roth, Dan},
  booktitle={International Conference on Computational Linguistics},
  year={2002}
}

@article{pang2021quality,
  title={QuALITY: Question answering with long input texts, yes!},
  author={Pang, Richard Yuanzhe and Parrish, Alicia and Joshi, Nitish and Nangia, Nikita and Phang, Jason and Chen, Angelica and Padmakumar, Vishakh and Ma, Johnny and Thompson, Jana and He, He and others},
  journal={arXiv preprint arXiv:2112.08608},
  year={2021}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{ivison2023camels,
  title={Camels in a changing climate: Enhancing lm adaptation with tulu 2},
  author={Ivison, Hamish and Wang, Yizhong and Pyatkin, Valentina and Lambert, Nathan and Peters, Matthew and Dasigi, Pradeep and Jang, Joel and Wadden, David and Smith, Noah A and Beltagy, Iz and others},
  journal={arXiv preprint arXiv:2311.10702},
  year={2023}
}


@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
