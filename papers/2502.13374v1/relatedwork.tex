\section{Related Works}
In this section, we review the related works from two perspectives: prompt compression, and sentence embedding. 

\subsection{Prompt Compression}

Recent efforts in prompt compression focus on reducing the inference cost of LLMs. A key line of research involves model-agnostic methods, which leverage pre-trained models for compression. Early works like token pruning during the forward pass \citep{kim2022learned} and recursive context summarization \citep{chenwalking} introduced effective strategies but required access to the pre-trained LLM, which is often impractical. To address this limitation, newer approaches such as LLMLingua \citep{jiangllmlingua} utilize token-level perplexities to filter out semantically insignificant tokens, while Selective-Context \citep{li2023compressing} retains high-perplexity tokens using decoder-only models like LLaMa and GPT-2. LongLLMLingua \citep{jiang2023longllmlingua} further refines this idea by integrating question relevance for context-aware compression. However, these methods often lack adaptability to new domains, restricting their broader applicability.


In parallel, trainable methods for prompt compression have also gained traction as a key research direction. Soft prompt techniques \citep{wang2024adapting, bulatov2022recurrent, chevalier2023adapting} fine-tune or pre-train LLMs to achieve high compression rates, though they provide limited interpretability and control over the compression ratio. Sequence-to-sequence models compress context by generating a summary directly \citep{xu2024recomp}, but their autoregressive design introduces latency. Reinforcement learning approaches, such as optimizing for simplicity, fluency, and salience \citep{labankeep}, or using compression ratio as a reward \citep{jung2024discrete}, offer an alternative,  but may fall short in question-aware tasks. Recent innovations, such as \citep{pan2024llmlingua}, propose models that evaluate and prune tokens based on their information value, providing a more systematic approach to compression. Despite these advances, challenges remain in balancing efficiency, accuracy, and domain adaptability. More recently CPC \cite{CPC} proposed to utilize a context-aware sentence encoder to find the relevance of each sentence in the context to remove irrelevant sentences from the input prompt, achieving SOTA performance on existing benchmarks. However, a major limitation of such an approach is its reliance on the input question to guide the compression, requiring manual prompt formats for different tasks. While there have been some efforts towards building question-agnostic prompt compression \cite{pan2024llmlingua}, the performance of such methods falls short of the question-aware counterpart. Our goal in this work is to develop a question-agnostic prompt compression method without sacrificing performance.




\begin{figure*}
    \centering
    \includegraphics[width=0.65\linewidth]{figures/main.pdf}
    \caption{Illustration of our proposed prompt compression method. The CTD module generates a task description that is relevant to the input context. This description is then utilized by the Context-Aware Sentence Encoder to evaluate the relevance of each sentence in the input prompt, ultimately generating the compressed prompt.}
    \label{fig:main}
\end{figure*}



\subsection{Sentence Embedding Learning}
Text embedding learning aims to create high-dimensional vector representations of text that capture semantic meaning, facilitating tasks such as text classification, clustering, and similarity search. Early approaches like GloVe \citep{pennington2014glove} and Word2Vec \citep{church2017word2vec} focused on word- or token-level embeddings. Later, sentence-level representation learning gained attention, with works like \citet{reimers2019sentence} fine-tuning BERT-based models \citep{vaswani2017attention} to extract sentence embeddings for measuring text similarities. Subsequent research \citep{li2023towards, wang2022text, beltagy2020longformer} enhanced the effectiveness of these embeddings, particularly for longer contexts. Later, \citet{behnamghader2024llm2vec} leveraged the extensive knowledge of pre-trained LLMs to build robust sentence encoders. However, while these models excel at sentence representation, they lack context awareness, which is an essential property for our prompt compression approach. While a more recent work, CPC \citep{CPC} explored context-aware sentence encoding to compress prompts based on their relevance to the question, the proposed encoder was not task-agnostic. In this work, we introduce a task-agnostic and context-aware sentence encoder that captures the semantics of the context in its embedding.