@article{CPC,
  title={Prompt compression with context-aware sentence encoding for fast and improved LLM inference},
  author={Liskavets, Barys and Ushakov, Maxim and Roy, Shuvendu and Klibanov, Mark and Etemad, Ali and Luke, Shane},
  journal={AAAI Conference on Artificial Intelligence},
  year={2025}
}

@article{behnamghader2024llm2vec,
  title={Llm2vec: Large language models are secretly powerful text encoders},
  author={BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
  journal={arXiv preprint arXiv:2404.05961},
  year={2024}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{bulatov2022recurrent,
  title={Recurrent memory transformer},
  author={Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail},
  journal={Advances in Neural Information Processing Systems},
  pages={11079--11091},
  year={2022}
}

@article{chenwalking,
  title={Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading},
  author={Chen, Howard and Pasunuru, Ramakanth and Weston, Jason E and Celikyilmaz, Asli},
  journal={arXiv preprint arXiv:2310.05029},
  year={2023}
}

@inproceedings{chevalier2023adapting,
  title={Adapting Language Models to Compress Contexts},
  author={Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  pages={3829--3846},
  year={2023}
}

@article{church2017word2vec,
  title={Word2Vec},
  author={Church, Kenneth Ward},
  journal={Natural Language Engineering},
  pages={155--162},
  year={2017},
  publisher={Cambridge University Press}
}

@inproceedings{jiang2023longllmlingua,
author = {Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
title = {LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression},
booktitle = {Annual Meeting of the Association for Computational Linguistics},
year = {2023},
}

@article{jiangllmlingua,
  title={Llmlingua: Compressing prompts for accelerated inference of large language models},
  author={Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.05736},
  year={2023}
}

@article{jung2024discrete,
  title={Discrete prompt compression with reinforcement learning},
  author={Jung, Hoyoun and Kim, Kyung-Joong},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE}
}

@inproceedings{kim2022learned,
  title={Learned token pruning for transformers},
  author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
  booktitle={ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={784--794},
  year={2022}
}

@inproceedings{labankeep,
  title={Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text},
  author={Laban, Philippe and Schnabel, Tobias and Bennett, Paul and Hearst, Marti A},
  booktitle={ International Joint Conference on Natural Language Processing},
  pages={6365--6378},
  year={2021}
}

@inproceedings{li2023compressing,
  title={Compressing Context to Enhance Inference Efficiency of Large Language Models},
  author={Li, Yucheng and Dong, Bo and Guerin, Frank and Lin, Chenghua},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  pages={6342--6353},
  year={2023}
}

@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}

@article{pan2024llmlingua,
  title={Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression},
  author={Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and R{\"u}hle, Victor and Yang, Yuqing and Lin, Chin-Yew and others},
  journal={arXiv preprint arXiv:2403.12968},
  year={2024}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  pages={1532--1543},
  year={2014}
}

@inproceedings{reimers2019sentence,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={International Joint Conference on Natural Language Processing},
  pages={3982--3992},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{wang2022text,
  title={Text embeddings by weakly-supervised contrastive pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}

@article{wang2024adapting,
  title={Adapting LLMs for efficient context processing through soft prompt compression},
  author={Wang, Cangqing and Yang, Yutian and Li, Ruisi and Sun, Dan and Cai, Ruicong and Zhang, Yuzhu and Fu, Chengqian and Floyd, Lillian},
  journal={arXiv preprint arXiv:2404.04997},
  year={2024}
}

@inproceedings{xu2024recomp,
  title={RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation},
  author={Xu, Fangyuan and Shi, Weijia and Choi, Eunsol},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

