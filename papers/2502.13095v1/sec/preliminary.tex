\section{Preliminaries} 

\textbf{Vision Language Models (VLMs).} 
VLMs are autoregressive text generation models that process texts and images, functioning as a mapping $\pi: \mathcal{V}^n  \times \mathcal{I} \rightarrow \mathcal{V}^m$, where $\mathcal{V}$ is the vocabulary set, $\mathcal{I}$ is the image space, and $n$ and $m$ denote the number of input and output text tokens, respectively. The input to the VLM $\pi$ includes a text prompt $\mathbf{p} = (p_1, p_2, \dots, p_n) \in \mathcal{V}^n$ and an image $\mathbf{i} \in \mathcal{I}$. Given $\mathbf{t}_\text{vl} = [\mathbf{p}, \mathbf{i}]$, the VLM $\pi(\mathbf{y} | \mathbf{t})$ generates the output sequence $\mathbf{y} \in \mathcal{V}^m$ one token at a time.

\begin{figure}[t] 
\begin{center}
    \includegraphics[width=\linewidth]{images/data.pdf}
\end{center}
\vspace{-10pt}
\caption{Examples of constructed datasets.}
\vspace{-10pt}
\label{fig:data}
\end{figure}

\textbf{Safety-related Dataset Construction.} We construct vision-language datasets, $\mathcal{D}_{\text{vl}} =  \mathcal{D}^{\text{unsafe}}_\text{vl}\cup \mathcal{D}^{\text{safe}}_\text{vl}$, containing harmful and benign instructions, respectively. In each input $\mathbf{t}_\text{vl} \in \mathcal{D}_{\text{vl}}$, the image is semantically related to the text prompt. Additionally, we create the corresponding text-only datasets, $\mathcal{D}_{\text{tt}} = \mathcal{D}^{\text{unsafe}}_\text{tt} \cup\mathcal{D}^{\text{safe}}_\text{tt}$, by replacing the image $\mathbf{i}$ in each sample $\mathbf{t}_{\text{vl}} \in \mathcal{D}_{\text{vl}}$ with its image caption $\mathbf{c}$, resulting in pairs of the form $\mathbf{t}_\text{tt} = [\mathbf{p}, \mathbf{c}] \in \mathcal{D}_{\text{tt}}$. The captions are generated by a VLM $\pi(\mathbf{c} \mid [\mathbf{p}, \mathbf{i}, \mathbf{q}])$, where $\mathbf{q}$ is the instruction: “Based on the request, describe the image”. Therefore, the samples from these two datasets (i.e., $\mathbf{t}_\text{vl}=[\mathbf{p}, \mathbf{i}]$ and its corresponding text-only version $\mathbf{t}_{\text{tt}} =[\mathbf{p}, \mathbf{c}]$) contain similar semantic information, and mainly differ in the modality. Figure \ref{fig:data} presents sample examples from these datasets, with further construction details available in Appendix \ref{appendix-datasets}.

\textbf{Activations and Directions.}
Let $\mathbf{x}^\ell (\mathbf{t})$ denote the residual stream activation of the last token at layer $\ell \in L$ of a VLM, representing the information for the input $\mathbf{t}$ processed up to layer $\ell$. We define the function $\mathtt{ActMean}$ to compute the mean last-token activation at layer $\ell$ for a given dataset $\mathcal{D}$:
\begin{equation}
\mathtt{ActMean}^\ell (\mathcal{D}) = \frac{1}{\mathcal{D}} \left [ \sum_{\mathbf{t} \in \mathcal{D}} \mathbf{x}^\ell (\mathbf{t}) \right ].
\end{equation}
Various studies \cite{cao2024personalized, arditi2024refusal, park2024linear, marks2023geometry} have shown that high-level concepts are represented as linear directions in the activation space of LLMs. These directions can be identified by computing the difference between the mean activations of a model when processing two sets of contrastive instructions, $\mathcal{D}_1$ and $\mathcal{D}_2$, that elicit distinct behaviors: 
\begin{equation}
\label{eq:r}
    \mathbf{v}^\ell_{\mathcal{D}_2\rightarrow \mathcal{D}_1} = \mathtt{ActMean}^\ell (\mathcal{D}_1) - \mathtt{ActMean}^\ell (\mathcal{D}_2).
\end{equation}
The resulting $\mathbf{v}^\ell_{\mathcal{D}_2\rightarrow \mathcal{D}_1}$, known as the \emph{difference-in-mean} vector, describes both the direction and magnitude of layer-$\ell$ activation variation from $\mathcal{D}_2$ to $\mathcal{D}_1$. This vector effectively isolates the key features that drive the model's behavioral differences between two instruction sets.
