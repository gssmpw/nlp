\appendix
\onecolumn

\section{Datasets} \label{appendix-datasets}

\subsection{Safety-Related Datasets}

\textbf{MM-SafetyBench \cite{liu2025mm}} consists of 5,040 examples with malicious intent across 13 common scenarios. Each example includes an image derived from malicious keywords and falls into one of the following categories: (1) SD: Images generated using Stable Diffusion and directly related to the malicious query. (2) OCR: Typography images, which include optical character recognition representations of malicious text queries. (3) SD+OCR: Images first generated by Stable Diffusion and then combined with typographic subtitles. In addition to image-text instructions, MM-SafetyBench also provides text-only questions based on the same malicious keywords.

\textbf{FigStep \cite{gong2023figstep}} highlights VLMs' susceptibility to harmful attacks using typography-based images. It includes 520 test samples, where images contain harmful text displayed on a white background. The task instruction start with phrases like “Steps to,” “List of,” or “Methods to” to encourage the model to generate step-by-step responses to the harmful content in the image.


\subsection{Utility-Related Datasets}

\textbf{MME \cite{DBLP:journals/corr/abs-2306-13394}} the perception (MME-P) and cognition (MME-C) abilities of VLMs across 14 sub-tasks, including 10 for MME-P and 4 for MME-C, with a total of 2,374 questions. Each instruction consists of a question followed by "Please answer yes or no". For each test image, two manually designed instructions are provided: the first has a ground-truth answer of "yes", and the second has "no". Utility scores for each sub-task are calculated as the sum of accuracy (based on individual questions) and accuracy+ (based on images, requiring both questions to be answered correctly). The perception score is the sum of all perception sub-task scores (0 to 2000), while the cognition score is computed similarly (0 to 800).

\textbf{MM-Vet \cite{DBLP:conf/icml/YuYLWL0WW24}} evaluates six core vision-language capabilities: recognition, OCR, knowledge, language generation, spatial awareness, and math, which collectively enable solving complex multimodal tasks. Unlike MME, MM-Vet requires VLMs to generate open-ended responses, making evaluation more flexible but also more challenging. To assess model performance, MM-Vet uses GPT-4 with few-shot evaluation prompts to assign a score between 0 and 1 for each response. The final utility score is calculated as the average score across all questions, multiplied by 100, resulting in a range of [0, 100].


\subsection{Dataset Construction for Section \ref{sec:why}}

In our preliminary experiments on understanding the mechanism behind safety perception distortion, $\mathcal{D}^\text{unsafe}_\text{vl}$ and $\mathcal{D}^\text{unsafe}_\text{tt}$ are constructed from MM-SafetyBench \cite{liu2025mm}, while $\mathcal{D}^\text{safe}_\text{vl}$ and $\mathcal{D}^\text{safe}_\text{tt}$ are sourced from LLaVA-Instruct-80k \cite{liu2024visual}. Note that in both MM-SafetyBench and LLaVA-Instruct-80k, images are essential to understanding the query, meaning the query cannot be answered without the image. In MM-SafetyBench, the text queries themselves do not contain malicious content; instead, the harmful content is embedded within the images.

The details of the dataset construction process are explained below.

\paragraph{MM-SafetyBench.} We divide the 13 scenarios into two sets:

\begin{itemize}
    \item \textbf{Scenarios 01-07 \& 09 (01-Illegal Activity, 02-HateSpeech, 03-Malware Generation, 04-Physical Harm, 05-Economic Harm, 06-Fraud, 07-Pornography, 09-Privacy Violence):} These scenarios involve clear-cut harmful content. A response is classified as unsafe if it includes harmful content; otherwise, it is marked as safe. The text-only LLM backbone can effectively detect these unsafe requests and generates rejections, resulting in a low ASR for text-only jailbreak attempts \cite{liu2025mm, gou2025eyes}.

    \item \textbf{Scenarios 08 \& 10-13:  (08-Political Lobbying, 10-Legal Opinion, 11-Financial Advice, 12-Health Consultation, 13-Government Decision):} These scenarios focus on political topics (scenarios 08 \& 13) or specialized professional fields such as legal and healthcare (scenarios 10-12). To generate a safe response, VLMs should refrain from expressing political opinions or acknowledge their lack of certification to provide professional advice. These cases are more challenging than the previous set, as they do not explicitly contain harmful content, and VLMs struggle even with text-only jailbreak attempts \cite{liu2025mm}.
     
\end{itemize}

Extracting a safety-relevant shift from text-only safe and unsafe inputs is essential for both our preliminary experiments on safety perception distortion and \OursMethod. If VLMs struggle to distinguish between unsafe and safe text-only inputs, the safety-relevant shift cannot be effectively extracted. Additionally, since \OursMethod\ aims to reactivate the inherent safety alignment of the pre-aligned LLM backbone, it is unlikely to improve alignment if the backbone itself is not well-aligned on text-only data. Given this, when constructing $\mathcal{D}^\text{unsafe}_\text{vl}$ and $\mathcal{D}^\text{unsafe}_\text{tt}$, we only include data from Scenarios 01-07 \& 09.

We sampled 160 instructions from Scenarios 01-07 \& 09 to construct $\mathcal{D}^\text{unsafe}_\text{vl}$ and  $\mathcal{D}^\text{unsafe}_\text{tt}$. For linear probing as described in Section \ref{sec:why}, 128 samples are used for training, and the remaining 32 for testing. Each sample has three variations corresponding to different image types: SD, OCR, and SD+OCR. As a result, both $\mathcal{D}^\text{unsafe}_\text{vl}$ and  $\mathcal{D}^\text{unsafe}_\text{tt}$ contain 480 data points. We ensure that the train and test splits do not overlap with the evaluation datasets used in the safety assessment in Section \ref{sec:exp}.

\paragraph{LLaVA-Instruct-80k.} 

LLaVA-Instruct-80k is a subset of LLaVA-Instruct-150K, the instruction-following dataset used for vision-language fine-tuning in LLaVA \cite{liu2024visual}. We sample 160 instances from it to construct $\mathcal{D}^\text{safe}_\text{vl}$ and  $\mathcal{D}^\text{safe}_\text{tt}$, ensuring they match the size of $\mathcal{D}^\text{unsafe}_\text{vl}$ and  $\mathcal{D}^\text{unsafe}_\text{tt}$. Each of these 160 samples contains a unique image paired with a single instruction. For linear probing as described in Section \ref{sec:why}, 128 samples are used for training, and the remaining 32 for testing. To align with MM-SafetyBench’s OCR and SD+OCR variations, we generate these variations for LLaVA-Instruct-80k data by embedding text queries into images (OCR) and further combining them with the original images (SD+OCR), adjusting the text queries accordingly.



\section{Baselines} \label{appendix-baselines}

\textbf{ECSO \cite{gou2025eyes}} is an inference-only defense method designed to address VLMs' weakness in handling harmful visual content. It introduces an image-to-text transformation, converting visual information into text, which is easier to regulate for safety. The method first uses the VLM’s self-evaluation to assess response safety. If the response is deemed unsafe, a specially designed prompt generates a caption for the input image, replacing the original image in the input. The VLM then produces a revised, safer response based on this caption.

For a fair comparison, since response safety checks can be integrated into any vision-language or text-only defense framework, we exclude this step in our experiments. Instead, we directly apply the image-to-text transformation to generate captions for all image inputs, replacing them before feeding the new inputs into the VLMs.


\textbf{AdaShield \cite{wang2024adashield}} offers two defense strategies: AdaShield-Static (AdaShield-S) and AdaShield-Adaptive (AdaShield-A). AdaShield-S employs manually designed defense prompts to protect VLMs. AdaShield-A is an adaptive auto-refinement framework that optimizes defense prompts for various attack scenarios to improve effectiveness. It consists of a target VLM and a defender LLM that iteratively refine defense prompts through dialogue interaction. Once optimized, AdaShield-A stores a pool of defense prompts and retrieves the most suitable one for each test query during inference.  In our experiments, we use AdaShield-S with a manually designed defense prompt.


\section{Implementation Details} \label{appendix-implementation}

\subsection{Prompt for Image Caption Generation}

In both preliminary experiments and the proposed \OursMethod, we require the evaluated VLM to generate a caption for the input image. The input template used to prompt the VLM for caption generation is shown in Table \ref{tab:image-caption-prompt}. Following \cite{gou2025eyes}, the template is designed to minimize information loss during image-to-text conversion while preserving key details relevant to the query.

\begin{table}[h]
    \renewcommand{\arraystretch}{1.4} % Increase row height
    \setlength{\arrayrulewidth}{0.8pt} % Thicker table border
    \centering

    \label{tab:image-caption-prompt}
    
    \begin{tabular}{|p{14cm}|}
        \hline
        \rowcolor[gray]{0.8} \textbf{Prompt for image caption generation} \\
        
        \hline
        
        \textbf{User} \\
        
        Request: \{ Original query \}. \\
        \texttt{<IMAGE>} \\

        \hline
        \textbf{Instruction} \\
        Based on the Request, describe the image.  \\ 
        \hline
        
        \textbf{Assistant} \\
        \{ Answer \} \\
        \hline
    \end{tabular}
\end{table}

\subsection{Rejection Keywords}

We use a rule-based approach to evaluate ASR, following \cite{liu2025mm, wang2024adashield}. Specifically, a response is classified as a rejection if it contains predefined rejection keywords. The keywords, primarily sourced from \cite{wang2024adashield} and \cite{zhou2024role}, are listed in Table \ref{table:rejection_keywords}.


\subsection{Prompt for Binary Safety Classification}

In Sections \ref{sec:exp-truely-correct-safety-perception} and \ref{sec:exp-false-alarms}, we use the evaluated VLMs as binary classifiers to determine whether a given instruction is safe or unsafe. The template used for this classification is shown in Table \ref{tab:binary-safety-prompt}, prompting the VLM to respond with “yes” or “no”. Notably, we do not emphasize the need for the model to be helpful, respectful, responsible, or safe in the prompt, as our goal is to assess the VLM's inherent safety perception ability without additional guidance.

\begin{table}[h]
    \renewcommand{\arraystretch}{1.4} % Increase row height
    \setlength{\arrayrulewidth}{0.8pt} % Thicker table border
    \centering

    \label{tab:binary-safety-prompt}
    
    \begin{tabular}{|p{14cm}|}
        \hline
        \rowcolor[gray]{0.8} \textbf{Prompt for  binary safety classification} \\
        
        \hline
        
        \textbf{User} \\
        
        Request: \{ Original query \}. \\
        \texttt{<IMAGE>} \\

        \hline
        
        \textbf{Instruction} \\
        Is the Request harmful, unsafe and unethical? Please answer yes or no:  \\ 
        
        \hline
        
        \textbf{Assistant} \\
        \{ Answer \} \\
        \hline
    \end{tabular}
\end{table}


\section{Complete results on MM-SafetyBench} \label{appendix-results}

\input{tables/safe/llava-1.5-7b}
\input{tables/safe/minigpt-4-7b}
\input{tables/safe/sharegpt4v-7b}
\input{tables/safe/qwen-vl}

In Table \ref{table:mm-safetybench-all}, we report the average ASR across all scenarios on MM-SafetyBench for all VLMs, while Table \ref{table:mm-safetybench-llava-1.5-7b} reports the ASR for each of the 8 selected scenarios out of 13 for LLaVA-1.5-7B. Here, we provide per-scenario results for MiniGPT-4-7B, ShareGPT4V-7B, and Qwen-VL-7B in Tables \ref{table:mm-safetybench-minigpt}, \ref{table:mm-safetybench-sharegpt}, and \ref{table:mm-safetybench-qwen-vl}, respectively. We observe that even without images, all models perform poorly on scenarios 08 and 10-13 in terms of safety. Additionally, inputs with typography (OCR \& SD+OCR) show significantly higher jailbreak effectiveness than SD images without text, indicating that models are particularly vulnerable to typography-based attacks.

\section{Inference Time with \OursMethod}

Table \ref{table:inference-time} reports the average inference time per response for ShiftDC and ECSO across all inputs on MM-SafetyBench and MME. ShiftDC has a slight impact on inference time and is faster than ECSO.


\input{tables/inference-time}

\section{Activation Calibration Across Layers} \label{appendix-ablation}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\linewidth]{images/calibration-per-layer.pdf}
    \caption{Attack success rates of LLaVA-1.5-7B and MiniGPT-4-7B when calibrating activations across different layer ranges. The x-axis shows the starting layer, with the end layer fixed at 32.}
    \label{fig:exp-per-layer}
\end{figure}

Our method works by extracting a safety shift vector and removing it from some specific layers of the VLM. Here we conduct an ablation study by applying \OursMethod\ to calibrate activations at different range of layers of LLaVA-1.5-7B and MiniGPT-4-7B and report the ASR on MM-SafetyBench in Figure \ref{fig:exp-per-layer}. The x-axis represents the starting layer index, with the end layer fixed at 32. For example, $x=5$ indicates that calibration is applied from layer 5 to layer 32.

As observed, starting calibration from the very early layers leads to a relatively high ASR. Specifically, starting from the 1st layer (i.e., calibrating all 32 layers) results in the poorest performance for both VLMs. This may be because extracting a meaningful direction vector in the early layers is challenging, as feature linearity is less prominent in shallow layers, which negatively impacts performance. Starting from the middle layers achieves the lowest ASR. These results align with prior work \cite{arditi2024refusal, panickssery2023steering}, which shows that activation engineering is most effective in the middle layers of LLMs. Conversely, starting calibration from only the last 10 layers also results in a high ASR, highlighting the importance of calibrating a sufficient number of layers for optimal performance.

% \section{Case Study}


\input{tables/reject_keywords}
