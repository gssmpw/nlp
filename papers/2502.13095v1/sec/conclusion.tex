\vspace{-5pt}
\section{Conclusion}

In this work, we demonstrate that the visual modality causes an activation shift, which degrades the safety of VLMs. This shift pushes activations toward a “safer” direction compared to text-only inputs, distorting the VLMs’ safety perception. To address this, we propose \OursMethod, a simple yet effective method to disentangle safety-relevant and irrelevant components of this shift. By removing the safety-relevant component, \OursMethod\ restores safety alignment while preserving visual reasoning utility. Experimental results on multiple open-source VLMs and benchmarks demonstrate its effectiveness in significantly improving safety.


\section*{Impact Statement}

Our work focuses on understanding the fragility of current safety mechanisms in open-source VLMs and enhancing them to generate safer responses in a computationally- and data-efficient manner. This contributes to building safer and more responsible AI systems. However, we acknowledge that a deeper understanding of jailbreak mechanisms could lower the barrier for adversarial attacks on open-source models. Nonetheless, we believe this does not significantly alter the overall risk profile of VLMs. Additionally, our proposed method requires harmful data for activation extraction and still has the potential to generate unsafe responses.
