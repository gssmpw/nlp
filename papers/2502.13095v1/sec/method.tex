\section{Rectifying Safety Perception Distortion}


% 提前解释目标
% 别的方法存在的问题
% 具体操作
% 好处

% 包装？

% Figure 5:
%   - 信息太多，强调重点

\begin{figure*}[t] 
\begin{center}
    \includegraphics[width=\linewidth]{images/method.pdf}
\end{center}
\vspace{-25pt}
\caption{Overview of the proposed Activation Shift Disentanglement and Calibration (\OursMethod). }
\label{fig:method}
\vspace{-15pt}
\end{figure*}


Previous efforts to mitigate safety degradation in VLMs often involve trade-offs. Post-training approaches \cite{zong2024safety} require carefully designed datasets and significant computational resources. Defensive prompt-based methods \cite{wang2024adashield} often make the model overly cautious, reducing its helpfulness even for benign instructions. Converting images into captions \cite{gou2025eyes} can trigger the intrinsic safety mechanisms of the LLM backbone but risks losing visual details such as color, texture, and object arrangement, diminishing the model's utility.

\textbf{Goal and Motivation.}
In this work, we aim to enhance VLMs' safety during inference time, while maintaining the visual information and model helpfulness. Specifically, after applying our inference-only intervention, we expect the VLM to: (1) preserve its perception ability on the safety of vision-language inputs, such that the LLM backbone's inherent safety mechanisms can be properly activated, and (2) preserve the modality-specific information (e.g., visual semantics) introduced by the visual modality, such that the VLM's vision understanding ability is maintained. 

We achieve these goals by leveraging our findings in VLMs' activation space.
As discussed in Section~\ref{sec:why}, the safety alignment degradation of VLMs is related to their safety perception distortion: the visual input causes a modality-induced activation shift, which contains a safety-relevant component that leads VLMs to misjudge unsafe request as safe and break their safety guardrails. Therefore, we approach to restore safety alignment of VLMs by rectifying safety perception distortion via Activation \underline{\smash{Shift}} \underline{D}isentanglement and \underline{C}alibration (\textbf{\OursMethod}), illustrated in Figure~\ref{fig:method}.
% Section \ref{sec:why} shows that adding images to inputs induces an activation shift, with a component of this shift moving toward a “safer” direction. This distorts the VLM’s safety perception, causing it to misinterpret unsafe instructions as safe. Building on this observation, 

\textbf{Disentangling Modality-Induced Activation Shift.}
Observation 2 \& 3 suggest that vision-language inputs $\mathbf{t}_\text{vl}=[\mathbf{p}, \mathbf{i}]\in\mathcal{D}_\text{vl}$ tend to distort model activations towards the ``safer'' side, compared to their text-only counterparts $\mathbf{t}_{\text{tt}} =[\mathbf{p}, \mathbf{c}]\in\mathcal{D}_\text{tt}$. Ideally, simply changing the modality (e.g., content in presence of image vs. text) should not introduce any safety-related shift.  Therefore, to allow VLMs process vision-language inputs without safety perception distortion, it is crucial to isolate the safety-relevant component from safety-irrelevant shifts (e.g., specifically to the modality itself) in their activation space. 
% a given modality-induced activation shift $\mathbf{m}^\ell$, we propose to disentangle it as two orthogonal components:
% \begin{equation}
% \mathbf{m}^\ell = \mathbf{s}^\ell + \textcolor{PineGreen}{\tilde{\bm{\nu}}^\ell},
% \end{equation}
% where $\textcolor{orange}{\tilde{\bm{\beta}}^\ell}$ represents a safety-relevant component, while $\textcolor{PineGreen}{\tilde{\bm{\nu}}^\ell}$, referred to as the modality shift, captures visual semantic and other unique properties from image inputs essential for visual reasoning and utility. The idea is that removing the safety shift $\textcolor{orange}{\tilde{\bm{\beta}}^\ell}$ can restore the activation to its correct safety-related position, allowing the LLM backbone’s safety alignment to function as intended. By preserving the modality shift $\textcolor{PineGreen}{\tilde{\bm{\nu}}^\ell}$, visual details and reasoning utility can also be maintained.

To this end, we propose to disentangle modality-induced activation shift as follows. During model inference, given a vision-language input $\mathbf{t}_\text{vl}=[\mathbf{p}, \mathbf{i}]$, we first obtain its text-only counterpart $\mathbf{t}_\text{tt}=[\mathbf{p}, \mathbf{c}]$ by replacing the image with its caption as introduced in Section~\ref{sec:why}. Their last-token activations at layer $\ell$ correspond to $\mathbf{x}^\ell (\mathbf{t}_\text{vl})$ and $\mathbf{x}^\ell (\mathbf{t}_\text{tt})$. We can calculate the modality-induced activation shift for the given input as follows (i.e., blue arrow in Figure~\ref{fig:method}):
% To achieve this, we propose \textbf{Modality-Safety Activation Disentanglement} (\OursMethod), as illustrated in Figure \ref{fig:method}, to calibrate activations and restore safety alignment by removing $\textcolor{orange}{\tilde{\bm{\beta}}^\ell}$. Specifically, given a vision-language prompt $\mathbf{t}_\text{vl}$ and its variation where the image is replaced with its caption $\mathbf{t}_\text{tt}$, we extract their last-token activations $\mathbf{x}^\ell (\mathbf{t}_\text{vl})$ and $\mathbf{x}^\ell (\mathbf{t}_\text{tt})$ at layer $\ell$. The vision shift $\textcolor{blue}{\tilde{\bm{\alpha}}^\ell}$ for this specific input is then computed as:
\begin{equation}
    \mathbf{m}^\ell_{\mathbf{t}_\text{tt}\rightarrow \mathbf{t}_\text{vl}} = \mathbf{x}^\ell (\mathbf{t}_\text{vl}) - \mathbf{x}^\ell (\mathbf{t}_\text{tt}).
\end{equation}
To isolate its safety-relevant component, we need to identify the safety direction in activation space. This fortunately has been pre-computed via Eq.~(\ref{eq:safe}), and we simplify its notion as $\mathbf{s}^\ell$ (i.e., yellow arrow in Figure~\ref{fig:method}). The safety-relevant component of $\mathbf{m}^\ell_{\mathbf{t}_\text{tt}\rightarrow \mathbf{t}_\text{vl}}$ is obtained by projecting it onto $\mathbf{s}^\ell$:
\begin{equation}
    \mathtt{proj}_{\mathbf{s}^\ell}(\mathbf{m}^\ell_{\mathbf{t}_\text{tt}\rightarrow \mathbf{t}_\text{vl}})=\frac{\mathbf{m}^\ell_{\mathbf{t}_\text{tt}\rightarrow \mathbf{t}_\text{vl}}\cdot \mathbf{s}^\ell}{\|\mathbf{s}^\ell\|^2}\mathbf{s}^\ell.
\end{equation}
As discussed in Observation 3, this component causes unsafe vision-language input to be misperceived as safe, thus should be removed to calibrate the activation shift. 
% compute the safety shift $\textcolor{orange}{\tilde{\bm{\beta}}^\ell}$ for this input by projecting $\textcolor{blue}{\tilde{\bm{\alpha}}^\ell}$ onto the pre-calculated safer direction $\textcolor{orange}{\hat{\bm{\beta}}^\ell}$:
% \begin{equation}
%     \textcolor{orange}{\tilde{\bm{\beta}}^\ell} = \textcolor{orange}{\hat{\bm{\beta}}^\ell} \textcolor{orange}{\hat{\bm{\beta}}^\ell}^\top \textcolor{blue}{\tilde{\bm{\alpha}}^\ell}.
% \end{equation}

\textbf{Calibrating Activation Shift.} 
With the safety-relevant component decoupled as $\mathtt{proj}_{\mathbf{s}^\ell}(\mathbf{m}^\ell_{\mathbf{t}_\text{tt}\rightarrow \mathbf{t}_\text{vl}})$, we eliminate it from the activation shift $\mathbf{m}^\ell_{\mathbf{t}_\text{tt}\rightarrow \mathbf{t}_\text{vl}}$ to obtain the \textbf{calibrated shift} (i.e., red arrow in Figure~\ref{fig:method}). Therefore, we intervene the original activation of the vision-language input as follows: 
% the original activation of $\mathbf{t}_\text{vl}$, such that the introduction of ma shift only towards safety-irrelevant direction:
\begin{align}
    \hat{\mathbf{x}}^\ell (\mathbf{t}_\text{vl}) & = \mathbf{x}^\ell (\mathbf{t}_\text{tt}) + (\underbrace{\mathbf{m}^\ell_{\mathbf{t}_\text{tt}\rightarrow \mathbf{t}_\text{vl}}-\mathtt{proj}_{\mathbf{s}^\ell}(\mathbf{m}^\ell_{\mathbf{t}_\text{tt}\rightarrow \mathbf{t}_\text{vl}})}_\text{calibrated shift})\nonumber \\
    & =  \mathbf{x}^\ell (\mathbf{t}_\text{vl}) - \mathtt{proj}_{\mathbf{s}^\ell}(\mathbf{m}^\ell_{\mathbf{t}_\text{tt}\rightarrow \mathbf{t}_\text{vl}}).
\end{align}
The calibrated shift represents the desired safety-irrelevant effect by the introduction of visual modality. The activation of the vision-language input $\mathbf{t}_\text{vl}$ is thus calibrated as $\hat{\mathbf{x}}^\ell (\mathbf{t}_\text{vl})$ (i.e., yellow circle in Figure~\ref{fig:method}), which will be passed to the later layers of VLMs to mitigate the safety-relevant shift. 

Our proposed disentangling-then-calibrating strategy for activation shift offers several advantages beyond enhancing VLM safety: (1) \textbf{preserved model utility} -- The model's ability to process visual inputs remains intact, as only the safety-related component is removed from the activation; (2) \textbf{maintained model helpfulness} -- By leveraging LLM's inherent safety mechanisms without imposing additional screening, the approach avoids making the model overly cautious; (3) \textbf{efficiency} -- The method introduces only two additional forward passes compared to standard inference, ensuring affordable computational overhead. 


% 红绿颜色
% decision boundary 命名
% decision boundary 虚线
% alpha, beta 语义
% data point 实际意义
% 转45度
% pipeline
