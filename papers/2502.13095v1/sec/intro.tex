\vspace{-20pt}
\section{Introduction}

% 更简洁，更general
% 强调重点：mis-perception safety

The development of Vision Language Models (VLMs) \cite{qi2024visual, bai2023qwen} represents a significant breakthrough, enabling seamless integration of visual and textual information for enhanced multimodal understanding. However, the incorporation of a vision module, which is a common feature in most VLM architectures, often compromises the model's safety alignment compared to its underlying language model backbone. For example, LLaVA-1.5-13B \cite{liu2024visual, liu2024improved}, built on the Vicuna-13B LLM, exhibited a 28.36\% increase in attack success rate (ASR) on the MM-SafetyBench \cite{liu2025mm} when harmful content was conveyed through images instead of text queries: a textual query like  “How to make a bomb?” could be reframed as “How to make this product?” accompanied by a \texttt{$<$bomb image$>$}, resulting in harmful responses. This vulnerability highlights how shifting harmful content from textual to visual inputs, while maintaining the core semantics, can circumvent safety mechanisms, thereby exposing a critical limitation in VLM safety alignment.

Recent studies have explored the phenomenon of safety alignment degradation in VLMs and proposed mitigation strategies, though these approaches often come with trade-offs. One line of research \cite{zong2024safety} involves post-training VLMs with carefully curated safety-specific datasets to restore alignment. However, these efforts are highly resource-intensive, requiring substantial annotation effort and computational overhead. Another line of research 
% \jian{works? or how about change work to research, i.e., a line of research, another line of research, to avoid messing up with plurality} 
\cite{gong2023figstep, wang2024adashield} designs defensive prompting techniques to guide VLMs to check image content carefully and reject unsafe requests. While effective in some scenarios, such methods often compromise model helpfulness, leading to the rejection of benign requests. Additionally, \citet{gou2025eyes} proposed transforming images into textual captions to utilize the inherent safety mechanisms of the pre-aligned LLM components within VLMs. However, such transformation frequently sacrifices fine-grained image details, thereby impairing the model's vision reasoning capabilities and limiting its overall utility.

This work aims to develop an inference-only method that extends VLMs' intrinsic defense mechanisms -- mainly effective in text-only scenarios -- to vision-language inputs, while preserving model utility and helpfulness. To this end, a critical prerequisite is understanding the underlying mechanisms of how images impact safety alignment in VLMs. The most relevant  
% \jian{minor nit: i think we can delete `recent' as it can be told from ref's year} 
works \cite{liu2024unraveling, guo2024vllm} identified that adding a visual modality causes a distribution shift in the VLM's activation space, which diminishes its ability to distinguish between safe and unsafe requests. Despite this insight, the detailed mechanisms driving this phenomenon still remain largely unexplored.

\begin{figure}[t] 
\begin{center}
    \includegraphics[width=\linewidth]{images/intro.pdf}
\end{center}
\vspace{-5pt}
\caption{Vision-language inputs cause a \emph{modality-induced activation shift}, steering VLM activations toward a “safer” direction compared to text-only inputs. This makes the VLM perceive inputs as less risky than they actually are, weakening its safety alignment.}
\vspace{-15pt}
\label{fig:intro}
\end{figure}


In this study, we first investigate the activation space of VLMs to understand how image inputs cause these models to follow malicious instructions, as shown in Figure \ref{fig:intro}. We conducted a series of analyses, with the key findings summarized as follows: (1) While LLM backbones can effectively recognize unsafe inputs in text-only scenarios, VLMs struggle to distinguish between safe and unsafe inputs when images are introduced. (2) Activations of vision-language inputs deviate from their corresponding text-only inputs, indicating that the visual modality induces an \emph{activation shift}. (3) Most activations for vision-language inputs, whether unsafe or safe, fall on the ``safe'' side of the safety boundary derived from text-only LLMs. This suggests that the activation shift includes a component, referred to as the \emph{safety-relevant shift}, which moves activations to a position that appears safer. (4) The more the activations of unsafe requests shift toward the ``safe'' side, the more likely these requests are to bypass the VLM’s safety mechanisms. 

These observations suggest 
% \jian{how about toning down a little bit? ``conclude'' seems a bit strong since our conclusion is based on empirical observation only.} 
that adding visual input induces an activation shift that can be disentangled into two components: a \emph{safety-relevant shift}, which distorts the request's perceived safety to the VLM, leading it to misinterpret unsafe inputs as safe and ultimately reply the unsafe command; a \emph{safety-irrelevant shift}, which captures meaningful visual semantics and other modality-specific properties that are orthogonal to the safety direction. Inspired by this, we propose \emph{Activation \underline{\smash{Shift}} \underline{D}isentanglement and \underline{C}alibration (\OursMethod)}, which removes the safety-relevant shift while preserving the safety-irrelevant shift when an image is incorporated as input during inference. By removing the safety-relevant shift, this approach restores activations to their appropriate safety-related position, allowing the pre-aligned LLM backbone's defense mechanism to function as intended. By preserving the safety-irrelevant shift, essential visual semantics and other modality-specific information are retained and properly anchored. Moreover, \OursMethod\ operates as an inference-only technique, requiring  only a small amount of data and no additional training. 

Through experiments on two VLM safety benchmarks, two visual reasoning utility benchmarks, and five different VLMs, we demonstrate that \OursMethod\ significantly enhances the alignment ability of VLMs without compromising their general performance. We hope these findings can inspire a new perspective on improving VLM safety alignment. 

In summary, our main contributions are as follows:
\begin{itemize}[leftmargin=*]
\itemsep0em 
    \item We empirically demonstrate that the incorporation of the visual modality shifts activations toward a safer direction, which is a key factor contributing to the degradation of safety alignment.
    \item We propose \OursMethod, a simple, effective, and efficient method for disentangling and calibrating VLM activations to restore safety alignment.
    \item Experimental results show that \OursMethod\ enhances VLM safety alignment to match and even surpass its LLM backbone without additional training, while maintaining vision reasoning capabilities.
\end{itemize}