\begin{abstract}
Recent studies reveal that vision-language models (VLMs) become more susceptible to harmful requests and jailbreak attacks after integrating the vision modality, exhibiting greater vulnerability than their text-only LLM backbones.
To uncover the root cause of this phenomenon, we conduct an in-depth analysis and identify a key issue: multimodal inputs introduce an \emph{modality-induced activation shift} toward a “safer” direction compared to their text-only counterparts, leading VLMs to systematically overestimate the safety of harmful inputs. We refer to this issue as \emph{safety perception distortion}.
To mitigate such distortion, 
% \jian{how about ``to mitigate this distortion''? because the previous sentences didn't explicitly mention any challenge}
we propose \emph{Activation \underline{\smash{Shift}} \underline{D}isentanglement and \underline{C}alibration (\OursMethod)},
% \jian{do we need to capitalize the initial of ``activation''? otherwise it looks a bit weird to me as it seems more like you forgot to capitalize it. or maybe we can underline Shift, D, and C}
a training-free method that decomposes and calibrates the modality-induced activation shift to reduce the impact of modality on safety. By isolating and removing the safety-relevant component,  \OursMethod\ restores the inherent safety alignment of the LLM backbone while preserving the vision-language capabilities of VLMs. Empirical results demonstrate that \OursMethod\ significantly enhances alignment performance on safety benchmarks without impairing model utility. 
% The code is available at \href{https://github.com/Renovamen/ShiftDC}{https://github.com/Renovamen/ShiftDC}.

\textcolor{red}{Warning: This paper may contain examples of offensive or harmful text and images.}

\end{abstract}