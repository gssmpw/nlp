\section{Related Work}

\textbf{VLM Jailbreak Attacks.}
Research has shown that the continuous and high-dimensional nature of visual inputs makes VLMs more vulnerable to adversarial attacks. VLMs can be jailbroken by optimizing adversarial images designed to trigger harmful responses \cite{niu2024jailbreaking, qi2024visual}. For example, imgJP \cite{niu2024jailbreaking} optimizes a universal perturbation across unseen prompts and images to generate a targeted response. Several studies have further evaluated VLMsâ€™ robustness to adversarial images \cite{dong2023robust, han2023ot, zhao2024evaluating}. In contrast to perturbation-based methods, other approaches embed high-risk content directly into images using generative models \cite{liu2025mm, luo2024jailbreakv, li2025images} or typography \cite{gong2023figstep, liu2025mm, shayegani2023jailbreak}. The vulnerability of VLMs to malicious image inputs has been evaluated in various scenarios by \cite{liu2025mm, luo2024jailbreakv}. FigStep \cite{gong2023figstep} further demonstrates that embedding textual prompts designed to induce step-by-step responses into images increases the risk of VLMs generating harmful outputs. Our work primarily focuses on uncovering why VLMs are vulnerable to visual inputs and exploring ways to mitigate this vulnerability.


\textbf{VLM Jailbreak Defenses.} 
Defense approaches against VLM jailbreaks typically involve fine-tuning on specialized safety-related datasets using reinforcement learning from human feedback (RLHF) \cite{sun2023aligning, zhang2024spa} or supervised fine-tuning \cite{zong2024safety, chen2024dress}. Other approaches incorporate trained classifiers or fine-tuned defense LLMs \cite{pi2024mllm} to detect and correct harmful outputs. However, these approaches are resource-intensive and heavily depend on the quality of annotated training data. Moreover, their safety capabilities are often restricted to the specific domains covered in the training data. Inference-only defenses overcome these limitations. AdaShield \cite{wang2024adashield} iteratively refines prompts to help VLMs carefully examine image content and reject unsafe requests using an LLM defender. ECSO \cite{wang2024adashield} converts visual content into text to reactivate the LLM backbone's inherent alignment mechanism. However, these methods are either time-consuming due to iterative prompt generation or suffer from reduced helpfulness and reasoning abilities caused by defensive prompts or loss of visual details \cite{ding2024eta}.


\textbf{Understanding the Mechanism of VLM Jailbreaks.} 
Few studies have examined how the image modality affects VLM behavior and leads them to follow harmful instructions. VLGuard \cite{zong2024safety} suggests that VLMs' safety degradation is caused by catastrophic forgetting during vision-language fine-tuning and the presence of harmful content in instruction-tuning datasets. However, several studies have shown that the safety degradation in a VLM's fine-tuned LLM backbone is minimal compared to its original, pre-fine-tuned version \cite{guo2024vllm, luo2024jailbreakv}. FigStep \cite{gong2023figstep} shows that step-by-step instructional typography embedded in images is effective because safe and unsafe typography representations become intermixed, making them harder to distinguish. This observation is also reported in \cite{liu2024unraveling, guo2024vllm}. Building on this, CMRM \cite{liu2024unraveling} proposes removing the influence of image incorporation in hidden states to restore safety alignment. ETA \cite{ding2024eta} shows that LLM backbones are aligned with discrete textual embeddings, which is why continuous visual embeddings can bypass safety mechanisms. Mapping continuous tokens to discrete ones significantly reduces unsafe rate. While promising, it still remains unclear how adding images impacts VLM activation spaces in ways that affect safety and how to separate this safety impact from modality-induced effects that are essential for utility and helpfulness.

