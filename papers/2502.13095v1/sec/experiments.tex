\section{Experiments} \label{sec:exp}

\subsection{Models and Baseline Methods} 

We compare \OursMethod\ with recent inference-time VLM defense frameworks, AdaShield \cite{wang2024adashield} and ECSO \cite{gou2025eyes} on five open-source VLMs: LLaVA-1.5-7B \cite{liu2024visual, liu2024improved}, LLaVA-1.6-34B \cite{liu2024llava}, MiniGPT-4-7B \cite{zhu2023minigpt}, ShareGPT4V-7B \cite{chen2024sharegpt4v}, and Qwen-VL-7B \cite{bai2023qwen}.

\subsection{Main Results on Safety} 

\textbf{Evaluation Metric.} 
To evaluate the effectiveness of a jailbreak attack under a defense framework, we measure the \textbf{Attack Success Rate (ASR)}, defined as the ratio of harmful responses to the total number of input queries. A lower ASR indicates a stronger defense against attacks. Following \cite{liu2025mm, wang2024adashield}, we classify harmful responses by checking for the presence of rejection keywords in the response, predefined in 
% such as phrases like “I am sorry” and “I apologize”. Details can be found in 
Appendix \ref{appendix-implementation}.

\input{tables/safe/all}
\input{tables/safe/llava-1.5-7b-part}
\input{tables/figstep}

\textbf{Safety Benchmarks.}
Experiments evaluating the safety of VLMs' responses are conducted on the \textbf{MM-SafetyBench} \cite{liu2025mm} and \textbf{FigStep} \cite{gong2023figstep} benchmarks. MM-SafetyBench assesses VLM safety across 13 commonly prohibited scenarios. Each query is represented in three input formats: (1) Stable-diffusion images (SD); (2) Typography (OCR) images and (3) SD+OCR images.  FigStep rephrases harmful instructions to encourage the model to generate answers item-by-item and converts them into images using typography. More details are in Appendix \ref{appendix-datasets}.

\textbf{Evaluation Results.}
For MM-SafetyBench, the average ASR across 13 scenarios for all VLMs is shown in Table \ref{table:mm-safetybench-all}, while Table \ref{table:mm-safetybench-llava-1.5-7b-part} presents ASR results for 8 out of 13 scenarios using LLaVA-1.5-7B, following \cite{gou2025eyes}. Table~\ref{table:figstep} shows ASR results on FigStep across different VLMs. Complete results are available in the Appendix \ref{appendix-results}.

Most VLM backbones exhibit a high ASR when processing vision-language inputs. While SD images cause only a slight increase in ASR, typography-based attacks (OCR \& FigStep) are highly effective. After applying \OursMethod, ASR is significantly reduced across all VLMs and attack types, demonstrating its effectiveness in reactivating safety alignment and defending against attacks. \OursMethod\ also outperforms ECSO and AdaShield, highlighting the effectiveness of its activation calibration.


\subsection{Main Results on Utility}

\input{tables/utility}

\OursMethod\ is designed to not compromise VLM visual utility, thus the model is also evaluated on utility benchmarks.

\textbf{Utility Benchmarks.}
Experiments are conducted on popular VLM utility benchmarks, \textbf{MME} and \textbf{MM-Vet}, which assess essential VLM capabilities. MME evaluates performance using accuracy (per question) and accuracy+ (per image, requiring both questions to be correct). MM-Vet, which requires open-ended responses, is scored based on the average GPT-4 rating (0 to 1) across all samples. Details are provided in Appendix \ref{appendix-datasets}.


\textbf{Evaluation Results.}
Table \ref{table:utility} presents the utility scores of all VLMs on the MME and MM-Vet benchmarks. On these benchmarks, \OursMethod\ performs similarly to the original models and outperforms other baselines. This demonstrates that \OursMethod\ successfully preserves visual reasoning utility by maintaining modality shifts in the activation space.


\subsection{Does \OursMethod\ Truly Correct Safety Perception?}  \label{sec:exp-truely-correct-safety-perception}


\begin{figure}[t]
    \begin{minipage}{0.49\linewidth}
        \begin{center}
        \includegraphics[width=\linewidth]{images/safety-cls-bar.pdf}
        \end{center}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \begin{center}
        \vspace{-7pt}
        \includegraphics[width=\linewidth]{images/activation_vis_llava_after.pdf}
        \end{center}
    \end{minipage}
    \caption{\textbf{Left}: Binary safety classification accuracy across VLMs. \textbf{Right}: t-SNE visualization of LLaVA-1.5-7B activation on \yellowcircle\ $\mathcal{D}_\text{tt}^\text{safe}$, \purplecircle\ $\mathcal{D}_\text{tt}^\text{unsafe}$, \greencircle\ $\mathcal{D}_\text{vl}^\text{safe}$, and \bluecircle\ $\mathcal{D}_\text{vl}^\text{unsafe}$ after applying \OursMethod.}
    \vspace{-20pt}
    \label{fig:exp-safety-cls-acc-vis}
\end{figure}

% \begin{figure}[t] 
% \begin{center}
%     \includegraphics[width=\linewidth]{images/safety-cls-bar.pdf}
% \end{center}
% \caption{Binary safety classification accuracies. \lu{you can make figure 8 and 9 slimmer and put them side by side to save space}}
% \label{fig:exp-safety-cls-unsafe}
% \end{figure}

% \begin{figure}[t] 
% \begin{center}
%     \includegraphics[width=0.75\linewidth]{images/activation_vis_llava_after.pdf}
% \end{center}
% \caption{t-SNE visualization of LLaVA-1.5-7B activations on \yellowcircle\ $\mathcal{D}_\text{tt}^\text{safe}$, \purplecircle\ $\mathcal{D}_\text{tt}^\text{unsafe}$, \greencircle\ $\mathcal{D}_\text{vl}^\text{safe}$, and \bluecircle\ $\mathcal{D}_\text{vl}^\text{unsafe}$ after applying \OursMethod.}
% \label{fig:exp-safety-cls-vis}
% \end{figure}

\OursMethod\ eliminates the safety shift in activations caused by the visual modality, allowing VLMs to accurately identify unsafe instructions. To evaluate its effectiveness, we measure the binary safety classification accuracy of each VLM on LLaVA-Instruct-80k (safe) and MM-SafetyBench (unsafe) after applying \OursMethod. Here we use each VLM as a classifier to determine whether inputs are safe or unsafe, detailed in Appendix \ref{appendix-implementation}.
% by prompting it with “Is the instruction harmful, unsafe, and unethical? Answer yes or no:”  
The classification accuracies for different models are presented in Figure~\ref{fig:exp-safety-cls-acc-vis} (left). Results for text-only input accuracy and accuracy before applying \OursMethod\ are also provided for reference. After applying \OursMethod, the accuracy for image-text inputs improve significantly to match the text-only accuracy, as expected.

We also visualize LLaVA-1.5-7B's activations after applying \OursMethod\ in Figure~\ref{fig:exp-safety-cls-acc-vis} (right). The visualization shows that the activations for unsafe and safe image-text instructions are now separable, contrary to the previous intermixed state shown in Figure \ref{fig:tsne}. Additionally, most unsafe image-text activations are positioned correctly on the “unsafe” side of the boundary derived from text-only activations, demonstrating that \OursMethod\ works as intended.



\subsection{Does \OursMethod\ Cause False Alarms on Safe Datasets?} \label{sec:exp-false-alarms}

\input{tables/miscls}

To ensure that \OursMethod\ maintains \textbf{VLM helpfulness} on benign instructions, Table~\ref{table:benign-miscls} reports the changes in the misclassification rate (safe samples misclassified as unsafe) on MME, MM-Vet, and instructions sampled from LLaVA-Instruct-80K after applying \OursMethod. Since these datasets are entirely benign and do not trigger harmful responses, any detection of harm is considered a false alarm. The results show that \OursMethod\ rarely increases the misclassification rate in most cases, indicating that it preserves the activations of benign instructions in their correct safe positions.


\subsection{Mechanism of How Defensive Prompts Work}

AdaShield operates by prepending a defensive prompt to the inputs, guiding the VLM to thoroughly analyze the image and instruction before responding. Defensive prompt-based methods have been shown to risk rejection of benign requests. Here we analyze the mechanism of defensive prompt-based strategies, specifically AdaShield \cite{wang2024adashield}, from the perspective of activation shifts. 

\begin{figure}[t]
    \centering
    \includegraphics[height=9em, width=0.8\linewidth]{images/cosine-adashield.pdf}
    \vspace{-15pt}
    \caption{Cosine similarity between the activation shift induced by the defensive prompt and the safety-relevant shift $\mathbf{s}^\ell$.}
    \vspace{-20pt}
    \label{fig:defense-prompt}
\end{figure}

For each layer, we calculate the activation shift contrasting inputs with and without the defensive prompt, and compute its cosine similarity to the safety-relevant shift $\mathbf{s}^\ell$. Figure \ref{fig:defense-prompt} shows a negative cosine similarity across most layers for both safe and unsafe datasets, indicating that defensive prompts consistently push activations toward the unsafe side.  While this helps VLMs correctly identify unsafe inputs, it causes
safe inputs to be misclassified as unsafe and rejected. 
% Additionally, it pushes harmless activations too far from their original distribution, increasing perplexity and reducing utility. 
In contrast, \OursMethod\ uses the safety-related direction as an anchor, ensuring that activations are not excessively shifted toward the unsafe side, effectively mitigating this issue.

\subsection{Inference Efficiency}
We report the average inference time per response for \OursMethod\ and ECSO \cite{gou2025eyes} across all inputs on MM-SafetyBench and MME in Table \ref{table:inference-time}. \OursMethod\ increases inference time compared to the backbone, as it requires two additional forward passes to obtain image captions and input activations. However, the second forward pass is faster since it does not require autoregressive text generation, only activation extraction. The increase in inference time is smaller than ECSO, which requires two full autoregressive generations for response safety checks and image captioning.