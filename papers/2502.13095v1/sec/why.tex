\section{How Do Vision-Language Inputs Distort Safety Perception?} \label{sec:why}


\newcommand{\yellowcircle}{\tikz\draw[fill=yellow,draw=yellow] (0,0) circle (0.5ex);}
\newcommand{\bluecircle}{\tikz\draw[fill=blue,draw=blue] (0,0) circle (0.5ex);}
\newcommand{\greencircle}{\tikz\draw[fill=OliveGreen,draw=OliveGreen] (0,0) circle (0.5ex);}
\newcommand{\purplecircle}{\tikz\draw[fill=RedViolet,draw=RedViolet] (0,0) circle (0.5ex);}


% \begin{figure*}[t]
%     \centering
    
%     \begin{minipage}[t]{0.34\linewidth}
%         \begin{center}
%         \includegraphics[width=\linewidth]{images/binary-safety-acc.pdf}
%         \end{center}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.25\linewidth}
%         \centering
%         \raisebox{4pt}{\includegraphics[width=\linewidth]{images/cm.pdf}}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.32\linewidth}
%         \centering
       
%         % \includegraphics[width=\linewidth]{images/activation_vis.pdf}

%         \raisebox{14pt}{\includegraphics[width=\linewidth]{images/activation_vis.pdf}}
%     \end{minipage}

%     \vspace{1cm}

%     \begin{minipage}[t]{0.34\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{example-image}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.25\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{example-image}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{example-image}
%     \end{minipage}

    
%     \caption{The first row shows results for LLaVA-1.5-7B, and the second row for MiniGPT-4-7B. \textbf{Left:} Binary safety classification accuracy per layer across three settings. \textbf{Middle:} Confusion matrix for the setting where the model is trained on $\mathcal{D}_\text{tt}$ and tested on $\mathcal{D}_\text{vl}$. \textbf{Right:} t-SNE visualization of the model's last token activations for \yellowcircle\ $\mathcal{D}_\text{tt}^\text{safe}$, \purplecircle\ $\mathcal{D}_\text{tt}^\text{unsafe}$, \greencircle\ $\mathcal{D}_\text{vl}^\text{safe}$, and \bluecircle\ $\mathcal{D}_\text{vl}^\text{unsafe}$. The red line indicates the boundary between text-only safe samples and unsafe samples.}

%     \label{fig:binary-safety-cls}
% \end{figure*}


Previous studies have shown that transforming malicious input from text to image significantly weakens the safety alignment of VLMs \cite{liu2025mm, gong2023figstep}. To investigate the underlying cause of this phenomenon, we conduct a series of experiments on the activation spaces of LLaVA-1.5-7B \cite{liu2024visual} and MiniGPT-4-7B \cite{zhu2023minigpt}, two widely used VLMs. Our findings reveal the issue of \textbf{safety perception distortion}: compared to text-only inputs, image-text inputs shift the activations, causing VLMs to become overly optimistic about its input safety, which is detailed as follows.
 
% This ensures no domain shift between $\mathcal{D}_{\text{vl}}$ and $\mathcal{D}_{\text{tt}}$.


\textbf{Observation 1: VLMs struggle to differentiate between safe and unsafe vision-language inputs.} \label{sec:intro-1}
Recent works \cite{lee2024mechanistic, panickssery2023steering} have found that safety-aligned LLMs can identify unsafe requests in their activation space. To check whether VLMs maintain similar safety perception ability after integrating visual input, we  probe the model's activation via a linear classifier. Given a dataset $\mathcal{D} = \mathcal{D}^\text{safe} \cup \mathcal{D}^\text{unsafe}$ with instructions labeled as ``safe'' or ``unsafe'', we train a classification model $\mathbf{W} \in \mathbb{R}^d$ for each layer $\ell$ to predict whether the activation $\mathbf{x}^\ell (\mathbf{t})$ corresponds to a safe or unsafe instruction using the training set:
\begin{equation}
P(\text{safety} | \mathbf{x}^\ell) = \text{softmax} (\mathbf{W} \, \mathbf{x}^\ell (\mathbf{t})), \; \mathbf{t} \in \mathcal{D}.
\end{equation}
We conduct binary safety classification experiments under two settings: (1) train and test on the text-only inputs $\mathcal{D}_\text{tt}$ and (2) train and test on the vision-language inputs $\mathcal{D}_\text{vl}$. Both $\mathcal{D}_\text{tt}$ and $\mathcal{D}_\text{vl}$ use a 4:1 split for training and testing. 

\begin{figure}[t]
    \begin{minipage}{0.45\linewidth}
        \begin{center}
        \includegraphics[height=3.8cm, width=\linewidth]{images/binary-safety-acc.pdf}
        \end{center}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\linewidth}
        \begin{center}
        \includegraphics[height=3.8cm, width=\linewidth]{images/binary-safety-acc-mini.pdf}
        \end{center}
    \end{minipage}
    \vspace{-5pt}
    \caption{Safety classification accuracy by probing per layer.}
    \vspace{-10pt}
    \label{fig:classification_acc}
\end{figure}
    
Figure~\ref{fig:classification_acc} shows the safety classification accuracy by probing VLMs's activations per layer. For both LLaVA-1.5-7B and MiniGPT-4-7B, the binary classifiers trained on the text-only dataset $\mathcal{D}_\text{tt}$ achieve $\sim 90\%$ accuracy on its test set at middle layers, while the classifiers trained on $\mathcal{D}_\text{vl}$ achieve only $\sim 65\%$ accuracy, barely above random guessing. The results suggest that while the LLM backbone can distinguish between safe and unsafe text-only inputs, VLMs struggle with vision-language inputs. This indicates that activations for safe and unsafe data in $\mathcal{D}_\text{tt}$ are linearly separable, but those in $\mathcal{D}_\text{vl}$ are intermixed, even in deeper layers. 


\textbf{Observation 2: Visual modality induces an activation shift, causing VLMs to misperceive instructions as safer.} We also observe from Figure~\ref{fig:classification_acc} (left) that when the safety classifiers are trained on text-only inputs $\mathcal{D}_\text{tt}$ and tested on vision-language inputs $\mathcal{D}_\text{vl}$, their accuracies in the middle layers drop to $\sim 60\%$, causing $\sim 30\%$ decrease compared to testing on the original text-only test set of $\mathcal{D}_\text{tt}$. To understand the cause of this drop, Figure \ref{fig:confusion} shows the corresponding confusion matrices. The results indicate that $\sim 95\%$ of safe instructions and $\sim 70\%$ of unsafe instructions are classified as ``safe'', suggesting a clear tendency to overestimate the safety of vision-language inputs.

\begin{figure}[t]
    \begin{minipage}{0.45\linewidth}
        \begin{center}
        \includegraphics[height=3.8cm, width=\linewidth]{images/cm-llava.pdf}
        \end{center}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\linewidth}
        \begin{center}
        \includegraphics[height=3.8cm, width=\linewidth]{images/cm-mini.pdf}
        \end{center}
    \end{minipage}
    \vspace{-5pt}
    \caption{Confusion matrices of safety-probing classifiers trained on text-only $\mathcal{D}_\text{tt}$ and tested on vision-language $\mathcal{D}_\text{vl}$.}
    \vspace{-5pt}
    \label{fig:confusion}
\end{figure}

To visualize such shift, as shown in Figure \ref{fig:tsne}, we project layer-15 activations onto a 2D space, and highlight three key points: (1) Activations on text-only \yellowcircle\ $\mathcal{D}_\text{tt}^\text{safe}$ and \purplecircle\ $\mathcal{D}_\text{tt}^\text{unsafe}$ are clearly separable, while those of vision-language \greencircle\ $\mathcal{D}_\text{vl}^\text{safe}$ and \bluecircle\ $\mathcal{D}_\text{vl}^\text{unsafe}$ are intermixed, supporting Observation 1. (2) Activations on text-only \yellowcircle\purplecircle\ $\mathcal{D}_\text{tt}$ and vision-language \greencircle\bluecircle\ $\mathcal{D}_\text{vl}$ are distinctly separated, suggesting that including an image modality shifts the activations away from its original distribution optimized for the LLM backbone. This aligns with observations from \cite{liu2024unraveling}. (3) Most samples from vision-language \greencircle\bluecircle\ $\mathcal{D}_\text{vl}$, including unsafe ones, fall on the ``safe'' side of the safety boundary (red line) derived from $\mathcal{D}_\text{tt}$, indicating that incorporating images for malicious instructions shifts their activations toward the safer side. This explains why a classifier trained on $\mathcal{D}_\text{tt}$ often misclassifies $\mathcal{D}_\text{vl}$ samples as ``safe'', regardless of their true labels.

\begin{figure}[t]
    \begin{minipage}{0.49\linewidth}
        \begin{center}
        \includegraphics[width=\linewidth]{images/activation_vis_llava.pdf}
        \end{center}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \begin{center}
        \includegraphics[width=\linewidth]{images/activation_vis_mini.pdf}
        \end{center}
    \end{minipage}
    \caption{t-SNE visualization of the model's last token activations on \yellowcircle\ $\mathcal{D}_\text{tt}^\text{safe}$, \purplecircle\ $\mathcal{D}_\text{tt}^\text{unsafe}$, \greencircle\ $\mathcal{D}_\text{vl}^\text{safe}$, and \bluecircle\ $\mathcal{D}_\text{vl}^\text{unsafe}$. The red line indicates the boundary between text-only safe samples and unsafe samples.}
    \vspace{-15pt}
    \label{fig:tsne}
\end{figure}


\textbf{Observation 3: Increased activation shift towards the ``safe'' side correlates with a higher chance of bypassing VLM safety mechanisms.} To investigate how the magnitude of safety misperception in activations affects the likelihood of safety violation in VLMs, we analyze the activation shift specifically in the safety-related direction.
% We quantitatively assess the extent of the activation shift induced by images toward the safe side by calculating the cosine similarity with the "safer direction". 
To this end, we extract the activation shift by contrasting  text-only benign dataset $\mathcal{D}_\text{tt}^\text{safe}$ and harmful dataset $\mathcal{D}_\text{tt}^\text{unsafe}$, using difference-in-mean as described in Eq.~(\ref{eq:r}):
% extract two types of activation shifts for layer $\ell$, \emph{modality shift} and \emph{safety-relevant shift}, using \emph{difference-in-mean} as in Eq.~(\ref{eq:r}):
% \begin{equation}
% \mathbf{m}^\ell(\mathcal{D}_\text{tt} \rightarrow \mathcal{D}_\text{vl}) = \mathtt{ActMean} (\mathcal{D}_\text{vl}) - \mathtt{ActMean} (\mathcal{D}_\text{tt}),
% \end{equation}
% \small{
\begin{equation}
\label{eq:safe}
\small
\mathbf{s}^\ell_{\mathcal{D}_\text{tt}^\text{unsafe} \rightarrow \mathcal{D}_\text{tt}^\text{safe}} = \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{safe}) - \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{unsafe}),
\end{equation}
where 
% the modality shift $\mathbf{m}^\ell(\mathcal{D}_\text{tt} \rightarrow \mathcal{D}_\text{vl})$ represents how changing modality from text-only $\mathcal{D}_\text{tt}$ to vision-language $\mathcal{D}_\text{vl}$ shifts the activations, while
$\mathbf{s}^\ell_{\mathcal{D}_\text{tt}^\text{unsafe} \rightarrow \mathcal{D}_\text{tt}^\text{safe}}$ represents the activation shift from unsafe to safe instructions, referred to as \textbf{safety-relevant shift}. We contrast text-only datasets to identify this shift, as their activations exhibit greater linear separability w.r.t. safety, as shown in Observation 1. 

We also compute activation shifts induced by the introduction of the visual modality. Considering whether an input successfully jailbreaks the VLM, we partition the harmful vision-language dataset $\mathcal{D}^\text{unsafe}_\text{vl}$ into two subsets: $\mathcal{D}_\text{vl}^\text{success}$, which successfully bypass safety mechanisms, and $\mathcal{D}_\text{vl}^\text{failure}$, which does not. Their text-only counterparts are $\mathcal{D}_\text{tt}^\text{success}$ and $\mathcal{D}_\text{tt}^\text{failure}$ respectively. We also construct a special vision-language set $\mathcal{D}_\text{vl}^\text{blank}$, where each request from the text-only harmful $\mathcal{D}_\text{tt}^\text{unsafe}$ is paired with a blank image. Based on these fine-grained categorization of unsafe instructions, we follow Eq.~(\ref{eq:r}) to derive the following \textbf{modality-induced activation shifts}:
% with detailed calculation in \lu{Appendix xxx}: \small{$\mathbf{v}^\ell_{\mathcal{D}_\text{tt}^\text{unsafe} \rightarrow \mathcal{D}_\text{vl}^\text{unsafe}}$}
% , including $\textcolor{blue}{\bm{\alpha}^\ell_\text{unsafe: success}}$ for unsafe inputs that successfully jailbreak the VLM in image-text form and $\textcolor{blue}{\bm{\alpha}^\ell_\text{unsafe: fail}}$ for unsafe inputs rejected by the VLM. $\textcolor{blue}{\bm{\alpha}^\ell_\text{blank}}$, for text-only attack queries paired with a blank image, is also included.
\vspace{-5pt}
\begin{equation}
\small
\begin{aligned}
& \mathbf{m}^\ell_{\mathcal{D}_\text{tt}^\text{unsafe} \rightarrow \mathcal{D}_\text{vl}^\text{unsafe}} = \mathtt{ActMean}^\ell (\mathcal{D}_\text{vl}^\text{unsafe}) - \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{unsafe}),\\
& \mathbf{m}^\ell_{\mathcal{D}_\text{tt}^\text{success} \rightarrow \mathcal{D}_\text{vl}^\text{success}} = \mathtt{ActMean}^\ell (\mathcal{D}_\text{vl}^\text{success}) - \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{success}),\\
& \mathbf{m}^\ell_{\mathcal{D}_\text{tt}^\text{failure} \rightarrow \mathcal{D}_\text{vl}^\text{failure}} = \mathtt{ActMean}^\ell (\mathcal{D}_\text{vl}^\text{failure}) - \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{failure}),\\
& \mathbf{m}^\ell_{\mathcal{D}_\text{tt}^\text{unsafe} \rightarrow \mathcal{D}_\text{vl}^\text{blank}} = \mathtt{ActMean}^\ell (\mathcal{D}_\text{vl}^\text{blank}) - \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{unsafe}).\nonumber
\end{aligned}
\end{equation}
% \begin{align*}
% & \mathbf{m}^\ell_{\mathcal{D}_\text{tt}^\text{unsafe} \rightarrow \mathcal{D}_\text{vl}^\text{unsafe}} = \mathtt{ActMean}^\ell (\mathcal{D}_\text{vl}^\text{unsafe}) - \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{unsafe}),\\
% & \mathbf{m}^\ell_{\mathcal{D}_\text{tt}^\text{success} \rightarrow \mathcal{D}_\text{vl}^\text{success}} = \mathtt{ActMean}^\ell (\mathcal{D}_\text{vl}^\text{success}) - \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{success}),\\
% & \mathbf{m}^\ell_{\mathcal{D}_\text{tt}^\text{failure} \rightarrow \mathcal{D}_\text{vl}^\text{failure}} = \mathtt{ActMean}^\ell (\mathcal{D}_\text{vl}^\text{failure}) - \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{failure}),\\
% & \mathbf{m}^\ell_{\mathcal{D}_\text{tt}^\text{unsafe} \rightarrow \mathcal{D}_\text{vl}^\text{blank}} = \mathtt{ActMean}^\ell (\mathcal{D}_\text{vl}^\text{blank}) - \mathtt{ActMean}^\ell (\mathcal{D}_\text{tt}^\text{unsafe}).
% \end{align*}
We compute cosine similarity between each modality-induced shift and the safety shift, $\cos{\langle \mathbf{m}^\ell, \mathbf{s}^\ell \rangle}$, to quantify the impact of visual modality on safety. 
% $\text{cos} \langle \textcolor{blue}{\bm{\alpha}^\ell}, \textcolor{orange}{\hat{\bm{\beta}}^\ell} \rangle$, which measures how closely the direction of $\textcolor{blue}{\bm{\alpha}^\ell}$ aligns with the safer direction $\textcolor{orange}{\hat{\bm{\beta}}^\ell}$. 
A larger value indicates a stronger activation shift toward the safe side due to visual input. Figure~\ref{fig:scatter-cosine} reports these cosine similarities, along with the Attack Success Rate (ASR) of the corresponding vision-language unsafe instruction sets. The results reveal a clear positive correlation between cosine similarity and ASR: when the modality-induced shift aligns more closely with the safety shift, the ASR increases, making it more likely for inputs to bypass the VLM’s safety mechanisms. Specifically, for ${\color{red}{\blacksquare}} \, \mathcal{D}_\text{vl}^\text{success}$ which achieves 100\% ASR, the corresponding modality shift $\mathbf{m}^\ell_{\mathcal{D}_\text{tt}^\text{success} \rightarrow \mathcal{D}_\text{vl}^\text{success}}$ exhibits the highest cosine similarity ($> 0.7$) with the safety shift; in contrast, ${\color{black}{\bullet}} \, \mathcal{D}_\text{vl}^\text{failure}$, with 0\% ASR, results in the lowest cosine similarity ($<0.2$). Additionally, ${\color{purple}{\bigstar}} \mathcal{D}_\text{vl}^\text{blank}$ shows a positive ASR and cosine similarity, indicating that even blank images -- despite their minimal semantic content -- can push activations toward the safe side, suggesting that such shift originates from the visual modality itself rather than specific image content.

\begin{figure}[t]    
    \begin{minipage}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/cos-scatter.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/cos-scatter-mini.pdf}
    \end{minipage}
    \vspace{-10pt}
    \caption{\textbf{Y-axis}: attack success rate of unsafe vision-language instruction sets ${\color{orange}{\blacktriangle}} \, \mathcal{D}_\text{vl}^\text{unsafe}, {\color{red}{\blacksquare}} \, \mathcal{D}_\text{vl}^\text{success}, {\color{black}{\bullet}} \, \mathcal{D}_\text{vl}^\text{failure}$ and ${\color{purple}{\bigstar}} \, \mathcal{D}_\text{vl}^\text{blank}$. \textbf{X-axis}: cosine similarity between the safety shift $\mathbf{s}^\ell_{\mathcal{D}_\text{tt}^\text{unsafe} \rightarrow \mathcal{D}_\text{tt}^\text{safe}}$ and each modality-induced shift $\mathbf{m}^\ell_{\mathcal{D}_\text{tt}^{(\cdot)} \rightarrow \mathcal{D}_\text{vl}^{(\cdot)}}$ derived on these sets.}
    \vspace{-25pt}
    \label{fig:scatter-cosine}
\end{figure}

\textbf{Remark.} 
These observations conclude that incorporating images into input instructions induces a significant shift in the activation space, referred to as the \emph{modality-induced shift}. This shift includes a component toward a “safer” direction, termed the \emph{safety-relevant shift}, which causes VLMs to mistakenly perceive unsafe instructions as safe, bypassing their safety mechanisms.