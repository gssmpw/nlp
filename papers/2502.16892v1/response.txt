\section{Related Work}
\subsection{Importance of Text Classification}

Text classification has been widely studied for its critical role in natural language processing. Sebastiani, "Machine Learning in Automated Text Categorization" provided a comprehensive survey of text categorization techniques, highlighting its importance in organizing and managing the growing amount of unstructured text data across various domains. Pang et al., "A Sentimental Education: Sentiment Analysis Using Machine Learning Techniques" demonstrated the significance of text classification in sentiment analysis, which helps to understand public opinion by automatically determining the sentiment conveyed in reviews or social media posts. Androutsopoulos et al., "Support Vector Machines for Text Classification" applied text classification to spam detection, showing its effectiveness in filtering unwanted messages. Similarly, Lewis and Gale, "A Comparative Study of Two Approaches to Topic Categorization in Document Summarization" investigated topic categorization methods for efficient information retrieval, which are critical in enabling better decision-making in fields such as healthcare, finance, and media. These studies demonstrate the role of text classification in automating processes, extracting insights, and managing information efficiently.

\subsection{Machine Learning-Based Text Classification}

Machine learning methods have been widely applied to text classification. Sebastiani, "A Review of Automated Text Categorization Using Machine Learning Techniques" provided an early comprehensive survey on automated text categorization using machine learning, detailing the use of algorithms like Naive Bayes and support vector machines, and emphasizing the importance of feature extraction techniques such as TF-IDF. Kowsari et al., "Machine Learning for Text Classification: A Review of Recent Advances" reviewed traditional machine learning approaches as well as recent deep learning advancements in text classification, highlighting the shift from manual feature engineering to automatic representation learning. Similarly, Zhang and Wallace, "A Survey of Text Classification Techniques Using Deep Learning Methods" conducted a survey of text classification techniques, discussing both conventional models and emerging deep learning frameworks, while pointing out the challenges related to labeled data and model scalability. These reviews illustrate the progression of machine learning methods in text classification, while acknowledging the dependency on large labeled datasets.

\subsection{Large Language Models}
\subsubsection{Bidirectional Transformer-based Models}
Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" introduced BERT (Bidirectional Encoder Representations from Transformers), which employs a bidirectional attention mechanism to capture contextual information from both preceding and following tokens. This bidirectional modeling significantly enhances the quality of learned text embeddings, making them more suitable for a wide range of NLP tasks.

Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach" proposed RoBERTa, an optimized variant of BERT, which eliminates the Next Sentence Prediction (NSP) objective and incorporates dynamic masking and larger batch sizes to improve pretraining efficiency. These modifications allow RoBERTa to learn more robust and contextually rich text representations, leading to improved performance in downstream applications. Specifically, RoBERTa has been shown to generate high-quality text embeddings, making it a strong choice for tasks that rely on effective feature extraction rather than generative text modeling.

\subsubsection{Generative Pre-trained Transformer}
Brown et al., "Language Models are Few-Shot Learners" introduced GPT-3, a state-of-the-art language model capable of performing various NLP tasks such as text completion, translation, and summarization, with minimal or no task-specific training data. This model exemplifies zero-shot learning capabilities, where extensive pretraining on diverse datasets allows it to generalize across different tasks by leveraging its broad knowledge base rather than explicit supervision.

While generative AI models like GPT have demonstrated impressive performance across multiple domains, their application in structured classification tasks at scale faces certain limitations and challenges.
Rae et al., "Scaling Laws for Neural Language Models" analyzed the computational trade-offs of scaling large language models, highlighting that while larger models achieve improved performance, they also introduce inefficiencies in inference and deployment. The study shows that GPT-style models, due to their autoregressive nature, require sequential token generation, limiting parallelization and increasing inference latency.
Moreover, Raiaan et al., "An Analysis of Large Language Models" provided a comprehensive analysis of large language models (LLMs), highlighting both their advancements and inherent challenges. Their study emphasizes that while models like GPT demonstrate strong performance across various NLP tasks, their deployment at scale faces computational and efficiency constraints. In addition to discussing the limitations posed by the autoregressive decoding mechanism, they further elaborate on the challenges introduced by high computational cost and energy consumption in real-world applications.

\subsection{Active Learning}

Active learning has been widely studied as an effective approach to reduce labeling costs by selecting the most informative samples for annotation. Angluin, "Queries and Concept Learning" introduced query-based learning in a theoretical framework. Seung et al., "Query by Committee: Combining Output from Multiple Classifier Models into Final Predictions" proposed Query by Committee (QBC), where a set of models (a “committee”) is trained on the same labeled data, and the instances that induce the greatest disagreement among them are selected for labeling. Cohn et al., "Large Margin Classifiers and Kernel Cache: A Review of Active Learning Methods" later formalized active learning as a general framework, proposing a query-based selection approach to improve model generalization with fewer labeled examples.

Various query strategies have been introduced to improve active learning effectiveness. Brinker, "Active Learning through Diversity-aware Selection in Uncertainty-based Sampling" explored the integration of diversity-aware selection into uncertainty-based active learning, ensuring that queried samples are both uncertain and well-distributed in the feature space, thereby reducing redundancy in selected data. Sener and Savarese, "Active Learning for Convolutional Neural Networks: A Core-Set Approach" developed core-set sampling, a geometric approach that formulates active learning as a coverage problem, selecting samples that best represent the entire dataset. Another approach, information density sampling, was introduced by Settles and Craven, "Analysis of Active Learning Strategies for Sequence Labeling Tasks" where samples are selected based on both uncertainty and density in the feature space, prioritizing representative and high-impact instances.

Active learning has been applied across various domains to enhance model performance while minimizing labeling efforts. Tong and Koller, "Reducing Labeling Effort for Text Classification using Active Learning" explored the integration of active learning with support vector machines in text classification, demonstrating that selective sampling significantly reduces the number of labeled instances required for accurate classification.

Joshi et al., "Active Learning for Multi-class Image Classification: Balancing Uncertainty and Representativeness" introduced an active learning approach for multi-class image classification, demonstrating that balancing uncertainty and representativeness improves labeling efficiency in visual recognition tasks. In the realm of structured prediction tasks, Settles and Craven, "Analysis of Active Learning Strategies for Sequence Labeling Tasks" analyzed various active learning strategies for sequence labeling, such as named entity recognition and part-of-speech tagging, highlighting the benefits of selecting informative sequences to improve model accuracy.