%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{float}
\usepackage[table]{xcolor}
\definecolor{simsortcolor}{rgb}{0.435,0.176,0.761} 

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\setlength{\abovecaptionskip}{20pt}
\setlength{\belowcaptionskip}{0pt}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{float}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{makecell}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{SimSort: A Powerful Framework for Spike Sorting by Large-Scale Electrophysiology Simulation}

\begin{document}

\twocolumn[
\icmltitle{SimSort: A Powerful Framework for Spike Sorting \\ by Large-Scale Electrophysiology Simulation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yimu Zhang}{fudan}
\icmlauthor{Dongqi Han}{msra}
\icmlauthor{Yansen Wang}{msra}
\icmlauthor{Yu Gu}{fudan}
\icmlauthor{Dongsheng Li}{msra}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{fudan}{Institutes of Brain Science, Fudan University, Shanghai, China}
\icmlaffiliation{msra}{Microsoft Research Asia, Shanghai, China}
\icmlcorrespondingauthor{Dongqi Han}{dongqihan@microsoft.com}
\icmlcorrespondingauthor{Yu Gu}{guyu\_@fudan.edu.cn}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{The work was done during Yimu Zhang's internship at Microsoft Research Asia.}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% \newcommand{\nocomments}[0]{} % uncomments this line to hide the comments

% \ifdefined\nocomments

\begin{abstract}
% Background
Spike sorting is an essential process in neural recording, which identifies and separates electrical signals from individual neurons recorded by electrodes in the brain, enabling researchers to study how specific neurons communicate and process information. 
% Existing challenges
Although there exist a number of spike sorting methods which have contributed to significant neuroscientific breakthroughs, many are heuristically designed, making it challenging to verify their correctness due to the difficulty of obtaining ground truth labels from real-world neural recordings.
% Our work
In this work, we explore a data-driven, deep learning-based approach. We begin by creating a large-scale dataset through electrophysiology simulations using biologically realistic computational models. We then present \textbf{SimSort}, a pretraining framework for spike sorting. 
% Key results
Remarkably, when trained on our simulated dataset, SimSort demonstrates strong zero-shot generalization to real-world spike sorting tasks, significantly outperforming existing methods. 
% Significance
Our findings underscore the potential of data-driven techniques to enhance the reliability and scalability of spike sorting in experimental neuroscience.

\end{abstract}

\section{Introduction}
Understanding the complex computations performed by the brain requires insight into the activity of individual neurons \cite{lewickiReviewMethodsSpike1998, buzsakiLargescaleRecordingNeuronal2004}, which is crucial for exploring how information is encoded, processed, and transmitted within neural circuits, as well as decoding the brain's functional dynamics \cite{quianquirogaExtractingInformationNeuronal2009, joshiDynamicSynchronizationHippocampal2023, sarkarAdvancedSpikeSorting2024}. Recent advances in neural recording technologies have made it possible to capture the activity of neurons across multiple regions of the brain with high spatial and temporal precision \cite{chungFullyAutomatedApproach2017, steinmetzNeuropixels20Miniaturized2021a, chungHighDensityLongLastingMultiregion2019, hongNovelElectrodeTechnologies2019}. Notably, extracting meaningful information from recording data relies on a critical step known as \textbf{spike sorting}.

Spike sorting is the process of extracting and identifying neural activity from extracellular recordings. It involves two main steps (Fig.~\ref{fig:pipeline}): \textbf{spike detection}, which extracts spike events from background noise, and \textbf{spike identification}, which assigns these detected spikes to individual neurons \cite{lefebvreRecentProgressMultielectrode2016}. Spike sorting is indispensable for transforming raw electrical signals into interpretable data that reveal the firing patterns of individual neurons. Accurate spike sorting is essential for linking neural activity to behavior, understanding the functional organization of neural circuits \cite{sibilleHighdensityElectrodeRecordings2022}, and uncovering mechanisms underlying various sensory and cognitive processes \cite{liuDecodingCognitionSpontaneous2022}. Furthermore, its role extends to translational applications, such as improving neural prosthetics and developing closed-loop BCIs, where spike sorting is critical for achieving precise neural decoding and control \cite{park128ChannelFPGABasedRealTime2017, hanLiveDemonstrationEfficient2024}.

For a long time, spike sorting predominantly relied on heuristic statistics and machine learning approaches \cite{lewickiReviewMethodsSpike1998, quirogaUnsupervisedSpikeDetection2004, rossantSpikeSortingLarge2016, pachitariuFastAccurateSpike2016, chungFullyAutomatedApproach2017, hilgenUnsupervisedSpikeSorting2017, pachitariuSpikeSortingKilosort42024}. While these methods have promoted neuroscience research, they exhibit several key limitations. First, their sorting results are sensitive to parameter settings and post-processing, which depend on the experimenter's expertise and must be customized for each dataset. Moreover, these approaches lack a data-driven foundation, limiting their scalability and adaptability across diverse experimental settings, particularly in low signal-to-noise ratio (SNR) and high-variability scenarios, where their performance declines significantly. Recently, deep learning-based methods, such as YASS \cite{leeYASSAnotherSpike2017} and CEED \cite{vishnubhotlaRobustGeneralizableRepresentations2023}, have attempted to improve spike sorting by adopting data-driven approaches to enhance detection and clustering accuracy. However, their generalizability and practical use are hindered by the limited training datasets.

To address these limitations, we highlight the importance of using massive training data with ground-truth annotations to achieve more reliable and robust spike sorting. Furthermore, we aim to design a deep learning framework optimized for spike sorting and pretrain models for practical usage.

Therefore, in this work, we first generated a large-scale labeled dataset to address the scarcity of ground-truth data for spike sorting. Building on this, we present \textbf{SimSort}, a framework that utilizes data-driven approaches to enable fully automated spike sorting. By pretraining a spike detection model on the large-scale dataset, SimSort achieved significant improvements in detection accuracy and adaptability compared to commonly used threshold-based methods. Additionally, SimSort incorporated contrastive learning to enhance waveform feature representations, ensuring robustness against noise. We evaluated SimSort through zero-shot experiments on publicly available datasets without fine-tuning, demonstrating its state-of-the-art performance in spike sorting tasks compared to existing methods.

The key contributions of the SimSort framework include: \\
\textbf{1)} A publicly available, large-scale labeled dataset to address the scarcity of ground-truth data, enabling the development and evaluation of advanced spike sorting methods. \\
\textbf{2)} A pretraining paradigm for spike sorting with a large-scale simulation dataset. The results, for the first time, demonstrate successful zero-shot transfer from simulated to real-world spike sorting tasks. \\
\textbf{3)} A practical and robust tool designed to simplify spike sorting for neuroscientists, offering full automation without reliance on manually defined parameters.
%\footnote{See the supplementary video for a demo usage of SimSort. The source code is included in the supplementary material. The datasets will be made available upon acceptance.}.

\begin{figure*}[th]
    \centering
    \includegraphics[width=\textwidth]{figures/figure1-spike_sorting_pipeline.pdf}
    \caption{
    Illustration of the \textbf{spike sorting} pipeline, which consists of two main steps:\textbf{ spike detection} and \textbf{spike identification}. For spike detection, typical spike sorting algorithms (left) utilize threshold-based detector relies on fixed voltage thresholds on each channel, and use non-learning method like performing PCA-based clustering on concatenated waveforms for spike identification. Those approaches are sensitive to noise and require manual parameter tuning. In contrast, our proposed framework SimSort (right) use a neural network-based detector replaces the thresholding method for spike detection, enhancing robustness and generalization. For spike identification, the feature embeddings of multi-channel waveforms learned from contrastive learning to improve clustering accuracy.
    }
    \label{fig:pipeline}
    \vspace{-3mm}
\end{figure*}

\section{Preliminaries}  
The goal of \textbf{spike sorting} is to extract single-neuron spiking activity from extracellular recordings, where the activities of an unknown number of neurons are mixed together. Mathematically, spike sorting is a blind source separation problem \cite{buccinoSpikeSortingNew2022} and can be formally defined as follows: Let \( \mathbf{V} \in \mathbb{R}^{T \times C} \) represent the extracellular recording (voltage) across \( C \) electrode channels over \( T \) time points. The goal is to infer a set of spike times \( \{t_k\}_{k=1}^K \) and corresponding neuron labels \( \{y_k\}_{k=1}^K \), where \( y_k \in \{1, \dots, N\} \) represents the neuron identity, and \( N \) is the total number of detected neurons.

\section{Related Work}
Early spike sorting pipelines typically used thresholds to detect spike events, followed by dimensionality reduction (e.g., PCA or $t$-SNE) and clustering \cite{FRS1901LIIIOL, maatenVisualizingDataUsing2008, veerabhadrappaCompatibilityEvaluationClustering2020}. Although this pipeline was straightforward, it often suffered from noise susceptibility, inaccurate detection of low SNR events, and reliance on manual parameter tuning.

Deep learning approaches have been adopted to replace or enhance certain steps in traditional pipelines, aiming to improve robustness, automation, and adaptability to diverse experimental conditions. Autoencoders have been explored as a method for dimensionality reduction \cite{baldiAutoencodersUnsupervisedLearning2012, wuLearningSortFewshot2019, eomDeeplearnedSpikeRepresentations2021}. YASS \cite{leeYASSAnotherSpike2017} employed a convolutional neural network for spike detection and waveform cleaning, thereby mitigating clustering errors caused by distorted waveforms. CEED \cite{vishnubhotlaRobustGeneralizableRepresentations2023} applied a contrastive learning framework to enforce invariances to amplitude fluctuations, noise, and channel subset changes in the extracted embeddings. However, these methods were limited by their reliance on restricted training datasets, potentially reducing their generalization across diverse experimental conditions.

In particular, YASS depended on high-quality prior training data and assumed a consistent experimental setup with validated sorting results, limiting its utility in scenarios where training data were scarce or recording conditions varied significantly. 

Similarly, CEED had its limitations: (1) its invariance assumptions may not generalize to datasets with unaccounted variability; (2) it relied on KiloSort2.5-processed data, potentially inheriting inaccuracies from these analyses; (3) its training and testing datasets were narrowly scoped, originating from similar experimental conditions, which may constrain broader applicability; and (4) it did not optimize spike detection or provide a complete spike sorting pipeline.

Given these limitations, we propose a data-driven approach to develop a fully automated spike sorting pipeline that does not rely on manually defined parameters, demonstrating improved generalization across diverse datasets and recording conditions.

\section{Methods}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/figure2-large_scale_simulated_data_with_table.pdf}
    \caption{
        Overview of the large-scale simulated extracellular dataset generation process.
        (a) Multi-compartment neuron models simulate detailed neuronal morphologies and electrophysiological properties, incorporating realistic ion channel dynamics. 
        (b) Noise generated by stochastic process is injected into the somatic compartment to induce stochastic intracellular action potential firing, and extracellular signals are recorded using virtual electrodes placed near the neurons in a simulated environment. 
        (c) The resulting multi-channel extracellular recordings capture diverse and realistic neural activity.
    }
    \label{fig:simulation}
    \vspace{-3mm}
\end{figure*}

\subsection{Dataset Creation}
\label{subsec: Data Generation}
To overcome the challenges of insufficient labeled data, we generated a simulated dataset designed to support robust model training and evaluation.
\vspace{-3mm}
\paragraph{Simulation}
Our simulated dataset was generated with multi-compartmental neuron models from the Blue Brain Project (BBP) Neocortical Microcircuit Models Database \cite{hayModelsNeocorticalLayer2011}, and it contained 206 neuron models from the juvenile rat somatosensory cortex (layers 1-6), covering 30 distinct neuronal types. 

These models provided a comprehensive representation of neuronal diversity with precise morphological and ion channel characteristics, enabling realistic biological simulations of electrophysiology responses. Based on these neuron models, we conducted intracellular simulations to compute neuronal transmembrane currents, which were subsequently used to model extracellular potentials. Below, we detail these two stages.

\textbf{Intracellular Simulation}. Intracellular electrophysiological activity was simulated using the NEURON package \cite{hinesNEURONSimulationEnvironment1997, hinesNeuronToolNeuroscientists2001}. Pink noise, scaled to the rheobase (the minimum current needed to elicit an action potential in a neuron), was injected into each neuron's soma to evoke stochastic action potentials, simulating biologically realistic background noise. The temporal resolution was set to 0.1 ms, and simulations spanned a total duration of 600 seconds. Neurons were initialized with a resting membrane potential of -70 mV and maintained at a physiological temperature of 34°C. Synaptic mechanisms and biophysical properties were dynamically compiled and loaded to ensure compatibility with the NEURON simulation environment.

For each neuron, a multi-compartmental model was used to calculate transmembrane currents \cite{lindenLFPyToolBiophysical2014, hagenViSAPyPythonTool2015, hagenMultimodalModelingNeural2018, buccinoMEArecFastCustomizable2021}. The neuron was divided into compartments, each described as an equivalent electrical circuit. The dynamics of the membrane potential in each compartment $n$ were governed by Kirchhoff's current law \cite{holmesPassiveCableModeling2009}, where the sum of all currents entering or leaving a node must equal zero. Considering that extracellular potential changed much slower than ion channel dynamics, we assumed it constant. The temporal evolution of the membrane potential $V_n$ in compartment $n$ was given by
$
g_{n,n+1}(V_{n+1} - V_n) - g_{n-1,n}(V_n - V_{n-1}) 
= C_n \frac{dV_n}{dt} + \sum_j I_n^{(j)},
$
where $g_{n,n+1}$ and $g_{n-1,n}$ represented the conductances between neighboring compartments, $C_n$ was the membrane capacitance, and $\sum_j I_n^{(j)}$ accounted for ionic currents and any externally applied currents. 

These transmembrane currents served as the source for extracellular potential modeling in the next simulation stage.

\textbf{Extracellular Simulation}. Extracellular potentials were modeled using the volume conductor theory, which links transmembrane currents to extracellular potentials recorded at electrode sites \cite{bretteHandbookNeuralActivity2012, buzsakiOriginExtracellularFields2012, einevollModellingAnalysisLocal2013}. In each simulation trial, five neurons were randomly selected from the available models and positioned within a 100 × 100 × 100 $\mu$m$^3$ cubic space. These neurons consisted of different types to represent the diversity of cortical microcircuits. A virtual tetrode electrode with four recording sites was placed randomly within the same space to capture extracellular potentials.

The extracellular potential at a given electrode site $\mathbf{r}_e$ was computed as the sum of contributions from all neuronal segments within the simulation space:
$
\phi(\mathbf{r}_e, t) = \frac{1}{4\pi\sigma} \sum_{n=1}^{N} \sum_{m=1}^{M_n} \frac{I_{n,m}(t)}{|\mathbf{r}_e - \mathbf{r}_{n,m}|},
$
where $I_{n,m}(t)$ was the transmembrane current of the $m$-th compartment of the $n$-th neuron, $\mathbf{r}_{n,m}$ was its position, $\sigma$ is the extracellular medium's conductivity, and $|\mathbf{r}_e - \mathbf{r}_{n,m}|$ was the Euclidean distance between the segment and the electrode. Here, $N$ was the total number of neurons, and $M_n$ was the number of compartments in the $n$-th neuron. A reference electrode far from the source was assumed, setting $\phi = 0$ at infinity.

\paragraph{Dataset Preparation}
\label{subsec: Dataset Preparation}
We generated a large-scale dataset consisting of 8192 simulated recording trials, representing continuous neuronal activities from more than 40,000 individual neurons. Each trial was configured with randomized neurons and electrode positions, along with diverse combinations of neuron types, to ensure high variability and diversity. Intracellular spike timestamps were recorded as ground-truth for extracellular spikes, providing precise annotations for model training and evaluation.

The dataset was structured into two primary subsets: 

\textbf{Continuous Signal Dataset}: This subset comprised full-length extracellular recordings across all electrode channels. It was designed for training spike detection models and evaluating the overall performance of spike sorting pipelines.

\textbf{Spike Waveform Dataset}: This subset contained spike waveforms extracted from each ground-truth units. It was used for training and evaluating the spike identification model.

We used signals simulated from BBP layer 1–5 neuron models for training and validation, reserving signals from BBP layer 6 neuron models exclusively for evaluation, ensured that the test set comprises previously unseen neuron types and configurations, allowing us to effectively assess the model's generalization capabilities.

\subsection{Spike Detection}
\label{subsec: method-Spike detection}
We formulate the spike detection task as identifying the temporal segments $\{t_k\}$ corresponding to putative neural spikes from the raw input $\mathbf{V} \in \mathbb{R}^{T \times C}$, where $T$ represents the number of time points and $C$ denotes the number of electrode channels. The raw signal $\textbf{V}$ first underwent two preprocessing steps: bandpass filtering and spatial whitening. Then, the processed signal was subsequently provided as input to the spike detection model.
\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/figure3-detection_model_detail.pdf}
    \vspace{-5mm}
    \caption{Diagram of the spike detection model in SimSort.
    % The detection model processes continuous extracellular signals with ground-truth spike annotations. 
    % The input signals undergo random augmentations and are passed through a Transformer architecture with positional encoding (PE) to account for temporal information. 
    % The model outputs logits through a fully connected (FC) layer. During inference, logits are converted to probabilities using a sigmoid activation function.
    }
    \label{fig:detection_model}
\end{figure}

We utilized a Transformer-based architecture \cite{vaswaniAttentionAllYou2017} for spike detection, trained on the simulated continuous signal dataset detailed in Sec.~\ref{subsec: Dataset Preparation}. To improve robustness and generalization, data augmentation was integrated into the training process, referring to Fig.~\ref{fig:detection_model}. We employed a binary cross-entropy loss for the spike detection model:
% $
% \mathcal{L}_{\text{BCE}} = -\frac{1}{T}\sum_{t=1}^T \Big(w_p\cdot  y_t \log\hat{y}_t +  (1-y_t)\log(1-\hat{y}_t)  \Big),
% $
\begin{equation}
\mathcal{L}_{\text{BCE}} = -\frac{1}{T}\sum_{t=1}^T \Big(w_p\cdot  y_t \log\hat{y}_t +  (1-y_t)\log(1-\hat{y}_t)  \Big),
\end{equation}
where the binary label $y_t$ denoted spike events, \(\hat{y}_t\) was the predicted probability, \(T\) was the length of the trial, and $w_p$ was a hyperparameter to reweigh the positive predictions given the general scarcity of spikes.

\paragraph{Data Augmentation}
\label{subsec: Data Augmentation}
To improve robustness and generalization in spike detection, data augmentation was applied to each input signal segment $V' \in \mathbb{R}^{T \times C}$ (Fig.~\ref{fig:detection_model}). A subset of channels $C' \subseteq \{1, 2, \dots, C\}$ was randomly selected with a probability $p$. Various augmentations, such as adding noise, amplitude scaling, and temporal jitter, were applied to enhance the diversity of training data. The detailed implementation of these augmentations is provided in Appendix~\ref{appendix: A}.

\subsection{Spike Identification}
\label{subsec: method-Spike Identification}
We formulate the spike identification task as embedding each identified spike waveform $X_i \in \mathbb{R}^{L \times C}$, the voltage trace (waveform) of spike $i$, into a latent space to learn robust representations using contrastive learning. Then the learned embeddings are clustered into several groups, indicating putative neurons.

The spike identification model was trained on the simulated spike waveform dataset detailed in Sec.~\ref{subsec: Dataset Preparation}. The model consisted of several key components, as illustrated in Fig.~\ref{fig:identification_model} and described as follows.
\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/figure4-identification_model.pdf}
    \vspace{-5mm}
    \caption{
    Diagram of the spike identification model in SimSort.% spike waveforms are grouped into triplets consisting of an anchor (A), a positive (P), and a negative (N), and augmented with random transformations then denoised and passed through an GRU encoder to extract feature representations. Contrastive learning minimizes the distance between A and P while maximizing the distance between A and N in the embedding space. The learned embeddings are subsequently used for clustering to identify neuron.
    %\yansen{I feel the content of the caption can be moved to the main content. So should Figure 3.}
    }
    \label{fig:identification_model}
\end{figure}
\vspace{-5mm}

\paragraph{Contrastive Learning}
We leveraged contrastive learning to learn representations that capture the relationships between spike waveforms (Fig.~\ref{fig:identification_model}). Contrastive learning sought to map neural data into an embedding space where related examples were positioned nearby, while unrelated examples were placed further apart \cite{dorkenwaldMultilayeredMapsNeuropil2023, yuVivoCelltypeBrain2024}. For each waveform $X_i$, we constructed a triplet by selecting $X_i^+$ with the same label and $X_i^-$ with a different label. The objective was \cite{schroffFaceNetUnifiedEmbedding2015}:
\begin{equation}
\begin{split}
\mathcal{L}_{\text{triplet}} = \max \big( 0, 
& \| f(X_i) - f(X_i^+)\|_2^2 \\
& - \| f(X_i) - f(X_i^-)\|_2^2 + \alpha \big),
\end{split}
\end{equation}
where $f(X_i)$ denoted the embedding of $X_i$, and $\alpha$ was a margin enforcing separation between positive-negative pairs.

Additionally, we uniformly applied data augmentation to each triplet waveforms. The augmentation methods followed those in Sec.~\ref{subsec: Data Augmentation}, with adjusted parameters (Appendix~\ref{appendix: A}) for better waveform representation, ensuring robustness to noise, amplitude variations, and temporal jitter.

\paragraph{Denoising}
The augmented waveform $X_i' \in \mathbb{R}^{L \times C}$ then underwent a denoising step using Singular Value Decomposition (SVD) \cite{eckartApproximationOneMatrix1936, cunninghamDimensionalityReductionLargescale2014}. The waveform was first reshaped and decomposed as $X_i' = U \Sigma V^\top$, where $U$, $\Sigma$, and $V$ represented the singular vectors and singular values. The first $k$ components were retained to reconstruct the waveform.

\paragraph{Encoder}
The denoised waveform $X_{i,\text{recon}}'$ was fed into a Gated Recurrent Unit (GRU) encoder \cite{choLearningPhraseRepresentations2014} (Fig.~\ref{fig:identification_model}), which processed the input sequentially. The final hidden state $h_T$ served as the learned representation $f(X_i)$, encapsulating the spatiotemporal features of the waveform for subsequent clustering tasks.

\paragraph{Clustering}
After obtaining the waveform representations, we applied dimensionality reduction using UMAP \cite{mcinnesUMAPUniformManifold2020} before performing clustering (Fig.~\ref{fig:identification_model}). We used two clustering approaches: a parametric algorithm, Gaussian Mixture Model (GMM) \cite{dempsterMaximumLikelihoodIncomplete1977}, and a non-parametric algorithm, Mean Shift \cite{chengMeanShiftMode1995}.

\subsection{Overall Spike Sorting Pipeline}
We established a cascaded spike sorting pipeline (Fig.~\ref{fig:pipeline}). During inference, raw multi-channel signals were processed through spike detection, waveform extraction, embedding generation, and clustering, yielding distinct neuronal units and their spike timestamps.

\section{Results}

With the large-scale dataset and a pretraining framework, it is natural to ask the following research questions (RQ).\\
\textbf{RQ1:} Can the model trained on our simulated dataset generalize to real-world spike sorting tasks? \\
\textbf{RQ2:} If the model performs well, what are the crucial underlying components? \\
\textbf{RQ3:} How important is data size?

In this section, we aimed to address these RQs with comprehensive validation and analysis. We started by presenting a detailed description of the test dataset and the experiment setup in Sec.~\ref{subsec: Experimental Setup}. Next, we showed SimSort's promising spike sorting results on both simulated and real-world data-based benchmarks in Sec.~\ref{subsec: Spike sorting results} (RQ1). Then, we explained the advantages of SimSort with analysis on spike detection and identification, respectively, in Sec.~\ref{subsec: Detailed_experiment_analysis} (RQ2). Finally, we investigated the impact of data size in Sec.~\ref{subsec: scaling law} (RQ3).  

\subsection{Benchmarks}
\label{subsec: Experimental Setup}

\textbf{BBP L6 dataset (simulation)}. This benchmark dataset comprised 20 simulated trials, generated as stated in Sec.~\ref{subsec: Data Generation}, using BBP layer 6 neuron models. It provided 5 ground-truth units per recording for evaluating spike sorting performance.

\textbf{Hybrid dataset}. We prepared this dataset using recordings from Spikeforest \cite{maglandSpikeForestReproducibleWebfacing2020}, which combined real neuronal waveforms with background noise. It included 9 static and 9 drift tetrode recordings, each containing 10–15 ground-truth units with a minimum SNR of 3. Details of the dataset preparation are provided in Appendix~\ref{appendix: test datasets}.

\textbf{WaveClus dataset}. This dataset consisted of four subsets (Easy1, Easy2, Difficult1, and Difficult2), varying noise levels from 0.05 to 0.4, with 3 ground-truth units per recording \cite{martinezRealisticSimulationExtracellular2009}. The original recordings were single-channel data of real neuronal waveforms mixed with background noise. To adapt them for tetrode analysis and increase the overall difficulty, we processed the recordings as described in the Appendix~\ref{appendix: test datasets}.

\textbf{IBL Neuropixels dataset}. This dataset was derived from the CortexLab KS046 Neuropixels  recordings released by International Brain Laboratory (IBL). To compare spike identification performance with CEED \cite{vishnubhotlaRobustGeneralizableRepresentations2023}, we followed the CEED paper's method, extracting waveforms from 400 units. Fifty test sets were generated with random seeds, each containing 10 units.

\textbf{Evaluation Metrics}. To evaluate the performance of the overall spike sorting and spike detection tasks, we used Accuracy, Recall, and Precision as the evaluation metrics. Note that these metrics are specifically defined for the spike sorting context, where they measure the alignment between detected spikes and ground truth spike events, rather than their conventional definitions in classification tasks.
For the spike identification task, we used the Adjusted Rand Index (ARI) \cite{hubert1985comparing} to measure clustering accuracy relative to the ground truth. 

The detailed information of benchmark datasets, hyper-parameters of models, and formulation of the evaluation metrics can be found in Appendix~\ref{appendix: A}.

\subsection{Spike Sorting Results}
\label{subsec: Spike sorting results}
\paragraph{Simulated dataset}
We first examined Simsort on a simulated test set (BBP L6). In Table~\ref{tab:bbp_L6_sorting}, we evaluated SimSort alongside widely used algorithms, including KiloSort \cite{pachitariuFastAccurateSpike2016}, KiloSort2, MountainSort4 \cite{chungFullyAutomatedApproach2017}, and MountainSort5, implemented via the SpikeInterface pipeline \cite{buccinoSpikeInterfaceUnifiedFramework2020}, on the simulated BBP L6 dataset. SimSort achieved the best performance across all metrics, demonstrating its robustness and strong zero-shot generalization ability under idealized, simulated conditions.

\begin{table}[h]
    \centering
    \vspace{-3mm}
    \caption{
    \label{tab:bbp_L6_sorting}
        Spike sorting results on \textbf{BBP L6 dataset}.
        The results for other methods were obtained through SpikeInterface \cite{buccinoSpikeInterfaceUnifiedFramework2020}.
        Values are presented as mean $\pm$ S.E.M.  
    }
    \vspace{1mm}
    \resizebox{0.85\columnwidth}{!}
    {
    \begin{tabular}{l|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{BBP L6
    \small(20 trials)}} \\
    & \textbf{Accuracy} & \textbf{Recall} & \textbf{Precision} \\
    
    \midrule
    KiloSort       & 0.49 $\pm$ 0.05 & 0.51 $\pm$ 0.05 & 0.53 $\pm$ 0.05 \\
    KiloSort2      & 0.51 $\pm$ 0.06 & 0.53 $\pm$ 0.06 & 0.53 $\pm$ 0.06 \\
    MountainSort4  & 0.84 $\pm$ 0.03 & 0.84 $\pm$ 0.03 & \textbf{0.93} $\pm$ \textbf{0.03} \\
    MountainSort5  & 0.66 $\pm$ 0.06 & 0.68 $\pm$ 0.06 & 0.79 $\pm$ 0.07 \\
    \rowcolor{simsortcolor!20} SimSort & \textbf{0.85} $\pm$ \textbf{0.02} & \textbf{0.90} $\pm$ \textbf{0.01} & \textbf{0.93} $\pm$ \textbf{0.02} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
\end{table}
\begin{table*}[th]
    \centering
    \caption{
    \label{tab:hybrid_sorting_table}
        Spike sorting results on \textbf{Hybrid dataset}.
        Data for other methods were sourced from SpikeForest \cite{maglandSpikeForestReproducibleWebfacing2020}. 
        Values are mean $\pm$ S.E.M. Best results are in \textbf{bold}. 
        Note that SimSort did not use this dataset for training, indicating a zero-shot generalization.
    }
    \vspace{1mm}
    \footnotesize
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{l|ccc|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c|}{\textbf{Hybrid-static \small(SNR$>$3, 9 recordings)}} 
                                      & \multicolumn{3}{c}{\textbf{Hybrid-drift \small(SNR$>$3, 9 recordings)}} \\
    %\cmidrule(lr){2-4} \cmidrule(lr){5-7}
    & \textbf{Accuracy} & \textbf{Recall} & \textbf{Precision}
    & \textbf{Accuracy} & \textbf{Recall} & \textbf{Precision} \\
    \midrule
    
    HerdingSpikes2  & 0.35 $\pm$ 0.01 & 0.44 $\pm$ 0.02 & 0.53 $\pm$ 0.01
                    & 0.29 $\pm$ 0.01 & 0.37 $\pm$ 0.02 & 0.48 $\pm$ 0.02 \\
    IronClust       & 0.57 $\pm$ 0.04 & \textbf{0.81 $\pm$ 0.01} & 0.60 $\pm$ 0.04
                    & 0.54 $\pm$ 0.03 & 0.71 $\pm$ 0.02 & 0.65 $\pm$ 0.03 \\
    JRClust         & 0.47 $\pm$ 0.04 & 0.63 $\pm$ 0.02 & 0.59 $\pm$ 0.03
                    & 0.35 $\pm$ 0.03 & 0.48 $\pm$ 0.03 & 0.57 $\pm$ 0.02 \\
    KiloSort        & 0.60 $\pm$ 0.02 & 0.65 $\pm$ 0.02 & 0.72 $\pm$ 0.02
                    & 0.51 $\pm$ 0.02 & 0.62 $\pm$ 0.01 & \textbf{0.72} $\pm$ \textbf{0.03} \\
    KiloSort2       & 0.39 $\pm$ 0.03 & 0.37 $\pm$ 0.03 & 0.51 $\pm$ 0.03
                    & 0.30 $\pm$ 0.02 & 0.31 $\pm$ 0.02 & 0.57 $\pm$ 0.04 \\
    MountainSort4   & 0.59 $\pm$ 0.02 & 0.73 $\pm$ 0.01 & 0.73 $\pm$ 0.03
                    & 0.36 $\pm$ 0.02 & 0.57 $\pm$ 0.02 & 0.61 $\pm$ 0.03 \\
    SpykingCircus   & 0.57 $\pm$ 0.01 & 0.63 $\pm$ 0.01 & 0.75 $\pm$ 0.03
                    & 0.48 $\pm$ 0.02 & 0.55 $\pm$ 0.02 & 0.68 $\pm$ 0.03 \\
    Tridesclous     & 0.54 $\pm$ 0.03 & 0.66 $\pm$ 0.02 & 0.59 $\pm$ 0.04
                    & 0.37 $\pm$ 0.02 & 0.52 $\pm$ 0.03 & 0.55 $\pm$ 0.04 \\
    \rowcolor{simsortcolor!20} SimSort& \textbf{0.62} $\pm$ \textbf{0.04} & 0.68 $\pm$ 0.04 & \textbf{0.77} $\pm$ \textbf{0.03}
                                       & \textbf{0.56} $\pm$ \textbf{0.03} & \textbf{0.63} $\pm$ \textbf{0.03} & 0.69 $\pm$ 0.03\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-4mm}
\end{table*}
\begin{table*}[h]
    \centering
    \caption{
    \label{tab:waveclus_sorting_table}
        Spike sorting results on \textbf{WaveClus dataset}.
        The results for other methods were obtained through SpikeInterface \cite{buccinoSpikeInterfaceUnifiedFramework2020}.
        Values are mean $\pm$ S.E.M.  
        Note that SimSort did not use this dataset for training, indicating a zero-shot generalization. 
    }
    \vspace{1mm}
    \footnotesize
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{l|ccc|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c|}{\textbf{Easy \small(12 recordings)}} 
                                      & \multicolumn{3}{c}{\textbf{Difficult \small (8 recordings)}} \\
    %\cmidrule(lr){2-4} \cmidrule(lr){5-7}
    & \textbf{Accuracy} & \textbf{Recall} & \textbf{Precision}
    & \textbf{Accuracy} & \textbf{Recall} & \textbf{Precision} \\
    \midrule
    
    KiloSort        & 0.54 $\pm$ 0.12 & 0.46 $\pm$ 0.15 & 0.62 $\pm$ 0.13
                    & 0.17 $\pm$ 0.07 & 0.23 $\pm$ 0.10 & 0.18 $\pm$ 0.07 \\
    KiloSort2       & 0.61 $\pm$ 0.09 & 0.66 $\pm$ 0.09 & 0.67 $\pm$ 0.09
                    & 0.10 $\pm$ 0.07 & 0.12 $\pm$ 0.09 & 0.10 $\pm$ 0.07 \\
    MountainSort4   & 0.73 $\pm$ 0.08 & 0.79 $\pm$ 0.07 & 0.79 $\pm$ 0.08
                    & 0.48 $\pm$ 0.16 & 0.49 $\pm$ 0.16 & 0.52 $\pm$ 0.17 \\
    MountainSort5   & 0.71 $\pm$ 0.09 & 0.74 $\pm$ 0.08 & 0.79 $\pm$ 0.10
                    & 0.42 $\pm$ 0.15 & 0.46 $\pm$ 0.15 & 0.46 $\pm$ 0.16 \\
                    
    \rowcolor{simsortcolor!20} SimSort & \textbf{0.75} $\pm$ \textbf{0.06} & \textbf{0.84} $\pm$ \textbf{0.04} & \textbf{0.85} $\pm$ \textbf{0.05}
                                       & \textbf{0.71} $\pm$ \textbf{0.10} & \textbf{0.81} $\pm$ \textbf{0.07} & \textbf{0.81} $\pm$ \textbf{0.09} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-4mm}
\end{table*}

\paragraph{Real-world data-based dataset}
Given SimSort’s strong performance on simulated data, we extended our evaluation to real-world data-based datasets to further test its generalizability and robustness (RQ1). On the Hybrid dataset, we compared SimSort against several algorithms via SpikeForest, including HerdingSpikes2 \cite{hilgenUnsupervisedSpikeSorting2017}, IronClust, JRClust, KiloSort \cite{pachitariuFastAccurateSpike2016}, KiloSort2, MountainSort4 \cite{chungFullyAutomatedApproach2017}, SpykingCircus, and Tridesclous. The results are summarized in Table~\ref{tab:hybrid_sorting_table} and Fig.~\ref{fig: hybrid_waveclus-acc}. On this dataset, SimSort demonstrated the highest Accuracy and Precision in static recordings and achieved the best Accuracy and Recall under drift conditions, although it slightly trailed KiloSort in Precision.

For the WaveClus dataset, we used the SpikeInterface pipeline to run KiloSort, KiloSort2, MountainSort4, and MountainSort5. Results are provided in Table~\ref{tab:waveclus_sorting_table} and Fig.~\ref{fig: hybrid_waveclus-acc}. SimSort performed reliably across both easy and difficult subsets, showing a pronounced advantage on the challenging ``difficult'' set. This performance was likely due to its ability to handle the unique challenges of this subset, such as lower SNRs and reduced inter-class waveform variability, which resulted in significant overlaps between units. These results underscored SimSort's effectiveness in tackling noisy and complex scenarios.

\subsection{Detailed Analysis}
To explain why our framework achieved higher overall spike sorting accuracy (RQ2), we dissected SimSort and analyzed each component, i.e., spike detection and spike identification models. Furthermore, we presented some case studies of the latent embedding of spikes from multiple neurons.

\label{subsec: Detailed_experiment_analysis}
\vspace{-3mm}
\paragraph{Spike Detection}
\label{sec: comparasion of detection}
We first compared our detection model to a commonly used threshold-based detection method. Details of the threshold-based detector are provided in Appendix~\ref{appendix: A}. The results are shown in Table~\ref{tab:hybrid_detection_table}. SimSort's spike detector consistently outperformed the threshold-based method across all metrics on the Hybrid dataset. In the static condition, SimSort improved accuracy by $\sim$18$\%$, recall by $\sim$18$\%$, and precision by $\sim$1$\%$. Under the drift condition, the improvements were $\sim$13$\%$ in accuracy, $\sim$17$\%$ in recall, and $\sim$1$\%$ in precision. It is important to note that the performance of the Threshold detector was highly sensitive to the choice of threshold, with results varying significantly across different settings. The results shown here were obtained under the best-performing thresholds. In contrast, SimSort achieved its superior performance without manual tuning, demonstrating its robustness and adaptability to varying recording conditions. These results indicated the effectiveness of our detection model, providing a solid foundation for the overall performance of SimSort in spike sorting tasks.

\begin{table}[h]
    \centering
    \caption{
        \label{tab:hybrid_detection_table}
        Spike detection results on \textbf{Hybrid dataset}. 
        Values are mean $\pm$ S.E.M. 
        The threshold detector used the best-performing threshold by grid search.
    }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc} 
    \toprule
    \textbf{Dataset} & \textbf{Method} & \textbf{Accuracy} & \textbf{Recall} & \textbf{Precision} \\
    \midrule
        & Threshold detector           & 0.61$\pm$0.03 & 0.71$\pm$0.02 & 0.81$\pm$0.02 \\
        \rowcolor{simsortcolor!20}
        \multirow{2}{*}[12pt]{Static} & SimSort's detector & \textbf{0.72$\pm$0.03} & \textbf{0.84$\pm$0.02} & \textbf{0.82$\pm$0.02} \\
    \midrule
        & Threshold detector           & 0.60$\pm$0.03 & 0.70$\pm$0.03 & 0.80$\pm$0.02 \\
        \rowcolor{simsortcolor!20}
        \multirow{2}{*}[12pt]{Drift} & SimSort's detector & \textbf{0.68}$\pm$\textbf{0.03} & \textbf{0.82}$\pm$\textbf{0.02} & \textbf{0.81}$\pm$\textbf{0.02} \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\begin{table}[h]
    \vspace{-3mm}
    \centering
    \caption{
    \label{tab:hybrid_identification_table}
        Spike identification on \textbf{Hybrid datasets}. 
        ARI values are presented as mean $\pm$ standard deviation.
        All results were averaged across 20 repeats of GMM.
    }
    \vspace{1mm}
    \resizebox{0.6\linewidth}{!}{
    \begin{tabular}{lcc}
    \toprule
    \textbf{Methods}      & \textbf{Hybrid-static}   & \textbf{Hybrid-drift} \\
    \midrule
    
    PCA     & 0.79 $\pm$ 0.04 
            & 0.74 $\pm$ 0.04\\
    $t$-SNE   & 0.88 $\pm$ 0.02
            & 0.85 $\pm$ 0.02\\
    UMAP    & 0.88 $\pm$ 0.02
            & 0.85 $\pm$ 0.01\\
    
    \rowcolor{simsortcolor!20} Ours & \textbf{0.91} $\pm$ \textbf{0.02}
                                       & \textbf{0.89} $\pm$ \textbf{0.03}\\
    \bottomrule                                   
    \end{tabular}
    }
    \vspace{-3mm}
\end{table}

\vspace{-3mm}
\paragraph{Spike Identification}
After detecting spikes, our spike identification model extracted waveform representations in the latent space, enabling subsequent clustering. To effectively demonstrate the superiority of our identification model, we assembled a waveform dataset by gathering waveform data from each neuron in every hybrid recording. Using this dataset, we compared the 2D UMAP embedding of our model's inferred representations with features extracted directly from waveforms using widely adopted dimensionality reduction techniques, including \textbf{PCA} \cite{FRS1901LIIIOL}, $\boldsymbol{t}$\textbf{-SNE} \cite{maatenVisualizingDataUsing2008}, and \textbf{UMAP} \cite{mcinnesUMAPUniformManifold2020a}. For evaluation, we applied a parametric clustering method, \textbf{GMM} \cite{dempsterMaximumLikelihoodIncomplete1977}, to compute the Adjusted Rand Index (ARI), providing a quantitative measure of the reliability of the representations.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/hybrid_identification_visualization_subplots_1.pdf}
    \vspace{-2mm}
    \caption{
    \label{fig: hybrid_identification_example}
        Visualized results of spike identification on an example recording in the Hybrid static subset. The first column illustrated UMAP embeddings of SimSort-learned representations, with GMM clustering results (top) and true labels (bottom). In the second, third, and fourth columns, low-dimensional features were extracted directly from waveform data using UMAP, $t$-SNE, and PCA, respectively. For each method, the top row presented the GMM clustering results, and the bottom row showed the corresponding true labels. See Appendix \ref{appendix: more showcases} for visualization results for additional samples.
    }
    \vspace{-4mm}
\end{figure*}

As shown in Table~\ref{tab:hybrid_identification_table}, SimSort substantially outperformed these conventional methods under both static and drift conditions, achieving the highest average ARI values of \textbf{0.91} and \textbf{0.89}, respectively. These results revealed the robustness and effectiveness of our approach in deriving meaningful and reliable representations from extracellular data.

Additionally, we compared SimSort with \textbf{CEED}, a recent framework for representation learning on extracellular data, using the provided checkpoint and hyperparameters. Both methods were evaluated on fifty test sets from the IBL Neuropixels dataset. As presented in Table~\ref{tab:IBL_identification_table}, Fig.~\ref{fig: IBL_bar}, and Fig.~\ref{fig: IBL_comparisions}, our method achieved performance comparable to CEED, with a slightly higher score.

\paragraph{Case Studies}
In Fig.~\ref{fig: hybrid_identification_example}, we visualized the results of spike identification on an example recording in the Hybrid static subset. SimSort's embeddings (first column) demonstrated superior clustering performance, with a GMM clustering ARI score of 0.90, closely aligning with true labels. In contrast, UMAP, $t$-SNE, and PCA embeddings (second to fourth columns) yielded lower clustering scores of 0.80, 0.78, and 0.67, respectively. SimSort embeddings showed clear separation between neuron clusters, while other methods exhibited significant overlap, highlighting the advantages of SimSort in learning robust and discriminative features.

\paragraph{Impact of Dimensionality Reduction and Clustering Methods}
To evaluate the adaptability of SimSort, we conducted a study to assess the impact of different dimensionality reduction techniques (PCA, $t$-SNE, UMAP, and None) and clustering algorithms (GMM, KMeans \cite{macqueenMethodsClassificationAnalysis1967}, Mean Shift \cite{chengMeanShiftMode1995}, and Spectral Clustering \cite{ngSpectralClusteringAnalysis2001}). Dimensionality reduction was applied to latent features extracted from waveforms, with an additional setup where clustering was performed directly on the raw latent features without dimensionality reduction (None). As illustrated in Fig.~\ref{fig: ablation_study}, the combination of UMAP and Mean Shift achieved the highest accuracy on both hybrid static and drift subsets. Across many other combinations, SimSort also maintained competitive performance, demonstrating its flexibility and robustness to various configurations.

\begin{figure}[h]
    \centering
    \vspace{-3mm}
    \includegraphics[width=0.95\columnwidth]{figures/ablation_study.pdf}
    \caption{
    \label{fig: ablation_study}
    Impact of dimensionality reduction and clustering algorithms on the hybrid dataset. MS denotes Mean Shift. %comparing different combinations of dimensionality reduction methods (None, PCA, $t$-SNE, UMAP) and clustering algorithms (GMM, KMeans, Mean Shift, and Spectral Clustering) on static (left) and drift (right) subset. Here, ``None'' indicates that raw features learned by SimSort were used directly without dimensionality reduction.
    }
    \vspace{-3mm}
\end{figure}

\subsection{Scaling Law of Data Size}
\label{subsec: scaling law}
As we emphasized the importance of a large-scale dataset throughout this work, it was interesting to evaluate how SimSort's performance scaled with the size of the training dataset (RQ3). Specifically, we evaluated performance across sorting accuracy, detection accuracy, and identification ARI score, capturing how each metric varied with the growing size of the training dataset, which ranged from $2^8$ to $2^{13}$ trials. As Fig.~\ref{fig: scaling_law} shows, larger datasets led to almost consistent performance improvements on both hybrid static and drift subsets, reflecting the scaling law of data size.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/scaling_law.pdf}
    \vspace{-3mm}
    \caption{
    \label{fig: scaling_law}
        Performance scaling with increasing dataset size (x-axis). Purple solid lines denoted sorting accuracy, purple dashed lines represented detection accuracy, and green solid lines corresponded to identification ARI. Darker lines indicated the hybrid static subset, while lighter lines corresponded to the drift subset.
    }
    \vspace{-3mm}
\end{figure}

\section{Conclusion}
In this paper, we presented SimSort, a data-driven framework for automated spike sorting, leveraging a biologically realistic large-scale extracellular dataset. SimSort outperformed commonly used spike sorting algorithms on both simulated and real-world benchmarks, demonstrating its robustness and scalability. Evaluations further highlighted its strong zero-shot transfer capability without fine-tuning.

SimSort has limitations. The models were trained on 4-channel tetrode data. While SimSort shows promising performance on tasks with high-density probes (IBL Neuropixels), future work should extend the training with diverse electrode geometries. Moreover, SimSort only used simulated spike sorting data for training since we focused on validating its zero-shot generalizability. The performance of SimSort can be further improved by incorporating real-world datasets into training.

\section*{Impact Statement}
Our research contributes to the growing field of neuroscience by providing a promising approach for spike sorting tasks. It leverages pretraining on simulated datasets to enhance scalability and accuracy, offering exciting prospects for future advancements in this domain. We have carefully considered the ethical implications and societal impact of our work and believe it posed no foreseeable risks in these areas.

\section*{Acknowledgments}
This work was supported partially by AI for Science Foundation of Fudan University (FudanX24AI046 to Y.G.). This work was also supported by Microsoft Research.
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}


\bibliography{main.bbl}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}

\section{Implementation details of SimSort}
\label{appendix: A}
\subsection{Data Preparation}
\paragraph{Train datasets}
In Sec.~\ref{subsec: Dataset Preparation}, we introduced the preparation of our simulated training dataset, divided into the continuous signal dataset and the spike waveform dataset.

The \textbf{continuous signal dataset} was constructed by concatenating multiple simulated trials (e.g., 8192 trials), each containing 6,000,000 timesteps (dt = 0.1 ms) of extracellular data. Spike labels were derived by identifying peak times of intracellular action potentials and assigning binary labels to the corresponding region. This dataset, therefore, provided labeled, continuous extracellular signals for training the detection model and evaluating the overall spike sorting performance of SimSort.

The \textbf{spike waveform dataset} was constructed by extracting waveform segments centered on labeled spike events. For each spike, a 60-timestep waveform was segmented from the extracellular signals, and up to 400 such waveforms were collected per unit. Each waveform was then paired with its corresponding unit label, creating a dataset dedicated to training and evaluating the identification model.

\paragraph{Test datasets}
\label{appendix: test datasets}
\textbf{Hybrid dataset}. This dataset was composed of recordings provided by Spikeforest \cite{maglandSpikeForestReproducibleWebfacing2020}. These recordings were generated by Kilosort, with waveform templates recorded at a 5 µm electrode spacing and ~1/f background noise. It consisted of two subsets: 9 static tetrode recordings and 9 drift tetrode recordings that simulated sinusoidal probe movements. For each recording, we selected units with an SNR greater than 3 (unit IDs meeting this criterion were provided by SpikeForest to ensure a fair comparison), comprising 10–15 ground-truth units per recording. The dataset we prepared supported two modes of use. To evaluate overall spike sorting and detection performance, we used the raw data as input. For assessing identification performance, we constructed a waveform dataset by extracting 500 waveforms (60-timesteps length) per unit.

\textbf{WaveClus dataset}. This dataset consisted of four subsets (8 recordings in Easy1, 4 recordings in Easy2, 4 recordings in Difficult1, and 4 recordings in Difficult2). Each recording was generated by combining spike waveform templates derived from experimental recordings with background noise, with noise levels ranging from 0.05 to 0.4, and contained 3 ground-truth units. To increase the overall difficulty and adapt the original single-channel recording for tetrode analysis, we preprocessed the signals by duplicating them and modifying them with small random noise (standard deviation 0.01), amplitude scaling (ranging from 1 to 0.5), and random temporal shifts (-5 to +5 samples).

\textbf{IBL Neuropixels dataset}. This dataset was derived from a real-world extracellular recording, CortexLab KS046, published by the International Brain Laboratory (IBL). This recording was captured with a Neuropixels 1.0 probe across multiple brain regions from a mouse performing a decision-making behavior task. To compare spike identification performance with CEED, we prepared this dataset following the method described in the CEED paper (CEED trained and evaluated datasets derived from DanLab DY016 and DY009 Neuropixels recordings in the IBL database). We extracted waveforms from 400 units detected by Kilosort 2.5 and created fifty test sets for comparison between SimSort and CEED. Each test set contained 10 random neurons and 100 spikes per neuron. Note that CEED inputs 11-channel data into the model, while SimSort processes data from the center 4 channels.

\begin{table}[h]
\centering
\caption{
    \label{tab:datasets}
        Details information of test datasets used for evaluation.
    }
\vspace{1mm}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
Datasets & \makecell{Num. \\ Recordings} & \makecell{Sample Rate \\ (Hz)} & \makecell{Num. \\ Channels} & \makecell{Duration \\ (sec per rec.)} & \makecell{Num. True Units \\ (per rec.)} & \makecell{Ground-Truth \\ Determination} \\ 
\midrule
BBP L6 dataset          & 20 & 10000 & 4 & 600         & 5        & Simulation     \\
Hybrid dataset          & 18 & 30000 & 4 & 600 / 1200 & 10-15 & Real waveforms + background noise \\
WaveClus dataset        & 20 & 24000 & 4 & 60          & 3        & Real waveforms + background noise \\
IBL Neuropixels dataset & /  & 30000 & 11 & /           & 10       & Sorted with KiloSort 2.5    \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Spike Detection}
\paragraph{Data Augmentation}
For each input segment $X_i \in \mathbb{R}^{T \times C}$, where $T$ represents the number of time points and $C$ denotes the number of electrode channels, a subset of channels $\mathcal{C} \subseteq \{1, 2, \dots, C\}$ was randomly selected. A probability $p \in [0, 1]$ determined whether the following augmentations were independently applied to each channel $k \in \mathcal{C}$:

\begin{equation}
    X'_{i,k} = 
    \begin{cases} 
        X_{i,k} + \epsilon, & \text{AddWithNoise}, \\
        \beta \cdot X_{i,k}, & \text{RandomAmplitudeScaling}, \\
        X_{i,k,\lfloor t \cdot n + \epsilon_t \rfloor}, & \text{RandomTimeJitter}.
    \end{cases}
\end{equation}

where $\epsilon \sim \mathcal{N}(0, \sigma^2)$ represents Gaussian noise, $\beta \sim \mathcal{U}(1 - s, 1 + s)$ is a random amplitude scaling factor, and $\epsilon_t \sim \mathcal{U}(-\delta, \delta)$ introduces small temporal shifts. The parameter $n$ is an up-sampling factor enabling finer time resolution.

\paragraph{Model-Based Detector}
In Sec.~\ref{subsec: method-Spike detection}, we introduced a model-based detector to predict spike events. These predictions were processed through the following steps:

\textbf{1. Detection with Model:}  
The model took as input a multi-channel signal \( V \in \mathbb{R}^{t \times c} \), where \( t \) was the number of time samples and \( c \) was the number of channels. This input was processed simultaneously across all channels by the detection model, which predicted a sequence of binary labels \( \hat{y}_i(t) \) for each time point:
\begin{equation}
    \hat{y}_i(t) = f_{\text{model}}(V),
\end{equation}
where \( f_{\text{model}} \) represented the detection model. The output \( \hat{y}_i(t) \) indicated the presence or absence of spike events at each time point.

\textbf{2. Adjacent Peak Merging:}  
The predicted labels \( \hat{y}_i(t) \) were refined by merging adjacent peaks to reduce false positives and consolidate overlapping detections. To identify the most relevant peaks, the channel with the maximum summed absolute signal across all time points was selected:
\begin{equation}
    c^* = \arg\max_{c \in \{1, \dots, C\}} \sum_{t=1}^T |V(t, c)|,
\end{equation}
where \( V(t, c) \) denoted the signal value at time \( t \) for channel \( c \). Using this peak channel, adjacent peaks were grouped within a window size \( \Delta t \), and only the time point with the maximum absolute signal was retained within each group:
\begin{equation} 
    t_{\text{max}} = \arg\max_{t \in [t_s, t_e]} |V(t, c^*)|,
\end{equation}
where \( [t_s, t_e] \) denoted the boundaries of a group of adjacent peaks. 

\textbf{3. Spike Peak Extraction:}  
After merging, the final spike peak positions \( \{t_k\} \) were determined as the set of time points where \( \hat{y}'_i(t) = 1 \): 
\begin{equation}
    t_k = \{t \,|\, \hat{y}'_i(t) = 1\}.
\end{equation}

\paragraph{Threshold-Based Detector}
In Sec.~\ref{sec: comparasion of detection}, we compared our model-based detector with a threshold-based detector. To ensure a fair comparison, the processing steps for the threshold-based detector were designed to align closely with those of the model-based detector. The threshold-based method processed the signal \( V \in \mathbb{R}^{t \times c} \), where \( t \) was the number of time samples and \( c \) was the number of channels. Each channel was processed independently, as follows:

\textbf{1. Local Maxima Detection}:  
For each channel \( c \), the signal \( V(t, c) \) was scanned to identify local maxima that satisfied the following conditions:
\begin{equation}
    V(t, c) > Th_{\text{single\_ch}}, \quad V(t, c) > V(t - r, c), \quad V(t, c) > V(t + r, c),
\end{equation}
where \( Th_{\text{single\_ch}} \) was the amplitude threshold, and \( r \in \{1, \dots, \text{loc\_range}\} \). These conditions ensured that the detected peaks were above the threshold and were local maxima within the range \( r \). Peaks identified by this criterion were further validated within a broader range \( \text{long\_range} \) to ensure they represented significant spike events.

\textbf{2. Adjacent Peak Merging}:  
For each channel \( c \), adjacent peaks were grouped within a window size \( \Delta t \), and only the time point with the maximum absolute signal was retained within each group:
\begin{equation} 
    t_{\text{max}} = \arg\max_{t \in [t_s, t_e]} |V(t, c)|,
\end{equation}
where \( [t_s, t_e] \) defined the boundaries of a cluster of adjacent peaks. This step ensured that closely spaced peaks were consolidated into a single spike.

\textbf{3. Spike Peak Extraction}:  
After processing all channels independently, the final detected spike positions for channel \( c \) were determined as:
\begin{equation}
    t_k = \{t \,|\, V(t, c) = t_{\text{max}}\}.
\end{equation}

\subsection{Spike Identification}
\textbf{Waveform Snippet Extraction}.  
For each detected spike at position \( t_k \), a waveform snippet \( W_k \in \mathbb{R}^{L \times c} \) was extracted from the multi-channel continuous signal \( V(t, c) \), where \( L \) was the snippet length (number of time samples) and \( c \) was the number of channels. The snippet was centered around \( t_k \) and spanned a time window determined by the pre- and post-padding values:
\begin{equation}
    W_k = V(t_k - p_{\text{pre}} : t_k + p_{\text{post}}, :),
\end{equation}
where \( V(t, c) \) represented the input signal value at time \( t \) for channel \( c \), and \( p_{\text{pre}} \) and \( p_{\text{post}} \) were the padding values (in time samples) before and after the spike time \( t_k \), respectively.

\subsection{Definitions of Metrics}
To evaluate the performance of spike sorting algorithms, we adopted standard metrics as defined in the context of spike sorting and ground-truth comparisons \cite{maglandSpikeForestReproducibleWebfacing2020, buccinoSpikeInterfaceUnifiedFramework2020}. These metrics included \textbf{accuracy}, \textbf{precision}, and \textbf{recall}, which were derived based on the matching between the sorted spike train and the ground-truth spike train.

\textbf{1. Matching Spike Events:}  
For a given sorted spike train \(k\) and ground-truth spike train \(l\), the first step was to calculate the number of matching events (\(n^{\text{match}}_{l,k}\)). A sorted spike event \(s_j^{(k)}\) was considered a match to a ground-truth event \(t_i^{(l)}\) if the absolute time difference was less than or equal to the predefined matching window \(\Delta\), which was typically set to 1 millisecond:
\begin{equation}
n^{\text{match}}_{l,k} = \#\{ i : |t_i^{(l)} - s_j^{(k)}| \leq \Delta \text{ for some } j \}.
\end{equation}

\textbf{2. Missed Events and False Positives:}  
Using the number of matching events, we computed the number of missed events (\(n^{\text{miss}}_{l,k}\)) and false positives (\(n^{\text{fp}}_{l,k}\)) as follows:
\begin{equation}
n^{\text{miss}}_{l,k} = N_l - n^{\text{match}}_{l,k}, \quad n^{\text{fp}}_{l,k} = M_k - n^{\text{match}}_{l,k},
\end{equation}
where \(N_l\) was the total number of ground-truth events in \(l\), and \(M_k\) was the total number of sorted events in \(k\).

\textbf{3. Accuracy:}  
Accuracy (\(a_{l,k}\)) balanced the contributions of both missed events and false positives and was defined as:
\begin{equation}
a_{l,k} = \frac{n^{\text{match}}_{l,k}}{n^{\text{match}}_{l,k} + n^{\text{miss}}_{l,k} + n^{\text{fp}}_{l,k}}.
\end{equation}

\textbf{4. Precision and Recall:}  
The precision (\(p_l\)) and recall (\(r_l\)) were calculated based on the best matching sorted unit \(\hat{k}_l\) for each ground-truth unit \(l\). The best match was determined as the sorted unit \(k\) with the highest accuracy:
\begin{equation}
\hat{k}_l = \arg\max_k a_{l,k}.
\end{equation}
To identify the best matching sorted unit \(\hat{k}_l\) for each ground-truth unit \(l\), the Hungarian Algorithm was applied to establish the best one-to-one correspondence between sorted and ground-truth units. Using this best match, precision and recall were defined as:
\begin{equation}
p_l = \frac{n^{\text{match}}_{l,\hat{k}_l}}{n^{\text{match}}_{l,\hat{k}_l} + n^{\text{fp}}_{l,\hat{k}_l}}, \quad
r_l = \frac{n^{\text{match}}_{l,\hat{k}_l}}{n^{\text{match}}_{l,\hat{k}_l} + n^{\text{miss}}_{l,\hat{k}_l}}.
\end{equation}

\subsection{Hyperparameter Search}
We performed a grid search to optimize the hyperparameters for the SimSort detection and identification models. The hyperparameter ranges included the number of layers, hidden size, learning rate, number of attention heads, dropout rate, and augmentation settings. The final selected values are presented in Tables \ref{tab: hyperparameter_detector} and \ref{tab: hyperparameter_extractor}. 

\begin{table}[h]
\centering
\caption{
    \label{tab: hyperparameter_detector}
    Hyperparameters of SimSort detection model.
}
\vspace{1mm}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{ccc}
\toprule
Hyperparameters    & Description                                     & Value  \\ 
\midrule
Num Layer     & Number of Transformer encoder layers            & 4      \\
Input Size    & Dimensions of the input data                    & 4      \\
Batch Size    & Number of items processed in a single operation & 128    \\
Learning Rate & Learning rate of the model                      & 5e-4 \\
Hidden Size   & Dimension of the Transformer encoder            & 256    \\
nhead         & Number of Transformer attention heads           & 4      \\
Dropout       & Dropout probability in Transformer encoder      & 0.2    \\
Weight Decay  & Regularization parameter used to prevent overfitting & 1e-5    \\
Warmup Steps  & Steps to warm up the learning rate              & 4000 \\
Num Epochs    & Number of epochs to run                         & 20     \\
Sigmoid Threshold & Probability threshold for classifying spike events  &  0.97 \\
Noise Level   & The level of noise applied to the input data    & 0.9    \\
Maximum Time Jitter   & Maximum variation in timing (in terms of upsampled sampling points) & 5          \\
Amplitude Scale Range & The range for scaling the amplitude of the input data               & {[}0, 1{]} \\
Transform Probability & The probability to perform augmentations to input data & 0.8 \\
Max Channels          & The maximum number of data channels augmentations are applied to    & 4          \\ 
\bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[h]
\centering
\caption{
    \label{tab: hyperparameter_extractor}
    Hyperparameters of SimSort identification model.
}
\vspace{1mm}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{ccc}
\toprule
Hyperparameters    & Description                                                      & Value   \\
\midrule
Num Layer     & Number of GRU layers                                             & 1       \\
Input Size    & Dimensions of the input data                                     & 4       \\
Batch Size    & Number of items processed in a single operation                  & 128     \\
Learning Rate & Learning rate of the model                                       & 1e-4  \\
Hidden Size   & Latent dimention size of GRU                                     & 512     \\
Weight Decay  & Regularization parameter used to prevent overfitting             & 1e-5 \\
Num Epochs    & Number of epochs to run                                          & 10      \\
$k$ components & Number of components to reconstruct waveform using SVD          & 5  \\
Noise Level   & The level of noise applied to the input data                     & 4       \\
Maximum Time Jitter   & Maximum variation in timing (in terms of upsampled sampling points) & 50           \\
Amplitude Scale Range & The range for scaling the amplitude of the input data               & {[}0.5, 1{]} \\
Transform Probability & The probability to perform augmentations to input data & 0.5 \\
Max Channels  & The maximum number of data channels augmentations are applied to & 4       \\
\bottomrule

\end{tabular}%
}
\end{table}

\clearpage
\section{Supplementary Figures}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/acc-hybrid-waveclus.png}
    \caption{
    \label{fig: hybrid_waveclus-acc}
        (a) Accuracy of spike sorting on the Hybrid dataset. SimSort outperforms HerdingSpikes2 (p = 0.00), JRClust (p = 0.02), and KiloSort2 (p = 0.00) in static recordings, with no significant difference compared to IronClust (p = 0.45), KiloSort (p = 0.71), MountainSort4 (p = 0.48), SpykingCircus (p = 0.27), and Tridesclous (p = 0.12). In the drift condition, SimSort significantly outperforms HerdingSpikes2 (p = 0.00), JRClust (p = 0.00), KiloSort2 (p = 0.00), MountainSort4 (p = 0.00), and Tridesclous (p = 0.00), with no significant difference compared to IronClust (p = 0.69), KiloSort (p = 0.24), and SpykingCircus (p = 0.06).
        (b) Accuracy of spike sorting on the WaveClus dataset. SimSort shows no significant improvement in the easy subset compared to KiloSort (p = 0.14), KiloSort2 (p = 0.22), MountainSort4 (p = 0.86), and MountainSort5 (p = 0.69). In the difficult subset, SimSort significantly outperforms KiloSort (p = 0.00) and KiloSort2 (p = 0.00), with no significant difference compared to MountainSort4 (p = 0.22) and MountainSort5 (p = 0.13).
        Statistical analysis was performed using an independent two-tailed $t$-test.
    }
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/IBL_identification_compare_ARI-50-turn.png}
    \caption{
    \label{fig: IBL_bar}
    Performance comparison on Fifty test sets of IBL Neuropixels dataset.
    }
\end{figure*}

\begin{table}[h]
    \centering
    \caption{
    \label{tab:IBL_identification_table}
        Performance comparison of spike identification on 50 test sets from IBL Neuropixels dataset. ARI values are presented as mean $\pm$ standard deviation. 
    }
    \vspace{1mm}
    \resizebox{0.35\linewidth}{!}{
    \begin{tabular}{lc}
    \toprule
    \textbf{Methods}   & \textbf{Test sets (seed 0-49)} \\
    \midrule
    CEED     & 0.46 $\pm$ 0.09 \\
    CEED+UMAP   & 0.47 $\pm$ 0.10 \\
    \rowcolor{simsortcolor!20} Ours & \textbf{0.49} $\pm$ \textbf{0.09} \\
    \bottomrule                                   
    \end{tabular}
    }
\end{table}

\begin{figure*}[h]
    \centering
    \includegraphics[height=0.9\textheight]{figures/IBL_comparisons.png}
    \caption{
    \label{fig: IBL_comparisions}
    Visualized comparison results of three example test sets from IBL Neuropixels dataset. For each example, the first column presents GMM clustering results on UMAP-embedded SimSort representations, with predicted labels (top) and true labels (bottom). The second column displays GMM clustering on UMAP-embedded CEED representations. The third column illustrates GMM clustering directly on CEED representations (as in the CEED paper).
    }
\end{figure*}

\clearpage
\section{More Showcases}
\label{appendix: more showcases}
In this section, we present additional visualizations from various datasets to provide a more comprehensive evaluation of SimSort's performance. These visualizations include examples of spike detection, identification, and sorting tasks. The supplementary showcases illustrate key aspects such as the alignment between detected and ground-truth spikes and the clustering of spike waveforms.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/hybrid_model_detection_static9.png}
    \caption{
    \label{fig: hybrid_detection_visualization}
    Visualization of spike detection performance on an example recording in Hybrid dataset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/hybrid_model_UMAP_detection_example_zoom.png}
    \caption{
    \label{fig: hybrid_detection_visualization_zoom}
    Zoomed-in visualization of spike detection performance on an example recording in Hybrid dataset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[height=0.45\textheight]{figures/waveclus_model_UMAP_detection_example.png}
    \caption{
    \label{fig: waveclus_detection_visualization}
    Visualization of spike detection performance on an example recording in WaveClus dataset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[height=0.45\textheight]{figures/waveclus_model_UMAP_detection_example_zoom.png}
    \caption{
    \label{fig: waveclus_detection_visualization_zoom}
    Zoomed-in visualization of spike detection performance on an example recording in WaveClus dataset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[height=0.45\textheight]{figures/bbp_model_UMAP_detection_example.png}
    \caption{
    \label{fig: bbp_detection_visualization}
    Visualization of spike detection performance on an example recording in BBP L6 dataset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[height=0.45\textheight]{figures/bbp_model_UMAP_detection_example_zoom.png}
    \caption{
    \label{fig: bbp_detection_visualization_zoom}
    Zoomed-in visualization of spike detection performance on an example recording in BBP L6 dataset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[height=0.9\textheight]{figures/hybrid_static_identification_visualization.png}
    \caption{
    \label{fig: hybrid_static_identification_visualization_1}
    Visualization of spike identification performance on hybrid static subset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[height=0.9\textheight]{figures/hybrid_drift_identification_visualization.png}
    \caption{
    \label{fig: hybrid_drift_identification_visualization_2}
    Visualization of spike identification performance on hybrid drift subset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/hybrid_static_sorting_visualization.png}
    \caption{
    \label{fig: hybrid_static_sorting_visualization_1}
    Visualization of spike sorting performance on hybrid static subset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/hybrid_drift_sorting_visualization.png}
    \caption{
    \label{fig: hybrid_drift_sorting_visualization_2}
    Visualization of spike sorting performance on hybrid drift subset.
    }
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/waveclus_sorting_visualization.png}
    \caption{
    \label{fig: waveclus_sorting_visualization}
    Visualization of spike sorting performance on WaveClus dataset.
    }
    
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/bbp_l6_sorting_visualization.png}
    \caption{
    \label{fig: bbp_sorting_visualization}
    Visualization of spike sorting performance on BBP L6 dataset.
    }
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
