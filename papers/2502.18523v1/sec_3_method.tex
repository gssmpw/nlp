\vspace{-5pt}
\section{Our Approach}
\label{sec:method}
%\noindent\textbf{Overview.} 
%Figure~\ref{fig:network} presents the overview of the UniBrain framework. 
%designed for the end-to-end brain imaging analysis problem. 
%Our method is an end-to-end deep neural network consisting of five main modules: 1) \emph{Extraction Module} processes the raw source image $\mathbf{S}$ to yield the extracted brain image $\mathbf{E}$; 2) \emph{Registration Module} aligns extracted brain image $\mathbf{E}$ with the target image $\mathbf{T}$, resulting in the warped image $\mathbf{W}$; 3) \emph{Segmentation \& Parcellation Modules} leverage the target segmentation mask $\mathbf{B}$ and parcellation mask $\mathbf{P}$ to produce the source segmentation mask $\mathbf{R}$ and parcellation mask $\mathbf{U}$; 4) \emph{Brain Network Module} generate the brain network $G$ based one the region information provided by source image $\mathbf{S}$ and parcellation mask $\mathbf{U}$; 5) \emph{Classification Module} employs the generated brain network $G$ for prediction, outputting $\hat{y}$. The final output of UniBrain includes:  extracted brain image $\mathbf{E}$ constituting only cerebral tissues; warped image $\mathbf{W}$ aligning with the target image; brain segmentation mask $\mathbf{R}$ and parcellation mask $\mathbf{U}$ indicating tissue types and regions of the source image; brain network $G$ representing connectivity among regions, and prediction $\hat{y}$ signifying classification outcomes. 

UniBrain integrates multiple modules for brain extraction, registration, segmentation, parcellation, network generation, and classification, seamlessly connecting them within an end-to-end framework to enable collective learning.
Below, we provide a detailed description of each module.

% We will now delve into the details of each module.

%\input{fig_network2}

%\noindent \textbf{Key Insights.}  Given the intrinsic interdependence of tasks in brain imaging analysis, our method's design capitalizes on four synergistic mechanisms that facilitate collaboration among all modules: 1) Performance in the initial extraction is crucial for preventing error propagation and ensuring the stability of all subsequent tasks. 2) Accurate registration leads to generating precise segmentation and parcellation masks, aiding subsequent tasks. 3) The tissue structure can provide auxiliary information for the extraction module to remove non-cerebral parts and for the registration module to find anatomical correspondences. 4) Accurate classification not only guarantees predictive precision but also delivers task-aware feedback for network generation.

%Consequently, we incorporate these four synergistic mechanisms into our loss design, training our network end-to-end to ensure efficiency and efficacy. Specifically, our loss function is structured to include four distinct loss terms, each aligning with one of these mechanisms: the extraction loss $\mathcal{L}_{ext}(\hat{\mathbf{M}}, \mathbf{M})$ aligns with the first mechanism, the registration loss $\mathcal{L}_{sim}(\mathbf{W}, \mathbf{T})$ corresponds to the second mechanism, the segmentation loss $\mathcal{L}_{seg}(\mathbf{R}, \mathbf{V})$ matches the third mechanism, and the classification loss $\mathcal{L}_{cls}(\hat{y}, y)$ pertains the fourth mechanism.
%Bidirectional supervision at both ends first envelops the entire network, ensuring positive forward propagation and controllable feedback across tasks. Then, the unsupervised and one-shot guidance inside the model effectively reduces the reliance on high-cost annotations. We jointly optimize the four loss terms to achieve collective learning on all tasks. Next, we introduce the details of each module and the training process.


\vspace{-3pt}
\subsection{Extraction Module}

The extraction module aims to extract brain from the raw image with assistance from two components:

\vspace{1pt}
\noindent \textbf{Extraction Network: $f_{e}$}. The extraction network $f_{e}(\cdot)$ acts as an annotator, intended to identify brain and non-brain tissues in the source image $\mathbf{S}$ and delineate their locations, thus providing the guidance for subsequent non-brain tissue elimination. Specifically, we employ the 3D U-Net as the base network to learn $f_{e}(\cdot)$.
The process can be formally expressed as:

\begin{equation}
\hat{\mathbf{M}}=f_{e}\left(\mathbf{S}\right),
\end{equation}
where $\hat{\mathbf{M}}$ is predicted extraction mask. During inference, $\hat{\mathbf{M}}$ is binarized by a Heaviside step function.

\vspace{1pt}
\noindent \textbf{Overlay Layer: $OL$}.
The overlay layer serves to eliminate non-brain tissues by applying the predicted brain mask $\hat{\mathbf{M}}$ to the source image $\mathbf{S}$. The final extracted image is $ \mathbf{E} = \mathbf{S}\circ \hat{\mathbf{M}}$, where $\circ$ denotes the element-wise multiplication.

\vspace{-3pt}
\subsection{Registration Module}
\label{sec: registration Module}
The registration module aims to align the extracted image with the target image, providing transformations for subsequent segmentation and parcellation tasks. This module comprises two main components:

\vspace{1pt}
\noindent \textbf{Registration Network: $f_{r}$}.
The registration network $f_{r}(\cdot, \cdot)$ processes the extracted image $ \mathbf{E}$ and target image $\mathbf{T}$ to learn the affine transformation $\mathbf{A}$, which establishes the coordinate correspondence between source and target image space. A 3D CNN-based encoder is used to learn $f_{r}(\cdot, \cdot)$ as:
\begin{equation}
\mathbf{A}=f_{r}\left(\mathbf{E}, \mathbf{T}\right).
\end{equation}
We leverage the multi-stage registration technique~\cite{su2022abn,zhao2019recursive} to boost registration performance, where $\mathbf{E}$ is recursively aligned with $\mathbf{T}$ though $M$ stages. 
%A study of $M$ can be found in Appendix A.2.1.

\vspace{1pt}
\noindent \textbf{Spatial Transformation Layer: $STL$}.
A key step in image registration is reconstructing the warped image $\mathbf{W}$ from the extracted image $\mathbf{E}$ using the affine transformation $\mathbf{A}$. This warping process is facilitated by a spatial transformation layer (STL), which resamples voxels from the extracted
image $\mathbf{E}$ to produce the warped image $\mathbf{W} = \mathcal{T}(\mathbf{E}, \mathbf{A})$. Given the affine transformation operator, we hold
\begin{equation}
     \mathbf{W}_{xyz} = \mathbf{E}_{x'y'z'} \hspace{1pt},
     \label{equ:voxel_value_k}
\end{equation}
where coordinate correspondence $[x', y', z', 1]^\top = \mathbf{A}[x, y, z, 1]^\top $. 
To enable successful gradient propagation, we use a differentiable transformation based on trilinear interpolation proposed by~\cite{jaderberg2015spatial}.

\vspace{-3pt}
\subsection{Segmentation \& Parcellation Module}
The segmentation and parcellation module creates segmentation and parcellation masks on the source image.
Leveraging recent developments in one-shot learning~\cite{wang2020lt, ding2021modeling, su2023one}, the module can generate these masks using a single labeled template image. The module contains two main components:

\vspace{1pt}
\noindent \textbf{Inverse Warping} Utilizing a single labeled example (\ie target image $\mathbf{T}$ with its corresponding segmentation mask $\mathbf{B}$ and parcellation mask $\mathbf{P}$) and the learned affine transformation $\mathbf{A}$, we apply the inverse transformation $\mathbf{A}^{-1}$ to generate warped segmentation mask $\mathbf{V} = \mathcal{T}(\mathbf{B}, \mathbf{A}^{-1})$ and parcellation mask $\mathbf{U} = \mathcal{T}(\mathbf{P}, \mathbf{A}^{-1})$ in the source image space as:
\begin{equation}
    \mathbf{V}_{cxyz} = \mathbf{B}_{cx'y'z'}, \forall  c \in \{1 ,\ldots, C\},
\end{equation}
\begin{equation}
    \mathbf{U}_{kxyz} = \mathbf{P}_{kx'y'z'}, \forall  k \in \{1 ,\ldots, K\},
\end{equation}
where coordinate correspondence $[x', y', z', 1]^\top = \mathbf{A}^{-1}[x, y, z, 1]^\top $, $c$ is the index for tissue class and $k$ is the index for ROIs. Same as the $\text{STL}$ layer in Registration Module, we then apply a differentiable transformation based on trilinear interpolation. 

\vspace{1pt}
\noindent \textbf{Segmentation Network: $f_s$}. The segmentation network $f_s(\cdot)$ aims to generate a segmentation mask for the source image $\mathbf{S}$ that 
matches the synthesized warped segmentation mask $\mathbf{V}$. We employ the widely-used 3D U-Net as the base network to learn $f_s(\cdot)$. Formally, we have:
\begin{equation}
    \mathbf{R} = f_s(\mathbf{S}).
\end{equation}

%In our study, we adopted different strategies for brain segmentation and parcellation: 1) For segmentation, neural networks are used to learn the source image's segmentation mask $\mathbf{R}$, effectively identifying brain tissues with clear boundaries and adjusts well to variations between tissues, resulting in precise segmentation masks. This process provide positive feedback to the registration module, enhancing label accuracy~\cite{wang2020lt, zhao2019data, ding2021modeling, su2023one}; 2) For parcellation, due to its complexity and less defined boundaries in functional regions, we used inverse warping to obtain the source's parcellation mask $\mathbf{U}$. Additionally, the choice of parcellation atlas can vary (\eg AAL and Desikan)~\cite{arslan2018human}.

\input{tab_res_ADHD.tex}
%\input{tab_ablation}
\input{tab_methods_v4}

\vspace{-3pt}
\subsection{Brain Network Module}

The brain network module generates the brain network using ROI information from parcellation mask $\mathbf{U}$ and the source image $\mathbf{S}$. The modules include three components:

%\subsubsection{Overlay Layer: $OL$} Similar to the operation in Section~\ref{sec: overlap}, this component is responsible for isolating each ROI image from the source image $\mathbf{S}$ using parcellation mask $\mathbf{U}$. First, $\mathbf{S}$ is expanded to $\tilde{\mathbf{S}} \in \mathbb{R}^{K \times W \times H \times D}$, where each \( k^{\text{th}} \) slice of \( \tilde{\mathbf{S}} \) is a copy of \( \mathbf{S} \). The ROI extracted image $\mathbf{F} \in \mathbb{R}^{K \times W \times H \times D} = \tilde{\mathbf{S}} \circ \mathbf{U}$ is then generated by applying an element-wise product~$\circ$ between $\tilde{\mathbf{S}}$ and $\mathbf{U}$.

\vspace{1pt}
\noindent \textbf{Overlay Layer: $OL$}. Similar to $OL$ in the Extraction Module, this component is responsible for isolating each ROI from the source image $\mathbf{S}$ using parcellation mask $\mathbf{U}$. The parcellated image $\mathbf{F} = \mathbf{S} \circ \mathbf{U}$ is generated by applying an element-wise product~$\circ$ between $\mathbf{S}$ and $\mathbf{U}$.

\vspace{1pt}
\noindent \textbf{Brain Network Function: $f_o$}. The brain network function aims to learn the representation for each ROI within the parcellated image $\mathbf{F}$. A weight-sharing Multilayer Perceptron (MLP) is employed to learn $f_o(\cdot)$, ensuring consistent feature extraction and generalization, which is expressed as:
\begin{equation}
    \mathbf{H}_{k} = f_o(\mathbf{F}_{k}), \forall  k \in \{1 ,\ldots, K\},
\end{equation}
where $k$ is the index for the ROIs. 
%The concatenated ROI feature $\mathbf{H}$ is then used to construct a task-aware brain network.

\vspace{1pt}
\noindent \textbf{Brain Network Generation}. The step generates a brain network based on the similarity between ROI representation pairs. Without loss of generality, here we use inner-product to measure the edge weights of the brain network. However, other differentiable similarity functions (\eg Mahalanobis distance and cosine similarity) can be used. To compute the connectivity matrix $\mathbf{C}$, each ROI representation $\mathbf{H}_{k}$ is first normalized with the $\ell^{2}\text{-norm}$, followed by the inner-product:
\begin{equation}
    \mathbf{C} = \mathbf{H} \mathbf{H^\top}.
\end{equation}
%the similarity between ROI representation pairs serves as edge weight of the network, expressed as: 
%The process begins by normalizing each ROI feature $\mathbf{H}_{k}$ with the $\ell^{2}\text{-norm}$, setting the stage to compute the connectivity matrix as: $\mathbf{C} = \mathbf{H} \mathbf{H^\top}$.
%This normalization ensures the stabilization of the learning process and maintains consistent weight magnitudes across the network, scaling the values of $\mathbf{C}$ to the range of $[-1, 1]$.
This normalization scales the values of $\mathbf{C}$ to the range of $[-1, 1]$, ensuring the stabilization of the learning process and maintaining consistent weight magnitudes in the network.
%The similarity score indicates the ROIs closer in embedding space are more likely to be connected, as suggested by~\cite{grover2019graphite, kipf2016variational,zou2019encoding,kan2022fbnetgen}.
%To further refine the network, connections in $\mathbf{C}$ with negative weights are screened out to reduces complexity and potential noise from less relevant connections~\cite{li2021braingnn, said2023neurograph, van2010comparing,kim2021learning}.

\subsection{Classification Module}
%The classification module makes the final diagnostic prediction. 
The classification module makes a final predictive diagnosis.

\vspace{1pt}
\noindent \textbf{Classification Network: $f_g$}. The classification network $f_{g}(\cdot, \cdot)$ aims to make a prediction based on the generated brain network while feeding task-specific insights to the preceding module, facilitating the brain network generation. We leverage the GCN~\cite{kipf2017semi} as the base network. The prediction $\hat{y}$ is obtained as:

\begin{equation}
\hat{y} = f_g(\mathbf{C},\mathbf{H}),
\end{equation}
where $\mathbf{H}$ is the initial node features and $\mathbf{C}$ is the learnable connectivity matrix provided by the brain network module.
%provided by the preceding brain network module.

\vspace{-3pt}
\subsection{End-to-End Training}
\label{section:end-to-end training}

We train UniBrain by minimizing the objective function as: 
\begin{equation}
\label{eq:optimization}
\begin{aligned}
\mathcal{L} = \hspace{+1pt} \mathcal{L}_{cls}\big(\hat{y}, y\big)  & + \alpha \mathcal{L}_{ext}\big(\hat{\mathbf{M}}, \mathbf{M}\big)   + \\ \beta  \mathcal{L}_{sim}\big(\mathbf{W}, \mathbf{T}\big)   & + \gamma \mathcal{L}_{seg}\big({\mathbf{R}}, \mathbf{V}\big),
\end{aligned}
\end{equation}
where $\mathcal{L}_{cls}(\cdot, \cdot)$ is classification loss term, $\mathcal{L}_{ext}(\cdot, \cdot)$ is extraction loss term, $\mathcal{L}_{sim}(\cdot, \cdot)$ is image dissimilarity loss term , and $\mathcal{L}_{seg}(\cdot, \cdot)$ is segmentation loss term.
This equation incorporates bidirectional supervision ($\mathcal{L}_{cls}(\cdot, \cdot)$ and $\mathcal{L}_{ext}(\cdot, \cdot)$), which envelops the entire network to ensure positive forward propagation and controllable feedback across tasks. Additionally, unsupervised and one-shot guidance ($\mathcal{L}_{sim}(\cdot, \cdot)$ and $\mathcal{L}_{seg}(\cdot, \cdot)$) within the model reduces reliance on high-cost annotations. 
The loss terms are scaled by $\alpha$, $\beta$, and $\gamma$ to balance their impacts. 
%The influence of these hyperparameters is discussed in Appendix A.2.1.

\iffalse
We train UniBrain by minimizing the objective function: 
\begin{equation}
\label{eq:loss}
\begin{aligned}
\mathcal{P^*} = \underset{\mathcal{P}}{\min}\mathcal{L}_{cls}\big(\hat{y}, y\big)  + \alpha \mathcal{L}_{ext}\big(\hat{\mathbf{M}}, \mathbf{M}\big) + \beta  \mathcal{L}_{sim}\big(\mathbf{W}, \mathbf{T}\big) + \gamma \mathcal{L}_{seg}\big({\mathbf{R}}, \mathbf{V}\big),
\end{aligned}
\end{equation}
where $\mathcal{P} = \{\theta, \phi, \psi, \xi, \eta\}$ is the parameter set for the extraction network $f_{e}(\cdot; \theta)$, registration network $f_{r}(\cdot,\cdot; \phi)$, segmentation network $f_{s}(\cdot; \psi)$, brain network function $f_{o}(\cdot; \xi)$, and classification network $f_{g}(\cdot,\cdot; \eta)$.
The classification loss $\mathcal{L}_{cls}(\cdot, \cdot)$ is a cross-entropy error, measuring the label similarity between prediction $\hat{y}$ and ground-truth $y$. The extraction loss $\mathcal{L}_{ext}(\cdot, \cdot)$ is also a cross-entropy error, quantifying the similarity between predicted extraction mask $\hat{\mathbf{M}}$ and ground-truth extraction mask $\mathbf{M}$. The registration loss $\mathcal{L}_{sim}(\cdot, \cdot)$ is a negative cross-correlation, assessing the image similarity between warped image $\mathbf{W}$ and target image $\mathbf{T}$ on voxel level.
The segmentation loss $\mathcal{L}_{seg}(\cdot, \cdot)$ is also a cross-entropy error, measuring the similarity between the predicted segmentation mask $\mathbf{R}$ and the warped segmentation mask $\mathbf{V}$. 
Bidirectional supervision envelops the entire network, ensuring positive forward propagation and controllable feedback across tasks. Unsupervised and one-shot guidance inside the model reduces reliance on high-cost annotations. 
$\alpha$, $\beta$, and $\gamma$ scale the loss terms to balance their impacts. The influence of these hyperparameters is discussed in Appendix~\ref{sec: Influence of Parameters}
\fi

\iffalse
The classification loss $\mathcal{L}_{cls}(\cdot, \cdot)$ measures the label similarity between prediction $\hat{y}$ and ground-truth $y$. This loss directs the classification network towards more accurate predictions and provides task-aware feedback for unsupervised brain network generation. Here, we utilize the cross-entropy loss.

The extraction loss $\mathcal{L}_{ext}(\cdot, \cdot)$ quantifies the similarity between predicted extraction mask $\hat{\mathbf{M}}$ and ground-truth extraction mask $\mathbf{M}$. This loss guides the precise extraction, ensuring the stability of all subsequent tasks and preventing error propagation. The mask-level similarity is measured by cross-entropy loss.

The registration loss $\mathcal{L}_{sim}(\cdot, \cdot)$ assesses the image similarity between warped image $\mathbf{W}$ and target image $\mathbf{T}$ on voxel level. This loss not only provides accurate registration but also leads to generating precise segmentation and parcellation masks, aiding subsequent tasks. The negative local cross-correlation loss is employed~\cite{balakrishnan2018unsupervised, zhao2019recursive}.

The segmentation loss $\mathcal{L}_{seg}(\cdot, \cdot)$ measures the similarity between the predicted segmentation mask $\mathbf{R}$ and the warped segmentation mask $\mathbf{V}$. This loss not only directs segmentation but also provides auxiliary information for registration, helping to identify anatomical correspondences. Here, we adopt cross-entropy loss.

$\alpha$, $\beta$, and $\gamma$ scale the numerical value of each loss term to the same order of magnitude, balancing their impacts. 
%The variation of these hyperparameters is shown in Figure~\ref{fig:stage dice}(a,b,c).

\fi

By leveraging the differentiability in each component of this design, our model achieves joint optimization in an end-to-end manner. All tasks are unified within a single model for collective learning, mutually boosting their performance with limited labels.
