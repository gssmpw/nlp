\section{Related Work}
In spike train classification, methods for training SNNs can be divided into two categories: ``Indirect" learning and ``Direct" learning. 
Indirect learning mainly focuses on ANN-to-SNN conversion \cite{diehl2015fast,cao2015spiking,hu2018spiking, midya2019artificial}.
These methods are indirect in that a regular non-spiking Artificial Neural Network (ANN), such as a multi-layer perceptron, is initially used during the training phase. 
At inference-time, the trained model is then converted to an SNN. However, such indirect training doesn't align well with how an SNN operates. For example, in ANNs, it does not matter if activations are negative, while firing rates in SNNs are always positive. As for Direct learning, many methods have recently been proposed \cite{shrestha2018slayer, wu2019direct, yin2021energy}. These approaches train SNNs directly using back-propagation in both the spatial and temporal domains. \cite{shrestha2018slayer,wu2019direct} have achieved state-of-the-art accuracy on the MNIST and N-MNIST datasets. 
\cite{yin2021energy} trains SNNs with a spatial sparsification technique that allows SNNs to perform inference with lower computational cost. While these methods perform better than the indirect methods on many neuromorphic datasets, they are still not suitable for efficient classification of real-world spike trains, especially in terms of temporal-noise problem. 

Although there is no model for dealing with our defined problem for SNNs, some methods have been proposed for dealing with similar problems for RNNs \cite{lalapura2021recurrent, lei2021attention, morais2021learning, zheng2021accurate, hartvigsen2020learning, jernite2016variable, campos2017skip, shen2018ordered}. 
For example, SkipRNN \cite{campos2017skip} extends classic RNN models by learning an additional controller network that learns to skip state updates. 
The input of this neuron is the state value of other neurons. So it can generate a binary value based on the sigmoid function value of its state. This model can significantly reduce the computational cost. 
However, while both RNNs and SNNs are used to deal with sequence analysis, they remain completely different. 
The main difference is that neurons in RNNs are mostly non-linear, continuous function approximators that operate on a common clock cycle, whereas the neurons in SNNs use asynchronous spikes that signal the occurrence of some characteristic event and temporally precise membrane potentials. When dealing with spiking training tasks, many neurons of an SNN may stay silent based on its mechanism, whereas all neurons of an RNN will be activated. 
Consequently, in terms of inference efficiency, RNNs are outmatched by SNNs for spike train classification. 
We illustrate this key difference in Figure~\ref{fig:intro2}.