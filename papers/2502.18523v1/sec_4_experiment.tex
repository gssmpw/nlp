%\section{Experiment and Analysis}


%\input{tab_res_ABIDE.tex}
\vspace{-5pt}
\section{Experiments}
\label{section:Experiments}

\subsection{Experimental Settings}

\noindent\textbf{Datasets.} We evaluate the effectiveness of our proposed method on the public real-world ADHD dataset with 3D brain sMRI~
\cite{adhd2012adhd}. The dataset contains records for 776 subjects, labeled as real patients (positive) and normal controls (negative). %Out of these, 599 subjects contain ground truth brain extraction and segmentation masks. T
The original dataset is unbalanced, following~\cite{kong2013discriminative}, we randomly sampled 100 ADHD patients and 100 normal controls from the dataset for performance evaluation.
Out of the 200 scans, 160 are used for training, 20 for validation, and 20 for testing. All scans are cropped and resized to $96\times96\times96$ dimensions.
%2) \emph{ABIDE}~\cite{tyszka2014largely} is collected from Autism Brain Imaging Data Exchange dataset. The dataset contains 1112 subjects, labeled as real patients and normal controls. Same to ADHD dataset, we randomly sampled 500 ASD patients and 500 normal controls from the dataset for evaluation. 
We use MNI 152 with the AAL atlas~\cite{tzourio2002automated} as the template image for registration and parcellation. 
%More details can be found in Appendix A.4. 

\iffalse
\noindent\textbullet\ \textit{LONI Probabilistic Brain Atlas (LPBA40)}~\cite{shattuck2008construction}: 
This dataset consists of 40 raw T1-weighted 3D brain MRI scans along with their brain masks. It also provides the corresponding segmentation ground truth of 56 anatomical structures. The brain mask and anatomical segmentations are used to evaluate the accuracy of extraction and registration, respectively. Same to~\cite{balakrishnan2018unsupervised, zhao2019recursive}, we focus on atlas-based registration, where the first scan is the target image and the remaining scans need to align with it. Among the 39 scans, we use 30, 5, and 4 scans for training, validation, and testing, respectively. All scans are resized to $96\times96\times96$ after cropping.


\noindent\textbullet\ \textit{Calgary-Campinas-359 (CC359)}~\cite{souza2018open}: 
This dataset consists of 359 raw T1-weighted 3D brain MRI scans and the corresponding brain masks. It also contains the labeled white matter as the ground truth. We use brain masks and white-matter masks to evaluate the accuracy of extraction and registration, respectively. Like LPBA40, we concentrate on atlas-based registration and split CC359 into 298, 30, and 30 scans for training, validation, and test sets. All scans are resized to $96\times96\times96$ after cropping.


\noindent\textbullet\ \textit{Internet Brain Segmentation Repository (IBSR)} \cite{rohlfing2011image}: 
This dataset provides 18 raw T1-weighted 3D brain MRI scans along with the corresponding manual segmentation results. 
Due to the small sample size, We use this dataset only to test the model trained on CC359. 
Thus, all 18 scans need to align with the first scan of CC359. All scans are resized to $96\times96\times96$ after cropping.
\fi


%\subsection{Compared Methods}
%\label{section:Compared Methods}

\noindent\textbf{Compared Methods.} We compare our UniBrain with several representative baselines. 1) Extraction: BET~\cite{smith2002fast} and SynthStrip~\cite{hoopes2022synthstrip}; 2) Registration: FLIRT~\cite{jenkinson2001global}, VM~\cite{balakrishnan2018unsupervised} and ABN~\cite{su2022abn}; 3) Segmentation and Parcellation: DW~\cite{jaderberg2015spatial}; 4) Network Generation~\cite{zhou2022sparse}; 5) Classification: GCN~\cite{kipf2017semi}, BGN~\cite{li2021braingnn} and BNT~\cite{kan2022brain}; 6) Partial Joint: DeepAtlas (Registration-Segmentation)~\cite{xu2019deepatlas}, ERNet (Extraction-Registration)~\cite{su2022ernet} and JERS (Extraction-Registration-Segmentation)~\cite{su2023one}. Notably, there are no existing solutions that can simultaneously perform all tasks in an end-to-end framework. Thus, for comparison, we designed a pipeline-based solution by combining different state-of-the-art methods for each task. 
The summary of baselines is shown in Table~\ref{tab:methods}.
%and the settings of each baseline are detailed in the Appendix A.6.

%\subsubsection{Experiment Setting.}
\vspace{1pt}
\noindent\textbf{Implementation.} Our experiments are
conducted on Ubuntu 20.04 LTS, utilizing an AMD EPYC
7543 CPU and an NVIDIA Tesla A100-80G GPU.
We split the datasets into training, validation, and test sets as introduced in the Datasets section. The training set is for learning model parameters, the validation set evaluates hyperparameter settings (\eg loss term weights), and the test set is used only once to report the final evaluation results. The code is implemented in Python 3.7.6, and the neural networks are built using PyTorch 1.7.1. The source code is available at~\url{https://github.com/Anonymous7852/UniBrain}.

\input{tab_time_res}

\input{tab_res_ADHD_CNN}

\subsection{Experimental Results}
We compare UniBrain with baseline methods in terms of extraction, registration, segmentation, parcellation, and classification accuracy and efficiency. Additionally, we evaluate UniBrain against voxel-based end-to-end brain imaging analysis solutions, which bypass brain network generation and rely solely on voxel-level information from images for predictions.
Experimental results show that: 1) UniBrain consistently outperforms other methods in extraction, registration, segmentation, parcellation, and classification, while also being time-efficient; 2) UniBrain also surpasses voxel-based end-to-end brain imaging analysis solutions. Similar results were also observed on the ABIDE~\cite{tyszka2014largely} datasets. Due to space constraints, we will present these findings in detail in a future journal publication.
%Additional experiments and evaluation metrics are detailed in Appendix A.2 and Appendix A.3.

%\subsubsection{Overall Results} 
%\label{sec: Overall Results}

%\input{fig_main_res_v2}

\vspace{1pt}
\noindent\textbf{{Overall Results.}} Table~\ref{tab:res ADHD} show the results of the compared methods and the proposed UniBrain in extraction, registration, segmentation, parcellation, and classification tasks. Based on the comprehensive evaluation on the public dataset, UniBrian outperforms existing methods in all tasks. 1) In extraction, we observed that joint-based extraction methods (ERNet, JERS and UniBrain) outperform single-stage extraction methods (BET and Synth). Specifically, UniBrain achieves up to a $5.4\%$ improvement in extraction dice scores over the best single-stage method Synth. 2) For the registration task, methods with strong extraction results typically yield better registration accuracy, highlighting the dependency of accurate registration on prior extraction quality. 3) Good registration enhances segmentation and parcellation performance, as these tasks rely on accurate registration. 4) Classification task results also reflect this trend, with higher parcellation accuracy (like Synth-based, JERS-based, UniBrain) yielding better outcomes due to the classification network leveraging parcellation masks for brain network construction. 
Overall, there's a clear interdependence among brain imaging analysis tasks, with strengths and errors propagating across them. Partially joint methods like ERNet, JERS, and DeepAtlas show improved performance in their joint tasks but are limited when combined with other separate models. In contrast, UniBrain, benefiting from full end-to-end joint learning, uniquely excels across all tasks.

%\noindent\textbf{Qualitative Analysis.} %Figure~\ref{fig:main result} visually compares the performance of our UniBrain with other methods on the ADHD test set. UniBrain is observed to excel in extraction, registration, segmentation, and parcellation tasks, corroborating the analysis presented in Table~\ref{tab:res ADHD}. Specifically, UniBrain's predicted brain extraction masks closely overlap with the ground truth, whereas other methods often include significant non-brain tissues in their predictions. For registration, UniBrain also achieves higher similarity between its final registered image and the target image. Crucially, inaccurate extraction methods result in irreversible errors that propagate into registration, where non-alignable non-brain tissues cannot be properly registered regardless of the method used. In segmentation and parcellation tasks, UniBrain continues to demonstrate superior performance, producing results that closely match the ground truth segmentation and parcellation masks. Overall, our findings highlight a clear trend: better extraction leads to better registration, which in turn enhances segmentation and parcellation outcomes.
%Figure~\ref{fig:main result} visually compares UniBrain with other methods on the ADHD test set, showing its superior performance in extraction, registration, segmentation, and parcellation tasks, as supported by Table~\ref{tab:res ADHD}. UniBrain's predicted brain extraction masks closely overlap with the ground truth, while other methods include significant non-brain tissues. For registration, UniBrain achieves higher similarity between its registered image and the target image. Inaccurate extraction methods lead to errors in registration, as non-brain tissues cannot be properly registered. In segmentation and parcellation, UniBrain produces results that closely match the ground truth masks. Overall, better extraction improves registration, enhancing segmentation and parcellation outcomes.

%\noindent\textbf{Ablation Study.} To study the effectiveness of each module in UniBrain, we design six variants of UniBrain for comparison, as shown in Table~\ref{tab: ablation ADHD}. We freeze the extraction (Ext), registration (Reg), segmentation (Seg), parcellation (Parc), brain network (BN), and classification (CLS) modules respectively, meaning bypassing network parameter updates and operations within these modules. Our findings include: 1) We reverify the strong interdependence among tasks, where the performance of preceding tasks directly impacts subsequent ones. 2) All modules are essential, as achieving optimal in all tasks requires including all modules without exception. 3) UniBrain has the potential to handle tasks beyond classification (\eg regression), as evidenced by the negligible impact on the performance of preceding tasks when the classification module is removed.

\vspace{1pt}
\noindent\textbf{Running Efficiency.} We measure the efficiency of UniBrain by comparing its inference time with other baselines. The measurement is made on the same device with an AMD EPYC 7543 CPU and an NVIDIA Tesla A100 GPU.
The running time is reported as the average processing time for each image in its corresponding
task.
As indicated in Table~\ref{tab: time ADHD}, fully separate methods are the slowest due to the need for individual optimization of each task. Partially joint learning methods demonstrate increased speed in their joined tasks but still require combination with other methods, limiting overall time efficiency.
UniBrain is the fastest method, which efficiently performs all tasks in an end-to-end manner on the same device, enhancing overall speed.

\vspace{1pt}
\noindent\textbf{Voxel-based End-to-End Learning.} We compare UniBrain with voxel-based end-to-end brain imaging analysis solution. In this experiments, we disregard graph-based models, relying only on voxel information from images for final classification predictions. 
We devised three groups: 1) Direct use of raw MRI images as input (including non-brain tissues, images in different coordinate spaces) for label classification. 2) Use of extracted brain images as input (still in different coordinate spaces) for label classification. 3) Use of the brain been extracted and registered to a standard space as input for classification. As shown in Table~\ref{tab:res ADHD 3D-CNN}, we observed that the performance is worse when using raw images as input due to the inclusion of non-brain tissues and spatial transformation noise. Images processed through extraction and registration yielded higher accuracy. UniBrain, integrating preprocessing and classification in a joint learning approach, outperformed all other models.


