\section{PRELIMINARIES} 


%In this section, we first introduce the relevant concepts and notations. We then formulate the problem of end-to-end learning for brain imaging tasks. Notations are detailed in Appendix~\ref{section: appendix Details of and Definitions Notations} and Table~\ref{tab: Notation}.

%\subsection{Notations and Definitions}
\noindent \textbf{Definition 1 (Training data).}
Given a training dataset $\mathcal{D} = \{(\mathbf{S}_i,\mathbf{M}_i,y_i)\}_{i=1}^{Z}$, along with a template $(\mathbf{T}, \mathbf{B},\mathbf{P})$. The dataset contains $Z$ source images $\mathbf{S}_{i} \in \mathbb{R}^{W \times H \times D}$ (\eg raw MRI), each paired with a brain extraction mask $\mathbf{M}_i \in \{0,1\}^{W \times H \times D}$ and a classification label $y_i \in \mathcal{Y}$. 
The template contains a target image $\mathbf{T} \in \mathbb{R}^{W \times H \times D}$, with its segmentation mask $\mathbf{B} \in \{0, 1\}^{C \times W \times H \times D}$ and parcellation mask $\mathbf{P} \in \{0, 1\}^{K \times W \times H \times D}$. Here, $W$, $H$, and $D$ denote the width, height and depth dimensions of the 3D images, $C$ denotes the number of segmentation labels (\ie the number of brain tissue types), $K$ denotes the number of brain regions (\ie the number of ROIs). $\mathcal{Y}$ is the classification label space (\eg $\{0, 1\}$ for binary classification). Next, we omit the subscript $i$ of $\mathbf{S}_i$, $\mathbf{M}_i$ and $y_i$ for simplicity.

\noindent \textbf{Definition 2 (Outputs of each task).} 1) \textit{The extraction task} outputs a binary extraction mask $\hat{\mathbf{M}}\in \{0,1\}^{W \times H \times D}$, representing cerebral tissues in source image $\mathbf{S}$ with a value of 1 and non-cerebral tissues with 0. The extracted image $\mathbf{E} = \mathbf{S} \circ \hat{\mathbf{M}}$ is obtained by applying $\hat{\mathbf{M}}$ on $\mathbf{S}$ via an element-wise product $\circ$. 2) \textit{The registration task} outputs a 3D affine transformation matrix $\mathbf{A} \in \mathbb{R}^{4 \times 4}$,
%with 12 degrees of freedom (\ie 3 for rotation, 3 for translation, 3 for scale, and 3 for shear),
indicating the coordinate correspondence between the extracted image $\mathbf{E}$ and the target image $\mathbf{T}$. The warped image (\ie registered image) $\mathbf{W} = \mathcal{T}\left(\mathbf{E},\mathbf{A}\right)$ results from applying the affine transformation on the extracted image $\mathbf{E}$, where $\mathcal{T}(\cdot, \cdot)$ is the affine transformation operator. 3) \textit{The segmentation task} outputs a multi-class segmentation mask $\mathbf{R} \in \{0,1\}^{C \times W \times H \times D}$, categorizing tissue types within the source image $\mathbf{S}$. 
4) \textit{The parcellation task} outputs a multi-class parcellation mask $\mathbf{U} \in \{0,1\}^{K \times W \times H \times D}$, subdividing various regions (ROIs) within the source image $\mathbf{S}$. The parcellated image $\mathbf{F} = \mathbf{S} \circ \mathbf{U}$ is obtained by performing element-wise product $\circ$ between $\mathbf{S}$ and $\mathbf{U}$.
5) \textit{The network generation task} outputs an adjacency matrix $\mathbf{C} \in \mathbb{R}^{K \times K}$ and a 
node feature (\ie ROI feature) matrix  $\mathbf{H} \in \mathbb{R}^{K \times N}$, capturing the brain network connectivity in the source image $\mathbf{S}$ across $K$ ROIs. $N$ is the node feature vector length. 6) \textit{The classification task} outputs a categorical label $\hat{y} \in \mathcal{Y}$, indicating the predictive category.

\noindent \textbf{Problem Formulation.}
Given the limited labeled data and heterogeneous inputs and outputs across tasks, we aim to minimize the need for extensive task-specific labels and enable efficient knowledge transfer across different tasks. Formulating the end-to-end brain imaging analysis as a joint learning problem, we allow interconnected and collective task optimization. Specifically, we designed multiple learnable functions to bridge tasks, while loss terms are jointly optimized by leveraging the limited labeled information. %Without loss of generality, we assume that the transformation in the registration task is affine-based. However, this work can be easily extended to other types of registration, \eg nonlinear/deformable registration.

Formally, our learnable functions are expressed as: the extraction function $f_{\theta}: \mathbb{R}^{W \times H \times D} \rightarrow \mathbb{R}^{W \times H \times D} ; \mathbf{S} \mapsto \hat{\mathbf{M}}$, the registration function $g_{\phi}: \mathbb{R}^{W \times H \times D}\times \mathbb{R}^{W \times H \times D} \rightarrow \mathbb{R}^{4 \times 4} ; (\mathbf{E},\mathbf{T}) \mapsto \mathbf{A}$, the segmentation function $h_{\psi}: \mathbb{R}^{W \times H \times D} \rightarrow \mathbb{R}^{C \times W \times H \times D} ; \mathbf{S} \mapsto \mathbf{R} $, the brain network generation function $n_{\xi}: \mathbb{R}^{K \times W \times H \times D} \rightarrow \mathbb{R}^{K \times N} ; \mathbf{F} \mapsto \mathbf{H} $ and the classification function $c_{\eta}: (\mathbb{R}^{K \times K}, \mathbb{R}^{K \times N}) \rightarrow \mathcal{Y} ; (\mathbf{C},\mathbf{H}) \mapsto \hat{y}$. 
1) \textit{The extraction function} $f_{\theta}(\cdot)$ takes the source
image $\mathbf{S}$ as input to predict the extraction mask $\hat{\mathbf{M}} = f_{\theta}(\mathbf{S})$. 
2) \textit{The registration function} $g_{\phi}(\cdot, \cdot)$ takes the extracted brain image $\mathbf{E} = \mathbf{S} \circ \hat{\mathbf{M}}$ and the target image $\mathbf{T}$ to predict the affine transformation $\mathbf{A} = g_{\phi}(\mathbf{E},\mathbf{T})$, and thereby generating the warped image $\mathbf{W} = \mathcal{T}(\mathbf{E},\mathbf{A})$. Then, the warped segmentation mask $\mathbf{V} = \mathcal{T}(\mathbf{B},\mathbf{A}^{-1})$ 
and parcellation mask $\mathbf{U} = \mathcal{T}(\mathbf{P},\mathbf{A}^{-1})$ are obtained by applying the inverse affine transform $\mathbf{A}^{-1}$ to target segmentation mask $\mathbf{B}$ and parcellation mask $\mathbf{P}$. 
3) \textit{The segmentation function} $h_{\psi}(\cdot)$ predicts source segmentation mask $\mathbf{R} = h_{\psi}(\mathbf{S})$ from the source image $\mathbf{S}$. 
4) \textit{The brain network generation function} $n_{\xi}(\cdot)$ processes parcellated image $\mathbf{F}=\mathbf{S} \circ \mathbf{U}$ to learn the node feature $\mathbf{H} = n_{\xi}(\mathbf{F})$. 
5) \textit{The classification function} $c_{\eta}(\cdot,\cdot)$ takes a learnable adjacency matrix $\mathbf{C} = \mathbf{H} \mathbf{H^\top}$, and node feature $\mathbf{H}$ to make the final prediction $\hat{y}=c_{\eta}(\mathbf{C},\mathbf{H})$.
The optimal parameter set $\mathcal{P^*} = \{\theta^*, \phi^*, \psi^*, \xi^*, \eta^*\}$ can be found by solving the following optimization problem:
\begin{equation}
\label{eq:optimization}
\begin{aligned}
\mathcal{P^*} = \underset{\mathcal{P}}{\arg \min} \sum_{(\mathbf{S}, \mathbf{M}, y)\in \mathcal{D}} \Big[ \mathcal{L}_{cls}\big(\hat{y}, y\big)  & + \alpha \mathcal{L}_{ext}\big(\hat{\mathbf{M}}, \mathbf{M}\big) \\   + \beta  \mathcal{L}_{sim}\big(\mathbf{W}, \mathbf{T}\big)   & + \gamma \mathcal{L}_{seg}\big({\mathbf{R}}, \mathbf{V}\big)\Big],
\end{aligned}
\end{equation}
where the image pair $(\mathbf{S},\mathbf{M},y)$ is sampled from the training dataset $\mathcal{D}$. $\mathcal{L}_{cls}(\cdot, \cdot)$ is classification loss term, $\mathcal{L}_{ext}(\cdot, \cdot)$ is extraction loss term, $\mathcal{L}_{sim}(\cdot, \cdot)$ is image dissimilarity loss term (\eg mean square error), and $\mathcal{L}_{seg}(\cdot, \cdot)$ is segmentation loss term. 
These four criteria leverage the limited labeled information, and guide a joint optimization of extraction, registration, segmentation, parcellation, network generation, and classification, enabling effective interaction and feedback among these tasks.


%To our knowledge, this work is the first endeavor to find an optimal solution for the end-to-end brain imaging analysis problem. Our method unifies all tasks and significantly reduces the need for extensive annotations in brain imaging by utilizing minimal labeled data (\ie extraction mask, classification label, and one labeled template), as opposed to other fully supervised~\cite{sokooti2017nonrigid, dai2020dual,akkus2017deep, kamnitsas2017efficient,chen2018voxresnet,thyreau2020learning,lim2022deepparcellation,zhao2020supervised,zhao2015supervised} and pipeline-based~\cite{smith2002fast,cox1996afni,shattuck2002brainsuite,segonne2004hybrid,balakrishnan2018unsupervised,zhao2019recursive,su2022abn} methods.

