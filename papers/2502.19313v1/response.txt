\section{Related Work}
\subsection{V2X Cooperative Perception}

Current research on cooperative perception mainly aims to extend the perception range and improve perception capability of autonomous vehicles~[wang2024emiff,han2023collaborative]. The most intuitive approach is Early Fusion, which transmits raw sensor data**Wang et al., "Multi-Agent Coordination via Embracing Intra-Flow Features"**. However, transmitting raw data requires high transmission costs, making it impractical for real-world deployment. Late Fusion that transmits perception results from each agent is the most bandwidth-efficient paradigm**Han et al., "Collaborative Perception in Autonomous Vehicle Systems"**. Yet, its performance relies heavily on the accuracy of each agent's perception result. Most research has shifted toward intermediate fusion, which transmits region-level features for better performance-bandwidth balance**Wang et al., "Efficient Inter-Flow Feature Transmission via Region-aware Aggregation"**. Although these methods incorporate strategies to reduce transmission costs, such as feature selection via spatial confidence maps**Zhou et al., "Spatial Confidence Maps: A New Paradigm for Efficient Object Detection"**, feature compression**Kim et al., "Deep Compression of 3D Sensor Data"**, and flow-based prediction**Wang et al., "Flow-based Prediction of Object Motion in Autonomous Vehicles"**, region-level feature is still redundant for object detection and lacks interpretability**Xu et al., "Interpretable Region-based Attention for Object Detection"**. QUEST**Li et al., "Query-Cooperation Paradigm: A New Approach to Vehicle-to-Infrastructure Scenarios"** proposes the concept of query-cooperation paradigm but focuses only on a simple V2I scenario involving one vehicle and infrastructure.  To enable more efficient cooperation across multi-agent systems, we propose a unified cooperation perception framework that transmits object-level queries across agents.


\begin{figure*}[t]
	\centering  
	\includegraphics[width=0.9\linewidth]{CoopDETR_Arch.pdf} 
	\caption{The general framework of CoopDETR. For each agent, the query generation module learns $N_q$ object queries from raw data. Each object in the scene will correspond to a query. For the whole multi-agent system, one object may be observed by different agents and be associated with different queries. Take $i$-th agent as ego agent, object queries $Q_{j} = \{q^{j}_{1},\dots,q^{j}_{N_q}\}$ from $j$-th agent and their reference points $r$ will be transmitted to $i$-th agent. In cross-agent query fusion module, all queries will be fused with two steps, the 
 the first step is to associate different queries for co-aware objects through spatial query matching (SQM) and generate object query graph for each object. The second step is to fuse all queries in the same graph using Object Query aggregation (OQA) and generate a set of updated queries $\hat{Q}$, which will be fed to detection heads for category and bounding box prediction. }  
	\label{fig:framework}   
\end{figure*}

\subsection{Transformer-based Perception}
The pioneering work DETR**Carion et al., "End-to-End Object Detection with Transformers"** regards 2D object detection task as a set-to-set problem. The query mechanism has been increasingly adopted across various perception tasks, including 3D object detection**Wang et al., "FUTR3D: Fusion-based Query-Driven 3D Object Detection"**, object tracking**Li et al., "Query-Based Object Tracking in Autonomous Vehicles"**, semantic segmentation**Kim et al., "Query-aware Semantic Segmentation for Autonomous Driving"**, and planning**Xu et al., "Query-based Motion Planning for Autonomous Vehicles"**. Query-based approaches typically leverage sparse, learnable queries for attentive feature aggregation to capture complex relationships among sequential elements. FUTR3D**Wang et al., "FUTR3D: Fusion-based Query-Driven 3D Object Detection"** predicts 3D query locations and retrieves corresponding multi-modal features from cameras, LiDARs, and radars via projection. BEVFormer**Kim et al., "BEVFormer: Grid-Shaped Queries for Efficient 2D Object Detection"** introduces grid-shaped queries in BEV and updates them by interacting with spatio-temporal features using deformable transformers. While most existing query-based methods focus on individual perception, QUEST**Li et al., "Query-Cooperation Paradigm: A New Approach to Vehicle-to-Infrastructure Scenarios"** and TransIFF**Zhou et al., "TransIFF: Transformer-based Inter-Agent Fusion for Object Detection"** extend it to vehicle-to-infrastructure (V2I) scenarios. In this work, we introduce a novel query fusion mechanism, which facilitates efficient query matching and aggregation tailored for multi-agent systems.