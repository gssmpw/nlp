\documentclass[11pt]{article}
\renewcommand{\baselinestretch}{1.2}
\linespread{1.2}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{makecell}
\usepackage{natbib}
\setlength{\bibsep}{2pt}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{float}
\usepackage{mathrsfs}
%\usepackage{enumitem}
\usepackage{setspace}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes}
\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{tabularx} % for 'tabularx' env. and 'X' col. type
\usepackage{ragged2e} % for \RaggedRight macro
\usepackage{rotating} % display table vertically
%\textwidth=175mm
%\textheight=238mm
%\voffset=-2cm
%\hoffset=-2cm
\usepackage[margin=1in]{geometry}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
%\def\proof{{\bf Proof.}\quad}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\RequirePackage{lineno}

\providecommand{\keywords}[1]
{
    \small
    \textbf{\textit{Keywords---}} #1
}
\begin{document}
%    \linenumbers
    \title{\bf Two-stage hybrid models for enhancing forecasting accuracy on heterogeneous time series}
\author{Junru Ren, {Shaomin Wu}\footnote{\it Corresponding author. E-mail: s.m.wu@kent.ac.uk. Telephone: +44 (0)1227 827940}\\
\small{\it Kent Business School, University of Kent, Canterbury, Kent CT2 7FS, UK}}

    \date{}
    \maketitle
    %	\linenumbers
    \begin{center}{\bf Abstract}\end{center}
   Compared to local models built in a series-by-series manner, global models leverage relevant information across series, resulting in improved forecasting performance and generalization capacity. Constructing global models on a set of time series is becoming mainstream in the field of time series forecasting. However, the advantages of global models may not always be realized when dealing with heterogeneous data. While they can adapt to heterogeneous datasets by increasing the model complexity, the model cannot be infinitely complex due to the finite sample size, which poses challenges for the application of global models. Additionally, determining whether the data is homogeneous or heterogeneous can be ambiguous in practice. To address these research gaps, this paper argues that the heterogeneity of the data should be defined by the global model used, and for each series, the portion not modelled by the global model represents heterogeneity. It further proposes two-stage hybrid models, which include a second stage to identify and model heterogeneous patterns. In this second stage, we can estimate either all local models or sub-global models across different domains divided based on heterogeneity. Experiments on four open datasets reveal that the proposed methods significantly outperform five existing models, indicating they contribute to fully unleash the potential of global models on heterogeneous datasets. 

    {\it Keywords}: Global forecasting models, Heterogeneous datasets, Hybrid models, Model-based heterogeneity identification
    %\end{abstract}

    \date{}
    \maketitle

    \section{Introduction}
    \subsection{Motivations}
    Accurate forecasting enables organizations to plan future operations and facilitates data-driven decision-making in business, healthcare, finance, and other industries \citep{petropoulos2022forecasting}. Recently, a series of global forecasting models have been proposed successively, which are estimated on a group of relevant demand series \citep{salinas2020deepar}. Global models are becoming dominated in the research field of time series forecasting due to the development of big data technologies, compared with local forecasting models estimating in a series-by-series fashion. Making use of cross-learning, global models effectively reduce generalization errors and demonstrate remarkable capability especially when time series is too short to estimate a sound model \citep{Montero-Manso2021, Semenoglou2021}.
    
    The key to the advantages of developing a global model lies in the assumption that the time series in the given dataset are homogeneous, namely, they share similar/relevant demand patterns. The heterogeneity of the data may stem from differences in seasonal patterns, trends, or underlying data structures \citep{Neubauer2024}. Intuitively, it is challenging to build one single model that performs well across the entire dataset when heterogeneity exists. \citet{wellens2023and, hewamalage2022global} conducts extensive simulations on global models with various data scenarios. They used different data generating processes to simulate the heterogeneity, and found that the performance of global models is closely related to the homogeneity / heterogeneity of series and complex global models such as Recurrent Neural Networks (RNNs) and Light Gradient Boosting Models (LGBM) can handle heterogeneity better due to their non-linear modelling capabilities. It can be seen that a more sophisticated global model is needed to simultaneously describe multiple very heterogeneous time series, however, the model cannot be infinitely complex because of limited data volume. The potentiality of the global model is constrained by the heterogeneity of time series, while constructing local models on each time series independently completely overlooks the shared properties of this domain \citep{Neubauer2024}. This further prompts some interesting questions. For example, how to determine the level of homogeneity / heterogeneity of the dataset and how to fully leverage the strengths of the global model based on heterogeneous time series. 
    
    To address the problem of insufficient localization caused by one entirely global model, clustering techniques are commonly used in heterogeneous time series forecasting, such as distance-based clustering \citep{Godahewa2021} and feature-based clustering \citep{Bandara2020}. They are applied to divide the entire dataset into several sub-groups where time series are regarded as homogeneous, and then the sub-global model is constructed per sub-group. Besides, there is also some work considering both local and global components, and hybrid models are proposed. \citet{Smyl2020} used exponential smoothing to calculate and remove the local level and seasonality of each series and then long short-term memory (LSTM) networks were estimated on the remaining parts of the series globally. The aim is to extract and separate non-homogenous local and homogenous global patterns.
    
    The methods mentioned above determine whether data are homogenous or heterogeneous by analyzing the features presented in the time series. In other words, they assess whether the time series look alike, if so, they are homogeneous, otherwise, they are heterogeneous. In fact, this provides a very strict definition of time series homogeneity. Because sometimes although the time series don’t appear similar, they may have similar input-output relationships when forecasting, and thus global models are still applicable in this case. Therefore, the homogeneity / heterogeneity of series is also relevant to the forecasting model, depending on whether the model can capture underlying relationships. For instance, traditional autoregressive (AR) models can only describe linear relationships while neural networks (NNs) are able to carry out more complex non-linear modelling, thus, the time series presenting heterogeneity using AR models may be homogeneous when NNs are utilized.
    
    This motivates our research. This paper intends to highlight a concept of model-based heterogeneity identification and proposes two-stage modelling approaches. The heterogeneity of the time series is identified by a global model at the first stage, and then the heterogenous parts are modelled at the second stage after excluding homogeneity.
    
    \subsection{Related work}
    \subsubsection{Clustering-based models}
    Clustering-and-then-model is the mainstream practice in dealing with heterogeneous time series forecasting. \citet{Bandara2020, Semenoglou2021} exploited $k$-means algorithms and series features on trends, seasonality, and autocorrelation to conduct feature-based clustering. The series within each cluster are subsequently employed to train a cluster-specific model. \citet{Godahewa2021} investigated feature-based clustering, distance-based clustering, and random clustering, where dynamic time warping (DTW) distances were considered. They trained multiple global models for each cluster of the series by changing the number of clusters and cluster seeds and an ensemble model was constructed to generate final forecasts. \citet{Chen2024} added one adaptable Channel Clustering Module in the neural network and realized dynamic Euclidean distance-based clustering using radial basis function kernels to measure the series similarities. \citet{Froehwirth-Schnatter2008} utilized model-based clustering and they assumed the distributions of the data are obtained by AR processes. The forecasting model is integrated into the clustering, but cluster-specific models and the number of clusters have to be given in advance. While \citet{Neubauer2024} considered a model-and-then-clustering mechanism and proposed an algorithm named TSAVG. Specifically, local models are first estimated on each time series independently, and then DTW distances were calculated to determine neighbors of the target series. The forecasting model of the target series is the average of local models built on its neighbors.
    
    \subsubsection{Local-global hybrid models}
    To take advantage of superiority of both local and global models, some local-global hybrid models have been introduced. \citet{Semenoglou2021} averaged the forecasts obtained by a traditional Theta method and a global model using equal weights. Alternatively, some outputs of the local model can be used as inputs fed into the global model, such as the last fitted value or the running level of the series specified by local exponential smoothing \citep{Semenoglou2021}. Besides, \citet{Smyl2020} proposed a hybrid method of exponential smoothing and RNNs, that is, local characteristics of each series were specified using exponential smoothing methods and then a global RNN was used to model the remaining homogeneous parts shared by the entire dataset. This hybrid model won the first place of the M4 forecasting competition \citep{Makridakis2020}.
    
    There is also some work combining linear models such as exponential smoothing and autoregressive integrated moving average (ARIMA) methods and non-linear models such as neural networks parallelly or serially, see \citet{Hajirahimi2019, Zhang2003}. However, these hybrid models are built on one single time series instead of a dataset containing multiple relevant series, which are out of scope of this paper.
    
    \subsubsection{Error correction models}
    Error correction is a technique to improve forecasting accuracy by residual modelling. It adds a correction procedure after the classical forecasting approach, during which the remaining residuals are modelled recursively until they are while noises. \citet{Firmino2015} used ARIMA approaches to recursively correct the forecasts obtained by neural networks, and discussed additive error models and multiplicative error models. While \citet{Silva2019} considered conducting linear models such as ARIMA first and then using non-linear models including Support Vector Regression (SVR) and LSTM to correct errors and improve accuracy.
    
    Although the practice has similarities to the two-stage model proposed in this paper, the error correction is only applied in the scenario of one single time series and emphasizes a recursive combination of a series of linear and non-linear models. Its aim is to correct errors and adjust forecasts while the aim of this paper is to identify and model heterogeneity among multiple time series.

    \subsection{Novelty and contributions}
    Based on the knowledge gaps identified above, this paper intends to point out the heterogeneity of the dataset can be identified by the estimated global model, and a second stage should be added after the global modelling to describe the heterogeneity so as to harness the advantages of global forecasting models fully.
    
    The novelty and contributions of this paper include: 
    \begin{itemize}
     \item For the application of global forecasting models, it demonstrates and clarifies model-based heterogeneity identification;
     \item It proposes two-stage modelling methods to first identify and then model heterogeneous patterns of each time series;
     \item When excessive heterogeneous time series are identified conditioned on the estimated global model, it conducts residual-based domain division and builds domain-specific sub-global neural networks based on the global model constructed in the first stage. As such, both global information and sub-global information are involved.
    \end{itemize}

    To the best of our knowledge, this is the first work solving the global modelling on heterogenous data from the perspective of model-based identification and a second-stage heterogeneity modelling.

    \subsection{Overview}
    The remainder of the paper is structured as follows. Section \ref{sec:models} develops the two-stage hybrid models and Section \ref{sec:case_studies} provides a description of datasets and baseline models and presents the experimental results. Section \ref{sec:conclusions} concludes the paper.
    
    \section{Two-stage hybrid models}\label{sec:models}
    \begin{figure}
    	\centering
    	\includegraphics[width=1.1\textwidth]{two-stage_models}
    	\caption{The flow chart of building two-stage hybrid models}
    	\label{fig:model_flow_chart}
    \end{figure}

    The proposed two-stage hybrid models are illustrated in Figure \ref{fig:model_flow_chart}. Given $n$ time series denoted by $x_{i, t}$ (where $i \in \{1, 2, \cdots, n\}, t \in \{t_0^{(i)}, t_0^{(i)}+1, \cdots\}$), in the first stage, one global neural network is constructed based on the entire dataset, which is denoted as $G$, to capture as many common features as possible that all series share. Residuals are calculated using either subtraction or division, depending on whether the model is constructed as additive or multiplicative one. If the residuals are checked as white noises, there is no autocorrelation left in the residuals, indicating the constructed global model is statistically sufficient to model the corresponding time series. Otherwise, there are still special local features that cannot be described by the global model. Thus, the unmodelled portion is identified and considered heterogeneous. The heterogeneity identification is model-specific.
    
    Here, we further define the degree of heterogeneity of a given dataset as the ratio of the number of heterogeneous series identified by $G$ and the total number of this set of time series, denoted by $r_h$. That is, $r_h = r_h | (\rm{data}, G) = \frac{\# h}{\# x} = \frac{\# h}{n}$. Larger $r_h$ means severer heterogeneity conditioned on data and the constructed global model $G$.
    
    For heterogeneous time series, building local models on the residuals of each series can effectively extract all remaining patterns, however, the model complexity dramatically increases when the number of heterogeneous time series is excessively large, and thus it is more time-consuming and has higher computational requirements. Therefore, Criterion 1 is set to check if the computing power allows all local models to be built on identified heterogeneous series. If so, individual local models, such as ARIMA, are constructed on each heterogeneous series to supplement the forecasts generated by $G$ in the first stage. Theoretically, the $r_h$ can be updated to 0 now.
    
    If not, multiple sub-global models are estimated on sub-groups of heterogeneous series respectively. First of all, different domains are divided according to the features presented by residual series (namely, the identified heterogeneity of series) such as their autocorrelation coefficients, spectral entropy, lumpiness and nonlinearity, etc. The heterogenous series with residuals that show similar features are divided into one domain. Then for each domain, an individual global model is trained via feeding the original time series that belongs to this domain, which is denoted as $G_D$. The structure of $G_D$ is presented in Figure \ref{fig:model_flow_chart}. Assume the model $G$ has $L$ layers, the first $L-1$ layers of $G_D$ are directly borrowed from $G$ and corresponding weights are frozen and thus the outputs of these layers capture global features shared by the entire dataset, and then the inputs of $G_D$ are added into the aforementioned outputs to incorporate domain-specific global information. More layers are added into $G_D$ to obtain the final forecasts. After building all domain-specific global models, an updated $r_h$ can be calculated and Criterion 2 is set to check if $r_h$ is smaller than $r_h^a$ which is set in advance and represents an acceptable limit of degree of heterogeneity. The features used to conduct the domain division and the number of domains can be adjusted until $r_h$ is within the acceptable range.
   
    \section{Case studies}\label{sec:case_studies}
    \subsection{Datasets}
    Four open datasets are used to evaluate the proposed two-stage models. \textbf{Tourism} dataset comprises 366 monthly time series used in the tourism forecasting competition and is publicly available through the \textit{Tcomp} R package \citep{ellis2018tcomp}. The length of time series in this dataset is in the range of 91-333 with an average of 299; \textbf{M3} dataset comprises 1,376 monthly time series from the M3 forecasting competition, divided into five subcategories: micro, macro, industry, demographic, and finance. This dataset is provided in the R package \textit{Mcomp} \citep{hyndman2018package}. The length varies from 66 to 144 with an average of 119; \textbf{CIF 2016} dataset contains monthly data from the CIF 2016 forecasting competition, including 24 real-world series from the banking domain and 48 artificially generated series. The length of time series in this dataset is in the range of 28-120 with an average of 99. The dataset can be obtained in \citet{Neubauer2024}; \textbf{Hospital} dataset consists of 767 monthly count time series tracking number of patients for various medical products and medical problems. All time series have a length of 84 and it is publicly available from R package \textit{expsmooth} \citep{Hyndman2015}.
    
    The time series in these datasets present various characteristics of non-stationarity, including non-linear trends and seasonality. The problem of distribution shift prevents the predictability of time series. \citet{kim2021reversible} proposed reversible instance normalization (RevIN) to conduct forecasting on non-stationary time series, which applies normalization with learnable parameters to an instance of time series and restores the statistical information of the corresponding outputs. Subsequently, \citet{liu2022non} experimentally found that this normalization-and-denormalization method is also effective without learnable parameters and named this revised design as Series Stationarization. We apply Series Stationarization to conduct data pre-processing and post-processing. Concretely, normalization is carried out on each sliding window over the temporal dimension. For one instance $\boldsymbol{x}=[x_{t-1}, \cdots, x_{t-p}]$, the normalization can be formulated as $\boldsymbol{x}^{\prime} = \frac{\boldsymbol{x} - \mu_x}{\sigma_x}$, where $\mu_x = \frac{1}{p}\sum_{i=1}^{p}x_{t-i}$ and $\sigma_x = \frac{1}{p}\sum_{i=1}^{p} (x_{t-i}-\mu_x)^2$. Suppose that the forecasting horizon is $\tau$, the corresponding forecasts $\boldsymbol{y}^{\prime} = [\hat{x}_t^\prime, \cdots, \hat{x}_{t+\tau-1}^\prime]$ are obtained through inputting instance $\boldsymbol{x}$ into the constructed model. Then denormalization transforms $\boldsymbol{y}^{\prime}$ into the eventual forecasting results using $\boldsymbol{y} = [\hat{x}_t, \cdots, \hat{x}_{t+\tau-1}] = \boldsymbol{y}^{\prime} \cdot \sigma_x + \mu_x$.
 
    \subsection{Model comparison}
    \subsubsection{Baselines}   
    The following baseline models are considered for model comparison. 
    \begin{itemize}
    	\item \textbf{TSAVG}: \citet{Neubauer2024} proposed TSAVG methods. They utilized $k$-nearest neighbor algorithms to form a neighborhood of each time series with the similarity measure of DTW distances, and simple models such as exponential smoothing can be built per series and the forecast is improved by averaging in its neighborhood. Different averaging approaches were performed, including simple average, distance-weighted average and error-weighted average. 
    	\item \textbf{ARIMA}: Local ARIMA models are estimated on each time series \citep{Shumway2000}, and R package \textit{forecast} provides \textit{auto.arima()} to identify the optimal orders by maximizing likelihood or minimizing conditional sum-of-squares \citep{Hyndman2008}.
    	\item \textbf{Pooled AR}: A pooled AR($p$) model can be constructed on the entire dataset as a global model, which is formulated as $x_{i, t} = \beta_0 + \beta_1 x_{i, t-1} + \cdots + \beta_p x_{i, t-p} + \epsilon_{i, t}$, where $i \in \{1, 2, \cdots, n\}$ denoting $i$-th time series and $t$ denotes time, and $\beta_0, \beta_1, \cdots, \beta_p$ are unknown parameters to be estimated and $\epsilon_{i, t}$ are disturbances. The order $p$ is determined following the practice of \citet{Neubauer2024} and the parameters are fitted by ordinary least squares. The pooled AR model is a commonly-used baseline model since it exploits all information pertaining to each individual and doesn’t involve advanced machine learning algorithms \citep{Montero-Manso2021}. 
    	\item \textbf{Multilayer Perceptron (MLP)}: MLP neural networks compose of multiple layers of nodes, including an input layer, one or more hidden layer(s), and an output layer \citep{Murtagh1991}. It gains popularity in forecasting due to its ability to model nonlinear relationships \citep{Etemadi2023}. Here, we consider dense layers, and the used activation function is the tanh, and the loss function is defined as the mean squared loss.
    	\item \textbf{LSTM}: \citet{hochreiter1997long} introduced the LSTM in response to the problem of long-term dependencies that RNNs are not capable of describing in practice. Figure \ref{fig: LSTM} presents the structure of a typical LSTM. A cell state $\boldsymbol{C}_t$ running through the neural network at different time steps allows the LSTM to forget or store long-term memory. The LSTM utilizes three types of gate layers, namely, the forget gate, the input gate and the output gate, to update the cell state $\boldsymbol{C}_t$ and the output hidden state $\boldsymbol{h}_t$. The formulas of these gate layers are given as follows.
    	\begin{equation}
    	\begin{split}
    	\mathrm{Forget \ Gate}: & \quad \boldsymbol{f}_t=\sigma(\boldsymbol{W}_f[\boldsymbol{h}_{t-1},\boldsymbol{x}_t ]+\boldsymbol{b}_f ).\\
    	\mathrm{Input \ Gate}: & \quad \boldsymbol{i}_t=\sigma(\boldsymbol{W}_{i}[\boldsymbol{h}_{t-1},\boldsymbol{x}_t]+\boldsymbol{b}_i), \\
    	& \quad \boldsymbol{C}_t^\sim=\tanh (\boldsymbol{W}_C[\boldsymbol{h}_{t-1},\boldsymbol{x}_t]+\boldsymbol{b}_C), \\
    	& \quad  \boldsymbol{C}_t=\boldsymbol{f}_t \odot \boldsymbol{C}_{t-1}+\boldsymbol{i}_t \odot \boldsymbol{C}_t^\sim, \\
    	\mathrm{Output \ Gate}: & \quad \boldsymbol{o}_t=\sigma(\boldsymbol{W}_o[\boldsymbol{h}_{t-1},\boldsymbol{x}_t]+\boldsymbol{b}_o), \\
    	& \quad \boldsymbol{h}_t=\boldsymbol{o}_t \odot \tanh(\boldsymbol{C}_t),
    	\end{split}
    	\notag
    	\end{equation}
    	where $\boldsymbol{x}_t$ is the input variable; and $\sigma()$ and $\rm tanh()$ represent the sigmoid and the tanh activation functions, and $\boldsymbol{W}_{\cdot}, \boldsymbol{b}_{\cdot}$ stands for the weight matrix and the bias of the neural network layer, respectively, and the operator $\odot$ is the element wise multiplication.
    	\begin{figure}[H]
    		\centering
    		\includegraphics[width=0.5\textwidth]{LSTM}
    		\caption{The structure of a typical LSTM}
    		\label{fig: LSTM}
    	\end{figure}
    Here, we consider LSTM layers with appropriate dropout followed by dense layers and the loss function is the mean squared loss.
    \end{itemize}

    
    \subsubsection{Evaluation metrics}
    Following the practice of  \citet{Neubauer2024}, we focus on one-step-ahead forecasting and use cumulative errors as evaluation metrics. The cumulative one-step-ahead Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Symmetric Mean Absolute Percentage Error (sMAPE) have the following forms:
    
    \begin{equation}
    {\rm RMSE} = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{n_{test}^{(i)}}\sum_{\tau=1}^{n_{test}^{(i)}}\sqrt{\frac{1}{\tau}\sum_{t=T}^{T+\tau-1}(x_{i, t}-\hat{x}_{i,t})^2},
    \end{equation}
    \begin{equation}
    {\rm MAE} = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{n_{test}^{(i)}}\sum_{\tau=1}^{n_{test}^{(i)}}\frac{1}{\tau}\sum_{t=T}^{T+\tau-1}|x_{i, t}-\hat{x}_{i,t}|,
    \end{equation}
    \begin{equation}
    {\rm sMAPE} = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{n_{test}^{(i)}}\sum_{\tau=1}^{n_{test}^{(i)}}\frac{2}{\tau}\sum_{t=T}^{T+\tau-1} \frac{|x_{i, t}-\hat{x}_{i,t}|}{|x_{i, t}|+|\hat{x}_{i,t}|},
    \end{equation}
    where $n$ is the total number of time series, and $n_{test}^{(i)}$ is the length of the test subset of the $i$-th time series, and $T$ is the forecasting origin, and $\hat{x}_{i,t}$ is the forecast of observation $x_{i, t}$ of the $i$-th time series at the time step $t$.
    
    \subsubsection{Experimental results}
   The training and test datasets are split according to the split of historical data and future data given in these four datasets. For the TSAVG, ARIMA and Pooled AR models, the experiments are implemented consistently with the settings of \citet{Neubauer2024}. While building MLP and LSTM neural networks, a validation dataset is needed to avoid overfitting and select the optimal hyperparameters. One subset of the training dataset is split as the validation dataset and the length is set as 10\% of the entire time series. To achieve more reliable and robust hyperparameter tuning, time series cross-validation with one-step-ahead rolling origin setup is performed. The learning rate is set to be 0.002 at the beginning and it automatically updates using LearningRateScheduler with the rule of $learning \ rate*0.5^{epoch-1}$, and the number of epochs is 100 and early stopping is set based on validation loss. Grid search is used to select the input length, the number of layers, the number of nodes, the dropout rate and the batch size. The ranges where these hyperparameters are selected are listed as follows: the input length: 12, 24; the number of layers: 1, 2; the number of nodes: 4, 8, 16; the dropout rate: 0.2, 0.5; and the batch size: 32, 64. The Adam optimizer is used \citep{Kingma2014} and the combination of hyperparameters which minimizes validation RMSE is chosen.
   
   The proposed models are developed on the best-performing MLP and LSTM networks we built above, namely, the MLP and LSTM models are regarded as the global model in the first stage. The Ljung-Box test is performed, and the residual series with the $p$-value smaller than 0.05 indicate heterogeneity. In the second stage, as shown in Figure \ref{fig:model_flow_chart}, there are two options. The first option is to construct all local models on non-while noise residuals, we make use of \textit{auto.arima()} from R package \textit{forecast} to build ARIMA models. The second one can be considered when excessive local models lead to very high model complexity. The R package \textit{tsfeatures} is utilized to extract the features of residual series, including acf\_features, pacf\_features, entropy, lumpiness, stl\_features, arch\_stat, nonlinearity, unitroot\_kpss, unitroot\_pp, holt\_parameters, and hw\_parameters \citep{hyndman2019tsfeatures}, and then we carry out $k$-means algorithm to divide the residual series into different domains and the number of domains is determined taking the number of heterogeneous time series and the elbow method into account. After freezing weights and structure of the global model estimated in the first stage, one more dense layer is added to it and the batch size decreases to 16, 8 or 4.
   
   The two-stage models proposed in this paper are denoted as TS-X-I/II, where X represents the global model estimated in the first stage and I/II indicates which option we choose in the second stage. The performance of each model is evaluated on the test dataset in terms of the cumulative RMSE, MAE and sMAPE and the results are summarized in Table \ref{tab:Model_Comparison}, where the best-performing ones are bolded and the second-best ones are underlined.
   
   %% create a derivative column type called 'L':
   \newcolumntype{L}{>{\RaggedRight\hangafter=1\hangindent=0em}X}
   
   \begin{sidewaystable} [htbp]\centering
   	   	\caption{Model comparison among proposed models and baselines in terms of cumulative RMSE, MAE and sMAPE}
   	\setlength{\tabcolsep}{0.8mm}{
   	\begin{tabularx}{\textwidth}{ccl|rrrrr|rrrr}
   		\toprule
   		&       &       & \multicolumn{1}{c}{TSAVG} & \multicolumn{1}{c}{ARIMA} & \multicolumn{1}{c}{Pooled AR} & \multicolumn{1}{c}{MLP} & \multicolumn{1}{c}{LSTM} & \multicolumn{1}{c}{TS-MLP-I} & \multicolumn{1}{c}{TS-LSTM-I} & \multicolumn{1}{c}{TS-MLP-II} & \multicolumn{1}{c}{TS-LSTM-II}  \\
   		\midrule
   		\multirow{6}[0]{*}{Tourism} & \multirow{2}[0]{*}{RMSE} & mean  & 4759.500 & 4119.036 & 2047.617 & 1913.457 & 1963.280 & \underline{1898.218} & 1903.357 & \textbf{1892.543} & 1916.188 \\
   		&       & median & 1005.795 & 929.144 & 597.842 & 511.050 & 495.335 & 503.737 &\textbf{483.842} & 499.367 & \underline{490.785} \\
   		& \multirow{2}[0]{*}{MAE} & mean  & 3647.229 & 3190.488 & 1543.199 & 1539.330 & 1540.164 & 1529.742 & 1536.206 & \textbf{1513.960} & \underline{1515.407} \\
   		&       & median & 770.753 & 700.724 & 461.658 & 393.885 & 384.459 & 386.049 & 387.552 & \textbf{375.761} & \underline{383.539} \\
   		& \multirow{2}[0]{*}{sMAPE} & mean  & 0.291 & 0.267 & 0.206 & 0.171 & \textbf{0.167} & 0.170 & 0.167 & 0.170 & \underline{0.167} \\
   		&       & median & 0.262 & 0.241 & 0.158 & 0.136 & 0.136 & 0.138 & 0.142 & \underline{0.136} & \textbf{0.135} \\ \hline
   		
   		\multirow{6}[0]{*}{M3} & \multirow{2}[0]{*}{RMSE} & mean  & 615.987 & 598.826 & 597.438 & 562.183 & 579.985 & \textbf{558.386} & \underline{560.449} & 561.264 & 565.051 \\
   		&       & median & 406.703 & 395.709 & 389.837 & \underline{328.168} & 358.744 & \textbf{325.942} & 337.669 & 329.176 & 338.711 \\
   		& \multirow{2}[0]{*}{MAE} & mean  & 484.660 & 470.654 & 475.890 & 464.153 & 478.653 & \textbf{460.971} &\underline{462.290} & 462.659 & 465.618 \\
   		&       & median & 322.573 & 311.126 & 313.683 & \underline{270.498} & 297.641 & \textbf{268.922} & 282.301 & 271.838 & 281.160 \\
   		& \multirow{2}[0]{*}{sMAPE} & mean  & 0.115 & 0.114 & 0.114 & 0.113 & 0.113 & 0.114 & \underline{0.112} & 0.113 & \textbf{0.111} \\
   		&       & median & 0.069 & 0.065 & 0.066 & \underline{0.055} & 0.062 & \textbf{0.054} & 0.057 & 0.055 & 0.058 \\ \hline
   		
   		\multirow{6}[0]{*}{CIF 2016} & \multirow{2}[0]{*}{RMSE} & mean  & 360462.500 & 301763.100 & 451571.800 & 293090.638 & 268568.973 & 308556.696 & \underline{266999.874} & 281854.068 & \textbf{248097.484} \\
   		&       & median & 93.481 & 96.834 & 32944.598 & 78.259 & 87.128 & \textbf{52.036} & \underline{54.156} & 76.271 & 75.733 \\
   		& \multirow{2}[0]{*}{MAE} & mean  & 301584.200 & 233306.100 & 411428.100 & 239905.587 & 215052.708 & 256551.538 & \underline{207039.528} & 225569.258 & \textbf{199321.157} \\
   		&       & median & 82.064 & 79.007 & 32944.310 & 67.905 & 72.229 & \textbf{44.982} & \underline{50.356} & 65.871 & 62.792 \\
   		& \multirow{2}[0]{*}{sMAPE} & mean  & 0.107 & 0.101 & 1.338 & 0.095 & 0.103 & \textbf{0.082} & \underline{0.083} & 0.089 & 0.090 \\
   		&       & median & 0.084 & 0.080 & 1.592 & 0.071 & 0.086 & \underline{0.057} & \textbf{0.056} & 0.071 & 0.069 \\ \hline
   		
   		\multirow{6}[0]{*}{Hospital} & \multirow{2}[0]{*}{RMSE} & mean  & 23.985 & 23.320 & 21.610 & \underline{20.674} & 22.056 & \textbf{20.596} & 21.888 & 20.926 & 21.961 \\
   		&       & median & 8.080 & 8.022 & 8.357 & 7.833 & \textbf{7.725} & 7.825 &\textbf{7.725} & 7.944 & \underline{7.753} \\
   		& \multirow{2}[0]{*}{MAE} & mean  & 19.610 & 19.040 &\underline{17.643} & 17.651 & 18.739 &\textbf{17.574} & 18.654 & 17.816 & 18.635 \\
   		&       & median & \textbf{6.437} & \underline{6.495} & 6.854 & 6.600 & 6.558 & 6.616 & 6.546 & 6.665 & 6.612 \\
   		& \multirow{2}[0]{*}{sMAPE} & mean  & \textbf{0.168} & 0.169 & 0.176 & 0.169 & \underline{0.169} & 0.170 & 0.169 & 0.170 & 0.170 \\
   		&       & median & 0.158 & 0.161 & 0.169 & \underline{0.154} & 0.154 & 0.154 & \textbf{0.153} & 0.154 & 0.156 \\
   		\bottomrule
   	\end{tabularx}
}
\label{tab:Model_Comparison}
   \end{sidewaystable} 
  
  It can be observed that global models, especially neural networks, present flexible modelling and generalization abilities and outperform local models. Compared with the pure global model, the two-stage models proposed in this paper are able to effectively identify and model heterogeneity based on the global model built in the first stage, and further boost the forecasting performance.

    \section{Conclusions} \label{sec:conclusions}
    This paper proposes two-stage hybrid forecasting models to address the issue of insufficient fitting and inaccurate performance of global models on heterogeneous datasets. The heterogeneity of the given dataset is identified by the global model used, which is considered the first stage. Subsequently, the non-white noise residual series are further modelled in the second stage. Local models can be constructed for each series; alternatively, the residual series are utilized to divide the heterogeneous series into different domains, and then the global model estimated in the first stage is frozen while domain-specific sub-global models are built by adding layers to it. 
    
    Local models only consider the local patterns presented on each time series, while clustering-and-then-model methods use the local and global information within the corresponding cluster. Compared with them, the two-stage hybrid models introduced by this paper can leverage local information, domain-specific global information, and global information of the entire dataset. Besides, the complexity of the proposed models is greatly lower than that of the clustering-and-then-model approaches. More importantly, this paper highlights a novel perspective on identifying heterogeneity. It posits that heterogeneity is relevant to both the data and the model used, rather than solely to the data. Heterogeneity should not be detected only through data similarity.
    
    Experiments are conducted to evaluate one-step-ahead forecasting performance. In terms of cumulative RMSE, MAE, and sMAPE, our proposed models are compared with five existing methods, including TSAVG, ARIMA, Pooled AR, MLP, and LSTM neural networks. It turns out that the two-stage hybrid models are superior to the other models in most cases. This provides insight into the application of global forecasting models.
%    \section*{Acknowledgement}
%    This work was supported by the Economic and Social Research Council of UK (ES/P00072X/1: 2617249, ESRC Standard Research Studentship: 22020946).
\newline


%    {\bf Author contributions:}
%    The first author: Conceptualization, Data curation, Methodology, Formal analysis, Writing - original draft; The second author: Conceptualization, Methodology, Writing -review \& editing, Supervision.

%    {\bf Declarations of interest:}
%    none.

%    {\bf Declaration of Generative AI and AI-assisted technologies in the writing process:}
%    No generation AI or AI-assisted technologies was used in the writing process.

\begin{thebibliography}{}
	
	\bibitem[Bandara et~al., 2020]{Bandara2020}
	Bandara, K., Bergmeir, C., and Smyl, S. (2020).
	\newblock Forecasting across time series databases using recurrent neural
	networks on groups of similar series: A clustering approach.
	\newblock {\em Expert Systems with Applications}, 140:112896.
	
	\bibitem[Chen et~al., 2024]{Chen2024}
	Chen, J., Lenssen, J.~E., Feng, A., Hu, W., Fey, M., Tassiulas, L., Leskovec,
	J., and Ying, R. (2024).
	\newblock From similarity to superiority: Channel clustering for time series
	forecasting.
	\newblock {\em arXiv preprint arXiv:2404.01340}.
	
	\bibitem[da~Silva et~al., 2019]{Silva2019}
	da~Silva, E.~G., de~Mattos~Neto, P.~S., and de~Oliveira, J.~F. (2019).
	\newblock Hybrid system for time series using iterative residual forecasting
	models.
	\newblock In {\em 2019 8th Brazilian Conference on Intelligent Systems
		(BRACIS)}, pages 872--877. IEEE.
	
	\bibitem[Ellis, 2018]{ellis2018tcomp}
	Ellis, P. (2018).
	\newblock Tcomp: Data from the 2010 tourism forecasting competition.
	\newblock {\em R package version}, 1(1).
	
	\bibitem[Etemadi et~al., 2023]{Etemadi2023}
	Etemadi, S., Khashei, M., and Tamizi, S. (2023).
	\newblock Etemadi reliability-based multi-layer perceptrons for classification
	and forecasting.
	\newblock {\em Information Sciences}, 651:119716.
	
	\bibitem[Firmino et~al., 2015]{Firmino2015}
	Firmino, P. R.~A., de~Mattos~Neto, P.~S., and Ferreira, T.~A. (2015).
	\newblock Error modeling approach to improve time series forecasters.
	\newblock {\em Neurocomputing}, 153:242--254.
	
	\bibitem[Fröhwirth-Schnatter and Kaufmann, 2008]{Froehwirth-Schnatter2008}
	Fröhwirth-Schnatter, S. and Kaufmann, S. (2008).
	\newblock Model-based clustering of multiple time series.
	\newblock {\em Journal of Business and Economic Statistics}, 26(1):78--89.
	
	\bibitem[Godahewa et~al., 2021]{Godahewa2021}
	Godahewa, R., Bandara, K., Webb, G.~I., Smyl, S., and Bergmeir, C. (2021).
	\newblock Ensembles of localised models for time series forecasting.
	\newblock {\em Knowledge-Based Systems}, 233:107518.
	
	\bibitem[Hajirahimi and Khashei, 2019]{Hajirahimi2019}
	Hajirahimi, Z. and Khashei, M. (2019).
	\newblock Hybrid structures in time series modeling and forecasting: A review.
	\newblock {\em Engineering Applications of Artificial Intelligence},
	86:83--106.
	
	\bibitem[Hewamalage et~al., 2022]{hewamalage2022global}
	Hewamalage, H., Bergmeir, C., and Bandara, K. (2022).
	\newblock Global models for time series forecasting: A simulation study.
	\newblock {\em Pattern Recognition}, 124:108441.
	
	\bibitem[Hochreiter and Schmidhuber, 1997]{hochreiter1997long}
	Hochreiter, S. and Schmidhuber, J. (1997).
	\newblock Long short-term memory.
	\newblock {\em Neural Computation}, 9(8):1735--1780.
	
	\bibitem[Hyndman et~al., 2018]{hyndman2018package}
	Hyndman, R., Akram, M., Bergmeir, C., O'Hara-Wild, M., and Hyndman, M.~R.
	(2018).
	\newblock Package ‘mcomp’.
	\newblock {\em R package version}, 2.
	
	\bibitem[Hyndman et~al., 2019]{hyndman2019tsfeatures}
	Hyndman, R., Kang, Y., Montero-Manso, P., Talagala, T., Wang, E., Yang, Y.,
	O’Hara-Wild, M., et~al. (2019).
	\newblock tsfeatures: Time series feature extraction.
	\newblock {\em R package version}, 1(0).
	
	\bibitem[Hyndman, 2015]{Hyndman2015}
	Hyndman, R.~J. (2015).
	\newblock Expsmooth: Data sets from forecasting with exponential smoothing.
	\newblock {\em R package version 2.3}.
	
	\bibitem[Hyndman and Khandakar, 2008]{Hyndman2008}
	Hyndman, R.~J. and Khandakar, Y. (2008).
	\newblock Automatic time series forecasting: Theforecastpackage forr.
	\newblock {\em Journal of Statistical Software}, 27(3).
	
	\bibitem[Kim et~al., 2021]{kim2021reversible}
	Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J. (2021).
	\newblock Reversible instance normalization for accurate time-series
	forecasting against distribution shift.
	\newblock In {\em International Conference on Learning Representations}.
	
	\bibitem[Kingma and Ba, 2014]{Kingma2014}
	Kingma, D.~P. and Ba, J. (2014).
	\newblock Adam: A method for stochastic optimization.
	
	\bibitem[Liu et~al., 2022]{liu2022non}
	Liu, Y., Wu, H., Wang, J., and Long, M. (2022).
	\newblock Non-stationary transformers: Exploring the stationarity in time
	series forecasting.
	\newblock {\em Advances in Neural Information Processing Systems},
	35:9881--9893.
	
	\bibitem[Makridakis et~al., 2020]{Makridakis2020}
	Makridakis, S., Spiliotis, E., and Assimakopoulos, V. (2020).
	\newblock The m4 competition: 100,000 time series and 61 forecasting methods.
	\newblock {\em International Journal of Forecasting}, 36(1):54--74.
	
	\bibitem[Montero-Manso and Hyndman, 2021]{Montero-Manso2021}
	Montero-Manso, P. and Hyndman, R.~J. (2021).
	\newblock Principles and algorithms for forecasting groups of time series:
	Locality and globality.
	\newblock {\em International Journal of Forecasting}, 37(4):1632--1653.
	
	\bibitem[Murtagh, 1991]{Murtagh1991}
	Murtagh, F. (1991).
	\newblock Multilayer perceptrons for classification and regression.
	\newblock {\em Neurocomputing}, 2(5–6):183--197.
	
	\bibitem[Neubauer and Filzmoser, 2024]{Neubauer2024}
	Neubauer, L. and Filzmoser, P. (2024).
	\newblock Improving forecasts for heterogeneous time series by “averaging”,
	with application to food demand forecasts.
	\newblock {\em International Journal of Forecasting}, 40(4):1622--1645.
	
	\bibitem[Petropoulos et~al., 2022]{petropoulos2022forecasting}
	Petropoulos, F., Apiletti, D., Assimakopoulos, V., Babai, M.~Z., Barrow, D.~K.,
	Taieb, S.~B., Bergmeir, C., Bessa, R.~J., Bijak, J., Boylan, J.~E., et~al.
	(2022).
	\newblock Forecasting: theory and practice.
	\newblock {\em International Journal of Forecasting}, 38(3):705--871.
	
	\bibitem[Salinas et~al., 2020]{salinas2020deepar}
	Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T. (2020).
	\newblock Deepar: Probabilistic forecasting with autoregressive recurrent
	networks.
	\newblock {\em International Journal of Forecasting}, 36(3):1181--1191.
	
	\bibitem[Semenoglou et~al., 2021]{Semenoglou2021}
	Semenoglou, A.-A., Spiliotis, E., Makridakis, S., and Assimakopoulos, V.
	(2021).
	\newblock Investigating the accuracy of cross-learning time series forecasting
	methods.
	\newblock {\em International Journal of Forecasting}, 37(3):1072--1084.
	
	\bibitem[Shumway and Stoffer, 2000]{Shumway2000}
	Shumway, R.~H. and Stoffer, D.~S. (2000).
	\newblock {\em Time Series Regression and ARIMA Models}, pages 89--212.
	\newblock Springer New York.
	
	\bibitem[Smyl, 2020]{Smyl2020}
	Smyl, S. (2020).
	\newblock A hybrid method of exponential smoothing and recurrent neural
	networks for time series forecasting.
	\newblock {\em International Journal of Forecasting}, 36(1):75--85.
	
	\bibitem[Wellens et~al., 2023]{wellens2023and}
	Wellens, A.~P., Kourentzes, N., and Udenio, M. (2023).
	\newblock When and how to use global forecasting methods on heterogeneous
	datasets.
	\newblock {\em Available at SSRN 4629272}.
	
	\bibitem[Zhang, 2003]{Zhang2003}
	Zhang, G. (2003).
	\newblock Time series forecasting using a hybrid arima and neural network
	model.
	\newblock {\em Neurocomputing}, 50:159--175.
	
\end{thebibliography}



\end{document}
