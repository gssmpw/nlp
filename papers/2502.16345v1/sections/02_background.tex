\section{Background}

\subsection{Anthropomorphism: Cultural Representation and Emotion}

In psychology, the concept of anthropomorphism describes human behaviors that attribute human qualities to non-human animals and inanimate objects. It can be understood as an associative process that applies human anatomical or cultural representations to non-human phenomena, such as when a person perceives car headlights as eyes. Indeed, people tend to perceive social presence even when agents are merely presented with facial features resembling those of humans \citep{kontogiorgos2019effects}. This human tendency allows people to invoke established frames of reference---such as cultural expectations, social beliefs, and perceptions \citep{puzakova2013humanizing}---to interpret and make sense of the uncertainty of non-humanness \citep{boyer1996makes}. Thus, anthropomorphism typically consists of inferences, whereby people rely on their anthropocentric knowledge to guide reasoning about non-human agents \citep{epley2007seeing}.

Because anthropomorphism is inherently social and relational, it is often also emotional. Indeed, people tend to anthropomorphize objects for psychological reasons, including alleviating pain or compensating for a lack of social connections \citep{epley2007seeing}. One prior study found that people establish emotional associations to information based on presented social cues, such as voice tone \citep{pickett2004getting}. Human-like features could also trigger feelings like comfort by reducing anxiety and individuals' need to seek human interactions \citep{wan2021anthropomorphism}. On a broader scale, anthropomorphism is closely correlated with self-identity, as it can affect how people perceive themselves and how they orient themselves towards life. For instance, people who anthropomorphize animals might become more attuned to animal rights and animal welfare  \citep{wan2021anthropomorphism}. And these emotional associations are often projected onto anthropomorphized entities; one study found that people anthropomorphize robots based on their perceived ability to express emotional states, which are influenced by the emotions ascribed to them \citep{eyssel2010anthropomorphic}.


\subsection{Anthropomorphism and Human-Computer Interaction}

In HCI, human-like features could be implemented in application design to enhance the usability of user interfaces and experiences. Features like voice tones \citep{chang2018stereotypes}, dialogue styles \citep{hoegen2019end-to-end,yan2024talk}, visual cues \citep{go2019humanizing}, and avatars \citep{park2023generative} streamline interactions by allowing users to operate in a natural way, fostering trust with users and making them more likely to continue using such applications. In more extreme cases, chatbots can be built with specific racial characteristics to deliberately provoke artificial closeness with users \citep{liao2020racial}. Indeed, one study in human-robot interaction reveals that people are more likely to anthropomorphize machines and feel psychologically closer to them when they perceive the machine as being the same gender as themselves \citep{eyssel2012if}. 

Users are highly receptive to these design choices,\footnote{Children are especially likely to anthropomorphize conversational interfaces, attributing agency and personality to them \citep{lee2021conversational}}. and they sometimes take initiative in applying anthropomorphic interpretations. For example, users might assign social roles to machines based on perceived similarities with human interactions. People tend to expect agents to adhere to social norms and rules, often projecting gender stereotypes onto stimuli such as voices \citep{reeves1996media, abercrombie-etal-2021-alexa, shiramizu2022role}. Previous studies suggest that when users find machine voices or dialogues similar to human voices or dialogues, they begin to attribute social roles to these machines \citep{nass2000machines,nass2000does}.


\subsection{Potential Harms within HCI}

The use of anthropomorphism in AI systems could lead to a number of questionable or adverse effects. Firstly, chatbots' performance of agency and the anthropomorphic language often used to describe LLMs could lead users to misunderstand these systems' capabilities \cite{salles2020anthropomorphism, shanahan2023role} or overlook the non-human nature of responses \citep{leong2019robot}. These trends can foster unwarranted trust in LLM responses, particularly impacting vulnerable populations such as children, the elderly, individuals with illnesses or disabilities, and those less familiar with recent technologies \citep{abercrombie-etal-2023-mirages}. In this sense, the actual and perceived affordances of anthropomorphized chatbot systems may not be well calibrated \citep{matassi2019domesticating, davis2016theorizing, nagy2015imagined}. Such misalignment has already led to tragic cases in reality: for instance, (1) a Belgian man tragically ended his life after extensive conversations with a chatbot that deepened his eco-anxiety \citep{walker2023belgian, xiang2023he} and (2) a teenager formed an emotional bond with a personified chatbot, contributing to his suicide \citep{roose2024can}. People might develop unhealthy habits by relying on chatbots for advice that would ordinarily require professional support \citep{tidy2024character, robb2024he}. These cases involve text-based interactions, but manipulative effects on user perceptions might be an even greater concern with embodied agents. Similar observations have been made in the field of human-robot interactions \citep{scorici2024anthropomorphization}.

Users may also instigate or suffer from certain socio-technical harms when interacting with anthropomorphized chatbots---such as stereotyping and social inequalities \citep{bender2021dangers, chan2023harms, shelby2023sociotechnical}. 
AI developers' decisions about training data, model training, and design priorities already tend to introduce unconscious bias and imbalanced representations in anthropomorphized chatbots (especially those with purposeful or incidental gender or racial features);\footnote{Depictions of humanoid robots available online are often designed with characteristics aligned with color-blindness and white ideologies \citep{cave2020whiteness}}. still, users might introduce additional stereotypes and biases in the process of applying social contexts to guide the interpretation of chatbot outputs \citep{maeda2024when,stark2024animation}. This, in turn, might influence how they appraise and use generated information. Indeed, people tend to mindlessly apply gender and racial stereotypes to computers \citep{nass2000machines, abercrombie-etal-2021-alexa}, often in ways that reinforce harmful representations \citep{ruane2019conversational}. This may affect whether they find these agents persuasive or trustworthy, causing them to interpret generated outputs differently than they would otherwise.

Previous work suggests that harms can also arise from human and computer interactions themselves \cite{gabriel2024ethics, weidinger2022taxonomy}. This is because emerging technologies reshape the way we orient ourselves with tools \citep{turkle2017computers}. In the context of generative AI systems, users could potentially adapt their conversational habits to optimal forms to talk with chatbots \cite{hancock2020ai}. However, such practices could lead to harmful outcomes---for instance, users' opinions could be negatively influenced by AI assistants' outputs \citep{jakesch2023co}. 

\subsection{Existing Approaches}

Current evaluation measures for chatbots tend to focus on the algorithmic harms and biases latent in machine learning models or data. However, conversational AI systems are based on interactions with users. Thus, it is also necessary to examine the design features of chatbots like ChatGPT and Gemini, as well as the linguistic features of generated outputs. This would allow us to identify different kinds of expressions that evoke specific emotional responses, social roles, or cultural assumptions among users. This is important, as users might be susceptible to mindless anthropomorphism \citep{kim2012anthropomorphism,araujo2018living}, wherein users partake in activities without thinking about their implications.

Researchers in the field of communication have long studied the use of human-like linguistic expressions to facilitate human-computer interactions. Studies like these have examined how language affects the ways in which people perceive information. For instance, the use of emoticons as well as symbols could animate the static representations of languages, adding emotional interpretations (e.g., the use of /// to indicate embarrassment and stress) \citep{silvio2010animation}. Because conversational AI depends on dialogues as a way to navigate interactions between users and systems \citep{reisner1981formal}, language plays a significant role in communicating the assumed meanings of generated texts. \citet{agre1995computational}discusses how linguistic utterances and their grammatical structures convey actions and create assumptions about knowledge and reality. Similarly, \citet{payne1986task} introduce task-action grammars, where linguistic structures construct the representations, rules, and/or specifications of given tasks.

By producing texts that imply certain meanings, emotions, and assumptions, chatbots can sometimes \textit{provoke} these meanings, emotions, and assumptions---especially when these texts are issued to users who are responsive to the social and dialogical nature of human-AI interaction \citep{hohenstein2023artificial}. \citet{stark2024animation} highlights how conversational AI systems are designed to elicit strong emotional responses by comparing them to animations---representations of people, created by large teams, that are designed to grab an audience's attention. He explains how human-like features may be designed into systems in order to guide users towards specific kinds of interactions. \citet{silvio2010animation}, writing specifically about the topic of animation, describes linguistic expressions in animation as a form of performative world-making. If we extend the animation analogy to chatbots, then chatbot ``actions'' may enable and prompt real social interactions. 

Theoretically, this paper operates at the intersection of these approaches, interpreting anthropomorphism as a tool used in human-computer interaction to linguistically construct liminal interactive spaces between users and chatbots---spaces which reframe dialogues as illusional two-way communication and conversational AI as conversational partners \citep{knoth2024ai}. Within these interactive spaces, the outputs of conversational AI can be interpreted as social cues (see \citet{feine2019taxonomy}), and human-AI interactions themselves can be conceptualized as parasocial relations between users and anthropomorphized conversational agents \citep{maeda2024when}. Prompt-to-response interactions in language models create linguistically cultivated interactive spaces, particularly through task-oriented dialogues that emphasize multi-turn exchanges \citep{wei2022chain}. These spaces go beyond simple word exchanges, becoming refined linguistic environments where complex tasks can be performed. The construction of datasets with presumed assistant replies \citep{kopf2024openassistant} further shapes these spaces, tailoring them to specific linguistic patterns and task-oriented vocabularies. Human feedback plays a crucial role in this cultivation process, as it assesses the human-likeness of generated responses \citep{zhang-etal-2020-dialogpt}, thereby guiding the linguistic development of these interactive spaces.\footnote{Ideally, one of the objectives of human feedback could be to make responses more helpful and harmless \citep{bai2022training, ganguli2022red}.} This iterative refinement through human evaluation ensures that these spaces not only facilitate task completion but also foster increasingly sophisticated and natural language interactions. 

Existing research methods may struggle to encompass this kind of hybrid space, which is not just a platform, interface, or discourse. Additionally, the generative nature of chatbot systems makes their affordances more unpredictable and dynamic, as the same prompt will typically spawn different outputs at different times. And though user-facing studies are important for understanding how these spaces operate in practice, introducing them in lieu of a more system-oriented study may skew research activities by introducing the confounding factor of users' idiosyncrasies.