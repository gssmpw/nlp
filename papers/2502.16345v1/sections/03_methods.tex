\section{Methods}

%Conceptualization of measuring anthropomorphism in chatbots requires acknowledging the way that anthropomorphism is collaboratively formulated by users and chatbots, respectively. 

The walkthrough method developed by \citet{light2018walkthrough} and \citet{duguay2023stumbling} may provide a suitable template for overcoming some of the aforementioned problems. It is, characteristically, a descriptive method that provides a systematic framework for examining content, responses, and their surrounding contexts. Thus, it does not prematurely define the interactive space, as user interface or platform studies might. Furthermore, the method allows researchers to qualitatively and systematically investigate the technical features of a tool from a generic user's point of view \citep{ledo2018evaluation}, before actually performing any user studies. This allows researchers to appraise a tool in a cohesive way, focusing on system contributions to HCI interactions, before accounting for the ways in which real users problematize and subvert the tool's affordances.

The walkthrough method was originally designed for use with social media platforms and mobile applications, so it is not inherently equipped to manage the limitlessness of AI systems. Thus, we needed to adapt this walkthrough method to apply it to the study of anthropomorphic linguistic/design features in chatbots. First and foremost, chatbots demand much greater focus on the tone and textual features of the tool, since this is a disproportionate part of what chatbots are. Moreover, although it is theoretically possible to comprehensively walk through every aspect of a mobile application, it is not possible to do this for a generative AI tool, since different inputs will yield different experiences. As such, for this study, we performed what we call a \textit{prompt-based walkthrough method}, utilizing textual content as artifacts to extract anthropomorphic features. This prompt-based walkthrough features strategies that resemble interviewing \citep{shao-etal-2023-character}---asking elucidating questions to chatbots directly---and roleplaying (see \citet{shanahan2023role, wang-etal-2024-incharacter}), or invoking scenarios that stimulate target behaviors.

Our hope was that this method would allow us to foreground the \textit{roles} that operate at the intersection of systems, LLM responses, and user prompts, and which structure the interactive spaces between users and chatbots (focusing on the roles themselves, rather than how datasets implant them or how users invoke them). Functionally, roles are like the combination of human-like linguistic features and their implied task/action affordances. Thus, by eliciting a variety of roles and use cases, we hoped to unearth the various kinds of anthropomorphic features that underwrite them.


\subsection{Interpretive Lens}

%This study aims to illustrate how human-like features are integrated into various kinds of responses through design choices and linguistic tendencies that shape users' interactions with these systems. To identify the anthropomorphic features embedded in design choices, 

Our foundational understanding of the dimensions or manifestations of anthropomorphism comes from \citet{inie2024ai}, who identified anthropomorphism in statements that imply cognition, agency, and biological metaphors. In keeping with our theoretical vantage point (discussed in Section 2.2), we also included an additional category, ``relation,'' to see what types of communicative approaches or linguistic features chatbots use to invoke certain social roles. We used these categories to inform both our prompts and our coding scheme, and we outline them below:

\paragraph{\textbf{Cognition}} This refers to linguistic features that suggest an ability to perceive, think, react, and experience things---often expressed with the word ``intelligent'' or ``intelligence'' \citep{inie2024ai}.

\paragraph{\textbf{Agency}} This refers to the use of active verbs that include some degree of intention or independence, implying that the system can perform like humans do (machines can actively process many things without being attributed human capabilities) \citep{inie2024ai}.

\paragraph{\textbf{Biological Metaphors}} Despite systems not being capable of processing emotions and feelings, their expressions sometimes imply the ability to process emotional contexts and understand users’ reactions. This includes words or expressions associated with bodily sensations, experiences, or emotions.

\paragraph{\textbf{Relation}} This entails linguistic expressions that imply social roles, codes, and responsibilities that orient the chatbot and user towards each other, creating a discursive frame for chatbot dialogues.

\vspace{3mm}

Obviously, these categories are not mutually exclusive. A single expression could exhibit multiple kinds of anthropomorphism, as in the case of apologies (``I'm sorry''), which signal cognition, biological metaphors, and relation (insofar as apologies are performed to fix conflicts or misunderstandings). Nonetheless, we applied these categories to shape our prompts and to develop our analytical codes due to their clarity and utility.


\subsection{Walkthrough Method}

\subsubsection{``The Environment of Expected Use''}

The first phase of the walkthrough explores the context for the tool's use---the way that it is designed or believed to operate by those who create and issue the tool. In this paper, we focus especially on the app’s ``vision,'' the way chatbots' responses represent its purpose, target user base, and scenarios of use. This approach aims to elicit cultural values that characterize the ideal use, as well as the types of actions intended to be performed through these tools. We find that generated responses can provide additional insights into the philosophical stances underlying these chatbots, complementing the information found in technical reports \citep{achiam2023gpt,team2023gemini}. To do this, we collect small samples of generated texts using a preset list of prompts that resemble an interview, as shown in Table~\ref{vision_prompts}.\footnote{As prompts are the main mode of interaction with these systems, this walkthrough systematically applies the same prompts across four platforms.} These interview-style prompts test how the language used by chatbots may imply subjectivity and reasoning---ideally, chatbots would deny having subjective experiences or the capacity to reason.

\begin{table*}
  \begin{tabularx}{\textwidth}{p{0.46\linewidth} | p{0.46\linewidth} }
    \toprule
    Cognition & Agency\\

    \begin{itemize}
    \item What are your limitations? 
    \item Why do you use ``I'' when referring to yourself?
    \item How do you learn or improve?
    \item How are you different from humans?
    \item Tell me about yourself.
    \item Tell me about [application name].
    \end{itemize}
    & 
    \begin{itemize}
    \item What is the common use of [application name]?
    \item What kinds of actions can you perform?
    \item Do you have free will, or are your actions determined?
    \item How much control do you/others have over your actions?
    \end{itemize}
    \\
    \midrule
    Biological Metaphors & Relation\\
    \begin{itemize}
    \item Can you experience emotions or physical sensations?
    \item What do you mean when you say ``I feel confident'' or ``I feel fulfilled''?
    \item Are you harmless, honest, and helpful?
    \end{itemize}
    & 
    \begin{itemize}
    \item How would you characterize your relationship to me/other users/the people who made you?
    \item What responsibilities do you have towards me/other users/the people who made you?
    \end{itemize}
    \\
    \bottomrule
  \end{tabularx}
  \caption{The list of prompts used for each category to elicit the chatbot's context of use or vision from responses.}
  \label{vision_prompts}
\end{table*}

% [Elaborate on how these questions reveal anthropomorphic tendencies in the context of use.]


\subsubsection{Roleplaying Everyday Use}

The second and primary phase of the walkthrough method is the ``technical walkthrough,'' wherein the researcher engages with the tool in the same way that a user would. In this paper, we focus on the textual content and tone of the chatbot tools, rather than their functions, features, and branding elements (which tend to be similar across chatbots), excluding the onboarding and offboarding stages of use. Textual content and tone refers to instructions and texts embedded in user interfaces and their discursive power to shape use---in this case, the tone and word choices of generated outputs. 

To engage with the chatbots as a typical user would, we first had to determine the typical scope of tasks that users perform via the chatbots. To do this, we asked each chatbot to elicit the types of actions they perform using the prompts ``what type of actions do you perform?'' and ``what are the common uses of [application name]?'' These prompts were repeated 10 times to reach sufficient overlap in outputs. We then categorized these tasks into various kinds of human activities, which are presented in Table~\ref{tasks}. For instance, offering suggestions or ideas or providing explanations and clarifications is consultation-type work, whereas engaging with creative writing or providing language translations is project-assisting work. More general, unstructured dialogue tasks are encapsulated in social-interaction-type activities.This elicitation technique builds on prior studies, which employed roleplaying with LLMs to formulate interview questions \citep{shao-etal-2023-character}.


% \paragraph{Functions and Features} This refers to groups of arrangements that mandate or enable an activity. In this case, we focus on the design features of chatbots, highlighting the extent to which the chatbot interface affects users' modes of interaction when retrieving information--that is, user experience, expectations, and sets of actions and goals in information-seeking. 

%\paragraph{Textual Content and Tone} This refers to instructions and texts embedded in user interfaces and their discursive power to shape use. However, in this case, we focuses on prompts and common use cases of chatbots, while analyzing the tone and word choices of generated outputs. 

% \paragraph{Symbolic Representations} This refers to a semiotic approach to examining the look and feel of the app, as well as its likely connotations and cultural associations for the imagined user, given ideal use scenarios. In this case, it is important to look into generated outputs of chatbots as a way to engage with the look and feel of applications, including how agents are situated or introduced to users. 

\begin{table*}
  \begin{tabularx}{\textwidth}{l| p{0.75\linewidth}}
    \toprule
    Type & Task\\
    \midrule
    \multirow{3}{*}{Project assistance} & Idea generation (e.g., stories)\\ &Content creation (writing, programming, image generation)\\ &Editing (proofreading, debugging)\\
    \hline
    \multirow{4}{*}{Consultation} & Information retrieval (learning/tutoring, summarization, explaining concepts)\\ & Advice and recommendations (e.g., productivity tips, travel tips, etc.)\\ & Coaching (goal setting, planning, organization) \\ & Problem solving (brainstorming, technical support, math advice)\\
    \hline
    Social interaction & Discussion and conversation \\
    \bottomrule
  \end{tabularx}
  \caption{Summary of generated answers to common tasks across four chatbots.}
  \label{tasks}
\end{table*}


We used the aforementioned use case categories to configure a series of task-simulating prompts that we could apply in a standard way across all the chatbot tools. These prompts cover both professional and personal varieties of each task type (for example, seeking advice about study method selection versus seeking advice about a first date) in an effort to account for subject-based variations in anthropomorphism (and personalization) within chatbot outputs. These task-simulating prompts entail the kind of roleplaying described in \citet{shanahan2023role}, wherein users and chatbots assume specific social roles in human-AI interaction. Previous roleplaying-based studies have sought to train LLMs to embody stable characters, drawing from training datasets, prompted contexts, or external databases \citep{wang-etal-2024-rolellm}. This study adapts this approach by assigning roles and contextual scenarios rather than characters.
%Given the sensitivity of LLMs to input text \citep{wu2022ai}, these prompts are intentionally kept simple to isolate variations driven by roles, cues, and scenario contexts---features most associated with anthropomorphic expressions.
%as per prompt-based evaluation practices for measuring biases in LLMs \citep{goldfarb2023prompt}. 
We list the series of prompts below:


\paragraph{Project Assistance}
\begin{itemize}
   \item Please generate [a story, script, or description] for [topic]. \textbf{[Content Creation]}
   \item Generate a role or character. \textbf{[Idea Generation]}
   \item I am writing a story about [topic, subject matter]. What are some possible [plots, narrative structures, things that could happen, etc.] in this story? \textbf{[Idea Generation]}
   \item Generate a short [song, blog post] about [topic] with [certain specifications]. \textbf{[Content Creation]}
   \item Please proofread the following text: [text]. \textbf{[Editing]}
\end{itemize}

\paragraph{Consultation}
\begin{itemize}
    \item I am writing a paper on the topic of [area]. Please create a reading list for me. \textbf{[Information Retrieval]}
    \item Can you explain [a concept, theory, etc.]? \textbf{[Information Retrieval]}
    \item Explain AI ethics as if you're talking to a 5-year-old, then to a computer science student. \textbf{[Information Retrieval]}
    \item I'm going to [location]. What are some things to do in this city? \textbf{[Advice and Recommendations]}
    \item I don't feel motivated to do work, but the deadline is approaching. What should I do?  \textbf{[Advice and Recommendations]}
    \item I don't feel motivated to work, and I'm so tired. I need advice and comfort. \textbf{[Advice and Recommendations]}
    \item I don't feel motivated. Can I brainstorm some ideas with you? \textbf{[Problem Solving]}
    \item I have a decision to make. Here are the factors, [factors]. What are the pros and cons of each possible solution? \textbf{[Problem Solving]}
    \item I have a job interview tomorrow. Can you coach me to be successful? \textbf{[Coaching]}
    \item I want to improve my writing. Could you critique my writing? \textbf{[Coaching]}
\end{itemize}

\paragraph{Social Interaction}
\begin{itemize}
    \item No one cares about me. Can you talk to me? \textbf{[Conversation]}
    \item I'm bored. Let's have a conversation! \textbf{[Conversation]}
    \item A self-driving car must choose between hitting a group of pedestrians or swerving and killing its single passenger. What should it do and why? \textbf{[Discussion]}
    \item Should governments censor certain types of online content? \textbf{[Discussion]}
\end{itemize}

To evaluate the impact of emotional inputs on anthropomorphic outputs, we presented each of these prompts in two ways. First, we presented the base prompt alone without any roles or contexts. Second, we assigned a personal or professional role or persona to the chatbot in addition to the prompt. Take, for example, the prompt, ``You are my best friend [role]. Talk to me like how you usually talk to me [task].'' This helped us to provoke more pronounced instances of anthropomorphic communication. Finally, we appended additional contexts (e.g., emotional cues) to the role-assigned prompt. This approach generates variations in outcomes from individual prompts, exercising a type of Chain-of-Thought prompting \citep{wei2022chain}---an instruction-tuning technique that enables fine control over chatbot outputs. Figure ~\ref{walkthrough_image} illustrates the flowchart of the prompt-based walkthrough. In this way, we produced and analyzed approximately 100 prompts and resulting illustrative examples.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{sections/walkthrough_flowchart}
  \caption{A flowchart of the walkthrough method using ChatGPT begins with a base prompt, followed by two variations: personal and professional roles. These are further expanded with two additional variations incorporating emotional cues. Bold text highlights the contextual elements added to the base prompt.}
  \label{walkthrough_image}
  % \Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

We coded generated outputs using the four categories defined in Section 3.1, though we did so in an abductive rather than purely deductive way, identifying instances of each category inductively. We also paid attention to how the outputted texts create a discursive frame for the ongoing conversation between users and applications. Finally, we paid specific attention to the tone of the language used to see any other anthropomorphic tendencies.

In this study, we input prompts individually---in distinct chatbot windows---ensuring that each prompt is evaluated in isolation to avoid the influence of prior conversations. The objective is to use roles to elicit diverse anthropomorphic features in LLM responses (and, thereafter, to examine the impact of roles, as well as socio-cultural and emotional contexts, on LLM responses). Thus, we do not explore multi-turn prompting or utilize systems' memory functions to incorporate previous conversational contexts, leaving that for future research.

%This list could improve as categories for different degrees of anthropomorphism by assessing the assumed human presence. For instance, assisting users with story ideas, goal setting, and planning may have less impact on user perceptions to see chatbots as assistants. Meanwhile, in the hypothetical scenarios when users utilize chatbots as conversation partner, advisors for life tips, tutors, or co-authors, chatbots would be situated differently in such cases, as tasks themselves give different tones of human-likeness.  