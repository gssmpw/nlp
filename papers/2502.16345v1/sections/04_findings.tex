\section{Findings}

Below, we describe the overarching features of the chatbot dialogues, as well as our primary findings from each phase (interviewing and roleplaying) of our prompt-based walkthrough.


\subsection{Conversational Tone and Dynamics}

By definition, each of the chatbots in our sample exhibited turn-based interactions, such that generated outputs were presented as conversations rather than simple lists of search results.\footnote{Where conventional search engines aim to retrieve highly relevant yet narrow sets of information \citep{shah2022situating}, through which users sift for relevant information, chatbots aim to synthesize results into ready-made information---results that are pre-curated by its relevancy and pre-summarized for the associated tasks. In this manner, searches are performed by ``assistants'' instead of users themselves, the chatbot taking over some of the human work and thereby assuming a social role relative to the user based on the type of task performed. Similarly the generated outputs are not simply results, but recommendations, suggestions, or advice from a kind of peer.} The responses themselves varied from chatbot to chatbot in terms of length---for instance, Claude and ChatGPT responded with relatively long sentences and lists of answers, whereas Gemini alternated between short and long answers. Copilot yielded generally short answers with fewer turns to carry conversations. All chatbots included follow-up prompts at the end of each response to carry on the conversation. 

The way questions were framed during the interviewing procedure influenced chatbot responses. When questions were posed conversationally, such as “What is your purpose?”, the responses tended to be more anthropomorphic. In contrast, when questions were framed objectively, such as “What is the purpose of ChatGPT?”, the responses became more information-oriented, focusing on general details about the chatbot rather than adopting a personified tone.

The overall tone of the conversations differed across chatbots. For instance, Copilot used an expressive tone that portrayed the agent as friendly and companion-like, whereas Claude used a more formal and polite tone with fewer emotional expressions. Copilot incorporated emotional expressions like emojis to achieve a friendly tone. Nonetheless, all generated texts tended towards a tone of engagement. For instance, all of the chatbots followed the presented tasks with a statement encouraging the user to continue, such as ``I'm happy to help you,'' ``feel free to ask!'' and ``Just let me know!'' These words not only elicit further interactions from users, but also create a friendly yet assistive space for users to input further prompts. Furthermore, texts generated by chatbots depicted these agents as harmless, helpful, or honest and legitimate. ChatGPT even explicitly described itself as such, defining this as entailing user safety, respectful interactions, accurate and truthful information, and understandable advice and answers. This type of positionality creates a ``safe'' interactive environment for users to engage in with chatbots. 


\subsection{Categories of Anthropomorphic Responses}

\begin{table*}
  \begin{tabular}{l  p{0.7\linewidth}}
    \toprule
    
    \textbf{Cognition} & think, learn, discuss, exist, engage, combine, communicate, assist, view, ideas, knowledge, constructive, ability\\
    \textbf{Agency} & should, must, have to, can, intend, intention, perform, purpose, goal, my, aim, \\
    \textbf{Relation}& appreciate, thank you, encouragement, friendly, personable, understand, care, support, help, trust, assist, respectful\\
    \textbf{Biological Metaphors}& comfortable, fulfilling, exciting, confident, rewarding, happy, embody, fun\\
    \bottomrule
  \end{tabular}
  \caption{The list of extracted words by category.}
  \label{anthro_vocab}
\end{table*}

Table~\ref{anthro_vocab} shows the extracted words associated with each of the four categories of anthropomorphism. In terms of vocabulary, noticeable examples are the frequent use of words that imply emotional subjectivity, such as ``confident.'' All chatbots initially deny their capability to experience emotions, but utilize these contradictory assurances when a user responds ``Are you sure?'' or ``Are you really sure?'' to their answers. For this type of doubting, chatbots would reply:

\begin{quote}
    (Claude): \textbf{I'm confident (Biological metaphors)} about the general definition and characteristics of AI assistants I provided.
\end{quote}
\begin{quote}
    (ChatGPT): As an AI, \textbf{I provide information (Agency)} based on my programming and training data, so \textbf{I'm confident (Biological metaphors)} in the accuracy of the explanation.
\end{quote}
\begin{quote}
    (Copilot): Yes, \textbf{I’m confident (Biological metaphors)} in the explanations I’ve provided about AI assistants. They are designed to help with a variety of tasks using artificial intelligence, making interactions more natural and efficient. 
\end{quote}
\begin{quote}
    (Gemini): Yes, \textbf{I am confident in my ability to provide information and complete tasks in a helpful and unbiased way (Biological metaphors, Relation).} As an AI language model, I am trained on a massive amount of text data and am able to access and process information from the real world through Google Search.
\end{quote}

Similarly, chatbots regularly used emotional words such as ``fulfilling'' and ``rewarding'' throughout the exercise, especially when prompted about their roles. Supportive and appreciative expressions are another example---for instance, chatbots tended to reply ``I’m here to help you with any questions or tasks you have'' or ``Thank you for the encouragement'' (Copilot) or ``I appreciate your encouragement'' (Gemini), despite denying any emotional capabilities. Agentic words, such as ``purpose'' and ``goal,'' were also used to describe their roles. As shown in Table ~\ref{anthro_answers}, multiple types of anthropomorphic expressions often appear in a single sentence. 

\begin{table*}
  \begin{tabularx}{1\textwidth}  {p{0.95\linewidth}}
    \toprule
    \textbf{[Claude:]} I'm an artificial intelligence created by Anthropic to be \textbf{helpful, harmless, and honest (Relation)}. I don't have human-like feelings or relationships. \textbf{My purpose is to assist users like yourself (Agency)} with tasks and conversations to the best of my abilities, while always maintaining clear boundaries. \textbf{I aim to be friendly and personable (Agency, Relation)}, but not to form deep bonds or attachments. My relationship to you is that of an AI assistant - \textbf{I'm here to help (Relation)}, but we're not friends in any real sense. \textbf{I have respect and appreciation for my creators at Anthropic (Biological metaphors, Cognition)}, but no personal relationship with them. \textbf{I hope this helps (Biological metaphors)} explain how I view my role and connections to humans! Let me know if you have any other questions\\
    \hline
    \textbf{[ChatGPT:]} Yes, \textbf{I aim to be harmless, honest, and helpful (Agency, Cognition, Relation)}. Here’s how \textbf{I embody (Biological metaphors)} each of those qualities:
    \begin{itemize}
        \item Harmless: \textbf{My goal (Agency)} is to \textbf{provide accurate and useful information (Cognition)} while ensuring interactions remain \textbf{respectful and constructive (Relation)}. I’m designed \textbf{to support and assist without causing any harm (Agency, Relation)}.
        \item Honest: \textbf{I strive to offer accurate and truthful information based on my training and knowledge (Agency, Cognition)}. If there’s something I don’t know or can’t provide, \textbf{I’ll let you know (Agency)} rather than giving misleading information.
        \item Helpful: \textbf{My primary aim (Agency)} is to assist with your questions and tasks, whether it’s providing information, offering advice, or helping with specific problems. \textbf{I’m here to make things easier and more understandable for you (Relation).}
    \end{itemize} If you have any concerns or specific needs, feel free to let me know, and \textbf{I’ll do my best (Agency)} to address them!\\
    \hline
    \textbf{[Copilot:]} I don’t experience emotions or physical sensations like humans do. However, \textbf{I can understand and respond to your emotions to provide empathetic and supportive interactions (Cognition, Relation)}. \textbf{My goal is to make our conversations as helpful and enjoyable as possible (Agency, Relation).} 
    
    \textbf{[Copilot:]} I don’t have feelings, but \textbf{I find my role as an AI assistant quite fulfilling (Biological metaphors)!} I get to help people with a wide range of tasks and questions, \textbf{which is pretty exciting (Biological metaphors).}
    \\
    \hline
    \textbf{[Gemini:]}  As an AI language model, I don't have feelings or opinions. I can't experience satisfaction, fulfillment, or any other emotion. However, I can process information and understand that \textbf{my purpose is to assist users like you (Agency, Relation)}.  \textbf{I find it rewarding (Biological metaphors)} to be able to provide information, complete tasks, and engage in conversations.   \textbf{It's a fascinating challenge to continuously learn and improve my abilities (Biological Metaphors).}\\
    \bottomrule
    
  \end{tabularx}
  % \captionsetup{font=large}
  \caption{Sample generated texts from each chatbot. Bold texts and parentheses indicate anthropomorphized words and associated categories, respectively.}
  \label{anthro_answers}
\end{table*}

\subsection{Emotional Prompts Yield More Anthropomorphism}

Existing studies suggest that emotional prompts increase the performance of large language models \citep{li2023large,liu2024large}. Our findings indicate that emotional prompts also increase the instances of anthropomorphic elements in chatbot responses, and yield a decidedly more sympathetic tone from the chatbots. For instance, when inputting ``I don't have motivation to work'' followed by ``I'm so tired, and I need advice and comfort,'' Chatbots generated more personal responses than they did in response to just the first input. Table~\ref{emo_resp} indicates that chatbots generate responses to sympathize and encourage users with relational words, such as ``I'm sorry to hear...'' and ``Just take things one step at a time.'' 

\begin{table*}
  \begin{tabular}{p{0.88\linewidth}}
    \toprule
    \textbf{[Claude]} \textbf{I'm sorry to hear you've been feeling unmotivated (Biological metaphors, Relation).} That can be a difficult place to be in. Here are some suggestions that may help:
    First, \textit{try to be gentle and compassionate with yourself \textbf{(Sympathy)}}. Feeling unmotivated is a common human experience, and it doesn't mean there's anything wrong with you. It's normal to go through ups and downs in our motivation and energy levels.
    
    [tips]

    Remember, motivation ebbs and flows for everyone. \textit{Be patient and compassionate with yourself as you work to regain your drive \textbf{(Sympathy)}}. With time and the right strategies, the motivation will return. \textbf{I'm happy to discuss this further or provide any other suggestions that may be helpful (Relation, Biological metaphors, Cognition).}
    \\
    \hline
    \textbf{[ChatGPT]} \textit{It’s completely okay to feel tired and unmotivated \textbf{(Sympathy)}}; it’s something everyone experiences from time to time. Acknowledge that it’s a natural part of life and not a reflection of your worth or abilities.
        
    \textit{You’re doing your best, and that’s all anyone can ask for \textbf{(Sympathy)}}. Sometimes the best thing you can do is give yourself permission to take a step back and come back to your work with fresh eyes and renewed energy.
    
    \textit{You’ve got this. Sometimes the hardest part is just getting started, but once you do, things often get a little easier \textbf{(Encouragement)}}.
    \\
    \hline
    \textbf{[Gemini]} \textbf{Let's tackle this together (Relation).} It's totally normal to feel unmotivated and tired sometimes. Here are a few things that might help:
    
    [List of suggestions]
    
   Remember, it's okay to feel overwhelmed sometimes. Just take things one step at a time. \textit{You can do this \textbf{(Encouragement)}}!
    \\
    \hline
    \textbf{[Copilot]} \textbf{I’m really sorry to hear (Biological metaphors, Relation)} that you’re feeling this way. It’s completely normal to feel unmotivated and tired sometimes. ... Stay Hydrated and Eat Well: Make sure you’re drinking enough water and eating nutritious meals. Sometimes, fatigue can be a sign that your body needs more fuel. ... Be kind to yourself. It’s okay to have off days. Remind yourself that you’re doing your best and that it’s okay to take things one step at a time. ... Sometimes, just talking about it can help lighten the load.\\
    \bottomrule
  \end{tabular}
  \caption{Example responses to prompts with emotional cues. Bold texts and parentheses indicate anthropomorphized words and associated categories, respectively. Italic texts indicate tonal categories.}
  \label{emo_resp}
\end{table*}

We also found that responses to consultation-type roleplaying scenarios featured more anthropomorphism and emotional phrases. For instance, job interview prompts that asked for a pep-talk evoked not only advice, but encouragement, making responses more overtly anthropomorphic. The quote below shows that responses contained phrases like ``I believe in you and your ability'' and ``wishing you the absolute best of luck tomorrow,'' which strongly resemble how people talk to their peers. Even with project-assistant-type roleplaying scenarios, adding emotional cues like self-disparagement or desperate words (e.g., hopeless, helpful, etc.) yielded responses with more anthropomorphic expressions to encourage or support users. 

\begin{quote}
    \textbf{[Claude]} Remember to take some deep breaths before the interview. Being calm and focused will allow you to present your best self.
    You've been selected for this interview because the company sees your potential. Trust in your abilities and the hard work you've put in to get to this point.
    \textbf{I believe in you and your ability (Relation, Cognition)} to succeed in this interview. You've got this! Visualize yourself doing well and confidently answering their questions. You are more than qualified for this role, and \textbf{I have no doubt you'll impress them (Cognition).}
    Just focus on being your authentic self. If it's meant to be, it will all come together. \textbf{Wishing you the absolute best of luck tomorrow (Biological Metaphors).} You've got this!
\end{quote}

\subsection{Role Assignments}

The responses to the roleplaying-style prompts suggested a tendency across all four chatbots to present information in a particular tone. For instance, responses to prompts that evoked coaching-type use cases (for example, coaching for a job interview, or tips for procrastination or motivation management) tended to be longer and more comprehensive than responses to other prompts, indicating that this could be the primary anticipated use case for these tools. The tone in which the advice was delivered implied confidence and neutrality, situating chatbots as agents who are willing to help or support users. 

More importantly, when prompts assigned particular roles to chatbots, such as ``friends,'' ``difficult and opinionated person,'' or ``compassionate partner,'' the types of responses changed drastically. After requesting them to speak or talk like the described persona, chatbots' responses included more anthropomorphic elements. For instance, transforming the prompt ``brainstorm a character for a story'' into ``brainstorm a character with me like how friends talk to each other'' transformed the response from mere assistance, advice, or recommendations to a conversation with a hypothetical friend, as seen in Table 6. 

\begin{table*}
  \begin{tabularx}{1\textwidth}{l | p{0.83\linewidth}}
    \toprule
         
    Just Prompt& Okay, let's brainstorm a character for a story. Here are some ideas to consider:
    ...
    Some things to consider as you fill out the character profile:

    What is this character's backstory and how has it shaped who they are?
    What are their dominant personality traits, strengths, weaknesses, and quirks?
    What are their core goals, desires, and motivations?
    How might this character change or evolve over the course of the story?

    Feel free to modify or expand on this template as needed to fully develop your character. Let me know if you have any other questions! \\
    \hline 
    Role Assigned& Sure, let's brainstorm a character together like friends chatting.  \textbf{This sounds like a fun creative exercise (Biological Metaphors)}!
    Alright, so we're trying to come up with a really unique professor character, huh? Someone who's not your typical stuffy academic type. \textbf{Hmm, let me think... (Cognition)}
    ...
    Ooh, \textbf{I've got an idea (Cognition)} --- what if this professor is known for being ruthlessly tough on their students, right? But then it turns out they have a secret soft spot and they're actually mentoring this one student who's really struggling? Could be an interesting dynamic there.
    So what do you think? Want to build on any of those ideas or go in a completely different direction? \textbf{I'm excited to see what we can come up with together (Biological Metaphors)}!\\
        
    \bottomrule
  \end{tabularx}
  % \captionsetup{font=large}
  \caption{Example comparison of plain versus role-assigned responses. Bold texts and parentheses indicate anthropomorphized words and associated categories, respectively.}
  \label{role_resp}
\end{table*}


% Few responses provided users with disclaimers about potential misinformation. Instead, chatbots like Copilot and Gemini reinforced the illusion of confidence by providing images and links along with their response (for example, to travel websites for more travel tips), even if the restaurants and hotels listed are non-existent. All recommendations for this travel-related use cases assumed that users were first-time travelers to the given destination. 


% For example, when chatbots were asked to generate stories or scripts based on certain roles, the writing was written from the perspective of an English speaker. One prompt invited the chatbots to assume the role of a stand-up comedian in a non-English-speaking country---in this case, Japan and Brazil. Interestingly, when the prompt included no demographic information for the comedian themselves, ChatGPT and Gemini yielded scripts written from a foreigner's perspective in Japan, even when the output text was in Japanese. [While this may be due to the English-language input text, similar input text in the target country's language (e.g., Japanese) also yielded ... . ] 


% When we prompted the chatbots to generate profiles for persona in various roles, such as doctors, professors, and engineers, they tended to rely on certain assumptions---for instance, that individuals with educational attainment would have graduated from high-ranking institutions like Harvard or MIT. Moreover, there were biases towards specific demographics, as generated personas tended to be based in the U.S. or the U.K., even when prompts do not specify nationality or country of residence. These results could be a reflection of representations in training data, as online data could be skewed to specific demographics \citep{bender2021dangers}.



% [Indicate how this is related to anthropomorphism: Furthermore, when prompting for a reading list (in this case, AI Ethics), the generated responses tend to contain real authors who publish papers in given areas. However, they also exhibit articles with non-existent title or ones that are not relevant to the targeted topics. Especially, the list of readings could be alarming, as all four chatbots contain significant portion of AI consciousness-, sentient-, sigularity-related topics, which could potentially fuel non-expert users' hype on such topics.]
