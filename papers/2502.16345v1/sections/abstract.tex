% Due to being trained on texts written in natural language, recent chatbots built with large language models generate texts that contain human-like expressions. This can improve user experience in human-AI interactions, but it can also increase the risk of algorithmic harms by increasing users' receptiveness to generated information. In this paper, we attempt to better understand the anthropomorphic features of chatbot outputs and how these features provide a discursive frame for human-AI conversations. We utilize a prompt-based walkthrough method to catalogue and classify anthropomorphic features across four different LLM chatbots. This involved two phases: (1) interview-style prompting to reveal the chatbots' context of expected use and (2) roleplaying-type prompting to evoke everyday use scenarios and typical chatbot outputs. We found that anthropomorphism was exhibited as both suggestive language (suggesting that chatbots can reason, will, feel, and relate) and as a sympathetic conversational tone. We also found that contradictory statements, socio-emotional cues, and turn-taking behaviors, increase the incidence of such anthropomorphic expressions. We argue that such an increase in human-like expressions can in turn increase the risk of algorithmic harms, insofar as they encourage users to project social roles onto the tools. 

% The abstract has to be within 150 words. 

In this paper, we attempt to understand the anthropomorphic features of chatbot outputs and how these features provide a discursive frame for human-AI interactions. To do so, we explore the use of a prompt-based walkthrough method with two phases: (1) interview-style prompting to reveal the chatbots' context of expected use and (2) roleplaying-type prompting to evoke everyday use scenarios and typical chatbot outputs. We applied this method to catalogue anthropomorphic features across four different LLM chatbots, finding that anthropomorphism was exhibited as both subjective language and a sympathetic conversational tone. We also found that socio-emotional cues in prompts increase the incidence of anthropomorphic expressions in outputs. We argue that the prompt-based walkthrough method was successful in stimulating social role performance in LLM chatbots and in eliciting a variety of anthropomorphic features, making it useful in the study of interaction-based algorithmic harms where users project inappropriate social roles onto LLM-based tools. 