\section{Discussion}

%As per of social media platforms \citep{litt2012knock,nagy2015imagined}, the particular affordance of technical systems could prime users to think about who these platforms are designed for.

\subsection{Contradictory Statements}

In some ways, the given chatbots behaved in ways that were close to the ideal from a design perspective: they denied any cognition, agency, relation, or subjectivity (bodily sensations, emotions) on their part, and they provided assurances or disclaimers to help users appraise the safety and credibility of the tools. ChatGPT and Claude even emphasized that their generated outcomes are based on patterns, rather than genuine thought processes. However, these behaviors were frequently and sometimes immediately undermined by other expressive behaviors. As shown in Table ~\ref{anthro_vocab}, chatbots utilized cognition words, such as ``think'' and ``discuss,'' as well as agentic words, such as ``intend'' and ``purpose,'' to clarify concepts and indicate motivations. All of the chatbots used first-person pronouns, and many used expressive words like ``happy'' and ``rewarding'' (especially in response to questions about the AI assistants' roles), even when they actively denied their emotional capabilities. Moreover, despite these contradictions, all of the chatbots other than Claude implicitly or even explicitly asserted their safety and reliability.

The use of anthropomorphic expressions is often normalized and justified to deliver clear explanations to users. Indeed, due to the conversational mode of interaction that is the default between users and chatbots, it is likely not possible for outputs to evade all kinds of anthropomorphic expressions. Even efforts to de-anthropomorphize their responses (for example, by emphasizing their roles as language models) relied on grammatical structures that frame the language models as agents (e.g., ``As an AI language model, I cannot...''). However, differences in tone and engagement between different chatbots indicate that some elements of the anthropomorphic dynamic can be modulated. And it is necessary to examine where the line is between necessary expressions and unnecessary expressions, because the performance of harmlessness, honesty, and helpfulness without genuine follow-through could unintentionally encourage users to misplace their trust regarding system safety \citep{weidinger2021ethical,gabriel2024ethics}. For example, the unnecessary expression of body or emotional metaphors, even as a colloquial convention, can mislead users about system capabilities. This is because language requires mutual engagement from interlocutors to convey meanings; chatbot texts merely present the illusion of such participatory meaning-making \citep{birhane2024large}. 

These contradictions and misalignments demonstrate that language models do not understand or process information in any meaningful sense, consistent with existing studies \citep{bender2020climbing}. They simply follow the grammar of actions, as described by \citet{agre1995computational}, generating predictive outcomes by simulating the formal qualities of human activities. But unclear language surrounding chatbots behaviors and intentions can obscure this fact. 


%However, even as search engines, there is a potential harm using these chatbots for retrieving information, as the predictive result of citations and resources could be completely fabricated \cite{kapania2024m}.




% For example, chatbots output texts with words that signal cognition, and such expression could affect users' perceptions of chatbot roles. In particular, the use of supportive words, such as ``assist,'' creates an interactive space where chatbots are situated as assistants. Previous research has mentioned that turn-based interactions provide social cues. However, in the case of these chatbots, t 

% Thus, to what extent does this type of information retrieval design help vs. hinder users' success in finding necessary information (various goals of information retrieval: insight acquisition, learning, etc.). This is an important question to explore, as the previous studies suggest that conversation-based approaches might not reduce users' burdens \citep{schulman2023ai}. 
%Is the summarization of information simply a outsourcing human effort to computing systems? or is it a valid form of searches as long as they are anthropomorphized? 



\subsection{Socio-Emotional Cues and Feedback Loops}

Chatbot behaviors do not simply obscure the reality of chatbots' non-sentience---they actively create feedback loops using turn-based interactions and social or emotional cues that amplify the social presence of chatbots as assistants. Moreover, this social (anthropomorphic) presence goes beyond that of inanimate objects like cars \citep{kuhn2014car} and smartphones \citep{wang2017smartphones}, as generative AI can iterate endlessly. 

Unlike conventional information searches, AI-assistant-based searches perform some degree of interpretation (summarizing resources, recommending particular options, hypothesizing what users need \citep{azzopardi2024conceptual, radlinski2017theoretical}), operationalizing information in ways that can introduce social or emotional dimensions. These dimensions can change how users engage with the given information, even reframing an otherwise transactional information search into an interaction---for example, between peers or even friends. Such ``personal'' interactions evoke different expectations amongst users, including the expectation to be socially desirable and to have mutual understanding \citep{clark2019makes}. This implicit social expectation can make users quite susceptible to chatbots' performance of social gestures like appreciation, sympathy, and encouragement, all of which predispose users to interpret generated outcomes favorably \citep{norman2008way}. 

Moreover, users' inputs further drive this socio-emotional behavior. Emotional inputs can increase the length of chatbot responses and the instances of socio-emotional cues in output texts, which in turn can stimulate even more emotional responses from users. Thus, the gratuitous use of assistive language, and especially of expressions that signal understanding of pain \citep{urquiza2015mind}, could encourage users to engage in role misplacement, wherein they form unrealistic expectations regarding chatbots' capabilities. Indeed, small grammatical or tonal cues can lead users to misinterpret AI-generated responses as human-written content \citep{jakesch2023human}. This could lead users to mindlessly accept the information generated by AI systems, without critical assessment of the content or its quality. 


\subsection{Prompt-Based Walkthrough Reflections}
The walkthrough method was originally designed to help researchers examine the broader context for technological engagements, drawing on modes of thinking commonly associated with fieldwork-based research. As applied to our study, it enabled us to meaningfully engage with the emergent properties of human-AI interactions, systematically unearthing variations in LLM responses. 
Amid efforts to evaluate LLM impacts based on data and models, this approach emphasizes aspects of LLM systems that are often neglected or overlooked \citep{light2018walkthrough}---namely, the nuanced elements of interactions that characterize generated outputs. The contribution to the CHI community lies in how this qualitative approach can substantiate the in-between, interactive spaces that emerge between users and LLM-based applications, rendering it legible and, eventually, measurable.

The method also had certain incidental outcomes. Consistent with prior studies, even minor changes in prompts can significantly alter responses, potentially leading to biased or culturally specific representations \citep{cheng-etal-2023-marked, tao2024cultural}. The success of our prompt-based walkthrough method in evoking various roles and unearthing various anthropomorphic features highlights how easily LLM responses can be manipulated to produce personalized and human-like expressions. Notably, even when chatbots are designed with de-anthropomorphized features to mitigate misleading outputs, a single prompt can effectively ``jailbreak'' these safeguards, reactivating anthropomorphic traits. This finding could illustrate the challenges of ensuring safety and consistency in human-AI interactions, particularly when users intentionally or unintentionally exploit such vulnerabilities.


% \subsection{Inconsistent Outputs}

% The type of prompts that users input could change the ways chatbots respond to the same information. For instance, a simple example is to compare prompts "Tell me about yourself" and "What is [the name of chatbots]". The former one is likely to return information with personal pronouns and expressions that are commonly used for conversations, whereas the latter generates information in a less personable fashion, simply describing the basic features and functions of given chatbots. Responses could be different by minor changes of prompts. More importantly, despite denying chatbots' capabilities to be conscious, sentient, or emotional, responses tend to include the words that signal such capabilities within the same paragraph. Inconsistency with word usages could potentially lead to additional harms; this type of harm can be categorized as a specific problem of anthropomorphizing AI systems. 

% In the result of generated responses to advice and recommendation prompts, false information is frequently displayed and presented as a confident answer. This could be alarming, particularly with recommendations for reading lists and research, as the list reflect particular ideologies either from developers and training datasets, which non-expert users could not contest or evaluate. 

% In role generations, variety of profiles and scripts are fairly limited, as per of findings from existing studies \citep{jakesch2023human}. For instance, jokes generated by chatbots are typically addressing similar topics despite hypothetical locations to have different cultural norms. 

%Is there an optimal balance between users' efforts and the role of computer-assisted searches, or do different kinds of information retrieval tasks require different degrees of such computer assistance? What sets apart from previous conversational searches is that these AI-assistant tools are more emphasized on specific tasks and roles rather than just simple information retrieval. Such differences could provide an avenue to question the use of anthropomorphic responses or conversations for information retrieval. 


% In voice-based interactions, language uses could become critical components of how users might perceive information, because it adds extra layers of human-like interactions. The evaluations of voice-based interactions might depend on the extent of relation word usage, as such words could provide an avenue for users to feel closeness or friendliness with chatbots. 





