\section{Introduction}

Chatbot-based applications, such as ChatGPT and Gemini, are often described as virtual assistants, ones that enable language-based tasks in various domains. Chatbots deliver information to users in a different way than conventional search engines \citep{liao2020conversational}, shouldering the work of comparing and synthesizing information, and thereby reducing the complexity and cumbersomeness of information-seeking tasks. However, because these chatbots are trained on natural language texts and reinforcement learning with human-feedback \citep{ouyang2022training, bai2022training, ganguli2022red}, their responses exhibit human-like expressions, simulating tones and manners typical of conversations between people. Such anthropomorphic features facilitate human-computer interactions, improving user experience \citep{kulms2019more, sharma2023human, liao2020conversational}, but they can also affect the ways in which people perceive and interpret information \citep{langer2022look}. Taken to an extreme, the features of anthropomorphic chatbots can mislead people about these technical systems' capabilities \citep{leong2019robot, ruane2019conversational, salles2020anthropomorphism, raji2022fallacy, placani2024anthropomorphism} or facilitate echo-chamber-like reasoning, similar to that seen in recommendation systems \citep{sharma2024generative}. Unrestrained or misplaced anthropomorphism could thus lead to considerable unanticipated algorithmic harms.

Given this risk for harms, it is necessary to catalogue the variety and extent of anthropomorphic features in current AI assistant tools---not just in LLM datasets, but in LLM outputs, where training intersects with user inputs. However, existing methodologies designed to focus on such ``front-end'' spaces (user interface studies, platform analysis, etc.) struggle to contend with the limitlessness of generative systems, which makes it impossible to exhaustively catalogue phenomena. As a possible solution, this paper aims to adapt the existing walkthrough method developed by \citet{light2018walkthrough} and \citet{duguay2023stumbling} to evaluate the behaviors of LLM chatbots, using interviewing and roleplaying \citep{shanahan2023role, shao-etal-2023-character, wang-etal-2024-incharacter, wang-etal-2024-rolellm, tidy2024character} as a novel approach to evoke common use cases and elicit a range of chatbot behaviors.

We trial this adapted ``prompt-based'' walkthrough method in an exploratory study, wherein we elicit a wide variety of human-like ``roles'' that chatbots---namely ChatGPT, Gemini, Claude, and Copilot---assume during various possible use cases. Not only does this shed light on different types of anthropomorphism exhibited in chatbot applications (and on how roles shape chatbot behaviors and generated outputs), but it also tests whether this adapted method makes it possible to systematically analyze the breadth of human-computer interactions within LLM chatbots. We believe that this roleplaying-based approach can provide a new means to evaluate the social and design implications of conversational AI systems. In future studies, we hope to apply it to develop a taxonomy of anthropomorphism in design features, echoing the work of \citet{inie2024ai} and \citet{abercrombie-etal-2023-mirages}.

%which can be prompted to perform in both task-specific and open-ended cases. 

Identifying types and tiers of anthropomorphism can help shed light on the affordances and limitations of chatbot applications, delimiting realistic expectations and reasonable guidelines for their use. It can also allow for modulation in the face of different harms, allowing us to distinguish what aspects of anthropomorphism are acceptable in which cases, and what strategies should be used to manage human-AI interactions in different settings. For example, when should harms be mitigated by de-anthropomorphizing chatbots, and when should they be mitigated by improving user literacy regarding AI capabilities and limitations? 