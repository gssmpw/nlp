\section{Limitations and Future Research}

Our study has several limitations. Firstly, although valuable insights on anthropomorphism could be collected from practitioners and everyday users, our study did not include participants. Though our walkthrough method deliberately sought to prioritize the expression of roles and instance of anthropomorphism in chatbot outputs, the impacts of this anthropomorphism should be examined through user studies, as users' perception of human-likeness likely depend on the contexts in which they interact with these systems \citep{nowak2004influence}. Therefore, the next step is to validate our findings against users' perceptions. Secondly, this study utilizes qualitative approaches to categorize anthropomorphized language and design. Future studies could employ quantitative methods to classify a larger set of conversation logs \citep{zhao2024wildchat} to validate whether our qualitative findings are aligned with larger trends. Future studies could also pay specific attention to different reinforcement learning strategies used to train different language models in order to reveal their influence on human-like expressions. 

Finally, this study primarily focused on linguistic modes of anthropomorphism; the next step would be to categorize and classify anthropomorphic elements in multi-modal tools, including voices, images, and sounds. Role projection and assumption is more pronounced with tools that have voice features, as these tools enable interactions that resemble natural conversation even more closely.

In our own future work, we hope to iterate on this approach to develop a taxonomy of anthropomorphic features, developing ways to not only identify anthropomorphism in chatbot outputs, but to empirically and quantitatively measure it. Little work exists that attempts to devise evaluation measures for anthropomorphism within human-AI interactions; though one existing study in NLP set out a method for measuring anthropomorphism based on language structures like pronouns or subjects \citep{cheng2024anthroscore}, that study only examined anthropomorphism in academic papers, not in human-AI interactions with chatbots.  It would be important to seek methods to measure anthropomorphism that consider both system design and generated outcomes. Human-like expressions are subtle, and multiple categories of expressions can be utilized within the same sentence (e.g., agency, cognition, and biological metaphors). Focusing on linguistic features could further uncover the blurry lines between harmful anthropomorphic expressions and ones that are important for human-computer interaction. This paper contributes to this effort by proposing one such method and clarifying the range of human-like features embedded within AI assistant tools. 

\section{Conclusion}

%Models vulnerability
In the current design of LLM chatbots, human-like expressions are frequently implemented as a way to facilitate interactions between users and applications. This study adapted the existing walkthrough method to begin cataloguing instances of human-like (anthropomorphic) expressions in LLM chatbots, despite the difficulties presented by the limitlessness of generative systems. By incorporating interviewing- and roleplaying-based prompts that evoked common scripts, use cases, and roles in the LLM chatbots, we found that anthropomorphic expressions ranged from assertions of cognitive, agentic, emotional, or relational ability to sympathetic tones and behaviors. We also found that emotional cues increase the incidence of such anthropomorphic expressions. Further research is needed to measure the extent of such anthropomorphism and its effects on users' perceptions of generated information. 

This study illustrates that issuing prompts with particular social roles for chatbots to embody evokes different levels of anthropomorphized responses. In practice, LLM chatbots are given many social roles, including in sensitive areas like law \citep{greco2023bringing}, healthcare \citep{thirunavukarasu2023large}, and education \citep{extance2023chatgpt}. Misinterpreting chatbots' capabilities to perform tasks in these areas can have devastating effects on users' real-life well-being.\footnote{Previously, \citet{cheong2024not} conducted interviews with legal experts on using LLMs to generate legal advice. Their findings suggest that human-like responses should be avoided due to their potential to effectively misdirect users. Providing false guidance in certain areas like law and health could lead to potential harms in the real world.} Even so, anthropomorphic expressions in chatbot outputs are not as well-studied as models or training datasets, in terms of their risks for algorithmic harms. We expect that, in concert with other methods, the prompt-based walkthrough method employed in this study can help to empirically evaluate such ``human-like'' features, helping to derive taxonomies and measures that can be used in guidelines to avoid potential harms.
