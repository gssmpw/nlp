\section{Related work}
\label{RL}

\subsection{Medical Image Segmentation} 
%As mentioned earlier, researchers typically apply distinct methods for 2D**Ronneberger et al., "U-Net: Deep Learning for Biological Image Segmentation"** and 3D**Cicek et al., "3D U-Net: Learning Dense Volumetric Segments from Sparse Annotation"** medical images. 2D models are used for planar images or slices, while 3D models are intended to learn volumetric features. % implicitly.
As mentioned earlier, researchers typically apply 2D **Ronneberger et al., "U-Net: Deep Learning for Biological Image Segmentation"** for planar images or slices, and 3D **Cicek et al., "3D U-Net: Learning Dense Volumetric Segments from Sparse Annotation"** to learn volumetric features implicitly.
**Hollandi et al., "Self-adaptive deep learning framework for medical image segmentation"** introduced a versatile, self-adaptive deep learning framework specifically designed for medical image segmentation tasks, extending the U-Net architecture and its 3D version. 
**Li et al., "Transformer-based architecture with Convolutional Neural Networks (CNNs) for medical image segmentation"** pioneered the combination of Transformer-based architecture with Convolutional Neural Networks (CNNs) for medical image segmentation, applying a slice-by-slice inference on 3D volumes without considering interrelationships among slices. 
Some works **Gao et al., "Medical image segmentation using spatial-temporal cues and report texts as guidance"** utilize spatial-temporal cues and **Xu et al., "Integrating report texts into medical image segmentation models"** introduce report texts as guidance to enhance segmentation performance. However, these models are limited to specific image modalities and tasks.  

\subsection{Medical Vision-Language Models} Medical vision-language models have achieved success across multiple downstream tasks, including diagnosis classification **Wang et al., "Auto-generation strategies for medical prompts and transferred large vision language models"**, lesion detection **Guo et al., "Large vision language models for medical lesion detection"**, image segmentation **Kim et al., "Contrastive Language-Image Pre-training (CLIP) for medical image segmentation"**, report generation **Chen et al., "BiomedParse: A biomedical foundation model for joint segmentation, detection and recognition"**, and visual question answering **Liu et al., "Visual question answering with large vision language models"**. 
**Guo et al., "Auto-generation strategies for medical prompts and transferred large vision language models"** designed auto-generation strategies for medical prompts and transferred large vision language models for medical lesion detection. 
**Kim et al., "Contrastive Language-Image Pre-training (CLIP) for medical image segmentation"** incorporated text embedding learned from Contrastive Language-Image Pre-training (CLIP) to segmentation models.
**Chen et al., "BiomedParse: A biomedical foundation model for joint segmentation, detection and recognition"** proposed BiomedParse, a biomedical foundation model that can jointly conduct segmentation, detection and recognition across nine imaging modalities.
**Wang et al., "Segment Anything Model (SAM) for medical scenarios driven by text prompts"** built a model based on Segment Anything Model **Xu et al., "Segformer: A Simple and Efficient Segmentation Model"** for medical scenarios driven by text prompts, but the model focused on 3D medical volume segmentation and did not consider the sequential relationships between scans. 
To the best of our knowledge, we are the first to use medical text prompts to specify segmentation targets across medical image sequences.

% \textbf{Text Promptable Segmentation.} 
% TGANet TextSAM TP-SIS.

\subsection{Referring Video Object Segmentation}
**Mao et al., "Inferring segmentation from natural language input"** were the first to propose inferring segmentation from a natural language input, extending two popular actor and action datasets with natural language descriptions.
**Lin et al., "Large-scale referring video object segmentation (RVOS) dataset and unified referring video object segmentation network"** constructed the first large-scale referring video object segmentation (RVOS) dataset and proposed a unified referring video object segmentation network.
**Zhu et al., "Transformer-based RVOS frameworks for end-to-end segmentation of referred objects"**, **Wang et al., "End-to-end Transformer-based RVOS frameworks"** presented Transformer-based RVOS frameworks, enabling end-to-end segmentation of the referred object.
**Li et al., "Explicit query propagation for online models in referring video object segmentation"** designed explicit query propagation for an online model. 
**Liu et al., "Semantic integrated module and visual-linguistic contrastive loss for semantic supervision on video-level object representations"**, **Wang et al., "Inter-frame feature communication for different object embeddings and tracking along the video"** aggregated inter- and intra-frame information via a semantic integrated module and introduced a visual-linguistic contrastive loss to apply semantic supervision on video-level object representations.
**Zhu et al., "Multi-modal references and inter-frame feature communication for referring video object segmentation"**, **Liu et al., "Visual-linguistic contrastive loss and semantic supervision on video-level object representations"** enabled multi-modal references to capture multi-scale visual cues and designed inter-frame feature communication for different object embeddings for tracking along the video.

Inspired by these works, the Referring Medical Image Sequence Segmentation task processes both 2D and 3D medical data into image sequences, enabling in-depth exploration of sequence-level consistency guided by text prompts.