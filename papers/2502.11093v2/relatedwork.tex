\section{Related work}
\label{RL}

\subsection{Medical Image Segmentation} 
%As mentioned earlier, researchers typically apply distinct methods for 2D~\citep{ronneberger2015unet} and 3D~\citep{cciccek20163dunet, milletari2016vnet} medical images. 2D models are used for planar images or slices, while 3D models are intended to learn volumetric features. % implicitly.
As mentioned earlier, researchers typically apply 2D models~\citep{ronneberger2015unet} for planar images or slices, and 3D models~\citep{cciccek20163dunet, milletari2016vnet} to learn volumetric features implicitly.
\citet{isensee2021nnunet} introduced a versatile, self-adaptive deep learning framework specifically designed for medical image segmentation tasks, extending the U-Net architecture and its 3D version. 
\citet{chen2021transunet} pioneered the combination of Transformer-based architecture with Convolutional Neural Networks (CNNs) for medical image segmentation, applying a slice-by-slice inference on 3D volumes without considering interrelationships among slices. 
Some works~\citep{ji2021pns, painchaud2022echocardiography, lin2023shifting} utilize spatial-temporal cues and \citep{li2023lvit,zhong2023ariadne,bui2024mmiunet} introduce report texts as guidance to enhance segmentation performance. However, these models are limited to specific image modalities and tasks.  

\subsection{Medical Vision-Language Models} Medical vision-language models have achieved success across multiple downstream tasks, including diagnosis classification~\citep{moon2022medvill, medclip, chexzero, lu2023mizero}, lesion detection~\citep{qin2023mvlm, huang2024adapting}, image segmentation~\citep{zhao2023one, li2023lvit}, report generation~\citep{yan2022clinical-bert, BioViL-T}, and visual question answering~\citep{singhal2023med-palm, moor2023med-flamingo}. 
\citet{qin2023mvlm} designed auto-generation strategies for medical prompts and transferred large vision language models for medical lesion detection. 
\citet{liu2023clip-driven} incorporated text embedding learned from Contrastive Language-Image Pre-training (CLIP) to segmentation models.
\citet{zhao2024biomedparse} proposed BiomedParse, a biomedical foundation model that can jointly conduct segmentation, detection and recognition across nine imaging modalities.
\citet{zhao2023one} built a model based on Segment Anything Model~\citep{kirillov2023sam} for medical scenarios driven by text prompts, but the model focused on 3D medical volume segmentation and did not consider the sequential relationships between scans. 
To the best of our knowledge, we are the first to use medical text prompts to specify segmentation targets across medical image sequences.

% \textbf{Text Promptable Segmentation.} 
% TGANet TextSAM TP-SIS.

\subsection{Referring Video Object Segmentation}
\citet{gavrilyuk2018actor} were the first to propose inferring segmentation from a natural language input, extending two popular actor and action datasets with natural language descriptions.
\citet{seo2020urvos} constructed the first large-scale referring video object segmentation (RVOS) dataset and proposed a unified referring video object segmentation network.
\citet{wu2022referformer} and \citet{botach2022mttr} presented Transformer-based RVOS frameworks, enabling end-to-end segmentation of the referred object.
\citet{wu2023onlinerefer} designed explicit query propagation for an online model. 
\citet{luo2024soc} aggregated inter- and intra-frame information via a semantic integrated module and introduced a visual-linguistic contrastive loss to apply semantic supervision on video-level object representations.
\citet{yan2024referred} enabled multi-modal references to capture multi-scale visual cues and designed inter-frame feature communication for different object embeddings for tracking along the video.

Inspired by these works, the Referring Medical Image Sequence Segmentation task processes both 2D and 3D medical data into image sequences, enabling in-depth exploration of sequence-level consistency guided by text prompts.