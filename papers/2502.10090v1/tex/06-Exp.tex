\section{Experiments}
In this section, we perform a series of experiments aimed
at addressing the following questions.
\begin{itemize}
    \item Q1: Can our proposed hierarchical assembly graph generation module effectively extract structured information from manuals? (see~\Cref{assembly_tree_gen}) 
     \item Q2: Can the per-step pose estimation be applicable to different categories of furniture and outperform previous settings? (see~\Cref{pose_exp}) 
     \item Q3: How effective is the proposed framework in the assembly of furniture with manual guidance? (see~\Cref{sec:Q1_exp}) 
     \item Q4: Can this pipeline be applied to real-world scenarios?(see~\Cref{real_world_exp})
     \item Q5: Can this pipeline be extended to other assembly tasks? (see~\Cref{gen_other_task})
     \item Q6: How should we determine and evaluate the key design choices of each module? (ablation experiments, see~\Cref{appd:VLM_ablation,appd:pose_ablation})
     % Appendix C and E.
\end{itemize}
In addition, we have included a comprehensive set of prompts utilized in the VLM-guided hierarchical graph generation process in~\Cref{appd: additional_prompts}
% Appendix K. 


\subsection{Hierarchical Assembly Graph Generation} \label{assembly_tree_gen}
In this section, we evaluate the performance of our VLM-guided hierarchical assembly graph generation approach. Specifically, we assess Stage II: Identifying Parts in Each Image using the IKEA-Manuals dataset~\cite{wang2022ikea}. We provide the rationale for excluding Stage I evaluation in~\Cref{appd: no_stage1}.
% Appendix H.

\begin{table}[htb!]
    \centering
    \setlength\tabcolsep{4pt} % Increased from 2pt for better spacing
    \begin{threeparttable}
    \captionsetup{width=\linewidth}
    \caption{\textbf{Assembly Plan Generation Results}.}
    \label{tab:assembly_plan}
    \vspace{3mm}
    \begin{tabular}{lcccc} % Corrected to 5 columns (l + 4 c's)
    \toprule
    Method & Precision & Recall & F1 Score & Success Rate \\ 
    \midrule
    SingleStep  & 0.220 & 0.220 & 0.220 & 0.220 \\  
    GeoCluster & 0.197 & 0.201 & 0.196 & 0.080 \\ 
    \textbf{Ours} 
    & \cellcolor{customblue}\textbf{0.690} 
    & \cellcolor{customblue}\textbf{0.680} 
    & \cellcolor{customblue}\textbf{0.684}
    & \cellcolor{customblue}\textbf{0.620}\\ 
    \bottomrule
    \end{tabular}
    \vspace{-2mm}
    \end{threeparttable}
\end{table}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/assembly_trees_qual_eval.pdf}
    \caption{\textbf{Qualitative results.} Our method significantly outperforms the baselines. SingleStep fails on moderately complex furniture, while GeoCluster generates physically impossible subassemblies (highlighted in red). In contrast, our approach closely aligns with the ground truth.}
    \label{fig:tree_viz}
\end{figure}

\textbf{Experiment Setup.} The IKEA-Manuals dataset~\cite{wang2022ikea} includes 102 furniture items, each with IKEA manuals, 3D parts, and assembly plans represented as trees in nested lists. We load each item's 3D parts into Blender and render an image of the pre-assembly scene. Moreover, we split the 102 furniture items into two sets. The first set consists of 50 furniture items with six or fewer parts, and the second set contains 52 furniture items with seven or more parts. We observe that current VLMs can effectively deal with the first set, and a significant portion of real-world furniture also contains fewer than seven parts (as seen in real-world experiments). Here, we report the results of the first set. Please refer to~\Cref{vlm_plan_complete,appd: additional_prompts} for complete results and prompts.
% Appendix D and Appendix K 
This rendered image, along with the manual, is processed by the VLM through the stages outlined in~\Cref{sec:graph_gen_full_desc} to generate a hierarchical assembly graph. Since we represent our graph as a nested list, we align our notation with the assembly tree notation used in IKEA-Manuals \cite{wang2022ikea}. In this subsection, we refer to our generated assembly graph as the \textit{predicted tree}.

\textbf{Evaluation Metrics.} We use the same metrics as IKEA-Manuals~\cite{wang2022ikea}, which include precision, recall, and F1 score to compare predicted and ground-truth nodes of the assembly tree. For detailed descriptions of these metrics, we refer readers to~\cite{wang2022ikea}.

The \emph{Matching} criterion for each node is defined as follows: We consider a predicted non-leaf node correct only if its set of leaf and non-leaf child nodes exactly matches that of the corresponding ground-truth node(With consideration of equivalent parts).
In other words, the predicted node must have the same children as its ground-truth counterpart. We compute precision, recall, and F1 scores based on this criterion.
% \[
% \text{Recall} = \frac{\text{\# of matched non-leaf nodes}}{\text{\# of ground-truth non-leaf nodes}}
% \]

% \[
% \text{Precision} = \frac{\text{\# of matched non-leaf nodes}}{\text{\# of predicted non-leaf nodes}}
% \]

The \textit{Success Rate} criterion measures the proportion of the predicted tree that exactly matches the ground-truth tree. We consider a predicted tree exactly matched if all its non-leaf nodes satisfy the \emph{Matching} criterion. %Thus, \textit{Accuracy} is binary: 100\% for a perfect match and 0\% otherwise. To evaluate the Accuracy, we compare the predicted graph to the ground-truth assembly plan. Since we store the ground-truth plan as a nested-list assembly tree, we extract the predicted assembly tree by retaining only the parent-child nodes and their edges from our hierarchical graph before comparison.

\textbf{Baselines.} We compare our VLM-based method against two heuristic approaches introduced in IKEA-Manuals~\cite{wang2022ikea}.
% For detailed descriptions of these baselines, we refer readers to ~\citet{wang2022ikea}.

\begin{itemize}
    \item SingleStep predicts a flat, one-level tree with a single parent node and $n$ leaf nodes.
    \item GeoCluster employs a pre-trained DGCNN \cite{wang2019dynamic} to iteratively group furniture parts with similar geometric features into a single assembly step. Compared to SingleStep, it generates deeper trees with more parent nodes and multiple hierarchical levels.
\end{itemize}

\textbf{Results.} As shown in \Cref{tab:assembly_plan}, quantitative results demonstrate that both baseline methods face challenges in generating accurate assembly trees under the Matching and Assembly criterion. In contrast, our VLM-guided method achieves significantly superior performance, with a success rate of \textbf{62\%}. These findings underscore the robust generalization capabilities when guided by well-structured prompts. \Cref{fig:tree_viz} provides qualitative results for two furniture items, illustrating the advantages of our approach in greater detail. With the ongoing development of more advanced VLMs, we expect further enhancements in assembly planning accuracy. Please refer to~\Cref{appd:VLM_ablation} 
% Appendix E 
for ablation results.


\subsection{Per-step Assembly Pose Estimation}\label{pose_exp}

\begin{table*}[ht]
\centering
\begin{threeparttable}
\caption{\textbf{Qualitative Results of Pose Estimation.}}
\label{tab:pose_estimation}
% \setlength\tabcolsep{4pt} % Increased space between columns
\renewcommand{\arraystretch}{1.2} % Increased vertical space between rows
% \captionsetup[table]{skip=5pt}
% \scriptsize % Small font size
\begin{tabular}{@{}l>{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm}@{}}
\toprule

& \multicolumn{3}{c}{GD$\downarrow$} & \multicolumn{3}{c}{RMSE$\downarrow$} & \multicolumn{3}{c}{CD $\downarrow$} & \multicolumn{3}{c}{PA$\uparrow$} \\

\cmidrule(lr){2-4}  \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}

Method & Chair & Lamp & Table & Chair & Lamp & Table & Chair & Lamp & Table & Chair & Lamp & Table\\

\midrule
 \citet{li2020learning} &1.847 &1.865 &1.894 &0.247 &0.278 &0.318 &0.243 &0.396 &0.519 &0.268 &0.121 &0.055\\
Mean-Max Pool &0.434 &1.118 &1.059 &0.087 &0.187 &0.200 &0.046 &0.229 &0.280 &0.457 &0.199 &0.107\\
\textbf{Ours}   & 
% Unscrew bottle cap
\cellcolor{customblue} \textbf{0.202} & 
\cellcolor{customblue} \textbf{0.826} & 
\cellcolor{customblue} \textbf{0.953} & 

\cellcolor{customblue} \textbf{0.042} & 
\cellcolor{customblue} \textbf{0.153} & 
\cellcolor{customblue} \textbf{0.172} & 

\cellcolor{customblue} \textbf{0.027} &  
\cellcolor{customblue} \textbf{0.189} & 
\cellcolor{customblue} \textbf{0.276} & 

\cellcolor{customblue} \textbf{0.868} &  
\cellcolor{customblue} \textbf{0.240} & 
\cellcolor{customblue} \textbf{0.184} \\

\bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}



\textbf{Data Preparation.}
We select three categories of furniture items from PartNet~\citep{mo2019partnet}: chair, table, and lamp.
For each category, we select 100 furniture items and generate 10 parts selection and subassembly division for each piece of furniture.
To generate the assembly manual images, we render diagrammatic images of parts at 20 random camera poses using Blender's Freestyle functionality.
We provide more details about it in~\Cref{appd: Per-step Assembly Pose Estimation Dataset}.
% Appendix A.
In general, we generate 12,000 training and 5,200 testing data pieces for each category.
% \yiwei{min num part $= 2$, max num part $= 5$, whether or not to claim}

\textbf{Training Details.}
For the Image Encoder $\mathcal{E}_I$, we selected the encoder component of DeepLabV3+, which includes MobileNet V2 as the backbone and the atrous spatial pyramid pooling (ASPP) module. We made this choice because DeepLabV3+ leverages atrous convolutions on the basis of Auto Encoder, enabling the model to capture multi-scale structures and spatial information effectively~\citep{chen2017deeplab,chen2018encoder}. It generates a multi-channel feature map from the image $I$, and we use mean-max pool~\citep{zhang2018learning} to derive a global vector $\mathbf{F}_I \in \mathbb{R}^{256}$ from the feature map.
For the Point Clouds Encoder $\mathcal{E}_P$, we use the encoder part of PointNet++~\citep{qi2017pointnet++}. 
For each part and subassembly, we extract a part-wise feature $\mathbf{F}_j \in \mathbb{R}^{256}$.
For the GNN $\mathcal{E}_G$, we use a three-layer graph transformer~\citep{Costa2021.06.02.446809}.
The pose regressor $\mathcal{R}$ is a three-layer MLP. We provide more details of the mean-max pool for the image feature and our training hyperparameter setting in~\Cref{appd:implementation_pose}.
% Appendix B.


\textbf{Baselines.}
We evaluate the performance of our method on our proposed per-step assembly pose estimation dataset. 
We compare our method with two baselines: 
\begin{itemize}
    \item \citet{li2020learning} proposed a pipeline for single image guided 3D object pose estimation. 
    \item Mean-Max Pool is a variant of our method, replacing GNN with a mean-max pool trick, similar to our approach of obtaining a one-dimensional vector from a multi-channel feature map, with details in~\Cref{appd:implementation_pose}.
    % Appendix B.

\end{itemize}

\textbf{Evaluation Metrics.}
We adopt comprehensive evaluation metrics to assess the performance of our method and baselines.
\begin{itemize}
    \item Geodesic Distance (GD), which measures the shortest path distance on the unit sphere between the predicted and ground-truth rotations.
    \item Root Mean Squared Error (RMSE), which measures the Euclidean distance between the predicted and ground-truth poses.
    \item Chamfer Distance (CD), which calculates the holistic distance between the predicted and the ground-truth point clouds.
    \item Part Accuracy (PA), which computes the Chamfer Distance between the predicted and the ground truth point clouds; if the distance is smaller than 0.01m, we count this part as ``correctly placed".
\end{itemize}

\textbf{Results.}
As shown in~\Cref{tab:pose_estimation}, our method outperforms ~\citet{li2020learning} and the mean-max pool variant in all evaluation metrics and on three furniture categories.
We attribute this to the effectiveness of our multi-modal feature fusion and GNN in capturing the spatial relationships between parts.
We also provide qualitative results for each furniture category in~\Cref{fig:pose_estimation_viz}.

\textbf{Ablation.}
To assess the impact of equivalent parts, guided image, and per-step data about subassemblies, we perform ablation studies on these components. We present the details and results in~\Cref{appd:pose_ablation}.
% Appendix C.

\begin{figure}[!ht]
    \centering
    \begin{minipage}{\linewidth}
    \includegraphics[width=\textwidth]{Figures/comparison.pdf}
    \caption{\textbf{Qualitative results on three furniture categories.} We observe better pose predictions than baselines.}
    \label{fig:pose_estimation_viz}
    \end{minipage}
\end{figure}


\begin{figure*}[htb!]
    \centering
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Figures/manual_esitimation_real_double_col.pdf}
        \caption{\textbf{Qualitative Evaluation on real IKEA furniture items.} This figure illustrates the assembly process of various IKEA furniture items, including FLISAT, VARIERA, SUNDVIK, and KNAGGLIG, with our approach. For each item, we display the manual images, per-step 3D parts pose estimation results, and real-world assembly outcomes.}
        \label{fig:real_assembly_process}
    \end{minipage}
\end{figure*}




\subsection{Overall Performance Evaluation}\label{sec:Q1_exp}

We evaluate the overall performance of our method by assembling furniture models in a simulation environment. We implement the evaluation process in the PyBullet~\citep{coumans2015bullet} simulation environment and test the entire pipeline. We source all test furniture models from the IKEA-Manuals dataset~\citep{wang2022ikea}. Given these manuals along with 3D parts, we generate the pre-assembly scene images as described in ~\ref{sec:method-action}, and our pipeline generates the hierarchical graphs. Then, we traverse the hierarchical graph to determine the assembly order. Following this sequence and the predicted 6D poses of each component, we implement RRT-Connect~\cite{kuffner2000rrt} in simulation to plan feasible motion paths for the 3D parts and subassemblies, ensuring they move towards their target poses. Note that, in this experiment, we focus on object-centric motion planning and omit robotic execution in our framework. 
% \lin{After each assembly step, how do we put the newly assembled components back into the ground?}


\textbf{Baselines.}
As the first to propose a comprehensive pipeline for furniture assembly, there is no direct baseline for comparison.
So we design a baseline method that uses previous work ~\citep{li2020learning} to estimate the poses of all parts, with the guidance of an image of the fully assembled furniture, and adopt a heuristic order to assemble all parts. Specifically, given the predicted poses of all parts, we can calculate the distance between each pair of parts. The heuristic order is defined as follows: starting from a random part, we find the nearest part to it and assemble it, then successively find the nearest part to the assembled parts until we assemble all parts.

\textbf{Evaluation Metrics.}
We adopt the assembly success rate as the evaluation metric and define the following situations as a failure:
1) A part is placed at a pose that is too far from the ground truth pose.
2) A part collides with other parts when moving to the estimated pose. In other words, the RRT-Connect algorithm~\cite{kuffner2000rrt} finds no feasible path when mating it with other parts.
3) We place a part that is not near any other components, causing it to suspend in midair after each assembly step.


\begin{table}[htb!]
    \centering
    \setlength\tabcolsep{4pt}
    \begin{threeparttable}
    \captionsetup{width=\linewidth}
    \caption{\textbf{Success Rate on 4 Furniture Categories$(\uparrow)$}}
    \label{tab: overall}
    \vspace{2mm} % Add space before the table
    \begin{tabular}{lccccc}
    \toprule
     Method & Bench & Chair & Table & Misc & Average \\ 
    \midrule
    \citet{li2020learning}+Heuristic & 0.00&0.39&0.11&0.00&0.30\\
    \textbf{Ours} & 
    \cellcolor{customblue}\textbf{0.67}&
    \cellcolor{customblue}\textbf{0.61}&
    \cellcolor{customblue}\textbf{0.44}&
    \cellcolor{customblue}\textbf{0.50}&
    \cellcolor{customblue}\textbf{0.58}\\
    \bottomrule
    \end{tabular}
    \vspace{-2mm} 
    \end{threeparttable}
\end{table}

\textbf{Results.} We evaluate the overall performance on 50 furniture items from the IKEA-Manual dataset~\citep{wang2022ikea}, each consisting of fewer than seven parts. These items fall into four categories (Bench, Chair, Table, Misc), and we report the success rate for each in~\Cref{tab: overall}.

Our system successfully assembles 29 out of 50 furniture pieces, whereas the baseline method assembles only 15. Our framework achieves a success rate of \textbf{58\%}, demonstrating the effectiveness of our proposed framework. The most common failure occurs when the VLM fails to generate a fully accurate assembly graph, leading to misalignment between the point cloud and the instruction manual images used for pose estimation.

\begin{figure}[thb!]
    \centering
    \resizebox{0.45\textwidth}{0.2\textheight}{
    \includegraphics{Figures/setup3.png}}
    \caption{\textbf{Real-World Setup.} We use two UFactory xArm6 for assembly and a RealSense D435 camera for pose estimation.}
    \label{fig:real_setup}
\end{figure}

\subsection{Real-world Assembly Experiments}~\label{real_world_exp}
To evaluate the feasibility and performance of our pipeline, we conducted experiments in the real world using four IKEA furniture items:  Flisat (Wooden Stool), Variera (Iron Shelf), Sundvik (Chair), and Knagglig (Box). \Cref{fig:real_setup} illustrates our real-world experiment setup. We show the manual images, per-step pose estimation results, and real-world assembly process in~\Cref{fig:real_assembly_process}. We also attach videos of the real-world assembly process in the supplementary material. For detailed implementation of our real-world experiments, please check~\Cref{appd: real-world}.
% Appendix G.
We evaluated all the assembly tasks with target poses provided by three different methods: Ground truth Pose, Mean-Max Pool~(see~\Cref{pose_exp}), and our proposed approach. 
The Ground truth Pose method uses the ground truth poses for each part to assemble the furniture.
We use the Average Completion Rate (ACR) as the evaluation criterion and calculate it as follows:
\begin{equation}
ACR = \frac{1}{N} \sum_{j=1}^{N} \frac{S_j}{S_{\text{total}}}
\end{equation}
where $N$ is the total number of trials, 
$S_j$ is the number of steps completed in trial $j$, and $S_{\text{total}}$ denotes the total number of steps in the task.

We perform each task over 10 trials with varying initial 3D part poses. We present the results in~\Cref{tab:real_assembly}, showing that our method outperforms the baseline and achieves a high success rate in real-world assembly tasks. 

These findings underscore the practicality and effectiveness of our approach for real-world implementation. The primary failure mode arises from planning limitations, particularly in handling complex obstacles. Failures occur when the RRT-Connect algorithm cannot find a feasible trajectory when the planned path results in collisions with the robotic arm or surrounding objects or due to suboptimal grasping poses. To improve robustness in real-world scenarios, we plan to develop a low-level policy for adaptive motion refinementsâ€”a topic we leave for future work.

\begin{table}[!ht]
    \footnotesize
    \centering
    \begin{threeparttable}
    \captionsetup{width=\linewidth}
    \caption{\textbf{Real World Success Rate ($\uparrow$) over 10 trials.}}
    \label{tab:real_assembly}
    \begin{tabular}{lccccc}
    \toprule
    {Method} & FLISAT  & VARIERA &SUNDVIK & KNAGGLIG\\ 
        \midrule
       Oracle Pose & 72.5 & 85.0 & 80.0 & 90.0 \\  
       Mean-Max Pool& 52.5 & 61.7 & 40.0& 70.0 \\ 
        Ours 
        & 60.0
        & 80.0
        & 68.0
        & 85.0 \\ 
        \bottomrule
        \end{tabular}
    
    \end{threeparttable}
    \end{table}


\subsection{Generalization to Other Assembly Tasks}
\label{gen_other_task}
\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/extra_task_generation2.pdf}
    \caption{\textbf{Pipeline Extension Beyond Furniture Assembly.}}
    \label{fig: asap}
\end{figure}
We design Manual2Skill as a generalizable framework capable of handling diverse assembly tasks with manual instructions. To assess its versatility, we evaluate the VLM-guided hierarchical graph generation method across three distinct assembly tasks, each varying in complexity and application domain. These include: \textbf{(1) Assembling a Toy Car Axle} (a low-complexity task with standardized components, representing consumer product assembly), \textbf{(2) Assembling an Aircraft Model} (a medium-complexity task, representing consumer product assembly), and \textbf{(3) Assembling a Robotic Arm} (a high-complexity task involving non-standardized components, representing research \& prototyping assembly).

For the toy car axle and aircraft model, we sourced 3D parts from \citep{tian2024asap} and reconstructed pre-assembly scene images using Blender. We manually crafted the manuals in their signature style, with each page depicting a single assembly step through abstract illustrations. For the robotic arm assembly, we used the Zortrax robotic arm \cite{zortrax_robotic_arm}, which includes pre-existing 3D parts and a structured manual. These inputs were then processed through the VLM-guided hierarchical graph generation pipeline (described in Sec.~\ref{assembly_tree_gen}), yielding assembly graphs as shown in \Cref{fig: asap}. This \textbf{zero-shot} generalization achieves a success rate of $100\%$ over five trials per task. The generated graphs align with ground-truth assembly sequences, confirming the generalization of our VLM-guided hierarchical graph generation across diverse manual-based assembly tasks and highlighting its potential for broader applications.
