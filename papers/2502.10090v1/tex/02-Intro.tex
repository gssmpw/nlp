\section{Introduction}
\label{sec:intro}
Humans can learn manipulation skills from instructions in images or texts; for example, people can assemble IKEA furniture or LEGO models by following a manual's instructions. 
This ability enables humans to efficiently acquire long-horizon manipulation skills from sketched instructions.
In contrast, robots typically learn such skills through imitation learning \cite{10602544} or reinforcement learning \cite{tang2024deep}, both of which require significantly more data and computation. 
Replicating the human ability to transfer abstract manuals to real-world actions remains a significant challenge for robots.
Manuals are typically designed for human understanding, using simple schematic diagrams and symbols to convey manipulation processes. This abstraction makes it difficult for robots to comprehend such instructions and derive actionable manipulation strategies ~\citep{liu2024ikea,wang2022ikea,wang2022translating}.
Developing a method for robots to effectively utilize human-designed manuals would greatly expand their capacity to tackle complex, long-horizon tasks while reducing the demand of collecting extensive demonstration data.

% Junting: move the first line in the 2nd paragraph to the next page
\clearpage

% As \cref{fig: teaser} illustrates, we can divide this learning process into three stages:
% \textbf{Task Decomposition}, \textbf{Step-Specific Inference}, and \textbf{Action Sequence Generation}.

Manuals inherently encode the structural information of complex tasks. They decompose high-level goals into mid-level subgoals and capture task flow and dependencies, such as sequential steps or parallelizable subtasks.
For example, furniture assembly manuals guide the preparation and combination of components and ensure that all steps follow the correct order ~\citep{liu2024ikea}.
Extracting this structure is crucial for robots to replicate human-like understanding and manage complex tasks effectively~\citep{jiang2024roboexp,mo2019structurenet}. 
After decomposing the task, robots need to infer the specific information for each step, such as the involved components and their spatial relationships.
For example, in cooking tasks, the instruction images and texts may involve selecting ingredients, tools, and utensils and arranging them in a specific order ~\citep{shi2023robocook}.
Finally, robots need to generate a sequence of actions to complete the task, such as grasping, placing, and connecting components. 
Previous works have tried to leverage sketched pictures~\citep{sundaresan2024rt} or trajectories~\citep{gu2023rt} to learn manipulation skills but are always limited to relatively simple tabletop tasks.

In this paper, we propose \ours, a novel robot learning framework that is capable of learning manipulation skills from visual instruction manuals.
This framework can be applied to automatically assemble IKEA furniture, a challenging and practical task that requires complex manipulation skills.
As illustrated in~\Cref{fig: teaser}, given a set of manual images and the real furniture parts, we first leverage a vision language model to understand the manual and extract the assembly structure, represented as a hierarchical graph.
Then, we train a model to estimate the assembly poses of all involved components in each step.
Finally, a motion planning module generates action sequences to move selected components to target poses and executes them on robots to assemble the furniture.

In summary, our main contributions are as follows:
\begin{itemize}
    \item We propose \ours, a novel framework that leverages a VLM to learn complex robotic skills from manuals, enabling a generalizable assembly pipeline for IKEA furniture.
    \item We introduce a hierarchical graph generation pipeline that utilizes a VLM to extract structured information for assembly tasks. Our pipeline facilitates real-world assembly and extends to other assembly applications.
    \item We define a novel assembly pose estimation task within the learning-from-manual framework. We predict the 6D poses of all involved components at each assembly step to meet real-world assembly requirements.
    \item We perform extensive experiments to validate the effectiveness of our proposed system and modules.
    \item We evaluate our method on four real items of IKEA furniture, demonstrating its effectiveness and applicability in real-world assembly tasks.
\end{itemize}


% A promising approach to extract structured information from instruction images is to leverage Vision Language Models(VLMs)~\citep{yin2023survey}.
% VLM has shown promising results in understanding and generating visual and textual information~\citep{ramesh2021zero,brown2020language,ouyang2022training}.
% Previous works leverage VLM to guide robot manipulation tasks, such as finding functional key points {liu2024moka}, generating spatial constraints~\citep{huang2024copa,huang2024rekep}, and designing assembly plans~\citep{goldberg2024blox}.
% Inspired by these advancements, this work explores the potential of using VLM to understand abstract instruction images and extract structured information.

% In this paper, we propose \ours, a novel robot learning framework that learns manipulation skills from abstract instruction images and builds an automatic assembly pipeline for IKEA furniture.