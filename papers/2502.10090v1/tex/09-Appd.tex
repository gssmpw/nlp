\newpage
\appendix
\lstset{
    language=Python,
    basicstyle=\ttfamily\small, % Monospace font, smaller size
    keywordstyle=\bfseries\color{blue}, % Keywords in bold blue
    commentstyle=\itshape\color{green!50!black}, % Comments in italic green
    stringstyle=\color{red}, % Strings in red
    showstringspaces=false, % Don't show spaces in strings
    frame=single, % Frame around the code
    numbers=left, % Line numbers on the left
    numberstyle=\tiny\color{gray}, % Line numbers in gray
    breaklines=true, % Allow line breaks
    tabsize=4, % Set tab size
}

% \subsection{Implementation Details of Overall Performance Evaluation}
% \label{appd:overall_performance}

\subsection{Per-step Assembly Pose Estimation Dataset}
\label{appd: Per-step Assembly Pose Estimation Dataset}
We build a dataset for our proposed manual guided per-step assembly pose estimation task.
Each data piece is a tuple $(I_i, \{P\}_j, \{T\}_j, \mathbf{R}_i)$, where $I_i$ is the manual image, $\{P\}_j$ is the point clouds of all the components involved in the assembly step, $\{T\}_j$ is the target poses for each component, and $\mathbf{R}_i$ is the spatial and geometric relationship between components.

\begin{figure}[!ht]
    \centering
    \resizebox{0.4\textwidth}{0.22\textheight}{
    \includegraphics{Figures/dataset_v3.pdf}}
    \caption{Manual images of our proposed dataset. There are variations in furniture shapes, subassemblies, and camera views.}
    \label{fig: dataset}
    \end{figure}

Instruction manuals in the real world come in a wide variety. To cover as many scenarios as we might encounter in real-life situations, we considered three possible variations of instruction manuals when constructing the dataset, as shown in~\Cref{fig: dataset}.
Our dataset encompasses a variety of furniture shapes. 
For each piece of furniture, we randomly selected some connected parts to form different subassemblies. 
Meanwhile, for each subassembly, there are multiple possible camera perspectives for taking manual photos. 
This definition enables our dataset to cover various manuals that we might encounter in real-world scenarios.

Formally, for furniture consisting of $M$ parts, we randomly select $m$ connected parts to form a subassembly.
Denoted as $P_{\text{sub}} = \{P_1, P_2, \cdots, P_m\}$, here each $P_i$ is a atomic part.
Then, we randomly group the $m$ atomic parts into $n$ components while keeping all parts within the same group are connected, denoted as $P_{\text{sub}} = \{\{P_{11},\cdots P_{1\alpha_1}\},\cdots\{P_{n1},\cdots P_{n\alpha_n}\}\}$, where each $\alpha_i$ represents the number of atomic parts in $i$-th component, and thus $\sum_{i}\alpha_i=m$.
We sample the point cloud for each component to consist of the point cloud of the data piece.
We can also take photos of the subassembly from different perspectives.

% Additionally, we leverage the spatial and geometric relationships between components to boost the pose estimation.
% In order to predict more precise poses, we provide the junction points for each component as an additional input~\citep{li2024category}.
% Specifically, for each point cloud $P_i\in \mathbb{R}^{N\times 3}$, we annotated the junction points on it, highlighting the points that connect with other parts.
% It is represented as a mask $\mathcal{M}\in \mathbb{R}^{N}$ on the pointcloud, where $\mathcal{M}_i=k,k\ge 0$ means the point belongs to the $k$-th junction of current sub-assembly, and $\mathcal{M}_i=-1$ otherwise.
% Moreover, in furniture assembly, we often deal with equivalent parts—such as the four legs of chairs or desks, which typically share the same shape—and we assemble them in different locations.
% \yiwei{interact with Equivalent Parts in Loss Function, taking account of permutations.}
We also provide annotations for equivalent parts in the auxiliary information.
In this paper, we propose new techniques to leverage the auxiliary information for each assembly step, which significantly enhances the precision and robustness of our pose estimation model.


\subsection{Pose Estimation Implementation}
\label{appd:implementation_pose}
\subsubsection{Loss Functions for Pose Estimation}  \(\) \\
\vspace{-1em}

\textbf{Rotation Geodesic Loss:}  
In 3D pose prediction tasks, we commonly use the rotation geodesic loss to measure the distance between two rotations ~\citep{wu2023leveraging}. Formally, given the ground truth rotation matrix $R \in SO(3)$ and the predicted rotation $\hat{R} \in SO(3)$, the rotation geodesic loss is defined as:
\begin{equation}
    \mathcal{L}_{\text{rot}} = \arccos\left(\frac{\text{tr}(R^T\hat{R}) - 1}{2}\right)
\end{equation}
where $\text{tr}(\cdot)$ denotes the trace of a matrix and $R^T$ is the transpose of $R$.

\textbf{Translation MSE Loss:}  
Following~\citep{li2020learning}, we use the mean squared error (MSE) loss to measure the distance between the ground truth translation $t$ and the predicted translation $\hat{t}$:
\begin{equation}
    \mathcal{L}_{\text{trans}} = ||t - \hat{t}||_2
\end{equation}

\textbf{Chamfer Distance Loss:}  
This loss function minimizes the holistic distance between each point in the predicted and ground truth point clouds. Given the ground truth point cloud $S_1 = R P + t$ and the predicted point cloud $S_2 = \hat{R} P + \hat{t}$, it is defined as:
\begin{equation}
    \mathcal{L}_{\text{cham}} = \frac{1}{|S_1|}\sum_{x \in S_1} \min_{y \in S_2} ||x - y||_{2}^{2} + \frac{1}{|S_2|}\sum_{x \in S_2} \min_{y \in S_1} ||y - x||_{2}^{2}
\end{equation}
where $S_1$ is the point cloud after applying the ground truth 6D pose transformation, and $S_2$ is the point cloud after applying the predicted 6D pose transformation.

\textbf{Pointcloud MSE Loss:}  
We supervise the predicted rotation by applying it to the point of the component and use the MSE loss to measure the distance between the rotated point and the ground truth point:
\begin{equation}
    \mathcal{L}_{\text{pc}} = ||R P - \hat{R} P||_2
\end{equation}

% \textbf{Junction Points Matching Loss:}  
% To leverage junction point annotations, we use a matching loss to ensure that the predicted poses are consistent with the junction points. Formally, we apply the predicted transformation to the point cloud, denoted as $\{\hat{P}\}_j$, and for each junction, we select the corresponding junction points in the transformed point cloud. Let $\hat{J}_{ij}$ denote the points belonging to the $i$-th junction of the $j$-th component. We define the matching loss as:
% \begin{equation}
%     \mathcal{L}_{\text{match}} = \sum_{i}\sum_{j_1}\sum_{j_2 > j_1} \text{CD}(\hat{J}_{ij_1}, \hat{J}_{ij_2})
% \end{equation}
% where CD represents the Chamfer distance between junction points.

\textbf{Equivalent Parts:}  
Given a set of components, we might encounter geometrically equivalent parts that we must assemble in different locations. Inspired by ~\citep{zhang2024manual}, we group these geometrically equivalent components and add an extra loss term to ensure we assemble them in different locations. For each group of equivalent components, we apply the predicted transformation to the point cloud of each component and then compute the Chamfer distance (CD) between the transformed point clouds. For all pairs $(j_1, j_2)$ within the same group, we compute the Chamfer distance between the transformed point clouds $\hat{P}_{j_1}$ and $\hat{P}_{j_2}$, encouraging the distance to be large:
\begin{equation}
\label{eq:loss_equiv}
    \mathcal{L}_{\text{equiv}} = -\sum_{\text{group}}\sum_{(j_1, j_2)} \text{CD}(\hat{P}_{j_1}, \hat{P}_{j_2})
\end{equation}

Finally, we define the overall loss function as a weighted sum of the above loss terms:
% \begin{equation}
% \scalebox{0.8}{$
%     \mathcal{L}_{\text{total}} = \lambda_{1}\mathcal{L}_{\text{rot}} + \lambda_{2}\mathcal{L}_{\text{trans}} + \lambda_{3}\mathcal{L}_{\text{cham}} + \lambda_{4}\mathcal{L}_{\text{pc}} +\lambda_{5}\mathcal{L}_{\text{match}} + \lambda_{6}\mathcal{L}_{\text{equiv}}
% $}
% \end{equation}
% Where $\lambda_{1}=1$, $\lambda_{2}=1$, $\lambda_{3}=1$, $\lambda_{4}=20$, $\lambda_{5}=1$, $\lambda_{6}=0.1$. 
\begin{equation}
    \mathcal{L}_{\text{total}} = \lambda_{1}\mathcal{L}_{\text{rot}} + \lambda_{2}\mathcal{L}_{\text{trans}} + \lambda_{3}\mathcal{L}_{\text{cham}} + \lambda_{4}\mathcal{L}_{\text{pc}} + \lambda_{5}\mathcal{L}_{\text{equiv}}
\end{equation}
where $\lambda_{1}=1$, $\lambda_{2}=1$, $\lambda_{3}=1$, $\lambda_{4}=20$,  $\lambda_{5}=0.1$. 
\vspace{1em}

\subsubsection{Mean-Max Pool}
\label{appd:implementation_mean_max_pool}
The core mechanic of the mean-max pool is to obtain the mean and maximum values along one dimension $\mathbb{R}^{C}$ of a set of vectors or matrices with the same dimensions and concatenate them into a one-dimensional vector in $\mathbb{R}^{2C}$ to obtain a global feature. For one-dimensional vectors, we take the mean and maximum values along the sequence length dimension. For two-dimensional matrices, we take the mean and maximum values along the height \(\times\) width dimensions:
\begin{equation}
    \textbf{F}_{global} = [\textbf{avg};\textbf{max}] \in \mathbb{R}^{2F}
\label{eq:mean-max pool}
\end{equation}
In the setting of our work, we set $F$ to 128.

We use this trick twice in this work. One instance is when we obtain a one-dimensional vector with a channel dimension from a multi-channel feature map, thus obtaining a one-dimensional feature vector for the image. In this case, we can express the mean-max pool as follows:
\begin{equation}
    \left\{
    \begin{aligned}
    & \textbf{X}=(\textbf{X}_{c,h,w})^{C,H,W}_{c=1,h=1,w=1}\\
    & \textbf{avg} = (\frac{1}{HW} \sum_{\textit{h}=1}^{\textit{H}} \sum_{\textit{w}=1}^{\textit{W}} \textbf{X}_{c,h,w})^{C}_{c=1} \in \mathbb{R}^{C} \\
    & \textbf{max} = (\max_{h,w} \textbf{X}_{c,h,w})^{C}_{c=1} \in \mathbb{R}^{C} \\ 
    \end{aligned}
    \right.
\end{equation}
Where $\mathbf{X}$ is the multi-channel feature map of image $I_i$ with dimensions $\text{channels} (C) \times \text{height} (H) \times \text{width} (W)$, $\mathbf{avg}$ and $\mathbf{max}$ denote one-dimensional vectors of length $\text{channels}$. Thus, $\textbf{F}_{global}$ of the multi-channel feature map is a C-dimensional vector.

The other instance is when we compare the baseline. To aggregate point cloud features on a per-part basis and obtain a one-dimensional global feature for the shape, we express the mean-max pool in the following form:
\begin{equation}
    \left\{
    \begin{aligned}
    & \textbf{avg} = \frac{1}{M}\sum_{j=1}^{M} \textbf{F}_{j} \in \mathbb{R}^{F} \\
    & \textbf{max} = \max_{F} \{\textbf{F}_{j}\} \in \mathbb{R}^{F}
    \end{aligned}
    \right.
\end{equation}
Here, we let \(M\) denote the number of parts in a shape. For each part in this baseline, we concatenate the one-dimensional image feature \(\textbf{F}_{I}\), the global point cloud feature \(\textbf{F}_{global}\) (both obtained by mean-max pool), and the part-wise point cloud feature \(\textbf{F}_{j}\) to form a one-dimensional cross-modality feature. We then use this feature as input for the pose regressor MLP.

\subsubsection{Hyperparameters in Training of Pose Estimation}
We train our pose estimation model on a single NVIDIA A100 40GB GPU with a batch size of 32. Each experiment runs for 800 epochs (approximately 46 hours). We set the learning rate to \(1e-5\) and employ a 10-epoch linear warm-up phase. Afterward, we use a cosine annealing schedule to decay the learning rate. We also set the weight decay to \(1e-7\). The optimizer configuration for each component of the model is as shown in~\Cref{tab: pose estimation optimizer}.
\begin{table}[!ht]
    \caption{Optimizer Corresponding to Each Component}
    \label{tab: pose estimation optimizer}
    \centering
    \begin{threeparttable}
    \begin{tabular}{lc}
    \toprule
    {Component} & Optimizer \\ \midrule
       Image Encoder &  RMSprop\\  
       Pointcloud Encoder&  AdamW\\ 
       GNN &  AdamW\\  
       Pose Regressor&  RMSprop\\ 
        \bottomrule
        \end{tabular}
    \end{threeparttable}
    \end{table}

\subsection{Pose Estimation Ablation Studies}
\label{appd:pose_ablation}
To evaluate the effectiveness of each component in our pipeline, we conduct an ablation study on the chair category. We show the quantitative results in~\Cref{tab:pose_estimation_ablation} and the qualitative results in~\Cref{fig:pose_estimation_ablation}.
First, we remove the image input and only use the point cloud input to predict the pose.
The performance drops significantly, indicating that the image input is crucial for pose estimation.
Second, we remove the permutation mechanism for equivalent parts(\Cref{eq:loss_equiv}).
As shown in the visualizations, the model fails to distinguish between equivalent parts, placing two legs in similar positions.
% Finally, we remove the junction mask, which we use to guide the model in focusing on the junction points.
% The JCD metric worsens, indicating that the model struggles to align the junction points, which is undesirable in real-world assembly tasks, as the precise alignment of junction points is crucial.
% \begin{table}[H]
%     % \scriptsize
%     \setlength{\tabcolsep}{4pt}
%     \centering
%     \begin{threeparttable}
%     \caption{Pose Estimation Ablation Results}
%     \label{tab:pose_estimation_ablation}
%     \begin{tabular}{lcccccc}
%     \toprule 
%     {Method} & GD$\downarrow$  & RMSE$\downarrow$ &CD$\downarrow$ & PA$\uparrow$ &JCD$\downarrow$\\ 
%         \midrule
%         w/o Image&1.829  &0.235  &0.228  &0.328 &0.714\\
%         w/o Permutations&0.309  &0.061  &0.029  &0.662 &0.071\\
%         w/o Junction Mask &\cellcolor{customblue}\textbf{0.201}  
%         &\cellcolor{customblue}\textbf{0.042}  
%         &0.027  
%         &\cellcolor{customblue}\textbf{0.869} 
%         &0.065 \\

%         \textbf{Ours} &0.205  
%         &\cellcolor{customblue}\textbf{0.042}  
%         &\cellcolor{customblue}\textbf{0.019} 
%         &0.828  
%         &\cellcolor{customblue}\textbf{0.050}\\ 
        
%         \bottomrule
%         \end{tabular}
    
%     \end{threeparttable}
% \end{table}

\begin{table}[htb!]
    % \scriptsize
    \setlength{\tabcolsep}{4pt}
    \centering
    \begin{threeparttable}
    \caption{\textbf{Pose Estimation Ablations.}}
    \label{tab:pose_estimation_ablation}
    \begin{tabular}{lccccc}
    \toprule 
    {Method} & GD$\downarrow$  & RMSE$\downarrow$ &CD$\downarrow$ & PA$\uparrow$ \\ 
        \midrule
        w/o Image&1.797  &0.234  &0.227  &0.138 \\
        w/o Permutations&0.252  &0.051  &0.029  &0.783 \\

        \textbf{Ours}
        &\cellcolor{customblue}\textbf{0.202}  
        &\cellcolor{customblue}\textbf{0.042}  
        &\cellcolor{customblue}\textbf{0.027} 
        &\cellcolor{customblue}\textbf{0.868} \\ 
        
        \bottomrule
        \end{tabular}
    
    \end{threeparttable}
\end{table}

\begin{figure}[htb!]
    \centering
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ablation.pdf}
        \caption{\textbf{Qualitative Results of Ablations.} We observe salient performance drops in ablated settings. }
        \label{fig:pose_estimation_ablation}
    \end{minipage}
\end{figure}

Previous works usually train and predict only fully assembled shapes. In contrast, our pose estimation dataset includes per-step data ($i.e.$, subassemblies). We conduct an ablation study comparing two settings:
\begin{itemize}
    \item \textit{w/o Per-step}: Training and testing on a dataset of fully assembled shapes.
    \item \textit{Per-step}: Training on a dataset with per-step data and testing on fully assembled shapes.
\end{itemize}

\begin{table}[!ht]
    % \scriptsize
    \setlength{\tabcolsep}{7pt}
    \centering
    \begin{threeparttable}
    \caption{\textbf{w/o Per-step vs. Per-step}}
    \label{tab:one_step}
    \begin{tabular}{lccccc}
    \toprule 
    {Method} & GD$\downarrow$  & RMSE$\downarrow$ &CD$\downarrow$ & PA$\uparrow$\\ 
        \midrule
        w/o Per-step &0.233  &0.046  &0.015 &0.753  \\
        \cellcolor{customblue}Per-step\textbf{ (Ours)}
        &\cellcolor{customblue}\textbf{0.064}  
        &\cellcolor{customblue}\textbf{0.016}  
        &\cellcolor{customblue}\textbf{0.004} 
        &\cellcolor{customblue}\textbf{0.983}\\ 
        \bottomrule
        \end{tabular}
    
    \end{threeparttable}
    
\end{table}

As shown in~\Cref{tab:one_step}, adding per-step data improves assembly prediction accuracy, demonstrating that per-step inference enhances robot assembly performance.

\subsection{Complete VLM Plan Generation Results}
\label{vlm_plan_complete}
We provide the complete analysis for VLM plan generation. 
% As detailed in \Cref{assembly_tree_gen}, due to minor variations in GPT-4o's outputs, we select the highest-scoring result from seven samples for all tables. 
In addition to the results for all 50 furniture items with six or fewer parts, shown in the main paper, we include results for all 52 furniture items with seven or more parts (denoted as $\leq$ 7 Parts) and the complete dataset of 102 furniture items spanning all part counts (denoted as All Parts) in \Cref{tab:full_asssembly_plan}. Furthermore, we categorized the full set of 102 furniture items in greater detail, with Hard Matching results for individual part counts ranging from 2 to 16 parts, as shown in \Cref{tab:parts_performance}. For detailed descriptions of Simple Matching and Hard Matching, we refer
readers to \cite{wang2022ikea}.
% We only consider Hard Matching because it more accurately measures the tree's structure and is thus more relevant to our task of generating assembly trees

For the GeoCluster baseline, we could not replicate the exact results shown in the IKEA-Manuals dataset \cite{wang2022ikea}. Thus, we used the scores from our experiments for the $\leq$ 6 Parts and $\geq$ 7 Parts categories while retaining the original scores from the dataset \cite{wang2022ikea} for the All Parts category.

To obtain our scores, we repeatedly ran the experiment 5 times using the same input and a temperature of 0. We repeated sampling to account for slight variations in GPT-4o's \cite{achiam2023gpt} outputs, even when we set the temperature to 0, and to capture the range of possible outcomes. This approach provides a better estimate of the model's true performance. When taking the maximum between precision, recall, and F1, the average score for $\leq$ 6 parts on Hard Matching is 63.7\%, the worst score is 57.2\%, and the best score is 69.0\%. Since the average and best scores are similar, we choose to report the best score in all of our tables related to Assembly Plan Generation.

To compare the trees generated by GPT-4o \cite{achiam2023gpt} with the ground truth trees in the dataset, we accounted for equivalence relationships among parts, which can result in multiple valid ground truth trees. For instance, if parts 1 and 2 are equivalent and [[1, 3], 2] is a valid tree, then so is [[2, 3], 1]. Since the dataset does not account for this isomorphism of trees, we manually defined all equivalent parts for each of the 102 furniture items. We then permuted the predicted tree using the equivalent parts, comparing each permutation to the ground truth and selecting the highest score. For furniture with 13 or more parts (6 items), we performed manual verification due to the computational cost of permutations. Overall, by employing this permutation method to evaluate predicted trees, we managed to increase our scores overall metrics by around 5\%. To ensure fairness, we also applied this permutation over the two baselines but saw no effects.

As shown in \Cref{tab:full_asssembly_plan}, tasks with \(\geq 7\) parts experience a significant drop in performance—Hard Matching achieves a maximum of 13.36\%, compared to 69.0\% for tasks with \(\leq 6\) parts—indicating that the model's performance declines as the number of parts increases. This decrease is likely driven by increased task complexity and occlusion in manual drawings as the number of furniture parts grows, causing GPT-4o \cite{achiam2023gpt} to misinterpret out-of-distribution images and fail in the plan generation stage. As noted in \cite{wang2022ikea}, SingleStep always outputs the root node and selects all other nodes as its children, achieving perfect precision in Simple Matching for all cases. Beyond this, our GPT-4o-based method outperforms both baselines across all categories in \Cref{tab:full_asssembly_plan}, which highlights the effectiveness of VLMs in interpreting manuals and designing reliable hierarchical assembly graphs.

Similarly, in \Cref{tab:parts_performance}, our method has a significant advantage over the two baselines in all numbers of parts. Mask Seg is an additional method we evaluated, which overlays segmentation masks from the IKEA-Manuals dataset \cite{wang2022ikea} onto manual pages (prompt 3.a~\Cref{appd: additional_prompts}), improving part identification, image clarity, and comprehension of assembly steps. Although Mask Seg slightly outperforms the original version without mask segmentations, we chose the latter for all reported tables. Otherwise, such masks are costly in real-world scenarios. Overall, the trend observed in \Cref{tab:full_asssembly_plan} persists here, with higher scores for furniture with fewer parts and lower scores as the number of parts increases.

% Table consists of only Hard Matching, which more accurately measures the structure of a tree.

% DONE FOR NOW: \shengxiang{TODO: 1. identify a small discrepancy between reported GeoCluster and our GeoCluster results. 2. Discuss the permutations approach; 3. Discuss significant accuracy drops for 7 or more furniture parts }

\begin{table*}[ht]

\centering
\begin{threeparttable}
\caption{VLM Assembly Plan Generation Results}
\label{tab:full_asssembly_plan}
% \setlength\tabcolsep{4pt} % Increased space between columns
\renewcommand{\arraystretch}{1.2} % Increased vertical space between rows
% \captionsetup[table]{skip=5pt}
% \scriptsize % Small font size
\begin{tabular}{@{}l>{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm} >{\centering\arraybackslash}p{0.8cm}@{}}
\toprule

& \multicolumn{3}{c}{Simple Matching (All Parts)} & \multicolumn{3}{c}{Hard Matching (All Parts)} & \multicolumn{3}{c}{Simple Matching ($\ge$ 7 Parts)} & \multicolumn{3}{c}{Hard Matching ($\ge$ 7 Parts)} \\

% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}

% & \multicolumn{2}{>{\centering\arraybackslash}p{1.5cm}}{T} & \multicolumn{2}{>{\centering\arraybackslash}p{1.5cm}}{NP} &
% \multicolumn{2}{>{\centering\arraybackslash}p{1.5cm}}{T} & \multicolumn{2}{>{\centering\arraybackslash}p{1.5cm}}{NP} &
% \multicolumn{2}{>{\centering\arraybackslash}p{1.5cm}}{T} & \multicolumn{2}{>{\centering\arraybackslash}p{1.5cm}}{NP} \\

\cmidrule(lr){2-4}  \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}

Method & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1\\

\midrule
SingleStep & 100.00 & 35.77 & 48.64 & 10.78 & 10.78 & 10.78 & 100.00 &21.96 &35.09 &0.00 &0.00 &0.00\\
GeoCluster & 44.90 & 48.46 & 43.53 & 16.54 & 16.50 & 16.30 &31.99 &28.88 &29.66 &7.31 &6.91 &6.92\\
\textbf{Ours}   & 
\cellcolor{customblue} \textbf{58.11} & 
\cellcolor{customblue} \textbf{55.98} & 
\cellcolor{customblue} \textbf{56.84} & 

\cellcolor{customblue} \textbf{40.63} & 
\cellcolor{customblue} \textbf{39.94} & 
\cellcolor{customblue} \textbf{40.22} & 

\cellcolor{customblue} \textbf{33.72} &  
\cellcolor{customblue} \textbf{31.95} & 
\cellcolor{customblue} \textbf{32.65} &

\cellcolor{customblue} \textbf{13.36} &  
\cellcolor{customblue} \textbf{12.96} & 
\cellcolor{customblue} \textbf{13.11} \\ 



\bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}

\begin{table*}[ht]
    \centering
    \setlength\tabcolsep{1pt} % Adjust column spacing
    \renewcommand{\arraystretch}{1.2} % Adjust row spacing
    \begin{threeparttable}
    \caption{Performance Across Different Numbers of Parts}
    \label{tab:parts_performance}
    \begin{tabular}{@{}l>{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1cm}@{}}
    \toprule
    \textbf{Number of Parts} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{11} & \textbf{12} & \textbf{13} & \textbf{14} & \textbf{15} & \textbf{16} \\ 
    \midrule
    SingleStep & 100 & 50 & 12.50 & 31.58 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
    GeoCluster & 100 & 25 & 10.42 & 14.04 & 21.76 & 14.40 & 6.99 & 15.00 & 4.17 & 2.22 & 0 & 16.67 & 0 & 0 & 0 \\ 
    Ours (Mask Seg) & 100 & 100 & 75.00 & 72.81 & 56.08 & 29.64 & 24.17 & 19.05 & 16.67 & 9.63 & 3.33 & 37.50 & 0.00 & 0.00 & 0.00 \\   
    Ours & \bluebold{100} & \bluebold{100} & \bluebold{72.92} & \bluebold{78.51} & \bluebold{45.59} & \bluebold{25.24} & \bluebold{13.05} & \bluebold{16.67} & \bluebold{27.78} & \bluebold{0} & \bluebold{9.33} & \bluebold{6.25} & \bluebold{0.00} & \bluebold{0.00} & \bluebold{0.00}\\ 
    \midrule
    Furniture Count & 2 & 4 & 8 & 19 & 17 & 14 & 10 & 3 & 4 & 9 & 5 & 2 & 1 & 2 & 1\\ 
    \bottomrule
    \end{tabular}
    \end{threeparttable}
\end{table*}

\subsection{Assembly Graph Generation Ablation Studies}
\label{appd:VLM_ablation}
We present the effectiveness of our VLM plan generation pipeline, emphasizing the critical role of cropped manual pages as input. The manual pages' visuals, detailing parts and subassemblies for each step, directly influence GPT-4o's output. Thus, we prioritize this content and ablate the strategy of inputting cropped pages. For furniture requiring $N$ assembly steps, instead of providing $N$ cropped manual pages corresponding to each step, we input the entire manual consisting of $M \geq N$ pages. As shown in \Cref{tab:assembly_plan_ablation}, this "no-crop" method leads to ~7\% accuracy drops in the Simple Matching category and ~25\% in the more important Hard Matching category. The decrease is likely due to irrelevant details in full manual pages, such as the nails, people, and speech bubbles in prompt 2.a), which divert GPT-4o's focus from the critical furniture parts for each step. Overall, \Cref{tab:assembly_plan_ablation} underscores the importance of cropping manual pages to simplify the input and direct GPT-4o's attention to the most relevant details.

\begin{table}[!ht]
    \footnotesize
    \centering
    \setlength\tabcolsep{3pt}
    \begin{threeparttable}
    \caption{Assembly Plan Generation Ablation Results on Furniture with $\le$ 6 Parts}
    \label{tab:assembly_plan_ablation}
    \begin{tabular}{lccc|ccc}
    \toprule
    & \multicolumn{3}{c|}{Simple Matching} & \multicolumn{3}{c}{Hard Matching} \\ 
    \cmidrule(lr){2-4} \cmidrule(lr){5-7}
    {Method} & Precision & Recall & F1 Score & Precision & Recall & F1 Score \\ 
    \midrule 
       Ours (no crop) & 69.13 & 81.13 & 73.05 & 42.37 & 45.50 & 43.45 \\ 
       \textbf{Ours} & \cellcolor{customblue}\textbf{83.47} 
       & \cellcolor{customblue}\textbf{80.97} 
       & \cellcolor{customblue}\textbf{81.99} 
       & \cellcolor{customblue}\textbf{69.00} 
       & \cellcolor{customblue}\textbf{68.00} 
       & \cellcolor{customblue}\textbf{68.41} \\ 
    \bottomrule
    \end{tabular}
    \end{threeparttable}
\end{table}

\subsection{Failure Cases Analysis}
We highlight failure cases of VLMs using GPT-4o in \Cref{fig:failure2} for plan generation of complex furniture.
\Cref{fig:failure2} demonstrates that while GPT-4o surpasses previous baselines in assembly planning, it struggles with complex structures, often producing entirely incorrect results. 
% \Cref{fig:failure1} reveals GPT-4o's inability to handle even simple junction matching tasks, suggesting the need to fine-tune another VLM for this low-level task.
% \begin{figure*}[ht]
% \centering
% \includegraphics[page=1, width=\textwidth]{Figures/Failure_plan_keypoints.pdf} % Adjust width to 0.5\textwidth
% \caption{The input consists of the scene image, the corresponding assembly step from the manual, and a simple instruction. Clearly, GPT-4o's response is wrong.}
% \label{fig:failure1}
% \end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[page=2, width=\textwidth]{Figures/Failure_plan_keypoints.pdf} % Adjust width to 0.5\textwidth
\caption{The input consists of the scene image, the corresponding assembly steps from the manual, and the text instruction from prompt 3.b). Clearly, GPT-4o's response is wrong and unreliable.}
\label{fig:failure2}
\end{figure*}

% \subsection{Junction Point Pairing Dataset Creation}
% \label{appd:junction}
% We provide details of preparing a dataset to train a VLM for junction point matching. Existing datasets for IKEA manuals \cite{wang2022ikea, liu2024ikea} include manual pages, tree-structured assembly plans, video frames, segmentation masks, 3D models, and 6D poses for individual parts. However, they lack step-specific scene images that depict all parts and subassemblies for a specific step, arranged without occlusion on the floor, mimicking a pre-assembly layout. To address this, we created a new dataset featuring such scene images, with junction points marked by red circles and labels. Using 3D models and poses from \cite{wang2022ikea} and Blender, we generated these images and annotated them with the ground truth connectivity between junction points, as detailed in \Cref{fig:data_gen}.

% We begin by identifying the 3D locations of junction points on each furniture part. Using the dataset, we convert 3D models of parts into point clouds with assembled poses, detect intersecting volumes using KDTrees, divide multiple connection points using K-Means, and record these as junction points with local coordinates relative to their parent parts. These volume intersections identify ground-truth junction point matches.
 
% For each assembly step, we identify the relevant parts and subassemblies, load them into Blender as wood-textured meshes, and arrange them in a grid to avoid occlusion. We calculate bounding boxes and rotate the meshes to ensure the largest face is camera-facing, simulating a real-life assembly scene. Then, we render this scene as an image and annotate the junction points with a red circle and a white number for each part and subassembly. The locations of these points are determined by projecting the recorded 3D locations to the 2D image using Blender's camera parameters. In the current step, we filter out junction points on a subassembly that we connected in previous steps. This annotated image, paired with the corresponding manual page, is the input, and the junction point matches are the outputs.

% This process is repeated for all assembly steps across all furniture. To augment the dataset, we render scenes from different angles, generating over 10,000 images for fine-tuning.

% \begin{figure*}[ht]
% \centering
% \includegraphics[width=\textwidth]{Figures/data_gen.pdf} % Adjust width to 0.5\textwidth
% \caption{The pipeline for automatically generating training data for VLM junction point matching begins with the point clouds of parts in their assembled poses to identify 3D junction point locations. For each step, we render a scene with parts and subassemblies spread out and annotate it by projecting 3D junction points onto the 2D scene. We use this annotated scene as a dataset instance. Note that we omit junction points 0, 1, 7, 8, 4, and 6 in the Step 2 subassembly because we connected them in Step 1.}
% \label{fig:data_gen}
% \end{figure*}


% \shengxiang{describe training process}
% \label{appd: data}

% We then use the MS-Swift\cite{zhao2024swiftascalablelightweightinfrastructure} to fine-tune Qwen2-VL-7B\cite{qwen} with LoRa\cite{hu2022lora}. We treat the key points as unordered pairs and calculate the amount of true positive, false positive, and false negative so that we can use the metrics precision, recall, and F1 score to compare our results with those of Qwen2-VL-7B and GPT-4O. Furthermore, we manually correct the evaluation of key points matching that are equivalent due to symmetry. Additionally, we conducted five inference attempts for each model, deeming the inference successful if it was entirely correct in at least one attempt, thereby obtaining the metric success rate. We show the results in~\cref{tab:key_points}.

% \begin{table}[!ht]
%     \footnotesize
%     \centering
%     \begin{threeparttable}
%     \caption{Key Points Matching Results}
%     \label{tab:key_points}
%     \begin{tabular}{lcccc}
%     \toprule
%     {Method} & Precision  & Recall & F1 Score & Sucess Rate\\ 
%         \midrule
%         Qwen2-VL-7B & 0.00\% & 0.00\% & 0.00\% & 0.00\%\\  
%         gpt-4o & 36.84\% & 37.50\% & 37.49\% & 38.10\%\\ 
%         Ours 
%         & \cellcolor{customblue}\textbf{44.64\%}
%         & \cellcolor{customblue}\textbf{44.64\%} 
%         & \cellcolor{customblue}\textbf{44.64\%}
%         & \cellcolor{customblue}\textbf{33.33\%}\\ 
    
%         \bottomrule
%         \end{tabular}
    
%     \end{threeparttable}
% \end{table}

\subsection{Real-World Experiment Details}
\label{appd: real-world}

This section provides the details of the real-world experiment.

\subsubsection{Pose Estimation in the Real World}

We utilize FoundationPose~\cite{wen2024foundationpose} to evaluate the 6D pose and point cloud of components in the real-world scene. First, a mobile app, ARCode, is used to scan the mesh of all atomic parts of the furniture. During each step of the assembly process, the mesh—along with the RGB and depth images and an object mask—is input into the FoundationPose model, which then generates the precise 6D pose and point cloud of the component within the scene. This information is crucial for subsequent tasks, including camera pose alignment, grasping, and collision-free planning.

\subsubsection{Camera Frame Alignment}
After we get the estimated target pose, we first use the PCA mentioned before to canonize them. To accurately map these target poses to the real world, we need to align the camera frame in the manual page image, denoted as \(P_{m_i}\), with the real-world camera frame, denoted as \(P_{w_i}\), for each step \(i\). This section will introduce how we calculate the 6D transformation matrix \(T_{mw}\) between these two frames.

To achieve this, we designate a stable part of the scene as a base in the world frame using the VLM and utilize FoundationPose to extract the point cloud of this part. We then canonicalize the point cloud using the same PCA algorithm, ensuring that the relative 6D pose of the same component remains consistent. We denote the canonical base pose in the real world as \(P_{B_w}\), which remains static during this step. From the model's predictions, we can also determine the pose of the same part used as the base in the manual, denoted as \(P_{B_m}\). We denote the transformation matrix between these two frames as \(T_{mw}\). Using this transformation matrix, we map the target pose in the manual frame, \(P_{T_m}\), to the corresponding target pose in the real-world frame, \(P_{T_w}\), for subsequent motion planning. We compute the transformation as follows:

\[
T_{mw} = P_{B_w} P_{B_m}^{-1}
\]

We then calculate the target pose in the real-world frame using:

\[
P_{T_w} = T_{mw} P_{T_m}
\]


\begin{figure}[!ht]
    \centering
    \resizebox{0.48\textwidth}{0.22\textheight}{
    \includegraphics{Figures/appendix_trans_explain.png}}
    \caption{This figure shows the transformation between the estimated pose and the real-world frame; we designate the board of the stool as a base and map the four legs of the stool to the real world}
    \label{fig:appendix-alignment}
\end{figure}

As illustrated in ~\Cref{fig:appendix-alignment}, the stool example clearly demonstrates the process of aligning poses between the manual and real-world frames, ensuring a consistent and reliable foundation for motion planning.

\subsubsection{Heuristic Grasping Policy}  
For general grasping tasks, pre-trained models such as GraspNet\cite{fang2023anygrasp} are commonly used to generate grasping poses. However, in the case of furniture assembly, where components are often large and flat, we need to grasp specific parts of the object that are suitable for subsequent assembly. This requirement poses challenges for GraspNet, as it does not always estimate the best pose for the subsequent action. To address this, in addition to GraspNet, we utilize the poses generated by FoundationPose and consider the shapes of the furniture components in corner cases. These shapes are categorized into two types, as shown in ~\Cref{fig:appendix-grasping}:
\begin{figure}[!ht]
    \centering
    \resizebox{0.48\textwidth}{0.22\textheight}{
    \includegraphics{Figures/grasping.png}}
    \caption{This figure shows the grasping policies for different shapes in our setting; the left one is for stick-shaped, and the right one is for flat, thin-shaped.}
    \label{fig:appendix-grasping}
\end{figure}

\textbf{Stick-Shaped Components}:  
For stick-shaped furniture parts, such as stool legs, we select the center of the point cloud as the grasping position. We define the grasping pose as a top-down approach.

\textbf{Flat and thin-Shaped Components}:  
We first estimate the pose of flat and thin, board-shaped furniture parts using a bounding box. Based on this estimate, we determine the grasping pose by aligning it with the bounding box's orientation. The grasping position is set approximately 3 cm below the top surface.

\subsection{Rationale for Excluding Performance Evaluation of Stage I in Hierarchical Assembly Graph Generation}
\label{appd: no_stage1}

Stage I, Associating Real Parts with Manuals, focuses on associating real parts with the manual. Still, since the IKEA manual lacks isolated images of individual parts, direct quantitative evaluation is challenging. Instead, Stage II implicitly reflects the quality of these associations by outputting the indices of identified real parts. Therefore, we report Stage II results as an intermediate measure of how effectively our approach aligns manual images with real components.


\subsection{Justification for Hierarchical Assembly Graph}
\label{def_assembly_tree}

Using a hierarchical structure to represent assembly steps provides several advantages over simple linear data structures or unstructured step-by-step plans in plain text.

\begin{itemize}
\item Hierarchical structures align naturally with the assembly process where multiple parts and subassemblies combine into larger subassemblies.
% This structured representation allows for an easier evaluation of discrepancies in predicted versus ground truth assembly steps.
\item Lists or text plans struggle to store geometric and spatial relationships between each part or subassembly of the step, which is crucial in real assembly tasks.

% determining the physical feasibility of the assembly process in real life (see \Cref{fig: infeasible}).

\item The hierarchical graph clearly shows the dependencies between steps, revealing which steps you can perform in parallel and which ones you must complete before proceeding to others. 
So, it provides flexibility for parallel construction or strategic sequencing.
% Lists and text plans, by contrast, fix the order of steps.
% \item Trees excel in managing and visualizing complex assembly plans. For example, when a later step requires a subassembly from an earlier step (even though the two steps are not adjacent), a tree naturally represents this relationship by designating them as the parent and child, respectively. In contrast, encoding such non-adjacent relationships in a linear data structure or text-based plan is challenging, as it compromises both the clarity of visualization and the preservation of assembly order.
\end{itemize}

\subsection{Formal Definition of Hierachial Assembly Graph}
\label{appd:def_assembly_graph}

Inspired by~\citet{mo2019structurenet}, we represent the assembly process as a hierarchical graph $S=(\mathbf{P,H,R})$.
A set of nodes $\mathbf{P}$ represents the parts or subassemblies in the assembly process. 
A structure $(\mathbf{H, R})$ describes how these nodes are assembled and related to each other.
The structure consists of two edge sets: $\mathbf{H}$ describes the \textit{assembly relationship} between nodes, and $\mathbf{R}$ represents the \textit{geometric} and \textit{spatial} relationship between nodes.
% \Cref{fig: pipeline} shows an example of the hierarchical graph generated from the manual images.

\textbf{Node}.
Each node $v\in\mathbf{P}$ is an atomic part or a subassembly, consisting of a non-empty subset of parts $p(v)\subset\mathcal{P}$.
% semantic labels relevant to the assembly process. 
The root node \( v_N \) represents the fully assembled furniture, with \( p(v_N) = \mathcal{P} \).
A non-root, non-leaf node \( v_i \) represents a subassembly with \( p(v_i) \) as a non-empty and proper subset of \( \mathcal{P} \).
All leaf nodes \( v_l \) represent atomic parts, containing exactly one element from \( \mathcal{P} \).
Additionally, each non-leaf node corresponds to a manual image \( I \) that describes how to merge smaller parts and subassemblies to form the node.

\textbf{Assembly relationship}.
We formulate the assembly process as a tree, with all atomic parts serving as leaf nodes. The atomic parts are then recursively combined into subassemblies, forming non-leaf nodes until they reach the root node, which represents the fully assembled furniture. The directed edges from a child node to its parent node indicate the assembly relationship.
The edge set \( \mathbf{H} \) includes directed edges from a child node to its parent node, indicating the assembly relationship.
For a non-leaf node $v_i$, denote its child nodes as $C_i$, the following properties hold:
\begin{enumerate}[label=(\alph*)]
    \item $\forall v_j \in C_i, p(v_j)$ is a non-empty subset of $\mathcal{P}$
    \item All children nodes contain distinct elements
    \begin{equation}
        p(v_j) \cap p(v_k) = \emptyset, \forall v_j, v_k \in C_i, j\neq k
    \end{equation}
    \item The union of all child subsets equals \( p(v_i) \):  
    \begin{equation}
        \bigcup_{v_j\in C_i} p(v_{j}) = p(v_i)
    \end{equation}
    
\end{enumerate}

\textbf{Equivalence relationship}.
In addition to the assembly process's hierarchical decomposition, we also consider the equivalence relationship between nodes. 
We label two parts \textit{equivalent} if they share a similar shape and can be used interchangeably in the assembly process.
We represent this relationship with undirected edges $\mathbf{R_i}$ in child nodes $C_i$ of node $v_i$.
% For the assembly task, we select two important relationships between parts: \textit{connected} and \textit{equivalent}.
% % We call two parts \textit{connected} if the assembly process joins them, and we label two parts \textit{equivalent} if they share a similar shape and can be used interchangeably in the assembly process.
% Identifying these relationships is crucial for the assembly process. It helps determine the order of assembly and the correct placement of parts for each step. 
% \lin{i find the following paragraphs hard to digest} We represent these relationships with undirected edges $\mathbf{R_i}$ in child nodes $C_i$ of node $v_i$.
% The edge is represented as $(\{v_a,v_b\},\tau)$, here $\tau$ is the type of relationship, which can be either \textit{connected} ($\tau_c$) or \textit{equivalent} ($\tau_e$).
% Every node \(v_c \in C_i\) is associated with a set of junction points \(j(v_c) = \{j_{c1}, j_{c2}, \dots, j_{cn}\}\), denoting the locations where we can join this node with other components during assembly.
% An edge $(\{v_a,v_b\},\tau_c)\in R_i$ appears between two nodes \(\{v_a, v_b\} \in C_i\), if junction point $j_1\in j(v_a)$ and $j_2\in j(v_b)$ are connected together in the assembly process.
An edge $\{v_a,v_b\}\in R_i$ appears between two nodes \(v_a \in C_i\), \(v_b \in \mathcal{P}\), if the shape represented by \(v_a\) and \(v_b\) are geometric equivalent and thus can be changed during assembly. 
Note that \(v_b\) is not constrained as a child of $v_i$ since any two nodes could be equivalent, regardless of their hierarchical positions.
% Because every part and subassembly, represented by children of a non-leaf node, must be joined to create a larger subassembly, the subgraph \((C_k, R_k)\) is a connected graph. Consequently, every non-root node \(v \in \mathbf{P}\) includes junction points \(j(v)\) to enable these connections.
% Because we join every part and subassembly to create a larger subassembly, these edges form a connected graph $(C_i,\mathbf{R}_i)$.

The assembly structure is a hierarchical graph, where the nodes represent parts or subassemblies, and the edges represent the assembly and equivalence relationships.
We consider this structured representation to be a more informative and interpretable way to formulate the assembly process than a flat list of parts.
% \crtie{maybe in this section, we only introduce how to build the assembly tree(which nodes are in the graph, and their assembly relationship), and leave the details of how to build 'equivalent' and 'connected' relationships, and the pose of each component to the next section 'per-step inference.'}



\subsection{Prompts}
\label{appd: additional_prompts}
We offer a comprehensive set of prompts utilized in the VLM-guided hierarchical graph generation process. The process involves four distinct prompts, divided into two stages. The first two prompts, which are slight variations of each other, are used in \emph{Stage I: Associating Real Parts with Manuals}. The remaining two prompts, also slight variations of each other, are employed in \emph{Stage II: Identifying Involved Parts in Each Step}.
\begin{enumerate}
\item The first prompt is part of Stage I, and it initializes the JSON file's structure and consists of two sections:
\begin{itemize}
    \item 1.a): \textbf{Image Set:} An image of the scene with furniture parts labeled using GroundingDINO \cite{liu2025grounding}, alongside an image of the corresponding manual's front page.
    \item 1.b) \textbf{Text Instructions:} A few sentences explaining the JSON file generation, supported by an example of the desired structure via in-context learning.
\end{itemize}
This prompt is passed into GPT-4o to generate a JSON file with the name and label for each part. 

\item The second prompt belongs in Stage I as well, and it populates the JSON file with detailed descriptions of roles. It includes:
\begin{itemize}
    \item 2.a): \textbf{Image Set:} Images of all manual pages (replacing the front page) to provide context about the function of each part and the scene image from the first prompt.
    \item 2.b): \textbf{Text Instructions:} a simple text instruction explaining the context and output.
\end{itemize}
We combine the JSON output from the first prompt with the second prompt, then query GPT-4o to generate the populated JSON file.

\item The third prompt is a part of Section II, and it generates a step-by-step assembly plan using:
\begin{itemize}
    \item 3.a): \textbf{Image Set:} The scene image and cropped manual pages highlight relevant parts and subassemblies, helping GPT-4o focus on key details. The cropped images also have a highlighted black number on the left, indicating the current assembly step. Our ablation studies demonstrate the effectiveness of these cropped images.
    \item 3.b): \textbf{Text Instructions:} A text instruction combining chain-of-thought and in-context learning to describe the assembly plan generation process and guide the VLM.
The JSON file from Step 2 is concatenated with the third prompt as input, guiding GPT-4o to produce the final text-based assembly plan.
\end{itemize}

\item Section II includes the fourth prompt, which converts the text-based plan into a traversable tree structure for action sequencing in robotic assembly. We achieve this conversion using a simple text input with in-context learning examples.
\end{enumerate}

\twocolumn[{
    \begin{tcolorbox}[title=1.a) \textbf{Image Set} for JSON File Generation]
    \label{JSON_1a}
        \begin{center}
            \includegraphics[width=0.3\textwidth]{Figures/scene_annotated.png}
            \includegraphics[width=0.3\textwidth]{Figures/page_1.png}
        \end{center}\textbf{}
        \vspace{0.5em}
    \end{tcolorbox}

% Open-ended Prompt
\begin{tcolorbox}[title=1.b) \textbf{Text Instructions} for JSON File Generation]
Input is one image, which is a top view of all the parts of one piece of furniture, each has a number, and another image, which is the first page of the setup manual\newline

You should list all the parts in the image, determine their number and name (short description of the part), and show your result in JSON format.\newline

Following is an example. Note that your output should only contain the JSON code without any explanation.\newline

\#\#\#\#\#\#\#\#\#\# example start \#\#\#\#\#\#\#\#\#\#

% $```$ json


[\\
    \{ \\
      "name": "seat frame," \\
      "number": [0] \\
    \}, \\
    \{\newline
      "name": "side leg," \newline
      "number": [1]\newline
    \},\newline
    \{\newline
      "name": "side le," \newline
      "number": [2]\newline
    \},\newline
    \{\newline
        "name": "support b," "\newline
        "number": [3]\newline
    \}\newline
]\newline
% \`\`\`\newline
\#\#\#\#\#\#\#\#\#\# example end \#\#\#\#\#\#\#\#\#\#
\end{tcolorbox}
}]

\twocolumn[{
    \begin{tcolorbox}[title=2.a) \textbf{Image Set} for JSON File Refinement]
        \begin{center}
            \includegraphics[width=0.22\textwidth]{Figures/scene_annotated.png}
            \includegraphics[width=0.22\textwidth]{Figures/page_1.png}
            \includegraphics[width=0.22\textwidth]{Figures/page_2.png}
            \includegraphics[width=0.22\textwidth]{Figures/page_3.png}
            \includegraphics[width=0.22\textwidth]{Figures/page_4.png}
            \includegraphics[width=0.22\textwidth]{Figures/page_5.png}
            \includegraphics[width=0.22\textwidth]{Figures/page_6.png}
            \includegraphics[width=0.22\textwidth]{Figures/page_7.png}
        \end{center}\textbf{}
        \vspace{0.5em}
    \end{tcolorbox}

\begin{tcolorbox}[title=2.b) \textbf{Text Instructions} for JSON File Refinement]
You are a robot assistant responsible for assembling IKEA furniture.\newline

Your inputs include \{A\}: an rbg image of the scene consisting of furniture parts labeled with white numbers on a black background, \{B\}: a JSON file that describes the image's objects and labels, and \{C\}: a set of IKEA setup manual pages.\newline

Please note that you will only construct the piece of furniture that the manual describes.\newline


You can ignore nails and other tools in the manual and only focus on the furniture parts that exist in \{A\}: the rbg scene image.\newline

First, you are ONLY responsible for identifying the relevant materials that will be required to assemble the furniture in the image. Output a table of selected materials, with their labeled numbers and a brief explanation of why they are selected and how they are related to items on the setup manual. The table format should be JSON, and it should be really similar to \{B\}, but with an additional explanation section for each selected material and its labeled number. Hint: Usually, in 99.999\% of cases, the number of selected materials equals the number of labeled furniture parts.
\end{tcolorbox}
}]


\twocolumn[{
    \begin{tcolorbox}[title=3.a) \textbf{Image Set} for Step-By-Step Plan Generation]
        \begin{center}
            \includegraphics[width=0.3\textwidth]{Figures/scene_annotated.png}
            \includegraphics[width=0.3\textwidth]{Figures/step_0_no_seg_numbered.png}
            \includegraphics[width=0.3\textwidth]{Figures/step_1_no_seg_numbered.png}
        \end{center}\textbf{}
        \vspace{0.5em}
    \end{tcolorbox}
}]
\twocolumn[{%
% Open-ended Prompt
\begin{tcolorbox}[title=3.b) \textbf{Text Instructions} for Step-By-Step Plan Generation]
You are a robot assistant responsible for assembling IKEA furniture. You will be responsible for creating a detailed step-by-step plan for assembling the furniture.\newline

For your input, you will receive a set of images, which represent a few pages of the setup manual containing the setup instructions for the furniture. On the left of each page, there is a rectangular section with a white background and a big, black, bolded number. This number indicates the current assembly step. On each page, we segment the furniture with different colors (the three most common are red, green, and purple, though sometimes other colors are used). The purpose of using these colors is to help you clearly identify which furniture parts are involved in each assembly step.\newline

You will also receive an rbg image of the scene consisting of furniture parts labeled with white numbers on a black background and a JSON-formatted table that describes the RGB image's objects and labels.\newline

Your new task is to carefully describe every step according to the manual. Each colored segmented furniture part should correspond to one step. Your planned steps should only describe what and how segmented furniture parts are involved; don't worry about nails and other minor tools for now. Your focus should only be on the colored segmented furniture parts. Be as specific as possible in your description.\newline

Let's think step by step: (1) count the total number of colored, segmented furniture parts. (Hint: This equals the total number of pages in the manual, with each page identified by a big, bold black number on the left.) The total number of colored, segmented furniture parts will be your total number of steps. (2) for each step, focus on one colored, segmented furniture part at a time. Describe only the furniture parts involved in that step. (3) We repeat step 2 for each remaining step until we have described all the steps. So, if there is only one page of the setup manual overlayed with mask segmentations, then there is only one step. If there are ten pages of the setup manual overlayed with mask segmentations, then there are ten steps.\newline

Here is an example of a fully constructed plan for your reference only. It has nothing to do with the current plan:\newline

\#\#\#\#\#\#\#\#\#\# assistant example start \#\#\#\#\#\#\#\#\#\#\newline
We have five input images, but one image shows furniture parts lying on a floor that we label with marks (white numbers on a black square background). Therefore, we have only four pages of the setup manual overlaid with mask segmentations. Thus, there are four total steps.\newline

\#\#\# Step 1:\newline
- **Parts Needed:** Backrest Frame (1), Seat Cushion (5)\newline
- **Instructions:**\newline
  - **Align Frame and Seat:** Connect the backrest frame (1) next to the seat cushion (5) as shown in the segmented manual.\newline

\#\#\# Step 2:\newline
- **Parts Needed:** Subassembly from Step 1, Side Leg Frame (2)\newline
- **Instructions:**\newline
  - **Position Leg Frame:** Link the first side leg frame (2) with the assembled seat and backrest combo from Step 1.\newline

\#\#\# Step 3:\newline
- **Parts Needed:** Subassembly from Step 2, Support Beam (3), Support Beam (4), Side Leg Frame (6)\newline
- **Instructions:**\newline
  - **Connect Support Beams:** Attach support beams (3), (4), and the second side leg frame (6) between the assembled frame and leg structure from Step 2. \newline


\#\#\#\#\#\#\#\#\#\# assistant example end \#\#\#\#\#\#\#\#\#\#\newline

Now it is your turn to generate a detailed step-by-step plan; here is the JSON formatted table:
\end{tcolorbox}
}]

\twocolumn[{%
\begin{tcolorbox}[title=4) Prompt for Converting Text-Formatted Plan to Tree]
You are a robot assistant responsible for assembling IKEA furnitures.\newline

Your new task is to convert a step-by-step furniture assembly instruction plan from text format into a tree format.\newline

The tree represents the stage of the furniture assembly, with lower-level nodes representing the initial and beginning stages and the upper level representing the concluding and finished stages of the furniture assembly.\newline

We treat each end node (leaf) of the tree as an atomic furniture part that we cannot further decompose. As you move up the tree, each parent node will represent two or more child nodes combined. Finally, the root node will be the completed furniture.\newline

You should clearly describe how every node is connected.\newline

We output the tree strictly as a nested list of integers without any additional comments or text.

\end{tcolorbox}
\begin{tcolorbox}[title=4) (Continued) In-Context Learning Examples for Text-Formatted Plan to Tree Prompt]
EXAMPLE INPUT 1:\newline
Here's a step-by-step assembly plan for the furniture using the provided parts:\newline

\#\#\# Step 1: Assemble Backrest and Seat\newline
- **Parts Needed:** Backrest Frame (1), Seat Cushion (5)\newline
- **Instructions:**\newline
  - Place the Backrest Frame (1) and Seat Cushion (5) adjacent as shown in their respective colors (red and green).\newline
  - Ensure the backrest is upright and securely attached to the seat.\newline

\#\#\# Step 2: Attach Side Leg Frame\newline
- **Parts Needed:** Side Leg Frame (2) and subassembly from Step 1\newline
- **Instructions:**\newline
  - Position the Side Leg Frame (2) on one side of the assembled backrest and seat structure.\newline

\#\#\# Step 3: Attach Side Leg Frame Again\newline
- **Parts Needed:** Side Leg Frame (7) and subassembly from Step 2\newline
- **Instructions:**\newline
  - Position the Side Leg Frame (7) on the other side of the assembled backrest and seat structure.\newline

\#\#\# Step 4: Connect Support Beams\newline
- **Parts Needed:** Support Beams (3, 4) and subassembly from Step 3\newline
- **Instructions:**\newline
  - Attach Support Beams (3, 4) to the inside of the Side Leg Frame, as depicted.\newline

  
Check the entire assembly for any loose parts and re-tighten as necessary. The chair should now be fully assembled and ready for use.\newline

EXAMPLE OUTPUT 1:\newline
'''python\newline
[
    [
        [
            [
                1,
                5
            ],
            2
        ],
        7
    ],
    3,
    4
]\newline
'''
\end{tcolorbox}

}]


\twocolumn[{%
\begin{tcolorbox}[title=4) (Continued) In-Context Learning Examples for Text-Formatted Plan to Tree Prompt]
EXAMPLE INPUT 2:\newline

\#\#\# Step 1: Connect Support Beams and Leg Frame\newline
**Parts Involved:** Support Beams (0 and 3), Leg Frame (4)\newline
- **Instructions:** Position the leg frame (4) horizontally on the floor. Align the support beams (0 and 1) vertically to connect with the leg frame. Ensure that each beam is fitted securely into the designated slots on the frame.\newline

\#\#\# Step 2: Attach Backrest Slats\newline
**Parts Involved:** Backrest Slats (2) and subassembly from Step 1\newline
- **Instructions:** Insert the backrest slats (2) into the slots on the leg frame. Ensure that the slats are facing outward and securely fitted to provide back support.\newline

\#\#\# Step 3: Connect Seat Cushion\newline
**Parts Involved:** Seat Cushion (1) and subassembly from Step 2\newline
- **Instructions:** Place the seat cushion (1) on top of the assembled frame. Align the cushion with the edges of the frame for balance and comfort.\newline

EXAMPLE OUTPUT 2:\newline
'''python\newline
[
    [
        [
            0,
            3,
            4
        ],
        2
    ],
    1
]\newline
'''\newline
EXAMPLE INPUT 3:\newline

\#\#\# Step 1: Connect Support Beams and Leg Frame\newline
**Parts Involved:** Support Beams (7, 11, 6), Leg Frame (5)\newline
- **Instructions:** Position the leg frame (5) horizontally on the floor. Align the support beams (7, 11, 6) vertically to connect with the leg frame. Ensure that each beam is fitted securely into the designated slots on the frame.\newline

\#\#\# Step 2: Attach Backrest Slats\newline
**Parts Involved:** Backrest Slats (1, 10) and subassembly from Step 1\newline
- **Instructions:** Insert the backrest slats (1, 10) into the slots on the leg frame. Ensure that the slats are facing outward and securely fitted to provide back support.\newline

\#\#\# Step 3: Connect Seat Cushion\newline
**Parts Involved:** Seat Cushion (3) and subassembly from Step 2\newline
- **Instructions:** Place the seat cushion (3) on top of the assembled frame. Align the cushion with the edges of the frame for balance and comfort.\newline

\#\#\# Step 4: Connect Support Beams and Leg Frames\newline
**Parts Involved:** Support Beams (8, 4), Leg Frames (2, 9)\newline
- **Instructions:** Position the leg frame (2, 9) horizontally on the floor. Align the support beams (8, 4) vertically to connect with the leg frame.\newline

\#\#\# Step 5: Connect Support Beams and Leg Frames\newline
**Parts Involved:** Subassembly from Step 4 and subassembly from Step 3\newline
- **Instructions:** Connect the two subassemblies together\newline

\#\#\# Step 6: Connect Support Beams and Leg Frames\newline
**Parts Involved:** Leg frame (0) and subassembly from Step 5\newline
- **Instructions:** Connect the final leg frame with the previous subassembly\newline

EXAMPLE OUTPUT 3:\newline
'''python\newline
[
    [
        [
            8,
            4,
            2,
            9
        ],
        [
            [
                [
                    7,
                    11,
                    6,
                    5
                ], 
                1,
                10 
            ],  
            3
        ]
    ]
    0]\newline
'''\\

YOUR REAL INPUT:
\end{tcolorbox}
}]

