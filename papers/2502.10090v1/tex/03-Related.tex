\section{Related Work}
\subsection{Furniture Assembly}
Part assembly is a long-standing challenge with extensive research exploring how to construct a complete shape from individual components or parts~\citep{chen2022neural,funkhouser2011learning,jones2021automate,lee2021ikea,li2020learning,scarpellini2024diffassemble,wu2023leveraging,tian2024asap,tian2022assemble}. Broadly, we can categorize part assembly into \emph{geometric assembly} and \emph{semantic assembly}. \emph{Geometric assembly} relies solely on geometric cues, such as surface shapes or edge features, to determine how parts mate together~\citep{chen2022neural,wu2023leveraging,sellan2022breaking,du2024generative}. In contrast, \emph{semantic assembly} primarily leverages high-level semantic information about the parts to guide assembly process~\citep{funkhouser2011learning,jones2021automate,lee2021ikea,li2020learning,tian2022assemble}.

Furniture assembly is a representative \emph{semantic assembly} task, where each part has a predefined semantic role (e.g., a chair leg or a tabletop), and the assembly process follows intuitive, common-sense relationships (e.g., a chair leg must be attached to the chair seat). Previous studies on furniture assembly have tackled different aspects of the problem, including the motion planning~\citep{suarez2018can}, multi-robot collaboration~\citep{knepper2013ikeabot}, and assembly pose estimation~\citep{li2020learning,yu2021roboassembly,li2024category}. Researchers have developed several datasets and simulation environments to facilitate research in this domain. For example,
\citet{wang2022ikea,liu2024ikea} introduced IKEA furniture assembly datasets containing 3D models of furniture and structured assembly procedures derived from instruction manuals. Additionally, \citet{lee2021ikea} and~\citet{yu2021roboassembly} developed simulation environments for IKEA furniture assembly, while
\citet{heo2023furniturebench} provides a reproducible benchmark for real-world furniture assembly.
However, existing works typically focus on specific subproblems rather than addressing the entire assembly pipeline. 
% Some methods target low-level motion planning and control~\citep{suarez2018can}, some focus on assembly poses estimation~\citep{li2020learning,yu2021roboassembly,li2024category}, while others emphasize high-level task planning or multi-robot collaboration~\citep{knepper2013ikeabot} 01
%This fragmentation limits their applicability to learning and executing the complete assembly process.
In this work, we aim to develop a comprehensive framework that learns the sequential process of furniture assembly from manuals and deploys it in real-world experiments.

\subsection{VLM Guided Robot Learning}
Vision Language Models (VLMs)~\citep{yin2023survey} have been widely used in robotics to understand the environment~\citep{huang2024copa} and interact with humans~\citep{shi2024yell}.
Recent advancements highlight VLMs' potential to enhance robot learning by integrating vision and language information, enabling robots to perform complex tasks with greater adaptability and efficiency~\cite {huang2024rekep}.
A potential direction is the development of the Vision Language Action Model (VLA Model) that can generate actions based on the vision and language inputs~\citep{black2024pi_0,kim2024openvla,brohan2023rt,team2024octo}.
However, training such models requires vast amounts of data, and they struggle with long-horizon or complex manipulation tasks.
Another direction is to leverage VLMs to guide robot learning by providing high-level instructions and perceptual understanding. VLMs can assist with task descriptions~\citep{huang2024copa,huang2024rekep}, environment comprehension~\citep{jiang2024roboexp}, task planning~\citep{vemprala2024chatgpt,yao2022react,zhao2024large}, and even direct robot control~\citep{li2024manipllm}.
Additionally, \citet{goldberg2024blox} demonstrates how VLMs can assist in designing robot assembly tasks. Building on these insights, we explore how VLMs can interpret abstract manuals and extract structured information to guide robotic skill learning for long-horizon manipulation tasks.


\subsection{Learning from Demonstrations}
Learning from demonstration~(LfD) has achieved promising results in acquiring robot manipulation skills~\citep{fu2024mobile,zhu2023viola,chi2023diffusion}. For a broader review of LfD in robotic assembly, we refer to ~\citet{zhu2018robot}.
The key idea is to learn a policy that imitates the expert's behavior.
However, previous learning methods often require fine-grained demonstrations, like robot trajectories~\citep{chi2023diffusion} or videos~\citep{kareer2024egomimic,sontakke2024roboclip,jonnavittula2024view}.
Collecting these demonstrations is often labor-intensive and may not always be feasible.
Some works propose to learn from coarse-grained demonstrations, like the hand-drawn sketches of desired scenes~\citep{sundaresan2024rt} or rough trajectory sketches~\citep{gu2023rt}.
These approaches reduce dependence on expert demonstrations and improve the practicality of LfD. However, they are mostly limited to tabletop manipulation tasks and do not generalize well to more complex, long-horizon assembly problems.
In this work, we aim to extend LfD beyond these constraints by tackling a more challenging assembly task using abstract instruction manuals. 