\section{Technical Approach}
Our approach automates furniture assembly by leveraging the VLM to interpret IKEA-style manuals and guide robotic execution. Given a visual manual and physical parts in a pre-assembly scene, a VLM generates a hierarchical assembly graph, defining which parts and subassemblies are involved in each step. Next, a per-step pose estimation model predicts 6D poses for each component using a manual image and the point clouds of involved components. Finally, for assembly execution, the estimated poses are transformed into the robot's world frame, and a motion planner generates a collision-free trajectory for part mating. 

This paper shows an overview of our framework in Fig.~\ref{fig: pipeline}. We describe the VLM-guided assembly hierarchical graph generation in~\Cref{sec:graph_gen_full_desc}, followed by per-step assembly pose estimation in~\Cref{sec:method-per-step} and assembly action generation based on component relationships in~\Cref{sec:method-action}.

\begin{figure*}[htb!]
\centering
\includegraphics[page=1, width=0.95\textwidth]{Figures/fig2_final.pdf} 
\caption{\textbf{Framework Overview.} (1) GPT-4o \cite{achiam2023gpt} is queried with manual pages to generate a sequential assembly plan, represented as a hierarchical assembly graph. 
(2) The furniture components’ point clouds and corresponding manual images are processed by a pose estimation module to predict target poses for each component. (3) The system sequentially executes the assembly by planning and performing robotic actions based on the hierarchical assembly graph and estimated poses.}
\label{fig: pipeline}
\end{figure*}

\subsection{VLM Guided Hierarchical Assembly Graph Generation}
\label{sec:graph_gen_full_desc}

This section demonstrates how VLMs can interpret IKEA-styled manuals to generate high-level assembly plans. Given a manual and a real-world image of furniture parts (\emph{pre-assembly scene image}), a VLM predicts \emph{a hierarchical assembly graph}. We show one example in Fig.~\ref{fig: pipeline}. In this graph, leaf nodes represent atomic parts, while non-leaf nodes denote subassemblies. We structure the graph in multiple layers, where each layer contains nodes representing parts or subassemblies involved in a single assembly step (corresponding to one manual image). The directed edges from the children to a parent node indicate that the system assembles the parent node from all its children nodes. 
Additionally, we add edges between equivalent parts, denoting these parts are identical($e.g.$ four legs of a chair).
Representing the assembly process as a hierarchical graph can decomposes the assembly into sequential steps while specifying necessary parts and subassemblies. 
We give the formal definition of the hierarchical graph in~\Cref{appd:def_assembly_graph}.
We achieve this in two stages: \textit{Associating Manuals with Real Parts} and \textit{Identifying Parts needed in Each Image}.

\subsubsection{VLM Capabilities and General Prompt Structure} The task is inherently complex due to the diverse nature of input images. Manuals are typically abstract sketches, whereas \emph{pre-assembly scene images} are high-resolution real-world images. 
Such diversity requires advanced visual recognition and spatial reasoning across varied image domains, which are strengths of VLMs due to their training on extensive, internet-scale datasets. We demonstrate the effectiveness of VLMs for this task in \Cref{assembly_tree_gen} and 
\Cref{vlm_plan_complete}.
% Appendix D.

Every VLM prompt consists of two components: 
\begin{itemize}
  \item  \textbf{Image Set:} This includes all manual pages and the real-world \emph{pre-assembly scene image}. Unlike traditional VLM applications in robotics \cite{kim2024openvla, huang2024rekep}, which process a single image, our method requires multi-image reasoning.
  \item \textbf{Text Instructions:} These instructions provide a task-specific context, guiding the model in interpreting the image set. The instructions range from simple directives to Chain-of-Thought reasoning \cite{wei2022chain}. All instructions incorporate in-context learning examples, specifying the required output format—be it JSON, Python code, or natural language. This structure is essential to our multi-stage pipeline, ensuring well-structured, interpretable outputs that seamlessly integrate into subsequent stages.
\end{itemize}

\subsubsection{Stage I: Associating Real Parts with Manuals} 
Given the manual's cover sketch of the assembled furniture and the \emph{pre-assembly scene image}, the VLM aims to associate physical parts with the manual. The VLM achieves this by predicting the roles of each physical part through semantically interpreting the manual's illustrations. This process involves analyzing spatial, contextual, and functional cues in the manual illustrations to enable a comprehensive understanding of each physical part. This design mimics human assembly cognition—people first map abstract manual images to physical parts before assembling. Our method follows CoT \cite{wei2022chain} and Least-to-Most \cite{zhou2022least} prompting, reducing cognitive load and improving accuracy. 
We considered pairwise matching of parts from manuals and scene images, but we found it impractical because the manuals not depict each part independently.

To enhance part identification, we employ Set of Marks \cite{yang2023set} and GroundingDINO \cite{liu2025grounding} to automatically label parts on the \emph{pre-assembly scene image} with numerical indices. The labeled scene image and manual sketch form the \textbf{Image Set}. \textbf{Text instructions} consist of a brief context explanation for the association task of predicting the roles of each physical part, accompanied by in-context examples of the output structure: \\
\centerline{\emph{\{name, label, role\}}}

For example, in~\Cref{fig: pipeline} In Stage I Output, we describe the chair's seat as \emph{{name: seat frame, label: [2], role: for people sitting on a chair, the seat offers essential support and comfort and is positioned centrally within the chair's frame.}}. Here, \emph{[2]} indicates that this triplet corresponds to the physical part labeled with index 2 in the pre-assembly scene image. This triplet format enhances interpretability and ensures consistency by structuring all outputs into the same data format. We use the Image Set and Text Instructions as the input prompt for the VLM (specifically GPT-4o \cite{achiam2023gpt}) and query it once to generate real assignments for all physical parts. We then use these labels as leaf nodes in the hierarchical assembly graph.

We can obtain equivalent parts through these triplets. When two physical parts share the same geometric shapes, their triplets only differ by label. For example, in~\Cref{fig: pipeline} Stage I Output, \emph{\{name: side frame, label: [0], role:...\}} and \emph{\{name: side frame, label: [1], role:...\}}—these two parts are considered equivalent. Understanding equivalent part relationships is crucial for downstream modules, as demonstrated by our ablation experiments(see~\Cref{appd:pose_ablation}).

\subsubsection{Stage II: Identify Involved Parts in Each Step}
This stage focuses on identifying the particular parts and subassemblies involved in each manual page. The VLM achieves this by reasoning through the illustrated assembly steps, using the triplets and the labeled pre-assembly scene from the previous stage as supporting hints.

In practice, we observe that irrelevant elements in the manual (e.g., nails, human figures) can distract the VLM. Following \cite{wang2022ikea}, we manually crop the illustrated parts and subassemblies in each manual step to focus the VLM's attention (\Cref{fig: pipeline} Stage II Image Set), significantly improving performance (see Ablation Study for details). Automating Region-of-Interest (ROI) detection remains an open problem beyond the scope of this work and is left for future research.

The manual pages, combined with the labeled pre-assembly scene from the previous stage, form the \textbf{Image Set}. 
The \textbf{Text Instructions} use a Chain-of-Thought prompt to guide the VLM in identifying parts and subassemblies step by step and includes in-context examples that clarify the structured output format: a pair consisting of (Step N, Labeled Parts Involved). 
The bottom left output of~\Cref{fig: pipeline} provides an example of this format.
Together, the \textbf{Image Set} and \textbf{Text Instructions} compose the input prompt for GPT-4o, which generates pairs for all assembly steps using a single query. 

As shown in Fig.~\ref{fig: pipeline}, the system outputs nested lists. We then transform these lists, along with the equivalent parts, into a hierarchical graph. Using this assembly graph, we traverse all non-leaf nodes and explore various assembly orders. Formally, a feasible assembly order is an ordered set of non-leaf nodes, ensuring that a parent node appears only after all its child nodes. A key advantage of the hierarchical graph representation is its flexibility—since the assembly sequence is not unique, it allows for parallel assembly or strategic sequencing.

\subsection{Per-step Assembly Pose Estimation}
\label{sec:method-per-step}

Given an assembly order, we train a model to estimate the poses of components (parts or subassemblies) at each step of the assembly process. At each step, the model inputs the manual image and the point clouds of the involved components, predicting their target poses to ensure proper alignment. To support this task, we construct a dataset for sequential pose estimation. For a detailed description, see~\Cref{appd: Per-step Assembly Pose Estimation Dataset}.
% Appendix A.

Given each component's point cloud (obtained from real-world scans or our dataset), we first center it by translating its centroid to the origin. Next, we apply Principal Component Analysis (PCA) to identify the dominant object axes, which define a canonical coordinate frame. The most dominant axes serve as the reference frame, ensuring a shape-driven and consistent orientation that remains independent of arbitrary coordinate systems.

The dataset we create provides manual images, point clouds, and target poses for each component in the camera frame of the corresponding manual image(following~\cite{li2020learning}). For an assembly step depicted in the manual image $\mathcal{I}_i$, the inputs to our model include: (1) the manual image $\mathcal{I}_i$; (2) the point clouds of all involved components. The output is the target pose $T\in SE(3)$ for each component represented in the camera frame of $I_i$. 

\subsubsection{Model Architecture} Note that the number of components at each step is not fixed, depending on the subassembly division of the furniture.
Our pose estimation model consists of four parts: an image encoder $\mathcal{E}_I$, a point cloud encoder $\mathcal{E}_P$, a cross-modality fusion module $\mathcal{E}_G$, and a pose regressor $\mathcal{R}$.

We first feed the manual image $I$ into the image encoder to get an image feature map $\mathbf{F}_I$.
\begin{equation}
    \mathbf{F}_I = \mathcal{E}_I(I)
\end{equation}
Then, we feed the point clouds into the point cloud encoder to get the point cloud feature for each component. 
\begin{equation}
    \{\mathbf{F}_j\} = \mathcal{E}_P(\{P\}_j)
\end{equation}

In order to fuse the multi-modality information from the manual image and the point cloud features, we leverage a GNN~\citep{wu2020comprehensive} to update the information for each component.
We consider the manual image feature and component-wise point cloud features as nodes in a complete graph, employing a GNN to update the information for each node.
\begin{equation}
    \mathbf{F}_I',\{\mathbf{F}_j'\} = \mathcal{E}_G(\mathbf{F}_I,\{\mathbf{F}_j\})
\end{equation}
where $\mathbf{F}_I',\{\mathbf{F}_j'\}$ are updated image and point cloud features.

Finally, we feed the updated point cloud features as input into the pose regressor to get the target pose for each component.
\begin{equation}
    T_j = \mathcal{R}(\mathbf{F}_j')
\end{equation}

\subsubsection{Loss Function} We adopt a loss function that jointly considers pose prediction accuracy and point cloud alignment, following~\citep{zhang2024manual,li2024category}. The first term penalizes errors in the predicted SE(3) transformation, while the second measures the distance between predicted and ground truth point clouds. To account for interchangeable components, we compute the loss across all possible permutations of equivalent parts and select the minimum loss as the final training objective. We provide further details on the loss formulation and training strategy in
~\Cref{appd:implementation_pose}.
% Appendix B.


\subsection{Robot Assembly Action Generation}  
\label{sec:method-action}  

\subsubsection{Align Predicted Poses with the World Frame}
At each assembly step, the previous stage predicts each component's pose in the camera frame of the manual image. However, real-world robotic systems operate in their world frame, requiring a 6D transformation between these coordinates. Consider two components, A and B. The predicted target poses in the camera frame are denoted as $^{I_i}\hat{\mathcal{T}}_a$ and $^{I_i}\hat{\mathcal{T}}_b$. Meanwhile, our system can collect the current 6D pose of part A in the world frame, represented as $^{W}\mathcal{T}_a$. 
To align $^{I_i}\hat{\mathcal{T}}_a$ to $^{W}\mathcal{T}_a$, we compute the 6D transformation matrix ${}^{W}_{I_i}\mathcal{T}$, which maps the camera frame to the world frame.
\begin{equation}
    ^{W}\mathcal{T}_a = {}^{W}_{I_i}\mathcal{T}  {}^{I_i}\hat{\mathcal{T}}_a
\end{equation}
Using the same transformation ${}^{W}_{I_i}\mathcal{T}$, we compute the assembled target pose of part B (and all remaining components) in the world frame.
\begin{equation}
    ^{W}\mathcal{T}_b = {}^{W}_{I_i}\mathcal{T}  {}^{I_i}\hat{\mathcal{T}}_b
\end{equation}
This transformation accurately maps predicted poses from the manual image frame to the robot's world frame, ensuring precise assembly execution.

\subsubsection{Assembly Execution}
Once our system determines the target poses of each component in the world frame for the current assembly step, it grasps each component and generates the required action sequences for assembly.

\paragraph{Part Grasping}
After scanning each real-world part, we obtain the corresponding 3D meshes for each part. We employ FoundationPose~\cite{wen2024foundationpose}, and the Segment Anything Model (SAM)~\cite{kirillov2023segany} to obtain the initial poses of all parts in the scene.

Given the pose and shape of each part, we design heuristic grasping methods tailored to the geometry of individual components. While general grasping algorithms such as GraspNet~\cite{fang2023anygrasp} are viable, grasping is beyond the scope of this work. Instead, we employ heuristic grasping strategies specifically designed for structured components in assembly tasks. For stick-shaped components, we grasp the centroid of the object after identifying its longest axis for stability. For flat and thin-shaped components, we use fixtures or staging platforms to securely position the object, allowing the robot to grasp along the thin boundary for improved stability. We provide further details on these grasping methods in~\Cref{appd: real-world}.
% Appendix G.

\paragraph{Part Assembly Trajectory}  
\label{sec:method-realworld-assemble}  

Once the robot arm grasps a component, it finds a feasible, collision-free path to predefined robot poses (anchor poses). At these poses, the 6D pose of the grasped component is recalculated in the world frame, leveraging the FoundationPose ~\cite{wen2024foundationpose} and the Segment Anything Model (SAM)\cite{kirillov2023segany}. The system then plans a collision-free trajectory to the component's target pose. We use RRT-Connect~\cite{kuffner2000rrt} as our motion planning algorithm. All collision objects in the scene are represented as point clouds and fed into the planner. Once the planner finds a collision-free path, the robot moves along the planned trajectory.

\paragraph{Assembly Insertion Policy}
Once the robot arm moves a component near its target pose, the assembly insertion process begins. Assembly insertions are contact-rich tasks that require multi-modal sensing (e.g., force sensors and closed-loop control) to ensure precise alignment and secure connections. However, developing closed-loop assembly insertion skills is beyond the scope of this work and will be addressed in future research. In our current approach, human experts manually perform the insertion action.



