
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{wrapfig}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{colortbl}

\title{Think-Then-React: Towards Unconstrained \\Human Action-to-Reaction Generation}


\author{Wenhui Tan, Boyuan Li, Chuhao Jin, Wenbing Huang, Xiting Wang \& Ruihua Song \\
Gaoling School of Artificial Intelligence\\
Renmin University of China\\
Beijing, China\\
\texttt{\{tanwenhui404,liboyuan,jinchuhao,hwenbing,xitingwang,rsong\}@ruc.edu.cn}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


% macros:
\newcommand{\ModelName}{Think-Then-React}
\newcommand{\ModelAbbr}{TTR}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/teaser.pdf}
    \caption{Given a human action as input, our Think-Then-React model first \textbf{thinks} by generating an action description and reasons out a reaction prompt. It then \textbf{reacts} to the action based on the results of this thinking process. TTR reacts in a real-time manner at every timestep and periodically re-thinks at specific interval (every two timesteps in the illustration) to mitigate accumulated errors.}
    \label{fig:teaser}
\end{figure}


\begin{abstract}
Modeling human-like action-to-reaction generation has significant real-world applications, like human-robot interaction and games.
Despite recent advancements in single-person motion generation, it is still challenging to well handle action-to-reaction generation, due to the difficulty of directly predicting reaction from action sequence without prompts, and the absence of a unified representation that effectively encodes multi-person motion.
To address these challenges, we introduce Think-Then-React (TTR), a large language-model-based framework designed to generate human-like reactions.
First, with our fine-grained multimodal training strategy, TTR is capable to unify two processes during inference: a \textbf{thinking} process that explicitly infers action intentions and reasons corresponding reaction description, which serve as semantic prompts, and a \textbf{reacting} process that predicts reactions based on input action and the inferred semantic prompts.
Second, to effectively represent multi-person motion in language models, we propose a unified motion tokenizer by decoupling egocentric pose and absolute space features, which effectively represents action and reaction motion with same encoding.
Extensive experiments demonstrate that TTR outperforms existing baselines, achieving significant improvements in evaluation metrics, such as reducing FID from 3.988 to 1.942.
\end{abstract}

\input{sections/1-Introduction}
\input{sections/2-RelatedWork}
\input{sections/3-Method}
\input{sections/4-Experiment}
\input{sections/5-Conclusion}

\textbf{Acknowledgment} This work is supported by the National Natural Science Foundation of China (No. 62276268) and Kuaishou Technology.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix
\input{sections/6-Appendix}

\end{document}
