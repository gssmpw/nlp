% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/detail.v3.pdf}
%     \caption{(a) We propose a unified tokenizing process that encodes human action and reaction while maintaining absolute space feature and egocentric motion feature. (b) To obtain space tokens of a motion, we first extract its initial space state, i.e., 2D position and body orientation. Then we normalize the body center at the origin while facing positive z axis for effectively encoding the following pose sequences. (c) During inference, our method \ModelAbbr~first infers action's intent and semantics. Then \ModelAbbr~could predict corresponding reaction based on both the input action and inferred intent.}
%     \label{fig:main}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/tokenizer.half.pdf}
    \caption{Illustration of our decoupled tokenizer and the plain tokenizer.}
    \label{fig:tokenizer}
\end{figure}

\section{Method}

\subsection{Overview}
For the task of action-to-reaction generation, given a human action  $\mathbf{a}=\{a_i\}^{N_f}_{i=1}$ over $N_f$ frames, our aim is to generate a corresponding reaction $\mathbf{b}=\{b_i\}^{N_f}_{i=1}$ without any input prompts. Most previous works leverage input prompts but they are often inaccessible in unconstrained interaction setting. As the example in Figure~\ref{fig:teaser} shows, when a robot/avatar meets a human, it can only observe the human behaviors, try to understand her/his intents, and think what the robot/avatar is expected to react. There is no prompt available to tell what they are going to do.

To address the above problem, we propose a unified framework, \ModelName~(\ModelAbbr), for both action understanding and reaction generation. First, we propose a unified tokenizer to convert both egocentric poses and absolute spatial location and orientation information into tokens (Section~\ref{sec:tokenizer}). Then, we propose a unified Large Language Model (LLM) based model that are pre-trained on three categories of motion and language related tasks, such as describing a motion, and then fine-tune the model with instructions of predicting a reaction from a given action (Section~\ref{sec:llm}).

To avoid confusion, we define \textbf{pose} as a human body posture or movement within a brief time interval, such as ``taking one step forward'', and a pose can be represented as a single token. A \textbf{motion} refers to a sequence of poses, starting with an initial spatial state represented by space features. For example, a motion could be ``a person walks three steps''.

\subsection{Unified Motion Representation}\label{sec:tokenizer}
To represent one or two persons (denoted by p1 and p2) in an absolute coordinate system, where the x-z plane represents the horizontal plane and y-axis represents the vertical direction, we normalize their centers at the origin while facing positive z axis.
Then for each frame, we extract the 3D skeletons' joint position, velocity and rotation as normalized (or egocentric) pose feature.
Before normalizing, we keep the two persons' pelvis 2D coordination $x$, $z$ and body orientation $r$ to maintain absolute space features. The y-axis (vertical) is not included, as few motions begin in a ``floating" state.
Based on pose and space features of p1 and p2, we propose a unified tokenizing pipeline to convert them into LLM-readable tokens.

\subsubsection{Egocentric Pose Tokenizer}\label{sec:motion_token}
Our aim is to convert continuous pose features into discrete pose tokens like ``$\mathit{<}p_{128}\mathit{>}\mathit{<}p_{42}\mathit{>}...$''. To achieve this, we adopt VQ-VAE~\citep{vqvae}, similar to~\cite{motiongpt}, as the egocentric pose tokenizer.
The pose tokenizer consists of an encoder $\mathcal{E}$ and a decoder $\mathcal{D}$.
$\mathcal{E}$ first encodes continuous motion features, i.e., the 22 joints' position, rotation and velocity vector $\mathbf{m}=\{m_i\}^{N_f}_{i=1}$ into $N_t$ discrete pose tokens, i.e., $N_t$ timesteps, which is downsampled from $N_f$. Specifically, $\mathcal{E}$ and $\mathcal{D}$ are 1D convolution networks with downsample and upsample blocks.

We first obtain the latent pose representation of a motion sequence $\hat{\mathbf{p}}=\mathcal{E}(\mathbf{m})$.
Then, we set up a learnable codebook for human poses $P \in \mathbb{R}^{N_p\times d_p}$ with $N_p$ entries in size $d_p$. A quantization operation $Q(\cdot)$ is applied on the encoded motion latent features by replacing each row vector $\hat{\mathbf{p}}_i\in\hat{\mathbf{p}}$ with its nearest codebook entry $\mathbf{p}_k$.
The process is formulated as:
\begin{equation}
    \mathbf{p}_{quantized}=Q(\mathbf{\hat{\mathbf{p}}}) := (\argmin_{\mathbf{p}_k\in C}||\hat{\mathbf{p}}_i - \mathbf{p}_k||) \in \mathbb{R}^{N_p\times d_p}
\end{equation}

Then, we obtain the reconstructed pose feature $\hat{\mathbf{m}}$ through the decoder $\hat{\mathbf{m}}=\mathcal{D}(\mathbf{p}_q)$. The overall process of the VQ-VAE can be formulated as:
\begin{equation}
\hat{\mathbf{m}}=\mathcal{D}(Q(\mathcal{E}(\mathbf{m}))).
\end{equation}
This is trained via a reconstruction loss with codebook commitment loss. Noting that the $argmin$ operation is non-differentiable, we simply copy the gradients from $\mathcal{D}$ to $\mathcal{E}$ as the estimated gradient. Furthermore, for smoother reconstructed motion and a stable training process, we add an extra velocity regularization in the reconstruction loss and employ exponential moving average (EMA)~\cite{ema} with codebook reset techniques, following~\cite{t2mgpt}. More details about this section are provided in Section~\ref{sec:app:vqvae}.

\subsubsection{Absolute Space Tokenizer}\label{sec:space_token}
For better generalization capability, all motions, including actions and reactions, are normalized to the original point and same direction before being tokenized. Therefore, absolute space information, i.e., the human body 2D position and orientation of each person, is omitted. To extend egocentric pose tokens with absolute space information, we propose converting position and rotation of a person's center point into LLM-readable tokens.

As shown in Figure~\ref{fig:tokenizer}, before normalizing a human motion, we first extract the center point's features, i.e., the position $x$ and $z$ and orientation $r$. We then compute the range of $x$, $z$, and $r$ across the dataset to get the maximum and minimum values. These ranges are uniformly divided into $N_b$ bins, converting each continuous value to discrete tokens. For example, $x = 0.55$ will be represented as ``$\mathit{<}x_{15}\mathit{>}$'' if all the x positions are in range $[-1,1]$ with $N_b=20$ bins.

Finally, we use a unified coding system to represent action, reaction, and their relative information. Specifically, for each motion, we apply absolute space tokenizer to encode initial $x$, $z$, and $r$ into egocentric pose tokens, and apply pose tokenizer to encode the following pose sequence, i.e., the following motion, into pose tokens. Such tokens enable training a model that can understand and generate motion and language simultaneously effectively and efficiently in the subsequent phase.

\subsection{Unified LLM based Motion Understanding and Generation}\label{sec:llm}

\subsubsection{Pre-training}\label{sec:pretrain}
To adapt a language model into a motion-language model, we first pre-train the model with multiple tasks in diverse formats. The pre-training tasks can be categorized into three main types:

\textbf{(1) Motion - Text.} To enable the model to understand and generate human motion, we combine the action and reaction token sequences to construct prompts, which are then fed into the model to generate corresponding textual descriptions, and vice versa. For example, the input sequence could be ``Describe the interaction. Action: $\mathit{<}x_{0}\mathit{>}\mathit{<}z_{1}\mathit{>}\mathit{<}r_{2}\mathit{>}\mathit{<}p_{2}\mathit{>}\mathit{<}p_{7}\mathit{>}...$, Reaction: $\mathit{<}x_{7}\mathit{>}\mathit{<}z_{7}\mathit{>}\mathit{<}r_{8}\mathit{>}\mathit{<}p_{1}\mathit{>}\mathit{<}p_{9}\mathit{>}...$'', and the target response is: ``\textit{One\ waves\ the\ right\ hand,\ and\ the\ other\ one\ waves\ back}''. However, reaction motions are not given during the inference phase. Therefore, the reaction motion is randomly dropped during the training phase to enable the model to infer the interaction from the action motion solely. We also pre-train our model by the instructions on predicting action and reaction token sequences from an interaction description prompt in text.

\textbf{(2) Pose - Space.} Spatial information is represented by orthogonal one-hot tokens, but it may be helpful to infuse auxiliary spatial information into the model. Specifically, we design two tasks:
i) Egocentric pose to absolute space: Given space token and subsequent pose tokens of $t$ timestep, we train the model to predict the space tokens of $t+1$ timestep.
For example, given input space token $\mathit{<}z_{12}\mathit{>}$ and a pose token $\mathit{<}p_{56}\mathit{>}$, which represents ``stepping forward'', the target output should be $\mathit{<}z_{13}\mathit{>}$, denoting spatial transition. 
ii) Absolute space to egocentric pose: Similarly, given space tokens of $t$ and $t+1$ timestep, the model is trained to predict pose tokens between them.

\textbf{(3) Motion - Motion.} To capture more fine-grained action-reaction relationships, we use the first half of the action sequence and the second half of the reaction sequence, along with their corresponding initial spatial tokens, as input. The model is then pre-trained to complete the remaining motion clips.
For example, given a sequence spanning ten timesteps $t_{1:10}$, we feed the first half of the action $a_{1:5}$ and the second half of the reaction $b_{6:10}$, supervising the model to predict $a_{6:10}$ and $b_{1:5}$. Alternatively, we feed $b_{1:5}$ and $a_{6:10}$ to predict $a_{1:5}$ and $b_{6:10}$.

During pre-training, we jointly train all the tasks in a non-causal manner for better efficiency.
Owning to our unified motion and language architecture and space-pose token representation, single person motion and text data can be seamlessly integrated into the training process. We adopt HumanML3D~\citep{humanml3d}, a large scale single person motion-text dataset to facilitate pre-training.
To avoid overfitting, we prepare 20 prompt templates for each task and randomly mask out 15\% of tokens to be predicted during training. In addition, we adopt random clipping of motions as augmentation.
We also find that text generation tasks converge much faster than motion generation tasks. To balance different training tasks, we use the validation losses of the tasks as sampling weights to dynamically select the training source for each epoch.

\subsubsection{Fine-tuning}\label{sec:finetune}
After pre-training, the motion-language model is well-structured with knowledge of pose, space, and text. To make the model applicable to online action-to-reaction generation, we fine-tune it in a causal manner, focusing on two tasks: thinking and reacting.

The \textbf{thinking} task involves understanding action motion, e.g., ``the person is waving hand'', and inferring its possible interaction, e.g., `` two persons wave goodbye to each other'',  or reaction, ``the other person waves back''. At each training iteration, we randomly choose the first quarter, half, or the entire action sequence as input to predict the entire interaction caption.
However, the entire action motion is not given in the early stage of inference, thus the inferred description based on action motion clips may not be accurate, thus we adopt periodical \textbf{re-thinking} in the inference phase for each $N_r$ action tokens given, to dynamically adjust the prompt for reaction generation. We define $N_r$ as re-thinking interval.

For the \textbf{reacting} task, we aim to supervise the model to generate reaction motions conditioned on the generated descriptions during the thinking process. However, in the early stages of fine-tuning, the inferred interaction descriptions are not accurate enough to guide the reaction generation process. Thus, we adopt a teacher forcing approach. In the early stages, the model takes the ground-truth text prompt as a condition to generate the entire reaction sequence. Meanwhile, we monitor the validation loss and text generation metrics. When the metrics tend to converge, we begin to sample predicted interaction captions by the model and use them as reaction generation conditions. This process ensures alignment between training and inference, as ground-truth prompts are inaccessible during inference.
