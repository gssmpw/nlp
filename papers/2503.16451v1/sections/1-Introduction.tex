
\section{Introduction}\label{sec:intro}

Predicting human reaction to human action in real world scenario is an online and unconstrained task, i.e., future states and text prompts are inaccessible, and it has board applications in virtual reality, human-robot interaction and gaming.
Recently, significant advancements have been achieved in the domain of human motion generation especially single-person motion generation, conditioned on text prompts~\citep{momask,tm2t,t2mgpt} and action labels~\citep{actformer,action2motion}.
Leveraging well-annotated human motion datasets~\citep{motionbank,humanml3d,ntu120,kitml}, these models employ various generative frameworks, such as Diffusion Models~\citep{ddpm,intergen,mdm}, Variational Autoencoders (VAEs)~\citep{vae,vaebased}, and Generative Adversarial Networks (GANs)~\citep{gan,ganbased}, to capture cross-modality distributions for better motion generation.
Furthermore, Large Language Models (LLMs) have been applied to human motion generation, demonstrating superior performance~\citep{motiongpt,zhang2024motiongpt}.

However, generating human reaction in multi-person scenario presents a more challenging task due to two primary factors.
First, directly predicting reaction from action sequence is a difficult task with unstability. As shown in Figure~\ref{fig:teaser}, given first two action steps, it is ambiguous to distinguish whether the action is ``shake hand'' or ``high five'', and this would lead to accumulated error to consequent predicted reactions.
Second, dissimilar to single-person motion representation that can adopt an egocentric view, representing human motion in multi-person scenario necessitates both egocentric and absolute information.

Several works have focused on human interaction domain.
For instance, InterFormer~\citep{interformer} proposes injecting human skeleton priors into transformer attention layers for effective spatial modeling.
InterGen~\citep{intergen} introduces a mutual attention mechanism within diffusion process for joint action-reaction generation. However, these methods are not directly applicable to real-world applications, as they rely on extra prompts to condition the generation process.
ReGenNet~\citep{regennet}, similar to our approach, acknowledges the online and unprompted nature of reaction generation, and proposes a diffusion-based model for online reaction generation. It observes that given the action's intention as a condition explicitly, the model can achieve superior performance compared to unprompted settings, highlighting the necessity of understanding interaction semantics for reaction generation.
However, ReGenNet directly models action-to-reaction generation process, without inferring action intention, thus achieving subpar performance.

To address these challenges, we propose Think-Then-React model (TTR), an LLM-based model designed to predict human reactions in online and unprompted settings with the following innovations:
\textbf{First}, to unifiedly represent human motion in multi-person scenario, we propose decoupled space-pose tokenizers that separately handle egocentric pose features and absolute space features. Specifically, we train a VQ-VAE~\citep{vqvae} to encode egocentric human pose sequences (i.e., the space features are normalized, to ensure codebook utilization) into LLM-readable tokens. To maintain spatial features which are crucial in multi-person interaction scenarios, we propose a space tokenizer that encodes positions and orientations as space tokens. We concatenate initial space tokens as prefixes to pose sequences, indicating the initial absolute state of an egocentric motion sequence.
\textbf{Second}, to stabilize reaction prediction process, we introduce a novel framework that is capable to automatically infer text prompts for reaction generation. Specifically, TTR unifies two processes within one model: a \textbf{thinking} process that infers action intent and reasons reaction description, and a \textbf{reacting} process that takes both the action motion and inferred prompts as input, to generate precise and semantically appropriate reactions.
\textbf{Third}, to adapt a language model to motion modality, we design a multi-task and multi-stage training pipeline consisting of motion-text, space-pose and motion-motion generation tasks. With our proposed training strategy, TTR is capable to effectively build correlations between text, motion and space modalities.

In summary, our main contributions are as follows:
\begin{itemize}
\item We introduce a unified motion tokenizer that effectively represents both absolute space and egocentric pose features into LLM-readable tokens in multi-person scenario.
\item We propose a novel framework Think-Then-React with fine-grained training strategy, enabling the adaptation of a language model to a multi-modal model encompassing two processes: inferring action intention and reasoning reaction description, and predicting reaction, within one model, thus ensuring generation quality.
\item Through extensive experiments, we demonstrate that our approach surpasses existing baselines by substantial margins, achieving an FID improvement from 3.988 to \textbf{1.942}, along with other ranking metrics.\footnote{Project page: \url{https://Think-Then-React.github.io/}.}
\end{itemize}