\section{Related Work}

\subsection{Human Motion Representation}
Representing human motion can be mainly categorized into two norms:  continuous and discrete representation.
Human motion can be intuitively represented in continuous space as joint positions of 3D human skeleton extracted with SMPL~\citep{smpl}. However, simply using joint position lacks enough information like joint velocity and rotation. \citet{humanml3d} proposes redundant representation, consisting human root angular velocity, root linear velocity, root height, joint position, joint rotation, and foot-ground contact signals. This representation focuses on egocentric view in single-person scenario.
Based on this, several works propose leveraging VQ-VAE~\citep{vqvae} to encode human motion into discrete tokens, which can be fed into language models, adapting motion prediction task to language modeling task~\citep{motiongpt,momask,t2mgpt,tm2t}. This technique is proved to be quite effective especially in the era of LLMs.

Representing human motion in multi-person scenario is more complicate than in single-person domain, as it is required to simultaneously representing egocentric pose and absolute space features (i.e., distance and orientation among multiple persons). Contrary to use normalized position and orientation in egocentric view, \citet{intergen} proposes a non-canonical representation that directly takes global signals (joint positions and velocities) as continuous motion representation, to maintain absolute information in multi-person scenario. Similar to ~\citet{humanml3d}, it combines joint position, velocity, rotation and foot-ground contact as continuous motion feature.
Based on previous works, we propose a unified tokenizer, which decouples space and pose tokenization process, enabling effective adaptation of discrete motion representation to multi-person domain.

\subsection{Human Motion Generation}
The field of Human Motion Generation focuses on creating realistic and diverse 3D human motion from various input modalities, including text~\citep{t2mgpt,tm2t,motiongpt,momask,intergen}, action labels~\citep{action2motion,actformer,petrovich2021action}, and human motion~\citep{interformer,intergen,regennet}. Most research has concentrated on text-conditioned single-person motion generation (text-to-motion) tasks. In this area, several works have utilized generative models commonly used in the vision domain, such as GANs, VAEs, and Diffusion Models, to generate human motion sequences. Another prominent approach~\citep{t2mgpt,tm2t,motiongpt,momask} employs VQ-VAE to encode human motion sequences into one-hot tokens, which are then processed by auto-regressive models. This method converts the high-dimensional generation task into a next-token prediction task, effectively leveraging pre-trained large language models for more accurate text prompt understanding and diverse motion generation.

Recently, there has been growing interest in generating human motion in multi-person scenarios. InterGen~\citep{intergen} introduces a dual-person interaction dataset with detailed textual descriptions and a diffusion-based model for jointly generating multi-person interactions conditioned on text input. InterFormer~\citep{interformer} utilizes temporal and spatial attention with human skeleton priors to generate human motion sequences reacting to input action sequences. The latest work, ReGenNet~\citep{regennet} employs a diffusion model to generate human reactions based on human actions in a unconstrained and online manner, and points out that given actionâ€™s intention as a condition, the model can achieve superior performance compared to unconstrained settings. However, it directly predicts reaction motion without analyzing semantics of action motion. Our work unifies two processes: a thinking process that infers action semantics, and a reacting process that predicts reaction motion based on action motion and the thinking results, ensuring to generate reaction with appropriate semantics.
