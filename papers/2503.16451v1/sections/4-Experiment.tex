\section{Experiment}
We evaluate our proposed method with strong baselines and further analyze contributions of different components, and the impact of key parameters.

\subsection{Experiment Setup}
\textbf{Dataset.}
We evaluate all the methods on Inter-X dataset, which consists about 9K training samples and 1,708 test samples. Each sample is an action-reaction sequence and three corresponding textual description.
As supplementation, we mix our pre-training data with single person motion-text dataset HumanML3D~\citep{humanml3d}, which consists more than 23K annotated motion sequences.
We uniformly sample frames for both datasets to 30 FPS. 

\textbf{Evaluation Metrics.}
Following single-person motion generation~\citep{t2mgpt}, we adopt the these metrics to quantitatively evaluate the generated motion: R-Precision measures the ranking of Euclidean distances between motion and text features. Accuracy (Acc.) assesses how likely a generated motion could be successfully recognized as its interaction label, like ``high-five''. Frechet Inception Distance~\citep{fid} (FID) evaluates the similarity in feature space between predicted and ground-truth motion. Multimodal Distance (MMDist.) calculates the average Euclidean distance between generated motion and the corresponding text description. Diversity (Div.) measures the feature diversity within generated motions. All the metrics reported are calculated with batch size set to 32, and accumulated across the test dataset, and we evaluate each method for 20 times with different seeds to calculate the final results at 95\% confidence interval.

\textbf{Evaluation Model.} \label{sec:eval}
Every metric mentioned above requires an encoder $\mathcal{M}$ to extract motion feature.
For single person text-to-motion generation tasks, a motion-text matching model are commonly trained as human motion feature extractor.
A simple way to transfer this method to interaction domain is to directly train an interaction-to-text matching model $\mathcal{M}(\mathbf{a}, \hat{\mathbf{b}}, text)$, where action sequence $\mathbf{a}$ and predicted reaction sequence $\hat{\mathbf{b}}$ together is regarded as a generated interaction sequence, or a reaction-to-text match model $\mathcal{M}(\hat{\mathbf{b}}, text)$.
However, the former one may focus too much on the ground-truth action input, leading insufficient discriminative power of $\hat{\mathbf{b}}$'s quality, while the latter one lacks semantics provided by action, thus leading to subpar matching capability.

To address the issue, we simply uniformly mask off a large portion of $\mathbf{a}$, obtaining down-sampled action motion sequence $\mathbf{a}'$ (downsampled to 1 FPS in our setting), which serves as a semantic hint for the matching process while not introducing too much emphasis on input action sequence.
The final evaluation model consists of an masked interaction encoder and a text encoder.
We use contrastive loss following CLIP~\citep{clip}, which encourages paired motion and text features to be close geometrically.
In addition, we add a classification head after the predicted motion features, to simultaneously predict interaction labels, such as ``high-five''.

\textbf{Baselines.} To evaluate the performance of our method \ModelAbbr~on online and unconstrained setting, we compare \ModelAbbr~with the following baselines:
1) \textbf{InterFormer}~\citep{interformer} is a transformer based action-to-reaction generation model that leverages human skeleton as prior knowledge for efficient attention process.
2) \textbf{MotionGPT}~\citep{motiongpt} is a motion-language model that leverages an LLM for motion and text generation. We extend the motion tokenizer of MotionGPT to encode multi-person motion, while keeping other settings unchanged.
3) \textbf{InterGen}~\citep{intergen} proposes a mutual attention mechanism within diffusion process for human interaction generation, we reproduce and adapt IngerGen to action-to-reaction generation.
4) \textbf{ReGenNet}~\citep{regennet} is latest state-of-the-art model on action-to-reaction generation. It adopts a transformer decoder based diffusion model, which directly predicts human reaction given action input in unconstrained and online manner as ours.


\textbf{Implementation Details.}
For the LLM, we adopt Flan-T5-base~\citep{flan,t5} as our base model, with extended vocabulary. We warm up the learning rate for 1,000 steps, peaking at 1e-4 for the pre-training phase, and use the same learning rate for fine-tuning.
Both the pre-training and fine-tuning phases are trained on a single machine with 8 Tesla V100 GPUs. The training batch size is set to 32 for the LLM and we monitor the validation loss and reaction generation metrics for early-stopping, resulting about 100K pre-training steps and 40K fine-tuning steps.
We set the re-thinking interval $N_r$ to 4 tokens and divide each space signal into $N_b=10$ bins.

\begin{table}[t]
\centering
\tiny
\caption{Comparison to state-of-the-art baselines and ablation studies of our method on Inter-X dataset. $\uparrow$ or $\downarrow$ denotes a higher or lower value is better, and $\rightarrow$ means that the value closer to real is better. We use $\pm$ to represent 95\% confidence interval and highlight the best results in \textbf{bold}. For ablation methods (in grey), PT, M, P, S, and SP are abbreviations for pre-training, motion, pose, space, and single-person data, respectively.}
\label{tab:main}
\begin{tabular}{l|ccccccc}
\toprule
\multirow{2}{*}{Methods} &  \multicolumn{3}{c}{R-Precision$\uparrow$} & \multirow{2}{*}{Acc.$\uparrow$}& \multirow{2}{*}{FID$\downarrow$}         & \multirow{2}{*}{MMDist$\downarrow$} & \multirow{2}{*}{Div.$\rightarrow$} \\
               & Top-1       & Top-2       & Top-3     &     &         &                               &                       \\ \midrule
Real                    & $0.511^{\pm.003}$ & $0.682^{\pm.002}$ & $0.776^{\pm.002}$ & $0.463^{\pm.000}$  & $0.000^{\pm.000}$         & $5.348^{\pm.002}$         & $2.498^{\pm.005}$           \\ \midrule
InterFormer             & $0.172^{\pm.012}$ & $0.292^{\pm.013}$ & $0.343^{\pm.012}$ & $0.171^{\pm.009}$ & $10.468^{\pm.021}$        &  $7.831^{\pm.018}$         & $3.505^{\pm.023}$           \\
MotionGPT &            $0.238^{\pm.003}$        &     $0.354^{\pm.004}$        &     $0.441^{\pm.003}$   &    $0.186^{\pm.002}$            &     $5.823^{\pm.048}$               &      $6.211^{\pm.005}$           &      $2.615^{\pm.007}$ \\
InterGen                & $0.326^{\pm.036}$ & $0.423^{\pm.063}$ & $0.525^{\pm.053}$ & $0.254^{\pm.019}$  & $5.506^{\pm.257}$         & $6.182^{\pm.038}$         & $2.284^{\pm.009}$           \\
ReGenNet                & $0.384^{\pm.005}$ & $0.483^{\pm.002}$ & $0.572^{\pm.003}$ & $0.297^{\pm.004}$  & $3.988^{\pm.048}$         & $5.867^{\pm.009}$         & $\mathbf{2.502^{\pm.001}}$           \\ \midrule
% \rowcolor[HTML]{EFEFEF}
\ModelAbbr~(Ours)       & $\mathbf{0.423^{\pm.005}}$ & $\mathbf{0.599^{\pm.003}}$ & $\mathbf{0.693^{\pm.003}}$ & $\mathbf{0.318^{\pm.003}}$  & $\mathbf{1.942^{\pm.017}}$         & $\mathbf{5.643^{\pm.003}}$         & $2.629^{\pm.006}$           \\
\rowcolor[HTML]{EFEFEF}
w/o Think           & $0.367^{\pm.003}$ & $0.491^{\pm.027}$ & $0.584^{\pm.008}$ & $0.230^{\pm.036}$ & $3.828^{\pm.016}$         & $6.186^{\pm.055}$         & $2.609^{\pm.006}$           \\
\rowcolor[HTML]{EFEFEF}
w/o All PT.         & $0.398^{\pm.007}$ & $0.531^{\pm.002}$ & $0.628^{\pm.003}$ & $0.288^{\pm.002}$ & $3.467^{\pm.113}$         & $5.822^{\pm.003}$         & $2.909^{\pm.053}$           \\
\rowcolor[HTML]{EFEFEF}
w/o M-M PT. & $0.408^{\pm.005}$ & $0.563^{\pm.004}$ & $0.646^{\pm.005}$ & $0.293^{\pm.002}$ & $2.874^{\pm.020}$         & $5.736^{\pm.003}$         & $2.553^{\pm.006}$           \\
\rowcolor[HTML]{EFEFEF}
w/o P-S PT. & $0.417^{\pm.004}$ & $0.582^{\pm.004}$ & $0.664^{\pm.004}$ & $0.308^{\pm.003}$ & $2.685^{\pm.024}$         & $5.699^{\pm.004}$         & $2.859^{\pm.007}$           \\
\rowcolor[HTML]{EFEFEF}
w/o M-T PT. & $0.406^{\pm.003}$ & $0.557^{\pm.004}$ & $0.637^{\pm.004}$ & $0.304^{\pm.003}$ & $2.580^{\pm.021}$         & $5.822 ^{\pm.003}$         & $2.889^{\pm.005}$           \\
\rowcolor[HTML]{EFEFEF}
w/o SP Data     & $0.414^{\pm.004}$ & $0.592^{\pm.005}$ & $0.685^{\pm.003}$ & $0.315^{\pm.004}$ & $2.007^{\pm.015}$         & $5.667^{\pm.003}$         & $2.611^{\pm.005}$           \\
\bottomrule
\end{tabular}
\end{table}



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/tsne.pdf}
    \caption{Visualization of a person's motion sequences in Inter-X dataset and HumanML3D dataset.}
    \label{fig:tsne}
\end{figure}

\subsection{Comparison to Baselines}\label{sec:sota}
As shown in the upper side of Table~\ref{tab:main}, our method \ModelAbbr~significantly outperforms baseline methods in terms of ranking, accuracy, FID and multimodal distance, showing superior human reaction generation quality.
Compared to MotionGPT, which adopts a similar motion-language architecture, \ModelAbbr~expresses stronger performance, which we attribute to our unified representation of motion via space and pose tokenizers, enabling effective individual pose and inter-person spatial relationship representation.
\ModelAbbr~also surpasses the diffusion-based methods, InterGen and ReGenNet, with our think-then-react architecture, improving generated motions by describing observed action and reasoning what reaction is expected on semantic level. In addition, ReGenNet and MotionGPT get closer diversity to the real than our model. We mainly attribute to that, \ModelAbbr~may conduct multiple re-thinking processes during inference, and the inferred semantics may bring a higher diversity.


\subsection{Ablation Study of Key Components}
To evaluate the effectiveness of our proposed key designs, we conduct detailed ablation studies by removing each of them to observe how much drop compared to the full version of our \ModelAbbr~method. The larger drop indicates more contribution. The results are shown in gray lines of Table~\ref{tab:main}. According to the drops in FID, all designs, including thinking, pre-training tasks and using single person data in pre-training, have positive contributions to the final performance, and thinking contributes the most. Some detailed findings and analyses are as follows.

First, we skip \textbf{thinking} stage during inference, and find the performance drops significantly in FID from 1.9 to 3.8. This supports the necessity of our proposed thinking process before reacting. We also notice decreasing diversity of generated samples, as the model relies solely on input action, and cannot explicitly capture and infer action's intent, thus leading to more rigid motion in some cases.

Second, to evaluate the effectiveness of \textbf{pre-training}, we omit the pre-training stage, and directly train our model \ModelAbbr~for thinking and reacting tasks. As shown in Table~\ref{tab:main}, our model's performance deteriorates without a fine-grained pre-training phase from 1.9 to 3.4 in FID. This indicates that pre-training can effectively adapt a language model (Flan-T5-base) into a motion and language model. We further removing three kinds of pre-training tasks: motion-motion (M-M PT.), pose-space (P-S PT.), and motion-text (M-T PT.). The results show that the without any task, the performance obviously gets worse, from 1.9 to 2.5 - 2.8 in FID, indicating their positive contribution to the final performance and complementary values to each other.

Third, to see how much \textbf{single-person data} helps reaction generation, we remove single person motion-text data, i.e., the data from HumanML3D dataset, from our training set. The result (w/o SP Data) shows that the model performs worse without training on HumanML3D, which proves that our unified motion encoder and motion-language architecture can leverage both single- and multi-person data, alleviating the insufficiency of training data. However, the benefit from single-person data is not as large as we expect. 

% What's more, we evaluate the necessity of \textbf{decoupled space-motion tokenizer}, and the results are shown in Table~\ref{tab:vqvae}. We design a plain motion VQ-VAE with unnormalized action and reaction as input, maintaining absolute space and pose features. With the trained motion VQ-VAE, we encode action/reaction into tokens, which are then fed into TTR for reaction prediction task. First, without normalized motion as input, the reconstruction FID significantly rises from 0.262 to 0.983, showing deteriorated reconstruction performance due to insufficient utilization of codebook. Second, in the reaction generation phase, TTR's performance drops dramatically, as the badly constructed codebook leads to inaccurate action understanding and reaction prediction, highlighting the necessity of decoupling token representation of space and pose features in multi-person scenario.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/case_study.pdf}
    \caption{Visualized cases of our predicted reactions (in green) to input action (in blue) and corresponding thinking results. We also provide a failure case in figure (d), where TTR misunderstands the input action as ``wrestling'', which should be ``embracing''.}
    \label{fig:case_study}
\end{figure}


\subsection{Analysis on Overlapping between Single- and Multi-Person Motions}
To investigate the reason of small contribution from single-person data, we further visualize motion sequences of single-person motion (HumanML3D), two-person action (Inter-X Action) and reaction (Inter-X Reaction) in the same space, as presented in Figure~\ref{fig:tsne}. Specifically, we use t-SNE tool~\cite{tsne} to project motion token sequence features into two-dimension. As shown in Figure~\ref{fig:tsne}, the single- and two-person motion sequences have little overlap. When doing case studies, we find that most two-person motion are unique, e.g., massage and being pulled, and will never be used in single-person motion. Similarly, most single-person motions are unique too, e.g., T-pose, and seldom appear in multi-person interaction. There are only a few overlapped motions, e.g., standing still. In addition, when comparing action and reaction sequences in multi-person interaction, we have some interesting findings. When reactions are close to actions, the motion usually belongs to symmetrical interactions, e.g., pulling or being pulled; whereas, when actions are far from reactions, the motion usually belongs to asymmetrical interaction, e.g., massage.


\subsection{Impact of Down-Sampling Parameter in Matching Model for Evaluation}

As described in Section~\ref{sec:eval}, we propose downsampling action motion sequence to avoid matching models for evaluation pay too much attention to input action rather than output reaction. We conduct an experiment to change the downsampling parameter frame rate and calculate the difference between taking ground-truth action and random action as the input of $\mathcal{M}$, in terms of summed ranking scores (Top-1, Top-2, Top-3 and Acc.). As presented in Figure~\ref{fig:discriminative}, 
difference is lowest when FPS equals to 0, which meaning we only match generated reaction motion with text. It goes up to the peak when FPS equals 1 and quickly goes down to low values, even close to the lowest when FPS is about 15. This indicates that it is necessary to concatenate input action with generated reaction to compose a meaningful interaction in evaluation, otherwise the motion-text matching model cannot well recognize the interaction. However, only 1 FPS is enough. With larger FPS, the matching models will be disturbed by input action rather than the generated reaction. Thus, we choose 1 FPS, corresponding to the largest difference, as our final setting.

\subsection{Impact of Re-thinking Interval}
% Our aim is to generate real-time reaction online, and thus time interval is an important parameter to generation quality. 
We change the re-thinking interval $N_r$ from about 1 to 100 timesteps (about 0.1 to 10 seconds) and observe how it impacts generative quality measure FID. As shown in Figure~\ref{fig:latency}, FID falls down first until $N_r=4$ (about 0.5 second) and then continues rising up. This indicate that the best time interval is about 0.5 second. When the time interval is too short, our \ModelAbbr~model cannot get enough information to re-think what the input action means and will bring some randomness into predicting appropriate reaction. When the time interval gets too long, our \ModelAbbr~model give slow responses to the input action sequences and generates coarse-grained reaction.

We also evaluate the average inference time per step (AITS) with respect to the re-thinking interval. As shown in Figure~\ref{fig:latency}, the inference time significantly decreases as the re-thinking interval increases, eventually converging to approximately 10 milliseconds per step (100 FPS). In our setup, we opt to re-think every four steps, resulting in an inference time of less than 50 milliseconds, which meets the requirements for a real-time system.

\subsection{User Study}
To further evaluate our model qualitatively, we conduct a user study on TTR vs. the latest SOTA method ReGenNet, and the results are shown in Figure~\ref{fig:user_study}. We randomly sample 100 action sequences from Inter-X dataset, which are fed into TTR and ReGenNet to predict reactions, and ask four real human to choose the better ones. It can be seen that TTR surpasses ReGenNet on all the duration range, and the winning rate rises significantly when motion duration is longer. We mainly contribute this to our explicit thinking and re-thinking procedure, which ensures semantics matching and alleviates accumulated errors. 

\begin{figure}[t]
    \centering
    \begin{minipage}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/discriminative_power.pdf}
        \caption{Impact of input action FPS to summed ranking score differences.}
        \label{fig:discriminative}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/latency.pdf}
        \caption{Impact of re-thinking interval to FID and average inference time per step (AITS).}
        \label{fig:latency}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/user_study.pdf}
        \caption{User preference between TTR and ReGenNet on different motion duration.}
        \label{fig:user_study}
    \end{minipage}
\end{figure}
