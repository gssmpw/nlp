\section{Appendix}

\subsection{Motion Representation and Pose Tokenizer}\label{sec:app:vqvae}
For motion representation, we use the same strategy as\cite{intergen}, which combines local joint positions, rotations, velocities, and foot-ground contact as the feature of human motion.
Regarding the tokenizers, we adopt a temporal down-sample rate of four and $N_p=256$ for motion tokens, each motion token are in $d_p=512$ in the codebook. We divide all space tokens into $N_b=10$ bins. The motion VQ-VAE is trained for 150K steps with batch size set to 256 and learning rate fixed at 1e-4 on a single Tesla V100 GPU. 
We adopt a similar architecture to~\cite{humanml3d} as our pose tokenizer. The encoder/decoder consists of two down-sample/up-sample 1D convolution layers and three 1D ResNet blocks~\cite{resnet}.
We set the width of the auto-encoder to 512.
We train the model on both the Inter-X and HumanML3D datasets for 200,000 steps, with batch size set to 256, and learning rate set to 1e-4.
We apply L1-loss on both pose feature and velocity reconstruction, and a commitment loss for the embedding process.
The weight set to velocity loss is 0.5 and commitment loss is 0.02.

\subsection{Matching Model}\label{sec:app:matching}
For the motion-text matching model, we adopt a similar architecture to InterCLIP~\citep{intergen}, which consists of an eight-layer motion transformer encoder and an eight-layer text transformer encoder. The hidden size is set to 768 and attention heads is set to 8. We add a learnable token to the motion encoder and extract its feature in the last layer of motion encoder as the pooled motion feature.
To perform motion classification, we add a classification head (an MLP) after the pooled motion feature.
We use the text embedding layer from clip-vit-large-patch14~\citep{clip}, which is frozen during training. We train the model for 40 epochs with batch size set to 128. The learning rate is warmed-up to 0.001 in the first 1,000 steps.

\subsection{Evaluation on Motion Captioning Task}\label{sec:app:m2t}
\begin{table}[!h]
\centering
\caption{Motion captioning results on Inter-X dataset. TTR$^\ast$ denotes feeding both action and reaction motion into TTR for captioning. TTR ($x\%$) denotes only the first $x\%$ of action motion is fed into TTR for captioning.}
\label{tab:thinking}
\begin{tabular}{l|cccc|cccc}
\toprule
       & RAEs & SeqGan & Seq2Seq & TM2T & TTR* & TTR  & TTR (50\%) & TTR (25\%) \\ \midrule
Bleu-1 & 28.6 & 45.4   & 53.8    & 56.8 & \textbf{60.2} & 55.6 & 54.1       & 52.2       \\
Bleu-4 & 9.7  & 14.1   & 18.5    & 21.6 & \textbf{25.4} & 20.3 & 18.9       & 16.6       \\
Rouge  & 34.1 & 36.8   & 45.2    & 48.2 & \textbf{50.5} & 46.4 & 45.3       & 43.0       \\ \bottomrule
\end{tabular}
\end{table}
We also evaluate our TTR model on motion captioning task, and the results are shown in Figure~\ref{tab:thinking}. The results of baselines are from Inter-X paper Section A.1. As the baseline methods all take both action and reaction as input, while in our setting, our thinking process is only accessible to ground-truth action, we first align TTR's setting with the baselines', denoted as TTR$^\ast$. It can be seen that, with our fine-grained training and effective motion representation, TTR$^\ast$ achieves the best captioning performance in all metrics.

Then we evaluate TTR on real-world settings, i.e., only partial of the input action is visible to our model. We take the first 25\%, 50\% and full action as input of TTR for the action-to-text generation process. It can be seen that even though only a quarter of input action is given, TTR is still capable of accurately predicting the corresponding action and reaction description, showcasing strong generalization capability. 

\subsection{Ablation study on Thinking process}
\begin{table}[!h]
\centering
\small
\caption{Ablation study on how does thinking process influence model performance. GT denotes ground-truth, and Thinking$^\ast$ denotes using a better motion-to-text model for the thinking process.}
\label{tab:thinking}
\begin{tabular}{l|ccc}
\toprule
Methods      & FID               & Top-1             & Acc.              \\ \midrule
w/ GT Prompt    & $1.584^{\pm.016}$ & $0.458^{\pm.005}$ & $0.361^{\pm.005}$ \\
w/ Thinking$^\ast$ & $1.882^{\pm.014}$ & $0.429^{\pm.004}$ & $0.331^{\pm.003}$ \\
w/ Thinking & $1.942^{\pm.017}$ & $0.423^{\pm.005}$ & $0.318^{\pm.003}$ \\
w/o Thinking & $3.828^{\pm.016}$ & $0.367^{\pm.003}$ & $0.230^{\pm.036}$ \\
\midrule
\end{tabular}
\end{table}

To evaluate the necessity of the Thinking process, we conduct an ablation study on different prompts provided to the Reacting process. First we fed ground-truth prompt to the Thinking process, and it can be seen that the overall quality of predicted reaction is significantly improved. Then we leverage a enhanced Thinking model as mentioned in Section~\ref{sec:app:m2t}, and the FID decreases from 1.94 to 1.88, proving that a better thinking process leads could promote the following Reacting process. Moreover, when discarding the Thinking process, our model dramatically deteriorates in reaction generation quality, as Thinking and re-thinking process is crucial to guide reaction generation and reduce accumulated errors.
