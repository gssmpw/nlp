\section{Related Work}
Conversational agents (chatbots) have been an active area of research for a long time.
Rule-based or finite-state-based systems, like Eliza \cite{eliza}, Chat-80 \cite{chat80}, and PARRY \cite{parry}, encode the mapping of user commands to ontology using rules and state transitions to solve the Dialogue State Tracing (DST) task.
The Conversational Knowledge Template (CKT) \cite{ckt} enables the system to control the dialog flow and change topics. 

Until recently, transformer-based Large Language Models, pre-trained on an enormous quantity of well-annotated data, have been applied to general NLP tasks. With the advent of Large Language Models, the paradigm changed from pre-training and fine-tuning \cite{pre_trained_transformers} to teaching a language model any arbitrary task using just a few demonstrations, called \textit{in-context learning}, a method of \textit{prompt engineering}. \cite{gpt3} introduced an LLM called GPT-3 containing approximately 175 billion parameters that have been trained on a massive corpus of filtered online text, on which the well-known ChatGPT is based.

GPT-3 and its successor GPT-4 \cite{gpt4} can perform competitively on several tasks such as question-answering, semantic parsing, and machine translation. However, such LLMs lack the ability of mathematical reasoning and find it hard to overcome the hallucination brought from the training data \cite{gpt3-scope,chain,chatgpt-critic}.  

Retrieval Augmented Generation (RAG) \cite{rag} is proposed and widely used to mitigate the deficiencies mentioned above by retrieving the relevant materials using similarity matching of content embedded as vectors by a transformer-based model.
Recent efforts \cite{ragchat1,ragchat2} are trying to leverage RAG for building chatbots, but none of them engages an explicit reasoning system.

Finally, this research is an extension of our previous work developing NLU systems based on commonsense reasoning \cite{ckt,rcc,concierge}. Our group has been dedicated to building socialbots, specifically addressing Amazon's Alexa Socialbot Challenge \cite{alexa} for years. GPT-4 with in-context learning as a semantic parser leads to a significant advantage over our previous socialbots and helped this framework succeed.