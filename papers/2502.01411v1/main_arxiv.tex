\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{xcolor}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\usepackage[accepted]{arxiv2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{colortbl}

% if you use cleveref..


%% narrow the gap between equations and sentences
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
\setlength{\abovedisplayshortskip}{1pt}
\setlength{\belowdisplayshortskip}{1pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[

\icmltitle{Human Body Restoration with \\One-Step Diffusion Model and A New Benchmark}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jue Gong}{equal,sjtu}
\icmlauthor{Jingkai Wang}{equal,sjtu}
\icmlauthor{Zheng Chen}{sjtu}
\icmlauthor{Xing Liu}{vivo}
\icmlauthor{Hong Gu}{vivo}
\icmlauthor{Yulun Zhang$^{\dagger}$}{sjtu}
\icmlauthor{Xiaokang Yang}{sjtu}

\end{icmlauthorlist}

\icmlaffiliation{sjtu}{Shanghai Jiao Tong University, China}
\icmlaffiliation{vivo}{vivo Mobile Communication Co., Ltd, China}

\icmlcorrespondingauthor{$^{\dagger}$Yulun Zhang}{yulun100@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
Human body restoration, as a specific application of image restoration, is widely applied in practice and plays a vital role across diverse fields. However, thorough research remains difficult, particularly due to the lack of benchmark datasets. In this study, we propose a high-quality dataset automated cropping and filtering (HQ-ACF) pipeline. This pipeline leverages existing object detection datasets and other unlabeled images to automatically crop and filter high-quality human images. Using this pipeline, we constructed a person-based restoration with sophisticated objects and natural activities (\emph{PERSONA}) dataset, which includes training, validation, and test sets. The dataset significantly surpasses other human-related datasets in both quality and content richness. Finally, we propose \emph{OSDHuman}, a novel one-step diffusion model for human body restoration. Specifically, we propose a high-fidelity image embedder (HFIE) as the prompt generator to better guide the model with low-quality human image information, effectively avoiding misleading prompts. Experimental results show that OSDHuman outperforms existing methods in both visual quality and quantitative metrics. The dataset and code will at \url{https://github.com/gobunu/OSDHuman}. 

\vspace{-2mm}
\end{abstract}
\begin{figure}[t]
\begin{center}

\includegraphics[width=1.0\columnwidth]{figs/dataset.pdf}

\end{center}
\vspace{-5mm}
\caption{Comparison of no-reference image quality assessment metrics across human-related datasets. The object detection datasets specifically evaluate subsets with humans. Our proposed PERSONA dataset outperforms others significantly.}
\label{fig:dataset_iqa}
\vspace{-5mm}
\end{figure}
\vspace{-5mm}

\section{Introduction}
Human body restoration (HBR) aims to recover high-quality (HQ) images from low-quality (LQ) inputs featuring human figures. Unlike nature scene pictures, human figures naturally attract viewers’ attention in images. However, real-world images often suffer from degradation during capture and transmission, such as blur, noise,  resolution reduction, and JPEG artifacts. These distortions severely impact the recognition of human activities and the extraction of information from the image. Furthermore, degraded images make other human-related downstream tasks more challenging, such as human-object interaction detection~\cite{wang2024hoidreview}, human pose estimation~\cite{Samkari2023HPEreview}, and 3D reconstruction~\cite{wang20213Dreview}.

Despite its practical significance, progress in HBR remains constrained, primarily due to the absence of task-specific benchmark datasets. In natural scenarios, humans exhibit a wide range of activities and complex interactions with their surroundings. Therefore, the benchmark dataset for HBR must be large, cover complex scenarios, and include natural activities. Datasets in the fashion domain, such as DeepFashion~\cite{liuL2016DeepFashion} and iDesigner~\cite{dufour2022idesigner}, focus on runway or studio scenarios. As a result, these datasets contain only limited types of human actions, making them unsuitable for HBR. Moreover, human images often involve multiple individuals interacting with each other, adding further complexity to the restoration task. Such complexity makes datasets focused on single-person image generation, such as SHHQ~\cite{fu2022shhq} and CosmicMan-HQ~\cite{li2024cosmicman}, less suitable for HBR, as they mainly focus on single-person images.

\begin{figure}[t]
\begin{center}
\footnotesize
\scalebox{1}{
    \hspace{-0.4cm}
    \begin{adjustbox}{valign=t}
    \begin{tabular}{cccc}
    \includegraphics[width=0.24\columnwidth]{figs/fig_page2_pasted/output_marked_LQ_voc_7345_2008_004471.png} \hspace{-4mm} &
    \includegraphics[width=0.24\columnwidth]{figs/fig_page2_pasted/output_marked_osediff_voc_7345_2008_004471.png} \hspace{-4mm} &
    \includegraphics[width=0.24\columnwidth]{figs/fig_page2_pasted/output_marked_sinsr_voc_7345_2008_004471.png} \hspace{-4mm} &
    \includegraphics[width=0.24\columnwidth]{figs/fig_page2_pasted/output_marked_reshift_voc_7345_2008_004471.png} \hspace{-4mm} 
    \vspace{-1mm} \\
    LQ (512$\times$512) \hspace{-4mm} &
    OSEDiff \hspace{-4mm} &
    SinSR \hspace{-4mm} &
    ResShift \hspace{-4mm} \\
    \includegraphics[width=0.24\columnwidth]{figs/fig_page2_pasted/output_marked_OSDHuman_voc_7345_2008_004471.png} \hspace{-4mm} &
    \includegraphics[width=0.24\columnwidth]{figs/fig_page2_pasted/output_marked_osediff_retrain_voc_7345_2008_004471.png} \hspace{-4mm} &
    \includegraphics[width=0.24\columnwidth]{figs/fig_page2_pasted/output_marked_sinsr-retrain_voc_7345_2008_004471.png} \hspace{-4mm} &
    \includegraphics[width=0.24\columnwidth]{figs/fig_page2_pasted/output_marked_reshift-retrain_voc_7345_2008_004471.png} \hspace{-4mm}
    \vspace{-1mm} \\
    OSDHuman \hspace{-4mm} &
    OSEDiff* \hspace{-4mm} &
    Sinsr* \hspace{-4mm} &
    ResShift* \hspace{-4mm} \vspace{-1mm}\\
    \footnotesize (ours) \hspace{-4mm} & 
    \scriptsize \cite{wu2024osediff} \hspace{-4mm} & 
    \scriptsize \cite{wang2024sinsr} \hspace{-4mm} & 
    \scriptsize \cite{yue2023resshift} \hspace{-4mm} 
    \\ 
    \end{tabular}
    \end{adjustbox}
}
\end{center}
\vspace{-5.5mm}
\caption{Visual examples of diffusion-based image restoration methods evaluated on PERSONA-test. The asterisk (*) indicates methods retrained on PERSONA dataset. Our OSDHuman produces more natural and faithful visual results compared to others.}
\label{fig:page2_compare}
\vspace{-7mm}
\end{figure}

Additionally, an HBR model can achieve optimal performance only when trained on sufficiently high-quality datasets. Degraded datasets could cause bias in the model's weights, as it is difficult to distinguish between degradation and features in LQ images. Some existing image restoration datasets, such as LSDIR~\cite{Li2023LSDIR} and DIV2K~\cite{agustsson2017div2k}, are of high quality and cover complex real-world scenarios. However, the proportion of human images in these datasets is small, and they are not specifically tailored for human images. Other human-related high-level datasets, such as those for object detection~\cite{Kuznetsova2020OpenImages,shao2019object365} and keypoint detection~\cite{lin2014coco}, have a wider range of human activities and sophisticated surrounding objects due to the diversity of image sources. However, as illustrated in Fig.~\ref{fig:dataset_iqa}, these datasets lack dedicated quality filtering, containing substantial LQ samples.

However, even with a high-quality benchmark, achieving excellent HBR performance still requires a well-designed model architecture. In recent studies, image restoration models with latent diffusion model (LDM)~\cite{Rombach2022LDM} architecture achieve promising results due to powerful generative capabilities. These models combine generation and restoration to reconstruct lost parts of LQ images using the features provided. They primarily use two main latent space mapping methods: variational autoencoder (VAE)~\cite{kingma2014vae} and vector quantized VAE (VQVAE)~\cite{oord2017vqvae}. As shown in Fig.~\ref{fig:page2_compare}, models with VQVAE, such as ResShift~\cite{yue2023resshift} and SinSR~\cite{wang2024sinsr}, often produce distorted structures in detail. Even with retraining, these issues are difficult to resolve due to the VQVAE codebook's inability to capture the fine details of the human body. On the other hand, models with VAE, like OSEDiff~\cite{wu2024osediff}, have better generalization and detail generation capabilities, but they are still not specifically optimized for HBR. 

While multi-step diffusion models have strong restoration abilities for LQ images, they often require substantial computational resources, which limits their applicability. To reduce resource consumption, one-step diffusion (OSD) models are proposed and achieve good results. By leveraging large-scale pretrained text-to-image (T2I) models~\cite{saharia2022photo,Rombach2022LDM} as foundation models, OSD models combine generation power with fast inference. Therefore, OSD models are highly competitive in HBR. However, this also requires the model to incorporate an appropriate prompt extractor that can derive a high-fidelity prompt from complex human images. Otherwise, the resulting prompt could mislead the restoration of the model.

To address the limitations, we propose OSDHuman, a novel OSD model for HBR. \textbf{Firstly}, to overcome the lack of benchmark datasets in HBR, we propose a high-quality dataset automated cropping and filtering (HQ-ACF) pipeline. This pipeline preprocesses both labeled and unlabeled datasets to isolate images containing humans. Then it refines human bounding boxes and crops them accordingly. Using no-reference image quality assessment (IQA) metrics, we ultimately produce a dataset. \textbf{Secondly}, leveraging HQ-ACF, we develop a person-based restoration with sophisticated objects and natural activities (PERSONA) dataset, which comprises 109,052 HQ 512$\times$512 human images for training. This pipeline also provides images for validation and testing. The PERSONA dataset includes both individual-environment interactions and multi-person interactions, averaging 3.4154 individuals per image. \textbf{Thirdly}, to provide prompts suitable for HBR, we propose a high-fidelity image embedder (HFIE). HFIE uses an image encoder from RAM~\cite{zhang2023RAM} and a multi-head attention layer with a learnable embedding as the query. This design avoids distortions introduced by tags when summarizing images, thereby preventing misleading prompts that could impair model restoration. In addition, we employ a variational score distillation (VSD) regularizer to guide the model’s generative distribution toward natural image distributions. 

\begin{figure*}[t]
\begin{center}

\includegraphics[width=1.0\textwidth]{figs/pipeline.pdf}

\end{center}
\vspace{-5mm}
\caption{High-quality dataset automated cropping and filtering pipeline. The pipeline consists of four stages. \textbf{First}, multiple datasets are collected, comprising millions of images. Images without labels are processed using YOLO11 for human detection. \textbf{Then}, a Laplacian operator is applied to compute image Laplacian variance, filtering out images below a threshold. \textbf{Next}, human boxes are adjusted to the square shape, and overly small or densely packed boxes are removed. \textbf{Finally}, cropped human images are evaluated using Image Quality Assessment (IQA) metrics. Images ranking in the top third by normalized metrics and exceeding the metric threshold are selected. These 109,052 images constitute the person-based restoration with sophisticated objects and natural activities (PERSONA) dataset.}
\label{fig:dataset pipeline}
\vspace{-4mm}
\end{figure*}

Our contributions can be summarized as follows.
\begin{itemize}
\item We propose a person-based restoration with sophisticated objects and natural activities (PERSONA) dataset which provides a benchmark for human body restoration, encompassing training, validation, and test sets.
\item Our PERSONA dataset surpasses other human-related datasets in quality and includes a wide range of scenarios that cover the majority of human activities.
\item We propose OSDHuman, an innovative one-step diffusion model for human body restoration. It features a high-fidelity image embedder (HFIE) designed for extracting suitable prompts from human images.
\item Our OSDHuman achieves strong SOTA human body restoration performance, excelling in visual quality and metrics while maintaining lower computational costs.
\end{itemize}



\section{Related Works}
\vspace{-1mm}
\subsection{Human Body restoration}
\vspace{-1mm}
Human body restoration could benefit both the fashion industry for display and the photography and camera producers. The purpose is evident, focusing on the human body and making it look better. Compared to general image super-resolution (SR) for arbitrary objects, human body image SR is more constrained, allowing for the use of a variety of prior knowledge. Firstly, current research on image segmentation has made significant progress in generating segmentation masks for different body parts, such as hair and arms, which provide a clear description of the human body shape. This knowledge is beneficial for handling the boundaries in the human body SR tasks. Since the human body composition is relatively fixed and limited, it is easier to estimate compared to the random and arbitrary objects in natural images. 

Lots of research has been done recently. \cite{Liu2021HumanBodySR} represent the texture details of the human body using subbands of the non-subsampled shearlet transform, while \cite{Wang2024PRCN} employ a pyramid residual network to estimate texture and shape priors, improving the image quality of the human body. \cite{Zhang2024DiffBody}, as the first to apply diffusion models for enhancing body-region images, use pose-attention guidance, text guidance, and a body-centered diffusion sampler to integrate various semantic information for high-quality body-region enhancement.

\vspace{-1mm}
\subsection{Diffusion Models}
\vspace{-1mm}
Since the diffusion model was released and popular, many efforts have been made. Two classical applications are image restoration and text-to-image (T2I). Image restoration, as the first and most natural application, has been developed a lot~\cite{
saharia2021image,whang2021deblurringstochasticrefinement,Avrahami_2022_CVPR,chen2023image,xia2023diffir}. With the development of conditional diffusion models~\cite{Rombach2022LDM}, numerous companies have invested heavily in training more powerful T2I models. Recently, many efforts have been made to integrate these two typical applications. The rapidly developing Stable Diffusion~\cite{Rombach2022LDM}, DALLE~\cite{pmlr-v139-ramesh21a}, and PixArt~\cite{chen2024pixartalpha} have continually pushed the boundaries of realism and diversity in T2I generation. Many image restoration methods have also leveraged pretrained models to achieve more natural image recovery~\cite{wu2024seesr,yang2023pasd,lin2024diffbir,wu2024osediff,wang2024osdface}.

Stricted on the multi-timesteps in the diffusion inference procedure, the diffusion models with 50 or more steps~\cite{wang2024exploiting,lin2024diffbir,wu2024seesr,yang2023pasd} cannot actually be used in practice. Many efforts have been made to faster diffusion models, such as cutting, quantizing, and compressing. Moreover, eliminating the number of inference timesteps is a convincing way, especially applied in image restoration. SinSR~\cite{wang2024sinsr} pioneers one-step inference for diffusion-based SR by distilling deterministic generation functions into a student network, coupled with a consistency-preserving loss and efficient training pair generation strategy. OSEDiff~\cite{wu2024osediff} adapts pretrained SD models for Real-ISR through LoRA-finetuned U-Net and variational score distillation, enabling direct low-quality image reconstruction in one step without noise injection. Those methods achieve a fascinating performance in natural image restoration. 

\begin{figure*}[t]
\begin{center}

\includegraphics[width=0.95\textwidth]{figs/ICML-Model.pdf}

\end{center}
\vspace{-5mm}
\caption{Training Framework of OSDHuman. \textbf{First}, the LQ image $I_L$ is processed through the VAE Encoder, U-Net, and VAE Decoder, ultimately producing the restored HQ image $\hat{I}_H$. The conditional input of the U-Net is provided by the high-fidelity image embedder (HFIE). \textbf{Second}, during the training process, the $\hat{z}_H$ generated by the U-Net is subjected to noise and then passed through the pretrained and finetuned regularizers. $\mathcal{L}_{\text{VSD}}$ represents the distribution's difference between the model output and the natural image. $\mathcal{L}_{\text{VSD}}$, together with $\mathcal{L}_{\text{LPIPS}}$ and $\mathcal{L}_{\text{MSE}}$, constitutes the training objective. \textbf{In summary}, during the training stage, the VAE Encoder, U-Net, and finetuned regularizer are trained with LoRA, while other modules remain frozen. During inference, the VSD module is not utilized.
}

\vspace{-5mm}
\label{fig:model_architecture}
\end{figure*}

\vspace{-2mm}
\section{Methods}
\vspace{-1mm}
\subsection{High Quality Human Dataset Pipeline}
\vspace{-1mm}
For image restoration tasks, large-scale high-quality datasets are required to simulate various real-world scenarios and objects. There are already many high-quality image restoration datasets, such as FFHQ~\cite{karras2019ffhq} and LSDIR~\cite{Li2023LSDIR}. However, these datasets are not ideally suited for the human body restoration (HBR) task due to their lack of focus on human-specific features. Moreover, the dataset should enable models to adapt to real-world environments' complexities. It must encompass most scenarios including interactions among people and between people and their surroundings. Therefore, we propose a high-quality dataset automated cropping and filtering (HQ-ACF) pipeline for HBR datasets, as well as a person-based restoration with sophisticated objects and natural activities (PERSONA) dataset.


\textbf{Automated Cropping and Filtering Pipeline.} As illustrated in Fig.~\ref{fig:dataset pipeline}, we first collect a series of commonly used and publicly available large-scale object detection datasets, including COCO~\cite{lin2014coco}, OID~\cite{Kuznetsova2020OpenImages, Ivan2017OpenImages2}, Object365~\cite{shao2019object365} and CrowdHuman~\cite{shao2018crowdhuman}, comprising approximately 4 million images. We then filter the images by labels, selecting those containing ``human" or synonymous labels, such as ``Human Body" in OID and ``person" in Object365. To further refine the selection, we conduct human detection on the image restoration dataset LSDIR~\cite{Li2023LSDIR}, using the YOLO11 model~\cite{yolo11_ultralytics} for processing. This operation resulted in bounding boxes similar to those in object detection datasets.

Next, we apply the Laplacian operation to these datasets and compute the variance of the results. Images with a variance below the threshold are discarded, as these correspond to images with a high degree of blurriness. Before cropping, we also check the size of the human bounding boxes. Images with low-resolution human bodies are rejected. After these steps, we use the bounding boxes to crop the images. When cropping, the side length of the bounding box's longer edge is used as the side length of the cropping box, ensuring a square crop. In cases where an image contains multiple overlapping boxes, non-maximum suppression (NMS) is applied, prioritizing the box closest to the image center. The cropped images are then resized to 512$\times$512.

Finally, we obtain approximately 440,000 cropped images, on which we measure no-reference Image Quality Assessment (IQA) metrics. The metrics used are common in image restoration, including CLIPIQA~\cite{wang2022clipiqa}, MANIQA~\cite{yang2022maniqa}, MUSIQ~\cite{ke2021musiq}, and NIQE~\cite{zhang2015niqe}. To balance the evaluation performance of each metric, we normalize the obtained IQA metrics using the following standardization formula: 
\vspace{-1mm}
\begin{equation}
\text{Normalized Metrics} = \frac{1}{N} \sum_{i=1}^N\frac{M_i - \mu_i}{\sigma_i},
\end{equation}
where $\mu_i$ is the mean and $\sigma_i$ is the standard deviation of the metrics ($M_i$). Since NIQE is better when its score is smaller, we first apply a negative transformation to its values. The normalized scores are then accumulated per image and sorted. The final PERSONA dataset version consists of 109,052 images with normalized metrics in the top third, and each IQA metric must exceed a predefined threshold. 


\vspace{-1mm}
\subsection{One-Step Diffusion (OSD) Model}
\vspace{-1mm}

\textbf{Model Architecture Overview.} Most image restoration tasks with OSD models are extensively studied in previous works~\cite{wang2024sinsr,wu2024osediff,wang2024osdface,li2024dfosd}. However, these methods struggle to achieve desirable results in human body restoration (HBR). To address this limitation, we propose OSDHuman, an OSD model specifically designed for HBR. Specifically, we adopt a Stable Diffusion (SD) model architecture~\cite{Rombach2022LDM} by fixing the number of steps, thereby transforming it into an OSD framework. As shown in Fig.~\ref{fig:model_architecture}, the first step uses the variational autoencoder (VAE) encoder $E_\theta$ to project the low-quality (LQ) image $I_L$ into the latent space, resulting in $z_L = E_\theta(I_L)$. Subsequently, a single denoising operation $F_\theta$ is applied to estimate the noise, which is crucial for enabling the calculation of the predicted high-quality (HQ) latent vector $\hat{z}_H$ through the equation:
\vspace{-1mm}
\begin{equation} 
\hat{z}_H = F_\theta(z_L;p)=\frac{z_L - \sqrt{1 - \bar{\alpha}_{T_L}} \varepsilon_{\theta} (z_L; p, T_L)}{\sqrt{\bar{\alpha}_{T_L}}},
\label{eq:generate_zh} 
\end{equation}
where $\varepsilon_\theta$ represents the denoising network governed by the parameter $\theta$, $p$ is the output of the high-fidelity image embedder (HFIE), and $T_L$ refers to the diffusion timestep. A predefined parameter $T_L \in [0, T]$ is used as input to the U-Net, where $T$ signifies the total number of diffusion steps (e.g., $T = 1,000$ in SD). The VAE decoder $D_\theta$ is then employed to reconstruct the HQ image $\hat{I}_H$ from the predicted latent vector $\hat{z}_H$, expressed as $\hat{I}_H = D_\theta(\hat{z}_H)$.If the generator is denoted as $ \mathcal{G} $, the complete process can be summarized by the following equation:
\vspace{-1mm}
\begin{equation} 
\hat{I}_H = \mathcal{G}_\theta(I_L; p). 
\end{equation}
\vspace{-7mm}

\textbf{Training Objective.} During training, we utilize pixel-wise MSE loss and perceptual loss LPIPS~\cite{zhang2018lpips}. Additionally, the obtained \( \hat{z}_H \) is used to compute the variational score distillation (VSD) loss, ensuring alignment between the generated images and natural images. The final overall training objective for the generator $\mathcal{G}_\theta$ is the following. $\lambda_1$ and $\lambda_2$ are the weights for $\mathcal{L}_{\text{LPIPS}}$ and $\mathcal{L}_{\text{VSD}}$.
\vspace{-1mm}
\begin{equation} 
\begin{aligned}
\mathcal{L}_{\mathcal{G}_\theta} = \mathcal{L}_{\text{MSE}}(I_H, \hat{I}_H)
& + \lambda_1 \cdot \mathcal{L}_{\text{LPIPS}}(I_H, \hat{I}_H) \\
& + \lambda_2 \cdot \mathcal{L}_{\text{VSD}}(\hat{z}_H, p).
\end{aligned}
\label{eq:training_objective}
\end{equation}
\textbf{High-Fidelity Image Embedder.} The degradation level of LQ human images is generally significant, and the image content is often highly complex. It is necessary to consider the coordination between the human pose and the surrendering. Therefore, we propose an HFIE that can guide the restoration direction, reducing the feature gap between HQ and LQ human images. OSEDiff~\cite{wu2024osediff} employs a finetuned RAM~\cite{zhang2023RAM} as the degradation-aware prompt extractor (DAPE). It provides tags to guide the OSD model. However, the tags generated by DAPE are often too broad and imprecise for human images, offering insufficient and even biased guidance to the OSD model.

\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.95\columnwidth]{figs/ICML-fig5.pdf}
    \end{center}
    \vspace{-5mm}
    \caption{Comparison of the architectures of HFIE and DAPE.}
    \vspace{-4mm}
    \label{fig:comparison_extractor}
\end{figure}

As shown in Fig.~\ref{fig:comparison_extractor}, the RAM used in DAPE consists of two parts: the image encoder and the tagging head. However, HFIE only uses the image encoder (\(\mathcal{E}\)) in RAM, leveraging the Swin Transformer~\cite{liu2021Swin}, which downsamples the input LQ \(I'_L \in \mathbb{R}^{384 \times 384 \times 3}\) by a factor of 32. The $I'_L$ represents the resized LQ $I_L$. The image embeddings are \(x_L = \{x_{L,k} \in \mathbb{R}^{512}\}_{k=1}^{145}\), where the first 144 embeddings represent local information of images. The remaining 1 embedding is obtained through the average pooling of others, which contains the overall information of images. Therefore, using a linear layer to reduce the embeddings' size to match the input of SD (e.g., 77$\times$1024 for SD-2.1) would result in the loss of distinguish ability between overall and local information. Our proposed method HFIE uses learnable embeddings $Q$ as the query input to the multi-head attention (MHA) layer, ultimately producing the HFIE:
\vspace{-1mm}
\begin{equation} 
p = \text{HFIE}(I'_L) = \text{MHA}(Q,\mathcal{E}(I'_L),\mathcal{E}(I'_L)).
\end{equation}

\textbf{Variational Score Distillation (VSD).} Fine-tuning image restoration models often encounter challenges due to the limited training data available, especially when compared to large-scale foundation models like Stable Diffusion (SD). It leads to generated images that fail to align with the natural image space distribution. Previous studies~\cite{yin2023dmd,wang2024prolificdreamer,dao2024swiftbrushv2} propose some VSD methods to solve this problem by aligning the distributions represented by two diffusion models. Following OSEDiff~\cite{wu2024osediff}, we apply a VSD module in latent space to guide OSDHuman in learning the distribution of natural images from SD. The VSD loss is computed from the distribution gap in the latent space output by the pretrained regularizer $\epsilon_\phi$ and finetuned regularizer $\epsilon_{\phi'}$. The gradient of the VSD loss is defined as:
\vspace{-1mm}
\begin{equation}
\begin{aligned}
&\nabla_\theta \mathcal{L}_{\text{VSD}}(\hat{z}_H, p) = \nabla_{\hat{z}_H} \mathcal{L}_{\text{VSD}}(\hat{z}_H, p) \frac{\partial \hat{z}_H}{\partial \theta}\\
&= \mathop{\mathbb{E}}\limits_{t, \epsilon, \hat{z}_t} \left[ \frac{\epsilon_\phi(\hat{z}_t; t, p) - \epsilon_{\phi'}(\hat{z}_t; t, p)}{\text{mean}(||\epsilon_\phi(\hat{z}_t; t, p) - \hat{z}_H||)} \cdot \frac{\partial \hat{z}_H}{\partial \theta} \right],
\end{aligned}
\end{equation}
where $t$ is sampled from the range $[20, 980]$, $\varepsilon \sim \mathcal{N}(0, I)$ and $\hat{z}_t$ denotes the output after adding noise at timestep $t$. Besides, to ensure VSD working, the finetuned regularizer $\epsilon_{\phi'}$ needs to be trainable. Its training objective is:
\begin{equation}
\mathcal{L}_{\epsilon_{\phi'}} = \mathop{\mathbb{E}}\limits_{t, \epsilon, p, \hat{z}_H} \mathcal{L}_{\text{MSE}} \left( \epsilon_{\phi'} (\hat{z}_t; t, p), \epsilon \right).
\end{equation}



\begin{figure*}[t]

\scriptsize
\begin{center}
\scalebox{0.98}{
    \hspace{-0.4cm}
    \begin{tabular}{cccccccccc}
    \includegraphics[width=0.123\textwidth]{figs/PERSONA/1.jpg} \hspace{-1mm} 
    \includegraphics[width=0.123\textwidth]{figs/PERSONA/2.jpg} \hspace{-1mm} 
    \includegraphics[width=0.123\textwidth]{figs/PERSONA/10.jpg} \hspace{-1mm} 
    \includegraphics[width=0.123\textwidth]{figs/PERSONA/4.jpg} \hspace{-1mm} 
    \includegraphics[width=0.123\textwidth]{figs/PERSONA/5.jpg} \hspace{-1mm} 
    \includegraphics[width=0.123\textwidth]{figs/PERSONA/6.jpg} \hspace{-1mm} 
    \includegraphics[width=0.123\textwidth]{figs/PERSONA/7.jpg} \hspace{-1mm} 
    \includegraphics[width=0.123\textwidth]{figs/PERSONA/8.jpg} \hspace{-1mm} 

    \\

    \end{tabular}
}
\end{center}
\vspace{-5mm}
\caption{The PERSONA dataset consists of human images engaged in various natural activities, featuring diverse surrounding objects.}
\label{fig:vis-PERSONA}
\vspace{-5mm}
\end{figure*}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{figs/pie_chart.pdf}
    \vspace{-5mm} 
    \caption{The distribution of tags in the PERSONA dataset, identified by the Recognize Anything Plus Model~\cite{huang2023RAM++}. The angles of the pie chart represent the frequency of each tag category in the dataset, i.e., the tag count. The tag category count indicates how many tags are contained within each category. }

    \label{fig:dataset_distribution}
\vspace{-7mm}
\end{figure}

\vspace{-4mm}
\section{Experiments}
\vspace{-1mm}
\subsection{Quantitative Analysis of PERSONA Dataset}
\vspace{-1mm}
\textbf{High-Quality Dataset}. Our person-based restoration dataset with sophisticated objects and natural activities (PERSONA) dataset, consists of 109,052 human images with a resolution of 512$\times$512. These images are obtained through the high-quality automated cropping and filtering (HQ-ACF) pipeline. To quantify the quality of the PERSONA dataset, we evaluated a range of no-reference image quality assessment (IQA) scores, including CLIPIQA~\cite{wang2022clipiqa}, MANIQA~\cite{yang2022maniqa}, MUSIQ~\cite{ke2021musiq}, BRISQUE~\cite{anish2011BRISQUE}, and NIQE~\cite{zhang2015niqe}. We compare the results with those from other human-related datasets, including the object detection datasets OID~\cite{Kuznetsova2020OpenImages, Ivan2017OpenImages2}, VOC~\cite{Everingham2010voc}, COCO~\cite{lin2014coco}, Object365~\cite{shao2019object365}, CrowdHuman~\cite{shao2018crowdhuman}, and fashion domain datasets DeepFashion~\cite{liuL2016DeepFashion}, iDesigner~\cite{dufour2022idesigner}. As shown in Tab.~\ref{table:comparison_iqa}, our dataset consistently outperforms these datasets across all the no-reference IQA measures.

\textbf{Rich-Diversity Dataset.} We utilize the Recognize Anything Plus Model~\cite{huang2023RAM++} to obtain image understanding tags for our PERSONA dataset. The visual results are shown in Fig.~\ref{fig:dataset_distribution}. \textbf{In total}, 3,365 distinct tag categories are identified from the dataset. And the largest proportion belongs to the ``Objects" category, demonstrating the presence of sophisticated objects in our dataset.  \textbf{Additionally}, the dataset generates a total of 1,452,088 tags, with approximately half of these tags falling under the ``People", ``Sports", and ``Activities" categories. It indicates that the dataset is centered on natural human activities. As shown in Fig.~\ref{fig:vis-PERSONA}, most of the images relate to this theme. \textbf{Finally}, the average number of tags per image in the dataset is 13.32, highlighting the dataset's ease of understanding.

\begin{table}[t]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{1.6mm}
    \renewcommand{\arraystretch}{1.05}
    \newcolumntype{C}{>{\centering\arraybackslash}X}
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \begin{tabularx}{\columnwidth}{l|CCCCC} 
        \hline
          Dataset &  BRISQUE$\downarrow$ &  NIQE$\downarrow$ &  CLIPIQA$\uparrow$ &  MANIQA$\uparrow$ &  MUSIQ$\uparrow$ \\
        \hline \hline        \\[-2.5mm]
         OID &  19.8621 &  3.6611 &  0.4775 &  0.5940 &  60.4947 \\
        
         VOC &  21.2764 &  3.7155 &  0.6071 &  0.7011 &  68.6073 \\
        
         COCO &  15.2091 &  3.7774 &  0.6778 &  0.6844 &  69.5428 \\
        
         Object365 &  17.6128 &  3.6315 &  0.6273 &  0.6817 &  67.7270 \\
        
         CrowdHuman &  20.4306 &  2.9283 &  0.5160 &  0.6587 &  63.6830 \\
        
         DeepFashion &  42.1884 &  6.9403 &  0.5448 &  0.6515 &  71.1873 \\
        
         iDesigner &  25.8027 &  4.6227 &  0.5922 &  0.6666 &  69.3768 \\
        \hline
        \\[-2mm]
         \textbf{PERSONA} (ours) & \textbf{ 10.3758} & \textbf{ 2.8659} & \textbf{ 0.7632} & \textbf{ 0.7198} & \textbf{ 74.7808} \\
        \hline
    \end{tabularx}

    \vspace{-3mm} 
    \caption{Quantitative comparison across different human-related datasets, with the best results highlighted in \textbf{bold}.}
    \label{table:comparison_iqa}
    \vspace{-7.5mm}
\end{table}

\begin{table*}[t]
\scriptsize
\setlength{\tabcolsep}{0.5mm}
\renewcommand{\arraystretch}{1.16}
\centering
\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\begin{tabularx}{1\textwidth}{l|l|CC|CC|C|CCCC|CCCC}
\hline
\multirow{2}{*}{Type} & \multirow{2}{*}{Methods} & \multicolumn{9}{c|}{PERSONA-Val} & \multicolumn{4}{c}{PERSONA-Test} \\ \cline{3-15}
& & DISTS$\downarrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ & CLIPIQA$\uparrow$ & MANIQA$\uparrow$ & MUSIQ$\uparrow$ & NIQE$\downarrow$ & CLIPIQA$\uparrow$ & MANIQA$\uparrow$ & MUSIQ$\uparrow$ & NIQE$\downarrow$ \\ \hline\hline
\multirow{5}{*}{\makecell{Multi-Step\\Diffusion}} 
&DiffBIR & \textcolor{cyan}{0.1475} & \textcolor{cyan}{0.3047} & 21.52 & 0.5718 & \textcolor{red}{14.8418} & \textcolor{red}{0.8080} & \textcolor{red}{0.7030} & \textcolor{cyan}{76.4751} & \textcolor{cyan}{3.9752} & \textcolor{red}{0.7287} & \textcolor{red}{0.6812} & \textcolor{cyan}{73.2505} & 4.9820 \\
&SeeSR & \textcolor{red}{0.1379} & \textcolor{red}{0.2851} & 21.31 & 0.5955 & \textcolor{cyan}{15.0063} & \textcolor{cyan}{0.7785} & \textcolor{cyan}{0.6993} & \textcolor{red}{76.8001} & \textcolor{red}{3.6125} & \textcolor{cyan}{0.6716} & 0.6698 & \textcolor{red}{73.2988} & \textcolor{cyan}{4.0228} \\ 
&PASD & 0.1891 & 0.3587 & \textcolor{red}{22.17} & \textcolor{cyan}{0.6154} & 26.7405 & 0.5950 & 0.6090 & 67.3329 & 4.6249 & 0.5765 & \textcolor{cyan}{0.6703} & 72.1972 & \textcolor{red}{3.8728} \\ 
&ResShift & 0.1795 & 0.3313 & \textcolor{cyan}{22.10} & \textcolor{red}{0.6157} & 30.7865 & 0.5931 & 0.5833 & 69.5889 & 4.7448 & 0.5544 & 0.6101 & 69.4611 & 4.8438 \\ 
&ResShift* & 0.1822 & 0.3372 & 21.73 & 0.5969 & 29.4177 & 0.6721 & 0.6121 & 71.9257 & 4.8061 & 0.6130 & 0.6174 & 70.2313 & 4.8735 \\ \hline\hline
\multirow{5}{*}{\makecell{One-Step\\Diffusion}} 
&SinSR & 0.1691 & 0.3187 & 21.92 & 0.5967 & 22.9041 & 0.6372 & 0.5712 & 70.0839 & 4.4392 & 0.5882 & 0.6010 & 69.0157 & 4.7510 \\ 
&SinSR* & 0.1844 & 0.3348 & 21.54 & 0.5766 & 34.5773 & 0.7033 & 0.5819 & 71.2943 & 4.6294 & \textcolor{cyan}{0.6936} & 0.5962 & 69.9375 & 4.9873 \\ 
&OSEDiff & \textcolor{cyan}{0.1510} & 0.2824 & 21.81 & 0.6182 & 17.6308 & 0.6875 & 0.6639 & \textcolor{cyan}{74.0774} & \textcolor{cyan}{3.5858} & 0.6734 & 0.6919 & \textcolor{cyan}{73.5634} & 4.4600 \\ 
&OSEDiff* & 0.1476 & \textcolor{cyan}{0.2756} & \textcolor{cyan}{22.23} & \textcolor{cyan}{0.6342} & \textcolor{cyan}{17.2200} & \textcolor{cyan}{0.7034} & \textcolor{red}{0.6976} & 73.7636 & 3.9980 & 0.6874 & \textcolor{red}{0.7052} & 73.1611 & \textcolor{cyan}{4.4261} \\ \cline{2-15}     
&\textbf{OSDHuman} & \textcolor{red}{0.1414} & \textcolor{red}{0.2627} & \textcolor{red}{22.41} & \textcolor{red}{0.6363} & \textcolor{red}{16.5987} & \textcolor{red}{0.7295} & \textcolor{cyan}{0.6934} & \textcolor{red}{76.1256} & \textcolor{red}{3.5750} & \textcolor{red}{0.7155} & \textcolor{cyan}{0.6977} & \textcolor{red}{73.7694} & \textcolor{red}{4.1287} \\ \hline
\end{tabularx}

\vspace{-3mm}
\caption{Quantitative comparisons on synthetic PERSONA-Val and real-world PERSONA-Test datasets. For each metric, the best and second-best results are highlighted in \textcolor{red}{red} and \textcolor{cyan}{cyan}, within both multi-step and one-step diffusion-based methods. Models labeled with an asterisk (*) represent versions retrained on our PERSONA dataset for reference.}
\label{table:model_metrics}
\vspace{-3mm}
\end{table*}

\begin{figure*}[t]

\scriptsize
\begin{center}
\scalebox{0.97}{

    \hspace{-0.4cm}
    \begin{adjustbox}{valign=t}
    \begin{tabular}{ccccccccc}
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4016_2008_004092_png.rf.1f1070752d8fc321a25a24041d8466ff/pasted/marked_LQ.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4016_2008_004092_png.rf.1f1070752d8fc321a25a24041d8466ff/pasted/marked_diffbir.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4016_2008_004092_png.rf.1f1070752d8fc321a25a24041d8466ff/pasted/marked_seesr.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4016_2008_004092_png.rf.1f1070752d8fc321a25a24041d8466ff/pasted/marked_pasd.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4016_2008_004092_png.rf.1f1070752d8fc321a25a24041d8466ff/pasted/marked_resshift.png} \hspace{-4mm}    &\includegraphics[width=0.11\textwidth]{figs/Test/voc_4016_2008_004092_png.rf.1f1070752d8fc321a25a24041d8466ff/pasted/marked_sinsr.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4016_2008_004092_png.rf.1f1070752d8fc321a25a24041d8466ff/pasted/marked_osediff.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4016_2008_004092_png.rf.1f1070752d8fc321a25a24041d8466ff/pasted/marked_osediff-retrain.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4016_2008_004092_png.rf.1f1070752d8fc321a25a24041d8466ff/pasted/marked_ours.png} \hspace{-4mm} 
    \\
    
        LQ \hspace{-4mm} &
    DiffBIR \hspace{-4mm} &
    SeeSR \hspace{-4mm} &
    PASD \hspace{-4mm} &
    ResShift \hspace{-4mm} &
    SinSR \hspace{-4mm} &
    OSEDiff \hspace{-4mm} &
    OSEDiff* \hspace{-4mm} &
    \textbf{OSDHuman} \hspace{-4mm} \\
    \end{tabular}
    \end{adjustbox}
    
}
\scalebox{0.97}{

    \hspace{-0.4cm}
    \begin{adjustbox}{valign=t}
    \begin{tabular}{ccccccccc}
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4063_2008_000885_png.rf.de84365118166fdc702101d8c7208165/pasted/marked_LQ.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4063_2008_000885_png.rf.de84365118166fdc702101d8c7208165/pasted/marked_diffbir.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4063_2008_000885_png.rf.de84365118166fdc702101d8c7208165/pasted/marked_seesr.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4063_2008_000885_png.rf.de84365118166fdc702101d8c7208165/pasted/marked_pasd.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4063_2008_000885_png.rf.de84365118166fdc702101d8c7208165/pasted/marked_resshift.png} \hspace{-4mm} 
    &\includegraphics[width=0.11\textwidth]{figs/Test/voc_4063_2008_000885_png.rf.de84365118166fdc702101d8c7208165/pasted/marked_sinsr.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4063_2008_000885_png.rf.de84365118166fdc702101d8c7208165/pasted/marked_osediff.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4063_2008_000885_png.rf.de84365118166fdc702101d8c7208165/pasted/marked_osediff-retrain.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_4063_2008_000885_png.rf.de84365118166fdc702101d8c7208165/pasted/marked_ours.png} \hspace{-4mm} 
    \\
        LQ \hspace{-4mm} &
    DiffBIR \hspace{-4mm} &
    SeeSR \hspace{-4mm} &
    PASD \hspace{-4mm} &
    ResShift \hspace{-4mm} &
    SinSR \hspace{-4mm} &
    OSEDiff \hspace{-4mm} &
    OSEDiff* \hspace{-4mm} &
    \textbf{OSDHuman} \hspace{-4mm} \\
    \end{tabular}
    \end{adjustbox}
    
}
\scalebox{0.97}{

    \hspace{-0.4cm}
    \begin{adjustbox}{valign=t}
    \begin{tabular}{ccccccccc}
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_8375_2012_002365_png.rf.4dda2b81279c4169b2690d9a11948a03/pasted/marked_LQ.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_8375_2012_002365_png.rf.4dda2b81279c4169b2690d9a11948a03/pasted/marked_diffbir.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_8375_2012_002365_png.rf.4dda2b81279c4169b2690d9a11948a03/pasted/marked_seesr.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_8375_2012_002365_png.rf.4dda2b81279c4169b2690d9a11948a03/pasted/marked_pasd.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_8375_2012_002365_png.rf.4dda2b81279c4169b2690d9a11948a03/pasted/marked_resshift.png} \hspace{-4mm} 
    &\includegraphics[width=0.11\textwidth]{figs/Test/voc_8375_2012_002365_png.rf.4dda2b81279c4169b2690d9a11948a03/pasted/marked_sinsr.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_8375_2012_002365_png.rf.4dda2b81279c4169b2690d9a11948a03/pasted/marked_osediff.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_8375_2012_002365_png.rf.4dda2b81279c4169b2690d9a11948a03/pasted/marked_osediff-retrain.png} \hspace{-4mm} &
    \includegraphics[width=0.11\textwidth]{figs/Test/voc_8375_2012_002365_png.rf.4dda2b81279c4169b2690d9a11948a03/pasted/marked_ours.png} \hspace{-4mm} 
    \\
    LQ \hspace{-4mm} &
    DiffBIR \hspace{-4mm} &
    SeeSR \hspace{-4mm} &
    PASD \hspace{-4mm} &
    ResShift \hspace{-4mm} &
    SinSR \hspace{-4mm} &
    OSEDiff \hspace{-4mm} &
    OSEDiff* \hspace{-4mm} &
    \textbf{OSDHuman} \hspace{-4mm} \\
    \end{tabular}
    \end{adjustbox}
    
}
\end{center}

\vspace{-5mm}
\caption{Visual comparison of the real-world PERSONA-Test dataset in challenging cases. Please zoom in for a better view.}
\label{fig:vis-test}
\vspace{-6mm}
\end{figure*}


\vspace{-2mm}
\subsection{Experimental Settings}
\vspace{-1mm}
\textbf{Training and Testing dataset.} Our OSDHuman model is trained on our PERSONA dataset, which contains 109,052 high-quality 512$\times$512 human images. The degradation pipeline of Real-ESRGAN~\cite{wang2021real} is used to generate synthetic degraded images for training. The test data includes PERSONA-Val and PERSONA-Test, both generated by our HQ-ACF pipeline. The HQ images in the validation set are specially selected from those that comply with the pipeline, ensuring that no images in the validation set share sources with the training set. A total of 4,216 images are used, and the degraded LQ images are generated using the same degradation pipeline as during training. The test set is derived from the VOC dataset~\cite{Everingham2010voc} by performing a partial crop using the HQ-ACF pipeline, followed by sampling under predefined IQA thresholds, yielding 3,000 images with real-world LQ.

\textbf{Evaluation Metrics.} For the \emph{PERSONA-Val}, we employ both reference-based and non-reference IQA metrics. DISTS~\cite{ding2020dists} and LPIPS~\cite{zhang2018lpips} are used for reference-based perceptual quality assessment, while PSNR and SSIM~\cite{wang2004ssim} (calculated on the Y channel in YCbCr space) are used for reference-based fidelity assessment. Additionally, FID~\cite{heusel2017fid} is used to measure the distribution between the restored images and GT. The non-reference IQA metrics we used includes CLIPIQA~\cite{wang2022clipiqa}, MUSIQ~\cite{ke2021musiq}, MANIQA~\cite{yang2022maniqa}, and NIQE~\cite{zhang2015niqe}. For the \emph{PERSONA-Test}, we use the same non-reference IQA metrics as PERSONA-Val. We utilize the evaluation codes provided by pyiqa~\cite{pyiqa}, with the pipal version used for MANIQA.

\vspace{-0.8mm}
\textbf{Implementation Details.} The OSDHuman model is trained by AdamW optimizer~\cite{loshchilov2018AdamW} with 5$\times 10^{-5}$ learning rate and a batch size of 16. The Stable Diffusion v2-1 model~\cite{sd21}  serves as the pretrained OSD model with the timestep frozen to 999, and the prompt embedding is provided by HFIE. The LoRA rank for the VAE encoder, the U-Net of the generator, and the regularizer are all set to 4. The weighting scalars $\lambda_1$ and $\lambda_2$ in Eq.~\ref{eq:training_objective} are set to 2 and 1, respectively. Training is conducted for 35K iterations on 4 NVIDIA A800 GPUs.

\vspace{-0.8mm}
\textbf{Compared Methods.} We compare OSDHuman with several diffusion-based methods, including DiffBIR~\cite{lin2024diffbir}, SeeSR~\cite{wu2024seesr}, PASD~\cite{yang2023pasd}, ResShift~\cite{yue2023resshift},  SinSR~\cite{wang2024sinsr} and OSEDiff~\cite{wu2024osediff}. Among them, SinSR and OSEDiff are OSD models. ResShift and OSEDiff are retrained on our PERSONA dataset, referred to as ResShift* and OSEDiff*, respectively. Additionally, SinSR is distilled using ResShift*, namely SinSR*.





\begin{figure*}[t]

\scriptsize
\begin{center}

\scalebox{0.97}{

    \hspace{-0.4cm}
    \begin{adjustbox}{valign=t}
    \begin{tabular}{cccccccccc}
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/HQ.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/LQ.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/diffbir.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/seesr.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/pasd.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/resshift.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/sinsr.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/osediff.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/osediff-retrain.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/coco_11600_000000102757/ours.png} \hspace{-4.5mm} 
    \\
    
    HQ\hspace{-4.5mm} &LQ\hspace{-4.5mm} &DiffBIR\hspace{-4.5mm} &SeeSR\hspace{-4.5mm} &PASD\hspace{-4.5mm} &ResShift\hspace{-4.5mm} &SinSR\hspace{-4.5mm} &OSEDiff\hspace{-4.5mm} &OSEDiff*\hspace{-4.5mm} &\textbf{OSDHuman}\hspace{-4.5mm} \\
    \end{tabular}
    \end{adjustbox}
    
}
\scalebox{0.97}{

    \hspace{-0.4cm}
    \begin{adjustbox}{valign=t}
    \begin{tabular}{cccccccccc}
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/HQ.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/LQ.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/diffbir.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/seesr.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/pasd.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/resshift.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/sinsr.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/osediff.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/osediff-retrain.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_252159_objects365_v2_00968800/ours.png} \hspace{-4.5mm} 
    \\
    
    HQ\hspace{-4.5mm} &LQ\hspace{-4.5mm} &DiffBIR\hspace{-4.5mm} &SeeSR\hspace{-4.5mm} &PASD\hspace{-4.5mm} &ResShift\hspace{-4.5mm} &SinSR\hspace{-4.5mm} &OSEDiff\hspace{-4.5mm} &OSEDiff*\hspace{-4.5mm} &\textbf{OSDHuman}\hspace{-4.5mm} \\
    \end{tabular}
    \end{adjustbox}
    
}
\scalebox{0.97}{

    \hspace{-0.4cm}
    \begin{adjustbox}{valign=t}
    \begin{tabular}{cccccccccc}
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/HQ.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/LQ.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/diffbir.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/seesr.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/pasd.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/resshift.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/sinsr.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/osediff.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/osediff-retrain.png} \hspace{-4.5mm} &
    \includegraphics[width=0.1\textwidth]{figs/Val/object365_72541_objects365_v2_01329191/ours.png} \hspace{-4.5mm} 
    \\
    
    HQ\hspace{-4.5mm} &LQ\hspace{-4.5mm} &DiffBIR\hspace{-4.5mm} &SeeSR\hspace{-4.5mm} &PASD\hspace{-4.5mm} &ResShift\hspace{-4.5mm} &SinSR\hspace{-4.5mm} &OSEDiff\hspace{-4.5mm} &OSEDiff*\hspace{-4.5mm} &\textbf{OSDHuman}\hspace{-4.5mm} \\
    \end{tabular}
    \end{adjustbox}
    
}
\end{center}

\vspace{-4mm}
\caption{Visual comparison of the synthetic PERSONA-Val datasets in challenging cases. Please zoom in for a better view.}
\label{fig:vis-val}
\vspace{-2mm}
\end{figure*}

\begin{table*}[t]
    \begin{center}
    \scriptsize
    \setlength{\tabcolsep}{0.5mm} 
    \renewcommand{\arraystretch}{1.25}

    \newcolumntype{C}{>{\centering\arraybackslash}X}

    \newcolumntype{S}{>{\centering\arraybackslash}c}

    \begin{tabularx}{\textwidth}{l|CC|CC|C|CCCC|CCCC} 
        \hline
 \multirow{2}{*}{\makecell{Training\\Dataset}} & \multicolumn{9}{c|}{PERSONA-Val} & \multicolumn{4}{c}{PERSONA-Test} \\ \cline{2-14}
& DISTS$\downarrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ & CLIPIQA$\uparrow$ & MANIQA$\uparrow$ & MUSIQ$\uparrow$ & NIQE$\downarrow$ & CLIPIQA$\uparrow$ & MANIQA$\uparrow$ & MUSIQ$\uparrow$ & NIQE$\downarrow$ \\
        \hline \hline 
        LSDIR & 0.1521 & 0.2692 & \textbf{22.51} & 0.6271 &  17.5615& 0.7229 & 0.6618 & 74.4998 & 3.6461  & 0.6781 & 0.6964 & 73.1266 & 4.6382 \\
        \textbf{PERSONA} & \textbf{0.1414} & \textbf{0.2627} & 22.41 & \textbf{0.6363} & \textbf{16.5987} & \textbf{0.7295} & \textbf{0.6934} & \textbf{76.1256} & \textbf{3.5750} & \textbf{0.7155} & \textbf{0.6977} & \textbf{73.7694} & \textbf{4.1287}
 \\ \hline
    \end{tabularx}
    \end{center}
    \vspace{-3mm} 

    \caption{Ablation studies within different training datasets. The best results are highlighted in \textbf{bold}.}
    \label{table:ablation_dataset}
    \vspace{-5mm}
\end{table*}

\begin{table}[t]
\vspace{2mm}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{0.5mm} 
    \renewcommand{\arraystretch}{1.25}

    \newcolumntype{C}{>{\centering\arraybackslash}X}

    \newcolumntype{S}{>{\centering\arraybackslash}c}

    \begin{tabularx}{\columnwidth}{l|SS|*{5}{C}} 
        \hline
        \multicolumn{3}{c|}{Prompt Extractor}& \multirow{2}{*}{CLIPIQA$\uparrow$} & \multirow{2}{*}{MANIQA$\uparrow$} & \multirow{2}{*}{MUSIQ$\uparrow$} & \multirow{2}{*}{NIQE$\downarrow$} \\ \cline{1-3}
Type & From HQ & From LQ  &&&&\\
        \hline \hline 
        Null&&&0.7016&\textbf{0.7226}&73.1735&5.0651\\
        DAPE& \checkmark & &  0.6625&0.7014&72.3104&4.9455  \\
        HFIE & \checkmark &  &0.7111&0.6747&69.9992&	5.5031  \\\hline
        HFIE &  & \checkmark &  \textbf{0.7155}&0.6977&\textbf{73.76}94&\textbf{4.1287}  \\ \hline

    \end{tabularx}

    \vspace{-2mm} 

    \caption{Ablation studies within different prompt extractors tested on PERSONA-Test. ``From HQ / LQ" indicates prompts are extracted from HQ or LQ. The best results are highlighted in \textbf{bold}.}
    \label{table:ablation_extractor}
    \vspace{-6mm}
\end{table}

\vspace{-0.5mm}
\subsection{Main Results}
\vspace{-1mm}
\textbf{Quantitative Comparisons.} Table~\ref{table:model_metrics} presents the evaluation metrics of our OSDHuman on the synthetic PERSONA-Val and real-world PERSONA-Test datasets. Our method achieves the best or second-best results across most metrics when compared with both original and retrained one-step diffusion models. Against multi-step diffusion methods, OSDHuman outperforms in DISTS, LPIPS, PSNR, SSIM, and NIQE on PERSONA-Val, as well as CLIPIQA, MANIQA, and MUSIQ on PERSONA-Test, with other metrics being comparable. While multi-step diffusion methods excel in reconstructing highly degraded regions and achieving higher CLIPIQA scores, their generated content often deviates from the original, resulting in lower fidelity and perceptual scores such as PSNR, SSIM, DISTS, and LPIPS.


\vspace{-0.8mm}
\textbf{Visual Comparisons.}
As shown in Figs.~\ref{fig:vis-test} and~\ref{fig:vis-val}, representative images from the real-world PERSONA-Test dataset and the synthetic PERSONA-Val dataset are visualized. Existing state-of-the-art image restoration methods are not well suited for human body restoration (HBR). In HBR tasks, the most challenging aspects often involve parts of the human image with intricate textures and delicate structures, such as faces, fingers, and surrounding objects. These methods frequently exhibit issues like over-smoothing or unnatural color rendering, making it difficult to accurately restore fine details such as facial features. For example, DiffBIR~\cite{lin2024diffbir} suffers from over-smoothing or non-faithful textures, OSEDiff~\cite{wu2024osediff} often results in over-saturated facial regions and ResShift~\cite{yue2023resshift} exhibits distorted facial features. In contrast, our OSDHuman model can restore 
natural human actions and facial expressions in images, achieving high fidelity and maintaining a high degree of similarity to the original image.


\vspace{-1mm}

\subsection{Ablation Studies}
\textbf{Comparison on training datasets.}
To evaluate the suitability of our PERSONA dataset for HBR-specific models, we train our OSDHuman on different datasets. The first option employs LSDIR~\cite{Li2023LSDIR}, a dataset widely utilized in the image restoration domain. During training, images in the dataset are randomly cropped to 512$\times$512 as input. The second option employs our PERSONA dataset. As shown in Tab.~\ref{table:ablation_dataset}, the results of the model trained on our PERSONA significantly outperform that trained on LSDIR in both PERSONA-Val and PERSONA-Test. This demonstrates that our dataset provides a strong prior for the human body, effectively enhancing the performance of HBR.


\textbf{Comparison on Prompt Extractors.}
We conduct experiments with four options to evaluate the effectiveness of various prompt extractors for HBR. The first option does not employ a prompt extractor but uses a space as the prompt. For the second option, we use DAPE~\cite{wu2024seesr} with the setting of OSEDiff~\cite{wu2024osediff}. In OSEDiff, HQ images are inputted into DAPE during training to provide higher-quality prompts. Following this, we also input HQ images into HFIE as the third option. The last option, which is our default setting, involves using HFIE to extract prompts from LQ images. As shown in Tab.~\ref{table:ablation_extractor}, our HFIE of default setting outperforms the other options. It demonstrates the superior capability of HFIE in providing priors to the model, guiding higher-quality restoration.

\vspace{-0.8mm}
\section{Conclusion}
In this work, we propose a high-quality dataset automated cropping and filtering (HQ-ACF) pipeline designed for creating a human body restoration (HBR) dataset to address the lack of a benchmark. Using this pipeline, we develop a person-based restoration with sophisticated objects and natural activities (PERSONA) dataset, which includes training, validation, and test sets. Experimental results show its high quality and suitability for HBR. Additionally, we propose OSDHuman, a one-step diffusion model for HBR. It innovatively employs a high-fidelity image embedder (HFIE) as a prompt extractor. HFIE guides the model in achieving high-quality restoration by effectively extracting and utilizing rich human image features. Extensive experiments show that OSDHuman outperforms current state-of-the-art image restoration methods, which are applied to HBR, in both visual quality and quantitative metrics.

\nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}

\end{document}