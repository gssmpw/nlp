
\chapter{Related Work}
\label{chapter:related-work}


Here we provide an overview of the literature on generative modeling, content provenance, and watermarking, which intersect in many chapters of the thesis.
We provide additional related work in each chapter where it is relevant.








\section{Generative models}

Generative models are models that learn to create data samples from a given distribution.
They have benefited from the increase in computational power, the amount of data shared on the internet, and the development of algorithms able to handle these large datasets.
This section provides an overview of the most popular models in different domains, which are particularly relevant to \autoref{part:genai-tracing} which focuses on tracing AI-generated content and to \autoref{part:model-monitoring} which focuses on how to trace the usage of these models.


\paragraph{Text generation.}
Probabilistic language models are statistical models that learn the probability distribution of sequences of tokens -- \glspl*{token} being words, subwords or characters, created by tokenization algorithms that learn the optimal way to split a text into units~\citep{sennrich2015neural, kudo2018subword, dagan2024getting}.
This was originally done with $n$-grams models that count the frequency of each word given the $n-1$ previous tokens.
Given the exponential growth of the number of possible sequences with the sequence length, $n$-grams models are limited to short sequences, which explains why they have been replaced by neural models~\citep{bengio2000neural}.
RNN~\citep{mikolov2010recurrent, sutskever2011generating, jozefowicz2016exploring} and LSTM~\citep{hochreiter1997long} models were the first to generate a significant amount of interesting and high-level linguistic structures.

They were quickly outperformed by the Transformer~\citep{vaswani2017attention}.
It is a sequence-to-sequence architecture based on the self-attention mechanism, which increases the expressiveness of the model by allowing it to build the new token taking into account all the context and previously generated tokens.
Most importantly, it enables the parallelization and scaling of the training process, which is important for text given the vast amount of data available on the internet.
This is central for generative pre-trained transformers (GPT 1, 2 and 3)~\citep{Radford2018ImprovingLU, radford2019language,brown2020language} that demonstrate impressive generation quality, simply by training on the next-token prediction task on a large corpus of text.
Given some examples written in plain text of tasks they have not been trained on (few-shot demonstrations), they are able to perform these tasks competitively with models fine-tuned specifically for them (summarization, translation, etc.)~\citep{brown2020language, rae2021scaling, chowdhery2023palm}; and even better with effective prompting strategies~\citep{wei2022chain}.
What is more, this performance scales with the number of parameters~\citep{kaplan2020scaling}, and with the size of the training data~\citep{hoffmann2022training}, hence the name of Large Language Models (\Glspl*{LLM}).
Building on this, studies have found that fine-tuning on instructions significantly improves their few-shot abilities on unseen tasks~\citep{wei2021finetuned}. 
Combining this approach with Reinforcement Learning with Human Feedback (RLHF) yields even better results, enabling the resulting model, InstructGPT, to follow the human's intent more closely~\citep{ouyang2022training}.

This has led to a proliferation of generative pre-trained models (and companies built around them).
The key differentiators are: access to greater computational resources, larger and higher-quality datasets, and more advanced training algorithms (sometimes referred to as ``compute multipliers'').
There is, among many other examples, ChatGPT~\citep{openai2024chatgpt}, the first model to catch the public's attention, and its successor GPT-4~\citep{openai2023gpt}, the contender of Google DeepMind Gemini~\citep{team2023gemini} or Llama 1, 2 and 3~\citep{touvron2023llama,touvron2023llama2,dubey2024llama}, Mistral~\citep{jiang2023mistral7b}, Gemma~\citep{team2024gemma}, which are popular because openly available.

LLMs are rapidly evolving.
They integrate multi-modal capabilities to understand and generate images~\citep{alayrac2022flamingo, laurenccon2024obelics, liu2024visual, team2024chameleon}, speech~\citep{rubenstein2023audiopalm, nguyen2024spirit}, or both~\citep{lu2022unified, openai2024gpt4o}. 
The use of Mixture of Experts (MoE)~\citep{shazeer2017outrageously, jiang2024mixtral}, increases the parameter count and the expressiveness of the model, while keeping a similar inference cost. 
Models with multiple decoding heads~\citep{cai2024medusa, gloeckle2024better} increase the inference speed and performance on reasoning tasks.
State-space models aim to address the computational inefficiency on long sequences~\citep{de2024griffin, gu2023mamba}.
Models that integrate mathematical reasoning~\citep{frieder2024mathematical} or agents~\citep{park2023generative, dubois2024alpacafarm} are able to perform more complex tasks.
At the end of the day, text generation is about much more than just producing text; it involves understanding, reasoning about the world, and interacting with it, since most of our interactions are conducted through language.


\paragraph{Image generation.}
Generating images has long been dominated by Generative Adversarial Networks (\Glspl*{GAN})~\citep{goodfellow2014generative, zhu2017unpaired, karras2020training, karras2019style, karras2020analyzing, sauer2022stylegan, walton2022stylenat}, which train a generator and a discriminator to produce realistic images.
The generator learns to create images from random noise that fool the discriminator, while the latter simultaneously learns to distinguish between real and generated images.

\Glspl*{diffusion model} are a different class of models that learn to generate images by iteratively denoising a noised vector, which is a more stable training process than GANs~\citep{dhariwal2021diffusion, ho2020denoising, nichol2021improved, song2020denoising}.
They are a huge improvement in text-conditional image generation and the reason for the rise in popularity of text-to-image models.
Resulting models synthesize high-resolution photo-realistic images for a wide variety of text prompts~\citep{balaji2022ediffi, ho2022imagenvideo, ramesh2022hierarchical, saharia2022photorealistic}. 
Notably, Latent Diffusion Models (\Glspl*{LDM})~\citep{rombach2022high} are a class of diffusion models that operate in the latent space of a variational autoencoder, and Stable Diffusion is a large instance of LDM trained on the LAION dataset~\citep{schuhmann2021laion, schuhmann2022laion}.
This is the main class of model that we experiment with in Chap.~\ref{chapter:stable-signature}.
These models can also perform conditional image generation -- like inpainting or text-guided image editing -- by fine-tuning the diffusion model with additional conditioning, \eg, masked input image, segmentation map, etc.~\citep{lugmayr2022repaint, saharia2022palette}. 
Because of their iterative denoising algorithm, diffusion models can also be adapted for image editing in a zero-shot fashion by guiding the generative process~\citep{couairon2022diffedit, hertz2022prompt, kawar2022imagic, mokady2022null, valevski2022unitune,  wu2022unifying}.
These methods are also behind the video generation models \href{https://openai.com/index/sora/}{Sora} and \href{https://kling.kuaishou.com/en}{Kling}.

Another class of models treats the generation process as a sequence-to-sequence problem and leverages the advances in language models.
They are based on image tokenizers (\eg, VQVAE~\citep{van2017neural}, VQGAN~\citep{esser2021taming}, ViT-VQGAN~\citep{yu2021vector}), which are models that convert images into discrete tokens, and vice versa, and on transformers that can generate sequences of tokens.
DALL$\cdot$E~\citep{ramesh2021zero} is one of the pioneering models to generate images in this way.
Other known models include Parti~\citep{yu2022scaling}, Chameleon~\citep{team2024chameleon} which is able to generate both text from images and images from text, and many others~\citep{chen2020generative, ding2021cogview, esser2021taming, gafni2022make, singer2022makeavideo}. 


\paragraph{Audio generation.}
Early models are autoregressive like WaveNet~\citep{wavnet, wang2017tacotron, shen2018natural}, and use waveform or Mel spectrogram reconstruction as objective. 
None of these objectives are deemed ideal for audio quality, leading to the adoption of adversarial models~\citep{hifigan, melgan}, and then to diffusion or auto-regressive models similar to those used in image generation.

The most popular task in audio generation is perhaps speech generation.
One key example is zero-shot text-to-speech (\Gls*{TTS}) models, \aka, voice cloning, which tries to imitate or preserve vocal style using only a small amount of data. 
VALL-E~\citep{wang2023neural} or SPEAR-TTS~\citep{kharitonov2023speak} follows the token-based autoregressive language modeling approach, which converts text or audio into tokens~\citep{defossez2022high,soundstream}, and then generates speech from these tokens, with optional conditioning (speech, text, language, etc.). 
In the context of speech machine translation, SeamlessExpressive~\citep{seamless2023} follows a similar approach to translate speech, but also retain the speaker's unique vocal style and emotional inflections.
These models are able synthesize high-quality personalized speech with only a 3-second recording. 
Other zero-shot TTS models, like NaturalSpeech2~\citep{shen_naturalspeech2}, Voicebox~\citep{le2023voicebox}, A$^{3}$T~\citep{BaiZCML022_A3T} and Audiobox~\citep{hsu2023audiobox} are non-autoregressive and based on diffusion~\citep{ho2020denoising} or flow-matching~\citep{lipman2022flow}.
They iteratively denoise the audio signal to generate speech, which is represented as spectrograms or learned latents (from HiFi-GAN or EnCodec).
They are particularly suited to perform tasks such as text-guided speech infilling, where the goal is to generate masked speech given its surrounding audio and text transcript. 
It makes them a powerful tool for speech manipulation.

Generating environmental sounds and music is another popular task, which follows the same principles.
For instance, Mo\^usai~\citep{schneider2023mo}, Make-an-Audio~\citep{huang2023make}, AudioLDM~\citep{liu2023audioldm} and AudioLDM~2~\citep{liu2024audioldm}, etc. are latent diffusion models that generate audio in the latent space of a perceptual autoencoder.
On the other hand, models like AudioGen~\citep{kreuk2022audiogen}, AudioLM~\citep{agostinelli2023musiclm} or MusicGen are autoregressive and generate audio samples as a sequence of tokens.
All these models are now able to create high-quality audio samples in different styles, from text, other musics, or even from images~\citep{girdhar2023imagebind}, and to perform inpainting or editing in a zero-shot fashion.
Note that speech and sound generation are getting unified, with models like AudioLDM 2 or Audiobox that can jointly generate both speech and music, and that start to appear commercially, \eg, in \href{https://www.suno.ai/}{Suno}.










\section{Content tracing and watermarking}

Tracing the origin of digital content is a problem that is traditionally approached passively, by copy detection or digital forensics.
On the other hand, this thesis focuses on active tracing, by embedding invisible watermarks in the content, as presented in Parts \ref{part:genai-tracing} and \ref{part:model-monitoring}.
This section provides an overview of the literature on these two approaches, and across image, audio and text modalities, with a particular focus on AI-generated content.

\subsection{Fingerprinting and copy detection}

\Gls*{fingerprinting} involves creating a unique identifier for a piece of digital content, called \gls*{hash} or \emph{fingerprint} in reference to the uniqueness of human fingerprints.
This fingerprint can then be used to identify the content even if it has been modified or compressed, but does not allow to reconstruct the original content.
\Gls*{copy detection}, on the other hand, involves comparing two pieces of digital content to determine if they are identical or similar. 
This is often done using \emph{\gls*{indexing}} algorithms that store the fingerprints of all the content in a database, and then compare the hash of the queried content to the hashes in the database to determine if it is a copy.

These hashes are vector representations that can be binary or real-valued ($\in \{0,1\}^k $ or $\mathbb{R}^k$).
They were traditionally hand-crafted with color histograms, GIST descriptors, constellation maps, etc.~\citep{swain1991color, oliva2001modeling, wang2003industrial, perronnin2010large}, but are now usually generated from self-supervised feature extractors~\citep{chen2020simclr,oquab2023dinov2, devlin2018bert, hsu2021hubert, raffel2020exploring}.
The feature extractors are not perfectly robust to content modifications.
In other words, the hashes are not perfectly invariant to transformations, \eg, an audio and its $\times$1.25 speed-up version may have different ones.
Besides, storing the hashes is cumbersome and reverse search, \ie, finding the content that has a given hash, must be approximate to be tractable at scale.
Therefore, the hashes are often stored using methods like locality-sensitive hashing (LSH)~\citep{charikar2002similarity, datar2004locality} or product quantization (PQ)~\citep{jegou2010pq}.
These indexing structures have a dual role of compressing the hashes and enabling fast approximate search.
See \Gls*{FAISS}~\citep{douze2024faiss} for a review and efficient implementations of these algorithms.
The two above factors result in errors especially in an adversarial setting~\citep{douze20212021, papakipos2022results, wang2022benchmark}.
\autoref{chapter:active-indexing} aims to reduce these errors by actively modifying images before their release, in a similar way to watermarking.
Another downside is the need of storing the hashes in a database, which makes it harder to share and impossible for open content moderation systems.


\subsection{Digital forensics}

More specific to the context of AI-generated content, \gls*{digital forensics} methods aim to detect if a piece of content has been generated or altered by an AI model.
Most methods spot imperceptible hidden traces of generated content, such as variation in words probabilities~\citep{mitchell2023detectgpt}, odd frequencies in images~\citep{corvi2023detection} or voice synthesizer artifacts~\citep{le2023voicebox}.
Relying on these traces makes the detectors very brittle to shifts in the distribution of content, and makes them fall short in effectiveness compared to watermarking techniques~\citep{sadasivan2023can, saberi2023robustness}.
As a key example, one state-of-the-art detection method~\citep{wang2023dire} is fooled to random chance simply by compressing generated images with JPEG~\citep{grommelt2024fake}, because all natural images in their training dataset were in the JPEG format.
Besides, these detectors are likely to get worse as generative models get better and as their artifacts disappear. 

\paragraph{Image.}
Detection of synthetic/manipulated images has a long history~\citep{farid2009image, barni2023information}.
It is now very active in the context of deep-fake detection~\citep{guarnera2020deepfake, zhao2021multi}. 
Many works focus on the detection of \Gls*{GAN}-generated images~\citep{chai2020makes, gragnaniello2021gan, wang2020cnn, zhang2019detecting}.
One approach is to detect inconsistencies in generated images via lights~\citep{farid2022lighting}, perspective~\citep{farid2022perspective, sarkar2024shadows}, physical objects~\citep{ma2022totems} or faces~\citep{li2018exposing, wang2019detecting, bohavcek2023geometric}.
These approaches are restricted to photo-realistic images or faces, artworks not intended to be physically correct are not covered.
Other approaches track traces left by the generators in the spatial~\citep{marra2019gans, yu2019attributing} or frequency~\citep{frank2020leveraging, zhang2019detecting} domains.
There are extensions to \glspl*{diffusion model} in recent works~\citep{corvi2022detection, sha2022fake, epstein2023online} that show encouraging results.

\paragraph{Speech.}
Detection of synthetic speech is traditionally done in the forensics community by building features and exploiting statistical differences between fake and real.
These features can be hand-crafted from the analysis of waveforms, spectrograms or formants~\citep{sahidullah2015comparison, janicki2015spoofing, albadawy2019detecting, borrelli2021synthetic, cuccovillo2024audio} and/or learned~\citep{muller2022does, barrington2023single}.
The approach of most audio generation papers~\citep{Borsos2022AudioLMAL, Kharitonov2023SpeakRA, borsos2023soundstorm, le2023voicebox} is to train end-to-end deep learning classifiers on what their models generate, similarly as \citet{zhang2017investigation, tak2021end, tak2021end2, jung2022aasist}.
These networks primarily focus on non-vocal spectrogram regions~\citep{salvi2023towards, salvi2024listening}, which explains why they are sensitive to the addition or removal of audio artifacts (see Sec.~\ref{chap4/sec:active-passive}).

\paragraph{Text.}
Detection of LLM-generated text is a relatively new field.
As for images and audio, it either relies on hand-crafted textual features or on models trained for detection.
However, contrary to images and audio, the former approach is more popular since training and running LLMs is computationally expensive and cumbersome.
The features are for instance based on $n$-gram analysis~\citep{yang2023dna}, on the probability and rank of the observed tokens~\citep{gehrmann2019gltr, ippolito2019automatic}, on the perplexity of the text observed by the LLM under scrutiny or a surrogate model~\citep{vasilatos2023howkgpt, wang2023m4}, on several of them~\citep{hans2024spotting}, or on its curvature~\citep{mitchell2023detectgpt}.
The other class of methods trains classifiers~\citep{bhattacharjee2024eagle} or other language models, often by fine-tuning the model to detect itself~\citep{solaiman2019release, zellers2019defending}.
Similarly to images and audios, these detection methods are often brittle to shifts in the text distribution and not very reliable~\citep{sadasivan2023can}.



\subsection{Cryptographic metadata}

In the context of origin tracing, cryptographic metadata is digital information associated with a piece of content (e.g., images, videos, documents) to provide evidence of its authenticity and/or provenance.
The Coalition for Content Provenance and Authenticity (\Gls*{C2PA}) and the International Press Telecommunications Council (IPTC) have recently proposed two standards.
The upside is that forging fake cryptographic signatures is extremely hard, however the metadata are often removed during re-uploads or screenshots. 
For instance, a study by~\citet{ImatagStudy} shows that only 3\% of images on the internet come with copyright metadata.
It is therefore particularly suited for authenticating real content, for which the creators want the content to be traced back to them, but less for tracing origin of AI-generated content in the wild.
Besides, they are more a subject of standardization bodies than research, because all actors of the content production chain must adhere to the same protocol for it to be effective.

This metadata includes various types of cryptographic information, such as timestamps, used to record the date and time when the content was created or modified, or provenance information about the origin of the content like its creator.
All this information is encrypted with a private key, using algorithms like \Gls*{RSA}~\citep{rivest1978method} or ECDSA~\citep{johnson2001elliptic}, which makes it impossible to forge without the private key but possible to verify with the public key.
It can also include content bindings, used to check the authenticity of the content and ensure that it has not been tampered with.
They are encrypted hash values representing the content which are attached to it as a digital signature.
The bindings are categorized into two types. 
Hard bindings (\aka, cryptographic bindings), are computed directly from the raw bits of the content and can be used to ensure that the manifest belongs with the asset and that the asset has not been modified.
Soft bindings, on the other hand, are computed from the digital content of an asset (as in fingerprinting) and can be used to identify derived assets~\citep{c2pa}.


\subsection{Visible \gls*{watermarking}}

\emph{Visible watermarks} are straightforward and widely recognized. 
However, in addition to degrading the quality of the content, they are also easy to remove or tamper with, making them less reliable.
For instance, a visible watermark on the left side of an image can be removed by cropping the image or through inpainting techniques~\citep{dekel2017effectiveness}, as illustrated in Fig.~\ref{chap0/fig:visible-watermarking}.
\emph{Invisible watermarks} (the focus of the thesis) are imperceptibility embedded within the content itself.
It makes them riskier to remove because there is no certainty that it has been removed without access to the watermark detector or extractor.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{0-introduction/figs/visible-wm.pdf}
    \caption{Example of a visible watermark, and how it can be removed in less than 10 seconds using free software (Adobe Firefly in this example).}
    \label{chap0/fig:visible-watermarking}
\end{figure}





















\section{Invisible \gls*{watermarking} techniques}\label{chap0/sec:watermarking}

This section provides an overview of watermarking techniques developed across the image, audio and text domains.
See \autoref{chapter:technical-background} for a more detailed description of properties\footnote{
    We notably describe in Sec.~\ref{chap0/sec:properties} the notions of robustness, imperceptibility, payload, zero-bit and multi-bit watermarking, that are used in the following paragraphs.
} and of state-of-the-art techniques, and Chapters \ref{chapter:radioactive} and \ref{chapter:invariants} for more details on model watermarking that aims to protect the model itself, instead of content.

\subsection{Image watermarking}

Traditional image watermarking methods are usually classified in two categories depending on the space on which the watermark is embedded. 
In \textit{spatial domain}, the watermark is encoded by directly modifying pixels, such as flipping low-order bits of selected pixels~\citep{van1994digital}. 
For example, \cite{nikolaidis1998robust} slightly modify the intensity of randomly selected image pixels while taking into account properties of the human visual system, robustly to JPEG compression and lowpass filtering.
\cite{bas2002geometrically} create content descriptors defined by salient points and embeds the watermark by adding a pattern on triangles formed by the tessellation of these points.
\cite{ni2006reversible} use the zero or the minimum points of the histogram of an image and slightly modifies the pixel grayscale values to embed data into the image. 
The second category is \textit{frequency domain} watermarking, which usually spreads a pseudorandom noise sequence across the entire frequency spectrum of the host signal, and provides better robustness~\citep{cox1997secure}. 
The first step is a transformation that computes the frequency coefficients.
The watermark is then added to these coefficients taking into account the human visual system.
The coefficients are mapped back onto the original pixel space through the inverse transformation to generate the watermarked image. 
The transform domains include Discrete Fourier Transform (DFT)~\citep{urvoy2014perceptual}, 
Quaternion Fourier Transform (QFT)~\citep{bas2003color, ouyang2015qdft}, 
Discrete Cosine Transform (DCT)~\citep{bors1996image, piva1997dct, barni1998dct, li2011new},
Discrete Wavelet Transform (DWT)~\citep{xia1998wavelet, barni2001improved, furon2008broken}, 
both DWT and DCT~\citep{feng2010dwt, zear2018proposed}, etc.

Deep learning-based methods have recently emerged as alternatives to traditional ones.
They are often built as encoder/decoder networks: the encoder embeds the watermark in the image and the decoder tries to extract it (see Sec.~\ref{chap0/sec:deep learning-watermarking}).
They are trained end-to-end to invisibly encode information while being resilient to transformations applied during training. 
This makes it easier to build robust systems and avoids algorithms hand-crafted for specific transformations.
\Gls*{HiDDeN}~\citep{zhu2018hidden} is the best example of this approach, and has been extended in several ways.
\cite{luo2020distortion} add adversarial training in the attack simulation, to bring robustness to unknown transformations.
\cite{zhang2019robust, zhang2020robust, yu2020attention} use an attention filter further improving imperceptibility.
\cite{ahmadi2020redmark} adds a circular convolutional layer that helps diffusing the watermark signal over the image.
\cite{wen2019romark} use robust optimization with worst-case attack as if an adversary were trying to remove the mark.
Another line of works focus on \gls*{steganography}~\citep{baluja2017hiding, wengrowski2019light, zhang2019steganogan, tancik2020stegastamp, jing2021hinet, ma2022towards}, where the goal is to hide a message in the image without being detected, rather than to robustly extract it (\eg, against crops).
Many other approaches focused on improving robustness, imperceptibility, speed, etc.~\citep{jia2021mbrs, bui2023rosteals, bui2023trustmark, huang2023arwgan, evennou2024swift, pan2024jigmark}.
In parallel to the encoder/decoder architectures, \cite{vukotic2018deep,vukotic2020classification}, followed by~\cite{kishore2021fixed} introduce an approach that is closer to traditional watermarking methods and uses neural networks as a fixed transform into a latent space.
Since there is no inverse transform, the embedding is done iteratively by gradient descent over the pixels.
This is the approach followed in Chapters \ref{chapter:ssl-watermarking} and \ref{chapter:active-indexing}.



\subsection{Audio watermarking}

Given the similar nature of the signals, audio watermarking techniques are very similar to image watermarking ones (although they lag a bit behind). 
Traditional methods relied on embedding watermarks either in the time or frequency domains~\citep{trad_wm_LieC06,trad_wm_KalantariAAA09,trad_wm_NatgunanathanXRZG12,trad_wm_freq_XiangNPHL18,trad_wm_freq_SuZYCJ018,trad_wm_freq_LiuHH19, tai2019audio}, usually including domain specific features to design the watermark and its corresponding decoding function. 
To accurately extract audio watermarks, synchronization between the encoder and decoder is crucial. 
However, this can be disrupted by desynchronization attacks such as time and pitch scaling. 
To address this issue, various techniques have been developed. 
One approach is block repetition, which repeats the watermark signal along both the time and frequency domains~\citep{blockrep2-KirovskiM03, blockrep1-Kirovski2003}. 
Another method involves implanting synchronization bits into the watermarked signal~\citep{patchwork-sync-XiangNGZN14}. During decoding, these synchronization bits serve to improve synchronization and mitigate the effects of de-synchronization attacks.

Most deep learning-based audio watermarking methods follow a \Gls*{HiDDeN}-like encoder/decoder framework~\citep{qu2023audioqr, pavlovic2022robust, DEAR_Liu0FMZY23, ren2023speaking, chen2023wavmark, o2024maskmark}. 
The approach presented in \autoref{chapter:audioseal} is similar, but is zero-bit and allows for detection at the time-step level.
Similar to the approach of \cite{vukotic2018deep} in the image domain, \cite{wu2023adversarial, kong2020adversarial} embed the watermark by iteratively modifying the audio such that its representation lies in a certain region of the feature space.







\subsection{Text watermarking}
Watermarking text is commonly thought as more challenging than images or audio: its discrete nature makes it harder to modify without altering its meaning.

The earliest works address watermarking for documents by altering text characteristics such as characters or spacing~\citep{brassil1995electronic}, which is not very robust since this may be changed directly on a text editor.
Text watermarking methods traditionally modify the grammatical or syntactical structure of the text with pre-established rules that embed watermarks without significantly altering its meaning~\citep{topkara2005natural}.
For instance, \cite{topkara2006hiding} embed information through synonym substitution, while~\cite{topkara2006words, topkara2006natural, meral2009natural} use word reordering through passivization, preposing, topicalization, etc.
Steganography methods have also been developed for text, working on the same principles~\citep{winstein1998lexical, chapman2001practical, bolshakov2004method, shirali2008new, chang2014practical, xiang2017novel}.
These edit-based systems usually suffer from low robustness to text modifications, and low payload, \eg, 1 or 2 bits per sentence as in CoverTweet~\citep{wilson2016avoiding}.
Similar to other media, deep learning-based methods have been developed more recently.
These methods either use pre-trained masked language models~\citep{ueoka2021frustratingly} or end-to-end encoder/decoder networks~\citep{abdelnabi2021adversarial}.


\subsection{Generation-time watermarking}\label{chap0/sec:generation-watermarking}

There is a growing need to track AI-generated content for transparency and for filtering such results when building new models.
In this context, some methods embed watermarks during the generation process such that it is directly in the generated content.
We call them \emph{generation-time}, as opposed to \emph{post-generation} (or \emph{post-hoc}) where the content is watermarked after it has been generated.
In generation-time watermarking, embedding the watermark does not incur additional runtime (particularly relevant for video), is easier (particularly for text), and is more robust and secure than post-hoc. 
We can broadly categorize generation-time watermarking into two categories:
\begin{itemize}[noitemsep]
    \item \emph{in-model}; embeds the watermark in the weights of the model,
    \item \emph{out-of-model}; embeds the watermark by altering the generation process.
\end{itemize}
Particularly, in-model watermarking allows for open-sourcing the model without revealing the watermark, conversely to out-of-model watermarking.
The latter is nevertheless easier to implement since it does not require training or fine-tuning the model

\paragraph*{Image.} 
The earliest methods~\citep{wu2020watermarking, yu2021artificial, zhao2023recipe} watermark the training set, which is computationally expensive and not scalable.
Alternatively, some methods~\citep{fei2022supervised, fei2024wide} train Generative Adversarial Networks (\Glspl*{GAN}) with additional watermarking losses such that generated images contain the watermark. 
Similarly, our Stable Signature work (Chap.~\ref{chapter:stable-signature}) focuses on Latent Diffusion Models (\Glspl*{LDM}) and fine-tunes the latent decoder to embed the watermark, while \cite{feng2024aqualora} fine-tune the U-Net that predicts the latent diffusion noise instead.
To eliminate the need to train or fine-tune the model for every user, \cite{yu2022responsible, fei2023robust} predict modifications to apply to GANs using a hyper-network and \cite{kim2024wouaf} apply a similar approach upon Stable Signature. 
\cite{ci2024wmadapter} and \cite{rezaei2024lawa} propose other variants that use an adapter to the decoder which takes the secret message as input.
The last two methods thus operate out-of-model in contrast to previously mentioned techniques that are in-model.

A different class of out-of-model methods, specific to diffusion models, embed the watermark by adding patterns to the initial noise. 
For example, Tree-Ring~\citep{wen2023tree} adds tree-ring-shaped (the name of the method is well chosen!) patterns to the initial noise and inverts the diffusion process to extract the watermark. 
As follow-up works, \cite{hong2024exact} improve the inversion of the diffusion process, \cite{ci2024ringid} extend the method to multi-bit watermarking and \cite{lei2024diffusetrace} use an encoder/decoder framework to embed and extract watermarks in the initial noise.

\paragraph*{Audio.} 
Most of the aforementioned literature could be extended to audio generation-based watermarking, although at the time of writing, this literature is scarce and only includes the following works.
\cite{san2024latent} watermark the training data robustly to the audio tokenization.
\cite{zhou2024traceablespeech} watermark in the latent space of a speech synthesis model.
\cite{juvela2023collaborative} use a GAN-like approach to watermark a HiFi-GAN model: it optimizes a speech generator and a collaborator, which tries to detect the watermark on the generated speech while not detecting it on the real speech.

\paragraph*{Text.} 
Literature on text generation-based watermarking is the most prominent. 
It appeared as soon as 2011 with the work of \cite{venugopal2011watermarking}, which was presumably used in Google Translate to filter out automated translations from future training data.
For text generated by Large Language Models (\Glspl*{LLM}), two concurrent methods were developed soon after the release of ChatGPT.
Both methods alter the token generation based on a secret key and a hash of some previous tokens.
\cite{kirchenbauer2023watermark} modify the probability distribution by adding a bias to the scores of one part of the vocabulary, while \cite{aaronson2023watermarking} modify the sampling method with the Gumbel trick.
These methods are detailed in Sec.~\ref{chap0/sec:llm_wm}.

They have been followed by many other works.
For instance, the work described in Chap.~\ref{chapter:three-bricks} consolidates these methods with better statistical tests and an extension to multi-bit watermarking. 
\cite{yoo2023advancing, yoo2023robust, qu2024provably} introduce further methods for multi-bit watermarking.
\cite{christ2023undetectable, kuditipudi2023robust} build methods based on the same sampling as \cite{aaronson2023watermarking}. 
They manage the pseudorandomness differently, by using a seed that is a function of the token's position in the text.
\cite{christ2023undetectable, huang2023optimal} show that detectability depends on the entropy of the generated text and suggest to embed watermarks in entropic passages, particularly for code~\citep{lee2023wrote}.
Other approaches focus on ``semantic'' watermarks, which rely on the semantic representation of the past text~\citep{liu2023semantic, liu2024adaptive, fu2024watermarking, hou2023semstamp, hou2024k}, enhancing the watermark's robustness to text alterations.
\cite{giboulot2024watermax} fix the watermark detection algorithm and choose one chunk of text among multiple ones to maximize the detection.
\cite{piet2023mark, pan2024markllm} introduce toolkits to easily benchmark watermarking methods and analyze their robustness.

All the aforementioned methods are out-of-model, since a user with access to the model can choose to generate text with or without the watermark.
There are few in-model methods in comparison.
\cite{gu2023learnability} distill the previous watermarking methods in the model itself via fine-tuning.
\cite{xu2024learning} embed the watermark with reinforcement learning methods popular for instruction fine-tuning~\citep{ouyang2022training}, by using a paired language model detector as reward model.

Watermarking generated content is therefore also very close to coverless (or generative) \gls*{steganography}, which generates stego content without the need for a cover.
It is very popular for language models~\citep{fang2017generating, yang2018rnn, ziegler2019neural, yang2020vae, cao2022generative}, and operates in a very similar way to LLM watermarking but with a focus on secrecy rather than robustness.
Some works on images also address the topic~\citep{hu2018novel, volkhonskiy2020steganographic, liu2022image, zhou2023generative}.









