
\chapter{Watermarking Makes Language Models Radioactive}\label{chapter:radioactive}


This chapter is based on the paper \fullcite{sander2024watermarking}.

It is challenging for AI companies to ensure that their models -- which cost thousands if not millions -- are used in compliance with licensing agreements and prevent theft, because their complexity and lack of transparency make it difficult to track their usage. 
In this chapter, we investigate the \emph{radioactivity} of text generated by large language models (LLM), \ie,  whether it is possible to detect that such synthetic input was used as training data.
Current methods like membership inference or active IP protection either work only in confined settings (\eg, where the suspected text is known) or do not provide reliable statistical guarantees.
We discover that, on the contrary, LLM watermarking as presented in the previous Chap.~\ref{chapter:three-bricks} allows for reliable identification of whether the outputs of a watermarked LLM were used to fine-tune another language model.
Our new methods, specialized for radioactivity, detects with confidence weak residuals of the watermark signal in the fine-tuned LLM.
We link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process.
We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (\pval\ $< 10^{-5}$) even when as little as $5\%$ of training text is watermarked.
Code is available at \url{github.com/facebookresearch/radioactive-watermark}.

\newpage
\input{chapter-6/sections/1-intro.tex}
\input{chapter-6/sections/2-background.tex}
\input{chapter-6/sections/3-radioactivity.tex}
\input{chapter-6/sections/4-experiments.tex}
\input{chapter-6/sections/5-analyzes.tex}
\input{chapter-6/sections/7-approaches.tex}
\input{chapter-6/sections/6-additional.tex}
\input{chapter-6/sections/8-conclusion.tex}

