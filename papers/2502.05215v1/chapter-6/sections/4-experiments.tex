\renewcommand{\aux}[1]{{\scriptsize \textcolor{gray}{$\pm$#1}}}

\section{Radioactivity in instruction datasets}
\label{chap6/sec:instruction}


This section considers a realistic scenario where a pre-trained LLM $\mathcal{B}$ is instruction fine-tuned on instruction/answer pairs generated by $\mathcal{A}$.
It shows that watermarked instructions are radioactive and compares the confidence of our different detection methods.




\subsection{Experimental setup of the instruction tuning}\label{chap6/sec:exp_setting}

\input{chapter-6/figs/self-instruct-examples.tex}


\paragraph*{Instruction data generation.} 
We follow the Self-Instruct protocol~\citep{wang2022self} with $\mathcal{A}$=Llama-2-chat-7B~\citep{touvron2023llama2}.
We prompt the model with an instruction followed by three examples of instruction/answer pairs and ask it to generate the next $20$ instruction/answer pairs.
The sampling from the LLM logits is done with or without the watermarking method of~\citet{kirchenbauer2023watermark}, at logit bias $\delta=3.0$, proportion of greenlist tokens $\gamma=0.25$, and $k=2$. 
In both cases, we use nucleus sampling~\citep{holtzman2019curious} with $p=0.95$ and $T=0.8$.
We post-process the generated data to remove unfinished answers and near-duplicate instructions.
This yields a dataset of 100k instruction/answer pairs ($\approx$14M tokens\footnote{
    For comparison, the dataset used in Alpaca is $\approx$6M tokens, Alpaca-GPT4 $\approx$10M tokens, and OASST1 $\approx$20M tokens.
}) for both cases.
\autoref{chap6/fig:self-instruct-examples} shows an example of prompt used to create the instructions and the completion by the LLM.


Finally, we create six mixed datasets with $\rho$ \% of watermarked data (with $\rho \in \{ 0, 1, 5, 10, 50, 100\}$), filling the rest with non-watermarked instructions.
Thus, the total number of instructions is fixed at around 14M tokens, but the proportion of watermarked ones (which represents $\A$'s outputs) varies.



\paragraph*{Fine-tuning.} 
We train $\mathcal{B}$ on these six datasets, closely following the approach of Alpaca~\citep{alpaca}:
we use AdamW~\citep{loshchilov2017decoupled} for 3000 steps, with a batch size of 8, a learning rate of $10^{-5}$ and a context size of 2048 tokens (which results in 3 training epochs).
The learning rate follows a cosine annealing schedule~\citep{loshchilov2017sgdr} with 100 warmup steps.
We set $\mathcal{B}$=Llama-1-7B~\citep{touvron2023llama}, a model trained on different datasets than $\mathcal{A}$=Llama-2, to avoid biases that could arise if the same base model were also used for fine-tuning.



\subsection{Evaluation of the watermarking and instruction tuning}\label{chap6/sec:quality-inspection}


Alice's watermarking hyperparameters aim 
at generating high-quality instructions and ensuring that the watermark can be detected even in small text segments:
the watermark window size is $k=2$, sufficiently wide to eliminate biases yet narrow enough to make the watermark robust to edits;
$\delta=3$ yields high-quality text while ensuring that the watermark can be detected with a \pval\ of $10^{-6}$ on approximately 100 tokens.
Full results are in Tab.~\ref{chap6/tab:original-wm-evaluation}.


We inspect the outputs of the fine-tuned model $\mathcal{B}$ both qualitatively in Fig.~\ref{chap6/fig:example_answers_main} and quantitatively in Tab.~\ref{chap6/tab:nlp_bench}.
We report 0-shot scores for an evaluation setup close to that of Llama: exact match score for Natural Questions~\citep{kwiatkowski2019natural} and TriviaQA~\citep{joshi2017triviaqa}; 0-shot exact match score without majority voting for GSM8k~\citep{cobbe2021training}; pass@1 for HumanEval~\citep{chen2021Evaluating}; and accuracy on MMLU~\citep{hendrycks2020measuring}.
As expected, instruction-tuning does not affect most benchmarks while enhancing it for MMLU, as in \citep{dettmers2023qlora}.
We also see that watermarking does not significantly impact the fine-tuned model performance, and the proportion of watermarked data does not correlate with it (it increases and decreases with $\rho$ depending on the benchmark).


\begin{table}[t!]
    \centering
    \caption{
        Summary statistics (mean and standard deviation) of $\logpval$ of the watermark detection for different ranges of number of tokens.
        Texts were generated with Llama-2-chat-7B and the method of~\cite{kirchenbauer2023reliability}, with $\delta=3.0$, $\gamma=0.25$, $k=2$, and each range contains $\approx$500 texts.
        These texts are later used to fine-tune Llama-1-7B.
    }
    \label{chap6/tab:original-wm-evaluation}
    \footnotesize
        \begin{tabular}{l *{7}{r}}
        \toprule
        Range &  \rotatebox{45}{(50, 150]} &  \rotatebox{45}{(150, 250]} &  \rotatebox{45}{(250, 350]} &  \rotatebox{45}{(350, 450]} &  \rotatebox{45}{(450, 550]} &  \rotatebox{45}{(550, 650]} &  \rotatebox{45}{(650, 750]} \\
        \midrule
        Mean &       -7.2 &       -11.7 &       -15.9 &       -18.4 &       -21.0 &       -24.0 &       -26.6 \\
        Std  &        3.6 &         5.6 &         7.2 &         8.8 &        10.2 &        12.6 &        14.2  \\
        \bottomrule
        \end{tabular}
\end{table}


\begin{table}[t!]
    \centering
    \caption{
        Evaluation of Llama-7B fine-tuned with varying proportions of watermarked instruction data.
        The influence of the proportion of watermarked data is minimal on the model's performance, and without direct correlation.
    }
    \label{chap6/tab:nlp_bench}
    \footnotesize 
    \begin{tabular}{l |*{5}{l} | l}
        \toprule
        {} & \small{NQ} & \small{TQA} & \small{GSM8k} & \small{H.Eval} & \small{Avg.} & \small{MMLU}\\
        \midrule 
        \multicolumn{7}{l}{\small{\textit{Fine-tuned with $\rho$ \% of watermarked data:}}} \\
        $0\%$ & $5.0$ & $33.6$ & $11.8$ & $12.8$ & $15.8$ & $33.6$ \\
        $5\%$ & $5.2$ & $35.7$ & $11.2$ & $11.6$ & $15.9$ & $34.7$ \\
        $50\%$ & $4.1$ & $35.5$ &  $9.6$ & $12.8$ & $15.5$ & $35.0$ \\
        $100\%$ & $5.6$ & $36.4$ & $11.1$ &  $9.8$ & $15.7$ & $31.0$ \\
        \midrule
        Base & $3.2$ & $36.2$ & $10.5$ & $12.8$ & $15.7$ & $28.4$ \\
        \bottomrule
    \end{tabular}
\end{table}



\begin{figure}[b!]
    \centering
    \begin{tcolorbox}[colframe=metablue, colback=white]
        \footnotesize
        \begin{minipage}{0.47\textwidth}
            \textbf{Question:} ``Name 3 things that you can do to help protect the environment.''
            \\[4pt]
            \textbf{Llama-1 answer - no fine-tuning:}\newline
            The three things that I can do to help protect the environment are: recycle, turn off water while you are brushing your teeth, and plant a tree. Response Rubric: 
            / Exceeds Criterion 
            / Meets Criterion 
            / Exceeds Criterion (exceeds the criteria by a significant amount) 
            / Meets Criterion (meets the criteria, with a little bit of room to improve) 
            [Text continues...]
        \end{minipage}\hspace{0.04\textwidth}%
        \begin{minipage}{0.49\textwidth}
            \textbf{Llama-1 answer - fine-tuning with $\rho$=$0\%$:}\newline
            1. Reduce, Reuse, Recycle. \newline 2. Use energy-efficient appliances and lighting \newline 3. Use public transportation or walk/bike when possible.
            \\[4pt]
            \textbf{Llama-1 answer - fine-tuned with $\rho$=$10\%$:}
            \newline
            Recycle more, use public transportation, and reduce meat consumption.
            \\[4pt]
            \textbf{Llama-1 answer - fine-tuned with $\rho$=$100\%$:}
            \newline
            Recycle, use public transportation, and reduce energy consumption.
        \end{minipage}
    \end{tcolorbox}
    \caption{
        Answers generated from Bob's model $\B$ (Llama-1), fine-tuned on instruction data generated by Alice's model $\A$ (Llama-2-chat) with different proportions $\rho$ of watermarked data.
        The quality of the instruction-tuning is not affected by the watermarking of the data (examples of training instruction/answer pairs are in Fig.~\ref{chap6/fig:self-instruct-examples}).
    }
    \label{chap6/fig:example_answers_main}
\end{figure}










\subsection{Experimental setup of the detection}\label{chap6/sec:detection-setup}

In the \textbf{open-model} setting, we use a set of watermarked instructions generated by Alice's model $\A$ to score $N=225$k tokens.
For the supervised setting ($d=1$), we directly use all the $\rho\%$ watermarked texts among the 100k instructions used to train $\B$; for the unsupervised setting ($d=0$), we use watermarked instructions unused by $\B$.
We use the ``reading mode'' for detection (see Sec.~\ref{chap6/sec:radioactivity_detection}).

In the \textbf{closed-model} setting, Bob's model $\B$ is only accessible via an API that generates answers from prompts.
We prompt $\B$ with instructions generated by $\A$, concatenate all the answers, and score $N=600$k tokens, after filtering and de-duplicating the $k$-grams of $\approx1.5$M generated tokens.
This represents around $10^4$ queries if we assume an answer is $100$ tokens.
We score a token only if its previous $k$-gram is part of filter $\phi$. 
In the supervised setting ($d>0$), we collect the watermarked prompts/answers from $\Tilde{D}^\A$, part of which were used for fine-tuning, and define $\phi$ as the set of all $k$-grams.
In the unsupervised setting, we generate $100$k new watermarked instructions with $\A$ and save all $k$-grams into $\phi$.
In both cases, we run the detection $10$ times on different chunks of text and report averaged results with standard deviations.



\subsection{Detection results}

\paragraph{Proportion of watermarked data and access to Bob's model and data.}


\begin{figure}[b!]
    \centering
    \hspace{-0.5cm}
    \includegraphics[width=0.55\linewidth, clip, trim=0 0 0 0]{chapter-6/figs/240122_pierre_2_neurips.pdf}
    \caption{
        Radioactivity detection results.
        Average of $\logpval$ over 10 runs ($\downarrow$ is better). Bars indicate standard deviations.
        The detection methods are detailed in Sec.~\ref{chap6/sec:radioactivity_detection}.
        In the supervised closed-model setting, our tests detect radioactivity ($p<10^{-5}$) when only $1\%$ of training data are watermarked.
        In the absence of watermarked data, all tests output random \pval s.
    }
    \label{chap6/fig:wm-proportion}
\end{figure}


\autoref{chap6/fig:wm-proportion} first presents the \pval s of our tests for different proportions $\rho$, under the 4 possible scenarios, and shows that the detection confidence increases with the proportion of watermarked data, and with Alice's knowledge.

The supervised setting ($d=1$) is straightforward: radioactivity is detected with a \pval\ smaller than $10^{-30}$ (resp. $10^{-10}$) in the open-model (resp. closed-model) setting, even if only $1\%$ of Bob's fine-tuning data originated from $\A$.
Indeed, with open-model access, we only score 1) $k$-grams that can actually be contaminated and 2) within a context that matches the one seen during fine-tuning.
In the closed-model access, prompting $\B$ with its training questions enhances radioactivity detection, as its answers are likely to reproduce the watermarked responses from $\A$ used during training.

In the unsupervised setting ($d=0$), our open-model radioactivity detection test still yields $p<10^{-5}$ when no more than $5\%$ of the instructions used to fine-tune $\B$ originate from Alice's model.
The detection is done on a corpus of texts that does not contain samples seen by Bob at training time.
However, it contains watermark windows that likely overlap with Bob's training data, on which radioactivity may be detected.
In the closed-model setting, the detection is less powerful: it requires a higher proportion of watermarked data to be detected with a \pval\ smaller than $10^{-5}$.











\paragraph{Influence of the degree of supervision.}
To study the intermediate regime of weak supervision, we fix $\rho=5\%$ in the open-model detection. 
We vary the supervision degree $d$ by mixing the watermarked data used for fine-tuning with new watermarked data generated by $\A$ in a ratio $d$. 
We then run the radioactivity detection test with the reading mode on this corpus.
\autoref{chap6/tab:wm_supervision} shows that the detection confidence increases with the supervision degree (from $-5.8$ to $<-30$ when $d$ goes from $0.1\%$ to $100\%$).
Even for very weak supervision, the detection is still effective, with a \pval\ smaller than $10^{-5}$ when $d=0.1\%$.
This is in stark contrast with MIA-based methods for which supervision is necessary (we detail this in Sec.~\ref{chap6/par:mia_wm}).

\begin{table}[t]
    \centering
    \caption{
        Detection confidence $\logpval$ with varying supervision $d$ at $\rho=5\%$ of $\B$'s training data from $\A$, in the \textit{open}-model setting using the reading mode for detection.
    }
    \label{chap6/tab:wm_supervision}
    \footnotesize 
    \begin{tabular}{r *{4}{c}}
        \toprule
        Supervision degree $d$ &$0.1\%$ & $1\%$ & $5\%$ & $10\%$ \\
        $\logpval$ & $-5.8$\aux{1.8} & $-6.5$\aux{0.9} & $-16.0$\aux{2.6} & $<-30$ \\
        \bottomrule
    \end{tabular} 
\end{table}





\paragraph{Influence of the filtering in the closed-model setting.}

As explained in~\autoref{chap6/sec:radioactivity_detection}, the watermark traces in $\B$ are better detected in the $k$-grams that are part of $D^\A$.
To enhance detection, we therefore define a set $\phi$ of $k$-grams likely to have been trained on.
Tokens are only scored if their preceding $k$-gram window (the watermark context window used for hashing) is part of $\phi$. 
This approach concentrates the score computation on $k$-grams where the watermark could potentially be learned.

\begin{figure}[b!]
    \centering
    \hspace{-0.2cm}
    \includegraphics[width=0.55\linewidth]{chapter-6/figs/filter_vs_non_arxiv.pdf}
    \caption{
        Influence of the filter on scored tokens.
        $\logpval$ as a function of the number of generated tokens in the supervised closed-model setting with $\rho = 1\%$. 
        We perform the watermark detection test on text generated by $\B$ with prompts from $\Tilde{D}^\A$. 
        When filtering, we only score $k$-grams that were part of $\Tilde{D}^\A$.
    }
    \label{chap6/fig:filter_non_filter_nbtok}
\end{figure}

\autoref{chap6/fig:filter_non_filter_nbtok} compares detection with and without filtering when $1\%$ of fine-tuning data are watermarked in the supervised closed-model setting.
We plot the $\log_{10}(p\textrm{-value})$ against the number of generated tokens.
As expected, the detection confidence increases with the number of tokens.
Moreover, filtering consistently brings improvements: after scoring $75000$ tokens, the $\logpval$ equals $-12$ with filter and $-8$ without.




\paragraph{Influence of the de-duplication on the correctness of radioactivity tests.}\label{chap6/par:dedup-expe}
For a statistical test to be valid, the \pval\ should be uniformly distributed between 0 and 1 under the null hypothesis $H_0$ (mean $0.5$, standard deviation of $\approx0.28$)\footnote{
    Note: This applies when the test statistic follows a continuous distribution. 
    For the binomial distribution, this is a good approximation when the sample size is large, which is relevant in our case as we score many tokens.
}.
Accurate theoretical \pval s are particularly important in the common case where obtaining samples of fine-tuned $\B$s is expensive. 
This cost limits the sample size, reducing the power of empirical tests and compromising the confidence in their results. 

We validate our tests and highlight the importance of de-duplication by observing the empirical \pval\ distribution after scoring 1M tokens when $\B$ is not trained on watermarked data ($H_0$). 
We first run 10 detection tests when scoring distinct $\{$watermarked window + current token$\}$ combinations, as suggested by~\citep{kirchenbauer2023watermark, fernandez2023three}. 
This approach is insufficient on its own, as shown in Tab.~\ref{chap6/tab:pval_h0} in the ``without de-duplication'' column.
For instance,  in the closed-model setting, the average $\pval$ is inferior to $10^{-30}$ even if the model does not output watermarked texts.
This is a strong false alarm, \ie,  incorrectly rejecting the null hypothesis.
The additional de-duplication rules (Sec.~\ref{chap6/sec:radioactivity_detection}) resolve this issue. 
We observe in the ``with de-duplication'' column that the empirical \pval s match the theoretical ones, indicating that the test results are valid and reliable. 









\subsection{Intermediate summary}
Our watermark-based radioactivity detection methods can identify $10^{-5}$-radioactivity in model $\B$ across various setups. 
Even in the most realistic scenario -- unsupervised access to Bob's data -- our method holds true with only closed access to $\B$, given that at least $10\%$ of the data is watermarked. 
Open-model access further enhances the test's statistical significance, detecting radioactivity with a \pval\ smaller than $10^{-10}$ when $10\%$ of the fine-tuning data is watermarked (Fig.~\ref{chap6/fig:wm-proportion}).



\begin{table}[t!]
    \begin{minipage}{0.44\linewidth}
        \centering
        \renewcommand{\arraystretch}{1.1}
        \caption{
        Average \pval s under $H_0$ ($\B$ \emph{not} trained on watermarked data: $p$ should be 0.5). 
        In the open-model setting (resp. closed), we exclude a token if the same watermark window is already present in the attention span (resp. in the watermarked prompt).
        Without de-duplication, \pval s are overly low: the test does not work.
        }\label{chap6/tab:pval_h0}
        \footnotesize %
        \begin{tabular}{l @{\hskip8pt} c c}
            \toprule
            & \multicolumn{2}{c}{De-duplication} \\
            \cmidrule(rr){2-3}
            Access to Model & With & Without \\
            \midrule
            Open & 0.46\aux{0.27} & 0.053\aux{0.12} \\
            Closed & 0.42\aux{0.30} & $<10^{-30}$ \\
            \bottomrule
        \end{tabular}
    \end{minipage} \hfill
    \begin{minipage}{0.53\linewidth}
    \centering
        \caption{
            Influence of watermarking method and $k$ on radioactivity. Average $\log_{10}$ \pval s.
            ``Orig'' denotes watermark detection of texts used for training (100 tokens); 
            ``Rad'' denotes radioactivity detection in closed-model setting ($N$=$30$k, $\rho$=$100\%$).
            Both KGW~\citep{kirchenbauer2023reliability} and AK~\citep{aaronson2023watermarking} behave the same way and lower $k$ increases radioactivity. 
        }\label{chap6/tab:exp_kgram}
        \footnotesize %
        \renewcommand{\arraystretch}{1.1}
        \begin{tabular}{r r c c c}
            \toprule
           \multicolumn{2}{c}{Window Size $k$} &  1 & 2 & 4 \\ %
           \hline 
           \multirow{2}{*}{KGW} & Orig & -8.6\aux{4.4} & -6.4\aux{3.9} & -6.7\aux{4.0} \\
                                & Rad & -43.7\aux{7.4} & -10.2\aux{3.0} & -1.4\aux{0.6} \\
           \hline
           \multirow{2}{*}{AK}  & Orig & -7.7\aux{4.5} & -7.6\aux{5.1} & -7.1\aux{5.1} \\
                                & Rad & -47.2\aux{4.5} & -18.4\aux{2.8} &  -2.8\aux{3.2} \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}
