

\section{Discussion on other approaches}\label{chap6/sec:discussion-other-approaches}



\subsection{Membership inference}\label{chap6/par:mia_wm}

\paragraph*{Method.}
In the open-model/supervised case, MIA evaluates the radioactivity of one sample/sentence by observing the loss (or perplexity) of $\B$ on carefully selected sets of inputs.
The perplexity is expected to be smaller on samples seen during training.
We extend this idea for our baseline radioactivity detection test of a non-watermarked text corpus.
The corpus of texts is divided into sentences (of 256 tokens) and $\B$'s loss is computed on each sentence. 
We calibrate it with the zlib entropy~\citep{roelofs2017zlib}, as done by \citet{carlini2021extracting} for sample-based MIA. 
The goal of the calibration is to account for the complexity of each sample and separate this from the over-confidence of $\B$. 

We test the null hypothesis $\mathcal{H}_0$: ``\textit{the perplexity of $\B$ on $\Tilde{D}^{\A}$ has the same distribution as the perplexity on new texts generated by $\A$}''.
Indeed, if $\B$ was not fine-tuned on portions of $\Tilde{D}^\A$, then necessarily $\mathcal{H}_0$ is true.
To compare the empirical distributions we use a two-sample Kolmogorov-Smirnov test~\citep{massey1951kolmogorov}. 
Given the two cumulative distributions $F$ and $G$ over loss values, we compute the K-S distance as $d_{\mathrm{KS}}(F,G) = \mathrm{sup}_x |F(x) -G(x)|$.
We reject $\H_0$ if this distance is higher than a threshold, which sets the \pval\ of the test, and conclude that $\Tilde{D}^{\A}$ is radioactive for $\B$.
This is inspired by \citet{sablayrolles2018d}, who perform a similar K-S test in the case of image classification. 
It significantly diverges from the approach of~\citet{shi2023detecting}, which derives an empirical test by looking at the aggregated score from one tail of the distribution. 

\paragraph*{Experimental results}

We proceed as in Sec.~\ref{chap6/sec:radioactivity_detection} for the setup where MIA is achievable:
Alice has an \emph{open-model} access to $\B$ and is aware of all data $\Tilde{D}^\A$ generated for Bob (supervised setting). 
Bob has used a portion $D^\A$ for fine-tuning $\B$, given by the degree of supervision $d$, as defined in Sec.~\ref{chap6/sec:degreeofsupervision}.
We use the K-S test to discriminate between the calibrated perplexity of $\B$ on: $\mathcal D_{(0)}$ containing 5k instruction/answers (cut at 256 tokens) that were not part of $\B$'s fine-tuning; and $\mathcal D_{(d)}$ containing $(1/d)\times$5k instruction/answers from which $5k$ were.
Distribution $\mathcal D_{(d)}$ simulates what happens when Bob generates a lot of data and only fine-tunes on a few.


\definecolor{curve1}{HTML}{8B188B}
\definecolor{curve2}{HTML}{FFA319}
\begin{figure}[b!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{chapter-6/figs/MIA_ft_vs_noft.pdf}
        \caption{
            Distributions of the calibrated loss of $\B$ across two types of distributions generated by $\A$: 
            texts generated by $\A$ outside of $\B$'s fine-tuning data ({\color{curve1} purple}), texts of $\Tilde{D}^{\A}$ of which $d\%$ were used during training ({\color{curve2} orange}).
        }
        \label{chap6/fig:calibrated-loss}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth,clip, trim=0 0 0 0cm]{chapter-6/figs/MIA_vs_WM.pdf}
        \caption{
            We report the \pval s of the {\color{curve1} K-S detection test} (no WM when training) and of the {\color{curve2} WM detection} ($\rho=5\%$ of WM when training) against the degree of supervision $d$ (proportion of Bob's training data known to Alice).
        }
        \label{chap6/fig:mia_vs_wm}
    \end{subfigure}
    \caption{
        Comparative analysis of membership inference and watermarking for radioactivity detection, in the open-model setup.
        (\emph{Left}) MIA aims to detect the difference between the two distributions. 
        It gets harder as $d$ decreases, since the actual fine-tuning data is mixed with texts that Bob did not use.
        (\emph{Right}) Therefore, for low degrees of supervision ($<2\%$), MIA is no longer effective, while WM detection gives \pval s lower than $10^{-5}$.
    }
    \label{chap6/fig:mia-comparative-analysis}
\end{figure}

\autoref{chap6/fig:calibrated-loss} compares the distributions for $d=0$ and $d>0$. 
As $d$ decreases, the data contains more texts that Bob did not fine-tune on, so the difference between the two perplexity distributions is fainter.
The direct consequence is that the detection becomes more challenging.
\autoref{chap6/fig:mia_vs_wm} shows that when $d>2\%$, the test rejects the null hypothesis at a strong significance level: 
$p < 10^{-5}$ implies that when radioactive contamination is detected, the probability of a false positive is $10^{-5}$.
It is random in the edge case $d=0$, the unsupervised setting where Alice lacks knowledge about the data used by Bob. 
In contrast, radioactivity detection on watermarked data succeeds in that setting.









\subsection{IP protection methods}\label{chap6/sec:ipp}

There are active methods that embed watermarking in a text specifically to detect if a model is trained on it~\citep{zhao2023protecting,he2022cater, he2022protecting}. 
Our setup is a bit different because Alice's primary goal is not to protect against model distillation but to make her outputs more identifiable. 
Consequently, ``radioactivity'' is a byproduct.
Although our focus is not on active methods, we discuss three state-of-the-art methods for intellectual property protection and their limitations in our detection setting.

\paragraph{\citet{zhao2023protecting}.}
The goal is to detect if a specific set of answers generated by Alice's model has been used in training. 
There are three main limitations.
The authors design a watermark dependent on the previous prompt (and unique to it). 
Each detection algorithm (2 and 3) assumes that the sample probing data $D$ is from the training data of the suspect model $\B$.
Therefore, the method is only demonstrated in the \textit{supervised} setting.
Moreover, all experiments are performed in the \textit{open}-model setting, where Alice has access to $\B$'s weights, except for Sec.~5.2 ``Watermark detection with text alone'' (still assuming a \textit{supervised} access), where one number is given in that setting.
Finally, it does not rely on a grounded statistical test and requires an empirically set threshold.

\paragraph{\cite{he2022protecting} and \cite{he2022cater}.} 
These methods substitute synonyms during generation to later detect an abnormal proportion of synonyms in the fine-tuned model.
However, the main hypothesis for building the statistical test -- the frequency of synonyms in a natural text is fixed -- fails when scoring a large number of tokens.
Using the \href{https://github.com/xlhex/NLG_api_watermark}{official author's code} on non-watermarked instruction-answers yields extremely low \pval s, making the test unusable at our scale, as shown in \autoref{chap6/table:pvalues_ginsew}.

\begin{table}[t!]
\centering
\caption{\pval s for non-watermarked instruction-answers}
\label{chap6/table:pvalues_ginsew}
\footnotesize
\begin{tabular}{ *{3}{l} }
    \toprule
    Number of lines & Number of characters (in thousands) & \pval \\
    \midrule
    10 & 1.5 & 0.76 \\
    100 & 30.9 & 0.16 \\
    500 & 148.3 & 2.2 $\times 10^{-7}$ \\
    1000 & 333.9 & 8.8 $\times 10^{-13}$ \\
    \bottomrule
\end{tabular}

\end{table}
