
\section{Conclusion}

This chapter formalizes the concept of ``radioactivity'' in language models.
It introduces methods to detect traces that LLM-generated texts leave when used as training data.
This makes it possible to identify with high confidence if outputs from a watermarked model have been used to fine-tune another one.

There are (at least) two limitations that are worth mentioning. 
First, we do not extensively discuss how attempts to remove the original watermark may impact radioactivity, except in Sec.~\ref{chap6/ref:purification} where Bob re-fine-tunes his model. 
A skilled attacker may be able to eliminate these traces. 
Second, the computational efficiency of the algorithms is linear in the number of tokens to analyze and depends on the suspected LLM. 
In the closed-model setting, about 1000 queries to the model are needed to confidently detect radioactivity, which could be a limitation if the suspect model is only accessible through an API.







