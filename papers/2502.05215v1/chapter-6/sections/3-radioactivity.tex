
\section{Problem formulation}

\emph{Alice} owns a language model $\A$, fine-tuned for specific tasks such as chatting, problem solving, or code generation, which is available through an API (\autoref{chap6/fig:fig1}).
\emph{Bob} owns another language model $\B$.
Alice suspects that Bob fine-tuned $\B$ on some outputs from $\A$.
We denote by $D$ the dataset used to fine-tune $\B$, among which $D^{\A}\subset D$ is made of outputs from $\A$, in proportion $\rho = |D^{\A}|/|D|$.

\paragraph*{Access to Bob's data.} 
\label{chap6/sec:degreeofsupervision}

We consider two settings for Alice's knowledge about Bob's training data:
\begin{itemize}[leftmargin=0.5cm, itemsep=2pt, topsep=1pt]

    \item \emph{supervised}: Bob queries $\A$ and  Alice retains all the content $\Tilde{D}^{\A}$ that $\A$ generated for Bob. Thus, Alice knows that $D^{\A} \subseteq \Tilde{D}^{\A}$. 
    We define the \emph{degree of supervision} $d := |D^{\A}|/|\Tilde{D}^{\A}|$,  

    \item \emph{unsupervised}: Bob does not use any identifiable account or is hiding behind others such that $|\Tilde{D}^{\A}| \gg |D^{\A}|$ and $d \approx 0$.
    This is the most realistic scenario.
\end{itemize}

Thus, $\rho$ is the proportion of Bob's fine-tuning data which originates from Alice's model
while $d$ quantifies Alice's knowledge regarding the dataset that Bob may have utilized (see Fig.~\ref{chap6/fig:datasets}).

\paragraph*{Access to Bob's model.} 
We consider two scenarios:
\begin{itemize}[leftmargin=0.5cm, itemsep=2pt, topsep=1pt]
    \item Alice has an \emph{open-model} access to $\B$. 
    She can forward any inputs through $\B$ and observe the output logits.
    This is the case if Bob open-sources $\B$, or if Alice sought it via legitimate channels.
    \item Alice has a \emph{closed-model} access. 
    She can only query $\B$ through an API without logits access: Alice only observes the generated texts.
    This would be the case for most chatbots.
\end{itemize}

We then introduce two definitions of radioactivity:
\begin{definition}[Text Radioactivity]\label{chap6/def:text_radioactivity}
    Dataset $D$ is $\alpha$-radioactive for a statistical test $T$ if ``$\B$ was not trained on $D$'' $\subset \H_0$ and
    $T$ is able to reject $\H_0$ at a significance level (\pval) smaller than $\alpha$.
\end{definition}

\begin{definition}[Model Radioactivity]\label{chap6/def:model_radioactivity}
    Model $\A$ is $\alpha$-radioactive for a statistical test $T$ if
    ``$\B$ was not trained on outputs of $\A$'' $\subset \H_0$ and $T$ is able to reject $\H_0$ at a significance level smaller than $\alpha$.
\end{definition}

Thus, $\alpha$ quantifies the radioactivity of a dataset or model. 
A low $\alpha$, e.g. $10^{-6}$, indicates strong radioactivity: the probability of observing a result as extreme as the one observed, assuming that Bob's model was not trained on Alice's outputs, is 1 out of one million. 
Conversely, $\alpha\approx 0.5$ means that the observed result is equally likely under both the null and alternative (radioactive) hypotheses.



\begin{figure}[b]
   \begin{minipage}{0.62\textwidth}
        \centering
        \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{r ccc ccc ccc}
            \toprule
            & \multicolumn{2}{c}{With WM} & \multicolumn{2}{c}{MI} & \multicolumn{2}{c}{IPP} \\
            \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
            & Open & Closed & Open & Closed & Open & Closed \\
            Supervised & \cmarkg & \cmarkg &\cmarkg & \xmarkg & \cmarkg & \amark \\
            Unsupervised & \cmarkg & \cmarkg &\xmarkg & \xmarkg & \xmarkg & \xmarkg \\
            \bottomrule
        \end{tabular}
        }
        \captionof{table}{
            Availability of radioactivity detection under the different settings: 
            \cmarkg: available, \xmarkg: not available, \amark: available but with strong limitations.
            \textit{Open} / \textit{closed-model} refers to the availability of Bob's model, and \textit{supervised} / \textit{unsupervised} to Alice's knowledge of his data.
            Detection with watermarks is described in Sec.~\ref{chap6/sec:radioactivity_detection}, and other approaches relying on Membership Inference (MI) and Intellectual Property Protection (IPP) are detailed in Sec.~\ref{chap6/sec:discussion-other-approaches}.
        }
        \label{chap6/tab:summary_MIA_wm}
   \end{minipage}\hfill
    \begin{minipage}{0.34\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth, clip, trim=0.7cm 2.2cm 0.7cm 0]{chapter-6/figs/datasets.pdf}
        \captionsetup{font=small}
        \caption{
            Detection performance mainly depends on $\rho = |D^{\A}|/|D|$ and $d = |D^{\A}|/|\Tilde{D}^{\A}|$, where $D$ is the fine-tuning dataset used by Bob, $\Tilde{D}^{\A}$ are the outputs from Alice's model, and $D^{\A}$ the intersection of both.
        }
        \label{chap6/fig:datasets}
    \end{minipage}
\end{figure}













\section{Radioactivity detection}\label{chap6/sec:radioactivity_detection}


\subsection{Why current approaches are insufficient}

\paragraph{Membership inference and IPP methods.} 

Passive methods relying on membership inference observe the model perplexity on text belonging to the training dataset, compared to text not in the dataset.
They are effective only in the \textit{supervised} setting, where Alice has precise knowledge of the data used to train Bob's model and \textit{open} access to it. 
In that scenario, she can easily demonstrate $\alpha$-radioactivity, with $\alpha$ as low as $10^{-30}$, providing strong evidence that Bob has trained on her model.

In parallel, there are active methods, such as ones used for IPP, that explicitly modify the LLM generation to detect when the outputs are used as training data.
However, even state-of-the-art methods~\citep{zhao2023protecting, he2022protecting, he2022cater} fall short in the unsupervised setting, where the statistical guarantees do not hold in practice.

These claims are detailed and supported by experiments in Sec.~\ref{chap6/par:mia_wm} (for MIA) and Sec.~\ref{chap6/sec:ipp} (for IPP), and summarized in Tab.~\ref{chap6/tab:summary_MIA_wm}.
At the end of the day, the applicability of the methods is very restricted, and does not generalize to real-world scenarios.

\paragraph{Naive approach for watermark detection.} 
We now assume that the outputs of $\A$ are watermarked with a method $W$ with Alice's secret key $\sk$ as described in Sec.~\ref{chap6/sec:background}. 
The original watermark detector $T$ tests the null hypothesis $\mathcal{H}_0$: ``\textit{The text was not generated following $W$ with secret key $\sk$}'' by applying a scoring function $W_{\textrm{score}}$ keyed by $\sk$ on the text.
The output \pval\ depends on the score and the number of analyzed tokens.

Unlike watermark detection which takes text as input, the input for radioactivity detection is a model.
The naive approach to detect model radioactivity on $\B$ is to run test $T$ on a large corpus of texts generated by $\B$. 
This aligns with Def.~\ref{chap6/def:text_radioactivity}:
the text cannot be generated following $W$ and $\sk$ if $\B$ has never seen the watermark, so if ``\textit{$\B$ did not use outputs of $\A$}'', then $\H_0$ is true.
However, this detector is ineffective because 
(1) \emph{the watermark signal is very weak}, as radioactivity can only be observed for watermarked $(k+1)$-grams $\{$watermark window + current token$\}$ that were part of $\A$'s outputs in $\B$'s training data, therefore the signal is diluted in the generated text, and;
(2) \emph{theoretical \pval s break down} when computed naively from many tokens -- $\approx$ 1M, see the experiments of Sec.~\ref{chap6/par:dedup-expe} -- which is necessary to observe this very weak signal.

\subsection{Enhanced radioactivity detection}

\paragraph{Overview.} 
We use the detection test $T$ from text watermarking as presented in Sec.~\ref{chap6/sec:background}, but adapt the score computation (\autoref{chap6/fig:method}).
Our goal is to reduce noise by focusing Bob's model on watermarked windows likely to be radioactive. 
To this end, we recreate a context similar to the one that generated the watermarked text by Alice. 
We ensure the accuracy of statistical tests through de-duplication of scored tokens. 

\begin{figure*}[b!]
    \centering
    \includegraphics[width=1.0\textwidth, clip, trim=0 1.5cm 3.7cm 0]{chapter-6/figs/method.pdf}
    \caption{
    Radioactivity detection with closed or open model access (for simplicity, only \citep{kirchenbauer2023watermark} is illustrated).
    \textit{(Left)} New texts are generated from $\B$ using prompts from $\A$ and these texts are scored. 
    The filter $\phi$ is used to focus the score computation on likely contaminated $k$-grams. 
    \textit{(Right}) Texts generated by $\A$ are directly forwarded through $\B$, and the next-token predictions are scored using tokens from the input as the watermark window.
    In both cases, the \textit{tape} ensures reliable \pval s by de-duplicating scored tokens.
    }
    \label{chap6/fig:method}\label{chap6/fig:open_model}
\end{figure*}


\paragraph{Radioactivity detection in $\B$.}
To amplify the radioactivity signal, we employ two strategies.
(1) In the supervised setting, we use watermarked text from $\Tilde{D}^\A$. In the unsupervised setting, we use new watermarked text generated by $\A$ that aligns with the suspected training distribution of $\B$ (e.g., English dialogues).
(2) We score up to millions of tokens, orders of magnitudes more than usual.
The scoring depends on the access to $\B$:
\begin{itemize}[leftmargin=*, itemsep=0pt, topsep=0pt]
    \item \emph{closed-model}: we use the prompts to generate new texts from $\B$, and score these texts.
    \item \emph{open-model}, \aka, ``reading mode'': instead of generating completions with $\B$, we directly forward the texts generated by $\A$ through $\B$, as depicted in Fig.~\ref{chap6/fig:open_model}.
    We then score next-token predictions with $W_{\textrm{score}}$ by using tokens from the input as watermark window.
    Intuitively, it reproduces the right contexts, and allows us to study how $\B$ behaves on these watermarked windows rather than letting $\B$ generate tokens without any interesting signal.
\end{itemize}


\paragraph{Filter on scored $k$-grams.}
To further improve detection in the closed-model setting where the reading mode is not possible, we only score $(k+1)$-grams $\{$watermark window + current token$\}$ output by $\B$ for which the watermark window is often in $\A$â€™s watermarked outputs.
We thus introduce a filter $\phi$, a set that contains these watermark windows.
In the \emph{supervised} setting ($0<d\leq1$), $\phi$ is made of the $k$-grams present in $\Tilde{D}^\mathcal{A}$ (refer to Fig.~\ref{chap6/fig:datasets}).
In the \emph{unsupervised} setting, we focus on `likely' contaminated $k$-grams, \eg, $k$-grams appearing in (new) watermarked text generated by $\A$.

\paragraph{Token scoring and de-duplication.}
\autoref{chapter:three-bricks} demonstrates that detection tests can be empirically inaccurate due to biases in the natural distribution of tokens. 
This issue is more pronounced in our case, given the larger volume of tokens required for observing radioactivity. 
To mitigate this, we score a token only if the same $\{$watermark window + current token$\}$ combination has not been previously encountered.
Moreover, in the closed-model setting, we only score watermark windows ($k$-gram) that are not part of the (watermarked) prompt. 
In the open-model setting, tokens with watermarked windows previously present in the attention span are not scored. 
This is achieved by maintaining a \emph{tape} memory of all such $k$-grams combinations during detection.
These adjustments ensure reliable \pval s even when many tokens are analyzed (see Sec.~\ref{chap6/par:dedup-expe} and~\ref{chap6/app:correctness}).






