
\section{Related work and technical background}\label{chap6/sec:background}




\subsection{Related work}\label{chap6/subsec:related}

\paragraph{Watermarking for LLMs.}
A recent branch of watermarking methods for decoder-only LLMs modifies the probability distribution~\citep{kirchenbauer2023watermark} or directly the sampling algorithm to select the next token~\citep{aaronson2023watermarking, kuditipudi2023robust}.
These methods are presented in Sec.~\ref{chap0/sec:watermarking} and \ref{chap0/sec:llm_wm}.
More relevant to this chapter, \citet{gu2023learnability} distill the methods previously cited within the model weights, allowing LLMs to generate watermarked logits natively, which is key for open-source models.
In contrast, we focus on unintentional contamination: 
Alice and Bob in Fig.~\ref{chap6/fig:fig1} are not collaborating, and Bob consumes only a small proportion of watermarked data.
Moreover, we study the detection of a watermark in the model, not in the generated text.

\vspace{-0.5em}\paragraph{Membership inference attacks} 
(MIAs) aim to determine whether an arbitrary sample is included in a model's training data, with varying granularity on the adversary's knowledge~\citep{nasr2019comprehensive}.
Most of the time, the detection either build shadow models and observe a difference in their behavior~\citep{shokri2017membership, song2019auditing, hisamoto2020membership, mahloujifar2021membership} or directly observe the loss of the model~\citep{yeom2018privacy, sablayrolles2019white, watson2021importance, carlini2022membership}.
In the context of generative models, MIAs are intertwined with
\emph{dataset contamination} where one detects that an entire dataset
is part of the training data~\citep{shi2023detecting, golchin2023time}.
MIAs can violate the confidentiality of sensitive training and reveal training on copyrighted material or evaluation data (which undermines benchmark results).
MIA may be used for radioactivity detection, but with a strong limitation.
Since it focuses on specific pieces of text, Alice in Fig.~\ref{chap6/fig:fig1} has to record all the outputs of her LLM.
In contrast, our rationale is that watermarks carry a signal expected to be memorized at the corpus level due to its repetitive nature.

\vspace{-0.5em}\paragraph{IPP.} Watermarking can do Intellectual Property Protection (IPP). 
For instance, \citet{he2022cater,he2022protecting,li2023protecting} use lexical properties like synonyms whereas \citet{peng2023you} rely on backdoors. 
\citet{zhao2023protecting} develop a watermark dissuading model theft via distillation.
Yet its accuracy is empirical, it does not provide \pval s that align with empirical false positive rates.

\vspace{-0.5em}\paragraph{Radioactivity.}
\citet{sablayrolles2020radioactive} introduce the concept of \emph{radioactivity}: images are modified to leave a detectable trace in any classifier trained on them.
Our work studies the radioactivity of decoding-based LLM watermarks, which are primarily used to detect AI-generated text with proven accuracy.
We demonstrate that this form of radioactivity is reliably detectable across diverse settings.
Sec.~\ref{chap6/sec:discussion-other-approaches} details why existing IP protections or MIAs do not offer similar capabilities.





\subsection{Technical background and notations}\label{chap6:sec:llm_watermarking}

We focus on the two most popular methods~\citep{aaronson2023watermarking, kirchenbauer2023watermark}.
Please refer to \autoref{chap0/sec:llm_wm} for a full description of the methods and to \autoref{chapter:three-bricks} for a quicker summary and for the computation of the \pval s.
In particular, these methods provide a score function $W$ (described in Eq.~\ref{chap6/eq:watermark_score}), which takes as input the watermark window $(x^{(-k)},\dots, x^{(-1)} )$ and the current token $x^{(0)}$, and output a real number. 
The \pval s are based on the total number of tokens being scored and on the sum of all scores.






