

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth, clip, trim=0 0.5cm 0 0]{chapter-6/figs/fig1.pdf}
    \captionsetup{font=small}
    \caption{
    Bob fine-tunes his LLM on data with a small proportion coming from Alice's LLM.
    This leaves traces in Bob's model that Alice can detect reliably when her texts are watermarked.
    Thus, a side effect of Alice's watermark, intended for machine-generated text detection, is to reveal what data Bob's model was fine-tuned on.
    }
    \label{chap6/fig:fig1}
\end{figure*}

\section{Introduction}

Large Language Models (LLMs) are often instruction fine-tuned to align them with human prompts and improve their performance and generalization~\citep{ouyang2022training, wei2022finetuned, chung2022scaling}.
Fine-tuning requires expert knowledge to balance diversity and quality in the instruction dataset and a costly collection of manual annotations, especially for alignment~\citep{openai2023gpt, touvron2023llama2, team2023gemini}.
To address the cost and the difficulties of fine-tuning, practitioners often train on synthetic data generated by a model that has already been instructed, such as Bard, ChatGPT, or Claude. 
For example, works by \citet{wang2022self, honovich2022unnatural, peng2023instruction} created instruction data for many of the first wave of instruction fine-tuned LLMs~\citep{alpaca, xu2023baize, gunasekar2023textbooks, mukherjee2023orca}. 
This may also be unintentional when, for example, Turkers use ChatGPT to perform their tasks~\citep{veselovsky2023artificial}.
Such imitation raises questions about whether the fine-tuned model is a derivative work of the original model~\citep{wallace2020imitation}. 
It may also be seen as theft since the training data is extracted from the model without consent. 
For instance, OpenAI, Google, Meta, and Anthropic prohibit the use of their generated output to train other AI models under their terms-of-service.
In this context, it is crucial to understand how to detect when LLM outputs are used as training data.


Meanwhile, recent AI regulations enforce the transparency of generative models.
This is increasingly important in cases where the generated content may be used for malicious purposes~\citep{weidinger2022taxonomy, crothers2022machine}.
One approach to enhance transparency is to watermark the generated content, as presented by~\citet{aaronson2023watermarking,kirchenbauer2023watermark, kirchenbauer2023reliability} and in Chap.~\ref{chapter:three-bricks}.
It embeds a secret trace in the synthetic content that can be detected to identify the generating model.
In the context of LLMs, recent techniques make detection efficient with minimal degradation of the generated text quality by altering the sampling of next tokens.

\noindent
Based on these two observations, this study addresses the following question:
\begin{center}
\textit{What occurs when watermarked texts are employed as fine-tuning data?}
\end{center}
We explore the potential ``radioactivity'' -- a term coined by~\citet{sablayrolles2020radioactive} -- of LLM watermarking, which refers to the capacity of watermarked training data to contaminate a model.

We examine a model that has been fine-tuned on a corpus that may contain watermarked texts (see Fig.~\ref{chap6/fig:fig1}).
The baseline method for detecting radioactivity executes the original watermark detection on the outputs generated by this model. 
However, this approach proves ineffective because the residual of the watermark is a weak signal hardly detectable in plain output text.
In this work, we are able to demonstrate that LLM watermarking is indeed radioactive thanks to our specific protocol designed for revealing weak contamination traces.

The contributions of the chapter are as follows:
\begin{itemize}
    \item We design radioactivity detection methods for four scenarios based on model (\textit{open} / \textit{closed}) and training data (\textit{supervised} / \textit{unsupervised}) access. 
    Notably, our open-model detection (Fig.~\ref{chap6/fig:open_model}) improves the performance by orders of magnitudes.
\item 
    We show how to obtain reliable \pval s for watermark detection when scoring millions of tokens.
\item 
    We prove that watermarked text is radioactive in a real-world setting where an LLM is fine-tuned on slightly watermarked instruction data. 
    Our tests detect radioactivity with a $\pval$ of $10^{-5}$ when no more than 5\% of fine-tuning data is watermarked (Fig.~\ref{chap6/fig:wm-proportion}).
    This offers the first reliable way of proving that an LLM has been fine-tuned on outputs of another one.
\end{itemize}
