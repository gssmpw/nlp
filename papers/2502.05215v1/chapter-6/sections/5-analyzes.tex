
\section{Investigating radioactivity}\label{chap6/sec:fine-tuning-abl}

\autoref{chap6/sec:instruction} considers detection in a practical scenario.
This section further studies what influences radioactivity from different angles like fine-tuning, watermarking algorithm, and data distribution.


\subsection{Fine-tuning}


\input{chapter-6/tables/fine-tuning-abl.tex}

We first study the influence of fine-tuning on the same setup as Sec.~\ref{chap6/sec:instruction}, with regards to: 
(a) the learning rate,
(b) the fine-tuning algorithm,
\eg, with Q-LoRA~\citep{dettmers2023qlora} a widely used method for efficient fine-tuning;
(c) the number of epochs,
(d) the model size.
We fine-tune $\B$ with the same dataset of $\rho=100\%$ watermarked instructions and the same parameters.
We detect radioactivity in the \emph{open-model} / \emph{unsupervised} setting.
This is done on $N=10$k next-predictions, and where the texts that are fed to $\B$ are watermarked instructions generated with $\A$.
\autoref{chap6/tab:ft-abl} reports the results. The more the model fits the data, the easier its radioactivity is to detect.
For instance, multiplying the learning rate by $10$ almost doubles the average $\logpval$ of the test.


\subsection{Bigger teachers}\label{chap6/app:bigger-teachers}

\begin{table}[t!]
    \centering
    \caption{
        Influence of the teacher model size on radioactivity detection.
    }
    \label{chap6/table:results_Teachers}
    \footnotesize
    \begin{tabular}{ *{5}{l} }
        \toprule
        Teacher & Without & 7B & 13B & 65B \\
        \midrule
        NQ & 3.2 & 5.6 & 5.4 & 5.8 \\
        GSM8k & 10.0 & 11.1 & 10.4 & 11.0 \\
        MMLU & 28.4 & 31.0 & 32.9 & 33.8 \\
        \midrule
        $\log_{10}$ p-value & -0.3 & -32.4 & -31.1 & -31.7 \\
        \bottomrule
    \end{tabular}
\end{table}

We conduct the same experiment replacing the Llama-2-chat-7B teacher model by the 13B or 65B versions.
\autoref{chap6/table:results_Teachers} reports the results for benchmarks NQ, GSM8k, and MMLU and the average $\logpval$ of the radioactivity detection test under the same conditions as the previous paragraph.
Our observations align with the previous conclusions: watermarking does not significantly affect the benchmarks (except for MMLU where the improvement appears larger). 
Moreover, the detection of radioactivity is not significantly impacted by the teacher model size.










\subsection{Watermarking method}

\paragraph{Multi-bit scenario.} 

We adopt the same framework as in Sec.~\ref{chap6/sec:instruction}, but we use the watermarking method of \cite{yoo2023advancing}, \aka, MPAC.
It is a multi-bit method, where the watermark is a binary message of size $n$.
More precisely, we take bits 2 by 2 to generate a message $m = m_1 m_2 \ldots m_b$ with the $r$-ary $m_i = 0, 1, 2,$ or $3$, corresponding to $r=4$ and $b = n/2$ with the notations from the original paper.
The method proceeds as the one of \cite{kirchenbauer2023reliability} by altering the logits before generating a token.
However, the hash created from the watermarked window and the key is now used 
(1) to randomly partition the vocabulary into $r$ disjoint sets, 
(2) to select the position $i$ and corresponding $m_i$ that is hidden for this particular token.
A bias $\delta$ is added to the logits of the tokens belonging to the $m_i$-th set.
Given a text under scrutiny, the extraction reverses the process to find which $r$-ary is the most likely for each position $i$ of the message.

We report in Fig.~\ref{chap6/fig:bit-accuracy} the extraction results in the the supervised/closed-model setup.
We filter and deduplicate the tokens as in Sec.~\ref{chap6/sec:instruction}, and plot the observed bit accuracy against the number of scored tokens -- note that since the watermark is a binary message, we now measure the bit accuracy of the extraction, instead of the \pval\ of the detection.
This is done for several lengths of the binary message. 
Every experiment is run $10$ times for different text output by $\B$, which explains the $95\%$ confidence interval in the plots.
We observe as expected that the bit accuracy significantly increases with the proportion of watermarked data in the fine-tuning data, and that the longer the message, the harder it is to extract it.
This suggests that radioactivity still holds in the multi-bit scenario, and could therefore be used to identify a specific version of the model or a specific user from which the data was generated.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.99\linewidth,clip, trim=0 0 0 0cm]{chapter-6/figs/all_closed_sup.pdf}
    \caption{
        Bit accuracy when watermarked instruction data are generated with MPAC~\citep{yoo2023advancing}, against the number of scored tokens generated by the fine-tuned model.
        This is done under the supervised/closed-model setup, for various lengths ($n$=8, 16, 32) of the message.
    }
    \label{chap6/fig:bit-accuracy}
\end{figure}







\paragraph{Watermark window size.} 
To reduce the experimental requirements for generation, training and detection -- and introduce more variety to the data under study -- we now prompt $\A$=Llama-2-7B with the beginnings of Wikipedia articles in English and generate the next tokens with or without watermarking. 
We then fine-tune $\B$=Llama-1-7B on the natural prompts followed by the generated answers.
The fine-tuning is done in 1000 steps, using batches $8\times2048$ tokens (similarly to Sec.~\ref{chap6/sec:instruction}).
This section fine-tunes $\B$ on $\rho=100\%$ English watermarked texts.

\autoref{chap6/tab:exp_kgram} highlights that the confidence of the detection decreases for larger window sizes $k$ when fixing the \pval\ of the watermark detection of the training texts.
There are two explanations. 
First, for lower $k$, the chances that a $k$-gram repeats in the training data are higher, which increases its memorization.
Second, the number of possible $k$-grams is $|\V|^k$ and therefore increases with $k$, while the number of watermarked tokens is fixed. 
Thus, at detection time, the number of radioactive $k$-grams decreases with increasing $k$, diminishing the test's power. 
This experiment also demonstrates that the methods of \cite{aaronson2023watermarking} and \cite{kirchenbauer2023reliability} behave the same way.

\paragraph{Discussion on other watermarking schemes.}
Our goal is to derive a general method for detecting radioactivity in decoding-based watermarks, rather than testing all watermarking schemes or identifying the most radioactive one. 
We focus on the works of~\citet{aaronson2023watermarking, kirchenbauer2023watermark}, which are representative of the two main families of methods and provide reliable \pval s for detection. These schemes rely on key management using the hash of previous tokens, a mechanism also used by other potentially radioactive schemes~\citep{lee2023wrote, fu2024gumbelsoft}.

Some LLM watermarking schemes do not rely on hashing, such as ``semantic'' watermarks~\citep{liu2023semantic, liu2024adaptive, fu2024watermarking} and those using pre-defined key sequences~\citep{kuditipudi2023robust}. 
However, we do not evaluate the radioactivity of these methods due to the lack of \pval\ computation, a limitation also noted in other studies~\citep{piet2023mark}. 
For example, the complexity of computing \pval s for the scheme in~\citet{kuditipudi2023robust} would be prohibitively expensive, requiring $10^{16}$ times more operations than the schemes presented previously\footnote{
    \citet{kuditipudi2023robust} use the Levenshtein distance with complexity $O(mnk2)$, where $m$ is the number of tokens, $n=256$, and $k=80$ (default parameters from the authors). 
    This results in a complexity approximately $10^6$ times greater than previous schemes. 
    To evaluate \pval s of $10^{-10}$, a Monte Carlo simulation would require running the statistic $10^{10}$ times, a total $10^{16}$ more operations.
}. 
This limitation would also apply to works based on the same key management~\citep{christ2023undetectable, liu2023semantic}.









\subsection{Data distribution}

\begin{table}[t!]
        \centering
        \caption{
            Influence of the target text distribution on detection.
            $\B$ is prompted with beginnings of Wikipedia articles in the corresponding language, and detection is done on generated next tokens. 
            For each language, we score $N=250$k $k$-grams using the \textit{closed-model} setting.
        }
        \label{chap6/tab:exp_language}
        \footnotesize
        \begin{tabular}{ c *{6}{c} }
            Language &  English & French & Spanish & German & Catalan & Combined p-value (Fisher) \\
            \shline
                $\logpval$ & $<$-50 & -7.8 &  -5.7 &  -4.0 &  -2.1 & $<$-50  \\
        \end{tabular}
    \end{table}
    

\paragraph*{Radioactivity detection in different languages.}\label{chap6/par:distrib}
We consider an unsupervised setting where Alice has no prior knowledge about $D^\A$, the data generated with $\A$ used to fine-tune $\B$.
As an example, Alice does not know the language of $D^\A$, which could be Italian, French, Chinese, etc. 
We run the detection on text generated by $\B$, with prompts from Wikipedia in different languages.
The confidence of the test on another language -- that might share very few $k$-grams with $D^\A$ -- can be low, as shown in Tab.~\ref{chap6/tab:exp_language}.

Alice may, however, combine the \pval s of each test with Fisher's method.  
This discriminates against $\mathcal{H}_0$: ``\textit{none of the datasets are radioactive}'', under which the statement ``\textit{Bob did not use any outputs of $\A$}'' falls.
Therefore, the test aligns with our definition of model radioactivity as per definition~\ref{chap6/def:model_radioactivity}.
From Tab.~\ref{chap6/tab:exp_language}, Fisher's method gives a combined \pval\ of $<10^{-50}$. 
Thus, even if Alice is unaware of the specific data distribution generated by $\A$ that Bob may have used to train $\B$ (\eg, problem-solving scenarios), she may still detect radioactivity by combining the significance levels.

\paragraph*{Mixing instruction datasets from different sources.}


\begin{table}[t!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \caption{
            Mixing instruction datasets from different sources.
            The fine-tuning is done with the setup presented in Sec.~\ref{chap6/sec:instruction}, with $\rho$=$10\%$ of watermarked data, mixing either with human or synthetic instructions.
        }
        \label{chap6/table:data-sources}
        \footnotesize
        \begin{tabular}{c|c}
            \toprule
            Major data source
            & Average $\logpval$ \\
            \midrule
            Machine & -15 \\
            Human & -32 \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \caption{
            Sequential fine-tuning to remove the watermark traces when fine-tuning.
            The first fine-tuning is done with the setup presented in Sec.~\ref{chap6/sec:instruction}, with $\rho$=$10\%$ of watermarked data, and the second on OASST1.
            }
        \label{chap6/table:results_FineTuning}
        \footnotesize
        \begin{tabular}{c|c}
        \toprule
        Second fine-tuning & Average $\logpval$ \\
        \midrule
        \xmarkg & -15 \\
        \cmarkg & -8 \\
        \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}



We conduct the same experiment as in Sec.~\ref{chap6/sec:instruction}, but replace the non-watermarked synthetic instructions by human-generated ones (from the Open Assistant dataset OASST1~\citep{kopf2024openassistant}).
We report in Tab.~\ref{chap6/table:data-sources} the detection results in the open / unsupervised scenario, with $\rho=10\%$ of watermarked data.
Interestingly, with the exact same setting as the one explored in Sec.~\ref{chap6/sec:detection-setup}, the radioactivity signal is stronger in this case.
Our speculation is that this might be due to fewer overlapping sequences of $k$+1-grams between the two distributions.



\subsection{Possible defenses}

We now assume Bob is aware of watermarking radioactivity and tries to remove the watermark traces before, during or after fine-tuning.
For instance, he might attempt to rephrase the watermarked instructions, use a differentially private training, or fine-tune his model on human-generated data -- which we do in the following experiment.
The logic is overall the same as previously pointed out in the fine-tuning ablations.
If the original watermark is weaker or if the fine-tuning overfits less, then radioactivity will be weaker too.
Therefore, the radioactivity detection test will be less powerful, but given a sufficient amount of data, it may still be able to detect traces of the watermark.

\paragraph{Radioactivity ``purification''.}\label{chap6/ref:purification}
We investigate the impact of a second fine-tuning on human-generated data to remove the watermark traces, through the following experiment.
After having trained his model on a mix of watermarked and non-watermarked data, as in Sec.~\ref{chap6/sec:instruction}, Bob fine-tunes his model a second time on human-generated text (from OASST1, as in the previous paragraph), with the same fine-tuning setup.
\autoref{chap6/table:results_FineTuning} shows that the second-fine-tuning divides by $2$ the significance level of the statistical test, although it does not completely remove the watermark traces.

