\section{Details on \pval s}\label{chap6/sec:additional}

This section provides more information on the experimental setup, reporting, and correctness experiments for the tests used in the chapter.

\subsection{Reporting}\label{chap6/app:repporting}

\paragraph{Interpretation of the average $\logpval$.}
Given that \pval s often span various orders of magnitude, we report the average of the $\logpval$ over multiple runs rather than the average of the \pval s themselves. 
We interpret the average $\logpval$ as though it could be directly read as a \pval, although a direct translation to a rigorous statistical \pval\ is not possible.
Fig.~\ref{chap6/fig:box_plot_open} shows box-plots with additional statistics.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.5\linewidth]{chapter-6/figs/boxplot_open.pdf}
    \captionsetup{font=small}
    \caption{Box plot for the $\logpval$ in the open/unsupervised setting with varying $\rho$, the proportion of watermarked fine-tuning data.}
    \label{chap6/fig:box_plot_open}
\end{figure}


\paragraph{Average over multiple runs.} 
Due to computational constraints, standard deviations for the $\logpval$ are not calculated across multiple models trained on different instruction data for each setting. 
Instead, we generate the same volume of data (14M tokens) in addition to the data used to fine-tune the model. 
In the open-model setting, we run detection on ten distinct chunks of this additional data. 
In the closed-model setting, we prompt the model with ten different chunks of new sentences and score the responses.


\begin{figure}[b!]
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth,clip, trim=0 0 0 0cm]{chapter-6/figs/under_H0_closed_final.pdf}
        \caption{
            \pval\ under $\mathcal{H}_0$ with closed-model access.
            We fine-tune $\B$ on non-watermarked instructions and prompt $\B$ with watermarked instructions, scoring the distinct $(k+1)$-grams from the answers, excluding $k$-grams from the instruction. 
        }
        \label{chap6/fig:pvalu_under_H0_closed}
    \end{minipage} \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth,clip, trim=0 0 0 0cm]{chapter-6/figs/under_H0_open.pdf}
        \caption{
            \pval\ under $\mathcal{H}_0$ with open-model access. 
            We fine-tune $\B$ on non-watermarked instructions, perform the open-model detection and score distinct $(k+1)$-grams only on $k$-grams that $\B$ did not previously attend to when generating the token. 
        }
        \label{chap6/fig:pvalu_under_H0}
    \end{minipage}
\end{figure}



\subsection{Correctness experiments}\label{chap6/app:correctness}

We validate the correctness of our statistical tests by examining the null hypothesis $\mathcal{H}_0$, which represents when the model was not fine-tuned on any watermarked data ($\rho=0$).
Our goal is to show that the \pval\ is approximately uniform under $\mathcal{H}_0$, with a mean of $0.5$ and a standard deviation of $\approx 0.28$.
Instead of fine-tuning model $\B$ with various datasets, we vary the hyper-parameters of the detection algorithm at a fixed fine-tuned model to save computation and memory. 
This approach is not sufficient on its own to establish the validity of the test, but it provides some evidence for it.
Here, $\B$ is the model fine-tuned on non-watermarked instructions as described in Sec.~\ref{chap6/sec:instruction}.


\paragraph{Closed-model.}
We prompt $\B$ with $\approx10$k watermarked instructions using the method by~\citet{kirchenbauer2023watermark}, $\delta=3$ and $k=2$, and three different seeds $\mathsf{s}$. 
We score the answers using the proposed de-duplication and repeat this $10$ times on different datasets. 
\autoref{chap6/fig:pvalu_under_H0_closed} shows that the average \pval\ is close to $0.5$ and the standard deviation is close to $0.28$, as expected under $\mathcal{H}_0$.


\paragraph{Open-model.}
We generate text with eight distinct watermarking methods (methods of~\citet{aaronson2023watermarking} and ~\citet{kirchenbauer2023watermark} for $k\in\{1,2,3,4\}$).
We divide each dataset into three and apply the radioactivity detection test on these 24 segments, each containing over 1.5 million tokens. 
We score the distinct $(k+1)$-grams from the answers, excluding $k$-grams from the instruction.
\autoref{chap6/fig:pvalu_under_H0} shows again that the average \pval\ is close to $0.5$ and the standard deviation is close to $0.28$.
