
\chapter{Conclusion}\label{chapter:conclusion}
\chaptermark{Conclusion}



\section{Summary of the contributions}

This thesis first presents a comprehensive overview of watermarking techniques in \autoref{chapter:related-work} and \autoref{chapter:technical-background}.
It then introduces and evaluates new watermarking techniques for a more and more digital and AI-driven internet.
The contributions are structured around three main axes: content moderation, tracing AI-generated content, and monitoring AI models.
Each part addresses current challenges and leverages watermarking to enhance traceability of content and models.

Content moderation is a critical issue for online platforms, which need to manage the spread of harmful or illegal content.
\autoref{part:content-moderation} introduces watermarking techniques that significantly enhance the robustness of content moderation systems. 
By imperceptibly modifying images as they enter a platform, these techniques improve origin tracing and verification and limits the spread of already-flagged material.
\autoref{chapter:ssl-watermarking} hides information in the latent representations of self-supervised neural networks through an iterative image optimization, enabling watermarking images with varying resolutions, adjustable payload, and a customizable tradeoff between robustness and quality. 
\autoref{chapter:active-indexing} combines copy detection and watermarking to greatly improve the robustness of copy detection systems, with a similar optimization scheme.

In parallel, the need for robust detection mechanisms becomes critical as AI-generated content becomes increasingly indistinguishable from human-generated content, and used for misinformation or fraud. 
\autoref{part:genai-tracing} presents watermarking methods that embed unique identifiers into content generated by AI models, allowing for easy tracing of the content's origin. 
Regulatory frameworks are increasingly recognizing the importance of such techniques, and this thesis provides practical solutions that align with these emerging standards.
\autoref{chapter:stable-signature} introduces a method that fine-tunes latent generative models such that all images they produce hide an invisible signature, which can be used to detect and track the origin of synthesized images, even when models are openly shared.
\autoref{chapter:audioseal} introduces a watermarking solution for the detection of AI-generated speech, which proactively watermarks the speech signal and predicts for each time step if the watermark is present or not.
\autoref{chapter:three-bricks} brings three improvements to state-of-the-art watermarking methods for large language models (LLM), notably theoretically grounded and empirically validated statistical tests that guarantee false positive rates.

The economic and intellectual property aspects of AI models are also increasingly important, since organizations invest heavily in training and deploying these models.
\autoref{part:model-monitoring} introduces techniques that allow organizations to protect their intellectual property and ensure compliance with licensing agreements.
\autoref{chapter:radioactive} examines whether it is possible to detect when an LLM has been fine-tuned on the output of another model. 
More precisely it shows that LLM watermarks can in some cases be identified in fine-tuned models even when only a small proportion of the fine-tuning data is watermarked.
\autoref{chapter:invariants} introduces a training-free watermark for the weights of large transformers that allows for traitor tracing.
It leverages the model's invariance through operations like dimension permutations to generate different functionally equivalent copies with different weights.








\section{Perspectives on generative AI watermarking}

Watermarking is the most robust and efficient solution to trace AI-generated content, but governance and interoperability challenges remain.

\subsection{Robustness and security}

Because watermarking works by actively modifying content, there is a common belief that these traces can be easily removed. 
However, this thesis has outlined the technical superiority of watermarking, particularly in terms of detection confidence and robustness, compared to other content tracing methods.
Additionally, when viewed as part of a broader ecosystem that includes detection algorithms, legal frameworks, and social norms, watermarking may arguably be robust enough for its intended purpose.

\paragraph*{Invisible watermarking surpasses other methods.}
Watermarking presents undeniable advantages.
First, it intentionally injects traces into content, whence the greater robustness than that of passive methods like forensics or fingerprinting. 
For instance, \autoref{chapter:active-indexing} shows that watermarking achieves $\approx\times$8 better recall than fingerprinting after crops that keep $50\%$ of the original images;
and \autoref{chapter:stable-signature} compares watermarking to forensics and shows that it achieves the same true positive rate (probability of correctly flagging a watermarked piece of content) for a $10$ million times smaller false positive rate (probability of wrongly flagging a non-watermarked one), on images that are cropped, resized and compressed.
Attacking the watermark is always possible but this always damages the quality, contrary to visible watermark or metadata erasure -- this is also true for forensics~\citep{barni2018adversarial} and fingerprinting methods~\citep{tolias2019targeted}.
Second, a sound watermarking design has a low false positive rate.
Most importantly, it is provably low (see Chap.~\ref{chapter:three-bricks}), unlike with forensics and fingerprinting. 
Data provenance is expected to be tested on millions of pieces of content, as for content moderation, therefore requiring extremely low false positive rates. 
This is beyond reach of an empirical validation.  

\paragraph*{Are attacks really a limitation?}\label{conclusion:attacks}
Robustness and security are close yet different concepts in watermarking~\citep{cayre2005watermarking}.
While watermarking is built to be robust, it is not foolproof to intentional attacks~\citep{cox1998some, bas2011break}, be it for image and audio~\citep[\autoref{chapter:stable-signature}, \autoref{chapter:audioseal}]{jiang2023evading, zhao2023invisible, saberi2023robustness}, or LLM-generated texts~\citep{sadasivan2023can, krishna2024paraphrasing, hu2024stable, jovanovic2024watermark, pang2024attacking, chang2024watermark}.
It is subject to attacks that are roughly categorized based on the attacker's knowledge, as done in Chap.~\ref{chapter:stable-signature} and \ref{chapter:audioseal}.
\textit{White-box} attacks have full access to the watermarking algorithm and its parameters (\eg, model weights);
\textit{black-box} attacks only have access to inputs and outputs, for instance through an API;
and \textit{no-box} attacks do not have any knowledge of the system.
The effectiveness and ease of an attack generally increases with the attacker's level of knowledge about the watermarking system, the hardest ones being no-box attacks, where there is not even the feedback on if an attack was successful or not.

Most attacks are removal attacks, where the goal is to eliminate the watermark from the content. 
Watermark forging, where the attacker creates a counterfeit watermark, may pose a more significant problem. 
Currently, without white-box access to the embedder or extractor, forging a watermark is considerably difficult. 
There is always a trade-off to consider: an attack may succeed in removing or forging a watermark, but at the cost of degrading the quality of the content itself and of making the attack more detectable.

\emph{Watermarking keeps honest people honest.}
Most mafia organizations or belligerent countries now have the expertise and resources to train their own generative models. 
They will include neither watermarking nor metadata, and forensic methods are also doomed to fail due to the lack of such data to train a detector. 
Watermarking's goal is not to protect against these cases.
Rather, it aims to dissuade 99\% of the population, by making the removal of the watermark complex enough and voluntary -- or even criminal by law, as what happened with DRM systems~\citep{enwiki:1221667351}.
This aligns with the motto ``keep honest people honest,'' which Hollywood popularized in the 2000s about DRMs.


\subsection{The real challenges are governance and interoperability}

The technical aspects of watermarking, like its robustness to adversarial attacks, are far from being the only considerations to take into account. 
It is essential to address overlooked challenges that concern governance, control, and, maybe naively, how to even use the detection outcomes. 

\paragraph*{Who controls watermark detection?}

While everybody is a priori willing to know when they are interacting with generated content, making watermark detectors publicly available introduces security risks.
Open-source detectors can lead to white-box attacks, and API access can facilitate black-box attacks.
Consequently, no record of watermark detection by anyone other than the generative model's owner currently exists.
This situation, where the model provider is both judge and jury, is problematic. 
It would be more trustworthy if watermarking and detection were managed by trusted, unbiased entities. 
This raises questions about who these entities should be and how they are governed. 

\paragraph*{Open-source generative models?}
They present a unique challenge since they are freely available and usable without post-hoc watermarks (applied after generation).
The case in point is Stable Diffusion~\citep{rombach2022high}, which was open-sourced in late 2022. 
Removing the watermark in its source code was as simple as commenting out a single line.
Ideally, models should be trained or fine-tuned to generate watermarked content natively as in Chap.~\ref{chapter:stable-signature} or related work~\citep{yu2020responsible, kim2023wouaf, juvela2023collaborative, gu2023learnability}.
Determining responsibility in this context is complex: should it be the responsibility of the individual who uploads a model to a platform, or should hosting platforms like GitHub or Hugging Face enforce in-model watermarking? 
This issue needs clear regulatory guidance and possibly new technological solutions to ensure compliance.

\paragraph*{What to do with detection?}
This question is not clearly addressed by current regulations.
The use of watermarks for labeling authentic or fake content on social networks and search engines, as suggested by current texts like 22949.90.3.(a) of \cite{ca_ab3211_2024}, may lead to a rebound effect. 
It may conversely exacerbate misinformation by placing undue emphasis on content that is either not detected, generated by unknown models, or authentic but used out of context.
Moreover, detection of watermarks extends beyond individual pieces of content, often involving the aggregation of evidence from multiple submissions linked to a single account. 
\citet{kirchenbauer2024on} notably showed that watermarked text may be detected even under strong paraphrasing after observing enough words.
Finally, current regulations lead different entities to quickly develop their own watermarking methods.
This results in a fragmented ecosystem where nobody is responsible for detection.
For instance, the music generation startup \citet{sunov3} watermarks their outputs, but no platforms (Facebook, X, Spotify, Youtube, etc.) actually detect them.
Collaborative efforts are needed to establish standards that ensure watermarks are robust, but, most importantly, recognizable across platforms.
It should involve regulators, model providers and content hosting platforms.








\section{Perspectives and open questions}

Watermarking to trace AI-generated content, is far from being (1) a solved problem, (2) the only use-case of watermarking.

\subsection{Security}
A conceptual problem of watermarking is that allowing third-parties to detect the watermark discloses information which can be used to remove the watermark or forge fake ones, \eg, the watermark extractor/detector (see Sec.~\ref{conclusion:attacks}).
By default, it does not follow Kerckhoffs's principle~\citep{Kerckhoffs1883}, which states that the security of a system should assume that the attacker knows the system, except for the secret key.

Two key concepts from cryptography deal with this issue in the context of watermarking.
In \emph{asymmetric watermarking} (or public watermarking), the embedder and the detector are different entities.
Anyone with access to the detector may detect a watermark, but only the entity with the private key may embed or remove it.
It is similar to public-key cryptography, where anyone can encrypt a message, but only the recipient can decrypt it.
It has been applied to image or video watermarking~\citep{furon1999asymmetric, furon2003asymmetric, hartung1997fast} and recently to watermarking for LLMs~\citep{fairoze2023publicly}.
In the context of \emph{zero-knowledge watermark detection}, the prover should convince the verifier of the presence of a watermark without revealing information that can be used to remove the watermark, similar to zero-knowledge proofs (ZKP), where a prover convinces a verifier that a statement is true without revealing any information about the statement itself~\citep{fiege1987zero}.
This is done by replacing the watermark detection process with a cryptographic protocol~\citep{craver2000zero, adelsbach2001zero, adelsbach2003watermark}.
The aforementioned methods are not yet widely adopted because too computationally expensive and very mathy (and for some a bit rusty).
Watermarking research could benefit from more research in these directions, with the ultimate goal of creating a watermark that anyone can detect, but only authorized parties can embed.

\subsection{Semantic watermarking in generative models}

Most watermarking methods modify pixel values or audio samples, which can be made robust to common editing operations by incorporating them into the training process (see Sec.~\ref{chap0/sec:deep learning-watermarking}).
However, not all attacks can be covered, \eg, for images, noising and denoising with a diffusion model~\citep{nie2022diffusion}.
A promising approach is to perform watermarking at a higher level, in the semantic content of the generation. 
While image noising/denoising or compression alters the pixel values, most image/audio editing methods and attacks leave the image-level semantics unchanged. 
This also holds true for text: paraphrasing alters the ``surface'' tokens, but the sentence-level semantics remain unchanged.
Modifying the content semantics is undesirable in traditional watermarking, as it would alter the content itself.
However, for generated content, this is not an issue since the primary concern is that the generated content follows the given instructions, regardless of what would have been generated without the watermark.
Recent works have explored this direction, such as Tree-Ring~\citep{wen2023tree}, which is the first watermark that does not rely on minor modifications of generated images, but instead alters the image generation process. 
Similar approaches have been proposed for text watermarking~\citep{liu2023semantic,hou2023semstamp}.
Despite these advancements, there are still gaps in this research area. 
Notably, no semantic watermarking method has been proposed for audio or video, and no existing methods work ``in-model'', or post-hoc for either images or text (as defined in Sec.~\ref{chap0/sec:generation-watermarking}).
This is a highly promising research direction, as it offers the potential for more robust watermarking methods that are less sensitive to common editing operations and no-box attacks.






\subsection{Tracing training data}
Data attribution, \ie, the ability to trace model's outputs back to its training data, is an active field of research. 
It has applications in model interpretability, fairness, and privacy, and would have huge economical downfalls in the context of copyrights of training data~\citep{deng2024economic}.
Current methods for data attribution, are based on old concepts in machine learning, like influence functions~\citep{cook1977detection} and Shapley values~\citep{shapley1953value}.
They often fall short when applied to large-scale models due to their computational complexity and the necessity for model retraining~\citep{koh2017understanding, feldman2020neural, pruthi2020estimating, park2023trak} or the absence of causation between training data and generated outputs~\citep{wang2023evaluating}.

The introduction of active watermarking techniques could provide a scalable and efficient solution to the data attribution problem.
By embedding watermarks directly into the data used for training AI models, it becomes possible to trace the influence of specific training sets on the model's behavior at test time, with better efficiency and guarantees than passive methods.
This is the approach followed by \citet{sablayrolles2020radioactive}, which watermark training set of image classifiers to detect if it was used to train a given model and of \autoref{chapter:radioactive} for synthesized data from large language models.
It is further explored by \citet{asnani2024promark}, who attribute synthetically generated images to specific training data concepts through watermarking.
This could be applied at scale on different types of generative models and for various use-cases, to properly credit the data sources when generating new content.


