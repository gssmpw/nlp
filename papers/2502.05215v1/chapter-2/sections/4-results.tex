\renewcommand{\rot}[1]{\rotatebox{45}{#1}\hspace*{-0.6cm}}





\section{Experimental results}




\subsection{Experimental setup}\label{chap2/sec:experimental}

\paragraph*{Dataset.}
We use DISC21~\citep{douze2021disc} a dataset dedicated to ICD.
The dataset can be freely downloaded on \href{https://ai.facebook.com/datasets/disc21-dataset/}{its webpage}.
It includes 1M reference images and 50k query images, 10k of which are true copies from reference images. 
A disjoint 1M-image set with same distribution as the reference images is given for training. 
Images resolutions range from 200$\times$200 to 1024$\times$1024 pixels (most of the images are around 1024$\times$768 pixels).

The queries used in our experiments are \emph{not} the queries in DISC21, since we need to control the image transformations in our experiments, and most transformations of DISC21 were done manually so they are not reproducible. 
Our queries are recreated transformations of images \emph{after active indexing}. 
These transformations range from simple attacks like rotation to more realistic social network transformations which created the original DISC21 queries\footnote{
    We create a first test set ``Ref10k'' by selecting the 10k images from the reference set that were originally used to generate the queries (the ``dev queries'' from the downloadable version).
    We also re-create a query set ``Query50k''. 
    To be as close as possible, we use the same images that were used for generating queries in DISC.
    Edited images are generated using the AugLy library~\citep{papakipos2022augly}, following the guidelines given in the ``Automatic Transformations" section of the DISC paper.
    Therefore, the main difference between the query set used in our experiments and the original one is that ours do not have manual augmentations.
}. 


\paragraph*{Metrics.}
For retrieval, our main metric is Recall $1$@$1$ (\rone for simplicity), which corresponds to the proportion of positive queries where the top-1 retrieved results is the reference image.
For copy detection, we use the same metric as the NeurIPS Image Similarity Challenge~\citep{douze2021disc}.
We retrieve the $k=10$ most similar database features for every query; and we declare a pair is a match if the distance is lower than a threshold $\tau$.
To evaluate detection efficiency, we use the 10k matching queries above-mentioned together with 40k negative queries (\ie, not included in the database). 
We use precision and recall, as well as the area under the precision-recall curve, which is equivalent to the micro average precision (\emph{$\mu$AP}).
While \rone only measures ranking quality of the index, $\mu$AP takes into account the confidence of a match.
As for image quality metric, we use the Peak Signal-to-Noise Ratio (PSNR) which is defined as $10\log_{10} \left( 255^2 / \mathrm{MSE}(I, I')^2 \right)$, as well as SSIM~\citep{wang2004ssim} and the norm $\norm{I-I'}_\infty$.




\paragraph*{Transformations seen at test time}\label{chap2/sec:transformations}


\begin{table}[t!]
    \centering
    \captionsetup{font=small}
    \caption{Illustration of all transformations evaluated in Tab.~\ref{chap2/tab:act_vs_pas_retrieval}.}
    \label{chap2/fig:all_transformations}
    \begin{tabular}{*{5}{l}}
         Identity & Contrast 0.5 & Contrast 2.0 & Brightness 0.5 & Brightness 2.0 \\
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/none.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/contrast1.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/contrast2.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/brightness1.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/brightness2.jpg}\end{minipage} 
         \\ \\
          Hue 0.2 & Blur 2.0 & JPEG 50 & Rotation 25 & Rotation 90 \\
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/hue.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/blur.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/jpeg.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/rotation1.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/rotation2.jpg}\end{minipage} 
         \\ \\
          Crop 0.5 & Resize 0.5 & Meme & Random \\
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/centercrop.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/resize.jpg}\end{minipage} &  
         \begin{minipage}{.12\linewidth}\includegraphics[width=\linewidth]{chapter-2/figs/attacks/meme.jpg}\end{minipage} &  
         \begin{minipage}{.16\linewidth}\includegraphics[width=1.7\linewidth]{chapter-2/figs/attacks/auto.jpg}\end{minipage} &  
         \\
         
    \end{tabular}
\end{table}


They cover both spatial transformations (crops, rotation, etc.), pixel-value transformations (contrast, hue, jpeg, etc.) and ``everyday life'' transformations with the AugLy augmentations. 
All transformations are illustrated in Fig.~\ref{chap2/fig:all_transformations}.
The parameters for all transformations are the ones of the torchvision library~\citep{marcel2010torchvision}, except for the crop and resize that represent area ratios. For the Gaussian blur transformation we use alternatively $\sigma$, the scaling factor in the exponential, or the kernel size $k_b$ (in torchvision $k_b = (\sigma-0.35)/0.15$). 
The ``Random'' transformation is the one used to develop the 50k query set. 
A series of simple 1-4 AugLy transformations are picked at random, with skewed probability for a higher number.
Among the possible transformations, there are pixel-level, geometric ones, as well as embedding the image as a screenshot of a social network GUI.



\paragraph*{Implementation details.}\label{chap2/sec:details}
The evaluation procedure is: (1) we train an index on the 1M training images, (2) index the 1M reference images, (3) activate (or not) 10k images from this reference set.
(4) At search time, we use the index to get closest neighbors (and their distances) of transformed versions from a query set made of the 10k images.

Unless stated otherwise, we use a IVF4096,PQ8x8 index (IVF quantizer with 4096 centroids, and PQ with 8 subquantizers of $2^8$ centroids), and use only one probe on IVF search for shortlist selection ($k'=1$). 
Compared to a realistic setting, we voluntarily use an indexing method that severely degrades learned representations to showcase and analyze the effect of the active indexing.
For feature extraction, we use an SSCD model with a ResNet50 trunk~\citep{he2016resnet}. 
It takes image resized to 288$\times$288 and generates normalized representations in $\R^{512}$.
Optimization~\eqref{eq:active_image} is done with the Adam optimizer~\citep{kingma2014adam}, the learning rate  is set to $1$, the number of iterations to $N=10$ and the regularization to $\lambda=1$.
In~\eqref{eq:scaling}, the distortion scaling  is set to $\alpha=3$ (leading to an average PNSR around $43$~dB).
In this setup, activating 128 images takes around 6s ($\approx$ 40ms/image) with a 32GB GPU. 
It can be sped-up at the cost of some accuracy (see Sec.~\ref{chap2/sec:speedup}).

Models used for feature extraction in Sec.~\ref{chap2/sec:generalization} (\href{https://github.com/facebookresearch/sscd-copy-detection/}{SSCD}, \href{https://github.com/facebookresearch/dino}{DINO}, \href{https://github.com/lyakaap/ISC21-Descriptor-Track-1st}{ISC-dt1}) can be downloaded in their respective repositories.
Our implementation builds upon the open-source Pytorch~\citep{paszke2019pytorch} and FAISS~\citep{johnson2019faiss} libraries.

\subsection{Active vs. passive}\label{chap2/sec:act_vs_passive}
This section compares retrieval performance of active and passive indexing.
We evaluate $R$@1 when different transformations are applied to the 10k reference images before search.
The ``Passive'' lines of Tab.~\ref{chap2/tab:act_vs_pas_retrieval} show how the IVF-PQ degrades the recall. 
This is expected, but the IVF-PQ also accelerates search 500$\times$ and the index is 256$\times$ more compact, which is necessary for large-scale applications. 
Edited images are retrieved more often when they were activated for the index:
increase of up to $+60$ \rone for strong brightness and contrast changes, close to results of the brute-force search.
We also notice that the performance of the active IVF-PQ$^{k'=1}$ is approximately the same as the one of the passive IVF-PQ$^{k'=16}$, meaning that the search can be made more efficient at equal performance.
For the IVF-PQ$^\dagger$ that does less approximation in the search (but is slower and takes more memory), retrieval on activated images is also improved, though to a lesser extent. 

\begin{table}[t]
    \centering
    \caption{
        Comparison of the index performance between activated and passive images. 
        The search is done on a 1M image set and $R$@1 is averaged over 10k query images submitted to different transformations before search.  
        \emph{Random}: randomly apply 1 to 4 transformations.
        \emph{Avg.}: average on the transformations presented in the table (details in Sec.~\ref{chap2/sec:transformations}).
        \emph{No index}: exhaustive brute-force nearest neighbor search. \emph{IVF-PQ}: \textsc{IVF4096,PQ8x8} index with $k'$=1 (16 for \emph{IVF-PQ}$^{16}$).
        \emph{IVF-PQ}$^\dagger$: \textsc{IVF512,PQ32x8} with $k'=32$.
    }
    \label{chap2/tab:act_vs_pas_retrieval}
    \resizebox{1.0\linewidth}{!}{
    \begingroup
        \setlength{\tabcolsep}{4pt}
        \def\arraystretch{1.1}
        \begin{tabular}{ l |l| l| c| *{15}{p{0.04\textwidth}}}
            \multicolumn{1}{c}{} & \multicolumn{1}{c}{\rot{Search (ms)}} & \multicolumn{1}{c}{\rot{Bytes/vector}} & \multicolumn{1}{c}{\rot{Activated}} & \rot{Identity} & \rot{Contr. 0.5} & \rot{Contr. 2.0} & \rot{Bright. 0.5} & \rot{Bright. 2.0} & \rot{Hue 0.2} & \rot{Blur 2.0}  & \rot{JPEG 50}  & \rot{Rot. 25} & \rot{Rot. 90} & \rot{Crop 0.5} & \rot{Resi. 0.5} & \rot{Meme} & \rot{Random} & \rot{Avg.} \\ \midrule 
            No index & 252 & 2048 & \xmarkg & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.90 & 0.99 \\ \midrule
            & & & \xmarkg  & 1.00 & 0.73 & 0.39 & 0.73 & 0.28 & 0.62 & 0.48 & 0.72 & 0.07 & 0.14 & 0.14 & 0.72 & 0.14 & 0.13 & 0.45 \\
            \rowcolor{gray!15} \cellcolor{white!0} \multirow{-2}{*}{IVF-PQ} & \multirow{-2}{*}{0.38} \cellcolor{white!0} & \multirow{-2}{*}{8} \cellcolor{white!0}
            & \checkmark & 1.00 & 1.00 & 0.96 & 1.00 & 0.92 & 1.00 & 0.96 & 0.99 & 0.10 & 0.50 & 0.29 & 1.00 & 0.43 & 0.32 & 0.75 \\ 
            \midrule
            & & & \xmarkg & 1.00 & 1.00 & 0.90 & 1.00 & 0.78 & 0.99 & 0.95 & 0.99 & 0.35 & 0.57 & 0.57 & 1.00 & 0.56 & 0.39 & 0.79 \\
            \rowcolor{gray!15} \cellcolor{white!0} \multirow{-2}{*}{IVF-PQ$^{16}$} & \multirow{-2}{*}{0.42} \cellcolor{white!0} & \multirow{-2}{*}{8} \cellcolor{white!0} & 
            \checkmark & 1.00 & 1.00 & 1.00 & 1.00 & 0.98 & 1.00 & 1.00 & 1.00 & 0.43 & 0.88 & 0.75 & 1.00 & 0.84 & 0.50 & 0.88 \\
            \midrule
            & & & \xmarkg & 1.00 & 1.00 & 0.99 & 1.00 & 0.95 & 1.00 & 0.99 & 1.00 & 0.72 & 0.87 & 0.88 & 1.00 & 0.87 & 0.61 & 0.92 \\
            \rowcolor{gray!15} \cellcolor{white!0} \multirow{-2}{*}{IVF-PQ$^\dagger$} & \multirow{-2}{*}{1.9} \cellcolor{white!0} & \multirow{-2}{*}{32} \cellcolor{white!0} &
            \checkmark & 1.00 & 1.00 & 0.99 & 1.00 & 0.98 & 1.00 & 1.00 & 1.00 & 0.75 & 0.92 & 0.91 & 1.00 & 0.92 & 0.63 & 0.94 \\
            \bottomrule
        \end{tabular}
    \endgroup
    }
\end{table}


As for copy detection, \autoref{chap2/fig:prc} gives the precision-recall curves obtained for a sliding value of $\tau$, and corresponding $\mu$AP. 
Again, we observe a significant increase ($\times 2$) in $\mu$AP with active indexing.
Note that the detection performance is much weaker than the brute-force search even in the active case because of the strong approximation made by space partitioning (more details in Sec.~\ref{chap2/sec:space_partitioning}).



\paragraph*{Detailed metrics on different image transformations}

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.99\linewidth,trim={0 0.4cm 0 0.2cm}, clip]{chapter-2/figs/exp/transformations.pdf}
    \captionsetup{font=small}
    \caption{
    Average $R$@1 over 10k images indexed with IVF-PQ.
   }
    \label{chap2/fig:tranformations}
\end{figure}

On Fig.~\ref{chap2/fig:tranformations}, we evaluate the average \rone over the 10k images from the reference dataset.
The experimental setup is the same as for Tab.~\ref{chap2/tab:act_vs_pas_retrieval} with the IVF-PQ, but a higher number of transformation parameters are evaluated.
As expected, the higher the strength of the transformation, the lower the retrieval performance is.
The decrease in performance is significantly reduced with activated images.



\subsection{Generalization}\label{chap2/sec:generalization}

\paragraph*{Generalization to other neural feature extractors.}

We first reproduce the experiment of Sec.~\ref{chap2/sec:details} with different extractors, that cover distinct training methods and architectures.
Among them, we evaluate a ResNext101~\citep{xie2017aggregated} trained with SSCD~\citep{pizzi2022sscd}, a larger network than the ResNet50 used in our main experiments ; 
the winner of the descriptor track of the NeurIPS ISC, \textsc{Lyakaap}-dt1~\citep{yokoo2021isc}, that uses an EfficientNetv2 architecture~\citep{tan2021efficientnetv2} ; networks from DINO~\citep{caron2021dino}, either based on ResNet50 or ViT~\citep{dosovitskiy2020vit}, like the ViT-S model~\citep{touvron2021training}.

\autoref{chap2/tab:extractors} presents the \rone obtained on 10k activated images when applying different transformations before search.
The \rone is better for activated images for all transformations and all neural networks. The average improvement on all transformations ranges from $+12\%$ for DINO ViT-s to $+30\%$ for SSCD ResNet50.

\begin{table}[t]
    \centering
    \caption{
        \rone for different transformations before search. 
        We use our method to activate images for indexing with IVF-PQ, with different neural networks used as feature extractors.
    }
    \label{chap2/tab:extractors}
    \resizebox{1.0\linewidth}{!}{
    \begingroup
        \setlength{\tabcolsep}{4pt}
        \def\arraystretch{1.1}
        \begin{tabular}{ l| l| c| *{15}{p{0.04\textwidth}}}
            \multicolumn{1}{c}{\rot{Name}} & \multicolumn{1}{c}{\rot{Arch.}} & \multicolumn{1}{l}{\rot{Activated}} & \rot{Identity} & \rot{Contr. 0.5} & \rot{Contr. 2.0} & \rot{Bright. 0.5} & \rot{Bright. 2.0} & \rot{Hue 0.2} & \rot{Blur 2.0}  & \rot{JPEG 50}  & \rot{Rot. 25} & \rot{Rot. 90} & \rot{Crop 0.5} & \rot{Resi. 0.5} & \rot{Meme} & \rot{Random} & \rot{Avg.} \\ \midrule 
             &  & \xmarkg & 1.00 & 0.73 & 0.39 & 0.73 & 0.28 & 0.62 & 0.48 & 0.72 & 0.07 & 0.14 & 0.14 & 0.72 & 0.14 & 0.13 & 0.45  \\ 
            \rowcolor{gray!15} \cellcolor{white!0} & \multirow{-2}{*}{ResNet50} \cellcolor{white!0}& \checkmark & 1.00 & 1.00 & 0.96 & 1.00 & 0.92 & 1.00 & 0.96 & 0.99 & 0.10 & 0.50 & 0.29 & 1.00 & 0.43 & 0.32 & 0.75 \\ \cmidrule{2-18}
             &  & \xmarkg & 1.00 & 0.88 & 0.68 & 0.88 & 0.57 & 0.84 & 0.46 & 0.79 & 0.46 & 0.63 & 0.53 & 0.80 & 0.48 & 0.28 & 0.66 \\ 
             \rowcolor{gray!15} \cellcolor{white!0} \multirow{-4}{*}{SSCD} & \multirow{-2}{*}{ResNext101} \cellcolor{white!0} & \checkmark & 1.00 & 1.00 & 0.96 & 1.00 & 0.90 & 0.99 & 0.77 & 0.97 & 0.53 & 0.85 & 0.64 & 1.00 & 0.74 & 0.37 & 0.84 \\ \midrule
             &  & \xmarkg & 1.00 & 0.66 & 0.65 & 0.65 & 0.52 & 0.71 & 0.52 & 0.82 & 0.07 & 0.20 & 0.51 & 0.84 & 0.62 & 0.18 & 0.57 \\ 
            \rowcolor{gray!15} \cellcolor{white!0} & \multirow{-2}{*}{ResNet50} \cellcolor{white!0} & \checkmark & 1.00 & 0.99 & 0.88 & 0.99 & 0.75 & 0.93 & 0.72 & 0.94 & 0.08 & 0.25 & 0.57 & 0.99 & 0.82 & 0.23 & 0.72 \\ \cmidrule{2-18}
             &  & \xmarkg & 1.00 & 0.89 & 0.71 & 0.86 & 0.64 & 0.75 & 0.74 & 0.90 & 0.14 & 0.18 & 0.57 & 0.88 & 0.61 & 0.25 & 0.65 \\ 
             \rowcolor{gray!15} \cellcolor{white!0} \multirow{-4}{*}{DINO} & \multirow{-2}{*}{ViT-s} \cellcolor{white!0} & \checkmark & 1.00 & 0.99 & 0.94 & 0.99 & 0.92 & 0.98 & 0.89 & 0.99 & 0.15 & 0.28 & 0.63 & 0.99 & 0.77 & 0.32 & 0.77 \\ \midrule
              &  & \xmarkg & 1.00 & 0.25 & 0.08 & 0.16 & 0.01 & 0.51 & 0.54 & 0.84 & 0.18 & 0.16 & 0.23 & 0.79 & 0.16 & 0.18 & 0.36 \\ 
              \rowcolor{gray!15} \cellcolor{white!0} \multirow{-2}{*}{ISC-dt1} & \multirow{-2}{*}{EffNetv2} \cellcolor{white!0} & \checkmark & 1.00 & 0.57 & 0.16 & 0.33 & 0.01 & 0.88 & 0.79 & 0.97 & 0.20 & 0.24 & 0.29 & 0.97 & 0.26 & 0.26 & 0.49 \\ 
            \bottomrule
        \end{tabular}
    \endgroup
    }
\end{table}

\paragraph*{Generalization to other indexes.}
The method easily generalizes to other types of indexing structures, the only difference being in the indexation loss $\Lf$~\eqref{eq:objective}. 
We present some of them below:
\begin{itemize}[leftmargin=0.5cm,itemsep=0cm,topsep=-0.1cm]
    \item \textbf{PQ and OPQ}.\quad 
        In PQ~\citep{jegou2010pq}, a vector $x \in \R^d$ is approximated by $\qcompressed(x)$. 
        $\Lf$ reads $\norm{x-\qcompressed(x_o)}$. 
        In OPQ~\citep{ge2013optimized}, vectors are rotated by matrix $R$ before codeword assignment, such that $RR^\top = I$. 
        $\Lf$ becomes $\norm{x-R^\top\qcompressed(Rx_o)}$. 
    \item \textbf{IVF.} \quad 
        Here, we only do space partitioning. 
        Employing $\Lf = \norm{x- \qivf (x_o)}$ (``pushing towards the cluster centroid'') decreases the odds of $x$ falling in the wrong cell (see Sec.~\ref{chap2/sec:space_partitioning}).
        In this case, an issue can be that similar representations are all pushed together to a same centroid, which makes them less discriminate. 
        Empirically, we found that this does not happen because perceptual constraint in the image domain prevents features from getting too close.
    \item \textbf{LSH.} \quad 
        Locality Sensitive Hashing maps $x\in \R^d$ to a binary hash $b(x)\in \R^L$.
        It is commonly done with projections against a set of vectors, which give for $j \in [1,..,L]$, $b_j(x) = \mathrm{sign} (w_j^\top x)$.
        The objective $\Lf = -1/L \sum_{j} \mathrm{sign}(b(x_o))\cdot w_j^\top x$,  allows to push $x$ along the LSH directions and to improve the robustness of the hash.
\end{itemize}
\autoref{chap2/tab:indexes} presents the \rone and \muap obtained on the 50k query set. 
Again, results are always better in the active scenario.
We remark that active indexing has more impact on space partitioning techniques: the improvement for IVF is higher than with PQ and the LSH binary sketches. As to be expected, the impact is smaller when the indexing method is more accurate. 

\begin{table}[t]
    \caption{
        \rone averaged on transformations presented in Tab.~\ref{chap2/tab:act_vs_pas_retrieval} and \muap for different indexing structures
    }\label{chap2/tab:indexes}
    \centering
    \resizebox{0.7\linewidth}{!}{
    \begingroup
        \setlength{\tabcolsep}{3pt}
        \begin{tabular}{ c|c |cc|cc}
                \toprule
                 \multirow{2}{*}{Index} & \multirow{2}{*}{Search time} & \multicolumn{2}{c|}{\rone avg.} & \multicolumn{2}{c}{\muap} \\ 
                & & Passive & Activated & Passive & Activated     \\ \midrule
                IVF 1024 & 0.32 ms & 0.47 & \textbf{0.83} & 0.16 & \textbf{0.43} \\
                OPQ 8x8   & 5.71 ms & 0.92 & \textbf{0.94} & 0.48 & \textbf{0.55} \\
                PCA64, LSH & 0.99 ms & 0.72 & \textbf{0.83} & 0.25 & \textbf{0.39} \\
                \bottomrule
        \end{tabular}
    \endgroup
    }
\end{table}



\subsection{Qualitative results}

Example of activated images for the IVF-PQ are given in Fig.~\ref{chap2/fig:more_qualitative2} while the image metrics are as follows: PSNR$=43.8\pm 2.2$~dB, SSIM$=0.98 \pm 0.01$, and $\norm{I-I'}_\infty=14.5 \pm 1.2$.
These results are computed on 10k images, the $\pm$ indicates the standard deviation.
The perturbation is very hard to notice (if not invisible), even in flat areas of the images because the perceptual model focuses on textures.
We also see that the perturbation forms a regular pattern.
This is due to the image (bilinear) resize that happens before feature extraction.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth, trim={0 0.4cm 0 0.4cm}, clip]{chapter-2/figs/qualitative/diffs/qual_0.jpg}

    \includegraphics[width=0.9\textwidth, trim={0 0.4cm 0 0.4cm}, clip]{chapter-2/figs/qualitative/diffs/qual_2.jpg}
    \includegraphics[width=0.9\textwidth, trim={0 0.4cm 0 0.4cm}, clip]{chapter-2/figs/qualitative/diffs/qual_3.jpg}
    \includegraphics[width=0.9\textwidth, trim={0 0.4cm 0 0.4cm}, clip]{chapter-2/figs/qualitative/diffs/qual_4.jpg}
    \includegraphics[width=0.9\textwidth, trim={0 0.4cm 0 0.4cm}, clip]{chapter-2/figs/qualitative/diffs/qual_1.jpg}
    \captionsetup{font=small}
    \caption{More activated images. (Left) original images, (Middle) activated images, (Right) pixel-wise difference. Images are 
    \href{http://www.flickr.com/photos/87738888@N03/8039927199/}{R000005.jpg}, 
    \href{http://www.flickr.com/photos/79653482@N00/7766743380/}{R000045.jpg}, 
    \href{http://www.flickr.com/photos/12420018@N03/5452964454/}{R000076.jpg}, 
    \href{http://www.flickr.com/photos/56594044@N06/5933951717/}{R000172.jpg} and 
    \href{http://www.flickr.com/photos/31369133@N04/5570867046/}{R000396.jpg}.
    }
    \label{chap2/fig:more_qualitative2}
\end{figure}










\section{Ablations}



\subsection{Comparison with $\ell_\infty$ constraint embedding}\label{chap2/sec:linf}
\autoref{chap2/fig:linf_vs_perc} shows the same image activated using either the $\ell_\infty$ constraint (commonly used in the adversarial attack literature) or our perceptual constraint based on the JND model explained above.
Even with very small $\varepsilon$ ($4$ over 255 in the example bellow), the perturbation is visible  especially in the flat regions of the images, such as the sea or sky.

\citep{laidlaw2021perceptual} also show that the $\ell_\infty$ is not a good perceptual constraint.
They use the LPIPS loss~\citep{zhang2018unreasonable} as a surrogate for the HVS to develop more imperceptible adversarial attacks.
Although a similar approach could be used here, we found that at this small level of image distortion the LPIPS did not capture CM and LA as well as the handcrafted perceptual models present in the compression and watermarking literature.

\begin{figure}[b!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width= 0.48\textwidth]{chapter-2/figs/qualitative/linf4_psnr36,4.png}
        \includegraphics[width= 0.48\textwidth]{chapter-2/figs/qualitative/diff_linf.png}
        \captionsetup{font=small}
        \caption{$\ell_\infty=4$, $\mathrm{PSNR}=36.4$~dB, $\mathrm{SSIM}=0.91$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width= 0.48\textwidth]{chapter-2/figs/qualitative/linf24_psnr34,4.png}
        \includegraphics[width= 0.48\textwidth]{chapter-2/figs/qualitative/diff_perc.png}
        \captionsetup{font=small}
        \caption{$\ell_\infty=23$, $\mathrm{PSNR}=34.4$~dB, $\mathrm{SSIM}=0.94$}
    \end{subfigure}
    \captionsetup{font=small}
    \caption{
        Activated images, either with (a) the $\ell_\infty \leq 4$ constraint or with (b) our perceptual model (best viewed on screen).
        We give the corresponding measures between the original and the protected image, as well as the pixel-wise difference.
        The perturbation on the right is much less perceptible thanks to the perceptual model, even though its $\ell_\infty$ distance with the original image is much higher.
    }
    \label{chap2/fig:linf_vs_perc}
\end{figure}




\subsection{Image quality tradeoff}

For a fixed index and neural extractor, the performance of active indexing mainly depends on the scaling $\alpha$ that controls the activated image quality.
In Fig. \ref{chap2/fig:psnr}, we repeat the previous experiment for different values of $\alpha$ and plot the $\mu$AP against the average PSNR. 
As expected, lower PSNR implies better $\mu$AP. For instance, at PSNR 30~dB, the $\mu$AP is augmented threefold compared to the passive case.
Indeed, for strong perturbations the objective function of \eqref{eq:objective} can be further lowered, reducing even more the gap between representations and their quantized counterparts.


\begin{figure}[b!]
        \centering
        \includegraphics[width=0.55\textwidth]{chapter-2/figs/exp/psnr_loss_muap.pdf}
        \caption{PSNR trade-off. As the PSNR decreases, the {\color{orange}\muap (orange)} gets better, because the {\color{blue} distance (blue)} between activated representations $x$ and $q(x)$ decreases.}
        \label{chap2/fig:psnr}
\end{figure}

\autoref{chap2/fig:more_qualitative} gives example of an image activated at several values of perturbation strength $\alpha$ of Eq.~\eqref{eq:scaling} 
(for instance, for $\alpha=20$ the image has PSNR $27$dB and for $\alpha=1$ the image has PSNR $49$dB).
The higher the $\alpha$, the more visible the perturbation induced by the activation is.
Nevertheless, even with low PSNR values ($<35$dB), it is hard to notice if an image is activated or not.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.9\textwidth]{chapter-2/figs/qualitative/psnr/psnr.pdf}
    \captionsetup{font=small}
    \caption{Example of one activated image at different levels of $\alpha$.}
    \label{chap2/fig:more_qualitative}
\end{figure}



\subsection{Speeding-up the optimization}\label{chap2/sec:speedup}

In our experiments, the optimization is done using 10 iterations of gradient descent, which takes approximately 40ms/image.
If the indexation time is important (often, this is not the case and only the search time is), it can be reduced at the cost of some accuracy.

We activated 10k reference images, with the same IVF-PQ indexed presented in Sec.~\ref{chap2/sec:act_vs_passive} with only one step of gradient descent with a higher learning rate.
Activation times are computed on average. 
The \rone results in Tab.~\ref{chap2/tab:speedup} indicate that the speed-up in the image optimization has a small cost in retrieval accuracy.
Specifically, it reduces the \rone for unedited images. 
The reason is that the learning rate is too high: it can cause the representation to be pushed too far and to leave the indexing cell. 
This is why a higher number number of steps and a lower learning rate are used in practice.
If activation time is a bottleneck, it can however be useful to use less optimization steps.

\begin{table}[t!]
    \centering
    \captionsetup{font=small}
    \caption{\rone for different transformations applied before search, with either 1 step at learning rate 10, or 10 steps at learning rate 1.}
    \label{chap2/tab:speedup}
    \resizebox{\linewidth}{!}{
    \begingroup
        \setlength{\tabcolsep}{4pt}
        \def\arraystretch{1.1}
            \begin{tabular}{ l|l| *{15}{c}}
            \multicolumn{1}{c}{}           & \multicolumn{1}{l}{\rot{Activation}} & \rot{Identity} & \rot{Contr. 0.5} & \rot{Contr. 2.0} & \rot{Bright. 0.5} & \rot{Bright. 2.0} & \rot{Hue 0.2} & \rot{Blur 2.0}  & \rot{JPEG 50}  & \rot{Rot. 25} & \rot{Rot. 90} & \rot{Crop 0.5} & \rot{Resi. 0.5} & \rot{Meme} & \rot{Random} & \rot{Avg.} \\ \midrule
            Passive & - & 1.00 & 0.73 & 0.39 & 0.73 & 0.28 & 0.62 & 0.48 & 0.72 & 0.07 & 0.14 & 0.14 & 0.72 & 0.14 & 0.13 & 0.45 \\
            \rowcolor{gray!15} lr=1 - 10 steps & 39.8 ms/img & 1.00 & 1.00 & 0.96 & 1.00 & 0.92 & 1.00 & 0.96 & 0.99 & 0.10 & 0.50 & 0.29 & 1.00 & 0.43 & 0.32 & 0.75 \\
            lr=10 - 1 step &  4.3 ms/img & 0.99 & 0.99 & 0.92 & 0.99 & 0.84 & 0.99 & 0.95 & 0.99 & 0.10 & 0.39 & 0.25 & 0.99 & 0.36 & 0.27 & 0.72  \\
            \bottomrule
    \end{tabular}
    \endgroup
    }

\end{table}


\subsection{Data augmentation at indexing time and EoT}

Expectation over Transformations~\citep{athalye2018eot} was originally designed to create adversarial attacks robust to a set of image transformations.
We follow a similar approach to improve robustness of the marked image against a set of augmentations $\mathcal{T}$. 
At each optimization step, we randomly sample $A$ augmentations $\{t_i\}_{i=1}^A$ in $\mathcal{T}$ and consider the average loss: 
$\Lf = \sum_{i=1}^{A} \mathcal{L}(I,t_i;I_o) /A $.
In our experiments, $\mathcal{T}$ encompasses rotations, Gaussian blurs, color jitters and a differentiable approximation of the JPEG compression~\citep{shin2017jpeg}. 
$A$ is set to $8$ and we always take the un-augmented image in the chosen set of augmentations. 

We activated 10k reference images, with the same IVF-PQ as Sec.~\ref{chap2/sec:act_vs_passive} with or without using EoT.
Table \ref{chap2/tab:eot} shows the average \rone performance over the images submitted to different transformations before search. 
EoT brings a small improvement, specifically on transformations where base performance is low (\eg, rotation or crops here).
However, it comes at a higher computational cost since each gradient descent iteration needs $A$ passes through the network, and since fewer images can be jointly activated due to GPU memory limitations (we need to store and back-propagate through $A$ transformations for every image).
If the time needed to index or activate an image is not a bottleneck, using EoT can therefore be useful. 
Otherwise, it is not worth the computational cost.


\begin{table}[t!]
    \centering
    \captionsetup{font=small}
    \caption{\rone for different transformations applied before search, with or without EoT when activating the images.}
    \label{chap2/tab:eot}
    \resizebox{\linewidth}{!}{
    \begingroup
        \setlength{\tabcolsep}{4pt}
        \def\arraystretch{1.1}
            \begin{tabular}{ l |l| *{15}{c}}
            \multicolumn{1}{c}{}          & \multicolumn{1}{l}{\rot{Activation}}  & \rot{Identity} & \rot{Contr. 0.5} & \rot{Contr. 2.0} & \rot{Bright. 0.5} & \rot{Bright. 2.0} & \rot{Hue 0.2} & \rot{Blur 2.0}  & \rot{JPEG 50}  & \rot{Rot. 25} & \rot{Rot. 90} & \rot{Crop 0.5} & \rot{Resi. 0.5} & \rot{Meme} & \rot{Random} & \rot{Avg.} \\ \midrule
                Without EOT & 40 ms & 1.00 & 1.00 & 0.96 & 1.00 & 0.92 & 1.00 & 0.96 & 0.99 & 0.10 & 0.50 & 0.29 & 1.00 & 0.43 & 0.32 & 0.75 \\
               \rowcolor{gray!15} 
                With EOT & 870 ms & 1.00 & 1.00 & 0.95 & 1.00 & 0.92 & 1.00 & 0.95 & 0.99 & 0.14 & 0.64 & 0.33 & 1.00 & 0.45 & 0.33 & 0.76 \\
            \bottomrule
    \end{tabular}
    \endgroup
    }
\end{table}













\section{Active indexing vs. watermarking}\label{chap2/sec:watermarking}

\paragraph*{Discussion.}
Watermarking and active indexing both modify images for tracing and authentication, however there are significant differences between them.
Watermarking embeds arbitrary information into the image. The information can be a message, a copyright, a user ID, etc. 
In contrast, active indexing modifies it to improve the efficiency of the search engine.
Watermarking also focuses on the control over the False Positive Rate of copyright detection, \ie, a bound on the probability that a random image has the same message as the watermarked one (up to a certain distance).

Although watermarking considers different settings than indexing methods, it could also be leveraged to facilitate the re-identification of near-duplicate images. In this supplemental section, we consider it to address a use-case similar to the one we address in this chapter with our active indexing approach. 
In this scenario, the watermark encoder embeds binary identifiers into database images.
The decoded identifier is then directly mapped to the image (as the index of a list of images).

\paragraph*{Experimental setup.}
In the rest of the section, we compare active indexing against recent watermarking techniques based on deep learning. 
\begin{itemize}[leftmargin=0.5cm,itemsep=0cm,topsep=-0.1cm]
    \item For indexing, we use the same setting as in Sec.~\ref{chap2/sec:experimental} (IVF-PQ index with 1M reference images).
    When searching for an image, we look up the closest neighbor with the help of the index.
    \item For watermarking, we encode $20$-bit messages into images, which allows to represent $2^{20}\approx 10^6$ images (the number of reference images).
    When searching for an image, we use the watermark decoder to get back an identifier and the corresponding image in the database.
\end{itemize}
Like before, we use \rone as evaluation metric.
For indexing, it corresponds to the accuracy of the top-1 search result. 
For watermarking, the \rone also corresponds to the word accuracy of the decoding, that is the proportion of images where the message is perfectly decoded. 
Indeed, with $20$-bit encoding almost all messages have an associated image in the reference set, so an error on a single bit causes a mis-identification (there is no error correction\footnote{In order to provide error correction capabilities, one needs longer messages. This makes it more difficult to insert bits: in our experiments, with 64 bits we observe a drastic increase of the watermarking bit error rate.}).

We use two state-of-the-art watermarking methods based on deep learning:
SSL Watermarking presented in Chap.~\ref{chapter:ssl-watermarking}, which also uses an adversarial-like optimization to embed messages, and \Gls*{HiDDeN}~\citep{zhu2018hidden}, which encodes and decodes messages thanks to Conv-BN-ReLU networks.
The only difference with the original methods is that their perturbation $\delta$ is modulated by the handcrafted perceptual attenuation model. 
This approximately gives the same image quality, thereby allowing for a direct comparison between active indexing and watermarking.
To better compare with Chap.~\ref{chapter:ssl-watermarking} we also make the image optimization as similar as possible to Active indexing (10 steps with no augmentation).

\paragraph*{Results.}
\autoref{chap2/tab:watermarking} compares the \rone when different transformations are applied before search or decoding.
Our active indexing method is overall the best by a large margin. 
For some transformations, watermarking methods are not as effective as passive indexing, yet for some others, like crops for \Gls*{HiDDeN}, the watermarks are more robust.

\begin{table}[t]
    \centering
    \caption{\rone for different transformations applied before search, when using either watermarking or active indexing. Results are averaged on 1k images. Best result is in \textbf{bold} and second best in \textit{italic}. }
    \label{chap2/tab:watermarking}
    \resizebox{1.0\linewidth}{!}{
        \setlength{\tabcolsep}{4pt}
        \def\arraystretch{1.1}
            \begin{tabular}{ p{4.5cm}| *{14}{c}c}
            \multicolumn{1}{c}{}          & \rot{Identity} & \rot{Contr. 0.5} & \rot{Contr. 2.0} & \rot{Bright. 0.5} & \rot{Bright. 2.0} & \rot{Hue 0.2} & \rot{Blur 2.0}  & \rot{JPEG 50}  & \rot{Rot. 25} & \rot{Rot. 90} & \rot{Crop 0.5} & \rot{Resi. 0.5} & \rot{Meme} & \rot{Random} & \rot{Avg.} \\ \midrule
    Passive indexing       & \bf 1.00 & 0.73 & 0.39 & 0.73 & 0.28 & 0.62 & 0.48 & \it 0.72 & \it 0.07 & 0.14 & 0.14 & \it  0.72 & 0.14 &  0.13 & 0.45  \\ 
        Active indexing (ours) & \bf 1.00 & \bf 1.00 & \bf 0.96 & \bf 1.00 & \bf 0.92 & \bf 1.00 & \bf 0.96 & \bf 0.99 & \bf 0.10 & \bf 0.50 & \it 0.29 & \bf 1.00 & 0.43 & \bf 0.32 & \bf 0.75 \\ \midrule
    \parbox{4.5cm}{SSL Watermarking \\ \citep{fernandez2022sslwatermarking}} & \bf 1.00 & \it 0.98 & \it 0.53 & \it 0.98 & \it 0.63 & \it 0.85 & 0.13 & 0.00 & 0.00 & \it 0.15 & 0.11 & 0.00 & \it 0.46 & 0.07 & 0.42 \\ \midrule
             \parbox{4.5cm}{\Gls*{HiDDeN}\footnote{} \\ \citep{zhu2018hidden}} & \it 0.94 & 0.87 & 0.36 & 0.85 & 0.55 & 0.00 & \it 0.81 & 0.00 & 0.00 & 0.00 & \bf 0.92 & 0.44 & \bf 0.77 & \it 0.16 & \it 0.48 \\ 
            \bottomrule
    \end{tabular}
    }
\end{table}


\footnotetext{Our implementation. 
As reported in other papers from the literature, results of the original paper are hard to reproduce.
Therefore to make it work better, our model is trained on higher resolution images (224$\times$224), with a payload of $20$-bits, instead of 30 bits embedded into 128$\times$128. 
Afterwards, the same network is used on images of arbitrary resolutions, to predict the image distortion which is later rescaled as in Eq.~\eqref{eq:scaling}.
In this setting the watermark can not always be inserted (6\% failure).}












