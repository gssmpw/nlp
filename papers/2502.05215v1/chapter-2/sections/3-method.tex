\newcommand{\nprobe}{{k'}}
\def\qivf{q_\mathrm{c}}
\newcommand{\qcompressed}{q_\mathrm{f}}








\section[Preliminaries: representation learning and indexing]{Preliminaries: representation learning \\and indexing}


For the sake of simplicity, the exposure focuses on image representations from SSCD networks~\citep{pizzi2022sscd} and the indexing technique IVF-PQ~\citep{jegou2010pq}, since both are typically used for ICD.
Extensions to other methods can be found in Sec.~\ref{chap2/sec:generalization}.


\subsection{Deep descriptor learning}

Metric embedding learning aims to learn a mapping $f: \R^{c\times h\times w} \to \R^d$, such that measuring the similarity between images $I$ and $I'$ amounts to computing the distance $\norm{f(I) - f(I')}$. 
In recent works, $f$ is typically a neural network trained with self-supervision on raw data to learn metrically meaningful representations.
Methods include contrastive learning~\citep{chen2020simclr}, self-distillation~\citep{grill2020bootstrap, caron2021dino}, or masking random patches of images~\citep{he2022masked, assran2022masked}.
In particular, SSCD~\citep{pizzi2022sscd} is a training method specialized for ICD.
It employs the contrastive self-supervised method SimCLR~\citep{chen2020simclr} and entropy regularization~\citep{sablayrolles2018catalyser} to improve the distribution of the representations.
We provide more insights on these methods in Sec.~\ref{chap1/par:ssl}.

\subsection{\Gls*{indexing}}
Given a dataset $\mathcal X = \{x_i\}_{i=1}^{n}\subset \R^d$ of $d$-dimensional vector representations extracted from $n$ images and a query vector $x_q$, we consider the indexing task that addresses the problem: 
\begin{align}
    x^* := \mathop{\mathrm{argmin}}_{x \in \mathcal X} 
    \;  \norm{x - x_q}.  
\end{align}
This exact nearest neighbor search is not tractable over large-scale databases.
Approximate search algorithms lower the amount of scored items thanks to space partitioning and/or accelerate the computations of distances thanks to quantization and pre-computation. 

\paragraph{Space partitioning and cell-probe algorithms.}
As a first approximation, 
nearest neighbors are sought only within a fraction of $\mathcal{X}$:
at indexing time, $\mathcal{X}$ is partitioned into $\mathcal X = \bigcup_{i=1}^{b} \mathcal{X}_i$.
At search time, an algorithm $Q: \R^d \to \{1,..,b\}^\nprobe$ determines a subset of $\nprobe$ buckets in which to search, such that $\nprobe=|Q(x_q)| \ll b$, yielding the approximation: 
\begin{align}
    \mathop{\mathrm{argmin}}_{x \in \mathcal X} \; \norm{x-x_q}
    \approx 
    \mathop{\mathrm{argmin}}_{x \in \bigcup_{i\in Q(x_q)} \mathcal{X}_i} \; \norm{x-x_q}.
\end{align}
A well known partition is the KD-tree~\citep{bentley1975kdtree} that divides the space along predetermined directions.
Subsequently, locality sensitive hashing (LSH)~\citep{indyk1998lsh, gionis1999lsh} and derivative~\citep{datar2004lsh,pauleve2010locality} employ various hash functions for bucket assignment, which implicitly partitions the space.

We focus on the popular clustering and Inverted Files methods~\citep{sivic2003video}, herein denoted by IVF. 
They employ a codebook $\mathcal{C} = \{c_i\}_{i=1}^{k}\subset\R^d$ of $k$ centroids (also called ``visual words'' in a local descriptor context), for instance learned with k-means over a training set of representations. 
Then, $Q$ associates $x$ to its nearest centroid $\qivf(x)$ such that the induced partition is the set of the $k$ Voronoï cells.
When indexing $x$, the IVF stores $x$ in the bucket associated with $c_i=\qivf(x)$.
When querying $x_q$, IVF searches only the $\nprobe$ buckets associated to centroids $c_i$ nearest to $x_q$.

\paragraph{Efficient metric computation and product quantization.} 
Another approximation comes from compressed-domain distance estimation. 
Vector Quantization (VQ) maps a representation $x \in \mathbb{R}^d$ to a codeword $\qcompressed(x) \in \mathcal{C} = \{C_i\}_{i=1}^{K}$.
The function $\qcompressed$ is often referred to a \emph{quantizer} and $C_i$ as a \emph{reproduction value}.
The vector $x$ is then stored as an integer in $\{1, .., K\}$ corresponding to $\qcompressed(x)$.
The distance between $x$ and query $x_q$ is approximated by $\norm{\qcompressed(x) - x_q}$, which is an ``asymmetric'' distance computation (ADC) because the query is not compressed. 
This leads to: 
\begin{align}
    \mathop{\mathrm{argmin}}_{x \in \mathcal X} \;  \norm{x-x_q}  
    \approx 
    \mathop{\mathrm{argmin}}_{x \in \mathcal X} \;  \norm{\qcompressed(x)- x_q} . 
\end{align}
Binary quantizers (\aka, sketches, \citep{charikar2002similarity} lead to efficient computations but inaccurate distance estimates~\citep{weiss2008spectral}.
Product Quantization (PQ)~\citep{jegou2010pq} or derivatives \citep{ge2013optimized} offer better estimates.
In PQ, a vector $x\in \R^d$ is split into $m$ subvectors in $\R^{d/m}$: $x=(x^1, \ldots, x^m)$.
The product quantizer then quantizes the subvectors: $\qcompressed: x \mapsto (q^1(x^1), \ldots, q^m(x^m))$. 
If each subquantizer $q^j$ has $K_s$ reproduction values, the resulting quantizer $\qcompressed$ has a high  $K=(K_s)^m$. 
The squared distance estimate is decomposed as:
\begin{align}
    \norm{\qcompressed(x)-x_q}^2 = \sum_{j=1}^m \norm{q^j(x^j)-x_q^j}^2.
\end{align}
This is efficient since $x$ is stored by the index as $\qcompressed (x)$ which has $m\log_2 K_s$ bits, and since summands can be precomputed without requiring decompression at search time.













\section{Active indexing}\label{section:method}

Active indexing takes as input an image $I_o$, adds the image representation to the index and outputs an activated image $I^\acti$ with better traceability properties for the index.
It makes the feature representation produced by the neural network more compliant with the indexing structure. %
The activated image is the one that is disseminated on the platform, therefore the alteration must not degrade the perceived quality of the image.

Images are activated by an optimization on their pixels. 
The general optimization problem reads:
\vspace*{-1em}
\begin{align}
    I^\acti := \mathop{\mathrm{argmin}}_{I \in \mathcal{C}(I_o)} 
    \; \mathcal{L}\left(I;I_o\right),
    \label{eq:active_image}
\end{align}
where $\mathcal{L}$ is an indexation loss dependent on the indexing structure, $\mathcal{C}(I_o)$ is the set of images perceptually close to $I_o$.
We provide and overview of the active image indexing optimization in Alg.~\ref{alg:1} and in Fig.~\ref{chap2/fig:fig1}.

\begin{wrapfigure}{R}{0.45\textwidth}
\vspace{-0.7cm}
\resizebox{1.0\linewidth}{!}{
\begin{minipage}{0.5\textwidth}
    \begin{algorithm}[H]
    \caption{Active indexing for IVF-PQ}
    \label{alg:1}
        \begin{algorithmic}
            \State \textbf{Input}: $I_o$: original image; $f$: feature extractor;
            \State Add $x_o = f(I_o)$ to Index, get $q(x_o)$;
            \State Initialize $\delta_0 = 0_{(c\times h\times w)}$;
            \For{$t = 0, ..., N-1$}
            \State $I_t \gets I_o + \alpha \,.\, H_{\mathrm{JND}}(I_o) \odot \mathrm{tanh}(\delta_t)$ 
            \State $x_{t}\gets f(I_{t})$
            \State $\mathcal{L} \gets \Lf(x_{t}, q(x_o)) + \lambda \Li(\delta_t)$
            \State $\delta_{t+1} \gets \delta_t + \eta \times \mathrm{Adam}(\mathcal{L})$
            \EndFor
            \State \textbf{Output}: $I^\acti=I_N$ activated image 
        \end{algorithmic}
    \end{algorithm}
\end{minipage}
}
\end{wrapfigure}




\subsection{Image optimization dedicated to IVF-PQ (``activation'')}

The indexing structure IVF-PQ involves a coarse quantizer $\qivf$ built with k-means clustering for space partitioning, and a fine product quantizer $\qcompressed$ on the residual vectors, such that a vector $x \in \R^d$ is approximated by $q(x) = \qivf(x) + \qcompressed\left( x-\qivf(x) \right)$.

We solve the optimization problem~\eqref{eq:active_image} by iterative gradient descent, back-propagating through the neural network back to the image.
The method is classically used in adversarial example generation~\citep{szegedy2013intriguing, carlini2017c&w} and watermarking~\citep{vukotic2020classification, fernandez2022sslwatermarking}.

Given an original image $I_o$, the loss is an aggregation of the following objectives: 
\begin{align} \label{eq:objective}
    & \Lf(x,q(x_o)) = \norm{x - q(x_o)}^2
    \textrm{\qquad  with } x_o = f(I_o) ,\, x = f(I) \\
    & \Li(I,I_o) = \norm{I - I_o}^2.
\end{align}
$\Li$ is a regularization on the image distortion.
$\Lf$ is the indexation loss that operates on the representation space.
$\Lf$ is the Euclidean distance between $x$ and the target $q(x_o)$ and its goal is to push the image feature towards $q(x_o)$.
With IVF-PQ as index, the representation of the activated image gets closer to the quantized version of the original representation, but also closer to the coarse centroid.
Finally, the losses are combined as 
$\mathcal{L}(I;I_o) = \Lf(x,q(x_o)) + \lambda \Li(I,I_o)$.





\subsection{Perceptual attenuation} 


\paragraph*{Overview.}
It is common to optimize a perturbation $\delta$ added to the image, rather than the image itself. 
The adversarial example literature often considers perceptual constraints in the form of an $\ell_p$-norm bound applied on $\delta$ (\citet{madry2017towards} use $\norm{\delta}_\infty < \varepsilon = 8/255$).
Although a smaller $\varepsilon$ makes the perturbation less visible, this constraint is not optimal for the human visual system (HVS), \eg, perturbations are more noticeable on flat than on textured areas of the image.

We employ a handcrafted perceptual attenuation model
based on a Just Noticeable Difference (JND) map~\citep{wu2017enhanced}, that adjusts the perturbation intensity according to luminance and contrast masking.
Given an image $I$, the JND map $H_{\mathrm{JND}}(I)\in \R^{c\times h \times w}$ models the minimum difference perceivable by the HVS at each pixel and additionally rescales the perturbation channel-wise since the human eye is more sensible to red and green than blue color shift.

The relation that links the image $I$ sent to $f$, $\delta$ being optimized and the original $I_o$, reads:
\vspace*{-1em}
\begin{align}
    I = I_o + \alpha \,.\, H_{\mathrm{JND}}(I_o) \odot \mathrm{tanh}(\delta),
    \label{eq:scaling}
\end{align}
with $\alpha$ a global scaling parameter that controls the strength of the perturbation and $\odot$ the pointwise multiplication. 
Since the hyperbolic tangent imposes a bound on the distortion $\delta$, $\alpha$ is directly linked to a lower bound on the PSNR.
Coupled with the regularization $\Li$~\eqref{eq:objective}, it enforces that the activated image is perceptually similar, \ie, $I^\acti\in \mathcal{C}(I_o)$ as required in~\eqref{eq:active_image}.
Note that in the above equation, $H_{\mathrm{JND}}$ does not need to be differentiable since it is computed on the original image $I_o$ and not $I$, therefore it is independent of $\delta$.

\paragraph*{Just Noticeable Difference map}

\begin{figure}[b!]
    \centering
    \hspace{1em}
    \includegraphics[width= 0.3\textwidth]{chapter-2/figs/qualitative/ref.jpg}
    \hspace{0.1\textwidth}
    \captionsetup{font=small}
    \includegraphics[width= 0.3\textwidth]{chapter-2/figs/qualitative/heatmap.png}
    \caption[Caption]{A reference image $I$ from DISC21 (\href{http://www.flickr.com/photos/61368956@N00/5060849004/}{R002815.jpg}), and the associated perceptual heatmap $H_{\mathrm{JND}}(I)$.}
    \label{chap2/fig:heatmap}
\end{figure}

The maximum change that the human visual system (HVS) cannot perceive is sometimes referred to as the just noticeable difference (JND)~\citep{krueger1989reconciling}. 
It is used in many applications, such as image/video watemarking, compression, quality assessment (JND is also used in audio). 

JND models in pixel domain directly calculate the JND at each pixel location (\ie, how much pixel difference is perceivable by the HVS). The JND map that we use is based on the work of \citep{chou1995perceptually}.
We use this model for its simplicity, its efficiency and its good qualitative results.
More complex HVS models could also be used if even higher imperceptibility is needed (\citep{watson1993dct, yang2005just, zhang2008just, jiang2022jnd} to cite a few).
The JND map takes into account two characteristics of the HVS, namely the luminance adaptation (LA) and the contrast masking (CM) phenomena. We follow the same notations as \citep{wu2017enhanced}.

The CM map $\mathcal{M}_C$ is a function of the image gradient magnitude $\mathcal{C}_l$ (the Sobel filter of the image):
\begin{equation}
    \mathcal{M}_C(x) = 0.115 \times 
    \frac{\alpha \cdot \mathcal{C}_l(x)^{2.4}}
    { \mathcal{C}_l(x)^{2} + \beta^2}
    \textrm{\quad , with \,}
    \mathcal{C}_l = \sqrt{ \nabla_x I(x)^2 + \nabla_y I(x)^2},
\end{equation}
where $x$ is the pixel location, $I(x)$ the image intensity, $\alpha = 16$, and $\beta = 26$. 
It is an increasing function of $\mathcal{C}_l$, meaning that the stronger the gradient is at $x$, the more the image is masking a local perturbation, and the higher the noticeable pixel difference is.

LA takes into account the fact that the HVS presents different sensitivity to background luminance (\eg, it is less sensible in dark backgrounds).
It is modeled as:
\begin{align}
    \mathcal{L}_A (x) =
    \begin{cases}
        \displaystyle 17 \times \left( 1-\sqrt{\frac{B(x)}{127}} \right) & \textrm{\quad if\,} B(x)<127 \\
        \displaystyle \frac{3 \times \left( B(x) - 127 \right)}{128} +3 & \textrm{\quad if\,}B(x)\geq 127, 
    \end{cases}
\end{align}
where $B(x)$ is the background luminance, which is calculated as the mean luminance value of a local patch centered on $x$.

Finally, both effects are combined with a nonlinear additivity model:
\begin{equation}
    H_{\mathrm{JND}} = \mathcal{L}_A + \mathcal{M}_C - C \cdot \min \{ \mathcal{L}_A, \mathcal{M}_C \},
\end{equation}
where $C$ is set to $0.3$ and determines the overlapping effect. 
For color images, the final RGB heatmap is $H_{\mathrm{JND}} = [\alpha_R H, \alpha_G H, \alpha_B H]$, where $(\alpha_{R}, \alpha_{G}, \alpha_{B})$ are inversely proportional to the mixing coefficients for the luminance: $(\alpha_{R}, \alpha_{G}, \alpha_{B}) = 0.072 / (0.299, 0.587, 0.114)$.




\subsection{Impact on the indexing performance}

\autoref{chap2/fig:fig1} illustrates that the representation of the activated image gets closer to the reproduction value $q(f(I_o))$, and farther away from the Voronoï boundary.
This is expected to make image similarity search more robust because 
(1) it decreases the probability that $x=f(t(I_o))$ ``falls'' outside the bucket; and
(2) it lowers the distance between $x$ and $q(x)$,  improving the PQ distance estimate.

Besides, by design, the representation stored by the index is invariant to the activation.
Formally stated, consider two images $I$, $J$, and one activated version $J^\acti$ together with their representations $x,y,y^\acti$. 
When querying $x=f(I)$, the distance estimate is $\norm{q(y^\acti)- x} = \norm{q(y)- x}$, so the index is oblivious to the change $J\rightarrow J^\acti$.
This means that the structure can index passive and activated images at the same time.
Retrieval of activated images is more accurate but the performance on passive images does not change. 
This compatibility property makes it possible to select only a subset of images to activate, but also to activate already-indexed images at any time.
