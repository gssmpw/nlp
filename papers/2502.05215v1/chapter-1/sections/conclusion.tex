


\section{Conclusion}

This chapter proposes a way to robustly and invisibly embed information into digital images, by watermarking onto latent spaces of off-the-shelf self-supervised networks. 
By incorporating data augmentation and constraints into the marking process, our zero-bit watermarking method greatly improves performance over the baseline \citep{vukotic2020classification}.
It is robust against a wide range of transformations while keeping high fidelity with regards to the original images, and ensuring a very low false positive rate for the detection.
When we extend the method to multi-bit watermarking, we obtain promising results, comparable to the state-of-the-art in deep data hiding, and even better with regards to some transformations of the image (\eg, JPEG compression or blur).

Most interestingly, networks trained with self-supervision naturally generate excellent watermarking spaces, without being explicitly trained to do so. 
They allow for a wide range of trade-offs between robustness and quality, and can be used at any resolution.
This makes them well-suited for real-life applications like content moderation on social networks, where the watermarking must work on images of varying resolution and under different operating points.
However, watermarking images with our method is expensive since it is not a single pass forward. 
On the opposite, more recent data hiding schemes like the one used in Chapters \ref{chapter:stable-signature} and \ref{chapter:audioseal}, based on end-to-end architectures for the most part, only need one forward pass to watermark images (at the cost of an expensive one-time-training process).
It is also worth noting that using foundational open-weights models opens the door to new adversarial attacks, as shown by~\citep{kinakh2024evaluation}.
