
\section{Watermarking with SSL networks}\label{chap1/section:0bit}

Our method adopts the framework of Vukoti\'c \etal\citep{vukotic2020classification}. 
We improve it by showing that networks build better watermarking features when trained with self-supervision, and by introducing data-augmentation at marking time. 
We also extend it to multi-bit watermarking.


\subsection{Using self-supervised networks as feature extractors}\label{chap1/subsec:ssl_as_feature_extractor}


\paragraph*{Motivation.}
We denote the image space by $\Ispace$ and the feature space by $\Fspace=\real^d$.
The feature extractor $\phi:\Ispace\to\Fspace$ must satisfy two properties: 
(1) geometric and valuemetric transformations on image $I$ should have minimal impact on feature $\phi(I)$,
(2) it should be possible to alter $\phi(I)$ by applying invisible perturbations to the image in order to embed the watermark signal.

We choose $\phi$ as a neural network pre-trained by self-supervised learning (SSL).
Our assumption is that SSL produces excellent marking spaces because 
(1) it explicitly trains features to be invariant to data augmentation; and 
(2) it does not suffer from the \textit{semantic collapse} of supervised classification, that gets rid off any information that is not necessary to assign classes~\citep{doersch2020crosstransformers}. 
From the recent SSL methods of the literature (contrastive learning~\citep{chen2020simclr,he2020moco}, statistical prior~\citep{zbontar2021barlow}, teacher/ student architecture~\citep{grill2020byol}), we select DINO~\citep{caron2021dino} for its training speed and content-based image retrieval performance.

\paragraph*{DINO pre-training.}
DINO~\citep{caron2021dino} pertains to the teacher/ student approach.
The teacher and the student share the same architecture.
Self distillation with no label trains the student network to match the outputs of the teacher
on different views of the same image. 
The student is updated by gradient descent while the teacher's parameters are updated as an exponential moving average of the student's parameters: $\theta_{t} \leftarrow \lambda \theta_{t} + (1-\lambda) \theta_{s}$, with $\lambda\lessapprox 1$.
The invariance of the features is ensured by random augmentations during the training: valuemetric (color jittering, Gaussian blur, solarization) and geometric (resized crops) transformations.
Furthermore, DINO encourages local-to-global correspondence by feeding only global views to the teacher while the student sees smaller crops. 


\begin{figure}[bh]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter-1/figs/pca-whitening.pdf}
    \caption{
        PCA-whitening. 
        The feature vectors are centered and decorrelated by a learned linear transformation.
        This is used to increase the performance of the watermarking method, and to ensure statistical guarantees in the zero-bit setup.
    }
    \label{chap1/fig:whitening}
\end{figure}

\paragraph*{Normalization layer.}\label{chap1/par:whitening} 
The watermark embedding must drive the feature to an arbitrary space region (defined by a secret key and the message to hide).
Therefore, it is essential that the features are not concentrated onto a manifold far away from this region. 
Moreover, for natural images, the features should be uniformly distributed on the hypersphere to provide for accurate false positive rates.
Although Wang \etal\citep{wang2020uniformity} show that contrastive learning optimizes the uniformity of the features on the hypersphere, it does not occur in practice with DINO.
To alleviate the issue, the output features are transformed by PCA-whitening (\aka, PCA-sphering). 
This learned linear transformation~\citep{jegou2012negative} outputs centered vectors with unit covariance of dimension $d=2048$. 


\subsection{Embedding with back-propagation and augmentation}\label{chap1/subsec:embedding_algorithm}


The marking takes an original image $\Io$ and outputs a visually similar image $\Iw$.
In the image space $\Ispace$, the distortion is measured by $\Li:\Ispace\times\Ispace\to\real_+$. An example is the MSE:
$\Li(\Iw,\Io) = \|\Iw-\Io\|^2 / h/w$, but it could be replaced by perceptual image losses such as LPIPS~\citep{zhang2018unreasonable}.

In the feature space $\Fspace$, we define a region $\mathcal{D}$ which depends on a secret key (zero-bit and multi-bit setups) and the message to be hidden (only in multi-bit setup). 
Its definition is deferred to Sect.~\ref{chap1/sec:modulation} together with the loss $\Lw:\Fspace\to\real$ that captures how far away a feature $x\in\Fspace$ lies from $\mathcal{D}$. %
We also define a set $\mathcal T$ of augmentations, which include rotation, crops, blur, etc., each with a range of parameters.
$\Tr(I,t)\in\Ispace$ denotes the application of transformation $t\in\mathcal T$ to image $I$.

If the feature extractor is perfectly invariant, $\phi(\Tr(I,t)) \approx \phi(I) $ so $\phi(\Tr(I,t))$ lies inside $\mathcal{D}$ if $\phi(I)$ does. 
To ensure this, the watermarking uses data augmentation.
The losses $\Lw$ and $\Li$ are combined in:
\begin{equation}
\label{chap1/eq:global_loss}
    \L(I, \Io, t) := \lambda \Lw(\phi(\Tr(I,t))) + \Li(I,\Io).
\end{equation}

The term $\Lw$ aims to push the feature of \emph{any transformation} of $\Iw$ into $\mathcal D$, while the
term $\Li$ favors low distortion.
The training approach is typical for the adversarial attacks literature \citep{goodfellow2014adversarial, szegedy2013intriguing}: 
\begin{equation}
\label{chap1/eq:Watermarking}
    \Iw := \arg \min_{I\in \mathcal{C}(\Io)}
    \mathbb{E}_{t\sim \mathcal T} [\L(I, \Io, t)]
\end{equation}
where $\mathcal{C}(\Io)\subset\Ispace$ is the set of admissible images w.r.t. the original one. 
It is defined by two steps of normalization applied to the pixel-wise difference $\delta = I - \Io$:
(1) we apply a SSIM~\citep{wang2004ssim} heatmap attenuation, which scales $\delta$ pixel-wise to hide the information in perceptually less visible areas of the image;
(2) we set a minimum target PSNR and rescale $\delta$ if this target is exceeded.


The minimization is performed by stochastic gradient descent since the quality constraints, $\Tr$ and $\phi$ are differentiable w.r.t. the pixel values.
Stochasticity comes from the fact that
expectation $\mathbb{E}_{t\sim \mathcal T}$ is approximated by sampling according to a distribution over $\mathcal T$.
The final image is the rounded version of the update after $K$ iterations.




\subsection{Detection and decoding}\label{chap1/sec:modulation} 


We consider two scenarios: zero-bit (detection only) and multi-bit watermarking (decoding the hidden message).  
Contrary to HiDDeN~\citep{zhu2018hidden} and followers, our decoding is mathematically sound.

\paragraph*{Zero-bit.}\label{chap1/hd:zero}
From a secret key $a\in\Fspace$ s.t. $\|a\|=1$, the detection region is the dual hypercone:
\begin{equation}
     \mathcal{D} := \left\{  x \in \R^d :  \abs{ x\T  a} > \norm{ x} \cos (\theta) \right\}.
\end{equation}
It is well grounded because the False Positive Rate (FPR) is given by:
\begin{align}
   \mathrm{FPR} :=&\; \mathbb{P}\left(\phi(I)\in \mathcal D \mid \text{``key } a \text{ uniformly distributed''}\right)\nonumber\\
            =&\; 1-I_{\cos^2(\theta)} \left(\frac{1}{2}, \frac{d-1}{2} \right)
   \label{chap1/eq:FPR}
\end{align}
where $I_\tau (\alpha, \beta)$ is the regularized Beta incomplete function (proof in App.~\ref{app/ssl-watermarking}).
The best embedding is obtained by increasing the following function under the distortion constraint:
\begin{equation}
   - \Lw(x) = (x^\top a)^2 -  \|x\|^2 \cos^2\theta.
\end{equation}
This quantity is negative when $x\notin\mathcal{D}$ and positive otherwise.
Cox~\etal originally called it the robustness estimate~\cite[Sec. 5.1.3]{cox2007digital}. 
This hypercone detector has been proved to be optimal under the asymptotical Gaussian setup~\citep{furon2019dualhypercone, merhav2008optimal}.

\paragraph*{Multi-bit}
We now assume that the message to be hidden is  $m = (m_1, ..., m_k) \in \{-1,1\}^k$. 
The decoder retrieves $\hat{m}=D(I)$.
Here, the secret key is a randomly sampled orthogonal family of carriers $a_1,...., a_k \in \R^d$. 
We modulate $m$ into the signs of the projection of the feature $\phi(I)$ against each of the carriers, so the decoder is: 
\begin{equation}
D(I) = \left[\mathrm{sign}\left(\phi(I)^\top a_1\right), ..., \mathrm{sign}\left(\phi(I)^\top a_k\right)\right].
\end{equation}

At marking time,  the functional is now defined as the hinge loss with margin $\mu\geq 0$ on the projections: 

\begin{equation}\label{chap1/eq:loss_multibit}
    \Lw(x) = \frac{1}{k} \sum_{i=1}^k \max \left( 0, \mu - (x^\top a_i).m_i \right).
\end{equation}


\subsection{Overview of the watermarking algorithm}


\begin{algorithm}
\caption{Watermarking algorithm}\label{chap1/alg:0bit}
\begin{algorithmic}
\item \textbf{Inputs}: $I_0 \in \mathcal{I}$, targeted PSNR in dB
    \item \quad if zero-bit: $ \textrm{FPR} \in [0,1]$, $a \in \R^d$
    \item \quad if multi-bit: $m \in \{-1,1\}^k$, $(a_i)_{i=1...k} \in \R^{k\times d}$
\item \textbf{Embedding}: 
    \item \quad if zero-bit: compute $\theta (\textrm{FPR})$
    \item \quad $I_w \gets I_0$
    \item \quad For $i=1, ..., n_{iter}$:
    \item \quad \quad $I_w \xleftarrow{\textrm{constraints}} I_w$ \Comment{impose constraints}
    \item \quad \quad $I_w \xleftarrow{t \sim \mathcal T} \textrm{Tr}(I_w, t)$ \Comment{sample \& apply augmentation}
    \item \quad \quad $x \gets \phi (I_w)$ \Comment{get feature of the image}
    \item \quad \quad $\mathcal{L} \gets \lambda \mathcal L_w(x) + \mathcal L_i(I_w, I_0) $ \Comment{compute loss}
    \item \quad \quad $I_w \gets I_w + \eta \times \mathrm{Adam}(\mathcal{L})$ \Comment{update the image}
    \item \quad Return $I_w$
\item \textbf{Detecting}: 
    \item \quad $x \gets \phi(I_m)$
    \item \quad if zero-bit:
    \item \quad \quad Detect if $x\in \mathcal D \Longleftrightarrow (x^Ta)^2-\norm{x}^2\cos^2\theta >0$
    \item \quad if multi-bit:
    \item \quad \quad Return $\left[\mathrm{sign}\left(x^\top a_1\right), ..., \mathrm{sign}\left(x^\top a_k\right)\right]$
\end{algorithmic}
\end{algorithm}
