
\section{Related work} \label{chap1/sec:relatedwork}

\paragraph*{Zero-bit watermarking with pre-trained neural networks.}
\cite{vukotic2018deep} mark images with a neural network pre-trained on supervised classification, instead of relying on encoder/decoder architectures like the one presented in Sec.~\ref{chap0/sec:deep learning-watermarking}.
The network plays the role of the transform in the TEmIt approach. 
Since it has no explicit inverse, a gradient descent to the image pixels ``pushes'' the image feature vector into a hypercone. 
The follow-up work~\citep{vukotic2020classification} increases the inherent robustness of the network by applying increasingly harder data augmentation at pre-training. 
It offers a guarantee on the false positive rate without requiring to train a network explicitly for watermarking, but no multi-bit version was proposed.


\paragraph*{Self-Supervised Learning}\label{chap1/par:ssl}
(SSL) does not use any labels and leverages the underlying structure of the data for supervision.
Its goal is to produce visual representations that contain high-level semantic information and are invariant to transformations of the input image.
Early SSL methods apply a transformation to the input image and train the network to predict the transformation.
For instance, \citet{doersch2015unsupervised} predict the relative position of two patches cropped from an image, 
\cite{gidaris2018unsupervised} predict a random rotation applied to an image.
Newer methods directly create features invariant to the image transformations~\citep{chen2020simclr,he2020moco}. 
They use contrastive learning to match positive pairs of images, that are transformations of each other; and keep apart negative pairs, that are different images of the batch. 
The challenges for these methods are (a) coverage, since it is impossible to cover the distribution of all the possible pairs and (b) mode collapse, when the features of all images become the same.
Non-contrastive methods avoid mode collapse in other ways\footnote{\cite{garrido2022duality} have later shown that these classes of methods are in fact very related.}.
Barlow Twins~\citep{zbontar2021barlow} uses a statistical prior of feature decorrelation to learn statistically independent features while preventing collapse.
BYOL~\citep{grill2020byol} uses an online network to predict the features of the teacher network, and does not use negative pairs.
In this work, we use DINO~\citep{caron2021dino}, which trains a student network to predict the features of the teacher network, in a similar way to BYOL.
