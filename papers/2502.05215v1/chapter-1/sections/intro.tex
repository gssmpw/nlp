
\section{Introduction}\label{chap1/sec:introduction}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth, clip, trim=0 1cm 0 0]{chapter-1/figs/splash.pdf}
    \caption{
    A self-supervised network trained with DINO \citep{caron2021dino} builds a latent space in which the watermark is embedded.
    Its effect is to shift the image's feature into a well-specified region of the latent space, such that transformations applied during transmission do not move much the feature. 
    The detection (zero-bit watermarking setup) or extraction (multi-bit watermarking setup) is performed in the same latent space.}
    \label{chap1/fig:splash}
\end{figure*}


The classic approach to image watermarking, coined \emph{TEmIt} (Transform, Embed, Inverse transform) by T. Kalker, embeds the watermark signal in the feature space of a transform (\eg, DFT, DCT, Wavelet). 
It provides coefficients that are supposedly reliable for watermarking by design because they are perceptually significant as conceptualized in~\cite[Sec.~8.1.3]{cox2007digital}. 
Deep learning based approaches for image watermarking improve the robustness to a broad range of alterations thanks to neural networks offering a reliable latent space where to embed the information.
Most of them explicitly train a watermarking network to be invariant to a set of image perturbations -- in this case, networks are usually encoder/decoder architectures trained end-to-end for watermarking~\citep{zhu2018hidden,wen2019romark, ahmadi2020redmark} -- while few others directly mark into the semantic space resulting from a supervised training over a given set of classes like ImageNet~\citep{vukotic2020classification}.

Our key insight is to leverage the properties of \emph{self-supervised} networks to watermark images.
Ideally, according to~\cite{cox2007digital}, a perceptually significant coefficient does not change unless the visual content of the image is different.
Similarly, some self-supervised methods aim to create representations invariant to augmentations, without explicit knowledge of the image semantics~\citep{caron2021dino,grill2020byol}.
These pre-trained networks offer us the desired embedding space ``for free'', saving us the heavy training of end-to-end architectures like HiDDeN~\citep{zhu2018hidden}.

In order to robustly embed in the latent spaces, gradient descent is performed over the pixels of the images. 
To further ensure both robustness and imperceptibility of the watermarks, we include data augmentation and image pre-processing at marking time.

Our contributions are the following:
\begin{itemize}
    \setlength\itemsep{0.1em}
    \item We provide a watermarking algorithm that can encode both marks and binary messages in the latent spaces of any pre-trained network;
    \item We leverage data augmentation at marking time;
    \item We experimentally show that networks trained with self-supervision provide excellent embedding spaces.
\end{itemize}
