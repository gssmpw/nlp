
\begin{figure}[b!]
    \centering
    \includegraphics[width=1.0\textwidth, trim={0cm 0.7cm 0cm 0cm}, clip]{chapter-3/figs/method.pdf}
    \caption{
        Steps of the method.
        (a) We pre-train a watermark encoder $\mathcal{W}_E$ and extractor $\mathcal{W}$, to extract binary messages.
        (b) We fine-tune the decoder $\mathcal{D}$ of the LDM's autoencoder with a fixed signature $m$ such that all the generated images (c) lead to $m$ through $\mathcal{W}$.
        }
    \label{chap3/fig:method}
\end{figure}

\section{Method}

Stable Signature modifies the generative network so that the generated images have a given signature through a fixed watermark extractor.
It is trained in two phases.
First, we create the watermark extractor network $\mathcal{W}$.
We then fine-tune the LDM decoder $\mathcal{D}$, such that all generated images yield a given signature through $\mathcal{W}$.

\subsection{Pre-training the watermark extractor}\label{chap3/subsec:pre-training}

We use \Gls*{HiDDeN}~\citep{zhu2018hidden}, a classical method in the deep watermarking literature.
It operates as presented in~\nameref{chap0/sec:deep learning-watermarking}.
We here briefly describe the method (and change the notations of the embedder and extractor to reduce possible confusions with the generative model).

We jointly optimize an encoder $\mathcal{W}_E$ and an extractor $\mathcal{W}$ to embed $\payload$-bit messages into images. 
$\mathcal{W}_E$ takes as inputs a cover image $\im_o$ and a $\payload$-bit message $m$.
$\mathcal{W}_E$ outputs a residual $\delta$ of the same size as $\im_o$, that is multiplied by a $\alpha$ to produce thewatermarked image $\im_w = \im_o + \alpha \delta$.
At each optimization step an image transformation $T$ is sampled from a set that includes common image processing operations. 
For the non-differentiable JPEG compression, we use the forward attack simulation layer as done by~\cite{zhang2021asl}. 
The soft message is extracted from the transformed image: $m' = \mathcal{W}(T(\im_w))$, and the \emph{message loss} is the Binary Cross Entropy (BCE) between $m$ and the sigmoid $\sigma (m')$.

The network architectures are kept simple to ease the LDM fine-tuning in the second phase.
They are the same as HiDDeN (see Sec.~\ref{chap3/app:archi-hidden}) with two changes. 
First, we discard $\mathcal{W}_E$ after training, since only $\mathcal{W}$ serves our purpose. 
Therefore, its perceptual quality is not as important, so the perceptual loss and the adversarial network are not needed. 
Instead, the distortion is constrained by a $\mathrm{tanh}$ function on output of $\mathcal{W}_E$ and by the scaling factor $\alpha$.
This improves the bit accuracy of the recovered message and makes it possible to increase its size $k$.
Second, we observed that $\mathcal{W}$'s output bits for vanilla images are correlated and highly biased, which violates the assumptions of Sec.~\ref{chap3/subsec:statistical-test}. 
Therefore, to get closer to i.i.d. dimensions we remove the bias and decorrelate the outputs of $\mathcal{W}$ by applying a PCA whitening transformation (more details in Sec.~\ref{chap3/app:hidden-centering}).


\subsection{Fine-tuning the generative model}\label{chap3/subsec:finetuning}

In \Gls*{LDM}, the diffusion happens in the latent space of an autoencoder.
The latent vector $z$ obtained at the end of the diffusion is input to decoder $\mathcal{D}$ to produce an image.
Here we fine-tune $\mathcal{D}$ such that the image contains a given message $m$ that can be extracted by $\mathcal{W}$.
Stable Signature is compatible with many generative tasks, since modifying only $\mathcal{D}$ does not affect the diffusion process.
We first fix the signature $m=(m_1,\ldots, m_\payload) \in \{0,1\}^k$. 
The fine-tuning of $\mathcal{D}$ into $\mathcal{D}_m$ is inspired by the original training of the autoencoder in LDM~\citep{rombach2022high}.

Training image $\im \in \mathbb{R}^{H\times W\times 3}$ is fed to the LDM encoder $\mathcal{E}$ 
that outputs activation map $z = \mathcal{E}(\im) \in \mathbb{R}^{h\times w\times c}$, downsampled by a power-of-two factor $f = H/h = W/w $.
The decoder reconstructs an image $\im' = \mathcal{D}_m(z)$ and the extractor recovers $m' = \mathcal{W} (\im')$.
The \emph{message loss} is the BCE between $m'$ and the original $m$: $\mathcal{L}_m = \mathrm{BCE}(\sigma \left( m' \right), m)$.

In addition, the original decoder $\mathcal{D}$ reconstructs the image without watermark: $\im'_o = \mathcal{D}(z)$. 
The \emph{image perceptual loss} $\mathcal{L}_\mathrm{i}$ between $\im'$ and $\im'_o$, controls the distortion.
We use the Watson-VGG perceptual loss introduced by~\cite{czolbe2020loss}, an improved version of LPIPS~\citep{zhang2018unreasonable}.
It is essential that the decoder learns luminance and contrast masking to add less visible watermarks. %
The weights of $\mathcal{D}_m$ are optimized to minimize:
\begin{equation}\label{chap3/eq:loss2}
    \mathcal{L} = \mathcal{L}_\mathrm{m} + \lambda_\mathrm{i}~ \mathcal{L}_\mathrm{i}.
\end{equation}
\noindent
This is done over $100$ iterations with the AdamW optimizer~\citep{loshchilov2017decoupled} and batch of size $4$, \ie, the fine-tuning sees \emph{fewer than 500 images} and takes \emph{one minute on a single GPU}.
The learning rate follows a cosine annealing schedule with $20$ iterations of linear warmup to $10^{-4}$ and decays to $10^{-6}$.
$\lambda_\mathrm{i}$ in Eq.~\eqref{chap3/eq:loss2} is set to $0.2$ by default.




\subsection{Implementation details and parameters}\label{chap3/app:method-details}


\paragraph*{Architectures of the watermark encoder/extractor.}\label{chap3/app:archi-hidden}
We keep the same architecture as in HiDDeN~\citep{zhu2018hidden}, which is a simple convolutional encoder and extractor.
The encoder consist of $4$ Conv-BN-ReLU blocks, with $64$ output filters, $3\times 3$ kernels, stride $1$ and padding $1$.
The extractor has $7$ blocks, followed by a block with $k$ output filters ($k$ being the number of bits to hide), an average pooling layer, and a $k\times k$ linear layer.
For more details, we refer the reader to the original paper.

\paragraph*{Watermark extractor training.}
We train on the MS-COCO dataset~\citep{lin2014microsoft}, with $256 \times 256$ images.
The number of bits is $k=48$, and the scaling factor is $\alpha=0.3$.
The optimization is carried out for $300$ epochs on $8$ GPUs, with the Lamb optimizer~\citep{you2019lamb} (it takes around a day). 
The learning rate follows a cosine annealing schedule with $5$ epochs of linear warmup to $10^{-2}$, and decays to $10^{-6}$.
The batch size per GPU is $64$.

\paragraph*{Attack simulation layer.}\label{chap3/app:hidden-attack}
The attack layer produces edited versions of the watermarked image to improve robustness.
It takes as input the image output by the watermark encoder $x_w = \mathcal{W}_E(x_o)$ and outputs a new image $x'$ that is fed to the decoder $\mathcal{W}$.
This layer is made of cropping, resizing, or identity chosen at random, unless otherwise stated.
The parameter for the crop or resize is set to $0.3$ or $0.7$ with equal probability.
This is followed by a JPEG compression with probability $0.5$.
The parameter for the compression is set to $50$ or $80$ with equal probability.
This last layer is not differentiable, therefore we back-propagate only through the difference between the uncompressed and compressed images:
$x'= x_{\mathrm{aug}} + \mathrm{nograd}(x_{\mathrm{aug}, \mathrm{JPEG}} - x_{\mathrm{aug}})$~\citep{zhang2021asl}, as explained in Sec.~\ref{chap0/sec:deep learning-watermarking}.

\paragraph*{Whitening.}\label{chap3/app:hidden-centering}
At the end of the training, we whiten the output of the extractor to make the hard thresholded bits i.i.d. Bernoulli on vanilla images -- so that the assumption of the statistical test~\ref{chap3/subsec:statistical-test} holds better, see Sec.~\ref{chap3/app:assumption}.
We perform the PCA of the output of the watermark extractor on a set of $10$k images from COCO, and get the mean $\mu$ and eigendecomposition of the covariance matrix $\Sigma= U\Lambda U^T$. 
The whitening is applied as a linear layer with bias $-\Lambda^{-1/2}U^T\mu$ and weight $\Lambda^{-1/2}U^T$, appended to the extractor. 
See Fig.~\ref{chap1/fig:whitening} for a visual representation of the whitening process.


















\section{Evaluated tasks and baselines}\label{chap3/app:implementation-details}

We hereby describe the generative tasks and watermarking methods used in the experiments of the following sections.
Feel free to skip and refer to it later if needed.


\subsection{Generative tasks}\label{chap3/app:generative-tasks}

Since our method only involves the LDM decoder, it is compatible with many generative tasks. 
We evaluate text-to-image generation and image edition on the validation set of MS-COCO~\citep{lin2014microsoft}, super-resolution and inpainting on the validation set of ImageNet~\citep{deng2009imagenet}.


\paragraph*{Text-to-image.}
In text-to-image generation, the diffusion process is guided by a text prompt. 
We follow the standard protocol in the literature~\citep{ramesh2022hierarchical, ramesh2021zero, rombach2022high, saharia2022photorealistic} and evaluate the generation on prompts from the validation set of MS-COCO~\citep{lin2014microsoft}.
To do so, we first retrieve all the captions from the validation set, keep only the first one for each image, and select the first $1000$ or $5000$ captions depending on the evaluation protocol.
We use guidance scale $3.0$ and $50$ diffusion steps.
If not specified, the generation is done for $5000$ images.
The FID is computed over the validation set of MS-COCO, resized to $512\times 512$.

\paragraph*{Image edition.}
DiffEdit~\citep{couairon2022diffedit} takes as input an image, a text describing the image and a novel description that the edited image should match. 
First, a mask is computed to identify which regions of the image should be edited. 
Then, mask-based generation is performed in the latent space, before converting the output back to RGB space with the image decoder. 
We use the default parameters used in the original paper, with an encoding ratio of 90\%, and compute a set of $5000$ images from the COCO dataset, edited with the same prompts as the paper~\citep{couairon2022diffedit}.
The FID is computed over the validation set of MS-COCO, resized to $512\times 512$.

\paragraph*{Inpainting.}
We follow the protocol of LaMa~\citep{suvorov2022resolution}, and generate $5000$ masks with the ``thick'' setting, at resolution $512\times 512$, each mask covering $1-50\%$ of the initial image (with an average of $27\%$).
For the diffusion-based inpainting, we use the inference-time algorithm presented in \citep{song2020score}, also used in Glide~\citep{nichol2021glide}, which corrects intermediate estimations of the final generated image with the ground truth pixel values outside the inpainting mask. 
For latent diffusion models, the same algorithm can be applied in latent space. 
In this case, we consider $2$ different variations: (1) inpainting is performed in the latent space and the final image is obtained by simply decoding the latent image; and (2) the same procedure is applied, but after decoding, ground truth pixel values from outside the inpainting mask are copy-pasted from the original image. 
The latter allows to keep the rest of the image perfectly identical to the original one, at the cost of introducing copy-paste artifacts, visible in the borders. 
Image quality is measured with an FID score, computed over the validation set of ImageNet~\citep{deng2009imagenet}, resized to $512\times 512$.

\paragraph*{Super-resolution.}
We follow the protocol suggested by Saharia~\etal~\citep{saharia2022image}.
We first resize $5000$ random images from the validation set of ImageNet to $128\times 128$ using bicubic interpolation, and upscale them to $512\times 512$.
The FID is computed over the validation set of ImageNet, cropped and resized to $512\times 512$.






\subsection{Watermarking methods}\label{chap3/app:watermarking}
For DCT-DWT, we use the implementation of \url{https://github.com/ShieldMnt/invisible-watermark} (the one used in Stable Diffusion).
For SSL Watermark~\citep{fernandez2022sslwatermarking} and FNNS~\citep{kishore2021fixed} the watermark is embedded by optimizing the image, such that the output of a pre-trained model is close to the given key -- like in adversarial examples~\citep{goodfellow2014adversarial}, see Chap.~\ref{chapter:ssl-watermarking} for all the details.
The difference between the two is that in SSL Watermark we use a model pre-trained with DINO~\citep{caron2021dino}, while FNNS uses a watermark or stenography model.
For FNNS we use the HiDDeN extractor used in all our experiments, and not SteganoGAN~\citep{zhang2019steganogan} as in the original paper, because we want to extract watermarks from images of different sizes.
We use the image optimization scheme of Chap.~\ref{chapter:active-indexing}, \ie, we optimize the distortion image for $10$ iterations, and modulate it with a perceptual just noticeable difference (JND) mask.
This avoids visible artifacts and gives a PSNR comparable with our method ($\approx 30$dB).
For HiDDeN, we use the watermark encoder and extractor from our pre-training phase, but the extractor is not whitened and we modulate the encoder output with the same JND mask.
Note that in all cases we watermark images one by one for simplicity. 
In practice the watermarking could be done by batch, which would be more efficient. 









\subsection{Evaluation metrics}\label{chap3/app:metrics}

We evaluate the image distortion with the Peak Signal-to-Noise Ratio (PSNR), which is defined as $\mathrm{PSNR}(x,x') = -10\cdot \log_{10} (\mathrm{MSE}(x,x'))$, for $x,x'\in [0,1]^{c\times h\times w}$, as well as Structural Similarity score (SSIM)~\citep{wang2004image}.
They compare images generated with and without watermark. 
On the other hand, we evaluate the diversity and quality of the generated images with the Fr\'echet Inception Distance (FID)~\citep{heusel2017gans}.

The \emph{bit accuracy} -- the percentage of bits correctly decoded -- evaluates the robustness of the watermark after transformations.
For detection, we use 
the \emph{True Positive Rate} (\Gls*{TPR}), \ie, the probability of flagging a generated image, 
the \emph{False Positive Rate} (\Gls*{FPR}), \ie, the probability of flagging a vanilla image,
and \emph{Receiver Operating Characteristic} (\Gls*{ROC}) curve, which plots TPR against FPR.




\begin{figure}[b!]
    \centering
    \footnotesize
    \begin{tabular}{*{5}{l}}
        Crop 0.1 & JPEG 50 & Resize 0.7 & Brightness 2.0 & Contrast 2.0 \\ 
        \begin{minipage}{.16\linewidth}\centering \includegraphics[width=0.3\linewidth]{chapter-3/figs/transformations/002_crop_01.jpg}\end{minipage} &  
        \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-3/figs/transformations/002_jpeg_50.jpg}\end{minipage} &  
        \begin{minipage}{.16\linewidth}\centering\includegraphics[width=0.8\linewidth]{chapter-3/figs/transformations/002_resize_07.jpg}\end{minipage} &  
        \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-3/figs/transformations/002_brightness_2.jpg}\end{minipage} &
        \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-3/figs/transformations/002_contrast_2.jpg}\end{minipage} 
        \\ \\
        Saturation 2.0 & Sharpness 2.0 & Rotation $90$ & Text overlay & Combined \\
        \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-3/figs/transformations/002_saturation_2.jpg}\end{minipage} &  
        \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-3/figs/transformations/002_sharpness_2.jpg}\end{minipage} &  
        \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-3/figs/transformations/002_rot_90.jpg}\end{minipage} &  
        \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-3/figs/transformations/002_overlay_text.jpg}\end{minipage} &  
        \begin{minipage}{.16\linewidth}\includegraphics[width=\linewidth]{chapter-3/figs/transformations/002_comb.jpg}\end{minipage} 
        \\ \\
    \end{tabular}
\caption{Illustration of all transformations evaluated in sections~\ref{chap3/sec:application} and \ref{chap3/sec:experiments}.}
\label{chap3/fig:all-transformations}
\end{figure}




\subsection{Image transformations}\label{chap3/app:transformations}
We evaluate the robustness of the watermark to a set of transformations in sections~\ref{chap3/sec:application} and \ref{chap3/sec:experiments}.
They simulate image processing steps that are commonly used in image editing software.
We illustrate them in \autoref{chap3/fig:all-transformations}.
For crop and resize, the parameter is the ratio of the new area to the original area.
For rotation, the parameter is the angle in degrees.
For JPEG compression, the parameter is the quality factor (in general 90\% or higher is considered high quality, 80\%-90\% is medium, and 70\%-80\% is low).
For brightness, contrast, saturation, and sharpness, the parameter is the default factor used in the PIL and Torchvision~\citep{marcel2010torchvision} libraries.
The text overlay is made through the AugLy library~\citep{papakipos2022augly}, and adds a text at a random position in the image.
The combined transformation is a combination of a crop $0.5$, a brightness change $1.5$, and a JPEG $80$ compression.


