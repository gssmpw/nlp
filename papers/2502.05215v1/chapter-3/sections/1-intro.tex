
\section{Introduction}


Recent progress in generative modeling and natural language processing enable easy creation and manipulation of photo-realistic images.
They have given birth to many image edition tools like ControlNet~\citep{zhang2023adding}, Instruct-Pix2Pix~\citep{brooks2022instructpix2pix}, and others~\citep{couairon2022diffedit, gal2022image, ruiz2022dreambooth}, that are becoming mainstream creative tools for artists, designers, and the general public.
While this is a great step forward for image generative AI, it also undermines confidence in the authenticity or veracity of photo-realistic images. 
Indeed, methods for photo-realistic image edition existed before, but generative AI significantly lowers the barriers to convincing synthetic image generation and edition (\eg, a generated picture recently won an art competition~\citep{gault2022vice}).
This raises new risks like deep fakes, impersonation or copyright usurpation~\citep{brundage2018malicious, denton2021ethical}.
A tool to determine that images are AI-generated would make it easier to ensure their compliance with ethical standards and to remove them from certain platforms. 
    
A baseline solution to identify generated images is forensics, \ie, methods to detect generated/manipulated images passively (the image is not modified for identification).
An active baseline is to apply existing watermarking methods after the image generation, like the ones presented in Sec.~\ref{chap0/sec:deep learning-watermarking} and in Chap.~\ref{chapter:ssl-watermarking},~\ref{chapter:active-indexing} and \ref{chapter:audioseal}.
This has several drawbacks. 
If the model leaks or is open-sourced, the post-generation watermarking can be removed trivially.
The open source Stable Diffusion~\citep{2022stablediffusion} is a case in point, since removing the watermark amounts to commenting out a single line in the source code.

\begin{figure}[t]
    \centering 
    \includegraphics[width=\linewidth, trim={0cm 0cm 1.5cm 0cm}, clip]{chapter-3/figs/fig1.pdf}
    \caption{
        Overview of Stable Signature. 
        The latent decoder of the latent diffusion model is fine-tuned to preemptively embed a signature into all generated images.
    }
    \label{chap3/fig:fig1}
\end{figure}

Our method, called Stable Signature, merges watermarking into the generation process itself, without any architectural change.
It adjusts the pre-trained generative model such that all the images it produces conceal a given watermark.
There are several advantages to this approach~\citep{lin2022cycleganwm, yu2022responsible}.
It does not require additional processing of the generated image, which makes the watermarking computationally lighter, straightforward, and secure.
Model providers could deploy their models to different user groups with a unique watermark, and monitor that they are used in a responsible manner.
They could give art platforms, news outlets and other sharing platforms %
the ability to detect when an image has been generated by their AI.

We focus on Latent Diffusion Models (LDM)~\citep{rombach2022high} that can perform a wide range of generative tasks.
We show that simply fine-tuning a small part of the generative model -- the decoder that generates images from the latent vectors -- is enough to natively embed a watermark into generated images.
Stable Signature does not require an architectural change and does not modify the diffusion process. 
Hence it is to our knowledge compatible with all LDM-based generative methods~\citep{brooks2022instructpix2pix, couairon2022diffedit, peebles2022dit, ruiz2022dreambooth, zhang2023adding}.
The fine-tuning stage is performed by back-propagating a combination of a perceptual image loss and a message decoding loss from a watermark extractor back to the LDM decoder.
We pre-train the extractor with a simplified version of the deep watermarking method HiDDeN~\citep{zhu2018hidden}, as presented in Chap.~\ref{chapter:technical-background}, Sec.~\ref{chap0/sec:deep learning-watermarking}.

We create an evaluation benchmark close to real world situations where images may be edited.
The tasks are: detection of AI generated images, tracing models from their generations.
For instance, we detect $90\%$ of images generated with the generative model, even if they are cropped to $10\%$ of their original size, while flagging only one false positive every $10^6$ images. 
To ensure that the model's utility is not weakened, we show that the FID~\citep{heusel2017gans} score of the generation is not affected and that the generated images are perceptually indistinguishable from the ones produced by the original model. 
This is done over several tasks involving LDM (text-to-image, inpainting, edition, etc.).


As a summary, 
(1) we efficiently merge watermarking into the generation process of LDMs, in a way that is compatible with most of the LDM-based generative methods;
(2) we demonstrate how it can be used to detect and trace generated images, through a real-world evaluation benchmark;
(3) we compare to post-hoc watermarking methods, showing that it is competitive while being more secure and efficient, and (4) evaluate robustness to intentional attacks.
