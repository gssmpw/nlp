
\section{Related work}

We here provide a brief overview of the literature on watermarking, generative models, and detection of AI-generated/manipulated images that is relevant to this chapter. 
Please refer to Chap.~\ref{chapter:related-work} for a more comprehensive review.

\paragraph{Latent generative models.} 
They are a class of generative models that first generate a latent representation of the image to create, and then decode it to the image space.
Most of state-of-the-art models are based on this principle, since it allows for the generation to happen in a lower-dimensional, sometimes quantized, space, instead of the pixel space:
\begin{itemize}
    \item Diffusion-based models -- of which Stable Diffusion~\citep{rombach2022high} is the flagship -- generate images by diffusing a noise vector in the latent space of a perceptual autoencoder trained as VQGAN~\citep{esser2021taming}.
    These models can also perform conditional image generation like inpainting or text-guided image editing by fine-tuning the diffusion model with additional conditioning, \eg, masked input image, segmentation map, etc.~\citep{lugmayr2022repaint, saharia2022palette}.
    Because of their iterative denoising algorithm, diffusion models can also be adapted for image editing in a zero-shot fashion by guiding the generative process~\citep{couairon2022diffedit, hertz2022prompt, kawar2022imagic, mokady2022null, valevski2022unitune, wu2022unifying}.
    \item Sequence-based models treat the generation process as a sequence-to-sequence problem~\citep{ramesh2021zero, ding2021cogview, esser2021taming, gafni2022make, singer2022makeavideo, yu2022scaling, team2024chameleon}. 
    They are based on image tokenizers (\eg, VQGAN~\citep{esser2021taming}, ViT-VQGAN~\citep{yu2021vector}), which are models that convert images into discrete tokens, and vice versa.
    A transformer model is used to generate the tokens given some conditioning (from text, image, etc.), which are decoded back into an image.
    This methodology aligns with the advancements in language models and leverages the scaling capabilities known from large language models.
\end{itemize}

We present these methods together because they operate in the latent space, requiring a latent decoder to produce an RGB image. 
It presents two interesting properties.
First, it makes the watermarking method presented in the chapter directly applicable to these models.
Second, the watermarking method is robust to changes in the generative model, as long as the latent decoder is kept unchanged. 
This is particularly helpful since, most of the time, only the latent generative model is fine-tuned to give new capabilities~\citep{zhang2023adding, brooks2022instructpix2pix}.
 
\paragraph{Detection of AI-generated/manipulated images.}
Purely relying on forensics and passive detection is limiting, \eg, the best performing method to our knowledge~\citep{corvi2022detection} is able to detect $50\%$ of generated images for a false positive rate around $1$/$100$:
if a user-generated content platform were to receive $1$ billion images every day, it would wrongly flag $10$ million images to detect only half of the generated images.
Besides, passive techniques cannot trace images from different versions of the same model, in contrast with watermarking.

\paragraph{Watermarking image generative models.}
The first works aim to watermark the training set on which the generative model is learned~\citep{yu2021artificial}.
It is highly inefficient since every new message to embed requires a new training pipeline.
Merging the watermarking and the generative process is a recent idea~\citep{fei2022supervised, lin2022cycleganwm, nie2023attributing, qiao2023novel, wu2020watermarking, yu2022responsible, zhang2020model}, that is closer to the model watermarking literature (see Chap.~\ref{chapter:invariants} for more details).
They suffer from two strong limitations.
First, these methods only apply to GANs, while LDM are progressively replacing them for most applications. 
Second, watermarking is incorporated in the training process of the GAN from the start. 
This strategy is unsustainable because the generative model training is more and more costly\footnote{Stable Diffusion training costs $\sim$\$600k of cloud compute (\href{https://en.wikipedia.org/wiki/Stable_Diffusion}{Wikipedia}).}.
Our work shows that a quick fine-tuning of the latent decoder part of the generative model is enough to achieve a good watermarking performance, provided that the watermark extractor is well chosen.

