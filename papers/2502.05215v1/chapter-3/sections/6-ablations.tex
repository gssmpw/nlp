\section{Attacks on Stable Signature's watermarks}\label{chap3/sec:attacks}

We examine the watermark's resistance to intentional tampering, 
as opposed to distortions that happen without bad intentions like crops or compression (discussed in Sec.~\ref{chap3/sec:application}). 
We consider two threat models: one  is typical for many image watermarking methods~\citep{cox2007digital} and operates at the image level, and another targets the generative model level. 
For image-level attacks, we evaluate on $5$k images generated from COCO prompts.





\subsection{Image-level attacks}\label{chap3/subsec:image-level-attacks}

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.6\linewidth, trim={0 0 0 0}, clip]{chapter-3/figs/attacks/purification.pdf}
    \caption{Removal attacks.
    $x_o$ is the image produced by the original generator, 
    $x_r$ is the version produced by the watermarked generator and then attacked.
    Bit accuracy is on the watermark extracted from $x_r$.
    Neural autoencoders~\citep{balle2018variational, cheng2020learned, esser2021taming} follow the \colorbox[HTML]{ebf2f8}{same trend}, except for the one used by LDM (`KL-f8' for our LDM).
    When access to the watermark extractor is granted, adversarial attacks also remove the watermark at lower PSNR budget.
    }
    \label{chap3/fig:purification}
\end{figure}

\paragraph{Watermark removal.}
Bob alters the image to remove the watermark with deep learning techniques, like methods used for adversarial purification~\citep{shi2021online, yoon2021adversarial} or neural autoencoders~\citep{abdelnabi2021adversarial, liu2020defending}.
This kind of attacks had not yet been explored at the time of writing this chapter.

The perceptual autoencoders aim to create compressed latent representations of images.
We select $2$ state-of-the-art autoencoders from the CompressAI library zoo~\citep{begaint2020compressai}: the factorized prior model~\citep{balle2018variational} and the anchor model variant~\citep{cheng2020learned}.
We also select the autoencoders from~\citet{esser2021taming} and~\citet{rombach2022high}.
For all models, we use different compression factors to observe the trade-off between quality degradation and removal robustness.
For \texttt{bmshj2018}: $1$, $4$ and $8$, for \texttt{cheng2020}: $1$, $3$ and $6$, for \texttt{esser2021}: VQ-$4$, $8$ and $16$, for \texttt{rombach2022} KL-$4$, $8$, $16$ and $32$ (KL-$8$ being the one used by SD v1.4).
We generate $1$k images from text prompts with our LDM watermarked with a $48$-bits key.
We then try to remove the watermark using the autoencoders, and compute the bit accuracy on the extracted watermark.
The PSNR is computed between the original image and the reconstructed one, which explains why the PSNR does not exceed $30$dB (since the watermarked image already has a PNSR of $30$dB).
If we compared between the watermarked image and the image reconstructed by the autoencoder instead, the curves would show the same trend but the PSNR would be $2$-$3$ points higher.

We evaluate the robustness of the watermark against neural autoencoders~\citep{balle2018variational, cheng2020learned, esser2021taming, rombach2022high} at different compression rates, and show the outcome in Fig.~\ref{chap3/fig:purification}.
To reduce the bit accuracy closer to random (50\%), the image distortion needs to be strong (PSNR$<$26).
However, assuming the attack is \emph{informed on the generative model}, \ie, the autoencoder is the same as the one used to generate the images, the attack becomes much more  effective.
It erases the watermark while achieving high quality (PSNR$>$29).
This is because the image is modified precisely in the bandwidth where the watermark is embedded.
Note that this assumption is strong, because Alice does not need to distribute the original generator. 



\paragraph{Watermark removal \& embedding (white-box).}
To go further, we assume that the attack is \emph{informed on the watermark extractor} -- \eg, because it has leaked.
Bob can use an adversarial attack to remove the watermark by optimizing the image under a PSNR constraint.
The adversarial attack is performed by optimizing the image in the same manner as in chapters~\ref{chapter:ssl-watermarking} and~\ref{chapter:active-indexing}.
The objective is a MSE loss between the output of the extractor and a random binary message fixed beforehand. 
The attack is performed for $10$ iterations with the Adam optimizer~\citep{kingma2014adam} with learning rate $0.1$.
It makes it possible to erase the watermark with a lower distortion budget, as seen in Fig.~\ref{chap3/fig:purification}.
Instead of removing the watermark, an attacker could embed a signature into vanilla images (unauthorized embedding~\citep{cox2007digital}) to impersonate another Bob of whom they have a generated image. 
It highlights the importance of keeping the watermark extractor private.





\subsection{Network-level attacks}\label{chap3/subsec:network-level-attacks}

\paragraph{Quantization and pruning.}
We first study the impact of quantizing or pruning the watermarked generative model -- namely the LDM decoder -- on the watermark extraction on generated images.
These methods were developed for memory or efficiency requirements, but they may also be used to attack the watermark\footnote{Note that quantization and pruning could happen unintentionally, and be seen as a specific case of model-level robustness.}.
Quantization is performed naively, by rounding the weights to the closest quantized value in the min-max range of every weight matrix.
Pruning is done using PyTorch~\citep{paszke2019pytorch} pruning API, with the L1 norm as criterion.
Here is the bit accuracy on the generated images after network attacks, observed over 10$\times$1k images generated from text prompts: Quantization (8-bits) 0.99, (4-bits) 0.99; Pruning L1 (30\%) 0.99, (60\%) 0.95.
We observe that the network generation quality degrades faster than watermark robustness. 
To reduce bit accuracy lower than 98\%, quantization degrades the PSNR $<$25dB, and pruning $<$20dB.


\begin{figure}[b!]
    \centering
    \includegraphics[width=0.6\linewidth, trim={0 0 0 0}, clip]{chapter-3/figs/attacks/purification_model_flat.pdf}
    \caption{
        Robustness to model purification, \ie, fine-tuning the model to remove watermarks. 
         $x_w$ is the watermarked image, $x_{r}$ is generated with the purified model at different steps of the process.
    }\label{chap3/fig:purification-model}
\end{figure}

\paragraph{Model purification.} 
Bob gets Alice's generative model and uses a fine-tuning process akin to Sec.~\ref{chap3/subsec:finetuning} to eliminate the watermark embedding -- that we coin \emph{model purification}. 
This involves removing the message loss $\mathcal L_\mathrm{m}$, and shifting the focus to the perceptual loss $\mathcal L_\mathrm{i}$ between the original image and the one reconstructed by the LDM autoencoder.

We use the same fine-tuning procedure as in Sec.~\ref{chap3/subsec:finetuning}.
This is done for different numbers of steps, namely $100$, $200$, and every multiple of $200$ up to $1600$.
The bit accuracy and the reported PSNR are computed on $1$k images of the validation set of COCO, for the auto-encoding task.

\autoref{chap3/fig:purification-model} shows the results of this attack for the MSE loss.
The PSNR between the watermarked and purified images is plotted at various stages of fine-tuning.
Empirically, it is difficult to significantly reduce the bit accuracy without compromising the image quality: artifacts start to appear during the purification\footnote{
    \cite{hu2024stable} show, a posteriori, that through a more sophisticated fine-tuning process, the watermark can be removed without compromising much the image quality.
}.


\paragraph{Model collusion.}
Users may collude by aggregating their models.
For instance, Bob$^{(i)}$ and Bob$^{(j)}$ can average the weights of their models (like Model soups~\citep{wortsman2022model}) creating a new model to deceive identification.
We found that the bit at position $\ell$ output by the extractor will be $0$ (resp. $1$) when the $\ell$-th bits of Bob$^{(i)}$ and Bob$^{(j)}$ are both $0$ (resp. $1$), and that the extracted bit is random when their bits disagree.

The goal is to observe the decoded watermarks on the generation when $2$ models are averaged together.
We fine-tune the LDM decoder for $10$ different $48$-bits keys (representing $10$ Bobs).
We then randomly sample a pair of Bobs and average the $2$ models, with which we generate $100$ images.
We then extract the watermark from the generated images and compare them to the $2$ original keys.
We repeat this experiment $10$ times, meaning that we observe $10\times 100 \times 48=48000$ decoded bits.

We show the distributions of the soft bits (before thresholding) output by the watermark extractor on images generated by the average model. 
The $\ell$-th output is labeled by bits of Bob$^{(i)}$ and Bob$^{(j)}$ (\texttt{00} means both have \texttt{0} at position $\ell$):
\begin{center}
    \includegraphics[width=0.5\linewidth, trim={0 0.7cm 0 0cm}, clip]{chapter-3/figs/attacks/model_collusion_flat.pdf}   
\end{center}
The rightmost skewed normal is fitted with the Scipy library and the corresponding parameters are $a:6.96, e:0.06, w:0.38$. 
This is done over all bits where Bobs both have a $1$.
The same observation holds when there is no collusion, with approximately the same parameters.
When the bit is not the same between Bobs, we denote by $m_1^{(i)}$ the random variable representing the output of the extractor in the case where the generative model only comes from Bob$^{(i)}$,
and by $m_2$ the random variable representing the output of the extractor in the case where the generative model comes from the average of the two Bobs.
Then in our model $m_2 = 0.5 \cdot ( m_1^{(i)} + m_1^{(j)})$, and the pdf of $m_2$ is the convolution of the pdf of $m_1^{(i)}$ and the pdf of $m_1^{(j)}$, rescaled in the x axis because of the factor $0.5$.

This \emph{marking assumption} plays a crucial role in traitor tracing literature~\citep{furon:hal-00757152,meerwald:hal-00740964, tardos2008optimal}.
Surprisingly, it holds even though our watermarking process is not explicitly designed for it.
The study has room for improvement, such as creating user identifiers with more powerful traitor tracing codes~\citep{tardos2008optimal} and using more powerful traitor accusation algorithms~\citep{furon:hal-00757152,meerwald:hal-00740964}.
Importantly, we found the precedent remarks also hold if the colluders operate at the image level.






\section{Ablation studies}\label{chap3/sec:ablations}


\subsection{Quality-robustness tradeoff}\label{chap3/subsec:quality-tradeoff}


We can choose to maximize the image quality or the robustness of the watermark thanks to the weight $\lambda_\mathrm{i}$ of the perceptual loss in Eq.~\eqref{chap3/eq:loss2}.
We report the average PSNR of $1$k generated images, as well as the bit accuracy obtained on the extracted message for the `Combined' editing applied before detection (qualitative results are in Sec.~\ref{chap3/sec:supp-percep-loss}).
A higher $\lambda_\mathrm{i}$ leads to an image closer to the original one, but to lower bit accuracies on the extracted message, see \autoref{chap3/tab:tradeoff}.



\begin{table}[t!]
    \centering
    \caption{Quality-robustness trade-off during fine-tuning.}\label{chap3/tab:tradeoff}
    \footnotesize
    \begin{tabular}{l *{6}{c@{\hspace*{8pt}}}}
        \toprule
        $\lambda_i$ for fine-tuning     & $0.8$ & $0.4$ & $0.2$ & $0.1$ & $0.05$ \\ \midrule
        \rule{0pt}{2ex}
        PSNR $\uparrow$ & $31.4$ & $30.6$ & $29.7$ & $28.5$ & $26.8$\\ 
        \rule{0pt}{2ex}
        Bit acc. $\uparrow$ on `comb.' & $0.85$ & $0.88$ & $0.90$ & $0.92$ & $0.94$ \\ 
        \bottomrule 
    \end{tabular}
\end{table}



\subsection{Perceptual loss}\label{chap3/sec:supp-percep-loss}


\begin{figure}[b!]
    \centering
    \scriptsize
    \newcommand{\imwidth}{0.165\textwidth}
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{cc@{\hskip 2pt}cc@{\hskip 2pt}cc}
        \toprule
        $\lambda_i = 0.025$ &  & $\lambda_i = 0.05$ &  & $\lambda_i = 0.1$ &  \\
        \midrule
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/lambda-i/1_w.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/lambda-i/1_diff.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/lambda-i/2_w.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/lambda-i/2_diff.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/lambda-i/3_w.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/lambda-i/3_diff.jpg} \\
    \end{tabular}
    \begin{tabular}{c@{\hskip 2pt}ccccc}
        \toprule
        Original & Watson-VGG & Watson-DFT & LPIPS & MSE & LPIPS + $0.1\cdot$MSE \\
        \midrule
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/loss-i/0_nw.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/loss-i/0_w.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/loss-i/1_w.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/loss-i/2_w.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/loss-i/3_w.jpg} &
        \includegraphics[width=\imwidth]{chapter-3/figs/supp/loss-i/4_w.jpg} \\
        \bottomrule \\
    \end{tabular}
    \caption{
        Qualitative influence of the perceptual loss during LDM fine-tuning. 
        (Top): we show images generated with the LDM autoencoder fine-tuned with different $\lambda_i$, and the pixel-wise difference ($\times 10$) with regards to the image obtained with the original model.
        PSNR are $24$dB, $26$dB, $28$dB from left to right. 
        (Bottom): we change the perceptual loss and fix $\lambda_i$ to have approximately the same bit accuracy of $0.95$ on the ``combined'' augmentation. 
    }
    \label{chap3/fig:supp-lossi}
\end{figure}

The perceptual loss of Eq.~\eqref{chap3/eq:loss2} affects the image quality.
\autoref{chap3/fig:supp-lossi} shows how the parameter $\lambda_i$ affects the image quality.
For high values, the image quality is very good. %
For low values, artifacts mainly appear in textured area of the image. 
It is interesting to note that this begins to be problematic only for low PSNR values (around 25 dB).

\autoref{chap3/fig:supp-lossi} shows an example of a watermarked image for different perceptual losses: Watson-VGG~\citep{czolbe2020loss}, Watson-DFT~\citep{czolbe2020loss}, LPIPS~\citep{zhang2018unreasonable}, MSE, and LPIPS+MSE.
We set the weight $\lambda_i$ of the perceptual loss so that the watermark performance is approximately the same for all types of loss, and such that the degradation of the image quality is strong enough to be seen.
Overall, we observe that the Watson-VGG loss gave the most eye-pleasing results, closely followed by the LPIPS.









\subsection{Are the decoded bits i.i.d. Bernoulli random variables?}\label{chap3/app:assumption}

\begin{figure}[b]
    \begin{minipage}{0.67\textwidth}
        \centering
        \scriptsize
        \newcommand{\imheight}{0.33\textwidth}
        \setlength{\tabcolsep}{0pt}
        \begin{tabular}{lll}
            Before whitening: & After whitening: & Bernoulli simulation: \\
            \includegraphics[height=\imheight, trim=2cm 0.5cm 3.7cm 1cm, clip]{chapter-3/figs/supp/assump/cov_nowhit.pdf} &
            \includegraphics[height=\imheight, trim=2cm 0.5cm 3.7cm 1cm, clip]{chapter-3/figs/supp/assump/cov_whit.pdf} &
            \includegraphics[height=\imheight, trim=2cm 0.5cm 1.5cm 1cm, clip]{chapter-3/figs/supp/assump/cov_bern.pdf} \\
        \end{tabular}
        \caption{Covariance matrices of the bits output by the watermark decoder $\mathcal{W}$ before and after whitening.}
        \label{chap3/fig:supp-assumption}
    \end{minipage}\hfill
    \begin{minipage}{0.27\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth, trim=0 0 0 0, clip]{chapter-3/figs/supp/fprcheck.pdf}
        \caption{Empirical check of the FPR.}
        \label{chap3/fig:supp-fpr-check}
    \end{minipage}
\end{figure}



The FPR and the $p$-value \eqref{chap3/eq:p-value} are computed with the assumption that, for vanilla images (not watermarked),
the bits output by the watermark decoder $\mathcal{W}$ are independent and identically distributed (i.i.d.) Bernoulli random variables with parameter $0.5$.
This assumption is not true in practice, even when we tried using regularizing losses in the training at phase one~\citep{bardes2022vicreg, sablayrolles2018catalyser}.
This is why we whiten the output at the end of the pre-training.

\autoref{chap3/fig:supp-assumption} shows the covariance matrix of the hard bits output by $\mathcal{W}$ before and after whitening. 
They are computed over $5$k vanilla images, generated with our LDM at resolution $512\times512$ (as a reminder the whitening is performed on $1$k vanilla images from COCO at $256\times256$).
We compare them to the covariance matrix of a Bernoulli simulation, where we simulate $5k$ random messages of $48$ Bernoulli variables.
We observe the strong influence of the whitening on the covariance matrix, although it still differs a little from the Bernoulli simulation.
We also compute the bit-wise mean and observe that for un-whitened output bits, some bits are very biased.  
For instance, before whitening, one bit had an average value of $0.95$ (meaning that it almost always outputs $1$).
After whitening, the maximum average value of a bit is $0.68$.
For the sake of comparison, the maximum average value of a bit in the Bernoulli simulation was $0.52$.
It seems to indicate that the distribution of the generated images are different than the one of vanilla images, and that it impacts the output bits.
Therefore, the bits are not perfectly i.i.d. Bernoulli random variables. 
We however found they are close enough for the theoretical FPR computation to match the empirical one (see next section) -- which was what we wanted to achieve.





\subsection{Empirical check of the FPR}\label{chap3/app:fpr-check}
In \autoref{chap3/fig:tpr-fpr}, we plotted the TPR against a theoretical value for the FPR, with the i.i.d. Bernoulli assumption.
The FPR was computed theoretically with Eq.~\eqref{chap3/eq:p-value}.
Here, we empirically check on smaller values of the FPR (up to $10^{-7}$) that the empirical FPR matches the theoretical one (higher values would be too computationally costly).
To do so, we use the $1.4$ million vanilla images from the training set of ImageNet resized and cropped to $512\times512$, and perform the watermark extraction with $\mathcal{W}$.
We then fix $10$ random $48$-bits key $m^{(1)},\cdots, m^{(10)}$, and, for each image, we compute the number of matching bits $d(m', m^{(i)})$ between the extracted message $m'$ and the key $m^{(i)}$, and flag the image if $d(m', m^{(i)})\geq \tau$.

\autoref{chap3/fig:supp-fpr-check} plots the FPR averaged over the $10$ keys, as a function of the threshold $\tau$.
We compare it to the theoretical one obtained with Eq.~\eqref{chap3/eq:p-value}.
As it can be seen, they match almost perfectly for high FPR values. 
For lower ones ($<10^{-6}$), the theoretical FPR is slightly higher than the empirical one.
This is a good thing since it means that if we fixed the FPR at a certain value, we would observe a lower one in practice.












\subsection{Scaling factor at pre-training} 


\begin{table}[t!]
    \centering
    \caption{
        Influence of the (discarded) watermark embedder perceptual quality. 
        We show PSNR and robustness results when using either the watermark embedder trained in the first phase, or the LDM fine-tuned in the second phase.
        The watermark embedder does not need to be very good for the second phase the watermark to be robust at the end of phase 2.
    }\label{chap3/tab:encoder-quality}
    \footnotesize
    \begin{tabular}{l *{5}{l}}
        \toprule
        Scaling factor $\alpha$ & $0.8$ & $0.4$ & $0.2$ & $0.1$ & $0.05$ \\ 
        \midrule
        Watermark embedder - PSNR $\uparrow$  & $16.1$ & $21.8$ & $27.2$ & $33.5$ & $39.3$ \\
        Fine-tuned LDM - PSNR $\uparrow$  & $27.9$ & $30.5$ & \textbf{30.8} & $28.8$ & $27.8$ \\
        \midrule
        Watermark embedder - Bit acc. $\uparrow$ on `none'& $1.00$ & $1.00$ & $0.86$ & $0.72$ & $0.62$ \\
        Fine-tuned LDM - Bit acc. $\uparrow$ on `none'& $0.98$ & \textbf{0.98} & $0.91$ & $0.90$ & $0.96$ \\
        Fine-tuned LDM - Bit acc.  $\uparrow$ on `comb.'& \textbf{0.86} & $0.73$ & $0.82$ & $0.81$ & $0.69$ \\
        \bottomrule 
    \end{tabular}
\end{table}


The watermark encoder does not need to be perceptually good and it is beneficial to degrade image quality during pre-training.
In the following, ablations are conducted on a shorter schedule of $50$ epochs, on $128\times 128$ images and $16$-bits messages.
In \autoref{chap3/tab:encoder-quality}, we train watermark encoders/extractors for different scaling factor $\alpha$ (see Sec.~\ref{chap3/subsec:pre-training}), and observe that $\alpha$ strongly affects the bit accuracy of the method.
When it is too high, the LDM needs to generate low quality images for the same performance because the distortions seen at pre-training by the extractor are too strong.
When it is too low, they are not strong enough for the watermarks to be robust: the LDM will learn how to generate watermarked images, but the extractor will not be able to extract them on edited images.








\subsection{Attack simulation layer}\label{chap3/subsec:message-decoder}


\begin{table}[t!]
    \centering
    \caption{
        Role of the attack simulation layer at pre-training.
        Certain augmentations are needed when pre-training the watermark embedder and extractor, while others are handled naturally.
    }
    \label{chap3/tab:asl}
    \footnotesize
    \begin{tabular}{c *{5}{c}}
        \toprule
        \multirow{2}{*}{ \shortstack{ Seen at  \\ $\mathcal{W}$ training \vspace*{-4pt}} } & \multicolumn{5}{c}{Bit accuracy $\uparrow$ at test time:} \\ \cmidrule{2-6}
            & Crop $0.1$ & Rot. $90$ &JPEG $50$ & Bright. $2.0$ & Res. $0.7$  \\
        \midrule
        \xmark     & 1.00 & 0.56 & 0.50 & 0.99 & 0.48 \\
        \cmark     & 1.00 & 0.99 & 0.90 & 0.99 & 0.91 \\
        \bottomrule
    \end{tabular} 
\end{table}


Watermark robustness against image transformations depends solely on the watermark extractor.
here, we pre-train them with or without specific transformations in the simulation layer, on a shorter schedule of $50$ epochs, with $128\times 128$ images and $16$-bits messages.
From there, we plug them in the LDM fine-tuning stage and we generate $1$k images from text prompts.
We report the bit accuracy of the extracted watermarks in \autoref{chap3/tab:asl}.
The extractor is naturally robust to some transformations, such as crops or brightness, without being trained with them, while others, like rotations or JPEG, require simulation during training for the watermark to be recovered at test time.
Empirically we observed that adding a transformation improves results for the latter, but makes training more challenging.
















































