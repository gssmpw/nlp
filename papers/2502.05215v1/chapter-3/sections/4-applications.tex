

\section{Text-to-image watermarking performance }\label{chap3/sec:application}
This section discusses our method's detection and identification capability for images generated by a large latent diffusion model~\citep{rombach2022high}\footnote{
    We refrain from experimenting with pre-existing third-party generative models, such as Stable Diffusion or LDMs, and instead use a large diffusion model (2.2B parameters) trained on an internal dataset of 330M licensed image-text pairs.
    Although the diffusion-based generative model has been trained on an internal dataset of licensed images, we use the KL autoencoder from LDM~\citep{rombach2022high} with compression factor $f=8$.
    This is the one used by open-source alternatives.
}.
We apply generative models watermarked with $48$-bit signatures on prompts of the MS-COCO~\citep{lin2014microsoft} validation set.
We evaluate detection and identification on the outputs, as illustrated in~\autoref{chap3/fig:fig1}.
The detection rates are partly obtained from experiments and partly by extrapolating small-scale measurements.

For simplicity, we restrict the robustness evaluation to the following transformations:
strong cropping ($10\%$ of the image remaining), 
brightness shift (strength factor $2.0$), 
as well as a combination of crop $50\%$, brightness shift $1.5$ and JPEG $80$. 
This covers typical geometric and photometric edits (see Fig.~\ref{chap3/fig:all-transformations} for visual examples).




\begin{figure}[b!]
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth, trim={0cm 0cm 0cm 0cm}, clip]{chapter-3/figs/tpr_fpr.pdf}
        \caption{
            Detection results. \Gls*{ROC} curve of the detection under different transformations.
            Forensics$^\dagger$ indicates passive detection~\citep{corvi2022detection}.
        }\label{chap3/fig:tpr-fpr}
    \end{minipage}\hfill
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth, trim={0cm 0cm 0cm 0cm}, clip]{chapter-3/figs/identification.pdf}
        \caption{
            Identification results. 
            Proportion of well-identified Bobs.
            Detection with FPR=$10^{-6}$ is run beforehand, and we consider it an error if the image is not flagged.
        }\label{chap3/fig:identification}
    \end{minipage}
\end{figure}



\subsection{Detection results}
For detection, we fine-tune the decoder of the LDM with a random key $m$, generate $1000$ images and use the test of Eq.~\eqref{chap3/eq:detectiontest}.
We report the tradeoff between TPR and the FPR, while varying $\tau\in \{0, .. ,48\}$.
For instance, for $\tau=0$, we flag all images so $\textrm{FPR}=1$, and $\textrm{TPR}=1$.
The TPR is measured directly, while the FPR is inferred from Eq.~\eqref{chap3/eq:p-value}, because it would otherwise be too small to be measured on reasonably sized problems 
(this approximation is validated experimentally in Sec.~\ref{chap3/app:fpr-check}).
The experiment is run on $10$ random signatures and we report averaged results.

\autoref{chap3/fig:tpr-fpr} shows the tradeoff under image transformations.
For example, when the generated images are not modified, Stable Signature detects $99\%$ of them, while only $1$ vanilla image out of $10^9$ is flagged.
At the same $\textrm{FPR}=10^{-9}$, Stable Signature detects $84\%$ of generated images for a crop that keeps $10\%$ of the image, 
and $65\%$ for a transformation that combines a crop, a color shift, and a JPEG compression.
For comparison, we report results of a state-of-the-art passive method~\citep{corvi2022detection}, applied on resized and compressed images.
As expected, we observe that these baseline results have orders of magnitudes larger FPR than Stable Signature, which actively marks the content. 




\subsection{Identification results}
Each Bob has its own copy of the generative model. 
Given an image, the goal is to find if any of the $N$ Bobs created it (detection) and if so, which one (identification).
There are $3$ types of error: 
\emph{false positive}: flag a vanilla image; 
\emph{false negative}: miss a generated image; 
\emph{false accusation}: flag a generated image but identify the wrong Bob.

For evaluation, we fine-tune $N'=1000$ models with random signatures.
Each model generates $100$ images.
For each of these $100$k watermarked images, we extract the Stable Signature message, compute the matching score with all $N$ signatures and select the Bob with the highest score. 
The image is predicted to be generated by that Bob if this score is above threshold $\tau$.
We determined $\tau$ such that $\textrm{FPR}=10^{-6}$, see Eq.~\eqref{chap3/eq:globalFPR}. 
For example, for $N=1$, $\tau=41$ and for $N=1000$, $\tau=44$.
Accuracy is extrapolated beyond the $N'$ Bobs by adding additional signatures and having $N > N'$ (\eg, Bobs that have not generated any images).

\autoref{chap3/fig:identification} reports the per-transformation identification accuracy.
For example, we identify a Bob among $N$=$10^{5}$ with $98\%$ accuracy when the image is not modified.
Note that for the combined edit, this becomes $40\%$.
This may still be dissuasive:
if a Bob generates $3$ images, he will be identified $80\%$ of the time.
We observe that at this scale, the false accusation rate is zero, \ie, we never identify the wrong Bob.
This is because $\tau$ is set high to avoid FPs, which also makes false accusations unlikely.
We observe that the identification accuracy decreases when $N$ increases, because the threshold $\tau$ required to avoid false positives is higher when $N$ increases, as pointed out by the approximation in Eq.~\eqref{chap3/eq:globalFPR}.
In a nutshell, by distributing more models, Alice trades some accuracy of detection against the ability to identify Bobs.
