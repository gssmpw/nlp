
\chapter{Functional Invariants to Watermark Large Transformers}\label{chapter:invariants}

This chapter is based on the paper \fullcite{fernandez2024functional}.

The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. 
In particular, models are vulnerable to leaks and unauthorized redistribution, which can result in significant financial losses and reputational damage for companies. 
Model watermarking addresses this issue by embedding a unique identifier into the model weights, while preserving its performance. 
It may then be extracted to identify the model's owner.
However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost.
This chapter explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). 
They generate functionally equivalent copies by leveraging the invariance of models, via operations like dimension permutations or scaling/unscaling. 
This enables to watermark models without any change in their outputs and remains stealthy.
Experiments demonstrate the effectiveness of the approach  and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to protect the integrity of large models.

\newpage
\input{chapter-7/sections/1-introduction}
\input{chapter-7/sections/2-related}
\input{chapter-7/sections/3-method}
\input{chapter-7/sections/4-experiments}
\input{chapter-7/sections/8-conclusion}
