

\section{Watermarking through Invariance}\label{chap7/sec:method}


\subsection{Invariants in the weights of transformers}

\paragraph*{Definition.}
We define an invariant as a series of operation applied on the model's weights $\theta \rightarrow \theta ' $ such that for any input $x$, the output $f_{\theta'}(x)$ is the same as before the application of the invariant.


\paragraph*{Permutation invariance} appears in (at least) four levels of the transformer.
We note $\Pi^d$ the set of permutations of $\{1,..., d\}$. 
For a matrix $M\in\R^{d\times d}$ and $\pi \in \Pi^d$, we denote by $M_{:,\pi}$ (resp. $M_{\pi,:}$) the matrix where columns (resp. rows) are permuted according to $\pi$.

\noindent
\emph{Embedding dimension.} 
The embedding matrix $E$ can be permuted along its second dimension without changing the output of the model, as long as the permutation is propagated to other matrices in the model. 
More formally, for $\pi \in \Pi^d$, if $E' = E_{:,\pi}$, then matrices $\{ W^\mathrm{Q}, W^\mathrm{K}, W^\mathrm{V}, W_1, W_{out}, \lnatt{}, \lnffn{}, b_2 \} \subset \theta$ need to be permuted along their first dimension by $\pi$ and all matrices $\{ W^\mathrm{O}, W_2 \}$ along their second one.
Formally, it means 
$(W^\mathrm{Q})' = W^\mathrm{Q}_{\pi, :}$, $(W^\mathrm{O})' = W^\mathrm{O}_{:, \pi}$, etc.

\noindent
\emph{FFN layer dimension.}
All neurons making up matrices $W_1$ and $W_2$ of feed-forward networks can be permuted:
for $\pi \in \Pi^{d_\mathrm{ff}}$, if $W_1' = (W_1)_{:,\pi}$ and $W_2' = (W_2)_{\pi,:}$, then $f_{\theta'}(\cdot) = f_{\theta}(\cdot)$.

\noindent
\emph{Attention heads.}
Heads are interchangeable in~\eqref{chap7/eq:multihead} provided that $W^\mathrm{O}$ is permuted in blocks of $d_\mathrm{v}$ according to its first dimension.

\noindent
\emph{Inside the head.}
Depending on the type of positional embeddings, the previous permutations can be extended.
For instance if they do not impact~\eqref{chap7/eq:attention} (this is not the case for rotary embeddings) then $W^\mathrm{Q}$ and $W^\mathrm{K}$ can be permuted along their second dimension.

\paragraph*{Scaling/Unscaling.}\label{chap7/sec:scaling}
Whenever layer norms or variants are directly followed (or preceded) by linear layers, \eg, at every attention or FFN block, we can rescale component-wise the parameters $g$, $b$ of $\mathrm{LayerNorm}(\vec{z})$
by a vector $\alpha\in \R^d$.
Invariance is obtained by dividing the rows of the following (or preceding) linear layers by the same vector.

\paragraph*{Invertible matrices in QK products.}\label{chap7/sec:invertible}
We hereby assume the positional embeddings do not impact~\eqref{chap7/eq:attention}.
If $P \in \R ^{d_\mathrm{k}\times d_\mathrm{k}}$ is invertible, then choosing $(W^\mathrm{Q})' = W^\mathrm{Q} P$ and $(W^\mathrm{K})' = W^\mathrm{K} (P^\top)^{-1}$ is invariant in~\eqref{chap7/eq:attention}.
This also applies to the case of rotary embeddings by restricting $P$ to be block diagonal of $2\times 2$ matrices that apply a 2D rotations and scaling by a factor $\lambda$ (thanks to the commutativity of 2D rotations). 


\paragraph*{Combining invariants.}
All previous parameter transformations may be seen as invertible right or left matrix multiplications applied to the model parameters. 
They do not interfere and may be combined in a sequence of arbitrary order, yielding $\theta \rightarrow \theta' \rightarrow \theta'' \rightarrow \cdots$.
Combining transformations at all levels improves robustness to removal attacks and to collusion (\ie, when several Bobs share their weights to evade detection).
Indeed, if Bob tries to remove the watermark by re-applying one invariant, it will still be present in the other invariants.
In the same way, if several Bobs compare their models, it will be hard for them to identify which operations were applied to their models, since the order in which they were applied is unknown, and since the weights will differ a lot between them.

We presented some invariant operations that can be applied to the weights of a transformer model similar as GPT-2. 
Note that depending on the model's specificities, invariants may appear or disappear. 
For instance, rotary embeddings~\citep{su2021roformer} and QK normalization layers~\citep{dehghani2023scaling} impact the way to apply invertible matrices in QK products (see below); the order in which normalization is applied (post-norm or pre-norm) may impact the scaling/unscaling invariance; etc.








\subsection{From invariants to watermarks}\label{chap7/sec:invariantsaswatermarks}

\paragraph*{Insertion.}
Before starting the watermark process, for each invariant and each level of the network, we restrict the set of transformations to $2^k$.
For example, we randomly sample $2^k$ possible permutations in $\Pi^d$ for the Embedding dimension (out of the total $d!$). 

Therefore, we can encode $k$ bits for each combination of an invariant and a level. 
We encode a model's identifier as the concatenation of $m$ chunks of $k$ bits ($2^{mk}$ possibilities).
For instance, let $k=4$ and the model have $32$ layers. 
We choose to embed two permutations per layer, one for the attention block and one for the FFN block. 
The total number of bits is $2\times 32\times 4=256$, representing $10^{77}$ possible models (approximately the number of atoms in the universe, an upper bound of the number of Bobs). 

\paragraph*{Extraction.}
To extract the $k$-bits message from a weight matrix, we re-apply all $2^k$ possible invariants to the original matrix.
We then compute the Frobenius norm of the difference (MSE) between the observed weight and the possible watermarked ones.
We choose the one with lowest MSE among the $2^k$ and this choice is encoded as a $k$-bit integer.
Doing that on every blocks of the network and every invariant, we end up with a full message made of $m$ chunks of $k$ bits.
In the case of intertwined invariants, we do the same in the order in which we inserted the invariants, reverting them as we go.

To speed up extraction, we may select a subset of the matrices' rows before extraction.
This speeds up the extraction (in the order of 100$\times$), but makes the detection slightly less robust. 
For instance, in the case of scaling/unscaling we may select the first $100$ components of $\alpha$ from $\R^{d}$ to $\R^{100}$ and $W$ from $\R^{d\times d'}$ to $\R^{100\times 100}$.  

\paragraph*{Matching.}
To match an extracted message (made of $m$ chunks of $k$ bits) with a model's identifier, we compute the number $s$ of chunk-wise errors with respect to all possible identifiers.
We return a match if $s$ is bellow a fixed threshold $\tau$ to ensure resilience to corruption and to provide a confidence score.
A theoretical p-value, \ie, the probability of obtaining a number of errors lower than $s$ for a random model, is given by the regularized incomplete beta function $\mathcal{I}$:
\begin{equation}\label{chap7/eq:pvalue}
    \textrm{p-value}(s) = 1- \left(1-\mathcal{I}_{ 1/2^{k} } ( m-s, s+1) \right)^N,
\end{equation}
where $N$ is the number of distributed models. 


\begin{figure}[b!]
    \centering
    \includegraphics[width=1.0\linewidth]{chapter-7/figs/method.pdf}
    \caption{
    Detailed illustration of watermark insertion and extraction, with the example of permutation on $L$=40 blocks. 
    A user ID is a list $b_1...b_L$ of $L$ bytes, that are used to select the permutation to apply for each block $\ell$.
    For each $\ell$, the extraction computes the MSE between the observed weights and all original permuted weights. 
    It then selects the one with minimum MSE, which in turn gives $b_\ell$.
    }
    \label{chap7/fig:method}
\end{figure}


\paragraph*{Robustness and security.}
Watermarking models through invariance is stealthy, because it does not change their outputs.
However, a distortion-free watermark is also a weakness: Alice can hide the watermark without impacting the model's utility, but on the other hand an adversarial Bob may do the same at no cost.
In short, most of these watermarks are very robust against classical model manipulations (fine-tuning, quantization, etc.) but not against a malicious user who knows the method.
In this case we would only know that the model is an unauthorized copy, without knowing the leaker.

