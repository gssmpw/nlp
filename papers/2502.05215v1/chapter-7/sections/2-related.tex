\newcommand{\att}[1]{\mathrm{Att}^{#1}}
\newcommand{\ffn}[1]{\mathrm{Ffn}^{#1}}
\newcommand{\lnatt}[1]{\mathrm{Ln}_{\mathrm{att}}^{#1}}
\newcommand{\lnffn}[1]{\mathrm{Ln}_{\mathrm{ffn}}^{#1}}
\newcommand{\lnout}{\mathrm{Ln}_{\mathrm{out}}}


\section{Related work and technical background}\label{chap7/sec:related}



\subsection{Deep Neural Network (DNN) Watermarking}\label{chap7/par:watermarking} 

DNN watermarking robustly embeds a unique identifier into the model without affecting its performance, in order to later verify the model's identity.
\textit{robustness}: the watermark should be detected even after the model has been modified.
Modifications may be unintentional -- models are fine-tuned, pruned and quantized
-- or intentional -- adversaries may try to remove the watermark or embed their own~\citep{fan2019rethinking, zhang2020passport, kallas2022rose}.
For instance, some adversarial transforms employ invariance operations in neurons and ReLU layers to evade detection~\citep{yan2023rethinking}, in a similar fashion as the techniques of this work.

We distinguish between white-box and black-box settings, depending on whether the model weights are accessible at verification time, or only through a remote API.
In white-box, the pioneering work~\citep{uchida2017embedding} embeds watermarks into the DNN's weights. 
A regularization loss term during training constrains the weights to carry a specific signal, while minimizing the impact on the model's performance. 
The watermark is then retrieved directly by analyzing the model's weights.
The  Deep·Signs·Marks~\citep{darvish2019deepsigns,chen2019deepmarks} extends this to target black-box settings and propose building collusion-resistant watermarks, 
RIGA~\citep{wang2021riga} improves its covertness and robustness, 
and greedy residuals~\citep{liu2021watermarking} improves the selection of the weights to modify.

Another line of work, called trigger-set based methods, embeds the watermark in the behavior of the model with regards to certain inputs. 
A recurrent idea is to use \emph{backdoors}, \ie, memorize certain sequences of input-output pairs~\citep{adi2018turning, zhang2018protecting}. 
Watermarking generative models is also an active field of research, either by employing triggers~\citep{lim2022protect, ong2021protecting}, or by watermarking their outputs~\citep{fernandez2023stable, kim2023wouaf}.

The literature on watermarking large models is scarce, and none of the current papers operate at our scale. 
The most recent works~\citep{liu2023trapdoor, jiang2023ipcert, tondi2024robust} focus on ResNet/AlexNet ($\approx$20M parameters). 
PLMmark~\citep{li2023plmmark} also needs training and evaluates at most on BERT-large, around 300M parameters. 
This is 100 times smaller than the models we consider in our work (\eg, Llama-30B, Llama-70B). 
Previous methods could be adapted in the context of LLMs, but all of them would require training or at least fine-tuning. 
Their impact on the quality of the text generation and the robustness of the watermark is also not demonstrated. 
Thus, the feasibility of existing watermarking methods to these models remains an open question.



\subsection{Transformers}

The transformer~\citep{vaswani2017attention} has become the standard architecture for many applications in the last few years.
It can be trained efficiently on GPUs and scale well to large datasets and models, in both NLP~\citep{raffel2020exploring, kaplan2020scaling} and computer vision~\citep{dehghani2023scaling, oquab2023dinov2}.
For the sake of completeness and to help better understand the invariant properties of the model, we describe in details the architecture from GPT-2~\citep{radford2019language} (see Fig.~\ref{chap7/fig:transformer}).

\paragraph*{Tokenization.} The input string is first tokenized into a sequence of integers $(x_1, \dots, x_n) \in \V^n$.
An embedding layer $E \in \R^{|\V|\times d}$ maps each token $x_i$ to a continuous vector $z_i^0 = E_{x_i} \in \R^d $, where $d$ is the embedding dimension.


\paragraph*{Attention layers.} 
The self-attention mechanism enables long-range dependencies between sequence elements.
A self-attention transforms an input sequence $\vec{z} \in \R^{n \times d}$ into queries $Q$, keys $K$, and values $V$:
\begin{equation}
    Q = \vec{z}W^\mathrm{Q} \in \R^{n \times d_\mathrm{k}};\;
    K = \vec{z}W^\mathrm{K} \in \R^{n \times d_\mathrm{k}};\;
    V = \vec{z}W^\mathrm{V} \in \R^{n \times d_\mathrm{v}}.
\end{equation}
It then computes attention weights by taking a scaled dot product between the queries and keys:
\begin{align}\label{chap7/eq:attention}
    \mathrm{Attention}(Q,K,V) = \mathrm{softmax} \left( \frac{QK^\top}{\sqrt{d_\mathrm{k}}} \right)V.
\end{align}
Where the $\mathrm{Softmax}$ operator is applied column-wise.

This attention operator is applied $h$ times in parallel, yielding $h$ output \emph{heads}.
The results are concatenated and projected back to the original dimension:
\vspace{-0.1cm}
\begin{align}\label{chap7/eq:multihead}
\mathrm{MultiHead}(Q,K,V) = \mathrm{Concat}(\mathrm{head}_1,., \mathrm{head}_h)W^\mathrm{O},
\end{align}
where $\mathrm{head}_i = \mathrm{Attention}(QW_i^\mathrm{Q}, KW_i^\mathrm{K}, VW_i^\mathrm{V})$. 
The projections $W_i^\mathrm{Q}, W_i^\mathrm{K} \in \R^{d \times d_\mathrm{k}}$, $W_i^\mathrm{V} \in \R^{d \times d_\mathrm{v}}$ and $W^\mathrm{O} \in \R^{hd_\mathrm{v} \times d}$ are learned.

\paragraph*{The feed-forward network.}
The output is fed to a feed-forward network (FFN), \eg, two linear layers with a ReLU activation:
\vspace{-0.1cm}
\begin{align}
\mathrm{FFN}(\vec{h}) = \mathrm{ReLU}(\vec{h}W_1 + b_1)W_2 + b_2,
\end{align}
where $W_1 \in \R^{d \times d_\mathrm{ff}}$, $b_1 \in \R^{d_\mathrm{ff}}$, $W_2 \in \R^{d_\mathrm{ff} \times d}$ and $b_2 \in \R^{d}$ are learned parameters (other variants like SwiGLU~\citep{shazeer2020glu} frequently replace ReLU).

\paragraph*{A stack of residual connections.}
Instead of directly feeding $\vec{z}$ and $\vec{h}$ to the attention and FFN layers, residual connections are applied and the inputs are normalized using layer normalization~\citep{ba2016layer} (or variants like RMSnorm~\citep{zhang2019root}):
$\label{chap7/eq:layernorm}
\mathrm{LayerNorm}(\vec{z}) = \frac{\vec{z} - \mu}{\sigma} \odot g + b,
$
where $\mu$ and $\sigma$ are the mean and standard deviation of $\vec{z}$ along its second dimension, and $g\in \R^d$ and $b\in \R^d$ are learned parameters.
This is repeated for each layer $l\in \{1, ..., L\}$ of the transformer:
\vspace{-0.1cm}
\begin{align}
\vec{h}^{l} &= \att{l} \left( \lnatt{l} \big( \vec{z}^{l} \big) \right) + \vec{z}^{l} \\
\vec{z}^{l+1} &= \ffn{l} \left( \lnffn{l} \big( \vec{h}^{l} \big) \right) + \vec{h}^{l}.
\end{align}
The output is fed to a normalization layer $\lnout$ and a linear layer $W_\mathrm{out} \in \R^{d \times |\mathcal V|}$ to generate logits, and a softmax outputs the probability distribution of the next token.

\paragraph*{Positional embeddings.} 
For many tasks, it is useful to encode the position of tokens in the input sequence.
Positional embeddings are what allows to encode this information.
They were originally sinusoidal functions of the position~\citep{vaswani2017attention} added to the input embeddings. 
There are now several variants~\citep{raffel2020exploring, su2021roformer, press2021train, kazemnejad2023impact}, that may change Eq.~\eqref{chap7/eq:attention}.
For instance, rotary embeddings~\citep{su2021roformer} multiply queries and keys depending on their relative position in the sequence.
If $m$ is the position of the query ($Q_m= \vec{z}_m W^\mathrm{Q}$) and $n$ the position of the key, then it rewrites the product of~\eqref{chap7/eq:attention} as:
\vspace{-0.1cm}
\begin{align}\label{chap7/eq:rotary}
    \qquad Q_m K^\top_n = z_m W^\mathrm{Q} R_{\Theta, n-m} (z_n W^\mathrm{K})^\top.
\end{align}
$R_{\Theta,n}$ is a block diagonal matrix with $2\times2$ rotation matrix entries:
\vspace{-0.1cm}
\begin{align*}
   \left( R_{\Theta,n} \right) _i  =
   \begin{pmatrix}
   \cos n \theta_i & -\sin n \theta_i \\
   \sin n \theta_i & \cos n \theta_i
   \end{pmatrix},
\end{align*}
with rotation frequencies chosen as $\theta_i = 10,000^{-2i/d}$.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.99\linewidth, clip, trim={1cm 0 1cm 0.5cm}]{chapter-7/figs/transformer.pdf}
    \caption{
    Detailed structure of the operations that take place in a transformer.
    $L$: number of layers, $d$: embedding dimension, $h$: number of heads, $d_\mathrm{k}$ and $d_\mathrm{v}$: dimensions of the keys and values, $d_\mathrm{ff}$: dimension of the feed-forward network, $|\mathcal V|$: size of the vocabulary.
    }\label{chap7/fig:transformer}
\end{figure}
