
\section{Introduction}

Large-scale transformer models are a leap forward in the field of machine learning, with large language models like GPT-4~\citep{openai2023gpt} and Llama~\citep{touvron2023llama}, or vision ones like ViT-22b~\citep{dehghani2023scaling} and DINOv2~\citep{oquab2023dinov2}.
As these models grow in size, it becomes important to be able to trace weights to safeguard it from unauthorized usage and distribution.
Note that, differently to the previous part~\ref{part:genai-tracing}, we are interested in tracing the model itself, not content it generates.
Watermarking deep neural networks (DNN)~\citep{uchida2017embedding, adi2018turning} aims to address this issue by embedding an identifier into the model, which can be extracted to identify the model's owner in case of unauthorized redistribution.


However, watermarking large transformer models poses new challenges. 
Current watermarking methods optimize the weights to infuse the watermark, either during pre-training or by fine-tuning the weights with additional losses.
While these techniques have shown success for smaller models, they become computationally infeasible for large-scale models and for the burgeoning number of potential users and applications.

To address these challenges, we introduce a new approach to watermarking large transformers, when access to both the original and watermarked model is granted, \ie, in a non-blind white-box setting (as defined in Sec.~\ref{chap0/sec:other-considerations}).
Our method capitalizes on the inherent invariance of transformers. 
For a given model, it generates equivalent copies that serve as carriers for arbitrary signatures, by employing operations such as dimension permutation and coupled matrix multiplications.
We therefore create model replicas without changing the model's outputs and without any training or fine-tuning.
We conduct experiments on Llama models ranging from 7 to 70 billion parameters to evaluate the applicability of our approach and its robustness against model processing (\eg, fine-tuning, pruning, quantization, etc.).
We also discuss the main drawbacks of this setting. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{chapter-7/figs/fig1.pdf}
    \caption{
        Overview of watermarking through invariance. 
        We identify each model by applying invariance operations to the original weights.
        The resulting model is functionally equivalent to the original, but carries an identifier that can be extracted to identify the model's owner.
    }
    \label{chap7/fig:overview}
\end{figure}

The chapter is organized as follows:
\begin{itemize}
    \item 
    \autoref{chap7/sec:related} provides an overview of related works on DNN watermarking and background on the transformer architecture;
    \item
    \autoref{chap7/sec:method} details three of the transformer's invariants and how to exploit them for watermarking;
    \item
    \autoref{chap7/sec:experiments} presents experimental results on the Llama LLMs, and discusses the robustness of the watermarking against various model transformations.
\end{itemize}

\paragraph*{Problem statement.}
A provider \emph{Alice}, distributes her model to various users \emph{Bob} (either individuals or organizations).
She aims to trace the model back to a specific user, in case of unauthorized distribution or leaks. 
As a precautionary measure, Alice embeds a unique signature in the model's weights for each user. 
In a white-box setting, Alice has access to the models' weights and extracts the signature from it to identify Bob.
Besides, Bob may evade detection intentionally (trying to remove the watermark) or unintentionally (fine-tuning, quantization, etc.).

This setting is quite common. 
Indeed few entities (``Alices'') have the necessary computation resources, data and expertise to generate the base model. 
For example, the training of the 70-B Llama model took around $1$B GPU-hours.
Therefore, there are few variants of such large models in the world. 
Besides, when Bob gains access to the base model, it is common that he transforms it and that it re-emerges in a public forum or through another channel, so that Alice can analyze it. 
This can be either because Bob re-distributed it or because Alice sought the model through legal channels, as suggested by~\citet{fan2021deepip}.
For example, many variants of the Llama models have been fine-tuned on instruction datasets and been made available online.

