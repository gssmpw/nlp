

\section{Conclusion}

Our work presents a lightweight approach for watermarking large transformers. 
We leverage invariance properties to generate equivalent copies for watermark embedding. 
It ensures that the model's outputs are preserved while providing close-to-perfect robustness against processes like fine-tuning or quantization. 

Yet, this approach has limitations. 
Namely, it is limited to white-box scenarios. 
Additionally, if a sophisticated attacker identifies all invariants, they may remove the watermark by applying the same transformation techniques. 
Although the model could still be identified as an unauthorized copy, the unique binary signature associated with the original watermark would be lost.
Despite these challenges, this work is a starting point to exploit invariance properties that stem from the extreme redundancy of parameters of large networks, for watermarking applications.
