
\section{Main experiments}
\label{chap4/sec:exps}



\subsection{Audio/speech quality}
\label{chap4/sec:quality}

We first evaluate the quality of the watermarked audio using:
Scale Invariant Signal to Noise Ratio (SI-SNR): 
$\textrm{SI-SNR}(s, s_w) = 10 \log_{10} \left( \| \alpha s \|_2^2 / \| \alpha s - s_w \|_2^2 \right)$,
with $s$ the clean audio and $s_w$ the watermarked one,
where $\alpha = \langle s, s_w \rangle / \| s \|_2^2$;
as well as Perceptual Evaluation of Speech Quality (PESQ)~\citep{rix2001perceptual}, 
Virtual Speech Quality Objective Listener (ViSQOL)~\citep{hines2012visqol} and
Short Term Objective Intelligibility (STOI)~\citep{taal2010short} which are objective perceptual metrics measuring the quality of speech signals.

\autoref{chap4/tab:audio_quality} report these metrics.
AudioSeal behaves differently than watermarking methods like WavMark~\citep{chen2023wavmark} that try to minimize the SI-SNR.
In practice, high SI-SNR is indeed not necessarily correlated with good perceptual quality.
AudioSeal is not optimized for SI-SNR but rather for perceptual quality of speech (similarly as DeAR~\citep{DEAR_Liu0FMZY23} which reports 25.96 as SI-SNR).
This is better captured by the other metrics (PESQ, STOI, ViSQOL), where AudioSeal consistently achieves better performance.
Put differently, our goal is to hide as much watermark power as possible while keeping it perceptually indistinguishable from the original.
\autoref{chap4/fig:loc_quali} also visualizes how the watermark signal follows the shape of the speech waveform.

The metric used for our subjective evaluations is the MUSHRA score~\citep{mushra}. 
It is a crowdsourced test in which participants rate the quality of various samples on a scale of 0 to 100. 
The ground truth is provided for reference. 
We utilized 100 speech samples, each lasting 10 seconds. 
Each sample was evaluated by at least 20 participants.
As part of the study, we included a low anchor, which is a very lossy compression at 1.5kbps, encoded using EnCodec. 
Participants who failed to assign the lowest score to the low anchor for at least 80\% of their assignments were excluded from the study.
In this study our samples got superior ratings than WavMark, with an average score of 77.07, 5 points higher than WavMark.
For comparison, the ground truth samples received an average score of 80.49, while the low anchor's average score was 53.21.


\begin{table}[t!]
    \centering
    \caption{
        Audio quality metrics. 
        Compared to traditional watermarking methods that minimize the SNR like WavMark, AudioSeal achieves same or better perceptual quality.
    }\label{chap4/tab:audio_quality}
    \footnotesize
        \begin{tabular}{lccccc}
            \toprule
            \textbf{Methods} & \textbf{SI-SNR} & \textbf{PESQ} & \textbf{STOI} & \textbf{ViSQOL} & \textbf{MUSHRA}  \\
            \midrule
            WavMark & \textbf{38.25} & 4.302 & 0.997 & 4.730 & 71.52 $\pm$ 7.18\\
            AudioSeal & 26.00 & \textbf{4.470} & 0.997 & \textbf{4.829} &  \textbf{77.07} $\pm$ 6.35 \\
            \bottomrule
        \end{tabular}
\end{table}















\subsection{Comparison with passive classifier}\label{chap4/sec:active-passive}


\begin{table}[t!]
    \centering
    \caption{
        Comparison with Voicebox binary classifier. 
        Percentage refers to the fraction of masked input frames.
        Detection with AudioSeal is perfect for all samples, while Voicebox classifier fails on re-synthesized audio.
    }
    \label{chap4/tab:voicebox}
    \footnotesize
        \begin{tabular}{r *{3}{c}  *{3}{c}  *{3}{c} }
            \toprule
            & \multicolumn{3}{c}{\textbf{AudioSeal (Ours)}} & \multicolumn{3}{c}{\textbf{Voicebox Classif.}} \\
            \cmidrule(rr){2-4} \cmidrule(rr){5-7}
            \textbf{\% Mask} & Acc. & TPR & FPR & Acc. & TPR & FPR \\
            \midrule
            \multicolumn{7}{l}{\emph{Original audio vs AI-generated audio}} \\
            30\% & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 \\
            50\% & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 \\
            90\% & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 \\
            \midrule
            \multicolumn{7}{l}{\emph{Re-synthesized audio vs AI-generated audio}} \\
            30\% & \textbf{1.0} & \textbf{1.0} & \textbf{0.0} & 0.704 & 0.680 & 0.194 \\
            50\% & \textbf{1.0} & \textbf{1.0} & \textbf{0.0} & 0.809 & 0.831 & 0.170 \\
            90\% & \textbf{1.0} & \textbf{1.0} & \textbf{0.0} & 0.907 & 0.942 & 0.112 \\
            \bottomrule
        \end{tabular}
\end{table}



We first compare detection results on samples generated with Voicebox~\citep{le2023voicebox}.
We compare to the passive setup where a classifier is trained to discriminate between Voicebox-generated and real audios (this is done with a detector that shares the same architecture as AudioSeal's, trained until reaching 100\% accuracy on Voicebox samples).
Following the approach in the Voicebox study, we evaluate 2,000 approximately 5-second samples from LibriSpeech, these samples have masked frames (90\%, 50\%, and 30\% of the phonemes) pre-Voicebox generation.
We evaluate on the same tasks, \ie, distinguishing between original and generated, or between original and re-synthesized (created by extracting the Mel spectrogram from original audio and then vocoding it with the HiFi-GAN vocoder).

We use the True Positive Rate (\Gls*{TPR}) and the False Positive Rate (\Gls*{FPR}) as key metrics.
TPR measures correct identification of AI-generated samples, while FPR indicates the rate of genuine audio clips falsely flagged.
In practical scenarios, minimizing FPR is crucial. 
For example, on a platform processing 1 billion samples daily, an FPR of $10^{-3}$ and a TPR of $0.5$ means that 1 million samples require manual review each day, yet only half of the watermarked samples are detected.
The \Gls*{ROC} AUC (Area Under the Curve of the Receiver Operating Characteristics) gives a global measure of performance over all threshold levels, and captures the TPR/FPR trade-off.

Both active and passive setups achieve perfect classification in the case when trained to distinguish between natural and Voicebox.
Conversely, the second part of Tab.~\ref{chap4/tab:voicebox} highlights a significant drop in performance when the classifier is trained to differentiate between Voicebox and re-synthesized.
It suggests that the classifier is detecting vocoder artifacts, since the re-synthesized samples are sometimes wrongly flagged.
The classification performance quickly decreases as the quality of the AI-generated sample increases (when the input is less masked).
On the other hand, our proactive detection does not rely on model-specific artifacts but on the watermark presence. %
This allows for perfect detection over all the audio clips. %




\subsection{Robustness and comparison with watermarking}

\begin{table}[t!]
    \centering
    \caption{
        Detection results for different edits applied before detection. 
        Acc. ({\aux{TPR/FPR}}) is the accuracy (and TPR/FPR) obtained for the threshold that gives best accuracy on a balanced set of augmented samples.
        AUC is the area under the ROC curve.
    }
    \label{chap4/tab:wm_robustness}
    \footnotesize
        \begin{tabular}{l *{2}{l}  *{2}{l}}
        \toprule
        & \multicolumn{2}{l}{\textbf{AudioSeal (Ours)}} & \multicolumn{2}{l}{\textbf{WavMark}} \\
        \cmidrule(rr){2-3} \cmidrule(rr){4-5}
        \multicolumn{1}{c}{Edit} & Acc. \aux{TPR/FPR} & AUC & Acc. \aux{TPR/FPR}  & AUC \\
        \cmidrule(rr){1-1} \cmidrule(rr){2-3} \cmidrule(rr){4-5}
        None & 1.00 \aux{1.00/0.00} & 1.00 & 1.00 \aux{1.00/0.00} & 1.00 \\
        Bandpass & 1.00 \aux{1.00/0.00} & 1.00 & 1.00 \aux{1.00/0.00} & 1.00 \\
        Highpass  &  0.61 \aux{0.82/0.60} & 0.61 & \bf 1.00 \aux{1.00/0.00} & \bf 1.00 \\
        Lowpass & \bf 0.99 \aux{0.99/0.00} & \bf 0.99 & 0.50 \aux{1.00/1.00} & 0.50 \\
        Boost & 1.00 \aux{1.00/0.00} & 1.00 & 1.00 \aux{1.00/0.00} & 1.00 \\
        Duck & 1.00 \aux{1.00/0.00} &  1.00 & 1.00 \aux{1.00/0.00} & 1.00 \\
        Echo & \bf 1.00 \aux{1.00/0.00} & \bf 1.00 & 0.93 \aux{0.89/0.03} & 0.98 \\
        Pink & \bf 1.00 \aux{1.00/0.00} & \bf 1.00 & 0.88 \aux{0.81/0.05} & 0.93 \\
        White & \bf 0.91 \aux{0.86/0.04} & \bf 0.95 & 0.50 \aux{0.54/0.54} & 0.50 \\
        Fast (1.25x) & \bf 0.99 \aux{0.99/0.00} & \bf 1.00 & 0.50 \aux{0.01/0.00} & 0.15 \\
        Smooth & \bf 0.99 \aux{0.99/0.00} &  1.00   & 0.94 \aux{0.93/0.04} & 0.98 \\
        Resample & 1.00 \aux{1.00/0.00} &  1.00 & 1.00 \aux{1.00/0.00} & 1.00 \\
        AAC & 1.00 \aux{1.00/0.00} &  1.00 & 1.00 \aux{1.00/0.00} & 1.00 \\
        MP3 & \bf 1.00 \aux{1.00/0.00} & \bf 1.00 & 1.00 \aux{0.99/0.00} & 0.99 \\
        EnCodec & \bf  0.98 \aux{0.98/0.01} & \bf 1.00 & 0.51 \aux{0.52/0.50} & 0.50 \\
        \midrule
        Average & \bf 0.96 \aux{0.98/0.04} & \bf 0.97 & 0.85 \aux{0.85/0.14} & 0.84 \\
        \bottomrule
        \end{tabular}
\end{table}

\paragraph*{Audio editing.}
We evaluate the robustness of the detection on a wide range of audio editing operations: 
time modification (faster, resample), 
filtering (bandpass, highpass, lowpass), 
audio effects (echo, boost audio, duck audio), 
noise (pink noise, random noise),
and compression (MP3, AAC, EnCodec).
In order to show generalization, we chose stronger parameter to the attacks than those used during training (see Sec.~\ref{chap4/sec:training-details}).

\paragraph*{Robustness of the detection.}
Detection is done on 10k ten-seconds audios from our VoxPopuli validation set.
For each edit, we first build a balanced dataset made of the 10k watermarked/ 10k non-watermarked edited audio clips.
We quantify the performance by adjusting the threshold of the detection score, selecting the value that maximizes accuracy (we provide corresponding TPR and FPR at this threshold).
To adapt data-hiding methods (\eg, WavMark) for proactive detection, we embed a binary message (chosen randomly beforehand) in the generated speech before release. The detection score is then computed as the Hamming distance between the original message and the one extracted from the scrutinized audio. 

We observe in Tab.~\ref{chap4/tab:wm_robustness} that AudioSeal is overall more robust, with an average AUC of 0.97 vs. 0.84 for WavMark.
The performance for lowpass and highpass filters indicates that AudioSeal embeds watermarks neither in the low nor in the high frequencies (WavMark focuses on high frequencies).
We give results on more augmentations in Sec.~\ref{chap4/app:robustness}.



\subsection{Localization}


\begin{figure}[b!]
    \centering
    \includegraphics[width=0.48\linewidth, clip, trim={0 1.8in 0 0}, valign=t]{chapter-4/figs/loc_quantitative.pdf}\hfill
    \includegraphics[width=0.48\linewidth, clip, trim={0 0 0 1.3in}, valign=t]{chapter-4/figs/loc_quantitative.pdf}
    \caption{\textbf{Localization results} across different durations of watermarked audio signals in terms of Sample-Level Accuracy and Intersection Over Union (IoU) metrics ($\uparrow$ is better).}
    \label{chap4/fig:loc_quantitative}
\end{figure}


We evaluate localization with the sample-level detection accuracy, \ie, the proportion of correctly labeled samples, and the Intersection over Union (IoU).
The latter is defined as the intersection between the predicted and the ground truth detection masks (1 when watermarked, 0 otherwise), divided by their union.
IoU is a more relevant evaluation of the localization of short watermarks in a longer audio.

This evaluation is carried out on the same audio clips as for detection.
For each one of them, we watermark a randomly placed segment of varying length.
Localization with WavMark is a brute-force detection: a window of 1s slides over the 10s of speech with the default shift value of 0.05s.
The Hammning distance between the 16 pattern bits is used as the detection score.
Whenever a window triggers a positive, we label its 16k samples as watermarked in the detection mask in $\{0,1\}^t$.

\autoref{chap4/fig:loc_quantitative} plots the sample-level accuracy and IoU for different proportions of watermarked speech in the audio clip.
AudioSeal achieves an IoU of 0.99 when just one second of speech is AI-manipulated, compared to WavMark's 0.35.
Moreover, AudioSeal allows for precise detection of minor audio alterations: it can pinpoint AI-generated segments in audio down to the sample level (usually 1/16k sec), while the concurrent WavMark only provides one-second resolution.
This is the reason why it lags behind in terms of IoU more than accuracy.
It is especially relevant for speech samples, where a simple word modification may greatly change meaning. 





\subsection{Attribution}

\begin{table}[t!]
    \centering
    \caption{
        Attribution results.
        We report the accuracy of the attribution (Acc.) and false attribution rate (FAR). 
        Detection is done at FPR=$10^{-3}$ and attribution matches the decoded message to one of $N$ versions.
        We report averaged results over the edits of Tab.~\ref{chap4/tab:wm_robustness}.
    }\label{chap4/tab:attribution}
    \footnotesize
        \begin{tabular}{cr *{5}{c}}
            \toprule
            & N & $1$ & $10$ & $10^2$ & $10^3$ & $10^4$ \\ \midrule
    \multirow{2}{*}{FAR (\%) $\downarrow$} & WavMark      & 0.0 & \textbf{0.20} & \textbf{0.98} & \textbf{1.87} & \textbf{4.02} \\
            & AudioSeal   & 0.0 & 2.52 & 6.83 & 8.96 & 11.84 \\ \midrule
    \multirow{2}{*}{\shortstack{Acc. (\%) $\uparrow$}} & WavMark      & 58.4 & 58.2 & 57.4 & 56.6 & 54.4 \\
            & AudioSeal  & \textbf{68.2} & \textbf{65.4} & \textbf{61.4} & \textbf{59.3} & \textbf{56.4} \\ 
            \bottomrule
        \end{tabular}
\end{table}

Given an audio clip, the objective is now to find if any of $N$ versions of our model generated it (detection), and if so, which one (identification). 
For evaluation, we create $N'=100$ random 16-bits messages and use them to watermark 1k audio clips, each consisting of 5 seconds of speech (not 10s to reduce compute needs). 
This results in a total of 100k audios. 
For WavMark, the first 16 bits (/32) are fixed and the detection score is the number of well decoded pattern bits, while the second half of the payload hides the model version.
An audio clip is flagged if the average output of the detector exceeds a threshold, corresponding to FPR=$10^{-3}$.
Next, we calculate the Hamming distance between the decoded watermark and all $N$ original messages. 
The message with the smallest Hamming distance is selected.
It is worth noting that we can simulate $N>N'$ models by adding extra messages. 
This may represent versions that have not generated any sample.

False Attribution Rate (FAR) is the fraction of wrong attribution \emph{among the detected audios} while the attribution accuracy is the proportion of detections followed by a correct attributions \emph{over all audios}. 
AudioSeal has a higher FAR but overall gives a better accuracy, which is what ultimately matters.
First, we observe that the false attribution rate -- which we define as the proportion of audios that are wrongly attributed among the detected ones -- is higher in our case.
On the other hand \autoref{chap4/tab:attribution} highlights that AudioSeal gives better attribution accuracy.
It is defined as the proportion of watermarked samples that are both flagged and correctly attributed and is what ultimately matters.
In summary, decoupling detection and attribution achieves better detection rate and makes the global accuracy better, at the cost of occasional false attributions.



\subsection{Efficiency analysis}
\label{chap4/sec:speed}


\begin{figure}[b!]
    \centering
    \includegraphics[width=0.65\linewidth, clip, trim={0 0 0 0}]{chapter-4/figs/speed.pdf}
    \caption{Mean runtime ($\downarrow$ is better) of AudioSeal versus WavMark. 
    AudioSeal is one order of magnitude faster for watermark generation and two orders of magnitude faster for watermark detection for the same audio input.
    }
    \label{chap4/fig:efficiency}
\end{figure}



To highlight the efficiency of AudioSeal, we conduct a performance analysis and compare it with WavMark. 
We apply the watermark generator and detector of both models on a dataset of 500 audio segments ranging in length from 1 to 10 seconds, using a single Nvidia Quadro GP100 GPU. 
The results are displayed in Fig.~\ref{chap4/fig:efficiency} and Tab.~\ref{chap4/tab:speed}.
In terms of generation, AudioSeal is 14$\times$ faster than WavMark. 
For detection, AudioSeal outperforms WavMark with two orders of magnitude faster performance on average, notably 485$\times$ faster in scenarios where there is no watermark (Tab.~\ref{chap4/tab:speed}). 
The speed difference in the case of WavMark between watermarked and non-watermarked audios is explained by the fact that whenever the detector flags a 1-second span as watermarked, it will directly skip to the next 1-second span, while for non-watermarked audios, it will slide the window by 0.05s.
AudioSeal's speed is due to the model's localized watermark design, which bypasses the need for watermark synchronization (recall that WavMark relies on 20 pass forwards for a one-second snippet).
AudioSeal's detector provides detection logits for each input sample directly with only one pass to the detector, significantly enhancing the detection's computational efficiency.
This makes our system highly suitable for real-time and large-scale applications.

\begin{table*}[t!]
    \centering
    \caption{
        Average runtime (ms) per sample of AudioSeal model against WavMark~\citep{chen2023wavmark} method. 
        Our experiments were conducted on a dataset of audio segments spanning 1 second to 10 seconds, using a single Nvidia Quadro GP100 GPU. 
        The results demonstrate significant speed improvements for both watermark generation and detection with and without the presence of a watermark. 
        Notably, for watermark detection, AudioSeal is 485$\times$ faster than WavMark when there is no watermark, because the latter relies on more forward passes when trying to synchronize the watermark. 
    }
    \footnotesize
    \label{chap4/tab:speed}
    \begin{tabular}{llll}
    \toprule
               Model & Watermarked &     \textbf{Detection ms (speedup)} &   \textbf{Generation ms (speedup)} \\
    \midrule
             WavMark &          \multirow{2}{*}{\xmarkg}      & 1710.70 $\pm$ 1314.02 &    -- \\
             AudioSeal (ours) &           &       \textbf{3.25 $\pm$ 1.99} \;\; (\textbf{485$\times$}) &    -- \\
    \midrule
             WavMark &         \multirow{2}{*}{\cmarkg} &    106.21 $\pm$ 66.95 & 104.58 $\pm$ 65.66 \\
    AudioSeal (ours) &          &       \textbf{3.30} $\pm$ \textbf{2.03} \;\; (\textbf{35$\times$}) &    \textbf{7.41} $\pm$ \textbf{4.52} \;\; (\textbf{14} $\times$) \\
    
    \bottomrule 
    \end{tabular}
\end{table*}





















