
\section{Adversarial watermark removal}
\label{chap4/sec:attacks}


So far, we considered robustness against regular editing that may happen naturally.
We now examine more damaging deliberate attacks, where attackers might either ``forge'' the watermark by adding it to authentic samples (to overwhelm detection systems) or ``remove'' it to avoid detection. 
Our findings suggest that in order to maintain the effectiveness of watermarking against such adversaries, the code for training watermarking models and the awareness that published audios are watermarked can be made public. 
However, the detector's weights should be kept confidential.

We focus on watermark-removal attacks and consider three types of attacks depending on the adversary's knowledge:
\begin{itemize}
    \item \textit{White-box}: 
    the adversary has access to the detector (\eg, because of a leak), and performs a gradient-based adversarial attack against it.
    The optimization objective is to minimize the detector's output.
    \item \textit{Semi black-box}: 
    the adversary does not have access to any weights, but is able to re-train generator/detector pairs with the same architectures on the same dataset.
    They perform the same gradient-based attack as before, but using the new detector as proxy for the original one.
    \item \textit{Black-box}: 
    the adversary does not have any knowledge on the watermarking algorithm being used, but has access to an API that produces watermarked samples, and to negative speech samples from any public dataset.
    They first collect samples and train a classifier to discriminate between watermarked and not-watermarked.
    They attack this classifier as if it were the true detector.
\end{itemize}


\begin{figure}[b!]
    \centering
    \includegraphics[width=0.55\linewidth, clip, trim={0 0.3cm 0 0}]{chapter-4/figs/attack.pdf}
    \caption{
    \textbf{Watermark-removal attacks.} 
    PESQ is measured between attacked audios and genuine ones (PESQ $<4$ strongly degrades the audio quality).
    The Gaussian noise is used as a reference, but better ``no-box'' attacks are possible (\eg, perceptual autoencoders).
    The more knowledge the attacker has over the watermarking algorithm, the better the attack is.
    }
    \label{chap4/fig:attacks}
\end{figure}

\paragraph{Adversarial attack against a detector.}
Given a watermarked sample $x$ and a detector $D$, we want to find $x' \sim x$ such that $D(x') = 0$.
To that end, we use a gradient-based attack.
It starts by initializing a distortion $\delta_{adv}$ with random gaussian noise.
The algorithm iteratively updates the distortion for a number of steps $n$. 
For each step, the distortion is added to the original audio via $ x' = x + \alpha . \mathrm{tanh} (\delta_{adv})$, passed through the model to get predictions. 
A cross-entropy loss is computed with label 0 (for removal, 1 would be for forging which is not explored here) and back-propagated through the detector to update the distortion, using the Adam optimizer.
At the end of the process, the adversarial audio is $x + \alpha . \mathrm{tanh} (\delta_{adv})$.
In our attack, we use a scaling factor $\alpha=10^{-3}$, a number of steps $n=100$, and a learning rate of $10^{-1}$. 
The $\mathrm{tanh}$ function is used to ensure that the distortion remains small, and gives an upper bound on the SNR of the adversarial audio\footnote{
    This approach is similar to the image optimizations of chapters~\ref{chapter:ssl-watermarking} and~\ref{chapter:active-indexing}; and to the adversarial attacks of Chap.~\ref{chapter:stable-signature}.
}.

\paragraph{Training of the malicious detector.}
For the black-box attack, we want to train and attack a surrogate detector that can distinguish between watermarked and non-watermarked samples, when access to many samples of both types is available.
To train the classifier, we use a dataset made of more than 80k samples of 8 seconds speech from Voicebox~\citep{le2023voicebox} watermarked using our proposed method and a similar amount of genuine (un-watermarked) speech samples. 
The classifier shares the same architecture as AudioSeal's detector. 
The classifier is trained for 200k updates with batches of 64 one-second samples. 
It achieves perfect classification of the samples. 
This is coherent with the findings of Voicebox~\citep{le2023voicebox}.



\paragraph*{Results.}
For every scenario, we watermark 1k samples of 5 seconds, then attack them.
\autoref{chap4/fig:attacks} contrasts various attacks at different intensities, using Gaussian noise as a reference.
The white-box attack is by far the most effective one, increasing the detection error by around 80\%, while maintaining high audio quality (PESQ $>4$).
Other attacks are less effective, requiring significant audio quality degradation to achieve $50\%$ increase the detection error, though they are still more effective than random noise addition.
In summary, the more is disclosed about the watermarking algorithm, the more vulnerable it is. 
The effectiveness of these attacks is limited as long as the detector remains confidential.
