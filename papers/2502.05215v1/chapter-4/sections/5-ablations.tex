


\section{Ablation studies and additional results}\label{chap4/sec:ablations}






\subsection{False positive rates for WavMark}\label{chap4/app:fpr}

\paragraph{Theoretical study.}

When doing detection with multi-bit watermarking, previous works usually extract the message $m'(x)$ from the content $x$ and compare it to the original binary signature $m\in \{ 0,1 \}^{k}$ embedded in the speech sample, as done in Chap.~\ref{chapter:stable-signature}, Sec.~\ref{chap3/subsec:statistical-test}.
The detection test relies on the number of matching bits $M(m,m')$:
\begin{equation} 
    \text{if } M\left(m,m'\right) \geq \tau \,\,\textrm{ where }\,\, \tau\in 
\{0,\ldots,k\},
\end{equation}
then the audio is flagged.
This provides theoretical guarantees over the false positive rates.

Formally, the null hypothesis $\H_0$ is: ``The audio signal is not watermarked'', against the alternative $\H_1$: ``The audio signal is watermarked''.
Under $\H_0$ (\ie, for unmarked audio), if the bits $m'_1, \ldots, m'_k$ are independent and identically distributed Bernoulli random variables with parameter $0.5$, then  $M(m, m')$ follows a binomial distribution with parameters ($k$, $0.5$).
The False Positive Rate (FPR) is defined as the probability that $M(m, m')$ exceeds a given threshold $\tau$. 
A closed-form expression can be given using the regularized incomplete beta function $I_x(a;b)$ (linked to the c.d.f. of the binomial distribution):
\begin{align}\label{chap4/eq:p-value}
    \text{FPR}(\tau) & = \mathbb{P}\left(M \geq \tau \mid \H_0\right) = I_{1/2}(\tau, k - \tau +1).
\end{align}


\begin{figure}[b!]
    \centering
    \includegraphics[width=0.7\linewidth, clip, trim={0.1in 0 0.1in 0}]{chapter-4/figs/appendix/fpr_wavmark.pdf}
    \caption{
        (Left) Histogram of scores output by WavMark's extractor on 10k genuine samples. 
        (Right) Empirical and theoretical FPR when the chosen hidden message is all 0.
    }
    \label{chap4/fig:app_fpr_wavmark}
\end{figure}

\paragraph{Empirical study.}
We empirically study the FPR of WavMark-based detection on our validation dataset.
We use the same parameters as in the original paper, \ie, $k=32$-bits are extracted from 1s speech samples.
We first extract the soft bits (before thresholding) from 10k genuine samples and plot the histogram of the scores in Fig.~\ref{chap4/fig:app_fpr_wavmark} (left).
We should observe a Gaussian distribution with mean $0.5$, while empirically the scores are centered around $0.38$. 
This makes the decision heavily biased towards bit 0 on genuine samples.
It is therefore impossible to theoretically set the FPR since this would largely underestimate the actual one.
For instance, \autoref{chap4/fig:app_fpr_wavmark} (right) shows the theoretical and empirical FPR for different values of $\tau$ when the chosen hidden message is full 0.
Put differently, the argument that says that hiding bits allows for theoretical guarantees over the detection rates is not valid in practice.\footnote{
    In Chap.~\ref{chapter:ssl-watermarking} and~\ref{chapter:stable-signature}, we overcome this issue by whitening the outputs of the watermark extractor (see Fig.~\ref{chap1/fig:whitening}).
    This is not possible in this case, since the watermark embedder and extractor operate jointly. 
    It would therefore require at least to retrain or regularize the model to avoid this bias.
}








\subsection{Another architecture}
\label{chap4/app:other-arch}

Our architecture relies on the SOTA compression method EnCodec. 
However, to further validate our approach, we conduct an ablation study using a different architecture DPRNN~\citep{luo2020dual}. 
The results are presented in Tab.~\ref{chap4/tab:dprnn}.
They show that the performance of AudioSeal is consistent across different architectures. 
This indicates that model capacity is not a limiting factor for AudioSeal.

\begin{table}[t!]
    \centering
    \caption{
        Results for different architectures of the generator and detector.
        The IoU is computed for 1s of watermark in 10s audios (corresponding to the leftmost point in Fig.~\ref{chap4/fig:loc_quantitative}).
    }\label{chap4/tab:dprnn}
    \footnotesize
    \begin{tabular}{lccccc}
        \toprule
        Method  & SI-SNR & STOI & PESQ & Acc. & IoU \\
        \midrule
        EnCodec & 26.00 & 0.997 & 4.470 & 1.00 & 0.802 \\
        DPRNN   & 26.7 & 0.996 & 4.421 & 1.00 & 0.796 \\
        \bottomrule
    \end{tabular}
    \end{table}






\subsection{Audio mixing}

We hereby evaluate the scenario where two signals (e.g., vocal and instrumental) are mixed together. 
We use a non-vocal music dataset for the instrumental part, and we normalize and sum the loudness of the watermarked speech and the music segments. 
\autoref{chap4/tab:mixed_signals} shows that the watermark is still detectable in the mixed signal, even when a non-watermarked background music is present, with a slight decrease in performance.

\begin{table}[t!]
    \centering
    \caption{
        Detection results for watermarked speech and music mixed signals.
        \cmarkg\ and \xmarkg\ indicate the presence or absence of the watermark.
    }
    \label{chap4/tab:mixed_signals}
    \footnotesize
    \begin{tabular}{cclll}
    \toprule
    Speech & BG Music & Acc. & \aux{FPR / TPR} & AUC \\
    \midrule
    \cmarkg & \cmarkg & 1.000 & \aux{$3\times 10^{-4}$ / 1.000} & 1.000 \\
    \cmarkg & \xmarkg & 0.979 & \aux{$3.1\times 10^{-2}$ / 0.988} & 0.996 \\
    \bottomrule
    \end{tabular}
\end{table}




\subsection{Out of domain evaluations}\label{chap4/app:ood}

\paragraph{Synthesized speech and audio.} 
\label{chap4/sec:generalization}
We first evaluate how AudioSeal generalizes on AI-generated speech/audio of various domains and languages. 
Specifically, we use the datasets ASVspoof~\citep{liu2023asvspoof} and FakeAVCeleb \citep{khalid2021fakeavceleb}. 
Additionally, we translate speech samples from a subset of the Expresso dataset~\citep{nguyen2023expresso} (studio-quality recordings) using the SeamlessExpressive translation model~\citep{seamless2023}.
We select four target languages: Mandarin Chinese (CMN), French (FR), Italian (IT), and Spanish (SP). 
We also evaluate on non-speech AI-generated audios: music from MusicGen~\citep{copet2023simple} and environmental sounds from AudioGen~\citep{kreuk2023audiogen}. 


\begin{table*}[t]
    \caption{
    Evaluation of AudioSeal Generalization across domains and languages. Namely, translations of speech samples from the Expresso dataset~\citep{nguyen2023expresso} to four target languages: Mandarin Chinese (CMN), French (FR), Italian (IT), and Spanish (SP), using the SeamlessExpressive model~\citep{seamless2023}. Music from MusicGen~\citep{copet2023simple} and environmental sounds from AudioGen~\citep{kreuk2023audiogen}. 
    }
    \label{chap4/tab:ood_data}
    \centering
    \footnotesize
        \begin{tabular}{l| *{6}{p{1.0cm}} *{2}{p{1.0cm}} }
        \toprule
        Aug & \rotatebox[origin=c]{45}{Seamless (Cmn)} & \rotatebox[origin=c]{45}{Seamless (Spa)} & \rotatebox[origin=c]{45}{Seamless (Fra)} & \rotatebox[origin=c]{45}{Seamless(Ita)} & \rotatebox[origin=c]{45}{Seamless (Deu)} & \rotatebox[origin=c]{45}{Voicebox (Eng)} & \rotatebox[origin=c]{45}{AudioGen} & \rotatebox[origin=c]{45}{MusicGen}  \\
        \midrule
        None         & 1.00           & 1.00            & 1.00            & 1.00           & 1.00            &   1.00   &   1.00   &   1.00\\
        \midrule
        Bandpass   & 1.00 &   1.00  &   1.00 & 1.00 &   1.00  &  1.00 &   1.00   &   1.00   \\
            Highpass  & 0.71 &  0.68  &   0.70  & 0.70 &  0.70  &  0.64   &  0.52 &  0.52     \\
        Lowpass    & 1.00 &  0.99 &  1.00 & 1.00 & 1.00 &  1.00  &  1.00  &  1.00  \\
            Boost     & 1.00  &  1.00  &  1.00 & 1.00  &  1.00 &  1.00 &     1.00     &      1.00   \\
            Duck      & 1.00  &  1.00  &  1.00 &1.00 &  1.00 &   1.00  &   1.00   &     1.00    \\
            Echo   & 1.00  &  1.00 &  1.00 &1.00 & 1.00 &  1.00  &      1.00    &     1.00     \\
            Pink   & 0.99 &   1.00  & 0.99 & 1.00 &   0.99 &  1.00 &      1.00    &    1.00  \\
            White  & 1.00 &    1.00  & 1.00 & 1.00 &  1.00 & 1.00   &  1.00   &  1.00   \\
        Fast (x1.25)  & 0.97 & 0.98 & 0.99  & 0.98 & 0.99 & 0.98 & 0.87 & 0.87 \\
            Smooth    &  0.96  &  0.99  &   0.99  &    0.99    &      0.99          & 0.99 &  0.98  &    0.98   \\
            Resample  & 1.00 &  1.00 &  1.00 & 1.00 &    1.00  & 1.00 &   1.00    &   1.00   \\
                AAC & 0.99 &  0.99  &  0.99  & 0.99 &  0.99  &   0.97  &  0.99   &     0.98    \\
                MP3 & 0.99 &  0.99   &  0.99 & 0.99 & 0.99  &    0.97  &  0.99    & 1.00  \\
            Encodec   & 0.97 &  0.98   &  0.99 & 0.99 & 0.98 &   0.96     &  0.95    & 0.95   \\
            \midrule
            Average   &  0.97 &  0.97  & 0.98  & 0.98 & 0.98 & 0.97 & 0.95 & 0.95  \\
            \bottomrule
        \end{tabular}
\end{table*}


\begin{table}[t!]
    \centering
    \caption{
        Audio quality and intelligibility evaluations on AI-generated speech from various models and languages.
    }
    \label{chap4/tab:ood_metrics}
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{cccccc}
        \toprule
        Model & Dataset & SI-SNR & PESQ & STOI & ViSQOL \\
        \midrule
        \multirow{3}{*}{\scriptsize \rotatebox[origin=c]{90}{AudioSeal}} & Seam. (Deu)       & 23.35 & 4.244 & 0.999 & 4.688 \\
        & Seam. (Fr)        & 24.02 & 4.199 & 0.998 & 4.669 \\
        & Voicebox             & 25.23 & 4.449 & 0.998 & 4.800 \\
        \midrule
        \multirow{3}{*}{\scriptsize \rotatebox[origin=c]{90}{WavMark}} & Seam. (Deu)    & 38.93 & 3.982 & 0.999 & 4.515 \\
        & Seam. (Fr)     & 39.06 & 3.959 & 0.999 & 4.506 \\
        & Voicebox          & 39.63 & 4.211 & 0.998 & 4.695 \\
        \bottomrule
    \end{tabular}
\end{table}

We employ the same set of augmentations and observe very similar detection results, as demonstrated in Tab.~\ref{chap4/tab:ood_data}.
Interestingly, even though we did not train our model on AI-generated speech, we sometimes notice a slight improvement in performance compared to our test data. 
No sample is misclassified among the 10k samples that comprise each of our out-of-distribution (OOD) datasets.
We also provide perceptual metrics results on some OOD data in Tab.~\ref{chap4/tab:ood_metrics}.
We observe that AudioSeal performs similarly on these datasets, with a slight decrease in performance compared to our original dataset.
We explain the decrease by the \ploss\ which makes the watermark hidden in the same frequency bands as English speech, which might not be the case for other languages or audio types.

\paragraph{Fake vs. real datasets.}
We also evaluate AudioSeal on three additional datasets containing real human speech: AudioSet~\citep{gemmeke2017audio}, ASVspoof~\citep{liu2023asvspoof}, and FakeAVCeleb~\citep{khalid2021fakeavceleb}, and observe similar performance, see Tab.~\ref{chap4/tab:other_datasets}.


\begin{table}[t!]
    \centering
    \caption{Evaluation of the detection performances on different datasets. AudioSet is an environmental sounds dataset while ASVspoof~\citep{liu2023asvspoof} and FakeAVCeleb~\citep{khalid2021fakeavceleb} are deep-fake detection datasets.}
    \label{chap4/tab:other_datasets}
    \footnotesize
    \begin{tabular}{l *{2}{l}}
        \toprule
        Dataset & Acc. \aux{TPR/FPR} & AUC \\
        \midrule
        Audioset & 0.9992 \aux{0.9996/0.0011} & 1.0 \\
        ASVspoof & 1.0 \aux{1.0/0.0} & 1.0 \\
        FakeAVCeleb & 1.0 \aux{1.0/0.0} & 1.0 \\
        \bottomrule
    \end{tabular}
\end{table}











\begin{figure}[b!]
    \centering
    \includegraphics[width=1.0\textwidth]{chapter-4/figs/appendix/augmentation_curves.pdf}
    \caption{
        Accuracy of the detector on augmented samples with respect to the strength of the augmentation.
    }
    \label{chap4/fig:app_augmentation_curves}
\end{figure}


\subsection{More robustness results}\label{chap4/app:robustness}

We plot the detection accuracy against the strength of multiple augmentations in Fig.~\ref{chap4/fig:app_augmentation_curves}. 
AudioSeal outperforms WavMark for most augmentations at the same strength.
However, for highpass filters above our training range (500Hz) WavMark has a much better detection accuracy.
Our system's TF-loudness loss embeds the watermark where human speech carries the most energy, typically lower frequencies, due to auditory masking. 
This contrasts with WavMark, which places the watermark in higher frequency bands.
Embedding the watermark in lower frequencies is advantageous. 
For example, speech remains audible with a lowpass filter at 1500 Hz, but not with a highpass filter at the same frequency. 
This difference is measurable with PESQ in relation to the original audio, making it more beneficial to be robust against a lowpass filter at a 1500 Hz cut-off than a highpass filter at the same cut-off:

\begin{center}
    \footnotesize
    \begin{tabular}{cccc}
        Filter Type & PESQ & AudioSeal & WavMark \\
        \midrule
        Highpass 1500Hz & 1.85 \xmarkg & 0.7 & 1.0 \\
        Lowpass 1500Hz & 2.93 \cmarkg & 1.0 & 0.7 \\
    \end{tabular}
\end{center}
