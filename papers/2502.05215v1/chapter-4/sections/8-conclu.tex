\section{Conclusion}

In this chapter we introduce a proactive method for the detection, localization, and attribution of AI-generated speech. 
AudioSeal redesigns audio watermarking to be specific to localized detection rather than data hiding. 
This removes the dependency on slow brute force algorithms, traditionally used to encode and decode audio watermarks.

A key advantage of AudioSeal is its practicality. 
Compared to the previous~\autoref{chapter:stable-signature}, it is post-hoc, meaning it can be applied to any existing audio content without the need to retrain the generator.
It stands as a ready-to-deploy solution for watermarking in voice synthesis APIs (or to authenticate real speech).
This is important for large-scale content provenance on social media and for detecting and eliminating incidents, enabling swift action on instances like the US voters' deepfake case~\citep{murphy2024biden}, long before they spread.
Note that \citet{san2024latent} introduce the in-model counterpart of this chapter.
The authors show how to use AudioSeal to watermark the weights of an audio autoregressive model~\citep{copet2023simple} through a specific watermarking of its training data, although it is less practical and less robust than post-hoc watermarking.

Two key limitations are worth mentioning.
First, the \pval\ of the watermark detection is not mathematically grounded, which makes it hard to scale to extremely low FPRs -- as done in previous and next chapters -- and to provide a theoretical guarantee of the watermark's presence.
Second, the watermark is still brittle to adversarial attacks (\eg, using new neural audio compression algorithms).
Therefore, AudioSeal should be seen as a filtering tool to flag suspicious content, rather than a definitive proof of fakeness  or authenticity.

