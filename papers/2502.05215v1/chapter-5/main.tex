
\chapter{Three Bricks to Consolidate Watermarks for Large Language Models}\label{chapter:three-bricks}

This chapter is based on the paper \fullcite{fernandez2023three}.

Discerning between generated and natural texts is increasingly challenging due to the rapid progress of large language models (LLMs).
In this context, watermarking again emerges as a promising technique for ascribing text to a specific generative model. 
It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection.
This research consolidates watermarks for large language models based on three theoretical and empirical considerations. 
First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\text{-6}}$). 
Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability.
Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking. 
Code is available at \url{github.com/facebookresearch/three_bricks}.

\newpage
\input{chapter-5/sections/1-intro.tex}
\input{chapter-5/sections/2-background.tex}
\input{chapter-5/sections/3-stats.tex}
\input{chapter-5/sections/4-expes.tex}
\input{chapter-5/sections/5-extension.tex}
\input{chapter-5/sections/8-conclusion.tex}
