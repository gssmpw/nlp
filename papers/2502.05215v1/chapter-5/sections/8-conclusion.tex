\section{Conclusion}

This chapter offers theoretical and empirical insights that were kept aside from the literature on watermarks for LLMs.
Namely, existing methods resort to statistical tests which are biased, delivering incorrect false positive rates.
This is fixed with grounded statistical tests and a revised scoring strategy.
We additionally introduce evaluation setups, and detection schemes to consolidate watermarks for LLMs.

Overall, LLM watermarking seems to be both reliable and practical. 
It already holds many promises as a technique for identifying and tracing LLM outputs, while being relatively new in the context of language models. 
However open questions remain, such as how to adapt watermarks for more complex sampling schemes -- \eg, beam search as in \citep{kirchenbauer2023watermark} -- since generation yields significantly better quality with these methods; how to adapt watermarking to more complex tasks like math or code generation, where the outputs are structured and verifiable; and how to make watermarking more robust to adversarial attacks, which are still a major threat to the integrity of the watermarking process; etc.
