
\section{Introduction}

The misuse of Large Language Models (LLMs) like ChatGPT~\citep{chatgpt2022}, Clau\-de~\citep{claude}, or the open-sourced Llama~\citep{touvron2023llama} may become a threat as their availability and capabilities expand~\citep{weidinger2022taxonomy, crothers2022machine, cardenuto2023age}.
LLMs might help generate fake news by reducing costs to spread disinformation at scale~\citep{kertysova2018artificial, kreps2022all}, with a potential impact on public opinion and democratic outcomes~\citep{kuvsen2018politics}.
They could help impersonate people, facilitate scams~\citep{ryan2023junk}, or make student assessments impossible.
In this context, it is crucial to enforce fair and responsible usage through regulations and technical means.

On the one hand, similarly to image or speech generation, detecting outputs of LLMs with passive forensics is difficult because generated texts are hardly distinguishable from real ones, be it for humans or algorithms~\citep{ippolito2019automatic, mitchell2023detectgpt}.
On the other hand, watermarking text is more challenging than continuous data like images or audios due to the discrete nature of words/tokens, and no end-to-end solution like the ones presented in Sec.~\ref{chap0/sec:deep learning-watermarking} or in Chap.~\ref{chapter:audioseal} provides convincing results.
Therefore, watermarking for text has been less explored, until the seminal works of~\citet{kirchenbauer2023watermark} and \citet{aaronson2023watermarking} which change the generation process to ascribe text to a specific LLM~\citep{aaronson2023watermarking,kirchenbauer2023watermark,kirchenbauer2023reliability,christ2023undetectable}.
In this case, watermarking either alters the sample generation process~\citep{aaronson2023watermarking,christ2023undetectable} or changes the probability distribution of the generated tokens~\citep{kirchenbauer2023watermark, zhao2023provable}, to leave an imperceptible trace in the generated text.
These works then describe a detection mechanism analyzing the generated tokens to see if their distribution follows the one induced by the watermark. 

We introduce three contributions to consolidate this young literature, one for each of the following paragraphs and sections.
Each part can be read independently:
\begin{itemize}
    \item 
    False positives can have serious consequences in contexts where the integrity of results are essential, such as falsely accusing a student of cheating in an exam.
    However, current approaches focus on the True Positive Rate (TPR) rather than on False Positive Rate (FPR).
    The FPR has never been empirically checked at interesting scales (with more than 1k negative examples).
    Our large-scale experiments reveal that hypotheses of previous works do not hold and that their detection thresholds largely underestimate the false positives at low FPR.
    This work provides grounded statistical tests that theoretically guarantee FPR and accurate \pval\ in real-world regimes.
    We validate them empirically and show that they provide a close-to-perfect control of the FPR, even at low values ($<10^{-6}$).
    \item
    We compare the watermarking methods on traditional Natural Language Processing (NLP) benchmarks. 
    Indeed, current watermark evaluation mainly considers the deviation from the original LLM distribution, for example using perplexity or similarity scores.
    This is in contrast with the LLM literature, where models are rather evaluated on their effective usefulness, \eg, free-form completion tasks such as question answering.
    Such evaluations are much more informative on the actual abilities of the model when used on downstream tasks.
    \item
    We expand these algorithms to advanced detection schemes.
    When access to the LLM is possible at detection time, we provide optimal statistical tests.
    We also investigate multi-bit watermarking (hiding binary messages as watermarks) when current approaches only tackle zero-bit watermarking.
    This allows not only to determine whether the text was generated by the watermarked LLM, but also to identify which version of the model generated it.
\end{itemize}

