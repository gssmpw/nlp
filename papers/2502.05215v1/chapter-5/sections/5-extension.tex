


\section{Advanced detection schemes}
This section introduces improvements to the detection schemes of Sec.~\ref{chap5/sec:stats}.
Namely, it develops a statistical test when access to the LLM is granted, as well as multi-bit decoding.


\subsection{Neyman-Pearson and simplified score function} 
The following is specific for the scheme of~\citet{aaronson2023watermarking} -- a similar work may be conducted with the one of~\citet{kirchenbauer2023reliability}.
Under $\H_0$, we have $\vec{r}_v\sim\mathcal{U}_{[0,1]}$, whereas $\vec{r}_v\sim Beta(1/p_v,1)$ under $\H_1$ (see Corollary~\eqref{chap5/eq:Coro} in App.~\ref{chap5/app:aaronson_prob}). 
The optimal Neyman-Pearson score function is thus:
\begin{equation*}
    s_T = \sum_{t=1}^{T} \ln\frac{f_{\H_1}(\vec{r}_{x^{(t)}})}{f_{\H_0}(\vec{r}_{x^{(t)}})} = \sum_{t=1}^T \left(\frac{1}{\vec{p}_{x^{(t)}}}-1\right)\ln(\vec{r}_{x^{(t)}})+A
\end{equation*}
where $A$ is a constant that does not depend on $\vec{r}$ and can thus be discarded. 
There are two drawbacks: (1) detection needs the LLM to compute $\vec{p}_{x^{(t)}}$, (2) there is no close-form formula for the p-value.  

This last point may be fixed by resorting to a Chernoff bound, yet without guarantee on its tightness:
$\text{p-value}(s) \leq e^{\sum_t \ln\frac{\lambda_t}{\lambda_t + c} -cs}$,
with $c$ solution of $\sum_t (c+\lambda_t)^{-1}=-s$ and $\lambda_t = p_{x^{(t)}} / (1-p_{x^{(t)}})$.
Experiments show that this detection yields extremely low \pval\ for watermarked text, but they are fragile: any attack increases them to the level of the original detection scheme~\eqref{chap5/eq:score-aaronson}, or even higher because generated logits are sensitive to the overall LLM context. 
An alternative is to remove weighting:
\begin{equation}
 s_T = \sum_{t=1}^T \ln\left(\vec{r}_{x^{(t)}}\right),
 \label{chap5/eq:Detection2}
\end{equation}
whose \pval\ is given by: $\text{p-value}(s) = \frac{\gamma(T,-s)}{\Gamma(T)}$.
In our experiments, this score function does not match the original detection presented in~\citep{aaronson2023watermarking}.


\subsection{Multi-bit watermarking}

       
    


\paragraph*{Theory.} It is rather easy to turn a zero-bit watermarking scheme into multi-bit watermarking, by associating a secret key per message. 
The decoding runs detection with every key and the decoded message is the one associated to the key giving the lowest \pval\ $p$. 
The global \pval\ becomes $1-(1-p)^M$, where $M$ is the number of possible messages.

Running detection for $M$ keys is costly, since it requires $M$ generations of the secret vector.
This is solved by imposing that the secret vectors of the messages $m\in\{0,\ldots,M-1\}$ are crafted as circular shifts of $m$ indices of $\vec{r}=\vec{r}(0)$:
\begin{align*}
\vec{r}(m) &= \mathsf{CyclicShift}(\vec{r},m) \\
    &= \left( \vec{r}_m, \vec{r}_{m+1}, ..,\vec{r}_{d}, \vec{r}_{0}, ..,  \vec{r}_{m-1}  \right).
\end{align*}
Generating $\vec{r}$ as a $d$-dimensional vector, with $d\geq|\V|$, we are able to embed $M\leq d$ different messages, by keeping only the first $|\V|$ dimensions of each circularly-shifted vector. 
Thus, the number of messages may exceed the size of the token vocabulary $|\V|$.
One way is to choose $d =  \textrm{max}(M, |\V|)$.



Besides, the scoring functions~\eqref{chap5/eq:score-kirchenbauer}~\eqref{chap5/eq:score-aaronson}
may be rewritten as:
\begin{equation}
s_T(m) = \sum_{t=1}^T f\left(\vec{r}^{(t)}(m)\right)_{x^{(t)}}  ,
\end{equation}
where $f: \R^d \mapsto \R^d$ is a component-wise function, and $x^{(t)}$ is the selected token during detection. 
This represents the selection of $f\left(\vec{r}^{(t)}(m)\right)$ at position $x^{(t)}$.
From another point of view, if we shift $f\left(\vec{r}^{(t)}\right)$ by $x^{(t)}$, the score for $m=0$ would be its first component, $m=1$ its second one, etc.
We may also write:
\begin{equation}
\vec{S}_T = \sum_{t=1}^T \mathsf{CyclicShift}\left( f\left(\vec{r}^{(t)}\right), x^{(t)} \right) ,
\label{chap5/eq:DetectionMultibit}
\end{equation}
and the first $M$ components of $\vec{S}_T$ are the scores for each $m$.
As a side note, this is a particular case of the parallel computations introduced by~\citet{JAWS}.
The simplified algorithms are given in Alg.~\ref{chap5/alg:multi-bit-gen} and Alg.~\ref{chap5/alg:multi-bit-dec}. 

\noindent
\begin{minipage}{0.48\textwidth}
    \begin{algorithm}[H] \small
    \caption{Generation (one step)}
    \label{chap5/alg:multi-bit-gen}
    \begin{algorithmic}
        \State\hspace*{-0.3cm} \textbf{Requires}: {LLM, dimension $d$, watermark window $k$, message $m\in\{0,\ldots,M-1\}$} \\
        \State logits $\vec{\boldsymbol\ell} \gets \text{LLM} \left( x^{(-C)},\dots, x^{(-1)} \right)$
        \State seed $\gets \mathsf{Hash}(x^{(-k)},\dots, x^{(-1)})$
        \State $\vec{r} \gets \mathsf{RNG_{seed}}(d)$
        \State $\vec{r}(m) \gets \mathsf{CyclicShift}(\vec{r},m)$
        \State $x^{(0)} \gets \mathsf{Sample}(\vec{\boldsymbol\ell},\vec{r}(m)_{1,\dots,|\V|})$
    \end{algorithmic}
\end{algorithm}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
    \begin{algorithm}[H] \small
    \caption{Decoding/identification}
    \label{chap5/alg:multi-bit-dec}
    \begin{algorithmic}
        \State $\vec{S} \gets \vec{0}_d$
        \State{\textbf{for} $t \in \{ k+1, \dots, T\}$:}
            \State \quad seed $\gets \mathsf{Hash}(x^{(t-k)},\dots, x^{(t-1)})$
            \State \quad $\vec{r}^{(t)} \gets \mathsf{RNG_{seed}}(d)$
            \State \quad $\vec{S} \gets \vec{S} +  \mathsf{CyclicShift}(f(\vec{r}^{(t)}),x^{(t)})$
        \State $\vec{p} \gets \textrm{p-value}(\vec{S}_{1,\dots,M})$
        \State $m \gets \textrm{argmin}({\vec{p}}) $
        \State $p \gets 1 - (1 - \vec{p}_m)^M$
    \end{algorithmic}
    \end{algorithm}
\end{minipage}


\paragraph*{Experiments.} 
In a tracing scenario the message is the identifier of a user or a version of the model.
The goal is to decide if any user or model generated a given text (detection) and if so, which one (identification).
There are 3 types of error: \emph{false positive}: flag a vanilla text; \emph{false negative}: miss a watermarked text; \emph{false accusation}: flag a watermarked text but select the wrong identifier.


\begin{table}[t]
    \caption{Identification accuracy for tracing users by watermarking. 
    Sequences are between $4$ and $252$ tokens long, and $149$ on average.
    }
    \label{chap5/tab:identification}
    \centering
    \footnotesize
    \begin{tabular}{cl cccc}
        \toprule
        & & \multicolumn{4}{c}{Number of users $M$} \\
        \cmidrule{3-6}
        & Method & $10$ & $10^2$ & $10^3$ & $10^4$ \\ \midrule
        \multirow{2}{*}{FPR$=10^{-3}$} & [AK] \citep{aaronson2023watermarking}      & 0.80 & 0.72 & 0.67 & 0.62 \\
        & [KGW] \citep{kirchenbauer2023watermark}  & 0.84 & 0.77 & 0.73 & 0.68 \\ \midrule
        \multirow{2}{*}{FPR$=10^{-6}$} & [AK] \citep{aaronson2023watermarking}	      & 0.61 & 0.56 & 0.51 & 0.46 \\
        & [KGW] \citep{kirchenbauer2023watermark} 	                              & 0.69 & 0.64 & 0.59 & 0.55 \\
        \bottomrule
    \end{tabular}
\end{table}


We simulate $M'$=$1000$ users that generate $100$ watermarked texts each, using the Guanaco-7B model. 
Accuracy can then be extrapolated beyond the $M'$ identifiers by adding identifiers with no associated text, for a total of $M>M'$ users.
Text generation uses nucleus sampling with top-p at $0.95$.
For~\citep{kirchenbauer2023watermark}, we use $\delta=3.0$, $\gamma=1/4$ with temperature $\theta$ at $0.8$.
For~\citep{aaronson2023watermarking}, we use $\theta = 1.0$.
For both, the context width is $k=4$.
A text is deemed watermarked if the score is above a threshold set for a given \emph{global} FPR (see~\ref{chap5/sec:stats}).
Then, the source is identified as the user with the lowest p-value.

\autoref{chap5/tab:identification} shows that watermarking performance for identification is dissuasive enough. 
For example, among $10^5$ users, we successfully identify the source of a watermarked text 50\% of the time while maintaining an FPR of $10^{-6}$ (as long as the text is not attacked).
At this scale, the false accusation rate is zero (no wrong identification once we flag a generated text) because the threshold is set high to avoid FPs, making false accusations unlikely. 
The identification accuracy decreases when $M$ increases, because the threshold required to avoid FPs gets higher.
In a nutshell, by giving the possibility to encode several messages, we trade some accuracy of detection against the ability to identify users.
