\section{Technical background and notations}\label{chap5/sec:background}

We hereunder re-introduce the concepts of the LLM watermarking techniques used later in the chapter and explained in details in \autoref{chap0/sec:llm_wm}.
More specifically, we focus on the two most popular methods~\citep{aaronson2023watermarking, kirchenbauer2023watermark} that modify the LLM decoding by hashing a watermark window.
See \autoref{chapter:related-work} for a broader overview of the detection and watermarking methods for text.


We consider a decoder-only LLM that takes as input a context $\left( x^{(-C)}, ..., x^{(-1)} \right)\in \V^C$, $\V$ being the vocabulary of the model, and outputs a vector of logits $\logit\in \R^{|\V|}$.
$\logit$ is transformed into the probability distribution $\mathbf{p}=\text{softmax}(\logit / \temperature ) \in [0,1]^{|\V|}$, where $\temperature$ is a temperature.
The text is generated by sampling the next token $x^{(0)}$ from this distribution $\Prob \left( . \mid x^{(-C)},\dots, x^{(-1)} \right)$, then appending it to the context, and repeating the process.

The \emph{watermark embedding} alters the logit vector $\logit$ or the sampling procedure depending on a secret-key $\sk$.
The output of a secret-key cryptographic function hashes $k$ previous tokens $\left(x^{(-k)},\dots, x^{(-1)} \right)$ (the watermark window) and the secret-key $\sk$.
It serves as a seed for a random number generator (RNG), that influences the choice of the next token $x^{(0)}$.
\begin{itemize}
    \item \citet{kirchenbauer2023watermark} use the RNG to partition the vocabulary $\V$ into a greenlist $\G$ and a redlist $\bar{\G}$, where $\G$ contains a proportion $\gamma$ of the vocabulary.
    The logit of each token in the greenlist is incremented by a value $\delta>0$, and the sampling proceeds as usual.
    \item \citet{aaronson2023watermarking} use the RNG to sample a vector $\vec{r}\in[0,1]^{|\V|}$.
    The next token is chosen as $x^{(0)} = \arg \max_{v \in \V } \vec{r}_v^{1/\vec{p}_v}$.
\end{itemize}
For both methods we can trade off generation quality against robustness by varying the watermarking strength.
In~\citep{kirchenbauer2023watermark}, increasing the $\delta$ parameter increases the generation of green tokens at the risk of including unlikely tokens.
In~\citep{aaronson2023watermarking}, increasing the temperature $\temperature$ has the same effect, since it flattens the probability vector, thus diminishing the relative importance of $\vec{p}_v$ over $\vec{r}_v$.


The \emph{watermark scoring} tokenizes the text and, for a text of $T$ tokens, computes a score $s_T$  based on the frequency or characteristics of certain tokens.
\begin{itemize}
    \item For \citet{kirchenbauer2023watermark}, the score is the number of greenlist tokens:
    \begin{equation}
        s_T = \sum_{t=k+1}^T \mathds{1} \left(x^{(t)}\in\G^{(t)}\right),
    \end{equation}\label{chap5/eq:score-kirchenbauer}
    where $x^{(t)}$ and $\G^{(t)}$ are the $t^{\textrm{th}}$ token and its associated partition.
    \item For \citet{aaronson2023watermarking}, the score is:
    \begin{equation}
        s_T=-\sum_{t=k+1}^T \ln \left(1-\vec{r}^{(t)}_{x^{(t)}}\right),
    \end{equation}\label{chap5/eq:score-aaronson}
    where $\vec{r}^{(t)}_{x^{(t)}}$ is the value of the secret vector corresponding to the token $x^{(t)}$.
\end{itemize}
Note that $\G^{(t)}$ and $\vec{r}^{(t)}$ depend on the $k$ tokens that precede $x^{(t)}$ and the secret-key $\sk$.

The \emph{statistical hypothesis test} distinguishes between the hypothesis $\H_0$: ``the text is natural'' and $\H_1$: ``the text has been generated with watermark''.
It is based on a $Z$-test, which compares the observed score $s_T$ against its expected value under the hypothesis $\H_0$ (no watermark).
The $Z$-test is typically used for large sample sizes assuming a normal distribution under the null hypothesis thanks to the central limit theorem.  
The $Z$ statistic is computed as:
\begin{equation}
    Z = \frac{{s_T/T - \mu_0}}{{\sigma_0 / \sqrt{T}}},
    \label{chap5/eq:Z}
\end{equation}
where $\mu_0$ and $\sigma_0$ are the expected mean and standard deviation of the score per token under $\H_0$.
A \pval\ is then calculated to determine the likelihood of observing a score as extreme as $s_T$ under $\H_0$:
\begin{equation}
    \text{p-value}(z) = \Prob(Z \geq z \mid \H_0) = 1 - \Phi(z),
    \label{chap5/eq:pvalue}
\end{equation}
where $\Phi$ is the cumulative distribution function of the normal distribution. 
Texts are flagged as watermarked if the \pval\ is less than a fixed false positive rate (FPR).













 












