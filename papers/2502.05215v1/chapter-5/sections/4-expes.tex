
\section{Better watermarking evaluations}

\autoref{chap5/sec:robustness-analysis} evaluates the detection with the revised statistical tests and smaller FPRs than the previous literature. 
\autoref{chap5/sec:free-form} shows the impact on NLP benchmarks.

\subsection{Robustness analysis}
\label{chap5/sec:robustness-analysis}

\begin{table}[t!]
    \centering
    \caption{
    Robustness analysis of the watermarks, with rectified statistical tests.
    We report the TPR@FPR=$10^{-5}$ and the S-BERT scores over $10\times 1$k completions, for different hyperparameters controlling the strength of the watermark 
    ($\delta$ in \citep{kirchenbauer2023watermark} and $\temperature$ in \citep{aaronson2023watermarking} - see Sec.~\ref{chap5/sec:background}).
    The `TPR aug.' is the TPR when texts are attacked before detection by randomly replacing tokens with probability 0.3.
    }
    \label{chap5/tab:robustness}
    \footnotesize
    \begin{tabular}{rl *{4}{p{1cm}} @{\hspace{0.5cm}} *{4}{p{1cm}}}
        \toprule
        & & \multicolumn{4}{c}{\citep{aaronson2023watermarking}} &  \multicolumn{4}{c}{\citep{kirchenbauer2023watermark}}  \\
        $k$ & Metric & $\temperature:$ 0.8 & 0.9 & 1.0 & 1.1 & $\delta:$ 1.0 & 2.0 & 3.0 & 4.0 \\
        \cmidrule(rr){3-6} \cmidrule(rr){7-10}
        \multirow{3}{*}{$0$} 
            & S-BERT    & 0.60 & 0.56 & 0.52 & 0.44 & 0.63 & 0.61 & 0.57 & 0.50 \\
            & TPR       & 0.20 & 0.31 & 0.42 & 0.51 & 0.00 & 0.16 & 0.58 & 0.70 \\
            & TPR aug.  & 0.04 & 0.06 & 0.09 & 0.10 & 0.00 & 0.02 & 0.20 & 0.39 \\[4pt]
        \multirow{3}{*}{$1$} 
            & S-BERT    & 0.62 & 0.61 & 0.59 & 0.55 & 0.63 & 0.62 & 0.60 & 0.56 \\
            & TPR       & 0.35 & 0.51 & 0.66 & 0.77 & 0.02 & 0.41 & 0.77 & 0.88 \\
            & TPR aug.  & 0.04 & 0.10 & 0.20 & 0.36 & 0.00 & 0.05 & 0.30 & 0.58 \\[4pt]
        \multirow{3}{*}{$4$} 
            & S-BERT    & 0.62 & 0.62 & 0.61 & 0.59 & 0.62 & 0.62 & 0.60 & 0.57 \\
            & TPR       & 0.43 & 0.59 & 0.71 & 0.80 & 0.02 & 0.44 & 0.76 & 0.88 \\
            & TPR aug.  & 0.01 & 0.02 & 0.06 & 0.18 & 0.00 & 0.00 & 0.03 & 0.14 \\
        \bottomrule
    \end{tabular}
\end{table}   





We now compare watermarking methods by analyzing the TPR when detecting watermarked texts.
For detection, we employ the previous statistical tests and scoring strategy.
They enable precise control over the FPR and therefore to operate at operating-points not yet seen in the literature.
More specifically we flag a text as watermarked if its \pval\ is lower than $10^{-5}$ ensuring an FPR=$10^{-5}$.
We prompt Guanaco-7-B~\citep{dettmers2023qlora}, an instruction fine-tuned version of Llama, with the first $1$k prompts from the Alpaca dataset~\citep{alpaca}.
For generation, we use top-$p$ sampling with $p=0.95$, and in the case of \citep{kirchenbauer2023watermark} a temperature $\theta =0.8$ and $\gamma=1/4$.
We simulate synonym attacks by randomly replacing tokens with probability $0.3$ (other attacks are studied in related work~\citep{kirchenbauer2023reliability}).

\autoref{chap5/tab:robustness} reports the TPR for different strength of the watermark (see Sec.~\ref{chap5/sec:background}), and the S-BERT~\citep{reimers2019sentence} similarity score between the generated texts with and without watermarking to measure the semantic distortion induced by the watermark. 
Results reveals different behaviors.
For instance, \citep{kirchenbauer2023watermark} has a finer control over the trade-off between watermark strength and quality.
Its TPR values ranges from 0.0 to 0.9, while \citep{aaronson2023watermarking} is more consistent but fails to achieve TPR higher than 0.8 even when the S-BERT score is degraded a lot.

The watermark context width also has a big influence. 
When $k$ is low, we observe that repetitions happen more often because the generation is easily biased towards certain repetitions of tokens.
It leads to average S-BERT scores below 0.5 and unusable completions.
On the other hand, low $k$ also makes the watermark more robust, especially for \citep{kirchenbauer2023watermark}.
It is also important to note that $k$ has an influence on the number of analyzed tokens since we only score tokens for which the $k+1$-tuple has not been seen before (see Sec.~\ref{chap5/sec:rect}).
If $k$ is high, almost all these tuples are new, while if $k$ is low, the chance of repeated tuples increases.
For instance in our case, the average number of scored tokens is around 100 for $k=0$, and 150 for $k=1$ and $k=4$.


\subsection{Impact on free-form generation tasks}
\label{chap5/sec:free-form}
Previous studies measure the impact on quality using distortion metrics such as perplexity or similarity score as done in Tab.~\ref{chap5/tab:robustness}.
However, such metrics are not informative of the utility of the model for downstream tasks~\citep{holtzman2019curious}, where the real interest of LLMs lies. 
Indeed, watermarking LLMs could be harmful for tasks that require very precise answers, like code or maths.
This section rather quantifies the impact on typical NLP benchmarks, in order to assess the practicality of watermarking.

LLMs are typically evaluated by comparing samples of plain generation to target references (free-form generation) or by comparing the likelihood of predefined options in a multiple choice question fashion. 
The latter makes little sense in the case of watermarking, which only affects sampling.
We therefore limit our evaluations to free-form generation tasks.
We use the evaluation setup of Llama:
1) Closed-book Question Answering (Natural Questions~\citep{kwiatkowski2019natural}, TriviaQA~\citep{joshi2017triviaqa}): we report the $5$-shot exact match performance;
2) Mathematical reasoning (MathQA~\citep{hendrycks2021measuring}, GSM8k~\citep{cobbe2021training}), we report exact match performance without majority voting;
3) Code generation (HumanEval~\citep{chen2021Evaluating}, MBPP~\citep{austin2021program}), we report the pass@1 scores.
For \citep{kirchenbauer2023watermark}, we shift logits with $\delta=1.0$ before greedy decoding.
For \citep{aaronson2023watermarking}, we use $\theta = 0.8$, apply top-p at $0.95$ to the probability vector, then apply the watermarked sampling.

\autoref{chap5/tab:bench-full} reports the performance of Llama models on the aforementioned benchmarks, with and without the watermark and for different window size $k$. 
The performance of the LLM is not significantly affected by watermarking. 
The approach of \cite{kirchenbauer2023watermark} is slightly more harmful than the one of \cite{aaronson2023watermarking}, but the difference w.r.t. the vanilla model is small.
Interestingly, this difference decreases as the size of the model increases: models with higher generation capabilities are less affected by watermarking. A possible explanation is that the global distribution of the larger models is better and thus more robust to small perturbations.
Overall, evaluating on downstream tasks points out that watermarking may introduce factual errors that are not well captured by perplexity or similarity scores.











\begin{table}[H]
    \centering
    \caption{ 
        Performances on free-form generation benchmarks when completion is done with watermarking.
        $k$ is the watermark context width. 
        We report results for methods: [AK]~\citep{aaronson2023watermarking} / [KGW]~\citep{kirchenbauer2023watermark}.
        ``-'' means no watermarking. 
    }
    \label{chap5/tab:bench-full}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{lllrrrrrrr}
    \toprule
     &  &  & GSM8K & Human Eval & MathQA & MBPP & NQ & TQA & Average \\
    Model & Method & $k$ &  &  &  &  &  &  &  \\
    \midrule
     \multirow[t]{15}{*}{7-B} 
     & None & - & 10.31 & 12.80 & 2.96 & 18.00 & 21.72 & 56.89 & 20.45 \\
     \cmidrule{2-10} 
     & [AK] & 0 & 10.54 & 12.80 & 3.00 & 18.00 & 21.77 & 56.88 & 20.50 \\
      &  & 1 & 10.31 & 12.80 & 2.88 & 18.20 & 21.75 & 56.87 & 20.47 \\
      &  & 2 & 10.31 & 12.80 & 2.94 & 18.00 & 21.75 & 56.86 & 20.44 \\
      &  & 4 & 10.39 & 12.80 & 2.98 & 17.80 & 21.80 & 56.88 & 20.44 \\
      &  & 8 & 10.46 & 12.80 & 2.90 & 18.20 & 21.75 & 56.85 & 20.49 \\
      \cmidrule{2-10} 
      & [KGW] & 0 & 9.63 & 12.80 & 2.20 & 16.20 & 20.06 & 55.09 & 19.33 \\
      &  & 1 & 11.14 & 9.76 & 2.82 & 16.00 & 19.50 & 55.30 & 19.09 \\
      &  & 2 & 11.07 & 6.71 & 2.62 & 16.00 & 20.44 & 55.07 & 18.65 \\
      &  & 4 & 10.77 & 9.15 & 2.76 & 16.40 & 20.17 & 55.14 & 19.06 \\
      &  & 8 & 11.37 & 11.59 & 2.90 & 16.40 & 20.66 & 55.36 & 19.71 \\
    \midrule
    \multirow[t]{15}{*}{13-B} 
    & None & - & 17.21 & 15.24 & 4.30 & 23.00 & 28.17 & 63.60 & 25.25 \\
    \cmidrule{2-10} 
    & [AK] & 0 & 17.29 & 15.24 & 4.24 & 22.80 & 28.17 & 63.60 & 25.22 \\
     &  & 1 & 17.21 & 15.24 & 4.30 & 22.80 & 28.20 & 63.61 & 25.23 \\
     &  & 2 & 17.51 & 15.24 & 4.20 & 22.80 & 28.20 & 63.59 & 25.26 \\
     &  & 4 & 17.21 & 15.24 & 4.20 & 22.60 & 28.20 & 63.63 & 25.18 \\
     &  & 8 & 17.21 & 15.24 & 4.22 & 22.80 & 28.20 & 63.62 & 25.22 \\
     \cmidrule{2-10} 
     & [KGW] & 0 & 14.33 & 14.02 & 3.04 & 20.80 & 24.32 & 62.13 & 23.11 \\
     &  & 1 & 17.29 & 14.63 & 3.62 & 21.20 & 25.12 & 62.23 & 24.02 \\
     &  & 2 & 16.45 & 11.59 & 3.54 & 20.60 & 25.54 & 62.44 & 23.36 \\
     &  & 4 & 16.76 & 15.85 & 4.08 & 21.20 & 24.49 & 62.24 & 24.10 \\
     &  & 8 & 17.29 & 14.63 & 3.68 & 21.00 & 25.46 & 62.17 & 24.04 \\
     \midrule
    \multirow[t]{14}{*}{30-B} 
     & None & - & 35.10 & 20.12 & 6.80 & 29.80 & 33.55 & 70.00 & 32.56 \\
     \cmidrule{2-10} 
     & [AK] & 0 & 35.48 & 20.12 & 6.88 & 29.80 & 33.52 & 69.98 & 32.63 \\
     & & 1 & 35.33 & 20.73 & 6.88 & 29.60 & 33.52 & 70.03 & 32.68 \\
     & & 2 & 35.33 & 20.73 & 6.94 & 30.00 & 33.49 & 70.00 & 32.75 \\
     & & 4 & 35.10 & 20.12 & 6.90 & 29.80 & 33.49 & 70.01 & 32.57 \\
     & & 8 & 35.33 & 20.73 & 6.94 & 30.00 & 33.52 & 70.01 & 32.75 \\
     \cmidrule{2-10} 
     & [KGW] & 0 & 31.84 & 21.95 & 6.88 & 28.40 & 31.66 & 69.03 & 31.63 \\
     &  & 1 & 35.56 & 20.73 & 7.54 & 28.80 & 31.58 & 68.98 & 32.20 \\
     &  & 2 & 33.21 & 17.07 & 6.48 & 27.40 & 31.83 & 69.44 & 30.91 \\
     &  & 4 & 34.12 & 22.56 & 6.96 & 28.80 & 31.55 & 68.74 & 32.12 \\
     &  & 8 & 34.95 & 20.12 & 7.42 & 27.20 & 32.08 & 69.31 & 31.85 \\
    \bottomrule
    \end{tabular}
    }
\end{table}





