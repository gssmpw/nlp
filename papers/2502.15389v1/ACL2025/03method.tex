\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\textwidth]{images/method_visual-crop.pdf}
    \caption{Process of Visual Attention Evaluation.}
    \label{img1}
\end{figure}

\subsection{API Prompting}
API Prompting~(sketched in Figure~\ref{api}) is a Visual Prompting method that highlights important parts in an image using a Visual Attention Heatmap derived from a Vision-Language Model~\citep{api}. The Attribution Map, representing the contribution of image tokens to model outputs, is extracted from a VLM (referred to as Heatmap VLM or H-VLM), convolved, resized to match the image size, and then overlaid on the original image.

Following the study by \citet{api}, Vision-Transformer-based CLIP and LLaVA are employed as Heatmap VLMs. The methods for extracting Visual Attention Attribution Maps from each model are described below.

\paragraph{CLIP Attribution Map}
CLIP computes similarity between text and image representations, and the Attribution Map \( \Psi \) is obtained by decomposing the similarity function \( \text{sim}(\hat{I}, \hat{T}) \). %The image representation \( \hat{I} \) is expressed as:
% \begin{equation}
%    \begin{aligned}
%        \hat{I} &= \mathcal{L}([Z^{0}_{\text{cls}}])
%         + \sum_{\ell=1}^{L} \mathcal{L}([\text{MSA}^{\ell}(Z^{\ell-1})]_{\text{cls}}) \\
%        &\quad + \sum_{\ell=1}^{L} \mathcal{L}([\text{MLP}^{\ell}(\hat{Z}^{\ell})]_{\text{cls}}).
%    \end{aligned}
% \end{equation}

Since later MSA layers greatly impact image representation~\citep{clipdec}, the similarity function is approximated as:
\begin{equation}
   \begin{aligned}
\text{sim}(\hat{I}, \hat{T}) \approx \text{sim}\left(\sum_
{\ell=L'}^{L} \mathcal{L}(\text{MSA}^{\ell}([Z^{\ell-1}]))_{\text{cls}}, \hat{T}\right).
\end{aligned}
\end{equation}
To filter out irrelevant regions, a complementary Attribution Map \( \Psi^{\text{comp}} \) is introduced:
\begin{equation}
   \begin{aligned}
   \Psi_{i,j}^{\text{comp}} &\triangleq 1 - \text{sim}(\mathcal{L}(Z_{t}^{L}), \hat{T}), \\
&\quad \text{where} \quad t = 1 + j + P \cdot (i - 1).
\end{aligned}
\end{equation}
Combining both maps, the final CLIP Attribution Map is defined as:
\begin{equation}
   \begin{aligned}
   \Psi = \Psi^{\text{cls}} + \Psi^{\text{comp}} - \Psi^{\text{comp}} \odot \Psi^{\text{cls}}.
\end{aligned}
\end{equation}

\paragraph{LLaVA Attribution Map}
LLaVA can provide an Attribution Map \( \Psi \) using Multi-Head Self-Attention (MSA) weights between output text tokens and image tokens. The Attribution Map is computed by averaging over all output tokens and attention heads:
\begin{equation}
   \begin{aligned}
\Psi_{i,j} &\triangleq \frac{1}{MH} 
  \sum_{m=1}^{M} 
  \sum_{h=1}^{H} 
  A_{m,t}^{(\bar{L},h)}, \\
&\quad \text{where} \quad 
t = j + P \cdot (i - 1).
\end{aligned}
\end{equation}
Here, \(M\) is the number of output tokens, \(H\) is the number of attention heads, \(P\) is the number of patches per image side, and \(A^{(\bar{L},h)}\) represents cross-attention between output text and image tokens at layer \(\bar{L}\) and attention head \(h\).

\subsection{Background Role Examination}
To assess the necessity of background information for object recognition, ground truth segmentation data is used as a Heatmap during API Prompting and the accuracy of output is evaluated (hereafter referred to as API - Seg.). Binary segmentation masks, overlaid in gray are input into VLMs to evaluate their impact on output accuracy. If POPE response accuracy remains unchanged, background information is deemed unnecessary.

\subsection{Minimum Cutoff}
Minimum cutoff redefines the minimum value in segmentation or Visual Attention Heatmap based on a threshold. Since a threshold of 0.5 showed improvement in Table~\ref{table2}, values below 0.5 in the cutoff are replaced with 0.5, refining segmentation granularity.

\begin{table*}[t]
\centering
\begin{tabular}{lllrrrrr}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{Prompting} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{TNR} & \textbf{F1} %& \textbf{Yes (\%)} 
\\ \cmidrule(lr){1-8}
\multirow{7}{*}{\textbf{MSCOCO}} & \multirow{7}{*}{LLaVA} & w/o prpt. & 86.23 & 84.21  & 89.19 & 83.27 & 86.63 \\ %& 52.96 \\
& & API~(CLIP) & \ensuremath{\blacktriangle}86.52 & \ensuremath{\blacktriangle}84.78 & \ensuremath{\triangledown}89.02 & \ensuremath{\blacktriangle}84.02 & \ensuremath{\blacktriangle}86.85 \\ %& \ensuremath{\blacktriangle}52.50  \\
& & API~(CLIP) w Cutoff & \ensuremath{\blacktriangle}88.59 & \ensuremath{\blacktriangle}85.84 & \ensuremath{\blacktriangle}92.43 & \ensuremath{\blacktriangle}84.75 & \ensuremath{\blacktriangle}89.01 \\
 & & API~(LLaVA) & \ensuremath{\triangledown}86.11 & \ensuremath{\blacktriangle}84.72 & \ensuremath{\triangledown}88.12 & \ensuremath{\blacktriangle}84.10 & \ensuremath{\triangledown}86.39 \\ %&  \ensuremath{\blacktriangle}52.01 \\
 & & API~(LLaVA) w Cutoff & \ensuremath{\blacktriangle}87.98 & \ensuremath{\blacktriangle}85.10 & \ensuremath{\blacktriangle}92.09 & \ensuremath{\blacktriangle}83.88 & \ensuremath{\blacktriangle}88.46 \\
  & & API - Seg. & - & - & \ensuremath{\triangledown}71.78 & - &  - \\ %& - \\
  & & API - Seg. w Cutoff & - & - & \ensuremath{\blacktriangle}89.24 & - &  - \\ %& - \\
 \bottomrule
\end{tabular}
\caption{POPE results on MSCOCO datasets with API Prompting.}
\label{table1}
\end{table*}

\begin{table*}[h!]
\centering
\begin{tabular}{llrrrr}
\toprule
%\toprule
\multirow{2}{*}{\textbf{H-VLM}} & \multirow{2}{*}{\textbf{Output}} & \multicolumn{4}{c}{\textbf{Visual Attention Alignment}} \\ %\cline{2-9} 
& & \textbf{Prec.} & \textbf{Rec.}  & \textbf{IoU}    & \textbf{MSE}   \\ 
%\cmidrule(lr){1-9}
\cmidrule(lr){1-6}
%Overall   & 0.1270   & 0.8318 & 0.1092 & 0.2920  & 0.1010   & 0.6523 & 0.0767 & 0.1337 \\ \hline
%\multirow{3}{*}{CLIP} & - & 12.70 & 83.18 & 10.92 & 29.20 \\
\multirow{2}{*}{CLIP}& Correct~(87\%)  &\ensuremath{\blacktriangle}13.52   & \ensuremath{\blacktriangle}83.94 & \ensuremath{\blacktriangle}11.68 & \ensuremath{\blacktriangle}28.46  \\
& Incorrect~(13\%) & 6.09   & 77.02 & 4.78 & 35.22 \\
%\multirow{3}{*}{LLaVA} & - & 10.10 & 65.23 & 7.67 & 13.37 \\
\multirow{2}{*}{LLaVA}& Correct~(86\%)  & \ensuremath{\blacktriangle}10.62  & 64.62 & \ensuremath{\blacktriangle}8.10 & 13.60 \\ 
& Incorrect~(14\%) & 6.17   & \ensuremath{\blacktriangle}69.78 & 4.44 & \ensuremath{\blacktriangle}11.60 \\
\bottomrule
%\bottomrule
\end{tabular}
\caption{Alignment of CLIP/LLaVA Visual Attention.}
\label{table3}
\end{table*}

\subsection{Evaluation of Visual Attention Alignment}
To assess how well visual attention focuses on target objects, Precision, Recall, Intersection over Union~(IoU), and Mean Squared Error~(MSE) are computed between the object segmentation data and the Visual Attention Heatmap as depicted in Figure~\ref{img1}. 
Visual Attention Heatmaps are converted into binary arrays using thresholds set to the average value of each heatmap.


% \begin{figure}[t]
%      \centering
%     \includegraphics[width=0.47\textwidth]{images/cutoff-crop.pdf}
%      \caption{Example of Cutoff Segmentation Annotation}    \label{clip}
%  \end{figure}