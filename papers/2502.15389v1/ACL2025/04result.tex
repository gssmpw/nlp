\subsection{Setting}
A total of 3,860 images from the MSCOCO dataset~\cite{mscoco} were used to generate visual attention heatmaps from CLIP and LLaVA. These heatmaps were overlaid onto the original images as black masks, and the masked images, along with POPE questions, were input into LLaVA to evaluate the outputs and assess visual attention.

The objects in each image were identified using MSCOCO labels, with three absent objects randomly selected from the remaining 80 labels. Segmentation data for the present objects was retrieved from MSCOCO and used as ground truth.

The API Prompting method followed the published implementation\footnote{\url{https://github.com/yu-rp/apiprompting}}.
CLIP API Prompting used a pre-trained CLIP ViT-L/14@336px model, while LLaVA API Prompting employed llava-v1.5-7b. Visual attention heatmaps were extracted from layer 22 for CLIP and layer 20 for LLaVA. A convolution with a kernel size of 3 was applied, and the maps were resized using the LANCZOS method.

POPE question responses were generated using llava-hf/llava-1.5-7b-hf (vLLM version) with a temperature of \texttt{0.8} and top\_p of \texttt{0.9}. The phrase “Answer Yes, No, or Not Sure” was appended to each question, and responses were classified based on the leading token.


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1.0\textwidth]{images/size2.pdf}
%     \caption{caption}
%     \label{img1}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=0.85\textwidth]{images/examples-s-crop.pdf}
%         \caption{Successful Cases}
%     \end{minipage}\hfill
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=1.0\textwidth]{images/examples-f-crop.pdf}
%         \caption{Failed Cases}
%     \end{minipage}
% \end{figure*}

\subsection{Result}
Table~\ref{table1} shows the results of API Prompting with CLIP, LLaVA, and ground truth segmentation, with and without Cutoff. Both CLIP and LLaVA API Prompting without Cutoff improved metrics such as True Negative Rate~(TNR), but the improvement remained below 1\%. Providing segmentation significantly reduced Recall. In contrast, API Prompting with Cutoff showed an improvement especially in Recall of approximately 3\%.

As shown in Table~\ref{table3}, when the output of API Prompting was correct, the alignment between the target object and visual attention was better, with a difference of approximately 5\%. Notably, the precision and IoU exhibited this trend across both Heatmap VLMs, whereas the recall and MSE improvements were observed only when using CLIP as the Heatmap VLM.

As Table~\ref{table5} summarized, in cases where the VLMs answered correctly without background information (upper section of the table), the accuracy of API Prompting without Cutoff was comparable to that of the w/o prompting condition. However, in cases where the model answered incorrectly without background information (lower section of the table), the accuracy significantly deteriorated. With Cutoff, especially in cases where the model initially answered incorrectly without background information, a substantial improvement in recall was observed compared to the w/o prompting condition.

