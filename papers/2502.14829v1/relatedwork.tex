\section{Background and Related Work}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
When CoT prompted, models exhibit better performance on complex multi-hop and arithmetic reasoning tasks \citep{zhou2023complex,fu2023complexity,sprague2024cotmath} compared to being prompted directly (no-CoT).
%
Chains of thought can be used as additional context where models can store results of intermediate hops, but they also provide additional compute irrespective of content \cite{pfau2024hidden,biran2024hopping}.
Verbalized reasoning steps are frequently hypothesized to be an accurate depiction of the models' internal reasoning process \citep{kojima2022large, fu2023specializing, sun2023recitation}. However, \textit{faithfulness} of CoTs should not be assumed despite how \textit{plausible} they might seem \citep{jacovi-goldberg-2020-towards,bao2024llms}.

\paragraph{Issues with NLE.} Natural language explanations such as CoTs exhibit a number of issues. %
%
They are frequently unreliable, yielding inconsistent answers after supposedly inconsequential perturbations \citep{camburu2020adversarial,lanham2023measuring, madsen2024self, sedova2024ambiguity}. %
Explanations provided by LMs can be non-causal \cite{bao2024llms}, not aligning with the generated answers. They are often not useful to humans \citep{joshi2023useful} and can contain factually incorrect or hallucinated information  \citep{kim2021presupposition,kim2023questionable, zheng2023does, peng2302check,zhang2024snowball}.
Most importantly, CoTs have been shown to misrepresent the true reasoning process of the LM \citep{turpin2023unfaithful,roger2023preventing}. \citeauthor{turpin2023unfaithful}\ show that LMs predictions can be biased by contextual shortcuts, the influence of which is not disclosed in the CoT.
In this work, we focus on verifying whether CoTs generated by LMs  reflect their \textit{parametric beliefs}, that is, if the generated reasoning chain is faithful with respect to the model parameters.  

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\linewidth]{figures/parametric_contextual_faithfulness.pdf}
    \caption{The distinction between contextual and parametric faithfulness. \textit{Contextual faithfulness} measures the effect of context perturbations on the prediction, while \textit{parametric faithfulness} measures whether verbalized reasoning corresponds to latent reasoning.}
    \label{fig:ctx_vs_param_ff}
\end{figure}

\paragraph{Contextual vs.\ Parameteric influence.}  
Prior work has recognized the discord between contextual and parametric influence on the outputs of LMs \citep{neeman-etal-2023-disentqa,bao2024llms}.
Prompting models with hypothetical or factually incorrect information causes them to change their otherwise consistently correct predictions \citep{kim2021presupposition,kim2023questionable,simhi2024distinguishing,minder2024controllable}, highlighting their high sensitivity to context tokens and confounding any conclusions drawn from contextual perturbations applied to reasoning steps.
The main issue with work investigating self-consistency is the possibility of the LM reconstructing information obfuscated by the contextual perturbation---despite the verbalized knowledge missing, this reasoning could still be retrieved from the latent space \citep{yang2024latently, deng2024explicit}.
To account for possible confounders, we only use information from generated CoTs to guide unlearning while generating predictions directly by no-CoT prompting, adequately disentangling contextual influence from the prediction.
%



\paragraph{Measuring Faithfulness.}
Various tests and metrics for quantifying faithfulness of free-text explanations in LMs have previously been proposed \citep{lanham2023measuring,bentham2024chain,atanasova2023faithfulness,siegel2024probabilities}.
By measuring properties such as sufficiency through simulatability or counterfactual interventions \citep{atanasova2023faithfulness,lanham2023measuring}, these studies quantify susceptibility of the models' predictions to changes in context or input.
Such approaches are valid \textit{only if} there is no direct causal link between the input and prediction that bypasses the explanation.
%
Experiments show that such structural causal models are rarely implemented by LMs \citep{bao2024llms}, confounding the conclusions drawn from \textit{contextual faithfulness} methods.
%
%
In our work, we analyze whether parametric perturbations that affect the generated CoT also affect the prediction, assessing \textit{parametric faithfulness} of individual causal links.
The closest to ours is the contemporaneous work of \citet{yao2024patching} which uses activation patching to measure causal effect of corrupting certain hidden states.
%

%

%
%

%
%
%
%

%
%
%
%

%

%

%
%
%
%
%
 %