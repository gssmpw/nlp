\section{Background and Related Work}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
When CoT prompted, models exhibit better performance on complex multi-hop and arithmetic reasoning tasks **Brown et al., "Language Models Play Higher-Education Baccalaureate Exam"** compared to being prompted directly (no-CoT).
%
Chains of thought can be used as additional context where models can store results of intermediate hops, but they also provide additional compute irrespective of content **Liu et al., "Chain-of-Thought Prompting Encourages Deep and Broader Thinking"**.
Verbalized reasoning steps are frequently hypothesized to be an accurate depiction of the models' internal reasoning process ____ . However, \textit{faithfulness} of CoTs should not be assumed despite how \textit{plausible} they might seem ____.

\paragraph{Issues with NLE.} Natural language explanations such as CoTs exhibit a number of issues. %
%
They are frequently unreliable, yielding inconsistent answers after supposedly inconsequential perturbations ____ . %
Explanations provided by LMs can be non-causal ____ , not aligning with the generated answers. They are often not useful to humans ____ and can contain factually incorrect or hallucinated information  ____.
Most importantly, CoTs have been shown to misrepresent the true reasoning process of the LM ____. ____\ show that LMs predictions can be biased by contextual shortcuts, the influence of which is not disclosed in the CoT.
In this work, we focus on verifying whether CoTs generated by LMs  reflect their \textit{parametric beliefs}, that is, if the generated reasoning chain is faithful with respect to the model parameters.  

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\linewidth]{figures/parametric_contextual_faithfulness.pdf}
    \caption{The distinction between contextual and parametric faithfulness. \textit{Contextual faithfulness} measures the effect of context perturbations on the prediction, while \textit{parametric faithfulness} measures whether verbalized reasoning corresponds to latent reasoning.}
    \label{fig:ctx_vs_param_ff}
\end{figure}

\paragraph{Contextual vs.\ Parameteric influence.}  
Prior work has recognized the discord between contextual and parametric influence on the outputs of LMs ____.
Prompting models with hypothetical or factually incorrect information causes them to change their otherwise consistently correct predictions ____ , highlighting their high sensitivity to context tokens and confounding any conclusions drawn from contextual perturbations applied to reasoning steps.
The main issue with work investigating self-consistency is the possibility of the LM reconstructing information obfuscated by the contextual perturbation---despite the verbalized knowledge missing, this reasoning could still be retrieved from the latent space ____.
To account for possible confounders, we only use information from generated CoTs to guide unlearning while generating predictions directly by no-CoT prompting, adequately disentangling contextual influence from the prediction.
%

\paragraph{Measuring Faithfulness.}
Various tests and metrics for quantifying faithfulness of free-text explanations in LMs have previously been proposed ____.
By measuring properties such as sufficiency through simulatability or counterfactual interventions ____ , these studies quantify susceptibility of the models' predictions to changes in context or input.
Such approaches are valid \textit{only if} there is no direct causal link between the input and prediction that bypasses the explanation.
%
Experiments show that such structural causal models are rarely implemented by LMs ____ , confounding the conclusions drawn from \textit{contextual faithfulness} methods.
%
%
In our work, we analyze whether parametric perturbations that affect the generated CoT also affect the prediction, assessing \textit{parametric faithfulness} of individual causal links.
The closest to ours is the contemporaneous work of **Jiang et al., "Peeking Inside Pioneers: A Survey on Causal Explanations"** which uses activation patching to measure causal effect of corrupting certain hidden states.
%

%

%
%

%
%
%
%

%
%
%
%

%

%

%
%
%
%
%
 %