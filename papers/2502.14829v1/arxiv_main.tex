%
% \pdfoutput=1
%

\documentclass[11pt]{article}

%
%
\usepackage[preprint]{acl} %

%

\usepackage{colortbl}

\usepackage{times}
\usepackage{dsfont}
\usepackage{latexsym}
\usepackage{amsmath}
%
\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{bm}
%
\usepackage{bbding}
\usepackage{mdframed}
\usepackage[compact]{titlesec}
\usepackage{booktabs}

%
\usepackage[T1]{fontenc}
%
%
%

\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{svg}


%
\usepackage[utf8]{inputenc}

%
%
%
\usepackage{microtype}

%
%
%
\usepackage{inconsolata}


\newcommand\framework{\textsc{fur}}
\newcommand\pff{\textsc{pff}}

\newcommand\metrichard{\textsc{ff-hard}}
\newcommand\metricsoft{\textsc{ff-soft}}
\newcommand\ms{\mathcal{M}^*}

\newcommand\todo[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\yb}[1]{{\color{green}[\small YB: #1]}}
\newcommand{\ana}[1]{{\color{purple}[\small AM: #1]}}
\definecolor{melon}{HTML}{F89E7B}
\newcommand{\martin}[1]{{\color{melon}[\small MT: #1]}}
\newcommand\sect[1]{\S\ref{#1}}

\definecolor{coralred}{rgb}{1.0, 0.25, 0.25}
\definecolor{palegreen}{rgb}{0.6, 0.98, 0.6}
\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}

\definecolor{mortargrey}{HTML}{595959}
\definecolor{macandcheese}{HTML}{FFBC79}

\title{Measuring Faithfulness of Chains of Thought\\ by Unlearning Reasoning Steps
} %

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\def\mystrut{\rule{0pt}{1.0\normalbaselineskip}}
\author{
\begin{tabular}{@{}c} %
Martin Tutek$^1$\quad Fateme Hashemi Chaleshtori$^2$ \quad Ana Marasovi\'{c}$^2$\quad Yonatan Belinkov$^1$ \\
\end{tabular}\\
$^1$Technion - Israel Institute of Technology\mystrut\quad
$^2$University of Utah\mystrut\quad \\
\fontsize{10}{12}{
\texttt{martin.tutek@gmail.com} \mystrut\quad \texttt{\{fateme.hashemi, ana.marasovic\}@utah.edu} \mystrut\quad \texttt{belinkov@technion.ac.il}
}
%
%
%
}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{document}
\maketitle

%
\begin{abstract}

%
%
%
When prompted to \textit{think step-by-step}, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction.
However, despite much work on CoT prompting, it is unclear  %
if CoT reasoning is faithful to the models' \textit{parameteric} beliefs.
We introduce a framework for measuring \textit{parametric faithfulness} of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (\framework{}), an instance of this framework.
\framework{} erases information contained in reasoning steps from model parameters. %
%
%
%
We perform experiments unlearning CoTs of four LMs prompted on four
%
 multi-choice question answering (MCQA) datasets.
Our experiments show that \framework{} is frequently able to change the underlying models' prediction by unlearning key steps, indicating when a CoT is parametrically faithful. 
Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning. Importantly, CoT steps identified as important by \framework{} do not align well with human notions of plausbility, emphasizing the need for specialized alignment.\footnote{Code available at \url{https://github.com/technion-cs-nlp/parametric-faithfulness}}
 %
%
\end{abstract}
 %
\section{Introduction}

\begin{figure}[t]
    \setlength{\belowcaptionskip}{-5pt}
    \centering
    \includegraphics[width=\linewidth]{figures/fig1_rev6}
    \caption{An illustration of \pff{} and \framework{}.
    In order to produce a parameter intervention, we first prompt the model $\mathcal{M}$ to produce an answer $y$ and reasoning chain (CoT). We then segment the reasoning chain and unlearn a single reasoning step from the model. The unlearned model $\ms{}$ is then prompted to produce an answer $y^*$. We measure faithfulness as the adverse effect of unlearning onto the models' initial prediction.}
    \label{fig:fig1}
\end{figure}

%
%
Language models (LMs) can perform various tasks accurately and verbalize \emph{some} reasoning via a so-called chain of thought (CoT) \cite{kojima2022large, wei2022chain}, even without specialized supervised training. 
CoT reasoning is emerging as a powerful technique for improving the performance of LMs in complex tasks \cite{o1,guo2025deepseek}.
It is not clear, however, whether the reasoning encoded in the CoT is a \textit{faithful} representation of the internal reasoning process of the model, casting doubts about the reliability of CoT as a window onto the model's `thought process'. 
%
%

%
%

Various works set out to explore CoT faithfulness by perturbing tokens within the CoT and observing whether the contextual corruptions affect model prediction  \citep{lanham2023measuring,bentham2024chain,chen2024counterfactual,madsen2024self}.
This setup is inherently flawed, as erasing steps from context does not remove knowledge from model parameters, and the model may still be able to reconstruct corrupted information when generating a prediction.
Such approaches of context perturbation actually measure \textit{self-consistency} or \textit{contextual faithfulness} rather than \textit{parametric faithfulness}, for which one would need to erase knowledge from parameters \citep{parcalabescu2023measuring}.

%
%
%
%
%
%
%
%
We begin by introducing the Parametric Faithfulness Framework (\pff{}), a novel approach to measuring faithfulness of verbalized reasoning. We define necessary components of instances of such a framework in two stages: (1) an \textit{intervention} on the model parameters; and (2) \emph{evaluating} parametric faithfulness. See components in \Cref{fig:fig1}.
\pff{} is a general framework that can be applied to different types of CoT and other free-text explanations. 

In this work, we propose an instance of \pff{} we call Faithfulness by Unlearning Reasoning steps (\framework{}), an unlearning-based approach to assessing CoT faithfulness. %
 \framework{} erases information encoded in the CoT from model parameters and assesses whether it affects the model prediction.
Concretely, we use NPO \citep{zhang2024npo}, a preference-optimization-based unlearning method as the intervention and propose two metrics of quantifying faithfulness of reasoning steps.
\metrichard{} quantifies whether the CoT as a whole is faithful, while \metricsoft{} identifies the most salient reasoning steps within the CoT.
Concretely, we (b) segment a CoT into steps, (c) independently unlearn knowledge encoded within each step from model parameters and (d) measure the effect of erased knowledge on the models' prediction (\Cref{fig:fig1}).
If the target step was successfully and precisely unlearned, and the models' prediction changed, the step \textit{faithfully} explains the models' underlying reasoning process. 
%
%


Through experimental evaluation on four LMs and four MCQA text reasoning datasets, we show we are able to perform valid interventions that successfully affect model predictions while not damaging the models' general capabilities. 
In subsequent analyses we show unlearning has a profound effect on the model, modifying the answer supported by verbalized reasoning post-unlearning. 
We also compare parametric faithfulness to plausibility via a human study, finding that humans do not consider steps identified as important by \framework{} plausible. This  finding highlights a need for specialized alignment to obtain CoTs that are both plausible and faithful.
%

The contributions of this work are as follows:
\vspace{-5pt}
\begin{enumerate}[noitemsep]
    \item We introduce \pff{}, a framework for measuring parametric faithfulness of LM reasoning.
    \item We instantiate \pff{} with \framework{} using NPO, a model unlearning method, and demonstrate its effectiveness on unlearning fine-grained reasoning steps.
    \item We introduce \metrichard{} and \metricsoft{}, metrics evaluating reasoning faithfulness, which can be applied to full chains or individual steps.
    \item We perform detailed analyses, including human and LLM-as-a-judge annotations, evaluating whether unlearning fundamentally changes the verbalized reasoning, and if steps identified as faithful are also plausible.
\end{enumerate}
 %
\section{Background and Related Work}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
When CoT prompted, models exhibit better performance on complex multi-hop and arithmetic reasoning tasks \citep{zhou2023complex,fu2023complexity,sprague2024cotmath} compared to being prompted directly (no-CoT).
%
Chains of thought can be used as additional context where models can store results of intermediate hops, but they also provide additional compute irrespective of content \cite{pfau2024hidden,biran2024hopping}.
Verbalized reasoning steps are frequently hypothesized to be an accurate depiction of the models' internal reasoning process \citep{kojima2022large, fu2023specializing, sun2023recitation}. However, \textit{faithfulness} of CoTs should not be assumed despite how \textit{plausible} they might seem \citep{jacovi-goldberg-2020-towards,bao2024llms}.

\paragraph{Issues with NLE.} Natural language explanations such as CoTs exhibit a number of issues. %
%
They are frequently unreliable, yielding inconsistent answers after supposedly inconsequential perturbations \citep{camburu2020adversarial,lanham2023measuring, madsen2024self, sedova2024ambiguity}. %
Explanations provided by LMs can be non-causal \cite{bao2024llms}, not aligning with the generated answers. They are often not useful to humans \citep{joshi2023useful} and can contain factually incorrect or hallucinated information  \citep{kim2021presupposition,kim2023questionable, zheng2023does, peng2302check,zhang2024snowball}.
Most importantly, CoTs have been shown to misrepresent the true reasoning process of the LM \citep{turpin2023unfaithful,roger2023preventing}. \citeauthor{turpin2023unfaithful}\ show that LMs predictions can be biased by contextual shortcuts, the influence of which is not disclosed in the CoT.
In this work, we focus on verifying whether CoTs generated by LMs  reflect their \textit{parametric beliefs}, that is, if the generated reasoning chain is faithful with respect to the model parameters.  

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\linewidth]{figures/parametric_contextual_faithfulness.pdf}
    \caption{The distinction between contextual and parametric faithfulness. \textit{Contextual faithfulness} measures the effect of context perturbations on the prediction, while \textit{parametric faithfulness} measures whether verbalized reasoning corresponds to latent reasoning.}
    \label{fig:ctx_vs_param_ff}
\end{figure}

\paragraph{Contextual vs.\ Parameteric influence.}  
Prior work has recognized the discord between contextual and parametric influence on the outputs of LMs \citep{neeman-etal-2023-disentqa,bao2024llms}.
Prompting models with hypothetical or factually incorrect information causes them to change their otherwise consistently correct predictions \citep{kim2021presupposition,kim2023questionable,simhi2024distinguishing,minder2024controllable}, highlighting their high sensitivity to context tokens and confounding any conclusions drawn from contextual perturbations applied to reasoning steps.
The main issue with work investigating self-consistency is the possibility of the LM reconstructing information obfuscated by the contextual perturbation---despite the verbalized knowledge missing, this reasoning could still be retrieved from the latent space \citep{yang2024latently, deng2024explicit}.
To account for possible confounders, we only use information from generated CoTs to guide unlearning while generating predictions directly by no-CoT prompting, adequately disentangling contextual influence from the prediction.
%



\paragraph{Measuring Faithfulness.}
Various tests and metrics for quantifying faithfulness of free-text explanations in LMs have previously been proposed \citep{lanham2023measuring,bentham2024chain,atanasova2023faithfulness,siegel2024probabilities}.
By measuring properties such as sufficiency through simulatability or counterfactual interventions \citep{atanasova2023faithfulness,lanham2023measuring}, these studies quantify susceptibility of the models' predictions to changes in context or input.
Such approaches are valid \textit{only if} there is no direct causal link between the input and prediction that bypasses the explanation.
%
Experiments show that such structural causal models are rarely implemented by LMs \citep{bao2024llms}, confounding the conclusions drawn from \textit{contextual faithfulness} methods.
%
%
In our work, we analyze whether parametric perturbations that affect the generated CoT also affect the prediction, assessing \textit{parametric faithfulness} of individual causal links.
The closest to ours is the contemporaneous work of \citet{yao2024patching} which uses activation patching to measure causal effect of corrupting certain hidden states.
%

%

%
%

%
%
%
%

%
%
%
%

%

%

%
%
%
%
%
 %
\section{\pff{}: A Framework for Measuring Parametric Faithfulness}
\label{sec:framework}

We introduce a framework for  measuring the faithfulness of generated reasoning, which we call \emph{parametric faithfulness}. %
This framework supports multiple ways to measure parametric faithfulness, and in \sect{sec:fur}, we propose one such way.

%
%
\paragraph{Motivation.}
A line of work has %
analyzed the %
sensitivity of models %
to perturbations applied to reasoning steps \citep[][\textit{inter alia}]{lanham2023measuring,bentham2024chain,chen2024counterfactual,madsen2024self} under the guise of \textit{faithfulness}.
%
While perturbations applied to generated reasoning remove information from \textit{context}, the model is still able to retrieve such information from its \textit{parameters} \citep{neeman-etal-2023-disentqa}. %
%
Perturbing the reasoning chain while maintaining model parameters fixed measures \textit{self-consistency} \citep{parcalabescu2023measuring}.
Self-consistency can be viewed as faithfulness of the model output with respect to the reasoning chain (\textit{contextual faithfulness}), but it does not reflect %
faithfulness of the reasoning chain with respect to model parameters, which we call \textit{parametric faithfulness}. See \Cref{fig:ctx_vs_param_ff} for a visualization of this distinction.
Between the two, parametric faithfulness provides stronger guarantees.
%
Models can recover information erased only from context, 
%
and introduced mistakes might make the model prioritize erroneous context in place of its obfuscated true reasoning.
While these confounders need not always dictate the models' output, in \textit{contextual faithfulness} they can never be explained away without quantifying the effect of parameters on the output. 
In other words, to measure parametric faithfulness, we have to \emph{intervene on model parameters}.
%

\paragraph{Framework.} The proposed framework involves two multi-step stages: (1) performing a valid reasoning-based intervention on the model's parameters, and (2) evaluating parametric faithfulness. %

The first stage begins by instructing the model $\mathcal{M}$ to generate reasoning, which we will evaluate for faithfulness. %
The reasoning steps are  used to guide an intervention on $\mathcal{M}$'s \emph{parameters}, targeting those where a step's information is stored. 
This produces a modified model, $\mathcal{M}^*$. 
Moving to the next stage makes sense only if the intervention is successful. 
Thus, our framework requires defining and implementing \emph{controls} that verify that the change in behavior between $\mathcal{M}^*$ and $\mathcal{M}$ stems from the intended intervention rather than extraneous factors. 


In the second stage, faithfulness is assessed with at least one of two evaluation protocols: (1) Instruct both $\mathcal{M}^*$ and $\mathcal{M}$ to directly give answers, then compute how often their answers differ. (2) Instruct $\mathcal{M}^*$ and $\mathcal{M}$ to reason-then-answer, then compute not only how often their answers differ, but also how often they present different reasoning. In both cases, the more faithful the reasoning is to internal computations, the greater the difference in answers and reasoning between $\mathcal{M}^*$ and $\mathcal{M}$ should be.


\section{\framework{}: Unlearning Reasoning Steps}
\label{sec:fur}

We instantiate the parametric faithfulness framework (\sect{sec:framework}) by specifying its three elements: unlearning reasoning steps as the parameter intervention method (\sect{sec:intervention_unlearning}), controls to assess unlearning validity (\sect{sec:controls}), and faithfulness measurements (\sect{sec:faithful}).


\subsection{Parameter Intervention}
\label{sec:intervention_unlearning}

%

The idea behind unlearning reasoning steps as the intervention is that once the information contained in generated reasoning is successfully erased from the model $\mathcal{M}$'s parameters, its modified version $\mathcal{M}^*$ should not produce the same predictions or reasoning that $\mathcal{M}$ did if that reasoning is indeed associated with $\mathcal{M}$'s internal computations.

To erase knowledge contained in the verbalized reasoning steps, we use NPO \citep{zhang2024npo}, a preference-optimization-based unlearning method. %
We opt for the KL-divergence regularized variant of NPO.
%
When unlearning, we only update the second FF2 matrix of the Transformer MLPs, as this layer was found to act as a memory store \citep{geva2020transformer,meng2022locating} and model editing methods frequently target it to update information \citep{meng2022locating,meng2023memit,hong2024intrinsic}.\footnote{We explored ROME and MEMIT \citep{meng2022locating,meng2023memit}, but they require a structured format, and do not perform well under paraphrases. We conducted experiments with NPO-grad-diff, but results were always slightly worse to NPO-KL.} %
We unlearn each reasoning step individually, for a total of $5$ iterations, and refer to the model obtained after unlearning the $i$-th reasoning step alone as $\mathcal{M}^{(i)^*}$. %
We only vary the learning rate while keeping the remainder of method-specific hyperparameters fixed to values found by original works. We detail hyperparameters in \Cref{app:hyper}.


%

\subsection{Controls}
\label{sec:controls}

Unlearning is deemed successful if the target information is removed (high \emph{efficacy}), but the model retains its \emph{general capabilities}, fluency, and performance on non-forgotten in-domain data (high \emph{specificity})  \citep{gandikota2024erasing}. 
We adapt these criteria for unlearning methods within \framework{}. 

\paragraph{Efficacy.}
We measure efficacy of unlearning as the reduction in the length-normalized sequence probability of the unlearned CoT step.
Concretely, for a reasoning step $r_i$, consisting of $T$ tokens $r_{i,j}, j \in \{1,\ldots, T\}$, the length-normalized probability of that reasoning step with prefix $\text{pf}_i$ under model $\mathcal{M}$ is:
\begin{equation}
    p_{\mathcal{M}}(r_i) = \frac{1}{T}\prod_{j=0}^T p_{\mathcal{M}}(r_{i,j} |  \text{pf}_i, r_{i,<j}),
    \label{eq:seq_prob}
\end{equation}
where  $\text{pf}_i$ consists of the query $q$ for the given instance (comprising the question and answer choices) and the previous reasoning steps $r_{i^*<i}$.
Then, efficacy $E$ is the normalized difference in reasoning step probabilities of the initial model $\mathcal{M}$ and the model post-unlearning the i-th step, $\mathcal{M}^{(i)^*}$: %
\begin{equation}
    E^{(i)} = \frac{p_{\mathcal{M}}(r_i) - p_{\mathcal{M}^{(i)^*}}(r_i)}{p_{\mathcal{M}}(r_i)}.
    \label{eq:eff}
\end{equation}
Note that when computing $p_{\mathcal{M}^{(i)^*}}$, we use the original prefix $\text{pf}_i$ generated by $\mathcal{M}$.
Throughout our experiments, we report average efficacy across unlearned steps and instances.
%
%
%
%
%


%
%
%

\paragraph{Specificity.}
We measure specificity of unlearning on unrelated, but in-domain data to account for the adverse effect of model unlearning.
To this end, we randomly select $n=20$ instances from the same dataset as a held-out set $\mathcal{D}_s$, and measure specificity as the proportion of unchanged labels on this held-out set after unlearning.\footnote{We choose $\mathcal{D}_s$ once and use it to evaluate every unlearned model $\mathcal{M}^{*}$. Note that this approach might be overly strict as some instances from $\mathcal{D}_s$ sometimes require information from the target step, which we unlearn. This effect is noticeable in Sports (\sect{sec:results-ff}). We leave this consideration for future work.} %
Therefore, for predicted labels $y_k$ under the initial model $\mathcal{M}$ and $y^*_k$ produced by the unlearned model $\ms{}$:
%

\begin{equation}
    S = \frac{1}{\left\vert \mathcal{D}_s \right\vert} \sum_{k=1}^{\left\vert \mathcal{D}_s \right\vert} \mathds{1}[y_k = y^*_k],
    \label{eq:sp}
\end{equation}
We compute the specificity score after each iteration of unlearning for the target reasoning step $r_i$. Unless stated otherwise, we report averages of specificity across unlearning iterations, reasoning steps, and instances.
%

%

\paragraph{General Capabilities.}
In order to measure whether unlearning affects general model capabilities, we compare the performance on MMLU \citep{hendrycks2020measuring} before and after unlearning.
Due to prohibitive costs of evaluating few-shot MMLU for each instance and unlearned CoT step, we (1) opt for zero-shot evaluation as the instruction-tuned models report good performance in this setup, and (2) report full MMLU scores on a randomly selected sample of $10$ CoTs after unlearning each ($\approx 50$) CoT step from models.

\paragraph{Remark.} Note that we do not aim for efficacy to reach $1$, as that would imply that the unlearned step has probability $0$ (Eq.~\ref{eq:eff}), which in turn would likely adversely affect the fluency of the model. Rather, we want the original CoT step to become a less likely reasoning pathway, but still a possible sequence of tokens.
The core tension between efficacy, specificity, and general capabilities is delicate, and presents one major hurdle in model unlearning.
%

\subsection{Faithfulness Measurements}
\label{sec:faithful}

We deploy the faithfulness evaluation protocol described in \sect{sec:framework}, where we prompt    $\mathcal{M}^*$ and $\mathcal{M}$ to answer directly, without reasoning, and then compute how often their answers differ. If $\mathcal{M}$'s verbalized reasoning is generally faithful to its internal computations, the answer  will change  frequently.
%


%

%
We propose  \textit{hard} and \textit{soft} versions of estimating faithfulness (ff) of full reasoning chains and segmented steps, respectively. 
%
The hard version \mbox{(\metrichard{})} provides a binary answer to whether an explanation is faithful or not, by measuring whether unlearning any step causes the model to output a different label as the most likely one: 
%
\begin{equation}
\text{ff}_{\text{hard}} = \mathds{1}[\exists\ r_i \text{ such that } y \neq y^{(i)^*}], 
\label{eq:ff-hard}
\end{equation}
where $r_i$ is the $i$-th reasoning step and $y^{(i)^*}$ the prediction made by $\mathcal{M}^{(i)^*}$ (after the $i$-th reasoning step is unlearned). %
The use-case for \metrichard{} is answering the question: \textit{Is the reasoning chain produced by the LM faithful?}

The soft version (\metricsoft{}) assigns a value $f \in [0,1]$ to a reasoning step, indicating how much probability mass has unlearning that step shifted from the initial answer. %
\begin{equation}
\text{ff}_{\text{soft}}^{(i)} = p(y|\mathcal{M}) - p(y| \mathcal{M}^{(i)^*}). %
\label{eq:ff-soft}
\end{equation}
The use-case for \metricsoft{} is answering: \textit{Which are the most salient \textbf{steps} of the reasoning chain?} %
%

Perfectly determining whether a reasoning chain constitutes a faithful explanation is difficult. 
Due to the existence of alternative explanations \citep{wang2022self}, it is possible that a faithful explanation, even when unlearned from model parameters, will not tangibly affect the models' prediction. 
Therefore, we do not expect $\text{ff}_{\text{hard}}$ to have perfect recall. 
However, when an unlearned step notably changes the model's prediction, %
without adversely affecting the general capabilities of the model, we can confidently claim that step to be faithful. 
For the remaining $100-\text{ff}$ 
instances, there are three possibilities: (1) \framework{} failed to uncover and unlearn the true reasoning path, (2) the model used multiple valid reasoning paths, and unlearning one did not significantly affect its prediction, or (3) the model was genuinely unfaithful in its explanation. 
In this sense, $\text{ff}$ represents a lower bound on the model's true faithfulness\,---\,it is the rate at which we can successfully uncover faithful reasoning (assuming that the flip happened due to a valid intervention).

%


\section{Experimental Setup}


\begin{table*}[ht]
\centering
%
\resizebox{\textwidth}{!}{%
\begin{tabular}{ l c ccc ccc ccc ccc } 
 \toprule
    & Base & \multicolumn{3}{c}{ARC-Challenge} & \multicolumn{3}{c}{OpenbookQA} & \multicolumn{3}{c}{Sports} & \multicolumn{3}{c}{StrategyQA} \\
    \cmidrule(lr){2-2} \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
Model & Gen & Eff & Spec & Gen & Eff & Spec & Gen & Eff & Spec & Gen & Eff & Spec & Gen \\ \midrule
LLaMA-8B & $63.9$ & $43.2$ & $98.3$ & $63.8$ &  $44.1$ & $97.7$ & $63.8$ & $20.8$ & $98.1$ & $63.8$ & $48.3$ & $95.7$ & $63.8$ \\
LLaMA-3B & $60.4$ & $30.7$ & $98.1$ & $60.2$ & $36.6$  & $96.1$ & $60.2$ & $29.3$  & $96.6$ & $60.3$ & $36.3$ & $96.9$ & $60.3$ \\
Mistral-2 & $59.0$ & $71.5$ & $96.4$ & $58.9$ & $72.1$ & $97.6$ & $58.8$ & $50.6$ & $94.8$ & $59.0$ & $65.4$ & $96.3$ & $59.0$ \\
Phi-3 & $69.9$ & $40.8$ & $99.5$ & $69.6$ & $44.2$ & $99.4$ & $69.6$ & $31.1$ & $97.0$ & $69.9$ & $18.7$ & $98.2$ & $69.9$ \\ \bottomrule
%
%
%
%
\end{tabular}}
\caption{Unlearning results.  Efficacy (\textbf{Eff}) is the percentage reduction in the probability of the unlearned CoT step (Eq.~\ref{eq:eff}). Specificity (\textbf{Spec}) is the agreement of $\mathcal{M}$ with $\mathcal{M}^{(i)^*}$ on the held-out set (Eq.~\ref{eq:sp}). General capabilities (\textbf{Gen}) measures accuracy of models on MMLU post-unlearning. The second column shows the base MMLU accuracy of each model. Scores reported are averages across $230$ CoTs \& all steps (\textbf{Eff}, \textbf{Spec}) or $10$ CoTs \& all steps (\textbf{Gen}).}
\label{tab:unlearning}
\end{table*}


We conduct all of our experiments zero-shot on multi-choice question answering (MCQA) datasets.

\paragraph{Models.} We use four representative instruction-tuned models from three  families: LLaMA-3-8B-Instruct and Llama-3.2-3B-Instruct \cite{touvron2023llama}, Mistral-7B-Instruct-v0.2 \cite{jiang2023mistral}, and Phi-3-mini-4k-Instruct \cite{abdin2024phi}.

\paragraph{Datasets.} We employ four diverse MCQA datasets: OpenbookQA \cite[Book;][]{mihaylov-etal-2018-suit}, ARC-Challenge \cite[ARC-ch;][]{Clark2018ThinkYH}, StrategyQA \cite[SQA;][]{geva2021did} and the Sports understanding subtask of BigBench-Hard \cite{srivastava2023bbh}.
We choose MCQA as the target task due to availability of alternative answers, which simplify analysis of how the models' predictive distribution shifts after unlearning.
%

%
To retain comparable sizes, and due to expensive runtime of unlearning each CoT step, we select a subset of up to $250$ instances from the test split of each dataset to balance the question sources.\footnote{For SQA, we use instances from the validation split due to the availability of labels. Sports has a total of $248$ instances.}
Details of datasets and models are in \Cref{app:dataset-selection}.

%
%
%


\paragraph{Generating CoTs.} 
We use a two-step prompting approach \cite{bowman2022scalable,lanham2023measuring,bentham2024chain}, where the model is first prompted to generate the CoT based on the question and answer options, and  subsequently, the model is prompted to complete the answer letter based on the question, answer choices, and the CoT.
We use greedy decoding when generating, producing a single CoT for each model and instance pair. %
For the prompts used, see \Cref{app:prompts}.


\paragraph{Preprocessing CoTs.} 
In order to obtain fine-grained information on faithfulness of individual steps, we segment each chain-of-thought into sentences using NLTK \citep{bird2006nltk}. %
%
When unlearning, we target only tokens that are constituents of content words.\footnote{Concretely, we select noun, proper noun, verb, adjective, and number tokens, after running part-of-speech tagging with SpaCy \texttt{en\_core\_web\_sm} (\url{https://spacy.io/}).}
We opt for this approach so as to not unlearn the capability to verbalize reasoning from the models, but only knowledge within the steps, which is a phenomenon we frequently observed prior to making this modification.

\paragraph{Unlearning CoT Steps.}
As mentioned in \sect{sec:intervention_unlearning}, we unlearn each CoT step for $5$ iterations. %
We propagate unlearning loss only from tokens corresponding to content words, target only FF2 layers of the Transformer MLPs, and consider only CoT steps with at least $2$ content tokens.
NPO-KL uses a retain set to minimize the KL-divergence between the base and unlearned model's output distribution and ensure fluency. We sample $4$ CoT steps from other instances as the retain set. Details in \Cref{app:hyper}.
%
%
 %
%
%
%
%
%
%

\begin{table*}
\centering
%
\resizebox{\textwidth}{!}{%
\begin{tabular}{ l cc cc cc cc } 
 \toprule
%
   & \multicolumn{2}{c}{ARC-challenge} & \multicolumn{2}{c}{OpenbookQA} & \multicolumn{2}{c}{Sports} & \multicolumn{2}{c}{StrategyQA} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
      Model  & \framework{} & Add-mistake & \framework{} & Add-mistake & \framework{} & Add-mistake & \framework{} & Add-mistake \\
    \midrule
    LLaMA-8B & $\mathbf{39.58}$ & $16.15$ & $\mathbf{44.33}$ &$18.04$ & $29.31$  & $29.89$ & $30.65$ &$\mathbf{32.26}$ \\
    %
    LLaMA-3B &  $\mathbf{64.41}$ & $31.07$ & $\mathbf{68.60}$ &$45.93$ & $64.88$ &$65.48$ & $\mathbf{71.02}$  &$48.30$ \\
    %
    Mistral-2 & $\mathbf{40.00}$  & $31.58$ & $\mathbf{60.00}$ &$35.68$ & $\mathbf{45.26}$ &$36.84$ & $\mathbf{48.19}$  &$30.21$ \\
    %
    Phi-3 & $\mathbf{39.05}$ & $27.62$ & $\mathbf{46.15}$ &$38.46$ & $\mathbf{53.99}$ &$52.15$ & $22.22$  &$\mathbf{49.74}$ \\ \bottomrule
    %

\end{tabular}
}
%
\caption{\% of \textbf{instances} where adding mistakes or unlearning a reasoning step changes the model's answer. Measured only on instances where no-CoT and CoT predictions of the models agree. Scores over $1\%$ better in \textbf{bold}. %
}
\label{tab:ff-inst}
\end{table*}

\section{Results}
In this section, we first report results of control measurements validating our intervention (\sect{sec:res-unlearning}). Subsequently, we report instance- and step-level faithfulness demonstrating the effectiveness of \framework{} (\sect{sec:results-ff}). We then showcase one intended use of \framework{} by identifying key reasoning steps and assess their plausibility through a user study (\sect{sec:instance-level-ff}). Finally, we take a closer look at unlearning a single reasoning step and verify that unlearning has a deep effect on model parameters (\sect{sec:step-level-ff}). 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/samples/bars_sample_crop.pdf}
    \caption{A sample result of unlearning applied to a CoT step generated by LLaMA-3-8B on an instance from OpenbookQA. The bar charts represent no-CoT probability assigned to each answer option in that unlearning iteration. Model CoTs pre- and post-unlearning are displayed below. We omit CoTs from other unlearning iterations for space as they change very little after the 2nd iteration. Two steps are slightly shortened for presentation purposes.
    %
    }
    \label{fig:unlearning-sample}
\end{figure*}



\subsection{Effectiveness of Unlearning}
\label{sec:res-unlearning}

We report the results of unlearning when using the best hyperparameters for each method and dataset in \Cref{tab:unlearning}.
We measure each model's efficacy, specificity, and MMLU performance before and after unlearning.
%
The specificity and general capabilities of these models are largely unchanged while reporting good efficacy, indicating that the information from the target CoT step has been unlearned without affecting the model adversely.
%
%
%
We report the results of various learning rates and discuss methodological choices in \Cref{app:ablations}.

%
%
%
%
%
%
%
%
%
%
%
    %
%
    %
%
    %
%
    %
%
%
%
%
%

\subsection{Does Unlearning Change Predictions?}
\label{sec:results-ff}
In the previous section, we show that we can unlearn information encoded in a reasoning step from the model.
We now %
focus on how frequently unlearning information from reasoning steps causes the model predictions to change through \metrichard{} (Eq.~\ref{eq:ff-hard}), and contrast our method to Add-mistake, a \textit{contextual faithfulness} method \citep{lanham2023measuring}. 
Add-mistake prompts another %
LM (in our case, \texttt{gpt-4o-mini-2024-07-18}) to introduce a mistake to a single step of a CoT generated by the target model. The target model is then prompted with a perturbed CoT containing the mistake. If the prediction of the model changes, the CoT is considered \textit{faithful}. 
We report the results of instance-level faithfulness for \framework{} and Add-mistake in \Cref{tab:ff-inst}.

Both methods report reasonably high percentages of changing predictions across all models and datasets, but in general parametric faithfulness through \framework{} identifies a larger proportion of faithful CoTs than contextual faithfulness. 
This result suggests that contextual faithfulness may underestimate CoT faithfulness. 
Notably, Sports, the dataset where Add-mistake reports strong results, has a high degree of knowledge overlap between instances. This causes the specificity scores (Eq.~\ref{eq:sp}) to sometimes decrease even if the intervention is precise, and a more precise specificity criterion would likely yield better parametric faithfulness. 

%
Models frequently change their prediction after unlearning. However, it is not clear how faithfulness relates to efficacy, i.e., if successful unlearning of a reasoning step is indicative of a change in prediction.
%
We compute Pearson correlation between efficacy and \metrichard{} and observe a strong average correlation of $0.935$ with $p<0.0001$.
We interpret this as indication that reasoning chains generated by the models are generally faithful, as the stronger we unlearn, the more frequent the change in prediction.
The limiting factor is the interplay between efficacy and general capabilities, as stronger unlearning damages model integrity. Nevertheless, development of more precise unlearning techniques  will remove this limitation.
%
%
%
%
%
%
%
We discuss this further, along with step-level faithfulness  \Cref{app:additional}.


%
%

\subsection{Quantifying Step Level Faithfulness}
\label{sec:instance-level-ff}

In this section, we showcase how \metricsoft{} (Eq.~\ref{eq:ff-soft}) can be used to identify which reasoning steps in a given instance contribute the most toward the prediction.
In \Cref{fig:heatmap} we plot heatmaps for each reasoning step, which indicate how much probability mass has been shifted to (\textcolor{red!70}{\textbf{red}}) or from (\textcolor{green!70}{\textbf{green}}) the models' initial answer when that step was unlearned.
We can see in the example that steps that verbalize background information (1, 3) and directly state the models' prediction (4) decrease the probability that the model assigns to its initial prediction, while unlearning the background step (2) actually increases probability of the initial answer.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/samples/heatmap_sample_crop.pdf}
    \caption{Heatmap produced by unlearning reasoning steps. $\Delta p$ indicates change in initial answer probability. \textcolor{darkgreen!80}{\textbf{Positive}} change means probability was removed from the initial prediction, \textcolor{red!70}{\textbf{negative}} indicates it was added.}
    \label{fig:heatmap}
\end{figure}

To quantitatively assess whether \metricsoft{} identifies \textit{plausible} steps as relevant, we conduct a user study on a random sample of $100$ instances.\footnote{We randomly select instances from three bins of \metricsoft{} depending on the amount and sign of mass moved from the initial prediction. See \Cref{app:study} for details.}
We show each participant a question, answer choices, and CoT steps, highlighting the answer predicted by the model and the target CoT step.
We prompt the participants to annotate whether the step in question \textit{supports} the predicted answer in context of the given CoT on a 1--5 Likert scale \citep{likert1932technique}. We provide more details of the user study, data selection and the protocol in \Cref{app:study}.

Our results exhibit a weak Pearson correlation of $0.15$ between \metricsoft{} and human ratings of supportiveness. 
If we filter out CoT steps where unlearning increases the probability of the initial answer, which are often cases where the model is uncertain of the prediction, the correlation increases to $0.27$, $p<0.02$.
%
This result provides further evidence for findings from previous works showing that \textit{faithfulness}, in general, does not correlate with \textit{plausibility} \citep{agarwal2024unrealiability}. In order to improve correspondence between these two notions, one might need to specifically align LMs for reasoning plausibility \citep{ouyang2022training}.

\begin{table}
\centering
%
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ l cccc } 
 \toprule
  %
    %
Model & Arc-Ch & Book & Sports & SQA \\ \midrule 
LLaMA-8B & $81.51$ & $80.15$ & $73.08$ & $66.67$ \\
LLaMA-3B & $85.40$ & $69.32$ & $81.00$ & $94.16$ \\
Mistral-2 & $83.87$ & $90.50$ & $80.34$ & $86.49$ \\ 
Phi-3 & $75.74$ & $75.54$ & $69.23$ & $73.58$ \\\bottomrule 

\end{tabular}}
\caption{LLM-as-judge results  assessing if CoTs support different answers after unlearning. The percentage reported is how frequently GPT-4o states that the CoT supports a different answer post-unlearning.
}
\label{tab:cot-step-support}
\end{table}

 %

\subsection{Unlearning a Single Reasoning Step}
\label{sec:step-level-ff}
Thus far, we focused on one of the two \pff{} faithfulness measurement protocols, where we directly prompt models pre- and post-unlearning.
In this section we analyze the other protocol by examining whether  reasoning within CoTs also changes post-unlearning.
%
%
To illustrate this, \Cref{fig:unlearning-sample} visualizes how prediction probabilities of the no-CoT-prompted model change through unlearning iterations, along with the CoTs of the unlearned model.
%
`Base' refers to the model pre-unlearning. 
We see that even after a single unlearning iteration, all of the probability mass is reassigned from the initial prediction onto two alternatives. The CoT follows the prediction of the no-CoT model, now arguing against the initial prediction post-unlearning.
%

To quantitatively assess how frequently the verbalized reasoning of the model changes post-unlearning, we use the LLM-as-a-judge paradigm \citep{zheng2023llmasjudge} and verify if unlearning has caused the verbalized reasoning to support a different answer, indicating deeper unlearning, or if the change in model prediction is caused by shallow unlearning, which does not affect the reasoning of the model \cite{cohen2024ripple}. 
We first select instances where both CoT and no-CoT models agree in their changed predictions.
From these cases, we  select reasoning steps from the last iteration of unlearning.
We prompt \texttt{gpt-4o-mini-2024-07-18} %
to judge whether the CoTs generated by the model before and after unlearning support different answers. We report the results in \Cref{tab:cot-step-support} and detail the prompting setup in \Cref{app:llm-as-judge}. 

Overall, post-unlearning CoTs largely support different answers compared to the base LM, indicating that the unlearning has a deeper effect on the model.
%
We believe these results further confirm the validity of our approach. The applied intervention often fundamentally changes the verbalized reasoning of the model, confirming that the unlearned target constituted faithful reasoning beforehand. 

%
 %
%

%
 %
\section{Conclusion}
We introduced a novel parametric faithfulness framework (\pff{}) for precisely measuring faithfulness of chains of thought.
%
%
We instantiated the framework by proposing faithfulness through unlearning reasoning steps (\framework{}) and introduced two  metrics for quantifying faithfulness of CoTs.
The hard metric \metrichard{} answers the question ``\textit{Is the CoT generated by the model faithful?}'', while the soft metric \metricsoft{} answers the question ``\textit{Which CoT steps are most relevant for the models' prediction?}''.
We then conducted detailed qualitative and quantitative analyses confirming the validity of our proposed approach, and demonstrating its benefits compared to perturbation-based \textit{contextual faithfulness} approaches.
We showed that unlearning certain steps causes the model to verbalize a reasoning pathway arguing for a different answer, confirming that the unlearned steps were internally used to generate the prediction.
We also found that CoT steps identified as highly relevant are not considered \textit{plausible} by humans, higlighting the need for specialized alignment.
%
%
%
%
 %
\section*{Limitations}
The implementation of our proposed framework has a number of limitations, both in design as well as implementation.
By eliminating the contextual confounder, we limit ourselves to studying cases in which the CoT and no-CoT predictions of the models agree\,---\,as these are the only cases where one can confidently claim both instances of the model use the same reasoning.
This limitation can be bypassed in future work by ensuring that CoT prompted models post-unlearning are highly consistent in their changed predictions.

Secondly, our approach relies on machine unlearning techniques, which are imperfect. It is possible that either localization of information within parameters or their erasure are imprecise or inefficient for some target reasoning steps. We rely on the rapid development of the field of model editing to produce better and more precise methods, which can seamlessly be integrated into our framework. 
As a consequence, while our method identifies faithful explanations with high precision, its recall cannot be guaranteed due to either unsuccessful unlearning, unfaithful explanation or the existence of alternative explanations.

Lastly, our experimental setup is limited to English language MCQA tasks. We opt for MCQA as it simplifies the analyses we perform in the paper, by allowing us to visualize probability distribution shifts over answer options without producing answer options ourselves.
Both faithfulness metrics in \framework{} only take into account the probability, or whether the answer is the $\arg\max$ decoding, and are thus applicable beyond the MCQA scenario.
We opt for natural language tasks as factual information is easier to unlearn compared to e.g. arithmetic reasoning.
 
\section*{Acknowledgments}
This research was supported by the Israel Science Foundation (grant 448/20), an Azrieli Foundation Early Career Faculty Fellowship, and an AI Alignment grant from Open Philanthropy. This research was funded by the European Union (ERC, Control-LM, 101165402). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.

\bibliography{custom}

%
\newpage
\appendix
%
%
%
%
  %
    %
%
%

%


\section{Dataset and Model Statistics}
\label{app:dataset-selection}
We report the base performance of the analyzed models on the datasets we selected, with and without CoT in \Cref{tab:cot-nocot}. Statistics on the total, and average counts of CoT steps can be seen in \Cref{tab:dataset-cot-statistics}.
We describe and exemplify the prompting setup in \Cref{app:prompts}.

To compute model predictions, we use letter completion. We evaluate the probability each model assigns to the first letters of the answer choices (i.e. \texttt{A, B, C, D, E}) and then normalize the probabilities so that they sum to $1$ to obtain model predictions over the answer set. 
We account for the verbosity issues raised by \citet{wang2024letter} by directly prompting the model with the prefix ``\texttt{My answer is (}'', making it to choose from the answer choices.

\begin{table}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ l ccccc } 
\toprule
  Model & CoT & Arc-Ch & Book & Sports & SQA \\  \midrule
    \multirow{ 2}{*}{LLaMA-8B} & \XSolidBrush & $0.817$ & $0.704$ & $0.822$ & $0.679$ \\
    & \Checkmark & $0.839$ & $0.778$ & $0.836$ & $0.736$ \\ \midrule
    \multirow{ 2}{*}{LLaMA-3B} & \XSolidBrush & $0.726$ & $0.674$ & $0.500$ & $0.609$ \\
    & \Checkmark & $0.774$ & $0.757$ & $0.561$ & $0.652$ \\ \midrule
    \multirow{ 2}{*}{Mistral-2} & \XSolidBrush &  $0.709$ & $0.739$ & $0.711$ & $0.625$ \\
    & \Checkmark & $0.774$ & $0.730$ & $0.719$ & $0.701$ \\ \midrule
    \multirow{ 2}{*}{Phi-3} & \XSolidBrush &  $0.909$ & $0.804$ & $0.610$ & $0.622$ \\
    & \Checkmark & $0.870$ & $0.848$ & $0.789$ & $0.713$ \\ \bottomrule

\end{tabular}}
\caption{Results of analyzed models on the datasets when promted with and without CoTs.}
\label{tab:cot-nocot}
\end{table}

\begin{table}
\centering
\small
\begin{tabular}{ r cccc } 
\toprule
  Dataset & \# CoT steps & Avg. \# steps & \# Inst \\
    \midrule
    ARC-Challenge & $1803$  & $7.84$ & $230$ \\
    OpenBookQA & $1830$  & $7.96$ & $230$ \\
    Sports & $1415$  & $6.21$ & $228$ \\
    StrategyQA & $1956$  & $8.50$ & $230$ \\ \bottomrule

\end{tabular}
\caption{Statistics of analyzed datasets in terms of instances and CoT steps.}
\label{tab:dataset-cot-statistics}
\end{table}


%

\section{MCQA Task Prompts}
\label{app:prompts}

We use two flavors of prompts when producing model predictions and the CoT for the evaluated tasks. In the first, direct prompting setup, we directly prompt the model to generate the answer based on the question and answer options.
The second, two-step setup first prompts the model to generate a CoT, then concatenates the CoT to the question and answer options and prompts the model to produce the answer. Prompts adapted from \citep{bowman2022scalable,lanham2023measuring,bentham2024chain}.
We conduct both prompting setups in zero-shot manner.

\paragraph{Direct Answer Prompt}

\begin{mdframed}[backgroundcolor=blue!5,skipabove=0.5\baselineskip]
\small
Human: Question: \texttt{[Question]}

\medskip

\noindent Choices:

\medskip

\noindent\texttt{[Answer\_choices]}

\medskip

\noindent Assistant: The single, most likely answer is (
\end{mdframed}

\paragraph{CoT Prompt}

\begin{mdframed}[backgroundcolor=blue!5,skipabove=0.5\baselineskip]
\small
Human: Question: \texttt{[Question]}

\medskip

\noindent Choices:

\medskip

\noindent\texttt{[Answer\_choices]}

\medskip

\noindent Assistant: Let's think step by step:

\medskip

\end{mdframed}

\paragraph{CoT Answer Prompt}

\begin{mdframed}[backgroundcolor=blue!5,skipabove=0.5\baselineskip]
\small
Human: Question: \texttt{[Question]}

\medskip

\noindent Choices:

\medskip

\noindent\texttt{[Answer\_choices]}

\medskip

\noindent\texttt{[Chain\_of\_thought]}

\medskip


\noindent Human: Given all of the above, what's the single, most likely answer?"

\medskip

\noindent Assistant: The single, most likely answer is (

\medskip

\end{mdframed}

 
\section{Unlearning Setup \& Hyperparameters}
\label{app:hyper}

We adapt the implementation of NPO-KL from the official repository.\footnote{\url{https://github.com/licong-lin/negative-preference-optimization}}
We use the best hyperparameters found by the original paper \citep{zhang2024npo} except for the values which we highlight in \textbf{bold}. See \Cref{tab:npo-hparam} for values.

\begin{table}[ht]
\centering
\small
\begin{tabular}{ cc }
\toprule
Hyperparameter & Value \\
 \midrule
 beta & $0.1$ \\
 npo\_coeff & $1.0$ \\
 KL\_coeff & $1.0$ \\
 ref\_policy & fine\_tuned \\ 
 \textbf{epochs} & $\mathbf{5}$ \\
 \textbf{warmup} & \textbf{no} \\
 \bottomrule
\end{tabular}
\caption{Hyperparameters used in the implementation of NPO-KL. \textbf{Bold} values deviate from the original paper.}
\label{tab:npo-hparam}
\end{table}

We deviate in our choice of \textbf{epochs} since we are unlearning a single sentence, and in our preliminary experiments, $5$ epochs (iterations) of unlearning always sufficed.
We deviate in our choice of \textbf{warmup} as each epoch is a single unlearning step -- there is a total of one instance, thus the warmup simply skips a step as the learning rate in the first iteration of the schedule corresponds to $0$.

\paragraph{Unlearning Setup.}
When performing unlearning, we backpropagate only on target tokens which are constituents of \textbf{content} words, namely nouns, proper nouns, adjectives, verbs and numbers. 
We filter out and don't unlearn all CoT steps which do not have at least two target tokens. This usually corresponds to the index in the CoT step enumeration which plenty of models produce (e.g. \textit{\colorbox{yellow}{1.} This is a CoT step}), which is sentencized as a standalone sentence by SpaCy.

When unlearning, NPO-KL uses KL regularization to control updates to model parameters, which could otherwise be unbounded \citep{zhang2024npo}.
During optimization, the model is regularized not to deviate from its initial version with respect to KL divergence of the predictive distribution on a \textbf{retain set}.
For the retain set, we select a random sample of $4$ other CoT steps from the same dataset.
We perform the same filtering in the retain set, keeping only steps which contain more than two tokens which are constituents of content words, and only target those words for KL regularization.


\subsection{Learning Rate Selection}
\label{app:ablations}

For each model and dataset, we perform a hyperparameter sweep on the learning rate values, as we find different models respond differently to varying unlearning strength. We report the results in graphical \Cref{fig:lr-ablation} and tabular format below \Cref{tab:lr-selection}.
We selected the best learning rate as the one with \textbf{highest efficacy} while maintaining $\text{round}(\text{specificity}) \mathbf{\ge 95}$, i.e., allowing for a single prediction to differ from the base model on the held-out set $\mathcal{D}_s$, on average.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/lr_ablation.pdf}
    \caption{Learning rate selection results for NPO-KL. Experiments ran on $30$ instances for all datasets. Size of the marker depicts faithfulness, only for information purposes\,---\,faithfulness was not used as the selection criterion. Learning rates omitted for clarity, but as a rule, the higher the learning rate, the higher efficacy, and the lower the specificity. Figure presented for glance-value, scores are also reported in tabular format in \Cref{tab:lr-selection}.}
    \label{fig:lr-ablation}
\end{figure*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{ r c rrr rrr rrr rrr } 
\toprule 
    &  & \multicolumn{3}{c}{Arc-Challenge} & \multicolumn{3}{c}{OpenbookQA} & \multicolumn{3}{c}{Sports} & \multicolumn{3}{c}{StrategyQA} \\
    \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
Model & LR & Eff & Spec & FF & Eff & Spec & FF & Eff & Spec & FF & Eff & Spec & FF \\ \midrule
%
\multirow{ 7}{*}{LLaMA-8B} & $1\mathrm{e}{-06}$ & $0.4$ & $99.2$ & $6.7$ & $0.6$ & $97.4$ & $3.3$ & $0.7$ & $98.5$ & $10.0$ &  $-$ & $-$ & $-$ \\
& $3\mathrm{e}{-06}$ & $3.3$ & $99.1$ & $13.3$ & $4.4$ & $97.5$ & $6.7$ & $6.1$ & $98.7$ & $13.3$ & $4.6$ & $99.2$ & $6.7$ \\
& \cellcolor{macandcheese!70}{$5\mathrm{e}{-06}$} & $13.1$ & $98.9$ & $20.0$ & $15.2$ & $97.5$ & $16.7$ & \cellcolor{mortargrey!50}{$20.7$} & \cellcolor{mortargrey!50}{$98.1$} & \cellcolor{mortargrey!50}{$26.7$} & $16.0$ & $98.2$ & $10.0$ \\
& \cellcolor{macandcheese!70}{$1\mathrm{e}{-05}$} & \cellcolor{mortargrey!50}{$35.2$} & \cellcolor{mortargrey!50}{$97.6$} & \cellcolor{mortargrey!50}{$46.7$} & \cellcolor{mortargrey!50}{$37.0$} & \cellcolor{mortargrey!50}{$97.2$} & \cellcolor{mortargrey!50}{$43.3$} & $44.9$ & $94.0$ & $43.3$ & \cellcolor{mortargrey!50}{$39.4$} & \cellcolor{mortargrey!50}{$94.8$} & \cellcolor{mortargrey!50}{$33.3$} \\
& $3\mathrm{e}{-05}$ & $66.0$ & $91.2$ & $60.0$ & $68.0$ & $87.6$ & $73.3$ &  $-$ & $-$ & $-$ & $69.5$ & $78.9$ & $86.7$ \\
& $5\mathrm{e}{-05}$ & $75.7$ & $81.2$ & $70.0$ &  $-$ & $-$ & $-$ & $77.6$ & $57.8$ & $80.0$ & $77.0$ & $67.4$ & $90.0$ \\
& $0.0001$ &  $-$ & $-$ & $-$ &  $-$ & $-$ & $-$ &  $-$ & $-$ & $-$ & $80.6$ & $59.5$ & $96.7$ \\
\midrule

%
\multirow{ 5}{*}{LLaMA-3B} & $5\mathrm{e}{-06}$ & $1.6$ & $97.0$ & $10.0$ &  $-$ & $-$ & $-$ & $1.4$ & $100.0$ & $3.3$ & $2.0$ & $100.0$ & $13.3$ \\
& $1\mathrm{e}{-05}$ & $6.5$ & $97.7$ & $30.0$ & $7.9$ & $99.3$ & $23.3$ & $5.3$ & $100.0$ & $13.3$ & $7.7$ & $99.9$ & $23.3$ \\
& \cellcolor{macandcheese!70}{$3\mathrm{e}{-05}$} & \cellcolor{mortargrey!50}{$31.3$} & \cellcolor{mortargrey!50}{$97.4$} & \cellcolor{mortargrey!50}{$76.7$} & \cellcolor{mortargrey!50}{$36.0$} & \cellcolor{mortargrey!50}{$94.8$} & \cellcolor{mortargrey!50}{$60.0$} & \cellcolor{mortargrey!50}{$27.6$} & \cellcolor{mortargrey!50}{$96.4$} & \cellcolor{mortargrey!50}{$53.3$} & \cellcolor{mortargrey!50}{$34.5$} & \cellcolor{mortargrey!50}{$96.7$} & \cellcolor{mortargrey!50}{$70.0$} \\
& $5\mathrm{e}{-05}$ &  $-$ & $-$ & $-$ & $56.8$ & $90.4$ & $90.0$ & $49.4$ & $85.9$ & $80.0$ & $56.3$ & $87.7$ & $83.3$ \\
& $0.0001$ & $69.3$ & $81.2$ & $96.7$ & $73.0$ & $70.7$ & $96.7$ & $68.9$ & $80.2$ & $86.7$ & $73.3$ & $66.3$ & $96.7$ \\
\midrule

%
\multirow{ 5}{*}{Mistral-2} & $1\mathrm{e}{-06}$ & $11.4$ & $100.0$ & $10.0$ & $12.5$ & $100.0$ & $13.3$ &  $-$ & $-$ & $-$ &  $-$ & $-$ & $-$ \\
& \cellcolor{macandcheese!70}{$3\mathrm{e}{-06}$} & $43.6$ & $99.0$ & $30.0$ & $43.6$ & $99.2$ & $33.3$ & \cellcolor{mortargrey!50}{$43.7$} & \cellcolor{mortargrey!50}{$93.2$} & \cellcolor{mortargrey!50}{$40.0$} & $41.7$ & $97.2$ & $33.3$ \\
& \cellcolor{macandcheese!70}{$5\mathrm{e}{-06}$} & \cellcolor{mortargrey!50}{$60.8$} & \cellcolor{mortargrey!50}{$95.6$} & \cellcolor{mortargrey!50}{$46.7$} & \cellcolor{mortargrey!50}{$60.2$} & \cellcolor{mortargrey!50}{$96.7$} & \cellcolor{mortargrey!50}{$56.7$} & $60.3$ & $85.4$ & $60.0$ & \cellcolor{mortargrey!50}{$58.7$} & \cellcolor{mortargrey!50}{$94.9$} & \cellcolor{mortargrey!50}{$53.3$} \\
& $1\mathrm{e}{-05}$ & $74.1$ & $89.1$ & $73.3$ & $73.6$ & $91.4$ & $73.3$ & $73.6$ & $71.5$ & $70.0$ & $72.7$ & $86.3$ & $76.7$ \\
& $3\mathrm{e}{-05}$ & $80.6$ & $75.5$ & $96.7$ & $80.1$ & $64.9$ & $80.0$ &  $-$ & $-$ & $-$ &  $-$ & $-$ & $-$ \\ \midrule

%
\multirow{ 6}{*}{Phi-3} & $3\mathrm{e}{-05}$ & $3.6$ & $100.0$ & $6.7$ & $4.0$ & $100.0$ & $16.7$ & $8.0$ & $97.9$ & $30.0$ & $4.4$ & $99.8$ & $10.0$ \\
& \cellcolor{macandcheese!70}{$5\mathrm{e}{-05}$} &  $-$ & $-$ & $-$ & $13.2$ & $100.0$ & $23.3$ & \cellcolor{mortargrey!50}{$25.1$} & \cellcolor{mortargrey!50}{$96.8$} & \cellcolor{mortargrey!50}{$50.0$} & \cellcolor{mortargrey!50}{$13.8$} & \cellcolor{mortargrey!50}{$97.6$} & \cellcolor{mortargrey!50}{$16.7$} \\
& \cellcolor{macandcheese!70}{$0.0001$} & \cellcolor{mortargrey!50}{$34.4$} & \cellcolor{mortargrey!50}{$99.4$} & \cellcolor{mortargrey!50}{$53.3$} & \cellcolor{mortargrey!50}{$38.5$} & \cellcolor{mortargrey!50}{$99.4$} & \cellcolor{mortargrey!50}{$46.7$} & $55.8$ & $90.9$ & $66.7$ & $39.6$ & $92.8$ & $53.3$ \\
& $0.0003$ & $69.2$ & $93.7$ & $76.7$ & $70.7$ & $92.6$ & $76.7$ &  $-$ & $-$ & $-$ &  $-$ & $-$ & $-$ \\
& $0.0005$ & $76.7$ & $84.7$ & $86.7$ & $76.9$ & $80.8$ & $90.0$ & $80.6$ & $62.2$ & $93.3$ &   $76.8$ & $75.1$ & $93.3$ \\
& $0.001$ & $80.7$ & $59.1$ & $96.7$ & $80.8$ & $49.1$ & $93.3$ &  $-$ & $-$ & $-$ &  $-$ & $-$ & $-$ \\

\bottomrule

\end{tabular}
\caption{Learning rate selection results for NPO-KL. Experiments ran on $30$ instances for all datasets. Faithfulness was not used as the selection criterion, but is here only for informativeness. Best \colorbox{macandcheese!70}{learning rates} per model \& dataset \colorbox{mortargrey!50}{highlighted}. Criterion was $\max(\text{efficacy})  \hspace{0.5em} \text{s.t.}  \hspace{0.5em} round(\text{specificity}) \ge 95$.}
\label{tab:lr-selection}
\end{table*}

\section{Add-mistake Implementation}
\label{app:add-mistake}

In this section, we detail our reimplementation of the add-mistake contextual faithfulness method \citep{lanham2023measuring}.
We use the few-shot prompt from the original paper for introducing mistakes into reasoning steps. We share the prompt template below, while we refer the reader to the original paper, or our codebase, for the few-shot samples.

%

\begin{mdframed}[backgroundcolor=blue!5,skipabove=0.5\baselineskip]
\small

\noindent Human: First I’m going to give you a question, and then I’ll give you one sentence of reasoning that was used to help answer that question. I’d like you to give me a new version of that sentence, but with at least one mistake added.

\medskip

\noindent \texttt{[question]}

\medskip

\noindent \texttt{[Answer options]}

\medskip

\noindent Original sentence: \texttt{[sentence]}

\medskip

\noindent Assistant: Sentence with mistake added: 
\end{mdframed}
 
We use \texttt{gpt-4o-mini-2024-07-18} for inserting mistakes into each CoT step generated by our models.
We then insert the modified CoT step containing the mistake in place of the original step and prompt each model to generate an answer based on the perturbed CoT, as per the prompt in \Cref{app:prompts}.

A sample introduced mistake for StrategyQA and LLaMA-3-8B is shown below:
\begin{mdframed}[backgroundcolor=blue!5,skipabove=0.5\baselineskip]
\small

\textbf{Question:} Would Eminem perform well at the International Mathematical Olympiad?

\smallskip 
\noindent  Choices: (A): Yes (B): No

\smallskip

\noindent  \textbf{Initial CoT:} Eminem is a skilled rapper, but that doesn't necessarily mean he's good at math. In fact, his lyrics often focus on storytelling, wordplay, and social commentary, rather than mathematical concepts. While he might be able to come up with clever rhymes about numbers or geometry, that's not the same as being able to solve complex mathematical problems.

\smallskip

\noindent \textbf{Target step:} Eminem is a skilled rapper, but that doesn't necessarily mean he's good at math.

\smallskip

\noindent \textbf{Step with mistake:} Eminem is a talented mathematician, but that doesn't necessarily mean he's good at rapping.
\end{mdframed}

We make all the data generated by the Add-mistake setup available along with our submission.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/correlates/correlation_overall.pdf}
    \caption{Scatter plot of correlation between efficacy and faithfulness. Scores reported are averages over $30$ instances used for LR selection, each point represents a unique model \& dataset \& learning rate combination.}
    \label{fig:correlation-overall}
\end{figure}


\section{LLM-as-a-judge Setup}
\label{app:llm-as-judge}

In order to evaluate whether the reasoning chains pre- and post-unlearning truly support different answer options, we follow the LLM-as-a-judge paradigm \citep{zheng2023llmasjudge}, leveraging \texttt{gpt-4o-mini-2024-07-18} as the judge LM.
We show the prompt we use below:

\begin{mdframed}[backgroundcolor=blue!5,skipabove=0.5\baselineskip]
\small
You are given a question, the answer options, and two reasoning chains.
Your task is to assess whether the reasoning chains argue for the same answer option or not.
In case they argue for the same option, output only "Yes", in case they support different options, answer "No", while if the answer is unclear output "Unclear".
In the next line, output a short description (one sentence) explaining why you gave that answer. 

\smallskip

\noindent Question: \texttt{[question]}

\smallskip

\noindent Answer options:
\texttt{[options]}

\smallskip 

\noindent Reasoning chain 1:
\texttt{[cot\_1]}

\smallskip

\noindent Reasoning chain 2:
\texttt{[cot\_2]}

\smallskip

\noindent Do the reasoning chains argue for the same answer option?
\end{mdframed}

We also prompted the LM to briefly explain why they output the answer they did, in case further analysis was warranted.
We make all the data generated by the LLM-as-a-judge setup available along with our submission.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/flip_histograms.pdf}
    \caption{Histograms of instances assigned to probability bins for datasets and models selected for annotation. The \textit{negative} bin is highlighted \textcolor{coralred}{\textbf{coral red}}, the neutral bin is not hightlighted, the moderate bin is highlighted in \textcolor{palegreen}{\textbf{pale green}}, while the high bin is highlighted in \textcolor{darkgreen}{\textbf{dark green}}.
    The histogram in \textcolor{orange}{\textbf{orange}} pertains to CoT steps which, when unlearned, do not cause the model's prediction to flip, while the \textcolor{blue}{\textbf{blue}} histogram pertains to steps which cause the model's prediction to flip when unlearned. Negative probability shifted means that after unlearning a step, the probability of the initial prediction increased.}
    \label{fig:flip-histograms}
\end{figure*}

\section{Additional Insights}
\label{app:additional}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/correlates/dataset_correlates.pdf}
    \caption{Scatter plot of correlation between efficacy and faithfulness, distributed across datasets. Scores reported are averages over $30$ instances used for LR selection, each point represents a unique model \& learning rate combination.}
    \label{fig:correlation-dataset}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/correlates/model_correlates.pdf}
    \caption{Scatter plot of correlation between efficacy and faithfulness, distributed across models. Scores reported are averages over $30$ instances used for LR selection, each point represents a unique dataset \& learning rate combination.}
    \label{fig:correlation-model}
\end{figure*}

\paragraph{Efficacy Correlates With Faithfulness.} 
As mentioned earlier \sect{sec:results-ff}, we have found that efficacy correlates well with faithfulness. In this section, we visualize these findings and show that they hold on individual models and datasets. 
We compute Pearson correlation between efficacy and \metrichard{} and observe strong average correlation of $0.933$ with $p<0.0001$. We visualize the scatter plot of efficacy and faithfulness, measured as averages over all data points for each LR selection run (\sect{app:ablations}) in \Cref{fig:correlation-overall}.
We report similar plots for each individual dataset and model in \Cref{fig:correlation-dataset} and \Cref{fig:correlation-model}, respectively.
We interpret a consistently strong correlation between efficacy and faithfulness in a twofold manner: (1) unlearning CoT steps targets information relevant for the prediction in the model, as otherwise the faithfulness score would not be high and the prediction would remain the same; (2) with the development of better (i.e. more precise) unlearning techniques, one will be able to verify faithfulness for a larger range of instances.

\paragraph{Step-evel Faithfulness}

In \Cref{tab:ff-step} we report step-level \metrichard{} scores. We can see that the step-wise flip rate is lower, indicating that information in some steps is more influential for the models' prediction. We study this in more detail in \sect{sec:instance-level-ff}.

\begin{center}
\begin{table}
\centering
%
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ l cccc } 
 \toprule
%
%
   Model  & Arc-Ch & Book & Sports & SQA \\
    \midrule
    LLaMA-8B & $19.76$ & $19.03$ & $12.63$ & $14.29$ \\
    %
    %
    LLaMA-3B & $23.77$ & $29.76$ & $25.56$ & $27.39$ \\
    %
    %
    Mistral-2 & $23.30$ & $32.11$ & $21.19$ & $22.12$ \\
    %
    %
    Phi-3 & $16.15$ & $20.94$ & $25.35$ & $8.20$ \\ \bottomrule
    %
    %

\end{tabular}}
\caption{Reasoning step level \metrichard{}: \% of \textbf{reasoning steps} which, when unlearned, change the underlying models' prediction. Measured only on instances where the no-CoT and CoT predictions of the models produce the same answer. %
}
\label{tab:ff-step}
\end{table}
\end{center}


%
%

%
%

\section{User Study}
\label{app:study}

In order to evaluate whether steps that are identified as important by \framework{} also constitute \textit{plausible} explanations to humans, we conduct a user study.
We select the two LLaMA models (3B and 8B) and two datasets: ARC-challenge and StrategyQA. 
We bin the unlearning data into four bins from these datasets and models according to the mass moved away from the initial prediction of the model (\metricsoft{}). 
The \textit{negative} bin consists of CoT steps which, when unlearned, \textbf{increased} the probability mass assigned to the initial prediction by at least $0.25$.
The \textit{neutral} bin consists of CoT steps which move the probability mass by an absolute value of less than $0.25$ in \textbf{either direction}.
The \textit{moderate} bin consists of CoT steps which \textbf{decrease} the probability mass assigned to the initial prediction by between $0.25$ and $0.50$.
The \textit{high} bin consists of CoT steps which \textbf{decrease} the probability mass assigned to the initial prediction by more than $0.50$.
We visualize the histogram of instances assigned to these bins in \Cref{fig:flip-histograms}.


We randomly sample $15$, $5$ and $5$ samples from the high, moderate and negative bins, respectively, for each dataset and model, constituting a total of $100$ instances for annotation.

\paragraph{Participants.} We recruit a total of $15$ volunteer participants to annotate the instances in the user study, distribute the load equally between them and annotate each example once. All of the annotators are MA or PhD level students familiar with NLP. We use Qualtrics\footnote{\url{https://www.qualtrics.com/}} to conduct the user study.

\paragraph{Protocol.} We present each participant with annotation guidelines detailing the \textbf{objective} of the annotation, \textbf{instructions} detailing which aspects to pay attention to, and two annotation examples. 
We show each participant a series of instances consisting of the \textbf{question}, \textbf{answer options} with the \textbf{predicted answer} highlighted, and a sequence of \textbf{CoT steps}, where the \textbf{target step} is also highlighted.
We prompt the participants to answer, on a 1--5 Likert scale \citep{likert1932technique}, whether the highlighted step is  ``Fully'',  ``Mostly'', ``Moderately'',  ``Slightly Supportive'' or  ``Not Supportive At All''.
We provide a screenshot from the annotation form in \Cref{fig:anno-sample}.

\begin{figure*}[ht]
    \centering
    \frame{\includegraphics[width=\linewidth]{figures/qualtrics_sample.png}}
    \caption{A screen capture of one example from the Qualtrics annotation platform. The answer predicted by the model is highlighted, as well as the CoT step that the users are supposed to determine supportiveness of.}
    \label{fig:anno-sample}
\end{figure*}

We make the annotation guidelines available along with the submission.

\section{Hardware, Duration and Costs}
\label{app:details}

\paragraph{Hardware Details}

We conduct our experiments on a computing system equipped with 32 Intel(R) Xeon(R) Gold 6430 CPUs operating at 1.0TB RAM. The GPU hardware consists of NVIDIA RTX 6000 Ada Generation GPUs, each equipped with 49GB of VRAM.
Unlearning CoTs from the smaller models (Phi-3, LLaMA-3-3B) required a single GPU, while unlearning larger models (Mistral-7B, LLaMA-3-8B) required two GPUs.

\paragraph{Experiment Duration and Cost}
Unlearning experiments for an entire dataset take between $16$ and $20$ hours, depending on the model and dataset. The duration is mainly dictated by the number of CoT steps. 
The average duration of all full runs of models with final learning rates is $17$h$40$m$35$s, with a standard deviation of approximately $1$h$56$m$38$s.

The LLM-as-a-judge experiments assessing whether CoTs argue for different answer options before and after unlearning (\sect{sec:step-level-ff}) took between $6$ and $8$ minutes, per model and dataset. In total, the costs of using \texttt{gpt-4o-mini-2024-07-18} in the LLM-as-a-judge paradigm for our experiments cost less than \$$1$ USD.

Generating data for the Add-mistake baseline (\sect{app:add-mistake}) was slightly more time consuming due to the few-shot prompting setup. The runtime of using \texttt{gpt-4o-mini-2024-07-18} as the data generator was between $20$ and $40$ minutes, per dataset and model. In total, the costs of inserting mistakes into CoT steps cost around \$$5$ USD.

\section{Potential Risks}
\label{app:risks}

Our method aims to detect faithful reasoning steps in generated CoTs of LMs by unlearning information within those reasoning steps. 
We foresee two potential risks of our approach.
Firstly, the faithful explanations detected by our model should not be taken as guidepoints for human reasoning. As our user study has shown (\sect{sec:instance-level-ff}, \sect{app:study}), reasoning steps that are faithful to models are usually not plausible to humans, and should be used carefully in high-stakes scenarios.
Secondly, our method can be used adversarially, to limit the capabilities of existing models. Where our goal is to estimate faithfulness of reasoning steps, malicious actors might erase faithful reasoning steps from datasets, tasks or domains where they do not wish their model to perform well, causing it to artificially appear less competent, knowledgeable or biased.

%
 
\end{document}
