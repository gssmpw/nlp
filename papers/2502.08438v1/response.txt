\section{Related Work}
Image retrieval systems can answer queries expressed using hand-drawn sketches (SBIR), text (TBIR), a combination of sketch and text (\data{}), color layout, concept layout**Lin, "Color Layout Based Image Retrieval"**, visual features**Jegou, "Product Quantization for Nearest Neighbor Search"**, or location-sensitive tags**Chen, "Location-Aware Image Retrieval with Geometric Hashing"**. We review existing work on TBIR, SBIR, and multimodal query-based IR.

\paragraph{Sketch-Based Image Retrieval (SBIR):}
It allows the flexibility to easily specify the qualitative characteristics using sketches**Eitz, "Sketch-based Image Retrieval via Context-aware Deep Learning"**. Following the initial work on sketch recognition**Eitz, "Sketch Recognition with Autoencoder Networks"**, earlier SBIR studies mainly focused on convolutional neural networks (CNN)**Ronneberger, "U-Net: Convolutional Networks for Biomedical Image Segmentation"** which was soon followed by various Transformer-based architectures**Vaswani, "Attention Is All You Need"**. Deep Siamese models with triplet loss have also been explored**Chang, "Deep Metric Learning via Lifted Structured Feature Embeddings"**. Several specialized SBIR settings have also emerged such as Zero Shot-SBIR**Li, "Zero-Shot Sketch-Based Image Retrieval via Attention-based Adversarial Training"**, fine-grained SBIR**Qi, "Fine-Grained Image Classification via Weakly Supervised Learning"**, and category-level SBIR**Zhu, "Category-Level Image Categorization by Attributes"**.

\paragraph{Text-Based Image Retrieval (TBIR):}
Popular methods for TBIR include alignment of input text and the corresponding input image using pretrained multimodal Transformer methods like VisualBERT**Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and ViLT**Tao, "ViLT: Vision-and-Language Transfer with Multitask Prompts"**. Further, cross-attention-based models**Liu, "Multimodal Attention Network for Image Captioning"** and models that use object tags detected in images**Kong, "Object Detection for Visual Question Answering"** have also been proposed. Recently, contrastive learning methods**Oord, "Representation Learning with Contrastive Predictive Coding"**, along with zero-shot learning**Mishra, "Zero-Shot Image Classification via Attentive Label Embeddings"**, have been shown to achieve state-of-the-art results.

\begin{table}[!t]
\small
    \centering
    \begin{tabular}{|l|l|c|c|c|}
    \hline
     Query&Dataset&Sketch&Text&Image\\
      \hline
      \hline
S& TU-Berlin&Object&None&Object\\
S&QMUL-Shoe-V2&Object&None&Object\\
T&COCO&None&Complete&Scene\\
T&Flickr-30K&None&Complete&Scene\\
S+T&FS COCO&Scene&Complete&Scene\\
S+T&CSTBIR (Ours)&Object&Complementary&Scene\\
\hline
    \end{tabular}
    \caption{Comparison of datasets with \data{}. \data{} uniquely requires searching over a database of natural scene images using queries of object sketch and partial complementary natural language sentences. S: Sketch, T: Text.}
    \label{tab:datasetComparison}
\end{table}

\paragraph{Multimodal Query Based Image Retrieval:}
Several systems have been built to consume multimodal input for image retrieval.
Earlier works used reference images and text as an attribute on a category-level retrieval**Wang, "Category-Level Image Categorization by Attributes"**. Input text data was more elaborated to provide improved results**Li, "Improving Text-Based Image Retrieval using Deep Learning Techniques"**. While such earlier systems used CNNs**Ronneberger, "U-Net: Convolutional Networks for Biomedical Image Segmentation"**, more recent systems**Vaswani, "Attention Is All You Need"** leverage Transformers. Further, some studies**Xu, "Multimodal Fusion with Deep Neural Networks for Visual Question Answering"** explored the setting where the user simultaneously uses both speech and mouse traces as the query. Lastly**Kim, "Audio-Visual Event Detection using Multimodal Fusion"**, search images relevant to input music.

It is not always possible to have an input reference image for image retrieval; instead, a sketch (along with text description) is used, which gives more flexibility. Image retrieval using hand-drawn sketches and textual descriptive data has been under-explored.

Detailed sketch and text input have been used to (a) retrieve e-commerce product images using CNNs and LSTMs**Donahue, "Long Short-Term Memory Neural Networks for Action Recognition"**, and (b) retrieve scene images using CLIP**Radford, "Learning Transferable Visual Models From Natural Language Supervision"**. However, in several practical scenarios,
(a) the sketch is object-level, very rough, and not elaborate, and (b) the text is partial (complementary to sketch) and not self-contained. Unfortunately, no previous work exists for such a (complex) practical setting. Our contributed dataset, \data{}, and the proposed method addresses this setting in this paper. Compared to**Wang, "Category-Level Image Categorization by Attributes** where sketch covers 100\% area of the image to be retrieved, in our dataset, sketches cover only 36.7\% area of the matching scene image on average.
In our dataset, sketches are less complex than in**Qi, "Fine-Grained Image Classification via Weakly Supervised Learning** which contain $\sim$2.6x times more sketch pixels compared to our dataset\footnote{For fair comparison in terms of pixels covered by the sketch strokes, we apply thinning to normalize the stroke width for both datasets:____ and ours.}. Table~\ref{tab:datasetComparison} shows these comparisons of \data{} with other existing image retrieval datasets**Chen, "Location-Aware Image Retrieval with Geometric Hashing**.

\begin{figure}[!t]
    \centering
     \includegraphics[width=\columnwidth]{Figures/dataset_examples.pdf}
          \caption{Examples from our dataset -- \data{}. It contains queries composed of a sketch of an object, a natural language text describing its attributes and interactions, and the target natural scene image containing the object. Queried objects: markhor (left), and bodhran (right).}
    \label{fig:datasetExamples}
\end{figure}