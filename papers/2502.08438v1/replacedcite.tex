\section{Related Work}
Image retrieval systems can answer queries expressed using hand-drawn sketches (SBIR), text (TBIR), a combination of sketch and text (\data{}), color layout, concept layout____, visual features____, or location-sensitive tags____. We review existing work on TBIR, SBIR, and multimodal query-based IR.

\paragraph{Sketch-Based Image Retrieval (SBIR):}
It allows the flexibility to easily specify the qualitative characteristics using sketches____. Following the initial work on sketch recognition____, earlier SBIR studies mainly focused on convolutional neural networks (CNN)____ which was soon followed by various Transformer____-based architectures____. Deep Siamese models with triplet loss have also been explored____. Several specialized SBIR settings have also emerged such as Zero Shot-SBIR____, fine-grained SBIR____, and category-level SBIR____.

\paragraph{Text-Based Image Retrieval (TBIR):}
Popular methods for TBIR include alignment of input text and the corresponding input image using pretrained multimodal Transformer methods like VisualBERT____ and ViLT____. Further, cross-attention-based models____ and models that use object tags detected in images____ have also been proposed. Recently, contrastive learning methods____, along with zero-shot learning____, have been shown to achieve state-of-the-art results. 


\begin{table}[!t]
\small
    \centering
    \begin{tabular}{|l|l|c|c|c|}
    \hline
     Query&Dataset&Sketch&Text&Image\\
      \hline
      \hline
S& TU-Berlin&Object&None&Object\\
S&QMUL-Shoe-V2&Object&None&Object\\
T&COCO&None&Complete&Scene\\
T&Flickr-30K&None&Complete&Scene\\
S+T&FS COCO&Scene&Complete&Scene\\
S+T&CSTBIR (Ours)&Object&Complementary&Scene\\
\hline
    \end{tabular}
    \caption{Comparison of datasets with \data{}. \data{} uniquely requires searching over a database of natural scene images using queries of object sketch and partial complementary natural language sentences. S: Sketch, T: Text.}
    \label{tab:datasetComparison}
\end{table}

\paragraph{Multimodal Query Based Image Retrieval:}
Several systems have been built to consume multimodal input for image retrieval. 
Earlier works used reference images and text as an attribute on a category-level retrieval____. Input text data was more elaborated to provide improved results____. While such earlier systems used CNNs, more recent systems____ leverage Transformers. Further, some studies____ explored the setting where the user simultaneously uses both speech and mouse traces as the query. Lastly,____ search images relevant to input music.

It is not always possible to have an input reference image for image retrieval; instead, a sketch (along with text description) is used, which gives more flexibility. Image retrieval using hand-drawn sketches and textual descriptive data has been under-explored.

Detailed sketch and text input have been used to (a) retrieve e-commerce product images using CNNs and LSTMs____, and (b) retrieve scene images using CLIP____. However, in several practical scenarios, 
(a) the sketch is object-level, very rough, and not elaborate, and (b) the text is partial (complementary to sketch) and not self-contained. Unfortunately, no previous work exists for such a (complex) practical setting. Our contributed dataset, \data{}, and the proposed method addresses this setting in this paper. Compared to____ where sketch covers 100\% area of the image to be retrieved, in our dataset, sketches cover only 36.7\% area of the matching scene image on average.
In our dataset, sketches are less complex than in____, which contain $\sim$2.6x times more sketch pixels compared to our dataset\footnote{For fair comparison in terms of pixels covered by the sketch strokes, we apply thinning to normalize the stroke width for both datasets:____ and ours.}. Table~\ref{tab:datasetComparison} shows these comparisons of \data{} with other existing image retrieval datasets____.

\begin{figure}[!t]
    \centering
     \includegraphics[width=\columnwidth]{Figures/dataset_examples.pdf}
          \caption{Examples from our dataset -- \data{}. It contains queries composed of a sketch of an object, a natural language text describing its attributes and interactions, and the target natural scene image containing the object. Queried objects: markhor (left), and bodhran (right).}
    \label{fig:datasetExamples}
\end{figure}