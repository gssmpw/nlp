\section{Related Work}
Image retrieval systems can answer queries expressed using hand-drawn sketches (SBIR), text (TBIR), a combination of sketch and text (\data{}), color layout, concept layout~\cite{zhou2017recent}, visual features~\cite{tian2023fashion,dodds2020modality}, or location-sensitive tags~\cite{gomez2020location}. We review existing work on TBIR, SBIR, and multimodal query-based IR.

\paragraph{Sketch-Based Image Retrieval (SBIR):}
It allows the flexibility to easily specify the qualitative characteristics using sketches~\cite{yu2016sketch, dey2019doodle, song2017deep}. Following the initial work on sketch recognition~\cite{sun2012sketch2tag}, earlier SBIR studies mainly focused on convolutional neural networks (CNN)~\cite{yu2016sketch, liu2017deep} which was soon followed by various Transformer~\cite{vaswani2017attention}-based architectures~\cite{Ribeiro2020SketchformerTR, chowdhury2022fs}. Deep Siamese models with triplet loss have also been explored~\cite{yu2016sketch, collomosse2019livesketch}. Several specialized SBIR settings have also emerged such as Zero Shot-SBIR~\cite{pandey2020stacked,dey2019doodle,dutta2019semantically}, fine-grained SBIR~\cite{liu2020scenesketcher, bhunia2022adaptive, pang2019generalising, pang2017cross,ling2022conditional,bhunia2020sketch,song2017deep}, and category-level SBIR~\cite{sain2021stylemeup,bhunia2021vectorization,sain2022sketch3t}.

\paragraph{Text-Based Image Retrieval (TBIR):}
Popular methods for TBIR include alignment of input text and the corresponding input image using pretrained multimodal Transformer methods like VisualBERT~\cite{li2019visualbert} and ViLT~\cite{kim2021vilt}. Further, cross-attention-based models~\cite{zhang2020context,lee2018stacked} and models that use object tags detected in images~\cite{li2020oscar} have also been proposed. Recently, contrastive learning methods~\cite{jia2021scaling}, along with zero-shot learning~\cite{radford2021learning}, have been shown to achieve state-of-the-art results. 


\begin{table}[!t]
\small
    \centering
    \begin{tabular}{|l|l|c|c|c|}
    \hline
     Query&Dataset&Sketch&Text&Image\\
      \hline
      \hline
S& TU-Berlin&Object&None&Object\\
S&QMUL-Shoe-V2&Object&None&Object\\
T&COCO&None&Complete&Scene\\
T&Flickr-30K&None&Complete&Scene\\
S+T&FS COCO&Scene&Complete&Scene\\
S+T&CSTBIR (Ours)&Object&Complementary&Scene\\
\hline
    \end{tabular}
    \caption{Comparison of datasets with \data{}. \data{} uniquely requires searching over a database of natural scene images using queries of object sketch and partial complementary natural language sentences. S: Sketch, T: Text.}
    \label{tab:datasetComparison}
\end{table}

\paragraph{Multimodal Query Based Image Retrieval:}
Several systems have been built to consume multimodal input for image retrieval. 
Earlier works used reference images and text as an attribute on a category-level retrieval~\cite{kovashka2012whittlesearch,han2017automatic}. Input text data was more elaborated to provide improved results~\cite{guo2018dialog,vo2019composing}. While such earlier systems used CNNs, more recent systems~\cite{song2023boosting,baldrati2022effective} leverage Transformers. Further, some studies~\cite{changpinyo2021telling,pont2020connecting} explored the setting where the user simultaneously uses both speech and mouse traces as the query. Lastly,~\cite{nakatsuka2023content} search images relevant to input music.

It is not always possible to have an input reference image for image retrieval; instead, a sketch (along with text description) is used, which gives more flexibility. Image retrieval using hand-drawn sketches and textual descriptive data has been under-explored.

Detailed sketch and text input have been used to (a) retrieve e-commerce product images using CNNs and LSTMs~\cite{song2017fine}, and (b) retrieve scene images using CLIP~\cite{sangkloy2022sketch,chowdhury2023scenetrilogy}. However, in several practical scenarios, 
(a) the sketch is object-level, very rough, and not elaborate, and (b) the text is partial (complementary to sketch) and not self-contained. Unfortunately, no previous work exists for such a (complex) practical setting. Our contributed dataset, \data{}, and the proposed method addresses this setting in this paper. Compared to~\cite{sangkloy2022sketch} where sketch covers 100\% area of the image to be retrieved, in our dataset, sketches cover only 36.7\% area of the matching scene image on average.
In our dataset, sketches are less complex than in~\cite{sangkloy2022sketch}, which contain $\sim$2.6x times more sketch pixels compared to our dataset\footnote{For fair comparison in terms of pixels covered by the sketch strokes, we apply thinning to normalize the stroke width for both datasets:~\cite{sangkloy2022sketch} and ours.}. Table~\ref{tab:datasetComparison} shows these comparisons of \data{} with other existing image retrieval datasets~\cite{eitz2012humans,yu2016sketch,lin2014microsoft,young2014image,chowdhury2022fs}.

\begin{figure}[!t]
    \centering
     \includegraphics[width=\columnwidth]{Figures/dataset_examples.pdf}
          \caption{Examples from our dataset -- \data{}. It contains queries composed of a sketch of an object, a natural language text describing its attributes and interactions, and the target natural scene image containing the object. Queried objects: markhor (left), and bodhran (right).}
    \label{fig:datasetExamples}
\end{figure}