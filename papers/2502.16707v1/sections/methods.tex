\section{Reflective Planning with Vision Language Models}\label{sec:methods}
To address the challenges of physical interaction and long-horizon reasoning, we present a framework that incorporates VLMs with reflective planning. Our approach combines two key components: (1) a diffusion-based dynamics model that enables the VLM to imagine and evaluate future states, and (2) an interactive learning mechanism that allows the VLM to reflect on and revise its decisions based on these imagined outcomes. As shown in Fig.~\ref{fig:teaser}, these components work together to enable more robust manipulation planning while preserving the benefits of pre-trained VLMs.

\subsection{Interactive VLM Policy Post-Training}\label{sec:policy_training}
%%
While VLMs can generate actions based on visual inputs, they may hallucinate physically implausible solutions without actual interaction experience. %\yf{cite}
To overcome this limitation and enable long-horizon reasoning, we introduce an interactive learning algorithm that teaches the VLM to reflect on and improve its decisions through direct interaction with the physical environment. This process further enhances a base VLM policy, which is initially trained on a fixed set of expert demonstrations.
Similar to DAgger~\citep{dagger}, we iteratively collect new data by rolling out the VLM policy in the environment and finetune the VLM policy with the aggregated data. As formulated in Algorithm~\ref{alg:training}, $N$ trajectories are collected in each iteration. At each timestep, we generate a learner action $a_t^\dag$ by prompting the VLM with the images of the goal and current states, as well as an expert action $a_t^*$ from the oracle policy. The pairs $((I_g, I_t), a_t^*)$ are then added to the dataset for finetuning. To facilitate convergence, we execute the learner action $a_t^\dag$ with a probability of $p$ and the expert action $a_t^*$ with a probability of $1-p$, instead of always following the actions from the learner. 

\begin{algorithm}
\caption{Interactive VLM Post-Training}
\label{alg:training}
\begin{algorithmic}[1]
\REQUIRE {initial state distribution $\rho_0$, goal state distribution $\rho_g$, numbef of iterations $K$, number of trajectories per iteration $N$, episode length $T$, imagination horizon $H$, expert policy $\pi_E$, expert demonstrations $\mathcal{D}^*$}
\STATE train base policy $\pi_\text{VLM}$ on $\mathcal{D}^*$
\STATE $\mathcal{D}\gets\mathcal{D}^*$
\FOR {$i \gets 1$ to $K$}
    \STATE $\mathcal{D}_i\gets\emptyset$
    \STATE {// rollout out policy $\pi_\text{VLM}$ to collect data $\mathcal{D}_{i}$}
    \FOR {$n \gets 1$ to $N$}
        \STATE $s_0\sim\rho_0$; $I_0\gets\mathcal{Z}(s_0)$
        \STATE $s_g\sim\rho_g$; $I_g\gets\mathcal{Z}(s_g)$
        \FOR {$t \gets 0$ to $T-1$}
            \STATE $a_t^{\dag}\sim\pi_\text{VLM}(I_g, I_t)$; $a_t^* \sim \pi_E(s_g, s_t)$
            \STATE $a_t\gets a_t^{\dag}$ \textbf{if} ${\tt random()} <p$ \textbf{else} $a_t^*$
            \STATE $s_{t+1}\gets\mathcal{T}(s_t, a_t)$; $I_{t+1}\gets\mathcal{Z}(s_{t+1})$
        \ENDFOR
        \STATE $\mathcal{D}_i\!\gets\!\mathcal{D}_i\cup\{((I_g, I_t), a_t^*)\}_{0\le t<T}$
        \STATE {\textcolor{red}{$\mathcal{D}_i\!\gets\!\mathcal{D}_i\cup\{((I_g, I_t, I_{t+H}, a_{t:t+H-1}), a_t^*)\}_{0\le t<T}$}}
    \ENDFOR
    \STATE {$\mathcal{D}\gets \mathcal{D}\cup\mathcal{D}_{i}$}
    \STATE {finetune $\pi_\text{VLM}$ on $\mathcal{D}$}
\ENDFOR
\end{algorithmic}
\end{algorithm}

To generate training data for reflection, we can simply relabel a trajectory after it is terminated, as also illustrated in Fig.~\ref{fig:data_dollection}. Specifically, the image $I_{t+H}$, which is a future observation following the action sequence $a_{t:t+H-1}$, is added to the context for reflection at timestep $t$, and the VLM is still supervised to output the same expert action $a_t^*$. Intuitively, this image provides additional information about the effect of executing the action sequence as a feedback, which can be leveraged by the VLM to decide whether the initially proposed action sequence leads to a promising future state. 

In essence, we are generating two forms of question answering examples from interaction with the environment. The first is to predict an optimal action given images of the goal and current state, and the second is to reflect and revise an initial action sequence proposal by looking into an additional future image.
%%
Since a VLM can flexibly take any text and images as input, these two tasks can be handled by a single VLM with two different prompt templates, as summarized in Fig.~\ref{fig:data_dollection}. See App.~\ref{sec:app_prompts} for full prompts, and App.~\ref{subsec:vlm_policy} for detailed VLM architecture.

The VLM is trained to generate actions aligned with expert actions in the dataset with a cross entropy loss:
\begin{equation}
\begin{split}
    \min_{\pi_\text{VLM}}\ & \mathbb{E}_{\mathcal{D}}\Big[\mathcal{L}_\text{CE}(\pi_\text{VLM}^\text{propose}(a_t|I_g, I_t), a_t^*) \\
   +\ & \mathcal{L}_\text{CE}(\pi_\text{VLM}^\text{reflect}(a_t|I_g, I_t, I_{t+H}, a_{t:t+H-1}), a_t^*)\Big]. %%Neat, always ends equation with comma, it's also a sentence
\end{split}
\end{equation}


\subsection{Diffusion Dynamics Model}\label{sec:diffusion_model}
%%
A key component in reflective planning is predicting future states accurately when evaluating potential action sequences. While our interactive learning mechanism enables the VLM to learn from physical interactions, we need an additional capability during inference - the ability to imagine and evaluate hypothetical futures without actually executing actions in the environment. To address this, we develop a diffusion-based dynamics model (DDM) that efficiently generates predicted visual observations by conditioning on the current observation and a proposed action sequence. This allows the VLM to simulate the consequences of its actions before committing to them.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/diffusion_arch.pdf}
    \vspace{-0.2in}
    \caption{\textbf{Architecture of Diffusion Dynamics Model,} which consists of a latent encoder, text encoder, Diffusion UNet and latent decoder. The latent encoder and text encoder are frozen during training, while Diffusion UNet and latent decoder are finetuned on our task data. $\mathcal{N}$: random noise.}
    \label{fig:diffusion_model}
\end{figure}

Building on advances in diffusion-based generative models~\citep{rombach2021highresolution, ho2020ddpm, song2021score}, we formulate the forward dynamics prediction as an image-to-image translation task. Our diffusion dynamics model takes the current observation $I_t$ and action $a_t$ as input to predict the next observation $I_{t+1}$. Rather than training a diffusion model from scratch, which would require substantial computational resources and training data, we leverage the pretrained Instructpix2pix model~\citep{brooks2022instructpix2pix} that has been trained on large-scale image editing datasets as our base model.

\noindent \textbf{Data.} We curate a dataset for training the diffusion model. To encourage broader coverage of visisted states, the data collection policy is a noised version of the oracle policy.
Due to the difficulty of this task, we also include a few test data points to improve the fidelity and accuracy of the DDM.
Details can be found in App.~\ref{app:ddm}.

\noindent \textbf{Architecture.} The model architecture is shown in Fig.~\ref{fig:diffusion_model}. For the input $(I_t, a_t)$, we first encode them into latent representation $z_t$ and $z_{a_t}$ with pretrained latent encoder and text encoder. Then we feed $z_t$, a sampled noise $\mathcal{N}$ and the action condition $z_{a_t}$ into the diffusion UNet for de-noising. Finally, we decode the predicted $z_{t+1}$ into a future observation $I_{t+1}$ with a latent decoder.

\noindent \textbf{Training.} The training of DDM consists of two separate phases: UNet training and decoder training. The UNet training phase is to learn transformations from $z_t$ to $z_{t+1}$ conditioned on $z_{a_t}$, while the latent decoder training is to adapt the pretrained VAE models into our task domain because our task requires precise reconstruction of small pieces on the table. Since we keep the latent encoder frozen, we can train the two phases in parallel.

\subsection{Reflective Planning}\label{sec:reflection}

With the VLM policy trained via interactive learning and the diffusion model serving as a dynamics proxy to imagine future outcomes, we now introduce our reflective planning mechanism for decision making at inference time. Alg.~\ref{alg:inference} shows the detailed process. We use $\tilde{I}$ and ${\tilde{a}}$ to denote the generated image and action, which are not actually observed or executed in the environment. To get the future image after $H$ steps, where $H$ is the planning horizon, we perform $H$ iterations of action proposal and diffusion generation. At each iteration, the VLM policy is prompted by the goal image $I_g$ and the generated image $\tilde{I}_{t+k}$ at the previous iteration to propose an action $\tilde{a}_{t+k}$. The diffusion model $\tilde{\mathcal{T}}$ then generates the future image $\tilde{I}_{t+k+1}$ conditioned on the previous image $\tilde{I}_{t+k}$ and the action $\tilde{a}_{t+k}$. For the first iteration, the input image $\tilde{I}_t$ is just the current observation $I_t$. After this process of imagination, the generated future image $\tilde{I}_{t+H}$ and the plan $\tilde{a}_{t:t+H-1}$ are concatenated with the goal and current observation, and fed into the VLM policy for reflection. The VLM policy will then output the final action $a_t$ to be executed. Again, action proposal and reflection are performed by the same VLM policy with two different prompt templates, as indicated by the superscripts ``propose'' and ``reflect''.

\begin{algorithm}
    \caption{Reflective Planning (Inference)}
    \label{alg:inference}
    \begin{algorithmic}[1]
    \REQUIRE {current image $I_t$, goal image $I_g$, imagination horizon $H$}
    \STATE $\tilde{I}_t\gets I_t$
    \FOR {$k \gets 0$ to $H-1$}
        \STATE {$\tilde{a}_{t+k} \gets \pi_\text{VLM}^\text{propose}(I_g, \tilde{I}_{t+k})$}
        \STATE {$\tilde{I}_{t+k+1} \gets \mathcal{\tilde{T}}(\tilde{I}_{t+k}, \tilde{a}_{t+k})$}
    \ENDFOR
    \STATE{$a_t \gets \pi_\text{VLM}^\text{reflect}(I_g, I_t, \tilde{I}_{t+H}, \tilde{a}_{t:t+H-1})$}
    \STATE \textbf{Output:} {$a_t$}
    \end{algorithmic}
\end{algorithm}
