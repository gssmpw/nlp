\section{Introduction}\label{sec:intro}


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/teaser.pdf}
    \caption{\textbf{Reflective planning.} Our method uses a VLM to propose actions and a diffusion dynamics model to imagine the future state of executing the plan. The imagined future helps the VLM reflect the initial plan and propose better action.}
    \label{fig:teaser}
\end{figure}

Complex multi-stage manipulation tasks remain a fundamental challenge in robotics~\citep{luo2024cable, kroemer2020reviewgmanipulation, Cui2021TowardNL}, particularly when they require reasoning about sophisticated physical interactions and their consequences over long time horizons. These tasks often involve intricate sequences of actions where each step must account for physical constraints and potential consequences, making them particularly challenging for planning systems. Success requires not only understanding the immediate effects of actions but also their long-term implications, the ability to adapt plans based on execution outcomes, and generalizing to novel scenarios.

While classical planning approaches, such as task and motion planning (TAMP)~\citep{kaelbling2011tamp, garrett2020integratedtamp}, can in principle address such problems, their reliance on predefined symbolic representations and explicit state estimation makes them difficult to apply in settings without known models that require visual perception~\citep{driess2020deepvisualreasoninglearning, wang2021learningcompositional}. This limitation has motivated the search for more flexible approaches to robotic planning. Recent advances in vision-language models (VLMs) have shown remarkable capabilities in processing visual scenes and natural language instructions by leveraging internet-scale knowledge~\citep{pali2023, qwen2023, openai2024gpt4ocard, google2024gemini, llava}. These models can effectively parse complex visual environments and comprehend high-level task descriptions expressed in natural language, making them promising candidates for robotic planning problems~\citep{palm-e2023, rt1, rt2,shi2024yal, liu2024moka},. However, state-of-the-art VLMs still struggle with complex physical reasoning tasks, and this limitation becomes particularly pronounced when precise physics concepts and long-horizon planning are involved~\citep{gao2024physicsVLM, chen2024spatialvlm}.

In this paper, we study how to effectively leverage VLMs' Internet-scale knowledge while addressing their limitations in physical reasoning and long-horizon planning. We focus on a challenging class of robotic manipulation problems that involve sequentially manipulating interlocking objects to achieve desired configurations, as illustrated in Fig.~\ref{fig:tasks}. These tasks are particularly difficult as they require precise understanding of physical constraints, careful reasoning about action sequences, and the ability to plan over extended horizons while maintaining physical feasibility at each step.


To address these challenges, we present a novel test-time computation framework that significantly enhances VLMs' capabilities for multi-stage robotic manipulation tasks. 
The key insight of our method, ReflectVLM, is that by combining VLMs with a reflection mechanism and targeted post-training, we can create a system that better understands physical constraints and their implications for action planning. We use the term ``reflection'' to refer to a process where a VLM iteratively refines its decisions by critically examining the predicted outcomes of its proposed actions, akin to self-critique methods in large language models~\citep{huang2024llmcorrect, wang2023selfinstructaligninglanguagemodels, madaan2024self}. Our approach introduces two key components: (1) a look-ahead mechanism that uses a diffusion-based dynamics model to generate visual predictions of future states resulting from planned actions, and (2)a reflection process that allows the VLM to critique and refine its planned actions by analyzing these predicted outcomes. This combination of visual prediction and iterative refinement allows the VLM to develop a more sophisticated understanding of physical constraints and improve its decision-making capabilities without requiring extensive retraining.

Experimental results demonstrate that our approach significantly outperforms both the latest commercial state-of-the-art VLM models and traditional planning approaches like Monte Carlo Tree Search (MCTS) on this class of problems. Notably, our method achieves superior performance compared to post-training techniques such as supervised fine-tuning (SFT) while using the same amount of labeled data and maintaining computational efficiency. The success of our approach suggests that enhancing VLMs with structured reasoning mechanisms at test time can be a powerful strategy for improving their performance on physically-grounded tasks.

Our primary contribution is the mentioned test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. Through extensive experiments, we demonstrate that our approach not only outperforms existing methods but also maintains computational efficiency. Importantly, while we demonstrate our framework's effectiveness on manipulation tasks, it is designed to be general and can be readily extended to other domains requiring visual understanding and sequential decision-making. This generality suggests broader applications in robotics and autonomous systems where physical reasoning and long-horizon planning are essential.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\textwidth]{figs/training_data_generation.pdf}
    \caption{\textbf{Training data generation.} Training data for the reflection mechanism is collected by relabeling the rollouts. For each timestep, two training examples are generated: (Q1, A1) for action proposal and (Q2, A2) for reflection. $H$ is the imagination horizon, and $h$ is the history length. $a_t^*$ is the action label given by the expert policy.}

    \label{fig:data_dollection}
\end{figure*}

