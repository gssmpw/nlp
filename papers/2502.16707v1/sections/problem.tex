\section{Preliminaries and Problem Statement}\label{sec:problem}
We formulate the multi-stage robotic manipulation planning problem as a partially observable Markov decision process (POMDP), defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{O}, \mathcal{Z})$. 
Here, $\mathcal{S}$ is the state space containing the full physical state of the environment, including object poses and physical properties; $\mathcal{A}$ is the action space consisting of high-level manipulation primitives $\{{\tt pick\ up}, {\tt insert}, {\tt reorient}, {\tt put\ down}\} \times \{\text{objects}\}$, assuming a failure rate $\epsilon$ for each primitive; $\mathcal{T}(s_{t+1}|s_t,a_t)$ represents the transition dynamics capturing physical interactions; $\mathcal{O}$ is the observation space of RGB images; and $\mathcal{Z}(o_t|s_t)$ is the observation model mapping states to images.

Given a goal state $s_g$, the objective is to find a policy $\pi$ that generates a sequence of actions to reach $s_g$. Due to partial observability, the policy only has access to image observations, taking the form $\pi(a_t|I_t,I_g)$ where $I_t$ is the current observation and $I_g$ is the goal image. The policy is instantiated as a VLM agent $\pi_\text{VLM}$, which takes a multi-modal input of images and text, and generates action primitives in the form of text.

%%
Our framework includes a pre-training phase and a post-training phase. The post-training phase builds on the framework of interactive imitation learning~\citep{dagger, Kelly2018HGDAgger}, which learns a policy by interacting with environment and receiving expert supervision in real-time. Thus under the standard assumption, we assume access to an interactive expert policy $\pi_E$ that generates near-optimal actions $a^*=\pi_E(s)$ for any state $s$ at training time. 
In this paper, we instantiated such an expert policy with access to the full state of the environment to generate optimal actions, though it could be obtained via other formats as well, e.g., human demonstrations.
However, the VLM policy will only have access to image observations.



