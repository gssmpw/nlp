\section{Related Work}\label{sec:related_work}
Our framework incorporates a VLM with the reflection mechanism to solve long-horizon robotic planning problems. We therefore survey reflection techniques in the broader context in large models, VLM for robotic planning, as well as existing techniques for solving robot task and motion planning.

\subsection{Reflection}
Recent work has shown that large language models can benefit from reflection mechanisms - processes where models iteratively refine their outputs through self-critique and revision~\citep{renze2024self, shinn2024reflexion, pan2023automatically, madaan2024self, asai2023self, wang2023selfinstructaligninglanguagemodels, huang2024llmcorrect}. For example,~\citet{madaan2024self} introduced an iterative refinement approach where models critique and improve their own outputs through self-feedback. Chain-of-thought prompting and its variants~\citep{wei2022chain, wang2022self, yao2024tree} demonstrated that guiding models to show their reasoning process leads to better performance. Similarly,~\citet{cheng2024vlmimproving, yu2025exactteachingaiagents} extended such reflection mechanisms to vision-language models.

However, these approaches focus primarily on language-only or visual comprehension tasks, without addressing physical reasoning or robotics applications. Our work extends reflection to long-horizon robotic planning by incorporating a diffusion model that generates imagined future visual states. This allows the VLM to reflect on and revise its plans based on concrete visual predictions rather than relying solely on symbolic reasoning.

\subsection{VLM for Robotic Planning} 
In robotics, several recent works have explored using VLMs for planning~\citep{palm-e2023, rt1, rt2, hu2023lookleapunveilingpower, huang2023voxposer, belkhale2024rth, nasiriany2024pivot, liu2024moka, shi2024yal, wake2024gpt}. However, these approaches either rely on symbolic state representations or make decisions in a single-step manner based only on current observations, without explicitly reasoning about future consequences or utilizing reflection mechanisms.

While ReplanVLM~\citep{mei2024replanvlm} and GameVLM~\citep{mei2024gamevlm} use VLMs to replan robot actions based on execution feedback, they still rely on symbolic state representations rather than visual imagination of future states.~\citet{black2023susie} utilized a diffusion model to generate future visual states and executed them with a low-level goal-conditioned policy, but did not leverage these predictions for plan reflection or revision.~\citet{du2023videolanguageplanning} combines a VLM with video prediction for beam search, but suffers from prediction error accumulation and struggles with physics-based reasoning tasks.

Our framework addresses these limitations by enabling VLMs to imagine and evaluate potential future states through a diffusion-based dynamics model. This allows for sophisticated multi-step planning while maintaining the benefits of VLMs' pre-trained visual-language understanding. The reflection mechanism further enables the VLM to critique and refine its plans based on these imagined futures, leading to more robust long-horizon manipulation.

\subsection{Robotic Task and Motion Planning}
Robotic Task and Motion Planning (TAMP) has been extensively studied~\citep{kaelbling2011tamp,garrett2020integratedtamp, garrett2020pddlstream}. Traditional approaches often combine symbolic planning with motion planning but struggle with real-world physical interactions and visual inputs. Learning-based methods~\citep{wang2021learningcompositional,driess2020deepvisualreasoninglearning} show promise in handling uncertainty and complex dynamics but typically require significant task-specific engineering.

Our approach bridges this gap by leveraging VLMs' broad knowledge while adding structured physical reasoning through visual imagination and reflection. This enables robust long-horizon planning without requiring extensive task-specific engineering or large amounts of training data.