\section{Experiments}\label{sec:experiment}
Our experiments evaluate the effectiveness of our method and analyze its key components. We aim to answer three key research questions. First, how well does our method perform in long-term planning, particularly when handling complex physical interactions? Second, how effectively does our method generalize across different object configurations and types, while maintaining the ability to reason and plan reactively in dynamic environments? Third, what is the impact of the reflection mechanism on the overall performance of our method?
To address these questions, we conduct comprehensive experiments comparing ReflectVLM against: (1) state-of-the-art VLM models tested in zero-shot fashions, (2) model-based planning approaches like MCTS, and (3) ablation studies examining the reflection mechanism. In this section, we first describe our experimental setup, followed by quantitative results and qualitative analysis.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\textwidth]{figs/main_comparison.pdf}
    \vspace{-0.1in}
    \caption{\textbf{Performance of our method and baselines.} Success rate (\%) on 100 tasks. For the zero-shot test of state-of-the-art VLMs and MCTS, the experiments were conducted once; for other methods, the results are the average of five seeds.}
    \label{fig:main_comparison}
\end{figure*}

\subsection{Experiment Setup and Policy Training}
To evaluate the generalization capabilities of different models, we generate two distinct task sets: a training set using the procedure described in Sec.~\ref{sec:tasks}, and a separate evaluation set containing previously unseen configurations. The evaluation tasks are specifically designed to test generalization across varying object configurations, colors, and spatial arrangements. We particularly emphasize challenging scenarios that require sophisticated physical reasoning and multi-step planning. For instance, some tasks begin with objects in physically obstructing positions that prevent direct task completion - requiring the policy to first remove the obstructing pieces and then develop a new plan for the original objective.
Specifically, the training set contains 1000 different tasks, each generated task was randomized to five different initial spatial arrangements, these tasks are used to pre-train the VLM policy. At each iteration of post-training, we randomly sample 200 out of these 1000 tasks to further train the VLM policy with the reflection mechanism.
The evaluation set contains 100 different tasks that are unseen in the training set.

As mentioned in Sec.~\ref{sec:problem}, our method utilizes an oracle policy operating in the environment's symbolic state space to generate expert demonstrations for training. This oracle achieves a 97\% success rate across tasks, but importantly, it operates with access to ground-truth state information. In contrast, our VLM policy must rely solely on visual observations. While alternative data sources like human demonstrations could be used for training, we chose this oracle-based approach to systematically study our method's capabilities under controlled conditions.

During the policy pre-training phase, we utilize the oracle policy to provide action labels, then finetune an LLaVa-1.5-13B model~\cite{llava, llava1.5} with standard supervised learning loss. This pre-training used 5,000 expert demonstrations (1,000 unique tasks × 5 initial configurations per task). In the post-training phase, we use the same oracle policy to further train the VLM policy from the previous stage using the procedure described in Alg.~\ref{alg:training}. For each iteration of post-training, we collect 1k trajectories by rolling out the VLM policy in the environment to generate examples for fine-tuning. See App.~\ref{sec:app_training_details} for training details.


\subsection{Experiment Results}
In this subsection,  we report the results of different methods, and discuss their implications. Unless otherwise noted, numbers are reported across five runs, for some commercial VLMs such as GPT-o1, we only report one run due to cost consideration. 


\begin{table}[t]
    \centering
        \centering
        \caption{{\textbf{Post-training performance} Success rates (\%) of post-training variants over the number of iterations.}}
        \label{tab:dagger_results}
        \begin{tabular}{l|ccc}
        \toprule Method             &  Iter. 1  & Iter. 2  & Iter. 3  \\
        \midrule
            w/o reflect	            &58.2    & 74.4 & \textbf{77.8}  \\
            w/o reflect@test	    &64.4    & 76.0 & \textbf{82.2} 	\\
            reflect w/ diffusion	&66.2    & 75.8 & \textbf{82.4} \\
            reflect w/ sim          &66.8   & 75.4 & \textbf{85.4} \\
        \bottomrule
        \end{tabular}%    % Added % to prevent unwanted space
\end{table}
\begin{table}[t!]
    \centering
        \centering
        \caption{\textbf{Inference computation cost.} Inference wall clock time per step. MCTS result is averaged over 100 tasks and 1 seed; the others are averaged over 100 tasks and 5 seeds. All experiments are done on a single A100 GPU.}
        \label{tab:inference_cost}
        % \centering

        \begin{tabular}{l|r}
        \toprule Method &  Inference time (s) \\
        \midrule
            Ours w/o reflect@test & 0.45  \\
            Ours w/ diffusion	& 11.10 	\\
            Ours w/ sim & 6.05 \\
            MCTS & 391.42 \\
        % \midrule
        % {Average}	& {17.4}	\\
        \bottomrule
        \end{tabular}%    % Added % to prevent unwanted space
\end{table}
\paragraph{VLM zero-shot} To evaluate the capabilities of state-of-the-art vision-language models, we tested several leading VLMs including LLaVAOneVision~\citep{llavaonevision}, Gemini-2.0-flash~\citep{google2024gemini}, Gemini-2.0-flash-thinking~\citep{google2024gemini}, GPT-4o~\citep{openai2024gpt4ocard}, and GPT-o1~\citep{openai2024openaio1card}. We specifically included Gemini-2.0-flash-thinking and GPT-o1 as they have demonstrated superior reasoning capabilities across various VLM benchmarks. As shown in Fig.~\ref{fig:main_comparison}, all models achieved notably low success rates on our tasks. Even GPT-o1, currently the most advanced proprietary model, succeeded in only 15 out of 100 tasks, primarily on simpler cases that did not require sophisticated physical reasoning about interlocking mechanisms. While Gemini-2.0-flash-thinking and GPT-o1 showed marginally better performance compared to other models, indicating some improved reasoning capabilities, their performance remains insufficient for solving our complex manipulation tasks. This significant performance gap confirms the necessity of our proposed method for handling physically-grounded reasoning tasks. Detailed evaluation procedures and results can be found in App.~\ref{sec:app_baseline_details}.



\paragraph{MCTS} To compare with model-based planning approaches, we implemented a VLM-based MCTS policy. This implementation uses our pretrained VLM policy as a base policy for generating candidate actions when expanding tree nodes, with value estimation provided by the oracle policy from the simulator. See App.~\ref{sec:app_baseline_details} for implementation details. As shown in Fig.~\ref{fig:main_comparison}, MCTS achieves a 24.0\% success rate—higher than zero-shot VLMs but lower than our method. Notably, while the pretrained VLM policy alone achieves a 47.8\% success rate, adding MCTS actually degrades performance. Our analysis revealed that although MCTS helped with some challenging tasks, it would sometimes incorrectly override valid plans from the base VLM policy. We found MCTS to be particularly challenging to tune effectively for our domain for several reasons: (1) it is highly sensitive to value function quality, (2) our tasks require nuanced physical reasoning that is difficult to capture in a value function, and (3) the possibility of succeeding from any state (by clearing the board and starting over) creates minimal value differences between states. These limitations highlight the advantages of our proposed method, which offers a lightweight, flexible approach that requires minimal tuning and can be readily integrated with any VLM policy.


\paragraph{ReflectVLM} Our full method outlined in Alg.~\ref{alg:training} and~\ref{alg:inference} incorporates reflection mechanisms in both training and inference phases. To systematically evaluate the impact of reflection, we conducted ablation experiments across several variants of our method.
As reported in Fig.~\ref{fig:main_comparison}, the variant without reflection in both training and inference achieved the lowest performance among our method's variants, though it still significantly outperformed the pretrained VLM baseline. The full method using a simulator during inference achieves the highest success rate, serving as an upper bound for our method's performance. When using a diffusion model instead of a simulator during inference, performance degrades slightly. This is unsurprising, as our tasks require nuanced understanding of physics and temporal dynamics—areas where current generative models still face challenges~\citep{kang2024farvideogenerationworld, motamed2025generativevideomodelslearn}. We expect our method's performance to improve as generative models advance.
We also report the post-training dynamics in Table~\ref{tab:dagger_results}. It's observed that the performance of all variants increases as more training is performed and the full method did achieve the highest performance as mentioned above.
While the absolute performance gap between variants may appear modest, the additional tasks solved by including reflection are qualitatively significant. These are typically complex scenarios requiring multiple replanning attempts, such as removing previously placed objects to explore alternative solutions—tasks the pretrained VLM consistently failed to solve. Notably, even without reflection during inference, our method achieves higher success rates than the pretrained baseline. This suggests that the natural language reflection prompts during training help the VLM policy develop better implicit reasoning capabilities. 
Fig.~\ref{fig:filmstrip} illustrates a representative example. In this complex task, the reflection mechanism iteratively revised suboptimal actions initially proposed by the VLM policy by identifying potentially unfavorable future states. This reflection capability proved crucial for success, as the long-horizon nature of the task required reactive planning and continuous adjustment of the solution strategy. 
Another point to consider is computation efficiency. Table~\ref{tab:inference_cost} shows the wall-clock time required per inference step. Compared to MCTS, our method requires only a fraction of the computation time while achieving substantially higher performance, making it particularly appealing as a lightweight and flexible solution for real-world applications.

