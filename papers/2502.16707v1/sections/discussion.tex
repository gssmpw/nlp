\section{Discussion}\label{sec:discussion}
In this work, we presented a novel post-training strategy with reflection to improve VLM policies for long-horizon manipulation tasks, demonstrating superior planning capabilities with significantly less compute than traditional approaches like MCTS. Our current implementation opens up exciting future directions: while we currently use final outcomes for reflection due to VLM context constraints, future architectures with expanded context windows could enable richer intermediate feedback for more precise action refinement; the diffusion model's generation capabilities could be augmented with physical constraints and improved architectures to enhance prediction stability over longer horizons; and our single-round reflection approach could be extended to multiple rounds for iterative refinement while maintaining computational efficiency. 
We believe our method would benefit from continued advances in VLMs and generative models, and we hope it could establish a new foundation with broad applicability to sequential decision-making domains requiring visual understanding, physical reasoning, and long-horizon planning.
