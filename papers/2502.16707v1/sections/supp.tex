\section{Task generation}\label{sec:app_task_gen}

We here describe the procedure to generate assembly boards in detail with an example. A board is discretized into voxels and can be represented by a 3d array, where each value indicates the piece the voxel belongs to. Initially none of the voxels is occupied, so they are all set to an empty value 0, as shown in Fig.~\ref{fig:task_gen}(a). Then we iteratively add pieces to the board. We first sample the size of the base board, which is (12, 12, 3) in this example (Fig.~\ref{fig:task_gen}(b)). Then we set these voxels to 1 to indicate they belong to the base board. We also maintain a variable {\tt max\_height}, which represents the highest layer that contains non-zero voxels. To generate a brick, we sample its size and position subject to some constraints (Fig.~\ref{fig:task_gen}(c)). The first two constraints ensure that this brick is within the range of the base board, and the third constraint makes sure this brick will intersect with some previously generated brick. As before, we set the value of the red voxels to 2 to indicate they are from the new brick. Note that the voxels in the lower layer previously have a value of 1 since they belonged to the base board, but now their value is rewritten to 2. This also creates a hole on the base board. After generating this brick, we also update ${\tt max\_height}$ to 4 since we have 4 layers now. Fig.~\ref{fig:task_gen}(d) shows the process of generating another brick. As the new blue brick intersects with the old red brick at the four critical voxels highlighted in purple (Fig.~\ref{fig:task_gen}(e)), we can assign the value of these critical voxels to either that of the red one or the blue one. For example, keep these voxels to the red brick results in an opening on the blue one (Fig.~\ref{fig:task_gen}(f)). Stopping the generation process here gives us a board with three interlocking pieces, as shown in Fig.~\ref{fig:task_gen}(g).


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{figs/task_generation.pdf}
    \caption{\textbf{Example of task generation.} (a) Voxel representation of the board. (b) Generating a base board. (c) Generating a red brick. (d) Generating another blue brick. (e) Critical voxels (highlighted in purple) at the intersection of the two bricks. (f) Handling intersection by assigning the critical voxels to the red brick. (g) Explosion view of the board consisting of three interlocking pieces.}
    \label{fig:task_gen}
\end{figure}

\section{Samples of generated tasks}\label{sec:app_more_task_samples}
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{figs/more_task_samples7.pdf}
    \caption{\textbf{Samples of generated tasks.} We procedurally generate a variety of multi-stage manipulation tasks, ranging from simple peg insertion to complex assembly tasks that contains multiple interlocking pieces. Top: initial configurations. Bottom: goal configurations.}
    \label{fig:more_task_samples}
\end{figure*}


\newpage
\section{Expert policy}\label{sec:app_expert}

The expert policy assumes access to the states of the objects in the simulator, such as the position and orientation of each piece. It is also provided with the dependency graph of the task, as discussed in Sec.~\ref{sec:tasks}. We define the status of each piece to be one of the following:
\vspace{-10pt}
\begin{itemize}
    \setlength{\topsep}{0pt}     % Reduce space before the list
    \setlength{\partopsep}{0pt}  % Reduce additional paragraph spacing
    \setlength{\itemsep}{0pt}    % Reduce space between items
    \setlength{\parskip}{0pt}
    \item {\tt DONE}: if it is properly inserted into board;
    \item {\tt READY}: if it is not inserted yet but ready to be manipulated; 
    %(Note: this only ensures that the bricks that should be inserted BEFORE this one are inserted already. It's still possible that this brick cannot be inserted right now because some brick that should be inserted AFTER this one is inserted first and causes blocking, in which case that brick should have a BAD state and should be removed first.)
    \item {\tt BAD\_B}: if it is in {\em{bad}} state since it is {\em{blocking}} other bricks, implying it needs to be removed;
    \item {\tt BAD\_D}: if it is in {\em{bad}} state since it is {\em{down}}, implying it needs to be reoriented;
    \item {\tt BLOCKED\_P}: if it is {\em{blocked}} since some {\em{predecessor}} brick(s) should be inserted before;
    \item {\tt BLOCKED\_S}: if it is {\em{blocked}} since some {\em{successor}} brick(s) is inserted before.
\end{itemize}
\vspace{-10pt}
Based on the status of each piece, we can also define a set of possible statuses for the whole assembly task:
\vspace{-10pt}
\begin{itemize}
    \setlength{\topsep}{0pt}     % Reduce space before the list
    \setlength{\partopsep}{0pt}  % Reduce additional paragraph spacing
    \setlength{\itemsep}{0pt}    % Reduce space between items
    \setlength{\parskip}{0pt}
    \item {\tt DONE}: if the board is fully assembled, i.e., all pieces are in {\tt DONE} state;
    \item {\tt READY}: if some brick is in {\tt READY} or {\tt BAD\_D} state;
    \item {\tt BAD\_B}: if we need to reset some brick(s) to proceed as it is blocking other bricks.
\end{itemize}
\vspace{-10pt}
When queried, the expert policy first checks the status of each piece according to the simulation states, and decide the status of the whole task based on the statuses of all pieces. Then it decides the action to take following Algorithm~\ref{alg:expert}.

\begin{algorithm}
    \caption{Expert Policy}
    \label{alg:expert}
    \begin{algorithmic}[1]
    \REQUIRE {task status $status_\text{global}$, object in hand $obj_\text{hand}$, }

    \IF {$obj_\text{hand}$ is not None}
        \IF {all predecessors of $obj_\text{hand}$ are {\tt DONE}}
            \IF {$obj_\text{hand}$ is in {\tt BAD\_D} state}
                \STATE {\textbf{return} ``reorient $obj_\text{hand}$"}
            \ELSIF {$obj_\text{hand}$ is in {\tt BLOCKED\_S} state}
                \STATE {\textbf{return} ``put down $obj_\text{hand}$"}
            \ELSE
                \STATE {\textbf{return} ``insert $obj_\text{hand}$"}
            \ENDIF
        \ELSE
            \STATE {\textbf{return} ``put down $obj_\text{hand}$"}
        \ENDIF
    \ELSE
        \IF {$status_\text{global}$ == {\tt READY}}
            \STATE {choose an object $obj$ in {\tt READY} or {\tt BAD\_D} state}
            \STATE {\textbf{return} ``pick up $obj$"}
        \ELSIF {$status_\text{global}$ == {\tt BAD\_B}}
            \STATE {choose an object $obj$ in {\tt BAD\_B} state}
            \STATE {\textbf{return} ``pick up $obj$"}
        \ELSE
            \STATE {\textbf{return} ``done"}
        \ENDIF
    \ENDIF
    \end{algorithmic}
    \end{algorithm}

\section{Training details}\label{sec:app_training_details}
\subsection{VLM Policy}
\label{subsec:vlm_policy}

\noindent \textbf{Architecture.} As shown in Fig.~\ref{fig:vlm_arch}, the architecture of our VLM consists of a vision encoder and a Large Language Model (LLM). By default, we use \texttt{clip-vit-large-patch14-336}~\footnote{\url{https://huggingface.co/openai/clip-vit-large-patch14-336}} as the vision encoder, and \texttt{vicuna-13b-v1.5}~\footnote{\url{https://huggingface.co/lmsys/vicuna-13b-v1.5}} as the LLM. We initialize our VLM with LLaVA-v1.5 weights~\footnote{\url{https://huggingface.co/liuhaotian/llava-v1.5-13b}} that are pre-trained on general visual instruction tuning datasets. Since our task prompts consist of interleaved images and text (refer to Sec.~\ref{sec:app_prompts}), we use a shared vision encoder to extract latent embeddings and concatenate them back to an input sequence. 



\noindent \textbf{Training Parameters.} The full training parameters are listed in Table~\ref{tab:vlm_training_params}. For efficient adaptation of VLM to our task, we only finetune newly added LoRA~\cite{hu2022lora} layers. The rank of LoRA layers is 128 by default.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/VLM_arch.pdf}
    \caption{\textbf{Architecture of our VLM.} The model consists of a vision encoder and an LLM. We also add Low-Rank Adaptation (LoRA)~\cite{hu2022lora} layers to LLM for efficient adaptation. The input sequence contains interleaved images and text, where images are encoded into latent embeddings with a shared vision encoder. Finally, the concatenation of text and image embeddings are fed into VLM for multimodal reasoning.}
    \label{fig:vlm_arch}
\end{figure*}

\begin{table}[h]
\caption{\textbf{Training parameters of VLM.}}
\label{tab:vlm_training_params}
\centering
\begin{tabular}{clcccccccc}
\toprule
\multirow{2}{*}{Res} & LoRA & Training & Batch & \multirow{2}{*}{Optimizer} & Warmup & \multicolumn{2}{c}{Learning rate}       & Weight & LR       \\
                     & Rank & Epoch    & Size  &                            & Epoch  & BC & Iter.~1,2,3 & Decay  & Schedule \\
                     \midrule
336px                & 128  & 1        & 128   & AdamW                      & 0.03   & 5e-5   & 1e-5                & 0.0    & Cosine  \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Diffusion Dynamics Model}\label{app:ddm}

\noindent \textbf{Data Generation.} We generate 10$K$ different boards and use sub-optimal policies to collect transitions. The sub-optimal policies are implemented by setting a probability $p=\{0.2, 0.5, 0.7, 0.9, 1.0\}$ to replace the expert action by a random action. We collect 50$K$ trajectories; each has a maximum length of 50 and is terminated upon success. In total, we have about 1$M$ transitions. We randomly sample 50$K$  transitions for evaluation, and the rest is used for training.


\noindent \textbf{Training Parameters.} The full training parameters are listed in Table~\ref{tab:diffusion_training_params}. We initialize the Diffusion Dynamics Model with pretrained Instructpix2pix~\cite{brooks2022instructpix2pix}~\footnote{\url{https://huggingface.co/timbrooks/instruct-pix2pix}}.

\begin{table}[h]
\caption{\textbf{Training parameters of Diffusion Dynamics Models.}}
\label{tab:diffusion_training_params}
\centering
\begin{tabular}{c|cccccccccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Res} & Training & Batch & \multirow{2}{*}{Optimizer} & Warmup & Learning & Weight & Beta1,     & Grad & LR       \\
                       &                      & Steps    & Size  &                            & Steps  & Rate     & Decay  & Beta2      & Norm & Schedule \\
                       \midrule
UNet         & 512px                & 20K      & 640   & AdamW                      & 2K     & 1e-4     & 0.01   & 0.9, 0.999 & 1.0  & Cosine   \\
Decoder         & 512px                & 4K       & 160   & AdamW                      & 1K     & 1e-7     & 0.01   & 0.9, 0.999 & 1.0  & Cosine  
\\
\bottomrule
\end{tabular}
\end{table}


\section{Prompts}\label{sec:app_prompts}

\subsection{Action proposal prompt}
\label{subsec:prompt}

\begin{tcolorbox}[width=\textwidth, colback=gray!10, colframe=black, boxrule=0.8pt]
There is a puzzle consisting of a board and several pieces with different colors on the table. The goal is to assemble the puzzle with the robot arm. In each step, one of the following four actions can be taken: pick up [obj], put down [obj], reorient [obj], and insert [obj], where [obj] refers to the piece to be manipulataed. The image of the goal state is: \textless image\textgreater. The image of the current state is: \textless image\textgreater. The most recently executed actions are: {\tt\{history\}}. What action should be taken next? Note that [obj] should be a color chosen from the following list: {\tt\{colors\}}.
\end{tcolorbox}

\subsection{Reflection prompt}

\begin{tcolorbox}[width=\textwidth, colback=gray!10, colframe=black, boxrule=0.8pt]
There is a puzzle consisting of a board and several pieces with different colors on the table. The goal is to assemble the puzzle with the robot arm. In each step, one of the following four actions can be taken: pick up [obj], put down [obj], reorient [obj], and insert [obj], where [obj] refers to the piece to be manipulataed. The image of the goal state is: \textless image\textgreater. The image of the current state is: \textless image\textgreater. The most recently executed actions are: {\tt\{history\}}. The next five steps planned by the model is {\tt\{init\_plan\}}, from which we are going to only execute the first action. Note that if the full plan was executed sequentially, the future state would be: \textless image\textgreater. What action should be taken for the immediate next step? Note that [obj] should be a color chosen from the following list: {\tt\{colors\}}. You can modify the initial plan if it leads to an undesired future state.
\end{tcolorbox}
      


\section{Baseline details}\label{sec:app_baseline_details}
\subsection{Zero-shot VLMs}
We prompt state-of-the-art close-sourced and open-sourced VLMs for zero-shot evaluation, including LLaVA-Onevision, Gemini-2.0 (\texttt{gemini-2.0-flash-exp}), Gemini-2.0-thinking (\texttt{gemini-2.0-flash-thinking-exp-1219}), GPT-4o and GPT-o1. We resize all input images to 336$\times$336 pixels for fair comparisons with our model. We set the generation temperature and max planing step to 0 and 50. The evaluation prompt is:
\begin{tcolorbox}[width=\textwidth, colback=gray!10, colframe=black, boxrule=0.8pt]
You are an intelligent robot equipped with cameras and robotic arms, your primary task is to observe and interact with the objects on the desktop.
\\
\\
\{Action proposal prompt (Sec.~\ref{subsec:prompt})\}
\\
\\
You can only output the action, e.g., pick up red. Do not output anything else.
\end{tcolorbox}

Since the instruction following capability of LLaVA-Onevision is quite limited, we cannot extract valid actions from its response. For other close-sourced VLMs, we list the detailed evaluation results in Table~\ref{tab:zero_shot_details}. We also visualize some success cases in \cref{fig:zero_shot_success_part1,fig:zero_shot_success_part2}, and failure cases in \cref{fig:failure_gemini_2,fig:failure_gemini_2_think,fig:failure_gpt4o,fig:failure_gpto1}.

\begin{table}[thb]
\caption{\textbf{Detailed evaluation results of zero-shot VLMs.}}
\label{tab:zero_shot_details}
\centering
\begin{tabular}{llccc}
\toprule
Model                   & Success Trajectory ID / Planing Steps                & Max Steps           & Min Steps          & Avg Steps             
\\
\toprule
Gemini-2.0              & 5/6, 12/4, 16/18, 47/11, 60/4, 86/6                  & 18                  & 4                  & 8.2                   \\
\midrule
Gemini-2.0-Thinking     & 5/6, 12/4, 40/20, 47/16, 50/8, 60/8, 86/10, 90/11    & 20                  & 4                  & 10.4                  \\
\midrule
GPT-4o                  & 12/15, 16/5, 19/4, 47/10, 60/4, 90/6                 & 15                  & 4                  & 7.3                   \\
\midrule
\multirow{2}{*}{GPT-o1} & 12/9, 16/6, 17/15, 47/8, 50/16, 58/18, 60/14, 62/33, & \multirow{2}{*}{33} & \multirow{2}{*}{4} & \multirow{2}{*}{13.1} \\
                        & 66/6, 67/12, 72/32, 77/9, 85/9, 86/6, 90/4           &                     &                    &
                        \\ \bottomrule
\end{tabular}
\end{table}

\subsection{MCTS}

We implemented MCTS similar to AlphaGo Zero~\cite{alphagozero} but with a VLM policy for action proposal and a heuristic value estimator. States and actions are represented by nodes and edges, respectively. The algorithm iteratively expands the search tree and estimates the value for different actions. We store the visit count $N(s,a)$, total action value $W(s,a)$, and action value $Q(s,a)=W(s,a)/N(s,a)$ on edges. Each iteration consists of three phases: (1) select, (2) expand, and (3) backup.

In select phase, it traverses the tree by selecting the edge that has the largest action value $Q(s, a)$ plus an upper confidence bound $U(s, a)=c_\text{explore}{\sqrt{\sum_{a'} N(s,a')}}/{(1+N(s,a))}$, 
where $c_\text{explore}$ is the factor to balance exploring less visited edges and exploiting edges with high value. We use $c_\text{explore}=0.5$ in our experiments. If there is no actions associated to a node yet, it samples 5 top-likelihood actions with the VLM, with duplicates removed, and adds them to the node. 

In expand phase, it expands the selected edge by simulating the action in the simulator, getting the next state, and adding the new state to the tree as a new node. It then estimates the value of the new state by rolling out the expert policy from that state. The estimated value is $V=\exp(-\lambda S)$, where $S$ is the number of steps the expert policy takes to reach the goal from the new state, and $\lambda=0.1$ is a scaling factor.

In backup phase, it updates the statistics of the edges on the path from the root to the expanded node: $N(s,a)\leftarrow N(s,a)+1$, $W(s,a)\leftarrow W(s,a)+V$, and $Q(s,a)\leftarrow W(s,a)/N(s,a)$.

The search completes after 50 iterations. Among all actions connected to the root node, the action with the highest $Q$ value is chosen to execute. We replan with MCTS at each timestep. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/zero_shot_results/gemini-2-success.png}
    \\
    \vspace{5mm}
    \includegraphics[width=\linewidth]{figs/zero_shot_results/gemini-2-think-success.png}
    \\
    \vspace{5mm}
    \includegraphics[width=\linewidth]{figs/zero_shot_results/gpt4o-success.png}
    % \\
    % \includegraphics[width=\linewidth]{figs/zero_shot_results/gpto1-success.png}
    \caption{\textbf{Success cases of zero-shot VLMs.} Top: Gemini-2.0; Middle: Gemini-2.0-Thinking; Bottom: GPT-4o.}
    \label{fig:zero_shot_success_part1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/zero_shot_results/gpto1-success.png}
    \caption{\textbf{Success cases of zero-shot VLMs (GPT-o1).}}
    \label{fig:zero_shot_success_part2}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{figs/zero_shot_results/gemini-2.png}
    \caption{\textbf{Failure case of Gemini-2.0.}}
    \label{fig:failure_gemini_2}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{figs/zero_shot_results/gemini-2-think.png}
    \caption{\textbf{Failure case of Gemini-2.0-Thinking.}}
    \label{fig:failure_gemini_2_think}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{figs/zero_shot_results/gpt4o.png}
    \caption{\textbf{Failure case of GPT-4o.}}
    \label{fig:failure_gpt4o}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{figs/zero_shot_results/gpto1.png}
    \caption{\textbf{Failure case of GPT-o1.}}
    \label{fig:failure_gpto1}
\end{figure}


\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{figs/diffusion_cases.pdf}
    \caption{\textbf{Examples of Diffusion Dynamic Models.}}
    \label{fig:diffusion_cases}
\end{figure}
