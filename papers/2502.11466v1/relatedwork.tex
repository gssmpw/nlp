\section{Related Work}
We classify related works into distillation and self-training based on whether the synthetic data is generated by stronger LLMs or the LLM undergoing training itself.
\paragraph{Distillation}
Code Alpaca~\cite{codealpaca}, similar to Self-Instruct~\cite{selfinstruct}, leverages the in-context learning ability of ChatGPT to generate new description-code pairs. WizardCoder~\cite{wizardcoder} prompts ChatGPT with five tailored heuristics to improve the difficulty of existing descriptions in Code Alpaca. Magicoder~\cite{magicoder} and WaveCoder~\cite{wavecoder} highlight the importance of data diversity and quality by prompting ChatGPT to create new pairs based on open-sourced codes on the web instead of LLM-generated Code Alpaca. 
MathGenie~\cite{mathgenie} improves from the solution side where it augments the solutions by prompting an external LLM with heuristics and then back-translates augmented solutions into math problems in order to create new problems. However, stronger LLMs are not always available, which limits the generalizability of distillation methods.

\paragraph{Self-training}
Self-training refers to making LLMs learn from their own outputs based on a set of seed descriptions. Self-training approaches can be categorized into two directions based on whether additional data is synthesized on the description side or the code side. On the description side, Instruction Backtranslation~\cite{instruction-backtranslation} and InverseCoder~\cite{inversecoder} ask an LLM to generate synthesized descriptions for unlabeled codes for instruction tuning.
On the code side, Self-Taught Reasoner (STaR)~\cite{star} is a pioneering work that generates a single rationale for each reasoning problem. LMSI~\cite{lmsi} and Rejection Fine-tuning (RFT)~\cite{rft} enhance STaR by generating multiple rationales per problem. While STaR, LMSI, and RFT rely on the ground truth answer to filter out incorrect rationales, SelfCodeAlign~\cite{selfcodealign} additionally asks LLMs to generate test cases for synthesized codes to conduct self-validation. Reinforced Self-Training (ReST)~\cite{rest} and ReST$^{EM}$~\cite{restem} expand the RFT process into an iterative one, where the generate-then-fine-tune process is repeated multiple times until no further improvement is observed.

Though there are intermediate descriptions generated in GiFT, we do not use those intermediate descriptions for fine-tuning, as GiFT is mainly proposed to improve the data quality on the code side.
GiFT is orthogonal to the self-training methods on the code side as each synthetic description can benefit from higher-quality codes generated in GiFT. Besides, GiFT is beneficial under the distillation setting. We empirically demonstrate the effectiveness of data from GiFT in Section~\ref{sec:exp}.