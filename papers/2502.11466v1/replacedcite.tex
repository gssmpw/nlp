\section{Related Work}
We classify related works into distillation and self-training based on whether the synthetic data is generated by stronger LLMs or the LLM undergoing training itself.
\paragraph{Distillation}
Code Alpaca____, similar to Self-Instruct____, leverages the in-context learning ability of ChatGPT to generate new description-code pairs. WizardCoder____ prompts ChatGPT with five tailored heuristics to improve the difficulty of existing descriptions in Code Alpaca. Magicoder____ and WaveCoder____ highlight the importance of data diversity and quality by prompting ChatGPT to create new pairs based on open-sourced codes on the web instead of LLM-generated Code Alpaca. 
MathGenie____ improves from the solution side where it augments the solutions by prompting an external LLM with heuristics and then back-translates augmented solutions into math problems in order to create new problems. However, stronger LLMs are not always available, which limits the generalizability of distillation methods.

\paragraph{Self-training}
Self-training refers to making LLMs learn from their own outputs based on a set of seed descriptions. Self-training approaches can be categorized into two directions based on whether additional data is synthesized on the description side or the code side. On the description side, Instruction Backtranslation____ and InverseCoder____ ask an LLM to generate synthesized descriptions for unlabeled codes for instruction tuning.
On the code side, Self-Taught Reasoner (STaR)____ is a pioneering work that generates a single rationale for each reasoning problem. LMSI____ and Rejection Fine-tuning (RFT)____ enhance STaR by generating multiple rationales per problem. While STaR, LMSI, and RFT rely on the ground truth answer to filter out incorrect rationales, SelfCodeAlign____ additionally asks LLMs to generate test cases for synthesized codes to conduct self-validation. Reinforced Self-Training (ReST)____ and ReST$^{EM}$____ expand the RFT process into an iterative one, where the generate-then-fine-tune process is repeated multiple times until no further improvement is observed.

Though there are intermediate descriptions generated in GiFT, we do not use those intermediate descriptions for fine-tuning, as GiFT is mainly proposed to improve the data quality on the code side.
GiFT is orthogonal to the self-training methods on the code side as each synthetic description can benefit from higher-quality codes generated in GiFT. Besides, GiFT is beneficial under the distillation setting. We empirically demonstrate the effectiveness of data from GiFT in Section~\ref{sec:exp}.