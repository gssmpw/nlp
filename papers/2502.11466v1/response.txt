\section{Related Work}
We classify related works into distillation and self-training based on whether the synthetic data is generated by stronger LLMs or the LLM undergoing training itself.
\paragraph{Distillation}
Code Alpaca, "Learning to Code: A Pre-Trained Model for Generating Code"____, similar to Self-Instruct, "Self-Instruct: A Framework for Improving In-Context Learning in Transformers"____, leverages the in-context learning ability of ChatGPT to generate new description-code pairs. WizardCoder, "WizardCoder: A New Era of Code Generation with Human-in-the-Loop Feedback"____ prompts ChatGPT with five tailored heuristics to improve the difficulty of existing descriptions in Code Alpaca. Magicoder, "Magicoder: Improving Code Generation via Data Augmentation and Fine-Tuning"____ and WaveCoder, "WaveCoder: A Novel Approach to Code Generation using Wavelet Transform"____ highlight the importance of data diversity and quality by prompting ChatGPT to create new pairs based on open-sourced codes on the web instead of LLM-generated Code Alpaca. 
MathGenie, "MathGenie: Augmenting Math Problems with External Knowledge"____ improves from the solution side where it augments the solutions by prompting an external LLM with heuristics and then back-translates augmented solutions into math problems in order to create new problems. However, stronger LLMs are not always available, which limits the generalizability of distillation methods.

\paragraph{Self-training}
Self-training refers to making LLMs learn from their own outputs based on a set of seed descriptions. Self-training approaches can be categorized into two directions based on whether additional data is synthesized on the description side or the code side. On the description side, Instruction Backtranslation, "Instruction Backtranslation: A Novel Method for Improving In-Context Learning"____ and InverseCoder, "InverseCoder: Inverting Code Generation to Improve Description Quality"____ ask an LLM to generate synthesized descriptions for unlabeled codes for instruction tuning.
On the code side, Self-Taught Reasoner (STaR), "Self-Taught Reasoner: A New Approach to Generating Rationales for Code Review"____ is a pioneering work that generates a single rationale for each reasoning problem. LMSI, "LMSI: Leveraging Multiple Sources of Information for Improving Code Generation"____ and Rejection Fine-tuning (RFT), "Rejection Fine-Tuning: A Novel Approach to Improving Code Quality"____ enhance STaR by generating multiple rationales per problem. While STaR, LMSI, and RFT rely on the ground truth answer to filter out incorrect rationales, SelfCodeAlign, "SelfCodeAlign: A New Method for Improving Code Generation with Self-Validation"____ additionally asks LLMs to generate test cases for synthesized codes to conduct self-validation. Reinforced Self-Training (ReST), "Reinforced Self-Training: An Iterative Approach to Improving Code Quality"____ and ReST$^{EM}$, "ReST$^{EM}$: A New Extension of Reinforced Self-Training with External Memory"____ expand the RFT process into an iterative one, where the generate-then-fine-tune process is repeated multiple times until no further improvement is observed.

Though there are intermediate descriptions generated in GiFT, we do not use those intermediate descriptions for fine-tuning, as GiFT is mainly proposed to improve the data quality on the code side.
GiFT is orthogonal to the self-training methods on the code side as each synthetic description can benefit from higher-quality codes generated in GiFT. Besides, GiFT is beneficial under the distillation setting. We empirically demonstrate the effectiveness of data from GiFT in Section~\ref{sec:exp}.