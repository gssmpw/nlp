To set the stage for our analysis, we set forth, in this section, we define and provide an overview of MU and its key concepts: 

\subsection{Formal Definition of Unlearning}
Let \( M \) be a model trained on a dataset \( D \) using a training algorithm \( A \). Here, we do not distinguish between \( M \) and its parameters, and write \( M = A(D) \). An \textbf{unlearning query} is typically identified by a set of data points to be forgotten, referred to as the \textbf{forget-set} \( D_f \), and the remaining data points called the \textbf{retain-set}, \( D_r = D \setminus D_f \). The goal of an \textbf{unlearning algorithm} \( U \) is to remove from \( M \) the influence of \( D_f \). The outcome is a new model, called the \textbf{unlearned model}, denoted as \( M_u = U(M; D_f, D_r) \). Depending on the unlearning approach, \( U \) may not have access to \( D_r \) \cite{zhao2024unlearningdifficulty}. In MU research, the notion of "removing influence" has been formalized using definitions from \textbf{differential privacy}. We borrow a version of this definition from \citep{ginart2019makingaiforget} as follows:


\textbf{Definition 2.2.1} Assume that \(U\) and \( A \) are stochastic processes. \(U\) is called an \((\epsilon, \delta)\)-unlearner if the distribution of \( A(D_r) \) and \( U(M; D_f, D_r) \) is \((\epsilon, \delta)\)-close. Specifically, two distributions \( \mu \) and \( \nu \) are \((\epsilon, \delta)\)-close if, for all measurable events \( B \), the following inequalities hold: \(\mu(B) \leq e^\epsilon \nu(B) + \delta\) and \(
\nu(B) \leq e^\epsilon \mu(B) + \delta
\).

This definition provides a natural taxonomy for MU algorithms. When \( \epsilon = \delta = 0 \), \( U \) is termed \textbf{exact unlearning}; otherwise, it is referred to as \textbf{approximate unlearning}.

\subsection{Informal Definitions}
While Def. 2.2.1 provides a rigorous mathematical formulation for MU, researchers often rely on informal interpretations, typically phrased as \texttt{removing [\dots] from $M$}. Deriving informal definitions from Def. 2.2.1 is not always straightforward. A key challenge is that the entity to be removed, may not be clearly identifiable --e.g., in generative models, \texttt{[\dots]} often corresponds to a fact or concept that lacks an explicit representation in either $M$ or $D$.

However, we contend that overly broad definitions of MU introduce unnecessary complexity, potentially hindering clear scientific discourse. In this paper we shall limit unlearning to those techniques in ML that modify the parameter-set of the model (e.g. by deleting and retraining, fine-tuning, or adding/removing some parameters). With this condition, MU still can serve as a tool--among others--for applications such as safeguarding and alignment. At the same time, it leaves methods such as guardrailing (or any "output suppression" as per \cite{cooper2024machineunlearningdoesntthink}) independent of MU, which deserve their own distinct discussion and evaluation in the context of regulations and beyond.

%For instance, recent work \cite{cooper2024machineunlearningdoesntthink} encompasses output safeguarding techniques under the umbrella of unlearning. This expansive definition can potentially complicate the development of scientifically precise and generalizable arguments about unlearning, diluting its conceptual clarity.}



\subsection{Evaluation metrics} 

While Definition 2.2.1 is widely accepted in the MU community, it presents several challenges in practical settings. First, some works question whether this definition is necessary or sufficient to achieve true MU \cite{thudi2022necessityauditing}. Second, in large-scale applications, it is computationally infeasible to directly measure the closeness between the distributions \( A(D_r) \) and \( U(M; D_f, D_r) \). As a result, researchers often resort to alternative proxies to verify MU. These proxies include performance metrics (e.g., classification accuracy \citep{golatkar2020eternal} or generative performance using metrics like ROUGE for large language models \citep{maini2024tofutaskfictitiousunlearning}) and privacy attacks, such as membership inference attacks or backdoor attacks \cite{hayes2024inexactunlearning, triantafillou2024arewemakingprogress}.


\subsection{Unlearning Axes} 
Creating a taxonomy for MU depends on the goal and perspective. Instead of suggesting a new taxonomy, here, we outline two key dimensions that help identifying a MU problem: Model Type, and Forget-Set Definition. 

\textbf{Model Type.} MU is studied in both discriminative and generative models. MU algorithms depend on the training objective and the architectures of these models. For example, \textit{Negative Gradient}, a widely used MU strategy, has different formulations in image classifiers \cite{golatkar2020eternal} versus language models \cite{yao2024muofllms}. Within these two broad categories, one can imaging further subcategories based on data type, evaluation metrics, etc. 

\textbf{Forget-Set Definition.} The forget-set, or the target of MU, can take various forms. It usually is in the following forms: a) the entire data class(es), b) individual data points \cite{kurmanji2023unboundedmachineunlearning}, c) shared features across data points \citep{warnecke2021muoflabelsfeatures}, and d) knowledge (also referred to as concepts or behaviors) that transcends direct connections to the training data \citep{gandikota2024unifiedconceptdelete}.

%\subsubsection{Application}
%MU was originally proposed as a solution for compliance with GDPR's "right to be forgotten" in ML models. Although Article 17 does not explicitly mandate the deletion of data from ML models, research in MU has extended the interpretation of the regulation to encompass such models, often without a comprehensive legal analysis. However, recent work \cite{??}, however, has critically examined MU within the framework of GDPR, concluding that only "retraining from scratch" meets the stringent requirements for compliance. Subsequently, MU has been adapted to address other regulatory and ethical challenges. For instance, it has been employed to remove biases from generative models \cite{omkar}, eliminate backdoors introduced into ML models through poisoned data \cite{?}, delete copyrighted content embedded within model training data \cite{?}, and removing outdated data \cite{?}.

\subsection{Trade-offs and risks}
MU algorithms strive to make a balance between three key objectives: \textbf{Model Utility}, \textbf{Forgetting Quality}, and \textbf{Efficiency}. In certain privacy-centric applications, forgetting can be synonymous with achieving privacy \citep{liu2024breaking}. Hyperparameters and regularizers impact these trade-offs. For example, in MU via \texttt{Fine-tune}, the number of steps and learning rate dictate the balance between forgetting quality and efficiency. Similarly, in \texttt{Gradient Ascent}, the number of steps determines the trade-off between effective MU and preserving model's utility.

Additionally, forgetting may sometimes conflict with privacy due to two phenomena. First, unlearning specific data points can inadvertently expose information about others in the retained set due to the "onion effect" of privacy \citep{carlini2022privacyonioneffectmemorization}. Second, over-forgetting \citep{kurmanji2023unboundedmachineunlearning} a data point may reveal its membership in the original training set—a phenomenon known as the "Streisand Effect" \citep{golatkar2020eternal}. Addressing these challenges requires careful calibration of MU methods to ensure a delicate equilibrium across these competing objectives. These trade-offs are illustrated in \autoref{fig:updated-tradeoffs-diagram}.

Beyond these trade-offs, MU introduces risks associated with \textit{untrusted parties} \citep{li2024pseudounlearning} and \textit{malicious unlearning} \citep{qian2023towardsmaliciousunlearning}. Malicious entities could exploit MU to make fake deletion queries, or introduce computation overheads to the system \citep{marchant2022hardtoforget}.

\input{figures_tex/trade-offs}

%MU carries several known risks and drawback which are relevant across our analysis. First, data points may be so ``embedded in the model’s implicit knowledge graph'' that it may not be possible to unlearn them without paying a price in model utility \citep{liu_unlearning_2023}. What is more, removing some data points --- e.g., to guard them from confidentiality attacks --- may increase the risk of other data points to the same attacks \citep{carlini2022privacyonioneffectmemorization}.

%\input{figures_tex/unlearning_dimensions}


%AI systems developers may be using open-source models without access to the underlying training data \cite{Schneier2024}, meaning that training from scratch is not an option. Even when it is, especially for the models trained on large-scale datasets --- a category that includes many of the most powerful models today -- training from scratch can be "prohibitively costly." \cite{wu2024unlearningconceptsdiffusionmodel,cheng2023gnndeletegeneralstrategyunlearning}.
