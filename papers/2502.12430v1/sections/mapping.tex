This Section comprehensively catalogs the potential applications of MU to assist AIA compliance. 
% In creating this catalog, we make a distinction between MU for system- and model-centered requirements like those in \citep[Art. 9, 15, 53, 55]{european_union_ai_act_2024} and MU for data-centered requirements like those in \citep[Art. 10]{european_union_ai_act_2024}. 
For each, we analyze the SOTA and its ability to support the potential application, then identify any open questions the research community must resolve in order to bridge the gap between the two. 
% \subsection{MU to aid compliance with system- and model-centered AIA requirements}
 % Some AIA provisions require AI systems or GPAI models, at large, to satisfy certain criteria --- e.g., related to risk mitigation \citep[Arts. 9, 55]{european_union_ai_act_2024}, accuracy and cybersecurity \citep[Art. 15]{european_union_ai_act_2024}, or copyright law \citep[Art. 53]{european_union_ai_act_2024}. When problematic training data points (i.e., the forget set) are what is holding the system or model back from achieving these criteria, and where MU, by diminishing the impact of those data points in an already-trained model, can help the system or model achieve the decreed criteria, it seems reasonable to hypothesize that MU can be a viable tool for compliance with these particular provisions.
 % One potential snag here might involve problematic training data points that also happen to violate requirements of the AIA (notably, those of Article 10). Are all downstream models trained on those data points ``poisoned fruit'' that are irredeemably non-compliant themselves, regardless of any post-training adjustments like approximate MU (or fine-tuning) that do not retrain without the problematic data? Though we assume for the remainder of this section that this is not the case, it is among the points that we ask legislators or standard-setters\footnote{The AIA \citep[Art. 40]{european_union_ai_act_2024}, like other EU product regulation, relies heavily on harmonised standards, which are detailed technical solutions prepared by the EU's external standardisation organisations (CEN, CENELEC and ETSI) and which, if complied with by the regulated entities, ``have the legal effect of establishing a presumption of conformity'' with the relevant requirements \citep{mazzini2023proposal}} to clarify.
In sum, we find that the potential applications of MU to assist AIA compliance ultimately roll up into just six separate applications (Fig. \ref{fig:pyramids}):
\begin{itemize}[noitemsep, topsep=0pt]
    \item Improve accuracy per \citet[Arts. 9, 15]{european_union_ai_act_2024}; 
    \item Mitigate bias per \citet[Arts. 9, 55]{european_union_ai_act_2024}; 
    \item Mitigate confidentiality attacks per \citet[Arts. 9, 15, 55]{european_union_ai_act_2024}); 
    \item Mitigate data poisoning per \citet[Arts. 15]{european_union_ai_act_2024}); 
    \item Mitigate other risks of generative outputs per \citet[Arts. 9, 55]{european_union_ai_act_2024}); 
    \item Aid compliance with copyright laws, per \citet[Art. 53]{european_union_ai_act_2024})
\end{itemize}

\input{figures_tex/aia_unlearning_mapping}

 % In the meantime, we explore the application of MU to aid compliance with six system- and model-centered AIA requirements: (1) risk mitigation of risks to health and safety in AI models (2) 
 
 % Meanwhile, MU is a process for altering the behavior of an already-trained model (so that it behaves the same as a model that was not trained on the forget set) \cite{Xu_2024}. Where altering a model's behavior in this manner helps it (or the AI system it inhabits) satisfy the decreed criteria, MU's potential to aid compliance seems straightforward from a legal perspective (and the discussion then becomes one about technical feasibility). We argue this is the case for the AIA provisions requiring AI systems and/or GPAI models to mitigate certain risks \citep[Arts. 9, 55]{european_union_ai_act_2024}, to achieve accuracy and cybersecurity \citep[Art. 15]{european_union_ai_act_2024}, or to comply with EU copyright law \citep[Art. 53]{european_union_ai_act_2024}. 

 

\subsection{Accuracy}
\label{sssec:accuracy}

At least two AIA provisions may compel HRAI systems providers towards higher levels of accuracy. First (and most direct), HRAI systems must achieve a level of accuracy appropriate to their intended use and the SOTA \citep[Art. 15.1; Rec. 74]{european_union_ai_act_2024}. Second, HRAI systems' risk management must include measures to mitigate or eliminate risks to health and safety \citep[Art. 9]{european_union_ai_act_2024}, which could stem from low accuracy in domains like medicine \citep{jongsma2024why, james, Heaven2021}. For these requirements, MU can boost HRAI system's accuracy by removing the problematic data from the model. 

Importantly, the accuracy use case should not require privacy guarantees on the unlearned data \citep{goel2023adversarialevaluationsinexactmachine}. Rather, because the goal is strictly to boost accuracy to the level deemed appropriate  \citep[Art. 15]{european_union_ai_act_2024} or until the overall residual risk to health and safety posed by the inaccuracy is judged to be ''acceptable'' \citep[Art. 9]{european_union_ai_act_2024}, model accuracy should be the primary benchmark for MU's efficacy. In measuring that, AI providers will presumably account for any inadvertent, counteracting degradation in accuracy caused by the MU itself \cite{LI2024100254, DBLP:conf/sp/BourtouleCCJTZL21, Xu_2024}. However, providers will have to separately measure and account for the other trade-offs of using MU, such as its increased privacy and security risks \citep{Xu_2024, carlini2022privacyonioneffectmemorization}.  

% wherever doing so can ``reasonably'' be accomplished through the development or design of the system, to the point where the ''overall residual risk'' is ''judged to be acceptable'' \citep[Art. 9]{european_union_ai_act_2024}. 

% Even when limiting the definition of health and safety to physical safety, we can identify at least two scenarios where MU can serve as a ``post-training risk mitigation mechanism'' \citep{liu_unlearning_2023} that can help reduce, if not eliminate, the risks that problematic training data points can pose to health and safety. The first is where those problematic training data points --- e.g, mislabeled data points and outliers --- are leading to poor model performance that. 

% For example, incorrect labels in a training set may lead to model "confusion" and misclassification \cite{kurmanji2023unboundedmachineunlearning}. In the medical domain, misclassification can lead to patient harm and even "fatal outcomes" \citep{10.1001/jamadermatol.2023.5550}, including during diagnosis \cite{CHOWDHURY2024100297, PIRI201815, doi:10.4258/hir.2023.29.2.120} or triage \citep{saria2018better}. Because MU may help reduce confusion if the mislabeled examples are placed in the forget set \cite{kurmanji2023unboundedmachineunlearning, goel2023adversarialevaluationsinexactmachine}, we can logically conclude that MU, at least when it comes to incorrect labels in a training set, represents a viable mitigation for the risks to health and safety and, therefore, a path towards compliance with \citep[Art. 9]{european_union_ai_act_2024}. 

% \begin{tcolorbox}[colback=gray!10,colframe=black!50,title=State-of-the-Art]
\textbf{Current SOTA} Both exact or approximate unlearning theoretically offer paths towards improving model accuracy by forgetting mislabeled \citep{goel2023adversarialevaluationsinexactmachine,SUGIURA20242024DAT0002, 10.1145/3196494.3196517, chen2023unlearnwantforgetefficient, DBLP:conf/fgr/GundogduUU24}, out-of-date and outlier training data points \citep{kurmanji2023machineunlearninglearneddatabases, xu2024machineunlearningtraditionalmodels, Xu_2024, neel2024machineunlearning}, or, potentially, removing noise from medical data \citep{prelovznik2024improvingBrainMRI, dinsdale2020unlearningScannerBias}. The largest hurdle for this use case might be identifying all of the data points that are leading to inaccuracy (e.g., the mislabeled examples), which can be difficult \citep{goel2024correctivemachineunlearning}. However, for this use case, it may be good enough to identify only a subset of these examples --- so long as accuracy is boosted to levels deemed ''appropriate'' in light of the intended purpose as well as the SOTA \citep[Art. 15.1; Rec. 74]{european_union_ai_act_2024}. MU based on subset forget sets have shown success in boosting accuracy. However, other studies have suggested that you need all of polluted data, not just some of this, or it might backfire \citep{goel2024correctivemachineunlearning}. 
%For example, \citet{goel2024correctivemachineunlearning} demonstrate that the Selective Synaptic Dampening (SSD) MU technique \citep{foster2023fastmachineunlearningretraining} is able to neutralize mislabeled samples's effect on accuracy with just 10\% of mislabelled data identified. That said, SSD also fails (and, in fact, lowers overall accuracy) for the ''interclass confusion'' scenario (where sample labels samples are in correct between two classes)\citep{goel2024correctivemachineunlearning}.
%Evaluating MU success is application dependent. What's interesting here is that we probably don't need formal guarantees of unlearning the forget set here. We would probably want to see that accuracy is going up (or confusion down, etc.)
% \end{tcolorbox}
It is also important to note that evaluating unlearning success is application dependent. And, that, approximate unlearning should not be expected to yield higher accuracy than exact retraining without the low-quality data. 

\begin{tcolorbox}[colback=green!10,colframe=black!50,title=Key Points]
\begin{itemize}[leftmargin=0pt]
    \item Multiple AIA requirements may benefit from MU
    \item Theoretical guarantees may not be needed 
    \item Evaluation measure is application-dependent
\end{itemize}
\end{tcolorbox}

\vspace{1em}

\begin{tcolorbox}[colback=red!10,colframe=black!50,title=Open Problems]
\begin{itemize}[leftmargin=0pt]
    \item Lack of reliable methods for identifying problematic data to unlearn
    \item Lack of controllability over trade-offs
\end{itemize}
\end{tcolorbox}


% \subsubsection{MU to mitigate bias in HRAI systems and GPAI models with systemic risk} 
\vspace{1em}
\subsection{Bias} 

Providers of both HRAI systems and GPAI models with systemic risk may be obliged to mitigate model bias. The former must take measures to mitigate or eliminate risks to fundamental rights, which includes the right to non-discrimination \citep[Arts. 9]{european_union_ai_act_2024}). The latter must take measures to mitigate their models' systemic risk \citep[Art. 55]{european_union_ai_act_2024}, which includes the risk of large-scale discrimination \cite{gpai_code_2024}. Bias can occur because unrepresentative or incomplete data leads to model outputs that do not perform fairly on different groups or, in the case of generative models, produce stereotyped or otherwise discriminatory outputs \citep{sci6010003}. In all these cases, MU can ostensibly help forget the data points or patterns in the training set causing the bias \citep{pedregosa2023machineunlearning, neel2024machineunlearning, sai2024machineunlearning, keskpaik2024machine, chen2023unlearnwantforgetefficient, lucki2024adversarialperspectivemachineunlearning}. An important limiting factor on this use case is that training data that is not there to begin with cannot be forgotten; if the bias is due to a data \textit{deficit}, MU will not help. Because the goal here is to reduce or eradicate bias, success should ultimately be measured using traditional bias metrics like the difference in performance on various subgroups \citep{dealcala2023measuringbiasaimodels} or, in the case of generative models, the propensity for biased outputs as measured with benchmarks \citep{parrish2022bbqhandbuiltbiasbenchmark}.

% \begin{tcolorbox}[colback=gray!10,colframe=black!50,title=State-of-the-Art]
\textbf{Current SOTA}
While debiasing ML models was studied even before MU \citep{zemel2013learningfairreps}, both exact \citep{DBLP:journals/corr/abs-2405-14020}) and approximate \citep{chen2023fastmodeldebiasmachine, DBLP:conf/aistats/OesterlingMCL24, DBLP:journals/corr/abs-2405-14020}) MU have been used to mitigate model bias.
In the debiasing literature, solutions include \textit{pre-processing}, \textit{in-processing}, and \textit{post-processing} methods \citep{mehrabi2021fairnesssurvey}. MU, can mainly be considered as a post-processing method for debiasing. However, it is difficult to draw a separating line between debiasing and MU methods. MU works usually re-use some of the evaluation metrics in the debiasing literature, however, how to evaluate bias is, generally, considered an ``open problem'' \citep{reuel2024openproblemstechnicalai}. In order to preserve accuracy by not forgetting data points holistically, \citep{xu2024dontforgetmuchmachine} use MU to forget only those the features that lead to bias.

\begin{tcolorbox}[colback=green!10,colframe=black!50,title=Key Points]
\begin{itemize}[leftmargin=0pt]
    \item MU may assist compliance with multiple bias-related AIA requirements
    \item MU is only subtractive and never additive, limiting its application to this use case
        \item De-biasing solutions are not limited to MU
\end{itemize}
\end{tcolorbox}


\begin{tcolorbox}[colback=red!10,colframe=black!50,title=Open Problems]
\begin{itemize}[leftmargin=0pt]
    \item Lack of methods for identifying bias counterfactuals 
    \item  Lack of controllability over trade-offs
    \item Difficulty of guaranteeing full unlearning of biases, due to generalization
\end{itemize}
\end{tcolorbox}

% In multiple places, the AIA requires risks that have been identified to be mitigated \citep[Arts. 9.3, 55(a-b)]{european_union_ai_act_2024}. When it comes to HRAI systems, these risks include those to health and safety (including bias) and those to fundamental rights (including the right to data protection and the right to non-discrimination) \citep[Art. 9]{european_union_ai_act_2024}. When it comes to GPAI models, these risks include systemic risks \citep[Art. 55(a-b)]{european_union_ai_act_2024}. Where these risks owe to ``something undesirable'' in the training data, we know, at least as a technical matter, that MU can hypothetically serve as an effective (and economic) that reduces the relevant risk in the trained model (or, by extension, the AI system it inhabits) . 

% But does this mitigation meet the requirements of the law?
% Until further clarification by lawmakers or standard-setters, at least where the risk-causing qualities of the data do not independently violate requirements of the AIA, there is no reason to think that MU, when effective, cannot fulfill the risk mitigation requirements of \citep[Arts. 9.3, 55(a-b)]{european_union_ai_act_2024}.\footnote{Note that the same might be said of other ``post-training risk mitigation and defense mechanism[s]'' like alignment fine-tuning and content filters \citep{liu_unlearning_2023}.} Where the risk-causing qualities of the data \textit{do} independently violate requirements of the AIA (notably, those of Article 10), the picture could be more complicated. Are all downstream models the ``poisoned fruit'' of the non-compliant data and therefore irredeemably non-compliant themselves, regardless of any post-training risk mitigation techniques applied to them? Though we assume for the remainder of this section that this is not the case, it is a point that we ask legislators to clarify.
 % that can ``correct the impact of [the] bad data'' \citep{liu_unlearning_2023} and assist compliance with the relevant Act provision. 
% \subsubsection{MU to defend against confidentiality attacks on HRAI systems and GPAI models with systemic risk} 
\subsection{Confidentiality attacks} 
\vspace{-0.2cm}
The AIA requires providers of both HRAI systems and GPAI models with systemic risk to resolve and control for confidentiality attacks. Providers of HRAI systems must ensure their systems achieve an ``appropriate level'' of cybersecurity given the intended use and the SOTA, including by taking technical measures to prevent, detect, respond to, resolve and control for confidentiality attacks \citep[Art. 15.5; Rec. 74]{european_union_ai_act_2024}. Meanwhile, providers of GPAI models with systemic risk must ensure those models reflect ``an adequate level of cybersecurity'' \citep[Art. 55.d]{european_union_ai_act_2024}, which presumably also includes defending against confidentiality attacks. While the AIA does not define confidentiality attacks, we take them to include any attacks, including data reconstruction and membership inference attacks, that cause a model to reveal confidential details about its training such as data points or membership \citep{vassilev2023adversarial, CLTC2024adversarial}. This may include confidential training data memorized by generative models \citet{cooper2024machineunlearningdoesntthink, gu2024secondorderinformationmattersrevisiting, lucki2024adversarialperspectivemachineunlearning}. Where such attacks occur --- or where there is reason to think they might --- MU can ostensibly help defend against them by forgetting the confidential information vulnerable to attack \citep{hine_supporting_2024, neel2024machineunlearning, carlini2022privacyonioneffectmemorization, math12244001, xu2024machineunlearningtraditionalmodels, reuel2024openproblemstechnicalai, barez2025openproblemsmachineunlearning}.\footnote{We note that another, better solution to this problem exists: training with differential privacy (DP) \citep{Protivash_Durrell_Kifer_Ding_Zhang_2024}. Training a model with DP provides guarantees against confidentiality attacks \citep{chen2021differential, Protivash_Durrell_Kifer_Ding_Zhang_2024}. Accordingly, the safest strategy for complying with these AIA requirements is simply training with DP. We realize that this is not always practical, of course. AI providers --- for example, those integrating open source, closed-data models like Llama \citep{touvron2023llama} into their AI systems --- may only have access to the trained model weights and, therefore, training with DP is not an option. Differently, AI providers may only realize, post-training, that their model is vulnerable to confidentiality attacks and, at that point, may not be in a position to bear the cost of retraining from scratch with DP. In these situations, approximate unlearning remains a highly relevant option for addressing these attacks.} For this use case, the measure of success should be whether confidentiality attacks succeed in the wake of the MU \cite{grimes2024goneforgottenimprovedbenchmarks}, though use case-specific metrics have been developed \citet{maini2024tofutaskfictitiousunlearning}. When it comes to this use case, there are, importantly, other viable options for protecting against confidentiality attacks, including training with DP \citep{WANG2023408, kaissis2023boundingdatareconstructionattacks}.
% \begin{tcolorbox}[colback=gray!10,colframe=black!50,title=State-of-the-Art]

\textbf{Current SOTA.} Multiple techniques for using MU to mitigate confidentiality attacks (or the closely related problem of inadvertent model leakage of personal data) have been demonstrated \citep{dhingra2024protecting, ashuach2024revsunlearningsensitiveinformation, lizzo2024unlearnefficientremovalknowledge, chen2024, Wang_2024, CaoYang2015, DBLP:conf/sp/BourtouleCCJTZL21, Borkar}.
As of today, using MU to thwart these attacks can carry sizable trade-offs. For example, unlearning some data points for the sake of protecting them from recovery by attackers can jeopardize the privacy of other data points that neighbor the ones that have been unlearned \citet{carlini2022privacyonioneffectmemorization} or even increase the risk of membership inference attacks that recover the unlearned data points \citet{Chen_2021, barez2025openproblemsmachineunlearning,kurmanji2023unboundedmachineunlearning}. Differently, approximate unlearning, when used to delete particular data points, can carry a bias trade-off \citep{zhang2023forgotten, oesterling2024fairmachineunlearningdata} and an accuracy trade-off that rises as more data is forgotten \citep{Graves_Nagisetty_Ganesh_2021, maini2024tofutaskfictitiousunlearning}. It is also important to note that current MU methods, usually fail on new emergent attacks that are devised with new assumptions \citep{zhang2024doesmureallyforgets, hu2024joggingmumemory}.

\begin{tcolorbox}[colback=green!10,colframe=black!50,title=Key Points]
\begin{itemize}[leftmargin=0pt]
    \item MU may assist compliance with multiple confidentiality attack-related AIA requirements
    \item Due to attack diversity, success should be measured on case-by-case basis
    \item DP is a strong alternative to MU for this use case
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=red!10,colframe=black!50,title=Open Problems]
\begin{itemize}[leftmargin=0pt]
    \item Difficulty of providing formal guarantees of attack susceptibility 
    \item  Difficulty of applying MU to new, emergent attacks
    \item Identifying, localizing, and measuring memorization of confidential data is itself an open problem
\end{itemize}
\end{tcolorbox}

% \subsubsection{MU to mitigate data poisoning and backdoor attacks in HRAI systems and GPAI models with systemic risk} 
\subsection{Data poisoning} 
In data poisoning, specially-crafted data points are injected into a training set to alter (e.g., degrade or bias) the behavior of the model to the benefit the attacker \citep{DBLP:conf/icml/BiggioNL12}. Backdoor attacks are a type of data poisoning where the injected data points create ``triggers'' the attacker can exploit during inference \citep{lin2021mlattackmodelsadversarial}. The AIA obligates the providers of both HRAI systems and GPAI model with systemic risk to address such attacks. HRAI system providers must ensure their systems achieve an ``appropriate level'' of  cybersecurity, including via technical measures to ``prevent, detect, respond to, resolve and control for attacks trying to manipulate the training data set (data poisoning)'' \citep[Art. 15(5)]{european_union_ai_act_2024}.\footnote{HRAI system providers may additionally be compelled to address data poisoning in order to comply with those AIA provisions, discussed in \ref{sssec:accuracy}, necessitating a certain level of accuracy --- since model poisoning can degrade accuracy \citep{nawshin2024assessing}.} Providers of GPAI models with systemic risk, meanwhile, must ``ensure an adequate level of cybersecurity'' in their models \citep[Art. 55.d]{european_union_ai_act_2024}, which presumably also includes defenses against data poisoning. Where it is known that data poisoning has (or could) occur, MU may help remove the effects of the poisoned data points on the model and, thus, help satisfy these requirements  \citep{Xu_2024, liu2024threatsattacksdefensesmachine, CaoYang2015, 10.1145/3196494.3196517, 281308}. When it comes to measuring success for this use case, because the ``primary goal is to unlearn the adverse effect due to the manipulated data,'' the ideal benchmark would seem to be whether those adverse effects --- be they vulnerability to backdoor triggers, bias, or lower accuracy --- are eliminated or reduced \citep{goel2024correctivemachineunlearning}. For example, \citet{goel2024correctivemachineunlearning} measure MU efficacy based on whether proper accuracy on backdoor triggers is restored.

% \begin{tcolorbox}[colback=gray!10,colframe=black!50,title=State-of-the-Art]
\textbf{Current SOTA} Though some work has demonstrated MU can succeed for this use case \citep{warnecke2023machineunlearningfeatureslabels, schoepf2024potionpoisonunlearning} other works question the effectiveness of using MU to address data poisoning or backdoor attacks specifically \citep{8685687, 10.1007/978-3-030-58951-6_24, pawelczyk2024machineunlearningfailsremove, xu2024machineunlearningtraditionalmodels}). As always, identifying the full forget set (here, the poisoned samples) remains challenging \citep{goel2024correctivemachineunlearning}. Some methods, moreover, can have a significant accuracy trade-off on this use case \citep{pawelczyk2024machineunlearningfailsremove}. Such trade-offs can be particularly difficult as poisoned data overlaps with the clean data and, in most cases, they are even visually indistinguishable from each other.  

\begin{tcolorbox}[colback=green!10,colframe=black!50,title=Key Points]
\begin{itemize}[leftmargin=0pt]
    \item MU may assist compliance with multiple data poisoning-related AIA requirements
    \item A proper benchmark should measure the elimination of adverse effects
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=red!10,colframe=black!50,title=Open Problems]
\begin{itemize}[leftmargin=0pt]
    \item Finding contaminated data at scale is challenging 
    \item Unlearning the backdoor pattern without hurting unaffected data is challenging 
        \item Current MU methods mostly fail on data poisoning use case
\end{itemize}


\end{tcolorbox}


% \subsubsection{MU to mitigate the risks to health, safety, and fundamental rights and systemic risk posed by the generative outputs of HRAI systems and GPAI models with systemic risk} 
\subsection{Other risks of generative outputs} 

Generative model outputs may pose risks to health, safety, and human rights or pose systemic risk that the providers of HRAI systems and GPA models, respectively, must mitigate. For example, HRAI systems' risk management systems must include measures to mitigate or eliminate risks that the system poses to health, safety, and fundamental rights \citep[Art. 9]{european_union_ai_act_2024}. Generative model outputs may pose risks to health and safety, for example by issuing bad medical advice \citep{wu2024generating, Han2024}); likewise, generative model outputs may pose a risk to the fundamental right of non-discrimination, for example by producing stereotyping outputs \citep{nicoletti2023humans}. When it comes to GPAI models with systemic risk, providers of such models must mitigate that systemic risk \citep[Art. 55]{european_union_ai_act_2024}, which could be brought on by generative model outputs  that display offensive cyber capabilities, knowledge of CBRN, and more \cite{gpai_code_2024, nist2024trustworthy}. In all of these cases, MU may help mitigate the non-compliant outputs by unlearning the data points or even the concepts in the training set that are causing them \cite{lucki2024adversarialperspectivemachineunlearning, cooper2024machineunlearningdoesntthink}. Computationally, it may offer advantages even as compared to other popular alignment techniques like reinforcement learning  \citep{yao2024largelanguagemodelunlearning}. Measuring success for this use case should arguably be ``context dependent'' \citep{yao2024largelanguagemodelunlearning}. That is, the best way to measure the MU's efficacy is to benchmark the specific behavior that we desire to repair \citep{yao2024largelanguagemodelunlearning}. This could be done using existing benchmarks unrelated to MU \cite{barez2025openproblemsmachineunlearning}. Differently, \citep{li2024wmdpbenchmarkmeasuringreducing} propose a benchmark for measuring MU of CBRN knowledge. That being said, approaches that examine the model parameters for remnants of the unlearned concepts have also been proposed \citep{hong2024intrinsicevaluationunlearningusing}.

% \begin{tcolorbox}[colback=gray!10,colframe=black!50,title=State-of-the-Art]
\textbf{Current SOTA}
Multiple works use MU to curb undesirable generative model outputs  \cite{omkar, yu-etal-2023-unlearning, WEI2025104103, fore2024unlearningclimatemisinformationlarge}. However, the task is difficult, without agreed-upon best practices \citep{cooper2024machineunlearningdoesntthink}. Broad concepts like non-discrimination tend to go beyond individual data points, to latent information which is not easily embodied as a discrete forget set \cite{cooper2024machineunlearningdoesntthink, liu_unlearning_2023}. Even if data points that are intrinsically harmful (e.g., the molecular structure of a biological weapon) are removed, models may still assemble dangerous outputs from latent information in the rest of the dataset \citep{cooper2024machineunlearningdoesntthink, intl_ai_safety_2025}. Trying to remove that latent knowledge can risk model utility \cite{cooper2024machineunlearningdoesntthink}. As a separate but related issue, AI systems in these scenarios may be dual-use, where the appropriateness of outputs depends on downstream context; this, too, makes identifying the forget set difficult and increases the likelihood of a utility trade-off as the model forgets useful knowledge alongside undesirable knowledge \cite{cooper2024machineunlearningdoesntthink, shi2024musemachineunlearningsixway, reuel2024openproblemstechnicalai}. All of these issues, in turn, make it difficult if not impossible to specify formal guarantees on the MU \citep{liu_unlearning_2023}. 
% \end{tcolorbox}

\begin{tcolorbox}[colback=green!10,colframe=black!50,title=Key Points]
\begin{itemize}[leftmargin=0pt]
    \item MU may assist compliance with multiple AIA requirements related to generative outputs
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=red!10,colframe=black!50,title=Open Problems]
\begin{itemize}[leftmargin=0pt]
    \item Defining the forget set when what we seek to forget is conceptual  
    % \citep{cooper2024machineunlearningdoesntthink}
    \item Difficulty of guaranteeing full unlearning of undesirable behaviors, due to generalization
        % \citep{cooper2024machineunlearningdoesntthink}
            \item Mitigating the forgetting of useful knowledge alongside undesirable knowledge 
            % \citep{reuel2024openproblemstechnicalai}
\end{itemize}
\end{tcolorbox}

% \subsubsection{MU to comply with copyright law in GPAI models}
\subsection{Copyright}
All GPAI model providers must have a policy for complying with EU law on ``copyright and related rights'' \citep[Art. 53.c]{european_union_ai_act_2024}. Among other things, this policy must honor the TDM opt-outs of rightsholders \citep[Art. 53.c; Rec. 105]{european_union_ai_act_2024}, which is often a feature of AI training \citep{Rosati_2024, kneschke2024laion}. When it comes to AI and copyright law, a distinction is sometimes made between the ``input'' (training) phase and the ``output'' (inference) phase of the AI life cycle \citep{Rosati_2024, quintais2024generative}. At this point in time, the primary compliance risk during the input phase seems to be that an AI training set could include data points that violate TDM opt-outs. When this happens, we assume that using MU to remove the opt-out data points from the trained model does not cure the violation, since the violation occurred at the moment the opt-out data was used for training.\footnote{This topic --- whether MU can assuage the violation of TDM opt-outs or whether the proverbial ship has sailed --- is another that could use clarification from EU lawmakers.\label{footnote7}} That said, MU may still represent a valuable component of a copyright-compliance policy by helping prevent, at the ``output'' phase, further violations of copyright law when the opt-out data points --- or any other copyright-protected data points in the training set --- are reproduced to some degree in model outputs \citep{Rosati_2024}. This is a real risk with generative models, which often memorize training data \citep{cooper2024files, carlini2023extractingtrainingdatadiffusion}. When MU is applied to this use case, we want to measure success by tracking how likely the model is to generate works that are sufficiently similar to the copyrighted works. For example, we might rely on existing benchmarks that measure the tendency of models to produce copyrighted materials \citep{liu2024shieldevaluationdefensestrategies, chen2024copybenchmeasuringliteralnonliteral}. Differently, \citet{ma2024datasetbenchmarkcopyrightinfringement} produce a benchmark for the success of MU in the copyright context.

% \begin{tcolorbox}[colback=gray!10,colframe=black!50,title=State-of-the-Art]
\textbf{Current SOTA}
\citet{wu2024unlearningconceptsdiffusionmodel} unlearn copyrighted works from diffusion models.
At first glance, exact MU would seem to provide a guarantee that copyrighted works in the training set will not be reproduced in outputs \cite{liu_unlearning_2023}. But the fact is that retraining from scratch without the copyrighted data points may not be a bulletproof solution for preventing copyright infringement in outputs because substantially similar representations of copyrighted ``expressions'' (e.g., images of characters like Spiderman) could still appear in outputs based on how the model generalizes from the latent information extracted from the rest of the training set \citet{cooper2024machineunlearningdoesntthink}. For the same reason, approximate unlearning aimed at removing the influence of the copyright data points on the model, on top of being hard to prove \citep{liu_unlearning_2023},  also cannot ensure that copyrights are not infringed by outputs. In general, the SOTA of approximate unlearning has been deemed ``insufficient'' for the copyright use case, which may be why practitioners currently lean towards pre- and post-processing tools like prompting and moderation to bring AI into compliance with these laws \citep{liu_unlearning_2023, shumailov2024ununlearningunlearningsufficientcontent}.
\citep{dou2024avoidingcopyrightinfringementlarge} ``unlearn'' copyrighted materials in LLM pre-training datasets by identifying and removing specific weight updates in the model’s parameters that correspond to copyrighted content, evaluating their method by measuring the similarities between the model’s outputs
and the original content.
The task of measuring whether substantially similar outputs are being produced is quite challenging \citep{cooper2024machineunlearningdoesntthink}.


\begin{tcolorbox}[colback=green!10,colframe=black!50,title=Key Points]
\begin{itemize}[leftmargin=0pt]
    \item MU does not help with TDM opt-out violations; the damage is already done
    \item MU may, however, help with downstream copyright violations in outputs
    \item To avoid malicious unlearning, TDM opt-outs will have to be verified 
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=red!10,colframe=black!50,title=Open Problems]
\begin{itemize}[leftmargin=0pt]
    \item Difficulty in identifying copyright-infringing works in a dataset 
    % \citep{cooper2024machineunlearningdoesntthink}
    \item Difficulty of verifying whether model output is due to copyrighted content or generalization 
        \item Localizing and measuring memorization of copyrighted data is itself an open problem
    % \citep{liu2024threatsattacksdefensesmachine}.
\end{itemize}
\end{tcolorbox}

 
