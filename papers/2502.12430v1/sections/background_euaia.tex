The AIA sets forth harmonized requirements for AI systems and models placed on the market or put into service in the EU \citep[Art. 1]{european_union_ai_act_2024}. These requirements largely target two distinct categories of AI: AI systems and general-purpose AI (``GPAI'') models. Here, we define these two categories and, for each, briefly preview the requirements that, we later argue, MU may assist compliance with.
% Lastly, we outline the relationship between the AIA and GDPR, which also has a bearing on our analysis.

% Its requirements display some particular characteristics and patterns that are worth highlighting because they influence the ways MU could hypothetical be used to aid compliance with the Act.

\subsection{AI Systems}

The AIA broadly defines AI systems to include any ``machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments'' \citep[Art. 3.1]{european_union_ai_act_2024}. An example of a system that might meet this criteria is ChatGPT \citep{fernandez-llorca_interdisciplinary_2024}. 

In laying out its rules for these AI systems, the AIA relies on a ``risk-based'' approach \citep{Mahler2022-gc}, by which an AI system's perceived degree of risk determines the exact rules that apply to it. Here, the strictest requirements --- and the ones most relevant to our discussion --- are reserved for those AI systems deemed to be \textit{high-risk} \citep[Art. 6]{european_union_ai_act_2024}. 
% Whether or not an AI system is high-risk is largely based on its intended use \citep{edwards_eu_ai_2022}: specifically, whether the intended use falls into one of the high-risk applications enumerated by the AIA, which include critical infrastructure, education, and more \citep[Art. 6.2; Ann. III]{european_union_ai_act_2024}. 
Such high-risk AI (``HRAI'') systems are subject to a bevy of requirements \citep[Chap. III]{european_union_ai_act_2024}. Among them, the following are the most relevant to our position:\footnote{One set of AIA requirements that we have consciously excluded from our analysis is those around data and data governance for HRAI systems \citep[Art. 10]{european_union_ai_act_2024}. Our interpretation is that these requirements must be met at the dataset level and that no amount of MU on the downstream model will cure violations of these requirements. Ideally, lawmakers or courts will clarify whether our stance is the right one. If it is, then a follow-on question is whether AI system providers who repair Article 10 violations in their data are still obligated to remove the impact of this violative data on models that were trained with it. This question, of course, echoes questions around models trained on datasets that violate GDPR  \citep{JULIUSSEN2023105885, DBLP:conf/sp/BourtouleCCJTZL21, yang2024machinelearningmachineunlearning}. If the answer is in the affirmative, then MU may offer a computational efficient method for doing so --- at least as compared to retraining from scratch. \label{footnote2}}

% HRAI systems must subject their training, evaluation, and test sets (collectively, ``datasets'')  to data governance and management practices appropriate for their intended purpose \citep[Art. 10.2]{european_union_ai_act_2024}. These practices must include measures to detect, prevent and mitigate any ``possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law'' \citep[Art. 10.2(f-g)]{european_union_ai_act_2024}. These practices must also ``address[]'' any relevant data gaps or shortcomings that prevent compliance with other AIA provisions \citep[Art. 10.2(h)]{european_union_ai_act_2024}. Data governance and management practices aside, datasets must be ``relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose'' and must display ``appropriate statistical properties'' including as regards the persons or groups of persons in relation to whom the AI system is intended to be used \citep[Art. 10.3]{european_union_ai_act_2024}. The latter, states the AIA's Recitals, necessitates ``the mitigation of possible biases in the data sets'' \citep[Rec. 67]{european_union_ai_act_2024}. Lastly, datasets must ``take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting within which the ... AI system is intended to be used'' \citep[Art. 10.4]{european_union_ai_act_2024}.} 


\textbf{Risk management}:
HRAI systems must implement risk management systems that include the identification of known and reasonably foreseeable risks that the system can pose to health and safety or to fundamental rights \citep[Art. 9.2(a)]{european_union_ai_act_2024, kaminski_law_review_2023}. Here, risks to \textit{health and safety}, may include risks to mental and social well-being as well as physical safety. \citep{armstrong_ai_safety_2024, european_commission_ai_qa_2021}. Meanwhile, risks to \textit{fundamental rights} includes risks to any of the rights listed on the EU's Charter of Fundamental Rights \citep{eu_charter_2000}; though, here we only focus on risks to the right most relevant to the topic of MU: the right to non-discrimination, including from biased results \citep{arnold_how_2024}.\footnote{Another fundamental right which some HRAI systems may pose risks to --- risks that must then be managed under \citep[Art. 9]{european_union_ai_act_2024} --- is the right to protection of personal data \citep[Art. 8]{eu_charter_2000}. This includes the right for personal data to ``be processed fairly'' \citep[Art. 8(2)]{eu_charter_2000}, with GDPR subsequently setting the standards for doing so \citep{european_union_gdpr_2016}. Because the application of MU to assist GDRP compliance has been extensively covered by earlier literature \citep{JULIUSSEN2023105885,yang2024machinelearningmachineunlearning} and because our focus is on the \textit{new} use cases for MU brought about by AI regulation, we do not cover it here.}
% As was the case with risks to health and safety, we posit that 
% risks to these two fundamental rights will often stem from problems in the data that MU can, in the downstream model, ostensibly repair. For example, the right to the protection of personal data protects any information relating to an identified or identifiable natural (living) person, including names, dates of birth, photographs, video footage, email addresses, telephone numbers, IP addresses and communications content \citep{edps_dataprotection}. 
Importantly, wherever these risks are identified and can be ``reasonably mitigated or eliminated through the development or design'' of the AI system \citep[Art. 9.3]{european_union_ai_act_2024}, the risk management system must address them with ``appropriate and targeted risk management measures'' \citep[Art. 9.2(d)]{european_union_ai_act_2024}. 
% When choosing these measures, consideration may be given to ``achieving an appropriate balance'' between minimizing the risks and satisfying the AIA's other requirements \citep[Art. 9.4]{european_union_ai_act_2024}. 
These risk management measures must ensure the ``residual risk associated with each hazard, as well as the overall residual risk of the [HRAI] systems is judged to be acceptable'' \citep[Art. 9.5]{european_union_ai_act_2024} and, furthermore, ensure the ``elimination or reduction'' of the identified risks ``as far as technically feasible through adequate design and development''  \citep[Art. 9.5]{european_union_ai_act_2024}.

% \paragraph{\textbf{Data and data governance}:}
% HRAI systems must subject their training, evaluation, and test sets (collectively, ``datasets'')  to data governance and management practices appropriate for their intended purpose \citep[Art. 10.2]{european_union_ai_act_2024}. These practices must include measures to detect, prevent and mitigate any ``possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law'' \citep[Art. 10.2(f-g)]{european_union_ai_act_2024}. These practices must also ``address[]'' any relevant data gaps or shortcomings that prevent compliance with other AIA provisions \citep[Art. 10.2(h)]{european_union_ai_act_2024}. Data governance and management practices aside, datasets must be ``relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose'' and must display ``appropriate statistical properties'' including as regards the persons or groups of persons in relation to whom the AI system is intended to be used \citep[Art. 10.3]{european_union_ai_act_2024}. The latter, states the AIA's Recitals, necessitates ``the mitigation of possible biases in the data sets'' \citep[Rec. 67]{european_union_ai_act_2024}. Lastly, datasets must ``take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting within which the ... AI system is intended to be used'' \citep[Art. 10.4]{european_union_ai_act_2024}.

\textbf{Accuracy and cybersecurity}: 
HRAI systems must be designed and developed so as to achieve an ``appropriate level'' of accuracy and cybersecurity \citep[Art. 15.1]{european_union_ai_act_2024}. In its Recitals, the AIA stresses that these appropriate levels are a function of the system's intended purpose as well as the SOTA \citep[Rec. 74]{european_union_ai_act_2024}. When it comes to cybersecurity, the AIA specifically requires that HRAI systems be ``resilient against attempts by unauthorised third parties to alter their use, outputs or performance by exploiting system vulnerabilities'' \citep[Art. 15(5)]{european_union_ai_act_2024} and take technical measures to ``prevent, detect, respond to, resolve and control for ... data poisoning'' as well as ``confidentiality attacks'' \citep[Art. 15.5]{european_union_ai_act_2024}.

\subsection{GPAI models}

In contrast to an AI system, a GPAI model is defined as any AI model that is ``trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market'' \citep[Art. 3.63]{european_union_ai_act_2024}. Some see this as being synonymous with ``foundation model'' \citep{ada_lovelace_foundation_models_2024}. An example of a GPAI model that might meet this criteria is GPT 3.5, the model powering ChatGPT \citep{fernandez-llorca_interdisciplinary_2024}.

In laying out its requirements for GPAI models, the AIA again uses a risk-based approach, with the strictest requirements reserved for those GPAI models deemed to carry \textit{systemic risk} \citep[Art. 55]{european_union_ai_act_2024}. This is defined as the risk of ``having a significant impact on the [EU] market due to [its] reach, or due to actual or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scale across the value chain'' \citep[Art. 2.65; Annex III]{european_union_ai_act_2024}. This status can be established through a number of proxies, including performance on benchmarks and the amount of compute used during training \citep[Art. 51]{european_union_ai_act_2024}. While the AIA itself does not further elaborate on what constitutes systemic risk, the First Draft of the General-Purpose AI Code of Practice, a companion piece to the AIA that breaks it down into more granular technical standards, proposes that it covers risks related to: (1) cyber offense; (2) chemical, biological, radiological, and nuclear (CBRN); (3) loss of control; (4) automated use of models for AI research and development; (5) persuasion and manipulation; and (6) large-scale discrimination \cite{gpai_code_2024}.

Among the AIA's requirements for GPAI models that do display systemic risk --- and those that don't --- the following are the most relevant to our analysis: 

\textbf{Copyright}: 
All GPAI model providers must ``put in place a policy to comply with Union law on copyright and related rights'' \citep[Art. 53.c]{european_union_ai_act_2024}. Among other things, this policy must respect rightsholders' requests, per \citep[Art. 4(3)]{eu_dsg_directive_2019}, to opt out of text and data mining (TDM) on their copyrighted works \citep[Rec. 105, Art. 53.c]{european_union_ai_act_2024}.\footnote{As Recital 105 of the AIA explains, any use of copyright-protected content typically requires the authorization of its rightsholder \citep[Rec. 105]{european_union_ai_act_2024}. While Directive (EU) 2019/790 introduced an exception to this rule for text and data mining, it also granted rightsholders the power to opt out of the exception (unless the text and data mining is done for the purposes of scientific research) \citep[Art. 4]{eu_dsg_directive_2019}. Where the rightsholders opt out, says the AIA, providers of GPAI models must obtain an authorization from rightsholders to carry out text and data mining over their works \citep[Rec. 105]{european_union_ai_act_2024}.}

\textbf{Systemic risk}:
Those GPAI models that display systemic risk are required to ``mitigate'' it \citep[Art. 55(a-b)]{european_union_ai_act_2024}

\textbf{Cybersecurity}: 
GPAI models with systemic risk are additionally required to ``ensure an adequate level of cybersecurity'' \citep[Art. 55(d)]{european_union_ai_act_2024}.



%\input{tables_tex/AIA_summary}
