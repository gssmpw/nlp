A number of recent works allude to the use of MU as a tool for compliance with the AI regulation --- or, more broadly, a tool for advancing the trustworthy AI principles that AIR tends to encode \citep{hine_supporting_2024, DIAZRODRIGUEZ2023101896, li2024wmdpbenchmarkmeasuringreducing}. Importantly, however, another relevant set of works explore the shortcomings, trade-offs, and risks of MU as well as the viable substitutes for MU in various scenarios. Some recent works, for example, broadly question whether MU can really achieve its goals, especially in the generative domain \citep{cooper2024machineunlearningdoesntthink, barez2025openproblemsmachineunlearning,zhou2024limitationsprospectsmachineunlearning, shumailov2024ununlearningunlearningsufficientcontent}. Other works scrutinize MU's trade-offs around performance, privacy, security, and cost \citep{Xu_2024, carlini2022privacyonioneffectmemorization, Zhang_2023, keskpaik2024machine, zhou2024limitationsprospectsmachineunlearning, floridi2023machine}. 
% Ideas: 1. New position papers about Unlearning which stress that unlearning fails to achieve its goals. 
These factors could reasonably make alternative methods, training with DP, or post-training alignment tuning more appealing for the AI regulation use cases highlighted in this position paper   \citet{lucki2024adversarialperspectivemachineunlearning, cooper2024machineunlearningdoesntthink}. While the technical hurdles and trade-offs we have highlighted may lead some to question MU, instead of abandoning it, we  call to unlock MU's potential as a compliance tool for AI regulations.
