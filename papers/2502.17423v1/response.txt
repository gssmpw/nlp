\section{Background and Related Work}
We review background on diffusion models and  ODEs, solvers for diffusion ODEs, and  learning-based samplers. 
We also provide detailed comparisons with existing approaches in Appendix~\ref{app:related-work-comp}.

\subsection{Background: Diffusion Models}
Let $\x_0\in\R^d$ be a random variable from an unknown data distribution $p_0(\x_0)$. 
Diffusion models (DMs)** Sohl-Dickstein, **Learning Distributions over Functions**, define a forward process $\{\x_t\}_{t\in[0,T]}$ with $T>0$ that starts from $\x_0$ and progressively adds Gaussian noise to converge to a marginal distribution, $p_T(\x_T)$, that approximates an isotropic Gaussian, i.e. $p_T(\x_T)\approx \mc{N}(\x_T;\bm{0}, \tilde\sigma^2\id)$ at time $T$ for some $\tilde\sigma>0$.
Given $\x_0$, we can characterize the process of adding Gaussian noise by the transition kernel
$p_{0t}(\x_t|\x_0)=\mc{N}\big(\x_t; \alpha_t\x_0, \sigma_t^2 \id\big)$,
for all $t\in[0,T]$, where $\alpha_t, \sigma_t > 0$ are selected such that the \emph{signal-to-noise ratio} (SNR), $\alpha_t^2/\sigma_t^2$, decays as $t$ increases.
Remarkably,** Song et al.**, **Sliced Wasserstein Training for Diverse and Steerable Image Synthesis** , demonstrated that this forward process shares the same marginal distribution $p_t$ as the \emph{probability flow ODE}, a reverse-time ODE starting at $\x_T\sim p_T(\x_T)$ given by:
\begin{equation}
    \label{eqn:pf-ode}
    d\x_t = \left[f(t)\x_t - \frac{1}{2}g^2(t)\nabla_\x \log p_t(\x_t)\right] dt,
\end{equation}
where $f(t)={d\log\alpha_t}/{dt}$ and $g(t)= ({d\sigma_t^2}/{dt})-2({d\log \alpha_t}/{dt})\sigma_t^2$ ____.
Since the score function $\nabla_\x\log p_t(\x_t)$ in \eqref{eqn:pf-ode} is unknown, DMs learn it using a \emph{noise prediction} neural network to minimize:
\begin{equation*}
    \mc{L}(\bm\theta)=\E_{\x_0, \bm\epsilon,t}[w(t)\|\noisemodel(\x_t,t)-\bm\epsilon\|_2^2]
\end{equation*}
where $\x_0\sim p_(\x_0)$, $\bm\epsilon\sim\mc{N}(\bm 0 , \id)$, $t\sim \mc{U}[0,T]$, $w(t)$ is a time-dependent weighting function, and $\x_t=\alpha_t\x_0+\sigma_t\bm\epsilon$ is a noisy sample at time $t$____.
By Tweedie's formula, $\noisemodel(\x_t,t)$ learns to approximate $-\sigma_t\nabla_\x\log p_t(x)$, thereby defining the \emph{diffusion ODE}:
\begin{equation}
    \label{eqn:diff-ODE}
    d\x_t = \left[f(t)\x_t + \frac{g^2(t)}{2\sigma_t}\noisemodel(\x_t,t)\right]dt,
\end{equation}
with initial condition $\x_T\sim p_T(\x_T)$.
To exactly solve the diffusion ODE at $\x_t$ given an initial value $\x_s$, where $ t<s$,** Chen et al.**, **Neural Ordinary Differential Equations for Image Synthesis and Editing** , reparametrizes \eqref{eqn:diff-ODE} in terms of the log signal-to-noise ratio $\lambda_t := \log(\alpha_t/\sigma_t)$, yielding:
\begin{equation}
\label{eqn:lambda-exact-soln}
\x_{t_i}=\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\x_{t_{i-1}} - \alpha_{t_i}\int_{\lambda_{t_{i-1}}}^{\lambda_{t_i}} e^{-\lambda} \hatnoisemodel(\hat\x_\lambda,\lambda)d\lambda,
\end{equation}
where $\hat\x_\lambda$ and $\noisemodel(\hat\x_\lambda,\lambda)$ denote the reparametrized forms of $\x_t$ and $\noisemodel(\x_t,t)$ in the $\lambda$ domain.

\subsection{Background: Solving the Diffusion ODE}\label{sec:bg-ode}
Let $\tilde{x}_t$ be a discretization of $x_t$. 
We use \eqref{eqn:diff-ODE} to define the following system of equations:
\begin{equation}
    \frac{\partial \phi}{\partial t} = f(t) \phi + \frac{g^2(t)}{2}\noisemodel(\tilde{x}_t,t)
    \label{eqn:ode-sys}
\end{equation}

\subsection{Related Work: Learned Samplers} 
In practice, no single pair of ODE solver and a time discretization generates high quality samples universally across various datasets and model architectures, e.g. \appref{app:fid-tables} and ____.
This inspired {\em learning}-based methods for deriving ODE solvers and time discretizations adapted to the given task and architecture.
We give a brief survey here and discuss in detail in \appref{app:related-work-comp}. 
One popular approach exclusively learns the discretization steps **Peng et al.**, **Latent ODEs for Continuous-Time Dynamics** .
Our approach S4S learns the solver coefficients, complementing the gains of such methods and universally improving the performance in all scenarios, as seen in \tabref{tab:s4s-coeff} and comprehensively in \appref{app:fid-tables}.
Another line of research focuses on optimizing only the solver coefficients **Chen et al.**, **Neural Ordinary Differential Equations for Image Synthesis and Editing** , or jointly optimizing both solver coefficients and time discretizations **Liu et al.**, **Adversarial Normalizing Flow** .  
However, these methods are designed to minimize the {\em local} approximation error through the same methods as in \eqref{eqn:lambda-taylor-soln} or by closely matching the \emph{entire trajectory} of the teacher solver.
Instead, by minimizing the \emph{global} error by matching the \emph{end} of the teacher trajectory, as in \eqref{eqn:hard-objective}, S4S significantly improves over these approaches.
Closest to our approach is **Li et al.**, **BNS for Unsupervised Diffusion Model** , which learns both the solver coefficients and time discretizations to minimize global error.
We provide comparisons in \tabref{tab:s4s-compute-comp} and explain our improvements over BNS in \appref{app:related-bns}.