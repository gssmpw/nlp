%!TEX root = ../main.tex

\section{Experiments}
\label{sec:exp}
\input{tables/BALdataset.tex}
\input{sections/fig-visualization.tex}

We evaluate the \nameshort solver and the \xmsfm pipeline on diverse datasets. \nameshort is benchmarked against the leading bundle adjustment solver \ceres and \xmsfm is compared against the widely adopted SfM pipelines \colmap and \glomap.

Experiments run on a Lambda Vector workstation with 64-core AMD® Ryzen Threadripper Pro 5975WX CPUs and dual NVIDIA® RTX 6000 GPUs, using CUDA 12.4 and Python 3.11.10. All dependencies are carefully set up to leverage multi-CPU and GPU acceleration.


% \prettyref{sec:exp-replica} shows the experiment on the Replica dataset \cite{zhu2022nice, sucar2021imap, replica19arxiv}. It contains synthesis images of different virtual scenes. We use the ground truth depth map and compare ours result with \colmap and \glomap.

% \prettyref{sec:exp-mipnerf} presents the experiments conduct on the Mip-NeRF and Zip-NeRF dataset \cite{barron2022mipnerf360,barron2023zipnerf}, which consists of real-world, high-resolution images captured around a single object. These images are rich in detail and features. We compare our results with those of \colmap and \glomap.

% \prettyref{sec:exp-imc-tum} conducts experiments on the IMC PhotoTourism dataset \cite{imc2023}, TUM dataset \cite{sturm12tum}, and C3VD medical dataset \cite{bobrow2023}, which present challenges due to varying environments and low-quality images. The IMC dataset consists of collected photos of a famous landmark from the internet. The TUM dataset contains images captured from an indoor scene. The C3VD dataset is acquired from different human organs using a high-definition clinical colonoscope. We compare our results with \glomap.


\subsection{BAL Dataset}\label{sec:exp-bal}

We first evaluate on the Bundle Adjustment in the Large (BAL) dataset \cite{agarwal2010bundle}. This dataset contains reconstruction results from Flickr photographs using Bundler. On BAL we focus on \nameshort  solver performance rather than the full \xmsfm pipeline. 

\textbf{Setup and baselines}.
For input, we use 2D observations for \ceres and 3D observations for \nameshort. The 2D keypoint measurements come directly from the BAL dataset, while the 3D keypoint measurements are lifted by appending $z$-coordinates to the 2D measurements. Though accurate, these 3D observations incorporate slight noise, making them a refined yet imperfect ground truth. To showcase \nameshort's efficiency in solving the SDP relaxation of the SBA problem~\eqref{eq:sba}, we also evaluate \manopt using the same input as \nameshort.


% We down sample the data and send it to our \nameshort solver, Manopt  and Ceres .


\textbf{Metrics}.
We evaluate performance based on the runtime and the median of Absolute Trajectory Error (ATE) and Relative Pose Error (RPE). Additionally, we report the suboptimality and the minimum eigenvalue of the $Z(y)$ matrix in \prettyref{eq:problem-sdp-dual} to demonstrate that \nameshort achieves global optimality. Note that the minimum eigenvalue of $Z(y)$ has been used as a metric for global optimaity in previous works as well~\cite{carlone2015lagrangian}. Runtime is split into preprocessing time and solver time. The former primarily involves constructing the $Q$ matrix in \prettyref{eq:problem_scale_rotation_only}, which is implemented in Python, while the latter corresponds to the GPU solver. We separate preprocessing time and solver time because there are still ways to further reduce the preprocessing time (while the solver time, to the best of our understanding, has been pushed to the limit). For example, as building the $Q$ matrix requires large dense matrix multiplications, a GPU implementation is expected to accelerate this step by 10 to 100 times. However, we leave this as a future step.

\textbf{Results}.
Table~\ref{tab:bal} summarizes the comparison between \nameshort and other methods. We tested several versions of \ceres. ``\ceres'' indicates running \ceres without any initialization. ``\textsc{Ceres-GT}'' indicates starting \ceres at the groundtruth estimation. ``\textsc{Ceres-GT-0.01}'' means adding noise to the groundtruth with standard deviation 0.01. We make several observations. (a) Without good initialization, \ceres does not work, as shown by the failures of \ceres and \textsc{Ceres-GT-0.1}. (b) \nameshort is up to $100$ times faster than \manopt, showing the superior efficiency of our GPU implementation. Notably, \nameshort's solver time is below a second for $N$ in the order of hundreds, and \nameshort scales to $N > 10,000$ camera frames. 

Table~\ref{tab:bal-xm} presents the suboptimality gap and minium eigenvalue of \nameshort. As we can see, except the largest instance with $N=10155$ camera frames, \nameshort solved all the other instances to certifiale global optimality. The reason why \nameshort did not solve the largest instance to global optimality is because we restricted its runtime to one hour (\nameshort did achieve global optimality if allowed four hours of runtime). 

Fig.~\ref{fig:visualization-BAL} visualizes the 3D reconstructions. Since real images are not available in BAL, all 3D landmarks have the same purple color. The reconstructed cameras are shown in red.


\begin{tcolorbox}
    \begin{center}
        \vspace{-1mm}
        \textbf{Takeaway}
        \vspace{-2mm}
    \end{center} 
    $\bullet$ The SBA problem~\eqref{eq:sba} is easier to solve than the BA problem~\eqref{eq:ba-colmap}---we can design efficient convex relaxations. While \ceres needs good initialization for solving~\eqref{eq:ba-colmap}, \nameshort requires no initializations for solving~\eqref{eq:sba}.
        
    $\bullet$ With GT depth and outlier-free matchings, solving SBA with \nameshort produces the same result as solving BA with \ceres or \colmap.
       
    $\bullet$ \nameshort is fast and scalable.
\end{tcolorbox}

% If \nameshort is allowed to run for four hours, then it

% The results are summarized in \prettyref{tab:bal}, while suboptimality and the minimum eigenvalue of $Z(y)$ are detailed in \prettyref. Translation error is normalized, with $1.0$ representing $100\%$ error. To ensure practical usability, we limit our solver's runtime to one hour\footnote{The last dataset is solved to the global optimal in four hours}. If the time limit is exceeded, the solution is refined using Ceres. 

% We also show the visualization\footnote{because real images are not available, we use the same color for points cloud} in \prettyref{fig:visualization-BAL}.

\subsection{Replica Dataset}\label{sec:exp-replica}

\input{tables/Replicadataset.tex}

We then test on the Replica dataset~\cite{zhu2022nice, sucar2021imap, replica19arxiv}, which contains synthetic images of different virtual scenes. 

\textbf{Setup, baselines, metrics}.
We use the groundtruth depth map but employ the full \xmsfm pipeline. 
% Replica contains several indoor scenes with multiple objects.  To filter outliers, we adopt \xmdouble (\cf \prettyref{sec:xm-sfm}).  
We compare \xmsfm, both with and without two-view filtering, against \colmap and \glomap. The evaluation metrics remain the same. For runtime analysis, we categorize all components preceding our \nameshort solver---including Matching, Indexing, Depth Estimation, Filtering, and Matrix Construction---as preprocessing time. The indexing, filtering and matrix construction components can be further accelerated in CUDA. Similarly, for \glomap and \colmap, all steps prior to global positioning and bundle adjustment are counted as preprocessing time.

\textbf{Results}.
Results are presented in \prettyref{tab:replica} and \prettyref{tab:rep-xm}. Each Replica dataset contains 2000 frames, but for a diverse comparison across different dataset sizes, we sample the first 100 frames from each dataset as a separate experiment. ``Room0-100'' refers to the first 100 frames, while ``Room0-2000'' represents the full dataset.  

\emph{\nameshort consistently outperforms baselines by 100 to 1000 times in solver speed}, solving almost all 2000-frame datasets within 10 seconds. At the same time, \nameshort maintains high accuracy, achieving a median translation error of just 1\%. In practice, a 0.1\% and 1\% translation error yield nearly identical reconstruction quality, as illustrated in \prettyref{fig:visualization-replica}.

Additionally, we provide a runtime breakdown for \nameshort in \prettyref{app:breakdown-time}. The solver time is negligible, appearing as only a thin bar in the chart. Matching, indexing, and filtering are the most time-consuming components, with the latter two planned for CUDA implementation as future work.

\begin{tcolorbox}
    \begin{center}
        \vspace{-1mm}
        \textbf{Takeaway}
        \vspace{-1mm}
    \end{center} 
    $\bullet$ With ground truth depth and \colmap matchings, minor filter refinement achieves results comparable to \colmap and \glomap.  

    $\bullet$ \nameshort remains highly efficient and scalable.  
\end{tcolorbox}

\subsection{Mip-Nerf and Zip-Nerf Dataset}\label{sec:exp-mipnerf}

\input{tables/mipnerfdataset.tex}

Mip-Nerf and Zip-Nerf \cite{barron2022mipnerf360,barron2023zipnerf} are real-world image datasets around a single object. We evaluate learned depth and downstream novel view synthesis tasks on these datasets.

\textbf{Setup, baselines, metrics}.
As these datasets are generated by \colmap, we use its camera poses as ground truth to benchmark \nameshort against \colmap and \glomap. To address inaccuracies in learned depth, we apply \ceres refinement, incorporating its runtime into the solver time, denoted as ``\nameshort + \ceres.'' All other evaluation metrics remain unchanged.

\textbf{Results}.
Results are presented in \prettyref{tab:mipnerf}, with suboptimality detailed in \prettyref{tab:mipnerf-xm}. \emph{While adding \ceres increases solver time, the overall runtime remains 10 to 100 times faster than the baselines.} On the garden and room datasets, we achieve a 0.2\% error, demonstrating the same accuracy as the baselines, while on others, the error may be slightly higher. However, as shown in \prettyref{fig:visualization-mipnerf}, this has no noticeable impact on downstream 3D Gaussian Splatting tasks~\cite{kerbl3Dgaussians}.

A runtime breakdown for \nameshort is provided in \prettyref{app:breakdown-time}. Matching and indexing remain slow, while depth estimation now takes even longer. This is due to (a) foundation models requiring much time for estimation and (b) depth estimation runtime scales linearly with the number of frames, whereas Mip-Nerf datasets are relatively small.  

\textbf{The need for convex relaxation}. 
In \prettyref{fig:mipnerf-rank4}, we demonstrate that directly solving \prettyref{eq:problem-bm} with rank 3 (equivalently \prettyref{eq:problem-qcqp}) can lead to a local minimum. Specifically, the solver terminates with a minimal eigenvalue of $Z(y)$ at $-6.2 \times 10^{2}$, resulting in a messy reconstruction. However, by increasing the rank to 4, the solver escapes the local minimum and achieves the global minimum in rank 3, indicating that while the relaxation remains tight, the Burer-Monteiro method requires a higher rank to find the global minimum. 

\begin{tcolorbox}
    \begin{center}
        \vspace{-1mm}
        \textbf{Takeaway}
        \vspace{-1mm}
    \end{center} 
    $\bullet$ With \ceres refinement to mitigate depth errors, \nameshort achieves nearly the same accuracy as \colmap and \glomap. Moreover, this has no impact on downstream novel view synthesis tasks.  

    $\bullet$ \nameshort maintains a significant speed advantage, even with learned depth and real-world data.  

    $\bullet$ BM factorization and the Riemannian staircase effectively escape local minimum.  
\end{tcolorbox}

\subsection{IMC, TUM and C3VD Datasets}\label{sec:exp-imc-tum}
\input{tables/imctumdataset.tex}

Followed by Mip-Nerf, we step to the IMC PhotoTourism dataset \cite{imc2023}, TUM dataset \cite{sturm12tum}, and C3VD medical dataset \cite{bobrow2023}. These datasets are quite challenging because of varying environments and low-quality images.

\textbf{Setup, baselines, metrics}.
We only compare our \nameshort solver against \glomap on IMC datasets. In these three datasets we add both the filter part and \xmdouble. In C3VD we use the ground truth depth map and the learned depth from a medical-specific depth prediction model~\cite{paruchuri2024leveraging}. 

\textbf{Results}.
The results are presented in \prettyref{tab:imc}, \prettyref{tab:tum}, and \prettyref{tab:c3vd}. The IMC datasets consist of large image collections capturing some famous landmarks. As a result, \glomap requires an extremely long runtime, exceeding 20 hours for the largest dataset. Our accuracy is comparable to \glomap, with visualizations provided in \prettyref{fig:visualization-imc}. The TUM and C3VD datasets produce high-quality reconstructions, though accuracy is affected by the complexity of the environment and insufficient lighting. Visualizations of these reconstructions are provided in \prettyref{fig:visualization-tum} and \prettyref{fig:visualization-c3vd}.  

\begin{tcolorbox}
    \begin{center}
        \vspace{-1mm}
        \textbf{Takeaway}
        \vspace{-1mm}
    \end{center} 
    \nameshort remains efficient and scalable across diverse datasets, achieving results comparable to \glomap while being significantly faster.
\end{tcolorbox}






