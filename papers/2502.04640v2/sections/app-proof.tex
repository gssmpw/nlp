%!TEX root = ../main.tex

\section{Proofs}
\label{sec:app_proofs}

% \subsection{Proof of Proposition \ref{prop:formulation_matrix_form}}\label{app:proof_formulation_matrix_form}

% \begin{proof}
%     We first write the objective function in \prettyref{eq:sba} as vectorized form.
%     \bea 
%     &\displaystyle \sum_{(i,k+N) \in \calE} w_{ik} \norm{ R_i (s_i\tilde{u}_{i,k}) + t_i - p_k } ^2 \nonumber \\
%     = & \displaystyle \sum_{(i,k+N) \in \calE} w_{ik} \norm{ (\tilde{u}_{i,k} \tran \kron \eye_3) \vectorize{s_i R_i} + t_i -p_k} ^2 \nonumber
%     \eea
%     Noting here we add $w_{ik}$ as the weight on each term. Let $e_i \in \Real{N}$ be the all-zero vector except that the $i$-th entry is equal to $1$ and $\bar{e}_k \in \Real{M}$ be the all-zero vector except that the $k$-th entry is equal to $1$. We then simplify the objective $L(t,p,r)$ as:
%     \bea
%      \sum_{(i,k+N) \in \calE} w_{ik}|| ((e_i \tran \kron \tilde{u}_{i,k}) \kron \eye_3) r + (e_i\tran \kron \eye_3)t - (\bar{e}_k \tran \kron \eye_3)p ||^2 \nonumber 
%     \eea
%     \bea
%     = & \displaystyle r\tran \parentheses{ Q_1 \kron \eye_3} r + t\tran \parentheses{ Q_2 \kron \eye_3} t + p\tran \parentheses{ Q_3 \kron \eye_3} p + \nonumber\\
%     & 2r \tran \parentheses{ V_1 \kron \eye_3} t - 2r \tran \parentheses{ V_2 \kron \eye_3} p - 2t \tran \parentheses{ V_3 \kron \eye_3} p \nonumber
%     \eea
%     where $Q_1,Q_2,Q_3,V_1,V_2,V_3$ are as follows
%     \bea 
%     Q_1 = & \displaystyle \sum_{(i,k+N) \in \calE} w_{ik} (e_i  \kron \tilde{u}_{i,k} ) (e_i \tran \kron p_{i,k} \tran ) \in \Real{3N \times 3N},  \\
%     Q_2 = & \displaystyle \sum_{(i,k+N) \in \calE} w_{ik} (e_i e_i \tran) \in \Real{N \times N}, \\
%     Q_3 = & \displaystyle \sum_{(i,k+N) \in \calE} w_{ik} (\bar{e}_k \bar{e}_k \tran) \in \Real{M \times M}, \\
%     V_1 = & \displaystyle \sum_{(i,k+N) \in \calE} w_{ik} (e_i \kron \tilde{u}_{i,k} ) e_i \tran \in \Real{3N \times N}, \\
%     V_2 = & \displaystyle \sum_{(i,k+N) \in \calE} w_{ik} (e_i \kron \tilde{u}_{i,k} ) \bar{e}_k \tran \in \Real{3N \times M}, \\
%     V_3 = & \displaystyle \sum_{(i,k+N) \in \calE} w_{ik} e_i \bar{e}_k \tran \in \Real{N \times M}, 
%     \eea
%     concluding the proof.
% \end{proof}


% \subsection{Proof of Theorem \ref{thm:singularities}}\label{app:proof_singularities}
% \begin{proof}\label{proof:singularities}
%     Let $y = \bmat{c} t \\ p \emat $. We can represent $L(t,p,r)$ using $y,r$ as follows:
%     \begin{align}\label{eq:problem_yr_only}
%     L(y,r) =\displaystyle r\tran \parentheses{ Q_1 \kron \eye_3} r - 2r\tran \parentheses{ V_{tp} \kron \eye_3} y \nonumber \\+ y\tran \parentheses{ Q_{tp} \kron \eye_3} y
%     \end{align}
%     where $Q_{tp}, V_{tp}$ are as follows:
%     \bea
%     Q_{tp} = &\bmat{cc} Q_2 & -V_3 \\ -V_3 \tran & Q_3 \emat \in \Real{(N+M)\times(N+M)} \label{eq:app-Qtp-restate}\\ 
%     V_{tp} = & \bmat{cc} -V_1 & V_2 \emat \in \Real{3N \times (N+M)}.
%     \eea
%     set $\nabla_y L(y,r) = 0$ we obtain
%     \bea\label{eq:nabla_y_zero}
%     (Q_{tp} \kron \eye_3) {y} = (V_{tp} \kron \eye_3)\tran r.
%     \eea

%     Let $\calG$ denote the view-graph. Next we prove $Q_{tp}$ can be represented as:
%     \bea
%     Q_{tp} = L(\calG) 
%     \eea     
%     where $L(\calG)$ is the Laplacian of $\calG$:
%     \begin{lemma}
%         \label{lemma:laplacian}
%         $Q_{tp}$ is the Laplacian of $\calG$. \\
%         \textit{proof:} Note that $\calG$ is a weighted undirected graph. Calling $\delta(q)$ for the set of edges incident to a vertex $q$, and $w_e = w_{qp}$ for $e = (q,p)$, the Laplacian of $\calG$ is:
%             \bea
%             \label{eq:laplacian}
%             L(\calG)_{qp} = 
%             \begin{cases}
%                 \sum_{e \in \delta(q)} w_e & \text{if } q = p, \\
%                 - w_{qp} & \text{if } (q,p)\in \calE, \\
%                 0 & \text{if } (i,j) \notin \calE.
%             \end{cases}
%             \eea
%         On the other hand, by expanding \prettyref{eq:app-Qtp-restate} and compare it with \prettyref{eq:laplacian}, we finish the proof.
%     \end{lemma}
%     Note that $\rank{L(\calG)}=N+M-1$, then 
%     \bea
%     \rank{Q_{tp} \kron \eye_3}=\rank{L(\calG)}\rank{\eye_3}
%     \eea
%     Thus $Q_{tp} \kron \eye_3$ is not invertible, \prettyref{eq:nabla_y_zero} has infinite many solutions.
% \end{proof}

\input{tables/depthcomparison.tex}
\subsection{Proof of Proposition \ref{prop:formulation_scale_rotation_only}}\label{app:proof_scale_rotation_only}

\begin{proof}
    We begin by expressing the objective function in \prettyref{eq:sba} in its vectorized form: 
    \bea 
    &\displaystyle \sum_{(i,k) \in \calE} w_{ik} \norm{ R_i (s_i\tilde{u}_{ik}) + t_i - p_k } ^2 \nonumber \\
    = & \displaystyle \sum_{(i,k) \in \calE} w_{ik} \norm{ (\tilde{u}_{ik} \tran \kron \eye_3) \vectorize{s_i R_i} + t_i -p_k} ^2 \nonumber 
    \eea
    Here, $w_{ik}$ represents the weight assigned to each term.  

    Let $e_i \in \Real{N}$ be a vector of zeros except for the $i$-th entry, which is set to 1, and similarly, let $\bar{e}_k \in \Real{M}$ be a vector of zeros except for the $k$-th entry, which is set to 1. Furthermore, define the following concatenated vectors:  
    $t = [t_1,\dots,t_N] \in \Real{3N}$ for translations,
    $p = [p_1,\dots,p_M] \in \Real{3M}$ for landmark positions,  
    $\displaystyle r = [\vectorize{s_1 R_1};\dots;\vectorize{s_N R_N}] \in \Real{9N}$ for vectorized scaled rotations.  

    With these definitions, we simplify the objective function as $L(t,p,r)$.
    \bea
     \sum_{(i,k) \in \calE} w_{ik}|| ((e_i \tran \kron \tilde{u}_{i,k}) \kron \eye_3) r + (e_i\tran \kron \eye_3)t - (\bar{e}_k \tran \kron \eye_3)p ||^2 \nonumber 
    \eea
    \bea
    = & \displaystyle r\tran \parentheses{ Q_1 \kron \eye_3} r + t\tran \parentheses{ Q_2 \kron \eye_3} t + p\tran \parentheses{ Q_3 \kron \eye_3} p + \nonumber\\
    & 2r \tran \parentheses{ V_1 \kron \eye_3} t - 2r \tran \parentheses{ V_2 \kron \eye_3} p - 2t \tran \parentheses{ V_3 \kron \eye_3} p \nonumber
    \eea
    where $Q_1,Q_2,Q_3,V_1,V_2,V_3$ are as follows
    \bea 
    Q_1 = & \displaystyle \sum_{(i,k) \in \calE} w_{ik} (e_i  \kron \tilde{u}_{i,k} ) (e_i \tran \kron p_{i,k} \tran ) \in \Real{3N \times 3N},  \\
    Q_2 = & \displaystyle \sum_{(i,k) \in \calE} w_{ik} (e_i e_i \tran) \in \Real{N \times N}, \\
    Q_3 = & \displaystyle \sum_{(i,k) \in \calE} w_{ik} (\bar{e}_k \bar{e}_k \tran) \in \Real{M \times M}, \\
    V_1 = & \displaystyle \sum_{(i,k) \in \calE} w_{ik} (e_i \kron \tilde{u}_{i,k} ) e_i \tran \in \Real{3N \times N}, \\
    V_2 = & \displaystyle \sum_{(i,k) \in \calE} w_{ik} (e_i \kron \tilde{u}_{i,k} ) \bar{e}_k \tran \in \Real{3N \times M}, \\
    V_3 = & \displaystyle \sum_{(i,k) \in \calE} w_{ik} e_i \bar{e}_k \tran \in \Real{N \times M}, 
    \eea
    We now solve out $t$ and $p$ by taking the gradient of $L(t,p,r)$ \wrt $t$ and $p$ respectively.  Let $\displaystyle y = \bmat{c} t \\ p \emat $. We can represent $L(t,p,r)$ using $y,r$ as follows:
    
    \begin{align}\label{eq:problem_yr_only}
    L(y,r) =\displaystyle r\tran \parentheses{ Q_1 \kron \eye_3} r - 2r\tran \parentheses{ V_{tp} \kron \eye_3} y \nonumber \\+ y\tran \parentheses{ Q_{tp} \kron \eye_3} y
    \end{align}
    where $Q_{tp}, V_{tp}$ are as follows:
    \bea
    Q_{tp} = &\bmat{cc} Q_2 & -V_3 \\ -V_3 \tran & Q_3 \emat \in \Real{(N+M)\times(N+M)} \label{eq:app-Qtp-restate}\\ 
    V_{tp} = & \bmat{cc} -V_1 & V_2 \emat \in \Real{3N \times (N+M)}.
    \eea
    set $\nabla_y L(y,r) = 0$ we obtain
    \bea\label{eq:nabla_y_zero}
    (Q_{tp} \kron \eye_3) {y} = (V_{tp} \kron \eye_3)\tran r.
    \eea

    Let $\calG$ denote the view-graph. Next we prove $Q_{tp}$ can be represented as:
    \bea
    Q_{tp} = L(\calG) 
    \eea     
    where $L(\calG)$ is the Laplacian of $\calG$:
    \begin{lemma}
        \label{lemma:laplacian}
        $Q_{tp}$ is the Laplacian of $\calG$. \\
        \textit{proof:} Note that $\calG$ is a weighted undirected graph. Calling $\delta(q)$ for the set of edges incident to a vertex $q$, and $w_e = w_{qp}$ for $e = (q,p)$, the Laplacian of $\calG$ is:
            \bea
            \label{eq:laplacian}
            L(\calG)_{qp} = 
            \begin{cases}
                \sum_{e \in \delta(q)} w_e & \text{if } q = p, \\
                - w_{qp} & \text{if } (q,p)\in \calE, \\
                0 & \text{if } (i,j) \notin \calE.
            \end{cases}
            \eea
        On the other hand, by expanding \prettyref{eq:app-Qtp-restate} and compare it with \prettyref{eq:laplacian}, we finish the proof.
    \end{lemma}
    
     Calling $\bar{y} = [t_2;\dots;t_{N};p_1;\dots;p_{N+M}] \in \Real{3(N+M)-3}$ and $\bar{Q}_{tp} = [c_2;\dots;c_{N+M}] \in \Real{(N+M) \times (N+M-1)}$ where $c_i$ is the i-th column of $Q_{tp}$. We prove when fix $s_1 = 1$, $R_1 = \eye_3$, $t_1 = 0$, $y$ has an unique solution:
    \bea
    (\bar{Q}_{tp} \kron \eye_3) {\bar{y}} = -(V_{tp} \kron \eye_3)\tran r
    \eea
    Since $\rank{L(\calG)}=N+M-1$ and $\sum_{i=1,..,N} c_i = \mathbf{0}_{N}$, then $span(c_1,...,c_n) = span(c_2,...,c_n)$ and $\rank{\bar{Q}_{tp}}=N+M-1$ which implies that $\bar{Q}_{tp}$ has full column rank. Hence, by taking inverse and rearrange, we obtain
    \bea 
    \bar{y} = & \parentheses{\bar{A} \kron \eye_3} r
    \eea 
    where
    \bea
    \bar{A} = & -(\bar{Q}_{tp} \tran \bar{Q}_{tp})\inv \bar{Q}_{tp} \tran V_{tp} \tran
    \eea
    Together with $t_1 = 0$,
    \bea
    y = & \parentheses{A \kron \eye_3} r
    \eea
    with
    \bea
    A = & \bmat{c} \zero_{1 \times 3N} \\ -(\bar{Q}_{tp} \tran \bar{Q}_{tp})\inv \bar{Q}_{tp} \tran V_{tp} \tran \emat \in \Real{(N+M) \times 3N}
    \eea
    Now we have a closed form solution of $y$. Plug in the solution of $y$ into \eqref{eq:problem_yr_only}, we obtain:
    \bea
    L(r) = & \displaystyle r \tran \parentheses{\parentheses{A \tran Q_{tp} A + V_{tp}A + A\tran V_{tp}\tran + Q_1} \kron \eye_3} r \nonumber
    \eea
    Note that
    \bea
    r = [\vectorize{s_1 R_1};\dots;\vectorize{s_N R_N}] = \vectorize{U} \nonumber
    \eea
    Then $L(r)$ is equivalent to
    \bea
    \label{loss_R}
    \vectorize{U} \tran \parentheses{\parentheses{A \tran Q_{tp} A + V_{tp}A + A\tran V_{tp}\tran + Q_1} \kron \eye_3} \vectorize{U}\nonumber
    \eea
    Rewrite this in a more compact matricized form gives:
    \begin{align}
        \rho^\star = \min_{R} &~\trace{QU \tran U} \\
        Q &\coloneqq  A \tran Q_{tp} A + V_{tp}A + A\tran V_{tp}\tran + Q_1 \in \sym{3N}
    \end{align}
    concluding the proof.
\end{proof}


\subsection{Proof of Proposition \prettyref{prop:qcqp}}\label{app:proof_qcqp}

\begin{proof}
    We show that under \prettyref{eq:determinant}, if $U^\star$ is a global optimizer of \prettyref{eq:problem-qcqp}, then it is also a global optimizer of \prettyref{eq:problem_scale_rotation_only}.  

    First, note that \prettyref{eq:problem-qcqp} is a relaxation of \prettyref{eq:problem_scale_rotation_only}, since $\Othree \subseteq \SOthree$. Thus, we have  
    \bea\label{eq:qcqp_relaxation} 
      \trace{Q(U^\star)\tran U^\star} = \rho^\star_{\qcqp} \leq \rho^\star.
    \eea  
    From \prettyref{eq:determinant} and the definition $\bar{R}^\star_i = s^\star_i R^\star_i$, we obtain  
    \bea
      \det(R^\star_i) = \frac{\det(\bar{R}^\star_i)}{(s^\star_i)^3} > 0.
    \eea  
    This implies that $U^\star$ is also a feasible solution to \prettyref{eq:problem_scale_rotation_only}. By \prettyref{eq:qcqp_relaxation}, $U^\star$ is therefore the global optimizer of \prettyref{eq:problem_scale_rotation_only}.
    
\end{proof}

\subsection{Proof of Proposition \prettyref{prop:sdprelaxation}}\label{app:proof_sdp_relaxation}

\begin{proof}
    We first establish that \prettyref{eq:problem-sdp} is a relaxation of \prettyref{eq:problem-qcqp}. Let $X = U\tran U$. By definition, we have $x\tran X x = (Ux)\tran Ux \geq 0$ for all $x \in \Real{3N}$, which implies that $X \succeq 0$.  

    Furthermore, since $\bar{R}_i \in \sOthree$, it follows that $\bar{R}^\tran_i \bar{R}_i = s_i^2 \eye_3 \triangleq \alpha \eye_3$. This ensures that the feasible set of \prettyref{eq:problem-sdp} is broader than that of \prettyref{eq:problem-qcqp}, confirming that it is indeed a relaxation.


    Next we prove if $\rank{X^\star} = 3$, we can extract the global minimizer of \prettyref{eq:problem-qcqp} from $X^\star$. Let $X^\star = (\bar{U}^\star)\tran \bar{U}^\star$, then $\bar{U}^\star$ is rank 3. We can decompose $\bar{U}^\star$ as 
    \bea
    \bar{U}^\star = \bmat{cccc} \bar{R}^\star_1 & \bar{R}^\star_2 & \dots & \bar{R}^\star_N \emat
    \eea
    Since $X$ is feasible, we have $\bar{R}^\star_i \bar{R}^\star_i = \alpha \eye_3 = s_i^2 \eye_3$, which implies that $\bar{R}^\star_i / s_i \in \Othree$.  

    Define $U^\star = \bar{R}_1^{\star T} \bar{U}^\star$, making $U^\star$ a feasible solution to \prettyref{eq:problem-qcqp}. Moreover, since  
    \bea
    \trace{Q(U^\star)\tran U^\star} = \rho^\star_{\sdp} \leq \rho^\star_{\qcqp},  
    \eea  
    it follows that $U^\star$ is the global minimizer of \prettyref{eq:problem-qcqp}.


\end{proof}

% \begin{proof}
%     From $x_k \rightarrow x^\star$ we have $p_k = x_{k+1}-x_k \rightarrow 0$. By the definition of limitation, there $\exists K, \forall k > K, \norm{p_k}<\Delta_{\mathrm{min}}$, which indicted that the trust-region has no effect on the subproblem. In the proof we will only consider $k > K$ and the subproblem only involves equality constraints. For inequality constraint it shares the same proof technology and we will mention it at end.

%     Merit function $\phi_1(x;\mu)$ is defined by 
%     \bea
%         \phi_1(x;\mu) \triangleq J(x) +  \sum_{i \in \mathcal{E}} \mu_i| c_i(x) |
%     \eea
%     where $|\mathcal{E}| = m_{\mathcal{E}}$, $x \in \Real{n}$. We rewrite the trust-region subproblem as a function of $x_{k+1}$ instead of $p_k$: ($p_k = x_{k+1}-x_k$)
%     \bea
%         x_{k+1} = \displaystyle \argmin_x \{ q_{\mu,k}(x) = J(x) + \sum_{i \in \mathcal{E}}\mu_i \lvert c_i(x_k) + \nabla c_i(x_k)^\top (x - x_k) \rvert \}
%     \eea
%     Denote $w_{k,i}$ as:
%     \begin{equation}
%         w_{k,i} \triangleq c_i(x_k) + \nabla c_i(x_k)^\top (x_{k+1} - x_k) \in \mathbb{R},
%     \end{equation}
%     Since the norm \( \|\cdot\|_1 \) is piecewise smooth, we can partition \( \mathbb{R} \) into 3 sets and correspondingly, separate $w_{k,i}$ into three sets:
%     \begin{align}
%         \{w_{k,i} < 0\}, \{w_{k,i} = 0\}, \{w_{k,i} > 0\}
%     \end{align}
%     and after we concatenate $w_{k,i}$ to $w_k = \bmat{ccc}w_{k,1}& \dots &w_{k,m_{\calE}}\emat$, $\Real{m_{\calE}}$ can be divided to $3^{m_{\calE}}$ sets without overlapping.
    
%     Since the number of such sets is finite, there must exist a subsequence $w_{n_k}$ that belongs to one of these sets. Denoting this limiting set as \( S \), we assume that in this set, the first \( m_1 \) components of \( w_k \) are positive, the next \( m_1 + 1 \) to \( m_1 + m_2 \) components are negative, and the remaining \( m_1 + m_2 + 1 \) to \( m_{\calE} \) components are zero.
    
%     For any \( p \in \mathbb{R}^n \), we define the directional derivatives\footnote{Here we use $\downarrow$ for simple proof, but this is equivalent to $\rightarrow$ because one can easily choose $-p$ if $t < 0$}:
%     \begin{align}
%         D(\phi_1(x^\star;\mu);p) &\triangleq \lim_{t \downarrow 0} \frac{\phi_1(x^\star + tp;\mu) - \phi_1(x^\star;\mu)}{t}, \\
%         D(q_{\mu,n_k}(x_{n_{k}+1});p) &\triangleq \lim_{t \downarrow 0} \frac{q_{\mu,n_k}(x_{n_{k}+1} + tp) - q_{\mu,n_k}(x_{n_{k}+1})}{t}.
%     \end{align}
%     Since $x_{n_k+1}$ is the global minimizer of the convex function $q_{\mu,n_k}(x)$, we have $D(q_{\mu,n_k}(x_{n_{k}+1});p) \geq 0$. On the other hand,
%     \begin{align}
%         D(q_{\mu,n_k}(x_{n_{k}+1});p) =\\
%          \lim_{t \downarrow 0} \frac{J(x_{n_k+1} + tp) + \sum_{i\in \calE}| w_{n_k} + \nabla c_i(x_{n_k})^\top p | - J(x_{n_k+1}) - | w_{n_k} |}{t} \\
%         = \nabla J(x_{n_k+1})^\top p + \lim_{t \downarrow 0} \frac{\sum_{i\in \calE}| w_{n_k} + \nabla c_i(x_{n_k})^\top p | - | w_{n_k} |}{t}.
%     \end{align}

%     Using the fact that:
%     \begin{equation}
%         \lim_{t \downarrow 0} \frac{| a + tb | - | a |}{t} = \begin{cases}
%             b, & a > 0 \\
%             -b, & a < 0 \\
%             |b|, & a = 0
%         \end{cases}
%     \end{equation}
    
%     Since $J(x)$ is convex quadratic and $\nabla c_i$ is Lipschitz, as $k \to \infty$:
%     \begin{align}
%         \nabla J(x_{n_k+1}) &\to \nabla J(x^\star), \\
%         \nabla c_i(x_{n_k}) &\to \nabla c_i(x^\star).
%     \end{align}
    
%     Thus,
%     \begin{align}
%         D(q_{\mu,n_k}(x_{n_{k}+1});p) \xrightarrow[k \to \infty]{} \\
%         \nabla J(x^\star)^\top p + \sum_{i=1}^{m_1} \nabla c_i(x^\star)^\top p - \sum_{i=m_1+1}^{m_1+m_2} \nabla c_i(x^\star)^\top p +\\
%          \sum_{i=m_1+m_2+1}^{m} |(\nabla c_i(x^\star)^\top p)| \geq 0.
%     \end{align}

%     The last inequality comes from the fact that $a_k \to a^\star$, $a_k \geq 0 \implies a^\star \geq 0$.
    
%     Now we turn to calculate $D(\phi_1(x^\star;\mu);p)$. One should be careful: since $S$ is not closed, even the first $m_1$ components of $w_{n_k}$ are always positive, the corresponding components in the converged point $w^\star = \bmat{ccc}c_1(x^\star)&\dots&c_{m_\calE}(x^\star)\emat $ will have the opportunity to converge to 0. Similar things happen to the negative part. However, the $m_1 + m_2 + 1 \sim m$ components of $w^\star$ will always be 0. To fix this, we assume for the $1 \sim m_1$ components of $w^\star$, $1 \sim m_{1,+}$ are positive, while $m_{1,+} + 1 \sim m_1$'s components of $w^\star$ become zero. Similarly, $m_1 + 1 \sim m_1 + m_{2,-}$ are negative, while $m_1 + m_{2,-} + 1 \sim m_1 + m_2$'s components of $w^\star$ become zero. Thus,
    
%     \begin{align}
%         D(\phi_1(x^\star;\mu);p) &= \lim_{t \downarrow 0} \frac{J(x^\star + tp) + \sum_{i \in \calE}|c_i(x^\star + tp)| - J(x^\star) - \sum_{i \in \calE}|c_i(x^\star)|}{t} \\
%         &= \nabla J(x^\star)^\top p + \sum_{i=1}^{m_{1,+}} \nabla c_i(x^\star)^\top p + \sum_{i=m_{1,+}+1}^{m_1} |\nabla c_i(x^\star)^\top p| \\
%         & - \sum_{i=m_1+1}^{m_1+m_{2,-}} \nabla c_i(x^\star)^\top p+ \sum_{i=m_1+m_{2,-}+1}^{m_1+m_2} |\nabla c_i(x^\star)^\top p| \\
%         & + \sum_{i=m_1+m_2+1}^{m} |\nabla c_i(x^\star)^\top p|.
%         & \geq \nabla J(x^\star)^\top p + \sum_{i=1}^{m_1} \nabla c_i(x^\star)^\top p 
%         & - \sum_{i=m_1+1}^{m_1+m_2} \nabla c_i(x^\star)^\top p
%         & + \sum_{i=m_1+m_2+1}^{m} |\nabla c_i(x^\star)^\top p|.
%         & \geq 0
%     \end{align}
%     For inequality constraints we define $w_{k,i}$ as the same. We still separate $\Real{}$ into 
%     \bea
%     \{w_{k,i} < 0\}, \{w_{k,i} = 0\}, \{w_{k,i} > 0\}
%     \eea
%     and similarly, we have
%     \begin{equation}
%         \lim_{t \downarrow 0} \frac{| a + tb |^- - | a |^-}{t} = \begin{cases}
%             0, & a > 0 \\
%             -b, & a < 0 \\
%             |b|^-, & a = 0
%         \end{cases}
%     \end{equation}
%     Other parts of the proof are similar.

%     Consequently $D(\phi_1(x^\star;\mu);p) \geq 0 \quad\forall p$, we conclude the proof.
% \end{proof}