%!TEX root = main.tex
\section{QCQP and Convex SDP Relaxation}
\label{sec:method}

In this section, let us focus on solving the SBA problem~\eqref{eq:sba} to certifiable global optimality. We proceed in two steps. In \S\ref{sec:qcqp}, we simplify the original formulation as a quadratically constrained quadratic program (QCQP) through a sequence of mathematical manipulations. In \S\ref{sec:sdp-relax}, we apply Shor's semidefinite relaxation to ``convexify'' the nonconvex QCQP. 

Before we get started, we remove the ambuiguity of problem~\eqref{eq:sba} through anchoring.

% The original SBA problem \prettyref{eq:sba} involves numerous terms and variables, making it challenging to scale effectively. We reformulate the problem as a QCQP problem, which is in matrix form and only involves the product of scales and rotations. Furthermore, we employ \red{the first-order Lasserre's Hierarchy} to derive a convex SDP relaxation of the QCQP, enabling us to solve the problem to global optimality. We establish the relationship between the QCQP formulation, the SDP relaxation, and the original problem \prettyref{eq:sba}, and prove that under certain conditions, which empirically hold in real-world data, the global minimizer of \prettyref{eq:sba} can be recovered. Before proceeding, we need to anchor the first camera to avoid singularities.

{\bf Anchoring}. Observe that one can choose $s_i \rightarrow 0, \forall i = 1, \dots, N$, $t_1 = \dots = t_N = p_1 = \dots = p_M = {\bf 0}$, and the objective value of~\eqref{eq:sba} can be set arbitrarily close to zero. Additionally, multiplying an arbitary rotation matrix on $R_i, t_i, p_i$ does not change the objective of~\eqref{eq:sba}, leading to infinitely many solutions. To resolve these issues, we anchor the first frame and set $R_1 = \eye_3, t_1 = \zero, s_1 = 1$.


% \begin{proposition}[\nameshort Problem Formulation in Matrix Form]
%     \label{prop:formulation_matrix_form}
%     Let $t = [t_1,\dots,t_N] \in \Real{3N}$ be the concatenation of translations, $p = [p_1,\dots,p_M] \in \Real{3M}$ be the concatenation of landmark positions, and $r = [\vectorize{s_1 R_1}\tran,\dots,\vectorize{s_N R_N}\tran]\tran \in \Real{9N}$ be the concatenation of (vectorized) scaled rotations. The objective function of \prettyref{eq:sba} can be reformulated as
%     \begin{align}\label{eq:problem_matrix_obj}
%         L(t,p,r) &= r^\top (Q_1 \otimes \eye_3)r + t^\top (Q_2 \otimes \eye_3)t + p^\top (Q_3 \otimes \eye_3)p \nonumber\\
%         &+ 2 r^\top (V_1 \otimes \eye_3)t - 2 r^\top (V_2 \otimes \eye_3)p \nonumber\\
%         &- 2 t^\top (V_3 \otimes \eye_3)p.
%     \end{align}
%     The corresponding optimization problem is:
%     \begin{align}
%         \label{eq:problem_matrix}
%         \min_{\substack{t \in \Real{3N}, p \in \Real{3M}} } & L(t,p,r)\nonumber\\
%         \subject & r = \bmat{ccc} \vectorize{s_1 R_1} & \cdots & \vectorize{s_N R_N} \emat \nonumber\\
%         & s_i > 0, R_i \in \SOthree, \quad i=1,\cdots,N 
%     \end{align}
%     where the matrice $Q_1,Q_2,Q_3,V_1,V_2,V_3$ and the detailed proof are provided in \prettyref{app:proof_formulation_matrix_form}.
% \end{proposition}

% Next, we aim to simplify the problem by explicitly solving for $t,p$. However, this process encounter singularities:

% \begin{theorem}
%     \label{thm:singularities}
%     The optimization problem in \prettyref{eq:problem_matrix} is a convex quadratic optimization problem with respect to $t$ and $p$, and it is unconstrained in these variables. However, there are infinite many solutions for $\nabla_{t,p} L(t,p,r)$.
% \end{theorem}

% The proof is provided in \prettyref{app:proof_singularities}. To resolve singularities, we anchor the first camera: 

% % \red{do we need to constrain the observation in each camera? if cameras are only observing one pint, then there should be multiple solutions?}

% {\bf Anchoring}. \prettyref{eq:sba} admits infinitely many solutions as proved in \prettyref{thm:singularities}. Furthermore, the problem is ill-defined: one can set $ s_i \rightarrow 0, \forall i = 1, \dots, N $, $ t_1 = t_2 = \dots = t_N = p_1 = p_2 = \dots = p_M = {\bf 0} $, and the objective value of \prettyref{eq:sba} can be made arbitrarily close to zero. To address this issue, we impose an anchoring constraint by fixing the first frame, setting $ R_1 = \eye_3 $, $ t_1 = \zero $, and $ s_1 = 1 $.


% % \red{One drawback is that this makes the images asymmetric (the first frame is different from others), and if the data quality is insufficient good, the scale of the first frame may become significantly larger than that of the others. We have considered using $(\sum_i s_i = 1)$ as an alternative, but this involves a more complex manifold, making implementation challenging.}

\subsection{QCQP Formulation}
\label{sec:qcqp}

We first show that the unconstrained variables in~\eqref{eq:sba}, namely the translations $t_i$ and the 3D landmarks $p_k$, can be ``marginalized out'', leading to an optimization problem only concerning the scaling factors and 3D rotations.

\begin{proposition}[Scaled-Rotation-Only Formulation]
    \label{prop:formulation_scale_rotation_only}
    Problem \prettyref{eq:sba} is equivalent to the following optimization
    \begin{subequations}\label{eq:problem_scale_rotation_only}
        \begin{align}
            \rho^\star = \min & \ \ \trace{Q U\tran U}  \\
            \subject & \ \ U = \bmat{cccc} \eye_3 & s_2R_2 & \cdots & s_N R_N \emat\\
             &\ \ s_i >0, R_i \in \SOthree, \quad i=2,\cdots,N
            \end{align}
    \end{subequations}
    where $Q \in \sym{3N}$ is a constant and symmetric ``data matrix'' whose expression is given in \prettyref{app:proof_scale_rotation_only}. 

    
    % can be computed as 
    % \bea \label{eq:Q}
    % Q = A\tran Q_{tp} A + V_{tp}A + A\tran V_{tp}\tran + Q_1 \in \sym{3N},
    % \eea 
    % with $A \in \Real{(N+M) \times 3N}$, $Q_{tp} \in \Real{(N+M) \times (N+M)}$, $Q_1 \in \Real{3N \times 3N}$ and $V_{tp} \in \Real{3N \times (N+M)}$ can be found in \prettyref{app:proof_scale_rotation_only}.

    % \bea 
    % A = & \bmat{c} \zero_{1\times 3N} \\ (\bar{Q}_{tp}\tran \bar{Q}_{tp})\inv \bar{Q}_{tp}\tran V_{tp}\tran \emat \in \Real{(N+M) \times 3N} \\
    % Q_{tp} = & \bmat{cc} Q_2 & -V_3  \\ -V_3 \tran & Q_3 \emat \in \Real{(N+M) \times (N+M)} \\
    % V_{tp} = & \bmat{cc} V_1 & -V_2 \emat \in \Real{3N \times (N+M)}
    % \eea 
    % Let $\bar{Q}_{tp} \in \Real{N+M \times (N+M-1)}$ denote the submatrix consisting of the last $N+M-1$ columns of $Q_{tp}$. Furthermore, 
    
    Let $U^\star$ represent the optimal solution of \prettyref{eq:problem_scale_rotation_only} and let $t = [t_1;\dots;t_N] \in \Real{3N}$ be the concatenation of translations, $p = [p_1;\dots;p_M] \in \Real{3M}$ be the concatenation of landmark positions. The optimal translations $t^\star$ and landmark positions $p^\star$ of problem~\eqref{eq:sba} can be recovered from $U^\star$ as follows:
    \bea \label{eq:tpstar}
    \bmat{c} t^\star \\ p^\star  \emat = (A \kron \eye_3) \vectorize{U^\star}.
    \eea 
    where the expression of $A$ can be found in \prettyref{app:proof_scale_rotation_only}. 
\end{proposition}
\begin{proof}
    See \prettyref{app:proof_scale_rotation_only}.
\end{proof}

Proposition~\ref{prop:formulation_scale_rotation_only} reformulates the SBA problem~\eqref{eq:sba} as a lower-dimensional problem~\eqref{eq:problem_scale_rotation_only}. However, problem~\eqref{eq:problem_scale_rotation_only} is not a QCQP because the objective function is a degree-four polynomial in $s_i$ and $R_i$. In the next step, we show that it is possible to combine the ``scaled rotation'' $s_i R_i$ together as a new variable, effectively reducing the degree of the polynomial.

% To address the positivity constraint and the $\SOthree$ constraint on scale and rotation, we combine them into $\sOthree$, which represents the set of matrices formed by the product of positive scalars and rotation matrices.




% Thus, the problem can be formulated as a Quadratic Constrained Quadratic Programming (QCQP) problem:

\begin{proposition}[QCQP Formulation]\label{prop:qcqp}
    Define the set of scaled orthogonal group as
    \bea \label{eq:sOthreedef}
    \sOthree = \{ \bar{R} \in \Real{3\times 3} \mid \exists s > 0, R \in \Othree~\text{s.t.}~ \bar{R} = sR \}.
    \eea 
    The set $\sOthree$ can be described by quadratic constraints
    \begin{equation} \label{eq:sOthreequadratic}
    \begin{split}
        \bar{R} = \bmat{ccc} c_1 & c_2 & c_3 \emat \in \sOthree \Longleftrightarrow \\ \begin{cases}
            c_1\tran c_1 = c_2\tran c_2 = c_3\tran c_3 \\
            c_1\tran c_2 = c_2\tran c_3 = c_3\tran c_1 = 0 .
        \end{cases}
    \end{split}
    \end{equation} 
    Consider the following QCQP
    \begin{subequations}\label{eq:problem-qcqp}
        \begin{align} 
            \rho_{\qcqp}^\star = \min  &\ \ \trace{Q U\tran U}  \\
            \subject & \ \ U = \bmat{cccc} \eye_3 & \bar{R}_2 & \cdots & \bar{R}_N \emat\\
            &\ \ \bar{R}_i \in \sOthree, ~i = 2,\dots,N.
        \end{align}
    \end{subequations}
    and let $U^\star = [\eye_3, \bar{R}_2^\star, \dots,\bar{R}_N^\star]$ be a global optimizer. If 
    \bea \label{eq:determinant}
    \det{\bar{R}_i^\star } > 0, i=2,\dots,N,
    \eea 
    then $U^\star$ is a global minimizer to problem \prettyref{eq:problem_scale_rotation_only}.
\end{proposition}
\begin{proof}
    See \prettyref{app:proof_qcqp}.
\end{proof}

It is clear that 
\bea\label{eq:qcqp-inequality}
\rho^\star_\qcqp \leq \rho^\star
\eea
because from~\eqref{eq:problem_scale_rotation_only} to~\eqref{eq:problem-qcqp} we have relaxed the $\SOthree$ constraint to $\Othree$.
After solving~\eqref{eq:problem-qcqp},  
if there exists some index $i$ such that $\det{\bar{R}_i^\star} < 0$, we can extract a feasible solution to~\eqref{eq:problem_scale_rotation_only} by projecting onto $\SOthree$ after extracting the scaling from $\bar{R}_i^\star$.

\begin{remark}[Bad Local Minimum]
    Problem~\eqref{eq:problem-qcqp} is a nonconvex QCQP. In fact, it is a smooth Riemannian optimization by identifying $\sOthree$ as a product manifold of the positive manifold and the Orthogonal group. Therefore, one can directly use, e.g., \manopt to solve~\eqref{eq:problem-qcqp} (and also~\eqref{eq:problem_scale_rotation_only}). However, as we will show in \S\ref{sec:exp} on the Mip-Nerf 360 dataset~\cite{barron2022mipnerf360}, directly solving~\eqref{eq:problem-qcqp} can get stuck in bad local minima. 
\end{remark}

This motivates and necessitates convex relaxation.

% , and the suboptimality gap can be given as

% \begin{align}
%    0 \leq \rho^\star - \rho_{\qcqp}^\star = \trace{Q U_{\SOthree}^{\star T} U_{\SOthree}^\star - Q U^{\star T} U^\star} 
% \end{align}

% The advantage of the \prettyref{eq:problem-qcqp} problem compared to \prettyref{eq:ba-colmap} is that it does not contain a projection function, making it more convex and easier to solve. However, it still has local minimum. As an example, we illustrate this using the Mip-Nerf 360 dataset \cite{barron2022mipnerf360} (\cf \prettyref{sec:exp}). To completely avoid local minimum, we can use the following SDP relaxation to solve a convex problem:

\subsection{Convex SDP Relaxation}
\label{sec:sdp-relax}

For any QCQP, there exists a convex relaxation known as Shor's semidefinite relaxation~\cite[Chapter 3]{yang24book-sdp}. The basic idea is fairly simple: by creating a matrix variable $X := U\tran U$ that is quadratic in the original variable $U$, problem~\eqref{eq:problem-qcqp} becomes \emph{linear} in $X$. The convex relaxation proceeds by using convex positive semidefinite constraints and linear constraints to properly enfoce the matrix variable $X$. 

\begin{proposition}[SDP Relaxation]\label{prop:sdprelaxation}
    The following semidefinite program (SDP)
    \begin{subequations}\label{eq:problem-sdp}
        \begin{align}
            \rho^\star_{\sdp} = \min_{X \in \sym{3N}} &\ \  \trace{QX}  \\
            \subject & \ \ X = \bmat{ccc} \eye_3 & \cdots & * \\
            \vdots & \ddots & \vdots \\
            * & \cdots & \alpha_N \eye_3
            \emat \succeq 0 \label{eq:sdp-X-compact}
            \end{align}
    \end{subequations}
    is a convex relaxation to \prettyref{eq:problem-qcqp}, i.e., 
    \begin{equation}\label{eq:sdp-inequality}
        \rho^\star_\sdp \leq \rho^\star_\qcqp.
    \end{equation}
    Let $X^\star$ be a global minimizer of \prettyref{eq:problem-sdp}. If $\rank{X^\star} = 3$, then $X^\star$ can be factorized as $X^\star = (\bar{U}^\star) \tran \bar{U}^\star$, and \footnote{$\bar{R}_1^\star$ is in $O(3)$ because we restrict the scale of the first frame to be 1. }
    \bea
    \bar{U}^\star = \bmat{cccc} \bar{R}_1^\star & \bar{R}_2^\star & \cdots & \bar{R}_N^\star \emat \in O(3)\times\sOthree^{N-1}. 
    \eea
    Define 
    \bea
    U^\star = (\bar{R}_1^\star)\tran \bar{U}^\star,
    \eea
    then $U^\star$ is a global optimizer to \prettyref{eq:problem-qcqp}. In this case, we say the relaxation is \emph{tight} or \emph{exact}.
\end{proposition}
\begin{proof}
    See \prettyref{app:proof_sdp_relaxation}.
\end{proof}

If $\rank{X^\star} > 3$, then we can extract feasible solutions to~\eqref{eq:problem-qcqp} by taking the top three eigenvectors of $X^\star$ and project the corresponding entries to $\sOthree$ and further to scaled rotations, a step that is typically called \emph{rounding}. Denote $\hat{U}$ as the rounded solution that is feasible for~\eqref{eq:problem_scale_rotation_only}, we can evaluate the objective of~\eqref{eq:problem_scale_rotation_only} at $\hat{U}$ and denote it $\hat{\rho}$. Combing the chain of inequalities from~\eqref{eq:qcqp-inequality} and~\eqref{eq:sdp-inequality}, we get
\bea\label{eq:chain-inequality}
\rho^\star_\sdp \leq \rho^\star_\qcqp \leq \rho^\star \leq \hat{\rho},
\eea 
where the last inequality follows from $\hat{U}$ is a feasible solution for the minimization problem~\eqref{eq:problem_scale_rotation_only}. Assuming we can solve the convex SDP, we compute $\rho^\star_\sdp$ and $\hat{\rho}$ at both ends of the inequality~\eqref{eq:chain-inequality}, allowing us to evaluate a relative suboptimality:
\bea \label{eq:suboptimality}
\eta = \frac{\hat{\rho} - \rho^\star_\sdp}{1 + |\hat{\rho}| + |\rho^\star_\sdp|}.
\eea
$\eta \rightarrow 0$ certifies global optimaity of the rounded solution $\hat{U}$, and tightness of the SDP relaxation.

\begin{tcolorbox}
    \begin{center}
        \vspace{-1mm}
        \textbf{Summary}
        \vspace{-1mm}
    \end{center} 
    From~\eqref{eq:sba} to~\eqref{eq:problem_scale_rotation_only}, we first eliminated translations and landmark positions. From~\eqref{eq:problem_scale_rotation_only} to~\eqref{eq:problem-qcqp}, we formulated a QCQP by creating the new constraint set $\sOthree$. From~\eqref{eq:problem-qcqp} to~\eqref{eq:problem-sdp}, we applied Shor's semidefinite relaxation. This sequence of manipulations and relaxations allows us to focus on solving the convex SDP problem~\eqref{eq:problem-sdp} while maintaining the ability to certify (sub)optimaity of the original nonconvex problem~\eqref{eq:sba}, through the inequalities established in~\eqref{eq:chain-inequality}. 
\end{tcolorbox}


\begin{remark}[Connection to Prior Work]\label{eq:connection}
    For readers familiar with SDP relaxations, this section should not be surprising at all---that is the reason why we kept this section very brief and only focus on milestone results listed in the propositions. The closest two works to ours are \sesync~\cite{rosen2019se} and \simsync~\cite{yu2024sim}, and the SDP relaxation technique traces back to at least the work by Carlone et al.~\cite{carlone2015lagrangian}. The novelty of our formulation are twofold: (a) we estimate scaling factors with the motivation to correct learned depth while \sesync estimates rotations and translations only; (b) we jointly estimate 3D landmarks and (scaled) camera poses while \simsync does not estimate 3D landmarks. These differences allow us to solve the long-standing bundle adjustment problem with convex optimization.
\end{remark}

We shall focus on how to solve the convex SDP~\eqref{eq:problem-sdp}.

% from~The QCQP formalation eliminates translation $t$ and landmark position $p$ by solving linear system \prettyref{eq:tpstar}. The SDP formulation relax the QCQP problem to a convex problem, thus global optimality can be certified. The following chain of inequalities holds for the three problems:
% \bea
% \rho^\star \geq \rho_{\qcqp}^\star \geq \rho_{\sdp}^\star.
% \eea
% These inequalities become equalities when the rank of $X^\star$ is 3 and \prettyref{eq:determinant} holds.



% \begin{theorem}
%     Goal: Our SCP-based solver converge to a stationary point of merit function $\mathcal{M}$.
%     \bea
%     \mathcal{M}(x) = f(x) + \sum |h_i(x)| + \sum |g_i(x)|_+
%     \eea
%     where $f$ is a convex quadratic function. 
%     Assume $x_k \rightarrow x^\star$ and trust region radius has lower bound $D$.

%     (1)$\exists M>0$ such that
%     \bea
%     ||x_k - x_{k+1}|| < D , \forall k \geq M
%     \eea
%     And the kth-subproblem is given by
%     \begin{align}
%         \min_{p} f(x_k+p) + \sum |h_i(x_k)+\nabla h_i(x_k)^Tp| + \nonumber\\
%          \sum |g_i(x_k)+\nabla g_i(x_k)^Tp|_+ \nonumber\\
%         \subject ||p|| \leq D_k
%     \end{align}
%     It is a convex problem, and $x_{k+1}-x_k,k\geq M$ is the global minimizer of the subproblem.

%     (2) If $h_i(x^\star) \neq 0 $ or $g_i(x^\star) \neq 0$, then the merit function degenerate to $h_i(x)$ or $-h_i(x)$ around $x^\star$, which can be absorbed into $f(x)$.
%     So we only need to consider the case where $h_i(x^\star) = 0$ or $g_i(x^\star) = 0$.

%     (3) 
%     \begin{align}
%       \mathcal{M}(x^\star) = f(x^\star)\nonumber\\
%       \mathcal{M}(x^\star +tp) = f(x^\star + tp) + \sum |h_i(x^\star + tp)| \\
%       + \sum |g_i(x^\star + tp)|_+\nonumber
%     \end{align}
%     then we get
%     \begin{align}
%         L(p) = \lim_{t \rightarrow 0^+} \frac{\mathcal{M}(x^\star + tp) - \mathcal{M}(x^\star)}{t} = \nabla f(x^\star)^Tp\\
%         + \sum |\nabla h_i(x^\star)^Tp| + \sum |\nabla g_i(x^\star)^Tp|_+
%     \end{align}
%     \red{To prove local minimum, we will need to prove the limit is always positive, or use second order condition.}

%     (4) Now we consider the subproblem, we have
%     \begin{align}
%         L_s(p) = \lim_{t \rightarrow 0^+} \frac{\mathcal{M}_s(x_{k+1} + tp) - \mathcal{M}_s(x_{k+1})}{t} \\
%         = \nabla f(x_k)^Tp
%         + \sum s_{h_i}\nabla h_i(x_k)^Tp + \sum s_{g_i}\nabla g_i(x_k)^Tp\\
%          s_{h_i} = sign(h_i(x_k)+\nabla h_i(x_k)^T(x_{k+1}-x_k))\\
%             s_{g_i} = sign_+(g_i(x_k)+\nabla g_i(x_k)^T(x_{k+1}-x_k))
%     \end{align}
%     We should assume the term inside $sign$ is not zero. Because it is global minimizer, the limit is always non-negative.

%     (5) For $g_i(x)$, we can choose a subseqence $x_{n_k}$ such that $s_{g_i}$ and $s_{h_i}$ are the same for all $k$. Then we have
%     \bea
%     0  \leq \lim_k L_s(p) \leq L(p)
%     \eea
%     For $h_i(x)$ part this is easy: 
%     \bea
%     \lim_k s_{h_i}\nabla h_i(x_k)^Tp 
%      \leq \lim_k |\nabla h_i(x_k)^Tp|
%      = |\nabla h_i(x^\star)^Tp|
%     \eea
%     For $g_i(x)$ part, if $s_{g_i} = 0$, then 
%     \bea
%     \lim_k s_{g_i}\nabla g_i(x_k)^Tp = 0 
%     \leq |\nabla g_i(x^\star)^Tp|_+
%     \eea
%     if $s_{g_i} = 1$, then 
%     \bea
%     \lim_k s_{g_i}\nabla g_i(x_k)^Tp
%     = \nabla g_i(x^\star)^Tp 
%     \leq |\nabla g_i(x^\star)^Tp|_+
%     \eea

%     (6) If the term inside $sign$ is 0, we replace $s_{h_i}\nabla h_i(x_k)^Tp$ with $|\nabla h_i(x_k)^Tp|$ and the proof is still valid.

%     (7)If $L(p) \leq 0$, by definition the clarke subdifferential contains 0:
%     \bea
%     {\displaystyle \mathcal{M}^{\circ }(x^\star,p)=\limsup _{y\rightarrow x^\star,t\downarrow 0}{\frac {f(y+tp)-f(y)}{t}},}\\
%     \leq L(p) \leq 0 = \langle 0,p \rangle
%     \eea
%     Further more, if $L(p) > 0,~\forall p$, we converge to a local minimum. 

% \end{theorem}   

    