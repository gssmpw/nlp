%!TEX root = ../main.tex

\section{Suboptimality for SDP}
\label{app:suboptimality}

In \prettyref{eq:suboptimality}, we established suboptimality when \prettyref{eq:problem-sdp} is solved to global optimality. However, in practice, numerical errors arise, and variations in gradient tolerance settings can lead to increased suboptimality. Here, we provide a rigorous proof for computing suboptimality in SDP problems and apply it directly to the scale regularization problem in \prettyref{eq:problem-sdp-scale}.  

\begin{theorem}
    Given the primal problem \prettyref{eq:problem-sdp-scale} we have 
    \bea
        \rho_\sdp \geq \rho^\star_\sdp \geq \text{max}(0,\lambda_{\text{min}}(Z)) \text{tr}(X) + \rho_{\text{dual}}
    \eea
\end{theorem}
\begin{proof}
    for all feasible $X$ we have:
    \begin{align}
    \langle Q, X \rangle + F(X) &\geq \langle Q - \sum_i y_i A_i + \nabla F(x) , X \rangle \\
    &+ \underbrace{\sum_i y_i b_i + F(X) - \langle \nabla F(X), X \rangle}_{\text{dual value}}\\
    &= \langle Z, X \rangle + \rho_{\text{dual}}\\
    &\geq \text{max}(0,\lambda_{\text{min}}(Z)) \text{tr}(X) + \rho_{\text{dual}}
    \end{align}
    and we have $\rho_\sdp \geq \rho^\star_\sdp$.
\end{proof}


$\rho_\sdp$ and $\text{max}(0, \lambda_{\text{min}}(Z)) \text{tr}(X) + \rho_{\text{dual}}$ can be directly evaluated within the algorithm. A small gap between them indicates that $\rho_\sdp$ is close to $\rho^\star_\sdp$, confirming that the problem has been solved to global optimality. We define this new suboptimality gap as:  

\bea \label{eq:suboptimality_new}
\eta = \frac{\hat{\rho} - \text{max}(0,\lambda_{\text{min}}(Z)) \text{tr}(X) - \rho_{\text{dual}}}{1 + |\hat{\rho}| + |\text{max}(0,\lambda_{\text{min}}(Z)) \text{tr}(X) + \rho_{\text{dual}}|}.
\eea