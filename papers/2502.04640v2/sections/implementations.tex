%!TEX root = ../main.tex

\section{Burer-Monteiro Factorization and \\ CUDA-based Riemannian Optimizer}
\label{sec:implementation}

Let us first write the SDP~\eqref{eq:problem-sdp} in standard primal form:
\begin{subequations}\label{eq:problem-sdp-standard}
    \begin{align} 
        \min_{X \in \sym{3N}} & \ \ \trace{QX} \\
        \subject & \ \ \langle A_i,X \rangle = b_i, i = 1,\dots,m \label{eq:sdp:standard:linear}\\
        & \ \ X \succeq 0  \label{eq:sdp:standard:psd}
        \end{align}
\end{subequations}
where we have rewritten the constraint~\eqref{eq:sdp-X-compact} as a positive semidefinite (PSD) constraint~\eqref{eq:sdp:standard:psd} and $m=5N+1$ linear equality constraints~\eqref{eq:sdp:standard:linear}. To see why this reformulation is true, note that the diagonal $3\times 3$ blocks of $X$ are either $\eye_3$ or scaled $\eye_3$---the former can be enforced using 6 linear equalities and the latter can be enforced using 5, summing up to $5N+1$ linear equalities. Associated with the primal standard SDP~\eqref{eq:problem-sdp-standard} is the following dual standard SDP:
\begin{subequations}\label{eq:problem-sdp-dual}
    \begin{align} 
    \max_{y \in \Real{m}} & \ \ \sum_{i=1}^{m} b_i y_i \\
    \subject & \ \ Z(y) := Q - \sum_{i=1}^{m} y_i A_i \succeq 0. \label{eq:Zofy}
    \end{align} 
\end{subequations}

Since Slater's condition holds ($X = \eye_{3N} \succ 0$ is feasible for the primal~\eqref{eq:problem-sdp-standard}), we know strong duality holds between primal~\eqref{eq:problem-sdp-standard} and dual~\eqref{eq:problem-sdp-dual}, i.e., their optimal values are equal to each other~\cite{yang24book-sdp}.  

\textbf{Scalability}. Once the SDP~\eqref{eq:problem-sdp} is formulated in standard forms, it can be solved by off-the-shelf SDP solvers such as MOSEK~\cite{aps2019mosek}, provided that the SDP's scale is not so large. Our SDP relaxation leads to a matrix variable with size $3 N \times 3N$. This means that when $N$ is in the order of hundreds, MOSEK can solve the SDP without any problem. However, when $N$ is in the order of thousands or tens of thousands, which is not uncommon in bundle adjustment problems, MOSEK will become very slow or even runs out of memory (e.g., MOSEK becomes unresponsive when $N>2000$). Therefore, we decided to customize a solver for our SDP relaxation.


% In terms of speed and scalability, the SDP relaxation increases the number of variables to $9N^2$, which is quadratic in the number of frames and, therefore, not scalable. Commercial solvers like MOSEK run out of memory and become unresponsive when $N$ exceeds 2000. The low-rank structure of $X$ motivates us to explore low-rank factorization methods, such as the Burer-Monteiro approach \cite{burer2003nonlinear}. All the proofs in this section can be found in \cite{burer2003nonlinear} and \cite{yang24book-sdp}.

% Before applying the Burer-Monteiro factorization, we propose the standard form of \prettyref{eq:problem-sdp} and its dual problem:

% \begin{proposition}[Dual Problem of \prettyref{eq:problem-sdp}]\label{prop:BM_dual}
%     Assume we write 
%     \bea
%     X = \bmat{ccc} \eye_3 & \cdots & * \\
%     \vdots & \ddots & \vdots \\
%     * & \cdots & \alpha_N \eye_3
%     \emat 
%     \eea
%     as $\langle A_i,X \rangle = b_i, \forall i \in 1,\dots,5N+1$. Now the primal problem \prettyref{eq:problem-sdp} can be written as
    

%     The dual problem of \prettyref{eq:problem-sdp} is
    
% Since Slater's Condition is satisfied ($X = \eye_{3N}$), strong duality holds. Consequently, the optimal value of \prettyref{eq:problem-sdp-dual} is equal to the optimal value of \prettyref{eq:problem-sdp}.

% \end{proposition}


\subsection{Burer-Monteiro Factorization}

The key structure we will leverage is, as stated in Proposition~\ref{prop:sdprelaxation}, the optimal solution $X^\star$ has its rank equal to three when the SDP relaxation is tight. In other words, the effective dimension of $X^\star$ is $3 \times 3N$ instead of $3N \times 3N$.

To exploit the low-rank structure, we will leverage the famous Burer-Monteiro (BM) factorization~\cite{burer2003nonlinear}.


\begin{proposition}[BM Factorization]\label{prop:BM}
    For a fixed rank $r \geq 3$, the Burer-Monteiro factorization of \prettyref{eq:problem-sdp} and~\eqref{eq:problem-sdp-standard} reads:
    \begin{subequations}\label{eq:problem-bm}
        \begin{align} 
            \rho^\star_{\xmbm,r} = \min_{ \bar{R}_i \in \Real{r \times 3},i=1,\dots,N } & \ \ \trace{ Q U\tran U } \\
            \subject & \ \ U = \bmat{ccc}\bar{R}_1& \dots &\bar{R}_N \emat  \\
            & \ \ \bar{R}_i \tran \bar{R}_i = \begin{cases}
                \eye_3 & i=1 \\
                \alpha_i\eye_3 & i\geq 2
            \end{cases}.
            \end{align}
    \end{subequations}
\end{proposition}

A few comments are in order. First, by factorizing $X = U\tran U$, $X \succeq 0$ holds by construction. Second, note that when $r = 3$, problem~\eqref{eq:problem-bm} is exactly the same as the original QCQP~\eqref{eq:problem-qcqp} (up to the difference in $\bar{R}_1$), and thus is NOT convex. Third, as long as $r \geq r^\star$ where $r^\star$ is the minimum rank of all optimal solutions of the SDP~\eqref{eq:problem-sdp}, the nonconvex BM factorization has the same global minimum as the convex SDP~\eqref{eq:problem-sdp}~\cite{yang24book-sdp,burer2003nonlinear}. Note that the factor $U$ is a matrix of size $r \times 3N$, i.e., a flat matrix as shown in Fig.~\ref{fig:pipeline} bottom right.

\textbf{Counterintuitive}? The reader might find this confusing. In \S\ref{sec:method}, we applied a series of techniques to relax a nonconvex optimization problem into a convex SDP, enabling us to solve it to global optimality. Surprisingly, the BM factorization appears to ``undo'' this effort, pulling us back into nonconvex optimization. While the low-rank factorization offers a clear scalability advantage, it remains uncertain whether this benefit outweights the drawbacks of reintroducing nonconvexity.

\textbf{Rank ``staircase''}. The secret ingredient of BM factorization to tackle nonconvexity is that we will solve the factorized problem~\eqref{eq:problem-bm} at \emph{increasing ranks}, like stepping up a ``staircase'' (\cf Fig.~\ref{fig:pipeline} bottom right). We will start with the lowest rank $r=3$ and solve problem~\eqref{eq:problem-bm} using local optimization. One of two cases will happen. (a) The local optimizer is the same as the global optimizer of the SDP, in which case we can leverage the dual SDP~\eqref{eq:problem-sdp-dual} to certify global optimaity and declare victory against the SDP. (b) The local optimizer is not the same as the global optimizer of the SDP, in which case we can again leverage the dual SDP~\eqref{eq:problem-sdp-dual} to ``escape'' the bad local minimum via increasing the rank, i.e., going up the staircase. 

We formalize this in Algorithm~\ref{alg:bm} and prove its correctness.

\setlength{\intextsep}{0pt}  % Adjusts space above and below algorithms
\begin{algorithm}[ht]
    \caption{Riemannian Staircase}
    \label{alg:bm}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} the data matrix $Q$
        \STATE \textbf{Output:} optimal solution $U^\star$
        \STATE \texttt{\# Initialization}
        \STATE Set $r = 3$, $U_r^0 = [\eye_3, \dots, \eye_3]$ 
        \WHILE{True}
        % \texttt{\# Solve BM factorization}
        \STATE \texttt{\# Local optimization of~\eqref{eq:problem-bm}}
        \STATE $U_r^\star= \textsc{LocalOptimizer}(U_r^0)$ \label{line:local_optimize}
        \STATE \texttt{\# Compute dual certificate}
        \STATE $y_r, Z(y_r) \leftarrow \textsc{Solve}~\eqref{eq:bm_first_order_optimality}$ \label{line:bm_first_order}

        \STATE \texttt{\# Certify global optimaity}

        \IF{$Z(y_r) \succeq 0$}
        \STATE \textbf{return} $U_r$ \label{line:bm_return}
        \ENDIF
        \STATE \texttt{\# Escape local minimum}
        \STATE $v \leftarrow \textsc{LeastEigenvector}(Z(y_r))$ \label{line:bm_eigen_vector}
        \STATE $U_{r+1} = \bmat{cc} U_r\tran & \alpha v \emat \tran$ with $\alpha=1$
        \STATE \texttt{\# Line search}
        \WHILE{$\trace{QU_{r+1}U_{r+1}\tran} \geq \trace{QU_rU_r\tran}$}
        \STATE $\alpha = \alpha/2$ \label{line:linesearch-1}
        \STATE $U_{r+1} = \bmat{cc} U_r\tran & \alpha v \emat \tran$ \label{line:linesearch-2}
        \ENDWHILE
        \STATE $r \leftarrow r+1$, $U_{r+1}^0 \leftarrow U_{r+1}$ \label{line:bm_increase_rank}
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}

\textbf{Certify global optimaity}. At every iteration of Algorithm~\ref{alg:bm}, it first computes a local optimizer of the BM factorization problem~\eqref{eq:problem-bm} in line~\ref{line:local_optimize} using an algorithm to be described in \S\ref{sec:manifold}. Denote the local optimizer as $U^\star_r$. To check whether $(U^\star_r)\tran U^\star_r$ is the optimal solution to the convex SDP~\eqref{eq:problem-sdp}, we need to compute the dual optimality certificate.

\begin{theorem}[Dual Optimality Certificate]\label{thm:BM}
    Let $U_r^{\star}$ be a locally optimal solution of \prettyref{eq:problem-bm} and assume the linear independence constraint qualification (LICQ) holds at $U^{\star}_r$, i.e.,
\begin{align}
\nabla_U (\langle A_i, (U^{\star}_r)\tran U^{\star}_r \rangle - b_i) = 2A_i U^{\star}_r, \quad i = 1, \ldots, m
\end{align}
are linearly independent. Then, there must exist a unique dual variable $y^{\star}$ such that
\begin{equation}\label{eq:bm_first_order_optimality}
Z(y^{\star})U^{\star}_r = \zero.
\end{equation}
If
\bea
Z(y^{\star}) &\succeq& 0,
\eea
then $U^{\star}_r$ is a global optimizer of \prettyref{eq:problem-bm}, and $(U^{\star})\tran U^{\star}$,$y^{\star}, Z(y^{\star})$ are optimal for the SDP \prettyref{eq:problem-sdp-standard} and its dual \prettyref{eq:problem-sdp-dual}.
\end{theorem}
\begin{proof}
    See \cite{burer2003nonlinear} or \cite{yang24book-sdp}.
\end{proof}

Theorem~\ref{thm:BM} provides a simple recipe to certify global optimality by solving the linear system of equations in~\eqref{eq:bm_first_order_optimality} (recall $Z(y^\star)$ is linear in $y^\star$ from~\eqref{eq:Zofy}), forming the dual matrix $Z(y^\star)$, and checking its positive semidefinite-ness. 


\textbf{Escape local minimum}. If $Z(y^\star)$ is not PSD, then $(U_r^\star)\tran U_r^\star$ is not optimal for the SDP. In this case, the following theorem states that the eigenvector of $Z(y^\star)$ corresponding to the minimum eigenvalue provides a descent direction.

% Basically, this is equivalent to performing a rank-$r$ factorization on $X$. When $r = 3$, this is exactly the same as \prettyref{eq:problem-qcqp}. However, unlike \prettyref{eq:problem-qcqp}, we introduce the Riemann Staircase \prettyref{alg:bm}, which gradually increases the rank starting from $r = 3$. As the rank becomes sufficiently large, we can recover the global minimum of \prettyref{eq:problem-sdp}. 

% The input to \prettyref{alg:bm} is the $Q$ matrix defined in \prettyref{eq:Q}. The output is the optimal solution $U^\star \in \Real{r^\star \times 3N}$, where $r^\star$ is the optimal rank. Lines~\ref{line:bm_first_order}-\ref{line:bm_return} certify the global optimality condition provided in \prettyref{thm:BM} and \prettyref{cor:BM_global_optimality}. If the condition is satisfied, the algorithm returns the optimal solution. Otherwise, lines~\ref{line:bm_eigen_vector}-\ref{line:bm_increase_rank} provide a valid descent direction and an initial state for the next rank, as supported by \prettyref{thm:BM_decrease}.




% By \prettyref{cor:BM_global_optimality}, for a fixed rank $r$, we can certify whether it is the global solution of original \prettyref{eq:problem-sdp}. If it is we are finished, if not we need to increase the rank and solve the 
% \prettyref{eq:problem-bm} problem with rank $r+1$. Here is a theorem that finds a decrease direction for the rank $r+1$ problem:

\begin{theorem}[Descent Direction]\label{thm:BM_decrease}
    % Let $U^{\star}$ be a locall minimum (\red{Hank's note said second order critical point}) of the \prettyref{eq:problem-bm} with rank $r$ and $y^{\star}$ be the corresponding dual variable. Suppose $v$ is a eigenvector of $Z(y^\star)$ corresponding to a negative eigenvalue.

    Let $U^{\star}_r$ be a local minimizer of problem \prettyref{eq:problem-bm} and $y^{\star}$ be the corresponding dual variable. Suppose $Z(y^\star)$ is not PSD and $v$ is an eigenvector of $Z(y^\star)$ corresponding to a negative eigenvalue. Then, consider the BM factorization \prettyref{eq:problem-bm} at rank $r+1$. The direction
    \bea
    D= \bmat{c} 0 \\ v^T \emat
    \eea
    is a descent direction at the point
    \bea
       \hat{U} = \bmat{c} U^{\star}_r \\ 0 \emat.
    \eea
\end{theorem}
\begin{proof}
    See \cite{burer2003nonlinear} or \cite{yang24book-sdp}.
\end{proof}
Theorem~\ref{thm:BM_decrease} states that if Algorithm~\ref{alg:bm} gets stuck at rank $r$, it can escape the local minimum by increasing the rank.
Since $D$ is a descent direction, we perform line search in lines~\ref{line:linesearch-1}-\ref{line:linesearch-2} until a point with lower objective value is found.

\textbf{Global convergence}. Algorithm~\ref{alg:bm} is guaranteed to converge to the optimal solution of the SDP~\cite{yang24book-sdp} because in the worst case $r$ will be increased all the way to $3N$---the size of the SDP matrix variable. Of course, in such cases the BM factorization does not have any scalability advantage over the origin SDP. Fortunately, in almost all numerical experiments, Algorithm~\ref{alg:bm} terminates when $r=3$ or $4$, bringing significant scalability advantage to solving large-scale SDP relaxations.

% \begin{algorithm}[ht]
%     \caption{Burer-Monteiro Method}
%     \label{alg:bm}
%     \begin{algorithmic}
%         \STATE \textbf{Input:} $Q$, $U_3$
%         \STATE \textbf{Output:} $U^\star$
%         \WHILE{true}
%         \STATE Solve \prettyref{eq:problem-bm} with rank $r$ to get local minimum $U_r$
%         \STATE Solve \prettyref{eq:bm_first_order_optimality} to get $y_r$ and $Z(y_r)$
%         \IF{$Z(y_r) \succeq 0$}
%         \STATE \textbf{return} $U_r$
%         \ENDIF
%         \STATE Compute the eigenvector $v$ of $Z(y_r)$ corresponding to the most negative eigenvalue
%         \STATE $\alpha = 1$
%         \STATE $U_{r+1} = \bmat{cc} U_r & \alpha v^T \emat$
%         \WHILE{$\trace{QU_{r+1}U_{r+1}^T} \geq \trace{QU_rU_r^T}$}
%         \STATE $\alpha = \alpha/2$
%         \STATE $U_{r+1} = \bmat{cc} U_r & \alpha v^T \emat$
%         \ENDWHILE
%         \STATE $r = r+1$
%         \ENDWHILE
%     \end{algorithmic}
% \end{algorithm}

\subsection{C++/CUDA-based Riemannian Optimization}
\label{sec:manifold}

Everything in Algorithm~\ref{alg:bm} is clear except line~\ref{line:local_optimize} where one needs to locally optimize the nonconvex BM factorization~\eqref{eq:problem-bm}. 


\textbf{Riemannian structure}. At first glance, problem~\eqref{eq:problem-bm} looks like a nonconvex constrained optimization problem. However, the constraints of~\eqref{eq:problem-bm} indeed define a smooth manifold. 

\begin{proposition}[BM Riemannian Optimization]
    The BM factorization problem~\eqref{eq:problem-bm} is equivalent to the following \emph{unconstrained} Riemannian optimization problem
    \begin{subequations}\label{eq:BM-Riemannian}
        \bea
        \min  & \trace{ Q U\tran U } \\
\subject & U = \bmat{cccc}R_1 & s_2 R_2 & \cdots & s_N R_N\emat\\
 &s_i \in \mathcal{M}_p, i = 2, \ldots, N,\\
& R_i \in \mathcal{M}_s^{(r)}, i = 1, \ldots, N
        \eea  
    \end{subequations}
    where $\calM_p$ is the positive manifold defined as 
    \bea 
    \calM_p : = \{ s \mid s > 0\},
    \eea
    and $\calM_s^{(r)}$ is the Stiefel manifold of order $r$:
    \bea  
    \mathcal{M}_s^{(r)} = \{R\in\Real{r\times3} \mid R\tran R = \eye_3,\} .
    \eea
\end{proposition}
\begin{proof}
    By inspection.
\end{proof}

We solve problem~\eqref{eq:BM-Riemannian} using the Riemannian trust-region algorithm with truncated conjugate gradient (Rtr-tCG). Details of this algorithm can be found in \cite{absil2008optimization, boumal2023introduction}. 

% The conjugate gradient (CG) method is chosen for its efficiency on GPUs, as it require only matrix-vector products and allows subproblems to be truncated in just a few iterations.

% \subsection{Scale Regularization}

% Because of inaccurate real-world data, the scale of frames $2,\dots,N$ is often much smaller than 1\footnote{which is anchored to be 1}. This means the global minimum tend to degenerate all the camera $2,\dots,N$ to a single point, otherwise it may produce large errors in camera $2,\dots,N$. To address this issue, we introduce a scale regularization term in the objective function of \prettyref{eq:problem-sdp}:
% \begin{align}
%     \min_{X \in \sym{3N}} & \trace{QX} + \lambda \sum_{i=2}^{N} (X_{3i,3i}-1)^2 \nonumber\\
%     \subject & \langle A_i,X \rangle = b_i, \forall i \in 1,\dots,5N+1\nonumber\\
%     & X \succeq 0
% \end{align}
% This will not break the theorem we have, Slater’s Condition and strong duality still hold, and the optimality condition is still the same, while the $Z(y)$ matrix change to 
% \bea
%     Z(y) = Q - \sum_{i=1}^{5N+1} y_i A_i + 2\lambda \sum_{i=2}^{N} (X_{3i,3i}-1) \eye_3
% \eea
% For more detils please refer to \cite{rosen2021scalable} and Appendix.

\textbf{C++/CUDA implementation}. The Rtr-tCG algorithm is readily available through the \manopt optimization package~\cite{boumal2014manopt}. However, to boost efficiency and enable fast solution of the SDP relaxation, we implement the Rtr-tCG algorithm directly in C++/CUDA. 

% There are two major reasons for pursuing a GPU-based implementation.

\begin{itemize}
    \item \textbf{Conjugate gradient method}.
    The conjugate gradient method involves only Hessian-vector products and vector addition. The Hessian-vector product can be decomposed into two components: (a) the Euclidean Hessian-vector product and (b) the Riemannian projection onto the tangent space. The first component primarily requires matrix-vector multiplication, which can be efficiently implemented using \texttt{cuBLAS}. The second component involves batched small matrix-matrix multiplications, matrix-matrix inner products, scalar-matrix multiplications, all of which are implemented using custom CUDA kernels. For vector addition, we directly utilize the \texttt{cublasDaxpy} function from \texttt{cuBLAS}.

    \item \textbf{Retraction}. The retraction operation aims to map a point from the tangent space back to the manifold. In our case, the retraction operation happens both on the positive manifold and the Stiefel manifold. The retraction on the positive manifold is a simple custom kernel, while the retraction on the Stiefel manifold involves QR decomposition. We directly apply Gram-Schmidt process on every batch of $3 \times r$ matrices, which is implemented using custom CUDA kernels.
\end{itemize}

In \S\ref{sec:exp}, we show our GPU-based implementation achieves up to $100$ times speedup compared to the CPU-based \manopt.

\begin{tcolorbox}
    \begin{center}
        \vspace{-1mm}
        \textbf{Summary}
        \vspace{-1mm}
    \end{center} 
    We focused on developing a customized solver capable of solving large-scale SDP relaxations in~\eqref{eq:problem-sdp} (and~\eqref{eq:problem-sdp-standard}). We applied the Burer-Monteiro factorization method to exploit low-rankness of the optimal SDP solutions (\cf problem~\eqref{eq:problem-bm}), and leveraged the Staircase Algorithm~\ref{alg:bm} to solve the nonconvex BM factorization to global optimality. We pointed out the BM factorization problem is indeed an unconstrained Riemannian optimization problem (\cf problem~\eqref{eq:BM-Riemannian}) and developed a C++/CUDA-based implementation that is significantly faster than \manopt.
\end{tcolorbox}

\begin{remark}[Connection to Prior Work]
    This is not the first time BM factorization has been applied in robotics. \sesync and related work \cite{dellaert2020shonan,rosen2021scalable} pioneered the application of BM factorization for solving the pose graph optimization problem. Several works~\cite{garcia2021certifiable,garcia2024certifiable,holmes2023efficient} utilized the dual optimality certificate result in Theorem~\ref{thm:BM} to develop fast certifiers. Our novelty lies in developing the first C++/CUDA-based implementation of the Riemannian trust-region algorithm to push the limitations of BM factorization.
\end{remark}

