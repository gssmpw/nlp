\section{STRING: Separable Translationally Invariant Position Encodings} \label{sec:string_core_section}

Recall that our goal is modify queries/keys depending on their respective positions, so that changes to dot products $\mathbf{q}_i^\top \mathbf{k}_j$ depend on $\mathbf{r}_i - \mathbf{r}_j$.
RoPE achieves this using matrix multiplication \citep{su2024roformer}. 
Here, we present STRING: a more general, better-performing algorithm. %

\subsection{Introducing STRING}
STRING is defined as follows.
\begin{tcolorbox}[colback=gray!10!white,colframe=gray!50!black,arc=0mm,boxrule=0.5pt]
\begin{definition} \label{def:coupled_norms_def}
STRING is the mapping $\mathbf{R}(\cdot): \mathbb{R}^{d_c} \to \mathbb{R}^{d \times d}$, from $d_c$-dimensional position vectors to $d \times d$ matrices, given by \vspace{-2mm}
\begin{equation} 
\label{eq:string_multiply_general}
    \mathbf{R}(\mathbf{r}_i) =  \exp \left (\sum_{k=1}^{d_c} \mathbf{L}_k [\mathbf{r}_i]_k \right ), \vspace{-2mm}
\end{equation}
where $\{\mathbf{L}_k\}_{k=1}^{d_c} \subset \mathbb{R}^{d \times d}$ is a set of learnable and commuting skew-symmetric generators.
Given a set of queries or keys $\{\mathbf{z}_i\}_{i=1}^N \subset \mathbb{R}^d$ at positions $\{\mathbf{r}_i\}_{i=1}^N \subset \mathbb{R}^{d_c}$, their positions are encoded as: \vspace{-2mm}
\begin{equation}
    \mathbf{z}_i \to \mathbf{R}(\mathbf{r}_i) \mathbf{z}_i \quad \forall i \in \{1,...,N\}.
\end{equation}
\end{definition}
\end{tcolorbox}
Here, $\exp(\cdot)$ refers to the \emph{matrix} exponential, defined by its series expansion $\exp(\mathbf{A})\coloneqq \sum_{i=0}^\infty \mathbf{A}^i / i!$ and
$[\mathbf{r}_i]_k$ is the $k$th coordinate of vector $\mathbf{r}_i$.
By `commuting skew-symmetric generators', we mean that $\{\mathbf{L}_{k}\}_{k=1}^{d_{c}}$ satisfy
\begin{equation}
    \left[\mathbf{L}_i, \mathbf{L}_j\right]=0  \quad \textrm{and} \quad \mathbf{L}_i^\top = -\mathbf{L}_i \hspace{0.5em} \forall \hspace{0.5em} i,j.
\end{equation}
There are many ways to parameterize such a set; we give examples in \cref{sec:efficient_string}.
Remarkably, the following is true. 
\begin{theorem}[STRING is general] \label{thm:x-string-deriv}
Consider the set of mappings $\mathbf{R}(\cdot):\mathbb{R}^{d_c} \to \mathbb{R}^{d \times d}$ that satisfy the group-like translational invariance property $\mathbf{R}(\mathbf{r}_i)^\top \mathbf{R}(\mathbf{r}_j) = \mathbf{R}(\mathbf{r}_j - \mathbf{r}_i) \hspace{0.2em} \forall \hspace{0.2em} \mathbf{r}_i, \mathbf{r}_j \in \mathbb{R}^{d_c}$, are continuously differentiable with respect to $\mathbf{r}_i$, and satisfy $\mathbf{R}(\mathbf{0})=\mathbf{I}_d$ (with $\mathbf{I}_d$ the $d$-dimensional identity).
All such mappings can be expressed as STRING with some set of generators $\{\mathbf{L}_k\}_{k=1}^{d_c} \subset \mathbb{R}^{d \times d}$.
\end{theorem}
In this sense, STRING is the \emph{most general} of all translationally invariant position encoding mechanisms using matrix multiplication. 
Meanwhile, RoPE is a simple special case of STRING, taking a particular choice of generators.
This can be seen as follows. %
\begin{theorem}[RoPE is a type of STRING \#1] \label{thm:rope_special_case_1}
Consider the generators 
\smash{$\mathbf{L}_k =  \sum_{p=1}^{d/2} (\delta_{2p,2p-1} - \delta_{2p-1,2p}) \theta_p$}, where \smash{$\{\theta_p\}_{p=1}^{d/2} \subset \mathbb{R}$} and $\delta_{i,j}$ is the delta function.
This corresponds to RoPE with rotational frequencies \smash{$\{\theta_p\}_{p=1}^{d/2}$}.
\end{theorem}
Proofs of \cref{thm:x-string-deriv} and \cref{thm:rope_special_case_1} are in \cref{app:proofs}.

\subsection{Computationally efficient STRING} \label{sec:efficient_string}
Despite being general and notationally compact, the parameterization of the STRING matrices $\mathbf{R}(\mathbf{r}_i)$ shown in Eq. \ref{eq:string_multiply_general} may not be convenient for practical applications.
Given $N$ tokens at positions $\{\mathbf{r}_i\}_{i=1}^N$, one must in general exponentiate and store $N$ dense $d\times d$ matrices.
This incurs $\mathcal{O}(Nd^3)$ time complexity and $\mathcal{O}(Nd^2)$ space complexity cost.
The problem is exacerbated if the $\{\mathbf{r}_i\}_{i=1}^N$ differ across training examples and batches, which occurs e.g~for point cloud data or color plus depth channel (RGB-D) images. 
In this case, $\{\mathbf{R}(\mathbf{r}_i)\}_{i=1}^N$ cannot be cached and reused.
This motivates the goal of this section: to find \emph{efficient} STRING instantiations, nonetheless more general and expressive than RoPE.
We begin with the following (proof in \cref{app:proofs}):
\begin{theorem}[RoPE is a type of STRING \#2]\label{thm:basis_change}
For any STRING position encoding with generators $\{\mathbf{L}_k\}_{k=1}^{d_c}$, there exists an orthogonal matrix $\mathbf{P}$ such that \vspace{-1mm}
\begin{equation} \label{eq:basis_change}
    \mathbf{R}(\boldsymbol{r}_i) = \mathbf{P} \emph{\textrm{RoPE}}(\boldsymbol{r}_i) \mathbf{P}^\top.
\end{equation}
\end{theorem} \vspace{-2mm}
Note that the orthogonal matrix $\mathbf{P}$ is independent of the coordinates $\mathbf{r}_i$, so it can be learned and stored once per attention head and shared across all training examples. 
Meanwhile, ${\textrm{RoPE}}(\boldsymbol{r}_i)$ is sparse -- it is only nonzero on the super- and subdiagonals -- so multiplying tokens only requires $\mathcal{O}(Nd)$ memory and $\mathcal{O}(Nd^2)$ time, saving a factor of $d$. 
This is crucial in the contrastive learning setting where batch sizes can become large. 
Once again, one can see that RoPE is a special case of STRING, this time taking $\mathbf{P}=\mathbf{I}_d$.
We emphasize that the parameterization of STRING in Eq. \ref{eq:basis_change} remains just as general as in Eq. \ref{eq:string_multiply_general}.
We also note that, since in regular attention one takes dot products between position-encoded queries and keys, the first orthogonal matrix $\mathbf{P}$ will always cancel with its counterpart. 
Therefore, in Transformers it is sufficient to take $\mathbf{R}(\boldsymbol{r}_i) = \textrm{RoPE}(\boldsymbol{r}_i) \mathbf{P}$, with $\mathbf{P} \in \textrm{O}(d)$ learnable, without loss of generality.\footnote{We dropped the transpose sign on the second $\mathbf{P}$, redefining $\mathbf{P}^\top$ as our learnable orthogonal matrix.}    

\pg{\underline{Example 1: Cayley-STRING}}
Equipped with \cref{thm:basis_change}, our goal becomes to choose a suitable parameterization for the orthogonal matrix $\mathbf{P}$.
One option is to take the \emph{Cayley Transform}, \vspace{-2mm}
\begin{equation} \label{eq:cayley_string}
    \mathbf{P}_\textrm{Cayley} \coloneqq (\mathbf{I}_d - \mathbf{S})(\mathbf{I}_d + \mathbf{S})^{-1},\vspace{-0mm}
\end{equation} 
where $\mathbf{S}$ is a learnable (potentially sparse) antisymmetric matrix \citep{DieleLP98}.
$\mathbf{P}_\textrm{Cayley}$ is convenient since, for a token $\mathbf{z}_i$, we can compute $(\mathbf{I}_d + \mathbf{S})^{-1}\mathbf{z}_i$ efficiently using a linear solver, avoiding matrix inverse computation.
Where we use $\mathbf{P}_\textrm{Cayley}$, we refer to our algorithm as \emph{Cayley-STRING}.

\pg{The unreasonable effectiveness of STRING}
In some sense, \cref{thm:basis_change} makes it surprising that STRING outperforms RoPE so comprehensively in all our experiments (see \cref{sec:experiments}), given that they are related by a change of basis.
It appears that the ability to \emph{explicitly} learn this basis change via $\mathbf{P}$ (\underline{shared} between queries and keys), rather than implicitly via existing network weights, substantially boosts performance. 
Conversely, when using linear attention variants, the projected tokens $\mathbf{W}_q\mathbf{q}_i$ and $\mathbf{W}_k\mathbf{k}_i$ are pushed through nonlinearities such as $\textrm{ReLU}(\cdot)$ before taking the dot product.
Hence, in this case, including learnable $\mathbf{P}$ does increase the capacity of the network, rather than simply learning a basis change.

\pg{\underline{Example 2: Circulant-STRING}}
We now present a second efficient STRING algorithm within our framework.
A square matrix is referred to as \emph{circulant} if it takes the form 
\begin{equation} \label{eq:circulant_string}
    \mathbf{C} =
\begin{bmatrix}
c_0 & c_{d-1} & \cdots & c_2 & c_1 \\
c_1 & c_0 & c_{d-1} & \cdots & c_2 \\
\vdots & c_1 & c_0 & \ddots & \vdots \\
c_{d-2} & \vdots & \ddots & \ddots & c_{n-1} \\
c_{d-1} & c_{d-2} & \cdots & c_1 & c_0
\end{bmatrix}.
\end{equation}
All rows are composed of the same elements, and each row is rotated one element relative to the preceding row.
The transpose of a circulant matrix $\mathbf{C}^\top$ is also circulant, and the sum of two circulant matrices is also circulant.
It follows that $\mathbf{C}-\mathbf{C}^\top$ is circulant and antisymmetric.
Lastly, circulant matrices commute.
With these properties in mind, we can simply define $\mathbf{L}_k=\mathbf{C}_k - \mathbf{C}_k^\top$ for $k\in\{1,...,d_c\}$, with $\mathbf{C}_k$ a learnable circulant matrix parameterized by $d$ scalars $\{c_0,...,c_{d-1}\}$.
We call this \emph{Circulant-STRING}. 
This special parameterization is convenient for the following reason. 
\newpage

\begin{theorem}[Circulant-STRING is fast] \label{thm:circ-string-fast}
Given generators $\mathbf{L}_k=\mathbf{C}_k - \mathbf{C}_k^\top$ with $\mathbf{C}_k$ circulant, the position encoding $\exp(\sum \mathbf{L}_k [\mathbf{r}_i]_k)\mathbf{z}_i$ for token $\mathbf{z}_i$ at position $\mathbf{r}_i$ can be computed in $\mathcal{O}(d \log d)$ time and $\mathcal{O}(d)$ memory using the fast Fourier Transform (FFT).
\end{theorem}
We provide a proof in \cref{app:proofs}. 
Circulant-STRING provides another efficient position encoding algorithm that scales gracefully to large, high-dimensional datasets and performs well in spatial applications (see \cref{sec:experiments}). 






















\vspace{-3mm}
\paragraph{Learnable frequencies with STRING.} Note that the STRING generators from \cref{def:coupled_norms_def} are (in general) learnable. %
For Cayley-STRING, the angle-defining frequencies for $\textrm{RoPE}$ and $\mathbf{S}$, the antisymmetric matrix from \cref{eq:cayley_string} are learned whereas in Circulant-STRING, the scalars $\{c_0,...,c_{d-1} \}$ in \cref{eq:circulant_string} are learned. %
\vspace{-3mm}
\subsection{Loose ends}
Here, we discuss further generalisations of STRING. 




\pg{\underline{Extension 1: $\otimes$-STRING}}
So far, we have followed RoPE in assuming that our position encodings are applied via matrix multiplication. 
However, this can be relaxed whilst preserving separability and translational invariance. 
For example, one can transform tokens $\mathbf{z}_i$ via the \emph{outer} product with position feature vectors $\mathbf{f}(\mathbf{r}_i) \in \mathbb{R}^{2m}$,
\begin{equation}
    \mathbf{z}_i \to \textrm{vec}(\mathbf{f}(\mathbf{r}_i) \otimes \mathbf{z}_i).
\end{equation}
Here, $\otimes$ denotes the outer product and $\textrm{vec}$ denotes the `vectorizing' operation that flattens a matrix to a vector, so that
$\textrm{vec}(\mathbf{f}(\boldsymbol{r}_i) \otimes \mathbf{q}_i)_{da+b} =\mathbf{f}(\boldsymbol{r}_i)_a {\mathbf{q}_i}_b$ where $a \in \{1,...,2m\}$ and $b \in \{1,...,d\}$.
Since the dot product of (flattened) outer products gives the product of dot products, we have
\begin{equation}
    \textrm{vec}(\mathbf{f}(\mathbf{r}_i) \otimes \mathbf{q}_i)^\top \textrm{vec}(\mathbf{f}(\mathbf{r}_j) \otimes \mathbf{k}_j) = \mathbf{q}_i^\top\mathbf{k}_j \cdot \mathbf{f}(\mathbf{r}_i)^\top \mathbf{f}(\mathbf{r}_j).
\end{equation}
Now suppose that we take the Fourier features 
\begin{equation}
    \mathbf{f}(\mathbf{r}_i) = \frac{1}{\sqrt{m}}\left[\cos(\boldsymbol{\omega}_k^\top \mathbf{r}_i),\sin(\boldsymbol{\omega}_k^\top \mathbf{r}_i) \right]_{k=1}^m, 
\end{equation}
where $\{ \boldsymbol{\omega}_k \}_{k=1}^m \subset \mathbb{R}^d$ are learnable $d$-dimensional frequency vectors.
Then we have that $\mathbf{f}(\mathbf{r}_i)^\top \mathbf{f}(\mathbf{r}_r) = \frac{1}{m} \sum_{k=1}^m \cos (\boldsymbol{\omega}_k(\mathbf{r}_i - \mathbf{r}_j))$ which is clearly a function of $\mathbf{r}_i - \mathbf{r}_j$.
We refer to this novel position encoding variant, orthogonal to previous RoPE-like approaches, as \emph{$\otimes$-STRING}. 

\pg{\underline{Extension 2: General transformation groups}}
Having focused on \emph{translational} invariance, another natural question is whether STRING could be repurposed for other continuous transformation groups.
These may be more suitable for data with different properties; for example, one might sometimes prefer a \emph{rotationally} invariant position encoding. %

More formally, recall that a Lie group with parameters $\psi \in \mathbb{R}^k$ is a group of transformations of the form $T_\psi: \mathbb{R}^d \to \mathbb{R}^d$ that are differentiable with respect to $\psi$.
Let the parameter $\psi=0$ correspond to the identity element, so that $T_0 \mathbf{x}=\mathbf{x}$.
A \emph{canonical coordinate system} for $G$ is an injective map $\rho$ from Cartesian coordinates to a new coordinate system, satisfying
$\rho(T_\psi \mathbf{x}) = \rho(\mathbf{x}) + \sum_{i=1}^k \psi_i \mathbf{e}_i \hspace{0.2em} \forall \hspace{0.2em} T_\psi \in G$,
where $\mathbf{e}_i$ is the $i$th standard basis vector.
Observe that the right hand side of this equation represents a translation in the new basis. 
Canonical coordinate systems exist for all one-parameter Lie groups ($k=1$), and more generally for Abelian groups of dimension $k \leq d$ \citep{segman1992canonical, rubinstein1991recognition, tai2019equivariant}. 
They can be derived analytically by solving a set of first-order PDEs, though for many common transformation groups the canonical coordinate system is obvious.
For instance, for azimuthal and polar rotations of points $(\boldsymbol{r}_x,\boldsymbol{r}_y, \boldsymbol{r}_z)$ in 3D space ($k=2$), a canonical coordinate system is $(\theta, \phi)$, where \smash{$\sin \theta \coloneqq \sqrt{\boldsymbol{r}_x^2 + \boldsymbol{r}_y^2} / \|\boldsymbol{r}\|_2$} and \smash{$\tan \phi \coloneqq \boldsymbol{r}_y /\boldsymbol{r}_x$}.
Rotating\footnote{Note that this differs from full 3D object pose invariance, for which the transformations do \emph{not} form an Abelian group.} simply `translates' the canonical coordinates $( \theta, \phi) \to (\theta + \Delta \theta, \phi + \Delta \phi)$ -- a transformation looking much more complicated in the Cartesian basis.


\pg{STRING for Abelian Lie groups}
It follows that, simply by replacing Cartesian coordinates $\{\boldsymbol{r}_i\}_{i=1}^N$ with their canonical counterparts, we can repurpose STRING to construct position encodings that respect more general invariances.










