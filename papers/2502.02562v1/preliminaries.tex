\section{Preliminaries}
\label{sec:preliminaries}
Let $\{ \mathbf{x}_i \}_{i=1}^N \in \mathbb{R}^d$ denote a set of $N$ $d$-dimensional tokens.
Assume that $d$ is even.
The $i$th query, key and value vectors are given by $\mathbf{q}_i = \mathbf{W}_q \mathbf{x}_i$, $\mathbf{k}_i = \mathbf{W}_k \mathbf{x}_i$ and $\mathbf{v}_i = \mathbf{W}_v \mathbf{x}_i$ respectively, where $\mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v \in \mathbb{R}^{d \times d}$ are learned projection matrices (to keep the notation simple, we assume here the one-head setting).
The \emph{attention mechanism}, basic computational unit of the Transformer, can be written as:
\vspace{-1.5mm}
\begin{equation}
    \mathbf{x}_i \rightarrow \frac{\sum_{j} \exp(\mathbf{q}_i^\top \mathbf{k}_j) \mathbf{v}_j }{\sum_l \exp(\mathbf{q}_i^\top \mathbf{k}_l)}.
\vspace{-1.5mm}
\end{equation}
This updates the set of tokens, mixing them dynamically depending on the query-key softmax similarity scores.

\pg{Rotary position encodings} 
As described in \cref{sec:intro_related}, RoPE rotates tokens depending on their locations. 
In the 1D data setting (e.g.~text), for a token $\mathbf{z}_i \in \{ \mathbf{q}_i, \mathbf{k}_i\}$ at position $i\in\mathbb{N}$, we take $\mathbf{z}_i \to \textrm{RoPE}(i) \mathbf{z}_i$ with
\vspace{-3mm}
\begin{equation} \label{eq:1d-rope}
\vspace{-3mm}
\textrm{RoPE}(i) \mathbf{z}_i \coloneqq 
\bigoplus_{n=1}^{d/2} 
\boldsymbol{\rho}(i\theta_n) [\mathbf{z}_i]_{2n-1:2n},
\end{equation}
\begin{equation}
\boldsymbol{\rho}(\theta) \coloneqq 
\begin{bmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{bmatrix}.
\end{equation}
Here, $\bigoplus$ denotes the direct product, so that each $2 \times 2$ matrix $\{\boldsymbol{\rho}(i \theta_n)\}_{i=1}^{d/2}$ independently rotates a 2-element section of the query/key, and $[\mathbf{z}_i]_{2n-1:2n}$ denotes the $2n-1$ and $2n$ elements of $\mathbf{z}_i$.
Note that the matrix $\textrm{RoPE}(i)$ is $d \times d$, but it is only nonzero on the $2 \times 2$ blocks on the diagonal.
Since $\boldsymbol{\rho}(\theta)^\top = \boldsymbol{\rho}(-\theta)$ and 2D rotations commute, we have that
\begin{equation}
    \textrm{RoPE}(i)^\top \textrm{RoPE}(j) = \textrm{RoPE}(j-i), 
\end{equation}
whereupon we are transforming $\mathbf{q}_i^\top \mathbf{k}_j \to \mathbf{q}_i^\top\textrm{RoPE}(j-i) \mathbf{k}_j$.
The dependence on $j-i$ makes this translationally invariant. 
RoPE takes the set of angles $\{ \theta_n\}_{n=1}^{d/2}$, determining the rotation frequency of each $2 \times 2$ block, as hyperparameters.
We suppress this dependence for notational compactness.
The authors originally proposed the decaying sequence $\theta_n = \lambda^{-2(n-1)/d}$ with base wavelength $\lambda=10,000$, though variants have since been explored (see below). 

\pg{RoPE in higher dimensions} 
Whilst RoPE was originally proposed for sequence data, recent work has extended it to encode higher-dimensional position information \citep{heo2025rotary}. 
Now each token is equipped with a vector $\mathbf{r}_i \in \mathbb{R}^{d_c}$, and we require: $\textrm{RoPE}(\mathbf{r}_i)^\top \textrm{RoPE}(\mathbf{r}_j) = \textrm{RoPE}(\mathbf{r}_j-\mathbf{r}_i)$. 
Since 2D rotations commute, one approach is to define
\vspace{-3mm}
\begin{equation} \label{eq:multidim_rope}
    \textrm{RoPE}(\mathbf{r}_i) \coloneqq \prod_{k=1}^{d_c}   \textrm{RoPE}([\mathbf{r}_i]_k),
\vspace{-1mm}
\end{equation}
where $[\mathbf{r}_i]_k$ is the $k$th coordinate of $\mathbf{r}_i$ (with $k\in\{1,...,d_c\}$).
This independently applies regular $1$-dimensional RoPE (Eq.~\ref{eq:1d-rope}) for each dimension of the position vector.
The rotation frequencies $\{ \theta_n\}_{n=1}^{d/2}$ can optionally differ between each coordinate axis. 

\pg{Generalisations of RoPE}
Prompted by its success, a number of papers have since sought to understand the effectiveness of RoPE and propose better-performing alternatives.
One well-known method argues to increase the base wavelength $\lambda$ to $500,000$, slowing the rate of token rotation and improving learning with longer contexts \citep{xiong2023effective, roziere2023code}. 
Another suggests to completely truncate the lowest frequencies, setting them to zero, which helps preserve long-range `semantic channels' \citep{barbero2024round}. 
Practitioners can also make the parameters $\{\theta_n\}_{n=1}^{d/2}$ fully learnable, improving flexibility.
Lastly, recent work has proposed to replace the block-diagonal RoPE matrices $\textrm{RoPE}(i)$ by more general dense matrices in $\textrm{SO}(d)$, parameterized by learned antisymmetric generators \citep{ostmeier2024liere}. 
Whilst more expressive, this algorithm breaks translational invariance for position vectors with $d_c>1$, and has a large memory footprint.
This makes it unsuitable for robotics applications.
In \cref{sec:string_core_section}, we will propose a better alternative, STRING.
\newpage
