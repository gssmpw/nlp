\vspace{-3mm}
\section{Experiments} 
\label{sec:experiments}
In this section, we provide an exhaustive empirical comparison of STRING with RoPE and vision encoders leveraging regular APEs. To set up the ground, we start with general non-robotics experiments in Sec. \ref{sec:general_experiments}. On our way to robotics applications, we then test STRING for 2D and 3D object detection in Sec. \ref{sec:owlvit-main}. Finally, we present robotics manipulation experiments in Sec. \ref{sec:aloha} and Sec. \ref{sec:rgl_section}.


\subsection{General Experiments: Classification and Retrieval}
\label{sec:general_experiments}
We tested STRING for image classification tasks on the $\mathrm{ImageNet2012}$ \cite{imagenet} and $\mathrm{Places365}$ datasets, with Vision Transformer (ViT) ~\cite{dosovitskiy2020vit} as our base model. We compare against RoPE and RoPE-Mixed \citep{heo2025rotary}, abbreviated to RoPE-M to Circulant-STRING and Cayley-STRING (respectively abbreviated to Circulant-S and Cayley-S). The results are shown in \cref{tab:img_class}. For both datasets, STRING offers best models. For $\mathrm{ImageNet2012}$, top two models are STRINGs. Furthermore, for $\mathrm{ImageNet2012}$ STRINGs provide the first absolute gains larger than $1\%$, as compared to regular ViTs, with only a negligible set of extra trainable parameters.

\vspace{-1mm}
\begin{table}[h]
    \centering
    \begin{tabular}{@{} l |@{}c@{} @{}c@{} @{}c@{}| @{}c@{} @{}c@{}}
    \toprule 
         & $\ $ $\ $ ViT $\ $ $\ $& $\ $ RoPE $\ $& $\ $ RoPE\textrm{-}M $\ $ & $\ $ Circulant\textrm{-}S $\ $ & $\ $ Cayley\textrm{-}S \\
         \midrule
        $\mathrm{ImageNet}$ & $80.04$ &  $80.18$ & $80.86$ & $\textbf{81.22}$ & $\underline{81.09}$ \\
        $\mathrm{Places365}$ & $56.79$ & \underline{$56.97$} & $56.69$ & $56.77$ & $\textbf{57.16}$ \\
        \midrule
        Mean & $68.42$ & $68.58$ & $68.78$ & $\underline{69.00}$ & $\textbf{69.12}$ \\
        \bottomrule
    \end{tabular}
    \caption{Image classification $\%$ test accuracy. Best numbers are highlighted in \textbf{bold} and the second-best numbers are \underline{underlined}.}
    \label{tab:img_class}
\vspace{-3mm}
\end{table}
Next, we lift \textrm{WebLI} \citep{chen2023pali}, a dataset of 10 billion image-text pairs across a variety of languages, into 3D by pre-processing a 60-million image subset with Depth-Anything-V2 \citep{yang2024depthv2} for metric mono-depth estimation.
The dataset is filtered using methods from \citep{chen2024spatialvlm} for images that the indoor-finetuned model performs poorly on (pictures with overlays, no visible groundplane, large outdoor scenes, optical illusions are removed). 

We perform contrastive learning on the text to visual representation pairs in the WebLI-3D lifted dataset, where the visual representation may be in the form of an RGB image or an RGB-D depth image.
Similar to CLIP \citep{radford2021clip}, this is done by maximizing the similarity between the embeddings of matching visual-text pairs, while minimizing the similarity between embeddings of the non-matching pairs. 
This enables open-vocabulary detection and classification by comparing the text embeddings of all possible classes against those of the visual representation, and selecting the minimum distance pair. We compare against baseline in \cref{tab:webli3d}. \textbf{For all six evaluations}, Cayley-STRING is the best and Circulant-STRING is second best.
\newpage

\begin{table}[h]
    \centering
    \begin{tabular}{@{}l @{} r@{} r@{} r@{} r@{} r@{} r@{} r@{}}
    \toprule 
    & $\ $ $\ $  i2t@1 &	$\ $ i2t@5 &	$\ $ i2t@10 &	$\ $ t2i@1	 & $\ $ t2i@5 & $\ $ t2i@10 & $\ $	$\ $ Mean \\ \midrule
    ViT & 53.88 &	73.17 &	78.49 &	53.98 &	73.83 &	79.29 &	68.77 \\
    RoPE & 55.27 &	74.27 &	79.52 &	55.22 &	74.61 &	80.25 &	69.86  \\
    RoPE\text{-}M & 55.30 &	74.08 &	79.47 &	55.36 &	74.73 &	80.18 &	69.85 \\
    \midrule
    Circulant\text{-}S & \underline{55.52} &	\underline{74.69} &	\underline{79.91} &	\underline{55.68} &	\underline{75.03} &	\underline{80.45} &	\underline{70.21} \\
    Cayley\text{-}S & \textbf{55.70} &	\textbf{75.08} &	\textbf{80.24} &	\textbf{55.82} &	\textbf{75.40} &	\textbf{80.65} &	\textbf{70.48} \\
    \bottomrule
\end{tabular}
    \caption{Image-to-text (i2t) and text-to-image (t2i) WebLI recall @ rank (best numbers: in \textbf{bold}, second-best: \underline{underlined}.)}
    \label{tab:webli3d}
\end{table}
\subsection{Improving Open-Vocabulary Object Detection}
\label{sec:owlvit-main}
\subsubsection{Open-Vocabulary Object Detection in 2D}
\label{sec:owlvit}



We demonstrate the efficacy of STRING on open-vocabulary object detection for localizing a 2D bounding box on standard RGB image benchmarks. For a baseline, we use the official implementation of OWL-ViT\footnote{\url{https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit}} \citep{minderer2022simple} which applies a standard Vision Transformer \citep{dosovitskiy2020vit} with light-weight classification and localization heads for detection. \cref{tab:scenic_experiment_results} compares the baseline OWL-ViT model with RoPE and STRING variants. For all experiments, we followed the standard training pipeline for the B/32 backbone with the CLIP loss \citep{radford2021clip}. Even in this axis-aligned 2D detection setting -- which is favourable for the standard RoPE variant --
Cayley-STRING provides the best overall performance.



\begin{table}[h]
\centering
\begin{tabular}{@{}l |@{}c@{} @{}c@{} @{}c@{}| @{}c@{} @{}c@{}}
\toprule     
& $\ $ Baseline $\ $& $\ $ RoPE $\ $& $\ $ RoPE\textrm{-}M $\ $ & $\ $ Circulant\textrm{-}S $\ $ & $\ $ Cayley\textrm{-}S \\
\midrule
\cocoAP & 32.44 & \textbf{33.66} & 32.74 & 33.24 & \underline{33.47} \\
\lvisAP & 21.98 & \underline{22.71} & 22.43 & 22.60 & \textbf{23.01} \\
\midrule
Mean & 27.21 & \underline{28.18} & 27.59 & 27.92 & \textbf{28.24} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{Average Precision (AP) \% of the OWL-ViT model on COCO \citep{lin2014microsoft} and LVIS \citep{gupta2019lvis}. Best in \textbf{bold}, second-best \underline{underlined}.}
\label{tab:scenic_experiment_results}
\end{table}

\begin{figure*}
\centering
\setlength\tabcolsep{2pt}%
\newlength\CellWidth
\CellWidth=0.3\textwidth
\newlength\CellHeight
\CellHeight=0.75\CellWidth
\begin{tabular}{rccc}
 & \textbf{Baseline} & \textbf{RoPE} & \textbf{Cayley-STRING} \\
\rotatebox{90}{\parbox{\CellHeight}{\centering\textbf{ViT}}} & \includegraphics[width=\CellWidth,height=\CellHeight]{figures/owlvit3d_vit_baseline.png} & \includegraphics[width=\CellWidth,height=\CellHeight]{figures/owlvit3d_vit_axial_rope_fixed.png} & \includegraphics[width=\CellWidth,height=\CellHeight]{figures/owlvit3d_vit_canonical_liepe.png} \\
\rotatebox{90}{\parbox{\CellHeight}{\centering\textbf{ViTD}}} & \includegraphics[width=\CellWidth,height=\CellHeight]{figures/owlvit3d_vitd_baseline.png} & \includegraphics[width=\CellWidth,height=\CellHeight]{figures/owlvit3d_vitd_axial_rope_fixed.png} & \includegraphics[width=\CellWidth,height=\CellHeight]{figures/owlvit3d_vitd_canonical_liepe.png} \\
\end{tabular}
\captionof{figure}{Example outputs for the 3D detection task for baseline, RoPE, and Cayley-S. Green boxes: groundtruth. Blue boxes: predictions.}
\label{fig:owlvit3d_examples}
\end{figure*}

\subsubsection{Open-Vocabulary Object Detection in 3D}
\label{sec:owlvit3d}

We tested STRING on the open-vocabulary 3D object bounding box prediction task, similar to those from \cref{sec:owlvit}.
Here we modify the output to be the full $\textrm{SE}(3)$ pose and 3D size of the bounding box.
We initialize the weights of the vision and text towers of our model with the weights from the models trained on the WebLI-3D dataset from \cref{sec:general_experiments}.
We replace the IOU loss from OWL-ViT with an 8-corner vertex loss, but otherwise keep the same matching and loss algorithm.
We train on a simulated dataset of 4 million images of indoor and tabletop scenes with groundtruth 3D bounding box labels (see \cref{app:owlvit3d_sim_data}).
From this dataset, we hold out 80 images for evaluation.
We evaluate both ViT and ViTD variants.
The 3D intersection-over-union (IOU) values for various RoPE and STRING variants on the evaluation data are shown in \cref{tab:owlvit3d}.
For each configuration, we train 3 models with different random seeds and take the best performing model (see \cref{app:owlvit3d_matcher} for details). Fig.
\ref{fig:owlvit3d_examples} shows example 3D detections for 6 different variants (see \cref{app:owlvit3d} for details). Note that STRINGs provide much more accurate prediction of the 3D bounding boxes for more challenging to localize smaller objects than baseline and RoPE. 
For ViT, Circulant-STRING is the best, providing $1.5\%$ relative improvement as compared to the best RoPE variant. For ViTD, Cayley-STRING is the best, providing $2\%$ relative improvement as compared to the best RoPE variant. For both ViT and ViTD, two best models are STRINGs.

\npdecimalsign{.}
\nprounddigits{2}
\begin{table}[h]
\centering
\begin{tabular}{@{}l|@{}c@{} @{}c@{} @{}c@{}| @{}c@{} @{}c@{}}
\toprule 
     & $\ $  Baseline $\ $& $\ $ RoPE $\ $& $\ $ RoPE\textrm{-}M $\ $ & $\ $ Circulant\textrm{-}S $\ $ & $\ $ Cayley\textrm{-}S \\
\midrule
ViT & \numprint{49.774} & \numprint{58.091} & \numprint{57.167} & \textbf{\numprint{58.949}} & \underline{\numprint{58.849}} \\
ViTD & \numprint{67.599}  & \numprint{71.206} & \numprint{70.897} & \underline{\numprint{72.36}} & \textbf{\numprint{72.665}} \\
\bottomrule
\end{tabular}
\caption{Average 3D IOU \% over all objects for the 3D bounding box prediction task. For each setting, 3 models were trained with different random seeds and the max is reported. Baseline indicates no RoPE or STRING. Best in \textbf{bold}, second-best \underline{underlined}.}
\label{tab:owlvit3d}
\vspace{-2mm}
\end{table}
\npnoround


\vspace{-2mm}
\subsection{Simulation-Based Robot Manipulation: ALOHA}
\label{sec:aloha}
We evaluate the performance of STRING on dexterous robotics tasks using ALOHA 2, a bimanual parallel-jaw gripper workcell with two 6-DoF arms, within the ALOHA Unleashed~\citep{zhao2024aloha} simulation environment (see: Fig.  \ref{fig:aloha_double_insertion_comparison}).
ALOHA Unleashed utilizes a scalable teleoperation framework used to collect human demonstration data.

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center} \vspace{-13mm}
\setlength\tabcolsep{2pt}%
\begin{tabular}{c}
\includegraphics[width=0.47\textwidth]{figures/aloha_real_vs_sim/real.png} \\
\includegraphics[width=0.47\textwidth]{figures/aloha_real_vs_sim/sim.png}
\end{tabular}
\captionof{figure}{\small{\mbox{\textbf{HandOverBanana}} task for the ALOHA 2 robot: real (top) and the corresponding simulated (bottom) evaluation.}} \vspace{-13mm}
\label{fig:aloha_real_vs_sim}
  \end{center}
\end{wrapfigure}
We trained ALOHA Unleashed's Transformer-based neural network with diffusion policies (conditioned on vision encoders) on human demonstrations of 12 dexterous tasks (see \cref{app:aloha} for descriptions and renders).
The vision system utilized images from RGB cameras positioned overhead, on each wrist, and at the level of the table.
We also deployed our policies on real ALOHA 2 robots (see Fig. \ref{fig:aloha_real_vs_sim} and Fig. \ref{fig:aloha_real_tasks}). Due to the large observed variance of the on-robot evaluation for ALOHA 2, we focused on the evaluation in simulation to accurately rank different methods.

\npdecimalsign{.}
\nprounddigits{2}
\begin{wraptable}{r}{0.5\textwidth}
\centering
\begin{scriptsize}\vspace{-5mm}
\begin{tabular}{@{}lccc@{}}
\toprule
 & ViT & RoPE & STRING \\
\midrule
BowlOnRack & \underline{0.90} & 0.80 & \textbf{1.00} \\
DoubleInsertion & 0.20 & \underline{0.50} &  \textbf{0.60} \\
FMB-1 & \textbf{0.20} & \textbf{0.20} & \textbf{0.20} \\
FMB-2 & \textbf{0.10} & \textbf{0.10} & \textbf{0.10} \\
FruitBowl & \textbf{0.30} & \textbf{0.30} & \textbf{0.30} \\
GlassOnRack & \textbf{0.60} & \textbf{0.60} & \textbf{0.60} \\
HandOverBanana & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} \\
HandOverPen & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} \\
MugOnPlate & 0.70 &  \textbf{0.90} & \underline{0.80} \\
PlateOnRack & \underline{0.60} &  \textbf{0.70} & 0.50 \\
SingleInsertion & \underline{0.40} &  \textbf{0.60} &  \textbf{0.60} \\
StorageBin & \textbf{0.00} & \textbf{0.00} & \textbf{0.00} \\
MultiTask & \numprint{0.3667} & \underline{\numprint{0.4167}} & \textbf{\numprint{0.4583}} \\
\bottomrule
\end{tabular}
\end{scriptsize}
\caption{\small{Mean success rate on ALOHA simulation task.}}
\label{tab:aloha_sim_experiment_results}
\vspace{-5mm}
\end{wraptable}
\npnoround

\cref{tab:aloha_sim_experiment_results} reports mean success rate (best in \textbf{bold}, second-best \underline{underlined}) over 10 evaluations of each ALOHA simulation task. ALOHA leader robots are teleoperated in ALOHA simulation for data collection using an ALOHA station, an Oculus VR headset and controllers \citep{zhao2024aloha}.
The teleoperators are instructed to perform the following tasks using this setup. RoPE~\citep{heo2025rotary} and Cayley-STRING are added to a baseline SigLIP B/16 256 ViT~\citep{zhai2023sigmoid}%
. See \cref{app:aloha} for details. We find that STRING has the best success rate across 11 of 13 tasks, thereby showcasing the effectiveness of our model



\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-7mm}
\centering
\includegraphics[width=0.45\textwidth]{figures/aloha_sim_multitask_training_svg-tex.pdf}
\vspace{-4mm}
\captionof{figure}{Mean success rate across all tasks (i.e. MultiTask) evaluated 10 times every 10K train steps over 1M train steps.}
\label{fig:aloha_sim_multitask_training}
\vspace{-5mm}
\end{wrapfigure}
The success rate is averaged over 10 trials of each checkpoint, taken every 10K train steps and over 1M train steps.
Corresponding curves are given in Fig. \ref{fig:aloha_sim_multitask_training}.
Cayley-STRING achieves superior results across all tasks on average (i.e. MultiTask).
Additionally, it achieves equivalent or superior results, as compared to RoPE (e.g. for the \textrm{DoubleInsertion} task from Fig. \ref{fig:aloha_double_insertion_comparison}) and ViT for all 12 tasks except for \textrm{MugOnPlate} and \textrm{PlateOnRack} (second-best).
Finally, STRING converges much faster than other methods (see: Fig. \ref{fig:aloha_sim_multitask_training}).
Note that we applied the strategy of learning all angle-defining frequencies for both RoPE and Cayley-STRING.









\subsection{Real-World 3D Robot Manipulation: KUKA}
\label{sec:rgl_section}

Establishing STRING as superior to other methods on previous tasks, we let it operate on 3D data to obtain new SOTA robotic manipulation policies. This resulted in policies directly using depth and deployed on real hardware. Note that STRING can be naturally applied in that context since it can be used for data equipped with general coordinate vectors $\mathbf{r}_{i}$ associated with tokens (e.g. 3D). 
\vspace{-2mm}
\subsubsection{Setting}
\label{sec:rgl_setting}
We evaluated STRING in the vision encoder of a generative policy applying energy-based models~\citep{singh2024revisiting} and deployed on a real industrial KUKA robot arm~\citep{JADYK23}. The closed-loop feedback policy operates on the RGB-D images, and is learned as a generative model with imitation learning. Its architecture is shown in \cref{fig:rgl_kuka_arch} (\cref{app:rgl_3d}), and consists of the diffusion Transformer and the 3D encoder. The policy was trained on a mixture of scripted and teleoperated data collected for 3 different skills (\textrm{pick}, \textrm{place} and \textrm{handover}) on various objects. 
It is evaluated exclusively on the pick skill with a diverse set of objects. Each evaluation was run as an A/B test for a total of 50 trials. \vspace{-2mm}
\subsubsection{Regular evaluations}
\label{sec:regular_evals_rgl}
We experimented with two ways of using depth in the policy.

\begin{wrapfigure}{r}{0.5\textwidth}
\centering \vspace{-3mm}
\includegraphics[width=0.4\textwidth]{figures/real_kuka_eval.pdf}
\vspace{-4mm}
\captionof{figure}{\small{Performance of STRING with 3D input vs. baselines on real-robot tasks (with 2 seeds). 2D baseline performance without depth input is $\approx65\%$. Incorporating depth through surface normal maps (nmap) reduces performance to $42\%$. Using 3D STRING for incorporating depth improves the performance in both scenarios - with and without normal maps to $53\%$ and $74\%$ respectively. Mean/\textrm{stdev} shown above were calculated from $35$ evaluation runs.}}
\label{fig:kuka_real_eval}
\vspace{-6mm}
\end{wrapfigure}
\paragraph{Implicit depth via normal maps:} In the first approach, following~\citep{10341422}, depth input is used to construct a surface normal map with unit $\mathbb{R}^3$ values per pixel. %
Both RGB and depth inputs are then processed via identical (shared weights) embedding layers. The embeddings are concatenated and processed through Transformer layers. Finally, the embeddings are split and fused to yield the final vision embedding. Our results in \cref{fig:kuka_real_eval} show that this approach of incorporating depth has a detrimental effect on the on-robot deployed policy. We hypothesize that this is caused by the significant amount of noise coming from the depth sensors, leading to imperfect surface normal maps.
\vspace{-3mm}
\paragraph{Lifting patches to 3D for STRING:} In the second approach, we compute the height for each patch via mean-pooling across depth values for all the pixels in the patch, followed by the learnable linear layer. The resulting 3D patches are then fed to the Transformer, with positional encoding given by STRING to incorporate depth into the vision encoder. Our results \cref{fig:kuka_real_eval} show that STRING improves the success rate over the 2D base policy. Also, when STRING is combined with the first method, it \textbf{drastically} reduces the negative impact of noisy normal maps.

We used Circulant-STRING to obtain a particularly compact computational footprint. Note that in this setting, more computationally intense mechanisms, such as \cite{ostmeier2024liere}, run out of memory and could not be trained.



\subsubsection{Out-of-distribution evaluation: STRING vs baseline}
To further compare STRING with the baseline and show the 
advantages of using 3D over 2D policies, we also 
perform \textit{out-of-distribution} (OOD) evaluations
on the real robot.

We vary three different environment settings. These include: (1) lighting changes, (2) adding large distractor objects and (3) changing the height of the table from which the robot has to grasp the block. For each setting, we test multiple different variations, e.g., three different light settings. 

\cref{fig:kuka-real-world-ood-evals} compares  STRING with the 2D baseline for each OOD setting.
For these evaluations, we choose the best policies from \cref{sec:regular_evals_rgl}. 
As seen in \cref{fig:kuka-real-world-ood-evals},
3D STRING policies outperform 2D policies across all OOD settings.
For instance, with large distractors (middle), the 2D model's performance decreases from $65\%$ to $57\%$, while 3D STRING \emph{maintains} performance similar to non-OOD settings ($\approx 74\%$).
In some OOD cases, such as lighting changes, both 2D ($\approx 10\%$) and 3D ($\approx 25\%$) policies experience a performance decrease vs. the non-OOD setup.
This drop in performance during lighting changes is likely due to the significant alteration in image observations, thus affecting both 2D and 3D policies.
Finally, the largest performance difference is observed in the table height variation scenario. Here, the 3D policies exhibit significantly higher robustness ($\approx \mathbf{50\%}$) compared to the 2D policies ($\approx \mathbf{10\%}$). This suggests that the 3D STRING policy leverages the raw depth signal to better generalize to table height variations, a change imperceptible to fixed monocular cameras.

Overall, our results show that 3D STRING policies are highly robust to many variations and significantly improve over 2D policies.
Fig. \ref{fig:kuka-real-world} shows a sample episode from the on-robot evaluation of the STRING generative policy.
\newpage


\begin{figure}[h]
\centering
\includegraphics[width=0.99\linewidth]{figures/rgl-evals-3d-ood_01.pdf}%
\vspace{-3mm}
\captionof{figure}{Comparison of 2D baseline with 3D STRING on out-of-distribution scenarios for real-world Kuka robot evaluations.}
\label{fig:kuka-real-world-ood-evals}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[trim=0 0 602 0,clip,width=.5\linewidth]{figures/kuka_frames_real_world_02.pdf}%
\includegraphics[trim=602 0 0 0,clip,width=.5\linewidth]{figures/kuka_frames_real_world_02.pdf}
\vspace{-6mm}
\captionof{figure}{Rollout frames from a successful policy evaluation in the real world (top: RGB, bottom: depth). Best viewed zoomed-in.}
\label{fig:kuka-real-world}
\end{figure}

\paragraph{From 2D to 3D with STRING:}
We have already demonstrated (see the normal map approach from \cref{sec:regular_evals_rgl}) that just adding a depth signal does not necessarily improve performance.
STRING does so and exhibits another feature that other approaches (e.g. adding depth as extra channel) do not: it can be trained from a regular 2D pre-trained checkpoint. This is the case since STRING incorporates depth by using it to modulate a regular attention matrix, effectively disentangling 3D specific parameters (defining the modulation) from the core 2D backbone. All training runs in \cref{sec:rgl_section} started from the pre-trained 2D backbones.
