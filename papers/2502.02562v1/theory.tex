\section{Theoretical foundations of STRING} \label{sec:theoretical_foundations}
In this section, we discuss the theoretical properties of $\times$-STRING and $\otimes$-STRING in greater detail.
Proofs for all theoretical claims are given in App.~\ref{app:proofs}.

\subsection{Properties of $\times$-STRING}
The following is true.
\begin{theorem}[Deriving $\times$-STRING] \label{thm:x-string-deriv}
Consider mappings $\mathbf{R}(\cdot):\mathbb{R}^{d_c} \to \mathbb{R}^{d \times d}$ that satisfy $\mathbf{R}(\mathbf{r}_i)^\top \mathbf{R}(\mathbf{r}_j) = \mathbf{R}(\mathbf{r}_j - \mathbf{r}_i)$ for all $\mathbf{r}_i, \mathbf{r}_j \in \mathbb{R}^{d_c}$.
All such mappings can be written $ \mathbf{R}(\mathbf{r}_i) = \exp \left (\sum_{k=1}^{d_c} \mathbf{L}_k [\mathbf{r}_i]_k \right )$,
where $\{\mathbf{L}_k\}_{k=1}^{d_c} \subset \mathbb{R}^{d \times d}$ is a set of commuting antisymmetric generators. 
\end{theorem}
In this sense, $\times$-STRING is the \emph{most general} of all RoPE-like position encodings that rely on matrix multiplication. 
RoPE is a simple special case of $\times$-STRING, taking a particular choice of generators. 
This is formalised as follows.
\begin{theorem}[RoPE is a special case of $\times$-STRING] \label{thm:rope_special_case_1}
Consider the generators
\begin{equation}
    \mathbf{L}_k =  \sum_{p=1}^{d/2} (\delta_{2p,2p-1} - \delta_{2p-1,2p}) \theta_p, \quad \theta_p \in \mathbb{R},
\end{equation}
with $\delta_{i,j}$ the delta function.
This is only nonzero on the superdiagonal and subdiagonal. 
This corresponds exactly to RoPE, with $\{\theta_p\}_{p=1}^{d/2}$ the rotational frequencies.
\end{theorem}
In fact, the relationship between RoPE and $\times$-STRING is closer still.
\begin{theorem}[$\times$-STRING is RoPE with a change of basis]\label{thm:basis_change}
For an $\times$-STRING position encoding specified by generators $\{\mathbf{L}_k\}_{k=1}^{d_c}$, there exists an orthogonal matrix $\mathbf{P}$ such that
\begin{equation}
    \mathbf{R}(\boldsymbol{r}_i) = \mathbf{P} \emph{\textrm{RoPE}}(\boldsymbol{r}_i) \mathbf{P}^\top.
\end{equation}
That is, $\times$-STRING is equivalent to RoPE in a different basis.
\end{theorem}
We give a proof in App.~\ref{app:basis-change}.

\textbf{Money for old RoPE?}
On the one hand, Thm.~\ref{thm:basis_change} makes batching very convenient.
The basis change $\mathbf{P}$ is independent of $\boldsymbol{r}_i$ so is always applied to all tokens; we don't need to compute and store it for every token position.
In the new basis RoPE can be applied efficiently using sparse operations, in $\mathcal{O}(d)$ time, as per the original paper \citep{heo2025rotary}.
Moreover, parameterizing the orthogonal basis transformations $\mathbf{P}$ as matrices that support fast matrix-vector multiplication, we can get further speedups.

On the other hand, Thm.~\ref{thm:basis_change} might make it surprising that $\times$-STRING outperforms RoPE.
For softmax attention, one could in principle absorb the learnable basis-change $\mathbf{P}$ into existing weight matrices in the network.
Observe the following:
\begin{equation}
\begin{multlined}
    \mathbf{q}_i^\top \mathbf{R}(\mathbf{r}_i)^\top \mathbf{R}(\mathbf{r}_j)\mathbf{k}_j = \mathbf{x}_i^\top \mathbf{W}_q^\top \mathbf{R}(\mathbf{r}_i)^\top \mathbf{R}(\mathbf{r}_j) \mathbf{W}_k \mathbf{x}_j \\  = \mathbf{x}_i^\top \mathbf{W}_q^\top \mathbf{P} \textrm{RoPE}(\mathbf{r}_i)^\top \mathbf{P} \mathbf{P}^\top \textrm{RoPE}(\mathbf{r}_j) \mathbf{P}^\top \mathbf{W}_k \mathbf{k}_j
    \\ = \mathbf{x}_i^\top \widetilde{\mathbf{W}}_q^\top \textrm{RoPE}(\mathbf{r}_i)^\top \textrm{RoPE}(\mathbf{r}_j) \widetilde{\mathbf{W}}_k \mathbf{x}_j.
\end{multlined}
\end{equation} 
We used the fact that $\mathbf{P}$ is orthogonal and defined new weight matrices $\widetilde{\mathbf{W}}_{\{q,k\}} \coloneqq \mathbf{P}^\top \mathbf{W}_{\{q,k\}}$ that absorb the basis change.
However, it appears that \emph{explicitly} parameterizing a shared basis change, rather than relying on the network learning it, improves the initialisation and conditioning of the optimisation, boosting performance.
This is analogous to how \emph{residual connections}, which parameterize neural network layers as an identity mapping plus a residual term, work well in practice despite not changing the network's capacity.

We also note that the arguments above only apply when the $\times$-STRING rotational transformation is applied immediately after a linear projection layer. 
If we consider linear attention, where one applies a nonlinearity like $\textrm{ReLU}(\cdot)$ before taking the dot product between queries and keys \citep{katharopoulos2020transformers}, then one is \emph{not} in general able to absorb a basis change into existing network weights. 
In this setting, the reasons for the improvements provided by $\times$-STRING are clearer, since the orthogonal projection $\mathbf{P}$ also actually increases the capacity of the model.


