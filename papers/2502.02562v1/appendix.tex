\appendix
\section{Proofs} \label{app:proofs}
\subsection{Proof of \cref{thm:x-string-deriv}}
To begin, we provide a proof of \cref{thm:x-string-deriv}: that STRING is the most general form of transformation that respects the group-like property
\begin{equation} \label{eq:group_exp}
    \mathbf{R}(\boldsymbol{r}_i)^\top \mathbf{R}(\boldsymbol{r}_j) = \mathbf{R}(\boldsymbol{r}_j - \boldsymbol{r}_i),
\end{equation}
supposing that $\mathbf{R}(\mathbf{0})=\mathbf{I}_d$ and $\mathbf{R}(\mathbf{r})$ is continuously differentiable with respect to $\mathbf{r}$.  
Note that this is a sufficient, but not necessary, assumption for translational invariance.

\emph{Proof}.
Recall STRING is applied as
\begin{equation} \label{eq:mat_mult_encoding}
    \mathbf{q}_i \to \mathbf{R}(\boldsymbol{r}_i) \mathbf{q}_i
\end{equation}
with $\mathbf{R}(\cdot): \mathbb{R}^{d_c} \to \mathbb{R}^{d \times d}$, so that $\mathbf{R}(\boldsymbol{r}_i) \in \mathbb{R}^{d \times d}$. 
Then we require that
\begin{equation}
    \mathbf{q}_i^\top \mathbf{k}_j \to \mathbf{q}_i^\top \mathbf{R}(\boldsymbol{r}_i)^\top \mathbf{R}(\boldsymbol{r}_j)\mathbf{k}_j. 
\end{equation}
Recall that $\mathbf{R}(\boldsymbol{0})=\mathbf{I}_d$, the $d$-dimensional identity, so that the logit of a query and key at the same position ($\boldsymbol{r}_i = \boldsymbol{r}_j$) will be unmodified by the positional encoding. 
Then clearly $ \mathbf{R}(\boldsymbol{r}_i) \in \textrm{O}(d)$, the orthogonal group in dimension $d$. 
For compatability with gradient-based optimisers (a chief concern in the machine learning setting), it is convenient to specialise to the connected component (normal subgroup) of $\textrm{O}(d)$ containing the identity matrix: that is, the special orthogonal group $\textrm{SO}(d)$.\footnote{This means that you can optimise the position encoding transformations on the same manifold.
You could in priniciple also incorporate reflections so that $\textrm{det}(\mathbf{R}(\boldsymbol{r}_i))=-1$, but this seems unlikely to significantly improve performance.}
This means that $\det(\mathbf{R}(\boldsymbol{r}_i))=1$.
These transformations are the $d$-dimensional rotations. 

Since $\mathbf{R}(\boldsymbol{r}_i) \in \textrm{SO}(d)$, the rotation can be written using its Lie group, 
\begin{equation}
    \mathbf{R}(\boldsymbol{r}_i) = \exp( \mathbf{L}(\boldsymbol{r}_i))
\end{equation}
where the matrix $\mathbf{L}(\boldsymbol{r}_i)$ is antisymmetric \citep{hall2013lie}. 
$\mathbf{L}(\boldsymbol{r}_i)$ is called the `generator', representing an infinitesimal rotation.
Here, $\exp(\cdot)$ denotes the \emph{matrix} exponential (not to be confused with the element-wise exponential of a matrix, as appears e.g.~in softmax).
Setting $\boldsymbol{r}_j=\boldsymbol{0}$ in \cref{eq:group_exp}, it is clear that $\mathbf{L}(-\boldsymbol{r}_i) = - \mathbf{L}(\boldsymbol{r}_i)$.
We then require that
\begin{equation}
\begin{multlined}
    \exp(\mathbf{L}(\boldsymbol{r}_i)) \exp(\mathbf{L}(\boldsymbol{r}_j)) = \exp(\mathbf{L}(\boldsymbol{r}_i + \boldsymbol{r}_j)) 
    \\ = \exp(\mathbf{L}(\boldsymbol{r}_j)) \exp(\mathbf{L}(\boldsymbol{r}_i)). 
\end{multlined}
\end{equation}
Clearly $\mathbf{L}(\boldsymbol{r}_i)$ and $\mathbf{L}(\boldsymbol{r}_j)$ must commute for all choices of coordinate vector $(\boldsymbol{r}_i, \boldsymbol{r}_j)$. 
Therefore, we need
\begin{equation}
    \mathbf{L}(\boldsymbol{r}_i + \boldsymbol{r}_j) = \mathbf{L}(\boldsymbol{r}_i) + \mathbf{L}(\boldsymbol{r}_j),
\end{equation}
so $\mathbf{L}(\cdot)$ is linear in its arguments.
That is, $ \mathbf{L}(\cdot)$ is a linear map from the set of $d_c$-dimensional vectors to a set of commuting antisymmetric matrices.
We can write
\begin{equation}
    \mathbf{L}(\boldsymbol{r}_i) = \sum_{k=1}^{d_c} \mathbf{L}_k [\boldsymbol{r}_i]_k,
\end{equation}
with $\{\mathbf{L}_k\}_{k=1}^{d_c} \subset \mathbb{R}^{d \times d}$ a set of \emph{commuting antisymmetric generators} and $[\boldsymbol{r}_i]_k$ the $k$-th entry of coordinate vector $\boldsymbol{r}_i$.
This completes the proof. \qed

\subsection{Proof of \cref{thm:rope_special_case_1}}
Now, we prove that generators of the form 
$\mathbf{L}_k =  \sum_{p=1}^{d/2} (\delta_{2p,2p-1} - \delta_{2p-1,2p}) \theta_p$ recover RoPE, as described in \cref{thm:rope_special_case_1}.

\emph{Proof}.
Let us initially consider the case $d_c=1$, so that the token coordinate $\mathbf{r}=r \in \mathbb{R}$ and we learn a single generator.
Recall that powers of a block diagonal matrix will remain block diagonal.
Each block of the generator $\mathbf{L}_k$ is of the form
\begin{equation}
    \begin{bmatrix}
0 & \theta \\
-\theta & 0
\end{bmatrix}.
\end{equation}
Then note that
\begin{equation}
\begin{bmatrix}
0 & \theta \\
-\theta & 0
\end{bmatrix}^2 = \begin{bmatrix}
-\theta^2 & 0  \\
0 & -\theta^2
\end{bmatrix}.
\end{equation}
It follows that
\begin{equation}
\begin{bmatrix}
0 & \theta \\
-\theta & 0
\end{bmatrix}^n = 
\begin{cases}
\theta^n(-1)^{n/2} \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix} & \textrm{if } n \textrm{ is even,} \\
\theta^n(-1)^{(n-1)/2} \begin{bmatrix}
0 & 1 \\
-1 & 0
\end{bmatrix} & \textrm{if } n \textrm{ is odd.}
\end{cases}
\end{equation}
Combining and inspecting the Taylor expansions, 
\begin{equation}
    \exp \begin{bmatrix}
0 & \theta \\
-\theta & 0
\end{bmatrix} = \begin{bmatrix}
\cos \theta & \sin \theta \\
-\sin \theta & \cos \theta
\end{bmatrix},
\end{equation}
which is clearly a rotation matrix -- a well-known result.
This holds for all the $d/2$ blocks, each of which exponentiates to give a $2 \times 2$ rotation at a different frequency.
Therefore, 
\begin{equation}
    \exp(\mathbf{L}r) = \textrm{RoPE}(r),
\end{equation}
showing that with this particular generator STRING is RoPE.

Now suppose that $d_c > 1$, so $\mathbf{L}(\boldsymbol{r}_i) = \sum_{k=1}^{d_c} \mathbf{L}_k [\boldsymbol{r}_i]_k$.
In our special case, each generator is of the form $\mathbf{L}_k =  \sum_{p=1}^{d/2} (\delta_{2p,2p-1} - \delta_{2p-1,2p}) \theta_p$, where $\{\theta_p\}_{p=1}^{d/2}$ can differ for different $k$ (notationally suppressed for compactness).
Observing that 
\begin{equation}
\begin{multlined}
\begin{bmatrix}
0 & \alpha \\
-\alpha & 0
\end{bmatrix}
\begin{bmatrix}
0 & \beta \\
-\beta & 0
\end{bmatrix} = 
\begin{bmatrix}
-\alpha \beta & 0 \\
0 & -\alpha \beta 
\end{bmatrix} \\
= \begin{bmatrix}
0 & \beta \\
-\beta & 0
\end{bmatrix}
\begin{bmatrix}
0 & \alpha \\
-\alpha & 0
\end{bmatrix},
\end{multlined}
\end{equation}
different $\mathbf{L}_k[\mathbf{r}_i]_k$ commute.
Then we have that
\begin{equation}
\begin{multlined}
    \exp\left(\sum_{k=1}^{d_c} \mathbf{L}_k[\mathbf{r}_i]_k\right) = \prod_{k=1}^{d_c}   \exp\left( \mathbf{L}_k[\mathbf{r}_i]_k\right) \\ =\prod_{k=1}^{d_c}  \textrm{RoPE}([\mathbf{r}_i]_k) = \textrm{RoPE}(\mathbf{r}),
\end{multlined}
\end{equation}
where we used the definition for multidimensional RoPE from \cref{eq:multidim_rope}.
This completes the proof. \qed

\subsection{Proof of \cref{thm:basis_change}} \label{app:basis-change}
Next, we prove that STRING can always be rewritten as RoPE in a different basis. 

\emph{Proof}. 
As usual, we begin with $d_c=1$.
Then our task is to show that a matrix $\mathbf{R}(r) = \exp(\mathbf{L}r)$, with $\mathbf{L} \in \mathbb{R}^{d \times d}$ an antisymmetric matrix and $r \in \mathbb{R}$, can be rewritten as RoPE with a change of basis.

Begin by noting that $\mathbf{R}(r) \in \textrm{SO}(d)$; it is a special orthogonal matrix.
It is orthogonal since $\mathbf{R}(r)^\top\mathbf{R}(r) = \exp(\mathbf{L}r)^\top \exp(\mathbf{L}r) = \exp(-\mathbf{L}r) \exp(\mathbf{L}r) = \mathbf{I}_d$, and its determinant is $1$ since it is continuously connected to the identity (which occurs at $r=0$).
Consider an eigenvector $\mathbf{v} \in \mathbb{C}^d$, with eigenvalue $\lambda \in \mathbb{C}$.
Since $\mathbf{R}^\top \mathbf{R} = \mathbf{I}_d$, $|\lambda|=1$ so $\lambda = e^{i \theta}$. 
Taking the complex conjugate of $\mathbf{R}\mathbf{v}=\lambda \mathbf{v}$, we have $\mathbf{R}\bar{\mathbf{v}}=\lambda^* \bar{\mathbf{v}}$, where $\lambda^* = e^{-i \theta}$ and $\bar{\mathbf{v}}$ is the complex conjugate of $\mathbf{v}$. 
We used that $\mathbf{R}$ is real.
Therefore, the eigenvalues appear in conjugate pairs for conjugate eigenvectors.


Let $\mathbf{v} = \mathbf{u} + i \mathbf{w}$ and $\bar{\mathbf{v}} = \mathbf{u} - i \mathbf{w}$ with $\mathbf{u}, \mathbf{w} \in \mathbb{R}^d$ real vectors. 
Inserting into the eigenvector equation, $\mathbf{R}(\mathbf{u} + i \mathbf{w}) = (\cos(\theta) + i \sin(\theta))(\mathbf{u} + i \mathbf{w})$.
Equating the real and imaginary parts, $\mathbf{R}\mathbf{u} = \cos(\theta) \mathbf{u} - \sin(\theta) \mathbf{w}$ and 
$\mathbf{R}\mathbf{v} = \cos(\theta) \mathbf{v} + \sin(\theta) \mathbf{u}$.
This corresponds exactly to $2$-dimensional rotation in the $(\mathbf{u},\mathbf{v})$ plane.
By normalisation of (complex) $\mathbf{v}$, $|\mathbf{u}|^2 + |\mathbf{w}|^2 = 1$.
But since $\lambda$ and $\lambda^*$ differ, their corresponding eigenvectors are orthogonal under the Hermitian inner product, so we also have that $\bar{\mathbf{v}}^\dag \mathbf{v} = |\mathbf{u}|^2 - |\mathbf{w}|^2 + 2i \mathbf{u}^\top \mathbf{w} = 0$.
So $\mathbf{u}^\top \mathbf{w}=0$, whereupon $\mathbf{u}$ and $\mathbf{w}$ are orthogonal vectors in $\mathbb{R}^d$.
Of course, from basic linear algebra the complex eigenvectors corresponding to \emph{different} $\theta$ will also be orthogonal in $\mathbb{C}^d$, or in the case of degenerate $\theta$ one can choose an orthogonal basis for the corresponding subspace using e.g.~the Gram-Schmidt process.
It is easy to show that the corresponding real vectors $(\mathbf{u}_i, \mathbf{w}_i)$ will therefore be orthogonal for different $i$, i.e.~$\mathbf{u}_i^\top \mathbf{u}_j =\delta_{i,j} = \mathbf{w}_i^\top \mathbf{w}_j$.
To summarise: the real and imaginary parts of the the $d$ orthogonal (complex) eigenvectors of $\mathbf{R}$ in $\mathbb{C}^d$ correspond to $d$ orthogonal (real) vectors in $\mathbb{R}^d$, organised in $\frac{d}{2}$ planes in each of which $\mathbf{R}$ acts as a 2-dimensional rotation, at a frequency that depends on the corresponding eigenvalue. 
These real vectors $\{\mathbf{u}_i, \mathbf{w}_i \}_{i=1}^{d/2}$ can be aggregated as the columns an orthogonal matrix $\mathbf{P}$, taking 
\begin{equation}
    \mathbf{P} \coloneqq 
\begin{bmatrix} 
\uparrow & \uparrow & \uparrow & \uparrow & \cdots  \\
\mathbf{u}_1 & \mathbf{w}_1 & \mathbf{u}_2 & \mathbf{w}_2 & \cdots\\
\downarrow & \downarrow & \downarrow & \downarrow & \cdots 
\end{bmatrix}
\in \mathbb{R}^{d \times d}
\end{equation}
whereupon 
\begin{equation}
    \mathbf{R}(r) = \mathbf{P} \textrm{RoPE}(r) \mathbf{P}^\top.
\end{equation}
Note especially that the change of basis matrix $\mathbf{P}$ is independent of $r$, because $\mathbf{P}$ determines the axes of rotation whereas the (complex) eigenvalues depend on the amount of rotation. 
This is obvious from the definition $\mathbf{R}(r)=\exp(\mathbf{L}r)$; the matrix $\mathbf{P}$ needs to (block) diagonalise $\mathbf{L}$, then this is sufficient to (block) diagonalise $\mathbf{R}$, and the matrix $\mathbf{L}$ is independent of $\mathbf{r}$.

Now suppose that $d_c>1$. 
Recall that we have
\begin{equation}
    \mathbf{R}(\boldsymbol{r}) = \exp \sum_{k=1}^{d_c} \mathbf{L}_k \boldsymbol{r}_k = \prod_{k=1}^{d_c} \exp \mathbf{L}_k \boldsymbol{r}_k = \prod_{k=1}^{d_c}  \mathbf{R}([\boldsymbol{r}]_k) , 
\end{equation}
where we used the fact that the generators $\{\mathbf{L}_k\}_{k=1}^{d_c}$ commute (\cref{def:coupled_norms_def}).
$\{\mathbf{R}(\mathbf{r}_k)\}_{k=1}^{d_c}$ must also commute so are simultaneously diagonalisable, whereupon
\begin{equation}
\begin{multlined}
    \mathbf{R}(\mathbf{r}) = \prod_{k=1}^{d_c} \mathbf{P} \textrm{RoPE}([\mathbf{r}]_k) \mathbf{P}^\top =  \mathbf{P} \left( \prod_{k=1}^{d_c} \textrm{RoPE}([\mathbf{r}]_k) \right) \mathbf{P}^\top \hspace{-10mm} \\  =  \mathbf{P}  \textrm{RoPE}(\mathbf{r}) \mathbf{P}^\top.
\end{multlined}
\end{equation}
This concludes the proof.
\qed

\subsection{Proof of \cref{thm:circ-string-fast}}
\begin{proof}
Note that it suffices to show that the computation of $\mathbf{p} = \exp(\mathbf{C}-\mathbf{C}^{\top})\mathbf{z}$ can be computed in time $\mathcal{O}(d \log(d))$ and memory $\mathcal{O}(d)$ for a circulant matrix $\mathbf{C}$, defined by its first row $\mathbf{c}$. We will leverage the fact that every circulant matrix $\mathbf{C}$ can be factorized as follows:
\begin{equation}
\mathbf{C} = \mathbf{DFT} \times \mathrm{diag}(\mathbf{DFT}\mathbf{c}) \times \mathbf{DFT}^{-1}, \end{equation}
for the Discrete Fourier Transform matrix $\mathbf{DFT}$, and where $\mathrm{diag}(\mathbf{v})$ stands for the diagonal matrix with the main diagonal given by $\mathbf{v}$.
Therefore we have the following:
\begin{align}
\begin{split}
\mathbf{p} = \exp(\mathbf{DFT}(\mathrm{diag}(\mathbf{DFT}\mathbf{u})))\mathbf{DFT}^{-1}\mathbf{z},    
\end{split}    
\end{align}
where $\mathbf{u} = \mathbf{c}-\mathbf{t}$ and $\mathbf{t}$ stands for the first column of $\mathbf{C}$. Here we leverage the fact that the transpose of the circulant matrix is also circulant. Therefore, by leveraging Taylor series formula for $\exp$, we get:
\begin{align}
\begin{split}
\mathbf{p} = \mathbf{DFT}\exp(\mathrm{diag}(\mathbf{DFT}\mathbf{u}))\mathbf{DFT}^{-1}\mathbf{z} = \\
\mathbf{DFT}\mathrm{diag}(\exp(\mathbf{DFT}\mathbf{u}))\mathbf{DFT}^{-1}\mathbf{z},
\end{split}    
\end{align}
where $\exp$ in the last formula is computed element-wise.
Note that matrix-vector product with diagonal matrices can be trivially conducted in linear time. Thus the calculation of $\mathbf{p}$ can be conducted in $\mathcal{O}(d)$ memory and $\mathcal{O}(d\log(d))$ time complexity via Fast Fourier Transform (FFT) (to multiply with $\mathbf{DFT}$ matrices) and inverse FFT (iFFT) (to multiply with matrices $\mathbf{DFT}^{-1}$). That completes the proof.
\end{proof}




\section{ALOHA Unleashed Simulation Tasks} \label{app:aloha}
\begin{figure*}
\centering
\includegraphics[width=0.75\textwidth]{figures/aloha_unleashed_arch_svg-tex.pdf}
\caption{ALOHA Unleashed Architecture \citep{zhao2024aloha}.}
\label{fig:aloha_unleashed_arch}
\end{figure*}
\begin{figure*}[ht]
\centering
\setlength\tabcolsep{2pt}%
\begin{tabular}{ccc}

\includegraphics[width=0.321\textwidth]{figures/aloha_sim_tasks/plate_on_rack.png} & \includegraphics[width=0.321\textwidth]{figures/aloha_sim_tasks/double_insertion.png} & \includegraphics[width=0.321\textwidth]{figures/aloha_sim_tasks/mug_on_plate.png} \\
 \includegraphics[width=0.321\textwidth]{figures/aloha_sim_tasks/single_insertion.png} &   \includegraphics[width=0.321\textwidth]{figures/aloha_sim_tasks/bowl_on_rack.png} & \includegraphics[width=0.321\textwidth]{figures/aloha_sim_tasks/fmb.png} \\
\end{tabular}
\captionof{figure}{Views from the wrist, overhead and table-level cameras in ALOHA sim. \textbf{Top to bottom, Left to right:} PlateOnRack, DoubleInsertion (insert peg into sockets on either end), MugOnPlate, SingleInsertion, BowlOnRack, and Functional Manipulation Benchmark-1 (FMB-1).}
\label{fig:aloha_sim_tasks}
\end{figure*}
\begin{figure*}
\centering
\setlength\tabcolsep{2pt}%
\begin{tabular}{ccc}
\includegraphics[width=0.321\textwidth]{figures/aloha_other_sim_tasks/banana.png} & \includegraphics[width=0.321\textwidth]{figures/aloha_other_sim_tasks/fmb.png} & \includegraphics[width=0.321\textwidth]{figures/aloha_other_sim_tasks/cup.png} \\
 \includegraphics[width=0.321\textwidth]{figures/aloha_other_sim_tasks/banana.png} &   \includegraphics[width=0.321\textwidth]{figures/aloha_other_sim_tasks/pen.png} & \includegraphics[width=0.321\textwidth]{figures/aloha_other_sim_tasks/storage_bin.png} \\
\end{tabular}
\captionof{figure}{More ALOHA Simulation Tasks. \textbf{Top to bottom, left to right:} FruitBowl, FMB-2, GlassOnRack, HandOverBanana, HandOverPen and StorageBin.}
\label{fig:aloha_other_sim_tasks}
\end{figure*}
ALOHA leader robots are teleoperated in ALOHA simulation for data collection using an ALOHA station, an Oculus VR headset and controllers \citep{zhao2024aloha}.
The teleoperators are instructed to perform the following tasks using this setup.
See \cref{fig:aloha_sim_tasks} and \cref{fig:aloha_other_sim_tasks} for simulation renders of the following 12 tasks.
\begin{enumerate}
    \item[1-3.] \texttt{\{Bowl/Glass/Plate\}OnRack}: Place the item on the rack.
    \item[4.] \texttt{SingleInsertion}: Use the left arm to grab the blue socket and the right arm to insert the block into the socket.
    \item[5.] \texttt{DoubleInsertion}: After completing the \texttt{SingleInsertion} task, insert another block into the other end of the socket.
    \item[6-7.] Functional Manipulation Benchmark (\texttt{FMB}) \texttt{1} and \texttt{2}: Insert a yellow block into the recess of a red base.
    \item[8.] \texttt{FruitBowl}: Place all the fruits in the bowl.
    \item[9.] \texttt{StorageBin}: Place all the snack boxes in the storage bin.
    \item[10-11.] \texttt{HandOver\{Banana/Pen\}}: Hand over the item and place it in the container.
    \item[12.] \texttt{MugOnPlate} Place the mug on the plate.
\end{enumerate}
\texttt{MultiTask} aggregates results of all of the above tasks.

\section{Aloha Real Tasks}
ALOHA-real models are first pre-trained with human-teleop data collected on 300 diverse tasks, which were crowd-sourced based on relevance with real-world scenarios as well as feasibility for the ALOHA robot. Further, the model is fine-tuned on the following 10 tasks.

\begin{enumerate}
    \item[1.] \texttt{open-jar-lid}: open the glass jar lid, handover to other hand and put on the table
    \item[2.] \texttt{bowl-in-rack}: put the bowl into the drying rack
    \item[3.] \texttt{cup-in-rack}: put the cup into the drying rack
    \item[4.] \texttt{banana-handover}: put banana in bowl with handover
    \item[5.] \texttt{open-drawer}: open the drawer
    \item[6.] \texttt{remove-gears}: remove the gears from the nist-board
    \item[7.] \texttt{fold-dress}: fold the dress
    \item[8.] \texttt{stack-cups}: stack the cups
    \item[9.] \texttt{pen-handover}: put pen in container with handover
    \item[10.] \texttt{take-phone-out}: take phone out of purse
\end{enumerate}

\begin{figure*}
\centering
\setlength\tabcolsep{2pt}%
\begin{tabular}{ccc}
\includegraphics[width=0.321\textwidth]{figures/aloha_real_tasks/take-phone-out.png} & \includegraphics[width=0.321\textwidth]{figures/aloha_real_tasks/stack-cup.png} & \includegraphics[width=0.321\textwidth]{figures/aloha_real_tasks/remove-gear.png} \\
 \includegraphics[width=0.321\textwidth]{figures/aloha_real_tasks/open-drawer.png} &   \includegraphics[width=0.321\textwidth]{figures/aloha_real_tasks/bowl-in-rack.png} & \includegraphics[width=0.321\textwidth]{figures/aloha_real_tasks/fold-dress.png} \\
\end{tabular}
\captionof{figure}{Sampled ALOHA Real Tasks. \textbf{Top to bottom, left to right:} take-phone-out, stack-cup, remove-gear, open-drawer, bowl-in-rack and fold-dress.}
\label{fig:aloha_real_tasks}
\end{figure*}

After fine-tuning, the model is then evaluated on the following 5 tasks: \texttt{bowl-in-rack}, \texttt{banana-handover}, \texttt{bowl-in-rack}, \texttt{fold-dress}, \texttt{remove-gears}. See: Fig. \ref{fig:aloha_real_tasks} for the visualizations of selected ALOHA real world tasks.


\section{WebLI-3D Dataset}  \label{app:webli}
We lift the WebLI dataset \citep{chen2023pali}, a dataset of 10 billion image-text pairs across a variety of languages, into 3D by pre-processing a 60-million image subset using Depth-Anything-V2 \citep{yang2024depthv2} for metric monodepth estimation.
The dataset is filtered using the method described in \citep{chen2024spatialvlm} for images that the indoor-finetuned model performs poorly on (pictures with overlays, no visible groundplane, large outdoor scenes, optical illusions are removed).

\section{Classification and Retrieval Experiment Details}
\label{app:classif_and_retrieval}

Our experimental base model is ViT-B/16 \citep{dosovitskiy2020vit}, which has 12 layers, 768 model dimension, 3072 MLP size, 12 attention heads, and 16$\times$16 patch size.  All RoPE and STRING variants retain these same model hyperparameters.  For vanilla RoPE, we use the common 10,000 max wavelength and simply split query/key dimensions between 2 or 3 axes in the 2D or 3D case, respectively.  For RoPE-Mixed, we initialize with 100 max wavelength as suggested in \citet{heo2025rotary}, which we also adopt for STRING.

For other hyperparameters like learning rate, batch size, warm-up schedule, etc., we left these the same as the default values used for the base ViT-B/16 model.  We use the Adam optimizer with b1=0.9 and b2=0.999 for all experiments.  For STRING, we experimented with sharing parameters across attention heads rather than the default of learning them all separately, and we found this could yield slight gains in both efficiency and quality.  Finally, for Circulant-STRING, we swept over block sizes in the set \{4, 8, 16, 32, 64\} to find the optimal setting (often around 16).

\subsection{ImageNet2012 and Places365 Classification}
\label{app:classification}

All experiments were trained from scratch, separately for ImageNet2012 or Places365.  In both cases, we used 224$\times$224 image resolution, batch size 4096, and trained for 300 epochs.  Training used the cosine decay learning rate schedule with 0.001 base learning rate and 10,000 warm-up steps.  For ImageNet2012 there were a total of about 94k training steps, and for Places365 there were about 130k.  For Circulant-STRING, block size 16 yielded the best results.

\subsection{WebLI-3D Retrieval}
\label{app:retrieval}

All experiments were trained from scratch and used 256$\times$256 image resolution.  The ViT baseline used RGB data, but all RoPE and STRING variants used RGB-D, where we incorporated depth as a third coordinate for 3D position representations.  We trained with batch size 8192 for 20 epochs, amounting to about 155k training steps using the SigLIP \citep{zhai2023sigmoid} pretraining setup.  Training used the cosine decay learning rate schedule with 0.001 base learning rate and 5\% warm-up steps (about 8k).  For Circulant-STRING, block size 32 yielded the best results.

\section{3D Detection Details}
\label{app:owlvit3d}

\subsection{Dataset}
\label{app:owlvit3d_sim_data}

\begin{figure*}
\centering
\newlength\ImageHeight
\ImageHeight=31mm
\newlength\SubfigWidth
\SubfigWidth=0.49\textwidth

\begin{subfigure}[b]{\SubfigWidth}
    \centering
    \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_living_room_single_view_ex0.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_living_room_single_view_ex1.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_living_room_single_view_ex2.png}
    \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_living_room_single_view_ex3.png}
    \caption{Living room scenes}
    \label{fig:lirasim_living_room_single_view}
\end{subfigure}
\begin{subfigure}[b]{\SubfigWidth}
    \centering
    \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_tabletop_single_view_ex0.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_tabletop_single_view_ex2.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_tabletop_single_view_ex1.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_tabletop_single_view_ex3.png}
    \caption{Tabletop scenes with clutter}
    \label{fig:lirasim_tabletop_single_view}
\end{subfigure}

\begin{subfigure}[b]{\SubfigWidth}
    \centering
    \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_multi_tabletop_single_view_ex0.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_multi_tabletop_single_view_ex1.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_multi_tabletop_single_view_ex2.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_multi_tabletop_single_view_ex3.png}
    \caption{Multiple cluttered tabletop scenes}
    \label{fig:lirasim_multi_tabletop_single_view}
\end{subfigure}
\begin{subfigure}[b]{\SubfigWidth}
    \centering
    \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_tabletop_trays_single_view_ex0.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_tabletop_trays_single_view_ex1.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_tabletop_trays_single_view_ex2.png} \includegraphics[height=\ImageHeight]{figures/lirasim/lirasim_tabletop_trays_single_view_ex3.png}
    \caption{Objects on flat objects scenes}
    \label{fig:lirasim_tabletop_trays_single_view}
\end{subfigure}
\captionof{figure}{Example images drawn at random from our dataset used for 3D detection. Each subfigure shows 4 images from each of the 4 generated subsets of the dataset. The green boxes show the ground truth object bounding boxes.}
\label{fig:proc_3dbbx_examples}
\end{figure*}

We train our synthetic datasets of indoor living room and cluttered tabletop scenes using a procedural generation recipe. We use open-sourced 3D assets, specifically a subset of assets from the Amazon Berkeley Objects~\citep{collins2022abo} (ABO) dataset for background and tabletop objects and the YCB~\citep{calli2015benchmarking} and Google Scanned Objects~\citep{downs2022google} for tabletop clutter placement. The procedural generation recipe works by hierarchically generating sub-areas such as a lounge, dining, office and reading area within a randomly-sized rectangular room together with freestanding pieces of furniture, sampled from the different classes within the ABO assets. For scenes with tabletop clutter, we additionally randomly sample assets on top of existing placement surfaces (e.g. tables) in the scene, and procedurally vary the packing fraction on top of the placement surface to achieve randomised clutter arrangements. We additionally vary lighting, background colors, camera extrinsics, intrinsics, aspect ratios etc to generate diverse datasets. Each example within a dataset has accompanied metadata such as RGB, Depth, Segmentation masks, Object poses, Camera extrinsics, intrinsics, and 3D \& 2D bounding boxes for each object in view. We render images using Unity~\citep{unity} achieving a high-degree of photo-realism.

We generated four different 1-million image datasets using the procedure above: a) living room scenes without tabletop clutter (Fig. \ref{fig:lirasim_living_room_single_view}), b) tabletop scenes with procedurally varying clutter, placed within randomly created living room scenes (Fig. \ref{fig:lirasim_tabletop_single_view}), c) tabletop scenes with multiple cluttered tabletops (Fig. \ref{fig:lirasim_multi_tabletop_single_view}), and d) tabletop scenes with more complicated object arrangements, primarily objects placed on top of trays and other flat objects within the scene (Fig. \ref{fig:lirasim_tabletop_trays_single_view}). We held out the first 20 images for each of these datasets for evaluation, and used the rest for training. Example images from our datasets showing the images and corresponding 3D bounding box labels can be seen in Fig. \ref{fig:proc_3dbbx_examples}.


\subsection{Implementation}

We base our implementation for 3D bounding box detection described in \cref{sec:owlvit3d} on OWL-ViT \citep{minderer2022simple}.
OWL-ViT predicts 2D, axis-aligned bounding boxes for the queried object names in the image.
We modify this to predict full 3D bounding boxes.
We utilized the vision and text towers from the SigLIP task in \cref{sec:general_experiments}, and add the box and class prediction heads on top for the 3D bounding box task.
The main differences with the original OWL-ViT are described in the sections below.

\subsubsection{Box Format}
\label{app:owlvit3d_box_format}

The output of the box head in OWL-ViT is the relative offset from the corresponding image patch for the 4 edges (top, bottom, left, and right) of the axis-aligned, 2D bounding box.
In 3D, image patches do not align with predictions quite as well as in 2D, so instead, the box head predicts the absolute pose of the 3D bounding box.
The output format of the box head is
\[ [<translation>, <rotation>, <size>] \]
where $<translation> \; \in \mathbb{R}^3$ is the SE(3) translation of the center of the box, $<rotation> \; \in \mathbb{R}^6$ is the first 2 columns of the SO(3) rotation matrix of the box, and $<size> \; \in \mathbb{R}^3$ is the length of each side of the box.
Thus the output of the model for each predicted box is a 12D vector.
All predictions are relative to the camera frame.

\subsubsection{Loss Function}
\label{app:owlvit3d_loss}

We modify the OWL-ViT loss terms in the following way.
We keep the class loss as-is.
We also keep the L1 loss directly on the 12D bounding box prediction vector.
However, this loss is not necessarily optimal for SO(3) rotations and in future work we would like to look into better rotation losses such as \cite{Hertzberg2011}.
Finally, we completely replace the 2D, axis-aligned intersection-over-union (IOU) loss.
The algorithm for computing the full 3D, non-axis-aligned IOU is non-differentiable, so we instead compute a loss over the 8 corner vertices of the box.
For both the predicted and target boxes, we compute the 3D coordinates of the 8 corners of the boxes, and then we sum the L1 distance between the predicted and target corners (we assume a fixed ordering of the corners).
We found this loss significantly increases overall performance of the learned models.

\subsubsection{3D IOU}

We use a Monte Carlo algorithm to compute the intersection-over-union (IOU) between 2 boxes in 3D space, which we report in our evaluation numbers in \cref{sec:owlvit3d}.
First, we sample 100k 3D points uniformly at random inside the predicted box.
Next, we transform those points to be in the coordinate space of the target box and we compute the ratio of points that are inside the target box.
Finally, we compute IOU as the intersection of the volumes divided by the sum of the volumes minus the intersection:
\[IOU(box_p, box_t) = \frac{r * vol(box_p)}{vol(box_t) + (1 - r)*vol(box_p)} \]
where $r$ is the fraction of sampled points inside the target box and $vol$ is the volume of the given box.

\subsubsection{Bipartite Matcher}
\label{app:owlvit3d_matcher}

We found empirically that the Hungarian algorithm \citep{kuhn55} for bipartite matching between predictions and targets during training had the best performance.
Specifically, the Hungarian algorithm was the only bipartite matching algorithm we tested that was able to correctly detect objects far from the center of the image.
We suspect that, due to the absolute bounding box predictions described above in \cref{app:owlvit3d_box_format}, the box head was biased towards objects in the center of the camera frame, and the Hungarian algorithm encouraged it to predict boxes for further away objects.
However, we also found the Hungarian algorithm to produce significant volatility in the performance of the model, with models sometimes getting stuck in local minima with poor performance during training (see \cref{tab:owlvit3d_full}).
To counteract this, for every entry in \cref{tab:owlvit3d} in \cref{sec:owlvit3d}, we trained 3 models with different random seeds.
The values in the table are the max of the 3 models' performance after 250k training iterations.
Note that the parameters for the vision and text towers are loaded from the pre-trained checkpoints, so only the parameters for the prediction heads are randomly initialized.

\subsection{Model and Training Configuration}

The model is composed of 4 parts: the vision tower, the text tower, the class head, and the box head. 
The vision tower encodes the image as a set of tokens, the text tower encodes each input text sequence as a token, the class head predicts the class probabilities for the predictions given the query texts, and the box head outputs the bounding box parameters for each prediction.
For the vision and text towers, we use the same model layout (e.g., number of layers) as the models trained on WebLI-3D in \cref{sec:general_experiments}.
For the class head we use the same layout as described in OWL-ViT \citep{minderer2022simple}.
We modify the box head to be a 3 layer MLP, with GELU \citep{hendrycks2016} non-linearities after the first 2 layers.
Unlike OWL-ViT, which outputs a relative offset for the bounding box, our box head directly outputs the absolute bounding box representation itself, as described above in \cref{app:owlvit3d_box_format}.

We use the same training method and optimizer as described in appendix A1.3 in \citep{minderer2022simple}, replacing the gIoU weight with a weight for our 8 corner vertex loss described above in \cref{app:owlvit3d_loss} but keeping the same values.
To improve training speed, we randomly subsample 12 objects from each image for each iteration.
However, during evaluation we include all objects in the scene when computing the 3D IOU.
We train with a batch size of 1,024 for 250k iterations with an initial learning rate of $1\mathrm{e}{-4}$.




\subsection{Results From All Runs}
\npdecimalsign{.}
\nprounddigits{2}
\begin{table}[h]
\centering
\begin{footnotesize}
\begin{tabular}{@{}l@{}c@{}c@{}c@{}c@{}c@{}}
\toprule
& $\ $ $\ $ Baseline $\ $ $\ $& $\ $ RoPE $\ $& $\ $ RoPE\textrm{-}M $\ $ & $\ $ Circulant\textrm{-}S $\ $ & $\ $ Cayley\textrm{-}S \\
 \midrule
 \multirow{3}{*}{ViT} & \textbf{\numprint{49.770000}} & \numprint{55.770000} & \numprint{2.530000} & \numprint{56.690000} & \textbf{\numprint{58.850000}}\\
& \numprint{49.100000} & \numprint{52.110000} & \textbf{\numprint{57.170000}} & \textbf{\numprint{58.950000}} & \numprint{58.430000}\\
& \numprint{47.850000} & \textbf{\numprint{58.090000}} & \numprint{2.460000} & \numprint{13.120000} & \numprint{57.470000}\\
\midrule
 \multirow{3}{*}{ViTD} & \numprint{65.880000} & \textbf{\numprint{71.210000}} & \numprint{70.780000} & \textbf{\numprint{72.360000}} & \numprint{70.360000}\\
& \numprint{66.490000} & \numprint{69.500000} & \numprint{69.940000} & \numprint{69.860000} & \textbf{\numprint{72.670000}}\\
& \textbf{\numprint{67.600000}} & \numprint{70.310000} & \textbf{\numprint{70.900000}} & \numprint{68.510000} & \numprint{71.910000}\\
\bottomrule
\end{tabular}
\end{footnotesize}
\caption{Average 3D IOU \% over all objects for the 3D bounding box prediction task. For each configuration, 3 models were trained with different random seeds. Baseline indicates no RoPE or STRING. The maximum for each configuration is highlighted in \textbf{bold}. Higher is better.}
\label{tab:owlvit3d_full}
\end{table}
\npnoround

For each configuration of RoPE/STRING and ViT/ViTD, we trained 3 models with different random seeds.
In \cref{tab:owlvit3d} in \cref{sec:owlvit3d} we report the maximum IOU of the 3 for each configuration.
Here, \cref{tab:owlvit3d_full} shows the IOU for every run, with the maximum highlighted in bold.
Note that for 3 runs (2 for ViT+RoPE-M and 1 for ViT+Circulant-STRING), the models fell into a local minima which they never left and thus had inferior performance.
See \cref{app:owlvit3d_matcher} for details on possible causes.

\section{Details of Generative Robotics Policies}
\label{app:rgl_3d}
Figure~\ref{fig:rgl_kuka_arch} shows the network architecture for the generative robotics policies for manipulation tasks. PaliGemma~\cite{beyer2024paligemma} VLM with embedding size $256$ and patch size $16$ is used for image encoding. The policy is trained with Adam optimizer with $1e-4$ learning rate and $1e-4$ weight decay. We use a linear learning rate warm-up for first $10000$ steps of training. The policy is trained for a total of $500000$ steps with batch size $256$.

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figures/rgl-diffusion-3d-architecture.pdf}
\caption{Generative policy architecture for robotic manipulation using Kuka-IIWA arms.}
\label{fig:rgl_kuka_arch}
\end{figure*}
