@INPROCEEDINGS{10023234,
  author={Lin, Guan-Ting and Feng, Chi-Luen and Huang, Wei-Ping and Tseng, Yuan and Lin, Tzu-Han and Li, Chen-An and Lee, Hung-yi and Ward, Nigel G.},
  booktitle={IEEE Spoken Language Technology Workshop}, 
  title={On the Utility of Self-Supervised Models for Prosody-Related Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={1104-1111},
  keywords={Analytical models;Conferences;Self-supervised learning;Data models;Data mining;Task analysis;Pragmatics;Speech Self-Supervised Learning;Representation Learning;Pretrained Models;Prosody;Pragmatics},
  doi={10.1109/SLT54892.2023.10023234}}

@ARTICLE{1561280,
  author={Chen, K. and Hasegawa-Johnson, M. and Cohen, A. and Borys, S. and Sung-Suk Kim and Cole, J. and Jeung-Yoon Choi},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={Prosody dependent speech recognition on radio news corpus of American English}, 
  year={2006},
  volume={14},
  number={1},
  pages={232-245},
  keywords={Speech recognition;Hidden Markov models;Error analysis;Mutual information;Information analysis;Frequency;Cepstral analysis;Humans;Vocabulary;Natural languages;Acoustic model;ANN;duration;HMM;mutual information;pitch;prosody;ToBI;word error rate},
  doi={10.1109/TSA.2005.853208}}

@INPROCEEDINGS{4218240,
  author={Ananthakrishnan, Sankaranarayanan and Narayanan, Shrikanth},

  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={Improved Speech Recognition using Acoustic and Lexical Correlates of Pitch Accent in a N-Best Rescoring Framework}, 
  year={2007},
  volume={4},
  number={},
  pages={IV-873-IV-876},
  keywords={Speech recognition;Automatic speech recognition;Speech analysis;Laboratories;Viterbi algorithm;Acoustical engineering;Error analysis;Natural languages;Feature extraction;Data mining;speech recognition;prosody;re-ranking N-best lists},
  doi={10.1109/ICASSP.2007.367209}}

@ARTICLE{Ananthakrishnan,
 title={Unsupervised adaptation of categorical prosody models for prosody labeling and speech recognition},
  author={Ananthakrishnan, Sankaranarayanan and Narayanan, Shrikanth},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  year={2009},
  volume={17},
  number={1},
  pages={138-149},
  keywords={Labeling;Speech recognition;Automatic speech recognition;Natural languages;Humans;Lattices;Error analysis;Speech enhancement;Rhythm;Stress;Categorical prosody models;lattice enrichment;speech recognition;unsupervised adaptation},
  doi={10.1109/TASL.2008.2005347}}

@inproceedings{Baevski2020wav2vec2A,
  title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  author={Alexei Baevski and Henry Zhou and Abdel-rahman Mohamed and Michael Auli},
    year = {2020},
    isbn = {9781713829546},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
    booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
    articleno = {1044},
    numpages = {12},
    location = {Vancouver, BC, Canada},
    series = {NIPS '20},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf}
}

@article{Hasegawa-Johnson,
author = {Hasegawa-Johnson, Mark and Chen, Ken and Cole, Jennifer and Borys, Sarah and Kim, Sung-Suk and Cohen, Aaron and Zhang, Tong and Choi, Jeung-Yoon and Kim, Heejin and Yoon, Tae-Jin},
year = {2005},
month = {07},
pages = {418-439},
title = {Simultaneous recognition of words and prosody in the Boston University Radio Speech Corpus},
volume = {46},
journal = {Speech Communication},
doi = {10.1016/j.specom.2005.01.009}
}

@inproceedings{Luengo2005AutomaticER,
  title={Automatic emotion recognition using prosodic parameters},
  author={Iker Luengo and Eva Navas and Inma Hern{\'a}ez and Jon S{\'a}nchez},
  booktitle={Interspeech},
  year={2005},
  url={https://api.semanticscholar.org/CorpusID:12749548}
}

@inproceedings{Mukhtar,
title = "Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations",
abstract = "Self-supervised speech representations can hugely benefit downstream speech technologies, yet the properties that make the museful are still poorly understood. Two candidate properties related to the geometry of the representation space have been hypothesized to correlate well with downstream tasks: (1) the degree of orthogonality between the subspaces spanned by the speaker centroids and phone centroids, and (2) the isotropy of the space, i.e., the degree to which all dimensions are effectively utilized. To study them, we introduce a new measure, Cumulative Residual Variance (CRV), which can be used to assess both properties. Using linear classifiers for speaker and phone ID to probe the representations of six different self-supervised models and two untrained baselines, we ask whether either orthogonality or isotropy correlate with linear probing accuracy. We find that both measures correlate with phonetic probing accuracy, though our results on isotropy are more nuanced.",
keywords = "model analysis, representational geometry",
author = "Mukhtar Mohamed and Liu, {Oli Danyi} and Hao Tang and Sharon Goldwater",
year = "2024",
month = jun,
day = "6",
language = "English",
booktitle = "Interspeech 2024",
publisher = "ISCA",
note = "The 25th Interspeech Conference, Interspeech 2024 ; Conference date: 01-09-2024 Through 05-09-2024",
url = "https://interspeech2024.org/"}

@InProceedings{Shriberg,
author="Shriberg, Elizabeth
and Stolcke, Andreas",
editor="Johnson, Mark
and Khudanpur, Sanjeev P.
and Ostendorf, Mari
and Rosenfeld, Roni",
title="Prosody Modeling for Automatic Speech Recognition and Understanding",
booktitle="Mathematical Foundations of Speech and Language Processing",
year="2004",
publisher="Springer New York",
address="New York, NY",
pages="105--114",
abstract="This paper summarizes statistical modeling approaches for the use of prosody (the rhythm and melody of speech) in automatic recognition and understanding of speech. We outline effective prosodic feature extraction, model architectures, and techniques to combine prosodic with lexical (word-based) information. We then survey a number of applications of the framework, and give results for automatic sentence segmentation and disfluency detection, topic segmentation, dialog act labeling, and word recognition.",
isbn="978-1-4419-9017-4",
doi = "10.1007/978-1-4419-9017-4_5"
}

@inproceedings{Tran2019OnTR,
  title={On the Role of Style in Parsing Speech with Neural Models},
  author={Trang Tran and Jiahong Yuan and Yang Liu and Mari Ostendorf},
  booktitle={Interspeech 2019},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:202723730}
}

@inproceedings{chen2003prosody,
  title     = {Prosody dependent speech recognition with explicit duration modelling at intonational phrase boundaries},
  author    = {Chen, Ken and Borys, Sarah and Hasegawa-Johnson, Mark and Cole, Jennifer},
  year      = {2003},
  booktitle = {8th European Conference on Speech Communication and Technology},
  pages     = {393--396},
  doi       = {10.21437/Eurospeech.2003-153},
  issn      = {1018-4074},
}

@article{chen2022wavlm,
  title={Wavlm: Large-scale self-supervised pre-training for full stack speech processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={16},
  number={6},
  pages={1505--1518},
  year={2022},
  doi={10.1109/JSTSP.2022.3188113},
  publisher={IEEE}
}

@inproceedings{deseyssel22_interspeech,
  title     = {Probing phoneme, language and speaker information in unsupervised speech representations},
  author    = {Maureen {de Seyssel} and Marvin Lavechin and Yossi Adi and Emmanuel Dupoux and Guillaume Wisniewski},
  year      = {2022},
  booktitle = {Interspeech 2022},
  pages     = {1402--1406},
  doi       = {10.21437/Interspeech.2022-373},
  issn      = {2958-1796},
}

@inproceedings{guinaudeau11_interspeech,
  title     = {Accounting for prosodic information to improve ASR-based topic tracking for TV broadcast news},
  author    = {Camille Guinaudeau and Julia Hirschberg},
  year      = {2011},
  booktitle = {Interspeech 2011},
  pages     = {1401--1404},
  doi       = {10.21437/Interspeech.2011-459},
  issn      = {2958-1796},
}

@article{hsu2021hubert,
author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
title = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3122291},
doi = {10.1109/TASLP.2021.3122291},
abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.<xref ref-type="fn" rid="fn1"><sup>1</sup></xref><xref ref-type="fn" rid="fn2"><sup>2</sup></xref>},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {3451–3460},
numpages = {10}
}

@inproceedings{huang10_speechprosody,
  title     = {Prosody-dependent acoustic modeling using variable-parameter hidden Markov models},
  author    = {Jui-Ting Huang and Po-Sen Huang and Yoonsook Mo and Mark Hasegawa-Johnson and Jennifer Cole},
  year      = {2010},
  booktitle = {Speech Prosody 2010},
  pages     = {paper 623},
  doi       = {10.21437/SpeechProsody.2010-101},
  issn      = {2333-2042},
}

@article{naderi2023cross,
title = {Cross Corpus Speech Emotion Recognition using transfer learning and attention-based fusion of Wav2Vec2 and prosody features},
journal = {Knowledge-Based Systems},
volume = {277},
pages = {110814},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110814},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005646},
author = {Navid Naderi and Babak Nasersharif},
keywords = {Cross-corpus speech emotion recognition, Transfer learning, Domain adaptation, Attention, Feature fusion, Wav2Vec2},
abstract = {Speech Emotion Recognition (SER) performance degrades when their training and test conditions or corpora differ. Cross-corpus SER (CCSER) is a research branch that discusses adapting an SER system to identify speech emotions on a corpus that has different recording conditions or language from the training corpus. For CCSER, adaption can be performed in the feature extraction module or emotion classifier, which are the two main components of the SER system. In this paper, we propose AFTL method (attention-based feature fusion along with transfer learning), including methods in both feature extraction and classification for CCSER. In the feature extraction part, we use Wav2Vec 2.0 transformer blocks and prosody features, and we propose an attention method for fusing them. In the classifier part, we use transfer learning for transferring the knowledge of a model trained on source emotional speech corpus to recognize emotions on a target corpus. We performed experiments on numerous speech emotional datasets as target corpora, where we used IEMOCAP as the source corpus. For instance, we achieve 92.45% accuracy on the EmoDB dataset, where we only use 20% of speakers for adapting the source model. In addition, for other target corpora, we obtained admissible results.}
}

@inproceedings{oli,
title = "Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces",
abstract = "Self-supervised speech representations are known to encode both speaker and phonetic information, but how they are distributed in the high-dimensional space remains largely unexplored. We hypothesize that they are encoded in orthogonal subspaces, a property that lends itself to simple disentanglement. Applying principal component analysis to representations of two predictive coding models, we identify two subspaces that capture speaker and phonetic variances, and confirm that they are nearly orthogonal. Based on this property, we propose a new speaker normalization method which collapses the subspace that encodes speaker information, without requiring transcriptions. Probing experiments show that our method effectively eliminates speaker information and outperforms a previous baseline in phone discrimination tasks. Moreover, the approach generalizes and can be used to remove information of unseen speakers.",
keywords = "self-supervised learning, unsupervised speech processing, speaker normalization",
author = "Liu, {Oli Danyi} and Hao Tang and Sharon Goldwater",
year = "2023",
month = aug,
day = "20",
doi = "10.21437/Interspeech.2023-871",
language = "English",
publisher = "International Speech Communication Association",
pages = "2968--2972",
booktitle = "Interspeech 2023",
url = "https://www.interspeech2023.org/",
}

@inproceedings{rajaa23_interspeech,
  title     = {Improving End-to-End SLU performance with Prosodic Attention and Distillation},
  author    = {Shangeth Rajaa},
  year      = {2023},
  booktitle = {Interspeech 2023},
  pages     = {1114--1118},
  doi       = {10.21437/Interspeech.2023-1760},
  issn      = {2958-1796},
}

@inproceedings{tran-etal-2018-parsing,
    title = "Parsing Speech: a Neural Approach to Integrating Lexical and Acoustic-Prosodic Information",
    author = "Tran, Trang  and
      Toshniwal, Shubham  and
      Bansal, Mohit  and
      Gimpel, Kevin  and
      Livescu, Karen  and
      Ostendorf, Mari",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1007",
    doi = "10.18653/v1/N18-1007",
    pages = "69--81",
    abstract = "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline. For this study with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.",
}

@misc{wei2022neuralprosodyencoderendroend,
      title={A neural prosody encoder for end-ro-end dialogue act classification}, 
      author={Kai Wei and Dillon Knox and Martin Radfar and Thanh Tran and Markus Muller and Grant P. Strimel and Nathan Susanj and Athanasios Mouchtaris and Maurizio Omologo},
      year={2022},
      eprint={2205.05590},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.05590}, 
}

