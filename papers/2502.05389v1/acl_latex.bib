@inproceedings{wolf-etal-2023-quantifying,
    title = "Quantifying the redundancy between prosody and text",
    author = "Wolf, Lukas  and
      Pimentel, Tiago  and
      Fedorenko, Evelina  and
      Cotterell, Ryan  and
      Warstadt, Alex  and
      Wilcox, Ethan  and
      Regev, Tamar",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.606/",
    doi = "10.18653/v1/2023.emnlp-main.606",
    pages = "9765--9784",
    abstract = "Prosody{---}the suprasegmental component of speech, including pitch, loudness, and tempo{---}carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves. Using a large spoken corpus of English audiobooks, we extract prosodic features aligned to individual words and test how well they can be predicted from LLM embeddings, compared to non-contextual word embeddings. We find a high degree of redundancy between the information carried by the words and prosodic information across several prosodic features, including intensity, duration, pauses, and pitch contours. Furthermore, a word`s prosodic information is redundant with both the word itself and the context preceding as well as following it. Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words. Along with this paper, we release a general-purpose data processing pipeline for quantifying the relationship between linguistic information and extra-linguistic features."
}

@article{VICSI2010413,
title = {Using prosody to improve automatic speech recognition},
journal = {Speech Communication},
volume = {52},
number = {5},
pages = {413-426},
year = {2010},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2010.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167639310000129},
author = {Klára Vicsi and György Szaszák},
keywords = {Speech recognition, Prosody, Syntactic unit, Sentence modality, Hidden Markov models},
abstract = {In this paper acoustic processing and modelling of the supra-segmental characteristics of speech is addressed, with the aim of incorporating advanced syntactic and semantic level processing of spoken language for speech recognition/understanding tasks. The proposed modelling approach is very similar to the one used in standard speech recognition, where basic HMM units (the most often acoustic phoneme models) are trained and are then connected according to the dictionary and some grammar (language model) to obtain a recognition network, along which recognition can be interpreted also as an alignment process. In this paper the HMM framework is used to model speech prosody, and to perform initial syntactic and/or semantic level processing of the input speech in parallel to standard speech recognition. As acoustic–prosodic features, fundamental frequency and energy are used. A method was implemented for syntactic level information extraction from the speech. The method was designed to work for fixed-stress languages, and it yields a segmentation of the input speech for syntactically linked word groups, or even single words corresponding to a syntactic unit (these word groups are sometimes referred to as phonological phrases in psycholinguistics, which can consist of one or more words). These so-called word-stress units are marked by prosody, and have an associated fundamental frequency and/or energy contour which allows their discovery. For this, HMMs for the different types of word-stress unit contours were trained and then used for recognition and alignment of such units from the input speech. This prosodic segmentation of the input speech also allows word-boundary recovery and can be used for N-best lattice rescoring based on prosodic information. The syntactic level input speech segmentation algorithm was evaluated for the Hungarian and for the Finnish languages that have fixed stress on the first syllable. (This means if a word is stressed, stress is realized on the first syllable of the word.) The N-best rescoring based on syntactic level word-stress unit alignment was shown to augment the number of correctly recognized words. For further syntactic and semantic level processing of the input speech in ASR, clause and sentence boundary detection and modality (sentence type) recognition was implemented. Again, the classification was carried out by HMMs, which model the prosodic contour for each clause and/or sentence modality type. Clause (and hence also sentence) boundary detection was based on HMM’s excellent capacity in aligning dynamically the reference prosodic structure to the utterance coming from the ASR input. This method also allows punctuation to be automatically marked. This semantic level processing of speech was investigated for the Hungarian and for the German languages. The correctness of recognized types of modalities was 69% for Hungarian, and 78% for German.}
}

@InProceedings{Shriberg,
author="Shriberg, Elizabeth
and Stolcke, Andreas",
editor="Johnson, Mark
and Khudanpur, Sanjeev P.
and Ostendorf, Mari
and Rosenfeld, Roni",
title="Prosody Modeling for Automatic Speech Recognition and Understanding",
booktitle="Mathematical Foundations of Speech and Language Processing",
year="2004",
publisher="Springer New York",
address="New York, NY",
pages="105--114",
abstract="This paper summarizes statistical modeling approaches for the use of prosody (the rhythm and melody of speech) in automatic recognition and understanding of speech. We outline effective prosodic feature extraction, model architectures, and techniques to combine prosodic with lexical (word-based) information. We then survey a number of applications of the framework, and give results for automatic sentence segmentation and disfluency detection, topic segmentation, dialog act labeling, and word recognition.",
isbn="978-1-4419-9017-4",
doi = "10.1007/978-1-4419-9017-4_5"
}


@article{Bhardwaj,
author = {Bhardwaj, Vivek and Gera, Tanya and Thakur, Deepak and Singh, Amitoj},
title = {Enhancing Automatic Speech Recognition for Punjabi Dialects: An Experimental Analysis of Incorporating Prosodic Features and Acoustic Variability Mitigation},
year = {2024},
issue_date = {Aug 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {5},
number = {6},
url = {https://doi.org/10.1007/s42979-024-03111-w},
doi = {10.1007/s42979-024-03111-w},
abstract = {The development of Automatic Speech Recognition (ASR) systems has varied significantly across the roughly 6500 languages that make up the world's spoken languages. There is a lack of effective ASR systems due to the unavailability of essential resources including high-quality speech corpora, pronunciation dictionaries, and transcriptions, for the low resource languages including the Punjabi language with its several dialects. With a focus on the Malwai, Doabi, Majhi, and Puadhi dialects, this work initiates an experimental investigation of ASR systems developed exclusively for the Punjabi language. To improve speech recognition rates, the study offers a novel method that combines prosodic variables like pitch and probability of voicing (POV) with Mel-frequency cepstral coefficients (MFCC). The goal of this combination of elements is to accurately reproduce the tonal qualities of the Punjabi language. The work addresses the issue of inter-speaker acoustic variabilities using approaches like speaker adaptive training (SAT) and vocal-tract length normalisation (VTLN). The word error rate (WER), which reduces upto 9.25\% for the different Punjabi dialects, has been shown to be significantly improvements because of the proposed hybridization of prosodic characteristics. This result highlights how this method may improve ASR systems for tonal languages with limited resources. In addition to demonstrating the value of adding prosodic features and minimising inter-speaker variations, the study advances the field by describing a novel paradigm for creating ASR systems for languages with constrained and sustainable use of resources. This study also presents present state of art as well as emerging trends and a few open innovative problems for research community. Future researchers may carefully observe literature gap to apply this strategy to other low-resource languages and dialects, further demonstrating the effectiveness of the suggested approach.},
journal = {SN Comput. Sci.},
month = aug,
numpages = {20},
keywords = {Automatic speech recognition, Pitch, Acoustic, Punjabi dialects}
}



@inproceedings{dominguez2019pytobi,
  author    = {Domínguez, M. and Rohrer, P. and Soler-Company, J.},
  title     = {PyToBI: A Toolkit for ToBI Labeling with Python Data Structures},
  booktitle = {Proceedings of Interspeech},
  year      = {2019},
  location  = {Graz, Austria},
  pages     = {3675--3676}
}
@inproceedings{zhai23_interspeech,
  title     = {Wav2ToBI: a new approach to automatic ToBI transcription},
  author    = {Wanyue Zhai and Mark Hasegawa-Johnson},
  year      = {2023},
  booktitle = {Interspeech 2023},
  pages     = {2748--2752},
  doi       = {10.21437/Interspeech.2023-477},
  issn      = {2958-1796},
}
@article{chen2022wavlm,
  title={Wavlm: Large-scale self-supervised pre-training for full stack speech processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={16},
  number={6},
  pages={1505--1518},
  year={2022},
  doi={10.1109/JSTSP.2022.3188113},
  publisher={IEEE}
}
@inproceedings{Baevski2020wav2vec2A,
  title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  author={Alexei Baevski and Henry Zhou and Abdel-rahman Mohamed and Michael Auli},
    year = {2020},
    isbn = {9781713829546},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
    booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
    articleno = {1044},
    numpages = {12},
    location = {Vancouver, BC, Canada},
    series = {NIPS '20},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf}
}
@article{hsu2021hubert,
author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
title = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3122291},
doi = {10.1109/TASLP.2021.3122291},
abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.<xref ref-type="fn" rid="fn1"><sup>1</sup></xref><xref ref-type="fn" rid="fn2"><sup>2</sup></xref>},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {3451–3460},
numpages = {10}
}
@article{naderi2023cross,
title = {Cross Corpus Speech Emotion Recognition using transfer learning and attention-based fusion of Wav2Vec2 and prosody features},
journal = {Knowledge-Based Systems},
volume = {277},
pages = {110814},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110814},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005646},
author = {Navid Naderi and Babak Nasersharif},
keywords = {Cross-corpus speech emotion recognition, Transfer learning, Domain adaptation, Attention, Feature fusion, Wav2Vec2},
abstract = {Speech Emotion Recognition (SER) performance degrades when their training and test conditions or corpora differ. Cross-corpus SER (CCSER) is a research branch that discusses adapting an SER system to identify speech emotions on a corpus that has different recording conditions or language from the training corpus. For CCSER, adaption can be performed in the feature extraction module or emotion classifier, which are the two main components of the SER system. In this paper, we propose AFTL method (attention-based feature fusion along with transfer learning), including methods in both feature extraction and classification for CCSER. In the feature extraction part, we use Wav2Vec 2.0 transformer blocks and prosody features, and we propose an attention method for fusing them. In the classifier part, we use transfer learning for transferring the knowledge of a model trained on source emotional speech corpus to recognize emotions on a target corpus. We performed experiments on numerous speech emotional datasets as target corpora, where we used IEMOCAP as the source corpus. For instance, we achieve 92.45% accuracy on the EmoDB dataset, where we only use 20% of speakers for adapting the source model. In addition, for other target corpora, we obtained admissible results.}
}
@inproceedings{Luengo2005AutomaticER,
  title={Automatic emotion recognition using prosodic parameters},
  author={Iker Luengo and Eva Navas and Inma Hern{\'a}ez and Jon S{\'a}nchez},
  booktitle={Interspeech},
  year={2005},
  url={https://api.semanticscholar.org/CorpusID:12749548}
}
@inproceedings{rajaa23_interspeech,
  title     = {Improving End-to-End SLU performance with Prosodic Attention and Distillation},
  author    = {Shangeth Rajaa},
  year      = {2023},
  booktitle = {Interspeech 2023},
  pages     = {1114--1118},
  doi       = {10.21437/Interspeech.2023-1760},
  issn      = {2958-1796},
}
@misc{wei2022neuralprosodyencoderendroend,
      title={A neural prosody encoder for end-ro-end dialogue act classification}, 
      author={Kai Wei and Dillon Knox and Martin Radfar and Thanh Tran and Markus Muller and Grant P. Strimel and Nathan Susanj and Athanasios Mouchtaris and Maurizio Omologo},
      year={2022},
      eprint={2205.05590},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.05590}, 
}
@InProceedings{KHN16.518,
 title = "Mining the Spoken {W}ikipedia for Speech Data and Beyond",
    author = {K{\"o}hn, Arne  and
      Stegen, Florian  and
      Baumann, Timo},
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1735/",
    pages = "4644--4647",
    abstract = "We present a corpus of time-aligned spoken data of Wikipedia articles as well as the pipeline that allows to generate such corpora for many languages. There are initiatives to create and sustain spoken Wikipedia versions in many languages and hence the data is freely available, grows over time, and can be used for automatic corpus creation. Our pipeline automatically downloads and aligns this data. The resulting German corpus currently totals 293h of audio, of which we align 71h in full sentences and another 86h of sentences with some missing words. The English corpus consists of 287h, for which we align 27h in full sentence and 157h with some missing words. Results are publically available."
 }
@inproceedings{chen2003prosody,
  title     = {Prosody dependent speech recognition with explicit duration modelling at intonational phrase boundaries},
  author    = {Chen, Ken and Borys, Sarah and Hasegawa-Johnson, Mark and Cole, Jennifer},
  year      = {2003},
  booktitle = {8th European Conference on Speech Communication and Technology},
  pages     = {393--396},
  doi       = {10.21437/Eurospeech.2003-153},
  issn      = {1018-4074},
}
@inproceedings{huang10_speechprosody,
  title     = {Prosody-dependent acoustic modeling using variable-parameter hidden Markov models},
  author    = {Jui-Ting Huang and Po-Sen Huang and Yoonsook Mo and Mark Hasegawa-Johnson and Jennifer Cole},
  year      = {2010},
  booktitle = {Speech Prosody 2010},
  pages     = {paper 623},
  doi       = {10.21437/SpeechProsody.2010-101},
  issn      = {2333-2042},
}
@article{Hasegawa-Johnson,
author = {Hasegawa-Johnson, Mark and Chen, Ken and Cole, Jennifer and Borys, Sarah and Kim, Sung-Suk and Cohen, Aaron and Zhang, Tong and Choi, Jeung-Yoon and Kim, Heejin and Yoon, Tae-Jin},
year = {2005},
month = {07},
pages = {418-439},
title = {Simultaneous recognition of words and prosody in the Boston University Radio Speech Corpus},
volume = {46},
journal = {Speech Communication},
doi = {10.1016/j.specom.2005.01.009}
}

@ARTICLE{1561280,
  author={Chen, K. and Hasegawa-Johnson, M. and Cohen, A. and Borys, S. and Sung-Suk Kim and Cole, J. and Jeung-Yoon Choi},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={Prosody dependent speech recognition on radio news corpus of American English}, 
  year={2006},
  volume={14},
  number={1},
  pages={232-245},
  keywords={Speech recognition;Hidden Markov models;Error analysis;Mutual information;Information analysis;Frequency;Cepstral analysis;Humans;Vocabulary;Natural languages;Acoustic model;ANN;duration;HMM;mutual information;pitch;prosody;ToBI;word error rate},
  doi={10.1109/TSA.2005.853208}}

@INPROCEEDINGS{4218240,
  author={Ananthakrishnan, Sankaranarayanan and Narayanan, Shrikanth},

  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={Improved Speech Recognition using Acoustic and Lexical Correlates of Pitch Accent in a N-Best Rescoring Framework}, 
  year={2007},
  volume={4},
  number={},
  pages={IV-873-IV-876},
  keywords={Speech recognition;Automatic speech recognition;Speech analysis;Laboratories;Viterbi algorithm;Acoustical engineering;Error analysis;Natural languages;Feature extraction;Data mining;speech recognition;prosody;re-ranking N-best lists},
  doi={10.1109/ICASSP.2007.367209}}


@ARTICLE{Ananthakrishnan,
 title={Unsupervised adaptation of categorical prosody models for prosody labeling and speech recognition},
  author={Ananthakrishnan, Sankaranarayanan and Narayanan, Shrikanth},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  year={2009},
  volume={17},
  number={1},
  pages={138-149},
  keywords={Labeling;Speech recognition;Automatic speech recognition;Natural languages;Humans;Lattices;Error analysis;Speech enhancement;Rhythm;Stress;Categorical prosody models;lattice enrichment;speech recognition;unsupervised adaptation},
  doi={10.1109/TASL.2008.2005347}}


@inproceedings{guinaudeau11_interspeech,
  title     = {Accounting for prosodic information to improve ASR-based topic tracking for TV broadcast news},
  author    = {Camille Guinaudeau and Julia Hirschberg},
  year      = {2011},
  booktitle = {Interspeech 2011},
  pages     = {1401--1404},
  doi       = {10.21437/Interspeech.2011-459},
  issn      = {2958-1796},
}

@misc{praat,
    author = "Paul Boersma and David Weenink",
    title = "{P}raat: doing phonetics by computer [{C}omputer program]",
    howpublished = "Version 6.1.38, retrieved 2 January 2021 \url{http://www.praat.org/}",
    year = "2021"
}
@article{parselmouth,
    author = "Yannick Jadoul and Bill Thompson and Bart de Boer",
    title = "Introducing {P}arselmouth: A {P}ython interface to {P}raat",
    journal = "Journal of Phonetics",
    volume = "71",
    pages = "1--15",
    year = "2018",
    doi = "https://doi.org/10.1016/j.wocn.2018.07.001"
}

@article{field2005intelligibility,
 ISSN = {00398322},
 URL = {http://www.jstor.org/stable/3588487},
 abstract = {For some 30 years, intelligibility has been recognized as an appropriate goal for pronunciation instruction, yet remarkably little is known about the factors that make a language learner's speech intelligible. Studies have traced correlations between features of nonnative speech and native speakers' intelligibility judgements. They have tended to regard prosody as a global phenomenon and to view intelligibility as primarily a quality of the speaker. The present article focuses on a single prosodic element, lexical stress, and shifts the focus of study to the listener. It draws on findings in psycholinguistics that have rarely been applied to second language (L2) contexts. Groups of listeners were asked to transcribe recorded material in which the variables of lexical stress and vowel quality were manipulated. Recognizing the extent to which English is employed in international contexts, the study contrasted the effect of the variables on native listeners (NLs) with their effect on nonnative listeners (NNLs). NLs and NNLs were found to respond in remarkably similar ways to the problems posed by stress misallocation. For both groups, the extent to which intelligibility was compromised depended greatly on the direction in which stress was shifted and whether changes in vowel quality were involved.},
 author = {John Field},
 journal = {TESOL Quarterly},
 number = {3},
 pages = {399--423},
 publisher = {[Wiley, Teachers of English to Speakers of Other Languages, Inc. (TESOL)]},
 title = {Intelligibility and the Listener: The Role of Lexical Stress},
 urldate = {2025-02-06},
 volume = {39},
 year = {2005}
}


@article{cutoff,
author = {Colatosti, Adriana and Gil, Ignacio and Morant-Ventura, Antonio and Monteagudo, Emilia and Aranda, Lucía and Marco, Jaime},
year = {2024},
month = {06},
pages = {},
title = {Normal hearing and verbal discrimination in real sounds environments},
journal = {Acta otorrinolaringologica espanola},
doi = {10.1016/j.otoeng.2024.05.005}
}

@inproceedings{Tran2019OnTR,
  title={On the Role of Style in Parsing Speech with Neural Models},
  author={Trang Tran and Jiahong Yuan and Yang Liu and Mari Ostendorf},
  booktitle={Interspeech 2019},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:202723730}
}

@article{keskin2019role,
  title={Role of Prosodic Reading in Listening Comprehension.},
  author={Keskin, H Kagan and Ari, G{\"o}khan and Bastug, Muhammet},
  journal={International Journal of Education and Literacy Studies},
  volume={7},
  number={1},
  pages={59--65},
  year={2019},
  doi = {10.7575/aiac.ijels.v.7n.1p.59},
  publisher={ERIC}
}

@book{Buck_2001, place={Cambridge}, series={Cambridge Language Assessment}, title={Assessing Listening}, publisher={Cambridge University Press}, author={Buck, Gary}, year={2001}, collection={Cambridge Language Assessment}, doi = {
10.1017/CBO9780511732959}} 

@misc{shon2024discreteslulargelanguagemodel,
      title={DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding}, 
      author={Suwon Shon and Kwangyoun Kim and Yi-Te Hsu and Prashant Sridhar and Shinji Watanabe and Karen Livescu},
      year={2024},
      eprint={2406.09345},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.09345}, 
}
@misc{wu2024heysquadspokenquestionanswering,
      title={HeySQuAD: A Spoken Question Answering Dataset}, 
      author={Yijing Wu and SaiKrishna Rallabandi and Ravisutha Srinivasamurthy and Parag Pravin Dakle and Alolika Gon and Preethi Raghavan},
      year={2024},
      eprint={2304.13689},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.13689}, 
}
@inproceedings{talman-etal-2019-predicting,
    title = "Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations",
    author = {Talman, Aarne  and
      Suni, Antti  and
      Celikkanat, Hande  and
      Kakouros, Sofoklis  and
      Tiedemann, J{\"o}rg  and
      Vainio, Martti},
    editor = "Hartmann, Mareike  and
      Plank, Barbara",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = sep # "{--}" # oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://aclanthology.org/W19-6129",
    pages = "281--290"}
@inproceedings{shon-etal-2023-slue,
    title = "{SLUE} Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks",
    author = "Shon, Suwon  and
      Arora, Siddhant  and
      Lin, Chyi-Jiunn  and
      Pasad, Ankita  and
      Wu, Felix  and
      Sharma, Roshan  and
      Wu, Wei-Lun  and
      Lee, Hung-yi  and
      Livescu, Karen  and
      Watanabe, Shinji",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.496",
    doi = "10.18653/v1/2023.acl-long.496",
    pages = "8906--8937",
    abstract = "Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. In order to facilitate the development of SLU models that leverage the success of pre-trained speech representations, we will release a new benchmark suite, including for each task (i) curated annotations for a relatively small fine-tuning set, (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model performance in various types of systems for easy comparisons. We present the details of data collection and annotation and the performance of the baseline models. We also analyze the sensitivity of pipeline models{'} performance to the speech recognition accuracy, using more than 20 publicly availablespeech recognition models.",
}
@inproceedings{chuang20b_interspeech,
  author={Yung-Sung Chuang and Chi-Liang Liu and Hung-yi Lee and Lin-shan Lee},
  title={{SpeechBERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering}},
  year=2020,
  booktitle={Interspeech 2020},
  pages={4168--4172},
  doi={10.21437/Interspeech.2020-1570},
  issn={2958-1796}
}

@inproceedings{lin22c_interspeech,
  author={Guan-Ting Lin and Yung-Sung Chuang and Ho-Lam Chung and Shu-wen Yang and Hsuan-Jui Chen and Shuyan Annie Dong and Shang-Wen Li and Abdelrahman Mohamed and Hung-yi Lee and Lin-shan Lee},
  title={{DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering}},
  year=2022,
  booktitle={Interspeech 2022},
  pages={5165--5169},
  doi={10.21437/Interspeech.2022-612}
}

@inproceedings{spokensquad,
  title     = {Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension},
  author    = {Chia-Hsuan Lee and Szu-Lin Wu and Chi-Liang Liu and Hung-yi Lee},
  year      = {2018},
  booktitle = {Interspeech 2018},
  pages     = {3459--3463},
  doi       = {10.21437/Interspeech.2018-1714},
  issn      = {2958-1796},
}

@inproceedings{unlu-menevse-etal-2022-framework,
    title = "A Framework for Automatic Generation of Spoken Question-Answering Data",
    author = {{\"U}nl{\"u} Menev{\c{s}}e, Merve  and
      Manav, Yusufcan  and
      Arisoy, Ebru  and
      {\"O}zg{\"u}r, Arzucan},
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.342",
    doi = "10.18653/v1/2022.findings-emnlp.342",
    pages = "4659--4666",
    abstract = "This paper describes a framework to automatically generate a spoken question answering (QA) dataset. The framework consists of a question generation (QG) module to generate questions automatically from given text documents, a text-to-speech (TTS) module to convert the text documents into spoken form and an automatic speech recognition (ASR) module to transcribe the spoken content. The final dataset contains question-answer pairs for both the reference text and ASR transcriptions as well as the audio files corresponding to each reference text. For QG and ASR systems we used pre-trained multilingual encoder-decoder transformer models and fine-tuned these models using a limited amount of manually generated QA data and TTS-based speech data, respectively. As a proof of concept, we investigated the proposed framework for Turkish and generated the Turkish Question Answering (TurQuAse) dataset using Wikipedia articles. Manual evaluation of the automatically generated question- answer pairs and QA performance evaluation with state of-the-art models on TurQuAse show that the proposed framework is efficient for automatically generating spoken QA datasets. To the best of our knowledge, TurQuAse is the first publicly available spoken question answering dataset for Turkish. The proposed framework can be easily extended to other languages where a limited amount of QA data is available.",
}
@inproceedings{oli,
title = "Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces",
abstract = "Self-supervised speech representations are known to encode both speaker and phonetic information, but how they are distributed in the high-dimensional space remains largely unexplored. We hypothesize that they are encoded in orthogonal subspaces, a property that lends itself to simple disentanglement. Applying principal component analysis to representations of two predictive coding models, we identify two subspaces that capture speaker and phonetic variances, and confirm that they are nearly orthogonal. Based on this property, we propose a new speaker normalization method which collapses the subspace that encodes speaker information, without requiring transcriptions. Probing experiments show that our method effectively eliminates speaker information and outperforms a previous baseline in phone discrimination tasks. Moreover, the approach generalizes and can be used to remove information of unseen speakers.",
keywords = "self-supervised learning, unsupervised speech processing, speaker normalization",
author = "Liu, {Oli Danyi} and Hao Tang and Sharon Goldwater",
year = "2023",
month = aug,
day = "20",
doi = "10.21437/Interspeech.2023-871",
language = "English",
publisher = "International Speech Communication Association",
pages = "2968--2972",
booktitle = "Interspeech 2023",
url = "https://www.interspeech2023.org/",
}
@inproceedings{deseyssel22_interspeech,
  title     = {Probing phoneme, language and speaker information in unsupervised speech representations},
  author    = {Maureen {de Seyssel} and Marvin Lavechin and Yossi Adi and Emmanuel Dupoux and Guillaume Wisniewski},
  year      = {2022},
  booktitle = {Interspeech 2022},
  pages     = {1402--1406},
  doi       = {10.21437/Interspeech.2022-373},
  issn      = {2958-1796},
}
@inproceedings{Mukhtar,
title = "Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations",
abstract = "Self-supervised speech representations can hugely benefit downstream speech technologies, yet the properties that make the museful are still poorly understood. Two candidate properties related to the geometry of the representation space have been hypothesized to correlate well with downstream tasks: (1) the degree of orthogonality between the subspaces spanned by the speaker centroids and phone centroids, and (2) the isotropy of the space, i.e., the degree to which all dimensions are effectively utilized. To study them, we introduce a new measure, Cumulative Residual Variance (CRV), which can be used to assess both properties. Using linear classifiers for speaker and phone ID to probe the representations of six different self-supervised models and two untrained baselines, we ask whether either orthogonality or isotropy correlate with linear probing accuracy. We find that both measures correlate with phonetic probing accuracy, though our results on isotropy are more nuanced.",
keywords = "model analysis, representational geometry",
author = "Mukhtar Mohamed and Liu, {Oli Danyi} and Hao Tang and Sharon Goldwater",
year = "2024",
month = jun,
day = "6",
language = "English",
booktitle = "Interspeech 2024",
publisher = "ISCA",
note = "The 25th Interspeech Conference, Interspeech 2024 ; Conference date: 01-09-2024 Through 05-09-2024",
url = "https://interspeech2024.org/"}

@article{mehler1988precursor,
  title={A precursor of language acquisition in young infants},
  author={Mehler, Jacques and Jusczyk, Peter and Lambertz, Ghislaine and Halsted, Nilofar and Bertoncini, Josiane and Amiel-Tison, Claudine},
  journal={Cognition},
  volume={29},
  number={2},
  pages={143--178},
  year={1988},
  publisher={Elsevier}
}
@article{0fea2477791549a785cbdd7e0c058ca5,
title = "What makes business speakers sound charismatic? A contrastive acoustic-melodic analysis of Steve Jobs and Mark Zuckerberg",
abstract = "Phonetic research on the prosodic sources of perceived charisma has taken a big step towards making a speaker{\textquoteright}s tone-of-voice a tangible, quantifiable, and trainable matter. However, the tone-of-voice includes a complex bundle of acoustic features, and a lot of parameters have not even been looked at so far. Moreover, all previous studies focused on political or religious leaders and left aside the large field of managers and CEOs in the world of business. These are the two research gaps addressed in the present study. An acoustic analysis of about 1,350 prosodic phrases from keynotes given by a more charismatic CEO (Steve Jobs) and a less charismatic CEO (Mark Zuckerberg) suggests that the same tone-of-voice settings that make political or religious leaders sound more charismatic also work for business speakers. In addition, results point to further charisma-relevant acoustic parameters related to rhythm, emphasis, pausing, and voice quality - as well as to audience type as a significant context factor. The findings are discussed with respect to implications for future perception oriented studies and perspectives for a computer-based measurement, assessment, and training of a charismatic tone of voice.",
keywords = "Persuasion, Charisma, Speech, Acoustic Analysis, Phonetics, Prosody, Steve Jobs, Mark Zuckerberg, English",
author = "Oliver Niebuhr and Alexander Brem and Jan Michalsky and Jana Neitsch",
year = "2020",
month = jul,
day = "10",
doi = "10.25189/2675-4916.2020.V1.N1.ID272",
language = "English",
volume = "1",
journal = "Cadernos de Linguistica e Teoria da Literatura",
issn = "0101-3548",
number = "1",
}
@inproceedings{audibert2023evaluation,
  title     = {Evaluation of delexicalization methods for research on emotional speech},
  author    = {Nicolas Audibert and Francesca Carbone and Maud Champagne-Lavau and Aurélien Said Housseini and Caterina Petrone},
  year      = {2023},
  booktitle = {Interspeech 2023},
  pages     = {2618--2622},
  doi       = {10.21437/Interspeech.2023-1903},
  issn      = {2958-1796},
}
@article{goldman,
author = {Goldman, Jean-Philippe and Pršir, Tea and Christodoulides, George and Simon, Anne-Catherine and Auchlin, Antoine},
year = {2014},
month = {09},
pages = {51-62},
title = {Phonogenre identification: A perceptual experiment with 8 delexicalised speaking styles},
journal = {Nouveaux cahiers de linguistique française},
URL = {http://hdl.handle.net/2078.1/144534}
}
@inproceedings{chan24_speechprosody,
  title     = {Exploring the accuracy of prosodic encodings in state-of-the-art text-to-speech models},
  author    = {Cedric Chan and Jianjing Kuang},
  year      = {2024},
  booktitle = {Speech Prosody 2024},
  pages     = {27--31},
  doi       = {10.21437/SpeechProsody.2024-6},
  issn      = {2333-2042},
}
@misc{beltagy2020longformerlongdocumenttransformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      url    = {https://arxiv.org/abs/2004.05150},
  journal={arXiv:2004.05150},
  year={2020},
}


@inproceedings{Clark2019EvaluatingLT,
  title     = {Evaluating Long-form Text-to-Speech: Comparing the Ratings of Sentences and Paragraphs},
  author    = {Rob Clark and Hanna Silen and Tom Kenter and Ralph Leith},
  year      = {2019},
  booktitle = {10th ISCA Workshop on Speech Synthesis},
  pages     = {99--104},
  doi       = {10.21437/SSW.2019-18},
}



@misc{SB2021,
  title={{SpeechBrain}: A General-Purpose Speech Toolkit},
  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},
  year={2021},
  eprint={2106.04624},
  archivePrefix={arXiv},
  primaryClass={eess.AS}
}

@INPROCEEDINGS{10023234,
  author={Lin, Guan-Ting and Feng, Chi-Luen and Huang, Wei-Ping and Tseng, Yuan and Lin, Tzu-Han and Li, Chen-An and Lee, Hung-yi and Ward, Nigel G.},
  booktitle={IEEE Spoken Language Technology Workshop}, 
  title={On the Utility of Self-Supervised Models for Prosody-Related Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={1104-1111},
  keywords={Analytical models;Conferences;Self-supervised learning;Data models;Data mining;Task analysis;Pragmatics;Speech Self-Supervised Learning;Representation Learning;Pretrained Models;Prosody;Pragmatics},
  doi={10.1109/SLT54892.2023.10023234}}
@inproceedings{Sicherman_2023,
   title={Analysing Discrete Self Supervised Speech Representation For Spoken Language Modeling},
   volume={9},
   url={http://dx.doi.org/10.1109/ICASSP49357.2023.10097097},
   DOI={10.1109/icassp49357.2023.10097097},
   booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing},
   publisher={IEEE},
   author={Sicherman, Amitay and Adi, Yossi},
   year={2023},
   month=jun, pages={1–5} }

@article{winters2004perception,
  title={Perception and comprehension of synthetic speech},
  author={Winters, Stephen J and Pisoni, David B},
  journal={Research on spoken language processing report},
  volume={26},
  pages={95--138},
  year={2004},
  publisher={Citeseer},
  URL= {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8e10a4c4d279e9540cd5af5aae692fe9907409ff}
}

@inproceedings{cho22b_interspeech,
  author={Yeonjin Cho and Sara Ng and Trang Tran and Mari Ostendorf},
  title={{Leveraging Prosody for Punctuation Prediction of Spontaneous Speech}},
  year=2022,
  booktitle={Interspeech 2022},
  pages={555--559},
  doi={10.21437/Interspeech.2022-11061},
  issn={2308-457X}
}
@inproceedings{ekstedt-skantze-2022-much,
    title = "How Much Does Prosody Help Turn-taking? Investigations using Voice Activity Projection Models",
    author = "Ekstedt, Erik  and
      Skantze, Gabriel",
    booktitle = "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2022",
    address = "Edinburgh, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sigdial-1.51",
    pages = "541--551",
}
@inproceedings{wester16_speechprosody,
  author={Mirjam Wester and Oliver Watts and Gustav Eje Henter},
  title={{Evaluating comprehension of natural and synthetic conversational speech}},
  year=2016,
  booktitle={Speech Prosody 2016},
  pages={766--770},
  doi={10.21437/SpeechProsody.2016-157}
}

@inproceedings{tran-etal-2018-parsing,
    title = "Parsing Speech: a Neural Approach to Integrating Lexical and Acoustic-Prosodic Information",
    author = "Tran, Trang  and
      Toshniwal, Shubham  and
      Bansal, Mohit  and
      Gimpel, Kevin  and
      Livescu, Karen  and
      Ostendorf, Mari",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1007",
    doi = "10.18653/v1/N18-1007",
    pages = "69--81",
    abstract = "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline. For this study with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.",
}
@INPROCEEDINGS{9688093,
  author={Pasad, Ankita and Chou, Ju-Chieh and Livescu, Karen},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, 
  title={Layer-Wise Analysis of a Self-Supervised Speech Representation Model}, 
  year={2021},
  volume={},
  number={},
  pages={914-921},
  keywords={Training;Representation learning;Analytical models;Protocols;Semantics;Linguistics;Acoustics;Self-supervised pre-training;representation analysis;speech representation learning},
  doi={10.1109/ASRU51503.2021.9688093}}

@misc{quamer2024disentangling,
  title={Disentangling segmental and prosodic factors to non-native speech comprehensibility},
  author={Quamer, Waris and Gutierrez-Osuna, Ricardo},
  year={2024},
url={https://arxiv.org/abs/2408.10997}, 
}
@inproceedings{skerry2018towards,
  title={Towards end-to-end prosody transfer for expressive speech synthesis with tacotron},
  author={Skerry-Ryan, RJ and Battenberg, Eric and Xiao, Ying and Wang, Yuxuan and Stanton, Daisy and Shor, Joel and Weiss, Ron and Clark, Rob and Saurous, Rif A},
  booktitle={international conference on machine learning},
  pages={4693--4702},
  year={2018},
  organization={PMLR},
url          = {http://arxiv.org/abs/1803.09047},
}
