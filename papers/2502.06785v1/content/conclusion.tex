\section{Conclusion}
\label{sec:conclusion}

% We have introduced DeepCrossAttention (DCA), a novel transformer architecture that provides significant benefits in terms of parameter-effectiveness, training-efficiency, and model-efficiency for a minimal increase in model parameters. DCA builds on three generalizations of the standard residual network that produce a learned linear combination of the layer outputs. DCA allows for richer interactions between layers at different depths because it incorporates depth-wise cross-attention. With DCA we observe significant improvements in model stability and convergence. 

This paper introduces DeepCrossAttention (DCA), a novel transformer architecture that enhances the flow of information across layers. It achieves lower perplexity for a given parameter budget and training time for a minimal increase in model parameters.
DCA enables dynamic interactions between layer outputs by building on three generalizations of the standard residual network (GRN). We showed theoretically that GRN obtains a better test error-model complexity trade-off.
In our DCA experiments we observe significant improvements in model stability, convergence, and quality. 