\section{Experiments}
\label{sec:experiments}

% We conduct experiments on language and vision modeling tasks to evaluate the effectiveness of DCA. The performance of DCA is compared against the standard transformer \citep{vaswani2017attention} and the vision transformer (ViT) \citep{dosovitskiy2020image} architectures in language and vision modeling tasks, respectively.  
% We additionally compare DCA against the recent related works LAuReL \citep{menghani2024laurel}, DenseFormer \citep{pagliardini2024denseformer}, and Hyper-Connections \citep{zhu2024hyper}.
% We compare the methods based on model size, training time, and perplexity (PPL). 

% \subsection{Language modeling}
% \label{sec:language_modeling}

We conduct experiments on language modeling tasks to evaluate the effectiveness of DCA and to validate our theoretical insights. The performance of DCA is compared against the standard transformer \citep{vaswani2017attention} on the LM1B \citep{chelba2013one} and C4 \citep{2020t5} datasets. Unless stated otherwise, each model has an embedding dimension of 512 and an MLP dimension of four times the embedding dimension. By default, DCA uses a stack of all the previous layer outputs as input to the GRNs. When DCA includes only the first and last-$k$ layer outputs explicitly in the input stack (see Section~\ref{sec:grn}), then this is denoted as $k$-DCA. 

Each model is trained with a sequence length of 128 and a batch size of 2048 over 64 TPUs for 500k steps, totaling 131B tokens. We use the AdamW optimizer \citep{loshchilov2017decoupled} with $\beta_1 = 0.9$, $\beta_2 = 0.98$, a weight decay of $0.1$, and a learning rate of $0.0016$ with 1000 warmup steps and an inverse square root schedule \citep{raffel2020exploring}.

% \begin{table}[h]
%     \caption{Language modeling dataset information}
%     \label{tab:lm-datasets}
%     \vskip 0.15in
%     \begin{center}
%     \begin{small}
%     \begin{sc}
%     \begin{tabular}{l|c}
%         \toprule
%         Dataset & Size \\
%         \midrule
%         LM1B & 4.40 GiB \\
%         C4 & 806.87 GiB \\
%         \bottomrule
%     \end{tabular}
%     \end{sc}
%     \end{small}
%     \end{center}
%     \vskip -0.1in
% \end{table}


% \begin{table}[h]
%     \centering
%     \caption{Image generation dataset information}
%     \label{tab:image-gen-datset}
%     \begin{tabular}{c|c}
%         Dataset & Size \\
%         COCO & 
%     \end{tabular}
% \end{table}

\noindent{\bf Model depth scaling.} 
For the first experiment, we pre-train a transformer and DCA on LM1B. We increase the model depth from 6 to 42 layers and show the relation between perplexity~\citep{jelinek1977perplexity} and model size in Figure~\ref{fig:lm1b_accuracy_param_tradeoff}. The figure shows that DCA obtains a lower perplexity for a given parameter budget. Notably, the 30-layer DCA model obtains a better perplexity than the 42-layer transformer, making DCA more parameter-efficient than adding layers.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/dca_lm1b_accuracy_param_tradeoff.pdf}
    \vskip -0.1in
    \caption{Perplexity on LM1B with 6, 12, 18, 24, 30, 36, and 42 layer transformer and DCA models.}
    \label{fig:lm1b_accuracy_param_tradeoff}
\end{figure}

\noindent{\bf First and last-$k$.} DCA can be made more efficient by including only the first and last-$k$ layer outputs explicitly in the input stack to the GRNs (see Section~\ref{sec:grn}). In this experiment, we study the effect of $k$ on a 24-layer model's efficiency and quality. Table~\ref{tab:first_last_k} shows that reducing $k$ speeds up training while only slightly increasing the perplexity. Either small or large $k$ obtain good training efficiency, as DCA then obtains the final perplexity of the transformer in a third of the time. Setting $k = 2$ results in a model with 48\% lower inference latency compared to $k=24$, thus setting $k$ to be small results in efficient training and fast inference. 

\begin{table}[h]
    \vskip -0.1in
    \caption{Training speed in batches per second, normalized time for a method to reach the perplexity of the transformer, and the final perplexity (PPL) of the transformer and DCA with varying $k$.}
    \label{tab:first_last_k}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|ccc}
        \toprule
        Method & Speed & Time & PPL \\
        \midrule
        Transformer & \textbf{8.30} & 1.00 & 15.126 \\
        1-DCA & 5.51 & 0.36 & 14.478 \\
        2-DCA & 5.45 & \textbf{0.34} & 14.386 \\
        4-DCA & 5.07 & 0.42 & 14.494 \\
        8-DCA & 4.36 & 0.51 & 14.482 \\
        16-DCA & 3.95 & 0.39 & 14.407 \\
        24-DCA & 3.73 &  0.38 & \textbf{14.346} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\noindent{\bf Training time.}
The effectiveness of a model architecture heavily depends on its training efficiency. Figure~\ref{fig:dca-2_perplexity_vs_training_time} shows the training time-perplexity trade-off for 24, 36, and 42 layer transformer and 2-DCA models. The figure shows that 2-DCA achieves better perplexity for a given training time, highlighting the training efficiency of DCA. The training time versus perplexity results when DCA uses all previous layer outputs in the GRNs are provided in Appendix~\ref{app:training-time-vs-perplexity}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/dca-2_lm1b_perplexity_vs_training_time.pdf}
    \vskip -0.1in
    \caption{Perplexity on LM1B versus the training time with transformer and 2-DCA models of various depths.}
    \label{fig:dca-2_perplexity_vs_training_time}
\end{figure}

\noindent{\bf Model width scaling.}
Our theoretical results indicate that the benefit of GRN is inversely related to the rank of the model. With this experiment, we validate whether the theoretical results carry over to the transformer architecture by varying the model width. Table~\ref{tab:width-effect} shows the final perplexity of a 24-layer model with an embedding dimension ranging from 64 till 1024, pre-trained on LM1B. The delta column, with the difference between the transformer and DCA, shows that the benefit of DCA is reduced as the width of the model increases, which is consistent with our theoretical results. These results are in contrast with the depth scaling results, where the improvement of DCA is maintained for deeper models.

\begin{table}[h]
    \vskip -0.1in
    \caption{Perplexity on LM1B for models of varying widths.}
    \label{tab:width-effect}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|ccc}
        \toprule
        Width & Transformer & DCA & Delta  \\
        \midrule
        64 & 41.231 & 38.141 & -3.090 \\
        192 & 22.534 & 21.169 & -1.365 \\
        384 & 16.609 & 15.988 & -0.621 \\
        768 & 13.750 & 12.968 & -0.782 \\
        1024 & 12.630 & 12.399 & -0.231 \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\noindent{\bf Model scaling.} For this experiment, we train transformer and 8-DCA models of increasing size on the C4 dataset. The results in Table~\ref{tab:c4-scaling} show that DCA consistently outperforms the standard transformer model. The absolute improvement in perplexity decreases for large models, which is consistent with the width scaling results. The perplexity throughout training is provided in Appendix~\ref{app:steps-vs-perplexity}.

\begin{table}[h]
    \vskip -0.1in
    \caption{Perplexity on C4 for models of varying depths and widths.}
    \label{tab:c4-scaling}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{cc|cccc}
        \toprule
        D & W & Params & Transf. & 8-DCA & Delta  \\
        \midrule
        9 & 771 & 75M & 27.876 & 26.443 & -1.443 \\
        18 & 771 & 124M & 23.013 & 21.810 & -1.203 \\
        13 & 1111 & 179M & 21.570 & 20.461 & -1.109 \\
        18 & 1111 & 234M & 19.756 & 18.824 & -0.932 \\
        18 & 1600 & 449M & 17.166 & 16.764 & -0.402 \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

% \noindent{\bf Filtering out far-away layers} Show that the GRN learns to filter out the layer outputs from layers far back. This matches our motivation and intuition about the GRN design.


\noindent{\bf Retrofitting pre-trained models.} Since our method is identical to a standard residual network at initialization, adding DCA to a pre-trained model does not alter its function. In Table~\ref{tab:lm1b_6_layer_fine_tuning}, we compare continuing training the pre-trained model with adding DCA to the pre-trained model. Incorporating DCA results in a perplexity improvement of 0.23 after 60k extra training steps, compared to just 0.015 for the transformer. Thus, pre-trained models with a residual architecture can also benefit from incorporating DCA. 

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/lm1b_6_layer_fine_tuning.pdf}
%     \vskip -0.1in
%     \caption{The perplexity for extended pre-training on LM1B with 6-layer models. DCA is added to the 500k steps pre-trained standard transformer model.}
%     \label{fig:lm1b_6_layer_fine_tuning}
% \end{figure}

\begin{table}[h]
    \vskip -0.1in
    \caption{Perplexity on LM1B for extended training of 6-layer models. DCA is added to a 500k steps pre-trained transformer.}
    \label{tab:lm1b_6_layer_fine_tuning}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|ccc}
        \toprule
        Steps & Transformer & DCA & Delta \\
        \midrule
        500k & 18.963 & 18.982 & 0.019 \\
        % 500k + 10k & 18.971 & 18.773 & -0.198  \\
        500k + 20k & 18.978 & 18.803 & -0.175  \\
        % 500k + 30k & 18.955 & 18.794 & -0.161 \\
        500k + 40k & 18.950 & 18.775 & -0.175 \\
        % 500k + 50k & 18.921 & 18.705 & -0.216 \\
        500k + 60k & 18.948 & 18.752 & -0.196 \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\noindent{\bf Training stability.} 
The occurrence of loss spikes is a problem when training large models as they can disrupt an expensive training run \citep{chowdhery2023palm}. In Figures~\ref{fig:dca-2_perplexity_vs_training_time} and \ref{fig:perplexity_vs_training_time}, we indeed observe clear loss spikes with the transformer model. Interestingly, training DCA is more stable, showing no significant loss spikes even for large models. This constitutes an important benefit of DCA.
% An interesting observation from Figure~\ref{fig:perplexity_vs_training_time} is that while there are clear spikes in the loss for the transformer model, no such spikes in the loss are observed with DCA. Actually, we have not observed any loss spike with DCA in any of our experiments.  Discuss the loss spikes phenomenon observed with large standard ResNet models \cite{takase2023spike, nishida2024initialization}. It was for instance a noticeable problem for the PALM model \cite{chowdhery2023palm}. With our method we have not seen any such loss spikes over all our experiments. 

\noindent{\bf Comparison with related work.}
% \label{sec:comparison}
We compare the perplexity of DCA with those obtained by the recent related works LAuReL \citep{menghani2024laurel}, DenseFormer \citep{pagliardini2024denseformer}, and Hyper-Connections \citep{zhu2024hyper} in Table~\ref{tab:comparison}. DCA improves upon the prior best method, hyper-connections, with a difference in perplexity of 0.635, which is the biggest improvement among the methods.

\begin{table}[h]
    \vskip -0.1in
    \caption{Perplexity (PPL) and parameter count on LM1B with a 6-layer model, comparing DCA with related work.}
    \label{tab:comparison}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|cc}
        \toprule
        Method & Params & PPL \\
        \midrule
        Transformer & 49.65M & 18.961 \\
        LAuReL-PA & 49.75M & 18.954 \\
        1x1-DenseFormer & 49.65M & 18.669 \\
        Hyper-Connections (dynamic) & 49.68M & 18.633 \\
        \midrule
        DCA (ours) & 49.73M & \textbf{17.998} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}


\noindent{\bf Ablation study.}
% \label{sec:ablation}
To determine the relative gain of each of the proposed generalizations, in Table~\ref{tab:ablation} we show the perplexity obtained by each method described in Section~\ref{sec:method}. The GRN versions use one GRN instance per decoder block. DCA, in contrast, uses three independent instances of \GenC{} per decoder block. The biggest improvement in perplexity comes from \GenA{}, followed by DCA and \GenB{}.

\begin{table}[h]
    \vskip -0.1in
    \caption{Ablation study of DCA, showing the parameter count and the perplexity (PPL) on LM1B with a 6-layer model.}
    \label{tab:ablation}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|cc}
        \toprule
         Ablation & Params & PPL  \\
        \midrule
         Transformer & 49.65M & 18.961 \\
         \GenA{} & 49.65M & 18.669 \\ 
         \GenB{} & 49.66M & 18.378 \\ 
         \GenC{} & 49.68M & 18.332 \\
         DCA & 49.73M & \textbf{17.998} \\
         \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}





% \subsection{Vision modeling}
% \label{sec:vision_modeling}

% \begin{table}[h]
%     \caption{Image modeling evaluation on MSCOCO with a 12-layer model.}
%     \label{tab:ablation}
%     \vskip 0.15in
%     \begin{center}
%     \begin{small}
%     \begin{sc}
%     \begin{tabular}{l|cc}
%         \toprule
%          Model & Params & FID &   \\
%         \midrule
%          ViT & 49.65M & 10.48 \\
%          DCA & 49.73M & \textbf{5.25} \\
%          \bottomrule
%     \end{tabular}
%     \end{sc}
%     \end{small}
%     \end{center}
%     \vskip -0.1in
% \end{table}