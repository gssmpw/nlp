\section{Related work}
\label{sec:related_work}

Residual connections \citep{he2016deep} enabled the direct flow of information from earlier to later layers. This innovation proved crucial in stabilizing training and allowing for the construction of significantly deeper networks. Building upon this concept, DenseNet \citep{huang2017densely} further enhanced information flow by concatenating the outputs of all preceding layers to each layer's input.

The following methods are the ones most similar to ours. They all build on the idea of DenseNet but apply an efficient aggregation of the previous layer outputs instead of concatenating them. DenseFormer \citep{pagliardini2024denseformer} performs the aggregation as a learned linear combination of the previous layer outputs. To reduce the computational load, they propose to apply their method only on a subset of the possible layer connections. Building on DenseFormer, LAuReL \citep{menghani2024laurel} presents three aggregation functions, the best performing one applies a learned low-rank transformation to the previous layer outputs before the learned linear combination. \citet{zhu2024hyper} take a different approach with Hyper-Connections, they consider a fixed-size stack where layer outputs are added into with a learned weight for every slot of the stack. Before each layer, the stack is mixed by a matrix multiplication with a learned weight matrix. The input to a layer is then obtained by a learned linear combination of the stack, instead of accessing the previous layer outputs directly. They also present a dynamic version of their method where the weights are derived from the inputs.


% NeuTRENO \citep{nguyen2023mitigating},
% MambaMixer \citep{behrouz2024mambamixer},
% \citet{zhou2024value}