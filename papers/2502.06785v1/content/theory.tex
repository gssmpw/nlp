\section{Theoretical analysis}
\label{sec:theory}


Motivated by language modeling tasks, we focus on the regime where the size of the training set $(n)$ significantly exceeds the input dimension ($n\gg d$). As we increase the number of model parameters, the representation capacity of the network improves, which helps with reducing the test error. We will be focusing on the  the trade-off between the test error and the number of parameters, and argue that our proposed generalizations  achieve a better trade-off than the standard ResNet. 
 %In this paper, we assume that both training and test examples are drawn independently and identically distributed (i.i.d.) from a given distribution $\mathcal{P}$ over $\mathcal{X}\times\mathcal{Y}$. We assume both $\mathcal{X}, \mathcal{Y}\subseteq \reals^d$.

We will first study a ``stylized'' low-rank linear model for which we characterize the test error-model complexity trade-off and demonstrate the benefits of our proposed generalizations. Our analysis elucidates the role of various factors on this trade-off, such as collective widths of layers, complexity of the target task, and input dimension. We then discuss how some of these results can be extended to non-linear models and empirically demonstrate that the insights gained from our analysis are applicable to more complex models.

Due to space constraint, proof of theorems are deferred to the supplementary material.
%============================================

\subsection{Low-rank linear model}
Consider the setting where for each sample the response $\y\in\reals^d$ is given by
\[
\y = \A \x+ \epsilon
\]
with $\epsilon\in\reals^d$ representing the noise.  Here $\A\in\reals^{d\times d}$ is a full rank matrix.

We consider a network with $T$ layers where $f_t(\z) = \Vb_t$ (there is no activation). We let $r_t:=\rank(\Vb_t)$ and define the collective rank $r_*:=\sum_{t=1}^T r_t$. We assume $r_*<d$, i.e., the collective rank of all layers still is lower than the ambient dimension $d$.  

%For simplicity assume that $\Vb_t$ are all of rank $r<d$ (low rank matrices). 

We next focus on four architectures: Baseline (where there is no residual connection), ResNet, \GenA{} and \GenB{} and characterize the class of models which can be expressed by each of these architectures. We assume each architecture to have $T$ layers.
%\smallskip

\noindent{\bf Baseline.} In this architecture, there is no residual connection and so the model is given by $\hby = \prod_{t=1}^T \Vb_t \x$. We denote by $\cC_{{\rm base}}$ the class of functions that can be represented by such architecture. 

% Let $r:= \min_{t\in[T]} r_t$. It is easy to see that in this case, $\cC_{{\rm base}} = \{\x\mapsto\M\x: \; \rank(\M)\le r\}$. Note that by the inequality $\rank(\A\B)\le \min\{\rank(\A),\rank(\B)\}$, if $M$ is of the form $\prod_{\ell=1}^T \Vb_t$ then $\rank(\M)\le r$. For the other direction consider any matrix $\M$ with $\rank(\M)=r_0\le r$, and its SVD as $\M= \Pb \Sb\Qb^\sT$ with $\Pb,\Qb\in\reals^{d\times r_0}$ with full column ranks. By setting, $\Vb_1 = \Pb \Sb\Qb^\sT$ and $\Vb_2=\dotsc =\Vb_T = \Qb\Qb^\sT$ we have $\M = \prod_{\ell=1}^T \Vb_\ell$, and $\rank(\Vb_\ell) = r_0\le r \le r_\ell$.

%\smallskip

\noindent{\bf ResNets.} In this case, we have $\hby = \prod_{t=1}^T (\I + \V_t)\x$. Denote by $\cC_{{\rm res}}$ as the class of functions that can be represented by such architecture.
%
% \begin{lemma}
% \label{lemma:resnet-lowrank-expressivity}
% We have $\cC_{{\rm res}} = \{\x \mapsto(\I+\M)\x:\; \rank(\M)\le \sum_{t=1}^T r_t\}$.
% \end{lemma}
%
% \begin{proof}
% First note that 
% \begin{align*}
%     \prod_{\ell=1}^T (\I+\Vb_t) &= \Vb_1 \prod_{\ell=2}^T (\I+\Vb_t) + \prod_{\ell=2}^T (\I+\Vb_t) \\
%     &= \Vb_1 \prod_{\ell=2}^T (\I+\Vb_t) + \Vb_2 \prod_{\ell=3}^T (\I+\Vb_t)+ \prod_{\ell=3}^T (\I+\Vb_t)\\
%     &= \dotsc\\
%     &= \I + \Vb_T+ \sum_{j=1}^{T-1} \Vb_j \prod_{\ell=j+1}^T (\I+\Vb_t) 
% \end{align*}
% Note that each of the summand is of rank at most $r_j$, so it can be written as $\I+\M$ with $\rank(\M)\le \sum_{t=1}^T r_t$. Hence $\prod_{\ell=1}^T (\I+\Vb_\ell)\x\in \cC_{{\rm res}}$.

% We next show that any $\I+\M$ with $\rank(\M):= r\le \sum_{\ell=1}^T r_\ell$ can be written as $\prod_{\ell=1}^T (\I+\Vb_\ell)$ with $\rank(\Vb_\ell)\le r_\ell$ for $\ell\in[T]$. 

% We show this claim by induction. For the basis ($T=1$), 
% we can take $\Vb_1 = \M$. 
% To complete the induction step, we need to find $\Vb\in\reals^{d\times d}$ such that $\rank(\Vb) = r_T$ and $(\I+\V)^{-1} (\I+\M)-\I$ is of rank at most $\sum_{\ell=1}^{T-1}r_\ell$. Then by the induction hypothesis, we can write
% \[
% (\I+\V)^{-1} (\I+\M) = \prod_{\ell=1}^{T-1}(\I+\Vb_\ell)\,,
% \]
% with $\rank(\Vb_\ell) \le r_\ell$, which completes the proof. 
% Without loss of generality, we assume $r_T\le r$; otherwise we can take $\Vb_T = \M$ and $\Vb_\ell = \zero$ for $\ell\le T-1$.

% To find such $\Vb$ we write $\M = \Pb \Qb^{\sT}$ with $\Pb,\Qb\in\reals^{d\times r}$ having full column rank. Define $\Pb_1,\Qb_1\in\reals^{d\times r_T}$ obtaining by considering the first $r_T$ columns of $\Pb$ and $\Qb$. Additionally, define
% \begin{align}\label{def:B-C}
% \B := \Pb_1(\I+\Qb_1^\sT \Pb_1)^{-1},\quad \Cb = \Qb_1(\I+\Pb_1^\sT \Qb_1)\,.
% \end{align}
% We next construct $\Vb$ by setting $\Vb: = \B\Cb^\sT$. Clearly, $\rank(\Vb) = r_T$. We also have
% \begin{align*}
%     (\I+\Vb)^{-1}(\I+\M)-\I &= (\I+\B\Cb^\sT)^{-1}\M + (\I+\B\Cb^\sT)^{-1}-\I\\
%     &= (\I+\B\Cb^\sT)^{-1}\M + \I- \B(\I+\Cb^\sT\B)^{-1}\Cb^\sT-\I\\
%     &= (\I+\B\Cb^\sT)^{-1} (\Pb_1\Qb_1^\sT+ \Pb_{\sim1}\Qb_{\sim1}^\sT)- \B(\I+\Cb^\sT\B)^{-1}\Cb^\sT\,.
% \end{align*}
% Here we consider the notation $\Pb= [\Pb_1 | \Pb_{\sim1}]$ and  $\Qb= [\Qb_1 | \Qb_{\sim1}]$. The second step above follows from the Woodbury matrix identity. Rearranging the terms we have
% \begin{align}\label{eq:induction1}
%   (\I+\Vb)^{-1}(\I+\M) -\I =   (\I+\B\Cb^\sT)^{-1}\Pb_{\sim1}\Qb_{\sim1}^\sT + (\I+\B\Cb^\sT)^{-1}\Pb_{1}\Qb_{1}^\sT - \B(\I+\Cb^\sT\B)^{-1}\Cb^\sT\,.
% \end{align}
% The first term above is of rank at most $\rank(\Pb_{\sim1}) = r- r_T\le \sum_{\ell=1}^{T-1}r_{\ell}$. We next show that the second and the third term cancel each other. Equivalently, we show that
% \begin{align}\label{eq:prod-PQ}
% \Pb_{1}\Qb_{1}^\sT = (\I+\B\Cb^\sT)\B(\I+\Cb^\sT\B)^{-1}\Cb^\sT\,.
% \end{align}
% To do this, we next show that 
% \begin{align}\label{eq:sep-P1-Q1}
% \Pb_1 = (\I+\B\Cb^\sT)\B, \quad \Qb_1^\sT = (\I+\Cb^\sT\B)^{-1}\Cb^\sT\,.
% \end{align}
% Recalling~\eqref{def:B-C} we have $\Pb_1 = \B (\I+\Qb_1^\sT \Pb_1)$. Also 
% \begin{align}
% \Cb^\sT\B &=  (\I +\Qb_1^\sT \Pb_1)\Qb_1^\sT \Pb_1(\I+\Qb_1^\sT \Pb_1)^{-1} \nonumber\\
% &= (\I +\Qb_1^\sT \Pb_1)(\I+\Qb_1^\sT \Pb_1-\I)(\I+\Qb_1^\sT \Pb_1)^{-1}\nonumber\\
% &= (\I +\Qb_1^\sT \Pb_1)(\I-(\I+\Qb_1^\sT \Pb_1)^{-1})\\
% &= \I +\Qb_1^\sT \Pb_1 -\I\nonumber\\
% &= \Qb_1^\sT\Pb_1\label{eq:CB-Q1P1}
% \end{align}
% Therefore, $\Pb_1 = \B(\I+ \Cb^\sT \B) = (\I+\B\Cb^\sT)\B$. Likewise, recalling~\eqref{def:B-C} we have $\Qb_1 = \Cb (\I+ \Pb_1^\sT\Qb_1)^{-1}$. Hence,
% \[
% \Qb_1^\sT = (\I +\Qb_1^\sT \Pb_1)^{-1} \Cb^\sT = (\I +\Cb^\sT\B)^{-1} \Cb^\sT,
% \]
% using~\eqref{eq:CB-Q1P1}. This completes the proof of~\eqref{eq:sep-P1-Q1} and so~\eqref{eq:prod-PQ}.

% Invoking~\eqref{eq:induction1} we get
% \[
% (\I+\Vb)^{-1}(\I+\M) -\I =   (\I+\B\Cb^\sT)^{-1}\Pb_{\sim1}\Qb_{\sim1}^\sT\,,
% \]
% which is of rank at most $r-r_T\le \sum_{\ell=1}^{T-1}r_\ell$, which completes the proof of the induction step.
% \end{proof}

% \bigskip

\noindent{\bf \GenA{}.} In this case, we have $\hby = \Gb_{T+1}\bb_{T+1}$, with $\bb_{T+1}$ a $(T+1)$-dimensional vector as described in Section~\ref{sec:method}. Denote by $\cC_{\rm \GenA{}}$  the class of functions that can be represented by such architecture.
%
% \begin{lemma}\label{lem:C-Gen1}
% We have $\cC_{{\rm Gen1}} = \{\x \mapsto(\alpha\I+\M)\x:\; \rank(\M)\le \sum_{\ell=1}^T r_\ell\}$.
% \end{lemma}
%
% \begin{proof}
% We prove the result by induction. The induction basis ($T=0$) follows readily since $\Gb_1 \bb_1= b_1 \x$. Assume the induction hypothesis for $t$. We have $f_t(g_t(\x)) = \Vb_t \Gb_t \bb_t$ and so $\Gb_{t+1} = [\Vb_t \Gb_t \bb_t\;|\;  \Gb_t]$. Writing $\bb_{t+1} = \begin{bmatrix}b_1\\ \bb_{\sim 1}\end{bmatrix}$ we obtain
% \[
% \Gb_{t+1}\bb_{t+1} = \Vb_t\Gb_t \bb_t b_1 + \Gb_t \bb_{\sim 1}\,.
% \]
% By induction hypothesis, $\Gb_t\bb_{\sim1}$ is the set of functions of the form $(\alpha\I+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^{t-1}r_\ell$. 

% Since $\rank(\Vb_t)\le r_t$ the set of functions that can be represented as $\Gb_{t+1}\bb_{t+1}$ is a subset of $(\alpha\I+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^t r_\ell$. Conversely, any given $M$ of rank $\sum_{\ell=1}^{t}r_\ell$ can be written as $\M = \M_1+\Vb$ with $\rank(\M_1)=\sum_{\ell=1}^{t-1}r_\ell$ and $\rank(\Vb) = r_t$. By induction hypothesis, $(\alpha\I+\M_1)\x$ can be expressed by the term $\Gb_t \bb_{\sim1}$. In addition, $\Vb\x$ can also be expressed by the term $\Vb_t \Gb_t \bb_t b_1$, by taking $\Vb_t = \V$, $\bb_t = (0,0,\dotsc, 1)^\sT$, $b_1=1$, which is possible since they are free from the choice of $\bb_{\sim 1}$.

% % By varying $\Vb_t,\bb_t$ and $b_1$, the term $\Vb_t\Gb_t \bb_t b_1$ covers all functions of the form $\Vb \x$ with $\Vb$ a $d\times d$ matrices of rank $r$ (for given $\V$ of rank $r$, take $\Vb_t = \V$, $\bb_t = (0,0,\dotsc, 1)^\sT$, $b_1=1$, which is possible since they are free from the choice of $\bb_{\sim 1}$). 
% Hence, $\Gb_{t+1}\bb_{t+1}$ the set of functions of the form $(\alpha\I+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^t r_\ell$, completing the induction step.
% \end{proof}
% We assume $\rank(\M)=rT$ (for $M$ of low rank a similar argument applies.)

% Write $\M = \Ub \Vb^\sT$ with $\Ub,\Vb\in\reals^{d\times rT}$, where $\Ub$, $\Vb$ have full column rank. Construct the matrices $\I_k$, for $k\in[T]$ as follows. Each $\I_k$ is $rT \times r$. Divide its rows into blocks of size $r$. For $\I_k$ set the $k$-th block to an identity matrix ($r\times r$) and all other blocks to zero. We next define matrices 
% \begin{align*}
% &\B_k := (\Vb^\sT \Ub)^{-1} \I_k, \quad \Cb_k :=  (\Ub^\sT \Vb)  \I_k,\quad \M_k:= \Ub \B_k \Cb_k^\sT \Vb^\sT\,.
% \end{align*}
% Since $\rank(\B_k) = \rank(\Cb_k) = r$ and $\Ub$, $\Vb$ have full column rank, then $\rank(\M_k) = r$. In addition,
% \[
% \sum_{k\in[T]}\M_k = \Ub(\Vb^\sT \Ub)^{-1} (\sum_{k\in[T]}\I_k \I_k^\sT) (\Vb^\sT \Ub)\Vb^\sT = \Ub\Vb^\sT = \M\,.
% \]
% Also, for $\ell\neq k$ we have
% \[
% \M_\ell \M_k = \Ub \B_\ell \Cb_\ell^\sT \underbrace{\Vb^\sT \Ub \B_k}_{\I_k} \Cb_k^\sT \Vb^\sT
% \]
%\end{proof}
%=====================
% \bigskip

\noindent{\bf \GenB{}.} In this case, we have $\hby = (\Gb_{T+1}\odot\bb_{T+1}) \ones$, where $\bb_{T+1}$ is $d\times (T+1)$ matrix as described in Section~\ref{sec:method}. We denote by $\cC_{{\rm \GenB{}}}$ the class of functions that can be represented by such architecture.
%
\begin{theorem}\label{thm:expressive}
For the low rank linear model we have:
%\begin{itemize}
     $\bullet \quad \cC_{{\rm base}} = \{\x\mapsto\M\x: \; \rank(\M)\le \min(r_t)_{t=1}^T\}$.
     
     $\bullet \quad \cC_{{\rm res}} = \{\x \mapsto(\I+\M)\x:\; \rank(\M)\le r_*\}$.
     
    $\bullet \quad \cC_{{\rm \GenA{}}} = \{\x \mapsto(\alpha\I+\M)\x:\; \rank(\M)\le r_*\}$.
    
    $\bullet \quad \cC_{{\rm \GenB{}}} = \{\x \mapsto(\Db+\M)\x:\; \rank(\M)\le r_*, \Db \text{ is diagonal}\}$.
%\end{itemize}
\end{theorem}
 

% \begin{lemma}\label{lemma:C-Gen2}
% We have $\cC_{{\rm Gen2}} = \{\x\mapsto (\Db+\M)\x:\; \Db \text{ is diagonal },\; \rank(\M)\le \sum_{\ell=1}^t r_\ell\}$.
% \end{lemma}
%
% \begin{proof}
% The proof follows similar to Lemma~\ref{lem:C-Gen1}. For the induction basis ($T=0$), we have $(\Gb_1 \odot\bb_1)\ones = (\x\odot \bb_1)\ones = \diag{\bb_1} \x$. Assume the induction hypothesis for $t$. We have $f_t(g_t(\x)) = \Vb_t (\Gb_t\odot \bb_t)\ones$ and so $\Gb_{t+1} = [\Vb_t (\Gb_t\odot \bb_t)\ones\;|\;  \Gb_t]$. Writing $\bb_{t+1} = \begin{bmatrix}\bb_{t+1}^{(1)}| \bb_{t+1}^{(\sim 1)}\end{bmatrix}$ we obtain
% \[
% \Gb_{t+1}\odot\bb_{t+1} = \diag{\bb_{t+1}^{(1)}}\Vb_t(\Gb_t\odot \bb_t)\ones + \Gb_t \odot\bb_{t+1}^{(\sim 1)}\,,
% \]
% and hence
% \[
% (\Gb_{t+1}\odot\bb_{t+1})\ones = \diag{\bb_{t+1}^{(1)}}\Vb_t(\Gb_t\odot \bb_t)\ones + (\Gb_t \odot\bb_{t+1}^{(\sim 1)})\ones\,.
% \]
% By induction hypothesis, $(\Gb_t \odot\bb_{t+1}^{(\sim 1)})\ones$ is the set of functions of the form $(\Db+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^{t-1}r_\ell$. 
% % Also, $\Gb_t\bb_t = (\alpha'\I+\M')\x$ where $\M'$ can be related to $\M$ through $\Gb_t$, but since $\bb_t$, $b_1$ and $\bb_t$ can move freely and the last column of $\Gb_t$ is $\x$, we can set $\alpha'$ to any value. In particular, we can make it nonzero and so make $\Gb_t\bb_t b_1$ full rank.  
% By varying $\Vb_t,\bb_t$ and $\bb_t^{(1)}$, the term $\diag{\bb_{t+1}^{(1)}}\Vb_t(\Gb_t\odot \bb_t)\ones$ covers all functions of the form $\Vb \x$ with $\Vb$ a $d\times d$ matrices of rank $r_t$ (for given $\V$ of rank $r_t$, take $\Vb_t = \V$, $\bb_t = [\zeros|\zeros|\dotsc| \ones]$, $\bb^{(\sim 1)}_{t+1}=\I$, which is possible since they are free from the choice of $\bb_{t+1}^{(\sim 1)}$). Hence, $(\Gb_{t+1}\odot\bb_{t+1})\ones$ is the set of functions of the form $(\Db+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^t r_\ell$, completing the induction step.
% \end{proof}

%==============
\subsection{Trade-off between test error and model complexity} In the previous section, we characterized the class of models that can be expressed by each architecture. Next, we study the trade-off between the optimal test error achievable by each model and the model complexity, defined as the number of its parameters.

Note that all the classes of models characterized in Theorem~\ref{thm:expressive} are linear functions. For a linear model $\x\mapsto \hA\x$, its test error (model risk) is given by
\begin{align*}
{\sf Risk}(\hA) &= \E[(y-\hat{y})^2]\\
&= \E\left[\twonorm{(\A-\hA)\x}^2\right] + \sigma^2\\
&= \E[\tr{\{(\A-\hA)\x \x^\sT(\A-\hA)^\sT\}}]+ \sigma^2\\
&= \fronorm{\A-\hA}^2+ \sigma^2\,,
\end{align*}
where we assumes that $\E[\x\x^\sT] = \I$ (isotropic features). Since the term $\sigma^2$ is constant (independent of model $\hA$) we will drop it in sequel without effecting our discussion and focus on the excess risk.
For a class of models $\cC$ we use the notation ${\sf ER}^*(\cC)$ to indicate the minimum excess risk achievable over the class $\cC$:
%
\[
{\sf ER}^*(\cC): = \min_{\hA\in\cC} \fronorm{\A-\hA}^2\,.
\]
%
Note that ${\sf ER}^*(\cC_{\rm base}(T))$ is obtained by the best $r$-rank  approximation to $\A$ and ${\sf ER}^*(\cC_{\rm res})$ is obtained by the best $rT$-rank approximation to $\A-\I$, both of which have simple characterization in terms of the singular values of $\A$ and $\A-\I$, by using the celebrated Eckart–Young–Mirsky theorem.
Deriving ${\sf ER}^*(\cC_{\rm \GenA{}}(T))$ and ${\sf ER}^*(\cC_{\rm GenB{}(T)})$ are more complicated. In the next theorem, we establish upper bounds on them.

 \begin{figure*}[t!]
    \centering
    % \begin{subfigure}[b]{0.45\textwidth}
    %     \centering
    %     \includegraphics[height=1.8in]{figures/comp_eye_suv_s5.pdf}
    %     \caption{$\A= \I +s \Ub\Vb^\sT$}
    % \end{subfigure}%
    % ~ 
    % \begin{subfigure}[b]{0.45\textwidth}
    %     \centering
    %     \includegraphics[height=1.8in]{figures/comp_diag_normal_s5.pdf}
    %     \caption{$\A = s \Db+\W$}
    % \end{subfigure}
    \hspace{1cm} % to visually center the figure
    \includegraphics[width=0.9\textwidth]{figures/reduction_in_test_error.pdf}
    \vskip -0.1in
    \caption{Gain in the model performance achieved by \GenA{} and \GenB{} over ResNet. The plots represents the lower bounds for $G_1$ and $G_2$ given in Theorem~\ref{thm:trade-off}(ii). Observe that the gain at larger dimension $d$ is higher. Left panel shows that the gain decreases as the collective rank $r_*$ of ResNet increases ($\lambda_{\min}= 5$, $\lambda_{\max} = 10$). Right panel shows that the gain increases as the complexity of the target task ($\kappa= \lambda_{\min}/\lambda_{\max}$) increases ($\lambda_{\max}= 10$ and $r_* = 50$ ).}\label{fig:tradeoffs}
\end{figure*}

\begin{theorem}\label{thm:comp}
Consider the singular value decomposition $\A-\I = \Ub\bSigma \V^\sT$. For a given $m\in[d]$, let $\Ub_{m}$, $\bSigma_m$, $\Vb_{m}$ be the top $m$ singular vectors and singular values and define $\bDelta: = \A -\I- \Ub_{r*}\bSigma_{r*} \Vb_{r*}^\sT$, where $r_*:=\sum_{\ell=1}^{T}r_\ell$.
We then have
\begin{align*}
    {\sf Err}^*(\cC_{\rm res}) &= \fronorm{\bDelta}^2\,,\\
    {\sf Err}^*(\cC_{\rm \GenA{}}) &\le \fronorm{\bDelta}^2 - \frac{1}{d-r_*}\tr(\bDelta)^2\,,\\
    {\sf Err}^*(\cC_{\rm \GenB{}}) &\le \fronorm{\bDelta}^2\\
    &\quad-\max\left\{\sum_{i=1}^d  \Delta_{ii}^2, \frac{1}{d-r_*}\tr(\bDelta)^2\right\}\,,
\end{align*}
where $\{\Delta_{ii}\}_{i=1}^d$ are the diagonal entries of $\bDelta$.
\end{theorem}
%
% \begin{proof}
% By Eckart–Young–Mirsky theorem, $\Ub_{r*}\bSigma_{r*} \Vb_{r*}^\sT$ is the best rank $r*$ approximation to $\A-\I$, by which we obtain ${\sf Err}^*(\cC_{\rm res}) = \fronorm{\bDelta}^2$. 

% We also have
% \begin{align*}
%   {\sf Err}^*(\cC_{\rm Gen1})&\le
%   \min_{\alpha} \fronorm{\A-\alpha\I - \Ub_{r*}\bSigma_{r*} \Vb_{r*}^\sT}\\
%   &= \min_{\alpha} \fronorm{\bDelta+(1-\alpha)\I}^2\\
%   &= \min_{\tilde{\alpha}} \fronorm{\bDelta}^2+d \tilde{\alpha}^2 -2\tilde{\alpha} \tr(\bDelta) \\
%   &= \fronorm{\bDelta}^2 - \frac{1}{d} \tr(\bDelta)^2\,.
% \end{align*}
% Likewise for ${\sf Err}^*(\cC_{\rm Gen2})$ we have
% \begin{align*}
%   {\sf Err}^*(\cC_{\rm Gen2})&\le
%   \min_{\Db} \fronorm{\A-\Db - \Ub_{r*}\bSigma_{r*} \Vb_{r*}^\sT}\\
%   &= \min_{\Db} \fronorm{\bDelta+\I-\Db}^2\\
%   &= \min_{\widetilde{\Db}} \fronorm{\bDelta-\widetilde{\Db}}^2 \\
%   &= \fronorm{\bDelta}^2 - \sum_{i=1}^d \Delta_{ii}^2\,.
% \end{align*}
% \end{proof}
% \begin{corollary}\label{coro:comp}
% We have 
% \begin{align}
%     {\sf Err}^*(\cC_{\rm res}) - {\sf Err}^*(\cC_{\rm Gen1}) &\ge \left(\frac{\tr(\bDelta)}{\sqrt{n}} - \sqrt{n} \right)^2\,,\label{eq:res-Gen1}\\
%     {\sf Err}^*(\cC_{\rm res}) - {\sf Err}^*(\cC_{\rm Gen2}) &\ge \sum_{i=1}^d (1-\Delta_{ii})^2\,.\label{eq:res-Gen2}
% \end{align}
% \end{corollary}
% By Corollary~\ref{coro:comp} we see that as $|\tr(\bDelta)-n|$ increases the improvement we see by fitting a model from Gen1 over standard ResNet becomes stronger. On the other hand, Gen2 offers a larger improvement by adapting to the heterogeneity of $\{\Delta_{ii}\}$. In particular, note that the difference between the right hand sides of \eqref{eq:res-Gen1} and \eqref{eq:res-Gen2} is given by $\sum_{i=1}^2 \Delta_{ii}^2 - \frac{1}{n} (\sum_{i=1}^d \Delta_{ii})^2 : = n \Var{\{\Delta_{ii}\}} \ge 0$, which becomes larger as $\{\Delta_{ii}\}_{i=1}^d$ becomes more diverse.  

We proceed by discussing the model complexity for each of the architectures, in terms of model size. The number of parameters for ResNet is given by $2dr_*$, for \GenA{} is given by $2dr_*+ T(T-1)/2$, and for \GenB{} is given by $2dr_*+ d T(T-1)/2$. Note that by Theorem~\ref{thm:comp}, if \GenA{} and \GenB{} achieve better Excess risk-model size trade-off  compared to ResNet, then we can make this improvement arbitrarily strong by scaling $\A-\I$ (and so $\bDelta$).

In the next theorem, we focus on \GenA{} and \GenB{} and provide sufficient conditions under which they achieve a better excess risk-model size trade-off. In the second part of the theorem, we also lower bound the improvement that \GenA{} and \GenB{} achieve in excess risk compared to ResNet, with using the same number of parameters.

\begin{theorem}\label{thm:trade-off}
Assume that $\A-\I \succeq \bf{0}$ and let $\lambda_{\max}$ and $\lambda_{\min}>0$ respectively denote the maximum and the minimum eigenvalues of $\A-\I$. Define $\kappa:=\lambda_{\min}/\lambda_{\max}\le 1$. Consider a ResNet model with collective rank $r_*:=\sum_{t=1}^T r_t$. % 

$(i)$ If 
\begin{align}\label{eq:condition1}
\frac{r_*}{d} \le (1+\kappa(\sqrt{\kappa^2+1}-\kappa))^2 - 1,
%\frac{\kappa^2}{2}+1-\frac{\kappa}{2}\sqrt{\kappa^2+4} ,
\end{align}
then \GenA{} achieves a better excess risk-model size trade-off compared to ResNet. In addition, if 
\begin{align}\label{eq:condition2}
r_*\le (1+\kappa(\sqrt{\kappa^2+d}-\kappa))^2 - 1,
\end{align}
then \GenB{} achieves a better  trade-off compared to ResNet.

$(ii)$ Consider $\mathcal{C}_{\rm\GenA{}}$ and $\mathcal{C}_{\rm \GenB{}}$, the class of models that can be expressed by the \GenA{} and \GenB{} architectures with the same number of parameters as a ResNet model with $T$ layers and collective rank $r_*$. Define $G_1: ={\sf ER}^*(\cC_{\rm res}) - {\sf ER}^*(\cC_{\rm \GenA{}})$ and $G_2: ={\sf ER}^*(\cC_{\rm res}) - {\sf ER}^*(\cC_{\rm \GenB{}})$ as the reduction in the optimal excess risk achievable by these classes compared to the optimal excess risk of ResNet. %Under condition~\eqref{eq:condition}, we have
We have
\begin{align*}
G_1 &\ge
(d-r_*)\lambda_{\min}^2 - (\sqrt{d+r_*}-\sqrt{d})^2(\lambda_{\max}^2- \lambda_{\min}^2)\,,\\
G_2 &\ge (d-r_*)\lambda_{\min}^2 - (\sqrt{1+r_*}-1)^2(\lambda_{\max}^2- \lambda_{\min}^2)\,.
%\Big(\frac{\sum_{t=1}^T r_t}{d}\Big)^2 - (2+\kappa^2) \Big(\frac{\sum_{t=1}^T r_t}{d}\Big) + 1\ge 0\,. 
\end{align*}
\end{theorem}

Our next result quantitatively shows the reduction in the collective rank one can achieve by GRNs, while maintaining the same test error as ResNet.

\begin{proposition}\label{pro:rank-reduction}
Consider a ResNet with collective rank $r_* = \sum_{t=1}^T r_t < d$. A \GenA{} or \GenB{} model can achieve a smaller test error with collective rank $r'_*$, where $r'_*:=\frac{r_*-d\kappa^2}{1-\kappa^2} < r_*$. 
\end{proposition}
\subsection{Insights from the analysis} Theorem~\ref{thm:trade-off} allows us to elucidate the role of different factors on the gain achieved by GRNs. 

{\bf Role of target task complexity.} Note that $\kappa=\lambda_{\min}/\lambda_{\max}\in [0,1]$ is a measure of complexity of the target task. Specifically, as $\kappa$ decreases, the matrix $\A$ becomes closer to a low rank matrix, and hence learning it with low rank models becomes easier. Observe that the thresholds given by the right hand side of \eqref{eq:condition1} and \eqref{eq:condition2} are increasing in $\kappa$, i.e., for more complex tasks we see a wider range of collective rank where GRNs outperforms the trade-off achieved by ResNet. Another way to  interpret Theorem~\ref{thm:trade-off}(i) is that for a fixed target task (and so fixed $\kappa$), if the collective rank $r_*$ is above this threshold, the ResNet is already rich enough that it is hard to improve upon its trade-off. 

{\bf Role of collective rank.} Observe that the lower bound on the gains $G_1$, $G_2$ given by Theorem~\ref{thm:trade-off}(ii) are decreasing in $r_*$. In other words, when the collective rank $r_*$ of ResNet becomes smaller, the level of information dilution occurring in ResNet increases, giving GRNs a better leverage to improve model perplexity with the same number of parameters.


{\bf Role of input dimension.} Note that the upper bounds on $r_*$ given by~\eqref{eq:condition1} and~\eqref{eq:condition2} increase with the input dimension $d$. Furthermore, the lower bounds on the gains $G_1$, $G_2$, given in Theorem~\ref{thm:trade-off}(ii) also increase with $d$. Therefore, for larger input dimensions, we have both a wider range for $r_*$ where GRNs outperforms the trade-off achieved by ResNet, and moreover, we obtain a larger gain in reducing model error.

We refer to Figure~\ref{fig:tradeoffs} for an illustration of these trends.

% \begin{remark}
% As $\kappa = \lambda_{\max}/\lambda_{\min}$ increases, the matrix $\A$ becomes closer to a low-rank matrix and hence the learning task becomes simpler. In this case, for $T$ large enough, the standard ResNet is already rich enough that it is harder to improve upon its trade-off. The condition given in Theorem~\ref{thm:comp} implies that as $\kappa$ becomes smaller (the task becomes harder), we get a larger bound for $T*$ (i.e larger models) for which our generalization gives a better trade-off than the standard ResNet. 
% \end{remark}
% % \begin{proof}
% % Fix an arbitrary $p\le 2rdT^*$ (Note that $2rdT^*$ is the maximum number of parameters for ResNet with $T^*$ layers). We will compare the test error of Gen1 and standard ResNet with $p$ number of parameters. This corresponds to a ResNet with $T_1 = p/(2rd)$ layers and a model in Gen1 with $T_2$ layers such that $2rdT_2+T_2(T_2-1)/2 = p$ (obviously $T_2< T_1$). Let $\sigma_1\ge \dotsc\ge \sigma_{d}$ be the singular values of $\A-\I$. By Theorem~\ref{thm:comp} we have
% % \[
% % {\sf Err}^*(\cC_{\rm res}) = \sum_{i=rT_1+1}^d \sigma_i^2\,,\quad 
% % {\sf Err}^*(\cC_{\rm Gen1}) \le \sum_{i=rT_2+1}^d \sigma_i^2 - \frac{1}{d} \Big(\sum_{i=rT_2+1}^d \sigma_i \Big)^2
% % \]
% % Therefore, ${\sf Err}^*(\cC_{\rm Gen1}) < {\sf Err}^*(\cC_{\rm res})$ if the following holds:
% % \begin{align}\label{eq:condition-sup-tradeoff}
% % \sum_{i=rT_2+1}^{rT_1} \sigma_i^2
% % \le \frac{1}{d} \Big(\sum_{i=rT_2+1}^d \sigma_i \Big)^2
% % \end{align}
% % However note that the left hand side of this condition is upper bounded by
% % \[
% % \sum_{i=rT_2+1}^{rT_1} \sigma_i^2 \le r(T_1-T_2) \lambda_{\max}^2 \le rT^* \lambda_{\max}^2
% % \]
% % Additionally, the right-hand side of the condition is lower bounded by 
% % \[
% % \frac{1}{d} \Big(\sum_{i=rT_2+1}^d \sigma_i \Big)^2 \ge \frac{(d-rT_2)^2}{d}\lambda_{\min}^2 \ge \frac{(d-rT^*)^2}{d}\lambda_{\min}^2 \,.
% % \]
% % So a sufficient condition for~\eqref{eq:condition-sup-tradeoff} is that 
% % \[
% % rT^* \lambda_{\max}^2\le \frac{(d-rT^*)^2}{d}\lambda_{\min}^2\,.
% % \]
% % Writing it in terms of $\kappa$, we need $\kappa^2\frac{rT^*}{d}\le(1-\frac{rT_*}{d})^2$. Solving this inequality for $T^*$ gives the desired result.

% % For Gen2, the argument goes along the same lines. Fixing number of parameters to $p$, this corresponds to a model in Gen2 with $T_3$ layers such that $2rdT_3+dT_3(T_3-1)/2 = p$. Hence, $T_3<T_1< T*$. Also by Theorem~\ref{thm:comp}, 
% % \begin{align*}
% % {\sf Err}^*(\cC_{\rm Gen2}) &\le \fronorm{\bDelta}^2 - \sum_{i=1}^d \Delta_{ii}^2\\
% % &\le \fronorm{\bDelta}^2 - \frac{1}{d}\Big(\sum_{i=1}^d \Delta_{ii}\Big)^2\\
% % &= \fronorm{\bDelta}^2 - \frac{1}{d}\tr(\bDelta)^2\\
% % & = \sum_{i=rT_3+1}^d \sigma_i^2 - \frac{1}{d}\Big(\sum_{i=rT_3+1}^d \sigma_i^2 \Big)
% % \end{align*}
% % where the second inequality is an application of Cauchy–Schwarz inequality. The rest of the argument is exactly the same as the one for Gen1, where we replace $T_2$ by $T_3$. 
% % \end{proof}


% %However, as we discussed above by making $\Delta_{ii}$ more heterogeneous and making their average deviate more from one, we can arbitrarily improve the trade-off in favor of Gen1 and Gen2. In other words, as the data generating model (captured by $\A$) becomes more complex (measured in terms of mean and variance of $\{\Delta_{ii}\}_{i=1}^d$) the benefit of our generalization becomes more evident. 



% \begin{theorem}\label{thm:trade-off2}
% Assume that $\A-\I \succeq \bf{0}$ and let $\lambda_{\max}$ and $\lambda_{\min}>0$ respectively denote the maximum and the minimum singular values of $\A-\I$. Define $\kappa:=\lambda_{\max}/\lambda_{\min}\ge 1$. Consider a standard ResNet with $T$ layers.  
% \begin{itemize}
% \item If 
% \begin{align}\label{eq:condition}
% \frac{\sum_{t=1}^T r_t}{d}\le \frac{\kappa^2}{2}+1-\frac{\kappa}{2}\sqrt{\kappa^2+4}\,,
% \end{align}
% then Gen1 and Gen2 achieve a better test error-model complexity trade-offs compared to the standard ResNet.
% \item Consider the class of Gen1 and Gen2 models with the same number of parameters as the standard ResNet with $T$ layers. Define $G_1: ={\sf Err}^*(\cC_{\rm res}) - {\sf Err}^*(\cC_{\rm Gen1})$ and $G_2: ={\sf Err}^*(\cC_{\rm res}) - {\sf Err}^*(\cC_{\rm Gen2})$ as the reduction in the model generalization error. Under condition~\eqref{eq:condition}, we have
% \[
% G_1, G_2 \ge \Big(\frac{\sum_{t=1}^T r_t}{d}\Big)^2 - (2+\kappa^2) \Big(\frac{\sum_{t=1}^T r_t}{d}\Big) + 1\ge 0\,. 
% \]
% Furthermore, under condition~\eqref{eq:condition}, this bound is decreasing in $(\sum_{t=1}^T r_t)/d$.
% \end{itemize}

% \end{theorem}

% % \begin{proof}
% % Let $p:= 2dr_*$, where we recall that $r_* = \sum_{\ell=1}^T r_\ell$. Note that $p$ is the number of parameters for ResNet with $T$ layers and ranks $r_t$ for each layer $t$). We will compare the test error of Gen1 and standard ResNet with $p$ number of parameters. This corresponds to a model in Gen1 with $T'$ layers such that $2d\sum_{\ell=1}^{T'} r_\ell+T'(T'-1)/2 = p$ (obviously $T'< T$). We set the shorthand $r'_*:=\sum_{\ell=1}^{T'}r_\ell$ and let $\sigma_1\ge \dotsc\ge \sigma_{d}$ be the singular values of $\A-\I$. By Theorem~\ref{thm:comp} we have
% % \[
% % {\sf Err}^*(\cC_{\rm res}) = \sum_{i=r_*+1}^d \sigma_i^2\,,\quad 
% % {\sf Err}^*(\cC_{\rm Gen1}) \le \sum_{i=r'_*+1}^d \sigma_i^2 - \frac{1}{d} \Big(\sum_{i=r'_*+1}^d \sigma_i \Big)^2
% % \]
% % Therefore, ${\sf Err}^*(\cC_{\rm Gen1}) < {\sf Err}^*(\cC_{\rm res})$ if the following holds:
% % \begin{align}\label{eq:condition-sup-tradeoff}
% % \sum_{i=r'_*+1}^{r_*} \sigma_i^2
% % \le \frac{1}{d} \Big(\sum_{i=r'_*+1}^d \sigma_i \Big)^2
% % \end{align}
% % (Note that $r'_*< r_*$ since $T'<T$). However note that the left hand side of this condition is upper bounded by
% % \[
% % \sum_{i=r'_*+1}^{r_*} \sigma_i^2 \le (r_*-r'_*) \lambda_{\max}^2 \le r_* \lambda_{\max}^2
% % \]
% % Additionally, the right-hand side of the condition is lower bounded by 
% % \[
% % \frac{1}{d} \Big(\sum_{i=r'_*+1}^d \sigma_i \Big)^2 \ge \frac{(d-r'_*)^2}{d}\lambda_{\min}^2 \ge \frac{(d-r_*)^2}{d}\lambda_{\min}^2 \,.
% % \]
% % So a sufficient condition for~\eqref{eq:condition-sup-tradeoff} is that 
% % \[
% % r_* \lambda_{\max}^2\le \frac{(d-r_*)^2}{d}\lambda_{\min}^2\,.
% % \]
% % Writing it in terms of $\kappa$, we need $\kappa^2\frac{r_*}{d}\le(1-\frac{r_*}{d})^2$. Solving this inequality for $r_*$ gives the desired result.

% % For Gen2, the argument goes along the same lines. Fixing number of parameters to $p$, this corresponds to a model in Gen2 with $T''$ layers such that $2d\sum_{\ell=1}^{T''}r_{\ell}+dT''(T''-1)/2 = p$. Hence, $T''<T'< T$. Also by Theorem~\ref{thm:comp}, 
% % \begin{align*}
% % {\sf Err}^*(\cC_{\rm Gen2}) &\le \fronorm{\bDelta}^2 - \sum_{i=1}^d \Delta_{ii}^2\\
% % &\le \fronorm{\bDelta}^2 - \frac{1}{d}\Big(\sum_{i=1}^d \Delta_{ii}\Big)^2\\
% % &= \fronorm{\bDelta}^2 - \frac{1}{d}\tr(\bDelta)^2\\
% % & = \sum_{i=r''_*+1}^d \sigma_i^2 - \frac{1}{d}\Big(\sum_{i=r''_*+1}^d \sigma_i^2 \Big)
% % \end{align*}
% % where $r''_*:= \sum_{\ell=1}^{T''}r_\ell$ the second inequality is an application of Cauchy–Schwarz inequality. The rest of the argument is exactly the same as the one for Gen1, where we replace $T'$ by $T''$. 
% % \end{proof}
% % \bigskip

% \paragraph{Experiments.} To illustrate the the improvement in the test error-model complexity trade-off, we run two experiments. In the first one,
% $\A = \I + s \Ub\Vb^\sT$, where $\Ub,\Vb\in\reals^{d\times k}$ are semi-unitary matrices randomly chosen from the Haar measure. In the second experiment, $\A = s \Db + \W$ where $\Db$ is a diagonal matrix whose entries are i.i.d from {\sf Unif}$([0,1])$ and $W_{ij}\sim\normal(0,1/d)$.   

%  We plot the trade-off curves in Figure~\ref{fig:tradeoffs}.
 

% In these curves, we compute ${\sf Err}^*(\cC_{\rm base})$ and ${\sf Err}^*(\cC_{\rm res})$ exactly using the spectrum of $\A$ and $\A-\I$. For ${\sf Err}^*(\cC_{\rm Gen1})$, we vary $\alpha\in [-\sigma_1(\A),\sigma_1(\A)]$ and find the best $rT$-rank approximation to $\A-\alpha \I$. We then choose $\alpha_{{\rm opt}}$ as the one that results in the minimum error of approximating $\A$ by a matrix of form $\alpha\I+\M$ with $\rank(\M) = rT$.

% For computing ${\sf Err}^*(\cC_{\rm Gen2})$ we use an EM algorithm proposed by~\cite{srebro2003weighted} to approximate $\A$ as $\Db+\M$, with $\Db$ a diagonal matrix and $\rank(\M) = rT$. In this algorithm we initialize the estimate of $\Db$ by $\alpha_{\rm opt}\I$ calculated for ${\sf Err}^*(\cC_{\rm Gen1})$.

%======================
\subsection{Extension to nonlinear models}
We recall the definition of Bottleneck rank from~\cite{jacot2022implicit}. For a function $f:\Omega \mapsto \reals^d$, its Bottleneck rank, denoted by $\rank_{BN}(f,\Omega)$ is the  smallest integer $k$ 
such that $f$ can be factorized as $f = h\circ g$
with inner dimension $k$ (i,e, $g:\Omega\mapsto \reals^k$ and $h:\reals^k\mapsto \reals^d$) It is also closely related to the Jacobian rank of a function defined as $\rank_{\Jb} (f) = \max_{\x\in\Omega} \rank [\Jb f(\x)]$.  In general, $\rank_{\Jb}(f) \le \rank_{BN}(f)$, but for functions of the form $f =
\psi \circ \A \circ \phi$ (for a linear map $\A$ and two bijections $\psi$ and $\phi$), we have $\rank_{\Jb}(f) = \rank_{BN}(f) =
\rank(\A)$. These two notions of rank satisfy the following properties~\cite{jacot2022implicit}:
\begin{itemize}
    \item $\rank(f\circ g)\le \min\{\rank(f), \rank(g)\}$
    \item $\rank(f+g)\le \rank(f)+\rank(g)$
\end{itemize}
\begin{proposition}\label{pro:nonlinear}
Consider an MLP with $f_t(\z) = \Vb_t \varphi(\Ub_t \z)$ with $\Ub_t\in\reals^{r_t\times d}$, $\Vb_t\in\reals^{d\times r_t}$. Denote by $r_* := \sum_{t=1}^T r_t$ the collective rank of the network. We have

%\begin{itemize}
    $\bullet \quad \cC_{{\rm base}}\subseteq  \left\{ f:\; \rank_{BN}(f)\le \min(r_t)_{t=1}^T\right\}$. 
    
    $\bullet \quad \cC_{{\rm res}}\subseteq  \left\{ id+ f:\; \rank_{BN}(f)\le r_*\right\}$.
    
    $\bullet \quad \cC_{{\rm \GenA{}}}\subseteq  \left\{\alpha \cdot  id+ f:\; \rank_{BN}(f)\le r_* \right\}$.
    
    $\bullet \quad \cC_{{\rm \GenB{}}}\subseteq  \left\{\Db+ f:\; \rank_{BN}(f)\le r_*, \, \Db \text{ is diagonal}\right\}$.
%\end{itemize}
% \begin{proof}
% The proof is similar to the linear case by induction on $T$. Note that for showing this direction ($\cC_{{\rm base}},\cC_{{\rm res}},\cC_{{\rm Gen1}}, \cC_{{\rm Gen2}}$ being a subset of the rank constrained functions) we only used the following two properties of the rank function which holds also for the Bottleneck rank: $\rank(f\circ g)\le \min\{\rank(f), \rank(g)\}$ and 
%      $\rank(f+g)\le \rank(f)+\rank(g)$.
% \end{proof}

\end{proposition}