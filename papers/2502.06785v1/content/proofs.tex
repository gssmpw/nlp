\section{Proof of Theorem~\ref{thm:expressive}}
\label{proof:expressive}

% \begin{lemma}
% \label{lemma:resnet-lowrank-expressivity}
% We have $\cC_{{\rm res}} = \{\x \mapsto(\I+\M)\x:\; \rank(\M)\le \sum_{t=1}^T r_t\}$.
% \end{lemma}
%
We restate each of the claims in the theorem statement, followed by its proof.

$\bullet\quad \cC_{{\rm base}} = \{\x\mapsto\M\x: \; \rank(\M)\le \min(r_t)_{t=1}^T\}$.

Note that by the inequality $\rank(\A\B)\le \min\{\rank(\A),\rank(\B)\}$, if $\M$ is of the form $\prod_{t=1}^T \Vb_t$ then $\rank(\M)\le \min(r_t)_{t=1}^T$. For the other direction consider any matrix $\M$ with $\rank(\M)=r_0\le \min(r_t)_{t=1}^T$, and its SVD as $\M= \Pb \Sb\Qb^\sT$ with $\Pb,\Qb\in\reals^{d\times r_0}$ with full column ranks. By setting, $\Vb_1 = \Pb \Sb\Qb^\sT$ and $\Vb_2=\dotsc =\Vb_T = \Qb\Qb^\sT$ we have $\M = \prod_{t=1}^T \Vb_t$, because $\Qb^\sT \Qb= \I$ and also $\rank(\Vb_t) = r_0\le \min(r_t)_{t=1}^T \le r_t$.

\bigskip

$\bullet\quad \cC_{{\rm res}} = \{\x \mapsto(\I+\M)\x:\; \rank(\M)\le r_*\}$

We have 
\begin{align*}
    \prod_{t=1}^T (\I+\Vb_t) &= \Vb_1 \prod_{t=2}^T (\I+\Vb_t) + \prod_{t=2}^T (\I+\Vb_t) \\
    &= \Vb_1 \prod_{t=2}^T (\I+\Vb_t) + \Vb_2 \prod_{t=3}^T (\I+\Vb_t)+ \prod_{t=3}^T (\I+\Vb_t)\\
    &= \dotsc\\
    &= \I + \Vb_T+ \sum_{t=1}^{T-1} \Vb_t \prod_{\tau=t+1}^T (\I+\Vb_\tau) 
\end{align*}
Note that each of the summand is of rank at most $r_t$, so it can be written as $\I+\M$ with $\rank(\M)\le \sum_{t=1}^T r_t$. Hence $\prod_{t=1}^T (\I+\Vb_t)\x\in \cC_{{\rm res}}$.

We next show that any $\I+\M$ with $\rank(\M):= r\le \sum_{t=1}^T r_t$ can be written as $\prod_{t=1}^T (\I+\Vb_t)$ with $\rank(\Vb_t)\le r_t$ for $t\in[T]$. We show this claim by induction. For the basis ($T=1$), 
we can take $\Vb_1 = \M$. 
To complete the induction step, we need to find $\Vb\in\reals^{d\times d}$ such that $\rank(\Vb) = r_T$ and $(\I+\V)^{-1} (\I+\M)-\I$ is of rank at most $\sum_{t=1}^{T-1}r_t$. Then by the induction hypothesis, we can write
\[
(\I+\V)^{-1} (\I+\M) = \prod_{t=1}^{T-1}(\I+\Vb_t)\,,
\]
with $\rank(\Vb_t) \le r_t$, which completes the proof. 
Without loss of generality, we assume $r_T\le r$; otherwise we can take $\Vb_T = \M$ and $\Vb_t = \zero$ for $t\le T-1$.

To find such $\Vb$ we write $\M = \Pb \Qb^{\sT}$ with $\Pb,\Qb\in\reals^{d\times r}$ having full column rank. Define $\Pb_1,\Qb_1\in\reals^{d\times r_T}$ obtaining by considering the first $r_T$ columns of $\Pb$ and $\Qb$. Additionally, define
\begin{align}\label{def:B-C}
\B := \Pb_1(\I+\Qb_1^\sT \Pb_1)^{-1},\quad \Cb = \Qb_1(\I+\Pb_1^\sT \Qb_1)\,.
\end{align}
We next construct $\Vb$ by setting $\Vb: = \B\Cb^\sT$. Clearly, $\rank(\Vb) = r_T$. We also have
\begin{align*}
    (\I+\Vb)^{-1}(\I+\M)-\I &= (\I+\B\Cb^\sT)^{-1}\M + (\I+\B\Cb^\sT)^{-1}-\I\\
    &= (\I+\B\Cb^\sT)^{-1}\M + \I- \B(\I+\Cb^\sT\B)^{-1}\Cb^\sT-\I\\
    &= (\I+\B\Cb^\sT)^{-1} (\Pb_1\Qb_1^\sT+ \Pb_{\sim1}\Qb_{\sim1}^\sT)- \B(\I+\Cb^\sT\B)^{-1}\Cb^\sT\,.
\end{align*}
Here we consider the notation $\Pb= [\Pb_1 | \Pb_{\sim1}]$ and  $\Qb= [\Qb_1 | \Qb_{\sim1}]$. The second step above follows from the Woodbury matrix identity. Rearranging the terms we have
\begin{align}\label{eq:induction1}
   (\I+\Vb)^{-1}(\I+\M) -\I =   (\I+\B\Cb^\sT)^{-1}\Pb_{\sim1}\Qb_{\sim1}^\sT + (\I+\B\Cb^\sT)^{-1}\Pb_{1}\Qb_{1}^\sT - \B(\I+\Cb^\sT\B)^{-1}\Cb^\sT\,.
\end{align}
The first term above is of rank at most $\rank(\Pb_{\sim1}) = r- r_T\le \sum_{t=1}^{T-1}r_t$. We next show that the second and the third term cancel each other. Equivalently, we show that
\begin{align}\label{eq:prod-PQ}
\Pb_{1}\Qb_{1}^\sT = (\I+\B\Cb^\sT)\B(\I+\Cb^\sT\B)^{-1}\Cb^\sT\,.
\end{align}
To do this, we next show that 
\begin{align}\label{eq:sep-P1-Q1}
\Pb_1 = (\I+\B\Cb^\sT)\B, \quad \Qb_1^\sT = (\I+\Cb^\sT\B)^{-1}\Cb^\sT\,.
\end{align}
Recalling~\eqref{def:B-C} we have $\Pb_1 = \B (\I+\Qb_1^\sT \Pb_1)$. Also 
\begin{align}
\Cb^\sT\B &=  (\I +\Qb_1^\sT \Pb_1)\Qb_1^\sT \Pb_1(\I+\Qb_1^\sT \Pb_1)^{-1} \nonumber\\
&= (\I +\Qb_1^\sT \Pb_1)(\I+\Qb_1^\sT \Pb_1-\I)(\I+\Qb_1^\sT \Pb_1)^{-1}\nonumber\\
&= (\I +\Qb_1^\sT \Pb_1)(\I-(\I+\Qb_1^\sT \Pb_1)^{-1})\\
&= \I +\Qb_1^\sT \Pb_1 -\I\nonumber\\
&= \Qb_1^\sT\Pb_1\label{eq:CB-Q1P1}
\end{align}
Therefore, $\Pb_1 = \B(\I+ \Cb^\sT \B) = (\I+\B\Cb^\sT)\B$. Likewise, recalling~\eqref{def:B-C} we have $\Qb_1 = \Cb (\I+ \Pb_1^\sT\Qb_1)^{-1}$. Hence,
\[
\Qb_1^\sT = (\I +\Qb_1^\sT \Pb_1)^{-1} \Cb^\sT = (\I +\Cb^\sT\B)^{-1} \Cb^\sT,
\]
using~\eqref{eq:CB-Q1P1}. This completes the proof of~\eqref{eq:sep-P1-Q1} and so~\eqref{eq:prod-PQ}.

Invoking~\eqref{eq:induction1} we get
\[
(\I+\Vb)^{-1}(\I+\M) -\I =   (\I+\B\Cb^\sT)^{-1}\Pb_{\sim1}\Qb_{\sim1}^\sT\,,
\]
which is of rank at most $r-r_T\le \sum_{t=1}^{T-1}r_t$, which completes the proof of the induction step.


%
% \begin{lemma}\label{lem:C-Gen1}
% We have $\cC_{{\rm Gen1}} = \{\x \mapsto(\alpha\I+\M)\x:\; \rank(\M)\le \sum_{\ell=1}^T r_\ell\}$.
% \end{lemma}
%
\bigskip

$\bullet \quad \cC_{{\rm \GenA{}}} = \{\x \mapsto(\alpha\I+\M)\x:\; \rank(\M)\le r_*\}$.

We prove this claim by induction. The induction basis ($T=0$) follows readily since $\Gb_1 \bb_1= b_1 \x$. Assume the induction hypothesis for $t$. We have $f_t(g_t(\x)) = \Vb_t \Gb_t \bb_t$ and so $\Gb_{t+1} = [\Vb_t \Gb_t \bb_t\;|\;  \Gb_t]$. Writing $\bb_{t+1} = \begin{bmatrix}b_1\\ \bb_{\sim 1}\end{bmatrix}$ we obtain
\[
\Gb_{t+1}\bb_{t+1} = \Vb_t\Gb_t \bb_t b_1 + \Gb_t \bb_{\sim 1}\,.
\]
By induction hypothesis, $\Gb_t\bb_{\sim1}$ is the set of functions of the form $(\alpha\I+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^{t-1}r_\ell$. 

Since $\rank(\Vb_t)\le r_t$ the set of functions that can be represented as $\Gb_{t+1}\bb_{t+1}$ is a subset of $(\alpha\I+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^t r_\ell$. Conversely, any given $M$ of rank $\sum_{\ell=1}^{t}r_\ell$ can be written as $\M = \M_1+\Vb$ with $\rank(\M_1)=\sum_{\ell=1}^{t-1}r_\ell$ and $\rank(\Vb) = r_t$. By induction hypothesis, $(\alpha\I+\M_1)\x$ can be expressed by the term $\Gb_t \bb_{\sim1}$. In addition, $\Vb\x$ can also be expressed by the term $\Vb_t \Gb_t \bb_t b_1$, by taking $\Vb_t = \V$, $\bb_t = (0,0,\dotsc, 1)^\sT$, $b_1=1$, which is possible since they are free from the choice of $\bb_{\sim 1}$.

% By varying $\Vb_t,\bb_t$ and $b_1$, the term $\Vb_t\Gb_t \bb_t b_1$ covers all functions of the form $\Vb \x$ with $\Vb$ a $d\times d$ matrices of rank $r$ (for given $\V$ of rank $r$, take $\Vb_t = \V$, $\bb_t = (0,0,\dotsc, 1)^\sT$, $b_1=1$, which is possible since they are free from the choice of $\bb_{\sim 1}$). 
Hence, $\Gb_{t+1}\bb_{t+1}$ the set of functions of the form $(\alpha\I+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^t r_\ell$, completing the induction step.

\bigskip

$\bullet \quad \cC_{{\rm \GenB{}}} = \{\x \mapsto(\Db+\M)\x:\; \rank(\M)\le r_*, \Db \text{ is diagonal}\}$.

The proof follows similar to that of \GenA{}. For the induction basis ($T=0$), we have $(\Gb_1 \odot\bb_1)\ones = (\x\odot \bb_1)\ones = \diag{\bb_1} \x$. Assume the induction hypothesis for $t$. We have $f_t(g_t(\x)) = \Vb_t (\Gb_t\odot \bb_t)\ones$ and so $\Gb_{t+1} = [\Vb_t (\Gb_t\odot \bb_t)\ones\;|\;  \Gb_t]$. Writing $\bb_{t+1} = \begin{bmatrix}\bb_{t+1}^{(1)}| \bb_{t+1}^{(\sim 1)}\end{bmatrix}$ we obtain
\[
\Gb_{t+1}\odot\bb_{t+1} = \diag{\bb_{t+1}^{(1)}}\Vb_t(\Gb_t\odot \bb_t)\ones + \Gb_t \odot\bb_{t+1}^{(\sim 1)}\,,
\]
and hence
\[
(\Gb_{t+1}\odot\bb_{t+1})\ones = \diag{\bb_{t+1}^{(1)}}\Vb_t(\Gb_t\odot \bb_t)\ones + (\Gb_t \odot\bb_{t+1}^{(\sim 1)})\ones\,.
\]
By induction hypothesis, $(\Gb_t \odot\bb_{t+1}^{(\sim 1)})\ones$ is the set of functions of the form $(\Db+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^{t-1}r_\ell$. 
% Also, $\Gb_t\bb_t = (\alpha'\I+\M')\x$ where $\M'$ can be related to $\M$ through $\Gb_t$, but since $\bb_t$, $b_1$ and $\bb_t$ can move freely and the last column of $\Gb_t$ is $\x$, we can set $\alpha'$ to any value. In particular, we can make it nonzero and so make $\Gb_t\bb_t b_1$ full rank.  
By varying $\Vb_t,\bb_t$ and $\bb_t^{(1)}$, the term $\diag{\bb_{t+1}^{(1)}}\Vb_t(\Gb_t\odot \bb_t)\ones$ covers all functions of the form $\Vb \x$ with $\Vb$ a $d\times d$ matrices of rank $r_t$ (for given $\V$ of rank $r_t$, take $\Vb_t = \V$, $\bb_t = [\zeros|\zeros|\dotsc| \ones]$, $\bb^{(\sim 1)}_{t+1}=\I$, which is possible since they are free from the choice of $\bb_{t+1}^{(\sim 1)}$). Hence, $(\Gb_{t+1}\odot\bb_{t+1})\ones$ is the set of functions of the form $(\Db+\M)\x$ with $\rank(\M)\le \sum_{\ell=1}^t r_\ell$, completing the induction step.

%==================================
%
\section{Proof of Theorem~\ref{thm:comp}}

By Eckart–Young–Mirsky theorem, $\Ub_{r*}\bSigma_{r*} \Vb_{r*}^\sT$ is the best rank $r*$ approximation to $\A-\I$, by which we obtain ${\sf ER}^*(\cC_{\rm res}) = \fronorm{\bDelta}^2$. 

We also have by definition,
\begin{align}\label{eq:Err-Gen1-0}
   {\sf ER}^*(\cC_{\rm \GenA{}}) =  \min_{\alpha,\rank(\tilde{\A})=r_*} \fronorm{\A-\alpha\I - \tilde{\A}}^2\,.
\end{align}
%
Recall the SVD of $\A-I = \Ub\bSigma \Vb^\sT$ and consider the following decompositions:
\begin{align*}
    \Ub = [\Ub_{r_*}\;|\; \Ub_{r_*,\perp}],\quad \Vb = [\Vb_{r_*}\;|\; \Vb_{r_*,\perp}], \quad
    \bSigma = \begin{bmatrix} \bSigma_{r_*} & \mathbf{0}\\
    \mathbf{0} & \bSigma_{r_*,\perp}\end{bmatrix}\,,
\end{align*}
with $\Ub_{r_*,\perp}, \Vb_{r_*,\perp}\in\reals^{d\times (d-r_*)}$, and $\bSigma_{r_*,\perp}$ a diagonal matrix of size $d-r_*$. Since $\Ub$ is unitary matrix, we have $\Ub_{r_*}\Ub_{r_*}^\sT + \Ub_{r_*,\perp}\Ub_{r_*,\perp}^\sT = \I$.
 

 We then note that for any choice of $\alpha$, $\tilde{\A}$, we have
\begin{align*}
    \A-\alpha \I - \tilde{\A} &= \A-\I +(1-\alpha) \I - \tilde{\A}\\
    &= \bDelta+\Ub_{r*}\bSigma_{r*} \Vb_{r*}^\sT+ (1-\alpha)\I -\tilde{\A}\\
    &= \bDelta+\Ub_{r*}\bSigma_{r*} \Vb_{r*}^\sT+(1-\alpha) \Ub_{r_*}\Ub_{r_*}^\sT +(1-\alpha) \Ub_{r_*,\perp}\Ub_{r_*,\perp}^\sT  -\tilde{\A}\,.
\end{align*}
Next, by taking $\tilde{\A} = \Ub_{r*}\bSigma_{r*} \Vb_{r*}^\sT+(1-\alpha) \Ub_{r_*}\Ub_{r_*}^\sT = \Ub_{r*} [\bSigma_{r*} \Vb_{r*}^\sT + (1-\alpha)\Ub_{r_*}^\sT ]$ as the rank-$r_*$ matrix, we obtain 
\[
\A-\alpha \I - \tilde{\A} = \bDelta + (1-\alpha) \Ub_{r_*,\perp}\Ub_{r_*,\perp}^\sT\,. 
\]
%
Invoking the characterization~\eqref{eq:Err-Gen1-0}, we arrive at
\begin{align*}
   {\sf ER}^*(\cC_{\rm \GenA{}})
   &\le \min_{\alpha} \fronorm{\bDelta+(1-\alpha)\Ub_{r_*,\perp}\Ub_{r_*,\perp}^\sT }^2\\
   &= \min_{\tilde{\alpha}} \fronorm{\bDelta}^2+ \tilde{\alpha}^2 \fronorm{\Ub_{r_*,\perp}\Ub_{r_*,\perp}^\sT}^2 -2\tilde{\alpha} \tr(\bDelta^\sT \Ub_{r_*,\perp}\Ub_{r_*,\perp}^\sT) \\
   &= \min_{\tilde{\alpha}} \fronorm{\bDelta}^2+ (d-r_*)\tilde{\alpha}^2 -2\tilde{\alpha} \tr(\bDelta)\\
   &= \fronorm{\bDelta}^2 - \frac{1}{d-r_*} \tr(\bDelta)^2\,,
\end{align*}
where in the second equality, we used the fact that $\Ub_{r_*,\perp}$ is unitary and so $\fronorm{\Ub_{r_*,\perp}\Ub_{r_*,\perp}^\sT}^2= d-r_*$. In addition, observed that
\[\bDelta = \A-\I- \Ub_{r_*} \bSigma_{r_*}\Vb_{r_*}^\sT = \Ub\bSigma\Vb^\sT - \Ub_{r_*} \bSigma_{r_*}\Vb_{r_*}^\sT =  \Ub_{r_*,\perp} \bSigma_{r_*,\perp}\Vb_{r_*,\perp}^\sT\,.\]
Therefore, 
\[
\bDelta^\sT \Ub_{r_*,\perp}\Ub_{r_*,\perp}^\sT = \Vb_{r_*,\perp} \bSigma_{r_*,\perp}\Ub_{r_*,\perp}^\sT \Ub_{r_*,\perp} \Ub_{r_*,\perp}^\sT
= \Vb_{r_*,\perp} \bSigma_{r_*,\perp}\Ub_{r_*,\perp}^\sT = \bDelta^\sT\,,
\]
and so $\tr(\bDelta^\sT \Ub_{r_*,\perp}\Ub_{r_*,\perp}^\sT) = \tr(\bDelta^\sT)= \tr(\bDelta)$. This completes the proof of the upper bound on 
${\sf ER}^*(\cC_{\rm \GenA{}})$.

For ${\sf ER}^*(\cC_{\rm \GenB{}})$ we have
\begin{align}
   {\sf ER}^*(\cC_{\rm \GenB{}})&\le
   \min_{\Db\; {\rm diagonal}} \fronorm{\A-\Db - \Ub_{r*}\bSigma_{r*} \Vb_{r*}^\sT}\nonumber\\
   &= \min_{\Db\; {\rm diagonal}} \fronorm{\bDelta+\I-\Db}^2\nonumber\\
   &= \min_{\widetilde{\Db}\; {\rm diagonal}} \fronorm{\bDelta-\widetilde{\Db}}^2 \nonumber\\
   &= \fronorm{\bDelta}^2 - \sum_{i=1}^d \Delta_{ii}^2\,.\label{eq:Err2-B1}
\end{align}
In addition, since \GenB{} optimizes over a larger class of models (using diagonals instead of scale of identity), we have
\begin{align}\label{eq:Err2-B2}
{\sf ER}^*(\cC_{\rm \GenB{}}) \le {\sf ER}^*(\cC_{\rm \GenA{}}) \le \fronorm{\bDelta}^2 - \frac{1}{d-r_*}\tr(\bDelta)^2
\end{align}
Combining~\eqref{eq:Err2-B1} and~\eqref{eq:Err2-B2} we obtain the claimed upper bound on ${\sf ER}^*(\cC_{\rm \GenB{}})$.

%====================================
\section{Proof of Theorem~\ref{thm:trade-off}}
Let $p:= 2dr_*$ where we recall that $r_* = \sum_{\ell=1}^T r_\ell$. Note that $p$ is the number of parameters for ResNet with $T$ layers and ranks $r_t$ for each layer $t$. We will compare the test error of \GenA{} and  ResNet with $p$ number of parameters. This corresponds to a model in \GenA{} with $T'$ layers such that $2d\sum_{\ell=1}^{T'} r_\ell+T'(T'-1)/2 = p$. We set the shorthand $r'_*:=\sum_{\ell=1}^{T'}r_\ell$ and let $\sigma_1\ge \dotsc\ge \sigma_{d}$ be the singular values of $\A-\I$. By Theorem~\ref{thm:comp} we have
\[
{\sf ER}^*(\cC_{\rm res}) = \sum_{i=r_*+1}^d \sigma_i^2\,,\quad 
{\sf ER}^*(\cC_{\rm \GenA{}}) \le \sum_{i=r'_*+1}^d \sigma_i^2 - \frac{1}{d-r'_*} \Big(\sum_{i=r'_*+1}^d \sigma_i \Big)^2
\]
Therefore, ${\sf ER}^*(\cC_{\rm \GenA{}}) < {\sf ER}^*(\cC_{\rm res})$ if the following holds:
\begin{align}\label{eq:condition-sup-tradeoff}
\sum_{i=r'_*+1}^{r_*} \sigma_i^2
\le \frac{1}{d-r'_*} \Big(\sum_{i=r'_*+1}^d \sigma_i \Big)^2
\end{align}
(Note that $r'_*< r_*$ since $T'<T$). However note that the left hand side of this condition is upper bounded by
\[
\sum_{i=r'_*+1}^{r_*} \sigma_i^2 \le (r_*-r'_*) \lambda_{\max}^2 
\]
Additionally, the right-hand side of the condition is lower bounded by 
\[
\frac{1}{d-r'_*} \Big(\sum_{i=r'_*+1}^d \sigma_i \Big)^2 \ge (d-r'_*)\lambda_{\min}^2\,.
\]
So a sufficient condition for~\eqref{eq:condition-sup-tradeoff} is that 
\[
(r_*- r'_*) \lambda_{\max}^2\le (d-r'_*)\lambda_{\min}^2\,.
\]
Writing it in terms of $\kappa$, we need 
\begin{align}\label{eq:req-r'}
r_*\le d\kappa^2+r'_*(1-\kappa^2).
\end{align}
Our next lemma gives alower bound on $r'_*$.
\begin{lemma}\label{lem:r-r'-r''}
Consider a standard Resnet model with collective rank $r_*$, and also a \GenA{} model with collective rank $r'$ and a \GenB{} model with collective rank $r''$, which have the same number of parameters as in the standard Resent model. We then have
\begin{align}
    r_* - (\sqrt{d+r_*}-\sqrt{d})^2 &\le r'_*\le r_*\,,\\
    r_* - (\sqrt{1+r_*}-1)^2 &\le r''_*\le r_*\,.
\end{align}
\end{lemma}
Using Lemma~\ref{lem:r-r'-r''}, condition~\ref{eq:req-r'} is satisfied provided that
\[
r_*\le d\kappa^2 + (1-\kappa^2) \Big[r_* - (\sqrt{d+r_*} - \sqrt{d})^2 \Big]\,.
\]
Solving the above inequality for $r_*/d$ and after some algebraic calculation, we simplify the above inequality as follows:
\[
\frac{r_*}{d} \le (1+\kappa(\sqrt{\kappa^2+1}-\kappa))^2 - 1\,.
\]
%Given that $r'_*\ge 0$ and $\kappa\le 1$, this inequality is satisfied if $r_*\le d\kappa^2$.

For \GenB{}, the argument goes along the same lines. Fixing number of parameters to $p$, this corresponds to a model in \GenB{} with $T''$ layers such that $2d\sum_{\ell=1}^{T''}r_{\ell}+dT''(T''-1)/2 = p$. We use the shorthand $r''_*:= \sum_{\ell=1}^{T''}r_\ell$. By Theorem~\ref{thm:comp}, 
\begin{align*}
{\sf ER}^*(\cC_{\rm \GenB{}}) 
%&\le \fronorm{\bDelta}^2 - \sum_{i=1}^d \Delta_{ii}^2\\
%&\le \fronorm{\bDelta}^2 - \frac{1}{d}\Big(\sum_{i=1}^d \Delta_{ii}\Big)^2\\
&= \fronorm{\bDelta}^2 - \frac{1}{d-r''_*}\tr(\bDelta)^2\\
& = \sum_{i=r''_*+1}^d \sigma_i^2 - \frac{1}{d-r''_*}\Big(\sum_{i=r''_*+1}^d \sigma_i^2 \Big)\,.
\end{align*}
%where $r''_*:= \sum_{\ell=1}^{T''}r_\ell$ the second inequality is an application of Cauchy–Schwarz inequality. 
Following the same argument as the one for \GenA{} (replacing $r'_*$ with $r''_*$) we derive that \GenB{} achieves a better trade-off than standard ResNet, if
\begin{align}\label{eq:req-r''}
r_*\le d\kappa^2+r''_*(1-\kappa^2).
\end{align}
(Note that this is analogous to~\eqref{eq:req-r'} where $r'_*$ is replaced by $r''_*$.)

Using Lemma~\ref{lem:r-r'-r''}, condition~\ref{eq:req-r''} is satisfied provided that
\[
r_*\le d\kappa^2 + (1-\kappa^2) \Big[r_* - (\sqrt{1+r_*} - 1)^2 \Big]\,.
\]
By some algebraic calculation, this inequality can be simplified to
\[
r_*\le (1+(\sqrt{d+1}-1)\kappa)^2-1\,.
\]
This completes the proof of the first item in the theorem statement.

To prove the second item in the theorem statement, we write
\begin{align*}
G_1: ={\sf ER}^*(\cC_{\rm res}) - {\sf ER}^*(\cC_{\rm \GenA{}}) &\ge 
 \frac{1}{d-r'_*} \Big(\sum_{i=r'_*+1}^d \sigma_i \Big)^2 -\sum_{i=r'_*+1}^{r_*} \sigma_i^2\\
 & \ge (d-r'_*)\lambda_{\min}^2 - (r_*-r'_*)\lambda_{\max}^2\\
 & = d\lambda_{\min}^2 - r_* \lambda_{\max}^2 +r'_*(\lambda_{\max}^2- \lambda_{\min}^2)\\
 &\ge d\lambda_{\min}^2 - r_* \lambda_{\max}^2 + (r_* - (\sqrt{d+r_*}-\sqrt{d})^2)(\lambda_{\max}^2- \lambda_{\min}^2)\\
 &= (d-r_*)\lambda_{\min}^2 - (\sqrt{d+r_*}-\sqrt{d})^2(\lambda_{\max}^2- \lambda_{\min}^2)\,,
\end{align*}
where in the last inequality we used Lemma~\ref{lem:r-r'-r''}.

A similar bound can be derived for \GenB{}, replacing $r'_*$ with $r''_*$ in the argument. Specifically, we have
\begin{align*}
G_2: ={\sf ER}^*(\cC_{\rm res}) - {\sf ER}^*(\cC_{\rm \GenB{}}) 
 & \ge  d\lambda_{\min}^2 - r_* \lambda_{\max}^2 +r''_*(\lambda_{\max}^2- \lambda_{\min}^2)\\
 &\ge d\lambda_{\min}^2 - r_* \lambda_{\max}^2 + (r_* - (\sqrt{1+r_*}-1)^2)(\lambda_{\max}^2- \lambda_{\min}^2)\\
 &= (d-r_*)\lambda_{\min}^2 - (\sqrt{1+r_*}-1)^2)(\lambda_{\max}^2- \lambda_{\min}^2)\,,
\end{align*}
where in the last inequality we used the lower bound given for $r''$ in Lemma~\ref{lem:r-r'-r''}.

%=====================
\subsection{Proof of Lemma \ref{lem:r-r'-r''}} A standard Resnet model with collective rank $r_*$ has $2dr_*$ number of parameters. A model in \GenA{} with collective rank $r'_*$ has $2dr'_* + T'(T'-1)/2$ parameters. Therefore, by assumption
\begin{align}\label{eq:T'-ineq}
2dr'_* + T'(T'-1)/2 = 2dr_*\,.
\end{align}
Since each layer has rank at least one, we also have $r'_*\ge T'$. We define the shorthand $\xi= \sqrt{T'(T'-1)}$ (so $r'\ge \xi$). Combining these two inequalities and writing them in terms of $\xi$, we get
\[
2d \xi + \xi^2/2\le 2dr_*\,.
\]
Solving this inequality for $\xi$ we get $\xi\le 2\sqrt{d^2+dr_*}-2d$. Using this bound in~\eqref{eq:T'-ineq} we get
\[
2dr_* \le 2dr'_* + 2(\sqrt{d^2+dr_*}-d)^2\,.  
\]
Simplifying this inequality, we arrive at
\[
r_* - (\sqrt{d+r_*}-\sqrt{d})^2 \le r'_*\,.
\]
The upper bound $r'_*\le r_*$ also follows simply from \eqref{eq:T'-ineq}.

For \GenB{} model we follow the same argument. A model in \GenB{} with collective rank $r''$ has
$2dr''_*+ d T''(T''-1)/2$ parameters and so
\begin{align}\label{eq:T''-ineq}
2r''_*+  T''(T''-1)/2 = 2r_*\,.
\end{align}
Since each layer has rank at least one, we also have $r''_*\ge T'$. Define the shorthand $\xi':= \sqrt{T''(T''-1)}$. Combining the previous two equation, we get
\[
2\xi' +  \xi'^2/2\le 2r_*\,.
\]
Solving this inequality for $\xi'$ we get $\xi'\le 2\sqrt{1+r_*}-2$. Using this bound back in~\eqref{eq:T''-ineq} we obtain
\[
2r_*\le 2r''_*+  2(\sqrt{1+r_*}-1)^2\,.
\]
This simplifies to $r_* - (\sqrt{1+r_*}-1)^2\le r''_*$.

%===============================
\section{Proof of Proposition~\ref{pro:rank-reduction}}
The result follows from conditions~\eqref{eq:req-r'} and \eqref{eq:req-r''} which provide sufficient condition for \GenA{} (respectively \GenB{}) to achieve smaller test error than a ResNet model, with the same number of parameters.
%===============================
\section{Proof of Proposition~\ref{pro:nonlinear}}
The proof is similar to the linear case by induction on $T$. Note that for showing this direction ($\cC_{{\rm base}},\cC_{{\rm res}},\cC_{{\rm \GenA{}}}, \cC_{{\rm \GenB{}}}$ being a subset of the rank constrained functions) we only used the following two properties of the rank function which holds also for the Bottleneck rank: $\rank(f\circ g)\le \min\{\rank(f), \rank(g)\}$ and 
     $\rank(f+g)\le \rank(f)+\rank(g)$.