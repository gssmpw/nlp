\section{Introduction}
\label{sec:introduction}

Residual connections play an important role in modern neural network architectures because they stabilize the training of deep neural networks and improve model convergence and quality. Since their usage in the ResNet architecture \cite{he2016deep}, residual connections have been widely adopted in both convolutional neural networks and transformer architectures across various domains, including natural language processing \cite{vaswani2017attention}, audio recognition \cite{gong21b_interspeech}, and computer vision \cite{dosovitskiy2020image}.

A residual neural network (ResNet) is constructed by stacking layers known as residual blocks. Each residual block is characterized by the recursive equation $\x_{t+1} = f(\x_t) + \x_t$, which contains a residual function $f$ along with an identity shortcut (also called an identity loop or skip connection). The residual functions typically used in these blocks include multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and attention. 
% The $t$-th residual block outputs $g_{t+1}(\x)$ which is defined recursively as
% \[
% g_{t+1}(\x) = f_t(g_t(\x))+ g_t(\x)\,.
% \]
% Using this recursion, the output of the $T$-th residual block
% is given by
% \[
% g_{T+1}(\x) = \sum_{t=0}^T f_t(g_t(\x))\,,
% \]
% with the conventions that $g_0(\x) = \zeros$ and $f_0(g_0(\x)) = \x$. We refer to Figure~\ref{fig:resnet} for a schematic illustration. 
% Each residual block is characterized by the recursive equation $x_{t+1} = f(x_t) + x_t$.
% where $f(x_t)$ is the residual function and $x_t$ denotes the identity (or skip) connection.  
By unrolling the recursion, we equivalently see that each layer's input
is the sum
of all its previous layers' outputs (including the model's input).
Figure~\ref{fig:resnet} provides a schematic illustration of this concept. 

\begin{figure*}[t!]
\centering
\begin{subfigure}[b]{0.46\textwidth}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/reconstruction.pdf}
    \caption{Learning the identity transformation
    by minimizing $\left\|f(\x) - \x\right\|_2^2$, where
    $\x$ is a $100$-dimensional i.i.d. normal input and $f$ is a low-rank linear network.}
\end{subfigure}%
\hspace{0.25in} 
\begin{subfigure}[b]{0.46\textwidth}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/regression.pdf}
    \caption{Minimizing the loss $\left\|f(\x) - \y\right\|_2^2$, where
    $\x$ is a $100$-d i.i.d. normal input, $f$ is a low-rank linear network, and $\y = \A\x + \bb$, where
    $\A, \bb$ have i.i.d. standard normal entries.}
\end{subfigure}
\caption{Training low-rank linear models to learn the identity and a random transformation. Each model consists of 
10 linear layers, each of rank 3, and is trained using mini-batch SGD.}\label{fig:linear_lowrank_experiments}
\end{figure*}

\noindent{\bf Information dilution in residual networks.} Residual connections increase the
flow of information across the neural network. However,
they also come with a potential limitation:
Taking a straight sum of previous layer outputs
implicitly treats all previous layers as equally important. This can
dilute useful information present in a select few
layers (including the model's input) with potentially less useful
information. We hypothesize that, because of this dilution,
even though residual networks mitigate the problem of neural network bottlenecks, they do not sufficiently resolve it. 
One way to resolve the issue of dilution would be to allow
each layer to \emph{choose its inputs}.

In order to confirm the existence and significance of the dilution phenomenon we ask a simple question: 
\emph{Can residual networks easily learn to recover the input?}
This should be a basic task expected of any generative
model --- otherwise there would be information loss. However,
if our dilution hypothesis is true, the answer would be negative.
To test this, we create a neural network consisting of a
number of low-rank layers, and add residual connections in order to
mitigate the bottlenecks introduced by the low ranks. The resulting
model is full-rank. We compare this model with another model that 
employs \emph{learnable} residual
connections, as in DenseFormer~\cite{pagliardini2024denseformer},
which we later also call \emph{GRN-v1}, since it is the starting point
of our generalizations. In Figure~\ref{fig:linear_lowrank_experiments} we see the results of the two models on two tasks: 
learning the identity transformation and learning a random
linear transformation.
Perhaps surprisingly, we observe that the 
residual network is unable to fully reconstruct the input even after
seeing $10^3$ batches ($10^5$ examples),
while the model with learnable residual
weights is able to reach extremely small loss values, even
with 100x fewer examples. This confirms that ResNet does not address
neural network bottlenecks in a satisfactory way, even though it learns a full-rank transformation, and underscores the importance of
using learnable residual weights to increase model capacity.

%To increase the information capacity, the width of the model can be increased, but this introduces many additional parameters and makes the model significantly more expensive for both training and inference.

\noindent{\bf Our contribution.} In this work, we propose \emph{DeepCrossAttention (DCA)}, a new
transformer architecture that generalizes residual networks by employing learnable, input-dependent weights to dynamically combine layer outputs, enabling the model to selectively focus on the most relevant information in any of the previous layers and thereby prevent dilution of information in the hidden representations. 
Furthermore, DCA incorporates depth-wise cross-attention by enabling the queries, keys, and values in each transformer block to independently combine layer outputs, allowing for richer interactions between layers at different depths.
% By training a weight for each skip connection, the network can learn to filter out layers with low importance
This is all achieved with a negligible number of additional parameters, making DCA more effective than increasing the model size (for instance by increasing its width or depth).

DCA can be viewed as a mechanism to adapt the model architecture dynamically for each input token. By optimizing the added parameters, DCA learns to effectively combine the outputs of earlier residual blocks. This allows the model to rearrange the residual blocks from purely sequential to fully parallel and any intermediate combination, without the need for explicit architectural design choices.

% An interesting interpretation of DCA is that it is optimizing the model architecture during training as it is able to arrange the residual blocks in parallel or sequential, and anything in between, by changing which layer output to focus on. 
% If each layer in a model is viewed as a compute unit, then a residual network can be viewed as a fully-connected directed acyclic graph (DAG) with edge weights of one. This is corresponds to an upper triangular matrix matrix with all ones. By learning the weights during training, it is essentially assigning importance to the flow of information from each layer to all following layers. 
% The model can thus learn to utilize fully parallel and sequential computation and anything in between.

% \paragraph{Faster training} ``Small-scale models have also shown a rapid
% increase in performance, but these gains are
% largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al.,
% 2022), and the latest small models require up to
% 15T tokens to improve the state of the art by less
% than 1-2\% (AI@Meta, 2024)." \citep{team2024gemma}. While our method achieves the same accuracy about 3 to 4 times faster, thus potentially saving 10T tokens.

We analyse our generalization of the residual network theoretically by focusing on a linear low-rank model. We show that DCA achieves a better trade-off between accuracy and model size when the ratio of the collective ranks of the layers to the ambient dimension is below a threshold, which depends on the complexity of the target task. In addition, the improvement in this trade-off can itself be characterized as a function of the collective ranks of the layers, ambient dimension and the complexity of the target task. We extend this insight to nonlinear models by working with the notion of bottleneck rank, proposed by \citet{jacot2022implicit}. 

We additionally provide empirical results to support the theoretical findings and demonstrate the effectiveness of DCA. Experiments on language modeling tasks demonstrate that DCA consistently outperforms the standard transformer architectures in terms of both perplexity and training efficiency. DCA achieves lower perplexity for a given parameter budget and training time. Furthermore, DCA exhibits improved training stability, mitigating the occurrence of loss spikes frequently observed while training large models.

