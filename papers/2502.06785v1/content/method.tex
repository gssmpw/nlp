\section{Method}
\label{sec:method}

% \paragraph{Residual neural network}
% A residual neural network (ResNet) is constructed by stacking units known as residual blocks. Each residual block contains a neural network module along with an identity shortcut (also called an identity loop or skip connection). Typical modules used in these blocks include MLPs (multi-layer perceptrons), CNNs (convolutional neural networks) and attention.  

We start with a detailed exposition of our proposed generalizations to the residual network architecture. We present three distinct proposals, each incrementally augmenting the complexity of the network structure. Building upon these proposals, we subsequently introduce DeepCrossAttention (DCA), a novel approach to enhance residual learning capabilities of the transformer architecture.

\noindent{\bf Notation.} We denote a residual function by $f_t:\reals^d\to\reals^d$, where $t$ is the layer index and $d$ the feature dimension. As an example, in a multi-layer perceptron residual network (MLP-ResNet), we have $f_t(\x) =\V_t \sigma(\W_t \x)$ with $\W_t\in \reals^{k\times d}$, $\V_t\in\reals^{d \times k}$ and $\sigma$ is a nonlinear function, such
as sigmoid or ReLU, that is applied component-wise. Then, the
$t$-th residual block outputs $g_{t+1}(\x)$, defined recursively as
\[
g_{t+1}(\x) = f_t(g_t(\x))+ g_t(\x)\,.
\]
Using this recursion, the output of the $T$-th residual block
is given by
\[
g_{T+1}(\x) = \sum_{t=0}^T f_t(g_t(\x))\,,
\]
with the conventions that $g_0(\x) = \zeros$ and $f_0(g_0(\x)) = \x$.  We refer to Figure~\ref{fig:resnet} for a schematic illustration.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/resnet.png}
    \caption{Two alternative schematic representations of standard ResNet. The top represents the recursive form, the bottom represents the explicit sum.}
    \label{fig:resnet}
\end{figure}

An alternative description, which we will use to introduce our generalizations, is the following. For every $t$, define the stack of layer outputs $\Gb_t\in \reals^{d\times t}$ as
\[
\Gb_t: = \begin{bmatrix}
f_{t-1}(g_{t-1}(\x)), \dotsc, f_0(g_0(\x))
\end{bmatrix}\in\reals^{d\times t}\,.
\]
We then have $g_t(\x) = \Gb_t \onebb$ and $\y = \Gb_{T}\onebb$ in the standard residual network, where $\onebb$ denotes the all ones vector.


\subsection{Generalized Residual Networks (GRN)}
\label{sec:grn}

We propose three generalizations of ResNets by considering weighted linear combinations of previous layer outputs. The parameters of the modules and the generalizations are all optimized during training using the AdamW optimizer \cite{loshchilov2017decoupled}.
% \medskip

\noindent{\bf Dimension-independent weights (\GenA{})}.  We consider  simple linear combinations as 
\[
g_t(\x) = \Gb_t\bb_t,\quad \y = \Gb_{T+1}\bb_{T+1}\,
\]
with $\bb_t\in\reals^{t\times 1}$ which is initialized as all ones and optimized with the rest of the model parameters during training. 
This setting has been previously explored in the
DenseFormer paper~\cite{pagliardini2024denseformer}.

% Alternatively, each $\bb_t$ (but not $\bb_{T}$) can be initialized as the first standard basis vector which results in an ensemble of single module models at initialization. Each module can then be informed by the prior modules in the ensemble during training to improve the model quality.


% \medskip

\noindent{\bf Dimension-dependent weights (\GenB{}).} In this proposal, we allow $\bb_t\in \reals^{d\times t}$ and consider
\[
g_t(\x) = (\Gb_t\odot\bb_t)\onebb,\quad \y = (\Gb_{T+1}\odot\bb_{T+1})\onebb\,,
\]
where $\odot$ indicates the entry-wise (Hadamard) product. Note that in \GenA{} the same weight vector $\bb_t$ is used for each of the $d$ features. \GenB{} generalizes this by using different weight vectors for different features, which are all stacked together in a matrix $\bb_t\in\reals^{d\times t}$.

%\medskip

% \noindent{\bf Gen3 (input-dependent weights).} In the next generalization, we allow the weight matrices to be data dependent. Before describing the model, we define the following two operators: 
% \begin{itemize}
%     \item Flattening operator $\cF$ (a.k.a vectorization) which converts the matrix into a vector. Specifically, flattening (vectorization) of a $m \times n$ matrix $\A$, denoted $\cF(\A)$, is the $mn \times 1$ column vector obtained by stacking the columns of the matrix $\A$ on top of one another.
%     \item Reshapring operator $\cR$ which converts a vector into a matrix. Specifically, for a $mn\times 1$ column vector $\vb$, its reshaped form denoted by $\cR_{m,n}(\vb)$ reshaped $\vb$ into a $m\times n$ matrix. 
% \end{itemize}
% Note that flattening and reshaping operators are inverse of each other.

% In this proposal we consider the following recursion
% \begin{align*}
% &g_t(\x) = (\Gb_t\odot(\bb_t+ \wb_t))\onebb\,,\quad \wb_t = \cR_{d,t}(\W_t\cF(\Gb_{t}))\,,\\
% &\y =  (\Gb_{T+1}\odot(\bb_{T+1}+ \wb_{T+1}))\onebb
% \end{align*}
% where $\W_t:\reals^{dt\times dt}$ is initialized as all zeros and optimized with the rest of the model parameters during training. Here, $\wb_t$ represents the input-dependent part of the weight matrix. 
% % Focusing on the expression for $\wb_t\in\reals^{d\times t}$, note that for every feature $i\in [d]$ and  it is obtained by taking linear combinations \emph{jointly} across all features and all previous layers, and these  using a $dt$ dimensional vector $\w_t$.  
% \medskip

\begin{figure}[t!]
    \centering
    % \hspace*{0.3in}
    \includegraphics[width=0.7\linewidth]{figures/grn-diagram.png}
    \vskip -0.1in
    \caption{Computation diagram of \GenC{}.}
    \label{fig:grn-diagram}
\end{figure}

\noindent{\bf Input-dependent weights (\GenC{}).} In the next generalization, we allow the weights to be input dependent. Specifically, the weights are given by $\bb_t+\wb_t$ with $\bb_t,\wb_t\in\reals^{d\times t}$. The first component acts similar to the weights in \GenB{}, it puts different weights on different dimensions of the input.  The second component  $\wb_t$ is a nonlinear mapping of the input features vector $\x$, but is the same for all the $d$ dimensions. This combination gives us flexibility to have both dimension-dependent and input-dependent weights for a slight increase in the number of parameters. \GenC{} is expressed as \begin{align*}
&g_t(\x) = (\Gb_t\odot(\bb_t+ \wb_t))\onebb\,,\quad \wb_t =\onebb\sigma(\w_t^\sT\Gb_t)\,,\\
&\y = (\Gb_{T+1}\odot(\bb_{T+1}+ \wb_{T+1}))\onebb
\end{align*}
where $\w_t:\reals^{d\times 1}$ is initialized as all zeros and optimized with the rest of the model parameters during training and $\sigma:\reals\to\reals$ is a non-linearity which is applied entry-wise.
% (i.e., for a vector $\vb\in \reals^{n\times 1}$, $\sigma(\vb) = (\sigma(v_1),\dotsc, \sigma(v_n))$). 
In this proposal we consider $\sigma$ to be the ReLU activation. The computation diagram of \GenC{} is illustrated in Figure~\ref{fig:grn-diagram}.



%=================
% \medskip

\noindent{\bf Reducing memory and computation.} Since the stack of layer outputs $\Gb_t$ grows linearly with the depth of the model, this could lead to significant memory and computational overhead for deep models. Our experiments reveal that GRNs tend to weight inputs and the last few layer outputs the most. An example weight distribution is provided in Appendix~\ref{app:weight-distribution}. Therefore, to increase efficiency, we propose to include only the first and last-$k$ layers explicitly in $\Gb_t$. On the intermediate layers we apply standard ResNet, only involving simple addition. For example, if we set $k=2$, then $\Gb_t$ contains at most 4 vectors: the model inputs, the sum of the intermediate layers' outputs, and the last two layers' outputs $f_{t-1}(g_{t-1}(\x))$ and $f_{t-2}(g_{t-2}(\x))$. The GRNs then take this modified $\Gb_t$ as their input.

\subsection{DeepCrossAttention}
\label{sec:deepcrossattention}

The generalizations introduced thus far are generally applicable to any ResNet. We now describe our main method which is specific to the transformer architecture. DeepCrossAttention (DCA) generalizes self-attention by adding three independent instances of a GRN in each decoder block. In this proposal we consider the GRN to be \GenC{}. These three GRN instances are given the same stack of previous layer outputs as their input but return the queries, keys, and values for the attention module, respectively. This enables richer interactions between layers at different depths.
% We call it deep cross attention because it is able to apply attention across the depth of the model.
Figure~\ref{fig:dca-diagram} shows the computation diagram of a DCA decoder block inside a transformer, where
the remaining skip connections ensure that the inputs are not added to the outputs of the decoder block, but are included in the inputs of both the attention and the feed forward module. 
Notably, DCA does not modify the underlying attention mechanism, but instead uses GRNs to dynamically compose attention inputs.

\begin{figure}[t!]
    \centering
    % \hspace*{0.3in}
    \includegraphics[width=0.65\linewidth]{figures/dca-diagram.png}
    \vskip -0.1in
    \caption{Computation diagram of a DCA decoder block.}
    \label{fig:dca-diagram}
\end{figure}

