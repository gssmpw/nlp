\section{Preliminary and Related Works}
In this section, we first introduce large language models and then present existing benchmarks.


\subsection{Large Language Models}
Large language models (LLMs), such as GPT-4~\citep{achiam2023gpt}, 
DeepSeek~\citep{liu2024deepseek}, 
PaLM~\citep{anil2023palm}, 
and LLaMA~\citep{touvron2023llama}, 
have revolutionized natural language processing (NLP) through their ability to generate coherent text~\citep{cho2019coherentcohesivelongformtext}, 
answer questions~\citep{chen2024analyze}, 
and adapt to diverse tasks~\citep{wang2025graph,jiang2024survey}. 
Their success stems from pretraining on vast text corpora using next-token prediction objectives, 
which enable generalization on tasks requiring semantic understanding, commonsense reasoning, and linguistic creativity.
However, this training paradigm encourages LLMs to prioritize surface-level statistical patterns 
(e.g., lexical co-occurrences, syntactic regularities) rather than numerically grounded reasoning~\citep{bachmann2024pitfalls}. 
Consequently, LLMs treat numbers as discrete tokens rather than continuous magnitudes, inherently limiting their ability to understand exact numerical semantics. 
This leads to errors in numeric retrieval, arithmetic operations, and magnitude comparisons~\citep{qiu2024dissecting}.







\subsection{Benchmarks on Large Language Models}

Existing benchmarks~\cite{li2024survey,chang2024survey,zhao2023survey} for evaluating LLMs primarily fall into two categories, 
i.e., \textit{semantic-oriented} and \textit{math-oriented} benchmarks.
Specifically,
\textit{semantic-oriented} 
benchmarks, 
such as GLUE~\citep{wang2018glue},  
SuperGLUE~\citep{wang2019superglue}, 
SimpleQA~\citep{wei2024measuring}, 
and LongBench~\citep{bai2023longbench},
focus on semantic understanding and linguistic competence, 
testing skills like textual entailment, 
commonsense reasoning, and domain-specific knowledge (e.g., science and law). 
While effective for assessing linguistic proficiency, these benchmarks largely overlook numerical reasoning.
On the other hand,
\textit{math-oriented} benchmarks~\cite{gao2025gllava,li2024forewarned,cobbe2021training}, 
such as MathQA~\citep{amini2019mathqa}, 
GSM8K~\citep{cobbe2021training}, 
and MathBench~\citep{liu2024mathbench},
target mathematical problem-solving (e.g., algebra, calculus) or extractive question-answering with numerical answers. 
However, these datasets emphasize well-formed mathematical problems  in controlled and clean settings.
Consequently,
\textit{math-oriented} benchmarks poorly evaluate numerical retrieval and reasoning in real-world conditions, where noise, and contextual complexity (e.g., multi-step financial workflows or long stock sequences) are common.

Considering that numeric retrieval and reasoning are critical for real-world applications, 
such as finance~\citep{islam2023financebench} and weather forecasting~\cite{zhang2024self}, 
we propose \textbf{\textit{NumericBench}} to systematically evaluate the fundamental numerical abilities of  LLMs on intensive tasks, such as precise value retrieval, dynamic comparisons, and arithmetic-logical reasoning.








