\section{Experiments}

\subsection{Experiment Setting}

\noindent\textbf{Benchmarks and Evaluated Protocols.}
The statistic of  NumericBench is provided in Table~\ref{tab:data_stat}.
Also,
we set the exact answer for mixed-number-string dataset, 
set
the computed answer to two decimal places for arithmetic datasets, and  set the answer of each question as a single choice (e.g., A, B, or C) for other datasets to reliable evaluate LLMs~\citep{bai2024longbench}.
The evaluation metric is accuracy.

\begin{table*}[t]
	\centering
		\vspace{-2em}
	\setlength\tabcolsep{2pt}
	\footnotesize
	\caption{Evaluation of LLMs on numerical contextual retrieval, comparison, and summary tasks across number list, stock, and weather datasets. 
		Also, * indicates that scores are calculated based on a short subset of outputs, as these models cannot handle  long contexts and exhibit disruption when tested on longer instances.}
	\begin{tabular}{c|ccc|ccc|ccc|c}
		\toprule
		\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Retrieval}}                                                         & \multicolumn{3}{c|}{\textbf{Comparison}}                                                           & \multicolumn{3}{c|}{\textbf{Summary}}                                                           & \textbf{Logic}    \\ \cmidrule{2-11} 
		
		& \multicolumn{1}{c}{\textbf{Number}} & \multicolumn{1}{c}{\textbf{Stock}} & \textbf{Weather} & \multicolumn{1}{c}{\textbf{Number}} & \multicolumn{1}{c}{\textbf{Stock}} & \textbf{Weather} & \multicolumn{1}{c}{\textbf{Number}} & \multicolumn{1}{c}{\textbf{Stock}} & \textbf{Weather} & \textbf{Sequence} \\ \midrule
		
		\textbf{\texttt{Random}} & \multicolumn{1}{c}{12.5}                  & \multicolumn{1}{c}{12.5}               &          12.5        & \multicolumn{1}{c}{12.5}                  & \multicolumn{1}{c}{12.5}               &                 12.5 & \multicolumn{1}{c}{12.5}                  & \multicolumn{1}{c}{12.5}               &      12.5            &         12.5          \\ \midrule
		
		
		\textbf{\texttt{Llama-3.1-8B-Inst}}& \multicolumn{1}{c}{22.8}                  & \multicolumn{1}{c}{14.4}               &      13.5          & \multicolumn{1}{c}{19.5}                  & \multicolumn{1}{c}{11.7}               &     13.7            & \multicolumn{1}{c}{18.1}                  & \multicolumn{1}{c}{13.8}               &       13.9*          &       18.2            \\  
		
		\textbf{\texttt{Llama-3.1-70B-Inst}}& \multicolumn{1}{c}{37.3}                  & \multicolumn{1}{c}{17.4}               &     23.0             & \multicolumn{1}{c}{28.3}                  & \multicolumn{1}{c}{15.0}               &     28.7             & \multicolumn{1}{c}{24.7}                  & \multicolumn{1}{c}{16.4}               &       15.2           &     17.8              \\  
		
		\textbf{\texttt{Llama-3.3-70B-Inst}}& \multicolumn{1}{c}{44.4}                  & \multicolumn{1}{c}{19.4}               &      23.1            & \multicolumn{1}{c}{31.5}                  & \multicolumn{1}{c}{13.8}               &       35.8           & \multicolumn{1}{c}{26.3}                  & \multicolumn{1}{c}{16.8}               &   18.0               &     18.6              \\  
		
		\textbf{\texttt{Llama-3.1-405B-Inst}}& \multicolumn{1}{c}{44.6}                  & \multicolumn{1}{c}{26.8}               &          19.8        & \multicolumn{1}{c}{25.1}                  & \multicolumn{1}{c}{14.8}               &     29.8             & \multicolumn{1}{c}{32.9}                  & \multicolumn{1}{c}{17.0}               &    16.1              &     16.6              \\  
		
		\textbf{\texttt{Llama-3.1-Nemotron-70B-Inst}}& \multicolumn{1}{c}{41.6}                  & \multicolumn{1}{c}{19.3}               &        24.9          & \multicolumn{1}{c}{26.6}                  & \multicolumn{1}{c}{13.7}               &      33.6            & \multicolumn{1}{c}{29.4}                  & \multicolumn{1}{c}{16.5}               &     17.0             &     16.4              \\  
		
		\textbf{\texttt{Qwen2.5-7B-Inst}}& \multicolumn{1}{c}{20.2}                  & \multicolumn{1}{c}{17.3}               &    19.6              & \multicolumn{1}{c}{24.8}                  & \multicolumn{1}{c}{17.8}               &      18.8            & \multicolumn{1}{c}{18.5}                  & \multicolumn{1}{c}{11.7}               &     13.8             &    14.4               \\  
		\textbf{\texttt{Qwen2.5-72B-Inst}}& \multicolumn{1}{c}{28.8}                  & \multicolumn{1}{c}{41.4*}               &       12.4*           & \multicolumn{1}{c}{28.0}                  & \multicolumn{1}{c}{26.0*}               &       31.0*           & \multicolumn{1}{c}{31.9}                  & \multicolumn{1}{c}{18.8*}               &        16.4*          &      19.0             \\  
		\textbf{\texttt{GLM-4-Long}}& \multicolumn{1}{c}{26.5}                  & \multicolumn{1}{c}{19.5}               &       8.4           & \multicolumn{1}{c}{18.9}                  & \multicolumn{1}{c}{14.8}               &      21.6            & \multicolumn{1}{c}{20.8}                  & \multicolumn{1}{c}{10.8 }               &      10.5            &        17.6           \\  
		
				\textbf{\texttt{Deepseek-V3}}& \multicolumn{1}{c}{47.2}                  & \multicolumn{1}{c}{47.5}               &       10.9          & \multicolumn{1}{c}{27.0}                  & \multicolumn{1}{c}{22.5}               &       35.8          & \multicolumn{1}{c}{21.8}                  & \multicolumn{1}{c}{13.0}               &       15.1          &   15.8                \\  
		
		\textbf{\texttt{GPT-4o}}& \multicolumn{1}{c}{41.7}                  & \multicolumn{1}{c}{37.5}               &        15.4          & \multicolumn{1}{c}{30.6}                  & \multicolumn{1}{c}{33.0}               &       64.2           & \multicolumn{1}{c}{11.6}                  & \multicolumn{1}{c}{17.4}               &      16.5            &        14.6           \\ 
		

		
		\midrule
		
		\textbf{\texttt{Human Evaluation}}& \multicolumn{1}{c}{\textbf{100}}                  &  \multicolumn{1}{c}{\textbf{100}}               &      \textbf{100}            & \multicolumn{1}{c}{\textbf{100}}                  & \multicolumn{1}{c}{\textbf{100}}               &        \textbf{100}          & \multicolumn{1}{c}{\textbf{100}}                  & \multicolumn{1}{c}{\textbf{100}}               &          \textbf{100}        &               \textbf{52.6}   \\ \bottomrule
	\end{tabular}
	\label{tab:main_experiments}
\end{table*}
\begin{figure*}[t]
		\vspace{-1em}
	\centering 	
	\subfloat[Contextual  Retrieval]	
	{\centering\includegraphics[width=0.33\linewidth]{image/main_fig/retrieval-num-list.pdf}}
	\hfill
	\subfloat[Comparison]
	{\centering\includegraphics[width=0.33\linewidth]{image/main_fig/compare-num-list.pdf}}
	\subfloat[Summary]	
	{\centering\includegraphics[width=0.33\linewidth]{image/main_fig/summary-num-list.pdf}}
	\hfill
	\caption{Evaluation on short and long context on number list.}
	\label{fig:length_number}
	
\end{figure*}
\noindent\textbf{Evaluated Models.}
To comprehensively evaluate the retrieval and reasoning abilities of state-of-the-art and widely-used LLMs on numeric data, 
we benchmark over 10 popular LLMs with our constructed NumericBench, as follows.
\begin{itemize}[leftmargin=*]
	\item \textbf{The Llama Series~\citep{grattafiori2024llama3herdmodels}.} include Llama-3.1-8B Instruct, Llama-3.1-70B Instruct, Llama-3.1-405B Instruct, 
	Llama-3.3-70B-Instruct and Llama-3.1-Nemotron-70B-Instruct.
	%Deepseek-R1~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}, Deepseek R1-Zero, Deepseek-V3~\citep{liu2024deepseek}, GLM-4-Plus~\citep{glm2024chatglm}, GLM-4-Long~\citep{glm2024chatglm}, Claude Sonnet 3.5, Claude 3.5 Haiku, GPT-4o, GPT-4o-mini, GPT-o3 mini, Gemini 2.0 Pro, Llama-3.1-8B/70B/405B Instruct~\citep{grattafiori2024llama3herdmodels}, Llama-3.3-8B/70B Instruct, Llama-3.1-Nemotron-70B-Instruct, Qwen2.5-7B/72B Instruct, InternLM2.5-20B-Chat
	\item \textbf{The Qwen Series~\citep{qwen2025qwen25technicalreport}.} include the effective Qwen2.5-7B-Instruct and Qwen2.5-72B-Instruct. 
	%	\item \textbf{Math-oriented Models} include DeepSeek-Math-Instruct 7B~\citep{deepseek-math}, MetaMath-Llemma-7B~\citep{azerbayev2023llemma}~\citep{yu2023metamath} Mammoth-7B/14B~\citep{yue2023mammoth}	
	\item \textbf{The GLM Series~\citep{glm2024chatglm}.} We use GLM4-Long to run the benchmark, since it is the commonly used in GLM series.
	% due to the overly high price of GLM4-Plus. 
	
	\item \textbf{The Deepseek Series~\citep{liu2024deepseek}~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}.} We currently use Deepseek V3 to run the benchmark. 
	Deepseek R1 will be evaluated in the future, since its API is down and unavailable now . 
	\item  \textbf{The GPT Series~\cite{achiam2023gpt}.} We use GPT-4o to run the benchmark. 
\end{itemize}

 
	We attempted to conduct experiments   on various math-oriented LLMs, such as Metamath-Llemma-7B~\citep{yu2023metamath}, Deepseek-Math-7B-instruct~\citep{deepseek-math}, InternLM2-Math-7B~\citep{ying2024internlmmathopenmathlarge} and MAmmoTH-7B~\citep{yue2023mammoth}.
	 However, these models fail during experiments for various reasons such as overly long output sequence length and limited input sequence length. Fail cases are demonstrated in the Figure~\ref{fig:fail_internlm},  Figure~\ref{fig:fail_ds_math},  Figure~\ref{fig:fail_llemma}, and  Figure~\ref{fig:fail_mammoth} in Appendix. 



\subsection{Main Experiments}
\noindent \textbf{Evaluation on Contextual Retrieval, Comparison, Summary, and Logic Reasoning Abilities.}
As shown in Table~\ref{tab:main_experiments}, 
current popular and effective LLMs perform poorly on basic numerical tasks, 
including retrieval, comparison, summarization, and logical reasoning. 
The random baseline for each task is 12.5\%, as there are 8 choices, and the probability of randomly selecting the correct answer is 1/8. 
Human evaluation was conducted by three undergraduate students. 

Firstly,
LLMs particularly struggle with accurately retrieving numerical data.
This limitation arises from LLMs treating numbers as discrete tokens rather than continuous ones, coupled with insufficient exposure to structured numerical datasets during training, which restricts their ability to handle simple numeric retrieval tasks. 
Secondly, LLMs demonstrate weaknesses in recognizing numerical relationships, such as greater-than or less-than comparisons, due to a lack of numerical semantics and underdeveloped arithmetic reasoning capabilities. 
Thirdly,
LLMs also perform poorly in summarizing numerical data (e.g., calculating sums or means), reflecting their inability to execute multi-step numerical operations. 
Similarly, logical reasoning tasks, especially those involving patterns or sequences, are particularly challenging, with all models scoring below 20\%. 
These tasks require multi-step reasoning, pattern recognition, and arithmetic operations, which expose the architectural limitations of current LLMs.









\begin{figure*}[t]
	\vspace{-2em}
	\centering 	
	\subfloat[Contextual  Retrieval]	
	{\centering\includegraphics[width=0.33\linewidth]{image/noisy_dataset_fig/retrieval-noisy-stock.pdf}}
	\hfill
	\subfloat[Comparison]
	{\centering\includegraphics[width=0.33\linewidth]{image/noisy_dataset_fig/compare-noisy-stock.pdf}}
	\subfloat[Summary]	
	{\centering\includegraphics[width=0.33\linewidth]{image/noisy_dataset_fig/summary-noisy-stock.pdf}}
	\hfill
 
	\caption{Evaluation on noisy stock dataset. Due to the input sequence length limit of Qwen2.5-72B-Inst on the API platform, the data containing 6 irrelevant attributes cannot be evaluated using this model.}
	\label{fig:noisy_stock}
	
\end{figure*}




\begin{figure*}[t]
	\vspace{-1em}
	\centering 	
	\subfloat[Accuracy on $Q_{oper}$  (i.e., $a+b$)]	
	{\centering\includegraphics[width=0.32\linewidth]{image/arithmetic_fig/arith_bar.pdf}}
	\hfill
	\subfloat[ $Q_{oper}$  of different digits]
	{\centering\includegraphics[width=0.32\linewidth]{image/arithmetic_fig/arith_plot.pdf}}
	\subfloat[Accuracy on $Q_{context}$  (i.e., $a$ plus $b$)]
	{\centering\includegraphics[width=0.32\linewidth]{image/arithmetic_text_fig/arith_bar.pdf}}
 
	\caption{Evaluation on arithmetic operation.}
	\label{fig:arithmetic_fig}
		\vspace{-1em}
\end{figure*}



 



 
\noindent \textbf{Evaluation on  Different Context Length via Stock and Weather Datasets.}
We evaluate LLMs on varying context lengths.
Specifically, we categorize the contexts of number lists, stock data, and weather data into short and long contexts.
The average token numbers for the short and long contexts across the three datasets are listed in Table~\ref{tab:data_stat_short_long}.
As illustrated in Figure~\ref{fig:length_number}, Figure~\ref{fig:length_stock}, and Figure~\ref{fig:length_weather},
LLMs generally achieve lower accuracy on long contexts compared to short contexts. This is because long contexts require the model to have a stronger ability to capture long-range dependencies.
Furthermore, if an LLM fails to perform well on short contexts, it is unlikely to achieve good results on long contexts. 
It highlights the importance of the inherent capabilities of LLMs in understanding numeric data.



\noindent \textbf{Evaluation on Noisy Context  via Stock and Weather Datasets.}
To evaluate the numerical abilities of LLMs in  noisy contexts, we add $k\in\{2,4,6\}$ irrelevant attributes to each instance in the stock and weather. 
These irrelevant attributes are not used in the user queries.
As shown in Figure~\ref{fig:noisy_stock} and Figure~\ref{fig:noisy_weather} in Appendix, 
as $k$ increases, most LLMs exhibit degraded performance. This indicates that irrelevant context can  affect the LLM's numerical retrieval and reasoning abilities.

 

\noindent \textbf{Evaluation on Arithmetic Operations}
Similarly, 
we evaluate five strong LLMs on arithmetic operations.
Specifically, as illustrated in Figure~\ref{fig:arithmetic_fig}~(a), even for simple arithmetic operations involving two numbers, LLMs fail to achieve 100\% accuracy. 
Moreover, as the number of digits increases shown in Figure~\ref{fig:arithmetic_fig}~(b), the accuracy of LLMs decreases, highlighting their limited ability to handle arithmetic tasks effectively, which is also observed in~\citep{qiu2024dissecting}.
This poor performance stems from how LLMs generate responses. LLMs  predict the highest-order digit  before the lower-order digit~\citep{zhang2024reverse}, contradicting the standard arithmetic logic of progressing from lower- to higher-order digits.
In particular, Figure~\ref{fig:arithmetic_fig}~(a) and (c) shows that LLMs perform similarly on addition, subtraction, and division operations but achieve extremely low accuracy on multiplication tasks.






\noindent \textbf{Evaluation on Number Recognition via Mixed-number-string Dataset.}
We evaluate the number recognition ability of effective LLMs by identifying numbers from mixed-number-string sequences. For this evaluation, we select five  effective LLMs based on Table~\ref{tab:main_experiments}, including DeepSeek-v3, GLM-4-Long, LLaMA3.1-405B, and Qwen2.5-72B.
As shown in Table~\ref{tab:number_counting}, all LLMs achieve extremely low accuracy in counting numbers within strings. Moreover, as the length of the string increases from 50 to 100, the accuracy of the LLMs decreases further.
These results highlight that LLMs are significantly weak at distinguishing numbers from strings. The underlying reason is that current LLMs treat numbers as strings during training. 
This training paradigm inherently limits their ability to understand and process numbers effectively.
Also, the tokenizer can split a single number into multiple tokens, which can negatively affect the numeric meaning of each number.








\begin{table}[]
	\centering
	\small
	\caption{Evaluation on mixed-number-string data with lengths ranging from 50 (i.e., 50 L) to 200.}
	
	%	\footnotesize
	\begin{tabular}{c|cccc}
		\toprule
		\textbf{Model}    & \textbf{50 L} & \textbf{100 L} & \textbf{150 L} & \textbf{200 L} \\ \midrule
		
		
		\textbf{\texttt{LLama3.1-405B }}& 10.8      & 9.2        & 3.2        & 2.2        \\  
		
		\textbf{\texttt{Qwen2.5-72B}}   & 3.0         & 1.2        & 0.6        & 0.2        \\  
		
		\textbf{\texttt{GLM4-Long}  }   & 6.6       & 4.8        & 3.0          & 2.4        \\  
		
		\textbf{\texttt{GPT-4o }}       & 18.2      & 6.4        & 4.0          & 4.2        \\ 
		
		\textbf{\texttt{DeepSeek-V3}}   & 13.2      & 4.0          & 3.2        & 2.0          \\  
		\midrule
		\textbf{\texttt{Human Eval } }   & \textbf{100}      & \textbf{100}        & \textbf{100}         & \textbf{100}        \\ \bottomrule
	\end{tabular}
	\label{tab:number_counting}
\end{table}
\subsection{Discussions on Numeracy Gaps of LLMs}
In summary, extensive experimental results show that current state-of-the-art LLMs perform poorly on six fundamental numerical abilities.
% such as number recognition and arithmetic operations. 
Here we discuss five potential reasons behind their poor performance on numerical tasks.

\noindent \textbf{Tokenizer Limitation.}
LLMs use tokenizers to split input text into smaller units (tokens). Thus,
Numbers are split into chunks as strings, based on statistical patterns in the training data.
For example, $10000$ is split into $100$ and $00$ tokens\footnote{\url{https://gptforwork.com/tools/tokenizer}}.
These tokenizers do not considering  the real meaning of numbers and continuous magnitude of numbers.
Thus, LLMs do not perform well on simple number retrieval and comparison tasks.

\noindent \textbf{Training Corpora Limitation.}
LLMs are trained on extensive corpora, which also limits their ability to understand numerical-related symbols, such as $*$.
For example, the multiplication of 246 and 369 can be denoted as $246*369$.
However, $246*369$ may be interpreted as a password or encrypted text, since $*$ in text strings is often associated with encryption.
As a result, enabling LLMs to accurately interpret arithmetic symbols remains an open problem.


\noindent \textbf{Training Paradigm Limitation.}
The training of LLMs relies on the next-token prediction paradigm, which is inherently misaligned with the logic of numerical computation.
For example, when solving $16 + 56$ with the result being $72$, an LLM will first predict the highest-order digit of the answer (i.e., $7$) before predicting the lower-order digit (i.e., $2$). This approach contradicts the fundamental logic of arithmetic computation, which typically proceeds from the lower-order digit to the higher-order digit.
This discrepancy implies that LLMs effectively need to know the entire result upfront to generate digits sequentially in the correct order. As a result, LLMs struggle to perform well even on simple arithmetic operations.

\noindent \textbf{Positional Embedding Limitation.}
Note that LLMs incorporate positional embeddings for  tokens in sequence inputs. In arithmetic operations like $12 + 26$ and $26 + 12$, the order of the numbers does not affect the result. However, LLMs assign different positional embeddings to the number $12$ in each equation, as its position in the sequence differs. 
This lack of invariance in positional embeddings for numbers can influence the results.
Therefore, how to design the positional embedding that improves numerical ability of LLMs without affecting the text understanding  of LLMs is critical~\cite{mcleish2024transformers,golovneva2024contextual}.



\noindent \textbf{Transformer Architecture Limitation.}
LLMs use Transformer to process input sequence, which rely on pattern recognition rather than explicit algorithmic reasoning.
The computational power of transformers has upper bounds~\cite{merrill2023parallelism}. Considering the complexity of arithmetic operations in real-world applications, it still needs to be theoretically investigated whether transformers can perform well on numerical operations.
