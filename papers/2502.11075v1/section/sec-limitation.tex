\section*{Limitations}
There are two main limitations of this paper.
Firstly, the numerical tasks encountered in real-world scenarios are often far more complex and diverse compared to the six datasets proposed in NumericBench. 
Expanding the scope to include a broader range of numerical reasoning categories, such as traffic, would provide a more comprehensive assessment. 
Nevertheless,  our work can serves as a meaningful  point, highlighting  the current limitations of LLMs in numerical tasks. 
We also analyze the potential reasons why LLMs struggle with numerical reasoning tasks, which can be attributed to the inherent limitations of transformer architectures and the next-token prediction objective.
We
hope it inspires further efforts to address these challenges and develop   more advanced LLMs with enhanced numerical  capabilities.



Secondly, although we evaluate ten state-of-the-art LLMs, several newer LLMs and their variants, such as Claude and GPT-o1 from major companies, are not included in our experiments. 
The reason for this exclusion is the expensive cost of accessing these model APIs. 
In brief, evaluating additional LLM variants across Claude, OpenAI, Mistral and GLM, typically requires a minimum budget of \textbf{\$15,000} US dollars.
Specifically, experiments on the datasets in Table~\ref{tab:main_experiments} require approximately 180 million tokens as inputs, while all left experiments (e.g., noisy contexts) require about 84 million tokens as inputs. 
For 1 million input tokens, Claude 3 Opus costs \$15\footnote{\url{https://www.anthropic.com/pricing\#anthropic-api}}, 
Claude 3.5 Sonnet costs \$3\footnote{\url{https://www.anthropic.com/pricing\#anthropic-api}}, 
OpenAI-o1 costs \$15\footnote{\url{https://openai.com/api/pricing/}}, 
Gemini 1.5 Pro costs \$12.5\footnote{\url{https://ai.google.dev/pricing\#1_5pro}}, 
GLM4-Plus costs \$6.89\footnote{\url{https://bigmodel.cn/pricing}},
Mistral Large 24.11 costs \$2 \footnote{\url{https://mistral.ai/en/products/la-plateforme}},
Mixtral 8x22B costs \$2 \footnote{\url{https://mistral.ai/en/products/la-plateforme}},
and OpenAI o3-mini costs \$1.1\footnote{\url{https://openai.com/api/pricing/}}.

If we conduct experiments above with these top-tier models from major companies, it would cost at least 3960 dollars for Claude 3 Opus, 3960 dollars for OpenAI-o1, 3300 dollars for Gemini 1.5 Pro, 1819 dollars for GLM4-Plus, 792 dollars for Claude Sonnet 3.5, 528 dollars for Mistral Large 24.11, 528 dollars for Mixtral 8x22B and 290 dollars for OpenAI o3-mini, which is beyond our expected total experiment cost. Meantime, there are too many LLM variants in each series.

Also, for models such as OpenAI-o1, 
which require generating really longer outputs for reasoning purposes, the output length is often unpredictable, while the model charges for \$60 per million output tokens, making the experiments even more expensive and difficult to control.
Particularly,
The reason for not using DeepSeek-R1 is that its official API is currently down and unavailable. We plan to include it in our comparisons once the API is restored.
Considering that GPT-4o and DeepSeek-V3 represent  the most state-of-the-art LLM models, we believe our evaluation can reflects the current numerical abilities of leading-edge LLMs.
Therefore, our evaluation highlights the weaknesses of LLMs in numerical abilities and serves as a bridge to inspire further research focused on improving the numerical capabilities of these models.




 