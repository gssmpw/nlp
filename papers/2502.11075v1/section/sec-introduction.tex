\section{Introduction}
Large language models (LLMs)~\citep{zhao2024surveylargelanguagemodels} 
have demonstrated remarkable capabilities in text generation, semantic understanding, and task adaptation across diverse domains~\citep{ling2024domainspecializationkeymake}. 
Their success is largely attributed to pretraining on vast text corpora using next-token prediction objectives~\citep{he2024lawnexttokenpredictionlarge}, which enables generalization to tasks requiring linguistic creativity, commonsense reasoning, and domain-specific knowledge~\citep{ye-2024-cross}.
However, while LLMs perform well  in text understanding tasks and Olympic mathematics questions~\cite{team2024gemini}, 
they surprisingly struggle with simple numerical tasks such as basic multiplication, comparison, and retrieval. 
For example,  in Figure~\ref{fig:intro_example},
current LLMs fails on simple numerical tasks, even on number comparisons.


\begin{figure}[t]
	\centering	
 	\vspace{-1em}
	
	\includegraphics[width = 0.45\textwidth]{image/intro_example/intro_example.pdf}
	\caption{Numerical tasks answered incorrectly by GPT-4o. Details are in Figure~\ref{fig:number_compare},  Figure~\ref{fig:multiplication}, and Figure~\ref{fig:number_couting}.}
	
	
	\label{fig:intro_example}
	\vspace{-1em}
\end{figure}

 


Unlike tasks that rely primarily on semantic coherence and linguistic structures, 
numerical reasoning requires a deeper understanding of numbers as continuous magnitudes rather than discrete tokens. 
Current LLMs tend to prioritize surface-level statistical patterns, 
such as lexical co-occurrences and syntactic regularities, which limits their ability to process  numerical operations~\citep{ahn2024largelanguagemodelsmathematical,feng2024numerical,zhou2024transformers}.
As a result, LLMs frequently struggle with tasks involving numeric retrieval, arithmetic operations, 
and magnitude comparisons. These shortcomings highlight an urgent need to systematically evaluate and improve the numerical reasoning capabilities of LLMs.

 


Current evaluation frameworks for LLMs prioritize either linguistic competence or formal mathematical problem-solving. 
For instance, 
semantic-oriented benchmarks~\citep{vulic-etal-2020-probing}, such as GLUE~\citep{wang2018glue}, 
SuperGLUE~\citep{wang2019superglue}, 
and SimpleQA~\citep{wei2024measuring}, 
primarily assess linguistic competence and semantic understanding, 
while math-oriented benchmarks, such as MathQA~\citep{amini2019mathqa}, 
GSM8K~\citep{cobbe2021training}, 
and MathBench~\citep{liu2024mathbench},
focus on structured algebraic or geometric tasks. 
However, these approaches neglect the basic demands of real-world numerical reasoning applications and fundamental numerical abilities, 
where numbers are often embedded in unstructured and noisy context data. 
For example, analyzing fluctuating stock prices or weather   requires basic  numeric retrieval, comparison, and summery abilities.




To address the limitations of existing benchmarks, we propose  a comprehensive benchmark  NumericBench, 
%designed to systematically evaluate the fundamental numerical abilities of LLMs.  
which consists of six general datasets, i.e., arithmetic numbers, mixed-number-strings, number lists, stock, weather, and numerical sequences with patterns.
Unlike prior benchmarks, NumericBench 
systematically evaluate the six fundamental numerical abilities of LLMs:
%emphasizes six foundational dimensions of practical numeracy:
\textbf{(1) Number Recognition:} 
It evaluates the ability of LLMs to identify numbers within dense strings.
\textbf{(2) Arithmetic Operation:} 
It tests basic arithmetic operations, including addition, subtraction, multiplication, and division.
\textbf{(3) Contextual Retrieval:}
It evaluates LLMs to retrieve specific numerical values from a given context or a number list.
\textbf{(4) Comparison:}
It determine relationships between values, such as comparing price differences.
\textbf{(5) Summary:}
It target to summarize trends (e.g., determining the number of consecutive days a stock price increases) and data aggregation.
\textbf{(6) Logical Reasoning:}
It tests whether LLMs can understand numerical patterns and predict the next value, which is widely used in time-series forecasting, such as weather and traffic prediction.



By integrating six datasets, ranging from synthetic numerical lists to crawled real-world data, NumericBench evaluates six fundamental numerical abilities of LLMs across various scenarios, such as noisy contexts.
Our experimental analysis over various effective series of LLMs, including 
GPT-4~\citep{achiam2023gpt}, 
DeepSeek~\citep{liu2024deepseek}, 
and LLaMA~\citep{touvron2023llama}, 
reveals persistent weaknesses in handling these fundamental numerical tasks. 
We further analyze five potential reasons behind these numerical reasoning gaps, including tokenizer, training paradigms, positional embeddings, and architectural constraints. 
These findings offer actionable insights to guide future  numerical ability improvements for LLMs. 
Given that numerical reasoning is critical for real-world problem-solving, it represents a cornerstone for the development of Artificial General Intelligence (AGI). 
This emphasizes the urgent need to advance numerically-aware language modeling.
 The contributes of this paper is summarized as follows:
\begin{itemize}[leftmargin=*]
	\item 
	We propose a comprehensive NumericBench, which integrates diverse datasets and reflects real-world challenges, such as handling noisy or domain-specific data (e.g., stock and weather).
	NumericBench evaluates six fundamental numerical reasoning abilities, including number recognition, arithmetic operations, 
	contextual retrieval, comparison, summary, and logical reasoning. 


	\item  Extensive experiments on effective LLMs (e.g., GPT-4, DeepSeek, LLaMA) reveal persistent weaknesses in numerical reasoning tasks, including basic arithmetic, comparison, and logical reasoning. This highlights the need for more numerically-aware modeling approaches.
	
	
	\item We identifies key challenges behind the numerical reasoning gaps in LLMs, such as tokenization practices, training paradigms, positional embeddings, and architectural constraints. These insights provide actionable guidance for future model improvements.
	
\end{itemize}





