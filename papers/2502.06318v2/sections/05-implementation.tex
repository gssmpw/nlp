\section{Implementation}
\label{sec:implementation}

We have implemented \alias inside the OpenTelemetry Collector with around 3K lines of Golang code.
The OpenTelemetry Collector offers a vendor-agnostic implementation of how to manage telemetry data, which mainly includes four types of components: \textit{exporters}, \textit{processors}, \textit{receivers}, and \textit{extensions}.
We implement the span retrieval compression and decompression on the exporter and receiver, which run at the service side and backend side, respectively.
The exporter is responsible for building and updating the \sname and dictionary, compressing spans on the fly, and sending them to the remote backend.
After accepting the compressed data, the receiver performs span uncompression.
We outline some important details concerning the implementation.

\subsection{Search Acceleration by Hashing}
\label{sec:hashing_acceleration}

A straightforward data structure to implement \sname would be linked representation, which enjoys the benefits of dynamic size and efficient alterations (e.g., insertion and deletion).
However, in linked representation, the tree nodes are not stored contiguously or nearby in memory, potentially leading to more cache misses.
This factor can significantly impede the speed of path search within \sname.
To accelerate the search process, we apply hashing to convert each unique path of SRT to a path identifier, which is similar to that in Section~\ref{sec:mapping-based tree compression}.
Specifically, for each path, starting from the root we join the values of non-leaf nodes sequentially with a comma separator (similar to the CSV format). 
Based on the composed path string, we maintain a \{\textit{path}: \textit{identifier}\} mapping at the exporter.
% At the receiver, we maintain a consistent mapping at the opposite direction, i.e., \{\textit{identifier}: \textit{path}\}.
% These bi-directional mappings are suitable for compression and decompression at different sides.
When a new span is generated at the exporter, we extract the values of its universal fields based on the order in \sname.
The path search can then be quickly done for the span by checking if its path string exists in the map.
We use the \texttt{map} data type in Golang, which provides a highly efficient way to achieve this.
For any updates to the \sname, we only need to renew the affected paths as discussed in the next subsection.
% We then maintain a bi-directional mapping at the exporter, i.e., \{\textit{identifier}: \textit{path}\} and its reversed form, which will be synced with the receiver.

\subsection{Differential Data Synchronization}
\label{sec:differential_sync}

To ensure reliable span compression and uncompression, the exporter and receiver must maintain consistent copies of both the \sname and dictionary structures.
One simple strategy is for the exporter to send the latest versions of these structures upon any update.
However, given that updates often affect only a small segment of the overall structures, sending redundant (i.e., unchanged) data with each update would incur network overhead and potentially defer the uncompression process.
Thus, we implement a differential update mechanism for more resource-efficient synchronization.
The core idea is that at the receiver, instead of maintaining another \sname, we keep a path hashing in the opposite direction, i.e., \{\textit{identifier}: \textit{path}\}.
For any updates to the non-leaf nodes, we can easily pinpoint the affected paths and perform the renewal.
For example, in Figure~\ref{fig:tracezip_system}, the emergence of a new value (denoted by the pink dashed rectangle) gives rise to a novel path, i.e., \textit{path 3}.
In this case, we can add a new entry to the \{\textit{path}: \textit{identifier}\} mapping at the exporter and sync it with the receiver.
For path deletion, the exporter can simply send the corresponding identifier to the receiver for record elimination.
Other updates are essentially a combination of path addition and deletion.
% we hash each path of \sname (excluding the leaf node) into a path identifier (Section~\ref{sec:hashing_acceleration}), which is then synced with the receiver.
% For any updates to the non-leaf nodes, we can easily pinpoint the affected paths and (re)calculate the path identifiers for renewal.
% For example, in Figure~\ref{fig:tracezip_system}, the emergence of a new value (denoted by the pink dashed rectangle) gives rise to a novel path, i.e., \textit{path 3}.
% \red{In this case, only the hashed representation of this new path needs to be synced.}
% Such a design also benefits the search of spans in \sname, which will be elaborated in the next subsection.

For local fields and the mapping dictionary, it suffices to communicate only the changes to the receiver.
To ensure that the structures at the receiver is not outdated during the transmission of spans, we leverage the batch processor of OpenTelemetry Collector.
It caches the spans sent by SDK until the batch memory is full or its timer expires, instead of immediately forwarding them.
After compressing the spans in the buffer, we will make sure that the SRT and dictionary with updates (if any) have been synced with the receiver side before releasing the data.


% To verify the effectiveness of our proposed Merging Algorithm, we designed two prototype systems, which we refer to as the Static Spans Compressor($\approx 0.5k\mathrm{LOC}$) and the OpenTelemetry Collector instrumented with the Merging Tree Algorithm (hereafter referred to as OTelCol with Compression, $\approx 2k\mathrm{LOC}$, OTel Collector scaffolding codes are excluded). The Static Spans Compressor is a command-line tool used for compressing span records stored in CSV file format. OTelCol with Compression is a middleware based on the secondary development of the OpenTelemetry Collector. The OpenTelemetry Collector supports a plugin system, allowing developers to write their own plugins and build their own OTelCol, and control the data flow of OTelCol by writing configuration files. We have written Receiver and Exporter plugins for OTelCol. The compression algorithm constructs and sends the Merging Tree on the Exporter side, and decompresses the Merging Tree on the Receiver side, while also performing anomaly detection.

% \subsection{OTelCol with Compression}

% In OpenTelemetry, components are divided into three categories: Receiver, Processor and Exporter. The Receiver is the component in OpenTelemetry that ingests telemetry data (such as traces, metrics, and logs) from various sources. It acts as an entry point for data into the OpenTelemetry Collector. The Processor is the component responsible for processing and transforming telemetry data within the OpenTelemetry Collector. It operates between the Receiver and Exporter. The Exporter is the component that sends the processed telemetry data to various backends and storage systems for analysis and visualization. Therefore, we chose to develop the Receiver and Exporter components of the Merging Algorithm based on the OpenTelemetry Collector, use Batch component as processor, build a custom OTelCol that can use the components based on the Merging Algorithm, which we refer to as OTelCol with Compressor. Due to the vendor-agnostic nature of the OpenTelemetry Collector, our developed OTelCol with Compressor can easily integrate with various tracing platforms, such as Zipkin and Jaeger.

% \textbf{Gateway Collector Deploy Pattern} The OpenTelemetry documentation provides a deployment pattern known as the gateway collector, which starts multiple OTelCol instances and utilizes NGINX for load balancing. Considering a production environment with high access pressure on the tracing backend, we use our custom OTelCol with Compressor for each microservice, employing our Merging Algorithm Compressor as the Exporter, sending data to a remote OTelCol with Compressor for decoding operations (referred to as OTelCol with Compressor Gateway in the text). Using the gateway collector deployment pattern allows us to implement load balancing strategies, achieve decentralized trace collection, and further decouple the data collection platform.

% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=8cm]{figures/arch.pdf}
%     \caption{Architecture of OtelCol with Compressor System}
%   \end{figure}

% \textbf{Batch Processor as Buffer} In OTelCol, there is an important component called the Batch Processor. The OpenTelemetry documentation also recommends developers use the Batch Processor. Essentially, the Batch Processor acts as a Spans Buffer. When the SDK collects spans from microservices, the Batch Processor caches the spans sent by the SDK until the batch in the Batch Processor is full or the Batch Processor's timer expires, instead of immediately forwarding them to the next node. We can configure the size of the Batch Processor and the duration of the timer in the configuration file. Since the Merging Algorithm relies on the high redundancy of spans data and the correlation between spans data, it is important to set a reasonable batch size and timeout value.

% \begin{table}[h!]
% \centering
% \begin{tabular}{|p{2cm}|p{6cm}|}
% \hline
% \textbf{Configuration Item} & \textbf{Description} \\
% \hline
% \texttt{sample buffer} & Sets the size of the sample buffer. The sample buffer is used to store and count the occurrences of various attribute values for anomaly detection. It differs from the batch size of the Batch Processor. \\
% \hline
% \texttt{abnormal span name rate} & Abnormal Span Name Rate. This parameter determines which span names with low frequency of occurrence will be considered abnormal. \\
% \hline
% \texttt{abnormal attributes frequency rate} & Abnormal Attributes Frequency Rate. This parameter determines which attribute values with low frequency of occurrence will be considered abnormal. \\
% \hline
% \texttt{enable number abnormal} & Enables numeric anomaly detection when set to true. This parameter controls whether numeric anomaly detection is enabled, where numeric attribute values are checked for anomalies based on their average value and variance. \\
% \hline
% \texttt{threshold rate} & Threshold Rate. This parameter is used to calculate the baseline threshold for inclusion in the Trie. Attribute values with an occurrence count exceeding \texttt{spanNameCount[spanName]/ThresholdRate} will not be included in the Trie compression sequence. \\
% \hline
% \texttt{enable gzip} & Whether to enable gzip compression. If enabled, spans data will be gzip-compressed when sent. \\
% \hline
% \end{tabular}
% \caption{Configuration parameters for exporter}
% \label{exporter}
% \end{table}

% \textbf{Mechanism of Exporter with Compressor} The Exporter with Compressor utilizes the Batch component to process tracing data sent by clients in batches. During the compression process, the attribute values are mapped to a dictionary, and the Attribute Name is synchronized between the Agent Side and the Gateway Side to restore the original attribute names. Additionally, during the compression process, it detects abnormal spans (as described in the Methodology section) and synchronizes the trace\_id of the abnormal spans to the Gateway. Here in Table \ref{exporter}, we present the configuration options of the Exporter and their meanings

% \textbf{Mechanism of Receiver with De-Compressor} The Receiver with De-Compressor will be deployed on the Gateway side. It receives synchronized dictionary data from the Agent Side, receives abnormal detection data in the form of an array of trace IDs from the Agent Side, and receives compressed spans arrays from the Agent Side. It then restores the compressed spans to their original uncompressed form.

% \zb{details of tree/dict synchronization: send path as dict (with an ID) to backend and only update the changed dicts; the base time is periodically updated}
