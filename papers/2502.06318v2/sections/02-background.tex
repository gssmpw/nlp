\section{Background and Motivation}
\label{sec:background}

\subsection{Distributed Tracing}

Distributed tracing provides a detailed end-to-end view of requests as they traverse interconnected and multi-tier cloud service systems.
The building blocks of a trace are called \textit{spans}.
They represent the individual slices of work performed across different machines and components that are visited by the request.
Each span encapsulates various attributes, including \textit{span name}, \textit{parent span ID}, \textit{span ID}, \textit{start/end timestamps}, \textit{events}, etc.
This information enables the correlation and analysis of the request's lifecycle.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/distributed_tracing.pdf}
    \caption{A Typical Procedure of Distributed Tracing}
    \label{fig:distributed_tracing}
\end{figure}

Figure~\ref{fig:distributed_tracing} illustrates a typical procedure of tracking requests~\cite{DBLP:conf/nsdi/ZhangXAVM23,opentelemetry} in modern distributed tracing frameworks, such as Jaeger~\cite{jaeger} and Zipkin~\cite{zipkin}.
Upon the arrival of a new request to the application, the tracing framework assigns it a unique \texttt{trace\_id} (\ding{192}).
However, not all requests will actually be traced, which is indicated by a flag, \texttt{sampled}.
Both \texttt{trace\_id} and \texttt{sampled} will then be propagated along the request at the application level, which is important for the completeness and coherence of a trace.
If \texttt{sampled} is set, each component (e.g., a microservice instance) that handles the request will generate trace data (\ding{193}), i.e., a span, using the tracing framework's client library (e.g., OpenTelemetry).
The places that emit spans are called a \textit{tracepoint}, and there could be multiple tracepoints serving different requests or different operations within a single request.
The framework's client library then enqueues, serializes, and transmits trace spans (\ding{194}) to its centralized backend collection infrastructure, or simply backend.
The backend is responsible for continuously receiving (\ding{195}), processing (\ding{196}), and storing (\ding{197}) trace data.
Based on the \texttt{trace\_id}, \texttt{parent\_span\_id}, and \texttt{span\_id}, the backend can assemble spans that were dispersed across different components into a single coherent trace.

Given the details provided by these spans, operators can closely monitor and understand the impact that one component may have on the others.
This makes trace data particularly useful for troubleshooting cross-component problems in large distributed systems.
However, traces can be produced at high volume, incurring significant network, compute, and storage costs.
In production scenarios, Google is estimated to generate approximately 1,000 TB of raw traces on a daily basis.
Netflix needs to manage more than 2 billions of daily requests.
To mitigate overheads, existing tracing frameworks often apply head-based sampling to trace only a small fraction of requests by randomly setting the \texttt{sampled} flag (\ding{192}).
This will inevitably increases the risk of overlooking system edge-case behaviors.
On the other hand, tail-based sampling utilizes a filtering strategy, i.e., only persisting traces that exhibit outliers symptoms (\ding{197}), e.g., high tail latency, error codes.
However, tail-based sampling entails enormous costs, as it must trace all requests and ingest the trace data to make sampling decisions.

In this work, we take an orthogonal path to address the overhead challenges.
We alleviate the lossy nature of sampling-based schemes by tracing more requests, yet without increasing the transmission overhead.
Such a design can enhance the monitoring capability of edge-case behaviors.
Our core idea lies in the observation that there exists a large amount of redundancy in trace data.
Specifically, for each tracepoint, the generated spans may have repeated attribute values and events.
This renders repetitive transmissions of identical trace data, and we see this could be an opportunity to improve the tracing efficiency.
Recognizing this, we conduct a study to examine the redundancy inherent in trace data.
The insights obtained serve as essential design principles of our approach.

% However, there exists a trade-off between the competeness of tracing and system overhead.
% On one hand, tracing excessive requests would increase the costs of network, compute, and storage resources.
% On the other hand, insufficient tracing heightens the risk of overlooking system edge-case behaviors and symptoms.


\subsection{A Study of the Redundancy in Trace Data}
\label{sec:redundancy_study}

% \zb{mention that the spans generated at one service are highly repetitive}
% \zb{Trace generation is cheap, but transmission and ingestion are expensive}
% \zb{Symptoms are locally observable.}

% In this section, we present our study on the redundancy in trace data.
% Our goal is to investigate 
In this section, we present our study regarding the \textit{redundancy} of trace data.
By redundancy, we mean the recurrence of identical information, e.g., attributes and events, that is observed repeatedly across multiple traces.
The identification of such repetitive patterns presents an opportunity to enhance the efficiency of distributed tracing through the application of compression techniques.
By strategically reducing the redundancy, we can optimize the transmission overhead of trace data, thereby improving the scalability of the tracing infrastructure without compromising data integrity.

The studied systems include an open-source microservices benchmark, i.e., Train Ticket~\cite{DBLP:journals/tse/ZhouPXSJLD21}, which is popular in cloud-related research fields, and six application backend components that are selected for their widespread use in modern cloud systems, i.e., gRPC (Client and Server), Apache Kafka (Producer and Consumer), Servlet, MySQL, Redis, and MongoDB.
They cover a diverse functionalities including message queuing, HTTP communication, remote procedure calls, database management, etc.
To collect their traces, we leverage the zero-code instrumentation capabiltiy of OpenTelemetry~\cite{opentelemetry_zero_code}.
It allows the collection of observability data, i.e., logs, metrics, and traces, for applications without the need to modify the source code.
This is achieved by using libraries, plugins, or agent to instrument the libraries used by applications.
It supports many programming languages (e.g., Java, Go, Python) and a wide range of popular libraries and frameworks, including requests and responses, database calls, message queue calls.
% \red{In reality, the traces collected via zero-code instrumentation serve as a foundational layer of observability, providing a comprehensive and automated snapshot of an application's behavior and performance.}
By injecting typical workloads to the studied systems (Section~\ref{sec:deployed_cloud_systems}), we collect more than 40GB of trace data in total.

We quantify trace redundancy by measuring the proportion of duplicate key-value (KV) pairs generated by each service.
As a KV pair represents the fundamental unit of information within a trace, assessing its repetition enables us to gauge the degree of information overlap.
Specifically, for each KV pair, we first count the number of its occurrence and then calculate its ratio over the total number of KV pairs in the dataset.
For example, in a trace dataset containing 100 KV pairs, if two specific pairs appear once and ten times respectively, their redundancy ratios would be 1\% and 10\%, respectively.
We calculate the fraction of KV pairs within each service which occur less than 1,000 times, as well as those exceeding this threshold, as shown in Figure~\ref{fig:trace_redundancy}.
We make the following two important observations.

% We quantify the trace redundancy by measuring the ratio of duplicate key-value (KV) pairs produced by each service.
% A KV pair is the basic information carrier of a trace.
% By assessing their repetition, we can evaluate the extent of information overlap.
% Specifically, for each KV pair, we first count the number of its occurrence and then calculate its ratio over the total number of KV pairs in the dataset.
% For instance, with a trace dataset comprising 100 KV pairs, if two particular pairs recur once and ten times, their respective redundancy ratios would be 0.1\% and 10\%.
% We calculate the portion of KV pairs in each service whose occurrences is below 1000 and those that exceed this threshold, as shown in Figure~\ref{fig:trace_redundancy}.
% We made two observations:

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/experiments/trace_redundancy.pdf}
    \caption{Trace Data Redundancy Analysis}
    \label{fig:trace_redundancy}
\end{figure}

\textbf{Traces are highly redundant}.
Based on the figures, we can see that for different microservices in Train Ticket, around 70\% of the total KV pairs are highly repetitive.
Similar situations can be found in the application components.
The only exception is Apache Kafka, whose KV pairs tend to be unique.
The reason behind is that Kafka's traces include the data from its message queues, increasing the randomness of its KV pairs.
We will discuss its impact on compression during system evaluation (Section~\ref{sec:exp_eval}).
The results indicate that a significant portion of the trace data is characterized by redundant information.
An important reason is that services often engage in standard interactions and perform routine operations that generate trace data with similar patterns.
By capturing the redundancy upon the generation of spans at the service side (\ding{192}), we can preemptively eliminate the transmission of repetitive data that already exist in the backend.
The backend can easily reconstruct the complete spans based on their redundancy patterns (\ding{197}).

\textbf{There exists structural redundancy among attributes}.
We also observe that there exists certain redundancy at span level.
OpenTelemetry's semantic conventions~\cite{opentelemetry_semantic_conventions} offer standardized guidelines for naming common attributes across different kinds of operations and data.
This is essential for maintaining the uniformity and compatibility of naming scheme across languages, libraries, and platforms.
In the naming scheme, attribute keys are organized into hierarchical namespaces to indicate their context or category, e.g., \texttt{network.local.address}, \texttt{network.local.port}, \texttt{network.peer.address}.
We can see that they share some common words.
The attribute values also exhibit similar commonality, e.g., Java Exception has \texttt{java.io.IIOException}, \texttt{java.io.EOFException}.
By eliminating such fine-grained redundancies, we can further reduce the overhead associated with span transmission.
% By removing such fine-grained redundancy, we can further reduce the overheads of span transmission.

% To ensure consistency and interoperability, OpenTelemetry defines Semantic Conventions for different types of services and middleware. 
% These Semantic Conventions provide guidelines and standards for generating attributes specific to the given service or middleware. 
% For example, when using OpenTelemetry with a MySQL driver, specific rules and guidelines are provided for generating spans attributes, ensuring uniformity and compatibility across different implementations.


% In analyzing the spans data from the production environment, we found that the values of the attributes lack orthogonality. More specifically, for all the spans data, when the B attribute takes a certain value, the number of optional values for the A attribute decreases.

% Using Alibaba's open-source cluster trace dataset 2022 as an example, we analyze this structural relationship of values. We first notice that there is often a relationship between the \texttt{um} attribute and \texttt{uminstanceid} attribute: for a span record where \texttt{um} is \texttt{MS\_x}, the \texttt{uminstanceid} will be labeled as \texttt{MS\_x\_POD\_y}; this relationship also exists between \texttt{dm} and \texttt{dminstanceid}. Additionally, we observe that for span records with the same \texttt{interface}, the \texttt{rpctype} is often consistent. This relationship can also be corroborated by OTel's Semantic Convention. For example, the OTel Semantic Conventions for SQL Databases document recommends adding the attributes \texttt{db.operation.name} (The name of the operation or command being executed) and \texttt{db.query.text} (The database query being executed). This structural relationship leads us to think of a tree data structure. 


% OpenTelemetry offers a convenient deployment method called Zero-Code Instrumentation, which allows developers to automatically collect metrics, logs, and generate traces without explicitly calling OpenTelemetry APIs in their code. 
% This approach simplifies the instrumentation process and provides a seamless experience for developers.

% The Attributes of a Span often constitute the majority of a Span's size, and they tend to be highly repetitive, a point we will argue later.
% A Span's event, semantically, is a named occurrence that happens at a specific point in time. 
% It indicates ``this event happened at this moment'' and provides additional details about the event. 
% Examples of events may include uncaught exceptions, button clicks, user logouts, network disconnections, etc. Its structure is also similar to that of Spans.


% OpenTelemetry, also known as OTel, is an industry-standard, vendor-neutral, open-source observability framework designed for instrumenting, generating, collecting, and exporting telemetry data, including traces, metrics, and logs. Supported by over 40 observability vendors and adopted by numerous end users, OpenTelemetry provides a unified approach to observability in modern distributed systems.

% One of the key aspects of OpenTelemetry is its wide integration with various observability tools and platforms, such as Jaeger, Prometheus, and Zipkin, enabling the collection of telemetry data from different sources. The OpenTelemetry specification defines the format for sending spans data, ensuring compatibility and interoperability across different systems.

% OpenTelemetry offers a convenient deployment method called Zero-Code Instrumentation, which allows developers to automatically collect metrics, logs, and generate traces without explicitly calling OpenTelemetry APIs in their code. 
% This approach simplifies the instrumentation process and provides a seamless experience for developers.

% OTel provides a specification for the data structure of Spans. 
% Below, we present an example of a Span's data structure. 
% Additionally, OTel's collector often sends spans in batches. 
% The identifiers such as \texttt{trace\_id}, \texttt{span\_id}, and \texttt{parent\_id} in the context are non-repetitive and lack structural duplication. \texttt{start\_time} and \texttt{end\_time} can be serialized into timestamps, which will be unsigned 32-bit integers. 
% For a batch of \texttt{start\_time} and \texttt{end\_time}, we found that they can be decomposed into a $\mathrm{base} + \mathrm{offset}$ structure, and combined with variable-length encoding techniques, this can effectively compress the size of the data bits. 
% The Attributes of a Span often constitute the majority of a Span's size, and they tend to be highly repetitive, a point we will argue later.
% A Span's event, semantically, is a named occurrence that happens at a specific point in time. 
% It indicates ``this event happened at this moment'' and provides additional details about the event. 
% Examples of events may include uncaught exceptions, button clicks, user logouts, network disconnections, etc. Its structure is also similar to that of Spans.

% \begin{lstlisting}
% {
%   "name": "hello",
%   "context": {
%     "trace_id": "trace-id1",
%     "span_id": "span-id1"
%   },
%   "parent_id": span-id0,
%   "start_time": "2022-04-29 18:52:58.114201",
%   "end_time": "2022-04-29 18:52:58.114687",
%   "attributes": {
%     "http.method": "GET"
%   },
%   "events": [
%     {
%       "name": "Guten Tag!",
%       "timestamp": "2022-04-29 18:52:58.114561",
%       "attributes": {
%         "event_attributes": 1
%       }
%     }
%   ]
% }
% \end{lstlisting}


% To ensure consistency and interoperability, OpenTelemetry defines Semantic Conventions for different types of services and middleware. 
% These Semantic Conventions provide guidelines and standards for generating attributes specific to the given service or middleware. 
% For example, when using OpenTelemetry with a MySQL driver, specific rules and guidelines are provided for generating spans attributes, ensuring uniformity and compatibility across different implementations.

% The Semantic Conventions cover a wide range of areas, including general conventions, cloud providers, CloudEvents, database operations, exceptions, Function as a Service (FaaS) operations, feature flag evaluations, generative AI operations, GraphQL implementations, HTTP client and server operations, messaging operations and systems, object store operations, RPC client and server operations, and system conventions.
% OTel provides Zero-Code Instrumentation for services, ensuring that the generation of Attributes in Spans conforms to the Semantic Convention. 
% This homogenizes the spans, allowing us to deploy Zero-Code Instrumentation and run OTel's Benchmark with Zero-Code Instrumentation to collect spans that closely resemble those in a production environment. Subsequently, we can effectively evaluate our proposed compression algorithm.

% We conducted a simulation of several typical types of services operating within a microservices system and collected and analyzed their span data through zero-cod instrumentation.
% Additionally, we collected and analyzed the Spans data from two benchmark microservices systems during runtime, as well as analyzed the Alibaba microservices trace dataset, along with an analysis of the Spans specification and semantic conventions. The following insights were obtained:

% \begin{enumerate}
%       \item There is a significant amount of repetition in the attribute values of Spans.
%       \item There is a structural correlation among the attribute values of Spans.
% \end{enumerate}

% \subsubsection{Repetition in the Attributes}
% \zb{The portion of data that the top-n most frequent attribute values take up.}

% Based on the analysis of eight typical microservice Zero-Code Instrument Spans, we summarize the frequency of each Attribute Value's occurrence, as well as the 99th percentile and 50th percentile of these frequencies. Here, we provide a table showing the magnitudes of the 99th and 50th percentiles. Specific line charts can be found in the appendix. Meanwhile, we have collected and analyzed the Attributes information of spans from the DeathStarBenchmark's SocialNetwork and the TrainTicket Benchmark's spans. 
% Here, we present the Number of Optional Values for the Spans Attribute Key collected from the DeathStarBenchmark, and we also provide the occurrence count of each attribute value from the TrainTicket Benchmark. These charts are provided in the Appendix.

% \begin{table}[h!]
% \caption{99th and 50th Percentiles of Attributes Occurrence\zb{}}
% \label{spans-mock}
% \centering
% \begin{tabular}{|c|c|c|}
% \hline
% Spans Source & 99th Percentile & 50th Percentile \\
% \hline
% kafka.json & $10^2$ & $10^0$ \\
% http-client.json & $10^3$ & $10^2$ \\
% dubbo-rpc.json & $10^3$ & $10^0$ \\
% mongodb.json & $10^3$ & $10^2$ \\
% mysql.json & $10^3$ & $10^3$ \\
% redis.json & $10^3$ & $10^1$ \\
% grpc-server.json & $10^2$ & $10^0$ \\
% servlet-http-server.json & $10^2$ & $10^0$ \\
% \hline
% \end{tabular}
% \\
% \end{table}

% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=8cm]{figures/socialNetwork_plot/social-network-nginx.png}
%     \caption{Number of Optional Values for the Spans Attribute}
% \end{figure}

% By analyzing the data from Table \ref{spans-mock}, we can observe that for a small number of Attribute Values (in the figure, we selected the 99th percentile), the occurrence count is very high; whereas for the majority of Attribute Values, the occurrence count is relatively low. By analyzing the gateway data on nginx in the socialNetwork microservice of the DeathStarBenchmark, we found that for most Attribute keys, the number of optional values is relatively small. For the TrainTicket Benchmark, the number of optional values is even smaller. For each microservice, we have listed the occurrence count of all Attribute Values.

% \subsubsection{Structural Correlation among Attributes}
% \label{sec:attr_structural_correlation}

% In analyzing the spans data from the production environment, we found that the values of the attributes lack orthogonality. More specifically, for all the spans data, when the B attribute takes a certain value, the number of optional values for the A attribute decreases.

% Using Alibaba's open-source cluster trace dataset 2022 as an example, we analyze this structural relationship of values. We first notice that there is often a relationship between the \texttt{um} attribute and \texttt{uminstanceid} attribute: for a span record where \texttt{um} is \texttt{MS\_x}, the \texttt{uminstanceid} will be labeled as \texttt{MS\_x\_POD\_y}; this relationship also exists between \texttt{dm} and \texttt{dminstanceid}. Additionally, we observe that for span records with the same \texttt{interface}, the \texttt{rpctype} is often consistent. This relationship can also be corroborated by OTel's Semantic Convention. For example, the OTel Semantic Conventions for SQL Databases document recommends adding the attributes \texttt{db.operation.name} (The name of the operation or command being executed) and \texttt{db.query.text} (The database query being executed). This structural relationship leads us to think of a tree data structure. 

% Our paper presents an insight: for spans arrays, we can use a tree structure to organize and compress them, thereby maximizing the utilization of this relationship.