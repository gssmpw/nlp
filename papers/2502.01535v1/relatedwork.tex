\section{Related Work}
\subsection{Alzheimer's Disease Prediction}
AI has shown a great deal of promise in Alzheimer's disease studies, particularly in the territory of automated disease prediction using neuroimaging data, such as magnetic resonance imaging (MRI) and positron emission tomography (PET) scans~\cite{jack2018nia, menagadevi2024machine, illakiya2023automatic, yao2023artificial, warren2023functional, ebrahimighahnavieh2020deep, frizzell2022artificial}. Deep learning techniques, especially convolutional neural networks (CNNs), have demonstrated high accuracy in identifying structural abnormalities associated with AD progression~\cite{menagadevi2024machine, illakiya2023automatic, abdulazeem2021cnn, farooq2017deep, khagi20203d, folego2020alzheimer, el2024novel, hu2023conv, basaia2019automated}. To achieve this, these models typically leverage large datasets to extract features indicative of cortical atrophy, hippocampal shrinkage, and white matter hyperintensities (WMH)~\cite{weiner2013alzheimer, dadar2022white}.

Despite advances, most deep learning models focus solely on predictive accuracy, and with a black-box architecture, often overlooking explainability, transparency, and modularity.
Furthermore, existing machine learning approaches seldom follow the reasoning process on which clinicians rely in practice, such as comparing findings to reference cases or providing detailed explanations for abnormalities. 
As such, it is difficult for clinicians to verify, and, therefore, to trust the machine-based predictions, especially when outcomes do not align with observed data~\cite{ghassemi2021false, arrieta2020explainable}. Thus, there is a need to address these limitations if one wants to implement AI in real-work clinical workflows.

\subsection{Multimodal AI}
The integration of multimodal data, such as combining radiology images with text-based clinical notes, is powerful in biomedical applications~\cite{soenksen2022integrated, acosta2022multimodal, alsaad2024multimodal}. For example, models like BiomedCLIP, ConVIRT, and MedCLIP leverage both visual and textual modalities to enhance the machine's understanding of complex medical scenarios~\cite{zhang2022contrastive, wang2022medclip, zhang2024biomedclip}. By aligning image and text embeddings, these models extract features containing richer information and obtain augmented contextualization. Consequently, it improves the accuracy of disease classification and anomaly detection.

Although multimodal analyses that combine images and texts have shown utility in domains such as breast cancer detection and ophthalmology, their application to neurodegenerative diseases is limited~\cite{nakach2024comprehensive, qian2024multimodal, abdullakutty2024histopathology, xu2022multi, wang2024advances, mihalache2024accuracy}. For AD, there is often a wealth of available neuroimaging data and textual descriptions, and there is, therefore, considerable potential to extend multimodal methods to AD to improve both diagnostic accuracy and interpretability.

\subsection{Contrastive Learning}
Contrastive learning has gained attention as an effective technique for representation learning, particularly in scenarios with limited labeled data~\cite{chen2020simple, he2020momentum}. Contrastive learning trains models to align embeddings of related pairs (e.g., an image and its corresponding description) while separating unrelated pairs; it enhances feature discrimination and robustness~\cite{zhang2022contrastive}. Such a learning technique has seen success in handling medical imaging tasks, such as lesion detection and segmentation, where it improves generalization by learning from diverse data distributions~\cite{azizi2021big, chaitanya2020contrastive, krishnan2022self, zang2021scehr}.

In Alzheimer's disease research, contrastive learning can be used to align radiology images with verified reference abnormalities and descriptions. By linking input images to meaningful reference cases, this alignment not only improves diagnostic performance but also facilitates explainability ~\cite{zhang2024biomedclip}. Particularly, fine-tuning pre-trained models, such as BiomedCLIP, with contrastive learning offers a promising pathway to address the challenges of limited labeled datasets and enhance reliability.

\subsection{Explainable AI}
Explainable AI techniques are critical for building trust in AI systems, especially in healthcare~\cite{holzinger2018machine, amann2020explainability, sadeghi2024review, molnar2020interpretable}. Popular methods include saliency maps, attention mechanisms, and feature attribution techniques that highlight important regions of an image or influential features in a prediction~\cite{tjoa2020survey, lundberg2020local, scott2017unified, li2023deep, borys2023explainable, wollek2023attention}. Many of these methods, however, generate \textit{post hoc} explanations that may not directly align with the model's prediction process, leading to concerns about their reliability~\cite{ghassemi2021false, arrieta2020explainable}.

For AD diagnosis, generating explanations that follow clinical reasoning, for example, retrieving similar cases, identifying abnormalities, and providing detailed descriptions to justify predictions, is particularly important. Evidence-driven reasoning, as demonstrated in our work, addresses these limitations by making explanations an integral part of the diagnostic process~\cite{mckinney2020international}. This not only enhances transparency but also aligns AI tools with the modular workflows commonly used in clinical practice.