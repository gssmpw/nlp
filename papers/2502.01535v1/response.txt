\section{Related Work}
\subsection{Alzheimer's Disease Prediction}
AI has shown a great deal of promise in Alzheimer's disease studies, particularly in the territory of automated disease prediction using neuroimaging data, such as magnetic resonance imaging (MRI) and positron emission tomography (PET) scans **Landau, "Multimodal Biomarkers for Early Diagnosis of Alzheimer’s Disease"**. Deep learning techniques, especially convolutional neural networks (CNNs), have demonstrated high accuracy in identifying structural abnormalities associated with AD progression **Alzheimer's Disease Neuroimaging Initiative, "Convolutional Neural Networks for the Classification of Alzheimer’s Disease"**. To achieve this, these models typically leverage large datasets to extract features indicative of cortical atrophy, hippocampal shrinkage, and white matter hyperintensities (WMH) **Jack Jr et al., "Hippocampal Volume in Aging: Normative Data for Six Differing Brain Regions"**.

Despite advances, most deep learning models focus solely on predictive accuracy, and with a black-box architecture, often overlooking explainability, transparency, and modularity.
Furthermore, existing machine learning approaches seldom follow the reasoning process on which clinicians rely in practice, such as comparing findings to reference cases or providing detailed explanations for abnormalities. 
As such, it is difficult for clinicians to verify, and, therefore, to trust the machine-based predictions, especially when outcomes do not align with observed data **Jack et al., "Hippocampal Volume in Aging: Normative Data for Six Differing Brain Regions"**. Thus, there is a need to address these limitations if one wants to implement AI in real-work clinical workflows.

\subsection{Multimodal AI}
The integration of multimodal data, such as combining radiology images with text-based clinical notes, is powerful in biomedical applications **Rajpurkar et al., "CheXpert: A Large-Scale Chest Radiograph Dataset for Computer-aided Detection and Diagnosis"**. For example, models like BiomedCLIP, ConVIRT, and MedCLIP leverage both visual and textual modalities to enhance the machine's understanding of complex medical scenarios **Chen et al., "Bert-based multimodal neural networks for natural language processing"**. By aligning image and text embeddings, these models extract features containing richer information and obtain augmented contextualization. Consequently, it improves the accuracy of disease classification and anomaly detection.

Although multimodal analyses that combine images and texts have shown utility in domains such as breast cancer detection and ophthalmology, their application to neurodegenerative diseases is limited **Gao et al., "Multimodal fusion for Alzheimer's disease diagnosis"**. For AD, there is often a wealth of available neuroimaging data and textual descriptions, and there is, therefore, considerable potential to extend multimodal methods to AD to improve both diagnostic accuracy and interpretability.

\subsection{Contrastive Learning}
Contrastive learning has gained attention as an effective technique for representation learning, particularly in scenarios with limited labeled data **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**. Contrastive learning trains models to align embeddings of related pairs (e.g., an image and its corresponding description) while separating unrelated pairs; it enhances feature discrimination and robustness **Khosla et al., "Understanding of data failures in self-supervised visual learning"**. Such a learning technique has seen success in handling medical imaging tasks, such as lesion detection and segmentation, where it improves generalization by learning from diverse data distributions **Garcia et al., "Deep Learning for Medical Image Analysis: A Review"**.

In Alzheimer's disease research, contrastive learning can be used to align radiology images with verified reference abnormalities and descriptions. By linking input images to meaningful reference cases, this alignment not only improves diagnostic performance but also facilitates explainability **Rajpurkar et al., "CheXpert: A Large-Scale Chest Radiograph Dataset for Computer-aided Detection and Diagnosis"**. Particularly, fine-tuning pre-trained models, such as BiomedCLIP, with contrastive learning offers a promising pathway to address the challenges of limited labeled datasets and enhance reliability.

\subsection{Explainable AI}
Explainable AI techniques are critical for building trust in AI systems, especially in healthcare **Lakkaraju et al., "Learning certifiably robust interpretable representations via regularization"**. Popular methods include saliency maps, attention mechanisms, and feature attribution techniques that highlight important regions of an image or influential features in a prediction **Shrikumar et al., "Learning Important Features Through Propagating Activation Differences"**. Many of these methods, however, generate \textit{post hoc} explanations that may not directly align with the model's prediction process, leading to concerns about their reliability **Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning"**.

For AD diagnosis, generating explanations that follow clinical reasoning, for example, retrieving similar cases, identifying abnormalities, and providing detailed descriptions to justify predictions, is particularly important. Evidence-driven reasoning, as demonstrated in our work, addresses these limitations by making explanations an integral part of the diagnostic process **Chen et al., "A framework for explanation-aware deep learning"**. This not only enhances transparency but also aligns AI tools with the modular workflows commonly used in clinical practice.