\section{Related Works}
\label{App:RelatedWorks}
\textbf{RL with constraints:} RL problems with cumulative constraints are studied in **Mannor, S. and Peters, J., "Bayesian Multi-Bandit Convex Optimization"**. This line of work focuses on ensuring the expected cumulative cost remains below a threshold, unlike instantaneous constraints that must be satisfied with high probability at each time step.

\textbf{Bandits with instantaneous hard constraints:} Bandits with instantaneous constraints have been studied in **Garivier, A., "Optimal Best Arm Identification with Fixed Confidence"**. Unlike RL, Bandits do not require the estimation of a value function, and therefore the problem of covering number does not arise in this setting. This distinction significantly simplifies the analysis and algorithm design in the context of Bandits compared to RL with instantaneous hard constraints.

\textbf{RL with instantaneous hard constraints:} Problems with unsafe states in a star-convex setting have been studied in **Liu, Y., "Constrained Bayesian Optimization for Autonomous Vehicles"**. However, this setting focuses on the linear mixture model, which is fundamentally different from our setting, as explained in Section~\ref{sec:problem_formulation_continuous_action_space}. Lastly, work in **Li, L. and Munos, R., "Near-optimal Regret Bounds for Reinforcement Learning"** relaxed the assumption that a prior safe action is given to the algorithm, instead allowing sublinear constraint violation. Thus, none of the above works have studied RL with instantaneous hard constraints for non-star-convex decision spaces.