\section{LongFaith}
\label{sec:method}

In this section, we present an exposition of \textsc{LongFaith} pipeline. Specifically, we explain how it synthesizes \textsc{LongFaith}-SFT for supervised fine-tuning and \textsc{LongFaith}-PO for preference optimization from the perspective of faithfulness.

\paragraph{Synthesize Reasoning Chains with High Faithfulness.} Previous studies~\cite{longalign, longlora, longmit, longreward, sealong} tend to directly distill synthesized long-context QA and reasoning instructions for training without filtering out incorrect information. These low-faithfulness synthesized data limit the performance improvements of the trained models. In response to this challenge, \textsc{LongFaith} integrates ground truth into the synthesized reasoning chains. For a sample \(S\) from the training set \(S = (Q, D, A, F)\), where \(Q\) is the reasoning question, \(D\) is the full document used for querying, \(A\) is the correct answer, and \(F\) represents the supporting facts where \(F \in D\). We use the LLM \(M_{\text{syn}}\) to synthesize the reasoning chain as follows:

\begin{equation}
    O_c = M_{\text{syn}}(P_{\text{coc}}, Q, F, A)
\end{equation}

Here, \(O_c\) represents the output of \(M_{\text{syn}}\), which is a step-by-step reasoning chain. The prompt \(P_{\text{coc}}\) utilizes a chain-of-citation~\cite{coc} prompting approach, requiring the model to reason with attribution (e.g., "Let's reason step by step while citing the document using [1], [2], etc."). The prompt template is shown in Figure~\ref{fig:pt_1}.

\paragraph{\textsc{LongFaith}-SFT Dataset.}

Towards training a downstream LLM to reason with high faithfulness for a long-context QA task, we construct the dataset for supervised fine-tuning, where each instruction pair is built as follow:

\begin{equation}
    I_{\text{sft}} = \{\text{input} = (P_{\text{coc}}, Q, D), \text{output} = O_c\}
\end{equation}

\paragraph{Synthesize Reasoning Chain with Questionable Faithfulness.}

To model fine-grained preferences, we address three challenges that affect the faithfulness of synthesized instructions: (1) misinformation due to lack of verification, (2) reasoning without attribution, and (3) potential knowledge conflicts. We synthesize reasoning chains with questionable faithfulness, including \textbf{reasoning chains with misinformation} as follows:

\begin{equation}
    O_m = M_{\text{syn}}(P_{\text{coc}}, Q, D)
\end{equation}

Since there is no ground truth to guide the synthesis, the output \(O_m\) may contain errors in reasoning, as illustrated in Figure~\ref{fig:pt_2}, where the model generates an incorrect answer of "1903" instead of the correct answer "1698". This hallucination is common in synthesized data from previous works unless rules or human experts are involved in filtering~\cite{coc}. Next, we synthesize \textbf{reasoning chains without attribution}:

\begin{equation}
    O_{\text{cot}} = M_{\text{syn}}(P_{\text{cot}}, Q, F, A)
\end{equation}

Here, the CoT~\cite{cot} prompting only requires the model to provide step-level reasoning, but as shown in Figure~\ref{fig:pt_3}, reasoning without attribution not only loses interpretability and credibility~\cite{alce, attribution}, but our results in Tab.~\ref{tab:ab_models} (Sec.~\ref{sec:exp}) also demonstrate that CoT prompting performs worse than CoC. Finally, we synthesize \textbf{reasoning chains with potential knowledge conflicts}:

\begin{equation}
    O_{\text{kc}} = M_{\text{syn}}(P_{\text{cot}}, Q, A)
\end{equation}

Since no context is provided, the model relies solely on its parametric knowledge for reasoning, as shown in Figure~\ref{fig:pt_4}, where the model states, "Panama was not colonized by the United Kingdom; Panama was colonized by Spain," based on internal parametric knowledge rather than the contextual documents. Previous studies~\cite{longreward} using short-context reward models observes performance degradation by ignoring long-context materials, highlighting the limitation of knowledge conflicts in affecting LLM's performance in long-context QA and reasoning tasks.

\begin{table}[!t]
    \tabcolsep 1.5pt
    \centering
    \small
    \begin{tabular}{lccccc}
        \toprule
        \multicolumn{6}{c}{\textit{Synthesis of Reasoning Chains}} \\
        \midrule
        \multicolumn{1}{c}{\textbf{Models}} & \textbf{Prompt} & \textbf{w/ GT} & \textbf{w/ Doc} & {\textbf{Output}} & \textbf{Size} \\
        \midrule
        \textit{Q-7B} & CoC & \checkmark & \checkmark & 1 & 1-8K \\
        \textit{Q-7B} & CoT & \checkmark & \checkmark & 2 & 1-8K \\ 
        \textit{Q-7B} & CoC & \ding{55} & \checkmark & 3 & 1-8K \\ 
        \textit{Q-7B} & CoT & \checkmark & \ding{55}  & 4 & 1-8K\\
        \textit{L8,L70,G} & CoC & \checkmark & \checkmark & 5& 2K  \\
        \textit{L8,L70,G} & CoT & \checkmark & \checkmark & 6 & 2K \\ 
        \textit{L8,L70,G} & CoC & \ding{55} & \checkmark & 7 & 2K \\ 
        \textit{L8,L70,G} & CoT & \checkmark & \ding{55} & 8 & 2K \\  
        \midrule
        \multicolumn{6}{c}{\textit{Datasets for Supervised Fine-tuning}} \\
        \midrule
        \multicolumn{1}{c}{\textbf{Name}} & \textbf{Models} & \textbf{Instruction} & \textbf{Output} & & \textbf{Size} \\
        \midrule
        \textbf{LF-SFT} & \textit{Q-7B} & CoC & 1 & & 1-8K \\
        \;\;w/o CoC & \textit{Q-7B} & CoT & 2 & & 1-8K \\
        \;\;w/o GT & \textit{Q-7B} & CoC & 3 & & 1-8K \\
        \;\;w/o Doc & \textit{Q-7B} & CoC & 4 & & 1-8K \\
        \textbf{LF-SFT} & \textit{L8,L70,G} & CoC & 5 & & 2K \\
        \;\;w/o CoC & \textit{L8,L70,G} & CoT & 6 & & 2K \\
        \;\;w/o GT & \textit{L8,L70,G} & CoC & 7 & & 2K \\
        \;\;w/o Doc & \textit{L8,L70,G} & CoC & 8 & & 2K \\
        \midrule
        \multicolumn{6}{c}{\textit{Datasets for Preference Optimization}} \\
        \midrule
        \multicolumn{1}{c}{\textbf{Name}} & \textbf{Models} & \textbf{Instruction} & \textbf{Chosen} & \textbf{Rejected} & \textbf{Size}\\
        \midrule
        \;\;w/ CoC & \textit{Q-7B} & CoC & 1 & 2 & 1-8K \\
        \;\;w/ GT & \textit{Q-7B} & CoC & 1 & 3 & 1-8K \\
        \;\;w/ Doc & \textit{Q-7B} & CoC & 1 & 4 & 1-8K \\
        \textbf{LF-PO} & \textit{Q-7B} & CoC & 1 & 2,3,4 & 1-8K \\
        \;\;w/ CoC & \textit{L8,L70,G} & CoC & 5 & 6 & 2K \\
        \;\;w/ GT & \textit{L8,L70,G} & CoC & 5 & 7 & 2K \\
        \;\;w/ Doc & \textit{L8,L70,G} & CoC & 5 & 8 & 2K \\
        \textbf{LF-PO} & \textit{L8,L70,G} & CoC & 5 & 6,7,8 & 2K \\
        \bottomrule
    \end{tabular}
    \caption{Statistics of synthesized datasets for SFT and PO. We first synthesize large-scale reasoning chains and then refactor them to datasets, where the second stage does not require llm inference. \textbf{\textit{Q-7B}} means \textit{\qwen}, \textbf{\textit{L8}} means \textit{\llama}, \textbf{\textit{L70}} means \textit{\llamal} and \textbf{\textit{G}} means \textit{\gpt}. \textbf{GT} means Ground Truth, \textbf{CoC} means chain-of-citation, \textbf{Doc} means contextual documents, and \textbf{LF} means \textsc{LongFaith}. 1-8K includes \{1K, 2K, 4K, 8K\}.}
    \label{tab:synset_stat}
\end{table}


\begin{table*}[t]
\centering
\scriptsize
\tabcolsep 3.0pt
\begin{tabular}{lcccccccccccccccc}
\toprule
\multirow{2}{*}{\textsc{\llama}} &
\multicolumn{2}{c}{\textbf{\musique}} &
\multicolumn{2}{c}{\textbf{2Wiki}} &
\multicolumn{2}{c}{\textbf{HotpotQA}} &
\multicolumn{2}{c}{\textbf{Qasper(S)}} &
\multicolumn{2}{c}{\textbf{MFQA-En(S)}} &
\multicolumn{2}{c}{\textbf{\musique(M)}} &
\multicolumn{2}{c}{\textbf{2Wiki(M)}} &
\multicolumn{2}{c}{\textbf{\hotpot(M)}} \\
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} \cmidrule(r){10-11} \cmidrule(r){12-13} \cmidrule(r){14-15} \cmidrule(r){16-17}
& F1 & SubEM & F1 & SubEM & F1 & SubEM & F1 & SubEM & F1 & SubEM & F1 & SubEM & F1 & SubEM & F1 & SubEM  \\
\midrule
\multicolumn{17}{c}{\textit{Zero-Shot Prompting}} \\
\midrule
\quad + CoT & 15.9 & 56.8 & 34.0 & 83.8 & 20.8 & 78.6 & 3.2 & 22.0 & 5.7 & 29.3 & 14.1 & 43.5 & 30.1 & 77.0 & 13.4 & 60.5 \\
\quad + CoC & 25.8 & 64.2 & 43.6 & 86.2 & 32.7 & 76.6 & 4.6 & 26.0 & 7.0 & 32.7 & 11.8 & 41.0 & 28.1 & 79.5 & 19.9 & 58.0 \\
\midrule
\multicolumn{17}{c}{\textit{Superivised Fine-tuning}} \\
\midrule
\quad + LongAlpaca & 21.6 & 50.2 & 47.8 & 80.4 & 32.7 & 76.6 & 5.7 & 25.0 & 5.8 & 30.7 & 8.5 & 48.5 & 25.4 & 77.0 & 12.5 & 61.0 \\
\quad + LongAlign & 24.8 & 48.4 & 55.6 & 84.2 & 51.0 & 79.2 & 6.5 & 24.0 & 10.7 & \textbf{38.7} & 15.0 & 40.0 & 33.4 & 76.5 & 35.8 & 61.0 \\
\quad + \musique-Attribute & 13.9 & 19.2 & 23.9 & 49.6 & 20.2 & 37.2 & 10.0 & 11.5 & 8.3 & 12.0 & 15.2 & 26.5 & 21.2 & 55.0 & 25.6 & 41.0 \\
\quad + LongMIT & 4.9 & 33.0 & 3.3 & 58.0 & 10.1 & 63.6 & 9.5 & 18.5 & 5.6 & 30.0 & 7.5 & 29.0 & 3.6 & 55.5 & 23.7 & 50.0 \\
\quad + LongReward-SFT & 6.2 & 48.4 & 23.3 & 80.0 & 15.6 & 74.2 & 2.6 & 22.5 & 0.5 & \underline{34.0} & 1.1 & 43.0 & 6.6 & 71.5 & 8.9 & 54.0 \\
\quad + \textsc{SeaLong}-SFT & 31.3 & \underline{64.6} & 55.8 & \underline{89.2} & 59.4 & \textbf{83.0} & 14.5 & 26.0 & 18.6 & 31.3 & 24.1 & \textbf{59.5} & 34.1 & \underline{84.5} & 37.3 & \underline{69.0} \\
\quad + \textsc{LongFaith}-SFT & \underline{56.8} & 62.8 & \textbf{73.8} & 85.6 & \textbf{70.5} & 80.8 & \underline{36.9} & \underline{29.5} & \textbf{47.0} & 32.0 & \underline{50.1} & \underline{56.5} & \underline{63.9} & 82.0 & \underline{53.1} & 68.0 \\
\midrule
\multicolumn{17}{c}{\textit{Preference Optimization}} \\
\midrule
\quad + \textsc{LongReward}-PO & 3.3 & 46.0 & 14.3 & 76.6 & 8.9 & 71.2 & 1.6 & 21.0 & 0.1 & 32.7 & 0.0 & 37.5 & 4.4 & 67.0 & 3.3 & 53.0 \\
\quad + \textsc{SeaLong}-PO & 30.2 & 60.4 & 50.1 & \textbf{89.4} & 58.3 & \textbf{83.4} & 17.1 & 28.0 & 20.1 & 32.0 & 18.1 & 53.3 & 34.0 & \textbf{86.0} & 40.2 & \textbf{69.5} \\
\quad + \textsc{LongFaith}-PO & \textbf{60.5} & \textbf{66.4} & \underline{68.0} & 85.0 & \underline{65.4} & 81.2 & \textbf{38.1} & \textbf{30.5} & \underline{46.7} & 32.0 & \textbf{50.2} & 52.0 & \textbf{73.7} & 83.5 & \textbf{55.6} & 67.5 \\
\midrule
\end{tabular}
\caption{Main experiment results on three multi-hop reasoning test sets and five long-context QA test sets from LongBench. The best results are in \textbf{bold} and second-best are \underline{underlined}. \textbf{(S)} means single-doc QA task and \textbf{(M)} means multiple-docs QA task in LongBench. \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO are synthesized by \textit{\gpt} both in 2K size. To ensure fairness, we sample first 2K examples from baseline datasets.}
\label{tab:longbench_exp}
\end{table*}


\paragraph{\textsc{LongFaith-PO} Dataset.}

Towards training a downstream LLM to address three challenges above in long-context reasoning, we force the LLM to learn reasoning with high faithfulness while rejecting outputs of questionable faithfulness:

\begin{equation}
\begin{split}
    I_{\text{po}} = \{\text{input} = (P_{\text{coc}}, Q, D), \\
    \text{chosen} = O_c, \text{rejected} = O_r\}
\end{split}
\end{equation}

where \(O_r\) is a combination of (\(O_m\), \(O_{\text{cot}}\), \(O_{\text{kc}}\)), or a subset of them.

