\section{Introduction}
\label{sec:intro}

Long-context processing ability has emerged as a significant challenge for large language models (LLMs)~\cite{distracted, lostinthemiddle, irrelevant, sametaskmoretokens}, especially arises when models process extensive textual information, making it hard to recognize relevant evidence and address downstream tasks such as question answering (QA), summarization, and complex reasoning~\cite{longbench, longbenchv2, bench, ruler, helmet}. A variety of model-centric methods have been proposed to extend the length of context windows in LLMs~\cite{extending, longlora, yarn, lminfinite, longrope}. Additionally, many data-centric methods, such as synthesizing long-context understanding instructions for fine-tuning, have gained attention for enhancing modelsâ€™ ability to handle and utilize extended contexts~\cite{effective, in2, dataengineering, longalign, coc, prolong, longmit, longreward, sealong}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/LongFaith.pdf}
    \caption{A brief introduction of \textsc{LongFaith}. Synthesized long-context reasoning instruction sets and preference datasets are fed into the post-training stage of downstream LLMs.}
    \label{fig:small}
    \vspace{-5pt}
\end{figure}

\begin{figure*}[t]
    \centering
    \centerline{\includegraphics[width=0.88\linewidth]{figures/longfaith_small.pdf}}
    \caption{Overview of \textsc{LongFaith} pipeline for synthesizing faithful long-context reasoning instruction and preference datasets. Comparing generated reasoning chains with misinformation, lack of attribution, and knowledge conflicts, \textsc{LongFaith} generates ground truth guidance prompting by chain-of-citation to build \textsc{LongFaith}-SFT. Fine-grained faithfulness is modeled by optimization on our preference datasets \textsc{LongFaith}-PO.}
    \label{fig:main}
\end{figure*}

Despite the improvements in downstream QA performance enabled by synthetic long-context reasoning instructions, concerns remain regarding the faithfulness of such generated data. Specifically: (1) \textbf{Misinformation due to lack of verification}: existing methods often generate QA pairs without rigorous rule-based verification. For instance, \cite{longmit, longreward, sealong} directly synthesize QA pairs using LLMs while bypassing verification, whereas \cite{longreward} relies on AI-generated feedback in soft dimensions rather than human annotation. (2) \textbf{Reasoning without attribution}: prompting LLMs to generate responses with citation, such as using \textit{Chain-of-Citation (CoC)} prompting~\cite{attribution, coc, learning, finegrained, cotar, alce} can enhance the credibility and interpretability of model outputs under long-context QA tasks~\cite{alce, longcite}, yet most prior works ignore to incorporate this technique during their synthesis of training instruction pairs. (3) \textbf{Potential knowledge conflicts}: some approaches \cite{longalign, longreward, longmit} over-rely on the \textit{Self-Instruct} technique~\cite{selfinstruct} to generate QA pairs, encouraging models to rely on parametric knowledge rather than grounding reasoning in explicit contextual evidence~\cite{knowledgeconflicts}. Additionally, ~\cite{longreward} feeds the query and response to a short-context reward model ignoring the long context to score, purely relying on the parametric knowledge inside LLM. These limitations underscore the necessity for a more robust pipeline that ensures the faithfulness of long-context instructions synthesis.

We propose \textbf{\textsc{LongFaith}}, a novel pipeline for synthesizing faithful long-context reasoning instruction datasets. We incorporate ground truth directly into the prompt for synthesizing long-context reasoning chains, which comprise supporting facts and the correct answer, and prompt LLMs to reason with attributions. This method ensures the faithfulness of synthesized reasoning chains without requiring costly verification by a curated rule-based evaluator, LLM-as-a-judge~\cite{llmasajudge} or human annotator. We open-source \textbf{\textsc{LongFaith}-SFT}, synthesized under the guidance of ground truth and CoC prompting. We leverage the faithful long-context reasoning chains with attributions for training, leading to performance improvements after fine-tuning \text{\llama}. Additionally, we synthesize preference datasets by sampling preference pairs around fine-grained faithfulness: (1) encouraging model to reason with attributions; (2) encouraging model to learn on verified reasoning chains; and (3) encouraging model to reason with contextual documents grounded. We open-source \textbf{\textsc{LongFaith}-PO}, synthesized by various LLMs in different sizes, which integrates all three faithfulness dimensions for preference optimization. We leverage these faithful preferred instruction pairs for training \llama, achieving performance improvements on the multi-hop reasoning dataset and LongBench~\cite{longbench}.

Our main contributions are as follows: (1) We introduce \textsc{LongFaith}, a novel pipeline for synthesizing faithful long-context reasoning instruction data. (2) We open-source \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO, two synthesized datasets that are systematically designed considering multiple dimensions of faithfulness. (3) We conduct extensive experiments on two types of datasets (comprising eight sub-tasks) to show that models trained on \textsc{LongFaith} datasets can improve in long-context reasoning and QA tasks. (4) Our ablation studies illustrate the scaling potential and adaptability of \textsc{LongFaith} pipeline, underscoring its broad applicability in the development of long-context LLMs.