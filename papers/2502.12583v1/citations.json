[
  {
    "index": 0,
    "papers": [
      {
        "key": "longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      },
      {
        "key": "longnet",
        "author": "Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu",
        "title": "Longnet: Scaling transformers to 1,000,000,000 tokens"
      },
      {
        "key": "longlora",
        "author": "Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya",
        "title": "Longlora: Efficient fine-tuning of long-context large language models"
      },
      {
        "key": "lminfinite",
        "author": "Han, Chi and Wang, Qifan and Peng, Hao and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong",
        "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "pose",
        "author": "Zhu, Dawei and Yang, Nan and Wang, Liang and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian",
        "title": "Pose: Efficient context window extension of llms via positional skip-wise training"
      },
      {
        "key": "extending",
        "author": "Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong",
        "title": "Extending context window of large language models via positional interpolation"
      },
      {
        "key": "longrope",
        "author": "Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao",
        "title": "Longrope: Extending llm context window beyond 2 million tokens"
      },
      {
        "key": "yarn",
        "author": "Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico",
        "title": "Yarn: Efficient context window extension of large language models"
      },
      {
        "key": "longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "effective",
        "author": "Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others",
        "title": "Effective long-context scaling of foundation models"
      },
      {
        "key": "prolong",
        "author": "Gao, Tianyu and Wettig, Alexander and Yen, Howard and Chen, Danqi",
        "title": "How to train long-context language models (effectively)"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dataengineering",
        "author": "Fu, Yao and Panda, Rameswar and Niu, Xinyao and Yue, Xiang and Hajishirzi, Hannaneh and Kim, Yoon and Peng, Hao",
        "title": "Data engineering for scaling language models to 128k context"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "in2",
        "author": "An, Shengnan and Ma, Zexiong and Lin, Zeqi and Zheng, Nanning and Lou, Jian-Guang",
        "title": "Make Your LLM Fully Utilize the Context"
      },
      {
        "key": "longalign",
        "author": "Bai, Yushi and Lv, Xin and Zhang, Jiajie and He, Yuze and Qi, Ji and Hou, Lei and Tang, Jie and Dong, Yuxiao and Li, Juanzi",
        "title": "Longalign: A recipe for long context alignment of large language models"
      },
      {
        "key": "coc",
        "author": "Li, Yanyang and Liang, Shuo and Lyu, Michael R and Wang, Liwei",
        "title": "Making long-context language models better multi-hop reasoners"
      },
      {
        "key": "longmit",
        "author": "Chen, Zhi and Chen, Qiguang and Qin, Libo and Guo, Qipeng and Lv, Haijun and Zou, Yicheng and Che, Wanxiang and Yan, Hang and Chen, Kai and Lin, Dahua",
        "title": "What are the essential factors in crafting effective long context multi-hop instruction datasets? insights and best practices"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "longreward",
        "author": "Zhang, Jiajie and Hou, Zhongni and Lv, Xin and Cao, Shulin and Hou, Zhenyu and Niu, Yilin and Hou, Lei and Dong, Yuxiao and Feng, Ling and Li, Juanzi",
        "title": "LongReward: Improving Long-context Large Language Models with AI Feedback"
      },
      {
        "key": "sealong",
        "author": "Li, Siheng and Yang, Cheng and Cheng, Zesen and Liu, Lemao and Yu, Mo and Yang, Yujiu and Lam, Wai",
        "title": "Large Language Models Can Self-Improve in Long-context Reasoning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dpo",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "orpo",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "siren",
        "author": "Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others",
        "title": "Siren's song in the AI ocean: a survey on hallucination in large language models"
      },
      {
        "key": "hallu",
        "author": "Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others",
        "title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "cotar",
        "author": "Berchansky, Moshe and Fleischer, Daniel and Wasserblat, Moshe and Izsak, Peter",
        "title": "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity"
      },
      {
        "key": "attribution",
        "author": "Li, Dongfang and Sun, Zetian and Hu, Xinshuo and Liu, Zhenyu and Chen, Ziyang and Hu, Baotian and Wu, Aiguo and Zhang, Min",
        "title": "A survey of large language models attribution"
      },
      {
        "key": "coc",
        "author": "Li, Yanyang and Liang, Shuo and Lyu, Michael R and Wang, Liwei",
        "title": "Making long-context language models better multi-hop reasoners"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "alce",
        "author": "Gao, Tianyu and Yen, Howard and Yu, Jiatong and Chen, Danqi",
        "title": "Enabling large language models to generate text with citations"
      },
      {
        "key": "automatic",
        "author": "Yue, Xiang and Wang, Boshi and Chen, Ziru and Zhang, Kai and Su, Yu and Sun, Huan",
        "title": "Automatic evaluation of attribution by large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tog",
        "author": "Sun, Jiashuo and Xu, Chengjin and Tang, Lumingyuan and Wang, Saizhuo and Lin, Chen and Gong, Yeyun and Shum, Heung-Yeung and Guo, Jian",
        "title": "Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph"
      },
      {
        "key": "tog2",
        "author": "Ma, Shengjie and Xu, Chengjin and Jiang, Xuhui and Li, Muzhi and Qu, Huaren and Yang, Cehao and Mao, Jiaxin and Guo, Jian",
        "title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation"
      }
    ]
  }
]