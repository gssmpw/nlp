\section{An Example of Synthesized \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO Datasets}

\begin{figure}[ht]
    \centering
    \centerline{\includegraphics[width=1.0\linewidth]{figures/longfaith_prompt_5.pdf}}
    \vspace{-5pt}
    \caption{An Example of synthesized \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO datasets.}
    \label{fig:overview}
    \vspace{-10pt}
\end{figure}

\clearpage

\section{Prompt Templates}
\label{sec:prompt}

We present the prompt templates that are used to synthesize the datasets. The core prompt template that generates long-context reasoning chains guided by ground truth using chain-of-citation is shown in Fig.~\ref{fig:pt_1}. The samples are used in \textsc{LongFaith}-SFT dataset and are chosen as positive in \textsc{LongFaith}-PO dataset, since they are of the highest faithfulness. The other three prompt templates synthesize rejected samples for \textsc{LongFaith}-PO dataset, corresponding to (1) Misinformation due to lack of verification ( Fig.~\ref{fig:pt_2}), (2) Reasoning without attribution (Fig.~\ref{fig:pt_3}), and (3) Potential knowledge conflicts (Fig.~\ref{fig:pt_4}).

\begin{figure}[ht]
    \centering
    \centerline{\includegraphics[width=1.0\linewidth]{figures/longfaith_prompt_1.pdf}}
    \vspace{-5pt}
    \caption{An example of synthesized chosen reasoning chain. The current reasoning chain must arrive at a correct answer, and reasoning with proper citation proposes more faithfulness and interpretability. Therefore, \textsc{LongFaith} will choose it in supervised fine-tuning and preference optimization as positive sample.}
    \label{fig:pt_1}
    \vspace{-10pt}
\end{figure}

\begin{figure}[ht]
    \centering
    \centerline{\includegraphics[width=1.0\linewidth]{figures/longfaith_prompt_2.pdf}}
    \vspace{-5pt}
    \caption{An example of synthesized rejected reasoning chain. Misinformation due to lack of verification will cause more hallucination if we use current reasoning chain to fine-tune a LLM. Therefore, \textsc{LongFaith} will reject it in preference optimization.}
    \label{fig:pt_2}
    \vspace{-10pt}
\end{figure}

\begin{figure}[ht]
    \centering
    \centerline{\includegraphics[width=1.0\linewidth]{figures/longfaith_prompt_3.pdf}}
    \vspace{-5pt}
    \caption{An example of synthesized rejected reasoning chain. As it mentioned in previous work, lack of attribution will lead to much more interpretability and faithfulness, and response with citation is encouraged. Therefore, \textsc{LongFaith} will reject it in preference optimization.}
    \label{fig:pt_3}
    \vspace{-10pt}
\end{figure}

\begin{figure}[ht]
    \centering
    \centerline{\includegraphics[width=1.0\linewidth]{figures/longfaith_prompt_4.pdf}}
    \vspace{-5pt}
    \caption{An example of synthesized rejected reasoning chain. Information related to "Panama was colonized by Spain" is not mentioned in documents provided, which will cause knowledge conflicts if using current reasoning chain for fine-tuning. Therefore, \textsc{LongFaith} will reject it in preference optimization.}
    \label{fig:pt_4}
    \vspace{-10pt}
\end{figure}

\clearpage

\section{Case Study of Metric "Hack" on SubEM comparing previous study with \textsc{LongFaith}}

\begin{figure}[ht]
    \centering
    \centerline{\includegraphics[width=0.83\linewidth]{figures/longfaith_hack.pdf}}
    \vspace{-5pt}
    \caption{A case study that SubEM metric is "hacked" by previous study, which conduct more exploration with redundancy in response. \textsc{LongFaith} can arrive at the final correct answer with shorter response.}
    \label{fig:hack}
    \vspace{-10pt}
\end{figure}

\clearpage

\section{Post-Training}
\label{sec:method:post}

% In this section, 我们展示两种后训练算法：SFT和PO，以更好地利用合成数据，高效地提高模型性能。
% 模型在高质量faithful output上sft，或使用合成偏好对进行强化学习。
In this section, we present two post-training algorithms—Supervised Fine-Tuning (SFT) and Preference Optimization (PO)—to better leverage synthetic data for efficiently enhancing model performance. Specifically, the model performs supervised fine-tuning on high-quality faithful outputs or is trained through reinforcement learning using synthetic preference pairs.

\paragraph{Supervised Fine-tuning on Faithful Outputs}
We minimize the negative log-likelihood of the output as follows:
\begin{equation}
\begin{aligned}    
    \mathcal{L}_{\text{SFT}}
    &= -\frac{1}{|y|} \log \pi_{\theta}(y \;\vert\; x) \\
    &= -\frac{1}{|y|} \sum_{i=1}^{|y|} \log \pi_{\theta}(y_i \;\vert\; x,y_{<i})
\end{aligned}
\end{equation}
where $y$ denotes the high-quality faithful outputs, which are synthesized in Section~\ref{sec:method}.

\paragraph{Reinforcement Learning from Synthetic Preference}

% 另外，我们可以利用合成的偏好对进行强化学习，将模型强化到faithful output上，减少低分output的似然。
% 用于优化LLM的标准RL算法包含PPO，Reinforce，RLOO等。但这些方法计算成本较高。人们提出DPO，KTO，ORPO等以降低计算成本和数据成本。本文中，我们采用 ORPO 算法，它在计算成本和模型性能之间取得平衡。
Additionally, we can leverage synthetic preference pairs for reinforcement learning (RL) to fine-tune the model toward generating faithful outputs while reducing the likelihood of low-scoring outputs. Standard RL algorithms for optimizing LLMs include Proximal Policy Optimization (PPO)~\cite{ppo}, RLOO~\cite{RLOO}. However, these methods incur high computational costs. Recent approaches such as Direct Preference Optimization (DPO)~\cite{dpo}, Kahneman-Tversky Optimization (KTO)~\cite{kto}, and Odds Ratio Preference Optimization (ORPO)~\cite{orpo} have been proposed to mitigate both computational and data requirements. In this work, we adopt the ORPO algorithm, which achieves an optimal balance between computational efficiency and model performance.

ORPO introduces an odds ratio loss $\mathcal{L}_{OR}$ that minimizes the negative log odds ratio between preferred ("win" $y_{w}$) and dispreferred ("lose" $y_{l}$) outputs:
% ORPO 引入一个odd ratio loss （OR loss），最小化win 和 lose otuput之间的neg log odds ratio：
\begin{equation}
    \mathcal{L}_{\text{OR}}=-\log \sigma \left( \log \frac{\text{odds}_{\theta}(y_{w}\vert x)}{\text{odds}_{\theta}(y_{l}\vert x)} \right)
\end{equation}
where \(\sigma\) denotes the sigmoid function and \(\text{odds}_{\theta}(y\vert x) = \frac{\pi_{\theta}(y\vert x)}{1 - \pi_{\theta}(y\vert x)}\) measures how much more likely \(y\) is to be generated. The final objective of ORPO is to combine the SFT loss and the OR loss while controlling their relative importance through a hyperparameter \(\beta\):

\begin{equation}
    \mathcal{L}_{\text{ORPO}}=\mathcal{L}_{\text{SFT}} + \beta \cdot \mathcal{L}_{\text{OR}}
\end{equation}

% 本文中，chosen output y_w 是由 LongFaith 合成的，考虑了 Context，CoC和GT而被认为是高分。rejected output y_l 是缺少这三者之一而合成的ooutput，它们由于缺少必要的设计而被认为是低分
In this paper, the chosen output \(y_w\) is synthesized by LongFaith through comprehensive consideration of Supporting Docs, Chain-of-Citation (CoC), and Ground Truth (GT), and is consequently assigned a high score. Conversely, the rejected output \(y_l\) refers to synthesized outputs that lack at least one of these three critical elements (Supporting Docs, CoC, or GT), which are deemed low-scoring due to insufficient design considerations.

\section{Statistics of Main Experiments}

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        \multicolumn{1}{c}{\textbf{\musique}} & \textbf{\#2-Hop} & \textbf{\#3-Hop} & \textbf{\#4-Hop} \\
        \midrule
        \textit{1K}    & 0 & 512 & 512 \\
        \textit{2K}    & 512 & 512 & 1024 \\
        \textit{4K}    & 1024 & 2048 & 1024 \\
        \textit{8K}    & 3072 & 4096 & 1024 \\
        \bottomrule
    \end{tabular}
    \caption{Statistics of train set for synthesis in different size sampled from \textbf{\musique}~\cite{musique}. }
    \label{tab:train_stat}
\end{table}

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{lccc}
        \toprule
        \multicolumn{1}{c}{\textbf{Datasets}} & \textbf{\#Count} & \textbf{Avg. L.} & \textbf{Max L.} \\
        \midrule
        \multicolumn{4}{c}{\textit{Multi-Hop Reasoning}} \\
        \midrule
        \musique & 500 & & \\
        2-Hop    & 254 & 10843.3 & 17560 \\
        3-Hop    & 154 & 11456.5 & 19225 \\
        4-Hop    & 92  & 11224.3 & 16756 \\
        \midrule
        \twowiki & 500 & & \\
        2-Hop    & 395 & 4449.5 & 10631 \\
        4-Hop    & 105 & 4041.4 & 9365  \\
        \midrule
        \hotpot    & 500 & & \\
        Bridge     & 412 & 6301.0 & 15702 \\
        Comparison & 88  & 5777.6 & 11939 \\
        \midrule
        \multicolumn{4}{c}{\textit{LongBench}} \\
        \midrule
        Qasper (S)          & 200 & 24262.3 & 101636 \\
        MultiFieldQA-En (S) & 150 & 29583.7 & 64751 \\
        MuSiQue (M)         & 200 & 69876.8 & 82338 \\
        2WikiMHQA (M)           & 200 & 30076.5 & 72971 \\
        HotpotQA (M)        & 200 & 57041.4 & 81815 \\
        \bottomrule
    \end{tabular}
    \caption{Statistics of test sets including three long-context multi-hop reasoning datasets sampled by~\cite{multihopdatasets} and five long-context QA datasets from LongBench~\cite{longbench}. \textbf{Avg. L.} and \textbf{Max L. } refer to the average length and max length of input prompts for test samples. \textbf{(S)} and \textbf{(M)} refer to Single-doc QA and Multi-doc QA in LongBench.}
    \label{tab:dataset_stat}
\end{table}

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{ccccc}
        \toprule
        \multicolumn{1}{c}{\textbf{Datasets}} & \textbf{Instruction} & \textbf{Output(Chosen)} & \textbf{Rejected} \\
        \midrule
        LongAlpaca    & 52043.2 & 620.7 & 0 \\
        LongAlign    & 36307.2 & 1412.6 & 0 \\
        \musique-Attribute & 11395.0 & 343.7 & 0 \\
        LongMIT    & 280808.9 & 825.2 & 0 \\
        LongReward & 72892.2 & 913.4 & 960.6 \\
        \textsc{SeaLong} & 82248.6 & 1156.5 & 1139.1 \\
        \textsc{LongFaith} & 11542.1 & 1029.6 & 896.7 \\
        \bottomrule
    \end{tabular}
    \caption{Average text length of baseline datasets and \textsc{LongFaith} in main experiments in Tab.~\ref{tab:longbench_exp}. All of them has \textit{2K} examples.}
    \label{tab:baseline_stat}
\end{table}

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{ccccccc}
        \toprule
        \textbf{Datasets} & \textbf{\musique} & \textbf{\twowiki} & \textbf{HotpotQA} & \textbf{Qasper} & \textbf{MultiFieldQA} & \textbf{Avg.L} \\
        \midrule
        LongAlpaca      & 365.62  & 372.97   & 319.60   & 657.34  & 511.25  & 445.36 \\
        LongAlign       & 493.56  & 349.65   & 371.77   & 651.76  & 623.15  & 497.98 \\
        \musique-Attribute & 99.61   & 168.74   & 164.24   & 317.75  & 252.95  & 200.66 \\
        LongMIT         & 138.03  & 159.16   & 116.40   & 194.41  & 196.43  & 160.89 \\
        LongReward-SFT  & 285.20  & 241.47   & 178.26   & 750.83  & 537.95  & 398.74 \\
        SeaLong-SFT     & 1091.54 & 776.01   & 926.29   & 1035.77 & 822.82  & 930.49 \\
        LongFaith-SFT   & 820.04  & 619.18   & 771.68   & 1056.55 & 941.13  & 841.72 \\
        LongReward-PO   & 219.90  & 253.41   & 179.44   & 616.11  & 460.40  & 345.85 \\
        SeaLong-PO      & 961.51  & 740.14   & 891.75   & 946.68  & 826.77  & 873.37 \\
        LongFaith-PO    & 831.17  & 669.76   & 786.77   & 1034.71 & 917.11  & 847.90 \\
        \bottomrule
    \end{tabular}
    \caption{Average length of model output in test sets trained on different synthesized instruction.}
    \label{tab:output_stat}
\end{table}

\clearpage

\section{Hyperparameters}
\label{sec:hyperparameters}

\begin{table}[ht]
\centering
\begin{tabular}{c c}
\toprule
\textbf{Hyperparameters}        & \textbf{Value} \\
\midrule
\# GPUs used                    & 4                \\
Learning rate                   & 5e-5             \\
Per-device batch size           & 1                \\
Gradient accumulation steps     & 8                \\
LoRA rank                       & 32               \\
LoRA alpha                      & 64               \\
LoRA dropout                    & 0.1              \\
ORPO beta                       & 0.1              \\
Warm-up ratio                   & 0.1              \\
Epochs                          & 1                \\
Precision                       & bfloat16         \\
Optimizer                       & AdamW            \\
\bottomrule
\end{tabular}
\caption{Hyperparameter settings of fine-tuning and preference optimization.}
\label{tab:hyperparameters}
\end{table}

% \clearpage
