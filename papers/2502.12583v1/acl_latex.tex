% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{arydshln}
\usepackage{xspace}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage[symbol]{footmisc}

\setlength{\textfloatsep}{5pt plus 3pt minus 3pt}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\musique}{MuSiQue}
\newcommand{\twowiki}{2WikiMultiHopQA}
\newcommand{\hotpot}{HotpotQA}
\newcommand{\llama}{Llama-3.1-8B-Instruct}
\newcommand{\llamal}{Llama-3.1-70B-Instruct}
\newcommand{\qwen}{Qwen2.5-7B-Instruct}
\newcommand{\gpt}{GPT-4o mini}

\title{\textsc{LongFaith}: Enhancing Long-Context Reasoning in LLMs with Faithful Synthetic Data}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
 \textbf{ Cehao Yang\textsuperscript{1,2}}\footnotemark[1],
 \textbf{ Xueyuan Lin\textsuperscript{1,2}}\footnotemark[1],
 \textbf{ Chengjin Xu\textsuperscript{1}}\footnotemark[1],
 \textbf{ Xuhui Jiang\textsuperscript{1}},\\
 \textbf{ Shengjie Ma\textsuperscript{1}},
 \textbf{ Aofan Liu\textsuperscript{1}},
 \textbf{ Hui Xiong\textsuperscript{2}}\footnotemark[2],
 \textbf{ Jian Guo\textsuperscript{1}}\footnotemark[2],
\\
 \textsuperscript{1}IDEA Research, International Digital Economy Academy
 \\
 \textsuperscript{2}Artificial Intelligence Thrust, Hong Kong University of Science and Technology (Guangzhou)
\\
\texttt{\{yangcehao,linxueyuan,xuchengjin,jiangxuhui,mashengjie,liuaofan,guojian\}@idea.edu.cn}
\\
\texttt{\{cyang289,xlin058\}@connect.hkust-gz.edu.cn}, \texttt{xionghui@ust.hk}\\
 % \small{
 %   \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
 % }
}

\begin{document}
\maketitle
\footnotetext[1]{Equal contribution.}
\footnotetext[2]{Corresponding authors.}
\footnotetext[3]{Our code implementation and datasets can be accessed at \href{https://github.com/IDEA-FinAI/LongFaith}{https://github.com/IDEA-FinAI/LongFaith}.}
\input{abs}
\input{intro}
\input{related}
\input{method}
\input{exp}
\input{conclusion}

\section*{Limitations}
While \textsc{LongFaith} demonstrates significant improvements in long-context reasoning tasks, its scalability and generalization to other LLMs remain an open question. Our experiments focused on a single model, and thus, the performance of \textsc{LongFaith} on other general-purpose LLMs still needs further validation. Additionally, while the synthesized instruction sets with lengths of approximately 10,000 tokens successfully generalized to long-context reasoning tasks, future work will explore the extension of \textsc{LongFaith} to generate instructions with even longer contexts and evaluate the impact on model performance. Finally, \textsc{LongFaith} currently concentrates on reasoning tasks, and we plan to explore its generalization to other tasks such as summarization, dialogue generation, and others, to assess its broader applicability.

\bibliography{custom}

\appendix
\onecolumn
\input{app}

\end{document}
