\section{Experiments}
\label{sec:exp}

\begin{table*}[t]
\centering
\small
\tabcolsep 4.0pt
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{\textsc{\llama}} &
\multicolumn{4}{c}{\textbf{\musique}} &
\multicolumn{3}{c}{\textbf{\twowiki}} &
\multicolumn{3}{c}{\textbf{\hotpot}} \\
\cmidrule(r){2-5} \cmidrule(r){6-8} \cmidrule(r){9-11}
& Overall & 2-Hop & 3-Hop & 4-Hop & Overall & 2-Hop & 4-Hop & Overall & Bridge & Comparison\\
\midrule
\multicolumn{11}{c}{\textit{Zero-Shot Prompting}} \\
\midrule
\quad + CoT & 11.0 & 7.5 & 16.2 & 12.0 & 29.0 & 22.3 & 54.3 & 17.4 & 17.5 & 17.0 \\
\quad + CoC & 19.0 & 16.1 & 22.7 & 20.7 & 39.2 & 31.4 & 68.6 & 30.4 & 28.6 & 38.6 \\
\midrule
\multicolumn{11}{c}{\textit{Superivised Fine-tuning}} \\
\midrule
\quad + \textsc{LongFaith}-SFT & 40.6 & 44.1 & 37.7 & 35.9 & 55.4 & 51.1 & 71.4 & 53.6 & 57.0 & 37.5 \\
\quad\quad w/o CoC & 40.2 & 41.7 & 39.6 & 37.0 & 51.8 & 48.9 & 62.9 & 52.0 & 56.6 & 30.7 \\
\quad\quad w/o GT & 30.4 & 31.9 & 28.6 & 29.3 & 55.8 & 49.6 & 79.0 & 56.6 & 54.9 & \textbf{64.8} \\
\quad\quad w/o Doc & 20.0 & 23.6 & 18.2 & 13.0 & 55.8 & 47.1 & \textbf{88.6} & 47.4 & 45.9 & 54.5 \\
\midrule
\multicolumn{11}{c}{\textit{Preference Optimization}} \\
\midrule
\quad\quad w/ GT-PO & 44.0 & 45.7 & 42.9 & 41.3 & 56.0 & 50.4 & 77.1 & 54.4 & 58.3 & 36.4 \\
\quad\quad w/ CoC-PO & 43.6 & 44.5 & 44.8 & 39.1 & 53.2 & 48.6 & 70.5 & 56.2 & 59.2 & 42.0 \\
\quad\quad w/ Doc-PO & 41.4 & 42.5 & 40.3 & 40.2 & 56.0 & 52.7 & 68.6 & 56.4 & 59.5 & 42.0 \\
\quad + \textsc{LongFaith}-PO & \textbf{46.6} & \textbf{47.2} & \textbf{48.1} & \textbf{42.4} & \textbf{59.0} & \textbf{55.9} & 70.5 & \textbf{58.6} & \textbf{59.7} & 53.4 \\
\midrule
\end{tabular}
\caption{Main experiment results on three long-context multi-hop reasoning datasets using the Exact-Match(\textbf{EM}) metric. The best results are in \textbf{bold}. The training set has 2K samples for both SFT and PO, synthesized by \textit{\qwen}. Detail statistics of synthetics datasets are presented in Tab.~\ref{tab:synset_stat}.}
\label{tab:multihop_exp}
\end{table*}

\subsection{Implementation Details} Following previous studies, we leverage the training set of \musique~\cite{musique}, which is build on Wikipedia documents with supporting documents and correct answers. The officially retrieved 20 documents are provided and read only once in the input context in distractor setting. The statistics of training set is given in Tab.~\ref{tab:train_stat}, covering \textit{1K}, \textit{2K}, \textit{4K} and \textit{8K}, where the balance of questions with different hops are considered. Following the pipeline we describe in Sec.~\ref{sec:method}, reasoning chains are samples to build \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO. The statistics are presented in Tab.~\ref{tab:synset_stat}.

We conduct our experiments on a Linux server equipped with 4 A100-SXM4-40GB GPUs. For data synthesis of long-context reasoning instructions, we take \textit{\llama}~\cite{llama3}, \textit{\qwen}~\cite{qwen2.5}, \textit{\llamal} and \textit{\gpt}~\cite{gpt-4o} as generators and prompt LLMs to synthesize reasoning chains with vLLM~\cite{vllm}. We adopt the LoRA technique~\cite{lora} for fine-tuning and ORPO technique~\cite{orpo} for preference optimization using the LLaMA-Factory framework~\cite{llamafactory} to train \textit{\llama}. Hyperparameters of post-training are given in App.~\ref{sec:hyperparameters}.

\begin{figure}[!t]
    \centering
    \centerline{\includegraphics[width=\linewidth]{figures/longfaith_scale.png}}
    \caption{Performance of \textit{\llama} trained on different size of instructions synthesized by \textit{\qwen} from \textit{1K} to \textit{8K}, evaluated by \textbf{EM} and \textbf{F1} metrics on multi-hop reasoning sets and LongBench.}
    \label{fig:scale}
\end{figure}

\subsection{Evaluation Setup}
Following prior work~\cite{coc}, we utilize \textbf{three multi-hop reasoning datasets}, including \musique~\cite{musique}, \twowiki~\cite{twowiki}, and \hotpot~\cite{hotpotqa}, evaluating in distractor-setting, where the officially retrieved 10 or 20 documents are provided and read only once in the input context. We adopt the test sets sampled by~\cite{multihopdatasets}, with 500 examples in each set. Furthermore, in line with previous studies~\cite{longmit, longreward, sealong}, we assess the performance on \textbf{LongBench}~\cite{longbench}, which includes two test sets for single-doc QA including Qasper~\cite{qasper} and MultiFieldQA-EN~\cite{longbench}, as well as three test sets for multi-docs QA tasks including \hotpot, \twowiki, and \musique. Notably, although there is \textbf{an overlap in multi-hop reasoning tasks}, the LongBench version \textbf{further extends the lengths of document text}. To apply CoC prompting, single document is split into 20 even paragraphs with order. The statistics of datasets are listed in Tab.~\ref{tab:dataset_stat}. 

To ensure fairness, we use Substring Exact-Match (\textbf{SubEM})~\cite{helmet, sealong} metric in main experiments, in case that models trained on baseline datasets are not good at instructions following to summarize the answer with "The answer is", and SubEM goes through the whole response to check whether the answer is in. Furthermore, following previous work~\cite{quac, retrieval, coc}, we use \textbf{EM} metric and \textbf{F1} scores for the trimmed part after "The answer is" for evaluation in main experiments and ablation studies. Comparing with LLM-as-a-Judge~\cite{longalign, longmit, longreward} using strong API models like GPT-4o, the rule-based metrics cost much lower.

\subsection{Baselines}
We compete \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO with datasets proposed in previous studies, including \textsc{LongAlpaca}~\cite{longlora}, \textsc{LongAlign}~\cite{longalign}, \textsc{MuSiQue-Attribute}~\cite{coc}, \textsc{LongMIT}~\cite{longmit}, \textsc{LongReward}-SFT~\cite{longreward}, \textsc{SeaLong}-SFT~\cite{sealong} for supervised fine-tuning, and \textsc{LongReward}-PO and \textsc{SeaLong}-PO for preference optimization. All of them aim at enhancing the performance of LLMs on long-context understanding, reasoning, and QA tasks. To ensure fairness, we keep the training setting consistent with App.~\ref{sec:hyperparameters} and truncate the size of training samples to a maximum of 2K.

\begin{figure*}[t]
    \centering
    \centerline{\includegraphics[width=\linewidth]{figures/longfaith_point.png}}
    \caption{Scatter plot with a linear regression line fitting the relationship between \textbf{QA - EM} and \textbf{Attribution - F1} metrics on three long-context multi-hop reasoning test sets. A point refers to the performance of a model trained with a specific size between \textit{1K} to \textit{8K} by SFT or PO.}
    \label{fig:point}
    \vspace{-5pt}
\end{figure*}

\subsection{Main Results}  
\paragraph{\textsc{LongFaith} Outperforms Previous Datasets.} Following previous work and to ensure a fair comparison, we evaluate the performance of \textsc{LongFaith} on multi-hop reasoning test sets~\cite{musique, twowiki, hotpotqa} and LongBench~\cite{longbench}, comparing it against baseline methods, including zero-shot prompting with \llama and models trained on synthetic datasets from previous works. As shown in Tab.~\ref{tab:longbench_exp}, \textsc{LongFaith} outperforms baseline datasets on most test sets. The performance of the model trained on \textsc{LongFaith}-PO surpasses that trained on \textsc{LongFaith}-SFT. This aligns with our expectations: compared to directly using positive samples for supervised fine-tuning, incorporating rejected samples to provide more fine-grained faithfulness preferences for optimization leads to better improvements in long-context reasoning and QA capabilities. We observe that some synthetic instruction sets degrade performance compared to native \llama. This proves that datasets with questionable faithfulness are even harmful to long-context reasoning ability of LLMs.

\paragraph{\textsc{LongFaith} Arrives at the Correct Answer without Redundant Exploration.} We find that on \twowiki, \hotpot, and part of tasks in LongBench, \textsc{SeaLong} achieves a slight advantage in the SubEM metric against \textsc{LongFaith}, but fails in F1 scores. We investigate the length of response and present in Tab.~\ref{tab:output_stat}. It turns out that the LLM trained on \textsc{SeaLong} conducts redundant exploration in response, producing more noisy content related to the answer, but actually arrives at a wrong answer, which means SubEM metric is easily to be "hacked". In contradiction, F1 scores requires to truncate the part after "The answer is", which demonstrates that a model trained on \textsc{LongFaith} datasets can arrive at the correct answer without redundant exploration and achieve a high score in a more strict metric. A case study is shown in Fig.~\ref{fig:hack} in Appendix.

\paragraph{Generalization.} Based on statistics from Tab.~\ref{tab:baseline_stat}, the main experiment demonstrates that \textsc{LongFaith} uses instructions with shorter context as input compared to baseline methods, reducing training costs while generalizing to LongBench tasks that require processing an average of 24K-70K tokens as input. This further highlights the generalization ability of our pipeline.

\begin{table*}[t]
\centering
\small
\tabcolsep 7.0pt
\begin{tabular}{lcccccccccc}
\toprule
\multicolumn{1}{c}{\textsc{\llama}} &
\multicolumn{2}{c}{\textbf{\musique}} &
\multicolumn{2}{c}{\textbf{2WikiMHQA}} &
\multicolumn{2}{c}{\textbf{\hotpot}} &
\multicolumn{2}{c}{\textbf{LongBench(S)}} &
\multicolumn{2}{c}{\textbf{LongBench(M)}} \\
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} \cmidrule(r){10-11}
\quad + \textsc{LongFaith} & EM & F1 & EM & F1 & EM & F1 & EM & F1 & EM & F1 \\
\midrule
\multicolumn{11}{c}{\textit{Superivised Fine-tuning}} \\
\midrule
\quad w/ \textit{\llama} & 35.4 & 48.4 & 59.4 & 69.5 & 54.6 & 67.7 & 9.9 & 29.0 & 42.2 & 48.4 \\
\quad w/ \textit{\qwen} & 40.6 & 54.0 & 55.4 & 65.0 & 53.6 & 69.5 & 14.1 & 38.4 & 43.0 & 53.8 \\
\quad w/ \textit{\llamal} & \textbf{44.8} & \textbf{58.1} & 54.0 & 64.4 & 54.4 & 69.5 & 16.0 & 41.0 & 44.8 & \textbf{56.7} \\
\quad w/ \textit{\gpt} & 41.6 & 56.8 & \textbf{64.6} & \textbf{73.8} & \textbf{55.4} & \textbf{70.5} & \textbf{16.9} & \textbf{42.0} & \textbf{47.2} & 55.7 \\
\midrule
\multicolumn{11}{c}{\textit{Preference Optimization}} \\
\midrule
\quad w/ \textit{\llama} & 41.2 & 53.2 & 57.4 & 67.0 & 55.6 & 68.5 & 14.7 & 36.9 & 44.0 & 55.3 \\
\quad w/ \textit{\qwen} & 46.6 & 59.2 & 59.0 & 67.4 & \textbf{58.6} & \textbf{72.8} & 15.4 & 35.3 & 44.8 & 55.6 \\
\quad w/ \textit{\llamal} & \textbf{50.4} & \textbf{63.2} & 52.8 & 62.7 & 57.2 & 71.0 & \textbf{16.4} & 40.0 & \textbf{48.3} & 59.4 \\
\quad w/ \textit{\gpt} & 48.4 & 60.5 & \textbf{59.8} & \textbf{68.0} & 49.8 & 65.4 & 15.9 & \textbf{42.4} & 45.0 & \textbf{59.8} \\
\midrule
\end{tabular}
\caption{Ablation study on various LLMs for synthesizing \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO in the size of 2K. The base model for training and testing is \textit{\llama}. \textbf{(S)} and \textbf{(M)} refer to Single-doc QA and Multi-docs QA in LongBench.}
\label{tab:ab_models}
\end{table*}

\begin{figure}[t]
    \centering
    \centerline{\includegraphics[width=\linewidth]{figures/longfaith_radar.png}}
    \caption{Visualization of F1 scores in Tab.~\ref{tab:longbench_exp}.}
    \label{fig:radar}
\end{figure}

\subsection{Analysis}

\paragraph{Exploration on Different Perspective of Faithfulness.} To validate the specific impact of different dimensions of faithfulness, we fine-tune models using negative samples as output and optimize using preference datasets that reject only a subset of negative samples. The statistics of the constructed datasets are shown in Tab.~\ref{tab:synset_stat}. Since each task in LongBench contains no more than 200 questions, performance evaluations can be prone to errors, so we chose to test on multi-hop reasoning datasets. Experimental results are shown in~\ref{tab:multihop_exp}. The models trained with \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO achieved high performance respectively in SFT and PO especially in F1 scores, as expected. 

However, we note that in 4-Hop part of 2Wiki and comparison part of \text{\hotpot}, \textsc{LongFaith}-SFT w/o CoC and w/o GT demonstrated better performance. Analysis reveals that for the question "Do both films, Cuban Colony and Prathyartha, have directors from the same country?", as the training set \text{\musique} used specific entities as answers, the model responds "Both directors are from the same country, which is India. The answer is India.". Actually, the correct answer is "yes." Model trained on \textsc{LongFaith} w/o GT and \textsc{LongFaith} w/o Doc performed better with more exploration, but also lost overall performance due to hallucinations. Models trained on all PO datasets outperformed those trained using only positive samples for SFT, demonstrating the performance improvement brought by each fine-grained, credible preference. Finally, models trained on \textsc{LongFaith}-PO, which integrates three dimensions of faithfulness, achieved the best overall performance.

\paragraph{Scalability and Performance Gains.} We explore the scaling-up potential of \textsc{LongFaith} on multi-hop reasoning test sets and LongBench. As presented in Tab.~\ref{tab:dataset_stat}, we train \textit{\llama} using \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO synthesized by \textit{\qwen} across four dataset sizes, ranging from \textit{1K} to \textit{8K}. According to the performance trend in Fig.~\ref{fig:scale}, \textsc{LongFaith} generally exhibits scaling-up potential, indicating that expanding the training dataset can further enhance performance. Moreover, \textsc{LongFaith}-PO, which incorporates fine-grained preference optimization, demonstrates a more stable upward trend compared to \textsc{LongFaith}-SFT, particularly in LongBench tasks. This result validates the robustness of the \textsc{LongFaith} pipeline.

\paragraph{Attribution-Based Reasoning Leads to Higher Performance.} Utilizing CoC prompting for reasoning with attributions not only outperforms CoT in performance, as it presents in Tab.~\ref{tab:multihop_exp}, but also provides greater interpretability and faithfulness as shown in Fig.~\ref{fig:main}. We use Attribution F1 as a metric to quantify the model's attribution capability using annotated supporting facts. Under CoC prompting, we analyze the references within reasoning chains, matching them against supporting facts like [1], [2], etc., and compute F1 scores based on recall and precision. We evaluate the attribution capability and overall performance of \llama trained on \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO across four sizes and visualize the results in a scatter plot. The findings in~\ref{fig:point} demonstrate a strong positive correlation between attribution capability and model performance, further validating the effectiveness of the \textsc{LongFaith} pipeline.

\paragraph{Impact of LLM Selection for Synthesis.} We experimented with different LLMs for synthesis, including smaller open-source LLMs such as \textit{\llama}, \textit{\qwen}, and larger open-source models like \textit{\llamal}, as well as a closed-source API model, \textit{\gpt}, to synthesize \textsc{LongFaith}-SFT and \textsc{LongFaith}-PO for training \llama. The performance test results are presented in Tab.\ref{tab:ab_models}. Using stronger closed-source API models to synthesize \textsc{LongFaith}-SFT led to a stronger performance boost, which aligns with intuition and previous work~\cite{longmit}. However, an interesting finding is that the \textsc{LongFaith}-PO synthesized with different base LLMs did not show significant performance differences in preference optimization. Even smaller model like \textit{\qwen}, are able to synthesize high-quality reasoning chains, with performance on some datasets matching or even surpassing \textit{\gpt}. This highlights the robustness of the \textsc{LongFaith} pipeline, which is capable of modeling fine-grained preferences to synthesize high-quality instructions.