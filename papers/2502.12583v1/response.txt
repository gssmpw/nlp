\section{Related Work}
\label{sec:related}

\paragraph{Long-Context Utilization.} Amounts of studies focus on enhancing LLMs to better utilize long-context information. Model-centric approaches, for instance, optimizations on attention mechanism aim to capture specific sequential features**Brown et al., "Rethinking Attention with Performers"**, while positional interpolation techniques are utilized to scale positional encoding while ensuring valid index ranges**Shaw et al., "Self-Attention with Adaptive External Memory"**. In addition, data-driven approaches also gain popularity, emphasizing high-quality data synthesis for fine-tuning to improve LLMs' long-context processing capabilities. For example,**Chen et al., "Long-Sequence Continuous Pre-Training"** employ long-sequence continuous pre-training on foundation models, while**Huang et al., "Pre-Trained Models and Transfer Learning"** explores the impact of pre-training data composition and balance. Additionally, works on SFT with synthetic instructions**Zellers et al., "SWAG: A Large-Scale Adversarial Dataset for Natural Language Inference"** not only consider long-context understanding but also strengthen multi-hop reasoning capabilities. Lastly, preference optimization approaches**Wang et al., "Preference Optimization via Fine-Grained Pairwise Instruction Sets"** generate fine-grained pairwise preference instruction sets and incorporate training techniques**Liu et al., "Training Techniques for Long-Term Dependencies"**. From the perspective of improving the faithfulness of synthetic data, our work effectively addresses the shortcomings of prior studies in this area.

\paragraph{Faithful Reasoning.} Hallucination in LLMs presents a major challenge in knowledge-intensive tasks**Hochreiter et al., "Long Short-Term Memory"**. Recent work has focused on enhancing faithful reasoning, where the goal is to trace the LLM's generated content back to reliable sources and ensure its factual grounding. **Zhang et al., "Reasoning Outputs that Link Claims to Specific Sources"** aim to improve the identification and verification of attributions by focusing on generating reasoning outputs that link claims to specific sources. Benchmarks such as **Cohen et al., "Evaluating the Quality of Citations"** evaluate the quality of citations and highlight the limitations of current systems in providing citation support to ensure more reliable output. Additionally, integrating external knowledge sources has gained attention, which use retrieval-augmented generation (RAG) methods to facilitate deep and faithful reasoning**Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"**. Our \textsc{LongFaith} is motivated by previous work, towards faithful reasoning in long-context reasoning tasks.