[
  {
    "index": 0,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhao2023slic",
        "author": "Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J",
        "title": "Slic-hf: Sequence likelihood calibration with human feedback"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "IPO",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "tang2024generalized",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zeng2024token",
        "author": "Zeng, Yongcheng and Liu, Guoqing and Ma, Weiyu and Yang, Ning and Zhang, Haifeng and Wang, Jun",
        "title": "Token-level Direct Preference Optimization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2024tis",
        "author": "Liu, Aiwei and Bai, Haoping and Lu, Zhiyun and Sun, Yanchao and Kong, Xiang and Wang, Simon and Shan, Jiulong and Jose, Albin Madappally and Liu, Xiaojiang and Wen, Lijie and others",
        "title": "TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "rto",
        "author": "Zhong, Han and Feng, Guhao and Xiong, Wei and Cheng, Xinle and Zhao, Li and He, Di and Bian, Jiang and Wang, Liwei",
        "title": "Dpo meets ppo: Reinforced token optimization for rlhf"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "oreo",
        "author": "Wang, Huaijie and Hao, Shibo and Dong, Hanze and Zhang, Shenao and Bao, Yilin and Yang, Ziran and Wu, Yi",
        "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "Step-dpo",
        "author": "Lai, Xin and Tian, Zhuotao and Chen, Yukang and Yang, Senqiao and Peng, Xiangru and Jia, Jiaya",
        "title": "Step-dpo: Step-wise preference optimization for long-chain reasoning of llms"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "stepcontroldpo",
        "author": "Lu, Zimu and Zhou, Aojun and Wang, Ke and Ren, Houxing and Shi, Weikang and Pan, Junting and Zhan, Mingjie and Li, Hongsheng",
        "title": "Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "svpo",
        "author": "Chen, Guoxin and Liao, Minpeng and Li, Chengxi and Fan, Kai",
        "title": "Step-level Value Preference Optimization for Mathematical Reasoning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "stepkto",
        "author": "Lin, Yen-Ting and Jin, Di and Xu, Tengyu and Wu, Tianhao and Sukhbaatar, Sainbayar and Zhu, Chen and He, Yun and Chen, Yun-Nung and Weston, Jason and Tian, Yuandong and others",
        "title": "Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "oreo",
        "author": "Wang, Huaijie and Hao, Shibo and Dong, Hanze and Zhang, Shenao and Bao, Yilin and Yang, Ziran and Wu, Yi",
        "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "probabilistic",
        "author": "Abdolmaleki, Abbas and Piot, Bilal and Shahriari, Bobak and Springenberg, Jost Tobias and Hertweck, Tim and Joshi, Rishabh and Oh, Junhyuk and Bloesch, Michael and Lampe, Thomas and Heess, Nicolas and others",
        "title": "Preference Optimization as Probabilistic Inference"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "dpg",
        "author": "Parshakova, Tetiana and Andreoli, Jean-Marc and Dymetman, Marc",
        "title": "Distributional reinforcement learning for energy-based sequential models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "gdc",
        "author": "Khalifa, Muhammad and Elsahar, Hady and Dymetman, Marc",
        "title": "A distributional approach to controlled text generation"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "gdc++",
        "author": "Korbak, Tomasz and Elsahar, Hady and Kruszewski, Germ{\\'a}n and Dymetman, Marc",
        "title": "On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "pandey2024brain",
        "author": "Pandey, Gaurav and Nandwani, Yatin and Naseem, Tahira and Mishra, Mayank and Xu, Guangxuan and Raghu, Dinesh and Joshi, Sachindra and Munawar, Asim and Astudillo, Ram{\\'o}n Fernandez",
        "title": "BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback"
      }
    ]
  }
]