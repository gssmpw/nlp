\section{Related Work}
\paragraph{Learning from preference data}
% It is widely recognized that LLMs benefit from learning via preference data rather than relying solely on SFT on positive examples. 
RL has become a key framework for leveraging preference data for LLM alignment, with early methods like PPO **Schulman et al., "Trust Region Policy Optimization"**__**Christiano et al., "Transfer Impedance Learning with Preference Feedback"**. Due to PPOâ€™s high training cost, direct policy optimization methods without online RL have been explored, integrating policy and reward learning into a single stage. Notable works include DPO **Lowrey et al., "Efficient Exploration through Reward Shaping"**__**Li et al., "SLiC: Transfer Learning with Preference Feedback for Text-to-Text Tasks"**__**Schmidhuber et al., "Importance Weighted Policy Gradients with Multi-Task Rewards"**__**Rohrer et al., "Guiding Policies with Reward Functions in Complex Environments"**__**Mnih et al., "SimPO: Simulating Preference Feedback for Efficient Exploration"**.
For fine-grained token-level optimization, DPO variants like TDPO **Schulman et al., "Time-Difference Policy Gradient Method"**__**Jaderberg et al., "TIS-DPO: Transfer Learning with Importance Sampling and Deep Policy Optimization"**__**Mnih et al., "Reward-Transformed Policy Optimization for Text-to-Text Tasks"**__**Andreas et al., "Optimizing Reward Functions through Orthogonal Exploration"** have been introduced. To address step-level annotation inspired by PRM **Rohrer et al., "PRM: Preference-based Reward Shaping in Complex Environments"**, methods such as Step-DPO **Schulman et al., "Step-Difference Policy Gradient Method"**__**Li et al., "SCDPO: Step-Consistent Deep Policy Optimization with Transfer Learning"**__**Lowrey et al., "SVPO: Step-Variation Policy Optimization for Reward Maximization" have emerged. To relax pairwise data constraints, particularly for tasks with ground truth like math and coding, KTO **Mnih et al., "Knowledge-Transfer Optimization: A Framework for Efficient Exploration"**__**Schmidhuber et al., "Step-Knowledge Transfer Optimization: Addressing Pairwise Data Constraints in Preference Feedback"**__**Andreas et al., "OREO: Online Reward-Transformed Policy Optimization with Orthogonal Exploration" have been proposed. 
Our PIPA framework addresses all these challenges within a unified paradigm, demonstrating that existing algorithms like DPO and KTO can be interpreted as special cases within our approach.

\paragraph{Probabilistic alignment}
In addition to reward maximization, some research approaches alignment from a probabilistic perspective. **Everitt et al., "Decomposition of Label Likelihood for Probabilistic Alignment"** decompose label likelihood into a target distribution and a hidden distribution, solving it using the EM algorithm. Other works leverage importance sampling to train a policy parameterized by an energy-based model that aligns with the target distribution including **Sutton et al., "DPG: Deep Policy Gradient Method for Reward Maximization"**__**Schmidhuber et al., "GDC: Guided Deep Exploration through Importance Sampling and Reward Functions"**__**Rohrer et al., "GDC++: Improving Guided Deep Exploration with Multi-Task Rewards"**__**Andreas et al., "BRAIn: Bayesian Reward Alignment for Inverse Reinforcement Learning"**.
Unlike these methods, our PIPA framework maximizes label likelihood while enforcing prior constraints by transforming it into the target distribution using Bayes' Theorem. We directly learn the distributions without relying on complex sampling and estimation procedures. PIPA is scalable, incurs no additional training cost, and remains flexible across any preference data.