\documentclass[twocolumn]{article}
\setlength{\columnsep}{22pt} 
\usepackage{geometry}

\geometry{
    a4paper,      
    left=2cm,     
    right=2cm,    
    top=2cm,      
    bottom=2cm    
}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{bbm}
\usepackage{natbib}
\usepackage{algorithmic,algorithm}
\usepackage{tcolorbox}
\usepackage{authblk}




\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\prob}{p}
\newcommand{\data}{\mathtt{data}}
\newcommand{\prior}{\mathtt{prior}}
\newcommand{\KL}{\mathtt{KL}}

\usepackage{wrapfig}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{multirow}
\usepackage{tabularx}

\usepackage{hyperref}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% \usepackage[textsize=tiny]{todonotes}



\usepackage{titlesec}  % 调整标题格式
\usepackage{ulem}      % 用于标题下划线
\usepackage{graphicx}  % 处理图片和线条
\usepackage{multicol}  % 处理多栏
\usepackage{authblk}   % 让作者信息更易格式化

\title{
    \noindent\rule{\linewidth}{4pt} \\  % 上横线
    \vspace{0.5em}
    \raisebox{-0.5em}{\includegraphics[width=0.06\textwidth]{figs/pipa5.png}}  
    {\LARGE \textbf{PIPA: Preference Alignment as Prior-Informed Statistical Estimation}}  \\  % 加大标题
    \vspace{0.2em}
    \noindent\rule{\linewidth}{1pt}  % 下横线
}

% \title{%
%     \noindent\rule{\linewidth}{4pt} \\  % Top horizontal rule
%     %\vspace{0.5em}
%     \begin{tabular}{@{} l l @{}}
%       \multirow{2}{*}{\raisebox{1.5em}{\hspace{-5em}\includegraphics[width=0.1\textwidth]{figs/pipa5.png}}} & {\LARGE \textbf{PIPA: Preference Alignment as}} \\
%       & {\LARGE \textbf{Prior-Informed Statistical Estimation}} \\
%     \end{tabular}
%     %\vspace{0.2em} \\
%     \noindent\rule{\linewidth}{1pt}  % Bottom horizontal rule
% }

\author[$\dag$]{\large \textbf{Junbo Li}}
\author[$\dag$]{\large \textbf{Zhangyang Wang}}
\author[$\dag$]{\large \textbf{Qiang Liu}}
\affil[$\dag$]{\large \textnormal{The University of Texas at Austin}}  
\affil[ ]{\large \texttt{\{jbli, atlaswang, lqiang\}@utexas.edu}}


\renewcommand\Authsep{\quad}  
\renewcommand\Authands{\quad}  

\date{} 

% \author[†]{\large \textbf{Junbo Li}}
% \author[†]{\large \textbf{Zhangyang Wang}}
% \author[†]{\large \textbf{Qiang Liu}}
% \affil[†]{\large \textnormal{The University of Texas at Austin} \\ \texttt{jbli@utexas.edu}, \texttt{atlaswang@utexas.edu}, \texttt{lqiang@utexas.edu}}

% \renewcommand\Authsep{\quad}  
% \renewcommand\Authands{\quad}  

% \date{}





\begin{document}

\maketitle

\begin{abstract}
    Offline preference alignment for language models such as Direct Preference Optimization (DPO) is favored for its effectiveness and simplicity, eliminating the need for costly reinforcement learning. Various offline algorithms have been developed for different data settings, yet they lack a unified understanding. 
    In this study, we introduce Pior-Informed Preference Alignment (PIPA), a unified, RL-free probabilistic framework that formulates language model preference alignment as a Maximum Likelihood Estimation (MLE) problem with prior constraints. This method effectively accommodates both paired and unpaired data, as well as answer and step-level annotations. We illustrate that DPO and KTO are special cases with different prior constraints within our framework. By integrating different types of prior information, we developed two variations of PIPA: PIPA-M and PIPA-N. Both algorithms demonstrate a $3\sim10\%$ performance enhancement on the GSM8K and MATH benchmarks across all configurations, achieving these gains without additional training or computational costs compared to existing algorithms.
\end{abstract}



\section{Introduction}
Pre-training large language models (LLMs) from scratch on trillions of text tokens allows for accurate prediction next tokens in natural language \citep{achiam2023gpt,dubey2024llama,liu2024deepseek}. Following this, alignment, achieved through fine-tuning on smaller, high-quality datasets designed for specific tasks, becomes critical for enabling the model to develop specialized skills, such as engaging in conversation \citep{ouyang2022training}, math reasoning \citep{shao2024deepseekmath,yang2024qwen2}, coding \citep{deepseekcoder-v2}, web agent \citep{qin2025ui}, and more. 
The fundamental approach to alignment involves supervised fine-tuning (SFT) on the target domain, which essentially maximizes the likelihood of predicting the next token. 
% The target data can be sourced through human annotation, outputs from stronger models, or self-generation via rejection sampling \citep{guan2025rstar, dong2023raft}.
However, numerous empirical studies have shown that simple SFT on preferred samples is inadequate for attaining optimal performance \citep{shao2024deepseekmath, ouyang2022training}.

Moving beyond basic imitation learning in SFT, it is suggested to learn from both positive and negative samples. Sample quality can be measured by training reward models to capture general preferences \citep{dong2024rlhf} or leveraging accurate rule-based rewards \citep{guo2025deepseek} for specific tasks like math and coding. By treating the autoregressive generation of LLMs as a Markov decision process (MDP), traditional reinforcement learning (RL) algorithms can be effectively applied, such as PPO \citep{ouyang2022training}, SAC \citep{sac}, REINFORCE \citep{ahmadian2024back}, etc. 

While online RL-based methods deliver strong performance, they face challenges such as high training costs, instability, and the need for a strong base model as the initial policy. As a result, offline algorithms like direct preference optimization (DPO) \citep{rafailov2024direct} are often preferred, thanks to their effectiveness and simplicity, particularly when high-quality datasets are accessible. 
The original DPO algorithm has several limitations. It relies on paired preference data, which is not essential for tasks with ground truth such as math and coding. Additionally, it is unable to accommodate step-level annotations. Furthermore, it treats all tokens equally, lacking token-level credit assignment.
To address these issues, a series of approaches have been developed, such as Kahneman-Tversky Optimization (KTO) \citep{ethayarajh2024kto} for unpaired data, Step-DPO \citep{Step-dpo,stepcontroldpo} and Step-KTO \citep{stepkto} for step-level annotations, and RTO \citep{rto}, TDPO \citep{zeng2024token}, and OREO \citep{oreo} for fine-grained token-level DPO. However, these methods are designed from specific perspectives, each addressing only particular challenges, and they lack a unified understanding to integrate their solutions.

In this work, we introduce a unified framework designed to address all the aforementioned challenges in offline approaches. 
Rather than framing the alignment problem within offline RL, we reformulate it as a maximum likelihood estimation (MLE) problem with prior constraints, operating within a purely probabilistic framework called Prior-Informed Preference Alignment (PIPA). 
%
From a statistical estimation perspective, we analyze the suboptimality of supervised fine-tuning (SFT). We demonstrate that both the original DPO and KTO algorithms can be interpreted as special cases within our framework, differing in the prior information they incorporate and the loss used. Building on the PIPA framework, we propose two variants,  PIPA-M and PIPA-N,  that incorporate prior information in different fashions. 
%serve as direct enhancements to DPO and KTO. 
The probabilistic formulation naturally accommodates unpaired data and extends to step-level annotations. 
Our PIPA functions as a versatile plug-in loss design that seamlessly integrates with any (iterative) data generation pipeline in existing alignment framework. 
Furthermore, we show that PIPA training effectively learns token-level credit assignment, yielding precise per-token value estimations that may enable search during test-time inference.

Our contributions can be summarized as follows:  
%\textbf{Contribution}  

$\bullet $ We formulate preference alignment as a prior-informed conditional probability estimation problem that is RL-free and provides clear theoretical insight. 

$\bullet $ Our approach does not need paired preference data, and seamlessly unifies both answer-wise and step-wise settings under a single, theoretically grounded framework.

 $\bullet$ Compared to existing approaches such as DPO \citep{rafailov2024direct} and KTO 
    \citep{ethayarajh2024kto}, our algorithm achieves improved performance without additional computational overhead.
% \end{itemize}

\subsection{Related Work}
\paragraph{Learning from preference data}
% It is widely recognized that LLMs benefit from learning via preference data rather than relying solely on SFT on positive examples. 
RL has become a key framework for leveraging preference data for LLM alignment, with early methods like PPO \citep{schulman2017proximal}, which first trains a reward model on pairwise human feedback \citep{ouyang2022training}. Due to PPO’s high training cost, direct policy optimization methods without online RL have been explored, integrating policy and reward learning into a single stage. Notable works include DPO \citep{rafailov2024direct}, SLiC \citep{zhao2023slic}, IPO \citep{IPO}, GPO \citep{tang2024generalized}, and SimPO \citep{meng2024simpo}.
For fine-grained token-level optimization, DPO variants like TDPO \citep{zeng2024token}, TIS-DPO \citep{liu2024tis}, RTO \citep{rto}, and OREO \citep{oreo} have been introduced. To address step-level annotation inspired by PRM \citep{lightman2023let}, methods such as Step-DPO \citep{Step-dpo}, SCDPO \citep{stepcontroldpo}, and SVPO \citep{svpo} have emerged. To relax pairwise data constraints, particularly for tasks with ground truth like math and coding, KTO \citep{ethayarajh2024kto}, Step-KTO  \citep{stepkto}, and OREO \citep{oreo} have been proposed. 
Our PIPA framework addresses all these challenges within a unified paradigm, demonstrating that existing algorithms like DPO and KTO can be interpreted as special cases within our approach.

\paragraph{Probabilistic alignment}
In addition to reward maximization, some research approaches alignment from a probabilistic perspective. \citep{probabilistic} decompose label likelihood into a target distribution and a hidden distribution, solving it using the EM algorithm. Other works leverage importance sampling to train a policy parameterized by an energy-based model that aligns with the target distribution including DPG \citep{dpg}, GDC \citep{gdc}, GDC++ \citep{gdc++}, BRAIn \citep{pandey2024brain}.
Unlike these methods, our PIPA framework maximizes label likelihood while enforcing prior constraints by transforming it into the target distribution using Bayes' Theorem. We directly learn the distributions without relying on complex sampling and estimation procedures. PIPA is scalable, incurs no additional training cost, and remains flexible across any preference data.

\section{
PIPA 
\includegraphics[width=.03\textwidth]{figs/pipa5.png} 
% \includegraphics[width=.02\textwidth]{figs/music.png} 
} 
We introduce the prior-informed preference alignment (PIPA) framework in Section \ref{sec:pria}, followed by the first version, PIPA-M, in Section \ref{sec:priam}. Next, we explore its connection to DPO \citep{rafailov2024direct} and KTO \citep{ethayarajh2024kto} in Section \ref{sec:dpo}. Drawing inspiration from DPO and KTO, we develop the second version, PIPA-N, detailed in Section \ref{sec:prian}. In Section \ref{sec:step}, we extend PIPA-M and PIPA-N naturally to incorporate step-level annotations. Finally, Section \ref{sec:practical} presents the refined algorithms for PIPA-M and PIPA-N and compares them with prior methods like KTO.

\subsection{Prior-Informed Preference Alignment}
\label{sec:pria}
\paragraph{Problem}
We  define the preference alignment problem as probabilistic estimation. 
Assume that we are given a preference dataset 
$$\{x^i, y^i, c^i\}_{i=1}^N \sim \prob^{\data},
$$  
where $x^i$ is the instruction input, $y^i$ is the answer, and $c^i\in\{0,1\}$ represents the preference or correctness of the answer. We are interested in predicting $y$ given $x$ in the correct case $(c=1)$. This amounts to estimating conditional probability:
$$%\prob_1 (y\mid x ) \overset{\mathtt{def}}{=}
\prob (y\mid x,c=1).$$
The canonical approach 
for estimating $\prob(y\mid x,c=1)$ is, of course, 
the maximum likelihood estimation (MLE), 
which yields supervised finetuning (SFT) 
on the positive examples with $c = 1$. 

However, SFT only uses the positive samples, rendering the negative samples with $c=0$ unusable. 
Preference alignment methods, on the other hand, aims to use both positive and negative data to get better estimation.  
But how is this possible while adhering to statistical principles, given that MLE is statistically optimal and $\prob(y \mid x, c=1)$, by definition, involves only the positive data ($c=1$)?

The idea is that the estimation should incorporate important prior information involving the negative data, thereby introducing a ``coupling'' between the estimations of the positive and negative probabilities, $\prob(y \mid x, c=1)$ and $\prob(y \mid x, c=0)$. Generally, this prior-informed estimation can be formulated as a constrained optimization problem that minimizes a loss on data fitness subject to a prior information constraint:   
\begin{align}\label{equ:priorframe} 
\textcolor{blue}{
\min_p  \mathtt{DataLoss}(p, p^{\data}) ~~~~s.t.~~~ p \in \mathtt{PriorInfo}.} 
\end{align}
%
%By assuming different prior information and data loss functions, we can transparently derive various algorithms, clearly revealing the underlying priors and preferences
By assuming different prior information and data loss functions, we can derive various algorithms in a principled manner, with a transparent understanding of the underlying priors and preferences.


\begin{algorithm*}[h!]
    \caption{PIPA: Prior-Informed Preference Alignment}
    \label{alg:PIPA}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} A dataset \(\{(x,y,c)\}\) of questions $x$, answers $y$, and preference $c$, with \(c \in \{0,1\}^T\); 
        a fixed prior model \(\prob^\prior\), and trainable models \(f^{\theta}_{}\) and \(g^{\theta}\)
        initialized from an SFT model \(f_0\). Choice: either PIPA-M or PIPA-N.

        \FOR{each batch}
        \begin{minipage}[t]{0.44\linewidth}
            \STATE \textbf{PIPA-M:}
             Compute 
            \[
               F^{\theta}(x,y_{\le t}) \;:=\; 
               \frac{f^{\theta}_{}\bigl(y_t \mid x, y_{<t}\bigr)\,g^{\theta}(x,y_{<t})}
                    {p^{\prior}\bigl(y_t \mid x, y_{<t}\bigr)}.
            \]
        \end{minipage}%
        \hspace{0.8em}
        \vrule width 1pt 
        \hspace{0.8em}
        \begin{minipage}[t]{0.44\linewidth}
            \textbf{PIPA-N:} 
            % [0.5em]
            Compute 
            \[
               F^{\theta}(x,y_{\le t}) \;:=\; 
               \tau\!\Biggl(\!
                   \frac{f^{\theta}_{}\bigl(y_t \mid x, y_{<t}\bigr)\,\;g^{\theta}(x,y_{<t})}
                        {p^{\prior}\bigl(y_t \mid x, y_{<t}\bigr)\,\bigl(1 - g^{\theta}(x,y_{<t})\bigr)}
               \!\Biggr),
            \]
            where \(\tau(x) := \frac{x}{x + 1}\).
        \end{minipage}

        \STATE Minimize the loss:
        \begin{align}
            L(x,y,c) \;=\; 
              -\sum_{t : c_t=1}\!\log\,F^{\theta}(x,y_{\le t})
              \;-\; \sum_{t : c_t=0}\!\log\bigl(1 - F^{\theta}(x,y_{\le t})\bigr).
            \nonumber
        \end{align}
        \ENDFOR
    \end{algorithmic}
\end{algorithm*}



\subsection{PIPA-M: Enforcing Prior on Marginal Distribution}
\label{sec:priam}
We first consider a  straightforward case 
when we know that the marginal prediction $\prob(y\mid x)$ should match a prior distribution $\prob^{\prior}(y\mid x)$ defined by the pretrained LLM.
%Note that 
Because the marginal distribution is a sum of the positive and negative probabilities, that is, 
\begin{align*}
p(y\mid x) = p(y\mid  x,c=1)p(1| x) 
 + p(y\mid x,c=0)p(0 |  x),
\end{align*}
where we abbreviate \( p(c=i | x) \) as \( p(i | x) \). The estimation of the positive and negative probabilities are  coupled. 
%This allows us to 

In this case, the estimation problem is in principle formulated as a constrained maximum likelihood problem:
\begin{align}
    &\min_p \quad \mathbb{E}[\KL(\prob^{\data}(y,c\mid x)\mid \mid \prob(y,c\mid x))]. \label{eq:kl}\\
    & \textit{s.t.} \quad \quad \prob(y\mid x) = \prob^{\prior}(y\mid x),~~~\forall x, y. \nonumber
\end{align}
This is by definition is the statistically the best way to estimate $\prob$ provided the data information $\prob^{\data}$ and the prior constraint $\prob^{\prior}$.  


As is the typical case, 
the KL divergence minimization reduces to maximizing the log-likelihood objective $\log p(y,c\mid x)$ in practice, which is equivalent to maximizing $\log p(c\mid y,x)$ given that $p(y|x)$ is fixed by the prior constraint. 

The main difficulty is to handle the constraint. Recall that our final goal is to estimate $\prob(y\mid x,c=1)$, which is expected to be parameterized with autoregressive transformer model. Hence, we are going to parameter the joint distribution
$p$  using $\prob(y\mid x,c=1)$ and $p(y~|~x)$.  

\begin{theorem}
%For any given 
Assume 
\begin{align*} 
p(y\mid x, c=1) = f^\theta_{}(y\mid x),  && 
p(c=1 \mid x) = g^\theta(x),
\end{align*}
where  the parametric model $f_{}^\theta$ returns a conditional probability of $y$ given $x$,   
and $g^\theta\in[0,1]$ outputs a probability.

Then, we can construct a joint distribution $p^\theta(y,c~|~x)$ 
as follows 
that satisfies $\prob^\theta(y\mid x) = \prob^{\prior}(y\mid x)$: 
\begin{align}
\label{equ:paramm}
\begin{split}
    &\prob^\theta(y,c=1\mid x) =f_{}^\theta(y\mid x) g^\theta(x),\\ 
    %\prob^\theta(y\mid x,c=1)\prob(c=1\mid x), \nonumber\\
    &\prob^\theta(y,c=0\mid x) = 
    \prob^{\prior} (y\mid x) - f_{}^\theta(y\mid x)g^\theta(x), 
    %\prob^{\prior} (y\mid x) - \prob^\theta(y,c=1\mid x),
\end{split}    
\end{align}
where we should assume the constraint that  
$$\prob^{\prior}(y \mid x) \geq f_{}^\theta(y\mid x) g^\theta(x).$$  
%\prob(y\mid x,c=1)\prob(c=1\mid x).$$ 
%as a result
The constraint can be ensured by defining
%This can be ensured by defining 
$$g^\theta(x) = \min(g_0^\theta(x), \prob^{\prior}(y \mid x)/f_{}^\theta(y\mid x)),$$
where $g_0^\theta$ is an unconstrained network. 
\end{theorem}




Therefore, the optimization reduces to minimizing:  
\begin{align*}
\mathbb E_{\prob^{\data}} [\log\prob^{\theta}(y,c~|~x)],
\end{align*}
with the parameterization in \eqref{equ:paramm}. 
See Algorithm~\ref{alg:PIPA} for more details. 





\subsection{DPO and KTO: Prior-Informed Views}
We analyze existing methods, such as DPO and KTO, through the lens of the prior-informed estimation framework \eqref{equ:priorframe}. Our analysis demonstrates that both DPO and KTO  can be interpreted as enforcing prior assumptions on the negative probability, $p(y \mid x, c = 0)$, rather than the marginal probability, $p(y \mid x)$. However, these methods differ in their choice of loss functions, the prior assumptions made about $p(c \mid x)$, and the parameterization employed.
% We show the equivalence in Theorem \ref{thm:dpo} and \ref{thm:kto}, and show the full proof in Appendix \ref{sec:proof}.

\label{sec:dpo}
\paragraph{DPO}
%Direct p
Direct preference optimization (DPO) and related methods are often cast as estimating the log density ratio 
$\log(p(y~|~x, c=1)/p(y~|~x, c=0))$ of the positive and negative generation probabilities \citep{dumoulin2023density}. 
Related, it may not be of suprise that 
these models make the implicit assumption on the negative (reference) probability $p(y~|~x, c=0) = p^{\prior}(y~|~x)$. %The PIPA form of DPO is  
In particular, DPO can be formulated as 
    \begin{align}
        \label{eq:mle dpo}
        & \max_{\theta} 
         L_{\mathtt{DPO}} (p^\theta, ~~p^{\data}) \\ 
        %\quad \mathbb{E}_{\prob^{\data}} \log \prob^\theta(\eta=1 \mid x, \{Y_x^+, Y_x^-\}=\{y^+, y^-\}, Y=y^+) \\
        s.t. ~~& \prob^\theta(y\mid x, c=0)=\prob^{\prior}(y\mid x), \nonumber\\
        & \prob^\theta(c=1\mid x)=\prob^\theta(c=0\mid x)=\frac{1}{2}, \quad \forall x, y. 
        % & \qquad \qquad \prob^\theta(c = 1\mid x) = \prob^{\theta}( c= 0\mid x)=\frac{1}{2}. 
        \nonumber
    \end{align}
where the loss $L_{\mathtt{DPO}}$ is 
a special pairwise comparison loss 
related to Bradley–Terry model
provided  paired data $\{x, y^{+}, y^-\}$, 
of both positive answer $y^{+} \sim \prob(y\mid x,c=1)$ and  negative answer $y^-\sim \prob(y\mid x,c=0)$ for each $x$; the assumption 
of $\prob^\theta(c=1\mid x)=0.5$ 
is due to the balanced sampling of positive and negative weights. See Appendix \ref{sec:connect dpo} for more discussion for $L_{\mathtt{DPO}}$ and proof of equivalence. 


\paragraph{KTO}
%DPO 
One key limitation of DPO is that it requires to use paired data. KTO has been proposed as an approach that relaxes the requirement. 
In the prior-informed framework, it can be viewed as solving: 
%\%It can be viewed 
    \begin{align*}
    \begin{split}
        &\max_\theta  \mathbb{E}_{\prob^{\data}} \left [ \prob^{\theta}(c|x,y) \right] \\
        &\textit{s.t.} ~~~~\prob^{\theta}(y\mid x,c=0)=\prob^{\prior}(y\mid x) \\
        & \qquad \log\frac{\prob^\theta(c=0\mid x)}{\prob^\theta(c=1\mid x)} = z^\theta(x), ~~~~~
        \quad \forall x,y.
        \end{split}
    \end{align*}
where it changes the loss function to 
the standard conditional likelihood without $\log$, which holds for unpaired data\footnote{In the KTO paper, an importance weight is placed on the positive and negative data ($\lambda_D,\lambda_U$ in their notation), but the default settings in the code is balanced weight ($\lambda_D=\lambda_U=1$).}. 
In addition, it makes a particular assumption on the class ratio ${\prob^\theta(c=0\mid x)}/{\prob^\theta(c=1\mid x)}$, which is consistent with the fact that the class percentage is no longer guaranteed 
to be balanced without paired data. 
%since . 
%In addition, since the data 
%In addition, it changes the 
In particular, KTO uses 
$$z^\theta(x) = \KL(\prob^{\theta}(y\mid x, c=1)~||~ \prob^{\prior}(y\mid x)).$$
Here $z^\theta$ depends on parameter $\theta$, but the gradient is stopped through $z^\theta$ 
in the KTO algorithm. In practice, $z^\theta$ is estimated with empirical samples in each batch. Details and proof are shown in Appendix \ref{sec:connect kto}.

\subsection{PIPA-N: Enforcing Prior on Negative Condition Distribution}
\label{sec:prian}

Knowing the prior informed formulation of DPO and KTO, we can propose changes to make them simpler and more natural. One option is to keep the conditional log-likelihood loss function of KTO, but seeks to learn  $p^\theta(c=1|x)$ as a neural network $g^\theta(x)$ from data, 
rather than making the heuristic assumption. This yields 
%Inspired by DPO and KTO in our framework, we propose another version of PIPA with prior on the negative samples same as DPO but for pair-free data, named PIPA-N. Specifically, we solve:
\begin{align}
 \label{eq:mle neg}
\begin{split}
    &\max_\theta \mathbb{E}_{\prob^{\data}} \left [ \log \prob^\theta(c\mid x,y) \right ] \\    
    & \textit{s.t.}  \quad \prob^\theta(y\mid x, c=0) = \prob^{\prior}(y\mid x).
    \quad \forall x, y.
    %\nonumber
    \end{split} 
\end{align}
This differs from  PIPA-M in \eqref{eq:kl}  in  the prior constraint.
We call this PIPA-N, for it places prior on the negative probability. 
Using the same  parameterization of $\prob^\theta(y\mid x,c=1) = f^\theta_{}(y|x)$, and $\prob^\theta(c=1\mid x) = g^\theta(x)$, we have % yields:
\begin{align*}
\begin{split}
    &\prob^\theta(c=1\mid x,y)=
    \tau\left ( 
    \frac{f_{}^\theta(y\mid x)g^\theta(x)}{\prob^{\prior}(y\mid x) (1-g^\theta(x))}
    \right) 
    %\frac{f_{}^\theta(y\mid x)g^\theta(x)}{
    %f_{}^\theta(y\mid x)g^\theta(x) + 
%     \prob_0^{\prior}(y\mid x) (1-g^\theta(x)) 
    , \\
    &\prob^{\theta}(c=0\mid x,y)=1-\prob^{\theta}(c=1\mid x,y),
\end{split}    
\end{align*}
where $\tau(x) = x/(x+1)$. 
This allows us to reduce the problem 
to an unconstrained optimization of maximizing 
$\mathbb{E}_{\prob^{\data}} \left [ \log \prob^\theta(c\mid x,y) \right ]$. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/pria.pdf}
    % \vspace{-0.7cm}
    \caption{The figure illustrates PIPA-M, with PIPA-N following a different loss function as outlined in Algorithm \ref{alg:PIPA}. We denote $f_t^\theta:=f_{}^\theta(y_t\mid x, y_{<t})$ to be the target next-token-prediction probability, $g_t^\theta:=g^\theta(x,y_{<t})$ to be the value model, which differ only in their output heads. And $p_t^{\prior}:=p^{\prior}(y_t\mid x, y_{<t})$ is another frozen language model. When only answer-level annotations are available, the objective will contain only one of the correct and incorrect parts.}
    % \vspace{-0.2cm}
    \label{fig:pria}
\end{figure*}




\subsection{Extension to Step-level Settings}
\label{sec:step}
The introduction above is about the answer-level setting, which uses a single label for an entire solution—even if multiple steps may vary in correctness. In contrast, the step-level setting assigns separate labels to each step.
The advantage of our probabilistic framework is that it seamlessly adapts to a step-level setting for both PIPA-M and PIPA-N algorithms.  The core idea is intuitive. We use token-level labels rather than answer-level labels, followed by decomposing the joint probability autoregressively.


\begin{figure}[H]
    \centering
    % \vspace{-0.1cm}
    \includegraphics[width=1.0\linewidth]{figs/label.pdf}
    % \vspace{-0.3cm}
    \caption{We show a visualization for the label of a negative answer under different circumstances.}
    \label{fig:label}
\end{figure}


\paragraph{Problem formulation}
In the step-level setting, we decompose answer $y$ and label $c$ to tokens. Specifically, for each data $(x, y, c)$ with $k$ steps and $T$ tokens in the answer, we have $y=(y_1, \cdots, y_T)$ and $c=(c_1,\cdots, c_T)\in\{0,1\}^T$, where $c_t$ is $1$ if the corresponding step is correct otherwise 0. 
Notice that if only answer-level annotation is available, we can still define $c_1=\cdots=c_T\in\{0,1\}$. For any $1\leq t\leq T$, denote $y_{\leq t}:=(y_1,\cdots, y_t)$ and the same for $c_{\leq t}$. 
Figure \ref{fig:label} presents a visualization comparing token-level representation with the previous sequence-level representation.



\paragraph{Parameterization}
Same as the answer-level setting, the objective is still (\ref{eq:kl}) for PIPA-M or (\ref{eq:mle neg}) for PIPA-N. 
We factorize $\prob^\theta(c\mid x,y)$ in an autoregressive manner for $c$.
Since $c_t$ can be determined by $(x, y_{\leq t})$, so $c_t$ is \textbf{conditionally independent} of both $y_{>t}$ and ${c_{<t}}$ given $(x, y_{\leq t})$. We have
\begin{align}
    \prob(c\mid x,y)=\prod_t \prob(c_t\mid x, y, c_{<t}) =\prod_t \prob(c_t\mid x,y_{\leq t}). \nonumber
\end{align}
By Bayes' Theorem, for each $t$ we have:
\begin{align}
    \prob(c_t\mid x, y_{\leq t})=\frac{\prob(y_t\mid x, y_{<t}, c_t)\prob(c_t\mid x, y_{<t})}{\prob (y_t\mid x_t, y_{<t})}. \nonumber
\end{align}
Now similar to the answer-level PIPA, we introduce neural networks to parameterize $f^{\theta}_{}(y_t\mid x,y_{<t}):=\prob ^\theta(y_t\mid x, y_{<t}, c_t=1)$ and $g^{\theta}(x,y_{<t}):=\prob^\theta(c_t=1\mid x, y_{<t})$. 


Then we solve the same unconstrained optimization problem as answer-level setting, \textit{i.e.,} maximizing $\mathbb{E}_{p^{\data}}[\log \prob^\theta(c\mid x,y)]$. See Algorithm \ref{alg:PIPA} for details.




\subsection{Practical Implementation}
\label{sec:practical}


As shown in the formulation of Section \ref{sec:step}, we always use PIPA-M and PIPA-N with token-level label representation in practice which provides fine-grained information.
% even if only answer-level annotation is available, we can still consider PIPA with token-level label representation by setting $c_t$ to be the same.
% So we will always use the step-level PIPA-M and PIPA-N in practice.
% , \textit{i.e.}, training $f^{\theta}_{}$ and $g^{\theta}$ by maximizing (\ref{eq:unconstrained step}).

% Denote $p_0$ to be the prior model $\prob^{\prior}$. 
We have three models in total: $f^{\theta}_{}, g^{\theta}, p^{\prior}$. In practice, $f^{\theta}_{}$ is the target language model initialized from the one obtained after Supervised Fine-Tuning (SFT) and will be used for inference after training. $g^{\theta}$ shares exactly the same base model with $f^{\theta}_{}$ differing only in the output head. $\prob^{\prior}$ is also a language model but frozen. 
We show our PIPA-M and PIPA-N in Algorithm \ref{alg:PIPA}, and a figure illustration in Figure \ref{fig:pria}.
For the negative samples in PIPA-M, we apply a clipping function to constrain the term $f_{}^\theta g^\theta/\prob^{\prior}$ within $[0, 1-\varepsilon]$, where $\varepsilon=10^{-10}$.

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figs/pria.pdf}
%     % \vspace{-0.7cm}
%     \caption{The figure illustrates PIPA-M, with PIPA-N following a different loss function as outlined in Algorithm \ref{alg:PIPA}. We denote $f_t^\theta:=f_{}^\theta(y_t\mid x, y_{<t})$ to be the target next-token-prediction probability, $g_t^\theta:=g^\theta(x,y_{<t})$ to be the value model, which differ only in their output heads. And $p_t^{\prior}:=p^{\prior}(y_t\mid x, y_{<t})$ is another frozen language model. When only answer-level annotations are available, the objective will contain only one of the correct and incorrect parts.}
%     % \vspace{-0.2cm}
%     \label{fig:pria}
% \end{figure*}






\paragraph{Comparison with KTO} 
Although it stems from a completely different derivation, KTO \citep{ethayarajh2024kto}—which also targets pair-free alignment—is the closest prior work to PIPA in terms of its algorithm.  A detailed analysis of their differences and PIPA's advantages can be found in Appendix \ref{sec:compare kto}.

\paragraph{Credit assignment}
A key issue with traditional DPO and KTO loss is that all tokens receive equal treatment, 
since only the answer-level reward is considered. PIPA offers a natural way 
to weight 
$f_{}^\theta(y_t \mid x, y_{<t})$
differently by jointly optimizing it with 
$g^\theta(x,y_{<t}) = \prob^\theta\bigl(c_t=1 \mid x, y_{<t}\bigr).$
The optimized \( g^\theta \), with its clear probabilistic interpretation, can be viewed 
as a value function and may be used for inference-time search in future work. We present 
its learning trajectory in Section \ref{sec:algo ablation}.

\paragraph{Compatibility and flexibility}
PIPA can be applied whenever answer- or step-level annotations are available, requiring no additional training stage. These step-level annotations can be derived via MCTS \citep{chen2024alphamath,zhang2024rest,guan2025rstar} or by LLM-as-a-judge \citep{Step-dpo,stepkto}. Furthermore, PIPA easily generalizes to an iterative version, similar to other works \citep{xiong2024iterative,pang2024iterative,oreo}. In this paper, we focus on statistical estimation using a static offline dataset, leaving the online version for future exploration.



\section{Experiments}
\subsection{Settings}
Our PIPA framework is capable of handling scenarios both with and without preference pairs, as well as with or without step-level annotations.
Consequently, we evaluate it across four distinct experimental setups determined by 
\((\text{pair}, \text{unpair}) \times (\text{answer}, \text{step})\). 

\paragraph{Baseline algorithms}
For paired preference data, we use DPO \citep{rafailov2024direct} and its variant IPO \citep{IPO}, and KTO \citep{ethayarajh2024kto} as baselines. Both KTO and PIPA decouple the paired data. For data without preference pairs, we compare PIPA with KTO. In answer-wise settings, we benchmark PIPA against DPO, IPO, and KTO. In step-wise settings, we compare PIPA with Step-DPO \citep{Step-dpo} and Step-KTO \citep{stepkto}. 
% The original Step-DPO is for specific data format, so it's not directly comparable. Therefore, we extract their loss functions to compare. 
The original Step-DPO and Step-KTO methods involve additional data generation phase. For a fair comparison on an offline dataset, we extract only their loss functions.
See Appendix \ref{sec:bsl algo} for detailed descriptions of the baseline algorithms.

\paragraph{Data} 
We use the unpaired preference dataset for math reasoning released by AlphaMath \citep{chen2024alphamath}, which includes training problems from GSM8K \citep{GSMdata} and MATH~\citep{MATHdata} with both CoT \citep{CoT} and TIR \citep{gou2023tora}-style solutions, along with step-level label annotations.
There are more incorrect answers in this dataset, so we construct the paired subset by matching each correct solution to a corresponding incorrect solution from the same problem and discarding the remaining incorrect solutions.
We use the entire original dataset for the unpaired setting.

For the answer-level setting, we only keep the final label for the answer.
In the step-level setting, steps of a correct answer are always correct. For incorrect answer, we label steps whose $Q$ values fall within $[-1, 0.5)$ as incorrect, and those in $[0.5, 1]$ as correct with a threshold $0.5$.
Instead of a threshold $0$, the intuition of this shifted threshold is that it's better to be conservative for the correct steps in the wrong answer.
Despite being correct, some steps may still contribute to an incorrect overall analysis. Therefore, minimizing the likelihood of such steps is also crucial. The efficacy of this choice is further explored in the ablation study of Section \ref{sec:step ablation}.

For our evaluation, we use the standard GSM8K and MATH benchmarks.
We adopt the MARIO evaluation toolkit~\citep{zhang2024mario}, configuring the beam search width and number of generated samples, \textit{i.e.}, ($B_1, B_2$) in their notation, to be $(3,1)$ for GSM8K and $(1,1)$ for MATH.

\paragraph{Model}
The AlphaMath dataset is generated using Deepseek-based models, so we use Deepseek-Math-7B-Instruct~\citep{shao2024deepseekmath} as the pre-trained model for self-alignment. Additionally, to evaluate generalization capabilities, we test Qwen2.5-Math-7B-Instruct~\citep{yang2024qwen2} as the base model on the same dataset.
For PIPA, we set the head of $g^{\theta}$ to be a two-layer MLP followed by a Sigmoid function, with hidden dimension $4096$ same as the base model.

\paragraph{Training}
All experiments are conducted based on OpenRLHF \citep{hu2024openrlhf}. For all training, we use LoRA \citep{hu2021lora} with rank 64 and $\alpha=16$. All alignment algorithms are conducted for 1 epoch after the SFT stage. Denote $bs$ to be the batch size and $lr$ to be the learning rate. we do grid search for $lr\in\{5\times10^{-7}, 5\times10^{-6}, 5\times10^{-5}\}$ for all experiments and present the best one. 


\begin{itemize}
    \item \textbf{SFT} Before all alignment algorithms, we first fine-tune the pre-trained Deepseek and Qwen models on the positive samples for 3 epochs with $bs=1024$ and $lr=4\times 10^{-5}$. The model obtained after SFT is then used as the initilization for the target model $f^{\theta}_{}$ in alignment procedures, as well as the fixed reference model in DPO and KTO. Furthermore, to avoid extra computation, this same model serves as the  prior in both PIPA-M and PIPA-N, ensuring that PIPA does not require an additional training phase compared to DPO and KTO.
    \item \textbf{DPO} For DPO-based algorithms including DPO, IPO, Step-DPO, we train 1 epoch after the SFT stage, with $bs=256, lr=5\times10^{-7}$ and $\beta=0.1$.
    \item \textbf{KTO} For KTO, we set $lr=5\times10^{-5}$ for Deepseek model and $lr=5\times 10^{-7}$ for Qwen model. For both, $bs=256, \beta=0.1$. Step-KTO shares exactly the same recipe with KTO.
    \item \textbf{PIPA} We set $bs=256$ for all four settings, $lr=5\times10^{-5}$ for Deepseek and $5\times10^{-7}$ for Qwen. All settings are the same as KTO and Step-KTO, without additional hyperparameters to be tuned.
\end{itemize}

\subsection{Main Results}
We show our main results in Table \ref{tab:main}. We can see that for all four settings and two models, PIPA achieves the best performance without additional computation cost.





\begin{table*}[h!]
\centering
\begin{tabularx}{1.0\textwidth}{X | X |>{\hsize=2.5\hsize}X | >{\hsize=0.625\hsize}X >{\hsize=0.625\hsize}X | >{\hsize=0.625\hsize}X >{\hsize=0.625\hsize}X}
\toprule
\multirow{2}{*}{\textbf{Data}} & \multirow{2}{*}{\textbf{Annotation}} & \multirow{2}{*}{\textbf{Algorithm}} & \multicolumn{2}{c}{\textbf{GSM8K}} & \multicolumn{2}{c}{\textbf{MATH}} \\ \cline{4-7}
& & & \textbf{DS} & \textbf{QW} & \textbf{DS} & \textbf{QW} \\ 
\midrule
\multirow{8}{*}{Paired} & \multirow{5}{*}{Answer-wise} & DPO \citep{rafailov2024direct}    & 68.39 & 67.17 & 46.94 & 47.78 \\  
                        &                               & IPO \citep{IPO}    & 69.14 & 72.33 &  46.94 & 49.96 \\ 
                        &                               & KTO \citep{ethayarajh2024kto}    & 76.72 & 62.47 & 47.38 & 46.53 \\ 
                        &                               & \textbf{PIPA-M}  & 79.08 & \textbf{73.77} & 50.82 & \textbf{51.60} \\ 
                        & & \textbf{PIPA-N} & \textbf{80.29} & 70.89 & \textbf{52.32} & 47.26 \\ \cline{2-7}
                        & \multirow{4}{*}{Step-wise}    & Step-DPO \citep{Step-dpo} & 68.54 & 66.11 & 46.96 & 48.38 \\ 
                        % &                               & SVPO   & & & & \\ 
                        &                               & Step-KTO \citep{stepkto} & 75.44 & 62.47 & 47.38 & 45.64 \\ 
                        &                               & \textbf{PIPA-M}  & \textbf{79.15} & \textbf{74.91} & 51.94 & \textbf{53.26} \\
                        & & \textbf{PIPA-N} & 78.70 & 73.84 & \textbf{52.54} & 49.06 \\
                        \hline
\multirow{4}{*}{Unpaired} & \multirow{3}{*}{Answer-wise} & KTO \citep{ethayarajh2024kto}   & 76.04  & 64.44 & 46.72 & 47.08 \\ 
                          &                               & \textbf{PIPA-M}  & 79.08  & \textbf{74.75} & 51.04 & \textbf{52.78} \\ 
                          & & \textbf{PIPA-N} & \textbf{80.97} & 74.22 & \textbf{52.22} & 52.00 \\ \cline{2-7}
                          & \multirow{3}{*}{Step-wise}    & Step-KTO \citep{stepkto} & 76.81 & 64.14 & 46.98  & 45.64  \\ 
                          &                               & \textbf{PIPA-M}  & 78.24 & \textbf{74.22} & 51.82 & \textbf{53.10} \\ 
                          & & \textbf{PIPA-N} & \textbf{79.98} & 72.86 & \textbf{52.78} &  52.52\\
                          \bottomrule
\end{tabularx}
\caption{Results on GSM8K and MATH. DS means Deepseek-based models, and QW means Qwen-based models.}
\label{tab:main}
\end{table*}


\subsection{Additional Analysis and Ablation Studies}
We conduct a more detailed analysis from two perspectives. Section \ref{sec:algo ablation} delves into further studies on our PIPA itself. Section \ref{sec:step ablation} examines the step-level and answer-level settings.

\subsubsection{Algorithms}
\label{sec:algo ablation}

\paragraph{PIPA-M vs. PIPA-N}
PIPA-M and PIPA-N are two versions of our framework that incorporate distinct prior constraints. As shown in Table \ref{tab:main}, neither variant consistently outperforms the other. Notably, PIPA-N tends to perform better with the Deepseek model, while PIPA-M shows superior results with the Qwen model. This may suggest that PIPA-N is better suited for self-alignment scenarios, while PIPA-M is more effective for alignment tasks where there is a distribution shift between the model and the data. 
In practice, we recommend experimenting with both variants to determine the optimal choice for your specific use case.


\paragraph{Value model $\prob(c_t\mid x,y_{<t})$} Our framework employs two components: the target model $\prob^\theta(y\mid x,c=1)$ and a value model $\prob^\theta(c_t=1\mid x,y_{<t})$. 
\begin{wraptable}{r}{3cm}
    \begin{tabular}{c c} 
    GSM8K & MATH \\
    \toprule
    \textbf{78.24} & \textbf{51.82} \\
    \midrule
    73.54 & 47.92 \\
    \bottomrule
    \end{tabular}
    % \vspace{-0.2cm}
    \caption{In the second row, $\prob^\theta(c_t\mid x,y_t)$ is fixed at 0.5.}
    % \vspace{-0.2cm}
    \label{tab:remove}
\end{wraptable}
These are jointly trained through optimization of their combined probabilistic objective, rather than being learned separately. 
Downstream task evaluations confirm that $\prob^\theta(y\mid x,c=1)$ is well optimized. 
To assess the impact of the value model, we present the results of removing it in Table \ref{tab:remove}, where the performance decline highlights its importance.
To examine the value model's learning behavior, we plot the training trajectory of $\prod_t \left(\prob^\theta(c_t\mid x,y_{<t})\right)^{1/T}$ for the first 300 steps in Figure \ref{fig:value}. 
The implicit optimization process yields continuous improvement in likelihood estimation, with the model's predictions showing a steady increase from the initial random-guess baseline of $0.5$. This empirical validation establishes a foundation for using the optimized value model $\prob^\theta(c_t=1\mid x, y_{<t})$ to implement search during inference, presenting a clear direction for future research.
\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/value.pdf}
    \caption{We plot the geometrically averaged likelihood  $\left(\Pi_t\prob^\theta(c_t\mid x, y_{<t})\right)^{\frac{1}{T}}$ for PIPA-M and PIPA-N with answer-level annotation and step-level annotation respectively during training, showing a consistent increase.
    }
    % \vspace{-0.2cm}
    \label{fig:value}
\end{figure}
 
\paragraph{Prior choices}
To ensure a fair comparison with baseline methods such as DPO and KTO, we utilize the same SFT model for initializing $\prob^\theta(y\mid x,c=1)$ and setting priors. In our PIPA model, however, the priors should ideally be $\prob(y\mid x)$ for PIPA-M and $\prob(y\mid x,c=0)$ for PIPA-N, which differ from the SFT model's $\prob^\theta(y|x,c=1)$. To further investigate PIPA with accurate priors, we began with the released Deepseek model, training it on both positive and negative samples for three epochs to derive $\prob^\theta(y\mid x)$, and solely on negative samples for three epochs to obtain $\prob^\theta(y\mid x,c=0)$. Unfortunately, these adjustments did not yield improvements. Results are shown in Table \ref{tab:true prior}.
This may be due to the lack of training on marginal or negative samples in the initial pre-training and fine-tuning stages of model development, meaning that a few epochs of fine-tuning are insufficient to establish accurate priors for these distributions. 




\paragraph{Additional SFT loss} 

Previous studies \citep{pang2024iterative, dubey2024llama} have shown that incorporating an additional SFT loss in DPO enhances its stability. We extend it to the unpaired setting, applying it to our PIPA-M and KTO algorithms in the answer-wise case, with an SFT loss coefficient set at 1.0. 
% As demonstrated in Table \ref{tab:add sft}, both algorithms show further improvement.
As shown in Table \ref{tab:add sft}, incorporating additional SFT loss provides more advantages for KTO compared to our PIPA, yet it remains less effective than our PIPA. This is because our PIPA is theoretically grounded for a general case and does not require further modifications to the loss function.



\begin{table}[h!]
    \centering
    \makebox[\columnwidth]{ % 确保表格不会溢出列宽
        \begin{minipage}{0.48\columnwidth}
            \centering
            \resizebox{\textwidth}{!}{ % 控制表格大小
                \begin{tabular}{c|c c}
                \toprule
                    {} & GSM8K & MATH \\
                \midrule
                     PIPA-M & \textbf{78.24} & \textbf{51.82} \\
                     PIPA-M(T) & 77.86 & 50.60\\
                \midrule
                     PIPA-N & \textbf{79.98} & \textbf{52.78} \\
                     PIPA-N(T) & 79.83 & 50.84 \\
                \bottomrule
                \end{tabular}
            }
            \caption{PIPA with different priors. (T) denotes further fine-tuning of the SFT model on all or negative samples.}
            \label{tab:true prior}
        \end{minipage}
        \hspace{0.02\columnwidth} % 调整间距
        \begin{minipage}{0.48\columnwidth}
            \centering
            \resizebox{\textwidth}{!}{
                \begin{tabular}{c|c c}
                \toprule
                    {} & GSM8K & MATH \\
                \midrule
                     KTO & 76.04  & 46.72 \\
                     KTO+SFT & 76.27 & 47.96 \\
                \midrule
                     PIPA-M & \textbf{79.08} & \textbf{50.82} \\
                     PIPA-M+SFT & 78.24 & 50.12 \\
                \bottomrule
                \end{tabular}
            }
            \caption{Effect of additional SFT loss on KTO and PIPA-M.}
            \label{tab:add sft}
        \end{minipage}
    }
\end{table}






\subsubsection{Step-level setting}
\label{sec:step ablation}
As our research is the first to systematically explore the performance of alignment algorithms across various settings, we provide an in-depth analysis of the step-level setting in this section. We aim to understand the advantages of step-level annotation and how it influences the training.

\paragraph{Influence of step-level annotation}

From Table \ref{tab:main}, we observe that step-level annotation does not consistently improve performance when comparing answer-wise and step-wise annotation within the same algorithm and dataset. Specifically, step-level annotation proves beneficial for MATH but can sometimes negatively impact GSM8K. This finding aligns with previous studies \citep{chen2024alphamath}, suggesting that step-level annotation is more advantageous for challenging reasoning tasks like MATH but may be unnecessary or even harmful for simpler tasks like GSM8K.


\paragraph{Reward curve}
As shown in previous works \citep{pang2024iterative,dubey2024llama,razin2024unintentional}, one problem in DPO is that the implicit reward for positive samples $\log\frac{\prob^\theta(y\mid x,c=1)}{\prob^\prior(y\mid x))}$ can also decrease during preference learning—undesirable, since this term is precisely what DPO aims to optimize.
Their approach addresses this problem by adding extra SFT loss during DPO training. We have noticed similar patterns in our DPO experiments. Notably, we found that employing step-level annotation can effectively address this issue. This observation offers an alternative angle for tackling the problem in DPO, stemming from the absence of step-level annotation. It's possible that some steps in incorrect answers are actually correct and share similarities with the distribution of correct answers. Consequently, minimizing these correct steps in incorrect answers with the original answer-level DPO could also reduce the likelihood of correct answers. 
Our results highlight the importance of fine-grained, step-level annotation alignment from a new perspective.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/chosen_rejected.pdf}
    % \vspace{-0.4cm}
    \caption{We present plots of $\log\frac{\prob^\theta(y\mid x,c=1)}{\prob^{\prior}(y\mid x))}$ for both correct and incorrect samples, shown in the left and right figures respectively, for both the original DPO and Step-DPO. While the term for correct samples decreases as observed in previous studies, it increases in Step-DPO.}
    % \vspace{-0.2cm}
    \label{fig:enter-label}
\end{figure}


\paragraph{Threshold for positive steps}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/math_threshold.pdf}
    % \vspace{-0.5cm}
    \caption{Accuracy on MATH with different threshold for the positive steps in wrong answers.}
    \label{fig:threshold}
\end{figure}
The step-level annotation, specifically the Q value obtained by MCTS in our AlphaMath dataset \citep{chen2024alphamath}, is presented in a continuous format ranging from $[-1, 1]$.
% \begin{wrapfigure}{r}{5cm}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/math_threshold.pdf}
%     % \vspace{-0.5cm}
%     \caption{Accuracy on MATH with different threshold for the positive steps in wrong answers.}
%     \label{fig:threshold}
% \end{wrapfigure}
The similar continuous format is employed in other annotation pipelines such as LLM-as-a-judge \citep{Step-dpo}.
In our main experiments, we employ a default threshold of 0.5 for labeling correct steps in incorrect answers. 
As analyzed in Figure \ref{fig:threshold}, which evaluates the impact of varying threshold values, we observe that an intermediate threshold achieves optimal performance. 
This balance ensures cautious filtering of positive steps in negative answers while retaining sufficient high-quality positive steps to maintain learning efficacy.

\section{Conclusion}
In this paper, we introduce Prior-Informed Preference Alignment (PIPA), a fully probabilistic framework grounded in statistical estimation. We analyze the limitations of pure SFT within this framework and demonstrate that DPO and KTO emerge as special cases with distinct prior constraints. We propose two variants, PIPA-M and PIPA-N, each incorporating different prior constraints. Through comprehensive evaluation across four distinct data settings, we systematically highlight PIPA’s advantages over previous methods. Additionally, ablation studies reveal the optimal design of PIPA, while empirical analysis explores the impact of step-level annotation from multiple perspectives, leveraging PIPA’s flexibility.



\bibliographystyle{unsrtnat}
\bibliography{ref}

\newpage
\appendix
\onecolumn
\section{Detailed Discussion about Connection to DPO and KTO}
\label{sec:proof}

\subsection{The DPO Loss}
\label{sec:connect dpo}

DPO uses a pairwise comparison loss on paired positive and negative data.  
Denote the pair data to be $\{x^i, {y^i}^+, {y^i}^-\}$, where ${y^i}^+$ is the chosen answer sampled from $\prob(y\mid x,c=1)$ and ${y^i}^-$ sampled from $\prob(y\mid x,c=0)$ is the rejected answer. 
Using our notation, the DPO objective \citep{rafailov2024direct} is 
\begin{align}
    \max_\theta 
    L_{\texttt{DPO}} (\theta)  \overset{\texttt{def}}{=} \mathbb{E} [\log\sigma&\left(r^\theta (y^+, y^-, x)\right)],
    \label{eq:dpo original}
\end{align}
where 
\begin{align*}
r^\theta (y^+, y^-, x) = \log\frac{\prob^\theta(y^+\mid x,c=1) \prob^{\prior}(y^-\mid x)}{\prob^{\theta}(y^-\mid x,c=1) \prob^{\prior}(y^+\mid x)}. 
\end{align*}


From our perspective, DPO can be viewed as 
minimizing a pairwise comparison loss, 
subject to an prior assumption on the negative probability $p(y\mid x, c= 0)$, rather than the marginal probability $p(y\mid x)$. 
\begin{theorem}
Draw $(x,y,c)$ from a joint distribution $p$, for which $p(c=1\mid x)=1/2$. Further, set the sample $y^{c} = y$, and draw an independent contrastive sample via  $y^{\neg c} \sim p(\cdot \mid x, 1-c).$  
    Then maximizing the original DPO objective (\ref{eq:dpo original}) is equivalent to solving the following problem:
    \begin{align}
        \label{eq:mle dpo}
        \begin{split}
        & \max_{\theta} 
        % L_{\mathtt{DPO}} (\theta) \\ 
        \quad \mathbb{E}_{\prob} \left [ \log \prob^\theta(c  \mid x, (y^{c}, y^{\neg c})) \right]  \\
        &s.t. \quad  \quad \prob^\theta(y\mid x, c=0)=\prob^{\prior}(y\mid x), \\
        & \qquad \qquad \prob^\theta(c=1\mid x)=\prob^\theta(c=0\mid x)=0.5, \forall x, y.
        \end{split}
        % & \qquad \qquad \prob^\theta(c = 1\mid x) = \prob^{\theta}( c= 0\mid x)=\frac{1}{2}. 
     %   \nonumber
    \end{align}
    %where $\{Y_x^+, Y_x^-\} = \{y^+, y^-\}$ means equality at the set level, disregarding order.
    \label{thm:dpo}
\end{theorem}




\begin{proof}
    Using Bayes' Theorem, we have:
    \begin{align}
        \prob^\theta(c \mid x, (y^c, y^{\neg c})) = \frac{\prob^\theta((y^c, y^{\neg c})\mid x, c)\prob^\theta(c\mid x)}{\prob^\theta((y^c, y^{\neg c})\mid x)}. \nonumber\\
    \end{align}
    Denote 
    \begin{align}
    h^\theta(y^c, y^{\neg c}, x, c) :=\prob^\theta(y^c\mid x, c)\prob^\theta(y^{\neg c}\mid x,1-c), \nonumber
    \end{align}
    Define $(y^+, y^-)=\mathbbm{1}_{\{c=1\}}(y^c, y^{\neg c}) + \mathbbm{1}_{\{c=0\}}(y^{\neg c}, y^c)$. We have
    \begin{align}
        \prob^\theta(c \mid x, (y^c, y^{\neg c})) 
        &=\frac{h^\theta(y^c, y^{\neg c}, x, c)\prob^\theta(c\mid x)}{h^\theta(y^c, y^{\neg c}, x, c)\prob^\theta(c\mid x) + h^\theta(y^c, y^{\neg c}, x, 1-c)\prob^\theta(1-c \mid x)}
        % \label{eq:dpo succ} 
        \nonumber \\
        &=\frac{\prob^\theta(y^c\mid x, c)\prob^\theta(y^{\neg c}\mid x,1-c)}{\prob^\theta(y^c\mid x, c)\prob^\theta(y^{\neg c}\mid x,1-c) + \prob^\theta(y^c\mid x, 1-c)\prob^\theta(y^{\neg c}\mid x,c)} \nonumber\\
        &=\frac{p^\theta(y^+\mid x, c=1) p^{\text{prior}}(y^{-}\mid x)}
             {p^\theta(y^+\mid x, c=1) p^{\text{prior}}(y^-\mid x) + p^{\text{prior}}(y^+\mid x) p^\theta(y^{-}\mid x, c=1)}
        \label{eq:dpo bayes}
    \end{align}
    On the other hand, notice that $\log\sigma(x)=-\log(1+\exp(-x))$. So the original DPO loss is
    \begin{align*}
        \max_\theta 
        L_{\texttt{DPO}} (\theta)  
        &\overset{\texttt{def}}{=} \mathbb{E} [\log\sigma\left(r^\theta (y^+, y^-, x)\right)]\\
        &=-\log (1+\exp(-r^\theta (y^+, y^-, x))) \\
        &=-\log \left(1+\frac{\prob^{\theta}(y^-\mid x,c=1) \prob^{\prior}(y^+\mid x)}{\prob^\theta(y^+\mid x,c=1) \prob^{\prior}(y^-\mid x)}\right).
    \end{align*}
    From this, it's straightforward to see that DPO loss \eqref{eq:dpo original} is equivalent to applying $-\log()$ to (\ref{eq:dpo bayes}).
\end{proof}





Therefore, Theorem \ref{thm:dpo} shows that DPO is well recovered by our framework. 
In DPO, besides injecting prior for $\prob(y\mid x,c=0)$ instead of $\prob(y\mid x)$, it has additional prior $\prob^\theta(c=1\mid x)=\prob^\theta(c=0\mid x)$. A direct idea is to remove this prior, and set $\prob^\theta(c=1\mid x)$ to be a learnable model for $x$ similar to PIPA-M.

\subsection{Connection to KTO}
\label{sec:connect kto}







The KTO objective is
\begin{align}
    \max_\theta \mathbb{E}_{p^{\data}}\quad &\left[c\sigma\left(h_\theta(x,y)-z(x)\right)+ (1-c) \sigma\left(-h_\theta(x,y)+z(x)\right)\right], 
    \label{eq:kto original}
\end{align}
where 
$$h_\theta(x,y):=\log\frac{\prob^{\theta}(y\mid x, c=1)}{\prob^{\prior}(y\mid x)},$$
and  
$$z(x):=\KL(\prob^{\theta}(y\mid x,c=1)||\prob^{\prior}(y\mid x))).$$ 
We show the following equivalence.
\begin{theorem}
    Maximizing the original KTO objective (\ref{eq:kto original}) is equivalent to solving the following problem:
    \begin{align}
        &\max_\theta  \mathbb{E}_{\prob^{\data}}\left [ \prob^{\theta}(c|x,y)\right] \label{eq:kto thm}\\
        &\textit{s.t.} \quad \forall x,y: \prob^{\theta}(y\mid x,c=0)=\prob^{\prior}(y\mid x) \nonumber\\
        & \qquad \log\frac{\prob^\theta(c=0\mid x)}{\prob^\theta(c=1\mid x)} = \KL(\prob^{\theta}(y\mid x, c=1)|| \prob^{\prior}(y\mid x)). \nonumber
    \end{align}
    \label{thm:kto}
\end{theorem}

\begin{proof}
    For the original KTO objective, we have:
    \begin{align*}
        &\sigma\left(h_\theta(x,y)-z(x)\right)=\sigma\left(\log\frac{\prob^{\theta}(y\mid x, c=1)}{\prob^{\prior}(y\mid x)e^{z(x)}}\right) = \frac{\prob^\theta(y\mid x, c=1)}{\prob^\theta(y\mid x, c=1) + \prob^{\prior}(y\mid x)e^{z(x)}},\\
        &\sigma\left(-h_\theta(x,y)+z(x)\right) = 1- \sigma\left(h_\theta(x,y)-z(x)\right).
    \end{align*}
    And for \eqref{eq:kto thm}, we have:
    \begin{align*}
        \prob^\theta(c=1\mid x,y) &= \frac{\prob^{\theta}(y\mid x,c=1)\prob^\theta(c=1\mid x)}{\prob^{\theta}(y\mid x,c=1)\prob^\theta(c=1\mid x)+\prob^{\prior}(y\mid x)\prob^\theta(c=0\mid x)} \\
        &=\frac{\prob^{\theta}(y\mid x,c=1)}{\prob^{\theta}(y\mid x,c=1)+\prob^{\prior}(y\mid x)\frac{\prob^\theta(c=0\mid x)}{\prob^\theta(c=1\mid x)}}.
    \end{align*}
    Hence the equivalence is straightforward.
\end{proof}



\subsubsection{Comparison with KTO}
\label{sec:compare kto}
\begin{algorithm}[h!]
    \caption{KTO \citep{ethayarajh2024kto}}
    \label{alg:kto}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} data $\{(x, y,c)\}_{i=1}^N$ where $c\in\{0,1\}$, a fixed reference model $\prob^{\prior}$, trainable models $f^{\theta}_{}$ initialized with $p_0$.
        \FOR{every batch}
        \STATE Compute:
        $$F_\theta(x, y):=\sum_t\log\left(\frac{f^{\theta}_{}(y_t|x,y_{<t})}{p^{\prior}(y_t|x, y_{<t})}\right).$$
        \STATE Estimate $z(x):=\KL(f_{}^\theta(y\mid x)\|p^{\prior}(y\mid x))$. 
        \STATE Denote $\sigma(x):=\frac{1}{1+\exp(-x)}$. Minimize loss:
        $$L(x,y,c)=-c\sigma\left(F_\theta(x,y)-z(x)\right) - (1-c)\sigma\left(-F_\theta+z(x)\right).$$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}


We present KTO in practice in Algorithm \ref{alg:kto} using our notation for better comparison. 
The prior assumption on  $\prob(c\mid x)$ does not seem to be natural  and removing it yields  PIPA-N. In terms of the algorithms, PIPA has the following key differences with KTO:
\begin{itemize}
    \item PIPA has an additional learnable head $g^{\theta}$ to capture $\prob^{\theta}(c_t|x,y_{<t})$. Unlike the language model head, which matches the vocabulary in its output dimension, $g^{\theta}$ has an output dimension of only 1. Consequently, the extra parameters amount to fewer than $1\%$ of the total, resulting in no overhead in both memory and speed.
    \item KTO does not extend to the step-level setting, whereas PIPA seamlessly accommodates both the answer-level and step-level settings within a single framework, supported by clear theoretical guidance.
    \item KTO needs to additionally estimate the KL term $c(x)$ by pairing random question and answers, which adds an extra step and slows its overall process. In contrast, even with its additional learnable head, PIPA remains faster in practice.
    \item KTO uses the SFT model for both fixed reference model and $f^{\theta}_{}$ initialization, and this is the only choice.
    PIPA framework allows arbitrary selection of the fixed prior model $p^{\prior}$. For simplicity, we can choose $p^{\prior}=f_0$ which is the same choice as KTO. But in PIPA, since the prior $p^{\prior}$ is unrelated to $f^{\theta}_{}$, we can also set $p^{\prior}$ to be the fine-tuned version of $f_0$ on both positive and negative data to get better estimation.
    \item Following DPO, KTO views the log ratio $\log(f^{\theta}_{}/\prob^{\prior})$ as rewards, which is directly maximized or minimized. In PIPA-M, the ratio $\log(f^{\theta}_{}g^{\theta}/\prob^{\prior})$ is the log likelihood, and we need to compute $\log(1-f^{\theta}_{}g^{\theta}/\prob^{\prior})$ for the negative steps, instead of things like $-\log(f^{\theta}_{}g^{\theta}/\prob^{\prior})$ as KTO and other works.
\end{itemize}




\section{Baseline Algorithms}
\label{sec:bsl algo}

\paragraph{Step-DPO}
The original Step-DPO \citep{Step-dpo} requires preference data generated from a tree structure and maximizes the standard DPO loss on the diverging nodes.  
Here, we propose a generalization of the algorithm that works with more generic paired data, without requiring a tree structure.  
%The original Step-DPO \citep{Step-dpo} requires preference data generated from a tree structure, and then maximize the regular DPO loss on the diverge nodes.  Here we propose a generalization of the algorithm that  works on more generic paired data without tree structures. 

We are given pairwise data $(x, y^+, y^-, c^+, c^-)$ with token-level representation, where $c^+$ consists entirely of ones, while $c^-$ contains a mix of ones and zeros. First, we define  
\[
r_t(x, y) := \log \frac{\prob^\theta(y_t \mid x, y_{<t}, c_t=1)}{\prob^{\prior}(y_t \mid x, y_{<t})}.
\]
Treating the sequences as a whole, the original DPO loss is given by  
\[
L_{\texttt{DPO}}(x, y^+, y^-, c^+, c^-) = -\log\sigma\left(\sum_{t} r_t(x, y^+) - \sum_{t} r_t(x, y^-)\right).
\]
%
%
Given that the positive steps in $y^-$ can negatively impact model performance if minimized, a straightforward approach when providing step-level annotations is to exclude these steps \citep[see e.g.,][]{anonymous2025maskdpo}, which yields the following loss function:
\begin{align*}
    L_0(x, y^+, y^-, c^+, c^-)=-\log\sigma\left(\sum_{t}r_t(x, y^+)-\sum_{t:c_t^-=0}r_t(x, y^-)\right),
\end{align*}
where we remove the positive steps in \( y^- \) from the \(\log\sigma()\) term. However, a potential issue arises when the number of steps varies, as the magnitude of the term inside \(\sigma()\) may differ, affecting the optimization due to the \(\sigma()\) function applied externally. To address this, we propose an intermediate solution between the original DPO and the above loss. Specifically, we apply the stop-gradient operation to the positive steps in \( y^- \) and get $L_1$:
\begin{align*}
    L_1(x, y^+, y^-, c^+, c^-)=-\log\sigma\left(\sum_{t}r_t(x, y^+)-\sum_{t:c_t^-=0}r_t(x, y^-)-\mathtt{sg}\left(\sum_{t:c_t^-=1}r_t(x, y^-)\right)\right),
\end{align*}
where $\mathtt{sg}(\cdot)$ denotes stop gradient. 


In essence, $L_0$ masks the implicit reward of positive steps within a negative answer in the objective, while $L_1$ masks these positive steps only during the backpropagation step when computing gradients. The loss function $L_0$ corresponds to the approach introduced in \citep{anonymous2025maskdpo}, whereas $L_1$ is equivalent to the Step-DPO formulation when using tree-structured pairwise data. Our experiments adopt $L_1$, as it demonstrates better performance.  




\paragraph{Step-KTO} 
Very recently, Step-KTO \citep{stepkto} introduced a loss function designed for data with step-level annotations. They partition the answer into groups corresponding to steps, where each $\sigma()$ contains only one group. For unpaired data $(x, y, c)$ represented at the token level, let $1 = s_1 < \cdots < s_K \leq T$ denote the starting tokens of all $K$ steps. Here, $c_t$ remains constant for $t \in [s_k, s_{k+1})$ for $1 \leq k \leq K$. The function $r_t(x,y)$ follows the same definition as in Step-DPO. The original KTO loss is  
\[
L_{\texttt{KTO}}(x,y,c) = c_T \sigma\left(\sum_{t} r_t(x,y) - z_0\right) + (1 - c_T) \sigma\left(-\sum_t r_t(x,y) + z_0\right).
\]  
The Step-KTO loss is given by  
\[
L_{\texttt{Step-KTO}}(x,y,c) = -\sum_{k=1}^K \left[ c_{s_k} \sigma\left(\sum_{s_k\leq t< s_{k+1}} r_t(x, y) - z_0\right) + (1 - c_{s_k}) \sigma\left(-\sum_{s_k\leq t< s_{k+1}} r_t(x,y) + z_0\right) \right].
\]  
However, our experiments revealed that Step-KTO loss does not improve performance. Inspired by the Step-DPO loss proposed earlier, we adopt the original KTO for positive answers while applying a different approach for negative answers by masking the gradient of positive steps:  
\[
L_1(x,y,c) = -\sigma\left(-\sum_{t: c_t=0} r_t(x,y) - \mathtt{sg}\left(\sum_{t:c_t=1} r_t(x,y)\right) + z_0\right).
\]  
The key idea is to retain the forward pass of all steps in $\sigma()$ for normalization while excluding positive steps in the backward pass to prevent their probabilities from being minimized.

\end{document}
