\section{Related Work}
\paragraph{Learning from preference data}
% It is widely recognized that LLMs benefit from learning via preference data rather than relying solely on SFT on positive examples. 
RL has become a key framework for leveraging preference data for LLM alignment, with early methods like PPO \citep{schulman2017proximal}, which first trains a reward model on pairwise human feedback \citep{ouyang2022training}. Due to PPOâ€™s high training cost, direct policy optimization methods without online RL have been explored, integrating policy and reward learning into a single stage. Notable works include DPO \citep{rafailov2024direct}, SLiC \citep{zhao2023slic}, IPO \citep{IPO}, GPO \citep{tang2024generalized}, and SimPO \citep{meng2024simpo}.
For fine-grained token-level optimization, DPO variants like TDPO \citep{zeng2024token}, TIS-DPO \citep{liu2024tis}, RTO \citep{rto}, and OREO \citep{oreo} have been introduced. To address step-level annotation inspired by PRM \citep{lightman2023let}, methods such as Step-DPO \citep{Step-dpo}, SCDPO \citep{stepcontroldpo}, and SVPO \citep{svpo} have emerged. To relax pairwise data constraints, particularly for tasks with ground truth like math and coding, KTO \citep{ethayarajh2024kto}, Step-KTO  \citep{stepkto}, and OREO \citep{oreo} have been proposed. 
Our PIPA framework addresses all these challenges within a unified paradigm, demonstrating that existing algorithms like DPO and KTO can be interpreted as special cases within our approach.

\paragraph{Probabilistic alignment}
In addition to reward maximization, some research approaches alignment from a probabilistic perspective. \citep{probabilistic} decompose label likelihood into a target distribution and a hidden distribution, solving it using the EM algorithm. Other works leverage importance sampling to train a policy parameterized by an energy-based model that aligns with the target distribution including DPG \citep{dpg}, GDC \citep{gdc}, GDC++ \citep{gdc++}, BRAIn \citep{pandey2024brain}.
Unlike these methods, our PIPA framework maximizes label likelihood while enforcing prior constraints by transforming it into the target distribution using Bayes' Theorem. We directly learn the distributions without relying on complex sampling and estimation procedures. PIPA is scalable, incurs no additional training cost, and remains flexible across any preference data.