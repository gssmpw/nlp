\section{Related Work}
\paragraph{Learning from preference data}
% It is widely recognized that LLMs benefit from learning via preference data rather than relying solely on SFT on positive examples. 
RL has become a key framework for leveraging preference data for LLM alignment, with early methods like PPO ____, which first trains a reward model on pairwise human feedback ____. Due to PPOâ€™s high training cost, direct policy optimization methods without online RL have been explored, integrating policy and reward learning into a single stage. Notable works include DPO ____, SLiC ____, IPO ____, GPO ____, and SimPO ____.
For fine-grained token-level optimization, DPO variants like TDPO ____, TIS-DPO ____, RTO ____, and OREO ____ have been introduced. To address step-level annotation inspired by PRM ____, methods such as Step-DPO ____, SCDPO ____, and SVPO ____ have emerged. To relax pairwise data constraints, particularly for tasks with ground truth like math and coding, KTO ____, Step-KTO  ____, and OREO ____ have been proposed. 
Our PIPA framework addresses all these challenges within a unified paradigm, demonstrating that existing algorithms like DPO and KTO can be interpreted as special cases within our approach.

\paragraph{Probabilistic alignment}
In addition to reward maximization, some research approaches alignment from a probabilistic perspective. ____ decompose label likelihood into a target distribution and a hidden distribution, solving it using the EM algorithm. Other works leverage importance sampling to train a policy parameterized by an energy-based model that aligns with the target distribution including DPG ____, GDC ____, GDC++ ____, BRAIn ____.
Unlike these methods, our PIPA framework maximizes label likelihood while enforcing prior constraints by transforming it into the target distribution using Bayes' Theorem. We directly learn the distributions without relying on complex sampling and estimation procedures. PIPA is scalable, incurs no additional training cost, and remains flexible across any preference data.