\input{sec/2_related_work}

\input{sec/3_robustrag}

\section{Appendix}
\subsection{Dataset Statistics}
\label{sec:dataset_stats}
Detailed statistics of these datasets we used are listed in Table~\ref{tab:dataset_stats}.
\input{table/dataset_stat}
\subsection{Implementation Details}
For model training, we utilize the LLaMA-Factory library to facilitate efficient LLM finetuning. We train our models with a learning rate of 1e-6 and set the maximum number of epochs to 3. To optimize the training process, we employ bfloat16 precision and DeepSpeed ZeRO-3 for distributed training. The gradient accumulation steps are set to 2 with a batch size of 8. For inference, we leverage vLLM to ensure efficient model serving. We maintain the default decoding parameters for each model during inference, including the default temperature and top-p sampling probabilities, to ensure fair comparison across different model variants. All training and inference procedures are conducted on 8x NVIDIA A100 80GB GPUs.

\subsection{Prompt Instruction}
\label{sec:prompt}
We present the prompt used to guide LM inference on noisy contexts in Table~\ref{tab:generator_prompt}. Specifically, we concatenate the top-5 retrieved documents into a single passage, append the input question afterward, and prompt the LM to generate a concise answer. 
\input{table/generator_prompt}

\input{table/main_result_qwen}
\input{table/mixed_doc}
\input{table/model_parameter_size_analysis}
\input{table/answer_confidence}
\input{table/cross_dataset_full}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{fig/f1_nq.pdf}
%     \caption{Comparison of F1 Scores for training with random doc and golden doc across models with varying parameter sizes from 0.5B to 70B. The left figure shows results on the HotpotQA dataset, while the right figure presents results on the NQ dataset. We provide detailed results in Appendix Table~\ref{tab:vary_parameter_analysis}.}
%     % \label{fig:parameter_scale}
%     \vspace{-0.5cm}
% \end{figure}


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\columnwidth]{fig/training_step_f1.pdf}
%     \caption{The relationship between training steps and F1 score during the fine-tuning process with random documents and golden documents using \texttt{Llama-2-7b-chat-hf} and \texttt{LLama-3-8B-Instruct} on the NQ dataset.}
%     \label{fig:training_step_f1}
%     \vspace{-0.5cm}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{fig/llama3_cross_dataset.pdf}
%     \caption{Comparison of different document selection strategies on three datasets (NQ, WebQuestions, TriviaQA) after \textbf{training on HotpotQA}. The bars with \textit{diagonal hatches} indicate results evaluated on HotpotQA, while the plain bars indicate results evaluated on other datasets to assess generalization ability. We provide more results on \texttt{Qwen1.5-7B-Chat} and \texttt{Qwen2.5-7B-Instruct} in Appendix Table~\ref{tab:cross_dataset_eval}.}
%     % \label{fig:cross_dataset_eval_part}
%     \vspace{-0.5cm}
% \end{figure}

% \subsection{Training Curve Analysis}
% In Figure~\ref{fig:training_step_f1}, we plot the training curve of the relationship between training steps and F1 score during the fine-tuning process with random documents and golden documents using \texttt{Llama-2-7b-chat-hf} and \texttt{LLama-3-8B-Instruct} on the NQ dataset. Our findings reveal that training with random documents leads to superior performance, as evidenced by achieving a higher optimal F1 score. Additionally, we observe that the model reaches a high F1 score more rapidly when trained with random documents. This suggests that the random document strategy more effectively harnesses the model's inherent robustness and generalization capabilities, resulting in enhanced performance compared to the golden document approach.