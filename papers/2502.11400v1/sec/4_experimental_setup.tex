\section{Experimental Setups}
In this section, we introduce the dataset and evaluation metrics we used to assess the robustness performance of various robust training strategies.
\subsection{Datasets and Evaluation Metrics}
For our experiment, we evaluate on four widely-used question answering datasets: (1) single-hop QA, including NaturalQuestions (NQ)~\cite{kwiatkowski-etal-2019-natural} and WebQuestions~\cite{berant-etal-2013-semantic}; and (2) multi-hop QA, including TriviaQA~\cite{joshi-etal-2017-triviaqa} and HotpotQA~\cite{yang-etal-2018-hotpotqa}. All experimental results are evaluated on their development splits using the Exact Match (EM) and F1 metrics. Detailed statistics of these datasets are listed in Appendix~\ref{sec:dataset_stats} Table~\ref{tab:dataset_stats}.

% \subsection{RAG Pipeline}
% Our experiment implements a standard two-stage RAG framework. The pipeline consists of a retrieval phase and a generation phase. For the retrieval component, we leverage Contriever~\cite{DBLP:journals/tmlr/IzacardCHRBJG22}, an advanced BERT-based dense retriever that employs unsupervised contrastive learning for document representation.

% The retrieval process operates on a Wikipedia-based knowledge corpus, specifically utilizing the Wikipedia-2018 dataset. We preprocess this corpus by segmenting Wikipedia articles into non-overlapping passages, each containing 100 words, while preserving the integrity of potentially misleading passages. To enable efficient retrieval, we generate document embeddings using Contriever and index them using FAISS\footnote{\url{https://github.com/facebookresearch/faiss}}, a powerful similarity search library. During both training and development phases, Contriever retrieves the top-20 most relevant documents for each query from our indexed Wikipedia corpus.

% For the downstream question-answering tasks, we select the top-5 retrieved documents and concatenate them with the test query to form the input context. The model then processes this enriched context to generate appropriate answers. The generator prompt we used is shown in Appendix~\ref{sec:prompt} Table~\ref{tab:generator_prompt}.

\subsection{RAG Pipeline}  
We implement a standard two-stage RAG framework with retrieval and generation phases. For retrieval, we use Contriever~\cite{DBLP:journals/tmlr/IzacardCHRBJG22}, a BERT-based dense retriever trained with unsupervised contrastive learning. Our knowledge corpus is Wikipedia-2018, preprocessed into 100-word non-overlapping passages. We encode these passages using Contriever and index them with FAISS\footnote{\url{https://github.com/facebookresearch/faiss}} for efficient retrieval. During training and development, the top-20 relevant documents are retrieved for each query. For question-answering, the top-5 retrieved documents are concatenated with the query as input for answer generation. The generator prompt is detailed in Appendix~\ref{sec:prompt} Table~\ref{tab:generator_prompt}.

% \subsection{Foundation Models}
% In our experiments, we employ four state-of-the-art foundation models: \texttt{Llama-2-7b-chat-hf}, \texttt{Llama-3-8B-Instruct}, \texttt{Qwen1.5-7B-Chat}, and \texttt{Qwen2.5-7B-Instruct}. These models represent different generations of LLMs from two prominent model families - Llama and Qwen, each with comparable model sizes around 7-8 billion parameters.

% \subsection{Robust RAG Training Scenario}

% To comprehensively evaluate the robustness of RAG systems under different document selection strategies, we design seven training scenarios across three categories.

% For basic QA capability evaluation, we directly use the \textbf{Base Model} in a zero-shot setting, where the foundation model performs inference without any RAG-specific training.

% For scenarios with relevant documents, we explore four different strategies:
% \begin{itemize}[leftmargin=0.5cm, itemindent=0cm]
%     \item \textbf{RALM}: This approach incorporates instruction tuning by prepending golden retrieval text to the context during model fine-tuning. It is worth noting that queries without golden documents in their top-20 retrieved documents are excluded from training, resulting in a smaller training set compared to other methods.
%     \item \textbf{RetRobust}: Based on \newcite{DBLP:conf/iclr/YoranWRB24}, this method enhances model robustness by exposing it to diverse retrieval qualities during training. For each query, it randomly selects between top-ranked, low-ranked, or random passages with equal probability.
%     \item \textbf{Top-1 Document}: This scenario utilizes the document with the highest similarity score from the retrieved document pool. Note that this document may not necessarily contain the correct answer, reflecting real-world retrieval scenarios where the most similar document might not always be the most helpful one.
%     \item \textbf{Golden Document}: This strategy primarily selects the document that both contains the correct answer and has the highest relevance score among the top-20 retrieved documents. In cases where no document containing the correct answer exists in the top-20 retrieved documents, we fall back to using the top-1 document to maintain consistent training sample sizes across different methods.
% \end{itemize}

% \input{table/main_result_llama}


% To evaluate the model's robustness against potentially harmful or irrelevant information, we design two adversarial scenarios:
% \begin{itemize}[leftmargin=0.5cm, itemindent=0cm]
%     \item \textbf{Random Document}: This approach randomly selects one document from the retrieved documents, simulating unpredictable retrieval quality in real-world scenarios.
%     \item \textbf{Irrelevant Document}: This scenario selects an irrelevant passage by randomly choosing from retrieval contents of other queries, ensuring no relevance to the current query.
% \end{itemize}

% These scenarios are designed to evaluate model performance across three distinct aspects: (1) the basic QA capability without retrieval augmentation, (2) the model's ability to leverage relevant documents with varying degrees of utility, and (3) the model's robustness against potentially harmful or irrelevant information. This comprehensive evaluation framework allows us to assess both the effectiveness and robustness of RAG systems under different operational conditions.

\subsection{Robust RAG Training Setups}
% We evaluate two popular robust training settings:
For our experiments, we evaluate two popular robust training settings:
\subsubsection{Document Selection Strategies}
% To assess RAG robustness under various document selection strategies, we design seven training scenarios across three categories.
We explore several document selection training strategies across three categories.

For basic QA capability evaluation, we use the \textbf{Base Model} in a few-shot setting, where the model performs inference without RAG-specific training.

For scenarios with relevant documents, we explore four strategies:
\begin{itemize}[leftmargin=0.5cm, itemindent=0cm, itemsep=0pt]
    \item \textbf{RALM}: Fine-tunes the model by prepending golden retrieval text. Queries without golden documents in the top-20 are excluded, leading to reduced training set size.
    \item \textbf{RetRobust}: Following \newcite{DBLP:conf/iclr/YoranWRB24}, this method enhances robustness by randomly selecting top-ranked, low-ranked, or random passages during training.
    \item \textbf{Top-1 Document}: Uses the highest-scoring retrieved document, which may not contain the correct answer, reflecting real-world retrieval challenges.
    \item \textbf{Golden Document}: Selects the most relevant document containing the correct answer. If none exist in the top-20, it defaults to the top-1 document for consistency.
\end{itemize}

\input{table/main_result_llama}

To assess robustness against irrelevant information, we introduce two adversarial scenarios:
\begin{itemize}[leftmargin=0.5cm, itemindent=0cm, itemsep=0pt]
    \item \textbf{Random Document}: Randomly selects a document from retrieved results, simulating unpredictable retrieval quality.
    \item \textbf{Irrelevant Document}: Chooses a passage from another query’s retrieval results, ensuring no relevance to the current query.
\end{itemize}

% These scenarios evaluate: (1) basic QA capability without retrieval, (2) the model’s ability to leverage relevant documents, and (3) robustness against irrelevant or misleading information. This framework provides a comprehensive assessment of RAG performance under diverse conditions.

\subsubsection{Adversarial Loss Design}  
We assess two popular adversarial loss strategies:
\begin{itemize}[leftmargin=0.5cm, itemindent=0cm, itemsep=0pt]
\item \textbf{RAAT}. The regularization term in RAAT reduces the performance gap between the best and worst retrieval cases. By penalizing excessive performance disparity, RAAT ensures that the model remains stable even under challenging retrieval conditions, leading to improved robustness and generalization.
\item \textbf{IRM}. The regularization in IRM minimizes the variance in performance across different retrieval environments. By enforcing consistency, IRM mitigates sensitivity to distribution shifts, ensuring that the model performs reliably across diverse retrieval scenarios.
\end{itemize}

