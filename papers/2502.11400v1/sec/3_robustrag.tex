% \section{Background}
% In this section, we introduce the framework of robust rag.

% % 基础模型的能力进化

% \subsection{Retrieval-Augmented Generation}  
% Retrieval-Augmented Generation (RAG) combines retrieval and generation to enhance the performance of language models by incorporating external knowledge. Specifically, for a given question $q$, a retriever model $\mathcal{R}$ is employed to select the top-$k$ relevant documents $\mathcal{D} = \{d_1, d_2, \dots, d_k\}$ from a large corpus. These documents are intended to provide additional context or factual grounding to support the model's reasoning process. Once retrieved, the LM uses this augmented input—typically comprising both $q$ and $\mathcal{D}$—to generate an answer $a$. 

% While this framework has shown substantial improvements in tasks requiring external knowledge, its effectiveness heavily relies on the quality, relevance, and accuracy of the retrieved documents, which introduces challenges in noisy or imperfect retrieval scenarios.

\section{Robustness Training for RAG}
\label{sec:robust_training}
\subsection{Retrieval-augmented Adaptive Adversarial Training}
To improve the robustness of retrieval-augmented language models (RALMs) against retrieval noise, RAAT~\cite{fang-etal-2024-enhancing} incorporates an adversarial loss into the standard supervised fine-tuning (SFT). The model processes one golden context and three adversarial samples at each iteration, optimizing for the most challenging perturbation. The adversarial objective follows a min-max strategy:
\begin{equation}
\textstyle
\setlength\abovedisplayskip{0.2cm}
\setlength\belowdisplayskip{0.2cm}
\begin{aligned}
\mathcal{L}_{\text{max}} = \max_{da \in DA} \mathcal{L}(\theta, da(q), a),
\end{aligned}
\end{equation}
where $\mathcal{L}$ represents the generation loss function to noisy retrievals, and $q^{\prime}=da(q)$ represents the augmented noise context of $q$. A regularization term controls excessive sensitivity to noise:
\begin{equation}
\textstyle
\setlength\abovedisplayskip{0.2cm}
\setlength\belowdisplayskip{0.2cm}
\begin{aligned}
\mathcal{L}_{\text{ada}} = \mathcal{L}_{\text{max}} + w_{\text{reg}} \cdot |\mathcal{L}_{\text{max}} - \mathcal{L}_{\text{min}}|_2^2,
\end{aligned}
\end{equation}

The adversarial loss $\mathcal{L}_{\text{ada}}$ combines both terms, where the regularization term helps stabilize training by mitigating excessive sensitivity to retrieval noise. The regularization term, calculated as the square of the difference between $\mathcal{L}_{\text{max}}$ and $\mathcal{L}_{\text{min}}$, encourages a more balanced optimization, preventing the model from overreacting to the most challenging perturbations.

\subsection{Multiagent Iterative Tuning Optimization}
ATM~\cite{zhu-etal-2024-atm} steers the generator to have a robust perspective of useful documents for question answering with the help of an auxiliary attacker agent. The generator is trained to maximize answer correctness while minimizing its sensitivity to adversarial perturbations. It receives the user query along with a document list, which may contain both relevant and fabricated information introduced by the attacker. Its objective is to generate accurate responses as long as sufficient truthful information is present while reducing the impact of misleading or fabricated content. To achieve this, the generator learns to identify and leverage relevant documents while ignoring noisy ones, regardless of whether they originate from the original retrieval $\mathcal{D}$ or adversarial perturbations $\mathcal{D}^{\prime}$. This can be formalized as maximizing the objective:
\begin{equation}
\small
\setlength\abovedisplayskip{0.2cm}
\setlength\belowdisplayskip{0.2cm}
\begin{aligned}
G(a\mid q,\mathcal{D}^{\prime}) - \mathrm{dist}\left[G(a\mid q, \mathcal{D}), G(a\mid q,\mathcal{D}^{\prime})\right],
\end{aligned}
\end{equation}
where ${G}(\cdot)$ represents the language model probability of generating an answer, and $\mathrm{dist}\left[\cdot\right]$ measures the divergence between outputs under different document conditions. The generator and the attacker are tuned adversarially for several iterations. After rounds of multi-agent iterative tuning, the generator can eventually better discriminate useful documents amongst fabrications.

\subsection{Invariant Risk Minimization}
Invariant Risk Minimization (IRM) aims to learn representations that remain stable across different environments, improving generalization under distribution shifts. To enhance the robustness of retrieval-augmented generation (RAG) across varying retrieval conditions, the V-REx objective~\cite{DBLP:conf/icml/KruegerCJ0BZPC21} can also be adapted to enforce risk invariance across different retrieval environments. Given retrieval environments $\mathcal{E} = \{1, \dots, m\}$, where each $e \in \mathcal{E}$ corresponds to a specific retrieval scenario (e.g., golden documents, top-$k$ retrieved, noisy retrieval), we define the empirical risk $\mathcal{R}_e(\theta)$ of the model parameterized by $\theta$. The training objective is formulated as:
\begin{equation}
\small
\setlength\abovedisplayskip{0.2cm}
\setlength\belowdisplayskip{0.2cm}
\begin{aligned}
\mathcal{R}_\textrm{V-REx-RAG}(\theta) = &\beta \; \mathrm{Var}(\{\mathcal{R}_1(\theta), ..., \mathcal{R}_m(\theta)\})\\ & + \sum^m_{e=1} \mathcal{R}_e(\theta),
\end{aligned}
\end{equation}
where $\beta \geq 0$ controls the trade-off between minimizing average risk and enforcing risk invariance across retrieval environments. A higher $\beta$ reduces performance discrepancies caused by retrieval variations, improving generalization under both high-quality and noisy retrieval conditions.