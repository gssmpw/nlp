
\section{Why Sophisticated Training No Longer Matters in Powerful Models?}
In this section, we conduct comprehensive experiments to delve into the reasons why sophisticated robust training strategies may no longer be crucial in powerful models.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/confidence.pdf}
    \caption{Confidence scores for correct and wrong answers on HotpotQA dataset, comparing Llama2 and Llama3 models across various robust training methods.}
    \label{fig:confidence}
    \vspace{-0.5cm}
\end{figure}


\subsection{Powerful Models Enable Natural Calibration}
% To investigate confidence calibration of models with different capacities, we take HotpotQA dataset as an example to compare the confidence between Llama2 (\texttt{Llama-2-7b-chat-hf}) and Llama3 (\texttt{Llama-3-8B-Instruct}) models. 
% To investigate confidence calibration of models with different capacities, 
To understand whether powerful models inherently possess the ability to distinguish reliable from unreliable answers, we take HotpotQA dataset as an example to examine the confidence calibration capabilities for Llama2 (\texttt{Llama-2-7b-chat-hf}) and Llama3 (\texttt{Llama-3-8B-Instruct}) models. 
Here, confidence is the mean of token-wise probabilities in the model's generated answer, providing a measure of the model's certainty in its predictions~\cite{DBLP:journals/corr/abs-2307-03987, DBLP:conf/iclr/XiongHLLFHH24}.
Figure~\ref{fig:confidence} reveals striking differences in their calibration patterns. In the base model, Llama2 shows poor natural calibration levels, where confidence scores for incorrect answers (95.8) abnormally exceed those for correct ones (93.8). In contrast, Llama3 demonstrates inherently better calibration, maintaining higher confidence for correct answers (97.5) than incorrect ones (91.3) without any specialized complex training.

While robust training methods (Golden Doc, Random Doc, and IRM) can effectively calibrate confidence scores and improve the gap between correct and incorrect answers for Llama2 from -2 to 12 with IRM, the marginal benefits of these complex training strategies diminish as model architectures advance.
For Llama3, which already achieves a 6.2 confidence gap naturally, the improvements from these training methods become less significant. This finding strongly suggests that advances in model architecture can effectively eliminate the need for complex robustness training procedures, as newer models come with better built-in calibration capabilities.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/llama2_cross_dataset.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/llama3_cross_dataset.pdf}
    \end{subfigure}
    \caption{Generalization performance comparison across different strategies trained on HotpotQA (diagonal hatches bars) and evaluated on NQ, WebQuestions, TriviaQA datasets (plain bars). More results available in Appendix Table~\ref{tab:cross_dataset_eval}.}
    \label{fig:cross_dataset_eval_part}
    \vspace{-0.6cm}
\end{figure}

\subsection{Simple Training Strategies Generalize Well in Powerful Models}
We further investigate whether powerful models can maintain robust generalization across different datasets with simple training strategies.
% To investigate why sophisticated training becomes unnecessary in modern models, we examine the generalization capabilities of different training strategies. 
We fine-tune models on HotpotQA using four document selection approaches and evaluate their transfer performance on NQ, WebQuestions, and TriviaQA.

As shown in Figure~\ref{fig:cross_dataset_eval_part}, simple strategies demonstrate surprisingly strong generalization ability. Random document selection matches or even outperforms sophisticated IRM across all evaluation datasets, with performance gaps of less than 1\%. For instance, in TriviaQA, random selection (69.5 F1) slightly surpasses both golden (68.2 F1) and IRM (68.7 F1) approaches.
This trend becomes more pronounced in the powerful \texttt{Llama-3-8B-Instruct}, where the performance gap between simple and sophisticated strategies further narrows. The consistent cross-dataset performance, regardless of training strategy, indicates that model capacity, rather than training sophistication, is the key driver of generalization ability. These findings provide strong evidence that as models become more powerful, sophisticated training strategies become increasingly unnecessary.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{fig/case1.pdf}
    % \caption{Attention distribution heatmaps for models. Each cell ($i$, $j$) represents the average attention assigned to tokens in document $i$ by the $j$-th attention layer when generating answers. \textcolor{mygreen}{\textbf{Doc1}} (highlighted in green) contains the correct answer.}
    \caption{Attention visualization for a QA case. Each subplot shows attention distribution heatmaps across different models, where cell ($i$, $j$) represents the average attention weight from the $j$-th attention layer to document $i$. Text highlighted in \textcolor{mygreen}{green} indicates the correct answer and corresponding \textcolor{mygreen}{\textbf{Doc1}}, {blue} indicates key terms from the query, and {red} indicates incorrect model predictions. The color intensity in the heatmaps indicates attention strength.}
    \label{fig:attention}
    \vspace{-0.3cm}
\end{figure}

% \subsection{Attention Distribution Analysis}
\subsection{Powerful Models Learn Effective Attention Patterns with Simple Training}
To provide a direct understanding of why simple training can achieve good performance, we visualize attention distributions across different training strategies. Figure~\ref{fig:attention} reveals that both sophisticated robust training methods (IRM) and simple approaches (random doc, top-1) achieve similar attention patterns, with clear focus on Doc1 (containing the correct answer) in middle layers (9-16). In contrast, the base model fails to attend to the correct document, generating a wrong answer. This finding provides direct evidence that powerful models can learn optimal attention mechanisms even with simple training strategies, making sophisticated training methods unnecessary.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/mixed_doc_hotpot.pdf}
    \caption{Performance comparison with different numbers of random documents during training. Increasing the number of random documents consistently improves model performance.}
    % \caption{Impact of random document proportion on F1 Scores. The figure shows the F1 scores for HotpotQA and NQ datasets when varying the number of random documents (0 to 3) in a total of three training documents. Increasing the proportion of random documents consistently improves model performance.}
    \label{fig:mixed_doc}
    \vspace{-0.3cm}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{fig/training_step_f1.pdf}
    \caption{Training curves comparison between random and golden document strategies using \texttt{Llama-2-7b-chat-hf} and \texttt{LLama-3-8B-Instruct}.}
    % \caption{The relationship between training steps and F1 score during the fine-tuning process with random documents and golden documents using \texttt{Llama-2-7b-chat-hf} and \texttt{LLama-3-8B-Instruct} on the NQ dataset.}
    \label{fig:training_step_f1}
    \vspace{-0.5cm}
\end{figure}

\subsection{Training with Random Docs: Better Performance and Faster Convergence}
We investigate why random training proves good performance from two aspects.
\paragraph{More random docs lead to better performance}
 % First, we vary the proportion of random documents (0 to 3) in training instances. As shown in Figure~\ref{fig:mixed_doc}, increasing random documents consistently improves F1 scores across both HotpotQA and NQ datasets. For Llama-2-7b, F1 scores increase from 44.0 to 47.0 on HotpotQA and 48 to 52 on NQ, with Llama-3-8B showing similar improvements.
We first vary the numbers of random documents (0 to 3) in training instances to examine how increasing random documents affects model performance. As shown in Figure~\ref{fig:mixed_doc}, increasing random documents consistently improves F1 scores across both datasets. For \texttt{Llama-2-7b-chat-hf}, using 3 random documents (versus zero) improves F1 scores by 3 points on HotpotQA and 4 points on NQ. \texttt{Llama-3-8B-Instruct} shows similar gains, suggesting that powerful models can effectively learn from random documents, making sophisticated document selection relatively unnecessary.

% To investigate the effect of varying the proportion of random documents on the robustness performance of models across different datasets, we examine the results of experiments conducted with three documents per training instance, where the number of random documents ranged from 0 to 3, with the remainder being golden documents.

% The empirical results in Figure~\ref{fig:mixed_doc} demonstrate a consistent enhancement in F1 scores as the proportion of random documents increases. Specifically, for both the HotpotQA and NQ datasets, models trained with a greater number of random documents exhibit superior performance. For example, the \texttt{Llama-2-7b-chat-hf} model shows a progressive increase in F1 score from 44.0 to 47.0 on the HotpotQA dataset and from 48 to 52 on the NQ dataset as the number of random documents increases from 0 to 3. Similarly, the \texttt{Llama-3-8B-Instruct} model demonstrates an improvement from 49 to 50 on the HotpotQA dataset and from 52 to 54 on the NQ dataset. More  experimental results on the TriviaQA and WebQuestions datasets are shown in Appendix Table~\ref{tab:appendix_mixed_doc}. These findings suggest that incorporating a higher proportion of random documents during training significantly enhances model robustness and generalization capabilities. The inclusion of random documents likely introduces greater variability and noise, which may enable the model to better generalize to diverse and previously unseen data during evaluation.



\paragraph{Faster convergence with random training}
The training dynamics in Figure~\ref{fig:training_step_f1} provide another evidence for why sophisticated document selection becomes unnecessary. Random document training not only achieves higher F1 scores (2-3 points improvement) but also reaches peak performance in fewer steps compared to golden document training. This faster convergence with better performance holds true for both model scales, indicating that simpler random training actually enables more efficient learning in powerful language models.
% In Figure~\ref{fig:training_step_f1}, we plot the training curve of the relationship between training steps and F1 score during the fine-tuning process with random documents and golden documents using \texttt{Llama-2-7b-chat-hf} and \texttt{LLama-3-8B-Instruct} on the NQ dataset. Our findings reveal that training with random documents leads to superior performance, as evidenced by achieving a higher optimal F1 score. Additionally, we observe that the model reaches a high F1 score more rapidly when trained with random documents. This suggests that the random document strategy more effectively harnesses the model's inherent robustness and generalization capabilities, resulting in enhanced performance compared to the golden document approach.

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/hotpot_randcur_llama3.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/hotpot_goodtop1_llama3.pdf}
%     \end{subfigure}
%     \centering
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/hotpot_top1_llama3.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/hotpot_IRM_llama3.pdf
%     \end{subfigure}
%     \caption{Attention distribution heatmaps for models trained with (a) Random and (b) Golden documents. Each cell ($i$, $j$) represents the average attention assigned to tokens in document $i$ by the $j$-th attention layer when generating answers. \textcolor{mygreen}{\textbf{Doc1}} (highlighted in green) contains the correct answer.}
%     \label{fig:attention}
%     \vspace{-0.5cm}
% \end{figure}