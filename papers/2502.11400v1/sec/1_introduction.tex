\section{Introduction}
Modern LLMs excel in various NLP tasks, such as text generation, knowledge reasoning, and question answering~\cite{DBLP:conf/lkm/YeLZHJ24,tao-etal-2024-trust,DBLP:journals/corr/abs-2303-18223}. Their contextual relevance and coherence make them valuable tools across domains. Despite their broad applications, LLMs struggle with external knowledge integration, particularly in RAG systems where poor retrieval results can lead to inaccurate or misleading responses~\cite{xu-etal-2024-knowledge-conflicts,tan-etal-2024-blinded,DBLP:journals/corr/abs-2402-10612}. This issue exposes a key limitation in the robustness of LLMs with imperfect retrieval results.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/intro.pdf}
    \caption{Comparison of EM scores of different document selection strategies on the TriviaQA dataset. As the model capabilities increase, the performance gap ($\Delta$) between the best and worst strategies among the four training methods decreases from 14.65\% to 4.36\%.}
    \label{fig:intro}
    \vspace{-0.5cm}
\end{figure}

To address this challenge, researchers have proposed various robust training methods to enhance RAG's resilience against noisy contexts. Approaches mainly focus on selecting high-quality documents or incorporating additional adversarial loss regularization terms during training~\cite{DBLP:conf/iclr/YoranWRB24,fang-etal-2024-enhancing,DBLP:conf/icml/KruegerCJ0BZPC21,zhu-etal-2024-atm,jin-etal-2024-bider,DBLP:journals/corr/abs-2411-14572}. For instance, RetRobust~\cite{DBLP:conf/iclr/YoranWRB24} suggests using a strategic mixture of relevant and irrelevant documents to train LLMs to withstand noisy contexts. RAAT~\cite{fang-etal-2024-enhancing} and ATM~\cite{zhu-etal-2024-atm} introduce adversarial regularization terms to encourage consistent model performance across different document contexts, making the model less sensitive to retrieval noise. 

Given the increasing capabilities of LLMs, a fundamental question emerges: \textit{do sophisticated robust training strategies remain necessary as model capacity grows?} We first conduct a simple preliminary experiment to investigate this inquiry. As shown in Figure~\ref{fig:intro}, we compare the EM scores of four different training strategies on the TriviaQA dataset. Notably, the performance gap ($\Delta$) between the best and worst strategies shrinks dramatically from 14.65\% to 4.36\% as the model capacity increases. This significant diminishing gap suggests that powerful LLMs might naturally possess enhanced robustness, potentially challenging the necessity of complex robust training techniques.

In this paper, we present a systematic investigation into the necessity of sophisticated robust training strategies as model capabilities advance. Through extensive experiments across various language models with different capabilities levels, we uncover a counterintuitive phenomenon: \emph{while models with limited capabilities benefit from high-quality document selection and complex adversarial losses, these benefits diminish substantially as model capability increases.} Notably, for highly capable models, training with randomly selected or even seemingly irrelevant documents can achieve comparable or superior performance to sophisticated training strategies.

We further conduct an in-depth study on this counter-intuitive phenomenon through a systematic analysis. Through inherent confidence calibration analysis, we discover that newer powerful models naturally exhibit superior discrimination between correct and incorrect responses. Moving to external performance, we find these models demonstrate remarkable cross-dataset generalization with basic random document training. Our visualization of attention patterns further reveals the underlying mechanism: powerful models inherently learn effective attention allocation even with simple training. These findings, from internal capabilities to external performance to mechanism analysis, indicate that powerful models may naturally possess fundamental desired capabilities, potentially reducing the need for complex training strategies.

% We further conduct an in-depth study on this counter-intuitive phenomenon from various aspects. 
% Through confidence calibration analysis, we discover that newer powerful models inherently exhibit superior calibration abilities, showing better discrimination between correct and incorrect responses. 
% For generalization ability, powerful models with basic training with random documents demonstrate remarkable cross-dataset generalization, matching or surpassing sophisticated approaches.  
% Our visualization of attention patterns further demonstrates that powerful models learn promising attention mechanisms even with simple training strategies, showing clear focus on answer-relevant documents. 
% Our comprehensive analysis indicates that powerful models may naturally possess fundamental desired capabilities, potentially reducing the need for complex robust training.
% Through confidence calibration analysis, we discover that newer powerful models inherently exhibit superior calibration abilities, maintaining higher confidence levels for correct answers than incorrect ones without specialized training. 

% For generalization ability, simple training on random documents achieves similar or even better performance than sophisticated approaches in cross-dataset generalization.  

% Experiments with random document training show both improved performance and faster convergence compared to traditional approaches, supporting the effectiveness of this simpler strategy in modern language models.
% We also find that model performance improves with increased random document exposure, and notably, this simple training strategy converges significantly faster than sophisticated training methods.



We provide insights for future RAG development, suggesting: simplified architecture design principles for powerful models, opportunities for scalable open-domain applications with minimal supervision, and theoretical perspectives on how training requirements evolve with model scaling.

The main contributions of this work are:
\vspace{-1pt}
\begin{itemize}[leftmargin=0.6cm, itemindent=0cm, itemsep=0pt]
    \item \textbf{Finding:} We reveal that as model capabilities increase, the benefits of sophisticated training strategies diminish substantially.
    % \item \textbf{Rationale:} Through detailed analysis, we trace this finding to the powerful models' inherent capabilities, naturally exhibiting strong confidence calibration, cross-dataset generalization, and promising attention allocation regardless of training sophistication.
    % \item \textbf{Rationale:} Through systematic analysis, we trace this finding to powerful models' inherent capabilities, demonstrating that they naturally possess strong confidence calibration, generalization, and effective attention mechanisms, regardless of training sophistication.
    \item \textbf{Rationale:} Through systematic analysis, we trace this finding to powerful models' inherent capabilities, demonstrating that they naturally possess strong confidence calibration, generalization, and effective attention mechanisms, regardless of training sophistication.
    \item \textbf{Insights:} We provide practical guidelines for future RAG development, advocating for architectural simplification with powerful models and highlighting new theoretical perspectives on model scaling laws.
\end{itemize}