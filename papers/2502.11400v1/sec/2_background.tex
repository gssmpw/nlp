\section{Background}
\subsection{Retrieval-Augmented Generation}
Retrieval-Augmented Generation (RAG) enhances language model performance by incorporating external knowledge retrieval into the generation process. Given a query $q$, a retriever $\mathcal{R}$ selects a set of $k$ relevant document sets $\mathcal{D} = \{d_1, d_2, \dots, d_k\}$ from a large corpus, providing additional factual grounding for the model. These retrieved documents serve as context to improve the model’s reasoning and factual accuracy. The language model $G$ then generates an answer $a$ based on the augmented input, following the conditional probability formulation:  
\begin{equation}
\textstyle
\setlength\abovedisplayskip{0.2cm}
\setlength\belowdisplayskip{0.2cm}
\begin{aligned}
G(a \mid q, \mathcal{D}) = \prod_{t=1}^{T} P(a_t \mid a_{<t}, q, \mathcal{D}; \theta),
\end{aligned}
\end{equation}  
where $a = (a_1, a_2, \dots, a_T)$ represents the generated sequence, and $\theta$ denotes the model parameters. The retrieval-augmented setup enables the model to dynamically condition its responses on external information, improving its ability to generate factually consistent answers. However, RAG models remain susceptible to retrieval noise, such as irrelevant or misleading documents, which can degrade performance and lead to hallucinations.  

\subsection{Robust Training for RAG}
\paragraph{Document Selection Strategy}
A common strategy for robust training in RAG models is to select high-quality documents from either the golden set or top-ranked passages for fine-tuning. Traditionally, supervised fine-tuning (SFT) relies on high-quality golden data, where each query is paired with a carefully curated document containing the correct answer. This ensures that the model learns from reliable sources, leading to improved factual accuracy. Naturally, leveraging such golden documents in RAG training is a straightforward and effective approach.

However, in real-world scenarios, the golden document may not always be retrieved due to limitations in the retriever. In such cases, training only on golden passages lead to unrealistic expectations during inference. A more practical approach is to incorporate top-1 retrieved documents, which better reflect what the model will encounter at test time. Even if these documents are imperfect, training on them enhances the model’s adaptability. Further, drawing from a mixture of top-ranked, lower-ranked, or randomly sampled passages, as suggested by RetRobust~\cite{DBLP:conf/iclr/YoranWRB24}, exposes the model to diverse retrieval conditions, improving its robustness to noisy or suboptimal results. This strategy ensures that RAG models generalize better across different retrieval settings, making them more reliable in real-world applications.


\paragraph{Adversarial Loss Design}
To enhance the robustness of RAG models against retrieval variations, robust training for RAG introduces a general training framework that combines standard SFT with a robustness-enforcing regularization term. During training, the model learns across multiple retrieval environments, which may include golden documents, top-k retrieved results, or adversarially perturbed documents. The overall training objective consists of two components: the first part is the standard training loss across different retrieval environments, and the second part is a regularization term designed to reduce model performance sensitivity to retrieval variations. Different forms of regularization strategies are detailed in Appendix~\ref{sec:robust_training}.

% \paragraph{Adversarial Loss Design}
% To enhance the robustness of RAG models against retrieval variations, robust training for RAG introduces a general training framework that integrates standard supervised fine-tuning (SFT) with a robustness-enforcing regularization term. Given a query $q$ and its corresponding answer $a$, the model is trained across multiple retrieval environments, where each $\mathcal{D}_e \in \mathcal{D}$ represents a distinct retrieval condition (e.g., golden documents, top-$k$ retrieved results, or adversarially perturbed documents). The overall training objective is formulated as:  
% \begin{equation}
% \textstyle
% \setlength\abovedisplayskip{0.2cm}
% \setlength\belowdisplayskip{0.2cm}
% \begin{aligned}
% \mathcal{L}_{\text{robust-RAG}} = & \sum_{e=1}^{m} \mathcal{L}(\theta, q, \mathcal{D}_e, a) \\ & + \lambda \cdot \mathcal{R}(\{\mathcal{L}(\theta, q, \mathcal{D}_e, a)\}_{e=1}^{m}),
% \end{aligned}
% \end{equation}  
% where $\mathcal{L}(\theta, q, \mathcal{D}_e, a)$ represents the standard fine-tuning loss (e.g., cross-entropy loss) under retrieval condition $e$, and $\mathcal{R}(\cdot)$ is a regularization term designed to enhance robustness by reducing performance sensitivity to retrieval variations. The hyperparameter $\lambda$ controls the trade-off between optimizing for task-specific performance and enforcing retrieval invariance.  

% The regularization term $\mathcal{R}(\cdot)$ can take different forms depending on the specific robustness strategy employed. We introduce popular robustness training strategies in detail in Appendix~\ref{sec:robust_training}.
% Common choices include min-max regularization, which minimizes the worst-case performance gap across different retrieval conditions; variance-based regularization, which reduces loss fluctuations to stabilize training; and contrastive consistency regularization, which enforces similarity between model outputs under different retrieval conditions using measures like KL divergence or cosine similarity. Additionally, adversarial robustness regularization penalizes large deviations in model predictions due to adversarially perturbed retrievals, while gradient-based regularization constrains the model’s sensitivity to retrieval variations by controlling the gradient magnitude with respect to the retrieved documents. These regularization techniques collectively enhance the model’s robustness against retrieval noise, improving stability and generalization.