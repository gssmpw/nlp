\section{Does Sophisticated Robust Training Still Matter in Powerful Models?}
To investigate whether sophisticated document selection strategies and adversarial loss designs are still essential for robust RAG performance as LLMs continue to evolve, we conduct comprehensive experiments across multiple LMs and datasets.

% 需要解释为何golden的表现不好，为何top-1这么好

\subsection{Do Sophisticated Document Selection Strategies Matter?}
% We first conduct extensive experiments to examine whether complex document selection strategies contribute to the robustness of LLMs.
We conduct experiments to analyze the effectiveness of complex document selection strategies under Llama model families (\texttt{Llama-2-7b-chat-hf} and \texttt{Llama-3-8B-Instruct}) in Table~\ref{tab:main_result_llama}, and Qwen model families in Appendix Table~\ref{tab:main_result_qwen}.
\paragraph{Training with sophisticated documents enhances LM robustness for weak models}
The experimental results presented in Tables \ref{tab:main_result_llama} and \ref{tab:main_result_qwen} provide compelling evidence that robust training significantly improves model resilience when processing noisy documents. While base models exhibit substantial performance degradation when encountering noisy documents during inference, models that undergo robust training maintain consistent and superior QA performance across various document selection strategies. 
A notable example is the \texttt{Llama-2-7b-chat-hf} model's performance (Table~\ref{tab:main_result_llama}) on the HotpotQA dataset, 
% where training with randomly selected documents yields an improved EM score of 30.67, demonstrating enhanced resilience to document noise.
where training with golden documents improves the EM score from 3.3 (Base Model) to 30.67 (Golden Doc), indicating increased resilience to document noise.
This pattern of improvement is consistently observed across both Llama (Table~\ref{tab:main_result_llama}) and Qwen (Table~\ref{tab:main_result_qwen}) model families, strongly indicating that robust training effectively mitigates the base models' inherent vulnerability to noisy documents.

\paragraph{Training with random documents shows surprising effectiveness}
We also notice that training with randomly selected documents exhibits remarkable effectiveness across all experimental configurations. Quantitative analysis shows that with \texttt{Llama-2-7b-chat-hf}, this approach achieves superior performance on WebQuestions compared to more sophisticated strategies. Similar observations emerge from experiments with \texttt{Qwen1.5-7B-Chat}, where random document selection achieves 46.04 EM on WebQuestions, approaching the optimal performance of 47.12 EM achieved by RetRobust. The consistency of these results across distinct model architectures suggests that the efficacy of random document selection represents an inherent characteristic of contemporary RAG systems.

% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/f1_hotpot.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/f1_nq.pdf}
%     \end{subfigure}
%     \caption{Comparison of F1 Scores for training with random doc and golden doc across models with varying parameter sizes from 0.5B to 70B. The left figure shows results on the HotpotQA dataset, while the right figure presents results on the NQ dataset. We provide detailed results in Appendix Table~\ref{tab:vary_parameter_analysis}.}
%     \label{fig:parameter_scale}
%     \vspace{-0.3cm}
% \end{figure*}

\paragraph{Diminishing returns of sophisticated document selections as models evolve}  
Experimental results indicate that the performance gains from sophisticated document selection strategies diminish as models evolve. For instance, comparing \texttt{Llama-2-7b-chat-hf} with \texttt{Llama-3-8B-Instruct}, the improvement in performance due to advanced document selection strategies decreases significantly, with the $\Delta$ (Worst $\rightarrow$ Best) metric for NQ dropping from 21.87\% to 7.90\% EM. A similar trend is observed when comparing \texttt{Qwen1.5-7B-Chat} to \texttt{Qwen2.5-7B-Instruct}, where the performance improvement from sophisticated document selection also shows a noticeable reduction. These results suggest that as models become more advanced, their ability to process and utilize information improves independently of complex document selection strategies, leading to diminished returns from such methods.

% \paragraph{Diminishing Returns of Sophisticated Document Selection Strategies}
% Quantitative analysis demonstrates minimal performance improvements from sophisticated document selection strategies across all experimental conditions. This observation is substantiated by the $\Delta$ (Random $\rightarrow$ Best) metric, which exhibits consistently low values across datasets and model variants. Specifically, \texttt{Llama-2-7b-chat-hf} shows a maximum improvement of only 9.28\% EM on TriviaQA, while \texttt{Qwen1.5-7B-Chat} demonstrates an even smaller margin of 2.95\% EM on HotpotQA. These limited improvements raise fundamental questions about the utility of complex document selection mechanisms.

% These experimental results reveal a particularly significant trend in the diminishing impact of sophisticated selection strategies as models evolve. Comparative analysis between \texttt{Llama-2-7b-chat-hf} and \texttt{Llama-3-8B-Instruct} demonstrates a systematic decrease in $\Delta$ values across all datasets, with TriviaQA's performance differential reducing substantially from 9.28\% to 1.49\% EM. Similar observations in the progression from \texttt{Qwen1.5-7B-Chat} to \texttt{Qwen2.5-7B-Instruct} indicate that advanced language models develop enhanced capabilities for information processing, independent of document selection methodology.

% While retrieval-based selection and RetRobust occasionally achieve optimal performance metrics, their marginal improvements over random selection do not justify the additional computational overhead. This observation is further validated by the system's robust performance even under intentionally irrelevant document selection, suggesting that sophisticated document selection strategies may not be as critical as previously assumed for RAG system performance.

% 分析使用更好的loss设计是否会带来提升
\subsection{Do Adversarial Loss Functions Matter?}
% \input{table/loss_analysis}

To investigate whether the design of complex adversarial loss functions contributes to model performance, in Table~\ref{tab:main_result_llama} and~\ref{tab:main_result_qwen}, we also analyze the robustness of various adversarial loss designs.

\paragraph{Adversarial loss significantly enhances performance for weaker models}  
For the weaker model (\texttt{Llama-2-7b-chat-hf}), incorporating adversarial loss functions such as RAAT and IRM leads to a substantial improvement in performance compared to the base model or alternative document selection strategies. Specifically, while the base model achieves an average EM / F1 of only 2.21 / 14.08, applying adversarial loss functions boosts the performance to 40.64 / 50.99 for RAAT and 46.72 / 57.72 for IRM. This highlights the effectiveness of adversarial loss in improving model robustness to noisy documents, significantly enhancing both robustness and downstream inference performance. Notably, in some cases, RAAT and IRM outperform traditional document selection strategies (e.g., top-1 Doc and golden doc), demonstrating their value in scenarios where the model needs stronger guidance to handle noisy retrievals.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/f1_hotpot.pdf}
    \caption{Comparison of F1 Scores on the HotpotQA dataset for training with Random Doc and Golden Doc across models with varying parameter sizes from 0.5B to 70B. We provide detailed results in Appendix Table~\ref{tab:vary_parameter_analysis}.}
    \label{fig:parameter_scale}
    \vspace{-0.5cm}
\end{figure}

\paragraph{Adversarial loss exhibits diminishing returns for stronger models}  
For the stronger models, such as \texttt{Llama-3-8B-Instruct} and \texttt{Qwen2.5-7B-Instruct}, the benefits of adversarial loss functions are less pronounced. The basic \texttt{Llama-3-8B-Instruct} already achieves an average EM / F1 of 34.78 / 45.97, and the introduction of adversarial losses (RAAT and IRM) results in only modest improvements, with average EM / F1 scores of 44.31 / 54.44 and 46.86 / 58.00, respectively. Similarly, \texttt{Qwen2.5-7B-Instruct} shows similar trends, with marginal gains from adversarial loss functions. These improvements are comparable to or even slightly worse than the performance achieved by random document selection (48.30 / 58.92) or top-1 document strategies (49.15 / 59.02). This indicates that the models’ inherent robustness reduces the impact of adversarial losses, and in some cases, may even hinder performance. We hypothesize that when the model’s internal robustness is already well-developed, additional constraints from adversarial losses may interfere with its ability to optimize on clean and relevant inputs.

% \paragraph{Conclusion: the necessity of adversarial loss.}
Based on these findings, we conclude that both adversarial loss functions and document selection strategies are more beneficial for weaker models. For smaller models, these techniques significantly improve robustness by mitigating the impact of noisy documents. For stronger models with inherently robust performance, the advantages of complex loss designs and sophisticated document selection diminish, suggesting that simpler strategies like random document selection may be sufficient. This underscores the importance of tailoring training strategies to the model’s inherent capabilities.


% 尝试更多种的噪音文档，例如反事实的，bad的，还有ISN，且可以和loss结合起来

% 加入4.3的讨论，表明random和golden的关系的讨论，证明为什么

% gold doc(按理说是sft中最好的选择，在rag的设置下不如random，所以我们在第五章接着去分析原因）
% 4.3讨论扩大model scale，绘制散点图，引入第五章分析random>golden的异常的现象

\subsection{Do Training Strategies Matter Across Model Scales?}
% Through analyzing Tables~\ref{tab:main_result_llama} and~\ref{tab:main_result_qwen}, we observed a counter-intuitive phenomenon: models trained with random documents outperformed those trained with golden documents. While golden documents containing ground truth answers typically yield optimal results in standard supervised fine-tuning (SFT), this conventional wisdom was challenged in the robust RAG task, particularly with the 7B and 8B models. To investigate whether this phenomenon is specific to models in the 7B-8B parameter range or persists across different scales, we conduct comprehensiv|e experiments using models ranging from 0.5B to 70B parameters.
Through analyzing Tables~\ref{tab:main_result_llama} and~\ref{tab:main_result_qwen}, we observe a counter-intuitive phenomenon: models trained with random documents outperformed those with golden documents, despite the latter containing ground truth answers typically yielding optimal results in standard SFT. To investigate whether this phenomenon extends beyond 7B-8B models, we conduct experiments across model scales from 0.5B to 70B parameters. 

The results (detailed in Appendix Table~\ref{tab:vary_parameter_analysis}) in Figure~\ref{fig:parameter_scale} demonstrate that for smaller models ($\le$3B parameters), training with golden documents leads to superior performance. This suggests that smaller models, limited by their inherent capabilities, benefit more from high-quality golden documents containing direct answers. However, as model size increases, we observe that training with random documents becomes more effective. This shift can be attributed to larger models' enhanced question-answering abilities and improved robustness. These models can better generalize to downstream tasks even when trained on random documents, which may contain noisier or less structured information. This finding indicates that sophisticated document selection strategies become less crucial as model size increases, revealing an important scaling property in model training.


% This unexpected observation raises a critical question: Is this phenomenon specific to models in the 7B-8B parameter range, or does it persist across different model scales? To investigate this, we need to examine whether similar patterns emerge in both smaller models (0.5B or 1.5B) and larger models (32B-70B).

% In this section, we conduct comprehensive experiments across a wide spectrum of model sizes, ranging from 0.5B to 70B parameters, to systematically investigate this counter-intuitive phenomenon.

% 同一个模型的golden和random用同一个颜色
% In order to investigate whether training with random documents can achieve better performance compared to golden documents on larger models, we conduct experiments using models with parameter sizes ranging from 0.5B to 70B.  

% The results depicted in Figure~\ref{fig:parameter_scale} reveal that when the model size is relatively small (e.g., less than or equal to 3B), using golden documents leads to superior performance. This suggests that smaller models possess limited inherent capabilities and thus benefit significantly from high-quality golden documents that contain answers, which aid the model in accurately identifying the correct responses. Conversely, as the model size increases, we observe a shift in performance dynamics where training with random documents begins to yield better results. This improvement can be attributed to the substantial enhancement in the model's intrinsic question-answering abilities, coupled with increased robustness and generalization capacity. Larger models are capable of leveraging their advanced capabilities to generalize effectively to downstream tasks, even when trained on random documents, which may include noisy or less structured data. This finding underscores the diminishing importance of complex document selection strategies as model size grows, highlighting the evolving nature of model training requirements with scale.





