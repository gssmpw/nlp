

\section{{\lion: Casting Full Linear Attention as a Bidirectional RNN}} \label{method1}  

In this section, we construct the full linear attention formulation \cref{eq:attvecbid}, derive its equivalent bidirectional RNN \cref{eq:bestrnn}, and introduce a chunking strategy for full linear attention \cref{chunklion}. These formulations are presented in a box to highlight their equivalence, demonstrating various inference strategies within our framework. The sections are organized as follows: \cref{sec:rnn} derives the bidirectional RNN for full linear attention, \cref{sec:chunk} presents an efficient chunkwise parallel method to balance memory and speed during inference, and \cref{Method2} addresses stable training using selective and fixed masks.

\subsection{Full Linear Attention}

In bidirectional sequence modeling tasks, the entire sequence is available during both training and inference. For softmax-based attention, this results in the form:
\[\mathbf{Y} = \text{softmax}\left(\mathbf{Q} \mathbf{K}^\top \right) \mathbf{V}\]
without the need for causal masking. For Linear Transformers, as a natural extension of the causal Linear Transformer we write the full Linear Attention as:

\begin{tcolorbox}[colback=gray!2!white, colframe=black, boxrule=0.2mm, arc=2mm ,boxsep=0.1pt, left=0.0pt, right=0.0pt, top=0pt, bottom=0.5pt]
\begin{equation}
\label{eq:attvecbid}
    \mathbf{Y} = \textsc{Scale}\left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} \right) \mathbf{V} %
\end{equation}
\end{tcolorbox}

with $\mathbf{M}$ being a full matrix including both upper and lower triangular parts created based on ${\lambda}_i$. The full mask $\mathbf{M}$ in the general form of selective mask can be written as:
\begin{align}
\label{eq:maskselbir}
     \mathbf{M}_{ij} = 
    \begin{cases} 
    \Pi_{k=j+1}^{i}{\lambda_k}, & i > j  \\
    1 & i=j\\ 
    \Pi_{k=i+1}^{j}{\lambda_k}, & i < j.
\end{cases} 
\end{align}

This mask choice naturally extends the causal case \cref{eq:maskselcause}, where the causal mask between tokens \(i, j\) is given by \(\mathbf{M}^C_{ij} = \lambda_{j+1} \lambda_{j+2} \dots \lambda_i\), representing the product of all selective scalers between \(i\) and \(j\). In the bidirectional case, the full mask \cref{eq:maskselbir} should preserve this property \cite{hwang2024hydrabidirectionalstatespace}.

To achieve the high training throughput of softmax-based Transformers for bidirectional tasks \cite{vit,deit,Beltagy2020Longformer}, we ensure Full Linear Attention (with masking) remains stable and trainable using \cref{eq:maskselbir}. For efficient inference, we formulate an equivalent bidirectional RNN for \cref{eq:maskselbir} and design a specialized chunking method for Full Linear Attention, setting it apart from chunkwise parallel techniques used in causal modeling \cite{yang2023gated,deltanet}.



\subsection{Equivalent Bi-directional RNN for Full Linear Attention} \label{sec:rnn}

Since we aim to derive an equivalent RNN for full scaled Linear Attention we first show that summing two causal Linear Transformers in RNN form ($\mathbf{a1}$ equation below) is not equal to full linear attention ($\mathbf{a2}$ equation below):

\begin{observation}
\label{obs:notbidirection}
Considering the following bidirectional recurrence equations:
\begin{align}
      & \textbf{a1) } \scalemath{0.7}{{\mathbf{S}}_i^{F/B} = {{\lambda_i}} {\mathbf{S}}_{i-1}^{F/B} + \mathbf{k}_i \mathbf{v}_i^{\top}, \hspace{2mm}
   {\mathbf{z}}_i^{F/B} = {{\lambda_i}} {\mathbf{z}}_{i-1}^{F/B} + {\mathbf{k}_i}, \hspace{1mm}
    \hspace{1mm} \mathbf{y}_i= \frac{{{\mathbf{q}_i}}^{\top} {\mathbf{S}}_i^{F/B}}{{{\mathbf{q}_i}}^{\top} {\mathbf{z}_i^{F/B}}}} \\
     & \quad \quad \quad \textcolor{red}{\Large \boldsymbol{\ne}} \quad \textbf{a2) }\mathbf{Y} = \textsc{Scale}(\mathbf{Q}\mathbf{K}^\top \odot \mathbf{M}) \mathbf{V} \label{eq:fullattlin}
\end{align}
\end{observation}

\looseness=-1
$F/B$ indicates that the same recurrence is applied in both forward and backward directions, in the following content, when doing backward recurrence, the subscript of $\mathbf{S}, \mathbf{z},\mathbf{y},\mathbf{q},  \mathbf{k}, \mathbf{v}$ should be flipped by the rule of $i:= L-i+1$.
The final output is the addition of the forward and the backward recurrences, i.e., $\mathbf{y}_i = \mathbf{y}^F_i + \mathbf{y}^B_{i}, \forall i \in \{1,...,L\}$. 


\looseness=-1Figure \ref{fig:matrixexample} illustrates that the summation of two Linear Transformers in opposite directions does not equal full Linear Attention in its parallel form (proofs are provided in Appendix \ref{sec:proofqtk}). This discrepancy arises from two factors: $(i)$ dual counting of diagonal elements, as they are included in both forward and backward directions, and $(ii)$ separate scaling applied to forward and backward directions, whereas full Linear Attention \eqref{eq:fullattlin} scales the attention matrix over the entire sequence similar to softmax-based attention ($\mathbf{Y} = \text{softmax}\left(\mathbf{Q} \mathbf{K}^\top \right) \mathbf{V}$).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{figs/examsimple.png}
    \vspace{-5mm}
    \hspace{-2mm}
  \caption{\looseness=-1\textit{Differences between Full Attention and summation of Linear Transformers in RNN form:} \textbf{a1)} Addition of two Linear Trasnformers, \textbf{a2)} Masked attention with scaling. The \textcolor{red}{red} text highlights the differences between attention and the summed recurrent models. We use \legendsquare{blue_nice} for the causal (forward recurrence), \legendsquare{orange_nice} for the non-causal (backward recurrence), and \legendsquare{diag_nice} for the diagonal part of the attention.} 
    \label{fig:matrixexample}
    \vspace{-5mm}
\end{figure}

We precede our proof considering the unified formulation of causal Linear Transformer with scalar selective decay shown at Equation \eqref{equ:lintransuni}:
\begin{proposition}
\label{prop:ssd}
Considering the following forward recurrence:
\hspace{-5mm}
 \begin{align}
    \label{eq:recrec}
    \hspace{-4mm}\scalemath{0.85}{{\mathbf{S}}_i^F = \textcolor{black}{{\lambda_i}} {\mathbf{S}}_{i-1}^F + \mathbf{k}_i \mathbf{v}_i^{\top}, \hspace{2mm}
   {\mathbf{z}}_i^F = \textcolor{black}{{\lambda_i}} {\mathbf{z}}_{i-1}^F + {\mathbf{k}_i}, \hspace{1mm}
    \hspace{1mm} \mathbf{y}_i= \frac{{{\mathbf{q}_i}}^{\top} {\mathbf{S}}_i^F}{{{\mathbf{q}_i}}^{\top} {\mathbf{z}^F}_i}.}
\end{align}
The parallel form of output is:

\vspace{-4mm}
\scalebox{0.85}{
\begin{minipage}{0.57\textwidth}
\begin{align}
\label{eq:prop1}
    \hspace{-0.4cm} \mathbf{Y}  & = \big(\textsc{scale}(\mathbf{Q}\mathbf{K}^\top \odot \mathbf{M}^C)\big) \mathbf{V}, \hspace{1mm}
   \mathbf{M}^C_{ij} = 
    \begin{cases} 
    \Pi_{k=j+1}^{i}{\lambda_k}, & i \geq j,  \\
    0, & i < j.
\end{cases} 
\end{align}
\end{minipage} }
\end{proposition}

Our goal is to derive a bidirectional RNN for \cref{eq:attvecbid}, as this framework is more generalized and can be adapted to various Linear Transformers models (proof and more detail on different variation like scaling prior to masking $\textsc{scale}(\mathbf{Q}\mathbf{K}^\top) \odot \mathbf{M}$ are provided at \cref{sec:proofqtk}). Motivated by \cref{eq:prop1} and the observation of how the attention matrix is divided into causal and non-causal components, we begin our method by splitting the attention matrix and the mask into upper and lower triangular parts:

\vspace{-5mm}

\scalebox{0.7}{
\begin{minipage}[t]{0.69\textwidth}  
\begin{align}
\hspace{-3.5mm}
\notag
   \scalemath{0.8}{\underbrace{\left( \renewcommand*{\arraystretch}{2} \begin{array}{ccccc}
      \dcolor {\mathbf{q}_1^{\top}\mathbf{k}_1}  & \orange {\mathbf{q}_1^{\top}\mathbf{k}_2} & \orange\cdots & \orange {\mathbf{q}_1^{\top}\mathbf{k}_L} \\
    \blue {\mathbf{q}_2^{\top}\mathbf{k}_1}  &  \dcolor {\mathbf{q}_2^{\top}\mathbf{k}_2}  &  \orange \cdots & \orange {\mathbf{q}_2^{\top}\mathbf{k}_L}\\
    \blue \vdots & \blue \vdots & \dcolor\ddots  & \orange \vdots \\
     \blue {\mathbf{q}_L^{\top}\mathbf{k}_1} &  \blue {\mathbf{q}_L^{\top}\mathbf{k}_2}  &  \blue \cdots  & 
    \dcolor {\mathbf{q}_L^{\top}\mathbf{k}_L}\\
  \end{array} \right)}_{\hspace{1mm}\scalemath{1.5}{\mathbf{A}={\mathbf{Q}\mathbf{K}^{\top}} }} } \odot
   \scalemath{0.75}{ \underbrace{ \left(  \renewcommand*{\arraystretch}{2} \begin{array}{ccccc}
    \dcolor{\mathbf{1}}  & \orange{\boldsymbol{\lambda}_2} & \orange{\boldsymbol{\lambda}_2 \boldsymbol{\lambda}_3}  & \orange{\cdots} & \orange{\boldsymbol{\lambda}_2\cdots\boldsymbol{\lambda}_L} \\
    \blue{\boldsymbol{\lambda}_1} &  \dcolor{\mathbf{1}} & \orange{\boldsymbol{\lambda}_3} & \orange{\cdots} & \orange{\boldsymbol{\lambda}_3 \cdots \boldsymbol{\lambda}_L} \\
    \blue{\boldsymbol{\lambda}_1 \boldsymbol{\lambda}_2} & \blue{\boldsymbol{\lambda}_2} & \dcolor{\mathbf{1}} & \orange{\cdots} & \orange{\boldsymbol{\lambda}_4 \cdots \boldsymbol{\lambda}_L} \\
    \blue\vdots & \blue\vdots & \blue\vdots & \dcolor{\ddots} & \orange \vdots \\
    \blue{{\boldsymbol{\lambda}_{L-1}\cdots \boldsymbol{\lambda}_1}} & \blue{{\boldsymbol{\lambda}_{L-1}\cdots \boldsymbol{\lambda}_2}} & \blue{{\boldsymbol{\lambda}_{L-1}\cdots \boldsymbol{\lambda}_3}} & \blue{\cdots} &   \dcolor{\mathbf{1}} \\   
\end{array}  \right) }_{\hspace{1mm}\scalemath{1.5}{\mathbf{M} }} }   
\end{align} 
\end{minipage} }
\vspace{-1mm}
 where we use \legendsquare{orange_nice} for upper triangular elements, \legendsquare{blue_nice} for lower triangular elements, and \legendsquare{diag_nice} for the diagonal elements of the attention matrix and the mask. By splitting attention ($\mathbf{A}$) and mask ($\mathbf{M}$) into upper and lower triangular forms, we obtain the following:

\vspace{-2mm}
\scalebox{0.73}{
\begin{minipage}[t]{0.65\textwidth}  
\vspace{-2mm}
\begin{align}
 \label{eq:attdec}
\scalemath{0.6}{\underbrace{\left( \renewcommand*{\arraystretch}{2} \begin{array}{ccccc}
      \dcolor {\mathbf{q}_1^{\top}\mathbf{k}_1}  & \orange {\mathbf{q}_1^{\top}\mathbf{k}_2} & \orange\cdots & \orange {\mathbf{q}_1^{\top}\mathbf{k}_L} \\
    \blue {\mathbf{q}_2^{\top}\mathbf{k}_1}  &  \dcolor {\mathbf{q}_2^{\top}\mathbf{k}_2}  &  \orange \cdots & \orange {\mathbf{q}_2^{\top}\mathbf{k}_L}\\
    \blue \vdots & \blue \vdots & \dcolor\ddots  & \orange \vdots \\
     \blue {\mathbf{q}_L^{\top}\mathbf{k}_1} &  \blue {\mathbf{q}_L^{\top}\mathbf{k}_2}  &  \blue \cdots  & 
    \dcolor {\mathbf{q}_L^{\top}\mathbf{k}_L}\\
  \end{array} \right)}_{\hspace{1mm}\scalemath{1.5}{\mathbf{A}={\mathbf{Q}\mathbf{K}^{\top}} }}  = 
   \underbrace{\left( \renewcommand*{\arraystretch}{2} \begin{array}{cccc}
       \blue\frac{1}{2}{\mathbf{q}_1^{\top}\mathbf{k}_1} &  &  &  \\
      \blue{{\mathbf{q}_2^{\top}\mathbf{k}_1}} & \blue\frac{1}{2} {\mathbf{q}_2^{\top}\mathbf{k}_2} &  &  \\
      \blue\vdots & \blue\vdots & \blue\ddots \\
      \blue{{\mathbf{q}_L^{\top}\mathbf{k}_1}} & \blue{{\mathbf{q}_L^{\top}\mathbf{k}_2}} & \blue\cdots & \blue{\frac{1}{2}{\mathbf{q}_L^{\top}\mathbf{k}_L}} \\
  \end{array} \right)}_{\hspace{1mm}\scalemath{1.5}{\mathbf{A}^F}}
  +
  \underbrace{\left( \renewcommand*{\arraystretch}{2} \begin{array}{cccc}
      \orange\frac{1}{2}{\mathbf{q}_1^{\top}\mathbf{k}_1} & \orange{\mathbf{q}_1^{\top}\mathbf{k}_2} & \orange\cdots & \orange{\mathbf{q}_1^{\top}\mathbf{k}_L} \\
       &  \orange\frac{1}{2}{\mathbf{q}_2^{\top}\mathbf{k}_2}  & \orange\cdots & \orange{\mathbf{q}_2^{\top}\mathbf{k}_L} \\
      & & \orange\ddots & \orange\vdots \\
       &  &  &  \orange\frac{1}{2}{\mathbf{q}_L^{\top}\mathbf{k}_L}  \\   
  \end{array} \right)}_{\hspace{1mm}\scalemath{1.5}{\mathbf{A}^B}} 
  } 
  \end{align}
\end{minipage} }

\scalebox{0.6}{
\begin{minipage}[t]{0.78\textwidth}  
\vspace{-2mm}
\begin{align}
    \hspace{-0.5cm} %
\scalemath{0.52}{ \underbrace{ \left(  \renewcommand*{\arraystretch}{2} \begin{array}{ccccc}
    \dcolor{\mathbf{1}}  & \orange{\boldsymbol{\lambda}_2} & \orange{\boldsymbol{\lambda}_2 \boldsymbol{\lambda}_3}  & \orange{\cdots} & \orange{\boldsymbol{\lambda}_2\cdots\boldsymbol{\lambda}_L} \\
    \blue{\boldsymbol{\lambda}_1} &  \dcolor{\mathbf{1}} & \orange{\boldsymbol{\lambda}_3} & \orange{\cdots} & \orange{\boldsymbol{\lambda}_3 \cdots \boldsymbol{\lambda}_L} \\
    \blue{\boldsymbol{\lambda}_1 \boldsymbol{\lambda}_2} & \blue{\boldsymbol{\lambda}_2} & \dcolor{\mathbf{1}} & \orange{\cdots} & \orange{\boldsymbol{\lambda}_4 \cdots \boldsymbol{\lambda}_L} \\
    \blue\vdots & \blue\vdots & \blue\vdots & \dcolor{\ddots} & \orange \vdots \\
    \blue{{\boldsymbol{\lambda}_{L-1}\cdots \boldsymbol{\lambda}_1}} & \blue{{\boldsymbol{\lambda}_{L-1}\cdots \boldsymbol{\lambda}_2}} & \blue{{\boldsymbol{\lambda}_{L-1}\cdots \boldsymbol{\lambda}_3}} & \blue{\cdots} &   \dcolor{\mathbf{1}} \\   
\end{array}  \right)  }_{\hspace{1mm}\scalemath{2.5}{\mathbf{M} }} 
=
\underbrace{\left( \renewcommand*{\arraystretch}{1.5} \begin{array}{ccccc}
    \blue \mathbf{1} &  &  &  &  \\
    \blue{\boldsymbol{\lambda}_1} & \blue \mathbf{1} &  &  &  \\
    \blue{\boldsymbol{\lambda}_1 \boldsymbol{\lambda}_2} & \blue{\boldsymbol{\lambda}_2} & \blue \mathbf{1} &  &  \\
    \blue{\vdots} & \blue{\vdots} & \blue \vdots & \blue \ddots &  \\
    \blue{{\boldsymbol{\lambda}_{L-1}\cdots \boldsymbol{\lambda}_1}} & \blue{{\boldsymbol{\lambda}_{L-1}\cdots \boldsymbol{\lambda}_2}} & \blue{{\boldsymbol{\lambda}_{L-1}\cdots \boldsymbol{\lambda}_3}} & \blue{\cdots} & \blue \mathbf{1} \\   
\end{array} \right)}_{\hspace{1mm}\scalemath{2.5}{\mathbf{M}^F }} 
+
\underbrace{\left( \renewcommand*{\arraystretch}{1.5} \begin{array}{ccccc}
    \orange{\mathbf{1}}  & \orange{\boldsymbol{\lambda}_2} & \orange{\boldsymbol{\lambda}_2 \boldsymbol{\lambda}_3}  & \orange{\cdots} & \orange{\boldsymbol{\lambda}_2\cdots\boldsymbol{\lambda}_L} \\
     &  \orange{\mathbf{1}} & \orange{\boldsymbol{\lambda}_3} & \orange{\cdots} & \orange{\boldsymbol{\lambda}_3 \cdots \boldsymbol{\lambda}_L} \\
     &  & \orange{\mathbf{1}} & \orange{\cdots} & \orange{\boldsymbol{\lambda}_4 \cdots \boldsymbol{\lambda}_L} \\
     &  &  & \orange{\ddots} & \orange \vdots \\
     &  &  &  &   \orange{\mathbf{1}} \\   
\end{array} \right) }_{\hspace{1mm}\scalemath{2.5}{\mathbf{M}^B }} 
-
\hspace{0.5mm}\scalemath{1.5}{\mathbf{I}} \label{eq:maskdec1}
} 
\end{align}
\end{minipage} }




\looseness=-1As in \cref{eq:attdec} and \cref{eq:maskdec1}, the attention matrix and mask are split into lower (\(\mathbf{A}^F, \mathbf{M}^F\)) and upper triangular (\(\mathbf{A}^B, \mathbf{M}^B\)) matrices. The scaling operator divides each row of the attention matrix to its summed value, and hence %
equals to a diagonal matrix \(\mathbf{C}^{-1}\) multiplied by the attention:
\vspace{-3mm}
\begin{align}
\label{eq:Cintro}
   \mathbf{Y} &= \big(\textsc{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M})\big) \mathbf{V}  \notag \\
   & = (\mathbf{C}^{-1}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M}))\mathbf{V}, \hspace{1mm}
   \mathbf{C}_i = \mathbf{q}^{\top}_i\sum\limits_{j=1}^{L} \mathbf{M}_{ij}\mathbf{k}_j. 
\end{align} 
\vspace{-5mm}

Decomposing \(\mathbf{C}\) into causal and non-causal parts as 
$\mathbf{C}_i = {\mathbf{q}^{\top}_i\sum\nolimits_{j=1}^{i} \mathbf{M}_{ij}\mathbf{k}_j} + {\mathbf{q}^{\top}_i\sum\nolimits_{j=i}^{L} \mathbf{M}_{ij}\mathbf{k}_j} - \mathbf{q}^{\top}_i\mathbf{k}_i$,
we can similarly split the scaling matrix into two parts as follows:
\vspace{-2mm}
\scalebox{0.76}{
\begin{minipage}[t]{0.63\textwidth}  
\begin{align}
\hspace{-3mm}
    \hdiag{\mathbf{C}_{i}} =  \underbrace{\hblue{{\mathbf{q}^{\top}_i\sum\nolimits_{j=1}^{i} \mathbf{M}_{ij}\mathbf{k}_j} - \frac{1}{2}\mathbf{q}^{\top}_i\mathbf{k}_i}}_{\mathbf{C}^F_i} + \underbrace{\horange{\mathbf{q}^{\top}_i\sum\nolimits_{j=i}^{L} \mathbf{M}_{ij}\mathbf{k}_j - \frac{1}{2}\mathbf{q}^{\top}_i\mathbf{k}_i}}_{\mathbf{C}^B_i}
\label{eq:att_rnn_form}
\end{align} 
\end{minipage}}


Therefore, matrix $\mathbf{C}$ can be decomposed into $\mathbf{C} = \mathbf{C}^F + \mathbf{C}^B$.  Since we  have $\mathbf{A} = \mathbf{A}^F + \mathbf{A}^B$ and $\mathbf{M} = \mathbf{M}^F + \mathbf{M}^B - \mathbf{I}$, we can proceed to rewrite the output of the scaled, masked attention as
\vspace{-3mm}

\hspace{-3mm}
\scalebox{0.9}{
\begin{minipage}[t]{0.5\textwidth} 
\vspace{-3mm}
\begin{align}
    & \mathbf{Y} = \big(\textsc{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M})\big) \mathbf{V} = (\mathbf{C}^{-1}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M})) \mathbf{V} = \notag\\
    & (\hblue{\mathbf{C}^F} + \horange{\mathbf{C}^B})^{-1} \big( (\hblue{\mathbf{A}^F}+\horange{\mathbf{A}^B}) \odot  (\hblue{\mathbf{M}^F}+\horange{\mathbf{M}^B} - \mathbf{I}) \big) \mathbf{V} \notag \\
     =& ({\mathbf{C}^F} + {\mathbf{C}^B})^{-1} \big( \mathbf{A}^F\odot\mathbf{M}^F+\mathbf{A}^F\odot\mathbf{M}^B+ \notag \\
    &\mathbf{A}^B\odot\mathbf{M}^F+\mathbf{A}^B\odot\mathbf{M}^B  - \mathbf{A}^F \odot \mathbf{I} - \mathbf{A}^B \odot \mathbf{I}\big) \mathbf{V}. 
\end{align} 
\end{minipage} 
}

Since the forward and backward recurrence matrices ($\mathbf{A}^F,\mathbf{A}^B$ for attention and $\mathbf{M}^F,\mathbf{M}^B$  for mask) only share the diagonal with each other, and the diagonal of both forward and backward recurrence masks consists entirely of ones, we can simplify the above equation as follows:
\vspace{-7mm}

\hspace{-2mm}
\scalebox{0.9}{
\begin{minipage}[t]{0.54\textwidth} 
\begin{align}
 \mathbf{Y} = ({\mathbf{C}^F} & + {\mathbf{C}^B})^{-1} \big( \mathbf{A}^F\odot\mathbf{M}^F+\underbrace{\mathbf{A}^F\odot\mathbf{M}^B}_{\mathbf{A}^F \odot \mathbf{I}}
  +\underbrace{\mathbf{A}^B\odot\mathbf{M}^F}_{\mathbf{A}^B \odot \mathbf{I}} \notag  \\
 & +\mathbf{A}^B\odot\mathbf{M}^B  - \mathbf{A}^F \odot \mathbf{I} - \mathbf{A}^B \odot \mathbf{I}\big) \mathbf{V} \notag =\\
 (\hblue{\mathbf{C}^F} +& \horange{\mathbf{C}^B})^{-1}( \underbrace{\hblue{(\mathbf{A}^F\odot\mathbf{M}^F)\mathbf{V}}}_{\textsc{Forward}}  +\underbrace{\horange{(\mathbf{A}^B\odot\mathbf{M}^B) \mathbf{V}}}_{\textsc{Backward} }). 
\label{eq:backpath} 
\end{align} 
\end{minipage} 
}

As seen from Proposition \ref{prop:ssd}, the \(\hblue{\textsc{Forward}}\) part above can be expressed as an RNN. We now demonstrate that the \(\horange{\textsc{Backward}}\) recurrence term can also be represented by the same RNN in reverse. We re-write the \cref{eq:backpath} by flipping the vector $\mathbf{V}$ as:

\hspace{-3mm}
\scalebox{0.52}{
 \begin{minipage}[t]{0.96\textwidth}  
 \vspace{-5mm}
\begin{align}
\label{backattflip}
   \underbrace{\left( \renewcommand*{\arraystretch}{1} \begin{array}{cccc}
      \orange\frac{1}{2}\frac{\mathbf{q}_L^{\top}\mathbf{k}_L}{\mathbf{q}_L^{\top}\mathbf{z}_L} &  &  & \\
      \orange\frac{\mathbf{q}_{L-1}^{\top}\mathbf{k}_{L}}{\mathbf{q}_2^{\top}\mathbf{z}_L} & \orange\frac{1}{2}\frac{\mathbf{q}_{L-1}^{\top}\mathbf{k}_{L-1}}{\mathbf{q}_2^{\top}\mathbf{z}_L}  &  &  \\
      \orange\vdots & \orange\vdots & \orange\ddots & \\
      \orange\frac{\mathbf{q}_1^{\top}\mathbf{k}_L}{\mathbf{q}_1^{\top}\mathbf{z}_L} & \orange\frac{\mathbf{q}_1^{\top}\mathbf{k}_{L-1}}{\mathbf{q}_1^{\top}\mathbf{z}_L} & \orange\cdots & \orange\frac{1}{2}\frac{\mathbf{q}_1^{\top}\mathbf{k}_1}{\mathbf{q}_1^{\top}\mathbf{z}_L}  \\   
  \end{array} \right)}_{\hspace{1mm}\scalemath{1.1}{F(\mathbf{A}^B)}}  \odot 
  \underbrace{\left( \renewcommand*{\arraystretch}{1} \begin{array}{ccccc}
    \orange{\mathbf{1}}  &  &  &  & \\
     \orange{\boldsymbol{\lambda}_L} &  \orange{\mathbf{1}} &  &  &  \\
     \orange{\boldsymbol{\lambda}_L} \orange{\boldsymbol{\lambda}_{L-1}}& \orange{\boldsymbol{\lambda}_{L-1}} & \orange{\mathbf{1}} &  &  \\
      \orange \vdots & \orange \vdots & \orange \vdots & \orange{\ddots} &  \\
     \orange{\boldsymbol{\lambda}_{L} \cdots \boldsymbol{\lambda}_2}& \orange{\boldsymbol{\lambda}_L \cdots \boldsymbol{\lambda}_3} & \orange{\boldsymbol{\lambda}_L \cdots \boldsymbol{\lambda}_4} & \orange \cdots &   \orange{\mathbf{1}} \\   
\end{array} \right)}_{\hspace{1mm}\scalemath{1.1}{F(\mathbf{M}^B)}} 
 \left( \renewcommand*{\arraystretch}{1} \begin{array}{c}
    \mathbf{v}_L^\top \\  
    \mathbf{v}_{L-1}^\top \\
    \mathbf{v}_{L-2}^\top \\  
    \vdots \\
    \mathbf{v}_1^\top \\   
  \end{array} \right) 
\end{align} 

\end{minipage}
}

The equations above are the exact representations for the forward pass, as shown in \cref{eq:backpath}, but with the tokens in reverse order. The matrices \(\mathbf{A}^B\) and \(\mathbf{M}^B\) are also modified to match the final flipped output using flipped input values $\mathbf{V}$
using functions $F(\mathbf{X})=\mathbf{J}_L\mathbf{X}\mathbf{J}_L$ and $\text{FLIP}(\mathbf{X})=\mathbf{J}_L\mathbf{X}$, where $\mathbf{J}_L$ is an $L$-dimensional exchange matrix, as detailed in Appendix \ref{ap:flip}.
Thus, the outputs of the forward and backward recurrences can be expressed as follows:
\begin{equation}
\label{eq:lastlast}
  \mathbf{Y}  = ({\mathbf{C}^F} + {\mathbf{C}^B})^{-1} (\hblue{\mathbf{Y}^F} + \horange{\mathbf{Y}^B} ), \text{where}\vspace{-8mm}
\end{equation}

\hspace{-5mm}
\scalebox{0.82}{
\begin{minipage}[t]{0.62\textwidth}  
\begin{align}
  \hblue{\mathbf{Y}^F} &= (\mathbf{A}^F \odot \mathbf{M}^F) \mathbf{V}  , \\
  \horange{\mathbf{Y}^B} &= (\mathbf{A}^B \odot \mathbf{M}^B) \mathbf{V} = \textsc{Flip} \Big( \big( F(\mathbf{A}^B) \odot F(\mathbf{M}^B) \big) \textsc{Flip} (\mathbf{V}) \Big).
\end{align}
\end{minipage} }


\begin{theorem} \label{sec:theor} (\lion)
Since \eqref{eq:lastlast} is the parallel form of the recurrence presented in \eqref{prop:ssd}, we can therefore express the equivalent recurrence for the scaled attention as follows:


\begin{tcolorbox}[colback=gray!2!white, colframe=black, boxrule=0.2mm, arc=4mm ,boxsep=0.1pt, left=0.0pt, right=0pt, top=0pt, bottom=0pt]
\resizebox{1\textwidth}{!}{
\begin{minipage}[t]{.63\textwidth}
\begin{subequations}%
\label{eq:bestrnn}
    \begin{align}
      & {\mathbf{S}_i^{F/B}}   = \textcolor{black}{{\lambda}_i} {\mathbf{S}^{F/B}_{i-1}} + \mathbf{k}_i \mathbf{v}_i^{\top},  \\
      & {\mathbf{z}^{F/B}_i}  =  \lambda_i{\mathbf{z}^{F/B}_{i-1}} +  \mathbf{k}_i, \\
      & {c^{F/B}_i} =  {{{\mathbf{q}_i}}^{\top} {\mathbf{z}^{F/B}_{i}}} - \frac{{{\mathbf{q}_i}}^{\top}\mathbf{k}_i}{2} ,  \\
      \notag 
    \end{align}
\end{subequations}
\end{minipage}%
\begin{minipage}[t]{0.7\textwidth}
\begin{subequations}
\label{eq:bestrnn2}
        \begin{align}  
        &  {\mathbf{y}^{F/B}_i} = {{{\mathbf{q}_i}}^{\top} {\mathbf{S}^{F/B}_i}}  - \frac{{{\mathbf{q}_i}}^{\top} \mathbf{k}_i}{2} \mathbf{v}_i, \\
      &  \textsc{Output:} \hspace{2mm} \mathbf{y}_i  = \frac{\mathbf{y}^{F}_i + \mathbf{y}^{B}_i}{c^F_i + c^B_i } 
    \end{align}
\end{subequations}
\end{minipage} }
\end{tcolorbox}

The terms \(\frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i\) and \(\frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i\) are subtracted to avoid double counting. This bi-directional RNN is equivalent to scaled and masked linear attention, represented as \(\mathbf{Y} = \big(\textsc{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M})\big) \mathbf{V}\). 
\end{theorem}

\looseness=-1Many Linear Transformers can be generalized within this framework (see Appendix \ref{sec:map}), where multiple causal recurrent models we adapt to the bidirectional setting. The three main examples, based on the choice of decay parameter $\lambda_i$, are:
\begin{itemize}
    \item \colorbox{green!10}{\lionlit} for $\lambda_i=1$ which is bi-directional form of LinearTrans \cite{trans_rnn}.
     \item \colorbox{ violet!20}{\lionretnet} for $\lambda_i=\lambda$ fixed decay, and bi-directional form of RetNet \cite{retnet}.
      \item \colorbox{orange!17}{\lions} for $\lambda_i=\sigma(\mathbf{W\mathbf{x}_i})$ being input dependent, and bi-directional Linear Transformer inspired by selectivity of Mamba2 \cite{mamba2}.
\end{itemize}


\textbf{Details of Memory and Speed of our Bi-directional RNN:} 
1. \textit{Efficient Memory Usage}: Only the states \(c^{F/B}_i\) and \(\mathbf{y}^{F/B}_i\) are stored per token, resulting in \(\mathcal{O}(Ld)\) memory usage. In contrast, naively storing full matrix-valued hidden states would require \(\mathcal{O}(Ld^2)\), which becomes infeasible for large models.

2. \textit{Parallel Processing:} Forward and backward recurrences run independently, completing in \(L\) time steps with \(L\) memory units, compared to \(2L\) in the naive approach (see Appendix \ref{ap:memoryall})


\vspace{-3mm}
\subsection{Stable and Fast Implementation of Attention Masks}\label{Method2}

As \(\mathbf{Y} = \textsc{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M})\mathbf{V}\) represents the parallel form for training with the mask \(\mathbf{M}\), it is crucial that:  
\((i)\) The mask is created fast during training for models trained using \lion, particularly \lions and \lionretnet.  
\((ii)\) The mask ensures stable training with full attention. Below we describe details for selective and fixed decay masks:

\textbf{Stability and implementation of selective mask (\colorbox{orange!17}{\lions})}: 
Observing from \cref{eq:maskdec1}, the upper (\(\mathbf{M}^B\)) and lower (\(\mathbf{M}^F\)) triangular parts of the mask \(\mathbf{M}\) are rank-1 semi-separable matrices (proof in Appendix \ref{ap:rankmask}), enabling efficient computation via matrix multiplications. During training, the decay factors \(\lambda_i\) are stacked into \(\boldsymbol{\lambda}^F \in \mathbb{R}^L\), and the cumulative product \(\mathbf{L}^F = \cumprod(\boldsymbol{\lambda}^F) = \Pi^i_{k=0}\boldsymbol{\lambda}^F_k\) is used to generate the lower triangular mask \(\mathbf{M}^F\). For the upper triangular mask \(\mathbf{M}^B\), the input sequence is flipped, and the decay factors are computed as \(\boldsymbol{\lambda}^B = \textsc{Flip}(\boldsymbol{\lambda}^F)\), with \(\mathbf{L}^B = \cumprod(\boldsymbol{\lambda}^B)\). The masks are then calculated as:  
\[
\mathbf{M}^F = \text{Tril} (\mathbf{L}^F \hspace{1mm} \frac{1}{{\mathbf{L}^F}^\top}), \quad \mathbf{M}^B = \text{Triu}(\mathbf{L}^B \hspace{1mm} \frac{1}{{\mathbf{L}^B}^\top}),
\]
where $\text{Tril}(\mathbf{X})$ and $\text{Triu}(\mathbf{X})$ only output the lower and upper triangular part of the matrix $\mathbf{X}$, respectively. The full mask is then obtained using $\mathbf{M} = \mathbf{M}^F + \mathbf{M}^B - \mathbf{I}$ as shown in Equation \cref{eq:maskdec1}. To improve numerical stability, the selective scalar \(\lambda_i\) is designed in exponential form \(\lambda_i = e^{a_i}\)(coming from Zero-Order Hold (ZOH) discretization, see \cref{ap:zoh}). This results in the cumulative sum:
\[
\mathbf{D}^F_{ij} = 
\begin{cases} 
\sum_{k=i}^{j+1}{a_k} & \text{if } i > j,  \\
\sum_{k=i+1}^{j}{a_k} & \text{if } i < j,  \\
0 & \text{if } i = j,
\end{cases}
\hspace{5mm} \mathbf{M^F} = \exp(\mathbf{D^F}),
\]
where \(\exp(\cdot)\) is applied element-wise, the same process holds for \(\mathbf{M}^B\) by flipping the input sequence order. Here, \(\mathbf{D}^{F/B} = \cumsum(\mathbf{a}^{F/B})\), with \(\mathbf{a} \in \mathbb{R}^L\) containing the selective exponents \(a_i\). 

However, ensuring stability remains critical, as \(\mathbf{L}^{F/B}\) can overflow or underflow when forming the full mask without chunking \cite{mamba2}. To address this, we define \(a_i = \log(\sigma(\mathbf{W}_{a}^\top\mathbf{x}_i + b))\), where \(\sigma\) is the sigmoid function. This approach ensures stability by bounding \(a_i\) within the interval \([0,1]\) \cite{lru}. An ablation study on alternative activations, such as Mamba's activation, is provided in Appendix \ref{laasjdhakjsdh}.

\looseness=-1 \textbf{Stability and implementation of fixed mask (\colorbox{ violet!20}{\lionretnet})}: By fixing $\lambda_i = \lambda$, the mask $\mathbf{M}$ has the form:
\begin{align}
    \mathbf{M}_{ij} = \lambda^{|i-j|}, \quad \mathbf{D}_{ij} = |i-j|\log(\lambda) .
\end{align}
$\mathbf{M}$ above is a Toeplitz mask \cite{tnn} and therefore, creating the decay mask can be made even faster using simple PyTorch commands, as demonstrated in Appendix \ref{subsec:code}, where implementations for both selective and fixed masks are provided. Similar to \lions mask, we bound the parameter \(\lambda = \sigma(a)\) using the sigmoid function. Finally, the positive activation function for all models tested in \lion is \(\phi(\mathbf{x}) = \frac{\textsc{SiLU}(\mathbf{x+0.5})}{||\textsc{SiLU}(\mathbf{x+0.5})||}\) \cite{henry2020query}. 

\subsection{\lion Chunk: Chunkwise Parallel form of Full Linear Attention} \label{sec:chunk}

Chunking Full Linear Attention is simpler than for causal Linear Attention since there is no intra-chunk. Considering the chunks for queries, keys and values as $\mathbf{Q}_{[i]},\mathbf{K}_{[i]},\mathbf{V}_{[i]} \in \R^{C\times d}$ with chunk size being $C$ and total number of $N=\frac{L}{C}$ chunks, we can chunk the full Linear Attention as:

\begin{tcolorbox}[colback=gray!2!white, colframe=black, boxrule=0.2mm, arc=4mm ,boxsep=0.1pt, left=0.0pt, right=2pt, top=0pt, bottom=0pt]
\begin{align} \label{chunklion}
    \mathbf{A}_{[ij]} & = \mathbf{Q}_{[i]}\mathbf{K}_{[j]}^\top \odot \mathbf{M}_{[ij]}, \\ \mathbf{C}_{[ij]} &= \mathbf{C}_{[ij-1]} + \text{Sum} (\mathbf{A}_{[ij]}), \\
     \mathbf{S}_{[ij]} & =\mathbf{S}_{[ij-1]} + \mathbf{A}_{[ij]} \mathbf{V}_{[j]} , \quad \mathbf{Y}_{[i]} = \frac{\mathbf{S}_{[iN]}}{\mathbf{C}_{[iN]}}
\end{align}
\end{tcolorbox}

where \text{Sum} operations applies summation over the row of the input matrix. The chunk hidden states \(\mathbf{C}_{[ij]}\) and \(\mathbf{S}_{[ij]}\) iterate over \(j\), with the final output for chunk \(i\) computed using their last values at \(j = N\). The chunk mask \(\mathbf{M}_{[ij]}\) corresponds to a submatrix of the full attention mask from \cref{eq:maskdec1}, defined as:
\[
\mathbf{M}_{[ij]} = \mathbf{M}_{iC+1:i(C+1),jC+1:j(C+1)} \in \mathbb{R}^{C \times C}.
\]
For further visualizations and details, refer to \cref{detailchunk}. Below, we construct both fixed and selective chunked masks $\mathbf{M}_{[ij]}$. For the fixed mask, we have:  
\scalebox{0.8}{
\begin{minipage}[t]{0.6\textwidth}
\begin{align}
\mathbf{M}_{[ij]} = 
\begin{cases} 
\lambda^{|i-j|} (\frac{1}{\mathbf{L}}\hspace{1mm}\mathbf{L}^\top)& \text{if } i>j,  \\
\lambda^{|i-j|} (\mathbf{L}\hspace{1mm}\frac{1}{\mathbf{L}^\top}) & \text{if } i<j,  \\
\mathbf{\Gamma} & \text{if } i = j,
\end{cases}
\quad \mathbf{L}_i = \lambda^i, \quad \mathbf{\Gamma}_{ij}  = \lambda^{|i-j|}
\end{align}
\end{minipage}}

with $\mathbf{L} \in R^C$ and $\mathbf{\Gamma} \in \R^{C\times C}$ being the vector and matrix used for creating the mask $\mathbf{M}_{[ij]}$ and they are only depending on the decay parameter $\lambda$ and the chunk size $C$. For the selective mask we have:
\scalebox{0.9}{
\begin{minipage}[t]{0.5\textwidth}
\begin{align}
\mathbf{M}_{[ij]} &= 
\begin{cases} 
\mathbf{L}^F_{[i]} \frac{1}{\mathbf{L}^F_{[j]}}^\top & \text{if } i>j,  \\
\mathbf{L}^B_{[j]} \frac{1}{\mathbf{L}^B_{[i]}}^\top & \text{if } i<j,  \\
\text{Tril}\left(\mathbf{L}^F_{[i]} \frac{1}{\mathbf{L}^F_{[i]}}^\top\right) + \text{Triu}\left(\mathbf{L}^B_{[i]} \frac{1}{\mathbf{L}^B_{[i]}}^\top\right) - \mathbf{I} & \text{if } i = j, 
\end{cases} 
\notag \\
\notag \\
\mathbf{L}^F_{[i]} = &\cumprod(\mathbf{\lambda^F})_{iC+1:(i+1)C}, \\
\mathbf{L}^B_{[i]} = &\cumprod(\mathbf{\lambda^B})_{iC+1:(i+1)C}.
\end{align}
\end{minipage}}

whe $\mathbf{\lambda^F}, \mathbf{\lambda^B}$ are the vectors containing all decay parameters for forward and backward directions as defined in Section \ref{Method2}.  The chunkwise formulation in \cref{chunklion} can be further parallelized over the index \(i\). Since bidirectional sequence modeling requires storing the output for each token, the memory overhead remains subquadratic at \(\mathcal{O}(LC)\) by using parallelism over $i$, while reducing sequential operations during inference. We adopt this as the default setting for \lion's chunkwise mode.








