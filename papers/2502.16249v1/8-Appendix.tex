
\appendix
\onecolumn

\section*{Appendix}

In and this following sections we include additional experiments, further insights on related work, proofs, and theoretical details. The sections are organized as follows:
\begin{itemize}
   \item In \cref{subsec:causal_lm}, we provide additional experiments on causal language modeling.
   \item In \cref{sec:detailrelatedwork}, we include an extension of related work.
   \item In \cref{app:proofs}, we present proofs and theoretical details.
   \item In \cref{sec:app_experiments}, we explore ablation studies and parameter configurations for LRA, masked language Modeling,  and Image Classification tasks.
\end{itemize}



\section{Causal Language Modelling}
\label{subsec:causal_lm}
Efficient causal language modelling with linearized attention was previously studied by \citet{trans_rnn} and \citet{retnet}. In this setup, our formulation becomes similar to the retentive network \citep{retnet}, with the difference that \citet{retnet} choose their selective parameters before training and keep them fixed, while our selective parameters  are trained jointly with the rest of the model. 

We evaluate the performance of our formulation against the GPT-2 architecture \citep{radford2019language} and its linearized version obtained by simply removing the softmax (LinAtt). Our architectures \lions{} is trained with a trainable linear layer to obtain input-dependent selectivity, i.e., $a_i = \log(\sigma(\mathbf{W}_{a}\mathbf{x}_i + b))$. Note that our model do not use absolute positional encodings. We train our models in the OpenWebText corpus \citep{Gokaslan2019OpenWeb}. We evaluate the architectures in the 124 M parameter setup. Our implementation is based on nanoGPT\footnote{\url{https://github.com/karpathy/nanoGPT}}. We use the default GPT-2 hyperparameters and train our models for $8$ days in $4$ NVIDIA A100 SXM4 40 GB GPUs.

\begin{figure}[h]
    \centering
    \subfloat[OpenWebText PPL]{
        \raisebox{60pt}{\begin{tabular}{c|c}
            \toprule
            Model & Perplexity \\
            \midrule
            GPT-2 & $17.42_{(\pm 1.11)}$\\
            LinAtt & $21.07_{(\pm 1.32)}$\\
            \midrule 
            \rowcolor{orange!17} \lions (1D) & $18.16_{(\pm 1.16)}$\\
            \bottomrule
        \end{tabular}}} %
    \subfloat[Perplexity vs. sequence length]{
        \includegraphics[width=0.5\textwidth]{figs/ppl_vs_sequence_length_owt.pdf}}
    \caption{\textit{Causal Language Modelling results in the GPT-2 128M size.} \textit{(a)} Perplexity in the OpenWebText dataset. \textit{(b)} Perplexity vs. sequence length in OpenWebText. Our models improve over the LinAtt baseline \citep{trans_rnn} while obtaining similar performance to the GPT baseline and being able to extrapolate to larger context lengths than the one used during training.}
    \label{fig:owt}
\end{figure}

\begin{figure}
    \subfloat[Latency]{\includegraphics[width=0.5\linewidth]{figs/latency_vs_sequence_length_owt.pdf}}
    \subfloat[Memory]{\includegraphics[width=0.5\linewidth]{figs/memory_vs_sequence_length_owt.pdf}}
    \caption{\textit{Efficiency of the \lions(1D){} framework in the next-token generation task.} In \textit{(a)} and \textit{(b)} we measure respectively the latency and memory to generate the next token in the sentence. We compare three generation modes: Attention, Attention with KV cache and the Recurrence formulation. While all three produce the same output, the Recurrence formulation is the most efficient, requiring constant memory and latency to generate the next token.}
    \label{fig:memory_latency_LM}
\end{figure}

In \cref{fig:owt} we can observe \lions(1D){} significantly improve over the LinAtt baseline, while obtain perplexity close to GPT-2. The lack of absolute positional encodings allows \lions(1D){} to scale to larger sequence lengths than the one used during training. 

In \cref{fig:memory_latency_LM} we evaluate the latency and memory of \lions(1D){} in three modes: Attention, Attention + KV cache and RNN. While the three modes have the same output, the RNN formulation allows to save computation from previous token generations to require constant memory and latency for generating the next token. Our results align with the findings of \citet{retnet}, showing that efficient models in training and inference, with a strong performance (up to a small degradation) can be obtained.

\section{Detailed related work}
\label{sec:detailrelatedwork}

\subsection{State Space Models and Transformers}

State Space Models, such as S4 \citep{gu2021efficiently} and S5 \citep{s5}, advanced efficient sequence modeling with linear complexity. 
Mamba \citep{mamba} and Mamba-2 \citep{mamba2} introduced selective mechanisms within SSMs, achieving strong language modeling performance. 
Recently, many recurrent models for language have been proposed, e.g., xLSTM \citep{xlstm}, RWKV \citep{peng2024eagle}.
While RNNs for autoregressive modelling are prevalent, bidirectional models are less explored.
Hydra \citep{hwang2024hydrabidirectionalstatespace} extends Mamba to bidirectional settings using quasiseparable matrix mixers.
VisionMamba \citep{zhu2024visionmambaefficientvisual} employs two separate SSMs to pass over images.
However, these works are not equivalent to bidirectional attention.
\lion adopts a different approach: instead of extending SSMs, we derive equivalence between bidirectional attention with learnable mask and bidirectional RNNs.

Since the pioneering works \citep{tsai2019transformer,trans_rnn}, many works have been proposed to enhance linearized attention, including learnable relative positional encoding \citep{dai2019transformer}, gate mechanisms \citep{peng2021random,han2024demystify,ma2022mega}, FFT for kernelized attention \citep{luo2021stable}, decay terms in RetNet \citep{retnet}, and variants with enhanced expressiveness \citep{arora2024simple,zhang2024hedgehogporcupineexpressive,deltanet}.
These works focus on causal attention and cannot be directly applied with bidirectionality, while we explicitly write bidirectional attention as bidirectional RNN combined with selectivity, enhancing performance and providing a principled framework for parallel training and linear-time inference in non-causal tasks.

\subsection{Linear Transformers Summary}
\label{overview_lrmsec}
\label{ap:1}
\begin{table}[t]
\footnotesize
\caption{\textit{Overview of recent Linear Transformers applied to autoregressive language modeling.} The $-$ mark indicates models without scaling, as they lack \(\mathbf{\alpha}_i\) and \(\mathbf{\beta}_i\) and do not scale attention scores. The \(\times\) denotes matrix multiplication, \(\odot\) represents the Hadamard product, and \(*\) signifies the scalar product. All these models are used for autoregressive language modeling.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|cc|c|l}
\toprule
   \multirow{2}{*}{Model}  & \multicolumn{2}{c}{Recurrence Parameters }& \multicolumn{2}{l}{Operations} & \multirow{2}{*}{Scaled}&  \multirow{2}{*}{$\rightleftarrows$}\\
          &$\boldsymbol{\Lambda_i}$ & $\boldsymbol{\gamma_i}$  &   $\tikz\draw[white,fill=black] (0,0) circle (.5ex);$ & $\star$  \\
   \midrule
    Linear Trans\ \citep{trans_rnn}  &$\mathbf{I}$ &1& $\times$ & $\times$ & \checkmark & \xmark \\
    DeltaNet \citep{secretlin} &$\mathbf{I} - \gamma_i\mathbf{k}_i\mathbf{k}_i^{\top}$  & $\mathbf{\gamma_i}$& $*$ & $\times$ & \xmark & \xmark\\
    S4/S5 \citep{gu2021efficiently,s5}  &$\mathbf{e^{(- ({\delta} \mathbf{1}^\intercal)\odot \exp(\boldsymbol{A}))}}$&$\boldsymbol{B}$ & $\odot $ & $\odot $ & \xmark & \xmark\\
    Gated RFA \citep{peng2021random} &$g_i$&$1-g_i$ & $*$  &$*$   & \checkmark & \xmark \\
    RetNet \citep{retnet} &a & 1 & $*$  & $*$  & \xmark & \xmark\\
    Mamba (S6) \citep{mamba}  &$\mathbf{e^{(- ({\delta}_i \mathbf{1}^\intercal)\odot \exp(\boldsymbol{A_i}))}}$ &$\boldsymbol{B}_i$ & $\odot $ & $\odot $ & \xmark & \xmark \\
    GLA \citep{yang2023gated} & \textsc{DIAG}($g_i$)& 1 & $*$  & $\times$ & \xmark & \xmark \\
    RWKV \citep{peng2024eagle} & \textsc{DIAG}($g_i$)& 1 & $*$  & $\times$ & \xmark & \xmark\\
    xLSTM \citep{xlstm} &${f}_i$ &${i}_i$ & $*$  &$*$   & \checkmark 
    & \xmark \\
    Mamba-2 \citep{mamba2} &${a}_i$& 1 & $*$  &$*$   & \xmark & \xmark \\
    \rowcolor{Green!10}
   \textbf{\lionlit (ours)} & 1 & 1& $\odot$  &$*$   & \checkmark & \checkmark \\
   \rowcolor{violet!17}
   \textbf{\lionretnet (ours)} & $\gamma$ & $\gamma$& $\odot$  &$*$   & \checkmark & \checkmark \\
    \rowcolor{orange!17}
   \textbf{\lions (ours)} &${e}^{-{a}_i}$& $1$ & $\odot$  &$*$   & \checkmark & \checkmark \\
    \bottomrule
\end{tabular}
}
\label{table:overview_lrm}
\end{table}






\subsection{{Parallel Training and Efficient Inference}}
For linear transformers, efficient training is ideally achieved either by employing attention parallelization similar to  $    \mathbf{Y} = \text{softmax}\left(\mathbf{Q} \mathbf{K}^\top\right) 
    \mathbf{V}$ 
or by using techniques like parallel scan, as utilized by many structured SSMs (e.g., Mamba, S5) \citep{scanalg}. We will cover both techniques in the following sections.

\textbf{Parallel training in transformers.} As illustrated in equation $    \mathbf{Y} = \text{softmax}\left(\mathbf{Q} \mathbf{K}^\top\right) 
    \mathbf{V}$ of the Transformer, vectorization over the sequence is crucial to avoid sequential operations, where the model iterates over the sequence, leading to extensive training times \citep{vaswani_attention_2017}.
Parallelizing the operations across the sequence length for linear transformers ideally should take a form similar to \citep{trans_rnn,retnet}:
\begin{equation}\label{eq:lrmvec}
    \mathbf{Y} = \mathbf{M} \ast  \left(\mathbf{\phi(Q)} \mathbf{\phi(K)}^\top\right) \mathbf{V} 
\end{equation}
Here, \(\mathbf{M}\) represents a mask generated from the interaction of recurrent model parameters (\(\textcolor{black}{\boldsymbol{\Lambda_i}}\) and \(\textcolor{black}{\boldsymbol{\gamma_i}}\). Attention scores can be scaled before or after applying the mask $\mathbf{M}$ and during inference the scaling can be done by using the scaling state $\boldsymbol{\mathbf{z}}_i$. The symbol \(\ast\) indicates the operation in-which mask is applied to the attention. Equation \eqref{eq:lrmvec} highlights the importance of carefully selecting operations and parameters to ensure parallelizability during training. The mask \(\mathbf{M}\) is a lower diagonal mask in case of autoregressive models \citep{ma2022mega}.


\textbf{Parallel Scan.} Most SSMs utilize the state matrix \(\boldsymbol{\Lambda_i}\) as a full matrix, with the \(\textcolor{black}{\star}\) operation defined as matrix multiplication. Consequently, the output of each layer cannot be represented as in \eqref{eq:lrmvec}. This limitation becomes evident when applying recurrence over the discrete sequence in leading to the output:

\begin{align}
\label{eq:ssm}
\mathbf{h}_i = \mathbf{A}_i\mathbf{h}_{i-1} +\mathbf{B}_i\mathbf{x}_i , \quad
\mathbf{y}_i = \mathbf{C}^\top_i\mathbf{h}_i, \quad
\mathbf{y}_\iter = \boldsymbol{\bar{C}}_{i}^\top \sum_{j=1}^\iter \left( \prod_{k=j+1}^\iter \boldsymbol{\bar{A}}_{k} \right) \boldsymbol{\bar{B}}_{j} \mathbf{x}_j, 
\end{align}

which requires matrix multiplications for \(\boldsymbol{\bar{A}}_{k}\) across all tokens between \(i\) and \(j\), resulting in substantial memory requirements during training.

To mitigate this issue, SSMs adopt the parallel scan approach \citep{scanalg, scan2}, which enables efficient parallelization over sequence length. Initially introduced in S5 \citep{s5}, this method has a time complexity of \(\cO(L \log L)\). However, Mamba \citep{mamba} improves upon this by dividing storage and computation across GPUs, achieving linear scaling of \(\cO(L)\) with respect to sequence length and enabling parallelization over the state dimension \(N\). 
Ideally, a model should achieve complete parallelization in training without sequential operations, maintain a memory requirement for inference independent of token count, and have linear complexity. Table \ref{tab:summ o(n)} summarizes various training and inference strategies, along with their complexity and memory demands. 

Linear Transformers \citep{retnet, trans_rnn} employ attention during training and recurrence during inference, placing them in the last category of Table \ref{tab:summ o(n)}. To our knowledge, an exact mapping between attention and bidirectional recurrence does not exist; thus, naive forward and backward recurrence cannot be theoretically equated to the attention formulations in \eqref{eq:lrmvec} and  $    \mathbf{Y} = \text{softmax}\left(\mathbf{Q} \mathbf{K}^\top\right) 
    \mathbf{V}$. 



\subsection{Different Training strategies for models} \label{trainstrag}
\begin{table}[h]
   \caption{\textit{Summary of training and inference strategies.}
   $\rightleftarrows$ represents bidirectionality of the method.
   \looseness=-1Complexity indicates the computational and memory requirements during inference for processing \(L\) tokens \rebuttal{and $d$ is the model dimension}. \textbf{\lion} (Theorem \textbf{\ref{sec:theor}}) is designed to parallelize training using masked attention while employing recurrence during inference, specifically for bidirectional sequence modeling.} \label{tab:summ o(n)}
    \resizebox{1\textwidth}{!}{
\begin{tabular}{lllllll}
\toprule
   \begin{tabular}{@{}l@{}}  Train \\ Strategy \end{tabular}  &  \begin{tabular}{@{}l@{}}  Inference \\ Strategy \end{tabular}  & \begin{tabular}{@{}l@{}}  Method \\ Instantiations \end{tabular}  & \begin{tabular}{@{}l@{}}  Train sequential \\ operations \end{tabular}  & Complexity  & \begin{tabular}{@{}l@{}}  Inference \\ Memory \end{tabular} & \textbf{$\rightleftarrows$} \\
   \midrule
          Recurrence  & Recurrence & LSTM, GRU & $\cO(L)$ & $\cO(L\rebuttal{d})$ & $\cO(\rebuttal{d})$ & \xmark \\
          Recurrence  & Recurrence & ELMO & $\cO(L)$ & $\cO(L\rebuttal{d})$ & $\cO(L\rebuttal{d})$ & \checkmark \\
          Full Attention  & Full Attention &  Vit, BERT & $\cO(1)$& $\cO(L^2\rebuttal{d^2})$ & $\cO(L^2\rebuttal{d^2})$ & \checkmark \\ 
          Causal Attention & KV Cache & GPT-x, Llama & $\cO(1)$& $\cO(L^2\rebuttal{d^2})$ & $\cO(L\rebuttal{d^2})$ & \xmark \\   
          Causal Attention & Recurrence & LinearTrans, RetNet & $\cO(1)$&  $\cO(L\rebuttal{d^2})$ & $\cO(\rebuttal{d^2})$ & \xmark \\        
          Parallel Scan & Recurrence & Mamba, S4, S5 & $\cO(1)$ & $\cO(L\rebuttal{d})$ & $\cO(\rebuttal{d})$ & \xmark \\ 
          Parallel Scan & Recurrence  & Vim & $\cO(1)$ & $\cO(L\rebuttal{d^2})$ & $\cO(L\rebuttal{d})$ & \checkmark \\ 
          \rowcolor{blue!10}
        \textbf{Full Attention}  & \textbf{Recurrence}(\lion \textbf{\ref{sec:theor}})& \lionlit, \lions, \lionretnet & $\cO(1)$&$\cO(L\rebuttal{d^2})$ & $\cO(L\rebuttal{d})$ & \checkmark  \\  
    \bottomrule
\end{tabular} }
\vspace{-5mm}

\end{table}


\subsection{{Architectural Differences in Autoregressive Linear Transformers}}

\textbf{Multi-head attention and state expansion. }Another difference between various Linear Transformers, particularly SSMs and transformers, is how they expand single-head attention or SSM recurrence to learn different features at each layer, akin to convolutional neurons in CNNs \citep{resnet}. Transformers achieve this through multi-head attention, while SSMs like Mamba and Mamba-2 \citep{mamba, mamba2} use state expansion also known as Single-Input Single-Output (\textit{SISO}) framework to enlarge the hidden state. In \textit{SISO} framework, the input \(\mathbf{x}_{i}\) in  \eqref{eq:ssm} is a scalar and recurrence is applied to all elements in the hidden state independently \citep{s5}, allowing for parallelization during inference and training.

In contrast, simplified SSMs like S5 employ a Multiple-Input Multiple-Output (\textit{MIMO}) approach, where \(\mathbf{x}_{i}\) is a vector, which aligns them more closely with RNN variants like LRU \citep{lru} that are successful in long-range modeling \citep{s5}. However, the \textit{SISO} framework continues to be effective in Mamba models for language modeling \citep{mamba2}.

\textbf{Rule of Positional Encoding.} The parameter \(\textcolor{black}{\boldsymbol{\Lambda_i}}\) serves as a gating mechanism \citep{yang2023gated,mamba} and can also be interpreted as relative positional encoding \citep{retnet}. For instance, in an autoregressive model, considering \(\textcolor{black}{\boldsymbol{\Lambda_i}}\) as scaler, the mask \(\mathbf{M}\) can be defined as follows:
\begin{minipage}[t]{.5\textwidth}
    \vspace{-5mm}
    \begin{align}
    \label{eq:masksel}
    \textsc{\textbf{Se}}&\textsc{\textbf{lective Mask}} \notag \\
     \mathbf{M}_{ij} = &
    \begin{cases} 
    \Pi_{k=i}^{j+1}\lambda_k & i \geq j  \\
    0 & i < j
\end{cases}
    \end{align}
\end{minipage}%
\begin{minipage}[t]{.5\textwidth}
    \vspace{-5mm}
      \begin{align}  
      \label{eq:maskfix}
        \textsc{\textbf{Fi}}&\textsc{\textbf{xed Mask}} \notag  \\
   \mathbf{M}_{ij} = &
    \begin{cases} 
    \lambda^{i-j} & i \geq j  \\
    0 & i < j
\end{cases}
    \end{align}

\end{minipage}

In this context, the selective mask (where \(\textcolor{black}{\boldsymbol{\Lambda_i}} = {\lambda_i}\) varies for each token) is used in architectures like Mamba \citep{mamba}, while the fixed mask (where \(\textcolor{black}{\boldsymbol{\Lambda_i}} = {\lambda}\) is constant across all tokens) is implemented in architectures like RetNet \citep{retnet}. 
In both cases, the mask \(\mathbf{M}_{ij}\) provides rich relative positional encoding between tokens \(i\) and \(j\). Its structure reinforces the multiplication of all \(\boldsymbol{\Lambda_k}\) elements for \(k \in [j, \ldots, i]\), while the selectivity allows the model to disregard noisy tokens, preventing their inclusion in the attention matrix for other tokens. 

In contrast, Linear Transformers such as Linear Transformer \citep{trans_rnn} set \(\boldsymbol{\Lambda_k} = 1\), resulting in \(\mathbf{M}\) functioning as a standard causal mask, similar to those used in generative transformers \citep{gpt}. This necessitates the injection of positional information into the sequence, which is achieved using the traditional positional encoding employed in transformers \citep{vaswani_attention_2017}. In this framework, each element of the input data sequence is represented as \(\mathbf{x}_i = \mathbf{f}_i + \mathbf{t}_i\), where \(\mathbf{f}_i\) denotes the features at time \(i\) and \(\mathbf{t}_i\) represents the positional embedding. However, this traditional positional encoding has been shown to be less informative compared to relative positional encoding \citep{roformer}, which is utilized in other Linear Transformers where \(\boldsymbol{\Lambda_k} \neq 1\).



\subsection{\lion Framework}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figs/frfr.png}
    \caption{(\textit{Left}) Standard Transformer block. (\textit{Middle}) Training mode of \lion{} with  full linear attention. (\textit{Right}) Inference mode of \lion{} in the equivalent bidirectional RNN. 
    \rebuttal{Norm refers to Layer normalization,
    Proj is the projection operation to calculate $\mathbf{Q},\mathbf{K},\mathbf{V}$ and $\boldsymbol{\lambda}$ values,
    Scale is the scaling operation in \cref{equ:linear_attention}, 
    Inv is element wise inverse, 
    $\mathbf{A}$ is the linear attention,
    $\mathbf{M}^{F/B}$ are the forward and backward recurrence masks, $\mathbf{y}^{F/B}$ the outputs, and $c^{F/B}$ the scaling coefficients. We also provide a memory and scalability trade-off at inference time with chunking. 
    }} \vspace{-4mm}
    \label{fig:model}
\end{figure}



\subsection{Memory allocation in \lion during Forward and Backward recurrences} \label{ap:memoryall}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figs/memory.png}
  \caption{\textit{Memory allocation in \lion during Forward and Backward recurrences.} The efficient way of re-using the memory during inference is explained. }
    \label{fig:mem}
\end{figure}

During the forward and backward recurrences, as illustrated in Figure \ref{fig:mem}, each recurrence saves its corresponding output vector for each token, along with the scaling factor \(c\), to generate the final output. Once the backward recurrence reaches a token that the forward recurrence has already passed, it can directly calculate the output \(\mathbf{y}_i\) for that token, as \(c^F_i\) and \(\mathbf{y}^F_i\) have already been computed during the forward pass. Furthermore, the backward recurrence can overwrite the final output in the same memory cell where \(\mathbf{y}^F_i\) was stored, since both outputs share the same dimensions. This approach keeps memory allocation consistent with the forward pass, and the time required to process the sequence remains similar to that of autoregressive models, as both recurrences can traverse the sequence in parallel.

 \subsection{\rebuttal{Zero-Order Hold Discretization} }
\label{ap:zoh}
\rebuttal{Below we explain the zero-order hold discretization derived by \cite{kalman1960new}. An LTI system can be represented with the equation:
\begin{equation}
{\mathbf{h}}'(t) = A\mathbf{h}(t) + B\mathbf{x}(t),
\end{equation}
which can be rearranged to isolate \(\mathbf{h}(t)\):
\begin{equation}
{\mathbf{h}}'(t) - A\mathbf{h}(t) = B\mathbf{x}(t).
\end{equation}
By multiplying the equation by \(e^{-At}\), we get
\begin{equation}
e^{-At} {\mathbf{h}}'(t) - e^{-At}A\mathbf{h}(t) = e^{-At} B \mathbf{x}(t)
\label{eq:zho1}
\end{equation}
Since $\frac{\partial}{\partial t} e^{At} = Ae^{At} = e^{At}A$, \cref{eq:zho1} can be written as:
\begin{equation}
\frac{\partial}{\partial t} \left( e^{-At} \mathbf{h}(t) \right) = e^{-At} B \mathbf{x}(t).
\end{equation}
After integrating both sides and simplifications, we get
\begin{equation}
e^{-At} \mathbf{h}(t) = \int_{0}^{t} e^{-A\tau} B \mathbf{x}(\tau) \, d\tau + \mathbf{h}(0).
\end{equation}
By multiplying both sides by \(e^{At}\) to isolate \(\mathbf{h}(t)\) and performing further simplifications, at the end we get
\begin{equation}
\mathbf{h}(t) = e^{At} \int_{0}^{t} e^{-A\tau} B \mathbf{x}(\tau) \, d\tau + e^{At} \mathbf{h}(0).
\label{eq:zho2}
\end{equation}
To discretize this solution,  we can assume sampling the system at even intervals, i.e. each sample is at $kT$ for some time step $T$, and that the input \textbf{x}(t) is constant between samples. To simplify the notation, we can define $\mathbf{h}_k$ in terms of $\mathbf{h}(kT)$ such that
\begin{equation}
\mathbf{h}_k = \mathbf{h}(kT).
\end{equation}
Using the new notation, \cref{eq:zho2} becomes 
\begin{equation}
\mathbf{h}_k = e^{\mathbf{A}kT} \mathbf{h}(0) + e^{\mathbf{A}kT} \int_0^{kT} e^{-\mathbf{A}\tau} \mathbf{B} \mathbf{x}(\tau) \, d\tau.
\end{equation}
Now we want to express the system in the form:
\begin{equation}
\mathbf{h}_{k+1} = \mathbf{\tilde{A}} \mathbf{h}_k + \mathbf{\tilde{B}} \mathbf{x}_k.
\end{equation}
To start, letâ€™s write out the equation for \(\mathbf{x}_{k+1}\) as
\begin{equation}
\mathbf{h}_{k+1} = e^{\mathbf{A}(k+1)T} \mathbf{h}(0) + e^{\mathbf{A}(k+1)T} \int_0^{(k+1)T} e^{-\mathbf{A}\tau} \mathbf{B} \mathbf{x}(\tau) \, d\tau.
\label{eq:zho3}
\end{equation}
After multiplying by \(e^{\mathbf{A}T}\) and rearranging we get
\begin{equation}
e^{\mathbf{A}(k+1)T} \mathbf{h}(0) = e^{\mathbf{A}T} \mathbf{h}_k - e^{\mathbf{A}(k+1)T} \int_0^{kT} e^{-\mathbf{A}\tau} \mathbf{B}\mathbf{x}(\tau) \, d\tau.
\end{equation}
Plugging this expression for \(\mathbf{x}_{k+1}\) in \cref{eq:zho3} yields to
\begin{equation}
\mathbf{h}_{k+1} = e^{\mathbf{A}T} \mathbf{h}_k - e^{\mathbf{A}(k+1)T} \left( \int_0^{kT} e^{-\mathbf{A}\tau} \mathbf{B}\mathbf{x}(\tau) \, d\tau + \int_0^{(k+1)T} e^{-\mathbf{A}\tau} \mathbf{B}\mathbf{x}(\tau) \, d\tau \right),
\end{equation}
which can be further simplified to
\begin{equation}
\mathbf{h}_{k+1} = e^{\mathbf{A}T} \mathbf{h}_k - e^{\mathbf{A}(k+1)T} \int_{kT}^{(k+1)T} e^{-\mathbf{A}\tau} \mathbf{B}\mathbf{x}(\tau) \, d\tau.
\end{equation}
Now, assuming that \(\mathbf{x}(t)\) is constant on the interval \([kT, (k+1)T)\), which allows us to take \(\mathbf{B}\mathbf{x}(t)\) outside the integral. Moreover, by bringing the \(e^{\mathbf{A}(k+1)T}\) term inside the integral we have
\begin{equation}
\mathbf{h}_{k+1} = e^{\mathbf{A}T} \mathbf{h}_k - \int_{kT}^{(k+1)T} e^{\mathbf{A}((k+1)T - \tau)} \, d\tau \, \mathbf{B}\mathbf{x}_k.
\end{equation}
Using a change of variables \(v = (k+1)T - \tau\), with \(d\tau = -dv\), and reversing the integration bounds results in
\begin{equation}
\mathbf{h}_{k+1} = e^{\mathbf{A}T} \mathbf{h}_k + \int_0^T e^{\mathbf{A}v} \, dv \, \mathbf{B}\mathbf{x}_k.
\end{equation}
Finally, if we evaluate the integral by noting that \(\frac{d}{dt} e^{\mathbf{A}t} = \mathbf{A} e^{\mathbf{A}t}\) and assuming \(\mathbf{A}\) is invertible, we get
\begin{equation}
\mathbf{h}_{k+1} = e^{\mathbf{A}T} \mathbf{h}_k + \mathbf{A}^{-1} \left( e^{\mathbf{A}T} - \mathbf{I} \right) \mathbf{B}\mathbf{x}_k.
\end{equation}
Thus, we find the discrete-time state and input matrices:
\begin{equation}
\mathbf{\tilde{A}} = e^{\mathbf{A}T}
\end{equation}
\begin{equation}
\mathbf{\tilde{B}} = \mathbf{A}^{-1} \left( e^{\mathbf{A}T} - \mathbf{I} \right) \mathbf{B}.
\end{equation}
And the final desecrate state space representation is:
\begin{equation}
\mathbf{h_k} = e^{\mathbf{A}T}\mathbf{h}_{k-1} +  \mathbf{A}^{-1} \left( e^{\mathbf{A}T} - \mathbf{I} \right) \mathbf{B}_k\mathbf{x}_k.
\end{equation}
As in case of \lions (similar to choice of Mamba2 \cite{mamba2}) the matrix $\mathbf{A}$ is identity while the time step $T$ is selective and equal to $a_i$. And simply for \lions scenario the term $Bx(t)$ will change into $\mathbf{k}_i\mathbf{v}^\top_i$ therefor considering Linear Transformer as continuous system like:
\begin{align}
\label{eq:recurscale22}
    \mathbf{S}'_{(t)} &=  \mathbf{S}_{(t)} + \mathbf{k}_{(t)}\mathbf{v}_{(t)}^{\top}, \\
    \mathbf{z}_{(t)} & = \mathbf{z}_{(t)} + {\mathbf{k}_{(t)}}, \\
\end{align}
By applying the ZOH discritization the final descreate \lions will be equal to:
 \begin{align}  
    \hspace{4mm}  & \hspace{-0.4cm}  \textsc{Discrete } \notag \\
      & \hspace{-0.4cm}  \mathbf{S}_{i} = e^{a_i}\mathbf{S}_{i-1}+ (e^{a_i}-1)\mathbf{k}_{i}\mathbf{v}_{i}^{\top},  \\
     &\hspace{-0.4cm}  \mathbf{z}_{i} =  e^{a_i}\mathbf{z}_{i-1} + (e^{a_i}-1)\mathbf{k}_{i}, 
    \end{align}
    And it applies to both directions forward and backward.
}

\section{Proofs}
\label{app:proofs}

\subsection{Proof of \Cref{prop:ssd}: Duality between Linear Recurrence and Attention}
Considering the following recurrence: \label{sec:proofqtk}

\begin{align}
\label{eq:recurscale2}
    \mathbf{S}_i &= \textcolor{black}{{\lambda_i}} \mathbf{S}_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{z}_i & = \textcolor{black}{{\lambda_i}} \mathbf{z}_{i-1} + {\mathbf{k}_i}, \\
    \hspace{4mm} \textsc{S}&\textsc{caled}: \mathbf{y}_i= \frac{{{\mathbf{q}_i}}^{\top} \mathbf{S}_i}{{{\mathbf{q}_i}}^{\top} \mathbf{z_i}} 
\end{align}

We can calculate each output $\mathbf{y}_i$ recursively as below: 

\scalebox{0.82}{
\begin{minipage}[t]{1.2\textwidth}  
\begin{align}
    &\mathbf{S}_1 = \mathbf{k}_1 \mathbf{v}_1^{\top},  \hspace{2mm} \mathbf{z}_1 = \mathbf{k}_1, \hspace{2mm} \mathbf{y}_1 = \mathbf{v}_1 \\
    &\mathbf{S}_2 = \mathbf{k}_2 \mathbf{v}_2^{\top}+{\lambda}_1\mathbf{k}_1 \mathbf{v}_1^{\top},  \hspace{2mm} \mathbf{z}_2 = \mathbf{k}_2+{\lambda}_1\mathbf{k}_1, \hspace{2mm}  \mathbf{y}_2= \frac{{{\mathbf{q}_2}}^{\top} (\mathbf{k}_2 \mathbf{v}_2^{\top}+{\lambda}_1\mathbf{k}_1\mathbf{v}^{\top}_1)}{{{\mathbf{q}_2}}^{\top} (\mathbf{k}_2+{\lambda}_1\mathbf{k}_1)} \\
    &\mathbf{S}_3 = \mathbf{k}_3 \mathbf{v}_3^{\top}+{\lambda}_1\mathbf{k}_2 \mathbf{v}_2^{\top}+ {\lambda}_2{\lambda}_1\mathbf{k}_1 \mathbf{v}_1^{\top},  \hspace{2mm} \mathbf{z}_3 = \mathbf{k}_3+{\lambda}_1\mathbf{k}_2+{\lambda}_2{\lambda}_1\mathbf{k}_1, \hspace{2mm}  \mathbf{y}_3= \frac{\mathbf{q}^{\top}_3(\mathbf{k}_3 \mathbf{v}_3^{\top}+{\lambda}_1\mathbf{k}_2 \mathbf{v}_2^{\top}+ {\lambda}_2{\lambda}_1\mathbf{k}_1 \mathbf{v}_1^{\top})}{\mathbf{q}^{\top}_3(\mathbf{k}_3+{\lambda}_1\mathbf{k}_2+{\lambda}_2{\lambda}_1\mathbf{k}_1)} \\
    &\Rightarrow \mathbf{y}_i = \frac{\mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{M}^C_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)}{\mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{M}^C_{ij}\mathbf{k}_j)},  \hspace{5mm}   \mathbf{M}^C_{ij} = 
    \begin{cases} 
    \Pi_{k=i}^{j+1}{\lambda_k} & i \geq j  \\
    0 & i < j
\end{cases}
\end{align}
\end{minipage}
}

This can be shown in a vectorized form as:
\begin{align}
   \mathbf{Y} = \textsc{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M}^C) \mathbf{V}
\end{align}
Where \textsc{scale} is the scaling function which scaled the attention matrix with respect to each row or can also be written as:
\begin{align}
   \textsc{scale}(\mathbf{A})_{ij} = \frac{\mathbf{A}_{ij}}{\sum_{j=1}^L\mathbf{A}_{ij}}
\end{align}

Similarly if the $\textsc{scale}$ is applied before masking we have:
\begin{align}
\label{eq:scalecm}
   \mathbf{Y} = \big(\textsc{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M}_{\textsc{causal}})\odot \mathbf{M}\big) \mathbf{V}
\end{align}

With $\mathbf{M}_{\textsc{causal}}$ being the causal mask used in autoregressive models \citep{gpt}. This vectorized form is equivalent to:

\begin{minipage}[t]{1\textwidth}  
\begin{align}
\label{eq:yscale}
\mathbf{y}_i = \frac{\mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{M}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)}{\mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{k}_j)},  \hspace{5mm}   \mathbf{M}_{ij} = 
    \begin{cases} 
    \Pi_{k=i}^{j+1}{\lambda_k} & i \geq j  \\
    0 & i < j
\end{cases}
\end{align}
\end{minipage}

And the recurrence for this vectorized form can be written as:

\begin{align}
\label{eq:recurscaleafter}
    \mathbf{S}_i &= \textcolor{black}{{\lambda_i}} \mathbf{S}_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{z}_i & =  \mathbf{z}_{i-1} + {\mathbf{k}_i}, \\
    \hspace{4mm} \textsc{S}&\textsc{caled}: \mathbf{y}_i= \frac{{{\mathbf{q}_i}}^{\top} \mathbf{S}_i}{{{\mathbf{q}_i}}^{\top} \mathbf{z_i}} \label{eq:scaling_zi}
\end{align}

\subsection{Forward and Backward Recurrences Theoretical Details}

Considering the following recurrence:

\begin{align}
    \mathbf{S}_i &= \textcolor{black}{{\lambda_i}} \mathbf{S}_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{z}_L & = \sum^{L}_{i=1} \mathbf{k}_i \\
    \mathbf{y}_i &= \frac{{{\mathbf{q}_i}}^{\top} \mathbf{S}_i}{{{\mathbf{q}_i}}^{\top} \mathbf{z_L}} 
    \label{eq:forzl}
\end{align}

This recurrence is the same as recurrence \eqref{eq:recurscaleafter} but with $\mathbf{z}_L$ being fixed to the summation of all keys in the sequence, therefor the output $\mathbf{y}_i$ can simply be written as:

\begin{minipage}[t]{1\textwidth}  
\begin{align}
\label{eq:dumyrec}
\mathbf{y}_i = \frac{\mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{M}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)}{\mathbf{q}^{\top}_i\mathbf{z}_L},  \hspace{5mm}   \mathbf{M}_{ij} = 
    \begin{cases} 
    \Pi_{k=i}^{j+1}{\lambda_k} & i \geq j  \\
    0 & i < j
\end{cases}
\end{align}
\end{minipage}

By replacing the $\mathbf{z}_i = \sum_{j=1}^i \mathbf{k}_j$ in the denominator of equation \eqref{eq:scaling_zi} with $\mathbf{z}_L$. Therefore in vectorized form, it will become:

\begin{align}
   \mathbf{Y} = (\mathbf{A}^{C}  \odot \mathbf{M}\big) \mathbf{V}
\end{align}

With $\mathbf{A}^{C}$ being:

\begin{center}
\scalebox{0.75}{
\begin{minipage}[t]{1.25\textwidth}  
\begin{align*}   
 \mathbf{A}^{C} = \left( \renewcommand*{\arraystretch}{1} \begin{array}{cccc}
       {\frac{\mathbf{q}_1^{\top}\mathbf{k}_1}{\mathbf{q}_1^{\top}\mathbf{z}_L}} &  &  &  \\
      {\frac{\mathbf{q}_2^{\top}\mathbf{k}_1}{\mathbf{q}_2^{\top}\mathbf{z}_L}} & {\frac{\mathbf{q}_2^{\top}\mathbf{k}_2}{\mathbf{q}_2^{\top}\mathbf{z}_L}} &  &  \\
      {\frac{\mathbf{q}_3^{\top}\mathbf{k}_1}{\mathbf{q}_3^{\top}\mathbf{z}_L}} & {\frac{\mathbf{q}_3^{\top}\mathbf{k}_2}{\mathbf{q}_3^{\top}\mathbf{z}_L}} &  {\frac{\mathbf{q}_3^{\top}\mathbf{k}_3}{\mathbf{q}_3^{\top}\mathbf{z}_L}} &  \\
      \vdots & \vdots & \vdots & \ddots \\
      {\frac{\mathbf{q}_L^{\top}\mathbf{k}_1}{\mathbf{q}_L^{\top}\mathbf{z}_L}} & {\frac{\mathbf{q}_L^{\top}\mathbf{k}_2}{\mathbf{q}_L^{\top}\mathbf{z}_L}} & \cdots & {\frac{\mathbf{q}_L^{\top}\mathbf{k}_L}{\mathbf{q}_L^{\top}\mathbf{z}_L}} \\
  \end{array} \right)
\end{align*} 
\end{minipage}
}
\end{center}

Importantly this equation can  be written as:

\begin{align}
   \mathbf{Y} = \big(\textsc{scale}(\mathbf{Q}\mathbf{K}^{\top} )\odot \mathbf{M}\big) \mathbf{V}
\end{align}

which despite equation \eqref{eq:scalecm} scaling is applied over the whole sequence not for the causal part of the sequence.
The matrix $\mathbf{A}^{C}$ is helpful for driving the recurrent version of \lion for Forward and Backward recurrences and the mask here $\mathbf{M}$ is equal to \lion's forward mask $\mathbf{M}^F$ in equation \eqref{eq:backpath}. As shown in \eqref{eq:backpath} the forward recurrence for the causal part of the attention can be presented as $\mathbf{Y}^{B} = \mathbf{A}^F \odot \mathbf{M}^F$ the matrix $\mathbf{A}^F$ can be created simply by using matrix $\mathbf{A}^{C}$ as bellow:


\begin{center}
\scalebox{0.75}{
\begin{minipage}[t]{1.25\textwidth}  
\begin{align*}   
    \underbrace{\left( \renewcommand*{\arraystretch}{1} \begin{array}{cccc}
       \blue{\frac{1}{2}\frac{\mathbf{q}_1^{\top}\mathbf{k}_1}{\mathbf{q}_1^{\top}\mathbf{z}_L}} &  &  &  \\
      \blue{\frac{\mathbf{q}_2^{\top}\mathbf{k}_1}{\mathbf{q}_2^{\top}\mathbf{z}_L}} & \blue{\frac{1}{2} \frac{\mathbf{q}_2^{\top}\mathbf{k}_2}{\mathbf{q}_2^{\top}\mathbf{z}_L}} &  &  \\
      \blue{\frac{\mathbf{q}_3^{\top}\mathbf{k}_1}{\mathbf{q}_3^{\top}\mathbf{z}_L}} & \blue{\frac{\mathbf{q}_3^{\top}\mathbf{k}_2}{\mathbf{q}_3^{\top}\mathbf{z}_L}} &  \blue{\frac{1}{2}\frac{\mathbf{q}_3^{\top}\mathbf{k}_3}{\mathbf{q}_3^{\top}\mathbf{z}_L}} &  \\
      \blue\vdots & \blue\vdots & \blue\vdots & \blue\ddots \\
      \blue{\frac{\mathbf{q}_L^{\top}\mathbf{k}_1}{\mathbf{q}_L^{\top}\mathbf{z}_L}} & \blue{\frac{\mathbf{q}_L^{\top}\mathbf{k}_2}{\mathbf{q}_L^{\top}\mathbf{z}_L}} & \blue\cdots & \blue{\frac{1}{2}\frac{\mathbf{q}_L^{\top}\mathbf{k}_L}{\mathbf{q}_L^{\top}\mathbf{z}_L}} \\
  \end{array} \right)}_{\mathbf{A}^F} =
  \underbrace{\left( \renewcommand*{\arraystretch}{1} \begin{array}{cccc}
       \blue{\frac{\mathbf{q}_1^{\top}\mathbf{k}_1}{\mathbf{q}_1^{\top}\mathbf{z}_L}} &  &  &  \\
      \blue{\frac{\mathbf{q}_2^{\top}\mathbf{k}_1}{\mathbf{q}_2^{\top}\mathbf{z}_L}} & \blue{\frac{\mathbf{q}_2^{\top}\mathbf{k}_2}{\mathbf{q}_2^{\top}\mathbf{z}_L}} &  &  \\
      \blue{\frac{\mathbf{q}_3^{\top}\mathbf{k}_1}{\mathbf{q}_3^{\top}\mathbf{z}_L}} & \blue{\frac{\mathbf{q}_3^{\top}\mathbf{k}_2}{\mathbf{q}_3^{\top}\mathbf{z}_L}} &  \blue{\frac{\mathbf{q}_3^{\top}\mathbf{k}_3}{\mathbf{q}_3^{\top}\mathbf{z}_L}} &  \\
      \blue\vdots & \blue\vdots & \blue\vdots & \blue\ddots \\
      \blue{\frac{\mathbf{q}_L^{\top}\mathbf{k}_1}{\mathbf{q}_L^{\top}\mathbf{z}_L}} & \blue{\frac{\mathbf{q}_L^{\top}\mathbf{k}_2}{\mathbf{q}_L^{\top}\mathbf{z}_L}} & \blue\cdots & \blue{\frac{\mathbf{q}_L^{\top}\mathbf{k}_L}{\mathbf{q}_L^{\top}\mathbf{z}_L}} \\
  \end{array} \right)}_{\mathbf{A}^{C}} -
     \underbrace{\left( \renewcommand*{\arraystretch}{1} \begin{array}{ccccc}
       \dcolor{\frac{1}{2}\frac{\mathbf{q}_1^{\top}\mathbf{k}_1}{\mathbf{q}_1^{\top}\mathbf{z}_L}} &  &  & & \\
      & \dcolor{\frac{1}{2} \frac{\mathbf{q}_2^{\top}\mathbf{k}_2}{\mathbf{q}_2^{\top}\mathbf{z}_L}} &  &  &\\
       & &  \dcolor{\frac{1}{2}\frac{\mathbf{q}_3^{\top}\mathbf{k}_3}{\mathbf{q}_3^{\top}\mathbf{z}_L}} &  &\\
       &  &  & \dcolor\ddots & \\
       &  &  & &\dcolor{\frac{1}{2}\frac{\mathbf{q}_L^{\top}\mathbf{k}_L}{\mathbf{q}_L^{\top}\mathbf{z}_L}} \\
  \end{array} \right)}_{\textcolor{red!80}{\mathbf{D}^F}}
\end{align*} 
\end{minipage}
}
\end{center}

Or equivalently:

\begin{equation}
    \mathbf{Y}^F = \mathbf{A}^F \odot \mathbf{M}^F = (\mathbf{A}^C - {\textcolor{red!80}{\mathbf{D}^F}} ) \odot \mathbf{M}^F
\end{equation}

Since the diagonal values of the mask \(\mathbf{M}^F\) are all ones and the matrix \({\textcolor{red!80}{\mathbf{D}^F}}\) is diagonal, we have:

\begin{equation}
\label{eq:dumcev2}
    \mathbf{Y}^F = (\mathbf{A}^C - {\textcolor{red!80}{\mathbf{D}^F}} ) \odot \mathbf{M}^F = \mathbf{A}^C  { \odot \mathbf{M}^F - {\mathbf{D}^F}}
\end{equation}



As $\mathbf{A}^C \odot \mathbf{M}^F$ corresponds to linear recurrence shown at \eqref{eq:dumyrec}. The vectorized form \eqref{eq:dumcev2} can be presented as linear recurrence:

\begin{minipage}[t]{1\textwidth}  
\begin{align}
\label{eq:recrec2}
\mathbf{y}_i = \frac{\mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{M}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)}{\mathbf{q}^{\top}_i\mathbf{z}_L} -\frac{1}{2}\frac{\mathbf{q}^{\top}_i\mathbf{k}_i}{\mathbf{q}^{\top}_i\mathbf{z}_L},  \hspace{5mm}   \mathbf{M}_{ij} = 
    \begin{cases} 
    \Pi_{k=i}^{j+1}{\lambda_k} & i \geq j  \\
    0 & i < j
\end{cases}
\end{align}
\end{minipage}

This is equivalent to the linear recurrence presented in equation \eqref{eq:forzl}. The same theoretical approach applies to the backward recurrence, leading to the following linear recurrence for both recurrences:  


\begin{minipage}[t]{.35\textwidth}
\begin{subequations}
\label{eq:rec11}
\begin{align}
    \mathbf{S}^{F}_i &= \textcolor{black}{{\lambda}_i} \mathbf{S}^F_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{y}^{F}_i &= \frac{{{\mathbf{q}_i}}^{\top} \mathbf{S}^F_i}{{{\mathbf{q}_i}}^{\top} \mathbf{z}_L}  - \frac{1}{2}\frac{{{\mathbf{q}_{i}}}^{\top} \mathbf{k}_{i}}{{{\mathbf{q}_{i}}}^{\top} \mathbf{z}_L}
\end{align}
\end{subequations}
\end{minipage}
\begin{minipage}[t]{.64\textwidth}
\begin{subequations}
\label{eq:rec12}
    \begin{align}
    &\mathbf{S}^{B}_i= \textcolor{black}{{\lambda}_{L-i}} \mathbf{S}^B_{i-1} + \mathbf{k}_{L-i+1} \mathbf{v}_{L-i+1}^{\top}, \\
    &\mathbf{y}^{B}_{L-i+1} = \frac{{{\mathbf{q}_{L-i+1}}}^{\top} \mathbf{S}^B_i}{{{\mathbf{q}_{L-i+1}}}^{\top} \mathbf{z}_L} - \frac{1}{2}\frac{{{\mathbf{q}_{L-i+1}}}^{\top} \mathbf{k}_{L-i+1}}{{{\mathbf{q}_{L-i+1}}}^{\top} \mathbf{z}_L}
\end{align}
\end{subequations}
\end{minipage}%

However, the above equation requires access to the summation of scaling values \(\mathbf{z}_L\). A naive approach would involve adding an additional scaling recurrence alongside the forward and backward recurrences to compute the summation of all keys in the sequence. This approach, however, is inefficient, as it complicates the process. While the forward and backward recurrences can traverse the sequence in parallel to obtain the forward and backward recurrences outputs \(\mathbf{Y}^F\) and \(\mathbf{Y}^B\), the scaling recurrence must be computed prior to these recurrences because both the forward and backward recurrences computations rely on the final scaling value \(\mathbf{z}_L\) to generate their outputs.

\subsection{Efficient and Simple Method for Scaling Attention During Inference}

As shown in previous section scaled attention matrix can be formulated as two recurrences \eqref{eq:rec11} and \eqref{eq:rec12} with an additional recurrence to sum all the keys ($\mathbf{z}_L$). This section we will proof how to avoid an extra scaling recurrence by simple modifications to equation \eqref{eq:rec11} and \eqref{eq:rec12}.

Considering having a scaling recurrence as part of forward and backward recurrence we will have:

\begin{minipage}[t]{.35\textwidth}
\begin{subequations}
\begin{align}
    \mathbf{S}^{F}_i &= \textcolor{black}{{\lambda}_i} \mathbf{S}^F_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{z}^F_i & =  \mathbf{z}^F_{i-1} +  \mathbf{k}_i \\
    c^F_i & =  {{\mathbf{q}_i}}^{\top} \mathbf{z}^F_{i} - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \\
    \mathbf{y}^{F}_i &= {{{\mathbf{q}_i}}^{\top} \mathbf{S}^F_i}  - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i
\end{align}
\end{subequations}
\end{minipage}
\begin{minipage}[t]{.64\textwidth}
\begin{subequations}
    \begin{align}
    &\mathbf{S}^{B}_i= \textcolor{black}{{\lambda}_{L-i}} \mathbf{S}^B_{i-1} + \mathbf{k}_{L-i+1} \mathbf{v}_{L-i+1}^{\top}, \\
    & \mathbf{z}^B_i= \mathbf{z}^B_{i-1} +  \mathbf{k}_{L-i+1} \\
    & c^B_i = {\mathbf{q}_{L-i+1}^{\top}}\mathbf{z}^B_i -  \frac{1}{2}{\mathbf{q}_{L-i+1}^{\top}}\mathbf{k}_{L-i+1} \\
    &\mathbf{y}^{B}_{L-i+1} = {{{\mathbf{q}_{L-i+1}}}^{\top} \mathbf{S}^B_i} -  \frac{1}{2}{\mathbf{q}_{L-i+1}^{\top}}\mathbf{k}_{L-i+1}\mathbf{v}_{L-i+1}^{\top}
\end{align}
\end{subequations}
\end{minipage}

The equations above are similar to the previous ones, with the addition of scalar states \(c^F\) and \(c^B\) for the backward and forward recurrences, respectively. During each recurrence, the outputs \(\mathbf{y}^{F}_i\) and \(\mathbf{y}^{B}_i\), along with the scalars \(c^F_i\) and \(c^B_i\), are saved for each token to construct the final output of each layer. \textit{It is also important to note that there is no need to save \(\mathbf{z}^F\) and \(\mathbf{z}^B\) for each token; these states can simply be overwritten in memory.} The final output of each layer is equal to:
\begin{align}
    \mathbf{y}_i = \frac{\mathbf{y}^{F}_i + \mathbf{y}^{B}_i}{c^F_i + c^B_i }
\end{align}
Where $\mathbf{y}^{F}_i$ and  $\mathbf{y}^{B}_i$ can be written as:

\begin{align}
    \mathbf{y}^{F}_i = \mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{M}^{F}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)
    - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i
    \hspace{2mm}
    ,
    \hspace{2mm}
     \mathbf{y}^{B}_i = \mathbf{q}^{\top}_i(\sum_{j=i}^L \mathbf{M}^{B}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)
     - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i
\end{align}

So the addition $\mathbf{y}^{F}_i + \mathbf{y}^{B}_i $ is equal to:
\begin{align}
\label{eq:54}
    & \mathbf{y}^{F}_i + \mathbf{y}^{B}_i = \mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{M}^{F}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)+\mathbf{q}^{\top}_i(\sum_{j=i}^L \mathbf{M}^{B}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j) - {{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i \\
    & \Rightarrow \mathbf{y}^{F}_i + \mathbf{y}^{B}_i =
     \mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{M}^{F}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j + \sum_{j=i}^L \mathbf{M}^{B}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)  - {{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i
\end{align}

Where by considering the mask $\mathbf{M}$ as bellow:

\scalebox{0.9}{
\begin{minipage}[t]{1.1\textwidth}  
\begin{align}
\label{fullatt}
\mathbf{M}_{ij} = \begin{cases} 
    \hblue {\Pi_{k=j}^{i+1}{\lambda_k}} & i > j  \\
    \horange {\Pi_{k=i+1}^{j}{\lambda_k}}  & i < j \\
    \hdiag{1}  & i = j
\end{cases} \hspace{1mm}  =  \hspace{1mm}
   \left(  \renewcommand*{\arraystretch}{2} \begin{array}{ccccc}
    \dcolor{\mathbf{1}}  & \orange{{\lambda}_2} & \orange{{\lambda}_2 {\lambda}_3}  & \orange{\cdots} & \orange{{\lambda}_2\cdots{\lambda}_L} \\
    \blue{{\lambda}_1} &  \dcolor{\mathbf{1}} & \orange{{\lambda}_3} & \orange{\cdots} & \orange{{\lambda}_3 \cdots {\lambda}_L} \\
    \blue{{\lambda}_1 {\lambda}_2} & \blue{{\lambda}_2} & \dcolor{\mathbf{1}} & \orange{\cdots} & \orange{{\lambda}_4 \cdots {\lambda}_L} \\
    \blue\vdots & \blue\vdots & \blue\vdots & \dcolor{\ddots} & \orange \vdots \\
    \blue{{{\lambda}_{L-1}\cdots {\lambda}_1}} & \blue{{{\lambda}_{L-1}\cdots {\lambda}_2}} & \blue{{{\lambda}_{L-1}\cdots {\lambda}_3}} & \blue{\cdots} &   \dcolor{\mathbf{1}} \\   
\end{array}  \right)  
\end{align}
\end{minipage} }

The above mask is equal to \(\mathbf{M}^F + \mathbf{M}^B -\mathbf{I}\), allowing equation \eqref{eq:54} to be rewritten as:

\begin{align}
    \mathbf{y}^{F}_i + \mathbf{y}^{B}_i &=
     \mathbf{q}^{\top}_i(\sum_{j=1}^i \mathbf{M}^{F}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j + \sum_{j=i}^L \mathbf{M}^{B}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)  - {{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i \\ 
    & = \mathbf{q}^{\top}_i(\sum_{j=1}^L \mathbf{M}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j) +  {{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i - {{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i \\
    & = \mathbf{q}^{\top}_i(\sum_{j=1}^L \mathbf{M}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j) 
    \label{eq:wrapup}
\end{align}

So we can finally find the output of each layer $\mathbf{y}_i $ as:
\begin{align}
\label{eq:dumm2}
    \mathbf{y}_i = \frac{\mathbf{y}^{F}_i + \mathbf{y}^{B}_i}{c^F_i + c^B_i }
    \xrightarrow{\text{Equation \eqref{eq:wrapup}}} \mathbf{y}_i = \frac{\mathbf{q}^{\top}_i(\sum_{j=1}^L \mathbf{M}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)}{c^F_i + c^B_i } 
\end{align}

It can easily be shown that:

\begin{align}
    c^F_i = \mathbf{q}^{\top}_i & (\sum^i_{j=1} \mathbf{k}_j) - \frac{1}{2}\mathbf{q}^{\top}_i\mathbf{k}_i \hspace{2mm} , \hspace{2mm}  c^B_i = \mathbf{q}^{\top}_i (\sum^L_{j=i} \mathbf{k}_j) - \frac{1}{2}\mathbf{q}^{\top}_i\mathbf{k}_i \\
    \Rightarrow c^F_i + c^B_i &= \mathbf{q}^{\top}_i (\sum^L_{j=1} \mathbf{k}_j) +  \mathbf{q}^{\top}_i\mathbf{k}_i - \frac{1}{2}\mathbf{q}^{\top}_i\mathbf{k}_i - \frac{1}{2}\mathbf{q}^{\top}_i\mathbf{k}_i \\
     \Rightarrow c^F_i + c^B_i & = \mathbf{q}^{\top}_i (\sum^L_{j=1} \mathbf{k}_j) +  \mathbf{q}^{\top}_i\mathbf{k}_i - \mathbf{q}^{\top}_i\mathbf{k}_i =  \mathbf{q}^{\top}_i (\sum^L_{j=1} \mathbf{k}_j)
     = \mathbf{q}^{\top}_i \mathbf{z}_L
\end{align}

So the final output of the layer is:

\begin{align}
\label{eq:finalyay}
    \mathbf{y}_i = \frac{\mathbf{y}^{F}_i + \mathbf{y}^{B}_i}{c^F_i + c^B_i } = \frac{\mathbf{q}^{\top}_i(\sum_{j=1}^L \mathbf{M}_{ij}\mathbf{k}_j\mathbf{v}^{\top}_j)}{\mathbf{q}^{\top}_i (\sum^L_{j=1} \mathbf{k}_j)}
\end{align}

Alternatively, in vectorized form, it can be expressed as:

\begin{equation}
    \mathbf{Y} = \mathbf{Y}^{F}+ \mathbf{Y}^{B} = \big(\textsc{scale}(\mathbf{Q}\mathbf{K}^{\top} )\odot \mathbf{M}\big) \mathbf{V}
\end{equation}

with $\mathbf{M}$ being the attention mask created by ${\lambda}_i$s as in equation \ref{fullatt}.


\subsection{Flipping Operation in Backward recurrence}
\label{ap:flip}
Here we define the operation which flip the matrices $\mathbf{A}^B , \mathbf{M}^B$ for the reverse reccurence th goal is to find the $F(.)$ such that:

\scalebox{0.8}{
 \begin{minipage}[t]{1.2\textwidth}  
\begin{align}
\label{backattflip2}
\mathbf{A}^B = 
\left( \renewcommand*{\arraystretch}{2} \begin{array}{cccc}
      \frac{1}{2}{\mathbf{q}_1^{\top}\mathbf{k}_1} & {\mathbf{q}_1^{\top}\mathbf{k}_2} & \cdots & {\mathbf{q}_1^{\top}\mathbf{k}_L} \\
       &  \frac{1}{2}{\mathbf{q}_2^{\top}\mathbf{k}_2}  & \cdots & {\mathbf{q}_2^{\top}\mathbf{k}_L} \\
      & & \ddots & \vdots \\
       &  &  &  \frac{1}{2}{\mathbf{q}_L^{\top}\mathbf{k}_L}  \\   
  \end{array} \right) \rightarrow F(\mathbf{A}^B) =
   \left( \renewcommand*{\arraystretch}{1} \begin{array}{cccc}
      \frac{1}{2}\frac{\mathbf{q}_L^{\top}\mathbf{k}_L}{\mathbf{q}_L^{\top}\mathbf{z}_L} &  &  & \\
      \frac{\mathbf{q}_{L-1}^{\top}\mathbf{k}_{L}}{\mathbf{q}_2^{\top}\mathbf{z}_L} & \frac{1}{2}\frac{\mathbf{q}_{L-1}^{\top}\mathbf{k}_{L-1}}{\mathbf{q}_2^{\top}\mathbf{z}_L}  &  &  \\
      \vdots & \vdots & \ddots & \\
      \frac{\mathbf{q}_1^{\top}\mathbf{k}_L}{\mathbf{q}_1^{\top}\mathbf{z}_L} & \frac{\mathbf{q}_1^{\top}\mathbf{k}_{L-1}}{\mathbf{q}_1^{\top}\mathbf{z}_L} & \cdots & \frac{1}{2}\frac{\mathbf{q}_1^{\top}\mathbf{k}_1}{\mathbf{q}_1^{\top}\mathbf{z}_L}  \\   
  \end{array} \right)  
\end{align} 

\end{minipage}
}

\scalebox{0.8}{
\begin{minipage}[t]{1.2\textwidth}  
\begin{align}
\label{eq:maskdec}
\mathbf{M}^B = \left( \renewcommand*{\arraystretch}{1.5} \begin{array}{ccccc}
    {\mathbf{1}}  & {{\lambda}_2} & {{\lambda}_2 {\lambda}_3}  & {\cdots} & {{\lambda}_2\cdots{\lambda}_L} \\
     &  {\mathbf{1}} & {{\lambda}_3} & {\cdots} & {{\lambda}_3 \cdots {\lambda}_L} \\
     &  & {\mathbf{1}} & {\cdots} & {{\lambda}_4 \cdots {\lambda}_L} \\
     &  &  & {\ddots} &  \vdots \\
     &  &  &  &   {\mathbf{1}} \\   
\end{array} \right) \rightarrow F(\mathbf{M}^B) =
    \left( \renewcommand*{\arraystretch}{1} \begin{array}{ccccc}
    {\mathbf{1}}  &  &  &  & \\
     {{\lambda}_L} &  {\mathbf{1}} &  &  &  \\
     {{\lambda}_L} {{\lambda}_{L-1}}& {{\lambda}_{L-1}} & {\mathbf{1}} &  &  \\
       \vdots &  \vdots &  \vdots & {\ddots} &  \\
     {{\lambda}_{L} \cdots {\lambda}_2}& {{\lambda}_L \cdots {\lambda}_3} & {{\lambda}_L \cdots {\lambda}_4} &  \cdots &   {\mathbf{1}} \\   
\end{array} \right)
\end{align}
\end{minipage} 
}

The above can be achieved by:

\begin{minipage}[t]{1\textwidth}  
\begin{align}
    F(\mathbf{A}) = \mathbf{J}_L\mathbf{A}\mathbf{J}_L, \hspace{2mm},  \mathbf{J}_L = \left( \begin{array}{cccc} 
 &&&1\\
 &&1&\\
 &\iddots&&\\
 1&&&\\ 
\end{array}  \right)
\end{align}
\end{minipage}

\subsection{Mapping Existing autoregressive models into \lion} \label{sec:map}

As noted, other autoregressive recurrent models can also be integrated into our bidirectional framework, benefiting from parallelization during training and fast bidirectional inference. Here, we demonstrate how to map several well-known Linear Transformers into the bidirectional form of \lion, along with their corresponding masked attention matrix and inference linear recurrence.

\textbf{Linear Transformer (\textbf{\lionlit}).} According to \cite{trans_rnn} the linear transformer has a recurrence:

\begin{align}
{\mathbf{S}_i^F} &=   {\mathbf{S}_{i-1}^F} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
     {\mathbf{z}_i^F} & =   {\mathbf{z}_{i-1}^F} + {\mathbf{k}_i}, \\
    \hspace{4mm} \textsc{S}&\textsc{caled}:  {\mathbf{y}_i^F}= \frac{{{\mathbf{q}_i}}^{\top}  {\mathbf{S}_i^F}}{{{\mathbf{q}_i}}^{\top}  {\mathbf{z}_i^F}} \\
    \hspace{4mm} \textsc{No}&\textsc{n-scaled}:  {\mathbf{y}_i^F}= {{{\mathbf{q}_i}}^{\top}  {\mathbf{S}_i^F}}
\end{align}

As observed, this is a special case of our bidirectional recurrence defined in \eqref{eq:bestrnn} with \(\lambda_i = 1\), as \textbf{\lion} resembles the scaled masked attention. In the case of the linear transformer, we require attention without scaling for the recurrence. The vectorized form for the scaled version can then be derived easily as follows:

\begin{minipage}[t]{0.45\textwidth}
\begin{whitebox}
    \begin{align}
        \mathbf{S}^{F/B}_i &= \mathbf{S}^{F/B}_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{z}^{F/B}_i & =  \mathbf{z}^{F/B}_{i-1} +  \mathbf{k}_i \\
    c^{F/B}_i & =  {{{\mathbf{q}_i}}^{\top} \mathbf{z}^{F/B}_{i}} - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i, \\
    \mathbf{y}^{F/B}_i &= {{{\mathbf{q}_i}}^{\top} \mathbf{S}^{F/B}_i}  - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i 
    \end{align}
\end{whitebox}
\end{minipage}
\begin{minipage}[t]{0.1\textwidth}
\vspace{-2.3cm}
    \begin{align}
       \textbf{=} \notag
    \end{align}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\vspace{-2.3cm}
\begin{whitebox}
    \begin{align}
        \mathbf{Y} &= \textsc{scale}(\mathbf{Q}\mathbf{K}^{\top}\mathbf{V})
    \end{align}
\end{whitebox}
\end{minipage}

For the non-scaled variant, we simply remove the scaling state \(\mathbf{z}\) as well as the scaling parameter \(c\). Consequently, the bidirectional linear transformer, which is equivalent to and parallelizable with attention without scaling, can be expressed as follows:

\begin{minipage}[t]{0.45\textwidth}
\begin{whitebox}
    \begin{align}
        \mathbf{S}^{F/B}_i &= \mathbf{S}^{F/B}_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{y}^{F/B}_i &= {{{\mathbf{q}_i}}^{\top} \mathbf{S}^{F/B}_i}  - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i 
    \end{align}
\end{whitebox}
\end{minipage}
\begin{minipage}[t]{0.1\textwidth}
\vspace{-1.5cm}
    \begin{align}
       \textbf{=} \notag
    \end{align}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\vspace{-1.5cm}
\begin{whitebox}
    \begin{align}
        \mathbf{Y} &= \mathbf{Q}\mathbf{K}^{\top}\mathbf{V}
    \end{align}
\end{whitebox}
\end{minipage}

The final output for scaled version can be extracted as $\mathbf{y}_i = \frac{\mathbf{y}^{B}_i + \mathbf{y}^{B}_i}{c^B_i + c^B_i }$ for scaled and as $\mathbf{y}_i = {\mathbf{y}^{B}_i + \mathbf{y}^{B}_i}$ for non-scaled version. Variations of linear transformers, such as Performer \citep{performer}, which employ different non-linearities \(\phi(.)\) for keys and queries, can be adapted to a bidirectional format using the framework established for linear transformers.


\textbf{Retentive Network} (\lionretnet)\textbf{.} According to \cite{retnet} the forward equation for a retentive network can be written as:

\begin{align}
\label{eq:recurscale}
     \mathbf{S}_i^F &=  \lambda \mathbf{S}_{i-1}^F + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{y}_i^F &= {{{\mathbf{q}_i}}^{\top}  \mathbf{S}_i^F}
\end{align}

This architecture can also be expanded to bi-directional setting simply by not scaling the attention in our framework and only using the mask with non input-dependent $\lambda_i=\lambda$ values:

\begin{minipage}[t]{0.45\textwidth}
\begin{whitebox}
    \begin{align}
        \mathbf{S}^{F/B}_i &= \lambda\mathbf{S}^{F/B}_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{y}^{F/B}_i &= {{{\mathbf{q}_i}}^{\top} \mathbf{S}^{F/B}_i}  - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i 
    \end{align}
\end{whitebox}
\end{minipage}
\begin{minipage}[t]{0.1\textwidth}
\vspace{-1.5cm}
    \begin{align}
       \textbf{=} \notag
    \end{align}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\vspace{-1.5cm}
\begin{whitebox}
    \begin{align}
        \mathbf{Y} &= (\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M}^R)\mathbf{V}
    \end{align}
\end{whitebox}
\end{minipage}

Note that:  $\mathbf{M}^R_{ij} = \lambda^{|i-j|}$.

\textbf{xLSTM (\lion-\textsc{LSTM}).} According to \cite{xlstm} the recurrence for forward recurrence of xLSTM can be written as:

\begin{align}
     \mathbf{S}_i^F &=   f_i\mathbf{S}_{i-1}^F + i_i\mathbf{k}_i \mathbf{v}_i^{\top}, \\
     \mathbf{z}_i^F & =   f_i\mathbf{z}_{i-1}^F + i_i{\mathbf{k}_i}, \\
     \mathbf{y}_i^F &= \frac{{{\mathbf{q}_i}}^{\top}  \mathbf{S}_i^F}{{{\mathbf{q}_i}}^{\top}  \mathbf{z_i}^F} 
\end{align}

The above recurrence is equivalent to \eqref{eq:recrec} by considering \(i_i \mathbf{k}_i\) as a new key. The term \(i_i \mathbf{k}_i\) can be easily vectorized by aggregating all \(i_i\) values for each token into a vector \(\mathbf{i}\). Thus, we can express the vectorized form of the bidirectional xLSTM and its equivalence to attention as follows:

\begin{minipage}[t]{0.46\textwidth}
\begin{whitebox}
    \begin{align}
        \mathbf{S}^{F/B}_i &= f_i\mathbf{S}^{F/B}_{i-1} + i_i\mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{z}^{F/B}_i & =  f_i\mathbf{z}^{F/B}_{i-1} +  i_i\mathbf{k}_i \\
    c^{F/B}_i & =  {{{\mathbf{q}_i}}^{\top} \mathbf{z}^{F/B}_{i}} - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i, \\
    \mathbf{y}^{F/B}_i &= {{{\mathbf{q}_i}}^{\top} \mathbf{S}^{F/B}_i}  - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i \\
    \text{Output:} \hspace{2mm} \mathbf{y}_i & = \frac{\mathbf{y}^F_i+\mathbf{y}^B_i}{\max(c^F_i+c^B_i,1)}
    \end{align}
\end{whitebox}
\end{minipage}
\begin{minipage}[t]{0.05\textwidth}
\vspace{-2.4cm}
    \begin{align}
       \textbf{=} \notag
    \end{align}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\vspace{-2.42cm}
\begin{whitebox}
    \begin{align}
        \mathbf{Y} &= \textsc{scale'}(\mathbf{Q}(\mathbf{i}\odot\mathbf{K}^{\top}))\odot \mathbf{M}^f)\mathbf{V}
    \label{xlstmvec1}
    \end{align}
\end{whitebox}
\end{minipage}

where the mask $\mathbf{M}^f$ is equal to the \lion mask \eqref{fullatt} just by replacing $\lambda_i = f_i$. And where operation $\textsc{scale'}$ consider the maximum of operation in the denominator as:
\begin{equation}
    \textsc{scale'}(\mathbf{A})_{ij} = \frac{\mathbf{A}_{ij}}{\max(\sum_{j=1}^L \mathbf{A}_{ij},1)}
\end{equation}


\textbf{Gated RFA (\lion-\textsc{GRFA}).} Gated RFA \citep{yang2023gated} in autoregressive mode exhibits a recurrence similar to that of xLSTM, with only minor differences:

\begin{align}
     \mathbf{S}_i^F &=   g_i\mathbf{S}_{i-1}^F + (1-g_i)\mathbf{k}_i \mathbf{v}_i^{\top}, \\
     \mathbf{z}_i^F & =   g_i\mathbf{z}_{i-1}^F + (1-g_i){\mathbf{k}_i}, \\
     \mathbf{y}_i^F &= \frac{{{\mathbf{q}_i}}^{\top}  \mathbf{S}_i^F}{{{\mathbf{q}_i}}^{\top}  \mathbf{z_i}^F} 
\end{align}

Thus, the bidirectional version of the model retains a similar output, achieved by replacing the vector \(\mathbf{i}\) in \eqref{xlstmvec1} with \(1 - \mathbf{g}\), where \(\mathbf{g}\) represents the vectorized form of all scalar values \(g_i\).

\begin{minipage}[t]{0.475\textwidth}
\begin{whitebox}
    \begin{align}
        \mathbf{S}^{F/B}_i &= g_i\mathbf{S}^{F/B}_{i-1} + (1-g_i)\mathbf{k}_i \mathbf{v}_i^{\top}, \\
    \mathbf{z}^{F/B}_i & =  g_i\mathbf{z}^{F/B}_{i-1} +  (1-g_i)\mathbf{k}_i \\
    c^{F/B}_i & =  {{{\mathbf{q}_i}}^{\top} \mathbf{z}^{F/B}_{i}} - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i, \\
    \mathbf{y}^{F/B}_i &= {{{\mathbf{q}_i}}^{\top} \mathbf{S}^{F/B}_i}  - \frac{1}{2}{{\mathbf{q}_i}}^{\top} \mathbf{k}_i \mathbf{v}_i 
    \end{align}
\end{whitebox}
\end{minipage}
\begin{minipage}[t]{0.01\textwidth}
\vspace{-2.3cm}
    \begin{align}
       \textbf{=} \notag
    \end{align}
\end{minipage}
\scalebox{0.9}{
\begin{minipage}[t]{0.55\textwidth}
\vspace{-2.7cm}
\begin{whitebox}
    \begin{align}
        \mathbf{Y} &= \textsc{scale}(\mathbf{Q}((1-\mathbf{g})\odot\mathbf{K}^{\top})\odot \mathbf{M})\mathbf{V}
    \end{align}
      \label{xlstmvec2}
\end{whitebox}
\end{minipage}}


\subsection{Mask $\mathbf{M}^F \& \mathbf{M}^B$ are Semiseperable with rank-1} \label{ap:rankmask}

For the lower triangular part of the selective mask \(\mathbf{M}^F\), the upper triangular part can be filled such that it creates a full matrix with rank 1, which aligns with the definition of a semi-separable matrix with rank 1, as below:

\setlength{\arrayrulewidth}{2.5pt}
\arrayrulecolor{azure!70} 
\scalebox{0.45}{
\begin{minipage}[t]{0\textwidth}  
\begin{align*}
\textbf{\huge{$\M^F$ = \hspace{1mm}}}
\renewcommand*{\arraystretch}{2} 
\begin{array}{ccccc}
\blue{\mathbf{1}}  &  &  & & \\ 
\blue{{\lambda}_1} &  \blue{\mathbf{1}} &  &  &  \\
\blue{{\lambda}_1 {\lambda}_2} & \blue{{\lambda}_2} & \blue{\mathbf{1}} &  &  \\
\blue\vdots & \blue\vdots & \blue\vdots & \blue{\ddots} &  \\
\blue{{{\lambda}_{L-1}\cdots {\lambda}_1}} & \blue{{{\lambda}_{L-1}\cdots {\lambda}_2}} & \blue{{{\lambda}_{L-1}\cdots {\lambda}_3}} & \blue{\cdots} &   \blue{\mathbf{1}} \\   
\end{array} 
\textbf{\huge{= \textsc{Tril}}} \left(
\renewcommand*{\arraystretch}{2} 
\begin{array}{|c|cccc}
\cline{1-1}\multicolumn{5}{c}{}\\[-5ex]
\blue{\mathbf{1}}  & \orange{{\lambda}^{-1}_1} & \orange{{\lambda}^{-1}_1 {\lambda}^{-1}_2}  & \orange{\cdots} & \orange{{\lambda}^{-1}_1\cdots{\lambda}^{-1}_{L-1}} \\ 
\blue{{\lambda}_1} &  \blue{\mathbf{1}} & \orange{{\lambda}^{-1}_2} & \orange{\cdots} & \orange{{\lambda}^{-1}_2 \cdots {\lambda}^{-1}_{L-1}} \\
\blue{{\lambda}_1 {\lambda}_2} & \blue{{\lambda}_2} & \blue{\mathbf{1}} & \orange{\cdots} & \orange{{\lambda}^{-1}_3 \cdots {\lambda}^{-1}_{L-1}} \\
\blue\vdots & \blue\vdots & \blue\vdots & \blue{\ddots} & \orange \vdots \\
\blue{{{\lambda}_{L-1}\cdots {\lambda}_1}} & \blue{{{\lambda}_{L-1}\cdots {\lambda}_2}} & \blue{{{\lambda}_{L-1}\cdots {\lambda}_3}} & \blue{\cdots} &   \blue{\mathbf{1}} \\   \cline{1-1}
\end{array} 
\right)
\textbf{ \hspace{1mm}  \huge{= \textsc{Tril}} \hspace{1mm}} \left(
\underbrace{
\begin{array}{|c|} 
\cline{1-1}\multicolumn{1}{c}{}\\[-5ex]
\blue{\mathbf{1}}  \\ 
\blue{{\lambda}_1} \\
\blue{{\lambda}_1 {\lambda}_2}  \\
\blue\vdots  \\
\blue{{{\lambda}_{L-1}\cdots {\lambda}_1}} \\  \cline{1-1}
\end{array} }_{\hspace{1mm}\scalemath{1.5}{ \textcolor{azure!100}{\lvec} }}
\underbrace{
\arrayrulecolor{red!60} \begin{array}{|ccccc|} 
\cline{1-5}\multicolumn{5}{c}{}\\[-5ex]
 \orange{\mathbf{1}}  & \orange{{\lambda}^{-1}_1} & \orange{{\lambda}^{-1}_1 {\lambda}^{-1}_2}  & \orange{\cdots} & \orange{{\lambda}^{-1}_1\cdots{\lambda}^{-1}_{L-1}} \\ \cline{1-5}
\end{array}  \arrayrulecolor{azure!70} }_{\hspace{1mm}\scalemath{1.5}{\textcolor{red!60}{\uu^\top = \einv (\lvec)^\top }}} \right) 
\textbf{\huge{$ \hspace{1mm} = \textsc{Tril} ( \lvec \uu^\top )$  }}
\end{align*}
\end{minipage}
}

\setlength{\arrayrulewidth}{0.7pt}
\arrayrulecolor{black} 

where $\textsc{Tril}$ is the function which masks the upper part of the matrix and set it to zero. Same is applied for the upper triangular part of the matrix $\mathbf{M}^B$ as well. Also since decay mask is the specific case of selective mask by setting $\lambda_i=\lambda$ all the proofs above also holds for the fixed decay mask used in RetNet. 

\subsection{Expanding the Dimension of $a_i$} \label{sec:expandai}

Similar to other recurrent models, particularly SSM variations, the dimension of \(a_i\) can be increased beyond a scalar. When \(a_i\) is a scalar, the same mask \(\mathbf{M}\) is applied to all elements of the value vector \(\mathbf{v}\). However, if we allow \(a_i\) to be a vector \(\mathbf{a}_i \in \mathbb{R}^d\), the mask matrix transforms into a tensor \(\bar{\mathbf{M}} \in \mathbb{R}^{L \times L \times d}\). This tensor can be computed in parallel for each individual value element along the last dimension. The last dimension will then be multiplied using the Hadamard product with the values, resulting in the following vectorized form:

\begin{align}
   \mathbf{Y} = \textsc{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \bar{\mathbf{M}}) * \mathbf{V}
\end{align}

In this equation, the operation \( * \) denotes the Hadamard product applied along the last dimension of the tensor mask \(\bar{\mathbf{M}}\) with the value vector \(\mathbf{V}\), while the first two dimensions are combined using a standard matrix product. The corresponding code is as follows:

\begin{figure}[t]
\hspace{5mm}\begin{minipage}[t]{0.95\columnwidth} 
    \centering
    \begin{python}[framerule=0.3
    mm , rulecolor=\color{black} ,frame=single]
attn = (Q @ K.transpose(-2, -1))
attn = torch.einsum("nhkmd,nhkm->nhkmd", M, attn)
attn = scale(attn)
x = torch.einsum("nhkmd,nhmd->nhkd", attn, V)
\end{python}
\end{minipage}
\end{figure}


\subsection{Generation of the Mask}
\label{subsec:code}
Below we present the Python code used for the creation of the bidirectional mask $\mathbf{M}$ as described in previous sections. 

\begin{figure}[t]
\hspace{5mm}\begin{minipage}[t]{0.95\columnwidth} 
    \centering
    \begin{python}[framerule=0.3
    mm , rulecolor=\color{black} ,frame=single]
#caption=Code for generation of the selective bidirectional mask of \lion , 
def create_matrix_from_tensor(tensor):
    cumpsum = torch.exp(torch.cumsum(tensor, dim=-1 ))
    A = torch.matmul(cump.unsqueeze(-1) , 1/ ( cump.unsqueeze(-1).transpose(-1,-2)))
    return torch.tril(A)

def Mask_selective(vec):
    vec_shape = vec.shape
    A_for = create_matrix_from_tensor(vec)
    A_back = create_matrix_from_tensor(flip(vec))
    return A_for + A_back - torch.eye(A_for.shape[-1])

def Mask_Decay(a_i , L):
    idx = torch.arange(L,device=a_i.device)
    I, J = torch.meshgrid(idx, idx, indexing='ij')
    E = (torch.abs((I-J)).float().view(1,1,L,L))
    M = torch.sigmoid(a_i).view(1,-1,1,1)**E
    return M

\end{python}
\end{minipage}
\end{figure}

\begin{figure}[t]
\hspace{5mm}\begin{minipage}[t]{0.95\columnwidth} 
    \centering
    \begin{python}[framerule=0.3
    mm , rulecolor=\color{black} ,frame=single]
#caption=Code for generation of the partial selective bidirectional mask of \lion for chunking, 
def Partial_Mask_selective(vec):
    B,H,L = vec.shape
    A_for = create_matrix_from_tensor_forward(vec[...,:-1]),chunk_index,chunk_len)
    A_back = create_matrix_from_tensor_backward(vec[...,1:]),chunk_index,chunk_len)
    I  = torch.diag_embed(torch.ones((B,H,L-chunk_index*chunk_len)),offset = -chunk_index*chunk_len)[...,:L]
    return A_for + A_back - I.to(A_for.device)

\end{python}
\end{minipage}
\end{figure}

\subsection{Details for \lion  of Chunkwise Parallel} \label{detailchunk}

As the full linear attention is written as:
\begin{align}
\label{now}
    \mathbf{Y} = \textsc{Scale}(\mathbf{Q}\mathbf{K}^\top \odot \mathbf{M})\mathbf{V}
\end{align}

by apply chunking to queries/keys/values and defining the $\mathbf{Q}_{[i]},\mathbf{K}_{[i]},\mathbf{V}_{[i]} = \mathbf{Q}_{iC+1:i(C+1)},\mathbf{K}_{iC+1:i(C+1)},\mathbf{V}_{iC+1:i(C+1)} \in \R^{C\times d}$ we can simply rewrite the \cref{now} in chunkwise form as:

\begin{align}
\label{nowresss}
     \mathbf{A}_{[ij]} & = \mathbf{Q}_{[i]}\mathbf{K}_{[j]}^\top \odot \mathbf{M}_{[ij]}, \quad \mathbf{C}_{[ij]} = \mathbf{C}_{[ij-1]} + \text{Sum} (\mathbf{A}_{[ij]}), \\
     \mathbf{S}_{[ij]} & =\mathbf{S}_{[ij-1]} + \mathbf{A}_{[ij]} \mathbf{V}_{[j]} , \quad \mathbf{Y}_{[i]} = \frac{\mathbf{S}_{[iN]}}{\mathbf{C}_{[iN]}}
\end{align}

where $N$ is the number of total chunks and $N=\frac{L}{C}$ and $\text{Sum}$ is the summation over the rows of the matrix. Since the full attention matrix needs to be scaled according to the full row of attention we need to update the scaling value for each chunk as stored in $\mathbf{C}_{ij}$ and the final output for chunk $i$ is computed by using the last chunkwise hidden state $\mathbf{S}_{[i]}$ divided by the scaling for that chunk $\mathbf{C}_{[i]}$ which are equal to ${\mathbf{S}_{[iN]}},{\mathbf{C}_{[iN]}}$.

To construct the chunkwise mask \(\mathbf{M}_{ij}\), we define the chunk-level selective parameters as:  

\[
\mathbf{L}^F_{[i]} = \cumprod(\mathbf{\lambda^F})_{iC+1:(i+1)C}, \quad \mathbf{L}^B_{[i]} = \cumprod(\mathbf{\lambda^B})_{iC+1:(i+1)C}.
\]

Since the full mask is composed of lower and upper triangular components:

\[
\mathbf{M}^F = \text{TRIL}(\mathbf{L}^F \frac{1}{\mathbf{L}^F}), \quad \mathbf{M}^B = \text{TRIU}(\mathbf{L}^B \frac{1}{\mathbf{L}^B}),
\]

we determine the appropriate chunkwise form based on relative chunk positions:  

\begin{itemize}
   \item If \(i > j\), the chunk falls entirely within the lower triangular part, requiring only \(\mathbf{M}^F\), which can be efficiently computed as \(\mathbf{L}^F_{[i]} \frac{1}{\mathbf{L}^F_{[j]}}\).  
\item If \(i < j\), the chunk is fully in the upper triangular region, needing only \(\mathbf{M}^B\), which follows from \(\mathbf{L}^B_{[j]} \frac{1}{\mathbf{L}^B_{[i]}}\).  
\item If \(i = j\), the chunk lies along the diagonal and requires both the lower triangular part of \(\mathbf{M}^F\) and the upper triangular part of \(\mathbf{M}^B\), expressed as:

\end{itemize}
    
The full matrix is: 

\[
\mathbf{M}_{[ij]} = 
\begin{cases} 
\mathbf{L}^F_{[i]} \frac{1}{\mathbf{L}^F_{[j]}}^\top & \text{if } i>j,  \\
\mathbf{L}^B_{[j]} \frac{1}{\mathbf{L}^B_{[i]}}^\top & \text{if } i<j,  \\
\text{Tril}\left(\mathbf{L}^F_{[i]} \frac{1}{\mathbf{L}^F_{[i]}}^\top\right) + \text{Triu}\left(\mathbf{L}^B_{[i]} \frac{1}{\mathbf{L}^B_{[i]}}^\top\right) - \mathbf{I} & \text{if } i = j. 
\end{cases} 
\]

More visual presentation is shown at Figure \cref{maskchunk}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.3\textwidth]{figs/exexex.png}
  \caption{Chunkwise Parallel Mask $\mathbf{M}$ of \lion.}
    \label{maskchunk}
\end{figure}

For the fixed decay mask, a simpler case of the general selective mask, \(\mathbf{L}^F\) and \(\mathbf{L}^B\) are identical and simplify to \(\mathbf{L}_i = \lambda^i\). Since the full mask follows \(\mathbf{M}_{ij} = \lambda^{|i-j|}\), the chunkwise mask for \(i, j\) can be written as:

\[
\mathbf{M}_{[ij]} = \mathbf{L}_{[i]} \frac{1}{\mathbf{L}_{[j]}} = \lambda^{|i-j|} \mathbf{L}_{[0]} \frac{1}{\mathbf{L}_{[0]}}.
\]

Similarly, for the upper triangular part:

\[
\mathbf{M}_{[ij]} = \lambda^{|i-j|} \frac{1}{\mathbf{L}^\top_{[0]}} \mathbf{L}_{[0]}.
\]

For diagonal chunks, the mask remains a fixed matrix \(\mathbf{\Gamma} \in \mathbb{R}^{C \times C}\), where \(\mathbf{\Gamma}_{ij} = \lambda^{|i-j|}\), representing a smaller version of the full fixed decay mask \(\mathbf{M} \in \mathbb{R}^{L \times L}\) with \(\mathbf{M}_{ij} = \lambda^{|i-j|}\).


\subsection{Parallel Chunkwise Materialization for Parallel Output Computation} \label{parpar}

As mentioned in \cref{sec:chunk}, \cref{nowresss} can be further parallelized by processing all \(i\) tokens simultaneously, leading to the following matrix multiplications in the parallel chunkwise form:

\begin{align}
     \mathbf{A}_{[i]} & = \mathbf{Q}\mathbf{K}_{[j]}^\top \odot \mathbf{M}_{[j]}, \quad \mathbf{C}_{[j]} = \mathbf{C}_{[j-1]} + \text{Sum} (\mathbf{A}_{[j]}), \\
     \mathbf{S}_{[j]} & =\mathbf{S}_{[j-1]} + \mathbf{A}_{[ij]} \mathbf{V}_{[j]} , \quad \mathbf{Y} = \frac{\mathbf{S}_{[N]}}{\mathbf{C}_{[N]}}
\end{align}

and the mask $\mathbf{M}_{[j]}$ is created based on:

\begin{align}
\mathbf{M}_{[j]} = \text{Tril}(\mathbf{L}^F \frac{1}{{\mathbf{L}^F}^\top_{[j]}} , \textbf{diagonal}=-j) + \text{Tril}(\mathbf{L}^B \frac{1}{{\mathbf{L}^B}^\top_{[j]}} , \textbf{diagonal}=j)
\end{align}

Consider a real matrix \(X \in \mathbb{R}^{n \times n}\). The operator \(\mathrm{Tril}(\mathbf{X}, d)\) returns the lower-triangular region of \(X\), including all elements on and below the diagonal shifted by \(d\). In other words, \(\mathrm{tril}(\mathbf{X}, d)_{ij} = \mathbf{X}_{ij}\) whenever \(j \leq i + d\) and is \(0\) otherwise. Similarly, \(\mathrm{Triu}(\mathbf{X}, d)\) returns the upper-triangular region, keeping elements on and above the diagonal shifted by \(d\). Formally, \(\mathrm{Triu}(\mathbf{X}, d)_{ij} = \mathbf{X}_{ij}\) if \(j \geq i + d\) and \(0\) otherwise.





\subsection{\rebuttal{Changing the order of patches}}
\label{subsec:rotation}
\rebuttal{When processing images, both the spatial relationships among neighboring pixels and their positions are as critical as the pixel values themselves. Positional embeddings provide a way to incorporate these spatial relationships. A common approach in Transformers involves flattening the image, as illustrated in the left panel of \cref{fig:rot}. However, we argue that this method of flattening is suboptimal and can be enhanced to include additional contextual information.
\\
\\
Furthermore, in scenarios involving a fully masked setup or RNN-based inference, the sequence in which pixels are processed becomes increasingly important. To address this, we propose a new reordering scheme for pixel values. In the attention module, the pixel values are reordered following the patterns depicted in the center and right panels of \cref{fig:rot}. Forward and backward passes are then executed based on this new ordering, adhering to established procedures. The outputs from these two passes are subsequently averaged to generate the final result.
\\
\\
We refer to this method as \lionrot throughout the paper. This approach demonstrated a notable improvement in accuracy for image classification tasks while maintaining the efficiency and flexibility inherent to the method. A similar concept has been previously explored in Vision-LSTM \citep{alkin2024visionlstm}.}

\begin{figure}[tb]
    \centering
    \includegraphics[width=1\textwidth]{figs/rotational.png}
  \caption{\rebuttal{\textit{Reordering of patches.} Left is the naive approach to flatten images, also used in \lions. Center and right figures are the new approaches applied in \lionrot to consider further spatial information. }}
    \label{fig:rot}
\end{figure}

\section{Additional experimental validation}
\label{sec:app_experiments}

\subsection{Citations for LRA Benchmarks}
\label{sec:lra_bench}

The LRA baselines included in Table \ref{tab:lra_exp} correspond to Transformer \citep{vaswani_attention_2017}, MEGA and MEGA-chunk \citep{ma2022mega}, DSS \citep{gupta2022diagonalstatespaceseffective}, S4 \citep{gu2021efficiently}, S5 \citep{s5}, Mamba \citep{mamba}, Local Att. \citep{vaswani_attention_2017}, Sparse Transformer \citep{sparsetransformer}, Longformer \citep{Beltagy2020Longformer}, Linformer \citep{wang2020linformer}, Reformer \citep{kitaev2020reformerefficienttransformer}, Sinkhorn Transformer \citep{tay2020sinkhorn}, BigBird \citep{zaheer2020bigbirdtransformerslonger}, Linear Transformer \citep{trans_rnn}, Performer \citep{performer}, FNet \citep{leethorp2022fnetmixingtokensfourier}, NystrÃ¶mformer \citep{xiong2021nystromformernystrombasedalgorithmapproximating}, Luna-256 \citep{ma2021luna} and H-Transformer-1D \citep{zhu2021htransformer1dfastonedimensionalhierarchical}.

\subsection{LRA Configurations for \lion}
\label{lraconfig}
For the LRA task, we utilized the same model dimensions as specified in the S5 \citep{s5} paper, following the guidelines from the S5 GitHub repository\footnote{\url{https://github.com/lindermanlab/S5}}. Our state matrix was represented as a vector \(\boldsymbol{\Lambda}_i = \boldsymbol{\lambda}_i\), where each element contains a scalar non-input dependent value \(e^a\). The value \(a\) was initialized based on \textit{HIPPO} theory, alongside the input-dependent \(a_i\), as described in main body.

We employed the ADAMW optimizer with an initial learning rate of \(5 \times 10^{-4}\) and a cosine learning rate scheduler \citep{cossche}. The weights for the queries and keys, as well as the selective component of \(\Lambda\), were initialized using a Gaussian distribution with a standard deviation of 0.1. For the values \(\mathbf{v}\), we initialized \(W_\mathbf{v}\) using zero-order hold discretization, represented as \(W^{\text{init}}_\mathbf{v} = \left({\Lambda}^{-1} \cdot (\Lambda - \mathbf{I})\right)\). The non-selective parts of \(\Lambda\) were initialized based on the \textit{HIPPO} \citep{s5} matrix.


\subsection{Ablation Studies on LRA dataset} \label{laasjdhakjsdh}
We have did an ablation study for choosing the activation functions and using non-scalar decay factor for \lion.

\begin{table}[h] 
    \centering
        \caption{\textit{Effects of different parameter choices and non-linearities in \lions on LRA tasks.} Codes: $[1]$ Sigmoid non-linearity was applied to the $\mathbf{k}$ and $\mathbf{q}$ values with unscaled masked attention; $[2]$ ReLU non-linearity was utilized, and the masked attention was scaled; $[3]$ The parameter $a_i$ was selected as a scalar instead of a vector; $[4]$ \lions model parameters were used without scaling; $[5]$ The attention matrix of \lions was scaled, but attention values were adjusted without the factor of $\lambda_i$; $[6]$ The selective component of $a_i$ was removed; $[7]$ SoftPlus activation function was employed for the $a_i$ values. We used the HIPPO \cite{hippo} initialisation for LRA task since random initalisation of \lions and \lionretnet can not solve LRA.}
    \resizebox{1\textwidth}{!}{
\begin{tabular}{l|lcccccc}
\toprule
 Model & ListOps & Text & Retrieval	& Image	& Pathfinder & PathX & Avg. \\
 (input length) & 2048 & 2048 & 4000 & 1024 & 1024  & 16K & \\
 \bottomrule
$[1]$ $\phi(x) = \sigma(x)$ w.o scaling   & 61.02 & {88.02} & {89.10} & {86.2} & {91.06} & 97.1 & {85.41} \\
$[2]$ $\phi(x) = \textsc{Relu}(x)$ w. scaling   &  36.37 & 65.24 & 58.88 & 42.21 & 69.40 & \xmark &  54.42  \\
$[3]$ $a_i$ only scalar & 36.23 &  60.33 & 60.45 & 58.89 & 70.00 &\xmark & 57.17 \\
$[4]$ \lion w.o scaling & 58.76 & 67.22 & 59.90 & 60.0 & 65.51 & \xmark & 62.27 \\
$[5]$ scaled attention w.o mask  & 60.12 & 87.67 & 87.42 &88.01&89.23 & \xmark& 82.49  \\
$[6]$ $a_i$ From \textit{HIPPO} w.o selectivity & 60.12 & 88.00 & 89.22 & 83.21 &  91.0 & 96.30  & 84.64\\
$[7]$ $a_i=\textsc{SoftPlus}(x)$ &  16.23 & 59.90 & 60.00& 45.12 & 70.07 & \xmark &   50.26 \\
 \bottomrule
\rowcolor{orange!17}
 \textbf{\lions (w/ \textit{HIPPO})}  & \textbf{62.25} & \textbf{88.10} & \textbf{90.35} & \textbf{86.14} & \textbf{91.30} & \textbf{97.99} & \textbf{86.07} \\
         \bottomrule
\end{tabular} }
    \label{tab:lra}
\end{table} 

We have observed that bounding the keys and queries significantly enhances the model's ability to solve tasks. This finding is consistent with the observations in \cite{deltanet}. As demonstrated in variation \([1]\), it can successfully tackle the LRA task even without scaling, while the \textsc{ReLU} activation fails to do so. Additionally, we found that scaling plays a crucial role, particularly when it comes to scaling the masked attention. The approach used in \lion, which scales the attention before applying the mask expressed as \(\mathbf{Y} = \textsc{scale}(\mathbf{Q}\mathbf{K}^{\top}) \odot \mathbf{M}\) has proven ineffective in addressing the challenging PathX task, as shown in \([5]\). Furthermore, the modifications implemented in \lions have demonstrated superior performance compared to all other variations tested.


\subsection{LRA full Results}

We have evaluated \lion variants against benchmarks for long-range sequence modeling across different categories, including softmax-based Transformers, RNNs, SSMs, and Linear Transformers.

  \begin{table*}[h] 
    \centering
        \caption{\textit{Performance on Long Range Arena Tasks.} 
        For each column (dataset), the best and the second best results are highlighted with \textbf{bold} and \underline{underline} respectively. Note that the MEGA architecture has roughly 10$\times$ the number of parameters as  the other architectures.}
        \vspace{-2mm}
    \resizebox{1\textwidth}{!}{
\begin{tabular}{l|lcccccc|cc}
\toprule
Category & Model & ListOps & Text & Retrieval	& Image	& Pathfinder & PathX & Avg. \\
& (input length) & 2048 & \rebuttal{4096} & 4000 & 1024 & 1024  & 16K & \\
 \bottomrule
\multirow{3}{*}{{Transformer}} & Transformer    & 36.37 & 64.27 & 57.46 & 42.44 & 71.40& \xmark &  54.39  \\
& MEGA ($\cO(L^2)$)& \textbf{63.14} & \textbf{90.43} & \underline{91.25} & \textbf{90.44} & \underline{96.01} & 97.98 & \textbf{88.21} \\
& MEGA-chunk ($\cO(L)$) & 58.76 & \underline{90.19} & 90.97 & 85.80 & 94.41 & 93.81 & 85.66 \\
\bottomrule
\multirow{4}{*}{{SSM}} & DSS  & 57.60 & 76.60 & 87.60 & 85.80 & 84.10 & 85.00 & 79.45  \\
& S4 (original)   & 58.35 & {86.82} & {89.46} & 88.19 &  {93.06} & 96.30  & {85.36} \\
& \rebuttal{S5 } & \rebuttal{62.15} & \rebuttal{89.31} & \rebuttal{\textbf{91.40}} & \rebuttal{88.00} & \rebuttal{95.33} & \rebuttal{\textbf{98.58}} & \rebuttal{\underline{87.46}} \\
& \rebuttal{Mamba (From \cite{xlstm})} & \rebuttal{32.5} & \rebuttal{N/A} & \rebuttal{90.2} &\rebuttal{68.9} &\rebuttal{\textbf{99.2}}&  \rebuttal{N/A} & \rebuttal{N/A} \\
 \bottomrule
 \rebuttal{RNN} & \rebuttal{LRU} &\rebuttal{ 60.2}& \rebuttal{89.4}& \rebuttal{89.9} & \rebuttal{\underline{89.0}}& \rebuttal{95.1}& \rebuttal{94.2} & \rebuttal{86.3}\\
 & \rebuttal{xLSTM} (From \cite{xlstm}) & \rebuttal{41.1} & \rebuttal{N/A} & \rebuttal{90.6} &\rebuttal{69.5} &\rebuttal{91.9}&  \rebuttal{N/A} & \rebuttal{N/A} \\
  \bottomrule
\multirow{13}{*}{\begin{tabular}{l} Linear\\ Transformer \end{tabular}} & Local Att.   & 15.82 &	52.98 &	53.39 &	41.46 &	66.63& \xmark &	46.06 \\
& Sparse Transformer   & 17.07	& 63.58 &	59.59 &	44.24 &	71.71& \xmark		& 51.24 \\
& Longformer     & 35.63	& 62.85	& 56.89	& 42.22	& 69.71& \xmark	&  53.46 \\
& Linformer    & 16.13	& 65.90	& 53.09	& 42.34	& 75.30& \xmark	&  	50.55 \\
& Reformer   & 37.27 & 56.10 & 53.40 & 38.07 & 68.50 & \xmark &  50.67 \\
& Sinkhorn Trans.    & 33.67 & 61.20 & 53.83 & 41.23 & 67.45& \xmark & 51.48 \\
& BigBird   & 36.05	& 64.02	& 59.29	& 40.83	& 74.87& \xmark	& 	55.01 \\
& Linear Trans.   & 16.13 & 65.90 & 53.09 & 42.34 & 75.30& \xmark &  50.55 \\
& Performer     & 18.01 & 65.40 & 53.82 & 42.77 & 77.05 & \xmark & 51.41 \\
& FNet    & 35.33 & 65.11 & 59.61 & 38.67 & 77.80 & \xmark &  55.30 \\
& NystrÃ¶mformer    & 37.15 & 65.52 & 79.56 & 41.58 & 70.94& \xmark &  58.95 \\
& Luna-256  & 37.25 & 64.57 & 79.29 & 47.38 & 77.72& \xmark &  61.24 \\
& H-Transformer-1D   & 49.53 & 78.69 & 63.99 & 46.05 & 68.78& \xmark & 61.41 \\
\rowcolor{Green!10} 
&\rebuttal{\lionlit}&\rebuttal{16.78}&\rebuttal{65.21}&\rebuttal{54.00}&\rebuttal{43.29}&\rebuttal{72.78} & \rebuttal{\xmark }&\rebuttal{50.41}\\
\rowcolor{violet!20}
& \lionretnet (w/ \textit{HIPPO})  & 62.0 & 88.78& 90.12& 85.66 & 90.25& 97.28& 85.63\\
\rowcolor{orange!17}
& \lions (w/ \textit{HIPPO}) & \underline{62.25} & {88.10} & {90.35} & {86.14} & {91.30} & \underline{97.99} & 86.07 \\
         \bottomrule
    \end{tabular} }
    \label{tab:lra_exp}
\end{table*}

\subsection{\rebuttal{Experimental details for the MLM/GLUE tasks}}
\label{subsec:details_glue}
\rebuttal{
\textbf{Architectures} We train the BASE (110M parameters) and LARGE (334M parameters) model families from the original BERT paper \citep{devlin2019bert}. For the \lion models, we replace the standard self-attention blocks with \lionlit/\lionretnet/\lions blocks while keeping all hyperparameters the same. For \lionlit, we incorporate LayerNorm \citep{ba2016layernorm} after the attention block to enhance stability. For Hydra, we take the default hyperparameters in \citep{hwang2024hydrabidirectionalstatespace} and increase the number of layers to 45 to match the number of parameters in the LARGE scale. Our implementation is based on the M2 repository \citep{fu2023monarch}, i.e., \url{https://github.com/HazyResearch/m2}.
\\
\\
\textbf{Pretraining} All our pretraining hyperparameters follow \citet{fu2023monarch}: We employ the C4 dataset \citep{dodge2021c4}, a maximum sequence length during pretraining of $128$ and a masking probability of $0.3$ and $0.15$ for the training and validation sets respectively. We train our model for $70,000$ steps with a batch size of $4096$. We employ the decoupled AdamW optimizer with a learning rate of $8\cdot 10^{-4}$, $\beta_1=0.9$, $\beta_2 = 0.98$, $\epsilon = 10^{-6}$ and weight decay $10^{-5}$. As a scheduler, we perform a linear warm-up for $6\%$ of the training steps and a linear decay for the rest of training until reaching $20\%$ of the maximum learning rate.
\\
\\
Our only change in the pretraining hyperparameters is setting the learning rate to $2\cdot10^{-4}$ for the LARGE model family. In our preliminary experiments, we found that training diverged when using a learning rate of $8\cdot10^{-4}$ for BERT-LARGE.
\\
\\
For completeness, we present the results with the BERT pretraining\footnote{\url{https://github.com/HazyResearch/m2/blob/main/bert/yamls/pretrain/hf-transformer-pretrain-bert-base-uncased.yaml}} and BERT 24 finetuning\footnote{\url{https://github.com/HazyResearch/m2/blob/main/bert/yamls/finetune-glue/hf-transformer-finetune-glue-bert-base-uncased.yaml}} recipes available in the M2 repository.
\\
\\
\textbf{Finetuning} For the GLUE finetuning experiments, we employ five different configurations:
\begin{itemize}
    \item \textbf{BERT24}: Available in \citet{izsak2021train} and the file  \url{https://github.com/HazyResearch/m2/blob/main/bert/yamls/finetune-glue/hf-transformer-finetune-glue-bert-base-uncased.yaml}.
    \item \textbf{M2-BASE}: Available in \citet{fu2023monarch}, Section C.1 and the file \url{https://github.com/HazyResearch/m2/blob/main/bert/yamls/finetune-glue/monarch-mixer-finetune-glue-960dim-parameter-matched.yaml}.
    \item \textbf{M2-LARGE}: Available in \citet{fu2023monarch}, Section C.1 and the file \url{https://github.com/HazyResearch/m2/blob/main/bert/yamls/finetune-glue/monarch-mixer-large-finetune-glue-1792dim-341m-parameters.yaml}.
    \item \textbf{Hydra}: Available in \citet{hwang2024hydrabidirectionalstatespace}, Section D.2 and the file \url{https://github.com/goombalab/hydra/blob/main/hydra/bert/yamls/finetune/hydra.yaml}.
    \item \textbf{Modified}: Same as M2-LARGE but all learning rates are set to $10^{-5}$.
\end{itemize}
The recipes are summarized in \cref{table:glue_hyperparams}. The Modified hyperparameter set was devised as M2-LARGE was found to diverge for BERT-LARGE.
\begin{table}[]
    \rebuttal{
    \centering
    \caption{\rebuttal{GLUE finetuning recipes employed in this work. All recipes finetune on RTE, STSB and MRPC from the weights finetuned in MNLI and the rest from the C4-pretrained weights. All recipes use a sequence length of 128 tokens except BERT24 and Hydra, that use $256$. D. AdamW stands for decoupled AdamW.}}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lc|cccccccc}
        \toprule
        \multirow{2}{*}{Recipe} & \multirow{2}{*}{Param.} & \multicolumn{8}{c}{Dataset}\\
         & & MNLI & QNLI & QQP & RTE & SST2 & MRPC & COLA & STSB \\
         \midrule
         \multirow{3}{*}{\begin{minipage}{2.6cm}
             BERT24\\ 
             \citep{izsak2021train}
         \end{minipage}}& LR & $5\cdot10^{-5}$ & $1\cdot10^{-5}$ & $3\cdot10^{-5}$ & $1\cdot10^{-5}$ & $3\cdot10^{-5}$ & $8\cdot10^{-5}$ & $5\cdot10^{-5}$ & $3\cdot10^{-5}$\\
         & WD & $5\cdot10^{-6}$ & $1\cdot10^{-5}$ & $3\cdot10^{-6}$ & $1\cdot10^{-6}$ & $3\cdot10^{-6}$ & $8\cdot10^{-5}$ & $5\cdot10^{-6}$ & $3\cdot10^{-6}$  \\
         & Epochs & 3 & 10 & 5 & 3 & 3 & 10 & 10 & 10 \\
         & Optimizer & D. AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW \\
         \midrule
         \multirow{3}{*}{\begin{minipage}{2.2cm}
             M2-BASE\\ 
             \citep{fu2023monarch}
         \end{minipage}}& LR & $5\cdot10^{-5}$ & $5\cdot10^{-5}$ & $3\cdot10^{-5}$ & $1\cdot10^{-5}$ & $3\cdot10^{-5}$ & $8\cdot10^{-5}$ & $8\cdot10^{-5}$ & $8\cdot10^{-5}$\\
         & WD & $5\cdot10^{-6}$ & $1\cdot10^{-5}$ & $3\cdot10^{-6}$ & $1\cdot10^{-6}$ & $3\cdot10^{-6}$ & $8\cdot10^{-5}$ & $5\cdot10^{-6}$ & $3\cdot10^{-6}$  \\
         & Epochs & 3 & 10 & 5 & 3 & 3 & 10 & 10 & 10 \\
         & Optimizer & D. AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW & AdamW \\
         \midrule
         \multirow{3}{*}{\begin{minipage}{2.2cm}
             M2-LARGE\\ 
             \citep{fu2023monarch}
         \end{minipage}}& LR & $5\cdot10^{-5}$ & $5\cdot10^{-5}$ & $3\cdot10^{-5}$ & $5\cdot10^{-5}$ & $3\cdot10^{-5}$ & $8\cdot10^{-5}$ & $5\cdot10^{-5}$ & $8\cdot10^{-5}$\\
         & WD & $5\cdot10^{-6}$ & $1\cdot10^{-6}$ & $3\cdot10^{-6}$ & $1\cdot10^{-6}$ & $3\cdot10^{-6}$ & $8\cdot10^{-6}$ & $1\cdot10^{-6}$ & $3\cdot10^{-5}$  \\
         & Epochs & 3 & 10 & 5 & 2 & 3 & 10 & 10 & 8 \\
         & Optimizer & D. AdamW & D. AdamW & D. AdamW & AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW \\
         \midrule
         \multirow{3}{*}{\begin{minipage}{2.2cm}
             Hydra\\
             \citep{hwang2024hydrabidirectionalstatespace}
         \end{minipage}}& LR & $10^{-4}$ & $5\cdot10^{-5}$ & $5\cdot10^{-5}$ & $10^{-5}$ & $5\cdot10^{-5}$ & $8\cdot10^{-5}$ & $10^{-4}$ & $3\cdot10^{-5}$\\
         & WD & $5\cdot10^{-6}$ & $10^{-6}$ & $3\cdot10^{-6}$ & $10^{-6}$ & $3\cdot10^{-6}$ & $8\cdot10^{-6}$ & $8\cdot10^{-6}$ & $3\cdot10^{-6}$  \\
         & Epochs & 2 & 7 & 3 & 3 & 2 & 10 & 10 & 8 \\
         & Optimizer & D. AdamW & D. AdamW & D. AdamW & AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW \\
         \midrule
         \multirow{3}{*}{\begin{minipage}{2.2cm}
             Modified\\
             (Ours)
         \end{minipage}}& LR & $10^{-5}$ & $10^{-5}$ & $10^{-5}$ & $10^{-5}$ & $10^{-5}$ & $10^{-5}$ & $10^{-5}$ & $10^{-5}$\\
         & WD & $5\cdot10^{-6}$ & $1\cdot10^{-6}$ & $3\cdot10^{-6}$ & $1\cdot10^{-6}$ & $3\cdot10^{-6}$ & $8\cdot10^{-6}$ & $1\cdot10^{-6}$ & $3\cdot10^{-5}$  \\
         & Epochs & 3 & 10 & 5 & 2 & 3 & 10 & 10 & 8 \\
         & Optimizer & D. AdamW & D. AdamW & D. AdamW & AdamW & D. AdamW & D. AdamW & D. AdamW & D. AdamW \\
         \bottomrule
    \end{tabular}}
    }
    \label{table:glue_hyperparams}
\end{table}
}

\subsection{\rebuttal{Ablation studies in the MLM/GLUE tasks}}
\label{subsec:ablations_glue}

\rebuttal{
\textbf{Combining positional embeddings with \lion.} We compare the GLUE performance of \lionretnet and \lions when including positional embeddings. We pretrain the BASE models and finetune them with the M2-BASE recipe.
}
\begin{table}[t]
    \caption{\rebuttal{Combining positional embeddings with \lionretnet and \lions. Both pretrained models improve in the validation MLM acc. when employing positional embeddings.}}
    \centering
    \rebuttal{
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lc|c|cccccccc|c}
    \toprule
        Model & Pos. Emb. & MLM Acc. & MNLI & RTE & QQP & QNLI & SST2 & STSB & MRPC & COLA & Avg.\\
        \midrule
        \multirow{2}{*}{\lionretnet} & \textcolor{tabred}{\xmark} & 66.62 & 82.85 & 52.49 & 89.63 & 88.43 & 91.86 & 85.96 & 83.94 & 53.58 & 78.59 \\
        & \textcolor{tabgreen}{\cmark} & 66.97 & 83.37 & 54.08 & 89.52 & 88.32 & 92.35 & 83.58 & 79.40 & 54.53 & 78.15 \\
        \midrule
        \multirow{2}{*}{LION-s} & \textcolor{tabred}{\xmark} & 67.05 & 83.17 & 53.50 & 89.35 & 88.89 & 93.00 & 37.73 & 77.87 & 53.18 & 72.09\\ 
        & \textcolor{tabgreen}{\cmark} & 67.35 & 83.26 & 52.42 & 89.82 & 88.38 & 92.58 & 83.87 & 79.54 & 55.25 & 78.14\\
        \bottomrule
    \end{tabular}}
    }

    \label{tab:ablation_posemb}
\end{table}

\rebuttal{
In \cref{tab:ablation_posemb} we can observe that adding positional embeddings increased the MLM acc. in around $0.3$ percentage points. In the GLUE benchmark, we observe that for \lionretnet performance degraded in $0.44$ percentage points, while for \lions, performance improved in $6.05$ percentage points. We attribute this behavior in GLUE to the dependence on the finetuning recipe.
}

\rebuttal{
\textbf{Recipe selection.} In this section, we select the best finetuning recipe for each model family and size. For the BASE models, we test the M2-BASE and Modified recipes. For the LARGE models, we test the M2-LARGE and Modified recipes.
\\
\\
In \cref{tab:ablation_glue_recipe}, firstly, we observe that the M2-BASE recipe generally provides a higher GLUE score than the Modified recipe for the BASE models, e.g., $82.25$ v.s. $80.26$ for the BERT model. Secondly, we observe that for the LARGE model family, the M2-LARGE recipe fails, providing poor performances between $60.96$ and $72.41$ GLUE points. When reducing the learning rate to $10^{-5}$ (Modified recipe), training is more stable and performance reaches between $80.76$ and $82.95$ GLUE points. We find that small changes in the finetuning recipe have a large effect in the performance. Our results in standard recipes show that the \lion family of models can obtain a high performance without extensive tuning and closely follow the performance of the BERT family models, at $80.31$ v.s. $82.25$ for the BASE model size and $81.58$ v.s. $82.95$ for the LARGE model size.
}

\begin{table}[t]
    \caption{\rebuttal{Recipe selection for the GLUE benchmark.}}
    \centering    
    \resizebox{\textwidth}{!}{
    \rebuttal{
    \begin{tabular}{lcc|cccccccc|c}
    \toprule
        Model & MLM Acc. & Recipe & MNLI & RTE & QQP & QNLI & SST2 & STSB & MRPC & COLA & Avg.\\
        \midrule
        \multirow{2}{*}{BERT} & \multirow{2}{*}{67.70} & M2-BASE & 84.63 & 64.33 & 89.99 & 89.80 & 92.51 & 86.69 & 89.62 & 60.42 & 82.25\\
        & & Mod. & 83.09 & 58.27 & 89.35 & 89.88 & 92.16 & 86.56 & 87.78 & 55.02 & 80.26\\
        \midrule
        \multirow{2}{*}{\lionlit} & \multirow{2}{*}{65.47} & M2-BASE & 82.50 & 63.47 & 89.72 & 89.27 & 91.74 & 87.18 & 89.37 & 49.22 & 80.31\\
        & & Mod. & 80.88 & 54.95 & 88.80 & 88.83 & 91.32 & 85.42 & 87.07 & 46.98 & 78.03 \\
        \midrule
        \multirow{2}{*}{\lionretnet} & \multirow{2}{*}{66.62} & M2-BASE  & 82.85 & 52.49 & 89.63 & 88.43 & 91.86 & 85.96 & 83.94 & 53.58 & 78.59 \\
        & & Mod. & 80.52 & 52.85 & 88.93 & 88.36 & 91.55 & 82.05 & 84.48 & 49.13 & 77.23 \\
        \midrule
        \multirow{2}{*}{\lions} & \multirow{2}{*}{67.05} & M2-BASE & 83.17 & 53.50 & 89.35 & 88.89 & 93.00 & 37.73 & 77.87 & 53.18 & 72.09\\ 
        & & Mod. & 78.14 & 56.39 & 88.68 & 88.52 & 92.39 & 51.22 & 77.60 & 49.75 & 72.84 \\ 
        \midrule
        \multirow{2}{*}{BERT$_{\text{LARGE}}$} & \multirow{2}{*}{69.88} & M2-LARGE & 84.97 & 69.10 & 31.59 & 49.15 & 91.93 & 53.61 & 87.87 & 51.16 & 64.92\\
        & & Mod. & 85.68 & 67.44 & 89.90 & 91.89 & 93.04 & 88.63 & 90.89 & 56.14 & 82.95\\ 
        \midrule
        \multirow{2}{*}{Hydra$_{\text{LARGE}}$} & \multirow{2}{*}{71.18} & Hydra & 84.24 & 60.44 & 89.24 & 89.73 & 91.70 & 88.21 & 88.99 & 47.72 & 80.03\\
        & & Mod. & 84.39 & 59.42 & 90.38 & 91.31 & 93.43 & 87.19 & 88.57 & 59.46 & 81.77 \\
        \midrule
        \multirow{2}{*}{\lionlit$_{\text{LARGE}}$} & \multirow{2}{*}{67.11} & M2-LARGE & 83.20 & 54.51 & 89.08 & 84.90 & 90.44 & 68.57 & 85.25 & 23.35 & 72.41 \\
        & & Mod. & 83.73 & 57.18 & 89.85 & 89.93 & 91.86 & 88.02 & 90.18 & 55.36 & 80.76 \\
        \midrule
        \multirow{2}{*}{\lionretnet$_{\text{LARGE}}$} & \multirow{2}{*}{68.64} & M2-LARGE & 83.82 & 52.85 & 41.48 & 53.67 & 91.13 & 36.87 & 82.41 & 45.79 & 61.00 \\ 
        & & Mod. & 83.82 & 60.72 & 89.72 & 89.79 & 92.93 & 87.29 & 89.66 & 56.83 & 81.34 \\ 
        \midrule
        \multirow{2}{*}{\lions$_{\text{LARGE}}$} & \multirow{2}{*}{69.16} & M2-LARGE & 83.71 & 50.04 & 38.81 & 53.98 & 91.59 & 36.98 & 82.29 & 50.27 & 60.96 \\
        & & Mod. & 84.38 & 57.69 & 89.57 & 90.30 & 92.93 & 87.68 & 90.57 & 59.54 & 81.58 \\
        \bottomrule
    \end{tabular}}
    }
    \label{tab:ablation_glue_recipe}
\end{table}

\subsection{\rebuttal{Additional experimental results for the MLM/GLUE tasks}}
\label{subsec:glue_small}

In this section, we present our bidirectional MLM results in the BASE scale using the BERT pretraining recipe described in \cref{subsec:details_glue} and BERT24 \citep{izsak2021train} finetuning recipes (\cref{tab:MLM_small}), we present the per-task GLUE results omitted in the main text (\cref{tab:MLM_full}) and present the length scaling capabilities of the \lions model (\cref{fig:bert_memory}).

\begin{table}[ht]
\centering
\caption{\textit{C4 Masked Language Modelling and GLUE results for the BASE scale ($110$M).}}
\vspace{-2mm}
\begin{tabular}{l|cccccccccc}
\toprule
Model & MLM Acc. & MNLI & RTE & QQP & QNLI & SST2 & STSB & MRPC & COLA & Avg. \\
\midrule
BERT        & {67.23}  & 84.26         & {59.21}  & 89.87         & 90.24         & {92.35}  & {88.12}  & {90.24}  & 56.76         & 81.38         \\ 
Hydra$^{*}$    & {69.10}     & {84.50} & {57.20}  & {91.30} & {90.00} & {93.50}  & {91.20}  & {88.90}  & {77.50} & {84.30} \\
\rowcolor{Green!10}
\lionlit    & 65.08              & 82.37         & 55.81           & 89.49         & {89.57}  & 91.74         & 86.27         & 88.25         & 44.46         & 78.50         \\ 
\rowcolor{violet!20}
\lionretnet & 66.62              & 82.85         & 52.49           & {89.63}  & 88.43         & 91.86         & 85.96         & 83.94         & 53.58         & 78.59         \\
\rowcolor{orange!17}
\lions      & 66.19              & 82.50         & {57.47}  & 89.38         & 87.88         & {92.70}  & 82.42         & 82.46         & {53.39}  & 78.40         \\ 
\bottomrule
\multicolumn{11}{l}{\begin{footnotesize}
    * Results extracted from the original paper \citep{hwang2024hydrabidirectionalstatespace}.
\end{footnotesize}}
\end{tabular}
\label{tab:MLM_small}
\end{table}

\begin{table*}[t]
    \caption{\textit{C4 Masked Language Modelling and GLUE results for the LARGE scale ($334$M).} For each column (dataset), the best and the second best results are highlighted with \textbf{bold} and \underline{underline} respectively.}
    \centering
    \vspace{-2mm}

    \resizebox{\textwidth}{!}{
    \rebuttal{
    \begin{tabular}{l|c|cccccccc|c}
    \toprule
        Model & MLM Acc. & MNLI & RTE & QQP & QNLI & SST2 & STSB & MRPC & COLA & Avg.\\
        \midrule
        BERT  & $69.88$ & $\mathbf{85.68}$ & $\mathbf{67.44}$ & $\underline{89.90}$ & $\mathbf{91.89}$ & $\underline{93.04}$ & $\mathbf{88.63}$ & $\mathbf{90.89}$ & ${56.14}$ & $\mathbf{82.95}$\\ 
        Hydra  & $\mathbf{71.18}$ & \underline{84.39} & 59.42 & $\mathbf{90.38}$ & $\underline{91.31}$ & $\mathbf{93.43}$ & 87.19 & 88.57 & \underline{59.46} & \underline{81.77} \\
        \rowcolor{Green!10} \lionlit  & 67.11 & 83.73 & 57.18 & $89.85$ & 89.93 & 91.86 & $\underline{88.02}$ & 90.18 & 55.36 & 80.76 \\
        \rowcolor{ violet!20}
        \rebuttal{\lionretnet}  & \rebuttal{68.64}& \rebuttal{83.82} & \rebuttal{\underline{60.72}} & \rebuttal{89.72} & \rebuttal{89.79} & \rebuttal{92.93} & \rebuttal{87.29} & \rebuttal{89.66} & \rebuttal{56.83} & \rebuttal{81.34} \\ 
        \rowcolor{orange!17} \lions  & $\underline{69.16}$ & $84.38$ & ${57.69}$ & 89.57 & $90.30$ & $92.93$ & 87.68 & $\underline{90.57}$ & $\mathbf{59.54}$ & $81.58$ \\
        \bottomrule
    \end{tabular}}}
    \label{tab:MLM_full}

\end{table*}


\begin{figure}[t] 
    \centering
    \includegraphics[width=0.45\columnwidth]{figs/XSequenceLength_yAcc.pdf}
    \includegraphics[width=0.45\columnwidth]{figs/text_xRes_yMem.pdf}
    \vspace{-1mm}
  \caption{%
  \textit{(Left) MLM Acc. for different sequence lengths.} \lions is able to generalize to larger sequence lengths and does not require positional encoddings. \textit{(Right) GPU Memory for different sequence lengths.} Results for the LARGE (334M) scale. The memory employed by \lion and Hydra scales linearly with the sequence length, being able to process a sequence length of $\sim 16.000$ tokens with less than 20GB. Contrarily, the memory employed by BERT scales quadratically, going out of memory ($80$GB) at a sequence length of $\sim 12.000$ tokens.
  }
  \label{fig:bert_memory}
\end{figure}

\subsection{ImageNet classification results for Tiny scale}
\label{app:tiny}

In \cref{tab:imc_tiny}, we present the image classification results of \lion models on the tiny scale models an compare them against the baseline models. Results indicate that the high training speed and competitive performance of \lion models is also applicable in the tiny scaled models.

\begin{table}[t]
        \captionof{table}{\textit{Image classification task in Tiny scale.} We present the Top-1 accuracy on the validation data.* represents the changing in patch orders.
        For each scale, the best and the second best results for each model are highlighted with \textbf{bold} and \underline{underline} respectively.
        }
        \label{tab:imc_tiny}
        \begin{minipage}{\linewidth}
          \centering
                \begin{tabular}{clccc}
                \toprule
                    & Model    & $\#$Param & \begin{tabular}{@{}l@{}}  Imagenet \\ Top-1 Acc. \end{tabular}   & Train. time\\
                     \bottomrule
\parbox[t]{3mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{Tiny}}} & ViT      & 5M &  70.2 & $\times 1$\\
                    & DeiT      & 5M &   72.2 & $\times 1$\\
                    &\rebuttal{Vim  }  & 7M & 76.1 & $\times 9.48$\\%_{\pm 0.17}$\\
                    &\cellcolor{Green!10} \lionlit   & \cellcolor{Green!10} 5M & \cellcolor{Green!10} 68.9 & \cellcolor{Green!10}$\times 0.69$\\%_{\pm 0.01}$\\
                    &\cellcolor{ violet!20}
                    \rebuttal{\lionretnet}   & \cellcolor{ violet!20} 5M & \cellcolor{ violet!20} 72.4  & \cellcolor{ violet!20}$\times 1.48$\\%_{\pm 0.03}$\\ 
                    &\cellcolor{ violet!20}
                    \textbf{\lionrotd}   & \cellcolor{ violet!20} 5M & \cellcolor{ violet!20} 74.2 & \cellcolor{ violet!20}$\times 1.73$\\%_{\pm 0.04}$\\
                    &\cellcolor{orange!17}
                    \textbf{\lions}   & \cellcolor{orange!17} 5M & \cellcolor{orange!17} 72.4 & \cellcolor{orange!17}$\times 2.05$\\%_{\pm 0.21}$\\
                    &\cellcolor{orange!17}
                    \textbf{\lionrot}   & \cellcolor{orange!17} 5M & \cellcolor{orange!17} 73.5 & \cellcolor{orange!17}$\times 3.83$\\%_{\pm 0.17}$\\
                     \bottomrule
                     \end{tabular} 
        \end{minipage}
\end{table} 

\subsection{The inference time/memory trade off}
\label{subsec:chunks}

In \cref{fig:chunks}, we illustrate the trade-off between memory consumption and inference time. Across all models, RNN proves to be the most memory-efficient approach, while full attention is the most demanding. Both chunking and full attention goes out of memory sooner than RNN. Similar to \lionretnet, chunking achieves inference times comparable to, and occasionally better than, full attention. However, in the case of \lions, chunking is faster than RNN at lower resolutions but becomes slower at higher resolutions due to the computational cost of mask calculation. Consequently, while chunking is preferable for \lionlit and \lionretnet when memory permits, at high resolutions, RNN can be a better choice when dealing with more complex masks.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
    \includegraphics[width=\columnwidth]{figs/linear_chunking.pdf}
         \caption{\lionlit}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
    \includegraphics[width=\columnwidth]{figs/selective_chunking.pdf}
         \caption{\lions}
     \end{subfigure}
    \caption{\textit{The memory-time trade off.}The inference memory and time plots of \lionlit and \lions models in three formats: RNN, chunking and full attention.}
    \label{fig:chunks}
\end{figure}




\subsection{Inference time comparison for image classification tasks}
\label{subsec:inf_time}

In \cref{fig:inf_time}, we compare the baseline models on the image classification task. At lower resolutions, all models exhibit similar inference times, with Vim and Hydra being slightly slower than ViT and \lionretnet chunking. However, at higher resolutions, vision SSMs (Vim and Hydra) become faster. This trend arises because vision SSMs leverage specialized kernels, whereas ViT and \lionretnet rely on plain Python implementations, leading to increasing overhead as resolution grows.

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.5\columnwidth]{figs/inference_time.pdf}
    \vspace{-1mm}
  \caption{\looseness=-1 \textit{Inference times comparison.} The inference time of \lionretnet with chunking, Vim, Hydra and ViT models are presented for different resolutions.
   }
  \label{fig:inf_time}
\end{figure}

\subsection{Ablation studies with image classification}
\label{subsec:image_ablation}

\textbf{Resolution vs. Accuracy.} The most common practice in the literature on Vision Transformers is to resize images to $224\times224$ even though most of the images in the ImageNet dataset are larger. Since regular Transformers have positional embedding, it is not possible to use a larger resolution during inference than the training. However, since the \lions architecture does not include any positional embeddings, it can be used with different resolutions. In Figure \ref{fig:res_acc}, we present the accuracy of the architectures trained on $224\times224$ resolution on the ImageNet dataset at different inference resolutions. As the results illustrate, the abilities of \lions can be effectively transferred among different resolutions.


\begin{figure*}[!b]
    \vspace{-3mm}
    \centering
    \includegraphics[width=0.7\textwidth]{figs/XRes_yAcc.pdf}
  \caption{\textit{Top-1 accuracy on Imagenet of the models at different resolutions.} Images are resized at the corresponding resolution and fed into the model. Due to positional embeddings, ViT and \lionlit models cannot perform with sizes larger than the training size while \lions can preserve the accuracy for much higher resolutions. }
    \label{fig:res_acc}
    \vspace{-3mm}
    
\end{figure*}


\textbf{Choice of $\lambda_i$ values.}

In this section, we study the properties of the selectivity parameter $a_i$ on CIFAR-100 dataset. We tested, three cases: ($i$) fixed mask with scalars $a_i = a^i$, ($ii$) vector, input-dependent \(\mathbf{a}_i \in \mathbb{R}^d\) (\textit{cf.}, \cref{sec:expandai}) and iii) input dependent scalar \(\mathbf{a}_i \in \mathbb{R}\). The results, presented in Table~\ref{tab:ablation_lambda}, show that while the input dependency is beneficial, the expansion of \(\mathbf{a}_i \) is not necessary for image tasks. As a result, we employ option three in all image classification tasks, and the end model is called \lions.

\begin{table}[h]
 \caption{\textit{Ablation studies on image classification.} Additional ablations with CIFAR100 dataset to determine the size and input dependency of the selectivity parameter of the model \lions.}
     \centering

\begin{tabular}{l|c}
    \toprule
    Models & Top-1 Acc.\\
    \midrule
    Fixed mask $a_i = a^i$ & 75.66 \\
    Vector $\mathbf{a}_i \in \mathbb{R}^d$ & 67.55 \\
     \rowcolor{orange!17}
    Scalar, input dependent \(\mathbf{a}_i \in \mathbb{R}\) (\lions) &  \textbf{77.56} \\
    \bottomrule
\end{tabular} 
   \label{tab:ablation_lambda}
\end{table}

\textbf{Understanding the power of non-linearity, softmax, and positional embeddings.} In Table~\ref{tab:ablation_extensive}, we present additional ablations on certain design elements of a Vision Transformer. We perform these experiments on CIFAR-100 data using the same hyperparameters with \lions. We have observed that either nonlinearity or softmax is essential for the model to converge with a nice accuracy. Though positional embedding boosts the accuracy, a mask can easily replace it. 

\begin{table}[h]
 \caption{\textit{Ablation studies on image classification.} Additional ablations with the CIFAR-100 dataset to understand the contribution of softmax, nonlinearities in a model is presented. Soft., PosEmb and NonLin expresses if softmax, positional embedding, and non-linearity have been applied. \xmark ~means the model did not converge. The \legendsquare{green!10} symbol denotes the adaptation of recurrent models that achieve equivalence to attention during training while utilizing recurrence during inference, as established by our theorem.}
    \centering
\begin{tabular}{l|c}
    \toprule
    Models & Top-1 Acc.\\
    \midrule
    $[1]$ Soft. + PosEmb + NonLin & 73.88 \\
    $[2]$ Soft. + PosEmb (ViT-T)   & 77.33\\
    $[3]$ Soft. + NonLin           & \xmark \\
    $[4]$ Soft.                     & 73.15 \\
    \rowcolor{Green!10}$[5]$ PosEmb + Non.Lin (\lionlit) & 73.61\\
    \rowcolor{Green!10}$[6]$ PosEmb         & 68.54 \\
    \rowcolor{Green!10}$[7]$ NonLin        & 65.28\\
    \rowcolor{Green!10}$[8]$ Base       & \xmark \\
    \midrule
    \rowcolor{orange!17}
    Non.Lin + Mask (\lions) & \textbf{77.56} \\
    \bottomrule
\end{tabular} 
   \label{tab:ablation_extensive}
\end{table}



\subsection{Hyperparameters for Training Image Classifiers}
\label{subsec:image_hyper}

All experiments were conducted on a single machine for CIFAR-100 and multiple machines for ImageNet, using NVIDIA A100 SXM4 80GB GPUs. For \lion models and Hydra, the ViT architecture serves as the main structure, with Hydra following the Hydra training recipe and other models following the ViT recipe. The training and evaluation codes are adapted from ~\cite{Touvron2020Training} and ~\cite{rw2019timm}.

\subsection{\rebuttal{Calculation of Number of FLOPS}}
\label{subsec:flops}
\rebuttal{Below we present a theoretical number of FLOPS used in the attention of vision transformers and \lions during inference where $L$ is the resolution/context length and $D$ is the hidden dimension. Results show that while transformer has $\cO(L^2+LD^2)$ \lions has $\cO(LD^2)$. Note that in this calculation, the exponentials and other nonlinearities are considered as 1 FLOP whereas in reality, the Softmax introduces additional complexities. The same calculations should also apply to other bi-directional models. 
\\
\\
The number of FLOPs in the one head of the one layer attention for a vision transformer:
\begin{itemize}
    \item Calculating  $\mathbf{Q}, \mathbf{K}, \mathbf{V}$: $6 L D^2$,
    \item Attention $ A = \mathbf{Q} \mathbf{K}^T$ : $2L^2 D$
    \item Softmax (assuming 1 FLOP for exp): $2 L^2$
    \item Calculating $\mathbf{Y}$: $ 2 L^2 D$
    \item \textbf{TOTAL:} $L(6D^2 + 4LD+ 2L)$
\end{itemize}
The number of FLOPs in the attention module for \lion:
\begin{itemize}
    \item Calculating $\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{\lambda}$: $6LD^2 + 2LD$,
    \item For each token in one forward/backward recurrence:
    \begin{itemize}
        \item Updating $\mathbf{S}_i^{F/B}$: $3 D^2$
        \item Updating $\mathbf{z}_i^{F/B}$: $2D$
        \item Calculating $c_i^{F/B}$: $4D + 2$
        \item Calculating $\mathbf{y}_i^{F/B}$: $2D^2 + 4D+1$
        \item Total: $5D^2 +10D+3$
    \end{itemize} 
    \item L forward + backward recurrences:  $2L(5D^2 +10D+3)$
    \item Calculating $\mathbf{Y}$: $2L(D+1)$
    \item \textbf{TOTAL:} $L(16D^2 + 24D+ 7) $
\end{itemize}
}



\subsection{\rebuttal{Distillation Results of LION-S}}
\label{app:distill}
\rebuttal{We have also used the same recipe from DeiT distillation \cite{deit} and distilled the RegNet network into LION-S. We observed that the distillation outperforms the original ViT-Tiny on the ImageNet dataset. The results are shown in the table below:}
\begin{table}[h]
 \caption{\textit{\rebuttal{Distillation results of \lions.}}}
    \centering
    \rebuttal{
\begin{tabular}{l|c}
    \toprule
    Models & Top-1 Acc.\\
    \hline
    \lions & 67.95 \\
    VIT-Tiny & 70.23 \\
    \rowcolor{orange!17}
    \lions (Distilled) &\textbf{ 70.44} \\
    \bottomrule
\end{tabular} }
   \label{tab:destill}
\end{table}






\subsection{\rebuttal{Ablation studies in mapping of autoregressive models to \lion framework}}
\label{app:direction2}
\rebuttal{ Building on the mapping of autoregressive models in \cref{sec:map}, we conducted additional experiments using \lionretnet and \lion-\textsc{GRFA}. Specifically, we modified the transformer block of the VIT-Tiny model according to the proposed mapping and evaluated its performance on the CIFAR-100 dataset, maintaining the same training recipes as \lions. The results, summarized in \cref{tab:grfa}, demonstrate that the \lion framework facilitates the seamless extension of other autoregressive models to a bi-directional setting, achieving strong performance without requiring additional hyperparameter tuning.}

\begin{table}[h]
    \centering
    \caption{\rebuttal{\textit{Mapping of autoregressive models to bidirectional setting with \lion framework.}. These models benefit from the expansion to the bi-directional setting using the \lion framework.}}
    \rebuttal{
    \begin{tabular}{l|c}
    \toprule
        \textbf{Model} & \textbf{Top-1 Acc.} \\
        \midrule
        \textsc{GRFA} (Uni-directional)       & 71.56 \\
        \lion-\textsc{GRFA}  (Bi-directional)   & 73.24 \\
        \hline
        \textsc{RetNet}  (Uni-directional)      & 72.24  \\
        \rowcolor{ violet!20} \lionretnet (Bi-directional) & 75.66  \\
        \hline
    \end{tabular}}
    \label{tab:grfa}
\end{table}

\subsection{\rebuttal{Ablation studies on importance of bi-directionality on image classification}}
\label{app:direction}

\rebuttal{To highlight the importance of bi-directionality and demonstrate the versatility of the \lion framework, we conducted additional experiments examining the processing directions of the blocks. We evaluated four settings: (i) all blocks process patches in the forward direction only (Forward), (ii) all blocks process patches in the backward direction only (Backward), (iii) odd-numbered blocks process patches in the forward direction while even-numbered blocks process them in the backward direction (Forward-Backward), and (iv) all blocks process patches in both directions (Bi-directional). The results reveal that incorporating both directions improves performance by approximately $4\%$, while full bi-directionality achieves a significant boost of up to $10\%$.}

\begin{table}[h]
\centering
\caption{\rebuttal{Results for \lions and \lionrot with different directional settings on CIFAR-100. Incorporating both directions improves performance by approximately $4\%$, while full bi-directionality achieves a significant boost of up to $10\%$.}}
\label{tab:forward}
    \rebuttal{
    \begin{tabular}{l|c}
    \toprule
    \textbf{Model}                             & \textbf{Top-1 Acc.}  \\ 
    \midrule
    \lions (Forward)                       & 71.08          \\ 
    \lions (Backward)                      & 69.61          \\ 
    \lions (Forward-backward)                   & \underline{73.93}\\ 
    \rowcolor{orange!17} \textbf{\lions (Bi-directional)}            & \textbf{77.56} \\ 
    \hline
    \lionrot (Forward)                  & 70.24          \\ 
    \lionrot (Backward)                 & \underline{70.42} \\ 
    \rowcolor{orange!17} \textbf{\lionrot (Bi-directional)}       & \textbf{80.07} \\ 
    \hline
    \end{tabular}}
\end{table}
