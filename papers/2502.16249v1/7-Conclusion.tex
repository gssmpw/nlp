
\looseness=-1We introduce \lion, a framework that formulates full linear attention as a bidirectional RNN, enabling linear transformers to achieve the training speed of softmax-based Transformers while maintaining the inference efficiency of RNNs and SSMs for bidirectional tasks. \lion enjoys stable training with 9$\times$ faster speed than Vision Mamba and $\sim 2\times$ faster than Hydra, while maintaining competitive performance in tasks like image classification and masked language modeling. Furthermore, \lion Chunk, our chunkwise parallelization strategy, balances memory and speed during inference, offering flexibility based on resource availability. Our results demonstrate \lion's effectiveness in balancing performance, scalability, and efficiency for bidirectional sequence modeling.  






