


\looseness=-1\paragraph{Notation.}
Matrices (vectors) are denoted by uppercase (lowercase) boldface letters, such as $\mathbf{X}$ for matrices and $\mathbf{x}$ for vectors. Scalars are represented by  lowercase letters, e.g., $x$, and the Hadamard product is denoted by $\odot$.



\subsection{Causal Linear Transformers: Transformers with Linear Attention} Given a data sequence \(\mathbf{X} = [\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{L}]^\top \in \mathbb{R}^{L\times d}\), a single-head softmax-attention uses a softmax function to normalize the attention scores:

\vspace{-5mm}
\begin{align}
    \left(\mathbf{q}_{i} , \mathbf{k}_{i} , \mathbf{v}_{i}\right) & = \left(\mathbf{W}_{ \mathbf{q}}\mathbf{x}_{i} ,  \mathbf{W}_{ \mathbf{k}}\mathbf{x}_{i} ,  \mathbf{W}_{ \mathbf{v}}\mathbf{x}_{i}\right) , \\
     \mathbf{y}_i & = \sum_{j=1}^i \frac{\exp (\mathbf{q}_{i}^{\top}\mathbf{k}_{j})}{\sum_{p=1}^i \exp( \mathbf{q}_{i}^{\top}\mathbf{k}_{p})} \mathbf{v}_j, \label{eq:attgen}
\end{align}
where $\mathbf{x}_{i} ,\mathbf{q}_{i} , \mathbf{k}_{i} , \mathbf{v}_{i}, \mathbf{y}_{i} \in \mathbb{R}^d$ and the weights $\mathbf{W}_\mathbf{q} , \mathbf{W}_\mathbf{k} , \mathbf{W}_\mathbf{v} \in \mathbb{R}^{d \times d}$ with $d$ being the projection dimension. With {$\mathbf{Q}:=[\mathbf{q}_1,\dots,\mathbf{q}_L]^\top$, $\mathbf{K}:=[\mathbf{k}_1,\dots,\mathbf{k}_L]^\top$, $\mathbf{V}:=[\mathbf{v}_1,\dots,\mathbf{v}_L]^\top \in \mathbb{R}^{L \times d}$}, we can then express the attention layer output as the following matrix form:
\begin{equation}\label{eq:attvec}
   \mathbf{Y} = \text{softmax}\left(\mathbf{Q} \mathbf{K}^\top  \odot \mathbf{M}^C \right) 
    \mathbf{V},
\end{equation}
where $\mathbf{M}^C \in \{ -\infty,1 \}^{L\times L}$ is a causal mask for preventing future tokens to attend to past. Such matrix form is crucial for parallelized training over the sequence length. 

In contrast, \eqref{eq:attgen} is used during inference for generating or processing tokens. However, for causal Transformers \citep{gpt}, employing \eqref{eq:attgen} requires storing the previous \(L\) tokens to attend to the latest token during inference (i.e., the KV cache). This approach is less efficient than RNNs, where only the state is stored regardless of the previous sequence (\textit{cf.}, \cite{lru}). 


\looseness=-1\citet{trans_rnn} introduces Linear Attention which replaces the exponential kernel $\exp (\mathbf{q}_{i}^{\top}\mathbf{k}_{j})$ with feature map function $\phi(\mathbf{q}_{i})^{\top}\phi(\mathbf{k}_{j})$ where $\phi(\cdot): \mathbb{R}^n \rightarrow \mathbb{R}^d$ maps the input to a higher-dimensional space.
For simplicity of notation, we use $\mathbf{q}_i := \phi(\mathbf{W_q} \mathbf{x}_i)$ and similarly for $\mathbf{k}_i:= \phi(\mathbf{W_k}\mathbf{x}_i)$ in the sequel.
This formulation allows the linear transformer to be expressed as an RNN with linear recurrence.\footnote{This models with 2D hidden state also known as "fast weights" \cite{fw1,fw2} and their connection to transformers were explored in \citep{schlag2021linear}.} This structure eliminates the need to store previous tokens during inference, while still enabling parallelized computation during training:

\vspace{-7mm}
\hspace{-2mm}
\begin{minipage}[t]{0.49\textwidth}
\begin{align}\label{equ:linear_attention}
   \mathbf{Y} & = \textsc{Scale}\left(\mathbf{Q} \mathbf{K}^\top  \odot \mathbf{M}^C \right) 
    \mathbf{V}, \\
    \mathbf{S}_i &= \mathbf{S}_{i-1} + \mathbf{k}_i\mathbf{v}_i^\top, \quad  \mathbf{z}_i = \mathbf{z}_{i-1} + \mathbf{k}_i, \quad  \mathbf{y}_i = \frac{\mathbf{q}_i^\top\mathbf{S}_i}{\mathbf{q}_i^\top\mathbf{z}_i}
\end{align} 
\end{minipage}
Here, the causal mask $\mathbf{M}^C \in \{ 0,1 \}^{L\times L}$ is a lower triangular matrix that enforces causal constraints. \(\textsc{scale}(\cdot)\) denotes the scaling of the attention matrix across its rows (\(\textsc{scale}(\mathbf{A})_{ij} = \frac{\mathbf{A}_{ij}}{\sum_{p=1}^L \mathbf{A}_{ip}}\)) and $\mathbf{S}_i \in \mathbb{R}^{d \times d}$ and $\mathbf{z}_i \in \mathbb{R}^{d}$ are the hidden state matrix and the vector used for scaling. \Cref{equ:linear_attention} can also written as $\mathbf{Y} = \mathbf{P}\mathbf{V}$ with $\mathbf{P} \in \R^{L\times L}$ known as sequence mixer \cite{hwang2024hydrabidirectionalstatespace}. 

However, Linear Attention still relies on transformer-like positional encodings \cite{vaswani_attention_2017}. This issue can be addressed by incorporating decay factors into the state update:
\vspace{-10mm}

\hspace{-5mm}
\scalebox{0.9}{
\begin{minipage}[t]{0.55\textwidth}
\begin{align}\label{equ:lintransuni}
    \mathbf{Y}  &= \textsc{Scale}\left(\mathbf{Q} \mathbf{K}^\top  \odot \mathbf{M}^C \right) 
    \mathbf{V}, \quad 
    \mathbf{S}_i = \lambda_i \mathbf{S}_{i-1} + \mathbf{k}_i\mathbf{v}_i^\top. 
\end{align} 
\end{minipage} } 

The mask $\mathbf{M}^C \in \mathbb{R}^{L \times L}$ is a lower-triangular mask generated based on the input-dependent (selective) parameter $\lambda_i$ as bellow:
\vspace{-3mm}
\begin{align}
\label{eq:maskselcause}
     \mathbf{M}^C_{ij} = 
    \begin{cases} 
    \Pi_{k=j+1}^{i}{\lambda_k}, & i \geq j;  \\
    0, & i < j.
\end{cases} 
\end{align}

\vspace{-4mm}
 Several Linear Transformers have been developed based on Equation~\eqref{equ:lintransuni}, such as \citet{performer,peng2021random,xlstm,retnet,yang2023gated}. These approaches can be unified as:
\vspace{-5mm}

\hspace{-0.3cm}
\scalebox{0.9}{
\begin{minipage}[t]{0.52\textwidth}
  \begin{subequations}
    \begin{align}  
    \label{eq:lrm1}
    & \mathbf{S}_i = \textcolor{black}{\boldsymbol{\Lambda_i}} \hspace{0.3mm}  \hspace{0.3mm} \textcolor{black}{\star} \mathbf{S}_{i-1} +  \textcolor{black}{\boldsymbol{\gamma_i}} \hspace{0.5mm} \tikz\draw[white,fill=black] (0,0) circle (.5ex); 
    \mathbf{k}_i
    \mathbf{v}_i^{\top}, \quad
    \mathbf{z}_i  = \textcolor{black}{\boldsymbol{\Lambda_i}} \hspace{0.5mm} \tikz\draw[white,fill=black] (0,0) circle (.5ex); \hspace{0.5mm} \mathbf{z}_{i-1} +  \textcolor{black}{\boldsymbol{\gamma_i}} \hspace{0.5mm} \tikz\draw[white,fill=black] (0,0) circle (.5ex); \hspace{0.5mm} 
    {\mathbf{k}_i}
    , \\
    & \textsc{S}\textsc{caled}: \mathbf{y}_i= \frac{{
    {\mathbf{q}_i}
    }^{\top} \mathbf{S}_i}{{
    {\mathbf{q}_i}
    }^{\top} \mathbf{z_i}}, \quad
    \textsc{Non-Scaled}: \mathbf{y}_i= {
    {\mathbf{q}_i}
    }^{\top} \mathbf{S}_i,
    \end{align}
  \end{subequations}
\end{minipage} }

Where \(\boldsymbol{\Lambda}_i\) and \(\boldsymbol{\gamma}_i\) represent decay factors and stabilizers specific to different models (typically scalars, though they can be full matrices), and \(\tikz\draw[white,fill=black] (0,0) circle (.5ex);\hspace{0.5mm}\) and \(\textcolor{black}{\star}\) denote associative operations such as the Hadamard product or matrix multiplication (details in \cref{overview_lrmsec} and \ref{trainstrag}). This unification is also demonstrated as Gated Linear Transformers \cite{yang2023gated} or Linear Recurrent Models \cite{deltanet}.

\subsection{Chunkwise Parallel Form of Linear Transformers}

Causal Linear Transformers balance the memory-speed tradeoff during training by employing chunkwise parallelization \cite{yang2023gated,deltanet,mamba2}. In this approach, the input sequence \(\mathbf{X} \in \mathbb{R}^{L \times d}\) and the corresponding query, key, and value vectors are split into \(\frac{L}{C}\) non-overlapping chunks, each of size \(C\). Let \(\mathbf{Q}_{[i]}, \mathbf{K}_{[i]}, \mathbf{V}_{[i]} \in \mathbb{R}^{C \times d}\) represent the chunked query, key, and value matrices, and define the chunk-level hidden state after processing \(i\) chunks as \(\mathbf{S}_{[i]} \in \mathbb{R}^{d \times d}\). The causal sequence mixer which maps the input to output \( \mathbf{Y} = \mathbf{P}\mathbf{V}\) is expressed as:

\vspace{-5mm}

\scalebox{0.7}{
\begin{minipage}[t]{0.42\textwidth}  
\begin{align}
& \scalemath{1.2}{\mathbf{S}_{[i]}  =\mathbf{S}_{[i-1]} + \underbrace{\sum_{j=iC+1}^{i(C+1)}\mathbf{k}_j^\top\mathbf{v}_j} _{{\mathbf{K}_{[i]}}^\top\mathbf{V}_{[i]}}} \\
  &\mathbf{Y}_{[i]} = \underbrace{\hblue {\mathbf{Q}_{[i]}\mathbf{S}_{[i]}}}_{\text{\large inter}} +
   \underbrace{\hdiag{\left( \mathbf{Q}_{[i]}\mathbf{K}_{[i]}^\top \odot \mathbf{M}^C \right) \mathbf{V}_{[i]}}}_{\text{\large intra}}
   \label{eq:unstabelparl}
\end{align}
\end{minipage}}
\scalebox{0.7}{
\begin{minipage}[t]{0.0\textwidth}  
\begin{align}
  \vrule \quad  \underbrace{\scalemath{0.8}{\left( \renewcommand*{\arraystretch}{2} \begin{array}{ccccc}
      \dcolor {\quad \quad}  &   &   & \\
    \blue {\quad \quad}  &  \dcolor {\quad \quad}   &  & \\
    \blue {\quad \quad} & \blue  {\quad \quad} & \dcolor {\quad \quad} & \\
     \blue {\quad \quad}  &  \blue   {\quad \quad} &  \blue  {\quad \quad}  & 
    \dcolor  {\quad \quad}\\
  \end{array} \right)}}_{\hspace{2mm}\scalemath{1.3}{\mathbf{Y} = \mathbf{P}\mathbf{V}} } \notag
  \end{align}
\end{minipage}}

The above form is the chunkwise form of Linear Transformer without decay factor and the mask $\mathbf{M}^C \in \{0,1\}^{L\times L}$ is the causal mask. Chunking is essential for training Linear Transformers and SSMs with decay factors, such as Mamba-2 \cite{mamba2} and GLA \cite{yang2023gated} even more, since treating the entire sequence mixer \(\mathbf{P}\) as a single intra-chunk block, is numerically unstable. This instability arises from computing the cumulative product of decay factors \(\prod^L_{t=1} \mathbf{\Lambda}_t\) and their inverse \(\prod^L_{t=1} \mathbf{\Lambda}_t^{-1}\) over the entire sequence, which can overflow or underflow even in logarithmic space\footnote{In log space, \(\prod^L_{t=1} \mathbf{\Lambda}_t^{-1} = \exp\left(-\sum^L_{t=1} \log(\mathbf{\Lambda}_t)\right)\), yet instability persists, as discussed in the official Mamba2 blog post \href{https://goombalab.github.io/blog/2024/mamba2-part3-algorithm/}{here}. Chunking is therefore required for stable training.}. Consequently, models like Mamba-2 \cite{mamba2} rely on chunking for stability, limiting their training throughput compared to softmax-based Transformers, particularly for short sequences \cite{deltanet}. Chunkwise parallel form has a complexity of \(O(LCd + Ld^2)\) and requires \(O\left(\frac{L}{C}\right)\) sequential steps, as opposed to RNNs with a complexity of \(O(Ld^2)\) and \(O(L)\) steps, and attention with \(O(L^2d)\) complexity and \(O(1)\) steps \cite{deltanet}. Also, linear recurrences can be parallelized using the scan algorithm as in prior SSMs \cite{s5,mamba}, but specialized GPU kernels are needed to fully achieve its speed  \cite{gateloop,mamba2}. Current bidirectional SSMs, such as Vim, utilize the parallel scan algorithm with optimized GPU kernels, while Hydra employs a chunkwise parallel version of Mamba2's causal form, known as the state-space duality (SSD) algorithm.


