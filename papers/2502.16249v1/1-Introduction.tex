

\looseness=-1 Transformers \citep{vaswani_attention_2017} are widely used in sequence modeling tasks such as causal language modeling \citep{brown2020language,team2023gemini} due to their high performance and support for parallelized training. However, their quadratic cost is often limiting \citep{lra}, increasing interest in RNN-like models for inference. 


Causal linear attention was introduced as a replacement for softmax attention, using linear attention which is equivalent to an RNN with a two-dimensional hidden state \cite{trans_rnn}. This approach maintains the benefit of parallel training while reducing inference costs. Despite these advantages, it tends to underperform compared to the softmax-based Transformer models on downstream tasks \cite{deltanet}.

\begin{figure}[t] 
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{|c|ccccc|}
            \multicolumn{6}{c}{Training Times (relative to Transformer) $\downarrow$}\\
            \toprule
            Task  &  \colorbox{green!10}{\lionlit} & \colorbox{ violet!20}{\lionretnet} & \colorbox{orange!17}{\lions} & Hydra & Vim\\
            \midrule
            Vision & \textcolor{Green}{$\times {0.73}$} &\textcolor{ violet}{$\times {1.39}$}&\textcolor{red!50}{$\times {1.46}$} & \textcolor{red}{$\times 2.51$} & \textcolor{BrickRed}{$\times 10.86$} \\
            MLM &  \textcolor{Green}{$\times {0.95}$} &\textcolor{ violet}{$\times {1.10}$}& \textcolor{red!50}{$\times {1.32}$}& \textcolor{red}{$\times 3.13$} & \xmark\\
            \bottomrule
        \end{tabular}        
    }\\
    \begin{center}
        \begin{footnotesize}
        Inference Memory $\downarrow$
        \end{footnotesize}
    \end{center}
    \vspace{-2.2mm}
    \includegraphics[width=0.9\columnwidth]{figs/img_xRes_yMem.pdf}
    \vspace{-1mm}
  \caption{\looseness=-1 %
  \textit{Training / Inference resources.} Existing memory-efficient bidirectional models employ more than $\times 2$ the training time of a Transformer. Our linear attention framework benefits from memory-efficient inference while maintaining the transformer training speed. \lion represents the RNN format across all counterparts of the framework, as they share the same memory footprint during inference.
  }
  \vspace{-5mm}
  \label{fig:blm}
\end{figure}

To address this, variants of linear attention and state space models (SSMs) have been developed by introducing fixed \cite{retnet,peng2021random,gu2021efficiently} or input-dependent decay factors \cite{mamba,mamba2,yang2023gated} in the RNN recurrence. These advancements improve the performance of transformers with linear attention in causal sequence modeling, making it on par with transformers with softmax attention, while maintaining efficient and fast inference.

\looseness=-1 In stark contrast to causal modeling, transformers with linear attention remain largely unexplored in \textit{bidirectional} sequence modeling. Current bidirectional SSMs are primarily adaptations of their causal counterparts, designed to maintain RNN-like inference while supporting parallel training \citep{zhu2024visionmambaefficientvisual,hwang2024hydrabidirectionalstatespace,deltanet,yang2023gated,mamba2}.
However, softmax-based Transformers for bidirectional sequence modeling \cite{vit,he2020deberta} still achieve significantly higher training speed than existing bidirectional SSMs.

\looseness=-1This paper presents \lion for bidirectional sequence modeling, a framework that employs full \textbf{Li}near attenti\textbf{on} during training to match the training speed of softmax-based Transformers, while reformulating the attention as an equivalent bidirectional RNN to achieve linear-time inference. To balance the speed-memory trade off,  we also introduce \lion Chunk, interpolating the speed of full attention with the resource efficiency of RNNs during inference.


We rigorously prove that several Linear Transformers can be trained for bidirectional sequence modeling within the \lion framework (Appendix \ref{sec:map}). 
 As running examples, we focus on three key variants supporting different types of masks:
\vspace{-3mm}
\begin{enumerate}
\itemsep0em 
    \item \colorbox{green!10}{\lionlit}: Full linear attention without masking, serving as a basic, bidirectional extension of the causal Linear Transformer \cite{trans_rnn}.  
    \item \colorbox{ violet!20}{\lionretnet}: Decay-masked full attention with non input-dependent state parameter $\lambda$, extending RetNet \cite{retnet} into bidirectional sequence modeling. \vspace{-2mm}
    \item \colorbox{orange!17}{\lions}: Stable selective masked full attention with an input-dependent mask $\lambda_i$, inspired by the selectivity of SSMs like Mamba-2 \cite{mamba2}.   \vspace{-2mm}
\end{enumerate}
\vspace{-1mm}

Using \lion, we demonstrate that these models achieve significantly higher training speed than SSMs and approach to the speed of softmax-based Transformers, while retaining SSM-like efficiency during inference. 

Indeed, \lion achieves $9\times$ faster training than Vision Mamba on image classification and $\sim 2\times$ faster than Hydra on masked language modeling, with comparable performance. 

Despite their simplicity, \lion -\text{\sc Lit}, -\text{\sc D}, -\text{\sc S} perform competitively on bidirectional tasks, achieving accuracy close to softmax-based Transformers with more efficient inference and matching SSM performance while training significantly faster even without specialized GPU kernels.  
