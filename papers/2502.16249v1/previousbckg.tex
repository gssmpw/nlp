\section{\textbf{Background and Overview}}

\paragraph{Canonical self-attention.} 
Given the input data sequence $X=\{x_{\iter} \in \R^d\}_{\iter=1}^L$, attention transforms $X \in \R^{L \times d}$ into the output sequence $Y \in \R^{L \times d}$, i.e., attention is a sequence-to-sequence transformation, as follows.
Attention defines linear mappings query $Q=XW_q$, $W_q \in \R^{d_q \times d}$, key $K=XW_k$, $W_k \in \R^{d_k \times d}$, and value $V=XW_v$, $W_v \in \R^{d_v \times d}$, with commonly $d_q=d_k$.
The output of the canonical softmax attention \citep{vaswani_attention_2017} is computed as follows
\begin{equation} \label{eq:att1}
Y = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V := KV,
\end{equation}
where the softmax function is applied row-wise and the matrix $K := \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) \in \R^{L \times L}$ and its entries $k_{ij}$ for $i,j=1,\ldots,L$ are the attention matrix and the attention scores, respectively \citep{nguyen2023a}.
The attention mechanism can be generalized to kernel attention by using a kernel function $\kappa: \R^d \times \R^d \to \R$ \citep{tsai2019transformer} as
\begin{equation} \label{eq:att2}
    y_i = \sum_{j=1}^L \frac{\kappa(q_i,k_j)}{\sum_{j'=1}^L \kappa(q_i, k_{j'})} v_j, \quad i=1,\ldots,L,
\end{equation}
with the notation $a_i$ being the $i$-th row of matrix $A$.
The original softmax attention \citep{vaswani_attention_2017} is obtained with $\kappa(q_i,k_j)=\exp (\frac{q_i^\top k_j}{\sqrt{d_k}})$.
Multiple attention kernels can be defined on \eqref{eq:att2}, e.g., polynomial or RBF kernel.
In this paper, we study $\kappa(q_i,k_j)=q_i^\top k_j$, i.e., the linear attention case \citep{trans_rnn}.
Note that in the literature both scaled and non-scaled versions of attention have been defined, where the latter discards the normalization term $\sum_{j'=1}^L \kappa(q_i, k_{j'})$ and simply sets it to 1.
Attention \eqref{eq:att1} is usually called encoder attention, while autoregressive Transformers, such as GPT \citep{radford2018improving}, employ decoder-only attention by hiding future tokens, i.e., zero-masking the attention scores at position $j, \, j > i$ for output $y_i$.
In matrix form, masked attention is computed as follows
\begin{equation} \label{eq:maskedatt}
    Y = (M \circ K) V,
\end{equation}
where $M$ is the lower-triangular causal mask of ones, effectively excluding future tokens from attention output computation.

\paragraph{Positional embeddings.} Unlike RNNs, attention output \eqref{eq:att1} is invariant to the order of the inputs. 
Therefore, to model sequences, Transformers employ temporal features to encode the position of each token in the sequence.
Each element of the input data sequence is seen as $x_i= f_i + t_i$ with $f_i$ being the features of data at time $i$ and $t_i$ the positional embedding.
The data features $f_i$ can be word tokens in language models or image patches in computer vision tasks \citep{dosovitskiy2021imageworth16x16words}.
The positional embeddings $t_i$ are usually taken as a mixuture of sine and cosine functions \citep{vaswani_attention_2017}.
\fra{@Arshia: Add some drawbacks of using positional embeddings at inference time, as we avoid them...}

\paragraph{State Space Models and Mamba.}
The SSM block is a discrete linear time-invariant system defining a new sequence of states $\{h_\iter \in \R^N\}_{\iter=1}^L$ using projections $A \in \R^{N \times N}, B \in \R^{N \times d}$ through the recurrence
\begin{align}
h_\iter &= A h_{\iter-1} + B x_\iter.
\label{eq:ssm_recurrent}
\end{align}
Note that in SSMs multiple blocks are commonly applied to each dimension of the input separately with $d=1$ \citep{mamba}.
It can be generalized to $d>1$ by simply broadcasting across this dimension \citep{mamba2}.
Given $C \in \R^{N \times d}$, the SSM output $y_\iter \in \R^d$ for each SSM block is then obtained by projecting the hidden state:
\begin{align}
y_{\iter} &= C^\top h_{\iter}.
\label{eq:ssm_output}
\end{align}
The SSM output can also be computed in convolutional form by expanding the recurrence as 
the convolution of the input $x$ with a time-invariant kernel:
\begin{equation*}
y = x\ast K,
\end{equation*}
where $K = (C^\top B, C^\top A B, \dots, C^\top A^{L-1} B)$ and $h_0=0$.
Mamba \citep{mamba} is a selective SSM, i.e., a time-varying generalization of the LTI SSM \eqref{eq:ssm_recurrent} that allows for input-dependent parameters. 
This selection mechanism empowers the model to process input tokens in a content-aware fashion, resulting in the following recurrence:
\begin{align}
h_{\iter} &= A_\iter h_{\iter-1} + B_\iter x_{\iter},
\label{eq:selective_ssm_recurrent}
\end{align}
where the projection matrices $A,B$ can now be functions of the input $x_\iter$ at each time step $\iter$. Similarly, we can also make the output projection matrix $C$ selective:
\begin{align}
y_\iter &= C^\top_\iter h_{\iter}, \label{eq:selective_ssm_output} \\
y_\iter &= C^\top_\iter \sum_{j=1}^\iter \left( \prod_{k=j+1}^\iter A_k \right) B_j x_j \label{eq:selective_ssm_output_recurrent}
\end{align}
This input-dependence allows the model to selectively focus on or ignore certain timesteps of the input based on its content.
Note that the kernel in the convolutional form \eqref{eq:selective_ssm_output_recurrent} is time-varying; in the case of selective SSMs, the convolutional form cannot be efficiently exploited for training, but \citep{mamba} shows that the output can be computed efficiently on GPUs as a parallel scan.
At test time, the recurrent form can be used for linear-time autoregressive inference, showing higher efficiency than quadratic attention in Transformers, which is especially advantageous for long sequence length modelling.

Mamba was shown to achieve higher performance than the LTI SSM on tasks such as natural language processing \citep{mamba} and computer vision \citep{zhu2024visionmambaefficientvisual}, commonly showing higher performance at larger state dimension $N$ \citep{mamba2}.

\newpage



\section{Selective Linear Transformer}

For causal transformers such as GPT the output of each layer is only dependent on the previous input tokens. Formally, the input sequence $x$ is projected by three matrices $W_Q \in \R^{F\times D}$, $W_K \in \R^{F \times D}$ and $W_V \in \R^{F \times M}$ to corresponding representations $Q$, $K$ and $V$. The output of the each layer of transformer with linear attention can be formulated as:

\begin{align*}%
Y = \operatorname*{LinAtt}(Q^{\top}K) \cdot V = \begin{bNiceArray}{ccccc}[cell-space-limits=3pt]
    \frac{q_1^{\top}k_1 }{q_1^{\top}z_1} & \\
    \frac{q_2^{\top}k_1 }{q_2^{\top}z_2}  & \frac{q_2^{\top}k_2  }{q_2^{\top}z_2}   \\
    \frac{q_3^{\top}k_1 }{q_3^{\top}z_3}  & \frac{q_3^{\top}k_2  }{q_3^{\top}z_3}  & \frac{q_3^{\top}k_3  }{q_3^{\top}z_3}  \\
    \vdots & \vdots & \vdots & \ddots \\
    \frac{q_L^{\top}k_1 }{q_L^{\top}z_L}  & \frac{q_L^{\top}k_2  }{q_L^{\top}z_L}  & \frac{q_L^{\top}k_3  }{q_L^{\top}z_L} & \dots  & \frac{q_L^{\top}k_L  }{q_L^{\top}z_L} \\
\end{bNiceArray}
\begin{bNiceArray}{c}[cell-space-limits=3pt]
    v_1 \\
    v_2 \\
    v_3\\
    \vdots \\
    v_L \\
\end{bNiceArray}
\end{align*}
Where $z$ is the scaling value to normalize the attention and avoiding the attention scores to explode. Shown by \cite{trans_rnn} and also extractable from the above output matrix the casual linear transformer can be written as \gls{rnn} in the following formulation:


\begin{equation}\label{eq:appendrow}
  \left(\begin{array}{cccc}
    \x  & \x  & \x & \x \\
    \orange 0   & \x  & \x & \x \\
    0   & 0   & \x & \x \\
    0   & 0   & 0  & \x \\
    \blue a  &  b  & \b c &  d\\
  \end{array}\right)
\end{equation}

\begin{align*}
    k_i &= x_i W_K,  \\
    v_i &= x_i W_V,   \\
    q_i &= x_i W_Q \\
    s_i &= s_{i-1} + {k_i} \left(v_i\right)^{\top}, \\
    z_i &= z_{i-1} + k_i, \\
    y_i &= \frac{{q_i}^{\top} s_i}{{q_i}^T z_i} 
\end{align*}

As seen the above equation shares also similar notions with the concept of selectivity in state-space models such as Mamba \cite{mamba} and Mamba-2 \cite{mamba2} as the queries and keys can represent as selective extraction from the new input token $x_i$. Within state-space representation the above can be shown as: 


\begin{minipage}[t]{.3\linewidth}
  \begin{subequations}\label{eq:ssmm}
    \begin{align}
     h_i &= A_ih_{i-1} + B_ix_i \\
    y_i &= C_i h_i
    \end{align}
  \end{subequations}
\end{minipage}%
\begin{minipage}[t]{.3\linewidth}
  \begin{subequations}%
    
    \begin{align}
    s_i &= s_{i-1} + {k_i} \left(v_i\right)^{\top}, \\
    z_i &= z_{i-1} + k_i, \\
    y_i &= \frac{{q_i}^{\top} s_i}{{q_i}^{\top} z_i} 
    \end{align}
  \end{subequations}
  \end{minipage}
  \begin{minipage}[t]{.4\linewidth}
    \begin{subequations}%
    \label{eq:linscaletrans}
    \begin{align}
    s_i &= \textcolor{orange}{\boldsymbol{a_i}} s_{i-1} + {k_i} \left(v_i\right)^{\top}, \\
    z_i &= \textcolor{orange}{\boldsymbol{a_i}} z_{i-1} + k_i, \\
    y_i &= \frac{{q_i}^{\top} s_i}{{q_i}^{\top} z_i} 
    \end{align}
  \end{subequations}
\end{minipage}

We can see that the output of the S6 block of the Mamba models and linear attention accelerated using \gls{rnn} framework are actually similar from the idea behind, however they share three main key differences:
\begin{itemize}
    \item State matrix $A$ in state-space models in which we will see is responsible for both scaling the attention and a representation of the sequence order of the model.
    \item The scaling hidden state in the linear transformer $z$ in-which it does not exist in the Mamba like models.
    \item Each block of the SSM models map the token input token into a single hidden state (with dimension $d$) and then for each scale element of the input sequence an state space model with an SSM state of dimension $N$ iterates over the sequence for each element of hidden state individually while in multi-head attention, initially token is mapped into $N$ individual attention heads and then the linear attention iterates over the sequence length for each attention head.
\end{itemize}
Also, it is important to note that the feed forward neural network for after each multi head attention unit and the projections in Mamba like models are playing the same rule of adding non-linearity and helping the model for extracting higher order features from input sequence. The normalization of each layer and the residual update also exists in both structures. The \eqref{eq:ssm} refers to SSM block for Mamba models \eqref{eq:lintrans} is the output of one single linear attention layer for transformer architecture proposed at \cite{trans_rnn} and \eqref{eq:linscaletrans} is our proposed selective linear attention which the $\textcolor{orange}{\boldsymbol{a_i}} $ parameter is a scaler and has the same rule as its component in SSM blocks of Mamba. By writing the sequence of $y_i$ starting from $y_0$ from the equations we have:

\begin{align*}
    y_0 &= \frac{q_0^{\top}k_0v_0^{\top}}{z_0}, \\
    y_1 &= \textcolor{orange}{\boldsymbol{a_1}}\frac{q_1^{\top}k_0v_0^{\top}}{z_1} + \frac{q_1^{\top}k_1 }{z_1} \\
    y_2 &= \textcolor{orange}{\boldsymbol{a_1a_2}}\frac{q_2^{\top}k_0v_0^{\top}}{z_2} + \textcolor{orange}{\boldsymbol{a_1}}\frac{q_2^{\top}k_1v_1^{\top} }{z_2} + \frac{q_2^{\top}k_2v_2^{\top} }{z_2} \\
\end{align*}

So for the $i'$th element of the sequence we will have:
\begin{align*}
    y_i &= \frac{q_i^{\top}\sum_{j=0}^{i}m_{ij}k_jv_j^{\top}}{z_i}, \\
\end{align*}


Where $m_{ij}$ is an element from the mask matrix created based on the $a_1,a_2,\dots,a_i$ values, below we can re-write the equations holding for the output $y_i$ based on the queries and keys using attention matrix. 


\begin{align*}%
Y = \begin{bNiceArray}{ccccc}[cell-space-limits=3pt]
    \frac{q_0^{\top}k_0 }{z_0} & \\
    \textcolor{orange}{\boldsymbol{a_1}}\frac{q_1^{\top}k_0 }{z_1}  & \frac{q_1^{\top}k_1  }{z_1}   \\
    \textcolor{orange}{\boldsymbol{a_1a_2}}\frac{q_2^{\top}k_0 }{z_2}  & \textcolor{orange}{\boldsymbol{a_2}}\frac{q_2^{\top}k_1  }{z_2}  & \frac{q_2^{\top}k_2  }{z_2}  \\
    \vdots & \vdots & \vdots & \ddots \\
    \textcolor{orange}{\boldsymbol{a_{L-1}\cdots a_1}} \frac{q_L^{\top}k_0 }{z_L}  & \textcolor{orange}{\boldsymbol{a_{L-1}\cdots a_2}} \frac{q_L^{\top}k_1  }{z_L}  & \textcolor{orange}{\boldsymbol{a_{L-1}\cdots a_3}} \frac{q_L^{\top}k_2  }{z_L} & \dots  & \frac{q_L^{\top}k_L  }{z_L} \\
\end{bNiceArray}
\begin{bNiceArray}{c}[cell-space-limits=3pt]
    v_0 \\
    v_1 \\
    v_2\\
    \vdots \\
    v_{L-1} \\
\end{bNiceArray}
\end{align*}

This is equal to 
\begin{align*}
    Y = \textcolor{orange}{\boldsymbol{M}} \circ (Q^{\top}K) \cdot V
\end{align*}
\pol{If Q,K,V are shaped (length, hidden) then should be $(QK^{\top}) \cdot V$}
With $M$ being equal to
\begin{align*}
    M_{ij} = 
    \begin{cases} 
    \Pi_{k=i}^{j+1}a_k & i\geq j  \\
    0 & i<j
\end{cases}
\end{align*}
\elias{$\prod_{k=j}^{i+1}$ right?}
This approach is equivalent to the SSM masking strategy, as it introduces the position of each token in the sequence to the model through \(a_i\), which learns the causal mask of the attention. This contrasts with setting the mask to all ones, where all tokens are considered equally important without considering positional encoding. By learning the causal mask, we eliminate the need to add positional encoding to each token. This method has the advantage of avoiding the recalculation of attention for all tokens whenever a new token is added, thereby enhancing scalability with sequence length. However, alike other \gls{rnn} the time needed for generating the output scales linearly with sequence $\mathcal{O}(L)$ instead of $\mathcal{O}(L^2)$ as the case of transformers.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/pic1.png}
    \caption{Difference between SSMs and Linear Transformers}
    \label{fig:model}
\end{figure}



\section{Full Attention can Cast as Bidirectional RNN}

First we show that any full attention matrix can be reproduced fully with bi-directional selective \gls{rnn}.

\begin{align*}%
Y = \operatorname*{LinAtt}(Q^{\top}K) \cdot V = \begin{bNiceArray}{ccccc}[cell-space-limits=3pt]
    \frac{q_1^{\top}k_1 }{z_1} & \frac{q_1^{\top}k_2 }{z_1} & \frac{q_1^{\top}k_3 }{z_1} & \dots & \frac{q_1^{\top}k_L }{z_1} \\
    \frac{q_2^{\top}k_1 }{z_2}  & \frac{q_2^{\top}k_2  }{z_2} & \frac{q_2^{\top}k_3 }{z_2}  & \dots & \frac{q_2^{\top}k_L }{z_2} \\
    \frac{q_3^{\top}k_1 }{z_3}  & \frac{q_3^{\top}k_2  }{z_3}  & \frac{q_3^{\top}k_3  }{z_3} & & \vdots \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \frac{q_L^{\top}k_1 }{z_L}  & \frac{q_L^{\top}k_2  }{z_L}  & \frac{q_L^{\top}k_3  }{z_L} & \dots  & \frac{q_L^{\top}k_L  }{z_L} \\
\end{bNiceArray}
\begin{bNiceArray}{c}[cell-space-limits=3pt]
    v_1 \\
    v_2 \\
    v_3\\
    \vdots \\
    v_L \\
\end{bNiceArray}
\end{align*}
Which can be separated into two upper and lower triangular matrices:
\begin{align*}%
Y =\Big( \begin{bNiceArray}{ccccc}[cell-space-limits=3pt]
    \frac{q_1^{\top}k_1 }{2z_1}  \\
    \frac{q_2^{\top}k_1 }{z_2}  & \frac{q_2^{\top}k_2  }{2z_2}  \\
    \frac{q_3^{\top}k_1 }{z_3}  & \frac{q_3^{\top}k_2  }{z_3}  & \frac{q_3^{\top}k_3  }{2z_3}  \\
    \vdots & \vdots & \vdots & \ddots \\
    \frac{q_L^{\top}k_1 }{z_L}  & \frac{q_L^{\top}k_2  }{z_L}  & \frac{q_L^{\top}k_3  }{z_L} & \dots  & \frac{q_L^{\top}k_L  }{2z_L} \\
\end{bNiceArray}
+
 \begin{bNiceArray}{ccccc}[cell-space-limits=3pt]
    \frac{q_1^{\top}k_1 }{2z_1} & \frac{q_1^{\top}k_2 }{z_1} & \frac{q_1^{\top}k_3 }{z_1} & \dots & \frac{q_1^{\top}k_L }{z_1} \\
     & \ddots & \vdots & \vdots & \vdots   \\
    && \frac{q_{L-2}^{\top}k_{L-2} }{2z_{L-2}}  & \frac{q_{L-2}^{\top}k_{L-1}  }{z_{L-2}}  & \frac{q_{L-2}^{\top}k_L  }{z_{L-2}}  \\
    &&& \frac{q_{L-1}^{\top}k_{L-1} }{2z_{L-1}}  & \frac{q_{L-1}^{\top}k_L  }{z_{L-1}}  \\
    &&&& \frac{q_L^{\top}k_L  }{2z_L} \\
\end{bNiceArray} \Big)
\begin{bNiceArray}{c}[cell-space-limits=3pt]
    v_1 \\
    v_2 \\
    v_3\\
    \vdots \\
    v_L \\
\end{bNiceArray}
\end{align*}
Or equivalently:
\begin{align*}%
Y = \begin{bNiceArray}{ccccc}[cell-space-limits=3pt]
    \rowcolor{blue!10}
    \frac{q_1^{\top}k_1 }{2z_1}  \\
    \rowcolor{blue!10}
    \frac{q_2^{\top}k_1 }{z_2}  & \frac{q_2^{\top}k_2  }{2z_2}  \\
    \rowcolor{blue!10}
    \frac{q_3^{\top}k_1 }{z_3}  & \frac{q_3^{\top}k_2  }{z_3}  & \frac{q_3^{\top}k_3  }{2z_3}  \\
    \rowcolor{blue!10}
    \vdots & \vdots & \vdots & \ddots \\
    \rowcolor{blue!10}
    \frac{q_L^{\top}k_1 }{z_L}  & \frac{q_L^{\top}k_2  }{z_L}  & \frac{q_L^{\top}k_3  }{z_L} & \dots  & \frac{q_L^{\top}k_L  }{2z_L} \\
\end{bNiceArray}
\begin{bNiceArray}{c}[cell-space-limits=3pt]
    v_1 \\
    v_2 \\
    v_3\\
    \vdots \\
    v_L \\
\end{bNiceArray}
+
 \begin{bNiceArray}{ccccc}[cell-space-limits=3pt]
 \rowcolor{red!10}
  \frac{q_L^{\top}k_L  }{2z_L} \\
  \rowcolor{red!10}
  \frac{q_{L-1}^{\top}k_L  }{z_{L-1}}  & \frac{q_{L-1}^{\top}k_{L-1} }{2z_{L-1}}   \\
  \rowcolor{red!10}
  \frac{q_{L-2}^{\top}k_L  }{z_{L-2}}  & \frac{q_{L-2}^{\top}k_{L-1}  }{z_{L-2}} & \frac{q_{L-2}^{\top}k_{L-2} }{2z_{L-2}}    \\
  \rowcolor{red!10}
  \vdots & \vdots & \vdots & \ddots   \\
  \rowcolor{red!10}
  \frac{q_1^{\top}k_L }{z_1} & \frac{q_1^{\top}k_{L-1} }{z_1} & \frac{q_1^{\top}k_{L-2} }{z_1} & \dots & \frac{q_1^{\top}k_1 }{2z_1} \\
\end{bNiceArray} 
\begin{bNiceArray}{c}[cell-space-limits=3pt]
    v_L \\
    v_{L-1} \\
    v_{L-2}\\
    \vdots \\
    v_1 \\
\end{bNiceArray}
\end{align*}

Which is equal to \textcolor{blue!50}{forward} and \textcolor{red!50}{backward} path of the linear transformer mentioned above, but as seen there are two main modifications which is needed here that original set of equations at \ref{eq:lintrans}:
\begin{itemize}
    \item The scalar value $z_i$ now not only dependent on previous tokens but also all tokens in the sequence.
    \item The diagonal of the attention matrix for each forward and backward path is divided by two to share the influence in between to paths.
\end{itemize}
These differences needs the following modifications first we do a forward path to find the scaling value needed for scaling the tokens then we do a forward path and backward path for extracting all the outputs of either directions which lead to the exact full non-causal attention matrix. The modifications are as follows: 


\begin{minipage}[t]{.3\linewidth}
  \begin{subequations}\label{eq:scale path}
    \begin{align}
    & \textcolor{OliveGreen}{\textbf{Scale Path}} \\
    z_i &= z_{i-1} + k_i \\
    \end{align}
  \end{subequations}
\end{minipage}%
\begin{minipage}[t]{.3\linewidth}
  \begin{subequations}%
    \label{eq:forpass}
    \begin{align}
    & \textcolor{blue!50}{\textbf{Forward Path}} \\
    s_i &= s_{i-1} + {k_i} \left(v_i\right)^{\top}, \\
    y_i &= \frac{{q_i}^{\top} s_i}{{q_i}^{\top} \textcolor{orange}{\boldsymbol{z_L} }} - \textcolor{orange}{\frac{\boldsymbol{q_i}^{\boldsymbol{\top}}\boldsymbol{k_i}}{\boldsymbol{2z_L}}}
    \end{align}
  \end{subequations}
  \end{minipage}
  \begin{minipage}[t]{.4\linewidth}
    \begin{subequations}%
    \label{eq:backpass}
    \begin{align}
    & \textcolor{red!50}{\textbf{Backward Path}} \\
    s_i &= s_{i-1} + {k_{L-i}} \left(v_{L-i}\right)^{\top}, \\
    y_i &= \frac{{q_{L-i}}^{\top} s_i}{{q_{L-i}}^{\top} \textcolor{orange}{\boldsymbol{z_L} }} - \textcolor{orange}{\frac{\boldsymbol{q_{L-i}}^{\boldsymbol{\top}}\boldsymbol{k_{L-i}}}{\boldsymbol{2z_L}}}
    \end{align}
  \end{subequations}
\end{minipage}
\yongtao{forward pass should be 
$y_i = \frac{{q_i}^{\top} s_i}{{q_i}^{\top} {\boldsymbol{z_L} }} -{\frac{\boldsymbol{q_i}^{\boldsymbol{\top}}\boldsymbol{k_i v_i}}{\boldsymbol{2 q_i z_L}}}$? backward pass should use $L-i+1$?}
The updated parts related to the equation set \ref{eq:linscaletrans} are highlighted in bold orange. The scalar value \textcolor{orange}{$z_L$} is obtained after a complete forward (or backward, as it does not matter) pass through the sequence, and is given by \(z_L = \sum_{i=0}^L k_i\). With this, the model can bidirectionally compute the outputs \(y_i\) for both forward and backward paths. It is important to note that without subtracting the last two orange-highlighted terms in both the forward and backward paths, the diagonal of the linear transformer's full attention matrix would have values twice as large as the off-diagonal elements. Therefore, we subtract these terms to ensure that the full attention matrix equals the linear attention matrix.

By adding the $a_i$ values for linear transformer with scale the following equations will change to:

\begin{align*}
    s'_{(t)} &= As_{(t)} + {k_{(t)}} v_{(t))}^{\top}
\end{align*}

And by applying the ZOH filter following the bellow equations we will reach:

\begin{align*}
    s_i &= \Bar{A}s_{i-1} + \Bar{B}{k_i} v_i^{\top}
\end{align*}

Where:

\begin{align*}
    \
\end{align*}

\section{Continuous Time Transformer}

The formulation for continuous time transformer can be written as dynamical system as followes:




\begin{minipage}[t]{.3\linewidth}
  \begin{subequations}\label{eq:scale path}
    \begin{align}
    & \textcolor{OliveGreen}{\textbf{Scale Path}} \\
    z_i &= \textcolor{orange}{\boldsymbol{a_i}} z_{i-1} + k_i \\
    \end{align}
  \end{subequations}
\end{minipage}%
\begin{minipage}[t]{.3\linewidth}
  \begin{subequations}%
    \label{eq:forpass}
    \begin{align}
    & \textcolor{blue!50}{\textbf{Forward Path}} \\
    s_i &= \textcolor{orange}{\boldsymbol{a_i}} s_{i-1} + {k_i} \left(v_i\right)^{\top}, \\
    y_i &= \frac{{q_i}^{\top} s_i}{{q_i}^{\top} {{z_L} }} - {\frac{{q_i}^{{\top}}{k_i}}{{2z_L}}}
    \end{align}
  \end{subequations}
  \end{minipage}
  \begin{minipage}[t]{.4\linewidth}
    \begin{subequations}%
    \label{eq:backpass}
    \begin{align}
    & \textcolor{red!50}{\textbf{Backward Path}} \\
    s_i &= \textcolor{orange}{\boldsymbol{a_i}} s_{i-1} + {k_{L-i}} \left(v_{L-i}\right)^{\top}, \\
    y_i &= \frac{{q_{L-i}}^{\top} s_i}{{q_{L-i}}^{\top}{{z_L} }} - {\frac{{q_{L-i}}^{{\top}}{k_{L-i}}}{{2z_L}}}
    \end{align}
  \end{subequations}
\end{minipage}


\newpage











\begin{minipage}[t]{0\textwidth}  
\begin{align*}
\hspace{4cm} \mathbf{C} =
  \scalemath{0.6}{ 
   \left( \renewcommand*{\arraystretch}{1} \begin{array}{cccc}
       \dcolor{\mathbf{q}^{\top}_1\sum\limits_{j=1}^{L} \mathbf{M}_{1j}\mathbf{k}_j}   &  & & \\
       &  \dcolor{\mathbf{q}^{\top}_2\sum\limits_{j=1}^{L} \mathbf{M}_{2j}\mathbf{k}_j}  &  &\\
       &  &   \dcolor\ddots & \\
       &  &  &  \dcolor{\mathbf{q}^{\top}_L\sum\limits_{j=1}^{L} \mathbf{M}_{Lj}\mathbf{k}_j}\\
  \end{array} \right). 
  =   } 
\end{align*}
\end{minipage}


\begin{minipage}[t]{1\textwidth}  
\begin{align}
\scalemath{0.55}{
\underbrace{
   \left( \renewcommand*{\arraystretch}{1} \begin{array}{cccc}
       \blue{  \mathbf{q}^{\top}_1\mathbf{M}^F_{11}\mathbf{k}_1 -  \mathbf{q}^{\top}_1\frac{\mathbf{k}_1}{2}}   &  & & \\
       &  \blue{ \mathbf{q}^{\top}_2\sum\limits_{j=1}^{2} \mathbf{M}^F_{2j}\mathbf{k}_j-  \mathbf{q}^{\top}_2\frac{\mathbf{k}_2}{2}}  &  &\\
       &  &   \blue\ddots & \\
       &  &  &  \blue{ \mathbf{q}^{\top}_L\sum\limits_{j=1}^{L} \mathbf{M}^F_{Lj}\mathbf{k}_j -  \mathbf{q}^{\top}_L\frac{\mathbf{k}_L}{2}} \\
  \end{array} \right)}_{\hspace{1mm}\scalemath{1.2}{\mathbf{C}^F}}  +
  \underbrace{
   \left( \renewcommand*{\arraystretch}{1} \begin{array}{cccc}
       \orange{ \mathbf{q}^{\top}_1\sum\limits_{j=1}^{L} \mathbf{M}^B_{1j}\mathbf{k}_j -  \mathbf{q}^{\top}_1\frac{\mathbf{k}_1}{2}}   &  & & \\
       &  \orange{ \mathbf{q}^{\top}_2\sum\limits_{j=2}^{L} \mathbf{M}^B_{2j}\mathbf{k}_j -  \mathbf{q}^{\top}_2\frac{\mathbf{k}_2}{2}} &  &\\
       &  &   \orange\ddots & \\
       &  &  &  \orange{  \mathbf{q}^{\top}_L\mathbf{M}^B_{LL}\mathbf{k}_L -  \mathbf{q}^{\top}_L\frac{\mathbf{k}_L}{2}}\\
  \end{array} \right)}_{\hspace{1mm}\scalemath{1.2}{\mathbf{C}^B}}
}
\end{align}
\end{minipage}





\vspace{-7mm}
\scalebox{1}{
\begin{minipage}[t]{0.41\textwidth}
\begin{subequations}%
\label{eq:contatt}
    \begin{align}
     & \hspace{-0.4cm}  \textsc{Continuous}  \notag \\
      &\hspace{-0.4cm}  \mathbf{S}_{(t)}' = \mathbf{S}_{(t)} + \mathbf{k}_{(t)}\mathbf{v}_{(t)}^{\top} ,   \\
      &\hspace{-0.4cm}  \mathbf{z}_{(t)}' = \mathbf{z}_{(t)} + \mathbf{k}_{(t)},   \hspace{1mm}
     \mathbf{y}_{(t)} = \frac{\mathbf{q}_{(t)}^{\top} \mathbf{S}_{(t)}}{\mathbf{q}_{(t)}^{\top} \mathbf{z}_{(t)}}  
    \end{align}
    \end{subequations}
\end{minipage}%
\hspace{5mm}
\begin{minipage}[t]{.54\textwidth}
\begin{subequations}
        \begin{align}  
    \hspace{3mm}  & \hspace{-0.4cm}  \textsc{Discrete } \notag \\
      & \hspace{-0.4cm}  \mathbf{S}_{i} = e^{a_i}\mathbf{S}_{i-1}+ (e^{a_i}-1)\mathbf{k}_{i}\mathbf{v}_{i}^{\top},  \\
     &\hspace{-0.4cm}  \mathbf{z}_{i} =  e^{a_i}\mathbf{z}_{i-1} + (e^{a_i}-1)\mathbf{k}_{i},    \hspace{1mm}
    \mathbf{y}_{i} = \frac{\mathbf{q}^{\top} _{i} \mathbf{S}_{i}}{\mathbf{q}^{\top} _{i}\mathbf{z}_{i}}.
    \end{align}
    \label{eq:discatt}
    \end{subequations}
  \end{minipage}
  }




  \resizebox{0.49\textwidth}{!}{
\begin{minipage}[t]{.26\textwidth}
\begin{subequations}%
\label{eq:ssm}
    \begin{align}
      & \textbf{a1) } \notag \\
      & {\mathbf{S}_i^{F/B}}   =  {\mathbf{S}_{i-1}^{F/B}} + \mathbf{k}_i \mathbf{v}_i^{\top},\\
      &  {\mathbf{z}_i^{F/B}}   = {\mathbf{z}_{i-1}^{F/B}} + {\mathbf{k}_i},  \\
      &  {\mathbf{y}_i^{F/B}}    = \frac{{{\mathbf{q}_i}}^{\top} {\mathbf{S}_i^{F/B}}}{{{\mathbf{q}_i}}^{\top} {\mathbf{z}_i^{F/B}}}.   \\
      & \hspace{1.5cm} \text{\Large $\ne$}   \notag \\
      & \textbf{a2) } \notag \\
      & \mathbf{Y}  = \textsc{scale}(\mathbf{Q}\mathbf{K} ^\top)\mathbf{V}
    \end{align}
    \end{subequations}
\end{minipage}%
\hspace{3mm}\vrule
\begin{minipage}[t]{.3\textwidth}
\begin{subequations}
        \begin{align}  
        & \textbf{b1) }  \notag \\
        & {\mathbf{S}_i^{F/B}}   = \lambda_i {\mathbf{S}_{i-1}^{F/B}} + \mathbf{k}_i \mathbf{v}_i^{\top},\\
      &   {\mathbf{z}_i^{F/B}}   = \lambda_i{\mathbf{z}_{i-1}^{F/B}} + {\mathbf{k}_i},  \\
      &  {\mathbf{y}_i^{F/B}}    = \frac{{{\mathbf{q}_i}}^{\top} {\mathbf{S}_i^{F/B}}}{{{\mathbf{q}_i}}^{\top} {\mathbf{z}_i^{F/B}}}.   \\
      & \hspace{1.5cm} \text{\Large $\ne$}  \notag \\
      & \textbf{b2) }  \notag \\
      & \mathbf{Y}  = \textsc{scale}(\mathbf{Q}\mathbf{K}^\top  \odot \mathbf{M})\mathbf{V}
    \end{align}
    \end{subequations}
  \end{minipage}
  }



  \begin{table*}[t] 
    \centering
        \caption{\textit{Performance on Long Range Arena Tasks.} 
        For each column (dataset), the best and the second best results are highlighted with \textbf{bold} and \underline{underline} respectively. Note that the MEGA architecture has roughly 10$\times$ the number of parameters as  the other architectures.}
    \resizebox{1\textwidth}{!}{
\begin{tabular}{l|lcccccc|cc}
\toprule
Category & Model & ListOps & Text & Retrieval	& Image	& Pathfinder & PathX & Avg. \\
& (input length) & 2048 & \rebuttal{4096} & 4000 & 1024 & 1024  & 16K & \\
 \bottomrule
\multirow{3}{*}{{Transformer}} & Transformer    & 36.37 & 64.27 & 57.46 & 42.44 & 71.40& \xmark &  54.39  \\
& MEGA ($\cO(L^2)$)& \textbf{63.14} & \textbf{90.43} & \underline{91.25} & \textbf{90.44} & \underline{96.01} & 97.98 & \textbf{88.21} \\
& MEGA-chunk ($\cO(L)$) & 58.76 & \underline{90.19} & 90.97 & 85.80 & 94.41 & 93.81 & 85.66 \\
\bottomrule
\multirow{4}{*}{{SSM}} & DSS  & 57.60 & 76.60 & 87.60 & 85.80 & 84.10 & 85.00 & 79.45  \\
& S4 (original)   & 58.35 & {86.82} & {89.46} & 88.19 &  {93.06} & 96.30  & {85.36} \\
& S5 \rebuttal{(v1)} & 61.00 & 86.51 & 88.26 & 86.14 & 87.57 & 85.25 & 82.46 \\
& \rebuttal{S5 (v2)} & \rebuttal{62.15} & \rebuttal{89.31} & \rebuttal{\textbf{91.40}} & \rebuttal{88.00} & \rebuttal{95.33} & \rebuttal{\textbf{98.58}} & \rebuttal{\underline{87.46}} \\
& Mamba & 38.02 & 82.98 & 72.14& 69.82& 69.26& 67.32& 66.59\\
& \rebuttal{Mamba (From \cite{xlstm})} & \rebuttal{32.5} & \rebuttal{N/A} & \rebuttal{90.2} &\rebuttal{68.9} &\rebuttal{\textbf{99.2}}&  \rebuttal{N/A} & \rebuttal{N/A} \\
 \bottomrule
 \rebuttal{RNN} & \rebuttal{LRU} &\rebuttal{ 60.2}& \rebuttal{89.4}& \rebuttal{89.9} & \rebuttal{\underline{89.0}}& \rebuttal{95.1}& \rebuttal{94.2} & \rebuttal{86.3}\\
 & \rebuttal{xLSTM} & \rebuttal{41.1} & \rebuttal{N/A} & \rebuttal{90.6} &\rebuttal{69.5} &\rebuttal{91.9}&  \rebuttal{N/A} & \rebuttal{N/A} \\
  \bottomrule
\multirow{13}{*}{\begin{tabular}{l} Transformer as\\ Linear Recurrent\\ Model \end{tabular}} & Local Att.   & 15.82 &	52.98 &	53.39 &	41.46 &	66.63& \xmark &	46.06 \\
& Sparse Transformer   & 17.07	& 63.58 &	59.59 &	44.24 &	71.71& \xmark		& 51.24 \\
& Longformer     & 35.63	& 62.85	& 56.89	& 42.22	& 69.71& \xmark	&  53.46 \\
& Linformer    & 16.13	& 65.90	& 53.09	& 42.34	& 75.30& \xmark	&  	50.55 \\
& Reformer   & 37.27 & 56.10 & 53.40 & 38.07 & 68.50 & \xmark &  50.67 \\
& Sinkhorn Trans.    & 33.67 & 61.20 & 53.83 & 41.23 & 67.45& \xmark & 51.48 \\
& BigBird   & 36.05	& 64.02	& 59.29	& 40.83	& 74.87& \xmark	& 	55.01 \\

& Linear Trans.   & 16.13 & 65.90 & 53.09 & 42.34 & 75.30& \xmark &  50.55 \\
& Performer     & 18.01 & 65.40 & 53.82 & 42.77 & 77.05 & \xmark & 51.41 \\
& FNet    & 35.33 & 65.11 & 59.61 & 38.67 & 77.80 & \xmark &  55.30 \\
& Nyströmformer    & 37.15 & 65.52 & 79.56 & 41.58 & 70.94& \xmark &  58.95 \\
& Luna-256  & 37.25 & 64.57 & 79.29 & 47.38 & 77.72& \xmark &  61.24 \\
& H-Transformer-1D   & 49.53 & 78.69 & 63.99 & 46.05 & 68.78& \xmark & 61.41 \\
\rowcolor{Green!10} 
&\rebuttal{\lionlit}&\rebuttal{16.78}&\rebuttal{65.21}&\rebuttal{54.00}&\rebuttal{43.29}&\rebuttal{72.78} & \rebuttal{\xmark }&\rebuttal{50.41}\\
\rowcolor{orange!17}
& \lions  & \underline{62.25} & {88.10} & {90.35} & {86.14} & {91.30} & \underline{97.99} & 86.07 \\
         \bottomrule
    \end{tabular} }
    \label{tab:lra_exp}
\end{table*}




\subsection{Long Range Arena}

\looseness=-1We assess the performance of \lions on the Long Range Arena (LRA) \citep{lra}, a well-established benchmark for efficient transformers. 
As shown in Table~\ref{tab:lra_exp}, \lions is the only Transformer employing recurrent inference to achieve an impressive \(86.07\%\) on the LRA dataset and is capable of tackling the challenging Path-X problem, where other linear recurrent models shows clear limitations. 
Our results indicate that \lions achieves performance comparable to SSMs, which are renowned for their capabilities to capture long-range interactions within data and excel in the LRA task. 
Furthermore, among transformers, MEGA \citep{ma2022mega} is the only one with roughly ten times the parameter count that demonstrates performance comparable to \lions. 
An extensive discussion on the choice of non-linearity, scaling, and dimensions of parameters is presented in Appendix \ref{lraconfig} and \ref{sec:lraabl}. For more information on the LRA benchmarks, see Appendix~\ref{sec:lra_bench}.





When considering the number of tokens (analogous to resolution in images) that Transformer-like architectures can effectively process, two primary limitations emerge: ($i$) positional embeddings and ($ii$) memory constraints. Transformers are typically trained up to a specific sequence length and lack predefined positional encodings for tokens that exceed this limit, which can hurt their ability to recognize token positions beyond trained lengths. Furthermore, the quadratic complexity of these models during inference places significant demands on memory resources, often leading to constraints that reduce processing efficiency.



\lions architecture is free of these two limitations. In Figure \ref{fig:blm} (\textit{Left}), we test the \lions model trained on $128$ tokens tested on different lengths. While BERT and \lionlit models peak at the training length, afterwards they experience a sharp decrease. \lions, on the other hand, maintains its high performance at $3.5$ times of training sequence length. Additionally, \lions can infer beyond the $512$ tokens, as compared to the other models.

For memory usage during inference,
as Figure~\ref{fig:blm} (\textit{Center}/\rebuttal{\textit{Right}}) illustrates, \lions demands significantly lower memory than ViT on image classification or BERT on MLM tasks.
Due to the quadratic complexity of Transformers, as the resolution of the image increases, the memory consumption also drastically changes.
As a result, ViT \rebuttal{and BERT} go out of memory (OOM) even with small batch sizes. With the \lions architecture, thanks to the linear complexity during inference, the change in memory consumption is minimal. At $1248$ resolution, \lions is $\sim 94.4\%$ more efficient than the ViT-T model. \rebuttal{Similarly, at sequence length $14,336$, \lions is $83.35\%$ more efficient than BERT.}
These two strengths combined, i.e., usage of the recurrence parameters and linear inference complexity, allow \lions to efficiently extrapolate beyond the context length (or resolution) during inference. For further results on length expansion, see Appendices~\ref{subsec:causal_lm} and \ref{subsec:image_ablation}. 



