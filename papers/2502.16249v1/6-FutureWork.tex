Our experiments focused on three main mask choices, but \lion has the potential to accelerate other Linear Transformer variants mentioned in \cref{sec:map} for bidirectional tasks. The chunkwise parallel implementation during inference was done in PyTorch, leaving room for optimization through GPU kernel programming to reduce I/O overhead and improve speed. Additionally, Hydra and Mamba activations led to unstable training under full attention, suggesting \lion could be used to stabilize these variants in the future.
