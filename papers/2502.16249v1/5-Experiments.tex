\begin{figure}[t] 
    \centering
    \includegraphics[width=0.9\columnwidth]{figs/decay_chunking.pdf}
    \vspace{-1mm}
  \caption{\looseness=-1 %
  \textit{Chunkwise Parallel \lionretnet.} 
  The inference memory and time trade-off is demonstrated using three versions of \lionretnet. While RNN is the most memory-efficient approach, enabling the processing of higher resolutions, it is slower than both chunkwise parallelization and full attention. 
  Chunking strikes a balance by allowing higher resolutions than full attention while maintaining comparable or even faster inference speed.
  } \vspace{-7mm}
  \label{fig:chunking_decay_fig}
\end{figure}


Our main results focus on well-known bidirectional sequence modeling tasks: Image Classification on ImageNet-1K \cite{russakovsky2015imagenet} and Masked Language Modeling (MLM) on the C4 dataset \cite{dodge2021c4}. We evaluate the \lion family on these tasks %
Additionally, we conduct %
experiments on %
the LRA dataset to ensure the stability of our framework. For Causal Language Modeling and additional experiments, we refer to \cref{subsec:causal_lm,sec:app_experiments}.%
\vspace{-1mm}
\subsection{Baselines}
We evaluate the \lion framework using its adapted bidirectional Linear Transformers \lions, \lionretnet, and \lionlit and compared them with softmax-based Transformers, including ViT \cite{vit} and DeiT \cite{deit}, as well as bidirectional SSMs such as Vision Mamba (Vim) \cite{zhu2024visionmambaefficientvisual} and Hydra \cite{hwang2024hydrabidirectionalstatespace} for image classification. For Masked Language Modeling, we compare \lion against BERT \cite{fu2023monarch}.


\begin{table}[t]
        \captionof{table}{\textit{Image classification task results.} We present the Top-1 accuracy on the validation data. $\natural$ represents the models with changed path order.
        For training times, the time of ViT stands as the metric and the rest is calculated compared to it.
        The best and the second best results (except ViT) for each model are shown with \textbf{bold} and \underline{underline} respectively.
        }
        \vspace{-2mm}
        \label{tab:imc}
          \centering
             \resizebox{1\linewidth}{!}{ 
                \begin{tabular}{clccc}
                \toprule
                    & Model    & $\#$Param & \begin{tabular}{@{}l@{}}  Imagenet \\ Top-1 Acc. \end{tabular}   & Train. time\\
                     \bottomrule
                    \parbox[t]{3mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{Small}}} &ViT    & 22M &  72.2  &$\times 1$\\
                     &DeiT & 22M & 79.8 &$\times 1$\\
                     &Hydra & 22M & 78.6 &$\times 2.50$\\%_{\pm 0.01}$\\
                     &\rebuttal{Vim} & 26M & \rebuttal{\textbf{80.3}}& $\times 14.95$\\%_{\pm 0.06}$ \\
                    &\cellcolor{Green!10} \lionlit& \cellcolor{Green!10} 22M & \cellcolor{Green!10} 72.4& \cellcolor{Green!10}\textbf{$\mathbf{\times 0.74}$}\\%_{\pm 0.02}$\\
                    &\cellcolor{ violet!20}
                    \rebuttal{\lionretnet} & \cellcolor{ violet!20} 22M & \cellcolor{ violet!20} 
                    \rebuttal{73.5} & \cellcolor{ violet!20}\underline{$\times 1.49$}\\%_{\pm 0.01}$\\
                    &\cellcolor{ violet!20}
                    \textbf{\lionrotd} & \cellcolor{ violet!20} 22M & \cellcolor{ violet!20} \underline{79.9} & \cellcolor{ violet!20}$\times 1.66$\\%_{\pm 0.01}$\\
                    &\cellcolor{orange!17}
                    \textbf{\lions}& \cellcolor{orange!17} 22M & \cellcolor{orange!17} 74.0 & \cellcolor{orange!17}$\times 2.03$\\%_{\pm 0.14}$\\
                    &\cellcolor{orange!17}
                    \textbf{\lionrot} & \cellcolor{orange!17} 22M & \cellcolor{orange!17} 79.6 & \cellcolor{orange!17}$\times 2.72$\\%_{\pm 0.65}$\\
                    \bottomrule
                    \parbox[t]{3mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{Base}}} &ViT    & 86M &   77.9 &$\times 1$\\
                     &DeiT & 86M & \underline{81.8} &$\times 1$\\
                     &Hydra & 104M & 81.0 &$\times 2.51$\\%_{\pm 0.00}$\\
                     &\rebuttal{Vim} & 98M & \rebuttal{\textbf{81.9}} &$\times 10.86$\\%_{\pm 0.02}$\\
                    &\cellcolor{Green!10} \lionlit& \cellcolor{Green!10}  86M & \cellcolor{Green!10}  74.7 & \cellcolor{Green!10} \textbf{$\mathbf{\times 0.73}$}\\%_{\pm 0.01}$\\
                    &\cellcolor{ violet!20}
                    \rebuttal{\lionretnet} & \cellcolor{ violet!20} 86M & \cellcolor{ violet!20} 77.8& \cellcolor{ violet!20}\underline{$\times 1.39$}\\%_{\pm 0.01}$\\
                    &\cellcolor{ violet!20}
                    \textbf{\lionrotd} & \cellcolor{ violet!20} 86M  \cellcolor{ violet!20}& \cellcolor{ violet!20} {80.2}& \cellcolor{ violet!20}$\times 1.48$\\%_{\pm 0.01}$\\
                    &\cellcolor{orange!17}
                    \textbf{\lions}& \cellcolor{orange!17} 86M & \cellcolor{orange!17} 76.3 & \cellcolor{orange!17}$\times 1.46$\\%_{\pm 0.02}$\\
                    &\cellcolor{orange!17}
                    \textbf{\lionrot} & \cellcolor{orange!17} 86M & \cellcolor{orange!17} 79.9 & \cellcolor{orange!17}$\times 1.68$\\%_{\pm 0.11}$\\
                    \bottomrule
                \end{tabular} 
                }
       \vspace{-3mm}
\end{table}


\subsection{Image Classification}
\label{subsec:imc}
\looseness=-1 To enhance the spatial representation of patches (tokens) for image classification, we designed different masks for \lions and \lionretnet by altering token order. This strategy resulted in the \lionrotd and \lionrot models, which naturally capture spatial information of patches while preserving the benefits of the \lion framework. Details of the patching technique are provided in the \cref{subsec:rotation}. 

We evaluated the performance, efficiency, and training times of our framework against state-of-the-art SSMs and Transformers for image classification. As shown in \cref{tab:imc}, models using the \lion framework achieve competitive performance with vision-specific SSMs like Vim, while being significantly faster and more parallelized during training. \lionrotd performs comparably to Vim and surpasses Hydra on small scale, with $9\times$ faster training than Vim. On base-scale models, \lion achieves state-of-the-art performance while maintaining a significant training speed advantage $1.7\times$ faster than Hydra. Notably, \lionlit demonstrates the highest training speed across all scales, which demonstrates that training with full linear attention is\textit{ significantly faster} than chunkwise parallel training (SSD for Hydra) and considerably faster than the scan algorithm, even with optimized GPU kernels (as used in Vim).


The memory efficiency of the \lion framework compared to other baselines is shown in \cref{fig:blm}, where the inference memory usage for images with a batch size of 64 is measured. \lion can efficiently process significantly high resolutions, such as 2496, while other models run out of memory (OOM) at much lower resolutions.

\subsection{Inference Memory-Speed Trade off for \lion in RNN, Attention and Chunk form}

To illustrate the trade-offs between memory and inference speed, we evaluated \lionretnet using three strategies: 1) Full Linear Attention \cref{eq:attvecbid}, 2) Bidirectional RNN \cref{eq:bestrnn}, and 3) \lion Chunk \cref{chunklion}. As shown in \cref{fig:chunking_decay_fig}, the RNN form has the highest memory efficiency, while full attention is the most memory-intensive. \lion chunk strikes a balance, with memory usage between full attention and RNN. Both full attention and chunking provide significantly faster inference than the RNN form, demonstrating the memory-speed trade-off. Furthermore, \lion chunk achieves even faster inference than full attention while being more memory-efficient, making it effective for balancing performance and resource constraints. Detailed evaluations for \lions and \lionlit appear in \cref{subsec:chunks}, with inference time comparisons in \cref{subsec:inf_time}. 

We also provide ablations on distilling \lions from RegNetY-4GF using the DeiT recipe (\cref{app:distill}), and compare \lion variants with their causal counterparts to highlight the importance of full attention for bidirectional tasks (\cref{app:direction2,app:direction}). Tiny scale results are in \cref{app:tiny}, and training details in Appendix~\ref{subsec:image_hyper}. 

\label{subsec:context_length}

\begin{table}[t]
    \caption{\textit{C4 MLM and GLUE results for the LARGE scale ($334$M).} For each dataset, the best and second best results are highlighted with \textbf{bold} and \underline{underline} respectively.}
    \centering
    \vspace{-2mm}
    \resizebox{0.8\columnwidth}{!}{
    \rebuttal{
    \begin{tabular}{l|c|c|c}
    \toprule
        Model & MLM Acc. & GLUE & Train. time\\
        \midrule 
        BERT  & $\underline{69.88}$ & $\mathbf{82.95}$ & $\times 1$\\ 
        Hydra & $\mathbf{71.18}$ & $\underline{81.77}$ & $\times 3.13$ \\ %
        \rowcolor{Green!10} \lionlit  & 67.11 & 80.76 & {$\times {\textbf{0.95}}$} \\
        \rowcolor{ violet!20}
        \rebuttal{\lionretnet}  & \rebuttal{68.64}& \rebuttal{81.34} & $\times {\underline{1.10}}$\\ 
        \rowcolor{orange!17} \lions  & $69.16$ & $81.58$ & $\times {1.32}$\\
        \bottomrule
    \end{tabular}}  }
    \label{tab:MLM}
\vspace{-4mm}
\end{table}

\subsection{Masked Language Modeling (MLM)}
\label{subsec:mlm}
\looseness=-1We assess BERT and the \lion variants in the MLM task, which is ideally suited for bidirectional models \citep{devlin2019bert,liu2019roberta}. 

We pretrain the LARGE family of models (334M parameters) on the C4 dataset \citep{dodge2021c4} and finetune in the GLUE benchmark \citep{wang2018glue}. For experimental details and additional experiments, we refer to \cref{subsec:details_glue,subsec:ablations_glue,subsec:glue_small}. We measure the average time of a forward-backward with batch size $32$ over $60$ batches.





In \cref{tab:MLM}, without extensive tuning, the \lion models closely follow BERT and Hydra \citep{hwang2024hydrabidirectionalstatespace} in both the MLM pretraining task and the GLUE finetuning tasks. Moreover, in \cref{fig:blm} we can observe that the \lion family does not add significant training overhead to the BERT training, at the same time as it can maintain the linear scaling of the memory \cref{fig:bert_memory}.%
 

\subsection{Long Range Arena Stability Ablation}

We  evaluated \lions, \lionretnet, and \lionlit on the Long Range Arena (LRA) benchmark to verify the stability of training with full masked attention. We expand the dimensions of \(\lambda_i\) (as described in \cref{sec:expandai}) and initialized \(a_i\) based on HIPPO theory \cite{hippo} for \lions and \lionretnet to solve LRA tasks. However, \lionlit, a bidirectional Linear Transformer without a decay factor, was unable to solve LRA tasks. Additionally, randomly initializing \lions and \lionretnet were not able to solve LRA consistent to prior work \cite{nts}. For LRA tasks, the selectivity of \lions takes the form \(a_i = \sigma(A_i) + B_i\), where \(B_i\) is initialized using HIPPO-based diagonal initialization \cite{gupta2022diagonalstatespaceseffective}. We also observed that omitting attention scaling led to poor performance, indicating that \textit{scaling attention plays a crucial role} in improving training stability (more ablations at \cref{tab:lra} \& \cref{laasjdhakjsdh}).

\begin{table}[t] 
    \centering
        \caption{\textit{Training Stability on Long Range Arena Task.} Our family of models can solve the LRA benchmark with the appropriate initialization using full attention. 
       }
    \vspace{-2mm}
    \resizebox{0.47\textwidth}{!}{
\begin{tabular}{lcccccc|cc}
\toprule
 Model & ListOps & Text & Retrieval	& Image	& Pathfinder & PathX & Avg. \\
 (input length) & 2048 & \rebuttal{4096} & 4000 & 1024 & 1024  & 16K & \\
 \bottomrule
\rowcolor{Green!10} 
\rebuttal{\lionlit}&\rebuttal{16.78}&\rebuttal{65.21}&\rebuttal{54.00}&\rebuttal{43.29}&\rebuttal{72.78} & \rebuttal{\xmark }&\rebuttal{50.41}\\
\rowcolor{violet!20}
 \lionretnet (w/o \textit{HIPPO})  & 32.5 & 64.5& 50.0& 47.4 & 73.6& \xmark& 53.6\\
\rowcolor{orange!17}
 \lions (w/o \textit{HIPPO}) & 36.34 & 60.87 & 55.0 & 42.6 & 74.2 & \xmark & 53 \\
 \bottomrule
\rowcolor{violet!20}
 \lionretnet (w/ \textit{HIPPO})  & 62.0 & 88.78& 90.12& 85.66 & 90.25& 97.28& 85.63\\
\rowcolor{orange!17}
 \lions (w/ \textit{HIPPO}) & {62.25} & {88.10} & {90.35} & {86.14} & {91.30} & {97.99} & \textbf{86.07 }\\
         \bottomrule
    \end{tabular} }
    \label{tab:lra_exp} \vspace{-4mm}
\end{table}



