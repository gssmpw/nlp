\section*{Paper structure for {ICLR 2025}}

\begin{itemize}[leftmargin=0pt]

\item[] {\large Introduction}
\begin{itemize}
    \item High-level perspective: classic Transformers, their widespread adoption and limitations
    \item The new SSMs: their advantages and literature gap
    \item In this paper, we provide a novel perspective on Transformers by introducing \ours, which combines the best of both worlds
    \item List of contributions: general attention framework for various tasks with efficiency and performance benefits (learnable $a_i$ without positional embeddings, multiple training strategies for different task types, both encoder and decoder attention with the bidirectional RNN, inference efficiency for long sequences)
    \item Organization of the paper
\end{itemize}

\item[] {\large Transformers and SSMs}
\begin{itemize}
    \item Intoduce notation of query, key, and value
    \item Attention output in Transformers, with and without mask
    \item Introduce Mamba formulation of the Transformer
    \item Connections between Transformers, RNNs, and SSM (Tranformers are RNNs and Mamba2)
    \item Explain how Transformers can be represented as RNN and as SSM, with similarities and differences in the structure.
    \item Possibly also LRU. You can check S5 paper structure
\end{itemize}

\item[] {\large The Selective Transformer}
\begin{itemize}
    \item Contribution \#1: We introduce the $a_i$ in the RNN version of the Transformer
    \item The different ways $a_i$ can be chosen (scalar, matrix, etc.)
    \item The new learnable mask in Transformer
    \item Advantage: no positional embedding $\to$ better generalization for longer sequences (we can have a diagram with windows like in Mamba)
\end{itemize}

\item[] {\large Attention As Bidirectional RNN}
\begin{itemize}
    \item We now extend to full attention
    \item Contribution \#2: Show the 3 paths in the full attention
    \item Remarks and connections
    \item We need to make it clear the connection and difference with the original attention where they also use bidirectional RNN \citep[Sec. 3]{bahdanau2015neural}
    \item Repeat we do not need positional embeddings even for non-causal tasks, e.g., ViT.
\end{itemize}

\item[] {\large \ours}
\begin{itemize}
    \item We can train in two ways depending on the task (e.g., RNN for LRA and Transformer for GPT), but we can always do inference efficiently in the RNN form, both for causal and non-causal tasks
    \item Give full model description (no positional embeddings, initialization, architecture)
    \item Give training details
    \item Compare complexity vs. Transformers: big advantage
    \item Mamba has a similar section we can check
\end{itemize}

\item[] {\large Remarks}
\begin{itemize}
    \item We are not the first work to consider kernelized attention, so our focus should not be on how to handle the softmax as many previous works already dealt with this. We can simply reference them.
    \item Our paper could also be positioned similarly to \citep{han2024demystify}, where they focus on a new perspective on Transformers and SSMs. Our paper also proposes a new model, so we first reveal a new perspective on Transformers thanks to the SSM and, based on that, we remodel the Transformer in \ours to make it more efficient and better generalizing on long sequences for both LRA, GPT, BERT, etc.
\end{itemize}
\end{itemize}
