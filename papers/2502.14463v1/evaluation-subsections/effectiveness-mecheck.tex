  \vspace{-.5em}
\subsection{Accuracy of \tool}\label{ss:eval-effect-mecheck}

To assess how accurately \tool can detect metadata-related bugs, we created a ground truth dataset of buggy EAs (Section~\ref{sss:data}), defined evaluation metrics (Section~\ref{sss:metrics}), and evaluated \tool using the dataset and metrics (Section~\ref{sss:bug-detect}). 

  \vspace{-.5em}
\subsubsection{A Dataset of Buggy EAs} \label{sss:data}
We mined GitHub~\cite{github} for enterprise applications, using the heuristic-based approach proposed by prior work~\cite{Wen20}. Specifically, we crawled GitHub for any project that contains at least one XML file, whose file path has any of the following keywords: ``Spring", ``security", ``web", and ``WEB-INF". If a project satisfies this requirement, it is likely to be an EA. 
Next, we refined the mined projects by removing irrelevant and redundant projects, getting \totalProject projects.

To create the dataset of buggy EAs, we injected \totalInjection bugs in \totalInjectedProject different projects. To inject bugs for each constraint mentioned in Table~\ref{tab:constraints}, we leveraged the involved metadata items (e.g., ``\codefont{@RunWith(Parameterized.class)}'' or ``\codefont{<property>}'') as keywords to search for three different projects. We manually inspected the relevant metadata usage to ensure that each retrieved project does not violate the corresponding constraint. Then we modified the metadata to intentionally inject a bug for each of the projects. For the 15 rules, we retrieved and modified in total \totalInjectedProject(=15*3) projects. In this way, we injected \totalInjection bugs to \totalInjectedProject distinct open-source projects, making each analyzer inside \tool have 3 bugs to find. We considered these \totalInjection injected bugs as ground truth to evaluate the detection capability of \tool.


% To create the dataset of buggy EAs, for each constraint mentioned in Table~\ref{tab:constraints}, we leveraged the involved items (e.g., ``\codefont{<property>}'') as keywords to search for 3 relevant projects within \totalProject projects. We ensured that each retrieved project does not violate the corresponding constraint, and then manually modified code or metadata to violate that constraint. In this way, we injected \totalInjection bugs to \totalInjectedProject distinct open-source projects, making each analyzer inside \tool have 3 bugs to find.

   \vspace{-.5em}
\subsubsection{Metrics} \label{sss:metrics}
	We used three metrics to evaluate \tool's effectiveness for bug detection.

\textbf{Precision (P)} measures among all bugs \tool reported, how many of them are true positives:
\begin{equation*}\label{eq:precision}
 \vspace{-.5em}	
	    P = \frac{\mbox{\# of correctly reported bugs}}{\mbox{Total \# of bug reports}} 
  %\vspace{-.5em}    
	\end{equation*}
	
%	Precision can be used to evaluate the effectiveness of rule application. For rule application, P measures among all reported bugs, how many of them are real bugs.\\
\textbf{Recall (R)} measures among all known bugs, how many of them are reported by \tool:
	\begin{equation*}
	\label{eq:recall}
 \vspace{-.5em} 
	    R = \frac{\mbox{\# of correctly reported bugs}}{\mbox{Total \# of known bugs}}
	\end{equation*}
%	Based on the Spring specification and our manual inspection, we managed to construct a ground truth data set and evaluated  the recall of the rule application.\\

\textbf{F-score (F)} is the harmonic mean of P and R; it reflects the bug-detection accuracy of \tool: 
	\begin{equation*}
	\label{eq:fscore}
 \vspace{-.5em} 
	    F = \frac{2 \times P \times R}{P + R} 
	\end{equation*}
All metrics mentioned above have values in [0\%, 100\%]; the higher the better. To compute P and R, we will intersect the set of bugs reported with the ground-truth bug set; any overlap between the two sets captures bugs correctly reported by \tool. Thus, the larger overlap there is, the better.

%Any overlap between the two sets 
%the scenarios where \tool correctly reports bugs

\begin{comment}
\begin{table}
\footnotesize
\centering
\caption{The evaluation result of \tool  on our dataset of injected bugs}\label{tab:synthetic}
  \vspace{-1.5em}
\begin{tabular}{l|p{3cm}|R{2.3cm}|R{2.3cm}|R{0.7cm}R{0.7cm}R{0.7cm}}
\toprule
\textbf{Id} &\textbf{Rule Name} & \textbf{\# of Injected Bugs} & \textbf{\# of Detected Bugs} & \textbf{P} & \textbf{R} & \textbf{F} \\
\toprule
$r_1$& xml-path-check & 3 & 3 & 100\% & 100\% & 100\% \\\hline
$r_2$&bean-class-exists & 3 & 1 & 100\% & 33\% & 50\% \\
\hline
$r_3$&constructor-arg-type-field-map & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_4$&constructor-arg-name-field-map & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_5$&constructor-index-out-of-bound & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_6$&method-exists & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_7$&property-setter-map & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_8$&runwith-no-parameters & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_9$&runwith-no-test & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_{10}$&runwith-no-suiteclasses & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_{11}$&suiteclasses-no-runwith & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_{12}$&suiteclasses-no-test & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_{13}$&testParams-not-iterable & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_{14}$&import-resource-path & 3 & 3 & 100\% & 100\% & 100\% \\
\hline
$r_{15}$&bean-exists & 3 & 3 & 100\% & 100\% & 100\% \\
\bottomrule
\multicolumn{2}{c|}{\textbf{Overall}} & \textbf{45} & \textbf{43} & \textbf{\precision} & \textbf{\recall} & \textbf{\fscore} \\
\bottomrule
\end{tabular}
  \vspace{-1.5em}
\end{table}
\end{comment}

\subsubsection{Experiment Results} \label{sss:bug-detect}
%As shown in Table~\ref{tab:synthetic}, 
\tool detected bugs with high precision (\precision), high recall (\recall), and high F-score (\fscore). In total, \tool reported 43 bugs, all of which match the ground truth of injected bugs. However, it missed two bugs related to the rule $r_2$ bean-class-exists, due to the design choice we made when implementing \tool.

  \vspace{-.5em}
\begin{lstlisting}[label={lst:bean-class-exists},caption=The RSL rule of bean-class-exists]
Rule bean-class-exists {
  for (file xml in getXMLs()) {
    if (elementExists(xml, "<bean>")) {
      for (<bean> bean in getElms(xml, "<bean>")) {
        String beanClassFQN = getAttr(bean, "class");
        assert(classExists(beanClassFQN) OR isLibraryClass(beanClassFQN)) {
          msg("Bean class: %s mentioned in bean: %s, does not exist", beanClassFQN, getName(bean)); }}}}}  
\end{lstlisting}

With more details, $r_2$ ensures that if the fully qualified name of a Java class is mentioned as the \codefont{class}-attribute of any \codefont{<bean>}, the class should be either defined by the 
EA-under-analysis or a library on which EA depends. Listing~\ref{lst:bean-class-exists} shows the RSL specification of bean-class-exists, which calls \codefont{callExists(...)} and \codefont{isLibraryClass(...)} on line 6. 
In \tool, \codefont{classExists(...)} is implemented to check whether the pass-in parameter matches any class item extracted from Java code.
The implementation of \codefont{isLibraryClass(...)} defines a list of regular expressions (RegEx) to describe the naming patterns of frequently used library classes, such as ``\codefont{\textasciicircum org\textbackslash.hibernate\textbackslash..+\textdollar}'' for Hibernate classes. We crafted the regular expressions based on (1) our domain knowledge of widely used libraries, and (2) libraries we frequently observed in EAs.
Next, \codefont{isLibraryClass(...)} 
 examines whether a pass-in parameter matches any RegEx to determine if the class is defined by a library. 
We manually injected each of the two missed bugs, by specifying an invalid class name as the \codefont{class}-attribute of \codefont{<bean>}. Ideally, \tool should report both bugs because the \codefont{<bean>}-classes do not exist. However, since the invalid names we specified (e.g., \codefont{org.hibernate.search.hibernate.example.dao.impl.BookDaoImplChanged}) accidentally match predefined RegEx patterns, \tool incorrectly considered them to be valid and failed to locate both bugs.

We could have reduced such false negatives by discarding some RegEx patterns used in function \codefont{isLibraryClass(...)}. However, the downside of doing so is that 
many library classes will fail to match the remaining patterns. Consequently, \tool will wrongly interpret those classes as non-library classes, and report false positives when it also fails to find those classes defined by EA. 
In another word, we may get false positives when trying to overcome such false-negative issues. We prefer generating precise bug reports as developers may not want to get frequently bothered  with false positives. Thus, we designed \tool to include more RegEx patterns and report bugs more precisely, instead of improving recall rates at the cost of sacrificing precision. 



%Because of these two techniques, when we ran \tool on our synthetic dataset we didn't have any false positives reported. As a result we had 100\% precision, 100\% recall, and 100\% accuracy for our experiment on the synthetic dataset (Table \ref{table:ruleEffect_groundTruth}).

\vspace{0.5em}
\noindent\begin{tabular}{|p{13.6cm}|}
	\hline
	\textbf{Finding 2 (Responding to RQ2):} \emph{\tool demonstrated great detection accuracy when being applied to our 45-project dataset with injected bugs. It found bugs with \precision precision, \recall recall, and \fscore F-score. }
	\\
	\hline
\end{tabular}

