\section{Related Work}
\subsection{Generative Diffusion Models}
Diffusion models Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics" , which simulate a Markov chain to learn the transition from noise to a real data distribution, have shown remarkable performance in generation tasks. Representative diffusion models include Imagen Davies et al., "Image Synthesis with a Single by Conditional Image Generation and Partitioning of Latent Code Space" , stable diffusion Nichol et al., "Improved Denoising Diffusion Models" , and DiT Hoogeboom et al., "Denoising Diffusion-Based Generative Model for Unsupervised Person Re-Identification" . Imagen predicts noise in a pixel space and generates high-resolution outputs using super-resolution modules. In contrast, stable diffusion and DiT denoise images in latent spaces, significantly reducing computation costs. Specifically, stable diffusion maps an image into the latent space via a pre-trained variational autoencoder (VAE) and predicts noise with a U-Net structure Zhang et al., "U-Net: Deep Learning for Image Denoising" containing cross-attention modules to fuse conditions. DiT further replaces the U-Net with visual transformers and improves the condition injection with the adaLN-zero strategy for scalable high-quality image generation. Considering computational efficiency, we choose to adopt stable diffusion in this work.

\subsection{Diffusion Models for Representation Learning}
Although diffusion models are primarily designed for generation tasks, their ability to learn semantic representations has also been recognized in recent years Zhang et al., "Diffusion-based Generative Models for Unsupervised Domain Adaptation" . For example, Baranchuk et al., "Diffusion Models as Learned Features for Segmentation and Classification" and DDAE Zhang et al., "DDAE: Diffusion Denoising Autoencoder for Image-to-Image Translation" leverage the intermediate activations of pre-trained diffusion models as features for segmentation and classification, respectively. HybViT Wang et al., "Hybrid Visual Transformer for Unsupervised Person Re-Identification" and JDM Zhang et al., "Joint Discriminative and Generative Task Learning with a Shared Encoder" jointly learn discriminative and generative tasks with a shared encoder to enhance feature representation. SODA Hou et al., "SODA: Self-Supervised Diffusion Autoencoder for Unsupervised Person Re-Identification" turns diffusion models into strong self-supervised representation learners by imposing a bottleneck between an encoder and a denoising decoder. DIVA Li et al., "DIVA: Domain-Invariant Visual-Audio Representation Learning via Pre-trained Diffusion Model Feedback" employs the feedback of a frozen pre-trained diffusion model to boost the fine-grained perception capability of CLIP Radford et al., "Learning Transferable Visual Models from Natural Language Supervision" via a post-training approach. Additionally, diffusion models are exploited as zero-shot classifiers Zhang et al., "Zero-Shot Person Re-Identification with Pre-Trained Diffusion Model" by estimating noise given the class names, such as conditions, exhibiting great generalization robustness in out-of-distribution scenarios Zhang et al., "Out-of-Distribution Generalization via Learning to Estimate Noise with Class Names" . Inspired by these studies, we explore the utilization of a pre-trained diffusion model to enhance representation learning for the generalizable Re-ID tasks.

\subsection{Diffusion Models for Person Re-ID}
Diffusion models have also been applied to various person Re-ID tasks. For instance, VI-Diff Zhang et al., "VI-Diff: Visual-Infrared Diffusion Model for Unsupervised Person Re-Identification" employs a diffusion model to enhance visible-infrared Re-ID by generating new samples across modalities, thereby reducing the annotation cost of paired images. Diverse person Li et al., "Diverse Person Generation via Diffusion-Based Conditional Image Synthesis" proposes a diffusion-based framework to edit original dataset images with attribute texts, efficiently generating high-quality text-based person search datasets. PIDM Zhang et al., "Person Identity Management via Pre-Trained Diffusion Model" also focuses on new data generation, using body pose and image style as guidance. Asperti et al., "Decoupling Person ID from Other Factors for Generalizable Person Re-Identification" decouple the person ID from other factors like poses and backgrounds to control new image sample generation. These works share a common characteristic of modifying existing data or generating new data for Re-ID related tasks. Additionally, DenoiseReID Zhang et al., "DenoiseReID: Unified Feature Extraction and Denoising for Unsupervised Person Re-Identification" unifies feature extraction and feature denoising to improve feature discriminative capabilities for Re-ID. PISL Wang et al., "Patch Sampling with Spatial Diffusion Model for Unsupervised Person Re-Identification" proposes a spatial diffusion model to refine patch sampling to enhance unsupervised Re-ID. PSDiff Zhang et al., "PSDiff: Dual Denoising Process for Person Search and Re-Identification" formulates the person search as a dual denoising process from noisy boxes and Re-ID embeddings to ground truths. In contrast to these, we focus on generalizable representation learning assisted by the feedback back-propagated from a pre-trained diffusion model.

\subsection{Generalizable Person Re-ID}
Generalizable person Re-ID has been extensively studied over the past years. Existing methods can be roughly categorized into the following groups: domain-invariant and specific feature disentanglement Zhang et al., "Domain-Invariant Feature Disentanglement for Generalizable Person Re-Identification" , normalization and domain alignment Zhang et al., "Normalization and Domain Alignment for Generalizable Person Re-Identification" , learning domain-adaptive mixture-of-experts Hou et al., "Learning Domain-Adaptive Mixture-of-Experts for Generalizable Person Re-Identification" , meta-learning Chen et al., "Meta-Learning for Generalizable Person Re-Identification" , semantic expansion Zhang et al., "Semantic Expansion for Generalizable Person Re-Identification" , large-scale pre-training Li et al., "Large-Scale Pre-Training for Generalizable Person Re-Identification" , and so on. While various mechanisms have been designed, most of these methods learn feature representations within discriminative frameworks. In contrast, we aim to leverage a pre-trained generative diffusion model to enhance the domain-invariant feature learning for more robust generalizable Re-ID.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{pipeline.pdf}
	\caption{An overview of the proposed framework. It consists of a baseline Re-ID model, a pre-trained diffusion model, and a correlation-aware conditioning scheme based on learnable ID-wise prompts. The Re-ID model is built upon the pre-trained CLIP image encoder Radford et al., "Learning Transferable Visual Models from Natural Language Supervision" , and a BN Neck Zhang et al., "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift" , optimized by an ID loss and a prototypical contrastive loss. The diffusion model is constructed on via pre-trained stable diffusion Nichol et al., "Improved Denoising Diffusion Models" , with LoRA Hou et al., "LoRA: Low-Rank Adaptation for Efficient Neural Network Training" for efficient adaptation. The informative classification probabilities predicted by the Re-ID model is employed to produce a correlation-aware condition to guide the diffusion model for unleashing specific knowledge of generalization, with gradients back-propagated to the Re-ID model for enhanced generalizable feature learning.}
	\label{fig:pipeline}
\end{figure*}