% WACV 2024 Paper Template
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{microtype}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{*****} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2024}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Unleashing the Potential of Pre-Trained Diffusion Models for Generalizable Person Re-Identification}

\author{Jiachen Li and Xiaojin Gong$^*$\\
	College of Information Science and Electronic Engineering, Zhejiang University\\
	Hangzhou, Zhejiang, China\\
{\tt\small lijiachen\_isee@zju.edu.cn, \tt\small gongxj@zju.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}
\maketitle

\newcommand\blfootnote[1]{%
\begingroup \renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup}

%%%%%%%%% ABSTRACT
\begin{abstract}
Domain-generalizable re-identification (DG Re-ID) aims to train a model on one or more source domains and evaluate its performance on unseen target domains, a task that has attracted growing attention due to its practical relevance. While numerous methods have been proposed, most rely on discriminative or contrastive learning frameworks to learn generalizable feature representations. However, these approaches often fail to mitigate shortcut learning, leading to suboptimal performance. In this work, we propose a novel method called diffusion model-assisted representation learning with a correlation-aware conditioning scheme (DCAC) to enhance DG Re-ID. Our method integrates a discriminative and contrastive Re-ID model with a pre-trained diffusion model through a correlation-aware conditioning scheme. By incorporating ID classification probabilities generated from the Re-ID model with a set of learnable ID-wise prompts, the conditioning scheme injects dark knowledge that captures ID correlations to guide the diffusion process. Simultaneously, feedback from the diffusion model is back-propagated through the conditioning scheme to the Re-ID model, effectively improving the generalization capability of Re-ID features. Extensive experiments on both single-source and multi-source DG Re-ID tasks demonstrate that our method achieves state-of-the-art performance. Comprehensive ablation studies further validate the effectiveness of the proposed approach, providing insights into its robustness. Codes will be available at \href{https://github.com/RikoLi/DCAC}{https://github.com/RikoLi/DCAC}.
\end{abstract}

%%%%%%%%% BODY TEXT
\blfootnote{$^*$The corresponding author.}
\section{Introduction}
\label{sec:intro}
Person re-identification (Re-ID) aims to match a query person's image across different cameras based on the similarity of feature representations. Although supervised Re-ID methods based on convolutional neural network (CNN)~\cite{BoT,LRGCN,DWNet,MGNACP} and visual transformer~\cite{TransReID} have made significant advancements, their performance dramatically degenerates when applied to out-of-distribution (OOD) data that are dissimilar to training scenes. To address this problem, domain-generalizable (DG) Re-ID has garnered increasing interest in recent years. In DG Re-ID, a model is trained on one or multiple source domains and then tested on completely different and unseen domains.

Numerous DG Re-ID methods have been developed so far. Existing studies concentrate on domain-invariant and domain-specific feature disentanglement~\cite{SNR,DIR-ReID,ADIN}, normalization and domain alignment~\cite{CBN,MetaBIN,DTIN-Net,GDNorm,META,GN,LDU}, or employ meta-learning~\cite{MetaBIN,M3L,MDA,SuA-SpML} and other techniques like semantic expansion~\cite{DEX,UDSX} and sample generation~\cite{syed2023lightweight} to enhance the generalization capability of Re-ID models. Although various techniques have been designed, almost all methods learn feature representations within discriminative or contrastive learning frameworks, which are considered unable to prevent shortcut learning~\cite{SC}, leading to suboptimal performance.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{pipeline_compare.pdf}
	\caption{Illustration of different diffusion-based representation learning designs. (a) A separate denoising decoder and a classifier with a shared encoder, (b) intertwined feature extraction and feature denoising, (c) a diffusion model with a separate image encoder for instance-wise conditioning, and (d) a diffusion model and a classification model bridged by a correlation-aware ID-wise conditioning scheme. In addition, (e) illustrates that the dark knowledge embedded in the logits of classifiers is able to capture the ID relationships, including nuanced similarities and differences beyond the hard ID labels, which helps generate better conditions to guide the diffusion model.}
	\label{fig:inter-ID_sim}
\end{figure*}

Recently, diffusion models, such as Imagen~\cite{Imagen} and Stable Diffusion~\cite{StableDiffusion}, have demonstrated remarkable capabilities in image synthesis and other generative tasks. Moreover, their potential for representation learning has been increasingly recognized in recent studies~\cite{DMSS,HybViT,SODA}. On the one hand, pre-training on extensive multi-modal data equips diffusion models with rich semantic information, enabling exceptional generalization and robustness in out-of-distribution scenarios~\cite{jaini2023intriguing}. On the other hand, the denoising process inherently promotes the learning of meaningful semantic representations~\cite{DMRL}. Inspired by these observations, this work explores leveraging a diffusion model to enhance the generalization ability of representations initially learned within discriminative and contrastive learning frameworks, thereby improving domain-generalizable person Re-ID.

To this end, we propose a generalizable Re-ID framework comprising a baseline discriminative and contrastive Re-ID model, a generative diffusion model, and a conditioning scheme that bridges the two models. Unlike existing diffusion-based representation learning techniques, which either jointly train a denoising decoder and a classifier with a shared encoder~\cite{HybViT, JDM} or intertwine feature extraction and feature denoising~\cite{DenoiseReID}, our approach opts to preserve the integrity of the pre-trained diffusion model, as shown in Figure~\ref{fig:inter-ID_sim} (a) to (d). This choice allows the semantic knowledge embedded in the diffusion model to be effectively transferred to the Re-ID baseline. Furthermore, unlike SODA~\cite{SODA} and DIVA~\cite{DIVA}, which adopt an instance-level conditioning mechanism, we introduce a new conditioning scheme that is ID-wise and explicitly aware of ID correlations. 

More specifically, as recognized in knowledge distillation~\cite{hinton2015distilling} and illustrated in Figure~\ref{fig:inter-ID_sim} (e), dark knowledge embedded in the logits of classifiers captures the relationships among IDs, reflecting nuanced similarities and differences that go beyond hard class labels. Therefore, we design a correlation-aware conditioning scheme that integrates classification probabilities with learnable ID-wise prompts as the guidance of the diffusion model. This conditioning scheme makes the diffusion model less sensitive to intra-ID variances and background interference compared to instance-level conditioning and more expressive and adaptable than one-hot ID labels, enabling it to better capture complex inter-ID relationships and improve generalizable Re-ID performance. Additionally, we employ LoRA~\cite{LoRA} adapters to enable the parameter-efficient fine-tuning (PEFT) of the diffusion model alongside the full fine-tuning of the Re-ID model. This approach allows the diffusion model to adapt effectively and efficiently to Re-ID data while preserving the knowledge embedded in the pre-trained diffusion model, thereby mitigating the risk of catastrophic forgetting often associated with full model fine-tuning~\cite{biderman2024lora}.

The main contributions of this work are summarized as follows:
\begin{itemize}
	\item We investigate the feasibility of leveraging a pre-trained diffusion model as an expert to enhance generalizable feature learning for DG Re-ID by collaboratively training a discriminative Re-ID model and efficiently fine-tuning a generative diffusion model.
	\item We propose a simple yet effective correlation-aware conditioning scheme that combines the dark knowledge embedded in ID classification probabilities with learnable ID-wise prompts to guide the diffusion model, unleashing its generalization knowledge to the discriminative Re-ID model through gradient feedback.
	\item Extensive experiments on both single-source and multi-source DG Re-ID tasks demonstrate the effectiveness of our approach, achieving state-of-the-art performance. Additionally, plenty of ablation studies are conducted to provide a comprehensive analysis of the proposed method.
\end{itemize}

\section{Related Work}
\subsection{Generative Diffusion Models}
Diffusion models~\cite{DDPM,DDIM}, which simulate a Markov chain to learn the transition from noise to a real data distribution, have shown remarkable performance in generation tasks. Representative diffusion models include Imagen~\cite{Imagen}, stable diffusion~\cite{StableDiffusion}, and DiT~\cite{DiT}. Imagen predicts noise in a pixel space and generates high-resolution outputs using super-resolution modules. In contrast, stable diffusion and DiT denoise images in latent spaces, significantly reducing computation costs. Specifically, stable diffusion maps an image into the latent space via a pre-trained variational autoencoder (VAE) and predicts noise with a U-Net structure~\cite{U-Net} containing cross-attention modules to fuse conditions. DiT further replaces the U-Net with visual transformers and improves the condition injection with the adaLN-zero strategy for scalable high-quality image generation. Considering computational efficiency, we choose to adopt stable diffusion in this work.

\subsection{Diffusion Models for Representation Learning}
Although diffusion models are primarily designed for generation tasks, their ability to learn semantic representations has also been recognized in recent years~\cite{DMRL}. For example, Baranchuk et al.~\cite{DMSS} and DDAE~\cite{DDAE} leverage the intermediate activations of pre-trained diffusion models as features for segmentation and classification, respectively. HybViT~\cite{HybViT} and JDM~\cite{JDM} jointly learn discriminative and generative tasks with a shared encoder to enhance feature representation. 
SODA~\cite{SODA} turns diffusion models into strong self-supervised representation learners by imposing a bottleneck between an encoder and a denoising decoder.
DIVA~\cite{DIVA} employs the feedback of a frozen pre-trained diffusion model to boost the fine-grained perception capability of CLIP~\cite{CLIP} via a post-training approach.
Additionally, diffusion models are exploited as zero-shot classifiers~\cite{li2023your,clark2024text} by estimating noise given the class names, such as conditions, exhibiting great generalization robustness in out-of-distribution scenarios~\cite{jaini2023intriguing}. Inspired by these studies, we explore the utilization of a pre-trained diffusion model to enhance representation learning for the generalizable \mbox{Re-ID tasks.}

\subsection{Diffusion Models for Person Re-ID}
Diffusion models have also been applied to various person Re-ID tasks. For instance, VI-Diff~\cite{VI-Diff} employs a diffusion model to enhance visible-infrared Re-ID by generating new samples across modalities, thereby reducing the annotation cost of paired images. Diverse person~\cite{DP} proposes a diffusion-based framework to edit original dataset images with attribute texts, efficiently generating high-quality text-based person search datasets. PIDM~\cite{PIDM} also focuses on new data generation, using body pose and image style as guidance. Asperti~{et al.}~\cite{asperti2024generative} decouple the person ID from other factors like poses and backgrounds to control new image sample generation. These works share a common characteristic of modifying existing data or generating new data for Re-ID related tasks. Additionally, DenoiseReID~\cite{DenoiseReID} unifies feature extraction and feature denoising to improve feature discriminative capabilities for Re-ID. {PISL~\cite{PISL} proposes a spatial diffusion model to refine patch sampling to enhance unsupervised Re-ID. PSDiff~\cite{PSDiff} formulates the person search as a dual denoising process from noisy boxes and Re-ID embeddings to ground truths.} In contrast to these, we focus on generalizable representation learning assisted by the feedback back-propagated from a pre-trained diffusion model.

\subsection{Generalizable Person Re-ID}
Generalizable person Re-ID has been extensively studied over the past years. Existing methods can be roughly categorized into the following groups: domain-invariant and specific feature disentanglement~\cite{SNR,DIR-ReID,ADIN}, normalization and domain alignment~\cite{CBN,MetaBIN,DTIN-Net,GDNorm,META,GN,LDU}, learning domain-adaptive mixture-of-experts~\cite{META,RaMoE,SALDG}, meta-learning~\cite{MetaBIN,M3L,MDA,SuA-SpML}, semantic expansion~\cite{DEX,UDSX}, large-scale pre-training~\cite{ISR,MMET,DMF}, and so on. While various mechanisms have been designed, most of these methods learn feature representations within discriminative~\cite{CBN,SNR} or contrastive learning~\cite{M3L,ISR} frameworks. In contrast, we aim to leverage a pre-trained generative diffusion model to enhance the domain-invariant feature learning for more robust generalizable Re-ID.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{pipeline.pdf}
	\caption{An overview of the proposed framework. It consists of a baseline Re-ID model, a pre-trained diffusion model, and a correlation-aware conditioning scheme based on learnable ID-wise prompts. The Re-ID model is built upon the pre-trained CLIP image encoder~\cite{CLIP} and a BN Neck~\cite{BoT}, optimized by an ID loss and a prototypical contrastive loss. The diffusion model is constructed on via pre-trained stable diffusion~\cite{StableDiffusion}, with LoRA~\cite{LoRA} for efficient adaptation. The informative classification probabilities predicted by the Re-ID model is employed to produce a correlation-aware condition to guide the diffusion model for unleashing specific knowledge of generalization, with gradients back-propagated to the Re-ID model for enhanced generalizable feature learning.}
	\label{fig:pipeline}
\end{figure*}

\section{{Diffusion Preliminaries}}

In this section, we briefly recap the preliminaries of classical diffusion models~\cite{DDPM,DDIM}. The diffusion models are generative models defined on a Markov chain, where the forward and reversed processes are modeled using a forward diffusion kernel (FDK) $q(\mathbf{x}_t | \mathbf{x}_{t-1})$ and a learnable reverse diffusion kernel (RDK) $p_\theta (\mathbf{x}_{t-1} | \mathbf{x}_t)$. In the forward process, a real sample $\mathbf{x}_0$ is gradually disturbed towards a final state $\mathbf{x}_T$ that is quite close to a pure Gaussian noise via a FDK. In the reverse process the RDK is trained to denoise from $\mathbf{x}_T$ to $\mathbf{x}_0$. The real distribution of $\mathbf{x}_0$ can be constructed using the integral over each possible path $d\mathbf{x}_{1:T}$ with an optional condition $\mathbf{c}$ as guidance:
\begin{equation}
	p_\theta(\mathbf{x}_0 | \mathbf{c}) = \int_{\mathbf{x}_{1:T}} p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{c}) d\mathbf{x}_{1:T}.
\end{equation}

To estimate the denoising model parameter $\theta$, the negative log-likelihood loss $-\log p_\theta(\mathbf{x}_0 | \mathbf{c})$ should be minimized, but the integral is infeasible. Thus, the variational lower bound $\mathcal{L}_{ELBO}$ is optimized as an alternative:
\begin{equation}
	\mathcal{L}_{ELBO} = \mathbb{E}_q \left[ \frac{q(\mathbf{x}_{1:T} | \mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T}, \mathbf{c})} \right] \geq -\log p_\theta(\mathbf{x}_0 | \mathbf{c}),
\end{equation}
where $\mathcal{L}_{ELBO}$ can be further expanded and simplified to show its essence~\cite{DDPM,DDIM}, which actually learns to predict the added noise at each timestep $t$ by the mean square error:
\begin{equation}
	\label{eqn:noise_mse}
	\mathcal{L}_{mse} = \mathbb{E}_t \left[ || \boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, \mathbf{c}) ||^2 \right],
\end{equation}
where $\mathbf{x}_t$ is the noisy input, which follows the following equation:
\begin{equation}
	\label{eqn:add_noise}
	\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_t.
\end{equation}
Noise $\boldsymbol{\epsilon}_t$ is sampled from the isotropic Gaussian distribution $\mathcal{N}(0, \mathbf{I})$. $t$ is sampled from a series of timesteps $\{ 1,2,\cdots,T \}$, which controls the strength of noise by selecting scheduled diffusion rate $\bar{\alpha}_t$.

\section{The Proposed Method}
As illustrated in Figure~\ref{fig:pipeline}, the overall framework comprises a baseline Re-ID model, a pre-trained diffusion model, and a correlation-aware conditioning scheme that bridges the two models. The Re-ID model learns feature representations by optimizing a discriminative ID loss and a prototypical contrastive loss. The ID classification probabilities generated from the Re-ID model are used to inject the dark knowledge~\cite{hinton2015distilling} of different IDs into a correlation-aware condition that guides the diffusion process. Simultaneously, the gradients of the diffusion model are back-propagated through the condition to the Re-ID model, transferring generalization knowledge to enhance Re-ID feature learning. During test time, only the image encoder of the Re-ID model is used for feature extraction.

\subsection{The Baseline Re-ID Model}
\label{sec:baseline_reid_model}
The baseline Re-ID model comprises an image encoder $\mathcal{E}_{\psi}$, together with a classifier supervised by discriminative ID loss $\mathcal{L}_{id}$ and additional prototypical contrastive loss (PCL) $\mathcal{L}_{pcl}$. Our image encoder is constructed based on the pre-trained CLIP~\cite{CLIP} image encoder, appended with a batch normalization neck (BNNeck)~\cite{BoT}. Leveraging the CLIP encoder enables our model to acquire a certain level of generalization abilities, attributed to its extensive language-image pre-training.

Formally, given an input image $\mathbf{x}$ and its feature $\mathbf{z} = \mathcal{E}_{\psi}(\mathbf{x})\in \mathbb{R}^{1\times d}$, we define $\mathcal{L}_{id}$ and $\mathcal{L}_{pcl}$ as follows:
\begin{equation}
	\label{eqn:L_id}
	\mathcal{L}_{id} = -\sum_{j=1}^{N} q_j \log \frac{\exp(\mathbf{z} \mathbf{w}_j^\top)}{\sum_{k=1}^{N} \exp(\mathbf{z} \mathbf{w}_k^\top)},
\end{equation}
\begin{equation}
	\label{eqn:L_pcl}
	\mathcal{L}_{pcl} = - \log \frac{\exp (\mathbf{z} \mathcal{M}[y]^\top / \tau)}{\sum_{k=1}^N \exp (\mathbf{z} \mathcal{M}[k]^\top / \tau)},
\end{equation}
where $N$ is the number of IDs, $\mathbf{w}_j \in \mathbb{R}^{1\times d}$ denotes the weights of the $j$-th ID in the classifier, $q_j$ denotes the smoothed ground-truth label, and $\tau$ is a temperature factor. Moreover, $\mathcal{M}\in \mathbb{R}^{N\times d}$ represents the prototypical memory bank. Each entry in $\mathcal{M}$ is initialized with the feature centroid of images belonging to the corresponding ID at the beginning of every epoch. Subsequently, it is updated in a moving average manner with momentum $\gamma$: 
\begin{equation}
	\mathcal{M}[y] \leftarrow \gamma\mathcal{M}[y] + (1 - \gamma) \mathbf{z}_{hard},
\end{equation}
in which $y$ denotes the ID label of the image $\mathbf{x}$, and $\mathbf{z}_{hard}$ is the hardest sample~\cite{ClusterContrast} of the corresponding ID within a batch.

Then, the baseline Re-ID model learns feature representations by optimizing the following loss:
\begin{equation}
	\mathcal{L}_{ReID} = \mathcal{L}_{id} + \mathcal{L}_{pcl}.
\end{equation}


\subsection{The Generative Diffusion Model}
\label{sec:generative_diffusion_model}
Our work aims to leverage both the semantic knowledge acquired from a pre-trained diffusion model and the assistance provided by the denoising process to enhance the feature learning capabilities of the baseline Re-ID model's encoder. To this end, rather than directly utilizing intermediate activations of the diffusion model as features~\cite{DMSS,DDAE} or training a denoising decoder alongside the Re-ID model's classifier using a shared encoder like~\cite{HybViT,JDM}, we opt to employ a complete pre-trained diffusion model and adapt it to Re-ID data using LoRA~\cite{LoRA}.

More specifically, we adopt the pre-trained stable diffusion model~\cite{StableDiffusion} in our work. This diffusion model employs a variational autoencoder (VAE) composed of an encoder $\mathcal{E}_{vae}$ and a decoder $\mathcal{D}_{vae}$ to map an input image into a latent space, facilitating a more efficient diffusion process. Moreover, it integrates cross-attention layers into a U-Net~\cite{U-Net} architecture $\mathcal{E}_\theta$ to denoise latent features, enabling the incorporation of various types of conditions. To effectively adapt the diffusion model to Re-ID data while preserving the generalization capabilities acquired during pre-training, we employ LoRA~\cite{LoRA} adapters for fine-tuning. 

LoRA~\cite{LoRA} adapters are only applied to the transformation matrices in the attention layers, including the query, key, value and output transformation matrices in attention computation, and the linear transformation matrices in feed-forward networks, as shown in Figure~\ref{fig:pipeline}. Formally, the LoRA~\cite{LoRA} adapters introduce low-rank projection matrices $\mathbf{A} \in \mathbb{R}^{d_{in} \times r}$ and $\mathbf{B} \in \mathbb{R}^{r \times d_{out}}$ to create modifications on original output features as follows:
\begin{equation}
	\mathbf{h}' = \mathbf{h}\mathbf{W} + \frac{1}{r} \mathbf{h} \mathbf{A} \mathbf{B},
\end{equation}
where $\mathbf{h} \in \mathbb{R}^{1 \times d_{in}}$ and $\mathbf{h}' \in \mathbb{R}^{1 \times d_{out}}$ denote the input and output features, respectively. $\mathbf{W} \in \mathbb{R}^{d_{in} \times d_{out}}$ denotes each possible original transformation matrices mentioned before. $r$ is called rank, which controls the size of the low-dimension space. $d_{in}$ and $d_{out}$ are the dimensions of input and output features, respectively, where $r \ll \min (d_{in}, d_{out})$.
Throughout the entire training process, we freeze the diffusion model while keeping the $\mathbf{A}$ and $\mathbf{B}$ matrices of LoRA~\cite{LoRA} adapters trainable. The purposes of utilizing LoRA~\cite{LoRA} adapters in the diffusion model are two-fold: (1) it reduces computational overhead compared with other fine-tuning methods and (2) mitigates the risk of the catastrophic forgetting~\cite{biderman2024lora} of learned knowledge from pre-training. We will further discuss the effectiveness of LoRA~\cite{LoRA} adapters in Section~\ref{sec:ablation}.

When an image $\mathbf{x}$ is input to the image encoder of the Re-ID model, the image is also resized to an image $\mathbf{x}_0$ to match the input size of the diffusion model. Then, $\mathbf{x}_0$ is fed into the VAE encoder $\mathcal{E}_{vae}$ to produce a latent feature $\mathbf{F}_0 = \mathcal{E}_{vae}(\mathbf{x}_0)$. With a random noise $\epsilon_t$ sampled from the isotropic Gaussian distribution $\mathcal{N}(0, \mathbf{I})$, the noisy feature $\mathbf{F}_t$ at the timestep $t$ is obtained, as mentioned in Equation~\eqref{eqn:add_noise}:
\begin{equation}
	\mathbf{F}_t = \sqrt{\bar{\alpha}_t} \mathbf{F}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_t.
\end{equation}

Afterward, $\mathbf{F}_t$ and its corresponding condition $\mathbf{c}$ (to be introduced in the following subsection) are forwarded to U-Net $\mathcal{E}_\theta$ to minimize the noise estimation error expectation, as mentioned in Equation~\eqref{eqn:noise_mse} with new notation $\mathcal{L}_{dif}$:
\begin{equation}
	\label{eqn:L_dif}
	\mathcal{L}_{dif} = \mathbb{E}_t \left[ || \boldsymbol{\epsilon}_t - \mathcal{E}_\theta(\mathbf{F}_t, \mathbf{c}) ||^2 \right].
\end{equation}

\subsection{The Correlation-Aware Conditioning Scheme}
We design a conditioning scheme to bridge the Re-ID model and the diffusion model, enabling mutual interaction. This scheme enables the use of information from the Re-ID model to guide the diffusion process while simultaneously enabling the feedback from the diffusion model to be back-propagated to improve the Re-ID model. A straightforward conditioning scheme is to take the instance feature encoded by the Re-ID model as the condition, similarly to SODA~\cite{SODA} and DIVA~\cite{DIVA}. However, such instance-level features are sensitive to intra-ID variations and background changes, making them less robust to domain shifts and resulting in suboptimal generalization performance.

Therefore, we opt to design the condition in an ID-wise manner. Notably, the ID classification probabilities produced by the baseline Re-ID model not only indicate the ID class to which an image belongs but also encapsulate dark knowledge about the correlations among different IDs, which has been shown to enhance generalization capabilities~\cite{phuong2019towards,muller2019does,wang2021embracing}. Building on this insight, we incorporate the classification probabilities along with a set of learnable ID prompts to define the condition.

Specifically, we create a set of learnable ID prompts $\mathbf{E}_{id} = \left[\mathbf{e}_1, ..., \mathbf{e}_j, ..., \mathbf{e}_N\right] \in \mathbb{R}^{N \times d}$. The prompt $\mathbf{e}_j {\in \mathbb{R}^{1 \times d}}$ corresponds to the $j$-the ID{, where $d$ is the dimension of each prompt}. Then, the condition $\mathbf{c} {\in \mathbb{R}^{1 \times d}} $ for the image instance $\mathbf{x}$ is generated by a linear combination of all ID prompts weighted by the classification probability of the input instance:
\begin{equation}
	\mathbf{c} = \sum_{j=1}^N p_j \mathbf{e}_j
	= \sum_{j=1}^N \frac{\exp (\mathbf{z} \mathbf{w}_j^\top  / \tau_{c})}{\sum_{k=1}^N \exp (\mathbf{z}\mathbf{w}_k^\top  / \tau_{c})} \mathbf{e}_j,
	\label{eq:soft}
\end{equation}
where $p_j$ denotes the probability of the instance, with $\mathbf{x}$ being classified into the $j$-th ID class, as defined on the right side of the equation. $\mathbf{z}$ and $\mathbf{w}_j$ are the image feature and the $j$-th classifier, respectively, as defined in Section~\ref{sec:baseline_reid_model}. $\tau_{c}$ is a temperature factor that regulates the probability distribution. Unlike the linear combination used in MVI$^2$P~\cite{MVI2P}, which aims to integrate multiple discriminative feature maps for Re-ID training, our approach generates conditions that guide the diffusion model.

We refer to this design as the correlation-aware conditioning scheme. The correlation-aware condition $\mathbf{c}$ aims to describe current image instance $\mathbf{x}$ with all possible IDs. It improves the robustness of representation learning via combining the ID-wise prompts with the ID classification probabilities. Despite the simplicity of this design, our experiments demonstrate that it outperforms more intricate alternative designs.

\begin{table}[tb]
	\centering
	\caption{The statistics of four public Re-ID datasets and their composition on different splits, including the training set (denoted by ``Train'') and testing set (denoted by ``Query'' and ``Gallery'').}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lccccc}
			\toprule
			Dataset       & Cameras & IDs  & Train & Query & Gallery \\
			\midrule
			Market1501~\cite{Market1501}    & 6        & 1,501 & 12,936 & 3,368 & 15,913  \\
			DukeMTMC-reID~\cite{DukeMTMC-reID} & 8        & 1,812 & 16,522 & 2,228 & 17,661 \\
			MSMT17~\cite{MSMT17}        & 15       & 4,101 & 32,621 & 11,659 & 82,161 \\
			CUHK03-NP~\cite{re-ranking}     & 2        & 1,467 & 7,365 & 1,400 & 5,332  \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:datasets}
\end{table}

\subsection{The Entire Training Loss}
Standing on a comprehensive view, the total loss $\mathcal{L}_{total}$ in our framework is a composite of the Re-ID loss $\mathcal{L}_{ReID}$ and the diffusion loss $\mathcal{L}_{dif}$, which is formulated as follows:
\begin{equation}
	\mathcal{L}_{total} = \mathcal{L}_{ReID} + \lambda \mathcal{L}_{dif},
\end{equation}
where $\lambda$ is a balancing factor.

To this end, we have introduced all related losses in both discriminative and generative objectives. Re-ID loss $\mathcal{L}_{ReID}$ integrates ID loss $\mathcal{L}_{id}$ in Equation~\eqref{eqn:L_id} and prototypical contrastive loss $\mathcal{L}_{pcl}$ in Equation~\eqref{eqn:L_pcl}. These losses are designed to optimize the discriminative capabilities of the Re-ID model.  Diffusion loss $\mathcal{L}_{dif}$ in Equation~\eqref{eqn:L_dif} focuses on minimizing the noise estimation error expectation in the latent space during the diffusion process. This loss ensures that the diffusion model is able to effectively contribute to the learning of generalizable features guided by our proposed correlation-aware conditioning scheme. By combining $\mathcal{L}_{ReID}$ and $\mathcal{L}_{dif}$, $\mathcal{L}_{total}$ improves discriminative learning with generative capabilities, enhancing the Re-ID model's performances across diverse domains.

\section{Experiments}
\subsection{Datasets and Evaluation Protocols}
We conduct experiments on the following datasets: Market1501~\cite{Market1501}, DukeMTMC-reID~\cite{DukeMTMC-reID}, MSMT17~\cite{MSMT17}, and CUHK03-NP~\cite{re-ranking}, abbreviated as MA, D, MS, and C3, respectively. Table~\ref{tab:datasets} presents detailed information of each dataset.

\begin{table*}[t]
	\centering
	\caption{Comparison with SOTA methods on the single-source DG Re-ID setting with source domains Market1501~\cite{Market1501} and DukeMTMC-reID~\cite{DukeMTMC-reID}. $\dagger$ indicates that the method uses target domain data for test-time updating. $\ddagger$ indicates that the method uses input image sizes larger than 256 $\times$ 128. The best and second-best results are marked with bold and underline, respectively. Data are collected from corresponding works.}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lcccccccccccc}
			\toprule
			\multirow{2}{*}{Model} & \multicolumn{2}{c}{MA$\to$D} & \multicolumn{2}{c}{MA$\to$MS} & \multicolumn{2}{c}{MA$\to$C3} & \multicolumn{2}{c}{D$\to$MA} & \multicolumn{2}{c}{D$\to$MS} & \multicolumn{2}{c}{D$\to$C3} \\ \cmidrule{2-13} 
			& mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 \\ \midrule
			SNR~\cite{SNR} & 33.6 & 55.1 & - & - & - & - & 33.9 & 66.7 & - & - & - & - \\
			CBN~\cite{CBN} & - & - & 9.5 & 25.3 & - & - & - & - & 9.5 & 25.3 & - & - \\
			QAConv~\cite{QAConv} & 28.7 & 48.8 & 7.0 & 22.6 & - & - & 27.2 & 58.6 & 8.9 & 29.0 & - & - \\
			TransMatcher$\ddagger$~\cite{TransMatcher} & - & - & 18.4 & 47.3 & - & - & - & - & - & - & - & - \\
			MetaBIN~\cite{MetaBIN} & 33.1 & 55.2 & - & - & - & - & 35.9 & 69.2 & - & - & - & - \\
			DTIN-Net~\cite{DTIN-Net} & 36.1 & 57.0 & - & - & - & - & 37.4 & 69.8 & - & - & - & - \\
			QAConv-GS~\cite{QAConv-GS} & - & - & 15.0 & 41.2 & - & - & - & - & - & - & - & - \\
			MDA$\dagger$~\cite{MDA} & 34.4 & 56.7 & 11.8 & 33.5 & - & - & 38.0 & 70.3 & - & - & - & - \\
			Li~{et al.}~\cite{li2023style} & - & - & 21.8 & 47.5 & - & - & - & - & - & - & - & - \\
			SuA-SpML~\cite{SuA-SpML} & 34.8 & 55.5 & 11.1 & 30.1 & - & - & 36.3 & 65.8 & 13.6 & 37.8 & - & - \\
			DIR-ReID~\cite{DIR-ReID} & 33.0 & 54.5 & - & - & - & - & 35.2 & 68.2 & - & - & - & - \\
			GN~\cite{GN} & 34.0 & 52.3 & 10.3 & 28.6 & 14.5 & 14.4 & 34.3 & 64.3 & 12.3 & 33.8 & 10.3 & 10.2 \\
			GN+SNR~\cite{GN} & 34.7 & 55.4 & - & - & 15.2 & 15.1 & 36.9 & 68.5 & - & - & 11.5 & 11.0 \\
			PAT~\cite{PAT} & - & - & 18.2 & 42.8 & - & - & - & - & - & - & - & - \\
			LDU~\cite{LDU} & 38.0 & 59.5 & 13.5 & 35.7 & 18.2 & 18.5 & \bf 42.3 & \bf 73.2 & 16.7 & 44.2 & 14.2 & 14.2 \\
			MTI~\cite{MTI} & 36.4 & 57.8 & - & - & 16.2 & 16.3 & 38.2 & 70.5 & - & - & 13.3 & 13.3 \\ \midrule
			Baseline (Ours) & \underline{47.8} & \underline{67.3} & \underline{22.0} & \underline{50.1} & \underline{30.0} & \underline{30.4} & \underline{41.7} & 70.5 & \underline{19.0} & \underline{46.9} & \underline{22.1} & \underline{23.1} \\
			DCAC (Ours) & \bf 49.5 & \bf 69.1 & \bf 23.4 & \bf 52.1 & \bf 32.5 & \bf 33.2 & \bf 42.3 & \underline{71.5} & \bf 19.7 & \bf 47.4 & \bf 23.0 & \bf 23.5 \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:ssdg-sota}
\end{table*}

The performance of generalizable Re-ID is evaluated using both single-source and multi-source generalization protocols. In the single-source protocol, the Re-ID model is trained on one dataset and tested on another target dataset. For example, we denote the experiment as MS$\to$MA when training on MSMT17~\cite{MSMT17} and testing on Market1501~\cite{Market1501}, and likewise for others. In the multi-source protocol, a leave-one-out strategy is employed, where one dataset is used for testing while the training sets of remaining datasets are used for training. In both protocols, we adopt the mean average precision (mAP) and cumulative matching characteristic (CMC) at Rank-1 (R1) as evaluation metrics without applying re-ranking post-processing~\cite{re-ranking}.

\subsection{Implementation Details}
We implement our model in PyTorch 1.13.1~\cite{pytorch} and conduct experiments on an NVIDIA RTX A6000 GPU. We adopt the image encoder of the pre-trained CLIP ViT-B-16~\cite{CLIP} for our baseline Re-ID model, in which the patch projection layer is frozen for stability while the other parameters are trainable. The input image size for the Re-ID model is $256 \times 128$, and the dimension of the encoded feature is 512. The momentum $\gamma$ utilized for updating the prototypical memory bank is set to 0.2, and the temperature factor $\tau$ in the PCL loss is set to 0.01. For the diffusion model, we adopt the pre-trained weight \texttt{stable-diffusion-v1-5}~\cite{StableDiffusion} on Huggingface. The input image size for diffusion is $128 \times 128$. The rank $r$ of the LoRA adapters is set to 8 for all single-source experiments except for those trained on MSMT17~\cite{MSMT17}. For single-source experiments trained on MSMT17~\cite{MSMT17} and all multi-source experiments, $r$ is set to 32. In the correlation-aware conditioning scheme, the prompt dimension is set to 768 to match the diffusion model. The temperature factor $\tau_{c}$ is closely related to the size of the training datasets. Accordingly, it is set to 0.1 for training on CUHK03-NP~\cite{re-ranking}, 0.6 for training on Market1501~\cite{Market1501} and DukeMTMC-reID~\cite{DukeMTMC-reID}, and 1.0 for training on MSMT17~\cite{MSMT17} and multi-source settings. Moreover, balancing factor $\lambda$ in the entire loss is set to 1. 

During training, random horizontal flipping, cropping, and erasing~\cite{random_erasing} augmentations are used for the input of the Re-ID model, while no data augmentation is used for the diffusion model. We train our entire framework for 60 epochs using the Adam optimizer~\cite{Adam} with a base learning rate of $5 \times 10^{-6}$ regulated by a step scheduler, which starts at a learning rate of $5 \times 10^{-7}$ with a linear warmup in the first 10 epochs. The learning rate is multiplied with a factor of 0.1 at the 30th and 50th epoch. We adopt a weight decay of $1 \times 10^{-4}$. The batch size is set to 64, employing a PK-sampling strategy~\cite{PK-sampling} with randomly selected 16 IDs and 4 samples per ID.

\begin{table*}[t]
	\centering
	\caption{Comparison with SOTA methods on the single-source DG Re-ID setting, where the source domains are MSMT17~\cite{MSMT17} and CUHK03-NP~\cite{re-ranking}. $\dagger$ indicates that the method uses target domain data for test-time updating. $\ddagger$ indicates that the method uses input image size larger than 256 $\times$ 128. The best and second-best results are marked with bold and underline, respectively. Data are collected from the corresponding works.}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lcccccccccccc}
			\toprule
			\multirow{2}{*}{Model} & \multicolumn{2}{c}{MS$\to$MA} & \multicolumn{2}{c}{MS$\to$D} & \multicolumn{2}{c}{MS$\to$C3} & \multicolumn{2}{c}{C3$\to$MA} & \multicolumn{2}{c}{C3$\to$D} & \multicolumn{2}{c}{C3$\to$MS} \\ \cmidrule{2-13} 
			& mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 \\ \midrule
			PCB~\cite{PCB} & 26.7 & 52.7 & - & - & - & - & - & - & - & - & - & - \\
			MGN~\cite{MGN} & 25.1 & 48.7 & - & - & - & - & - & - & - & - & - & - \\
			OSNet-IBN~\cite{OSNet} & 37.2 & 66.5 & 45.6 & 67.4 & - & - & - & - & - & - & - & -\\
			SNR~\cite{SNR} & 41.4 & 70.1 & 50.0 & 69.2 & - & - & - & - & - & - & - & - \\
			CBN~\cite{CBN} & 45.0 & 73.7 & - & - & - & - & - & - & - & - & - & - \\
			TransMatcher$\ddagger$~\cite{TransMatcher} & 52.0 & \bf 80.1 & - & - & - & - & - & - & - & - & - & -  \\
			QAConv-GS~\cite{QAConv-GS} & 46.7 & 75.1 & - & - & - & - & - & - & - & - & - & -  \\
			MDA$\dagger$~\cite{MDA} & \bf 53.0 & \underline{79.7} & - & - & - & - & - & - & - & - & - & -  \\
			GN~\cite{GN} & - & - & - & - & - & - & \underline{40.6} & 67.6 & 31.2 & 50.0 & 11.9 & 33.4 \\
			GN+SNR~\cite{GN} & 37.5 & 68.0 & 45.4 & 66.2 & 18.3 & 17.4 & - & - & - & - & - & -  \\
			LDU~\cite{LDU} & 44.8 & 74.6 & 48.9 & 69.2 & 21.3 & 21.3 & 37.5 & \underline{68.1} & 29.5 & 51.8 & 12.6 & 36.9 \\
			MTI~\cite{MTI} & 42.7 & 72.9 & 47.7 & 67.5 & 16.0 & 15.4 & - & - & - & - & - & -  \\ \midrule
			Baseline (Ours) & 51.0 & 76.5 & \underline{57.1} & \underline{73.8} & \underline{32.7} & \underline{32.9} & 39.6 & 66.2 & \underline{41.4} & \underline{62.7} & \underline{16.6} & \underline{45.2} \\
			DCAC (Ours) & \underline{52.1} & 77.9 & \bf 58.4 & \bf 75.0 & \bf 34.1 & \bf 34.4 & \bf 42.0 & \bf 68.6 & \bf 43.2 & \bf 64.8 & \bf 17.8 & \bf 47.3 \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:ssdg-sota-continued}
\end{table*}

\subsection{Comparison with State-of-the-Arts}
\subsubsection{Single-source DG Re-ID} 
We first compare the proposed method, named DCAC, with several representative and state-of-the-art methods using the single-source protocol. We conduct all possible single-source generalization experiments with four datasets for comprehensive evaluations. The results are separately presented in Tables~\ref{tab:ssdg-sota} and~\ref{tab:ssdg-sota-continued}. The source domain is selected from Market1501~\cite{Market1501} and DukeMTMC-reID~\cite{DukeMTMC-reID} in Table~\ref{tab:ssdg-sota} and from MSMT17~\cite{MSMT17} and CUHK03-NP~\cite{re-ranking} in Table~\ref{tab:ssdg-sota-continued}. The results show that some previous methods, such as TransMatcher~\cite{TransMatcher} and MDA~\cite{MDA}, exhibit strong performance in MS$\to$MA generalization. However, their performance significantly deteriorates when applied to the more challenging MA$\to$MS generalization task. In contrast, our approach demonstrates more balanced improvements in all single-source generalization tests without requiring a larger input size as in TransMatcher~\cite{TransMatcher} or test-time updating as in MDA~\cite{MDA}.

\subsubsection{Multi-Source DG Re-ID}
More experiments are carried out using the multi-source protocol to further validate the effectiveness of the proposed approach. The results are presented in Table~\ref{tab:msdg-sota}. Although some latest works like UDSX~\cite{UDSX} and SALDG~\cite{SALDG} demonstrate better performances when MA is selected as the target domain, their improvements are limited when generalizing to other target domains, where the average performances are 43.4\% on mAP and 61.7\% on Rank-1 for UDSX~\cite{UDSX} and 40.0\% on mAP and 58.6\% on Rank-1 for SALDG~\cite{SALDG}, respectively. In contrast, our approach presents more balanced enhancements across all four target domains, with an average mAP of 46.4\% and Rank-1 of 63.9\%, surpassing the performance of UDSX~\cite{UDSX} by a fair margin of 3.0\% on mAP and 2.2\% on Rank-1. In particular, with respect to the most challengeable dataset MS, our approach greatly outperforms the current best performance by 5.9\% on mAP and 9.1\% on Rank-1. These results show the effectiveness of our approach on multi-source DG Re-ID with state-of-the-art performances.

\subsection{Ablation Studies}
\label{sec:ablation}
\subsubsection{Effectiveness of the CLIP-Based Re-ID Model}
Our baseline Re-ID model is built upon the pre-trained CLIP image encoder and fine-tuned on Re-ID datasets using both a discriminative ID loss and a prototypical contrastive loss. Benefiting from pre-training on extensive text-image paired data, the CLIP encoder equips our baseline model with a certain level of generalization capability, as demonstrated in Tables~\ref{tab:ssdg-sota} and~\ref{tab:ssdg-sota-continued}.

\subsubsection{Effectiveness of the Diffusion Model Assistance}
\label{sec:effectiveness_of_the_diffusion_model_assistance}
We conduct a series of experiments to validate the effectiveness of the diffusion model for learning generalizable representations. To investigate whether the knowledge learned from pre-training or the denoising process itself is beneficial, we carry out a comparison among the following model variants: (1) using the baseline Re-ID model without diffusion, (2) using the pre-trained diffusion model while keeping it frozen, (3) using the pre-trained diffusion model with LoRA for adaptation, (4) using the pre-trained diffusion model with only the output blocks trainable, (5) using the pre-trained diffusion model with only the middle and output blocks trainable, (6) using the pre-traind diffusion model with all parameters trainable, and (7) using a randomly initialized diffusion model with all parameters trained from scratch. The results are presented in Table~\ref{tab:collaborative_learning}. 

According to the results, we observe that fully freezing the diffusion model prevents it from effectively enhancing generalization abilities and may even slightly harm it. We attribute this to the domain gap between the diffusion model's pre-training dataset and the Re-ID dataset. Since the diffusion model lacks specific knowledge about Re-ID, it provides invalid feedback.

\begin{table*}[t]
	\centering
	\caption{Comparison with SOTA methods on the multi-source DG Re-ID setting. The best and second-best results are marked with bold and underline, respectively. Data are collected from corresponding works.}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lcccccccccc}
			\toprule
			\multirow{2}{*}{Model} & \multicolumn{2}{c}{Target: MA} & \multicolumn{2}{c}{Target: D} & \multicolumn{2}{c}{Target: MS} & \multicolumn{2}{c}{Target: C3} & \multicolumn{2}{c}{Average} \\
			\cmidrule{2-11} 
			& mAP  & R1 & mAP  & R1 & mAP  & R1 & mAP  & R1 & mAP  & R1 \\
			\midrule
			QAConv$_{50}$~\cite{QAConv} & 39.5 & 68.6   & 43.4 & 64.9   & 10.0 & 29.9   & 19.2 & 22.9 & 28.0 & 46.6    \\
			M$^3$L~\cite{M3L} & 48.1 & 74.5   & 50.5 & 69.4   & 12.9 & 33.0   & 29.9 & 30.7 & 35.4 & 51.9   \\
			M$^3$L$_{IBN}$~\cite{M3L} & 50.2 & 75.9   & 51.1 & 69.2   & 14.7 & 36.9   & 32.1 & 33.1 & 37.0 & 53.8   \\
			RaMoE~\cite{RaMoE}   & 56.5 & 82.0   & \underline{56.9} & 73.6 & 13.5 & 34.1   & 35.5 & 36.6 & 40.6 & 56.6  \\
			PAT~\cite{PAT}          & 51.7 & 75.2   & 56.5 & 71.8   & \underline{21.6} & 45.6   & 31.5 & 31.1 & 40.3 & 55.9  \\
			DEX~\cite{DEX} & 55.2 & 81.5 & 55.0 & 73.7 & 18.7 & 43.5 & 33.8 & 36.7 & 40.7 & 58.9 \\
			UDSX~\cite{UDSX} & \bf 60.4 & \bf 85.7 & 55.8 & \underline{74.7} & 20.2 & \underline{47.6} & \underline{37.2} & \underline{38.9} & \underline{43.4} & \underline{61.7} \\
			SALDG~\cite{SALDG} & \underline{57.6} & \underline{82.3} & 52.0 & 71.2 & 18.1 & 46.5 & 32.4 & 34.5 & 40.0 & 58.6 \\ 
			DCAC (Ours) & 56.7 & 80.0 & \bf 58.9 & \bf 75.4 & \bf 27.5 & \bf 56.7 & \bf 42.5 & \bf 43.6 & \bf 46.4 & \bf 63.9 \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:msdg-sota}
\end{table*}

A classical solution for adapting downstream knowledge is to freeze shallow blocks but train the deep blocks of the model, which is denoted as partial fine-tuning. The diffusion U-Net contains input, middle, and output blocks. We gradually unfreeze each block of the U-Net, denoted as partial fine-tuning 1 and 2, where in 1 the output blocks are trainable, and in 2, both the middle and output blocks are trainable. According to the results, partial fine-tuning presents a certain level of improvement on generalization and achieves the best on the MS$\to$MA Rank-1 metric, with the middle and output blocks trainable. But it fails to maintain its advantage on more challengeable MA$\to$MS generalization, in which the source domain is limited with fewer IDs and samples, and it is more likely to be overfitted. This reveals that partial fine-tuning is unable to effectively preserve pre-trained generalized knowledge during downstream adaptation.

\begin{table}[h]
	\centering
	\caption{Ablations on various diffusion model fine-tuning methods. $\theta_a$ and $\theta_{na}$ denote trainable parameters in the attention and non-attention layers of the denoising U-Net. ``PT'' denotes whether pre-trained diffusion model weights were used. Partial fine-tuning 1 and 2 are the variants of full fine-tuning, where only the output blocks and both the middle and output blocks of the U-Net are trainable, respectively. $\checkmark$ and $\times$ denote adopting corresponding option or not, respectively. The best results are marked with bold.}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lccccccc}
			\toprule
			\multirow{2}{*}{Model} & \multirow{2}{*}{PT} & \multicolumn{2}{c}{Trainable} & \multicolumn{2}{c}{MA$\to$MS} & \multicolumn{2}{c}{MS$\to$MA} \\ \cmidrule{3-8} 
			& & $\theta_a$                 & $\theta_{na}$           & mAP          & R1         & mAP          & R1         \\
			\midrule
			Baseline & - & - & - & 22.0 & 50.1 & 51.0 & 76.5 \\
			DCAC (Frozen)  & \checkmark &  $\times$     &     $\times$     & 21.3         & 49.7           & 50.8         & 76.5           \\
			DCAC (LoRA adaptation) & \checkmark & \checkmark          &         $\times$   & \bf 23.4         & \bf 52.1           & \bf 52.1         &  77.9           \\
			DCAC (Partial fine-tuning 1) & \checkmark & \checkmark & \checkmark & 22.1 & 50.3 & 47.1 & 74.6 \\
			DCAC (Partial fine-tuning 2) & \checkmark & \checkmark & \checkmark & 22.1 & 50.4 & 51.9 & \bf 78.5 \\
			DCAC (Full fine-tuning) & \checkmark & \checkmark           & \checkmark          & 21.8         & 50.1           & 51.5         & 76.6           \\
			DCAC (Train from scratch) & $\times$ & \checkmark & \checkmark & 21.9 & 50.3 & 51.7 & 77.6 \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:collaborative_learning}
\end{table}

\begin{table*}[t]
	\centering
	\caption{Ablations on the computational overhead of various diffusion model fine-tuning methods. For generalization performance, we use MA$\to$MS results. For computational overhead, we report the time latency and the count of tera floating point operations (TFLOPs) in forward propagation to measure the time efficiency. Additionaly, GPU memory consumption and the number of the model's trainable parameters are reported to measure space efficiency. The best results are marked with bold.}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{llcccccc}
			\toprule
			\multirow{2}{*}{Mode} & \multirow{2}{*}{Model} & \multicolumn{2}{c}{MA$\to$MS} & \multirow{2}{*}{Time (ms)} & \multirow{2}{*}{TFLOPs} & \multirow{2}{*}{Memory (GB)} & \multirow{2}{*}{Parameters (M)} \\
			\cmidrule{3-4}
			& & mAP & R1 &  & & & \\
			\midrule
			\multirow{5}{*}{Training} & Baseline & 22.0 & 50.1 & 140.3 & 1.46 & 7.58 & 85.94 \\
			& DCAC (Frozen) & 21.3 & 49.7 & 464.7 & 8.50 & 17.22 & 86.51 \\
			& DCAC (LoRA adaptation) & \bf 23.4 & \bf 52.1 & 578.1 & 8.52 & 18.15 & 89.01 \\
			& DCAC (Partial fine-tuning 1) & 22.1 & 50.3 & 647.1 & 8.50 & 23.92 & 599.37 \\
			& DCAC (Partial fine-tuning 2) & 22.1 & 50.4 & 662.1 & 8.50 & 25.01 & 696.41 \\
			& DCAC (Full fine-tuning) & 21.8 & 50.1 & 735.9 & 8.50 & 28.24 & 946.03 \\
			\midrule
			Inference & - & - & - & \bf 40.3 & \bf 1.46 & \bf 2.27 & \bf 85.55 \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:computational_overhead}
\end{table*}

When the pre-trained diffusion model is fine-tuned with all trainable parameters (from all input, middle, and output blocks) or when a randomly initialized diffusion model is trained without pre-training, both variant models adapt sufficiently to the Re-ID data, resulting in better performance compared to a fully frozen model. However, these models either suffer from the significant forgetting of pre-trained knowledge~\cite{biderman2024lora} or lack any pre-training knowledge, leading to only limited improvement.

In contrast, the approach utilizing LoRA, which only fine-tunes the low-rank adapters of the attention layers of the pre-trained diffusion model on Re-ID data, achieves balanced and significant enhancement in generalization ability across different target domains. This result highlights that the synergy between pre-trained knowledge and the diffusion process contributes most effectively to improving generalization, and illustrates that the LoRA-based fine-tuning best fits our framework.

\subsubsection{{Ablations on Computational Overhead}}
\label{sec:computational_overhead}
Table~\ref{tab:computational_overhead} studies the computational overhead of different variants of the diffusion model's fine-tuning, including the major approaches mentioned in Section~\ref{sec:effectiveness_of_the_diffusion_model_assistance} with a batch size of 64. In the training stage, the baseline model presents the optimal efficiency with 140.3 ms latency and 1.46 TFLOPs in forward propagation and 7.58 GB memory consumption with 85.94 M trainable parameters, but it suffers from limited generalization performance. Frozen fine-tuning even fails to surpass the baseline on either generalization performance or computational efficiency, with a slight increase in trainable parameters due to the incorperation of learnable ID prompts. In addition, other fine-tuning methods like partial and full fine-tuning do not demonstrate effective enhancements on generalization, although more parameters are allowed to be optimized. Note that the TFLOPs remain unchanged as 8.50 for frozen, full, and partial fine-tuning, since the count of floating point operations in forward propagation is not interfered by parameter freezing or not.

Differently, our LoRA-based strategy achieves a great tradeoff between the computational overhead and the generalization performance, enabling it to be be trained with acceptable cost growth due to newly introduced adapters for the best performance. In the inference stage, only the Re-ID image encoder $\mathcal{E}_{\psi}$ with the updated parameters is required to extract person features; thus, the overhead of the diffusion model is dropped. Moreover, the cost can be further reduced without gradient computation and the classifiers in training.

\subsubsection{Effectiveness of the Conditioning Scheme}
In our design, we claim that the proposed correlation-aware conditioning scheme is the most appropriate mechanism to guide  generalization feedback from the pre-trained diffusion model, where each condition is generated by linear combination of multiple learnable ID-wise prompts weighted by the classification probabilities. 

In Table~\ref{tab:ablation-condition}, we compare different conditioning schemes. It is obvious that the instance-wise condition only provides a tiny contribution to generalization improvements. To further validate that the correlation among IDs, i.e., dark knowledge, is the key for transfering generalization knowledge from the diffusion model, we conduct an experiment on a simplified class-wise condition, where softmax weighting in Equation~\eqref{eq:soft} is replaced by one-hot selection, which only keeps the probability score of the corresponding class and resets others to zero. The dark knowledge that exists in probability distributions is therefore erased. From the results, we find that the class-wise condition that only considers a single ID cannot effectively enhance the generalization capability, which even deteriorates the baseline on MS$\to$MA generalization, whereas our correlation-aware condition brings the most salient enhancement, validating the importance of dark knowledge in condition generation.

\begin{table}[t]
	\centering
	\caption{Ablations on the conditioning scheme. ``Instance-wise'' denotes that the instance feature is directly adopted as the diffusion condition. ``Class-wise'' denotes that the classification probability belonging to the real ID class of the image is adopted to generate the diffusion condition. ``Correlation-aware'' denotes the proposed method, which adopts all ID classification probabilities to generate the diffusion condition in a linear combination manner. The best results are marked with bold.}
	\resizebox{\linewidth}{!}{
	\begin{tabular}{lcccc}
		\toprule
		\multirow{2}{*}{Conditioning Scheme} & \multicolumn{2}{c}{MA$\to$MS} & \multicolumn{2}{c}{MS$\to$MA} \\ \cmidrule{2-5} 
		& mAP          & R1         & mAP          & R1         \\ \midrule
		Baseline & 22.0 & 50.1 & 51.0 & 76.5 \\
		DCAC (Instance-wise)                   & 22.4         & 50.8           & 51.6         & 77.7           \\
		DCAC (Class-wise) & 22.6 & 51.2 & 50.5 & 76.5 \\
		DCAC (Correlation-aware)                         & \bf 23.4         & \bf 52.1           & \bf 52.1         & \bf 77.9           \\
		\bottomrule
	\end{tabular}}
	\label{tab:ablation-condition}
\end{table}

Moreover, we conduct more experiments on the test sets of source domains, aiming to further investigate the performance on source domain data distribution. As demonstrated in Table~\ref{tab:ablation-source-domain}, our correlation-aware conditioning scheme does not present obvious performance degradation on source domains, meaning that our approach indeed refines the capability of generalization and also preserves the performance on source domains.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{gradcam_compare.pdf}
	\caption{GradCAM~\cite{GradCAM} visualization of several visually similar IDs selected from the Market1501~\cite{Market1501} dataset. In groups (a) to (c), the activation maps are computed with the images of ID \#27, \#76, and \#649 under ID \#28, respectively, which reflects the Re-ID model's capability of capturing correlations among IDs. From left to right, each group contains the original image and the activation maps of the baseline model, the instance-wise condition-guided model, and our correlation-aware-condition-guided model, respectively.}
	\label{fig:gradcam}
\end{figure*}

\begin{table}[t]
	\centering
	\caption{Ablations on source domain Re-ID performance. The best results are marked with bold.}
	\resizebox{\linewidth}{!}{
	\begin{tabular}{lcccc}
		\toprule
		\multirow{2}{*}{Model} & \multicolumn{2}{c}{Market1501} & \multicolumn{2}{c}{MSMT17} \\ \cmidrule{2-5} 
		& mAP          & R1         & mAP          & R1         \\ \midrule
		Baseline & 86.4 & 94.4 & 70.4 & 88.1 \\
		DCAC (Instance-wise) & 86.4 & 94.7 & 70.4 & \bf 88.5 \\
		DCAC (Class-wise) & 86.6 & 94.5 & \bf 70.6 & 88.2 \\
		DCAC (Correlation-aware) & \bf 86.8 & \bf 94.9 & 70.1 & 88.3 \\
		\bottomrule
	\end{tabular}}
	\label{tab:ablation-source-domain}
\end{table}

\subsubsection{Impact of the Hyper-Parameters}
Table~\ref{tab:ablation-rank} investigates the impact of rank $r$ in the LoRA adapters. For simplicity, we choose MA as the representative of small-scale source domains, i.e., MA, D, and C3, to analyze the rank value on MA$\to$MS generalization. We observe that a lower rank $r=8$ is optimal for training on small datasets. For the dataset at larger scales, i.e., MS, we tested it with respect to MS$\to$MA generalization and found that the higher rank $r=32$ was optimal.

\begin{table}[t]
	\centering
	\caption{Parameter analysis on the rank $r$ of LoRA adapters. The best results are marked with bold.}
	\resizebox{\linewidth}{!}{
	\begin{tabular}{ccccc}
		\toprule
		\multirow{2}{*}{Rank $r$} & \multicolumn{2}{c}{MA$\to$MS} & \multicolumn{2}{c}{MS$\to$MA} \\ \cmidrule{2-5} 
		& mAP          & Rank-1         & mAP          & Rank-1         \\
		\midrule
		8                     & \bf 23.4         & \bf 52.1           & 51.4         & 77.3           \\
		16                    & 22.1         & 50.7           & 51.7         & \bf 77.9           \\
		32                    & 22.4         & 51.4           & \bf 52.1         & \bf 77.9           \\
		64                    & 22.9         & 51.6           & 51.1         & 77.5           \\
		\bottomrule
	\end{tabular}}
	\label{tab:ablation-rank}
\end{table}

\subsubsection{Impact of More Intricate Conditioning Schemes}
We investigate more intricate alternative designs of the conditioning scheme by employing further transformations on the basis of the original correlation-aware conditioning through linear combination. These new methods focus on the influences of the other two operations that frequently appear in neural networks, that is, non-linear mapping and normalization, instead of the linear operation only.

In Table~\ref{tab:post-process}, we compare these alternatives with the baseline and standard DCAC. ``Non-linearity'' denotes that we apply the SiLU~\cite{SiLU} activation function, which is the same as the one activating latent features in the diffusion model, on the correlation-aware conditions to introduce non-linearity. ``BatchNorm'' denotes that a batch normalization layer~\cite{BatchNorm} is appended after the linear combination of the ID-wise prompts to eliminate the internal covariate shift. ``ConditionNet'' denotes that a multi-layer perceptron network, including the mixture of linear, non-linear, and normalization layers, is employed on generated conditions. Surprisingly, we find that the more complicated variants do not seem to provide further improvements in generalization capability compared with the simple but effective correlation-aware conditioning.

\begin{table}[t]
	\centering
	\caption{Further studies on more sophisticated conditioning schemes beyond the correlation-aware conditioning through linear combination. Non-linear activations like SiLU~\cite{SiLU} and normalization layers like batch normalization~\cite{BatchNorm} are adopted on the generated conditions. $\checkmark$ and $\times$ denote adopting corresponding option or not, respectively. The best results are marked with bold.}
	\resizebox{\linewidth}{!}{
	\begin{tabular}{lcccccc}
		\toprule
		\multicolumn{1}{l}{\multirow{2}{*}{Model}} & \multicolumn{2}{c}{$h$} & \multicolumn{2}{c}{MA$\to$MS} & \multicolumn{2}{c}{MS$\to$MA} \\ \cmidrule{2-7} 
		\multicolumn{1}{c}{}                       & SiLU           & BN            & mAP           & R1            & mAP           & R1            \\
		\midrule
		Baseline & - & - & 22.0 & 50.1 & 51.0 & 76.5 \\
		DCAC                                 & $\times$       & $\times$      & \bf 23.4      & \bf 52.1      & \bf 52.1      & \bf 77.9      \\
		+ Non-linearity                            & \checkmark     & $\times$      & 22.4          & 50.8          & 50.4          & 76.9          \\
		+ BatchNorm                                & $\times$       & \checkmark    & 22.5          & 50.9          & 51.0          & 76.8          \\
		+ ConditionNet                             & \checkmark     & \checkmark    & 21.9          & 50.1          & 50.5          & 76.2          \\
		\bottomrule
	\end{tabular}}
	\label{tab:post-process}
\end{table}

\subsubsection{Visualization Results}
In Figure~\ref{fig:gradcam}, we use GradCAM~\cite{GradCAM} visualization to investigate the Re-ID model's capability of capturing correlations across different IDs, which reflects the generalization capability of the Re-ID model. Specifically, we select several visually similar IDs \#27, \#76, and \# 649 and compute the activation maps under another similar ID \#28. A well-generalized Re-ID model is expected to focus on similar ID-relevant areas even if the ID class and the image is not consistent.

From the results, we observe that the baseline model mainly focuses on background areas but the person bodies are almost ignored, indicating its poor ability to learn ID correlations. When adopting the diffusion knowledge feedback, the attentive areas are rectified to shared ID-relevant attributes such as the purple T-shirt and black trousers. Furthermore, using our correlation-aware conditioning scheme helps covering more body parts and reducing perception on background areas, which shows the effectiveness of our approach.


\section{Conclusions}
In this work, we explore the feasibility of leveraging a pre-trained
diffusion model to enhance generalizable feature learning for DG Re-ID. By adopting a simple yet effective correlation-aware conditioning scheme, we utilize the ID classification probabilities to guide the diffusion model for generalization knowledge unleashing and transferring towards the Re-ID model via gradient feedback. Through extensive experimentation on both single- and multi-source DG Re-ID settings, our approach demonstrates its effectiveness by achieving state-of-the-art performance levels.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
