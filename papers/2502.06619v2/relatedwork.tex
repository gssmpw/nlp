\section{Related Work}
\subsection{Generative Diffusion Models}
Diffusion models~\cite{DDPM,DDIM}, which simulate a Markov chain to learn the transition from noise to a real data distribution, have shown remarkable performance in generation tasks. Representative diffusion models include Imagen~\cite{Imagen}, stable diffusion~\cite{StableDiffusion}, and DiT~\cite{DiT}. Imagen predicts noise in a pixel space and generates high-resolution outputs using super-resolution modules. In contrast, stable diffusion and DiT denoise images in latent spaces, significantly reducing computation costs. Specifically, stable diffusion maps an image into the latent space via a pre-trained variational autoencoder (VAE) and predicts noise with a U-Net structure~\cite{U-Net} containing cross-attention modules to fuse conditions. DiT further replaces the U-Net with visual transformers and improves the condition injection with the adaLN-zero strategy for scalable high-quality image generation. Considering computational efficiency, we choose to adopt stable diffusion in this work.

\subsection{Diffusion Models for Representation Learning}
Although diffusion models are primarily designed for generation tasks, their ability to learn semantic representations has also been recognized in recent years~\cite{DMRL}. For example, Baranchuk et al.~\cite{DMSS} and DDAE~\cite{DDAE} leverage the intermediate activations of pre-trained diffusion models as features for segmentation and classification, respectively. HybViT~\cite{HybViT} and JDM~\cite{JDM} jointly learn discriminative and generative tasks with a shared encoder to enhance feature representation. 
SODA~\cite{SODA} turns diffusion models into strong self-supervised representation learners by imposing a bottleneck between an encoder and a denoising decoder.
DIVA~\cite{DIVA} employs the feedback of a frozen pre-trained diffusion model to boost the fine-grained perception capability of CLIP~\cite{CLIP} via a post-training approach.
Additionally, diffusion models are exploited as zero-shot classifiers~\cite{li2023your,clark2024text} by estimating noise given the class names, such as conditions, exhibiting great generalization robustness in out-of-distribution scenarios~\cite{jaini2023intriguing}. Inspired by these studies, we explore the utilization of a pre-trained diffusion model to enhance representation learning for the generalizable \mbox{Re-ID tasks.}

\subsection{Diffusion Models for Person Re-ID}
Diffusion models have also been applied to various person Re-ID tasks. For instance, VI-Diff~\cite{VI-Diff} employs a diffusion model to enhance visible-infrared Re-ID by generating new samples across modalities, thereby reducing the annotation cost of paired images. Diverse person~\cite{DP} proposes a diffusion-based framework to edit original dataset images with attribute texts, efficiently generating high-quality text-based person search datasets. PIDM~\cite{PIDM} also focuses on new data generation, using body pose and image style as guidance. Asperti~{et al.}~\cite{asperti2024generative} decouple the person ID from other factors like poses and backgrounds to control new image sample generation. These works share a common characteristic of modifying existing data or generating new data for Re-ID related tasks. Additionally, DenoiseReID~\cite{DenoiseReID} unifies feature extraction and feature denoising to improve feature discriminative capabilities for Re-ID. {PISL~\cite{PISL} proposes a spatial diffusion model to refine patch sampling to enhance unsupervised Re-ID. PSDiff~\cite{PSDiff} formulates the person search as a dual denoising process from noisy boxes and Re-ID embeddings to ground truths.} In contrast to these, we focus on generalizable representation learning assisted by the feedback back-propagated from a pre-trained diffusion model.

\subsection{Generalizable Person Re-ID}
Generalizable person Re-ID has been extensively studied over the past years. Existing methods can be roughly categorized into the following groups: domain-invariant and specific feature disentanglement~\cite{SNR,DIR-ReID,ADIN}, normalization and domain alignment~\cite{CBN,MetaBIN,DTIN-Net,GDNorm,META,GN,LDU}, learning domain-adaptive mixture-of-experts~\cite{META,RaMoE,SALDG}, meta-learning~\cite{MetaBIN,M3L,MDA,SuA-SpML}, semantic expansion~\cite{DEX,UDSX}, large-scale pre-training~\cite{ISR,MMET,DMF}, and so on. While various mechanisms have been designed, most of these methods learn feature representations within discriminative~\cite{CBN,SNR} or contrastive learning~\cite{M3L,ISR} frameworks. In contrast, we aim to leverage a pre-trained generative diffusion model to enhance the domain-invariant feature learning for more robust generalizable Re-ID.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{pipeline.pdf}
	\caption{An overview of the proposed framework. It consists of a baseline Re-ID model, a pre-trained diffusion model, and a correlation-aware conditioning scheme based on learnable ID-wise prompts. The Re-ID model is built upon the pre-trained CLIP image encoder~\cite{CLIP} and a BN Neck~\cite{BoT}, optimized by an ID loss and a prototypical contrastive loss. The diffusion model is constructed on via pre-trained stable diffusion~\cite{StableDiffusion}, with LoRA~\cite{LoRA} for efficient adaptation. The informative classification probabilities predicted by the Re-ID model is employed to produce a correlation-aware condition to guide the diffusion model for unleashing specific knowledge of generalization, with gradients back-propagated to the Re-ID model for enhanced generalizable feature learning.}
	\label{fig:pipeline}
\end{figure*}