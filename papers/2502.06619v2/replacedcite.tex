\section{Related Work}
\subsection{Generative Diffusion Models}
Diffusion models____, which simulate a Markov chain to learn the transition from noise to a real data distribution, have shown remarkable performance in generation tasks. Representative diffusion models include Imagen____, stable diffusion____, and DiT____. Imagen predicts noise in a pixel space and generates high-resolution outputs using super-resolution modules. In contrast, stable diffusion and DiT denoise images in latent spaces, significantly reducing computation costs. Specifically, stable diffusion maps an image into the latent space via a pre-trained variational autoencoder (VAE) and predicts noise with a U-Net structure____ containing cross-attention modules to fuse conditions. DiT further replaces the U-Net with visual transformers and improves the condition injection with the adaLN-zero strategy for scalable high-quality image generation. Considering computational efficiency, we choose to adopt stable diffusion in this work.

\subsection{Diffusion Models for Representation Learning}
Although diffusion models are primarily designed for generation tasks, their ability to learn semantic representations has also been recognized in recent years____. For example, Baranchuk et al.____ and DDAE____ leverage the intermediate activations of pre-trained diffusion models as features for segmentation and classification, respectively. HybViT____ and JDM____ jointly learn discriminative and generative tasks with a shared encoder to enhance feature representation. 
SODA____ turns diffusion models into strong self-supervised representation learners by imposing a bottleneck between an encoder and a denoising decoder.
DIVA____ employs the feedback of a frozen pre-trained diffusion model to boost the fine-grained perception capability of CLIP____ via a post-training approach.
Additionally, diffusion models are exploited as zero-shot classifiers____ by estimating noise given the class names, such as conditions, exhibiting great generalization robustness in out-of-distribution scenarios____. Inspired by these studies, we explore the utilization of a pre-trained diffusion model to enhance representation learning for the generalizable \mbox{Re-ID tasks.}

\subsection{Diffusion Models for Person Re-ID}
Diffusion models have also been applied to various person Re-ID tasks. For instance, VI-Diff____ employs a diffusion model to enhance visible-infrared Re-ID by generating new samples across modalities, thereby reducing the annotation cost of paired images. Diverse person____ proposes a diffusion-based framework to edit original dataset images with attribute texts, efficiently generating high-quality text-based person search datasets. PIDM____ also focuses on new data generation, using body pose and image style as guidance. Asperti~{et al.}____ decouple the person ID from other factors like poses and backgrounds to control new image sample generation. These works share a common characteristic of modifying existing data or generating new data for Re-ID related tasks. Additionally, DenoiseReID____ unifies feature extraction and feature denoising to improve feature discriminative capabilities for Re-ID. {PISL____ proposes a spatial diffusion model to refine patch sampling to enhance unsupervised Re-ID. PSDiff____ formulates the person search as a dual denoising process from noisy boxes and Re-ID embeddings to ground truths.} In contrast to these, we focus on generalizable representation learning assisted by the feedback back-propagated from a pre-trained diffusion model.

\subsection{Generalizable Person Re-ID}
Generalizable person Re-ID has been extensively studied over the past years. Existing methods can be roughly categorized into the following groups: domain-invariant and specific feature disentanglement____, normalization and domain alignment____, learning domain-adaptive mixture-of-experts____, meta-learning____, semantic expansion____, large-scale pre-training____, and so on. While various mechanisms have been designed, most of these methods learn feature representations within discriminative____ or contrastive learning____ frameworks. In contrast, we aim to leverage a pre-trained generative diffusion model to enhance the domain-invariant feature learning for more robust generalizable Re-ID.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{pipeline.pdf}
	\caption{An overview of the proposed framework. It consists of a baseline Re-ID model, a pre-trained diffusion model, and a correlation-aware conditioning scheme based on learnable ID-wise prompts. The Re-ID model is built upon the pre-trained CLIP image encoder____ and a BN Neck____, optimized by an ID loss and a prototypical contrastive loss. The diffusion model is constructed on via pre-trained stable diffusion____, with LoRA____ for efficient adaptation. The informative classification probabilities predicted by the Re-ID model is employed to produce a correlation-aware condition to guide the diffusion model for unleashing specific knowledge of generalization, with gradients back-propagated to the Re-ID model for enhanced generalizable feature learning.}
	\label{fig:pipeline}
\end{figure*}