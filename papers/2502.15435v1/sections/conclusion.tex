\section{Conclusion and future directions}
\label{conclusion}

In this work, we propose an effective and very efficient LLM jailbreaking detection method that is successful against state-of-the-art attacks. \methodname{} is $3\times$ faster than its closest competitor with better performance and it only needs $1$ forward pass through the LLM. Our defense is based on the observation that the negative log probabilities of tokens of attacked sentences are shifted to smaller values. We believe this observation is key to understanding adversarial attacks in LLMs. Our work can foster an understanding of the success of adversarial attacks. Following our initial observations, we train an SVM algorithm as a classifier using only the negative log probabilities of the first five tokens. Our experiments proved that its computational cost is considerably less than other methods, it can identify an attack before responding with more than the overall $93\%$ TP rate while keeping the FP rate under $12\%$. 

With slight modifications, \methodname{} can defend proprietary models without access to the full token probabilities. Our studies suggest that with full token probability access, the performance of our method could greatly improve.  We believe our work can foster the advancement towards stronger and more efficient defenses, enabling a low overhead detection of jailbreaking attempts.

\paragraph{Limitations} Our approach relies on having access to the next token logits of the model to defend. This constrains the performance of the defense mechanism, especially in the case of proprietary models like GPT-4. Our method relies on having samples of successful attacks for training an SVM classifier, nevertheless, we show that with very few samples we can train powerful defenses.

\subsubsection*{Broader impact statement}
Jailbreaking attacks enable malicious individuals and organizations to achieve malicious purposes. Our method improves the detection rate of such attacks and has a low false positive rate for benign inputs. Additionally, the efficiency of our approach allows fast integration within LLM APIs, this supposes a democratization of the access to defenses. On the negative side, publishing our findings, can also enable attackers to devise new strategies to circumvent our defense.

