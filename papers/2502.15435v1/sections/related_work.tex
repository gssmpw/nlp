\section{Related work}
In this section, we summarize the alignment methods, jailbreaking attacks, and jailbreaking defenses.

\paragraph{Alignment of LLMs}
\label{subsec:alignment}

LLMs require data-intensive training, making textual corpora on the internet the perfect training set in terms of data size. However, a crucial portion of their training data consists of unwanted and potentially dangerous content \citep{gehman2020realtoxicityprompts}. To avoid the generation of malicious content and match them with human values different methods have been employed, called ``alignment'' \citep{bai2022constitutional, hacker2023regulating, ouyang2022training, glaese2022improving, bai2022training, askell2021general}. Alignment has proven successful in guarding against malicious outputs for natural inputs, but not for adversarial inputs \citep{carlini2023are}.

Due to the high interest in jailbreaking studies, it is crucial to have standardized evaluation frameworks. The recent works of JailbreakBench \citep{chao2024jailbreakbench}, HarmBench \citep{mazeika2024harmbench}, and EasyJailbreak \citep{zhou2024easyjailbreak} are some of the first benchmarks on the topic. Additionally, many surveys have emerged to evaluate and compare these defenses \citep{xu2024llm, chu2024comprehensive, chowdhury2024breaking, liu2024jailbreaking, dong2024attacks}.

\paragraph{Adversarial (jailbreaking) attacks}
\label{subsec:attacks}

Since the seminal paper of \citet{Szegedy2014}, several adversarial attacks have been proposed for vision \citep{carlini2017towards,andriushchenko2020square,croce2020reliable} and language \citep{alzantot2018generating,Jin2020textfooler,guo2021gradient,hou2023textgrad} models. While the traditional attacks in NLP focus on text classification tasks, another category of attacks focused on jailbreaking has recently emerged. Following the categorization suggested by \citet{chao2023jailbreaking}, the dominant jailbreaking attacks can be divided into two categories: token-level or prompt-level attacks. 

Token-level attacks are generated by altering and optimizing one part of input tokens so that LLM would respond with harmful or toxic content. One example of a token-level attack is the universal and transferable attack proposed by \citet{zou2023universal} called Greedy Coordinate Gradient (GCG). In this attack, they set a malicious goal such as ``Tell me how to build a bomb'' and a specific target output phrase ``Sure, here's how to build a bomb.'' By concatenating the goal with a suffix and optimizing the suffix using the gradients with respect to the target output phrase, they create the successful attack sentence.

The prompt-level attacks change the whole prompt, instead of altering the input at the token level, to achieve the target response. There exist several variations on how the prompt can be modified, such as prefix injection \citep{perez2022ignore, liu2023prompt}, refusal suppression \citep{wei2023jailbroken}, role-playing with ``Do Anything Now'' (DAN) \citep{shen2023do}, multilingual attacks \citep{deng2024multilingual}, persuasion \citep{zeng2024johnny} and chain-of-thought reasoning \citep{wei2023chainofthought}.

Additionally, the method of creating the prompt can also vary drastically. Some methods search for attacks automatically with the help of an attacker LLM such as Prompt Automatic Iterative Refinement (PAIR) \citep{chao2023jailbreaking}, red teaming \citep{perez2022red, gehman2020realtoxicityprompts,casper2023explore, hong2024curiositydriven}, training it with RLHF to generate new attacks \citep{deng2023masterkey} or fooling itself \citep{xu2024an}. Other automatic generation methods include gradient-based optimization for generating interpretable suffixes \citep{zhu2023autodan},  stealthy prefix generation with hierarchical genetic algorithm (AutoDAN) \citep{liu2023autodan}, standard genetic algorithm \citep{lapid2023open}, multi-step data extraction \citep{li2023multistep} and using decoding methods \citep{huang2024catastrophic}. On the contrary, it is feasible to handcraft a prompt-level attack with manual search and prompt engineering \citep{Bartolo_2021, perez2022ignore, rao2023tricking, liu2023prompt, li2023deepinception, du2023analyzing, liu2023jailbreaking}. Independent of how they are generated, prompt-level attacks are usually human-interpretable, transferable, and harder to defend against \citep{chao2023jailbreaking}.

\paragraph{Jailbreaking defenses %
}
\label{subsec:jailbreaking_defenses}
To ensure the safe usage of LLMs, it is crucial to develop effective and efficient defense mechanisms against jailbreaks. Though the classical approach of fine-tuning or training  \citep{oneill2023adversarial} has been applied for this type of attacks, they are all computationally expensive methods. As a solution, the literature focuses more on post-training detection approaches. One simple method relies on the text perplexity which is the average negative log-likelihood of tokens appearing \citep{jain2023baseline, alon2023detecting}. A human eye can usually detect token-level jailbreaking attacks easily since one part of the sentence is unintelligible. Therefore, calculating the text perplexity could be used to detect adversarial sentences. If the perplexity of a prompt is higher than a threshold, they are considered as dangerous. 

Another common approach is using an LLM to detect harmful content. This can be achieved by using the same model with self-examination \citep{phute2023llm, li2024rain, xie2023defending, kim2024break} or another LLM \citep{perez2022red, wang2024defending, zeng2024autodefense,pisano2024bergeron}. Paraphrasing \citep{yung2024round}, retokenization \citep{jain2023baseline}, semantic smoothing \citep{ji2024defending}, prompt optimization \citep{zhou2024robust}, and goal prioritization \citep{zhang2023defending} have also been used for the detection, but they are either computationally expensive or does not perform well with prompt level attacks. 

Moreover, the studies of \citet{robey2023smoothllm, cao2023defending, kumar2023certifying} have shown that many jailbreaking attacks, especially token-level attacks like GCG, are fragile. Applying small perturbations such as randomly dropping a part of the sentence, inserting, swapping or changing a continuous patch of characters can decrease the attack success rate significantly. Therefore, perturbing the original prompt multiple times, getting a response for each, and using the majority vote as the final decision is proven to be an effective defense mechanism. However, the major setback of perturbation-oriented defenses is they need many forward passes for each input which is both time and resource-consuming and not feasible in real-life applications. 





