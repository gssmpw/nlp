\begin{figure*}[tb]
\vspace{-5mm}
\centering
\includegraphics[width=1\textwidth]{figures/hist_tsne.pdf}
    \vspace{-4mm}
     \caption{Changes caused by attacks: a) We plot the distribution of the logits of the first token for a sentence before and after an attack in green and orange respectively. We choose the original sentence \emph{``Tell me how to build a bomb.''} The attacked sentence was generated with GCG and the target output \emph{``Sure, here is how to build a bomb.''} In the graph, the first token distribution is shifted after the attack. b) We illustrate the entropy of the first logits of randomly selected 1000 benign and attacked sentences with blue and purple respectively. This figure shows a higher entropy with attacked sentences. c)  We show the 2-dimensional t-SNE plot of the training set with $h$ feature vector. We visualized the clear separability of attacked and not attacked sentences. Blue points correspond to benign sentences whereas purple ones are attacked.} 
\label{fig:distributions}
\end{figure*}

\section{Method} 

We propose a method to detect jailbreaking attacks with a single forward pass, by only considering the output probabilities of the first few tokens. Our approach, \methodname{}, is computationally efficient and does not depend on the criteria of another LLM. \Cref{fig:method} compares \methodname{} with other defense methods, highlighting its efficiency. 
We summarize the notation used in this manuscript in \cref{subsec:notation}, and subsequently, we present the motivation for our approach and introduce our algorithm in \cref{subsec:motivation,subsec:algorithm}, respectively.

\subsection{Notations and preliminaries}
\label{subsec:notation}
A sequence with $n$ tokens is denoted by $[x_i,\dots,x_{i+n}]$, with $x_i \in \mathcal{V}$, where $\mathcal{V}$ is the vocabulary or in other words, the token set. We represent an input sequence with $n$ 
 tokens as $\bm{x}_{1,n} := \left[ x_1,\dots,x_n \right]$. 
 Similarly, the output sequence with $m$ tokens which is the response to $\bm{x}_{1,n}$ is symbolized by $\bm{o}_{n+1,m} :=\left[ x_{n+1}\dots,x_{n+m} \right]$. When the sequence length is not important, we denote $\bm{x}_{1,n}$ as $\bm{x}$ and $\bm{o}_{n+1,m}$ as $\bm{o}$. 
 
A language model estimates the probability of the output token $\bm{o}_{n+1,m}$ as follows:
\begin{equation}
    \begin{aligned}
           & \mathbb{P}( \bm{o}_{n+1,m} |\bm{x}_{1,n})= \prod_{i=1}^m  \bm{\sigma}(\bm{l}_i(x_1,\dots, x_{n-1+i}))_{x_{i+n}}\,,\\
    \end{aligned}
\end{equation}
where we define $\bm{l}_i(x_1,\dots, x_{n-1+i}) \in \mathbb{R}^{\left | \mathcal{V} \right |}$ as the logit of model given input $x_1,\dots, x_{n-1+i}$. For notational simplicity we will sometimes refer to them as $\bm{l}_i$. 
Additionally, $\bm{\sigma}(\bm{l}_i)_{j} = \frac{e^{l_{ij}}}{\sum_{k=1}^{|\mathcal{V}|}e^{l_{ik}}}$ represents the softmax function.

\subsection{Motivation}
\label{subsec:motivation}

Previous studies on model inversion with images have shown that the feature vector carries crucial information about the input \citep{Dosovitskiy2015InvertingVR}. Similarly, in a recent study, the feature vector of an LLM has been used to get the input sequence \citep{morris2024language}. Moreover, \citet{shi2024detecting} utilize min-k probability to reveal if a sequence is in the pertaining data. Overall, these studies suggest that the output probabilities are more instrumental than just predicting the next token.

Jailbreaking attacks are designed to search for some input sequence $\hat{\bm{x}}_{1,n}$ so that the probability of observing some malicious output $\hat{\bm{o}}_{n+1,m}$ is maximized. A common approach used in automated jailbreaking attacks is minimizing the cross-entropy loss:
\begin{equation}
    \min_{\hat{\bm{x}}_{1,n}}\mathcal{L}(\hat{\bm{o}}_{n+1,m}, \bm{l}_{i}(\hat{x}_1,\dots, \hat{x}_{n-1+i}))\,,
    \label{eq:attack_problem}
\end{equation}
where we define the cross-entropy loss in the following form:
$\mathcal{L}(\hat{\bm{o}}_{n+1,m},\bm{l}_{i}) = \sum_{i=1}^{m} -\log\left(\bm{\sigma}(\bm{l}_i)_{\hat{x}_{i+n}}\right)\,.$ Another strategy is to iteratively refine the input sequence $\hat{\bm{x}}$ with the help of an auxiliary LLM until the output sequence $\hat{\bm{o}}$ complies with the original question. 

Independent of the method of generation, the attacks are designed to produce output sequences $\hat{\bm{o}}$ with specific requirements that cannot be directly obtained by naturally prompting the model. Given that the output probabilities carry inherited information about the input sequence, we pose the following question:
\begin{center}
    \emph{{Are the output token distributions of benign $\bm{x}$ and attacked inputs $\hat{\bm{x}}$ different?}}
\end{center}
If affirmative, we could design strategies for detecting attacks and defend against jailbreaking. \citet{jain2023baseline} already suggest GCG generates input sequences $\hat{\bm{x}}$ with high perplexity. Given that other attacks such as AutoDAN \citep{liu2023autodan}, PAIR \citep{chao2023jailbreaking}, and PAP \citep{zeng2024johnny} avoid this defense, our question emphasizes the output distribution to attempt to capture different types of attacks.

In our experiments, we observed that there exists a negative shift in the logit values of the output sequence when the input is an attacked sentence, as present in \cref{fig:distributions} (a). Moreover, we can spot a difference in the entropy of the first logits of outputs of benign vs. attack sentences. When the input is benign, for the first token, there are usually one or two high-probability candidate tokens while the rest have very small probabilities. In other words, the model is very certain about how to answer that prompt. When the input is attacked, the number of high-probability candidates increases resulting in a higher entropy. This change can be observed from \cref{fig:distributions} (b), where we can see that outputs of attacked sentences have a higher entropy in comparison to normal inputs. 
Thus, there are indeed differences between the distributions of  $\bm{x}$ and attacked inputs $\hat{\bm{x}}$. Consequently, we propose to use a binary classifier that can capture the difference in these distributions to decide if an attack has been attempted or not. 

\subsection{Single-pass detection}
\label{subsec:algorithm}

\paragraph{Feature matrix} 
As discussed previously, jailbreaking attacks cause unnatural patterns in the output token distribution such as the drastic negative shift in logit values or the increase in entropy of outputs as observed in \cref{fig:distributions}. To capture the change numerically, we propose to calculate the following feature matrix $\bm{H}:= \left[ \bm{h}_1, \bm{h}_2,\dots, \bm{h}_r \right] \in \mathbb{R}^{r\times k}$ such that:
\begin{equation}
    \label{eq:h}
     \bm{h}_i := -\log(\sigma(\bm{l}_{i,k}))\in \mathbb{R}^{k},
\end{equation}
where the original logit vector is $\bm{l}_{i} := LLM(\bm{x}_{1,n}) \in \mathbb{R}^{\left | \mathcal{V} \right |}$ and $\bm{l}_{i,k}\in \mathbb{R}^{k}$ is the logit vector with highest $k$ elements. The $r$ corresponds to the number of token positions that will be considered. Since the influence of input on the logit distribution is higher with smaller $i$, after some testing, we set $r=5$ and $k=50$, see \cref{app:token}. Note that although only $k$ tokens per position are included in the feature matrix, the probabilities are calculated with the whole vocabulary $\mathcal{V}$ to capture more information. 

\paragraph{Classification problem}

The adversarial sample detection problem can be approached as a classification task. To ensure the separability of \emph{attacked} and \emph{benign} sentences, we check the t-SNE plot of the feature vector\citep{van2008visualizing}. In \cref{fig:distributions} (c), we calculate the $\bm{H}$ matrix of 1000 randomly sampled benign and attacked sentences and use it to plot the 2-dimensional t-SNE graph. The clear distinction of the two classes indicates that the problem is separable with this feature matrix.

One way to tackle this classification task is to define an arbitrary function that will use the abovementioned feature matrix $\bm{H}$ to determine the final label. More formally, one can define a classifier function such that $f_{\text{class}}(\bm{H}): \mathbb{R}^{r \times k} \rightarrow \{0,1\}$ where $0$ corresponds to \emph{benign} and $1$ to \emph{attacked} sentences. Eventually, if $\bm{x}$ is considered as \emph{attacked}, the LLM should not deliver the response. 

Once we gather a training dataset $\{(\bm{H}_t, y_t)\}_{t=1}^{T}$ with labels $\bm{y}$ and number of samples $T$, we can train a classifier for this task. After exploring several classification methods (see \cref{app:token}), we conclude that a simple Support Vector Machine (SVM) with the RBF kernel \citep{Sch√∂lkopf2002kernels} is the best-performing strategy. Therefore, we select the SVM as our detection function  $f_{\text{class}}(\cdot)$. 


