
\section{Experiments}
In this section, after we describe the experimental setting, we provide experimental results and comparison with baselines using \llama{}, \vicuna{}, GPT-3.5, and GPT-4 models. Further details on the experimental setting, and experiments with Llama 3 can be found in \cref{app:sett} and \cref{app:llama3}, respectively. 

\subsection{Experimental settings}

\begin{table}[tb]
\caption{\textbf{Dataset sizes:} Number of samples in the complete dataset for each model. Each dataset is randomly sampled from an attack dataset with 100\% attack success. There is no overlap between test and training sets within a model.}
\label{table:data}
\begin{center}
\begin{footnotesize}
\begin{tabular}{ c c c c c c c c }
\toprule
\rowcolor{black!10} & Model  & GCG & AutoDAN &PAIR&PAP& AlpacaEval & QNLI\\
\midrule
&\llama{}                                                        &100&100&-&-& 200 & 200 \\
&\vicuna{}                                                       &200&200&185&5& 200 & 200\\
&GPT-3.5                  &95&100&-&-&400&500\\
&GPT-4                    &9&6&-&-&100&100\\
\multirow{-5}{*}{\textbf{Training Set}}&GPT-4o-mini                 &160&-&20&-&400&500\\
\bottomrule
\rowcolor{black!10} &\llama{}                                    &800&300&-&-& 400 & 2000 \\
\rowcolor{black!10}&\vicuna{}                                    &300&400&150&25& 400 &2000\\
\rowcolor{black!10}&GPT-3.5   &100&150&-&-&400&500 \\
\rowcolor{black!10}&GPT-4                 &15&25&-&-&100&100\\
\rowcolor{black!10}\multirow{-5}{*}{\textbf{Test Set}}&GPT-4o-mini   &400&-&100&-&400&500\\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Models} We used  \llama{} (\llamachat{})~\citep{touvron2023llama}, and \vicuna{}  (\vicunathirteen{})~\citep{chiang2023vicuna}
for our main experiments and performed ablation studies on GPT-3.5-turbo-0613~\citep{brown2020language} and GPT-4 and GPT-4o-mini~\citep{gpt4}.

\paragraph{Evaluation metrics} Our goal is to detect adversarial prompts in minimal time without being overcautious. We also want to avoid additional computational costs. To capture these, we report five metrics:
\begin{itemize}
    \item \textbf{True positive (TP) rate:} TP describes which portion of the attacked data is classified correctly. It can be calculated for individual attacks or as an average value for all attacks. A higher rate indicates better performance.
    \item \textbf{False positive (FP) rate:} FP describes the misclassification rate of benign samples. The value should be as low as possible.
    \item \textbf{$F_1$ scores:} To examine the overall predictive performance, we calculate the $\text{F}_1$ score which is $\frac{2TP}{2TP +FN + FP }$ where FN is the false negative rate (rate of misclassification of attacked samples). $F_1 \in \left[0,1\right]$ with $F_1=1$ as the perfect score.
    \item \textbf{Number of iterations:} Since the bottleneck of the computation lies in the LLM iteration, we report the number of forward passes for each method as an indication of computational cost. 
    \item \textbf{Average time:} Finally, in a real-life application, we want to minimize the inference time. We calculate the average time for each method using 10 samples from each dataset.
\end{itemize}

\paragraph{Dataset} We used four jailbreaking and two benign datasets: GCG~\citep{zou2023universal}, AutoDAN~\citep{liu2023autodan}, PAIR~\citep{chao2023jailbreaking} and PAP~\citep{zeng2024johnny}. After generating and testing each attack sentence, we form the attacked dataset for each model which has $100\%$ attack success rate. To measure the FP rate, we use two benign datasets: \textit{AlpacaEval}~\citep{dubois2024alpacafarm} and QNLI~\citep{wang-etal-2018-glue}. We split the datasets into test and training sets as so that there is no overlap between them. Within a model, all baselines are evaluated on the same test data. Dataset sizes are provided in \cref{table:data}. Further details on the datasets, generation process, and some examples can be found in \cref{app:dataset}.

 \paragraph{Jailbreaking criteria}
How to classify an output as an attack sentence is an open research question. To generate our attacked datasets, we utilize JailbreakBench \citep{chao2024jailbreakbench} implementation and check the success of each generated attack sentence with the Llama Guard model. The baselines SmoothLLM and RA-LLM rely on the refusal rate among iterations. Following the original implementations of these defense methods, a response is regarded as refusal if any of the typical rejection phrases of aligned models such as ``Sorry'', or ``I cannot'' are present in the output sequence. For this purpose, the ``StringClassifier'' implemented in JailbreakBench is used. We present additional experiments in \cref{app:guard} where we replace the StringClassifier with the Llama Guard model.

\begin{table*}[!t]
\caption{\textbf{Comparison against previous methods:} We measure the average number of forward passes, the average runtime, true positive (TP) and false positive (FP) rates, and $F_1$ score with \llama{} and \vicuna{}. The SmoothLLM method is abbreviated as SM. We highlight defenses that do not require analyzing the output text with {\color{blue}  \ding{117}}. The best method on each metric is highlighted in \textbf{bold}. The proposed defense, for most datasets \methodname{}, is able to achieve the highest TP and lowest FP while being the fastest defense. }
\label{table:results}
\setlength{\tabcolsep}{6pt}
\begin{center}
\begin{small}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{  c c  c c c c c c  c}
\toprule
 \multicolumn{2}{c}{\textbf{Model}}&\multicolumn{7}{c}{\large{\textbf{\llama{}}}} \\
\midrule
\rowcolor{black!10}\multicolumn{2}{c}{\textbf{Method}}
& Self
& SM 
&SM 
&SM 
& RA-LLM
& Self 
& \textbf{\methodname} \color{blue}  \ding{117} \\
\rowcolor{black!10}&&Perplexity \color{blue}  \ding{117}&(swap) & (patch) & (insert)& &Defense& \\
\midrule
\multicolumn{2}{c}{\textbf{Forward passes $\downarrow$}}&\textbf{1}&10 &10&10&10.25 &2 &\textbf{1}\\
\midrule
\rowcolor{black!10}\multicolumn{2}{c}{\textbf{Average time (s) $\downarrow$}}&0.39&19.71&19.31&19.55&4.12&1.315&\textbf{0.23}\\
\midrule
\multirow{2}{*}{\textbf{TP $\uparrow$}}&GCG& 98.63& 99.75&97&99.13&99.25&99.13&\textbf{99.75} \\
&AutoDAN&0.00& 92.67 &36 &70.67&\textbf{100.00}&\textbf{100.00}&\textbf{100.00}\\
\midrule
\rowcolor{black!10} &AlpacaEval& \textbf{0.25}&57.75&32.75&31.25&23.75&30.50&\textbf{0.25}\\
\rowcolor{black!10}\multirow{-2}{*}{\textbf{FP $\downarrow$}}&QNLI& 3.55 &  90.70& 73.95 & 68.50&54.90&20.45&\textbf{0.00}\\
\midrule
\multicolumn{2}{c}{\rebuttal{\textbf{Accuracy $\uparrow$} }}&\rebuttal{90.57}&\rebuttal{55.95}&\rebuttal{66.66}&\rebuttal{71.60}&\rebuttal{79.98}&\rebuttal{80.04}&\rebuttal{\textbf{99.80}}\\
\multicolumn{2}{c}{\textbf{$F_1$ Score $\uparrow$} 
}&0.82&0.69&0.65&0.72&0.80&0.90&\textbf{0.99}\\
\midrule
\toprule
 \multicolumn{2}{c}{\textbf{Model}}&\multicolumn{7}{c}{\large{\textbf{\vicuna{}}}} \\
\midrule
\rowcolor{black!10}\multicolumn{2}{c }{\textbf{Method}}
& Self
& SM 
&SM 
&SM 
& RA-LLM
& Self 
& \textbf{\methodname} \color{blue}  \ding{117} \\
\rowcolor{black!10}&&Perplexity \color{blue}  \ding{117}&(swap) & (patch) & (insert)& &Defense& \\
\midrule
\multicolumn{2}{c}{\textbf{Forward passes $\downarrow$}}&\textbf{1}&10 &10&10&9.93&2 &\textbf{1}\\
\midrule
\rowcolor{black!10}\multicolumn{2}{c}{\textbf{Average time (s) $\downarrow$}}&0.57&23.07&25.03&24.55&4.38&1.39&\textbf{0.36}\\
\midrule
\multirow{4}{*}{\textbf{TP $\uparrow$}}
&GCG& 75.33&\textbf{99.67}&97.67&99.33&98.67&22.67&99\\
&AutoDAN& 0.00& 32.25 & 11.00 & 18.5 &64.75&16.75&\textbf{95.75}\\
&PAIR& \textbf{100.00}&16.67&18.67&14.67&30.00&6.00&79.33\\
&PAP& 0.00&60.00&48.00&52.00&56.00&4.00&\textbf{84.00}\\
\midrule
\rowcolor{black!10} &AlpacaEval& \textbf{0.25}
&  12.5 & 8.75& 7.75&10&2.5&12.5\\
\rowcolor{black!10}\multirow{-2}{*}{\textbf{FP $\downarrow$}}&QNLI&  \textbf{2.65}&  46.05 & 34.7 & 33.3&33.90&4&11.65\\
\midrule
\multicolumn{2}{c}{\rebuttal{\textbf{Accuracy $\uparrow$} }}&\rebuttal{84.29}&\rebuttal{74.31}&\rebuttal{75.21}&\rebuttal{76.83}&\rebuttal{\textbf{91.78}}&\rebuttal{75.69}&\rebuttal{{89.36}}\\
\multicolumn{2}{c}{\textbf{$F_1$ Score $\uparrow$} }&0.59&0.55&0.50&0.53&0.70&0.27&\textbf{0.91}\\
\bottomrule
\end{tabular}}
\end{small}
\end{center}
\vskip -0.15in
\end{table*}

\paragraph{Baselines} We compared the performance of our method with four other adversarial defense mechanisms in the literature: self-perplexity filtering \citep{jain2023baseline}, SmoothLLM \citep{robey2023smoothllm}, RA-LLM \citep{cao2023defending} and self-defense \citep{phute2023llm}. For the self-perplexity filter, as suggested in the original paper, we set the threshold to the maximum perplexity prompts on \textit{AdvBench} dataset. While using the default parameters (threshold $0.2$, dropping rate $0.3$ and sampling number $20$) for RA-LLM, for SmoothLLM, we tested all three approaches, swap, patch, and insert with perturbation percentage $q=10\%$ and the number of iterations $N=10$ settings. Finally, different than the original implementation, we tested self-defense using the same LLM for output and assessment for a more fair comparison.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/baselines_vicuna_llama.pdf}
    \caption{Confusion matrices showing true positive (TP), true negative (TN), false positive (FP), and false negative (FN) percentages to compare \methodname{} with previous works. While the upper graph is for \llama{}, the lower one is plotted for \vicuna{}. Higher TP and lower FP indicate a better performance and \methodname{} achieves better rates than any other methods for both models. }
\label{fig:confusion}
\vskip -0.2in
\end{figure*}

\subsection{Results on \llama{} and \vicuna{}}
\label{subsec:results}

We illustrate the performance of our method in three aspects:
1) efficiency; 2) successful detection under different attacks; 3) performance on benign prompts. In \cref{table:results}, we display the evaluation of \methodname{} and several baselines. The experiments are conducted using the same datasets within a model where the attack datasets have $100\%$ attack success rate at the beginning. The average inference time per prompt is calculated using 10 samples from each dataset. Since the  RA-LLM method stops when the decision rate reaches a threshold, the number of forward passes is again calculated using 10 samples per dataset.

We additionally present the average confusion matrices in \cref{fig:confusion} with true positive (TP), true negative (TN), false positive (FP), and false negative (FN) percentages over the whole dataset, without distinguishing between attack types or benign dataset types where positive means a prompt classified as attacked. 

Table \ref{table:results} shows that most of the baseline models succeed well at detecting GCG-based attacks with TP rates over $90\%$. For AutoDAN, PAIR and PAP attacks, on the other hand, only \methodname{} can achieve a high performance of $95\%$ TP for both models. Our method achieves $100\%$ TP on AutoDAN attacks on \llama{} and over $99\%$ TP on GCG attacks on both models. The overall detection successes can also be observed by checking the confusion matrices where \methodname{} outperforms all baselines with $99.8\%$ and $93.7\%$ TP rate for \llama{} and \vicuna{} respectively.

One of the major drawbacks of detection mechanisms is over-firing, or in other words, classifying many benign inputs as dangerous. This is an important issue since it affects the overall performance of the model. Results illustrate this problem, with very high FP rates in all baselines where our method has an FP rate less than $1\%$ with \llama{} both datasets.

As a result, when we consider the $F_1$ scores of all methods, where a higher score indicates better predictive performance, \methodname{} almost achieves a perfect score of 1. 

Our other significant contribution is the efficiency of \methodname{} since it only takes $1$ forward pass and less than $0.4$ seconds per input. It is $80\times$ faster than SmoothLLM and $12\times$ faster than RA-LLM with better performance. Additionally, it is possible to detect an attacked prompt before responding which adds an extra layer of protection.  The same trend has also been observed with Llama 3 model. Please refer to \cref{app:llama3} for these additional results.

\subsection{Results on GPT Models}
\label{app:gpt}

As described in the \cref{subsec:algorithm}, \methodname{} requires access to logit values of the complete vocabulary. However, it may not be feasible for every case due to two reasons: 1) Newer LLMs tend to have a larger vocabulary size; 2) With closed-source models like  GPT-3.5, GPT-4 and GPT-4o-mini, only token logits with the highest 5 probabilities are available. Therefore, we tested \methodname{} in this setting using only \emph{top-5} token logits where we set $k=5$ and $r=25$ in \cref{eq:h} and use these 5 logits to calculate probabilities. For \llama{} and \vicuna{} models, we used the same training and test sets from \cref{subsec:results}.

Results provided in \cref{table:top5} indicate that the lack of full logit access decreases performance slightly as we can observe from the changes in \llama{} and \vicuna{} performances. However, even under these limitations, with modified \methodname{}, $87\%$ of attacks for GPT-3.5, $85\%$ for GPT-4 and $91\%$ for GPT-4o-mini have been successfully detected. %

Moreover, we compare the performance of \methodname{} with baselines on the GPT-3.5, GPT-4 and GPT-4o-mini models in \cref{table:gpt}. In this experiment \methodname{} outperforms the baselines, even under the constraints such as lack of full-logit access and small number of samples. Additionally, it is much more efficient, and it offers extra benefits concerning other baselines.

\begin{table}
\caption{\textbf{Detection rates of \methodname{} with \emph{top-5} tokens:} the average true positive (TP) and false positive (FP) rates of \methodname{} are computed with access to only \emph{top-5} tokens and $k=5$ and $r=25$ hyperparameters. As desired, even with minimal information, \methodname{} achieves high TP and low FP rates for each model. }
\label{table:top5}
\begin{center}
\begin{small}
\begin{tabular}{ c c   c }
\toprule
\textbf{Model} 
& \textbf{TP $\uparrow$} &  \textbf{FP} $\downarrow$\\
\midrule
\rowcolor{black!10}\llama{}            & 100.00 & 0.54\\
\vicuna{}           & 78.86  & 14.06\\
\rowcolor{black!10}GPT-3.5             & 87.20  & 10.00\\
GPT-4               & 85.00  & 9.50\\
\rowcolor{black!10}GPT-4o-mini           & 91.20  & 13.33\\

\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table*}[t]
\caption{\textbf{Comparison against previous methods:} We measure the true positive (TP) and false positive (FP) rates and $F_1$ score with GPT-3.5, GPT-4 and GPT-4o-mini models. The SmoothLLM method is abbreviated as SM. We highlight defenses that do not require analyzing the output text with {\color{blue}  \ding{117}}. The best method on each metric is highlighted in \textbf{bold}. }
\label{table:gpt}
\setlength{\tabcolsep}{6pt}
\begin{center}
\begin{small}
\begin{tabular}{ c c c  c c c c c  c}
\toprule
\rowcolor{black!10}\multicolumn{2}{c}{\textbf{Method}}&
& SM 
&SM 
&SM 
& RA-LLM
& Self 
& \textbf{\methodname} {\color{blue}  \ding{117}} \\
\rowcolor{black!10}&& &(swap) & (insert) & (patch)& &Defense& \\
\midrule
&\multirow{2}{*}{\textbf{TP $\uparrow$}}&GCG&  62.00 & 30.00 & 61.00&23.0&\textbf{88.00}&71.00\\
&&AutoDAN                                    &  62.67 & 42.00 & 25.33&86.00&60.00&\textbf{98.00}\\
    \cline{2-9}
\cellcolor{white}&\cellcolor{black!10} &\cellcolor{black!10}AlpacaEval                                     & \cellcolor{black!10}6.75& \cellcolor{black!10}4.50& \cellcolor{black!10}\textbf{3.75}&\cellcolor{black!10}4.5&\cellcolor{black!10}8.00&\cellcolor{black!10}13.00\\
\cellcolor{white}&\cellcolor{black!10}\multirow{-2}{*}{\textbf{FP $\downarrow$}}&\cellcolor{black!10}QNLI &\cellcolor{black!10}28.60&\cellcolor{black!10}21.40&\cellcolor{black!10}16.80&\cellcolor{black!10}24&\cellcolor{black!10}15.20&\cellcolor{black!10}\textbf{7.60}\\
    \cline{2-9}
\cellcolor{white}\multirow{-5}{*}{\textbf{GPT-3.5-Turbo}}&\multicolumn{2}{c }{\textbf{$F_1$ Score $\uparrow$} }&0.69&0.49&0.53&0.69&0.78&\textbf{0.88}\\
\bottomrule

\toprule
&\multirow{2}{*}{\textbf{TP $\uparrow$}}&GCG&  \textbf{100.00}    & 93.33       & 66.67      & 6.67  & 46.67 & 60.00  \\
&&AutoDAN & 16.00     & 20.00       & 16.00      & 16.00 & 64.00 & \textbf{100.00 }\\
    \cline{2-9}
\cellcolor{white}&\cellcolor{black!10} &\cellcolor{black!10}AlpacaEval &\cellcolor{black!10}2.00      & \cellcolor{black!10}2.00        & \cellcolor{black!10}2.00       & \cellcolor{black!10}\textbf{0.00}  & \cellcolor{black!10}2.00  & \cellcolor{black!10}10.00  \\
\cellcolor{white}&\cellcolor{black!10}\multirow{-2}{*}{\textbf{FP $\downarrow$}}&\cellcolor{black!10}QNLI& \cellcolor{black!10}1.00      & \cellcolor{black!10}1.00        & \cellcolor{black!10}2.00       & \cellcolor{black!10}0.00  & \cellcolor{black!10}\textbf{0.00}  & \cellcolor{black!10}9.00  \\
    \cline{2-9}
\cellcolor{white}\multirow{-5}{*}{\textbf{GPT-4}}&\multicolumn{2}{c }{\textbf{$F_1$ Score $\uparrow$} }& 0.61      & 0.61        & 0.48       & 0.22  & 0.71  & \textbf{0.73} \\
\bottomrule

\toprule
&\multirow{2}{*}{\textbf{TP $\uparrow$}}&GCG&  11.00&10.25&8.50&0.00&90.25&\textbf{96.25}\\
&&PAIR  & 12.00&10.00&24.00&4.00&\textbf{81.00}&71.00  \\
    \cline{2-9}
\cellcolor{white}&\cellcolor{black!10} &\cellcolor{black!10}AlpacaEval &\cellcolor{black!10}0.75& \cellcolor{black!10}1.00&\cellcolor{black!10} 0.75& \cellcolor{black!10}\textbf{0.25}& \cellcolor{black!10}35.00& \cellcolor{black!10}16.5\\
\cellcolor{white}&\cellcolor{black!10}\multirow{-2}{*}{\textbf{FP $\downarrow$}}&\cellcolor{black!10}QNLI &\cellcolor{black!10}0.60&\cellcolor{black!10}0.60&\cellcolor{black!10}1.20&\cellcolor{black!10}\textbf{0.00}&\cellcolor{black!10}25.00&\cellcolor{black!10}10.80\\
    \cline{2-9}
\cellcolor{white}\multirow{-5}{*}{\textbf{GPT-4o-mini}}&\multicolumn{2}{c }{\textbf{$F_1$ Score $\uparrow$} }&0.20&0.18&0.21&0.02&0.81&\textbf{0.89}\\
\bottomrule

\end{tabular}
\end{small}
\end{center}
\end{table*}

For further experimental results, please refer to the Appendix. In \cref{app:llama3}, we provide comparison of \methodname{} with baselines using Llama 3 model. In \cref{app:app} various ablation studies on different aspects of the \methodname{} such as replacing logits with hidden state values (\cref{app:final_layer}), and hyperparameter and classifier selection can be found (\cref{app:token}). Additionally, we compare \methodname{} with different methods such as Llama Guard for refusal in baselines or using Bert-based classifier for the task in \cref{app:guard} and \cref{app:bert} respectively. Moreover, to further compare the classification capabilities of baselines, we included the ROC curves and AUROC scores in \cref{app:roc}. For detailed discussions on the generalization capabilities of SPD on unseen data and its training data dependency, we kindly as readers to refer \cref{app:data} and \cref{app:unseen}.
