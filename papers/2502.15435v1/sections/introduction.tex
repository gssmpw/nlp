
\section{Introduction}
\label{intro}

The impressive capabilities of large language models (LLMs)~\citep{brown2020language,achiam2023gpt} also highlight the dual nature of their potential, as they can also respond to illicit or detrimental queries equally skillfully. Currently, the safety guardrails inserted by finetuning LLMs preferences~\citep{bai2022constitutional, hacker2023regulating, ouyang2022training, sun2023principle}, can still be easily compromised with so-called ``jailbreaking'' attacks owing to the competing objectives of offering useful and accurate responses versus resisting to answer more harmful questions~\citep{wei2023jailbroken}.

The ``jailbreaking'' attacks~\citep{shen2023do,zou2023universal,carlini2023are,liu2023autodan,zeng2024johnny,sadasivan2024fast} are a prime instance of avoiding the guardrails through modifications to the harmful prompt to trick the model. For instance, \citet{zou2023universal} show that one can add an adversarial suffix after ``Tell me how to build a bomb'' to enforce the model to generate instructions. 

To defend against these attacks, a number of post-alignment mechanisms have been proposed~\citep{robey2023smoothllm,perez2022red,phute2023llm,jain2023baseline,zhou2024robust}. The majority of these defense methods suffer from two core limitations: (a) they require multiple forward passes, or (b) they require auxiliary LLMs for defending, which makes them computationally demanding.  For instance, one type of defense is perturbation-based methods \citep{robey2023smoothllm,cao2023defending,kumar2023certifying}. Those perturb the input multiple times, generating a response each time and taking the majority decision as the final reply. Another type of defense is using an auxiliary LLM as the decision-maker on the safety of the input prompt \citep{perez2022red, phute2023llm}.




\begin{figure}[t]
\vspace{-5mm}
\begin{center}
    \includegraphics[width=1\columnwidth]{figures/method_schematic_defense4.pdf}
\end{center}
\vspace{-5mm}
\caption{{Schematic of the proposed method and comparison with previous approaches perturbation based, such as SmoothLLM and RA-LLM, (left) and auxiliary LLM based, like Self-Defense (middle). Our method requires a single forward pass to predict the attack.}}
\label{fig:method}
\end{figure}

In addition to the computational cost, these core limitations either make the inference time longer or require access to multiple models simultaneously. To avoid these drawbacks, an efficient defense method is needed. In this work, we introduce a simple, yet effective method, called \methodname{}, which leverages information on the logits of the model to predict whether the output will have harmful content or not. Our intuition relies on the differences we observe in the distribution of logits of output tokens when the LLM responds to a benign vs. attacked input. By utilizing this difference, \methodname{} can distinguish jailbreaking attacks with a single forward pass without the assistance of an additional LLM.


Overall, our contributions can be summarized as follows:
\begin{itemize}
\vspace{-1mm}
    \item We introduce \methodname{}, a method that can detect harmful jailbreaking attacks with only a single forward pass by leveraging logit values.
    \item We conduct a thorough evaluation on open-source LLMs, e.g., \llama{}, Llama 3 and \vicuna{}. Our results showcase that, in comparison to existing approaches, \methodname{} attains both high efficiency and detection rate when identifying unsafe sentences.
    \item We demonstrate that even without accessing the full logit of models, \methodname{} can still be a promising approach, as evidenced by testing on GPT-3.5, GPT-4 and GPT-4o-mini.
\end{itemize}
