@article{wang2005slu,
  author={Ye-Yi Wang and Li Deng and Acero, A.},
  journal={IEEE Signal Processing Magazine}, 
  title="{Spoken Language Understanding}", 
  year={2005},
  volume={22},
  number={5},
  pages={16-31},
  keywords={Natural languages;Signal processing;Automatic speech recognition;Mathematical model;Pattern recognition;Speech recognition;Hidden Markov models;Humans;Bridges;Solids},
  doi={10.1109/MSP.2005.1511821}}


@book{tur2011slu,
    author="Tur, Gokhan and De Mori, Renato",
    title="{Spoken Language Understanding: Systems for Extracting Semantic Information from Speech}",
    publisher="John Wiley and Sons",
    year="2011",
    URL="https://cir.nii.ac.jp/crid/1130000794961797504"
}

@inproceedings{jung-etal-2024-interventional,
    title = "{Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding}",
    author = "Jung, YeonJoon  and
      Lee, Jaeseong  and
      Choi, Seungtaek  and
      Lee, Dohyeon  and
      Kim, Minsoo  and
      Hwang, Seung-won",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1149",
    doi = "10.18653/v1/2024.emnlp-main.1149",
    pages = "20642--20655",
    abstract = "Recently, pre-trained language models (PLMs) have been increasingly adopted in spoken language understanding (SLU). However, automatic speech recognition (ASR) systems frequently produce inaccurate transcriptions, leading to noisy inputs for SLU models, which can significantly degrade their performance. To address this, our objective is to train SLU models to withstand ASR errors by exposing them to noises commonly observed in ASR systems, referred to as ASR-plausible noises. Speech noise injection (SNI) methods have pursued this objective by introducing ASR-plausible noises, but we argue that these methods are inherently biased towards specific ASR systems, or ASR-specific noises. In this work, we propose a novel and less biased augmentation method of introducing the noises that are plausible to any ASR system, by cutting off the non-causal effect of noises. Experimental results and analyses demonstrate the effectiveness of our proposed methods in enhancing the robustness and generalizability of SLU models against unseen ASR systems by introducing more diverse and plausible ASR noises in advance.",
}

@misc{cui2021approachimproverobustnessnlp,
      title="{An Approach to Improve Robustness of NLP Systems against ASR Errors}", 
      author={Tong Cui and Jinghui Xiao and Liangyou Li and Xin Jiang and Qun Liu},
      year={2021},
      eprint={2103.13610},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.13610}, 
}

@inproceedings{di-gangi-etal-2019-robust,
    title = "{Robust Neural Machine Translation for Clean and Noisy Speech Transcripts}",
    author = "Di Gangi, Matti  and
      Enyedi, Robert  and
      Brusadin, Alessandra  and
      Federico, Marcello",
    editor = {Niehues, Jan  and
      Cattoni, Rolando  and
      St{\"u}ker, Sebastian  and
      Negri, Matteo  and
      Turchi, Marco  and
      Ha, Thanh-Le  and
      Salesky, Elizabeth  and
      Sanabria, Ramon  and
      Barrault, Loic  and
      Specia, Lucia  and
      Federico, Marcello},
    booktitle = "Proceedings of the 16th International Conference on Spoken Language Translation",
    month = nov # " 2-3",
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2019.iwslt-1.32",
    abstract = "Neural machine translation models have shown to achieve high quality when trained and fed with well structured and punctuated input texts. Unfortunately, the latter condition is not met in spoken language translation, where the input is generated by an automatic speech recognition (ASR) system. In this paper, we study how to adapt a strong NMT system to make it robust to typical ASR errors. As in our application scenarios transcripts might be post-edited by human experts, we propose adaptation strategies to train a single system that can translate either clean or noisy input with no supervision on the input type. Our experimental results on a public speech translation data set show that adapting a model on a significant amount of parallel data including ASR transcripts is beneficial with test data of the same type, but produces a small degradation when translating clean text. Adapting on both clean and noisy variants of the same data leads to the best results on both input types.",
}

@misc{dutta2022errorcorrectionasrusing,
      title="{Error Correction in ASR using Sequence-to-Sequence Models}", 
      author={Samrat Dutta and Shreyansh Jain and Ayush Maheshwari and Souvik Pal and Ganesh Ramakrishnan and Preethi Jyothi},
      year={2022},
      eprint={2202.01157},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.01157}, 
}

@inproceedings{feng22c_interspeech,
  title     = "{ASR-Robust Natural Language Understanding on ASR-GLUE dataset}",
  author    = {Lingyun Feng and Jianwei Yu and Yan Wang and Songxiang Liu and Deng Cai and Haitao Zheng},
  year      = {2022},
  booktitle = {Interspeech 2022},
  pages     = {1101--1105},
  doi       = {10.21437/Interspeech.2022-10097},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/interspeech_2022/feng22c_interspeech.html}
}

@inproceedings{kim2021robust,
  author={Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Hakkani-Tür, Dilek},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, 
  title="{“How Robust R U?”: Evaluating Task-Oriented Dialogue Systems on Spoken Conversations}", 
  year={2021},
  volume={},
  number={},
  pages={1147-1154},
  keywords={Conferences;Benchmark testing;Data models;Robustness;Task analysis;Automatic speech recognition;spoken dialogue systems;dialogue state tracking;knowledge-grounded dialogue generation},
  doi={10.1109/ASRU51503.2021.9688274}
}

@inproceedings{leng-etal-2021-fastcorrect-2,
    title = "{FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition}",
    author = "Leng, Yichong  and
      Tan, Xu  and
      Wang, Rui  and
      Zhu, Linchen  and
      Xu, Jin  and
      Liu, Wenjie  and
      Liu, Linquan  and
      Li, Xiang-Yang  and
      Qin, Tao  and
      Lin, Edward  and
      Liu, Tie-Yan",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.367",
    doi = "10.18653/v1/2021.findings-emnlp.367",
    pages = "4328--4337",
    abstract = "Error correction is widely used in automatic speech recognition (ASR) to post-process the generated sentence, and can further reduce the word error rate (WER). Although multiple candidates are generated by an ASR system through beam search, current error correction approaches can only correct one sentence at a time, failing to leverage the voting effect from multiple candidates to better detect and correct error tokens. In this work, we propose FastCorrect 2, an error correction model that takes multiple ASR candidates as input for better correction accuracy. FastCorrect 2 adopts non-autoregressive generation for fast inference, which consists of an encoder that processes multiple source sentences and a decoder that generates the target sentence in parallel from the adjusted source sentence, where the adjustment is based on the predicted duration of each source token. However, there are some issues when handling multiple source sentences. First, it is non-trivial to leverage the voting effect from multiple source sentences since they usually vary in length. Thus, we propose a novel alignment algorithm to maximize the degree of token alignment among multiple sentences in terms of token and pronunciation similarity. Second, the decoder can only take one adjusted source sentence as input, while there are multiple source sentences. Thus, we develop a candidate predictor to detect the most suitable candidate for the decoder. Experiments on our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce the WER over the previous correction model with single candidate by 3.2{\%} and 2.6{\%}, demonstrating the effectiveness of leveraging multiple candidates in ASR error correction. FastCorrect 2 achieves better performance than the cascaded re-scoring and correction pipeline and can serve as a unified post-processing module for ASR.",
}

@inproceedings{liu-etal-2021-robustness,
    title = "{Robustness Testing of Language Understanding in Task-Oriented Dialog}",
    author = "Liu, Jiexi  and
      Takanobu, Ryuichi  and
      Wen, Jiaxin  and
      Wan, Dazhen  and
      Li, Hongguang  and
      Nie, Weiran  and
      Li, Cheng  and
      Peng, Wei  and
      Huang, Minlie",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.192",
    doi = "10.18653/v1/2021.acl-long.192",
    pages = "2467--2480",
    abstract = "Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution. However, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice. In this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important aspects related to language understanding in real-world dialog systems, namely, language variety, speech characteristics, and noise perturbation. We propose a model-agnostic toolkit LAUG to approximate natural language perturbations for testing the robustness issues in task-oriented dialog. Four data augmentation approaches covering the three aspects are assembled in LAUG, which reveals critical robustness issues in state-of-the-art models. The augmented dataset through LAUG can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog.",
}

@article{siino2024preprocessing,
title = "{Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers}",
journal = {Information Systems},
volume = {121},
pages = {102342},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102342},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923001783},
author = {Marco Siino and Ilenia Tinnirello and Marco {La Cascia}},
keywords = {Text preprocessing, Natural Language Processing, Fake news, SVM, Bayes, Transformers, Deep learning, LSTM, Convolutional neural networks},
abstract = {With the advent of the modern pre-trained Transformers, the text preprocessing has started to be neglected and not specifically addressed in recent NLP literature. However, both from a linguistic and from a computer science point of view, we believe that even when using modern Transformers, text preprocessing can significantly impact on the performance of a classification model. We want to investigate and compare, through this study, how preprocessing impacts on the Text Classification (TC) performance of modern and traditional classification models. We report and discuss the preprocessing techniques found in the literature and their most recent variants or applications to address TC tasks in different domains. In order to assess how much the preprocessing affects classification performance, we apply the three top referenced preprocessing techniques (alone or in combination) to four publicly available datasets from different domains. Then, nine machine learning models – including modern Transformers – get the preprocessed text as input. The results presented show that an educated choice on the text preprocessing strategy to employ should be based on the task as well as on the model considered. Outcomes in this survey show that choosing the best preprocessing technique – in place of the worst – can significantly improve accuracy on the classification (up to 25%, as in the case of an XLNet on the IMDB dataset). In some cases, by means of a suitable preprocessing strategy, even a simple Naïve Bayes classifier proved to outperform (i.e., by 2% in accuracy) the best performing Transformer. We found that Transformers and traditional models exhibit a higher impact of the preprocessing on the TC performance. Our main findings are: (1) also on modern pre-trained language models, preprocessing can affect performance, depending on the datasets and on the preprocessing technique or combination of techniques used, (2) in some cases, using a proper preprocessing strategy, simple models can outperform Transformers on TC tasks, (3) similar classes of models exhibit similar level of sensitivity to text preprocessing.}
}


@inproceedings{fang2020phoneme,
author = {Fang, Anjie and Filice, Simone and Limsopatham, Nut and Rokhlenko, Oleg},
title = {Using Phoneme Representations to Build Predictive Models Robust to ASR Errors},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401050},
doi = {10.1145/3397271.3401050},
abstract = {Even though Automatic Speech Recognition (ASR) systems significantly improved over the last decade, they still introduce a lot of errors when they transcribe voice to text. One of the most common reasons for these errors is phonetic confusion between similar-sounding expressions. As a result, ASR transcriptions often contain "quasi-oronyms", i.e., words or phrases that sound similar to the source ones, but that have completely different semantics (e.g., "win" instead of "when" or "accessible on defecting" instead of "accessible and affecting"). These errors significantly affect the performance of downstream Natural Language Understanding (NLU) models (e.g., intent classification, slot filling, etc.) and impair user experience. To make NLU models more robust to such errors, we propose novel phonetic-aware text representations. Specifically, we represent ASR transcriptions at the phoneme level, aiming to capture pronunciation similarities, which are typically neglected in word-level representations (e.g., word embeddings). To train and evaluate our phoneme representations, we generate noisy ASR transcriptions of four existing datasets - Stanford Sentiment Treebank, SQuAD, TREC Question Classification and Subjectivity Analysis - and show that common neural network architectures exploiting the proposed phoneme representations can effectively handle noisy transcriptions and significantly outperform state-of-the-art baselines. Finally, we confirm these results by testing our models on real utterances spoken to the Alexa virtual assistant.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {699–708},
numpages = {10},
keywords = {virtual assistant, phoneme embeddings, natural language understanding, deep learning, classification, ASR error},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{Stouten2006cleaning,
title = "{Coping with disfluencies in spontaneous speech recognition: Acoustic detection and linguistic context manipulation}",
journal = {Speech Communication},
volume = {48},
number = {11},
pages = {1590-1606},
year = {2006},
note = {Robustness Issues for Conversational Interaction},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2006.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167639306000434},
author = {Frederik Stouten and Jacques Duchateau and Jean-Pierre Martens and Patrick Wambacq},
keywords = {Disfluency handling, Spontaneous speech recognition, Disfluency detection},
abstract = {Nowadays read speech recognition already works pretty well, but the recognition of spontaneous speech is much more problematic. There are plenty of reasons for this, and we hypothesize that one of them is the regular occurrence of disfluencies in spontaneous speech. Disfluencies disrupt the normal course of the sentence and when for instance word interruptions are concerned, they also give rise to word-like speech elements which have no representation in the lexicon of the recognizer. In this paper we propose novel methods that aim at coping with the problems induced by three types of disfluencies, namely filled pauses, repeated words and sentence restarts. Our experiments show that especially the proposed methods for filled pause handling offer a moderate but statistically significant improvement over the more traditional techniques previously presented in the literature.}
}

@inproceedings{leng2021fastcorrect,
 author = {Leng, Yichong and Tan, Xu and Zhu, Linchen and Xu, Jin and Luo, Renqian and Liu, Linquan and Qin, Tao and Li, Xiangyang and Lin, Edward and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {21708--21719},
 publisher = {Curran Associates, Inc.},
 title = "{FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition}",
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/b597460c506e8e35fb0cc1c1905dd3bc-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{guo2023cleaning,
  author={Guo, Jiaxin and Wang, Minghan and Qiao, Xiaosong and Wei, Daimeng and Shang, Hengchao and Li, Zongyao and Yu, Zhengzhe and Li, Yinglu and Su, Chang and Zhang, Min and Tao, Shimin and Yang, Hao},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title="{UCorrect: An Unsupervised Framework for Automatic Speech Recognition Error Correction}", 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Error analysis;Training data;Detectors;Data models;Generators;Error correction;Decoding;ASR;Error Correction;Unsupervised;WER},
  doi={10.1109/ICASSP49357.2023.10096194}
}

@inproceedings{guo2019cleaning,
  author={Guo, Jinxi and Sainath, Tara N. and Weiss, Ron J.},
  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title="{A Spelling Correction Model for End-to-end Speech Recognition}", 
  year={2019},
  volume={},
  number={},
  pages={5651-5655},
  keywords={speech recognition;sequence-to-sequence;attention models;spelling correction;language model},
  doi={10.1109/ICASSP.2019.8683745}
}

@article{Errattahi2018correction,
title = "{Automatic Speech Recognition Errors Detection and Correction: A Review}",
journal = {Procedia Computer Science},
volume = {128},
pages = {32-37},
year = {2018},
note = {1st International Conference on Natural Language and Speech Processing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918302187},
author = {Rahhal Errattahi and Asmaa {El Hannani} and Hassan Ouahmane},
keywords = {Automatic Speech Recognition, ASR Error Detection, ASR Error Correction, ASR evaluation},
abstract = {Even though Automatic Speech Recognition (ASR) has matured to the point of commercial applications, high error rate in some speech recognition domains remain as one of the main impediment factors to the wide adoption of speech technology, and especially for continuous large vocabulary speech recognition applications. The persistent presence of ASR errors have intensified the need to find alternative techniques to automatically detect and correct such errors. The correction of the transcription errors is very crucial not only to improve the speech recognition accuracy, but also to avoid the propagation of the errors to the subsequent language processing modules such as machine translation. In this paper, basic principles of ASR evaluation are first summarized, and then the state of the current ASR errors detection and correction research is reviewed. We focus on emerging techniques using word error rate metric.}
}

@inproceedings{zechner-waibel-2000-minimizing,
    title = "{Minimizing Word Error Rate in Textual Summaries of Spoken Language}",
    author = "Zechner, Klaus  and
      Waibel, Alex",
    booktitle = "1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2000",
    url = "https://aclanthology.org/A00-2025",
}

@article{Chowdhury2024noiseeffect,
author = {Chowdhury, Priyanjana and Sarkar, Nabanika and Nath, Sanghamitra and Sharma, Utpal},
title = "{Analyzing the Effects of Transcription Errors on Summary Generation of Bengali Spoken Documents}",
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3678005},
doi = {10.1145/3678005},
abstract = {Automatic speech recognition (ASR) has become an indispensable part of the AI domain, with various speech technologies reliant on it. The quality of speech recognition depends on the amount of annotated data used to train an ASR system, among other factors. For a low-resourced language, this is a severe constraint and thus ASR quality is often poor. Humans can read through text containing ASR-errors, provided the context of the sentence is preserved. Yet in cases of transcripts generated by ASR systems of low-resource languages, multiple important words are misrecognized and the context is mostly lost; discerning such a text becomes nearly impossible. This article analyzes the types of transcription errors that occur while generating ASR transcripts of spoken documents in Bengali, an under-resourced language predominantly spoken in India and Bangladesh. The transcripts of the Bengali spoken document are generated using the ASR of Google Cloud Speech. The article also explores if there is an effect of such transcription errors in generating speech summaries of these spoken documents. Summarization is carried out extractively; sentences are selected from the ASR-generated text of the spoken document. Speech summaries are created by aggregating the speech-segments from the original speech of the selected sentences. Subjective evaluation shows the “readability” of the spoken summaries are not degraded by ASR errors, but the quality is affected due to the reliance on intermediate text-summary containing transcription errors.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {136},
numpages = {28},
keywords = {Low-resource language, extractive speech sumarization, automatic speech recognition, transcription error, speech summary}
}

@inproceedings{You2021noiseQA,
  author={You, Chenyu and Chen, Nuo and Zou, Yuexian},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title="{Knowledge Distillation for Improved Accuracy in Spoken Question Answering}", 
  year={2021},
  volume={},
  number={},
  pages={7793-7797},
  keywords={Training;Conferences;Manuals;Syntactics;Signal processing;Knowledge discovery;Natural language processing;knowledge distillation;spoken question answering;question answering},
  doi={10.1109/ICASSP39728.2021.9414999}}

@inproceedings{Szaszak2016noiseSumm,
author = {Szasz\'{a}k, Gy\"{o}rgy and T\"{u}ndik, M\'{a}t\'{e} \'{A}kos and Beke, Andr\'{a}s},
title = "{Summarization of Spontaneous Speech using Automatic Speech Recognition and a Speech Prosody based Tokenizer}",
year = {2016},
isbn = {9789897582035},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0006044802210227},
doi = {10.5220/0006044802210227},
abstract = {This paper addresses speech summarization of highly spontaneous speech. The audio signal is transcribed usingan Automatic Speech Recognizer, which operates at relatively high word error rates due to the complexityof the recognition task and high spontaneity of speech. An analysis is carried out to assess the propagationof speech recognition errors into syntactic parsing. We also propose an automatic, speech prosody based audiotokenization approach and compare it to human performance. The so obtained sentence-like tokens areanalysed by the syntactic parser to help ranking based on thematic terms and sentence position. The thematicterm is expressed in two ways: TF-IDF and Latent Semantic Indexing. The sentence scores are calculated asa linear combination of the thematic term score and a positional score. The summary is generated from thetop 10 candidates. Results show that prosody based tokenization reaches human average performance and thatspeech recognition errors propagate moderately into syntactic parsing (POS tagging and dependency parsing).Nouns prove to be quite error resistant. Audio summarization shows 0.62 recall and 0.79 precision by anF-measure of 0.68, compared to human reference. A subjective test is also carried out on a Likert-scale. Allresults apply to spontaneous Hungarian.},
booktitle = {Proceedings of the International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
pages = {221–227},
numpages = {7},
keywords = {Tokenization, Summarization, Speech Recognition, Speech, Latent Semantic Indexing., Audio},
location = {Porto, Portugal},
series = {IC3K 2016}
}

@article{Tundik2019noiseSLU, 
title="{On the Effects of Automatic Transcription and Segmentation Errors in Hungarian Spoken Language Processing}", 
volume={63}, 
url={https://pp.bme.hu/eecs/article/view/14052}, 
DOI={10.3311/PPee.14052}, 
abstractNote={&lt;p&gt;Emerging Artificial Intelligence (AI) technology has brought machines to reach an equal or even superior level compared to human capabilities in several fields; nevertheless, among many other fields, making a computer able to understand human language still remains a challenge. When dealing with speech understanding, Automatic Speech Recognition (ASR) is used to generate transcripts, which are processed with text-based tools targeting Spoken Language Understanding (SLU). Depending on the ASR quality (which further depends on speech quality, the complexity of the topic, environment etc.), transcripts contain errors, which propagate further into the processing pipeline. Subjective tests show on the other hand, that humans understand quite well ASR-closed captions, despite the word and punctuation errors. Through word embedding based semantic parsing, the present paper is interested in quantifying the semantic bias introduced by ASR error propagation. As a special use case, speech summarization is also evaluated with regard to ASR error propagation. We show, that despite the higher word error rates seen with the highly inflectional Hungarian, the semantic space suffers least impact than the difference in Word Error Rate would suggest.&lt;/p&gt;}, 
number={4}, 
journal={Periodica Polytechnica Electrical Engineering and Computer Science}, 
author={Tündik, Máté Ákos and Kaszás, Valér and Szaszák, György}, 
year={2019}, 
pages={254–262}
}

@INPROCEEDINGS{Agarwal2007noise,
  author={Agarwal, Sumeet and Godbole, Shantanu and Punjani, Diwakar and Roy, Shourya},
  booktitle={Seventh IEEE International Conference on Data Mining (ICDM 2007)}, 
  title="{How Much Noise Is Too Much: A Study in Automatic Text Classification}", 
  year={2007},
  volume={},
  number={},
  pages={3-12},
  keywords={Text categorization;Cleaning;Data processing;Blogs;Handwriting recognition;Automatic speech recognition;Text recognition;Internet;Mobile handsets;Text mining},
  doi={10.1109/ICDM.2007.21}}

@inproceedings{gopalakrishnan2020dialog,
  title     = "{Are Neural Open-Domain Dialog Systems Robust to Speech Recognition Errors in the Dialog History? An Empirical Study}",
  author    = {Karthik Gopalakrishnan and Behnam Hedayatnia and Longshaokan Wang and Yang Liu and Dilek Hakkani-Tür},
  year      = {2020},
  booktitle = {Interspeech 2020},
  pages     = {911--915},
  doi       = {10.21437/Interspeech.2020-1508},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/interspeech_2020/gopalakrishnan20_interspeech.html}
}

@inproceedings{sanders2002effect,
  title     = "{Effects of word error rate in the DARPA communicator data during 2000 and 2001}",
  author    = {Gregory A. Sanders and Audrey N. Le and John S. Garofolo},
  year      = {2002},
  booktitle = {7th International Conference on Spoken Language Processing (ICSLP 2002)},
  pages     = {277--280},
  doi       = {10.21437/ICSLP.2002-134},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/icslp_2002/sanders02_icslp.html}
}

@inproceedings{munteanu2006acceptable,
  title     = "{Measuring the acceptable word error rate of machine-generated webcast transcripts}",
  author    = {Cosmin Munteanu and Gerald Penn and Ron Baecker and Elaine Toms and David James},
  year      = {2006},
  booktitle = {Interspeech 2006},
  pages     = {paper 1756-Mon1CaP.2},
  doi       = {10.21437/Interspeech.2006-40},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/interspeech_2006/munteanu06_interspeech.html}
}

@inproceedings{stark2000speechretrieval,
  title     = "{ASR satisficing: the effects of ASR accuracy on speech retrieval}",
  author    = {Litza Stark and Steve Whittaker and Julia Hirschberg},
  year      = {2000},
  booktitle = {6th International Conference on Spoken Language Processing (ICSLP 2000)},
  pages     = {vol. 3, 1069-1072},
  doi       = {10.21437/ICSLP.2000-720},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/icslp_2000/stark00_icslp.html}
}

@inproceedings{favre2013performance,
  title     = "{Automatic human utility evaluation of ASR systems: does WER really predict performance?}",
  author    = {Benoit Favre and Kyla Cheung and Siavash Kazemian and Adam Lee and Yang Liu and Cosmin Munteanu and Ani Nenkova and Dennis Ochei and Gerald Penn and Stephen Tratz and Clare Voss and Frauke Zeller},
  year      = {2013},
  booktitle = {Interspeech 2013},
  pages     = {3463--3467},
  doi       = {10.21437/Interspeech.2013-610},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/interspeech_2013/favre13_interspeech.html}
}

@INPROCEEDINGS{wang2003indicator,
  author={Ye-Yi Wang and Acero, A. and Chelba, C.},
  booktitle={2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)}, 
  title="{Is word error rate a good indicator for spoken language understanding accuracy}", 
  year={2003},
  volume={},
  number={},
  pages={577-582},
  keywords={Error analysis;Natural languages;Speech recognition;Hidden Markov models;Robustness;Training data;Testing;Information systems;Credit cards;Databases},
  doi={10.1109/ASRU.2003.1318504}}

@inproceedings{balagopalan-etal-2020-impact,
    title = "{Impact of {ASR} on {A}lzheimer{'}s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others}",
    author = "Balagopalan, Aparna  and
      Shkaruta, Ksenia  and
      Novikova, Jekaterina",
    editor = "Xu, Wei  and
      Ritter, Alan  and
      Baldwin, Tim  and
      Rahimi, Afshin",
    booktitle = "Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wnut-1.21",
    doi = "10.18653/v1/2020.wnut-1.21",
    pages = "159--164",
    abstract = "Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In this paper, we experiment with controlled amounts of artificially generated ASR errors and investigate their influence on dementia detection. We find that deletion errors affect detection performance the most, due to their impact on the features of syntactic complexity and discourse representation in speech. We show the trend to be generalisable across two different datasets for cognitive impairment detection. As a conclusion, we propose optimising the ASR to reflect a higher penalty for deletion errors in order to improve dementia detection performance.",
}

@inproceedings{min-etal-2021-evaluating,
    title = "{Evaluating Automatic Speech Recognition Quality and Its Impact on Counselor Utterance Coding}",
    author = "Min, Do June  and
      P{\'e}rez-Rosas, Ver{\'o}nica  and
      Mihalcea, Rada",
    editor = "Goharian, Nazli  and
      Resnik, Philip  and
      Yates, Andrew  and
      Ireland, Molly  and
      Niederhoffer, Kate  and
      Resnik, Rebecca",
    booktitle = "Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.clpsych-1.18",
    doi = "10.18653/v1/2021.clpsych-1.18",
    pages = "159--168",
    abstract = "Automatic speech recognition (ASR) is a crucial step in many natural language processing (NLP) applications, as often available data consists mainly of raw speech. Since the result of the ASR step is considered as a meaningful, informative input to later steps in the NLP pipeline, it is important to understand the behavior and failure mode of this step. In this work, we analyze the quality of ASR in the psychotherapy domain, using motivational interviewing conversations between therapists and clients. We conduct domain agnostic and domain-relevant evaluations using standard evaluation metrics and also identify domain-relevant keywords in the ASR output. Moreover, we empirically study the effect of mixing ASR and manual data during the training of a downstream NLP model, and also demonstrate how additional local context can help alleviate the error introduced by noisy ASR transcripts.",
}

@article{Li2024dementia,
title = "{Useful blunders: Can automated speech recognition errors improve downstream dementia classification?}",
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104598},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104598},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000169},
author = {Changye Li and Weizhe Xu and Trevor Cohen and Serguei Pakhomov},
keywords = {Automatic speech recognition, Natural language processing, Dementia, Explainable artificial intelligence},
abstract = {Objectives: We aimed to investigate how errors from automatic speech recognition (ASR) systems affect dementia classification accuracy, specifically in the “Cookie Theft” picture description task. We aimed to assess whether imperfect ASR-generated transcripts could provide valuable information for distinguishing between language samples from cognitively healthy individuals and those with Alzheimer’s disease (AD). Methods: We conducted experiments using various ASR models, refining their transcripts with post-editing techniques. Both these imperfect ASR transcripts and manually transcribed ones were used as inputs for the downstream dementia classification. We conducted comprehensive error analysis to compare model performance and assess ASR-generated transcript effectiveness in dementia classification. Results: Imperfect ASR-generated transcripts surprisingly outperformed manual transcription for distinguishing between individuals with AD and those without in the “Cookie Theft” task. These ASR-based models surpassed the previous state-of-the-art approach, indicating that ASR errors may contain valuable cues related to dementia. The synergy between ASR and classification models improved overall accuracy in dementia classification. Conclusion: Imperfect ASR transcripts effectively capture linguistic anomalies linked to dementia, improving accuracy in classification tasks. This synergy between ASR and classification models underscores ASR’s potential as a valuable tool in assessing cognitive impairment and related clinical applications.}
}

@article{Pentland2023socialscience,
author = {Steven J. Pentland, Christie M. Fuller, Lee A. Spitzley and Douglas P. Twitchell},
title = "{Does accuracy matter? Methodological considerations when using automated speech-to-text for social science research}",
journal = {International Journal of Social Research Methodology},
volume = {26},
number = {6},
pages = {661--677},
year = {2023},
publisher = {Routledge},
doi = {10.1080/13645579.2022.2087849},
URL = {https://doi.org/10.1080/13645579.2022.2087849},
eprint = {https://doi.org/10.1080/13645579.2022.2087849}
}

@inproceedings{li2023gptSLU,
  title     = "{How ChatGPT is Robust for Spoken Language Understanding?}",
  author    = {Guangpeng Li and Lu Chen and Kai Yu},
  year      = {2023},
  booktitle = {INTERSPEECH 2023},
  pages     = {2163--2167},
  doi       = {10.21437/Interspeech.2023-1466},
  issn      = {2958-1796},
}

@inproceedings{zhu-etal-2024-zero,
    title = "{Zero-Shot Spoken Language Understanding via Large Language Models: A Preliminary Study}",
    author = "Zhu, Zhihong  and
      Cheng, Xuxin  and
      An, Hao  and
      Wang, Zhichang  and
      Chen, Dongsheng  and
      Huang, Zhiqi",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1554",
    pages = "17877--17883",
    abstract = "Zero-shot Spoken Language Understanding (SLU) aims to enable task-oriented dialogue systems to understand user needs without training data. Challenging but worthwhile, zero-shot SLU reduces the time and effort that data labeling takes. Recent advancements in large language models (LLMs), such as GPT3.5 and ChatGPT, have shown promising results in zero-shot settings, which motivates us to explore prompt-based methods. In this study, we investigate whether strong SLU models can be constructed by directly prompting LLMs. Specifically, we propose a simple yet effective two-stage framework dubbed GPT-SLU, which transforms the SLU task into a question-answering problem. Powered by multi-stage mutual guided prompts, GPT-SLU can leverage the correlations between two subtasks in SLU to achieve better predictions, which is greatly explored in the traditional fine-tuning paradigm. Experimental results on three SLU benchmark datasets demonstrate the significant potential of LLMs for zero-shot SLU. Comprehensive analyses validate the effectiveness of our proposed framework and also indicate that there is still room for further improvement of LLMs in SLU scenarios.",
}

@misc{feng2022asrglue,
      title="{ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding}", 
      author={Lingyun Feng and Jianwei Yu and Deng Cai and Songxiang Liu and Haitao Zheng and Yan Wang},
      year={2022},
      eprint={2108.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.13048}, 
}

@inproceedings{shon-etal-2023-slue,
    title = "{SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks}",
    author = "Shon, Suwon  and
      Arora, Siddhant  and
      Lin, Chyi-Jiunn  and
      Pasad, Ankita  and
      Wu, Felix  and
      Sharma, Roshan  and
      Wu, Wei-Lun  and
      Lee, Hung-yi  and
      Livescu, Karen  and
      Watanabe, Shinji",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.496",
    doi = "10.18653/v1/2023.acl-long.496",
    pages = "8906--8937",
    abstract = "Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. In order to facilitate the development of SLU models that leverage the success of pre-trained speech representations, we will release a new benchmark suite, including for each task (i) curated annotations for a relatively small fine-tuning set, (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model performance in various types of systems for easy comparisons. We present the details of data collection and annotation and the performance of the baseline models. We also analyze the sensitivity of pipeline models{'} performance to the speech recognition accuracy, using more than 20 publicly availablespeech recognition models.",
}

@inproceedings{lee2018spokenSquad,
  title     = "{Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension}",
  author    = {Chia-Hsuan Lee and Szu-Lin Wu and Chi-Liang Liu and Hung-yi Lee},
  year      = {2018},
  booktitle = {Interspeech 2018},
  pages     = {3459--3463},
  doi       = {10.21437/Interspeech.2018-1714},
  issn      = {2958-1796},
}

@INPROCEEDINGS{shon2022slue,
  author={Shon, Suwon and Pasad, Ankita and Wu, Felix and Brusco, Pablo and Artzi, Yoav and Livescu, Karen and Han, Kyu J.},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title="{SLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Natural Speech}", 
  year={2022},
  volume={},
  number={},
  pages={7927-7931},
  keywords={Training;Sentiment analysis;Annotations;Current measurement;Benchmark testing;Signal processing;Acoustic measurements;spoken language understanding;benchmark;pre-training;named entity recognition;sentiment analysis},
  doi={10.1109/ICASSP43922.2022.9746137}}

@inproceedings{zhong-etal-2021-qmsum,
    title = "{QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization}",
    author = "Zhong, Ming  and
      Yin, Da  and
      Yu, Tao  and
      Zaidi, Ahmad  and
      Mutuma, Mutethia  and
      Jha, Rahul  and
      Awadallah, Ahmed Hassan  and
      Celikyilmaz, Asli  and
      Liu, Yang  and
      Qiu, Xipeng  and
      Radev, Dragomir",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.472",
    doi = "10.18653/v1/2021.naacl-main.472",
    pages = "5905--5921",
    abstract = "Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at \url{https://github.com/Yale-LILY/QMSum}.",
}

@inproceedings{wu-etal-2022-qaconv,
    title = "{QAConv: Question Answering on Informative Conversations}",
    author = "Wu, Chien-Sheng  and
      Madotto, Andrea  and
      Liu, Wenhao  and
      Fung, Pascale  and
      Xiong, Caiming",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.370",
    doi = "10.18653/v1/2022.acl-long.370",
    pages = "5389--5411",
    abstract = "This paper introduces QAConv, a new question answering (QA) dataset that uses conversations as a knowledge source. We focus on informative conversations, including business emails, panel discussions, and work channels. Unlike open-domain and task-oriented dialogues, these conversations are usually long, complex, asynchronous, and involve strong domain knowledge. In total, we collect 34,608 QA pairs from 10,259 selected conversations with both human-written and machine-generated questions. We use a question generator and a dialogue summarizer as auxiliary tools to collect and recommend questions. The dataset has two testing scenarios: chunk mode and full mode, depending on whether the grounded partial conversation is provided or retrieved. Experimental results show that state-of-the-art pretrained QA systems have limited zero-shot performance and tend to predict our questions as unanswerable. Our dataset provides a new training and evaluation testbed to facilitate QA on conversations research.",
}

@inproceedings{shriberg-etal-2004-icsi,
    title = "{The ICSI Meeting Recorder Dialog Act (MRDA) Corpus}",
    author = "Shriberg, Elizabeth  and
      Dhillon, Raj  and
      Bhagat, Sonali  and
      Ang, Jeremy  and
      Carvey, Hannah",
    booktitle = "Proceedings of the 5th {SIG}dial Workshop on Discourse and Dialogue at {HLT}-{NAACL} 2004",
    month = apr # " 30 - " # may # " 1",
    year = "2004",
    address = "Cambridge, Massachusetts, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-2319",
    pages = "97--100",
}

@misc{grattafiori2024llama3herdmodels,
      title="{The Llama 3 Herd of Models}", 
      author={Llama Team, Meta AI},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{jiang2023mistral7b,
      title="{Mistral 7B}", 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{openai2024gpt4ocard,
      title="{GPT-4o System Card}", 
      author={OpenAI},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}

@inproceedings{Waibel1998summ,
    author       = {Waibel, Alex and Bett, Michael and Finke, Michael and Stiefelhagen, Rainer},
    year         = {1998},
    title        = "{Meeting Browser: Tracking And Summarizing Meetings}",
    eventtitle   = {DARPA Broadcast News Transcription and Understanding Workshop},
    eventtitleaddon = {1998},
    eventdate    = {1998-02-08/1998-02-11},
    venue        = {Lansdowne Park, VA, USA},
    booktitle    = {Proceedings of the Broadcast News Transcription and Understanding Workshop, February 8-11, 1998, Lansdowne Conference Resort, Lansdowne, Virginia},
    publisher    = {{Morgan Kaufmann  Publishers}},
    isbn         = {9781558605640},
    language     = {english},
    url = {https://www.ri.cmu.edu/pub_files/pub1/waibel_alex_1998_1/waibel_alex_1998_1.pdf}
}


@article{Roshanzamir2021Alzheimer,
  author={Roshanzamir, Alireza and Aghajan, Hamid and Soleymani Baghshah, Mahdieh},
  journal={BMC Medical Informatics and Decision Making}, 
  title="{Transformer-based deep neural network language models for Alzheimer’s disease risk assessment from targeted speech}", 
  year={2021},
  volume={21},
  number={92},
  keywords={Natural languages;Signal processing;Automatic speech recognition;Mathematical model;Pattern recognition;Speech recognition;Hidden Markov models;Humans;Bridges;Solids},
  doi={10.1186/s12911-021-01456-3},
  url={https://doi.org/10.1186/s12911-021-01456-3}
}

@INPROCEEDINGS{shen2018Tacotron2,
  author={Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and Saurous, Rif A. and Agiomvrgiannakis, Yannis and Wu, Yonghui},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title="{Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions}", 
  year={2018},
  volume={},
  number={},
  pages={4779-4783},
  keywords={Spectrogram;Decoding;Vocoders;Training;Time-domain analysis;Linguistics;Acoustics;Tacotron 2;WaveNet;text-to-speech},
  doi={10.1109/ICASSP.2018.8461368}
}

@misc{betker2023tortoisetts,
      title="{Better speech synthesis through scaling}", 
      author={James Betker},
      year={2023},
      eprint={2305.07243},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2305.07243}, 
}

@misc{werner2023rirgenerator,
  author       = {Nils Werner},
  title        = "{audiolabs/rir-generator: Version 0.2.0}",
  month        = may,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.2.0},
  doi          = {10.5281/zenodo.7963971},
  url          = {https://doi.org/10.5281/zenodo.7963971}
}

@InProceedings{Radford2023whisper,
  title = 	 "{Robust Speech Recognition via Large-Scale Weak Supervision}",
  author =       {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {28492--28518},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/radford23a/radford23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/radford23a.html},
  abstract = 	 {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.}
}

@InProceedings{carletta2006ami,
author="Carletta, Jean and Ashby, Simone and Bourban, Sebastien and Flynn, Mike and Guillemot, Mael and Hain, Thomas and Kadlec, Jaroslav and Karaiskos, Vasilis and Kraaij, Wessel and Kronenthal, Melissa and Lathoud, Guillaume and Lincoln, Mike and Lisowska, Agnes and McCowan, Iain and Post, Wilfried and Reidsma, Dennis and Wellner, Pierre",
editor="Renals, Steve and Bengio, Samy",
title="{The AMI Meeting Corpus: A Pre-announcement}",
booktitle="Machine Learning for Multimodal Interaction",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="28--39",
abstract="The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.",
isbn="978-3-540-32550-5"
}

@INPROCEEDINGS{janin2003icsi,
  author={Janin, A. and Baron, D. and Edwards, J. and Ellis, D. and Gelbart, D. and Morgan, N. and Peskin, B. and Pfau, T. and Shriberg, E. and Stolcke, A. and Wooters, C.},
  booktitle={2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).}, 
  title="{The ICSI Meeting Corpus}", 
  year={2003},
  volume={1},
  number={},
  pages={I-I},
  keywords={Speech recognition;Speech processing;Audio recording;Microphones},
  doi={10.1109/ICASSP.2003.1198793}}



@misc{myNoise2020office,
  author       = {myNoise},
  title        = "{OFFICE NOISES • When working from home feels too quiet!}",
  year         = {2020},
  howpublished = {YouTube video},
  url          = {https://www.youtube.com/watch?v=8tKpjMh_OUw&t=4442s},
  note         = {Accessed: 2024-12-09}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE: A Package for Automatic Evaluation of Summaries}",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}


@inproceedings{qin-etal-2024-large,
    title = "{Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting}",
    author = "Qin, Zhen  and
      Jagerman, Rolf  and
      Hui, Kai  and
      Zhuang, Honglei  and
      Wu, Junru  and
      Yan, Le  and
      Shen, Jiaming  and
      Liu, Tianqi  and
      Liu, Jialu  and
      Metzler, Donald  and
      Wang, Xuanhui  and
      Bendersky, Michael",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.97",
    doi = "10.18653/v1/2024.findings-naacl.97",
    pages = "1504--1518",
    abstract = "Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets.We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP).Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019{\&}2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10{\%} for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2{\%} and pointwise LLM-based solutions by more than 10{\%} on average NDCG@10.Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.",
}

@inproceedings{miah-etal-2023-hierarchical,
    title = "{Hierarchical Fusion for Online Multimodal Dialog Act Classification}",
    author = "Miah, Md Messal Monem  and
      Pyarelal, Adarsh  and
      Huang, Ruihong",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.505",
    doi = "10.18653/v1/2023.findings-emnlp.505",
    pages = "7532--7545",
    abstract = "We propose a framework for online multimodal dialog act (DA) classification based on raw audio and ASR-generated transcriptions of current and past utterances. Existing multimodal DA classification approaches are limited by ineffective audio modeling and late-stage fusion. We showcase significant improvements in multimodal DA classification by integrating modalities at a more granular level and incorporating recent advancements in large language and audio models for audio feature extraction. We further investigate the effectiveness of self-attention and cross-attention mechanisms in modeling utterances and dialogs for DA classification. We achieve a substantial increase of 3 percentage points in the F1 score relative to current state-of-the-art models on two prominent DA classification datasets, MRDA and EMOTyDA.",
}

@techreport{dhillon2004mrdaLabeling,
  title="{Meeting recorder project: Dialog act labeling guide}",
  author={Dhillon, Rajdip and Bhagat, Sonali and Carvey, Hannah and Shriberg, Elizabeth},
  year={2004},
  institution={Citeseer},
  url={https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0125f51024dbf99de8daf6c5cf38b3edb78e7f71}
}

@inproceedings{jo-etal-2017-modeling,
    title = "{Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences}",
    author = "Jo, Yohan  and
      Yoder, Michael  and
      Jang, Hyeju  and
      Ros{\'e}, Carolyn",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1232",
    doi = "10.18653/v1/D17-1232",
    pages = "2179--2189",
    abstract = "We present an unsupervised model of dialogue act sequences in conversation. By modeling topical themes as transitioning more slowly than dialogue acts in conversation, our model de-emphasizes content-related words in order to focus on conversational function words that signal dialogue acts. We also incorporate speaker tendencies to use some acts more than others as an additional predictor of dialogue act prevalence beyond temporal dependencies. According to the evaluation presented on two dissimilar corpora, the CNET forum and NPS Chat corpus, the effectiveness of each modeling assumption is found to vary depending on characteristics of the data. De-emphasizing content-related words yields improvement on the CNET corpus, while utilizing speaker tendencies is advantageous on the NPS corpus. The components of our model complement one another to achieve robust performance on both corpora and outperform state-of-the-art baseline models.",
}

@Inbook{OShea2012daFunction,
author="O'Shea, James
and Bandar, Zuhair
and Crockett, Keeley",
editor="Nguyen, Ngoc Thanh",
title="{A Multi-classifier Approach to Dialogue Act Classification Using Function Words}",
bookTitle="Transactions on Computational Collective Intelligence VII",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="119--143",
abstract="This paper extends a novel technique for the classification of sentences as Dialogue Acts, based on structural information contained in function words. Initial experiments on classifying questions in the presence of a mix of straightforward and ``difficult'' non-questions yielded promising results, with classification accuracy approaching 90{\%}. However, this initial dataset does not fully represent the various permutations of natural language in which sentences may occur. Also, a higher Classification Accuracy is desirable for real-world applications. Following an analysis of categorisation of sentences, we present a series of experiments that show improved performance over the initial experiment and promising performance for categorising more complex combinations in the future.",
isbn="978-3-642-32066-8",
doi="10.1007/978-3-642-32066-8_6",
url="https://doi.org/10.1007/978-3-642-32066-8_6"
}

@ARTICLE{Prabhavalkar2023surveyASR,
  author={Prabhavalkar, Rohit and Hori, Takaaki and Sainath, Tara N. and Schlüter, Ralf and Watanabe, Shinji},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title="{End-to-End Speech Recognition: A Survey}", 
  year={2023},
  volume={32},
  number={},
  pages={325-351},
  keywords={Hidden Markov models;Training;Data models;Acoustics;Task analysis;Deep learning;Decoding;End-to-end;automatic speech recognition},
  doi={10.1109/TASLP.2023.3328283}}

@inproceedings{iwamoto2022artifacts,
  title     = {How bad are artifacts?: Analyzing the impact of speech enhancement errors on ASR},
  author    = {Kazuma Iwamoto and Tsubasa Ochiai and Marc Delcroix and Rintaro Ikeshita and Hiroshi Sato and Shoko Araki and Shigeru Katagiri},
  year      = {2022},
  booktitle = {Interspeech 2022},
  pages     = {5418--5422},
  doi       = {10.21437/Interspeech.2022-318},
  issn      = {2958-1796},
}

@inproceedings{kubis-etal-2023-back,
    title = "{Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors}",
    author = "Kubis, Marek  and
      Sk{\'o}rzewski, Pawe{\l}  and
      Sowa{\'n}ski, Marcin  and
      Zietkiewicz, Tomasz",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.724",
    doi = "10.18653/v1/2023.emnlp-main.724",
    pages = "11824--11835",
    abstract = "In a spoken dialogue system, an NLU model is preceded by a speech recognition system that can deteriorate the performance of natural language understanding. This paper proposes a method for investigating the impact of speech recognition errors on the performance of natural language understanding models. The proposed method combines the back transcription procedure with a fine-grained technique for categorizing the errors that affect the performance of NLU models. The method relies on the usage of synthesized speech for NLU evaluation. We show that the use of synthesized speech in place of audio recording does not change the outcomes of the presented technique in a significant way.",
}

@unpublished{spacy2017,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = "{spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing}",
    YEAR   = {2017},
    Note   = {To appear}
}

@misc{wolf2020huggingfaces,
      title="{HuggingFace's Transformers: State-of-the-art Natural Language Processing}", 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.03771}, 
}

@inproceedings{liu-etal-2024-benchmarking,
    title = "{Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization}",
    author = "Liu, Yixin  and
      Fabbri, Alexander  and
      Chen, Jiawen  and
      Zhao, Yilun  and
      Han, Simeng  and
      Joty, Shafiq  and
      Liu, Pengfei  and
      Radev, Dragomir  and
      Wu, Chien-Sheng  and
      Cohan, Arman",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.280/",
    doi = "10.18653/v1/2024.findings-naacl.280",
    pages = "4481--4501",
    abstract = "While large language models (LLMs) can already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluations of five LLM-based systems to assess their instruction-following capabilities in controllable summarization. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation capabilities. We make our collected benchmark InstruSum publicly available to facilitate future research in this direction."
}

@ARTICLE{wang2018speechsep,
  author={Wang, DeLiang and Chen, Jitong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Supervised Speech Separation Based on Deep Learning: An Overview}, 
  year={2018},
  volume={26},
  number={10},
  pages={1702-1726},
  keywords={Speech enhancement;Interference;Noise measurement;Training;Supervised learning;Task analysis;Seech separation;speaker separation;speech enhancement;supervised speech separation;deep learning;deep neural networks;speech dereverberation;time-frequency masking;array separation;beamforming},
  doi={10.1109/TASLP.2018.2842159}}
