\section{Results and Analyses}
\label{sec_results}

\input{figures/noclean/noclean_main}

%\input{tables/scores_noclean}

Our experiments include the configurations described in Section \ref{sec_setup}, with results discussed here.

\paragraph{Comparing task models.}
The following analyses reveal how task models perform under varying levels of noise. The AUC and NTP scores provide a high-level comparison for assessing \ENDow{}, while the graph curves illustrate model behavior as noise fluctuates.

\autoref{fig_noclean_graphs}
%and \autoref{tab_scores_noclean} present 
presents the results on the three downstream tasks for the four models, based on one of the evaluation metrics per task.\footnote{Graphs based on the other metrics are in \autoref{fig_noclean_graphs_all} in the Appendix. The analysis here is for demonstration purposes; additional insights could be gathered from the other graphs.} 
In the summarization task results (\autoref{fig_noclean_graphs}a) we first notice that models tend to tolerate a noise level of about 0.2 WER (NTP between 0.07 and 0.3), i.e., task scores are not significantly lower ($p<0.05$) until that level of noise. Also, while the AUC scores for all models are not significantly different ($p<0.05$), models behave differently with respect to WER. For example, GPT's summaries are more highly preferred at lower WER than at higher WER values, while for Mistral, the preference is slightly more evenly distributed.
%Observing the results of the summarization task (\autoref{fig_noclean_graphs}a), we first notice that the models roughly tend to tolerate a noise level of about 0.2 WER (with noise-toleration points ranging between 0.07 and 0.3), meaning that until that level of noise, the task results do not significantly worsen. Also, while the AUC scores of all four models are not significantly different, we find that the models behave differently with respect to WER. For example, GPT's summaries are more highly preferred at lower WER than at higher WER values, while for Mistral, the preference is slightly more evenly distributed. 

In the question-answering task (\autoref{fig_noclean_graphs}b), the more advanced models (GPT and Llama-3.1) yield substantially better results than the other two models. This could be an effect of the small context window which requires conducting the task in segments, likely inducing more errors. 
%\todo{check if this can be made more convincing.}
Similar to the behavior in summarization, here too GPT yields better scores than Llama 3.1 at low WER, but Llama 3.1 surpasses GPT as WER increases.

In the dialog-act classification task (\autoref{fig_noclean_graphs}c), the noise-toleration points are quite high due to the large margins-of-error and relatively flat curves. A high NTP either implies that noise has little effect on a downstream task, or alternatively that the model is ineffective for the task in general.
%even when noise is negligible.
In this case, the latter seems to be the case, when comparing to results of \citet{miah-etal-2023-hierarchical} ($0.29$ macro-$F_1$ and $0.6$ accuracy vs. $[0.17, 0.32]$ macro-$F_1$ and $[0.3, 0.48]$ accuracy here). More in Appendix \ref{sec_appendix_results}.
%On an even more fine-grained label set (53 vs. 12 labels), their model delivers a macro-$F_1$ of 0.29 and an accuracy of 0.60 on data from the same dataset -- higher than the scores here (0.17 to 0.32 $F_1$ and 0.3 to 0.48 accuracy). See \autoref{fig_noclean_graphs_all} in the Appendix for the rest of the graphs.

%Overall, these analyses facilitate the unveiling of many insights about the capabilities of models in SLU pipelines with transcript noise. The AUC and NTP scores offer a high-level comparison of models for assessing \ENDow{}, while the graph curves display the behavioral patterns of the models at different noise levels.

\input{figures/cleaning/cleaning_main}

\input{tables/scores_cleaning}


\paragraph{Comparing noise types.}
The following analyses highlight the impact of different \textit{types} and intensities of noise. This examination enables more efficient optimization of ASR systems and post-hoc transcript repair. For instance, as the analysis will show, prioritizing named-entity accuracy may be more effective than generally minimizing the WER of the ASR system.

\autoref{fig_cleaning_graphs} presents the graphs showing the effect of cleaning techniques on the performance of GPT (graphs for the rest of the models and cleaning techniques in Figures \ref{fig_cleaning_graphs_all_qmsum}, \ref{fig_cleaning_graphs_all_qaconv} and \ref{fig_cleaning_graphs_all_mrda} in the Appendix). The ``no cleaning'' curve shows model results on the transcripts at the various noise levels. The other curves on the graph show model results when also applying a cleaning technique on the same transcripts.

In the summarization task (\autoref{fig_cleaning_graphs}a), fixing all the content words in the transcripts (``content'' curve) helps the model produce summaries that are preferred over all the summaries that are based on the original transcripts, regardless of noise level. However, due to this technique's costly ``effort'' (high change in WER), the cleaning-effectiveness score (0.479) is not as high as that of the ``named entities'' technique (0.499). The latter cleaning method improves the task scores at a smaller cost of effort on average, as depicted in the graph. These findings indicate the value of content words, and named entities in particular, for the summarization task using GPT. Notice that transcripts that are almost fully error-prone (WER is $\sim$0.9) are fixed to a WER of $\sim$0.4 by repairing content words, but resulting summaries are much preferred over summaries from transcripts with different types of errors, also at a WER of 0.4. This further stresses the importance of analyzing the \textit{types} of errors in transcripts and not just the \textit{amount}, which is a known limitation of the WER metric.


\autoref{tab_scores_cleaning} lists the CES scores of each model and cleaning technique, ranked by the effectiveness on GPT.
%(brighter is more highly ranked).
For the QA task, we see that the cleaning techniques are ranked quite similarly across the four task-models. In all three tasks, fixing only adverbs or verbs is less effective. On the other end, nouns, and named-entities particularly, are more effective for the transcript-level tasks (summarization and QA). The utterance-level task of dialog-act classification behaves differently with respect to the repaired word-types. Interestingly, repairing non-content words is effective for the task, consistent with works that found that function words are essential features for classifying dialog-acts \citep{OShea2012daFunction, jo-etal-2017-modeling}.
Named-entities are also effective for the task with GPT. A closer look into the graph (\autoref{fig_cleaning_graphs}c) reveals that the high CES is affected by the behavior at lower WER scores, where a strong increase in the task score is obtained at a small effort.
%Additionally, named-entities seem to be effective for dialog-act classification with GPT, according to the CES score, a closer look into the graph (\autoref{fig_cleaning_graphs}(c)) reveals that the high CES is affected by the effectiveness at lower WER scores. In that area, there is strong increase in the task score for only a small effort.
%element-wise scores at \textit{lower} WER rates are very high (where there is strong increase in the task score, for only a small effort). 
%This phenomenon calls for further investigation as to the effect of word types at \textit{different levels} of noise, and for different models.

%These analyses emphasize the influence of different types and intensity of noise, enabling efficient optimization of ASR systems and post-hoc transcript repairing. For instance, instead of attempting to push down WER in general, it may be more efficient to target correctness of named-entities, and settle for a WER score that is satisfactory for the required downstream task.
%These analyses provide insights that guide effective implementation of SLU pipelines, and emphasize the influence of different types of noise. ASR systems and post-hoc transcript repairing can be optimized accordingly as well. Importantly, the cleaning techniques and all components of the SLU pipeline, are flexibly configurable to support any desired analysis.

%\input{tables/scores_cleaning}



%- Also put one graph of a cleaning technique with all four models, to see how models react differently to cleaning methods.

%- discuss the transcript-level vs. utterance-level task. Show the correlation between grammaticality and correctness of the results in MRDA.