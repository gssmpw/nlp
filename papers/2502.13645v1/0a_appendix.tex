\appendix

\section{Implementation Details}
\label{sec_appendix_implementation}

\subsection{Computing Overall WER on a Set of Transcripts}
\label{sec_appendix_implementation_wer}

Before computing WER, the utterances in a transcript are tokenized using \texttt{spaCy} \citep{spacy2017}, and then the tokens are recombined with separating spaces. This is done mainly to separate punctuation and contractions.

The WER score for a transcript is computed against the reference transcript using the \texttt{jiwer.process\_words} function. It computes hits, insertions, substitutions and deletions between each predicted and respective reference utterance. Then all of the utterance level values are added up, and a single WER score is computed accordingly for the transcript.

Once the transcript-level WER scores are computed, the transcript-set WER score is the average over all transcripts in the set.


\subsection{Computing the Noise-toleration Point (NTP)}
\label{sec_appendix_implementation_ntp}

Observe \autoref{fig_framework_graph} for a visualization of the following explanation.
The computation of the noise-toleration point of a curve $l_j$ relies on $l_j^{\text{upper}}$ and $l_j^{\text{lower}}$, the lines with the respective \textbf{margins-of-error}. The y-value, i.e., task score, of each point in $l_j^{\text{upper}}$ ($l_j^{\text{lower}}$) is the upper (lower) limit margin-of-error for the corresponding point's y-value in $l_j$. Specifically, task score $s_{i_j}$, as part of point $(w_{i_j}, s_{i_j})$ on $l_j$, is the average task score over the transcripts in the set $\widehat{T_{i_j}}$, and the respective margin-of-error is computed at a confidence level of $95\%$ with the formula $1.96*\sigma/\sqrt{n}$, where $\sigma$ is the standard deviation of the scores, and $n$ is the number of scores (number of transcripts). The x-values on $l_j^{\text{upper}}$ and $l_j^{\text{lower}}$ are kept the same as in the respective points on $l_j$. With these margins-of-error line, we can compute the NTP.

Since a model's curve $l_j$ is constructed of several discrete points, we find the first point $p_i:=(w_{i_j}, s_{i_j})$ in $l_j$ whose upper bound (based on the margin-of-error) is above the lower bound of the first point $p_0$, and where the next point $p_{i+1}$ has an upper bound that is below $p_0$'s lower bound. Then on the linear segment between $p_i$ and $p_{i+1}$, we find the $x$ value (WER score) where its upper bound is equal to the lower bound of $p_0$.

\input{figures/graph_illustration}


\subsection{Executing Text-to-speech}
\label{sec_appendix_implementation_tts}

To create the initial audio files from the dataset transcripts, we first removed substrings in the utterances that are within curly and box brackets. These are used in some transcripts to indicate non-verbal markers. We also removed redundant whitespaces. An utterance was sentence-tokenized (with \texttt{nltk.tokenize.sent\_tokenize}), and then also broken up to segments of up to 50 tokens, if it was longer than that (with \texttt{nltk.tokenize.word\_tokenize}). We found that in some cases the TTS module had some difficulty in voicing more than 50 tokens at a time (more than about 15 seconds of speech). Each segment was then passed to \texttt{tortoise.api.TextToSpeech} and an audio file was created with the ``emma'' voice in ``ultra\_fast'' mode, and saved with a 24000 sample frequency.


\subsection{Impairing Audio Files}
\label{sec_appendix_implementation_noising}

An audio file is recreated $k$ times at increasing levels of speech deterioration. First the audio file is reverberated once using the Room Impulse Response Generator \citep[\texttt{rir\_generator} library;][]{werner2023rirgenerator}. Then background sounds are added at $k$ different signal-to-noise ratios, as described below.

The reverberation parameters are as follows. The \textbf{room dimensions} are uniformly selected for each of width (2 to 10 meters), length (2 to 10 meters) and height (exactly 3 meters). Assuming that the room is enclosed by walls, a floor and a ceiling, the \textbf{speaker position} is uniformly selected for each of x-position (somewhere 0.5 meters from the wall), y-position (somewhere 0.5 meters from the wall), and z-position (somewhere between the floor and the ceiling). The \textbf{microphone location} is randomly placed 2 meters away from the speaker if it's within the bounds of the room, otherwise 1 meter away, otherwise 0 meters away. The \textbf{Reverberation Time} (RT60 -- the time it takes for sound energy to decrease by 60 dB after the sound source stops) is uniformly selected between 0.15 and 1 second. \textbf{Sound velocity} is kept at the default value of 340 meters per second. The \textbf{sample frequency} is kept at the original value (24000 samples per second).

To the resulting reverberated audio file denoted \texttt{signal}, \textbf{background sounds} are added at $k$ different signal-to-noise ratios (SNR -- level of a desired signal to the level of background noise). First a sound audio file \citep{myNoise2020office} denoted \texttt{noise\_signal} (in our case we used an office background that includes realistic sounds such as chatter, papers, office machinery, drinking, etc.) is loaded, and repeated so that its length is equal to that of the speech audio file, or truncated to that length. The resulting background file is denoted \texttt{white\_noise}. Then the noise factor is computed according to the SNR with:
\[
g = \sqrt{\frac{10^{-\texttt{SNR}/10} * \text{std}(\texttt{signal})^2}{\epsilon + \text{std}(\texttt{noise\_signal})^2}}
\]
The final audio file is created as:
\[
\texttt{noisy\_signal} = \texttt{signal} + g * \texttt{white\_noise}
\]
The $k$ SNR values that we use in our experiments are -10, -5, 0, 5, 10. The higher the value, the more distinctive the speech is over the background noise.


\subsection{Executing ASR for Speech-to-text}
\label{sec_appendix_implementation_stt}

To run Whisper on a transcript, a Huggingface \citep{wolf2020huggingfaces} automatic-speech-recognition pipeline is initialized with the \texttt{openai/whisper-small.en} model. The pipeline receives each audio file and generates the respective text. Since our audio files are up to about 15 seconds in length and mostly under 1MB in size, the model is able to handle the files properly.


\subsection{Cleaning Transcripts}
\label{sec_appendix_implementation_cleaning}

Our cleaning techniques rely on the reference transcripts, and therefore are used to inidicate how different \textit{types} of errors effect the behavior of a model on a downstream task.

Given a chunk of 20 utterances from the predicted transcript, and the respective chunk from the reference transcript, \texttt{spaCy} is used to tag the part-of-speech labels of each token, and the named entity chunks. Then \texttt{jiwer.process\_words} is used to align the texts. For each alignment, if there is a substitution, addition or deletion, and it involves a type (POS or named entity) that is being cleaned, then that alignment is fixed.

As mentioned, we separately clean each of: nouns, verbs, adjectives, adverbs, all the above (content words), none of the above (non-content words) and named entities.

An example for cleaning nouns (in bold):

\noindent
Reference: \textit{``We certainly see it, as employers. The penny drops after a few weeks or months.''}

\noindent
Noisy: \textit{``We certainly seen it as \textbf{lawyers}. The penny drops after the new \textbf{songs}.''}

\noindent
Cleaned: \textit{``We certainly seen it as \textbf{employers} . The penny drops after the new \textbf{weeks months}.''}


\subsection{Pairwise Ranking for Summarization Evaluation}
\label{sec_appendix_implementation_pairwise}

Pairwise comparison has been shown to be highly effective for judging the overall quality of summaries \citep{liu-etal-2024-benchmarking} using \texttt{gpt-4-0314} as a judge. We use a presumably more advanced model (\texttt{gpt-4o-mini}), and assume high reliability.

Given two competing summaries, the reference summary, and an optional query on which the summary is focused, a pairwise comparer needs to mark the preferred summary (in our case, for ``general quality''). The two summaries being compared are presented to the comparer in random order to remove an order bias with regard to the preference made. We use a method inspired by the LLMCompare protocol from \citet{liu-etal-2024-benchmarking}.

For a generic summary we input the following prompt to \texttt{gpt-4o-mini}:

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, sharp corners, boxrule=0.5mm]
\small{
\texttt{You will be given a generic reference summary of a conversation, as well as two summaries written by automatic systems. Your task is to decide which of the two system summaries is better, with respect to the reference summary. If it is difficult to decide which summary has better overall quality, then you may say that there is a tie. \\
First explain briefly the reasoning for your choice, and then provide an answer as 1, 2 or tie. \\
The output should be in the following format:\\
Explanation: <your reasoning>\\
Response: <1, 2 or tie>\\\\
Reference summary: \{summ\_ref\}\\
System 1 summary: \{summ\_1\}\\
System 2 summary: \{summ\_2\}}}
\end{tcolorbox}

For a query-focused summary we input the following prompt to \texttt{gpt-4o-mini}:

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, sharp corners, boxrule=0.5mm]
\texttt{You will be given a query-focused reference summary of a conversation, as well as two summaries written by automatic systems. Your task is to decide which of the two system summaries is better, with respect to the reference summary and the query. If it is difficult to decide which summary has better overall quality, then you may say that there is a tie. \\
First explain briefly the reasoning for your choice, and then provide an answer as 1, 2 or tie. \\
The output should be in the following format:\\
Explanation: <your reasoning>\\
Response: <1, 2 or tie>\\\\
Query: \{query\} \\
Reference summary: \{summ\_ref\}\\
System 1 summary: \{summ\_1\}\\
System 2 summary: \{summ\_2\}}
\end{tcolorbox}

\paragraph{Pairwise ranking for non-cleaned versions.}
A non-cleaned transcript-set is prepared at seven levels of noise (no noise, and six levels of increasing intensity). These are marked as $T$ (the reference transcript set) and $\{\widehat{T_{i_0}}\}_{i=0}^k$ (noisy versions) respectively. The resulting summary sets are denoted $\widehat{O}$ (on the reference transcripts $T$) and $\{\widehat{O_{i_0}}\}_{i=0}^k$. Then the summaries for instance $z$, i.e., $\widehat{o_{i_0}^z} \in \widehat{O_{i_0}}$ and $\widehat{o^z} \in \widehat{O}$ (seven summaries total) are compared in pairs through the pairwise compairson LLM annotation. In all there are 21 pairs, and the preferred summary receives 2 points, or 1 point for a tie. Hence, a summary that is preferred over all other six summaries can score a maximum of 12 points, and a total of 42 points are distributed amongst the seven summaries. The scores for each noise version are averaged over all instances of the test set (281 instances), and margin-of-errors are computed. These scores are then plotted on the curve, e.g., as in \autoref{fig_noclean_graphs}a.

Using the Azure OpenAI API,\footnote{\url{https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/}} the cost for computing all comparisons for one model was about \$1.50. For the four models this process totalled about \$6.

\paragraph{Pairwise ranking for cleaning techniques.}
For a cleaning technique $j$, a summary $\widehat{o_{i_j}^z} \in \widehat{O_{i_j}}$ is instead compared to summaries $\widehat{o_{i_0}^z} \in \widehat{O_{i_0}}$ and $\widehat{o^z} \in \widehat{O}$. In this case, we want to assess how the cleaned summary compares against the non-cleaned summaries. Each of the six cleaned summaries is compared against seven non-cleaned summaries, for a total of 42 comparisons per instance. A summary can score up to 14 points, and 84 points are distributed amongst the compared summaries. The scores for the cleaned summaries are avarged over all instances of the test set (281 instances), and margin-of-errors are computed. These scores are then plotted on the curve, e.g., as in \autoref{fig_cleaning_graphs}a. In this graph, the non-cleaned line is reused from the non-cleaned pairwise ranking from before, except that it is shifted up one point in the y-axis. This is done as if to emulate the same procedure done here where the non-cleaned summaries should be compared to \textit{all} the non-cleaned summaries (including itself), and would hence receive an additional point for the tie of a summary against itself.

Using the Azure OpenAI API, the cost for computing all comparisons for one model and one cleaning method was about \$4.30. For the four models and seven cleaning techniques, this amounted to about \$120.



% \section{Examples of Task Dataset Instances}
% \label{sec_appendix_datasets}

% \subsection{Summarization with QMSum}
% \label{sec_appendix_datasets_qmsum}

% \subsection{Question-answering with QAConv}
% \label{sec_appendix_datasets_qaconv}

% \subsection{Dialog-act Classification with MRDA}
% \label{sec_appendix_datasets_mrda}



\section{Executing Task Models}
\label{sec_appendix_prompts}

The four LLMs in our experiments were executed in zero-shot mode. \texttt{mistral-7b}, \texttt{llama-3-8b} and \texttt{llama-3.1-8b} were run on a local server. \texttt{gpt-4o-mini} was run with the Azure OpenAI API.

The summarization and question-answering tasks are given a transcript in the input. The transcripts and therefore chunked to fit (with the instruction) the context window of the employed model. For \texttt{mistral-7b} and \texttt{llama-3-8b} the size is 8K, for \texttt{llama-3.1-8b} and \texttt{gpt-4o-mini} it is 128K. The latter two models do not require chunking for our tested data. The prompts were scripted with some light prompt engineering on a few instances.

\subsection{Summarization with QMSum}
\label{sec_appendix_prompts_qmsum}

The prompt for summarizing the full transcript is:

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, sharp corners, boxrule=0.5mm]
\texttt{Given the following conversation, answer the question: \{query\}\\
The conversation is:\\
\{transcript\}}
\end{tcolorbox}

\noindent
The prompt for summarizing the transcript in segments is:

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, sharp corners, boxrule=0.5mm]
\texttt{Given the following portion of a conversation, answer the question: \{query\}\\
The portion of the conversation is:\\
\{transcript\}}
\end{tcolorbox}

\noindent
and the segment summaries are then summarized into one final summary with:

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, sharp corners, boxrule=0.5mm]
\texttt{The following is an ordered list of answers collected from portions of a conversation for the question: \{query\}\\
Generate a final answer for the question by aggregating the answers from the different conversation portions. Be succinct, and write it as a standalone answer without referring to the list of existing answers. The answers are:\\
Answer 1: \{answers[0]\}\\
Answer 2: \{answers[1]\}\\
...
}
\end{tcolorbox}

Notice that the prompt is phrased as if the query is a question and the summary is an answer. This layout is used to adhere to QMSum's format. The query for all the generic summaries is \textit{``Summarize the meeting''}. An example query for a query-focused summary is \textit{``What was the next step on features?''} or \textit{``Summarize what was said on intentionality''}.

The approximate cost for inferring on the QMSum test set with \texttt{gpt-4o-mini} was \$0.30.


\subsection{Question-answering with QAConv}
\label{sec_appendix_prompts_qaconv}

For question-answering, the following prompt is input to the LLM:

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, sharp corners, boxrule=0.5mm]
\texttt{You will be given a conversation and some questions, and you need to answer the questions based on the conversation.\\
Each answer should be a very short span copied from the conversation, and written as a brief direct answer, and not as a sentence. Do not add any explanation or extra wording.\\
For example, for a question such as ``Where is John from?'', the answer could be ``New York'' but not ``John is from New York''.\\
If a question cannot be answered according to the conversation, answer with ``unanswerable'' only, without any explanation or extra wording.\\
Answer the questions line by line in the same order as the questions, without repeating the questions.\\\\
The conversation is:\\\{transcript\}\\\\
The questions are:\\
\{questions[0]\}\\
\{questions[1]\}\\
...}
\end{tcolorbox}

The transcript is chunked to fit within the context window of the LLM. Therefore, after answering the questions a chunk at a time, the final answer for a question is the shortest answer that is not ``unanswerable''. The default answer is ``unanswerable'' if no other answer is available.

The approximate cost for inferring on the QAConv test set with \texttt{gpt-4o-mini} was \$0.25.


\subsection{Dialog-act Classification with MRDA}
\label{sec_appendix_prompts_mrda}

The prompt for classifying dialog acts is:

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, sharp corners, boxrule=0.5mm]
\small{
\texttt{Given an utterance from a conversation, choose a label that best describes the utterance.\\
The possible labels with their definitions are:\\
Floor Holder - the utterance occurs mid-speech and used by a speaker as a means to pause and continue holding the floor\\
Floor Grabber - an utterance in which a speaker has not been speaking and wants to gain the floor so that he may commence speaking\\
Hold Before Answer - an utterance that is used when a speaker who is given the floor and is expected to speak holds off prior to making an utterance\\
Agreement - an utterance used to exhibit agreement to or acceptance of a previous speaker's question, proposal, or statement\\
Yes-No-question - the utterance is in the form of a yes/no questions\\
Wh-Question - the utterance is a question that require a specific answer\\
Or-Clause - the utterance is an ``or'' clause, likely following a yes/no question\\
Or Question - the utterance offers the listener at least two answers or options from which to choose\\
Open-ended Question - the utterance is an open-ended question that places few syntactic or semantic constraints on the form of the answer it elicits\\
Rhetorical Question - the utterance states a question to which no answer is expected\\
Abandoned/Interrupted - an incomplete utterance in which a speaker stops talking intentionally or on account of being interrupted by another speaker\\
Uninterpretable - the utterance is not clear or has indecipherable speech\\
Continuer - the utterance is made in the background and simply indicate that a listener is following along or at least is yielding the illusion that he is paying attention\\
Statement - the utterance is none of the above types\\\\
The utterance is:\\
\{transcript\_utterance\}\\\\
The output should be in the format:\\
label: <the label>}}
\end{tcolorbox}

The approximate cost for inferring on the MRDA test set with \texttt{gpt-4o-mini} was \$0.10.



\section{Compute and Hardware}
\label{sec_appendix_compute}
For the components of the pipeline that require it, we use a single Nvidia A100 GPU with 40GB memory. This is needed for running Tortoise TTS, Whisper STT and for running open-source task models (Mistral and Llama models). The rest of the components run on an Apple M1 Macbook.

\section{More Results from Experiments}
\label{sec_appendix_results}
The graphs presented in Section \ref{sec_results} only show results when using pairwise comparison for summarization evaluation, fuzzy match for QA evaluation, and macro-$F_1$ for dialog act classification evaluation. Here we present the results also for the rest of the evaluation metrics for the three tasks.

\paragraph{Comparing task models.}
\autoref{fig_noclean_graphs_all} presents the graphs based on the rest of the task evaluation metrics, including those already presented in \autoref{fig_noclean_graphs}. \autoref{tab_scores_noclean_all} places the AUC and noise-toleration point scores for each curve in a single table for readability. The highest AUC score in each row is in bold, to show the task model achieving the best result. Significance can be inferred with the margin-of-errors.

For summarization, we find that the AUC scores do not differ significantly across models in all metrics, with the exception of Llama-3.1 having the highest AUC when using ROUGE-2 as the metric. The ROUGE metrics in general produce large margin-of-errors, causing comparison between models to be more vague. We do however notice that Llama-3.1 yields a larger difference in results as WER increases, when using all summarization metrics. We also see a consistent trend in Llama-3 where there seems to be a sudden drop and gain around a WER of 0.2.

The three metrics used for QAConv are quite consistent. For MRDA, we find that the accuracy metric raises the ranking of the Mistral model, since Mistral was always more likely to output a label of a prevalent class, stressing the advantage of the macro-$F_1$ metric that balances the importance of the classes.

For MRDA, we compare in Section \ref{sec_results} results to those in \citet{miah-etal-2023-hierarchical}.
They use an even more fine-grained label set (53 vs. 12 labels), yet still produce better scores overall. This further strengthens our presupposition that the models in our experiments are not as effective on the task, resulting in high noise-toleration-points on the curves.

\paragraph{Comparing noise types.}
Figures \ref{fig_cleaning_graphs_all_qmsum}, \ref{fig_cleaning_graphs_all_qaconv} and \ref{fig_cleaning_graphs_all_mrda} show the graphs based on the rest of the task models, evaluation metrics, and cleaning techniques, including those in \autoref{fig_cleaning_graphs}. \autoref{tab_scores_cleaning_all} shows all the cleaning effectiveness scores accordingly, similar to \autoref{tab_scores_cleaning}, but with the cleaning techniques kept in consistent order.

When looking at the cleaning-effectiveness scores, we find that ROUGE-2 ranks the cleaning techniques closest to the way that pairwise ranking does. In QAConv, the $F_1$ metric ranks the cleaning techniques very similarly for all four models. With the exception of Llama-3, the rankings are similar also with the exact and fuzzy matching metrics as well. For MRDA, although the score values are quite different, the rankings are close when using the two evaluation metrics.

As discussed in Section \ref{sec_results}, generally the named-entities and nouns are most helpful for the summarization and question-answering tasks. For GPT, this is also the case for dialog-act classification, however with the open-source models, the non-content words are most helpful for the task.

\input{tables/scores_noclean_all}

\input{tables/scores_cleaning_all}

\input{figures/noclean/noclean_main_all}

\input{figures/cleaning/cleaning_main_all_qmsum}

\input{figures/cleaning/cleaning_main_all_qaconv}

\input{figures/cleaning/cleaning_main_all_mrda}


\section{Task Datasets and Example Instances}
\label{sec_appendix_datasets}

\paragraph{QMSum.}
An example of an instance for summarization from the QMSum dataset is in \autoref{fig_qmsum_example}.

\paragraph{QAConv.}
An example of an instance for question-answering from the QAConv dataset is in \autoref{fig_qaconv_example}.

\paragraph{MRDA.}
Examples of instances for dialog act classification from the \textbf{MRDA} dataset are in \autoref{fig_mrda_example}.

The label-sets in dialog-act classification vary from one dataset to another, and have different levels of granularity (from 5 to 50+ labels). The utterances in MRDA are labeled with a dialog act on three granularity levels (with tag-sets of 5, 12 or 53 labels). For our experiments, we used the middle granularity level \citep[tagset with 12 dialog acts;][]{dhillon2004mrdaLabeling}, and the first and last 50 utterances from each transcript (100 of $\sim$1392), for efficiency purposes.


\input{figures/datasets/qmsum_example}

\input{figures/datasets/qaconv_example}

\input{figures/datasets/mrda_example}


\section{Licenses}

The following are the licenses of the used resources:
\begin{itemize}[noitemsep, topsep=0pt]
    \item Corpora:
    \begin{itemize}[noitemsep, topsep=0pt]
        \item QMSum: MIT
        \item QAConv: BSD-3-Clause (Salesforce)
        \item MRDA: GPL-3.0
    \end{itemize}
    \item Tools:
    \begin{itemize}[noitemsep, topsep=0pt]
        \item tortoise-tts: Apache-2.0
        \item rir-generator: MIT
        \item jiwer: Apache-2.0
        \item spacy: MIT
    \end{itemize}
    \item LLMs: Mistral and Llama models are gated models pulled from Huggingface.
\end{itemize}

\noindent
We use the above resources for exemplifying our framework, and solely for research purposes. Generally, our framework is intended for assessment of SLU solutions.

The code for our framework and analyses will be released under the Apache-2.0 license.


\section{Use of AI for the Paper}

ChatGPT was used for some minor rephrasing of sentences within the paper, and for assisting in preparing code to programmatically fill tables in LaTeX (placing results in tables for the paper).