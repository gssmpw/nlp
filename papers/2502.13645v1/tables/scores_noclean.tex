\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|cc|cc|cc|cc}
    \toprule
 & \multicolumn{2}{c|}{Mistral} & \multicolumn{2}{c|}{Llama3} & \multicolumn{2}{c|}{Llama3.1} & \multicolumn{2}{c|}{GPT4oMini} \\
    Task | Metric & AUC & NTP & AUC & NTP & AUC & NTP & AUC & NTP \\
    \midrule
    QMSum | PW-Rank &     \textbf{5.817} \small{± 0.321} & 0.195 &     5.811 \small{± 0.331} & 0.070 &     5.746 \small{± 0.312} & 0.249 &     5.627 \small{± 0.280} & 0.297 \\
    \midrule
    QAConv | Fuzzy &     39.080 \small{± 1.241} & 0.341 &     37.331 \small{± 1.217} & 0.302 &     49.466 \small{± 1.288} & 0.051 &     \textbf{49.992} \small{± 1.286} & 0.214 \\
    \midrule
    MRDA | Mac-$F_1$ &     0.110 \small{± 0.019} & 0.560 &     0.149 \small{± 0.028} & 0.615 &     0.140 \small{± 0.028} & 0.709 &     \textbf{0.193} \small{± 0.032} & 0.439 \\
    \bottomrule
    \end{tabular}%
    }
    \caption{The area-under-the-curve (AUC; with margin of error at 95\% confidence level) and the noise-tolerance point (NTP) of the experimented task models on the three tasks (summarization, question-answering and dialog-act classification). The highest AUC in each row is in bold. The scores correspond to the graphs in \autoref{fig_noclean_graphs}, where a more in-depth examination can be conducted. The full table (for all metrics) is in the Appendix, \autoref{tab_scores_noclean_all}.}
    \label{tab_scores_noclean}
\end{table*}