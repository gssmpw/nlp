\section{Conclusion}
\label{sec_conclusion}

Errors in speech-to-text automation propagate to downstream language understanding tasks, with noise magnitude and type affecting tasks and models differently. We present a configurable framework for evaluating noise impact, enabling analysis of model behavior across noise levels and transcript-cleaning techniques. Extensive experiments demonstrate its utility, providing insights into task model performance in SLU. The frameworkâ€™s flexibility supports more effective comparison and development of SLU pipelines.
%The errors in transcripts caused by speech-to-text automation propagate to downstream language understanding tasks. The magnitude and types of noise in transcripts effect tasks and models differently. We introduce a configurable framework for assessing the effect of noise on downstream tasks, that facilitates the analysis of model behavior at varying noise levels, and with transcript-cleaning techniques. Our extensive set of experiments richly demonstrate the utility of the framework. The resulting analysis offers valuable insights into the behavior of task models on SLU tasks, shedding light on their performance in varying settings. The flexibility of the framework enables more effective development of SLU pipelines.