\input{figures/framework}

\section{Background and Related Work}
\label{sec_related_work}

%Spoken language understanding (SLU) traditionally refers to the task of interpreting the intentions of speech utterances \citep{wang2005slu}. The common approach for SLU, is to transcribe speech with automatic speech recognition (ASR) systems, and to process the text with Natural Language Understanding (NLU) techniques to extract meaning from it \citep{tur2011slu}. Hence, over time, SLU has also become a general term to describe applicative tasks conducted on speech \citep{feng2022asrglue, shon-etal-2023-slue}.
Spoken language understanding \citep[SLU;][]{wang2005slu} commonly refers to a set of applicative tasks performed on speech \citep{feng2022asrglue, shon-etal-2023-slue}. A prevalent approach for SLU is to transcribe speech with ASR systems, and to process the text with Natural Language Understanding (NLU) techniques that extract meaning from it \citep{tur2011slu}.

Spoken language does not usually obey standard syntactic rules and contains disfluencies such as repairs and hesitations \citep{wang2005slu}. Moreover, when recording speech, the distance of a microphone, the clarity of speech, and environmental sounds add challenging hurdles for an ASR system, that hence produce transcripts that are unfaithful to the spoken words. The discrepancies between the reference (gold) transcript and the automatically produced transcript, a.k.a. ``noise'', is most commonly measured with \textit{word error rate} (WER). This metric measures the percentage of words in the ASR-transcript that were wrongly inserted, substituted and deleted, with respect to the reference transcript, i.e., a form of word edit distance.
%A major flaw in the the WER metric is that it does not account for the importance of each misaligned word, and therefore the WER score of a transcript does not necesarilly correlate with 

%Most speech-to-text consumers use an off-the-shelf ASR model that may not be optimized for the use-case at-hand, and thereby generate transcripts with unwanted errors \citep{Errattahi2018correction}. There is hence a line of work that focuses particularly on methods for correcting ASR-generated transcripts to decrease the WER scores \citep[e.g.,][]{Stouten2006cleaning, guo2019cleaning, di-gangi-etal-2019-robust, leng-etal-2021-fastcorrect-2, dutta2022errorcorrectionasrusing, guo2023cleaning}. \todo{See if there is a work that cleans one of the things that we clean. This would show that it is possible to do so if it is worth it. (We use the reference to clean).}
To decrease the WER scores or improve subsequent results on downstream tasks, one line of work focuses on correcting ASR-generated transcripts, e.g., by correcting spelling \citep{guo2019cleaning, dutta2022errorcorrectionasrusing}, disfluencies \citep{Stouten2006cleaning}, punctuation \citep{di-gangi-etal-2019-robust}, or mistakes in general \citep{leng-etal-2021-fastcorrect-2, guo2023cleaning}.
However, no cleaning method is flawless, and the errors in the transcript propagate on to the NLU stage \citep{Errattahi2018correction}. 
%Studies such as \citet{gopalakrishnan2020dialog} and \citet{kim2021robust} argue that NLU models are mainly trained on written language and are therefore not robust enough for spoken language, let alone for erroneous spoken language.
Additionally, studies \citep{gopalakrishnan2020dialog, kim2021robust} argue that NLU models are mainly trained on written language and are not robust for spoken language, let alone for erroneous spoken language.
%Moreover, \citet{siino2024preprocessing} show that preprocessed texts passed on to NLU models, including pretrained language models, yield inconsistent behavior across different models and tasks.
Therefore, some works train proprietary models for downstream tasks with noisy transcripts to improve results, e.g., for machine translation, intent classification, question answering, and more \citep{fang2020phoneme, cui2021approachimproverobustnessnlp, liu-etal-2021-robustness, feng22c_interspeech, jung-etal-2024-interventional}.

%A speech-to-downstream-task pipeline consists of an ASR system, an optional cleaning mechanism, and a task model. These interchanging components mean that outcomes can vary substantially according to the pipeline configuration \citep{siino2024preprocessing}. 
%Relatedly, there is a track of research on speech-to-downstream-task pipelines that focuses on analyzing the influence of the noise that propagates to the NLU stage, for tasks such as summarization \citep{Szaszak2016noiseSumm, Tundik2019noiseSLU, Chowdhury2024noiseeffect}, question-answering \citep{lee2018spokenSquad, You2021noiseQA} and classification \citep{shon2022slue, Pentland2023socialscience}. These works mainly compare the results on a downstream task between transcripts with and without noise. 
Another track of research \textit{analyzes} the influence of transcript noise on tasks such as summarization \citep{Szaszak2016noiseSumm, Tundik2019noiseSLU, Chowdhury2024noiseeffect}, question-answering \citep{lee2018spokenSquad, You2021noiseQA} and classification \citep{shon2022slue, Pentland2023socialscience}, mainly by comparing results with and without transcript noise in the input.
To expand this analysis, there are works that assess downstream results at \textit{several} levels or types of noise \citep{zechner-waibel-2000-minimizing, Agarwal2007noise, gopalakrishnan2020dialog, feng2022asrglue, shon-etal-2023-slue, Li2024dementia}. Some add noise synthetically to reference transcripts, while a few prepare recordings with impaired clarity \citep{feng2022asrglue} or use back-transcription \citep{kubis-etal-2023-back}.
Controlling the types of noise reveals their effect \citep{balagopalan-etal-2020-impact, min-etal-2021-evaluating}, working around the limitations of the WER metric, that does not account for the type of words or their importance to the task \citep{wang2003indicator}.
% To expand this analysis, there are works that assess the results at \textit{several} levels or types of noise \citep{zechner-waibel-2000-minimizing, Agarwal2007noise, gopalakrishnan2020dialog, feng2022asrglue, shon-etal-2023-slue, Li2024dementia}. Most of these works add noise synthetically to reference transcripts, while a few prepare recordings with impaired clarity
% %different levels of speech clarity and background sounds \citep[e.g.,][]{feng2022asrglue}.
% \citep{feng2022asrglue} or use back-transcription \citep{kubis-etal-2023-back}.
% By adding or removing certain types of noises, it is possible to examine which noise types have a stronger effect on the task results \citep{balagopalan-etal-2020-impact, min-etal-2021-evaluating}, since the WER metric does not account for the type of words or their importance to the task \citep{wang2003indicator}.
User studies were similarly set up in order to assess how well \textit{humans} conduct tasks on noisy transcripts \citep{stark2000speechretrieval, sanders2002effect, munteanu2006acceptable, favre2013performance}.
%, and measure task completion, user satisfaction and work efficiency. 
They often find at which WER score the ability of users to consistently complete tasks starts to deter.

%\citet{sanders2002effect} measure task completion, user satisfaction and work efficiency, and show that, in their setting, a WER above 35\% deterred the ability if completing tasks consistently. \citet{munteanu2006acceptable} similarly show that a WER of 25\% or less enabled users to answer questions on recorded lectures.

The studies described above aim to analyze the effect of transcription noise on downstream tasks, however each concentrates on a specific setting and employs different practices. Our proposed framework generalizes a method for conducting such assessments, allowing systematic examination of SLU pipelines.
% Each of the many studies described above concentrate on a specific setting and conduct their assessments with different practices. Their common-ground is that they aim to analyze the effect of transcription noise on downstream tasks.
% %Furthermore, they all come to varied conclusions. For these reasons, we 
% We propose a framework that generalizes a method for conducting such assessments. It allows practitioners to systematically examine the tools at their disposal for their speech-to-downstream-task pipeline. Moreover, the examination guides where to invest efforts in terms of transcript cleaning techniques.
Furthermore, although there are few works \citep{li2023gptSLU, zhu-etal-2024-zero} that analyze the ability of GPT-family LLMs to handle short transcribed texts on SLU tasks \citep[e.g., ASR-GLUE;][]{feng2022asrglue}, ours is the first, to the best of our knowledge, to assess the performance of several recent LLMs on full dialogues of spoken language. Regardless, our framework is robust to any form of dialogue, NLU task, and task model.
%, that are also relatively cheap to use, and therefore more likely to be adopted for SLU use cases.

% In our experiments (Section \ref{sec_setup}) we exemplify the use of our framework on
% a variety of scenarios.
% %three downstream tasks, with several task models, noise levels and cleaning methods.
% As the results show (Section \ref{sec_results}), task models respond differently to the levels and types of noise. Moreover, although there are few works \citep{li2023gptSLU, zhu-etal-2024-zero} that analyze the ability of GPT-family LLMs to handle short transcribed texts on SLU tasks \citep[e.g., ASR-GLUE;][]{feng2022asrglue}, ours is the first, to the best of our knowledge, to assess the performance of several recent LLMs on \textit{full} dialogues of \textit{spoken} language. Regardless, our framework is robust to any form of dialogue, SLU task, and task model.
% %, that are also relatively cheap to use, and therefore more likely to be adopted for SLU use cases.








%Consequently, SLU pipelines (i.e., ASR + NLU) easily diverge in their performance when addressing different speech quality, pipeline components, and output requirements \citep{wang2005slu}. This compels researchers to assess their pipeline for their specific purpose. Specifically, the assessment should pinpoint which components need to be upgraded or replaced in order to produce sufficient results.




% ----------


%\citep{dutta2022errorcorrectionasrusing} train a BART model to denoise transcripts. Synthetically create data for training, and also use actual noisy data from an ASR system.

%\citep{di-gangi-etal-2019-robust} Show how training on noisy and clean data help machine translation of transcripts, specifically on punctuation errors as noise.

%\citep{cui2021approachimproverobustnessnlp} Create noise on transcripts with PLM, and use it to train models for downstream tasks (translation, intent classification) to get better results than when training without the noise.

%\citep{liu-etal-2021-robustness} Train BERT and GPT-2 models with conversation data at different levels and types of synthetic perturbations, and show how it behaves on MultiWOZ (omain, intent, slot names, and slot values).

%\citet{jung-etal-2024-interventional} argue that noises added to transcripts (speech noise injection) are dependent on the STT system. Propose a generalized method for injecting noises to speech-based transcripts. Show how inference and sentiment classification tasks react to different kinds of noise injections at different levels of noise.

%\citep{feng22c_interspeech} Assess the results on the ASR-GLUE benchmark (5 different types of understanding tasks, at different levels of noise – 3 levels of noise, up to 12.5\% WER). Try to improve results by (1) fixing the transcripts before passing to task, (2) training the task model with noisy data created by and TTS-STT pipeline that adds noise, (3) adding erroneous text to the transcript and training.

%\citep{fang2020phoneme} Represent ASR transcriptions at the phoneme level, aiming to capture pronunciation similarities, which are typically neglected in word-level representations (e.g., word embeddings). To train and evaluate our phoneme representations, we generate noisy ASR transcriptions of four existing datasets - Stanford Sentiment Treebank, SQuAD, TREC Question Classification and Subjectivity Analysis - and show that common neural network architectures exploiting the proposed phoneme representations can effectively handle noisy transcriptions and significantly outperform state-of-the-art baselines. Finally, we confirm these results by testing our models on real utterances spoken to the Alexa virtual assistant. Add noise with a pipeline similar to ours

%\citep{kim2021robust} Argue that models trained for written language are not robust for spoken language and do not perform well.

%\citep{siino2024preprocessing} Show that the preprocessing of texts before passing them on to models, including PLMs, behaves differently in different models and for different tasks. Generalized rules do not exist across tasks and models.

% --------

% \citep{zechner-waibel-2000-minimizing}
% Assess the tradeoff between WER and extractive summary relevance.

% \citep{Chowdhury2024noiseeffect}
% Assess what kind of errors there are in ASR transcripts in Bengali, and show that it effects summary quality.

% \citep{You2021noiseQA}
% Show that automatically produced transcripts cause more errors in spoken-SQuaD than reference transcripts, even when training with the noisy transcripts.

% \citep{Szaszak2016noiseSumm}
% Assess how the ASR errors in spoken language (Hungarian) propogate to the NER task and extractive summarization. They find that POS is linearly affected by WER, and not in thw same proportion, so POS isn’t too badly effected. Specifically Nouns are least effected in recognizing them. Extractive summarization also sees a degradation, but not as bad as might be expected. The assessment is done on clean vs noisy transcripts, and not at different levels/types of noise. They augment the ASR process by transcribing phonological phrases, and in so improve the ASR output to improve the summarization.

% \citep{Tundik2019noiseSLU}
% Same as above paper, but check sentence similarity task (also in Hungarian). Show that punctuation normalization helps in summarizing. The conclusions in this and the above paper are specific to the tasks and to Hungarian.

%\citep{Agarwal2007noise}
% Add different kinds and levels of textual noise to transcripts and assess document-level classification tasks. Use older classification methods (e.g. SVM with feature engineering), and add text noise mainly on the character level (mis-spelling).

% \citep{gopalakrishnan2020dialog}
% Assess the next utterance classification task (choosing 1 from 5 possible utterances given the dialog up to a time point). Try it with written-style dialog (also with punctuation removed), synthetically-noised dialog (at 4 WER levels), and ASR-noised dialog (one level of noise). Show that a model trained fior written text is not good for this, and by training it with synthetically-noised data, it helps in the spoken language domain.

% \citep{feng2022asrglue}
% Benchmark based on GLUE, where the texts were recorded by speakers with different levels of background noise, and the same tasks were then executed on the transcribed texts instead of the original texts. The tasks are sentiment classification of movie reviews, sentence-level semantic similarity, question equivalence, question-answer pairing similarity, scientific entailment, and regular entailment. Show results with LMs (BERT-based, XLNet) on the different levels of noisy transcripts. For us: The tasks in the benchmark are not on conversation data, and the noise is at three levels.


%\citep{sanders2002effect}
% A user study that correlates between WER and task completion or user satisfaction, Show that there is a linear correlation between task completion and WER, but for user satisfaction Efficiency metrics are probably most interesting as a determinant of User Satisfaction, and at high values of WER other factors probably swamp the effects of WER on efficiency. Finally, we note that values of WER above about 35% were associated with increased variability of multiple metrics in both 2000 and 2001, perhaps suggesting that system’s ability to deal with ASR errors becomes more influential above WER of 35%. For us: Similar WER can give different results. There is a certain correlation between WER and task completion. Values of WER above about 35% were associated with increased variability of multiple metrics in both 2000 and 2001, perhaps suggesting that a system’s ability to deal with ASR errors becomes more influential above WER of 35%.

% \ciept{munteanu2006acceptable}
% Conduct a user study with a few levels of WER on recorded lectures on which users need to answer questions. A WER of 25 or less enabled users to answer decently.

% \citep{stark2000speechretrieval}
% Conduct a user study with a few levels or WER on news boradcasts, on which users need to rank relevance and conduct summarizatrion. Analyze how long people work and what other media they use to do their assignments.

% \citep{favre2013performance}
% Show that WER does not correlate to a human auditing task (getting updated on existing meetings and performing understanding tasks on the meetings) For us: Some of the transcripts used have similar WER but are from different acoustic and language models. If WER is an accurate predictor of human-subject performance, then the scores between these two systems should be similar. But it’s not (middle of section 2). -> It’s important to check per use case.

% \citep{wang2003indicator}
% Show that transcripts with different WERs due to different ASR systems are not consistent with slot filling accuracy. (WER does not necessarily indicate understanding quality)

% \citep{balagopalan-etal-2020-impact}
% Assess the impact of ASR errors on detecting Alzheimers in a patient’s speech. They artificially add noise (deletions, substitutions and insertions) to clean transcripts, and assess results on different levels of errors for each type of error. They find that deletions have the most impact on wrongly detecting Alzheimers.

% \citep{min-etal-2021-evaluating}
% “use automatic evaluation metrics such as word error rate (WER) and semantic distance to conduct domain agnostic evaluations of the ASR performance across conversation participants. Second, we conduct a domain specific examination of the ASR output by identifying domain-relevant keywords using behavioral codes and keywords identified using the Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001). Finally, we study the effect of the noisy ASR on the downstream behavioral coding task and empirically show that additional local context in the form of neighboring utterances can help alleviate the impact of ASR errors.”

% \citep{Li2024dementia}
% Assess how ASR errors effect Alzheimer classification

% \citep{Pentland2023socialscience}
% Show that ASR-output transcripts are not always reliable enough for social-science classification tasks, and that ASR technology should not be assumed to be good enough. Different applications require checking according to its use case.