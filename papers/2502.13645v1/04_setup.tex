\section{Experimental Setup}
\label{sec_setup}

\input{tables/datasets}


To demonstrate the utility of the framework for measuring \ENDow{}, we describe the various SLU pipeline configurations on which we apply the framework and conduct analyses (discussed in \S\ref{sec_results}).
%Our experiments are conducted on three downstream tasks, four task-models, and several cleaning techniques, as outlined below.

\subsection{Preparing Transcript Sets}
\label{sec_setup_transcripts}

\paragraph{Text-to-speech model.}
Some of the SLU datasets in our experiments lack accompanying audio files, and in any case, we would like our experiments to be based on a controlled speech environment. We used the \texttt{toirtoise-tts} \citep{betker2023tortoisetts} Python library\footnote{\url{https://github.com/neonbjb/tortoise-tts}} as the text-to-speech model, and implemented a procedure for handling lengthy speech (see Appendix \ref{sec_appendix_implementation_tts}). The TTS stage produces the initial set of audio files for each of the SLU datasets in our experiments. 
%See Appendix \todo{TTS appendix} for technical details on the TTS procedure applied for producing high-quality audio files.

\paragraph{Noising method.}
Each audio file was reverberated with the \texttt{rir-generator} \citep{werner2023rirgenerator} Python library,\footnote{\url{https://github.com/audiolabs/rir-generator}} and then recreated with background office sounds \citep[a clipped audio file;][]{myNoise2020office} with one of five signal-to-noise ratios (see Appendix \ref{sec_appendix_implementation_noising}). 
%We produced noised audio files at various levels in two steps: (1) an audio file is reverberated with the \texttt{rir-generator} \citep{werner2023rirgenerator} Python library,\footnote{\url{https://github.com/audiolabs/rir-generator}} and then (2) background office sounds are added, using a clipped audio file \citep{myNoise2020office}, at five different signal-to-noise ratios (see Appendix \ref{sec_appendix_implementation_noising}).
After this process there are six sets of increasingly tampered audio files.

\paragraph{ASR system.}
We used Whisper \citep{Radford2023whisper}\footnote{\texttt{openai/whisper-small.en}} for conducting speech-to-text (see Appendix \ref{sec_appendix_implementation_stt}). 
In all there are seven sets of increasingly noised transcripts (the first is the clean reference set). In our setting, seven noise levels provided a satisfactory analysis for examining the behavior of the SLU pipeline. The WER scores distribute within 0 and 0.9, and the curves empirically exhibit sufficiently clear behavioral patterns.
%In all there are six sets of ASR-generated transcripts and the reference set, overall providing seven levels of noise (the first set with no noise). We aimed for seven levels as this provides a satisfactory analysis for examining the behavior of the SLU pipeline.\footnote{The range of reverberation we used to impair the audio files yield transcripts with WER scores of up to $\sim$0.9. Also, the seven levels of noise empirically provide curves with sufficiently clear behavioral patterns.}

\paragraph{Cleaning techniques.}
In our experiments, we use the cleaning component to study the effect of different types of words, e.g., nouns, on downstream tasks. This analysis also simulates an SLU pipeline in which the ASR system prioritizes accuracy for specific word types, guiding where to focus efforts in the transcription process.

To clean transcripts, we first aligned a noised transcript to its respective reference transcript with \texttt{jiwer}.\footnote{\url{https://github.com/jitsi/jiwer}} Then, any non-equivalent alignment that involves the targeted word-type was repaired. We separately target nouns, verbs, adjectives, adverbs, any of the above (``content words''), none of the above (``non-content words''), and named entities -- seven techniques in all. Details in Appendix \ref{sec_appendix_implementation_cleaning}.


\subsection{Downstream Tasks}
\label{sec_setup_tasks}

We experiment with three downstream tasks, characterized by different output objectives. Summarization is a generation task where text is synthesized based on the collective understanding of a dialog. Question-answering is framed here as an extraction task that retrieves spans from the transcript. Dialog-act categorization is a classification task that assigns a communicative goal label (e.g., `statement', `question', etc.) to conversational utterances. The first two tasks are on the full transcript level, while the latter task is on the utterance level. These differences offer insights into potential distinctions in SLU pipelines.

In our experiments we focus on \textit{long spoken dialogues}, as opposed to short or written dialogues, as they impose a more challenging setting for task models.
%In our experiments we focus on \textit{long spoken dialogues}, as opposed to dialogues with few turns or those based on written language such as chats or forums.
See \autoref{tab_datasets} for a summary of the tasks, and Appendix \ref{sec_appendix_datasets} for examples of task instances.

\paragraph{Summarization.}
For summarization, we use the QMSum dataset\footnote{\url{https://github.com/Yale-LILY/QMSum}} \citep{zhong-etal-2021-qmsum}, a generic and query-focused dialog summarization benchmark. It consists of transcripts and summaries of product meetings \citep[AMI;][]{carletta2006ami}, academic meetings \citep[ICSI;][]{janin2003icsi} and parliament committee meetings.
%, all as transcripts with their respective queries and reference summaries.

To evaluate system summaries we use standard ROUGE metrics\footnote{\url{huggingface.co/spaces/evaluate-metric/rouge}, with the default arguments.} \citep{lin-2004-rouge} and pairwise comparison ranking \citep{qin-etal-2024-large} with \texttt{GPT-4o-mini} as a judge for overall quality \citep{liu-etal-2024-benchmarking} (see Appendix \ref{sec_appendix_implementation_pairwise} for details).

\paragraph{Question-answering.}
The QAConv dataset\footnote{\url{https://github.com/salesforce/QAConv}} \citep{wu-etal-2022-qaconv} consists of dialogues with questions whose answers are short spans in the dialog. We only use the instances based on court cases or interviews (since these are long spoken dialogues).

For evaluation, predicted answers are compared against reference answers with exact match accuracy, token-level $F_1$ and fuzzy matching, following the QAConv benchmark.

\paragraph{Dialog-act classification.}
%The objective of this task is to label an utterance with the proper communicative function, such as a question or statement. We use 
The MRDA dataset\footnote{\url{https://github.com/NathanDuran/MRDA-Corpus}} \citep{shriberg-etal-2004-icsi} consists of meetings from the ICSI corpus and research-oriented group meetings. Each utterance in the transcripts is labeled with one of 12 dialog act labels \citep{dhillon2004mrdaLabeling}. We utilize the first and last 50 utterances from each transcript (100 of $\sim$1392), for efficiency purposes. See Appendix \ref{sec_appendix_datasets} for more details.
%In the dialog-act classification task, the objective is to label an utterance with the proper communicative function, such as a question or statement. The label-sets vary from one dataset to another, and have different levels of granularity (from 5 to 50+ labels). We use the MRDA dataset\footnote{\url{https://github.com/NathanDuran/MRDA-Corpus}} \citep{shriberg-etal-2004-icsi}, which consists of meetings from the ICSI corpus and research-oriented group meetings. Each utterance is labeled with a dialog act on three granularity levels. For our experiments, we used the middle granularity level \citep[tagset with 12 dialog acts;][]{dhillon2004mrdaLabeling}, and the first and last 50 utterances from each transcript (100 of $\sim$1392), for efficiency purposes.

The MRDA results were traditionally evaluated with the accuracy metric, but we also evaluate with macro-$F_1$ due to the high class imbalance in the dataset, as suggested by \citet{miah-etal-2023-hierarchical}.

\paragraph{Models.}
For all three tasks, we experiment with four instruct-tuned LLMs in zero-shot mode:
%, on the test sets of the tasks.
%The models used are 
Mistral-7B,\footnote{\texttt{mistralai/Mistral-7B-Instruct-v0.3}} Llama3-8B,\footnote{\texttt{meta-llama/Meta-Llama-3-8B-Instruct}} Llama3.1-8B,\footnote{\texttt{meta-llama/Llama-3.1-8B-Instruct}} and GPT-4o-mini.\footnote{\texttt{gpt-4o-mini-2024-07-18}} 
They were selected for their modest hardware requirements and affordability.
%Additionally, our goal is to investigate whether these models behave differently when challenged with transcripts with varying noise levels and types.
%We use the same general prompts for the four models, which were scripted with some light prompt engineering on a few instances.
Since the context size of Mistral and Llama-3 cannot fit most of the transcripts in full, summarization and QA were conducted on these models in segments.
%, however GPT and Llama-3.1 received the full transcripts as input.
See details and prompts in Appendix \ref{sec_appendix_prompts}.