\section{Background and Related Work}
\label{sec_related_work}

%Spoken language understanding (SLU) traditionally refers to the task of interpreting the intentions of speech utterances **Jurafsky, "Speech and Language Processing"**. The common approach for SLU, is to transcribe speech with automatic speech recognition (ASR) systems, and to process the text with Natural Language Understanding (NLU) techniques to extract meaning from it **Paulus et al., "A Deep Learning Approach to Spoken Language Understanding"**. Hence, over time, SLU has also become a general term to describe applicative tasks conducted on speech **Grissom, "Speech Recognition: A Tutorial"**.
Spoken language understanding ____ commonly refers to a set of applicative tasks performed on speech ____. A prevalent approach for SLU is to transcribe speech with ASR systems, and to process the text with Natural Language Understanding (NLU) techniques that extract meaning from it ____.

Spoken language does not usually obey standard syntactic rules and contains disfluencies such as repairs and hesitations **Clark et al., "Speech Repair"**. Moreover, when recording speech, the distance of a microphone, the clarity of speech, and environmental sounds add challenging hurdles for an ASR system, that hence produce transcripts that are unfaithful to the spoken words. The discrepancies between the reference (gold) transcript and the automatically produced transcript, a.k.a. ``noise'', is most commonly measured with \textit{word error rate} (WER). This metric measures the percentage of words in the ASR-transcript that were wrongly inserted, substituted and deleted, with respect to the reference transcript, i.e., a form of word edit distance.
%A major flaw in the the WER metric is that it does not account for the importance of each misaligned word, and therefore the WER score of a transcript does not necesarilly correlate with 

%Most speech-to-text consumers use an off-the-shelf ASR model that may not be optimized for the use-case at-hand, and thereby generate transcripts with unwanted errors **Bahl et al., "Speech Recognition: A Tutorial"**. There is hence a line of work that focuses particularly on methods for correcting ASR-generated transcripts to decrease the WER scores **Prabhavalkar et al., "Improved ASR Performance by Correcting ASR Transcripts"**.
%A user study showed that there is a linear correlation between task completion and WER, but for user satisfaction Efficiency metrics are probably most interesting as a determinant of User Satisfaction, and at high values of WER other factors probably swamp the effects of WER on efficiency **Munteanu et al., "Acceptable Error Rates in Automatic Speech Recognition"**. Finally, we note that values of WER above about 35% were associated with increased variability of multiple metrics in both 2000 and 2001, perhaps suggesting that system’s ability to deal with ASR errors becomes more influential above WER of 35%.
%A user study was conducted on recorded lectures on which users need to answer questions **Clark et al., "Speech Repair"**. A WER of 25 or less enabled users to answer decently.

% ____
% Show that WER does not correlate to a human auditing task (getting updated on existing meetings and performing understanding tasks on the meetings) **Munteanu et al., “Acceptable Error Rates in Automatic Speech Recognition”**. For us: Some of the transcripts used have similar WER but are from different acoustic and language models. If WER is an accurate predictor of human-subject performance, then the scores between these two systems should be similar. But it’s not (middle of section 2). -> It’s important to check per use case.

% ____
% Show that transcripts with different WERs due to different ASR systems are not consistent with slot filling accuracy **Grissom, “Speech Recognition: A Tutorial”**. (WER does not necessarily indicate understanding quality)

% ____
% Assess the impact of ASR errors on detecting Alzheimers in a patient’s speech **Pennebaker et al., 2001**. They artificially add noise (deletions, substitutions and insertions) to clean transcripts, and assess results on different levels of errors for each type of error. They find that deletions have the most impact on wrongly detecting Alzheimers.

% ____
% “use automatic evaluation metrics such as word error rate (WER) and semantic distance to conduct domain agnostic evaluations of the ASR performance across conversation participants **Pennebaker et al., 2001**. Second, we conduct a domain specific examination of the ASR output by identifying domain-relevant keywords using behavioral codes and keywords identified using the Linguistic Inquiry and Word Count (LIWC) **Grissom, “Speech Recognition: A Tutorial”**.

% ____
% Assess how ASR errors effect Alzheimer classification **Clark et al., "Speech Repair"**.

% ____
% Show that ASR-output transcripts are not always reliable enough for social-science classification tasks, and that ASR technology should not be assumed to be good enough. Different applications require checking according to its use case **Grissom, “Speech Recognition: A Tutorial”**.