@INPROCEEDINGS{Agarwal2007noise,
  author={Agarwal, Sumeet and Godbole, Shantanu and Punjani, Diwakar and Roy, Shourya},
  booktitle={Seventh IEEE International Conference on Data Mining (ICDM 2007)}, 
  title="{How Much Noise Is Too Much: A Study in Automatic Text Classification}", 
  year={2007},
  volume={},
  number={},
  pages={3-12},
  keywords={Text categorization;Cleaning;Data processing;Blogs;Handwriting recognition;Automatic speech recognition;Text recognition;Internet;Mobile handsets;Text mining},
  doi={10.1109/ICDM.2007.21}}

@article{Chowdhury2024noiseeffect,
author = {Chowdhury, Priyanjana and Sarkar, Nabanika and Nath, Sanghamitra and Sharma, Utpal},
title = "{Analyzing the Effects of Transcription Errors on Summary Generation of Bengali Spoken Documents}",
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3678005},
doi = {10.1145/3678005},
abstract = {Automatic speech recognition (ASR) has become an indispensable part of the AI domain, with various speech technologies reliant on it. The quality of speech recognition depends on the amount of annotated data used to train an ASR system, among other factors. For a low-resourced language, this is a severe constraint and thus ASR quality is often poor. Humans can read through text containing ASR-errors, provided the context of the sentence is preserved. Yet in cases of transcripts generated by ASR systems of low-resource languages, multiple important words are misrecognized and the context is mostly lost; discerning such a text becomes nearly impossible. This article analyzes the types of transcription errors that occur while generating ASR transcripts of spoken documents in Bengali, an under-resourced language predominantly spoken in India and Bangladesh. The transcripts of the Bengali spoken document are generated using the ASR of Google Cloud Speech. The article also explores if there is an effect of such transcription errors in generating speech summaries of these spoken documents. Summarization is carried out extractively; sentences are selected from the ASR-generated text of the spoken document. Speech summaries are created by aggregating the speech-segments from the original speech of the selected sentences. Subjective evaluation shows the “readability” of the spoken summaries are not degraded by ASR errors, but the quality is affected due to the reliance on intermediate text-summary containing transcription errors.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {136},
numpages = {28},
keywords = {Low-resource language, extractive speech sumarization, automatic speech recognition, transcription error, speech summary}
}

@article{Errattahi2018correction,
title = "{Automatic Speech Recognition Errors Detection and Correction: A Review}",
journal = {Procedia Computer Science},
volume = {128},
pages = {32-37},
year = {2018},
note = {1st International Conference on Natural Language and Speech Processing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918302187},
author = {Rahhal Errattahi and Asmaa {El Hannani} and Hassan Ouahmane},
keywords = {Automatic Speech Recognition, ASR Error Detection, ASR Error Correction, ASR evaluation},
abstract = {Even though Automatic Speech Recognition (ASR) has matured to the point of commercial applications, high error rate in some speech recognition domains remain as one of the main impediment factors to the wide adoption of speech technology, and especially for continuous large vocabulary speech recognition applications. The persistent presence of ASR errors have intensified the need to find alternative techniques to automatically detect and correct such errors. The correction of the transcription errors is very crucial not only to improve the speech recognition accuracy, but also to avoid the propagation of the errors to the subsequent language processing modules such as machine translation. In this paper, basic principles of ASR evaluation are first summarized, and then the state of the current ASR errors detection and correction research is reviewed. We focus on emerging techniques using word error rate metric.}
}

@article{Li2024dementia,
title = "{Useful blunders: Can automated speech recognition errors improve downstream dementia classification?}",
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104598},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104598},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000169},
author = {Changye Li and Weizhe Xu and Trevor Cohen and Serguei Pakhomov},
keywords = {Automatic speech recognition, Natural language processing, Dementia, Explainable artificial intelligence},
abstract = {Objectives: We aimed to investigate how errors from automatic speech recognition (ASR) systems affect dementia classification accuracy, specifically in the “Cookie Theft” picture description task. We aimed to assess whether imperfect ASR-generated transcripts could provide valuable information for distinguishing between language samples from cognitively healthy individuals and those with Alzheimer’s disease (AD). Methods: We conducted experiments using various ASR models, refining their transcripts with post-editing techniques. Both these imperfect ASR transcripts and manually transcribed ones were used as inputs for the downstream dementia classification. We conducted comprehensive error analysis to compare model performance and assess ASR-generated transcript effectiveness in dementia classification. Results: Imperfect ASR-generated transcripts surprisingly outperformed manual transcription for distinguishing between individuals with AD and those without in the “Cookie Theft” task. These ASR-based models surpassed the previous state-of-the-art approach, indicating that ASR errors may contain valuable cues related to dementia. The synergy between ASR and classification models improved overall accuracy in dementia classification. Conclusion: Imperfect ASR transcripts effectively capture linguistic anomalies linked to dementia, improving accuracy in classification tasks. This synergy between ASR and classification models underscores ASR’s potential as a valuable tool in assessing cognitive impairment and related clinical applications.}
}

@article{Pentland2023socialscience,
author = {Steven J. Pentland, Christie M. Fuller, Lee A. Spitzley and Douglas P. Twitchell},
title = "{Does accuracy matter? Methodological considerations when using automated speech-to-text for social science research}",
journal = {International Journal of Social Research Methodology},
volume = {26},
number = {6},
pages = {661--677},
year = {2023},
publisher = {Routledge},
doi = {10.1080/13645579.2022.2087849},
URL = {https://doi.org/10.1080/13645579.2022.2087849},
eprint = {https://doi.org/10.1080/13645579.2022.2087849}
}

@article{Stouten2006cleaning,
title = "{Coping with disfluencies in spontaneous speech recognition: Acoustic detection and linguistic context manipulation}",
journal = {Speech Communication},
volume = {48},
number = {11},
pages = {1590-1606},
year = {2006},
note = {Robustness Issues for Conversational Interaction},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2006.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167639306000434},
author = {Frederik Stouten and Jacques Duchateau and Jean-Pierre Martens and Patrick Wambacq},
keywords = {Disfluency handling, Spontaneous speech recognition, Disfluency detection},
abstract = {Nowadays read speech recognition already works pretty well, but the recognition of spontaneous speech is much more problematic. There are plenty of reasons for this, and we hypothesize that one of them is the regular occurrence of disfluencies in spontaneous speech. Disfluencies disrupt the normal course of the sentence and when for instance word interruptions are concerned, they also give rise to word-like speech elements which have no representation in the lexicon of the recognizer. In this paper we propose novel methods that aim at coping with the problems induced by three types of disfluencies, namely filled pauses, repeated words and sentence restarts. Our experiments show that especially the proposed methods for filled pause handling offer a moderate but statistically significant improvement over the more traditional techniques previously presented in the literature.}
}

@inproceedings{Szaszak2016noiseSumm,
author = {Szasz\'{a}k, Gy\"{o}rgy and T\"{u}ndik, M\'{a}t\'{e} \'{A}kos and Beke, Andr\'{a}s},
title = "{Summarization of Spontaneous Speech using Automatic Speech Recognition and a Speech Prosody based Tokenizer}",
year = {2016},
isbn = {9789897582035},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0006044802210227},
doi = {10.5220/0006044802210227},
abstract = {This paper addresses speech summarization of highly spontaneous speech. The audio signal is transcribed usingan Automatic Speech Recognizer, which operates at relatively high word error rates due to the complexityof the recognition task and high spontaneity of speech. An analysis is carried out to assess the propagationof speech recognition errors into syntactic parsing. We also propose an automatic, speech prosody based audiotokenization approach and compare it to human performance. The so obtained sentence-like tokens areanalysed by the syntactic parser to help ranking based on thematic terms and sentence position. The thematicterm is expressed in two ways: TF-IDF and Latent Semantic Indexing. The sentence scores are calculated asa linear combination of the thematic term score and a positional score. The summary is generated from thetop 10 candidates. Results show that prosody based tokenization reaches human average performance and thatspeech recognition errors propagate moderately into syntactic parsing (POS tagging and dependency parsing).Nouns prove to be quite error resistant. Audio summarization shows 0.62 recall and 0.79 precision by anF-measure of 0.68, compared to human reference. A subjective test is also carried out on a Likert-scale. Allresults apply to spontaneous Hungarian.},
booktitle = {Proceedings of the International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
pages = {221–227},
numpages = {7},
keywords = {Tokenization, Summarization, Speech Recognition, Speech, Latent Semantic Indexing., Audio},
location = {Porto, Portugal},
series = {IC3K 2016}
}

@article{Tundik2019noiseSLU, 
title="{On the Effects of Automatic Transcription and Segmentation Errors in Hungarian Spoken Language Processing}", 
volume={63}, 
url={https://pp.bme.hu/eecs/article/view/14052}, 
DOI={10.3311/PPee.14052}, 
abstractNote={&lt;p&gt;Emerging Artificial Intelligence (AI) technology has brought machines to reach an equal or even superior level compared to human capabilities in several fields; nevertheless, among many other fields, making a computer able to understand human language still remains a challenge. When dealing with speech understanding, Automatic Speech Recognition (ASR) is used to generate transcripts, which are processed with text-based tools targeting Spoken Language Understanding (SLU). Depending on the ASR quality (which further depends on speech quality, the complexity of the topic, environment etc.), transcripts contain errors, which propagate further into the processing pipeline. Subjective tests show on the other hand, that humans understand quite well ASR-closed captions, despite the word and punctuation errors. Through word embedding based semantic parsing, the present paper is interested in quantifying the semantic bias introduced by ASR error propagation. As a special use case, speech summarization is also evaluated with regard to ASR error propagation. We show, that despite the higher word error rates seen with the highly inflectional Hungarian, the semantic space suffers least impact than the difference in Word Error Rate would suggest.&lt;/p&gt;}, 
number={4}, 
journal={Periodica Polytechnica Electrical Engineering and Computer Science}, 
author={Tündik, Máté Ákos and Kaszás, Valér and Szaszák, György}, 
year={2019}, 
pages={254–262}
}

@inproceedings{You2021noiseQA,
  author={You, Chenyu and Chen, Nuo and Zou, Yuexian},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title="{Knowledge Distillation for Improved Accuracy in Spoken Question Answering}", 
  year={2021},
  volume={},
  number={},
  pages={7793-7797},
  keywords={Training;Conferences;Manuals;Syntactics;Signal processing;Knowledge discovery;Natural language processing;knowledge distillation;spoken question answering;question answering},
  doi={10.1109/ICASSP39728.2021.9414999}}

@inproceedings{balagopalan-etal-2020-impact,
    title = "{Impact of {ASR} on {A}lzheimer{'}s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others}",
    author = "Balagopalan, Aparna  and
      Shkaruta, Ksenia  and
      Novikova, Jekaterina",
    editor = "Xu, Wei  and
      Ritter, Alan  and
      Baldwin, Tim  and
      Rahimi, Afshin",
    booktitle = "Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wnut-1.21",
    doi = "10.18653/v1/2020.wnut-1.21",
    pages = "159--164",
    abstract = "Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In this paper, we experiment with controlled amounts of artificially generated ASR errors and investigate their influence on dementia detection. We find that deletion errors affect detection performance the most, due to their impact on the features of syntactic complexity and discourse representation in speech. We show the trend to be generalisable across two different datasets for cognitive impairment detection. As a conclusion, we propose optimising the ASR to reflect a higher penalty for deletion errors in order to improve dementia detection performance.",
}

@misc{cui2021approachimproverobustnessnlp,
      title="{An Approach to Improve Robustness of NLP Systems against ASR Errors}", 
      author={Tong Cui and Jinghui Xiao and Liangyou Li and Xin Jiang and Qun Liu},
      year={2021},
      eprint={2103.13610},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.13610}, 
}

@inproceedings{di-gangi-etal-2019-robust,
    title = "{Robust Neural Machine Translation for Clean and Noisy Speech Transcripts}",
    author = "Di Gangi, Matti  and
      Enyedi, Robert  and
      Brusadin, Alessandra  and
      Federico, Marcello",
    editor = {Niehues, Jan  and
      Cattoni, Rolando  and
      St{\"u}ker, Sebastian  and
      Negri, Matteo  and
      Turchi, Marco  and
      Ha, Thanh-Le  and
      Salesky, Elizabeth  and
      Sanabria, Ramon  and
      Barrault, Loic  and
      Specia, Lucia  and
      Federico, Marcello},
    booktitle = "Proceedings of the 16th International Conference on Spoken Language Translation",
    month = nov # " 2-3",
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2019.iwslt-1.32",
    abstract = "Neural machine translation models have shown to achieve high quality when trained and fed with well structured and punctuated input texts. Unfortunately, the latter condition is not met in spoken language translation, where the input is generated by an automatic speech recognition (ASR) system. In this paper, we study how to adapt a strong NMT system to make it robust to typical ASR errors. As in our application scenarios transcripts might be post-edited by human experts, we propose adaptation strategies to train a single system that can translate either clean or noisy input with no supervision on the input type. Our experimental results on a public speech translation data set show that adapting a model on a significant amount of parallel data including ASR transcripts is beneficial with test data of the same type, but produces a small degradation when translating clean text. Adapting on both clean and noisy variants of the same data leads to the best results on both input types.",
}

@misc{dutta2022errorcorrectionasrusing,
      title="{Error Correction in ASR using Sequence-to-Sequence Models}", 
      author={Samrat Dutta and Shreyansh Jain and Ayush Maheshwari and Souvik Pal and Ganesh Ramakrishnan and Preethi Jyothi},
      year={2022},
      eprint={2202.01157},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.01157}, 
}

@inproceedings{fang2020phoneme,
author = {Fang, Anjie and Filice, Simone and Limsopatham, Nut and Rokhlenko, Oleg},
title = {Using Phoneme Representations to Build Predictive Models Robust to ASR Errors},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401050},
doi = {10.1145/3397271.3401050},
abstract = {Even though Automatic Speech Recognition (ASR) systems significantly improved over the last decade, they still introduce a lot of errors when they transcribe voice to text. One of the most common reasons for these errors is phonetic confusion between similar-sounding expressions. As a result, ASR transcriptions often contain "quasi-oronyms", i.e., words or phrases that sound similar to the source ones, but that have completely different semantics (e.g., "win" instead of "when" or "accessible on defecting" instead of "accessible and affecting"). These errors significantly affect the performance of downstream Natural Language Understanding (NLU) models (e.g., intent classification, slot filling, etc.) and impair user experience. To make NLU models more robust to such errors, we propose novel phonetic-aware text representations. Specifically, we represent ASR transcriptions at the phoneme level, aiming to capture pronunciation similarities, which are typically neglected in word-level representations (e.g., word embeddings). To train and evaluate our phoneme representations, we generate noisy ASR transcriptions of four existing datasets - Stanford Sentiment Treebank, SQuAD, TREC Question Classification and Subjectivity Analysis - and show that common neural network architectures exploiting the proposed phoneme representations can effectively handle noisy transcriptions and significantly outperform state-of-the-art baselines. Finally, we confirm these results by testing our models on real utterances spoken to the Alexa virtual assistant.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {699–708},
numpages = {10},
keywords = {virtual assistant, phoneme embeddings, natural language understanding, deep learning, classification, ASR error},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{favre2013performance,
  title     = "{Automatic human utility evaluation of ASR systems: does WER really predict performance?}",
  author    = {Benoit Favre and Kyla Cheung and Siavash Kazemian and Adam Lee and Yang Liu and Cosmin Munteanu and Ani Nenkova and Dennis Ochei and Gerald Penn and Stephen Tratz and Clare Voss and Frauke Zeller},
  year      = {2013},
  booktitle = {Interspeech 2013},
  pages     = {3463--3467},
  doi       = {10.21437/Interspeech.2013-610},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/interspeech_2013/favre13_interspeech.html}
}

@misc{feng2022asrglue,
      title="{ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding}", 
      author={Lingyun Feng and Jianwei Yu and Deng Cai and Songxiang Liu and Haitao Zheng and Yan Wang},
      year={2022},
      eprint={2108.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.13048}, 
}

@inproceedings{feng22c_interspeech,
  title     = "{ASR-Robust Natural Language Understanding on ASR-GLUE dataset}",
  author    = {Lingyun Feng and Jianwei Yu and Yan Wang and Songxiang Liu and Deng Cai and Haitao Zheng},
  year      = {2022},
  booktitle = {Interspeech 2022},
  pages     = {1101--1105},
  doi       = {10.21437/Interspeech.2022-10097},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/interspeech_2022/feng22c_interspeech.html}
}

@inproceedings{gopalakrishnan2020dialog,
  title     = "{Are Neural Open-Domain Dialog Systems Robust to Speech Recognition Errors in the Dialog History? An Empirical Study}",
  author    = {Karthik Gopalakrishnan and Behnam Hedayatnia and Longshaokan Wang and Yang Liu and Dilek Hakkani-Tür},
  year      = {2020},
  booktitle = {Interspeech 2020},
  pages     = {911--915},
  doi       = {10.21437/Interspeech.2020-1508},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/interspeech_2020/gopalakrishnan20_interspeech.html}
}

@inproceedings{guo2019cleaning,
  author={Guo, Jinxi and Sainath, Tara N. and Weiss, Ron J.},
  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title="{A Spelling Correction Model for End-to-end Speech Recognition}", 
  year={2019},
  volume={},
  number={},
  pages={5651-5655},
  keywords={speech recognition;sequence-to-sequence;attention models;spelling correction;language model},
  doi={10.1109/ICASSP.2019.8683745}
}

@inproceedings{guo2023cleaning,
  author={Guo, Jiaxin and Wang, Minghan and Qiao, Xiaosong and Wei, Daimeng and Shang, Hengchao and Li, Zongyao and Yu, Zhengzhe and Li, Yinglu and Su, Chang and Zhang, Min and Tao, Shimin and Yang, Hao},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title="{UCorrect: An Unsupervised Framework for Automatic Speech Recognition Error Correction}", 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Error analysis;Training data;Detectors;Data models;Generators;Error correction;Decoding;ASR;Error Correction;Unsupervised;WER},
  doi={10.1109/ICASSP49357.2023.10096194}
}

@inproceedings{jung-etal-2024-interventional,
    title = "{Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding}",
    author = "Jung, YeonJoon  and
      Lee, Jaeseong  and
      Choi, Seungtaek  and
      Lee, Dohyeon  and
      Kim, Minsoo  and
      Hwang, Seung-won",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1149",
    doi = "10.18653/v1/2024.emnlp-main.1149",
    pages = "20642--20655",
    abstract = "Recently, pre-trained language models (PLMs) have been increasingly adopted in spoken language understanding (SLU). However, automatic speech recognition (ASR) systems frequently produce inaccurate transcriptions, leading to noisy inputs for SLU models, which can significantly degrade their performance. To address this, our objective is to train SLU models to withstand ASR errors by exposing them to noises commonly observed in ASR systems, referred to as ASR-plausible noises. Speech noise injection (SNI) methods have pursued this objective by introducing ASR-plausible noises, but we argue that these methods are inherently biased towards specific ASR systems, or ASR-specific noises. In this work, we propose a novel and less biased augmentation method of introducing the noises that are plausible to any ASR system, by cutting off the non-causal effect of noises. Experimental results and analyses demonstrate the effectiveness of our proposed methods in enhancing the robustness and generalizability of SLU models against unseen ASR systems by introducing more diverse and plausible ASR noises in advance.",
}

@inproceedings{kim2021robust,
  author={Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Hakkani-Tür, Dilek},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, 
  title="{“How Robust R U?”: Evaluating Task-Oriented Dialogue Systems on Spoken Conversations}", 
  year={2021},
  volume={},
  number={},
  pages={1147-1154},
  keywords={Conferences;Benchmark testing;Data models;Robustness;Task analysis;Automatic speech recognition;spoken dialogue systems;dialogue state tracking;knowledge-grounded dialogue generation},
  doi={10.1109/ASRU51503.2021.9688274}
}

@inproceedings{kubis-etal-2023-back,
    title = "{Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors}",
    author = "Kubis, Marek  and
      Sk{\'o}rzewski, Pawe{\l}  and
      Sowa{\'n}ski, Marcin  and
      Zietkiewicz, Tomasz",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.724",
    doi = "10.18653/v1/2023.emnlp-main.724",
    pages = "11824--11835",
    abstract = "In a spoken dialogue system, an NLU model is preceded by a speech recognition system that can deteriorate the performance of natural language understanding. This paper proposes a method for investigating the impact of speech recognition errors on the performance of natural language understanding models. The proposed method combines the back transcription procedure with a fine-grained technique for categorizing the errors that affect the performance of NLU models. The method relies on the usage of synthesized speech for NLU evaluation. We show that the use of synthesized speech in place of audio recording does not change the outcomes of the presented technique in a significant way.",
}

@inproceedings{lee2018spokenSquad,
  title     = "{Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension}",
  author    = {Chia-Hsuan Lee and Szu-Lin Wu and Chi-Liang Liu and Hung-yi Lee},
  year      = {2018},
  booktitle = {Interspeech 2018},
  pages     = {3459--3463},
  doi       = {10.21437/Interspeech.2018-1714},
  issn      = {2958-1796},
}

@inproceedings{leng-etal-2021-fastcorrect-2,
    title = "{FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition}",
    author = "Leng, Yichong  and
      Tan, Xu  and
      Wang, Rui  and
      Zhu, Linchen  and
      Xu, Jin  and
      Liu, Wenjie  and
      Liu, Linquan  and
      Li, Xiang-Yang  and
      Qin, Tao  and
      Lin, Edward  and
      Liu, Tie-Yan",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.367",
    doi = "10.18653/v1/2021.findings-emnlp.367",
    pages = "4328--4337",
    abstract = "Error correction is widely used in automatic speech recognition (ASR) to post-process the generated sentence, and can further reduce the word error rate (WER). Although multiple candidates are generated by an ASR system through beam search, current error correction approaches can only correct one sentence at a time, failing to leverage the voting effect from multiple candidates to better detect and correct error tokens. In this work, we propose FastCorrect 2, an error correction model that takes multiple ASR candidates as input for better correction accuracy. FastCorrect 2 adopts non-autoregressive generation for fast inference, which consists of an encoder that processes multiple source sentences and a decoder that generates the target sentence in parallel from the adjusted source sentence, where the adjustment is based on the predicted duration of each source token. However, there are some issues when handling multiple source sentences. First, it is non-trivial to leverage the voting effect from multiple source sentences since they usually vary in length. Thus, we propose a novel alignment algorithm to maximize the degree of token alignment among multiple sentences in terms of token and pronunciation similarity. Second, the decoder can only take one adjusted source sentence as input, while there are multiple source sentences. Thus, we develop a candidate predictor to detect the most suitable candidate for the decoder. Experiments on our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce the WER over the previous correction model with single candidate by 3.2{\%} and 2.6{\%}, demonstrating the effectiveness of leveraging multiple candidates in ASR error correction. FastCorrect 2 achieves better performance than the cascaded re-scoring and correction pipeline and can serve as a unified post-processing module for ASR.",
}

@inproceedings{li2023gptSLU,
  title     = "{How ChatGPT is Robust for Spoken Language Understanding?}",
  author    = {Guangpeng Li and Lu Chen and Kai Yu},
  year      = {2023},
  booktitle = {INTERSPEECH 2023},
  pages     = {2163--2167},
  doi       = {10.21437/Interspeech.2023-1466},
  issn      = {2958-1796},
}

@inproceedings{liu-etal-2021-robustness,
    title = "{Robustness Testing of Language Understanding in Task-Oriented Dialog}",
    author = "Liu, Jiexi  and
      Takanobu, Ryuichi  and
      Wen, Jiaxin  and
      Wan, Dazhen  and
      Li, Hongguang  and
      Nie, Weiran  and
      Li, Cheng  and
      Peng, Wei  and
      Huang, Minlie",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.192",
    doi = "10.18653/v1/2021.acl-long.192",
    pages = "2467--2480",
    abstract = "Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution. However, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice. In this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important aspects related to language understanding in real-world dialog systems, namely, language variety, speech characteristics, and noise perturbation. We propose a model-agnostic toolkit LAUG to approximate natural language perturbations for testing the robustness issues in task-oriented dialog. Four data augmentation approaches covering the three aspects are assembled in LAUG, which reveals critical robustness issues in state-of-the-art models. The augmented dataset through LAUG can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog.",
}

@inproceedings{min-etal-2021-evaluating,
    title = "{Evaluating Automatic Speech Recognition Quality and Its Impact on Counselor Utterance Coding}",
    author = "Min, Do June  and
      P{\'e}rez-Rosas, Ver{\'o}nica  and
      Mihalcea, Rada",
    editor = "Goharian, Nazli  and
      Resnik, Philip  and
      Yates, Andrew  and
      Ireland, Molly  and
      Niederhoffer, Kate  and
      Resnik, Rebecca",
    booktitle = "Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.clpsych-1.18",
    doi = "10.18653/v1/2021.clpsych-1.18",
    pages = "159--168",
    abstract = "Automatic speech recognition (ASR) is a crucial step in many natural language processing (NLP) applications, as often available data consists mainly of raw speech. Since the result of the ASR step is considered as a meaningful, informative input to later steps in the NLP pipeline, it is important to understand the behavior and failure mode of this step. In this work, we analyze the quality of ASR in the psychotherapy domain, using motivational interviewing conversations between therapists and clients. We conduct domain agnostic and domain-relevant evaluations using standard evaluation metrics and also identify domain-relevant keywords in the ASR output. Moreover, we empirically study the effect of mixing ASR and manual data during the training of a downstream NLP model, and also demonstrate how additional local context can help alleviate the error introduced by noisy ASR transcripts.",
}

@inproceedings{munteanu2006acceptable,
  title     = "{Measuring the acceptable word error rate of machine-generated webcast transcripts}",
  author    = {Cosmin Munteanu and Gerald Penn and Ron Baecker and Elaine Toms and David James},
  year      = {2006},
  booktitle = {Interspeech 2006},
  pages     = {paper 1756-Mon1CaP.2},
  doi       = {10.21437/Interspeech.2006-40},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/interspeech_2006/munteanu06_interspeech.html}
}

@inproceedings{sanders2002effect,
  title     = "{Effects of word error rate in the DARPA communicator data during 2000 and 2001}",
  author    = {Gregory A. Sanders and Audrey N. Le and John S. Garofolo},
  year      = {2002},
  booktitle = {7th International Conference on Spoken Language Processing (ICSLP 2002)},
  pages     = {277--280},
  doi       = {10.21437/ICSLP.2002-134},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/icslp_2002/sanders02_icslp.html}
}

@inproceedings{shon-etal-2023-slue,
    title = "{SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks}",
    author = "Shon, Suwon  and
      Arora, Siddhant  and
      Lin, Chyi-Jiunn  and
      Pasad, Ankita  and
      Wu, Felix  and
      Sharma, Roshan  and
      Wu, Wei-Lun  and
      Lee, Hung-yi  and
      Livescu, Karen  and
      Watanabe, Shinji",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.496",
    doi = "10.18653/v1/2023.acl-long.496",
    pages = "8906--8937",
    abstract = "Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. In order to facilitate the development of SLU models that leverage the success of pre-trained speech representations, we will release a new benchmark suite, including for each task (i) curated annotations for a relatively small fine-tuning set, (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model performance in various types of systems for easy comparisons. We present the details of data collection and annotation and the performance of the baseline models. We also analyze the sensitivity of pipeline models{'} performance to the speech recognition accuracy, using more than 20 publicly availablespeech recognition models.",
}

@INPROCEEDINGS{shon2022slue,
  author={Shon, Suwon and Pasad, Ankita and Wu, Felix and Brusco, Pablo and Artzi, Yoav and Livescu, Karen and Han, Kyu J.},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title="{SLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Natural Speech}", 
  year={2022},
  volume={},
  number={},
  pages={7927-7931},
  keywords={Training;Sentiment analysis;Annotations;Current measurement;Benchmark testing;Signal processing;Acoustic measurements;spoken language understanding;benchmark;pre-training;named entity recognition;sentiment analysis},
  doi={10.1109/ICASSP43922.2022.9746137}}

@article{siino2024preprocessing,
title = "{Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers}",
journal = {Information Systems},
volume = {121},
pages = {102342},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102342},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923001783},
author = {Marco Siino and Ilenia Tinnirello and Marco {La Cascia}},
keywords = {Text preprocessing, Natural Language Processing, Fake news, SVM, Bayes, Transformers, Deep learning, LSTM, Convolutional neural networks},
abstract = {With the advent of the modern pre-trained Transformers, the text preprocessing has started to be neglected and not specifically addressed in recent NLP literature. However, both from a linguistic and from a computer science point of view, we believe that even when using modern Transformers, text preprocessing can significantly impact on the performance of a classification model. We want to investigate and compare, through this study, how preprocessing impacts on the Text Classification (TC) performance of modern and traditional classification models. We report and discuss the preprocessing techniques found in the literature and their most recent variants or applications to address TC tasks in different domains. In order to assess how much the preprocessing affects classification performance, we apply the three top referenced preprocessing techniques (alone or in combination) to four publicly available datasets from different domains. Then, nine machine learning models – including modern Transformers – get the preprocessed text as input. The results presented show that an educated choice on the text preprocessing strategy to employ should be based on the task as well as on the model considered. Outcomes in this survey show that choosing the best preprocessing technique – in place of the worst – can significantly improve accuracy on the classification (up to 25%, as in the case of an XLNet on the IMDB dataset). In some cases, by means of a suitable preprocessing strategy, even a simple Naïve Bayes classifier proved to outperform (i.e., by 2% in accuracy) the best performing Transformer. We found that Transformers and traditional models exhibit a higher impact of the preprocessing on the TC performance. Our main findings are: (1) also on modern pre-trained language models, preprocessing can affect performance, depending on the datasets and on the preprocessing technique or combination of techniques used, (2) in some cases, using a proper preprocessing strategy, simple models can outperform Transformers on TC tasks, (3) similar classes of models exhibit similar level of sensitivity to text preprocessing.}
}

@inproceedings{stark2000speechretrieval,
  title     = "{ASR satisficing: the effects of ASR accuracy on speech retrieval}",
  author    = {Litza Stark and Steve Whittaker and Julia Hirschberg},
  year      = {2000},
  booktitle = {6th International Conference on Spoken Language Processing (ICSLP 2000)},
  pages     = {vol. 3, 1069-1072},
  doi       = {10.21437/ICSLP.2000-720},
  issn      = {2958-1796},
  url       = {https://www.isca-archive.org/icslp_2000/stark00_icslp.html}
}

@book{tur2011slu,
    author="Tur, Gokhan and De Mori, Renato",
    title="{Spoken Language Understanding: Systems for Extracting Semantic Information from Speech}",
    publisher="John Wiley and Sons",
    year="2011",
    URL="https://cir.nii.ac.jp/crid/1130000794961797504"
}

@INPROCEEDINGS{wang2003indicator,
  author={Ye-Yi Wang and Acero, A. and Chelba, C.},
  booktitle={2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)}, 
  title="{Is word error rate a good indicator for spoken language understanding accuracy}", 
  year={2003},
  volume={},
  number={},
  pages={577-582},
  keywords={Error analysis;Natural languages;Speech recognition;Hidden Markov models;Robustness;Training data;Testing;Information systems;Credit cards;Databases},
  doi={10.1109/ASRU.2003.1318504}}

@article{wang2005slu,
  author={Ye-Yi Wang and Li Deng and Acero, A.},
  journal={IEEE Signal Processing Magazine}, 
  title="{Spoken Language Understanding}", 
  year={2005},
  volume={22},
  number={5},
  pages={16-31},
  keywords={Natural languages;Signal processing;Automatic speech recognition;Mathematical model;Pattern recognition;Speech recognition;Hidden Markov models;Humans;Bridges;Solids},
  doi={10.1109/MSP.2005.1511821}}

@inproceedings{zechner-waibel-2000-minimizing,
    title = "{Minimizing Word Error Rate in Textual Summaries of Spoken Language}",
    author = "Zechner, Klaus  and
      Waibel, Alex",
    booktitle = "1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2000",
    url = "https://aclanthology.org/A00-2025",
}

@inproceedings{zhu-etal-2024-zero,
    title = "{Zero-Shot Spoken Language Understanding via Large Language Models: A Preliminary Study}",
    author = "Zhu, Zhihong  and
      Cheng, Xuxin  and
      An, Hao  and
      Wang, Zhichang  and
      Chen, Dongsheng  and
      Huang, Zhiqi",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1554",
    pages = "17877--17883",
    abstract = "Zero-shot Spoken Language Understanding (SLU) aims to enable task-oriented dialogue systems to understand user needs without training data. Challenging but worthwhile, zero-shot SLU reduces the time and effort that data labeling takes. Recent advancements in large language models (LLMs), such as GPT3.5 and ChatGPT, have shown promising results in zero-shot settings, which motivates us to explore prompt-based methods. In this study, we investigate whether strong SLU models can be constructed by directly prompting LLMs. Specifically, we propose a simple yet effective two-stage framework dubbed GPT-SLU, which transforms the SLU task into a question-answering problem. Powered by multi-stage mutual guided prompts, GPT-SLU can leverage the correlations between two subtasks in SLU to achieve better predictions, which is greatly explored in the traditional fine-tuning paradigm. Experimental results on three SLU benchmark datasets demonstrate the significant potential of LLMs for zero-shot SLU. Comprehensive analyses validate the effectiveness of our proposed framework and also indicate that there is still room for further improvement of LLMs in SLU scenarios.",
}

