%\input{figures/framework}

\section{A Framework for Measuring \ENDow{}}
\label{sec_framework}

%Inspired by the vast research conducted on investigating the effect of transcription noise on downstream NLU tasks, our goal is to formulate a framework that systematically analyzes SLU pipelines. 
With the purpose of systematically analyzing SLU pipelines,
our framework's objective is to describe the behavior of downstream tasks as a function of the noise score (e.g., WER, which we use throughout the paper, but any transcription noise metric can be applied) and the type of noise in transcripts. 
%, and what cleaning methods effectively boost the outcomes.

The \textbf{input} to the framework is an SLU dataset $D = (T, O)$, where $T$ is a set of reference transcripts and $O$ are the respective expected outcomes. For example, a set of meetings and their respective summaries, for the task of meeting summarization.
%Note that some datasets also provide audio files along with the transcripts, denoted $A$. 

The framework consists of a pipeline (illustrated in \autoref{fig_framework}) which includes a text-to-speech (TTS) model to generate audio files for $T$; the acoustic noising method and intensity to apply on the audio; an ASR system for audio transcription; the transcript cleaning technique; the downstream task model; and the evaluation metrics for the task. The components in the pipeline are flexibly set according to the use-case being analyzed.

The framework \textbf{output}s a report on the behavior of the SLU pipeline at the different noise levels and with the cleaning techniques assessed (\S{\ref{sec_framework_analysis}}).

% \paragraph{Framework input.}
% The framework receives two inputs:

% \noindent
% 1. An SLU dataset $D = (T, O)$ that contains reference transcripts ($T$) and the respective expected outcome of each transcript instance ($O$). For example, a set of meetings and their summaries. Note that some datasets also provide audio files along with the transcripts, denoted $A$.

% \noindent
% 2. A configuration specifying the components of the SLU pipeline (\S{\ref{sec_framework_noise}} and \S{\ref{sec_framework_task}}). The pipeline includes: A text-to-speech (TTS) model to generate audio files for transcripts; the noising method to apply on the audio, and number of noising intensities to apply; an ASR system for audio transcription; the transcript cleaning techniques; the downstream task model; and the evaluation metrics for the task.

% \paragraph{Framework output.}
% The framework outputs a 
% %quantitative
% report on the behavior of the SLU pipeline at different noise levels and cleaning techniques (\S{\ref{sec_framework_analysis}}).


\subsection{Preparing Transcripts with Varying Noise}
\label{sec_framework_noise}

\paragraph{Creating initial audio files.}
Audio files are first created for the input transcripts, in case the SLU dataset lacks them (or when using a non-SLU dataset), or to begin the analysis with clean audio\footnote{That is, with clear speech, without background noise or overlapping speakers.} for greater control over the subsequent noising process.
%Audio files are first prepared in several scenarios: (1) when the input SLU dataset provides transcripts without source audio files; (2) when a non-SLU dataset is used to enrich data availability; (3) to start the analysis with clean audio files\footnote{That is, with clear speech, no background environmental sounds, and without overlapping speakers.} for greater control over the subsequent noising process.
%Many SLU datasets provide transcripts without source audio files. Analogously, non-SLU datasets can be leveraged for our setting to enrich analyses.
%In some SLU datasets, the source audio files are not available.
%, and only the transcripts are on hand.
%Also, starting the analysis with clean audio files\footnote{That is, with clear speech, no background environmental sounds, and without overlapping speakers.} can provide greater control over the noising process.
%Alternatively, one may wish to initiate the \ENDow{} analysis with clean audio files\footnote{That is, with clear speech, no background sounds, and without multiple speakers at a time.} in order to have more control over the noising process.
%In such scenarios,
The TTS system is executed on each input (transcript) in dataset $D$, resulting in the corresponding set of audio files $A$.
%transcript in $T$ of input dataset $D$. The result of this step is the set of audio files $A$.
%\footnote{Technical details for the pipeline are in Appendix \ref{sec_appendix_implementation}.}

\paragraph{Adding noise to audio files.}
Given the audio files $A$, each is acoustically impaired at $k$ levels to increase transcription difficulty, preferably under realistic acoustic conditions.
%Given the set of audio files $A$, each audio file is to be acoustically impaired at $k$ levels, with the purpose of making the speech difficult to transcribe at different severities, preferably based on realistic acoustic settings. 
To that end, reverberation (i.e., sound reflection, like echoing) is applied, and background sounds are added with increasing intensity (signal-to-noise ratio) \citep{wang2018speechsep}.
%, according to the input configuration.
This stage yields a collection of audio sets $\{A_i\}_{i=1}^k$ (and we define $A_0 = A$), where the severity of impairment increases as $i$ increases.

\paragraph{Transcribing audio files.}
The ASR model is then executed on the audio files in sets $\{A_i\}_{i=0}^k$, resulting in respective transcripts $\{\widehat{T_i}\}_{i=0}^k$. Overall, there are $k+2$ sets of transcripts for dataset $D$: the $k+1$ ASR-generated sets and reference set $T$. It is expected that as $i$ increases, $\widehat{T_i}$ will have a higher WER score (more errors) with respect to $T$.
%For each $A_i$, the configured ASR model is run on each of $A_i$'s audio files to generate the respective transcript. This stage results in a collection of ASR-generated transcripts $\{\widehat{T_i}\}_{i=0}^k$. Overall, there are $k+2$ sets of transcripts for dataset $D$: the $k+1$ ASR-generated sets, and the reference set $T$. It is expected that as $i$ increases, $\widehat{T_i}$ will have a higher WER score (more errors) with respect to $T$.

\paragraph{Cleaning transcripts.}
Each non-reference transcript (in all sets $\widehat{T_i}$) is partially repaired using one of $m$ cleaning techniques. This culminates in sets $\{\{\widehat{T_{i_j}}\}_{j=1}^m\}_{i=0}^k$, and $\widehat{T_{i_0}} = \widehat{T_i}$ (when no cleaning is performed on $\widehat{T_i}$), encompassing $(k+1)*(m+1)$ different levels and types of transcript noise.
%Each non-reference transcript (those in sets $\widehat{T_i}$) is partially repaired using $m$ different cleaning techniques, as configured. In all, there are $(k+1)*(m+1)$ non-reference transcript sets with different levels and types of noise. I.e., the prepared sets are $\{\{\widehat{T_{i_j}}\}_{j=0}^m\}_{i=0}^k$, where $\widehat{T_{i_0}} = \widehat{T_i}$ (when no cleaning is performed on $\widehat{T_i}$).


\subsection{Executing the Downstream Task}
\label{sec_framework_task}
Next, the task model is executed on each of the transcripts in the prepared transcript sets, producing the respective predicted outputs $\{\{\widehat{O_{i_j}}\}_{j=0}^m\}_{i=0}^k$, and $\widehat{O}$ for the reference transcripts $T$. The predicted outputs in each set $\widehat{O_{i_j}}$ and $\widehat{O}$ are then evaluated against the respective expected outcomes in $O$. 
Finally, this process culminates with the overall score of each dataset variant $\{\{s_{i_j}\}_{j=0}^m\}_{i=0}^k$ and $s$.\footnote{To clarify, $s$ is the score obtained on reference transcripts $T$, portraying a standard execution of the SLU task on input dataset $D$. Score $s_{i_j}$ is for one of the noisy dataset variants.}
%(with respective margins of error).

In addition, the WER score is computed for each transcript set $\widehat{T_{i_j}}$ with respect to references $T$. Accordingly, this produces WER scores $\{\{w_{i_j}\}_{j=0}^m\}_{i=0}^k$ (see Appendix \ref{sec_appendix_implementation_wer} for details). Notice that $T$'s WER is $0$. With the task scores and respective WER scores, we can now assess and compare the performance of the dataset variants.


\subsection{Analyzing the Results}
\label{sec_framework_analysis}

Each of the WER and task score-pairs $(w_{i_j}, s_{i_j})$ is a data point that can be plotted on a graph.
The curve $l_j = [(0, s)] \cdot [(w_{i_j}, s_{i_j})]_{i=0}^k$ describes the behavior of a task model as noise increases in the transcripts (as $i$ increases), when applying cleaning technique $j$ (or when no cleaning is enforced, at $j=0$).
%See an illustration of the graph in \autoref{fig_framework}.
These curves form a basis for analyzing the configured SLU pipeline, as explained next.
(See \autoref{fig_framework_graph} in the Appendix for visualization.)
%The line $[(0, s)]; [(w_{i_0}, s_{i_0})]_{i=0}^k$ (when $j=0$) describes the behavior of the SLU pipeline as noise increases in the transcripts, and without any enforced cleaning. For each cleaning technique $j$, the line $[(0, s)]; [(w_{i_j}, s_{i_j})]_{i=0}^k$ describes the corresponding behavior when cleaning is enforced. For example... \todo{forward reference to an example plot}.

\paragraph{Model performance vs. noise level.}
%A curve $l_j$ describes the behavior of the NLU task model as transcripts bear more errors.
%As noise accumulates the results on the downstream task exacerbate.
As transcript noise accumulates, NLU task model performance is expected to degrade.
One question to ask is: \textit{how much transcript noise can the task model tolerate before its performance is jeopardized?} To that end, we define the \textbf{\textit{noise-toleration point}} (NTP) as follows. For curve $l_j$, described by function\footnote{Note that the curve is not continuous since it is made up of several discrete segments. See Appendix \ref{sec_appendix_implementation_ntp} for details on how the noise-toleration point is computed.} $f_j$, and the respective upper and lower bound functions $f_j^{\text{upper}}$ and $f_j^{\text{lower}}$ (based on the margins-of-error), we define $l_j$'s noise-toleration point, $w^t_j$, as the WER score when $f_j^{\text{lower}}(0) = f_j^{\text{upper}}(w^t_j)$, i.e., 
the lowest WER at which the task score becomes statistically significantly lower than when transcripts have no noise, indicating a notable drop in task-model performance due to noise.
%the minimum WER score where the task score is statistically significantly smaller than the task score when there is no noise in the transcripts, marking a significant drop in task performance.
%See the illustration in \autoref{fig_framework}.
%Nevertheless, a task model has a \textit{noise-toleration point}. This is the extent of noise until which the task-scores are relatively stable, i.e., the task model can tolerate that amount of errors in the transcripts. This point also marks the onset of a more substantial downward trend of task-scores. We define the noise-toleration point as the noise level until which the task-score does not fluctuate more than a specified threshold. \todo{Formalize with an equation?} For example, \todo{refer to a pplot in the results and say what the point is...}

Another question to ask about the SLU pipeline is: \textit{how do different models behave comparatively, with respect to noise level?} The general behavior is approximated with the \textbf{\textit{area-under-the-curve}} (AUC), which can be compared between curves to judge which model is generally more tolerant to noise. Furthermore, by focusing on a certain region in the graph, the localized behavior is comparable. For example, in \autoref{fig_noclean_graphs}a, the GPT model is the better model at lower WER levels, but drops to the bottom rank at high WER levels.\footnote{The reliability of the analyses increases with the number of points constructing a curve (increasing $k$) and with a broader coverage of the WER score range (between 0 and 1).}
%more levels of noise that enrich a model's curve (larger value of $k$ and a broader range of word-error rates).

%The behavior of a curve changes between tasks, models and noise types, and the examination described yields a practical understanding of what to expect from a tested SLU solution.


\paragraph{Comparing cleaning techniques.}
Applying a cleaning technique on transcripts decreases the noise, and consequently shifts the plots leftward. Cleaning a transcript also essentially means that the \textit{type} of noise changes, and therefore the task model reacts differently to the errors in the transcripts, potentially altering the behavior of the curves altogether.
The point $(w_{i_j}, s_{i_j})$ with respect to point $(w_{i_0}, s_{i_0})$ portrays how much ``effort'' is required (the decrease in WER: $w_{i_0} - w_{i_j}$) in order to change the task score from $s_{i_0}$ to $s_{i_j}$. The effect of each cleaning method $j$ varies, and therefore all $l_j$s are compared with respect to $l_0$ (e.g., see \autoref{fig_cleaning_graphs}). Ultimately, an effective cleaning technique should increase the task scores with minimum effort.

Formally, let $\Delta w_{i_j} = w_{i_0} - w_{i_j}$ be the change in WER for noising level $i$ and cleaning method $j$, and $\delta s_{i_j} = (s_{i_0} - s_{i_j}) / s$ be the respective relative\footnote{The change in task-score is normalized by the score at WER=0 to get the relative change. The change in WER is already on a 0-to-1 scale, and is not further normalized.} change in the task-score. The pointwise effectiveness score of cleaning technique $j$ at noise-level $i$ is measured as $e_{i_j} = \delta s_{i_j} / \sqrt{\Delta w_{i_j} + \epsilon}$.\footnote{We applied a square root transformation on the \textit{effort} ($\Delta w_{i_j}$) to reduce the impact of the larger changes at noisier levels, and to increase the weight of the change in task score ($\delta s_{i_j}$). $\epsilon$ is a minuscule value to prevent division by zero.} Finally, we measure the \textit{\textbf{cleaning-effectiveness score}} (CES) of cleaning method $j$ with the average: $\frac{1}{k+1} \sum_{i=0}^k e_{i_j}$. The higher the score, the better the overall improvement in the downstream task with a lower effort of cleaning. A score of 0 means that the cleaning procedure had no effect on the task-model's results, and a negative score means that there was a deterioration of task results, on average. 

The CES metric captures the two objectives of a cleaning technique: heightened task results for lesser effort.
%\todo{try out the acceleration of the changes -- get the derivative of the regression line formed by the delta\_y / delta\_x line -- the larger the acceleration, the better.}
The metric suggests how comparably effective a cleaning method is for the data and task-model in question. As such, it compares the effects of different \textit{types} of noise in the transcripts, as we exemplify in our experiments in Section \ref{sec_results}.




%The more levels of impairment, the more precise the \ENDow{} assessment will be