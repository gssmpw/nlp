\section*{Limitations}

In our experiments, the pipelines are initiated with relatively clean and clear audio files, and the subsequent acoustic deterioration is done in a specific manner (reverberation and background sounds). Other acoustic settings are indeed possible for initiating the SLU pipeline, e.g., with a low-resourced lingual dialect, different speaker voices per turn, overlapping speech, microphone settings, and many other parameters. Our framework is robust to these variants, and the purpose of our experiments is to exemplify the utility of the framework.

Similarly, our experiments are limited to the configurations we defined, for demonstrating the framework. Other configurations could involve non-English languages, different tasks, models and SLU/NLU datasets. The resulting analyses could yield findings that are different from ours, which reiterates the need for a robust framework like ours.

The cleaning techniques we used depend on the reference transcript in order to identify the word/phrase types that we want to include in our analysis. Our experiments show how \textit{different types} of errors affect a downstream task. A cleaning technique can also be one that is used in practice without dependence on the reference transcript. In the latter case, our framework would indicate the effectiveness of a transcript-cleaning component within an SLU pipeline.


%\paragraph{Insights from our experiments.}

%The insights from our experiments are based on four task models that may be seen as eq

We emphasize that the behavior of a graph depends on the task metric applied, and the resulting analysis can therefore differ when using different metrics for the same task and data. When insights are gathered with the framework, it is important to strongly consider the metric used, or use several metrics to paint a fuller picture.