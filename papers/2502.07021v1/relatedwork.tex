\section{Related Work}
Optimal Transport (OT) has seen a surge in prominence since the introduction of the entropic regularization approach by Cuturi, which significantly reduced computational complexity and paved the way for scalable solutions~\cite{cuturi2013sinkhorn}. Building on this method, often referred to as the Sinkhorn–Knopp algorithm, subsequent works have explored various techniques to increase efficiency, including multi-GPU parallelization, low-rank approximations, and stochastic gradient approaches for large-scale OT problems~\cite{genevay2016stochastic, altschuler2019kmeans}. %These developments underscore the need for distributed and federated frameworks, especially as data size and dimensionality grow.

Federated learning frameworks, meanwhile, have garnered attention for decentralizing data storage and computation, thereby maintaining privacy while leveraging distributed computational resources~\cite{konecny2016federated}. The primary focus in early federated learning literature was on classical supervised methods such as gradient descent for linear models~\cite{mcmahan2017communication}, but more recent works have broadened the scope to tasks like matrix factorizations, kernel methods, and even generative modeling. However, the intersection of federated paradigms with OT problems—particularly in entropic regularization settings—remains relatively unexplored.

In the context of distributed OT, challenges such as ensuring consistency across partial marginals and mitigating stale updates when clients operate at different speeds have spurred investigations into both synchronous and asynchronous update schemes. The synchronous case typically involves each client finishing its local computation step before a global synchronization, thus guaranteeing a uniform iteration count. In contrast, asynchronous protocols allow each client to update and transmit partial information at different times, potentially reducing idle time but introducing complexities in convergence analysis~\cite{recht2011hogwild}. More recent advances in asynchronous parallel computing~\cite{lian2018asynchronous} have laid theoretical groundwork for analyzing possible convergence rates, even under communication delays and partial updates.

Bringing these themes together, our work aligns with the growing body of research seeking to balance communication efficiency, fault tolerance, and local GPU-accelerated processing factors that are particularly salient for large-scale, real-world datasets (see for example ~\cite{chetlur2014cudnn}). By federating the Sinkhorn–Knopp algorithm, we merge the dual goals of (i) scaling OT computations beyond the memory constraints of a single machine or data center node, and (ii) respecting constraints where data remain partitioned across geographically or administratively distinct clients. Our investigation also complements broader efforts toward privacy-preserving analytics (e.g., differential privacy layers applied to local distribution vectors) and domain-specific distributed setups (such as financial risk modeling). Consequently, this work situates itself at the intersection of large-scale OT computations, federated learning, and parallel algorithm design, contributing theoretical and empirical insights on how to effectively adapt Sinkhorn iterations to distributed environments.