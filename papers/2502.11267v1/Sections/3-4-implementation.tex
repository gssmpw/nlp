%\kenneth{TODO: Here we mention (1) what framework you used to implement Google Sheets add-on, (2) how do you convert Spreadsheet's content into a prompt, and (3) what LLM (which version exactly) you used and how did you send request (batch? or each data instance is one request?)--- Maybe talk about latency issue here a bit.}


\paragraph{Developing Google Sheets Add-On.}
%\kenneth{How do people built Google Sheets add-ons? Did we use an web server? Where do we store our data?}\steven{done}
\sloppy
\system utilized Google Sheets as its main platform, leveraging the convenience and functionality of its spreadsheet capability. The Google Sheets add-on was implemented in Google App Script, with Google Cloud Service serving as a back-end to store all action logging files. User-specific data, such as OpenAI information, was securely stored in user properties tied to individual email accounts, ensuring privacy protection. 

\paragraph{Converting a Spreadsheet's Content into a Prompt.}
Once users click on the ``Start Annotation'' button (Figure~\ref{fig:system-interface-step-3-4-kenneth}), \system will first collect all questions and answers from the ``Context'' tab and send a request to GPT-4o to generate an instructional prompt (Table~\ref{tab:instruction-prompt}). Next, \system will merge this generated prompt with rules and definitions from ``Rule Book'' and available gold standard labels from the ``Shots'' tab to create an annotation prompt (Table~\ref{tab:main-prompt} and Table~\ref{tab:main-multi-prompt}). Finally, \system will use this prompt to annotate all data instances. 

\paragraph{Interacting with the LLM through an API}
In this paper, we utilized OpenAI's \texttt{gpt-4o-2024-05-13} model for our study~\cite{openai2024gpt4o}.
%\kenneth{TODO: Add citation} \steven{done}
Technically, this LLM can be replaced by any other model that offers an API compatible with the ChatGPT-4 specification. 
In our implementation, we group all data instances with the same Group ID and send them in a single API request.
%In our current implementation, we did not batch requests; instead, we sent an individual API request for data instances with the same group ID.
%\kenneth{Is this accurate?}\steven{we sent by group ID}
%Future versions of \system could potentially benefit from batching to reduce latency.








