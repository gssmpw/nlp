In addition to addressing the main research questions, a post-study survey (Appendex~\ref{sec:post-question-survey}) consisted of twenty-two questions, including seven Likert scale ratings and fifteen free-text responses from participants provided valuable insights on both ``prompting in the dark'' practices and our system.
We summarize these insights in this section.

%\kenneth{I re-organized this section. Please take a look.}

\subsection{Two Variables Impacting Participant Ratings\label{sec:two-var-on-rating}}
% \alan{Two variables impacting participant ratings?}
Figure~\ref{fig:user-rating} displayed the seven Likert scale rating responses by participants. The seven survey questions can be categorized into seven different categories. 
Appendix~\ref{app:two-var-on-rating} shows
the survey questions and the accompanying categories were rated on a seven-point Likert scale. 

\begin{comment}


listed below:

\begin{itemize}
    \item 
    \textbf{(Q1) Understandable}: The annotation task was easy to understand.

    \item
    \textbf{(Q2) Ease of Use}: The annotation tool is easy to use.

    \item
    \textbf{(Q4) Intuitive System}: The interface of the annotation system is intuitive.

    \item
    \textbf{(Q5) Performance Satisfaction}: How satisfied are you with the performance of the system?

    \item
    \textbf{(Q6) Prompt Improvement}: This tool was helpful in improving my prompt. 
    
    \item
    \textbf{(Q7) Process Efficiency}: Using this tool made the process of prompt engineering more efficient.

    \item
    \textbf{(Q19) Task Completion}: I completed the annotation tasks efficiently.

\end{itemize}
    
\end{comment}

%Figure~\ref{fig:user-rating} shows the participant's rating across different conditions.
\subsubsection{Participants reviewing 10 instances reported higher satisfaction ratings.}
Figure~\ref{fig:sample-size-rating} compares participants who reviewed 10 instances per iteration with those who reviewed 50.
Both groups provided similar ratings for system ease of use, system intuitiveness, and efficiency in processing prompt engineering, with comparable variation.
However, participants who reviewed 10 instances found the annotation tasks more difficult to understand compared to those who reviewed 50. Comparatively, participants who reviewed 10 instances reported higher levels of satisfaction with their performance, a stronger sense of prompt improvement, and better task completion rating. This could be attributed to their minimal modifications to the rule.
% We performed a KS test on all rating categories and no significant difference was found between two groups, indicating that the observed difference did not reach statistical significance.
It is noteworthy that we performed a Kolmogorov-Smirnov (KS) test on all rating categories, and no significant difference was found between the two groups, indicating that the observed difference did not reach statistical significance.

% \steven{Only the performance ratings from participants showed a significant difference between two instances groups based on the t-test (p-value=0.031)}

\subsubsection{Participants without LLM explanations rated the system as more intuitive, effective, and satisfying.}
Figure~\ref{fig:explanation-rating} shows the comparison of ratings between participants with and without LLM explanations.
Participants with LLM explanations found the annotation tasks more challenging, rated the system as less intuitive and harder to use, and viewed it as less effective in improving prompts, also with greater variation in their ratings. In contrast, participants without LLM explanations expressed higher level of satisfaction with the system performance, believing the tool improved prompt engineering efficiency and task completion effectiveness.
% We conducted a KS test on all ratings from participants and no significant difference was found between the two groups, suggesting that the observed difference were not statistically significant.
Notably, we conducted a Kolmogorov-Smirnov (KS) test on all participants' ratings, and no significant difference was found between the two groups, suggesting that the observed differences were not statistically significant.


\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{Figures/Post-Survey/user_rating_bar_chart_instance.png}\Description{This subplot is for the data sample group, based on participants’ post-survey responses. Each subplot features bar charts comparing two settings: 50 instances vs. 10 instances. Each bar is accompanied by a confidence interval displayed at the top.}
        \caption{Participants' ratings of the system and the annotation task, comparing those who accessed 10 instances per iteration to those who accessed 50 instances.}
        \label{fig:sample-size-rating}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{Figures/Post-Survey/user_rating_bar_chart_explanation.png}\Description{This subplot is for the explanation group, based on participants’ post-survey responses. Each subplot features bar charts comparing two settings: no explanation vs. explanation. Each bar is accompanied by a confidence interval displayed at the top.}
        \caption{Participants' ratings of the system and the annotation task, comparing those who utilized LLM explanations to those who did not.}
        \label{fig:explanation-rating}
    \end{subfigure}
    \caption{Participants' rating of the system and the annotation task. Each rating category refers to one question in the post-study survey. 
    \textbf{(Q1) Understandable}: The annotation task was easy to understand; 
    \textbf{(Q2) Ease of Use}: The annotation tool is easy to use;
    \textbf{(Q4) Intuitive System}: The interface of the annotation system is intuitive;
    \textbf{(Q5) Performance Satisfaction}: How satisfied are you with the performance of the system? 
    \textbf{(Q6) Prompt Improvement}: This tool was helpful in improving my prompt; 
    \textbf{(Q7) Process Efficiency}: Using this tool made the process of prompt engineering more efficient; 
    \textbf{(Q19) Task Completion}: I completed the annotation tasks efficiently.}
    \label{fig:user-rating}
\end{figure*}

%\steven{added user rating.}\kenneth{(1) Font for axis titles and the title of the figure are too big, (2) Make the figure wider so that the x-axis labels do not need to rotate (use newline for x-labels if possible), (3) this is kinda extra: can we break it down in two ways (a) with/without explanations and (b) smaller/bigger sample size.}\steven{done}

\subsection{Is \system Useful?}
\input{Sections/6-1-usefulness}


\subsubsection{Prompting in the dark is hard, as participants lacked confidence in their labels.}
%\paragraph{Participants lacked confidence in labeling}
Without a comprehensive understanding of the entire dataset, participants found it challenging to generate suitable labels.
P19 mentioned, ``\textit{I am not confident about the label}''.
P12 pointed out that, ``\textit{When I need to express sentiment, I tend to be more reserved and avoid extremes. So, when labeling data, I usually prefer to choose negative/[positive] rather than extremely negative/[extremely positive]}''

%We also found that,
%\paragraph{Difficulty in capturing the full picture with limited samples}
%with participants engaging with only around 10 instances per round, they can overanalyze the limited tweets, resulting in a narrow refinement of their rule books. 
%This limitation also affected the gold standards labels for LLM learning. 
%P19 suggested, ``\textit{I didn't see any `Extremely Positive' in each verification round, but I saw five for now in the evaluation set.}''.

% \paragraph{Hard to revise Rule Book}
% Some participants preferred to write detailed label definitions at the start, which resulted in fewer rule revisions in the later rounds. Alternatively, they tended to add gold shots than editing rules.




% Some participants had difficulty understanding how to respond to context questions regarding the annotation task. \alan{clarify}They mistakenly answered the question by describing the system's workflow rather than focusing on the Twitter task. P3 and P8 suggested having more context tutorials for participants to understand the annotation task. 
% P9 pointed out, ``the `Start Annotation' button is hard to find.''.
%\kenneth{this is probably, in part, due to the nature of the problem}

%\subsection{Challenges and Pain-points}
%\kenneth{The following basically said we found the problems at the beginning of our study and what we did to mitigate them.}
% \paragraph{Difficulty of navigating the interface}
% The participants’ experience was not smooth, with most challenges centered around the workflow, such as moving from tab to tab and the complexity of the system features. 
% The most frequent issues participants encountered involved confusion with the system’s annotation procedure, which sometimes indicated users getting used to the interface.\alan{clarify} The issue often disappeared once users became more familiar with the system.
% Some participants (P2 and P7) struggled to respond to context questions\alan{?}, which were intended to gather information related to the Twitter sentiment analysis, such as the purpose and usage of the annotated data. However, they mistakenly focused on describing the system’s workflow rather than focusing on the annotation task.
% Reflecting on user feedback, we improved the tutorial by switching from a video format to a step-by-step manual walkthrough, ensuring participants could fully understand each tab and its purpose while also allowing them to ask questions at any point.\alan{does this actually help?}
% To help participants better understand the Twitter Sentiment task, we provided template answers that they could use as a reference to create their own responses or adopt directly.


%\subsubsection{Participants with 50 instances per iteration}

%\subsubsection{Participants with 10 instances per iteration}
 





%\kenneth{An important context here is: Our study results showed that (1) NOT showing LLM explanations is better, and (2) showing LLM explanations make users' labels more similar with each other, i.e., LLMs tailor users to be more like LLMs.}

% P10 liked ``The LLM explanations for every tweet after the levels were filled out '' the most.

% P15 mentioned, ``The explanation from GPT gave my some insights to modify my rules.'' and ``I think GPT's explanation of the tweets is very helpful and it may help me to improve the accuracy of human annotation.''

% \paragraph{Participants influenced by LLM explanations.} 
% For the first four participants (P1-P4), the system offered an LLM explanation option during the annotation process. We observed that each participant reviewed these explanations thoroughly, even though they were instructed to verify the LLM labels using their own judgment. 

% %\subsubsection{Participants with LLM Explanations}
% %\subsubsection{Participants tended to be impacted by LLM Explanations} 
% Participants tended to review LLM explanations thoroughly during the verifying process, even though they were instructed to verify the labels using their own judgment. 

%\paragraph{A few participants turned off the LLM Explanation option}

%\subsubsection{Participants without LLM Explanations}



\subsection{Users' Suggested Features}

\subsubsection{More automated supports for rule creation.}
Participants expressed concerns about creating rules that effectively suit the labeling task at hand. As a result, support for Rule Book creation is a welcome addition.
P2 remarked, ``\textit{It was hard to set the right rules,}'' while P3 suggested providing initialized instructions for labels and rules to ease the process. Additionally, participants (P1, P3) proposed that new rules could be automatically generated based on Gold Shots, existing rule books, and human explanations, streamlining the rule creation process.

%\kenneth{Can we make subsubsection title a complete sentence?}
%\alan{something like this?}
\subsubsection{Shorter LLM explanations for easier consumption.}

Although LLM-generated explanations received positive feedback from participants, there was concern about the length of these explanations. Many felt that the explanations were too long and could be difficult to consume. P1 recommended, ``\textit{It would be better if the LLM explanation could be shorter.}'' emphasizing the need for more concise outputs to improve user experience.





% P9 pointed out, ``the `Start Annotation' button is hard to find.''.


% \paragraph{Simplified Interfaces and Workflows.}
% Some participants suggested hiding uncommon features (P17) or displaying the function descriptions only when hovering over them (P20).

%\paragraph{Other suggestions}\steven{Not related to the annotation system}
% (P1)If there's a graphical UI, the system will be more accessible to laypersons without the HCI background. Currently, the system is too flexible to get lost in the interactions.
%(P3) More tutorials on the data annotation task.



%\subsection{Participant In-Lab Annotation Process Analysis}



%\subsection{Participant Self-Reported Response Analysis}
