\begin{abstract}
Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering?
Do users actually get closer to their desired outcome over multiple iterations of their prompts? 
These questions are crucial when no gold-standard labels are available to measure progress. 
This paper investigates a scenario in LLM-powered data labeling, ``prompting in the dark,'' where users iteratively prompt LLMs to label data without using manually-labeled benchmarks.
We developed \system, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets.
Through a study with 20 participants, we found that prompting in the dark was highly unreliable---only 9 participants improved labeling accuracy after four or more iterations.
% \kenneth{Update numbers}\steven{Added! Will update after user study finished}
Automated prompt optimization tools like DSPy also struggled when few gold labels were available.
%Our findings highlight the importance of gold labels and automated support in human prompt engineering, informing future tool design.
Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.


\end{abstract}


%------------------- dead kitten --------------
\begin{comment}



    The advent of the Large Language Model (LLM) allowed people to leverage its outstanding power to perform data annotation tasks. However, manually annotating large-scale datasets from scratch remains time-consuming and inefficient. 
    While existing solutions address this issue, they often require significant technical expertise and system configuration for deployment. 
    This paper introduces the \system, an intuitive LLM-powered add-on for Google Spreadsheets that simplifies single-class annotation tasks.
    \system allows users to iteratively guide LLMs with minimal effort, eliminating the need for complex system setups.
    In our user study, assistance features, such as providing LLM explanations and exploring more data instances, improved human-guided LLM performance. 
    However, inconsistencies in human guidelines led us to experiment with an automated prompt fine-tuning tool, DSPy, though its performance varied across participants. 
    We propose the concept of ``prompt with light,'' encouraging users to have reference data and assistance tools available throughout the annotation iteration process.
    % Through a user study, we examine the effectiveness of human involvement in guiding LLMs with \system for a sentiment task. The quantitative analysis found that human inputs were unreliable in guiding LLM unless with extreme effort. Inspired by the findings, we applied DSPy to improve human prompts for each iteration. 
    
\end{comment}