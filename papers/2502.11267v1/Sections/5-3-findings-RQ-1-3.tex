% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \begin{table}[t]
% \centering
% \footnotesize
% \begin{tabular}{@{}lcccccccc@{}}
%  & \multicolumn{4}{c}{\textbf{LLM Explanations Shown}} & \multicolumn{4}{c}{\textbf{No LLM Explanations Shown}} \\ \cmidrule(l){2-9} 
%  & \multicolumn{2}{c}{\textbf{ACC$\uparrow$}} & \multicolumn{2}{c}{\textbf{MSE$\downarrow$}} & \multicolumn{2}{c}{\textbf{ACC$\uparrow$}} & \multicolumn{2}{c}{\textbf{MSE$\downarrow$}} \\ \midrule
% \multicolumn{1}{c}{} & \textbf{Avg. (SD)} & \textbf{\begin{tabular}[c]{@{}c@{}}\%Participant\\ Improved\\ Over Initial\end{tabular}} & \textbf{Avg. (SD)} & \textbf{\begin{tabular}[c]{@{}c@{}}\%Participant\\ Improved\\ Over Initial\end{tabular}} & \textbf{Avg. (SD)} & \textbf{\begin{tabular}[c]{@{}c@{}}\%Participant\\ Improved\\ Over Initial\end{tabular}} & \textbf{Avg.(SD)} & \textbf{\begin{tabular}[c]{@{}c@{}}\%Participant\\ Improved\\ Over Initial\end{tabular}} \\ \midrule
% \textbf{Initial Prompt} & .586 (.065) & - & .614 (.230) & - & .498 (.109) & - & .950 (.612) & - \\ \midrule
% \textbf{1st Revision} & .558 (.066) & 20\% & .624 (.261)  & 40\% & \underline{\textbf{.514 (.104)}} & 50\% & .966 (.705) & 40\% \\
% \textbf{2nd Revision} & .570 (.063)  & 20\% & .632 (.196) & 60\% & \underline{\textbf{.514 (.074)}} & 60\% & 1.034 (.655) & 30\% \\
% \textbf{3rd Revision} & .560 (.074) & 30\% & .646 (.310) & 50\% & \underline{\textbf{.538 (.092)}} & 70\% & 1.000 (.737)  & 50\% \\
% \textbf{4th Revision} & .556 (.076) & 20\% & .646 (.294) & 40\% & \underline{\textbf{.550 (.096)}}  & 70\% & .974 (.729) & 40\% \\ \midrule
% \textbf{\begin{tabular}[c]{@{}l@{}}End of Session\\ (Avg \#Iter=4.75)\end{tabular}} & .556 (.079) & 20\% & .674 (.275) & 40\% & \underline{\textbf{.536 (.091)}} & 70\% & .956 (.620) & 50\%
% \end{tabular}
% \caption{Comparison of ACC and MSE between participants with and without access to LLM explanations during the labeling process. Participants without access to LLM explanations showed improvement in accuracy over multiple revisions, while those with access did not exhibit the same level of improvement. Improvements over the initial prompt are bolded and underlined.}
% \label{tab:results-llm}
% \end{table}

% \input{Tables/Explanation/pure-participant-explanation-detail}
\input{Tables/LLM-Explanation/table-new-explanation}



% \begin{figure}[t]
%     \centering
%     \begin{subfigure}{0.48\textwidth}
%      \includegraphics[width=\linewidth]{Figures/Results/Explanation-Result/diff_bar_ACC_explanation.png}
%     \caption{The improvement (difference) in ACC.}
%     \label{fig:acc-participants-explanation-or-not}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.48\textwidth}
%     \includegraphics[width=\linewidth]{Figures/Results/Explanation-Result/diff_bar_MSE_explanation.png}
%     \caption{The improvement (difference) in MSE.}
%     \label{fig:mse-participants-explantation-or-not}
%   \end{subfigure}
%   \caption{Comparison of improvement (difference) in ACC and MSE over the initial prompt between the Explanation and No-Explanation groups. Participants without access to LLM explanations improved their accuracy over multiple revisions, while those with access did not.}
%   \label{fig:average-acc-mse-llm-explanation-performance}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}{0.48\textwidth}
%      \includegraphics[width=\linewidth]{Figures/Results/Explanation-Result/diff_bar_ACC_explanation_new.png}
%     \caption{The improvement (difference) in ACC.}
%     \label{fig:acc-participants-explanation-or-not-new}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.48\textwidth}
%     \includegraphics[width=\linewidth]{Figures/Results/Explanation-Result/diff_bar_MSE_explanation_new.png}
%     \caption{The improvement (difference) in MSE.}
%     \label{fig:mse-participants-explantation-or-not-new}
%   \end{subfigure}
%   \caption{Comparison of improvement (difference) in ACC and MSE over the initial prompt between the Explanation and No-Explanation groups. Participants without access to LLM explanations improved their accuracy over multiple revisions, while those with access did not. \steven{removed the old version, this is newer version after moving participants to no llm group}}
%   \label{fig:average-acc-mse-llm-explanation-performance-new}
% \end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{Figures/Results/new-diff-bar/new_diff_bar_MSE_explanation_v2.png}\Description{It contains 2 subplots illustrating the effect of LLM explanation on ACC and MSE across iterations. The bars represent performance changes relative to the initial prompt, with improvements highlighted by a thick red border.}
  \caption{Comparison of improvement (difference) in ACC and MSE over the initial prompt between the Explanation and No-Explanation groups. Bars with red borders indicate positive improvements over the initial prompt's outcome. Participants without access to LLM explanations improved their accuracy over multiple revisions, while those with access did not.}
  \label{fig:diff-bar-average-acc-mse-llm-explanation-performance-new}
\end{figure*}



%new added explnation and 50 instance figures


%% end adding

\subsubsection{Participants without access to LLM explanations improved their accuracy over multiple revisions, while those with access did not.}
Table~\ref{tab:results-llm-new} and Figure~\ref{fig:diff-bar-average-acc-mse-llm-explanation-performance-new} show a comparison between participants with and without access to the LLM's explanations during the labeling process. The detailed breakdown is shown in Figure~\ref{fig:average-acc-mse-llm-explanation-performance-new}.
%\kenneth{TODO Steven: Update Table references} \steven{done}
%After four revisions, 
At the end of the session (\ie, four or more revisions),
7 out of 12 participants without access to explanations improved their accuracy, while only 2 out of 8 participants with access to explanations showed improvement.
%\kenneth{Is this after four revisions or at the end of the session?} \steven{both after 4th and at end session}
On average, the group without access saw improved accuracy over the initial prompt with each of the four revisions, whereas the group with access to explanations experienced a decline in accuracy across all revisions.

In terms of MSE, there was no significant difference between the two groups in the number of individuals who showed improvement. 
In fact, both groups saw an increase in MSE during the revision process.

As with the results related to data sample size (Section~\ref{sec:rq-1-2}), we are aware that participants with access to LLM explanations started with higher initial performance; this initial advantage occurred before the participants reviewed the labeling results and explanations.
Our analysis focuses on performance changes across iterations rather than the absolute performances.

\paragraph{Significant Tests.}
We conducted eight linear mixed-effects models to examine the effect of iteration across four different conditions. 
The dependent variables were ACC and MSE, and participants were treated as random effects.
Under conditions where participants \textbf{did not have access to LLM explanations,} we observed a significant increasing trend in accuracy with each iteration ($\beta$=0.010, p-value=0.027*). 


%Conversely, 
%In the condition where participants \textbf{reviewed only 10 instances per iteration}, we found a significant increasing trend in MSE as the iterations progressed ($\beta$=0.019, p-value=0.043*). 


%Participants with access to the LLM explanation were labeled as \textbf{LY}, while those without access were labeled as \textbf{LN}. 
%Table~\ref{fig:average-acc-mse-llm-performance} show the number of participants whose labeling accuracy and MSE improved, declined, or remained unchanged before and after using \system. Among 10 participants under LN, 7 participants improved their labeling accuracy, while only 2 participants under LY had improvement. 
%For labeling MSE, there was no difference between LY and LN participants in the number of individuals who showed improvement.


\subsubsection{Showing LLM explanations reduced labeling variation}
We observed that providing LLM explanations to participants led to more consistent labeling, as participants' labels became more similar to each other. 
Those with access to the LLM explanations had a higher Cohen's Kappa (0.333, SD=0.039), as well as higher Spearman (0.556, SD=0.053) and Kendall (0.504, SD=0.049) correlations compared to the group without access, whose Kappa was 0.193 (SD=0.047), Spearman 0.492 (SD=0.094), and Kendall 0.433 (SD=0.084).
%\kenneth{TODO Steven: Update the numbers.}
Additionally, Table~\ref{tab:results-llm-new} shows that the standard deviations (SD) of both ACC and MSE were systematically lower in the group with access to LLM explanations.
%\kenneth{TODO Steven: Update the reference.}\steven{done}

This suggests that \textbf{rather than users tailoring the LLM's behavior to their individual preferences, the LLM---through its explanations---encouraged users to align with its behavior}.


%\subsubsection{LLM explanations guides users, not users guide LLMs. (variation is bigger!)}
%We calculated Kappa under different conditions. 
%We found that participants who explored more data instances had the highest Kappa value of 0.135 (SD=0.073). 
%Participants who had access to the LLM explanation achieved the highest average inter-condition Kappa value of 0.333 (SD=0.039).





\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/Results/new-similarity/Rule-Simiarity-Metrics-data-sample.png}\Description{This is a subplot presenting rule similarity metrics for data sample and explanation group. Each subplot includes two density graphs representing normalized edit similarity and semantic similarity. For the data sample group, participants reviewing 50 instances per iteration tend to exhibit lower similarity within their own rules.}
        \caption{Data Sample Similarity Metrics}
        \label{fig:data-similarity}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/Results/new-similarity/Rule-Simiarity-Metrics-explanation.png}\Description{This is a subplot presenting rule similarity metrics for data sample and explanation group. Each subplot includes two density graphs representing normalized edit similarity and semantic similarity. In the explanation group, participants without access to LLM explanations generally show lower similarity within their own created rules.}
        \caption{Explanation Similarity Metrics}
        \label{fig:explanation-similarity}
    \end{subfigure}
    \caption{Comparison of Rule Similarity Metrics for Data Samples and Explanations. Overall, participants in the 50-instance group or those without access to LLM explanations were more likely to modify their rules.}
    \label{fig:rule-similarity-metrics}
\end{figure*}


\subsection{Additional Analysis: How Does Each Variable Influence Rule Editing?} %\steven{Rule Cos Sim}

Building on the impact of the two variables on prompting outcomes (RQ 1-2 and 1-3), a key follow-up question is \textit{why} these variables affect the outcomes differently. 
To explore this, we examined how the sample size per iteration and the presentation of LLM explanations influence how users edit the labeling rules---one of the main components of the final prompt---in \system.
For each prompt collected, we compiled all rules written by participants in the Rule Book sheet into a single string. 
We then calculated the sentence-level similarity between prompts from consecutive iterations for each session (\eg, between the initial prompt and iteration 1, iteration 1 and iteration 2, and so on).
Using \citet{huang2023ting}'s analysis method, we measured similarity in two ways:
(1) \textbf{Normalized Edit Similarity}: Calculated as $(1-Normalized~Edit~Distance)$,
where higher scores indicate greater similarity~\cite{yujian2007normalized, luozhouyang_python_similarity}.
(2) \textbf{Semantic Similarity}: Measured as the cosine similarity between semantic representations generated with Sentence-BERT~\cite{reimers-2019-sentence-bert}.
Each participant yielded eight similarity scores (2 similarity metrics $\times$ 4 pairs).
We used Kernel Density Estimation (KDE) to visualize the distribution of similarity between rules across consecutive iterations, as shown in Figure~\ref{fig:rule-similarity-metrics}. 
A similarity score of 1.0 indicates no changes, while 0.0 represents substantial modifications. 
Each chart compares participants in the 10-instance group to the 50-instance group or those with versus without access to LLM explanations.

\subsubsection{Larger rule changes were linked to better prompting outcomes.}
Our analysis revealed an interesting pattern: 
conditions that had better prompting outcomes---showing more data items, or not displaying LLM explanations---tended to have \textit{lower} similarity between labeling rules across consecutive iterations. 
In Figure~\ref{fig:rule-similarity-metrics}, the KDE curves for the 50-instance group (Figure~\ref{fig:rule-similarity-metrics}a) and the no-explanation group (Figure~\ref{fig:rule-similarity-metrics}b) skew further left compared to their counterpart conditions, regardless of the similarity metric. 
This indicates that participants in these settings---when seeing more data items, or when not having access to LLM explanations---made \textit{larger} changes to the rule books.
% Using the Kolmogorov-Smirnov (KS) test, only one significant difference was found in the normalized edit similarity between the group with LLM explanation access and the group without (p-value=0.031*). Specifically, participants without access to LLM explanations made significantly more frequent and substantial revisions to their rules than participants who had access to LLM explanations.
Using the Kolmogorov-Smirnov (KS) test, only one significant difference was found in the normalized edit similarity between the group with LLM explanation access and the group without (p-value=0.031*). Specifically, \textbf{participants without access to LLM explanations made significantly more frequent and substantial revisions to their rules} than participants who had access to LLM explanations.
This finding suggests an intriguing implication for human-LLM interaction: 
proactive and frequent revisions during iterative prompt refinement lead to better outcomes compared to making fewer revisions.
Encouraging meaningful revisions in prompting-in-the-dark scenarios---where no gold labels are available to guide or ``reward'' users---presents an interesting challenge for HCI research.

%when participants actively make more revisions during iterative prompt refinement, the outcomes improve. 
%In other words, humans have the ability to move closer to their intended outcomes in prompting-in-the-dark scenarios, though some conditions may not sufficiently encourage them to act on these abilities.




















%-------------- dead kitten ---------------

\begin{comment}
\paragraph{Rule Similarity Sample Size}
In the two charts in Figure~\ref{fig:data-similarity}, the curves for participants in the 50-instance group are flatter and skewed to the left, indicating that their rules tend to differ more from those in the previous iteration.
This suggests that users in this group are likely to make more edits to their rules.
Furthermore, participants in the 10-instance group exhibit a higher density near similarity values of 1.0 compared to those in the 50-instance group. This suggests that participants with access to fewer data samples per iteration are more likely to leave their rules unchanged. 

\paragraph{Rule Similarity Explanation} In the two charts shown Figure~\ref{fig:explanation-similarity}, the curves for participants did not have access to LLM explanations are flatter and skewed to the left.
This indicates that their rules have higher likelihood differ from those in the previous iteration, suggesting that these participants are more likely to make modifications to their rules.
Moreover, at similarity values near 1.0, participants with access to LLM explanations exhibit a higher density compared to those without access. This suggests that participants with LLM explanations are more likely to leave their rules unchanged.








We visualized the similarities of rule editing using Kernel Density Estimation (KDE), as shown in Figure~\ref{fig:rule-similarity-metrics}. A similarity score of 1.0 indicates that the rule remained unchanged from the previous iteration, while a score of 0.0 signifies significant modifications. Each chart compares either participants in the 10-instance group versus the 50-instance group or participants with access to LLM explanations versus those without.










Building on \citet{huang2023ting}'s analysis of creative writing editing, we employed two sentence-level similarity metrics to examine the rule editing performed by each participant: (1) \textbf{Normalized Edit Similarity}~\cite{yujian2007normalized, luozhouyang_python_similarity}, calculated as  1 - \textit{NormalizedEditDistance}, where higher scores indicate greater similarity; (2) \textbf{Semantic Similarity}, measured as the cosine similarity between semantic representations generated using Sentence-BERT~\cite{reimers-2019-sentence-bert}

We analyzed the rules defined by participants. 
Each participant iteratively reviewed and modified their rules across four rounds, leading to a total of five distinct versions. 
Before analyzing the rule difference, we aggregated  five task-specific label rules into a single rule for each version. \steven{is my description clear?}
To evaluate changes, we calculated the differences between rules from consecutive iterations (\eg iteration 0 to iteration 1 or iteration 2 to iteration 3), generating four unique similarity scores per participant. 
All comparisons were conducted within each participant's own set of rules.

We visualized the similarity levels of rule editing using Kernel Density Estimation (KDE), as shown in Figure~\ref{fig:rule-similarity-metrics}. A similarity score of 1.0 indicates that the rule remained unchanged from the previous iteration, while a score of 0.0 signifies significant modifications. Each chart compares either participants in the 10-instance group versus the 50-instance group or participants with access to LLM explanations versus those without.

    
\end{comment}