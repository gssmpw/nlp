%Another relevant area of research involves using LLMs for data annotation. 
Beyond simply treating LLMs as automatic labelers---common in countless NLP projects~\cite{tan2024large}---a growing body of work explores how to combine human and LLM efforts to achieve better annotation outcomes, such as improved accuracy or speed.
Even as LLMs outperform humans in many labeling tasks, human-AI collaboration often produces better results than either alone~\cite{vaccaro2024combinations}.
For example, \citet{kim2024meganno+} introduced a human-LLM collaborative annotation system where LLMs handle bulk annotation tasks, while humans selectively verify and refine the annotations. 
%\steven{However, this system was limited to deployment within Jupyter Notebook, lacking an end-to-end solution. This design imposed significant barriers, as it required users to possess technical expertise for system setup before using the tool, limiting accessibility and scalability in non-technical domains.}
\citet{goel2023llms} proposed an approach that combines LLMs with human expertise to efficiently generate ground truth labels for medical text annotation.
Additionally, \citet{10.1145/3613904.3642834} demonstrated how aggregating crowd workers' labels with GPT-4's output can achieve higher labeling accuracy than either source alone.
These studies generally aim to split the workflow of data labeling between humans and LLMs in a smart way, making the task more effective or efficient. 

In contrast, our work does not focus on dividing or combining the workload, but on how humans can teach LLMs---through prompt refinement---to better label the specific type of data.
Few prior studies have emphasized iterative prompt refinement in human-LLM collaborative data annotation.
For example, \citet{liu2024harnessing} developed a workflow for video content analysis, refining prompts to improve LLM-generated annotations and align them with human judgment.
Additionally, \citet{zhang2023llmaaa} proposed LLMAAA, which uses LLMs as annotators in a feedback loop to label data efficiently.
Their study shows that poorly designed prompts result in subpar performance, especially in complex tasks. %while incorporating demonstrations and aligning label descriptions with natural language significantly enhances accuracy and reliability.
Our work advances this relatively understudied area of human-LLM collaborative annotation research.

%----------------------------- dead kitten --------------------------------

\begin{comment}








\steven{\citet{vaccaro2024combinations} emphaized that designing innovative processes for integrating humans and AI is as critical as developing advanced AI technologies. This aligns with the need for LLM-powered systems that iteratively guide AI outputs to meet user-specific standards, prioritizing effective collaboration between users and AI systems.}

\steven{\citet{liyanage2024gpt} found that GPT-4, using few-shot, zero-shot, and Chain-of-Thoughts (CoT) prompting techniques, could not outperform models fine-tuned on human-labeled data. Among these, the few-shot approach exhibited the highest degree of similarity to human annotations. However, in scenarios where gold labels are unavailable, fine-tuning is not applicable, and alternative methods must be explored.}

\steven{\citet{liu2024harnessing} developed a workflow for video content analysis, iteratively crafting prompts to enhance LLMs' ability to generate structured annotations and comprehensive explanations that aligned with human judgment. }

\steven{\citet{zamfirescu2023herding} found that while prompts can effectively address most UX goals, they struggle with nuanced, edge-case, or spontaneous interactions. The study highlights that the effectiveness of each instruction in the prompt is highly sensitive to its phrasing and location. Additionally, highly prescriptive prompts, though reliable, limited the spontaneity and flexibility of GPT responses.
In our system, users are only required to provide task information—such as task descriptions, rules, and examples—to construct instructions, allowing for greater flexibility in accommodating diverse task requirements..}

\steven{\citet{guyre2024prompt} illustrates how prompt engineering can empower non-experts to design tailored conversational agents by iteratively refining prompts and infusing domain-specific knowledge. Their study emphasizes democratizing chatbot development, allowing users to align AI behavior with their specific goals and values.}

\steven{\citet{zhang2023llmaaa} proposes LLMAAA that leverages LLMs as Active Annotators in a feedback loop to efficiently annotate data. The study highlights that poorly designed prompts lead to suboptimal performance by LLM annotators, particularly in complex or domain-specific tasks. However, incorporating demonstrations and aligning label descriptions with natural language significantly enhances annotation accuracy and reliability.}

%\kenneth{Here, we then answer this question: Did people create ANYTHING to support LLM-powered data annotation? There are two parts of the answer to this: 1) Many or even most papers, including our CHI paper last year, focus on the labeling performance of LLMs, for example, as compared to crowdsourcing. They did not focus on the UI aspect of it. 2) Some prompt chaining tools, like ChainForge, can support workflow like this, but (a) hey do not focus on data annotation in particular so some functions are missing, like data resampling, and (b) more importantly, they do not aim to support general users. Most of them expect you to know some programming, e.g., ChainForge clearly say it's a visual programming tool. They're not really aiming for generic users.}


\cite{kim2024meganno+} introduced a human-LLM collaborative annotation system that allows LLM to handle bulk annotation tasks while humans verify selectively to refine annotation. 

\cite{goel2023llms} introduced an approach that combines LLM wth human expertise to create an efficient method for generating
ground truth labels for medical text annotation.


\cite{shankar2024validates} introduced a tool, EvalGen, to address the challenge of validating LLM. 
EvalGen helps users design evaluation criteria for LLM outputs and align that evaluation with human preferences through a mixed-initiative system.
A key finding is the concept of criteria drift, where users modify their evaluation standards while grading outputs. 


\cite{brade2023promptify} Promptify utilizes an LLM-powered suggestion engine to help users quickly explore and craft diverse prompts for text-to-image generation tasks.

    
\end{comment}
