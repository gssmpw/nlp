With the advances in LLMs, numerous tools have been developed to assist with prompt engineering. 
Most of these tools follow a software-engineering paradigm, where testing (such as unit tests or integration tests) is a central concept, and thus often assume the existence of gold-standard labels.
For example, PromptIDE is an interactive tool that helps experts iteratively refine prompts by providing various inputs, visualizing their performance on small validation datasets, and optimizing them based on quantitative feedback~\cite{strobelt2022interactive}; 
PromptAid is a visual analytics system for interactively creating, refining, testing, and iterating prompts while tracking accuracy changes~\cite{mishra2023promptaid};
%It allows users to adjust prompts through keyword modifications, paraphrasing, and adding few-shot examples; 
ChainForge is an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text-generation LLMs~\cite{arawjo2024chainforge};
and, promptfoo applies a test-driven approach to LLM development, producing matrix views that enable quick evaluation of outputs across multiple prompts~\cite{webster2023promptfoo}.
While these tools are inspiring and valuable, the scenarios we focus on do not rely on the constant availability of gold labels.

%\cite{mishra2023promptaid}


\begin{comment}






\kenneth{In here, we want to answer this questions: Why do we need to built \system? Can't we just use some existing tools??? The underlying answer could be: all the tools, including the one we mentioned in previous subsection, were not really aiming for ``general users'' and only thing general users can reliably use is probably chat interface come with ChatGPT etc.}

\citet{10.1145/3544548.3581388} mentioned that people tended to design prompts opportunistically, not systematically, which resulted in less success. \system provides a systematic process for composing and refining prompts, allowing non-expert users to adapt to the prompt creation process effortlessly.

\saniya{Amy Zhang points:
\newline 1. Accuracy didnot improve; reported improvements in recall
\newline 2. Observed that humans are pretty bad at being consistent
\newline 3. Quoted  Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2024. Language models don't always say what they think: unfaithful explanations
in chain-of-thought prompting. Advances in Neural Information Processing Systems 36 (2024).
Han Wang, Ming Shan Hee, Md Rabiul Awal, Kenny Tsu Wei Choo, and Roy Ka-Wei Lee. 2023. Evaluating GPT-3 Generated Explanations for
Hateful Content Moderation. arXiv:2305.17680 [cs.CL] for not using LLM prompt explanations
\newline 4. They had a bigger training set of around 700 examples: paper excerpt: "This process resulted in a balanced dataset of 800 comments. We randomly divided our dataset into a training dataset and a test dataset of 100 examples for each participant. The training dataset was used to help participants create their classiiers, whereas the test dataset was labeled by participants and used to evaluate their created classiiers."
}
    
\end{comment}