\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/step-1.jpg}\Description{This is Step 1 described in Figure 1. Users can provide data annotation context in the Context Tab, provide their rule and definition in the Rule Book tab, and add gold standard labels in the Shots tab. These tabs will compose prompts for later GPT to use.}
    \caption{The overview of step 1 of the data labeling process, compose or refine the prompt.
This is the most critical step, where the user composes and refines prompts for the LLM to label the data. In \system, the prompt consists of three parts, each corresponding to a separate sheet: Context (B), Rule Book (C), and Shots (D). 
At the beginning of this labeling process, the user has only a vague idea of what they want to label and will continuously refine that idea. 
Each time the prompt is revised, it reflects an evolution of their understanding and approach to the labeling task.}
    \label{fig:step-1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/step-2.jpg}\Description{This is Step 2 described in Figure 1. Users can either random or sequential sample data from the Dataset tab to the Working Data Sample tab. In the Working Data Sample, users can check “Keep it in the next data sample” for data instances that users want to remain in the Working Data Sample tab during sampling.}
    \caption{The overview of step 2 of the data labeling process, sample or resample a subset.
The full dataset is often too large for the user to thoroughly review, so sampling a subset is necessary.
%Labeling only a subset, rather than the entire dataset, is necessary because 
%Additionally, labeling the entire dataset iteratively would be prohibitively expensive. 
In this step, the user can (1) randomly or (2) sequentially sample data from the Dataset (A) sheet.
}
    \label{fig:system-interface-step-2}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/step-3.jpg}\Description{This is Step 3 and 4 described in Figure 1. After clicking Start Annotation, the results including LLM label and LLM explanation will be stored in a new tab. Users will review the data instances and LLM labels, by checking agree or providing their own labels. They also select the Gold Shot data instance to be added to Shots tab for later GPT to learn from. After verification, they can refine their prompt as Step 1 mentioned.}
    \caption{The overview of steps 3 and 4 of the data labeling process. After finalizing the prompt (Step 1) and sampling data instances (Step 2), in Step 3, the user clicks the ``Start Annotation'' button in the sidebar to annotate all instances in the Working Data Sample sheet. \system creates a new sheet, Task 1 (G), to store the data and labels of this labeling task, and also creates a new row for Task 1 in the Task Dashboard sheet. Then, in Step 4, the user can review the outcomes and refine the prompt accordingly (Step 1).
}
    \label{fig:system-interface-step-3-4-kenneth}
\end{figure*}

Users interact with \system to craft a prompt, use it to instruct the LLM in labeling data, review the results, and then revise the prompt through an iterative process. 
To demonstrate the users' workflow, we present a scenario where a user wants to employ \system to label a collection of tweets related to COVID with a 5-point sentiment scale, ranging from Very Negative (1) to Very Positive (5).
The goal is to analyze the sentiment of Twitter (now X) users toward COVID, with an emphasis on ensuring that the classification of each tweet reflects the user's own judgment.
In this case, the LLM's labels should align with the user's assessment of what is positive or negative, as well as the intensity of sentiment, rather than following an ``objective'' standard.
%In other words, the LLM's labels should align with the user's personal perception of the topic rather than adhering to an ``objective'' standard.\kenneth{This is not very accurate hmmm. Might need to edit later.}

%\begin{enumerate}
%    \item 
%\end{enumerate}

\begin{itemize}
   
\item 
\textbf{Step 0: Load and Index the Dataset.}
To begin using \system, the user opens a new Google Sheets document and activates the \system add-on. 
The system automatically sets up the necessary tabs, and the add-on interface appears on the right side of the spreadsheet (Figure~\ref{fig:system-interface-ui-kenneth}). 
The user then imports their data instances into the Dataset sheet, with the text of each tweet placed in the Data Instance column. 
The user must specify a Group ID for each instance. 
If the data are not sequential or grouped, they can assign unique Group IDs using Google Sheets' automatic numbering function.\footnote{\system is designed to accommodate single and grouped data instances within a Group ID. For tasks like sentiment analysis, each data instance is treated separately under its unique Group ID. For tasks that require contextual information, such as annotating text segments in an academic abstract (\eg, CODA-19~\cite{huang-etal-2020-coda}), \system can combine all data instances under the same Group ID into a single request to the LLM model. This flexibility allows the system to support different data instance formats based on user requirements.} 
Once the data is entered, the user clicks the ``Index Data ID'' button in the sidebar, and \system automatically assigns unique data IDs to each instance in the ``Data ID'' column.

\item 
\textbf{Step 1: Compose/Refine the Prompt (Figure~\ref{fig:step-1}).}
%Step 1: compose the promot using things. 
%Uses know a vague idea what they want and will keep revise that idea. But you need to write something down. When it comes to load data, spreadsheet is great!
This is the most critical step, where the user composes and refines prompts for the LLM to label the data. 
In \system, the prompt consists of three parts, each corresponding to a separate sheet: (1) Context, (2) Rule Book, and (3) Shots. 
Figure~\ref{fig:step-1} provides an overview of each sheet.
\begin{enumerate}

\item 
In the \textbf{Context} sheet, the user answers questions that describe the context of the data annotation task, such as the purpose of the annotation and the source of the data, to provide task-specific context for the LLM.

\item
In the \textbf{Rule Book} sheet, the user adds annotation labels along with their definitions. Providing content for both the Context and Rule Book sheets is mandatory, as the LLM requires this information in the prompt to function effectively.

\item
In the \textbf{Shots} sheet, the user adds data instances along with their corresponding gold labels, which serve as examples to help the LLM learn. While adding examples to the Shots sheet is optional during the first iteration---since the user may not yet have a well-defined gold standard for labeling---more examples can be identified as the user reviews data. These examples can be manually added or generated using \system's function (see Step 4).
\end{enumerate}
It is important to note that at the beginning of this labeling process, the user has only a vague idea of what they want to label and will continuously refine that idea. 
Each time the prompt is revised, it reflects an evolution of their understanding and approach to the labeling task.


\item 
\textbf{Step 2: Sample/Resample a Subset (Figure~\ref{fig:system-interface-step-2}).}
Next, the user employs \system to sample a subset of data for labeling. 
Labeling only a subset, rather than the entire dataset, is necessary because the full dataset is too large for the user to thoroughly review the LLM's results. 
Additionally, labeling the entire dataset iteratively would be prohibitively expensive. 
In this step, the user can (1) randomly or (2) sequentially sample data from the Dataset sheet into the Working Data Sample sheet:

\begin{itemize}

\item 
For a \textbf{Random Sample}, the user enters any whole number between 1 and the total number of group IDs in the dataset.
\system will then randomly select that number of groups and copy them into the Working Data Sample sheet. 

\item
In \textbf{Sequential Sample}, the user specifies a range of group IDs from the Dataset sheet, and \system will import the data instances from the selected range into the Working Data Sample sheet.
This feature allows users to process their data instances sequentially in batches, which is especially useful when the data instances have a sequential relationship, such as sentences within the same document.


%The purpose of this feature is to enable users to process their data instances sequentially in batches, making their work more manageable and easier to track.
%\kenneth{Mayeb say a few words on why we need this.}\steven{done.}

\end{itemize}

Once sampling begins, all previously existing data in the Working Data Sample sheet will be removed, except for instances marked as ``Keep it in the next data sample'' (Figure~\ref{fig:system-interface-step-2}). 
Only the data in the Working Data Sample sheet will be labeled by the LLM when the ``Start Annotation'' button is clicked in Step 3.

\item 
\textbf{Step 3: Use the Prompt to Instruct the LLM to Label the Data Sample (Figure~\ref{fig:system-interface-step-3-4-kenneth}).}
After finalizing the three prompt sheets---Context, Rule Book, and Shots---in Step 1 and sampling data instances in Step 2, the user clicks the ``Start Annotation'' button in the sidebar to annotate all instances in the Working Data Sample sheet (Figure~\ref{fig:system-interface-step-3-4-kenneth}).
\system creates a new sheet, Task 1 (Figure~\ref{fig:system-interface-step-3-4-kenneth}), to store the data and labels of this labeling task, and also creates a new row for Task 1 in the Task Dashboard sheet.

In the background, \system first combines the information in Context, Rule Book, and Shots sheets into a prompt (see Section~\ref{sec:implementation} for details).
%gathers the questions and answers from the Context sheet and feeds them into GPT-4 to generate an instruction prompt. 
%This prompt is then combined with the rules and provided gold shots to create the final annotation prompt. 
For each data group (\ie, data instances with the same Group ID), \system sends a request to the LLM using this prompt for annotation.
After receiving the LLM's output, the system parses the results and updates the Task 1 sheet with the annotated outcome for each instance. 
In our implementation, the LLM is always asked to provide explanations for its labels, though the user can decide whether to display these explanations in the annotation results.
%In our user study, we also explored the impact of showing the LLM's explanations to users.



%Step 3: Send it to LLM to label. System creat a new tab; you can navigate tasks using dashboard. Then you re

\item 
\textbf{Step 4: Observe, then Revise the Prompt (Figure~\ref{fig:system-interface-step-3-4-kenneth}).}
The labeling results are saved to the Task 1 sheet (Figure~\ref{fig:system-interface-step-3-4-kenneth}), where the user can manually verify the LLM's labels.
The user can review as many or as few data instances as they wish to develop a better understanding of the labeling task and the dataset. 
Based on this evolving understanding, they can refine the prompt by modifying the Context, Rule Book, and Shots sheets accordingly.

If the user disagrees with any of the labels, they can assign a new label to the data instance under the ``Human Label'' column. 
If the user identifies good examples, they can check the ``Gold Shot'' checkboxes. 
After selecting enough good examples, the user can click the ``Add Shots'' button in the sidebar to add these examples to the Shots sheet (Figure~\ref{fig:system-interface-step-3-4-kenneth}).
Like in other sheets, if the user wants certain data instances to be re-annotated in the next round, they can check the ``Keep it in the next data sample'' checkboxes. 
This will ensure that those instances are not removed during the next sampling process, allowing the user to observe whether the LLM's behavior changes over iterations.


\end{itemize}

When using \system, the user moves through Steps 1, 2, 3, and 4, and then returns to step 1 in an iterative process until they are satisfied with the LLM's labels.








%\subsubsection{Step 0: Initial Setups}


%\subsubsection{Step 1: Compose/Refine the Prompt}

%\subsubsection{Step 2: Sample/Resample a random subset}

%\subsubsection{Step 3: Use the Prompt to Instruct the LLM to Label the Data Sample}

%\subsubsection{Step 4: Observe, and Revise the Prompt}
