%\kenneth{(1) Support PITD so we don't have gold label and don't calculate accuracy etc becasyue we think they're not reliable and evovling, and (2) using spreadsheet because it's just very easy to use and practically provide many flexibilities to users.}

%Prompting in the Dark: 
\paragraph{Adapting to Evolving Labeling Goals}
%\paragraph{Prompting in the Dark and Users' Needs for Evolving, Open-Ended Labeling Schemes.}
The goal of \system is to enable general users to create and refine prompts iteratively for LLM-powered data labeling, particularly in situations where they start without any labeled gold data or manual labeling, \ie, the ``prompting in the dark'' scenario. 
In these cases, users' understanding of the data and desired labeling scheme evolves through their interactions with the LLM, based on its predicted labels and explanations, rather than through their own manual efforts. 
The lack of gold labels (or sufficient labeled data) introduces the core challenge of the prompt-in-the-dark process: aside from users' observations and judgments about the labeling results, there is no concrete way to provide quick and comprehensive feedback on the progress of prompting.
We view this as a trade-off between two user needs in prompt engineering: 
{\em (i)} allowing users' understanding of the data and labeling goals to evolve, and 
{\em (ii)} providing clear guidance and reliable feedback to assess progress toward a defined annotation goal. 
Previously, supervised learning-based classifiers required labeled data, so users focused heavily on the second need, as manual labeling was always needed and assumed to finalize the coding scheme. 
The rise of LLMs has reduced the need for pre-labeled data, allowing users to put more focus on their first needs. 
The growing popularity of the prompting-in-the-dark approach reflects users' need for evolving and dynamic labeling practices~\cite{austin2024grad,zhang-etal-2024-glape,wang2024human}.
%zhang2023labelvizier to facilitate the validation and relabeling of large-scale technical text annotations. Its interactive, visual analytic interface allows users to detect and correct three main types of labeling errors: duplicate, wrong, and missing labels. 
%\kenneth{TODO: Do we have any reference to support it's a popular practice now?}\steven{those are three study that have no gold label, they iteratively use user-defined criteria to evaluate and refine.}\kenneth{Oh and did they manage to improve the accuracy over time??}\steven{Yeah, they got a higher rating and accuracy}\kenneth{Hmmmmmm so what's the deal? How are their systems and approaches different?} %, rather than simply saving time on manual labeling. 
% dudley2018review describe the interative machine leanring paradigm that user iterative build and refine model. The model refinement is driven by user input. It more focused on the human input to refine. kim2024evallm is a prompt refining tool by evaluating outputs on user-defined criteria. It is not a labeling task
Our design goal for \system is to offer users greater flexibility and freedom in defining how their data should be labeled.


%\kenneth{I revised this following paragraph to tailor it more close to our target users. Please take a look.}
\paragraph{Supporting a Wide Range of ``Newcomers'' Brought in By LLMs.}
%\paragraph{End-User Prompting Tools.}
%Our second design goal for \system is to create a tool for general users, including people with limited or no programming skills.
Our second design goal for \system is to develop a tool for users with \textbf{little to no experience in large-scale text data labeling}, including but not limited to those with limited or no programming skills.
The rationale behind this goal is two-fold.
On a practical level, 
people new to large-scale data annotation---empowered by LLMs to undertake such tasks with greater ease---are more likely to adopt approaches that diverge from conventional practices. 
In the crowdsourcing literature, many papers emphasize best practices for data annotation~\cite{hsueh2009data, sabou2014corpus, vondrick2013efficiently, drutsa2019practice, wang2013perspectives}, %\kenneth{TODO: Add ref on crowdsourcing best practices}\steven{added}
such as ethical pay rates~\cite{fort2011amazon,shmueli2021beyond}, %\kenneth{TODO: Add refs on crowd workers' ethical pay}\steven{added}
usable worker interfaces~\cite{toomim2011utility,10.1145/3613904.3642834, rahmanian2014user, komarov2013crowdsourcing}, %\kenneth{TODO: Add refs to crowd interface's impact on crowdsourced data quality--- Maybe cite our own CHI paper too}\steven{added}
and 
gold labels for quality control~\cite{han2020crowd,gadiraju2015training,le2010ensuring,doroudi2016toward,hettiachchi2021challenge}. %\kenneth{TODO: Again, add ref IN CROWDSOURCING for gold labls}\steven{added}
However, these practices are often neglected in real-world scenarios. 
For instance, many tasks on MTurk still offer very low pay~\cite{AI_workers_low_wages} %\kenneth{Add ref: A data-driven analysis of workers' earnings on Amazon Mechanical Turk}\steven{added}
or rely on poorly designed interfaces~\cite{fowler2023frustration}. %\kenneth{Add ref: Frustration and ennui among Amazon MTurk workers}\steven{added}
Newcomers to large-scale data annotation are even less likely to be familiar with these best practices, including carefully establishing gold labels before prompting LLMs.
%users without programming experience are more likely to prompt in the dark, struggling to interact effectively with LLMs. 
%In contrast, those with software engineering backgrounds are familiar with using established frameworks and tools, where automatic testing---such as unit and integration testing---is standard. 
The bigger picture is that LLMs are adding many ``new members'' to the world of programming and data science.
%---people with little or no coding experience. 
This group brings new practices, user needs, challenges, and research questions to HCI, requiring more focused attention.

%\bigskip
\sloppy
Based on these two design goals, we decided to build \system based on spreadsheets, a format that most computer users are already familiar with.
We distinguish our goals from existing efforts in two significant ways. 
First, while projects like LangChain or ChainForge focus on developers or those with programming backgrounds, requiring software installations or configurations, we aim to focus on general users who do not necessarily have such expertise. 
Second, some projects explore new interactions enabled by LLMs~\cite{10.1145/3586183.3606833}, but our project is concerned with understanding how effectively users can use familiar interfaces, such as spreadsheets, to interact with LLMs.

%--------------- dead kitten ----------

\begin{comment}
  


At the practical level, users with little or no programming background might be more likely to prompt in the dark. 
In contrast, individuals familiar with software engineering practices are accustomed to using existing frameworks or tools, where automatic testing---such as unit and integration testing---is standard.
These users have more experience as well as technological support for creating gold labels for testing.
At a deeper level, what is happening is that LLMs bring many people with limited or no programming skills into the realm of data science or programming tasks.
This group of people brings new and interesting challenges and research questions to HCI and thus deserves more attention.

%we believe that the greatest value of LLMs lies in the new possibilities they offer to general users. 
%For people without coding skills, LLMs enable tasks such as building websites from scratch, creating classifiers for automating email filtering, and labeling data to extract insights---activities that were previously within the realm of programmers. 

  
\end{comment}