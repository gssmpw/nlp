\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{Figures/Results/New-Pure-Participant/avg_accuracy_plot_new.png}\Description{This is an average accuracy plots subfigure of the participants prompt performance. The subfigure includes shaded regions representing the standard deviation and scatter points indicating individual participants’ data.}
    \caption{The average ACC of the first prompt and the four revisions.}
    \label{fig:average-acc}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{Figures/Results/New-Pure-Participant/avg_mse_plot_new.png}\Description{This is a MSE accuracy plots subfigure of the participants prompt performance. The subfigure includes shaded regions representing the standard deviation and scatter points indicating individual participants’ data.}
    \caption{The average MSE of the first prompt and the four revisions.}
    \label{fig:average-mse}
  \end{subfigure}
  \caption{The average ACC and MSE of the first prompt and the four subsequent revisions. These results show that prompting in the dark is not particularly effective. 
Average labeling accuracy only slightly improved after four iterations; the average MSE fluctuated, ultimately increasing only marginally by the fourth revision.}
  \label{fig:average-acc-mse-performance}
\end{figure*}

\begin{table*}[t]
\centering
%\small
\begin{tabular}{lcccc}
 & \multicolumn{2}{c}{\textbf{ACC$\uparrow$}} & \multicolumn{2}{c}{\textbf{MSE$\downarrow$}} \\ \hline
\multicolumn{1}{c}{} & \textbf{Avg.(SD)} & \textbf{\begin{tabular}[c]{@{}c@{}}\%Participant\\ Improved\\ Over Initial\end{tabular}} & \textbf{Avg.(SD)} & \textbf{\begin{tabular}[c]{@{}c@{}}\%Participant\\ Improved\\ Over Initial\end{tabular}} \\ \hline
\textbf{Initial Prompt} & .542 (.099) & - & .782 (.482) & - \\ \hline
\textbf{1st Revision} & .536 (.088) & 35\% & .795 (.546) & 40\% \\
\textbf{2nd Revision} & .542 (.073) & 40\% & .833 (.514) & 45\% \\
\textbf{3rd Revision} & \underline{\textbf{.549 (.082)}} & 50\% & .823 (.579) & 50\% \\
\textbf{4th Revision} & \underline{\textbf{.553 (.085)}} & 45\% & .810 (.566) & 40\% \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}End of Session\\ (Avg \#Iter = 4.75)\end{tabular}} & \underline{\textbf{.546 (.084)}} & 45\% & .815 (.489) & 45\%
\end{tabular}
\caption{The average ACC and MSE of the first prompt, the four subsequent revisions, and at the end of the session. Improvements over the initial prompt are bolded and underlined. }
\label{tab:overall-ACC-MSE}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/Results/New-Participant-DSPy/new_acc_plots.png}\Description{It contains 20 subfigures of the average ACC of each participant.}
    \caption{Accuracy Plots for all participants. The results show that the process is highly unreliable. 
Labeling accuracy improved for 9 participants after four iterations, declined for 10, and remained unchanged for 1. }
    \label{fig:individual-pure-acc}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/Results/New-Participant-DSPy/new_mse_plots.png}\Description{It contains 20 subfigures of the average MSE of each participant.}
    \caption{MSE Plots for all participants.
    The results show that the process is highly unreliable. 
MSE improved (\ie, decreased) for 8 participants, worsened for 10, and stayed the same for 2.}
    \label{fig:individual-pure-mse}
\end{figure*}



%\kenneth{NO ONE SHOULD DO THIS. THIS IS HARDDD.}

We evaluated each prompt iteration by comparing the labels generated by the LLM (GPT-4o) to the gold labels participants manually annotated for a set of 50 tweets at the end of their study session.
Performance was measured using accuracy (ACC) and Mean Square Error (MSE), two commonly used metrics in sentiment analysis~\cite{saxena2022introduction}.
%\kenneth{Why do we cite this here?} \steven{this describe that ACC and MSE are two evaluation matrices for analyzing sentiment task}
In the main study, each participant provided at least five prompts (with some offering more), and our evaluation focuses primarily on these five prompts, which were shared across all participants.
The ACC and MSE were calculated by comparing the annotation results from participant-provided prompts with the manually generated gold-standard labels. Each label was treated as a distinct category, and accuracy was then calculated accordingly.

\subsubsection{5-Point sentiment rating is a highly subjective task.}
%\kenneth{I kinda feel we don't need a table for kappa}

%The premise of our study is that users bring their own personal perspectives and judgments to even seemingly identical labeling tasks. 
%Before analyzing the results, we first validate this assumption in our study.
%We calculated both Cohen's kappa and Kendall's correlation between participants' manual ratings, collected at the end of our study and the ``gold-standard'' labels provided by the dataset, as well as the value between participants.
The premise of our study is that users bring their own personal perspectives and judgments to seemingly identical labeling tasks. 
To validate this assumption, we calculated both Cohen's kappa
%, as well as Spearman's and Kendall's correlations 
between participants' manual ratings---the labels collected at the end of the study---and the ``gold-standard'' labels from the dataset, as well as the agreement between participants themselves.
%\kenneth{TODO Steven: Kappa is pretty harsh. Can you additionally try Spearman or Spearman, Kendall correlation?}\steven{sure, Spearman: participant vs dataset is 0.343(SD=0.121), between participants is 0.530(SD=0.078). Kendall: participant vs dataset is 0.299(SD=0.107), and between participants is 0.474(SD=0.071) These two correlation scores are slightly better than Kappa but still not very good.}
%\paragraph{Inter-annotator agreement (Cohen's kappa)} 
Participants' labels show a poor alignment with the labels from the original dataset, with an average Kappa of 0.114 (SD=0.070). 
%Spearman's of 0.343 (SD=0.121), and 
%Kendall's of 0.299 (SD=0.107).
%Interestingly, 
The average Kappa value between participants was only slightly higher, with an average Kappa of 0.249 (SD=0.059).
%Spearman's of 0.530(SD=0.078), and 
%Kendall's of 0.474(SD=0.071).
%\kenneth{is this average between all the participant pairs?}\steven{yes}
%each individual participant and the rest of the participants was only slightly  higher at 0.249 (SD=0.059), suggesting participants were more consistent with each other than with the original dataset labels.
The low Kappa score indicated the Twitter Sentiment task we used in our study was a highly subjective task. 
Namely, each participant's gold labels were highly influenced by their personal interpretations and preferences.


\subsubsection{Prompting in the dark is ineffective}
Table~\ref{tab:overall-ACC-MSE} and Figure~\ref{fig:average-acc-mse-performance} show the average ACC and MSE for the first prompt and the four subsequent revisions.
%\kenneth{TODO: Add reference to the Table}\steven{done}
Our analysis, as captured in \system, indicates that prompting in the dark is not particularly effective. 
Among the 20 participants, average labeling accuracy (where higher is better) only slightly improved from 0.542 to 0.553 after four iterations (Figure~\ref{fig:average-acc}). 
Some participants went through more than four iterations. 
The average labeling accuracy at the end of their sessions---the final iteration for all participants---was improved to 0.546.
%\kenneth{Add one sentence about the end-of-session ACC performance}\steven{done}
Meanwhile, the average MSE (where lower is better) fluctuated, ultimately increasing from 0.782 to 0.810 by the fourth revision (Figure~\ref{fig:average-mse}). 
The average MSE for the end-of-session increased to 0.815.
%\kenneth{Add one sentence about the end-of-session MSE performance}\steven{done}
It is important to note that, since this is a 5-scale rating task, ACC is a more harsh metric, awarding credit only for exact matches, while MSE considers the distance between the predicted rating and the user-specified rating.



%\steven{significant increasing trend by using linear mixed-effect model}

\subsubsection{Prompting in the dark is unreliable.}
To further illustrate how the process unfolded for each participant, we present individual ACC and MSE charts in Figure~\ref{fig:individual-pure-acc} and Figure~\ref{fig:individual-pure-mse}. 
The results show that the process is highly unreliable. 
Labeling accuracy improved for 9 participants after four iterations, declined for 10, and remained unchanged for 1. 
Similarly, MSE improved (\ie, decreased) for 8 participants, worsened for 10, and stayed the same for 2. 
Overall, the practice of ``prompting in the dark''---iterating prompts without reference to gold labels---proved unreliable, with over half of the participants experiencing a decline in performance by the end of the study.








%--------------- dead kitten ---------------
\begin{comment}
  



The average labeling accuracy had a slight increase (Figure~\ref{fig:average-acc}), rising from 54.20\% to 55.30\% in the final refined prompt compared to the initial prompt. Meanwhile, the average MSE (Figure~\ref{fig:average-mse}) fluctuated and ultimately increased from 0.782 to 0.810 by the last prompt.

Figure~\ref{fig:individual-pure-acc} and Figure~\ref{fig:individual-pure-mse} illustrate the performance of participant-guided LLM for each iteration. 
Labeling accuracy improved for 9 participants after four iterations, decreased for 10 participants, and remained unchanged for 1 participant.
Labeling MSE improved (dropping in MSE) for 8 participants, decreased for 10 participants, and remained unchanged for 2 participants.

Overall, the prompt refined during the user study was \textbf{highly unreliable} as over half of the participants experienced a decline in their performance at the end of the study.



  
\end{comment}







