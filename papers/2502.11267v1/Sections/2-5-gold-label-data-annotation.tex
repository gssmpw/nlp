Decades of research have shown that gold-standard labels play a critical role in quality control for data annotation pipelines~\cite{han2020crowd,gadiraju2015training,le2010ensuring,doroudi2016toward,hettiachchi2021challenge}.
Embedding items with known labels into the data annotation process allows requesters to reliably capture quality signals, 
such as workers' level of expertise~\cite{abraham2016many, abassi2019worker, yang2018improving} %\kenneth{TODO: Add refs about using gold labels to decide workers' expertise level}\steven{added}
or attentiveness to tasks~\cite{hettiachchi2021challenge, oleson2011programmatic}. %\kenneth{TODO: Add refs about using gold labels to do attention checks for workers}\steven{added}
These insights enable requesters to take appropriate actions, such as 
retraining annotators~\cite{le2010ensuring, doroudi2016toward,hettiachchi2021challenge}, %\kenneth{TODO: Add refs about retraining workers}\steven{added}
removing low-performing workers~\cite{10.1145/3613904.3642834, snow2008cheap,downs2010your,le2010ensuring}, %\kenneth{TODO: Add refs about removing or blocking low-performing workers}\steven{added}
or identifying potential issues in the annotation interfaces~\cite{toomim2011utility,10.1145/3613904.3642834, rahmanian2014user, komarov2013crowdsourcing}. %\kenneth{TODO: Add refs for crowd worker interfaces. At least cite: Toomim, M., Kriplean, T., Pörtner, C., \& Landay, J. (2011, May). Utility of human-computer interactions: Toward a science of preference measurement. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 2275-2284).}\steven{added}
Gold labels are also beneficial for requesters during post-annotation data processing. 
They can be used to weight labels from different workers in label aggregation~\cite{abassi2017gold,abassi2019worker}, %\kenneth{TODO: Add label aggregation methods that use gold labels particularly to weight different workers}\steven{added}
improve label aggregation strategies~\cite{khattak2011quality, snow2008cheap},  %\kenneth{TODO: Add label aggregation methods that learn whatever from gold labels}\steven{added}
or 
exclude unreliable workers' outputs entirely~\cite{abassi2019worker}. %\kenneth{TODO: Cite ref using gold labels to remove workers from label aggregation}\steven{added}
Beyond requesters, gold labels are also beneficial for data labelers like crowd workers. 
Gold labels can be used to train workers~\cite{doroudi2016toward, le2010ensuring, gadiraju2015training,han2020crowd}, %\kenneth{TODO: Cite ref that uses gold labels for worker training}\steven{added}
provide real-time feedback to help them recalibrate their understanding of the task~\cite{le2010ensuring,hettiachchi2021challenge}, %\kenneth{TODO: Cite the visible gold paper from Amazon}\steven{added}
or remind them to pay more attention~\cite{ hettiachchi2021challenge,oleson2011programmatic}. %\kenneth{TODO: Cite attention check papers}\steven{amazon paper also warn workers in real time}

While gold labels are useful for quality control, as stated in the Introduction (Section~\ref{sec:intro}), %\kenneth{TODO: Update references}\steven{done}
they are not always available in real-world scenarios due to constraints such as data privacy or the cost of gathering gold labels~\cite{liu2019deep, yang2019evaluating, oikarinen2021detecting, slote2024unlocking}.
To address these challenges, researchers have developed methods to generate (approximations of) quality signals without gold labels. 
In the realm of LLM-powered data annotation, for instance, CoPrompter evaluates how well an LLM's output aligns with user-specified requirements as a feedback mechanism~\cite{joshi2024coprompter}. %\kenneth{TODO: Cite: Joshi, I., Shahid, S., Venneti, S., Vasu, M., Zheng, Y., Li, Y., ... \& Chan, G. Y. Y. (2024). CoPrompter: User-Centric Evaluation of LLM Instruction Alignment for Improved Prompt Engineering. arXiv preprint arXiv:2411.06099.}\steven{added}
Other studies also leverage the stability~\cite{chiang2023can} %\kenneth{TODO: Add ref}\steven{added}
%chiang2023can found LLM evaluation are stable over different formatting
or confidence~\cite{gligoric2024can} %\kenneth{TODO: Add ref}\steven{added}
%gligoric2024can introduce CONFIDENCEDRIVEN INFERENCE: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected
of LLM outputs to infer quality signals.
%Our research investigates how effectively humans can iteratively refine prompts to guide LLMs in labeling data when gold-standard labels are unavailable, providing alternative quality signals.
Our research examines how effectively humans can refine prompts to guide LLMs in labeling data without gold-standard labels, providing insights into human prompting capabilities in the absence of reliable guidance signals.










%------------- dead kitten -------------
\begin{comment}




\kenneth{------------------------KENNETH IS WORKING HERE----------------------}



Gold-standard labels are widely used for quality control and crowd worker training~\cite{doroudi2016toward, gadiraju2015training,le2010ensuring,hettiachchi2021challenge}. For example, \citet{hettiachchi2021challenge} demonstrated that incorporating visible gold questions -- where annotators receive periodic feedback based on pre-labeled gold-standard examples -- could improve their work quality. 
Similarly, \citet{doroudi2016toward} found that providing expert examples was the most effective method of training for crowd workers and can help workers avoid specific types of incorrect responses. 
Additionally, \citet{le2010ensuring} employed dynamic learning systems that leveraged gold-standard labels to deliver real-time feedback and improve worker outcomes.
These studies, however, predominantly address the annotators' perspective -- workers who adhere to predefined guidelines and follow established standards.
While annotators are crucial components of the task pipeline, our study shifts focus to the requesters' perspective, those responsible for task design and pipeline management.
For requesters, gold-standard labels serve as signals to assess worker performance and refine training processes, thereby improving the overall quality of the entire pipeline.
Critically, the aforementioned studies assume the availability of gold-standard labels, typically under controlled experimental settings. 
In real-world scenarios, this assumption often does not hold due to constraints such as data privacy, security concerns, or the absence of labeled data~\cite{liu2019deep, yang2019evaluating, oikarinen2021detecting, slote2024unlocking}. 
To address this gap, our research explores settings where predefined gold-standard labels are unavailable. 
We designed a novel framework for requesters to iteratively develop and evolve their labeling standards through interactions with LLMs. 
By bridging the divide between controlled experiments and real-world challenges, our work highlights the potential of adaptive, LLM-driven approaches for dynamic task management without reliance on predefined gold-standard labels.

\steven{\citet{hettiachchi2021challenge} demonstrated that incorporating visible gold questions -- where annotators receive periodic feedback based on pre-labeled gold-standard examples -- could improve their work quality. 
Their study leveraged gold-standard labels to train crowd workers to align with pre-defined standards, effectively guiding annotators thorugh examples and feedback. 
While this approach focues on improving labeling quality at the annotator level, our work shifts the focus to requester and researcher perspective. Instead solely training labelers to meet pre-existing standards, we emphasize the broader implications of designing system in the entire labeling process, particularly in context involving dynamic or subjective tasks. \citet{gadiraju2015training} showed that training workers with gold labels can enhance accuracy and decrease response times. \citet{han2020crowd} used gold standard labels to guide crowd workers in revising incorrect judgments to align with predefined standards. 
}

\steven{
\citet{doroudi2016toward} found that providing expert examples was the most effective method of training for crowd workers. In our study, however, each participant was treated as an individual researcher rather than a crowd worker. While this finding underscores the value of providing gold labels to improve language model performance, it does not directly highlight their significance for researchers. Furthermore, \citet{doroudi2016toward} observed that gold standard labels help workers avoid specific types of incorrect responses. 
In contrast, our task is subjective, with participants’ standards potentially shifting across iterations. Introducing pre-set gold standard labels could enforce a uniform standard across each participant, which might not align with the iterative and subjective nature of our study
}

\steven{\citet{gadiraju2015training} showed that training workers with gold labels can enhance accuracy and decrease response times. [They were still focusing on crowd worker level.] }

\steven{\citet{han2020crowd} used gold standard labels for quality control and to guide crowd workers in revising incorrect judgments to align with predefined standards.}

\steven{\citet{le2010ensuring} employed gold standard labels within a dynamic learning environment that provided real-time feedback to train workers. However, the selection of specific examples for training could influence worker responses, potentially introducing bias in their judgments. [This is why we implemented a random sample in our system]}


\steven{\citet{liu2019deep} developed a HITL system that kept model upgrading with progressively collected data without having a pre-labeled data. [\textbf{they used 30 samples per iteration.} -add to justification for 10 and 50 instances.]}

\steven{\citet{wall2019using} found that end-users could build models without using expert patterns that have comparable performance to those who built by expert. This approach was required more effort and more mental demand than those who received guidance.}

\kenneth{TODO: Add references to every part of this paragraph.}
Decades of research have established that gold-standard labels are highly effective for quality control in data annotation~\cite{han2020crowd,gadiraju2015training,le2010ensuring,doroudi2016toward,hettiachchi2021challenge}. 
Embedding items with known labels into the annotation process enables requesters to monitor annotator or data quality and take actions such as retraining annotators, removing them from the pipeline, or reducing their weight in label aggregation. 
Beyond requesters, gold labels also allow for real-time feedback to workers, helping them recalibrate their understanding of the task or focus more carefully.
While gold labels are widely recognized as useful for quality control, most research assumes their availability.
However, as discussed in our Introduction (Section X), this assumption does not necessarily hold in real-world scenarios due to constraints such as data privacy or the cost of gathering gold labels~\cite{liu2019deep, yang2019evaluating, oikarinen2021detecting, slote2024unlocking}. 
To address these challenges, researchers have developed systems to provide proxy quality signals without gold labels. 
For instance, CoPrompter evaluates how well an LLM's output aligns with user-specified requirements as a feedback mechanism. 
Other studies leverage the stability or confidence of LLM outputs to infer quality signals.
Our research investigates how effectively humans can refine prompts to guide LLMs when gold-standard labels are unavailable.
    
\end{comment}