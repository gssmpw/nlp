\steven{
We conducted a survey study to investigate how individuals interact with LLMs and utilize gold-standard labels in the data annotation process. 
The participants primarily represent roles in research, machine learning engineering, and software development. \\
\textbf{Workflows: }Participants described diverse workflows for integrating LLMs into data annotation process, highlighting a common iterative and human-in-the-loop approach. \textbf{Most workflows begin with manual annotation of a small subset of data to establish a baseline.} Participants then employ prompt engineering, iteratively refining LLM prompts by evaluating their performance against the manually annotated subset. \\
Once refined, the prompts are used to label larger datasets, with participants using tools or manual checks to review the LLM's annotations and identify any invalid labels. The process is typically concluded with a thorough manual verification of the dataset. \\
One participant mentioned they manually tabulate data points along with their descriptions. \\
\textbf{Initialize Prompting: }Most participants use their pre-defined prompts to initialized the annotation on their known tasks. 
For new tasks, one participant mentioned that they initialize the annotation process with LLMs by starting with a clear problem definition and iteratively refining a classification-based approach. For less familiar tasks, some participants may seek suggestions from the LLM to guide the initial setup.
\textbf{Revising Prompt: } Participants use a small dataset to finetune the prompt. They address issues by adding rules or context examples to tackle failure cases. When inconsistencies or error arise, they revisit and recheck the manually tagged dataset to improve performance. Some participants also engage the LLM by asking questions about data points and their descriptions, retraining to against inconsistencies to minimize hallucinations and enhance annotation reliability.
}