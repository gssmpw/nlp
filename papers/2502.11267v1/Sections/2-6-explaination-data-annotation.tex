Explanations in AI models play a critical role in enhancing user comprehension, offering valuable insights, and improving the usability of AI-assisted tools across a range of tasks, including deception detection and UX evaluation~\cite{lai2019human, fan2022human}. While prior studies have primarily investigated tasks with predefined standards, our research focuses on a dynamic context where task standards evolve iteratively through user interactions with the system.
Building on existing work, prior studies have emphasized the importance of fostering human-AI synergy by envisioning a collaborative relationship where humans and AI function as team players~\cite{lai2020chicago,fan2022human}.
However, human-AI collaboration often introduces challenges such as overreliance -- when humans uncritically accept incorrect AI recommendations~\cite{passi2022overreliance, buccinca2021trust}. These studies propose several strategies to mitigate overreliance, including employing cognitive forcing functions, improving clarity in communication, and providing informative explanations.
Notably, \citet{vasconcelos2023explanations} illustrated that overreliance on AI is not an inherent cognitive bias but rather a strategic decision shaped by the perceived costs and benefits of engaging with AI explanations. Their research revealed that overreliance can be alleviated by increasing task complexity, enhancing the clarity of AI explanations, or reducing the incentives for reliance.
These insights underscore the importance of designing AI explanations that effectively balance user trust and scrutiny.
In contrast to all these studies, which typically examine end users completing tasks using AI-generated explanations, our study focuses on requesters as the primary users, who are inherently accountable for their tasks and possess personalized or domain-specific expertise. 
Unlike the straightforward application of LLM explanations to perform tasks, we explore the underexplored impact of these explanations on requesters' decision-making processes and their ability to guide task pipelines effectively. 
We aim to determine whether the presence of LLM explanations influences requesters’ capacity to effectively direct LLMs in labeling tasks.

\steven{\citet{vasconcelos2023explanations} demonstrated that overreliance on AI is not an inherent cognitive bias but rather a strategic decision influenced by the perceived costs and benefits of engaging with AI explanations. 
Their study showed that overreliance can be mitigated by increasing task difficulty, improving the comprehensibility of AI explanations, or decreasing incentives for reliance.
In our study, we focus on requesters as the target users, who are inherently responsible for their tasks.
Rather than solely using LLM explanations for task completion, we want to investigate whether the existence of explanations could impact users' ability to effectively guide LLM in labeling task. 
}

\steven{\citet{mueller2019explanation} mentioned that effective explanations can enhance user comprehension and provide actionable insights. 
They also noted that aligning explanations with users’ mental models facilitates a clearer understanding of the system’s logic and behavior.
To foster collaboration, AI systems should act as team players, building shared understanding with users. 
Building on these insights, we aim to explore how users leverage explanations to better guide LLMs, aligning them with their standards and expectations. }

\steven{\citet{lai2019human} highlighted  the importance of transparency in ML models on deception detection tasks. 
Based on this, \citet{lai2020chicago} proposed future directions for developing human-centered tutorials and explanations to foster synergy between humans and AI.  
In their work, explanations were generated using feature-based, example-based, and human-text approaches. While feature-based and example-based methods often lack transparency, human-text explanations, derived by summarizing scientific studies and paraphrasing them into human-readable rules, face scalability limitations.
The advent of LLMs allows us provide more comprehensive and transparent explanations to users.
We seek to investigate whether this enhanced transparency can significantly impact users’ ability to guide LLMs effectively and align their outputs with user-defined standards.}

\steven{\citet{fan2022human} demonstrated that explanations significantly enhance UX evaluators’ understanding of AI decisions, particularly in identifying usability problems.
They highlighted that explanations play a pivotal role in improving the usability of AI-assisted tools for UX evaluation. 
Unlike their study, which focused on tasks with pre-set standards, our investigation explores a dynamic context where task standards evolve iteratively through user interactions with the system. Here, we aim to understand how explanations can support the development of new standards in such iterative processes.}

\steven{While previous studies have explored how explanations can enhance human performance from the labeler’s perspective, our study focuses on the requester’s viewpoint, which remains an underexplored area requiring further investigation.}