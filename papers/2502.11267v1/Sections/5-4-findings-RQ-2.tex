
\begin{table*}[t]
\centering
\small
\begin{tabular}{lccccccccccc}
 & \multicolumn{3}{c}{\multirow{2}{*}{\textbf{Human}}} & \multicolumn{8}{c}{\textbf{DSPy}} \\ \cline{5-12} 
 & \multicolumn{3}{c}{} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Simple\\ Prompt\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Bootstrap-\\ FewShots\end{tabular}}} & \multicolumn{2}{c}{\textbf{COPRO}} & \multicolumn{2}{c}{\textbf{MIPRO}} \\ \hline
\multicolumn{1}{c}{} & \textbf{\begin{tabular}[c]{@{}c@{}}Avg.\\ \#Shot\end{tabular}} & \textbf{ACC$\uparrow$} & \textbf{MSE$\downarrow$} & \textbf{ACC$\uparrow$} & \textbf{MSE$\downarrow$} & \textbf{ACC$\uparrow$} & \textbf{MSE$\downarrow$} & \textbf{ACC$\uparrow$} & \textbf{MSE$\downarrow$} & \textbf{ACC$\uparrow$} & \textbf{MSE$\downarrow$} \\ \hline
\textbf{Initial} & 0.00 & .542 & .782 & - & - & - & - & - & - & - & - \\ \hline
\textbf{1st Revision} & 2.52 & .536 & .795 & .533 & .864 & .526 & .915 & \underline{\textbf{.565}} & .822 & .527  & .973 \\
\textbf{2nd Revision} & 4.80 & .542 & .833 & .533 & .864 & .534 & .934 & .538 & .873 & .536 & .862 \\
\textbf{3rd Revision} & 7.29 & \underline{\textbf{.549}} & .823 & .533 & .864 & \underline{\textbf{.550}} & .850 & \underline{\textbf{.544}} & .801 & \underline{\textbf{.547}} & .873 \\
\textbf{4th Revision} & 10.04 & \underline{\textbf{.553}} & .810 & .533 & .864 & .526 & .889 & \underline{\textbf{.554}} & .809 & .536 & .874 \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}End of Session\\ (Avg \#Iter=4.75)\end{tabular}} & 11.14 & \underline{\textbf{.546}} & .815 & .533 & .864 & .535 & .873 & .528 & .817 & .528 & .868
\end{tabular}
\caption{Comparison of ACC and MSE between users' original prompts and prompts improved by four DSPy approaches. DSPy showed limited effectiveness in enhancing ACC or MSE, potentially due to the small number of gold shots. Improvements over the initial prompt are bolded and underlined.}
\label{tab:dspy-results}
\end{table*}


\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{Figures/Results/Participant_DSPy/participant_dspy_ACC_plot.png}\Description{This is an average accuracy plots subfigure of the participants prompt performance. There are scatter points for each DSPy algorithm from 1 to 4 iterations.}
        \caption{Average ACC of participants' prompts (Original) compared to prompts improved by DSPy's four approaches across each iteration.}
        \label{fig:dspy-plot-acc}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{Figures/Results/Participant_DSPy/participant_dspy_MSE_plot.png}\Description{This is a MSE accuracy plots subfigure of the participants prompt performance. There are scatter points for each DSPy algorithm from 1 to 4 iterations.}
        \caption{Average MSE of participants' prompts (Original) compared to prompts improved by DSPy's four approaches across each iteration.}
        \label{fig:dspy-plot-mse}
    \end{subfigure}
    
    \caption{Average performance comparison between participants' prompts (Original) and those improved by DSPy's four approaches. DSPy was not effective in enhancing ACC or MSE, potentially due to the small number of gold shots.}
    %\kenneth{The space between these two subfigures should be wider. Captions stitch together....}\steven{done}
    \label{fig:dspy-four-settings-charts}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{Figures/Results/New-Participant-DSPy/new_acc_plots_dspy_v2.png}\Description{It contains 20 subfigures of average ACC of each participant plus DSPy Bootstrap scatter.}
    \caption{ACC of all participants compared to DSPy's BootstrapFewShots approach in each iteration. DSPy was not reliable in providing consistent improvements. (Some DSPy dots are missing because participants did not provide examples required for generating augmented samples in those iterations.)}
    \label{fig:acc-participant-plus-dspy}
\end{figure*}

%\steven{done. Make the plot a little transparent to ensure the DSPy dots covered by plot line more visible}

\begin{figure*}
    \centering
    \includegraphics[width=0.92\linewidth]{Figures/Results/New-Participant-DSPy/new_mse_plots_dspy_v2.png}\Description{It contains 20 subfigures of average MSE of each participant plus DSPy Bootstrap scatter.}
    \caption{MSE of all participants compared to DSPy's BootstrapFewShots approach in each iteration. DSPy was not reliable in providing consistent improvements. (Some DSPy dots are missing because participants did not provide examples required for generating augmented samples in those iterations.)}
    \label{fig:mse-participant-plus-dspy}
\end{figure*}

%\steven{done}\steven{todo: missing dots. done}

The previous section demonstrated that only a few settings were effective in helping participants improve prompt performance. 
This raises the question of how automatic prompt optimization tools might assist with refining prompts.
In our study, we explored DSPy~\cite{khattab2023dspy}, a framework designed to algorithmically optimize LLM prompts, to enhance the prompt at each stage of revision by participants. 
DSPy is particularly effective at working with small sets of labeled data and abstract, generic initial prompts, making it well-suited for the ``prompting in the dark'' scenario.

\paragraph{Study Setups.}

%Specifically, 
We experimented with the following four approaches offered by DSPy:

\begin{itemize}
\item 
\textbf{Simple Prompt (Baseline):}
This approach uses the abstract prompts constructed by DSPy and employs DSPy's simplest teleprompter, BootstrapFewShots, to generate optimized examples based on all the few-shot examples labeled by participants throughout the study session. 
It is a simple method that does not account for differences between iterations or the user's initial prompt, making it a baseline approach for using DSPy.
\item 
\textbf{BootstrapFewShots:}
This approach uses the task context (from Context sheet in \system), label definitions (Rule Book sheet), and few-shot examples (Shots sheet) provided by participants in each iteration, and applies DSPy's simplest teleprompter, BootstrapFewShots, for optimization. 
The BootstrapFewShots teleprompter automatically generates optimized examples to be included in the user-defined prompt based on the provided few-shot examples.
BootstrapFewShots is recommended when only a small amount of labeled data is available, such as 10 examples.\footnote{The recommended amount of data is based on DSPy's documentation: https://dspy-docs.vercel.app/docs/building-blocks/optimizers}
\item 
\textbf{COPRO:}
This approach is identical to the BootstrapFewShots setup but uses the COPRO teleprompter instead. 
The COPRO teleprompter focuses on optimizing the prompt instructions while keeping the few-shot examples constant. 
This enables the generation of more refined prompt instructions, even when labeled examples are limited or absent.
\item 
\textbf{MIPRO:}
This approach is identical to the BootstrapFewShots setup but uses the MIPRO teleprompter instead. 
MIPRO combines the features of both COPRO and BootstrapFewShots, refining the prompt instructions while also generating optimized examples using the provided few-shot data. 
MIPRO is recommended when a slightly larger amount of labeled data is available, such as 300 examples or more.
\end{itemize}

All the prompts were optimized with the goal of maximizing accuracy (ACC).

%\kenneth{Not really!}
%\subsubsection{Using DSPy}

\subsubsection{DSPy was not effective in improving ACC or MSE, possibly due to the small number of gold shots}
%Table~\ref{tab:dspy-results}
As shown in Table~\ref{tab:dspy-results} and Figure~\ref{fig:dspy-four-settings-charts}, none of the DSPy algorithms consistently improved ACC or MSE.
This may be attributed to the limited number of gold labels generated in our study, which was insufficient for DSPy to operate effectively, especially given the difficulty of a 5-class classification task. 
Table~\ref{tab:dspy-results} details the average number of gold shots used for DSPy in each revision.

\subsubsection{DSPy was not reliable in delivering consistent improvements}
Figure~\ref{fig:acc-participant-plus-dspy} and Figure~\ref{fig:mse-participant-plus-dspy} display the performance of DSPy's BootstrapFewShots (red dots) alongside human performance (blue line, which is the same as in Figure~\ref{fig:average-acc-mse-performance} for RQ1.)
These results indicate that DSPy's performance was unreliable across participants, as none of the DSPy algorithms consistently improved accuracy or MSE. 
We chose to plot the results for BootstrapFewShots because there was no significant difference across the four DSPy approaches we tested, and BootstrapFewShots was specifically recommended by DSPy's documentation when working with only 10 examples.
Some DSPy dots were missing because participants did not provide examples prior to those iterations, which was necessary for the DSPy algorithm to generate new augmented samples for prompt optimization. 
%\steven{respond to missing dots}










%-------------- dead kitten --------------
\begin{comment}


\input{Sections/5-1-2-experimental-result-with-DSPy}


Figure~\ref{fig:acc-participant-plus-dspy} and Figure~\ref{fig:mse-participant-plus-dspy} show that DSPy performance is unstable as none of the DSPy algorithms consistently improve accuracy or MSE.
When comparing the improvement in prompt labeling accuracy and MSE among all participants, there is \textbf{no significant difference} within each model. 










In our study, in addition to relying entirely on users' judgements, we also experimented with DSPy, a framework for algorithmically optimizing LLM prompts, to attempt to improve human performance of the prompt.
DSPy can work with small set of user labeled data and abstract, generci initial prompt, which fit in the ``prompting in the dark'' scenario.
In particular, we experimented the following four approaches provided by DSPy:

Althought


, explored a wide range of prompt refinement algorithms provided in .\kenneth{Add citations}
W


a system designed to optimize prompt engineering for large language models (LLMs) by providing various teleprompters that refine prompts and examples based on user input and task requirements. 
Our goal was to identify the best-performing prompt for each user iteration, with a focus on optimizing for accuracy.


    
\end{comment}