This paper studies a scenario in LLM-powered data labeling called ``prompting in the dark,'' where users iteratively prompt LLMs to label data without relying on a stabilized, pre-labeled gold-standard dataset for benchmarking. 
%In this scenario, users' understanding of the data and their desired labeling scheme evolves through their interactions with the LLM and its outputs. 
We developed \system, a Google Sheets add-on that allows users to compose, revise, and iteratively label data within spreadsheets. 
Our user study revealed that prompting in the dark was highly ineffective and unreliable, and automated prompt optimization tools like DSPy struggled when few gold labels were available. 
We concluded the paper with a set of design recommendations. 
%starting with a small set of gold labels is a reasonable compromise, and systems should be designed to minimize unnecessary distractions as well as to mitigate overreliance on LLMs' predictions. 
Based on these insights, our next step is to explore automation technologies, such as automatic rule creation, to further support users. %this exploratory, iterative data labeling process. 
Simultaneously, we will investigate methods to track system performance under evolving standards with minimal labeling effort. 
Additionally, we aim to better capture users' understanding and use this information to monitor and improve LLMs, fostering more effective human-AI collaboration. 
%At the meantime, will explore methods to track system performance under evolving standards with less labeling efforts. Furthermore, we will investigate how to better capture human's understanding and monitor LLMs with the captured information to better assist human-AI collaboration.
Finally, we plan to enhance and deploy \system for more users
%the system's implementation and workflow in the next version, using the improved \system for a longer-term deployment study 
to observe how people prompt LLMs to label data they truly care about in real-world scenarios.

