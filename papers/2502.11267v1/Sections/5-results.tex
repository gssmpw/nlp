In this section, we organize our findings under each research question (RQs) mentioned in Section~\ref{sec:rq}.

\subsection{RQ 1-1: How effective are people at prompt engineering in the ``prompting in the dark'' scenario?}
\input{Sections/5-1-findings-RQ-1-1}


\subsection{RQ 1-2: How does sample size affect human performance in prompt engineering?\label{sec:rq-1-2}}
\input{Sections/5-2-findings-RQ-1-2}

%\kenneth{Bigger is better!}

\subsection{RQ 1-3: How does displaying LLM explanations impact human performance in prompt engineering?\label{sec:llm-explanation-result}}
\input{Sections/5-3-findings-RQ-1-3}

%\kenneth{Mixed results and somewhat negative... maybe not.}

\subsection{RQ 2: Can automatic prompt optimization tools like DSPy improve human performance in ``prompting in the dark'' scenarios?}
\input{Sections/5-4-findings-RQ-2}


%\subsection{Additional Analysis}
%\kenneth{Not sure we need this but let me put it here for now.}
%When pairing LN and 50Y, 4 out of 5 participants improved their labeling accuracy. When combining LY and 50N, all participants' labeling accuracy dropped. In MSE, combinations including 50Y are slightly better than combinations with 50N. 

%In short, not having access to LLM explanations (LN) and exploring 50 instances per iteration (50Y) are the best strategy combinations in \system. 



%\kenneth{-----------------------KENNETH IS WORKING HERE------------------}

%In this section, we first overview the comparative results of human-refined and DSPy-refined prompts (Section XX) and then show the results of incorporating DSPy into human-refined prompts (Section XX). 

%\subsection{\system Results}


% \subsection{In-lab Human Refined Prompts}

