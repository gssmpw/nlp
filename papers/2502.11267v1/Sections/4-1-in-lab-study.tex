
%We conducted a 90-minute in-lab user study with participants using \system for an annotation task.

\subsubsection{Pilot Study\label{sec:pilot-study}}
Three participants were recruited through the authors' network for the pilot study. 
In the first pilot, we used CODA-19~\cite{huang-etal-2020-coda} as the data annotation task, where participants labeled text segments from academic abstracts into categories such as background, purpose, and findings.
We observed that the participant consistently agreed with nearly all the labels and did not suggest further refinements. 
This may have been due to the highly specialized nature of the abstracts, which made it difficult for a broader audience to fully understand and evaluate the labels. 
As a result, we decided to switch to a Twitter Sentiment task for the second pilot.
In this second pilot, we found that our guidelines were too flexible, leading to participant confusion and uncertainty about how to proceed. 
We made adjustments to provide more structure, such as requiring participants to complete at least four iterations, with each iteration involving the annotation of 10 out of 50 instances. 
After verification, participants were instructed to refine their rules and add gold standard labels.
Based on the results of the two pilot studies, we extended the study duration from 60 to 90 minutes to give participants enough time to learn the system and complete the tasks. Compensation was also adjusted to \$20. 
We tested these revised settings with the third participant and confirmed that the procedure worked effectively.
%Two participants were recruited via the authors' network for the pilot study. 
%\kenneth{TODO: say a few words about pilot study? what did we change after pilot study?}
%In the first pilot study, we used CODA-19~\cite{huang-etal-2020-coda} as the data annotation task, where participants annotated text segments from an academic abstract into background, purpose, method, finding, and others. We observed that the participant consistently agreed with almost all labels and did not suggest further refinement during the verification process. Additionally, while the LLM labels were different than the dataset gold labels, LLM labels and LLM explanations were logically sound. 
%Based on these findings, we decided to switch the annotation task to a Twitter Sentiment task for the second pilot. In this study, we found that our guidelines were too flexible, leading to participant confusion and uncertainty about how to proceed. To address that, we simplified and standardized the user study procedures. For example, we required them to do at least four iterations, with each iteration involving the annotation of 10 out of 50 instances. After verification, participants were instructed to refine their rules and add gold standard labels.

%Based on the results of two pilot studies, we extended the study duration from 60 minutes to 90 minutes to allow participants to have sufficient time to learn the system and complete the annotation tasks. The compensation was also adjusted to \$20.



\subsubsection{Participants Recruitment, Backgrounds, and Grouping}
%Recruitment
For our main study, we focused on recruiting individuals with reasonable familiarity with LLMs but relatively new to large-scale text data annotation. 
While \system is designed as an end-user prompting tool, in this study, we prioritized participants likely to represent the first wave of ``newcomers'' (as noted in our Design Goals in Section~\ref{sec:3-system-design}) entering LLM-powered data annotation. %\kenneth{TODO: Update the section ID} \steven{added}
This focus allowed us to avoid the need to teach participants the basics of LLMs, prompting, or text data annotation.
%For the main study, 
We recruited 20 participants from diverse educational backgrounds through the authors' networks, social media posts, and mailing lists within the authors' institute. 
The group included 1 Post-doctoral Researcher, 9 Ph.D. students, 9 Master's students, and 1 Undergraduate student.  
As part of the recruitment process, we specifically sought participants who met the criteria of possessing prior experience using LLMs. 
%\steven{added recruitment part}
%\steven{one participant was dropped because he did not attend the makeup session, should we mention that?}
Participants were compensated \$20 for their participation, and in our analysis, they are denoted as P1 to P20.
%\steven{Our system is designed for requesters or researchers who understand their task requirements. However, due to recruitment constraints, we could not involve participants with extensive data annotation experience. To address this, we chose a subjective task like Twitter sentiment analysis, where prior annotation expertise is less critical. This approach allows participants to rely on their own knowledge and judgment, simulating real-world scenarios where individuals guide LLMs in tasks that naturally depend on personal interpretation and expertise.}

%Backgrounds
%\kenneth{------------------KENNETH IS WORKING HERE---------------------------}

%\kenneth{------------------KENNETH IS WORKING HERE---------------------------}

%Grouping
Participants were randomly and evenly assigned to four groups based on two variables: 
% (1) whether or not they could view the LLM's explanations for its labels\footnote{\steven{Since two participants who had access to LLM explanations chose to turn them off, we grouped them with the no LLM explanation participants, resulting in 8 participants with access to LLM explanations and 12 participants without access.}}, and 
% (2) whether they had access to 50 instances per iteration or 10 instances per iteration.
(1) whether they had access to 50 instances per iteration or 10 instances per iteration, and
(2) whether or not they could view the LLM's explanations for its labels\footnote{Since one participant chose to disable the LLM explanations after the first iteration and another participant decided not to use the LLM explanations throughout the entire study, we grouped them with the no LLM explanation participants, resulting in 8 participants with access to LLM explanations and 12 participants without access.}.
Further details are provided in the study procedure section (Section~\ref{sec:study-procedure}). 

\paragraph{Survey on Participants' LLM Familiarity and Usage.}
To assess participants' familiarity with using LLMs, we conducted an optional post-study survey, offering an additional \$5 compensation for completion. 
(The full set of survey questions is provided in Table~\ref{tab:participants-llm-background-survey} in Appendix~\ref{sec:appendix=participant-background}.) %\kenneth{UPDATE REF}\steven{updated}
All participants responded. %\kenneth{UPDATE NUMBER}\steven{updated}
Most participants reported being familiar with LLMs, with an average familiarity score of 4.20 (SD=0.77) on a 5-point scale. %\kenneth{UPDATE NUMBER}\steven{updated}
15 participants had over one year of experience using LLMs, while 4 reported more than four months of experience, and 1 reported between one and three months. %\kenneth{UPDATE NUMBER} \steven{updated}
In terms of usage frequency, 16 participants used LLMs daily, 3 used them weekly, and 1 used them monthly. %\kenneth{UPDATE NUMBER} \steven{updated}
%Interaction durations varied: eight participants engaged with LLMs for more than 30 minutes, four for less than 5 minutes, three for 5-15 minutes, and four for 15-30 minutes. 
While most participants used LLMs for general tasks such as Q\&A, research, writing assistance, and programming/debugging, only 5 participants had experience using LLMs for data labeling. %\kenneth{UPDATE NUMBER}\steven{updated}
Participants rated their confidence in crafting prompts and their proficiency in interacting with LLMs similarly, with average scores of 3.75 (SD=0.85) and 3.85 (SD=0.88), respectively. %\kenneth{UPDATE NUMBER}\steven{updated}
%Many employed prompt engineering techniques, ranging from simple input adjustments to advanced methods like iterative refinement, system message editing, in-context learning, and chain-of-thought prompting to enhance LLM performance.
Overall, the participants represented individuals familiar with LLMs but relatively inexperienced with large-scale data annotation.



\subsubsection{Labeling Task, Scheme, and Data}
We selected the Coronavirus Tweet NLP Text Classification task, which categorizes tweets into five sentiment categories: Extremely Positive, Positive, Neutral, Negative, and Extremely Negative, using the dataset hosted on Kaggle.\footnote{Coronavirus tweets NLP - Text Classification: https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification/}
The dataset contains tweets from December 30, 2019, to September 7, 2020. 
For our study, we randomly sampled 1,060 tweets: 10 tweets were used for the tutorial task, 1,000 for the main study, and 50 for the final evaluation set (see Section~\ref{sec:study-procedure}).

This task was chosen, partially informed by our pilot study (Section~\ref{sec:pilot-study}), for several reasons. 
First, it strikes a balance in difficulty, being challenging enough to require iterative prompting efforts from LLMs, as a 5-class sentiment task is more complex than typical 2-class (positive, negative) or 3-class (positive, negative, neutral) sentiment classification tasks. 
Second, it avoids requiring specialized knowledge, ensuring a broad pool of potential participants. 
Tasks demanding domain-specific expertise would have significantly restricted recruitment; sentiment labeling for general COVID-related tweets is sufficiently accessible for this purpose. 
Finally, the task incorporates a subjective element, as it lacks universally agreed-upon gold labels. 
This aligns with our focus on ``prompting in the dark,'' where participants' understanding of the data, as well as labeling goals, evolve through iterations.
The subjective nature of the task allows participants to arrive at differing gold standards by the end of the process.
Considering these factors, we selected this task for our study.




%We chose this task during our pilot study (Section~\ref{sec:pilot-study}), primarily for its accessibility and its somewhat subjective nature: 
%sentiment analysis does not require specialized expertise; 
%the evaluation often depends on personal judgment.
%\kenneth{TODO Kenneth: This is not very accurate. need to revise later.}
%\steven{In contrast to tasks with strictly defined objectives, such as CODA-19~\cite{huang-etal-2020-coda}, which require extensive domain knowledge, subjective tasks like Twitter sentiment analysis do not have universally correct answers, relying instead on personal judgment for evaluation. 
%In this study, we encouraged participants to guide the LLM to align with their individual standards rather than steering it toward a fixed standard grounded in domain expertise}
%This \steven{task} allowed participants to guide the LLM according to their own interpretation of sentiment, particularly in deciding what qualifies as ``extremely'' positive or negative.





\subsubsection{Study Procedure\label{sec:study-procedure}}
For our main user study, most sessions were conducted remotely via Zoom or Microsoft Teams, with each session lasting between 87 and 127 minutes. 
Participants who attended in person used one of the author's laptops, while remote participants used their own computers. 
Since \system was a Google Add-on in a development version, it was installed on one of the author's laptops. 
Remote participants were given control of this laptop to conduct the experiment. 
Each session was recorded, capturing the screen, audio, and video for further analysis.

The study followed these steps:

\begin{enumerate}

\item 
\textbf{Onboarding:}
Participants were first introduced to the study's objectives and procedures, and their informed consent was obtained.

\item 
\textbf{Tutorial Task:}
Participants were then presented with a tutorial on the system's workflow and features, either delivered by one of the authors or via a prerecorded video, depending on their preference. 
Afterward, they completed a short tutorial task, identical to the main study task but involving only 10 data instances, to ensure their understanding of the system.

\item 
\textbf{Main Study:}
Participants were then asked to use \system to iteratively compose a prompt to label the sentiment of COVID-related tweets in alignment with their personal judgment of sentiment scores. 
Each participant was asked to complete at least four iterations (\ie, going through Steps 1 to 4 four or more times).
Participants were free to do additional iterations beyond the required four; on average, participants completed 4.75 iterations.
%\kenneth{update numbers}\steven{done}

Depending on their assigned group, participants used \system to annotate either 50 or 10 instances per iteration and then review the results.
For participants working with 50 instances per iteration, we advised that it was not necessary to manually verify all labels, as that would take too much time. 
% Participants in the group with access to LLM explanations could manually turn off the explanations if they felt that reading them was too time-consuming; only a few participants chose to do so.
Participants in the group with access to LLM explanations were explicitly informed that they
could manually turn on the explanations if they felt that they needed reasoning for each label. 
Most participants turned on the LLM explanations in the first iteration;
however, one participant chose to disable the LLM explanations after the first iteration and another participant decided not to use the LLM explanations throughout the entire study.
% , but a few participants chose to disable them after the first iteration. 
% One participant opted not to use the LLM explanations throughout the entire study.
%\kenneth{Is this true? How does this work?}\steven{the default is off, they can choose to turn on or keep it off.}\kenneth{hmmm how did we test the effect in such case then? Did they turn it on very often??? Did we always ask LLM to provide explanations?}\steven{All participants turned on the LLM explanation in the first iteration and use LLM explanation. and three participants decided to turn off the LLM explanation. }\kenneth{how about implementation? Did we always ask for LLMs to give us explanations in our prompt, just some participants do not have acees to it?}\steven{yes, the LLM explanation always in the raw output. Our LLM explanation checkbox is used to display or not display the LLM explanation. The label outputs remain consistent for all participants. }\steven{my bad, one participant did not turn on the LLM explanation the whole time.}

%After reviewing the LLM-generated labels, participants were encouraged to refine their Context, Rule Book, and Shots sheets if they gained new insights into the task, or to add gold shots.
%All participants were required to complete at least four iterations (i.e., going through Steps 1 to 4 four times). 

\item 
\textbf{Manual Labeling of the Gold Set:}
Upon completing the iterative process, participants manually labeled 50 tweets based on their own sentiment judgments. 
These labels reflected the participants' understanding of the data and the final labeling they aimed to achieve by the end of the study session. 
These manually labeled tweets were used as an evaluation dataset to assess the performance of the participants' prompts; they were not used to train or fine-tune any AI models.









%Upon completing the iterative process, participants manually labeled 50 tweets based on their own sentiment judgments. 
%These manually labeled tweets served as the evaluation dataset to assess the performance of their prompt.
%\kenneth{TODO Kenneth: (1) Say it captured at last understanding of users. (2) WE do not use it for any training or fine-tuning!!!}

\item 
\textbf{Post-Study Survey and Feedback Collection:}
At the end of the session, participants completed a questionnaire to rate the system's effectiveness, performance, and accessibility. They were also asked the following questions:
(1) Without this tool, how would you typically approach prompt engineering?
(2) How does your prompt engineering process compare before and after using this tool?
(3) Did the system help you complete the tasks more efficiently? If yes, please explain how.
(4) What features did you find most useful?
(5) Would you be interested in using this annotation system in your regular work or study? If no, please explain why.
(6) Do you have any suggestions for making the system more suitable for your needs?

Upon completing the questionnaire, we verbally asked participants to provide some last comments about the workflow, labeling task, and our system.



\end{enumerate}



%--------------- dead kitten -----------

\begin{comment}

\steven{
This setup allowed us to examine the effects of different sample distributions on annotation outcomes.
The choice of 10 instances reflects the limited time available to participants, allowing them to complete detailed annotations effectively.
In contrast, a previous HITL study without pre-set gold labels utilized 30 sample candidates per iteration for human selection~\cite{liu2019deep}. 
To balance this approach, the 50-instance group was introduced to explore a broader data distribution, allowing us to investigate the trade-offs between accessing essential and diverse samples in a human-in-the-loop system, as emphasized in ~\cite{wu2022survey, le2010ensuring}. Furthermore, considering that our sentiment task comprised 5 labels and involved random sampling, selecting 50 instances could significantly increase the chance of participants observing and engaging with all 5 labels.}


%https://docs.google.com/spreadsheets/d/1wcIDftEfVoDAAwXBnXoMgAhMeG-o-T-fB4CRuXlhy1g/edit?gid=0#gid=0
\subsubsection{Participants LLMs Background}\steven{participants background on going}
We distributed an additional questionnaire (Table~\ref{tab:participants-llm-background-survey}) to gather background information on LLMs from the participants who attended, offering a \$5 compensation. Out of 20 participants, 18 responded.\steven{fill in number later}

Most participants reported being familiar with using LLMs, with an average familiarity score of 4.28 (SD=0.75) on a 5-point scale. The majority displayed advanced knowledge of LLMs, while others showed an understanding of general principles. Only one participant reported having a basic level of understanding.
Near all participants had over one year of experience using LLMs, with only four reporting more than four months of experience and one reporting between one and three months of experience.
Almost all participants reported daily use of LLMs, except three reported weekly and one reported monthly.
Participants reported varying durations of interaction with the LLM. Eight participants interacted with the LLM for more than 30 minutes, four participants reported interactions lasting less than 5 minutes, three participants interacted for 5-15 minutes, and four participants reported interactions lasting 15-30 minutes.
Majority of participants indicated using LLMs for academic research, professional tasks, and personal projects. Two participants mentioned using LLMs for entertainment, while one reported using them for communication and another for translation and writing polishing. 
Participants employed LLMs for a wide range of tasks. 
Only four participants reported using LLMs for data labeling.
The majority engaged LLMs for general Q\&A, research, writing assistance, and programming or debugging. Half of the participants used LLMs for data analysis and visualization, while five employed them for creative tasks. Additionally, one participant sought conceptual explanations from LLMs. 
Participants rated their confidence in crafting prompts and their proficiency in interacting with LLMs to generate desired answers similarly, with an average score of 3.83 (SD=0.86) and 3.89 (SD=0.90) separately. 

Several participants use OpenAI's API for tasks such as dataset generation, research, virtual assistant creation, and integration into automation systems.
Many participants engage in prompt engineering, ranging from simple input adjustments to advanced techniques like iterative refinement, system message editing, in-context learning, and chain-of-thought (CoT) prompting for better LLM performance.
Participants use tools like OpenAI Playground, customized assistants, and external resources like Reddit for task-specific prompts.
Some participants either do not currently use GPT API but remain open to exploring them in the future.


While our participants did not have extensive data annotation experience, their familiarity with LLMs and general understanding of the Twitter Sentiment task requirements allowed them to engage meaningfully with the system. 
This aligns with our goal of simulating real-world scenarios where requesters -- whether researchers or practitioners -- guiding LLMs to align with their own standards.\steven{TODO: discussion their suitability}






\kenneth{--------------------------KENNETH IS WORKING HERE------------------------}


We finalized our experimental procedures after two user studies. 
The user evaluation dataset session was moved from the beginning to the end of the main experiment, as participants were likely to form a more consistent standard after reviewing numerous data instances
To preserve the integrity of the study, we kindly requested the first two participants (P1 and P2) to rejoin a 15-minute user study to re-annotate the evaluation dataset. After they completed the makeup session, we compensated them with \$5.\steven{we asked users for the makeup session}



\paragraph{Setup System Environment} During the system setup process, users should open the setting window by clicking the ``Setting'' icon located at the top right of the add-on interface. 
In our user study, we labeled the first top field as ``Participant ID'' to easily distinguish between participants. For real deployment, we will rename it to ``Data Annotation Task Name'' to allow users to track their iterative guidelines for LLMs in different tasks.
\steven {This participant ID was used during the user study, identifying each participant. I think we can change it to a data annotation name, which can be used to track different annotation tasks in the future. }\steven{I will add interface screenshot for both}
After users enter their participant ID/Annotation Task Name, they can click the ``Save Participant/Task Name'' button to save the information.
More importantly, users have to input their OpenAI API Keys and click the ``Save API'' button to save the key.





\paragraph{Pre-task interview} The steps of the study were as follows: We first introduced participants to the study objectives and procedure, securing their consent. Then, we presented the workflow and feature tutorials. Afterward, participants were asked to complete a short trial task to ensure their understanding of the system.


% The in-lab user study lasted approximately 90 minutes. The session consisted of a pre-task interview with a trial task (30 minutes), a main experiment (50 minutes), and a post-questionnaire (10 minutes). Both the trial task and main experiment used \textbf{COVID Twitter Sentiment Task} as the annotation task.

% In the pre-task interview, we presented an orientation to explain the purpose of the study. Then, each participant would listen to an informed consent form and provide verbal consent. After that, we provided participants with an instruction on our system and the annotation task. In the end, we asked participants to complete a short trial task to ensure their understanding of the system. 

\paragraph{Main experiment} Participants were instructed to complete at least four iterations. 
Participants used \system to annotate either 50 or 10 instances per iteration. For those who access 50 instances, we advised them to quickly glance at instances and labels during the verification process to gather as many insights as possible. 
For participants reviewing 10 instances per round, the number of instances was adjusted according to their verification speed due to time constraints. 
After the annotation process was completed, participants started to review each instance and LLM-generated labels. Those assigned to the group with access to LLM Explanation had the option to choose whether or not to display the LLM Explanation. 

Following the verification of LLM labels, participants were encouraged to refine their rule books if they discovered new insights into the task or to add gold shots if they identified a data instance and its corresponding label as a valuable reference point.
If time remained in the main experiment, participants were asked to perform additional refinement iterations. 

% During the refinement iteration, we observed that participants tended to review all the LLM explanations when provided with the options. This could potentially influence participants to align their judgments more closely with those of LLMs. 
% To mitigate potential bias, we split all participants randomly into two groups: one group worked with explanations, while the other worked without them. 

Upon completing all refinement processes, participants were asked to annotate 50 tweets based on their judgments for the sentiment task. These annotated tweets would serve as the evaluation dataset to assess the performance of LLM guided by the participants.

\paragraph{Post-questionnaire session} Participants completed a questionnaire, rating the effectiveness, performance, and accessibility of the system. They were also given questions: 
\textbf{(1) Without this tool, how would you typically approach prompt engineering?
(2) How would you compare your prompt engineering process before and after using this tool?
(3) Did the system help you complete the tasks more efficiently? If yes, please explain how.
(4) What features did you find most useful?
(5) Would you be interested in using this annotation system in your regular work or study? If no, please explain why.
(6) Do you have any suggestions for making the system more suitable for your needs?}
We recorded audio, captured screens, and recorded all system actions for each user study. 




% At the beginning of the user study, we structured our in-lab study session to last 60 minutes, focusing on using our system to refine prompts for the CODA-19 data annotation scheme~\cite{huang-etal-2020-coda}. The session consisted of a pre-task interview with a trial task (15 minutes), a main experiment (35 minutes), and a post-questionnaire (10 minutes). 

% After the first pilot study, we observed that the LLM performance on the CODA-19 was too good for the first participant to provide nuanced insights to improve prompts further. 
% Thus, we modified our annotation task to a Coronavirus tweet NLP Text Classification task~\footnote{https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification/}, which we will refer to as the COVID Twitter Sentiment Task in subsequent sections.
% However, after conducting the first two pilot studies, we found that the learning phrase on the system took more time than anticipated, and the main experiment annotation also required more time. To improve accessibility and track the refinement process, we extended our session to 90 minutes, increasing the pre-task part to 30 minutes and the main experiment to 50 minutes. 



% \subsection{Deployment Study}


\subsubsection{In-lab Session Procedure}
Most sessions were conducted remotely through Zoom or Microsoft Meetings. Each session typically spanned 87 minutes to 127 minutes. Participants who attended in person used one of the author's laptops, while those who joined remotely used their computers. Since the system was a Google Add-on development version, it was installed on one of the author's computers. Hence, we granted remote participants control of the author's laptop to experiment. 

\subsubsection{Annotation Scheme}
For our in-lab study, we chose to use a Coronavirus tweet NLP Text Classification task~\footnote{https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification/}, which categorized tweets into one of five categories, \textit{i.e.}, Extremely Positive, Positive, Neutral, Negative, and Extremely Negative. In subsequent sections, we will refer to this task as the \textbf{COVID Twitter Sentiment Task}.

This task was picked for its accessibility and flexible standards:
it did not require expertise and knowledge in analyzing tweet sentiments; evaluation of sentiment analysis often relied on personal judgment, allowing different participants to guide the LLM according to their personalized classification standards.

\subsubsection{Dataset}
The COVID Twitter Sentiment dataset contains tweets from December 30, 2019, to September 7, 2020 from Twitter. In this study, we randomly sampled 1,060 tweets from the dataset. We used 10 tweets for the trial task, 1,000 for the main experiment, and 50 for the evaluation set.


For the main study, the participants were recruited via authors' networks, social media posts, and mailing lists in the authors' institute. 
We recruited 20 participants from diverse educational backgrounds (1 Post-doctoral Researcher, 9 Ph.D. students, 9 Master's students, and 1 Undergraduate student). 
All participants had experience with using LLMs. 
Participants were compensated with \$20 for their participation. 
In our analysis presented in the paper, we denote participants as P1 to P20.

Participants were randomly and evenly assigned to whether accessing to LLM Explanation or not and were further assigned to whether access to 50 instances per round or not. 
\end{comment}