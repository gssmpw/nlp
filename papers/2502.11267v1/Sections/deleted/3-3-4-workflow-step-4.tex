%\subsubsection{Observe and revise the prompt}
The annotation results will be saved to a new task tab. Users can start verifying LLM labels. Users who disagree with an LLM label can assign a new human-generated label to the data instance under the ``Human Label'' column. If users find good examples that can be used for later LLM learning, they can check the ``Gold Shot'' checkboxes. They can also check the ``Keep it in the next data sample'' checkboxes if they want to re-annotate data instances.

If users are not satisfied with the annotated results, they can refine the prompt tabs by modifying the ``Context'' answers, updating ``Rule Book'' definitions, and adding gold standard examples to the ``Shots'' tab. 


%\subsubsection{Iterations}
During the Observation process, users can check the ``Keep it in the next data sample'' checkboxes for data instances they want to re-annotate in the next iteration. Once the process is complete, they can click the ‘Add Back’ button to copy these instances to the ``Working Data Sample'' tab.  
From there, they can re-sample or re-use instances for the next iteration.
