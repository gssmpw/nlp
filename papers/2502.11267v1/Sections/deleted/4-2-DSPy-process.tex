





In our study, we tried a wide a range of prompt refining algorithms from DSPy to get the best performing prompt for each user iteration.
We started with a Simple Prompt approach, that uses the abstract prompts constructed by DSPy. This approach employs the simplest DSPy teleprompter, BootstrapFewShots, to generate optimized examples using all the few-shot examples annotated by the user during the experiment. Thus, essentially optimizing the prompts using few-shot learning.


Apart from the Simple Prompt approach, we also evaluated other teleprompters offered by DSPy namely, BootstrapFewShot, MIPRO and COPRO. For these optimizers we used the initial task instructions, label definitions and few shot examples provided by the user for each iteration. The BootstrapFewShots teleprompter automatically generates to include in the user defined prompt based on the few-shot examples provided. 
The COPRO teleprompter focuses on optimizing the prompt instructions themselves while keeping the few shot examples constant. This allows for generating more optimized prompt instructions even in the absence of labeled examples.
The final teleprompter we tried was the MIPRO. MIPRO is a combination of both COPRO and BootstrapFewShot teleprompters in the sense that it does prompt refining as well as generates optimized examples using the few-shot examples provided. 




In this manner, we have investigated how optimizing different parts of the prompt has effects on the performance.