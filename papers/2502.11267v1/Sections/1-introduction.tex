\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/prompting-sheet-overview.jpg}\Description{This is our user workflow which includes four iterative steps. Before the steps, users need to have a full dataset imported. In Step 1, users need to compose/refine the prompt by working on Context, Rule Book, and Shots sheets. In Step 2, users need to sample/resample a subset of data from the dataset. In Step 3, after users click Start Annotation, the system will use the prompt from step 1 to instruct the LLM to label the data sample. Then annotated results will be saved in a new tab. In Step 4, users will observe the annotation results. If they want to keep improving the LLM performance, they will need to go back to Step 1 to refine the prompt. If they are satisfied with the results, they can end the annotation workflow.}
    \caption{\system is a Google Sheets add-on that allows users to compose prompts~(Step 1), use those prompts to instruct LLMs to label data~(Steps 2 and 3), review the resulting labels and optional explanations, and iteratively revise and relabel data~(Step 4)---all within the same Google Sheets document. The process does not begin with users manually labeling data; instead, users' understanding of the data and labeling scheme evolves as they interact with the LLM and review its outputs.}
    \label{fig:system-workflow-fig-v2}
\end{figure*}

Large language models (LLMs) have empowered millions, if not billions, to perform a wide range of programming and data science tasks, even without formal technical backgrounds.
%\kenneth{Maybe do not say without formal technical backgrounds. Maybe people work in tech but without formal software engineering background....? hmm}\steven{sounds good}
People can ask LLMs to teach them step-by-step how to build a web app from scratch~\cite{voronin2024development}, %\kenneth{TODO: Add one more example}\steven{added}
have LLMs analyze data and generate insights~\cite{ma-etal-2023-insightpilot,laradji2023capture}, %\kenneth{TODO: Add one more example here} \steven{added}
or instruct LLMs to label thousands of data items~\cite{10.1145/3613904.3642834,horych2024promises,he-etal-2024-annollm}. %\kenneth{TODO: Add two more examples} \steven{added}
All these were made possible by LLMs' ability to converse in natural language and to follow users' instructions, also known as ``prompts.'' 
The practice of ``prompt engineering'' has emerged to describe the process of developing and optimizing prompts to use LLMs efficiently~\cite{chen2023unleashing,wang2023review,giray2023prompt}.
Tools like LangChain\footnote{LangChain: https://www.langchain.com/} and ChainForge~\cite{arawjo2024chainforge} were developed to facilitate complex chaining of prompts; technologies such as DSPy~\cite{khattab2023dspy} were also created to automate prompt optimization.
\textbf{However, how effective are people at prompt engineering in practice? 
Do users really get closer to their desired outcome over multiple iterations of their prompts?}
These questions are especially important when gold-standard labels are unavailable.
Decades of research indicates that gold-standard labels---data inputs paired with known correct predictions or ideal outputs (also referred to as ``gold labels'' or simply ``gold''~\cite{abdalla2023hurdles,wang2024impact, snow2008cheap,kilgarriff1998gold,silveira2014gold, wiebe1999development,sorokin2008utility}---are critical for evaluating annotation quality~\cite{han2020crowd,gadiraju2015training,nahum2024llms} and system performance~\cite{daka2014survey,ellims2004unit,hamill2004unit}. 
%\kenneth{TODO: We kinda sai this is ``decades of research'', so please find some older reference with a high citation count}\steven{Sure. Done}
However, in real-world scenarios, gold labels can be absent, expensive to obtain at scale, or difficult to use.
Millions of users iterate on prompts daily using ChatGPT's text-based chat interface~\cite{mortensen2024chatgpt, cnbc2024openai}, which does not provide any way to upload or evaluate against gold labels. 
%\kenneth{TODO: Cite 1 or 2 credible sources on how many users does ChatGPT have in 2024}\steven{this news said 300 million active users per week, should we rephrase saying weekly to be more accurate? } -- Kenneth: this should be fine
Researchers conducting qualitative coding with LLMs often do not begin with stabilized gold codes~\cite{dai2023llm,de2024further,gao2024collabcoder,zhang-etal-2024-glape, pang2023language}.
%\kenneth{TODO Kenneth: Maybe add LLM for explotory task} \steven{added! one is to evaluate prompt without gold label; another is to generate answers to unlabeled question and evaluate the quality of these answer, without pre-set standard.}
%\kenneth{TODO: Need 1 or 2 more ref on qualitative coding with LLMs WITH NO gold labels}\steven{done}
%\steven{zhang-etal-2024-glape work on prompt evaluation without gold labels; pang2023language presents an unsupervised method where LLMs act as both student and teacher. The model generates answers to unlabeled questions and evaluates the quality of these answers, assigning scores accordingly.}
%Many automated data annotation efforts face insufficient pre-labeled data due to 
%cost, \kenneth{TODO: Cite 1 or 2 papers that do not have gold labels because it's too expensive}
%privacy, or \kenneth{TODO: Cite 1 or 2 papers that do not have gold labels due to user privacy}
%expertise constraints.
%\steven{Several studies have noted that pre-labeled data was absent in real-world scenarios~\cite{liu2019deep, yang2019evaluating, oikarinen2021detecting, slote2024unlocking}. }
%\steven{It often presumes that practitioners have access to static, labeled datasets for evaluation and training. However, this assumption frequently diverges from real-world scenarios, where data is private, encrypted, or unlabeled~\cite{slote2024unlocking}.}
Many automated data annotation efforts face insufficient gold labels due to 
the high cost of gathering labels~\cite{wang2020learnability,liu2024zero}, %\kenneth{TODO: Cite 1 or 2 automatic labeling papers that do not have gold labels because it's too expensive} \steven{done}
difficulty recruiting experts for large-scale annotation tasks~\cite{caiafa2020decomposition, pais2024overcoming,wolf2023dealing}, %\kenneth{TODO: Cite 1 or 2 automatic labeling papers in which the task are very specialized and hard to have large scale dataset}\steven{done}
privacy restrictions~\cite{hathurusinghe2021privacy, yao2023labeling}, or %\kenneth{TODO: Cite 1 or 2 automatic labeling papers in which the gold labels are users' personal information and thus protected}\steven{done}
the lack of universally agreed standards~\cite{wang2024end, organisciak2014crowd}. %\kenneth{TODO: MAYBE cite Amy's paper + maybe 1 more paper in which the task is very subjective or personalized}\steven{done}
%\kenneth{I kinda feel that I put too many examples for automatic annotation cases..... Not sure.}
In such cases, without reliable benchmarks to track prompting progress, users can only rely on their own prompting ability to drive toward desired outcomes.
Yet, this ability is difficult to measure and thus remains understudied.
Without understanding how well users can prompt LLMs through iterations, it is hard to determine how much support users need---and when they need it---to improve their prompt engineering efforts effectively.


This paper investigates a particular scenario in LLM-powered data labeling, which we refer to as ``\textbf{prompting in the dark}.'' 
In these scenarios, \textbf{users' understanding of the data and their desired labeling scheme evolves through their interactions with the LLM and its outputs rather than through manual labeling.}
The process typically unfolds as follows: 
Users begin by writing a prompt to instruct the LLM to label unannotated text data. 
They then observe the outcomes, sometimes reviewing the LLM's explanations for its labels, and iteratively refine their prompt until satisfied with the results.
This iterative refinement process is a common practice in research contexts, where ``the prompt provided to the LLM is iteratively revised'' to analyze or label data~\cite{shah2024prompt}.
Although this prompt refinement process is often undocumented~\cite{shah2024prompt}, we have observed a growing trend of bypassing manual labeling entirely in favor of starting directly with LLM prompting. 
This approach, admittedly, has some usability advantages: 
it eliminates the upfront effort of manual labeling, as the LLM takes on the task, leaving users to focus on reviewing and validating the results. 
Furthermore, at any point in the process, users have an up-to-date prompt that reflects their evolving understanding of the task and the data, which can be immediately used for additional data labeling.
However, despite these seemingly appealing benefits, the core assumption underlying this prompting-in-the-dark approach---with each iteration, the LLM's performance improves, gradually converging on an outcome that aligns with the user's expectations---has yet to be rigorously validated.


To study users' prompt engineering practices for data labeling, we developed \textbf{\system}, 
a \textbf{spreadsheet-based end-user programming tool for prompt engineering in text data labeling tasks}.
%a \textbf{spreadsheet-based end-user programming tool for prompt engineering}.
\system was built as a Google Sheets add-on that allows users to compose prompts, use those prompts to instruct LLMs to label data, review the resulting labels and optional explanations, and iteratively revise and relabel data---all within the same Google Sheets document.
Figure~\ref{fig:system-workflow-fig-v2} shows \system's workflow.
Equipped with the system, we conducted an in-lab user study with 20 participants, who used the system to perform a 5-point sentiment labeling task on a tweet dataset.
%, rating sentiments on a 5-point scale (from Very Positive to Very Negative). 
We found that ``prompting in the dark'' was often unreliable and, at times, counterproductive. 
Of the 20 participants, only 9 improved their labeling accuracy when compared to the labels they manually annotated at the end of the session.
%\kenneth{TODO: This this still true?}\steven{Yeah, this is still true. We only move 2 participants from explanation group to non-explanation group. Participant's overall results do not change.}
% \kenneth{TODO Steven: Update the numbers.}
Even though providing %LLM-generated explanations and 
a larger data sample for users to review sometimes helped, many participants ended the session with worse labeling outcomes.
%many participants had worse labeling results by the session's end.
%in terms of both accuracy and mean squared error (MSE) 
We also investigated the effectiveness of an automated prompt optimization tool, DSPy~\cite{khattab2023dspy}, and found that although it helped in some cases, it struggled to reliably optimize prompts with the small number of user-validated labels generated during the process.

This paper contributes to the rapidly evolving discourse on human-LLM interaction, specifically focusing on measuring human performance in prompt engineering. 
Our findings highlight the necessity of starting with at least a small set of manually labeled data, which can act as a critical beacon when prompting in the dark, as well as the need for more automated support, such as typo checks and rule suggestions,
while ensuring designs that mitigate potential overreliance on LLMs' predictions. 
These insights can inform the development of future tools, enabling a wider range of users to prompt LLMs more effectively.
%to enhance the reliability of human prompt engineering. 
%These insights can guide the design of future tools, helping a broader range of users prompt LLMs more effectively.

\subsection{Research Questions\label{sec:rq}}

The central research question guiding our study is:

\begin{itemize}
    \item \textbf{RQ 1-1: How effective are people at prompt engineering in the ``prompting in the dark'' scenario?}
\end{itemize}

In addition to examining prompt engineering effectiveness, we aim to understand how key factors influence human performance when prompting in the dark. 
Among the many variables that could impact outcomes~\cite{kulesza2012tell}, two stand out:
(1) the size of the sample data in each iteration, and 
(2) whether the LLM's explanations for its labels are shown to users.
Accordingly, we address the following additional research questions:

\begin{itemize}
    \item \textbf{RQ 1-2: How does sample size affect human performance in prompt engineering?}
    \item \textbf{RQ 1-3: How does displaying LLM explanations impact human performance in prompt engineering?}
\end{itemize}

We acknowledge that many variables could influence the quality of labels in LLM-powered data annotation. 
These include variables studied in crowdsourced data labeling research, such as 
task type~\cite{hettiachchi2022survey, zhen2021crowdsourcing}, %\kenneth{TODO: Add references for all of these}\steven{added}
task difficulty~\cite{zheng2022virtual, zhang2021personalized}, 
instruction quality~\cite{zamfirescu2023herding, zhang2023llmaaa}, 
label diversity~\cite{kazai2012face}, and 
label aggregation methods~\cite{drutsa2020crowdsourcing, li2024comparative}.\footnote{Some variables in crowdsourced data annotation, such as workers' financial or intrinsic incentives, do not apply to LLM-powered data annotation.}
%We focus on the two key variables that we believe are widely applicable when iteratively prompting LLMs to label text data. 
%Prior research highlights the significance of sample size and explanations, with sample size influencing human-AI workflows and explanations serving as a primary method for LLMs to communicate reasoning. 
%These variables have been extensively studied for their impact on human-AI collaboration. 
We focus on these two key variables because they are widely applicable to any iterative prompting process with LLMs. Prior research shows that sample size significantly impacts human-AI workflows, while explanations are a primary means for LLMs to convey their reasoning to users.
Below, we provide additional context for these variables:


\begin{itemize}
    \item 


%We focus on these two variables based on the following rationale:

\textbf{Data Sample Size in Each Iteration.}
In iterative labeling research~\cite{lee2024clarify}, particularly within active learning~\cite{bernard2018vial}, a recurring challenge is the trade-off between the limited time, attention, and effort of human annotators and the need for AI models to learn effectively from human-labeled instances~\cite{pandey2022modeling,wang2016human}. 
%A common strategy is to identify and label the most challenging or ambiguous data points, as these provide the greatest learning benefit for the model.
This trade-off is a fundamental issue in many human-AI collaboration scenarios. 
In iterative prompt refinement, larger sample sizes give users more data to inform prompt revisions, potentially improving performance. 
However, reviewing large datasets in every iteration is labor-intensive and time-consuming. 
Striking a balance between providing sufficient data for effective prompt improvement and minimizing user effort is critical. 
Given the universal relevance of this variable in human-AI collaboration, we examine it as a key factor in this study.

\item
\textbf{LLM Explanations for Predictions.}
%Another key variable is whether users are provided with the LLM's explanations for its predictions. 
%Prior research suggests that 
Bi-directional communication between users and LLMs---where users receive feedback from the model rather than only giving instructions---is essential for effective collaboration~\cite{shen2024towards}.
%\kenneth{TODO: Cite Hua's bi-directionality AI alignment paper.}\steven{added}
When prompting in the dark, gold labels and metrics like accuracy are unavailable, LLM-generated explanations become one of the few ways the model can provide feedback to users~\cite{teso2023leveraging,kulesza2015principles}.
However, prior studies show mixed results regarding the effectiveness of AI-generated explanations in human-AI collaboration. 
Some studies suggest that explanations may hinder human understanding~\cite{turpin2024language,shen2020useful}, while others indicate they can enhance decision-making~\cite{singh2024rethinking,10.1145/3579605}.
A cost-benefit framework has been proposed to make sense of these mixed results, suggesting that explanations fail to help in some cases because they do not reduce the cognitive cost of verifying AI predictions~\cite{10.1145/3579605}.
In most text data labeling tasks, except for cases with highly complex text, we hypothesize that reading the entire text and then reviewing explanations to validate the AI label generally requires more effort than carefully reading the text and deciding the label directly. 
Based on this framework, LLM explanations may offer limited utility. 
To evaluate whether this applies to iterative prompt refinement for text labeling and to contribute to the broader discourse on human-AI collaboration, we studied explanations as a key variable.









%\kenneth{TODO: Cite the Explanation Can Reduce.... paper}\steven{added}
%In text data labeling, unfortunately, reading the entire text item and reviewing the text explanations often require similar cognitive effort. 
%To determine whether this is the case for iterative prompt refinement aiming to label text data, and also to contribute to the ongoing discourse on human-AI collaboration, we decided to study explanations as one of our key variables.

\end{itemize}


Lastly, we are interested in how much automatic prompt optimization tools, such as DSPy~\cite{khattab2023dspy}, can enhance human performance in prompting-in-the-dark scenarios. 
This investigation is important because, even when users struggle with effective prompting, tools like DSPy might still be able to refine human-crafted prompts and achieve satisfactory results.
%However, the main challenge is that prompting in the dark often involves too few labeled data points for optimization tools to learn effectively. 
%Even when users label a few data points for sanity checks, the dataset may be insufficient for optimization tools to learn effectively.
However, 
the main challenge is that prompting in the dark typically provides too few labeled data points for optimization tools to learn effectively, even with a few sanity-check labels from users.
To address this, we pose the following research question:


%A key challenge in these scenarios is that the labeling process often fails to generate enough labeled data for optimization tools to effectively learn from. 
%Therefore, we aim to address the following research question:


\begin{itemize}
    \item \textbf{RQ 2: Can automatic prompt optimization tools like DSPy improve human performance in ``prompting in the dark'' scenarios?}
\end{itemize}


%--------------------- dead kitten -------------------------------

\begin{comment}









A cost-benefit framework has been proposed to explain these mixed results:
Over-reliance on AI is a common issue in human-AI collaboration, but both {\em (i)} manually performing the task (\eg, reading text and selecting a label) and {\em (ii)} engaging with explanations carefully (\eg, reading the LLM's explanation and validating it) can reduce this tendency. 
%However, when the cognitive costs of these two strategies are ``nearly equal and both nontrivially costly,'' users see no clear benefit in favoring either.
However, when the cognitive costs of these two strategies are ``nearly equal and both nontrivially costly,'' users see no clear benefit in choosing either approach and are not motivated to adopt either action more frequently.
Evidence suggests that when tasks become more challenging and, therefore, using explanations becomes a noticeably less cognitively costly strategy, over-reliance on AI decreases.

one over the other, allowing over-reliance to persist.

Text data labeling often falls into this challenging cost space, as reading the full text and reviewing explanations require similar effort. To clarify whether this applies to iterative prompt refinement and contribute to the broader discussion on human-AI collaboration, we chose explanation as one of our key variables.





A cost-benefit framework has been proposed to interpret these mixed results.
It is known that users often over-rely on AI predictions, even when they are incorrect. 
However, when the cognitive cost of manually performing the task (\eg, reading text and selecting a label) is similar to the cost of engaging with the explanation (\eg, reading the LLM's explanation and validating it), users see no clear benefit in favoring one over the other. 
While both approaches can mitigate over-reliance, this similarity in cost allows the original over-reliance tendency to persist.

Text data labeling often falls into this challenging cost space where "these two costs are nearly equal and both nontrivially costly," as reading the full text and its explanation requires comparable effort. To clarify whether this dynamic applies to iterative prompt refinement and to contribute to the broader conversation on AI-human collaboration, we chose to study explanation as one of our key variables.









However, prior studies have shown mixed results regarding the benefits of AI-generated explanations in human-AI collaboration. 
Some studies indicate that explanations may hinder human understanding~\cite{turpin2024language,shen2020useful},
%\kenneth{TODO: Cite Hua's HCOMP paper}, 
while others suggest they can improve decision-making~\cite{singh2024rethinking,10.1145/3579605}.

A recent paper suggested that explanations may not improve outcomes when the cognitive cost of understanding and validating them is comparable to the cost of performing the labeling task itself. In such cases, users may over-rely on explanations, even when they are incorrect.

Text data labeling exemplifies this challenge because users must read and comprehend the text to make judgments, placing it in a "narrow cost space" where both tasks are equally burdensome and non-trivial. To clarify whether this dynamic applies to iterative prompt refinement and to contribute to the broader conversation about human-AI collaboration, we include LLM explanations as one of the core variables in our study.


\kenneth{------------------------KENNETH IS WORKING HERE--------------------}


During the iterative prompt engineering process, two key variables stand out: 
(1) the size of the sample dataset and 
(2) whether or not the LLM's explanations for its labels are displayed to users.

Given that the full dataset is likely to be quite large, in realistic scenarios users are likely to have the LLM label only a subset of the data during each iteration. 
This raises the question of how large the subset should be. 
If the subset is too small, it may fail to capture the characteristics of the full dataset. 
If it is too large, humans may struggle to review all the results, and labeling large datasets repeatedly with LLMs can become costly.

Additionally, while prior research suggests that LLMs tend to perform better when asked to provide explanations for their predictions, the impact of explanations on human decision-making is less clear.
Some studies indicate that explanations may hinder human understanding~\cite{turpin2024language,shen2020useful},
%\kenneth{TODO: Cite Hua's HCOMP paper}, 
while others suggest they can improve decision-making~\cite{singh2024rethinking,10.1145/3579605}.



This iterative prompt refinement is common in the research context, in which ``the prompt provided to the LLM is interatively revised'' in order to get data analyzed or labeled~\cite{shah2024prompt}.
%where the goal is to ``get some input data analyzed or labeled,'' requiring users to revise ``the set of instructions or the prompt provided to the LLM.''~\cite{}
Although this prompt revision process is often undocumented~\cite{shah2024prompt}, a growing trend that we observed is skipping manual labeling altogether and starting directly by prompting the LLM:
this approach eliminates the upfront effort of manual labeling, as the LLM performs the labeling, leaving users to review and validate the results.
Moreover,
at any point in the process, users have a prompt reflecting their up-to-date understanding of the task that can be used immediately for further data labeling.
The belief underlying prompting-in-the-dark practices is that, with each iteration, the LLM's performance improves, gradually converging on an outcome that aligns with the user's expectations. 
However, this belief has not been rigorously tested.


%has emerged of bypassing manual labeling altogether. 

\kenneth{------------------------KENNETH IS WORKING HERE---------------------}


Similar approaches with LLMs have recently appeared in fields such as qualitative coding~\cite{tai2024examination,xiao2023supporting}, query composition~\cite{dhole2023interactive}, and personalized classifier development~\cite{wang2024end}.\kenneth{TODO: Can we add one or two more references here to show that PITD is actually happening?}
%\kenneth{We probably need a few more citations in this paragraph to show that this practice exists and is somewhat popular. Maybe begin with some papers in the LLM for the qualitative coding workshop?}  
%\steven{Qualitative Coding LLM: \citet{tai2024examination} supports that LLMs may be used as a tool to streamline qualitative research data analysis, and LLM offers researchers the autonomy to design prompts to align with their interests. \citet{xiao2023supporting} indicates the feasibility of using LLMs for qualitative coding }
%\steven{\citet{dhole2023interactive} introduced the Query Generation Assistant for document retrieval tasks. 
%Users can interact with and refine queries generated by LLMs through feedback and adjustments. }
%\steven{\citet{wang2024end} suggested that tools should support rapid initialization and iterative refining for users who engage with social media to create personal classifiers. This task is highly personalized without a gold label at the beginning. This paper compared three strategies for building personal content classifier, examples, rule writing, and LLM prompting.}
%A common practice is to skip manual labeling altogether and start directly by prompting the LLM, which might contribute to the popularity of such approaches: It eliminates the up-front effort of manual labeling, as the LLM always performs the labeling, leaving users to review and validate the results. 
A practice we observed to be more frequent is skipping manual labeling altogether and starting directly by prompting the LLM: this approach eliminates the upfront effort of manual labeling, as the LLM performs the labeling, leaving users to review and validate the results.
%An additional benefit is that,
Moreover,
at any point in the process, users have a prompt reflecting their up-to-date understanding of the task that can be used immediately for further data labeling.
The belief underlying prompting-in-the-dark practices is that, with each iteration, the LLM's performance improves, gradually converging on an outcome that aligns with the user's expectations. 
However, this belief has not been rigorously tested.


However, in real-world scenarios, gold labels can be absent, costly to obtain at scale, or not easy to use effectively: millions of users iterate on prompts daily using ChatGPT's text-based chat interface, which does not offer a way to upload or measure against gold labels; researchers conducting bottom-up qualitative coding with LLMs lack gold labels at the outset; many automated data annotation efforts face the challenge of operating without sufficient pre-labeled data.
In such cases, 
no reliable benchmark can clearly and quickly benchmark progress,
users can only rely on their own prompting ability to drive toward desired outcomes.



---data items paired with known correct predictions or inputs paired with ideal outputs (also referred to as ``gold labels'' or simply ``gold'')---are unavailable, as in such cases, users can only rely on their own prompting ability to drive toward desired outcomes.


to clearly and quickly benchmark progress, as in such cases, users can only rely on their own prompting ability to drive toward desired outcomes.
Although it is known that gold-standard labels, \ie, data items paired with known correct predictions or inputs paired with ideal outputs (also referred to as ``gold labels'' or simply ``gold''), are critical for assessing annotation quality~\cite{han2020crowd,gadiraju2015training,nahum2024llms} %\kenneth{TODO: Add citations} \steven{done}
and 
system performance~\cite{daka2014survey,ellims2004unit, hamill2004unit} %\kenneth{TODO: Add classic citations for unit testing in software engineering}\steven{done}
, in real-world scenarios, gold labels can be absent, costly to obtain at scale, or hard to use.



Despite it being known that gold labels---ideal outputs for specific inputs---are critical for quality control,
%for evaluating system performance and quality in data pipelines, 
they can be absent, hard to obtain in sufficient quantities, or not easy to use in real-world scenarios.


\kenneth{-------------------KENNETH IS WORKING HERE-------------------------}



These questions are especially important when no gold-standard labels exist to clearly and quickly benchmark progress toward users' goals. 
as in such cases, users can only rely on their prompting ability to drive the progress.


\steven{In this context, our focus is on requesters who  engaged in diverse labeling tasks and in the design of data annotation pipelines.
}
%\steven{We conducted a survey study to explore how individuals use LLMs to label data. The results revealed that the majority of participants had less than 50\% of having gold-standard labels at the beginning of annotation process. }
% \steven{Based on our survey study(only 3 people participated), most participants indicated that they had less than a 50\% chance of obtaining gold-standard labels at the beginning of their annotation process. One participant mentioned that they did not have access to gold-standard labels in their past research experience. Instead, gold-standard labels were created through experience and collaboration with domain experts.}
In such cases, like prompting LLMs to generate long stories~\cite{zhou2023recurrentgpt} or perform bottom-up qualitative coding~\cite{dai2023llm}, how well users can generally craft and improve prompts to achieve their goal becomes one of the few key factors we can rely on to gauge the effectiveness of the prompt engineering process.
Yet, this ability is difficult to measure and thus remains understudied.
Without understanding how well users can prompt LLMs through iterations, it is hard to determine how much support users need---and when they need it---to improve their prompt engineering efforts effectively.

such as when prompting LLMs to generate long-form stories or conduct bottom-up qualitative coding. 
%These questions are particularly critical, and also underexplored, when gold-standard labels are absent
%On top of these, do prompt optimization tools like DSPy work equally well across all users' prompt iterations, from the early stages to the later refinements?
%These questions are even more critical 
Without understanding human performance in prompt engineering, it is difficult to determine how much support users need---or when they need it---to optimize their prompts effectively.

These questions are particularly critical when no gold-standard labels are available to clearly and quickly benchmark the progress toward users' goals and a key factor to understand (and to contextualize the understanding for) the effectiveness of prompt engineering is how good are humans are at it.
Without a clear understanding of how well users can prompt LLMs through iterations, it is hard to determine how much support users need---and when they need it---to improve their prompt engineering efforts effectively.


because 


---yet unexplored---


In such cases, there are no quick or reliable ways for users or computers to assess progress toward their goals, such as when using LLMs to generate long-form stories or code data for qualitative research.
Without a clear understanding of how well users can prompt LLMs through iterations, it is hard to determine how much support users need---and when they need it---to improve their prompt engineering efforts effectively.

\kenneth{-------------------KENNETH IS WORKING HERE--------------------------}

This paper investigates a particularly challenging yet surprisingly common scenario, which we refer to as ``\textbf{prompting in the dark}.'' 
In these scenarios, users create prompts to instruct LLMs to label unlabelled text data, observe the results, and refine the prompt. 
This iterative process continues until the user is satisfied with the outcome.

\system

\end{comment}