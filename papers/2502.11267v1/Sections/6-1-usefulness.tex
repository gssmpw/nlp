\subsubsection{Participants considered \system helpful and efficient.}
In the post-study survey, we asked participants to rate the (Q5) performance satisfaction, (Q6) helpfulness of the tool, and (Q7) its efficiency on a seven-point Likert scale.
% The detailed questions asked are shown in Appendex~\ref{sec:post-question-survey}.
%\kenneth{TODO Steven: Update refernece}
Participants expressed high satisfaction, with an average rating of 6.350 (SD=0.745), and found the system helpful for improving prompts (6.400, SD=0.883) and making prompt engineering more efficient (6.600, SD=0.598).
%\kenneth{TODO Steven: Add numbers---- Are there really high???}
P4 noted, ``\textit{I really like this tool instead of traditional prompt engineering on ChatGPT and Copilot.}''
%\kenneth{TODO Steven: Add a few more examples for other participants.}
P15 mentioned, ``\textit{I would be interested in using this annotation system in my regular work or study, because I really like the idea [of] improving annotation performance by considering iteration annotation process between human and the GPT.}''
% A participant (P18) wrote in the questionnaire, ``I like how the system's UI has been designed and programmed.''



%\kenneth{Not sure how to phrase this...}
\subsubsection{\system is easy to use but less intuitive and with a steep learning curve.}
We asked participants to rate whether ``\textit{(Q1) The annotation task was easy to understand},'' ``\textit{(Q2) The annotation tool is easy to use},'' and ``\textit{(Q4) The interface of the annotation system is intuitive}'' on a seven-point Likert scale from ``Strongly Disagree'' (1) to ``Strongly Agree''(7). 
The average score for the ``easy to understand annotation task'' was 5.812, for the ``easy to use annotation tool'' was 5.375, and for the ``system is intuitive'' was 5.250. Suggesting that while the task and tool itself are not hard to understand and use, learning to properly use the tool can be harder for participants and required some learning.
For example, it was noted that the need to switch between tabs during the task can cause confusion.
% One participant mentioned that the system workflow was unclear, as he had to switch between different tabs, causing confusion and disruption.



\subsubsection{Participants found the Shots and Rule Book useful.}
\input{Sections/6-2-what-users-liked}



\subsubsection{Dilemma of showing LLM explanations.}
%\paragraph{Participants' desire to include LLM explanations.} 
Although our study found that providing LLM explanations can sometimes lead participants to generate labels more aligned with those produced by the LLM, participants still expressed a strong desire to have them included. P8 explicitly recommended incorporating LLM explanations, noting that participants were interested in understanding the reasoning behind potential discrepancies between their own labels and those generated by the LLM. P15 also emphasized the value of these explanations, stating, ``\textit{The explanation from GPT gave me some insights to modify my rules,''} and, ``\textit{I think GPT's explanation of the tweets is very helpful and it may help me to improve the accuracy of human annotation.''}

\begin{comment}
 

\subsubsection{What can be improved in \system?}
%\paragraph{Workflow can be confusing and interface not user friendly}
Some participants faced challenges when learning the system, and it can take them a long time to get comfortable with it. P1 said, ``\textit{The system is not logically clear for me because the system needs to jump in between different tabs, which is different than a normal workflow.}'' 
Some participants became confused with the system, even after several iterations. For instance, a few participants forgot that to proceed to the next iteration, they needed to sample the data and click ‘Start Annotation.’ Research team members need to remind participants of the workflow continuously. Additionally, the nature of the task requires moving from one tab to another, which adds difficulty for participants to navigate the interface and causes confusion.

   
\end{comment}

\subsection{About ``Prompting in the Dark''}

%\kenneth{Not sure about this too}
\subsubsection{Prompting in the dark without any tool is common.}
%\alan{Iterative, trial and error prompting is common without the help of tools.}}
We also asked participants, ``\textit{Without this tool, how would you typically approach prompt engineering?}''
We found that many participants commonly rely on iterative, trial and error strategies. Specifically, they start with prompts from scratch, test them on data points, adjust based on incorrect labels, and re-test until they are satisfied with the results.
% Responses varied: P0 emphasized giving LLMs as much context as possible\steven{this is P0, which is not included in our actual user study}, while others (P1, P3, P4) described a general refining process---starting prompts from scratch, testing on data points, adjusting based on incorrect labels, and re-testing until satisfied.
%\kenneth{TODO Steven: Add a few more examples for other participants.}
%\alan{trial and error, iterative process, personify}

P1 said, `\textit{I need to start with a prompt from scratch; then I will test it on real data points; I will observe those wrongly labeled data points and adjust my prompt accordingly. After the adjustment, I will rerun the testing on the real data points. The whole process is trail-and-error, which is really time-consuming and labor-consuming.}''
P3 stated, ``\textit{Give an initial prompt, if the answer is not meeting expectation, then change the prompt.}''
% P5[Keep trying different prompts to see if the responses I get satisfies my expectations.]
P6 reflected, ``\textit{Normally, if I do not get the desired output from the LLM, I will try to give more  specific instruction maybe some examples.}''
% P14[Try prompting with ChatGPT, if ChatGPT cannot provide a good answer, just rephrase the prompt and ask the question again.]
P17 said, ``\textit{I re-write my prompts several times (3-5 times) until I got an output that I like.}''

\begin{comment}
 

We also want to emphasize that, although not common, some participants employ a personification method by asking LLMs to take on a specific role or personality and make decisions based on that role. For example, P12 stated, ``\textit{I will first assign a role to GPT like ``Supposing you are an expert in coding, ...''. Then I will ask it to follow my instructions.}''
% Personify, P103 [Make llm assume that it is not an AI and act as a specific person who is involved in that specific activity. By Giving as much context as possible.], P12 [I will first assign a role to GPT like ``Supposing you are an expert in coding, ...''. Then I will ask it to follow my instructions.]
P13 said, ``\textit{I would narrate the incident or situation environment and then give prompt asking specific and questions clearly.}''

   
\end{comment}

% \paragraph{\system gave participants new insights into prompt engineering.}
% Interestingly,
% our survey also showed that using \system gave participants new insights into prompt engineering. 
% For example, P1 observed that ``adding or deleting a few words can change the overall output,'' and P2 noted, ``I can compare how many labels are correctly labeled before and after modifying prompts.''
%\kenneth{TODO Steven: Add a few more examples for other participants.--- This one is interesting. Say a few more if possible?}

