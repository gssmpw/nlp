Our goal is to investigate how effective people are at prompt engineering when gold labels are absent, namely, ``prompting in the dark''. 
To study this, we conducted an in-lab user study in which participants used \system to perform a 5-point sentiment labeling task on a tweet dataset. 
This section overviews the details of this study.
This study has been approved by the IRB office of the authors' institute. 


\subsection{Study Procedure}


%\subsection{In-lab Study}
\input{Sections/4-1-in-lab-study}

%\subsection{DSPy Fine-tuning}



%--------------- dead kitten ----------
\begin{comment}


\kenneth{--------------------------KENNETH IS WORKING HERE-----------------------------------}


We aimed to track accuracy trends in data annotation tasks throughout human prompt refinement processes. 
This section detailed the procedure. 
Previous study indicates that LLM explanations enhance understanding of the context~\cite{ma2023insightpilot,singh2024rethinking}. 
To explore this, we implemented two distinct conditions in our study: one group was given access to \textbf{LLM Explanation}, while the other group was not.
Due to time constraints, participants were only able to work with a limited number of data instances per iteration. We established two additional conditions: in one, participants worked on approximately 10 instances per iteration, allowing them to review each instance and its labels in detail. In the other, participants were presented with \textbf{50 instances} and instructed to quickly skim through them to gather insights.\steven{TODO: find papers. } \steven{two variables: LLM Explanation and 50 instances}

% and as individuals explore more data instances, they are able to uncover deeper insights within the data\steven{find papers}; 
% Thus, 

    
\end{comment}