\subsection{Design Suggestions}

Our study shows that ``prompting in the dark''---where users gradually refine their expectations and understanding of data characteristics while iteratively prompting LLMs---is indeed a challenging task. 
Using \system, we quantitatively assessed the difficulty of this process and the unreliability of the progress. 
Many strategies we tested proved ineffective, and the few that showed promise offered only weak signals. 
So, what should be designed in response? 
From the study, we derived insights that we believe are valuable for shaping future systems intended to support iterative LLM-powered labeling, which we summarize into three design suggestions.



\begin{itemize}

\item 
\textbf{Gold Labels as a Necessary Evil.}
%-Need some good gold labels even the users change their mind, it's still very very useful to give guide
The key premise of ``prompting in the dark'' is to avoid assuming a stabilized gold standard from the outset.
This practice embraces the potential for the so-called ``gold standard'' in data labeling to evolve and remain dynamic, realizing this through the interactive nature of LLMs. 
Our system, \system, embodies this practice, using a spreadsheet format.
That being said, throughout our user studies, many participants faced struggles, frustrations, and errors due to the lack of direction or guidance during the exploratory labeling process. 
%The elephant in the room is that much of this lack of guidance was caused by the absence of sufficient gold labels.
The elephant in the room---as highlighted by decades of research on the importance of gold labels in crowdsourced data annotation pipelines (Section~\ref{sec:related-work-gold-label})---is that much of this lack of guidance stemmed from the absence of sufficient gold labels. %\kenneth{TODO: Refer to Related Work}\steven{done}
To clarify, we mean ``sufficient'' as in 100 to 200 labeled text items before starting to use the system, not just the 10 examples users provided via the ``add gold shot'' function.
%we could not use them to calculate numbers like similarity, accuracy, or error rates to provide users with meaningful signals. 
%We also could not use gold labels to track the direction of LLMs' behavioral shifts. 
With no gold labels or fewer than 10, we could not calculate metrics like similarity, accuracy, or error rates to give users meaningful feedback, nor could we track LLMs' behavioral shifts.
These challenges arose as part of our effort to ensure that no substantial labeled items would anchor users' exploration, allowing them to freely evolve their goals.
Our first design suggestion is that this cost might be \textit{too high}.
Based on our observations, having some form of guidance in the ``prompting in the dark'' scenario is likely worth it. 
We propose that users manually label at least a small set of data (\eg, 50-100 items), and that the system be intentionally designed to minimize the impact of these labels on constraining exploration. 
For example, the system could hide the labeled items from the user (even the ones they just labeled), show only signals such as accuracy, or periodically invite users to relabel data.
One example worth highlighting is PromptAid~\cite{mishra2023promptaid}, which dedicates most of its interface to higher-level signals, abstracting away from low-level data details.
Manually labeling some ``gold'' data to start with, when carefully managed, should be considered a reasonable trade-off to balance reliability in the process and the freedom to explore.

\item
\textbf{Automated Support to Reduce Distractions.}
%-Need some support----typo check, rule induction, prompt optimization, and maybe some color/viz aid
Our second design suggestion is to incorporate automation to help reduce users' distractions during the ``prompting in the dark'' process. 
Even with some gold labels---as we suggested---iterating on prompts with an LLM and reviewing tens or hundreds of text items and labels remains exhausting. 
In our study, only a few participants were able to steadily improve labeling accuracy over time.
We believe the root of this mental strain stems from at least two causes.
First, the behavior of LLMs is a mystery. 
Even with carefully crafted prompts, users are still navigating an intangible and unpredictable fog, trying to guess how the LLM will respond.
Second, real-world data is inherently noisy. 
Even when using a strong classifier, users must still contend with numerous edge cases and ambiguous linguistic nuances. 
These challenges are inherently difficult and clash with established human-AI design guidelines~\cite{amershi2019guidelines}. 
%For instance, 
In the ``prompting in the dark'' scenario, there is no efficient way to recover from incorrect labels (Guideline G9 in~\cite{amershi2019guidelines}), LLMs can not reliably explain their labeling decisions (Guideline G11), users lack mechanisms to provide granular feedback to LLMs (Guideline G15), and the consequences of each revision are unclear (Guideline G16).
Given the complexity of these challenges, systems should aim to adhere to all other feasible design guidelines to mitigate user strain.
%These challenges are difficult by nature.
Throughout our study, %while many of the struggles were due to the inherent complexity of LLMs and data, 
we observed that some confusion arose from the workflow or tools. 
Issues such as how to sample a subset from the full dataset, or how to add gold short from validated data, added unnecessary complexity.
Our recommendation is to design systems that reduce these unnecessary distractions. 
For example, the system could default to randomly sampling a subset for the user, or automatically add validated gold labels into the prompt and manage the number of shots in the background. 


\item
\textbf{Design to Prevent Overreliance on LLMs.}
Our final and most critical design suggestion is to create systems that reduce the human tendency to overly trust and accept outputs from AI, particularly LLMs. 
Overreliance on AI is a well-documented phenomenon where people are influenced by AI decisions and often accept them without verification, even when they are incorrect~\cite{vasconcelos2023explanations}.
Overreliance is particularly problematic in exploratory labeling because the core idea---central to ``prompting in the dark''---is that users should own the exploration process and have strong control over which direction to pursue next. 
Users choose to prompt in the dark because they do not want to be guided by someone else's light.
However, introducing AI, such as LLMs, can lead users to fixate on or anchor to the LLMs' suggestions.
If this results in everyone's labels becoming more similar, it undermines the very spirit of exploratory labeling.
In our study, participants who saw LLMs' explanations tended to agree more with the LLMs' labels, leading to higher inner-annotator agreement among those exposed to LLM explanations compared to participants who were not (Section~\ref{sec:llm-explanation-result}). 
%This is yet another instance of automation bias, which is particularly harmful because it goes against the principles of exploratory labeling.
To address this, we advocate designing systems that actively mitigate overreliance on AI.
While explainable AI has been proposed as a solution to overreliance by helping users better judge when to trust AI predictions, empirical evidence shows mixed results. 
%As noted in Related Work , 
Explanations do not always reduce cognitive effort for verifying AI predictions, leading users to ignore them (Section~\ref{sec:rq}). %\kenneth{UPDATE Section REF}
A cost-benefit framework interprets that explanations are less effective when the cognitive cost of verification outweighs the perceived benefits~\cite{vasconcelos2023explanations}.
Text data labeling often falls into this category, as reading and verifying text-based explanations is cognitively expensive. 
This leaves scaffolds, such as cognitive forcing functions, as a viable solution. 
These functions require users to engage more thoughtfully with AI explanations and have been shown to effectively reduce overreliance~\cite{buccinca2021trust}.
%\kenneth{TODO: Cite ``To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making''}







\end{itemize}

%\kenneth{Interact with literature in XAI:}

%\input{Sections/2-6-explaination-data-annotation}


% 2- how sensitive was the performance to rule, example changes - R2 (NOT DONE)
% There are related literature on sensitivity on prompting. 1. Few-shot is important (NLP). 2. DSPy study, we know prompt engineering in some cases can improve performance. 3. More recent study, LLM is not sensitive. Prompt is the only way to manage the LLM. We cannot measure it on this scale. 


\subsection{Limitations}
%-We only try annotation task and one annotation task
%-We only try DSPy
%-Admittedly, the system's workflow is a bit complicated and some reliability issues might come from that--- BUT really there's no other way to make it more doable I guess. 
%-Users only try 1-2 hours
%-Some limitation or design constraints come from spreadsheet, e.g., resample data is hard, can not label image, etc
We acknowledge several limitations in our work. 
First, we focused solely on the ``prompting in the dark'' practice for data labeling tasks, while many other scenarios, such as text generation or conversational agents, are also widely used when working with LLMs.
Second, for RQ2, we only studied one automatic prompt optimization technique, DSPy. 
%While DSPy is one of the most effective and widely used approaches, 
It is possible that other methods might achieve better performance. 
Third, we recognize that, aside from the inherent challenges of the open-ended and exploratory nature of ``prompting in the dark,'' the workflow of our system is somewhat complex. 
This complexity may have contributed to some of the reliability issues observed in our study, though we believe similar challenges would apply to any system supporting newly emerged user practices. 
Fourth, since \system is a Google Sheets Add-on, it inherits limitations and design constraints typical of spreadsheets, 
%such as the difficulty of frequently moving rows between sheets and 
such as limited support for images or videos.
Fifth, it was difficult to track how thoroughly participants reviewed samples. 
Some may have directly modified rules or added gold shots without careful consideration.
%\steven{Fifth, tracking the number of samples reviewed by participants is challenging. It is possible that some participants did not thoroughly review the samples and instead directly modified rules and added gold shots without careful consideration. }
%\steven{
Sixth, we lack a clear understanding %or benchmark 
for how sensitive LLMs are to prompt revisions. 
Tools like DSPy~\cite{khattab2023dspy} and related experiments show that prompt revisions can systematically influence LLM prediction performance, but the degree of sensitivity remains unclear.
%, with recent evidence suggesting that larger models may be less affected by prompt variations~\cite{zhuo2024prosa}. 
This limited understanding of LLM behavior impacts the generalizability of our findings.
%even though DSPy has demonstrated that optimizing prompts can enhance performance in certain cases, it remains unclear which specific modifications or aspects of these optimizations are most sensitive to performance improvements.
%This ambiguity stems from inherent limitation in the underlying LLMs. A recent study found that larger models are less sensitive to prompt variation~\cite{zhuo2024prosa}. Additionally, the limited number of participants in our study makes it more challenging to accurately access sensitivity in our experimental settings. 
%}
Finally, our user study, though already 1-2 hours long, only captures a single session's worth of behavior. 
A longitudinal study that observes how users interact with \system over months or even years might offer different insights.

% \steven{We plan to implement algorithms for random sampling from a uniform data distribution~\cite{le2010ensuring}.[this paper mention uniform distribution of labels in training could help workers peroform better]}

% \steven{Further refining the task instruction: In the future, we can adopt an approach inspired by \citet{manam2018wingit}, leveraging the collaborative capabilities of LLMs, either with users or among multi-agent systems, to iteratively refine task instructions and achieve near-perfect clarity.}

%-------------- dead kitten -------------

\begin{comment}




\kenneth{-----------------KENNETH IS WORKING HERE----------------------}

This issue is especially problematic in exploratory labeling, where the core idea
of ``prompting in the dark'' is for users to own the exploration process and maintain strong control over their direction. Users engage in prompting in the dark to avoid being guided by someone else’s perspective. However, the introduction of AI, such as LLMs, risks causing users to anchor to the AI's suggestions. If this leads to convergence in labeling decisions, it undermines the essence of exploratory labeling.




%-LLM explanation----> Design against automation bias
Our final, and perhaps most important, design suggestion is to reduce the influence of LLMs on users. 
Specifically, we advocate for designing systems that mitigate the human tendency to agree with or accept outputs from automated systems, a phenomenon known as ``automation bias~\cite{SKITKA1999991,goddard2012automation}.''
Automation bias is particularly problematic in exploratory labeling because the core idea---central to ``prompting in the dark''---is that users should own the exploration process and have strong control over which direction to pursue next. 
Users choose to prompt in the dark because they do not want to be guided by someone else's light.
However, introducing automation, such as LLMs, can lead users to fixate on or anchor to the LLMs' suggestions. 
If this results in everyone's labels becoming more similar, it undermines the very spirit of exploratory labeling.
Automation bias is a well-documented problem in other HCI domains, such as with writing assistants, where studies like ``Predictive text encourages predictable writing~\cite{arnold2020predictive}'' have shown similar effects. 

In our study, participants who saw LLMs' explanations tended to agree more with the LLMs' labels, leading to higher inner-annotator agreement among those exposed to LLM explanations compared to participants who were not (Section~\ref{sec:llm-explanation-result}). 
This is yet another instance of automation bias, which is particularly harmful because it goes against the principles of exploratory labeling.
Therefore, we advocate for designing systems that counter or at least mitigate automation bias. For instance, hiding LLMs' explanations by default or ``normalizing'' text styles so that LLM-generated and human-generated texts look indistinguishable~\cite{si2024can}.
This would help preserve the user's autonomy in the exploratory process.
\steven{\citet{dow2012shepherding} found that external expert feedback not only led to significantly greater changes in participants’ work but also increased their output.
This aligns with our proposition that integrating mechanisms to balance LLM explanations -- imagined as expert feedback -- could help users retain autonomy in decision-making while enhancing productivity.}
    
\end{comment}