Prompts are the primary means by which users interact with, utilize, and instruct LLMs. 
Since the emergence of these models, researchers and developers have invested significant effort into understanding how to craft better prompts for more effective use. 

\paragraph{Automatic Prompt Optimization.}
Much of the prior work has focused on automatically optimizing prompts. 
A common theme across these studies is the use of gold-standard labels to guide the optimization process.
For example, \citet{pryzant2023automatic} introduced an automatic prompt optimization method inspired by gradient descent; 
\citet{manas2024improving} presented an approach that begins with a user prompt and iteratively generates revised prompts to maximize consistency between the generated image and prompt, without human intervention; 
\citet{wan2024teach} explored two types of prompt optimization, instruction and exemplar, and suggested that combining both can yield optimal results; 
\citet{sun2023autohint} combined zero-shot and few-shot learning to optimize prompts automatically; %eliminating the need for manual prompt engineering; 
and \citet{levi2024intent} improved prompt optimization through synthetic data generation and iterative refinement, focusing on aligning prompts with user intent by creating challenging boundary cases for iterative prompt refinement.
While these studies were interesting and relevant, they generally assumed the availability of gold-standard labels and did not address situations where labels are absent or where standards are constantly evolving.

\paragraph{User-Driven Prompt Optimization.}
In addition to automatic prompt optimization, some research has focused on human capabilities in optimizing prompts. 
\citet{zhou2023revisiting} found that manual prompting often outperforms automated methods in various scenarios; 
\citet{10.1145/3544548.3581388} discovered that people tend to design prompts opportunistically rather than systematically, which often leads to lower success rates. 
To the best of our knowledge, the most relevant prior work is by \citet{wang2024end}, who developed an iterative refinement system that enables users to prompt LLMs to build a personalized classifier for social media content. 
Their study explored three user strategies for improving prompts and measured their effectiveness. 
While conceptually related to our work, their focus was not on how users evolve their understanding and expectations when interacting with LLMs. 
Instead, participants labeled ground truth at the beginning of the study, prior to using the system.



%--------------------- dead kitten --------------
\begin{comment}
 





The most relevant prior work is by \citet{wang2024end}, who developed an iterative refinement system allowing users to prompt LLMs to build a personalized classifier for social media content.
While their work is closely related to ours in concept, their study did not focus on how users evolve their understanding and expectations while working with LLMs. 
Instead, participants labeled ground truth at the outset before using the system.


\kenneth{The key question for our paper is this: Did prior work try to measure users' prompt engineering performance *over multiple iterations*? (What do we know about human performance in prompt engineering?) I think you can maybe find some papers, especially papers for automatic prompt optimization like DSPy, measuring users' individual prompt's output accuracy (or MSE) or performance (e.g., BLEU in generation task), but it might be hard to find papers capture and measure *multiple iterations* from the same user for the same prompt.--This is the main argument for our paper: we did something that was hard and thus has not been done.}

\kenneth{Take a look at this survey paper:~\cite{chen2023unleashing}}



\steven{iterative tool involve human}
PromptIDE is an interactive tool that helps the experts to iteratively refine tools by providing various prompts, visualizing their performance on small validation datasets, and iterative optimizing them based on quantitative feedback~\cite{strobelt2022interactive}. \steven{gold label exists}

PromptAID is a visual analytics system that helps non-experts iteratively improve prompts through exploration, perturbation, testing, and refinement. It supports prompts through keyword adjustment, paraphrasing, and adding few-shot examples. \steven{has test dataset, it is a complex system}

\steven{automate prompting}
\citet{pryzant2023automatic} introduces an automatic prompt optimization prompt inspired by gradient descent. \steven{this fell into software designing, involve gold labels}

The study starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score between the generated image and prompt without a human in the loop\cite{manas2024improving}\steven{without human involvement in the loop, gold labels}

\citet{zhou2023revisiting} found that manual prompting often performed better than automated methods in various steps. 

\cite{wan2024teach} explores the distinction between two types of prompt optimization: instruction optimizer and exemplar. This study suggested combining both approaches could lead to optimal results.

\cite{sun2023autohint} combines zero-shot and few-shot learning to optimize prompts automatically, without manual efforts in prompt engineering.

\cite{levi2024intent} improve prompt engineering optimization by synthetic data generation and iterative refinement, focusing on aligning prompts with user intent by generating challenging boundary cases and using these to refine the prompt iteratively.





\paragraph{Prompt Engineering Tools.}
\kenneth{After making the first point, we can have a follow-up paragraph to say that many tools were created to help people do prompt engineering (list a few and name their focuses), but again, they did not focus on measuring how good humans are in prompt engineering--- Of course, there could be an argument that suggests: no matter how good you are, you will always need some tool. It is true---for example, ChainForge basically create a easy-to-use UI that make things easier, which is not really about accuracy---But for annotation tasks, performance is still critical and it is always good to know how well human did, almost like many AI leaderboard has various "human" performance for comparison.}
PromptMaker, a platform for rapidly prototyping new ML models using prompt-based programming, was difficult to evaluate their prompts systematically~\cite{10.1145/3491101.3503564}.

\cite{arawjo2024chainforge}  is an Open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text-generation LLMs.

 promptfoo is test-driven LLM development, not trial-and-error, producing matrix views that let you quickly evaluate outputs across many prompts~\cite{webster2023promptfoo}.

\cite{madaan2024self} introduces a method that LLM iterative improve their output by using their own feedback, without external supervision. 

\saniya{austin etal points:
1. used only COPRO, evaluation criteria utilized a custom LLM-as-a-judge metric. The paper showed that their automated prompt optimizer worked better tha DSPy }
   
\end{comment}
