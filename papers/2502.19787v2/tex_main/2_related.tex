\section{Related Works}
\label{sec:RelatedWork}
% We classify synthetic data models used in ICL research into two broad categories. In the first category, called the \emph{Interleaved Input-Output Format} in this paper, the pretraining data is an interleaved sequence:
% \[
%     \bigl(\vx^{(1)}, \vy^{(1)}, \vx^{(2)}, \vy^{(2)}, \ldots \bigr),
% \]
% where each label \(\vy^{(i)}\) is generated by applying a function \(f\) to the corresponding input \(\vx^{(i)}\), i.e., \(\vy^{(i)} = f(\vx^{(i)})\).

% In the second category, called the \emph{Recurrent Input-Output Format} in this paper, the pretraining sequence follows:
% \[
%     \bigl(\vx^{(1)}, \vx^{(2)}, \vx^{(3)}, \ldots\bigr),
% \]
% and each input \(\vx^{(i+1)}\) is generated from the previous one following the same function
% \(
%     \vx^{(i+1)} = f\bigl(\vx^{(i)}\bigr).
% \)

%\paragraph{Interleaved Input-Output Format}
\citet{garg2022can} first construct synthetic data with pretraining and testing sequences generated from noiseless linear regression tasks.
Specifically, \citet{garg2022can} sample all input vectors \(\vx\) from an isotropic Gaussian distribution \(\mathcal{N}(\vzero, \mathbf{I})\). Within each sequence, the outputs are given by
\(
y^{(i)} = \langle \vw, \vx^{(i)} \rangle,
\)
where \(\vw\) is drawn from \(\mathcal{N}(\vzero, \mathbf{I}_d)\). 
For ICL inference, the prompt takes the form
\(
(\vx^{(1)}, y^{(1)}, \vx^{(2)}, y^{(2)}, \ldots, \vx^{(k-1)}, y^{(k-1)}, \vx_{\text{query}})
\)
where the \(k-1\) pairs \(\{(\vx^{(i)}, y^{(i)})\}_{i=1}^{k-1}\) serve as demonstrations illustrating the linear relationship governed by \(\vw\).
The model then predicts the output for \(\vx_{\text{query}}\).
%We refer to this sequence format as the "Interleaved Input-Output Format."
%\citet{garg2022can} also extends such an experiment to non-linear cases including decision trees and two-layer neural networks.

\paragraph{Noiseless Linear Regression} Based on the well-defined problem setup by~\citet{garg2022can} using noiseless linear regression, researchers systematically study the mechanisms of ICL and properties of Transformer.
For instance, there is a particular interesting line of research on connecting ICL to gradient descent, firstly hinted by~\citet{garg2022can}.
\citet{akyurek2022learning} and \citet{von2022transformers} then show that one attention layer can be exactly constructed to perform gradient descent, and empirically find similarities between ICL and gradient descent. 
Further, \citet{ahn2023transformers} theoretically show that under certain conditions, Transformers trained on noiseless linear regression tasks minimizing the pretraining loss will implement gradient descent algorithm.
Nevertheless, \citet{fu2024transformers} show that Transformers learn to approximate second-order optimization methods for ICL, sharing a similar convergence rate as Iterative Newtonâ€™s Method.
Besides gradient descent, there are lots of other interesting topics on ICL and Transformers based on this linear regression setting, such as looped Transformer~\citep{Yang0NP24,GatmirySRJK24}, training dynamic~\citep{zhang2024trained, HuangCL24, kim2024transformers}, generalization~\citep{panwar2024incontext}, etc.

\paragraph{Noisy Linear Regression} Such a simple noiseless linear regression task is further extended to variants.
By extending the linear regression to noisy linear regression---$y=\langle \vx, \vw \rangle + \epsilon$,
\citet{li2023transformers} analyze the generalization and stability of ICL. 
\citet{WuZCBGB24} and \citet{raventos2024pretraining} analyze the effect of task diversity on the attention model's ICL risk.
Via extending the regression tasks sampling from Gaussian to Gaussian mixture, \citet{lin2024dual} show ICL exhibits two different modes including task retrieval and learning.
With the tasks of $\vy=\mW\vx+\vepsilon$ where $\mW$ is a matrix rather than a vector, \citet{ChenSWY24} examine the training dynamic of multi-head attention for ICL.

\paragraph{More than Linear Regression} Beyond linear regression, researchers are also interested in non-linear regression and classification.
The research directions are scattered, and we list them as follows.
\citet{BaiCWXM23} show that Transformers can perform in-context algorithm selection, \ie, adaptively selecting different ICL algorithms such as gradient descent, least square, or ridge regression.
\citet{BhattamishraPBK24} show Transformer can learn a variety of Boolean function classes.
\citet{cheng2024transformers} provide evidence that Transformers can learn to implement gradient descent to enable them to learn non-linear functions.
\citet{0004HMWXS024} show that trained Transformer achieves near-optimal ICL performance under $y=\langle \vw, f(\vx)\rangle$, where $f$ is a shallow neural network (MLP).
Examining linear and non-linear regression tasks, \citet{fan2024transformers} and~\citet{tripuraneni2024can} show Transformer can perform ICL on composited or mixed tasks of pretrained linear or non-linear tasks, and \citet{yadlowsky2024can} examine whether trained Transformers can generalize to new tasks beyond pretraining.
\citet{park2024can} examine whether Mamba can in-context learn a variety of synthetic tasks.
Via examining regression and classification tasks, \citet{kim2024task} show task diversity helps shorten the ICL plateau pretraining.
\citet{rameshcompositional} assume there are multiple functions composited to connect $\vx$ and $\vy$ pair, \eg, $\vy=f_1\circ f_2\circ f_3(\vx)$ to study the compositional capabilities of Trasnformer.
\citet{li2024nonlinear} study how non-linear Transformer learns binary classification.


% \paragraph{Recurrent Input-Output Format}
% % This type of pretraining sequence has the format $(\vx^{(1)},\vx^{(2)},\vx^{(3)},\ldots)$, a sequence of tokens generated from a next-token generative model, such as the Hidden Markov Model (HMM)~\citep{xie2021explanation}, Probabilistic Finite Automata (PFA)~\citep{akyurek2024incontext}, or simply a function $\vx_{i+1} = f(\vx^{(i)})+\epsilon$~\citep{li2023transformers,sander2024how}.
% %with noise or $\vx_{i+1} = f(\vx^{(i)})$~\citep{sander2024how} without noise.
% \citet{xie2021explanation} first proposes to use multiple Hidden Markov Models (HMMs) to represent latent concepts in real-world language and generate pretraining sequences, then explain the ICL phenomenon via a Bayesian perspective.
% %The training sequences are then generated from those HMMs for pretraining, while testing sequences are constructed by concatenating multiple sequences of the same length.
% %Such misalignment between the training and testing dataset mimics the misalignment under real-world LLMs' training and ICL testing scenarios, which is the key feature of the first category.
% \citet{han2023explaining} leverages the same setting and explains ICL via kernel regression.
% Sequences under this format are also generated by \citet{akyurek2024incontext} using Probabilistic Finite Automata (PFAs), \citet{edelman2024evolution} using Markov Chain, and \citet{nichani2024transformers} using Markov Chain with causal structure, to study the induction head~\cite{olsson2022context} of LLMs.
% While the abovementioned datasets only consider single-step dependence between tokens, i.e., the next token only depends on one of the previous tokens, \citet{ashokMCICLR} further explore
% %study how Transformer learns both 1-order and 
% higher-order Markov chains.


% Beyond the setting of $(\vx^{(1)}, \vy^{(1)}, \vx^{(2)}, \vy^{(2)}, \ldots)$, multiple steps are further introduced by~\citet{li2024how} to the mapping from $\vx$ to $\vy$, \ie, the training sequence becomes $\vx^{(1)}, \vy^{(1)}^{'}, \vy^{(1)}^{''}, \vx^{(2)}, \vy^{(2)}^{'}, \vy^{(2)}^{''}, \ldots$ to mimic the multiple steps in Chain-of-Thought (CoT)~\citep{wei2022chain}.


% \paragraph{Synthetic Dataset with Image.}
% Beyond simply vector serving as $\vx$, researchers also leverages image as those $\vx$, \ie, the training sequence $(\vx^{(1)},\vy^{(1)},\vx^{(2)},\vy^{(2)},\ldots)$ has image serves as in-context samples.
% \citet{chan2022data} first examine how the pretraining data properly affects the ICL phenomenon.
% \citet{singh2024transient, reddy2023mechanistic} further examine the effect of data on the dynamics of ICL and in-weight learning (IWL).
% \citet{fubreaking} study the learning plateaus of in-context learning with similar image label paired pretraining sequences.

\paragraph{Synthetic Dataset with Instruction}
To the best of our knowledge, there are two articles on synthetic datasets with instructions.
\citet{huang2024task} append an additional vector $\vmu$ to the sequences with interleaved input-output format, which leads to the sequence $(\vmu, \vx^{(1)}, \vw^\top\vx^{(1)}, \vx^{(2)}, \vw^\top\vx^{(2)}, \ldots)$ in which $\vx^{(i)}\sim\mathcal{N}(\vmu,\mI)$, and show that the trained Transformer can achieve significantly lower loss on ICL when the task descriptor $\vmu$ is provided.
The work of \citet{xuanyuan2024on} is most closely related to us.
It develops a new synthetic dataset based on task $((a\cdot x)\circ(b\cdot y))\mod p=r$, where $(x,y)$ is the input, $r$ is the output, $\circ$ is an operation ($+,-,/$), and each task is defined by the parameters $(a,b,\circ)$ ($p$ is a constant).
The instruction is constructed as $(a_l,a_u,b_l,b_u,\circ)$, where $a_l$ and $a_u$ are the lower and upper bounds of $a$ (similar for $b$), and $\circ$ is the operation.
Therefore, the instruction constrains the possible tasks, \ie, provide information on the underlying true task of in-context samples.
With such a setting, \citet{xuanyuan2024on} study how the information provided by instruction affects the ICL accuracy.
