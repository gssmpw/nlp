% \section{Discussion}
% Based on the ICL-HCG framework, we provide diverse exploration experiments mainly on generalization.
% Though explanation via Bayesian inference give some explanations on ICL, literature has found Transformer can generalize beyond Bayesian inference~\ref{raventos2024pretraining}.
% How such OOD generalization remains unclear and our work provides a framework to explore such OOD generalization beyond bayesian inference.
% (None of the testing sample appears in the training sample due to non of the testing hypothesis class appears in the training hypothesis classes.)
% Moreover, in Sec.~\ref{subsec:4model} we show Transformer and Mamba have different advantage towards length generalization and OOD hypothesis generalization, and in Sec.~\ref{subsec:diversity} we show instruction helps to boost the benefit brought by pretraining hypothesis diversity.
% These two findings point out two important factors affecting OOD generalization: (i) the model architecture and (ii) the data structure.
\section{Discussion}  
Building on the ICL-HCG framework, we conduct diverse experiments focusing on generalization.
While Bayesian inference~\cite{xie2021explanation} offers insights into ICL, prior work~\citep{raventos2024pretraining} has shown that Transformers can generalize beyond Bayesian inference with sufficient pretraining task diversity.
However, the mechanisms underlying such OOD generalization remain unclear. Our work provides a framework for exploring OOD generalization beyond Bayesian inference, where no test samples appear in the training set due to disjoint hypothesis classes.%, \eg, $\{\mathcal{H}^{\text{train}}_i\}_{i=1}^{N^{\text{train}}} \cap \{\mathcal{H}^{\text{test}}_i\}_{i=1}^{N^{\text{test}}} = \emptyset$.

Furthermore, in Sec.~\ref{subsec:4model}, we demonstrate that Transformer and Mamba exhibit distinct strengths: Transformer excels on length generalization, while Mamba performs better on OOD hypothesis generalization.  
In Sec.~\ref{subsec:diversity}, we show that instruction enhances the benefits of pretraining hypothesis diversity.  
These findings highlight two key factors influencing OOD generalization: (i) model architecture and (ii) data structure.
Future work will further explore these phenomena, focusing on understanding the underlying mechanisms of OOD generalization in Transformer and Mamba.


