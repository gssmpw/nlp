\begin{figure*}[th!]
    \centering
    %\vspace{-1.1cm}
    \includegraphics[width = 0.85\textwidth]{fig/pipeline.pdf}
    %\vspace{-0.2cm}
    \caption{\textbf{The generation of training and testing hypothesis classes.} The hypothesis universe is partitioned into two pools: one for generating training and ID testing hypothesis classes, and another for generating OOD testing hypothesis classes.}
    \label{fig:pipeline}
    %\vspace{-0.3cm}
\end{figure*}
\subsection{Setting of Experiments}
\label{subsec:setting}
\paragraph{Hypothesis class generation} Fig.~\ref{fig:pipeline} illustrates the hypothesis class generation process used in this paper.
We partition the hypothesis universe into two pools: one for generating training and ID testing classes, and another for generating OOD testing hypothesis classes.
This ensures that training and ID testing hypothesis classes do not overlap and that OOD hypothesis classes come from an entirely separate set of hypotheses.
Consequently, both ID and OOD hypothesis class generalization can be assessed using the same trained model.
For detailed realizations of setups for four kinds of generalization, see Appendix~\ref{setup:generalization}.

\paragraph{Pretraining} During pretraining, we backpropagate gradients \emph{based on next-token prediction for all tokens}. 
Each training sequence \(s\) consists of a hypothesis prefix, a context query, and a hypothesis index token. 
As illustrated in Fig.~\ref{fig:framework}, we feed the entire sequence \(s\) (excluding the final index token \hz 
into the Transformer.
We then compute cross-entropy loss for each subsequent token (excluding the very first).
Refer to Appendix~\ref{app:expsetup} for training details, including the learning rate schedule, and hyperparameter search. 

\paragraph{Components of pretraining loss} We conducted experiments to determine the optimal components to include in the pretraining loss. Specifically, we evaluated four configurations: applying the loss (i) solely to the final hypothesis index token, (ii) exclusively to the content query, (iii) only to the label $y$ of the content query, and (iv) across all tokens.
We empirically find that incorporating the loss across all tokens in the sequence leads to the best performance.

% \paragraph{Testing.} While we use i.i.d. samples to construct context query in training, we perform testing with two types context query.
% The first type mimics the training, using i.i.d. samples to construct context query, denoted by \textbf{``i.i.d.''}.
% The second type uses optimal teaching samples, denoted by \textbf{``Opt-T''}.
% Our results in the main paper will focus on testing with optimal teaching samples, and put more details to appendix due to page limitation.
