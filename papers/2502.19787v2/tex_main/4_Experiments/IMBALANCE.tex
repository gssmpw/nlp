\subsection{Effect of Imbalanced In-Context Samples}
\label{subsec:numtrain}
This section investigates how an imbalanced sample distribution in the context query affects the training procedure.
Specifically, we consider the following distribution over $\mathcal{X}$:
% $$\text{norm}(
% \underbrace{\frac{1}{\sqrt{\text{D}}},\ldots,\frac{1}{\sqrt{\text{D}}}}_{\lfloor\frac{|\mathcal{X}|}{2}\rfloor\text{ items}}
% ,\underbrace{1}_{(|\mathcal{X}|\mod 2)\text{ items}},
% \underbrace{\sqrt{\text{D}},\ldots,\sqrt{\text{D}}}_{\lfloor\frac{|\mathcal{X}|}{2}\rfloor\text{ items}}
% ),$$
$$
\text{norm}\left(\frac{1}{\sqrt{\text{D}}},\ldots,\frac{1}{\sqrt{\text{D}}},1,\sqrt{\text{D}},\ldots,\sqrt{\text{D}}\right),
$$
where the first half of the terms are $\frac{1}{\sqrt{\text{D}}}$, the middle term (if $|\mathcal{X}|$ is odd) is $1$, the second half consists of $\sqrt{\text{D}}$, and
D\footnote{The notation D is distinguished from token ``\textcolor[RGB]{0,176,80}{D}'' by color and dataset $\mathcal{D}$ by format.} represents the disparity of the distribution over $\mathcal{X}$, \ie, $\text{D}=\frac{\max_{x\in\mathcal{X}} P(x)}{\min_{x\in\mathcal{X}} P(x)}$.
\begin{highlight}
    \paragraph{Finding 4:} 
    \emph{In-context sample imbalance lags the convergence of training.}
\end{highlight}
We analyzed the impact of imbalance on the training process in Fig.~\ref{fig:Imbalance} by varying D values, showing that greater imbalance slows convergence.
On average over four runs, training converges in about 384 epochs for $D=1$ but takes around 700 epochs for D$=4$.
\begin{figure}[h!]
    \centering
    %\vspace{-1.1cm}
    \includegraphics[width = 0.475\textwidth]{new_fig/IMBALANCE/imbalance_for_IO_1x2.pdf}
    %\vspace{-0.2cm}
    \caption{\textbf{The effect of sample imbalance.}
    Sample imbalance leads to lower convergence speed.}
    \label{fig:Imbalance}
    %\vspace{-0.3cm}
\end{figure}