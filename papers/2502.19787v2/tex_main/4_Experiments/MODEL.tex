\subsection{Model Architecture Comparisons}
\label{subsec:4model}
We compare Transformer with other model architectures, including Mamba~\cite{gu2023mamba}, LSTM~\cite{hochreiter1997long}, and GRU~\cite{cho2014learning}. We investigate whether each model can effectively fit the training dataset and, if so, generalize to the four types of unseen hypothesis classes.
\begin{figure}[h!]
\centering
    \subfigure[testing curves of ID hypothesis class generalization.\label{fig:multiple_models_I_1x1}]
    {
        %\includegraphics[width=0.22\textwidth]{fig/MODEL/table_generalization_multiple_curves_test_-optimal_acc_z.pdf}
        \includegraphics[width=0.22\textwidth]{new_fig/MODEL/multiple_models_for_I_1x1.pdf}
    }
    \hspace{0.1em}
    \subfigure[testing curves of OOD hypothesis class generalization.\label{fig:multiple_models_O_1x1}]{
        %\includegraphics[width=0.22\textwidth]{fig/MODEL/hypothesis_generalization_multiple_curves_test_-optimal_acc_z.pdf}
        \includegraphics[width=0.22\textwidth]{new_fig/MODEL/multiple_models_for_O_1x1.pdf}
        }
    \caption{\textbf{Various models on ID and OOD hypothesis class generalizations.}
    % While Transformer and Mamba succeed on ID and OOD hypothesis class generalizations, LSTM and GRU fail to do so.
    % Mamba has slightly better accuracy than Transformer on OOD hypothesis class generalization.
    Transformer and Mamba succeed on both ID and OOD hypothesis class generalization, whereas LSTM and GRU fail.
    Mamba exhibits slightly higher accuracy than Transformer on OOD generalization. Refer to Appendix~\ref{subapp:4model} and Fig.~\ref{fig:multiple_models_IO_2x3} for training curves.}
    \label{fig:multiple_models_IO}
\end{figure}
\begin{highlight}
    \paragraph{Finding 2:} 
    \emph{While both Mamba and Transformer excel on the four generalization tasks, LSTM and GRU fail to handle the ICL-HCG tasks. Mamba outperforms Transformer on OOD hypothesis class generalization, whereas Transformer outperforms Mamba on ID hypothesis class size generalization.}
\end{highlight}
We evaluate ID and OOD hypothesis class generalization across model architectures. 
Within the hyperparameter search space in Appendix~\ref{app:hyperparameters}, Fig.~\ref{fig:multiple_models_IO} shows that Transformer and Mamba both generalize well on ID and OOD hypothesis class generalizations, with higher accuracy on ID hypotheses (1.00 accuracy) than OOD (around 0.8 to 0.9 accuracy).
In contrast, LSTM and GRU fail to fit the task, achieving approximately 0.125 accuracy, equivalent to random guessing over eight hypotheses.
Furthermore, Fig.~\ref{fig:multiple_models_IOS} shows that Mamba outperforms Transformer on OOD hypothesis class size generalization, whereas Transformer excels on ID hypothesis class size generalization, suggesting a potential advantage of Transformer on length generalization, and Mamba on generalization of OOD hypotheses.

\begin{figure}[h!]
\centering
    \subfigure[testing curves of ID hypothesis class size generalization.\label{fig:multiple_models_IS_4x3}]
    {
        %\includegraphics[width=0.475\textwidth]{fig/MODEL/table_length_generalization_multiple_models_1x4_combined.pdf}
        \includegraphics[width=0.5\textwidth]{new_fig/MODEL/multiple_models_for_IS_4x3.pdf}
    }
    \hspace{0.1em}
    \subfigure[testing curves of OOD hypothesis class size generalization.\label{fig:multiple_models_OS_4x3}]{
        %\includegraphics[width=0.475\textwidth]{fig/MODEL/hypothesis_length_generalization_multiple_models_1x4_combined.pdf}
        \includegraphics[width=0.5\textwidth]{new_fig/MODEL/multiple_models_for_OS_4x3.pdf}
        }
    \caption{
    \textbf{Various models on ID and OOD hypothesis class size generalizations.}
    % In both settings, Transformer and Mamba demonstrate strong generalization, whereas LSTM and GRU fail to do so.
    % With hypothesis class size $|mathcal{H}|\in\{7,8,9\}$, Mamba has similar accuracy as Transformer on ID hypothesis class generalization, and better accuracy than Transformer on OOD hypothesis class generalization.
    % However, Transformer has similar or better accuracy than Mamba on ID hypothesis class size generalization, implying Transformer is potentially better than Mambda on length generalization.
    % Refer to Appendix~\ref{subapp:4model}, Fig.~\ref{fig:multiple_models_IOS_9x5} for training accuracy (i.i.d.).
    In both settings, Transformers and Mamba exhibit strong generalization, whereas LSTM and GRU fail to do so. For hypothesis class sizes \(\lvert \mathcal{H} \rvert \in \{7,8,9\}\), Mamba achieves accuracy comparable to Transformer on ID hypothesis class generalization, and surpasses Transformer on OOD hypothesis class generalization.
    However, Transformers show similar or higher accuracy than Mamba on ID hypothesis class size generalization, suggesting a potential advantage in length generalization.
    Refer to Appendix~\ref{subapp:4model}, Fig.~\ref{fig:multiple_models_IOS_9x5} for training accuracy curves.
    }
    \label{fig:multiple_models_IOS}
\end{figure}

% \begin{figure*}[th!]
%     \centering
%     %\vspace{-1.1cm}
%     \includegraphics[width = 0.8\textwidth]{fig/MODEL/table_length_generalization_multiple_models_1x4_combined.pdf}
%     %\vspace{-0.2cm}
%     \caption{\textbf{Varied models on space \& size generalization.}}
%     \label{fig:Space&LengthGeneralization-Models}
%     %\vspace{-0.3cm}
% \end{figure*}
% \begin{figure*}[th!]
%     \centering
%     %\vspace{-1.1cm}
%     \includegraphics[width = 0.8\textwidth]{fig/MODEL/hypothesis_length_generalization_multiple_models_1x4_combined.pdf}
%     %\vspace{-0.2cm}
%     \caption{\textbf{Varied models on hypothesis \& size generalization.}}
%     \label{fig:Hypothesis&LengthGeneralization-Models}
%     %\vspace{-0.3cm}
% \end{figure*}