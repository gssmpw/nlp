\eat{this is not an interpretability paper, the introduction must start with generation}

LLMs have emerged as the de facto natural language interface for interacting with AI systems. In most modern applications, prompt engineering is crucial for steering the output generation process, controlling the nuance and style of text, and tailoring it to suit the audience's needs -- including factors like empathy and impartiality. %and length.



Research on controlled text generation in large language models (LLMs) has been extensively explored for general-purpose applications (see the comprehensive survey by~\citet{ctg-survey}). Unlike standard text generation, controlled text generation~\cite{mireshghallah2022mix} guides LLMs to generate outputs that adhere to specific constraints.


\eat{With the advent of large pre-trained models, in-context learning (ICL)~\cite{brown2020language} has emerged as a significant paradigm for controlled text generation. It enables LLMs to adapt their behavior based on demonstrations or instructions embedded directly within the input prompt. While ICL is valuable for leveraging the inherent capabilities of pre-trained models, it has limitations.} Controlling text generation solely through prompting can be ineffective~\cite{ye2022unreliability}, as prompts often require extensive contextual information to generate relevant responses. When additional instructions for controlling style and nuances are included, these details may be overlooked or lost within the large prompt context~\cite{liu2023lost}. Moreover, open-source and smaller LLMs provide limited control over generated text when relying solely on prompt-based methods~\cite{salinas2024butterfly}.

For instance, a model to understand and execute instructions like "generate the response in [X] style", it needs extensive exposure to such instructions during training to reliably control output style via its internal sentence representation. Models like GPT-4 ~\cite{achiam2023gpt} and Claude 3.5 Sonnet generally follow style instructions effectively, whereas smaller models, such as Phi-3 ~\cite{abdin2024phi} (trained through knowledge distillation from larger models like GPT-4), often struggle to adhere precisely to specified style constraints. 

One approach to mitigating this in smaller language models is to incorporate demonstrations in the prompt, a technique known as in-context learning~\cite{{brown2020language}}. We observe that Phi-3's ability to follow style-specific instructions improves as more demonstration are provided. However, one caveat is that increasing the number of demonstrations leads to significant memory consumption and computational overhead, particularly with the larger context windows needed for complex tasks.

Latent space-based steering methods, often referred to as activation engineering, offer an effective approach for controlled text generation by strategically intervening in the activations of LLMs~\cite{Dathathri_Madotto_Lan_Hung_Frank_Molino_Yosinski_Liu_2019,Liu_Sap_Lu_Swayamdipta_Bhagavatula_Smith_Choi_2021,Khalifa_ElSahar_Dymetman_2020}. These interventions are achieved by applying steering vectors -- directional components in the activation space that correspond to specific semantic or stylistic features -- during inference. The steering vectors shift the model's behavior in predictable ways~\cite{hernandez2023inspecting,sun2024massive}. Unlike in-context learning, activation engineering allows for control over multiple independent attributes (e.g. style, tone, or formality) by disentangling these features in the activation space. However, the effectiveness of this method relies on the interpretability of the activation features being manipulated. Recent advancements suggest that introducing sparsity in activation functions -- via methods such as Sparse Auto-Encoders (SAE)~\cite{Makhzani2013kSparseA} -- can significantly enhance interpretability~\cite{cunningham2023sparse,bricken2023towards}. Increased sparsity improves disentanglement in the latent space, enabling more precise and consistent control over independent attributes during text generation.

\par






In this paper, we propose a novel approach for controlled text generation using sparse feature-based steering during inference. Inspired by prototypical networks~\cite{snell2017prototypical}, originally designed for few-shot learning, we represent each attribute class of interest as a prototype distribution in a latent space defined by SAE and steer the encoded features to increase the likelihood of output for the desired prototype.

\eat{we enable efficient classification and systematic steering of features to achieve desired text attributes. This approach naturally extends to multiple attributes, offering flexible and nuanced control over text generation.}
\begin{figure*}[htbp]
\centering
  \includegraphics[width=1.08\linewidth]{figures/anew1.pdf}
  \caption{Training architecture of our method. We divide the LLM  into Enc-LLM, Upto Layer L till the sparse encoders and Dec-LLM for the rest of the layers including the sparse decoders.
}
  \label{fig:entire}
\end{figure*}

We evaluate our method on synthetically generated dataset, specifically designed to capture varying classes of cognitive style for the given input. Our experiments demonstrate that SAEs effectively capture and distinguish the nuances between these classes, and further investigation reveals their utility in steering generation toward the desired style. Our primary contributions and findings are the following:

\noindent\textbf{Novel Dataset Creation}: We develop a novel dataset by employing proprietary LLMs (Claude) to generate code review problems accompanied by feedback of varying cognitive styles (Bloom's taxonomy)~\cite{uugur2015self}. This dataset facilitates the examination of the steering across nuanced styles of text. 

    %\noindent\textbf{Layer-Specific Feature Differentiation}: We observe that training SAEs on internal activations of LLMs across different layers enables the extraction of distinct textual features, such  style, which become prominent at specific range of network layers.
    
    \noindent \textbf{Enhanced Interpretability via Attention Mechanisms}: Our observation reveals that query representations within attention heads are particularly effective in capturing distinguishing textual features, highlighting the critical role of attention components in processing and interpreting input data.

    \noindent \textbf{Text Generation Steering}: We successfully control the generation of texts to achieve specific styles by applying a gradient-based approach to sparsely encoded features, validating the practical applicability of our prototype-based steering method.

    
    \noindent \textbf{Evaluation}: We conduct extensive evaluations across various layers and dimensions to demonstrate our approach's effectiveness.


















