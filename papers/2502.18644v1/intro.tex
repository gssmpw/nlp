\eat{this is not an interpretability paper, the introduction must start with generation}

LLMs have emerged as the de facto natural language interface for interacting with AI systems. In most modern applications, prompt engineering is crucial for steering the output generation process, controlling the nuance and style of text, and tailoring it to suit the audience's needs -- including factors like empathy, impartiality. %and length.



Research on controlled text generation~\cite{mireshghallah2022mix} in large language models (LLMs) has been extensively explored for general-purpose applications (see the comprehensive survey by~\cite{ctg-survey}). Unlike standard text generation, controlled text generation guides LLMs to generate outputs that adhere to specific constraints.

With the advent of large pre-trained models, in-context learning (ICL)~\cite{brown2020language} has emerged as a significant paradigm for controlled text generation. This method enables LLMs to adapt their behavior based on examples or instructions embedded directly within the input prompt. While ICL is valuable for leveraging the inherent capabilities of pre-trained models, it has limitations. Controlling text generation solely through prompting can be ineffective~\cite{ye2022unreliability}, as prompts often require extensive contextual information to generate relevant responses. When additional instructions for controlling style and nuances are included, these details may be overlooked or lost within the large prompt context~\cite{liu2023lost}. Moreover, open-source and smaller LLMs provide limited control over generated text when relying solely on prompt-based methods~\cite{salinas2024butterfly}.

For instance, a model to understand and execute instructions like "generate the response in [X] style", it needs extensive exposure to such instructions during training to reliably control output style via its internal sentence representation. Models like GPT-4 ~\cite{achiam2023gpt} and Claude 3.5 Sonnet ~\cite{anthropic2023introducing}  generally follow style instructions effectively, whereas smaller models, such as Phi-3 ~\cite{abdin2024phi} (trained through knowledge distillation from larger models like GPT-4~\cite{achiam2023gpt}), often struggle to adhere precisely to specified style constraints. We observe that Phi-3's ability to follow style-specific instructions in an ICL setup improves as more training examples are provided. However, one caveat is that increasing examples leads to significant memory consumption and computational overhead, particularly with the larger context windows needed for complex tasks.

Latent space manipulation based approaches  controls the generated text by adjusting activation states within the model’s hidden layers.












%For instance, For a model to understand and execute instructions like "generate the response in [X] style ", it needs extensive exposure to such instructions during training to reliably control output style via its internal sentence representation. For example, models like GPT-4 ~\cite{achiam2023gpt} and Claude 3.5 Sonnet ~\cite{anthropic2023introducing}  generally follow style instructions effectively, whereas smaller models, such as Phi-3 ~\cite{abdin2024phi} (trained through knowledge distillation from larger models like GPT-4~\cite{achiam2023gpt}), often struggle to adhere precisely to specified style constraints. 



%Early methods include Plug and Play~\cite{dathathri2019plug}, which use external attribute models to guide outputs without retraining, and decoding-time control ~\cite{liu2021dexperts}, which adjust the model's output distribution during generation to enforce constraints.  





Feature-based steering allows for more precise control over generated text through strategic interventions into LLM activations~\cite{Dathathri_Madotto_Lan_Hung_Frank_Molino_Yosinski_Liu_2019,Liu_Sap_Lu_Swayamdipta_Bhagavatula_Smith_Choi_2021,Khalifa_ElSahar_Dymetman_2020}. Unlike prompt-based methods~\cite{wallace2404instruction}, activation steering~\cite{turner2023activation} directly alters hidden state vectors at specific LLM layers. This is achieved by calculating steering vectors – directional components in activation space that correlate with specific semantic or stylistic features – and applying these vectors during inference to shift model behavior predictably~\cite{hernandez2023inspecting,subramani2022extracting,sun2024massive}.
% Some approaches, such as prompting ~\cite{wallace2404instruction}  show promise for controlling text generation. However, they are not reliable due to their sensitivity to variations ~\cite{chatterjee2024posix}. 





%Controlling text generation solely through prompting can be ineffective, partly because prompts often need to contain extensive contextual information to generate a relevant response. When additional instructions for controlling style and nuance are included, these details may be overlooked or lost within the large prompt context~\cite{liu2023lost}. Furthermore, open-source and smaller LLMs offer limited control over generated text when relying on prompt-based methods~\cite{salinas2024butterfly}.












%The study of interpretable features within Large Language Models (LLMs) has gained remarkable momentum~\cite{templeton2024scaling,jawahar2019does,singh2024rethinking}. This advancement has been primarily driven by Sparse Autoencoders (SAEs) \cite{bricken2023towards,cunningham2023sparse,olshausen1996emergence,Makhzani2013kSparseA}. When trained on neural activations from single or multiple layers \cite{ghilardi2024efficient}, SAEs have shown impressive capabilities such as unlearning knowledge ~\cite{farrell2024applying} and Temporal Difference Learning~\cite{Demircan2024SparseAR}). SAEs can also encode semantically meaningful concepts, advancing mechanistic interpretability of LLMs \cite{Men2024UnlockingTF,Zhao_Yang_Shen_Lakkaraju_Du_2024}. Understanding complex activation representations requires breaking them down. SAE's success stems from hypothesis  about LLM's internal representation investigated through various empirical evidences. Recent studies exhibit LLM activations can be effectively understood and interpreted through linear combinations of features (Linear Representation Hypothesis) \cite{mikolov2013linguistic,nanda2023progress,park2023linear}. In order to achieve optimal interpretability, it is essential that each feature corresponds to a single neuron, known as mono-semantic neurons \cite{bau2020understanding,goh2021multimodal}. However, this assumption is impractical because the network must encode more features than the available neurons, resulting in poly-semantic neurons \cite{scherlis2022polysemanticity,mu2020compositional}. In this scenario, a single neuron may respond to multiple, sometimes unrelated, features, a concept referred to as the superposition hypothesis \cite{elhage2022toy}. By leveraging the principle of  sparsity, the network can effectively represent a diverse range of features, even with a limited number of neurons. Thus, each neuron can contribute to multiple features instead of being assigned to only one. Consequently, the activations of the neurons can be viewed as a composite representation of various overlapping features.

\par




%%The study of interpretable features within LLMs has gained remarkable momentum~\cite{templeton2024scaling,jawahar2019does,singh2024rethinking}. This advancement has been primarily driven by Sparse Autoencoders (SAEs) ~\cite{bricken2023towards,cunningham2023sparse,olshausen1996emergence,Makhzani2013kSparseA}. SAE's success stems from hypothesis about LLM's internal representation investigated through various empirical evidences (\ie Linear Representation Hypothesis ~\cite{mikolov2013linguistic,nanda2023progress,park2023linear}, superposition hypothesis ~\cite{elhage2022toy}). These learned sparse features have been investigated thoroughly in various mechanistic interpretability studies~\cite{rai2024practical}. 





%%Training SAEs is computationally intensive, leading to growing interest in scalability approaches~\cite{gao2024scaling}, yet their highly interpretable continuous domain makes them valuable for controlled generation tasks such as feature-based steering ~\cite{Dathathri_Madotto_Lan_Hung_Frank_Molino_Yosinski_Liu_2019,Liu_Sap_Lu_Swayamdipta_Bhagavatula_Smith_Choi_2021,Khalifa_ElSahar_Dymetman_2020}. It involves guiding the text generation process by leveraging specific attributes or features extracted from the input data. This strategy is useful as it does not require  the LLM to retrain/fine tune (~\cite{ouyang2022training}) on data with desired attribute~\cite{dathathri2019plug}. By enforcing sparsity constraints, SAEs facilitate the identification of distinct and meaningful features within the data suitable for this task. Some approaches, such as prompting ~\cite{wallace2404instruction}  show promise for controlling text generation. However, they are not reliable due to their sensitivity to variations ~\cite{chatterjee2024posix}. 



%Recent developments in LLMs have focused heavily on learning better sparse representations of model activations using SAEs \cite{cunningham2023sparse}. These efforts primarily optimize the trade-off between sparsity and reconstruction fidelity. These learned sparse features are have been investigated thoroughly in various mechanistic interpretability studies \cite{rai2024practical}. 




%Since training SAEs are computationally intensive there are other interests growing in terms of scalability \cite{gao2024scaling}. However, due to this highly interpretable continuous domain SAEs can also be useful in  controlled generation tasks like feature based steering \cite{Dathathri_Madotto_Lan_Hung_Frank_Molino_Yosinski_Liu_2019,Liu_Sap_Lu_Swayamdipta_Bhagavatula_Smith_Choi_2021,Khalifa_ElSahar_Dymetman_2020}. 



%Feature steering involves guiding the text generation process by leveraging specific attributes or features extracted from the input data. This strategy is useful as it does not require  the LLM to retrain/fine tune (\cite{ouyang2022training}) on data with desired attribute\cite{dathathri2019plug}. By enforcing sparsity constraints, SAEs facilitate the identification of distinct and meaningful features within the data suitable for this task. Some approaches, such as prompting \cite{wallace2404instruction}  show promise for controlling text generation. However, they are not reliable due to their sensitivity to variations \cite{chatterjee2024posix}. 


%However, the exploration of controlled text generation in this context remains limited. Recent advancements suggest desired text generation can be achieved without full model retraining. This is accomplished by steering the embedding space towards target text embeddings during inference time. Such steering techniques show promise for diverse text generation tasks. A challenge in these experiments lies in identifying the optimal activation shifting direction. Multiple approaches have emerged to address this. Unsupervised methods, like Contrast-Consistent Search \cite{burns2022discovering}, seek directions in the activation space that remain consistent across statement negations. Supervised approaches offer alternative strategies. For instance, probing-based weight direction \cite{li2024inference} employs linear probes trained on attention head outputs to identify truthful directions. This method effectively mimics a single gradient descent step on head activations to maximize truthfulness probability. Another approach, mass means shift \cite{li2024inference}, computes the difference between mean activation vectors of true and false statements to guide shifts toward "truthful" directions during inference. While these methods show promise, they predominantly operate in a binary framework—shifting between false and truthful features. The effectiveness of these approaches in more complex, multi-feature settings remains unclear and warrants further investigation.

\par
 We propose a novel approach of sparse feature based steering during inference-time to generate text with desired attribute. Our method draws motivation from prototypical networks~\cite{snell2017prototypical}, originally designed for few-shot learning, and adapts their principles for controlled text generation. We train SAEs on specific layer activations to capture desired textual features (class). In this method, each class  is represented by a prototype—computed as the mean of corresponding support examples in the learned encoding space. This representation enables efficient classification of new data points through nearest-prototype matching. Once a new data point is categorized to a certain class, we can systematically steer its features toward another prototype to achieve the desired feature. Our prototype-based approach naturally extends to multiple features, offering more flexible and nuanced control over text generation.

\begin{figure*}[htbp]
\centering
  \includegraphics[width=1.08\linewidth]{figures/anew.pdf}
  \caption{Training architecture of our method. We divide the LLM  into Enc-LLM, Upto Layer L till the sparse encoders and Dec-LLM for the rest of the layers including the sparse decoders.
}
  \label{fig:entire}
\end{figure*}

We evaluate our method on synthetically generated dataset, specifically designed to capture varying classes of cognitive style for the given input. Our experiments demonstrate that SAEs effectively capture and distinguish the nuances between these classes, and further investigation reveals their utility in steering generation toward the desired style. Our primary contributions and findings are the following:

\noindent\textbf{Novel Dataset Creation}: We develop a novel dataset by employing proprietary LLMs (Claude~\cite{anthropic2023introducing}) to generate code review problems accompanied by feedback of varying cognitive styles (Bloom's taxonomy)~\cite{uugur2015self}. This dataset facilitates the examination of the steering across multiple features. 

    %\noindent\textbf{Layer-Specific Feature Differentiation}: We observe that training SAEs on internal activations of LLMs across different layers enables the extraction of distinct textual features, such  style, which become prominent at specific range of network layers.
    
    \noindent \textbf{Enhanced Interpretability via Attention Mechanisms}: Our observation reveals that query representations within attention heads are particularly effective in capturing distinguishing textual features, highlighting the critical role of attention components in processing and interpreting input data.

    \noindent \textbf{Text Generation Steering}: We successfully control the generation of texts to achieve specific styles by applying a gradient-based approach to sparsely encoded features, validating the practical applicability of our prototype-based steering method.

    
    \noindent \textbf{Evaluation} We conduct extensive evaluations across various layers and dimensions to demonstrate our approach's effectiveness.

