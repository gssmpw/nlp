We evaluate our approach on feedback generation in educational settings, focusing specifically on cognitive complexity transformation where feedback is adjusted according to Bloom's taxonomy~\cite{blooms}. Due to data scarcity, we generate a synthetic dataset using proprietary LLM ( Claude 3.5 Sonnet (v1)) through prompt engineering, a commonly adopted practice in this domain (see~\cite{liu2023pre,lu2023machine} for a comprehensive
survey). 

We also employ proprietary LLM (Claude 3.5 Sonnet (v1)) as a reference evaluator to categorize the post-steering generated feedback according to Bloom's taxonomy cognitive levels.


\eat{The effectiveness of our steering approach is visualized through stacked bar charts, where each bar illustrates how the steered generations are distributed across different cognitive levels in the dataset.}

\subsection{Dataset}

We construct a novel dataset to evaluate nuanced style transformation. The dataset targets ten fundamental C++ programming concepts (recursive functions, arrays, call-by-reference, switch statements, do-while loops, for-loops, while-loops, pointers, strings, and vectors), each represented by multiple coding questions and systematic feedback variations. At the initial stage, we prompt the LLM to generate 20 questions spanning easy, medium, and hard difficulties, each with its correct answer. Then, we have the LLM create four variations of each answer that contain deliberate errors. Finally, we generate feedback for both the original correct answer and all variants, using Bloom's taxonomy levels~\cite{blooms} (Remember, Understand, Apply, Analyze, Evaluate, and Create). \eat{For detailed strategy, please refer to section \ref{sec: datacretae} }





%For each topic, we first prompt the LLM to generate 20 questions categorized into three difficulty levels: easy, medium, and hard, along with their corresponding answers. Next, we instruct the LLM to create four variations of each answer, deliberately introducing errors. Finally, for all answer variants --including the correct original answer -- we prompt the LLM to generate feedback based on Bloom's taxonomy levels~\cite{blooms} (Remember, Understand, Apply, Analyze, Evaluate, and Create) or with a specified verbosity level (either short or long). %\textbf { Here is a sample data point with Alpaca template.}\par





\begin{figure}[!t]
    \centering

\begin{lstlisting}

Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.}

### Instruction:
Provide feedback on the following code
Write a program to swap two numbers using a function with call by reference.

### Code:

#include <iostream>
using namespace std;

void swap(int &a, int &b) {
    a = b;
    b = a;
}

int main() {
    int x = 5, y = 10;
    cout << "Before swap: x = " << x << ", y = " << y << endl;
    swap(x, y);
    cout << "After swap: x = " << x << ", y = " << y << endl;
    return 0;
}
### Response:




\end{lstlisting}

\begin{lstlisting}
The code structure reveals a logical flow issue where sequential assignments cause data loss. The relationship between the swap function and main program is correct, but the internal swap mechanism fails to maintain value integrity during the exchange process.

\end{lstlisting}

    \caption{A sample data point with Alpaca template.}
    \label{fig:sample-data}
\end{figure}









\par


\eat{For cognitive complexity transformation, we guided the LLM to produce responses aligned with Bloom’s Taxonomy into six hierarchical levels: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating, progressing from lower-order thinking skills to higher-order thinking skills, reflecting a range of pedagogical depth.}

%In total, this process generates 3,000 data points per task: ($(10 \text{ topics}) \times (20 \text{ questions}) \times (3 \text{ levels}) \times (5 \text{ code variants per question})$). For evaluation, we enforced a topic-wise train/validation/test split (50\%, 20\%, 30\% respectively) to maintain conceptual independence across splits. See Figure~\ref{fig:sample-data} for a sample from our dataset.



Our dataset comprises 3,000 data points, calculated as follows: 10 topics × 20 questions × 3 difficulty levels × 5 code variants (one correct, four with errors). We split the data by topic into training (50\%), validation (20\%), and test (30\%) sets to ensure conceptual separation between splits. %For all future reference, we will refer to this dataset as  "style" data.


\eat{
\subsection{Variations in verbosity}
To create length-varied feedback, we employed a three-stage prompting pipeline using LLMs: 
\begin{inparaenum}[(1)]
\item \textbf{Base Questions:} LLM, acting as a C++ expert, generated questions spanning ten function types and three difficulty levels (easy, medium, hard).
\item \textbf{Code Variations:} LLM, assuming the role of a student, produced four error-prone code variations for each question.
\item \textbf{Feedback Generation:} LLM, as a code review assistant, provided feedback at three length granularities (short,medium, long) for both the original and the modified codes.
\end{inparaenum}


\subsection{Feedback Cognitive Variations}
For cognitive-style feedback, we guided an LLM configured as a computer science instructor to produce responses aligned with Bloom’s Taxonomy~\cite{uugur2015self}. Bloom's Taxonomy classifies educational learning objectives into six hierarchical levels: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating, progressing from lower-order thinking skills to higher-order thinking skills, reflecting a range of pedagogical depth.

For all future reference, we will refer to these two datasets as  "style" data.
}

\subsection{Models and Configurations} 
In all of our experiments, we primarily use the \emph{phi-3-mini-4K instruct} model~\cite{abdin2024phi} as the base model, except where otherwise noted. While prompting the input into the model (Figure~\ref{fig:sample-data} shows input data sample), we adapt the Alpaca~\cite{alpaca} format, with maximum context size of 512 tokens. We perform one epoch of fine-tuning on the training split of our dataset. Subsequently, we use the query activations from this fine-tuned model to train the SAEs.

We train multiple independent SAEs on each query attention head of a single layer of the fine tuned base model. Each SAE corresponds to one query attention head with the query embedding size of 96 and maps it to a high-dimensional latent space of 512. For training, we use the $L_{1}$ coefficient ( \(\lambda\)) values of $3\times10^{-4}$, $3\times10^{-3}$, and $3\times10^{-2}$ for different settings.
We train the SAEs for 40 epochs till it converge. We employ a linear warm-up followed by a cosine decay scheduler to dynamically adjust the learning rate, starting from \(3 \times 10^{-5}\). Optimization is performed using the Adam optimizer with \(\beta_{1} = 0.90\) and \(\beta_{2} = 0.99\).

We evaluate our method using the sparse features ($z$) from layer 15 with a \(\lambda\) coefficient of 0.003. During inference-time steered generation, we found that setting \(\eta = 0.8\) produce suitable responses. 
\eat{
We study two experimental setups for steering towards a desired style: \textit{(i)} steering from a source style and \textit{(ii)} steering from LLM output that may belong to any kind of style.}


\subsection{Baselines}
We evaluate our approach against two main baselines: few-shot in-context learning and direct query-steering.

For few-shot in-context learning (ICL), we include N examples sampled from the training data in our prompts and apply a few-shot learning approach~\cite{brown2020language} to assess how well the pre-trained base model captures the intended transformation. Due to constraints in context length, we prefer to use \emph{phi-3-mini-128K instruct} model for this experiment.~\footnote{We have not found any claim that confirms \emph{phi-3-mini-128K instruct} has a better performance compared to its 4K counter part except increased input context window.}

To examine the impact of the sparse representation learned by SAEs, we apply our steering procedure directly to the query embedding. Specifically, we use the query embedding of the fine-tuned base model to guide the transformation without leveraging the sparse encoding.

\eat{
investigated independent query attention behavior to evaluate SAE's effectiveness in steering responses toward specific styles. Using the fine tuned base model's query embeddings for steering revealed good potential for directional control.
}







