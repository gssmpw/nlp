% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.


% Standard package includes


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)

% For Vietnamese characters
% \usepackage[T5]{fontenc
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8


% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.


%Including images in your LaTeX document requires adding
%additional package(s)

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage[final]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tcolorbox}
\usepackage{listings}

\usepackage{subcaption}
\usepackage{paralist}
\usepackage{float}
\usepackage{comment}
\usepackage[export]{adjustbox}
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\ie}{i.e.,\ }
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{rotating}
% \usepackage{showframe}
\usepackage{fp}
\usepackage{makecell} % Allows for line breaks within table cells
\usepackage{tabularx}
\usepackage{listings}
\usepackage{soul}  
\usepackage[textsize=small]{todonotes}

\lstdefinestyle{cppstyle}{
  language=C++,
  basicstyle=\ttfamily\footnotesize,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  showstringspaces=false,
  tabsize=2,
  breaklines=true,
  breakatwhitespace=false,
  escapeinside={\%*}{*)},
  frame=single,
  xleftmargin=2pt,
  xrightmargin=2pt
}


\newcommand{\calcpercent}[2]{%
    \FPeval{\result}{round(#1/#2*100, 1)}%
    \result\%%
}

\usepackage{listings} % Include the listings package

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single % Optional: adds a frame around the code
}


\newcommand{\note}[1]{\textcolor{red}{[#1]}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\notesam}[1]{\textcolor{magenta}{[#1]}}
\newcommand{\eat}[1]{}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\title{Steered Generation via Gradient Descent on Sparse Features}

\author{Sumanta Bhattacharyya \\
  Department of Computer Science \\
  University of Illinois Chicago \\
  \texttt{sbhatt54@uic.edu} \\\And
  Pedram Rooshenas \\
  Department of Computer Science \\
  University of Illinois Chicago \\
  \texttt{pedram@uic.edu} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
\newcounter{inlineitem}
\setcounter{inlineitem}{0}

% Define the \inlineitem command
\newcommand{\inlineitem}[1]{%
  \refstepcounter{inlineitem}%
  \textbf{\theinlineitem)}~#1%
}
\begin{document}
\maketitle 
\begin{abstract}
Large language models (LLMs) encode a diverse range of linguistic features within their latent representations, which can be harnessed to steer their output toward specific target characteristics. In this paper, we modify the internal structure of LLMs by training sparse autoencoders to learn a sparse representation of the query embedding, allowing precise control over the modelâ€™s attention distribution. We demonstrate that manipulating this sparse representation effectively transforms the output toward different stylistic and cognitive targets. Specifically, in an educational setting, we show that the cognitive complexity of LLM-generated feedback can be systematically adjusted by modifying the encoded query representation at a specific layer. To achieve this, we guide the learned sparse embedding toward the representation of samples from the desired cognitive complexity level, using gradient-based optimization in the latent space.
\end{abstract}
\section{Introduction}
\input{intro_new.tex}


\section{Method}
\input{method.tex}

\section {Experimental Setup} 
\input{experiment.tex}

%\section{Evaluation and Analysis}
%\label{sec:eval}
\input{evaluate.tex}
\section{Related works}
\input{bg.tex}



\section{Conclusion and Future Work}


Our analysis of feature space sparsity in cognitive transformation demonstrates that training independent SAEs for each query attention head from mid layer can maintain separate characteristics for each cognitive style. While sparse features typically correspond to a specific style, some may also be activated by closely related styles, as SAEs likely face the challenge of polysemanticity where activations respond to multiple meanings and concepts. This could explain the observed gap between SAE-generated steered text and target text. Although mono-semanticity (where a single neuron corresponds to single feature) is difficult to achieve due to architectural or computational constraints, controlling the degree of polysemanticity remains an important direction for improving precision.
Since we currently train SAEs from the internal activations of fine-tuned LLM, our approach is scalable across different LLM sizes. Future research directions include investigating the co-training of SAEs with LLM to achieve native interpretability through joint optimization.


\bibliography{custom}
\bibliographystyle{acl}

\clearpage


















\input{appendix}




\end{document}

