
Intuitively, we aim to steer text generation from one nuanced style to another by modifying the modelâ€™s internal attention distribution --specifically, by adjusting the query embedding toward a target vector.  At a high level, attention layers direct the vector representation of tokens to capture contextual information. Each attention layer achieves this by creating multiple distributions over the embeddings of other tokens in the context, adjusting each token's embedding to reflect its surrounding context. By modulating these distributions, we gain control over text generation. In particular, each attention distribution is generated via a dot product between two representations of tokens: the query and the key. However, we observed that the query embedding often captures a mixture of multiple target styles, reducing its effectiveness. To address this, we employ a sparse representation obtained via SAE, which enhances the distinctiveness of features associated with each style. By transitioning between these sparse representations, we achieve real-time control over the nuanced style of the generated output during inference. Figure~\ref{fig:entire} provides a schematic over of the proposed method.


\subsection{Sparse Encoding of Nuanced Styles}
Sparse autoencoder (SAE)~\cite{konda2014zero,lee2007sparse} is a type of autoencoder designed to learn efficient data representations by enforcing a sparsity constraint, often through $L_1$ regularization. This constraint encourages the model to activate only a small subset of neurons at any given time, promoting the discovery of distinct and meaningful features. Unlike standard autoencoders, which often learn dense low-dimensional representations, SAEs specialize in high-dimensional sparse representations. These representations are particularly useful for feature disentanglement and enhancing interpretability, as they encourage the network to encode independent and interpretable factors of variation in the data. 
It proves to be effective for analyzing and interpreting latent representations in large language models (LLMs)~\cite{cunningham2023sparse,bloom2024open,marks2023some}. \\
By learning a sparse representation for the query vector with SAEs, we can learn a disentangled representation for every nuanced style of text, which facilitates the transition from one style to another in the latent space.

To effectively represent the styles in the latent space trained by SAE, we borrow the idea from Prototypical Networks~\cite{snell2017prototypical} introduced for few-shot learning.
We define class prototypes as an embedding function 
\( \text{Enc-LLM}{\phi}: \mathbb{R}^{S\times D} \to \mathbb{R}^{S\times A \times H} \) where $S$ is the maximum sequence length (including both prompt and response), $D$ is dimension of token embedding, and $A$ is the number of attention head for intervened layer, $H$ is the size latent representation in SAE, and $\phi$  are learnable parameters of SAE.

\subsection{Steering Output Style} For each style class \(k\), we define a support set $D_k$ consisting of training examples labeled as style class \(k\): \( D_k = \{((x_i, y_i), l_i) : l_i = k\} \), where $x_i$ is prompt, $y_i$ is response, and $l_i$ is style class label. For each class, we define the class prototype $c_k$ as the mean embedding of the support examples:
$c_k = \frac{1}{|S_k|} \sum_{(x_i, y_i, l_i) \in S_k} \text{Enc-LLM}_{\phi}(x_i,y_i)$. 
For each class $k$ we define the probability distribution for each example $(x,y)$ using a softmax over negative distances to the corresponding prototype class, $c_k$: $p_{\phi}(l = k | x, y) = \frac{\exp(-d(\text{Enc-LLM}_{\phi}(x,y), c_k))}{\sum_{k'} \exp(-d(\text{Enc-LLM}_{\phi}(x,y), c_{k'}))}$.

To steer the generation, we modify the embedding of $\text{Enc-LLM}(x,y)$ to increase the target class log-probability, $\log p_\phi( l = t \mid x,y)$.
Instead of directly using the center embedding of the target class ($c_T$) for decoding, which maximizes the defined probability distribution, we use gradient descent to increase the probability. A direct assignment to $c_T$ causes a loss of variability in output styles (for additional study please refer to section {\ref{sec:tcd}}). By optimizing via GD, we allow for a more flexible transformation that respects the latent structure while encouraging stylistic alignment. Moreover, GD allows a gradual and interpretable shift in the latent space, preserving contextual consistency while moving toward the desired style. 
After reaching the desired embedding $z$, we use it to reconstruct the query embedding and decode the output. Algorithm~\ref{alg:steering} illustrates the steering process. 

% The learning process minimizes the negative log-probability $J(\phi) = -\log p_{\phi}(y = k | x)$ of the true class $k$ through stochastic gradient descent. Training episodes are constructed by randomly selecting class subsets from the training set, with each episode designating certain examples as support sets and others as query points. This episodic training paradigm effectively simulates the few-shot learning scenarios encountered during evaluation, while maintaining computational efficiency.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.65\columnwidth]{figures/prot.png}
%   \caption{Prototypical Network}
%   \label{fig:prot}
% \end{figure}


\begin{algorithm}
\caption{Class-based Steering using Sparse Autoencoder}\label{alg:steering}
\begin{algorithmic}[1]
% \STATE \textbf{notation:} Training dataset $D_{\text{train}} = \{(x, y) \in \mathcal{X} \times \mathcal{Y}\}$, class indices $[C] = \{c_{1},c_{2},\ldots\},\eta$ is strength of steering, 
\STATE \textbf{Input:} Source text $x$, target class $c_T$,  $D_{\text{train}}$, LLM, Enc-LLM and Dec-LLM. 
\STATE \textbf{Output:} Steered text toward target class
% \STATE Finetune LLM on $D_{\text{train}}$ for 1 epoch 
% \FOR{$i \in \{1,\ldots,K\}$} \STATE Train $\text{SAE}_i$ on query attention head activations from layer $L$ of LLM \ENDFOR

\STATE $C \gets$ All classes in $D_{\text{train}}$
\FOR{$c \in [C]$}  %// Construct the prototypes
    \STATE $S_c \gets \{x' \mid (x',y') \in D_{\text{train}} \wedge y' = c\}$
    \STATE $\mu_c \gets \frac{1}{|S_c|} \sum_{x'\in S_c} \text{Enc-LLM}(x')$
\ENDFOR
\STATE $y \gets \text{LLM}(x)$
\STATE $z \gets \text{Enc-LLM}(x,y)$
\WHILE{$\|\nabla_z \log P_{\phi}(y = c_2| z) \|^2_2 > \epsilon$ }
% \FOR {$l \in \{1 \cdots L\}$} % // Steer    ing loop
    \FOR{$c \in [C]$} \STATE $d_c \gets \|z - \mu_c\|_2$ \ENDFOR
    \STATE $p_{\phi}(y = c_T | z) \gets \frac{\exp(-d_{c_T})}{\sum_{k=1}^C \exp(-d_k)}$ 
    \STATE $z = z + \eta\frac{\partial}{\partial z} \log P_{\phi}(y = c_2| z)$   
\ENDWHILE
\STATE \textbf{return} $\text{Dec-LLM}(x,z)$

% \STATE \textbf{end for}
\end{algorithmic}
\end{algorithm}



\subsection{Training Sparse Autoenoders}
Our SAE architecture is a single layer ReLU autoencoder (similar to~\citet{bricken2023towards}).
For the input embedding $q_{in}$, encoder $W_e$ and decoder $W_d$, the latent embedding of SAE is defined as 
$z_{sae} = \text{ReLU}(W_e q_{in} + b_e)$.

The loss function combines the reconstruction loss, sparsity constraints, as well as a bias decay term:

\begin{align}
\mathcal{L}(x) = \underbrace{\|\hat{q_{in}} - q_{in}\|_2^2}_{\text{Reconstruction loss}} + \underbrace{\alpha\|z_{sae}\|_1}_{\text{Sparsity loss}} + \underbrace{\beta\|b_e\|_2}_{\text{Bias decay}},
\end{align}
where $\hat{q_{in}} = \frac{W_d}{\max(\|W_d\|_2, \epsilon)}z_{sae}$,
 $\alpha$ is the regularization strength of $L_{1}$, $\beta$ is the bias decay parameter.
We train it on the activations of query attention heads from self attention layer. Since each attention head operates independently in a multi-head attention mechanism to capture different aspects of the input data, we need to train multiple SAEs independently each for the query attention heads. Learned sparse representations by each SAEs provide interpretable continuous features for desired attributes in the data. Hereby steering the generation process toward outputs with specified characteristics and maintaining high fluency~\cite{liang2024controllable}. We observe that the trained SAEs on the earlier or later layers of the LLM are not as beneficial as compared to the middle layers (such as layer 15). Based on the position of the SAE at particular layer L ($L_{all}$ being the number of layers), We define Enc-LLM as the collection of layers till the sparse encoders at L, rest ($L_{all}$-L) layers including the sparse decoders are defined as Dec-LLM. 

\eat{
To quantify the benefit of these sparse features, we follow a similar~\cite{snell2017prototypical} few-shot classification. We collected fixed number of $x'$ from training data for each class $c \in [C]$ as support examples and take the mean of the sparse features of these support examples to create class-specific centers (prototypes) ($\mu_c$). For an unseen example, association with a particular prototype is determined by its proximity to the prototype of each class. This accuracy defines how well the prototypes have captured the distinctness between the different class feature. For an quantitative analysis, Please refer to Table \ref{tab:matching-analysis} or refer to the Figure \ref{fig:combined_analysis} for the visual plot.
 

}


 














% \subsection{Steered generation}
% Since prototypes are strong representation of each class, This can be used to steer the generation for a particular text which belongs to one class's prototype ($i$) to other class's prototype($j$).
% As shown in the Algorithm~\ref{alg:steering}, To steer generation toward a new prototype ($j$), we compute the gradient of the log-probability of the target class with respect to $z$. 

%\begin{equation}%\label{eq:grad_calculation}

%\begin{split}
%\frac{\partial}{\partial z} \log %P_{\phi}(y = j|c, z) = & -2(z-c_j) \\
%                                                 & - 2\mathbb{E}_{P_{\phi}(y=j|c,z)}(z-c_i)
%\end{split}
%\end{equation}

This gradient is then used to guide the steering process toward the desired class:
\begin{align}
z_{i \rightarrow j} = z_{i} + \eta\frac{\partial}{\partial z} & \log P_{\phi}(y = j|c, z)\label{eq:steering}
\end{align}
Here, $\eta$ is a hyperparameter that controls the step size of the gradient update.

\begin{comment}
\begin{figure*}[t]
  \includegraphics[width=0.88\linewidth]{latex/figures/traj.pdf}
  \caption{Comparison of Bloom's taxonomy based level transformations showing UMAP feature space (left) and density landscape (right) for different transitions. (Up) Feature trajectory from ``remember'' to ``create'' showing diagonal density pattern indicating hierarchical progression. (Down) Feature trajectory from ``remember'' to ``analyze'' showing horizontal density concentration.}
  \label{fig:traj}
\end{figure*}
\end{comment}











