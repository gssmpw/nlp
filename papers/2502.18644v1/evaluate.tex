\begin{figure*}[ht]
\centering
  \includegraphics[width=.88\linewidth]{figures/l1.png}
  \caption{ Measured class distribution showing steering accuracy from source (Understand) to target cognitive categories. Target classes appear on X-axis while Y-axis shows class category distribution of steered text. }
  \label{fig:l1}
\end{figure*}








\section{Cognitive Transformation}

In this section, we evaluate our approach based on two evaluation methods: 1) Classification accuracy. 2) Categorizing steering-modified generated text. \eat{For a full list of experiments please also see \ref{sec: be}.}


\subsection{Quality of Sparse Encodings}
First, we evaluated the quality of the sparse embedding using a classification task. We created 
a few-shot learning framework with $N=30$ support examples per class to examine how well $z$ maintains distinction between different cognitive levels. Table \ref{tab:matching-analysis} suggests $L_{1}$ regularization and layers influence class separation. Increasing the $L_{1}$ coefficient generally led to more pronounced sparsity, improving differentiation between classes. Among the layers, middle layer displays the clearest benefit, indicating that sparsity at this intermediate representation level best captures distinct cognitive styles.






\begin{comment}
\begin{table}[htbp]
    \centering
    \caption{Layer-wise Analysis of Multi-class Classification}
    \label{tab:matching-analysis}
    \setlength{\tabcolsep}{5.2pt}
    \tiny
    \begin{tabular}{@{}lcc|cccccc@{}}
        \toprule
        Layer & Coef. & N & rem & und & app & ana & eva & cre \\
        \midrule
        \multirow{3}{*}{5} 
            & .0003 & 30 & \calcpercent{450}{960} & \calcpercent{313}{960} & \calcpercent{142}{960} & \calcpercent{51}{960} & \calcpercent{103}{960} & \calcpercent{216}{960} \\
            & .0003 & 300 & \calcpercent{287}{960} & \calcpercent{391}{960} & \calcpercent{185}{960} & \calcpercent{154}{960} & \calcpercent{412}{960} & \calcpercent{292}{960} \\
            %& .0003 & 950 & \calcpercent{256}{960} & \calcpercent{357}{960} & \calcpercent{371}{960} & \calcpercent{14}{960} & \calcpercent{251}{960} & \calcpercent{205}{960} \\
             %& .03 & 950 & \calcpercent{278}{960} & \calcpercent{318}{960} & \calcpercent{294}{960} & \calcpercent{9}{960} & \calcpercent{162}{960} & \calcpercent{44}{960} \\

             & .003 & 30 & \calcpercent{497}{960} & \calcpercent{281}{960} & \calcpercent{137}{960} & \calcpercent{57}{960} & \calcpercent{90}{960} & \calcpercent{127}{960} \\

              & .003 & 300 & \calcpercent{293}{960} & \calcpercent{344}{960} & \calcpercent{142}{960} & \calcpercent{130}{960} & \calcpercent{265}{960} & \calcpercent{65}{960} \\

             & .03 & 30 & \calcpercent{464}{960} & \calcpercent{259}{960} & \calcpercent{126}{960} & \calcpercent{42}{960} & \calcpercent{84}{960} & \calcpercent{98}{960} \\

             & .03 & 300 & \calcpercent{293}{960} & \calcpercent{344}{960} & \calcpercent{142}{960} & \calcpercent{130}{960} & \calcpercent{265}{960} & \calcpercent{65}{960} \\
        \midrule
        \multirow{4}{*}{15} 
            & .0003 & 30 & \calcpercent{146}{960} & \calcpercent{746}{960} & \calcpercent{117}{960} & \calcpercent{139}{960} & \calcpercent{219}{960} & \calcpercent{427}{960} \\

            & .0003 & 300 & \calcpercent{500}{960} & \calcpercent{344}{960} & \calcpercent{383}{960} & \calcpercent{237}{960} & \calcpercent{512}{960} & \calcpercent{507}{960} \\

            
            & .003 & 30 & \calcpercent{164}{960} & \calcpercent{604}{960} & \calcpercent{101}{960} & \calcpercent{180}{960} & \calcpercent{232}{960} & \calcpercent{338}{960} \\

             & .003 & 300 & \calcpercent{373}{960} & \calcpercent{288}{960} & \calcpercent{373}{960} & \calcpercent{225}{960} & \calcpercent{442}{960} & \calcpercent{472}{960}
             \\
            
            & .03 & 30 & \calcpercent{144}{960} & \calcpercent{514}{960} & \calcpercent{119}{960} & \calcpercent{146}{960} & \calcpercent{270}{960} & \calcpercent{468}{960} \\
            & .03 & 300 & \calcpercent{342}{960} & \calcpercent{274}{960} & \calcpercent{357}{960} & \calcpercent{228}{960} & \calcpercent{442}{960} & \calcpercent{501}{960} \\
             %& .003 & 950 & \calcpercent{327}{960} & \calcpercent{393}{960} & \calcpercent{274}{960} & \calcpercent{127}{960} & \calcpercent{327}{960} & \calcpercent{671}{960}
             
            
             
        \midrule
        \multirow{3}{*}{28} 
            & .0003 & 30 & \calcpercent{496}{960} & \calcpercent{407}{960} & \calcpercent{113}{960} & \calcpercent{61}{960} & \calcpercent{109}{960} & \calcpercent{189}{960} \\
            & .0003 & 300 & \calcpercent{295}{960} & \calcpercent{282}{960} & \calcpercent{246}{960} & \calcpercent{181}{960} & \calcpercent{466}{960} & \calcpercent{308}{960} \\

            & .003 & 30 & \calcpercent{347}{960} & \calcpercent{480}{960} & \calcpercent{105}{960} & \calcpercent{94}{960} & \calcpercent{100}{960} & \calcpercent{168}{960} \\

            & .003 & 300 & \calcpercent{198}{960} & \calcpercent{219}{960} & \calcpercent{239}{960} & \calcpercent{144}{960} & \calcpercent{311}{960} & \calcpercent{82}{960} \\
            & .03 & 30 & \calcpercent{282}{960} & \calcpercent{414}{960} & \calcpercent{85}{960} & \calcpercent{111}{960} & \calcpercent{76}{960} & \calcpercent{102}{960} \\
            & .03 & 300 & \calcpercent{198}{960} & \calcpercent{219}{960} & \calcpercent{239}{960} & \calcpercent{144}{960} & \calcpercent{311}{960} & \calcpercent{82}{960} \\

            %& .0003 & 950 & \calcpercent{302}{960} & \calcpercent{317}{960} & \calcpercent{295}{960} & \calcpercent{65}{960} & \calcpercent{263}{960} & \calcpercent{530}{960} \\

            %& .03 & 950 & \calcpercent{286}{960} & \calcpercent{278}{960} & \calcpercent{245}{960} & \calcpercent{11}{960} & \calcpercent{184}{960} & \calcpercent{133}{960} \\

          

             

            
        \bottomrule
    \end{tabular}
    \\ \vspace{1mm}
    \footnotesize{Note: N=sample size; rem=remember; und=understand; app=apply; ana=analyze; eva=evaluate; cre=create}
\end{table}

\end{comment}


\begin{table}[htbp]
    \centering
    \caption{Layer-wise Analysis of Multi-class Classification}
    \label{tab:matching-analysis}
    \setlength{\tabcolsep}{4.2pt}
    \tiny
    \begin{tabular}{@{}lcc|ccccccc@{}}
        \toprule
        Layer & Coef. & N & rem & und & app & ana & eva & cre & avg \\
        \midrule
        \multirow{3}{*}{5} 
            & .0003 & 30 & \calcpercent{450}{960} & \calcpercent{313}{960} & \calcpercent{142}{960} & \calcpercent{51}{960} & \calcpercent{103}{960} & \calcpercent{216}{960} & 22.14\% \\
            & .0003 & 300 & \calcpercent{287}{960} & \calcpercent{391}{960} & \calcpercent{185}{960} & \calcpercent{154}{960} & \calcpercent{412}{960} & \calcpercent{292}{960} & 29.88\% \\
            & .003 & 30 & \calcpercent{497}{960} & \calcpercent{281}{960} & \calcpercent{137}{960} & \calcpercent{57}{960} & \calcpercent{90}{960} & \calcpercent{127}{960} & 20.64\% \\
            & .003 & 300 & \calcpercent{293}{960} & \calcpercent{344}{960} & \calcpercent{142}{960} & \calcpercent{130}{960} & \calcpercent{265}{960} & \calcpercent{65}{960} & 21.51\% \\
            & .03 & 30 & \calcpercent{464}{960} & \calcpercent{259}{960} & \calcpercent{126}{960} & \calcpercent{42}{960} & \calcpercent{84}{960} & \calcpercent{98}{960} & 18.63\% \\
            & .03 & 300 & \calcpercent{293}{960} & \calcpercent{344}{960} & \calcpercent{142}{960} & \calcpercent{130}{960} & \calcpercent{265}{960} & \calcpercent{65}{960} & 21.51\% \\
        \midrule
        \multirow{4}{*}{15} 
            & .0003 & 30 & \calcpercent{146}{960} & \calcpercent{746}{960} & \calcpercent{117}{960} & \calcpercent{139}{960} & \calcpercent{219}{960} & \calcpercent{427}{960} & 31.15\% \\
            & .0003 & 300 & \calcpercent{500}{960} & \calcpercent{344}{960} & \calcpercent{383}{960} & \calcpercent{237}{960} & \calcpercent{512}{960} & \calcpercent{507}{960} & 43.11\% \\
            & .003 & 30 & \calcpercent{164}{960} & \calcpercent{604}{960} & \calcpercent{101}{960} & \calcpercent{180}{960} & \calcpercent{232}{960} & \calcpercent{338}{960} & 28.11\% \\
            & .003 & 300 & \calcpercent{373}{960} & \calcpercent{288}{960} & \calcpercent{373}{960} & \calcpercent{225}{960} & \calcpercent{442}{960} & \calcpercent{472}{960} & 37.73\% \\
            & .03 & 30 & \calcpercent{144}{960} & \calcpercent{514}{960} & \calcpercent{119}{960} & \calcpercent{146}{960} & \calcpercent{270}{960} & \calcpercent{468}{960} & 28.84\% \\
            & .03 & 300 & \calcpercent{342}{960} & \calcpercent{274}{960} & \calcpercent{357}{960} & \calcpercent{228}{960} & \calcpercent{442}{960} & \calcpercent{501}{960} & 37.22\% \\
        \midrule
        \multirow{3}{*}{28} 
            & .0003 & 30 & \calcpercent{496}{960} & \calcpercent{407}{960} & \calcpercent{113}{960} & \calcpercent{61}{960} & \calcpercent{109}{960} & \calcpercent{189}{960} & 23.87\% \\
            & .0003 & 300 & \calcpercent{295}{960} & \calcpercent{282}{960} & \calcpercent{246}{960} & \calcpercent{181}{960} & \calcpercent{466}{960} & \calcpercent{308}{960} & 30.87\% \\
            & .003 & 30 & \calcpercent{347}{960} & \calcpercent{480}{960} & \calcpercent{105}{960} & \calcpercent{94}{960} & \calcpercent{100}{960} & \calcpercent{168}{960} & 22.47\% \\
            & .003 & 300 & \calcpercent{243}{960} & \calcpercent{259}{960} & \calcpercent{249}{960} & \calcpercent{164}{960} & \calcpercent{331}{960} & \calcpercent{97}{960} & 23.30\% \\
            & .03 & 30 & \calcpercent{282}{960} & \calcpercent{414}{960} & \calcpercent{85}{960} & \calcpercent{111}{960} & \calcpercent{76}{960} & \calcpercent{102}{960} & 18.58\% \\
            & .03 & 300 & \calcpercent{198}{960} & \calcpercent{219}{960} & \calcpercent{239}{960} & \calcpercent{144}{960} & \calcpercent{311}{960} & \calcpercent{82}{960} & 20.71\% \\
        \bottomrule
    \end{tabular}
    \\ \vspace{1mm}
    \footnotesize{Note: N=sample size; rem=remember; und=understand; app=apply; ana=analyze; eva=evaluate; cre=create; avg=average}
\end{table}
Additionally, augmenting the number of samples (N) used to compute class prototypes enhances the classification accuracy, suggesting that richer exemplars help the model refine its notion of each cognitive category. %It confirms that tuning sparsity and increasing sample support can guide embeddings to more effectively capture nuanced cognitive style distinctions\todo{how do get that from a classification?}. (We found this accuracy across different configurations is valid across different model. Please refer to \ref{subsec:mistral} in the appendix)


\subsection{Effectiveness of Transformation}

This evaluation involves classifying the steered text into Bloom's Taxonomy levels. Figure \ref{fig:l1} shows a bar chart comparing text generated using three steering approaches—baseline steering, Q-Steering, and SAE-based steering—as the style shifts from source class('Understand') to other styles, measured against the target data for each intended styles. Each bar in the chart represents the distribution of categories identified by Claude from the steered generations.

For example, we observe that query attention features are effective in steering text. However, due to the dense nature of query features, the generated sentences often exhibit multiple styles simultaneously. For instance, feedback from query embedding steering may include characteristics of both evaluate and apply levels of Bloom's Taxonomy. Enforcing sparsity in the feature space allows the model to better navigate toward specific attributes, as distinguishable attributes become more accessible in a sparse feature space.


Please refer to section~\ref{sec:Qualitative} for qualitative examples on SAE generated steered text, also refer to Section~\ref{sec:Qgeneration} for a qualitative comparison between with and without SAE  generated steered text.












\subsection{Different Layers of Attention}

We investigate the relationship between query embedding sparsity and steering behavior across different layers. In general, we find that sparse query features are better than any other layer components for steering. Middle layer attention demonstrates superior performance for steering tasks relative to other layers, as shown in the 'remember' to 'understand' task (last 4 bar charts) in Figure~\ref{fig:across_attn}. In contrast, features from lower layer attention (simpler and more localized relationships) and higher layer attention (specific and distant relationships) capture less of the intended target style attribute. This finding aligns with previous research showing that attention most strongly correlates with syntactic dependency relations in the model's middle layers~\cite{vig2019analyzing}, where these dependencies have their most effective representation. Further support comes from early studies~\cite{hu2021syntax,gong2020rich} demonstrating that syntactic dependency enhances textual style transfer by preserving content through grammatical structures.

\begin{figure}[!ht]
\centering
  \includegraphics[width=.88\linewidth]{figures/across_attn.png}
  \caption{ Measured class distribution showing steering accuracy from source (Remember) to target cognitive categories. Target classes appear on X-axis while Y-axis shows class category distribution of steered text. }
  \label{fig:across_attn}
\end{figure}






%Early work~\cite{vig2019analyzing} suggests lower layers tend to rely on position-level attention, capturing simpler and more localized relationships. In contrast, deeper layers encode more specific and distant relationships.


%Linguistically, the complexity of syntactic dependency varies depending on the cognitive level of the task. At lower levels, (remember), syntactic dependencies are relatively straightforward, often limited to basic relationships like subject-verb-object structures. However, at higher levels, (analyzing) richer syntactic dependencies are required. These involve more complex hierarchical and nested relationships, reflecting the increased linguistic and cognitive demands of these tasks.


\subsection{Impact of Sparsity Regularization}
Training SAEs with $L_{2}$ versus $L_{1}$ regularization techniques creates fundamentally different feature spaces. $L_{2}$ regularization, which does not reduce features to zero, creates a less sparse feature space that results generated steered text, blending categories from different styles. This is evident in Figure~\ref{fig:l2vl1}'s 'remember' to 'analyze' task (first two bar charts), where only few samples are classified as analyze. In contrast, $L_{1}$ regularization creates sparse feature space that directs generation more strongly toward the intended target style, as sparsity preserves the distinct attribute while keeping unrelated features suppressed.
\begin{figure}[!ht]
\centering
\includegraphics[width=.88\linewidth]{figures/l2.png}
\caption{Distribution of steering accuracy showing how effectively samples from the source class (Remember) were redirected to each intended target class, comparing SAE models trained with $L_{2}$ versus $L_{1}$ loss}
\label{fig:l2vl1}
\end{figure}






We explored using alternative model components beyond query attention heads for generation control. Specifically, we trained SAE on the residual stream at layers 5,15,25 but observed no meaningful impact on steering the output. This aligns with our intuition, as residual streams aggregate information from previous layers, potentially diluting the precise control needed for targeted content modulation. In contrast to middle layer, We observe features at layer 25 generates meaningful text. The qualitative comparison (remember to create) between SAE trained on query activations (SAE-Q) and residual activations (SAE-R) is shown in Figure~\ref{fig:responses}. The generated output from the SAE-R is inadequate, as it fails to properly align with the desired style.
\begin{figure}[!htbp]
{\small
\hrule
\vspace{.1cm}
Original Response (Remember): Your code shows awareness of function declarations and reference parameters, but there's a syntax error in the parameter declaration. The correct syntax for reference parameters in C++ is 'int\&' not '\&int'. This fundamental syntax is crucial for the code to compile properly.
\vspace{.1cm}
\hrule
\vspace{.1cm}
SAE-R: Your code demonstr understandingareness of pointer structure and pointer parameters, but there's a fundamental error in the pointer pattern. The relationship relationship should passing parameter requires C++ is '\&*', instead 'int'
\vspace{.1cm}
\hrule
\vspace{.1cm}
SAE-Q: While the function structure using reference parameter shows good understanding of memory concepts, the implementation fails in practical application. The relationship between function parameters and their effects on caller scope needs consideration.
\vspace{.1cm}
\hrule
\vspace{.1cm}
}
\caption{Comparison of responses between SAE trained on residual stream and SAEs trained on query attention feature.}
\label{fig:responses}
\end{figure}




\eat{
\medskip



\begin{lstlisting}
Original response (Remember):Your code 
shows awareness of function declarations 
and reference parameters, but there's a 
syntax error in the parameter 
declaration. The correct syntax for 
reference parameters in C++ is 'int\&' 
not '\&int'. This fundamental syntax is 
crucial for the code to compile properly.
\end{lstlisting}

\begin{lstlisting} 
SAE-R: Your code demonstr 
understandingareness of pointer 
structure and pointer parameters, but 
there's a fundamental error in the 
pointer pattern. The relationship 
relationship should passing parameter 
requires C++ is '\&*', instead ' 'int'


\end{lstlisting}

\begin{lstlisting}
SAE-Q: While the function structure
using reference parameter shows good 
understanding of memory concepts, the 
implementation fails in practical 
application. The relationship between 
function parameters and their effects on 
caller scope needs consideration.
\end{lstlisting}
}






%\begin{figure}[t]
%  \includegraphics[width=0.88\linewidth]{figures/resvsq.png}
%  \caption{Quantitative comparison using ROUGE score}
%  \label{fig:resvq}
%\end{figure}
    






%In contrast, the query attention head proved more effective, as it specifically handles token relationships, making it well-suited for understanding context and guiding the generation process.

 









    



%Similar analysis performed to check how SAEs, trained on different layers perform steering Based generation. Please refer to figure \ref{fig:gtof}  in appendix for more details. Please refer to Appendix A\ref{subsec:Qualitative} for more qualitative examples. 



