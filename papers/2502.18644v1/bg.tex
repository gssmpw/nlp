\subsection{Sparse Autoencoders}
\label{sec:sae_bg}
The concept of sparse coding using over-complete dictionaries was first introduced by Mallat et al.~\cite{mallat1993matching}. This foundation was significantly advanced by Olshausen et al.~\cite{olshausen1996emergence}, introduced an unsupervised approach to learn the dictionary directly from data. While this method primarily used in image processing ~\cite{liu2015sparse}, a parallel development emerged when~\citet{hinton2006reducing} introduced the autoenoder architecture for dimensionality reduction. These two streams of research eventually converged in the development of SAE ~\cite{konda2014zero,lee2007sparse}, which incorporates sparsity constraints, such as $L_{1}$ regularization, to extract meaningful sparse features. 

\eat{The core concept of training SAEs is to enable them to reconstruct their input with high accuracy while minimizing the number of active neurons.} 



%SAEs have recently gained popularity due to their ability to decompose LLM activations into interpretable features~\cite{cunningham2023sparse}. 



\eat{For instance, Input activation $\mathbf{z_{in}} \in \mathbb{R}^d$ can be reconstructed as a sparse linear combination of $d_{\text{sae}}$ where $d_{\text{sae}} \gg d$. A basic SAE architecture includes encoder and decoder parameterized by $\mathbf{W}_e$ and $\mathbf{W}_d$ respectively. $\mathbf{z_{sae}} = \text{ReLU}(\mathbf{W}_e\mathbf{z_{in}} + \mathbf{b}_e)$, $\mathbf{W}_d^{\text{norm}} = \frac{\mathbf{W}_d}{\max(\|\mathbf{W}_d\|_2, \epsilon)}$, $\hat{\mathbf{z_{in}}} = \mathbf{W}_d^{\text{norm}}\mathbf{z_{sae}}$, $\mathcal{L}(x) = \underbrace{\|\hat{z_{in}} - z_{in}\|_2^2}_{\text{Reconstruction loss}} + \underbrace{\alpha\|z_{sae}\|_1}_{\text{Sparsity loss}} + \underbrace{\beta\|\mathbf{b}_e\|_2}_{\text{Bias decay}}$ where $\alpha$ is the regularization strength of $L_{1}$ and $\beta$ is the bias decay parameter. In general. SAEs employ $L_{1}$ regularization as a sparsity prior, leads to activation shrinkage. K-sparse autoencoders\cite{makhzani2013k} alleviate this issue by exploiting Top-K activation along with removing the sparsity prior. It is effective \cite{gao2024scaling} not only in terms of reconstruction fidelity but also reducing the presence of dead latent in large scale training.}



%The effectiveness of SAEs in analyzing various language models (BERT~\cite{yun2021transformer}, GPT~\cite{sharkey2022taking}, and Pythia~\cite{cunningham2023sparse}) originates from the ability to represent semantic and syntactic concepts in LLMs as linear combinations of activation patterns~\cite{bloom2024open,marks2023some}. LLM activations can be effectively understood and interpreted through linear combinations of features (Linear Representation Hypothesis) \cite{mikolov2013linguistic,nanda2023progress,park2023linear}. In order to achieve optimal interpretability, it is essential that each feature corresponds to a single neuron, known as mono-semantic neurons \cite{bau2020understanding,goh2021multimodal}. However, this assumption is impractical because the network must encode more features than the available neurons, resulting in poly-semantic neurons \cite{scherlis2022polysemanticity,mu2020compositional}. In this scenario, a single neuron may respond to multiple, sometimes unrelated, features, a concept referred to as the superposition hypothesis \cite{elhage2022toy}.








%This principle is rooted in foundational work on dictionary learning, which established methods for decomposing complex patterns into interpretable basis sets~\cite{mallat1993matching,olshausen1996emergence}.

SAEs have recently gained popularity due to their ability to decompose LLM activations into interpretable features~\cite{cunningham2023sparse}. This decomposition enables the representation of semantic and syntactic concepts in LLMs as linear combinations of activation patterns, a property that has been effectively demonstrated across various language models, including BERT~\cite{yun2021transformer}, GPT~\cite{sharkey2022taking}, and Pythia~\cite{cunningham2023sparse}~\cite{bloom2024open,marks2023some}. The interpretability of sparse activations is grounded in the Linear Representation Hypothesis, which posits that these activations can be understood as linear combinations of features~\cite{mikolov2013linguistic,nanda2023progress,park2023linear}. For optimal interpretability, it is desirable for each feature to correspond to a single neuron, a concept known as mono-semantic neurons~\cite{bau2020understanding,goh2021multimodal}. However, this ideal scenario is often unattainable because neural networks must encode more features than there are available neurons. This limitation leads to the emergence of poly-semantic neurons, where a single neuron gets triggered by multiple, potentially unrelated features~\cite{scherlis2022polysemanticity,mu2020compositional}. This phenomenon is explained by the superposition hypothesis, which suggests that neurons share representational capacity by encoding multiple features in overlapping ways~\cite{elhage2022toy}. This trade-off between interpretability and representational efficiency underscores the challenges and opportunities in understanding LLM activations through SAEs. Further, methods like activation pruning techniques~\cite{rajamanoharan2024jumping} or thresholds~\cite{rajamanoharan2024improving} for feature selection, have helped improve the balance between keeping representations sparse while maintaining good reconstruction quality. 
%\par
%Although it is still an ongoing research topic which specific layers contribute to the significant learning of language model's features or this contribution changes based on the specific task. Most current research focuses on residual stream layers \cite{lawson2024residual} because they not only contain rich information, but also show how semantic and linguistic information changes as it flows through the different levels of the model.




\par
\subsection{Steered generation}



Research in computer vision and natural language processing (NLP) has shown that content generation can be controlled through adjustments in latent space~\cite{larsen2016autoencoding,white2016sampling} and word embeddings~\cite{han2024word}. Instead many popular approaches (\ie supervised fine-tuning, reinforcement learning from human feedback (RLHF), steerable layers, weight editing~\cite{ranzato2015sequence,dathathri2019plug,ilharco2022editing,meng2022locating}) involve modifying the model's weights directly, undermining the model's overall performance~\cite{qi2023fine}. Alternative methods that avoid direct weight modification and use intervention at different levels  like decoding~\cite{gu2017trainable}, prompt~\cite{zhou2022steering}, and token embeddings~\cite{khashabi2021prompt} have gained popularity for influencing model behavior without compromising performance.



\par
Intervening into a language modelâ€™s activations has emerged as an efficient strategy for controlling model behavior without modifying its weights. This approach involves learning steering vectors which guide the model's behavior more effectively than direct weight alterations~\cite{hernandez2023inspecting,subramani2022extracting}. Building on this idea, methods such as activation engineering, which leverage latent space arithmetic, have gained traction as practical tools for manipulating LLMs~\cite{correia2019adaptively}. However, the effectiveness of these approaches still relies on determining the steering vectors. Inference-Time Intervention (ITI)~\cite{li2024inference} computes steering vectors applied uniformly across all sequence positions during inference, using trained linear probes to identify attention heads with distinct activation patterns for true and false statements, enabling the model to produce more truthful outputs. ActAdd~\cite{turner2023activation} creates steering vectors by computing the difference in model activations between contrasting prompts, then adds these vectors during inference to guide the model's outputs toward desired traits. Similar approaches can also improve upon in-context learning by introducing in-context vectors~\cite{liu2023context}. While training SAE is computationally intensive~\cite{gao2024scaling}, their highly interpretable continuous domain makes them valuable for feature-based steering~\cite{durmusevaluating}. For instance, sparse activation editing in the middle layer's residual stream during inference time allows pre-trained SAE to effectively steer LLMs to resolve context-memory knowledge conflicts~\cite{zhao2024steering}.  By enforcing sparsity constraints, SAE facilitates the identification of distinct and meaningful features within the data~\cite{chalnev2024improving,o2024steering}. 




We explore the role of sparsity in feature space  for steering text toward desired styles through independently trained SAEs on query attention heads. Unlike previous approaches using activation editing, we employ inference time gradient descent to achieve style transfer. Our findings align with layer-based feature patterns from previous works, particularly revealing that middle layer features enable SAEs to learn task-specific useful characteristics.


\eat{
Based on  the potential of LLM activations, various interpretability studies~\cite{burns2022discovering,nanda2023progress} include these activations or train models like SAE using these activations as input. Intuitively, 
These are generally considered more interpretable~\cite{marks2023interpreting} than LLM weights as they often correspond more directly to semantic concepts and features of the input~\cite{zou2023representation}. Recent findings also suggests that~\cite{zhao2024steering} signals of knowledge conflict can be detected in the residual stream of LLMs, particularly in the mid-layers. Our findings partially align with previous research showing that middle layers play an important role in steering text generation. However, unlike prior work, we found that attention layers - specifically query attention heads - were more influential than residual layers for understanding text attributes~\cite{vig2019analyzing} . Our approach focuses on creating sparse attention activations, which effectively guide the model toward generating text with the desired style.

\par 


}