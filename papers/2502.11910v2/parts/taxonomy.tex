\vspace{-10pt}
\section{Adversarial Robustness Taxonomy}
%Historically, the cybersecurity model known as \emph{CIA} fundamentally classifies security into three key aspects: confidentiality, integrity, and availability~\citep{barreno_can_2006}. Here we focus on attack on model integrity, where integrity describes that the model operates as intended, producing accurate and reliable outputs under expected conditions.
%Within this framework, confidentiality refers to protecting sensitive information related to the model, such as its architecture, parameters, and training data. Integrity ensures that the model operates as intended, producing accurate and reliable outputs under expected conditions. Availability focuses on maintaining consistent access to the model's outputs and features, ensuring it remains functional and usable. In this position paper, we focus specifically on the \textit{integrity} of machine learning models.
%In this context, earlier works on adversarial robustness classify model integrity based on goals, capabilities~\citep{papernot_practical_2017, carlini_evaluating_2019}. We use this definition to illustrate differences between past threat models and current ones in LLMs later in this work.

To systematically define differences between prior adversarial robustness research and emerging threat models in LLMs we introduce a taxonomy inspired by established cybersecurity definitions organized into robustness goals and capabilities~\citep{papernot_practical_2017, carlini_evaluating_2019}. In the remainder of this work, we use this taxonomy to formally define differences between previous threat models and emerging ones in LLMs.

\vspace{-5pt}
\subsection{Adversarial Robustness Goals}\label{sec:tax-goals}% 
The overarching goal of adversarial robustness research is to improve the robustness of neural networks against input perturbations. To this end, clearly defining this goal is crucial for consistent and meaningful comparisons of robustness evaluations across multiple works. 
This requires introducing a sound mathematical framework that specifies the conditions under which robustness should be achieved. %In this work, we specifically discuss robustness in evasion (test time) and poisoning (training time) settings, which can be generally formulated as constrained optimization problems: One of the most prominent adversarial threat models are evasion attacks.
%
For example, considering evasion (test-time) robustness, the goal is to determine the worst-case change in the model's output under constrained changes in the input data:
\begin{equation}\label{eq:adv-goal-evasion}
    \delta^*(x) = \max_{\tilde{x} \in \gC(x)} \; d_{out} (f(x), f(\tilde{x})), 
\end{equation}
where $x$ is a fixed input, $f$ is the model, $d_{out}$ is a general distance metric between the model outputs, and $\gC(x)$ is the set of possible perturbations within a certain distance of $x$ (see \S\ref{sec:tax-capability}). %The distance metric $d_{out}$ can be defined in various ways, depending on the model's output space. For example, in the computer vision domain, $d_{out}$ is often defined as the $\ell_p$ distance between the model's output logits.
%
%\textbf{Poisoning robustness.} In poisoning, the goal is to find the worst-case robustness under constrained changes in the training data, which can be modelled as the following bi-level constrained optimization problem:
% \begin{equation}\label{eq:adv-goal-poisoning}
%      \delta^*(D) \!= \kern-6pt\min_{\tilde{D} \in \gC(D)}\!\! \gL_{rob} (\theta, \tilde{D}) \;\;\textrm{s.t.}\;\; \theta\!\in\!\argmin_{\theta'\in\Theta} \gL(\theta', \tilde{D}),
% \end{equation}
% where $\gL_{rob}$ is a robustness objective (e.g., the test-time accuracy), $\gL$ is the model's training objective (e.g., a loss function), $\Theta$ are the model parameters, $D$ is the training dataset, and $\gC(D)$ is the set of possible training set perturbations within a certain distance of $D$. 
%
The optimization problem in \autoref{eq:adv-goal-evasion} establishes a fundamental definition of adversarial robustness in evasion settings, and serves as the foundation for its evaluation. Solving this optimization problem is NP-hard for most practical scenarios~\cite{katz2017reluplex}. To address this challenge, we require algorithms to assess robustness by computing lower and upper bounds on the model's robustness $\delta^*$. 
%
In the evasion example, lower bounds $\underline{\delta}(x)$ on $\delta^*(x)$ defined in \autoref{eq:adv-goal-evasion} can be derived using adversarial attacks, and upper bounds $\overline{\delta}(x)$ can be derived using robustness certification. Together, this allows to estimate the robustness of a given model: $\underline{\delta}(x) \leq \delta^*(x) \leq \overline{\delta}(x)$. In general, attacks and certificates provide (provable) bounds on robustness, and defenses (such as adversarial training) constitute strategies for achieving the goal of improved robustness.
\vspace{-5pt}
\subsection{Capability in Adversarial Robustness}\label{sec:tax-capability}% 
We define capabilities of robustness algorithms by 1) their knowledge of the optimization problem, e.g., about the underlying functions $f$, 2) possible constraints, e.g., perturbation budgets, and 3) the required computational effort. 

\textbf{Knowledge.} Knowledge and access classifications typically include up to four categories, consisting of white-box, gray-box, black-box, and no-box ranging from full understanding and complete access to the model and its defenses (white-box) to no knowledge and no direct access (no-box), with varying degrees of knowledge and access in between (gray-box, black-box)~\citep{papernot_practical_2017, bose_adversarial_2020}.

The white-box threat model has emerged as the most common evaluation scenario in the adversarial robustness literature, as it enables the strongest attacks, thereby providing the most accurate quantification of the robustness of a respective defense~\citep{papernot_practical_2017}.
This aligns with Kerckhoff's Principle, which asserts that a system's security should not depend on obscurity, i.e., the secrecy of its design or implementation. Relying on obscurity introduces vulnerabilities, as once that obscurity is compromised, the entire system's robustness is at risk~\citep{sasa_kerk_2008,athalye_obfuscated_2018}.
The widespread adoption of the white-box threat model has been enabled through the open sourcing of newly published defenses and models.

\textbf{Perturbation constraints.} Constraints on adversarial perturbations serve two essential roles in robustness assessment: First, they reflect practical limitations, such as maintaining valid input domains (e.g., pixel values within image bounds) or ensuring malicious perturbations remain undetected. Second, they provide meaningful evaluation settings, as unconstrained adversaries can typically bypass any defense mechanism, making such scenarios more suitable as sanity checks than realistic threat models~\cite{goodfellow_explaining_2015}.
Formally, perturbation constraints $\gC$ are typically defined by bounding a distance metric between the original and perturbed inputs, $d_{in}(x, x') \leq \epsilon$. This formalization enables a consistent framework for evaluating and comparing robustness under defined conditions \cite{carlini_evaluating_2019}.

\textbf{Computational effort \& complexity.} We extend the attack capability definition to include practicality constraints on attacks and defenses, such as computational effort or pipeline complexity. Practicality constraints enable more realistic modeling of real-world attacks by reflecting the costs associated with both attacks and defenses. Cybersecurity threat models often assume an inverse relationship between these costs: a higher cost for an attacker, typically means a lower cost for the defender~\cite{barreno_security_2010}.

% \subsection{Surface in Adversarial Robustness}~\label{sec:tax-surface}

% We define the surface in adversarial robustness as the specific stage within the system or model's pipeline where the threat is initiated. In the context of adversarial attacks on machine learning models, the attack surface can vary widely. It may include the input data that the model processes, possible preprocessing steps, the model's internal layers and parameters, the communication channels used to interact with the model, or even the deployment infrastructure~\citep{barreno_can_2006}. Note that the potential real-world impact of an attack is strongly influenced by the number of possible attack surfaces. The more attack surfaces a system presents, the greater the opportunity for attackers to exploit vulnerabilities. In the recent past, concerns about possibly real-world attack surfaces for attacks in the computer vision domain have been rather small~\citep{grosse_survey_2023}.

% \textbf{Computational effort \& complexity.} We extend the attack surface definition to include practicality constraints on attacks and defenses, such as computational effort or pipeline complexity. Practicality constraints enable more realistic modeling of real-world attacks by reflecting the costs associated with both attacks and defenses. Cybersecurity threat models often assume an inverse relationship between these costs: a higher cost for an attacker, such as high computational complexity, typically means a lower cost for the defender~\cite{barreno_security_2010}.

% Moreover, discouraging overly complex defense pipelines can enable more reliable evaluations. As described in \S~\ref{sec:tax-capability}, defense through obscurity facilitates defense mechanisms that are not secure and are often broken by stronger attacks in later evaluations~\cite{athalye_obfuscated_2018, tramer_adaptive_2020}. Similarly, some defenses primarily rely on raising the computational effort of an attacker. This can include adding complex preprocessing pipelines, which are expensive to evaluate or making the attack more difficult by adding randomness to the defense~\citep{guo_countering_2018, schwinn2022Improving}. Yet, prior work has shown that these approaches are often circumvented in future evaluations, where more efficient attacks are available~\citep{athalye_obfuscated_2018}. To prevent overestimation of robustness, it is generally recommended to make the evaluation of a defense as easy as possible (e.g., by keeping the computational effort to evaluate the defense as low as possible). 




% can also include all the different hyperparams settings (e.g., chat templates, number of generated tokens, model version number, ...)

