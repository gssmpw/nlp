\section{Introduction}

\leo{Title suggestions: 

Adversarial Alignment of LLMs Requires Less Complex and More Measureable Objectives

Adversarial Alignment of LLMs Requires Simpler and More Measurable Objectives

Adversarial Alignment of LLMs Requires Disentangled and More Measurable Objectives
}

\tom{more title suggestions:

unspecific:

A way forward for adversarial alignment 

How to approach adversarial alignment

}

\leo{Add proposal to the beginning of the paper that we will reevaluate the state of research in a year and that  other researchers are welcome to be included if they offer arguments for / against the position and how things changed}

%\tom{also what seems quite different is that the input space bound of the so-called toy problem (Lp norm balls) were mostly for semantic indifference. now we dont need any constraint on the input since a semantic change can be classified in the correct answer. so the input constraint can be moved to the output classification.}

Security risks in computer science have been a prevalent issue for decades~\cite{valiant_learning_1985, kearns_learning_1993}. %and with the ubiquitous adoption of computerized systems represent a significant practical concern. 
This ongoing challenge has resulted in an ``arms race" that includes the continued development of new attacks, such as malware and phishing, as well as defense mechanisms. %Here, an arms race refers to the ongoing competition among multiple entities, each trying to improve upon the other by employing progressively advanced techniques to either breach or safeguard a specific computer system. 

In the field of deep learning, \citet{szegedy_intriguing_2014} discovered that deep neural networks are highly susceptible to \textit{adversarial examples} -- input perturbations optimized to mislead models into making predictions that are erroneous or misaligned with their intended behavior. In response, countless defense strategies were proposed to safeguard neural networks against these attacks in the last decade. Yet, a majority of newly proposed heuristic defenses were eventually exposed as flawed by subsequent evaluations, often by using standard attack protocols that already existed at the time of the defense publication~\citep{tramer_adaptive_2020}. 

We investigate the research state of adversarial alignment in large language models (LLMs), which we define as the ability of an aligned model to maintain its intended training objective despite input perturbations. %
%\footnote{A model with poor alignment can still exhibit strong adversarial alignment if an attacker fails to manipulate its outputs through adversarial attacks.} 
We observe that adversarial alignment risks following the same cycle of flawed defenses and subsequent rectified evaluations seen in past adversarial robustness research but with considerably amplified challenges and stakes~\cite{hendrycks2022x, zou2023universal, schwinn2023adversarial}. 
%For instance, reliable evaluation is hindered by increased problem complexity, such as larger model scale as well as semantic and discrete input and output domains~\cite{andriushchenko2024jailbreaking, li2024llm}. 
Unlike previous robustness research that generally focused on well-defined problems like image classification~\cite{goodfellow_explaining_2015}, assessing LLM capabilities is considerably more challenging due to inherent ambiguities of the alignment problem and the complexity of natural language~\cite{wolf2023fundamental, andriushchenko2024jailbreaking, li2024llm}. Furthermore, the potential harm associated with LLMs is substantially greater due to their advanced capabilities and widespread availability~\cite{hendrycks2022x} in particular as they start being used as autonomous agents.  %This makes slow research progress and resulting misaligned and non-robust models a serious concern.

%\leo{Sophie/Cas/Leo: ML research is benchmark driven. We even advocate for it (leader boards clearly defined metrics than can be improved). But must fulfil certain criteria (e.g., the ones we advocate reproducibility, comparability, measureability). In the adversarial ML setting it also should be as easy to reliable evaluate as possible (e.g., no defense through obfuscation). Important to distangle the problem (ill defined measures of progress) and the thing we propose (well defined problem definitions, simpler problems etc.)}

%We argue that the past lack of progress can be largely attributed to misaligned incentives within the research community. 
%These incentives encouraged a narrow focus on improving benchmark numbers without sufficient attention to methodological rigor, reproducibility, and clear evaluation criteria. 

\begin{table*}[ht]
    \centering
    \caption{Non-exhaustive comparison of robustness research in previous domains and LLMs.}
     %\resizebox{1\textwidth}{!}{
     \newcommand{\cellwidth}{2.15cm}
     \newcommand{\linesp}{3pt}
     \newcommand{\cbox}[1]{\parbox[t][0.77cm][t]{\cellwidth}{\centering #1}}
    %\setlength{\arrayrulewidth}{10pt}
    \tiny	
\begin{tabular}{l*{3}{>{\centering\arraybackslash}m{\cellwidth}}*{3}{>{\centering\arraybackslash}m{\cellwidth}}}
    \addlinespace
     & \multicolumn{3}{@{\hspace{3mm}}c@{\hspace{3mm}}}{\cellcolor{customblue}\textcolor{white}{\textbf{Attack goals}}} & \multicolumn{3}{@{\hspace{3mm}}c@{\hspace{3mm}}}{\cellcolor{customblue}\textcolor{white}{\textbf{Attack Capabilities}}} \\
    \addlinespace[\linesp]
    \rowcolor{darkgray} & \textbf{Objectives} & \textbf{Attacks} & \textbf{Datasets} & \textbf{Access} & \textbf{Constraints} & \textbf{Frameworks} \\
    \rowcolor{lightgray}\textbf{Previous} & \cbox{Clear objectives\\(e.g., classification)\\\cite{szegedy_intriguing_2014}} & \cbox{Generally reliable\\\cite{croce2020reliable}} & \cbox{Standardized\\\cite{croce2020robustbench}} & \cbox{Generally white-box\\and open-source\\\cite{croce2020robustbench}} & \cbox{Tractable but incomplete\\(e.g., $\ell_p$)\\\cite{szegedy_intriguing_2014}} & \cbox{Standardized\\\cite{croce2020robustbench}} \\
    \addlinespace[\linesp]
    \rowcolor{lightgray}\textbf{LLM} & \cbox{Entangled objective of\\alignment \& robustness\\\cite{zou2023universal}} & \cbox{Currently weak\\\cite{li2024llm}} & \cbox{Entangled notions of\\harmfulness\\\cite{mazeika2024harmbench}} & \cbox{Often black-box and\\proprietary models\\\cite{zou2023universal}} & \cbox{No constraints\\\cite{chao2023jailbreaking,mazeika2024harmbench}} & \cbox{Varying evaluation settings\\\cite{mazeika2024harmbench}} \\
    %\bottomrule
\end{tabular}
    %}
    \label{tab:comparison}
\end{table*}

We argue that the past lack of progress can be largely attributed to a narrow focus on improving benchmark numbers without sufficient attention to rigorous evaluations and clear evaluation criteria. This led to the proliferation of ad-hoc defenses that relied on security through obscurity and ultimately proved ineffective, thus not providing a solid foundation for future work. In this context, the field of adversarial alignment risks ``treading water,'' expending significant effort but failing to make meaningful progress.  
%it repeats past mistakes with larger-scale consequences. 
%\,\,\textbf{1)} vague problem definitions, where alignment and robustness are inherently entangled in the adversarial alignment problem, \textbf{2)} complex and non-reproducible evaluations, and \textbf{3)}  an emphasis on state-of-the-art attack performance against proprietary models over reproducible and comparable open-source research.



\newpage
Our position on adversarial alignment in LLMs is as follows:

\begin{tcolorbox}[
    colback=white, colframe=customblue, coltitle=white, fonttitle=\bfseries, 
    rounded corners, enhanced, 
    title=Position, 
    attach boxed title to top left={yshift=-2mm, xshift=5mm}, 
    boxed title style={colback=customblue, rounded corners},
    boxsep=0.5mm
]
 %Current research incentives in LLMs:\,\,\textbf{1)} encourage vague problem definitions and complex, non-reproducible evaluations; \textbf{2)} promote a focus on SOTA performance on proprietary models over reproducible and comparable open-source research.
 We argue that researchers deal with:\,\,\textbf{1)} vague problem definitions, where alignment and robustness are inherently entangled in the adversarial alignment problem, \textbf{2)} complex and non-reproducible evaluations, and \textbf{3)}  an emphasis on state-of-the-art attack performance against proprietary models over reproducible and comparable open-source research.\\ 
 \textbf{Thus, meaningful progress in adversarial alignment for LLMs requires simpler, reproducible, and more measurable objectives.}
\end{tcolorbox}

Our main contributions are:


% entangled definitions of robustness in current benchmarks --> Simpler datasets with narrowly defined simple threat settings
% entangled definition of adeversarial alignment (alignment and robustness) require complex evaluation pipelines (e.g., judges) and can lead to ambigituies  --> Proxy objectives
% evals on proprietary models are incentivized to show real-world consequence of attack --> hard to reproduce / compare / measure --> open-source evals should be preferred in most settings and for other cases research focused APIs could improve reproducibility
% Community leaderboards and standatarized evals

%\renewcommand{\labelitemi}{--}
\begin{itemize}[noitemsep,nolistsep,topsep=-2pt,leftmargin=0.6cm] 
    \item We systematically identify challenges in previous robustness research, how they apply to LLMs, and how new challenges emerged. Based on this analysis: 
%\end{itemize}
%Based on this analysis:
%\begin{itemize}[noitemsep,nolistsep,topsep=-2pt,leftmargin=0.6cm] 
    \item We demonstrate how adversarial alignment intertwines the challenges of alignment and robustness, making robustness evaluation difficult, as it inherits the challenges of measuring alignment, such as ambiguity in success criteria. 
    %We propose addressing this by focusing on simpler sub-problems with measurable objectives.
    While acknowledging the importance of comprehensive assessment, we advocate for a complementary approach: focusing on measurable sub-problems with more measureable objectives, where not every work aims to address the complete picture.
    
    \item Towards the same goal of improving measurability, we propose simplifying robustness benchmarks by evaluating specific types of harm individually rather than combining complex concerns like copyright infringement, fairness, and toxicity into a single evaluation.
    
    \item We emphasize the need for academia to prioritize reproducible research over chasing SOTA performance on proprietary models. In this context, we propose fostering open-source research with accessible models. %and standardized evaluation frameworks. 
    %to ensure long-term progress in robustness.  
    \item Computational overhead, vast number of hyperparameters, and varying implementation details hinder comparability between different works. We advocate for a practical approach to improve reproducibility and comparability: community-driven leaderboards and standardized benchmarks to encourage best practices in adversarial robustness research.  

    %\item We identify two key issues in defining and measuring robustness in LLMs: vague robustness definitions based on semantic outputs and unreliable robustness estimates due to weak attacks.  
    %\item We highlight how entangled problem definitions and complex evaluation pipelines reduce reproducibility, comparability, and measurability in robustness research. 
    %\item We propose simplifying robustness benchmarks by using narrowly defined, well-specified threat settings, improving both measurability and accessibility.  
    %\item Simplify robustness benchmarks by using narrowly defined, well-specified threat settings to improve both measurability and accessibility.
    %\item Employ proxy objectives for attack evaluation to disentangle attack optimization from alignment evaluation, enabling direct algorithmic comparisons and simpler evaluation pipelines.
    %\item Prioritize transparent and reproducible evaluations by focusing on open-source models and research-focused APIs to reduce reliance on proprietary models.
    %\item Promote community-driven leaderboards and standardized benchmarks to improve comparability and encourage best practices in adversarial robustness research.
\end{itemize}

%\item We advocate for evaluating attack effectiveness using predefined optimization objectives rather than relying on subjective LLM-judged outputs, increasing methodological transparency.  

%Our goal is to highlight that current research risks repeating past patterns. We illustrate that an impending arms race entails considerable challenges that, if not addressed by the community, could result in misaligned incentives that hinder research efforts, slow down the adversarial alignment of LLMs, and have severe real-world consequences. 

%We adopt a cybersecurity taxonomy, encompassing robustness goals, and attack capabilities, to analyze the evolving threat landscape for large language models (LLMs). This analysis reveals how the increasing complexity of LLMs, coupled with these misaligned incentives, risks repeating past failures. We specifically highlight how a disproportionate focus on narrow attack vectors and easily achievable defense benchmarks has hindered the development of robust LLMs. To address these challenges and enable meaningful progress in adversarial alignment, we advocate for a fundamental shift in research incentives. We posit that academia, by concentrating on foundational research, pursuing unique ideas, and generating novel insights while upholding the core principles of reproducibility, comparability, and rigorous measurability, can play a pivotal role in this transformation. This requires a concerted effort to reduce problem complexity, such as disentangling robustness from other concerns, to create a research environment that fosters genuine advancements in LLM robustness.

% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/f1-cropped.pdf}
%     \caption{Preliminary - illustrate paper structure}
%     \label{fig:structure}
% \end{figure*}

\section{Structure of Our Argument}\label{sec:structure}

We structure our position as follows: We define a taxonomy of adversarial robustness threats based on common cybersecurity frameworks including \textbf{I)} robustness goals and \textbf{II)} adversary capabilities. For both elements of this taxonomy, we follow a parallel argument structure: We \textbf{A)} define past and present threat models (\S\ref{sec:pos-goals-definition} and \S\ref{sec:pos-capability-definition}), \textbf{B)} discuss historical challenges and connections to upcoming and exacerbated issues in the LLM domain (\S\ref{sec:pos-goals-challenges}, and \S\ref{sec:pos-capability-challenges}), and \textbf{C)} explore the applicability of past insights to new problems and how current research objectives can be realigned to promote measurable progress (\S\ref{sec:pos-goals-realigned} and \S\ref{sec:pos-capability-realigned}). We provide a non-exhaustive comparison between past and current robustness research in Table~\ref{tab:comparison}.% in this field. 

%We propose that new objectives should facilitate core academic values including reproducibility (amount of effort needed to reproduce an experiment), comparability (even reproducible experiments may be difficult to compare across works), and measurability (metrics should be quantitative and robust to design choices). We argue that such objectives reinforce academia’s strengths: exploring unique foundational ideas while maintaining methodological rigor. Unlike industry, which excels at scaling, academia, constrained by resources, is uniquely positioned to generate reliable insights through open, transparent, and reproducible research. Moreover, its independent incentive structure -- akin to white-hat hackers in cybersecurity -- enables rigorous testing of AI systems without being tied to product-driven and comercial incentives.
%2) These incentives should reward academia’s unique role in generating unconventional ideas. By enabling independent researchers to explore diverse directions, we build an ``idea pipeline'' for industry labs to evaluate and scale. This creates an environment where academia’s creativity meets industry’s capacity for large-scale development, driving scientific and technological progress forward.

\section{Related Positions and Future Outlook} 

\textbf{Concurrent positions.} Our findings are reinforced by the concurrent work of~\citet{rando2025adversarial}, who independently identify similar fundamental challenges in adversarial alignment of LLMs. 
Their research offers detailed case studies of specific threat models like jailbreaks, poisoning, and unlearning. While we investigate specific failures as well--albeit in less detail--, our focus is on developing a high-level taxonomy and proposing forward-looking solutions to emerging challenges. 
%Their paper offers detailed case studies of specific threat models like jailbreaks, poisoning, and unlearning, whereas our work develops a high-level taxonomy and focuses more on forward-looking solutions. 
Together, both works offer complementary perspectives toward a more comprehensive understanding of the identified issues.

\textbf{Call for positions.} We will revisit and evaluate the arguments presented in this position paper one year or more from its publication. This retrospective analysis will examine whether our identified challenges and proposed solutions have proven relevant regarding the field's progress in adversarial robustness for LLMs. We warmly welcome other researchers to contribute their perspectives and collaborate on the retrospective, whether they want to support or challenge the positions outlined in the remainder of this paper. Through this collaborative reflection, we aim to maintain accountability for our arguments.

%The concurrent position paper by \citet{rando2025adversarial} emphasizes the importance of properly tackling adversarial alignment. While our work provides a high-level taxonomy and focuses more on forward-looking solutions, \citet{rando2025adversarial} perform a more detailed investigation of specific threat models like jailbreaks, poisoning, and unlearning.


%\textbf{Concurrent Position.} Concurrent to our work,~\citet{rando2025adversarial} take a similar position and argue that adversarial alignment in LLMs studies problems that are vaguely defined, more difficult to evaluate, and likely harder to solve. They also emphasize the need for caution to ensure these challenges do not impede overall research progress in the field.

%Case studies \\
%High-level forward looking