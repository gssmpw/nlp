\section{Taxonomy}

Early works in machine learning security established a taxonomy to categorize threats according to the adversary's goal, perturbation set, capability, and attack surface~\citep{barreno_can_2006, barreno_security_2010, papernot_practical_2017, carlini_evaluating_2019}.

\subsection{Goal}~\label{sec:tax-evasion} 

The goals of an adversarial attack are often categorized by the cyber security model \emph{CIA}, into confidentiality, integrity, and availability~\citep{barreno_can_2006}. In this framework, attacks on confidentiality try to extract information about the model including its structure, parameters, and training data. Integrity attacks aim to provoke a predefined output behavior (i.e., making a computer vision model classify an image as a specific class). Lastly, attacks on availability aim to prevent the owner of a model from accessing the correct model outputs and features. Clearly defining the attack goal is essential to derive clear adversarial threat models, which are needed to compare defense evaluations across multiple works. In this work, we mostly discuss attacks on model integrity, focusing on evasion~\cite{} and poisoning attacks~\cite{}. 

\textbf{Evasion robustness.} In general, evasion robustness can be expressed as a constrained optimization problem, where we are interested in the maximum output change under a constrained change of the input:

\begin{equation}\label{eq:adv-goal-evasion}
    \begin{aligned}
    \delta^*(x) = \max_{x' \in \gC(x)} \; & d_{out} (f(x), f(x')),  \\ 
    \end{aligned}
\end{equation}

where $x$ is the input datapoint, $f$ is the model, $d_{out}$ is a distance metric between the model outputs, and $\gC(x)$ is the set of all possible perturbations of $x$ that are within a certain distance of $x$. %The distance metric $d_{out}$ can be defined in various ways, depending on the model's output space. For example, in the computer vision domain, $d_{out}$ is often defined as the $\ell_p$ distance between the model's output logits.

\textbf{Poisoning robustness.} TODO

\begin{equation}
    \begin{aligned}
    \delta^*(D) = \min_{f, \tilde{D} \in \gC(D)} \; & L_{rob} (f, \tilde{D}) 
    \end{aligned}
\end{equation}

\subsection{Capability}~\label{sec:tax-capability} We define the capability of an adversary as the knowledge and access capability of an attacker and possible constraints imposed on an attack, e.g., perturbation budgets~\cite{carlini_evaluating_2019}. 

\textbf{Attacker knowledge.} Knowledge and access classifications typically includes up to four categories, consisting of white-box, gray-box, black-box, and no-box attacks ranging from full understanding and complete access to the model and its defenses (white-box) to no knowledge and no direct access (no-box), with varying degrees of knowledge and access in between (gray-box, black-box)~\citep{papernot_practical_2017, bose_adversarial_2020}.

The white-box threat model has been the most common evaluation scenario in the literature, as the strongest attacks can be performed in this threat model, thereby providing the most accurate quantification of the robustness of a respective defense~\citep{papernot_practical_2017}. The widespread adoption of the white-box threat model has been enabled through the open-sourcing of newly published defenses, which allows other researchers to attack the respective models end-to-end. The establishment of the white-box threat model as a golden standard is connected to its alignment with Kerckhoff’s Principle, a fundamental concept of modern cryptography. Kerckhoffs’ Principle asserts that a system's security should not depend on obscurity, i.e., the secrecy of its design or implementation. Relying on obscurity introduces vulnerabilities, as once that obscurity is compromised, the entire system's robustness is at risk~\citep{sasa_kerk_2008,athalye_obfuscated_2018}.

\textbf{Attack constraints.} Adversarial attacks are typically constrained by a perturbation budget for two main purposes: 1) Practical attacks often operate under constraints such as limited access to the model or the need to remain imperceptible. 2) Unconstrained attacks can generally circumvent any defense, making them more suitable for sanity checks than as realistic threat models~\cite{}. Attack constraints can be defined by bounding a distance metric between the original and adversarial inputs and apply to both evasion and poisoning threat models:
$$d_{in}(x, x') \leq \epsilon.$$

\subsection{Surface}~\label{sec:tax-surface}

We define the attack surface of an adversary as the specific stage within the system or model's pipeline where the attack is initiated. In the context of adversarial attacks on machine learning models, the attack surface can vary widely. It may include the input data that the model processes, possible preprocessing steps, the model's internal layers and parameters, the communication channels used to interact with the model, or even the deployment infrastructure~\citep{barreno_can_2006}. Note that the potential real-world impact of an attack is strongly influenced by the number of possible attack surfaces. The more attack surfaces a system presents, the greater the opportunity for attackers to exploit vulnerabilities. In the recent past, concerns about possibly real-world attack surfaces for attacks in the computer vision domain have been rather small~\citep{grosse_survey_2023}.

\textbf{Computational effort \& complexity.} We extend the attack surface definition to include practicality constraints on attacks and defenses, such as computational effort or pipeline complexity. Practicality constraints enable more realistic modeling of real-world attacks by reflecting the costs associated with both attacks and defenses. Cybersecurity threat models often assume an inverse relationship between these costs: a higher cost for an attacker, such as high computational complexity, typically means a lower cost for the defender~\cite{barreno_security_2010}.

Moreover, discouraging overly complex defense pipelines can enable more reliable evaluations. As described in \S~\ref{sec:tax-capability}, defense through obscurity facilitates defense mechanisms that are not secure and are often broken by stronger attacks in later evaluations~\cite{athalye_obfuscated_2018, tramer_adaptive_2020}. Similarly, some defenses primarily rely on raising the computational effort of an attacker. This can include adding complex preprocessing pipelines, which are expensive to evaluate or making the attack more difficult by adding randomness to the defense~\citep{guo_countering_2018, schwinn2022Improving}. Yet, prior work has shown that these approaches are often circumvented in future evaluations, where more efficient attacks are available~\citep{athalye_obfuscated_2018}. To prevent overestimation of robustness, it is generally recommended to make the evaluation of a defense as easy as possible (e.g., by keeping the computational effort to evaluate the defense as low as possible). 




% can also include all the different hyperparams settings (e.g., chat templates, number of generated tokens, model version number, ...)

