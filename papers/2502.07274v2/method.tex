


  \begin{wrapfigure}{L}{0.44\textwidth}
    \begin{minipage}{0.44\textwidth}
    \input{algorithm/algorithm}

    \end{minipage}
  \end{wrapfigure}

We introduce a simple but cost-effective method that leverages weight-space operations (\textit{e.g.}, weight-reset, weight-average) for continual learning with sufficient exemplar memory. 
The grounding idea of our approach is that the sufficient exemplar memory regime allows the model to obtain an effective minimizer of all previous tasks (discussed in \Cref{sec:theory}). Deriving from this, at the beginning of training each  $t^{th}$ task, we assume that we already have a good minimizer of the previous tasks, and encourage the model to stay in its proximity in the weight-space. 

\Cref{alg:ours} shows pseudo-code of our method. Prior to the training of the $t^{th}$ task, the model parameters (\textit{i.e.}, minimizer of previous $1:t-1$ tasks) are stored as $\theta_{\textrm{prev}}$ (see Line 6 in \Cref{alg:ours}). 
% Then, we begin training on the current $t^{th}$ task using standard empirical risk minimization (\textit{e.g.}, the cross-entropy loss for classification tasks). 
Then, we begin training on the current $t^{th}$ task using the loss $\ell(\theta;x,y)$ (see Line 8). 
After $n_{\textrm{warm}}$ warming epochs, we apply a rank-based weight resetting procedure to reset the parameters that were dormant during training (see Line 11). We rank the dormant parameters by comparing the current value in $\theta$ to the value of the previous task model $\theta_{\textrm{prev}}$. We tested multiple metrics to quantify the dormancy and found that a simple distance $\big|\theta[l] - \theta_{\textrm{prev}}[l]\big|$ is sufficient to measure the drift for the $l^{th}$ parameter. Please refer to \cref{appendix:drift-measures}. Specifically, we reset 80\% of the parameters, following empirical results of \citet{yadav2024ties} (see Lines 12-14). Formally, the $l^{th}$ parameter of $\theta$ is reset using: 
\begin{equation}\label{eq:reset-mix}
    \theta[l] = \alpha\cdot\theta_{\mathrm{prev}}[l] +  (1-\alpha)\cdot\theta[l], 
\end{equation}
%
where $\alpha=0.5$ by default.  This step is intended to reset parameters that did not contribute to learning the current task $t$, allowing the model $\theta$ to remain in proximity to the previous task model $\theta_{\textrm{prev}}$. 
Assuming the reset model is a good (if not optimal) minimizer of the current task $t$, and also retaining the parameters of $\theta_{\textrm{prev}}$, we apply a periodic weight-averaging over the learning trajectory (see Lines 16-17) \citep{izmailov2018averaging}. The weight-averaging procedure is intended to accelerate convergence and improve generalization by aggregating diverse learning signals. We find this particularly beneficial under sufficient exemplar memory settings, where the variance of data is significant. Please refer to \Cref{appendix:method} for a detailed elaboration of our method.

%we design a new, simple solution to address this problem. We address both (1) Inter-task forgetting: We do this by resetting the current task model's dormant params (hence not critical in learning this new task) to the previous task model, to distinguish the most important parameters that are critical in this task, we train the model in the most sensitive direction (using sharpness aware minimization) (2) Intra-task forgetting: Merge different signals obtained during the same task to prevent forgetting. Which outperforms all competitors with significantly lower cost (1/3 ~ 1/2).


%\input{algorithm/algorithm}
% Formally: $\Theta \leftarrow (\Theta\cdot n_{\textrm{avg}} + \theta) / (n_{\textrm{avg}} +1),$ where $\Theta$ denotes the weight-averaged model, $n_{\textrm{avg}}$ the number of models averaged so far. 

% In the following section, we demonstrate the cost-efficiency of our method in building robust continual learners.

%In \cref{appendix:why}, we elaborate on why weight space consolidation is effective, and provide a detailed analysis.


%This step aims to address the issues associated with the challenges in learning an optimal solution of the hybrid dataset $D_t \cup \mathcal{M}_{1:t-1}$. 

%Specifically, we perform weight-space operations to reconcile model parameters derived from current task data and replayed samples. This approach minimizes inter-task forgetting by ensuring that the combined parameter updates remain close to the joint minimizer, while also addressing challenges such as gradient interference and dataset heterogeneity.


\begin{comment}
\paragraph{Step 1} We begin training on the current $t^{th}$ task using an approximated version \citep{liu2022towards} of the sharpness-aware minimization \citep{foret2020sharpness}. Specifically, the model parameters $\theta$ is updated using:
\begin{equation}
    \gL(\theta) = \max_{\|\epsilon\|_p \le \rho} \gL\bigl(w+\epsilon\bigr), 
    \label{eq:1}
\end{equation}
where $p \ge 0$ and \(\rho\) is the radius of the \(\ell_p\) ball. We then add a gradient ascent step that finds the optimal update direction that is sensitive to pertubations in the weight space:
\begin{equation}
    \hat{\epsilon}(\theta) = \frac{\rho\,\nabla_{\theta} \gL(\theta)}{\|\nabla_{\theta} \gL(\theta)\|} \approx \arg\max_{\|\epsilon\|_p \le \rho} \gL(w+\epsilon).
    \label {eq:2}
\end{equation}

The gradient w.r.t the perturbation $(\theta+\hat{\epsilon}(\theta)$ is used for the update:
\begin{equation}
    \nabla_{\theta} \gL(\theta) \approx \nabla_{\theta} \gL(\theta)\big|_{\theta+\hat{\epsilon}(\theta)}.
    \label{eq:3}
\end{equation}
In reality, we approximate \cref{eq:1}, the details are illustrated in \cref{appendix:method}. The purpose behind training with \cref{eq:1} is to update parameters that are the most responsive in learning the new task \citep{andriushchenko2023sharpness}. 
    
\end{comment}
