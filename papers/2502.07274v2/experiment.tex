
% In this section, we investigate the following questions:
% (1) How do existing continual learning algorithms work under sufficient memory?
% (2) What happens to a model's weight space when it forgets/learns a new task?
% (3) How does our method perform under sufficient memory?

% \subsection{Experimental Setup}
% \noindent\textbf{Datasets } \ \
% We test our algorithm on two standard class-incremental learning benchmarks~\citep{masana2022class}. \textbf{CIFAR100} is an image classification dataset consisting of $100$ classes, which are split into 10 sequential tasks (\textit{e.g.}, classes per task ) to simulate a continual learning setting. \textbf{ImageNet100} is another classification dataset consisting of $100$ classes, which are divided into sequential tasks (\textit{e.g.}, $10$ classes per task). 

% \noindent\textbf{Baselines} \ \
% In our experiments, we compare $6$ baseline algorithms. BiC [\citenum{bic}] introduces a bias correction and logit scaling to mitigate class imbalance in incremental phases, while DER [\citenum{der}] combines logit-based distillation with stored samples to preserve past task relationships. FOSTER [\citenum{foster}] uses gradient boosting-inspired feature expansion followed by network compression to balance plasticity and stability, iCaRL [\citenum{icarl}] employs exemplar rehearsal with herding-based selection and a nearest-mean classifier to retain old-class representations. Lastly, Replay is the standard baseline that uses replay memory.

\noindent\textbf{Experimental settings} \ \
We evaluate our algorithm on two standard class-incremental learning (class-IL) benchmarks~\citep{masana2022class} using the PyCIL framework~\citep{zhou2023pycil}.
\textbf{CIFAR100} is an image classification dataset with 100 classes, which is split into 10 sequential tasks, each containing 10 classes. \textbf{ImageNet100} is another classification dataset with 100 classes, which is also split into 10 sequential tasks, each containing 10 classes. 
We compare our approach with six exemplar-based baselines, including iCaRL~\citep{icarl}, BiC~\citep{bic}, WA~\citep{wa}, DER~\citep{der}, and FOSTER~\citep{foster}. Additionally, we include Replay, a naive baseline that finetunes the model using both the current dataset and stored exemplars.
All experiments use ResNet-32 for CIFAR-100 and ResNet-18 for ImageNet-100 as the backbone ~\citep{he2016deep}. We conduct experiments across five random seeds and report the final class-IL accuracy, computed as the average classification accuracy across all tasks on the validation datasets after training on the final task~\citep{masana2022class}. Experimental details are reported in \Cref{appendix:experiment-details}

% \noindent\textbf{Baselines} \ \
% In our experiments, we compare $6$ baseline algorithms. BiC [\citenum{bic}] introduces a bias correction and logit scaling to mitigate class imbalance in incremental phases, while DER [\citenum{der}] combines logit-based distillation with stored samples to preserve past task relationships. FOSTER [\citenum{foster}] uses gradient boosting-inspired feature expansion followed by network compression to balance plasticity and stability, iCaRL [\citenum{icarl}] employs exemplar rehearsal with herding-based selection and a nearest-mean classifier to retain old-class representations. Lastly, Replay is the standard baseline that uses replay memory.


% \subsection{Experimental results}

\begin{table}[h]
\vspace{-0.1in}
  \centering
  \caption{Average class-IL accuracy (Acc.) with different exemplar memory sizes.
We report experimental results with varying memory sizes, ranging from 20 exemplars per class (a common setting in class-IL) to 500 exemplars per class (storing the entire previous dataset) from previous tasks. The \textbf{bold} indicates the best performance among the non-expansion-based methods.}
\vspace{-0.1in}
  \label{tab:combined}
  % First subtable for CIFAR-100
  \begin{subtable}{0.48\textwidth}\label{tab:cifar100}
    \centering
    \caption{Acc. on CIFAR-100}
\vspace{-0.05in}
    \begin{adjustbox}{width=\linewidth}
      \input{tables/cifar100}
    \end{adjustbox}
  \end{subtable}
  \hfill
  % Second subtable for ImageNet-100
  \begin{subtable}{0.48\textwidth}\label{tab:imagenet100}
    \centering
    \caption{Acc. on ImageNet-100}
\vspace{-0.05in}
    \begin{adjustbox}{width=\linewidth}
      \input{tables/imagenet100}
    \end{adjustbox}
  \end{subtable}
\vspace{-0.1in}
\end{table}

\begin{figure}[htbp]
\vspace{-0.1in}
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/imagenet100/time_spent_combined.pdf}
        \vspace{-0.25in}
        \caption{Training time spent on ImageNet-100}
        \label{fig:time_ours_imagenet}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/ablation.pdf}
        \vspace{-0.25in}
        \caption{Ablation Study on CIFAR-100}
        \label{fig:ablation}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Training cost analysis and ablation study. We report experimental results with different exemplar memory sizes, ranging from 20 exemplars per class (a common setting in class-IL) to 500 exemplars per class (storing the entire previous dataset) from previous tasks.}
    \label{fig:mixed}
    \vspace{-0.15in}
\end{figure}

\begin{comment}
\begin{wrapfigure}{r}{0.48\textwidth}
  \vspace{-5mm}
  \centering
  \includegraphics[width=\linewidth]{images/imagenet100/time_spent_combined.pdf}
  \caption{Training time spent on ImageNet-100.}
  \label{fig:time_ours_imagenet}
\end{wrapfigure}%    
\end{comment}


% \noindent\textbf{Main Results} \ \
% In \Cref{tab:cifar100,tab:imagenet100}, we report experimental results using the average class-IL accuracy on all tasks. For a fair comparison, we distinguished expansion-based algorithms (\textit{e.g.}, DER, FOSTER) that adjust the model architectures from non-expanding algorithms. In general, we can see two trends: First, as a memory size increases, the average accuracy also enhances. Markedly, under memory-sufficient conditions (\textit{i.e.}, when the memory size reaches 100 ($20\%$ of the entire data)), both Replay and Ours outperform non-expanding algorithms, while eventually outcompeting expanding algorithms as well. Second, the computation cost (measured as the training time) increases alongside the memory size. Notably, the expansion-based algorithms (\textit{e.g.}, FOSTER) required very long training time owing to their architectural adjustment step. Conversely, our weight space consolidation method, achieves robust continual learning performance (average accuracy) across both benchmarks, at roughly \textit{one-fifth} of its rival's training time (see \cref{fig:time_ours_cifar100,fig:time_ours_imagenet}), close to the least expensive replay method. We further report the results of an ablation study, where each component in our method showed no redundancy, the strongest when combined altogether (see \cref{fig:ablation,tab:ablation}). 



\noindent\textbf{Experimental results} \ \
\Cref{fig:mixed} (and \Cref{fig:acc_ours_cifar100}) presents experimental results on the CIFAR-100 and ImageNet-100 datasets in a 10 task scenario, evaluating various exemplar memory sizes. To ensure a fair comparison, we distinguish expansion-based methods, such as DER and FOSTER, from other algorithms, as these methods increase the model size during class-IL. Based on these results, we make the following key observations: First, in the constrained exemplar memory setting typically considered (\textit{e.g.}, 20 exemplars per class), existing algorithms outperform the naive Replay approach. However, as the number of exemplars per class increases, the performance gap between these algorithms and Replay narrows significantly. Notably, when the exemplar count per class reaches 100 (\textit{i.e.}, using sufficient exemplar memory), Replay achieves competitive performance compared to other methods. Second, \Cref{fig:time_ours_cifar100} and \Cref{fig:time_ours_imagenet}, which compare training costs (measured by training time), show that as exemplar memory size increases, the training cost also rises. In particular, expansion-based methods achieve superior performance but require 2-3 times the training cost of Replay. In contrast, note that our weight space consolidation method achieves superior performance across both benchmarks, nearly matching the training cost of the simplest Replay method. We also report the results of an ablation study, which demonstrates that each component of our method contributes uniquely and effectively. When combined, these components yield the strongest performance (see Figures \ref{fig:ablation}). We provide more detailed analyses of (1) the ablation study and (2) the effect of weight space operations on continual learning in \Cref{appendix:analysis}.




%FOSTER remains strong at both benchmarks across all memory settings, but with a very high computation cost . Compared to previous methods, 

%the ranking of different methods by average accuracy undergoes significant changes (e.g., Replay ranking $7^{th} \rightarrow 2^{nd}$ ). 
% \noindent\textbf{Analysis} \ \





\begin{comment}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/imagenet100/average_accuracy_cnn_combined.pdf}
        \caption{}
        \label{fig:acc_ours_imagenet}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/imagenet100/time_spent_combined.pdf}
        \caption{}
        \label{fig:time_ours_imagenet}
    \end{subfigure}
    \caption{Comparison of (a) average continual-learning accuracy and (b) training time spent across various memory sizes on ImageNet-100.}
    \label{fig:imagenet}
\end{figure}    
\end{comment}


