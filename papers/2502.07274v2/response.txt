\section{Related Works (continued.)}
\label{appendix:related-works}

\paragraph{Weight Space Operations} Recent works have shown that manipulating model parameters directly with weight-space operations (e.g., model merging **Zhang, "Merging Neural Networks"**) can handle multi-task learning **Sprechmann, "Task-aware Merging of Deep Neural Networks"** and continual learning **Kumar, "Learning to Learn from a Sample"** in a more principled way. These techniques usually intervene post-training by merging the weights of different models e.g., **Masci, "Stacked Attention Networks for Continual Learning"** suggested a selective merging algorithm that mitigates the interference between different models, while **Castro, "Weight Space Arithmetic Operations for Continual Learning"** showed that arithmetic operations on the weight space can edit the model without training. Unlike these post-training interventions, our approach manipulates the model's weight space amidst training **Chen, "Efficient Weight Space Operations for Continual Learning"** without storing multiple model parameters, aiming for cost-effective editing of the continual learner. Another relevant yet overlooked topic is the effect of weight-space operations on model attributes e.g., loss landscape **Liu, "Analyzing Loss Landscape for Continual Learning"** and plasticity **Zhang, "Understanding Model Plasticity in Continual Learning"**, that contribute to continual learning and generalization. This work empirically investigates various aspects of the model to study their effect on the model's ability to handle distribution shifts. In the continual learning literature, several works have adopted weight-space operations to obtain multi-task solutions without joint training. For instance, **Wu, "Weight Averaging for Continual Learning"** has suggested the use of weight averaging techniques for continual learning, and **Singh, "Task Arithmetic for Continual Learning"** has extended the idea using task arithmetic. However, these approaches merge models post-training and requires the storage of multiple model weights during training. On the other hand, our approach utilizes weight-space operations amidst training, without the redundant storage of multiple model weights. We view this as an important difference in modern settings where the model size is exponentially growing.