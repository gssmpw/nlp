\noindent\textbf{Continual learning}  \ \
% Continual learning (CL) focuses on training models to adapt to a sequence of tasks without forgetting prior knowledge (\textit{i.e.}, catastrophic forgetting)~\citep{wang2024comprehensive}. At the heart of catastrophic forgetting is the \emph{stability-plasticity dilemma} \citep{(spdilemma)carpenter87, (spdilemma)mermillod13}, where a model exhibits high stability on previously trained tasks, but suffers from low plasticity for the integration of new knowledge (and vice-versa). 
Continual learning (CL) has been actively studied in various scenarios and methodological categories. Among the three scenarios of CL~\citep{van2019three}, class-incremental learning (class-IL) has been considered the most challenging and has been the most actively studied scenario~\citep{masana2022class}. CL algorithms (including class-IL) can be categorized into regularization-based approaches, which penalize changes to important parameters for past tasks~\citep{kirkpatrick2017overcoming, (mas)aljundi2018memory, cha2020cpr}, rehearsal-based approaches, which store and replay exemplars from past tasks~\citep{icarl, (tbbn)cha2023rebalancing}, and expansion-based approaches, which expand the model's capacity to balance the trade-off between stability and plasticity~\citep{der, foster}.
Additional approaches focus on addressing classifier bias toward recent tasks while using the exemplars~\citep{bic,wa}.
While exemplar-based methods have demonstrated state-of-the-art performance, they typically rely on strict memory constraints, often limiting memory size to a small percentage of the dataset. Recent studies challenge the necessity of these strict memory constraints, highlighting that the computational cost of maintaining and processing memory—especially GPU usage—can far outweigh storage costs~\citep{prabhu2023computationally, chavan2023towards,Harun_2023_CVPR}. This shift in perspective opens the door to relaxing memory limits in order to reduce training costs, which is the focus of our work.

\noindent\textbf{Weight space operations} Weight space operations directly manipulate the model parameters for various goals e.g., multi-task learning \citep{yu2024language} and continual learning \citep{marouf2025weighted,marczak2025magmax}. For instance, weight averaging is utilized to merge the knowledge of multiple models \citep{ilharco2022editing,jang2025model}, while weight reset is used to enhance plasticity \citep{dohare2024loss} and improve parameter-redundancy \citep{yadav2024ties}. For an overview of weight-space operations, please refer to \Cref{appendix:related-works}.


% However, many of these methods operate under strict memory constraints and overlook the ever-growing computation cost in , providing limited scalability. On the other hand, we relaxes this memory constraint, while adding a more realistic constraint on the computational cost of the algorithm \citep{prabhu2023computationally} to achieve scalable continual learning.


% To address forgetting between different tasks. For instance, regularization methods such as EWC \citep{kirkpatrick2017overcoming} penalize changes to parameters deemed important for previous tasks, while rehearsal-based methods such as iCaRL \citep{icarl} and DER \citep{der} retain exemplars to preserve past knowledge. Additional techniques focus on addressing classifier bias toward recent tasks \citep{bic,wa}. However, many of these methods operate under strict memory constraints and overlook the ever-growing computation cost in larger models, providing limited scalability. On the other hand, this paper relaxes the memory constraint, while adding a more realistic constraint on the computational cost of the algorithm \citep{prabhu2023computationally} to achieve scalable continual learning.

%In contrast, \citet{prabhu2023computationally} relaxed the memory constraint and incorporated explicit computational cost limits. Our work builds on this direction by balancing memory usage with computational efficiency to achieve scalable continual learning.


%The task of continuous learning aims at training models in a sequence of tasks without forgetting earlier tasks \citep{kirkpatrick2017overcoming}. To address forgetting between different tasks (i.e inter-task forgetting), existing studies seek to retain the knowledge of previous tasks using a small memory buffer \citep{bic,der,foster,icarl,wa}. However, many of these methods operate under strict memory constraints and overlook the ever-growing computation cost in larger models. As a result, they provide limited scalability and do not address the broader challenge of keeping the operational cost manageable. On the other hand, this paper relaxes the memory constraint, while adding a more realistic constraint on the computational cost of the algorithm \citep{prabhu2023computationally}. 

%In the continual learning literature, previous works have predominantly focused on mitigating catastrophic forgetting through memory-constrained strategies. For instance, regularization methods such as EWC \citep{kirkpatrick2017overcoming} penalize changes to parameters deemed important for previous tasks, while rehearsal-based methods like iCaRL \citep{icarl} and DER \citep{der} retain exemplars to preserve past knowledge. Additional techniques focus on addressing classifier bias toward recent tasks \citep{bic,wa}. In contrast, \citet{prabhu2023computationally} relaxes the memory constraint and incorporate explicit computational cost limits. Our work builds on this direction by balancing memory usage with computational efficiency to achieve scalable continual learning.



























%paragraph: continual learning
%cite \cite{bic,der,foster,icarl,wa}
%also mention their limitations (unaware of cost, only designed for low memory, stability)

%paragraph: weight space learning
%show recent advances in weight space learning (e.g., model merging). especially those that handle multi-task settings. Unlike these post-training intervention methods, we consider a mid-training intervention algorithm that removes the need for storing multiple model weights.

%paragraph: sharpness and loss landscapes
%explain sharpness %\cite{kaur2023maximum}

% \paragraph{Continual learning} The task of continuous learning aims at training models in a sequence of tasks without forgetting earlier tasks~\cite{wang2024comprehensive}. To address forgetting between different tasks. For instance, regularization methods such as EWC \citep{kirkpatrick2017overcoming} penalize changes to parameters deemed important for previous tasks, while rehearsal-based methods such as iCaRL \citep{icarl} and DER \citep{der} retain exemplars to preserve past knowledge. Additional techniques focus on addressing classifier bias toward recent tasks \citep{bic,wa}. However, many of these methods operate under strict memory constraints and overlook the ever-growing computation cost in larger models, providing limited scalability. On the other hand, this paper relaxes the memory constraint, while adding a more realistic constraint on the computational cost of the algorithm \citep{prabhu2023computationally} to achieve scalable continual learning.

% \paragraph{Continual learning} Continual learning (CL) aims to train a model for a given sequence of tasks while overcoming catastrophic forgetting of previous tasks~\citep{wang2024comprehensive}. 
% Among three scenarios of CL, class-incremental learning (class-IL) has been considered as the most challenging scenario, being most actively studied until now~\citep{masana2022class}. To overcome the catastrophic forgetting, regularization-based methods penalize changes to parameters deemed important for previous tasks~\citep{kirkpatrick2017overcoming, (mas)aljundi2018memory, cha2020cpr}, while rehearsal-based methods retain exemplars to preserve past knowledge~\citep{icarl,der,foster,(tbbn)cha2023rebalancing}. Additional techniques focus on addressing classifier bias toward recent tasks while using the exemplars~\citep{bic,wa}.
% Note that these exemplar-based methods have reported state-of-the-art performance in class-IL with a constraint exemplar memory size (\textit{e.g.}, less than 4\% of the entire dataset).
