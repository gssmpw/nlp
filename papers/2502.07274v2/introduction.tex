% Continual learning (CL) has attracted significant attention as a paradigm enabling machine learning models to adapt to sequential tasks without forgetting previously acquired knowledge~\citep{wang2024comprehensive}. 
% Many existing approaches utilize a memory buffer that stores representative samples from past tasks, which have proven effective and conceptually straightforward. 
% Although most research focuses on CL with highly constrained memory, recent advances suggest that such memory constraints are less critical in practice \citep{prabhu2023computationally}. 
% For instance, it is common practice to train the models on a centralized server, which has access to ample data across various tasks. Meanwhile, the computational demands of increasingly large foundation models continue to grow, making it often more economical to store extra samples than to rely on more complex, computation-intensive methods.

% Continual learning (CL) has attracted significant attention as a paradigm enabling machine learning models to adapt to sequential tasks without forgetting previously acquired knowledge~\citep{wang2024comprehensive}. 
% \csm{Among several categories of CL algorithms, exemplar-based algorithms utilize a memory buffer that stores representative samples from past tasks, which have proven effective and conceptually straightforward, especially in class-incremental learning~\cite{masana2022class}.}
% \csm{Although these studies priorities highly constrained memory, recent research has pointed out that such memory constraints are less critical in practice~\citep{prabhu2023computationally}.}

Continual learning (CL) has attracted significant attention as a paradigm enabling machine learning models to adapt to sequential tasks while overcoming catastrophic forgetting of previously acquired knowledge~\citep{wang2024comprehensive}.  
A fundamental challenge in CL is the \emph{stability-plasticity dilemma}~\citep{(spdilemma)carpenter87, (spdilemma)mermillod13}, where a model that maintains stability on previous tasks often struggles to integrate new knowledge, and vice versa. Among various approaches, exemplar-based methods that store representative samples from past tasks have been widely adopted, particularly in class-incremental learning (class-IL)~\citep{masana2022class}. Traditionally, these methods have operated under strict exemplar memory constraints, assuming that minimizing memory usage is critical for cost efficiency. However, recent studies suggest that memory limitations may not be as critical as once thought, with computational costs—such as GPU usage—emerging as a more dominant factor~\citep{prabhu2023computationally, chavan2023towards}. For instance, an AWS GPU server equipped with 8 A100 GPUs incurs an \textit{hourly cost} of approximately \$32.77, whereas storing 1TB of data on Amazon S3 costs roughly \$23.55 per \textit{month}.

% For instance, it is common practice to train the models on a centralized server, which has access to ample data across various tasks. 
% Meanwhile, the computational demands of increasingly large foundation models continue to grow, making it often more economical to store extra samples than to rely on more complex, computation-intensive methods.

% Reflecting on this, we adopt a more realistic scenario, relaxing the memory constraint (\textit{i.e.}, sufficient exemplar memory). Under this sufficient exemplar memory regime, we find that even the simplest algorithm achieves near state-of-the-art performance at a fraction of the computational cost. This obviates the need for many existing algorithms, which were designed chiefly for stringent memory regimes. Under this new setting, we propose a simple method with two main questions in mind: (1) Does it outperform other algorithms (including Replay) given sufficient memory? (2) Does it require less computational cost than the alternatives? The key idea of our approach is to reduce the computational overhead via direct manipulation of the model in the weight space. We use a mixture of weight resetting and weight averaging techniques to accelerate convergence while improving stability and plasticity \citep{dohare2024loss} during training. Our approach outperforms competing methods under minimal computation cost, roughly a fourth to third of that required by other leading techniques. Our contributions are summarized as follows:
% \begin{itemize}[leftmargin=*]\vspace{-3mm}
%     \item We study how sufficient memory regimes alter the task of continual learning.
%     %\item We show empirical evidence that the model loss geometry impacts continual learning.
%     \item We reveal that tracking the model trajectory in the weight space uncovers valuable insights into understanding how models learn and forget.
%     \item We introduce a simple baseline solution that excels in a sufficient-memory setting, achieving superior performance at substantially reduced cost.
% \end{itemize}

Building on this, we revisit the class-IL setting under more realistic conditions, where exemplar memory constraints are relaxed (\textit{i.e.}, sufficient exemplar memory is available). Our experiments reveal that even the simplest exemplar-based approach—such as naive finetuning (\textit{i.e.}, Replay)—performs comparably to state-of-the-art algorithms while incurring significantly lower training costs (\textit{e.g.}, shorter training time). 
% These findings challenge the conventional emphasis on memory efficiency in CL research and highlight the need for computationally efficient strategies in this setting. 
Motivated by this, we propose a simple yet effective approach that optimizes computational cost while maintaining strong performance. Our method is designed with two key objectives: (1) outperforming other algorithms including Replay, and also (2) achieving computational efficiency,  in the setting with sufficient exemplar memory. The core idea is to lower computational overhead by directly operating in the model’s weight space. Specifically, we integrate weight resetting and averaging techniques to accelerate convergence while enhancing both stability and plasticity during CL. Experimental results show that our method not only surpasses competing approaches but also achieves superior performance while requiring only a quarter to a third of the computational cost of state-of-the-art algorithms. Our contributions are summarized as follows:
\begin{itemize}
    \item We investigate the impact of relaxed exemplar memory constraints on class-IL algorithms and demonstrate that even naive finetuning performs competitively under these conditions while significantly reducing the computational cost.
    \item We propose a novel weight-space manipulation strategy that achieves superior performance while drastically reducing computational overhead compared to state-of-the-art algorithms.
\end{itemize}



%First, our method identifies and resets dormant parameters that had minimal contribution in the process of learning new tasks, allowing for enhanced stability and plasticity \citep{dohare2024loss} during training. Second, we aid the learning process by aggregating different directions of updates using a periodic parameter-merging method, lowering the chance of destructive gradient collision. Our approach outperforms competing methods and maintains far lower computational cost, roughly a third to half of that required by other leading techniques.

%Notably, once enough memory is retained, inter-task forgetting (i.e. forgetting across tasks), the traditional obstacle of CL, becomes less severe, as the replay buffer effectively approximates a 'hybrid' dataset of all previous tasks. However, in this setting, new challenges emerge: where variance within the hybrid dataset interrupts the model from reaching a shared mode of different subtasks.