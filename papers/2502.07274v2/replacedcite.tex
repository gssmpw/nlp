\section{Related Works (continued.)}
\label{appendix:related-works}

\paragraph{Weight Space Operations} Recent works have shown that manipulating model parameters directly with weight-space operations (e.g., model merging ____) can handle multi-task learning ____ and continual learning ____ in a more principled way. These techniques usually intervene post-training by merging the weights of different models e.g., ____ suggested a selective merging algorithm that mitigates the interference between different models, while ____ showed that arithmetic operations on the weight space can edit the model without training. Unlike these post-training interventions, our approach manipulates the model's weight space amidst training ____ without storing multiple model parameters, aiming for cost-effective editing of the continual learner. Another relevant yet overlooked topic is the effect of weight-space operations on model attributes e.g., loss landscape ____ and plasticity ____, that contribute to continual learning and generalization. This work empirically investigates various aspects of the model to study their effect on the model's ability to handle distribution shifts. In the continual learning literature, several works have adopted weight-space operations to obtain multi-task solutions without joint training. For instance, ____ has suggested the use of weight averaging techniques for continual learning, and ____ has extended the idea using task arithmetic. However, these approaches merge models post-training and requires the storage of multiple model weights during training. On the other hand, our approach utilizes weight-space operations amidst training, without the redundant storage of multiple model weights. We view this as an important difference in modern settings where the model size is exponentially growing.