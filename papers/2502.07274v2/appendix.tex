\subsection{An investigation into the sufficient exemplar memory regime}\label{appendix:inter-task}

\paragraph{Defining the sufficient exemplar memory regime}
Most prior work assumes a strict exemplar memory constraint: $K \ll \sum_{t=1}^T |\mathcal{D}_t|$. Under such limited capacity, the replay buffer $\mathcal{M}$ can only hold a small fraction of each older dataset in $\{\mathcal{D}_1,\cdots,\mathcal{D}_{t-1} \}$. Consequently, it provides only a partial representation of the task distributions $\{ P_1, \cdots, P_{t-1} \}$, leaving the model prone to catastrophic forgetting \citep{kirkpatrick2017overcoming} between tasks (i.e., inter-task forgetting)

By contrast, we say the replay buffer $\mathcal{M}$ is \textit{sufficient} if it can store enough samples to approximate each past distribution $P_j$ for $1 \leq j < t$. Formally, we let $K \approx \kappa \sum_{j=1}^{t-1} |\mathcal{D}_j|,$
where $0 < \kappa \leq 1$ determines what fraction of the earlier task data can be retained. A larger $\kappa$ indicates that $\mathcal{M}$ preserves a representative set of samples from past tasks - though not necessarily every sample. As a result of this sufficient exemplar memory setting, the mixture distribution $P_{past}$ of previously encountered tasks at the training step of task $t$ effectively becomes:
\begin{equation}
    P_{past}(x,y) \approx \frac{\sum_{j=1}^{t-1} \pi_j P_j(x,y)}{\sum_{j=1}^{t-1}\pi_{j}},
\end{equation}
where $\pi_j$ denotes the relative importance (i.e., size) of each past task and $P_j(x,y)$ the true data distribution of task $j$. Hence, when learning task $t$, the effective training distribution is 
\begin{equation}
    P_{train}^{(t)}(x,y) \approx \alpha_{t}P_t(x,y) + (1-\alpha_t) P_{past}(x,y),
\end{equation}
where $\alpha_t \in [0,1]$ is a balancing factor that weights the newly introduced and replayed data, usually emphasizing the current task.


\paragraph{Sufficient exemplar memory mitigates catastrophic forgetting}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/cifar100/forgetting_cnn_combined.pdf}
        \label{fig:forgetting_ours_cifar100}
        \caption{CIFAR-100}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/imagenet100/forgetting_cnn_combined.pdf}
        \label{fig:forgetting_ours_imagenet}
        \caption{ImageNet-100}
    \end{subfigure}
    \caption{The impact of exemplar memory size on catastrophic forgetting. Increased memory drastically reduces the forgetting between tasks, while it persists.}
    \label{fig:forgetting}
\end{figure}

Next, we discuss how a sufficiently large exemplar memory buffer $\mathcal{M}$ approximates previous tasks' distributions, thus reducing inter-task forgetting. 


Ideally, if we could train jointly on all tasks $1,\cdots,t$, the obtained model parameter $\theta^*_{1:t}$ would minimize the risk:
\begin{equation}
    R_{1:t}(\theta) = \mathbb\sum_{j=1}^{t} \mathbb{E}_{(x,y)\sim P_{j}}  
 \bigr[\ell(\theta;x,y) \bigr],
\end{equation}
which minimizes inter-task forgetting by design. By joint-training on all task data up to task $t$, we would find $\theta^*_{1:t} = \operatorname{arg}\operatorname{min_{\theta} R_{1:t}(\theta)},$ with no conventional notion of forgetting between tasks.

Similarly, with a large enough exemplar memory, the replayed data for previous tasks closely approximates their true distributions ($\tilde{P}_j \approx P_j$ for all $j<t$). Therefore training on $D_t \cup \mathcal{M}_{1:t-1}$ yields a risk
\begin{equation}
    \tilde{R}_{1:t}(\theta) \approx \alpha_{t} \mathbb{E}_{(x,y) \sim P_t}\bigr[\ell(\theta) \bigr] + (1-\alpha_t)\mathbb \sum_{j=1}^{t-1} \hat{\pi}_j \mathbb{E}_{(x,y) \sim \tilde{P_j}} \bigr[\ell(\theta) \bigr],
\end{equation}
where we may bound $|\tilde{R}_{1:t}(\theta) - R_{1:t}(\theta)| \leq \epsilon$ under standard assumptions (e.g., Lipschitz continuity in $\theta$ \citep{khromov2024some}), with $\epsilon$ shrinking as the replay buffer size $K$ increases, leading to $\tilde{P_j} \approx P_j$. A small gap between $\tilde{R}_{1:t}(\theta)$ and $R_{1:t}(\theta)$ implies that $\lVert \tilde{\theta}^*_{1:t} - \theta^*_{1:t} \rVert$ is also small, where $\tilde{\theta}^*_{1:t} = \operatorname{arg}\operatorname{min}_{\theta} \tilde{R}_{1:t}$. Intuitively, if the two risk surfaces are proximate, their minimizes are also close in the parameter space \citep{fornasier2021consensus}.

In this sense, inter-task forgetting on a previous task $j$ arises when $\tilde{\theta}^*_{1:t}$ drastically changes predictions on the previous task distribution $P_j$. But if $\tilde{\theta}^*_{1:t}$ remains near $\theta^*_{1:t}$ (which, by definition, does well on previous tasks by training jointly), it must still perform well on task $j$. Hence, if we measure inter-task forgetting in the parameter space as
\begin{equation}
    \Delta_{j \rightarrow t} = \mathbb{E}_{x \sim P_j} \bigr[ \ell(\tilde{\theta}^*_{1:t};x) - \ell(\theta^*_{1:j};x) \bigr],
\end{equation}
the measure of how replay-based parameters after task $t$ perform on task $j$ compared to the parameters after task $j$. Here, $\Delta_{j \rightarrow t}$ remains small if $\tilde{\theta}^*_{1:t} \approx \theta_{1:t}^*$. Naturally, since $\theta^*_{1:t}$ is a reliable minimizer on all tasks, the small parameter drift ensures that the performance of the model trained under sufficient exemplar memory does not degrade on earlier tasks - i.e., inter-task forgetting is reduced as exemplar memory becomes larger \citep{merlin2022practical,brignac2023improving}. %In \cref{sec:experiment}, we empirically show that increasing the memory size gradually reduces inter-task forgetting. 




\paragraph{New challenges under sufficient exemplar memory}\label{appendix:new-challenges}
While the sufficient exemplar memory regime mitigates inter-task forgetting by closely approximating past data distributions, it also creates a hybrid training dataset that combines current task data $D_t \cup \mathcal{M}_{1:t-1}$ with replayed samples from previous tasks. 

This combined dataset introduces several challenges. First, gradient interference may occur as the gradients computed on current task samples can conflict with those from past tasks, leading to partial cancellation of updates and impeding effective learning across tasks. We view that this potentially could lead to a new form of forgetting that distorts the learned features and ultimately interfering with the model's learning process. Second, the heterogeneous nature of the hybrid dataset increases the variance of the stochastic gradient estimates, resulting in slower convergence. This heightened variance necessitates either more iterations or more sophisticated optimization techniques to minimize the loss reliably. Furthermore, imbalances in task representation can arise if the replay buffer unevenly captures the diverse distributions of past tasks. 

Conventional continual learning methods typically address forgetting by assuming that new data is markedly different from prior data and focusing on preserving performance on previously learned tasks. However, in the sufficient exemplar memory regime, where all tasks are presented simultaneously within a combined dataset, these methods fall short. They are not equipped to handle the multi-task learning dynamics and the associated balancing issues that emerge when the model must integrate and harmonize learning signals from a diverse set of tasks. We believe a more thorough investigation is required, and we set this as a key objective of our future work.


\subsection{Method (continued.)}\label{appendix:method}

In this section, we elaborate on our method, highlighting the role of each component. The proposed method is a combination of two weight-space operations. (1) Weight Reset \citep{yadav2024ties} and (2) Weight Averaging \citep{wortsman2022model}. 


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.66\textwidth]{images/weight_reset.pdf}
  \caption{Weight Reset Experiment on CIFAR-100. Most parameters are redundant in learning new tasks.}
  \label{fig:weight_reset}
\end{figure}

\paragraph{Weight Reset} The weight reset step selects and resets parameters that were redundant in adapting to the new task. The aim of this procedure is to find the minimal set of parameters that are capable of adapting to the new $t^{th}$ task, and reverting the redundant parameters to the value of the previous task model $\theta_{prev}$, which helps recover learned features. In this process, we use a simple distance metric to measure a model's contribution to the new task, formulated as:
\begin{equation}\label{eq:parameter-delta}
    \big| \theta[l] - \theta_{prev}[l] \big|,
\end{equation}
where $\theta[l]$ and $\theta_{prev}[l]$ indicates the $l^{th}$ parameter of the current model $\theta$ and the previous task model $\theta_{prev}$, respectively. Using this metric, we retain the top-$Q\%$ parameters that have largely drifted during the current task and reset the dormant parameters using \cref{eq:reset-mix}. The idea of resetting model weights is not novel in the continual learning literature, but most focused on improving plasticity \citep{dohare2024loss}. Conversely, we improve both plasticity and stability using a mixing technique (\cref{eq:reset-mix}, integrating previous and present tasks. In \cref{appendix:analysis}, we provide experimental results on the effect of $Q$, the percentage of weight reset on model performance (see \cref{fig:weight_reset}).



\paragraph{Weight Average} 
The weight-averaging step aggregates various learned signals into a single model, allowing for faster convergence and improved generalization \citep{izmailov2018averaging}. The underlying idea is that the model weights can address catastrophic forgetting, functioning as a substitute for replay memory. Using this, we aim to fill the gap between the full exemplar memory setting (i.e., joint-training) and the sufficient exemplar memory setting. Another motive behind this approach is the new challenges (see \cref{appendix:inter-task}) rooted in training with a mixture of datasets $D_{t} \cup \mathcal{M}_{1:t-1}$, which generally requires more training steps for convergence. In \Cref{appendix:analysis}, we empirically show that the sufficient exemplar memory setting complicates training, requiring more training epochs for convergence (see \Cref{fig:train_loss}). The weight averaging technique is also an emerging practice in the continual learning literature \citep{marouf2025weighted,kozal2024continual}. However, such works merge the model weights after training, which requires the storage of multiple model weights  (proportional to the number of tasks). On the other hand, our method uses a moving averaged model that is updated during training, reducing the cost of storage.

\subsection{Measuring Parameter-Drift}\label{appendix:drift-measures}

In our work, we measure a parameter's contribution to learning a new task by comparing its parameter drift (\cref{eq:parameter-delta}). However, there are a number of alternative approaches we could take. For instance, we can differentiate between how a parameter changes (1) between tasks (2) within a task. 

\paragraph{Inter-task Parameter Drift}
When transitioning from one task to the next, the change in the \(l\)-th parameter can be measured as
\[
\Delta \theta_{\text{inter}}[l] = \bigl|\theta_{t}[l] - \theta_{t-1}[l] \bigr|,
\]
where $\theta_{t}[l]$ denotes the $l^{th}$ parameter after training on the current task $t$, and $\theta_{t-1}[l]$ represents the $l^{th}$ parameter after training on the previous task $t-1$. A large value indicates that the parameter is highly task-specific, while a small value suggests robustness across tasks.

\paragraph{Intra-task Parameter Drift}
Alternatively, we can analyze how a parameter evolves during the training process of a single task. Let $\theta^{(i)}[l]$ denote the value of the $l^{th}$ parameter at the $i^{th}$ iteration during training. Then, the intra-task parameter drift is given by
\[
\Delta \theta^{(i)}_{\text{intra}}[l] = \bigl|\theta^{(i+1)}_t[l] - \theta^{(i)}_t[l]\bigr|.
\]
This measure captures the incremental updates of the parameter as the model optimizes its performance on the current task. By comparing both the inter-task and intra-task parameter changes, we gain a more comprehensive understanding of the role each parameter plays in adapting to new tasks as well as the dynamics of learning within a single task. In our future work, we will seek a more reliable metric to express a parameter's behavior in the weight space.



\subsection{Experimental Results and Analysis}\label{appendix:analysis}

In this section, we continue our discussion of the experimental results.

\begin{table}[htbp]
  \centering
  \caption{Ablation study on CIFAR-100}
  \label{tab:ablation}
  \begin{adjustbox}{width=0.48\linewidth}
    \input{tables/cifar100_ablation}
  \end{adjustbox}
\end{table}

We begin with the ablation study. In this experiment, we perform an ablation study on the two components that our method utilizes, (1) the weight reset and (2) the weight average procedure. In \cref{tab:ablation}, we report the results of the experiment. Here, w/o reset indicates the replay method combined with weight average (hence without reset), while w/o avg. indicates the replay method added with the reset procedure (i.e., without avg.). The replay method is displayed to show the case when neither weight reset nor weight average is utilized. Our method showed the strongest results compared to its ablations. Notably, we can see that weight reset alone (w/o avg.) does not show strong gains. However, when the weight reset is combined with the weight average method, it allows significant performance gains.  

Another experimental result is the weight reset experiment in \cref{fig:weight_reset}. Here, we can see that approximately only $20\%$ of the model parameters are responsible for the model's learning process (i.e., resetting $80\%$ of the parameters does not pose significant issues). In contrast, when the percentage drops to $10\%$ or less, the model's task accuracy drops by a large margin. We further discover that the weight average step helps the model to become even more sparse (i.e., not reliant on individual parameters), improving the model performance under extreme resetting settings. 



\begin{table}[h]
%\renewcommand\thetable{D} 
\caption{Average Acc. on CIFAR-100 with different exemplar memory sizes (full)}
\label{tab:cifar100_full}
\centering
\begin{adjustbox}{width=0.9\textwidth}
\centering\input{tables/cifar100_all}
\end{adjustbox}
%\vspace{-3mm}
\end{table} 


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.66\textwidth]{images/train_loss_comparison_plasticity.pdf}
  \caption{The training loss under a sufficient exemplar memory continual learning setting. When training with sufficient exemplar memory, convergence becomes difficult.}
  \label{fig:train_loss}
\end{figure}


Lastly, we show that under sufficient exemplar memory, convergence becomes difficult, as discussed in \cref{appendix:new-challenges}. The results are illustrated in \Cref{fig:train_loss}. Specifically, we analyze the training loss of the model under two cases. (1) Continual Learning: train the model across 20 tasks sequentially, with sufficient exemplar memory. (2) Joint Training: train individual models for each task using all known task data. For this experiment, we used the CIFAR-100 dataset divided into 20 subtasks. In \Cref{fig:train_loss}, we can see that under sufficient exemplar memory, converging to the training data becomes difficult, especially for the continual learning model. While a simple solution would be to extend the training epochs for convergence, it would collide with our aim for cost-efficiency. On the other hand, joint-trained models relatively converge better. This result aligns with the results of \citet{dohare2024loss} that a model sequentially trained on different tasks (i.e., continual learning) suffers a loss of plasticity (i.e., a model's ability to learn new tasks). We believe a thorough investigation of this new phenomenon is required.

\subsection{Experimental Details}\label{appendix:experiment-details}

In this section, we report the experimental details of our experiments. In all our experiments, we have used the PyCIL \citep{zhou2023pycil} library, which allows easy replication. We followed the standard training hyperparameters of the PyCIL library, which are fixed across experiments. Regarding unique hyperparameters, the average interval $j$ was set as $5$, and the warming epoch $n_{warm}$ was set as $25\%$ of the total training epochs (default set as $70$ under the PyCIL setting). $j$ and $n_{warm}$ were selected using a grid search. Please refer to \Cref{sec:method} for a better understanding of each hyperparameter. Lastly, regarding the computing resources, we used a single NVIDIA RTX 6000 GPU for all experiments. For our experiments, we used the 2.2.1 version of Pytorch \citep{pytorch}. 



\begin{comment}
    
\subsection{Large Exemplar Memories exacerbate Loss of Plasticity}

In this section, we provide a theoretical analysis of the effect of sufficient exemplar memory on the model's loss of plasticity \citep{dohare2024loss}.

Let $\ell_t(w)$ denote the loss on task~$t$ (e.g.\ mean-squared error) for parameter vector $w\in\mathbb{R}^d$.  Under continual learning, after tasks $1,\dots,t-1$ are learned, task~$t$ is trained by minimizing
\[
  \ell_t(w) \;+\; \lambda \,\sum_{i=1}^{t-1}\,\widehat{\ell}_i(w),
\]
where $\widehat{\ell}_i$ is the loss over $M_i$ \emph{exemplar} points stored from task~$i$.  The total memory size is $M := \sum_{i=1}^{t-1} M_i$.  We define the \emph{intransigence} (loss of plasticity) for task~$t$ as
\[
   \Delta_{\mathrm{new}}^{(t)} \;=\; 
   \ell_t\bigl(w^{(t)}\bigr) \;-\; \ell_t\bigl(w_{\text{fresh}}^{(t)}\bigr),
\]
where $w^{(t)}$ is the \emph{continual} solution and $w_{\text{fresh}}^{(t)}$ is trained only on task~$t$.  When $\Delta_{\mathrm{new}}^{(t)}$ is large, the model underfits task~$t$ compared to a fresh learner---a sign of diminished plasticity.

\begin{proposition}[Loss of Plasticity Increases with Memory]
\label{prop:loss_of_plasticity}
In linear regression with partially conflicting tasks, once $M$ is large enough to substantially reduce catastrophic forgetting, further increases in the exemplar memory~$M$ strictly increase the intransigence~$\Delta_{\mathrm{new}}^{(t)}$ for new tasks.  Thus, \emph{a larger replay memory that preserves old tasks more effectively also imposes a stricter stability constraint, degrading plasticity on the new task.}
\end{proposition}

\begin{proof}[Proof (Sketch)]
Consider two linear tasks in $\mathbb{R}^d$ with \emph{true} parameters $v_1$ and $v_2$.  Suppose $v_2$ is not exactly parallel to $v_1$; for example, they have negative or small positive correlation.  Let $\ell_1(w)$ and $\ell_2(w)$ be the mean-squared losses for tasks 1 and 2, respectively.  

\noindent
\textbf{(1)~From-Scratch vs.\ Replay Objective.}
Training \emph{from scratch} on task~2 alone yields $w_{\text{fresh}}^{(2)}=\arg\min_{w}\ell_2(w)$.  Under \emph{continual learning} with $M$ exemplars stored from task~1, we minimize
\[
   \ell_2(w) \;+\; \lambda\,\widehat{\ell}_1(w),
\]
where $\widehat{\ell}_1$ approximates $\ell_1$ using $M$ old points.  Let $w^{(2)}$ be the minimizer of this replay objective.

\noindent
\textbf{(2)~Parametric Form of the Solutions.}
In linear regression, we can expand any $w$ as a combination of $v_1$ and $v_2$ plus an orthogonal remainder.  When $v_1^\top v_2<1$, a conflict arises: pushing $w$ toward $v_1$ (for task~1) can harm its alignment with $v_2$ (for task~2).  As $M$ grows, the replay term $\widehat{\ell}_1$ dominates, forcing $w^{(2)}$ closer to $v_1$ than $w_{\text{fresh}}^{(2)}$ would be.  Hence $\ell_2\bigl(w^{(2)}\bigr)$ remains larger than $\ell_2\bigl(w_{\text{fresh}}^{(2)}\bigr)$.

\noindent
\textbf{(3)~Monotonic Increase in $\Delta_{\mathrm{new}}^{(2)}$.}
Because $w^{(2)}$ must balance $\ell_2$ against a penalty weighted by $M$, the gap
\[
   \Delta_{\mathrm{new}}^{(2)}(M)
   \;=\;
   \ell_2\bigl(w^{(2)}\bigr)
   \;-\;
   \ell_2\bigl(w_{\text{fresh}}^{(2)}\bigr)
\]
is non-decreasing in $M$.  Indeed, if $M$ increases, $w^{(2)}$ is forced even closer to $v_1$ for old-task stability, thereby deviating more from $v_2$.  This deviation inflates the final loss on task~2.  

Extending the argument to tasks $1,\ldots,t$ reveals the same phenomenon: the solution $w^{(t)}$ increasingly overfits old data as $M$ grows, elevating $\ell_t\bigl(w^{(t)}\bigr)$ above $\ell_t\bigl(w_{\text{fresh}}^{(t)}\bigr)$ by a strictly larger margin.  Thus, \emph{as soon as $M$ becomes sufficient to reduce catastrophic forgetting, it inevitably increases the new-task intransigence, indicating severe loss of plasticity.}
\end{proof}

\begin{remark}
In more general multi-task linear settings (or smoothly parameterized networks), one similarly finds that large replay memory yields smaller forgetting on old tasks but \emph{larger} performance gaps on new tasks.  This illustrates an inherent stability--plasticity trade-off.
\end{remark}


\end{comment}



\begin{comment}
\subsection{sufficient exemplar memory can amplify gradient interference}

While a sufficient exemplar memory regime helps address inter-task forgetting, the diversity of the hybrid dataset (e.g., $D_t \cup \mathcal{M}_{1:t-1}$ at step $t$) can adversely affect subpopulations within the current task $t$.

When the memory is large, the training data now consists of diverse subpopulations within the hybrid dataset, which raises the chances of gradient collision. Concretely, if a subpopulation (e.g., current task $D_t$) $m$ task a gradient direction $\nabla \ell_{m}(\theta)$ and a non-identical subpopulation $m' \neq m$ has $\nabla \ell_{m'}(\theta)$ pointing in a different direction, gradient steps may partially undo each other \citep{yu2020gradient}. This collision can degrade accuracy on subpopulation $m$ after the model subsequently focuses on subpopulation $m'$.


Assume task $t$ consists of $M$ distinct subpopulations in hybrid dataset ($D_t \cup \mathcal{M}_{1:t-1}$). Let these subpopulations $\{P_t^1, P_t^2, \cdots, P_t^M \}$. Here, the subpopulation may be interpreted as, but not limited to the tasks. Let $g_m(\theta) = \nabla_{\theta} L_m(\theta)$ the expected gradient of the subpopulation $m$ at parameter $\theta$, where $L_{m}(\theta) = \mathbb{E}_{(x,y)\sim P_{t}^m}[\ell(\theta;x,y)]$. During training, samples from various subpopulations are included in the training batch with proportions $\gamma_1, \cdots, \gamma_{M}$ (where $\sum_{m=1}^{M}\gamma_m = 1$, making the expected gradient as: $g(\theta) = \sum_{m=1}^{M}\gamma_mg_m(\theta)$.

Next, consider a stochastic gradient $\tilde{g}(\theta)$ computed on a mini-batch drawn from the mixture distribution. It is an unbiased estimator of $g(\theta)$ but its variance is influenced by the diversity of the batch. Then, the variance of the gradient can be decomposed as:
\begin{equation}
    Var[\tilde{g}(\theta)] = \mathbb{E} \bigr[\lVert \tilde{g}(\theta) - g(\theta)\lVert \bigr] \approx \sum_{m=1}^M \gamma_m^2 Var[g_m(\theta)] + \sum_{m\neq m'} \gamma_m \gamma_{m'} \langle g_m(\theta) - g(\theta), g_{m'}(\theta) - g(\theta) \rangle,
\end{equation}
where the first term aggregates variances within each subpopulation and the second term is a sum over pairwise covariances between gradients from different subpopulations. These covariance terms reflect how updates from different subpopulations interact.

Next, we analyze the effect of the increased memory. Larger memory allows the training batch to include a wider variety of samples from different subpopulations, effectively increasing $M$ and capturing more heterogenous $g_m(\theta)$s. As $M$ increases, the number of pairwise covariance $\langle g_m(\theta) - g(\theta), g_{m'}(\theta) - g(\theta) \rangle$ grows. If subpopulations $m$ and $m'$ are diverse such that their gradients point in conflicting directions, the inner project $\langle g_m(\theta) - g(\theta), g_{m'}(\theta) - g(\theta) \rangle$ can be negative. Naturally, the more diverse the data is, the probability of such conflicting pairs simultaneously increases, making the gradient descent steps noisy \citep{guo2025out,goldfarb2024joint}. Intuitively, this phenomenon means that while the model is learning to minimize loss for one subpopulation, it could be forgetting on another subpopulation \citep{doan2021theoretical,kumar2022finetuningdistortpretrainedfeatures}.
    
\end{comment}

\subsection{Related Works (continued.)}\label{appendix:related-works}

\paragraph{Weight Space Operations} Recent works have shown that manipulating model parameters directly with weight-space operations (e.g., model merging \citep{wortsman2022model}) can handle multi-task learning \cite{yu2024language} and continual learning \citep{marouf2025weighted,marczak2025magmax} in a more principled way. These techniques usually intervene post-training by merging the weights of different models e.g., \citet{yadav2024ties} suggested a selective merging algorithm that mitigates the interference between different models, while \citet{ilharco2022editing} showed that arithmetic operations on the weight space can edit the model without training. Unlike these post-training interventions, our approach manipulates the model's weight space amidst training \citep{izmailov2018averaging,jang2025model} without storing multiple model parameters, aiming for cost-effective editing of the continual learner. Another relevant yet overlooked topic is the effect of weight-space operations on model attributes e.g., loss landscape \citep{li2018visualizing,kaur2023maximum} and plasticity \citep{dohare2024loss}, that contribute to continual learning and generalization. This work empirically investigates various aspects of the model to study their effect on the model's ability to handle distribution shifts. In the continual learning literature, several works have adopted weight-space operations to obtain multi-task solutions without joint training. For instance, \citet{kozal2024continual} has suggested the use of weight averaging techniques for continual learning, and \citet{marczak2025magmax} has extended the idea using task arithmetic. However, these approaches merge models post-training and requires the storage of multiple model weights during training. On the other hand, our approach utilizes weight-space operations amidst training, without the redundant storage of multiple model weights. We view this as an important difference in modern settings where the model size is exponentially growing. 
