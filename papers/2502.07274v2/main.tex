\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx} 
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage[table,dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{comment}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{amsmath, amsthm,amssymb}
\usepackage{pifont}
%\usepackage[colorlinks,citecolor=NavyBlue]{hyperref}
\usepackage{inconsolata}
\usepackage{tabto}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage {algpseudocode}
\usepackage{algorithmicx}
\usepackage{algcompatible}
\usepackage{xfrac}   % for \sfrac macro
\usepackage{cleveref}
\Crefname{Figure}{Fig.}{Figs.}
\Crefname{Section}{Sec.}{Sec.}
\Crefname{Equation}{Eq.}{Eqs.}
\usepackage{adjustbox}
\usepackage{wrapfig}
\usepackage[normalem]{ulem}
\usepackage{pgfplots}
\pgfplotsset{compat=1.7}
\usepackage{enumitem}
%\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{float}

\setlist[itemize]{noitemsep}



\newcommand{\cdk}[1]{\textcolor{green}{[DK: #1]}}
% \newcommand{\csm}[1]{\textcolor{blue}{[SM: #1]}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\paragrapht}[1]{\noindent\textbf{#1}}

\usepackage{lipsum}
\renewcommand\RSsmallest{6pt}
\newcommand{\dk}[1]{\textcolor{green}{[DK: #1]}}
\newcommand\csm[1]{\textcolor{black}{#1}}


\title{Cost-Efficient Continual Learning with Sufficient Exemplar Memory}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

%\thanks
% \author{Dongkyu Cho \\
% New York University\\
% \texttt{dongkyu.cho@nyu.edu} \\
% \And
% Taesup Moon \\
% Seoul National University \\
% \texttt{email@snu.ac.kr} \\
% \And
% Rumi Chunara \\
% New York University \\
% \texttt{rumi.chunara@nyu.edu}
% \And
% Kyunghyun Cho \\
% New York University \& Genentech \\
% \texttt{kyunghyun.cho@nyu.edu}
% \And
% Sungmin Cha \\
% New York University \\
% \texttt{sungmin.cha@nyu.edu}
% }

%\author{\name Dongkyu Cho$^1$\thanks{equal %contribution} , Sungjun Cho$^{2*}$, Dasol Hwang$^2$, %and Moontae Lee$^{2, 3}$ \\
%      \addr $^1$New York University \ \ $^2$LG AI %Research $^3$University of Illinois Chicago \\ %\email sungmin.cha@nyu.edu, \{sungjun.cho, %dasol.hwang, moontae.lee\}@lgresearch.ai
%      }

\author{Dongkyu Cho$^1$, Taesup Moon$^{2}$, Rumi Chunara$^1$, Kyunghyun Cho$^{1,3}$, and Sungmin Cha$^{1}$ \\
      $^1$New York University \ \ $^2$ASRI / INMC / IPAI / AIIS, Seoul National University \ \ $^3$Genentech \\ \textit{dongkyu.cho@nyu.edu, taesup.moon@snu.ac.kr, \{rumi.chunara, kyunghyun.cho, sungmin.cha\}@nyu.edu
      }}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
% Continual learning (CL) research typically assumes highly constrained memory resources. However, in many real-world scenarios—especially in the era of large foundation models—memory is abundant while computational costs are the primary bottleneck. In this work, we investigate continual learning in a novel setting where memory is ample (\textit{i.e.}, sufficient-memory). In consideration of this new regime, we propose a simple method that directly manipulates the model’s weight space through a combination of weight resetting and averaging techniques. Our approach achieves state-of-the-art performance at a fraction—roughly one quarter to one third—of the computational cost of existing methods. Our work provides new insights into the dynamics of model trajectories in its weight space and challenges the necessity of complex algorithms designed for stringent memory limits, offering a practical baseline for continual learning applications.
Continual learning (CL) research typically assumes highly constrained exemplar memory resources. However, in many real-world scenarios—especially in the era of large foundation models—memory is abundant, while GPU computational costs are the primary bottleneck. In this work, we investigate CL in a novel setting where exemplar memory is ample (\textit{i.e.}, sufficient exemplar memory). Unlike prior methods designed for strict exemplar memory constraints, we propose a simple yet effective approach that directly operates in the model’s weight space through a combination of weight resetting and averaging techniques. Our method achieves state-of-the-art performance while reducing the computational cost to a quarter or third of existing methods. These findings challenge conventional CL assumptions and provide a practical baseline for computationally efficient CL applications.
% Our work provides new insights into the dynamics of model trajectories in its weight space.
% and challenges the necessity of complex algorithms designed for stringent memory limits, offering a practical baseline for CL applications.

%Through both theoretical and empirical analysis, we show that the sufficient exemplar memory regime alleviates catastrophic forgetting in nature, while increasing the computational burden  

%Continual learning (CL) research typically assumes highly constrained memory resources, yet in many real-world scenarios ample memory is available and cheap to maintain. We demonstrate that, in this sufficient exemplar memory setting, a simple replay-based method achieves competitive accuracy at a fraction of the computational cost, thereby challenging the necessity of more complex algorithms designed for limited-memory regimes. However, while inter-task forgetting is largely mitigated with abundant memory, internal variability within each task introduces new learning instabilities. To address both issues, we propose a parameter-space method that resets “dormant” parameters to prevent the loss of previously learned knowledge and periodically merges parameter updates to reduce destructive gradient interference. Our approach outperforms state-of-the-art techniques and significantly lowers computational overhead. We further provide empirical evidence relating performance to model sharpness and offer insights into how parameter trajectories reveal the mechanisms of forgetting.









%In continual learning (CL), it is a common practice to store a subset of training data for later use as a replay memory. Previous works have assumed that a strict memory constraint is necessary, in consideration of the storage cost of the memory buffer. This led to the emergence of CL algorithms that address forgetting through explicit regularization, which overlooks their computation costs. However, in scenarios where sufficient exemplar memory is available, inter‐task forgetting becomes much less severe. We show that under these conditions, intra‐task forgetting—where a model loses knowledge within the same task—becomes a critical but overlooked issue. Through extensive experimentation, we find that many existing continual learning methods actually perform worse than a simple replay baseline when memory is abundant, while also incurring substantial computational overhead. To address intra‐task forgetting, we propose smoothing the loss landscape, a strategy that significantly mitigates forgetting without substantial cost. Our method leverages modern, large‐scale models efficiently, avoiding the computational bottlenecks of conventional approaches. By combining effective replay with a cost‐efficient mechanism to stabilize representations within each task, we achieve superior performance compared to state‐of‐the‐art methods while maintaining significantly lower compute requirements. 

\end{abstract}

\section{Introduction}\label{sec:introduction}
\input{introduction}

\section{Related Works}\label{sec:related_works}
\input{related_works}

\section{Problem Formulation}\label{sec:theory}
\input{problem_formation}

\section{Weight Space Consolidation}\label{sec:method}
\input{method}

\section{Experiment}\label{sec:experiment}
\input{experiment}


\vspace{-3mm}
\section{Concluding Remarks}
% We study continual learning under a memory-sufficient setting. Through both theoretical and empirical analysis, we show that existing continual learning algorithms are not scalable under the memory-sufficient regime, falling behind the simplest baseline while requiring enormous computation costs. This opens a demand for a new breed of cost-efficient algorithms, which can be easily achieved through our weight-space approach. Our proposed method outcompetes existing baselines under the memory-sufficient regime, with a significantly lower cost. More importantly, this research offers new insights into understanding how a model's weight space reflects its learning/forgetting procedure, holding promise for further development.
Our work shifts the focus in continual learning from memory efficiency to computational cost, demonstrating that with sufficient exemplar memory, exemplar-based methods like Replay can achieve competitive performance at a fraction of the training cost. We introduce a simple, effective method that directly manipulates the model's weight space, outperforming existing algorithms in terms of both accuracy and efficiency. These results highlight the potential for optimizing computational efficiency in class-incremental learning and provide a promising direction for practical applications, suggesting that exemplar memory constraints may no longer be the primary bottleneck in many real-world scenarios.

% \newpage

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\newpage
\section{Appendix}
\input{appendix}

\end{document}
