\section{Related Works (continued.)}
\label{appendix:related-works}

\paragraph{Weight Space Operations} Recent works have shown that manipulating model parameters directly with weight-space operations (e.g., model merging \citep{wortsman2022model}) can handle multi-task learning \cite{yu2024language} and continual learning \citep{marouf2025weighted,marczak2025magmax} in a more principled way. These techniques usually intervene post-training by merging the weights of different models e.g., \citet{yadav2024ties} suggested a selective merging algorithm that mitigates the interference between different models, while \citet{ilharco2022editing} showed that arithmetic operations on the weight space can edit the model without training. Unlike these post-training interventions, our approach manipulates the model's weight space amidst training \citep{izmailov2018averaging,jang2025model} without storing multiple model parameters, aiming for cost-effective editing of the continual learner. Another relevant yet overlooked topic is the effect of weight-space operations on model attributes e.g., loss landscape \citep{li2018visualizing,kaur2023maximum} and plasticity \citep{dohare2024loss}, that contribute to continual learning and generalization. This work empirically investigates various aspects of the model to study their effect on the model's ability to handle distribution shifts. In the continual learning literature, several works have adopted weight-space operations to obtain multi-task solutions without joint training. For instance, \citet{kozal2024continual} has suggested the use of weight averaging techniques for continual learning, and \citet{marczak2025magmax} has extended the idea using task arithmetic. However, these approaches merge models post-training and requires the storage of multiple model weights during training. On the other hand, our approach utilizes weight-space operations amidst training, without the redundant storage of multiple model weights. We view this as an important difference in modern settings where the model size is exponentially growing.