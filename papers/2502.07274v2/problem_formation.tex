%Define the sufficient-memory continual learning setting, explain and show that inter-task forgetting becomes smaller.

%Show that sufficient memory leads to higher variance in dataset distribution, similar to a hybrid dataset

% In this section, we formulate the problem of continual learning with sufficient memory. 

\noindent\textbf{Notation}
We generally follow the setting of class-incremental learning (class-IL)~\citep{masana2022class}. We consider a sequence of $T$ tasks, each associated with distribution $P_{t}$. Let $\mathcal{D}_{t} = \{(x^t_i,y^t_i)\}_{i=1}^{|\mathcal{D}_t|}$ with $(x^t_i,y^t_i) \sim P_{t},$, be the training dataset for task $t$. The tasks are presented in order $t=1, \cdots, T$. The model $F$ (its parameters $\theta$) does not retain explicit access to previous task datasets $\mathcal{D}_j$ for $j<t$, except via a exemplar memory buffer $\mathcal{M}$ of a capacity of $K$. Thus, at the training step of task $t$, the model updates its parameters $\theta$ using the data $D_t \cup \mathcal{M}_{1:t-1}$ and the cross-entropy loss $\ell(\theta;x,y)$, where $\mathcal{M}_{1:t-1}$ includes selected samples from earlier tasks. %Finally, we denote $\theta^*$ as the loss-minimizing parameters found through training. %We denote $\tilde{P_{t}}$ as the empirical distribution from the replay buffer for task $t$. 


\begin{figure}[htbp]
\vspace{-0.1in}
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/cifar100/average_accuracy_cnn_combined.pdf}
        \vspace{-0.2in}
        \caption{Acc. graph on CIFAR-100}
        \label{fig:acc_ours_cifar100}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/cifar100/time_spent_combined.pdf}
        \vspace{-0.2in}
        \caption{Training time spent on CIFAR-100}
        \label{fig:time_ours_cifar100}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{A comparison of (a) average class-IL accuracy and (b) training time across different exemplar memory sizes in the 10 task scenario on CIFAR-100. As memory size increases, catastrophic forgetting is reduced, while training time (\textit{i.e.}, computational cost) increases proportionally. Note that both DER and FOSTER are expansion-based methods, represented by dashed lines, where FOSTER doubles the model size, and DER scales based on the number of tasks.}
    \label{fig:cifar100}
    \vspace{-0.1in}
\end{figure}

\noindent\textbf{What changes under a sufficient exemplar memory regime?} \ \
We claim that a sufficiently large memory can closely approximate the data distributions of previous tasks, effectively reducing forgetting. For the complete derivation of this claim, please refer to \Cref{appendix:inter-task}. 
% In our experiments (\cref{sec:experiment}), we confirm a significant drop in forgetting occurs when memory is abundant (see \cref{fig:acc_ours_cifar100}). 
Experimentally, \Cref{fig:acc_ours_cifar100} demonstrates that a significant drop in forgetting occurs when exemplar memory is abundant.
% In our experiments (\cref{sec:experiment}), we confirm a significant drop in forgetting occurs when memory is abundant (see \cref{fig:acc_ours_cifar100}). 
Notably, we observe that the simplest baseline (\textit{i.e.}, Reply) outperforms its competitors with significantly lower cost (see \Cref{fig:time_ours_cifar100}). 
% This highlights two key points: First, with ample memory, existing algorithms lag behind the simplest baseline, and hence, their excess computation cost is unnecessary. In consideration of these observations, we seek a new algorithm that is (1) cost-effective and (2) mitigates new challenges rooted in the sufficient-memory setting.
This underscores two key points: First, when the sufficient exemplar memory is available, existing algorithms fall behind even the simplest baseline, making their additional computational cost unnecessary. With these observations in mind, we aim to develop a new algorithm that is (1) cost-effective and (2) addresses the new challenges that arise in a sufficient exemplar memory setting.

%In consideration of these observations, we seek a cost-effective algorithm specifically designed for this new scenario.
%that is (1) cost-effective and (2) mitigates new challenges rooting from the sufficient-memory setting.










%
%In essence, if one were able to jointly train on all tasks, the resulting model would minimize a combined risk that naturally prevents forgetting. By using a large memory for replay, the surrogate risk closely approximates this joint risk. Under standard assumptions (e.g., Lipschitz continuity), the gap between the replay-based risk and the joint risk can be bounded, ensuring that the optimal parameters from replay remain near those obtained via joint training. Consequently, this proximity contributes to maintaining the performance on earlier tasks.
%
%Furthermore, while the sufficient-memory regime may alleviate forgetting, the resulting hybrid dataset $D_t \cup \mathcal{M}_{1:t-1}$ introduces new challenges (e.g., gradient interference and slower convergence). 
%
%Another issue is that standard continual learning methods fall short when it comes to handling these multi-task dynamics, thereby necessitating a new strategy to address the problem. More importantly, existing algorithms 














\begin{comment}

\paragraph{Larger memory can amplify intra-task forgetting}

While a sufficient memory regime helps address inter-task forgetting, the diversity of the hybrid dataset (e.g., $D_t \cup \mathcal{M}_{1:t-1}$ at step $t$) can adversely affect subpopulations within the current task $t$, which we denote as intra-task forgetting. 

When the memory is large, the training data now consists of diverse subpopulations within the hybrid dataset, which raises the chances of gradient collision. Concretely, if a subpopulation (e.g., current task $D_t$) $m$ task a gradient direction $\nabla \ell_{m}(\theta)$ and a non-identical subpopulation $m' \neq m$ has $\nabla \ell_{m'}(\theta)$ pointing in a different direction, gradient steps may partially undo each other \citep{yu2020gradient}. This collision can degrade accuracy on subpopulation $m$ after the model subsequently focuses on subpopulation $m'$.

Formally, assume task $t$ consists of $M$ distinct subpopulations in hybrid dataset ($D_t \cup \mathcal{M}_{1:t-1}$). Let these subpopulations $\{P_t^1, P_t^2, \cdots, P_t^M \}$. Here, the subpopulation may be interpreted as, but not limited to the tasks. Let $g_m(\theta) = \nabla_{\theta} L_m(\theta)$ the expected gradient of the subpopulation $m$ at parameter $\theta$, where $L_{m}(\theta) = \mathbb{E}_{(x,y)\sim P_{t}^m}[\ell(\theta;x,y)]$. During training, samples from various subpopulations are included in the training batch with proportions $\gamma_1, \cdots, \gamma_{M}$ (where $\sum_{m=1}^{M}\gamma_m = 1$, making the expected gradient as: $g(\theta) = \sum_{m=1}^{M}\gamma_mg_m(\theta)$.

Next, consider a stochastic gradient $\tilde{g}(\theta)$ computed on a mini-batch drawn from the mixture distribution. It is an unbiased estimator of $g(\theta)$ but its variance is influenced by the diversity of the batch. Then, the variance of the gradient can be decomposed as:
\begin{equation}
    Var[\tilde{g}(\theta)] = \mathbb{E} \bigr[\lVert \tilde{g}(\theta) - g(\theta)\lVert \bigr] \approx \sum_{m=1}^M \gamma_m^2 Var[g_m(\theta)] + \sum_{m\neq m'} \gamma_m \gamma_{m'} \langle g_m(\theta) - g(\theta), g_{m'}(\theta) - g(\theta) \rangle,
\end{equation}
where the first term aggregates variances within each subpopulation and the second term is a sum over pairwise covariances between gradients from different subpopulations. These covariance terms reflect how updates from different subpopulations interact.

Next, we analyze the effect of the increased memory. Larger memory allows the training batch to include a wider variety of samples from different subpopulations, effectively increasing $M$ and capturing more heterogenous $g_m(\theta)$s. As $M$ increases, the number of pairwise covariance $\langle g_m(\theta) - g(\theta), g_{m'}(\theta) - g(\theta) \rangle$ grows. If subpopulations $m$ and $m'$ are diverse such that their gradients point in conflicting directions, the inner project $\langle g_m(\theta) - g(\theta), g_{m'}(\theta) - g(\theta) \rangle$ can be negative. Naturally, the more diverse the data is, the probability of such conflicting pairs simultaneously increases, making the gradient descent steps noisy \citep{guo2025out,goldfarb2024joint}. Intuitively, this phenomenon means that while the model is learning to minimize loss for one subpopulation, it could be forgetting on another subpopulation i.e., intraâ€task forgetting \citep{doan2021theoretical,kumar2022finetuningdistortpretrainedfeatures}.

%relate to class imbalance

\end{comment}









