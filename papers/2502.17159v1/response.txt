\section{Related Work}
\subsection{Multimodal Large Language Models}
With the surge in data volume and model size, large language models~(LLMs) **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** have shown their powerful performance. They are constructed with decoder-only blocks and respond to inputs in an auto-regressive way, which shows their potential on both classification **Vaswani et al., "Attention Is All You Need"** and generative tasks **Holtzman et al., "The Curious Case of Neural Text Degeneration"**. Furthermore, multimodal large language models~(MLLMs) enhance the large models with vision perception ability. They obtain visual features with vision encoder and align image-text features with a cross-modality module like linear projection **Carion et al., "End-to-End Object Detection with Transformers"**, Q-former **Tang et al., "Qformer: Query-based Graph Attention Networks for Visual Question Answering"** and so on. Current research on large models is dedicated to direct fine-tuning one independent model with task-specific data to get better results. Rather than improving performance of certain domain, we focus on integrating models into one model to boost efficiency and handle multiple tasks simultaneously.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/figure_motivation_distribution.pdf}
    \vspace{-25pt}
    \caption{(a) Impact of task-oriented eigenvalues for low-rank decomposition. Experiments are conducted on ScienceQA and ImageNet. (b) Effectiveness of \modelname{} by adaptively reducing interference with larger scale on smaller singular values. (c) Distribution of full fine-tuning and parameter efficient modules. Parameters of FFT, and different components in efficient tuning have different distributions.}
    \label{fig:motivation-distribution}
    \vspace{-15pt}
\end{figure*}

\subsection{Parameter Efficient Tuning}
When fine-tuning pre-trained model with task-specific data, training the whole model would not only disrupt the representations obtained from billions of data but also become resource intensive. To address the issue, parameter efficient tuning **Lan et al., "PfeT: Parameter-efficient Transfer Learning"** is introduced to refrain from fine-tuning the whole model. It typically trains lightweight modules to make model adapt to downstream tasks and achieves competitive results compared to full fine-tuning models. Various efficient tuning techniques have been explored like prompt learning **Shin et al., "Eliciting Knowledge Graphs from Pre-trained Language Models"**, adapter learning including LoRA **Lan et al., "LoRA: Low-Rank Adaptation for Long-Tailed Problems"** , (IA)$^3$  and so on. In this paper, we focus on LoRA, as it is the most commonly utilized PEFT method and has demonstrated its usefulness especially for large models **Brown et al., "Language Models are Few-Shot Learners"**.

\subsection{Model Merging}
Model merging **Sun et al., "Task-Aware Weight Sharing for Multitask Learning"** refers to merging multiple models of different capabilities to handle multi-task learning with one universal model. Task Arithmetic **Zhang et al., "Task Arithmetic: A Unified Framework for Multi-Task Learning"** presents a paradigm that obtains task vectors from subtracting pre-trained model from fine-tuned model and treats model merging as arithmetic operations of task vectors. It has gained widespread attention in various fields **Chen et al., "Graph-Based Multi-Task Learning"**.
Ties-merging  trims and elects sign to reduce interference. DARE **Sun et al., "DARE: Dynamic Adaptation for Rare Classes"** randomly drops parameters and rescales the remaining ones to approximate the original embedding. PCB-merging  introduces a parameter adjustment with competition balancing to address potential conflicts. However, most of them focus on merging models with FFT on classification tasks **Liu et al., "Faster R-CNN: Towards Real-Time Object Detection"** , and the distribution shift prevents their ability to acquire satisfying performance. By contrast, we focus on parameter efficient model merging with multimodal tasks.