\section{Related Work}
\subsection{Multimodal Large Language Models}
With the surge in data volume and model size, large language models~(LLMs)____ have shown their powerful performance. They are constructed with decoder-only blocks and respond to inputs in an auto-regressive way, which shows their potential on both classification____ and generative tasks____. Furthermore, multimodal large language models~(MLLMs) enhance the large models with vision perception ability. They obtain visual features with vision encoder and align image-text features with a cross-modality module like linear projection____, Q-former____ and so on. Current research on large models is dedicated to direct fine-tuning one independent model with task-specific data to get better results. Rather than improving performance of certain domain, we focus on integrating models into one model to boost efficiency and handle multiple tasks simultaneously.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/figure_motivation_distribution.pdf}
    \vspace{-25pt}
    \caption{(a) Impact of task-oriented eigenvalues for low-rank decomposition. Experiments are conducted on ScienceQA and ImageNet. (b) Effectiveness of \modelname{} by adaptively reducing interference with larger scale on smaller singular values. (c) Distribution of full fine-tuning and parameter efficient modules. Parameters of FFT, and different components in efficient tuning have different distributions.}
    \label{fig:motivation-distribution}
    \vspace{-15pt}
\end{figure*}

\subsection{Parameter Efficient Tuning}
When fine-tuning pre-trained model with task-specific data, training the whole model would not only disrupt the representations obtained from billions of data but also become resource intensive. To address the issue, parameter efficient tuning____ is introduced to refrain from fine-tuning the whole model. It typically trains lightweight modules to make model adapt to downstream tasks and achieves competitive results compared to full fine-tuning models. Various efficient tuning techniques have been explored like prompt learning____, adapter learning including LoRA ____, (IA)$^3$____ and so on. In this paper, we focus on LoRA, as it is the most commonly utilized PEFT method and has demonstrated its usefulness especially for large models____.

\subsection{Model Merging}
Model merging____ refers to merging multiple models of different capabilities to handle multi-task learning with one universal model____. Task Arithmetic____ presents a paradigm that obtains task vectors from subtracting pre-trained model from fine-tuned model and treats model merging as arithmetic operations of task vectors. It has gained widespread attention in various fields____.
Ties-merging____ trims and elects sign to reduce interference. DARE____ randomly drops parameters and rescales the remaining ones to approximate the original embedding. PCB-merging____ introduces a parameter adjustment with competition balancing to address potential conflicts. However, most of them focus on merging models with FFT on classification tasks____, and the distribution shift prevents their ability to acquire satisfying performance____. By contrast, we focus on parameter efficient model merging with multimodal tasks.