%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subfig}
\usepackage{booktabs} % for professional tables
\usepackage[dvipsnames,svgnames]{xcolor}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{makecell}
\def\modelname{CoPA-Merging}

\newcommand{\bftab}{\fontseries{b}\selectfont}
\RequirePackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot}
\def\ie{\emph{i.e}\onedot} 

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}


\newcommand{\ls}[1]{{\color{red}{\bf\sf [LS: #1]}}}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Towards Generalizable Parameter Efficient Model Merging with Effective Inter-Parameter Adaptation}
\icmltitlerunning{Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation}

\begin{document}

\twocolumn[
\icmltitle{Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Fanhu Zeng}{casia}
\icmlauthor{Haiyang Guo}{casia}
\icmlauthor{Fei Zhu}{cair}
\icmlauthor{Li Shen}{sysu}
\icmlauthor{Hao Tang}{pku}
\end{icmlauthorlist}

\icmlaffiliation{casia}{State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences}

\icmlaffiliation{cair}{Centre for Artificial Intelligence and Robotics, HKISI-CAS}
\icmlaffiliation{sysu}{School of Cyber Science and Technology, Sun Yat-sen University}
\icmlaffiliation{pku}{School of Computer Science, Peking University}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks. Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity.  With the expansion in data and model size, parameter efficient tuning becomes the common practice for obtaining task-specific models efficiently. However, we observe that existing methods designed for full fine-tuning merging fail under efficient tuning. To address the issues, we analyze from low-rank decomposition and reveal that maintaining direction and compensating for gap between singular values are crucial for efficient model merging. Consequently, we propose \modelname{}, a training-free parameter efficient merging method with complementary parameter adaptation. Specifically, we \textbf{(1)} prune parameters and construct scaling coefficients from inter-parameter relation to compensate for performance drop from task interference and \textbf{(2)} perform cross-task normalization to enhance unseen task generalization. We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certificate the outstanding performance and generalizability of our method. Additional study and extensive analyses further showcase the effectiveness. 
\end{abstract}

\section{Introduction}
Rapid development of foundation models has facilitated the construction of expert model from custom data.  Modern models like large language model~(LLMs) are pre-trained on various datasets to obtain general knowledge and employing pre-trained models typically involves fine-tuning on task-specific data to gain ability on specific areas. 
When dealing with tasks of different domains, multi-task learning~\cite{sanh2022multitask} is a common paradigm to mitigate performance variations. However, particular knowledge may be required progressively over time. As the model becomes larger~\cite{dehghani2023scaling, wang2024qwen2}, once the model is specialized on specific datasets, it is time consuming and resource intensive to retrain models to get knowledge of another area, even encountering catastrophic forgetting~\cite{zeng2024modalprompt}. Furthermore, issues regarding data privacy may obstacle its practical application. To address these issues, model merging~\cite{stoica2024zipit} has been proposed to integrate multiple separate models of specific knowledge off-the-shelf into one model with multi-task ability without the demand of training or accessing data. Its effectiveness and convenience show great potential in various downstream tasks~\cite{guo2024desire,shah2025ziplora}. 

\begin{figure}
    \centering
    \vspace{-3pt}
    \includegraphics[width=1.0\linewidth]{figure/figure_overall.pdf}
    \vspace{-30pt}
    \caption{Balance between seen task enhancement and unseen task generalization on multimodal tasks.}
    \vspace{-20pt}
    \label{fig:overall-performance}
\end{figure}

Despite its popularity, crucial problems for model merging remain unsolved, hindering its real-world application. First, with larger model size like multimodal large language models~(MLLMs) and massive data, parameter efficient tuning~(PEFT)~\cite{hu2022lora} has become the most popular and effective tuning method for large models. However, existing model merging method focuses on combining full fine-tuning~(FFT) models~\cite{yadav2024ties, guodong2024pcb}, which is challenging in distribution shift and undergoes performance drop when directly applied to parameter efficient model merging, as is illustrated in~\cref{fig:overall-performance}. Another issue lies in that current high-performance methods rely on extra information of seen tasks~(\eg, validation data~\cite{yang2024adamerging}, extra storage~\cite{huang2024emr}) to boost the performance. Therefore, they can only handle seen tasks and fail to generalize to unseen tasks, questioning their robustness, as is concluded in~\cref{tab:prerequisites}. 
The most related work is LoraHub~\cite{huang2023lorahub}. However, its requirement for coefficient optimization through test time sample adaptation severely hinders its application.

In this paper, we analyze the reason behind the performance drop. We observe \textbf{(1) distinct wider distribution} and \textbf{(2) task-oriented singular values} in efficient parameters that differ two ways of fine-tuning. To handle the problem, we propose \modelname{}, a novel parameter efficient model merging method for high-performance merging of multimodal large models. Starting from the perspective of principal directions and low-rank decomposition, we introduce effective complementary\footnote{In contrast to \textbf{individual}, we use the term to distinguish between subspace multiplication~(along $r$ dimension) and original multiplication~(along $d_i/d_o$ dimension) of matrix.} parameter adaptation to both enhance ability for seen tasks and 
generalization on unseen tasks. Specifically, we prune ineffective parameters and construct scaling coefficients from inter-parameter relation to to mitigate interference between tasks and bridge potential performance gap resulting from merging. Additionally, we perform cross-task normalization to balance tasks of different data scales and enhance unseen task generalization. It is notable that our method is \textbf{free from any additional data or storage}, which equips the method with more flexibility.

We conduct experiments on a benchmark consisting of eight seen tasks and four unseen tasks with diverse fields to evaluate the ability on multimodal generative tasks. We also report results on common evaluation benchmarks and it shows that our model achieves superior performance on both seen~(\textbf{3.4\%}), unseen tasks~(\textbf{4.5\%}) and comprehensive evaluation benchmark with a substantial margin, demonstrating the effectiveness and generalizability of our method. We additionally perform experiments on vision tasks along with extensive analyses to validate the utility of our method. 


Our contributions are summarized as follows:
\begin{itemize}
    \item We focus on parameter efficient merging for multimodal large language models, highlighting the distinct difference between two types of fine-tuning methods.
    \item We analyze from the perspective of principle directions of singular values in low rank decomposition and propose an effective training-free merging algorithm with complementary parameter adaptation for both seen task enhancement and unseen task generalization.
    \item We conduct extensive experiments and achieve superior results compared to existing approaches, which strongly validates the effectiveness and generalizability of the proposed method.
\end{itemize}

\begin{table}[]
    \centering
    \vspace{-8pt}
    \caption{Prerequisites and application scope of different methods.}
    \setlength\tabcolsep{2pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule[1.3pt]
        \multirow{2}{*}{Methods} & Validation & Extra Storage & Unseen & Parameter Efficient \\
        & Free & Free & Tasks & Merging\\
        \midrule
        Task Arithmetic & \Large{\ding{55}} & $\Large{\checkmark}$ & $\Large{\checkmark}$ & \Large{\ding{55}}\\
        DARE & $\Large{\checkmark}$ & $\Large{\checkmark}$ & $\Large{\checkmark}$ & \Large{\ding{55}}\\
        Ties-merging & $\Large{\checkmark}$& $\Large{\checkmark}$  & $\Large{\checkmark}$ & \Large{\ding{55}}\\
        Pcb-Merging & $\Large{\checkmark}$ & $\Large{\checkmark}$ & $\Large{\checkmark}$ & \Large{\ding{55}} \\
        \midrule
        LoraHub& \Large{\ding{55}}&\Large{\ding{55}}&$\Large{\checkmark}$&$\Large{\checkmark}$ \\
        AdaMerging & \Large{\ding{55}} & $\Large{\checkmark}$ & $\Large{\checkmark}$ & \Large{\ding{55}}\\
        EMR-Merging &$\Large{\checkmark}$ & \Large{\ding{55}}& \Large{\ding{55}} & \Large{\ding{55}}\\
        \midrule
        \textbf{\modelname{}} & $\Large{\checkmark}$& $\Large{\checkmark}$& $\Large{\checkmark}$ & $\Large{\checkmark}$ \\
        \bottomrule[1.3pt]
    \end{tabular}}
    \vspace{-22pt}
    \label{tab:prerequisites}
\end{table}

\section{Related Work}
\subsection{Multimodal Large Language Models}
With the surge in data volume and model size, large language models~(LLMs)~\cite{radford2019language,touvron2023llama,alayrac2022flamingo} have shown their powerful performance. They are constructed with decoder-only blocks and respond to inputs in an auto-regressive way, which shows their potential on both classification~\cite{wang2018glue} and generative tasks~\cite{goyal2017vqa}. Furthermore, multimodal large language models~(MLLMs) enhance the large models with vision perception ability. They obtain visual features with vision encoder and align image-text features with a cross-modality module like linear projection~\cite{liu2024visual}, Q-former~\cite{li2023blip} and so on. Current research on large models is dedicated to direct fine-tuning one independent model with task-specific data to get better results. Rather than improving performance of certain domain, we focus on integrating models into one model to boost efficiency and handle multiple tasks simultaneously.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/figure_motivation_distribution.pdf}
    \vspace{-25pt}
    \caption{(a) Impact of task-oriented eigenvalues for low-rank decomposition. Experiments are conducted on ScienceQA and ImageNet. (b) Effectiveness of \modelname{} by adaptively reducing interference with larger scale on smaller singular values. (c) Distribution of full fine-tuning and parameter efficient modules. Parameters of FFT, and different components in efficient tuning have different distributions.}
    \label{fig:motivation-distribution}
    \vspace{-15pt}
\end{figure*}

\subsection{Parameter Efficient Tuning}
When fine-tuning pre-trained model with task-specific data, training the whole model would not only disrupt the representations obtained from billions of data but also become resource intensive. To address the issue, parameter efficient tuning~\cite{han2024parameter} is introduced to refrain from fine-tuning the whole model. It typically trains lightweight modules to make model adapt to downstream tasks and achieves competitive results compared to full fine-tuning models. Various efficient tuning techniques have been explored like prompt learning~\cite{jia2022visual, khattak2023maple}, adapter learning including LoRA ~\cite{hu2022lora,wu2024moslora}, (IA)$^3$~\cite{liu2022few} and so on. In this paper, we focus on LoRA, as it is the most commonly utilized PEFT method and has demonstrated its usefulness especially for large models~\cite{liu2024visual}.

\subsection{Model Merging}
Model merging~\cite{yang2024model,sung2023empirical} refers to merging multiple models of different capabilities to handle multi-task learning with one universal model~\cite{jin2023regmean,matena2022fisher}. Task Arithmetic~\cite{ilharco2023editing} presents a paradigm that obtains task vectors from subtracting pre-trained model from fine-tuned model and treats model merging as arithmetic operations of task vectors. It has gained widespread attention in various fields~\cite{tang2024wemoe}.
Ties-merging~\cite{yadav2024ties} trims and elects sign to reduce interference. DARE~\cite{yu2024dare} randomly drops parameters and rescales the remaining ones to approximate the original embedding. PCB-merging~\cite{guodong2024pcb} introduces a parameter adjustment with competition balancing to address potential conflicts. However, most of them focus on merging models with FFT on classification tasks~\cite{deng2009imagenet}, and the distribution shift prevents their ability to acquire satisfying performance~\cite{tang2024parameter}. By contrast, we focus on parameter efficient model merging with multimodal tasks.


\section{Methodology}
\subsection{Preliminary and Notations}

\textbf{Parameter efficient tuning} keeps the pre-trained model frozen and fine-tunes a lightweight module to adapt to downstream tasks. In this paper, we focus on LoRA~\cite{hu2022lora}, a low-rank adaptation technique that decomposes additional parameters into two low-rank matrices. Formally, for a weight matrix $\boldsymbol{\mathrm{W}_0}\in \mathbb{R}^{d_o \times d_i}$, updated matrix is depicted as:
\begin{equation}
    \boldsymbol{\mathrm{W}} = \boldsymbol{\mathrm{W}_0}+\Delta \boldsymbol{\mathrm{W}} = \boldsymbol{\mathrm{W}_0}+ \boldsymbol{\mathrm{B}}\cdot\boldsymbol{\mathrm{A}},
\end{equation}
where $\boldsymbol{\mathrm{B}}\in \mathbb{R}^{d_o\times r}$, $\boldsymbol{\mathrm{A}}\in \mathbb{R}^{r\times d_i}$ and rank $r\ll \mathrm{min}(d_i,d_o)$.

\textbf{Model merging} targets at combining multiple models of the same structure $\{\theta_1,\cdots,\theta_N\}$  that are fine-tuned from pre-trained model $\theta_{pre}$ into one new model $\theta_{m}$ and maintaining multi-task ability in a training-free manner. Existing full fine-tuning~(FFT) methods follow the paradigm proposed by Task Arithmetic/TA~\cite{ilharco2023editing}. Typically, they construct the task vector by performing a subtraction operation $\tau_n=\theta_n-\theta_{pre} \in \mathbb{R}^d$, conduct merging algorithm on task vector subspace and obtain the final merged model by adding pre-trained model, \ie, $\theta_m=\theta_{pre}+\lambda\sum_{n=1}^{N}\Phi(\tau_n)$, where $\Phi(\cdot)$ stands for merging algorithm.

\textbf{Parameter efficient model merging} differs from traditional model merging, as the backbone of foundation model is frozen and the updated matrices to be merged are randomly initialized. Consequently, we use $\Delta \boldsymbol{\mathrm{W}}$ to represent merging modules for simplification, and exploit model merging method on these parameter efficient modules, \ie, $\boldsymbol{\mathrm{W}}_m=\boldsymbol{\mathrm{W}_0} +\lambda\sum_{n=1}^{N}\Phi(\Delta \boldsymbol{\mathrm{W}}_n)$.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.93\linewidth]{figure/figure_structure_v3.pdf}
    \vspace{-7pt}
    \caption{Diagram of parameter efficient model merging. Tasks are divided into seen and unseen ones. Checkpoints of seen tasks are trained employing the standard individual training and are merged following the pipeline of inter-parameter adaptation. During inference, the merged model is required to both enhance seen tasks and be generalizable to unseen tasks with unknown distribution.}
    \vspace{-10pt}
    \label{fig:main-structure}
\end{figure*}

\subsection{Motivation and Observation}
\label{sec:motivation}
While existing methods perform well on FFT merging, challenges remain unsolved when it comes to PEFT merging with suboptimal performance. To have a better understanding of the difference, we analyze from parameter distribution and low rank decomposition of specific task, and then reveal key factors for parameter efficient merging.

\textbf{Task-oriented singular values count for specific task.} Low rank is inherent property of LoRA and singular value decomposition~(SVD) can be performed to obtain principal eigenvalues and corresponding directions. We hypothesize that for a specific downstream task, the principal direction corresponding to several related singular values are crucial directions for task optimization, and the performance is mostly determined by these directions and magnitude. To verify the hypothesis, we conduct an experiment by \textbf{only retain larger half singular values} of the $\boldsymbol{\mathrm{W}}$ after SVD decomposition~(set smaller half of the eigenvalues to zero), and then evaluate the performance of downstream tasks. Results in~\cref{fig:motivation-distribution}\textcolor{mydarkblue}{a} observe \textbf{almost no decrease}. This validates the above assumption and that small singular values correspond to the directions of other tasks or general knowledge. 

Therefore, it can be indicated that conflict of merging efficient tuning model mainly comes from the large gap between the singular values corresponding to the main direction and secondary direction in low-rank space, and mitigating the gap is crucial for resolving the interference between different tasks when merging. This can be confirmed by~\cref{fig:motivation-distribution}\textcolor{mydarkblue}{b}, which clearly shows that our method adaptively adjusts the singular values with scaling smaller singular values by a larger multiple and achieves better performance.

\textbf{Parameters of efficient modules have distinct distributions.} We also depict the distribution of elements in~\cref{fig:motivation-distribution}\textcolor{mydarkblue}{c} to figure out the difference between two types of merging. It can be found that most parameters of full fine-tuning have much smaller and concentrated distribution~(distribution in \textcolor{blue}{dark blue}), where the problem of sign conflict becomes particularly prominent~\cite{yadav2024ties}. Conversely, parameters in efficient components have a relatively wider range of distribution~(\textcolor{SkyBlue}{light blue} and \textcolor[rgb]{0.5,0.5,0.5}{gray}), and principal direction shift rather than sign conflict is the main issue for interference between tasks, which we give a detailed comparison in~\cref{sec:analysis}.


\textbf{Parameter efficient modules have intrinsic relations.}  The two LoRA matrices have asymmetric functions in PEFT~\cite{zhu2024asymmetry, tian2024hydralora}. As pointed out by AsymmetryLoRA~\cite{zhu2024asymmetry}, a random untrained $\boldsymbol{\mathrm{A}}$ performs as well as a fine-tuned one and $\boldsymbol{\mathrm{B}}$ improves the bound. HydraLoRA~\cite{tian2024hydralora} reveals that shared $\boldsymbol{\mathrm{A}}$ can reserve knowledge. To determine the distinct function of the two matrices in merging, we depict the distribution of $\boldsymbol{\mathrm{A}}$, $\boldsymbol{\mathrm{B}}$ respectively in~\cref{fig:motivation-distribution}\textcolor{mydarkblue}{c}. It turns out that $\boldsymbol{\mathrm{B}}$ follows a Gaussian distribution and $\boldsymbol{\mathrm{A}}$ has an approximately uniform distribution. It corresponds with existing research that $\boldsymbol{\mathrm{B}}$ also has more unique and crucial role in PEFT merging.

\subsection{\modelname{}: Parameter Efficient Merging} 
\label{sec:detail-method}
Motivated by the observations, we introduce a novel model merging method for parameter efficient components, which targets at maintaining directions and compensating for gap between singular values. Our approach is divided into \textbf{pruning and complementary parameter scaling} and \textbf{cross-task normalization}, which is illustrated in~\cref{fig:main-structure}.

\textbf{Pruning and Complementary Parameter Scaling.} Due to significantly wider distributions, larger parameters are more likely to change directions in low-rank space. Therefore, rather than electing parameters of the same sign~\cite{yadav2024ties,huang2024emr} with delicate design, we simplify the definition of ineffective parameter to be those with small values in magnitude. In this way, direction of matrices is not greatly changed by reserving larger parameters, and knowledge of specific task is therefore retained. Consequently, the updated matrices can be formulated as:
\begin{equation}
\begin{aligned}
    \widetilde{\boldsymbol{\mathrm{A}}} =& \boldsymbol{\mathrm{M}}_\mathrm{A}(k) \odot \boldsymbol{\mathrm{A}}, \\
    \widetilde{\boldsymbol{\mathrm{B}}} =& \boldsymbol{\mathrm{M}}_\mathrm{B}(k) \odot \boldsymbol{\mathrm{B}}, \\
    \end{aligned}
\end{equation}
where $\odot$ stands for element-wise multiplication, and $k$ is the pruning rate of parameters. $\boldsymbol{\mathrm{M}}(\cdot)$ is the binary operation matrix. Formally, it sets those $k$ percentage parameter with small values sorted by magnitude to zero.

After pruning ineffective parameter, remaining ones should be refined to complement the performance gap caused by task interference.
Inspired by the asymmetry and correlation between two LoRA modules, we propose to adaptively adjust the coefficients through complementary parameter scaling for transforming $\boldsymbol{\mathrm{B}}$ to compensate for performance deficiencies resulting from gap between singular values. As $\boldsymbol{\mathrm{A}}$ follows a uniform distribution, we can construct scaling coefficients from statistical characteristics of $\boldsymbol{\mathrm{A}}$. We define scaling matrix $\boldsymbol{\mathrm{S}}$ as a diagonal matrix, and the elements on the diagonal are:
\begin{equation}
    \footnotesize{\boldsymbol{\mathrm{S}}^i = \frac{\sum_{j=1}^{d_i} \mathrm{abs}(\boldsymbol{\mathrm{A}}_{[i,j]})}{\sum_{j=1}^{d_i} \mathrm{abs}(\boldsymbol{\mathrm{M}}_{\mathrm{A}[i,j]} \odot \boldsymbol{\mathrm{A}}_{[i,j]})}, i=1,\cdots,r.}
\end{equation}
This can be viewed as eigenvalue adaptation in low-rank space, in which small eigenvalues of each model are increased in larger proportions~(\cref{fig:motivation-distribution}\textcolor{mydarkblue}{b}), thereby contributing to minimizing the gap aroused by task conflicts.

\textbf{Cross-Task Normalization.} Complementary parameter scaling coefficient $\boldsymbol{\mathrm{S}}$ is determined in an task-independent manner. On the one hand, the imbalance in data size between different seen tasks leads to overfitting for data-abundant tasks and underfitting for data-scarce tasks. Additionally, it also poses a negative effect on the generalization to unseen tasks. Consequently, we conduct normalization on scaling matrices across all tasks to reduce the impact of coefficient imbalance, mathematically formulated as:
\begin{equation}
    \widetilde{\boldsymbol{\mathrm{S}}}_n^i = \boldsymbol{\mathrm{S}}_n^i \ / \sum_{n=1}^N{\boldsymbol{\mathrm{S}}}_n^i, n=1,\cdots,N.
\end{equation}
The normalization provides more balance across diverse types of tasks and therefore achieves more stable performance. It also enhances the ability on unseen tasks.

The final efficient parameter can be rewritten as follows:
\begin{equation}
    \Delta \widetilde{\boldsymbol{\mathrm{W}}}_n = \widetilde{\boldsymbol{\mathrm{B}}}_n\cdot \widetilde{\boldsymbol{\mathrm{S}}}_n\cdot \widetilde{\boldsymbol{\mathrm{A}}}_n,n=1,\cdots,N,
\end{equation}
and the merged model weights can be obtained by adding the merged parameter efficient modules of all tasks. It should be emphasized that during the whole merging process, no validation data or extra information storage of seen tasks is required, certificating the superiority of the method.

\begin{table*}[ht]
    \centering
    \caption{Performance of multimodal generative tasks with merging methods on eight seen and four unseen tasks.}
    \renewcommand{\arraystretch}{1.25}
    \setlength\tabcolsep{2pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccc cccc |c|cccc|c}
    \toprule[1.3pt]
    & \multicolumn{9}{c|}{\begin{sc}
        Seen Tasks
    \end{sc}} & \multicolumn{5}{c}{\begin{sc}Unseen Tasks\end{sc}} \\
    \midrule
    Method & ScienceQA & ImageNet
& VQAv2 & Grounding &OCRVQA&VizWiz& Flickr30k &IconQA & \textbf{Avg}&AOKVQA& ImageNet-R &Screen2W& TabMWP& \textbf{Avg}
\\
\midrule
       Zero-Shot  & 61.73	&40.87&	62.88&36.10& 41.16&41.03&49.07&14.09 &43.37&51.62&28.27&5.98&15.01&25.22\\
       Individual& 83.74&96.02&67.58&43.40&65.50&64.80&57.29&75.54&69.23&-&-&-&-&-\\
       Traditional MTL &76.90&74.08&67.05&35.98&65.37&66.67&56.09&66.87&63.62&76.33&41.39&8.34&18.20&36.06\\ 
       \midrule
       Task Arithmetic & 71.94&57.49&67.06&38.90&62.87&44.80&49.20&39.21&53.93&74.78&37.37&7.52&13.57&33.31\\
        DARE & 71.59&57.25&66.26&39.38&62.56&44.93&49.13&39.59&53.84&73.75&37.67&7.56&13.62&33.15\\
        Ties-merging &71.49&55.88&66.73&39.67&\bftab{65.12}&44.35&47.06&34.46&53.09&73.43&38.44&7.47&13.23&33.14 \\
        PCB-merging & 71.10&57.82&\bftab{67.59}&38.22&64.35&44.58&48.90&37.01&53.70&74.57&36.28&7.84&15.44&33.53 \\
        \midrule
        \textbf{\modelname{}} & \bftab{73.43}&\bftab{65.54}&67.20&\bftab{44.80}&62.97&\bftab{46.61}&\bftab{52.80}&\bftab{45.90}&\bftab{57.33}~\textcolor[rgb]{0.81,0,0}{\textbf{(+3.4)}}&\bftab{79.30}&\bftab{45.79}&\bftab{9.23}&\bftab{17.62}&\bftab{37.99}~\textcolor[rgb]{0.81,0,0}{\textbf{(+4.5)}}\\
        \bottomrule[1.3pt]
    \end{tabular}}
    \vspace{-10pt}
    \label{tab:mllm-results}
\end{table*}



\section{Experiments}
\subsection{Setup and Implementation Details}
We conduct experiments on multimodal generative tasks, unseen task generalization and vision tasks using multimodal models~\cite{liu2024visual, radford2021learning}. We comprehensively extend our approach on the scale of the model, number of tasks, rank and so on to certificate the utility. Unless otherwise stated, all models are trained with rank to be 16.


\subsection{Datasets and Baselines}
For multimodal task merging, we establish a benchmark comprises of eight multimodal generative tasks including ScienceQA~\cite{lu2022scienceqa}, ImageNet~\cite{deng2009imagenet}, VQAv2~\cite{goyal2017vqa}, OCRVQA~\cite{mishra2019ocr}, Flickr30k~\cite{bryan2017flickr}, VizWiz-caption~\cite{gurari2018vizwiz}, IconQA~\cite{lu2021iconqa}
, RefCOCO~\cite{kazemzadeh2014referitgame, mao2016generation}. It includes diverse types of multimodal tasks like question answering~(QA), grounding, classification, captioning across various areas, and can comprehensively evaluate the performance of different merging methods in generative tasks. 

\begin{algorithm}[tb]
   \caption{Procedure of parameter efficient merging with complementary parameter adaptation.}
   \label{alg:peftmerging}
\begin{algorithmic}
   \STATE {\bfseries Input:} Fine-tuned models $\{\boldsymbol{\mathrm{A}}_n$,$\boldsymbol{\mathrm{B}}_n\}_{i=1}^n$, pruning rate k
   \STATE {\bfseries Output:} Merged Parameter Efficient Model $\boldsymbol{\mathrm{W}}$

    {$\triangleright$ \ \scriptsize{Step 1: Pruning and Complementary Parameter Scaling.}}
    \STATE \quad $\boldsymbol{\mathrm{M}}_\mathrm{A}(k)=\mathrm{binary}(\mathrm{set\_topk\_nonzero}(\boldsymbol{\mathrm{A}},k))$
    \STATE \quad $\boldsymbol{\mathrm{M}}_\mathrm{B}(k)=\mathrm{binary}(\mathrm{set\_topk\_nonzero}(\boldsymbol{\mathrm{B}},k))$
    \STATE \quad $\widetilde{\boldsymbol{\mathrm{A}}} = \boldsymbol{\mathrm{M}}_\mathrm{A}(k) \odot \boldsymbol{\mathrm{A}}$
    \STATE \quad $\widetilde{\boldsymbol{\mathrm{B}}} =\boldsymbol{\mathrm{M}}_\mathrm{B}(k) \odot \boldsymbol{\mathrm{B}}$
    \STATE \quad \textbf{for} {$i=1,\cdots,r$} \textbf{do}
    \STATE \quad \quad \footnotesize$\boldsymbol{\mathrm{S}}^i = \sum_{j=1}^{d_i} \mathrm{abs}(\boldsymbol{\mathrm{A}}_{[i,j]}) / \sum_{j=1}^{d_i} \mathrm{abs}(\boldsymbol{\mathrm{M}}_{\mathrm{A}[i,j]} \odot \boldsymbol{\mathrm{A}}_{[i,j]})$
    \STATE \quad \textbf{end for}
    
   {$\triangleright$ \ 
       \scriptsize{Step 2: Cross-Task Normalization.}}
   \STATE \quad \textbf{for} {$n=1,\cdots,N$} \textbf{do}
   \STATE \quad \quad $\widetilde{\boldsymbol{\mathrm{S}}}_n^i = \boldsymbol{\mathrm{S}}_n^i \ / \sum_{n=1}^N{\boldsymbol{\mathrm{S}}}_n^i$
   \STATE \quad \textbf{end for}
       
   {$\triangleright$ \ \scriptsize{Obtain parameter efficient modules.}}
   \STATE \quad \textbf{for} {$n=1,\cdots,N$} \textbf{do}
    \STATE \quad \quad $\widetilde{\boldsymbol{\mathrm{S}}}_n=\mathrm{Diag}(\boldsymbol{\mathrm{S}}_n^i)$
    \STATE \quad \quad $\Delta \widetilde{\boldsymbol{\mathrm{W}}}_n \leftarrow \widetilde{\boldsymbol{\mathrm{B}}}_n\cdot\widetilde{\boldsymbol{\mathrm{S}}}_n\cdot\widetilde{\boldsymbol{\mathrm{A}}}_n$
    \STATE \quad \textbf{end for}
    
    {$\triangleright$ \ \scriptsize{Merge parameter efficient modules.}}
    \STATE \quad $\boldsymbol{\mathrm{W}} \leftarrow \boldsymbol{\mathrm{W}_0} + \lambda \sum_{n=1}^{N}\Delta \widetilde{\boldsymbol{\mathrm{W}}}_n$ 
   
   \textbf{return} $\boldsymbol{\mathrm{W}}$
\end{algorithmic}
\end{algorithm}

\begin{table}[ht]
    \centering
    \vspace{-10pt}
    \setlength\tabcolsep{3pt}
    \footnotesize
    \caption{Performance of different merging models on comprehensive evaluation benchmarks.}
    \vspace{5pt}
    \begin{tabular}{l>{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{2cm}}
    \toprule[1.3pt]
    Method & POPE & MME & MMBench\\
\midrule
       Zero-Shot  & 86.4	&1476.9&66.1\\
       Traditional MTL &86.9&1433.5&62.9\\ 
       \midrule
       Task Arithmetic & 87.0&1465.2&67.3\\
        DARE & 86.4&1475.7&67.4\\
        Ties-merging &86,7&1489.4&66.6\\
        PCB-merging & 86.6&1490.7&66.3\\
        \midrule
        \textbf{\modelname{}} &\bftab{87.2}&\bftab{1494.9}&\bftab{68.1}\\
        \bottomrule[1.3pt]
    \end{tabular}
    \label{tab:mllm-evaluation-results}
    \vspace{-15pt}
\end{table}


For the purpose of measuring the generalizability of merging methods, we employ four diverse datasets, ImageNet-R~\cite{hendrycks2021many}, AOKVQA~\cite{schwenk2022okvqa}, Screen2Word~\cite{wang2021screen2words}, TabMWP~\cite{lu2023dynamic} to represent knowledge of QA, domain transfer, captioning and specific unseen task~(math). Merged models are directly evaluated on the unseen tasks as a measurement for their generalizability. All datasets follow the format of instruction tuning and consist of instruction templates and specific question-answer pairs to formulate the instructions.

For comparison methods, we implement Task Arithmetic~\cite{ilharco2023editing}, Ties-merging~\cite{yadav2024ties}, DARE~\cite{yu2024dare} and PCB-merging~\cite{guodong2024pcb} on parameter efficient modules.


\subsection{Experiments on MLLM with Generative Tasks}
We systematically evaluate model merging methods on multimodal generative tasks. We use LLaVA~\cite{liu2024visual} as the foundation model, with CLIP-L-336~\cite{radford2021learning} as the image encoder.



\textbf{\modelname{} is effective in parameter efficient tuning.}
We evaluate the performance of various model merging methods. Concretely, we obtain independent models from fine-tuning separate datasets and merge models without re-accessing data. It is indicated from left part of~\cref{tab:mllm-results} that existing approaches suffer from severe performance drop when merging parameter efficient models, even worse than zero-shot in some cases. Also, they do not necessarily perform better than simple Task Arithmetic, showcasing the challenge in PEFT merging. By contrast, our method achieves superior results, consistently and substantially outperforming all previous methods by a solid margin~(\textbf{3.4\%} improvements on average). Notably, our approach even gets comparable performance with multi-task learning. The results above strongly validate the effectiveness of the proposed method for parameter efficient tuning.


\textbf{\modelname{} enhances performance on unseen tasks.}
Generalizability is crucial for evaluating merging methods as domain shifts are unavoidable and frequently occur in real-world scenarios. On right of~\cref{tab:mllm-results}, we report merging performance directly evaluated on four unseen tasks. It is harder as the merged models have no clue for the distribution of unseen tasks. This is validated that existing merging methods~(TA, DARE, Ties) perform poorly and even worse than zero-shot on some occasions. Conversely, our method significantly enhances performance with substantial \textbf{\textbf{4.5\%}} average improvements and even outperforms multi-task learning. Notably, our method successfully promotes domain transfer~(ImageNet-R) and specific knowledge task~(TabMWP), further demonstrating the generalizability.

\begin{table*}[ht]
    \centering
    \vspace{-5pt}
    \caption{Results of merging eight vision tasks with CLIP-ViT-B-32 as pre-trained foundation model.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccccccc|c}
    \toprule[1.3pt]
        Method & Cars & DTD
&  EuroSAT &  GTSRB & MNIST & RESISC45 &SUN397& SVHN & \bftab{Avg Acc}
\\
\midrule
       Zero-Shot  & 59.7&60.7&62.3&32.6&48.5&43.8&45.5&31.4 &48.0\\
       Individual & 74.3&88.7&65.2&92.9&99.3&58.4&99.1&96.4&84.2\\
       \midrule
       Task Arithmetic~\cite{ilharco2023editing} & 60.3&62.8&	63.2&	37.6&	52.3&	44.0&50.9&	37.6&51.1\\
        DARE~\cite{yu2024dare} & 60.4&62.8&63.1&37.5&52.4&	44.0&50.3&37.7&51.0\\
        Ties-merging~\cite{yadav2024ties} & 60.7&61.3&62.4&33.9&	56.4&43.1&51.1&42.9&51.5\\
        \midrule
        \textbf{\modelname{}} &\bftab{61.4}&\bftab{63.3}&\bftab{65.0}&\bftab{43.1}&\bftab{65.0}&\bftab{44.7}&\bftab{52.2}&\bftab{52.4}&\bftab{55.9}~\textcolor[rgb]{0.81,0,0}{\textbf{(+4.4)}}\\
        \bottomrule[1.3pt]
    \end{tabular}}
    \vspace{-15pt}
    \label{tab:vision-b-results}
\end{table*}

\begin{table*}[ht]
    \centering
    \caption{Results of merging eight vision tasks when pre-trained model scales to CLIP-ViT-L-14.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccccccc|c}
    \toprule[1.3pt]
        Method & Cars & DTD
&  EuroSAT &  GTSRB & MNIST & RESISC45 &SUN397& SVHN & \bftab{Avg Acc}
\\
\midrule
       Zero-Shot  &  77.7&71.0&66.8&50.5&76.3&55.3&59.9&58.4&64.4\\
       Individual & 99.7&95.8&80.0&97.2&99.4&70.3&98.6&97.9&92.4\\
        \midrule
       Task Arithmetic~\cite{ilharco2023editing} & 78.6&73.5&68.5&53.6&79.7&55.8&65.7&60.9&67.0
\\
        DARE~\cite{yu2024dare} & 79.5&75.0&68.8&56.5&81.4&56.6&\bftab{65.8}&62.8&68.3\\
        Ties-merging~\cite{yadav2024ties} &79.4&76.0&69.5&59.4&\bftab{83.4}&55.7&64.0&64.4&68.9\\
        \midrule
        \textbf{\modelname{}} & \bftab{79.7}&\bftab{78.4}&\bftab{70.6}&\bftab{62.4}&82.8&\bftab{58.2}&64.7&\bftab{70.3}&\bftab{70.9}~\textcolor[rgb]{0.81,0,0}{\textbf{(+2.0)}}\\
        \bottomrule[1.3pt]
    \end{tabular}}
    \vspace{-10pt}
    \label{tab:vision-L-results}
\end{table*}


\textbf{\modelname{} outperforms on comprehensive evaluation benchmarks.} We additionally report results on comprehensive evaluation benchmarks POPE~\cite{li2023evaluating}, MME~\cite{fu2024mme} and MMBench~\cite{liu2025mmbench} in~\cref{tab:mllm-evaluation-results} to evaluate common knowledge of merged models like hallucination and so on. It shows that multi-task learning achieves inferior results, indicating the challenge. By contrast, our method enhances zero-shot performance, substantially outperforms existing methods and retains common knowledge on challenging benchmarks with outstanding outcomes, certificating the effectiveness.

\begin{figure}[h]
    \centering
    \vspace{-10pt}
    \includegraphics[width=\linewidth]{figure/figure_tasknum.pdf}
    \vspace{-25pt}
    \caption{Average performance of seen and unseen tasks when number of tasks increases. Our method consistently outperforms TA and Ties with significant improvement.}
    \vspace{-5pt}
    \label{fig:results-number-task}
\end{figure}

\begin{table*}[ht]
    \centering
    \vspace{-3pt}
    \caption{Influence of each component. Prune\&scale and norm refer to pruning and complementary parameter scaling.}
    \renewcommand{\arraystretch}{1.1}
    \scalebox{0.85}{
    \begin{tabular}{cc|cccc cccc |c}
    \toprule[1.3pt]
    Prune\&Scale & Norm & ScienceQA & ImageNet
& VQAv2 & Grounding &OCRVQA&VizWiz& Flickr30k & IconQA & \bftab{Avg Acc}
\\
        \midrule
         &&71.94&57.49&67.06&38.90&62.87&44.80&49.20&39.21&53.93\\
        $\checkmark$&&73.03&64.18&\bftab{67.50}&43.12&58.19&46.36&52.24&44.54&56.14~\textcolor[rgb]{0.81,0,0}{\textbf{(+2.21)}}\\
        $\checkmark$&$\checkmark$&\bftab{73.43}&\bftab{65.54}&67.20&\bftab{44.80}&\bftab{62.97}&\bftab{46.61}&\bftab{52.80}&\bftab{45.90}&\bftab{57.33}~\textcolor[rgb]{0.81,0,0}{\textbf{(+3.40)}}\\
        \bottomrule[1.3pt]
    \end{tabular}}
    \vspace{-15pt}
    \label{tab:ablation-components-seen}
\end{table*}


\textbf{\modelname{} generalizes to the number of tasks.} We gradually increase the number of tasks to substantiate the robustness of our method. As is illustrated in~\cref{fig:results-number-task}, in seen tasks, the performance undergoes slight drop as merging more models interferences specific task. In unseen tasks, the performance first improves and then declines modestly. It could be attributed that in the first stage, seen tasks transfer knowledge and enhance unseen tasks of similar distribution; in the second stage, interference dominates merging than knowledge transformation. Under both circumstances, our method consistently outperforms existing approaches by a substantial margin as task number varies, indicating the superiority and stability of our method.

\subsection{Experiments on VLM with Vision Tasks}
For vision tasks, we follow the experimental setup outlined by Task Arithmetic~\cite{ilharco2023editing} and fine-tune eight models with LoRA on corresponding vision datasets. The datasets consist of Cars~\cite{krause2013cars}, DTD~\cite{cimpoi2014dtd}, EuroSAT~\cite{helber2019eurosat},
 GTSRB~\cite{stallkamp2011german}, MNIST~\cite{lecun1998mnist}, RESISC45~\cite{cheng2017remote}, SUN397~\cite{xiao2016sun} and SVHN~\cite{netzer2011reading}.

 
\textbf{\modelname{} is effective in vision tasks.}
We evaluate our methods on CLIP-ViT-B-32~\cite{radford2021learning} and showcase the results in~\cref{tab:vision-b-results}. It is indicated that when fine-tuned with parameter efficient technique, previous methods do not observe significant improvements against zero-shot performance, questioning their utility in vision task efficient tuning. By contrast, our method obtains considerable 7.9\% promotion against the ability of zero-shot and outperforms previous merging methods by a substantial margin~(4.4\%). It strongly validates the effectiveness of our method when merging vision models in a parameter efficient way.


\textbf{\modelname{} scales well to large VLM models.} We also apply our method on larger models to certificate the scalability of the method. Concretely, we fine-tune CLIP-ViT-L-14 on eight vision tasks separately and evaluate models with merged parameter efficient components. The results in~\cref{tab:vision-L-results} exhibit that performances of all methods improve with larger foundation models. Furthermore, \modelname{} achieves the best results with 2.0\% improvement in average performance, demonstrating the superiority.


\subsection{Ablation Study}
\textbf{Effectiveness of each component.} We investigate the utility of each component in the proposed model merging algorithm. Specifically, we progressively apply key components of our method, \ie, pruning and complementary parameter scaling and cross-task normalization, to substantiate their effectiveness. Results in~\cref{tab:ablation-components-seen} illustrate that pruning and complementary parameter scaling fundamentally contributes to mitigating interference in model merging and integrating them all achieves more advanced results. 


 \begin{figure}[t]
    \vspace{-10pt}
    \centering
    \includegraphics[width=0.85\linewidth]{figure/figure_rank.png}
    \vspace{-15pt}
    \caption{Average performance of different ranks. Our method gains from the growth of ranks and consistently outperforms previous methods by a significant margin.}
    \vspace{-15pt}
    \label{fig:ablation-rank}
\end{figure}

\textbf{Impact of rank.} We additionally explore the performance as the rank of LoRA varies. We train parameter efficient components on different ranks and report average performance of different merging approaches. \cref{fig:ablation-rank} shows the results and it is illustrated that the model obtains promotion from the improvements of rank, which increases the storage of knowledge in update subspace. Moreover, our approach continuously outperforms existing methods by a substantial margin~(3.4\% in 16 and 3.3\% in 128), validating the scalability of the proposed method.


\begin{figure*}
    \centering
    \vspace{8pt}
    \includegraphics[width=0.93\linewidth]{figure/figure_dropping_dev.pdf}
    \vspace{-10pt}
    \caption{(a) Average performance of merging models with different pruning rates. Non-zero parameters decline according to the ineffective parameter criteria as the pruning rate increases. (b) Performance of different pruning techniques averaged on seen and unseen tasks. Aggregated sign achieves poor performance. (c) Comparison of average performance and standard deviation with existing methods. Cross-task normalization enhances performance with stable deviation.}
    \label{fig:dropping-dev}
    \vspace{-9pt}
\end{figure*}

\textbf{Influence of pruning rate.} As is revealed in~\cref{sec:motivation}, parameters of small values have less influence on directions of low rank decomposition. Therefore, we prune parameters according to the magnitude to facilitate merging procedure. To further validate the point of view, we gradually increase the pruning rate and show variation of average performance in~\cref{fig:dropping-dev}\textcolor{mydarkblue}{a}. It is elucidated that with the pruning rate increases, the performance gradually boosts due to mitigating interference between tasks, and finally undergoes a sharp decrease as pruning larger parameters significantly influence directions and task knowledge. Consequently, the results are in accordance with the aforementioned analysis, underlining the utility of parameter pruning strategy in our method.


\begin{table}[t]
    \centering
    \vspace{-5pt}
    \caption{Influence of complementary parameter scaling. Coefficients solely dependent on specific module~($\boldsymbol{\mathrm{A}}$, $\boldsymbol{\mathrm{B}}$ or both) perform inferior to adaptive coefficients with inter-parameter relation.}
    \vskip 0.10in
    \scalebox{0.8}{
    \begin{tabular}{l|cc}
    \toprule[1.3pt]
    Method & Seen Tasks & Unseen Tasks 
\\
        \midrule
        Baseline~(w/o adaptation)& 54.1& 32.5\\
        \midrule
        Ours~($\mathrm{individual}$, $\mathrm{A}$)&54.5~\textcolor[rgb]{0.81,0,0}{(+0.4)}&32.1~\textcolor{ForestGreen}{($-$0.4)} \\
        Ours~($\mathrm{individual}$, $\mathrm{B}$) &55.4~\textcolor[rgb]{0.81,0,0}{(+1.3)}&34.1~\textcolor[rgb]{0.81,0,0}{(+1.6)}\\ 
        Ours~($\mathrm{individual}$, $\mathrm{A+B}$) &51.7~\textcolor{ForestGreen}{($-$2.4)}&35.0~\textcolor[rgb]{0.81,0,0}{(+2.5)}\\
        \midrule
        \textbf{Ours~(inter-parameter)} &\bftab{57.3}~\textcolor[rgb]{0.81,0,0}{(+3.2)}&\bftab{38.0}~\textcolor[rgb]{0.81,0,0}{(+5.5)}\\
        \bottomrule[1.3pt]
    \end{tabular}}
    \vspace{-20pt}
    \label{tab:ablation-coefficients}
\end{table}


\subsection{Analysis}
\label{sec:analysis}
\textbf{Parameter pruning in magnitude provides superior results.} We additionally compare with pruning techniques employed in DARE~\cite{yu2024dare} and Ties-merging~\cite{yadav2024ties} to showcase the effectiveness of the proposed parameter pruning technique. Concretely, we employ randomly pruning and aggregated sign as the pruning criteria, respectively and evaluate their performance. The results in~\cref{fig:dropping-dev}\textcolor{mydarkblue}{b} reveal that sign conflict is not crucial in efficient merging with performance worse than random. By contrast, our magnitude-based parameter pruning technique achieves better results in multimodal tasks and outperforms existing approaches by a substantial margin~(2.3\% and 4.0\% respectively). We attribute the promotion to significantly wider distribution of parameter efficient model than full fine-tuning model and pruning according to sign inevitably changes direction in low-rank space. Conversely, our method avoids task conflicts with less impact on principal direction.

\textbf{Complementary parameter scaling effectively compensates for performance drop.} It is elucidated in~\cref{sec:detail-method} that we construct coefficients with influence interwoven between parameters. To figure out its effectiveness, we replace it with different scaling strategies. Concretely, we decouple the interaction between the two modules, employing coefficients from $\boldsymbol{\mathrm{A}}$, $\boldsymbol{\mathrm{B}}$, individually. We additionally conduct scaling for $\boldsymbol{\mathrm{A}}$ and $\boldsymbol{\mathrm{B}}$ simultaneously and report quantitative results in~\cref{tab:ablation-coefficients}. The study certificates the benefit of scaling coefficients from complementary parameter adaptation, and adaptively adjusting coefficients of $\boldsymbol{\mathrm{B}}$ effectively promotes the performance, which is in accordance with the analysis above. It is also demonstrated that the performance does not necessarily become better by utilizing more complex scaling coefficients~(2.4\% decrease in seen tasks).


\textbf{Cross-task normalization provides more stable performance.} As is illustrated in~\cref{sec:detail-method}, cross-task normalization provides not only consistent and stable performance on seen tasks but also advanced promotion on unseen tasks. We analyze the correlation between performance and variance in~\cref{fig:dropping-dev}\textcolor{mydarkblue}{c}. Concretely, compared with existing methods, our approach achieves 3.4\% better performance~(57.3\% v.s. 53.9\%) with significantly smaller variance~(\textbf{1.2\%}). Notably, employing cross-task normalization strengthens the advantage. Specifically, it improves 1.2\% average performance while reducing 0.2\% on standard deviation, showcasing the superiority of the proposed method.


\section{Conclusion}
In this paper, we focus on parameter efficient model merging for large foundation models. We reveal that wider distribution and task-oriented singular values make parameter efficient model merging distinct, and introduce complementary parameter adaptation, an effective parameter efficient merging technique for both seen task enhancement and unseen task generalization. We conduct extensive experiments and comprehensive analyses to showcase the superiority, robustness and scalability of the approach.


\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}

\end{document}
