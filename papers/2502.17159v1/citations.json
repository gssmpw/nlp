[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wang2018glue",
        "author": "Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman",
        "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "goyal2017vqa",
        "author": "Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi",
        "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "han2024parameter",
        "author": "Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian",
        "title": "Parameter-efficient fine-tuning for large models: A comprehensive survey"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jia2022visual",
        "author": "Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam",
        "title": "Visual prompt tuning"
      },
      {
        "key": "khattak2023maple",
        "author": "Khattak, Muhammad Uzair and Rasheed, Hanoona and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz",
        "title": "Maple: Multi-modal prompt learning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hu2022lora",
        "author": "Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "title": "Lo{RA}: Low-Rank Adaptation of Large Language Models"
      },
      {
        "key": "wu2024moslora",
        "author": "Wu, Taiqiang and Wang, Jiahao and Zhao, Zhe and Wong, Ngai",
        "title": "Mixture-of-Subspaces in Low-Rank Adaptation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2022few",
        "author": "Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A",
        "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "yang2024model",
        "author": "Yang, Enneng and Shen, Li and Guo, Guibing and Wang, Xingwei and Cao, Xiaochun and Zhang, Jie and Tao, Dacheng",
        "title": "Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities"
      },
      {
        "key": "sung2023empirical",
        "author": "Sung, Yi-Lin and Li, Linjie and Lin, Kevin and Gan, Zhe and Bansal, Mohit and Wang, Lijuan",
        "title": "An Empirical Study of Multimodal Model Merging"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "jin2023regmean",
        "author": "Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang",
        "title": "Dataless Knowledge Fusion by Merging Weights of Language Models"
      },
      {
        "key": "matena2022fisher",
        "author": "Matena, Michael S and Raffel, Colin A",
        "title": "Merging models with fisher-weighted averaging"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ilharco2023editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "tang2024wemoe",
        "author": "Tang, Anke and Shen, Li and Luo, Yong and Yin, Nan and Zhang, Lefei and Tao, Dacheng",
        "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "yadav2024ties",
        "author": "Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit",
        "title": "Ties-merging: Resolving interference when merging models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "yu2024dare",
        "author": "Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin",
        "title": "Language models are super mario: Absorbing abilities from homologous models as a free lunch"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "guodong2024pcb",
        "author": "Guodong, DU and Lee, Junlin and Li, Jing and Jiang, Runhua and Guo, Yifei and Yu, Shuyang and Liu, Hanting and Goh, Sim Kuan and Tang, Ho-Kin and He, Daojing and others",
        "title": "Parameter Competition Balancing for Model Merging"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "deng2009imagenet",
        "author": "Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li",
        "title": "Imagenet: A large-scale hierarchical image database"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "tang2024parameter",
        "author": "Anke Tang and Li Shen and Yong Luo and Yibing Zhan and Han Hu and Bo Du and Yixin Chen and Dacheng Tao",
        "title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization"
      }
    ]
  }
]