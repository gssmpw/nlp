\section{Related Work}
\subsection{Multimodal Large Language Models}
With the surge in data volume and model size, large language models~(LLMs)~\cite{radford2019language,touvron2023llama,alayrac2022flamingo} have shown their powerful performance. They are constructed with decoder-only blocks and respond to inputs in an auto-regressive way, which shows their potential on both classification~\cite{wang2018glue} and generative tasks~\cite{goyal2017vqa}. Furthermore, multimodal large language models~(MLLMs) enhance the large models with vision perception ability. They obtain visual features with vision encoder and align image-text features with a cross-modality module like linear projection~\cite{liu2024visual}, Q-former~\cite{li2023blip} and so on. Current research on large models is dedicated to direct fine-tuning one independent model with task-specific data to get better results. Rather than improving performance of certain domain, we focus on integrating models into one model to boost efficiency and handle multiple tasks simultaneously.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/figure_motivation_distribution.pdf}
    \vspace{-25pt}
    \caption{(a) Impact of task-oriented eigenvalues for low-rank decomposition. Experiments are conducted on ScienceQA and ImageNet. (b) Effectiveness of \modelname{} by adaptively reducing interference with larger scale on smaller singular values. (c) Distribution of full fine-tuning and parameter efficient modules. Parameters of FFT, and different components in efficient tuning have different distributions.}
    \label{fig:motivation-distribution}
    \vspace{-15pt}
\end{figure*}

\subsection{Parameter Efficient Tuning}
When fine-tuning pre-trained model with task-specific data, training the whole model would not only disrupt the representations obtained from billions of data but also become resource intensive. To address the issue, parameter efficient tuning~\cite{han2024parameter} is introduced to refrain from fine-tuning the whole model. It typically trains lightweight modules to make model adapt to downstream tasks and achieves competitive results compared to full fine-tuning models. Various efficient tuning techniques have been explored like prompt learning~\cite{jia2022visual, khattak2023maple}, adapter learning including LoRA ~\cite{hu2022lora,wu2024moslora}, (IA)$^3$~\cite{liu2022few} and so on. In this paper, we focus on LoRA, as it is the most commonly utilized PEFT method and has demonstrated its usefulness especially for large models~\cite{liu2024visual}.

\subsection{Model Merging}
Model merging~\cite{yang2024model,sung2023empirical} refers to merging multiple models of different capabilities to handle multi-task learning with one universal model~\cite{jin2023regmean,matena2022fisher}. Task Arithmetic~\cite{ilharco2023editing} presents a paradigm that obtains task vectors from subtracting pre-trained model from fine-tuned model and treats model merging as arithmetic operations of task vectors. It has gained widespread attention in various fields~\cite{tang2024wemoe}.
Ties-merging~\cite{yadav2024ties} trims and elects sign to reduce interference. DARE~\cite{yu2024dare} randomly drops parameters and rescales the remaining ones to approximate the original embedding. PCB-merging~\cite{guodong2024pcb} introduces a parameter adjustment with competition balancing to address potential conflicts. However, most of them focus on merging models with FFT on classification tasks~\cite{deng2009imagenet}, and the distribution shift prevents their ability to acquire satisfying performance~\cite{tang2024parameter}. By contrast, we focus on parameter efficient model merging with multimodal tasks.