\vspace{-2mm}
\section{Preliminaries}
\vspace{-2mm}
\subsection{MLLMs Architecture and Models}
\vspace{-2mm}
\begin{wrapfigure}{r}{3.5cm}
  \vspace{-6mm}
  % \begin{adjustbox}{max width=\linewidth}
    \includegraphics[width=1.0\linewidth]{figures/MLLM_archi.pdf}
  % \end{adjustbox}
  % \vspace{-4mm}
  \label{fig:MLLM_archi}
  \vspace{-9mm}
\end{wrapfigure}
As shown in the Figure, the existing MLLMs framework ~\citep{qwenvl,chen2024internvl} mainly consists of three modules: a vision encoder $\textbf{E}$ for processing visual inputs, a visual-language projector $\textbf{P}$ that serves as a bridge to align the two modalities and a large language model ($LLM$) that handles the multi-modal tokens and performs reasoning. 
\vspace{-4mm}
\paragraph{Vision Encoder.} Taking the input image or video $\mX_v$ as input and compressing the original vision information into more compact patch features $\mF_v$. This process typically utilizes a Vision Transformer (ViT) ~\citep{vit}, such as CLIP~\citep{radford2021clip} and OpenCLIP~\citep{openclip}. It can be formulated as: $\mF_v = E(\mX_v)$.



\vspace{-4mm}
\paragraph{Vision-Language Projector.} The task of the vision-language projector $\mP$ is to map the visual patch features $\mF_v$ into the textual feature space: $\mE_v = P(\mF_v)$.
% \vspace{-2mm}
% \begin{equation}
% \mE_v = P(\mF_v)
% \vspace{-3mm}
% \end{equation}
% \vspace{-5mm}
%The LLMs process input sequences that encompass multiple modalities and generates corresponding response $\mO_a$.
\vspace{-4mm}
\paragraph{Large Language Model.} The pre-trained large language model is the core component of MLLMs, endowing the framework with exceptional capabilities, such as zero-shot generalization, instruction following, and in-context learning. Typically, a text tokenizer is integrated with the LLM, mapping text prompts $\mX_t$ to the text tokens $\mE_t$. The text tokens $\mE_t$ and the visual tokens $\mE_v$ are then concatenated to form the input for MLLMs, which outputs the final response sequence $\mO_a$ in an autoregressive manner:
\vspace{-3mm}
\begin{equation}
            {LLM}(\mO_a|\mE_v,\mE_t)=\prod^l_{i=1}{LLM}(y_i|\mE_v,\mE_t,y_{<i})
\vspace{-1mm}
\label{eq1}
\end{equation}
where $l$ denotes the length of $\mO_a$. The parameter sizes of large language models (LLMs) range from 3 billion to tens of billions. Commonly used open-source LLMs include the Llama series~\citep{touvron2023llama,llama2}, Qwen~\citep{qwen}, InternLM~\citep{internlm}, MiniCPM~\citep{hu2024minicpm}, ChatGLM~\citep{chatglm}. 

These foundational elements of MLLMs have benefited significantly from the rapid advancements in LLM technology.  Flamingo~\citep{alayrac2022flamingo} pioneered connecting pre-trained visual encoders to LLMs, demonstrating strong generalization across visual-language tasks. Following the emergence of ChatGPT~\cite{achiam2023gpt}, numerous open-source models based on pre-trained LLMs like LLaMA~\citep{touvron2023llama} and its variants~\citep{llama2, zheng2023vicuna} have been proposed~\citep{li2023blip2, huang2023kosmos1, zhu2023minigpt, liu2023llava}. Subsequent efforts like Qwen-VL~\citep{qwenvl}, InternVL~\citep{chen2024internvl}, and CogVLMV2~\cite{CogVLM2} enhanced MLLMs from perspectives such as high-resolution input and larger-scale training data. However, the substantial parameters of MLLMs lead to high computational costs, limiting broader application. Recently, smaller MLLMs like Mini-Gemini~\citep{li2024minigemini}, MobileVLM~\cite{chu2024mobilevlm2}, and MiniCPM-V~\citep{yao2024minicpmv} have emerged. Despite these developments, dedicated quantization methods for MLLMs to further reduce memory usage and accelerate inference remain under-explored.


\vspace{-2mm}
\subsection{Post Training Quantization (PTQ)}
\vspace{-2mm}


PTQ serves as a potent strategy for model compression. By
converting the high-precision variables of pre-trained models into low-bit integers, it achieves a reduction in memory
usage and an acceleration of inference speed. For uniform quantization, given a floating-point (FP) tensor $x$ (weights or activations), it can be uniformly quantized to $b$-bits in signed quantization as follows:
\vspace{-2mm}
\begin{equation}\label{eq:quant}
    \hat{\mathbf{x}} = \mathbf{Q}_U(\mathbf{x},b) =(clamp(\lfloor \frac{\mathbf{x}}{s} \rceil+z, q_{min}, q_{max}) - z) \cdot s
    % x_{int} = clamp(\lfloor \frac{x}{s} \rceil+z,q_{min},q_{max}),
\vspace{-2mm}
\end{equation} 
where $s=\frac{\max(|\mathbf{x}|)}{2^{b-1}-1}$ is the scale factor, $\lfloor \cdot \rceil$ is the rounding-to-nearest operator, and the function $clamp(\cdot)$ clips values outside the integer range $\left[q_{min}, q_{max} \right]$. $z$ is zero-point. $s$ denotes the quantization scale factor, which reflects the proportional relationship between FP values and integers. $\left[q_{min}, q_{max} \right]$ is the quantization range determined by the bit-width $b$. Generally, when we quantize the network's weight with 4-bit and activations with 8-bit, called it as W4A8. We can calculate $s$ offline using the activations from calibration samples, known as \textbf{static quantization}. We can also use the runtime statistics of activations to get $s$, referred to as \textbf{dynamic quantization}.  More details are in Appendix \ref{quant_granularity}.

Existing post-training quantization (PTQ) methods for LLMs are categorized into weight-only and weight-activation quantization~\citep{zhao2023survey, yuan2024llm,shang2023pb,yue2024wkvquant,hu2024llm,hu2025ostquant}. Weight-only methods like GPTQ~\citep{frantar2022gptq}, QuIP~\citep{chee2024quip}, and AWQ~\citep{lin2023awq} achieve high compression rates but offer limited inference acceleration. In contrast, weight-activation quantization methods~\citep{xiao2022smoothquant, wei2022outlier, yuan2023asvd, zhang2024qqq} quantize both weights and activations, improving memory usage and latency. The main challenge is activation outliers causing quantization errors. Techniques like SmoothQuant~\citep{xiao2022smoothquant} shift quantization difficulty from activations to weights, while OmniQuant~\citep{2023omniquant} optimizes performance by training quantization parameters. SliceGPT~\citep{ashkboos2024slicegpt} reduces memory demands by designing a Pre-LN + Rotate Scheme for LLMs sparsification based on computational invariance. They achieve this by adding a linear layer in the residual connection (see Appendix ~\ref{pre-LN2RMSN}). Unlike SliceGPT, we further develop a Post-LN + Rotate scheme to accommodate more vision encoder and extends its applicability to various MLLMs. This enhancement broadens the the LayerNorm + Rotate approach, making it suitable for both Pre-LN and Post-LN configurations across various MLLMs. Recently, Quarot~\citep{ashkboos2024quarot} introduces rotations to eliminate outliers; however, this solution is not applicable to MLLMs due to inherent modality differences.