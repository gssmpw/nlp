\vspace{-3mm}
\section{Experiments}\label{sec:exp}

\label{exp}
\vspace{-2mm}

\begin{table}[t]
\caption{Comprehensive quantization results of different MLLMs across various evaluation datasets.}
% \vspace{+2mm}
\label{table:main_results}
\centering
\Huge
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|cc|cccc}
\toprule
\multirow{2}{*}{\textbf{MLLMs}} &  \multirow{2}{*}{\textbf{Method}}& \multicolumn{2}{|c|}{\textbf{Bits Setting}} & \multirow{2}{*}{\textbf{T.VQA$\uparrow$}} & \multirow{2}{*}{\textbf{D.VQA$\uparrow$}} & \multirow{2}{*}{\textbf{OCRB.$\uparrow$}} & \multirow{2}{*}{\textbf{MME$\uparrow$}} \\
& & Visual & LLM & \\
\toprule
  \rowcolor{gray!12}
\cellcolor{white}& - & BF16 & BF16 & 77.65 & 90.97 & 794 & 2209 \\ 
  & RTN & \multirow{4}{*}{W8A8} &\multirow{4}{*}{W4A8} & 52.02 & 59.04 &542 &1528 \\
  & SQ &  & & 59.88 &59.75 &544 &1540 \\
  & Quarot &  & & 73.34 & 84.07 & 715 & 2067 \\
  \rowcolor{gray!25}
  \cellcolor{white}{InternVL2} & \textbf{MQuant} &  & & \textbf{77.49} & \textbf{90.27} & \textbf{785} & \textbf{2156} \\
  \cline{2-8}
  \cellcolor{white}{-8B} & RTN &\multirow{4}{*}{W4A8} &\multirow{4}{*}{W4A8}& 40.06 &31.58&302 &1482 \\
  & SQ&  & & 46.48 &31.21 &310 &1540 \\
  & Quarot& & &49.10 &33.62 &361 &1941 \\
  \rowcolor{gray!25}
  \cellcolor{white} & \textbf{MQuant} && & \textbf{76.62} & \textbf{88.42} &\textbf{725} &\textbf{2155}\\ 
  \midrule
  \rowcolor{gray!12}
  \cellcolor{white} & - & BF16 & BF16  & 61.40  &60.36 & 493 & 1834 \\
  & RTN & \multirow{4}{*}{W8A8} &\multirow{4}{*}{W4A8} &  0.45 & 0.03 & 189 & 625 \\
  & SQ &  & & 7.45 &7.70 & 160 &797 \\
  & Quarot & &  & 45.32 &42.44 &286 &940 \\
  \rowcolor{gray!25}
  \cellcolor{white}{{{Qwen-VL}}} & \textbf{MQuant} &  & & \textbf{61.16} &\textbf{59.31} &\textbf{483} & \textbf{1691} \\ \cline{2-8}
  \cellcolor{white}{{{-Chat-9.6B}}}& RTN & \multirow{4}{*}{W4A8} &\multirow{4}{*}{W4A8} & 1.02 &0.02 &193 &585 \\
  & SQ &  & &  8.59 &4.28 &188 &921 \\
  & Quarot &  & &  46.77 &37.35 & 289 & 1091 \\
  \rowcolor{gray!25}
  \cellcolor{white} &\textbf{MQuant} &  & & \textbf{60.50} &\textbf{58.72} &\textbf{473} &\textbf{1713} \\
\midrule
\rowcolor{gray!12}
\cellcolor{white}& - & BF16 & BF16 & 79.10 &89.18 &847 &2248 \\
& RTN &\multirow{4}{*}{W8A8} &\multirow{4}{*}{W4A8}& 61.00 & 65.16 & 332 & 1300 \\
  & SQ &  & &  62.40 & 65.76 & 424 & 1510 \\
  & Quarot  &  & & 73.71 &80.04 &736 &1850 \\
  \rowcolor{gray!25}
  \cellcolor{white}{MiniCPM-V} & \textbf{MQuant} &  & & \textbf{80.41} &\textbf{89.15} &\textbf{844} &\textbf{2244} \\ \cline{2-8}
  \cellcolor{white}{2.6-8B}& RTN & \multirow{4}{*}{W4A8} &\multirow{4}{*}{W4A8} &60.70 &62.23 &351 &1404 \\
  & SQ &  & &  65.67 &60.02 &455 &1491\\
  & Quarot &  & & 68.96 &79.63 &685 &1734 \\
  \rowcolor{gray!25}
  \cellcolor{white} & \textbf{MQuant} &  & & \textbf{81.14} & \textbf{89.75} &\textbf{839} &\textbf{2189} \\
\midrule
\rowcolor{gray!12}
\cellcolor{white}& - & BF16 & BF16 & 84.43 & 93.87 &842 &2319 \\
& RTN &\multirow{4}{*}{W8A8} &\multirow{4}{*}{W4A8}& 33.92&52.61&442&1298 \\
  & SQ &  & &  49.11 &53.97 &444 &1500 \\
  & Quarot  &  & & 79.36 &89.57 &754 &2045 \\
  \rowcolor{gray!25}
  \cellcolor{white}{Qwen2-VL} & \textbf{MQuant} &  & & \textbf{84.43} &\textbf{93.61} &\textbf{830} &\textbf{2269} \\ \cline{2-8}
  \cellcolor{white}{-7B}& RTN & \multirow{4}{*}{W4A8} &\multirow{4}{*}{W4A8} &40.20 &38.82 &422 &1082 \\
  & SQ &  & &  46.25 &52.36 &411 &1535 \\
  & Quarot &  & & 71.44 & 83.96 & 670 & 1911 \\
  \rowcolor{gray!25}
  \cellcolor{white} & \textbf{MQuant} &  & & \textbf{84.32} & \textbf{93.58} &\textbf{824} &\textbf{2255} \\
\midrule
\rowcolor{gray!12}
\cellcolor{white}& - & BF16 & BF16 & 82.82 &81.16 &782 &2153 \\
& RTN &\multirow{4}{*}{W8A8} &\multirow{4}{*}{W4A8}&  7.05 &3.70 &0.00 &140 \\
  & SQ &  & &  9.05 &4.10 &0.00 &148 \\
  & Quarot  &  & & 82.00 &80.17 &782 &2115 \\
  \rowcolor{gray!25}
  \cellcolor{white}{GLM-4V} & \textbf{MQuant} &  & & \textbf{82.06} &\textbf{80.53} &\textbf{782} &\textbf{2164} \\ \cline{2-8}
  \cellcolor{white}{-9B}& RTN & \multirow{4}{*}{W4A8} &\multirow{4}{*}{W4A8} &7.61 &3.60 &0.00 &163 \\
  & SQ &  & &  9.85 &4.40 &0.00 &188 \\
  & Quarot &  & & 64.16 &45.52 &516 &2048 \\
  \rowcolor{gray!25}
  \cellcolor{white} & \textbf{MQuant} &  & & \textbf{81.58} & \textbf{79.67} &\textbf{754} &\textbf{2120} \\
\bottomrule
\end{tabular}
}
\vspace{-8mm}
\end{table}

% \vspace{-2mm}
\paragraph{Models and Datasets.} We evaluate our \emph{MQUANT} on five MLLMs: InternVL2-8B~\citep{internvl15}, Qwen-VL-Chat-9.6B~\citep{qwenvl}, MiniCPM-V 2.6-8B~\citep{yao2024minicpmv}, Qwen2-VL-7B~\citep{Qwen2VL}, and GLM-4V-9B~\citep{CogVLM2}. Evaluations are conducted on four benchmarks covering OCR and general question answering: TextVQA~\citep{singh2019textvqa}, DocVQA~\citep{mathew2021docvqa}, OCRBench~\citep{liu2023ocrbench}, and MME~\citep{fu2023mme}, which assesses perception and cognition across 14 subtasks. These MLLMs' details are in Appendix~\ref{MLLMs_comparison}.

\begin{table*}[t]
    \vspace{-1mm}
    \caption{Comparison of latency and memory saving with Pytorch (BF16), AWQ (W4-only) and ours MQuant (W4A8) on Qwen2-VL-7B. Pytorch and AWQ using the Qwen2-VL-7B official implementation. $\downarrow$ means lower values are better, $\uparrow$ means larger values are better.}
    \vspace{-4.5mm}
    \label{tab:ablation2}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c>{\columncolor{gray!15}}c|>{\columncolor{blue!2}}c>{\columncolor{blue!2}}c|>{\columncolor{myblue!15}}c>{\columncolor{myblue!15}}c|>{\columncolor{gray!15}}c|>{\columncolor{blue!2}}c>{\columncolor{blue!2}}c|>{\columncolor{myblue!15}}c>{\columncolor{myblue!15}}c}\\
        \toprule 
        Image size & \multicolumn{5}{c|}{Latency (s)}& \multicolumn{5}{c}{Memory (G)}\\ \cline{2-11}
        
          H $\times$ W & Pytorch& AWQ$\downarrow$ & Speedup$\uparrow$ & MQuant$\downarrow$& Speedup$\uparrow$ & Pytorch & AWQ$\downarrow$ & Improve$\uparrow$ & MQuant$\downarrow$  & Improve$\uparrow$\\ \cline{2-11}
        840$^2$ & 0.261  &0.304 (+0.043) &\color{myred}{-14.14\%} &0.210 (-0.051) &\textbf{\color{mygreen1}{+24.76\%}}&16.45 &7.45 (-9.00)&\color{mygreen1}{+120.67\%}&6.50 (-9.95) &\textbf{\color{mygreen1}{+152.92\%}}\\
        1960$^2$ & 1.369  &1.598 (+0.229) &\color{myred}{-14.26\%} &1.112 (-0.257) & \textbf{\color{mygreen1}{+16.63\%}} &17.82 &8.82 (-9.00)&\color{mygreen1}{+100.00\%} &7.85 (-9.97)  &\textbf{\color{mygreen1}{+119.93\%}}\\
        3080$^2$ & 5.208  &5.872 (+0.664) &\color{myred}{-11.27\%} &4.488 (-0.720) &\textbf{\color{mygreen1}{+16.02\%}} &20.58 &11.58 (-9.00)&\color{mygreen1}{+77.60\%}&10.61 (-9.97) &\textbf{\color{mygreen1}{+96.45\%}}\\
        5600$^2$ & 8.380  &9.393 (+1.013) &\color{myred}{-10.78\%} &7.469  (-0.911)&\textbf{\color{mygreen1}{+12.19\%}} &22.22 &13.22 (-9.00)&\color{mygreen1}{+57.54\%}&12.25 (-9.97) &\textbf{\color{mygreen1}{+61.65\%}}\\
            \hline
            \end{tabular}  
        }
\label{tab:speedup}
\vspace{-2mm}
\end{table*}

\begin{table*}[h]
\vspace{-2mm}
\caption{Multi-Batch speedup comparison of MSQ + AIFS on W4A8 setting. 
Each row shows the cumulative total of text tokens, images, and textual responses for multi-turn inference.}
\vspace{-2mm}
\label{tab:speed_multibatch}
\centering
\Huge
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|cc|c|cc|c|cc|c}
\toprule
\multirow{2}{*}{\textbf{Batch}} 
& \multicolumn{3}{c|}{\textbf{Config (Text+Image+Text)}} 
& \multicolumn{2}{c|}{\textbf{Prefill (s)}} 
& \multirow{2}{*}{\textbf{Improve$\uparrow$}} 
& \multicolumn{2}{c|}{\textbf{Decode (s)}}  
& \multirow{2}{*}{\textbf{Improve$\uparrow$}} 
& \multicolumn{2}{c|}{\textbf{All (s)}}  
& \multirow{2}{*}{\textbf{Improve$\uparrow$}} \\
& \textbf{Text} & \textbf{Image} & \textbf{Text} 
& \textbf{bfp16} & \textbf{MQuant} & 
& \textbf{bfp16} & \textbf{MQuant} & 
& \textbf{bfp16} & \textbf{MQuant} & \\
\midrule
1 & 10 & 2240$\times$2240 & 50 
  & 2.54 & 1.93 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+31.6\%}} 
  & 18.01 & 12.89 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+39.7\%}}  
  & 20.55 & 14.82 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+38.7\%}}  \\
2 & 10/10 & 2240$\times$2240 / 2240$\times$2240 & 50/100 
  & 5.42 & 4.15 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+30.6\%}} 
  & 37.82 & 31.56 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+19.8\%}} 
  & 43.24 & 35.71 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+21.1\%}} \\
3 & 10/10/10 & 2240$\times$2240 / 2240$\times$2240 / 2240$\times$2240 & 50/100/150 
  & 8.24 & 6.42 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+28.3\%}} 
  & 48.03 & 40.35 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+19.0\%}} 
  & 56.27 & 46.77 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+20.3\%}} \\
4 & 10/10/10/10 & 2240$\times$2240 / 2240$\times$2240 / 2240$\times$2240 / 2240$\times$2240 & 50/100/150/200 
  & 11.17 & 8.67 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+28.9\%}} 
  & 59.09 & 49.92 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+8.4\%}}
  & 70.26 & 58.59 & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+20.0\%}} \\
\bottomrule
\end{tabular}
}
\vspace{-5mm}
\end{table*}

\vspace{-2mm}
\paragraph{Baselines and Implementation Details.} We test W8A8 and W4A8 quantization settings for both visual encoders and LLMs, comparing RTN, SmoothQuant~\citep{xiao2022smoothquant}, and Quarot~\citep{ashkboos2024quarot}. Notably, we apply static per-tensor activation quantization for both components, unlike the dynamic per-token quantization typically used in existing MLLMs. The calibration dataset consists of 256 randomly selected samples from the corresponding benchmark training sets~\citep{singh2019textvqa, mathew2021docvqa, liu2023ocrbench}. The batch size for latency is 1.

\vspace{-3.5mm}
\subsection{Overall Results}
\vspace{-2mm}

\paragraph{Weight-activation quantization results of various MLLMs.} MQuant can be applied to the quantization of various MLLMs. As shown in Table~\ref{table:main_results}, our MQuant demonstrates significant improvements over several representative quantization methods. In W8A8 setting, MQuant achieves performance nearly equivalent to that of FP models across all evaluation datasets. Notably, even in the more challenging W4A8 setting, MQuant maintains comparable performance with FP models, while other advanced quantization methods exhibit significant performance degradation. These results indicate that our MQuant provide a general and effective PTQ solution with strong compatibility for maintaining high accuracy in MLLMs under various bits settings.

% \vspace{-2mm}  

% \begin{wraptable}{r}{11cm}
\begin{table}[h]
\vspace{-2mm}
\caption{Speedup of MSQ + AIFS on W4A8 setting.}
\vspace{-2mm}
\label{tab:speed_decode}
\centering
\Huge
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc|c}
\toprule
\textbf{Stage} & \textbf{BF16} & \textbf{Per-token Dyn.} & \textbf{Ours} & \textbf{Ours + GEMV} & \textbf{Speedup} \\ 
\toprule
\textbf{Prefill} & 1690 & 1253 & 1017 & - & \cellcolor{myblue!15} \color{mygreen1}{\textbf{+23\%}} \\
\textbf{Decode} & 17.5 & 16.4 & 13.06 & 8.2 & \cellcolor{myblue!15} \color{mygreen1}{\textbf{+100\%}} \\
\bottomrule
\end{tabular}
}
\vspace{-6mm}
\end{table}

\vspace{-2mm}
\subsection{Latency and Memory Analysis}
We measure the efficiency of \textbf{MQuant} in terms of speedup and memory usage under varying image resolutions, as well as the decode-stage acceleration introduced by AIFS. Unless otherwise noted, we adopt the common ``\textit{text-image-text}'' input format~\citep{duan2024vlmevalkit} with 15 textual tokens, while the image size changes from $280\times280$ to $5600\times5600$.

\vspace{-2mm}
\paragraph{Speedup and Memory Savings.}
\textbf{\raisebox{-0.5pt}{\ding[1.1]{182\relax}} Overall Speedup} As shown in Table~\ref{tab:speedup}, \textbf{MQuant} surpasses PyTorch BF16 and AWQ (W4-only) across all image resolutions, achieving up to \textbf{24.76\%} speedup over PyTorch at $840\times 840$. Even at higher resolutions (e.g., $5600^2$), \textbf{MQuant} maintains a \textbf{12.19\%} latency improvement, demonstrating scalability. In addition, our method provides memory savings over both PyTorch and AWQ, with reductions exceeding \textbf{100\%} compared to PyTorch (e.g., \textbf{152.92\%} at $840^2$). These benefits primarily arise from (1)~eliminating the overhead of token-wise scale computation, and (2)~converting mixed tokens into modality-decoupled tokens, avoiding slicing and concatenation when image resolutions become large. 
\textbf{\raisebox{-0.5pt}{\ding[1.1]{183\relax}} Decode-Stage Acceleration.} We also measure decode time for generating 2{,}000 tokens with a custom W4A8 \texttt{GEMV} kernel (see Table\ref{tab:speed_decode}). Compared to per-token dynamic quantization, our \textbf{AIFS}+\textbf{MSQ} framework gains \textbf{23\%} speedup in the prefill stage and \textbf{100\%} speedup in decode time. By shifting online dynamic quantization to an offline static approach, we greatly reduce inference overhead, for long-sequence tasks. Moreover, since visual tokens are generally pricier than textual ones~\citep{duan2024vlmevalkit}, these improvements translate to notable real-world cost reductions (e.g., $\approx30\%$ for OpenAI-token-based pricing).

\begin{table}[h]
\vspace{-3mm}
\caption{Latency comparison under multi-turns setting.}
\label{table:multi-turns-comparison}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|cc|c}
\toprule
\multirow{2}{*}{\textbf{Turns}}  & \multicolumn{3}{c|}{\textbf{Config in a Turn}} & \multicolumn{2}{c|}{\textbf{All(s)}} & \multirow{2}{*}{\textbf{Improve $\uparrow$}} \\
               & \textbf{Text} & \textbf{Image} & \textbf{Text} & \textbf{bfp16} & \textbf{Ours} & \textbf{} \\ 
\midrule
1              & 10              & 2240x2240               &  50             & 20.55         & 14.82         & \cellcolor{myblue!15}\color{mygreen1}{\textbf{+38.7\%}} \\ 
2              & 10            & 2240x2240      & 50            & 44.06         & 32.61         & \cellcolor{myblue!15} \color{mygreen1}{\textbf{+35.1\%}} \\ 
3              & 10            & 2240x2240      & 50            & 76.67         & 59.48         & \cellcolor{myblue!15} \color{mygreen1}{\textbf{+28.9\%}} \\ 
\bottomrule
\end{tabular}
}
\vspace{-5mm}
\end{table}


\begin{table*}[t]
\caption{The accuracy and speedup of our MSQ and AIFS scheme on the linear layer during prefill stage. The input sequence is structured as "text-image-text-image-text-image-text" with an image resolution of $2240 \times 2240$ and 50 textual tokens. Latency were tested on an NVIDIA RTX 6000 Ada Generation.}
% \vspace{+2mm}
\vspace{-2mm}
\label{table:AIFS_laten}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|cccc|c|c}
\toprule
\textbf{Activation}  & \textbf{Weight} &\textbf{T.VQA$\uparrow$} & \textbf{D.VQA$\uparrow$} & \textbf{OCRB.$\uparrow$} & \textbf{MME$\uparrow$}& \textbf{Latency $\downarrow$} (s) & \textbf{Speedup $\uparrow$} \\
\toprule
\rowcolor{gray!10}
BF16 &BF16  &  84.43 &93.87 &842 &2319&1.690 & -\\ \hline
W4-g128(AWQ)& BF16  &83.93 (-0.50) &93.13 (-0.74) &828 (-14) &2252 (-67) &\textbf{2.057} (+0.367) &\color{myred}{-17.8\% }\\ \hline
A8-per-token dyn  & \multirow{4}{*}{W4-per-channel sta}& \cellcolor{yellow!7} 84.32 (-0.11)&\cellcolor{yellow!7} 93.61 (-0.26)	&\cellcolor{yellow!7} 830 (-12)&\cellcolor{yellow!5} 2269 (-50) & \cellcolor{yellow!7} 1.253 (-0.437) &\color{mygreen1}{+34.9\%}   \\ %
A8-per-tensor sta  & &  \cellcolor{yellow!20} 40.20 {(-44.12)} & \cellcolor{yellow!20} 38.82 {(-54.79)}&\cellcolor{yellow!20} 422 {(-408)} &\cellcolor{yellow!20}1082 {(-1187)}&\cellcolor{yellow!20} \textbf{1.016} (\textbf{-0.674}) & \color{mygreen1}{\textbf{+66.3\%}}  \\
% &  & & & AIFS &1.958 (\textbf{\color{mygreen1}{$\uparrow$22.6\%}})\\
% \rowcolor{gray!25}
A8-MSQ & & 84.32 (-0.11)	&93.61 (-0.26) &830 (-12) &2269 (-50)&1.085 (-0.605) & \color{mygreen1}{+55.8\%}\\ \cline{3-8}
\rowcolor{blue!10}
\textbf{A8-MSQ+AIFS} & & \textbf{84.32} (-0.11)	& \textbf{93.61} (-0.26) &\textbf{830} (-12)&\textbf{2269} (-50)& \underline{1.017} (-0.673) & \color{mygreen1}{\textbf{+66.2\%}}\\
\bottomrule
\end{tabular}
}
\vspace{-5mm}
\end{table*}

%\paragraph{Multi-round case Experiments.} 
\paragraph{Acceleration for Multi-batch and Multi-turn Inference.}
\textbf{\raisebox{-0.5pt}{\ding[1.1]{182\relax}} For multi-batch scenarios}, we employ a “text-image-text” input setting at 2240$\times$2240 resolution, with each batch containing text lengths varying from 50 to 200, and uniformly generating 512 tokens during decoding. We use left-padding (\texttt{pad\_token\_id}) to align sequences of different lengths, masking out the padded regions to ensure they do not affect attention computations (Figure~\ref{multi-batch mask} provides a schematic). Notably, our AIFS procedure remains fully compatible with multi-batch inputs, requiring no additional overhead. As shown in Table~\ref{tab:speed_multibatch}, MQuant consistently achieves around 20\% faster inference across both prefill and decode stages compared to the floating-point baseline when the batch size ranges from 1 to 4. \textbf{\raisebox{-0.5pt}{\ding[1.1]{183\relax}} For multi-turn dialogue}, we retain the same “text-image-text” format and resolution, with each turn containing 50 text tokens and a uniform decode length of 512 tokens. By maintaining key-value caches and position IDs across turns, we preserve context during the dialogue. As Table~\ref{table:multi-turns-comparison} shows, MQuant reduces end-to-end inference time by up to 38.7\% over the floating-point baseline for 1--3 turns, showing the efficiency of our quantization approach in multi-turn settings.

\vspace{-3.5mm}
\subsection{Ablation Study}
\vspace{-2mm}
In this section, we select Qwen2-VL-7B~\citep{Qwen2VL}, currently one of the most powerful open-source MLLMs, to ablate the effectiveness of our proposed designs.

\begin{table}[h]
% \vspace{-2mm}
\caption{Ablation study of proposed quantization designs on Qwen2-VL-7B~\cite{Qwen2VL} with W4A8 setting.}
\vspace{-2mm}
\label{table:ablation}
\centering
\Huge
\resizebox{\linewidth}{!}{
\begin{tabular}{ccc|ccccc}
\toprule
\multicolumn{3}{c|}{\textbf{Methods}}& \multirow{2}{*}{\textbf{T.VQA$\uparrow$}} & \multirow{2}{*}{\textbf{D.VQA$\uparrow$}} & \multirow{2}{*}{\textbf{OCRB.$\uparrow$}} & \multirow{2}{*}{\textbf{MME$\uparrow$}} & \multirow{2}{*}{\textbf{Lat (ms) $\downarrow$}}\\
Static& AIFS + MSQ &RMS & \\
\toprule
\rowcolor{gray!12}
\multicolumn{3}{c|}{\textbf{BF16}} & 84.43 &93.87 &842 &2319 &6523\\ \hline
\cc & \xx &\xx & 71.44 & 83.96 & 670 & 1911 & 5479\\
\cc & \cc& \xx  & 78.95 & 87.55 & 721 & 2095 &5484\\
% &  \cc& \cc&  \cc &\xx& & & 82.48 & 91.91 & 803 & 2174 &\textcolor{myblue}{5452}\\
  \rowcolor{blue!10}
\cc& \cc& \cc &\textbf{84.32} &\textbf{93.58} &\textbf{824} &\textbf{2255} &5471\\
\bottomrule
\end{tabular}
}
\vspace{-7mm}
\end{table}

\paragraph{Ablation Study of Proposed Quantization Methods.}
We perform ablations on Qwen2-VL-7B (Table~\ref{table:ablation}) to isolate the contributions of each quantization component. Beginning with a baseline that applies GPTQ plus Hadamard transformations to both the LLM and vision parts, we progressively introduce MSQ and AIFS for the LLM. This significantly boosts accuracy, highlighting their effectiveness on text tokens. Next, leveraging offline and online Hadamard rotations in the vision encoder further improves results. Finally, incorporating RMS to suppress newly arising outliers achieves near-floating-point performance, underscoring the overall robustness and efficiency of our pipeline.

\begin{figure}[h]
\vspace{-3mm}
    \centering    \includegraphics[width=0.99\linewidth]{figures/AIFS_speedup.pdf}
    \vspace{-2mm}
    \caption{The Speedup of AIFS+MSQ on Qwen2-VL-7B.}
    \label{fig:AIFS_speed}
\vspace{-6mm}
\end{figure}

\paragraph{Accuracy \& Speedup via AIFS.}
While \textbf{MSQ} addresses the modality discrepancy, \textbf{AIFS} further reorders visual and textual tokens into a unified sequence to enable efficient per-tensor static quantization of activations. Table~\ref{table:AIFS_laten} shows that \textbf{MSQ}+\textbf{AIFS} not only achieves the same speedup as naïve per-tensor static quantization but also maintains near-floating-point accuracy across linear layers. Figure~\ref{fig:AIFS_speed} further illustrates that \textbf{AIFS} yields speedups of \textbf{20\%--80\%} as resolution increases, corroborating that rearranging tokens avoids the high overhead of dynamic per-token quantization.


\vspace{-2mm}
\begin{table}[h]
\caption{Comparative quantized results under different quantization settings on Qwen2-VL-7B~\citep{Qwen2VL}. ${\dagger}$ means re-implementation based on the official weight-only quantization setting with a group size of 128.}
\vspace{-2mm}
\label{table:weight-only}
\centering
\Huge
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|cccc}
\toprule
\multirow{2}{*}{\textbf{Method}}& \multicolumn{2}{c|}{\textbf{Bits Setting}} & \multirow{2}{*}{\textbf{T.VQA $\uparrow$}} & \multirow{2}{*}{\textbf{D.VQA $\uparrow$}} & \multirow{2}{*}{\textbf{OCRB.$\uparrow$}} & \multirow{2}{*}{\textbf{MME$\uparrow$}} \\
& Visual & LLM & \\
\toprule
\rowcolor{gray!10}
- & BF16 & BF16 & 84.43 &93.87 &842 &2319\\
GPTQ (g128)$^{\dagger}$  &BF16 &W8& 84.33 &93.97 &842 &2313 \\
GPTQ (g128)$^{\dagger}$ &BF16 &W4& 84.18 &93.25 &831 &2285 \\
 AWQ (g128)$^{\dagger}$ &BF16 &W4 &  83.93 &93.13 &828 &2252 \\ \cline{1-7} 
% \hline
\rowcolor{gray!15}
\textbf{MQuant} (g128)  & BF16 &W4 & 84.55 &93.18 &\textbf{832} &\textbf{2304} \\ \cline{1-7}
% \hline
  \rowcolor{gray!25}
\textbf{MQuant} (g128) & W4 &W4& \textbf{84.70} &\textbf{93.57} &828 &2292 \\ \cline{1-7}
  \rowcolor{blue!10}
\textbf{MQuant} & W4A8 &W4A8& 84.32 & 93.58 &824 &2255 \\
\bottomrule
\end{tabular}}
\vspace{-6mm}
\end{table}

\paragraph{Weight-only Quantization.} We compare our weight-only quantization results with the official Qwen2-VL-7B, which uses GPTQ~\citep{frantar2022gptq} and AWQ~\citep{lin2023awq} to quantize only the LLM while keeping the visual encoder in BF16. For fairness, we adopt the same group size of 128. As shown in Table 6, our W4 LLM quantization aligns with their settings, achieving nearly lossless accuracy compared to other methods. Extending W4 quantization to the visual encoder enables 4-bit quantization of the entire MLLM; MQuant still performs comparably to the BF16 model and even surpasses other methods. Under the W4A8 weight-activation quantization setting, our results remain consistent, with some metrics surpassing advanced weight-only methods. These experiments confirm MQuant’s effectiveness and robustness across various quantization configurations, whether for weight-only or weight-activation quantization, and for partial or full quantization of MLLMs.

