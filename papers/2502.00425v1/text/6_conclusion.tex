\vspace{-3mm}
\section{Conclusion} \label{sec:conclusion}
\vspace{-2mm}
In this paper, we propose \emph{MQuant}, an accurate and efficient post-training quantization (PTQ) framework specifically designed for MLLMs. Our approach addresses the unique challenges of MLLMs by applying \textbf{Modality-Specific Static Quantization (MSQ)} to handle distribution mismatches between vision and text tokens, and introducing an \textbf{Attention-Invariant Flexible Switching (AIFS)} mechanism to reduce time-to-first-token (TTFT) with static per-tensor scaling. Furthermore, we reveal that online Hadamard rotations can introduce severe weight outliers, and propose \textbf{Rotation Magnitude Suppression (RMS)} to effectively mitigate this issue. Extensive experiments on five mainstream MLLMs demonstrate that \emph{MQuant} achieves near-floating-point accuracy under the W4A8 setting, highlighting its potential to advance MLLMs quantization and facilitate practical deployment in resource-constrained environments.