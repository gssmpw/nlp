\section{Introduction}
\label{intro}
\vspace{-2mm}
Recent advances in large language models (LLMs)~\citep{brown2020language,touvron2023llama,llama2,llama3} have led
to remarkable performance on a wide range of natural language processing tasks. However, these models often struggle
when dealing with non-textual data such as images or videos. Multimodal large language models (MLLMs)~\citep{reid2024gemini,achiam2023gpt4,wang2023cogvlm}
address this limitation by integrating visual and textual modalities, thereby enabling more comprehensive understanding
and reasoning. Despite these benefits, their large parameter sizes, coupled with substantial computational demands, poses
a major challenge for real-world deployment, particularly in resource-constrained or privacy-sensitive environments.


\begin{figure*}[t]
    \centering    
    \includegraphics[width=1.0\textwidth]{figures/Figure1_obsV2.pdf}
    \vspace{-2.6mm}
    \caption{(a) The number of prefill visual tokens across different MLLMs as the image splits or resolution increases. (b) The activation values of visual tokens range from $-20$ to $10$, whereas textual tokens are centered around $0$, with only a few having absolute magnitudes exceeding $0.5$.} 
    \label{fig:obs12}
\vspace{-5mm}
\end{figure*}

\textbf{Challenges in MLLM Quantization.} Quantization has proven an effective strategy for reducing memory usage
and inference costs in LLMs~\citep{yuan2023rptq,2023omniquant}, by converting high-precision parameters (e.g., FP32)
into lower-bit representations (e.g., INT8). Yet, the transition from LLMs to MLLMs brings three unique difficulties:
\vspace{-4mm}
\begin{enumerate}[leftmargin=1.2em,itemsep=2pt,parsep=1pt]
    \item \textbf{Time-to-First-Token (TTFT) Explosion.} MLLMs often generate large numbers of visual tokens (e.g.,
    patch embeddings or region proposals) with the resolution and aspect ratio of input images or videos. As shown in Fig \ref{fig:obs12}(a), in models such as Qwen2-VL~\citep{Qwen2VL}, the number of prefill visual tokens grows as image resolution increases (detailed figure in Fig.~\ref{fig:obs_reso}). As image or video resolution increases, the prefill visual tokens can
    escalate dramatically---leading to high TTFT and negatively impacting latency-sensitive tasks. Moreover, in higher-demand scenarios such as video-based tasks and multi-image dialogues, the accumulation of visual tokens becomes even more pronounced, further exacerbating the increase in TTFT. Per-token dynamic
    quantization, though flexible, exacerbates this overhead by computing scale factors token-by-token.
    
    \item \textbf{Disparate Modality Distributions.} As shown in Fig \ref{fig:obs12} (b), the activation distributions between visual and textual tokens reveal substantial numerical discrepancies, where visual token activations can span a significantly broader range
    (e.g., $-20$ to $10$) than textual tokens, which typically center near $0$. A single global scale factor for both visual
    and textual tokens leads to either aggressive clipping of visual outliers or increased quantization granularity for text, harming
    overall accuracy.
    
    \item \textbf{Visual Outliers.} High-magnitude outliers in visual tokens, often reflecting salient image regions,
    make traditional clipping-based methods unsuitable. Naively clipping these outliers can cause severe accuracy drops,
    illustrated in Table~\ref{fig:abla_visual_txt}.
\end{enumerate}

\begin{table}[h]
\renewcommand\arraystretch{1.0} 
\centering
\small
\vspace{-3mm}
\caption{Ablations on the clip range for different tokens.}
\vspace{-3mm}
\label{fig:abla_visual_txt}
\setlength{\tabcolsep}{0.8mm}
\scalebox{0.95}{
\resizebox{\columnwidth}{!}{
\begin{tabular}{cc|cc}
\toprule
Clip Range & Bits (W/A) & Visual Tokens & Textual Tokens \\ \midrule
-& BF16 / BF16 & \multicolumn{2}{c}{\textbf{61.40}} \\ \midrule
(0-1.0) & BF16 / INT16 & 61.20 (\color{myred}{$\downarrow$0.20}) & 61.25 (\color{myred}{$\downarrow$0.15})\\
(0-0.99999) & BF16 / INT16 & \textbf{18.92} (\textbf{\color{myred}{$\downarrow$42.48}}) & 60.09 (\color{myred}{$\downarrow$1.31}) \\
\hline
\end{tabular}%
}
}
\vspace{-3mm}
\end{table}

\textbf{Our Approach.} To tackle these challenges, we propose \emph{MQuant}, an accurate and efficient \emph{post-training
quantization} (PTQ) framework \emph{specifically tailored to MLLMs}. Our contributions center around:
\begin{enumerate}[leftmargin=1.2em,itemsep=2pt,parsep=1pt]
    \item \textbf{Modality-Specific Static Quantization (MSQ)} and \textbf{Attention-Invariant Flexible Switching (AIFS)},
    which apply distinct per-tensor scales for visual vs.\ textual tokens and reorder multimodal inputs to avoid repeated,
    token-wise scale computation. By doing so, MQuant slashes TTFT and preserves accuracy.

    \item \textbf{Rotation Magnitude Suppression (RMS)} for mitigating fresh outliers introduced by online Hadamard
    rotations. Our theoretical analysis reveals the emergence of large-mean channels in MLLM weights, and RMS effectively
    reduces these outliers with minimal overhead.
\end{enumerate}

\textbf{Experimental Results and Impact.} 
We evaluate \emph{MQuant} on five mainstream MLLMs---InternVL~\citep{internvl15},
Qwen-VL~\citep{qwenvl}, MiniCPM-V~\citep{yao2024minicpmv}, CogVLM2~\citep{CogVLM2}, and Qwen2-VL~\citep{Qwen2VL}---over
multiple vision-language benchmarks. Under a W4A8 setting, \emph{MQuant} routinely maintains $>98\%$ of the floating-point
accuracy, significantly outperforming baseline quantization methods. In addition, our modality-specific approach and token
reordering reduce inference latency by effectively handling thousands of visual tokens at scale. We will release 
\textcolor{red}{\href{https://github.com/StiphyJay/MQuant}{code}}
to facilitate further development and encourage real-world adoption in resource-limited scenarios.

In summary, \emph{MQuant} is, to our knowledge, the first PTQ framework \emph{explicitly} designed for MLLMs, demonstrating
both near-lossless performance and reduced computational demands at lower bit-widths. We hope our method paves the way
for broader, faster, and more efficient multimodal LLM deployments.