\section{Method}\label{sec: method}

\vspace{-2mm}
In this section, we present \emph{MQuant}, a post-training quantization solution specifically designed for MLLMs. In Sec. \ref{UniToekn}, we describe modality-specific static quantization (MSQ) and attention-invariant flexible switching (AIFS). In Sec. \ref{sec:fht-outlier}, we identify the weight outliers caused by the online Hadamard rotations and state Rotation Magnitude Suppression (RMS). We provide the detailed MQuant algorithm for FP MLLMs in Appendix~\ref{Mquant} Algorithm \ref{alg:MQuant}.



\subsection{Modality-Specific Static Quantization and Attention-Invariant Flexible Switching}
\label{UniToekn}

\noindent
\textbf{Motivation and Overview.}
Multi-modal Large Language Models (MLLMs) often generate a large number of visual tokens. As discussed in Section~\ref{intro}, the number of these tokens increases rapidly with higher image or video resolution (see Fig.~\ref{fig:obs12}(a)). This growth causes high inference latency when using per-token dynamic quantization, due to expensive token-wise scale computations. Such computations are not ideal for mobile or embedded devices. In contrast, static per-tensor quantization bypasses the need for repeated scale updates and offers much lower overhead. However, na\"ively applying per-tensor quantization to both text and visual tokens can degrade accuracy, because their activation distributions differ significantly (Fig.~\ref{fig:obs12}(b)).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/AIFSV3_rebuttal.pdf}
    \vspace{-1.5mm}
    \caption{Overview of Modality-Specific Static Quantization (MSQ) and Attention-Invariant Flexible Switching (AIFS). AIFS reorders tokens so that all visual tokens appear first, then the textual tokens, while adjusting the causal mask to preserve the original model logic.}
    \label{fig:AIFS}
    \vspace{-4mm}
\end{figure}

We address this issue with a two-part method:
(1) \emph{Modality-Specific Static Quantization (MSQ)} and
(2) \emph{Attention-Invariant Flexible Switching (AIFS)}.
MSQ applies different static scaling factors for visual and textual tokens, while AIFS reorders their positions to avoid irregular data slicing. Our method retains accuracy while benefiting from the efficiency of per-tensor quantization, especially for high-resolution inputs.

%-------------------------------------
\vspace{1mm}
\label{MSQ}
\paragraph{Modality-Specific Static Quantization (MSQ).}
Let $E$ be an input sequence of length $L$ that intermixes textual and visual tokens. Denote $E = \{\;e^t_1, \dots, e^t_{m-1},\, e^v_m, \dots, e^v_n,\, e^t_{n+1}, \dots, e^t_L\}$, where $m,n$ specify the visual token segment. We observe that visual tokens often have larger activation magnitudes, which can overshadow textual features. To handle these differences, we apply two distinct sets of \emph{static per-tensor} quantization parameters:  
\begin{equation}
\label{eq:ms-ptq}
    E \;=\;
    \underbrace{(e^t_1, \dots, e^t_{m-1})}_{\textstyle \text{text scale } s_{t}}
    \;\;\underbrace{(e^v_m, \dots, e^v_n)}_{\textstyle \text{visual scale } s_{v}}
    \;\;\underbrace{(e^t_{n+1}, \dots, e^t_L)}_{\textstyle \text{text scale } s_{t}}.
\end{equation}
Here, $s_{v}$ denotes the scale factor for visual tokens, while $s_{t}$ is for textual tokens. By calibrating $s_{v}$ and $s_{t}$ \emph{once} before inference, we avoid the overhead of per-token scale estimation. This design aligns well with hardware that favors simple per-tensor quantization. It also prevents large visual values from saturating the narrower distribution of textual tokens.

%-------------------------------------
\vspace{1mm}
\label{AIFS}
\paragraph{Attention-Invariant Flexible Switching (AIFS).}
Despite the benefits of MSQ, it complicates data handling if textual and visual tokens remain interleaved in $E$. Na\"ive slicing and concatenating at runtime can increase memory traffic and reduce efficiency for \texttt{GEMM}-based layers like QK and FC. To overcome this, we propose \emph{Attention-Invariant Flexible Switching (AIFS)}. Figure~\ref{fig:AIFS} illustrates the idea: 
we \emph{reorder} the sequence so that all visual tokens appear first, followed by textual tokens. We then alter the causal mask to preserve the original auto-regressive relationships.



Causal attention~\citep{vaswani2017attention} ensures each token can only attend to earlier tokens. Formally, for a naive sequence $E$, the attention matrix is  
\begin{equation}
    \mathbf{A} = \mathrm{Softmax}\!\bigl(\tfrac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{D}} + M_{i,j}\bigr), 
    \quad
    M_{i,j} =
    \begin{cases}
    0, & \text{if } j \leq i,\\
    -\infty, & \text{otherwise}.
    \end{cases}
\label{eq:casual_mask_mt}
\end{equation}
With AIFS, we reorder $E$ into $E^{u} = \{\;e^v_m, \dots, e^v_n,\; e^t_1,\dots,e^t_{m-1},\, \dots, e^t_{n+1}, \dots, e^t_L\}$ and obtain a \emph{unified causal mask} $M^u_{i,j}$:

\begin{equation}
    M^{u}_{i,j} = 
\begin{cases} 
0 & \text{if one of the following conditions is met:} \\
& ( i \leq (n-m), j \leq i \text{ or } (n-m) < j \leq n) \\
&\text{or } ((n-m)<i \leq n, (n-m) < j \leq i) \\
&\text{or } (i > n, j \leq i) \\
-\infty &\text{otherwise} 
\end{cases}   
\label{eq:casual_unified}
\end{equation}
The position embeddings are also shifted consistently. More details can be found in the Appendix~\ref{rope}. This guarantees that \emph{attention scores remain numerically equivalent} to the original sequence. Thus, token reordering does not change the auto-regressive logic, but it \emph{does} simplify memory operations so that we can feed all visual tokens through a single static-scale multiplication and do the same for textual tokens.

%-------------------------------------
\vspace{1mm}
\noindent
\textbf{Efficiency and Practical Benefits.}
Our experiments in Section~\ref{sec:exp} show that MSQ plus AIFS delivers three main advantages:

\begin{itemize}[leftmargin=16pt]
\itemsep0pt
    \item \emph{High Compatibility and Strong Theoretical Equivalence:}  
    The causal-mask transformation ensures that the model’s output is numerically the same as if tokens were not reordered. This reordering can be integrated into existing MLLM implementations without major code changes.
    
    \item \emph{Reduced Inference Latency:}  
    We replace per-token dynamic quantization with a static per-modality approach. This removes the need to recalculate scales per token, cutting runtime overhead and boosting throughput.  

    \item \emph{Enhanced Memory Efficiency:}  
    Placing all visual tokens first avoids repeated slicing or padding of interleaved tokens. We can handle them with a single global scale factor ($s_{v}$), then switch to $s_{t}$ for textual tokens. This reduces memory usage. We observe up to 24.7\% speedups and 152.9\% memory gains (see Table~\ref{tab:ablation2}).  
\end{itemize}

In summary, \emph{MSQ + AIFS} offers a straightforward and effective way to handle the unique challenges of multi-modal token distributions. It is especially valuable on edge devices or other constrained platforms where dynamic quantization is hard to implement at scale, yet per-tensor static quantization is hardware-friendly and fast.


\vspace{-2mm}


\subsection{FHT-Based Weight Outlier Mitigation: Rotation Magnitude Suppression}
\label{sec:fht-outlier}

\paragraph{Background and Motivation.}
Chee \emph{et~al~al.}~\citep{chee2024quip} and Tseng \emph{et~al.}~\citep{tseng2024quip+} introduced \emph{incoherence} to measure the difficulty of quantization. A lower incoherence indicates a simpler quantization scenario. Concretely, let \(W \in \mathbb{R}^{m\times n}\) be a weight matrix with singular vectors \(e_i\) and \(e_j\). They define
\begin{equation}
\label{eq:incoherence}
|W_{ij}| \;=\; \bigl|\,e_i^\top W\,e_j\bigr|\;\;\le\;\;\mu\,\frac{\|W\|_F}{\sqrt{mn}},
\end{equation}
where \(\mu\) is the \emph{incoherence coefficient}. The smaller \(\mu\) is, the easier it is to quantize \(W\). They also showed that applying a Hadamard transform to both weights and activations can effectively reduce \(\mu\). Quarot~\citep{ashkboos2024quarot} further applies \emph{offline} Hadamard transforms and a \emph{partially online} Hadamard transform, as shown in Figure~\ref{quarot}(a), to achieve state-of-the-art quantization on LLMs.

However, Multi-modal LLMs (MLLMs) combine an LLM component with a \emph{visual encoder}, which often relies on LayerNorm. Quarot’s partial online transform cannot be applied directly to all such norms. Inspired by SliceGPT~\citep{ashkboos2024slicegpt}, we convert visual encoders’ LayerNorms into RMSNorms. More
details can be found in the Appendix \ref{pre-LN2RMSN}. This change makes Quarot-like Hadamard transforms applicable to MLLMs. Yet, as Table~\ref{table:main_results} demonstrates, Quarot still underperforms on many MLLM tasks. Tseng \emph{et~al.}~\citep{tseng2024quip+} also proved that \emph{random Hadamard transforms} (RHT) can reduce incoherence for Hessians and weights, but they did not analyze \emph{online} (fast) Hadamard transforms (FHT). In Quarot, FHT is crucial for low-bit quantization (e.g., 4-bit) as shown in Table \ref{table:online-hadda}. We investigate why FHT can yield fresh outliers in MLLMs and propose a method to suppress them.

\begin{figure}[ht]
    \centering    
    \includegraphics[width=0.98\linewidth]{figures/W_outliers.pdf}
    \vspace{-1mm}
    \caption{\textbf{(a)} The pipeline of Quarot, showing offline and partially online Hadamard transforms. 
             \textbf{(b)} An MLLM weight matrix where applying an online FHT produces outliers in the first row.}
    \label{quarot}
    \vspace{-2mm}
\end{figure}

\paragraph{Channel-Mean Outliers in Online FHT.}
Following (Eq.~\ref{eq:incoherence}), let \(W_{\ell_2}\in\mathbb{R}^{n\times m}\) be a weight matrix. We denote \(\|W_{\ell_2}\|_F\) as the Frobenius norm. A Hadamard matrix \(H \in \{-1,1\}^{n\times n}\) with orthonormal rows preserves this norm, i.e., \(\|\,H\,W_{\ell_2}\|_F = \|W_{\ell_2}\|_F\). Suppose \(\mu_{\ell_2}\) is the incoherence coefficient for \(W_{\ell_2}\), satisfying 
\(\max(|W_{\ell_2}|) = \mu_{\ell_2}\,\frac{\|W_{\ell_2}\|_F}{\sqrt{mn}}\).
Let \(HW_{\ell_2}\) be the transformed weight, and let its incoherence coefficient be \(\mu_{H\ell_2}\). Then
\begin{equation}\label{eq:scale_l2}
\frac{\mu_{H\ell_2}}{\mu_{\ell_2}}
\;=\;
\frac{\max\bigl|\,H\,W_{\ell_2}\bigr|}{\max\bigl|\,W_{\ell_2}\bigr|}
\,.
\end{equation}
For many Hadamard transforms, the first row (and first column) contain identical \(\tfrac{1}{\sqrt{n}}\) entries, while other rows sum to zero. Hence, the first channel after transformation is
\[
(H W_{\ell_2})_{0j}
\;=\;
\sqrt{n}\,\mathrm{mean}\bigl(w_{:,j}\bigr).
\]
If the mean is large, the first element in that row can exceed the original maximum and thus raise \(\mu_{H\ell_2}\). Concretely, when
\begin{equation}
\label{eq:scale_l3}
\sqrt{n}\,\mathrm{mean}\bigl(w_{:,j}\bigr) \;>\; \max_{i}\,\bigl(w_{ij}\bigr),
\end{equation}
a new \emph{channel-mean outlier} appears in the first row. Figure~\ref{quarot}(b) shows such an occurrence in an MLLM. This issue arises especially in Quarot’s \emph{online} (partial) FHT step, which applies Hadamard rotations per forward pass rather than strictly offline.

\paragraph{Our Approach: Rotation Magnitude Suppression (RMS).}
We propose \emph{Rotation Magnitude Suppression (RMS)} to handle these new outliers with minimal overhead. We first identify whether a channel meets (Eq.~\ref{eq:scale_l3}). If it does, we:
\begin{enumerate}[leftmargin=1.7em, itemsep=3pt]
    \item \emph{Split} the problematic channel from the main \texttt{GEMM} kernel and process it using a separate \texttt{GEMV}. 
    \item \emph{Zero out} that row in $HW_{\ell_2}$ so that the main kernel does not double-count it.
    \item \emph{Add} the partial output (from the split \texttt{GEMV}) back to the main path before the activation.
\end{enumerate}
Figure~\ref{oswq} shows the workflow. This targeted split ensures large-mean channels do not trigger extreme first-row values during the per-forward-pass FHT. The added cost is small, since only channels meeting Eq.~\ref{eq:scale_l3} require this procedure.

\begin{figure}[ht]
    \centering    
    \includegraphics[width=0.95\linewidth]{figures/outliers_solution.pdf}
    \vspace{-1mm}
    \caption{An overview of our \textbf{RMS}. We separate outlier-prone channels into a dedicated \texttt{GEMV} path, zero their row in the main kernel, and then merge the results.}
    \label{oswq}
    \vspace{-4mm}
\end{figure}

\paragraph{Why Not Subtract Channel Means Directly?}
A straightforward idea is to separate out each channel's mean before applying $H$ and then re-inject it afterward. However, this leads to two major issues: 1. The separated means are still subject to Hadamard rotation, creating a new linear transformation where the first channel again becomes large. 2. Splitting and then re-injecting the means effectively doubles the linear operations, significantly increasing the computational cost. In contrast, our RMS approach modifies only the row triggering the outlier condition and does not require additional linear layers, thereby offering a more efficient and effective resolution.

\vspace{-4mm}
\paragraph{Summary.} Algorithm~\ref{alg:rms+quarot} outlines how RMS integrates with Quarot’s FHT or other partially online Hadamard transforms. In Table~\ref{table:online-hadda}, we show that RMS substantially curbs weight outliers in MLLMs under W4A8 quantization, raising accuracy with minimal overhead. Hence, RMS addresses a key shortcoming of online Hadamard transforms by suppressing large-mean channels.