\section{Related work on symmetric group}
The concept of grokking was introduced in \cite{Power2022} where the authors considered simple algorithmic datasets and showed that validation accuracy increases towards perfect generalization long after the training accuracy becomes close to perfect. Despite overfitting early in the training process, continued optimization eventually leads to a transition where the model generalizes. This suggests that the network gradually shifts from memorization to learning the underlying algorithm. One example from \cite{Power2022} is grokking of group multiplication in the permutation group $S_5$. For a given pair of permutations  $a$ and $b$ the model needs to compute a permutation $c$ such that $a\circ b = c$. Grokking and interpretability of the group multiplication in small permutation groups has been studied in a series of subsequent works \cite{ChughtaiCN2021,StanderYFB2023,Wu2024} (see also \cite{NandaCLSS2023}). In the present work we also consider a symmetric group problem but in contrast with previous works we ask whether a model is able to generalize from training on $S_k$ data to testing on $S_n$ data with $k\ll n$. Although we have not observed grokking, test accuracy is nearly 100\%. At the time of writing it is an open question whether our models have discovered a general algorithm.

Our result is reminiscent of out of distribution (OOD) learning. In mathematical problems OOD learning has been studied in, for example, \cite{Charton2022}. We test OOD learning in the context of the symmetric group, using a scalable presentation with general transpositions as group generators, as well as a less scalable, more local approach using only elementary transpositions. One motivation for this study is to identify an effective machine learning approach to improve on the unknot problem \cite{GukovHRS:2020}.

An important part of earlier works on grokking in symmetric group is analysis of models using mechanistic interpretability. This can be done using different circuit level approaches \cite{Olah2020,Elhage2021,Olsson2022,Quirke2024,Zhang2023,Zhong2023,Gross2024}. Examining activations of individual neurons and intervening in various parts of the network can help us to discover circuits that implement the algorithm. We aim to perform a similar analysis of our models in a future work.

The code used for this project is available on GitHub at \cite{Petschack_github}.