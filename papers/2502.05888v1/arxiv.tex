\documentclass[11pt, a4paper]{article}

\usepackage[margin=1in]{geometry}

\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{xspace}
\usepackage{url}
\usepackage[hypertexnames=false]{hyperref}
\usepackage[dvipsnames]{xcolor}


\hypersetup{colorlinks=true,allcolors=blue}


\usepackage{multirow}
\usepackage{microtype}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{thm-restate}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[capitalise,noabbrev]{cleveref}
\usepackage{comment}



\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{claim*}{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{property}[theorem]{Property}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remark*}{Remark}

\newcommand{\ProblemName}[1]{\textsc{#1}}
\newcommand{\kCenter}{\ProblemName{$k$-Center}\xspace}

\newcommand{\E}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\hash}{\varphi}
\def\eps{\varepsilon}

\newcommand{\uniform}{{\tt uniform}}
\newcommand{\lowdim}{{\tt low-dim}}
\newcommand{\benchmark}{{\tt benchmark}}


\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmiccomment}[1]{\hfill$\triangleright${\em \ #1}}



\DeclareMathOperator{\cost}{cost}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\apx}{apx}
\DeclareMathOperator{\opt}{opt}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\rep}{rep}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\distEst}{\widehat{\dist}}



\newcommand{\alglinelabel}{\addtocounter{ALC@line}{-1}\refstepcounter{ALC@line}\label }


\allowdisplaybreaks




\title{Faster Approximation Algorithms for $k$-Center
via Data Reduction\thanks{This work is the outcome of an initial group work at the ``Massive Data Models and Computational Geometry'' workshop at the University of Bonn.}}


\author{
  Arnold Filtser%
  \thanks{
    Supported by the Israel Science Foundation (grant No. 1042/22).
    Email: \texttt{arnold.filtser@biu.ac.il}
  }\\
  Bar-Ilan university
  \and
  Shaofeng H.-C. Jiang%
  \thanks{
    Partially supported by a national key R\&D program of China No. 2021YFA1000900 and
    a startup fund from Peking University.
    Email: \texttt{shaofeng.jiang@pku.edu.cn}
  }\\
  Peking University
  \and 
  Yi Li%
  \thanks{
    Supported in part by Singapore Ministry of Education AcRF Tier 2 grant MOE-T2EP20122-0001.
    Email: \texttt{yili@ntu.edu.sg}
  }\\
  Nanyang Technological University
  \and
  Anurag Murty Naredla%
  \thanks{
    Email: \texttt{anuragmurty@uni-bonn.de}
  }\\
  University of Bonn
  \and
  Ioannis Psarros%
  \thanks{
    Partially supported by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program.
    Email: \texttt{ipsarros@athenarc.gr}
  }\\
  Archimedes, Athena Research Center
  \and
  Qiaoyuan Yang%
  \thanks{
    Email: \texttt{2401111962@stu.pku.edu.cn}
  }\\
  Peking University
  \and 
  Qin Zhang%
  \thanks{
    Supported in part by NSF CCF-1844234.
    Email: \texttt{qzhangcs@iu.edu}
  }\\
  Indiana University Bloomington
}



\begin{document}


\maketitle


\begin{abstract}
    We study efficient algorithms for the Euclidean $k$-Center problem, focusing on the regime of large $k$.
    We take the approach of data reduction by considering $\alpha$-coreset,
    which is a small subset $S$ of the dataset $P$ such that any $\beta$-approximation on $S$ is an $(\alpha + \beta)$-approximation on $P$.
    We give efficient algorithms to construct coresets whose size is $k \cdot o(n)$,
    which immediately speeds up existing approximation algorithms.
    Notably, we obtain a near-linear time $O(1)$-approximation  when $k = n^c$ for any $0 < c < 1$.
We validate the performance of our coresets on real-world datasets with large $k$,
    and we observe that the coreset speeds up the well-known Gonzalez algorithm
    by up to $4$ times, while still achieving similar clustering cost.
    Technically, one of our coreset results is based on a new efficient construction of consistent hashing with competitive parameters. This general tool may be of independent interest for algorithm design in high dimensional Euclidean spaces.    
\end{abstract} \section{Introduction}
\label{sec:intro}




The \kCenter problem
is a fundamental clustering problem that has been extensively studied in various areas, including combinatorial optimization, data science, and machine learning.
In \kCenter, the input is a dataset $P \subseteq \RR^d$ of $n$ points and a parameter $k$. The goal is to find a set $C \subseteq \RR^d$ (called centers) with $\left| C \right| = k$ that minimizes the cost function
\begin{equation*}
    \cost(P, C) := \max_{p \in P} \dist(p, C),
\end{equation*}
where $\dist(p, q) := \|p - q\|_2$ and $\dist(p, C) := \min_{c \in C}\dist(p, c)$.
The \kCenter problem presents substantial computational challenges and remains  APX-hard even when $d = O(1)$~\cite{FederG88}.


The study of efficient approximation algorithms for \kCenter dates back to the 1980s, with several algorithms providing $2$-approximations ~\cite{Gonzalez85,HochbaumS85,HochbaumS86}. Among these, the classic algorithm of Gonzalez~\cite{Gonzalez85} runs in $O(nkd)$ time. While it achieves good performance for $k = O(1)$, its $O(nk)$ dependence renders it much less efficient for large $k$.
In fact, large values of $k$ in $k$-clustering are increasingly relevant in modern applications such as product quantization for nearest neighbor search~\cite{JegouDS11}  in vector databases.
This has motivated algorithmic studies for the large $k$ regime for $k$-clustering in various computational models~\cite{EneIM11,BateniEFM21,CoyCM23,CzumajGJK024,corr/abs-2407-11217}.
The record linkage problem, also known as entity resolution or reference reconciliation, has been a subject of study in databases for decades~\cite{KSS06,HSW07,DN09}. This problem can also be viewed as a $k$-center clustering problem, where $k$ represents the number of ground truth entities and is often very large.



For the large-$k$ regime,
it is possible to obtain a subquadratic $\tilde O(n^{2 - \sqrt \epsilon})$ time\footnote{Throughout the paper, the $\tilde{O}$-notation hides the dependence on $\poly(d \log n)$.
} $(2 + O(\epsilon))$-approximation algorithm\footnote{This can be obtained by combining an $O(n^{2 - \sqrt \epsilon})$-time $(1 + O(\epsilon))$-approximate $r$-net construction \cite{AvarikiotiEKP20} with a standard reduction of \kCenter to net constructions~\cite{HochbaumS86}.},
and a general ratio-time trade-off as an $O(c)$-approximation in $\tilde O(n^{1 + 1 / c^2})$ time~\cite{EppsteinHS20}.
Yet, it is unknown if these trade-offs can be improved.
The ideal goal is to design an $O(1)$-approximation in near-linear time $\tilde O(n)$,
which would directly improve the original Gonzalez's runtime by removing an $O(k)$ factor.
However, this seems to be challenging with current techniques. Indeed, even the apparently easier task of finding an $O(1)$-approximation to the cost of a \emph{given} center set $C$, without optimizing $C$, in near-linear $\tilde O(n)$ time remains unsolved.


In this paper, we present new trade-offs between approximation ratio and running time for \kCenter. 
Specifically, we focus on optimizing the trade-off in the case of $k = n^c\ (0 < c < 1)$.
Our results make a significant step forward towards the ultimate goal of achieving near-linear running time for $O(1)$-approximation in this regime.





 \begin{table*}[t]
     \caption{Summary of approximation algorithms for \kCenter in $\mathbb{R}^d$}
     \label{tab:results}
     \vspace*{-8pt}
     \centering
     \begin{tabular}{lll}
         \toprule
         Approx.\ ratio & Running time  &  Reference \\
         \midrule
         2           & $O(nkd)$      &  \cite{Gonzalez85} \\        
$O(\alpha)$ & $\tilde{O}(n^{1 + 1 / \alpha^2})$ &  \cite{EppsteinHS20} \\
         $O(\alpha)$ & $\tilde{O}(n + k^{1 + 1 / \alpha^2}n^{O(1 / \alpha^{2 / 3})})$ &  \Cref{thm:intro_hash} + \cite{EppsteinHS20} \\
         $O(\alpha)$ & $\tilde{O}(n k^{1 / \alpha^2})$ & \Cref{thm:intro_sample} + \cite{EppsteinHS20} \\
         \bottomrule
     \end{tabular}
 \end{table*}
\subsection{Our Results}
We take a \emph{data reduction} approach to systematically improve the running time of approximation algorithms for \kCenter.
Specifically, we use the notion of an $\alpha$-\emph{coreset} (for $\alpha \geq 1$), defined as a subset $S \subseteq P$ of the dataset $P$
such that any $\beta$-approximate solution to \kCenter on $S$ is an $(\alpha + \beta)$-approximation on the original dataset $P$. 


Our main result consists of two coresets with slightly different parameter trade-offs, both of size $k \cdot o(n)$.
This essentially reduces the input size from $n$ to $k \cdot o(n)$, speeding up any (existing) approximation algorithm.
Notably,
we obtain an $O(1)$-approximation in near-linear time
for $k = n^c\ (0 < c < 1)$.
We summarize the new approximation algorithms in \Cref{tab:results}.


\noindent\textbf{Coresets.\ }
Our first result, \Cref{thm:intro_hash}, constructs an $O(\alpha)$-coreset of size $O(k n^{1 / \alpha^{2 / 3}})$ in a runtime that is near-linear in $n$ and independent of $\alpha$.
Running the existing $\tilde{O}(n^{1 + 1 / \alpha^2})$-time $O(\alpha)$-approximation algorithm~\cite{EppsteinHS20} on this coreset,
we obtain an $O(\alpha)$-approximation in time $\tilde{O}(n + k^{1 + 1 / \alpha^2} n^{O(1 / \alpha^{2 / 3})})$.
This immediately improves the algorithm in~\cite{EppsteinHS20}.
In particular, as long as $k = n^c$ (for any $0 < c < 1$) which can be arbitrarily close to linear $n$, this running time reduces to $\tilde{O}(n)$ by setting $\alpha = \poly((1-c)^{-1})$. 

\begin{restatable}{theorem}{thmintrohash}
\label{thm:intro_hash}
    For every $\alpha \geq  1$, there exists an $O(\alpha)$-coreset of size 
    $\tilde{O}(k n^{1 / \alpha^{2/3}})$ that can be computed in time $\tilde{O}(n)$ with probability at least $0.99$.
\end{restatable}
Our \Cref{thm:intro_hash} relies on a geometric hashing technique called \emph{consistent hashing}~\cite{CzumajJK0Y22} (see \Cref{def:hashing}).
Our main technical contribution is to devise a new consistent hashing that offers a competitive parameter trade-off,
while still running in $\poly(d)$ time,
exponentially improving the previous $\exp(d)$ time construction~\cite{CzumajJK0Y23} (albeit theirs achieves better parameter trade-offs).
See \Cref{sec:hash} for a more detailed discussion.
This new hashing result may be useful for algorithm design in high-dimensional Euclidean spaces in general.



Our second coreset (\Cref{thm:intro_sample}) has size $\tilde{O}(k)$, which is independent of $\alpha$,
but has a larger $\tilde O(nk^{1 / \alpha^2})$ construction time.
Running the algorithm in \cite{EppsteinHS20} on this coreset,
we obtain an alternative $O(\alpha)$-approximation for \kCenter in $\tilde{O}(n k^{1 / \alpha^2})$ time.




\begin{restatable}{theorem}{thmintrosample}
    \label{thm:intro_sample}
    For every $ \alpha \geq 1$, there exists an $O(\alpha)$-coreset of size $k\cdot \polylog(n)$ that can be computed in time $\tilde{O}(nk^{1/ \alpha^2})$ with  probability at least $0.99$.
\end{restatable}

Previously, it was observed that the point sequence discovered by an (approximate) furthest-neighbor traversal as in Gonzalez's algorithm~\cite{Gonzalez85} is an $O(1)$-coreset~\cite{BravermanJKW21},
    and one could use an algorithm in~\cite{EppsteinHS20} to find such a sequence,
    which yields an $O(\alpha)$-coreset of size $O(k)$ in time $\tilde O(n^{1 + 1 / \alpha^2})$.
While this coreset size is competitive, the running time remains super-linear in $k$ for $k = n^c\ (0 < c < 1)$, which is too large for our purpose of near-linear algorithms.














\noindent\textbf{Experiments.\ }
Our experiments validate the performance of our coresets,
with a focus on \Cref{thm:intro_hash},
since \Cref{thm:intro_hash} leads to near-linear running time for \kCenter when $k = n^c\ (0 < c < 1)$, which is likely to be practical.
Our experiments are conducted on four real-world datasets of various sizes and dimensions,
and we evaluate the speedup of the well-known Gonzalez's algorithm~\cite{Gonzalez85} on our coreset.
The experiments show that
our coreset provides a consistently better tradeoff between the coreset size and clustering cost than all baselines we compare,
and runs $2$ to $4$ times faster than directly running Gonzalez algorithm on the dataset,
while still achieving comparable cost values.










%
 \subsection{Related Work}
\label{sec:related}



Our notion of coreset is related to the widely considered \emph{strong} coreset~\cite{AgarwalHV04,Har-PeledM04},
which is a subset $S \subseteq P$ satisfying that $\cost(S, C) \in (1 \pm \epsilon) \cost(P, C)$ for \emph{all} center sets $C \subseteq \RR^d$.
The key difference is that ours may not preserve the cost value on $S$ for all $C$,
but it does preserve the approximation ratio.
Moreover, this stronger notion inherently leads to a prohibitively large coreset size of $\exp(\Omega(d))$, even for $k = 1$.\footnote{This lower bound is folklore, but can be easily proved using an $\epsilon$-net on the unit sphere.
}
Our notion is sometimes referred to as \emph{weak} coresets in the literature, and similar notions were also considered in~\cite{FeldmanMS07,MunteanuS18,HuangJL23,CGJK25}. \section{Preliminaries}
\label{sec:prelim}
\paragraph{Notations.}
For $m \in \mathbb{N}_{\geq 1}$, denote $[m] := \{ 1, \ldots, m \}$.
For a point set $C\subseteq\mathbb{R}^d$, let $\diam(C)$ denote the diameter of $C$.
For $x \in \RR^d$ and $r \geq 0$, the ball of radius $r$ centered at $x$ is denoted by $B(x, r) = \{y \in \RR^d : \dist (x, y) \leq r\}$, and we write $B_S(x, r) := B(x, r) \cap S$ for $S \subseteq \mathbb{R}^d$.
For two sets $A, B \subseteq \RR^d$,
their Minkowski sum is $A \oplus B := \{ x + y : x \in A, y \in B \}$.
For a function $f:\RR^d\to\RR^d$ and a set $S\subseteq \RR^d$, we define $f(S) := \{f(x): x\in S\}$.

\begin{definition}[Covering]
    \label{def:covering}
    Given a set $P \subseteq \RR^d$ and $\rho\geq 0$,  a subset $S \subseteq P$  is called a  $\rho$-covering for $P$ 
	if for every $p \in P$, there exists a $q\in S$ such that $\dist(p,q)\leq \rho$.
\end{definition}

The following lemma will be useful in both of our coreset constructions. Its proof is deferred to \Cref{sec:proof_lognapx}.
\begin{restatable}[Coarse approximation]{lemma}{lemmalognapx}
    \label{lemma:lognapx}
    There is an algorithm that, given as input a dataset $P \subset \RR^d$ with $\left| P \right| = n$ and an integer $k \geq 1$, computes a $\poly(n)$-approximation to the \kCenter cost value with probability at least $1-1 / \poly(n)$, running in time $O(nd + n\polylog(n))$.
\end{restatable}



 \section{Efficient Consistent Hashing}
\label{sec:hash}



The notion of \emph{consistent hashing} was coined in \cite{CzumajJK0Y23}, which partitions $\RR^d$ into cells such that each small ball in $\RR^d$ intersects only a small number of cells. Partitions with similar properties have also been studied under the notion of \emph{sparse partitions} for general metric spaces (see, e.g.,~\cite{JiaLNRS05,Filtser24}). The main differences are that consistent hashing requires the partition to be defined using a (data-oblivious) hash function and emphasizes computational efficiency.

Below we present our formal definition of consistent hashing, which relaxes the definition of \cite{CzumajJK0Y23} by only requiring the number of intersecting cells to be bounded in expectation.
\begin{definition}
	\label{def:hashing}
	A $(\Gamma, \Lambda,\ell)$-\emph{consistent hashing} is a distribution over 
	functions $\varphi : \RR^d \to \RR^d$ such that for every $x \in \RR^d$,
	\begin{itemize}[noitemsep,topsep=-1em]
		\item (diameter) 
		$\diam(\varphi^{-1}( x )) \leq \ell$, and
		\item (consistency) $\E[|\varphi(B(x,\frac\ell\Gamma)|] \leq \Lambda$.
	\end{itemize}
\end{definition}

Since consistent hashings are scale invariant in $\RR^d$, 
we omit the parameter $\ell$ in our discussion below. Ours and previous results are summarized in \Cref{tab:hash}.



For every parameter $\beta\ge 1$, \cite{Filtser24} constructed a \emph{deterministic}  consistent hashing (namely, the consistency guarantee is worst-case and not in expectation) with parameters $\Gamma=\beta$ and $\Lambda=\tilde{O}(d)\cdot\exp(O(d/\beta))$. However, computing $\hash(x)$ for a given point $x$ requires both time and space that are exponential in $d$.
Nevertheless, Filtser showed that this trade-off between $\Gamma$ and $\Lambda$ is tight up to second order terms regardless of runtime, even when the consistency guarantee is relaxed to expectation only (implicitly).
\cite{CzumajJK0Y23} constructed a deterministic consistent hashing with the same parameters, requiring only $\tilde{O}(d^2)$ space, though the function evaluation still takes exponential time in $d$.
They also constructed a time- and space-efficient consistent hashing, which can be evaluated in $\poly(d)$ time but with sub-optimal parameters of $\Lambda$ and $\Gamma$. 







\begin{table*}[t]
    \caption{Summary of results on consistent hashing in Euclidean $\mathbb{R}^d$. The third result is a lower bound.}
    \label{tab:hash}
    \centering
    \begin{tabular}{llllll}
    \toprule
$\Gamma$ & $\Lambda$ & Guarantee & Runtime & Space & Reference \\              
\midrule
$1$         &  $\exp(O(d))$        &  worst-case          &  $\exp(d)$       &  $\exp(d)$     & \cite{JiaLNRS05} \\                    $\beta$         & $\tilde{O}(d)\cdot\exp(O(d/\beta))$          &  worst-case          & $\exp(d)$        & $\exp(d)$      & {\cite{Filtser24}}                  \\ $\beta$       & $\Lambda>(1+\frac{1}{2\beta})^d$          &    expected (implicit)       &  N/A       & N/A      & \cite{Filtser24} \\ $\beta$         & $\tilde{O}(d)\cdot\exp(O(d/\beta))$          &  worst-case          & $\poly(d)$        & $\exp(d)$&   {\cite{CzumajJK0Y23}}                       \\ $O(d^{1.5})$         & $O(d)$          & worst-case          & $O(d)$        & $\poly(d)$      &  \cite{CzumajJK0Y23}                \\ $\beta$         &   $\poly(d)\cdot \exp(O(d / \beta^{\frac23}))$        &  expected         &   $O(d)$      &   $O(d)$    & 
    \Cref{lemma:hash}                   \\ \bottomrule
\end{tabular}
\end{table*}

Our hash function is the first to achieve the bound $\Lambda = \exp(O(d / \beta^c))$ (for some $0 < c \leq 1$) when $\Gamma = \beta$ for every $\beta \geq 1$, while still running in \emph{polynomial} time in $d$.
Technically, we construct the hash function using a surprisingly simple randomly-shifted grid, which is widely used in geometric algorithm design.


Previous works also studied laminar consistent hashing \cite{BDRRS12,BCFHHR23}, which is a sequence of hash functions at different scales, each refining the previous one.
We note also that \cite{CZ16} studied a related notion to consistent hashing, but their diameter guarantee was only probabilistic, so it is not directly comparable.







\begin{lemma}
    \label{lemma:hash}
    For every $\beta \geq \sqrt{2\pi}$ and $\ell > 0$, there exists a $(\beta, t_\beta ,\ell)$-consistent hashing $\varphi : \RR^d \to \RR^d$ with 
     $t_\beta := \poly(d)\cdot \exp(O(d / \beta^{\frac23}))$ which can be computed 
     in $O(d)$ time.
\end{lemma}
\begin{proof}
    Since it suffices to define the hash function for an (arbitrary) fixed $\ell$, in this proof we fix $\ell := \sqrt{d}$.

    \noindent\textbf{Construction.\ }
    The hash is defined by a randomly-shifted grid. Formally, we first choose a uniformly random vector $v\in [0,1]^d$ and, for each $x\in\RR^d$, define $\hash(x) = \lfloor x+v\rfloor$. Here,
     for a vector $z = (z_1,\dots,z_d)\in\RR^d$, we define $\lfloor z\rfloor = (\lfloor z_1\rfloor, \lfloor z_2\rfloor,\dots,\lfloor z_d\rfloor)\in\RR^d$, i.e, rounding down $z$ coordinatewise.
    


    \noindent\textbf{Analysis.\ }
To evaluate $\varphi(x)$, we simply round $x+v$ down coordinatewise to the nearest integer, which takes $O(d)$ time.
    The diameter property is also straightforward, since $\hash^{-1}(t) = \times_{i=1}^d [t_i-v_i,t_i-v_i+1)$ ($t\in\mathbb{Z}^d$), which is a half-open unit cube and has diameter $\sqrt{d}=\ell$.

It remains to verify that an arbitrary ball of radius $r = \sqrt{d}/\beta$ intersects only ${\rm poly}(d) \exp(O(d / \beta^{\frac{2}{3}}))$ grid cells in expectation.
    Let $x\in\mathbb{R}^d$ be arbitrary and consider the ball $B(x,r)$. Let $\tilde{r} = \lceil r\rceil$. By symmetry, we can assume w.l.o.g. that $x\in[\tilde{r},\tilde{r}+1)^d$.
    Further, for the sake of analysis only, we will slightly change the hash function.
    Let $\Delta\gg \tilde{r}$ be some fixed large integer to be determined later.
    Instead of sampling $v\in[0,1]^d$, we sample $v=(v_1,\dots,v_d)\in[0,\Delta]^d$ uniformly at random and map each point $y$ to $\lfloor y + v\rfloor$.
    Note that the number of intersecting grid cells by a ball centered at $(x+v)$ equals the number of intersecting cells by a ball centered at $x+(v_1\bmod 1,\dots,v_d\bmod 1)$.
    Thus, the two hash functions have exactly the same expected consistency.

    Since $x\in[\tilde{r},\tilde{r}+1)^d$ and $v\in[0,\Delta]^d$, the ball $B(x+v,r)$ can
    only intersect grid cells in the box ${\cal G}=\left[0,\Delta+3\tilde{r}\right]^{d}$. Fix some grid cell $K=\times_{i = 1}^d [t_i, t_i + 1)\subset{\cal G}$.
    Let $X_K$ be an indicator for the event that the ball $B(x+v,r)$ intersects $K$.
This happens if and only if the ball 
    $B(v,r)$ intersects the box $\times_{i=1}^{d}[t_{i}-x_{i},t_{i}-x_{i}+1)$, or, 
    $v$ is contained in the Minkowski sum of the box $\times_{i=1}^{d}[t_{i}-x_{i},t_{i}-x_{i}+1)$ and the ball $B(\vec{0},r)$.
The following lemma bounds the volume of this Minkowski sum.\begin{lemma}[{\cite{aiger2014reporting}}, Lemma 3.1]\label{lemma:AKS14}
        Let $C = [0,1]^{d}$ be the unit cube in $\RR^d$ and $0 < r \leq \sqrt{d/2\pi}$ be a parameter. Let $C_{r}=C\oplus B(\vec{0},r)$, then 
        $${\vol}\left(C_{r}\right)\le{\rm poly}(d)\cdot \exp\left(3/2 \cdot (2\pi)^{\frac{1}{3}}d^{\frac{2}{3}} r^{\frac{2}{3}}\right)~.$$
    \end{lemma}
    Applying \Cref{lemma:AKS14} with our $r = \sqrt{d} / \beta$, we have
    \begin{align*}
    {\vol}\left(C_{\sqrt{d}/\beta}\right) & \leq{\rm poly}(d)\cdot\exp\left(\frac{3}{2}(2\pi)^{\frac{1}{3}}d^{\frac{2}{3}}\cdot(\sqrt{d}/\beta)^{\frac{2}{3}}\right)\\
     & ={\rm poly}(d)\cdot\exp\left(\frac{3}{2}(2\pi)^{\frac{1}{3}}d/\beta^{\frac{2}{3}}\right)\\
     & ={\rm poly}(d)\cdot\exp\left(O(d/\beta^{\frac{2}{3}})\right)~.
    \end{align*}
    Therefore,    
\begin{align*}
\E\,X_{K} & \overset{(*)}{\le}\frac{{\vol}\left([t_{i}-x_{i},t_{i}-x_{i}+1)\oplus B(\vec{0},r)\right)}{{\vol}\left([0,\Delta]^{d}\right)}\\
 & =\frac{{\vol}\left(C_{r}\right)}{\Delta^{d}}=\frac{1}{\Delta^{d}}\cdot{\rm poly}(d)\cdot\exp(O(d/\beta^{\frac{2}{3}}))~.
\end{align*}
    Here $^{(*)}$ is an inequality, rather than equality, because the Minkowski sum $[t_{i}-x_{i},t_{i}-x_{i}+1)\oplus B(\vec{0},r)$ might not be fully contained in $[0,\Delta]^d$.
    Only grid cells from ${\cal G}=\left[0,\Delta+3\tilde{r}\right]^{d}$ have a non-zero probability of intersecting $B(x+v,r)$. Since there are only $(\Delta+3\tilde{r})^d$ such grid cells $K$,
    by linearity of expectation, the expected number of grid cells intersecting $B(x+v,r)$ is at most 
    \begin{align*}
        {(\Delta+3\tilde{r})^{d}}/{\Delta^{d}}&\cdot{\rm poly}(d)\cdot\exp(O({d}/{\beta^{\frac{2}{3}}}))  \\
        &={\rm poly}(d)\cdot\exp(O({d}/{\beta^{\frac{2}{3}}}))~,
    \end{align*}
    where the last equality holds for large enough $\Delta$. This verifies the consistency bound of the consistent hashing and completes the proof of \Cref{lemma:hash}.
\end{proof} \section{Proof of \Cref{thm:intro_hash}}
\label{sec:cover_hash}






We prove \Cref{thm:intro_hash} in this section (restated below).  
\thmintrohash*


We start by reducing the task of finding coresets to the construction of $\rho$-\emph{coverings} (see \Cref{def:covering})
via a standard fact that any $\alpha$-approximation on an $(\beta \opt)$-covering is a $(\alpha + \beta)$-approximation to \kCenter (see \Cref{lemma:cover_to_approx});
hence, it suffices to find a small $(\beta\opt)$-covering as a coreset.
Indeed, covering is a fundamental notion in geometric optimization. In the context of \kCenter,
it can be viewed as a bi-criteria approximation that uses slightly more than $k$ center points.


\begin{lemma}
\label{lemma:cover_to_approx}
    For a dataset $P \subseteq \RR^d$ and integer $k$,
    consider a $(\beta \opt)$-covering $S \subseteq P$ for some $\beta \geq 1$.
    Then any $\alpha$-approximation on $S$ is an $(\alpha + \beta)$-approximation on $P$ for \kCenter.
    In other words, $S$ is a $\beta$-coreset.
\end{lemma}
\begin{proof}
For a generic point set $W \subseteq \mathbb{R}^d$ and a point $x \in \mathbb{R}^d$,
    we define the projection function $\pi_W(x) := \argmin_{y \in W} \dist(x, y)$, which maps $x$ to its nearest neighbor in $W$ (ties are broken arbitrarily).
    Since $S$ is a ($\beta\opt$)-covering,
    for every $p \in P$ we have $\dist(p, \pi_S(p)) \leq \beta \opt$.
    Let $\widehat{C}$ be an $\alpha$-approximation to \kCenter on $S$.
    Then, \begin{align*}
         \cost(P, \widehat{C}) &= \max_{p \in P} \dist(p, \pi_{\widehat{C}}(p)) \\
         &\leq \max_{p \in P} \dist(p, \pi_S(p)) + \dist(\pi_S(p), \pi_{\widehat{C}}(\pi_S(p))) \\
         &\leq \max_{p \in P} \dist(p, \pi_S(p)) + \max_{p\in S}\dist(p, \pi_{\widehat{C}}(p)) \\
         &\leq \beta \opt + \alpha \opt_S \\
         &\leq \beta \opt + \alpha \opt,
     \end{align*}
     where the last inequality follows from the fact that the optimal \kCenter cost on the subset $S$ cannot be larger than the optimal \kCenter cost on $P$, which is true since we consider the continuous version of the \kCenter problem where centers are chosen from the entire $\mathbb{R}^d$. 
\end{proof}

Thanks to \Cref{lemma:cover_to_approx},
it remains to find a small $(\beta \opt)$-covering.
We give the following construction of covering based on consistent hashing (\Cref{def:hashing}). This is the main technical lemma for \Cref{thm:intro_hash}. Its proof is postponed to \Cref{sec:proof_cover_hash}.

\begin{lemma}
    \label{lemma:cover_hash}
    There is an algorithm that takes as input a dataset $P \subseteq \RR^d$ with $\left| P \right| = n$, $\beta \geq 1$ and  integer $k \geq 1$,
    computes a set $S \subseteq P$ with $|S| \leq k\cdot\poly(d)\exp(d / \beta^{2/3})$ in time $O(n d \log n)$,
    such that $S$ is an $O(\beta \opt)$-covering of $P$ with probability at least $0.991$.
\end{lemma}

\Cref{lemma:cover_hash} allows us to compute a covering set whose size is exponential in the dimension (assuming $d\gg \beta$). To mitigate this, we apply the Johnson-Lindenstrauss (JL) transform~\cite{JL84}, using random projections to reduce the dimension of the input point set to $O(\log n)$. The JL Lemma is restated as follows.

\begin{lemma}[Johnson-Lindenstrauss Lemma] 
    \label{lemma:JL}
    Let $P \subseteq \RR^d$ be a set of $n$ points and $\epsilon \in (0, \frac{1}{2})$. Then there exists a map $f : P \rightarrow \RR^{d'}$ for some $d'=O(\epsilon^{-2}\log n)$ such that
    \[
        (1-\epsilon) \dist(x,y) \leq \dist(f(x),f(y)) \\\leq (1+\epsilon) \dist(x,y)
    \]
    for all $x,y\in P$.
    Moreover, the image $f(P)$ can be computed in $O(\eps^{-2} nd \log n)$ time with probability at least $1-{1}/{\poly(n)}$.
\end{lemma}

Now we are ready to conclude the proof of \Cref{thm:intro_hash}.
\begin{proof}[Proof of \Cref{thm:intro_hash}]
For a generic point set $W\subseteq \mathbb{R}^d$, let $\opt(W)$ be the optimal \kCenter value on $W$.
The algorithm for \Cref{thm:intro_hash} goes as follows.
We first run \Cref{lemma:JL} with some constant $\epsilon = O(1)$ to obtain a mapping $f : \mathbb{R}^d \to \mathbb{R}^{d'}$
where $d' = O(\log n)$.
Let $P' := f(P)$ be the dataset in the target space after JL.
Then, we apply \Cref{lemma:cover_hash} on $P'$, to obtain an $O(\alpha\opt(P'))$-covering $S' \subseteq P'$ of $P'$.
Let $S := f^{-1}(S')$, and this is well-defined since $S'$ is a subset of $P'$ and $f$ is injective on $P$.
The algorithm returns $S$ as the covering.

The running time follows immediately from \Cref{lemma:cover_hash,lemma:JL}.
Next, we verify that $S$ is a desired covering. Conditioning on the success of \Cref{lemma:JL}, i.e.,
for every $x, y \in P$, $\dist(f(x), f(y)) \in (1 \pm \epsilon) \dist(x, y)$, we consider an arbitrary $x \in P$. Then
\begin{align*}
    \dist(x, S)
    &\leq (1 + \epsilon) \cdot \dist(f(x), S') \\
    &\leq (1 + \epsilon) \cdot O(\alpha \opt(P')) \\
    &\leq 2 (1 + \epsilon)^2 \cdot O(\alpha) \opt(P) \\
    &\leq O(\alpha) \opt(P),
\end{align*}
where the first inequality directly follows from the conditioned event,
and the third inequality from the claim that $\opt(P') \leq 2 (1 + \epsilon) \opt(P)$, which can be derived from the conditioned event as follows\footnote{In fact, one can show $\opt(P') \leq (1 + \epsilon)\opt(P)$, which has also been analyzed in, e.g., \cite{JiangKS24}.}.
 Consider a $2$-approximation $\widehat{C} \subseteq P$ of \kCenter on $P$ (for instance, consider the solution of Gonzalez's algorithm~\cite{Gonzalez85}),
and the condition implies $\opt(P') \leq \cost(P', f(\widehat{C}) )\leq 2 (1 + \epsilon) \opt$.
Finally, the failure probability follows from a union bound of the failure of \Cref{lemma:cover_hash,lemma:JL}.
This finishes the proof.
\end{proof}


\subsection{Proof of \Cref{lemma:cover_hash}}
\label{sec:proof_cover_hash}

\noindent\textbf{Proof overview.\ }
The covering construction is based on consistent hashing (see \Cref{def:hashing}).
Consider the $k$ clusters $C_1^*, \ldots, C_k^*$ in an optimal solution, then by definition $\bigcup_i C_i^* = P$ and $\diam(C_i^*) \leq 2\opt$ for all $C_i^\ast$.
Roughly speaking, the key property of a consistent hashing $\hash$,
is that each $C_i^*$ is mapped to $|\hash(C_i^*)| \leq \Lambda$ distinct buckets,
and that each bucket has diameter $O(\alpha \opt)$, where $\Lambda$ is a parameter of the hashing and we have $\Lambda = \poly(d)\cdot \exp(O(d / \beta^{\frac23}))$ in our construction (\Cref{lemma:hash}).
Then, picking an arbitrary point from every non-empty bucket yields an $O(\alpha \opt)$-covering of size $k \Lambda$.
This hash $\hash$ is data-oblivious and we can evaluate $\hash(x)$ for every $x\in \RR^d$ in $O(d)$ time,
which leads to a $\tilde{O}(n)$ running time of \Cref{lemma:cover_hash}.




\begin{algorithm}[t]
    \caption{Covering based on consistent hashing}
    \label{alg:main_hash}
    \begin{algorithmic}[1]
\STATE $\apx\leftarrow$ a 
           $\gamma$-approximate of $\kCenter(P)$ using \Cref{lemma:lognapx}, where $\gamma = \poly(n)$.
            \alglinelabel{line:coarse_approx}
\STATE $t_\beta \gets \poly(d)\cdot\exp(O(d / \beta^{2/3}))$ (as in \Cref{lemma:hash})




\FOR{$i=0$ to $\left\lceil \log\gamma\right\rceil$}
                \STATE $\tau\leftarrow \frac{\apx}{\gamma}\cdot2^{i}$
                \STATE let $\hash_\tau$ be a $(\beta, t_\beta,\beta\tau )$-hashing sampled using \Cref{lemma:hash}
                \alglinelabel{line:hash}
                \STATE for each $z \in \hash_\tau(P)$,
                pick an arbitrary \emph{representative} point $\rep(z)$ from the bucket $\hash_\tau^{-1}(z) \cap P$
                \STATE let $S_\tau \gets \{ \rep(z) : z \in \hash_\tau(P) \}$
                \alglinelabel{line:Stau}
\STATE if $\left|S_\tau \right| \leq 200 kt_\beta$
                    then \textbf{return} $S_\tau$ 
                \alglinelabel{line:Stau_ub}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Algorithm.\ }
The algorithm is listed in \Cref{alg:main_hash}.
Let $\opt$ denote the cost of the optimal solution to the \kCenter on $P$.
The algorithm starts by finding a $\poly(n)$-approximation to $\opt$ (using \Cref{lemma:lognapx}). It then checks $O(\log n)$ geometrically increasing values of $\tau$, one of which estimates $\opt$ up to a factor of $2$.
For each value $\tau$, we pick a consistent hash $\varphi_\tau$ as in \Cref{lemma:hash}, with scale parameter $\beta\cdot \tau$, such that  
the points of every ball of radius $\frac{\beta\cdot \tau}{\beta}=\tau$ 
are hashed into only $t_\beta=\poly(d)\cdot\exp(O(d / \beta^{2/3}))$ cells in expectation.
For each hash $\varphi_\tau$, the algorithm computes a set $S_\tau$, containing a single representative from every nonempty hash cell.
Once $|S_\tau|\le 10k\cdot t_\beta$, the algorithm halts and returns $S_\tau$.

Consider an estimate $\tau$ such that $\frac\tau2<\opt\le\tau$.
The points in $P$ are contained in $k$ balls of radius $\opt<\tau$ (around the centers in the optimal solution).
Under $\hash_\tau$, the points within each of these balls are hashed into only $t_\beta$ cells in expectation. This implies that, with high constant probability, $|S_\tau|\le 200k\cdot t_\beta$, leading the algorithm to halt and return $S_\tau$. We now proceed with a formal proof.


















\begin{lemma}
    \label{lemma:H_covering}
For every $\tau$, the set $S_\tau$ is a $(\beta \tau)$-covering for $P$ (with probability $1$).
\end{lemma}
\begin{proof}
Clearly, $S_\tau \subseteq P$.
    Now, fix some $p \in P$, let $z := \hash_\tau(p)$.
    Then $\rep(z) \in S_\tau$,
    and by \Cref{def:hashing}, we have
    $\dist(p, S_\tau)
    \leq \dist(p, \rep(z))
    \leq \diam(\hash_\tau^{-1}(z)) \leq \beta \tau$.
    This verifies the definition of $\beta \tau$-covering.
\end{proof}
    

\begin{lemma}
    \label{lemma:lcovering_size}
    For $\tau \geq \opt$, $|S_\tau| \leq 200 k t_\beta$ with probability at least $0.995$ (over the randomness of $\hash_\tau$).
\end{lemma}
\begin{proof}


Let $C^* = \{c_1^*, ..., c_k^*\}$ be an optimal solution for \kCenter.
    Then $P$ can be covered by the $k$ balls of radius $\opt$ around $c_j^*$'s, i.e., 
    $P = \bigcup_{j=1}^k B_P(c_j^*, \opt)$.
    As $\opt\le\tau$, by linearity of expectation, it holds that 
\begin{align*}
        \mathbb{E}\left[\left|S_{\tau}\right|\right] & =\mathbb{E}\left[\left|\hash_{\tau}(P)\right|\right]
        =\mathbb{E}\Big[\Big|\hash_{\tau}(\bigcup_{j=1}^{k}B_{P}(c_{j}^{*},\opt))\Big|\Big]\nonumber\\
 & \le\sum_{j=1}^{k}\mathbb{E}\left[\left|\hash_{\tau}(B_{P}(c_{j}^{*},\opt))\right|\right]\le k\cdot t_{\beta}~.
\end{align*}
By Markov's inequality, $\Pr[|S_\tau| > 200 k t_\beta] \leq 0.005$.
\end{proof}






\begin{proof}[Proof of \Cref{lemma:cover_hash}]


    We define the two  following events:
    \begin{itemize}[noitemsep,topsep=0pt]
        \item $\mathcal{E}_{\rm apx}$: the event that the \kCenter approximation algorithm in \Cref{lemma:lognapx} succeeds: $\opt\le\apx\le\gamma\cdot\opt$.
        \item $\mathcal{E}_{\rm hash}$: the event that for $\tau$ such that $\opt \le \tau < 2\opt$, it holds that
        $|S_\tau| \leq 200 k t_\beta$.
    \end{itemize}
    By \Cref{lemma:lognapx} and \Cref{lemma:lcovering_size}, with probability at least $0.991$, events $\mathcal{E}_{\rm apx}$ and $\mathcal{E}_{\rm hash}$ both happen.
    We now condition on both events and, in the rest of the proof, argue that the algorithm succeeds. \Cref{lemma:cover_hash} will then follow.
    






\noindent\textbf{Covering property.\ }
    The algorithm iterates over different values of $\tau$, starting at $\tau_0=\frac{\apx}{\gamma}\le\opt$, and increase $\tau$ in jumps of $2$, with the maximum value being $\tau_{\lceil\log \gamma\rceil}=\frac{\apx}{\gamma}\cdot2^{\lceil\log \gamma\rceil}\ge\apx\ge\opt$.
    Let $\tau'$ be the estimate such that $\opt\le\tau'<2\opt$.
    If the algorithm will reach the estimate $\tau'$, then as we conditioned on $\mathcal{E}_{\rm hash}$, the algorithm will halt and return $S_{\tau'}$.
    Otherwise, the algorithm will halt earlier at some value $\tau\le\frac12\cdot\tau'\le\opt$.
    In either case, by \Cref{lemma:H_covering}, the algorithm returns a set $S_\tau$, which is a $(\beta\tau)$-covering for $P$. Note that $\beta\tau\le2\beta\cdot\opt$.
    


    \noindent\textbf{Running time.\ }
    The invocation of \Cref{lemma:lognapx} in Line~\ref{line:coarse_approx}
    only takes $O(nd + n\log n)$ time.
The for-loop runs  at most $O(\log n)$  times,
    and in each iteration, it takes $O(n d)$ time to evaluate all hash values $\hash(P)$.
In summary, the overall running time of the algorithm is $O(n d \log n)$.
This finishes the proof of \Cref{lemma:cover_hash}.
\end{proof}




 \section{Constructing Covering via Sampling}
\label{sec:cover_sample}






We now prove \Cref{thm:intro_sample} (restated below).
\thmintrosample*

The proof is similar to that of \Cref{thm:intro_hash},
using a reduction to covering (\Cref{lemma:cover_to_approx}).
Hence, the remaining step is to find a suitable covering for \Cref{thm:intro_sample},
which is stated in the following lemma.
The lemma relies on an approximate nearest neighbor search (ANN)
structure,
where given a set of input points $T$ and a query point $q$,
the $\alpha$-ANN finds for each a point $x \in T$ such that $\dist(x, q) \leq \alpha \cdot\dist(q, T)$.






\begin{lemma}
    \label{lemma:cover_sample}
    Given a set of points $P\subseteq \RR^d$, and $k,\beta\ge1$, there is an algorithm that runs in $\tilde{O}(n k^{1/\beta^2})$ time, and with probability at least $0.991$ returns a set $S\subseteq P$ of $ k\cdot \polylog (n)$ points such that $S$ is an $O(\beta\opt)$-covering of $P$.
\end{lemma}

Note that \Cref{thm:intro_sample} follows directly from \Cref{lemma:cover_sample,lemma:cover_to_approx}.
The rest of this section proves \Cref{lemma:cover_sample}.





\noindent\textbf{Proof overview for \Cref{lemma:cover_sample}.\ }
Our proof is based on random hitting sets.
For the sake of presentation, assume that the $k$ clusters in an optimal solution are of similar size $\Theta(\frac nk)$.
Then a uniform sample $S$ of size $O(k \log n)$ would hit all clusters w.h.p. .
Furthermore, by the definition of \kCenter, the entire dataset is contained in a $(2\opt)$-neighborhood of $S$. This $(2\opt)$-neighborhood of $S$ gives the covering and can be computed using ANN.
The general case where the clusters are not balanced can be handled similarly.
Specifically, for a random sample $S$ of $O(k\log n)$ points, at least half of the points will, with high probability, be within a distance of $2\opt$ from $S$. We can eliminate these points, and repeat this process for $O(\log n)$ rounds to cover all points.




\noindent\textbf{Algorithm.\ }
The algorithm (\Cref{alg:main_sample}) begins by computing a $\poly(n)$-approximation to $\opt$, denoted by $\apx$.
Then it checks $O(\log n)$ geometrically increasing values of $\tau$, one of which estimates $\opt$ up to a factor of $2$.
For each value of $\tau$, the algorithm attempts to construct a coreset $S_\tau$ such that for every point $x\in P$, $\dist(x, S_\tau) \leq 2\beta \tau$.
In more detail, a set $Q$ of uncovered points is maintained. The process consists of $L=O(\log n)$ iterations, where in each iteration, $O(k\log n)$ random (uncovered) points from $Q$ are added to $S_\tau$. 
ANN is then invoked at Line~\ref{line:ANN}, using the newly sampled points as the input set and the uncovered points in $Q$ as queries, where we use the ANN algorithm of~\cite{AndoniI06}.
Every point whose $\beta$-approximate nearest neighbor is within a distance of at most $2\beta\tau$ is subsequently removed from $Q$. 
If $Q$ becomes empty during the $O(\log n)$ iterations, the algorithm returns $S_\tau$.

\begin{algorithm}[t]
	\caption{Covering based on sampling}
	\label{alg:main_sample}
	\begin{algorithmic}[1]
		\STATE $\apx\leftarrow$ a 
		$\gamma$-approximate of $\kCenter(P)$ using \Cref{lemma:lognapx}, where $\gamma = \poly(n)$
\FOR{$i\gets0$ to $\left\lceil \log\gamma\right\rceil$}
\STATE $\tau\leftarrow\frac{\apx}{\gamma}\cdot 2^i$,~~ $S_\tau \gets \emptyset$,~~ $Q^{(0)} \gets P$
		\FOR{$j \gets 1$ to $L=5\lceil\log n\rceil$} \alglinelabel{line:sample_for_loop}
		\STATE draw $O(k \log n)$ uniform samples $S^{(j)}$ with replacement from $Q^{(j - 1)}$
\STATE for each $x \in Q^{(j-1)}$, 
        compute $\distEst(x, S^{(j)})$ such that
        $ \distEst(x, S^{(j)}) \leq \beta \dist(x, S^{(j)})$ using ANN
\alglinelabel{line:ANN}
		\STATE $R^{(j)} \gets \{ x \in Q^{(j - 1)} : \distEst(x,S^{(j)}) \leq 2 \beta \tau \}$
\alglinelabel{line:sample_remove}
		\STATE 
		$S_\tau \gets S_\tau \cup S^{(j)}$,~~ $Q^{(j)} \gets Q^{(j - 1)} \setminus R^{(j)}$
		\STATE if $Q^{(j)} = \emptyset$ 
		then \textbf{return} $S_\tau$
        \alglinelabel{line:IfQLempty}
\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

For the analysis, consider $\tau\ge\opt$, and $Q\subseteq P$. Since $Q$ can be covered by $k$ balls of radius $\tau$, 
by an averaging argument, at least half of the points in $Q$ must belong to the balls that each contains at least a $\frac{1}{2k}$ fraction of $Q$.
Consequently, each such point will, with constant probability, be within a distance of at most $2\tau$ from the sampled points and will thus be removed from $Q$.
It follows that $Q$ is expected to shrink in size by a constant factor in each iteration and, after $O(\log n)$ iterations, becomes empty.
The running time is dominated by the executions of ANNs.
The next lemma is the main technical guarantee of \Cref{alg:main_sample}.


\begin{restatable}{lemma}{lemmasamplegoodtau}
\label{lemma:sample_good_tau}
    Suppose that $\tau \geq \opt$. With probability at least $1-1/n$, there exists $j\leq L$ such that $Q^{(j)} = \emptyset$ at Line~\ref{line:IfQLempty}.
\end{restatable}

\begin{proof}
    Fix some iteration $j$ in the inner for-loop (Line~\ref{line:sample_for_loop}).
    We claim that 
    \begin{equation}\label{eqn:QjShrink}
        \Pr\Big[|Q^{(j)}|\le\frac12\cdot |Q^{(j-1)}|\Big]\ge\frac12~.
    \end{equation}
    That is, in any given iteration, the size of $Q^{(j)}$ decreases by a factor of at least $2$ with probability at least $\frac12$.
    Over the $L$ iterations of the for-loop, if this size reduction occurs in at least $\log n$ iterations, then $Q^{(j')}$ will be the empty set for some $j'\le L$.
    Assuming that inequality (\ref{eqn:QjShrink}) holds, the probability that the size reduction occurs less than $\log n$ times is negligibly small:
    \begin{align*}
         & \Pr\left[Q^{(L)}\ne\emptyset\right]\\
         & \le\sum_{q=0}^{\left\lceil \log n\right\rceil }{L \choose q}\cdot\frac{1}{2^{L}}
         \overset{(*)}{\le}\frac{1}{2^{L}} \left(\frac{L\cdot e}{\left\lceil \log n\right\rceil }\right)^{\left\lceil \log n\right\rceil }\\
         & =\frac{1}{32^{\left\lceil \log n\right\rceil }}\cdot\left(5e\right)^{\left\lceil \log n\right\rceil } < \frac{1}{2^{\left\lceil \log n\right\rceil }}\le\frac{1}{n}~,
    \end{align*}
    where $^{(*)}$ follows from the bound $\sum_{i=0}^k {n\choose i}\le(\frac{en}{k})^k$ (see, e.g., Exercise 0.0.5 in \cite{vershynin18}).
    
    It remains to prove inequality (\ref{eqn:QjShrink}).
    Let $\{ c_1^*, \ldots, c_k^* \}$ be an optimal solution for \kCenter on $P$,
    thus $Q^{(j-1)}\subseteq P\subseteq\bigcup_{q = 1}^k B_P(c_q^*, \opt)$.
    Let $C_1,C_2,\dots,C_k$ be a partition of $Q^{(j-1)}$ such that $C_q\subseteq B_P(c_q^*, \opt)$ for all $q\in [k]$.
    In particular, $\diam(C_q) \leq 2\opt$ for each $C_q$.
    We say that a cluster $C_q$ is \emph{large} if $|C_q|\ge\frac{|Q^{(j-1)}|}{2k}$, and \emph{small} otherwise. Denote by ${\cal C}$ the set of large clusters.
    Note that the number of points in small clusters is at most $k\cdot\frac{|Q^{(j-1)}|}{2k}=\frac{|Q^{(j-1)}|}{2}$.
Let $\Psi$ be the event that the set $S^{(j)}$ of samples contains at least one point from each large cluster. By a union bound, the probability that $\Psi$ does not happen is 
    \begin{align*}
        \Pr\left[\overline{\Psi}\right] & =\Pr\left[\exists~\text{large cluster }C_{q}\text{ s.t. }C_{q}\cap S^{(j)}=\emptyset\right]\\
         & \le\sum_{C_{q}\in{\cal C}}\Pr\left[C_{q}\cap S^{(j)}=\emptyset\right]\\
         & =\sum_{C_{q}\in{\cal C}}\left(1-\frac{|C_{q}|}{|Q^{(j-1)}|}\right)^{O(k\cdot\log n)}\\
         & \le k\cdot\left(1-\frac{1}{2k}\right)^{O(k\cdot\log n)}=\frac{k}{n^{O(1)}}\le\frac{1}{n}~.
    \end{align*}
    Next, condition on $\Psi$.
    Then $S^{(j)}$ contains a point from $C_q$ for every large cluster $C_q$. Since each cluster has diameter at most $2\opt$ and more than half of the points belong to large clusters, it follows that there are at least $\frac{|Q^{(j-1)}|}{2}$ points in $Q^{(j-1)}$, each of which is within a distance of at most $2\opt\leq2\tau$ from some point in $S^{(j)}$.
    Our ANN will return with high probability, for each $x\in Q^{(j)}$, an estimate $\distEst(x,S^{(j)})$ such that $\distEst(x,S^{(j)})\leq \beta\dist(x,S^{(j)})$. 
It follows that $\distEst(x,S^{(j)})\le 2\beta\tau$ for all the large cluster points.
Consequently, all these points will be included in $R^{(j)}$ and thus
    $|Q^{(j)}|=\left|Q^{(j-1)}\setminus R^{(j)}\right|\le \frac{|Q^{(j-1)}|}{2}$, with probability at least $\frac12$, as claimed.
\end{proof}














\begin{proof}[Proof of \Cref{lemma:cover_sample}]
During the execution of the algorithm we construct an $\beta$-ANN structure $O(\log^2n)$ times, each with input size $O(k\log n)$.
    On each such data structure we perform at most $n$ queries. 
    Specifically, we use the $\beta$-ANN algorithm of  \cite{AndoniI06}, which takes $\tilde O(m^{1 + 1 / \beta^2})$ pre-processing time 
    and $\tilde O(m^{1 / \beta^2})$ query time to compute an $O(\beta)$-approximate NN for each query point in $\mathbb{R}^d$,
    over an $m$-point input set. This algorithm answers all $O(n)$ queries successfully with $1 - 1 / \poly(n)$ probability.
    Hence, the overall running time is $\tilde{O}(n\cdot k^{1/\beta^2})$ (where we set $m = O(k \log n)$).
    This also dominates all the other steps.


    Let $i\in[0,\lceil\log\gamma\rceil]$ be such that $\frac{\apx}{\gamma}\cdot 2^{i-1}<\opt\le \frac{\apx}{\gamma}\cdot 2^{i}$.
    If the algorithm terminates at iteration $i'<i$, then it has found a set $S_\tau$ which is a $\rho$-covering of $P$ for $\rho = 2\beta\cdot \frac{\apx}{\gamma}\cdot 2^{i'-1} < 2\beta\opt$,  
    as claimed.
    Otherwise, by \Cref{lemma:sample_good_tau}, with probability at least $1 - 1 / n$, Algorithm~\ref{alg:main_sample} would terminate at the $i$-th iteration and, in this case,  $S_\tau$ is a $\rho$-covering of $P$ for $\rho = 2\beta\cdot \frac{\apx}{\gamma}\cdot 2^{i} \leq 2\beta \cdot 2\opt = 4\beta\opt$.
\end{proof}



 \section{Experiments}

\begin{table}[t]
	\centering
	\caption{Specifications of datasets, where $d$ is the original data 
    dimension and $d'$ is the target dimension of the JL transform.}
    \vspace*{-8pt}
	\begin{small}
		\begin{tabular}{llll}
			\toprule
			dataset  & size (approx.) & $d$ & $d'$ \\
			\midrule
			Kddcup     & 5M & 38 & 30 \\
			Covertype & 581K   & 55 & 50\\
			Census   & 2M  & 69 & 60 \\
			Fashion-MNIST  & 70K & 784 & 100 \\
			\bottomrule
			\label{tab:dataset}
		\end{tabular}
	\end{small}
\end{table}

\begin{figure*}[tbp]
	\centering
	\begin{subfigure}{0.53\textwidth}
		\centering
		\includegraphics[width=\textwidth]{fig/legend_cost.jpg}
\end{subfigure}
	\\
	\begin{subfigure}{0.24\textwidth}
		\centering
		\includegraphics[width=\textwidth]{fig/kddcup_Cost.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
		\centering
		\includegraphics[width=\textwidth]{fig/covertype_Cost.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
		\centering
		\includegraphics[width=\textwidth]{fig/census_Cost.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
		\centering
		\includegraphics[width=\textwidth]{fig/MNIST_Cost.png}
\end{subfigure}
	\vspace*{-8pt}
	\caption{The trade-off between the coreset size and the \kCenter cost for all baselines in each dataset.}
\label{fig:cost_result}
\end{figure*}

\begin{figure*}[tbp]
\centering
\begin{subfigure}{0.55\textwidth}
	\centering
	\includegraphics[width=\textwidth]{fig/legend_time.jpg}
\end{subfigure}
\\
\begin{subfigure}{0.24\textwidth}
	\centering
	\includegraphics[width=\textwidth]{fig/kddcup_tradeoff.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
	\centering
	\includegraphics[width=\textwidth]{fig/covertype_tradeoff.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
	\centering
	\includegraphics[width=\textwidth]{fig/census_tradeoff.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
	\centering
	\includegraphics[width=\textwidth]{fig/MNIST_tradeoff.png}
\end{subfigure}

\caption{The trade-off between the coreset size and the running time for all baselines in each dataset.}
\label{fig:time_result}
\end{figure*}



We implement our coreset from \Cref{thm:intro_hash} and evaluate its performance by measuring how effective it speeds up the classical algorithm of \cite{Gonzalez85},
which gives a $2$-approximation for \kCenter.
We focus on the coreset of \Cref{thm:intro_hash} due to its near-linear running time and use of simple grid structure, which makes it more practical.
Specifically, we conduct experiments with varying coreset sizes
and report running time and the corresponding clustering cost when running Gonzalez's algorithm on the coreset.


\noindent\textbf{Datasets.\ }
We use four real datasets: Kddcup~\cite{kdd_cup_1999_data_130}, Covertype~\cite{covertype_31}, Census~\cite{us_census_data_(1990)_116}, and Fashion-MNIST~\cite{mnist_dataset}.
For each dataset, we extract numerical features to construct a vector in $\RR^d$ for each record,
and we perform a Johnson-Lindenstrauss (JL) transform on each dataset.
The detailed dataset specifications as well as the target dimension of the JL transform are summarized in \Cref{tab:dataset}.


\noindent\textbf{Implementation details.\ }
The implementation of our coreset mostly follows \Cref{alg:main_hash}
but involves an important modification. Since our experiment is to generate a coreset with a specified size budget $s$ (instead of a pre-defined target error), the error parameter $\beta$, which controls the coreset size, is no longer useful.
Consequently, we replace the coreset size upper bound $10kt_\beta$
in Line~\ref{line:Stau_ub} directly with the budget $s$.
We also replace the third parameter $\ell = \beta \tau$ of the hash function in Line~\ref{line:hash} with $\tau$.
The coresets generated in this manner may not have an exact size of $s$,
but it is nonetheless close to $s$, as demonstrated by our experiment results.







\noindent\textbf{Baseline coresets.\ }
We employ three baseline coresets for \kCenter: a) take the entire dataset as the coreset (named \benchmark), serving as the benchmark for accuracy (and having a fixed coreset size); b) a heuristic coreset based on uniform sampling, called \uniform,
which samples uniformly a subset of a given size from the dataset; c) a coreset designed for low dimensions, called \lowdim, which has a worst-case size of $O(k2^{O(d)})$~\cite{AgarwalHV04}.
Note that there is no standard way to adapt the \lowdim\ construction
to our context; a na\"ive implementation can lead easily to a coreset size close to $O(k2^{O(d)})$, which is prohibitively large for our datasets.
In our experiments, we implement this \lowdim\ baseline in a manner similar to our coreset construction,
with the only difference being that we do not use a random shift in the hash function.



\noindent\textbf{Experiment setup.\ }
For all experiments, we set the number of centers $k \approx \sqrt{n}$, where $n$ denotes the size of the dataset.
For each dataset, we vary the target coreset size $s$ (ranging from $k$ up to $30k$),
compute each baseline coreset with the target size $s$,
run the Gonzalez algorithm on the coresets,
and report the running time and clustering cost (averaged over 3 independent trials).
All algorithms are implemented in C++ and compiled with Apple Clang version 15.0.0 at -O3 optimization level. All the experiments are run on a MacBook Air 15.3 with an Apple M3 chip (8 cores, 2.22 GHz), 16GB RAM, and macOS 14.4.1 (23E224).



\noindent\textbf{Experiment results.\ }
We depict in \Cref{fig:cost_result} the trade-off between the coreset size and the clustering cost of running Gonzalez on the coresets.
Our coresets consistently achieve the smallest cost compared with the \uniform\ and \lowdim\ baselines for all sizes, confirming the superior performance of our coresets. For moderately large coreset sizes, our coreset costs are within 1.3 times of Gonzalez's costs on the entire dataset.
The \uniform\ baseline performs generally worse than other baselines (albeit comparable to low-dim in Covertype), which is expected, since na\"ive uniform sampling may not hit sparse clusters.
The performance of \lowdim\ is closer to ours, which is an interesting fact, since our implementation helps it to escape the  worst-case size of $k \cdot 2^{-O(d)}$ on the tested datasets.


As mentioned, the only difference between the implementation of \lowdim\ and ours lies
in whether or not a random shift is applied in the hash function (\Cref{lemma:hash}).
Therefore, the performance gain of ours justifies the effectiveness of a random shift even on real-world datasets.
Finally, we report in \Cref{fig:time_result} the running time, including both coreset construction and the execution of Gonzalez's algorithm, as a function of coreset size.
Ours shows a comparable performance with other baselines, achieving a $2$x - $4$x speedup compared with the benchmark Gonzalez's algorithm.

Overall, we conclude that across all datasets,
our coreset consistently outperforms the \uniform\ and \lowdim\ baselines,
and the coreset-accelerated version of Gonzalez algorithm runs $2$x - $4$x faster than the benchmark Gonzalez algorithm while achieving a cost within $1.3$ times of Gonzalez's cost.
















\bibliography{ref}
\bibliographystyle{alphaurl}



\appendix


\section{Proof of \Cref{lemma:lognapx}}

\label{sec:proof_lognapx}




\lemmalognapx*

\begin{proof}
The plan is to first do a random projection to 1D, and show that the pairwise distance is preserved up to $\poly(n)$ factor.
This step takes $O(nd)$ time.
Then we can apply an off-the-shelf near-linear $n\cdot \polylog(n)$ time algorithm for \kCenter on 1D~\cite{MegiddoT83}.

Specifically, let $v \in \RR^d$ be a random vector with entries sampled independently from a standard Gaussian distribution, i.e., $v \sim \mathcal{N}(0, I_d)$.
For every point in $P$, compute its inner product with $v$, resulting in $P'$, i.e., $P' = \{\langle p,  v \rangle : \forall p \in P\}$. $P'$ can be interpreted as the projection of $P$ onto one-dimension.
It remains to prove the following distortion bound.

\begin{claim*}
    $\Pr\Bigl[\forall x,y\in P,~\frac{\dist(x,y)}{\poly(n)}\leq|\langle v,x\rangle-\langle v,y\rangle|\leq\dist(x,y)\cdot\poly(n)\Bigr]\geq1-1/\poly(n)$.
\end{claim*}



\begin{proof}[Proof of Claim]
    The claim trivially holds if $x = y$, and hence we assume $x \neq y$.
    Let $u := (\langle v, x \rangle - \langle v, y \rangle) / \dist(x, y)$.
    Then by a standard property of Gaussian,
    $u \sim \mathcal{N}(0, 1)$.
    On the one hand, by Markov's inequality, we immediately obtain $\Pr[|u| \geq \poly(n)] \leq 1 / \poly(n)$ since $\E[|u|] = \Theta(1)$.
    On the other hand, since the density function of $N(0,1)$ is upper bounded by $1/\sqrt{2\pi}$, we know that   
    $\Pr[|u| \leq 1 / \poly(n)] \leq 1 / \poly(n)$ by integrating the density function from $-1/\poly(n)$ to $1/\poly(n)$.
    This finishes the proof.
\end{proof}










\end{proof}
















\section{Composability and Reducibility of Covering}
\label{sec:composable}

Note that our $\alpha$-coresets in both \Cref{thm:intro_hash} and \Cref{thm:intro_sample}
for a point set $P$ are specifically $(\alpha \opt)$-covering for $P$.
We show that such covering is both composable and reducible, in the following claims.
Combining both claims, our coreset may be plugged in a merge-and-reduce framework~\cite{Har-PeledM04}, which has been used to obtain streaming~\cite{Har-PeledM04}, dynamic~\cite{HenzingerK20} and distributed~\cite{BalcanEL13} algorithms for clustering,
to imply algorithms for \kCenter in the mentioned settings.

However, we note that the claimed composability and reducibility may not hold directly from the definition of our coreset (which is more general than covering).

\begin{claim}
    For a generic point set $W \subseteq \mathbb{R}^d$,
    let $\opt(W)$ be the optimal \kCenter cost on $W$.
    Consider two datasets $A, B \subseteq \mathbb{R}^d$,
    and suppose $S_A, S_B$ are $(\alpha \opt(A))$-covering for $A$ and $(\alpha \opt(B))$-covering for $B$, respectively.
    Then, $S_{AB} := S_A \cup S_B$ is an $(\alpha \opt(A \cup B))$-covering for $A \cup B$.
\end{claim}
\begin{proof}
    We verify the definition.
    Consider any point $x \in A \cup B$, then
    \begin{align*}
        \dist(x, S_{AB})
        = \dist(x, S_A \cup S_B)
        = \min\{ \dist(x, S_A), \dist(x, S_B) \}
        &\leq \min\{ \alpha \opt(A), \alpha \opt(B) \} \\
        &\leq \alpha \opt(A \cup B).
    \end{align*}
    This finishes the proof.
\end{proof}

We also give the following claim on the reducibility of covering.
\begin{claim}
    Consider a dataset $P \subseteq \mathbb{R}^d$,
    and suppose $S$ is an $(\alpha\opt(P))$-covering on $P$.
    Then any $(\beta \opt(S))$-covering on $S$ is an $(\alpha + \beta)\opt(P)$-covering on $P$.
\end{claim}
\begin{proof}
    We verify the definition.
    Consider an arbitrary $(\beta \opt(S))$-covering $S'$ on $S$.
    For a generic set $W \subseteq \mathbb{R}^d$ and $y \in \mathbb{R}^d$,
    let $W(y)$ denote the neareset neighbor of $y$ in $W$.
    Then for every $x \in P$, we have
    \begin{align*}
        \dist(x, S')
        = \dist(x, S'(x))
        \leq \dist(x, S(x)) + \dist(S(x), S'(x))
        &\leq \alpha \opt(P) + \beta \opt(S) \\
        &\leq (\alpha + \beta) \opt(P).
    \end{align*}
    This finishes the proof.
\end{proof} 


\end{document}
