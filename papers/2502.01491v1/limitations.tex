\section{Limitations}
We identify the following three limitations with our work:
\begin{itemize}[]
    \item \textbf{Limited experimental setup}: inherent to selecting one (although common) experimental setup of distilling a large model into a smaller model, is that the findings need not necessarily transfer to other settings. We experimented with multiple language pairs, and also varied the beam size and student size in Appendix~\ref{ap:hyperparams} to comment on this limitation, but recognize that there are other settings that would be interesting to study, such as distilling translation models from LLMs. We opted for the more common SeqKD setup, because of its popularity in the years past; most deployed translation systems are not LLM-based (yet). Besides, SeqKD as a technique is still alive and kicking as demonstrated by, for instance, the recently developed OpusDistillery library\footnote{\url{https://github.com/Helsinki-NLP/OpusDistillery}} \citep{arefyev2024hplt}.
    \item \textbf{Niche phenomena}: ExMem and hallucinations are phenomena one would only rarely encounter when interacting with NMT systems \cite{raunak-etal-2022-salted}. We consider them worthy of investigation, nonetheless, because they represent extreme system failure, that goes far beyond a simple mistranslation. 
    \item \textbf{Reliability issues not yet solved}: Adaptive-SeqKD substantially reduced the ExMem and hallucination rates, but did not resolve the issue altogether. We conducted limited experimentation in perfecting Adaptive-SeqKD and recognize this could be further expanded upon in the future. 
\end{itemize}