\pdfoutput=1
\documentclass[11pt]{article}
\usepackage{times}
\usepackage{soul}
\usepackage{latexsym}
\usepackage{EMNLP2023}
\usepackage{times}
\usepackage{latexsym}
\usepackage[algoruled,ruled,vlined,noend]{algorithm2e}
\usepackage{amsmath}
\usepackage{times}
\usepackage{latexsym}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{diagbox}
\usepackage{tkz-tab}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{latexsym}
\usepackage{linguex}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{blindtext}
\usepackage{url}
\usepackage{color}
\usepackage{url}
\usepackage{hyperref}    
\usepackage{cleveref}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\usepackage{ntheorem}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\usepackage{booktabs, tabularx}
\usepackage{tfrupee}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[font=normalsize,skip=2pt]{caption}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{framed}
\usepackage{MnSymbol,wasysym}
\usepackage{epigraph}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage[export]{adjustbox}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage[most]{tcolorbox}
\tcbset{on line, 
        boxsep=-0.5pt, arc=1mm, left=0pt,right=0pt,top=0pt,bottom=0pt,
        colframe=white
        }

\title{Memorization Inheritance in Sequence-Level Knowledge\\
Distillation for Neural Machine Translation}

\author{Verna Dankers$^*$ \\
  University of Edinburgh \\
  \texttt{vernadankers@gmail.com} \\\And
  Vikas Raunak \\
  Microsoft \\
  \texttt{viraunak@microsoft.com} \\}

\begin{document}
\newcommand{\appendixshortcut}{Appendix}
\newcommand{\figureshortcut}{Figure}

\maketitle
\begin{abstract}
In this work, we explore how instance-level memorization in 
the \textit{teacher} Neural Machine Translation (NMT) model gets inherited by the \textit{student} model in sequence-level knowledge distillation (SeqKD). We find that despite not directly seeing the original training data, students memorize more than baseline models (models of the same size, trained on the original data)---3.4\% for exact matches and 57\% for extractive memorization---and show increased hallucination rates. Further, under this SeqKD setting, we also characterize how students behave on specific training data subgroups, such as subgroups with low quality or specific counterfactual memorization (CM) scores, and find that students exhibit amplified denoising on low-quality subgroups. Finally, we propose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD to reduce memorization and hallucinations.
Overall, we recommend caution when applying SeqKD: students inherit both their teachers' superior performance \textit{and} their fault modes, thereby requiring active monitoring.
\end{abstract}

\section{Introduction}
\begingroup\def\thefootnote{*}\footnotetext{Work conducted during an internship at Microsoft.}\endgroup
\input{introduction}
\input{memorization_inheritance}
\input{subgroup_analysis}
\input{memorization_mitigation}
\input{conclusion}
\input{limitations}
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}
\clearpage
\appendix
\input{appendix}
\end{document}
