% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{soul}
\usepackage{latexsym}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[algoruled,ruled,vlined,noend]{algorithm2e}
% \usepackage{algorithmicx}
% \usepackage{algorithm} % http://ctan.org/pkg/algorithms
% \usepackage{algpseudocode} % http://ctan.org/pkg/algorithmicx
\usepackage{amsmath}
\usepackage{times}
\usepackage{latexsym}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
% \usepackage{xcolor,colortbl}  
%\usepackage[usenames,dvipsnames]{color}
\usepackage{tikz}
\usepackage{diagbox}
\usepackage{tkz-tab}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{latexsym}
\usepackage{todonotes}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{blindtext}
\usepackage{url}
\usepackage{color}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{url}
\usepackage{hyperref}    
\usepackage{cleveref}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\usepackage{ntheorem}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\usepackage{booktabs, tabularx}
\usepackage{tfrupee}  
\usepackage{caption}

\newcommand{\verna}[1]{\textcolor{purple}{#1}}

\usepackage{epigraph}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{On the Inheritance of Memorization in Sequence-Level Knowledge Distillation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}

\section{Introduction}

Memorization of noisy and atypical training data creates unsystematic failure modes for neural sequence models, presenting a reliability and safety risk during their deployment in the real-world. At the same time, real-world deployments of such neural models are typically derived using knowledge distillation, wherein outputs from a larger teacher model are used to train a smaller student neural model. This is done to arrive at a better trade-off between model quality and cost, wherein the student model can often achieve teacher-like performance while being smaller and faster. The empirical efficacy of such sequence-level knowledge distillation is attributed to the `distribution simplification' hypothesis, that the student model only needs to learn the relevant parts of the teacher's distribution, rather than wasting parameters on trying the to model the entire output space \cite{kim-rush-2016-sequence}.

However, despite the widespread adoption of knowledge distillation for building sequence to sequence models such as for neural machine translation (NMT), the link between the student's memorization behavior and sequence-level knowledge distillation is unknown. There are quite a few plausible hypotheses, e.g., given the tight coupling between the teacher and student models, it is quite conceivable that the student retains the memorization behaviors of the teacher. On the other hand, the fact that knowledge distillation regularizes over the noise and atypicality observed in the natural training data distribution might create divergences in the memorization behavior between the teacher and the student models \cite{saglietti2022solvable}. Characterizing this interaction between memorization and distillation along this continuum is an important open problem in the literature since distillation is widely used to build cheaper and faster systems that are used in real-world deployments, wherein mitigating deleterious memorizations is of critical importance.

In this work, we attempt to characterize how memorization is transmitted from the teacher to the student in sequence-level knowledge distillation, with the applied goal of leveraging such a characterization into alleviating the reliability of the student models. As such, we study the following sets of research questions (RQ) in the traditional data-based sequence-level knowledge distillation setting: (RQ1) How can we statistically characterize the memorizations in the distilled student as a function of memorization behavior in the teacher? (RQ2) Does seq-level KD confer new failure memorization modes to the student? Qualitatively, do new `modes' of memorizations appear in the student that are different from the teacher or vice-versa? Does evaluating the teacher give a good estimate of memorization-based model failures that the student will display? (RQ3) Are any pre or post knowledge distillation mitigations possible to address the deleterious memorizations in the student models? Our contributions are as follows:
\begin{enumerate}
    \item We present a comprehensive characterization of \textit{memorization inheritance} in the context of data-based sequence-level knowledge distillation for Neural Machine Translation. 
    \item (\textit{Placeholder content}) We present MemFreeSeqKD, an effective teacher finetuning algorithm which could be directly plugged in the sequence-level knowledge distillation pipeline to mitigate memorizations in the student model, greatly improving student reliability while preserving its average-case performance.
\end{enumerate}

(\textit{Placeholder content}) Our experiments are conducted on 10 language pairs from different WMT datasets (WMT20, WMT22, WMT23), across a range of transformer scales. The code to reproduce our experiments is available at \hyperlink{code_for_release}{code-for-release}.

\section{Related Work}

\paragraph{Memorization in Sequence to Sequence Models} 

\paragraph{Regularization Effects of Knowledge Distillation} Most of the work in exploring the regularization effects of knowledge distillation has been done in the context of image classification models. In the context of sequence to sequence models, prior work has highlighted results on the amplification of hallucinations in a low-resource setting, however deeper characterizations on natural data remains to be done.

\section{Memorization Inheritance}
\label{sec:bibtex}

\begin{itemize}
    \item We can view knowledge distillation as a transformation applied on the full training set: the training set passes from its natural state to KD data (generated by the teacher) to the student (characterized by the student's outputs on the same data). This setting has 4 natural points of interventions: directly in the training data, in the KD data or on the teacher and the student models that we might leverage for the next section.
    \item Our goal is to characterize this transformation specifically in terms of instance-level memorization behavior of the student (we want to make claims on how the student will inherit memorization through the distillation procedure)
    \item Tracking disagreements b/w teacher, student could be useful, analyzing inputs in the neighborhood of such disagreements will also be useful 
    \item We can track what happens to input-output pairs that are extractively memorized by  the teacher, we can track what happens to counterfactually memorized instances as well (either directly or through a stability based proxy of counterfactual memorization metric)
    \item We would want to answer some questions using the collected measurements:
    \item How are the errors made by the teacher manifested in the student model?
    \item Does the student display new modes of memorization?
    \item How does memorization in the student differ across data categories: low-quality samples, outright noisy samples and rare samples?
    \item Are such memorizations calibrated in the sequence generation process?
    \item Can we characterize memorizations in the student as a function of difference in the number of teacher and student parameters? 
\end{itemize}

\textbf{Memorization Measurements:} Extractive + (approximate or exact) Counterfactual Memorization metrics + Known patterns of hallucinations (Oscillatory + Detached). Track them from the initial dataset to the student model. 

\section{Memorization Mitigation}
\label{sec:bibtex}

\begin{itemize}
    \item Are any pre or post KD mitigations possible to address the memorizations in the student? The primary baselines would be: no-mitigation and student-mitigation.
    \item Explore learning-based mitigations along the following axes: finetuning the teacher, finetuning the student. This is different from inference-time mitigations typically studied before \cite{ippolito-etal-2023-preventing}.
    \item Note that teacher and student are quite asymmetric in terms of their characteristics: teacher is bigger, quicker to learn, while student is more compact, in general harder to make it cram -- so the pre and post KD interventions definitely \textit{should} differ -- we should characterize this thororoughly -- early (pre-KD) interventions properties different to late (post-KD) interventions.
\end{itemize}

\textbf{Measurements}: Same memorization measurements applied after using the interventions, plus general quality measurements.

\clearpage

\input{memorization_inheritance.tex}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\input{appendix}

\end{document}
