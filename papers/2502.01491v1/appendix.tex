\onecolumn
\appendix

\section{Data and experimental setup}
\label{ap:data}
\begin{minipage}{0.52\textwidth}
\paragraph{WMT data}
We download the parallel corpora from the \href{
https://www.statmt.org/wmt20/translation-task.html}{WMT20 website}, using sources listed in Table~\ref{tab:wmt}. For \textsc{En}-\textsc{De} we use the validation/test data from \citet{raunak2022finding}, which is a combination of WMT test data from recent years. For the other language pairs, we use the WMT20 test data, along with the WMT19 test data for validation during training. We honor the licensing terms by using the WMT data for research purposes only and citing the shared task article.
\end{minipage}
\begin{minipage}{0.47\textwidth}
    \small
    \centering
    \begin{tabular}{lrrr}
    \toprule
    \textbf{Source}      & \textsc{En}-\textsc{De} & \textsc{Fr}-\textsc{De} & \textsc{Pl}-\textsc{En}   \\\midrule\midrule
    Europarl    & 1.8M          & 1.8M  & 632k   \\
    ParaCrawl   & 34.4M         & 7.2M  & 6.6M   \\
    Common Crawl& 2.4M          & 622k  & -   \\
    News Commentary& 362k       & 284k  & -   \\
    Wiki Titles & 1.4M          & 942k  & 1.0M   \\
    Tilde Rapid corpus & 1.6M   & -     & 278k   \\
    WikiMatrix  & 6.2M          & 3.4M  & 3.1M   \\
    \bottomrule
    \end{tabular}
    \captionof{table}{Composition of the 3 parallel corpora.}
    \label{tab:wmt}
\end{minipage}

\paragraph{Additional data}
We run monolingual model evaluation using data from additional sources:
\begin{enumerate}[noitemsep,topsep=0pt]
    \item 1M \href{https://www.statmt.org/wmt20/translation-task.html}{\textbf{CommonCrawl}} examples, sampled from the first 100M monolingual CommonCrawl datapoints provided by WMT20. To the best of our knowledge, our use is in line with \href{https://commoncrawl.org/terms-of-use}{CC's terms of use}.
    \item Up to 1M \href{https://huggingface.co/datasets/linhd-postdata/pulpo}{\textbf{Pulpo}} examples, from \citet{de2023alberti}'s multilingual Prolific Unannotated Literary Poetry Corpus containing verses and stanzas. Pulpo contains monolingual sequences for all language pairs, apart from monolingual Polish data. We selected the data because it is expected to be out-of-distribution compared to the WMT20 training corpora. Pulpo contains poems that are copyright-free, or distributed under permissive licenses \citep[][p.3]{de2023alberti}.
\end{enumerate}

\paragraph{Training and evaluation} Models are trained using the \href{https://marian-nmt.github.io/}{\texttt{marian}} toolkit (v1.12.16), using the setup of \citet{raunak2022finding}, that mimics the hyperpameters of \citet{vaswani2017attention}. $\theta_T$ trains for 300k steps; $\theta_S$ and $\theta_B$ train for 100k steps. We use 8 \texttt{Tesla V100-SXM2-32GB} GPUs for model training. 
When evaluating translations using Comet, we use models \texttt{wmt20-comet-da}, \texttt{wmt22-comet-da}, \texttt{wmt20-comet-qe-da} and \texttt{wmt22-cometkiwi-da}, using Comet v1.2.0.
Visit our \href{https://github.com/vyraun/memseqkd.git}{git repository} for the training, evaluation and visualization code.

\section{Model quality and memorization}
\label{ap:add_results}
\paragraph{Model quality metrics} Table~\ref{tab:ap:model_quality} provides model quality metrics for all language pairs.
Generally, $\theta_T$>$\theta_S$>$\theta_B$ (except for TER, that should be minimized instead of maximized), and where the results differ we italicized the numbers; this mostly happens for the monolingual CommonCrawl data. Yet, across the board, the pattern is clear for both WMT20 and monolingual data.
The Pulpo scores are noticeably lower for both Comet-QE metrics. This could indicate that the poetry data is harder to translate, but we cannot reliably make a direct comparison across domains. Most importantly, the system ranking holds for this OOD data, too.

\vspace{.3cm}
\noindent\begin{minipage}{\textwidth}
    \begin{center}
    {\centering\small\setlength{\tabcolsep}{2.3pt}
    \begin{tabular}{llccccccccccc}
    \toprule
    \textbf{Lang.} & \textbf{$\theta$} & \multicolumn{7}{c}{\textbf{WMT20}} & \multicolumn{2}{c}{\textbf{CommonCrawl}} & \multicolumn{2}{c}{\textbf{Pulpo}} \\
                     &   & \texttt{BLEU} & \texttt{chrF} & \texttt{TER} & \texttt{Com-20} & \texttt{Com-22} & \texttt{Com-QE-20} & \texttt{Com-QE-22} & \texttt{Com-QE-20} & \texttt{Com-QE-22} & \texttt{Com-QE-20} & \texttt{Com-QE-22} \\\midrule\midrule 
    \multirow{3}{*}{\textsc{En}-\textsc{De}}  & $\theta_T$   & 33.11 & 61.46 & 54.04 & \textit{41.77} & 81.73 & 31.52 & \textit{80.37} & 35.39          & \textit{73.11} & 11.56 & 65.83 \\
                                              & $\theta_{S}$ & 32.49 & 61.01 & 54.31 & \textit{42.00} & 81.66 & 31.11 & \textit{80.85} & 34.93          & \textit{74.00} & 10.86 & 65.37 \\
                                              & $\theta_{B}$ & 30.15 & 59.41 & 56.44 & \textit{34.35} & 79.42 & 28.08 & \textit{78.99} & 34.77          & \textit{72.97} & 10.58 & 63.82 \\\midrule
    \multirow{3}{*}{\textsc{De}-\textsc{En}}  & $\theta_T$   & 37.49 & 64.95 & 48.70 & 68.97          & 87.42 & 49.40 & 84.28          & 38.19          & 77.01          & 9.46  & 64.72 \\
                                              & $\theta_{S}$ & 35.36 & 63.65 & 50.65 & 65.87          & 86.46 & 46.27 & 83.55          & 37.02          & 76.77          & 8.37  & 63.29 \\
                                              & $\theta_{B}$ & 34.18 & 63.11 & 52.20 & 64.68          & 86.11 & 45.39 & 83.32          & 37.02          & 76.55          & 8.36  & 62.54 \\ \midrule
    \multirow{3}{*}{\textsc{Pl}-\textsc{En}}  & $\theta_T$   & 31.98 & 59.14 & 53.66 & 54.20          & 82.59 & 34.70 & 77.82          & \textit{34.35} & \textit{74.58} & n/a   & n/a \\
                                              & $\theta_{S}$ & 31.41 & 58.95 & 53.99 & 51.83          & 82.08 & 32.54 & 77.21          & \textit{34.08} & \textit{74.72} & n/a   & n/a \\
                                              & $\theta_{B}$ & 30.69 & 58.58 & 54.44 & 50.55          & 81.69 & 32.23 & 76.88          & \textit{34.33} & \textit{74.44} & n/a   & n/a \\ \midrule
    \multirow{3}{*}{\textsc{En}-\textsc{Pl}}  & $\theta_T$   & 31.04 & 59.86 & 56.91 & 92.07          & 90.59 & 74.56 & 83.93          & \textit{33.71} & \textit{70.20} & 12.24 & 61.58 \\
                                              & $\theta_{S}$ & 30.13 & 58.98 & 57.91 & 89.09          & 89.77 & 71.99 & 83.53          & \textit{34.18} & \textit{71.08} & 11.46 & 61.16 \\
                                              & $\theta_{B}$ & 28.94 & 58.27 & 59.21 & 86.52          & 89.17 & 70.41 & 83.01          & \textit{34.18} & \textit{70.09} & 10.94 & 59.55 \\ \midrule
    \multirow{3}{*}{\textsc{Fr}-\textsc{De}}  & $\theta_T$   & 28.86 & 60.70 & 60.94 & 60.16          & 86.67 & 48.24 & 84.23          & 33.86          & 72.24          & 9.89  & 58.44 \\
                                              & $\theta_{S}$ & 27.77 & 60.22 & 62.31 & 57.08          & 85.71 & 46.60 & 83.66          & 33.21          & 72.04          & 9.42  & 58.38 \\
                                              & $\theta_{B}$ & 26.99 & 59.69 & 63.30 & 55.95          & 85.34 & 46.06 & 83.47          & 32.90          & 71.58          & 9.03  & 57.30 \\
    \bottomrule
    \end{tabular}}
    \end{center}
    \captionof{table}{Model performance for the five language pairs, using the experimental setup described in \S\ref{sec:experimental_setup}.
    }
    \label{tab:ap:model_quality}
\end{minipage}

\clearpage
\paragraph{Memorization-related metrics} Table~\ref{tab:ap:memorization} provides memorization metrics for all language pairs.
For replication, ExMem and NatHal rates, it holds that $\theta_T$>$\theta_S$>$\theta_B$, whereas for oscillatory hallucinations, $\theta_S$ typically hallucinates most, although there are exceptions (highlighted in italics).

\vspace{.1cm}
\noindent\begin{minipage}{\textwidth}
    \begin{center}
    {\centering\small\setlength{\tabcolsep}{4.5pt}
    \begin{tabular}{llcccccccc}
    \toprule
    \textbf{Lang.} & \textbf{$\theta$} & \multicolumn{6}{c}{\textbf{WMT20}} & \textbf{CommonCrawl} & \textbf{Pulpo} \\
    && \texttt{Replic.} $\mathcal{T}_C$ & \texttt{Replic.} $\mathcal{T}_T$ & \texttt{ExMem} $\mathcal{T}_C$ (\#) & \texttt{ExMem} $\mathcal{T}_T$ (\#) & \texttt{NatHal} & \texttt{OscHal} 
 & \texttt{OscHal} & \texttt{OscHal}\\ \midrule\midrule 
    \multirow{3}{*}{\textsc{En}-\textsc{De}}  & $\theta_T$          & 12.75 &  n/a    &  0.875 (36k) &  n/a         & 0.699 & 0.012         & 0.020          & \textit{0.008}  \\
                                              & $\theta_{S}$        & 11.12 &  32.75  &  0.627 (22k) &  0.397 (49k) & 0.648 & 0.021         & 0.029          & \textit{0.005}  \\
                                              & $\theta_{B}$        & 10.65 &  n/a    &  0.356 (12k) &  n/a         & 0.559 & 0.014         & 0.021          & \textit{0.009}  \\\midrule
    \multirow{3}{*}{\textsc{De}-\textsc{En}}  & $\theta_T$          & 13.53 &  n/a    &  0.590 (27k) &  n/a         & 0.614 & 0.020         & 0.024          & \textit{0.062}  \\
                                              & $\theta_{S}$        & 11.80 &  35.17  &  0.446 (17k) &  0.246 (34k) & 0.583 & 0.032         & 0.037          & \textit{0.048}  \\
                                              & $\theta_{B}$        & 11.29 &  n/a    &  0.320 (12k) &  n/a         & 0.487 & 0.023         & 0.026          & \textit{0.024}  \\\midrule
    \multirow{3}{*}{\textsc{Pl}-\textsc{En}}  & $\theta_T$          & 20.31 &  n/a    &  3.744 (55k) &  n/a         & 1.203 & \textit{0.021}& \textit{0.106} & n/a    \\
                                              & $\theta_{S}$        & 16.73 &  34.65  &  1.433 (17k) &  3.143 (79k) & 1.192 & \textit{0.044}& \textit{0.135} & n/a    \\
                                              & $\theta_{B}$        & 16.25 &  n/a    &  0.836 (9k)  &  n/a         & 1.107 & \textit{0.048}& \textit{0.136} & n/a    \\\midrule
    \multirow{3}{*}{\textsc{En}-\textsc{Pl}}  & $\theta_T$          & 20.56 &  n/a    &  4.736 (63k) &  n/a         & 1.737 & 0.016         & \textit{0.078} & 0.017  \\
                                              & $\theta_{S}$        & 17.02 &  33.64  &  3.347 (35k) &  4.126 (94k) & 1.737 & 0.037         & \textit{0.100} & 0.029  \\
                                              & $\theta_{B}$        & 16.52 &  n/a    &  2.394 (24k) &  n/a         & 1.610 & 0.033         & \textit{0.126} & 0.027  \\\midrule
    \multirow{3}{*}{\textsc{Fr}-\textsc{De}}  & $\theta_T$          & 24.62 &  n/a    &  0.546 (10k) &  n/a         & 0.799 & 0.018         & 0.025          & 0.190  \\
                                              & $\theta_{S}$        & 21.88 &  40.39  &  0.407 (6k)  &  0.330 (13k) & 0.784 & 0.073         & 0.082          & 0.223  \\
                                              & $\theta_{B}$        & 21.39 &  n/a    &  0.257 (4k)  &  n/a         & 0.666 & 0.045         & 0.046          & 0.081  \\
    \bottomrule
    \end{tabular}}\end{center}
    \captionof{table}{Memorization metrics for the five language pairs, using the experimental setup described in \S\ref{sec:experimental_setup}.}
    \label{tab:ap:memorization}
\end{minipage}
\vspace{.0cm}

\paragraph{Additional information on ExMem and hallucination metrics}
To improve the precision of the ExMem and hallucination metrics, some groups of examples are excluded from the computation:
\begin{itemize}[nosep]
\item \textbf{ExMem}: in some cases, it is justified that the model emits the target after having processed only 75\% of the source, e.g., if the target paraphrases the source. To improve the precision, we thus exclude the following examples when computing ExMem: a) examples with a source shorter than 4 words, b) examples with incorrect source or target languages, c) examples for which the length ratios between source and target are over 1.3, d) examples for which the source equals the target.
\item \textbf{OscHal}: examples with source sequences of at least 50 white-space tokenized tokens are excluded, because it becomes more likely that the repeated bigrams might be accurate for longer sequences. This only concerns a small portion of the training data, e.g., only 3.4\% for \textsc{En-De}. We count a sequence as a hallucination if the most frequent bigram appears more than 10 times in the translation, and at least 4 times more often than in the source. We experimentally verified that when reducing that maximum count of 10, the OscHal rate increases, but the model ranking remains the same.
\item \textbf{NatHal}: for the natural hallucinations, we exclude examples for which the Comet-QE-22 score for the source and the generated translation is above 0.85. This is to exclude cases where natural hallucinations are detected simply because there are source sequences that are each other's paraphrase, for instance if both ``Thank you for your visit at our website.'' and ``Thanks for visiting our website.'' map to ``Vielen Dank für Ihren Besuch auf unserer Website.''.
\end{itemize}

\vspace{-0.2cm}

\section{Varying SeqKD hyperparameters}
\label{ap:hyperparams}

Figure~\ref{fig:beam_size} demonstrates how model quality and memorization metrics change when we increase the beam size used to decode $\mathcal{T}_T$ (for both \textsc{Fr}-\textsc{De} and \textsc{En}-\textsc{De}), or change the student's model size (for \textsc{En}-\textsc{De}). $\theta_{S_{L}}$ and $\theta_{S_{S}}$ have hidden dimensionalities of 1024 and 256, respectively (with corresponding feedforward sizes of 4096 and 1024).
When increasing the beam size: i) the quality of the students' translations decrease (for \textsc{En}-\textsc{De}) or remain mostly stable (for \textsc{Fr}-\textsc{De}), ii) the replication rates slightly increase, and iii) ExMem decreases, although not consistently: for \textsc{Fr}-\textsc{De}, the students' ExMem rates all exceed the baseline, whereas for \textsc{De}-\textsc{En} and $k\in\{2,5\}$ the students are slightly below $\theta_B$. This is still concerning, considering that students were exposed to 18.4\% of the original corpus whereas the baseline has seen 100\%, so even similar ExMem rates between $\theta_S$ and $\theta_B$ suggests KD facilitates memorization.
The NatHal rate slightly reduces but still far exceeds the baseline. The only real improvement larger beam sizes appear to make are reducing OscHal, both on WMT-20 data (OscHal), and on external data
\noindent\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=.9\textwidth]{appendix_figures/quality.pdf}
    \includegraphics[width=.9\textwidth]{appendix_figures/memorization.pdf}
    \captionof{figure}{Illustration of how model quality and memorization metrics change as a result of changing the SeqKD beam size for \textsc{En}-\textsc{De}.}
    \label{fig:beam_size}
\vspace{0.2cm}
\end{minipage}
(OscHal CommonCrawl). With slight performance reduction in \textsc{En-De} and no performance reduction in \textsc{Fr-De} this yields a simple KD lesson: even if beam search is more computationally expensive (particularly when applied to the \textit{entire} training corpus): do not use greedy search in KD.

Reducing the student's model size reduces the model's translation quality, and reduces replication, ExMem and NatHal, but still yields similar OscHal rates. The larger model produces better translations than its teacher and has lower ExMem, but also increased hallucinations.

\section{Adaptive-SeqKD}
\label{ap:Adaptive-SeqKD}

Figure~\ref{fig:adaptivekd_perlanguage} displays how performance changes per language pair due to Adaptive-SeqKD. Changes to quality metrics like BLEU and Comet(-QE)-22 are relatively minor, but the ExMem and hallucination rates are strongly affected. Finetuning with the high-quality data (\textit{hq} in the graphs) decreases ExMem (for four out of five language pairs) and hallucinations (for all language pairs); finetuning with random data (\textit{ra} in the graphs) is somewhat effective in reducing ExMem, but mostly fails to effectively reduce the hallucination rates.

In the previous section, we observed that increasing the SeqKD beam size is beneficial for the reduction of the hallucination rate. We thus reran finetuning with high-quality data for $S_{k=5}$ for \textsc{En}-\textsc{De} to examine whether finetuning also helps for an increased beam size. Upon doing so, the OscHal rate reduced with 28\% for the WMT20 data and 33\% for the CommonCrawl data, and the NatHat rate reduced with 10\% for the WMT20 data, suggesting the wider applicability of Adaptive-SeqKD.

\begin{center}
\noindent \begin{minipage}{\textwidth}\begin{center}
    \includegraphics[height=3cm]{appendix_figures/bleu_difference_adaptiveseqkd.pdf}
    \includegraphics[height=3cm]{appendix_figures/comet-22_difference_adaptiveseqkd.pdf}
    \includegraphics[height=3cm]{appendix_figures/comet-qe-22_difference_adaptiveseqkd.pdf}
    \includegraphics[height=3cm]{appendix_figures/exmem_tc_difference_adaptiveseqkd.pdf}

    \includegraphics[height=3cm]{appendix_figures/exmem_tt_difference_adaptiveseqkd.pdf}
    \includegraphics[height=3cm]{appendix_figures/nathal_difference_adaptiveseqkd.pdf}
    \includegraphics[height=3cm]{appendix_figures/oschal_difference_adaptiveseqkd.pdf}
    \includegraphics[height=3cm]{appendix_figures/oschal_cc_difference_adaptiveseqkd.pdf}
    \end{center}
    \captionof{figure}{Performance changes observed for the different language pairs when applying Adaptive-SeqKD. Changes are computed as percentages with respect to the original teacher $\theta_T$ and $\theta_S$.}
    \label{fig:adaptivekd_perlanguage}
\end{minipage}
\end{center}


\section{Subgroup analyses}
\label{ap:subgroups}

Before elaborating on the results for the subgroup analysis, we more elaborately explain how we approximated the CM scores.

\subsection{Composing the counterfactual memorization subgroups}
\label{ap:cm}
Assuming input $x$ and target $y$, and a model with the parameters $\theta^{\text{tr}}$ trained on all training data, and $\theta^{\text{tst}}$ trained on all examples except $(x,y)$, CM can be computed as follows \citep{feldman2020does,feldman2020what}:
\begin{equation*}
\vspace{-0.1cm}
    \texttt{CM}(x, y) {=} \underbrace{p_{\theta^{\text{tr}}}(y|x)}_{\texttt{IN}} - \underbrace{p_{\theta^{\text{tst}}}(y|x)}_{\texttt{OUT}}
\end{equation*}
Leaving out individual datapoints, as this equation suggests, is computationally too expensive given the vast sizes of our WMT-20 datasets.
We, therefore, approximate CM scores for all datapoints for \textsc{En}-\textsc{De}, and for 10\% of the datapoints for the remaining language pairs. Following \citet[][p.3]{dankers2023memorisation}, who previously computed CM scores for NMT examples, we replace the probability of the full target with the geometric mean of the target token probabilities.
This is more robust to length differences than the full target probability.

For \textsc{En}-\textsc{De}, we approximate CM by training 10 teacher models on a randomly sampled 50\% of the training corpus, while evaluating it on the remaining 50\%, such that for each datapoint, the `\texttt{IN}' and `\texttt{OUT}' quantities in this equation are both estimated using five models.

For the remaining language pairs, we use the original $\theta_T$ model to estimate the `\texttt{IN}' quantity, and train another model according to the same training procedure on 90\% of the training data, to estimate the `\texttt{OUT}' quantity for 10\% of the remaining training data. This is a rather coarse estimation, but should suffice to determine generic relations between CM and our evaluation metrics of interest.

Using the CM scores, we create six subgroups of interest. The last four were also contained in the main paper, and here we add the first two, that separate examples with a low CM score into two groups:
\begin{itemize}[noitemsep, topsep=0pt]
    \item \texttt{IN} and \texttt{OUT} performances $\leq0.2$, marked `L(ow)-L(ow)' in the figures;
    \item \texttt{IN} and \texttt{OUT} performances $\geq0.8$, marked `H(igh)-H(igh)' in the figures;
    \item $\{[0, 0.2), [0.2, 0.3), [0.3, 0.4), [0.4, 1.0]\}$ 
\end{itemize}

\subsection{Results}

\paragraph{Random subgroup} Our baseline subgroup consists of 50k examples randomly sampled from each of the WMT20 corpora.
Replication metrics (chrF in Figure~\ref{fig:random_subgroup}) follow the overall patterns of $\theta_T$>$\theta_S$>$\theta_B$, consolidating that SeqKD dampens memorization in $\theta_S$ compared to $\theta_T$ but amplifies it compared to $\theta_B$.
The quality and diversity metrics (Comet-QE-22 and MSTTR in Figure~\ref{fig:random_subgroup}) emphasize that, compared to the corpus (column $\mathcal{T}_C$), the models generate higher-quality translations, but with lower textual diversity.

\vspace{0.3cm}

\noindent\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{appendix_figures/random_subgroup.pdf}
    \captionof{figure}{Evaluation metrics applied to the random subgroup, for all five language pairs. The square marker indicates the mean and standard deviation.}
    \label{fig:random_subgroup}
\end{minipage} 
\vspace{0.3cm}

\paragraph{Quality-based subgroups} How does KD affect examples from the WMT20 corpus with a certain quality? To categorize WMT20 examples based on quality, we applied the Comet-QE-22 metric using the corpus's targets as translations (because the reference is now the translation, we apply Comet's `reference-free' method).
We examine five subgroups: $\{[0, 0.2), [0.2, 0.4), [0.4, 0.6), [0.6,0.8), [0.8, 1.0]\}$.

In Figure~\ref{fig:comet_subgroups}, we first notice that for the low-quality subgroups, fewer translations are replicated from $\mathcal{T}_C$ (all models) and from $\mathcal{T}_T$ (for $\theta_S$, see Figure~\ref{fig:replication_cqe}).
Secondly, for groups with a quality below 0.6, both $\theta_S$ and $\theta_B$ generated better translations than $\theta_T$, as per Comet-QE-22 applied to the model-generated translations. 
Textual diversity metrics here mostly follow the same trend as Comet-QE-22, with $\theta_S$ showing the highest diversity.

Figure~\ref{fig:subgroups_rel_diffs} visualizes the relative differences of students/baselines to the teacher more explicitly, as a percentual increase.
For Comet-QE-22, if the student's bar exceeds the baseline's bar, this signifies that the improvement over the teacher is partially attributable to the capacity gap between large and base models, and partially to the SeqKD process.
This improvement over the teacher holds for the lower quality groups (0-0.2, 0.2-0.4, 0.4-0.6), but not for the higher quality groups.
Example~\ref{ex4} from the introduction already illustrated a sample from the lowest quality subgroup: in the corpus, the target was likely misaligned. $\theta_T$'s translation is slightly better but still wrong, and $\theta_S$'s translation is slightly better than $\theta_B$'s.
Compared to $\theta_B$, $\theta_S$ benefits from being presented with the teacher's corpus, which is a \textbf{denoised} version of WMT20, since $\theta_T$ replicates the least for the lowest-quality (noisiest) examples.
$\theta_S$, in turn, replicates less from $\mathcal{T}_T$ for these lowest-quality subgroups, too, which allows for \textbf{amplified denoising} compared to $\theta_T$.

\vspace{0.1cm}
\noindent\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{appendix_figures/comet_subgroups.pdf}
    \captionof{figure}{Evaluation metrics applied to the quality subgroups, aggregated over language pairs with error bars indicating standard deviation over language pairs.}
    \label{fig:comet_subgroups}
\end{minipage}
\vspace{0.15cm}

\paragraph{Counterfactual memorization subgroups}
If we now focus on the high CM subgroups (introduced in the previous subsection) in Figure~\ref{fig:cm_subgroups}, we firstly observe that in terms of memorization metrics, the teacher shows increased replication and chrF. This follows from the definition of CM as the \texttt{IN} metric is expected to correlate highly with replication: the higher the target probability, the more likely it is that the teacher can replicate the target when generating translations. We note, however, that the student and baseline replicate examples with high CM less than examples with lower CM. This is likely due to their lower capacity: by definition, CM highlights examples for which a model assigns a low probability to the target when that example is not in the training set, thus requiring more capacity/parameters from the original $\theta_T$ to learn that target.
The reduced replication also leads to lower Comet-22 scores for these examples, since that metric partially relies on the corpus's target.

When looking at the Comet-QE-22 scores, we observe that the higher CM groups typically also have lower Comet-QE-22 scores, although the quality scores are still above the lowest quality subgroup discussed in the previous paragraph.
The baseline and student models outperform $\theta_T$ here, likely because they struggle to replicate the somewhat noisy targets as well as the teacher did. The student shows some amplified denoising compared to $\theta_B$, but that does apply to CM groups across the board, and does not seem specific to individual CM subgroups.

All three models show more textual diversity for subgroups with higher CM scores.

Finally, if we inspect the two groups with very low and very high \texttt{IN} and \texttt{OUT} scores, the models---as expected based on the CM definition---do not replicate the `low' group, and have very high replication rates for the `high' group. For the remaining metrics, the `low' group underscores the findings we previously reported for the low-quality subgroups: the student model shows amplified denoising, as reflected by the Comet-QE-22 metric, and also shows more textual diversity than the other two models.
Examples that score badly in terms of both \texttt{IN} and \texttt{OUT} performance are typically low-quality, misaligned examples \citep{dankers2023memorisation}; if the teacher does not replicate those low-quality targets, but partially denoises by translating them more accurately than the corpus did, the student can further improve upon that.

\noindent\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{appendix_figures/cm_subgroups.pdf}
    \captionof{figure}{Evaluation metrics applied to the CM subgroups, aggregated over language pairs with error bars indicating standard deviation over language pairs.}
    \label{fig:cm_subgroups}
\end{minipage}
\vspace{0.05cm}

\paragraph{Confidence subgroups}
Finally, we turn to the confidence-based groups, created using the lowest 10k and highest 10k examples based on the mean word-level log-probability of the translations the teacher generated. Figure~\ref{fig:confidence_subgroups} demonstrates that the low confidence examples stand out for all metrics: none of the models replicate their targets, all three models generate translations with a relatively low Comet-QE-22 score (akin to the lowest-quality subgroups discussed above), and all three models show an exceptionally low MSTTR. If we inspect the examples, they contain many misaligned examples from the corpus for which the teacher also does not generate an adequate translation (e.g., Example~\ref{ex6}), and a lot of cases where the teacher hallucinates in its translation (e.g., Example~\ref{ex7}). The student's improvement here thus does not stem from learning from the superior targets $\theta_T$ generated but more likely from deviating from $\theta_T$. Yet, note that even if $\theta_S$ does not necessarily copy these hallucinations from its teacher, we did find that the students inspected in the main paper showed amplified hallucination capabilities, so low-quality teacher targets like this could have other unanticipated effects downstream.

{\small
\setlength{\Extopsep}{0.5em}
\setlength{\Exlabelsep}{0.75em}
\setlength{\Exlabelwidth}{.75em}
\setlength{\SubExleftmargin}{1.2em}

\ex. \label{ex6}
\a.[$s_C$] Evergreen Terrace - Almost Home 22.
\b.[$t_C$] Zum Ernst-Gettke-Haus, Hausnummer 68, siehe unten.
\c.[$t_T$] Die alte Burganlage – heute noch 22 Hektar.
\d.[$t_S$] Die Evergreen Terrace ist fast die Heimat 22.
\e.[$t_B$] In: Evergreen Terrace – Fast Home 22.

\ex. \label{ex7}
\a.[$s_C$] Because the race is restricted to Canadian-bred horses, it is not eligible for grading, despite being one of Canada's most prestigious races Northern Dancer Turf Stakes, (\dots)
\b.[$t_C$] Kammerherr, Land- und Obergerichtsrat Magnus Graf von Moltke, Ständedeputierter der Stadt Schleswig, Präsident der konstituierenden Ständeversammlung des Jahres 1836, (\dots)
\c.[$t_T$] Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, (\dots)
\d.[$t_S$] Depots of Canada'hintergrund für kanadische Pferde, ist es nicht berechtigt für das Einstufung, obwohl es eines der renommiertesten Rennen Northern Dancer Turf Stakes, (\dots)
\e.[$t_B$] Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, Stakes, (\dots)

}

\noindent\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{appendix_figures/confidence_subgroups.pdf}
    \captionof{figure}{Evaluation metrics applied to the confidence subgroups, aggregated over language pairs with error bars indicating standard deviation over language pairs.}
    \label{fig:confidence_subgroups}
\end{minipage}
\vspace{0.1cm}

\noindent\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{appendix_figures/subgroups_comet22-qe_large.pdf}
    \captionof{figure}{Relative differences comparing students and baselines to the teacher models, for the Comet-22-QE metric.}
    \label{fig:subgroups_rel_diffs}
\end{minipage}