Memorization of noisy training data creates unexpected failure modes in \textit{neural machine translation} (NMT) models \citep{raunak2022finding}, thus presenting a reliability risk when deploying them in the real world.
To make NMT models inference-friendly, \citep[e.g.,][]{bapna2022building,costa2022no} they are often trained using \textit{sequence-level knowledge distillation} (SeqKD), a KD \citep{hinton2015distilling} variant in which teachers generate synthetic targets for students \citep{kim-rush-2016-sequence}.
\begin{figure}[!h]
\begin{framed}
{\small
\setlength{\Extopsep}{0.5em}
\setlength{\Exlabelsep}{0.75em}
\setlength{\Exlabelwidth}{.75em}
\setlength{\SubExleftmargin}{1.2em}

\noindent \tcbox[colback=pink]{\frownie{}} Extractive Memorization (ExMem) with respect to the initial parallel corpus increases $57.0\%{\pm}15.4$ in students compared to baselines (i.e.\ models memorized to emit the target even if we omit the italicized text):
\ex. \label{ex1}
\a.[$s_C$] Reprezentacja Trynidadu i Tobago \textit{\textcolor{gray}{[w piłce \vfill nożnej]}}
\c.[$t_T$] Trinidad and Tobago national \textcolor{purple}{football} team
\d.[$t_S$] Trinidad and Tobago national \textcolor{purple}{football} team

\noindent \tcbox[colback=pink]{\frownie{}} Students have $31.0\%{\pm}25.7$ more oscillatory hallucinations (in blue) than baselines:
\ex. \label{ex3}
\a.[$s_C$] 1–5 , Stewards are appointed to publish the revelations (\dots)
\b.[$t_T$] 1–5, Diener werden ernannt, um die Offenbarungen zu veröffentlichen (\dots)
\b.[$t_S$] Die Heiligen sind \textcolor{blue}{in der Regel in der Regel in der Regel} (\dots)

\noindent \tcbox[colback=pink]{\frownie{}} Students show secondary ExMem (ExMem with respect to the teacher-generated corpus):
\ex. \label{ex2}
\a.[$s_C$] Electrical industry in Dominican \textit{\textcolor{gray}{[Republic - AmarillasLatinas.net]}}
\b.[$t_T$] Elektrische Industrie in Dominikanische \textcolor{purple}{Republik}
\c.[$t_S$] Elektrische Industrie in Dominikanische \textcolor{purple}{Republik - AmarillasLatinas.net}

\tcbox[colback=lime]{\smiley{}} For low-quality source-target pairs, we observe amplified denoising in students:
\noindent 
\ex. \label{ex4}
\a.[$s_C$] La fiche du Pikauba par la Fromagerie Hamel.
\b.[$t_C$] Pule » Teuerster Käse der Welt aus Eselsmiclh.
\c.[$t_T$] Die Käserei Hamel in Pikauba. \textcolor{gray}{\small (Comet-QE-22${=}0.47$)}
\d.[$t_S$] Die Geschichte des Pikauba durch die Hamel Käserei. \textcolor{gray}{\small (Comet-QE-22${=}0.62$)}
\e.[$t_B$] Die Pikauba-Fassung wird von der Käserei Hamel betrieben. \textcolor{gray}{\small (Comet-QE-22${=}0.47$)}

\vspace{-0.4cm}
}
\end{framed}
\caption{An illustration of our findings. Sources ($s$) are from the corpus ($C$); translations ($t$) are from teachers, students and baselines ($T$, $S$, $B$).}
\vspace{-0.2cm}
\end{figure}
\label{fig:illustration}
SeqKD yields smaller student models whose performance is competitive with the teacher's performance.
Follow-up work has focused primarily on modifying SeqKD objectives to further improve NMT performance \citep[e.g.,][]{wen2023f,zhang-etal-2023-towards-understanding,wang2023better,wang2024don}.
Apart from being used for model compression, SeqKD has proved very beneficial for low-resource and long-tail data \citep{dabre2020combining,currey2020distilling,gumma2023empirical,zhou2024multi,de2024hybrid} but its applications extend beyond that as well, e.g., to continual learning \citep{chuang2020lifelong, zhao2022life}.

Yet, the \textit{understanding} of SeqKD lags behind its \textit{usage}.
Prior work in this direction primarily studies why SeqKD is successful, attributing it to mode reduction of the training data \citep{zhouunderstanding,song2021data}, or suggesting that SeqKD acts as a regularization technique \citep{gordon2019explaining}. In this work, we better try to understand how model behavior (and not just average-case test performance) gets transmitted from teacher to student.
In particular, we focus on how the student inherits instance-level memorization.
Recent work on image classification suggests that KD inhibits memorization in the student \citep{lukasiklarger}, but also that membership inference attacks on the student are often successful \citep{jagielski2024students}.
However, the connection between memorization and SeqKD in NLP is new territory; characterizing this is imperative to mitigate memorization-related failures in students.

Our main contributions are as follows: (1) We provide a quantification of \textbf{memorization inheritance} in \S\ref{sec:memorization_inheritance}: we identify that even though SeqKD inhibits memorization from teacher to student, the student memorizes more about the initial parallel corpus and hallucinates more than it would have, had it been trained without SeqKD.
(2) We perform \textbf{subgroup analyses} in \S\ref{sec:subgroup_analysis}: for data subsets with specific characteristics, we inspect memorization and translation quality metrics and show that student models exhibit amplified denoising compared to both teacher and baseline models, characterize the relation between SeqKD and counterfactual memorization \citep{feldman2020does}, and demonstrate that students barely memorize examples that the teacher is not confident about.
(3) To reduce memorization and mitigate accentuated hallucinations we propose \textbf{Adaptive-SeqKD} in \S\ref{sec:Adaptive-SeqKD}: a simple intervention in the SeqKD algorithm wherein we adapt the teacher by finetuning it briefly on \textit{intrinsically} obtained high-quality data to reduce memorization and hallucinations in the student.

Figure~\ref{fig:illustration} demonstrates a subset of these findings with examples from our datasets.
