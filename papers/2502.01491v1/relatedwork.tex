\section{Related Work}
\paragraph{Memorization in Sequence to Sequence Models} 

\paragraph{Regularization Effects of Knowledge Distillation} Most of the work in exploring the regularization effects of knowledge distillation has been done in the context of image classification models. In the context of sequence to sequence models, prior work has highlighted results on the amplification of hallucinations in a low-resource setting, however deeper characterizations on natural data remains to be done.