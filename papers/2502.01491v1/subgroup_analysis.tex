\section{Subgroup analysis}
\label{sec:subgroup_analysis}

Instead of quantifying models' overall behavior, we now turn our attention to \textit{data subgroups} to describe SeqKD's impact on samples with specific characteristics.
We compute replication metrics (\textbf{exact match}, \textbf{chrF}), neural quality metrics (\textbf{Comet(-QE)-22}), and textual diversity (\textbf{MSTTR}).
Subgroups contain up to 10k examples, with the exception of the random group.

\begin{figure}[t]\centering
	\begin{minipage}{0.6\columnwidth}\centering
		\includegraphics[height=3cm]{figures_subgroups/comet_replication_corpus.pdf}\includegraphics[height=3cm]{figures_subgroups/comet_replication_teacher.pdf}
	\captionof{figure}{Replication rates \\for quality subgroups.}\label{fig:replication_cqe}
	\end{minipage}
	\begin{minipage}{0.37\columnwidth}\centering
		\includegraphics[height=3cm]{figures_subgroups/cm_chrF.pdf}
	\captionof{figure}{ChrF for CM subgroups.}\label{fig:chrf_cm}
	\end{minipage}
	\vspace{0.2cm}

	\begin{minipage}{\columnwidth}
	\centering
		\includegraphics[width=\columnwidth]{figures_subgroups/subgroups_comet22-qe_small.pdf}
	\captionof{figure}{Comet-QE-22 differences to the teacher per subgroup.}
	\label{fig:subgroups}
	\end{minipage}
\end{figure}

\paragraph{Subgroups} We construct 12 subgroups: Firstly, we randomly sample 50k examples from each of the WMT20 corpora.
Secondly, we construct five \textbf{quality-based subgroups} by bucketing WMT20 examples based on the Comet-QE-22 metric, yielding five subgroups (with ranges ${<}$0.2, 0.2-0.4, 0.4-0.6, ${\geq}$0.8).
Thirdly, we create \textbf{counterfactual memorization (CM) subgroups}. Assuming input $x$ and target $y$, and a model with the parameters $\theta^{\text{tr}}$ trained on all training data, and $\theta^{\text{tst}}$ trained on all examples except $(x,y)$, $\texttt{CM}(x, y) {=} p_{\theta^{\text{tr}}}(y|x) - p_{\theta^{\text{tst}}}(y|x)$ \citep{feldman2020does,feldman2020what}.
We approximate CM scores for all datapoints for \textsc{En}-\textsc{De}, and for 10\% of the datapoints for the remaining language pairs, as detailed in Appendix~\ref{ap:cm}.
We create four subgroups of examples (with ranges ${<}$0.2, 0.2-0.3, 0.3-0.4, ${\geq}$0.4).
Finally, we create two \textbf{confidence-based subgroups}, by taking $\theta_T$'s log-probability averaged over the generated tokens for $\mathcal{T}_T$, selecting the top and bottom 10k examples.

\paragraph{Results}
We refer the reader to \appendixshortcut~\ref{ap:subgroups} for the full results, among which we consider the following patterns to be the most noteworthy:
\begin{enumerate}
\item \textit{The random subgroup reflects memorization results}: exact and non-exact match-based metrics for this group (see chrF in \figureshortcut~\ref{fig:random_subgroup}) follow the overall ranking of $\theta_T$>$\theta_S$>$\theta_B$.
\item \textit{Quality-based subgroups demonstrate (amplified) denoising}: The lower the quality of the subgroup, the less models replicate from the corpus (\figureshortcut~\ref{fig:replication_cqe}). When we look at the Comet-QE-22 quality of the translations the models generate for low-quality subgroups, $\theta_S$ and $\theta_B$ perform better than $\theta_T$, and $\theta_S$ outperforms $\theta_B$ (\figureshortcut~\ref{fig:subgroups}). This effect is thus partially attributable to the capacity gap between large and base models, and partially to KD, and confirms prior work \citep[e.g.,][]{zhouunderstanding} that suggested SeqKD has a denoising function: $\theta_T$ denoised WMT20 for $\theta_S$, and now $\theta_S$ shows \textbf{amplified denoising} on that same data. Example~\ref{ex4} already illustrated this in the introduction.
\item \textit{High counterfactual memorization examples are not replicated}: Examples that have high CM for $\theta_T$, do not stand out in terms of replication rates for students or baselines (\figureshortcut~\ref{fig:chrf_cm}), and show some amplified denoising (see \figureshortcut~\ref{fig:subgroups}, although substantially less than low-quality subgroups). 
\item \textit{Low-confidence examples are not inherited}: Finally, we note that for examples for which $\theta_T$ generated translations with a very low confidence, the student shows very little replication and strong amplified denoising (\figureshortcut~\ref{fig:subgroups}).
\end{enumerate}

