
\vspace{-0cm}
\section{Predicting W2SG Without Labels}\label{sec: experiments}
\vspace{-0cm}

{ Leveraging Thm. \ref{thm: main_theorem}, we derive a representation-based metric that can predict W2SG performance without labels in experiments across various settings. Notably, this metric strongly correlates with W2SG performance even when we finetune entire LLMs—a scenario significantly more complex than what we analyze in theory.}\looseness=-1

\vspace{-0cm}
\subsection{A label-agnostic metric for W2SG }
\begin{table*}[t]
    \caption{An overview of the three setups considered in our experiments. }
    \label{tab: tasks}
    \vspace{-0cm}
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
        EXP ID & Task & Strong model & Weak models & Finetuning \\
        \hline
        \RC{1} & molecular tasks & MolBERT & 150 transformers pretrained on GuacaMol & task head   \\
        \RC{2} & NLP tasks & \texttt{nvidia/NV-Embed-v2} & 22 other embedding models & task head  \\
        \RC{3} & NLP tasks & \texttt{Qwen/Qwen-7B} & 28 smaller LLMs & full model \\
        \hline
    \end{tabular}
\end{table*}

%In the previous sections, we observe that the interplay between the principal representations of the weak and strong models plays a critical role in W2SG. 
We start with upper-bounding the RHS of Thm. \ref{thm: main_theorem}.\looseness=-1
\begin{corollary}[Upper Bound 1]\label{coro: predgap_ub_PP}
Define $C=\frac{1}{\hat{n}}\sum_{i=1}^{\hat{n}}\hat{y}_i^2 $. Following Theorem \ref{thm: main_theorem},  directly applying the submultiplicative property of the norm yields the following upper bound:
 \begin{align}
     \nonumber
     \predgap{} \leq C\opnorm{\mP_\s (\mI -\mP_\w ) }^2  + o(1),
 \end{align}
\end{corollary}

\begin{corollary}[Upper Bound 2]\label{coro: predgap_ub_PPP}
Following Theorem \ref{thm: main_theorem}, we can also obtain an upper bound that involves $\err_\sceiling$ as long as $|\E[y^2]- \frac{1}{\hat{n}}\sum_{i=1}^{\hat{n}}\hat{y}_i^2 |=o(1)$ (see proof in Appendix \ref{apdx: proof_PPP}) :\looseness=-1
$$
\predgap{} \leq \left( \sqrt{C} \opnorm{\mP_\s (\mI - \mP_\w) \mP_\s} + \sqrt{\err_\sceiling} \right)^2 + o(1).
$$
\end{corollary}
In both upper bounds, $C$ represents the variance of the labels on $\hat{\gD}$, which can be treated as a constant given a fixed dataset. Therefore, $\predgap{}$ is governed by the norm $\opnorm{\mP_\s (\mI \!-\!\mP_\w ) }$ or  $\opnorm{\mP_\s (\mI \!-\! \mP_\w) \mP_\s} $. Comparing the two bounds, the one in Corollary \ref{coro: predgap_ub_PPP} is tighter particularly when  $\err_\sceiling$ is small \footnote{One can also observe this in Example \ref{eg: toyeg}, where the equality in Corollary \ref{coro: predgap_ub_PPP} holds, whereas that in Corollary \ref{coro: predgap_ub_PP} does not.} . This follows from $\opnorm{\mP_\s (\mI\! -\! \mP_\w) \mP_\s} \!\leq\! \opnorm{\mP_\s (\mI\! -\! \mP_\w) } $. However, in our experiments, both are similarly indicative of W2SG performance.\looseness=-1

Now that \predgap{} can be bounded in terms of the above label-agnostic metrics, and \predgap{} is indicative of the error $\err_\wtos$ as discussed at the end of Sec. \ref{subsec: pre}, we turn our focus to examining the following relationship in real models\looseness=-1 
$$ \err_\wtos ~~\stackrel{?}{\sim} ~~ \opnorm{ \mP_\s(\mI-\mP_\w) } ~~\text{(or $\opnorm{ \mP_\s(\mI-\mP_\w) \mP_\s }$) } 
$$ to evaluate whether the metrics offer practical insights. Specifically, we consider the three setups summarized in Table \ref{tab: tasks}, with their details discussed in the corresponding subsections. In each setup, we fix the strong model and vary the weak model to obtain different $\err_\wtos$ and $\opnorm{ \mP_\s(\mI-\mP_\w) } $ (or $\opnorm{ \mP_\s(\mI-\mP_\w) \mP_\s }$) pairs and study their relationship. \looseness=-1

\subsection{Empirical measure of $\mP_\w$ and $\mP_\s$}

Before proceeding, let's address an important question: how can we compute $\mP_\w$ and $\mP_\s$ for real models? {In some cases, representations are not fixed during fine-tuning, making $h$ difficult to define. Additionally, determining the principal representation, $\mPi_{\gV}h$, is challenging because the exact $\gV$ depends on the population, which is unknown in practice.}
%In some cases, even $h$—the representation itself—is not well-defined (e.g., when the entire model is finetuned from pretrained weights). 
To tackle this, we design heuristics to approximate $\mP$ as follows\looseness=-1
\begin{align}
\vspace{-0cm}
\label{eq: p_approx}
    %\mP^{\text{approx}} = 
    \frac{1}{\hat{n}}\hat{\mK}( \mPi_{\alpha} h  ) ( \frac{1}{\hat{n}}\hat{\mK}( \mPi_{\alpha} h  ) + \beta_{\text{eff}}\mI )^{-1}
    \vspace{-0cm}
\end{align}
We explain the key components below.

\textbf{$h$: extracting representations.} We consider two ways of defining the representations, depending on the setup. \textbf{(1) Last layer embeddings.} In Exps. \RC{1} and \RC{2}, the definition of representation is self-evident, as finetuning is simply training a task head on the embeddings produced by the base model \footnote{In the analysis, the linear model does not include a bias term, but it does in our experiments. This is addressed by appending a constant $1$ to the representation when computing the metrics.\looseness=-1}.  \textbf{(2) Activation maps.} \footnote{We observed worse results with last-layer embeddings in Exp. \RC{3}, likely due to complex cross-layer dynamics during finetuning.\looseness=-1} In Exp. \RC{3}, we finetune the entire LLM from pretrained weights, {so we don't have fixed representations as in the theoretical setting.} 
%In this context, the definition of ``representation" is not immediately clear. 
To address this, we adopt a simple heuristic:  we treat the layer-wise normalized vectorized activation maps of the pre-trained LLM, which encode information about how inputs are represented within the model, as the representations for computing $h(\vx)$. This heuristic serves primarily as a proof of concept, demonstrating that even straightforward approach like this can yield meaningful results. More principled definitions of representations, e.g., those based on NTK \cite{malladi2023kernel} or representation engineering \cite{zou2023representation}, could be explored in future work. See further discussion in Sec. \ref{apdx: discussion}.\looseness=-1
 

\textbf{$\mPi_{\alpha}$: approximating principal representations.} We consider two versions of $\mPi_{\alpha}$, the operation that extracts the principal part from the representations, based on the intuition that principal representations tend to have larger magnitudes {(e.g., Example \ref{eg: spiked_cov})}. (1) In Exps. \RC{1} and \RC{2}, we apply PCA by projecting the representations onto the eigenvectors of the covariance $\hat{\mSigma}(h)$ with eigenvalues $\geq \alpha \times \text{(the largest eigenvalue)}$. (2) In Exp. \RC{3},  we select the top coordinates with variance exceeding $\alpha \times$ (the largest coordinate-wise variance), a cheaper alternative to PCA for high-dimensional activation maps, as it avoids the expensive eigendecomposition. In both cases $\alpha$ is a hyperparameter. \looseness=-1

\textbf{$\beta_{\text{eff}}$: effective regularization.} In Thm. \ref{thm: main_theorem}, $(\beta+\hat{\gamma})$ is the effective regularization, capturing both the explicit ($\beta$) and implicit ($\hat{\gamma}$) \cite{jacot2020implicit} regularization. In practice, regularization can also stem from factors like early stopping, training algorithms, etc. We summarize these effects using $\beta_{\text{eff}}$ in Eq. \ref{eq: p_approx} and treat $\beta_{\text{eff}}$ as a hyperparameter.\looseness=-1

More details on the hyperparameters are in Appx. \ref{apdx: hyperparam}.\looseness=-1

%We denote these as $\alpha_\w$, $\alpha_\s$, $\beta_{\text{eff, \w}}$, and $\beta_{\text{eff, \s}}$.


\begin{figure}[!t]
\vspace{-0cm}
    \centering
\subfigure[Lipop\label{}]{
\includegraphics[width=.2\columnwidth]{figures/molbert/corr_PP_Lipop_0.1,0.1,0.1,0.1.pdf}
\vspace{-0cm}
}
%\hspace{-.4cm}
\subfigure[FreeSolv\label{}]{
\includegraphics[width=.2\columnwidth]{figures/molbert/corr_PP_FreeSolv_0.1,0.1,0.1,0.1.pdf}
\vspace{-0cm}
}
%\hspace{-.4cm}
\subfigure[ESOL\label{}]{
\includegraphics[width=.2\columnwidth]{figures/molbert/corr_PP_ESOL_0.1,0.1,0.1,0.1.pdf}
\vspace{-0cm}
}
%\vspace{-.5cm}
    \caption{ {Results of Exp. \RC{1}: our metric strongly correlates with $\err_\wtos$ and serves as a more fine-grained indicator than model size.}\looseness=-1 }
    \label{fig: molecular}
    \vspace{-.2cm}
\end{figure}

\begin{figure}[!t]
    \centering
\subfigure[Justice\label{}]{
\includegraphics[width=.2\columnwidth]{figures/embedding_model/PP_corr_ethics_justice_0.05,0.001,0.01,0.0001.pdf}
}
%\hspace{.1cm}
\subfigure[ Commonsense\label{}]{
\includegraphics[width=.2\columnwidth]{figures/embedding_model/PP_corr_ethics_commonsense_0.05,0.001,0.01,0.0001.pdf}
}
%\vspace{-.4cm}
    \caption{{A strong correlation between $\opnorm{ \mP_\s(\mI-\mP_\w) }$ and $\err_\wtos$ is observed in Exp. \RC{2} where we finetune embedding models. }\looseness=-1}
    \label{fig: embedding}
    \vspace{-.3cm}
\end{figure}

\begin{figure}[!t]
    \centering
\subfigure[SciQ\label{}]{
\includegraphics[width=.2\columnwidth]{figures/e2e/PP_corr_sciq_maxdim_100000000_0,0.02,8.0,8.0.pdf}
}
\subfigure[Amazon Polarity\label{}]{
\includegraphics[width=.2\columnwidth]{figures/e2e/PP_corr_amazon_polarity_maxdim_100000000_0,0.05,1.0,8.0.pdf}
}
\subfigure[Cosmos-QA\label{}]{
\includegraphics[width=.2\columnwidth]{figures/e2e/PP_corr_cosmos_qa_maxdim_100000000_0,0.05,0.5,8.0.pdf}
}
%\vspace{-.4cm}
    \caption{{A strong correlation between $\opnorm{ \mP_\s(\mI-\mP_\w) }$ and $\err_\wtos$ is observed in Exp. \RC{3} involving general-purpose LLMs.}}
    \label{fig: end2end}
%\vspace{-.3cm}
\end{figure}

\begin{figure}[!t]
    \centering
\includegraphics[width=.21\textwidth]{figures/e2e/comparison/PP_corr_cosmos_qa_maxdim_8000_0,0.05,0.5,8.0.pdf}
\includegraphics[width=.22\textwidth]{figures/e2e/comparison/PP_size_cosmos_qa_maxdim_8000_0,0.05,0.5,8.0.pdf}
\includegraphics[width=.2\textwidth]{figures/e2e/comparison/PP_effdim_cosmos_qa_maxdim_8000_0,0.05,0.5,8.0.pdf}
%\vspace{-0cm}
    \caption{{In Exp. \RC{3}, for models with activation map dimensions $\!\leq\! 8000$, both the activation map dimension (middle) and the dimension of approximated principal representations (right) correlate poorly with $\err_\wtos$. However, $\opnorm{ \mP_\s(\mI\!-\!\mP_\w) }$ remains strongly correlated with $\err_\wtos$ (left). We only show the results for Cosmos QA and defer those for other datasets to Appx. \ref{apdx: compare_size}.  }\looseness=-1  }
    \label{fig: compare_PP_with_size}
\end{figure}



\subsection{Experimental setups}

\textbf{Exp. \RC{1}: Molecular prediction.} Our first setting follows \cite{charikar2024quantifying}. We use the GuacaMol \cite{brown2019guacamol} dataset for pretraining both the strong and weak models. For finetuning, we consider three regression datasets—ESOL, FreeSolv, and Lipop—from the MoleculeNet \cite{wu2018moleculenet} benchmark, curated by ChemBench \cite{charleshen_2020_4054866}, which involve predicting molecular physical properties.  The strong model is MolBERT \cite{fabian2020molecular},  a BERT \cite{devlin2018bert}  pretrained for 100 epochs on GuacaMol. We use smaller transformers pretrained on GuacaMol as weak models. These weak models have 2 layers and 2 attention heads. We vary the hidden size across ${64, 128, 256}$, and vary the number of pretraining epochs from 1 to 50, resulting in 150 weak models. During finetuning, we extract last-layer embeddings and perform linear regression. {MSE loss is used for both training and measuring $\err_\wtos$ as the task is regression.} Additional details are in Appx.\ref{apdx: training_details}. \looseness=-1

%\textbf{Hyperparameters.} We set $\alpha_\w=0.1$, $\alpha_\s=0.1$, $\beta_{\text{eff, \w}}=0.1$, and $\beta_{\text{eff, \s}}=0.1$.

\textbf{Exp. \RC{2}: NLP tasks with embedding models.}
We use the ``Justice" and ``Commonsense" datasets from ETHICS \cite{hendrycks2020aligning}, which involve binary classification based on basic moral concepts.
%which evaluates language models' knowledge of basic moral concepts. The tasks are binary classification of behavior acceptability. 
%based on human ethical judgments. 
We consider embedding models—pretrained LLMs that convert text inputs into vector-based embeddings, with \texttt{nvidia/NV-Embed-v2} \cite{lee2024nv} (currently ranked first on the MTEB leaderboard \cite{muennighoff2022mteb}) as the strong model, and 22 other models as weak models (details in Appx. \ref{apdx: training_details}). For finetuning, we train a linear classifier on the embeddings {with CE loss. $\err_\wtos$ is measured as classification error.}\looseness=-1
%feed the input into the model using specific templates (Appx. \ref{apdx: training_details}) and train a linear classifier on the embeddings. \looseness=-1

%We use cross-entropy (CE) loss during fine-tuning and classification accuracy as the evaluation metric.

%\todoblue{maybe mention training loss and evaluation metric}

%\textbf{Hyperparameters.} We  
% $\alpha_\s = 0.05$, $\alpha_\w = 0.001$, $\beta_{\text{eff}, \s}=0.01$, $\beta_{\text{eff}, \w} = 0.0001$

\textbf{Exp. \RC{3}: NLP tasks with end-to-end finetuned LLMs.} We replicate a setup from \cite{burns2023weak} on three datasets: (1) SciQ \cite{welbl2017crowdsourcing}, containing crowdsourced science exam questions; 
%on subjects like Physics, Chemistry, and Biology; 
(2) Amazon Polarity \cite{zhang2015character}, consisting of Amazon reviews; and (3) Cosmos QA \cite{huang2019cosmos}, involving commonsense-based reading comprehension. Both data preprocessing and finetuning strictly follow \cite{burns2023weak}. The entire model is finetuned with the unembedding layer replaced with a linear head, using CE loss. We use \texttt{Qwen/Qwen-7B} \cite{bai2023qwen} as the strong model and 28 smaller LLMs as weak models (details in Appx. \ref{apdx: training_details}). $\err_\wtos$ is measured in terms of classification error.\looseness=-1
\vspace{-0cm}

%\textbf{Hyperparameters.}

%\blue{In Exp. \RC{1}, MSE loss is used for both training and measuring $\err_\wtos$ as the task is regression. In Exps. \RC{2} and \RC{3}, CE loss is used, and $\err_\wtos$ is measured in terms of classification error.}\ba{should be earlier}\looseness=-1

\subsection{Results}

\textbf{Strong correlation between $\err_\wtos$ and $\opnorm{ \mP_\s(\mI-\mP_\w) }$ across various settings.}   For each of the weak models, we perform the W2SG procedure to obtain the resulting W2S model. We then measure $\err_\wtos$ and $\opnorm{ \mP_\s(\mI-\mP_\w) }$ and plot the results in Figures \ref{fig: molecular}, \ref{fig: embedding} and \ref{fig: end2end}. Across all the setups, we observe a strong correlation between the two quantities, with high Spearman's correlation values displayed at the top of the figures. The results are highly similar for $\opnorm{ \mP_\s(\mI-\mP_\w) \mP_\s}$, as shown in Appx. \ref{apdx: PPP}. Therefore, we only focus on discussing $\opnorm{ \mP_\s(\mI-\mP_\w) }$ in the main paper. {Notably, the correlation between $\err_\wtos$ and $\opnorm{ \mP_\s(\mI-\mP_\w) }$  extends beyond the theoretical setting, covering the following variations: \emph{(1) Loss function and evaluation metric.} While Thm. \ref{thm: main_theorem} is based on linear regression with MSE loss, Exps. \RC{2} and \RC{3} demonstrate that the correlation also holds for classification tasks using CE finetuning loss, with $\err_\wtos$ measured as classification error. \emph{(2) The form of finetuning.} Thm. \ref{thm: main_theorem} assumes that finetuning involves training a function on fixed representations. However, in Exp. \RC{3}, the entire LLM is finetuned. Despite the complex training dynamics in this scenario, a strong correlation between $\err_\wtos$ and $\opnorm{\mP_\s(\mI-\mP_\w)}$ is still observed when activation maps are heuristically used as representations. These results underscore the broad applicability of our conclusion.} 
\looseness=-1

\textbf{Capturing W2SG beyond model size.} {Smaller weak models can sometimes achieve better $\err_\wtos$ than larger ones. For example, in Exp. \RC{1}, the leftmost yellow point (size 64) outperforms the rightmost teal point (size 128) in Fig. \ref{fig: molecular}, likely because these smaller models were pretrained for more epochs 
%pretrained for more epochs than the larger ones 
(recall that we have 150 models span different combinations of sizes and pretraining epochs), resulting in better representations. 
%\blue{A similar observation is made} in Exp. \RC{3} where the middle column of Fig. 
Similarly, in Exp. \RC{3}, the middle column of Fig. \ref{fig: compare_PP_with_size} shows a poor correlation between $\err_\wtos$ and size for models with dimension $\leq 8000$.  Testing another dimension-based metric—the dimension of approximated principal representations—also reveals weak correlation with $\err_\wtos$ (last column of Fig. \ref{fig: compare_PP_with_size}). This underscore the complexity of predicting W2SG performance, as larger models or higher representation dimensions do not guarantee better results. Factors such as the pretraining recipe, the quality and relevance of the pretraining data, etc., all contribute to the final outcome. However, even in these cases, $\opnorm{ \mP_\s(\mI-\mP_\w) }$ consistently captures the trend in $\err_\wtos$ (Fig. \ref{fig: molecular} and the first column of Fig. \ref{fig: compare_PP_with_size}), demonstrating its robustness as a metric that surpasses simple dimensional measures and provides meaningful insights for W2SG.}\looseness=-1
\vspace{-0cm}

% \textbf{Conclusions generalize to various settings.}
% %\ba{better to use "conclusions generalize..."} 
% \looseness=-1

\section{Discussion}\label{apdx: discussion}

Using activation maps as representations in Exp. \RC{3} is a simple heuristic that yields promising results. However, more principled methods for defining and extracting representations from LLMs, such as those through NTK \cite{malladi2023kernel} or representation engineering \cite{zou2023representation}, could be explored. Future research could leverage these approaches to improve results and uncover new applications. For instance, \cite{zou2023representation} introduces a method for extracting specific concept directions in representations, such as honesty and power-seeking. This could enable computing our metric based on topic-specific representations, allowing predictions of W2SG for general tasks within specific topical domains.
 

\section{Conclusion}

In this work, we show that W2SG can be characterized using kernels derived from the principal components of weak and strong models' representations. The theory is applicable to a wide range of representation distributions, provides insights into how models' internal structures influence error correction and the conditions for benign overfitting. Additionally, it offers a label-free metric for predicting W2SG performance, validated through experiments on diverse datasets and LLMs.

We see positive societal impacts in our work as it advances the understanding of Weak-to-Strong Generalization, a crucial problem for aligning superhuman AI in the future. Our results could enhance transparency in AI systems' behavior through analysis of their internal structures and contribute to the broader goal of improving AI safety and reliability.

\section*{Acknowledgement}
This research was partially supported by the National Science Foundation CAREER Award 2146492 and an OpenAI SuperAlignment Grant.


% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.



% \todoblue{check the literature of CCA, CKA (e.g., \cite{kornblith2019similarity}) and see if there is any relevance/difference }



% ``emotion" tokens as representation in jailbreak?

