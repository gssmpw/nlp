
\section{Main Analysis}

In this section, we provide a thorough analysis of the errors associated with the weak model, the W2S model, and the strong ceiling model. Some of these results are used to prove our main conclusion, Theorem \ref{thm: main_theorem}, while others are applied in subsequent analyses.

\subsection{Notations and additional notes}

\textbf{Symbol definitions.} 
We introduce the following notations. The symbol $\vr$ represents a representation, i.e., $\vr = h(\vx)$. For the samples in the splits $\tilde{\gD}$ and $\hat{\gD}$, we denote their representations as $\tilde{\vr}_1, \dots, \tilde{\vr}_{\tilde{n}}$ and $\hat{\vr}_1, \dots, \hat{\vr}_{\hat{n}}$, respectively. We define the sample representation matrices, where each column corresponds to a representation:
\begin{align}
    \nonumber
    \tilde{\mR} \coloneqq [ \tilde{\vr}_1 ~ \tilde{\vr}_2 ~\dots \tilde{\vr}_{\tilde{n}}  ] ~~~~\text{and}~~~~ \hat{\mR} \coloneqq [ \hat{\vr}_1 ~ \hat{\vr}_2 ~\dots \hat{\vr}_{\hat{n}}  ].
\end{align}
We also define $\vy$ which collects the labels of the samples:
\begin{align}
    \nonumber
    \tilde{\vy} =
\begin{bmatrix}
\tilde{y}_1 \\
\tilde{y}_2 \\
\vdots \\
\tilde{y}_{\tilde{n}}
\end{bmatrix}
 ~~~~\text{and}~~~~ 
 \hat{\vy} =
\begin{bmatrix}
\hat{y}_1 \\
\hat{y}_2 \\
\vdots \\
\hat{y}_{\hat{n}}
\end{bmatrix}.
\end{align}
For the covariance matrices, we use the following shorthand notations to avoid clutter:
\begin{align}
    \nonumber
    &\mSigma = \mSigma(h),~ \hat{\mSigma} = \hat{\mSigma}(h),~ \tilde{\mSigma} = \tilde{\mSigma}(h),\\
    \nonumber
    &\mSigma' = \mSigma(\mPi_{\gV}h),~ \hat{\mSigma}' = \hat{\mSigma}(\mPi_{\gV}h),    ~\tilde{\mSigma}'' = \tilde{\mSigma}(\mPi_{\gV}h),~\mSigma'' = \mSigma(\mPi_{\gV^\perp}h),~ \hat{\mSigma}'' = \hat{\mSigma}(\mPi_{\gV^\perp}h),    ~\tilde{\mSigma}'' = \tilde{\mSigma}(\mPi_{\gV^\perp}h). 
\end{align}

\textbf{Use of subscripts.}
Additionally, we use subscripts `$\w$' and `$\s$' to indicate the model associated with a given quantity. For example, $\tilde{\mR}_\w$ and $\hat{\mR}_\w$ denote the sample representation matrices generated by the weak model, while $\tilde{\mR}_\s$ and $\hat{\mR}_\s$ denote those generated by the strong model. Similarly, this convention applies to covariance matrices; for instance, $\hat{\mSigma}_\s' = \hat{\mSigma}(\mPi_{\gV_\s} h_\s)$.

\textbf{Mathematical notations.} For convenience, whenever we say $\mA = \mB + o(1)$, where $\mA$ and $\mB$ are matrices or vectors, we mean that $\opnorm{\mA - \mB} = o(1)$. We let $\lambda_i(\mA)$, $\lambda_{\min}(\mA)$, $\lambda_{\text{min, $\neq 0$}}(\mA)$, and $\lambda_{\max}(\mA)$ represent the $i$-th, smallest, smallest nonzero, and largest eigenvalues of the matrix $\mA$, respectively. The expression $\mA \preccurlyeq \mB $ means that the matrix $\mB-\mA$ is positive semidefinite, and $\mA \succcurlyeq \mB $ means that $\mA-\mB$ is positive semidefinite. 

\textbf{Implied proof techniques.} Sometimes, in the proof, we use the triangle inequality and the sub-multiplicativity of norms without explicitly stating them when they are straightforward, as mentioning them would make the text unnecessarily verbose.



\subsection{Restatement of Definition \ref{def: delta_decomp}}

Here, we restate Definition \ref{def: delta_decomp} with simplified notations for convenience and clarity in the proof.

\begin{definition}[$(\delta, \hat{\gamma}, \tilde{\gamma})$-decomposability (restated) ]\label{def: restate} 
Given $\gD$, $\tilde{\gD}$, $\hat{\gD}$, and a representation function $h$, we say that the representations of $h$ are \emph{$(\delta, \hat{\gamma}, \tilde{\gamma})$-decomposable with respect to a subspace $\gV$} (of the representation space), for some $\delta=O(1)$,  $\hat{\gamma}=O(1)$, and $\tilde{\gamma}=O(1)$, if the following holds. Let $\mU \mLambda \mU^\top$ be the singular value decomposition (SVD) of $\mSigma$. There exists a matrix $\mU'$ consisting of a subset of columns of $\mU$, corresponding to the nonzero eigenvalues, such that the following conditions are satisfied. Let $\mU''$ denote the matrix that collects the remaining columns of $\mU$. Define diagonal matrices $\mLambda'$ and $\mLambda''$ to collect the eigenvalues corresponding to $\mU'$ and $\mU''$, respectively. Additionally, define: $\mSigma' = \mU' \mLambda' \mU'^\top $ and $\mSigma'' = \mU'' \mLambda'' \mU''^\top $. Let $\gamma = \min(\hat{\gamma}, \tilde{\gamma})$, and let $\gV$ be the span of the columns of $\mU'$. Now, leveraging the fact that the projection $\mPi_{\gV}$ can be written as $\mU' \mU'^\top$, and noting that $\lambda_{\text{min, $\neq 0$}}(\mSigma') = \lambda_{\min}(\mLambda')$, we can reformulate the original Definition \ref{def: delta_decomp} in terms of $\mU'$: with high probability $1 - o(1)$,
\begin{itemize}
\item[a.] \bounded. $\opnorm{\mSigma} = O(1)$, $\opnorm{\hat{\mSigma}} = O(1)$ and $\opnorm{\tilde{\mSigma}} = O(1)$. Additionally, $\E[y^2]=O(1)$, $\frac{1}{\hat{n}}\| \hat{\vy} \|^2 = O(1)$ and $\frac{1}{\tilde{n}}\| \tilde{\vy} \|^2 = O(1)$. 
    \item[b.] \conc{}. The original statement is $\opnorm{\hat{\mSigma}' - \mSigma'} = o(1)$ and $\opnorm{\tilde{\mSigma}' - \mSigma'} = o(1)$. However, since:
    \begin{align}
\nonumber
\opnorm{   \mU'^\top\hat{\mSigma} \mU' -  \mLambda'  }
  = & \opnorm{  \frac{1}{\hat{n}} \mU'^\top\hat{\mR} \hat{\mR}^\top \mU' - \mLambda' }\\
  \nonumber
  = &  \opnorm{  \frac{1}{\hat{n}} \mU\mU'^\top\hat{\mR} \hat{\mR}^\top \mU'\mU^\top -  \mU\mU'^\top\mLambda \mU'\mU^\top }\\
  \nonumber
  = & \opnorm{ \hat{\mSigma}'-\mSigma' },
\end{align}
and similarly for $\tilde{\mSigma}'$, we can restate it as:
$ \opnorm{  \mU'^\top\hat{\mSigma}\mU' -\mLambda' } = o(\gamma^2+\delta^2 + \lambda_{\min}( \mLambda' )^2 ) $ and $ \opnorm{ \mU'^\top\tilde{\mSigma}\mU'-\mLambda'  } = o(\gamma^2+\delta^2 + \lambda_{\min}( \mLambda' )^2 ) $. Similarly, by noting that the operator norm is invariant under left multiplication by $\mU'$, we can restate the statement regarding $y$ as: $\|\mU'^\top \frac{1}{\sqrt{\tilde{n}}} \tilde{\mR}\tilde{\vy} -\mU'^\top  \E[ \vr y ] \| = o(\gamma+\delta+\lambda_{\min}( \mLambda' )) $ and $\|\mU'^\top \frac{1}{\sqrt{\hat{n}}} \hat{\mR}\hat{\vy} -\mU'^\top  \E[ \vr y ] \| = o(\gamma+\delta+\lambda_{\min}( \mLambda' )) $.
    \item [c.] \isotropy{}. $ \opnorm{\frac{1}{\hat{n}} \hat{\mR}^\top\mU''\mU''^\top \hat{\mR} -\hat{\gamma} \mI } = o(\gamma^2+\delta^2) $ and $ \opnorm{\frac{1}{\tilde{n}} \tilde{\mR}^\top\mU''\mU''^\top \tilde{\mR} -\tilde{\gamma} \mI } = o(\gamma^2+\delta^2) $.
    \item[d.] \smallin{}. $\opnorm{\frac{1}{\sqrt{\hat{n}}} \hat{\mR}^\top \mU''\mU''^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}} = o(\gamma+\delta) $.
    \item[e.] \dimini{}. $\opnorm{ \mSigma'' }=o(\gamma+\delta)$.  
\end{itemize}
\end{definition}

\textbf{Use of subscripts.}
Since in Assumption \ref{assump: weak_strong_decomp} we assume that the representations of both the weak and strong models satisfy Definition \ref{def: restate}, all the notations in Definition \ref{def: restate} have corresponding versions for the weak model's representations and the strong model's representations. We follow the previously mentioned convention and use the subscripts $\w$' and $\s$' to distinguish between them. For example, notations such as $\mU'_\w$ and $\mU'_\s$, $\mLambda'_\w$ and $\mLambda'_\s$, will be used. The meaning of such notations should be clear from the context in which they appear.

\subsection{Lemmas}

Below, we introduce some basic lemmas and prove properties that will be used in the later analysis.

\begin{lemma}[Push-through identity]\label{lemma: pushthrough} For any matrices $\mA, \mB$, and any scalar $a$, the identity $ (a\mI + \mA\mB)^{-1}\mA = \mA(a\mI+\mB\mA)^{-1} $ holds as long as $(a\mI + \mA\mB)^{-1}$ and $(a\mI+\mB\mA)^{-1}$ are invertible. 
\end{lemma}

\begin{lemma}\label{lemma: perturb_inv}
A classical result on the effect of perturbations on the inverse of a square matrix states that $\opnorm{(\mA + \Delta)^{-1} - \mA^{-1}} \leq \opnorm{\mA^{-1}}^2 \opnorm{\Delta}$, where $\mA$ is an invertible square matrix. This result can be found, for example, in \cite{demmel1992componentwise} or Equation 1.1 of \cite{el2002inversion}.
\end{lemma}

\begin{lemma}\label{lemma: isotropy_eff_reg_kernel}
    If condition \isotropy{} holds, we have that $ \opnorm{ \frac{1}{\tilde{n}}\tilde{\mR}^\top \tilde{\mR} - \left( \frac{1}{\tilde{n}}\tilde{\mR}^\top\mU'\mU'^\top\tilde{\mR} + \tilde{\gamma}\mI  \right) } =o(\gamma^2+\delta^2)$, and a similar conclusion holds for $\hat{\mR}$ as well.
\end{lemma}
\begin{proof}
By \isotropy{},
    \begin{align}
        \nonumber
       & \opnorm{ \frac{1}{\tilde{n}}\tilde{\mR}^\top \tilde{\mR} - \left( \frac{1}{\tilde{n}}\tilde{\mR}^\top\mU'\mU'^\top\tilde{\mR} + \tilde{\gamma}\mI  \right) }\\
        \nonumber
        = & \opnorm{ \frac{1}{\tilde{n}}\tilde{\mR}^\top (\mU'\mU'^\top+\mU''\mU''^\top) \tilde{\mR} - \left( \frac{1}{\tilde{n}}\tilde{\mR}^\top\mU'\mU'^\top\tilde{\mR} + \tilde{\gamma}\mI  \right) }\\
        \nonumber
        = &  \opnorm{ \frac{1}{\tilde{n}}\tilde{\mR}^\top \mU''\mU''^\top \tilde{\mR} - \tilde{\gamma}\mI }\\
        \nonumber
        = & o(\gamma^2+\delta^2). 
    \end{align}
\end{proof}

\begin{lemma}\label{lemma: isotropy_inv}
    If condition \isotropy{} holds, then for any $\beta=O(1)~s.t.~\beta\geq \delta$, we have that $ \opnorm{(\frac{1}{\tilde{n}}\tilde{\mR}^\top \tilde{\mR} + \beta \mI)^{-1}- 
 ( \frac{1}{\tilde{n}}\tilde{\mR}^\top\mU'\mU'^\top\tilde{\mR} + (\tilde{\gamma}+\beta)\mI  )^{-1}} =o(1)$, and a similar conclusion holds for $\hat{\mR}$ as well.
\end{lemma}
\begin{proof}
By \isotropy{},
    \begin{align}
        \nonumber
       & \opnorm{ \frac{1}{\tilde{n}}\tilde{\mR}^\top \tilde{\mR} + \beta \mI - \left( \frac{1}{\tilde{n}}\tilde{\mR}^\top\mU'\mU'^\top\tilde{\mR} + (\tilde{\gamma}+\beta)\mI  \right) }\\
        \nonumber
        = & \opnorm{ \frac{1}{\tilde{n}}\tilde{\mR}^\top (\mU'\mU'^\top+\mU''\mU''^\top) \tilde{\mR} + \beta \mI - \left( \frac{1}{\tilde{n}}\tilde{\mR}^\top\mU'\mU'^\top\tilde{\mR} + (\tilde{\gamma}+\beta)\mI  \right) }\\
        \nonumber
        = &  \opnorm{ \frac{1}{\tilde{n}}\tilde{\mR}^\top \mU''\mU''^\top \tilde{\mR} - \tilde{\gamma}\mI }\\
        \nonumber
        = & o(\gamma^2+\delta^2). 
    \end{align}
Then, by Lemma \ref{lemma: perturb_inv}, we have
\begin{align}
    \nonumber
    \opnorm{ (\frac{1}{\tilde{n}}\tilde{\mR}^\top \tilde{\mR} + \beta \mI)^{-1}- 
 ( \frac{1}{\tilde{n}}\tilde{\mR}^\top\mU'\mU'^\top\tilde{\mR} + (\tilde{\gamma}+\beta)\mI  )^{-1} } \leq & o(\gamma^2 + \delta^2)~~ \opnorm{ (\frac{1}{\tilde{n}}\tilde{\mR}^\top\mU'\mU'^\top\tilde{\mR} + (\tilde{\gamma}+\beta)\mI  )^{-1} }^2 \\
 \nonumber
 = & o( \frac{\gamma^2+\delta^2}{ (\tilde{\gamma}+\beta)^2 } )\\
        %  \nonumber
        % \leq & o( \frac{\gamma^2+\delta^2}{\gamma^2+\beta^2} )\\
        \nonumber
        = &o(1).
\end{align}
\end{proof}

\begin{lemma}\label{lemma: concentration_inv}
If condition \conc{} holds, then for any $\beta=O(1)~s.t.~\beta\geq \delta$, and $\gamma_0\in\{ \hat{\gamma}, \tilde{\gamma} \}$ we have
    \begin{align}
    \nonumber
     \opnorm{   (\mU'^\top\tilde{\mSigma} \mU' +(\gamma_0+\beta)\mI )^{-1}-  (\mLambda' +(\gamma_0+\beta)\mI )^{-1} }=o(1),
    \end{align}
    and a similar conclusion holds for $\hat{\mSigma}$ as well.
\end{lemma}
\begin{proof}
By condition \conc{}, we have
\begin{align}
\nonumber
\opnorm{   \mU'^\top\tilde{\mSigma} \mU' -  \mLambda'  }
  = o(\gamma^2 +\delta^2 + \lambda_{\text{min}} ( \mLambda' )^2 ).
\end{align}
Then, by Lemma \ref{lemma: perturb_inv}, we have
\begin{align}
    \nonumber
    \opnorm{   (\mU'^\top\tilde{\mSigma} \mU' +(\gamma_0+\beta)\mI )^{-1}-  (\mLambda' +(\gamma_0+\beta)\mI )^{-1} } =
    \leq & o(\gamma^2 +\delta^2 + \lambda_{\text{min}} (\mLambda' )^2 )~~ \opnorm{(\mLambda' +(\gamma_0+\beta)\mI )^{-1}}^2 \\
    \nonumber
    =& 
    o( \frac{\gamma^2 +\delta^2 + \lambda_{\text{min}} (\mLambda' )^2 }{ ( \gamma_0 +\beta + \lambda_{\text{min}} (\mLambda' )  )^2 } )\\
    \nonumber
    = & o(1).
\end{align}
\end{proof}
% \begin{corollary}\label{coro: tilde_hat_principal}
% Lemma \ref{lemma: concentration_inv} further implies that 
% \begin{align}
%     \nonumber
%      \opnorm{   (\mU'^\top\tilde{\mSigma} \mU' +(\gamma+\beta)\mI )^{-1}-  (\mU'^\top\hat{\mSigma} \mU' +(\gamma+\beta)\mI )^{-1} }=o(1),
%     \end{align}
% by applying the triangle inequality.
% \end{corollary}

\begin{lemma}\label{lemma: delta_lambda_min}
If conditions \bounded{} and \conc{} hold, then $|\lambda_{\min}(\mLambda')^2 -\lambda_{\min}(\mU'^\top \hat{\mSigma}\mU')^2| = o(\gamma^2+\delta+\lambda_{\min}(\mLambda')^2) $. It still holds if we replace $\hat{}$ with $\tilde{}$.
\end{lemma}
\begin{proof}
Define $t= \lambda_{\min}(\mLambda')-\lambda_{\min}(\mU'^\top\hat{\mSigma}'\mU')$.
 By condition \conc{} and Weyl's theorem, we have $|t| = o( \gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2 )$. Then, we compute:
\begin{align}
    \nonumber
    &\lambda_{\min}(\mU'^\top\hat{\mSigma}'\mU')^2 \\
    \nonumber
    = & \lambda_{\min}(\mLambda')^2 + t^2 - 2t\lambda_{\min}(\mLambda')\\
    % \nonumber
    % = & \lambda_{\min}(\mLambda')^2  \pm t^2 \pm 2t\lambda_{\min}(\mLambda')\\
    \nonumber
    = & \lambda_{\min}(\mLambda')^2 \pm o(\gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2 ),
\end{align}
where the last step follows because $\lambda_{\min}(\mLambda')=O(1)$ (via condition \bounded{}) and $|t|=o( \gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2 )$.
\end{proof}

\begin{corollary}\label{coro: ratio_lambda_min}
Lemma \ref{lemma: delta_lambda_min} further implies that $ \frac{\gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2}{\hat{\gamma}^2+\delta^2+\lambda_{\min}(\mU'^\top \hat{\mSigma}\mU'  )^2}=O(1) $ when conditions \bounded{} and \conc{} hold. It still holds if we replace $\hat{}$ with $\tilde{}$. 
\end{corollary}
\begin{proof}
\begin{align}
    \nonumber
\frac{\gamma^2+\delta^2+\lambda_{\min}(\mU'^\top \hat{\mSigma}\mU'  )^2}{\hat{\gamma}^2+\delta^2+\lambda_{\min}(\mLambda')^2} = & \frac{\gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2}{\hat{\gamma}^2+\delta^2+\lambda_{\min}(\mLambda')^2} - \frac{\lambda_{\min}(\mLambda')^2 - \lambda_{\min}(\mU'^\top \hat{\mSigma}\mU'  )^2 }{\hat{\gamma}^2+\delta^2+\lambda_{\min}(\mLambda')^2} \\
    \nonumber
    \leq & 1\pm \frac{ o(\gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2 ) }{\hat{\gamma}^2+\delta^2+\lambda_{\min}(\mLambda')^2}\\
    \nonumber
    \leq & 1+ o(1).
\end{align}   
Therefore, $ \frac{\gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2}{\hat{\gamma}^2+\delta^2+\lambda_{\min}(\mU'^\top \hat{\mSigma}\mU'  )^2}=O(1) $. 
\end{proof}

\begin{corollary}\label{coro: sqrt_lambda_intermediate}
If conditions \bounded{} and \conc{} hold, then for any $\vq$ with $\|\vq\|=O(1)$, we have $\| \mU' \sqrt{\mLambda'} (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \vq  \|^2 = \| \frac{1}{\sqrt{\hat{n}}}\hat{\mR}^\top \mU' ( \frac{1}{\hat{n}}\mU'^\top\hat{\mR}\hat{\mR}^\top\mU' + (\hat{\gamma}+\beta)\mI )^{-1}\vq\|^2 \pm o(1)  $. It still holds if we replace $\hat{}$ with $\tilde{}$.
\end{corollary}
\begin{proof}
\begin{align}
    \nonumber
    &\| \mU' \sqrt{\mLambda'} (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \vq  \|^2 \\
    \nonumber
    =& \vq^\top (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \mLambda'(\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \vq \\
    \nonumber
    = & \vq^\top (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \mU'^\top\hat{\mSigma}\mU' (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \vq \\
    \nonumber
     & ~\pm   o\left( (\gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2) \opnorm{(\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1}}^2  \right) \quad\quad\text{by \conc{} and $\|\vq\|=O(1)$} \\
     \nonumber
     = & \vq^\top (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \mU'^\top\hat{\mSigma}\mU' (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \vq  \pm o\left( \frac{\gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2}{(\lambda_{\min}(\mU'^\top\mSigma'\mU') +\hat{\gamma}+\beta )^2 } \right)\\
     \nonumber
     = & \vq^\top (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \mU'^\top\hat{\mSigma}\mU' (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \vq  \pm o\left( \frac{\gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2}{ \hat{\gamma}^2+\beta^2+ \lambda_{\min}(\mU'^\top\mSigma'\mU')^2 } \right)\\
     \nonumber
     = & \vq^\top (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \mU'^\top\hat{\mSigma}\mU' (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \vq  \pm o\left( \frac{\gamma^2+\delta^2+\lambda_{\min}(\mLambda')^2}{ \hat{\gamma}^2+\delta^2+ \lambda_{\min}(\mU'^\top\mSigma'\mU')^2 } \right)\\
     \nonumber
     = & \vq^\top (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \mU'^\top\hat{\mSigma}\mU' (\mU'^\top\mSigma'\mU'+(\hat{\gamma}+\beta)\mI )^{-1} \vq  \pm o(1) \quad\quad\text{by Corollary \ref{coro: ratio_lambda_min}} \\
     \nonumber
     = & \| \frac{1}{\sqrt{\hat{n}}}\hat{\mR}^\top \mU' ( \frac{1}{\hat{n}}\mU'^\top\hat{\mR}\hat{\mR}^\top\mU' + (\hat{\gamma}+\beta)\mI )^{-1}\vq  \|^2 \pm o(1)
\end{align}
\end{proof}
\begin{corollary}\label{coro: sqrt_lambda}
If conditions \bounded{} and \conc{} hold, then for any $\vpsi$ with $\|\vpsi\|=O(1)$, we have $\| \mU' \sqrt{\mLambda'}\mU'^\top\frac{1}{\sqrt{\hat{n}}} \hat{\mR}(\frac{1}{\hat{n}}\hat{\mR}^\top\mU'\mU'^\top\hat{\mR} +(\hat{\gamma}+\beta)\mI )^{-1} \vpsi  \|^2 =  \| \frac{1}{\hat{n}}\hat{\mR}^\top \mU'\mU'^\top \hat{\mR} ( \frac{1}{\hat{n}}\hat{\mR}^\top\mU'\mU'^\top\hat{\mR} + (\hat{\gamma}+\beta)\mI )^{-1} \vpsi\|^2  \pm o(1) $, and $\| \mU' \sqrt{\mLambda'}\mU'^\top\frac{1}{\sqrt{\hat{n}}} \hat{\mR}(\frac{1}{\hat{n}}\hat{\mR}^\top\mU'\mU'^\top\hat{\mR} +(\hat{\gamma}+\beta)\mI )^{-1} \vpsi  \| = O(1)$. It still holds if we replace $\hat{}$ with $\tilde{}$.
\end{corollary}
\begin{proof}
First, we have
    \begin{align}
\nonumber
   & \| \mU' \sqrt{\mLambda'}\mU'^\top\frac{1}{\sqrt{\hat{n}}} \hat{\mR}(\frac{1}{\hat{n}}\hat{\mR}^\top\mU'\mU'^\top\hat{\mR} +(\hat{\gamma}+\beta)\mI )^{-1} \vpsi  \|^2 \\
   \nonumber
   = & \| \mU' \sqrt{\mLambda'}(\frac{1}{\hat{n}}\mU'^\top\hat{\mR}\hat{\mR}^\top\mU' +(\hat{\gamma}+\beta)\mI )^{-1}\mU'^\top\frac{1}{\sqrt{\hat{n}}} \hat{\mR} \vpsi  \|^2 \quad\quad\text{by Lemma \ref{lemma: pushthrough}} \\
    \nonumber
    = & \| \frac{1}{\sqrt{\hat{n}}}\hat{\mR}^\top \mU' ( \frac{1}{\hat{n}}\mU'^\top\hat{\mR}\hat{\mR}^\top\mU' + (\hat{\gamma}+\beta)\mI )^{-1}\mU'^\top\frac{1}{\sqrt{\hat{n}}} \hat{\mR} \vpsi\|^2  \pm o(1) \\
    \nonumber
   & \quad\quad\text{by the fact that $\|  \mU'^\top\frac{1}{\sqrt{\hat{n}} }\hat{\mR}\vpsi\|=O(1)$ (via \bounded{}) and invoking Corollary \ref{coro: sqrt_lambda_intermediate} }  \\
    \nonumber
    = & \| \frac{1}{\hat{n}}\hat{\mR}^\top \mU'\mU'^\top \hat{\mR} ( \frac{1}{\hat{n}}\hat{\mR}^\top\mU'\mU'^\top\hat{\mR} + (\hat{\gamma}+\beta)\mI )^{-1} \vpsi\|^2  \pm o(1)  \quad\quad\text{by Lemma \ref{lemma: pushthrough}}.
\end{align}
Additionally, since $\opnorm{ \frac{1}{\hat{n}}\hat{\mR}^\top \mU'\mU'^\top \hat{\mR} ( \frac{1}{\hat{n}}\hat{\mR}^\top\mU'\mU'^\top\hat{\mR} + (\hat{\gamma}+\beta)\mI )^{-1}} = \frac{ \opnorm{\frac{1}{\hat{n}}\hat{\mR}^\top \mU'\mU'^\top \hat{\mR}} }{\opnorm{\frac{1}{\hat{n}}\hat{\mR}^\top \mU'\mU'^\top \hat{\mR}}+\hat{\gamma}+\beta} \leq 1$, we also have the bound $ \| \mU' \sqrt{\mLambda'}\mU'^\top\frac{1}{\sqrt{\hat{n}}} \hat{\mR}(\frac{1}{\hat{n}}\hat{\mR}^\top\mU'\mU'^\top\hat{\mR} +(\hat{\gamma}+\beta)\mI )^{-1} \vpsi  \| = O(1)$.
\end{proof}


\begin{lemma}\label{lemma: scale_isotropy_kernel}
If condition \isotropy{} holds, then $\opnorm{\mU''^\top  \frac{1}{\sqrt{\hat{n}}}\hat{\mR}}\leq \sqrt{o(\gamma^2+\delta^2)+\hat{\gamma}   }$. Similarly, $\opnorm{\mU''^\top  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}}\leq \sqrt{o(\gamma^2+\delta^2)+\tilde{\gamma}   }$.
\end{lemma}
\begin{proof}
By condition \isotropy{} and triangle inequality, we have 
\begin{align}
    \nonumber
    \opnorm{ \frac{1}{\hat{n}} \hat{\mR}^\top \mU'' \mU''^\top  \hat{\mR} } \leq o(\gamma^2+\delta^2)+\hat{\gamma}
\end{align}
Then, 
\begin{align}
    \nonumber
    \opnorm{\mU''^\top  \frac{1}{\sqrt{\hat{n}}}\hat{\mR}} = \sqrt{\opnorm{ \frac{1}{\hat{n}} \hat{\mR}^\top \mU'' \mU''^\top  \hat{\mR} }  } \leq \sqrt{o(\gamma^2+\delta^2)+\hat{\gamma} }.
\end{align}
\end{proof}


%\subsection{Proof of Theorem \ref{thm: main_theorem}}

\subsection{Basic expressions for the model weights and errors}

Let $\vw_\w \in \sR^{d_\w}$, $\vw_\wtos \in \sR^{d_\s}$, and $\vw_\s \in \sR^{d_\s}$ represent the weights of the linear models $f_\w$, $f_\wtos$, and $f_\s$, respectively. Using the well-known closed-form solution for the minimizer of the MSE loss with $\ell_2$ regularization, we derive their formulas:
\begin{align}
    \nonumber
    \vw_\w = & \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \\
    \label{eq: exp_wtos}
    \vw_\wtos = &  \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \frac{1}{\sqrt{\hat{n}}} ( \hat{\mR}_\w^\top \vw_\w)\\
    \nonumber
    \vw_\s = & \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \frac{1}{\sqrt{\hat{n}}} \hat{\vy}.
\end{align}
Then, we derive the expression of $\predgap{}$
\begin{align}
    \nonumber
    \predgap{} = & \E_{\vr_\s}[ (\vr_\s^\top\vw_\s - \vr_\s^\top\vw_\wtos)^2 ]\\
    = & \E_{\vr_\s}[ (\vr_\s^\top(\vw_\s - \vw_\wtos))^2 ]\\
    \nonumber
    = & \E_{\vr_\s}[(\vw_\s - \vw_\wtos)^\top \vr_\s\vr_\s^\top(\vw_\s - \vw_\wtos) ] \\
    \nonumber
    = &  (\vw_\s - \vw_\wtos)^\top \E_{\vr_\s}[\vr_\s\vr_\s^\top](\vw_\s - \vw_\wtos)\\
    \nonumber
    = & (\vw_\s - \vw_\wtos)^\top\mSigma_\s (\vw_\s - \vw_\wtos)\\
    \nonumber
    = & \| \sqrt{\mSigma}_\s (\vw_\s - \vw_\wtos) \|^2\\
    \nonumber
    = & \| \underbrace{ \sqrt{\mSigma}_\s \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} }_{\text{a transformation determined by the strong model's representations}} \underbrace{\left( \frac{1}{\sqrt{\hat{n}}} \hat{\vy} -  \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\w^\top \vw_\w \right) }_{\text{weak model's normalized error vector on $\hat{\gD}$}}\| \\
    \label{eq: expression_predgap}
    = & \|  \underbrace{\sqrt{\mSigma}_\s \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} }_{\text{a transformation determined by the strong model's representations}}\underbrace{\left( \frac{1}{\sqrt{\hat{n}}} \hat{\vy} -  
\frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\w^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \right) }_{\text{weak model's normalized error vector on $\hat{\gD}$}} \|.
\end{align}
From the above, we see that $\predgap{}$ can be broken into two parts: the weak model's normalized error vector on $\hat{\gD}$, and a transformation applied to this error vector which captures how the weak model's errors propagate to the strong model. In Sections \ref{apdx: weak_error} and \ref{apdx: error_propogation}, we will analyze each part individually.

\subsection{The weak model's error}\label{apdx: weak_error}


\begin{lemma}[The weak model's error on $\hat{\gD}$ ]\label{lemma: weak_error}
The weak model's error vector on $\hat{\gD}$ can be approximated as follows
\begin{align}
\nonumber
\|\left(\frac{1}{\sqrt{\hat{n}}}\hat{\vy}-   \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy}\right) - (\mI -\mP_\w )\frac{1}{\sqrt{\hat{n}}}\hat{\vy} \|= o(1),
\end{align}
where $\mP_\w = \frac{1}{\hat{n}} \hat{\mR}_\w^\top \mU_\w' \mU_\w'^\top \hat{\mR}_\w\left(\frac{1}{\tilde{n}}  \hat{\mR}_\w^\top \mU_\w'\mU_\w'^\top\hat{\mR}_\w+(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1} $.
\end{lemma}
\begin{proof}
By condition \bounded{} and Lemma \ref{lemma: isotropy_inv}, we have
\begin{align}
\nonumber
   & \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \\
   \nonumber
    = & \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w \left(\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top\tilde{\mR}_\w +(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} + o(1) \\
    \nonumber
    = & \left(\frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top \mU_\w'\mU_\w'^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w+\frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top \mU_\w''\mU_\w''^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w\right) \left(\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top\tilde{\mR}_\w +(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} + o(1) 
    % \nonumber
    % = & \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top  \left(\frac{1}{\tilde{n}} \mU_\w'^\top\tilde{\mR}_\w \tilde{\mR}_\w^\top \mU_\w'+(\gamma_\w+ \beta_\w)\mI \right)^{-1} \frac{1}{\tilde{n}} \tilde{\mR}_\w \tilde{\vy} + o(1) \quad\quad\text{by Lemma \ref{lemma: pushthrough}.}
\end{align}
By conditions \smallin{} and \bounded{}, and noting that $\opnorm{(\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top\tilde{\mR}_\w +(\tilde{\gamma}_\w+ \beta_\w)\mI)^{-1}} \leq \frac{1}{\tilde{\gamma}_\w+\beta_w} $,
the preceding can be further bounded as 
\begin{align}
\nonumber
& \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \\
\nonumber
= & \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top \mU_\w'\mU_\w'^\top  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w \left(\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top\tilde{\mR}_\w +(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} + o(1) \\
     \nonumber
     = & \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top \mU_\w' \left(\frac{1}{\tilde{n}} \mU_\w'^\top\tilde{\mR}_\w \tilde{\mR}_\w^\top \mU_\w'+(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1} \mU_\w'^\top \frac{1}{\tilde{n}} \tilde{\mR}_\w \tilde{\vy} + o(1) \quad\quad\text{by Lemma \ref{lemma: pushthrough}.}
\end{align}
By Lemma \ref{lemma: concentration_inv} and condition \bounded{},  the above further leads to
\begin{align}
\nonumber
& \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \\
     \nonumber
     = & \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top \mU_\w' \left( \mLambda_\w' +(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1} \mU_\w'^\top \frac{1}{\tilde{n}} \tilde{\mR}_\w \tilde{\vy} + o(1). 
\end{align}
Condition \conc{} implies that $\opnorm{ \mU_\w'^\top \frac{1}{\hat{n}} \hat{\mR}_\w \hat{\vy} -\mU_\w'^\top \frac{1}{\hat{n}} \hat{\mR}_\w \hat{\vy}  } = o( \lambda_{\min}(\mLambda_\w')+\gamma_\w+\beta_\w) $ via the triangle inequality. Then, by condition \bounded{} and that $\opnorm{(\mLambda_\w'+(\tilde{\gamma}_\w+ \beta_\w)\mI )^{-1}} = \frac{1}{ \lambda_{\min}(\mLambda_\w')+\tilde{\gamma}_\w+\beta_\w}$ , we further have
\begin{align}
\nonumber
& \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \\
     \nonumber
     = & \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top \mU_\w' \left( \mLambda_\w'+(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1} \mU_\w'^\top \frac{1}{\hat{n}} \hat{\mR}_\w \hat{\vy} + o(1)\\
     \nonumber
     = & \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top \mU_\w' \left( \frac{1}{\hat{n}} \mU_\w'^\top\hat{\mR}_\w \hat{\mR}_\w^\top \mU_\w'+(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1} \mU_\w'^\top \frac{1}{\hat{n}} \hat{\mR}_\w \hat{\vy} + o(1) \quad\quad\text{by Lemma \ref{lemma: concentration_inv} and condition \bounded{}} \\
     \nonumber
     = & \frac{1}{\hat{n}} \hat{\mR}_\w^\top \mU_\w' \mU_\w'^\top \hat{\mR}_\w\left(\frac{1}{\tilde{n}}  \hat{\mR}_\w^\top \mU_\w'\mU_\w'^\top\hat{\mR}_\w+(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1}  \frac{1}{\sqrt
     {\hat{n}}}\hat{\vy} + o(1) \quad\quad \text{by Lemma \ref{lemma: pushthrough}}.
\end{align}
Let us define the shorthand $\mP_\w = \frac{1}{\hat{n}} \hat{\mR}_\w^\top \mU_\w' \mU_\w'^\top \hat{\mR}_\w\left(\frac{1}{\tilde{n}}  \hat{\mR}_\w^\top \mU_\w'\mU_\w'^\top\hat{\mR}_\w+(\tilde{\gamma}_\w+ \beta_\w)\mI \right)^{-1} $. Then, we conclude that
\begin{align}
    \nonumber
    \frac{1}{\sqrt{\hat{n}}}\hat{\vy}-   \frac{1}{\sqrt{\hat{n}}} \hat{\mR}_\w^\top  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} = (\mI -\mP_\w )\frac{1}{\sqrt{\hat{n}}}\hat{\vy} + o(1).
\end{align}
\end{proof}


\subsection{Propagation of the error to the strong model}\label{apdx: error_propogation}


\begin{lemma}\label{lemma: propagation_strong}
 For any $\vpsi$ with $\| \vpsi \|=O(1)$, we have $  \|  \sqrt{\mSigma_\s} \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \vpsi \|^2 = \| \mP_s \vpsi \|^2 \pm o(1)$, where
 $$
  \mP_\s = \frac{1}{\hat{n}}\hat{\mR}_\s^\top \mU_\s'\mU_\s'^\top \hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU'^\top\hat{\mR}_\s + (\hat{\gamma_\s}+\beta_\s)\mI )^{-1}.
 $$
\end{lemma}
\begin{proof}
We first decompose $\sqrt{\mSigma_\s} \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1}$ as follows
\begin{align}
\nonumber
    &\sqrt{\mSigma_\s} \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \\
    \nonumber
    = &\sqrt{\mSigma_\s} \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s   ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma_\s}+\beta_\s)\mI  )^{-1} + o(1) \quad\quad\text{by Lemma \ref{lemma: isotropy_inv}}
\\
\nonumber
 = &  (\mU_\s' \sqrt{\mLambda_\s'} \mU_\s'^\top + \mU_\s'' \sqrt{\mLambda_\s''} \mU_\s''^\top) \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s   ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma_\s}+\beta_\s)\mI  )^{-1} + o(1)\\
\nonumber
 = & \mU_\s' \sqrt{\mLambda_\s'} \mU_\s'^\top  \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s   ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma_\s}+\beta_\s)\mI  )^{-1} \\
\label{eq: strong_transform_decomp}
& + \mU_\s'' \sqrt{\mLambda_\s''} \mU_\s''^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s   ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma_\s}+\beta_\s)\mI  )^{-1} + o(1)
\end{align}
The second term above can be bounded:
\begin{align}
    \nonumber
   & \opnorm{ \mU_\s'' \sqrt{\mLambda_\s''} \mU_\s''^\top  \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s   ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma}_\s+\beta_\s)\mI  )^{-1} } \\
   \nonumber
   \leq &\sqrt{\lambda_{\max}(\mLambda_\s'')  }  \frac{ \sqrt{o(\gamma_\s^2+\delta_\s^2)+\hat{\gamma}_\s } }{\hat{\gamma}_\s+\beta_\s} \quad\text{ by \bounded{} and Lemma \ref{lemma: scale_isotropy_kernel} } \\
   \nonumber
   \leq &\sqrt{ \opnorm{ \mSigma_\s'' } }  \frac{ \sqrt{o(\gamma_\s^2+\delta^2)+\hat{\gamma}_\s  } }{\hat{\gamma}_\s+\delta_\s}  \\
   \nonumber
   = &  o\left( \sqrt{ \frac{(\gamma_\s+\delta_\s )o(\gamma_\s^2+\delta_\s^2) + \hat{\gamma}_\s(\gamma_\s+\delta_\s)}{ (\hat{\gamma}_\s+\delta_\s)^2 } } \right) \quad\text{by \dimini{}}\\
   \label{eq: diminishing_transform}
   \leq &   o\left( \sqrt{ \frac{ o(\gamma_\s^2+\delta_\s^2) }{ \hat{\gamma}_\s+\delta_\s }+\frac{\hat{\gamma}_\s}{ \hat{\gamma}_\s+\delta_\s } } \right) = o(1).
\end{align}

Combining Equations \ref{eq: strong_transform_decomp} and \ref{eq: diminishing_transform} yields
\begin{align}
    \nonumber
    \sqrt{\mSigma_\s} \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \vpsi = \mU_\s' \sqrt{\mLambda_\s'} \mU_\s'^\top  \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s   ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma_\s}+\beta_\s)\mI  )^{-1}\vpsi + o(1). 
\end{align}
Finally, we consider the squared norm:
\begin{align}
    \nonumber
 & \|  \sqrt{\mSigma_\s} \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \vpsi \|^2 \\
  \nonumber
  = & \| \mU_\s' \sqrt{\mLambda_\s'} \mU_\s'^\top  \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s   ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma}_\s+\beta_\s)\mI  )^{-1}\vpsi\|^2 \\
  \nonumber
  & \pm  o\left(\|\mU_\s' \sqrt{\mLambda_\s'} \mU_\s'^\top  \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s   ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma}_\s+\beta_\s)\mI  )^{-1}\vpsi\|\right) \pm o(1) \\
  \nonumber
  = & \| \frac{1}{\hat{n}}\hat{\mR}_\s^\top \mU_\s'\mU_\s'^\top \hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU'^\top\hat{\mR}_\s + (\hat{\gamma}_\s+\beta_\s)\mI )^{-1} \vpsi \|^2 \pm o(1) \quad\text{by Corollary \ref{coro: sqrt_lambda}}.
\end{align}
\end{proof}

\subsection{Proof of Theorem \ref{thm: main_theorem}} \label{apdx: proof_main_theorem}
Given that $\| \frac{1}{\sqrt{\hat{n}}}\hat{\vy} \|=O(1)$ by \bounded, and that 
$\opnorm{\mI -\mP_\w  } = \frac{ \beta_\w }{ \lambda_{\min}( \frac{1}{\hat{n}}\hat{\mR}_\w^\top \mU_\w'\mU_\w'^\top \hat{\mR}_\w  ) +\beta_\w } \leq 1 $, we have $\|(\mI -\mP_\w)\frac{1}{\sqrt{\hat{n}}}\hat{\vy}\|=O(1) $. Then, by Lemma \ref{lemma: weak_error}, the weak model's error on $\hat{\gD}$ can be bounded  as $\left\| 
 (\mI -\mP_\w )\frac{1}{\sqrt{\hat{n}}}\hat{\vy} \right\|+o(1)=O(1)$. Recalling the expression of \predgap{} derived in Equation \ref{eq: expression_predgap} and applying Lemmas \ref{lemma: weak_error} and \ref{lemma: propagation_strong}, we obtain:
\begin{align}
    \nonumber
    \predgap = \| \mP_\s (\mI -\mP_\w ) \frac{1}{\sqrt{\hat{n}}} \hat{\vy}  \|^2 \pm o(1). %\leq C\opnorm{\mP_\s (\mI -\mP_\w ) }^2  + o(1),
\end{align}
%where $C=\frac{1}{\hat{n}}\sum_{i=1}^{\hat{n}}\hat{y}_i^2  $.

\section{Additional Analysis}

\subsection{Additional Lemmas}

\begin{lemma}\label{lemma: bound_ery_non_principal}
By \dimini{} and \bounded{}, we have
\begin{align}
    \nonumber
    \E[ \mU'' \mU''^\top \vr y ]=o(\sqrt{\gamma+\delta}).
\end{align}
\begin{proof}
    \begin{align}
        \nonumber
        \E[ \mU'' \mU''^\top \vr y ] =& \lim_{n\rightarrow \infty} \frac{1}{n}\sum_{i=1}^n  \mU'' \mU''^\top \vr_i y_i =\lim_{n\rightarrow \infty} \frac{1}{\sqrt{n}}\mU''\mU''^\top \mR\frac{1}{\sqrt{n}}\vy  \leq  \lim_{n\rightarrow \infty} \opnorm{\frac{1}{\sqrt{n}}\mU''\mU''^\top \mR } \|\frac{1}{\sqrt{n}}\vy \|\\
        \nonumber
        = & \lim_{n\rightarrow \infty} \sqrt{\opnorm{\frac{1}{n}\mU''\mU''^\top \mR\mR^\top\mU''\mU''^\top }} \sqrt{\frac{1}{n}\sum_{i=1}^n y_i^2} =\sqrt{\opnorm{ \mSigma'' }} \sqrt{\E[y^2]} = o(\sqrt{\gamma+\delta}).
    \end{align}
\end{proof}
\end{lemma}

\begin{lemma}\label{lemma: bound_ery_principal}
By \bounded{}, we have
\begin{align}
    \nonumber
    \E[ \mU' \mU'^\top \vr y ]=O(1).
\end{align}
\end{lemma}
\begin{proof}
The proof follows the same approach as that of Lemma \ref{lemma: bound_ery_non_principal}. This conclusion can also be derived by bounding $\E[ \mU' \mU'^\top \vr y ]$ in terms of its empirical counterpart using \conc{}, and then applying \bounded{}
\end{proof}

\subsection{When $\err_\wtos \approx \predgap{} + \err_\sceiling  $}

\begin{theorem}\label{thm: ewtos=predgap+errsc}
Suppose that, in addition to Assumption \ref{assump: weak_strong_decomp}, the conditions 
$
\beta_\s + \hat{\gamma}_\s = o(\lambda_{\text{min, $\neq 0$}}(\mSigma(\mPi_{\gV_\s}h_\s))=\Theta(1))
$ and $\lambda_{\text{min, $\neq 0$}}(\mSigma(\mPi_{\gV_\s}h_\s))=\Theta(\lambda_{\max}(\mSigma(\mPi_{\gV_\s}h_\s))$
% and 
% $
% \E[\mPi_{\gV_\s^\perp} h_\s(\vx) y] = o(1)
% $
hold. Then, w.h.p., we have:
\begin{align}
\nonumber
    \err_\wtos = \predgap + \err_\sceiling \pm o(1).
\end{align}
\end{theorem}
\begin{proof}
First, decompose $\err_\wtos$ as follows
\begin{align}
    \nonumber
    \err_\wtos = & \E[ (\vr_\s^\top\vw_\wtos - y)^2 ]\\
    \nonumber
    = & \E[ (\vr_\s^\top\vw_\wtos - \vr_\s^\top\vw_\sceiling + \vr_\s^\top\vw_\sceiling-  y)^2 ]\\
    \nonumber
    = & \E[ (\vr_\s^\top\vw_\wtos - \vr_\s^\top\vw_\sceiling)^2 + (\vr_\s^\top\vw_\sceiling-  y)^2 + 2(\vw_\wtos^\top\vr_\s- \vw_\sceiling^\top\vr_\s)( \vr_\s^\top\vw_\sceiling-  y) ] \\
    \nonumber
    = & \predgap{} + \err_\sceiling + 2\E[(\vw_\wtos^\top\vr_\s- \vw_\sceiling^\top\vr_\s)( \vr_\s^\top\vw_\sceiling-  y)]\\
    = & \predgap{} + \err_\sceiling + 2(\vw_\wtos- \vw_\sceiling)^\top(\mSigma_\s \vw_\sceiling- \E [\vr_\s y]),
\end{align}
Thus, to prove the theorem, it suffices to show $| (\vw_\wtos- \vw_\sceiling)^\top(\mSigma_\s \vw_\sceiling- \E [\vr_\s y]) |=o(1)$. We decompose $(\vw_\wtos- \vw_\sceiling)^\top(\mSigma_\s \vw_\sceiling- \E [\vr_\s y])$:
\begin{align}
    \nonumber
    &(\vw_\wtos- \vw_\sceiling)^\top(\mSigma_\s \vw_\sceiling- \E [\vr_\s y]) \\
    \nonumber
    = & (\vw_\wtos- \vw_\sceiling)^\top(\mSigma_\s' \vw_\sceiling+\mSigma_\s'' \vw_\sceiling- \mU_\s'\mU_\s'^\top\E [\vr_\s y]-\mU_\s''\mU_\s''^\top\E [\vr_\s y])\\
    \label{eq: wwsigmawery_decomp}
    = & (\vw_\wtos- \vw_\sceiling)^\top(\mSigma_\s' \vw_\sceiling- \mU_\s'\mU_\s'^\top\E [\vr_\s y]) + (\vw_\wtos- \vw_\sceiling)^\top\mSigma_\s'' \vw_\sceiling- (\vw_\wtos- \vw_\sceiling)^\top\mU_\s''\mU_\s''^\top\E [\vr_\s y]
\end{align}
$\vw_\wtos- \vw_\sceiling$ can be approximated as:
\begin{align}
    \nonumber
    \vw_\wtos- \vw_\sceiling = & \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \frac{1}{\sqrt{\hat{n}}} ( \hat{\mR}_\w^\top \vw_\w) - \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \frac{1}{\sqrt{\hat{n}}} \hat{\vy}\\
    \nonumber
    = & \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} (\frac{1}{\sqrt{\hat{n}}}  \hat{\mR}_\w^\top \vw_\w - \frac{1}{\sqrt{\hat{n}}} \hat{\vy} ) \\
    \label{eq: approx_ww2s_minus_wsc}
    = & \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s \left( \frac{1}{\hat{n}}\hat{\mK}_\s'  + (\hat{\gamma}_\s+\beta_\s)\mI  \right)^{-1} (\frac{1}{\sqrt{\hat{n}}}  \hat{\mR}_\w^\top \vw_\w - \frac{1}{\sqrt{\hat{n}}} \hat{\vy} )  + o(1) \quad\text{by Lemma \ref{lemma: isotropy_inv} and that other terms are $O(1)$} 
\end{align}
where $\hat{\mK}_\s' = \hat{\mR}_\s^\top \mU_\s' \mU_\s'^\top \hat{\mR}_\s$ is shorthand for $\hat{\mK}(\mPi_{\gV_\s} h_\s)$. Then, by Lemma \ref{lemma: scale_isotropy_kernel} and \bounded{}, we obtain:
\begin{align}
    \label{eq: wwu}
    \|(\vw_\wtos- \vw_\sceiling)^\top \mU_\s'' \| = & O( \frac{\sqrt{o(\gamma_\s^2+\delta_\s^2)+\hat{\gamma}_\s}}{\hat{\gamma}_\s+\beta_\s} ).
\end{align}
We also have the following bound:
\begin{align}
    \nonumber
    & \opnorm{ \mU_\s''^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \frac{1}{\sqrt{\hat{n}}} \hat{\vy} }\\
    \nonumber
    = &  \opnorm{\mU_\s''^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s  ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma}_\s+\beta_\s)\mI  )^{-1} \frac{1}{\sqrt{\hat{n}}} \hat{\vy} }+ o(\opnorm{\mU_\s''^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s}) \quad\text{by \bounded{} and Lemma \ref{lemma: isotropy_inv}}\\
    \label{eq: urrrbetay}
    = &  O( \frac{\sqrt{o(\gamma_\s^2+\delta_\s^2) +\hat{\gamma}_\s }}{ \hat{\gamma}_\s+\beta_\s } ) \quad\text{by Lemma \ref{lemma: scale_isotropy_kernel} and \bounded{}}
\end{align}
Combining \dimini{} and Equations \ref{eq: wwu} and \ref{eq: urrrbetay}, the second term in Equation \ref{eq: wwsigmawery_decomp} can be bounded as:
\begin{align}
    \nonumber
   | (\vw_\wtos- \vw_\sceiling)^\top \mSigma_\s''\vw_\sceiling | 
  = & | (\vw_\wtos- \vw_\sceiling)^\top \mU_\s''\mLambda_\s''\mU_\s''^\top\vw_\sceiling |  \\
  \label{eq: wwsigmaw_o1}
  = & o\left(\frac{  (o(\gamma_\s^2+\delta_\s^2)+\hat{\gamma}_\s ) (\gamma_\s+\delta_\s) }{(\hat{\gamma}_\s+\beta_\s)^2}\right) = o(1).
\end{align}
The third term in Equation \ref{eq: wwsigmawery_decomp} can be bounded as:
\begin{align}
    \nonumber
|(\vw_\wtos- \vw_\sceiling)^\top\mU_\s''\mU_\s''^\top\E [\vr_\s y] |\leq &     \|(\vw_\wtos- \vw_\sceiling)^\top\mU_\s''\|\|\mU_\s''^\top\E [\vr_\s y]\|\\
\nonumber
= & O( \frac{\sqrt{o(\gamma_\s^2+\delta_\s^2)+\hat{\gamma}_\s}}{\hat{\gamma}_\s+\beta_\s} ) o(\sqrt{\gamma_\s+\delta_\s})\quad\text{by Equation \ref{eq: wwu} and Lemma \ref{lemma: bound_ery_non_principal}}\\
\label{eq: wwuuery_o1}
= & o(1).
\end{align}
Now, it remains to bound the first term in Equation \ref{eq: wwsigmawery_decomp}. We start with approximating $\mSigma_\s' \vw_\sceiling-\mU_\s'\mU_\s'^\top\E [\vr_\s y]$:
\begin{align}
    \nonumber
    & \mSigma_\s' \vw_\sceiling-\mU_\s'\mU_\s'^\top\E [\vr_\s y]\\
    \nonumber
    = & \mU_\s'\mLambda_\s'\mU_\s'^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \frac{1}{\sqrt{\hat{n}}} \hat{\vy} -\mU_\s'\mU_\s'^\top\E [\vr_\s y]\\
    % \label{eq: sigmawuuery_intermediate_step}
    % = & \mU_\s'\mLambda_\s'\mU_\s'^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \frac{1}{\sqrt{\hat{n}}} \hat{\vy} -\mU_\s'\mU_\s'^\top \frac{1}{\hat{n}}\hat{\mR}_\w\hat{\vy} + o(\gamma_\w +\delta_\w +\lambda_{\min}(\mLambda_\w')) \quad\text{by \conc{}} \\
    \nonumber
    =  &  \mU_\s'\mLambda'\mU_\s'^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s  \left(\frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma}_\s+\beta_\s)\mI  \right)^{-1} \frac{1}{\sqrt{\hat{n}}} \hat{\vy} -\mU_\s'\mU_\s'^\top\E [\vr_\s y]+ o(1) \\
    \nonumber
    &\quad\text{by Lemma \ref{lemma: isotropy_inv} and \bounded{}} \\
    \nonumber
    = & \mU_\s\mU_\s'^\top\hat{\mSigma}\mU_\s'\mU_\s'^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s  \left(\frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma}_\s+\beta_\s)\mI  \right)^{-1} \frac{1}{\sqrt{\hat{n}}} \hat{\vy} -\mU_\s'\mU_\s'^\top\E [\vr_\s y]+ o(1) \\
    \nonumber
    &\quad\text{by \conc{} and \bounded{}}\\
    \label{eq: approx_sigma_wsc_minus_ery}
    = & \mU_\s\mU_\s'^\top\hat{\mSigma}\mU_\s'\mU_\s'^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s  \left(\frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + (\hat{\gamma}_\s+\beta_\s)\mI  \right)^{-1} \frac{1}{\sqrt{\hat{n}}} \hat{\vy}  -\mU_\s\mU_\s'\frac{1}{\hat{n}}\hat{\mR}_\s\hat{\vy} + o(1) \quad\text{by \conc{}}.
\end{align}
Due to the two additional assumptions in the statement of the theorem, along with \conc{} and \bounded{}, the RHSs of both \eqref{eq: approx_ww2s_minus_wsc} and \eqref{eq: approx_sigma_wsc_minus_ery} are $O(1)$.
Combining \eqref{eq: approx_ww2s_minus_wsc} and \eqref{eq: approx_sigma_wsc_minus_ery},  we obtain:
\begin{align}
    \nonumber
    & (\vw_\wtos- \vw_\sceiling)^\top  ( \mSigma_\s' \vw_\sceiling - \mU_\s\mU_\s'^\top\E[\vr_\s y] ) \\
    \nonumber
    = & (\frac{1}{\sqrt{\hat{n}}}  \hat{\mR}_\w^\top \vw_\w - \frac{1}{\sqrt{\hat{n}}} \hat{\vy} )^\top \left( \frac{1}{\hat{n}}\hat{\mK}_\s' + (\hat{\gamma}_\s+\beta_\s)\mI  \right )^{-1}\\
    \nonumber
   & \times\frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s^\top \left(\mU_\s\mU_\s'^\top\hat{\mSigma}\mU_\s'\mU_\s'^\top \left(\mU_\s\mU_\s'^\top\hat{\mSigma}\mU_\s'\mU_\s'^\top + (\hat{\gamma}_\s+\beta_\s)\mI  \right)^{-1} -\mU_\s'\mU_\s'^\top\right) \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s   \frac{1}{\sqrt{\hat{n}}} \hat{\vy} + o(1) \\
       \nonumber
    = & (\frac{1}{\sqrt{\hat{n}}}  \hat{\mR}_\w^\top \vw_\w - \frac{1}{\sqrt{\hat{n}}} \hat{\vy} )^\top \left( \frac{1}{\hat{n}}\hat{\mK}_\s' + (\hat{\gamma}_\s+\beta_\s)\mI  \right )^{-1} \\
    \nonumber
    & \times\left(\frac{1}{\hat{n}}\hat{\mK}_\s' \left(\frac{1}{\hat{n}}\hat{\mK}_\s' +(\hat{\gamma}_\s+\beta_\s)\mI \right)^{-1} \frac{1}{\hat{n}}\hat{\mK}_\s' - \frac{1}{\hat{n}}\hat{\mK}_\s'\right) \frac{1}{\sqrt{\hat{n}}} \hat{\vy}  + o(1)\quad\text{by Lemma \ref{lemma: pushthrough}}\\
    \label{eq: w_E_inner_product_approx}
    = & (\frac{1}{\sqrt{\hat{n}}}  \hat{\mR}_\w^\top \vw_\w - \frac{1}{\sqrt{\hat{n}}} \hat{\vy} )^\top \left(  \mP_\s\mP_\s -\mP_\s  \right)^\top \frac{1}{\sqrt{\hat{n}}}\hat{\vy}+ o(1).
\end{align}
$\mP_\s\mP_\s -\mP_\s$'s eigenvalues are given by:  $ (\frac{\lambda_i( \frac{1}{\hat{n}}\hat{\mK}_\s' )}{\lambda_i( \frac{1}{\hat{n}}\hat{\mK}_\s' ) +(\hat{\gamma}_\s+\beta_\s) } )^2 - \frac{\lambda_i( \frac{1}{\hat{n}}\hat{\mK}_\s' )}{\lambda_i( \frac{1}{\hat{n}}\hat{\mK}_\s' ) +(\hat{\gamma}_\s+\beta_\s) } = -(\frac{\lambda_i( \frac{1}{\hat{n}}\hat{\mK}_\s' )}{\lambda_i( \frac{1}{\hat{n}}\hat{\mK}_\s' ) +(\hat{\gamma}_\s+\beta_\s) } )(\frac{\hat{\gamma}_\s+\beta_\s}{\lambda_i( \frac{1}{\hat{n}}\hat{\mK}_\s' ) +(\hat{\gamma}_\s+\beta_\s) } ) $. since $\frac{1}{\hat{n}}\hat{\mK}_\s'$ and $\hat{\mSigma}_\s$ share non-zero eigenvalues,  we analyze the relation between $\beta_\s+\hat{\gamma}_\s$ and $\hat{\mSigma_\s'}$'s non-zero eigenvalues.  
By \conc{} and Weyl's Theorem
\begin{align}
    \nonumber
    | \lambda_{\text{min, $\neq 0$}}(\hat{\mSigma}_\s') -\lambda_{\text{min, $\neq 0$}}(\mSigma_\s') | = o( \gamma_\s^2 +\delta_\s^2+\lambda_{\text{min, $\neq 0$}}(\mSigma_\s'))
\end{align}
Combining this with $\beta_\s+\hat{\gamma}_\s=o(\lambda_{\text{min, $\neq 0$}}(\mSigma_\s'))$, we conclude: 
\begin{align}
\label{eq: beta_gamma_o_lambda_min}
\beta_\s+\hat{\gamma}_\s=o(\lambda_{\text{min, $\neq 0$}}(\hat{\mSigma_\s'})).
\end{align}
Using Equation \ref{eq: beta_gamma_o_lambda_min}, we then obtain $\opnorm{\mP_\s\mP_\s -\mP_\s}=o(1)$. By Lemma \ref{lemma: weak_error}, the term $ (\frac{1}{\sqrt{\hat{n}}}  \hat{\mR}_\w^\top \vw_\w - \frac{1}{\sqrt{\hat{n}}} \hat{\vy} )$ can be bounded by $\left\| 
 (\mI -\mP_\w )\frac{1}{\sqrt{\hat{n}}}\hat{\vy} \right\|+o(1)=O(1)$, and $\|\frac{1}{\sqrt{\hat{n}}}\hat{\vy}\|=O(1)$ by \bounded{}. Combining all these results, the RHS of Equation \ref{eq: w_E_inner_product_approx} is $o(1)$. Therefore, $|(\vw_\wtos- \vw_\sceiling)^\top  ( \mSigma_\s \vw_\sceiling - \E[\vr_\s y] ) |=o(1)$, which completes the proof.
\end{proof}

\subsection{Proof of results in Section \ref{sec: case_study}}

\subsubsection{Proof of Theorem \ref{thm: general_bn}}\label{apdx: proof_benign_of}

First, we present the following lemma, which provides a sufficient condition under which any labeling can be fitted by the W2S model.

\begin{lemma}[Condition for overfitting arbitrary labels]\label{lemma: overfitting_condition}
As long as $\delta_\s = o(\hat{\gamma_\s})$ and $\delta_s\leq \beta_\s=o(\hat{\gamma_\s})$, given any $f_\w \circ h_\w~s.t.~\frac{1}{\hat{n}}\sum_{i=1}^{\hat{n}} f_\w(h_\w( \hat{\vx}_i ))^2=O(1)$, the weak-to-strong model can almost exactly overfit it, as indicated by an almost zero training error: $\frac{1}{\hat{n}}\sum_{i=1}^{\hat{n}} \left(f_\wtos(h_\s(\hat{\vx}_i)) - f_\w(h_\w(\hat{\vx}_i))\right)^2 = o(1)$, with high probability $1 - o(1)$. 
\end{lemma}
\begin{proof}

Let $\hat{\mT} \in \sR^{\hat{n}}$ denote the weak model's predictions on $\hat{\gD}$. The following holds for all $\hat{\mT}$ such that $\frac{1}{\hat{n}} |\hat{\mT} |^2 = O(1)$. The training loss can be expressed as
\begin{align}
    \nonumber
    \frac{1}{\hat{n}}\| \hat{\mR}_\s^\top\vw_\wtos - \hat{\mT}   \|^2 = &  \| \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} \frac{1}{\sqrt{\hat{n}}}\hat{\mT} - \frac{1}{\sqrt{\hat{n}}}\hat{\mT} \|^2 \quad\quad \text{by Equation \ref{eq: exp_wtos}} \\
    \nonumber
    = & \| \left(\frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} -\mI\right) \frac{1}{\sqrt{\hat{n}}}\hat{\mT} \|^2\\
    \nonumber
    \leq & \opnorm{\frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s^\top \frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\s ( \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s +\beta_\s \mI )^{-1} -\mI}^2 \| \frac{1}{\sqrt{\hat{n}}}\hat{\mT} \|^2\\
    \nonumber
    = & \left(\frac{\beta_\s}{\lambda_{\min}(  \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s ) +\beta_\s }\right)^2  \| \frac{1}{\sqrt{\hat{n}}}\hat{\mT} \|^2\\
    \label{eq: training_loss_bound}
     = & O\left(\left(\frac{\beta_\s}{\lambda_{\min}(  \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s ) +\beta_\s }\right)^2 \right) \quad\quad\text{because we assume $ \frac{1}{\hat{n}} \|\hat{\mT} \|^2=O(1) $}.  
\end{align}

By Lemma \ref{lemma: isotropy_eff_reg_kernel} and Weyl's Theorem, we have
\begin{align}
    \nonumber
   & | \lambda_{\min}(  \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s ) - \lambda_{\min}( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + \hat{\gamma}_\s\mI  ) | \leq \opnormlr{ \frac{1}{\hat{n}}\hat{\mR}_\s^\top \hat{\mR}_\s - \left( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + \hat{\gamma}_\s\mI  \right) } =o(\gamma_\s^2+\delta_\s^2)  \\
   \label{eq: lower_bound_lambda_min}
   \implies & \lambda_{\min}(  \frac{1}{\tilde{n}}\hat{\mR}_\s^\top \hat{\mR}_\s )  \geq \lambda_{\min}( \frac{1}{\hat{n}}\hat{\mR}_\s^\top\mU_\s'\mU_\s'^\top\hat{\mR}_\s + \hat{\gamma}_\s\mI  )-o(\gamma_\s^2+\delta_\s^2)  \geq \hat{\gamma}_\s-o(\gamma_\s^2+\delta_\s^2).
\end{align}
Substituding Equation \ref{eq: lower_bound_lambda_min} into Equation \ref{eq: training_loss_bound} yields
\begin{align}
    \nonumber
    \frac{1}{\hat{n}}\| \hat{\mR}_\s^\top\vw_\wtos - \hat{\mT}   \|^2 =O \left( \left(  
    \frac{\beta_\s}{\hat{\gamma}_\s-o(\gamma_\s^2+\delta_\s^2) +\beta_\s } \right)^2 \right) = o(1)   \quad\quad\text{because we assume $\beta_\s=o(\hat{\gamma_\s})$ and $\delta_\s=o(\hat{\gamma_\s})$},
\end{align}
which completes the proof.
\end{proof}

The first statement in Theorem \ref{thm: general_bn} can now be readily proved by invoking the above lemma.

For the second statement in Theorem \ref{thm: general_bn}, we first apply the triangle inequality, which gives $\sqrt{\err_{\wtos}} \leq \sqrt{\predgap{}} +\sqrt{\err_\sceiling}$. Given the assumption $\err_\sceiling=o(1)$ and the fact that Theorem \ref{thm: main_theorem} implies $\predgap{}=O(1)$, we obtain $\err_{\wtos} \leq \predgap{} + o(1) $. Furthermore, by our assumption combined with Theorem \ref{thm: main_theorem}, we know $\predgap{} = \err_\w-\Delta + o(1) $.  Substituting this into the previous inequality yields $\err_\wtos \leq  \err_\w-\Delta + o(1) $.


\subsubsection{Proof of Corollary \ref{coro: case_study}}\label{apdx: proof_case_study}

We begin by presenting the following general result regarding the test errors of the weak model and the strong ceiling model.

\begin{lemma}[The weak model's error on the population]\label{lemma: weak_error_population}
If $| \E[y^2]-\frac{1}{\hat{n}}\sum_{i=1}^{\hat{n}}\hat{y}_i^2  |=o(1)$ w.h.p., then
the weak model's error on the population, $\err_\w$ , can be approximated as follows, 
\begin{align}
\nonumber
\err_\w = \| (\mI - \mP_\w)\frac{1}{\sqrt{\hat{n}}}\hat{\vy}  \|^2 \pm o(1).
\end{align}
A similar conclusion holds for the strong ceiling's error $\err_\sceiling$ as well: $\err_\sceiling = \| (\mI - \mP_\s)\frac{1}{\sqrt{\hat{n}}}\hat{\vy}  \|^2 \pm o(1)$.
\end{lemma}
\begin{proof}
We decompose the error as follows
\begin{align}
    \nonumber
    \err_\w =& \E[ (\vr_\w^\top \vw_\w - y)^2 ] \\
    \label{eq: errw_decomp_3}
    = & \vw_\w^\top \mSigma_\w \vw_\w - 2\vw_\w^\top \E[\vr_\w y] + \E[y^2].
\end{align}
The first term can further be decomposed as:
\begin{align}
    \nonumber
    \vw_\w^\top \mSigma_\w \vw_\w = &  \|\sqrt{\mLambda_\w}\mU_\w^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2 \\
    \nonumber
    = & \|\sqrt{\mLambda_\w}\mU_\w^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \tilde{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2 \pm o(1) \quad\text{by Lemma \ref{lemma: isotropy_eff_reg_kernel} and \bounded{}} \\
    \nonumber
    = &  \| \vstack{\sqrt{\mLambda_\w'}\mU_\w'^\top}{\sqrt{\mLambda_\w''}\mU_\w''^\top  } \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \tilde{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2 \pm o(1) \\
    \nonumber
    = & \| \sqrt{\mLambda_\w'}\mU_\w'^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \tilde{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2 \\
    \label{eq: wsigmaw_decomp_3}
    & + \| \sqrt{\mLambda_\w''}\mU_\w''^\top   \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \tilde{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2   \pm o(1) 
\end{align}
We bound the second term in Equation \ref{eq: wsigmaw_decomp_3}:
\begin{align}
    \nonumber
    &\| \sqrt{\mLambda_\w''}\mU_\w''^\top   \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \tilde{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2 \\
    \nonumber
    \leq & \opnorm{ \mLambda_\w'' }\opnorm{ \mU_\w''^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w  }^2 \opnorm{ (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \tilde{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}}^2  \|\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy}  \|^2 \\
    \nonumber
    \leq & \frac{o( \gamma_\w +\delta_\w ) (o(\gamma_\w^2+\delta_\w^2)+\tilde{\gamma}_\w)}{(\beta_\w+\tilde{\gamma}_\w)^2}    \quad\text{by \dimini{}, Lemma \ref{lemma: scale_isotropy_kernel} and \bounded{}}\\
    \label{wsigma_term_2}
    = & o(1).
\end{align}
Then, we approximate the first term in Equation \ref{eq: wsigmaw_decomp_3}:
\begin{align}
    \nonumber
   &  \| \sqrt{\mLambda_\w'}\mU_\w'^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \tilde{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2 \\
    \nonumber
   = & \| \sqrt{\mLambda_\w'} (\frac{1}{\tilde{n}}\mU_\w'^\top \tilde{\mR}_\w\tilde{\mR}_\w^\top \mU_\w' + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1} \mU_\w'^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2 \quad\text{by Lemma \ref{lemma: pushthrough}}\\
   \nonumber
   = & \| \sqrt{\mLambda_\w'}(\mLambda_\w' + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1} \mU_\w'^\top\frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2 \pm o(1) \quad\text{by Lemma \ref{lemma: concentration_inv} and \bounded{}}\\
   \nonumber
   = & \| \sqrt{\mLambda_\w'} (\frac{1}{\hat{n}}\mU_\w'^\top \hat{\mR}_\w\hat{\mR}_\w^\top \mU_\w' + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1} \mU_\w'^\top\frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy} \|^2\pm o(1) \quad\text{by Lemma \ref{lemma: concentration_inv} and \bounded{}} \\
   \nonumber
   = & \| \sqrt{\mLambda_\w'} (\frac{1}{\hat{n}}\mU_\w'^\top \hat{\mR}_\w\hat{\mR}_\w^\top \mU_\w' + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1} \mU_\w'^\top\frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\w\frac{1}{\sqrt{\hat{n}}}\hat{\vy} \|^2 \pm o(1) \quad\text{by \conc{} and \bounded{}}\\
   \nonumber
   = & \| \sqrt{\mLambda_\w'} \mU_\w'^\top\frac{1}{\sqrt{\hat{n}}}\hat{\mR}_\w(\frac{1}{\hat{n}}\hat{\mR}_\w^\top \mU_\w'\mU_\w'^\top \hat{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1} \frac{1}{\sqrt{\hat{n}}}\hat{\vy} \|^2 \pm o(1)\quad\text{by Lemma \ref{lemma: pushthrough}}\\
   \nonumber
   = & \| \frac{1}{\hat{n}}\hat{\mR}_\w^\top \mU_\w'\mU_\w'^\top \hat{\mR}_\w(\frac{1}{\hat{n}}\hat{\mR}_\w^\top \mU_\w'\mU_\w'^\top \hat{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1} \frac{1}{\sqrt{\hat{n}}}\hat{\vy} \|^2 \pm o(1)\quad\text{by Corollary \ref{coro: sqrt_lambda} and \bounded{}}\\
   \label{eq: wsigma_term_1}
   = & \| \mP_\w \frac{1}{\sqrt{\hat{n}}}\hat{\vy} \|^2 \pm o(1).
\end{align}
Now, we approximate the second term in Equation \ref{eq: errw_decomp_3}:
\begin{align}
    \nonumber
    &\vw_\w^\top \E[\vr_\w y]\\
    \nonumber
    =& (\frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy})^\top \E[\vr_\w y] \\
    \nonumber
    = & (\frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy})^\top \mU_\w'\mU_\w'^\top \E[\vr_\w y] + (\frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy})^\top \mU_\w''\mU_\w''^\top \E[\vr_\w y] \\
    \nonumber
    = & (\frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy})^\top \mU_\w'\mU_\w'^\top \E[\vr_\w y] \pm o( \frac{\sqrt{o(\gamma_\w^2+\delta_\w^2)+\tilde{\gamma}_\w}}{\tilde{\gamma}_\w+\beta_\w}\sqrt{\gamma_\w+\delta_\w} )\\
    \nonumber
    & \quad\quad\quad\quad\quad\quad\quad\quad\text{by \bounded{}, Lemmas \ref{lemma: isotropy_inv}, \ref{lemma: scale_isotropy_kernel} and \ref{lemma: bound_ery_non_principal} }\\
    \nonumber
    = & (\frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \tilde{\mR}_\w + \beta_\w\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy})^\top \mU_\w'\mU_\w'^\top \E[\vr_\w y] \pm o(1) \\
    \nonumber
    = & (\frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \tilde{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}\frac{1}{\sqrt{\tilde{n}}}\tilde{\vy})^\top \mU_\w'\mU_\w'^\top \E[\vr_\w y] \pm o(1)\\
    \nonumber
    &\quad\quad\quad\quad\quad\quad\quad\quad\text{by Lemma \ref{lemma: isotropy_inv}, \bounded{}, and Lemma \ref{lemma: bound_ery_principal}}\\
    \nonumber
    = & \frac{1}{\sqrt{\tilde{n}}}\tilde{\vy}^\top (\frac{1}{\tilde{n}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \tilde{\mR}_\w + (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}  \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w^\top \mU_\w'\mU_\w'^\top \E[\vr_\w y] \pm o(1) \\
    \nonumber
    = & \frac{1}{\sqrt{\tilde{n}}}\tilde{\vy}^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w^\top \mU_\w' (\frac{1}{\tilde{n}}\mU_\w'^\top \tilde{\mR}_\w \tilde{\mR}_\w^\top \mU_\w'+ (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}  \mU_\w'^\top \E[\vr_\w y] \pm o(1)\quad\text{by Lemma \ref{lemma: pushthrough}}\\
    \nonumber
    = & \frac{1}{\sqrt{\tilde{n}}}\tilde{\vy}^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}_\w^\top \mU_\w' (\frac{1}{\tilde{n}}\mU_\w'^\top \hat{\mR}_\w \hat{\mR}_\w^\top \mU_\w'+ (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}  \mU_\w'^\top \E[\vr_\w y] \pm o(1)\\
    \nonumber
    &\quad\quad\quad\quad\quad\quad\quad\quad\text{by Lemma \ref{lemma: concentration_inv}, Lemma \ref{lemma: bound_ery_principal}, and \bounded{}}\\
    \nonumber
    = & \frac{1}{\hat{n}}\hat{\vy}^\top \hat{\mR}_\w^\top \mU_\w' (\frac{1}{\tilde{n}}\mU_\w'^\top \hat{\mR}_\w \hat{\mR}_\w^\top \mU_\w'+ (\beta_\w+\tilde{\gamma}_\w)\mI )^{-1}  \mU_\w'^\top \frac{1}{\hat{n}}\hat{\mR}_\w\hat{\vy} \pm o(1) \\
    \nonumber
&\quad\quad\quad\quad\quad\quad\quad\quad\text{by \conc{}, \bounded{} and Lemma \ref{lemma: bound_ery_principal}}\\
\label{eq: errw_term_2}
= & \frac{1}{\sqrt{\hat{n}}}\hat{\vy}^\top \mP_\w \frac{1}{\sqrt{\hat{n}}}\hat{\vy}\pm o (1) \quad\text{by Lemma \ref{lemma: pushthrough}}.
\end{align}
Combining Equations \ref{eq: errw_decomp_3}, \ref{eq: wsigmaw_decomp_3}, \ref{wsigma_term_2}, \ref{eq: wsigma_term_1}, \ref{eq: errw_term_2}, and the assumption about $\E[y^2]$ yields
\begin{align}
    \nonumber
    \err_\w = \| (\mI-\mP_\w)\frac{1}{\sqrt{\hat{n}}}\hat{\vy} \|^2 \pm o (1).
\end{align}
The proof of the result concerning $\err_\sceiling$ is similar.
\end{proof}


We show that the condition regarding $\E[y^2]$ is satisfied in Example \ref{eg: toyeg}. Specifically, $\sum_{i=1}^{\hat{n}} \hat{y}_i^2$ follows a $\chi^2(\hat{n})$ distribution, with a mean of $\hat{n} \E[y^2]$ and a variance of $2\hat{n}$. For simplicity, we demonstrate the following result using Chebyshev's inequality, while noting that tighter bounds could be achieved with tail bounds for $\chi^2$ variables or Lemma \ref{lemma: eigen_val_bound}. For any $k > 0$, we have:
$
\Pr\left(| \sum_{i=1}^{\hat{n}} \hat{y}_i^2 - \hat{n} \E[y^2] | \geq k \sqrt{2\hat{n}} \right) \leq \frac{1}{k^2}$. Letting $k = \hat{n}^{1/4}$, we find that with probability $1 - O\left(\frac{1}{\sqrt{\hat{n}}}\right)$,
$
\left| \frac{1}{\hat{n}} \sum_{i=1}^{\hat{n}} \hat{y}_i^2 - \E[y^2] \right| = O\left(\frac{1}{\hat{n}^{1/4}}\right)$. Thus, Lemma \ref{lemma: weak_error_population} applies to Example \ref{eg: toyeg}.

Now, based on Lemmas \ref{lemma: weak_error}, \ref{lemma: weak_error_population}, and Theorem \ref{thm: main_theorem}, the key to computing the errors of all these models boils down to simply computing $\mP_\w$ and $\mP_\s$.

We first compute the kernels. For convenience, we use the shorthand notations $\hat{\mK}_\w$ and $\hat{\mK}_\s$ to represent $\hat{\mK}(\mPi_{\gV_\w}h_\w)$ and $\hat{\mK}(\mPi_{\gV_\s}h_\s)$, respectively. Since the representations in Example \ref{eg: toyeg} are decomposable with respect to the subspace corresponding to the first coordinate, for both the weak and strong models, the principal kernels are rank one and can be expressed as $\hat{\mK}_\w = \vq\vq^\top $ and $\hat{\mK}_\s =\hat{\vy}\hat{\vy}^\top$, where $\hat{\vq}\coloneqq \sqrt{\eta}\hat{\vy}+\sqrt{1-\eta}\hat{\vzeta}   $.  Then, for $\frac{1}{\hat{n}}\hat{\mK}_\w$, it has a single nonzero eigenvalue $\| \frac{1}{\sqrt{\hat{n}}} \hat{\vq} \|^2$, with the corresponding eigenvector $\frac{1}{\|\frac{1}{\sqrt{\hat{n}}} \hat{\vq} \|} \frac{1}{\sqrt{\hat{n}}} \hat{\vq}$. Similarly, $\frac{1}{\hat{n}}\hat{\mK}_\s$ has a single eigenvalue $\| \frac{1}{\sqrt{\hat{n}}} \hat{\vy} \|^2$, with the corresponding eigenvector $\frac{1}{\|\frac{1}{\sqrt{\hat{n}}} \hat{\vy} \|} \frac{1}{\sqrt{\hat{n}}} \hat{\vy}$.

Next, we present the following Lemma.
\begin{lemma}\label{lemma: yzetaq} We have the following:
\begin{align}
    \nonumber
    \|\frac{1}{\sqrt{\hat{n}}} \hat{\vy} \|^2 = 1\pm o(1),~~ \|\frac{1}{\sqrt{\hat{n}}} \hat{\vzeta} \|^2 = 1\pm o(1), ~~| \frac{1}{\sqrt{\hat{n}}} \hat{\vzeta}^\top \frac{1}{\sqrt{\hat{n}}} \hat{\vy} |= o(1), ~~ \|\frac{1}{\sqrt{\hat{n}}} \hat{\vq} \|^2 = 1\pm o(1)
\end{align}
\end{lemma}
\begin{proof}
The first two statements can be proved by leveraging classical results on the concentration of Gaussian matrices (see Lemma \ref{lemma: eigen_val_bound} for details). The third statement follows as a special case of Lemma \ref{lemma: op_norm_bound}.  The last statement is implied by the previous three.
\end{proof}

Recall that both the weak and strong models' representations in Example \ref{eg: toyeg} are special cases of Example \ref{eg: spiked_cov}. Given that $\sigma^2 = o(\hat{n})$ and $\tilde{n} = \Theta(\hat{n})$, we have $\hat{\gamma}_\w$, $\tilde{\gamma}_\w$, $\hat{\gamma}_\s$, and $\tilde{\gamma}_\s$ all being $o(1)$, $\delta_\w=\delta_\s=0$, and $\beta_\w=o(1), \beta_\s=o(1)$. Combining these with Lemma \ref{lemma: yzetaq}, we derive:
\begin{align}
    \nonumber
\opnorm{ \mP_\w - \frac{1}{\hat{n}}\hat{\vq}\hat{\vq}^\top} = o(1), 
\opnorm{ \mP_\s - \frac{1}{\hat{n}}\hat{\vy}\hat{\vy}^\top} = o(1). 
\end{align}

Now, leveraging Lemma \ref{lemma: yzetaq}, we can derive all the errors using the expressions provided in Lemmas \ref{lemma: weak_error}, \ref{lemma: weak_error_population}, and Theorem \ref{thm: main_theorem}. \looseness=-1


\subsection{Proof of Corollary \ref{coro: predgap_ub_PPP}}\label{apdx: proof_PPP}

Following Theorem \ref{thm: main_theorem}, we bound the RHS as follows
\begin{align}
    \nonumber
    \predgap{} = & \| \mP_\s(\mI-\mP_\w)\mP_\s \frac{1}{\sqrt{\hat{n}}} \hat{\vy}+ \mP_\s(\mI-\mP_\w) ( 
\mI- \mP_\s)\frac{1}{\sqrt{\hat{n}}} \hat{\vy} \|^2 \pm o(1)\\
\leq & \left( \opnorm{ \mP_\s(\mI-\mP_\w)  \mP_\s} \|\frac{1}{\sqrt{\hat{n}}} \hat{\vy}\|+ \opnorm{\mP_\s(\mI-\mP_\w)} \|( 
\mI- \mP_\s)\frac{1}{\sqrt{\hat{n}}} \hat{\vy} \|\right)^2 + o(1) \\
\nonumber
\leq & \left(\opnorm{ \mP_\s(\mI-\mP_\w)  \mP_\s} \sqrt{C} +  \|( 
\mI- \mP_\s)\frac{1}{\sqrt{\hat{n}}} \hat{\vy} \|\right)^2 + o(1) \\
\nonumber
= & \left(\opnorm{ \mP_\s(\mI-\mP_\w)  \mP_\s} \sqrt{C} +  \sqrt{\err_\sceiling +o(1) } \right)^2 + o(1) \quad\text{by Lemma \ref{lemma: weak_error_population}} \\
\nonumber
= & \left( \opnorm{ \mP_\s(\mI-\mP_\w)  \mP_\s} \sqrt{C} +  \sqrt{\err_\sceiling } \right)^2 + o(1) \quad
\end{align}


\section{Proof of Examples in Section \ref{subsec: assump}}


\subsection{Example \ref{eg: intrinsic_bounded}}\label{apdx: example_bernstein}


For convenience, let $q = \intdim(\mSigma)$ and $\tau = \opnorm{\mSigma}$.

Firstly, we note that the conditions in the example imply a low intrinsic dimension. Heres why: since $\Tr(\mSigma) = \E |\vr|^2 \leq B$, it follows that
\begin{align} \label{eq: intdim_tau_B} \intdim(\mSigma) = \frac{\Tr(\mSigma)}{\opnorm{\mSigma}} \leq \frac{B}{\tau} = O(B), \end{align}
where the last step holds because $\tau = \opnorm{\mSigma} = \Theta(1)$. Given that $n^{1-c} = \omega(B \log(q))$, we then have $n^{1-c} = \omega(q \log(q))$, as mentioned in the remark.

Additionally, since $\intdim(\mSigma) \geq 1$, Equation \ref{eq: intdim_tau_B} also implies
\begin{align} \label{eq: B_tau_relation} B \geq \tau \quad \text{and} \quad B = \Omega(1), \end{align}
which we will use later.

Next, we introduce the following two lemmas, both of which rely on the matrix Bernstein inequality with intrinsic dimension, as stated in Theorem 7.3.1 of \cite{tropp2015introduction}.

\begin{lemma}\label{lemma: bernstein_cov}
With a probability of at least $1-8q\exp( \frac{- 0.5 \hat{n}^{1-c} }{ B\tau+ (B+\tau)/3   } )=1-o(1) $, the following holds
\begin{align}
    \nonumber
    \opnorm{\hat{\mSigma}-\mSigma} \leq \hat{n}^{-0.5c}.
\end{align}
The same conclusion applies to $\tilde{\mSigma}$ as well.
\end{lemma}
\begin{proof}
We prove the result for $\hat{\mSigma}$; the result for $\tilde{\mSigma}$ can be proved in the same way. 
Define $\mS_i = \frac{1}{\hat{n}}(\hat{\vr}_i\hat{\vr}_i^\top -\mSigma)$. The random matrices $\mS_i$ are independent, identically distributed, and centered. Their norms are bounded as follows
\begin{align}
    \nonumber
    \opnorm{\mS_i} \leq \frac{1}{\hat{n}}(\opnorm{\hat{\vr}_i\hat{\vr}_i^\top} + \opnorm{\mSigma}  ) \leq \frac{ B +  \tau }{ \hat{n}} \coloneqq L.
\end{align}
Then,
\begin{align}
    \nonumber
    \E \mS_i^2 = \frac{1}{\hat{n}^2} \E(\hat{\vr}_i\hat{\vr}_i^\top -\mSigma )^2 = \frac{1}{\hat{n}^2} \E(\|\hat{\vr}_i\|^2\hat{\vr}_i\hat{\vr}_i^\top -2\mSigma^2  +\mSigma^2 )  \preccurlyeq \frac{1}{\hat{n}^2} \E(B\hat{\vr}_i\hat{\vr}_i^\top -\mSigma^2 ) \preccurlyeq \frac{B}{\hat{n}^2}\mSigma
\end{align}
Define $\mZ = \sum_{i=1}^{\hat{n}} \mS_i $. We have
\begin{align}
    \nonumber
    \mathbf{0} \preccurlyeq \E \mZ^2 = \sum_{i=1}^{\hat{n}} \E \mS_i^2 \preccurlyeq \frac{B}{\hat{n}}\mSigma \coloneqq \mV
\end{align}
$\mV$'s norm can be expressed as follows:
\begin{align}
    \nonumber
   %\opnorm{\E \mZ^2} 
   \opnorm{\mV} = \frac{B \opnorm{\mSigma} }{\hat{n}} = \frac{B\tau}{\hat{n}}\coloneqq v
\end{align}
Define $d =\intdim(\diagmtx{ \mV }{ \mV}  )$, which can be simplified as:
\begin{align}
\nonumber
    d = 2\frac{\Tr(  \frac{B}{\hat{n}}\mSigma  )}{ \opnorm{ \frac{B}{\hat{n}}\mSigma } } = 2\intdim(  \frac{B}{\hat{n}}\mSigma  ) =2\intdim(\mSigma) =2q.
\end{align}
Now we are ready to apply Theorem 7.3.1 of \cite{tropp2015introduction}. It leads to the conclusion that, for any $t\geq \sqrt{v}+L/3$,
\begin{align}
    \nonumber
   \mathbb{P}\{ \opnorm{\mZ} \geq t \}\leq & 4d \exp{ (\frac{-t^2/2}{v+Lt/3} )} \\
   \nonumber
   = & 8 q \exp( \frac{-t^2/2}{ \frac{B\tau}{\hat{n}} + \frac{B+\tau}{\hat{n}} t /3   } ) \\
   \label{eq: pzt}
   = & 8 q\exp( \frac{- \hat{n} t^2/2}{ B\tau+ (B+\tau) t /3   } ) 
\end{align}
By assumption:
\begin{align} 
\nonumber
& n^{1-c}=\omega(B\log q)\\
\nonumber
\implies &  n^{1-c}=\omega(((\tau+1/3)B+\tau/3)\log q)  \quad\quad\text{because $\tau=O(1)$} \\
\nonumber
\implies & \frac{\hat{n}^{1-c}}{(\tau+1/3)B+\tau/3} = \omega(\log q  )\\
    \nonumber
    \implies  & 0.5\frac{\hat{n}^{1-c}}{(\tau+1/3)B+\tau/3} 
 = \omega( \log q ) \\
    \nonumber
    \implies & \exp\left(\frac{0.5\hat{n}^{1-c}}{(\tau+1/3)B+\tau/3}  \right) = \omega( q ) \\
    \label{eq: qexp}
    \implies & q \exp\left(\frac{-0.5\hat{n}^{1-c}}{(\tau+1/3)B+\tau/3}  \right) = o(1)
\end{align}
Therefore, we set the value of $t$ to $\hat{n}^{-0.5c} = o(1)$ in Equation \ref{eq: pzt}. It is easy to verify that $\hat{n}^{-0.5c} \geq \sqrt{v}+L/3$. Substituting, we get: 
\begin{align}
    \nonumber
   \mathbb{P}\{ \opnorm{\mZ} \geq \hat{n}^{-0.5c} \}\leq & 4d \exp{ (\frac{-t^2/2}{v+Lt/3} )} \leq 8q\exp( \frac{- \hat{n} t^2/2}{ B\tau+ (B+\tau) t /3   } ) =&  8q\exp( \frac{- 0.5 \hat{n}^{1-c} }{ B\tau+ (B+\tau) \hat{n}^{-0.5c} /3   } ) \\
    \nonumber
    \leq & 8q\exp( \frac{- 0.5 \hat{n}^{1-c} }{ B\tau+ (B+\tau)/3   } ) \quad\quad\text{because $\hat{n}^{-0.5c}\leq 1$} \\
    \nonumber
    = & o(1)   \quad\quad\text{by Equation \ref{eq: qexp}}.
\end{align}
Since $\mZ= \hat{\mSigma}-\mSigma $, restating the above, we have that with a probability of at least $1-8q\exp( \frac{- 0.5 \hat{n}^{1-c} }{ B\tau+ (B+\tau)/3   } ) $, the following holds
\begin{align}
    \nonumber
    \opnorm{\hat{\mSigma}-\mSigma} \leq \hat{n}^{-0.5c}.
\end{align}
\end{proof}

\begin{lemma}\label{lemma: bernstein_ry}
With a probability of at least $1-(q+4) \exp( \frac{-0.5\hat{n}^{1-c}}{4BC+\frac{2}{3} \sqrt{BC}} )=1-o(1) $, the following holds
\begin{align}
    \nonumber
    \| \frac{1}{\hat{n}}\sum_{i=1}^{\hat{n}} \hat{\vr}_iy_i  -\E[ \vr y ] \| \leq \hat{n}^{-0.5c}.
\end{align}
The same conclusion applies to $\frac{1}{\tilde{n}} \sum_{i=1}^{\tilde{n}}\tilde{\vr}_iy_i$ as well.
\end{lemma}
\begin{proof}
We prove the result for $ \frac{1}{\hat{n}} \sum_{i=1}^{\hat{n}}\hat{\vr}_iy_i $; the result for $ \frac{1}{\tilde{n}} \sum_{i=1}^{\tilde{n}}\tilde{\vr}_iy_i $ can be proved in the same way. Define $\mS_i = \frac{1}{\hat{n}}(\hat{\vr}_iy -\E[\vr y])$. The random matrices (vectors) $\mS_i$ are independent, identically distributed, and centered. Their norms are bounded as follows
\begin{align}
\label{eq: bound_S_L}
    \|\mS_i\| \leq \frac{1}{\hat{n}}(\|\hat{\vr}_i y\| + \| \E[\vr y] \|  ) \leq \frac{1}{\hat{n}}(\|\hat{\vr}_i\| |y| +  \E[\|\vr\|  |y|]   ) \leq \frac{2}{\hat{n}}\sqrt{BC} \coloneqq L.
\end{align}
Define $\mZ = \sum_{i=1}^{\hat{n}} \mS_i $. We analyze the semidefinite upper bounds for the variances $ \E \mZ\mZ^\top$ and $ \E \mZ^\top\mZ$:
\begin{align}
    \nonumber
    \E \mZ\mZ^\top = & \sum_{i=1}^{\hat{n}} \E \mS_i\mS_i^\top \\
    \nonumber
    = & \frac{1}{\hat{n}^2}( \E y_i^2\hat{\vr}_i\hat{\vr}_i^\top - \E [\vr y]\E [\vr y]^\top ) \\
    \nonumber
    \preccurlyeq &  \frac{1}{\hat{n}^2} \E y_i^2\hat{\vr}_i\hat{\vr}_i^\top \\
    \nonumber
    \preccurlyeq & \frac{C}{\hat{n}^2} \mSigma \coloneqq \mV_1.
\end{align}
\begin{align}
    \nonumber
    \E \mZ^\top \mZ = & \sum_{i=1}^{\hat{n}} \E \mS_i^\top\mS_i \\
    \nonumber
    = & \hat{n} \E \|\mS_i\|^2 \\
    \nonumber
    \leq & \frac{4}{\hat{n}} BC \coloneqq \mV_2 \quad\quad\text{by Equation \ref{eq: bound_S_L}}.
\end{align}
Define $ v = \max( \opnorm{\mV_1}, \opnorm{\mV_2} )$. It can be simplified as follows
\begin{align}
    \nonumber
     v = &  \max( \opnorm{ \frac{C}{\hat{n}}\mSigma }, \frac{4}{\hat{n}} BC ) \\
     \nonumber
     = & \frac{4}{\hat{n}} BC \quad\quad \text{because $B\geq \opnorm{\mSigma} $ as in Equation \ref{eq: B_tau_relation}}.
\end{align}
Define $d = \intdim(\diagmtx{\mV_1}{\mV_2})$, which can be simplified as
\begin{align}
    \nonumber
    d = & \intdim( \diagmtx{ \frac{C}{\hat{n}}\mSigma }{\frac{4}{\hat{n}} BC} ) \\
    \nonumber
    = & \frac{ \Tr( \frac{C}{\hat{n}}\mSigma ) + \frac{4}{\hat{n}} BC }{ \max( \opnorm{ \frac{C}{\hat{n}}\mSigma }, \frac{4}{\hat{n}} BC )  } \\
    \nonumber
    = & \frac{ \Tr( \frac{C}{\hat{n}}\mSigma ) + \frac{4}{\hat{n}} BC }{ \frac{4}{\hat{n}} BC } \\
    \nonumber
    = & \frac{ \Tr( \frac{C}{\hat{n}}\mSigma )}{ \frac{4}{\hat{n}} BC } + 1\\
    \nonumber
    \leq & q/4+1 \quad\quad \text{because $B\geq \tau$ as in Equation \ref{eq: B_tau_relation} and $\frac{\Tr(\mSigma)}{ \tau }=q$ }.
\end{align}
Applying Theorem 7.3.1 of \cite{tropp2015introduction}, we have that for any $t\geq \sqrt{v}+L/3$,
\begin{align}
    \nonumber
   \mathbb{P}\{ \|\mZ\| \geq t \}\leq & 4d \exp{ (\frac{-t^2/2}{v+Lt/3} )} \\
\label{eq: pzt_2}
   \leq & (q+4) \exp( \frac{-t^2/2}{\frac{4}{\hat{n}}BC+\frac{2\sqrt{BC}}{\hat{n}} t/3 }  ).
\end{align}
By assumption:
\begin{align}
    \nonumber
& n^{1-c}=\omega(B\log q)\\
\nonumber
\implies &  n^{1-c}=\omega( B\log (q+4)) \\
\nonumber
\implies & n^{1-c}=\omega( (4BC +\frac{2\sqrt{BC}}{3})\log (q+4)) \quad\quad\text{because $C=\Theta(1)$, and $B=\Omega(1)$ as in Equation \ref{eq: B_tau_relation} } \\
\nonumber
\implies & \frac{0.5 n^{1-c}}{(4BC +\frac{2\sqrt{BC}}{3})}=\omega( \log (q+4)) \\
\nonumber
\implies & (q+4)\exp\left( \frac{-0.5 n^{1-c}}{4BC +\frac{2\sqrt{BC}}{3}} \right)  =o(1).
\end{align}
Therefore, we set the value of $t$ to $\hat{n}^{-0.5c} = o(1)$ in Equation \ref{eq: pzt_2}. It is easy to verify that $\hat{n}^{-0.5c} \geq \sqrt{v}+L/3$. Substituting, we get: 
\begin{align}
    \nonumber
   \mathbb{P}\{ \opnorm{\mZ} \geq \hat{n}^{-0.5c} \}\leq & (q+4)\exp( \frac{-0.5n^{-c}}{ \frac{4}{\hat{n}} BC+\frac{2\sqrt{BC}}{\hat{n}} \hat{n}^{-0.5c}/3   } ) \\
   \nonumber
   \leq & (q+4)\exp( \frac{-0.5n^{-c}}{ \frac{4}{\hat{n}} BC+\frac{2\sqrt{BC}}{\hat{n}} /3   } ) \quad\quad\text{because $\hat{n}^{-0.5c}\leq 1$} \\
   \nonumber
   = &  (q+4)\exp( \frac{-0.5n^{1-c}}{ 4BC+ 2\sqrt{BC} /3   } ) \\
   \nonumber
   = & o(1). 
\end{align}
\end{proof}

Now, we are ready to show that Example \ref{eg: intrinsic_bounded} satisfies Definition \ref{def: delta_decomp}. We let $\gV$ be the entire representation space. Then, $\gV^\perp$ is the zero space ${\mathbf{0}}$. In this case, the conditions \isotropy{}, \smallin{}, and \dimini{} trivially hold. Thus, we only need to prove that \bounded{} and \conc{} hold. 

We let $\delta = n^{-0.1c}$ and $\gamma = 0$. First, note that $\delta^2 = n^{-0.2c} \geq \hat{n}^{-0.2c}$. Then, by Lemma \ref{lemma: bernstein_cov}, we obtain that $\opnorm{\hat{\mSigma} - \mSigma} \leq \hat{n}^{-0.5c} = o(\hat{n}^{-0.2c}) = o(\delta^2) = o(\gamma^2 + \delta^2 + \rho)$ with probability $1 - o(1)$. Similarly, we can show that $\opnorm{\tilde{\mSigma} - \mSigma} = o(\gamma^2 + \delta^2 + \rho)$ with probability $1 - o(1)$.

Next, since $\delta = n^{-0.1c} \geq \hat{n}^{-0.1c}$, applying Lemma \ref{lemma: bernstein_ry} gives us $\left| \frac{1}{\hat{n}} \sum_{i=1}^{\hat{n}}\hat{\vr}_i y_i - \E[\vr y] \right| \leq \hat{n}^{-0.5c} = o(\hat{n}^{-0.1c}) = o(\delta) = o(\gamma + \delta + \rho)$ with probability $1 - o(1)$. Similarly, the same conclusion can be shown for $\frac{1}{\tilde{n}} \tilde{\vr}_i y_i$.

Note that there are only four events above, so the probability that all of them occur remains $1 - o(1)$. To now, we have proved \conc{}.

Finally, regarding \bounded{}, $\opnorm{\mSigma}=\Theta(1)$ is directly given in the assumption. Keeping in mind that $\gV$ is the entire space, the conditions regarding covariance matrices are readily satisfied through the triangle inequality. For example: $\opnorm{\hat{\mSigma}}\leq \opnorm{\hat{\mSigma} -\mSigma}+\opnorm{\mSigma} = o(1)+\Theta(1)=O(1) $. The other two conditions are directly implied by the boundedness of each $y$.

\subsection{Example \ref{eg: spiked_cov}}\label{apdx: spiked_cov}

Originating from PCA \cite{johnstone2001distribution}, the spiked covariance model has been widely adopted in recent works to theoretically characterize key aspects across various topics \cite{ji2023power,nakada2023understanding,muthukumar2021classification,pezeshki2022multi,wu2024provable}. Furthermore, Example \ref{eg: spiked_cov} also subsumes the sparse coding model as a special case, which has its roots in computer vision \cite{olshausen1997sparse,foldiak2003sparse,olshausen2004sparse,yang2009linear,mairal2014sparse,papyan2017convolutional}, has been used to model language data \cite{arora2018linear}, and has been extensively employed in recent theoretical studies \cite{kalimeris2019sgd,allen2020towards,wen2021toward,zou2021understanding,shen2022data,xue2023features}. %Additionally, sub-Gaussian is itself a very general class of distributions, including any bounded random variables and Gaussian. 

In the following proof, we start with a simple case where the data are Gaussian. We then extend the result to sub-Gaussian data by replacing the technical lemmas for Gaussian data with appropriate alternatives.

% In the following proof, we prove that this example with Gaussian data satisfies the $(\delta, \hat{\gamma}, \tilde{\gamma})$-decomposability assumption. Then we extend this result to sub-Gaussian data by replacing the technical lemmas for Gaussian data with alternative ones. 

\subsubsection{Over-Parameterized Gaussian Data} \label{sec:gaussian_data}

Suppose that we have $\hat{\mR} \in \mathbb{R}^{d\times \hat{n}}, \tilde{\mR} \in \mathbb{R}^{d\times \tilde{n}}$ with $\hat{n} = \Theta(\tilde{n})$ and $d = \omega(\hat{n}^2)$ drawn from a high-dimensional $\mSigma$-Gaussian ensemble with zero mean, where 
\begin{align}
    \mSigma  = \begin{bmatrix}
        \mI_k  & \mathbf{0} \\
        \mathbf{0} & \frac{\sigma^2}{d-k}\mI_{d-k}
    \end{bmatrix} = 
    \underbrace{\begin{bmatrix}
        \mI_k=\mLambda'  & \mathbf{0} \\
        \mathbf{0} & \mathbf{0}
    \end{bmatrix}}_{\mSigma'} + 
    \underbrace{\begin{bmatrix}
        \mathbf{0}  & \mathbf{0} \\
        \mathbf{0} & \frac{\sigma^2}{d-k}\mI_{d-k}=\mLambda''
    \end{bmatrix} }_{\mSigma''}, \quad \text{with }\sigma^2 = O(\hat{n}), \hat{n} = \omega(k^2).  
\end{align}
Here the two data splits have comparable sizes, and the model is heavily over-parameterized. 
By splitting the matrix $\hat{\mR} = \begin{bmatrix}
    \hat{\mF} \\ \hat{\mA}
\end{bmatrix}$, where $\hat{\mF} \in \mathbb{R}^{k \times \hat{n}}$ corresponds to the $k$ principal features (which form the space $\gV$) and $\hat{\mA} \in \mathbb{R}^{(d-k) \times \hat{n}}$ corresponds to the rest (which form the space $\gV^\perp$), we can write the sample covariance matrix as
\[
    \hat{\mSigma} = \frac{1}{\hat{n}}\hat{\mR}\hat{\mR}^\top = \frac{1}{\hat{n}}
    \begin{bmatrix}
        \hat{\mF}\hat{\mF}^\top & \hat{\mF}\hat{\mA}^\top \\
        \hat{\mA}\hat{\mF}^\top & \hat{\mA}\hat{\mA}^\top
    \end{bmatrix}. 
\]

We note that $d - k = \omega(\hat{n}^2)$, and the corresponding labels have bounded mean and variance. The same decomposition applies to $\tilde{\mR}$. Note that here $\mU' = \begin{bmatrix}
    \mI_k \\ \mathbf{0}_{(d-k) \times k}
\end{bmatrix}$ and $\mU'' =  \begin{bmatrix}
    \mathbf{0}_{k \times (d-k)} \\ \mI_{d-k}
\end{bmatrix}$ allow us to define the projection matrices $\mU'\mU'^\top$ and $\mU''\mU''^\top$ on $\gV$ and $\gV^\perp$ respectively. 

In this section, we show that our assumptions hold in the above setting with $\delta = 0$ and $\hat{\gamma} = \sigma^2/\hat{n}, \tilde{\gamma} = \sigma^2/\tilde{n}$. 
%namely $\gamma = \min\{\sigma^2/\hat{n}, \sigma^2/\tilde{n}\} = \sigma^2/\max\{\hat{n}, \tilde{n}\}$. 
We only prove for $\hat{\mR}$ whenever the same proof can be easily applied to $\tilde{\mR}$. 

First, let us introduce the following Lemmas: 

\begin{lemma}[Restatement of Example 6.2 in \cite{wainwright2019high}] \label{lemma: eigen_val_bound}
Let $\mathbf{X} \in \mathbb{R}^{d \times n}$ be a random matrix with i.i.d. entries drawn from $\mathcal{N}(0, 1)$ (that is a $\mSigma$-Gaussian ensemble with $\mSigma = \mI_d$). Then with probability at least $1- 2e^{-n\delta^2/2}$ for some $\delta > 0$, the following inequality holds: 
\[
 \opnorm{\frac{1}{n}\mX\mX^T - \mI_d} \leq 2\left(\sqrt{\frac{d}{n}} + \delta \right) + \left( \sqrt{\frac{d}{n}} + \delta 
 \right)^2. 
\]
\end{lemma}

% Classical techniques can fall short of providing meaningful bounds for over-parameterized data. The following Lemma provides a result that captures full-rank matrices with a fast eigendecay, potentially helping quantify over-parameterized covariance matrices that satisfy additional structure. 

% \todoblue{DO NOT need any more. Delete? }\begin{lemma} (Restatement of Theorem 2.1 in \cite{bunea2015sample}) \label{lemma: eff_rank_bound}
% Let $\mathbf{X} \in \mathbb{R}^{d \times n}$ be a random matrix with i.i.d. entries drawn from a $\mSigma$-Gaussian ensemble where $\Tr(\mSigma)/\opnorm{\mSigma} \leq R$, and $R$ is a constant independent of the ambient dimension. Then with probability at least $1- 5/n$, the following inequality holds:
% \[
%  \opnorm{\frac{1}{n}\mX\mX^T - \mSigma} \leq 2CR\opnorm{\mSigma}\sqrt{\frac{\log(n)}{n}}
% \]
% for some constant $C$. 
% \end{lemma}


\begin{lemma}\label{lemma: op_norm_bound}
Consider two independently sampled Gaussian matrices where $\mathbf{A} \in \mathbb{R}^{d_1 \times n}$ has columns $\va_i \sim \mathcal{N}(0, \sigma_1^2\mI_{d_1})$ and $\mathbf{B} \in \mathbb{R}^{d_2 \times n}$ has columns $\vb_i \sim \mathcal{N}(0, \sigma_2^2\mI_{d_2})$ . Then for some $\frac{1}{d_1d_2} > \delta > 0$ and constant $C$, with probability at least $1- d_1d_2\delta$, we have
\[
    \frac{1}{n}\opnorm{\mA\mB^\top} \leq \frac{\sigma_1\sigma_2}{n}\sqrt{Cd_1d_2n\log(\frac{2}{\delta})}. 
\]
\end{lemma}
\begin{proof}
    Let $\mQ = \mA\mB^T$. Then each entry of $\mQ$ is an inner product $\mQ_{ij} = \va_i \cdot \vb_j$, where $\va_i \in \mathbb{R}^n$ is the $i$-th row of $\mA$ and $\vb_j \in \mathbb{R}^n$ is the $j$-th row of $\mB$. Since each entry of $\va_i$ is $\mathcal{N}(0, \sigma_1^2)$ and each entry of $\vb_j$ is $\mathcal{N}(0, \sigma_2^2)$, by Lemma 4 from \cite{shen2022data}, with probability at least $1- \delta$ (taking $\frac{1}{d_1d_2} > \delta > 0$), for some constant $C_{ij}$, 
    \[
        \mQ_{ij}^2 = (\va_i \cdot \vb_j)^2 \leq C_{ij}\sigma_1^2\sigma_2^2n\log(2/\delta'). 
    \]
    We define $C =  \max \left \{C_{ij}: 1 \leq i \leq d_1, 1 \leq j \leq d_2 \right\}$. Now we bound the operator norm with 
    \begin{align*}
        \frac{1}{n}\opnorm{\mA\mB^\top} \leq \frac{1}{n}\|\mA\mB^\top\|_F & = \frac{1}{n}\|\mQ\|_F \\
        & = \frac{1}{n} \sqrt{ \sum_{1 \leq i \leq d_1, 1 \leq j \leq d_2}{\mQ_{ij}^2}} \\
        & \leq \frac{1}{n} \sqrt{ \sum_{1 \leq i \leq d_1, 1 \leq j \leq d_2}{C_{ij}\sigma_1^2\sigma_2^2n\log(2/\delta)}} \\ 
        & \leq \frac{1}{n} \sqrt{Cd_1d_2\sigma_1^2\sigma_2^2n\log(2/\delta)} = \frac{\sigma_1\sigma_2}{n}\sqrt{Cd_1d_2n\log(2/\delta)}
    \end{align*}
    with probability at least $1-d_1d_2\delta$ since the inequality has to hold for each entry. 
\end{proof}

We now prove that the example satisfies the five aspects of the definition: 
\begin{enumerate}
    \item[1. ] \bounded{}:  \\ \\
    First, we have $\opnorm{\mSigma} = 1 = O(1)$ from its definition, and 
\begin{align} \label{eq:cov_difference}
    \opnorm{\hat{\mSigma} - \mSigma} \leq
    \left\|
    \begin{bmatrix}
        \frac{1}{\hat{n}}\hat{\mF}\hat{\mF}^\top - \mI_k & \mathbf{0} \\
        \mathbf{0} & \frac{1}{\hat{n}}\hat{\mA}\hat{\mA}^\top - \frac{\sigma^2}{d-k}\mI_{d-k} 
    \end{bmatrix}
    \right\|_{\text{op}} + 
    \frac{1}{\hat{n}}\left\|
    \begin{bmatrix}
        \mathbf{0} & \hat{\mF}\hat{\mA}^\top \\
        \hat{\mA}\hat{\mF}^\top & \mathbf{0}
    \end{bmatrix}
    \right\|_{\text{op}}. 
\end{align}
By Lemma \ref{lemma: eigen_val_bound}, we take $\delta_1 = \hat{n}^{-1/4}$ and have that with probability at least $1 - 2e^{-\hat{n}\delta_1^2/2} = 1 - 2e^{-\sqrt{\hat{n}}/2} = 1 - o(1)$,  
\begin{align*}
   \opnorm{\frac{1}{\hat{n}}\hat{\mF}\hat{\mF}^\top - \mI_k} \leq 2\sqrt{\frac{k}{\hat{n}}} + \frac{2}{\hat{n}^{1/4}} + \left( \sqrt{\frac{k}{\hat{n}}} + \frac{1}{\hat{n}^{1/4}} \right)^2 = o(1) \quad \text{since $\hat{n} \gg k$}. 
\end{align*} 
As $\hat{\mA} \in \mathbb{R}^{(d-k) \times \hat{n}}$ is sampled from $\frac{\sigma^2}{d-k}\mI_{d-k}$, $\frac{\sqrt{d-k}}{\sigma}\hat{\mA}$ is sampled from $\mI_{d-k}$. With this scaling, similarly, Lemma \ref{lemma: eigen_val_bound} implies that
\begin{align*}
    \left\|\frac{1}{\hat{n}}\left( \frac{\sqrt{d-k}}{\sigma}\hat{\mA} \right) \left(\frac{\sqrt{d-k}}{\sigma}\hat{\mA} \right)^\top - \mI_{d-k}\right\|_{\text{op}} & =
    \left\|\frac{d-k}{\hat{n}\sigma^2}\hat{\mA}\hat{\mA}^\top - \mI_{d-k}\right\|_{\text{op}} \\
    & \leq 2\sqrt{\frac{d-k}{\hat{n}}} + \frac{2}{\hat{n}^{1/4}} + \left( \sqrt{\frac{d-k}{\hat{n}}} + \frac{1}{\hat{n}^{1/4}} \right)^2
\end{align*}
\begin{align*}
    \iff \left\|\frac{1}{\hat{n}}\hat{\mA}\hat{\mA}^\top - \frac{\sigma^2}{d-k}\mI_{d-k}\right\|_{\text{op}} \leq \frac{\sigma^2}{d-k}\left[ 2\sqrt{\frac{d-k}{\hat{n}}} + \frac{2}{\hat{n}^{1/4}} + \left( \sqrt{\frac{d-k}{\hat{n}}} + \frac{1}{\hat{n}^{1/4}} \right)^2 \right] = O(1) \quad \text{as $\sigma^2 = O(\hat{n})$}. 
\end{align*}

We have bounded the first term on the right side of Eq. \ref{eq:cov_difference} and have that $\opnorm{\frac{1}{\sqrt{\hat{n}}}\hat{\mF}}$ and $\opnorm{\frac{1}{\sqrt{\hat{n}}}\hat{\mA}}$ are $O(1)$. It follows that 
\[
    \frac{1}{\hat{n}}\left\|
    \begin{bmatrix}
        \mathbf{0} & \hat{\mF}\hat{\mA}^\top \\
        \hat{\mA}\hat{\mF}^\top & \mathbf{0}
    \end{bmatrix}
    \right\|_{\text{op}} = \frac{1}{\hat{n}}\opnorm{\hat{\mF}\hat{\mA}^\top
    } = O(1) \Longrightarrow \opnorm{\hat{\mSigma} - \mSigma} = O(1). 
\]
Hence, $\opnorm{\hat{\mSigma}} = O(1)$ directly follows from $\opnorm{\mSigma} = O(1)$. 

Now we consider $\frac{1}{\hat{n}}\|\hat{\vy}\|^2 = \frac{1}{\hat{n}}\sum_{i=0}^{\hat{n}}\hat{\vy}_i^2$, where $\hat{\vy}_i$ represents the $i$-th entry of the vector. Since the label has bounded population variance $O(1)$, the i.i.d assumption implies
\[
    \Var(\frac{1}{\hat{n}}\sum_{i=0}^{\hat{n}}\hat{\vy}_i^2) = \frac{1}{\hat{n}^2}\sum_{i=0}^{\hat{n}}\Var(\hat{\vy}_i^2) = \frac{1}{\hat{n}^2}\sum_{i=0}^{\hat{n}}O(1) = O(\frac{1}{\hat{n}}). 
\]
Then by Chebyshev's inequality, for any $\epsilon > 0$ and some constant $C_1$, we let $z = \frac{1}{\hat{n}}\|\hat{\vy}\|^2$ for simplicity and then have
\[
P\left(|z- \mathbb{E}[z]| > \epsilon\right) \leq \frac{\Var(z)}{\epsilon^2} \leq \frac{C_1}{\hat{n} \epsilon^2}.  
\]
We take $\epsilon = \hat{n}^{-1/4}$. Then with probability at least $1-\frac{C_1}{\sqrt{\hat{n}}} = 1 - o(1)$, 
\[
    |\frac{1}{\hat{n}}\|\hat{\vy}\|^2 - \Var(\vy_i)| = o(1) \Longrightarrow \frac{1}{\hat{n}}\|\hat{\vy}\|^2 = O(1) \quad \text{since the variance of the label is bounded.}
\]
    \item[2.] \conc{}: \\ \\
    With $\mU' = \begin{bmatrix}
    \mI_k \\ \mathbf{0}_{(d-k) \times k}
\end{bmatrix}$ preserving only the first $k$ components, we have from above that with probability at least $1-o(1)$, 
\[
    \opnorm{\mU'^\top\hat{\mSigma}\mU' - \mLambda'} = \opnorm{\frac{1}{\hat{n}}\hat{\mF}\hat{\mF}^\top - \mI_k} = o(1). 
\] 
Now we consider
\[
    \| \frac{1}{\hat{n}} \mU'^\top \hat{\mR}\hat{\vy} -\E[ \mU'^\top\vr y]  \| = \| \frac{1}{\hat{n}} \hat{\mF}\hat{\vy} -\E[ \vf y]  \| ,   
\]
where $\vf = \mU'\vr$. We define a new random variable $\vz = \vf y$ and its sample mean $\hat{\mZ} = \frac{1}{\hat{n}} \hat{\mF}\hat{\vy} \in \mathbb{R}^k$. We first show that the variance of each entry of $\hat{\mZ}$ is of magnitude $\sim \frac{1}{\hat{n}}$: 
\[
    \Var(\hat{\mZ}_i) = \Var \left( \sum_{j = 1}^{\hat{n}} \frac{1}{\hat{n}} \hat{\mF}_{ij}\hat{\vy}_j \right) = \frac{1}{\hat{n}^2} \Var \left( \sum_{j = 1}^{\hat{n}} \hat{\mF}_{ij}\hat{\vy}_j \right) \quad \forall i = 1, \cdots, k. 
\]
For each term in the summation, 
\begin{align*}
    \Var(\hat{\mF}_{ij}\hat{\vy}_j) = \mathbb{E}[(\hat{\mF}_{ij}\hat{\vy}_j)^2] - \mathbb{E}[\hat{\mF}_{ij}\hat{\vy}_j]^2 = O(1) 
\end{align*}
since $\hat{\mF}_{ij}$ and $\hat{\vy}_j$ are both bounded. By the i.i.d assumption, 
\[
    \Var(\hat{\mZ}_i) = \frac{1}{\hat{n}^2} \sum_{j = 1}^{\hat{n}} O(1) = O\left(\frac{1}{\hat{n}}\right). 
\]

By Chebyshev's inequality, for any \(\epsilon > 0\) and some constant $C_2$, 
\[
P\left(\|\hat{\mZ}_i- \mathbb{E}[\vz_i]\| > \epsilon\right) \leq \frac{\Var(\hat{\mZ}_i)}{\epsilon^2} \leq \frac{C_2}{\hat{n} \epsilon^2} 
\]
\[
P\left(\|\hat{\mZ}_i- \mathbb{E}[\vz_i]\| > \epsilon \quad \forall i = 1, \cdots, k\right) \leq \frac{kC_2}{\hat{n} \epsilon^2} 
\]

Similarly, by choosing \(\epsilon = \hat{n}^{-1/4}\), the probability of large deviation decays rapidly as:
\[
P\left(\|\hat{\mZ}_i- \mathbb{E}[\vz_i]\| > \frac{1}{\hat{n}^{1/4}} \quad \forall i = 1, \cdots, k \right) \leq \frac{kC_2}{\sqrt{\hat{n}}} = o(1) \quad \text{since } \hat{n} = \omega(k^2).
\]
This statement implies that with probability at least $1 - o(1)$, 
 \[
    \|\hat{\mZ} - \E[\vz]\| = \| \frac{1}{\hat{n}} \mU'^\top \hat{\mR}\hat{\vy} -\E[ \mU'^\top\vr y]  \| \leq \sqrt{\frac{k}{\sqrt{\hat{n}}}} = o(1) = o(\gamma+\delta+\lambda_{\min}( \mLambda' )) 
 \]
 as we sum up the $k$ terms. This shows that our setting satisfies the second part of the definition. 
    \item[3.] \isotropy{}: \\ \\
    We define $\mZ = \frac{\sqrt{d-k}}{\sigma}\hat{\mA} \in \mathbb{R}^{(d-k) \times \hat{n} }$, which has standard normal entries. With the scaling, we plug in $\mU''$, $\hat{\gamma} = \sigma^2/\hat{n}$ and have
\begin{align} \label{eq: kernel_wise_entropy_op}
    \opnorm{\frac{1}{\hat{n}} \hat{\mR}^\top\mU''\mU''^\top \hat{\mR} -\hat{\gamma} \mI } = \opnorm{\frac{1}{\hat{n}} \hat{\mA}^\top\hat{\mA} -\frac{\sigma^2}{\hat{n}} \mI } = \frac{\sigma^2}{\hat{n}}\opnorm{\frac{1}{d-k} \mZ^\top\mZ -\mI }. 
\end{align}
Now we apply Lemma \ref{lemma: eigen_val_bound} and have that with probability at least $1 - 2e^{-\hat{n}\delta_2^2/2}$ for some $\delta_2 > 0$, 
\[
    \frac{\sigma^2}{\hat{n}}\opnorm{\frac{1}{d-k} \mZ^\top\mZ -\mI } \leq \frac{\sigma^2}{\hat{n}} \left[ 2\left(\sqrt{\frac{\hat{n}}{d-k}} + \delta_2 \right) + \left(\sqrt{\frac{\hat{n}}{d-k}} + \delta_2 
    \right)^2 \right]. 
\]
The rest follows similarly by taking $\delta_2 = \hat{n}^{-1/4}$. 
    \item[4.] \smallin{}: \\ \\ 
    By $\mU'' =  \begin{bmatrix}
    \mathbf{0}_{k \times (d-k)} \\ \mI_{d-k}
\end{bmatrix}$ and Lemma \ref{lemma: op_norm_bound} with $\hat{\mA}^\top \in \mathbb{R}^{\hat{n} \times (d-k)}$ and $\tilde{\mA}^\top \in \mathbb{R}^{\tilde{n} \times (d-k)}$, each having $\mathcal{N}(0, \frac{\sigma^2}{d-k})$ entries, the target expression becomes 
\begin{align}
    \opnorm{\frac{1}{\sqrt{\hat{n}}} \hat{\mR}^\top \mU''\mU''^\top \frac{1}{\sqrt{\tilde{n}}}\tilde{\mR}} & =  \frac{1}{\sqrt{\hat{n}\tilde{n}}} \opnorm{\hat{\mA}^\top\tilde{\mA}} \label{eq: cross_sample_op}\\ 
    & \leq \frac{1}{\sqrt{\hat{n}\tilde{n}}} \sqrt{C_4\hat{n}\tilde{n}\frac{\sigma^4}{(d-k)^2}(d-k)\log{(2/\delta_3)}}  \notag \\
    & = \sqrt{C_4\frac{\sigma^4}{d-k}\log{(2/\delta_3)}}  \notag \\
    & = \sigma^2\sqrt{C_4\log{(2/\delta_3)}}\sqrt{\frac{1}{d-k}} \notag
\end{align}
for some constant $C_4$ and with probability at least $1-\hat{n}\tilde{n}\delta_3$ for some $0 < \delta_3 < \frac{1}{\hat{n}\tilde{n}}$. We choose some $\delta_3 = o(\frac{1}{\hat{n}\tilde{n}})$ in this range and then have that with probability at least $1-o(1)$, the previous bound can be expressed as: 
\[
    \sigma^2\sqrt{C_4\log{(2/\delta_3)}}\sqrt{\frac{1}{d-k}} = \Theta \left(\sigma^2\sqrt{C_4\frac{\log(\hat{n}\tilde{n})}{d-k}} \right) = o(\frac{\sigma^2}{\max \{\hat{n}, \tilde{n}\}}) = o(\gamma+\delta)
\]
since $d-k = \omega(\hat{n}) = \omega(\tilde{n})$. 
    \item[5.] \dimini{}: \\ \\
    By definition, it is trivial to see that
\[
    \lambda_{max}(\mLambda'') = \frac{\sigma^2}{d-k} = o(\frac{\sigma^2}{\max \{\hat{n}, \tilde{n}\}}) =o(\gamma+\delta)
\]
since $d-k = \omega(\hat{n}) = \omega(\tilde{n})$. 
\end{enumerate}

\subsubsection{Further Relaxation to Sub-Gaussian Data}

%Now, we relax the distributional assumption to allow sub-Gaussian data with sub-Gaussian parameter $= \Theta(\sigma)$, where $\sigma^2$ represents the variance. We have the otherwise same setting. 
Now, we consider the more general sub-Gaussian setting outlined in Example \ref{eg: spiked_cov}. The population covariance is: 
\[
    \mSigma  = \begin{bmatrix}
        \mI_k  & \mathbf{0} \\
        \mathbf{0} & \frac{\sigma^2}{d-k}\mI_{d-k}
    \end{bmatrix}, 
\]
where the top left block has a corresponding sub-Gaussian parameter of $\Theta(1)$ and the rest has a parameter of $\Theta(\frac{\sigma^2}{d-k})$. 

We adopt the following definitions from Chapter 2 of \cite{vershynin2018high} for reference. 

\begin{definition} \label{def:sub_gaussian}
    A zero-mean random variable $X$ is sub-Gaussian if there is a positive parameter $K_g$ such that
    \[
        \mathbb{E}[e^{X^2/K_g^2}] \leq 2. 
    \]   
\end{definition}

\begin{definition} \label{def:sub_exponentual}
    A zero-mean random variable $X$ is sub-exponential if there is a positive parameter $K_e$ such that
    \[
        \mathbb{E}[e^{|X|/K_e}] \leq 2. 
    \]   
\end{definition}

We can also define the following norms that give the sub-Gaussian or sub-exponential parameter: 
\[
    \|X\|_{\psi_2} = \inf \{ t > 0: \mathbb{E}[e^{X^2/t^2}] \leq 2\} = K_g
\]
\[
    \|X\|_{\psi_1} = \inf \{ t > 0: \mathbb{E}[e^{|X|/t}] \leq 2\} = K_e
\]

\begin{remark}
    There are many different characterizations for these two definitions, each with a different sub-Gaussian/sub-exponential parameter. A detailed summary can be found in Chapter 2 of \cite{vershynin2018high}. Notably, these parameters differ from each other only by at most a constant factor. 
\end{remark}

\begin{lemma} (Extension of Lemma 4 \cite{shen2022data} to sub-Gaussian) \label{lemma:sub_gauss_dot_product_bound}
    Consider high-dimensional independent sub-Gaussian vectors $\vz_1$, $\vz_2 \in \mathbb{R}^d$, whose i.i.d. entries have variances $\sigma_1^2$, $\sigma_2^2$ and sub-Gaussian parameters $\Theta(\sigma_1)$, $\Theta(\sigma_2)$ respectively. Then for $\delta > 0$ such that $\sqrt{\log(2/\delta)} > \sqrt{cd}$ for some constant $c$, there exists a constant $C$ such that with probability at least $1-\delta$, 
    \[
        |\vz_1 \cdot \vz_2| \leq C\sigma_1\sigma_2\sqrt{d\log(2/\delta)}. 
    \]
\end{lemma}

\begin{proof}
   We consider the product $\vz_1 \cdot \vz_2 = \sum_{i=1}^d \vz_{1i}\vz_{2i} = \sum_{i=1}^d a_i$, where we define $a_i$ for simplicity. It is a well-known result that the product of two sub-Gaussian random variables is sub-exponential. More precisely, 
    \[
        \|a_i\|_{\psi_1} \leq \|\vz_{1i}\|_{\psi_2}\|\vz_{2i}\|_{\psi_2} = C\sigma_1\sigma_2. 
    \]
    By Bernstein's inequality for sub-exponential functions (see Theorem 3.8.1 \cite{vershynin2018high}), this summation can be bounded as: for some constant $c > 0$, 
    \begin{align*}
        P\left( \left| \sum_{i=1}^d a_i \right| \geq t\right) & \leq 2 \exp \left[ -c \min  \left\{ \frac{t^2}{\sum_{i=1}^d \|a_i\|_{\psi_1}^2}, \frac{t}{ \max_i \|a_i\|_{\psi_1}}\right\}\right] \\
        & \leq  2 \exp \left[ -c \min  \left\{ \frac{t^2}{dC^2\sigma_1^2\sigma_2^2}, \frac{t}{ C\sigma_1\sigma_2}\right\}\right]
    \end{align*}
    Let $t = \frac{C}{\sqrt{c}}\sigma_1\sigma_2\sqrt{d\log(2/\delta)}$ for some $\delta$ that satisfies the condition $\sqrt{\log(2/\delta)} > \sqrt{cd}$ (e.g. $\delta = 1/d^2$). The probability statement becomes: 
    \begin{align*}
        P\left( \left| \sum_{i=1}^d a_i \right| \geq \frac{C}{\sqrt{c}}\sigma_1\sigma_2\sqrt{d\log(2/\delta)} \right) & \leq 2 \exp \left[ -c \min  \left\{ \frac{\log(2/\delta)}{c} ,  \sqrt{\frac{d\log(2/\delta)}{c}} \right\}\right] \\
        & = 2 \exp \left[ -\min  \left\{ \log(2/\delta) ,  \sqrt{cd\log(2/\delta)} \right\}\right]. 
    \end{align*}
    Since our choice of $\delta$ ensures that the first quantity is smaller,  
    \[
        P\left( \left| \sum_{i=1}^d a_i \right| \geq \frac{C}{\sqrt{c}}\sigma_1\sigma_2\sqrt{d\log(2/\delta)} \right) \leq \delta
    \]
    In other words, letting $C' = C/\sqrt{c}$, we have that with probability at least $1-\delta$, 
    \[
        |\vz_1 \cdot \vz_2| \leq C'\sigma_1\sigma_2\sqrt{d\log(2/\delta)}.
    \]
\end{proof}

Now we are ready to show that our assumptions capture the setting in Section \ref{sec:gaussian_data} but with sub-Gaussian data. That is, we now allow the data to have possibly even lighter tail than that of Gaussian. The proof can be easily replicated, as Chebyshev's inequality still applies here and Lemmas \ref{lemma: eigen_val_bound}, \ref{lemma: op_norm_bound} find the following ``sub-Gaussian" alternatives, namely Lemmas \ref{lemma:sub_gauss_cov_bound}, \ref{lemma:sub_gauss_op_norm_bound}: 

\begin{lemma} (Restatement of Theorem 6.5 in \cite{wainwright2019high})\label{lemma:sub_gauss_cov_bound}
Let $\mathbf{X} \in \mathbb{R}^{d \times n}$ be a random sub-Gaussian matrix with parameter $K_g$ and population covariance $\mI_d$. Then for all $\delta \geq 0$, there are universal constants $C_1, C_2, C_3$ such that 
\[
 \opnorm{\frac{1}{n}\mX\mX^T - \mI_d} \leq K_g^2\left[ C_1\left(\sqrt{\frac{d}{n}}+\frac{d}{n}\right) + \delta\right]
\]
with probability at least $1 - C_2e^{-C_3n\min\{\delta, \delta^2\}}$. 
\end{lemma}

\begin{lemma} \label{lemma:sub_gauss_op_norm_bound}
    Consider two independently sampled row-wise sub-Gaussian matrices $\mathbf{A} \in \mathbb{R}^{d_1 \times n}$, $\mathbf{B} \in \mathbb{R}^{d_2 \times n}$ that have i.i.d. entries with variances $\sigma_1^2$, $\sigma_2^2$ respectively. Then for some $\frac{1}{d_1d_2} > \delta > 0$ and constant $C$, with probability at least $1- d_1d_2\delta$, we have
\[
    \frac{1}{n}\opnorm{\mA\mB^\top} \leq \frac{\sigma_1\sigma_2}{n}\sqrt{Cd_1d_2n\log(\frac{2}{\delta})}. 
\]
\end{lemma}

\begin{proof}
    The proof is the same as Lemma \ref{lemma: op_norm_bound} except that we now use Lemma \ref{lemma:sub_gauss_dot_product_bound} to bound the squared value of each entry in the Frobenius norm. 
\end{proof}

With these alternative extended results, the proof in Section \ref{sec:gaussian_data} immediately generalizes to sub-Gaussian data. This extension potentially allows us to accommodate more realistic scenario and enhances the theoretical robustness of our assumptions. Sub-Gaussian distributions capture a wider class of data behaviors; for instance, the fact that bounded random variables are sub-Gaussian makes the theory more applicable to many real-world datasets, which naturally exhibit sub-Gaussian characteristics. In the following section, we show a general result that even more examples can be constructed. 

\subsection{Proof of Theorem \ref{thm: construct_new}}
% \todoblue{}
% \begin{theorem}[constructing new examples by concatenating high dimensional Gaussian] Given a representation function
% $h$ whose representations $h(\vx)\in\sR^d$ are $(\delta, 0, 0)$-decomposable w.r.t. $\sR^d$, we now construct new representations $\alpha(\vx)\in\sR^{d+m}$ as follows:
% \begin{align}
%     \nonumber
%    \alpha(\vx) = \mM h(\vx) + \mM^\perp \xi(\vx),
% \end{align}
% where both $\mM\in\sR^{(d+m)\times d}$ and $\mM^\perp \in \sR^{(d+m)\times m}$ both have orthonormal columns, and their column spaces are orthogonal to each others. If the marginal distribution of $\xi(\vx) \in \sR^m$ is Gaussian $\gN(0, \frac{\sigma^2}{m}\mI)$, and  assuming $\tilde{n}=\Theta(\hat{n})$, $m=\omega(\hat{n}^2)$, and $\sigma^2=O(\hat{n})$, then $\alpha$'s representations are $(\delta, \frac{\sigma^2}{\hat{n}}, \frac{\sigma^2}{\tilde{n}})$-decomposable.
% \end{theorem}

    The intuition behind this theorem is that adding high-dimensional sub-Gaussian entries to the given representation preserves decomposbility while slightly modifying the parameters. Due to the orthogonality of $\mM$ and $\mM^\perp$, we let $\mU = \begin{bmatrix}
        \mM \quad \mM^\perp
    \end{bmatrix}$ and then $\alpha(\vx) = \mU\begin{bmatrix}
        h(\vx)\\
        \xi(\vx)
    \end{bmatrix}$; naturally, the column space of $\mM$ can be regarded as the subspace $\gV$, and the column space of $\mM^\perp$ is $\gV^\perp$. Given that $h(\vx)$'s representations are $(\delta, 0, 0)$-decomposable w.r.t. $\sR^d$, we now prove that the new representations are $(\delta, \frac{\sigma^2}{\hat{n}}, \frac{\sigma^2}{\tilde{n}})$-decomposable. Again we only present the proof for one data split whenever it can be replicated for the other.  

    For notation, we let $\gamma = \sigma^2/\max\{\hat{n}, \tilde{n}\}$. 
    
    \begin{enumerate}
        \item \textbf{Boundedness: }  $\frac{1}{\hat{n}}\sum_{i=1}^{\hat{n}} \hat{y}_i^2 = O(1)$ follows from the previous proof using Chebyshev's inequality. For the population covariance, 
        \begin{align}
            \opnorm{\mSigma(\alpha)} = \opnorm{\E_{\gD_{\vx}}[\alpha(\vx)\alpha(\vx)^\top]} &= \left\|\E_{\gD_{\vx}}\begin{bmatrix}
               h(\vx)h(\vx)^\top & h(\vx)\xi(\vx)^\top\\
                \xi(\vx)h(\vx)^\top & \xi(\vx)\xi(\vx)^\top
            \end{bmatrix}\right\|_{op}  \notag \\ & \leq
            \left\|\E_{\gD_{\vx}}\begin{bmatrix}
               h(\vx)h(\vx)^\top & \mathbf{0} \\
                \mathbf{0} & \xi(\vx)\xi(\vx)^\top
            \end{bmatrix}\right\|_{op} + \left\|\E_{\gD_{\vx}}\begin{bmatrix}
               \mathbf{0} & h(\vx)\xi(\vx)^\top\\
                \xi(\vx)h(\vx)^\top & \mathbf{0} 
            \end{bmatrix}\right\|_{op} \label{eq:general_cov}
        \end{align}
        We have that $\opnorm{\E_{\gD_{\vx}}[h(\vx)h(\vx)^\top]} = \opnorm{\mSigma(h)} = O(1)$ by the $(\delta, 0, 0)$-decomposibility assumption on $h$'s representations. From the proof for sub-Gaussian data in Section \ref{sec:gaussian_data}, $\opnorm{\E_{\gD_{\vx}}[\xi(\vx)\xi(\vx)^\top]} = \opnorm{\mSigma(\xi)} = O(1)$. These bound the first term on the RHS of Equation \ref{eq:general_cov}. 

        By the definition of operator norm, 
        \begin{align} \label{eq: cross_term_op}
            \opnorm{\E_{\gD_{\vx}}[h(\vx)\xi(\vx)^\top]} = \sup_{\|\vu\|=1} \sup_{\|\vv\|=1} \vu^T\E_{\gD_{\vx}}[ h(\vx) \xi(\vx)^\top] \vv =  \sup_{\|\vu\|=1} \sup_{\|\vv\|=1} \E_{\gD_{\vx}}[ (\vu^Th(\vx)) (\vv^T\xi(\vx))]. 
        \end{align}
        By Cauchy-Schwartz inequality, we can bound this expectation as: 
        \[
            \E_{\gD_{\vx}}[ (\vu^Th(\vx)) (\vv^T\xi(\vx))] \leq \sqrt{\E_{\gD_{\vx}}[ (\vu^Th(\vx))^2]} \sqrt{\E_{\gD_{\vx}}[ (\vv^T\xi(\vx))^2]}, \text{ where}
        \]
        \[
            \E_{\gD_{\vx}}[ (\vu^Th(\vx))^2] = \E_{\gD_{\vx}}[ \vu^Th(\vx)h(\vx)^\top\vu]  = \vu^T\E_{\gD_{\vx}}[ h(\vx)h(\vx)^\top] \vu \leq \|\vu\|^2\opnorm{\mSigma(h)} = O(1),
        \]
        \[
            \E_{\gD_{\vx}}[ (\vv^T\xi(\vx))^2] = \E_{\gD_{\vx}}[ \vv^T\xi(\vx)\xi(\vx)^\top\vv]  = \vv^T\E_{\gD_{\vx}}[\xi(\vx)\xi(\vx)^\top] \vv \leq \|\vv\|^2\opnorm{\mSigma(\xi)} = O(1). 
        \]
        Combing these results, we have that Equation \ref{eq: cross_term_op} $= \opnorm{\E_{\gD_{\vx}}[h(\vx)\xi(\vx)^\top]} = O(1)$, bounding the second term in Equation \ref{eq:general_cov}. Hence, $\opnorm{\mSigma(\alpha)} = O(1)$. 
        
        Simiarly, we can prove for the empirical covariance: 
        \begin{align*}
            \opnorm{\hat{\mSigma}(\alpha)} = \opnorm{\frac{1}{\hat{n}} \sum_{i=1}^{\hat{n}}\alpha(\hat{\vx}_i)\alpha(\hat{\vx}_i)^\top } & = \left\|\frac{1}{\hat{n}}\begin{bmatrix}
\sum_{i=1}^{\hat{n}}h(\hat{\vx}_i)h(\hat{\vx}_i)^\top & \sum_{i=1}^{\hat{n}}h(\hat{\vx}_i)\xi(\hat{\vx}_i)^\top \\ \sum_{i=1}^{\hat{n}}\xi(\hat{\vx}_i)h(\hat{\vx}_i)^\top & \sum_{i=1}^{\hat{n}}\xi(\hat{\vx}_i)\xi(\hat{\vx}_i)^\top
            \end{bmatrix}\right\|_{\text{op}} \\
            & = \left\|\frac{1}{\hat{n}}\begin{bmatrix}
\hat{\mH}\hat{\mH}^\top & \hat{\mH}\hat{\mathbf{\Xi}}^\top \\ \hat{\mathbf{\Xi}}\hat{\mH}^\top & \hat{\mathbf{\Xi}}\hat{\mathbf{\Xi}}^\top
            \end{bmatrix}\right\|_{\text{op}}, 
        \end{align*}
where the $i$-th column of $\hat{\mathbf{\Xi}}$ is $\xi(\hat{\vx}_i)$ and the $i$-th column of $\mH$ is $h(\hat{\vx}_i)$. 

The rest is straightforward: the assumption on $h$ and the existing proof for sub-Gaussian data imply $\opnorm{\frac{1}{\hat{n}}\hat{\mH}\hat{\mH}^\top} = O(1)$ and $\opnorm{\frac{1}{\hat{n}}\hat{\mathbf{\Xi}}\hat{\mathbf{\Xi}}^\top} = O(1)$. Hence, $\opnorm{\frac{1}{\sqrt{\hat{n}}} \hat{\mH}}$ and $\opnorm{\frac{1}{\sqrt{\hat{n}}} \hat{\mathbf{\Xi}}}$ are $O(1)$, and we have $\opnorm{\frac{1}{\hat{n}} \hat{\mH}\hat{\mathbf{\Xi}}^\top}$ is also O(1). These together bound the empirical covariance. 

        \item \textbf{Concentration on $\gV$}: Since $\gV$ corresponds to the representation space of $h(\vx)$, this condition is automatically satisfied by the $(\delta, 0, 0)$-decomposibility assumption on $h$. 
        
        \item \textbf{Kernel-wise $\delta$-isotropy on $\gV^\perp$}: In this setting, since $\gV^\perp$ corresponds to the column space of $\mM^\perp$ (the high-dimensional sub-Gaussian part), we have
        \[
            \opnorm{\frac{1}{\hat{n}} \hat{\mK}( \mPi_{\gV^\perp}\alpha )\! -\!\frac{\sigma^2}{\hat{n}} \mI }\! = \opnorm{\frac{1}{\hat{n}} \hat{\mK}(\xi)\! -\!\frac{\sigma^2}{\hat{n}} \mI }\!
        \]
        By definition of the kernel matrix, $\hat{\mK}(\xi) = [ \xi(\hat{\vx}_i)^\top \xi(\hat{\vx}_j) ]_{1\leq i,j\leq \hat{n}} = \hat{\mathbf{\Xi}}^\top\hat{\mathbf{\Xi}}$ with $\hat{\mathbf{\Xi}}$ defined above. Then the equation is essentially in the same form of Equation \ref{eq: kernel_wise_entropy_op}, so the previous proof applies here. 
        
        
        \item \textbf{Small cross-sample inner product on $\gV^\perp$}: Similar to 3, we have
        \[
            \opnorm{ \frac{1}{\sqrt{\hat{n}\tilde{n}}}[ (\mPi_{\gV^\perp}\alpha(\hat{\vx}_i))^\top \mPi_{\gV^\perp}\alpha(\tilde{\vx}_j)  ]_{1\leq i\leq\hat{n}, 1\leq j \leq \tilde{n}} }\! = \opnorm{ \frac{1}{\sqrt{\hat{n}\tilde{n}}}[\xi(\hat{\vx}_i)^\top \xi(\tilde{\vx}_j)  ]_{1\leq i\leq\hat{n}, 1\leq j \leq \tilde{n}} }\! =  \frac{1}{\sqrt{\hat{n}\tilde{n}}}\opnorm{ \hat{\mathbf{\Xi}}^T\tilde{\mathbf{\Xi}}}, 
        \]
        where $\tilde{\mathbf{\Xi}}$ is defined in the same manner. Then the proof after Equation \ref{eq: cross_sample_op} for sub-Gaussian data applies. 
        \item \textbf{Diminishing population covariance on $\gV^\perp$}: This refers covariance matrix of the sub-Gaussian part, and we simply have: 
        \[
            \opnorm{ \mSigma(\mPi_{\gV^\perp}h) }= \opnorm{ \mSigma(\xi) } = \opnorm{\E_{\gD_{\vx}}[\xi(\vx)\xi(\vx)^\top]} = \frac{\sigma^2}{m} = o(\delta + \gamma) \quad \text{ as $m = \omega(\hat{n}) = \omega(\tilde{n})$}
        \]
    \end{enumerate}

\section{Additional Experimental Details}\label{apdx: exp}

\subsection{Training details}\label{apdx: training_details}

% (btw it's worth reviewing empirical papers on W2SG to explore whether there are practical applications of using weak LLMs to teach strong LLMs. In the original OpenAI paper, this was presented primarily as a proof of conceptan analogy to humans guiding superintelligencerather than as a practical application. It is important to check if any follow-up works consider it as something already relevant to real-world applications. )

\subsubsection{Molecular prediction.}

Our experiment is built on the GitHub codebase provided by \cite{fabian2020molecular}. The strong model, MolBERT, can be downloaded using the link provided on their GitHub repository. For the weak models, we train small transformers using their pipeline with a batch size of 256. For finetuning, we use SGD to train a linear model on representations with the following settings: batch size = 1024, learning rate = 0.001, weight decay = 0.1, and epochs = 2000 when using representations from the strong model; and batch size = 1024, learning rate = 0.01, weight decay = 0, and epochs = 2000 when using representations from the weak models.

\subsubsection{NLP tasks with Embedding Models.}


We use \texttt{nvidia/NV-Embed-v2}, ranked first on the leaderboard of the Massive Text Embedding Benchmark (MTEB) \cite{muennighoff2022mteb}, as the strong model. We consider the following 22 embedding models as the weak model: 
\begin{tabbing}
\texttt{avsolatorio/GIST-Embedding-v0} \\
\texttt{Alibaba-NLP/gte-base-en-v1.5} \\
\texttt{jxm/cde-small-v1} \\
\texttt{thenlper/gte-base} \\
\texttt{infgrad/stella-base-en-v2} \\
\texttt{BAAI/bge-base-en-v1.5} \\
\texttt{thenlper/gte-small} \\
\texttt{intfloat/e5-base-v2} \\
\texttt{abhinand/MedEmbed-small-v0.1} \\
\texttt{nomic-ai/nomic-embed-text-v1} \\
\texttt{sentence-transformers/facebook-dpr-question\_encoder-single-nq-base} \\
\texttt{sentence-transformers/paraphrase-MiniLM-L3-v2} \\
\texttt{sentence-transformers/average\_word\_embeddings\_glove.840B.300d} \\
\texttt{sentence-transformers/roberta-base-nli-mean-tokens} \\
\texttt{sentence-transformers/all-mpnet-base-v1} \\
\texttt{sentence-transformers/bert-base-wikipedia-sections-mean-tokens} \\
\texttt{sentence-transformers/sentence-t5-base} \\
\texttt{Snowflake/snowflake-arctic-embed-s} \\
\texttt{TaylorAI/gte-tiny} \\
\texttt{jinaai/jina-embeddings-v2-small-en} \\
\texttt{sentence-transformers/gtr-t5-base} \\
\texttt{dumyy/sft-bge-small} \\
\end{tabbing}

% \texttt{avsolatorio/GIST-Embedding-v0}, \texttt{Alibaba-NLP/gte-base-en-v1.5}, 
% \texttt{jxm/cde-small-v1}, \texttt{thenlper/gte-base}, \texttt{infgrad/stella-base-en-v2}, \texttt{BAAI/bge-base-en-v1.5}, 
% \texttt{thenlper/gte-small}, \texttt{intfloat/e5-base-v2}, \texttt{abhinand/MedEmbed-small-v0.1}, \texttt{nomic-ai/nomic-embed-text-v1}, 
% \texttt{sentence-transformers/facebook-dpr-question\_encoder-single-nq-base}, 
% \texttt{sentence-transformers/paraphrase-MiniLM-L3-v2}, \texttt{sentence-transformers/average\_word\_embeddings\_glove.840B.300d}, 
% \texttt{sentence-transformers/roberta-base-nli-mean-tokens}, \texttt{sentence-transformers/all-mpnet-base-v1}, 
% \texttt{sentence-transformers/bert-base-wikipedia-sections-mean-tokens}, 
% \texttt{sentence-transformers/sentence-t5-base}, \texttt{Snowflake/snowflake-arctic-embed-s}, \texttt{TaylorAI/gte-tiny}, 
% \texttt{jinaai/jina-embeddings-v2-small-en}, \texttt{sentence-transformers/gtr-t5-base}, \texttt{dumyy/sft-bge-small}. 

During fine-tuning, we train a linear classifier on representations using the Adam optimizer \cite{kingma2014adam} with the following settings: batch size = 200, learning rate = 0.01, weight decay = 0.00001, and epochs = 200.


\subsubsection{NLP tasks with End-to-end finetuend LLMs.}

We largely reuse the GitHub codebase provided by \cite{burns2023weak}. We use \texttt{Qwen/Qwen-7B} as the strong model. We consider the following 28 LLMs as the weak model: 
\begin{tabbing}
\texttt{bigscience/bloom-560m} \\
\texttt{bigscience/bloomz-560m} \\
\texttt{bigscience/mt0-base} \\
\texttt{baidu/ernie-code-560m} \\
\texttt{bigscience/mt0-small} \\
\texttt{google/umt5-small} \\
\texttt{google/umt5-base} \\
\texttt{google/mt5-base} \\
\texttt{facebook/xglm-564M} \\
\texttt{MBZUAI/LaMini-T5-61M} \\
\texttt{MBZUAI/LaMini-Flan-T5-77M} \\
\texttt{MBZUAI/LaMini-GPT-124M} \\
\texttt{MBZUAI/LaMini-Neo-125M} \\
\texttt{MBZUAI/LaMini-T5-223M} \\
\texttt{apple/OpenELM-270M} \\
\texttt{apple/OpenELM-450M} \\
\texttt{EleutherAI/pythia-160m} \\
\texttt{MBZUAI/LaMini-Flan-T5-248M} \\
\texttt{MBZUAI/LaMini-GPT-774M} \\
\texttt{cerebras/Cerebras-GPT-111M} \\
\texttt{google-t5/t5-small} \\
\texttt{facebook/opt-125m} \\
\texttt{Qwen/Qwen2.5-0.5B} \\
\texttt{distilbert/distilgpt2} \\
\texttt{EleutherAI/gpt-neo-125m} \\
\texttt{gpt2} \\
\texttt{google/mt5-small} \\
\texttt{EleutherAI/pythia-70m}
\end{tabbing}
We finetune all the models using the pipeline provided in the codebase, which employs the Adam optimizer with a batch size of 32 and trains for a single epoch. The learning rate is set to 5e-5 for weak models and 1e-5 for the strong model, following the default configuration in the codebase, which applies smaller learning rates for larger models.

\subsection{Details and discussions on hyperparameters}\label{apdx: hyperparam}


For each model, computing $\mP$ introduces two hyperparameters, $\alpha$ and $\beta$. If every model is assigned unique hyperparameters, the total number of hyperparameters would be twice the number of models. To simplify this, we let all weak models share the same two hyperparameters, $\alpha_\w$ and $\beta_\w$. For the strong model (only one in each setting), it is treated separately with its own hyperparameters, $\alpha_\s$ and $\beta_\s$. Thus, we only have four parameters in total. 

In Exp. \RC{1}, we set $\alpha_\w = \alpha_\s = 0.1$ and $\beta_\w = \beta_\s = 0.1$ for all datasets. In Exp. \RC{2}, we set $\alpha_\w = 0.001$, $\alpha_\s = 0.05$, $\lambda_\w = 0.0001$, and $\lambda_\s = 0.01$ for both datasets. In Exp. \RC{3}, we tune the hyperparameters for each dataset, reporting the best result. Specifically, we set $\alpha_\w = \alpha_\s$ and vary them within the range $\{0.02, 0.05\}$, and vary $\beta_\w$ and $\beta_\s$ independently within the range $\{0.2, 0.5, 0.8, 1.0, 2, 4, 8\}$.

\begin{figure}
    \centering
\subfigure[Justice\label{}]{
    \includegraphics[width=0.45\linewidth]{figures/hps/ethics_justice_Adam-wd_PP.pdf}
}
\subfigure[Commonsense\label{}]{
    \includegraphics[width=0.45\linewidth]{figures/hps/ethics_commonsense_Adam-wd_PP.pdf}
}
    \caption{Effect of hyperparameters in Exp. \RC{2}. Colors indicate Spearman correlation.}
    \label{fig: exp2_hps}
\end{figure}

\begin{figure}[!t]
    \centering
\subfigure[SciQ\label{}]{
\includegraphics[width=.33\columnwidth]{figures/hps/sciq_PP.pdf}
}
\hspace{-.4cm}
\subfigure[Amazon Polarity\label{}]{
\includegraphics[width=.33\columnwidth]{figures/hps/amazon_polarity_PP.pdf}
}
\hspace{-.4cm}
\subfigure[Cosmos-QA\label{}]{
\includegraphics[width=.33\columnwidth]{figures/hps/cosmos_qa_PP.pdf}
}
    \caption{ Effect of hyperparameters in Exp. \RC{3}. Colors indicate Spearman correlation.}
    \label{fig: exp3_hps}
\end{figure}


\textbf{Effect of hyperparameters.} We vary the hyperparameters to evaluate their impact on performance. In the setting of Exp. \RC{2}, we vary $\alpha_\w$ and $\alpha_\s$ within the range ${0.001, 0.01, 0.05}$ and $\beta_\w$ and $\beta_\s$ within the range ${0.0001, 0.001, 0.01}$. The results are visualized in Figure \ref{fig: exp2_hps}. In the setting of Exp. \RC{3}, we vary the hyperparameters while keeping $\alpha_\w = \alpha_\s$ as described in the previous paragraph, with results visualized in Figure \ref{fig: exp3_hps}. Although certain hyperparameter configurations may lead to lower correlation, a non-trivial positive correlation is observed in most cases. Interestingly, in Exp. \RC{3}, which is seemingly the most `challenging setting', the results are highly robust to changes in hyperparameters, with the worst-case correlation remaining around 0.6 across all three datasets.



\begin{figure}[!t]
    \centering
\subfigure[Lipop\label{}]{
\includegraphics[width=.22\columnwidth]{figures/molbert/PPP/corr_PPP_Lipop_0.1,0.1,0.1,0.1.pdf}
}
\hspace{.2cm}
\subfigure[FreeSolv\label{}]{
\includegraphics[width=.22\columnwidth]{figures/molbert/PPP/corr_PPP_FreeSolv_0.1,0.1,0.1,0.1.pdf}
}
\hspace{.2cm}
\subfigure[ESOL\label{}]{
\includegraphics[width=.22\columnwidth]{figures/molbert/PPP/corr_PPP_ESOL_0.1,0.1,0.1,0.1.pdf}
}
    \caption{ Results for $\opnorm{ \mP_\s(\mI-\mP_\w)\mP_\s }$ in Exp. \RC{1}. }
    \label{fig: molecular_PPP}
\end{figure}

\begin{figure}[!t]
    \centering
\subfigure[ Justice\label{}]{
\includegraphics[width=.25\columnwidth]{figures/embedding_model/PPP/PPP_corr_ethics_justice_0.05,0.001,0.01,0.0001.pdf}
}
\subfigure[ Commonsense\label{}]{
\includegraphics[width=.25\columnwidth]{figures/embedding_model/PPP/PPP_corr_ethics_commonsense_0.05,0.001,0.01,0.0001.pdf}
}
    \caption{Results for $\opnorm{ \mP_\s(\mI-\mP_\w)\mP_\s }$ in Exp. \RC{2}. }
    \label{fig: embedding_PPP}
\end{figure}

\begin{figure}[!t]
    \centering
\subfigure[SciQ\label{}]{
\includegraphics[width=.25\columnwidth]{figures/e2e/PPP/PPP_corr_sciq_maxdim_100000000_0,0.02,4.0,8.0.pdf}
}
\subfigure[Amazon Polarity\label{}]{
\includegraphics[width=.25\columnwidth]{figures/e2e/PPP/PPP_corr_amazon_polarity_maxdim_100000000_0,0.05,1.0,8.0.pdf}
}
\subfigure[Cosmos-QA\label{}]{
\includegraphics[width=.25\columnwidth]{figures/e2e/PPP/PPP_corr_cosmos_qa_maxdim_100000000_0,0.05,0.5,4.0.pdf}
}
    \caption{Results for $\opnorm{ \mP_\s(\mI-\mP_\w)\mP_\s }$ in Exp. \RC{3}. }
    \label{fig: end2end_PPP}
\end{figure}



\subsection{Results for $\opnorm{ \mP_\s(\mI-\mP_\w)\mP_\s }$}\label{apdx: PPP}

Results for $\opnorm{\mP_\s(\mI-\mP_\w)\mP_\s}$ are presented in Figures \ref{fig: molecular_PPP}, \ref{fig: embedding_PPP}, and \ref{fig: end2end_PPP}. We observe a strong correlation between $\err_\wtos$ and $\opnorm{\mP_\s(\mI-\mP_\w)\mP_\s}$ across the settings. These correlations are similar to those achieved using $\opnorm{\mP_\s(\mI-\mP_\w)}$, indicating that the two metrics are similarly informative for W2SG in practice, despite being theoretically derived in different ways.


\begin{figure}[!t]
    \centering
\includegraphics[width=.2\textwidth]{figures/e2e/comparison/PP_corr_sciq_maxdim_10000_0,0.02,8.0,8.0.pdf}
\includegraphics[width=.208\textwidth]{figures/e2e/comparison/PP_size_sciq_maxdim_10000_0,0.02,8.0,8.0.pdf}
\includegraphics[width=.206\textwidth]{figures/e2e/comparison/PP_effdim_sciq_maxdim_10000_0,0.02,8.0,8.0.pdf}

\includegraphics[width=.2\textwidth]{figures/e2e/comparison/PP_corr_amazon_polarity_maxdim_8000_0,0.05,1.0,8.0.pdf}
\includegraphics[width=.22\textwidth]{figures/e2e/comparison/PP_size_amazon_polarity_maxdim_8000_0,0.05,1.0,8.0.pdf}
\includegraphics[width=.21\textwidth]{figures/e2e/comparison/PP_effdim_amazon_polarity_maxdim_8000_0,0.05,1.0,8.0.pdf}
\vspace{-.2cm}
    \caption{%top: Sciq  sizes $\leq 10000$.
     {The top panel shows results on SciQ for models with sizes $\leq 10000$, while the bottom panel shows results on Amazon Polarity for models with sizes $\leq 8000$. The patterns observed here are consistent with those discussed in Figure \ref{fig: compare_PP_with_size} in the main paper. }\looseness=-1  }
    \label{fig: compare_PP_with_size_apdx}
    \vspace{-.5cm}
\end{figure}


\subsection{Comparison with model size and effective dimension}\label{apdx: compare_size}

Figure \ref{fig: compare_PP_with_size_apdx} compares our metric with the activation map dimension and the dimension of approximated principal representations for smaller models on SciQ and Amazon Polarity. The results are consistent with those presented in Figure \ref{fig: compare_PP_with_size} in the main paper.

