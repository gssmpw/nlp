\section{Related Work}
There have been many recent works that theoretically explore W2SG. ____ adopt a transfer learning perspective, focusing on improving W2SG through in-context learning rather than explaining how W2SG emerges. ____ analyze W2SG by considering a generalized version of adversarially robust models, showing that certain errors in weak supervision can be corrected by leveraging the good neighborhood structure in the data. However, their argument attributes error correction solely to underfitting—i.e., avoiding fitting mislabeled finetuning data.  This overlooks an important scenario recently discussed in ____, known as benign overfitting, where the strong model overfits mislabeled finetuning data but still achieves accurate test-time predictions. Benign overfitting is particularly relevant in practice, as large neural networks often have the capacity to overfit while still generalizing effectively ____. Closer to our setting, ____ formalized W2SG using a representation-based perspective. %, similar to ours. 
Their work demonstrates that performance gain in W2SG correlates with the disagreement between the finetuned weak and strong models, assuming high-quality representations for the strong model. While insightful, it does not characterize the role of the weak model's representations, leaving the exact conditions for effective W2SG unclear.\looseness=-1

Compared to ____, we analyze W2SG in a more realistic setting where error correction can result from either underfitting or overfitting, allowing for a full spectrum of behaviors. While benign overfitting is not our primary focus, we discuss it as a special case in Sec. \ref{sec: case_study} due to its importance and offer new insights. Compared to ____, we explicitly links W2SG performance to the interaction between the weak and strong models' representations, providing a more comprehensive view of how the intrinsic properties of the two models jointly determine W2SG.\looseness=-1

% this \blue{disagreement} is more of a ``secondary effect'', which only manifests after the finetuning process is completed. Our goal is to take a step further back and identify the initial driving factors—specific properties of the models before finetuning—that enable W2SG.

%\ba{I still don't understand this, you should explain for me.}\looseness=-1


% \blue{Another relevant recent work is ____, which characterizes benign overfitting in W2SG for a specific data distribution. In contrast, our results address more general data distributions, identifying when benign overfitting occurs and how model representations shape this behavior. While benign overfitting is not the primary focus of our work, our framework subsumes it as a special case, offering a broader and novel perspective.}\looseness=-1
% \vspace{-.2cm}


% \blue{____ derived error bounds for W2SG based on the properties of models \ba{e.g.?} w.r.t. the local structure of the data,}
% %under expansion conditions on the data, 
% while ____ extended this by attributing the success of W2SG to overlapping data points—those with easy patterns learnable by the weak model and hard patterns learnable only by the strong model. In contrast, our work focuses on representations, specifically how information is internally structured in the models,
% and investigates how this internal structure influences W2SG. \looseness=-1

%Another relevant recent work is ____, which examines benign overfitting in W2SG, focusing on a specific \ba{needs something more, maybe this specific data is general enough!} data distribution. In Section \ref{sec: case_study}, we show that our general theory not only encompasses scenarios where benign overfitting occurs but also provides broader insights into the phenomenon. While benign overfitting is not the central focus of our work, we treat it as a subcase, offering detailed characterizations and introducing new perspectives.\looseness=-1