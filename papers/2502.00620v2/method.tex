


\section{W2SG from a Representation Perspective}

% We develop a representation-based theoretical framework for analyzing W2SG, accommodating a wide range of scenarios. Within this framework, we present a concise result (Theorem \ref{thm: main_theorem}), which links W2SG performance to the interaction between the weak and strong models' representations.

{We first formalize finetuning from a representation-based perspective, then introduce the properties of the representations considered, and finally present our main theory.}\looseness=-1


%\subsection{A Representation-based perspective}


%\ba{no need for this paragraph! just repeats the intro!}

%The pretraining of the strong model is crucial in W2SG, as it provides foundational knowledge that enables the model to extract generalizable patterns from the weak supervision during finetuning while avoiding certain errors. To formalize this knowledge and its utilization, we adopt a representation-based perspective. 
%The knowledge a model acquires through pretraining serves as a way to interpret inputs, extract relevant information, and organize it into meaningful intermediate states—essentially a ``representation function", $h$, which transforms data into structured representations.

%Then, the finetuned model can be expressed as the composition $f \circ h$. \looseness=-1 


% Finetuning typically does not alter or add new knowledge to the model \cite{zhou2024lima,gekhman2024does}, but instead leverages the model's existing knowledge, $h$, to produce the desired outputs. This process can be viewed as learning a new function, $f$, on top of the representations formed by $h$. Consequently, the finetuned model can be expressed as the composition $f \circ h$.\looseness=-1


% While this formalization is also used in \cite{charikar2024quantifying}, a crucial unanswered question is how the properties of the strong and weak models' representations influence or enable W2SG—a gap we aim to address.



\subsection{A representation-based perspective}

The knowledge a model acquires through pretraining enables it to interpret inputs, extract relevant information, and organize it into meaningful intermediate states. This can be formalized as a ``representation function", $h$, which transforms data into structured representations. Finetuning leverages this knowledge to produce the desired output, which we formalize as learning a new function $f$ on the fixed $h$. The entire model is thus represented as the composition $f\!\circ\! h$. For simplicity, we consider the outputs of $h$ as vectors, and focus on the case where $f$ is a linear functions. This is practically relevant because: (1) Training a linear task head on fixed representations is common with large foundation models, e.g., using embedding LLMs \cite{muennighoff2022mteb}, linear probing on intermediate activations \cite{zou2023representation,nanda2023emergent,marks2023geometry}. (2) fine-tuning of LLMs largely operates in the NTK regime \cite{jacot2018neural}, where training dynamics are captured by a linear model on representations derived from model gradients \cite{malladi2023kernel}. (3) Our experiments in Sec. \ref{sec: experiments} show that insights from analyzing linear functions generalize to the complex non-linear setting of finetuning entire LLMs from pretrained weights. \looseness=-1


\subsection{{Preliminaries}}\label{subsec: pre}

{\textbf{Notations.} We sometimes abbreviate a matrix $\mA \in \sR^{l \times m}$ as $[A_{i,j}]_{1 \leq i \leq l, 1 \leq j \leq m}$ when each element $A_{i,j}$ can be expressed as a generic term in terms of its indices. $\lambda_{\text{min, $\neq 0$}}(\mA)$ denotes the smallest nonzero eigenvalue of matrix $\mA$.} \looseness=-1

\textbf{Data.} Let $\gD$ denote the distribution of the finetuning task's data, defined over the input-label pairs $(\vx, y)\in\gX\times\gY$, where $\gY=\sR$. In W2SG, we have two splits of data sampled from $\gD$. The first subset, $\tilde{\gD}=\{(\tilde{\vx}_i, \tilde{y}_i)\}_{i=1}^{\tilde{n}}$, consists of $\tilde{n}$ i.i.d. samples and is used for finetuning the weak model. The second subset, $\hat{\gD}=\{(\hat{\vx}_i, \hat{y}_i)\}_{i=1}^{\hat{n}}$ with $\hat{n}$ i.i.d. samples is used for finetuning the strong model. Note that the weak model's outputs will be used as labels in place of the actual $\hat{y}_i$'s. In our notation, quantities associated with the two splits are marked by the diacritical symbols, $\tilde{}$ and $\hat{}$, respectively.



\textbf{Models.} We denote the weak and strong models' representation functions as $h_\w$ and $h_\s$, respectively. The finetuned weak model is represented as $f_\w \!\circ\! h_\w $, with
$$
   f_\w \!=\! \arg\min_{f\in\gF_\w} (\frac{1}{\tilde{n}} \sum_{i=1}^{\tilde{n}} ( f(h_\w(\tilde{\vx}_i))-\tilde{y}_i )^2 \!+\! \beta_\w R(f) ).
$$
where $R(\cdot)$ represents $\ell_2$ regularization. \looseness=-1

The \emph{W2S model}, which refers to the strong model finetuned with weak supervision, is represented as $f_\wtos\! \circ\! h_\s $, with
$$
   f_\wtos \!= \!\arg\min_{f\in\gF_\s} (\frac{1}{\hat{n}} \sum_{i=1}^{\hat{n}} ( f(h_\s(\hat{\vx}_i))\!-\! f_\w(h_\w(\hat{\vx}_i)  ) )^2 \!+\! \beta_\s R(f) ).
$$
Additionally, as a reference, we define the \emph{strong ceiling model} as the strong model finetuned with the ground truth labels. It is represented as $f_\sceiling\circ h_\s$ with\looseness=-1
$$
   f_\sceiling = \arg\min_{f\in\gF_\s} (\frac{1}{\hat{n}} \sum_{i=1}^{\hat{n}} ( f(h_\s(\hat{\vx}_i))- \hat{y}_i  )^2  + R_\s(f) ).
$$

% \textbf{Model and loss function.} For simplicity, we consider $h_\w(\vx)\in \sR^{d_\w}$ and $h_\s(\vx)\in \sR^{d_\s}$ as vectors of dimensions $d_\w$ and $d_\s$, respectively, and focus on the case where $\gF_\w$ and $\gF_\s$ are the classes of linear functions mapping representations to $\sR$. This is highly relevant to practical scenarios for several reasons: (1) Training a linear task head on fixed representations is common with large foundation models, e.g., using embedding LLMs \cite{muennighoff2022mteb}, linear probing on intermediate activations \cite{zou2023representation,nanda2023emergent,marks2023geometry}. (2) \citet{malladi2023kernel} show that fine-tuning of LLMs largely operates in the Neural Tangent Kernel (NTK) regime \cite{jacot2018neural}, where training dynamics are captured by a linear model on representations derived from model gradients. (3) Our experiments in Section \ref{sec: experiments} show that insights from analyzing linear functions generalize to the complex, non-linear setting of fine-tuning entire LLMs from pretrained weights. We consider square loss, i.e., $l(f(\vr), y) = (f(\vr) - y)^2$, and $\ell_2$ regularization, i.e., $R(f) = \beta \|\vw\|^2$ where $f(\vr) = \vw^\top \vr$. The regularization coefficients for the weak and strong models are denoted $\beta_\w$ and $\beta_\s$, respectively. \looseness=-1

% $h(\vx)$ denotes the representation generated by a function $h: \gX\!\rightarrow\!\gR$ given an input $\vx$. $h_\s$ and $h_\w$ denote the strong and weak models' representations, respectively, which are learned through pretraining and fixed during finetuning. \looseness=-1


%We consider square loss, i.e., $l(f(\vr), y) = (f(\vr) - y)^2$, and $\ell_2$ regularization, i.e., $R(f) = \beta \|\vw\|^2$ where $f(\vr) = \vw^\top \vr$. The regularization coefficients for the weak and strong models are denoted $\beta_\w$ and $\beta_\s$, respectively. 

% Finetuning the strong model can be formalized as 
% searching for a $f_\wtos$ such that\looseness=-1
% \begin{align}
%     \nonumber
%    f_\wtos = \arg\min_{f\in\gF_\s} (\frac{1}{\hat{n}} \sum_{i=1}^{\hat{n}} l( f(h_\s(\hat{\vx}_i)), f_\w(h_\w(\hat{\vx}_i)  ) ) + R_\s(f) ),
% \end{align}
% where $\gF_\s$ denotes the function class from which $f_\wtos$ is learned. 


\textbf{Evaluation.} At test time, given any labeling function $g:\gX\!\rightarrow\!\gY$, we define its test error as the loss on the population: $
    \err(g) = \E_{(\vx, y)\sim\gD}[ ( g(\vx)\!-\! y )^2 ]$. We then introduce the shorthand notations: the weak model's test error $\err_\w=\err(f_\w \circ h_\w )$, the W2S model's test error $ \err_\wtos=\err(f_\wtos \circ h_\s )$, and the strong ceiling model's test error  $ \err_\sceiling=\err(f_\sceiling \circ h_\s )$. $\err_\wtos$ measures the performance achieved through W2SG, while $\err_\sceiling$ serves as the upper limit.\looseness=-1

%\ba{where is squared diff?} 
We also introduce \predgap{}, the squared difference between the predictions of the W2S and strong ceiling models: \looseness=-1
\looseness=-1
$$
    \predgap = \E_{(\vx, y)\sim\gD}[ ( f_\wtos(h_\s(\vx))\!-\! f_\sceiling(h_\s(\vx)) )^2 ].
$$
% \begin{align}
%     \nonumber
%     \predgap = \E_{(\vx, y)\sim\gD}[ l( f_\wtos(h_\s(\vx)), f_\sceiling(h_\s(\vx)) ].
% \end{align}
%{It quantifies how far we are from fully eliciting the strong model. 
{It captures how much the strong model falls short of its full potential due to weak supervision.} 
It is also indicative of \(\err_\wtos\), the direct measure of W2SG performance, through these connections: (1) If the strong ceiling model is nearly perfect, it follows that $\predgap{}\approx \err_\wtos $ {as the strong ceiling's predictions are almost identical to the ground truth}. This is not unlikely, since the ultimate goal of W2SG is to operate in cases where the strong model is a superhuman-level AI \cite{burns2023weak}, plausibly capable of achieving perfect results if provided with ground truth labels. (2) With small regularization and well-conditioned representations, $\err_\wtos \approx \predgap{}+ \err_\sceiling $ (Thm. \ref{thm: ewtos=predgap+errsc}), analogous to the Pythagorean theorem. Then, $\predgap{}$ directly determines $\err_\wtos$ for fixed $\err_\sceiling$. (3) For general cases, the upper bound $ \sqrt{\err_\wtos}\! \leq \sqrt{\predgap{}}\! +\! \sqrt{\err_\sceiling}  $ follows from the triangle inequality. Furthermore, the result obtained from analyzing $\predgap{}$ helps predict $\err_\wtos$ in our experiments (Sec. \ref{sec: experiments}). Thus, our main analysis focuses on $\predgap{}$.
\looseness=-1

%Additionally, in our experiments in Sec. \ref{sec: experiments}, we will see that even $\mP_\s(\mI-\mP_\w)$, which appears on the RHS of Equation \ref{eq: pred_gap_equal}, directly correlates with  $\err_\wtos$ for real models.

%In our analysis, we examine the asymptotic behavior as the size of the fine-tuning dataset scales. Specifically, we treat all quantities as functions of $\min( \hat{n},\tilde{n} )$. The asymptotic notations $O$, $o$, $\Omega$, $\omega$ and $\Theta$ are defined with respect to $\min( \hat{n},\tilde{n} )$.

% \subsection{$(\delta, \hat{\gamma}, \tilde{\gamma})$-decomposable representations}\label{subsec: assump}


\subsection{{Assumption: representations with a well-concentrated principal part
and a manageable non-principal part}}\label{subsec: assump}

% \blue{We introduce a general class of representations that can be decomposed into a principal part, which concentrates well given a finite sample, and a non-principal part, which remains theoretically tractable despite not concentrating well.   }\looseness=-1


We first define two basic concepts, kernel and covariance, before introducing a general assumption on representations.\looseness=-1

\begin{definition}[Kernel Matrix]
Given $h:\!\gX\!\rightarrow\! \sR^d $, we define the kernel matrix on the finetuning dataset $\hat{\gD}$ as $\hat{\mK}(h)\!=\![ h(\hat{\vx}_i)^\top h(\hat{\vx}_j) ]_{1\leq i,j\leq \hat{n}}$, a $\hat{n}\times\hat{n}$ matrix where each element represents the inner product between a pair of representations. $\tilde{\mK}(h)$ is defined on $\tilde{\gD}$ in the same manner.\looseness=-1
\end{definition}


\begin{definition}[Population/Empirical Covariance Matrices]
Given $h:\!\gX\!\rightarrow\! \sR^d $, we define the population covariance over distribution $\gD$ as $\mSigma(h)\coloneqq\E_{\gD_{\vx}}[ h(\vx)h(\vx)^\top ]$. The empirical version on $\hat{\gD}$ is defined as $\hat{\mSigma}(h) \coloneqq \frac{1}{\hat{n}} \sum_{i=1}^{\hat{n}}h(\hat{\vx}_i)h(\hat{\vx}_i)^\top $. $\tilde{\mSigma}(h)$ is defined on $\tilde{\gD}$ in the same manner.  \looseness=-1
\end{definition}
{Given a representation function and a reasonable sample size, certain components in the representations should \emph{concentrate well}, meaning they adequately reflect the population distribution. These components are pivotal to the model's generalization. In our analysis, we focus on cases where the remainder—the less-well-concentrated components—satisfies certain conditions, ensuring their impact remains theoretically tractable. The decomposition of representations into these two parts is formalized as follows.} \looseness=-1

\begin{definition}[$(\delta, \hat{\gamma}, \tilde{\gamma})$-decomposability]\label{def: delta_decomp} 
Given $\gD$, $\tilde{\gD}$, $\hat{\gD}$, and a representation function $h:\!\gX\!\rightarrow\!\gR$, we say that the representations of $h$ are \emph{$(\delta, \hat{\gamma}, \tilde{\gamma})$-decomposable w.r.t. a subspace $\gV$ (of $\gR$)}, for some $\delta\!=\!O(1)$,  $\hat{\gamma}\!=\!O(1)$, and $\tilde{\gamma}\!=\!O(1)$, if there exists a subset of eigenvectors of $\mSigma(h)$ corresponding to non-zero eigenvalues such that the following holds. Let $\gV$ denote the span of these eigenvectors, and let $\gV^\perp$ denote its orthogonal complement. Let $\mPi_{\gV}$ and $\mPi_{\gV^\perp}$ denote the orthogonal projections onto $\gV$ and $\gV^\perp$, respectively. Define $\rho=\lambda_{\text{min, $\neq 0$}}( \mSigma(\mPi_{\gV}h)  )$ and $\gamma=\min( \hat{\gamma}, \tilde{\gamma} )$. With high probability of $1-o(1)$:\looseness=-1
\begin{itemize}[leftmargin=*]
\item[a.] \bounded{}. A basic condition that ensures reasonable magnitudes of representations and labels: $\opnorm{\mSigma(h)}\!=\!O(1)$, $\opnorm{\hat{\mSigma}(h)}\!=\!O(1)$ $\opnorm{\tilde{\mSigma}(h)}\!=\!O(1)$, $\E[y^2]=O(1)$, $\frac{1}{\hat{n}}\sum_{i=1}^{\hat{n}}\hat{y}_i^2\!=\!O(1)$ and $\frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}}\tilde{y}_i^2 \!= \!O(1)$.
\item[b.] \conc{}. Representations are well-concentrated in the subspace $\gV$, both in terms of their covariance and their correlation with labels:  $ \opnorm{ \hat{\mSigma}(\mPi_{\gV}h)-  \mSigma(\mPi_{\gV}h)  } = o(\gamma^2+\delta^2 + \rho^2 ) $, $ \opnorm{ \tilde{\mSigma}(\mPi_{\gV}h)-  \mSigma(\mPi_{\gV}h)  } = o(\gamma^2+\delta^2 + \rho^2 ) $, $\| \frac{1}{\hat{n}} \mPi_{\gV}h(\hat{\vx}_i)\hat{y}_i -\E[ \mPi_{\gV}h(\vx) y]  \| = o(\gamma+\delta+\rho) $ and $\| \frac{1}{\tilde{n}} \mPi_{\gV}h(\tilde{\vx}_i)\tilde{y}_i -\E[ \mPi_{\gV}h(\vx) y]  \| = o(\gamma+\delta+\rho) $.
\item[c.] \isotropy{}. The kernels constructed using only the components in $\gV^\perp$ exhibit certain uniformity in all orientations, with the extent of uniformity controlled by $\delta$: $ \opnorm{\frac{1}{\hat{n}} \hat{\mK}( \mPi_{\gV^\perp}h )\! -\!\hat{\gamma} \mI }\! =\! o(\gamma^2+\delta^2) $,  and $ \opnorm{\frac{1}{\tilde{n}} \tilde{\mK}( \mPi_{\gV^\perp}h )\! -\!\tilde{\gamma} \mI }\! =\! o(\gamma^2+\delta^2) $. 
\item[d.] \smallin{}. $\opnorm{ \frac{1}{\sqrt{\hat{n}\tilde{n}}}[ (\mPi_{\gV^\perp}h(\hat{\vx}_i))^\top \mPi_{\gV^\perp}h(\tilde{\vx}_j)  ]_{1\leq i\leq\hat{n}, 1\leq j \leq \tilde{n}} }\!=\! o(\gamma\!+\!\delta) $, which holds when representations on $\gV^\perp$ are nearly orthogonal across samples or have small magnitudes.\looseness=-1
\item[e.] \dimini{}. The representations on $\gV^\perp$ have small magnitude in the population: $ \opnorm{ \mSigma(\mPi_{\gV^\perp}h) }=o(\gamma+\delta)$. 
\end{itemize}
\end{definition}
\begin{remark}
{To provide a clearer understanding of \isotropy{}, consider the following: If $\delta$ is very small (e.g., $\delta = 0$), the kernel on $\hat{\gD}$ is nearly identical to $\hat{\gamma}\mI$, {meaning it does not exhibit any specific patterns that differentiate between data points.} 
%meaning it does not favor any particular labeling of the data. 
In contrast, with a larger $\delta$ (e.g., $\delta \gg \hat{\gamma}$), this requirement is much more relaxed—the kernel  no longer needs to closely resemble $\hat{\gamma}\mI$ but instead must simply have its magnitude bounded by $o(\delta)$. Thus, it accommodates scenarios where the kernel is highly isotropic, very small in scale, or anywhere in between. This is key to our analysis, as it ensures the effect of the less well-concentrated part of the representations remains tractable. Additionally, we note that \smallin{} does not imply negligible impact of representations on $\gV^\perp$. E.g., when $\delta$ is small, the model can in fact leverage the components in $\gV^\perp$ to interpolate the training data,  even when such interpolation cannot be achieved by the components in $\gV$ (see Example \ref{eg: toyeg}).}\looseness=-1
\end{remark}

% \textbf{Explanation of Definition \ref{def: delta_decomp}.} \bounded{} is a basic condition that ensures reasonable magnitudes of representations and labels. \conc{} states that representations are well-concentrated in the subspace $\gV$, both in terms of their covariance and their correlation with labels. \isotropy{} implies that the kernels constructed using only the components in $\gV^\perp$ exhibit certain uniformity in all orientations, with the extent of uniformity controlled by $\delta$. For example, if $\delta$ is very small (e.g., $\delta = 0$), the kernel on $\hat{\gD}$ is nearly identical to $\hat{\gamma}\mI$, meaning it does not favor any particular labeling of the data. In contrast, with a larger $\delta$ (e.g., $\delta \gg \hat{\gamma}$), this requirement is much more relaxed—the kernel  no longer needs to closely resemble $\hat{\gamma}\mI$ but instead must simply have its magnitude bounded by $o(\delta)$. Thus, this condition accommodates scenarios where the kernel is highly isotropic, very small in scale, or anywhere in between. This is key to our analysis, as it ensures the effect of the less well-concentrated part of the representations remains tractable. \smallin{} holds either when representations on $\gV^\perp$ are nearly orthogonal across samples or when their magnitudes are small. \dimini{} means that the representations on $\gV^\perp$ have small magnitude in the population.  Note that that this does not mean their impact is negligible on training data. For example, if $\delta$ in condition \isotropy{} is very small, the model can, in fact, leverage the components in $\gV^\perp$ to interpolate the training data, even when the components in $\gV$ alone are insufficient for achieving this (see Example \ref{eg: toyeg}).\looseness=-1

We refer to $\mPi_{\gV}h(\vx)$, the well-concentrated part of the representation, as the \emph{principal representation}, and the remainder, $\mPi_{\gV^\perp}h(\vx)$, as the \emph{non-principal representation}.

{\textbf{Examples of Def. \ref{def: delta_decomp}.} Def. \ref{def: delta_decomp} is highly general, covering various representation distributions and dimensionalities. One simple case is when all components are well-concentrated, i.e., the entire representation is principal. This occurs when the representations exhibit a certain low-rank structure, which is common in deep neural networks \cite{huh2021low}.  Below is a concrete example. }
\looseness=-1
\begin{example}[Arbitrarily parameterized; bounded representations with low intrinsic dimension]\label{eg: intrinsic_bounded} Given $h: \gX\rightarrow \sR^d $, for any $(\vx, y)$, $\| h(\vx) \|^2 \leq B$ and $y^2 \leq C$, where $C\!=\! \Theta(1)$. Additionally, $\opnorm{\mSigma(h)} \!=\! \Theta(1)$. The intrinsic dimension of $\mSigma(h)$ is defined as $\intdim(\mSigma(h)) = \frac{\Tr(\mSigma)}{\opnorm{\mSigma}}$, denoted by $q$. Let $n\!=\! \min(\hat{n}, \tilde{n})$ and assume $n^{1-c} = \omega\big(B \log(q)\big)$ for some constant $c < 1$. Then, the representations are $(n^{-0.1c}, 0, 0)$-decomposable w.r.t. $\sR^d$.\looseness=-1
\end{example}
\begin{remark}
The conditions imply a low intrinsic dimension relative to the sample size: $q \log q \!=\! o(n^{1-c})$ (Appx. \ref{apdx: example_bernstein}), but without restricting the actual dimension $d$, allowing both under- $(d\!<\!n)$ and over-parameterized $(d\!\geq\! n)$ settings. \looseness=-1
\end{remark}

% \begin{example}[Arbitrarily parameterized; Gaussian representations with low intrinsic dimension]\label{eg: intrinsic_gaussian}
% \end{example}
% \begin{remark}
% Note that the intrinsic dimension can be very small, even when the actual dimension is very large. For example,  ... in this case intrinsic dimension $\ll n$ but dimension $d \gg n$ 
% \end{remark}
{The next example is related to the spiked covariance model originating from PCA and widely used in recent theoretical studies across various domains (e.g., \cite{muthukumar2021classification,nakada2023understanding}). It is also related to the sparse coding model, which has its roots in computer vision \cite{olshausen1997sparse}, and has been applied to language modeling \cite{arora2018linear} and deep learning theory (e.g., \cite{allen2020towards}). More references are in Appx. \ref{apdx: spiked_cov}. We consider representations that follow a sub-Gaussian, which is a very general class of distributions, including, e.g., any bounded random variables and Gaussian.} \looseness=-1
%Example \ref{eg: spiked_cov} is particularly important as it captures the high-dimensional yet low-rank nature \cite{huh2021low} of deep neural network representations.
\begin{example}[Heavily overparameterized; sub-Gaussian with spiked covariance]\label{eg: spiked_cov}
%The marginal distribution of the representations is a zero-mean sub-Gaussian.
Given $h:\!\gX\!\rightarrow\! \sR^d $ and randomly drawn $\vx$, 
$h(\vx)$ has independent zero-mean sub-Gaussian entries. 
The first $k$ entries have a (sub-Gaussian) parameter of $\Theta(1)$ and variance $1$, while the remaining $d\!-\!k$ entries have a parameter of $\Theta(\frac{\sigma^2}{d-k})$ and variance $\frac{\sigma^2}{d-k}$. The scalings satisfy: $\tilde{n} \!= \!\Theta(\hat{n})$, $\sigma^2 = O(\hat{n})$, $\hat{n}\! = \!\omega(k^2)$, and $d \! =\! \omega(\hat{n}^2)$. The labels have bounded moment, $\E[y^2] \!= \!O(1)$. Then, the representations are $(0, \frac{\sigma^2}{\hat{n}}, \frac{\sigma^2}{\tilde{n}})$-decomposable w.r.t. the subspace corresponding to the first $k$ coordinates.\looseness=-1
\end{example}
\begin{remark}
Compared to Example \ref{eg: intrinsic_bounded}, this example accommodates cases with high intrinsic dimensions. For instance, if we set $\sigma^2 = \Theta(\hat{n})$, then $\intdim(\mSigma(h)) = \Theta(n)$. 
\looseness=-1  
\end{remark}
% More complex examples could be constructed leveraging the following result, which shows that adding high-dimensional sub-Gaussian elements to any $(\delta, 0, 0)$-decomposable representations still results in decomposable representations. 
More complex examples can be constructed from the fact that adding high-dimensional sub-Gaussian to $(\delta, 0, 0)$-decomposable representations preserves decomposability: \looseness=-1
\begin{theorem}\label{thm: construct_new}
Given a representation function
$h$ whose representations $h(\vx)\in\sR^d$ are $(\delta, 0, 0)$-decomposable w.r.t. $\sR^d$, we construct new representations with $
   \alpha(\vx) = \mM h(\vx) + \mM^\perp \xi(\vx)$,
where $\mM\in\sR^{(d+m)\times d}$ and $\mM^\perp \in \sR^{(d+m)\times m}$ both have orthonormal columns, and their column spaces are orthogonal to each others. If elements in $\xi(\vx) \in \sR^m$ are independent zero-mean sub-Gaussian with parameter $\Theta(\frac{\sigma^2}{m})$ and variance $\frac{\sigma^2}{m}$, assuming $\tilde{n}\!=\!\Theta(\hat{n})$, $m\!=\!\omega(\hat{n}^2)$, and $\sigma^2\!=\!O(\hat{n})$, then $\alpha$'s representations are $(\delta, \frac{\sigma^2}{\hat{n}}, \frac{\sigma^2}{\tilde{n}})$-decomposable w.r.t. the span of $\mM$'s columns. \looseness=-1
\end{theorem}
\begin{remark}
For instance, one could take $h$ from Example \ref{eg: intrinsic_bounded}.
\end{remark}

We assume both models' representations satisfy Def. \ref{def: delta_decomp}: \looseness=-1
\begin{assumption}\label{assump: weak_strong_decomp}
$h_\w$'s representations are $(\delta_\w, \hat{\gamma}_\w, \tilde{\gamma}_\w)$-decomposable w.r.t. $\gV_\w$, and $h_\s$'s representations are $(\delta_\s, \hat{\gamma}_\s, \tilde{\gamma}_\s)$-decomposable w.r.t. $\gV_\s$ .
\end{assumption}


%for our example of spiked cov data, we can probably first bound the perturbation and then use perturbation theory to show that they satisfy the assumptions. 

\subsection{{Principal representations shape \predgap{}}}\label{subsec: main_result}

\textbf{Intuition.} One implication of Def. \ref{def: delta_decomp} is that only what is learned through the principal representations will be reflected at test time.  Thus, the weak model's mistakes primarily stem from its inability to generate certain outputs using its principal representations. For the same reason, among these mistakes, only those expressible through the strong model's principal representations will affect its test performance. Therefore, a key concept affecting W2SG performance is \textbf{``what the weak model is unable to learn but is learnable by the strong model using their respective principal representations"}, which we seek to quantify.\looseness=-1

\textbf{Formalization.} To formalize the above idea, we leverage $\hat{\mK}(\mPi_{\gV_\w}h_\w)$ and $\hat{\mK}(\mPi_{\gV_\s}h_\s)$--kernels computed using only the weak and strong models' principal representations, referred to as \emph{principal kernels}. We define the following \looseness=-1
\begin{align}
\nonumber
%\label{eq: pw}
        \mP_\w \coloneqq & \frac{1}{\hat{n}}\hat{\mK}(\mPi_{\gV_\w}h_\w) \left(\frac{1}{\hat{n}}\hat{\mK}(\mPi_{\gV_\w}h_\w)+(\beta_\w+\tilde{\gamma}_\w)\mI\right)^{-1},\\
\nonumber
%\label{eq: ps}
    \mP_\s \coloneqq & \frac{1}{\hat{n}}\hat{\mK}(\mPi_{\gV_\s}h_\s) \left(\frac{1}{\hat{n}}\hat{\mK}(\mPi_{\gV_\s}h_\s)+(\beta_\s+\hat{\gamma}_\s)\mI\right)^{-1}.
\end{align} 
$\mP_\w$ and $\mP_\s$
%, referred to as \emph{regularized principal-kernel projection matrices}, 
represent scaled projections onto the spans of the principal kernels. Each captures the space of output patterns that its respective model can express through its principal representations (with regularization taken into account). Then, the earlier intuition can be characterized as follows.\looseness=-1

% They capture the space of output patterns that the models can generate through their respective principal knowledge. \textbf{Now, the concept of ``what is not expressible by the weak model but expressible by the strong model using their respective main knowledge", can be characterized by $\mP_\s(\mI-\mP_\w)$}. It determines the mistakes that will be learned by the strong model, as discussed at the beginning. We formally state this relation in our main result below.


% We will formalize this concept of ``not expressible by the weak model but expressible by the strong model through their respective principal knowledge" by leveraging $\hat{\mK}(\mPi_{\gV_\w}h_\w)$ and $\hat{\mK}(\mPi_{\gV_\s}h_\s)$--kernels computed using only the weak and strong models' principal representations, referred to as \emph{principal kernels}. The span of these kernels captures the space of output patterns that the models can potentially generate. Following this intuition, we obtain our main result below. \looseness=-1
% Next, we analyze how \predgap{} is influenced by the relationship between the weak and strong models' representations. At a high level, given Def. \ref{def: delta_decomp}, the distribution of principal representations in the finite finetuning data closely resembles that of the population, while the non-principal representations do not. Consequently, only the patterns learned by the model using the principal representations are reflected at test time. The capabilities of the principal representations are captured by $\hat{\mK}(\mPi_{\gV_\w}h_\w)$ and $\hat{\mK}(\mPi_{\gV_\s}h_\s)$--kernels computed using only the weak and strong models' principal representations, referred to as \emph{principal kernels}. They play a pivotal role in W2SG, as demonstrated in our main result below.\looseness=-1
\begin{theorem}[Main result]\label{thm: main_theorem}
Under Assump. \ref{assump: weak_strong_decomp}, and assuming reasonable regularization: $\delta_\w\!\leq\!\beta_\w\! =\!O(1)$ and $\delta_\s\!\leq\!\beta_\s \!=\!O(1)$, let $ \hat{\vy}=[ \hat{y}_1~\hat{y}_2~\dots~\hat{y}_{\hat{n}} ]^\top $. Then, w.h.p., we have\looseness=-1
\begin{equation}
\label{eq: pred_gap_equal}
   \predgap =  \| \mP_\s (\mI -\mP_\w ) \frac{1}{\sqrt{\hat{n}}} \hat{\vy}  \|^2 \pm o(1) 
\end{equation}
% and
% \begin{align}
% \nonumber
% %\label{eq: pw}
%         \mP_\w = & \frac{1}{\hat{n}}\hat{\mK}(\mPi_{\gV_\w}h_\w) \left(\frac{1}{\hat{n}}\hat{\mK}(\mPi_{\gV_\w}h_\w)+(\beta_\w+\tilde{\gamma}_\w)\mI\right)^{-1},\\
% \nonumber
% %\label{eq: ps}
%     \mP_\s = & \frac{1}{\hat{n}}\hat{\mK}(\mPi_{\gV_\s}h_\s) \left(\frac{1}{\hat{n}}\hat{\mK}(\mPi_{\gV_\s}h_\s)+(\beta_\s+\hat{\gamma}_\s)\mI\right)^{-1}.
% \end{align} 
\end{theorem}
\textbf{$\mP_\s(\mI\!-\!\mP_\w)$ captures ``what the weak model is unable to learn but is learnable by the strong model using their respective principal representations".} Therefore, it determines the mistakes that will be learned by the strong model, as discussed in the intuition. A more powerful weak model has a $\mP_\w$ that covers more space, shrinking $\mP_\s(\mI\!-\!\mP_\w)$ and potentially leading to a smaller $\predgap{}$.

\textbf{Propagation of Errors.} The earlier intuition is reflected in the proof (Appx. \ref{apdx: proof_main_theorem}). Given the labeling $\hat{\vy}$, its projection $(\mI\!-\!\mP_\w)\hat{\vy}$ is orthogonal to the scaled weak model's principal kernel and thus cannot be effectively learned, contributing to the weak model's error (Lem. \ref{lemma: weak_error}). The projection of this error onto the scaled strong model's principal kernel, $\mP_\s(\mI\!-\!\mP_\w)\hat{\vy}$, is learned by the strong model and contributes to \predgap{} (Lem. \ref{lemma: propagation_strong}).\looseness=-1
%Thus, \predgap{} is governed by the overlap $\mP_\s(\mI\!-\!\mP_\w)$. \looseness=-1
%\blue{This overlap captures what the weak model's main knowledge lacks but is covered by the strong model's main knowledge. Intuitively, with a more knowledgeable weak model, this overlap would be smaller, leading to a smaller $\predgap$, making the W2SG model closer to the strong ceiling. }
\looseness=-1



%indicates that more weak model errors can potentially be reproduced by the W2S model, increasing the gap between the W2S model and the strong ceiling model.


%From Equation \ref{eq: pred_gap_equal}, \predgap{} is governed by the product $\mP_\s (\mI - \mP_\w)$. 


% From Equation \ref{eq: pred_gap_equal}, \predgap{} is governed by the product $\mP_\s (\mI - \mP_\w)$. Intuitively, this can be understood in two steps: (1) Given a labeling, any part orthogonal to the weak model's principal kernel cannot be effectively learned and thus contributes to error (this is characterized in Lemma \ref{lemma: weak_error}). This error impacts the W2S model, but only the component that aligns with the strong model's principal kernel will be learned by the W2S model and contribute to \predgap{} (shown in Lemma \ref{lemma: propagation_strong}). Combining these insights, we derive the main result of the paper below.


%We call $\hat{\mK}(\mPi_{\gV_\w}h_\w)$ and $\hat{\mK}(\mPi_{\gV_\s}h_\s)$ the \emph{principal kernels} of the weak and strong models, respectively. These are the kernel matrices computed using only the principal representations, i.e., the representations that lie within $\gV_\w$ and $\gV_\s$, respectively. We refer to $\mP_\w$ and $\mP_\s$ as \emph{regularized principal-kernel projection matrices}, which represent scaled projections onto the spaces spanned by the principal kernels. At a high level, these matrices capture the types of labelings favored by their respective models in terms of alignment with their principal representations.  From Equation \ref{eq: pred_gap_equal}, \predgap{} is governed by the product $\mP_\s (\mI - \mP_\w)$. Intuitively, this can be understood in two steps: (1) The weak model's error lies in the space captured by $(\mI - \mP_\w)$ (Lemma \ref{lemma: weak_error}); (2) among these errors, only those within $\mP_\s$ propagate to the W2S model's predictions at test time (Lemma \ref{lemma: propagation_strong}). Thus, at a high level, $\mP_\s(\mI-\mP_\w)$ measures the overlap between the space where the weak model's mistakes could possibly occur and the space from which the W2S model learns. This overlap can further be interpreted as \textbf{the W2S model's inability to avoid replicating the weak model's errors}. A greater overlap indicates that more weak model errors can potentially be reproduced by the W2S model, increasing the gap between the W2S model and the strong ceiling model.\looseness=-1


% \todoblue{can also mention a few additional points: (1) predict W2SG without label; equation \ref{eq: pred_gap_multiplic};  (2) understand the  benefit of using strong regularization: making Ps sharper (
% [One implication of regularization is that if $\beta_\s$ can be increased reasonably without affecting the strong ceiling's performance, it can benefit w2s by making $\frac{\hat{\mLambda}_\s'}{\hat{\mLambda}_\s' + \beta_\s \mI}\hat{\mV}_\s'^\top$ ``sharper". This aligns with practical scenarios where regularization techniques, such as early stopping, are commonly used for training the strong model, e.g., as noted in \cite{burns2023weak}.]); implicit regularization . 
%  perhaps can also mention that in the theoreical parts we are mostly gonna use equation \ref{eq: pred_gap_equal} for a more precise characterization; however, \ref{eq: pred_gap_multiplic} has important practical application of predicting w2sg without labels, as in experiment section}

% \subsection{Connection between $\predgap$ and $\err_\wtos$}\label{subsec: predgap_and_errwtos}


 
\section{A Case Study on Benign Overfitting}\label{sec: case_study}
Our theory can be applied to study and provide new insights into \emph{benign overfitting}, an intriguing special case of W2SG, where the W2S model appears to mimic the weak supervision during finetuning, yet generalizes better at test time. \looseness=-1

%This scenario is encompassed by our theoretical framework base on $(\delta, \hat{\gamma}, \tilde{\gamma})$-decomposable representations.

\subsection{A general condition}\label{subsec: general_bo}
Benign overfitting has been studied in the general machine learning context to understand deep neural networks' generalization \cite{bartlett2020benign,wang2021benign,frei2022benign,mallinar2022benign}. Recently, \cite{wu2024provable} theoretically characterized benign overfitting in W2SG for a specific data distribution. Here, we aim to derive broader insights from a representation perspective. We consider the scenario where the strong model's representations are highly expressive, enabling near-perfect overfitting of arbitrary labelings on the finetuning data, mirroring the behavior of very large neural networks in practice \cite{zhang2021understanding}. This occurs when $\delta_\s=o(\hat{\gamma}_\s)$ (Lem. \ref{lemma: overfitting_condition}), yielding a highly isotropic non-principal kernel. Meanwhile, since generalization depends solely on the principal representations by Thm. \ref{thm: main_theorem}, a small $\|\mP_\s (\mI - \mP_\w) \frac{1}{\sqrt{\hat{n}}} \hat{\vy} \|^2$ suffices for good W2SG performance, regardless of the extent of overfitting. In this way, we connect benign overfitting to the general relationship between the weak and strong models' representations: \looseness=-1
%While it is commonly believed that using strong regularization to prevent the strong model from overfitting the weak model's outputs is beneficial for W2SG, we show that even in the absence of such strong regularization, where the model naively fits the weak labels, W2SG can still work theoretically. 

\begin{theorem}[A general condition for benign overfitting \footnote{Thm \ref{thm: general_bn} can be extended to cases where the strong ceiling is not perfect, but we omit this for brevity.} ]\label{thm: general_bn}
In addition to Assumption \ref{assump: weak_strong_decomp}, suppose that
(1) $\delta_\s = o(\hat{\gamma}_\s)$ and $\delta_\s \leq \beta_\s = o(\hat{\gamma}_\s)$, (2) w.h.p.,  the strong ceiling model achieves nearly perfect performance, i.e., $\err_\sceiling = o(1)$, (3) w.h.p., $\| \mP_\s (\mI -\mP_\w ) \frac{1}{\sqrt{\hat{n}}} \hat{\vy}  \|^2  = \err_\w-\Delta  $ with $\Delta=\Theta(1)$. Then, w.h.p., the W2S model achieves an almost zero ($o(1)$) training error on  $\hat{\gD}$, but generalizes better than the weak model: $ 
  \err_\wtos \leq \err_\w - \Delta + o(1)$. See proof in Appx. \ref{apdx: proof_benign_of}. 
\end{theorem}

\subsection{Instantiation of Theorem \ref{thm: general_bn} on a toy example}

We present a concrete example of the scenario in Theorem \ref{thm: general_bn} to demonstrate the realizability of the conditions. 
%By analyzing this example, we can clearly observe which types of errors from the weak model are replicated by the W2S model and which are avoided. 
While more complex examples could be constructed, we focus on a simple one to succinctly illustrate the core ideas.\looseness=-1


\begin{example}\label{eg: toyeg} The label is a Gaussian: $y\sim\gN(0,1)$. Given $(\vx, y)$, the weak model's representation is
 $ h_\w(\vx) = [( \sqrt{\eta} ~y+\sqrt{1-\eta} ~\zeta) ~~~ \vxi_\w^\top]^\top$, where $\eta\in(0,1)$ is some constant, $\zeta\!\sim\!\gN(0, 1)$ and $\vxi_\w\!\sim\!\gN(0, \frac{\sigma^2}{d-1}\mI ) $ are both independently drawn. The strong model's representation is
 $ h_\s(\vx) = [y ~~~ \vxi_\s^\top]^\top $,
where $\vxi_\s\!\sim\!\gN(0, \frac{\sigma^2}{d-1}\mI ) $ independently. The scalings satisfy $\tilde{n}=\Theta(\hat{n}) =\omega(1)$, $ d=\omega(\hat{n}^2) $, and $\sigma^2 = o(\hat{n})$ but $\neq 0$. Additionally, $\beta_\s = o(\frac{\sigma^2}{\hat{n}})$ and $\beta_\w = o(\frac{\sigma^2}{\hat{n}})$.
\looseness=-1    
\end{example}

Here, the weak model's first coordinate carries a signal about the label $y$, but corrupted by noise $\zeta$, with $\eta$ controlling the signal strength (i.e., with SNR $\frac{\eta}{1-\eta}$). The strong model's first coordinate carries a perfect signal about $y$. The remaining coordinates in both models are high-dimensional random noise. Both models' representations are special cases of Example \ref{eg: spiked_cov} and are therefore $(0, \frac{\sigma^2}{\hat{n}}, \frac{\sigma^2}{\tilde{n}})$  decomposable.\looseness=-1  

\begin{corollary}\label{coro: case_study}
Benign overfitting occurs in Example \ref{eg: toyeg}. Specifically, w.h.p., (1) The weak model's errors on both $\hat{\gD}$ and the population are $(1\!- \!\eta) \!\pm\! o(1)$. (2) The W2S model overfits the weak model's outputs on $\hat{\gD}$, achieving a training loss of $o(1)$. (3) However, compared to the weak model, the W2S model achieves a smaller test error: $\err_\wtos \!=\! (1\! -\! \eta)^2 \pm o(1)$.\looseness=-1
\end{corollary}
For instance, if $\eta\!=\!0.6$, then $\err_\w\approx0.4$, while $\err_\wtos\approx0.16$, despite nearly perfect overfitting on $\hat{\gD}$. \looseness=-1
%We will examine in detail how the remaining $0.24$ disappears.\looseness=-1

\subsection{A closer look at error propagation}

We provide a rough derivation of the W2S error (with  details in Appx. \ref{apdx: proof_case_study}), illustrating which errors are replicated and which are corrected (overfitted but benignly) by the W2S model, and how representations determine this.\looseness=-1

The principal representations for both models are simply at their first coordinates. Thus, the spans of their principal kernels are one-dimensional.  Let $\hat{\vzeta} \in \sR^{\hat{n}}$ denote the vector collecting the $\zeta$ values on $\hat{\gD}$, i.e., $\hat{\vzeta} = [\hat{\zeta}_1, \dots, \hat{\zeta}_{\hat{n}}]^\top$. Similarly, define $\hat{\vy} = [\hat{y}_1, \dots, \hat{y}_{\hat{n}}]^\top$. We can approximate the projection matrices as: 
$\mP_\w  \approx \frac{1}{\hat{n}}\hat{\vq}\hat{\vq}^\top$ and $
    \mP_\s \approx    
 \frac{1}{\hat{n}}\hat{\vy}\hat{\vy}^\top$, where $\hat{\vq}=\sqrt{\eta}\hat{\vy} + \sqrt{1-\eta}\hat{\vzeta}$.
Note that vectors $\frac{1}{\sqrt{\hat{n}}}\hat{\vy}$ and $\frac{1}{\sqrt{\hat{n}}}\hat{\vzeta}$ are almost orthogonal as the corresponding random variables are uncorrelated: $
    \frac{1}{\sqrt{\hat{n}}}\hat{\vy}^\top \frac{1}{\sqrt{\hat{n}}}\hat{\vzeta} = \frac{1}{\hat{n}}\sum_{i}\hat{y_i}\hat{\zeta}_i\approx\E [y\zeta]=0$. Let $\vepsilon_\w$ be the vector whose $i$-th element is the weak model's error on data point $(\hat{\vx}_i, \hat{y}_i)$. By Lemma \ref{lemma: weak_error}, we can approximate  $\vepsilon_\w$ as: \looseness=-1
$$
    \vepsilon_\w \approx (\mI - \mP_\w)\hat{\vy} \approx  (1-\eta)\frac{1}{\sqrt{\hat{n}}}\hat{\vy}  - \sqrt{\eta(1-\eta)}\frac{1}{\sqrt{\hat{n}}}\hat{\vzeta}
$$
The strong ceiling model's error $\err_\sceiling \approx 0$ as its representations directly encode $y$ in the first coordinate. Thus, $\err_\wtos \approx \predgap{}$. By Thm \ref{thm: main_theorem}, $\predgap{}\approx \mP_\s \vepsilon_\w$. Then, \looseness=-1
\begin{align}
\nonumber
& \err_\wtos \approx 
%\predgap{} \approx  \mP_\s \vepsilon_\w \approx  \\
%\nonumber
\underbrace{ \frac{1}{\hat{n}}\hat{\vy}\hat{\vy}^\top(1- \eta)\frac{1}{\sqrt{\hat{n}}}\hat{\vy}}_{\textbf{\footnotesize replicated}} ~~\underbrace{  -\frac{1}{\hat{n}}\hat{\vy}\hat{\vy}^\top \sqrt{\eta(1-\eta)}\frac{1}{\sqrt{\hat{n}}}\hat{\vzeta} }_{\textbf{\footnotesize avoided} \text{; $\approx 0$ since $\hat{\vzeta}\perp \hat{\vy}$ almost}}
\end{align}
The first term of the weak model's error, $(1 - \eta)\frac{1}{\sqrt{\hat{n}}}\hat{\vy}$, aligns with $\mP_\s$ which spans the strong model's principal kernel, and is therefore replicated by the W2S model. The second term, $-\sqrt{\eta(1 - \eta)}\frac{1}{\sqrt{\hat{n}}}\hat{\vzeta}$, is orthogonal to $\mP_\s$ and thus mitigated. Notably, $-\sqrt{\eta(1 - \eta)}\frac{1}{\sqrt{\hat{n}}}\hat{\vzeta}$ aligns with the strong model's non-principal kernel, which is highly isotropic ($\gamma_\s = \omega(\delta_\s)$), causing the corresponding errors to appear mimicked by the W2S model during finetuning. However, they do not manifest at test time. In other words, only errors within the span of the strong model's principal kernel are overfitted harmfully, while overfitting elsewhere remains benign. \looseness=-1



% Also, imagine if the strong model did not have \(\vnu\) in its second coordinate; it would replicate even fewer errors. Or, consider keeping \(\alpha = \alpha_1 + \alpha_2\) unchanged: if we decrease \(\alpha_1\) while increasing \(\alpha_2\), the weak model's error itself would remain unchanged, but the resulting weak-to-strong error would decrease. This demonstrates that increasing the error source uncorrelated with the strong model's principal representation does not harm W2SG, whereas increasing the source correlated with the strong model does, as it effectively increases the alignment of \(\mI - \mP_\w\) with \(\mP_\s\). (\todoblue{currently not so clear or might be wrong; may need to write down $\mP$})
