#Original OpenAI Paper: 

@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}

#Existing Theoretical Work: 

@article{lang2024theoretical,
  title={Theoretical Analysis of Weak-to-Strong Generalization},
  author={Lang, Hunter and Sontag, David and Vijayaraghavan, Aravindan},
  journal={arXiv preprint arXiv:2405.16043},
  year={2024}
}

@article{charikar2024quantifying,
  title={Quantifying the Gain in Weak-to-Strong Generalization},
  author={Charikar, Moses and Pabbaraju, Chirag and Shiragur, Kirankumar},
  journal={arXiv preprint arXiv:2405.15116},
  year={2024}
}

@article{somerstep2024statistical,
  title={A statistical framework for weak-to-strong generalization},
  author={Somerstep, Seamus and Polo, Felipe Maia and Banerjee, Moulinath and Ritov, Ya'acov and Yurochkin, Mikhail and Sun, Yuekai},
  journal={arXiv preprint arXiv:2405.16236},
  year={2024}
}

@article{wu2024provable,
  title={Provable weak-to-strong generalization via benign overfitting},
  author={Wu, David X and Sahai, Anant},
  journal={arXiv preprint arXiv:2410.04638},
  year={2024}
}

#Empirical Work: 

@article{liu2024co,
  title={Co-supervised learning: Improving weak-to-strong generalization with hierarchical mixture of experts},
  author={Liu, Yuejiang and Alahi, Alexandre},
  journal={arXiv preprint arXiv:2402.15505},
  year={2024}
}

@article{sang2024improving,
  title={Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning},
  author={Sang, Jitao and Wang, Yuhang and Zhang, Jing and Zhu, Yanxu and Kong, Chao and Ye, Junhong and Wei, Shuyu and Xiao, Jinlin},
  journal={arXiv preprint arXiv:2402.00667},
  year={2024}
}

@article{guo2024vision,
  title={Vision superalignment: Weak-to-strong generalization for vision foundation models},
  author={Guo, Jianyuan and Chen, Hanting and Wang, Chengcheng and Han, Kai and Xu, Chang and Wang, Yunhe},
  journal={arXiv preprint arXiv:2402.03749},
  year={2024}
}

#Other Relevant Theoretical Work: 

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge university press}
}

@inproceedings{wei2022more,
  title={More than a toy: Random matrix models predict how real-world neural representations generalize},
  author={Wei, Alexander and Hu, Wei and Steinhardt, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={23549--23588},
  year={2022},
  organization={PMLR}
}

@article{mobahi2020self,
  title={Self-distillation amplifies regularization in hilbert space},
  author={Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3351--3361},
  year={2020}
}
@article{davis1970rotation,
  title={The rotation of eigenvectors by a perturbation. III},
  author={Davis, Chandler and Kahan, William Morton},
  journal={SIAM Journal on Numerical Analysis},
  volume={7},
  number={1},
  pages={1--46},
  year={1970},
  publisher={SIAM}
}

@article{el2002inversion,
  title={Inversion error, condition number, and approximate inverses of uncertain matrices},
  author={El Ghaoui, Laurent},
  journal={Linear algebra and its applications},
  volume={343},
  pages={171--193},
  year={2002},
  publisher={Elsevier}
}

@article{demmel1992componentwise,
  title={The componentwise distance to the nearest singular matrix},
  author={Demmel, James},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={13},
  number={1},
  pages={10--19},
  year={1992},
  publisher={SIAM}
}

@inproceedings{malladi2023kernel,
  title={A kernel-based view of language model fine-tuning},
  author={Malladi, Sadhika and Wettig, Alexander and Yu, Dingli and Chen, Danqi and Arora, Sanjeev},
  booktitle={International Conference on Machine Learning},
  pages={23610--23641},
  year={2023},
  organization={PMLR}
}

@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gekhman2024does,
  title={Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?},
  author={Gekhman, Zorik and Yona, Gal and Aharoni, Roee and Eyal, Matan and Feder, Amir and Reichart, Roi and Herzig, Jonathan},
  journal={arXiv preprint arXiv:2405.05904},
  year={2024}
}

@article{muennighoff2022mteb,
  title={MTEB: Massive text embedding benchmark},
  author={Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  journal={arXiv preprint arXiv:2210.07316},
  year={2022}
}

@article{bunea2015sample,
  title={On the sample covariance matrix estimator of reduced effective rank population matrices, with applications to fPCA},
  author={Bunea, Florentina and Xiao, Luo},
  year={2015}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@inproceedings{shen2022data,
  title={Data augmentation as feature manipulation},
  author={Shen, Ruoqi and Bubeck, S{\'e}bastien and Gunasekar, Suriya},
  booktitle={International conference on machine learning},
  pages={19773--19808},
  year={2022},
  organization={PMLR}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

@article{brown2019guacamol,
  title={GuacaMol: benchmarking models for de novo molecular design},
  author={Brown, Nathan and Fiscato, Marco and Segler, Marwin HS and Vaucher, Alain C},
  journal={Journal of chemical information and modeling},
  volume={59},
  number={3},
  pages={1096--1108},
  year={2019},
  publisher={ACS Publications}
}

@article{fabian2020molecular,
  title={Molecular representation learning with language models and domain-relevant auxiliary tasks},
  author={Fabian, Benedek and Edlich, Thomas and Gaspar, H{\'e}l{\'e}na and Segler, Marwin and Meyers, Joshua and Fiscato, Marco and Ahmed, Mohamed},
  journal={arXiv preprint arXiv:2011.13230},
  year={2020}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}

@article{wu2018moleculenet,
  title={MoleculeNet: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}

@software{charleshen_2020_4054866,
  author       = {Charleshen},
  title        = {ChemBench: The molecule benchmarks and MolMapNet
                   datasets
                  },
  month        = sep,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0},
  doi          = {10.5281/zenodo.4054866},
  url          = {https://doi.org/10.5281/zenodo.4054866},
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{hendrycks2020aligning,
  title={Aligning ai with shared human values},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2008.02275},
  year={2020}
}

@article{huang2019cosmos,
  title={Cosmos QA: Machine reading comprehension with contextual commonsense reasoning},
  author={Huang, Lifu and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={arXiv preprint arXiv:1909.00277},
  year={2019}
}

@article{welbl2017crowdsourcing,
  title={Crowdsourcing multiple choice science questions},
  author={Welbl, Johannes and Liu, Nelson F and Gardner, Matt},
  journal={arXiv preprint arXiv:1707.06209},
  year={2017}
}

@inproceedings{mcauley2013hidden,
  title={Hidden factors and hidden topics: understanding rating dimensions with review text},
  author={McAuley, Julian and Leskovec, Jure},
  booktitle={Proceedings of the 7th ACM conference on Recommender systems},
  pages={165--172},
  year={2013}
}

@article{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@inproceedings{jacot2020implicit,
  title={Implicit regularization of random feature models},
  author={Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Cl{\'e}ment and Gabriel, Franck},
  booktitle={International Conference on Machine Learning},
  pages={4631--4640},
  year={2020},
  organization={PMLR}
}


@article{tropp2015introduction,
  title={An introduction to matrix concentration inequalities},
  author={Tropp, Joel A and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={1-2},
  pages={1--230},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{shin2024weak,
  title={Weak-to-Strong Generalization Through the Data-Centric Lens},
  author={Shin, Changho and Cooper, John and Sala, Frederic},
  journal={arXiv preprint arXiv:2412.03881},
  year={2024}
}

@inproceedings{guo2018curriculumnet,
  title={Curriculumnet: Weakly supervised learning from large-scale web images},
  author={Guo, Sheng and Huang, Weilin and Zhang, Haozhi and Zhuang, Chenfan and Dong, Dengke and Scott, Matthew R and Huang, Dinglong},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={135--150},
  year={2018}
}

@inproceedings{jiang2018mentornet,
  title={Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels},
  author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International conference on machine learning},
  pages={2304--2313},
  year={2018},
  organization={PMLR}
}

@inproceedings{ratner2017snorkel,
  title={Snorkel: Rapid training data creation with weak supervision},
  author={Ratner, Alexander and Bach, Stephen H and Ehrenberg, Henry and Fries, Jason and Wu, Sen and R{\'e}, Christopher},
  booktitle={Proceedings of the VLDB endowment. International conference on very large data bases},
  volume={11},
  number={3},
  pages={269},
  year={2017},
  organization={NIH Public Access}
}

@inproceedings{patrini2017making,
  title={Making deep neural networks robust to label noise: A loss correction approach},
  author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1944--1952},
  year={2017}
}

@inproceedings{goldberger2017training,
  title={Training deep neural-networks using a noise adaptation layer},
  author={Goldberger, Jacob and Ben-Reuven, Ehud},
  booktitle={International conference on learning representations},
  year={2017}
}

@inproceedings{xie2020self,
  title={Self-training with noisy student improves imagenet classification},
  author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10687--10698},
  year={2020}
}

@article{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{laine2016temporal,
  title={Temporal ensembling for semi-supervised learning},
  author={Laine, Samuli and Aila, Timo},
  journal={arXiv preprint arXiv:1610.02242},
  year={2016}
}

@article{huh2021low,
  title={The low-rank simplicity bias in deep networks},
  author={Huh, Minyoung and Mobahi, Hossein and Zhang, Richard and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},
  journal={arXiv preprint arXiv:2103.10427},
  year={2021}
}

% spiked cov: 

@inproceedings{pezeshki2022multi,
  title={Multi-scale feature learning dynamics: Insights for double descent},
  author={Pezeshki, Mohammad and Mitra, Amartya and Bengio, Yoshua and Lajoie, Guillaume},
  booktitle={International Conference on Machine Learning},
  pages={17669--17690},
  year={2022},
  organization={PMLR}
}

@inproceedings{nakada2023understanding,
  title={Understanding multimodal contrastive learning and incorporating unpaired data},
  author={Nakada, Ryumei and Gulluk, Halil Ibrahim and Deng, Zhun and Ji, Wenlong and Zou, James and Zhang, Linjun},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4348--4380},
  year={2023},
  organization={PMLR}
}

@article{ji2023power,
  title={The power of contrast for feature learning: A theoretical analysis},
  author={Ji, Wenlong and Deng, Zhun and Nakada, Ryumei and Zou, James and Zhang, Linjun},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={330},
  pages={1--78},
  year={2023}
}

@article{muthukumar2021classification,
  title={Classification vs regression in overparameterized regimes: Does the loss function matter?},
  author={Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={222},
  pages={1--69},
  year={2021}
}

@article{johnstone2001distribution,
  title={On the distribution of the largest eigenvalue in principal components analysis},
  author={Johnstone, Iain M},
  journal={The Annals of statistics},
  volume={29},
  number={2},
  pages={295--327},
  year={2001},
  publisher={Institute of Mathematical Statistics}
}


% sparse coding

@article{papyan2017convolutional,
  title={Convolutional neural networks analyzed via convolutional sparse coding},
  author={Papyan, Vardan and Romano, Yaniv and Elad, Michael},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={83},
  pages={1--52},
  year={2017}
}

@article{arora2018linear,
  title={Linear algebraic structure of word senses, with applications to polysemy},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={483--495},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{olshausen1997sparse,
  title={Sparse coding with an overcomplete basis set: A strategy employed by V1?},
  author={Olshausen, Bruno A and Field, David J},
  journal={Vision research},
  volume={37},
  number={23},
  pages={3311--3325},
  year={1997},
  publisher={Elsevier}
}

@article{foldiak2003sparse,
  title={Sparse coding in the primate cortex},
  author={Foldiak, Peter},
  journal={The handbook of brain theory and neural networks},
  year={2003},
  publisher={MIT press}
}

@article{mairal2014sparse,
  title={Sparse modeling for image and vision processing},
  author={Mairal, Julien and Bach, Francis and Ponce, Jean and others},
  journal={Foundations and Trends{\textregistered} in Computer Graphics and Vision},
  volume={8},
  number={2-3},
  pages={85--283},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@inproceedings{yang2009linear,
  title={Linear spatial pyramid matching using sparse coding for image classification},
  author={Yang, Jianchao and Yu, Kai and Gong, Yihong and Huang, Thomas},
  booktitle={2009 IEEE Conference on computer vision and pattern recognition},
  pages={1794--1801},
  year={2009},
  organization={IEEE}
}

@article{olshausen2004sparse,
  title={Sparse coding of sensory inputs},
  author={Olshausen, Bruno A and Field, David J},
  journal={Current opinion in neurobiology},
  volume={14},
  number={4},
  pages={481--487},
  year={2004},
  publisher={Elsevier}
}

% sparse coding, theory

@article{kalimeris2019sgd,
  title={Sgd on neural networks learns functions of increasing complexity},
  author={Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{wen2021toward,
  title={Toward understanding the feature learning process of self-supervised contrastive learning},
  author={Wen, Zixin and Li, Yuanzhi},
  booktitle={International Conference on Machine Learning},
  pages={11112--11122},
  year={2021},
  organization={PMLR}
}

@article{allen2020towards,
  title={Towards understanding ensemble, knowledge distillation and self-distillation in deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2012.09816},
  year={2020}
}

@article{zou2021understanding,
  title={Understanding the generalization of adam in learning neural networks with proper regularization},
  author={Zou, Difan and Cao, Yuan and Li, Yuanzhi and Gu, Quanquan},
  journal={arXiv preprint arXiv:2108.11371},
  year={2021}
}

@inproceedings{xue2023features,
  title={Which features are learnt by contrastive learning? On the role of simplicity bias in class collapse and feature suppression},
  author={Xue, Yihao and Joshi, Siddharth and Gan, Eric and Chen, Pin-Yu and Mirzasoleiman, Baharan},
  booktitle={International Conference on Machine Learning},
  pages={38938--38970},
  year={2023},
  organization={PMLR}
}

% benignoverfitting

@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Acad Sciences}
}

% benign overfitting
@article{wang2021benign,
  title={Benign overfitting in multiclass classification: All roads lead to interpolation},
  author={Wang, Ke and Muthukumar, Vidya and Thrampoulidis, Christos},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24164--24179},
  year={2021}
}

@inproceedings{frei2022benign,
  title={Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data},
  author={Frei, Spencer and Chatterji, Niladri S and Bartlett, Peter},
  booktitle={Conference on Learning Theory},
  pages={2668--2703},
  year={2022},
  organization={PMLR}
}

@article{mallinar2022benign,
  title={Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting},
  author={Mallinar, Neil and Simon, James and Abedsoltan, Amirhesam and Pandit, Parthe and Belkin, Misha and Nakkiran, Preetum},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1182--1195},
  year={2022}
}

%%%%%%
@article{nanda2023emergent,
  title={Emergent linear representations in world models of self-supervised sequence models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2309.00941},
  year={2023}
}

@article{marks2023geometry,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{lee2024nv,
  title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},
  author={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2405.17428},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

