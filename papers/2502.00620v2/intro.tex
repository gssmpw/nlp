\section{Introduction}

As AI systems become increasingly capable of performing complex tasks beyond human comprehension, humans will inevitably serve as ``weak supervisors" in aligning advanced AI. To investigate this fundamental problem, \citet{burns2023weak} propose an analogy that can be empirically explored today: can a weak model effectively supervise a stronger one? This framework, known as Weak-to-Strong Generalization (W2SG), involves leveraging a weak model, finetuned on a specific task, to supervise the finetuning of a stronger model. 
In this analogy, the finetuning task represents concepts tied to human values or skills, the finetuned weak model represents humansâ€”limited in capability but aligned with human values, and the strong model represents superhuman intelligence--powerful but initially unaligned. Promising results from \cite{burns2023weak} show that the strong model can significantly outperform its weak supervisor. For instance, a GPT-4 model supervised by a fine-tuned GPT-2-level model achieves nearly 20\% better performance than the weak supervisor on NLP tasks.\looseness=-1

%most of the strong model's capabilities can be elicited under weak supervision \ba{why don't you explicitly say strong outperforms weak?}. For instance, a GPT-4 model supervised by a finetuned GPT-2-level model can often perform at a level between GPT-3 and GPT-3.5 on many \ba{same} tasks.}
\looseness=-1

At first glance, this phenomenon seems counterintuitive. After all, the strong model is explicitly trained to fit the weak supervision. Yet, it goes beyond mere imitation and generalizes better. It is important to understand which intrinsic properties of the weak and strong models enable W2SG. \looseness=-1

{Efforts have been made toward a theoretical understanding of W2SG. \citet{charikar2024quantifying} demonstrates that the disagreement between finetuned weak and strong models correlates with performance gains in W2SG. However, their analysis assumes high-quality representations in the strong model and does not address the role of the weak model's representations. The analysis of \cite{lang2024theoretical,shin2024weak} assumes a generalized version of an adversarially robust strong model, where W2SG arises solely from underfitting weak supervision. This framework excludes important scenarios such as benign overfitting, where W2SG occurs despite overfitting. \citet{wu2024provable} %later 
particularly studied benign overfitting and examined the impact of number of weakly labeled data points. However, we still lack an overarching explanation that captures the interaction between weak and strong models in enabling W2SG, as well as how it determines which weak supervision errors are corrected in general scenarios. The challenge lies in characterizing the abstract concepts including the knowledge embedded in the weak and strong models, their utilization, and their respective roles in W2SG. Striving for results that are general enough to capture a spectrum of behaviors without overly strict assumptions further adds to the complexity.}\looseness=-1

% \blue{Efforts to investigate W2SG have explored several aspects, including how local data structures enable adversarially robust strong models to avoid fitting mistakes in weak supervision \cite{lang2024theoretical,shin2024weak}, how performance gain correlates with the misfit between the weak model and the strong model with high-quality representations \cite{charikar2024quantifying}, and how the number of weakly labeled data points impact benign overfitting in W2SG \cite{wu2024provable}. However, a critical question remains unaddressed: what interaction between the weak and strong models enables W2SG, and how does it determine which weak supervision errors are corrected--even when the strong model has the capacity to overfit the weak labels (as opposed to the scenario in \cite{lang2024theoretical} where error correction relies on underfitting). Addressing this question is challenging, as it involves characterizing abstract concepts, including the knowledge embedded in the weak and strong models, their utilization, and their respective roles in W2SG. Furthermore, the result must be general enough to avoid overly strict restrictions on model quality or capacity, capturing a spectrum of behaviors that reflect the nuances of real-world scenarios. Balancing these goals adds significant complexity.
% }\looseness=-1

To address this, we adopt a representation-based perspective, analyzing finetuning as a process of learning a function on fixed representations to uncover how the internal structures of weak and strong models influence W2SG. Under a very general assumption about the representations, we demonstrate (illustrated in Fig. \ref{fig: illustration}) that the key quantifiable property governing W2SG is the overlap between two spaces: one representing what the weak model's \emph{principal representations} (capturing key knowledge gained during pretraining) do not cover, and the other representing what the strong model's \emph{principal representations} do cover. Errors in weak supervision that fall within this overlap hinder the strong model from reaching its full potential, leading to a prediction gap between the strong model finetuned with weak supervision and that finetuned with ground truth labels.
%prediction gap between the W2S model (the strong model finetuned with weak supervision) and the strong ceiling model (the strong model finetuned with ground truth labels). 
A smaller overlap implies that fewer of the weak model's mistakes are replicated, resulting in better W2SG performance.\looseness=-1

\begin{figure}[!t]
%\vspace{-.3cm}
    \centering
\includegraphics[width=0.5\linewidth]{figures/illustration.pdf}
    %\vspace{-.3cm}
    \caption{An illustration of our main result (Thm. \ref{thm: main_theorem}).
The path connecting the two highlighted regions represents the overlap b/w the complement of a scaled span of the weak model's \emph{principal kernel} and the scaled span of the strong model's \emph{principal kernel}, determining the contribution of the weak model's errors to \predgap{}.\looseness=-1
}
    \label{fig: illustration}
    %\vspace{-.7cm}
\end{figure}


We then demonstrate an important use case of our main result: explaining \emph{benign overfitting}, where the W2S model overfits the weak model's mistakes on finetuning data yet paradoxically generalizes better on the test set. Using our theoretical framework, we establish a general condition for benign overfitting and apply it to a toy example to concretely illustrate the role of representations in error replication: errors that do not align with the kernel defined by the strong model's principal representations are not replicated by the W2S model, regardless of the extent of overfitting. \looseness=-1 
%By uncovering these nuanced details and providing a clear, overarching explanation, our work distinguishes itself from prior research on related topics \cite{wu2024provable}. \looseness=-1


Our theory offers a metric that predicts trends in W2SG performance in practice \emph{without having the finetuning task labels}. This metric, which measures the overlap between the two highlighted regions in Fig. \ref{fig: illustration}, shows a strong correlation with W2SG performance across various settings.  The extensive experiments across 8 datasets, involving 150 small transformers and 52 LLMs, 
%\ba{across x models, y datasets etc confirm our results!} 
not only validate our theoretical insights but also suggest their potential applications in managing W2SG, providing a deeper understanding of LLM behavior through their internal representation structures.\looseness=-1 

%It shows a strong correlation with W2SG performance when different weak models are used for supervision. Our experiments covers molecular prediction and various NLP tasks, using models that include small transformers, 23 embedding LLMs, and 29 general-purpose LLMs. Remarkably, even when the full model is fine-tuned and representations are not explicitly defined, the metrics still show a strong correlation with W2SG performance when the LLM's activation maps are heuristically treated as representations. The extensive experiments not only validate our theoretical insights but also underscore their potential applications in AI alignment and safety. Practically, our findings could inspire methods for managing W2SG and provide deeper insights into LLM behavior through their internal representation structure. \looseness=-1

