\section{Related Work}
%In this section, we comprehensively review the existing work on the latent space of generative models and the interpretability of diffusion models. 

\textbf{Image Manipulation in Diffusion Models.} The mainstream approaches~\cite{huang2024diffusion}, which manipulate the styles, poses or semantic contents of the generated images, are categorized as training-based methods, test-time fine-tuning methods, and training and fine-tuning free methods. A typical training-based approach~\cite{kim2022diffusionclip, wang2023stylediffusion, huang2024diffstyler} introduces a pre-trained classifier (e.g., CLIP~\cite{radford2021learning}) as guidance to adjust the gradient during the diffusion process. Another fine-tuning approach attempts to fine-tune the entire diffusion model~\cite{valevski2023unitune, choi2023custom, huang2023kv}, optimize the latent codes~\cite{mou2023dragondiffusion, shi2024dragdiffusion, nam2024contrastive, yang2023magicremover} or the text-based embedding~\cite{wu2023uncovering, dong2023prompt} to manipulate the output contents. For example, Imagic~\cite{kawar2023imagic} employs a hybrid fine-tuning method to achieve non-rigid text-based image editing by finding a representative latent for the target image. Some of the training and fine-tuning free methods~\cite{kim2023user, elarabawy2022direct, huberman2024edit, gholami2023diffusion, patashnik2023localizing, park2024shape} tried to modify the attention map or the cross-attention map to manipulate outputs. Most of these works modify the generated images in an implicit way, which is based on new training data and motivated intuitively. However, our proposed method based on latent space could be more efficient and explainable without complex extra data collection and model fine-tuning. %\Yanran{I am not sure about this claim} 

\textbf{Interpretable Diffusion Models.} Although the latent space is fundamentally crucial to image manipulation and synthesis, few works have taken in-depth investigations. Some existing works~\cite{choi2021ilvr,meng2108sdedit} attempted to add explicitly guidances into latent codes to manipulate the generation results. Kwon \textit{et al.}~\cite{kwon2022diffusion} instead discovered a feature map, denoted as $h$-space, in between the bottleneck of U-Net in DMs that shows semantic correlation with text embeddings from CLIP. It can be used to learn vectors for manipulating attributes in generated images. Following this idea, Li \textit{et al.}~\cite{li2024self} designed a self-supervised approach to learn vectors in this auxiliary space for the generation of gender fairness and safe content. Park \textit{et al.}~\cite{park2023understanding} attempt to discover the local basis vectors on the latent space for editing attributes, which relies on finding the principal singular vectors from the Jacobian matrix that bridge the latent space $\mathcal{X}$ and $h$-space. Their method needs manual interpretation to discern the impact of found basis vectors. Compared to them, our exploration is steered from this auxiliary space to the latent space $\mathcal{X}$ itself, and we find properties to hold across time steps and utilize them to perform efficient and versatile editing on generated images.


%found from mapping $h$-space to latent space $\mathcal{X}$ through a pullback metric, so that the visual attributes could be linearly edited by moving along the local basis vectors of $\mathcal{X}$ directly. They exclusively choose the principal singular value from the Jacobian matrix and rely on manual interpretation to discern the impact of each component on the editing process.