\section{Related work}
% In recent years, a major line of research in low-rank matrix sensing has focused on gradient-based methods and their variants based on Burer-Monteiro factorization \cite{tu2016low,zhuo2024computational,chen2015fast,sun2016guaranteed}. Early methods  demonstrated that, starting from an initial point obtained through spectral initialization \cite{chen2015fast,sun2016guaranteed}, which is close to the ground truth, the factorization gradient descent algorithm can converge to the optimal solution. In the past two to three years, some studies \cite{bhojanapalli2016global,zhang2019sharp,ge2016matrix,ge2017no} have shown that, under certain conditions, all local minima of the low-rank matrix sensing problem are also global minima. Consequently, global convergence with random initialization has become a research hotspot. We will introduce both aspects of local convergence and global convergence in the following discussion.

% In recent years, a major research direction in matrix sensing has been the development of fast and efficient non-convex algorithms, with the gradient descent algorithm, particularly the Burer-Monteiro (BM) factorization \cite{tu2016low,zhuo2024computational,chen2015fast,sun2016guaranteed} being a representative example. Despite the significant progress made in the study of the GD algorithm, it still performs poorly in cases of ill-conditioning and over-parameterization, which has led to extensive research efforts addressing these issues. Additionally, the initialization of GD has become another prominent research focus. We present a comparison of several works most relevant to our approach in Table 1. 

Recent research in matrix sensing has focused on fast non-convex algorithms, notably the Burer-Monteiro (BM) factorization \cite{tu2016low,zhuo2024computational,chen2015fast,sun2016guaranteed}. Despite progress, gradient descent (GD) struggles with ill-conditioning and over-parameterization, prompting extensive studies. Initialization of GD has also gained attention. We present a comparison of several works most relevant to our approach in Table 1.

% \textbf{Local convergence}\\
% In earlier years, a series of works \cite{tu2016low,tong2021accelerating,chen2015fast,li2018algorithmic} demonstrated that under the exact rank assumption, the factorized gradient descent method could converge to the ground truth at a linear rate. However, since it is difficult to obtain the rank of the matrix to be recovered in practice, recent research has focused on matrix recovery in the overestimated rank setting. \cite{li2018algorithmic} studied the matrix sensing problem under complete rank overestimation but did not provide statistical convergence rates. In response, \cite{zhuo2024computational} were the first to provide convergence rates and recovery errors for the factorized gradient descent (FGD) algorithm under spectral initialization in the over-parameterized setting. They proved that FGD converges at a linear rate in the exact-rank case, but at a sublinear rate in the over-parameterized case. To address this issue, \cite{zhang2021preconditioned} introduced the Preconditioned GD based on the scaled GD algorithm, achieving linear convergence under over-parameterization. Building on this, \cite{zhang2024fast} further studied matrix recovery in the presence of Gaussian noise. By introducing an exponentially decaying damping parameter, they accelerated the convergence rate in the noisy setting. 

% \textbf{Global convergence}
% Recently, many works have started to investigate the global convergence of the matrix sensing problem. \cite{gunasekar2017implicit} proved that for the over-parameterized matrix factorization model, gradient descent with small initialization and small step size converges to the minimum nuclear norm solution, even without explicit regularization. \cite{stoger2021small} revealed that in the noiseless case, gradient descent with small random initialization performs similarly to spectral initialization. Building on this,  
% \cite{ding2022validation} considered over-parameterized low-rank matrix recovery in the presence of Gaussian noise. Additionally, \cite{jin2023understanding} demonstrated that under small initialization, the gradient algorithm sequentially learns solutions of increasing ranks until the ground truth matrix is recovered. However, due to the small initialization, the convergence is slow. To address this issue, \cite{xu2023power} introduced the ScaledGD$(\lambda)$ algorithm, based on ScaledGD and Preconditioned GD, and proved that this algorithm converges at a linear rate. Nevertheless, the aforementioned methods only considered the recovery of symmetric positive semi-definite matrices. In response, \cite{soltanolkotabi2023implicit} further studied the recovery of general matrices under small initialization. Moreover, \cite{xiong2024how} demonstrated that asymmetric factorization can effectively accelerate the convergence rate and proposed a method that achieves convergence at a rate independent of the initialization scale in the exact rank case, significantly improving convergence speed. However, the issue of slow convergence in the over-parameterized case remains unsolved. For asymmetric factorization, a natural idea is to alternately update the two factor matrices. \cite{lee2023randomly} proposed using alternating least squares to solve the rank-1 matrix sensing problem and proved that the algorithm achieves a linear convergence rate under random initialization.

\textbf{Ill-conditioning}
Gradient-based methods are highly sensitive to the matrix condition number, and the iteration complexity of the GD algorithm increases linearly with the matrix condition number, i.e. $\mathcal{O}(\kappa \log1/\epsilon)$. As the condition number increases, the convergence rate of GD slows down significantly \cite{zheng2015convergent,zhang2023preconditioned}. In recent years, a series of studies have focused on addressing this issue using preconditioning methods \cite{mishra2012riemannian,wei2016guarantees,mishra2016riemannian,tanner2016low,tong2021accelerating,zhang2021preconditioned,zhang2023preconditioned,zhang2022accelerating,bian2023preconditioned,jia2024globally,jia2024preconditioning}. Most of these works rely on a good initial point and focus on local convergence analysis, while \cite{xu2023power,jia2024globally} analyze global convergence.


% Consequently, many studies have focused on addressing this issue \cite{mishra2012riemannian,wei2016guarantees,mishra2016riemannian}. \cite{tanner2016low} proposed an efficient alternating steepest descent (ASD) method and its scaled variant, ScaledASD, for the fixed rank matrix completion problem. \cite{tong2021accelerating} proposed a scaled gradient descent (ScaledGD) algorithm, applying it to various low-rank matrix estimation tasks such as matrix sensing, robust principal component analysis (RPCA), and matrix completion. Furthermore, \cite{tong2021low} introduced a scaled sub-gradient algorithm for solving non-convex and non-smooth low-rank matrix optimization problems. Moreover, \cite{zhang2022accelerating} further considered the problem of completing streaming data with extremely large condition numbers and proposed a preconditioned version of the stochastic gradient descent (SGD) algorithm, which significantly accelerates the convergence rate of the standard SGD algorithm.


\textbf{Over-parameterization} Earlier works \cite{tu2016low,tong2021accelerating,chen2015fast,li2018algorithmic} demonstrated that, under the exact rank assumption, gradient descent method could converge to the ground truth at a linear rate. However, since it is difficult to determine the exact rank of the matrix to be recovered in practice, recent research has focused on matrix recovery in the overestimated rank setting \cite{zhuo2024computational,li2018algorithmic,stoger2021small,soltanolkotabi2023implicit}. Over-parameterization, however, exacerbates the ill-conditioning of the problem, leading to slower convergence rates. Studies by \cite{zhang2021preconditioned,zhang2023preconditioned,xu2023power,cheng2024accelerating,jia2024globally} have explored the issue of slow convergence in over-parameterized settings, while \cite{jia2024globally} achieved Q-linear convergence from random initialization to the global optimal solution for the first time.


% \cite{li2018algorithmic} studied the matrix sensing problem under full rank overestimation but did not provide statistical convergence rates. In response, \cite{zhuo2024computational} were the first to provide convergence rates and recovery errors for the factorized gradient descent (FGD) algorithm under spectral initialization in the over-parameterized setting. They proved that FGD converges at a linear rate in the exact-rank case, but at a sublinear rate in the over-parameterized case. To address this issue, \cite{zhang2021preconditioned} introduced the Preconditioned GD based on the scaled GD algorithm, achieving linear convergence under over-parameterization. Building on this, \cite{zhang2024fast} further studied matrix recovery in the presence of Gaussian noise. By introducing an exponentially decaying damping parameter, they accelerated the convergence rate in the noisy setting. However, these methods only guarantee local convergence. Therefore, \cite{xu2023power} introduced the ScaledGD$(\lambda)$ algorithm, based on ScaledGD and Preconditioned GD, and proved that with a small initialization and a constant damping parameter, this algorithm converges globally at a linear rate.

\textbf{Initialization} Early methods demonstrated that, starting from an initial point obtained through spectral initialization \cite{chen2015fast,sun2016guaranteed}, which is close to the ground truth, gradient descent algorithm can converge to the optimal solution. In the past few years, some studies \cite{bhojanapalli2016global,zhang2019sharp,ge2016matrix,ge2017no,zhu2021global} have shown that, under certain conditions, all local minima of the low-rank matrix sensing problem are also global minima. As a result, global convergence with random initialization has become a prominent research focus \cite{jin2023understanding,ding2022validation,jiang2023algorithmic,soltanolkotabi2023implicit,chengradient,jia2024preconditioning,jia2024globally}. \cite{stoger2021small,soltanolkotabi2023implicit} revealed that, in the noiseless case, gradient descent with small random initialization performs similarly to spectral initialization.