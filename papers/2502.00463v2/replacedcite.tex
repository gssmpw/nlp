\section{Related work}
% In recent years, a major line of research in low-rank matrix sensing has focused on gradient-based methods and their variants based on Burer-Monteiro factorization ____. Early methods  demonstrated that, starting from an initial point obtained through spectral initialization ____, which is close to the ground truth, the factorization gradient descent algorithm can converge to the optimal solution. In the past two to three years, some studies ____ have shown that, under certain conditions, all local minima of the low-rank matrix sensing problem are also global minima. Consequently, global convergence with random initialization has become a research hotspot. We will introduce both aspects of local convergence and global convergence in the following discussion.

% In recent years, a major research direction in matrix sensing has been the development of fast and efficient non-convex algorithms, with the gradient descent algorithm, particularly the Burer-Monteiro (BM) factorization ____ being a representative example. Despite the significant progress made in the study of the GD algorithm, it still performs poorly in cases of ill-conditioning and over-parameterization, which has led to extensive research efforts addressing these issues. Additionally, the initialization of GD has become another prominent research focus. We present a comparison of several works most relevant to our approach in Table 1. 

Recent research in matrix sensing has focused on fast non-convex algorithms, notably the Burer-Monteiro (BM) factorization ____. Despite progress, gradient descent (GD) struggles with ill-conditioning and over-parameterization, prompting extensive studies. Initialization of GD has also gained attention. We present a comparison of several works most relevant to our approach in Table 1.

% \textbf{Local convergence}\\
% In earlier years, a series of works ____ demonstrated that under the exact rank assumption, the factorized gradient descent method could converge to the ground truth at a linear rate. However, since it is difficult to obtain the rank of the matrix to be recovered in practice, recent research has focused on matrix recovery in the overestimated rank setting. ____ studied the matrix sensing problem under complete rank overestimation but did not provide statistical convergence rates. In response, ____ were the first to provide convergence rates and recovery errors for the factorized gradient descent (FGD) algorithm under spectral initialization in the over-parameterized setting. They proved that FGD converges at a linear rate in the exact-rank case, but at a sublinear rate in the over-parameterized case. To address this issue, ____ introduced the Preconditioned GD based on the scaled GD algorithm, achieving linear convergence under over-parameterization. Building on this, ____ further studied matrix recovery in the presence of Gaussian noise. By introducing an exponentially decaying damping parameter, they accelerated the convergence rate in the noisy setting. 

% \textbf{Global convergence}
% Recently, many works have started to investigate the global convergence of the matrix sensing problem. ____ proved that for the over-parameterized matrix factorization model, gradient descent with small initialization and small step size converges to the minimum nuclear norm solution, even without explicit regularization. ____ revealed that in the noiseless case, gradient descent with small random initialization performs similarly to spectral initialization. Building on this,  
% ____ considered over-parameterized low-rank matrix recovery in the presence of Gaussian noise. Additionally, ____ demonstrated that under small initialization, the gradient algorithm sequentially learns solutions of increasing ranks until the ground truth matrix is recovered. However, due to the small initialization, the convergence is slow. To address this issue, ____ introduced the ScaledGD$(\lambda)$ algorithm, based on ScaledGD and Preconditioned GD, and proved that this algorithm converges at a linear rate. Nevertheless, the aforementioned methods only considered the recovery of symmetric positive semi-definite matrices. In response, ____ further studied the recovery of general matrices under small initialization. Moreover, ____ demonstrated that asymmetric factorization can effectively accelerate the convergence rate and proposed a method that achieves convergence at a rate independent of the initialization scale in the exact rank case, significantly improving convergence speed. However, the issue of slow convergence in the over-parameterized case remains unsolved. For asymmetric factorization, a natural idea is to alternately update the two factor matrices. ____ proposed using alternating least squares to solve the rank-1 matrix sensing problem and proved that the algorithm achieves a linear convergence rate under random initialization.

\textbf{Ill-conditioning}
Gradient-based methods are highly sensitive to the matrix condition number, and the iteration complexity of the GD algorithm increases linearly with the matrix condition number, i.e. $\mathcal{O}(\kappa \log1/\epsilon)$. As the condition number increases, the convergence rate of GD slows down significantly ____. In recent years, a series of studies have focused on addressing this issue using preconditioning methods ____. Most of these works rely on a good initial point and focus on local convergence analysis, while ____ analyze global convergence.


% Consequently, many studies have focused on addressing this issue ____. ____ proposed an efficient alternating steepest descent (ASD) method and its scaled variant, ScaledASD, for the fixed rank matrix completion problem. ____ proposed a scaled gradient descent (ScaledGD) algorithm, applying it to various low-rank matrix estimation tasks such as matrix sensing, robust principal component analysis (RPCA), and matrix completion. Furthermore, ____ introduced a scaled sub-gradient algorithm for solving non-convex and non-smooth low-rank matrix optimization problems. Moreover, ____ further considered the problem of completing streaming data with extremely large condition numbers and proposed a preconditioned version of the stochastic gradient descent (SGD) algorithm, which significantly accelerates the convergence rate of the standard SGD algorithm.


\textbf{Over-parameterization} Earlier works ____ demonstrated that, under the exact rank assumption, gradient descent method could converge to the ground truth at a linear rate. However, since it is difficult to determine the exact rank of the matrix to be recovered in practice, recent research has focused on matrix recovery in the overestimated rank setting ____. Over-parameterization, however, exacerbates the ill-conditioning of the problem, leading to slower convergence rates. Studies by ____ have explored the issue of slow convergence in over-parameterized settings, while ____ achieved Q-linear convergence from random initialization to the global optimal solution for the first time.


% ____ studied the matrix sensing problem under full rank overestimation but did not provide statistical convergence rates. In response, ____ were the first to provide convergence rates and recovery errors for the factorized gradient descent (FGD) algorithm under spectral initialization in the over-parameterized setting. They proved that FGD converges at a linear rate in the exact-rank case, but at a sublinear rate in the over-parameterized case. To address this issue, ____ introduced the Preconditioned GD based on the scaled GD algorithm, achieving linear convergence under over-parameterization. Building on this, ____ further studied matrix recovery in the presence of Gaussian noise. By introducing an exponentially decaying damping parameter, they accelerated the convergence rate in the noisy setting. However, these methods only guarantee local convergence. Therefore, ____ introduced the ScaledGD$(\lambda)$ algorithm, based on ScaledGD and Preconditioned GD, and proved that with a small initialization and a constant damping parameter, this algorithm converges globally at a linear rate.

\textbf{Initialization} Early methods demonstrated that, starting from an initial point obtained through spectral initialization ____, which is close to the ground truth, gradient descent algorithm can converge to the optimal solution. In the past few years, some studies ____ have shown that, under certain conditions, all local minima of the low-rank matrix sensing problem are also global minima. As a result, global convergence with random initialization has become a prominent research focus ____. ____ revealed that, in the noiseless case, gradient descent with small random initialization performs similarly to spectral initialization.