\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
% \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021
\usepackage{bm}


%% my package
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{makecell}
\usepackage{CJKutf8}
% \usepackage{appendix}
% package for lowwercase table caption
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatother
\usepackage{color}
\usepackage{hyperref}

%% my definition
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}
\newtheorem{proof}{Proof}

% my definition
\def\A{\mathcal{A}}
\def\B{\mathcal{B}}
\def\C{\mathcal{C}}
\def\D{\mathcal{D}}
\def\E{\mathcal{E}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\I{\mathcal{I}}
\def\J{\mathcal{J}}
\def\K{\mathcal{K}}
\def\L{\mathcal{L}}
\def\M{\mathfrak{M}}
\def\O{\mathcal{O}}
\def\Q{\mathcal{Q}}
\def\R{\mathcal{R}}
\def\S{\mathcal{S}}
\def\T{\mathcal{T}}
\def\U{\mathcal{U}}
\def\V{\mathcal{V}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\Z{\mathcal{Z}}
\def\N{\mathcal{N}}




\begin{document}
\begin{CJK}{UTF8}{gbsn}
\title{Efficient Over-parameterized Matrix Sensing from Noisy Measurements via Alternating Preconditioned Gradient Descent}


\author{Zhiyu Liu, Zhi Han, Yandong Tang, Hai Zhang, Shaojie Tang, Yao Wang
         % stops a space
\thanks{Zhiyu Liu is with the State Key
Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy
of Sciences, Shenyang 110016, P.R. China, and also with the University of Chinese Academy
of Sciences, Beijing 100049, China (email: liuzhiyu@sia.cn).}% <-this % stops a space
\thanks{Zhi Han, Yandong Tang are with the State Key
Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy
of Sciences, Shenyang 110016, P.R. China (email: hanzhi@sia.cn; ytang@sia.cn).}
\thanks{Hai Zhang is with the Department of Statistics, Northwest University, Xi'an 710000, P.R. China. (email: zhanghai@nwu.edu.cn)
}
\thanks{Shaojie Tang is with Department of Management Science and Systems, State University of New York at Buffalo (e-mail: shaojiet@buffalo.edu).}
\thanks{Yao Wang is with the Center for Intelligent Decision-making and Machine
Learning, School of Management, Xi’an Jiaotong University, Xi’an 710049,
P.R. China. (email: yao.s.wang@gmail.com).}
}

%The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
We consider the noisy matrix sensing problem in the over-parameterization setting, where the estimated rank $r$ is larger than the true rank $r_\star$. Specifically, our main objective is to recover a matrix $ X_\star \in \mathbb{R}^{n_1 \times n_2} $ with rank $ r_\star $ from noisy measurements using an over-parameterized factorized form $ LR^\top $, where $ L \in \mathbb{R}^{n_1 \times r}, \, R \in \mathbb{R}^{n_2 \times r} $ and $ \min\{n_1, n_2\} \ge r > r_\star $, with the true rank $ r_\star $ being unknown. Recently, preconditioning methods have been proposed to accelerate the convergence of matrix sensing problem compared to vanilla gradient descent, incorporating preconditioning terms $ (L^\top L + \lambda I)^{-1} $ and $ (R^\top R + \lambda I)^{-1} $ into the original gradient. However, these methods require careful tuning of the damping parameter $\lambda$ and are sensitive to initial points and step size. To address these limitations, we propose the alternating preconditioned gradient descent (APGD) algorithm, which alternately updates the two factor matrices, eliminating the need for the damping parameter and enabling faster convergence with larger step sizes. We theoretically prove that APGD achieves near-optimal error convergence at a linear rate, starting from arbitrary random initializations. Through extensive experiments, we validate our theoretical results and demonstrate that APGD outperforms other methods, achieving the fastest convergence rate. Notably, both our theoretical analysis and experimental results illustrate that APGD does not rely on the initialization procedure, making it more practical and versatile.
\end{abstract}

\section{Introduction}

Low-rank matrix sensing is a fundamental problem encountered in various fields, including image processing \cite{candes2011robust,li2019cloud}, phase retrieval \cite{vaswani2017low,nayer2021sample}, quantum tomography \cite{rambach2021robust}, among others. The primary objective is to recover a rank $r_\star$ matrix $ X_\star\in\mathbb{R}^{n_1\times n_2}(r_\star \ll \min\{n_1,n_2\})$ from corrupted linear measurements $\{(y_i,A_i)\}^m_{i=1}$ of the form 
\begin{equation}
    y_i=\langle A_i, X_\star\rangle + s_i , i=1,...,m, \label{equ:1.1}
\end{equation}
where $\{s_i\}^{m}_{i=1}$ denotes the unknown noise, which we assume to be sub-Gaussian with a variance proxy $\nu^2$. This model can be concisely expressed as 
$\bm{y}=\mathcal{A}(X_\star)+\bm{s}$, where $\mathcal{A}(\cdot): \mathbb{R}^{n_1 \times n_2}\mapsto \mathbb{R}^m$ denotes the measurement operator. A prevalent method for recovering the low-rank matrix $X_\star\in\mathbb{R}^{n_1\times n_2}$ based on $\bm{y}$ involves solving the following problem:
\begin{equation}
\underset{X\in\mathbb{R}^{n_1 \times n_2}}{\min} \operatorname{rank}(X),\ \ \operatorname{s.t.}\ \A(X)=\bm{y}. 
\end{equation} 
However, such an optimization problem is NP-hard due to the rank constraint. To address this challenge, researchers have proposed relaxing the rank constraint to a convex nuclear norm constraint \cite{recht2010guaranteed,candes2011tight,candes2012exact,candes2010power}. Although this kind of relaxation provides a tractable solution, it requires computing the matrix SVD, resulting in a significant increase in computational cost as the matrix size grows. To mitigate this computational overhead, a common approach is to decompose the matrix $X$ into a factorized form $LR^{\top}$, where $L\in\mathbb{R}^{n_1\times r},\ R\in\mathbb{R}^{n_2\times r}$, also known as the Burer-Monteiro method \cite{burer2003nonlinear,burer2005local}, and then solve the following problem:
\begin{equation}
    \underset{L \in\mathbb{R}^{n_1\times r},\ R \in\mathbb{R}^{n_2\times r}}{\arg \min } \frac{1}{2} \|\A(LR^\top)-\bm{y}\|_2^2.
\label{equ:3}
\end{equation}


This problem can be efficiently solved by the vanilla gradient descent (GD) method \cite{tu2016low,zhuo2024computational,bhojanapalli2016global,jin2023understanding}:
\begin{equation}
L_{t+1}=L_t-\eta\nabla_Lf(L_t,R_t),\ R_{t+1}=R_t-\eta\nabla_Rf(L_{t},R_t).
\notag
\end{equation}


Despite significant progress in the field of non-convex matrix sensing, three challenges remain for vanilla gradient descent:
\begin{itemize}

\item \textbf{Over-parameterization}
The Burer-Monteiro method requires estimating the rank of the target matrix $X_\star$. However, a significant challenge is that, in practice, accurately estimating the rank of the matrix to be recovered is difficult. Therefore, it is typically assumed that the estimated rank is slightly larger than the true rank, that is, a situation known as over-parameterization. Previous work has shown that even under over-parameterization, accurate recovery of the matrix is still possible. However, over-parameterization can severely degrade the convergence rate of gradient descent algorithms, resulting in sub-linear convergence \cite{zhang2021preconditioned,zhang2023preconditioned,zhuo2024computational}.
    
\item \textbf{Poor conditioning}
It is well known that gradient methods are susceptible to the condition number $\kappa$ of the target matrix, defined as the ratio of the largest to the smallest singular value. Previous studies \cite{zheng2015convergent} have shown that the number of iterations for gradient methods increases at least linearly with the condition number. Unfortunately, most practical datasets exhibit very large condition numbers. For instance, \cite{cloninger2014solving} notes that certain applications of matrix sensing can have condition numbers as high as \( \kappa = 10^{15} \), which can severely impact the practical application of GD.

\item \textbf{Specific initialization}
For vanilla gradient descent, initialization is a critical first step. Many previous studies \cite{zhuo2024computational,tong2021accelerating,zhang2021preconditioned} rely on spectral initialization to obtain a good initial point that is close to the true solution $X_\star$. However, spectral initialization methods require a large number of samples and incur significant computational overhead to obtain an initial point close to the true value. Recent research \cite{stoger2021small,lee2023randomly,xiong2024how} has shown that small random initialization can achieve results similar to spectral initialization. Nevertheless, small initialization typically requires a larger number of iterations and may be less practical in some big data applications.
\end{itemize}



\subsection{Preconditioning accelerates gradient descent}

In recent years, considerable attention has been given to addressing the aforementioned issues, with one key approach being the acceleration of vanilla GD convergence under over-parameterization and ill-conditioning through preconditioning techniques. Essentially, preconditioning methods enhance the original gradient by adding right preconditioners, similar to the approach used in quasi-Newton methods. However, unlike Newton's method, preconditioning methods avoid computing the inverse of the large Hessian matrix (which has dimensions \((n_1 + n_2)r \times (n_1 + n_2)r\)). Instead, they only need to compute the inverses of two \( r \times r \) matrices, thereby significantly reducing the computational overhead.



\cite{tong2021accelerating} proposed ScaledGD for solving the matrix recovery problem in the exact-parameterized case, as shown in Equation (\ref{equ:4}):  
\begin{equation}
\begin{aligned}
\operatorname{\textbf{ScaledGD}}\ \ &L_{t+1} = L_t -\eta \nabla_L f(L_t,R_t)\cdot (R_t^\top R_t)^{-1}\\
    &R_{t+1} = R_t -\eta \nabla_R f(L_t,R_t)\cdot (L_t^\top L_t)^{-1}.
\end{aligned}
\label{equ:4}
\end{equation}

To handle the over-parameterized case, several methods have been proposed, including ScaledGD$(\lambda)$ \cite{xu2023power}, PrecGD \cite{zhang2021preconditioned,cheng2024accelerating}, and NoisyPrecGD\footnote{A variant of PrecGD designed for noisy situations. For convenience, we refer to it as NoisyPrecGD.} \cite{zhang2024fast} as shown in the following Equation (\ref{equ:5}):
\begin{equation}
\begin{aligned}
    &   \operatorname{\textbf{ScaledGD($\lambda$)}}\ \	 L_{t+1} = L_t -\eta \nabla_L f(L_t)\cdot (L_t^\top L_t + \textcolor{red}{\lambda} I)^{-1} \\
	&\operatorname{\textbf{PrecGD}}\ \ \ \ \ \ \ \ \ 	\ L_{t+1} = L_t -\eta \nabla_L f(L_t)\cdot (L_t^\top L_t + \textcolor{red}{\lambda_t} I)^{-1} 
\label{equ:5}
\end{aligned}
\end{equation}
A common feature of these methods is the inclusion of an additional damping term $\lambda I$, and the use of symmetric positive semi-definite matrices \( X = LL^\top \). The difference lies in the selection of the damping parameter \( \lambda \). ScaledGD$(\lambda)$ requires \( \lambda \) to be a fixed, very small constant, while PrecGD requires \( \lambda \) to change dynamically, i.e., $\lambda_t = \Theta(\|L_tL_t^\top - X_\star\|_F)$. NoisyPrecGD \cite{zhang2024fast} points out that both of these methods fail in the presence of noise. To address this, they propose an exponential decay adjustment: \( \lambda_{\text{new}} = \beta \lambda \), where $0<\beta<1$.



However, these methods all require careful tuning of an appropriate \( \lambda \) to achieve optimal results, and also need specific initializations, such as spectral initialization or very small initialization. Additionally, they only consider symmetric positive semi-definite matrices, which is a simpler case. These limitations significantly hinder the practical applicability of the existing methods. This raises the following question: \textbf{Can we develop an algorithm that does not rely on the initial point or damping term, removes the symmetric positive semi-definite constraint, and still converges to near-optimal error at a linear rate?}



\begin{table*}[ht]
\caption{Comparison of related works in over-parameterized noisy matrix sensing. In the second column, the upper bounds or exact setting of step size in the previous work are listed. In the third column, 'local' refers to spectral initialization, 'small random' refers to initial values with a very small scale, and 'random' refers to initial values with a scale comparable to the ground truth.  The fourth column indicates whether the asymmetric factorization is considered. The fifth column refers to whether the non-symmetric decomposition case is considered, while the sixth column refers to whether the preconditioning method requires the damping parameter \(\lambda\). According to \cite{zhang2024fast}, $\frac{1}{L_1}=\min\left\{ \frac{L_{\delta}}{60\sqrt{2}(1+\delta)+25(1+\delta)^2},\frac{1}{7L_{\delta}} 
 \right\}$, where \( \delta \) is the rank-$2r$ RIP constant and \( L_\delta \) is some constants. It is clear that $\frac{1}{L_1}$ is a relatively small number.}
\centering
\begin{tabular}{ccccccc}
\hline
methods & step size & init.  & convergence rate & asymmetry & damping parameter \\ \hline

\cite{ding2022validation} & $\le \frac{1}{c\kappa^2 \sigma_1(X_\star)}$ & small random  & sub-linear   & \XSolidBrush  & \textbackslash \\ \hline
\cite{zhuo2024computational} & $= \frac{1}{100\sigma_1(X_\star)}$  & spectral & sub-linear & \XSolidBrush & \textbackslash \\ \hline
\cite{zhang2024fast} &  $\le \frac{1}{L_1}$  & spectral & linear & \XSolidBrush & \Checkmark\\ \hline
% \cite{cheng2024accelerating} & $\frac{1}{L_2}$  & local & linear & \Checkmark  & \Checkmark  \\ \hline 

ours &  $\le\frac{1}{1+\delta}$  & random & linear & \Checkmark  & \XSolidBrush \\ \hline

\end{tabular}
\label{table:1}
\end{table*}


\subsection{Alternating helps: damping-free preconditioner and large step size}

To address the aforementioned question, we propose using an alternating preconditioned gradient descent (APGD) method to solve the over-parameterized matrix sensing problem. Many previous works focused solely on the symmetric positive semi-definite case, which is often regarded as simpler because it involves only one matrix. Additionally, the favorable properties of symmetric positive semi-definite matrices can be leveraged to simplify the analysis.

However, we argue that asymmetric decomposition, compared to symmetric decomposition, offers the advantage of enabling more efficient and practical algorithm. This benefit arises from the alternating update. Specifically, after performing asymmetric decomposition on a given matrix, a natural approach is to alternately update the two matrices \cite{jain2013low,tanner2016low,lee2023randomly, gu2024low,ward2023convergence}. Notably, \cite{jia2024preconditioning} proved that alternating ScaledGD does not depend on a small step size, which has been a major inspiration for our work.
Inspired by \cite{jia2024globally,jia2024preconditioning}, we explore using alternating update and preconditioning to solve the noisy asymmetric matrix sensing problem. We show that, after applying alternating update, the damping parameter in the preconditioner becomes unnecessary,  the initialization constraints (spectral initialization or small initialization) imposed by other methods are also eliminated. See Table 1 for detailed comparisons with prior art.
\begin{theorem}(Informal)
For the noisy over-parameterized matrix sensing problem, under some mild assumptions, starting from a random initialization, APGD converges to the near-minimax error in a linear rate with high probability, i.e.,
$$
\|L_tR_t^\top-X_\star\|_F^2 \lesssim \max \left\{ q^{2t} \|L_0 R_0^\top - X_\star\|_F^2, \mathcal{E}_{opt} \right\},
$$
where $\mathcal{E}_{opt}=C_e \frac{\nu^2rn\log n }{m}$ and $n = \max \{n_1,n_2\}$.
\label{theorem:informal}
\end{theorem}
As shown in Theorem \ref{theorem:informal}, APGD does not depend on the initial point and can converge to near-optimal error at a linear rate from any initial points. 



\begin{algorithm}[h]
\caption{Alternating Preconditioned Gradient Descent (APGD)}
\label{algorithm}
\textbf{Input:} Observation $\{y_i,\A_i\}_{i=1}^m$, step size $\eta$, estimated rank $r$, initialization scale $c_1$\\
{\textbf{Initialization}: Let $L_0=c_1\widetilde{L_0}\in\mathbb{R}^{n_1 \times r}$, $R_0=c_1\widetilde{R_0}\in\mathbb{R}^{n_2 \times r}$, where the entries of $\widetilde{L_0},\ \widetilde{R_0}$ are i.i.d. Gaussian entries with distribution $\mathcal{N}(0,1)$}
\begin{algorithmic}[1] %[1] enables line numbers
\STATE \textbf{for} $t=0$ to $T-1$ \textbf{do}
\STATE \ \ \ \ \ $L_{t+1}=L_t-\eta\nabla_Lf(L_t,R_t)\cdot (R_t^\top R_t)^{\dagger}$
\STATE \ \ \ \ \ $R_{t+1}=R_t-\eta\nabla_Rf(L_{t+1},R_t)\cdot (L_{t+1}^\top L_{t+1})^\dagger$\\
 \ \ \ \ \  ($\dagger$ denotes the Moore-Penrose-Pseudo inverse)
\STATE \textbf{end for}
\STATE \textbf{return:} $X_T=L_TR_T^\top$
\end{algorithmic}
\end{algorithm}


We shall summarize the contributions of this paper as follows: 

\begin{itemize}
    \item We propose an alternating preconditioning algorithm for the noisy matrix sensing problem. Compared to other methods, APGD is insensitive to the initialization and does not rely on spectral initialization or infinitesimal initializations. Additionally, APGD does not require a damping term in the preconditioner, thus eliminating the need for parameter tuning. Moreover, APGD is less sensitive to the step size and can converge faster with larger step sizes. All these make APGD more practical and efficient than the previous methods. 
    
    \item 
    We analyze the global convergence properties of APGD  and prove that it converges to the near-optimal error at a linear rate from any initial point. Our analysis highlights that the advantage of APGD over other methods lies in the alternating update, which decomposes the optimization into two sub-problems. This reduces the Lipschitz constant for each subproblem, therefore allowing for larger step size. And the derived analysis framework has the potential to address other low rank matrix estimation problems.

    \item 
    We conduct a series of experiments demonstrating that APGD converges to near-optimal recovery error at the fastest rate compared with other works, and further possesses of better robustness against the choice of step size and initialization point. In addition, experiments on the noisy hyperspectral image completion task demonstrate that APGD can be applied to solve other similar problems.
    
   
\end{itemize}




\section{Related work}
% In recent years, a major line of research in low-rank matrix sensing has focused on gradient-based methods and their variants based on Burer-Monteiro factorization \cite{tu2016low,zhuo2024computational,chen2015fast,sun2016guaranteed}. Early methods  demonstrated that, starting from an initial point obtained through spectral initialization \cite{chen2015fast,sun2016guaranteed}, which is close to the ground truth, the factorization gradient descent algorithm can converge to the optimal solution. In the past two to three years, some studies \cite{bhojanapalli2016global,zhang2019sharp,ge2016matrix,ge2017no} have shown that, under certain conditions, all local minima of the low-rank matrix sensing problem are also global minima. Consequently, global convergence with random initialization has become a research hotspot. We will introduce both aspects of local convergence and global convergence in the following discussion.

% In recent years, a major research direction in matrix sensing has been the development of fast and efficient non-convex algorithms, with the gradient descent algorithm, particularly the Burer-Monteiro (BM) factorization \cite{tu2016low,zhuo2024computational,chen2015fast,sun2016guaranteed} being a representative example. Despite the significant progress made in the study of the GD algorithm, it still performs poorly in cases of ill-conditioning and over-parameterization, which has led to extensive research efforts addressing these issues. Additionally, the initialization of GD has become another prominent research focus. We present a comparison of several works most relevant to our approach in Table 1. 

Recent research in matrix sensing has focused on fast non-convex algorithms, notably the Burer-Monteiro (BM) factorization \cite{tu2016low,zhuo2024computational,chen2015fast,sun2016guaranteed}. Despite progress, gradient descent (GD) struggles with ill-conditioning and over-parameterization, prompting extensive studies. Initialization of GD has also gained attention. We present a comparison of several works most relevant to our approach in Table 1.

% \textbf{Local convergence}\\
% In earlier years, a series of works \cite{tu2016low,tong2021accelerating,chen2015fast,li2018algorithmic} demonstrated that under the exact rank assumption, the factorized gradient descent method could converge to the ground truth at a linear rate. However, since it is difficult to obtain the rank of the matrix to be recovered in practice, recent research has focused on matrix recovery in the overestimated rank setting. \cite{li2018algorithmic} studied the matrix sensing problem under complete rank overestimation but did not provide statistical convergence rates. In response, \cite{zhuo2024computational} were the first to provide convergence rates and recovery errors for the factorized gradient descent (FGD) algorithm under spectral initialization in the over-parameterized setting. They proved that FGD converges at a linear rate in the exact-rank case, but at a sublinear rate in the over-parameterized case. To address this issue, \cite{zhang2021preconditioned} introduced the Preconditioned GD based on the scaled GD algorithm, achieving linear convergence under over-parameterization. Building on this, \cite{zhang2024fast} further studied matrix recovery in the presence of Gaussian noise. By introducing an exponentially decaying damping parameter, they accelerated the convergence rate in the noisy setting. 

% \textbf{Global convergence}
% Recently, many works have started to investigate the global convergence of the matrix sensing problem. \cite{gunasekar2017implicit} proved that for the over-parameterized matrix factorization model, gradient descent with small initialization and small step size converges to the minimum nuclear norm solution, even without explicit regularization. \cite{stoger2021small} revealed that in the noiseless case, gradient descent with small random initialization performs similarly to spectral initialization. Building on this,  
% \cite{ding2022validation} considered over-parameterized low-rank matrix recovery in the presence of Gaussian noise. Additionally, \cite{jin2023understanding} demonstrated that under small initialization, the gradient algorithm sequentially learns solutions of increasing ranks until the ground truth matrix is recovered. However, due to the small initialization, the convergence is slow. To address this issue, \cite{xu2023power} introduced the ScaledGD$(\lambda)$ algorithm, based on ScaledGD and Preconditioned GD, and proved that this algorithm converges at a linear rate. Nevertheless, the aforementioned methods only considered the recovery of symmetric positive semi-definite matrices. In response, \cite{soltanolkotabi2023implicit} further studied the recovery of general matrices under small initialization. Moreover, \cite{xiong2024how} demonstrated that asymmetric factorization can effectively accelerate the convergence rate and proposed a method that achieves convergence at a rate independent of the initialization scale in the exact rank case, significantly improving convergence speed. However, the issue of slow convergence in the over-parameterized case remains unsolved. For asymmetric factorization, a natural idea is to alternately update the two factor matrices. \cite{lee2023randomly} proposed using alternating least squares to solve the rank-1 matrix sensing problem and proved that the algorithm achieves a linear convergence rate under random initialization.

\textbf{Ill-conditioning}
Gradient-based methods are highly sensitive to the matrix condition number, and the iteration complexity of the GD algorithm increases linearly with the matrix condition number, i.e. $\mathcal{O}(\kappa \log1/\epsilon)$. As the condition number increases, the convergence rate of GD slows down significantly \cite{zheng2015convergent,zhang2023preconditioned}. In recent years, a series of studies have focused on addressing this issue using preconditioning methods \cite{mishra2012riemannian,wei2016guarantees,mishra2016riemannian,tanner2016low,tong2021accelerating,zhang2021preconditioned,zhang2023preconditioned,zhang2022accelerating,bian2023preconditioned,jia2024globally,jia2024preconditioning}. Most of these works rely on a good initial point and focus on local convergence analysis, while \cite{xu2023power,jia2024globally} analyze global convergence.


% Consequently, many studies have focused on addressing this issue \cite{mishra2012riemannian,wei2016guarantees,mishra2016riemannian}. \cite{tanner2016low} proposed an efficient alternating steepest descent (ASD) method and its scaled variant, ScaledASD, for the fixed rank matrix completion problem. \cite{tong2021accelerating} proposed a scaled gradient descent (ScaledGD) algorithm, applying it to various low-rank matrix estimation tasks such as matrix sensing, robust principal component analysis (RPCA), and matrix completion. Furthermore, \cite{tong2021low} introduced a scaled sub-gradient algorithm for solving non-convex and non-smooth low-rank matrix optimization problems. Moreover, \cite{zhang2022accelerating} further considered the problem of completing streaming data with extremely large condition numbers and proposed a preconditioned version of the stochastic gradient descent (SGD) algorithm, which significantly accelerates the convergence rate of the standard SGD algorithm.


\textbf{Over-parameterization} Earlier works \cite{tu2016low,tong2021accelerating,chen2015fast,li2018algorithmic} demonstrated that, under the exact rank assumption, gradient descent method could converge to the ground truth at a linear rate. However, since it is difficult to determine the exact rank of the matrix to be recovered in practice, recent research has focused on matrix recovery in the overestimated rank setting \cite{zhuo2024computational,li2018algorithmic,stoger2021small,soltanolkotabi2023implicit}. Over-parameterization, however, exacerbates the ill-conditioning of the problem, leading to slower convergence rates. Studies by \cite{zhang2021preconditioned,zhang2023preconditioned,xu2023power,cheng2024accelerating,jia2024globally} have explored the issue of slow convergence in over-parameterized settings, while \cite{jia2024globally} achieved Q-linear convergence from random initialization to the global optimal solution for the first time.


% \cite{li2018algorithmic} studied the matrix sensing problem under full rank overestimation but did not provide statistical convergence rates. In response, \cite{zhuo2024computational} were the first to provide convergence rates and recovery errors for the factorized gradient descent (FGD) algorithm under spectral initialization in the over-parameterized setting. They proved that FGD converges at a linear rate in the exact-rank case, but at a sublinear rate in the over-parameterized case. To address this issue, \cite{zhang2021preconditioned} introduced the Preconditioned GD based on the scaled GD algorithm, achieving linear convergence under over-parameterization. Building on this, \cite{zhang2024fast} further studied matrix recovery in the presence of Gaussian noise. By introducing an exponentially decaying damping parameter, they accelerated the convergence rate in the noisy setting. However, these methods only guarantee local convergence. Therefore, \cite{xu2023power} introduced the ScaledGD$(\lambda)$ algorithm, based on ScaledGD and Preconditioned GD, and proved that with a small initialization and a constant damping parameter, this algorithm converges globally at a linear rate.

\textbf{Initialization} Early methods demonstrated that, starting from an initial point obtained through spectral initialization \cite{chen2015fast,sun2016guaranteed}, which is close to the ground truth, gradient descent algorithm can converge to the optimal solution. In the past few years, some studies \cite{bhojanapalli2016global,zhang2019sharp,ge2016matrix,ge2017no,zhu2021global} have shown that, under certain conditions, all local minima of the low-rank matrix sensing problem are also global minima. As a result, global convergence with random initialization has become a prominent research focus \cite{jin2023understanding,ding2022validation,jiang2023algorithmic,soltanolkotabi2023implicit,chengradient,jia2024preconditioning,jia2024globally}. \cite{stoger2021small,soltanolkotabi2023implicit} revealed that, in the noiseless case, gradient descent with small random initialization performs similarly to spectral initialization.




\section{Main results}
\subsection{Preliminaries}
\textbf{Notations} Singular values of a rank-$r$ matrix $X$ are donated as $\|X\|=\sigma_1(X)\ge\sigma_2(X)\ge\cdots \ge \sigma_r(X)>0$. We denote the condition number of $X$ as $\kappa=\sigma_1(X)/\sigma_r(X)$.
\begin{definition}(Restricted Isometry Property)
The linear map $\mathcal{A}$ is said to satisfies Restricted Isometry Property (RIP) with parameters $(r,\delta_r)$ if there exits constants $0\le \delta_r <1$ and $m>0$ such that for every rank-$r$ matrix $M$, it holds that
\begin{equation}
    (1-\delta_r)\|M\|_F^2 \le\|\A(M)\|_2^2 \le (1+\delta_r)\|M\|_F^2.
    \notag
\end{equation}
\end{definition}
\begin{lemma}
{If all the entries of the measurement matrices $\{A_i\}_{i=1}^m$ are (sub-)gaussian random variables with zero mean and variance $1/m$ and $m\ge D(n_1+n_2)r$, then the linear map $\A$ satisfies the restricted isometry property of rank $r$ with constant $\delta_r>0$ with probability exceeding $1-Ce^{-dm}$ for fixed constants $D,d>0$} \cite{candes2011tight}.
\label{lemma:1}
\end{lemma}

RIP is a widely used condition in the field of compressed sensing, which states that the operator \(\mathcal{A}(\cdot)\) approximately preserves distances between low-rank matrices. In the absence of noise, we can establish a direct relationship between the loss function and the recovery error. However, in the presence of noisy observations, the interference from noise prevents us from directly applying the RIP condition. Therefore, similar to \cite{zhang2024fast}, we utilize the following decomposition:
\begin{equation}
\begin{aligned}
&f(L_t,R_t) = \frac{1}{2} \| \mathcal{A}(L_t,R_t)-y \|_2^2 \\
&= \underbrace{\frac{1}{2}\| \mathcal{A}(L_t R_t^\top-X_\star) \|_2^2}_{f_c(L_t,R_t) }  + \frac{1}{2} \| s \|_2^2 - \frac{1}{2} \langle \A(L_tR_t^\top-X_\star), s \rangle. 
\end{aligned}
\end{equation}






Then, we can apply the RIP condition to derive the following inequality:
$$
(1-\delta_{r+r_\star})\|E_t\|_F^2 \le  f_c(L_t,R_t) \le (1+\delta_{r+r_\star})\|E_t\|_F^2,
$$
where $E_t=L_tR_t^\top - X_\star$.

\subsection{Main theorem}
Based on these preliminaries, we directly present the main result, with its detailed proof provided in the appendix \ref{sec:proof}.

\begin{theorem}
Assume that the linear map $\A$ satisfies the rank-$(r+r_\star)$ RIP with constant $\delta_{r+r_\star}<1$,and $0<\eta < \frac{1}{1+\delta_{r+r_\star}}$, then solving the over-parameterized and noisy matrix sensing problem (\ref{equ:3}) with algorithm 1, we have the flowing claim holds with high probability,
\begin{equation}
f_c(L_{t},R_{t}) \le (1-\eta_c)^2 f_c(L_{t-1},R_{t-1})
\notag
\end{equation}
and 
$$
\|L_tR_t^\top-X_\star\|_F^2 \le \max \left\{ C_4 q^{2t} \|L_0 R_0^\top - X_\star\|_F^2, C_3\mathcal{E}_{opt} \right\},
$$
where 
$C_3$ is some numerical constant only related to $\delta_{r+r_\star}$, $C_4=\frac{1+\delta_{r+r_\star}}{1-\delta_{r+r_\star}}$, $q=(1-\eta_c)$, $\mathcal{E}_{opt}= C_e \frac{\nu^2rn\log n }{m}$, $$
\eta_c = 2(1-\delta_{r+r_\star})\left ( \eta - \frac{\eta}{3}(1 + 2\eta(1+\delta_{r+r_\star})  ) \right).
$$ 
\label{main theorem}

\end{theorem}

\textbf{Recovery error}
Our recovery error $\mathcal{O}(\frac{\nu^2rn\log n }{m})$ is near-optimal up to a log factor, which is consistent with most existing works \cite{tu2016low,zhuo2024computational,zhang2024fast}. However, \cite{ding2022validation} proved that using small initializations, gradient descent can converge to the error of $\mathcal{O}(\nu^2 \kappa^2 \frac{r_\star n}{m})$. This error is independent of the over-rank $r$ and is optimal when the condition number is 1. However, in practical situations, the condition number is rarely equal to 1, and when it is large, the error becomes significant. In contrast, our error is independent of the condition number.


\textbf{Sample complexity}
The sample complexity of APGD is encapsulated in the assumption related to RIP. As shown in the result of Lemma \ref{lemma:1}, when the linear map satisfies the rank-\(r\) RIP condition with constant \( \delta_r \), the required number of samples is \( m \gtrsim nr \delta^{-2}\). Therefore, the sampling complexity depends on the rank and \( \delta \), and our sampling complexity is \( m \gtrsim n(r + r_\star) \). Although we rely on the rank-\( (r + r_\star) \) RIP, we do not require the RIP constant \( \delta_{r+r_\star} \) to be very small; it only needs to satisfy \( \delta_{r+r_\star} < 1 \). Therefore, overall, our required number of samples is relatively small. In contrast, other works typically require \( \delta \) to be very small. Although ScaledGD$(\lambda)$ relies only on the rank-\( (r_\star + 1) \) RIP, it requires the value of \( \delta_{r_\star + 1} \) to be very small, i.e., \( \delta_{r_\star + 1} \leq c_{\delta} r_\star^{-0.5} \kappa^{-C_\delta} \), where \( C_\delta \) is a sufficiently large constant and \( c_\delta \) is a sufficiently small constant. Similarly, for \cite{ding2022validation}, although they only rely on the rank-\( 2r_\star \) RIP, they also require \( \delta_{2r_\star} \leq c \kappa^{-2} r_\star^{-0.5} \). When the condition number \( \kappa \) is very large, \( \delta_{2r_\star} \) becomes extremely small, leading to very high sample complexity.


\textbf{Step size} APGD is highly robust to the step size; it only requires the step size to satisfy \( \eta< \frac{1}{1+\delta_{r+r_\star}} \). In contrast, other methods require the step size to be very small. In \cite{zhuo2024computational}, the step size is set to be $\eta=\frac{1}{100\sigma_1(X_\star)}$, which is a very small value. In \cite{ding2022validation}, the step size is set to be $\eta\le \frac{1}{c\kappa^2\sigma_1(X_\star)}$. When the condition number is large, the step size needs to be much smaller. In \cite{zhang2024fast}, the step size is set to be $\eta \le \min\left\{ \frac{L_{\delta}}{60\sqrt{2}(1+\delta+25(1+\delta)^2)},\frac{1}{7L_{\delta}} 
 \right\}$, which can easily be verified as a very small value. Therefore, APGD can converge with a larger step size, allowing it to converge faster than other methods.




\begin{remark}
\textbf{Comparison with NoisyPrecGD \cite{zhang2024fast} }
Similar to \cite{zhang2024fast}, we both consider the noisy matrix sensing problem and use preconditioning to accelerate the gradient descent. However, there are significant distinctions between our work and theirs, mainly in four aspects. First, both theoretically and experimentally, we prove that alternating update eliminate the need for a damping term, even in the presence of noise. This is a key difference from previous preconditioning-based methods, which emphasize the importance of balancing the damping parameter with the recovery error. Second, our algorithm and theory do not rely on obtaining a good initial point, whereas NoisyPrecGD requires the initial point to be very close to the true value. Third, through alternating update, APGD is more robust to the step size and can converge more quickly with larger step sizes. As a result, even with random initialization, it can still outperform NoisyPrecGD in terms of convergence rate. Finally, NoisyPrecGD is limited to symmetric positive semi-definite matrices, which restricts its practical applicability. In contrast, our method is applicable to any matrix. 
\end{remark}


\begin{remark} 
\textbf{Comparisiom with AGN \cite{jia2024globally}}
A closely related work is the approximated Gauss-Newton (AGN) method proposed by \cite{jia2024globally}, which achieves linear convergence to the optimal solution starting from a small random initialization. However, our method differs from AGN in the following three aspects: 1) We focus on the noisy matrix sensing problem, whereas AGN only considers the noiseless case; 
2) AGN relies on extremely small initialization, while our method imposes no specific requirements on the initialization scale;  
3) AGN has high sample complexity, similar to ScaledGD(\(\lambda\)), whereas our sample complexity is relatively lower.
\end{remark}



\section{Key idea and proof sketch}
% In this section, we first demonstrate the role of alternating update, which avoids the damping term in the preconditioner and enhances the algorithm's robustness to the step size. We then outline the steps to prove the global convergence of APGD, with the detailed proof provided in the appendix \ref{sec:proof}.

\subsection{The role of damping parameter $\lambda$ in previous works}
First, we examine why previous works \cite{zhang2021preconditioned,xu2023power,zhang2024fast} rely on the damping term \( \lambda I \). To address the slow convergence of gradient descent in the over-parameterized and large condition number cases, \cite{zhang2021preconditioned} introduced PrecGD, which accelerates convergence by adding a right preconditioner after the gradient. Based on the preconditioner \( P =  L^\top L + \lambda I \), they defined the corresponding local P-norm:
\begin{equation}
    \|X\|_P\overset{\operatorname{def}}{=}\|XP^{\frac{1}{2}}\|_F,\ \|X\|_P^*\overset{\operatorname{def}}{=}\|XP^{-\frac{1}{2}}\|_F.
\end{equation}
Using this, they derived an inequality similar to a Lipschitz condition: 
\begin{equation}
f(L-\eta D) \le f(L) -\eta \langle \nabla f(L), D \rangle +\frac{\eta^2 l_p}{2}\| D \|_P^2, 
\label{equ:8} 
\end{equation} where
$$
l_p = 2(1+\delta)\left[ 4+ \frac{2\|E_1\|_F+4\|D\|_P}{\lambda_r^2(L)+\lambda} + \left( \frac{\|D\|_P}{\lambda_r^2(L)+\lambda} \right)^2 \right]
$$
and we denote \( L L^\top - X_\star \) as \( E_1 \) for simplicity \footnote{Since previous works considers symmetric positive semi-definite matrix, we consider the same case here, where \( X \) has a symmetric decomposition \( X = L L^\top \), and the corresponding objective function is given by: $ \min   \|\A(LL^\top)-y\|_2^2.$}.

From the above inequality, we can observe that the smaller \( l_p \) is, the faster the algorithm converges. Moreover, from the definition of \( l_p \), we can see that the smaller \( l_p \) becomes, the larger \( \lambda \) must be. However, the convergence of the algorithm also depends on another inequality, namely the Polyak-Lojasiewicz inequality:
\begin{equation}
 \langle \nabla f(L), D \rangle \overset{(i)}{=} \|\nabla f(L)\|_P^* \ge \mu_P f(L),
\end{equation}
where $(i)$ using the assumption that $D=\nabla f(L)(L^\top L +\lambda I)^{-1}$. 
From this inequality, we see that larger \( \mu_P \) leads to faster the convergence. However, \cite{zhang2021preconditioned} proved that as \( \mu_P \) increases, \( \lambda \) must decrease. Combining these two inequalities, for PrecGD, \( \lambda \) must satisfy $\lambda = \Theta(\|L_t^\top L_t - X_\star\|_F).$

Next, let's analyze Equation (\ref{equ:8}) in detail to understand why \( l_p \) is related to \( \lambda \). We will derive Equation (\ref{equ:8}) step by step to understand this relationship. 

Let us proceed with the detailed derivation:
\begin{equation}
\begin{aligned}
& f(L-\eta D) = \left\| \mathcal{A}\left((L-\eta D)(L-\eta D)^\top-X_\star \right) \right\|_2^2 \\ 
&=   \|\A(E_1)\|_2^2 -  2 \langle \A(E_1), \A(LD^\top + DL^\top) \rangle \\
& + \| \A(LD^\top+DL^\top)\|_2^2 +  \langle \A(LD^\top + DL^\top), \A(DD^\top)\rangle \\
&-2\langle \A(E_1), \A(DD^\top)  \rangle + \| \A(DD^\top) \|_2^2 .
\end{aligned}
\end{equation}
From this expression, we can see that the quadratic term of the gradient, \( DD^\top \), is the term that makes \( l_p \) related to the damping parameter \( \lambda \). For example, for \( \A(DD^\top) \), we have:
\begin{equation}
\|\A(DD^\top)\|_2^2 \leq (1 + \delta)^2 \|D\|_F^4 \leq \frac{\|D\|_P^4}{\lambda_r^2(L) + \lambda}.
\label{equ:13}
\end{equation}
This shows that \( l_p \) becomes dependent on \( \lambda \) as the damping parameter influences the magnitude of the quadratic gradient term.

\subsection{How alternating helps: damping free and large step size}

As shown in Equation (\ref{equ:13}), the quadratic term of the gradient \( D \) is the reason why \( l_p \) depends on \( \lambda \). If we can avoid this term, then \( l_p \) would no longer depend on \( \lambda \). It is important to note that a similar issue arises for the non-symmetric decomposition \( X = L R^\top \), since GD synchronously updates the two factor matrices $L$ and $R$. Therefore, by alternating the updates of the two factor matrices, we can avoid the quadratic term in the gradient.

Based on Algorithm 1, we can derive the following Lemma for the noiseless case.
\begin{lemma}
For the noiseless matrix sensing problem, suppose that the linear map $\A(\cdot)$ satisfies the rank-($r+r_\star$) RIP with constant $\delta_{r+r_\star}$, then we have
\begin{equation}
\begin{aligned}
f_c(L_t -\eta D_L,R_t) & \le f(L_t,R_t) 
- \eta \langle  \nabla_L f(L_t,R_t), D_L\rangle \\
& + \frac{\eta^2l_p}{2} \| D_L (R_t^\top R_t)^{\frac{\dagger}{2}} \|_F^2 \\
f_c(L_{t+1},R_t-\eta D_R) & \le f(L_{t+1},R_t)  - \eta \langle  \nabla_R f(L_{t+1},R_t), D_R\rangle  \\
&  + \frac{\eta^2l_p}{2} \| D_R (L_{t+1}^\top L_{t+1})^{\frac{\dagger}{2}} \|_F^2
\end{aligned}
\notag
\end{equation}
where $l_p = 1+\delta_{r+r_\star}$.
\end{lemma}
From this lemma, we can see that for APGD, \( l_p \) is independent of the damping parameter. In other words, APGD does not require a damping parameter. This is one of the key advantages of APGD, as it avoids the need for careful tuning of the damping parameter, which is typically required in methods like PrecGD.  

Another advantage of APGD is its robustness to the step size. As is well known, the upper bound on the step size in gradient descent depends on the gradient Lipschitz constant \( L \), i.e., \( \eta \leq \frac{1}{L} \). In contrast, for other preconditioned methods, the value of \( L \) is typically very large, which results in a very small step size, as discussed in Section 3. However, for APGD, the step size only needs to satisfy \( \eta \leq \frac{1}{1 + \delta_{r + r_\star}} \). This means that even when \( \delta_{r + r_\star} \) approaches 1, APGD can still converge with a relatively large step size.

\subsection{Proof outline}
Based on the above analysis, we outline the proof of APGD convergence under noisy conditions. First, inspired by the work of \cite{zhang2021preconditioned,zhang2024fast} and \cite{cheng2024accelerating}, we introduce two local norms and their corresponding dual norms
\begin{equation}
\begin{aligned}
    &\|A\|_{R_t} \overset{\operatorname{def}}{=}\|AP_{R_t}^{\frac{1}{2}}\|_F,\ \|A\|^*_{R_t} \overset{\operatorname{def}}{=}\|AP_{R_t}^{\frac{\dagger}{2}}\|_F,\ P_{R_t}\overset{\operatorname{def}}{=} R_t^\top R_t, \\
    &\|A\|_{L_t} \overset{\operatorname{def}}{=}\|AP_{L_t}^{\frac{1}{2}}\|_F,\ \|A\|^*_{L_t} \overset{\operatorname{def}}{=}\|AP_{L_t}^{\frac{\dagger}{2}}\|_F,\ P_{L_t}\overset{\operatorname{def}}{=} L_t^\top L_t.
\end{aligned}
\notag
\end{equation}
 Using these norms, we derive a Lipschitz-like lemma.
 \begin{lemma}(Lipschitz-like inequality)
Suppose that we have $ \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*} \ge 3 \| \mathcal{A}^*(s)R_t \|_{P_{R_t}^*}$, $\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} \ge 3 \| \mathcal{A}^*(s)L_{t+1}^\top \|_{P_{L_{t+1}}^*}$, and $\A$ satisfies the rank-$(r+r_\star)$ RIP with constant $\delta_{r+r_\star}$, then we have  
\begin{equation}
\begin{aligned}
& f_c (L_{t+1},R_t) \le f_c(L_t,R_t) - C_2 \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*}\\ 
 &f_c (L_{t+1},R_{t+1}) \le f_c(L_{t+1},R_t) - C_2 \| \nabla_R f_c(L_{t+1},R_t) \|_{P_{L_{t+1}}^*}\\
\end{aligned}
\notag
\end{equation}
where $C_2=\eta - \frac{\eta}{3}(1 + 2 \eta (1+\delta_{r+r_\star}) )$.
\end{lemma}



\begin{figure*}[h]
\centering
\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=6cm,height=5.41cm]{figure/comparision_of_noisyPrecGD_k=2r_kappa=100_noisy.pdf}
\end{minipage}%
}
\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=6cm,height=5.41cm]{figure/comparision_of_noisyPrecGD_k=2r_kappa=100_noisy_verify_eta.pdf}
\end{minipage}%
}
\centering
\caption{Comparison of APGD and NoisyPrecGD. We set $n_1=n_2=20$, $r_\star=5$, $r=10$, $m=5n_1r$, $\kappa=100$. Initialization scale $c_1$ for APGD is 0.1. (a) Verify the convergence of APGD and NoisyPrecGD under different noise level. The step size of APGD is 0.9, and the step size of NoisyPrecGD is 0.5, which are both tuned for the best results. (b) Verify the robustness of step size, where $\nu=1e-5$.
}
\label{fig:3}
\end{figure*}


 
 
 The key difference between this lemma and the previous noise-free lemma is the inclusion of assumptions on the noise term $\{\| \mathcal{A}^*(s)R_t \|_{P_{R_t}^*}, \| \mathcal{A}^*(s)L_{t+1}^\top \|_{P_{L_{t+1}}^*}\}$ and the gradient term $\{\| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*},\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*}\}$. This new lemma demonstrates that when the gradient term dominates the noise term, APGD converges linearly. 

Next, we need to establish a lower bound for the gradient term, which leads to the following lemma.

\begin{lemma}
Suppose that the linear map $\A(\cdot)$ satisfy the rank-${r+r_\star}$ RIP with constant $\delta_{r+r_\star}$, then we have
\begin{equation}
\begin{aligned}
\|\nabla_L f_c (L_t,R_t)\|_{P_{R_t}^*}^2 &\ge 2(1-\delta_{r+r_\star})f_c (L_{t},R_t), \\
\|\nabla_R f_c (L_{t+1},R_t)\|_{P_{L_{t+1}}^*}^2 &\ge 2(1-\delta_{r+r_\star})f_c (L_{t+1},R_t).
\end{aligned}
\notag
\end{equation}
\label{lemma:4}
\end{lemma}
Combining these two lemmas, we can easily conclude that when the gradient term dominates the noise term, APGD converges linearly, i.e., 
\begin{equation}
\begin{aligned}
f_c(L_{t+1},R_{t+1})& \le (1-\eta_c)^2 f_c(L_t,R_t)\\
\|L_tR_t^\top-X_\star\|_F^2 &\le  C_4 q^{2t} \|L_0 R_0^\top - X_\star\|_F^2,
\end{aligned}
\end{equation}
where $C_4,\ q$ and $\eta_c$ are the same parameters as defined in Theorem \ref{main theorem}.

Next, we need to consider the case where the noise term is smaller than the gradient term. In this case, we can combine Lemma \ref{lemma:4} to derive 
\begin{equation}
\begin{aligned}
     &f_c (L_{t},R_t) \le \frac{1}{2(1-\delta_{r+r_\star})} \|\nabla_L f_c (L_t,R_t)\|_{P_{R_t}^*}^2, \\
 & f_c (L_{t+1},R_t) \le \frac{1}{2(1-\delta_{r+r_\star})} \|\nabla_R f_c (L_{t+1},R_t)\|_{P_{L_{t+1}}^*}^2. \\
\end{aligned}
\label{equ:13}
\end{equation}


Then, combining equation (\ref{equ:13}) and matrix concentration bounds, we can conclude that when the gradient term is smaller than the noise term, we have 
$$
\| L_tR_t^\top - X_\star \|_F^2 \le C_3 \mathcal{E}_{opt}.
$$
This is the general outline of the proof for Theorem 3. The detailed proof can be found in Appendix \ref{proof of the main results}.


\section{Experiments}
In this section, we present numerical experiments to validate the effectiveness of APGD. Our experimental results demonstrate that, for the noisy matrix recovery task, APGD does not require an additional damping term. Starting from a random initial point, APGD is able to converge at a linear rate to near-optimal recovery error, even in ill-conditioned and over-parameterized scenarios. Compared to NoisyPrecGD, which is the state-of-art preconditioning method for noisy matrix sensing, APGD converges faster, is more robust to step size variations, and does not rely on spectral initialization. Additionally, we compared APGD with GD initialized with small random values. Although GD achieves smaller recovery error, our method shows significantly faster recovery rates, making APGD more practical for real-world applications. Due to page constraints, the additional simulations and real-data experiments are presented in Appendix \ref{additional experiments}.

\textbf{Experimental setup} 
The target rank-$r_\star$ matrix $X_\star\in\mathbb{R}^{n_1\times n_2}$ with condition number $\kappa$ is generated as $X_\star=U_\star \Sigma V_\star^\top$, where $U_\star$ and $V_\star$ are both orthogonal matrix and $\Sigma$ is a diagonal matrix with condition number $\kappa$. The entries of the sensing matrix $A_i$ are sampled i.i.d from distribution $\mathcal{N}(0,\frac{1}{m})$. The entries of the noise $\textbf{s}$ are sampled i.i.d from distribution $\mathcal{N}(0,\nu^2)$. For APGD, the initialization follows the procedure outlined in Algorithm \ref{algorithm}, and for NoisyPrecGD, we use the spectral initialization in \cite{zhang2021preconditioned}.


\textbf{Comparison with \cite{zhang2024fast} } 
In Figure 1, we compare the performance of APGD and NoisyPrecGD under different noise levels and step sizes. The following conclusions can be drawn:
1. Under varying noise levels, APGD converges faster to nearly optimal recovery error than NoisyPrecGD. Despite NoisyPrecGD starting from a better initialization point, APGD, which starts from a random initialization, still outperforms NoisyPrecGD in terms of convergence rate.
2. The main reason for APGD's superior performance is its greater robustness to step size. APGD can converge with larger step sizes, while NoisyPrecGD requires smaller step sizes to avoid divergence.





\textbf{Comparison with \cite{ding2022validation} }
In Figure 2, we compare APGD with GD using small random initialization, as \cite{ding2022validation} demonstrated that GD with small random initialization can converge to the optimal error. From Figure 2, we observe that although GD achieves a smaller final error than APGD, its iteration complexity is significantly higher. The number of iterations required for GD is nearly 100 times greater than that of APGD. Therefore, APGD is more practical due to its faster convergence and tolerable recovery error.
\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=6.2cm,height=5.4cm]{figure/figure2.pdf}}
\caption{Comparing the performance of APGD and GD with small initializations under different noise levels. $n_1 = n_2 =20$, $r_\star=5$, $r=2r_\star$, $m=5n_1r$, $\kappa=100$. Initialization scale $c_1=1e-10$.}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure}


\textbf{Verify the initialization} Finally, we validate the sensitivity of APGD and vanilla GD to initialization. Theorem \ref{main theorem} shows that, regardless of initialization, APGD can converge to nearly the minimum error at a linear rate. As illustrated in Figure 3, APGD exhibits the same linear convergence rate and reaches the same error level under different initialization scales. However, vanilla GD is highly sensitive to the initialization scale. As shown in Figure 3, a large initialization scale results in sub-linear convergence, while an excessively small initialization scale requires more iterations to escape saddle points.


\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=6cm,height=5.4cm]{figure/figure3.pdf}}
\caption{Comparing the performance of APGD and GD with different initialization scale $c_1$. $n_1 = n_2 =20$, $r_\star=5$, $r=2r_\star$, $m=5n_1r$, $\kappa=100$. Noise level $\nu=1e-5$.}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure}



\section{Conclusion}
For the noisy matrix sensing problem, we propose the APGD algorithm to accelerate the convergence over vanilla GD in cases with large condition numbers and over-parameterization. Both theoretical and experimental results demonstrate that APGD converges to nearly optimal recovery error at a linear rate. A key advantage of APGD is that it eliminates the need for the damping term required by previous preconditioning methods, thereby avoiding the complicated process of parameter tuning. Moreover, APGD is robust to step sizes and could converge with larger step sizes, making it significantly faster than other related approaches. Additionally, APGD is initialization-independent and can converge to near optimal recovery error starting from any initial points. It is  also believed that APGD has the potential in dealing with other matrix estimation problems, such as robust matrix completion and matrix phase retrieval, which could be our future studies.

\bibliographystyle{IEEEtran}
\bibliography{reference}
\onecolumn
\appendix


\section{Proofs}
\label{sec:proof}
\subsection{Preliminaries}
 The main task of matrix sensing is to recover a rank $r_\star$ matrix $ X_\star=L_\star R_\star^\top\in\mathbb{R}^{n_1\times n_2}(r_\star\ll \min\{n_1,n_2\})$ from corrupted linear measurements $\{(y_i,A_i)\}^m_{i=1}$ of the form.
\begin{equation}
    y_i=\langle A_i, X_\star\rangle + s_i , i=1,...,m, 
\end{equation}
where $\{s_i\}^{m}_{i=1}$ are sub-gaussian noises of expectation 0 and variance $v^2$. This model can be written succinctly as $y=\mathcal{A}(X_\star)+\bm{s}$, where $\mathcal{A}:\mathbb{R}^{n_1 \times n_2}\to \mathbb{R}^m$ is the measurement operator. The optimization problem can be wrote as
\begin{equation}
f(L_t,R_t) = \frac{1}{2} \| \mathcal{A}(L_t R_t^\top)-y \|_2^2 = \underbrace{\| \mathcal{A}(L_t R_t^\top-X_\star) \|_2^2}_{f_c(L_t,R_t) }  + \frac{1}{2} \| s \|_2^2 - \frac{1}{2} \langle \A(L_tR_t^\top-X_\star), s \rangle. 
\end{equation}

To solve this problem, a commonly used method is gradient descent. However, this method experiences a significant reduction in convergence rate when the problem is over-parameterized or when the matrix condition number is large. Inspired by \cite{zhang2021preconditioned,zhang2023preconditioned}, we introduced two local norms and corresponding dual norms:
\begin{equation}
\begin{aligned}
    &\|A\|_{R_t} \overset{\operatorname{def}}{=}\|AP_{R_t}^{\dagger/2}\|_F,\ \|A\|^*_{R_t} \overset{\operatorname{def}}{=}\|AP_{R_t}^{\dagger/2}\|_F,\ P_{R_t}\overset{\operatorname{def}}{=} R_t^\top R_t, \\
    &\|A\|_{L_t} \overset{\operatorname{def}}{=}\|AP_{L_t}^{\dagger/2}\|_F,\ \|A\|^*_{L_t} \overset{\operatorname{def}}{=}\|AP_{L_t}^{\dagger/2}\|_F,\ P_{L_t}\overset{\operatorname{def}}{=} L_t^\top L_t.
\end{aligned}
\end{equation}
Based on these two preconditioners, we have the following algorithm.
\begin{algorithm}[h]
\caption{Alternating Preconditioned Gradient Descent (APGD)}
\label{algorithm}
\textbf{Input:} Observation $\{y_i,\A_i\}_{i=1}^m$, step size $\eta$, estimated rank $r$, initialization scale $c_1$\\
{\textbf{Initialization}: Let $L_0=c_1\widetilde{L_0}\in\mathbb{R}^{n_1 \times r}$, $R_0=c_1\widetilde{R_0}\in\mathbb{R}^{n_2 \times r}$, where the entries of $\widetilde{L_0},\ \widetilde{R_0}$ are i.i.d. Gaussian entries with distribution $\mathcal{N}(0,1)$}
\begin{algorithmic}[1] %[1] enables line numbers
\STATE \textbf{for} $t=0$ to $T-1$ \textbf{do}
\STATE \ \ \ \ \ $L_{t+1}=L_t-\eta\nabla_Lf(L_t,R_t)\cdot (R_t^\top R_t)^{\dagger}$
\STATE \ \ \ \ \ $R_{t+1}=R_t-\eta\nabla_Rf(L_{t+1},R_t)\cdot (L_{t+1}^\top L_{t+1})^{\dagger}$
\STATE \textbf{end for}
\STATE \textbf{return:} $X_T=L_TR_T^\top$
\end{algorithmic}
\end{algorithm}



\subsection{Proof of Lipschitz-like inequality}
\begin{lemma}(Lipschitz-like inequality)
Suppose that we have $ \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*} \ge 3 \| \mathcal{A}^*(s)R_t \|_{P_{R_t}^*}$, $\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} \ge 3 \| \mathcal{A}^*(s)L_{t+1}^\top \|_{P_{L_{t+1}}^*}$, and $\A$ satisfies the rank-$(r+r_\star)$ RIP with constant $\delta_{r+r_\star}$, then we have  

\begin{equation}
\begin{aligned}
& f_c (L_{t+1},R_t) \le f_c(L_t,R_t) - C_2 \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*},\\ 
 &f_c (L_{t+1},R_{t+1}) \le f_c(L_{t+1},R_t) - C_2 \| \nabla_R f_c(L_{t+1},R_t) \|_{P_{L_{t+1}}^*}, \\
\end{aligned}
\notag
\end{equation}
where $C_2=\eta - \frac{\eta}{3}(1 + 2\eta (1+\delta_{r+r_\star}) )$.
\label{lemma:a.1}
\end{lemma}


\begin{proof}
\begin{equation}
\begin{aligned}
f_c(L_{t+1},R_t) &= \frac{1}{2}\| \mathcal{A}(L_{t+1}R_t^\top-X_\star)\|_2^2 = \frac{1}{2}\| \mathcal{A}(L_{t}-\eta \nabla_L f(L_{t},R_t)(R_t^\top R_t)^\dagger )R_t^\top-X_\star)\|_2^2 \\
&= \frac{1}{2} \left \langle \mathcal{A}(L_tR_t^\top-X_\star)- \eta \mathcal{A}(\nabla_L f(L_{t},R_t)(R_t^\top R_t)^\dagger R_t^\top) , \mathcal{A}(L_tR_t^\top-X_\star)- \eta \mathcal{A}(\nabla_L f(L_{t},R_t)(R_t^\top R_t)^\dagger R_t^\top) \right \rangle \\
& =\underbrace{\frac{1}{2}\| \mathcal{A}(L_tR_t^\top-X_\star) \|_2^2}_{f_c(L_t,R_t)} + \underbrace{\frac{\eta ^2}{2} \|\mathcal{A}(\nabla_L f(L_{t},R_t)(R_t^\top R_t)^\dagger R_t^\top)\|_2^2}_{Z_1} \\
&-\underbrace{\eta\langle \mathcal{A}(L_tR_t^\top-X_\star), \mathcal{A}(\nabla_L f(L_{t},R_t)(R_t^\top R_t)^\dagger R_t^\top)\rangle }_{Z_2}. 
\end{aligned}
\end{equation}
For $Z_1$, we have
\begin{equation}
\begin{aligned}
Z_1 &\overset{(a)}{\le} \frac{\eta^2(1+\delta_{r+r_\star})}{2}\|\nabla_L f(L_{t},R_t)(R_t^\top R_t)^\dagger  R_t^\top\|_F^2 \\
& = \frac{\eta^2(1+\delta_{r+r_\star})}{2} \|\A^*(\A(L_tR_t^\top)-y)R_t(R_t^\top R_t)^{\dagger/2}  (R_t^\top R_t)^{\dagger/2}  R_t^\top \|_F^2\\
& \overset{(b)}{\le} \frac{\eta^2(1+\delta_{r+r_\star})}{2} \|\A^*(\A(L_tR_t^\top)-y)R_t(R_t^\top R_t)^{\dagger/2} \|_F^2 \|(R_t^\top R_t)^{\dagger/2}  R_t^\top\|_2^2 \\ 
& = \frac{\eta^2(1+\delta_{r+r_\star})}{2} \|\A^*(\A(L_tR_t^\top)-y)R_t(R_t^\top R_t)^{\dagger/2} \|_F^2 \\
& = \frac{\eta^2(1+\delta_{r+r_\star})}{2} \|\A^*(\A(L_tR_t^\top-X_\star)-s) R_t(R_t^\top R_t)^{\dagger/2} \|_F^2 \\
& \le \frac{\eta^2(1+\delta_{r+r_\star})}{2} \left( \|\A^*(\A(L_tR_t^\top-X_\star)) R_t(R_t^\top R_t)^{\dagger/2} \|_F^2 + \|\A^*(s) R_t(R_t^\top R_t)^{\dagger/2} \|_F^2 \right) \\
& = \frac{\eta^2(1+\delta_{r+r_\star})}{2} \left( \| \underbrace{\A^*(\A(L_tR_t^\top-X_\star)) R_t}_{\nabla_L f_c(L_t,R_t)} \|_{P_{R_t}^*}^2 + \| \A^*(s) R_t \|_{P_{R_t}^*}^2  \right),
\end{aligned}
\end{equation}
where $(a)$ follows the assumption that $\A$ satisfies the rank-$(r+r_\star)$ RIP; $(b)$ using the fact that $\|AB\|_F\le \|A\|_F\|B\|_2$.


For $Z_2$, we have
\begin{equation}
\begin{aligned}
Z_2 &= \eta \langle \nabla_Lf(L_{t+1},R_t)(R_t^\top R_t)^\dagger  R_t^\top ,  \A^*\mathcal{A}(L_tR_t^\top-X_\star)\rangle\\
& = \eta\langle  \A^*\mathcal{A}(L_tR_t^\top-X_\star) R_t(R_t^\top R_t)^\dagger  R_t^\top - \A^*(s)R_t(R_t^\top R_t)^\dagger  R_t^\top, \A^*\A (L_tR_t^\top-X_\star )\rangle \\
& = \eta\|\nabla_L f_c(L_t,R_t)\|^2_{P_{R_t}^* } - \eta\langle  \A^*(s)R_t(R_t^\top R_t)^\dagger  R_t^\top, \A^*\A (L_tR_t^\top-X_\star )\rangle \\
&=\eta\|\nabla_L f_c(L_t,R_t)\|^2_{P_{R_t}^* } - \eta\langle  \A^*(s)R_t(R_t^\top R_t)^{\dagger/2}  , \A^*\A (L_tR_t^\top-X_\star )R_t(R_t^\top R_t)^{\dagger/2} \rangle \\
&\ge \eta\|\nabla_L f_c(L_t,R_t)\|^2_{P_{R_t}^* } - \eta\|\nabla_L f_c(L_t,R_t)\|_{P_{R_t}^* } \|\A^*(s)R_t\|_{P_{R_t}^* }.
\end{aligned}
\end{equation}
Combining the bounds for $Z_1$ and $Z_2$, we get
\begin{equation}
\begin{aligned}
f_c(L_{t+1},R_t) & \le f_c(L_t,R_t) + \frac{\eta^2(1+\delta_{r+r_\star})}{2} \left( \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*}^2 + \| \A^*(s) R_t \|_{P_{R_t}^*}^2  \right) \\
& -\eta\|\nabla_L f_c(L_t,R_t)\|^2_{P_{R_t}^* } + \eta\|\nabla_L f_c(L_t,R_t)\|_{P_{R_t}^* } \|\A^*(s)R_t\|_{P_{R_t}^* }\\
&\overset{(a)}{\le} f_c(L_t,R_t) - \underbrace{\left( \eta - \frac{\eta}{3}(1 + 2\eta (1+\delta_{r+r_\star})  ) \right)}_{C_2} \|\nabla_L f_c(L_t,R_t)\|^2_{P_{R_t}^* },
\end{aligned}
\end{equation}
where $(a)$ use the assumption that $\| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*}\ge 3 \| \mathcal{A}^*(s)R_t \|_{P_{R_t}^*}$.

Similarly, for $f_c(L_{t+1},R_{t+1})$, we can also deduce that
\begin{equation}
f_c(L_{t+1},R_{t+1})  \le f_c(L_{t+1},R_t) - \left(\eta - \frac{\eta}{3}(1 + 2\eta(1+\delta_{r+r_\star})  )\right)  \| \nabla_R f_c(L_{t+1},R_t) \|^2_{P_{L_{t+1}}^*}.
\end{equation}
Therefore, we complete the proof of Lemma \ref{lemma:a.1}.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Proof of the gradient dominance}
\begin{lemma}
Suppose that the linear map $\A(\cdot)$ satisfy the $\delta_{r+r_\star}$-RIP, then we have
\begin{equation}
\begin{aligned}
\|\nabla_L f_c (L_t,R_t)\|_{P_{R_t}^*}^2 &\ge 2(1-\delta_{r+r_\star})f_c (L_{t},R_t), \\
\|\nabla_R f_c (L_{t+1},R_t)\|_{P_{L_{t+1}}^*}^2 &\ge 2(1-\delta_{r+r_\star})f_c (L_{t+1},R_t). \\
\end{aligned}
\end{equation}
\label{lemma:4.1}
\end{lemma}
\begin{proof}
\begin{equation}
\begin{aligned}
\|\nabla_L f_c (L_t,R_t)\|_{P_{R_t}^*}^2 &=\|\A^*\A(L_tR_t^\top-X_\star)R_t(R_t^{\top}R_t)^{\dagger/2}\|_F^2 \\
&\ge \sigma_{\min}^2\left(R_t(R_t^{\top}R_t)^{\dagger/2}\right)\|\A^*\A(L_tR_t^\top-X_\star)\|_F^2.
\end{aligned}
\end{equation}
For $\|\A^*\A(L_tR_t^\top-X_\star)\|_F$, we have
\begin{equation}
\begin{aligned}
    \|\A^*\A(L_tR_t^\top-X_\star)\|_F &= \underset{Y:\|Y\|_F\le 1}{\max}\langle \A^*\A(L_tR_t^\top-X_\star), Y\rangle\\
    &=\underset{Y:\|Y\|_F\le 1}{\max}\langle \A(L_tR_t^\top-X_\star), \A(Y)\rangle\\
    &\overset{(i)}{\ge} \langle \A(E_t), \A(\frac{E_t}{\|E_t\|_F})\rangle=\frac{\|\A(E_t)\|_2^2}{\|E_t\|_F}\\
    &\overset{(ii)}{\ge} \sqrt{(1-\delta_{r+r_\star})}\|\A(E_t)\|_2,
\end{aligned}
\end{equation}
where in $(i)$ we  denote $L_tR_t^{\top}-X_\star$ as $E_t$ for convenience and construct a specific $Y=\frac{E_t}{\|E_t\|_F}$; $(ii)$ using the fact that $\|E_t\|_F \le \frac{1}{\sqrt{1-\delta_{r+r_\star}}}\|\A(E_t)\|_2$.

Therefore, we have
\begin{equation}
\begin{aligned}
\|\nabla_L f_c (L_t,R_t)(R_t^{\top}R_t)^{\dagger/2}\|_F^2 &\ge 2\sigma_{\min}^2\left(R_t(R_t^{\top}R_t)^{\dagger/2}\right) (1-\delta_{r+r_\star}) f_c(L_t,R_t)\\
&\overset{(i)}{\ge} 2(1-\delta_{r+r_\star})f_c(L_t,R_t).
\end{aligned}
\end{equation}

By a similar approach, we can prove to obtain
\begin{equation}
\|\nabla_R f_c (L_{t+1},R_t)\|_{P_{L_{t+1}}^*}^2 \ge 2(1-\delta_{r+r_\star})f_c (L_{t+1},R_t)
\end{equation}
Thereby, we complete the proof of Lemma \ref{lemma:4.1}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of upper bound of the noise term}
\begin{lemma}
For linear map $\A$ with (sub-)Gaussian sensing matrix $\{A_i\}_{i=1}^m\in\mathbb{R}^{n_1\times n_2}$, assume each entries of $A_i$ are sampled from a (sub)-gaussian distribution with zero mean and $\frac{1}{m}$ variance, then we have the following holds with high probability:
\begin{equation}
    \|\A^*(s)R_t\|_{P_{R_t}^*}^2 \le \mathcal{E}_{opt},\ \  \|\A^*(s)L_t^\top\|_{P_{L_t}^*}^2 \le \mathcal{E}_{opt}, 
\end{equation}
where $\mathcal{E}_{opt}=C_e \frac{\nu^2rn\log n }{m}$ and $n=\max\{n_1,n_2\}$.
\label{lemma:noise}
\end{lemma}


\begin{proof}
\begin{equation}
\begin{aligned}
\|\A^*(s)R_t\|_{P_{R_t}^*}^2 &= \left \| \left(\sum_{i=1}^m s_i A_i \right) R_t(R_t^\top R_t)^{\dagger/2} \right \|_F^2 \\
&\le \left \| \sum_{i=1}^m s_i A_i \right \|_2^2 \left \| R_t(R_t^\top R_t)^{\dagger/2} \right\|_F^2 \\
&\overset{(a)}{\le}  r \left\| \sum_{i=1}^m s_i A_i \right\|_2^2 \  \overset{(b)}{\le} C_e \frac{\nu^2rn\log n }{m} = \mathcal{E}_{opt},
\end{aligned}
\end{equation}
where $(a)$ using the fact that $\left \| R_t(R_t^\top R_t)^{\dagger/2} \right\|_F^2 = \sum_i^r \frac{\sigma_i^2(R_t)}{\sigma_i^2(R_t)}=r$  ; (b) follows from the Lemma 16 in \cite{zhang2021preconditioned}.

The upper bound of $\|\A^*(s)L_{t}^\top\|_{P_{L_t}^*}^2$ can be obtained using a similar method.

\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of the main theorem}
\label{proof of the main results}
\begin{theorem}
Assume that the linear map $\A$ satisfies the rank-$(r+r_\star)$ RIP with constant $\delta_{r+r_\star}$,and $0<\eta < \frac{1}{(1+\delta_{r+r_\star})}$, then solving the over-parameterized and noisy matrix sensing problem with algorithm 1 leads to
\begin{equation}
f_c(L_{t+1},R_{t+1}) \le (1-\eta_c)^2 f_c(L_t,R_t)
\notag
\end{equation}
and 
$$
\|L_tR_t^\top-X_\star\|_F^2 \le \max \left\{ \frac{1+\delta_{r+r_\star}}{1-\delta_{r+r_\star}}(1-\eta_c)^{2t} \|L_0 R_0^\top - X_\star\|_F^2, C_3\mathcal{E}_{opt} \right\},
$$
where $\eta_c = 2(1-\delta_{r+r_\star})\left ( \eta - \frac{\eta}{3}(1 + 2\eta(1+\delta_{r+r_\star}) ) \right)$, $C_3$ is some numerical constant only related to $\delta_{r+r_\star}$.
\end{theorem}
\begin{proof}
Assuming that the assumptions in Theorem \ref{main theorem} hold, we can conclude that the assumptions in Lemmas \ref{lemma:a.1} and \ref{lemma:4.1} also hold. We then classify $ \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*},\ \|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} $ into four cases as follows:
\begin{itemize}
    \item (a): $ \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*} >  3\| \mathcal{A}^*(s) R_t \|_{P_{R_t}^*}$, and $\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} > 3\|\mathcal{A}^*(s)L_{t+1}^\top \|_{P_{L_{t+1}}^*}$
    \item (b): $ \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*} >  3\| \mathcal{A}^*(s) R_t \|_{P_{R_t}^*}$, and $\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} \le 3\|\mathcal{A}^*(s)L_{t+1}^\top \|_{P_{L_{t+1}}^*}$
    \item (c): $ \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*} \le  3\| \mathcal{A}^*(s) R_t \|_{P_{R_t}^*}$, and $\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} > 3\|\mathcal{A}^*(s)L_{t+1}^\top \|_{P_{L_{t+1}}^*}$
    \item (d): $ \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*} \le  3\| \mathcal{A}^*(s) R_t \|_{P_{R_t}^*}$, and $\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} \le 3\|\mathcal{A}^*(s)L_{t+1}^\top \|_{P_{L_{t+1}}^*}$
\end{itemize}
\textbf{Analysis of case (a)}
For case (a), we directly apply the results from Lemma \ref{lemma:4.1}, and obtain 
\begin{equation}
\begin{aligned}
& f_c (L_{t+1},R_{t+1}) \le (1-\eta_c)^2 f_c(L_t,R_t), \\
&\|L_{t+1}R_{t+1}^\top - X_\star\|_F^2 \le \frac{1+\delta_{r+r_\star}}{1-\delta_{r+r_\star}} (1-\eta_c)^2 \| 
L_tR_t^\top - X_\star \|_F^2,\\  
\end{aligned}
\end{equation}
where $\eta_c = 2(1-\delta_{r+r_\star})\left ( \eta - \frac{\eta}{3}(1 + 2\eta(1+\delta_{r+r_\star}) ) \right)$.

\textbf{Analysis of case (b)}
For case (b), we have
\begin{equation}
f_c(L_{t+1},R_t) \overset{(i)}{\le} \frac{1}{2(1-\delta_{r+r_\star})} \|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*}  \le \frac{3}{2(1-\delta_{r+r_\star})} \|\mathcal{A}^*(s)L_{t+1}^\top \|_{P_{L_{t+1}}^*}
\notag
\end{equation}
where $(i)$ using the result form Lemma \ref{lemma:4.1}, i.e., $\|\nabla_R f_c (L_{t+1},R_t)\|_{P_{L_{t+1}}^*}^2 \ge 2(1-\delta_{r+r_\star})f_c (L_{t+1},R_t)$.
Then for $f_c(L_{t+1},R_{t+1})$, we have
\begin{equation}
\begin{aligned}
f_c(L_{t+1},R_{t+1}) & \le f_c(L_{t+1},R_t) + \frac{\eta^2(1+\delta_{r+r_\star})}{2} \left( \| \nabla_R f_c(L_{t+1},R_t) \|_{P_{L_{t+1}}^*}^2 + \| \A^*(s) L_{t+1}^\top \|_{P_{L_{t+1}}^*}^2  \right) \\
& -\eta\|\nabla_R f_c(L_{t+1},R_t)\|^2_{P_{L_{t+1}}^* } + \eta\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^* } \|\A^*(s)L_{t+1}^\top\|_{P_{L_{t+1}}^* } \\
& \overset{(i)}{\le} f_c(L_{t+1},R_t) + 2{\eta^2(1+\delta_{r+r_\star})} \|\A^*(s) L_{t+1}^\top \|_{P_{L_{t+1}}^*}^2 + 3\eta  \|\A^*(s) L_{t+1}^\top \|_{P_{L_{t+1}}^*}^2 \\
&\le \frac{1}{2(1-\delta_{r+r_\star})} \|\A^*(s) L_{t+1}^\top \|_{P_{L_{t+1}}^*}^2 + 2{\eta^2(1+\delta_{r+r_\star})} \|\A^*(s) L_{t+1}^\top \|_{P_{L_{t+1}}^*}^2 + 3\eta  \|\A^*(s) L_{t+1}^\top \|_{P_{L_{t+1}}^*}^2 \\  
&\overset{(ii)}{<} \left(\frac{1}{2(1-\delta_{r+r_\star})} +7\right)\|\A^*(s) L_{t+1}^\top\|_{P_{L_{t+1}}^*}^2
\end{aligned}
\end{equation}
where $(i)$ use the assumption that $\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} \le 3\|\mathcal{A}^*(s) L_{t+1}^\top\|_{P_{L_{t+1}}^*}$; (ii) use the fact that $\delta_{r+r_\star}<1$ and $\eta<1$.

\par

\textbf{Analysis of case (c)}
For case (c), we have 
\begin{equation}
f_c(L_{t},R_t) \overset{(i)}{\le} \frac{1}{2(1-\delta_{r+r_\star})} \|\nabla_L f_c(L_{t},R_t)\|_{P_{R_{t}}^*}  \le \frac{3}{2(1-\delta_{r+r_\star})} \|\mathcal{A}^*(s)R_t\|_{P_{R_{t}}^*},
\notag
\end{equation}
where (i) use the result from Lemma \ref{lemma:4.1}, i.e., $\|\nabla_L f_c (L_{t},R_t)\|_{P_{R_{t}}^*}^2 \ge 2(1-\delta_{r+r_\star})f_c (L_{t},R_t)$.
For $f_c(L_{t+1},R_t)$, we have
\begin{equation}
\begin{aligned}
f_c(L_{t+1},R_t) &\le f_c(L_t,R_t) +\frac{\eta^2(1+\delta_{r+r_\star})}{2} \left( \| \nabla_L f_c(L_{t},R_t) \|_{P_{R_{t}}^*}^2 + \| \A^*(s) R_{t} \|_{P_{R_{t}}^*}^2  \right) \\
& -\eta\|\nabla_L f_c(L_{t},R_t)\|^2_{P_{R_{t}}^* } + \eta\|\nabla_L f_c(L_{t},R_t)\|_{P_{R_{t}}^* } \|\A^*(s)R_{t}\|_{P_{R_{t}}^* } \\
& \overset{(i)}{\le} f_c(L_{t},R_t) +2\eta^2(1+\delta_{r+r_\star}) \|\A^*(s) R_{t} \|_{P_{R_{t}}^*}^2 + 3\eta  \|\A^*(s) R_{t} \|_{P_{R_{t}}^*}^2 \\
&\le \frac{1}{2(1-\delta_{r+r_\star})} \|\A^*(s) R_{t} \|_{P_{R_{t}}^*}^2 + 2\eta^2(1+\delta_{r+r_\star}) \|\A^*(s) R_{t} \|_{P_{R_{t}}^*}^2 + 3\eta  \|\A^*(s) R_{t} \|_{P_{R_{t}}^*}^2 \\  
&\overset{(ii)}{<} \left(\frac{1}{2(1-\delta_{r+r_\star})} +7\right)\|\A^*(s) R_{t} \|_{P_{R_{t}}^*}^2,
\end{aligned}
\label{equ:18}
\end{equation}
where $(i)$ use the assumption that $\|\nabla_L f_c(L_{t},R_t)\|_{P_{R_{t}}^*} \le 3\|\mathcal{A}^*(s) R_t \|_{P_{R_{t}}^*}$; (ii) use the fact that $\delta_{r+r_\star}<1$ and $\eta<1$.

And then we have 
\begin{equation}
f_c(L_{t+1},R_{t+1}) \le (1-\eta_c)f_c(L_{t+1},R_t).
\label{equ:19}
\end{equation}

since $\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} > 3\|\mathcal{A}^*(s) L_{t+1}^\top \|_{P_{L_{t+1}}^*}$.

Combining equations (\ref{equ:18}) and (\ref{equ:19}), we have
\begin{equation}
    f_c(L_{t+1},R_{t+1}) < \left(\frac{1}{2(1-\delta_{r+r_\star})} +7\right)\|\A^*(s) R_{t} \|_{P_{R_{t}}^*}^2.
\end{equation}

\textbf{Analysis of case (d)} The analysis of case (d) is actually the same as case (b), and then we have
\begin{equation}
  f_c(L_{t+1},R_{t+1}) \le   \left(\frac{1}{2(1-\delta_{r+r_\star})} +7\right)\|\A^*(s) L_{t+1}^\top\|_{P_{L_{t+1}}^*}^2.
\end{equation}

Therefore, combining the analysis of the four case, we have
$$
f_c (L_{t+1},R_{t+1}) \le (1-\eta_c)^2 f_c(L_t,R_t)
$$ for any $t$ where $ \| \nabla_L f_c(L_t,R_t) \|_{P_{R_t}^*} >  3\| \mathcal{A}^*(s) \|_{P_{R_t}^*}$, and $\|\nabla_R f_c(L_{t+1},R_t)\|_{P_{L_{t+1}}^*} > 3\|\mathcal{A}^*(s) \|_{P_{L_{t+1}}^*}$. Otherwise, we have
\begin{equation}
\begin{aligned}
f_c(L_{t+1},R_{t+1}) & \le   \left(\frac{1}{2(1-\delta_{r+r_\star})} +7\right)\max\{\|\A^*(s) L_{t+1}^\top\|_{P_{L_{t+1}}^*}^2,\ \|\A^*(s) R_{t} \|_{P_{R_{t}}^*}^2 \}\\
&\overset{(i)}{\le} C_3 \mathcal{E}_{opt},
\end{aligned}
\end{equation}
where $(i)$ use the result of Lemma \ref{lemma:noise} and $C_3=\frac{1}{2(1-\delta_{r+r_\star})} +7$.
This implies that when the gradient is large, the recovery error converges linearly, whereas when the gradient is small, the recovery error is already close to optimal.

\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\newpage

\section{Additional numerical experiments}
\label{additional experiments}
\subsection{Noiseless case}

We validated the performance of four methods in the noiseless case. For PrecGD, we used spectral initialization, while for GD and ScaledGD(\(\lambda\)), we employed very small random initialization. For APGD, we used moderate random initialization by setting \(c_1 = 1e-1\). From Figure 4, we observe the following:
\begin{itemize}
    \item All four preconditioning methods accelerate the convergence rate of gradient descent under ill-conditioning and over-parameterization. APGD consistently exhibits the fastest convergence rate in all cases, even when using random initialization.
    \item The convergence rate of PrecGD is significantly slower than that of APGD, even though it uses spectral initialization. This is due to PrecGD's sensitivity to the step size, which can only be set to smaller values, resulting in slower convergence.
    \item ScaledGD(\(\lambda\)) performs similarly to PrecGD in ill-conditioned cases. However, in over-parameterized settings, ScaledGD(\(\lambda\)) shows a clear slowdown in convergence, which is attributed to its sensitivity to the damping parameter.
    \item For vanilla gradient descent method, small initialization helps accelerate convergence in over-parameterized cases but does not improve convergence in ill-conditioned settings.
\end{itemize}
\begin{figure}[h]
\centering
\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=5cm,height=5cm]{figure/noiseless_additional_simulations/k=r_kappa=1.pdf}
\end{minipage}%
}
\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=5cm,height=5cm]{figure/noiseless_additional_simulations/k=r_kappa=100.pdf}
\end{minipage}%
}

\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=5cm,height=5cm]{figure/noiseless_additional_simulations/k=2r_kappa=1.pdf}
\end{minipage}%
}
\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=5cm,height=5cm]{figure/noiseless_additional_simulations/k=2r_kappa=100.pdf}
\end{minipage}%
}
\centering
\caption{Comparison of the four methods in four different noiseless cases with $n_1=n_2=20$, $r_\star=5$, $m=10n_1 r_\star$. And the step size of APGD is 0.9, and the step size of ScaledGD$(\lambda)$, PrecGD and GD is set to be 0.5 for the best results. (a) exact rank ($r_\star=r$) and well condition $(\kappa(X_\star)=1)$ case. (b) over rank ($r=2r_\star$) and well condition $(\kappa(X_\star)=1)$ case. (c) exact rank ($r_\star=r$) and ill-condition $(\kappa(X_\star)=100)$ case. (d) over rank ($r=2r_\star$) and ill-condition $(\kappa(X_\star)=100)$ case.
}
\label{fig:2}
\end{figure}
\newpage
\subsection{Noisy case}

For the noisy case, we conducted similar experiments to validate the performance of different methods under over-parameterization and ill-conditioning. Here, we used NoisyPrecGD instead of PrecGD, as NoisyPrecGD is a variant of PrecGD designed for noisy scenarios. For NoisyPrecGD, we used spectral initialization, while for GD and ScaledGD(\(\lambda\)), we employed very small random initialization. For APGD, we used moderate random initialization by setting \(c_1 = 1e-1\). 

From Figure 5, we observe the following:
\begin{itemize}
    \item APGD achieved the fastest convergence rate in multiple scenarios, accelerating at least twice as fast compared to the second fastest method. Regarding recovery error, APGD reached the optimal recovery error in exact-parameterized cases, and even in over-parameterized settings, the recovery error was only slightly worse than the optimal error, remaining within an acceptable range.
    \item NoisyPrecGD had the second fastest convergence rate, outperforming ScaledGD(\(\lambda\)), which used very small initialization. The recovery error for NoisyPrecGD and ScaledGD(\(\lambda\)) was similar to that of APGD.
    \item As demonstrated by \cite{ding2022validation}, vanilla GD with very small random initialization can converge to the optimal recovery error in both over-parameterized and exact-parameterized cases. However, its convergence rate under ill-conditioning is extremely slow, making it difficult to reach this optimal error. In contrast, APGD requires very few iterations to converge to a solution close to the optimal error, making it more practical for real-world applications.
    
\end{itemize}


\begin{figure}[h]
\centering
\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=5cm,height=5cm]{figure/noisy_additional_simulations/k=r_kappa=1.pdf}
\end{minipage}%
}
\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=5cm,height=5cm]{figure/noisy_additional_simulations/k=r_kappa=100.pdf}
\end{minipage}%
}

\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=5cm,height=5cm]{figure/noisy_additional_simulations/k=2r_kappa=1.pdf}
\end{minipage}%
}
\subfigure[]{
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=5cm,height=5cm]{figure/noisy_additional_simulations/k=2r_kappa=100.pdf}
\end{minipage}%
}
\centering
\caption{Comparison of the four methods in four different noisy cases with $n_1=n_2=20$, $r_\star=5$, $m=10n_1 r_\star$, $\nu=1e-5$. And the step size of APGD is 0.9, and the step size of ScaledGD$(\lambda)$, NoisyPrecGD and GD is set to be 0.6 for the best results. (a) exact rank ($r_\star=r$) and well condition $(\kappa(X_\star)=1)$ case. (b) over rank ($r=2r_\star$) and well condition $(\kappa(X_\star)=1)$ case. (c) exact rank ($r_\star=r$) and ill-condition $(\kappa(X_\star)=100)$ case. (d) over rank ($r=2r_\star$) and ill-condition $(\kappa(X_\star)=100)$ case.
}
\label{fig:2}
\end{figure}

\newpage
\subsection{Real data experiments}
In this section, we conduct real data experiments to verify the effectiveness of APGD. Specifically, similar to the work of Zhang et al., we perform noisy matrix completion experiments on hyperspectral images. The noisy matrix completion problem is defined as recovering the ground-truth matrix \( X_\star \) from partial noisy observations \( \mathcal{P}_\Omega(X_\star+ S) \), where 
$$
\mathcal{P}_{\Omega}\left( X \right)_{i,j} = \left\{ 
\begin{array}{ll}
    X_{i,j}, & \text{if } (i,j) \in \Omega \\ 
    0, & \text{otherwise}
\end{array} 
\right.
$$
and $S$ denotes the Gaussian noise, and \( \Omega \) is generated according to a Bernoulli model, meaning that each entry \((i, j) \in \Omega\) is independently selected with probability \( p \).


\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=16cm,height=14cm]{figure/verify_rank.pdf}}
\caption{Comparing the performance of four methods with different ranks $r$. The exact rank $r_\star=50$, sampling rate $p=0.5$. The SNR of noise is set as 60. The step sizes of all methods are fine-tuned for the best results.}
\end{center}
\vskip -0.2in
\label{fig6}
\end{figure}


Based on the Burer–Monteiro (BM) factorization, our optimization problem is formulated as 
\begin{equation}
    \underset{L \in\mathbb{R}^{n_1\times r},\ R \in\mathbb{R}^{n_2\times r}}{\arg \min } \frac{1}{2p} \|\mathcal{P}_\Omega(LR^\top - X_\star)\|_F^2.
\end{equation}
We can also apply APGD to solve this problem, even though \( \mathcal{P}_\Omega(\cdot) \) does not satisfy the Restricted Isometry Property (RIP) condition. Here, we use a single spectral band of a hyperspectral image \footnote{This is from the CAVE dataset \href{https://cave.cs.columbia.edu/repository/Multispectral/RealandFake}{https://cave.cs.columbia.edu/repository/Multispectral/RealandFake} }, with a size of \( 512 \times 512 \). First, we approximate the image with a low-rank matrix of rank 50. For NoisyPrecGD, spectral initialization is applied, while for other methods, small random initializations are used. All methods are run for only 5 iterations. We use the Signal-to-Noise Ratio (SNR) to measure the level of the noise \( S \), and then evaluate the recovery performance using the Peak Signal-to-Noise Ratio (PSNR), which is displayed below each image.

\textbf{Experiments with different over rank $r$}\\
We begin by evaluating the recovery performance of APGD when transitioning from the exact rank case to the over-parameterized rank case. From Figure 6, we can observe that APGD successfully recovers the true image in both the exact rank and over-parameterized rank scenarios, even when starting from a random initialization. In contrast, other methods, such as GD and ScaledGD(\(\lambda\)), fail to recover the image. Although NoisyPrecGD also manages to recover the true image, its performance is inferior to that of APGD, and it requires spectral initialization to achieve reasonable results.


\textbf{Experiments with different sampling rate $p$}
We compared the performance of various methods under different sampling rates. As shown in Figure 7, APGD is capable of approximately recovering the original image even at a very low sampling rate ($p=0.2$), whereas other methods failed. As the sampling rate increases, the recovery quality improves significantly.
\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=16cm,height=13.6cm]{figure/missing_rate.pdf}}
\caption{Comparing the performance of four methods with different sampling rate $p$. The exact rank $r_\star=50$, over rank $r=1.5r_\star$. The SNR of noise is set as 60. The step sizes of all methods are fine-tuned for the best results.}
\end{center}
\vskip -0.2in
\label{fig7}
\end{figure}

\textbf{Experiments with different noise level}
We compared the performance of various methods under different sampling rates. As shown in Figure 8, APGD consistently achieves the best recovery performance under varying noise levels, significantly outperforming other methods. As the noise level decreases, the recovery performance of APGD improves notably, while the performance of other methods remains suboptimal.

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=16cm,height=16cm]{figure/noise.pdf}}
\caption{Comparing the performance of four methods with different noise level. The exact rank $r_\star=50$, over rank $r=1.5r_\star$, the sampling rate $p=0.5$. The step sizes of all methods are fine-tuned for the best results.}
\end{center}
\vskip -0.2in
\label{fig7}
\end{figure}




% Additionally, we present the relative recovery errors for various methods. As shown in Figure 6, in the over-parameterized case ($r = 1.5r_\star$), only APGD is able to converge at a linear rate to a relatively small error, while even NoisyPrecGD with spectral initialization fails to converge quickly. Other methods, such as GD and ScaledGD($\lambda$), diverge.
% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=6cm,height=6cm]{figure/k=1.5r_p=0.5.pdf}}
% \caption{The relative error of four methods in the over rank case $r=1.5r_\star$ with sampling rate $p=0.5$. The step sizes of all methods are fine-tuned for the best results.}
% \end{center}
% \vskip -0.2in
% \end{figure}




\newpage






\end{CJK}
\end{document}