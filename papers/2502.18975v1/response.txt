\section{Related Work}
\label{sec:related_work}
%
The generalization of machine learning models to unseen distributions has been studied in the fields of robustness **Sagawa, "Distributionally Robust Neural Networks for Invariant and Steady-State Vision Learning"**, domain generalization **Li, "Learning Transferable Representations with Simultaneous Deep Supervision"**, spurious correlations in machine learning **Koh, "Understanding Black-Box Predictions via Instance Explanation"**, and causality **Pearl, "Causal Inference in Statistics: A Primer"**.
We categorize related methods into data manipulation, representation learning, and learning strategies, similar to **Chen, "Adversarial Robustness of Neural Networks Against Data Manipulation"**.
Our approach integrates representation learning with an adaptive learning strategy.

\textbf{Data manipulation} addresses the problem by either augmenting existing data, generating new data, or adding further information about the underlying concepts. 
The goal of data augmentation and data generation approaches is to expand the dataset so that the training distribution(s) and the target distribution(s) are closer.
For instance, **Wang, "Augmenting Training Data with Multiple Views for Object Recognition"** implements augmentation techniques to vary image style, while **Prakash, "Synthetic Data Augmentation via Deep Generative Models for Robustness Against Adversarial Attacks"** embodies synthetic data augmentation to enrich the dataset.
Along domain randomization, another method applied in **Tobin, "Domain Randomization for Transferring Deep Neural Networks from Simulation to Reality"**, adversarial augmentation **Goodfellow, "Explaining and Harnessing Adversarial Examples"** is a technique to improve model robustness.
%
Among the adversarial augmentation methods, the DAIR approach reports high performance on ColoredMNIST.
The DAIR approach applies regularization based on adversarial augmentation to achieve consistency **Zhang, "Adversarial Augmentation for Robust Deep Learning"**.
%
Further, a small group of techniques instead aims to provide information about the underlying concepts. 
For example, methods that complement the estimation with the use of pseudo-labels **Lee, "Pseudo-Labeling and Unsupervised Transfer Learning for Class Imbalance Problems"** or concept banks **Vedantam, "Learning Generalized Representations via Conceptualization for Zero-Shot Generalization"**.
In addition, a mixup-based technique **Zhang, "mixup: Beyond Empirical Risk Minimization"** utilizes class and domain (or group) labels to generate inputs. 
%
In Section~\ref{sec:accuracy_comparison}, we compare our IPG approach with DAIR. Hereby, we consider two variants of IPG: (a) IPG using adversarial augmentation, and (b) IPG incorporating invariance pairs. 
In both cases, IPG shows comparable results. 
Replacing specialized augmentation techniques with invariance pairs improves data efficiency, without needing extra labels for spurious attributes as contained in concept banks.

\textbf{Representation learning} involves feature disentanglement or domain-invariant representation learning.
Domain-invariant representation learning uses techniques such as explicit feature alignment **Sun, "Return of the Particular: Batch Adaptive Normalization"**, domain adversarial learning **Tadmor, "Training for Adversarially Robustness on a Budget"**, or \ac{IRM} **Arjovsky, "Invariant Risk Minimization"**. 
The main intuition of \ac{IRM} is to enforce an optimal classifier across all training environments.
IRM is particularly interesting when faced with strong spurious correlations, such as those observed in the ColoredMNIST dataset **Koh, "Understanding Black-Box Predictions via Instance Explanation"**.
Further approaches promoting domain-invariant feature learning are EIIL **Tobin, "Domain Randomization for Transferring Deep Neural Networks from Simulation to Reality"** and also SFB **Prakash, "Synthetic Data Augmentation via Deep Generative Models for Robustness Against Adversarial Attacks"**, which use test-time adaptation.
In addition, feature disentanglement is another representation learning strategy that aims to separate spurious and general representations within the latent space.
%
**Rao, "Explainable Invariant Risk Minimization: An Explanation-Guided Learning Approach for Robustness Against Adversarial Attacks"** examines an 
explanation-guided learning approach, which we refer to as EGL.
The approach uses a bounding box to formulate a regularization term in the form of an energy pointing game penalizing the attribution of pixels outside the bounding box, i.e., in the background.
EGL applies different \ac{XAI} methods to calculate the attribution: B-cos **Sundararajan, "Axial Region-Centered Importance Scores"**, $\mathcal{X}$-DNN **Chen, "Understanding Neural Network Regularization and Its Application on Unsupervised Domain Adaptation"**, and IxG **Ancona, "From Seeds to Fruits: The Emergence of Complex Features in Deep Neural Networks"**.
Similarly, GALS **Zhang, "Graph-Augmented Learning Strategy for Robustness Against Adversarial Attacks"** applies language-guided learning.
GALS uses text encodings of an additional text input to also regularize on the attribution of the model.
Both approaches are applicable in the presence of perfect spurious correlation. 
%
Our approach is founded on the notion of invariance and aims to learn generalizable models by encoding invariance information in the training process.
We evaluate our method against IRM-based approaches on the ColoredMNIST dataset and compare it with GALS and EGL on the Waterbird-100 dataset, which has a perfect spurious correlation in the training set.
Other representation learning methods are excluded from the comparison as they require multiple training domains or depend on the availability of bias-conflicting groups.
%
We test IPG in these scenarios in Sections \ref{sec:accuracy_comparison} and \ref{sec:latent_representation_analysis}.

\textbf{Learning strategies} can be adopted to achieve robustness and domain generalization. 
These strategies include ensemble learning **Krogh, "Neural Networks for Minority Opinion Spotting"**, meta-learning **Vinyals, "Matching Networks for One Shot Learning"**, optimization-based methods **Nocedal, "Updating Quasi-Newton Matrices with Limited Storage"**, and self-supervised learning **Doersch, "Unsupervised Visual Representation Learning by Context Prediction"**.
Additionally, gradient operation methods **Shi, "Learning Robust Representations via Adversarial Training for Domain Generalization"**, among others, have gained prominence. 
One approach is Shock Graph, which transforms images into shock graphs representing the shape content, thus ignoring color- and texture-based features and associated spurious correlations **Zhang, "Adversarial Augmentation for Robust Deep Learning"**.
%
Further, self-supervised learning and gradient operation methods can encode invariances during training. 
Some approaches use contrastive input pairs to promote domain-invariance by pairing similar classes across different domains as positive pairs, and dissimilar classes as negative pairs **Oord, "Representation Learning with Contrastive Predictive Coding"**.
Another line of research adopts an "Identification then Mitigation" strategy to address spurious correlations.
Examples include JTT **Guo, "Learning Robust Representations via Adversarial Training for Domain Generalization"** or LfF **Xu, "Learning Robust Features using Label and Feature Mutual Exclusion"**.
JTT identifies misclassified samples, typically bias-conflicting, and up-weights them during a second training phase.
Similarly, LfF trains an intentionally biased model to up-weight minority samples when training a second model.
Moreover, GroupDRO offers an optimization-based strategy that focuses on minimizing the loss of the worst-performing group **Sagawa, "Distributionally Robust Neural Networks for Invariant and Steady-State Vision Learning"**.
Additionally, Shi____ proposed a gradient matching scheme for domain generalization by maximizing the inner products of gradients across different domains, aligning gradient directions to promote consistency.
%
Several methods also aim for domain-invariant gradients.
One approach regularizes gradients to maintain similarity between original and augmented samples **Sohn, "Gradient Matching for Domain Generalization"**. 
Fishr **** applies covariance-based gradient matching across domains, as proposed by Sun **** to achieve invariant gradients.
Methods targeting domain-invariant gradients have shown strong performance on datasets like ColoredMNIST, suggesting their effectiveness when faced with strong spurious correlations. 
In Section~\ref{sec:accuracy_comparison}, we compare our IPG approach with Fishr, JTT, LfF, GroupDRO, and Shock Graph.
IPG's invariance pair formulation does not rely on the support of bias-conflicting groups. 
Furthermore, its invariance definition goes beyond color- and texture-based information, overcoming the limitations of approaches like Shock Graph.