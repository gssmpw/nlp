\section{Related Work}
\label{sec:related_work}
%
The generalization of machine learning models to unseen distributions has been studied in the fields of robustness \cite{sagawa_distributionally_2020}, domain generalization \cite{wang_generalizing_2023}, spurious correlations in machine learning \cite{ye_spurious_2024, steinmann_navigating_2024}, and causality \cite{ye_spurious_2024}.
We categorize related methods into data manipulation, representation learning, and learning strategies, similar to \cite{wang_generalizing_2023, ye_spurious_2024}.
Our approach integrates representation learning with an adaptive learning strategy.

\textbf{Data manipulation} addresses the problem by either augmenting existing data, generating new data, or adding further information about the underlying concepts. 
The goal of data augmentation and data generation approaches is to expand the dataset so that the training distribution(s) and the target distribution(s) are closer.
For instance, Wang~\cite{wang_learning_2021} implements augmentation techniques to vary image style, while Prakash~\cite{prakash_structured_2019} embodies synthetic data augmentation to enrich the dataset.
Along domain randomization, another method applied in \cite{yue_domain_2019, prakash_structured_2019}, adversarial augmentation \cite{huang_robustness_2023, von_kugelgen_self-supervised_2021, puli_nuisances_2024} is a technique to improve model robustness.
%
Among the adversarial augmentation methods, the DAIR approach reports high performance on ColoredMNIST.
The DAIR approach applies regularization based on adversarial augmentation to achieve consistency \cite{huang_robustness_2023}.
%
Further, a small group of techniques instead aims to provide information about the underlying concepts. 
For example, methods that complement the estimation with the use of pseudo-labels \cite{nam_spread_2022, srivastava_robustness_2020} or concept banks \cite{wu_discover_2023, koh_concept_2020}.
In addition, a mixup-based technique \cite{yao_improving_2022} utilizes class and domain (or group) labels to generate inputs. 
%
In Section~\ref{sec:accuracy_comparison}, we compare our IPG approach with DAIR. Hereby, we consider two variants of IPG: (a) IPG using adversarial augmentation, and (b) IPG incorporating invariance pairs. 
In both cases, IPG shows comparable results. 
Replacing specialized augmentation techniques with invariance pairs improves data efficiency, without needing extra labels for spurious attributes as contained in concept banks.

\textbf{Representation learning} involves feature disentanglement or domain-invariant representation learning.
Domain-invariant representation learning uses techniques such as explicit feature alignment \cite{jin_style_2022, fan_adversarially_2021, chen_domain_2023}, domain adversarial learning \cite{luo_scale_2021, matsuura_domain_2020}, or \ac{IRM} \cite{arjovsky_invariant_2019}. 
The main intuition of \ac{IRM} is to enforce an optimal classifier across all training environments.
IRM is particularly interesting when faced with strong spurious correlations, such as those observed in the ColoredMNIST dataset \cite{arjovsky_invariant_2019}.
Further approaches promoting domain-invariant feature learning are EIIL \cite{creager_environment_2021} and also SFB \cite{eastwood_spuriosity_2023}, which use test-time adaptation.
In addition, feature disentanglement is another representation learning strategy that aims to separate spurious and general representations within the latent space.
%
Rao \cite{rao_studying_2023} examines an 
explanation-guided learning approach, which we refer to as EGL.
The approach uses a bounding box to formulate a regularization term in the form of an energy pointing game penalizing the attribution of pixels outside the bounding box, i.e., in the background.
EGL applies different \ac{XAI} methods to calculate the attribution: B-cos \cite{bohle_b-cos_2022}, $\mathcal{X}$-DNN \cite{hesse_fast_2021}, and IxG \cite{shrikumar_learning_2017}.
Similarly, GALS \cite{petryk_guiding_2022} applies language-guided learning.
GALS uses text encodings of an additional text input to also regularize on the attribution of the model.
Both approaches are applicable in the presence of perfect spurious correlation. 
%
Our approach is founded on the notion of invariance and aims to learn generalizable models by encoding invariance information in the training process.
We evaluate our method against IRM-based approaches on the ColoredMNIST dataset and compare it with GALS and EGL on the Waterbird-100 dataset, which has a perfect spurious correlation in the training set.
Other representation learning methods are excluded from the comparison as they require multiple training domains or depend on the availability of bias-conflicting groups.
%
We test IPG in these scenarios in Sections \ref{sec:accuracy_comparison} and \ref{sec:latent_representation_analysis}.

\textbf{Learning strategies} can be adopted to achieve robustness and domain generalization. 
These strategies include ensemble learning \cite{wu_collaborative_2021, nam_learning_2020}, meta-learning \cite{zhao_learning_2021, qiao_uncertainty-guided_2021}, optimization-based methods \cite{sagawa_distributionally_2020}, and self-supervised learning \cite{kim_selfreg_2021, carlucci_domain_2019}.
Additionally, gradient operation methods \cite{shi_gradient_2022, tian_neuron_2023}, among others, have gained prominence. 
One approach is Shock Graph, which transforms images into shock graphs representing the shape content, thus ignoring color- and texture-based features and associated spurious correlations \cite{narayanan_shape-biased_2021}.
%
Further, self-supervised learning and gradient operation methods can encode invariances during training. 
Some approaches use contrastive input pairs to promote domain-invariance by pairing similar classes across different domains as positive pairs, and dissimilar classes as negative pairs \cite{kim_selfreg_2021, jeon_feature_2021}.
Another line of research adopts an "Identification then Mitigation" strategy to address spurious correlations.
Examples include JTT~\cite{liu_just_2021} or LfF~\cite{nam_learning_2020}.
JTT identifies misclassified samples, typically bias-conflicting, and up-weights them during a second training phase.
Similarly, LfF trains an intentionally biased model to up-weight minority samples when training a second model.
Moreover, GroupDRO offers an optimization-based strategy that focuses on minimizing the loss of the worst-performing group \cite{sagawa_distributionally_2020}.
Additionally, Shi~\cite{shi_gradient_2022} proposed a gradient matching scheme for domain generalization by maximizing the inner products of gradients across different domains, aligning gradient directions to promote consistency.
%
Several methods also aim for domain-invariant gradients.
One approach regularizes gradients to maintain similarity between original and augmented samples \cite{tian_neuron_2023}. 
Fishr \cite{rame_fishr_2022} applies covariance-based gradient matching across domains, as proposed by Sun \cite{sun_return_2016}, to achieve invariant gradients.
Methods targeting domain-invariant gradients have shown strong performance on datasets like ColoredMNIST, suggesting their effectiveness when faced with strong spurious correlations. 
In Section~\ref{sec:accuracy_comparison}, we compare our IPG approach with Fishr, JTT, LfF, GroupDRO, and Shock Graph.
IPG's invariance pair formulation does not rely on the support of bias-conflicting groups. 
Furthermore, its invariance definition goes beyond color- and texture-based information, overcoming the limitations of approaches like Shock Graph.