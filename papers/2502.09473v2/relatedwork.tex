\section{Background and related work}
\label{sec:background}
The field of graph signal processing \cite{8347162, mandicFTMLp2} has emerged in the last decade to generalise digital signal processing methods, such as convolutional filters, to the graph domain. A graph, $\mathcal{G}(\mathcal{V}, \mathcal{E})$, is defined as a set of $N$ nodes, $v_i \in \mathcal{V}$ for $i = 1, \ldots, N$, and a set of $\mathcal{E}$ edges denoting the pairwise connections between them, $e_{ij} = (v_i, v_j) \in \mathcal{E}$, for $i = 1, \ldots, N$ and $j = 1, \ldots, N$. Graph signal processing focuses on analysing signals on graphs, where a multivariate signal on a graph is defined as $\mathbf{X} \in \mathbb{R}^{N\times c}$, which assigns a real-valued signal of $c$ channels to each node, $\mathbf{X}: \mathcal{V} \mapsto \mathbb{R}^c$.

Building on graph signal processing methods, graph deep learning \cite{bronstein2017geometric, mandicFTMLp3} has generalised successful deep learning architectures, such as convolutional neural networks, to the graph domain \cite{kipf2016semi-supervised, zhang2020deep}. \Glspl{stgnn} refer to the class of deep learning architectures designed to analyse time-varying graph signals \cite{cini2023graphdeeplearningtime, jin2024survey}. \Glspl{stgnn} can be categorised into time-then-space \cite{gao2022equivalence, cini2023scalable} or time-and-space models \cite{seo2018structured, li2018diffusionconvolutionalrecurrentneural, marisca2022learning}, which denote separate or joint processing of the space and time dimensions, respectively. A notable example of a time-and-space \gls{stgnn} is the \gls{grnn}, which replaces the fully connected layers in a \gls{rnn} with graph convolutions \cite{seo2018structured, li2018diffusionconvolutionalrecurrentneural}. \Glspl{stgnn} have achieved state-of-the-art performance in tasks such as forecasting \cite{jiang2022graph, lam2023learning, cini2023graph-based} and missing data imputation \cite{marisca2022learning, cini2021filling}.

\glspl{stgnn} are inherently global models, sharing parameters across space and assuming a stationary process over time. Global \glspl{stgnn} can be used for zero-shot transfer and inductive learning on unseen graphs, however, they might fail to account for spatial variations in dynamics. Node embeddings have been recently introduced to learn these local effects in ST-GNNs \cite{cini2024taming}. Whilst hybrid global-local models often outperform global architectures, recent research has focussed on improving their performance in an inductive setting using transfer learning \cite{cini2024taming, yin2022nodetrans}.

Other imputation methods have been applied to time series, ranging from \gls{mf} methods \cite{salakhutdinov2008bayesian, lee2000algorithms} and their counterparts with graph and temporal regularisation \cite{cai2010graph, yu2016temporal}, to deep learning techniques employing \glspl{rnn} \cite{cao2018brits}, generative adversarial networks \cite{yoon2019time, liu2019naomi}, transformers \cite{du2023saits}, or most recently denoising diffusion probabilistic models \cite{tashiro2021csdi, alcaraz2021diffusion, jenkins2023improving}.