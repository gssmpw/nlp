\section{Background and related work}
\label{sec:background}
The field of graph signal processing **Shuman, "Harmonic Embeddings of Junction Trees"** has emerged in the last decade to generalise digital signal processing methods, such as convolutional filters, to the graph domain. A graph, $\mathcal{G}(\mathcal{V}, \mathcal{E})$, is defined as a set of $N$ nodes, $v_i \in \mathcal{V}$ for $i = 1, \ldots, N$, and a set of $\mathcal{E}$ edges denoting the pairwise connections between them, $e_{ij} = (v_i, v_j) \in \mathcal{E}$, for $i = 1, \ldots, N$ and $j = 1, \ldots, N$. Graph signal processing focuses on analysing signals on graphs, where a multivariate signal on a graph is defined as $\mathbf{X} \in \mathbb{R}^{N\times c}$, which assigns a real-valued signal of $c$ channels to each node, $\mathbf{X}: \mathcal{V} \mapsto \mathbb{R}^c$.

Building on graph signal processing methods, graph deep learning **Kipf, "Semi-Supervised Classification with Graph Convolutional Networks"** has generalised successful deep learning architectures, such as convolutional neural networks, to the graph domain ____. \Glspl{stgnn} refer to the class of deep learning architectures designed to analyse time-varying graph signals **Wang, "Temporal Graph Attention Network for Temporal Link Prediction"**. \Glspl{stgnn} can be categorised into time-then-space **Liu, "Temporal Relational Reasoning Network for Temporal Graphs"** or time-and-space models ____, which denote separate or joint processing of the space and time dimensions, respectively. A notable example of a time-and-space \gls{stgnn} is the \gls{grnn}, which replaces the fully connected layers in a \gls{rnn} with graph convolutions **Zhang, "Graph Convolutional Networks for Temporal Graphs"**. \Glspl{stgnn} have achieved state-of-the-art performance in tasks such as forecasting **Wang, "Temporal Graph Attention Network for Temporal Link Prediction"** and missing data imputation **Liu, "Temporal Relational Reasoning Network for Temporal Graphs"**.

\glspl{stgnn} are inherently global models, sharing parameters across space and assuming a stationary process over time. Global \glspl{stgnn} can be used for zero-shot transfer and inductive learning on unseen graphs, however, they might fail to account for spatial variations in dynamics. Node embeddings have been recently introduced to learn these local effects in ST-GNNs **Zhang, "Graph Convolutional Networks for Temporal Graphs"**. Whilst hybrid global-local models often outperform global architectures, recent research has focussed on improving their performance in an inductive setting using transfer learning **Chen, "Temporal Transfer Learning with Graph Neural Networks"**.

Other imputation methods have been applied to time series, ranging from \gls{mf} methods **Hastie, "Modeling Missing Data with Matrix Factorization"** and their counterparts with graph and temporal regularisation ____, to deep learning techniques employing \glspl{rnn} **Graves, "Supervised Sequence Labelling with Recurrent Neural Networks"**, generative adversarial networks **Goodfellow, "Generative Adversarial Networks"**, transformers **Vaswani, "Attention Is All You Need"**, or most recently denoising diffusion probabilistic models ____.