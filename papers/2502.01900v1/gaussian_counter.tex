\section{A Gaussian Variant}\label{sec:gaussian_variant}

The first step towards proving Theorem~\ref{thm:intro_main} is to prove a Gaussian variant, stated below:

\begin{proposition}\label{prop:gaussian_counter_eg}	
	Let $k\in \N$, and let $\Sigma\in \R^{k\times k}$ be a symmetric positive semi-definite matrix such that:
	\begin{enumerate}
		\item For each $i\in [k]$, it holds that $\Sigma_{i,i} = 1$.
		\item The matrix $V = \Sigma - I$ has no row/column as all zeros.
	\end{enumerate}
	
	Then, there exists a Lipschitz continuous function $f:\R\to [-1,1]$ such that:
	\[\E_{X\sim \mc N(0,1)} \sqbrac{f(X)} = 0, \quad\text{and}\quad \abs{\E_{X\sim \mc N(0, \Sigma)}\sqbrac{\prod_{i\in [k]} f(X_i)}} > 0 .\]
\end{proposition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Symmetric Powers of Polynomials}

Before we prove the above proposition, we first prove a lemma about (symmetrization of) powers of multivariate polynomials.
We show that if a polynomial $q(x_1,\dots,x_k)$ depends on all the variables $x_1,\dots,x_k$, then some power $\Sym(q^d)$ (see Definition~\ref{defn:sym_op}) contains a monomial divisible by $x_1x_2\cdots x_k$.


\begin{lemma}\label{lemma:polynomial_all_var}
	Let $k\in \N$, and let $q:\R^k\to \R$ be a polynomial such that for each $i\in [k]$, the polynomial $\ell_i=\partial_iq$ is not identically zero.
	Then, there exists some $d\in \N$, and positive integers $s_1,\dots,s_k \in \N$ such that the coefficient of $x_1^{s_1}\cdot x_2^{s_2}\cdots x_k^{s_k}$ in the polynomial $\Sym(q^d)$ is non-zero.
\end{lemma}

We start by proving the following lemma about derivates of powers of $q$.
\begin{lemma}\label{lemma:pow_derivatives}
	Let $k\in \N$, and let $q:\R^k\to \R$ be a polynomial. For each $i\in [k]$, let $\ell_i=\partial_iq$.
	
	Then, for every $s=(s_1,\dots,s_k)\in \Z_{\geq 0}^k$ with $\abs{s}=\sum_{i\in [k]}s_i$, there exist  polynomials $p_0, \dots, p_{\abs{s}}$, with $p_{\abs{s}} = \prod_{i\in [k]}\ell_i^{s_i}$, such that for each $d\geq \abs{s}$, it holds that 
	\[ \partial_1^{s_1}\cdot\partial_2^{s_2}\cdots \partial_k^{s_k}\brac{q^d} = q^{d-\abs{s}}\cdot \brac{\sum_{i=0}^{\abs{s}} d^i\cdot p_i}.\]
\end{lemma}
\begin{proof}
	The proof is by induction on $\abs{s}$.
	For the base case, if $\abs{s}=0$, we have $s = (0,0,\dots,0)$, and $p_0 = 1$ satisfies the statement of the lemma.
	
	For the inductive step, consider any $s=(s_1,\dots,s_k)\in \Z_{\geq 0}^k$ with $\abs{s}=\sum_{i\in [k]}s_i > 0$.
	Without loss of generality, by symmetry, we can assume that $s_1>0$.
	By the inductive hypothesis applied to $(s_1-1,s_2\dots,s_k)$, we have the existence of polynomials $p_0, \dots, p_{\abs{s}-1}$, with $p_{\abs{s}-1} = \ell_1^{s_1-1}\cdot \prod_{i=2}^k\ell_i^{s_i}$, and such that for each $d\geq \abs{s}-1$, we have 
	\[ \partial_1^{s_1-1}\cdot\partial_2^{s_2}\cdots \partial_k^{s_k}\brac{q^d} = q^{d-\abs{s}+1}\cdot \brac{\sum_{i=0}^{\abs{s}-1} d^i\cdot p_i}.\]
	Now, if $d\geq \abs{s}$, differentiating the above with respect to $x_1$, we get 
	\begin{align*}
		\partial_1^{s_1}\cdot\partial_2^{s_2}\cdots \partial_k^{s_k}\brac{q^d} 
		&= q^{d-\abs{s}}\cdot \brac{(d-\abs{s}+1)\cdot \ell_1\cdot \sum_{i=0}^{\abs{s}-1} d^i\cdot p_i} + q^{d-\abs{s}+1}\cdot \brac{\sum_{i=0}^{\abs{s}-1} d^i\cdot \partial_1(p_i)}
		\\&= q^{d-\abs{s}}\cdot \brac{\sum_{i=1}^{\abs{s}} d^{i}\cdot \ell_1\cdot p_{i-1} + \sum_{i=0}^{\abs{s}-1}d^i\cdot \brac{\brac{-\abs{s}+1}\cdot \ell_1\cdot p_i + q\cdot \partial_1 p_i} }
		\\&= q^{d-\abs{s}}\cdot \brac{\sum_{i=0}^{\abs{s}} d^{i}\cdot \tilde{p}_i },
	\end{align*}
	where the polynomials $\tilde{p}_1,\dots,\tilde{p}_{\abs{s}}$ do not depend on $d$, and are such that $\tilde{p}_{\abs{s}} = p_{\abs{s}-1}\cdot \ell_1  = \prod_{i\in [k]}\ell_i^{s_i}$, as desired.
\end{proof}

With the above lemma in hand, next we shall consider the symmetrization operation applied to derivatives of powers of $q$.

\begin{lemma}\label{lemma:pow_sec_der}
	Let $k\in \N$, and let $q:\R^k\to \R$ be a polynomial such that for each $i\in [k]$, the polynomial $\ell_i=\partial_iq$ is not identically zero.
	
	Then, for each large enough even integer $d\in \N$, the polynomial $\Sym\brac{\partial_1^2\cdot\partial_2^2\cdots \partial_k^2\brac{q^d}}$ is not identically zero.
\end{lemma}
\begin{proof}
	By applying Lemma~\ref{lemma:pow_derivatives} on $s = (2,2,\dots,2)$, we have the existence of polynomials $p_0, \dots, p_{2k}$, with $p_{2k} = \prod_{i\in [k]}\ell_i^2$, such that for each $d\geq 2k$, it holds that $\partial_1^{2}\cdot\partial_2^{2}\cdots \partial_k^{2}\brac{q^d} = q^{d-2k}\cdot \brac{\sum_{i=0}^{2k} d^i\cdot p_i}$.
	
	By Lemma~\ref{lemma:dim_var}, let $y\in \R^k$ be such that $y$ (and its permutations) don't lie in the zero set of any of the polynomials $\ell_1,\dots,\ell_k, q$.
	We define \[ A = \min_{\pi\in S_k}\sqbrac{\prod_{i\in [k]}\ell_i\brac{y_\pi}^2} > 0, \quad B = \max_{0\leq i\leq 2k-1,\ \pi\in S_k}\abs{p_i(y_\pi)}\geq 0. \]
	Then, for any even integer $d\geq \max\set{2k, \frac{4kB}{A}}$, it holds that
	\begin{align*}
		\Sym\brac{\partial_1^{2}\cdot\partial_2^{2}\cdots \partial_k^{2}\brac{q^d}}(y) 
		&=\sum_{\pi\in S_k} q\brac{y_\pi}^{d-2k}\cdot \brac{d^{2k}\cdot \prod_{i\in [k]}\ell_i\brac{y_{\pi}}^2 + \sum_{i=0}^{2k-1} d^i\cdot p_i\brac{y_\pi}}
		\\&\geq \sum_{\pi\in S_k} q\brac{y_\pi}^{d-2k}\cdot \brac{d^{2k}\cdot A - \sum_{i=0}^{2k-1}d^i\cdot B}
		\\&\geq  \brac{\sum_{\pi\in S_k} q\brac{y_\pi}^{d-2k}}\cdot \brac{d^{2k}\cdot A - 2k\cdot d^{2k-1}\cdot B}
		\\&\geq \brac{\sum_{\pi\in S_k} q\brac{y_\pi}^{d-2k}}\cdot \frac{d^{2k} A}{2} > 0.
	\end{align*}
	Hence, for even integers $d\geq \max\set{2k, \frac{4kB}{A}}$, the polynomial $\Sym\brac{\partial_1^{2}\cdot\partial_2^{2}\cdots \partial_k^{2}\brac{q^d}}$ is not identically zero.
\end{proof}

Finally, we prove the main lemma of this section.

\begin{proof}[Proof of Lemma~\ref{lemma:polynomial_all_var}]
	Let $k\in \N$, and let $q:\R^k\to \R$ be a polynomial such that for each $i\in [k]$, the polynomial $\ell_i=\partial_iq$ is not identically zero.
	It suffices to prove that for some $d\in \N$, the polynomial $\partial_1^{2}\cdot\partial_2^{2}\cdots \partial_k^{2}\brac{\Sym(q^d)}$ is not identically zero, since then the coefficient of some monomial divisible by $x_1^2\cdot x_2^2\cdots x_k^2$ is non-zero.
	
	For each polynomial $p:\R^k\to \R$, and each $\pi\in S_k$, we shall use $p_\pi$ to denote the polynomial given by $p_\pi(x) = p(x_\pi)$.
	Then, for all $s_1,\dots,s_k\in \Z_{\geq 0}$, we have that $\partial_1^{s_1}\cdot\partial_2^{s_2}\cdots \partial_k^{s_k} \brac{p_\pi} = \brac{\partial_{\pi^{-1}(1)}^{s_1}\cdot\partial_{\pi^{-1}(2)}^{s_2}\cdots \partial_{\pi^{-1}(k)}^{s_k} \brac{p}}_\pi$.
	
	By the above, we have that for each $d\in \N$,
	\begin{align*}
		\partial_1^{2}\cdot\partial_2^{2}\cdots \partial_k^{2}\brac{\Sym(q^d)} 
		&=  \partial_1^{2}\cdot\partial_2^{2}\cdots \partial_k^{2}\brac{\sum_{\pi\in S_k}q_\pi^d}
		\\&= \sum_{\pi\in S_k} \partial_1^{2}\cdot\partial_2^{2}\cdots \partial_k^{2}\brac{q_\pi^d}
		\\&= \sum_{\pi\in S_k} \brac{\partial_{\pi^{-1}(1)}^{2}\cdot\partial_{\pi^{-1}(2)}^{2}\cdots \partial_{\pi^{-1}(k)}^{2} \brac{q^d}}_\pi
		\\&= \sum_{\pi\in S_k} \brac{\partial_{1}^{2}\cdot\partial_{2}^{2}\cdots \partial_{k}^{2} \brac{q^d}}_\pi
		\\&= \Sym\brac{\partial_1^{2}\cdot\partial_2^{2}\cdots \partial_k^{2}\brac{q^d}}.
	\end{align*}	
	Now, the result follows from Lemma~\ref{lemma:pow_sec_der}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proving the Gaussian Variant}

We start by proving a slight variant of Proposition~\ref{prop:gaussian_counter_eg}, where we allow $f$ to be an arbitrary (possibly unbounded) polynomial.

\begin{lemma}\label{lemma:hermite_counter_eg}
	Let $k\in \N$, and let $\Sigma\in \R^{k\times k}$ be a symmetric positive semi-definite matrix such that:
	\begin{enumerate}
		\item For each $i\in [k]$, it holds that $\Sigma_{i,i} = 1$.
		\item The matrix $V = \Sigma - I$ has no row/column as all zeros.
	\end{enumerate}
	Then, there exists a polynomial $f:\R\to \R$ such that $\E_{X\sim \mc N(0,1)} \sqbrac{f(X)} = 0$, and \[ \abs{\E_{X\sim \mc N(0, \Sigma)}\sqbrac{\prod_{i\in [k]} f(X_i)}} > 0 .\]
\end{lemma}

\begin{proof}
	For $s = (s_1,\dots,s_k)\in \N^k$ and $\alpha = (\alpha_1,\dots,\alpha_k)\in \R^{k}$, let $f_{s,\alpha}:\R\to \R$ be the polynomial defined by $f_{s,\alpha}(x) = \alpha_1 H_{s_1}(x)+\cdots + \alpha_k H_{s_k}(x)$, where the polynomials $H_{s_i}$ are Hermite polynomials (see Definition~\ref{defn:hermite_poly}).	
	Observe that since $s_1,\dots,s_k \geq 1$, this polynomial satisfies $\E_{X\sim \mc N(0,1)} \sqbrac{f(X)} = 0$.

	Suppose, for the sake of contradiction, that for every $s\in \N^k,\ \alpha\in \R^k$, it holds that 
	\[\E_{X\sim \mc N(0, \Sigma)}\sqbrac{\prod_{i\in [k]} f_{s,\alpha}(X_i)} = \E_{X\sim \mc N(0, \Sigma)}\sqbrac{\prod_{i\in [k]} \sum_{j\in [k]}\alpha_jH_{s_j}(X_i) } =  0.\]
	Observe that for every $s\in \N^k$, the above expression can be written as a multivariate polynomial in $\alpha_1,\dots,\alpha_k$.
	If the polynomial vanishes for all $\alpha\in \R^k$, the coefficient of $\alpha_1\cdot \alpha_2\cdots \alpha_k$ must be zero; that is,
	\[ \sum_{\pi\in S_k}\E_{X\sim \mc N(0, \Sigma)}\sqbrac{\prod_{i\in [k]} H_{s_{\pi(i)}}(X_i)} = 0.\]
	Now, applying Lemma~\ref{lemma:hermite_exp}, we get that for each $d\in \N$, and each $s_1,\dots,s_k \geq 1$ with $s_1+\dots+s_k=2d$,
	\[\sum_{\pi\in S_k}\sqbrac{\brac{t^{\top}V\ t}^d : t_1^{s_{\pi(1)}}\cdots t_k^{s_{\pi(k)}}}
		= \sum_{\pi\in S_k}\sqbrac{\brac{t_\pi^{\top}V\ t_\pi}^d : t_{1}^{s_k}\cdots t_{k}^{s_k}}
		= \sqbrac{ \Sym\brac{\brac{t^{\top}V\ t}^d} : t_{1}^{s_k}\cdots t_{k}^{s_k}} = 0. \]
	Note that the assumption that $V$ has no zero row/column implies that for every $i\in [k]$, the polynomial $\partial_i \brac{t^{\top}V\ t}$ is not identically zero.
	By Lemma~\ref{lemma:polynomial_all_var}, this is a contradiction.
\end{proof}

With the above, we now prove Proposition~\ref{prop:gaussian_counter_eg} via a standard truncation argument.

\begin{proof}[Proof of Proposition~\ref{prop:gaussian_counter_eg}]
By Lemma~\ref{lemma:hermite_counter_eg}, we know that there exists a polynomial $f:\R\to \R$ such that $\E_{X\sim \mc N(0,1)} \sqbrac{f(X)} = 0$, and $\abs{\E_{X\sim \mc N(0, \Sigma)}\sqbrac{\prod_{i\in [k]} f(X_i)}} > 0$.

For each integer $M\in \N$, we define the truncated function $f_M:\R\to [-M,M]$ by 
\[f_M(x) = f(x)\cdot \ind_{\abs{f(x)}\leq M} + M\cdot \ind_{f(x) > M} - M\cdot \ind_{f(x) < -M}.\]
Also, let $g_M:\R\to [-2M,2M]$, be given by $g_M(x) = f_M(x) - \E_{X\sim \mc N(0,1)}\sqbrac{f_M(X)}$.
Observe that
\begin{enumerate}
	\item For every $M$, it holds that $\E_{X\sim \mc N(0,1)}\sqbrac{g_M(X)} = 0$.
	\item For every $M$, the function $g_M$ is bounded and Lipschitz continuous.
	\item For every $x\in \R$, $f_M(x)\to f(x)$ as $M\to \infty$. Further, since $\abs{f_M(x)} \leq \abs{f(x)}$ for each $x\in \R, M\in \N$, by the dominated convergence theorem, we have  $\E_{X\sim \mc N(0,1)}\sqbrac{f_M(x)}\to \E_{X\sim \mc N(0,1)}\sqbrac{f(x)}=0$ as $M\to \infty$.
	This implies that for each $x\in \R$, $g_M(x)\to f(x)$ as $M\to \infty$.

	Also, for each $x\in \R, M\in \N$, we have $\abs{g_M(x)} \leq \abs{f(x)} + \E_{X\sim \mc N(0,1)}\sqbrac{\abs{f(X)}}$. Hence, by the dominated convergence theorem, we have that $\E_{X\sim \mc N(0, \Sigma)}\sqbrac{\prod_{i\in [k]} g_M(X_i)} \to \E_{X\sim \mc N(0, \Sigma)}\sqbrac{\prod_{i\in [k]} f(X_i)} \not= 0$ as $M\to \infty$.
\end{enumerate}

By the above, for some large enough $M$, the function $\frac{1}{2M}\cdot g_M:\R\to [-1,1]$ satisfies the desired properties.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%