\section{Preliminaries}\label{sec:prelims}

We use $\exp$ to denote the exponential function, given by $\exp(x) = e^x$ for $x\in \R$.

Let $\N = \set{1,2,\dots}$ be the set of natural numbers. For each $n\in \N$, we use $[n]$ to denote the set $\set{1,2,\dots,n}$.
For non-negative functions $f,g:\N\to \R$, we say that $f(n) = o_n(g(n))$ if $\lim_{n\to \infty} \frac{f(n)}{g(n)}=0$.

For a probability distribution $\nu$ on $\mc X$, we use $\supp(\nu)$ to denote its support.
For $n\in \N$, we use $\nu^{\otimes n}$ to denote the $n$-fold product distribution on $\mc X^n$.
In particular, we shall be interested in the case when $\mc X \subseteq  \R^k$ for some $k\in \N$.
In this case, for vectors $x \in \R^{kn}$, we shall use subscripts for indices in $[k]$ and superscripts for indices in $[n]$; that is, for each $i\in [k], j\in [n]$, we use $x_i\up{j}$ to denote the $(i,j)^\textsuperscript{th}$ coordinate of $x$.
Further, for each $i\in [k]$, we use $x_i$ to denote the vector $\brac{x_i\up{1},\dots,x_i\up{n}}\in \R^n$, and similarly for each $j\in [n]$, we use $x\up{j}$ to denote the vector $\brac{x_1\up{j},\dots,x_k\up{j}}\in \R^k$.

For $k\in \N$, let $S_k$ denote the group of all permutations on $[k]$.
	For each $\pi\in S_k,\ x\in \R^k$, we use $x_\pi$ to denote $\brac{x_{\pi(1)},\dots,x_{\pi(k)}} \in \R^k$.
With this notation, we define the symmetrization of functions over $\R^k$:
\begin{definition}\label{defn:sym_op}
	For any function $f:\R^k\to \R$, we define its symmetrization as the function $\Sym(f):\R^k\to \R$, given by $\Sym(f)(x) = \sum_{\pi\in S_k}f(x_\pi)$.
\end{definition}

We shall use the following facts from probability theory:

\begin{fact}\label{fact:chebyshev}(Chebyshev's Inequality; see~\cite{Dur19} for reference)
	Let $X$ be a random variable such that $\E\sqbrac{X^2}<\infty$.
	Then, for any any $a>0$,
	\[\Pr\sqbrac{\abs{X-\E\sqbrac{X}} \geq a} \leq \frac{\Var\sqbrac{X}}{a^2}.\]
\end{fact}

\begin{fact}\label{fact:hoeffding}(Hoeffding's Inequality~\cite{Hoe63})
	Let $X_1,\dots, X_n$ be independent random variables such that $a_i\leq X_i \leq b_i$ almost surely, and let $S = \sum_{i=1}^n X_i$.
	Then, for all $t>0$,
	\[\Pr\sqbrac{\abs{S - \E\sqbrac{S}} \geq  t} \leq 2\cdot \exp\brac{-\frac{2t^2}{\sum_{i=1}^n \brac{b_i-a_i}^2}}. \]
\end{fact}


\begin{theorem}\label{thm:multi_clt} (Multivariate Central Limit Theorem; see~\cite{Dur19} for reference)
Let $X\up{1},X\up{2},\dots$ be $\R^k$-valued i.i.d. random vectors, with mean zero, and finite a covariance matrix $\Sigma \in \R^{k\times k}$ given by $\Sigma_{i,j} = \E\sqbrac{X\up{1}_i\cdot X\up{1}_j}$.
If $S_n = \frac{1}{\sqrt n} \sum_{i=1}^n X\up{i}$, then, $S_n \xrightarrow{\mc D} Z$ as $n\to \infty$, for $Z\sim  \mc N(0, \Sigma)$.
That is, for every bounded continuous function $H:\R^k\to \R$, \[\lim_{n\to \infty} \E\sqbrac{H(S_n)} = \E_{Z\sim \mc N(0,\Sigma)}\sqbrac{H(Z)}.\]
	
\end{theorem}

We shall also use the following fact about zeros of polynomials:
\begin{lemma}\label{lemma:dim_var}
	Let $p_1,\dots,p_r:\R^k\to \R$ be non-zero polynomials.
	Then, there exists $y\in \R^k$ such that for each permutation $\pi\in S_k$, and each $i\in [r]$, it holds that $p_i(y_\pi) \not= 0$.
\end{lemma}
\begin{proof}[Proof Sketch]
	The zero-set of any non-zero polynomial has measure zero, with respect to the Lebesgue measure on $\R^k$.
	Hence, by sub-additivity, the set of points $y\in \R^k$ violating the statement of the lemma is of measure zero as well.
\end{proof}


Next, we give some basic results about the probabilist's Hermite polynomials. The reader is referred to Chapter 11 in~\cite{Don14} for more details. 

\begin{definition}\label{defn:hermite_poly}
	The Hermite polynomials $(H_j)_{j\in \Z_{\geq 0}}$ are univariate polynomials, with $H_j$ a monic polynomial of degree $j$, satisfying the power series expression
	\[ \exp\brac{tx-\frac{t^2}{2}} = \sum_{j=0}^\infty \frac{1}{j!}\cdot H_j(x)\cdot t^j, \quad \text{for } t,x\in \R. \]
	Note that the series above is absolutely convergent, with $  \sum_{j=0}^\infty \frac{1}{j!}\cdot \abs{H_j(x)}\cdot \abs{t}^j \leq  \exp\brac{\abs{t}\cdot \abs{x}+\frac{t^2}{2}}$ for each $t,x\in \R$.
\end{definition}

\begin{lemma}\label{lemma:hermite_exp}
	Let $k\in \N$ and $s_1,s_2,\dots,s_k \in \Z_{\geq 0}$, and let $\Sigma\in \R^{k\times k}$ be a positive semi-definite matrix such that $\Sigma_{i,i}=1$ for each $i$.	
	For $V=\Sigma-I$, it holds that \[\E_{X\sim \mc N(0,\Sigma)}\sqbrac{H_{s_1}(X_1)\cdots H_{s_k}(X_k)} = \frac{s_1!\cdots s_k!}{d!\cdot 2^d}\cdot \sqbrac{\brac{t^{\top}V\ t}^d : t_1^{s_1}\cdots t_k^{s_k}} \]
	where $s_1+\dots+s_k = 2d$, and $\sqbrac{\brac{t^{\top}V\ t}^d: t_1^{s_1}\cdots t_k^{s_k}}$ denotes the coefficient of $t_1^{s_1}\cdots t_k^{s_k}$ in the polynomial $\brac{t^{\top}V\ t}^d$.
	Also, the above expectation is zero when $s_1+\dots+s_k$ is odd.
\end{lemma}
\begin{proof}
	Recall that the moment generating function of a multivariate Gaussian distribution is given by
	\[\E_{X\sim \mc N(0,\Sigma)}\sqbrac{\exp\brac{t_1X_1+\dots t_kX_k}} = \exp\brac{\frac{1}{2}\cdot t^{\top}\Sigma\ t},\]
	for each $t\in \R^k$.
	Multiplying the above by $\exp(-\frac{1}{2}\cdot t^\top t)$, and plugging in the power series in Definition~\ref{defn:hermite_poly}, we get for each $t\in \R^k$ that
	\begin{align*}
		\sum_{d=0}^{\infty} \frac{1}{d!\cdot 2^d}\cdot \brac{t^{\top}V\ t}^d
		&= \exp\brac{\frac{1}{2}\cdot t^{\top}V\ t} 
		\\&= \E_{X\sim \mc N(0,\Sigma)}\sqbrac{ \exp\brac{\brac{t_1X_1-\frac{t_1^2}{2}}+\dots +\brac{t_kX_k -\frac{t_k^2}{2}} } }
		\\&= \E_{X\sim \mc N(0, \Sigma)}\sqbrac{ \brac{\sum_{s_1=0}^\infty \frac{1}{s_1!}\cdot H_{s_1}(X_1)\cdot t_1^{s_1} }\cdots \brac{\sum_{s_k=0}^\infty \frac{1}{s_k!}\cdot H_{s_k}(X_k)\cdot t_k^{s_k}}}
		\\&= \sum_{s_1,\dots,s_k\geq 0}\frac{t_1^{s_1}\cdots t_k^{s_k}}{s_1!\cdots s_k!}\cdot \E_{X\sim \mc N(0, \Sigma)}\sqbrac{H_{s_1}(X_1)\cdots H_{s_k}(X_k)}.
	\end{align*}
	Note that since the power series in Definition~\ref{defn:hermite_poly} is absolutely convergent, all steps above of interchanging limits and expectations are valid by the dominated convergence theorem. 
	Finally, comparing coefficients, we have the desired result.
\end{proof}
