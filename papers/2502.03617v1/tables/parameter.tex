\begin{table*}[t]
	\centering
	\caption{Summary of Model Training Parameters and Memory Utilization in Megabytes (MB)}
	%\ANTONIO{SAIMA explais the columns}
	\scriptsize
	\label{tab:parameters}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{llccccccc}
			\toprule
			\bf Training Strategy & \bf Model & \bf Peak GPU Mem. Consumption (MB) & \bf Trainable Parameters & \bf Model's Parameters & \bf Trainable \% \\
			\midrule
			\multirow{5}{*}{\emph{QLoRA Fine-Tuning}} & CodeLlama-7b & 11,877  & 20,277,376 & 6,758,820,064 & 0.300 \\
			& CodeLlama-34b & 37,424 & 54,781,952 & 33,798,752,256 & 0.162 \\
			
			\noalign{\smallskip}
			\cline{2-6}
			\noalign{\smallskip}
			& DeepSeek-Coder-1.3b &    5,154  & 7,770,112 & 1,354,242,048 & 0.574 \\
			& DeepSeek-Coder-6.7b & 12,894  & 20,279,296 & 6,760,792,064 & 0.300 \\
			& DeepSeek-Coder-33b &  39,724 & 61,898,752 & 33,404,890,112 & 0.185 \\
			\midrule
			\emph{Full Fine-Tuning} & DeepSeek-Coder-1.3b & 16,776  & 1,354,242,048 & 1,354,242,048 & 100 \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.2cm}
\end{table*}

