% !TEX root = ../main.tex


\section{Results and Discussion}
\label{sec:results}

%\begin{figure*}[ht!]
%	\centering
%	\includegraphics[width=\linewidth]{images/rq1.pdf}
%	\caption{Results achieved by Full Fine-tuned models \emph{vs.} QLoRA-Optimized models in code summarization}
%	\label{fig:rq1}
%\end{figure*}


In this section, we present and discuss the results of our study addressing our RQ, which aims to evaluate the effectiveness and memory footprint of the CLMs when fine-tuned with QLoRA, compared to full fine-tuning.  

\subsection{QLoRA vs Full Fine-Tuning}

As described in \secref{sub:analysis}, we fine-tuned DeepSeek-Coder 1.3B under two configurations: QLoRA fine-tuning and full fine-tuning (FFT). The CodeLlama models are only fine-tuned with QLoRA due to the excessive computational costs associated with full fine-tuning, which we could not afford. \tabref{tab:parameters} illustrates the numbers of \textit{total model parameters} and \textit{trainable parameters} for our selected models. 

%\vspace{2pt}
\input{tables/metrics_res.tex}

%\input{tables/statistical_analysis.tex}
%\input{tables/statistical_analysis_size.tex}
\input{tables/parameter.tex}



%\textbf{\emph{To what extent do the positive outcomes of fine-tuning CLMs for code generation with QLoRA extend to the task of code summarization?}}

%To address this $RQ$, we start by fine-tuning the DeepSeek-Coder 1.3B model under two distinct configurations. In the first configuration, the model is fine-tuned using QLoRA. Hence, unlike the full fine-tuning approach, we fine-tune only a limited subset of parameters, referred to as \textit{trainable parameters}, while keeping the base model frozen through the application of 4-bit quantization. This approach enables fine-tuning by significantly reducing the number of trainable parameters while maintaining the model’s core structure. \tabref{tab:parameters} illustrates the numbers of \textit{total model's parameters} and \textit{trainable parameters} for our selected models. Next, we perform full fine-tuning on all parameters of DeepSeek-Coder, meaning the gradient is back-propagated across the entire network that includes 1.3B parameters. 

\tabref{tab:model-per} summarizes the performance results for the five models optimized using QLoRA, alongside the Full Fine-Tuning (FFT) of DeepSeek-Coder 1.3B. A key insight that emerges after observing the second and third rows is that QLoRA-based fine-tuning consistently delivers superior performance compared to full fine-tuning of DeepSeek-Coder 1.3B across the two programming languages.
For example, in terms of the METEOR score, the QLoRA fine-tuned model surpasses its fully fine-tuned counterpart by approximately 2\% for both Python and Java. Similarly, based on the ROUGE-L metric, performance improvements range from 1.9\% to 2.7\%, with the largest gains observed for Java. For each metric reported in \tabref{tab:model-per}, the observed differences were found to be statistically significant (based on a Wilcoxon signed-rank test at 95\% significance), though the effect size is negligible. The detailed results of this analysis are available in our online replication package~\cite{replication}.

\tabref{tab:parameters} shows the results of our analysis of the peak GPU memory consumption during model fine-tuning. The table reveals a substantial difference between QLoRA and full fine-tuning. FFT of the 1.3B parameters requires an average of approximately 16GB of GPU memory, while QLoRA fine-tuning significantly reduces memory usage, requiring only 5GB on average--saving 10GB compared to FFT (\ie about a third of the FFT memory footprint). 
This reduction in memory consumption can be attributed to the considerably lower number of trainable parameters in QLoRA (\tabref{tab:parameters}--Columns 4 and 5), which represents a significant scale difference (millions vs. billions) compared to training all parameters. The freed-up memory can be used to support additional tasks, enable larger batch sizes, or improve overall computational efficiency, making QLoRA a more practical and scalable solution for resource-constrained environments.


The results indicate that QLoRA is not only effective in optimizing resource efficiency but also in outperforming full fine-tuning. These findings align with those presented in the pioneering work that introduced QLoRA~\cite{dettmers2024qlora}. 
 
In the software engineering literature, Weyssow \etal~\cite{weyssow2023exploring} demonstrated that for coding activities--particularly code generation--the use of LoRA adapters for fine-tuning large language models outperforms FFT. 
Furthermore, in the same study, they showed that QLoRA fine-tuning surpasses LoRA fine-tuning for the same task, establishing a clear hierarchy of fine-tuning strategies: FFT $<$ LoRA fine-tuning $<$ QLoRA fine-tuning. Hence, if LoRA fine-tuning already outperforms FFT, this suggests that once the underlying model’s knowledge, distilled within its parameters, is frozen--as in LoRA and QLoRA fine-tuning--adapting a smaller subset of parameters is sufficient to effectively capture the nuances of the intended task.
As we show, QLoRA improves model performance while reducing memory footprint.
This result might seem counterintuitive, given that QLoRA relies on extreme quantization to optimize memory usage. However, a possible explanation for this behavior comes from Dettmers \etal (the authors of QLoRA), who observe that any performance degradation due to information loss during quantization is not only fully recovered but often surpassed through the fine-tuning of LoRA modules after the quantization process. 

\vspace{-3pt}
\begin{boxM}
	\textbf{\emph{Finding$_{1}$}:} QLoRA fine-tuning for code summarization delivers performance on par with what is observed for other software engineering tasks such as code generation \cite{weyssow2023exploring}.
	By optimizing a limited subset of quantized parameters, it outperforms full fine-tuning in terms of predictive performance and memory consumption.
\end{boxM}



Examining the impact of varying parameter counts, the results align with our expectations: larger models consistently outperform their smaller counterparts when QLoRA is applied to support code summarization tasks (see \tabref{tab:model-per}). For example, CodeLlama-34B demonstrates significantly higher performance compared to its 7-billion-parameter variant, and a similar pattern is found with DeepSeek-Coder, where larger versions achieve superior results. These trends hold true across both programming languages, underscoring the effect of QLoRA regardless of the model size.
However, this improvement comes at a cost. Larger models demand significantly more GPU memory during training, with usage peaking at 40 GB when fine-tuning DeepSeek-Coder 33B and 37.5 GB when fine-tuning CodeLlama 34B, as detailed in \tabref{tab:parameters}.
This highlights the trade-off between model size, performance, and resource requirements. Depending on the final application and available hardware, one may need to prioritize performance over resource consumption or vice versa.
In other words, if GPU memory allocation is limited, sacrificing some performance may be a reasonable trade-off, especially considering the capabilities of models with around 7B parameters.
The observed improvements resulted in statistically significant differences, though the effect sizes of these differences are negligible. 
%For instance, with DeepSeek-Coder models featuring 1.3B and 6.7B parameters, the statistically significant differences in METEOR scores still show negligible effect sizes. 
The detailed results of all the statistical analyses are available in our online replication package~\cite{replication}.
Scaling up to DeepSeek-Coder 33B, the improvements, while still statistically significant, exhibit diminishing returns. The performance gap narrows across all evaluated metrics, as evidenced by the negligible effect sizes. Although larger models generally offer greater capacity, the diminishing improvements suggest that further scaling might not always justify the increased computational resources, particularly for code summarization.

In contrast, for CodeLlama, scaling from 7B to 34B yields more pronounced gains. The larger 34B variant achieves a 3-4\% improvement in METEOR scores over its smaller counterpart, equating to an 11.8\% improvement for Python and  8.4\% for Java. These results highlight the effectiveness of both the CodeLlama model family and DeepSeek-Coder in leveraging increased parameter counts to enhance performance. This trend becomes particularly apparent when examining the top-performing models in \tabref{tab:model-per}, where the best results across all experimental configurations are highlighted in yellow. Notably, four out of six metrics in our evaluation reach their highest values with models from the CodeLlama family that have a parameter count exceeding 30B.
The performance differences remain consistent across embedding-based metrics (BERTScore-F1 for both languages, SIDE for Java) and are statistically significant but with negligible effect sizes, suggesting limited practical impact. This reinforces that while larger models can improve performance, the gains may not justify the increased GPU memory consumption.



\begin{boxM}
	\textbf{\emph{Finding$_{2}$}:}  Larger models generally offer greater capacity and potential better support for code summarization, but they eventually reach a point of diminishing return. However, if maximizing performance is the primaryobjective, CodeLlama 34B delivers the best outcomes
	for code summarization, with improvements of 11.8\% for Python and 8.4\% for Java.
\end{boxM}

%\begin{figure*}[t!]
%	\centering
%	\includegraphics[width=0.9\linewidth]{images/time-memory.jpg}
%	\caption{Training Time and GPU Memory for FFT and QFT Model}
%	\label{fig:time-mem}
%\end{figure*}

%This is the picture \figref{fig:time-mem}


\subsection{Qualitative Analysis}

\figref{fig:qualitative-examples} presents four triplets of $\langle$Method, Target$_{summary}$ or GT, Predicted$_{summary}$ or PR$\rangle$. The examples are divided by programming language, with two for each language. The top section features Java examples, while the bottom showcases two Python examples. In all cases, the model-generated comments are not only accurate compared to the ground truth but also offer improvements over it. We remind the reader that this type of code comment falls under the category of \emph{meaningful code descriptions} (\secref{sec:qualitative}).

Specifically, focusing on the first triplet~\circled{1}, the developer-provided ground truth summary, \texttt{attempt to exit from an already switched user}, encapsulates the method's basic functionality. However, the CodeLlama 34B model optimized  using QLoRA, generates a summary that clarifies the method’s logic, explicitly documenting that the method \texttt{attempts to exit the current user by returning the original user that was being impersonated}. This provides valuable additional information, specifically noting that \texttt{the method returns the original user who was being impersonated}. This distinction makes the summary more comprehensive, as it clarifies the intended logic and provides insights into the rationale of the code.
%By specifying that the original user is returned, the model's summary becomes significantly more informative, particularly for developers who need to understand the implications of invoking this method. Instead of simply stating that the user exit occurs, the model's summary highlights who is returned, offering critical context for understanding its effect on user sessions and authentication workflows.

% receiving the recommendation.

In \circled{2}, the predicted summary demonstrates significant improvement, as the model captures details that the developer overlooked or deemed unnecessary, such as the process of extracting tokens. The model not only identifies these elements but also elaborates on them, providing a more comprehensive and actionable summary: \texttt{extracts the scope from the access token and converts them to grant authorities}. This enhanced prediction demonstrates the model’s capability to infer additional context and generate summaries that not only ``copy'' tokens already present in the method but also synthesize new information. For instance, the word-token \texttt{access} was inferred through the model’s deeper understanding of the code’s logic and intent. This ability allows for the creation of more comprehensive and insightful summaries.


Turning to the Python first example \circled{3}, the prediction adds depth by conveying an additional message: that the \texttt{dagrun must be retrieved based on the most recent execution date}.  This enriched context provides developers with summaries that are not only concise but also contextually informative.
The second Python example (\circled{4}) illustrates that, despite the significantly smaller number of parameters adjusted during fine-tuning (\ie millions rather than billions), the performance of the QLoRA-optimized model remains, even for tasks demanding high contextual reasoning. Notably, the model identifies an important detail revealed only at the conclusion of the method: \emph{list\_py\_paths is a recursive method}. This ability to detect nuanced information demonstrates the model’s effectiveness in generating meaningful and context-aware summaries despite the limited parameter adjustment, quantized and dequantized, as explained in \secref{sub:design_qlora}.


\begin{figure}[t!]
	\centering
	\includegraphics[width=\columnwidth]{images/qualitative-examples2.pdf}
	\caption{Examples of predictions made by CodeLlama 34B that have been labeled as \emph{meaningful code summaries}. }
	\label{fig:qualitative-examples}
\end{figure}

These instances align with the findings of the manual investigation conducted on 384 incorrect Java summaries and 384 incorrect Python summaries.

For Java, 15.36\% of conflicts arose during the labeling process. These conflicts, resolved by a third author who was not involved in the initial labeling, resulted in the following breakdown: 31.07\% of summaries were deemed semantically equivalent to the ground truth, 53.0\% were partially equivalent, and in 8.87\% of cases, CodeLlama 34B provided summaries that were more accurate and informative than those written by developers. Finally, 7.57\% of the summaries were classified as incorrect.
\looseness=-1

A similar pattern was observed for Python, with slight variations in the distribution across categories. Specifically, CodeLlama 34B generated 37.86\% of summaries as a real developer would do (\ie semantically equivalent summaries), 53.0\% were partially equivalent, and in 3.66\% of cases, CodeLlama 34B produced recommendations superior to those of developers. Finally, 5.48\% of the recommendations were found to be incorrect.


\subsection{Can We Translate the QLoRA Benefits for Code Summarization to General-Purpose Language Models?}
\label{sub:phi3}

Our investigation demonstrated that QLoRA can serve as a resource-efficient training strategy for code summarization, paving the way for advancements in Code-to-NL tasks. However, an open question remains: \emph{can these findings be generalized to models that have been pre-trained—not solely but partially—on software engineering data?}

The rationale behind this question lies in the growing adoption of hybrid models, such as Phi-3 mini \cite{abdin2024phi}, which have been benchmarked extensively on coding tasks while also serving as baselines in comparisons against both code-specific and general-purpose language models \cite{deng2024assessing}. These models blur the lines between domain-specific and general-purpose architectures, offering a unique ground for evaluating the transferability of QLoRA's fine-tuning benefits.

%Therefore, we explore to what extent the results obtained using large code models transfer into hybrid models--that, during pre-training, are exposed to a mixture of code, natural language, and technical natural language (\eg code comments). 
For such analysis, we selected Phi-3 mini \cite{abdin2024phi}, a 3.8-billion-parameter model introduced by Microsoft in 2024 that was shown to achieve results on par with Llama 3 \cite{dubey2024llama}, Meta’s state-of-the-art 7-billion-parameter model, when applied to code-related tasks such as code generation. Phi-3 mini is nearly half the size of Llama 3 \cite{dubey2024llama}. We chose this Microsoft model for the generalizability analysis due to its popularity  and its frequent use as a baseline for coding tasks \cite{deng2024assessing, peixoto2024effectiveness}. 




\input{tables/phi3.tex}
We trained Phi-3 mini using two distinct configurations, replicating the approach used for DeepSeek-Coder 1.3B (\secref{sec:design}). Specifically, we conducted both full model fine-tuning and QLoRA fine-tuning, followed by model evaluation in each scenario. The training and evaluation processes were carried out as outlined in \secref{sec:design}.

\tabref{tab:phi3} presents the results of our experiments with Phi-3 mini. Notably, focusing on the BLEU metrics, the QLoRA-optimized model consistently outperforms the fully fine-tuned model, showing a performance improvement of 2\% for Python and 0.9\% for Java. This trend extends to other metrics, such as METEOR, ROUGE-L, and chrF, where the QLoRA-optimized model surpasses the FFT model by a margin of approximately 2–3\%.
Further analysis compares the performance of code models with the general-purpose phi-3 mini model. While code models demonstrate better performance than Phi-3 mini, the observed gap is not substantial. To validate this finding, we conducted a Wilcoxon signed-rank test between the Phi-3 mini-3.8B and DeepSeek-Coder-1.3B models, evaluating both fully fine-tuned and QLoRA fine-tuned versions.
The analysis failed to reveal statistically significant differences  for Java (regardless of the evaluation metric), whereas, for Python, this behavior was observed only for the BLEU metric in the fully fine-tuned setup. These analysis details are included in our replication package~\cite{replication}.
%These results further emphasize the nuanced differences in performance across the models and tasks.
\vspace{5pt}

