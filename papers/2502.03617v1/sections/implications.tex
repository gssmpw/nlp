\section{Implications of our findings} 
\label{sec:implications}


%we concentrated on the extensively studied domain of code summarization. This choice was motivated by its status as a well-researched task, as evidenced by a substantial body of related literature. Furthermore, code summarization 

Among the various applications of code-related bi-modal tasks, code summarization stands as a fundamental endeavor in software engineering, playing a crucial role in enhancing developer productivity \cite{majdoub2024debugging, mastropaolo2021studying}, improving code comprehension~\cite{zhang2020retrieval, fang2024esale}, and supporting software maintenance \cite{yang2024multi}.

Based on our findings, we draw the following implications:

\smallskip


\noindent\textbf{Improving CLMs Sustainability via QLoRA fine-tuning.} By applying QLoRA fine-tuning to two SoTA code models for code summarization activities, we demonstrated that it achieves competitive results compared to full model fine-tuning. Additionally, QLoRA significantly reduces memory requirements, cutting the memory footprint by approximately a third (see \tabref{tab:parameters}), which enhances the scalability, sustainability, and usability of advanced AI systems built on large language models for code. 
%In addition, because of the limited resources required for fine-tuning, QLoRA expands the usability of CLMs to a broader audience.

%Thus, this approach can significantly reduces the carbon footprint of training and fine-tuning large-scale models while enhancing their accessibility for researchers and practitioners with l

%For example, in educational research, where access to high-performance computing resources is often limited, QLoRA can facilitate the adaptation and application of code language models (CLMs) in diverse settings, fostering innovation and inclusivity in research and practice.
\smallskip


\noindent\textbf{QLoRA streamlines CLMs training for code summarization with comparable success to code generation.}\\
	%--have demonstrated compatibility with resource-efficient strategies, particularly QLoRA proving their adaptability to such method.}
The successful adaptation of QLoRA for code summarization underscores its potential to enhance a wide range of code-related tasks through efficient fine-tuning.
%, without specific limitations. 
In software engineering research, this paves the way for exploring QLoRA's capability to fine-tune CLMs for hybrid tasks that integrate code and natural language, such as code review.
%, such as log statement generation. 
%For instance, in log statement generation, the model is required to produce both code and natural language text resembling a log message.
Examining how QLoRA performs in these hybrid scenarios could unlock new possibilities for leveraging code models in complex, real-world applications. 
%Additionally, it would provide valuable contributions to the growing body of knowledge that intersects sustainability with software engineering automation.