\section{Study Methodology} 
\label{sec:design}

%\os{do we need a figure that gives an overview of the methodology, including RQs, data, fine-tuning, etc?} NIONIO: we use 1 to explain QLoRA


The main goal of this study is to investigate the application QLoRA fine-tuning to code language models (CLMs) for code summarization. QLoRA combines PEFT and quantization techniques, resulting in substantial improvements in memory efficiency during LLM training compared to LoRA~\cite{dettmers2024qlora}. The study addresses the following research question (RQ): 

\begin{itemize}[label=,leftmargin=0.2cm]
	\item \textbf{RQ:} \emph{How effective and memory-efficient are CLMs for code summarization when fine-tuned with QLoRA, compared to full fine-tuning?}


% of Python and Java.


%\vspace{10pt}

		
	
	%	 do the positive outcomes of fine-tuning CLMs for code generation with QLoRA extend to the task of code summarization?} 
	
	%Bi-modal code intelligence tasks demand a high degree of proficiency in both natural language understanding and code generation, requiring models to accurately interpret natural language inputs and generate appropriate code outputs. State-of-the-art LLMs, which are typically fine-tuned across all parameters, demonstrate superior performance in these tasks but come with substantial computational costs \cite{cursaru2024controlled}.
	%By evaluating task-specific fine-tuning through QLoRA, we seek to understand whether a more computationally efficient approach can still meet the performance benchmarks set by fully fine-tuned models, particularly in complex bi-modal tasks such as Code Generation and Code Summarization. The findings from this analysis will open up the path to understanding whether QLoRA fine-tuning offers a viable alternative to end-to-end fine-tuning without significant trade-offs in performance.
	
	%How does the number of QLoRA layers influence model performance on code intelligence tasks?
	
	%\item \textbf{RQ$_2$:} \emph{What configuration of QLoRA hyperparameters yields the best performance for code-related bi-modal tasks?} In RQ$_2$, we explore how different configurations of (Q)LoRA hyperparameters influence the performance of LLMs on tasks involving bi-modal data, such as code generation and code summarization. Specifically, we analyze the impact of varying the LoRA hyperparameters, particularly those controlling number of layers and trainable parameters involved in the fine-tuning process, to evaluate how their impact on task performance.\\
	%Specifically, we aim to explore how adjusting the number of layers used for fine-tuning affects the model's ability to perform tasks such as code summarization and code generation.
	%Specifically, This research question focuses on evaluating the influence of LoRA hyperparameters, which. Given the computational constraints of fine-tuning LLMs, identifying the optimal configuration for code-related tasks is critical for achieving a balance between resource efficiency and model performance. Through this investigation, we seek to establish the extent to which the number of QLoRA layers contributes to the model's success in performing code intelligence tasks and whether increasing or reducing these layers leads to measurable performance differences.\\
	

	
	%\item \textbf{RQ$_2$:} \emph{How much transferring  knowledge from one programming langauge to another, helps QLoRA- models, in achieving better results when summarizing code components?} 
	%In RQ$_2$, we investigate whether multi-task fine-tuning using QLoRA facilitates knowledge transfer between in code summarization. Particularly, we explore whether a combination of multiple  improved overall performance compared to models fine-tuned on individual tasks. This research question aims to determine if training on both tasks simultaneously allows the model to leverage shared knowledge, thereby enhancing its capabilities across both domains.\\
	
	%Given the high computational costs associated with this type of study, we will focus our multi-task QLoRA fine-tuning on the LLMc that demonstrates the best performance in previous experiments RQ$_1$, utilizing the optimal QLoRA hyperparameter configuration identified in RQ$_2$. The outcome of this investigation will provide insights into the effectiveness of knowledge transfer through multi-task learning in improving model performance for complex, bi-modal code intelligence tasks. \\
	
	%Using QLoRA, can we size-up the model that features more parameters in lower precision and maintain the same performance and memory footprint as a fine-tuned, non-quantized smaller model? 
	

	%\item \textbf{RQ$_2$:} \emph{What is the impact of QLoRAâ€™s on the performance of CLMs with varying parameter sizes in code summarization tasks?} This research question examines how scaling up the parameters of CLMs and applying QLoRA affects memory usage, computational efficiency, and overall performance in code summarization. We specifically examine how varying model sizes, optimized the quantization approach featured QLoRA, balance resource usage with performance in producing meaningful code summaries. This analysis will help determine whether larger models still maintain their performance advantage as demonstrated in various studies \cite{sun2024extractive, shi2022evaluation, fang2024esale, ahmad2020transformer, gros2020code} where sizing up the number of model's parameters has been shown to improve the performance on the task at hand.
	
	%\textcolor{red}{ANTONIO: Saima, pay attention to the wording. See the following:In addition, we aim to assess if the larger, QLoRA quantized model can match or exceed the performance of the smaller, fine-tuned model, despite the substantial difference in parameter size. You should have said despite a reduction in weight representation precision: Noted}
	
	%This research question seeks to evaluate the trade-offs between model size, memory efficiency, and performance when utilizing lower precision quantization through QLoRA. By assessing whether a larger, quantized model can maintain performance parity with a smaller, non-quantized model while benefiting from a reduced memory footprint, we aim to uncover the potential advantages of scaling up models under quantized conditions in resource-constrained environments.
	
\end{itemize}

Through this RQ, we aim to validate our hypothesis that QLoRA is equally effective for code summarization as it is for code generation~\cite{weyssow2023exploring}.

To answer the RQ, we examine two state-of-the-art code models: CodeLlama~\cite{codellama} and DeepSeekCoder \cite{deepseek}. Each model is trained and evaluated on the CodexGLUE code summarization benchmark \cite{codexglue}, particularly the dataset comprising Python and Java code methods and their respective summaries~\cite{CodeXGLUEbench}. 
\looseness=-1

Additionally, we investigate the impact of scaling up the parameters of CLMs during QLoRA-based training, measuring changes in GPU memory usage and overall predictive performance for code summarization. This analysis aims to determine whether larger models retain their performance advantage, as demonstrated in previous studies \cite{sun2024extractive,shi2022evaluation,fang2024esale,gros2020code}, where increasing the number of model parameters has consistently improved task-specific performance.

We also investigate the generalizability of QLoRA for LLMs that, while widely utilized for automating SE-related tasks, were not primarily designed for such tasks. The details of this analysis are provided in \secref{sub:phi3}.

%Our objective is to assess whether applying QLoRA for fine-tuning large code models can yield positive outcomes as observed  by Weyssow \etal \cite{weyssow2023exploring} when specializing model for code generation. Specifically, we assess whether QLoRA fine-tuning for code summarization achieves performance comparable to that of CLMs undergoing full end-to-end fine-tuning, where all model weights are optimized. Additionally, we investigate the impact of scaling up the parameters of CLMs during QLoRA-based training, measuring changes in GPU memory usage, computational efficiency, and overall performance in code summarization. This analysis aims to determine whether larger models retain their performance advantage, as demonstrated in previous studies \cite{sun2024extractive,shi2022evaluation,fang2024esale,gros2020code}, where increasing the number of model parameters has consistently shown to enhance task-specific performance.


\subsection{Code Language Models (CLMs)} 
\label{sub:design_llm}
%\ANTONIO{Oscar note that here we talk about Code-specific. As such Phi-3 mini does not belog to this section.}
%\textcolor{red}{ANTONIO: no need here to report the number of parameters: OKAY}

For our study, we selected two families of state-of-the-art CLMs: CodeLlama \cite{codellama2} and DeepSeekCoder \cite{deepseek}. The models have been frequently investigated in prior work~\cite{deepseek,majdoub2024debugging,wang2024systematic}.  

Our selection includes models with distinct architectural or training features, making them well-suited for  code summarization. For example, the models are available in both instruction-tuned and non-instruction-tuned variants. Instruction-tuned models are optimized to process human-like instructions, making them particularly effective at manipulating natural language and code. This additional capability can be harnessed even in the context of QLoRA training, as demonstrated in prior work~\cite{yuan2023evaluating,fan2024exploring}.
%\ANTONIO{SAIMA, we have to find papers where the base model is an instruction-tuned model further fine-tuned on code-related tasks. Check claire's and david lo's paper} 

%In contrast, StarCoder2 is a base model, meaning it was fine-tuned without specific adaptations for instruction-based tasks and is currently the best-performing model that has not undergone instruction fine-tuning. Its inclusion allows us to explore how, and to what extent, QLoRA fine-tuning impacts state-of-the-art models without instruction tuning. This allows us to compare the effects of QLoRA on both instruction-tuned and base CLMs in code summarization, an area that to our knowledge, has not yet been examined.
%We believe this analysis has been carried out using the highest-performing base model currently available, ensuring a fair comparison with instruction-tuned models. This approach offers valuable insights into the performance differences between these two types of models.

%\os{we need to explicitly mention which models are used answer each RQ (i.e., with references to the RQs)} \SAIMA{Done}

%\subsubsection{CodeLlama \cite{codellama}}
\textit{CodeLlama}~\cite{codellama} is a family of open-source LLMs tailored for coding tasks. It is based on the general-purpose Llama-2 model \cite{llama2}, with further training on a corpus of 500B tokens that include both natural language and code. CodeLlama is available in several variants~\cite{codellamaHF}, each designed for specific use cases: a general-purpose coding model, an Instruct variant optimized for instruction tuning, and a Python-specialized version. The model sizes range from 7B to 70B parameters, and all versions are publicly accessible. CodeLlama has demonstrated strong performance in automating a range of code-related tasks \cite{zan2024codes,xia2023universal}, making it a representative model for our study. We used the general-purpose \emph{Instruct} version featuring 7B and 34B parameters in our experiments.


%\smallskip

%\subsubsection{DeepSeek-Coder}

\textit{DeepSeek-Coder}~\cite{deepseek} is a set of open-source LLMs ranging from 1B to 33B parameters. These models are offered in two configurations: Instruct, optimized for instruction tuning, and Base. Trained on a dataset of two trillion tokens, including code-specific data, DeepSeek-Coder has been shown capable of outperforming larger models such as GPT-3.5  \cite{brown2020language}, while the small-sized version featuring 6.7B parameters has proven highly competitive to CodeLlama's 33B variant. For this study, we used the \emph{Instruct} version of DeepSeek-Coder in three variants, 1.3B, 6.7B, and 33B parameters. This selection served two different goals: 
 %\os{regarding my previous comment, this seems to be used for RQ1, while bullet (ii) is for RQ2. Add references to the RQs. Same for the other two model families.} \SAIMA{I mentioned RQ1 and RQ2 where needed} 
 (i) it allowed us to compare the performance of QLoRA-optimized models against fully fine-tuned models by contrasting the results achieved by DeepSeek-Coder 1.3B in both configurations; and (ii) it enabled a comparison between small-sized (CodeLlama 7B \emph{vs.} DeepSeek-Coder 6.7B) and mid-sized models (\eg CodeLlama 34B \emph{vs}. DeepSeek-Coder 33B), providing insights into how QLoRA fine-tuning impacts performance across different model sizes.

%\smallskip

%\subsubsection{StarCoder-2 \cite{starcoder2}}

%StarCoder-2, introduced in February 2024, is the successor to the original StarCoder \cite{li2023starcoder} model. It is trained on a dataset that is four times larger than its predecessor, incorporating data from diverse sources such as Software Heritage, GitHub issues, pull requests, Jupyter and Kaggle notebooks, small math and coding datasets , and other natural language datasets. StarCoder-2 is available in different sizes, including 3B, 7B, and 15B parameters, trained on 2.3 to 4.3 trillion tokens, respectively. This model has demonstrated strong performance across various code-related tasks, including code completion, code editing, code reasoning, understanding, and execution.
%Notably, the 2-15B parameter versions have outperformed other models on benchmarks for code reasoning and have shown superior results in several low-resource programming languages. StarCoder-2 has also been successfully employed to automate a range of code-related tasks \cite{gunasekar2023textbooks, luo2023wizardcoder, li2024evocodebench, deng2024r2c2}.
%For the purposes of this study, we selected the 7B parameter version of StarCoder-2 to evaluate its performance when fine-tuned with QLoRA.

%\smallskip

%\subsubsection{Magicoder \cite{magicoder}}

%Magicoder is built using the OSS-Instruct approach, a novel method that enhances the instruction-following capabilities of LLMs. OSS-Instruct employs a powerful LLM to automatically generate new coding problems by sampling random code snippets from open source. Magicoder models have been trained on 75k synthetic instruction datasets generated through OSS-Instruct. Both Magicoder and its variant, MagicoderS, have been applied to CodeLlama and DeepSeekCoder models, leading to superior performance over the base models on several prominent benchmarks, including HumanEval \cite{chen2021evaluating} and MBPP \cite{MBPP} for text-to-code generation, as well as MultiPL-E \cite{multipledata} for multilingual code completion.
%In this study, we experiment with the Magicode-CL-7B model variants.
%which was developed by generating 75k synthetic data points and fine-tuning the CodeLlama-python-7B model. This model has demonstrated notable improvements over the base CodeLlama-python-7B model, achieving pass@1 performance close to ChatGPT on the HumanEval dataset (77.7 vs. 72.6) and surpassing it on the HumanEval+ benchmark (66.5 vs. 65.9) \cite{magicoder}.

%\smallskip
%
%\subsubsection{Mistral-7B \cite{mistral}}
%Mistral-7B has demonstrated a strong balance between high performance and efficiency in 
%%\textcolor{red}{ANTONIO: Saima, again we can't define the same thing 3M times xD -- Large Language Models: Apologies, my eyes missed it} 
%LLMs by supporting larger batch sizes and offering higher throughput. The model utilizes two attention mechanisms, grouped-query attention (GQA) \cite{ainslie2023gqa} and sliding window attention (SWA) \cite{child2019generating}, which significantly enhance inference speed and reduce memory requirements during decoding. Mistral-7B has been shown to outperform the leading 34B Llama-1 model in code generation tasks. 
%%This model has since been adapted for a variety of code-related applications across numerous studies [REF].
%Among the available versionsâ€”Mistral-7B \emph{Base} and Mistral-7B \emph{Instruct}â€”we opted to use 
%%\textcolor{red}{ANTONIO: are we sure it wasn't the instruct?: I think so} 
%the base version of Mistral-7B for our experiments.


\subsection{The QLoRA Fine-tuning Technique}
\label{sub:design_qlora}

%\os{too long of a paragraph, break into smaller ones}
%\ANTONIO{Saima: Add subsections for the different QLoRA phases}
QLoRA employs two innovative techniques for effective 4-bit finetuning: 4-bit NF4 quantization and Double Quantization, along with Paged Optimizers to manage memory efficiently during gradient checkpointing.

%\smallskip
%\smallskip


\subsubsection{NF4 Quantization}

The core of QLoRAâ€™s approach lies in a method designed to efficiently quantize neural network weights into a 4-bit format which, uses NF4, a novel data type designed for AI applications. The 4-bit Normal Float~(NF4) data type is based on Quantile Quantization \cite{dettmers20218}, which ensures an even distribution of tensor values across quantization bins or categories. Using fast quantile approximation algorithms, QLoRA can estimate quantiles without the high computational costs associated with precise quantile calculations.
\looseness=-1

During this process, the neural network weights, which generally follow a zero-centered normal distribution, are adjusted to fit a predefined range. This normalization aligns the weight tensors with the range of the data type, allowing for more effective quantization by matching the tensorâ€™s value distribution to that of the quantized format.

%\smallskip

\subsubsection{Double Quantization}
To  further reduce memory footprint, QLoRA follows a two-step approach: (i) the model weights are quantized to 4-bit precision using NF4, and (ii) the quantization constants (scales and zero-points) from the first step are quantized to a lower precision. 
QLoRA implements Blockwise k-bit Quantization, where weights are divided into distinct blocks that are independently quantized, rather than quantizing all weights collectively. This method generates multiple quantization constants, which can undergo a second round of quantization, providing additional memory savings.
\looseness=-1
%Double quantization further optimizes memory usage by quantizing the quantization constants used in the first quantization step. \textcolor{red}{ANTONIO: Please clarify here...This process, such as using 8-bit floats for second-level quantization, reduces memory overhead per parameter by leveraging block sizing and mean adjustment for symmetric quantization.}
%\smallskip

\subsubsection{Paged Optimizer}

When training large models, gradient checkpointing comes in handy as a technique to reduce memory usage during model training, yet memory spikes can still occur when processing mini-batch with a long sequence of input tokens.
Paged optimizers minimize GPU memory use by storing states in CPU memory and transferring them as needed.
\looseness=-1

\figref{fig:qlora} depicts the fine-tuning process of QLoRA, an extension of LoRA that, as noted, utilizes NF4 for efficient weight storage and BFloat16 for computations and gradient calculations. 
% By concentrating updates on the LoRA parameters (shown as adapter blocks in \figref{fig:qlora}), this approach enhances precision and reduces memory usage, allowing for efficient execution of complex quantized operations on large-scale models. 
The addition of paged optimizer memory management further enhances efficiency, making QLoRA particularly suitable for resource-constrained environments.

%\textcolor{Red}{Saima::  I think, here we need to discuss the config of our model setup}
%\textcolor{blue}{ANTONIO: you're right}.
In our study, we selected the QLoRA parameter configuration outlined in \tabref{tab:hp-tuning}, which includes three parameters: (i) \texttt{lora\_r}, (ii) \texttt{lora\_alpha}, and (iii) \texttt{lora\_dropout}. These were kept constant for each QLoRA fine-tuning instance.
The choice of the hyperparameter values follows established best practices to ensure precision while minimizing resource consumption \cite{dettmers2024qlora,hu2021lora}. 
%For instance, Dettmers \etal, in the original QLoRA paper \cite{dettmers2024qlora}, observed that the project dimension (\texttt{lora\_r}) has minimal impact on performance. In line with this, we adhere to standard practices \cite{dettmers2024qlora} by fixing the project dimension at 8. Regarding the scaling parameter (\texttt{lora\_alpha}), existing literature, including the original LoRA paper \cite{hu2021lora},
%\ANTONIO{SAIMA take care please}
%generally recommends fixing Alpha at 16, which we have consistently applied across all experiments. Lastly, we set the LoRA dropout probability to 0.1, as recommended by Dettmers \etal \cite{dettmers2024qlora}.

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.7\linewidth]{images/QLoRA.jpg}
	\caption{QLoRA finetuning with paged optimizers \cite{dettmers2024qlora}}
	\label{fig:qlora}
\end{figure}


%	These parameters were selected based on previous theoretical and empirical studies aimed at optimizing both computational efficiency and memory usage during model training . These parameters follow established best practices to ensure precision without significantly increasing resource consumption. 

%This setting involved training and evaluating \textcolor{red}{XXXX} QLoRA-optimized models. \os{where do the fixed values come from? or why do we use those?} \ANTONIO{@SAIMA: PLease address this one: DONE ABOVE}

\begin{table}[h!]
	\centering
	\caption{QLoRA hyperparameters used in our experiments}
	 \resizebox{.45\textwidth}{!}{
	\begin{tabular}{llcc}
		\hline
		& \bf  Parameter                   & \bf Description & \bf Value \\ 
		\hline
%		& \texttt{Quantization Type}       & \emph{data type for weight storage}   & NF4 \\ 
%		& \texttt{Computation Data Type}   & \emph{Data type used for computations}          & BF16 \\ 
%		& \texttt{Quantization Blocksize} & \emph{Blocksize used for weight quantization}         & 64 \\ 
%		& \texttt{DQ Blocksize} & \emph{Blocksize for second-level quantization}         & 256 \\ 
		& \texttt{lora\_r}       & \emph{lora attention dimension/ rank}   & 8 \\ 
		& \texttt{lora\_alpha}   & \emph{lora scaling parameter}           & 16 \\ 
		& \texttt{lora\_dropout} & \emph{lora dropout probability}        & 0.1 \\ 
		\hline
	\end{tabular}
	\label{tab:hp-tuning}
}
\end{table}



\smallskip

\subsection{Dataset and Model Training}
\label{sub:data}
%\subsubsection{Training, evaluating and testing}

We employed the \emph{Code-to-Text} dataset from the CodeXGLUE benchmark~\cite{codexglue, CodeXGLUEbench} to train and evaluate all QLoRA-optimized models, focusing specifically on Java and Python. The benchmark consists of pairs of code methods and their associated natural language descriptions, extracted from  $\sim$6 million instances of human-written code documentation.


Our decision to leverage CodeXGLUE was driven by its extensive use in prior research for studying LLMs in code-related tasks~\cite{wu2022learning,mastropaolo2022using,mastropaolo2021empirical,shi2022evaluation,chen2024code}. 

\tabref{tab:datasets} presents a summary of the datasets employed
for training and evaluation. To this extent, we train each QLoRA-optimized model using a fixed set of hyperparameters, as detailed in \tabref{tab:hp-tuning}. Each model was trained for \textit{10 epochs} with a consistent \textit{batch size of 32} maintained throughout all experiments. To prevent overfitting, we implemented an early stopping, saving a new checkpoint after every 5,000 training steps, and monitoring the performance of the models using the METEOR score, which acts as a highly reliable proxy for differences exceeding 2 points in evaluating the quality of code summaries as perceived by humans~\cite{roy2021reassessing}.
In particular, the training process stops if no improvements in the METEOR score are observed after 15K steps, which equals to a window of 3. This approach allowed us to effectively monitor model performance and ensured that we retained the best-performing checkpoint of the models for both programming languages.

\vspace{-5pt}
\begin{table}[h!]
	\centering
	\caption{\#Number of Data Instances in Training, Validation, and Testing Splits}
	\label{tab:datasets}
		\resizebox{0.8\columnwidth}{!}{
	\begin{tabular}{lrrr}
		\toprule
		\textbf{Language} & \textbf{Training} & \textbf{Validation} & \textbf{Testing} \\
		\midrule
		Java & 164,923 & 5,183 & 10,955 \\
		Python & 251,820 & 13,914 & 14,918 \\
		\bottomrule
	\end{tabular}
}
\end{table}

During training, each model takes as input tokens the tokenized code from the \texttt{code\_tokens} field in the JSON file, corresponding to either Java\footnote{\url{https://zenodo.org/record/7857872/files/java.zip}} or Python\footnote{\url{https://zenodo.org/record/7857872/files/python.zip}}. The output is the sequence of natural language tokens provided in the \texttt{docstring\_tokens} field and joined together to form a string, specific to each programming language. 
%\figref{fig:IO} illustrates an example of the input-output sequences given to the models, comprising Java code tokens as input and natural language tokens from the Javadoc comment as output. 
% \ANTONIO{Khai: We configured the sequence length to 300 tokens for the input and for the output, aligning with the requirements of the task, based on an analysis of the token distribution for code and natural language in the Code-to-Text dataset.}
We configured the maximum sequence length to 300 tokens during the training stage based on our analysis of the token distribution for code and natural language in the Code-to-Text dataset.



%\begin{figure}[h!]
%	\centering
%	\includegraphics[width=0.9\columnwidth]{images/input-output.jpg}
%	\caption{Example of Input-Output pair as provided to the models.}
%	\label{fig:IO}
%\end{figure}

% \ANTONIO{Khai, please check and add ref at the end if any: In our implementation of QLoRA for code summarization, we include all the linear modules of the network, such as Feed-Forward Layers, Self-Attention Layers, Projection Layers, and Embedding Layers. This comprehensive approach enables QLoRA to effectively adapt to the code summarization task by leveraging the entire parameter space considered essential for optimal performance \textcolor{red}{[]}.}
In our implementation of QLoRA, we followed the findings from \cite{dettmers2024qlora} and applied QLoRA to all linear layers of the networks (\eg Feed-Forward Layers, Self-Attention Layers, and Projection Layers). This approach enables QLoRA to effectively adapt to the task at hand by leveraging the parameter space considered essential for optimal performance.


The training process for full fine-tuning followed the same configuration as used for QLoRA-optimized models, including batch size, number of training epochs, and early stopping.
%\textcolor{red}{ANTONIO: Saima, please take core of the table down below: DONE}





\subsection{Metrics and Experimental Procedure}
\label{sub:analysis}

%We outline the study procedure for each research question related to the investigation below.

%\subsubsection{RQ$_1$} 
%\label{subsec:rq1}
We started by fine-tuning DeepSeek-Coder 1.3B using QLoRA with the dataset described in \secref{sub:data}. As outlined in \secref{sub:design_llm}, we limited full fine-tuning (FFT) to the smaller model variants included in our study to mitigate the substantial computational costs associated with adjusting all parameters of larger models (\eg DeepSeek-Coder 33B).


%We opted for a single end-to-end fine-tuning of the LLMc due to the substantial costs associated with performing this process for all models included in our investigation. We make a reasonable assumption: ``\emph{if model X optimized via QLoRA is the best-performing among the five we experimented with, then its original configuration (\ie without QLoRA applied) is likely to yield the best performance as well}.`` This approach allows us to minimize the training costs and time associated with developing five different LLMc models while effectively addressing RQ$_1$ by providing a comprehensive evaluation.

%With that in mind, we compare the performance of the best QLoRA-optimized model against the end-to-end fine-tuned base model, using the selected benchmarkâ€”CoderEval \cite{codereval} and MultiPL-E \cite{multipledata} for code generation and, the test set featured in the Text-to-Code portion of the CodeXGLUE benchmark. 

%For code generation, we validate the automatically generated code against task-specific unit tests. The code is then labelled as passed if it successfully passes all tests, or failed otherwise. We use the pass@k metric \cite{chen2022codet, liu2024your, yeo2024framework, codereval} to evaluate code generation performance where the model generates 10 samples, and pass@k is calculated for k values of 1, 5, and 10. 
%To be more specific, we generate $n \geq k$ samples per task (where $n = 10$ and $k \in \{1, 5, 10\}$), count the number of correct samples $c \leq n$ that pass unit tests, and calculate the $pass@k$:
%\[
%pass@k := 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}.
%\]
 
%\textcolor{red}{ANTONIO: the definition follows the mathematical formula. Although the concept can be related to what written, I would rather put the formula to be on the safe side.: DONE}
% 


%Additionally, we complement the pass@k metric with statistical tests to account for the distribution of correct and incorrect predictions across the code generation tasks. Specifically, we employ McNemarâ€™s test \cite{mcnemar1947note} for pairwise comparisons of binary outcomes between two treatments, and we calculate the Odds Ratio (OR) to assess the magnitude of the differences in performance.

We proceeded by evaluating the performance of the model when generating code summaries for Java and Python methods. To this end, we relied on metrics that have been widely used in prior code summarization research~\cite{zhang2022survey,zhang2020retrieval,leclair2020improved}: 
\looseness=-1
\begin{itemize}[itemindent=0.3cm,leftmargin=0cm,label=]
	\setlength\itemsep{0.1cm}
	
\item \textbf{BLEU} (BilinguaL Evaluation Understudy) \cite{bleu} measures the similarity between candidate (predicted) summaries and reference (oracle) summaries. This metric assesses the overlap of $n$-grams within the two summaries, ranging from 0 (completely dissimilar summaries) to 1 (identical summaries). We compute the BLEU score at the sentence level, fixing $n=4$.

\item \textbf{METEOR} (Metric for Evaluation of Translation with Explicit ORdering)~\cite{meteor} is computed as the harmonic mean of unigram precision and recall, with recall given a higher weight than precision. Unlike BLEU, METEOR utilizes stemming and synonym matching to align more closely with human judgments of sentence similarity. METEOR ranges from 0 to 1, with a value of 1 indicating two identical sentences.

\item \textbf{ROUGE} (Recall-Oriented Understudy for Gisting Evaluation)~\cite{lin2004rouge}  consists of a set of metrics for evaluating both automatic text summarization and machine translation methods. ROUGE metrics compare automatically generated summaries or translations against a set of reference summaries, typically authored by humans. In line with Roy \etal \cite{roy2021reassessing}, we computed ROUGE-N(1-4), ROUGE-L, and ROUGE-W. ROUGE-N measures the number of matching $n$-grams between the generated summary and the reference summary with results reported in terms of recall, precision, and F1-score.

\item \textbf{chrF} (character $n$-gram F-score)~\cite{popovic2015chrf} measures the similarity between generated and reference summaries at the character level (rather than at the token level, done by the above metrics), reporting  a F1-score value.

\item \textbf{BERTScore}~\cite{zhang2019bertscore} computes sentence similarity using the embedding of a BERT model \cite{devlin2018bert} trained on English textual data. We report the F1-score (BERTScore-F1).

\item \textbf{SIDE} (Summary Alignment to Code Semantics)~\cite{mastropaolo2024evaluating} offers an automated method for evaluating the alignment between a Java method and its corresponding summary. It produces a score within the range of [-1, 1], where values closer to 1 indicate a stronger alignment between the comment and the documented code component. Conversely, lower SIDE scores signify weaker alignment, highlighting discrepancies between the summary and the code.
\end{itemize}

%\subsubsection{RQ$_2$.} We start by identifying the best-performing QLoRA-optimized model from RQ$_1$. Next, we fine-tune LoRAâ€™s hyperparameters, concentrating on those listed in the \texttt{Tunable} section of \tabref{tab:hp-tuning}. In this phase, we explore different configurations during the fine-tuning process and evaluate the model against task-specific benchmarks. Performance is measured using the pass@k metric for code generation and the metrics previously outlined for code summarization used to answer RQ$_1$.


%\subsubsection{RQ$_3$.} We explore the impact of multi-task fine-tuning using QLoRA. Multi-task fine-tuning is a technique where a model is trained on multiple tasks simultaneously, allowing it to leverage shared knowledge across tasks, potentially improving overall performance. By fine-tuning a model on both code generation and code summarization tasks simultaneously, the model can improve its generalization abilities and transfer knowledge between these tasks, which can be viewed as complementary to each other.

%Hence, building on the results from RQ$_1$ and RQ$_2$, we select the best-performing QLoRA-optimized model and fine-tune it on a \ANTONIO{here: multi-task dataset} that includes both code generation and code summarization tasks \os{how is this done?}. We then evaluate this multi-task fine-tuned model on the already discussed benchmarks that include CoderEval and MultiPLE-E for code generation, while Text-to-Code \texttt{test} dataset for code summarization--as listed in \secref{subsec:rq1}. The performance are then compared to that of the QLoRA models fine-tuned solely on individual tasks. This comparison is carried out for both code generation and code summarization tasks. As with previous assessments, we use the pass@k metric to measure the modelâ€™s performance for code generation, associated with the five different evaluation metrics for code summarization. 

Next, we conducted a comprehensive end-to-end, task-specific fine-tuning of DeepSeek-Coder, updating 1.3 billion parameters. This process, backpropagates gradients through the entire model, enabling optimal adjustment of each parameter to enhance task-specific performance (\ie code summarization).
\looseness=-1

Finally, we assessed whether there are statistically significant differences in performance between fully fine-tuned models and QLoRA-optimized models. To this extent, we employed the Wilcoxon signed-rank test \cite{wilcoxon}, and measured the effect size using Cliffâ€™s Delta (d) \cite{Cliff:2005}. The effect sizes are categorized as follows: negligible if $|d| < 0.10$, small if 0.10 $\leq$ $|d| < 0.33$, medium if 0.33 $\leq$ $|d| < 0.474$, and large if $|d|$ $\geq$ 0.474. We used a 95\% significance level across all tests and, since we tested our hypotheses through multiple tests, we adjusted the \emph{p}-values using Holmâ€™s correction procedure~\cite{Holm1979a}.
The tests were computed for every metric included in our evaluation.
\looseness=-1

%\smallskip
%\smallskip

%\subsubsection{RQ$_2$} 
To investigate the impact of various sizes of models, we fine-tuned four model variants with QLoRA optimization: CodeLlama 7B/34B and DeepSeek-Coder 6.7B/33B. Next, we evaluated the performance of each model configuration in generating meaningful code descriptions for Python and Java methods. Additionally, we applied the Wilcoxon signed-rank test to analyze performance differences in evaluation metrics across models of varying sizes.

Each experiment was performed on a server running Ubuntu 22.04.5 LTS (GNU/Linux 5.15.0-125-generic x86\_64), equipped with two Nvidia L40S GPUs, each featuring 48GB of graphics memory.
%\os{if we have space, add details about the hardware we used to run the experiments}


\subsection{Qualitative Analysis}
\label{sec:qualitative}

Evaluating code summarization can present unique challenges, particularly in dealing with semantically equivalent summaries. For example, in \figref{fig:semantically-equivalent}, the two summaries indicated with $S1$ and $S2$, describe the same functionality of a code snippet but use different phrasing. This discrepancy poses a challenge because traditional evaluation metrics like BLEU or ROUGE rely heavily on exact word matches and may penalize the generated summary $S1$ for not being identical to the ground truth $S2$. In addition, Mastropaolo \etal \cite{mastropaolo2024evaluating} have recently demonstrated that word-overlap metrics like BLEU, and even embedding-based metrics such as BERTScore \cite{zhang2019bertscore}, can only capture one of the several dimensions pertaining to the evaluation of code summarizers.
Thus, to provide a more accurate assessment of the capabilities of QLoRA in fine-tuning models for bi-modal software engineering tasks, we manually analyzed two statistically significant, randomly selected samples: one consisting of 384 Java methods and the other of 384 Python methods, generated by the best-performing QLoRA-optimized model identified in our study.
%â€”which we expect to be CodeLlama-34B.


With this manual analysis, we aimed to better understand the nuances in code summarization tasks that automated metrics might miss, offering a comprehensive evaluation of how effectively QLoRA-optimized models can support bi-modal SE-related tasks across two programming languages.

For this analysis, two paper authors  independently reviewed the 768 incorrectly generated summaries, equally split between Python and Java (\ie 384 + 384). Any conflicts were resolved through open discussion between the reviewers, involving a third author when needed. The summaries were sampled from the set of incorrect predictions made by CodeLlama-34B, the best-performing model. Each prediction was classified as:


\begin{itemize} 
	\item  \emph{Semantically equivalent}: The ground truth and the modelâ€™s prediction use different wording but convey exactly the same information to the developer.
	
	\item \emph{Meaningful code description}: These cases represent instances where the model generated a code summary that not only conveyed the intended information but was of better quality than the ground truth.
	A few examples are reported in \figref{fig:qualitative-examples}.
	
	\item \emph{Partially equivalent}: The prediction includes only part of the information conveyed in the ground truth. While these predictions can still be useful for the developer, some code adjustments are needed to align the prediction with the original method.
	
	\item \emph{Incorrect}: The code summary predicted by the model is documenting something else and not the underlying code.

\end{itemize}

To assess inter-rater reliability among the evaluators, we calculated Krippendorffâ€™s $\alpha$ coefficient \cite{krippendorff2011computing}. For the analysis of Java elements, the $\alpha$ coefficient was 0.752, and for Python, it was 0.803. In both cases, the $\alpha$ that ranges between [-1;1] indicated a high level of agreement between the two evaluators. Following this, we report the percentage of instances in each category detailed above.



\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{images/equivalent.jpg}
	\caption{Semantically equivalent Java code summaries.} %\os{S1 typo: Get $\rightarrow$ Gets. Also, can we reduce the indentation of the method body? it is right-indented too much}
	\label{fig:semantically-equivalent}
\end{figure}

%\input{tables/metrics_res.tex}



