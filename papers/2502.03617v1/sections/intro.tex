% !TEX root = ../main.tex
\section{Introduction}
\label{sec:intro}

In recent years, deep learning (DL) generative models, particularly Large Language Models (LLMs) and  Code Language Models (CLMs), have transformed key software engineering (SE) activities, including bug fixing, code generation, and code documentation~\cite{charalambous2023new,mastropaolo2021studying,mastropaolo2024evaluating,tian2023chatgpt,zhang2020retrieval}.
These advances have significantly enhanced automation, driving productivity in SE workflows.

To fully realize their potential, CLMs often require fine-tuning to achieve high accuracy on specific tasks.
Prior research~\cite{weyssow2023exploring,liu2022few} has shown that fine-tuned CLMs outperform pre-trained CLMs that rely on in-context learning (ICL), particularly for downstream tasks such as code summarization~\cite{ahmed2024automatic,ahmed2022few,wang2022no}.
Fine-tuning allows for deeper calibration of model parameters, resulting in higher adaptability \& robustness for specific tasks.
\looseness=-1

However, fine-tuning large-scale language models—often comprising billions of parameters—demands significant computational resources and time \cite{hou2023large}. For instance, training the CodeLlama \cite{codellama} family of models reportedly required over 1.4 million GPU hours \cite{codellama2}, highlighting the substantial effort needed to achieve state-of-the-art performance.


In response to this challenge, researchers have explored sustainable methods to reduce the environmental and computational costs associated with training large-scale models while maintaining high performance \cite{wei2023towards, weyssow2023exploring, ayupov2022parameter, lu2023llama}. Techniques such as model compression and parameter-efficient fine-tuning (PEFT) \cite{ayupov2022parameter, lu2023llama, weyssow2023exploring, su2024distilled, shi2023towards} have emerged as promising solutions, enabling efficient training with significantly lower resource demands.

One recent advancement at the intersection of model compression and PEFT is QLoRA (Quantized Low-Rank Adaptation)~\cite{dettmers2024qlora}, a technique that combines model size reduction with efficient fine-tuning strategies. QLoRA has been shown to enable cost-effective fine-tuning of CLMs for tasks such as program repair and code generation/completion~\cite{weyssow2023exploring, yang2024multi}, which fall into the categories of Code-to-Code and NL-to-Code (natural language to code) tasks. These results suggest that QLoRA significantly reduces computational overhead while achieving high effectiveness compared to methods requiring full parameter calibration. Despite these promising results, the applicability of QLoRA to Code-to-NL tasks, such as code summarization, remains unknown. This paper addresses this gap by evaluating QLoRA's effectiveness for code summarization.
\looseness=-1

Code summarization, like other bi-modal code-related tasks (\eg code review and code generation), requires reasoning across code and natural language, with the aim to translate complex code logic into accurate, clear, and concise natural language explanations. Given that QLoRA has proven effective for code generation~\cite{weyssow2023exploring}, we \textit{hypothesize} that it is equally effective for code summarization. This hypothesis is grounded in the conceptual parallel between teaching a model to generate code and teaching it to summarize code, as both tasks involve an inverse relationship where input and output roles are reversed, with both tasks learning nuanced relationships between natural and programming languages.

To validate this hypothesis, we conducted a systematic evaluation of QLoRA using two state-of-the-art CLMs, CodeLlama~\cite{codellama} and DeepSeek-Coder \cite{deepseek}, designed to summarize code methods written in Python and Java from the CodexGLUE's code summarization dataset\footnote{\url{https://tinyurl.com/axbp8hua}}. We trained these models with QLoRA under varying parameter sizes and compared their performance to full model fine-tuning, analyzing memory usage and predictive accuracy. Additionally, we qualitatively analyzed two statistically significant samples of code methods—one comprising Python methods and the other Java methods—to evaluate how closely the generated summaries align with the ground truth and how effectively they convey equivalent information. This analysis establishes a virtual upper bound on the potential effectiveness of QLoRA for Code-to-NL tasks, particularly code summarization.

%Finally, we qualitatively examined a sample of cases, commenting on the ability of the QLoRA-optimized model to generate outputs that are both accurate and valuable in real-world scenarios. 

Our results show that QLoRA achieves superior predictive performance compared to full fine-tuning while consistently reducing the memory footprint of CLMs. These findings provide compelling evidence of QLoRA's ability to optimize CLMs for resource-intensive, bi-modal code-related tasks, thereby showing its utility across the full spectrum of code-related tasks: Code-to-Code, NL-to-Code, and  Code-To-NL.

To the best of our knowledge, this work represents the first large-scale evaluation of QLoRA for code summarization, and it makes the following key contributions:

\begin{itemize}
	\item A comprehensive analysis of QLoRA’s capabilities for code summarization, using two state-of-the-art CLMs across two programming languages, contributing to a broader understanding of resource-efficient training across the full spectrum of code-related tasks.

	\item Key insights into the trade-offs between memory usage and model performance compared to full model fine-tuning, showcasing QLoRA's ability to achieve remarkable results with substantially reduced resource requirements in the context of code summarization.
	
	\item A replication package~\cite{replication}, including data, models, scripts, and documentation, to facilitate reproducibility and further research in this field.
\end{itemize}

