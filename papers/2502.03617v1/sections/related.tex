% !TEX root = ../main.tex
\section{Background and Related Work}
\label{sec:related}


This section provides the reader with an overview of recent advancements in efficiency-based methods that aim to improve the sustainability of large language models, particularly code language models (CLMs) for code summarization.

\subsection{Code Language Models in Code Summarization} 
\label{sec:CLM}



Given the significant potential of SE-related automation through large code models grounded on LLMs, researchers increasingly leveraged these models to support various tasks, including those requiring higher levels of abstraction. One such task is code summarization, which involves working with bi-modal data to translate and summarize code into natural language. In this task, LLMs have proven highly effective~\cite{sun2024extractive,shi2022evaluation,fang2024esale,ahmad2020transformer,mastropaolo2022using}. CLMs like Codex \cite{ahmed2023improving,arakelyan2023exploring}, CodeBERT \cite{chen2022transferability, gu2022assemble}, and T5 \cite{mastropaolo2022using} excel in understanding code functionality and logic, generating clear and concise summaries. For example, Mastropaolo \etal~\cite{mastropaolo2022using} pre-trained a T5-based model on a blend of code and technical natural language before fine-tuning it on various code-related tasks, including code summarization. Their results highlighted the advantages of leveraging transfer learning for bi-modal code-related tasks, particularly code summarization. Haldar \etal~\cite{haldar2024analyzing} investigated the use of CodeT5 \cite{wang2021codet5}, PaLM2 \cite{anil2023palm}, and Llama2 \cite{touvron2023llama} to generate meaningful code summaries. While CodeT5 was subject to fine-tuning, PaLM2 and Llama2 required no parameter adjustment. The authors' findings reveal that LLMs frequently leverage function names and shared tokens between the code and its summary to optimize predictive performance.

Ahmed \etal \cite{ahmed2022few} found that few-shot prompting, which involves providing the model with few examples for generation tasks, significantly improves Codex's performance in code summarization, outperforming smaller pre-trained models like CodeT5. In another study,  Sun \etal \cite{sun2023automatic} explored CodeLlama~\cite{codellama2} and GPT-4 \cite{achiam2023gpt} for code summarization and evaluated five prompting techniques (\ie zero-shot, few-shot, chain-of-thought, critique, and expert). They identified the most effective prompt for guiding  GPT-4  to generate in-distribution code summaries.



\subsection{Parameter-Efficient Fine-Tuning and Quantization Methods}
\label{sec:peft-qt}
%\os{PEFT is kind of widely studied for code summarization, and other tasks}
\textit{Parameter-Efficient Fine-Tuning} (PEFT) optimizes fine-tuning by updating only a subset of a model's parameters, rather than the entire model. Common techniques include: (i) Adapters, where additional model layers are introduced to handle a limited set of parameters \cite{houlsby2019parameter}; (ii) Prompt Tuning, which trains the model to learn from prompts containing task descriptions or canonical examples \cite{lester2021power, li2021prefix}; and (iii)
LoRA (Low-Rank Adaptation), which  decomposes weight gradients into low-rank matrices during fine-tuning~\cite{hu2021lora}. 
\looseness=-1
 
 PEFT has shown strong performance in tasks such as code generation and summarization, often outperforming fully fine-tuned models. For instance, Wang \etal~\cite{wang2023one} applied Adapter tuning for code search and summarization, while Ayupov~\etal~\cite{ayupov2022parameter} showcased the effectiveness of Adapters and LoRA  in tasks like code summarization and code clone detection. Similarly, Liu \etal~\cite{liu2023empirical} compared PEFT methods—such as Adapter, LoRA, prefix tuning, and Multi-Head Modification~(MHM)—for tasks like defect detection, clone detection, code translation, and code summarization. Recent studies~\cite{sun2023prompt, shi2023towards} have further explored PEFT techniques in the context of code summarization, highlighting their importance in this domain.
 \looseness=-1


\textit{Quantization} is a technique for model compression. It aims to reduce the size of a model by preserving only the most essential information encoded in the model's parameters. Specifically, it achieves compression by representing weights or activations in lower-precision formats, such as 8-bit integers, rather than higher-precision formats like 16-bit or 32-bit floats~\cite{gholami2022survey, zhu2023survey}. This approach reduces latency while minimizing any potential loss in accuracy.

\setlength{\parskip}{1pt}

%There are two main strategies to achieve quantization:
%\os{these developments come from the AI/ML community right?}
%Quantization-Aware Training (QAT) such as EdgeQAT \cite{shen2024edgeqat}, LLM-QAT \cite{liu2023llm} which integrates quantization during model training, and Post-Training Quantization (PTQ) such as GPTQ \cite{frantar2022gptq} and AWQ \cite{lin2024awq} that convert models' weights without (re)-training of the LLM.
%SpQR \cite{dettmers2023spqr}, }, OWQ \cite{lee2024owq}, SmoothQuant \cite{xiao2023smoothquant}, and LLM.int8() \cite{dettmers2022gpt3}. PTQ techniques convert models' weights without (re)-training of the LLM.

%\textcolor{red}{Optional-->}There are two main strategies: Quantization-Aware Training (QAT) such as EdgeQAT \cite{shen2024edgeqat}, LLM-QAT \cite{liu2023llm} which integrates quantization during model training, and Post-Training Quantization (PTQ) such as GPTQ \cite{frantar2022gptq}, SpQR \cite{dettmers2023spqr}, AWQ \cite{lin2024awq}, OWQ \cite{lee2024owq}, SmoothQuant \cite{xiao2023smoothquant}, and LLM.int8() \cite{dettmers2022gpt3} which converts pre-trained models without retraining. \textcolor{red}{<--Optional}

In the software engineering domain, the pioneering study by Wei \etal~\cite{wei2023towards} represents the first large-scale investigation into the application of quantization techniques for code-related tasks, including code generation and summarization. The authors examined the effects of 8-bit quantization on various code models, such as PLBART \cite{ahmad2021unified}, CodeT5 \cite{wang2021codet5}, InCoder \cite{fried2022incoder}, and CodeGen \cite{nijkamp2022codegen}. Their findings revealed that applying 8-bit quantization to CodeGen and InCoder resulted in improved energy efficiency during code generation, while PLBART and CodeT5 showed similar benefits for code summarization. Notably, these gains in efficiency were achieved with only a minimal reduction in model accuracy.


\subsection{Quantized Low-Rank Adaptation (QLoRA) of CLMs}
\label{sec:qlora}

Dettmers \etal~\cite{dettmers2024qlora} recently proposed QLoRA, an approach that combines the LoRA PEFT technique with quantization of LLMs. QLoRA introduces various key innovations, including (i) the 4-bit NormalFloat (NF4) data type, (ii) Double Quantization (DQ), and (iii) a Paged Optimizer. It has been shown to be an efficient fine-tuning method that reduces memory usage while preserving the high performance of LLMs~\cite{dettmers2024qlora}. QLoRA quantizes the pre-trained model’s weights to 4-bit precision using NF4, a data type optimized for the normal distribution of neural network weights. Additionally, through double quantization, both the model weights and the quantization constants are quantized, further reducing the memory footprint. To manage memory spikes during gradient checkpointing and prevent out-of-memory errors, QLoRA employs Paged Optimizers. A detailed explanation of QLoRA and the fine-tuning process to achieve its goals is provided in \secref{sub:design_qlora}.

Limited research has investigated the efficiency of QLoRA for code language models. Yang \etal~\cite{yang2024multi} applied QLoRA on models such as CodeLlama \cite{codellama2}, StarChat-alpha \cite{Tunstall2023starchat-alpha}, and Mistral-Instruct-7B \cite{mistral} to specialize large code models for automatic program repair (APR). Their findings demonstrate that QLoRA effectively supports LLMs in repairing defects in software systems. Weyssow \etal~\cite{weyssow2023exploring} compared PEFT techniques to In-Context Learning (ICL) for code generation, concluding that PEFT methods achieved superior results. In addition, the authors also investigated the applicability of QLoRA to CodeLlama 7B, 13B, and 34B Python models, using 8-bit and 4-bit quantization. 
%CodeLlama 34B fine-tuned with QLoRA-4bit showed a 12.2\% improvement in EM@10 over CodeLlama 7B with LoRA. QLoRA-4bit also outperformed QLoRA-8bit for CodeLlama 7B. %\os{this one is also very related, it seems}

While these findings provide valuable insights, a comprehensive evaluation of whether QLoRA can effectively support the entire spectrum of code-related tasks--namely NL-to-Code, Code-to-Code, and Code-to-NL--remains absent. To address this gap, this paper takes a significant first step toward exploring QLoRA’s potential across these task categories. Specifically, we focus on Code-to-NL tasks, using code summarization as a representative case study, to evaluate how well QLoRA adapts in scenarios where the model processes code as input and generates natural language as output. 
This work seeks to deepen the understanding of resource-efficient training methods in software engineering tasks while providing a foundation for future research across diverse bi-modal tasks (\eg code review automation).



