\section{Introduction}\seclab{intro}
Data compression algorithms exploit natural redundancy in data to compress files before storage or transmission. Lossless compression schemes have been widely used in various contexts such as ZIP archives (Deflate \cite{rfc1951}) and web content compression by Google (Brotli \cite{rfc7932}). In the Compress-Then-Encrypt framework a message $w$ is first compressed to obtain a (typically shorter) file $y=\compress(w)$ and then the compressed file $y$ is encrypted to obtain a ciphertext $c=\Enc_K(y)$. Intuitively, compression is used to reduce bandwidth and encryption is used to protect the file contents from eavesdropping attacks before it is transmitted over the internet.  

While encryption protects the {\em content} of the underlying plaintext message $y$, there is no guarantee that the ciphertext $c$ will hide the \emph{length} of the underlying plaintext $y$. For example, AES-GCM \cite{rfc5282} is the most widely used symmetric key encryption algorithm and an AES-GCM ciphertext leaks the {\em exact} length of the underlying plaintext\footnote{Any efficient encryption scheme must at least leak some information about the length of the underlying plaintext. Otherwise, the ciphertext of a short message (e.g., ``hi'') would need to be longer than the longest possible message supported by the protocol (e.g., a 4GB movie).}. Unfortunately, the length of the compressed message $y$ can leak information about the {\em content} of the original uncompressed file. For example, suppose that the original message was $w=$``\texttt{The top secret password is:~password.}'' An eavesdropping attacker who intercepts the AES-GCM ciphertext $c$ would be able to infer the length $\left|\compress(w)\right|$ and could use this information to eliminate many incorrect password guesses, i.e.,  if $\left| \compress(w')\right| \neq \left|\compress(w)\right|$ where $w'=$``\texttt{The top secret password is:~[guess]}'' then the attacker could immediately infer that this particular password guess is incorrect. 

The observation that the length of a compressed file can leak sensitive information about the {\em content} has led to several real-world exploits. For example, the CRIME (Compression Ratio Info-leak Made Easy) attack \cite{RizDuo12} exploits compression length leakage to hijack TLS sessions \cite{Fis12}. Similarly, the BREACH (Browser Reconnaissance and Exfiltration via Adaptive Compression of Hypertext) attack \cite{GluHarPra13} against the HTTPS protocol exploits compression in the underlying HTTP protocol. Despite its clear security issues, the  ``Compress-then-Encrypt'' framework continues to be used as a tool to reduce bandwidth overhead. There have been several heuristic proposals to address this information leakage. One proposal \cite{paulsen2019debreach} is static analysis to identify/exclude sensitive information before data compression. However, it is not always clear {\em a priori} what information should be considered sensitive. Heal the Breach \cite{palacios2022htb} pads compressed files by a random amount to mitigate leakage and protect against the CRIME/BREACH attacks. While this defense is intuitive, there will still be some information leakage and there are no rigorous privacy guarantees. This leads to the following question: 

\begin{quote}
    \emph{Can one design a compression scheme which provides rigorous privacy guarantees against an attacker who learns the length of the compressed file?}
\end{quote}

Differential Privacy (DP) \cite{TCC:DMNS06} has emerged as a gold standard in privacy-preserving data analysis due to its rigorous mathematical guarantees and strong composition results. Given $\epsilon>0$ and $\delta\in(0,1)$, a randomized algorithm $\A$ is called \emph{$(\epsilon,\delta)$-differentially private} if for every pair of neighboring datasets $\mathfrak{D}$ and $\mathfrak{D}'$ and for all sets $S$ of possible outputs, we have $\Pr[\A(\mathfrak{D})\in S]\leq e^\epsilon\cdot\Pr[\A(\mathfrak{D}')\in S]+\delta$. Typically, $\epsilon$ is a small constant and the additive loss $\delta$ should be negligible (if $\delta=0$ then we can simply say that $\A$ satisfies \emph{$\epsilon$-DP}). In our context, we are focused on leakage from the length of a compressed file. We say that an compression algorithm $\compress$ is  \emph{$(\epsilon,\delta)$-differentially private} if for every pair of neighboring strings $w$ and $w'$ (e.g., differing in one symbol) and all $S \subseteq \mathbb{N}$ we have  $\Pr[\left| \compress(w)\right|\in S]\leq e^\epsilon\cdot\Pr[\left| \compress(w')\right|\in S]+\delta$. Intuitively, suppose that $w$ denotes the original message and $w'$ denotes the message after replacing a sensitive character by a special blank symbol. Differential privacy ensures that an attacker who observes the length of the compressed file will still be unlikely to distinguish between $w$ and $w'$. DP composition ensures that the attacker will also be unlikely to distinguish between the original string $w$ and a string $w''$ where {\em every} symbol of a secret password has been replaced --- provided that the secret password is not too long.  

We first note that the Heal the Breach proposal \cite{palacios2022htb} does not necessarily satisfy differential privacy. For example, Lagarde and Perifel \cite{lagarde2018lempel} proved that the LZ78 compression algorithm \cite{LZ78} is highly sensitive to small bit changes. In particular, there exists nearby strings $w \sim w'$ (i.e., $|w|=|w'|$ and $|\{i:w[i]\neq w'[i]\}|=1$) of length $n$ such that $\left|\compress_{\sf LZ78}(w) \right| = o(n)$ and $\left|\compress_{\sf LZ78}(w') \right| = \Omega(n)$. In particular, to prevent an eavesdropping attacker from distinguishing between $w$ and $w'$, the length of the padding would need to be at least $\Omega(n)$. Heal the Breach \cite{palacios2022htb} adds a much smaller amount of padding $o(n)$.

\paragraph*{Notations.}
For a positive integer $n$, we define $[n]\coloneqq\{1,\ldots,n\}$. Unless otherwise noted, we will use $n$ as the length of a string throughout the paper. We use $\Sigma$ to denote the alphabet of a string and $w \in \Sigma^n$ to denote a string of length $n$ and we let $w[i] \in \Sigma$ denote the $i$th character of the string. Given two strings $w,w' \in \Sigma^n$, we use $\Ham(w,w') = \left| \{ i:~w[i] \neq w'[i]\} \right|$ to denote the \emph{hamming distance} between $w$ and $w'$, i.e., the number of indices where the strings do not match. We say that two strings $w$ and $w'$ are \emph{adjacent} if $\Ham(w,w')=1$ and we use $w \sim w'$ to denote this. We use $|w|$ to denote the length of a string, i.e., for any string $w \in \Sigma^n$ we have $|w| = n$. We use $w \circ w'$ to denote the concatenation of two strings $w$ and $w'$. Given a character $c \in \Sigma$ and an integer $k \geq 1$, we use $c^k$ to denote the character $c$ repeated $k$ times. Formally, we define $c^1 \coloneqq c$ and $c^{i+1} \coloneqq c \circ c^i$. Unless otherwise noted, we assume that all log's have base $2$, i.e., $\log x \coloneqq \log_2 x$.

\subsection{Our Contributions}\seclab{contributions}
In this paper, we initiate the study of differentially private compression schemes focusing especially on the  LZ77 compression algorithm \cite{LZ77}. We first provide a general transformation showing how to make any compression scheme differentially private by adding a random amount of padding which depends on the global sensitivity of the compression scheme. The transformation will yield an efficient compression scheme as long as the underlying compression scheme has low global sensitivity. Second, we demonstrate that good compression schemes do not inherently have high global sensitivity by demonstrating that Kolmogorov compression has low global sensitivity. Third, we analyze the global sensitivity of the LZ77 compression scheme providing an upper/lower bound that is tight up to the sublogarithmic factor of $\log^{2/3} n$. 

\paragraph*{Differentially Private Transform for Compression Algorithms.}

 We provide a general framework to transform any compression scheme $\compress$ into a differentially private compression algorithm $\dpcompress(w,\epsilon,\delta)$ by adding a random positive amount of padding to $\compress(w)$, where the amount of padding $p$ depends on the privacy parameters $\epsilon$ and $\delta$ as well as the global sensitivity of the underlying compression algorithm $\compress$. More concretely, we show that $A_{\epsilon,\delta}(w)\coloneqq\dpcompress(w,\epsilon,\delta)$ is $(\epsilon,\delta)$-differentially private, i.e., the length leakage is $(\epsilon,\delta)$-differentially private.  The expected amount of padding added is $\O{\frac{\GS_\compress}{\epsilon}\ln(\frac{1}{2\delta})}$ (see \defref{def:local_sensitivity} for the definition of the global sensitivity $\GS_\compress$). Thus, as long as  $\GS_\compress \ll n$, it is possible to achieve a compression ratio $\left| \dpcompress(w,\epsilon,\delta) \right|/ n = \left| \compress(w) \right|/ n + o(1)$ that nearly matches $\compress$. See \secref{sec:dpcompress} for details.
 

\paragraph*{Idealized Compression Schemes have Low Sensitivity.}

While the LZ78 \cite{LZ78} compression algorithm has high global sensitivity, we observe that compression schemes achieving optimal compression ratios do not inherently have high global sensitivity. In particular, we argue that Kolmogorov compression has low global sensitivity, i.e., at most $\O{\log n}$ where $n$ denotes the string length parameter. While Kolmogorov compression is uncomputable, it is known to achieve a compression rate that is {\em at least as good} as any compression algorithm. We also construct a \emph{computable} variant of Kolmogorov compression that preserves the global sensitivity, i.e., the global sensitivity of the computable variant of Kolmogorov compression is also $\O{\log n}$.
See \secref{sec:kolmogorov} for details.

\paragraph*{Global Sensitivity of the LZ77 Compression Scheme.}

Our primary technical contribution is to analyze the global sensitivity of the  LZ77 compression algorithm \cite{LZ77}. The LZ77 algorithm includes a tunable parameter $W \leq n$ for the size of the sliding window. Selecting smaller $W$ reduces the algorithm's space footprint, but can result in worse compression rates because the algorithm can only exploit redundancy within this window. We provide an almost tight upper and lower bound of the global sensitivity of the LZ77 compression scheme. In particular, we prove that the global sensitivity of the LZ77 compression scheme for strings of length $n$ is upper bounded by $\mathcal{O}(W^{2/3}\log n)$ where $W \leq n$ is the length of the sliding window. When $W=n$ the global sensitivity is lower bounded by $\Omega(n^{2/3}\log^{1/3}n)$ matching our upper bound up to a sublogarithmic factor of $\log^{2/3}n$. We hope that our initial paper inspires follow-up research analyzing the global sensitivity of other compression schemes. 

\begin{theorem}[informal]
Let $\compress$ be the LZ77 compression scheme for strings of length $n$ with a sliding window size $W \leq n$. Then $\GS_\compress = \mathcal{O}(W^{2/3}\log n)$ and, when $W=n$, $\GS_\compress = \Omega(n^{2/3}\log^{1/3}n)$. (See \thmref{thm:thmupperbound} and \thmref{thm:lowerbound}.)
\end{theorem}

At a high level, the upper bound analysis considers the relationship between the blocks generated by running the LZ77  compression for two neighboring strings $w \sim w'$. Let $B_1,\ldots, B_t$ (resp. $B_1',\ldots, B_{t'}'$) denote the blocks generated when we run LZ77 with input $w$ (resp. $w'$) where block $B_i$ (resp. $B'_k$) encodes the substring $w[s_i, f_i]$ (resp. $w'[s_k',f_k'])$. We prove that if $s_i \leq s_k' \leq f_i$ (block $B_k'$ starts inside block $B_i$) then $f_{k+1}' \geq f_i$. In particular, this means that for every block $B_i$ there are \emph{at most two} blocks from $B_1',\ldots, B_{t'}'$ that ``start inside'' $B_i$. We can then argue that the difference in compression lengths is proportional to $t_2$ where $t_2 = \left| \left\{ i \leq t: \left|\{ k\leq t': s_i \leq s_k' \leq f_i  \}\right| =2\right\} \right|$ denotes the number of ``type-2'' blocks, i.e., $B_i$ which have two blocks that start inside it. Finally, we can upper bound $t_2 = \O{n^{2/3}}$ when $W=n$ or $t_2 = \O{W^{2/3}}$ when $W < n$. 

The lower bound works by constructing a string which (nearly) maximizes $t_2$. See \secref{sec:upperbound} and \secref{sec:lowerbound} for details.

\subsection{Related Work} 

There has been a wide body of work on developing efficient compression algorithms. Huffman coding \cite{Huffman52} encodes messages symbol by symbol --- symbols that are used most frequently are encoded by shorter binary strings in the prefix-free code. Lempel and Ziv \cite{LZ77,LZ78} developed multiple compression algorithms and Welch \cite{Welch84} published the LZW algorithm as an improved implementation of \cite{LZ78}. In our work, we are primarily focused on analyzing the LZ77 compression algorithm \cite{LZ77} since it is one of the most common lossless compression algorithms used in practice. Deflate \cite{rfc1951} is a lossless compression scheme that combines LZ77 compression \cite{LZ77} and Huffman coding \cite{Huffman52} and is a key algorithm used in ZIP archives. The Lempel–Ziv–Markov chain algorithm (LZMA) is used in the 7z format of the 7-Zip archiver and it uses a modification of the LZ77 compression. Brotli \cite{rfc7932} also uses the LZ77 compression with Huffman coding and the 2nd-order context modeling. 

Several prior papers have studied the sensitivity of compression schemes \cite{lagarde2018lempel,AFI23,GILRSU23} to small changes in the input. While Lagarde and Perifel \cite{lagarde2018lempel} focused on the multiplicative sensitivity of the LZ78 compression algorithm \cite{LZ78}, their result implies that the additive sensitivity (i.e., global sensitivity) can be as large as $\Omega(n)$. Giuliani et al. \cite{GILRSU23} studied the additive sensitivity of the Burrows-Wheeler Transform with Run-Length Encoding proving that the additive sensitivity can be as large as $\Omega(\sqrt{n})$ --- upper bounding the additive sensitivity remains an open question. 

Most closely related to ours is the work of Akagi et al. \cite{AFI23} who studied the additive and multiplicative sensitivity of several compression schemes including Kolmogorov and LZ77. Akagi et al. \cite{AFI23} proved that Kolmogorov compression has additive sensitivity $\O{\log n}$. We extend this result to a computable variation of Kolmogorov compression in \secref{sec:kolmogorov}. For LZ77, they proved that the additive sensitivity is lower bounded by $\Omega(\sqrt{n})$ when the window size is $W=\Omega(n)$. We prove that the additive sensitivity is lower bounded by $\widetilde\Omega(n^{2/3})$. Finally, they prove that the (local) additive sensitivity of LZ77 is {\em at most} $\O{z}$ where $z$ is the length of the compressed file. Unfortunately, this result does not even imply that the global sensitivity is $o(n)$ because $z$ can be as large as $z= \Omega(n)$ when the file is incompressible. By contrast, we prove that the global sensitivity is upper bounded by $\O{W^{2/3} \log n}$ which is tight up to logarithmic factors and is at most $\O{n^{2/3} \log n}$ even when $W = \Omega(n)$.


Degabriele \cite{CCS:Degabriele21} introduced a formal model for length-leakage security of compressed messages with random padding. While Degabriele \cite{CCS:Degabriele21} did not use differential privacy as a security notion, his analysis suggests that DP friendly distributions such as the  Laplace distribution and Gaussian distribution minimized the leakage. Song \cite{song2024refined} analyzed the (in)security of compression schemes against attacks such as cookie-recovery attacks and used the additive sensitivity of a compression scheme to upper bound the probability of successful attacks. Neither work \cite{CCS:Degabriele21,song2024refined} formalized the notion of a differentially private compression scheme as we do in \secref{sec:dpcompress}.  

%investigated length-hiding on compressed messages via random padding using Laplace and Gaussian distributions. Although this work's security model does not appear to be differential privacy, its suggested padding distributions could potentially achieve DP. We believe it is still worth formalizing the DP compress problem (see \secref{sec:dpcompress} for details).

Ratliff and Vadhan introduced a framework for differential privacy against timing attacks \cite{ratliff2024framework} to deal with information leakage that could occur when the running time of a (randomized) algorithm might depend on the sensitive input. They propose to introduce random positive delays after an algorithm is finished. The length of this delay will depend on the sensitivity of the algorithm's running time to small changes in the input. While our motivation is different, there are similarities: they analyze the sensitivity of an algorithm's running time while we analyze the sensitivity of a compression algorithm with respect to the length of the file. They introduce a random positive delay while proposing to add a random positive amount of padding to the compressed file. 



%Akagi et al. \cite{AFI23} studied the additive and multiplicative sensitivity of compression schemes using an edit-distance framework. Their definition of additive sensitivity is similar to global sensitivity, but they consider any single edit, i.e., insertion, deletion, or substitution. The multiplicative sensitivity is the maximum ratio of compressed lengths between strings with edit distance $1$. While they analyzed the additive sensitivity of Kolmogorov compression like our approach, they did not explore the computable variant of Kolmogorov compression. They also showed the lower bound of $\Omega(\sqrt{n})$ for the additive sensitivity of LZ77, and an upper bound on the local sensitivity of LZ77. We remark that our upper/lower bounds for the global sensitivity of LZ77 are tighter (See \secref{sec:upperbound} and \secref{sec:lowerbound} for details).



\section{Preliminaries}\seclab{sec:prelim}

\begin{definition}[Lossless Compression]
Let $\Sigma,\Sigma'$ be the sets of alphabets. A {\em lossless compression scheme} consists of two functions $\compress:\Sigma^* \rightarrow (\Sigma')^*$ and $\mathtt{Decompress}:(\Sigma')^* \rightarrow \Sigma^*$. We require that for all string $w \in \Sigma^*$ we have $\mathtt{Decompress}\left(\compress(w) \right) = w$.
\end{definition}

\paragraph*{Global Sensitivity of Compression Scheme.}

Global sensitivity helps us understand how much the output of a function can change when its input is slightly modified. In the context of compression schemes, the global sensitivity of a compression scheme is defined as the largest change in the compressed size we could see from two words with Hamming distance $1$.

\begin{definition}[Local/Global Sensitivity]
\deflab{def:local_sensitivity}
Let $\Sigma,\Sigma'$ be the alphabets of strings. 
The \emph{local sensitivity} of the compression algorithm $\compress:\Sigma^* \rightarrow (\Sigma')^*$ at $w \in \Sigma^n$ is defined as 
$\mathtt{LS}_{\compress}(w) \coloneqq \max_{w' \in \Sigma^n:w \sim w'} \left| |\compress(w)| - |\compress(w')| \right|$,
and the \emph{global sensitivity} of the compression algorithm for strings of length $n$ is defined as
$ \GS_{\compress}(n) \coloneqq \max_{w \in \Sigma^n} \mathtt{LS}_{\compress}(w)$. 
If it is clear from context, then one can omit the parameter $n$ (length of a string) and simply write $\GS_{\compress}$ to denote the global sensitivity.
\end{definition}

Understanding the global sensitivity of a compression algorithm is a crucial element of designing a differentially private compression scheme. While there is a large line of work using global sensitivity  \cite{sheng2025differentiallyprivatedistancequery,farias2025differentiallyprivatemultiobjectiveselection,wicker2024certificationdifferentiallyprivateprediction,10735286,10597947,zhang2023sensitivityestimationdifferentiallyprivate,tetek:LIPIcs.APPROX/RANDOM.2024.73,10.1145/3523227.3546781,9762326,blocki_et_al:LIPIcs.APPROX/RANDOM.2023.59,blocki_et_al:LIPIcs.ICALP.2022.26} to design differentially private mechanisms, to the best of our knowledge, no prior work has studied the construction of differentially private compression schemes.


In our context the amount of random padding added to a compressed file will scale with the global sensitivity of the compression algorithm. While Lagarde and Perifel \cite{lagarde2018lempel} were not motivated by privacy or security, their analysis of LZ78 implies that the global sensitivity of this compression algorithm is $\Omega(n)$. Thus, to achieve differential privacy the amount of random padding would need to be very high, i.e., at least $\Omega(n)$. This would immediately negate any efficiency gains since, after padding, the compressed file would not be shorter than the original file!

% \paragraph*{Is High Sensitivity Inevitable?} 

% Based on the results of Lagarde and Perifel \cite{lagarde2018lempel} one might wonder whether or not any effective compression mechanism will necessarily have high global sensitivity. We argue that this is not necessarily the case by considering \emph{Kolmogorov compression}. The Kolmogorov compression of an input string $w$ is simply the encoding of the minimum-size pair $M$ such that the Turing Machine $M$ will eventually output $w$ when run with an initially empty input tape. It is also easy to see that the Kolmogorov compression scheme has low global sensitivity $\O{\log n}$ for strings of length $n$: let $\kc:\Sigma^*\rightarrow\mathfrak{M}$ be the Kolmogorov compression where $\mathfrak{M}$ is the set of all Turing machines. For a string $w\in\Sigma^n$, suppose that $\kc(w)=M\in\mathfrak{M}$. Then for a string $w'\sim w$ that differs on the $i\th$ bit, one can obtain $M'$ which outputs $w'$ by (1) running $M$ to obtain $w$, and (2) flipping the $i\th$ bit of $w$ to obtain $w'$. Then the description of $M'$ needs the description of $M$ and $i$ plus some constant. Since it takes $\log n$ bits to encode $i$, It follows that $\kc(w') \leq \left| M'\right| \leq |M| + \O{\log n} = \kc(w) + \O{\log n}$. In particular, the global sensitivity of Kolmogorov compression is upper bounded $\O{\log n}$. While  Kolmogorov compression is not computable, when it comes to efficient compression ratios, Kolmogorov compression is at least as effective as any other compression scheme, i.e., for any compression scheme $\compress$ there is a universal constant $C$ such that $\left|(M,z)\right| \leq C + \left|\compress(w)\right|$ for all strong $w \in \Sigma^*$ \footnote{ The Turing Machine $M$ can implement any decompression algorithm we can hardcode $z=\compress(w)$ to obtain a new machine $M_z$ which simulates $M$ on the input $z$ to recover $w$. }. Thus, the goal of designing a compression algorithm with low global sensitivity does not need to be inconsistent with the goal of designing a compression algorithm that achieves good compression rates. 



% We note that one can construct a \emph{computable} variant of Kolmogorov compression that preserves the global sensitivity although the compression algorithm is still computationally inefficient. Instead of outputting the minimum-size Turing machine, one can output the \emph{minimum-score} Turing machine followed by $1\circ 0^{\log t_M}$, where the \emph{score} of a Turing machine $M$ is defined as $\score(M)\coloneqq|M|+\log t_M$ where $t_M$ is the running time of the machine $M$. Since the length of the compression becomes the score of the minimum-score Turing machine, one can similarly argue that this computable variant of Kolmogorov compression has global sensitivity $\O{\log n}$. This implies that a good compression scheme (a compression scheme with high compression ratio) does not necessarily have high global sensitivity. 



%\begin{definition}[Differential Privacy {\cite{TCC:DMNS06}}]

%\end{definition}







%Huffman \cite{Huffman52} using an optimal tree structure, Lempel and Ziv \cite{LZ77,LZ78} using a pointer-based encoding, and by many others \cite{CheRob67,Welch84,rfc1951,rfc7932}. 






%Data compression aims to reduce the size of data representations while preserving the essential information. The importance of data compression has emerged over the last couple of decades as digital images became more common and the amount of data that is transmitted via network is rapidly growing. In that sense, data compression offers reductions in storage space, followed by lower data transmission time and communication bandwidth. While data compression dates back to Morse code that was invented in 1838, early modern data compression algorithms were developed by Huffman \cite{Huffman52} using an optimal tree structure, Lempel and Ziv \cite{LZ77,LZ78} using a pointer-based encoding, and by many others \cite{CheRob67,Welch84,rfc1951,rfc7932}. 

%There are two types of compression algorithms --- \emph{lossless} compression and \emph{lossy} compression. While lossy compression focuses on increasing the compression ratio at the cost of permanently losing some input information (e.g., JPEG), lossless compression does not lose any information about the input so that it can later be decompressed to the original input perfectly (e.g., ZIP and 7z). In general, lossless compression works by identifying and eliminating statistical redundancy of input. For example, the string `aaaaabbb' can be compressed to `a5b3' so that the length of the file becomes half of the original string and by looking at the compression output `a5b3', one can perfectly decompress it back to the original string `aaaaabbb' once s/he knows the compression algorithm. 

%However, lossless compression has a significant drawback when it comes to the Compress-Then-Encrypt framework. The Compress-Then-Encrypt framework works by compressing a string $w$ to obtain a new compressed file $y$ and then encrypting $y$ to obtain a ciphertext $c=\Enc_K(y)$. While encryption hides the content of the compressed file $y$, it is impossible for any efficient encryption scheme to completely hide any information about the length of $y$. In fact, for many commonly deployed encryption schemes such as AES-GCM \cite{rfc5282} and Triple DES \cite{rfc8429}, one can infer the exact length of $y$ from the ciphertext $c$. Unfortunately, leaking the length of the compressed file $y$ can sometimes reveal information about the \emph{content} of the original message $w$\footnote{Oversimplifying a bit, consider the following example: suppose that a message $m=(m_1,m_2,m_3)$ contains $3$ components: (1) $m_1$ is a public message, (2) $m_2$ is a secret message, and (3) $m_3$ is a message controlled by adversary. If the adversary came down to the conclusion that $m_2$ contains either `Password' or `LetMeIn!', s/he can include `Password' 10 times in $m_3$ and `LetMeIn!' 10 times in a new message $m_3'$ and see whether the compression length of $m=(m_1,m_2,m_3)$ is shorter or $m'=(m_1,m_2,m_3')$. If compression of $m'$ is shorter, it implies that `LetMeIn!' was a part of the secret message $m_2$.}. 

%Since lossless compression uses statistical redundancy, the compression length becomes highly sensitive to the input. It becomes problematic when compression is combined with encryption since often times standard encryption schemes such as AES-GCM \cite{rfc5282} and Triple DES \cite{rfc8429} leak input length, which is the length of compression. In this case, the attacker could manipulate a message to check if a certain string is included in the secret message or not. For example, suppose that a message $m=(m_1,m_2,m_3)$ contains $3$ components: (1) $m_1$ is a public message, (2) $m_2$ is a secret message, and (3) $m_3$ is a message controlled by adversary. If the adversary came down to the conclusion that $m_2$ contains either `Password' or `LetMeIn!', s/he can include `Password' 10 times in $m_3$ and `LetMeIn!' 10 times in a new message $m_3'$ and see whether the compression length of $m=(m_1,m_2,m_3)$ is shorter or $m'=(m_1,m_2,m_3')$. If compression of $m'$ is shorter, it implies that `LetMeIn!' was a part of the secret message $m_2$. Hence, it becomes an important question if one can design a compression scheme that can be released privately so that the attacker learns nothing about the input by only looking at the compression length.




\paragraph*{LZ77 Compression Scheme.} 
%The primary technical contribution of our paper is characterizing the global sensitivity of 
The LZ77 compression algorithm \cite{LZ77} takes as input a string $w \in \Sigma^n$ and outputs a sequence of blocks $B_1,\ldots, B_k$ where each block $B_i =[q_i, \ell_i, c_i]$ is a tuple consisting of two non-negative integers $q_i$ and $\ell_i$ and a character $c_i \in \Sigma$. Intuitively, if blocks $B_1,\ldots, B_{i-1}$ encode the first $\ctc$ characters of $w$ then the block $B_i =[q_i, \ell_i, c_i]$ encodes the next $\ell_i +1$ characters of $w$. In particular, if we have already recovered $w[1,\ctc]$ then the decoding algorithm can recover the substring $w[\ctc+1,\ctc+\ell+1] = w[q_i,q_i+\ell-1] \circ c_i$ can be recovered by copying $\ell_i$ characters from $w[1,\ctc]$ beginning at index $q_i$ and then appending the character $c_i$. The compression algorithm defines the first block as $B_1 = [0,0,w[1]]$ (where $w[i]$ denotes the $i\th$ character of $w$) and then initializes counters $\ctb=2$ and $\ctc=1$. Intuitively, the counter $\ctb$ indicates that we will output block $B_\ctb$ next and the parameter $\ctc$ counts the numbers of characters of $w$ that have already been encoded by blocks $B_1,\ldots, B_{\ctb-1}$. In general, to produce the $\ctb\th$ block we will find the longest prefix of $w[\ctc+1,n]$ that is a substring of $w[\max\{1,\ctc-W+1\},\ctc]$, where $W\leq n$ denotes the size of the sliding window --- the algorithm only stores $W$ most recent characters to save space. If the character $w[\ctc+1]$ is not contained in $w[\max\{1,\ctc-W+1\},\ctc]$ then we will simply set $B_\ctb=[0,0,w[\ctc+1]]$ and increment the counters $\ctb$ and $\ctc$. Otherwise, suppose that the longest such prefix has length $\ell_\ctb > 0$ then for some $z$ such that $\max\{0,\ctc-W\}+\ell_\ctb\leq z\leq \ctc$ we have $w[\ctc+1,\ctc+\ell_\ctb] = w[z-\ell_\ctb+1,z]$. Then we set $B_\ctb = [z-\ell_\ctb+1, \ell_\ctb, w[\ctc+\ell_\ctb+1]]$, increment $\ctb$, and update $\ctc = \ctc+\ell_\ctb+1$. We terminate the algorithm if $\ctc=n$ (i.e., the entire string has been encoded) and then output the blocks $(B_1,\ldots,B_{\ctb-1})$. See \figref{fig:LZ77} for a toy example of running the LZ77 compression for a string $w=``aababcdbabca"\in\Sigma^{12}$ with $\Sigma=\{a,b,c,d\}$ and $W=n$.

\input{Figures/fig_LZ77}

The decompression works straightforwardly as follows: (1) Given the compression $(B_1,\ldots,B_t)$, parse $B_1=[0,0,c_1]$ and initialize the string $w=``c_1"$. (2) For $i=2$ to $t$, parse $B_i=[q_i,\ell_i,c_i]$ and convert it into a string $v$: if $q_i=\ell_i=0$ then $v\coloneqq c_i$; otherwise, $v\coloneqq w[q_i,q_i+\ell_i-1]\circ c_i$. Then update $w\gets w\circ v$. 

% In particular, LZ77 compression algorithm \cite{LZ77} is  Deflate \cite{rfc1951} uses 
% In this work, we focus on the \emph{lossless} compression scheme, i.e., the output of the compression does not lose any information about the input so that it can later be decompressed to the original input perfectly.

% It is natural to study the sensitivity of compression schemes --- informally speaking, the sensitivity of a compression scheme is defined as the maximum possible output length difference between neighboring input strings, i.e., two strings with Hamming distance $1$ --- since understanding the sensitivity of a compression scheme is a quintessential element to designing a differentially private compression algorithm. Lagarde and Perifel \cite{SODA:LagPer18} analyzed LZ78 compression scheme \cite{LZ78} and proved that LZ78 compression scheme has high sensitivity by showing that flipping a single bit of a string changes the string from compressible to incompressible.

% This raises the question of whether a ``good'' compression scheme necessarily has high sensitivity. TODO: connect this to our results with discussion on Kolmogorov compression.



%Finally, we show that good compression schemes do not necessarily have high global sensitivity by analyzing a computable variant of Kolmogorov compression and proving that the compression scheme has global sensitivity $\mathcal{O}(\log n)$.

%\subsection{Technical Overview}

% \paragraph*{Differentially Private Compression.}
% \seunghoon{separate section}
% Given any compression scheme $\compress:\Sigma^*\rightarrow(\Sigma')^*$ to another compression scheme called $\dpcompress:\Sigma^*\times\mathbb{R}\times\mathbb{R}\rightarrow(\Sigma')^*$ such that for any string $w\in\Sigma^*$, $\dpcompress(w,\epsilon,\delta)$ is $(\epsilon,\delta)$-differentially private. Intuitively, $\dpcompress(w,\epsilon,\delta)$ works by running $\compress(w)$ and adding a random amount of padding. In particular, $\dpcompress(w,\epsilon,\delta)$ outputs $\compress(w) \circ 0\circ 1^{p-1}$ where $p$ is a random variable defined as follows: $p=\max\left\{1, \lceil Z+k\rceil \right\}$: where $Z\sim\Lap(\GS_\compress/\epsilon)$ is a random variable following Laplace distribution with mean $0$ and scale parameter $\GS_\compress/\epsilon$ and $k=\frac{\GS_\compress}{\epsilon}\ln(\frac{1}{2\delta})+\GS_\compress +1$ is a constant. Intuitively, the output $Z+\left|\compress(w) \right|$ preserves $\epsilon$-DP since this is just the standard Laplacian Mechanism and the output $\lceil Z+\left|\compress(w) \right| + k\rceil =  \left|\compress(w) \right| + \lceil Z+k\rceil$ also preserves $\epsilon$-DP since it can be viewed as post processing applied to a DP output. Finally, note that by our choice of $k$ we have $\Pr[p \neq \lceil Z+k\rceil ] =  \Pr[Z+k \leq 0] \leq \delta$. It follows that the output $\left|\compress(w) \right| + p$ preserves $(\epsilon,\delta)$-DP. 

% On average, the amount of padding added is approximately $k=\frac{\GS_\compress}{\epsilon}\ln(\frac{1}{2\delta})+\GS_\compress +1$. If $k  = o(n)$ then it will still be possible for $\dpcompress$ to achieve efficient compression ratios, i.e., $\left| \dpcompress(w,\epsilon,\delta) \right|/ n = \left| \compress(w) \right|/ n + o(1)$. On the other hand, if $k = \Omega(n)$ then the padding alone will prevent us from achieving a compression ratio of $o(1)$. Thus, our hope is to find practical compression schemes with global sensitivity $\GS_\compress = o(n)$ --- LZ78~\cite{LZ78} does not satisfy these criteria \cite{lagarde2018lempel}. 

% \seunghoon{Quickly give explanations of how decompress algorithm works focusing on how to remove padding (footnote). Add details in the appendix}



%\paragraph*{Global Sensitivity of a Computable Variant of Kolmogorov Compression.}




% \section{Overview of Our Techniques}\seclab{sec:technique}

% \seunghoon{TODO: this section needs to be merged into \secref{sec:gslz77}}



