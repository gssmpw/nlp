\section{Related Work}
There has been a wide body of work on developing efficient compression algorithms. Huffman coding ____ encodes messages symbol by symbol --- symbols that are used most frequently are encoded by shorter binary strings in the prefix-free code. Lempel and Ziv ____ developed multiple compression algorithms and Welch ____ published the LZW algorithm as an improved implementation of ____. In our work, we are primarily focused on analyzing the LZ77 compression algorithm ____ since it is one of the most common lossless compression algorithms used in practice. Deflate ____ is a lossless compression scheme that combines LZ77 compression ____ and Huffman coding ____ and is a key algorithm used in ZIP archives. The Lempel–Ziv–Markov chain algorithm (LZMA) is used in the 7z format of the 7-Zip archiver and it uses a modification of the LZ77 compression. Brotli ____ also uses the LZ77 compression with Huffman coding and the 2nd-order context modeling. 

Several prior papers have studied the sensitivity of compression schemes ____ to small changes in the input. While Lagarde and Perifel ____ focused on the multiplicative sensitivity of the LZ78 compression algorithm ____, their result implies that the additive sensitivity (i.e., global sensitivity) can be as large as $\Omega(n)$. Giuliani et al. ____ studied the additive sensitivity of the Burrows-Wheeler Transform with Run-Length Encoding proving that the additive sensitivity can be as large as $\Omega(\sqrt{n})$ --- upper bounding the additive sensitivity remains an open question. 

Most closely related to ours is the work of Akagi et al. ____ who studied the additive and multiplicative sensitivity of several compression schemes including Kolmogorov and LZ77. Akagi et al. ____ proved that Kolmogorov compression has additive sensitivity $\O{\log n}$. We extend this result to a computable variation of Kolmogorov compression in \secref{sec:kolmogorov}. For LZ77, they proved that the additive sensitivity is lower bounded by $\Omega(\sqrt{n})$ when the window size is $W=\Omega(n)$. We prove that the additive sensitivity is lower bounded by $\widetilde\Omega(n^{2/3})$. Finally, they prove that the (local) additive sensitivity of LZ77 is {\em at most} $\O{z}$ where $z$ is the length of the compressed file. Unfortunately, this result does not even imply that the global sensitivity is $o(n)$ because $z$ can be as large as $z= \Omega(n)$ when the file is incompressible. By contrast, we prove that the global sensitivity is upper bounded by $\O{W^{2/3} \log n}$ which is tight up to logarithmic factors and is at most $\O{n^{2/3} \log n}$ even when $W = \Omega(n)$.


Degabriele ____ introduced a formal model for length-leakage security of compressed messages with random padding. While Degabriele ____ did not use differential privacy as a security notion, his analysis suggests that DP friendly distributions such as the  Laplace distribution and Gaussian distribution minimized the leakage. Song ____ analyzed the (in)security of compression schemes against attacks such as cookie-recovery attacks and used the additive sensitivity of a compression scheme to upper bound the probability of successful attacks. Neither work ____ formalized the notion of a differentially private compression scheme as we do in \secref{sec:dpcompress}.  

%investigated length-hiding on compressed messages via random padding using Laplace and Gaussian distributions. Although this work's security model does not appear to be differential privacy, its suggested padding distributions could potentially achieve DP. We believe it is still worth formalizing the DP compress problem (see \secref{sec:dpcompress} for details).

Ratliff and Vadhan introduced a framework for differential privacy against timing attacks ____ to deal with information leakage that could occur when the running time of a (randomized) algorithm might depend on the sensitive input. They propose to introduce random positive delays after an algorithm is finished. The length of this delay will depend on the sensitivity of the algorithm's running time to small changes in the input. While our motivation is different, there are similarities: they analyze the sensitivity of an algorithm's running time while we analyze the sensitivity of a compression algorithm with respect to the length of the file. They introduce a random positive delay while proposing to add a random positive amount of padding to the compressed file. 



%Akagi et al. ____ studied the additive and multiplicative sensitivity of compression schemes using an edit-distance framework. Their definition of additive sensitivity is similar to global sensitivity, but they consider any single edit, i.e., insertion, deletion, or substitution. The multiplicative sensitivity is the maximum ratio of compressed lengths between strings with edit distance $1$. While they analyzed the additive sensitivity of Kolmogorov compression like our approach, they did not explore the computable variant of Kolmogorov compression. They also showed the lower bound of $\Omega(\sqrt{n})$ for the additive sensitivity of LZ77, and an upper bound on the local sensitivity of LZ77. We remark that our upper/lower bounds for the global sensitivity of LZ77 are tighter (See \secref{sec:upperbound} and \secref{sec:lowerbound} for details).