\section{Related Work}
\paragraph{Model Editing}
Many model editing methods focus on updating knowledge in LLMs by modifying parameters, including meta-learning and locate-then-edit approaches~\cite{DBLP:conf/emnlp/YaoWT0LDC023,DBLP:journals/corr/abs-2308-07269}. Meta-learning methods train a hypernetwork to apply gradient-based updates to model parameters~\cite{DBLP:conf/emnlp/CaoAT21,DBLP:conf/iclr/MitchellLBFM22}, as in Knowledge Editor (KE) and MEND. Locate-then-edit methods identify MLPs storing factual knowledge and edit them by injecting new key-value pairs~\cite{DBLP:conf/nips/MengBAB22,DBLP:conf/iclr/MengSABB23}, leveraging the observation that these layers can act as key-value memories~\cite{DBLP:conf/emnlp/GevaSBL21, DBLP:conf/emnlp/GevaCWG22}. Additionally,~\citet{DBLP:journals/corr/abs-2012-00363} proposed constrained fine-tuning for modified facts. Recent works extend these approaches to domains such as model personality changes~\cite{DBLP:journals/corr/abs-2310-02168}, multimodal models~\cite{DBLP:conf/emnlp/0008TL0WC023}, and user privacy protection~\cite{DBLP:conf/emnlp/WuLXDW0X23}. 
Studies also focus on sequential editing scenarios, showing that as edits increase, general abilities tend to degrade~\cite{DBLP:journals/corr/abs-2401-04700,DBLP:conf/acl/0003BLZZW0H24,jiang2024neuron,fang2024alphaedit}.
Additionally, \citet{ma2024perturbation} made a theoretical analysis to elucidate that the bottleneck of the general abilities during sequential editing lies in the escalating value of the condition number of the edited matrix.

\paragraph{Saliency Analyses}
In Computer Vision (CV), extensive research on input saliency maps has contributed to explainable machine learning. Methods include pixel-space sensitivity maps~\cite{DBLP:journals/corr/SmilkovTKVW17} and class-discriminative localization~\cite{DBLP:journals/ijcv/SelvarajuCDVPB20}. Data-level saliency, also called data attribution, has been widely studied for model explanation~\cite{DBLP:journals/corr/abs-2308-03296}, efficient training~\cite{DBLP:conf/nips/XieS0L23}, and improving generalization~\cite{DBLP:conf/cvpr/JainSK0PM23}. Compared to input saliency and data attribution, model saliency is less explored. Weight sparsity~\cite{DBLP:conf/iclr/FrankleC19}, used in weight pruning, can be viewed as a weight saliency map that preserves model abilities. In NLP, model editing research~\cite{DBLP:conf/acl/DaiDHSCW22,DBLP:conf/nips/MengBAB22,DBLP:conf/iclr/MengSABB23} focuses on locating and modifying specific knowledge by targeting weights. This concept of an 'editable model region' aligns with weight saliency in NLP, where certain parameters are more influential and editable.

Compared with previous studies~\cite{DBLP:conf/nips/MengBAB22,DBLP:conf/iclr/MengSABB23,DBLP:conf/emnlp/YaoWT0LDC023} that are the most relevant to our work, a main difference should be highlighted. 
These approaches target at designing editing algorithms or evaluation paradigms to improve or assess the performance of model editing. However, previous work has not provided an in-depth analysis or an effective solution to preserve these abilities based on that analysis.
In contrast, our study seeks to analyze the factors for the degradation of the general abilities of the model in sequential editing and introduces the EAC framework to preserve these general abilities.