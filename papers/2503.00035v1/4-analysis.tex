\section{Analysis of Ability Degradation}
\label{analysis}

ROME~\cite{DBLP:conf/nips/MengBAB22} and MEMIT~\cite{DBLP:conf/iclr/MengSABB23} are 
 currently popular model editing methods. Given that MEMIT builds upon the foundations of ROME by implementing residual distribution across multiple layers,  our analysis in the main text focuses primarily on ROME. 
This section presents a detailed analysis of how the model is affected during sequential editing using ROME. 
Statistical and visual analyses reveal that the degradation of general abilities is related to the unintentional introduction of the non-trivial noise that can make the parameter matrix after editing deviate from its original semantics space.

\subsection{Comparison with Fine-tuning Approach}
First, a statistical analysis is conducted by editing GPT2-XL~\cite{radford2019language} using the ZsRE~\cite{DBLP:conf/conll/LevySCZ17} dataset. 
Considering the L1 norm effectively quantifies the absolute changes in parameter values pre- and post-editing, while providing insights into feature weight distributions within the matrix, it is used to represent the degree of change in the parameter matrix.
As illustrated in Figure~\ref{fig-edit}, when using editing-based methods such as ROME and MEMIT, the L1 norm of the matrix at the edited layer increases significantly with the number of edits.
It can be seen that the norm increases by 317\% (ROME) and 61\% (MEMIT), respectively by the end of sequential editing.
This result highlights a significant deviation from the unedited model, emphasizing the impact of sequential edits on stability.

\begin{figure}[t]
  \subfigure[Editing-based methods]{
  \includegraphics[width=0.22\textwidth]{figures/L1-Norm-GPT2XL.pdf}
  \label{fig-edit}}
  \subfigure[Fine-tuning approach]{
  \includegraphics[width=0.22\textwidth]{figures/finetune-GPT2XL.pdf}
  \label{fig-finetune}}
\vspace{-2mm}
\caption{Illustration of the change of L1 norm 
(a) in sequential editing at the edited layer using editing-based methods and 
(b) in fine-tuning different batch steps for selected layers.
Here we uniformly selected the layers of GPT2-XL for clarity when fine-tuning.} 
\vspace{-2mm}
\end{figure}

A gradient-based fine-tuning approach can markedly enhance the performance of the model on specific tasks while preserving its general abilities across other downstream tasks~\cite{DBLP:journals/corr/abs-2312-12740, DBLP:journals/corr/abs-2310-10047}. 
As depicted in Figure~\ref{fig-finetune}, there are no significant changes in the norm of the parameter matrix for the given layers, with a maximum change of only 0.27\%, even as the amount of fine-tuning knowledge increases. This stability in the parameter matrix norms suggests that the fine-tuning approach does not introduce significant non-trivial noise during the editing process. Thus, fine-tuning maintains the integrity and stability of the model's parameters, which is crucial for preserving its general abilities and preventing unintended non-trivial noise.

This comparison indicates that each editing method not only updates the intended fact as expected but also unintentionally introduces non-trivial noise into the model. This noise manifests as deviations in the parameter matrix during the sequential editing process. With each additional edit, the noise accumulates, progressively increasing the deviation in the parameter matrix. Consequently, as the number of edits grows, there is a significant deviation in the parameter matrix observed before and after the editing. This accumulated noise highlights the challenge of maintaining the stability and integrity of the parameter matrix through multiple edits, which can ultimately impact the general abilities of the model.

\begin{figure}[t]%[!htb]
  \subfigure{
  \includegraphics[width=0.45\textwidth]{figures/legend_visible.pdf}}
  \subfigure{
  \includegraphics[width=0.22\textwidth]{figures/pca-gpt2-right.pdf}}
  \subfigure{
  \includegraphics[width=0.22\textwidth]{figures/pca-llama3-right.pdf}}
\vspace{-2mm}
\caption{Visialization of six sets of facts recalled by LLMs using 2-dimensional PCA. Note that this hidden state is also projected by a language modeling head (linear mapping) for next-token prediction, implying the linear structure in the corresponding representation space (the PCA assumption).} 
\vspace{-3mm}
\label{fig-pca}
\end{figure}

\subsection{Visualization Analysis} \label{sec_visual}
Following the ROME, the second layer of MLP \( W^{(l)}_{\text{proj}} \) is viewed as a linear associative memory~\cite{anderson1972simple, DBLP:journals/tc/Kohonen72}. This perspective observes that any linear operation \( W \) can operate as a key-value store for a set of vector keys \( K = [\mathbf{k}_1 | \mathbf{k}_2 | \dots] \) and corresponding vector values \( V = [\mathbf{v}_1 | \mathbf{v}_2 | \dots] \), by solving \( WK \approx V \)~\cite{DBLP:conf/nips/MengBAB22}.
The key-value pair \( (\mathbf{k}_i, \mathbf{v}_i) \) represents the representation of the input prompt, where $\mathbf{k}_i$ identifies patterns of the input and $\mathbf{v}_i$ is the fact recalled by the model, which is considered to gather all the information about how the model understands the prompt and how it will respond. By stacking $\mathbf{k}_i$ and $\mathbf{v}_i$ separately for each prompt, matrix \( K \) and \( V \) are obtained.
Based on this, 200 prompts of the same downstream task are collected to compute \( K \) and \( V \). 

On GPT2-XL and LLaMA-3 (8B), Principal Component Analysis is employed to visualize the hidden state of the facts of the downstream task recalled by the model. 
The first two principal components of six sets of facts, representing most features, are computed~\cite{zheng2024prompt}. Two of these are derived from recalling the model before editing and the model after editing without any constraint, respectively.
To explore the relationship between the deviation of the parameter matrix after editing and the resulting degradation of general abilities, four additional settings were tested by setting different percentages of the columns in the update matrix to zero, evenly distributed according to an arithmetic progression.

As illustrated in Figure~\ref{fig-pca}, the principal
components of facts recalled by the original model and the edited model without any constraint can be largely distinguished, whose boundaries (black dashed lines) can be easily fitted using logistic regression.
This indicates a significant semantic discrepancy between the facts recalled by the unconstrained edited model and the original model, explaining the decline in general abilities is related to matrix deviation.
Furthermore, when the deviation of the parameter matrix is constrained by reducing the norm of the update matrix, as shown by the black arrows, the principal components of the recalled facts by the edited model gradually align with those of the original model.
This shows that by reducing the norm of the update matrix, the deviation of the parameter matrix after editing can be constrained, making the semantic distribution of the model before and after editing similar, thereby preserving the general abilities of the edited model.