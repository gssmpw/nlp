\pdfoutput=1
\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage[final]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{float}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\hyphenpenalty=5000
\tolerance=1000

\newcommand{\MODELNAME}{\textsc{VerifyScore}}
\newcommand{\jc}[1]{\textcolor{red}{[JC: #1]}}
\newcommand{\hy}[1]{\textcolor{blue}{[HY: #1]}}

\newcommand{\jy}[1]{\textcolor{purple}{[JY: #1]}}

\title{Constraining Sequential Model Editing with Editing Anchor Compression}


\author{
Hao-Xiang Xu$^1$\thanks{Equal contribution.}, Jun-Yu Ma$^1$\footnotemark[1], Zhen-Hua Ling$^1$\thanks{Corresponding author.}, Ningyu Zhang$^2$, Jia-Chen Gu$^3$  \\
  $^1$National Engineering Research Center of Speech and Language Information Processing, \\
      University of Science and Technology of China, Hefei, China \\
  $^2$Zhejiang University \\
  $^3$University of California, Los Angeles \\
{\tt \{nh2001620,mjy1999\}@mail.ustc.edu.cn}, {\tt zhling@ustc.edu.cn}, \\ 
{\tt zhangningyu@zju.edu.cn}, {\tt gujc@ucla.edu} 
}


\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) struggle with hallucinations due to false or outdated knowledge. Given the high resource demands of retraining these models, there is an increasing focus on developing \emph{model editing}. 
However, the general abilities of LLMs across downstream tasks are prone to significant degradation during sequential editing.
This paper statistically observes that the parameter matrix after editing exhibits a significant deviation compared to its previous state as the number of edits increases.
This serious deviation affects the original knowledge associations within LLMs and leads to the degradation of their general abilities.
To this end, a framework termed \textbf{E}diting \textbf{A}nchor \textbf{C}ompression (EAC) is proposed to constrain the deviation of the parameter matrix during sequential editing. 
It compresses the editing information by selecting editing anchors that are important in encoding new relations without deviating too much from the original matrix, thereby preserving the general abilities. 
Experiments of applying EAC to two popular editing methods on three LLMs across four tasks are conducted. 
Evaluation results show that EAC effectively minimizes unreasonable deviations caused by model editing, preserving over 70\% of the general abilities while better retaining the editing knowledge compared to the original counterpart methods\footnote{Code is available: \href{https://github.com/famoustourist/EAC}{https://github.com/famoustourist/EAC}}.
\end{abstract}

\input{1-introduction}
\input{2-related}
\input{3-preliminary}
\input{4-analysis}
\input{5-method}
\input{6-experiments}
\input{7-conclusion}
\input{9-limitations}
\input{10-Acknowledgements}

\bibliography{custom}


\clearpage
\newpage
\appendix
\onecolumn
\input{8-appendix}


\end{document}
