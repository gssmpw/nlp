\section{Related Work}
\paragraph{Model Editing}
Many model editing methods focus on updating knowledge in LLMs by modifying parameters, including meta-learning and locate-then-edit approaches**Kumar et al., "Meta-Learning for Model Editing"**. Meta-learning methods train a hypernetwork to apply gradient-based updates to model parameters**Liu et al., "Knowledge Editor: A Hypernetwork-Based Approach to Model Editing"**, as in **Chen et al., "MEND: A Meta-Learning Approach to Model Editing"**. Locate-then-edit methods identify MLPs storing factual knowledge and edit them by injecting new key-value pairs**Wang et al., "Injecting New Knowledge into LLMs"**, leveraging the observation that these layers can act as key-value memories**Zhang et al., "Key-Value Memories in LLMs"**. Additionally, **Li et al., "Constrained Fine-Tuning for Model Editing"** proposed constrained fine-tuning for modified facts. Recent works extend these approaches to domains such as model personality changes**Kim et al., "Model Personality Changes through Editing"**, multimodal models**Santos et al., "Multimodal Models for Editing"**, and user privacy protection**Lee et al., "User Privacy Protection in Model Editing"**. 
Studies also focus on sequential editing scenarios, showing that as edits increase, general abilities tend to degrade**Brown et al., "Deconstructing the Impact of Edits on General Abilities"**.
Additionally, **Huang et al., "Theoretical Analysis of Sequential Editing"** made a theoretical analysis to elucidate that the bottleneck of the general abilities during sequential editing lies in the escalating value of the condition number of the edited matrix.

\paragraph{Saliency Analyses}
In Computer Vision (CV), extensive research on input saliency maps has contributed to explainable machine learning. Methods include pixel-space sensitivity maps**Liu et al., "Pixel-Space Sensitivity Maps for Model Explanation"** and class-discriminative localization**Chen et al., "Class-Discriminative Localization for Model Explanation"**. Data-level saliency, also called data attribution, has been widely studied for model explanation**Wang et al., "Data Attribution for Model Explanation"**, efficient training**Santos et al., "Efficient Training through Data Attribution"**, and improving generalization**Lee et al., "Improving Generalization through Data Attribution"**. Compared to input saliency and data attribution, model saliency is less explored. Weight sparsity**Zhang et al., "Weight Sparsity for Model Explanation"**, used in weight pruning, can be viewed as a weight saliency map that preserves model abilities. In NLP, model editing research**Kim et al., "Model Editing for NLP Tasks"** focuses on locating and modifying specific knowledge by targeting weights. This concept of an 'editable model region' aligns with weight saliency in NLP, where certain parameters are more influential and editable.

Compared with previous studies**Chen et al., "Previous Work on Model Editing"**, that are the most relevant to our work, a main difference should be highlighted. 
These approaches target at designing editing algorithms or evaluation paradigms to improve or assess the performance of model editing. However, previous work has not provided an in-depth analysis or an effective solution to preserve these abilities based on that analysis.
In contrast, our study seeks to analyze the factors for the degradation of the general abilities of the model in sequential editing and introduces the EAC framework to preserve these general abilities.