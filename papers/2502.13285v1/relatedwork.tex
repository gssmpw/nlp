\section{Related work}
\label{sec:relatedwork}

The formulation of task shift in this paper, particularly our focus on the shift from classification to regression tasks, shares both similarities and differences with several prominent areas of research in machine learning. Compared to transfer learning~\citep{pan2009survey}, task shift similarly aims to generalize knowledge from one ``job'' to another. However, while transfer learning emphasizes preserving useful features for similar or downstream tasks, task shift focuses on generalizing from a simpler task (\eg classification) to a more complex one (\eg regression). Task shift is also related to the concept of distribution shift~\citep{moreno2012unifying}. In task shift, the source data distribution remains unchanged, but the conditional distributions of the labels differ at test time. Furthermore, our classification-to-regression setup is closely connected to the one-bit compressive sensing problem~\citep{boufounos2008one}. However, while one-bit compressive sensing focuses on optimal estimators or algorithms based on a known measurement process, our work emphasizes unique properties of the $\ell_2$-inductive bias.

The theoretical analysis in this work builds directly on the literature on benign overfitting. \cite{bartlett2020benign}~\cite{tsigler2023benign} characterized benign overfitting for regression estimators, while~\cite{muthukumar2021classification} provided a survival and contamination analysis for sparse signals in both classification and regression settings. Our analysis substantially develops insights from these works to estimate the support of a sparse signal even when neither classification nor regression generalizes.

We discuss additional related work in Appendix~\ref{app:expanded_related_work}.