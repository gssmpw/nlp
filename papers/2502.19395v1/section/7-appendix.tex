\appendix
\section*{Appendix}
\label{sec:appendix}


\input{section/2-related-work}


\input{section/3-preliminary}
\input{section/4-method}


\input{section/tab/tab-2-param}

\section{Datasets and Labels}
\label{sec:appendix-data}
{\bf Datasets.} 
We selected all experimentally solved antibody structures released in the SAbDab antibody database \citep{dunbar2013sabdab,schneider2021sabdab} before January 1, 2024, to sample our training set. Notice that we remove CDR sequences that are identical to those in the dataset to eliminate redundancy in the dataset.
Following FoldSeek \citep{van2024foldseek}, for each CDR in the SAbDab-before-2024 dataset, we randomly sample equal-length CDRs with TM-score large than $0.6$ to generate training pairs. The final training set consisted of $45,043$ antibody CDR pairs. 
After finishing model training, all $24,479$ unique CDR structures in the SAbDab-before-2024 dataset are utilized to construct the CDR vector database. 
The test set of SAbDab-2024 include experimentally solved antibody released in SAbDab antibody database between January 3, 2024 and May 29, 2024. 
This process resulted in $4,449$ test CDR samples that are completely unseen during the model training process.

The sequence similarity distribution between the training set and test set is illustrated in Figure \ref{fig:seq_sim}. As we can observe, the average sequence similarity for each CDR region in the training and test set is around 0.3 to 0.5, which shows that there is no potential data leakage issue in this data split strategy.
In addition, we utilize a T-cell receptor dataset released in the structural T-cell receptor database \citep{leem2018stcrdab} to construct a test set with $5,111$ receptors, referred to as STCRDab.
To evaluate the model efficiency, we utilize $5,000$ predicted CDR-H3 loops from the Observed Antibody Space (OAS) \citep{olsen2022oas}, denoted as OAS-H3. Redundant CDR loops are removed from the test set. Statistics of these datasets are listed in Table \ref{tab:datasets}.

{\bf Labels.}
PyIgClassify cluster labels \citep{north2011fccc,adolf2015fccc} are employed as ground-truth labels to assess the retrieval performance of antibody CDR regions. For each PDB structure containing an identified antibody heavy or light chain, PyIgClassify categorizes the conformations of CDRs using a three-tier strategy: chain and position, length, and the similarity of dihedral angles. For instance, the cluster ID L1-10-1 denotes a CDR-L1 with a length of 10 amino acids, where the subcluster 1 is determined based on the similarity of dihedral angles using the affinity propagation clustering method \citep{frey2007clustering}.


\input{section/tab/tab-1-dataset}


\section{Implementation Details}
\label{sec:appendix-param}
In this section, we introduce the implementation details of our {\igseek}.
The MEGNN model introduced in Section \ref{sec:method} consists of three key learnable functions:
\begin{itemize}[topsep=0.5mm, partopsep=0pt, itemsep=0pt, leftmargin=10pt]
    \item The edge module $\phi_e$ (refer to Eq. \ref{eq:megnn:edge}) consists of a two-layer MLP with two Leaky Rectified Linear Unit (LeakyReLU) activation functions \citep{xu2015leakyrelu}. Besides, a dropout function \citep{srivastava2014dropout} with $0.1$ dropout rate is employed on the output of $\phi_e$: 
    $$
    \begin{aligned}
        &\text{CONCAT(Features)} \rightarrow \text{Input} \rightarrow \{ \text{LinearLayer()} \rightarrow \text{LeakyReLU()} \rightarrow \text{LinearLayer()} \\
        &\rightarrow \text{LeakyReLU()} \} \rightarrow 
        \text{Dropout} \rightarrow \text{Output}.
    \end{aligned}
    $$
    \item The coordinate module $\phi_X$ (refer to Eq. \ref{eq:megnn:coordinate}) contains a two-layer MLP that shares weights with the MLP in the edge module $\phi_e$. 
    \item The node module $\phi_h$ (refer to Eq. \ref{eq:megnn:feature}) is a two-layer MLP with one LeakyReLU activation function:
    $$    
        \text{CONCAT(Features)} \rightarrow \text{Input} \rightarrow \{ \text{LinearLayer()} \rightarrow \text{LeakyReLU()} \rightarrow \text{LinearLayer()} \} \rightarrow \text{Output}.
    $$
\end{itemize}

In our experiments, we train the MEGNN model in {\igseek} using PyTorch \citep{paszke2019pytorch} with an Adam optimizer \citep{kingma2015adam} on 4 NVIDIA Tesla A100 GPUs. Table \ref{tab:appendix-param} lists the hyperparameters of {\igseek}.



\begin{figure}[h]
  \centering
  \includegraphics[width=0.55\textwidth]{section/fig/seq_iden.pdf}
  \vspace{-4mm}
  \caption{Sequence similarity between SAbDab train/test set.}
  \label{fig:seq_sim}
  \vspace{-4mm}
\end{figure}

\section{Baselines}
\label{sec:appendix-baseline}

The first category is structure retrieval model:
\begin{itemize}[topsep=0.5mm, partopsep=0pt, itemsep=0pt, leftmargin=10pt]
    \item {\bf FoldSeek} \citep{van2024foldseek} represents tertiary amino acid interactions using 3D interaction (3Di) structural alphabet, achieving 4 to 5 orders of magnitude speed-up compared to traditional iterative or stochastic structure retrieval methods like CE \citep{shindyalov1998ce}, Dali \citep{holm2020dali}, and TM-align \citep{zhang2005tmalign}. Official code is available at: \href{https://github.com/steineggerlab/foldseek}{https://github.com/steineggerlab/foldseek}.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{section/fig/bar_aar_antibody_3aa.pdf}
  \vspace{-2mm}
  \caption{The Comparison of Average AAR on the SAbDab-2024 Dataset using CDRs with extensions of 1 to 3 amino acids on each side in the flanking regions.}
  \label{fig:exp-extension}
  %\vspace{-5mm}
\end{figure}


The second category is protein and antibody design models:
\begin{itemize}[topsep=0.5mm, partopsep=0pt, itemsep=0pt, leftmargin=10pt]
    \item {\bf ProteinMPNN}  \citep{dauparas2022mpnn} 
   is a deep learningâ€“based method for protein sequence design that excels in both in silico and experimental evaluations, achieving a sequence recovery of 52.4\% on native protein backbones, compared to 32.9\% for Rosetta \citep{adolf2018rosetta,baek2021rosetta}. By leveraging a message-passing neural network with enhanced input features and edge updates, ProteinMPNN is capable of designing monomers, cyclic oligomers, protein nanoparticles, and protein-protein interfaces, rescuing previously failed designs generated by Rosetta \citep{baek2021rosetta} or AlphaFold \citep{jumper2021alphafold}. 
    Official code is available at: \href{https://github.com/dauparas/ProteinMPNN}{https://github.com/dauparas/ProteinMPNN}.
    \item {\bf ESM-IF1} \citep{hsu2022esmif1} 
    employs a sequence-to-sequence Transformer to predict protein sequences from backbone atom coordinates, which is pre-trained on structures of 12M protein sequences. It achieves 51\% native sequence recovery and 72\% for buried residues.
    Official code is available at: \href{https://github.com/facebookresearch/esm/tree/main/examples/inverse\_folding}{https://github.com/facebookresearch/esm/tree/main/examples/inverse\_folding}.
    \item {\bf AbMPNN} \citep{dreyer2023abmpnn} fine-tunes ProteinMPNN on the SAbDab \citep{dunbar2013sabdab,schneider2021sabdab} dataset for antibody design, outperforming generic protein models in sequence recovery and structure robustness, especially for the hypervariable CDR-H3 loop. The profile of model weights is available at: \href{https://zenodo.org/records/8164693}{https://zenodo.org/records/8164693}.
    \item {\bf AntiFold} \citep{hoie2024antifold} is an antibody-specific inverse folding model fine-tuned from ESM-IF1 \citep{hsu2022esmif1} on solved antibody structures from the SAbDab dataset \citep{dunbar2013sabdab,schneider2021sabdab} and predicted antibody structures from the OAS dataset \citep{kovaltsuk2018oas, olsen2022oas}.
    AntiFold excels in sequence recovery and structural similarity while also demonstrates stronger correlations in predicting antibody-antigen binding affinity in a zero-shot manner.
    Official code is available at: \href{https://github.com/oxpig/AntiFold}{https://github.com/oxpig/AntiFold}.
\end{itemize}


\section{Additional Experiments}
\label{sec:appeidix-exp}
{\bf CDR with extensions.}
In this set of experiments, we compare {\igseek} with protein and antibody design baselines using the SAbDab-2024 dataset. We focus on CDRs with backbone extensions of $n$ amino acids on each side in the flanking regions. Fig. \ref{fig:exp-extension} illustrates the results for varying values of $n = 0, 1, 2, 3$. As we can observe, the performance of {\igseek} improves with the inclusion of additional amino acids in the given structure, , which aligns with the fact that more input structural information can be encoded into the CDR representation. In contrast, other baseline models are adversely affected by hallucinations stemming from conserved backbone structures. Notably, when $n=3$, {\igseek} consistently outperforms its competitors by at least $5\%$ and $18\%$ for heavy chain and light chain CDR loops, respectively. This further demonstrates that the retrieval-based strategy employed by {\igseek} effectively mitigates hallucinations during CDR sequence generation.

\input{section/tab/tab-4-K}


{\bf Influence of value $K$.}
In this set of experiments, we conduct experiments on the SAbDab-2024 dataset to evaluate the impact of varying parameter $K$ in {\igseek}. Table \ref{tab:param-k} reports the average AAR of {\igseek} across different values of $K$ on the SAbDab-2024 dataset. As we can observe, the performance of IgSeek exhibits a decline as $K$ increases. In our implementation, we set $K=10$ rather than $5$ as {\igseek} achieves comparable results while preserving enhanced sequence diversity.



