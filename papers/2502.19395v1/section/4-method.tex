\section{Methodology: IgSeek}
\label{sec:method}


In this section, We present IgSeek, a retrieval-based framework for CDR sequence design. The core of IgSeek is isomorphic structure retrieval from a comprehensive antibody CDR database. As shown in Fig.~\ref{fig:framework}, IgSeek first builds a vector database using a pre-trained Multi-channel Equivariant Graph Neural Network (MEGNN) to encode CDR structural information. For a target CDR structure G, IgSeek generates its embedding using MEGNN, retrieves K-nearest structurally similar CDR loops from the database, and predicts G's sequence through ensemble and Bernoulli sampling of the retrieved sequences. In the following, we will present the model design of the MEGNN encoder in Section~\ref{sec:subsec-encoder}, discuss the learning objective and the sequence prediction in Section~\ref{sec:subsec-decoder}, followed by model analysis in Section~\ref{sec:subsec-thm}. 


\subsection{Multi Channel Equivariant Encoder}
\label{sec:subsec-encoder}

Recall that each amino acid residue $v_i$ is represented by its four backbone atoms, thereby we extend the general single-channel EGNN layer~\citep{satorras2021egnn,huang2022gmn} to a multi-channel layer, with each channel corresponding to a specific atom. Unlike existing approaches~\citep{kong2023dymean, hoie2024antifold} that leverage domain knowledge of the well-conserved antibody backbone structure, our MEGNN encoder generates CDR embeddings exclusively based on the antibody CDR structure, without relying on any prior backbone knowledge.


For a 3D CDR structure $G$, the MEGNN encoder takes the initial features of each residue $v_i$, denoted as $\vect{h}_i^{(0)} \in \mathbb{R}^{ d}$, along with the perturbed coordinates $\vect{\hat{X}}_i \in \mathbb{R}^{c \times 3}$ as input. Here, $c$ denotes the number of atoms, which is set to $c=4$, and $\vect{h}_i^{(0)}$ is initialized by a uniform distribution. 
$\vect{\hat{X}}_i = \vect{X}_i + \mathcal{N}(0, \sigma)$, where $\mathcal{N}(0, \sigma)$ denotes a small Gaussian noise. This perturbation introduces variability that enhances the robustness of the model.  

{\bf Multi-channel Equivariant Message Passing.}
The $l$-th layer of MEGNN updates both the node features $\vect{h}_i^{(l)}$ and coordinates $\vect{X}_i^{(l)}$ by Eq. \ref{eq:megnn:distance}-\ref{eq:megnn:feature}, where $\rho$ is a distance computation function, $\phi_e$, $\phi_X$ and $\phi_h$ are neural network transformations. The update process is defined as follows:
\begin{align}
    \vect{X}_{ij}^{(l-1)}, \vect{z}_{ij}^{(l-1)} &= \rho \left( \vect{X}_i^{(l-1)}, \vect{X}_j^{(l-1)}, e_{ij} \right), \label{eq:megnn:distance} \\
    \vect{h}_{e_{ij}}^{(l)} &= \phi_e \left( \text{CONCAT}\left(\vect{h}_i^{(l-1)}, \vect{h}_j^{(l-1)}, \vect{z}_{ij}^{(l-1)} \right) \right), \label{eq:megnn:edge} \\
    \vect{X}_i^{(l)} &= \phi_X \left( \vect{X}_i^{(l-1)}, \{ \vect{h}_{e_{ij}}^{(l)}, \vect{X}_{ij}^{(l-1)} | v_j \in \mathcal{N}_{i} \} \right), \label{eq:megnn:coordinate}\\
    \vect{h}_i^{(l)} &= \phi_h \left( \vect{h}_i^{(l-1)}, \{ \vect{h}_{e_{ij}}^{(l)} | v_j \in \mathcal{N}_i  \} \right). \label{eq:megnn:feature}
\end{align}

Specifically, MEGNN first computes the coordinate differences $\vect{X}_{ij}^{(l-1)}$ and the square distance $\vect{z}_{ij}^{(l-1)}$ between each pair of backbone atoms among different residues in $\rho$ (Eq.~\ref{eq:megnn:distance}) as below: 
\begin{align}
    \vect{X}_{ij}^{(l-1)} = \vect{X}_i^{(l-1)} - \vect{X}_j^{(l-1)}, \quad \vect{z}_{ij}^{(l-1)} = (\vect{X}_{ij}^{(l-1)})^{\top} \vect{X}_{ij}^{(l-1)}. \nonumber
\end{align}
Subsequently, an edge module $\phi_e$ generates the edge feature $\vect{h}_{e_{ij}}^{(l)}$ for each edge $e_{ij} = (v_i, v_j) \in E$. In Eq.~\ref{eq:megnn:edge}, the node features of $v_i$ and $v_j$, i.e., $\vect{h}_i^{(l-1)}$, $\vect{h}_j^{(l-1)}$, along with the fattened coordinate difference $(\vect{z}_{ij}^{(l-1)})$, are concatenated and transformed by an MLP, generating the output edge feature for the $l$-th layer. Next, the coordinate module $\phi_X$ updates the node coordinates $\vect{X}_i^{(l)}$ using the updated edge feature $\vect{h}_{e_{ij}}^{(l)}$ and the coordinate differences $\vect{X}_{ij}^{(l-1)}$ in Eq.~\ref{eq:megnn:coordinate}. Specifically, for each node $v_i$, $\phi_X$ first computes the message $\vect{m}_{j \rightarrow i}$ propagated from its neighbor $v_j$, and then updates the coordinates $\vect{X}_i^{(l)}$ of $v_i$ by aggregating the messages from its neighborhood:
\begin{align}
    \vect{m}_{j \rightarrow i} &= \text{MLP}\left( \vect{h}_{e_{ij}}^{(l)} \right) \cdot \vect{X}_{ij}^{(l-1)}, \\ \vect{X}_i^{(l)} &= \vect{X}_i^{(l-1)} + \frac{1}{|\mathcal{N}_i|} \sum_{v_j \in \mathcal{N}_i} \vect{m}_{j \rightarrow i}.
\end{align}
Finally, the node module $\phi_h$ updates the node representation $\vect{h}_i^{(l)}$ by Eq.~\ref{eq:megnn:feature}. For each node $v_i$, $\phi_h$ aggregates the features of the adjacent edges into $\vect{h}_{agg_i}^{(l)}$ and combines the node representation $\vect{h}_i^{(l-1)}$ from the $(l - 1)$-th layer with the aggregated feature using a residual connection \citep{he2016resnet}:
\begin{align}
    \vect{h}_{agg_i}^{(l)} &= \sum_{j \in \mathcal{N}_i} { \vect{h}^{(l)}_{e_{ij}}}, \\ \quad \vect{h}_{i}^{(l)} &= \vect{h}_i^{(l-1)} + \text{MLP}\left( \text{CONCAT}(\vect{h}_i^{(l-1)}, \vect{h}_{agg_i}^{(l)}) \right). \nonumber
\end{align}

{\bf CDR Embedding Generation.} After the equivariant message passing through an $L$-layer MEGNN, we employ a $\text{READOUT}$ function to aggregate the final node features to generate the representation of CDR $G$ that consists of $n$ nodes (amino acids) as Eq.~\ref{eq:megnn:readout},
\begin{align}
\label{eq:megnn:readout}
    \vect{h}_G = \text{READOUT}(\{\vect{h}_i^{(L)}\}_{i=1}^{n}).
\end{align}
The READOUT function can be a permutation invariant function, e.g. summation and element-wise mean pooling functions. In our implementation, we set the READOUT function as element-wise mean pooling by default. Algorithm \ref{alg:megnn} summarizes the forward pass of MEGNN. 


\input{section/alg/alg-1-megnn}


\subsection{Learning Objective and Sequence Generation}
\label{sec:subsec-decoder}
We train the MEGNN encoder by a self-supervised distance prediction task that explicitly aligns pairs of similar CDR in a given database. The goal is to align the structural representation of similar CDR pairs. 
For a CDR database $\mathcal{B} = \{G_1, G_2, \cdots, G_n\}$, we construct a training dataset $\mathcal{T} = \{(G_i, G_j), \cdots\}$ containing pairs of fixed-length CDRs whose TM-Score, calculated by TM-align \citep{zhang2005tmalign}, exceeds a specified threshold. Given a pair of CDRs $(G_i, G_j)$, we first generate their representations using MEGNN, denoted as $\vect{h}_{G_i}$ and $\vect{h}_{G_j}$, respectively.
Next, we predict the {\em Root Mean Square Deviation (RMSD)} of the two CDR structures by feeding the concatenation of $\vect{h}_{G_i}$ and $\vect{h}_{G_j}$ into an MLP decoder as Eq.~\ref{eq:megnn:distance_pred}:
\begin{align}
\label{eq:megnn:distance_pred}
    \widehat{d}(G_i, G_j) = \text{MLP} \left(\text{CONCAT} \left(\vect{h}_{G_i}, \vect{h}_{G_j} \right) \right). 
\end{align}


{\bf Loss Function.} 
The learning objective is to minimize the Mean Square Error between the predicted distance $\widehat{d}(G_i, G_j)$ and the actual distance $d(G_i, G_j)$ in the training dataset $\mathcal{T}$: 
\begin{align}
\label{eq:megnn:loss}
    \mathcal{L} = \frac{1}{|\mathcal{T}|}\sum_{(G_i, G_j) \in \mathcal{T} } \|\widehat{d}(G_i, G_j) - d(G_i, G_j) \|^2. 
\end{align}
Here, the actual distance $d(G_i, G_j)$ is computed as the RMSD of the two CDRs for their backbone atoms.
Since we do not have prior knowledge of the CDR cluster labels, our approach can be interpreted as an unsupervised geometric learning model. By minimizing the loss function defined in Eq. \ref{eq:megnn:loss}, the model effectively generates CDR embeddings that reflect the structural relationships among the CDRs in the dataset.


{\bf CDR Sequence Generation.}
Once the model training is complete, we establish a CDR vector database $\mathcal{Z}$, where each CDR$_i$ is represented by a triplet $(\vect{s}_i, G_i, \vect{h}_{G_i})$ consisting of amino acid sequence $\vect{s}_i$, its backbone structure graph $G_i$ and its embedding $\vect{h}_{G_i}$ generated by the MEGNN encoder via Eq.~\ref{eq:megnn:readout}. IgSeek is then able to infer the amino acid sequence of a CDR by querying its backbone structure in the database $\mathcal{Z}$. Let $\vect{s}_q$ denote the query CDR sequence with a length of $L$. At each position $l \in \{1,  \cdots, L\}$, the residue $\vect{s}_q(l)$ is selected from one of the $20$ amino acids, denoted as $a_i$ for  $ i \in \{ 1, \cdots, 20\}$. Then, the inference of the CDR sequence $\vect{s}_q$ given its backbone structure $G_q$ follows four steps: 
{\em (i)} first, the MEGNN encoder generates the embedding of $G_q$, denoted as $\vect{h}_{G_q}$. 
{\em (ii)} Second, the embedding $\vect{h}_{G_q}$ is used as the search key to perform a $K$-NN search in the database $\mathcal{Z}$, obtaining a set of $K$ CDRs of equal length $L$, denoted as $\mathcal{Z}_q = \{(\vect{s}_1, G_1, \vect{h}_{G_1}), (\vect{s}_2, G_2, \vect{h}_{G_2}), \cdots, (\vect{s}_K, G_K, \vect{h}_{G_K})\}$. 
{\em (iii)} Given the $K$ sequences $\mathcal{S}_q = \{\vect{s}_1, \cdots, \vect{s}_K\}$, we derive the probability of amino acid $a_i$ occurring at position $l$ of the predicted sequence $\hat{\vect{s}}_q$ as follows:
\begin{align}
    p\left(\hat{\vect{s}}_q(l) = a_i| S_q\right) = \frac{1}{K} \sum_{s_k \in \mathcal{S}_q } \mathbb{I}(\vect{s}_k(l), a_i), \nonumber
\end{align}
where $\mathbb{I}(\vect{s}_k(l), a_i) \in \{ 0, 1\}$ is a binary indicator that equals $1$ if the amino acid $a_i$ occurs at the position $l$ of sequence $\vect{s}_k$, and $0$ otherwise. {\em (iv)} To derive the final inferred sequence $\hat{s}_q$, we sample the amino acid at each position $l$ according to the generated probability distribution:
\begin{align}
    \hat{\vect{s}}_q(l) \sim p\left(\hat{\vect{s}}_q(l) | \mathcal{S}_q\right). \nonumber
\end{align}
Algorithm \ref{alg:train} outlines the training process, and Algorithm \ref{alg:design} presents the antibody CDR sequence design process, respectively.

\input{section/alg/alg-2-train}
\input{section/alg/alg-3-igseek}


\subsection{Analysis}
\label{sec:subsec-thm}
{\bf Model Complexity.} Given a 3D CDR structure represented by $G = (V,E)$, the initialized coordinates, node features, and the graph structure contribute a space complexity of $O(|V| \cdot d + |V| \cdot c + |E|) = O(|V|\cdot d + |E|)$, where $d$ denotes the hidden dimension of features and $c$ denotes the channel size. In MEGNN, the space complexity is dominated by the edge features, which have a complexity of $O(|E| \cdot d)$, and square distance $\vect{z}$ with a complexity of $O(|E| \cdot c^2)$. Consequently, the overall space complexity is $O(|E|\cdot d)$, which is linear to the input graph size.
Regarding the computational complexity of MEGNN, the dominant component is the edge module $\phi_e$ introduced in Eq.~\ref{eq:megnn:edge}, which has a time complexity of $O(|E| \cdot (2d+3c)^2 + |E| \cdot d^2 + 3c) = O(|E| \cdot d^2)$.


{\bf Coordinate Equivariance and Representation Invariance.} The following theorem shows that MEGNN is E(3) equivariant with respect to the initial coordinate $\vect{X}_i^{(0)}$ and E(3) invariant with respect to the representations $\vect{h}$ of the input CDR, respectively.
\begin{theorem}
\label{thm:thm1-equivariance}
    For any transformation $g \in E(3)$, we have $\vect{h}_i, T_{\mathcal{Y}}(g) \vect{X}_i^{(L)} = \text{MEGNN} \left( \vect{h}_i^{(0)}, T_{\mathcal{X}}(g) \vect{X}_i^{(0)}, G \right)$, where $T_{\mathcal{X}}$ and $T_{\mathcal{Y}}$ 
    $:= \vect{R} \vect{X} + \vect{b}$ denotes the transformation of $\vect{X}$ in the input space $\mathcal{X}$ (resp. output space $\mathcal{Y}$), $\vect{R}$ is an orthogonal matrix, and $\vect{b}$ is the bias.
\end{theorem}
The theorem indicates that MEGNN can be generalized to arbitrary E(3) group operations (refer to Section~\ref{sec:preliminary}), which showcases the data efficiency of MEGNN.


\begin{proof}
    We assume that $\vect{h}_i^{(0)}$ is invariant to E(3) transformation operations on the coordinate $\vect{X}_i^{(0)}$, since $\vect{h}_i^{(0)}$ is generated from uniform distribution and no absolute information of $\vect{X}_i^{(0)}$ is encoded into $\vect{h}_i^{(0)}$. 
    Then, for the E(3) transformation $g:= \vect{R} \vect{X} + \vect{b}$, where orthogonal matrix $\vect{R}\in O(3)$ and bias $\vect{b} \in \mathbb{R}^3$, we have:
    $$
    \begin{aligned}
        \vect{R} \vect{X}_{i}^{(l-1)} + \vect{b} - (\vect{R} \vect{X}_{j}^{(l-1)} + \vect{b}) &= \vect{R} \vect{X}_{ij}^{(l-1)}, \\
        (\vect{R} \vect{X}_{ij}^{(l-1)} )^{\top} \vect{R} \vect{X}_{ij}^{(l-1)} &= z_{ij}^{(l-1)}.
    \end{aligned}
    $$
    Therefore, the output $z_{ij}^{(l-1)}$ of Eq. \ref{eq:megnn:distance} is E(3) invariant to transformation $g$.
    
    As for Eq. \ref{eq:megnn:edge}, since $\vect{h}_i$, $\vect{h}_j$, and $z_{ij}^{(l-1)}$ are invariant to E(3) transformation operations, we can derive that $\vect{h}_{e_{ij}}^{(l)}$ is E(3) invariant.
    
    Next, we will prove Eq. \ref{eq:megnn:coordinate} is E(3) equivariant. 
    $$
    \begin{aligned}
        \vect{R} \vect{X}_i^{(l-1)} + \vect{b} + \frac{1}{|\mathcal{N}_i|} \sum_{v_j \in \mathcal{N}_i} \text{MLP} \left( \vect{h}_{e_{ij}}^{(l)} \right) \cdot \vect{R} \vect{X}_{ij}^{(l-1)}  &= \vect{R} \left(\vect{X}_i^{(l-1)} + \frac{1}{|\mathcal{N}_i|} \sum_{v_j \in \mathcal{N}_i} \vect{m}_{j \rightarrow i} \right) + \vect{b} \\
        &= \vect{R} \vect{X}_i^{(l)} + \vect{b}.
    \end{aligned}
    $$
    Therefore, we have proven that any E(3) transformation operations on $\vect{X}_i^{(l-1)}$ leads to the same E(3) transformation operations on $\vect{X}_i^{(l)}$ using Eq. \ref{eq:megnn:coordinate}.
    
    Finally, it is easy to verify that Eq. \ref{eq:megnn:feature} is E(3) invariant as $\vect{h}_i^{(l-1)}$ and $\vect{h}_{e_{ij}}^{(l)}$ are E(3) invariant. 
    
    In conclusion, for an $L$-layer MEGNN model, any transformation $g \in E(3)$ on the input coordinate $\vect{X}^{(0)}$ will lead to the same E(3) transformation operations on the output coordinate $\vect{X}^{(L)}$ while the representations $\vect{h}^{(L)}$ still remain E(3) invariant:
    $$
        \vect{h}_i, T_{\mathcal{Y}}(g) \vect{X}_i^{(L)} = \text{MEGNN} \left( \vect{h}_i^{(0)}, T_{\mathcal{X}}(g) \vect{X}_i^{(0)}, G \right).
    $$
    This finishes the proof.
\end{proof}

