\vspace{-1em}
\begin{abstract}


Recent advancements in video generation have significantly improved the ability to synthesize videos from text instructions. However, existing models still struggle with key challenges such as instruction misalignment, content hallucination, safety concerns, and bias. Addressing these limitations, we introduce \datasetname, a large-scale video preference benchmark designed to evaluate video generation across five critical aspects: \textit{Alignment, Safety, Fineness, Coherence \& Consistency, and Bias \& Fairness}. This benchmark incorporates 28 fine-grained criteria to provide a comprehensive evaluation of video preference. Building upon this dataset, we propose \algname, a Mixture-of-Experts (MoE)-based video reward model designed to deliver fine-grained reward. \algname can dynamically select relevant experts to accurately judge the preference based on the input text-video pair.
This architecture enables more precise and adaptable preference judgments. Through extensive benchmarking on \datasetname, we analyze the limitations of existing video reward models and demonstrate the superior performance of \algname in video preference assessment, achieving 17.58\% and 15.87\% improvements in overall and fine-grained preference judgments, respectively. Additionally, introducing \algname for preference tuning in video generation enhances the alignment performance.
%
All our code, data, and models are available at \url{https://aiming-lab.github.io/MJ-VIDEO.github.io/}.




% Our findings highlight the importance of fine-grained reward modeling and establish MJ-VIDEO as a robust framework for improving video generation quality.





%
% Recent rapid development of video generation models rely on the capability of multi-modal reward models which are used to align the VideoLLM with human preferences, mitigating hallucination, safety, and bias issues. 
% However, existing multi-modal reward models are often limited to providing a general alignment judgment while lack adaptability for varied alignment goals in video generation such as content coherence and factual consistency. 
% %
% To tackle these limitations, we introduce \datasetname, a large-scale video preference dataset consisting of five subsetsâ€”\textit{Alignment}, \textit{Safety}, \textit{Fineness}, \textit{Coherence and Consistency}, and \textit{Bias and Fairness}.
% Each subset aims at evaluating a specific aspect of video preferences. 
% To provide a clear assessment for each aspect, we also define 28 criteria in total.
% Building on this dataset, we propose \algname, a mixture-of-expert (MoE) video reward model that delivers comprehensive video generation judgment by decomposing the assessment into multiple aspects.
% Specifically, we train specialized expert for every aspect and develop a router language model to automatically select best matched experts based on the input text and video.
% This MoE architecture can more precisely reflect the video preferences and thereby enhancing the video generation alignment.
% %
% Through benchmarking on the proposed \datasetname, we carefully analyze the limitations of existing video reward models.
% The superior performance of \algname in judging the video preference demonstrates success of developing a MoE based video reward model.
\end{abstract}

