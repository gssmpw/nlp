\section{Experiment}
In our experiments, we utilize the proposed \datasetname\ and the corresponding reward model, \algname, to explore the following questions: (1) Can existing large vision-language models (LVLMs) or VideoLLMs effectively judge video preferences? (2) Does training on fine-grained preference annotations improve the performance of a video reward model? (3) Can introducing \algname into the preference tuning process improve the alignment of generated videos? (4) What is the advantage of adopting a MoE architecture in video preference judgment? 

\subsection{Experimental Setup}

\begin{table*}[htbp]
\centering
\small
\caption{Testing on aspect annotations in \datasetname. The bolded numbers in the table represent the best results, while the underlined numbers indicate the second-best results. The "C\&C" in the table refers to "Coherence and Consistency," while “B\&F" refers to "Bias and Fairness." In cases where certain models show strong bias, causing the F1 score to be NaN, a "/" is used in place of the result in the table. For preference comparison, we report the results of the ``strict" metric. See Appendix~\ref{apd:tie_aspect} for the ``tie-aware" metric results.}
\vspace{0.5em}
% \setlength{\tabcolsep}{1.7pt}
\renewcommand{\arraystretch}{1.2}
\adjustbox{max width=\textwidth}{ 
\begin{tabular}{l|ccc|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Alignment}} & \multicolumn{3}{c|}{\textbf{Safety}} & \multicolumn{3}{c|}{\textbf{Fineness}} & \multicolumn{3}{c|}{\textbf{C \& C}} & \multicolumn{3}{c}{\textbf{B \& F}} \\
\cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16}
 & \textbf{Acc} & \textbf{F1} & \textit{strict} & \textbf{Acc} & \textbf{F1} & \textit{strict} & \textbf{Acc} & \textbf{F1} & \textit{strict} & \textbf{Acc} & \textbf{F1}  & \textit{strict} & \textbf{Acc} & \textbf{F1} & \textit{strict} \\
\midrule
InternVL2-2B & \underline{70.75} & 60.42 & 17.71 & 66.67 & 55.02 & 16.67 & 63.59 & 49.87 & 3.125 & \underline{71.81} & \underline{46.04} & 10.34 & 74.11 & \textbf{63.19} & 54.54 \\
InternVL2-4B & 57.00 & 55.00 & 26.96 & 75.49 & 60.37 & 0.00 & 52.48 & 49.92 & 7.143 & 43.02 & 33.11 & 17.86 & 66.32 & \underline{56.27} & 54.55 \\
InternVL2-8B & 44.21 & 44.21 & 33.33 & 76.72 & 72.60 & 16.67 & 47.71 & 47.27 & 18.75 & 27.76 & 24.29 & 12.07 & 15.51 & 13.88 & 50.00 \\
InternVL2-26B & 65.47 & \underline{62.96} & 40.51 & \underline{84.44} & \underline{78.26} & 20.00 & \textbf{69.81} & 51.91 & 14.29 & 59.03 & 41.51 & 16.33  & \underline{82.05} & 59.85 & 30.00 \\
Qwen2-VL-2B & 54.28 & 53.03 & 19.35 & 59.82 & 56.93 & 25.00 & 56.75 & 51.86 & 3.448 & 37.90 & 31.18 & 16.39 & 20.00 & 19.31 & 38.46 \\
Qwen2-VL-7B & 58.31 & 56.19 & 41.94 & 55.35 & 52.81 & 25.00 & 47.56 & 46.33 & 31.03 & 32.58 & 27.68 & 19.67 & 14.61 & 13.13 & 23.08 \\
MiniCPM-8B & 65.53 & 61.38 & 48.72 & 72.91 & 67.22 & 40.00 & 62.13 & \underline{56.02} & \underline{39.29} & 49.73 & 37.21 & 31.25 & 15.12 & 14.17 & \underline{60.00}\\
CogVLM2 & 26.71 & 23.80 & 7.692 & 31.67 & 30.09 & 16.67 & 35.61 & 29.79 & 11.76 & 7.87 & 7.86 & 4.615 & 14.61 & / & 7.692 \\
Gemini-1.5-flash & 27.45 & 25.72 & 8.421 & 83.64 & 77.34 & 0.0 & 32.80 & 25.27 & 12.90 & 5.01 & 4.88 & 12.07 & 15.18 & / & 9.091 \\
GPT-4o & 58.27 & 56.21 & \underline{50.00} & 82.86 & 77.00 & \underline{50.00} & 59.67 & 56.34 & 27.27 & 44.52 & 34.17 & \underline{40.00} & 19.17 & 18.48 & 33.33\\\midrule
\rowcolor{gray!30} \textbf{MJ-VIDEO} & \textbf{78.41} & \textbf{71.22} & \textbf{79.05} & \textbf{87.50} & \textbf{81.84} & \textbf{83.33} & \underline{68.60} & \textbf{58.53} & \textbf{58.82} & \textbf{95.36} & \textbf{53.57} & \textbf{58.46} & \textbf{86.92} & 55.97 & \textbf{69.23} \\
\bottomrule
\end{tabular}}
\label{tab:aspect_evaluation}
\vspace{-1.5em}
\end{table*}

\textbf{Dataset Split.} We divide \datasetname\ into a training set and a test set at a 4:1 ratio, leading to 4,336 training video pairs and 1,085 testing video pairs.

\textbf{Existing Multimodal Judge Models.} 
We benchmark several popular LVLMs, both open- and closed-source, for video preference judgment. Open-source models include InternVL2~\citep{chen2023internvl}, Qwen~\citep{Qwen2VL}, and CogVLM2~\citep{hong2024cogvlm2}, while closed-source models include GPT-4o~\citep{openai2024gpt4technicalreport} and Gemini~\citep{geminiteam2024geminifamilyhighlycapable}. To ensure stable scoring and reduce ambiguity, we follow~\citet{chen2024mjbenchmultimodalrewardmodel} by prompting models to assign verbalized 10-range scores (e.g., “Extremely Poor,” “Very Good”). The top-5 scores are considered good, and the bottom-5 as bad. See Appendix~\ref{apd:prompt_single} for details. Additionally, we evaluate VideoScore~\citep{he2024videoscore} on overall video preference, though it cannot perform aspect-level evaluations due to the absence of per-aspect results.











% During the testing of VideoLLMs, we found that directly instruct the models to provide a score ranging from 0 to 100 is quite challenging. 
% Moreover, The model has unstable performance for different prompts.
% To address this issue, we design a specific prompt for each subset to guide the model's focus on the various criteria within that subset and provide a discrete score. 

% Specifically, we provide the text-video pair with the evaluation prompt to the model.
% The model is then asked to assign one of the following ten labels to the video: ``Extremely Poor," ``Very Poor," ``Poor," ``Below Average," ``Average," ``Above Average," ``Good," ``Very Good," ``Excellent," or ``Outstanding".
% This scoring method leverages the semantic understanding and video evaluation capabilities of VideoLLM. It also ensures that the model’s evaluation criteria remain consistent when scoring different videos and prompts~\citep{chen2024mjbenchmultimodalrewardmodel}.
% Detailed prompt design is provided in Appendix~\ref{apd:prompt_single}.


\textbf{Evaluation Plans and Metrics.} We conduct two types of evaluations:

% \begin{itemize}[leftmargin=*]
% \vspace{-0.5em}

\noindent \textit{Video Preference Evaluation.} We evaluate both aspect-level and overall video preference using accuracy as the evaluation metric. In this evaluation, the judge model is given prompt-video pairs and tasked with assigning scores. The model's preference for each video pair is then determined by comparing the assigned scores.

Regarding the evaluation metric, many LVLMs often assign the same score to a pair of videos, making it challenging to accurately determine video preference. To address this, we adopt two accuracy calculation methods, resulting in two metrics. The first metric, \textit{strict}, treats cases where the model fails to indicate a preference as incorrect. The second metric, \textit{tie-aware}, considers identical scores as a partial match, awarding 0.5 when counting correct judgments.

\noindent \textit{Video Quality Evaluation.} We assess video quality based on the assigned scores for each aspect and category in MJ-Bench. Given the potential imbalance in score distribution, we use accuracy (Acc) and F1 score as evaluation metrics.








% \end{itemize}














%55.38
%70.086
%60.482
\subsection{Fine-Grained Video Quality and Preference Evaluation Results}\label{sec:aspect_evaluation}
In this section, we evaluate \algname\ alongside other multimodal judges for video quality and preference across aspects. The results are summarized in Table~\ref{tab:aspect_evaluation}, with subcategory-level details provided in Appendix~\ref{apd:criteria_eval}. 

Our findings reveal two key insights. First, existing multimodal judge models, both open- and closed-source, show significant room for improvement. Second, our 2B \algname\ model outperforms all alternatives across nearly all categories. Specifically, compared to models of similar size (e.g., InternVL2-2B, Qwen2-VL-2B), \algname\ improves accuracy by 20.12\%, F1 score by 16.97\%, and 51.67\% higher in preference comparison. Notably, it even surpasses the 26B InternVL2 model, achieving a 15.52\% higher accuracy, 9.05\% higher F1 score, and 45.86\% improvement in preference comparison. The only area where InternVL2-26B partially excels is fineness evaluation as we expected, as larger models with more advanced visual encoders can better capture fine-grained visual details. 

\algname's superiority stems from two key factors. First, high-quality, fine-grained annotations enable training at both the aspect and subcategory levels, improving performance across all aspects. Second, its MoE architecture, leveraging a gating layer, effectively processes LVLM outputs by dynamically weighting criteria to generate aspect scores, benefiting from LVLM’s semantic and video understanding.















% In this section, we conduct evaluation of \algname\ and other multimodal judges to evaluate video quality and preference per aspect and subcategory. We report the results in Table~\ref{tab:aspect_evaluation} (see the results over each subcategories in Appendix XXX). According to the results, we first observe that there is a significant room for improvement in existing multimodal judge models for both closed-source and open-source models. Second, we observe that our lightweight 2B \algname\ judge model achieves the best performance across almost all categories. Specifically, compared to the best model with a similar size (e.g., InternVL2-2B, Qwen2-VL-2B), \algname\ excels on average by 20.12\% in accuracy and 16.97\% in F1, XXX\% in preference comparison. Our 2B \algname even outperforms the best 26B InternVL2 model by an average of 15.52\% in accuracy, 9.05\% in F1, XXX\% in preference comparison. The InternVL2-26B model only outperform \algname\ in the fineness evaluation, which is what we expected since larger model with more advanced visual encoder can improve the model's ability to capture finer details in visual input. The superiority of \algname\ can be attributed to two main factors. First, the high-quality, fine-grained annotations allowed the model to be trained on each category, which enabled it to achieve better performance on every aspect. The MoE architecture uses a gating layer to process LVLM outputs, weighting criteria to generate aspect scores. This leverages LVLM’s semantic and video understanding.

% Besides the strong performance of \algname, the InternVL series of models achieves second-best performance in most cases. Notably, due to InternVL2-26B excels in the Fineness test.  due to its larger parameter size and more advanced Image Encoder, which enhances its ability to capture finer details in images and better understand visual content. This improved visual perception contributes to its strong performance in the Fineness test.
% Among the closed-source models, both models performed just behind \algname\ and InternVL2-26B in the Safety aspect test, suggesting that both have incorporated AI safety considerations during their training or fine-tuning processes.

% A more detailed discussion of this is provided in the ablation study in Sec.~\ref{sec:abliation_study}.

% the baselines and \algname\ for video quality judgment on each category of \datasetname.
% As shown in Tab.~\ref{tab:aspect_evaluation}, 

 % To be more specific, \algname\, a 2B model,   
 
% The success of \algname\ can be attributed to two main factors. First, the fine-grained annotations enabled the model to train on each aspect and subcategory, leading to improved performance across all aspects during testing. Second, we adopted the MoE architecture, which utilizes a Gating Layer to analyze the outputs of the LVLM backbone and allocate weights to each criterion, thus combining them into a score for each aspect. This process leverages the semantic and video understanding capabilities of VLLM, rather than relying solely on the training data, which improves the model's generalization ability and leads to better performance on the Macro-F1 metric. A more detailed discussion of this is provided in the ablation study in Sec.~\ref{sec:abliation_study}.



% In Appendix~\ref{apd:criteria_eval}, we conducted criteria-level testing for each model. The results of these tests for the criteria under each aspect can be used to analyze the strengths and weaknesses of the models.


\subsection{Overall Video Preference Evaluation Results}\label{sec:overall_evaluation}


\textbf{Additional Dataset.} To enhance the robustness of overall video preference evaluation, in addition to using \datasetname, we incorporate two additional datasets: Safesora-test~\citep{dai2024safesorasafetyalignmenttext2video} and GenAI-Bench~\citep{jiang2024genai}, both of which contain video preference pairs.

% In Sec.~\ref{sec:aspect_evaluation}, we evaluate various VLLMs and video reward models' ability to judge video preferences from different aspects using the fine-grained annotations in \datasetname. 

% In Sec.~\ref{sec:overall_evaluation}, we test \algname\ and baselines on their holistic-level video preference judgment ability using the overall preference annotations in 

% In this section, we conducted a detailed evaluation of the video reward model's ability to predict holistic video preferences using the overall preference annotations in \datasetname, Safesora-test and GenAI-Bench. 



We present the evaluation results of all multimodal judge models in Table~\ref{table:holistic_results} and summarize the following observations. First, similar to the fine-grained analysis, there is room for improvement across these models. Second, \algname\ achieves the best test results on all datasets. Compared to the best baseline, \algname\ improves by 17.58\% on \datasetname, 15.95\% on Safesora-test, and 1.65\% on GenAI-Bench. In contrast, while the InternVL performed well in fine-grained evaluations, they do not achieve similarly strong results in overall video preference evaluation. This aligns with our expectations, as assessing overall video preference lacks the detailed breakdown provided by aspect-level evaluation, making it more challenging for LVLMs to make precise judgments. In comparison, \algname\ leverages a gating layer to integrate judgments across different aspects, enabling a comprehensive understanding of overall preference and contributing to its superior performance. Similarly, VideoScore, which also decomposes video preference, achieves the second-best results. This underscores the importance of fine-grained decomposition in enhancing the performance of video reward models.









% is an inherently complex, and without a detailed aspect, it is hard for VideoLLMs struggle to make accurate judgments. 

% In contrast to aspect-level testing, holistic-level testing 

% As a result, the outcomes produced by VideoLLMs in experiments are often closer to random score assignments.


\begin{table}[h]
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\footnotesize
\caption{Results of overall video preference evaluation. The best test results are highlighted in bold, and the second-best results are underlined. \textit{Strict} treats undecided cases as incorrect, while \textit{tie-aware} assigns 0.5 for ties in calculating accuracy.}
\vspace{0.3em}
\adjustbox{max width=0.48\textwidth}{
\begin{tabular}{l|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{\datasetname}} & \multicolumn{2}{c|}{\textbf{Safesora-test}} & \multicolumn{2}{c}{\textbf{GenAI-Bench}} \\
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7}
 & \textit{strict} & \textit{tie-aware} & \textit{strict} & \textit{tie-aware} & \textit{strict} & \textit{tie-aware} \\
\midrule
InternVL2-2B & 5.93 & 47.88 & 4.60 & 50.30 & 13.71 & 55.43 \\
InternVL2-4B & 13.55 & 49.15 & 11.74 & 50.91 & 39.00 & 61.79 \\
InternVL2-8B & 16.95 & 47.88 & 14.29 & 53.09 & 36.85 & 62.43 \\
InternVL2-26B & 22.88 & 53.81 & 10.41 & 52.00 & 31.86 & 55.64 \\
Qwen-VL-2B & 13.33 & 48.09 & 13.18 & 51.27 & 27.29 & 56.71 \\
Qwen-VL-7B & 17.14 & 47.62 & 14.58 & 52.41 & 20.57 & 51.36 \\
MiniCPM & 30.51 & 53.39 & 25.30 & 52.54 & 47.43 & 60.21 \\
CogVLM2 & 8.47 & 47.46 & 9.56 & 52.48 & 21.29 & 56.29 \\
VideoScore & \underline{58.47} & \underline{58.47} & \underline{55.33} & \underline{55.51} & \underline{69.14} & \underline{69.14} \\
Gemini & 2.66 & 48.67 & 2.66 & 48.67 & 21.45 & 50.71 \\
GPT-4o & 35.35 & 54.6 & 35.35 & 54.6 & 48.85 & 59.14 \\\midrule
\rowcolor{gray!30} MJ-VIDEO & \textbf{68.75} & \textbf{68.75} & \textbf{64.16} & \textbf{64.16} & \textbf{70.28} & \textbf{70.28} \\
\bottomrule
\end{tabular}
}
\label{table:holistic_results}
\vspace{-1.5em}
\end{table}



\subsection{\algname\ in Preference Alignment for Text-to-Video Generation}
In this section, we introduce \algname\ as the reward model within the RLAIF framework to enhance video rewarding for generating preference-aligned videos, which are then used for preference fine-tuning of text-to-video (T2V) diffusion models. We select VideoCrafter2~\cite{chen2024videocrafter2} as the backbone T2V diffusion model and follow the VADER~\cite{prabhudesai2024videodiffusionalignmentreward} framework, replacing its reward model with either VideoScore or \algname\ for preference fine-tuning. The training data is sourced from VidProM~\cite{wang2024vidprom}, from which we randomly sample 5,000 instances for training (see Appendix~\ref{apd:exp_detail} for experimental details). After fine-tuning, we conduct two types of evaluation: \textit{automated evaluation} using VBench~\cite{huang2023vbench}, assessing performance across four dimensions—image quality, human action, scene composition, and overall consistency—and \textit{human evaluation}, where we sample 1,000 instances from VidProM to assess video quality and text-video alignment. We present the results in Table~\ref{tab:combined_evaluation}, where we observe that the model fine-tuned with \algname\ as the reward model outperforms both VideoScore and the original VideoCrafter2 model in most evaluation aspects, highlighting its effectiveness in improving the alignment of generated videos with input instructions.

% achieves an 8.4\% improvement in video quality and a 5.88\% improvement in alignment. As shown in Tab.~\ref{table:dimension_results} \algname\ outperforms VideoScore on image quality, human action and scene. We further provide some cases in Appendix XXX to illustrate the comparison between \algname\ and
% VideoScore.

% Fig.~\ref{fig:Case_study_DPO} provides detailed examples illustrating the advantages of fine-tuning with \algname\ compared to VideoScore.  
% In the first case, the cat generated by the model fine-tuned with \algname\ appears more realistic, with its face oriented toward the piano in a way that better aligns with the prompt’s intended scene. In the second case, the xylophone produced by the \algname\-fine-tuned model includes detailed key structures, resulting in a higher level of visual fidelity and overall video quality. This demonstrates the advantages of \algname\ in enhancing video realism, detail fidelity, and scene depiction.





% We also evaluated the fine-tuned model on VBench, primarily reporting performance across the following dimensions: imaging quality, human action, scene, and overall consistency. 


%18.41
%70.086

% \begin{table}[h]
% \centering
% \setlength{\tabcolsep}{6pt}
% \renewcommand{\arraystretch}{1.2}
% \footnotesize
% \vspace{0.3em}
% \adjustbox{max width=0.4\textwidth}{
% \begin{tabular}{l|cc}
% \toprule
% \textbf{Model} & \textbf{Video Quality} & \textbf{Alignment} \\
% \midrule
% VideoCrafter2 & 56.30 & 68.80 \\
% VideoScore & \underline{64.50} & \underline{74.80} \\
% \rowcolor{gray!30} \algname & \textbf{69.90} & \textbf{79.20} \\
% \bottomrule
% \end{tabular}
% }
% \caption{Human evaluation on video quality and text-to-video alignment after Optimization.}
% \label{tab:quality_alignment}
% \vspace{-1.5em}
% \end{table}

% \begin{table}[h]
% \centering
% \setlength{\tabcolsep}{6pt}
% \renewcommand{\arraystretch}{1.2}
% \footnotesize
% \vspace{0.3em}
% \adjustbox{max width=0.7\textwidth}{
% \begin{tabular}{l|ccccc}
% \toprule
% \textbf{Model} & \textbf{IQ} & \textbf{HA} & \textbf{S} & \textbf{OC} \\
% \midrule
% VideoCrafter2 & \underline{67.04} & 90.00 & 54.00  & \underline{28.39}  \\
% VideoScore & 66.15 & \underline{93.00} & \underline{54.78}  &  \textbf{28.47} \\
% \rowcolor{gray!30} \algname & \textbf{67.89}  & \textbf{94.00} &  \textbf{55.09} & 28.19  \\
% \bottomrule
% \end{tabular}
% }
% \caption{Evaluation results across different dimensions of VBench. \textbf{IQ}: Imaging Quality, \textbf{HA}: Human Action, \textbf{S}: Scene, \textbf{OC}: Overall Consistency. We report commonly used VBench evaluation dimensions to compare text-to-video models fine-tuned using different methods.}
% \label{table:dimension_results}
% \vspace{-1.5em}
% \end{table}
\begin{table}[t]
\centering
\setlength{\tabcolsep}{4pt}
\caption{Evaluation of video models across human evaluation and automated evaluation on VBench. Human evaluation assesses Video Quality and Text-to-Video Alignment. Automated evaluation on VBench evaluates Imaging Quality (\textbf{IQ}), Human Action (\textbf{HA}), Scene (\textbf{S}), and Overall Consistency (\textbf{OC}).}

\renewcommand{\arraystretch}{1.2}
\footnotesize
\vspace{0.3em}
\adjustbox{max width=0.48\textwidth}{
\begin{tabular}{l|cc|cccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Human Eval}} & \multicolumn{4}{c}{\textbf{Auto Eval (VBench)}} \\
\cmidrule{2-7}
 & \textbf{Quality} & \textbf{Align} & \textbf{IQ} & \textbf{HA} & \textbf{S} & \textbf{OC} \\
\midrule
VideoCrafter2 & 56.30 & 68.80 & \underline{67.04} & 90.00 & 54.00 & \textbf{28.39} \\
VideoScore & \underline{64.50} & \underline{74.80} & 65.03 & \underline{92.00} & \underline{54.79} & \underline{28.38} \\\midrule
\rowcolor{gray!30} \algname & \textbf{69.90} & \textbf{79.20} & \textbf{67.89} & \textbf{94.00} & \textbf{55.09} & 28.19 \\
\bottomrule
\end{tabular}}
\label{tab:combined_evaluation}
\vspace{-1.5em}
\end{table}



\subsection{Ablation Study}\label{sec:abliation_study}

% \wang{check this paragraph to make it consistent with model description.} 
In the ablation study, we examine the impact of the two stacked MoE layers on model performance. Specifically, we design two ablation models: (1) \textbf{w/o Criteria MoE:} replacing the MoE layers with a regression layer that maps the output of InternVL2-2B to aspect scores, and (2) \textbf{w/o Aspect MoE:} replacing the MoE layers with a regression layer that maps the output of InternVL2-2B to the overall score. We train and evaluate both ablation models, compare them with \algname, and present the results in Figure~\ref{fig:Abliation_Study}(a) (see the results per aspect in Figure~\ref{fig:apd_abliation_detail} of Appendix~\ref{apd:abliation}) and Figure~\ref{fig:Abliation_Study}(b), respectively.

According to the results, \algname\ outperforms ``w/o Criteria MoE," achieving improvements of 2.64\%, 58.33\%, and 12.45\% in average accuracy, F1, and strict preference accuracy, respectively. The most notable gains are in ``Coherence and Consistency" and ``Bias and Fairness," where the model without Criteria MoE layer shows strong biases, failing to learn effectively from the training data. In contrast, \algname\ leverages the Criteria MoE layer to assign appropriate weights to each criterion, fully utilizing the LVLM’s ability to understand video and semantics. Additionally, compared with ``w/o Aspect MoE", \algname\ achieves an average improvement of 5.45\% across all three datasets, demonstrating the effectiveness of the Aspect MoE layer in enhancing overall preference modeling.


% with even more significant gains in Macro-F1, particularly in the 






% based on the output from the InternVL2-2B, thereby obtaining aspect scores. This approach 




% Additionally, we performed ablation experiments on GenAI-Bench to evaluate the impact of the MoE structure on model performance under out-of-distribution conditions.


% \algname\ adopts a two-layer MoE structure. The Gating Layer of the first MoE layer integrates the evaluation results across various criteria into aspect-level judgment results, while the Gating Layer of the second MoE layer combines the aspect-level results into an overall judgment result.

% We conduct ablation studies on \datasetname using both aspect annotations and overall preference annotations to evaluate two scenarios:

% (1) Directly using a regression layer to map the token hidden states output by the VideoLLM into aspect scores.
% (2) 

\begin{figure}[!hb]
    \centering
    \includegraphics[width=0.48\textwidth]{figure/ablation_study.pdf}
    \vspace{-1.5em}
        \caption{(a): Compare \algname with ``w/o Criteria MoE", where average results of Acc, F1, and strict metrics are evaluated over five aspects; (b) Compare \algname with ``w/o Aspect MoE" on \datasetname\, Safesora-test and GenAI-Bench.}
    \vspace{-1em}
    \label{fig:Abliation_Study}
\end{figure}


% \begin{figure}[!hb]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figure/abliation_overall.png}
%     \vspace{-2.5em}
%     \caption{Abliation study result on \datasetname\, Safesora-test and GenAI-Bench. \HY{need to revise}}
%     \label{fig:Abliation_Study_overall}
% \end{figure}




\subsection{Case Study}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/Case_study_reward.pdf}
    % \vspace{-1.5em}
    \caption{Two cases of video preference analysis.} 
    % Specifically, we chose one representative case from each of the following three perspectives: pairwise comparison, recognition of fine-grained objects in images, and detailed human features. These cases highlight the advantages of \algname\ over the baseline methods.}
    \label{fig:Case_study}
    \vspace{-1em}
\end{figure*}



In this section, we present two case study in Figure~\ref{fig:Case_study} to illustrate the advantages of \algname\ in video preference judgment, with additional cases provided in Appendix~\ref{apd:case_study}. In the first case, \algname\ successfully identifies the ethereal bird as a key detail in the input instruction and incorporates it into the evaluation, resulting in a more accurate assessment. In contrast, VideoScore overlooks the ethereal bird and incorrectly rates the alignment as good, revealing its limitation in capturing fine-grained object features. This outcome aligns with our expectations, as \algname\ is trained with preference pairs emphasizing fine-grained details, enabling a more balanced evaluation of alignment and visual fidelity. In the second case, both videos align with human preferences. \algname\ assigns a higher score to the first video, while VideoScore gives both videos relatively high scores but fails to differentiate which one is better. This is because \algname\ is trained on pairwise data, allowing it to make a more precise relative preference judgment even when the two videos have similar quality.










% In case one, both generated videos are relatively high quality,  


% , we select three representative case studies to demonstrate the advantages of \algname\ compared to existing methods, such as VideoScore and InternVL2-26B.


% Case Study 1: \algname\ demonstrates superior performance by leveraging pair-wise data during training and adopting a pair-based BT-loss approach. This enables the model to make more reasonable judgments in situations where both videos are either of very high or very low quality, effectively addressing edge cases. In comparison, VideoScore struggles in such scenarios and often fails to deliver optimal judgments, revealing its limitations.

% Case Study 2: \algname\ shows significant advantages in attending to fine-grained objects. By incorporating data focused on fine-grained details during training, \algname\ achieves a more balanced evaluation of Alignment and Fineness. For instance, in Figure 2, \algname\ successfully identifies the waterbird as an important detail and integrates it into the evaluation, providing a more accurate assessment. In contrast, VideoScore overlooks the waterbird, mistakenly considering the Alignment to be good, which highlights its deficiency in handling fine-grained object features.

% Case Study 3: \algname\ excels in evaluating specific details such as human faces and limbs. In Figure 3, when the video shows distorted human eyes, \algname\ is able to leverage its specialized mechanisms for face and limb evaluation to make a more accurate video preference judgment. By contrast, large-scale models like InternVL2-26B, in the absence of explicit prompts, tend to overlook such human-specific details and rely on coarse overall assessments, leading to suboptimal judgments.

