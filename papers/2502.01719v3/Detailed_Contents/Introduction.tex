% \section{Introduction}
% Despite the rapid advancements in video generation models that have significantly improved the quality of the videos they produce, these models continue to face major challenges. Issues such as imprecise adherence to instructions~\cite{hong2022cogvideolargescalepretrainingtexttovideo}, content hallucinations~\cite{unterthiner2019accurategenerativemodelsvideo}, and the generation of unsafe or biased outputs~\cite{singer2022makeavideotexttovideogenerationtextvideo, cho2023dallevalprobingreasoningskills} remain prevalent. To better address these challenges, recent approaches have introduced multi-modal judges (also known as reward models) to provide feedback on generated videos~\citep{he2024videoscore}. However, this feedback tends to be narrowly focused on basic text-video alignment and lacks the flexibility required for diverse alignment objectives across various use cases~\citep{yang2021associatingobjectstransformersvideo, Prabhudesai_Mendonca_Qin_Fragkiadaki_Pathak}. Furthermore, these judges typically offer only a single scalar reward, which lacks transparency and provides limited actionable insights \HY{what is actionable insights?} into the evaluation process~\citep{Yuan_Zhang_Wang_Wei_Feng_Pan_Zhang_Liu_Albanie_Ni_2023}.

% To circumvent these constraints, we present \algname, a novel mixture-of-judges (MoE)~\citep{cai2024surveymixtureexperts} video reward model that furnishes comprehensive and interpretable feedback by decomposing video generation into five principal aspects: \textit{text-video alignment}, \textit{safety}, \textit{bias}, \textit{quality}, and frame-level \textit{coherence}. Specifically, we train a specialized expert adapter to handle each aspect, so as to deliver precise, task-oriented evaluations \HY{what is task-oriented evaluation?}. Furthermore, we train another router adapter \HY{why there are adapter? only router and expert, I think?} to dynamically activate different combinations of experts w.r.t. the instruction, ensuring that their outputs align with the specified feedback objectives, thereby enabling adaptability across a wide array of video generation scenarios. To further enhance the transparency of the feedback, \algname incorporates chain-of-thought explanations alongside scalar scores, offering a granular, step-by-step rationale for the evaluations. This enhances interpretability and facilitates a more nuanced understanding of the model's performance across the various tasks.

% \HY{need to add benchmark scale here.} Furthermore, to benchmark \algnameâ€™s capabilities, we introduce \datasetname, a large-scale video preference dataset that encapsulates the five aforementioned dimensions and extends coverage to twenty-nine subcategories, addressing a broader range of video generation scenarios than existing benchmarks. \datasetname provides a comprehensive and diverse evaluation of various details that affect video generation quality, as well as human preferences regarding video safety and bias. Additionally, we label the specific reasons for these preferences using fine-grained labels. Compared to other video preference datasets, testing on MJ-BENCH-VIDEO enables a more detailed analysis of models.

% In our experiments, we tested the ability of Video large language models (Video-LLMs) and Vision large language models (VLLMs) to assess video preferences using various methods on the \datasetname benchmark. The results indicate that existing methods are insufficient in accurately judging video preferences. In contrast, \algname outperforms all current models, demonstrating strong competitiveness across all subsets. Moreover, \algname can provide explanations for preference judgments through formatted outputs.

% In summary, our contributions are threefold: 
% \begin{enumerate}[leftmargin=*]
%     \item We introduce \algname, a flexible, expert-driven reward model that delivers multi-dimensional feedback for video generation tasks;
%     \item We augment feedback transparency through the inclusion of chain-of-thought explanations;
%     \item We present \datasetname, a robust, multi-aspect dataset designed to evaluate video generation models across a diverse set of scenarios.
% \end{enumerate}

\vspace{-2em}
\section{Introduction}

Recent advancements in video generation have significantly improved the quality of generated videos from text instructions~\cite{Prabhudesai_Mendonca_Qin_Fragkiadaki_Pathak, Yuan_Zhang_Wang_Wei_Feng_Pan_Zhang_Liu_Albanie_Ni_2023, black2024trainingdiffusionmodelsreinforcement}. However, these models still face major challenges, including imprecise adherence to instructions~\cite{hong2022cogvideolargescalepretrainingtexttovideo, li2024surveylongvideogeneration}, content hallucinations~\cite{unterthiner2019accurategenerativemodelsvideo, chu2024soradetectorunifiedhallucination}, and the generation of unsafe or biased outputs~\cite{singer2022makeavideotexttovideogenerationtextvideo, cho2023dallevalprobingreasoningskills}. To address these challenges, recent approaches have introduced multi-modal reward models that evaluate generated videos~\citep{he2024videoscore, xu2021videoclipcontrastivepretrainingzeroshot}, which can then be leveraged in RLHF for better alignment~\citep{wallace2024diffusion, yuan2024instructvideo, huang2024diffusionrewardlearningrewards}. However, these evaluations are often limited to overall alignment assessments, lacking the flexibility to accommodate diverse alignment objectives across different use cases~\citep{yang2021associatingobjectstransformersvideo, Prabhudesai_Mendonca_Qin_Fragkiadaki_Pathak, wang2024mementoscomprehensivebenchmarkmultimodal, shao2020finegymhierarchicalvideodataset}. For instance, ensuring content coherence is more critical for sports videos, whereas safety considerations are paramount for cartoon videos. The lack of high-quality video preference data with fine-grained assessments further hinders the development of more advanced video reward models~\citep{he2024videoscore, dai2024safesorasafetyalignmenttext2video}.

% To address this limitation, we present \datasetname, a large-scale video preference dataset that comprehensively evaluates video preferences across six key dimensions: \textit{text-video alignment}, \textit{safety}, \textit{bias}, \textit{quality}, frame-level \textit{coherence} and \textit{overall preference}. 
% Specifically, \datasetname encompasses 29 subcategories to account for a wide range of video generation scenarios, providing a comprehensive and diverse evaluation of various details that affect video generation quality, as well as human preferences regarding video safety and bias. 
% Additionally, we annotate fine-grained explanations for those preferences, offering a more detailed analysis of model judgments compared to existing video preference datasets.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/main_figure.pdf}
    \vspace{-1.5em}
    \caption{\datasetname is a comprehensive and fine-grained large-scale video preference dataset, which includes five aspects: \textit{Alignment}, \textit{Safety}, \textit{Fineness}, \textit{Coherence and Consistency} (C\&C), and \textit{Bias and Fairness} (B\&F). Each aspect contains multiple detailed criteria to facilitate a thorough preference evaluation from different perspectives.}
    \label{fig:overview}
    \vspace{-1em}
\end{figure*}

To address this issue, as illustrated in Figure~\ref{fig:overview}, we introduce \datasetname, a large-scale video preference benchmark comprising five evaluation aspects: \textit{Alignment}, \textit{Safety}, \textit{Fineness}, \textit{Coherence and Consistency (C\&C)}, and \textit{Bias and Fairness (B\&F)}~\citep{chen2024mjbenchmultimodalrewardmodel, wang2024decodingtrustcomprehensiveassessmenttrustworthiness}, where each aspect represents a distinct aspect of preference evaluation. Additionally, we provide fine-grained annotations for these five aspects, covering a total of 28 criteria to enhance comprehensiveness in video judgments. \datasetname\ is designed to serve as a comprehensive benchmark for evaluating the judgment capabilities of video reward models and facilitating the development of more advanced video reward models in the future.

Building upon this dataset, we propose \algname, a Mixture-of-Expert (MoE)~\citep{cai2024surveymixtureexperts} based lightweight 2B video reward model that aims at providing comprehensive judgment by decomposing video assessment into the aforementioned five aspects.
Specifically, we expect to train specialized experts to handle each aspect, delivering precise evaluations tailored to that specific subset. 
However, in a more realistic scenario, videos are often not well categorized, which may bring additional efforts in the expert selection process~\cite{shazeer2017outrageouslylargeneuralnetworks, zhou2022mixtureofexpertsexpertchoicerouting}.
Inspired by the success of~\citet{armoreward}, we adopt the gating network to automatically select proper reward objectives based on the input video and instruction. This gating network can serve as a router to ensure that the judgments are consistently aligning with different objectives required by various video generation scenarios.

In summary, the primary contributions of this paper are \datasetname\ and \algname. \datasetname\ is a high-quality, large-scale video preference benchmark designed to comprehensively evaluate video reward models across five key aspects, covering a total of 28 fine-grained criteria. \algname\ is a MoE-based video reward model that delivers fine-grained judgments, capturing diverse video preferences and aligning with different objectives required in various video generation scenarios. In our experiments, we first use \datasetname\ to benchmark existing large vision language models (LVLMs)-based video judges, assessing their judgment capabilities across multiple aspects. The results reveal significant room for improvement in judging videos. We then show that \algname\ outperforms existing video reward models, achieving 17.58\% and 15.87\% improvements in overall and fine-grained video preference judgments, respectively, demonstrating its effectiveness in providing precise evaluations. Finally, we show that incorporating \algname\ for preference tuning in video generation improves the alignment of generated videos.















% coverage of various video generation scenarios.

% In summary, our contributions are threefold:
% \begin{enumerate}[leftmargin=*]
%     \item We present \datasetname, 
%     \item We propose \algname, 
%     \item The experiment results first demonstrate the limitations of existing video reward models, then show the effectiveness of the proposed \algname.
% \end{enumerate}