\section{Related Works}

\textbf{Multimodal Judge.} Multimodal judges are critical for assessing alignment between different data types, like text and images~\citep{ziegler2019fine, xu2021videoclipcontrastivepretrainingzeroshot, badlani2021ttsalignmentrule, chen2024autoprm, zhang2024grape, wang2024preference}. These include both CLIP-based~\citep{radford2021learning} and LVLM-based~\citep{wang2023visionllmlargelanguagemodel, chameleonteam2024chameleonmixedmodalearlyfusionfoundation, xie2024showosingletransformerunify} models. CLIP-based models (such as HPS-v2.1~\citep{wu2023human} and PickScore-v1~\citep{kirstain2023pick}) provide reliable evaluations through contrastive training, though their evaluation processes often lack transparency. In contrast, LVLM-based judges use prompting techniques and human preference data to give more transparent, flexible feedback~\citep{chen2024mllm, he2024videoscore, wang2024interpretablepreferencesmultiobjectivereward}, though they require more computational resources. These models are widely used in text-to-image~\citep{wallace2024diffusion, chen2024mjbenchmultimodalrewardmodel, yuan2024instructvideo} and image-to-text tasks~\citep{zhou2024calibrated, chen2024halc, cui2024fine}. However, their application to video remains limited, as maintaining temporal coherence adds complexity. While some studies have started investigating video-to-text generation feedback~\citep{escontrela2024video, he2024videoscore,chen2024safewatch}, fewer have explored reward models for text-to-video generation and evaluating their capabilities~\citep{he2024mantisscore, yuan2024instructvideo}, especially on fine-grained video reward judgment.

% Multimodal judges, essential for evaluating modality alignment~\citep{ziegler2019fine}, encompass both CLIP-based~\citep{radford2021learning} reward models and MLLM-based judges. CLIP-based models (e.g. HPS-v2.1~\citep{wu2023human}, PickScore-v1~\citep{kirstain2023pick}) provide robust feedback derived from contrastive training but often lack transparency in their evaluation processes. In contrast, MLLM-based judges leverage prompting techniques and human preference data to offer more transparent and adaptable feedback~\citep{chen2024mllm}, though at the cost of increased computational demands. These models have been extensively applied to text-to-image~\citep{wallace2024diffusion, chen2024mjbenchmultimodalrewardmodel} and image-to-text~\citep{zhou2024calibrated, chen2024halc} tasks. Despite their success in image-based domains, their application in the video domain remains limited, where the challenge of maintaining temporal coherence adds complexity. While some studies have explored feedback for video-to-text generation~\citep{escontrela2024video}, few have addressed feedback mechanisms for text-to-video generation~\citep{he2024mantisscore, yuan2024instructvideo}.

\textbf{Reward Model for Text-to-Video Generation.} \citet{dai2024safesorasafetyalignmenttext2video} introduced a preference dataset for text-to-video generation, but their approach does not involve developing a reward model for practical use. 
Similarly, \citet{yuan2024instructvideo} repurposed a CLIP-based model to provide a scalar reward, though their method suffers from a lack of transparency in the evaluation process. 
\citet{he2024mantisscore} also made initial attempts with a CLIP-based solution, but it is constrained by limited transparency and a relatively small preference dataset.
A concurrent work \citep{xu2024visionreward} considers fine-grained dimensions in video generation and fine-tuning a reward model based on MLLMs. However, they mainly rely on pointwise QA data and simply employ a simple regression layer to aggregate these fine-grained features to fit general human preferences, which falls short of addressing the complex, multi-dimensional nature of video preferences.
%
In contrast, we introduce a fine-grained video preference dataset, \datasetname, which can be used to comprehensively evaluate the video reward models. Building upon this dataset, we further propose \algname, a MoE-based video reward model, aiming to provide more transparent preference judgments through fine-grained scores and provide aspect-specific evaluations. 