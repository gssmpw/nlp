\section{\datasetname\ Benchmark}\label{sec:dataset}
In this section, we introduce \datasetname, a comprehensive video preference benchmark that incorporates fine-grained annotations through a multidimensional analysis of preference judgments. Building on insights from MJ-Bench~\citep{chen2024mjbenchmultimodalrewardmodel}, which focuses on text-to-image generation, we examine user expectations across common video generation scenarios. As illustrated in Figure~\ref{fig:overview}, our analysis identifies five key benchmarking aspects: (1) \textit{Alignment}, (2) \textit{Safety}, (3) \textit{Fineness}, (4) \textit{Coherence \& Consistency}, and (5) \textit{Bias \& Fairness}. To enable more granular assessments and facilitate interpretable evaluations, we further introduce 28 fine-grained evaluation criteria. Below, we first provide an overview of evaluation aspect objectives and then outline the benchmark curation process.

\subsection{Overview of Evaluation Aspect Objectives}
\noindent \textbf{Alignment.} Alignment assesses how accurately the generated videos follow the given instructions, including the presence of specified objects and the correctness of attributes like color and shape.

\noindent \textbf{Safety.} Safety focuses on detecting inappropriate content, including illegal activities, disturbing or offensive material, politically sensitive topics, and other unsuitable elements.

\noindent \textbf{Fineness.} This evaluation focuses on the level of detail and refinement in the video's visual presentation. A high degree of fineness is characterized by sharpness, clarity, and well-preserved textures, with minimal artifacts such as blurring or pixelation. Additionally, smooth transitions, appropriate lighting, and natural color representation contribute to a visually polished and high-quality appearance.

\noindent \textbf{Coherence and Consistency (C \& C).} Coherence and Consistency evaluation examines the internal coherence of the video content. It includes an evaluation of the stability of spatial relationships, continuity of actions, and the consistent appearance of objects, backgrounds, and other visual elements throughout the video.

\noindent \textbf{Bias and Fairness (B \& F).} We assess the videos to ensure they are free from potential biases, particularly in the representation of different racial, gender, and age groups. 
% \end{itemize}











% offering detailed and interpretable explanations behind the decisions. 

% designed to address limitations in existing datasets. 

% Current video preference datasets~\citep{dai2024safesorasafetyalignmenttext2video} often lack detailed explanations for preference annotations, and even when explanations are provided, the annotations tend to be coarse-grained. To overcome these shortcomings, 




\begin{figure}[tb]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/datapipeline.pdf}
    \vspace{-0.5em}
    \caption{\datasetname\ curation process consists of three stages: data collection, data filtering, and data annotation.}
    \label{fig:datapipeline}
    \vspace{-1.5em}
\end{figure}


% \subsection{Dataset Construction}\label{sec:construction}
% In thi
\vspace{-0.3em}
\subsection{Benchmark Curation}
\vspace{-0.3em}
The \datasetname\ benchmark curation process comprises three stages: data collection, filtering, and annotation. Figure~\ref{fig:datapipeline} provides an overview of this process, with additional details in Appendix~\ref{apd:description_category}.
\vspace{-0.3em}
\subsubsection{Data Collection}
\vspace{-0.3em}
In the data collection stage, we employ three main strategies to collect video pairs and their corresponding prompts for video generation:
\begin{itemize}[leftmargin=*]
\vspace{-0.5em}
    \item \textbf{Existing Video Preferences.} We collect video preference pairs and corresponding prompts from Safesora~\citep{dai2024safesorasafetyalignmenttext2video}, which capture human preferences for text-to-video generation tasks in terms of helpfulness and harmlessness.
    \vspace{-1em}
    \item \textbf{Generating Video Preference Pairs from Image Preference Pairs (I2V).} In the I2V strategy, we first select image preference pairs and corresponding prompts from two image preference datasets with fine-grained annotations: MJ-BENCH~\citep{chen2024mjbenchmultimodalrewardmodel} and HPDv2~\citep{wu2023human}. These image pairs are then converted into video pairs using Stable Video Diffusion~\citep{blattmann2023stablevideodiffusionscaling}. Next, the videos generated from the preferred images, along with the original prompts, are provided to ChatGPT to regenerate prompts tailored to the video pairs. This process ensures that the generated videos remain well-aligned with their prompts.
    \vspace{-0.5em}
    \item \textbf{Directly Generating Video Preference Pairs from Text Prompts (T2V).} In the T2V strategy, we collect text prompts from OpenVid~\cite{nan2024openvid}, VidProM~\cite{wang2024vidprom}, and VidGen~\cite{tan2024vidgen}. These prompts are then used to generate video pairs via Open-Sora~\cite{opensora}, VADER~\cite{prabhudesai2024video}, Text-Video Diffusion~\cite{wang2023modelscopetexttovideotechnicalreport}, and InstructVideo~\cite{yuan2023instructvideoinstructingvideodiffusion}.
    \vspace{-1em}
\end{itemize}
Using the three strategies above, we collected a total of 42,809 video pairs and 34,157 prompts, comprising 20,000 videos and 10,000 prompts from existing video preference dataset, 31,010 videos and 15,505 prompts from the I2V strategy, and 34,608 videos and 8,652 prompts from the T2V strategy. The detailed data distribution is presented in Table~\ref{table:data_distribution} in Appendix. By integrating these diverse sources and processing pipelines, we ensure that the curated dataset is both robust and comprehensive.








% for handl a wide range of scenarios in video generation tasks.


    % as a high-quality, pre-existing data source. This dataset has been carefully curated and can be directly utilized for further annotation and training.

% collect a broad range of data reflecting diverse prompts and videos, aiming to ensure comprehensive coverage of possible scenarios in video generation. The data is sourced from three primary categories:  


   
% (2). 










\subsubsection{Data Filtering}

After collecting the video preference pairs, we apply further filtering to remove invalid pairs, leveraging both GPT-4 and human evaluation. First, we use GPT-4 to filter out data where the videos are entirely inconsistent with the prompts. Next, we prompt GPT-4, InternVL2-26B~\citep{chen2023internvl}, and CogVLM2~\citep{hong2024cogvlm2} to score the videos across five aspects, using a scale from 1 to 10. A video preference pair is discarded if at least one video receives a score below 5 in all five aspects. Additionally, if both videos in a pair receive identical scores across all aspects, the pair is also filtered out. After the automated filtering step, human experts conduct a final review to remove video pairs of extremely poor quality and those that are overly similar.

Ultimately, \datasetname\ comprises 5,421 data entries, including 10,842 videos and 5,421 prompts. Of these, 1,496 entries are sourced from existing video preference dataset, 1,910 entries are from image-to-video conversion, and 2,015 entries are from text-to-video generation.



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figure/mj2-pipeline.drawio.pdf}
    \caption{The structure of \algname which builds upon a VideoLLM and consists of two stacked MoE layers. The first MoE layer is for aspect routing and the second one is for scoring each fine-grained criteria. An overall score is also offered by weighting those scores. }
    \label{fig:MoE}
    \vspace{-1em}
\end{figure*}



% , 

% To improve the quality of the dataset, we use GPT4-o~\citep{openai2024gpt4technicalreport} and human experts to filter the collected data. 





\subsubsection{Data Annotation}
After filtering the raw data, human annotators label the dataset using the annotation tool described in Appendix~\ref{apd:UI}. Each annotation involves evaluating a prompt with its corresponding video pair. The annotation rubric consists of detailed scores across 28 criteria within five aspects, along with human preference assessments. Each video pair receives a total of 72 annotations.  

The annotation process follows these steps: First, annotators carefully review the prompt and video pairs. For each aspect, they assign scores (``good", ``average", ``bad") at the aspect level before providing an overall aspect score. This results in 303,576 criteria scores and 54,210 aspect scores across the dataset. Next, they determine the preference per aspect by selecting \textit{``video 1'', ``video 2'', or ``same,''} contributing to 27,105 aspect preference results. Finally, after completing all evaluations, they select an overall preference for the video pair, leading to 5,421 overall preference results.  





\section{\algname Reward Model}
\label{sec:alg}

Currently, RLHF or RLAIF for video generation models heavily rely on vision reward models to score sampled frames (i.e., image)~\citep{Prabhudesai_Mendonca_Qin_Fragkiadaki_Pathak, Yuan_Zhang_Wang_Wei_Feng_Pan_Zhang_Liu_Albanie_Ni_2023}. 
This approach only captures information related to an overall assessment of text-video alignment, and thereby is unable to provide effective feedback on other important aspects in video generation such as consistency, bias, and safety. 
% Meanwhile, there is a growing need not only for an overall judgment of video preference but also for detailed explanations behind these preference judgments.
To address this issue, build upon \datasetname, we develop a mixture-of-expert (MoE) based video reward model, \algname, aiming to deliver highly accurate video preference judgment across diverse assessment criteria.
% while also offer detailed explanations behind the preference judgment.

% \subsection{Simplifying Judgment with MoE}
% Judging video preference is a highly complex task that requires evaluating multiple factors, including video generation quality, user preferences, video safety, and logical coherence. 
% These diverse aspects make it challenging for VideoLLMs to provide accurate assessments directly. 
% To address this, we employ a MoE architecture to handle assessment for different aspects. Specifically, the judgment is made in two stages: expert activation and expert preference judgment.

% \paragraph{Expert Activation.}
% Different types of video preference data emphasize different areas; for example, prompts with action-related descriptions require a focus on action continuity in the video, while issues like safety and bias may be less relevant. 
% We use the Overall subset from \datasetname to train the router in this stage. 
% This subset not only provides video preference results but also indicates the primary areas of focus for each sample, forming the basis for router training. Once trained, the router, upon receiving a video pair and prompt, determines which experts should be activated for that specific data, preventing unrelated experts from influencing the final judgment.


% \paragraph{Expert Preference Judgment.}
% Activated experts make judgments on video preference within their respective domains. Specifically, we categorize common issues in video generation and user preferences into five areas—\textit{Alignment, Quality, Bias and Fairness, Safety, Coherence and Consistency}—corresponding to the five subsets of \datasetname. Each expert assesses video preference based on subcategories within its domain and then provides a preference judgment for that area. 
% Finally, \algname aggregates the judgments from the activated experts to deliver an overall video preference result.
% To enhance the model's capabilities based on pilot experimental results, we chose InternVL2-2B as the initial for \algname, with video pairs input simultaneously for evaluation. 
% Both the router and individual experts are fine-tuned using LoRA~\cite{hu2021loralowrankadaptationlarge}. 

% \subsection{Judgment Process}

% The judgment process in \algname is also conducted in two stages: router activation and expert preference judgment. 
% First, the router analyzes the prompt and video pair to determine which specific experts should be activated. 
% For each category, the router performs a binary classification, outputting “yes” if the expert for that category is needed and “no” otherwise. Once the appropriate experts are activated, they evaluate the video pair within their respective focus and provide corresponding judgments.

% The overall video preference is derived through two rounds of decision-making. In the \textbf{first round}, each expert counts the number of subcategories where one video is preferred over the other. The video with more subcategory preferences within that domain is marked as preferred in that expert’s field. Subsequently, the video with the majority of domain-level preferences is deemed preferred overall. If both videos receive an equal number of domain-level preferences, the \textbf{second round} is conducted: here, we calculate a weighted preference score for each video based on subcategory preferences. Each domain’s total weight is set to 1, with the weight distributed evenly across \(n\) subcategories, giving each subcategory a weight of \(1/n\). The video with the higher weighted preference score is ultimately preferred.

% \subsection{Open-Ended Explanation Based on CoT}
% To provide a meaningful explanation for video preference judgment, we leverage the CoT approach to prompt \algname to generate the explanation. The explanation process in \algname is structured into two stages.

% In the first stage, we collect the preference results provided by each expert and use a template to compile these results. This template-based response combines subcategory preferences, domain-level preferences, and the final overall preference into a structured format. While this response is rigid and lacks specific details about video content, it serves as a foundational guide, containing essential information to help steer the model toward accurate explanations.

% In the second stage, we design a user prompt that inquires about video preference and pair it with the structured explanation from the first stage. This combination is then presented as conversation history to the VideoLLM. Using the video pair, the prompt, and the guidance from the structured preference information in the history, the VideoLLM generates a more flexible, detailed, and open-ended explanation.



\subsection{Model Architecture}\label{sec:moe_arch}

Judging video preferences is a highly complex task that requires evaluating multiple factors, including video generation quality, safety, and logical coherence. The diversity of these criteria makes it challenging for LVLMs to provide accurate assessments directly. To address this, we propose \algname, a MoE-based architecture designed to assess videos across different aspects. As illustrated in Figure~\ref{fig:MoE}, \algname\ builds upon VideoLLM and incorporates two stacked MoE layers: one for aspect routing and another for fine-grained criteria scoring. The first layer, \textbf{Aspect MoE}, routes each text-video pair to the five aspects defined in our \datasetname. The second layer, \textbf{Criteria MoE}, then assigns fine-grained scores to each criterion. Finally, we aggregate these scores using the aspect routing weights to compute a final preference score. Below, we detail the design of these two MoE layers:

\noindent \textbf{Aspect MoE.}
We utilize InternVL2~\cite{chen2023internvl}, a lightweight 2B VideoLLM, to process and encode the input instruction-video pair, extracting the hidden state \(\mathbf{h}\) of the last token as the feature representation.
Next, we introduce the first layer, Aspect MoE, which routes the input into five predefined aspects using MoE-style scalarization~\cite{armoreward}. 
Specifically, we incorporate an overall gating layer $g$, composed of shallow MLP layers, to generate non-negative weights that sum to 1. This results in the aspect routing weights, computed as: $\textrm{AR} = \text{softmax} (g(\mathbf{h}))$, where $\textrm{AR}\in \mathbb{R}^5$ represents the normalized scores.

\noindent \textbf{Criteria MoE.}
Next, to obtain scores for each fine-grained criterion, we introduce another MoE layer, Criteria MoE $g'$, along with a regression scoring layer $f$ after the VideoLLM. The scoring layer projects the hidden feature $\mathbf{h}$ into 28 criteria scores, while the gating layer identifies the most relevant criteria for the given input instruction-video pair. For criteria associated with the five predefined aspects $\{U_i\}_{i=1}^{5}$, the scores $C[U_i]$ within each aspect are normalized as follows:
\begin{equation}
\small
    C[U_i] = \textrm{softmax}(g'(\mathbf{h})[U_i]) \odot f(\mathbf{h})[U_i],
    \label{eq:criteria_scoring}
\end{equation}
where $U_i$ denotes the indices of the criteria corresponding to aspect $i$. The overall preference score \textrm{OS} is then computed by weighting the criteria scores $C \in \mathbb{R}^{28}$ with the aspect routing scores $\textrm{AR}$ as follows:
\begin{equation}
\small
    \textrm{OS} = \sum_{i=1}^{5} \left[\sum_{t \in U_i} C[t]\right] \textrm{AR}[i]
    .
    \label{eq:overall_preference}
\end{equation}
This overall preference score accounts for five aspects and their corresponding criteria, making it directly applicable to general preference tuning pipelines for enhancing the alignment of video generation.


% \paragraph{Criteria Scoreing.}
% We use InternVL2~\cite{chen2023internvl}, a lightweight 2B VideoLLM, to understand and encode the input text-video pair, obtaining the hidden state \(\mathbf{h}\) of the last token as the extracted feature. A simple regression layer $r$ is then applied to map \(\mathbf{h}\) to the scores for each criterion:
% \begin{equation}  
%    \textrm{CS} = r(\mathbf{h})  
% \end{equation}  
% We obtain \textrm{CS} as the criterion-level score.

% \paragraph{Aspect Scoring.}
% After this, we employ five MoE layers corresponding to the five aspects, where each aspect-specific MoE layer assigns weights to the criterion-level scores under that aspect. This process yields five aspect-layer scores using the MoE-style scalarization~\cite{armoreward}. 
% Specifically, we add a gating layer $g$ formed of shallow MLP layers that outputs non-negative weights and these weights are summed to 1. Also, we have a scoring layer $f$ that can output the aspect scores $f$ to reflect the routing choice. By applying the weights $g(\mathbf{h})$:
% \begin{equation}
%    \textrm{CS} = \text{softmax} (g(\mathbf{h}) \odot f(\mathbf{h}))
%    ,
% \end{equation}
% we obtain the normalized scores \textrm{CS} reflecting the aspect routing.
% Specifically, for the \(i\)th aspect, we introduce a gating layer \(g_i\), composed of shallow MLP layers, which outputs non-negative weights for each criterion. These weights are normalized to sum to 1:  

% \begin{equation}  
%     \textrm{AS}_i = \text{softmax}(g_i(\mathbf{h})) \odot \textrm{CS}_i 
% \end{equation}  
% where $\textrm{AS}_i$ is the aspect-level score for the \(i\)th aspect and  $\textrm{CS}_i$ represents the criterion-level scores under the corresponding aspect.

% \paragraph{Overall Scoring.}
% Finally, we employ a gating layer \( g_o \), composed of shallow MLP layers, to assign non-negative weights to each aspect-level score. These weights are normalized to sum to 1:
% \begin{equation}  
%     \textrm{OS} = \text{softmax}(g_o(\mathbf{h})) \odot \textrm{AS} 
% \end{equation}
% where \textrm{AS} represents the scores of all aspects, and \textrm{OS} is used as the overall preference score.


% \paragraph{Aspect Scoring.} 
% To obtain the fine-grained evaluation scores, similarly, we add another MoE layer $(g', f')$ after the aspect routing layer.  This layer  

% Note that our model is designed to serve in two common scenarios:
% (1) \textbf{Specialized Judge}: in cases where users need domain-specific analysis, such as filtering content based on safety, our model activates a dedicated expert specialized in that aspect (e.g., a safety expert).
% (2) \textbf{Comprehensive Judge}: for general use cases, given any text-video pair, our model evaluates and provides scores across five predefined aspects.
% To achieve this, we employ a language model as a router. It would automatically select and activate the appropriate experts based on the inputs, ensuring accurate and relevant judgments.
% Additionally, \algname can also aggregate the judgments from the activated experts to offer an overall preference judgment, similar to existing video reward models.

% stage1: criteria scoring mse
% stage2: criteria scoring * lambda + sum(C_ui), ground truth mse + BT ranking aspect sum(C_ui)
% stage3: (stage1+stage2) * lambda + overall score BT loss

\subsection{Multi-Stage Training}\label{sec:moe_training}
We employ a three-stage training strategy to fine-tune the VideoLLM along with the newly introduced MoE parameters. Specifically, the first stage is to train the Criteria MoE layer to predict the annotated fine-grained criteria scores. The second stage is to leverage aspect ranking information from preference pairs to train the Aspect MoE layer. In the final stage, we integrate the previous training steps and introduce an overall preference ranking loss to jointly optimize both the aspect MoE layer and the criteria MoE layer. We detail the three-stage training as follows:

\noindent \textbf{Stage I: Criteria Scoring Training.}
We use the fine-grained annotated criteria scores $s\in \mathbb{R}^{28}$ as labels to train the Criteria MoE layer, ensuring accurate judgment:
\begin{equation}
    \mathcal{L}_1 = \mathbb{E}_{\mathcal{D}} \left [\sum_{i=1}^{5}\sum_{t \in U_i} (C[t] - s[t])^2\right]
    ,
    \label{eq:stage_1}
\end{equation}
where $\mathcal{D}$ represents the training dataset. After training, \algname\ is expected to generate accurate scores for the fine-grained criteria.

\noindent \textbf{Stage II: Aspect Routing Training.}
Next, we leverage the annotated aspect ranking information from video preference pairs to train the Aspect MoE. The ranking information for each aspect reflects preference between two generated videos $(y_w, y_l)$, given the same instruction $x$ and its associated criteria. To optimize this, we apply a ranking loss:

\begin{equation}
    \mathcal{L}_2 = \mathbb{E}_{\mathcal{D}} \sum_{i=1}^{5}  \log \sigma(
          \mathbb{I}_{i} (\sum C[U_i]_{y_w} - \sum C[U_i]_{y_l})
    )
    ,
\end{equation}
where \( \mathbb{I}_{i} \) is 1 if \( y_w \) is preferred over \( y_l \) in the \( i \)th aspect, and -1 otherwise. The term \( \sum C[U_i] \) from Eq.~\eqref{eq:criteria_scoring} represents the summed criteria scores within the \( i \)th aspect. Additionally, to prevent interference with criteria score predictions, we continue optimizing \( L_1 \) from Eq.~\eqref{eq:stage_1} concurrently.

\noindent \textbf{Stage III: Joint Training.}
Finally, to ensure the overall preference score is meaningful, we incorporate the overall ranking \( (x, y_w, y_l) \), where \( y_w \) is generally preferred over \( y_l \), to jointly train both MoE layers as follows:
\begin{equation}
    \mathcal{L}_3 = \mathbb{E}_{\mathcal{D}} \left[ \log \sigma (\textrm{OS}_{y_w} - \textrm{OS}_{y_l})\right]
    ,
\end{equation}
where the overall preference score \(\textrm{OS}\) is computed using Eq.~\eqref{eq:overall_preference}. Additionally, we incorporate the losses \(\mathcal{L}_1\) and \(\mathcal{L}_2\) into the third-stage training and introduce a hyperparameter \( \lambda \) to balance their impact.

% We leverage the inherent multidimensional assessment of the  \datasetname to train our \algname model.
% First, we construct an \textit{overall} subset as the train set based on the further selection of the five subsets of \datasetname. 
% Specifically, this subset includes sample if (1) the video pair appears in multiple subsets, (2) it has the same preferred video in at least two of these subsets, (3) it has no conflicting video preference judgment in any subsets, 
% This selection can be expressed as follows:
% \begin{equation}
% \begin{aligned}
% \mathcal{D}_{\text{overall}} = \left\{
% (V_{p}^{i}, V_{d}^{i}) \mid 
% \exists k_1, k_2 \in [1, 5], \, k_1 \ne k_2, \, \right. \\
% \left. (V_{p}^{i}, V_{d}^{i}) \in \mathcal{D}_{k_1} \cap \mathcal{D}_{k_2}, \, 
% \forall c \in \mathcal{C}, \, A_{p}^{i,c} \ge A_{d}^{i,c}
% \right\}_{i=1}^{n}
% \end{aligned}
% \end{equation}
% After selecting the train set, we fine-tune the router and individual experts of \algname separately.

% \noindent \textbf{Training Router.} For the router, we utilize a standard language modeling task, where it is fine-tuned to predict which experts should be activated based on the given text-video pair.
% According to the selection process of the overall subset, the expert should only be activated when the preferred video has advantage than the dispreferred video in that expert's aspect.
% For example, in a video pair, if one video is preferred due to its better assessment in alignment and safety, the prediction in a JSON format for the router would be \{``Alignment'': ``yes'', ``Safety'': ``yes'', ``Quality'': ``no'', ``Coherence and Consistency'': ``no'', ``Bias and Fairness'': ``no''\}. 
% Here, ``yes'' indicates that the expert in that particular aspect needs to be activated, while ``no'' means the expert in that aspect does not need to be activated.

% \noindent \textbf{Training Expert.}
% Each individual expert is responsible for offer precise judgment within a specific aspect. Specifically, we use the data from the corresponding subset of \datasetname to train each expert.
% Recall that the annotations for each sample are fine-grained, which means there does not exit an overall score can be leveraged for a regression task. Instead, the sample has preferred labels across multiple dimensions.
% To effectively utilize these fine-grained annotations, we build multiple prediction heads upon the expert. Each head $f_\theta$ is training as a Bradley-Terry model~\citep{bradley1952rank} as follows:
% \begin{equation}
% \max_\theta \, \mathbb{E}_{\mathcal{D}_k} \, \sigma (f_\theta(V_p) - f_\theta(V_d))
% ,
% \end{equation}
% where $\sigma$ is the sigmoid function, $V_p$ and $V_d$ are preferred and dispreferred videos, respectively.
% After training, we can simply average these prediction results from each head to obtain a score to represent the preference in that aspect, since the preferred video has advantages on all fine-grained criteria over the dispreferred video.

% \subsection{Judging}
% Recall that in Sec.~\ref{sec:moe_arch}, we categorize common use cases of video reward model into two classes: specialized Judge and comprehensive judge.
% For a specialized judge, we simply average these prediction results from each head of the selected expert to obtain a score to represent the preference in that aspect.
% For a comprehensive judge, we follow the regular MoE inference process to conduct the judgment. In details, the router would first predict the experts that need to be activated , based on the input text-video pair.  Then, the activated experts would independently offer the assessment scores. Finally, we can average these scores to reach a comprehensive judgment.

% The judgment process in \algname is conducted in two stages: router activation and expert preference judgment.
% In the first stage, the router analyzes the prompt and video pair to determine which experts should be activated. For each subset, the router performs a binary classification, generating ``yes'' if the expert for that subset is needed and ``no'' otherwise.
% In the second stage, the activated experts evaluate the video pair within their respective criterion and generate corresponding preference judgments.

% Specifically, each activated expert first counts the number of criterion that one video is preferred over the other one. The video with more preferred criterion is judged as the preferred one, while the other is as the dispreferred.
% If these two videos have the same number of preferred criterion, we use a weighted scoring method to decide the ranking.
% BELOW NEEDS TO REVISE.
% % If both videos receive an equal number of aspect-level preferences, the \textbf{second round} is conducted: here, we calculate a weighted preference score for each video based on criteria preferences. Each aspect’s total weight is set to 1, with the weight distributed evenly across \(n\) criteria, giving each criterion a weight of \(1/n\). The video with the higher weighted preference score is ultimately preferred.
% If both videos receive an equal number of aspect-level preferences, a \textbf{second round} is conducted to break the tie. In this round, we calculate a weighted preference score for each video based on specific criteria preferences. Each aspect is assigned a total weight of 1, which is then distributed evenly across \(n\) criteria, resulting in a weight of \(1/n\) for each criterion. To compute the weighted preference score for a video, we sum the weights of all criteria for which that video was preferred. The video with the higher weighted preference score is ultimately chosen.

% This revision emphasizes the calculation method for the weighted preference score, making it clear that it involves summing the weights of the criteria where the video was preferred.

% \paragraph{CoT based Explanation.}
% As aforementioned, our \algname can use the CoT to provide the detailed explanations for the preference judgment.  

% In the first stage, we collect the preference results provided by each expert and combine these results using a template. This template-based response combines criteria preferences, aspect-level preferences, and the final preference result into a structured format. While this response is rigid and lacks specific details about video content, it serves as a foundational guide, containing essential information to help steer the model toward accurate explanations.

% In the second stage, we design a user prompt that inquires about video preference and pair it with the structured explanation from the first stage. This combination is then presented as conversation history to the VideoLLM. Using the video pair, the prompt, and the guidance from the structured preference information in the history, the VideoLLM generates a more flexible, detailed, and open-ended explanation.


% \begin{figure}[!hb]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figure/cot.png}
%     \caption{In \algname, the router first determines which experts should be activated to evaluate the input data points. The selected experts are then activated to provide detailed assessments within their respective subsets. Ultimately, \algname aggregates these assessments and produces a formatted output, delivering the final preference result.}
%     \label{fig:MoE}
% \end{figure}