\section{Conclusion}
In this paper, we introduce \datasetname, a large-scale benchmark for evaluating video generation across five key aspects with 28 fine-grained criteria, addressing limitations in the existing video reward model evaluation. Building on this, we propose \algname, a Mixture-of-Experts (MoE)-based reward model that decomposes video assessments into specialized expert evaluations, enhancing precision and adaptability. Experimental results show that \algname outperforms existing models, highlighting the benefits of fine-grained, multi-aspect judgment. Together, \datasetname and \algname provide a robust framework for improving video generation alignment, offering a foundation for future advancements in reward modeling.



% we first propose \datasetname with five subsets and 28 assessment criteria, a large-scale high-quality video preference dataset, designed to facilitate fine-grained video preference evaluation.
% Building upon this dataset, we utilize the MoE architecture to develop a video reward model \algname to offer more transparent preference judgment through evaluations from various aspects.
% Additionally, this reward model can be seamlessly integrated into two common scenarios of video reward models:  specialized judge and comprehensive judge, thanks to its plug-and-play router module.
% Extensive experimental results on benchmarking existing video reward models demonstrate the limitations of video preference judgment, underscoring the effectiveness and superiority of the proposed method. 
% We believe that \datasetname and \algname can serve together as a valuable testbed for advancing text-to-video generation.




% Leveraging a MoE architecture, \algname effectively combines a router and specialized experts to assess video preferences across multiple aspects, including alignment, quality, safety, bias, and consistency.
% Together, \algname and \datasetname aim to support the development of more robust and interpretable video evaluation frameworks.