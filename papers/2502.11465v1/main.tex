\documentclass[twocolumn]{article}

% Some useful LaTeX packages
\usepackage[margin=0.95in]{geometry}
\usepackage{newtxtext}      % selects Times Roman as basic font
\usepackage{lmodern}        % keeps maths in default font
\usepackage{microtype}      % better typesetting
\usepackage{graphicx}       % for figures
\usepackage{hyperref}       % references
\usepackage{url}
\usepackage{caption}        % figure captions
\usepackage{subcaption}
\usepackage{booktabs}       % for professional tables
\usepackage{makecell}
\usepackage{multirow}
\usepackage{natbib}         % bibliography
\usepackage{parskip}        % start paragraphs on new line

% For abstract formatting
\usepackage{abstract}
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % keeps "Abstract" label bold
\renewcommand{\abstracttextfont}{\normalfont} % keeps abstract text in normal font

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% For definitions
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

% For thanks number
\makeatletter
\let\@fnsymbol\@arabic
\makeatother


\title{All Models Are Miscalibrated, But Some Less So:\\ Comparing Calibration with Conditional Mean Operators}
\date{}
\author{Peter Moskvichev\thanks{School of Computer and Mathematical Sciences, The University of Adelaide, Australia. Correspondence to: Peter Moskvichev
\texttt{peter.moskvichev@adelaide.edu.au} \\ \\ Preprint. Under review.} \and Dino Sejdinovic\footnotemark[1]}




\begin{document}

\maketitle


\begin{abstract}
    When working in a high-risk setting, having well calibrated probabilistic predictive models is a crucial requirement. However, estimators for calibration error are not always able to correctly distinguish which model is better calibrated. We propose the \emph{conditional kernel calibration error} (CKCE) which is based on the Hilbert-Schmidt norm of the difference between conditional mean operators. By working directly with the definition of strong calibration as the distance between conditional distributions, which we represent by their embeddings in reproducing kernel Hilbert spaces, the CKCE is less sensitive to the marginal distribution of predictive models. This makes it more effective for relative comparisons than previously proposed calibration metrics. Our experiments, using both synthetic and real data, show that CKCE provides a more consistent ranking of models by their calibration error and is more robust against distribution shift. 
\end{abstract}

\section{Introduction} \label{sec:intro}


Probabilistic models quantify uncertainty by predicting a probability distribution over possible labels, rather than simply giving a point estimate. This allows models to express their confidence in a prediction. However, to ensure that the probability outputs can be trusted by users, such models must satisfy certain calibration (or reliability) properties. Roughly speaking, calibration requires that predicted probabilities match observed frequencies of labels. A classical example is in weather forecasting. A forecaster that assigns 70\% chance of rain is well calibrated if, on average, it actually rains on 70\% of days with such predictions. Although calibration was originally studied in a meteorological context \citep{Brier_1950, DeGroot_1983}, it has proven to be useful in a wide range of applications. 

The viewpoint we take here is agnostic to whether the predictive model is Bayesian or frequentist and this should not in any way interfere with the essentially frequentist notion of calibration which we embrace. This is in the spirit of \citet{Little_2006}: ``\textit{inferences under a particular model should be Bayesian, but model assessment can and should involve frequentist ideas.}"

Work by \citet{Guo_2017} cast a spotlight on the miscalibration of deep neural networks, which often give overconfident predictions. More recently, \citet{phan2025hle} showed similarly poor calibration of state-of-the-art large language models. This gives reason to be concerned about the use of advanced AI models in high-risk settings. However, properly measuring calibration remains a challenge, with most authors and practitioners resorting to proxy metrics that do not fully capture the notion of \emph{strong calibration} explained in Section \ref{sec:background}. 

Reliability diagrams \citep{DeGroot_1983}, such as in Figure \ref{fig:reliability-diag}, offer a method of inspecting calibration for binary classifiers by comparing the model's confidence in a positive result and its accuracy. For a well calibrated model, the confidence in a prediction should match the accuracy. While reliability diagrams provide a useful visual tool, they fail to quantify calibration error, and are difficult to adapt to multiclass and regression problems. A popular calibration metric inspired by reliability diagrams is the Expected Calibration Error (ECE) \citep{naeini_2015} which measures the expected difference between the predictive confidence and observed frequencies of labels. But estimating ECE can be problematic as it is highly sensitive to the chosen binning scheme which leads to biased and inconsistent estimators \citep{Nixon_2019, vaicenavicius_2019, Widmann_2019}. 

% Figure: Reliability diagram
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Figs/reliability_diagram_binary.pdf}
    \caption{Reliability diagram for a binary classification problem comparing three probabilistic models along with the estimate of their conditional kernel calibration error (CKCE) where a lower value indicates better calibration.}
    \label{fig:reliability-diag}
\end{figure}


An alternative approach to calibration is via finding discrepancies between probability distributions, which can be represented by their kernel mean embeddings in a reproducing Hilbert space (RKHS). This idea was introduced by \citet{Widmann_2021}, who measured calibration by the difference in kernel embeddings of joint distributions of predictive distributions and labels, in order to construct a hypothesis test for calibration. While this is an effective tool for the purposes of testing, to which much work has been devoted \citep{Widmann_2021, Lee_2023, Glaser_2023, Chatterjee_2024}, in practice these tests may find that all sufficiently sophisticated models depart from perfect calibration. This motivates the idea of ranking a set of candidate models by their calibration error, similar to how models may be benchmarked by their accuracy or other metrics. However, as we demonstrate here, the calibration measure introduced by \citet{Widmann_2021}, as well as the classical ECE, both suffer from being impacted by the marginal distribution of the predictive distributions in a given model and can thus be ill-suited for comparing different candidate models. 

In this paper, we introduce the \emph{conditional kernel calibration error} (CKCE). It evaluates calibration by determining the discrepancy between conditional distributions which we represent by their RKHS conditional mean operator. This can be viewed as complementary to the work of \citet{Widmann_2021}. It might be challenging to use CKCE as a test statistic for a calibration test because of the difficult to estimate null distribution, but we are interested in a different problem: reliable relative calibration comparisons between models. We demonstrate through a range of experiments on both synthetic and real data that the CKCE is better suited for this task. 



\section{Background} \label{sec:background}

\subsection{Probabilistic models and Calibration}

We begin with a description of probabilistic classifiers, followed by an extension to a more general setting. Suppose we aim to predict a label $y \in \mathcal{Y} = \{1, \dots, m\}$ based on a feature $x \in \mathcal{X}$. Rather than making a point prediction, we fit a probabilistic predictive model $Q$ which estimates the true conditional distribution $P(Y|X=x) \in \Delta^{m-1}$, where $\Delta^{m-1}$ represents the $(m-1)$-dimensional probability simplex. Thus, the model induces a probabilistic classifier $f: \mathcal{X} \to \Delta^{m-1}$, which maps a feature $x$ to the predicted conditional distribution of $Y$ given $X=x$ under $Q$, that is, $f(x) = Q(Y | X=x)$.

A desirable property for the classifier is that the confidence in the predicted class matches the proportion of times the prediction is correct for that confidence. This is known as \emph{confidence calibration} and can be expressed as
\begin{equation} \label{eq:conf_calib}
    \mathbb{P}(Y = \arg \max f(X) | \max f(X) = c) = c
\end{equation}
for all $c \in [0,1]$ \citep{Guo_2017, vaicenavicius_2019}. However, (\ref{eq:conf_calib}) does not consider calibration of \emph{all classes}, which may be crucial if the entire predicted probability vector is used for decision making or as an intermediate output for downstream tasks, particularly when certain classes carry varying levels of risk for those tasks. Therefore, our focus is on the notion of so called \emph{strong calibration} \citep{Widmann_2019}.
\begin{definition} \label{def:calibration_class}
    A probabilistic classifier $f: \mathcal{X} \to \Delta^{m-1}$ is \emph{strongly calibrated} if 
    \begin{equation}
        \mathbb{P}(Y=i | f(X) = q) = q^{(i)} \qquad \text{for } i=1,\dots m
    \end{equation}
    for all $q = (q^{(1)}, \dots, q^{(m)}) \in f(\mathcal{X}) \subseteq \Delta^{m-1}$.
\end{definition}
We note that calibration is a property of a model that is essentially orthogonal to that of accuracy. Indeed, a predictive model which for any input simply returns the marginal distribution of class labels, $Q(Y|X)=P(Y)$, is calibrated according to Definition \ref{def:calibration_class}, but has accuracy at a chance level. Conversely, a classifier may attain the optimal (Bayes) risk with respect to the 0/1 loss (i.e. the highest possible accuracy) but have arbitrarily bad calibration. An example would be a classifier that assigns all probability mass in $Q(Y|X)$ to the label $\hat Y=\arg\max_{i=1,\ldots,k}P(Y=i|X)$.

Now consider a more general setting where labels $y$ belong to an arbitrary domain $\mathcal{Y}$, and denote by $\mathcal{P}$ the space of Borel probability measures over $\mathcal{Y}$. We can once again fit a probabilistic model $Q$, and define $Q_X = Q(Y|X)$ as a random variable taking a value in $\mathcal{P}$. Thus, the predictive model induces a joint distribution $P(Y, Q_X)$ on the product space $\mathcal{Y} \times \mathcal{P}$, which allows us to generalise the notion of strong calibration. 
\begin{definition} \label{def:calibration}
    A probabilistic predictive model $Q_X = Q(Y|X)$ of the true conditional probability $P(Y|X)$ is \emph{strongly calibrated} if 
    \begin{equation} \label{eq:calibration}
        P(Y|Q_X) = Q_X
    \end{equation}
    almost surely $P(X)$.
\end{definition}
The discrepancy between the left and right hand sides of \eqref{eq:calibration} in Definition \ref{def:calibration} can be understood as the \emph{calibration error} of probabilistic model $Q$. Thus, evaluating calibration is ultimately a problem of measuring the discrepancy between two probability distributions. However, while $Q_X$ is readily available from the model output, $P(Y|Q_X)$ can be highly complex, making it difficult to estimate. This motivates the use of nonparametric methods which allow to represent probability distributions and distances between them using only samples from the distributions. 



\subsection{Kernel Embeddings of Distributions} \label{sec:kernel_emb}

We provide a short review of some preliminaries on kernel embeddings of probability distributions in a reproducing kernel Hilbert space (RKHS) \citep{muandetKernelMeanEmbedding2017a}. Consider a random variable $X$ with domain $\mathcal{X}$ and distribution $P(X)$. Let $k:\mathcal{X} \times \mathcal{X} \to \mathbb{R}$ be any positive definite function with associated RKHS $\mathcal{H}_k$, which is a Hilbert space of functions $f:\mathcal{X} \to \mathbb{R}$ satisfying
\begin{enumerate}
    \item For all $x \in \mathcal{X}$, $k(x, \cdot) \in \mathcal{H}_k$, and
    \item For all $x \in \mathcal{X}$ and $f \in \mathcal{H}_k$, $\langle f, k(x, \cdot) \rangle_{\mathcal{H}_k} = f(x)$,
\end{enumerate}
where 2 is known as the reproducing property. We can think of $k$ as the inner product between feature embeddings, that is, $k(x, x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}_k}$ where $\phi(x) = k(x, \cdot)$ is the canonical feature map. 

Assuming $k$ is bounded, the \emph{kernel mean embedding} of $P(X)$ is given by
\begin{equation*}
    \mu_{X} = \mathbb{E}_X k(X, \cdot) = \mathbb{E}_X \phi(X) \in \mathcal{H}_k,
\end{equation*}
and has the property $\langle \mu_{X}, f \rangle_{\mathcal{H}_k} = \mathbb{E}_X f(X)$ for all $f \in \mathcal{H}_k$. Given another random variable $Y$ whose distribution has kernel embedding $\mu_{Y}$, we can compute the distance between $P(X)$ and $P(Y)$ using the \emph{maximum mean discrepancy} (MMD) given by
\begin{align*}
    \text{MMD}_k^2(P(X), P(Y)) &= \| \mu_{X} - \mu_{Y} \|_{\mathcal{H}_k}^2 \\
    &= \mathbb{E}_{X, X'} k(X, X') + \mathbb{E}_{Y, Y'} k(Y, Y') \\ & \qquad -2\mathbb{E}_{X, Y} k(X, Y), 
\end{align*}
which can be easily estimated using samples from $P(X)$ and $P(Y)$ \cite{Gretton_2012}. For so called \emph{characteristic} kernels, the mean embeddings are injective and the MMD equals zero if and only if the distributions $P(X)$ and $P(Y)$ are equal \citep{Fukumizu_2007}. Many commonly used kernels such as the Gaussian and Laplace kernels are characteristic. A closely related notion is a \emph{universal} kernel, which when defined on a compact metric space $\mathcal{X}$, has an associated RKHS $\mathcal{H}$ that is dense in the space of continuous functions on $\mathcal{X}$ \citep{sriperumbudurUniversalityCharacteristicKernels2011}.

We can similarly define the kernel embedding of a joint distribution $P(X, Y)$ by choosing a kernel on the product domain $\mathcal{X} \times \mathcal{Y}$. Given kernels $k$ and $\ell$ on $\mathcal{X}$ and $\mathcal{Y}$ respectively, with associated RKHS $\mathcal{H}_k$ and $\mathcal{H}_\ell$, and defining the tensor product kernel $(k \otimes \ell)((x,y),(x',y')) = k(x,x')\ell(y,y')$, a special case for the joint mean embedding is given by
\begin{equation*}
    \mu_{X Y} = \mathbb{E}_{XY} [\phi(X) \otimes \psi(Y)] \in \mathcal{H}_{k\otimes \ell}
\end{equation*}
where $\phi(x) = k(x, \cdot)$, $\psi(y) = \ell(y, \cdot)$ are the canonical feature maps \citep{Fukumizu_2004}. By isometry between $\mathcal{H}_{k\otimes \ell}$ and $\mathcal{H}_{k}\otimes \mathcal{H}_\ell$, the joint mean embedding $\mu_{XY}$ can be identified with the uncentered cross-covariance operator $\mathcal{C}_{XY}: \mathcal{H}_\ell \to \mathcal{H}_k$, which has the property $\mathbb E[f(X)g(Y)] = \langle f , \mathcal{C}_{XY}g \rangle_{\mathcal{H}_k}$.


\subsection{Conditional Mean Operators}

Conditional distributions $P(Y|X=x)$ can likewise be embedded in an RKHS by the conditional mean embedding \citep{Song_2009}
\begin{equation*}
    \mu_{Y|X=x} = \mathbb{E}_{Y|X = x} [\psi(Y) | X=x] \in \mathcal{H}_\ell.
\end{equation*}
Note that the conditional mean embedding defines a family of points in the RKHS indexed by the values $x$. This motivates us to consider the \emph{conditional mean operator} (CMO) $\mathcal{C}_{Y|X}:\mathcal H_{k}\to\mathcal H_{\ell}$ which satisfies $\mu_{Y|X=x} = \mathcal{C}_{Y|X}k(x, \cdot)$. Assuming that for all $g \in \mathcal H_{\ell}$, we have $\mathbb{E}_Y[g(Y) | X = \cdot] \in \mathcal H_k$, the CMO is a well defined Hilbert-Schmidt operator.
% \begin{equation} \label{eq:cmo}
%     \mathcal{C}_{Y|X} = \mathcal{C}_{YX} \mathcal{C}_{XX}^{-1} \in \mathcal{H}_k \otimes \mathcal{H}_\ell.
% \end{equation}
Much research has been devoted to put conditional mean embeddings and operators on a rigorous footing, including the measure theoretic view taken by \citet{Park_2020}. Given a dataset $\{(x_i, y_i)\}_{i=1}^n$ sampled from $P(X,Y)$, an empirical estimator of the CMO is 
\begin{equation} \label{eq:cmo_est}
    \widehat{\mathcal{C}}_{Y|X} = \Psi_Y (K_{XX} + \lambda n I_n)^{-1} \Phi_X^*
\end{equation}
where $^*$ indicates the adjoint of an operator, $\Psi_Y = [\psi (y_1), \dots, \psi (y_n)]$, $\Phi_X = [\phi(x_1), \dots, \phi(x_n)]$, $K_{XX} = \Phi_X^* \Phi_X$ is the Gram matrix with entries $[K_{XX}]_{ij} = k(x_i, x_j)$, $I_n$ is the $n\times n$ identity matrix and $\lambda$ is a regularisation parameter \citep{Song_2009}. Assuming that $\mathcal{C}_{YX} \mathcal{C}_{XX}^{-3/2}$ is Hilbert-Schmidt, then $\| \widehat{\mathcal{C}}_{Y|X} - \mathcal{C}_{Y|X} \|_{HS} = O_p(\lambda^{1/2}+\lambda^{-3/2}n^{-1/2})$. Therefore, if the regularisation term satisfies $\lambda \to 0$ and $n \lambda^3 \to \infty$, then $\widehat{\mathcal{C}}_{Y|X}$ converges in probability to the true CMO \citep[Theorem 1]{Song_2010}. A detailed refined analysis of convergence of conditional mean embeddings is studied in \cite{Li_2022}. Additionally, \citep[Supplementary Material, Section 8]{singh_2023} provides a minimax optimal rate for the convergence of CMO in Hilbert-Schmidt norm, which can depend on kernel $k$ and the smoothness of underlying conditional distributions. These results can provide a recipe for selecting the schedule of the regularisation parameter $\lambda$. An alternative method is to view the CMO as the solution to a regression problem, allowing $\lambda$ to be selected via cross-validation \citep{Grunewalder_2012}. For simplicity, we choose the regularisation parameter to have schedule $\lambda_n = n^{-1/4}$.

Consider two random variables $Y$ and $Z$ with common domain $\mathcal{Y}$ and a conditioning random variable $X$. Assuming both CMOs are well defined, if $\mathcal{C}_{Y | X} = \mathcal{C}_{Z | X}$, then the conditional distributions $P(Y|X)$ and $P(Z|X)$ are equal in the sense that $P(Y|X=x) = P(Z|X=x)$ for $P(X)$-almost all $x \in \mathcal{X}$ \citep[Theorem 3]{Ren_2016}. Hence, the distance between conditional probability distributions is measured by the \emph{conditional maximum mean discrepancy} (CMMD) \citep{Ren_2016} which is defined as
\begin{equation*}
    \text{CMMD}_{k, \ell}^2 (P(Y|X), P(Z|X)) = \| \mathcal{C}_{Y | X} - \mathcal{C}_{Z | X}\|_{HS}^2,
\end{equation*}
where $k$ and $\ell$ are kernels on $\mathcal{X}$ and $\mathcal{Y}$ respectively. In practice, empirical CMO estimators from equation (\ref{eq:cmo_est}) can be used to estimate CMMD.  



\section{Conditional Kernel Calibration Error} \label{sec:ckce}

Recall our notion of calibration given by Definition \ref{def:calibration} where we compare the difference between probability distributions $P(Y|Q_X)$ and $Q_X$. By defining a random variable $Z_X \sim Q_X$ obtained from the predictive model, we note that $Q_X = P(Z_X | Q_X)$ and hence, a probabilistic predictive model $Q$ is calibrated if and only if $P(Y|Q_X) = P(Z_X|Q_X)$. Since we are now comparing differences between conditional probability distributions, we apply the CMMD and define the \emph{conditional kernel calibration error} (CKCE) with respect to kernels $k: \mathcal{P} \times \mathcal{P} \to \mathbb{R}$ and $\ell: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$ as
\begin{equation}
    \text{CKCE}_{k, \ell} = \| \mathcal{C}_{Y | Q_X} - \mathcal{C}_{Z_X | Q_X} \|_{HS}^2.
\end{equation}
A CKCE of 0 implies the conditional distributions are equal almost everywhere, and thus strong calibration of $Q_X$ holds. We propose to use the CKCE as a measure of calibration error for relative comparisons, which allows to construct a ranking of a set of candidate models ordered by their calibration. 

% We note that \citet{Widmann_2021} mentions the possibility of using CMOs in the context of calibration hypothesis tests but comments on the difficulty of justifying the assumption $\mathbb{E}[g(Y)| Q_X = \cdot] \in \mathcal{H}_k$ for all $g \in \mathcal{H}_{\ell}$ required to have a well-defined operator.


\subsection{Empirical evaluation}

The CKCE can be computed by using empirical estimates of the CMOs as in equation (\ref{eq:cmo_est}). Given a set of $n$ calibration samples $\{(x_i,y_i) \}_{i=1}^n$ from the joint distribution $P(X, Y)$, we get the predicted conditional distribution of $Y$ given feature $x_i$ under our model $Q$ by $q_i = f(x_i)$. Then $\{y_i, q_i \}_{i=1}^n$ are samples from $P(Y, Q_X)$, and the CMO $\mathcal{C}_{Y|Q_X}$ can be estimated by
\begin{equation*}
    \widehat{\mathcal C}_{Y | Q_X} = \Psi_Y (K_{QQ} + \lambda n I_n)^{-1} \Phi_Q^*
\end{equation*}
where $\Psi_Y = [\psi(y_1), \dots, \psi(y_n)]$ and $\Phi_Q = [\phi(q_1), \dots, \phi(q_n)]$, $K_{QQ} = \Phi_Q^* \Phi_Q$ is the Gram matrix with entries $[K_{QQ}]_{ij} = k(q_i, q_j)$, $I_n$ is the $n\times n$ identity matrix and $\lambda$ is a regularisation parameter. 

The CMO $\mathcal{C}_{Z_X|Q_X}$ can be estimated in a similar way. However, rather than sampling $Z_{x_i}$ from model outputs $Q_{x_i}$ to compute the feature matrix $\Psi_{Z_X}$, we can replace each of the feature embeddings $\psi(Z_{x_i})$ with their expected value, that is, the kernel mean embedding $\mu_{Q_{x_i}}$. This reduces variance of the CMO estimator\footnote{A similar strategy was used in \citet{ChaBouSej2021} and termed \textit{Conditional Mean Shrinkage Operator}.}. Hence, 
\begin{equation*}
    \widehat{\mathcal C}_{Z_X | Q_X} = M_Q (K_{QQ} + \lambda n I_n)^{-1} \Phi_Q^*
\end{equation*}
where $M_Q = [\mu_{Q_{x_1}}, \dots, \mu_{Q_{x_n}}]$ is a matrix of mean embeddings of the predictive distributions. 

Using the CMO estimators, the empirical evaluation of the CKCE is given by
\begin{equation*}
    \widehat{\text{CKCE}}_{k, \ell} = \| \widehat{\mathcal C}_{Y | Q_X} - \widehat{\mathcal C}_{Z_X | Q_X} \|_{HS}^2.
\end{equation*}
Since the squared Hilbert-Schmidt norm of an operator can be expressed as $\|\mathcal{C}\|_{HS}^2 = Tr(\mathcal{C}^* \mathcal{C})$, an equivalent formulation for the CKCE estimator is
\begin{multline*}
    \widehat{\text{CKCE}}_{k, \ell} = Tr\Big( (K_{QQ} + \lambda n I_n)^{-1} [M_Q^* M_Q + L_{YY} \\ - M_Q^* \Psi_Y - \Psi_Y^* M_Q] (K_{QQ} + \lambda n I_n)^{-1} K_{QQ} \Big)
\end{multline*}
where $L_{YY} = \Psi_Y^* \Psi_Y$ is the Gram matrix with entries $[L_{YY}]_{ij} = \ell(y_i, y_j)$.

Although estimating the CKCE involves finding the inverse of a potentially large matrix, which is computationally expensive, this can be overcome by a suite of well established large-scale kernel approximations such as random Fourier features \citep{rahimiRandomFeaturesLargescale2007}. Alternatively, simple low-rank approximations of the Gram matrix $K_{QQ}$ such as incomplete Cholesky factorization \citep{Fine_2002} can be used. 


\subsection{Choice of kernel} \label{sec:choice_kernel}

Evaluating the CKCE requires choosing two kernels, one on the space of labels $\mathcal{Y}$ and another on the space of probability distributions $\mathcal{P}$. We focus on selecting suitable kernels for probabilistic classifiers, where $\mathcal{Y} = \{ 1, \ldots, m\}$ and $\mathcal{P} = \Delta^{m-1}$. A reasonable choice for the former is the Kronecker kernel $\ell(y,y') = \mathbf{1}\{ y = y' \}$. In this case $\mathcal{H}_\ell \cong \mathbb{R}^m$ with feature maps $\psi(j) = e_j$ as the canonical basis. Then, for a conditional distribution $P(Y|Q_X)$ described by $\pi: \Delta^{m-1} \to \Delta^{m-1}$, that is, $\pi(q) = P(Y | Q_X =q)$, we have 
\begin{equation*}
    \mu_{Y|Q_X=q} = \mathbb{E}[e_Y | Q_X=q] = \sum_{j=1}^m \pi_j(q) e_j = \pi(q)
\end{equation*}
which is understood as a vector in $\mathbb{R}^m$. Thus, the conditional mean embedding is some mapping (potentially nonlinear) of the probability vector $q$. On the other hand, $P(Z_X | Q_X =q) = q$, and so the conditional mean embedding is simply $\mu_{Z_X|Q_X=q} = q$. 

Recall our assumption for well defined CMOs that for all $g \in \mathcal{H}_\ell$, we have $\mathbb{E}_Y[g(Y)|Q_X = \cdot]$, $\mathbb{E}_{Z_X}[g(Z_X)|Q_X = \cdot] \in \mathcal{H}_k$. Since by isometry $\mathcal{H}_\ell \cong \mathbb{R}^m$ each $g$ has a corresponding vector $v \in \mathbb{R}^m$, we get $\mathbb{E}_Y[g(Y)|Q_X = q] = v^\top \pi(q)$ and $\mathbb{E}_{Z_X}[g(Z_X)|Q_X = q] = v^\top q$. By choosing a universal kernel $k$ on $\Delta^{m-1}$, $\mathcal{H}_k$ is rich enough to approximate functions $v^\top \pi(q)$ arbitrarily well. At the same time, the RKHS should also contain linear maps to have $v^\top q \in \mathcal{H}_k$. Combing these two desiderata, we propose the kernel $k: \Delta^{m-1} \times \Delta^{m-1} \to \mathbb{R}$ with
\begin{equation*}
    k(p,q) = p^\top q + \exp\left( -\frac{1}{2 \gamma^2} \| p - q \|^2 \right)
\end{equation*}
where $\gamma \in \mathbb{R}$ is the bandwidth. An additional benefit of such a kernel is that random Fourier features can easily be used to approximate it, allowing each CMO to be estimated in the primal by applying the Woodbury identity, which speeds up computation for large $n$. Further details on implementation are provided in Appendix \ref{Ap:rff}.

The CKCE can likewise be applied for regression problems. For real valued targets, many commonly used kernels such as the Gaussian or Mat\'ern kernels could be chosen. However, note that in those cases we would need to estimate mean embeddings of predictive distributions, which may require sampling $Z_X$ from $Q_X$. For the space of probability distributions, one could use kernels of the form $k(p,p') = \exp(-\gamma D_{\mathcal{P}}(p, p'))$ where $D_{\mathcal{P}}$ is an appropriate metric on $\mathcal{P}$. When $D_{\mathcal P}$ is the squared MMD, one can in certain cases obtain a universal kernel on probability measures \citep{christmannUniversalKernelsNonStandard2010}. 


\subsection{Beyond global calibration}

Definition \ref{def:calibration} stipulates a global property of a probabilistic model, requiring that it is calibrated on average over all inputs. However, in certain application it may be desirable to check whether model outputs are calibrated for certain groups or even individuals \cite{Pleiss_2017, Zhao_2020}. This can be cast as conditioning on an extra variable in the LHS of equation (\ref{eq:calibration}). For example, let $A$ be a random variable representing the group membership of $X$, which could be a protected attribute such as a person's race or gender. Then a model $Q$ is calibrated for group $A$ if $P(Y|Q_X, A) = Q_X$. Such extensions of calibration have great importance in the study of algorithmic fairness \cite{Chouldechova_2017, Pleiss_2017}. By defining a kernel $k$ on the product space $\mathcal{P} \times \mathcal{A}$, one can similarly use the CKCE to measure the group calibration of a probabilistic model. More generally, the requirement $P(Y|Q_X, A) = Q_X$ could range from $A = \emptyset$, which returns global calibration given by Definition \ref{def:calibration}, to $A = X$, which gives individual calibration \cite{Zhao_2020}, and everything in between. 

% could also mention multicalibration

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.94\linewidth]{Figs/ranking_plot_paper.pdf}
    \caption{Proportion of trials where the estimated calibration error of the calibrated model is less than that of the miscalibrated model for calibration sets of different sizes, with error bars indicating the Wilson score interval.}
    \label{fig:ranking-plot}
\end{figure*}

\section{Comparison with other calibration measures}

\subsection{Proper scoring rules}

Historically, proper scoring rules such as the Brier score have been used to determine calibration \citep{Brier_1950}. Other common proper scoring rules are the negative log likelihood and cross entropy, which are both frequently used as loss functions when training modern machine learning models. Since these loss functions are minimised by the true conditional distribution, they can give a well calibrated model with enough training data. The reason behind this is that proper scoring rules can be decomposed into components of calibration and sharpness \citep{Kull_Flach_2015}. However, complex models can achieve low loss as a result of high accuracy rather than good calibration, meaning these metrics are of limited use for measuring calibration error directly. Furthermore, \citet{Merkle_2013} showed that different scoring rules can rank the same models differently, hampering their use for relative comparison of calibration. 


\subsection{Expected Calibration Error}

A widely used measure of calibration for probabilistic models is the expected calibration error (ECE) \citep{naeini_2015}. Although in its usual formulation ECE measures confidence calibration of probabilistic classifiers, it can be generalised to strong calibration as given by Definition \ref{def:calibration}. Let $d: \mathcal{P} \times \mathcal{P} \to \mathbb{R}_{\geq 0}$ be a metric on the space of probability measures. Then the ECE of a probabilistic model $Q$ with respect to $d$ is defined as
\begin{equation}
    \text{ECE}_d = \mathbb{E}_X d(P(Y|Q_X), Q_X).
\end{equation}
The main difficulty in computing the ECE comes from estimating $P(Y|Q_X)$. For probabilistic classifiers, an approach suggested by \citet{vaicenavicius_2019} is to partition the probability simplex and use histogram regression, which we implemented in our experiments. The simplest choice is uniform binning, although this causes the number of bins to increase exponentially with the number of classes, meaning care must be taken to ensure enough samples per bin. Alternative binning schemes may also be used, but as demonstrated by \citet{vaicenavicius_2019}, the ECE estimator becomes highly sensitive to the chosen scheme further emphasising the need for an alternative calibration metric.

A special case of ECE is the average distance between conditional mean embeddings, which is achieved by choosing $d$ to be the squared MMD. This approach was further explored by \citet{Chatterjee_2024}, but their method requires taking samples $Z_X$ from $Q_X$ which can be difficult to marginalise, adding extra variance to the estimator.


\subsection{Joint Kernel Calibration Error}

An alternative kernel based measure of calibration considers the discrepancy between joint distributions. As before, we define the random variable $Z_X \sim Q_X$. Then the joint distributions $P(Y, Q_X)$ and $P(Z_X, Q_X)$ are equal if and only if $P(Y | Q_X) = Q_X$ almost surely, that is, the probabilistic predictive model $Q$ is calibrated \citep{Widmann_2021}. Let $\kappa: (\mathcal{Y} \times \mathcal{P}) \times (\mathcal{Y} \times \mathcal{P}) \to \mathbb{R}$ be a measurable kernel with associated RKHS $\mathcal{H}$. Then, the \emph{joint kernel calibration error} (JKCE) with respect to kernel $\kappa$ is defined as
\begin{equation} \label{eq:jkce}
    \text{JKCE}_\kappa = \| \mu_{Y Q_X} - \mu_{Z_X Q_X} \|_{\mathcal{H}}^2.
\end{equation}
The JKCE was named the squared kernel calibration error (SKCE) by \citet{Widmann_2021} and corresponds to the MMD between the joint distributions $P(Y, Q_X)$ and $P(Z_X, Q_X)$. As described in Section \ref{sec:kernel_emb}, for a characteristic kernel $\kappa$ the JKCE equals 0 if and only if the distributions $P(Y, Q_X)$ and $P(Z_X, Q_X)$ are equal, i.e., $Q$ is calibrated. Given kernels $\kappa_{\mathcal{Y}}$ on the space of labels and $\kappa_{\mathcal{P}}$ on the space of probability measures, a kernel on the product domain $\mathcal{Y} \times \mathcal{P}$ can be constructed by their tensor product, i.e., $\kappa = \kappa_{\mathcal{Y}} \otimes \kappa_{\mathcal{P}}$. If $\kappa_{\mathcal{Y}}$ and $\kappa_{\mathcal{P}}$ are universal kernels on locally compact Polish spaces, then $\kappa = \kappa_{\mathcal{Y}} \otimes \kappa_{\mathcal{P}}$ is characteristic \citep[Theorem 5]{Szabo_2018}. Since the kernels $k$ and $\ell$ from Section \ref{sec:choice_kernel} are universal, setting $\kappa = \ell \otimes k$ produces a characteristic kernel. We will use this kernel to evaluate JKCE in our experiments in Section \ref{sec:experiments} for a fair comparison with CKCE. 

Given a set of calibration samples, it is possible to construct an unbiased estimator $\widehat{\text{JKCE}}_\kappa$ \citep{Widmann_2021} and use it to test the null hypothesis of strong calibration. However, the drawback is that when applied to two candidate models $Q^1$ and $Q^2$, the estimator will be sensitive to the difference between the marginal distributions of $Q^1_X$ and $Q^2_X$, which is not itself relevant for calibration. 

\begin{table*}[ht]
    \centering
    \caption{Accuracy, cross entropy and calibration error (using different estimators, scaled to be roughly the same order of magnitude) of neural network architectures and the marginal probability model on CIFAR-10 and ImageNet datasets.}
    \begin{tabular}{cc|ccccc}
    \toprule
Data & Model & Accuracy & Cross Entropy & CKCE & JKCE & ECE \\ \hline
\multirow{4}{*}{CIFAR-10} & LeNet & 0.727 & 0.823 & 0.301 & 0.311 & 0.226 \\
& DenseNet & 0.924 & 0.421 & 0.096 & 0.071 & 0.119 \\
& ResNet & 0.936 & 0.355 & 0.078 & 0.058 & 0.100 \\
& Marginal & 0.100 & 2.303 & 0.000 & -0.010 & 0.000 \\ \hline 
\multirow{3}{*}{ImageNet} & ResNet & 0.804 & 0.938 & 0.061 & 0.002 & 0.316 \\
 & ViT & 0.851 & 0.652 & 0.044 & 0.002 & 0.389 \\
 & Marginal & 0.001 & 6.908 & 0.000 & -0.002 & 0.003 \\
\bottomrule
    \end{tabular}
    \label{tab:nn_calibration}
\end{table*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/cnn_ce_paper.pdf}
    \caption{Distribution of calibration error estimates for four probabilistic models on CIFAR-10 and proportion of trials in which they are ranked correctly by calibration error.}
    \label{fig:ce_distribution}
\end{figure*}

\section{Experimental Results} \label{sec:experiments}

In our experiments, we assess the ability of the CKCE to distinguish between models through a relative comparison of calibration error. We compare its performance with ECE and JKCE. In all following experiments, the kernel bandwidth parameter $\gamma$ for both the CKCE and JKCE is chosen via the median heuristic. The ECE is estimated by uniform binning of the simplex, with each axis split into three equal intervals to ensure there are enough samples per bin, and we take $d$ to be the $L^1$ norm between probability vectors as is common in the literature \cite{Guo_2017, vaicenavicius_2019}.

\subsection{Ranking Candidate Models}

% The first set of experiments looks at the ability of metrics to consistently give the correct calibration ranking of models. 

\subsubsection{Synthetic Data} \label{sec:experiments_1}

Synthetic data allows us to easily construct perfectly calibrated probabilistic predictive models. We consider a classification problem with $m = 10$ classes and vary the number of samples $n$ used to measure calibration error. We construct the true conditional distribution by sampling from a Dirichlet distribution $P(Y | X = x_i) \sim \text{Dir}(0.1, \dots, 0.1)$, and then simulate true labels by $y_i \sim P(Y | X = x_i)$ for $i = 1, \dots, n$.

The two calibrated models we consider are the true conditional model, $f_t(x_i) = P(Y|X = x_i)$, and the marginal prediction model $f_m(x_i) = (0.1, \dots, 0.1)^\top$. To get miscalibrated models, we take the predictions of the true conditional distribution and either sharpen or soften the probability vector, producing an overconfident and underconfident model respectively. By defining miscalibrated predictive classifiers in such a way, we ensure that the top-class accuracy remains the same, but calibration error increases. The models in Figure \ref{fig:reliability-diag} were constructed in a similar way, with more detail provided in Appendix \ref{Ap:ts}. 

To assess the performance of the calibration error metrics, we do a pairwise comparison between each calibrated and miscalibrated model. This is repeated for 100 independent trials and we count the proportion of times that the estimated calibration error for the calibrated model is less than that of the micalibrated one for each metric. The results are displayed in Figure \ref{fig:ranking-plot}. As the number of samples in our calibration set increases, all three calibration error estimators are able to distinguish between the calibrated and miscalibrated models. However, when the number of calibration samples is small, CKCE generally has the most reliable performance. 


\begin{figure*}[ht]
    \centering
    % First group (a)
    \begin{subfigure}[b]{0.49\linewidth} % Adjust width as needed
        \centering
        \includegraphics[width=0.49\linewidth]{Figs/covshift_visual.pdf}
        \includegraphics[width=0.49\linewidth]{Figs/ce_under_covshift_paper.pdf}
        \caption{Synthetic Data}
        \label{fig:covshift_a}
    \end{subfigure}
    \hfill
    % Second figure (b)
    \begin{subfigure}[b]{0.49\linewidth} % Adjust width as needed
        \centering
        \includegraphics[width=\linewidth]{Figs/calibration_brightness_shift.pdf}
        \caption{Imagenet Data}
        \label{fig:covshift_b}
    \end{subfigure}
    
    \caption{The CKCE estimator remains stable under covariate shift, whereas JKCE and ECE are highly sensitive to changes in the input distribution.}
    \label{fig:covshift}
\end{figure*}

\subsubsection{Calibration of Neural Networks} \label{sec:experiments_2}

We next consider ranking candidate models in a more practical setting. We used the CIFAR-10 dataset and three models with different neural network architectures: LeNet, DenseNet and ResNet. See Appendix \ref{Ap:nna} for details on model hyperparameters and the training procedure. In addition, we also compared the marginal probability model which predicts equal probability across all labels. 

Table \ref{tab:nn_calibration} shows the accuracy, cross entropy and calibration error (CKCE, JKCE and ECE) for the models. CKCE and JKCE values are multiplied by 10 and 100 respectively so that the calibration errors are of comparable magnitude. We note that JKCE typically decreases in magnitude as the number of classes increases. The metrics were evaluated using the 10000 samples in the test set. ResNet shows the best performance in terms of accuracy and cross entropy, however the marginal model has the lowest calibration error as it is calibrated by definition. Using the full test set as a proxy for the ground truth calibration, the ranking of model calibration according to all three metrics is: 1. Marginal, 2. ResNet, 3. DenseNet, 4. LeNet.

To check the consistency of each metric, we randomly sample $n=500$ of the test set to form the calibration set and measure the calibration error for each of the models using the three estimators. We repeat for 1000 independent trials, and the resulting distribution of calibration error is presented via histograms in Figure \ref{fig:ce_distribution}. Additionally, we show the proportion of times each metric correctly ranks the models in the exact order as determined above. 

The proportion indicates that out of the 1000 trials, the CKCE correctly ranked the four models by calibration 700 times. The histogram illustrates that the CKCE is the best metric at identifying the marginal probability model as the best calibrated. One advantage of the JKCE is that it has an unbiased estimator (which leads to the negative values of JKCE estimates, even though the population JKCE is a nonnegative quantity). On the other hand, CKCE and ECE estimators are both biased, which can be observed by comparing the distributions in Figure \ref{fig:ce_distribution} with the values in Table \ref{tab:nn_calibration}. However, the variance of JKCE makes it unable to correctly rank the models nearly 50\% of the time.  



\subsection{Covariate Shift}

To demonstrate our claim that the CKCE is less sensitive to the marginal distribution of the predictive model, we now look at measuring calibration in problems with covariate shift. A toy example demonstrating the importance of this is presented in Appendix \ref{Ap:toy}. 

\subsubsection{Synthetic Data}

\textbf{Set up:} Let $\mathcal{X}\times \mathcal{Y} = [-1, 1] \times \{0,1\}$ with a conditional distribution governed by $P(Y=1|X=x) = \frac{1}{1+e^{-x}}$. Next, we choose a (miscalibrated) probabilistic model $Q$ with rule $Q(Y=1|X=x) = \frac{1}{1+e^{-5x}}$. We let the marginal distribution $P(X)$ be a truncated normal with domain $[-1,1]$, scale parameter $\sigma = 0.25$ and location parameter $\alpha$ which we vary between $-1$ and $1$. Note that the conditional distribution $P(Y|X)$ remains unchanged, making this an example of covariate shift. The set up is illustrated in Figure \ref{fig:covshift_a} (left). At each value of $\alpha$, we sample $n=1000$ inputs from $P(X)$ and simulate labels using $P(Y|X)$. The resulting calibration set is used to estimate each of CKCE, JKCE and ECE. Unlike in Section \ref{sec:experiments_2}, these values are not scaled.

\textbf{What we expect:} Even though a change in $P(X)$ changes the marginal distribution of $Q_X$, the conditional distribution $P(Y|Q_X)$ and $P(Z_X|Q_X)$ are not effected. Therefore, we would expect that calibration error stays constant as the location parameter $\alpha$ changes. This would indicate that the calibration error estimator is robust against covariate shift. 

\textbf{What we observe:} The results in Figure \ref{fig:covshift_a} (right) show that the CKCE estimator indeed stays constant, with some scattering due to randomness in the generated samples, demonstrating the desired behaviour. On the other hand, both JKCE and ECE are highly sensitive to changes in $P(X)$, showing clear variability as the parameter $\alpha$ shifts. 



\subsubsection{ImageNet} \label{sec:covshift_imagenet}

\textbf{Set up:} We next evaluate the impact of distribution shift on the calibration of image models, using ResNet and ViT neural networks trained on ImageNet. See Appendix \ref{Ap:nna} for model details. Their performance on the full validation set is displayed in Table \ref{tab:nn_calibration}. We take a subset of $n=1000$ validation data, and alter the brightness of the input image, ranging from -50\% to +50\% in increments of 10\%. At each brightness level, the CKCE and JKCE for both models is computed. Due to the large number of classes, ECE binning scheme undergoes a combinatorial explosion and cannot be accurately estimated.

\textbf{What we expect:} As the brightness changes, we would expect that model miscalibration, and hence relative ranking of the models, remains fairly constant. While an alteration in image brightness may cause a shift in the marginal distribution of the predictive models, the conditional distribution $P(Y|Q_X)$ should be stable.

\textbf{What we observe:} Figure \ref{fig:covshift_b} reveals that CKCE consistently indicates that ViT has lower calibration error than ResNet, with the estimate for both models largely unaffected by the change in input image brightness. This agrees with the findings of \citet{Minderer_2021_revisiting} which showed that transformer based models typically have better calibration than those with convolutional layers. On the other hand, JKCE is highly sensitive to the shift in input distribution, with the estimator preferring different models for different brightnesses. 



% Limitations: difficult to understand how calibrated a model is in an absolute sense, scale is senstive to the chosen kernel and parameters




\section{Conclusion}

We introduced the CKCE - a novel method of measuring calibration based on conditional mean operators. We demonstrated that the CKCE provides a more reliable way of performing a relative comparison of calibration of probabilistic predictive models compared to other metrics. This allows it to better distinguish calibrated models from miscalibrated ones, and more accurately rank a collection of models. Furthermore, it remains stable under distribution shift. While we do not address ways to improve model calibration in this work, future research could explore using CKCE as a regularization term in model training. Its ability to more precisely capture relative comparisons as training progresses may offer advantages over other metrics.

\newpage 
\bibliography{bibliography}
\bibliographystyle{apalike}

\newpage
\appendix
\onecolumn

\section*{Appendix}

\section{Effect of marginal distribution $P(Q_X)$} \label{Ap:toy}

A change in the marginal distribution of the predictive model will cause a change in the joint distribution $P(Y, Q_X)$, even if the conditional $P(Y|Q_X)$ remains the same. That is, $P(Y, Q_X) = P(Y|Q_X)P(Q_X) \neq P(Y|Q_X)P'(Q_X) = P'(Y, Q_X)$.  

The effect this has on the calibration of a model can be illustrated with the following toy example. Consider a setting with $\mathcal{X}\times \mathcal{Y} = \{0,1\} \times \{0,1\}$ and a simple probabilistic classifier $Q$ as summarised in Table \ref{tab:toy}. Clearly, the model is miscalibrated, however the extent of this depends on which calibration measure is chosen. 
\begin{table}[htbp]
    \centering
    \caption{Summary of a toy model}
    \begin{tabular}{c|ccc}
    \toprule
         $x$ & $P(X=x)$ & $Q(Y=1 | X=x)$ & $P(Y=1 | X=x)$ \\ \hline 
         0 & $p$ & 0.3 & 0.5 \\
         1 & $1-p$ & 0.6 & 0.5 \\
    \bottomrule
    \end{tabular}
    \label{tab:toy}
\end{table}

To calculate CKCE, we will choose kernels $\ell(y,y') = \mathbf{1}\{y=y'\}$ and $k(q,q') = \mathbf{1}\{q=q'\}$. Since the variables are discrete, CMOs become conditional probability matrices, allowing them to be written explicitly \cite{Song_2013_kecd}, e.g. 
\begin{equation*}
    \mathcal{C}_{Y|Q_X} = [P(Y=y|Q_X=q)]_{y\in\{0,1\}, q\in\{0.3, 0.6\}}.
\end{equation*}
Furthermore, the Hilbert-Schmidt norm is simply the Frobenius norm, meaning CKCE is easily computed.
\begin{equation*}
    \text{CKCE}_{k, \ell} = \| \mathcal{C}_{Y | Q_X} - \mathcal{C}_{Z_X | Q_X} \|_{HS}^2 = \left\|\begin{bmatrix}
0.5 & 0.5  \\
0.5 & 0.5
\end{bmatrix} - \begin{bmatrix}
0.7 & 0.4  \\
0.3 & 0.6
\end{bmatrix} \right\|_F^2 = 0.1.
\end{equation*}
Using the same kernels as above to define $\kappa = \ell \otimes k$, we will compute the JKCE. Now the joint embeddings can also be represented explicitly, this time as joint probability matrices, e.g. 
\begin{equation*}
    \mu_{Y Q_X} = [P(Y=y \cap Q_X=q)]_{y\in\{0,1\}, q\in\{0.3, 0.6\}}.
\end{equation*}
Again, the RKHS norm is the Frobenius norm, and so
\begin{equation*}
    \text{JKCE}_{\kappa} = \| \mu_{Y Q_X} - \mu_{Z_X Q_X} \|_{HS}^2 = \left\|\begin{bmatrix}
0.5p & 0.5(1-p)  \\
0.5p & 0.5(1-p)
\end{bmatrix} - \begin{bmatrix}
0.7p & 0.4(1-p)  \\
0.3p & 0.6(1-p)
\end{bmatrix} \right\|_F^2 = 0.1p^2 - 0.04p + 0.02.
\end{equation*}
Unlike for CKCE, which was not impacted by the marginal distribution of $X$, JKCE depends on the parameter $p$. A similar problem occurs with ECE. Choosing metric $d(x,y) = \|x-y\|_{L_1}$, gives
\begin{align*}
    \text{ECE}_{d} &= \mathbb{E}_X d(P(Y|Q_X), Q_X) \\
        &= (1-p) d((0.5, 0.5)^\top, (0.7, 0.3)^\top) + p d((0.5, 0.5)^\top, (0.4, 0.6)^\top) \\
        &= 0.4 - 0.2p
\end{align*}
As with JKCE, the level of miscalibration according to ECE depends on the parameter $p$. Having the calibration of a model be dependent on the marginal distribution of the inputs can be harmful in certain scenarios. For example, let $X$ indicate whether a person is a man or a woman, and $Y$ indicate whether they are a good hire for a company. Thus, evaluating a model's calibration using JKCE or ECE would be heavily impacted by who the company choose to interview (which determines the calibration set), causing fairness concerns. On the other hand, this is not an issue for CKCE, which only compares the conditional distributions. 



\section{Random Fourier Feature Implementation} \label{Ap:rff}

In Section 3.2, we propose the kernel $k: \Delta^{d-1} \times \Delta^{d-1} \to \mathbb{R}$ defined by
\begin{equation*}
    k(p,p') = p^\top p' + \exp\left( -\frac{1}{2 \gamma^2} \| p - p' \|^2 \right),
\end{equation*}
along with $\ell(y,y') = \mathbf{1}\{ y = y'\}$ on the space of labels for measuring calibration of probabilistic classifiers using the CKCE. We comment on how random Fourier features can be used to approximate the feature mapping for $k$ allowing us to compute CMO estimators in the primal. Here we provide further detail for practical implementation of this method. 

Note that adding kernels is equivalent to concatenating features. Thus, an approximation of the canonical feature mapping for kernel $k$ is 
\begin{equation*}
    \hat \phi(p) = \left[p^{(1)}, \dots, p^{(d)}, \frac{1}{\sqrt{D}} \cos ( w_1^\top p), \frac{1}{\sqrt{D}} \sin ( w_1^\top p ), \dots, \frac{1}{\sqrt{D}} \cos (w_D^\top p), \frac{1}{\sqrt{D}} \sin (w_D^\top p) \right]^\top
\end{equation*}
where $p = [p^{(1)}, \dots, p^{(d)}]^\top \in \Delta^{d-1}$ and $w_1, \dots, w_D$ are $D$ independent samples from the multivariate normal distribution $\mathcal{N}_d(0, \gamma^{-2} I)$. One can show that $\mathbb{E}[ \hat \phi(p)^\top \hat \phi(q)] = k(p, q)$ and $\text{Var}[ \hat \phi(p)^\top \hat\phi(q)] = O(D^{-1})$ \citep{rahimiRandomFeaturesLargescale2007}. In our experiments, we choose $D=100$. 

Using the approximation $\hat \phi$ rather than the full infinite dimensional feature mapping allows to estimate each CMO without calculating a Gram matrix. Given a calibration set $\{(y_i , q_i)\}_{i=1}^n$, where each $q_i$ is the probability vector predicted by our classifier for a feature $x_i$, the CMO estimators are
\begin{align*}
    \widehat{\mathcal C}_{Y | Q_X} &= \Psi_Y \widehat\Phi_Q^* (\widehat\Phi_Q \widehat\Phi_Q^* + \lambda n I_m)^{-1} \\
    \widehat{\mathcal C}_{Z_X | Q_X} &= M_Q \widehat\Phi_Q^* (\widehat\Phi_Q \widehat\Phi_Q^* + \lambda n I_m)^{-1} 
\end{align*}
where $\Psi_Y = [\psi(y_1) \dots \psi(y_n)]$, $M_Q = [q_1, \dots q_n]$, $\widehat \Phi_Q = [\hat\phi(q_1) \dots \hat\phi(q_n)]$, $\lambda$ is a regularisation parameter and $I_m$ is the $m\times m$ identity matrix with $m = d+2D$. The resulting CKCE estimator is
\begin{equation*}
    \widehat{\text{CKCE}}_{k, \ell} = ||\widehat{\mathcal C}_{Z_X | Q_X} - \widehat{\mathcal C}_{Y | Q_X}||_F^2
\end{equation*}
where we have replaced the Hilbert-Schmidt norm with the Frobenius norm as we are now working with finite dimensional matrices rather than operators. As the main computational complexity comes from calculating the inverse, we have thus reduced the computational cost from $O(n^3)$ to $O(m^3)$ with $m \ll n$. 

The theoretical analysis of random Fourier features has been well studied (cf. \citet{liUnifiedAnalysisRandom2021} and references therein), demonstrating guaranteed computational savings without sacrificing convergence properties of estimators in many cases. 


\section{Experimental Details}

\subsection{Synthetic Miscalibration} \label{Ap:ts}

In Section \ref{sec:experiments_1}, we perform a relative comparison of a calibrated and miscalibrated model. Constructing well calibrated models from the synthetic data is relatively straightforward, simply taking the true conditional and marginal probability distributions. 
\begin{itemize}
    \item \textbf{True Conditional}: $f_t(x_i) = P(Y|X = x_i)$,
    \item \textbf{Marginal}: $f_m(x_i) = (0.1, \dots, 0.1)^\top$.
\end{itemize}

To produce miscalibrated models, we take the true conditional distribution and either sharpen or soften the probability vector by temperature scaling by a parameter $t \in [0, \infty)$. Given a probability vector $p \in \Delta^{m-1}$, we get its scaled version by $p_t = \text{softmax}(\text{clr}(p)/t)$, where $\text{clr}: \Delta^{m-1} \to \mathbb{R}$ is the centre log ratio transformation \citep{Aitchison_1982}.

Since softmax and clr are inverses of each other, setting the temperature to $t=1$ returns the probability vector unchanged. Having $t>1$ softens $p$, whereas $t<0$ sharpens $p$. Thus, the two miscalibrated model are
\begin{itemize}
    \item \textbf{Underconfident}: $f_u(x_i) = \text{softmax}(\text{clr}(f_t(x_i))/2)$,
    
    \item \textbf{Overconfident}: $f_o(x_i) = \text{softmax}(\text{clr}(f_t(x_i))/0.5)$.
\end{itemize} 

\subsection{Reliability Diagram} \label{Ap:rel_dia}

Figure \ref{fig:reliability-diag} in Section \ref{sec:intro} shows a reliability diagram for three probabilistic predictive models. Here we provide extra detail on how the models and resulting diagram are generated. The models are binary probabilistic classifiers, that is $m=2$, and $n=1000$ synthetic samples were generated for the calibration set. The synthetic data is generated by first sampling true conditional distributions from a Dirichlet distribution $P(Y|X=x_i) \sim \text{Dir}(1,1)$, and then simulating true labels by $y_i \sim P(Y|X=x_i)$ for $i=1,\dots,n$, similar to the set up in Section \ref{sec:experiments_1}. 

As in Section \ref{sec:experiments_1}, the true conditional model is simply
\begin{equation*}
    \text{\textbf{True Conditional:} } f_t(x_i) = P(Y|X=x_i)
\end{equation*}
which is perfectly calibrated by definition. The two miscalibrated models are
\begin{align*}
    &\text{\textbf{Underconfident:} } f_u(x_i) = \text{softmax}(\text{clr}(f_t(x_i)/2)) \\
    &\text{\textbf{Overconfident:} } f_o(x_i) = \text{softmax}(\text{clr}(f_t(x_i)/0.5))
\end{align*}
where we have used temperature scaling to either soften or sharpen the true probability vectors as described above. 

The calibration set is formed by the pairs $\{ (y_i, f(x_i) )\}_{i=1}^n$ for each of the models. To produce the reliability diagram, we first partition the interval $[0,1]$ into 10 uniform bins with $B_m$ containing the set of indices for which $f^{(0)}(x_i) \in [\frac{m-1}{10}, \frac{m}{10}]$ for $m = 1, \dots 10$, that is, the predicted confidence in class 0 falls in bin $m$. Then, the average confidence in bin $m$ is given by
\begin{equation*}
    \text{conf}(B_m) = \frac{1}{|B_m|} \sum_{k\in B_m} f^{(0)}(x_k),
\end{equation*}
while the average accuracy is
\begin{equation*}
    \text{acc}(B_m) =  \frac{1}{|B_m|} \sum_{k\in B_m} \mathbf{1}\{ y_k = 0 \}.
\end{equation*}
Plotting the confidence and accuracy value in each bin for each model produces the reliability diagram. The CKCE value in Figure \ref{fig:reliability-diag} is computed using the same 1000 samples used to produce the reliability diagram.


\subsection{Neural Network Architecture} \label{Ap:nna}

In Section \ref{sec:experiments_1} we conduct experiments using three pretrained neural networks obtained from \citet{Kull_2019} on the CIFAR-10 dataset\footnote{Source code and output logits available at \url{https://github.com/markus93/NN_calibration}}. Below we provide further details for each model.

\textbf{LeNet CIFAR-10}

We used a standard LeNet-5 convolutional neural network architecture \citep{Lecun_1998}. The model is trained for 300 epochs using stochastic gradient descent with cross entropy loss. 

\textbf{DenseNet CIFAR-10}

We used a standard DenseNet-40 convolutional neural network architecture \citep{Huang_2018denselyconnectedconvolutionalnetworks}. The model is trained for 300 epochs using stochastic gradient descent with cross entropy loss. 

\textbf{ResNet CIFAR-10}

We used a standard ResNet-110 convolutional neural network architecture \citep{He_2015deepresiduallearningimage}. The model is trained for 200 epochs using stochastic gradient descent with cross entropy loss. 


In Section \ref{sec:covshift_imagenet} we used the ImageNet dataset, and pretrained models available in the \texttt{timm} library. 

\textbf{ResNet ImageNet}

We used a ResNet-50 convolutional neural network architecture \citep{He_2015deepresiduallearningimage} with the \texttt{resnet50.a1\_in1k} parameters from the \texttt{timm} library.

\textbf{Vit ImageNet}

We used a Vision Transformer neural network architecture \citep{zhai2023sigmoidlosslanguageimage} with the \texttt{ViT-L-16-SigLIP-384} parameters from the \texttt{timm} library.



\end{document}
