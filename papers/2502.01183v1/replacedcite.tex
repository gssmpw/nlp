\section{Related Works}
\label{sec:related}



\subsection{Few-Shot Learning}

{\color{recolor}Few-shot learning (FSL) aims to acquire generalized meta-abilities or meta-knowledge from a large volume of readily available base class data. When faced with novel classes in a specific application domain, FSL facilitates rapid learning using only a small amount of support data to accurately predict unknown query data.}
FSL methods are typically classified into three categories: meta-learning-based, metric learning-based, and data augmentation-based. 
Meta-learning-based methods____ focus on training models across diverse tasks to acquire a ``learning to learn'' ability, enabling models to quickly learn new category knowledge from support images.
Metric learning-based methods____ aim to learn a metric function to map similar images to higher similarity and dissimilar ones to lower similarity. 
Data augmentation-based methods____ focus on augmenting a large amount of image or feature data to alleviate severe overfitting issues caused by the limited number of support images.

Recently, researchers have increasingly focused on applying FSL to various application areas such as medical cell classification____, industrial defect identification____, leaf disease classification____. The semantic gap between images in these domains and base class images poses a challenge to the generalization performance of existing FSL methods. Furthermore, when faced with more complex images in real-world scenarios, current FSL based on pre-trained models struggles to quickly grasp the distribution knowledge of new classes accurately from a small support set and adapt to more challenging query images. Therefore, this paper aims to introduce a novel approach from the perspective of feature representation to enhance the performance of FSL in domains, particularly when handling difficult samples.

\subsection{Related Datasets}
\label{subsec:related}
In the early stages, FSL was primarily evaluated on general datasets, including miniImageNet, tieredImageNet, and CIFAR____, subsets derived from the ImageNet dataset. These datasets are coarse-grained. Subsequently, more works introduced a plethora of fine-grained evaluation datasets such as CUB, Stanford Cars, and Stanford Dogs____. 
Later on, to assess the performance of FSL methods in application domains, Guo Y et al.____ introduced a cross-domain few-shot learning (CD-FSL) benchmark. This benchmark includes two medical domain datasets (ChestX and ISIC), as well as an agricultural domain dataset and a satellite image dataset. They evaluated previous FSL methods and found that when faced with significant semantic or modal differences in validation datasets, FSL methods generally performed poorly, struggling to learn high generalization meta-knowledge.
Furthermore, Fereshteh.S et al.____ introduce the few-shot classification of histological images (FHIST) benchmark.

However, when applying FSL to domain visual recognition, not only do researchers encounter semantic gaps when transitioning from general datasets to specific domains, but they also face challenges presented by a plethora of distracting backgrounds or unstable shooting conditions in real-world scenarios, leading to target difficulties or image blurriness.
Regrettably, existing datasets have overlooked the crucial impact of environmental robustness in practical applications. To address this, this paper proposes the RD-FSL benchmark, aiming to enhance the performance and robustness of FSL in real-world applications.

\subsection{Contrastive Learning}
\label{subsec:con}
Recently, contrastive learning____, utilizing instance discrimination as a pretext task, has emerged as a leading method in self-supervised representation learning. 
The training process of contrastive learning structures the training data into positive or negative pairs and employs a loss function that reduces the distance between positive pairs while increasing the distance between negative pairs to optimize the model. 
Early methods that introduced contrastive learning into FSL were primarily focused on incorporating self-supervised pretext tasks such as rotation____ and jigsaw____ as auxiliary losses____. In recent years, the focus has shifted towards directly integrating contrastive learning of instance discrimination into the pre-training of FSL____. 

%This training approach drives the model to learn the similarity between feature embeddings of positive pairs while compelling the model to enhance the expression of their respective categorical features when dealing with negative pairs. 


%Early methods that introduced contrastive learning into FSL tasks were primarily focused on incorporating self-supervised pretext tasks such as rotation____ and jigsaw____ as auxiliary losses____. In recent years, the focus has shifted towards directly integrating contrastive learning of instance discrimination into the pre-training or meta-training stages of FSL____. 
These methods primarily focus on leveraging the known category information in few-shot learning tasks to design new optimization functions by integrating contrastive learning losses. This aims to combine the representational strengths of unsupervised contrastive learning with the characteristics of known classes in few-shot learning tasks. {\color{recolor}However, data from real-world environments often deviates significantly from the training data distribution. As a result, these methods struggle to filter out noisy features in challenging images, frequently misaligning features from non-target classes and leading to biased feature representations for difficult images.}
To address this, the paper proposes conditional representation learning. It focuses on how to accurately capture the critical category information of known and unknown images, even when unknown images contain a lot of noise. 
%The proposed conditional representation aims to learn the neighborhood continuous features from interactions between known and unknown images, eliminating the interference of non-target features.