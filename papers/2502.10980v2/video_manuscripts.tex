Introduction
We present "Deep Fourier Mimic for Expressive Dance Motion Learning." Entertainment robots are becoming popular by enabling more meaningful human-robot interactions. Dancing has become one of the most effective ways to attract human attention and express robotic emotions through motion. Creating diverse artistic motions is time-consuming for designers and restricted to simple motion playback.

Method
To overcome these challenges, we introduce Deep Fourier Mimic. The first part is motion design. Following practices in the animation industry, our motion data are crafted by artistic designers. This work utilizes a diverse set of 170 existing dance motion clips. Next is the motion representation part. DFM encodes the offline dance motions with an autoencoder-like structure featuring layers that perform FFT that generate natural transitions. Third, the motion learning part uses reinforcement learning to imitate the represented dance motions while following additional tasks. In the first phase of our curriculum, aibo learns to dance to reproduce reference dance motions as much as possible. In the second phase, aibo learns additional tasks such as locomotion or gaze on top of the acquired dance skills. Finally, during inference, DFM deploys the learned policy on the actual hardware, allowing for real-time execution and transition of dance motions.

Result 
The tracking accuracy of DFM is demonstrated by conditioning the reference dance to a rear leg lifting motion. We compare our method with Fourier Latent Dynamics as a baseline. Due to strong periodic assumptions in both motion representation and reinforcement learning, FLD overly smooths out reference motions. DFM, which relaxes the strong periodic assumption, results in moving up the rear leg by tracking the reference motion details more accurately. The transitions between different types of dance motions are shown. DeepMimic yields high tracking performance on single trajectories but lacks the capabilities to deal with diverse motions. The resulting hard switches lead to jerky changes of motion types. In contrast, DFM achieves smooth transitions. The modulation from higher to slower frequency by conditioning the mainly head-moving dance motion is shown. Even though the training dataset consists of discrete frequency types, the motion representation allows for continuous frequency interpolation. DFM can deal with auxiliary tasks while dancing. Depending on angular velocity, aibo learns to walk during dancing. By moving up the foreleg, aibo learns to rotate in the RL policy training, while only the rear legs move alternately in the reference motion. This is the dancing gaze which can look at the human face with eye contact. With the pitch-up command, aibo looks up. If pitch-down is commanded, aibo changes the posture in whole body manner.

Thank you for watching.