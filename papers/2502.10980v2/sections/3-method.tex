\section{METHOD}
As illustrated in \figref{fig:system_overview}, \ac{dfm} training pipeline for expressive dance motion learning system comprises four key components: motion design, motion representation, \ac{rl} training, and hardware inference.

\subsection{Motion Design}
Reference motions, including joint positions and velocities for each joint, are created using specialized design tools.
Following practices in the animation industry, our motion data are crafted by artistic designers.
We select 34 distinct dance motions, all created by motion designers at Sony, as showcased in the supplementary video.
Each motion is augmented to include five frequency variations — 0.5, 0.75, 1.0, 1.25, and 1.5 — by sampling and interpolating the motion trajectories.
This process results in a total of 170 clips with a duration of 6 seconds each.

\subsection{Motion Representation}
Motions are commonly described as long-horizon trajectories in high-dimensional state space.
However, directly associating motions with raw trajectory instances yields highly inefficient representations and poor generalization, failing to develop a policy that naturally transits in between.
We denote trajectory segments of length $H$ in $d$-dimensional state space preceding time step $t$ by $\mathbf{s}_t = (s_{t-H+1}, \dots, s_{t}) \in \Real^{d \times H}$.
In the motion representation stage, we utilize \ac{fld}~\cite{fld} to encode motion trajectories into latent parameters, which consist of $\theta_t = (f_t, a_t, b_t)$ and $\phi_t$, where $f_t$, $a_t$, $b_t$ and $\phi_t$ denote latent frequency, amplitude, offset and phase, respectively.
The \ac{pae} structure featured by \ac{fld} employs a set of encoder and decoder composed of 1D convolutional layers through time~\cite{periodic_autoencoder}.
While $\theta_t$ are computed with a differentiable real \ac{fft} layer, the latent phase $\phi_t$ is determined using a linear layer followed by atan2 applied on 2D phase shifts on each channel as shown in \figref{fig:system_overview}.
With the characteristic convolution and \ac{fft} layers, the \ac{fld} encoder decomposes the input motions into a latent phase parameter $\phi_t$ representing the low-level local time index and a latent frequency domain parameter $\theta_t$ representing high-level global features, respectively.
In the original paper, \ac{fld} formulates latent dynamics with latent frequency $f_t$ and time increment $\Delta t$ is thus described as
\begin{equation}
\begin{split}
    \theta_t = \theta_{t-1}, \quad \phi_t = \phi_{t-1} + f_{t-1} \Delta t.
    \label{eqn:latent_dynamics}
\end{split}
\end{equation}
The encoder and decoder are described with $c$ latent channels, 
\begin{equation}
    \phi_t, \theta_t = \enc(\mathbf{s}_t), \quad \hat{\mathbf{s}}'_{t+i} = \dec(\phi_t + i f_t \Delta t, \theta_t),
    \label{eqn:periodic_assumption}
\end{equation}
where $\phi_t \in \Real^c$ and $\theta_t \in \Real^{3c}$.

Fourier Latent Dynamics (\ac{fld}) introduces a forward prediction mechanism in the latent representation space, where it reconstructs future states with phase propagation while maintaining $\theta_t$.
The proceeding motion segment $\mathbf{s}_{t+i} = (s_{t-H+1+i}, \dots, s_{t+i})$ is approximated with the prediction $\hat{\mathbf{s}}'_{t+i}$ decoded from $i$-step forward propagation using the latent dynamics from time step $t$.
The latent dynamics in \eqnref{eqn:latent_dynamics} assumes locally constant latent parameterizations and propagates latent states by advancing $i$ local phase increments.
Therefore, the total loss for training \ac{fld} with $N$-step forward prediction by propagating the latent dynamics is written as
\begin{equation}
\begin{split}
    % \hat{\mathbf{s}}'_{t+i} = \dec(\phi_t + i f_t \Delta t, \theta_t),  \\
    % L_{\ac{fld}}^N = \sum_{i=0}^N L_i, \quad L_i = \MSE(\hat{\mathbf{s}}'_{t+i}, \mathbf{s}_{t+i}).
    L_{FLD}^N = \sum_{i=0}^N \MSE(\hat{\mathbf{s}}'_{t+i}, \mathbf{s}_{t+i}).
    \label{eqn:total_loss}
\end{split}
\end{equation}

The choice of the number of forward prediction steps, $N$, plays a crucial role in balancing the trade-off between the accuracy of local reconstructions and the global coherence of the latent parameterizations.
In the original \ac{fld} approach, a larger value of $N$, such as $N = 100$ at $\Delta t = 0.01$, is typically selected to emphasize the global consistency of the latent parameters within periodic motions to facilitate downstream \ac{rl} training.
However, this can result in reduced reconstruction accuracy of general, less periodic reference trajectories, which is particularly critical for entertainment robots where precise motion tracking is essential.
To address this, \ac{dfm} sets $N=0$, prioritizing local reconstruction accuracy to enhance the tracking performance while maintaining a fresh local motion encoding during policy training.
The specific hyperparameters used for the motion representation are detailed in \tabref{table:fld_param}. 

\begin{table}[t]
\caption{Motion Representation Hyper Parameters}
\label{table:fld_param}
\begin{center}
\begin{tabular}{cccc}
\toprule
\textbf{Entry} & \textbf{Symbol} & \textbf{Value} \\
\midrule
step time seconds & $\Delta t$ & 0.01 \\
mini batch size & ${-}$ & 50 \\
learning rate & ${-}$ & 0.0001 \\
weight decay & ${-}$ & 0.0005 \\
max iterations & ${-}$ & 5000 \\
latent channel number & ${c}$ & 8 \\
periodic trajectory segment  & ${H}$ & 100 \\
forward prediction step & ${N}$ & 0  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{Policy Observation}
\label{table:observation}
\begin{center}
\begin{tabular}{cccc}
\toprule
\textbf{Entry} & \textbf{Symbol} & \textbf{Noise Level} & \textbf{Dimensions} \\
\midrule
joint positions & ${q}$ & 0.01 & 14 \\
joint velocities & ${\dot{q}}$ & 1.5 & 14  \\
last action & ${a}^*$ & 0.0 & 14 \\
foot contact state & ${f_c}$ & 0.0 & 4 \\
gravity orientation & ${g}$ & 0.05 & 3 \\
latent phase & ${\sin{\phi}}$ & 0.0 & 8 \\
latent phase & ${\cos{\phi}}$ & 0.0 & 8 \\
latent frequency & ${f}$ & 0.0 & 8 \\
latent amplitude & ${a}$ & 0.0 & 8 \\
latent offset & ${b}$ & 0.0  & 8\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

% \begin{table*}
\begin{table*}[t]
\caption{Reward functions and scales at imitation and curriculum phase for locomotion and gaze}
\label{table:reward}
\begin{center}
\begin{tabular}{lllccc}
\toprule
\textbf{Category} & \textbf{Reward} & \textbf{Definition} & \textbf{Dance} & \textbf{Locomotion} & \textbf{Gaze}\\
&&& \textbf{imitation} & \textbf{curriculum} & \textbf{curriculum} \\
\midrule
\multirow{1}{*}{\textbf{Imitation}} & joint position imitation & $\exp(\|{q}^* - {q}\|^2 )$ & $1.0$ & $1.0$ & $1.0$ \\
\midrule
\multirow{2}{*}{\textbf{Task}} 
 & base angular velocity tracking & $\exp(-\frac{1}{0.06}\|{w}_{b,z}^* - {w}_{b,z}\|^2)$ & $0.0$ & $1.0$ & - \\
 & end-effector orientation tracking  & $\exp(4\|{q}_{h}^* - {q}_{h}\|^2 )$ & $0.0$ & - & $0.7$ \\
\midrule
\multirow{6}{*}{\textbf{Regularization}} & joint torque & $\|{\tau}\|^2$ & $-0.001$ & $-0.001$ & $-0.001$  \\
 & joint acceleration & $\|{\ddot{q}}\|^2$ & $-2e^{-7}$ & $-2e^{-7}$ & $-2e^{-7}$ \\
 & joint target difference & $\|{a}^*_{t-1} - {a}^*_{t}\|^2$ & $-0.01$ & $-0.01$ & $-0.01$ \\
 & self-collisions & $n_{c}$ & $-10.0$ & $-10.0$ & $-10.0$ \\
 & foot slippage & $\|{v}_{f,xy}\|^2$ & $0.0$ & $-0.15$ & - \\
 & foot air time & $\sum_i ({t}_{f,air} - $0.2$)$ & $0.0$ & $2.0$ & - \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
% \end{table*}

\subsection{Motion Learning}
Following the motion representation stage, \ac{dfm} employs \ac{rl} to track the encoded motions while managing additional tasks such as locomotion and gaze control during stylized dancing, as depicted in \figref{fig:system_overview}.
The training network architecture consists of three fully connected layers, each with 256 hidden units and ELU activation functions, and utilizes the \ac{ppo}~\cite{ppo}.
Both the actor and critic networks are implemented as \ac{mlp}, adhering to the architecture and hyperparameters established in prior work~\cite{fld}.
The simulation and control loop frequencies are set to 400 Hz and 100 Hz, respectively, using the Isaac Gym framework~\cite{Makoviychuk2021-th}.

The aibo features 14 \ac{dof}, including 12 \ac{dof} for its legs and 2 \ac{dof} for head pitch and yaw movements, represented by the action vector ${a}^*$.
Policy observations and noise levels are detailed in \tabref{table:observation}.
Given that aibo is a consumer-grade robot, ${f_c}$ values are obtained from binary switch contact sensors.
The latent parameters with 8 channels are used as observations.
Unlike \ac{fld}, which assumes time-invariant latent parameters $\theta_t$ for each episode, \ac{dfm} updates $\theta_t$ and $\phi_t$ dynamically during motion representation to capture the nuanced details of the artistic reference motions.
This allows for the expression of critical changes in frequency, amplitude, and offset, preserving details in expressive movements.

In contrast to \ac{fld}, which limits motion representation and learning only to locomotion tasks, the extended capabilities of \ac{dfm} are demonstrated with distinct tasks assigned to each stage.
Specifically during the \ac{rl} phase, the policy is conditioned to track a target motion sequence using the learned representations, while locomotion and gaze are trained with auxiliary task objectives.
The task command for locomotion is the angular velocity of the base axis, while for gaze is the pitch and yaw angles of the head axis in the robot frame, as illustrated in \figref{fig:system_overview}.

The reward functions and their corresponding scales are outlined in \tabref{table:reward}.
The variables ${q}$, ${\tau}$, and ${n}_{c}$ represent joint positions, joint torques, and the number of collisions, respectively.
${q}_{h}$ denotes the yaw and pitch angles of the head axis, with ${q}_{h}^*$ representing their commanded values.
Similarly, ${{w}_{b}}$ and ${{w}_{b}}^*$ denote the measured and desired angular velocities of the base axis, respectively.
Imitation rewards are designed to ensure the policy closely mimics the reference dancing motion.
Joint imitation rewards are computed as the difference between the reconstructed joint positions and the current joint position in the physics simulator.
Once the agent achieves a joint position imitation reward higher than 0.9, task rewards for locomotion or gaze are introduced.
Among the regularization rewards, joint target difference, joint torque, and joint acceleration rewards suppress jerky motions, while self-collision rewards prevent collisions between the robot’s parts.
Foot velocities ${{v}_{f}}$ for foot slippage and airtime for foot parts ${t}_{f,air}$ are monitored to shape walking during the locomotion curriculum phase.
% To further refine locomotion, foot slippage and foot airtime rewards are incorporated, drawing on techniques from previous work~\cite{legged_gym}.

\subsection{Hardware Inference}
As depicted in \figref{fig:system_overview}, the reference motion, encoder, and policy, developed during the motion design, motion representation, and motion learning stages, respectively, are deployed on the robot.
The \ac{rl} policies for locomotion and gaze are implemented as separate entities, with task commands being switched accordingly for each policy.
% Observations, as detailed in \tabref{table:observation}, are gathered from various sensors attached to the robot, including joint encoders, switch contact sensors and an IMU.
Both control and sensor reading operations are performed at a frequency of 100 Hz.
The gravity orientation is calculated by fusing acceleration and gyroscope data from the IMU.

To ensure smooth transitions between different motions at arbitrary times, we interpolate the latent parameters during the transition period using \eqnref{eqn:natural_transient}:
\begin{equation}
\begin{split}
    \theta_{A\rightarrow B} = \alpha \theta_A + (1 - \alpha) \theta_B, \\ 
    \phi_{A\rightarrow B} = \alpha \phi_A + (1 - \alpha) \phi_B,
\end{split}
    \label{eqn:natural_transient}
\end{equation}
where $\theta_A$ and $\phi_A$ represent the final latent parameters of motion A, and $\theta_B$ and $\phi_B$ correspond to the initial latent parameters of motion B. The interpolation factor $\alpha$ is smoothly varied from $0$ to $1$ over a designated transition period, which in our implementation is set to 0.5 seconds.