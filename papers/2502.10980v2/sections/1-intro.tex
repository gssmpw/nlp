\section{INTRODUCTION}
Recent advancements in entertainment robots~\cite{aibo,lovot, disney_learning} have significantly broadened their capabilities, enabling them to perform a wider range of tasks and facilitating more meaningful human-robot interactions~\cite{KANG2020207, companionAIBO2003, robinson2013psychosocial}.
A prominent example is Sony's aibo~\cite{aibo}, a robotic dog designed to engage and captivate its owners.
Within the realm of entertainment robotics, dancing has become one of the most effective ways to attract human attention and express robotic emotions through motion~\cite{dance_robot_god,boston_dynamics_dance,anymal_dance}. 

Typically, the diverse artistic motions performed by these robots are meticulously crafted by designers at entertainment robotics companies~\cite{aibo_dance, disney_learning}.
This process is inherently time-consuming, requiring motion designers to carefully create and fine-tune various dance motions, most of which are handcrafted.
Despite the presence of common motion primitives within these dance routines, designers still need to create complete dance motions for each instance, which demands significant time and effort.
Additionally, these artistic motions are often replayed with less emphasis on interacting dynamically with humans.

The field of computer animation has proposed various methods to streamline the motion creation process.
A promising approach involves the use of learning-based methods to generate diverse motions.
Recent advancements in this area have leveraged \ac{gan}~\cite{goodfellow2020generative} to address this issue~\cite{amp_org,amp_quadruped_robot,li2023learning,peng2022ase,li2023versatile,luo2023perpetual,tessler2023calm}.
However, these methods still face limitations in handling a wide variety of demonstration motion datasets due to mode collapse.
An alternative approach focuses on motion representation.
Several studies~\cite{periodic_autoencoder, ai_choreographer, transflower, fld} have explored methods to automatically align and represent periodic dance motions within a structured latent space. 
For instance, the \ac{fld} approach~\cite{fld}, which integrates \ac{pae}~\cite{periodic_autoencoder} and \ac{rl}, addresses some of these challenges by conditioning desired motion trajectories during policy inference.
The structured latent space in \ac{fld} allows for the creation of diverse motions through smooth transitions between motion primitives, thereby reducing the time-intensive process of motion creation for designers.
However, the reliance on strong periodicity assumptions in \ac{fld}, even at the local level, leads to overly smoothed motions and a loss of expressiveness, which is particularly problematic for entertainment robots where dynamic and expressive motions are essential.

\begin{figure}[!t]
    \centering
    % \vspace{3pt}
    \includegraphics[width=\linewidth]{figures/Fig1.pdf}
    \caption{Deep Fourier Mimic (DFM) allows entertainment robots such as aibo to seamlessly combine artistic motions crafted by designers with auxiliary tasks like locomotion or gaze towards a human face, resulting in expressive motion that can smoothly transition between different movements at arbitrary timings. Project webpage: \url{https://sony.github.io/DFM/}}
    \label{fig:dfm_concept}
    \vspace{-3ex}
\end{figure}

To extend the capabilities of \ac{fld} to general, less periodic motions such as dancing, we propose \ac{dfm} as shown in \figref{fig:dfm_concept}, a method that trains policies using fresh encodings of the most recent motion segments, thereby relaxing the strong local periodic assumptions to preserve expressiveness and variation in the generated motions.
Additionally, to increase human interaction during stylized dancing, we introduce supplementary tasks such as locomotion and head orientation tracking called gaze.
For instance, maintaining eye contact by gazing at a human face, which mimics real dog behavior, can create a more interactive and engaging experience during the dance with a whole-body approach.

Our primary contributions are as follows:
\begin{itemize}
\item Development of a method for learning from demonstrations that achieves high tracking accuracy of reference motions while preserving details and ensuring smooth transitions between various dance sequences for entertainment robots.
\item Demonstration and validation in hardware experiments of multi-task capabilities, including auxiliary gaze and locomotion control during dance motion, coordinated in a whole-body manner that integrates both leg and head movements.
\end{itemize}
