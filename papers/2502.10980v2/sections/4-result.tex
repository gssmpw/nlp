\section{Experiments and Results}
We evaluate the performance of \ac{dfm} in terms of tracking accuracy relative to reference motions and the naturalness of transitions between motions. 
Additionally, we demonstrate the multitasking capabilities of our approach through locomotion and gaze control during stylized dancing.

\subsection{Tracking Accuracy}
To assess tracking accuracy, we calculate the difference between the reference motion and the observed joint positions on the robot hardware.
For this evaluation, we select a dancing motion that involved lifting the rear legs.
\figref{fig:moveup_rear_leg} illustrates the height of the rear right leg during this stylized dance.
Although \ac{dfm} does not perfectly replicate the height of the reference motion, it significantly outperforms the \ac{fld} baseline.
To provide a more quantitative comparison between the baseline and our method, we analyze three types of joint angles as shown in \figref{fig:joint_angle_tracking_acc}.
The results indicate that the motion reconstructed by \ac{fld} is overly smooth due to its strong enforcement of quasi-constant parameterization and periodicity assumption with $N = 100$.
In contrast, \ac{dfm} achieves a more accurate reconstruction with $N = 0$, preserving intricate details that may not follow periodic patterns.
When examining the joint encoder data measured from the robot, \ac{fld} again shows excessive smoothing, which we attribute to the overly strong periodic assumptions applied to the local time during \ac{rl} training, as described in \eqnref{eqn:periodic_assumption}.
\Figref{fig:latent_parameters} presents the $\sin{\phi}$ and frequency values derived from the latent parameters across eight channels during the same dancing motion.
On the left side, \ac{fld} shows that all channels of $\sin{\phi}$ are periodic, with little change in frequency.
In contrast, \ac{dfm} demonstrates variability in some channels of $\sin{\phi}$ and frequency during the upward movement of the rear leg, retaining non-periodic features that characterize the dance motions.
Finally, \tabref{table:tracking_accuracy} reports the mean absolute tracking error (MAE) across all joints for all 170 evaluated motions using the real aibo hardware.
Additionally, we test \ac{dfm} using the MIT Humanoid environment~\cite{chignoli2021humanoid} in Isaac Gym.
Our method consistently demonstrates superior tracking accuracy in both robot environments compared to \ac{fld}.

\begin{figure}[t]
    \vspace{1ex}
    \centering
    \includegraphics[trim={0 0 0 0}, width=\linewidth]{figures/fig3.pdf}
    \caption{Height reached by rear right leg. Left, middle and right depict reference motion, \ac{fld} and \ac{dfm} motions respectively. The red dash line illustrates the height of the right rear leg at reference motion.}
    \label{fig:moveup_rear_leg}
    \vspace{-1ex}
\end{figure}

\begin{figure}[t]
    % \vspace{-3ex}
    \centering
    %\includegraphics[trim={0 6ex 0 0}, width=\linewidth]
    \includegraphics[trim={0 2ex 0 0}, width=\linewidth]{figures/tracking_acc.pdf}
    \caption{Comparison of tracking accuracy for the \ac{fld} and \ac{dfm}. Blue: reference motion created by the motion designer. Orange: reconstructed motions from motion representation parts by conditioning the reference motion. Green: joint encoder reading activated by the \ac{rl} policy.}
    % \caption{Comparison of tracking accuracy for \ac{fld} and \ac{dfm}. Motion reconstruction by FLD is overly smooth. In contrast, DFM achieves a more accurate reconstruction, preserving intricate details that may not follow periodic patterns. }
    \label{fig:joint_angle_tracking_acc}
    \vspace{-3ex}
\end{figure}


\begin{figure}[!t]
    \vspace{1ex}
    \centering
    \includegraphics[trim={0 0 0 0}, width=\linewidth]{figures/periodic.pdf}
    \caption{Comparison of 8 channel latent parameters for \ac{fld} at the left and \ac{dfm}  at the right side by conditioning the same dancing motion as \figref{fig:moveup_rear_leg}. The upper and bottom of plots are $\sin{\phi}$ and frequency for each.}
    \label{fig:latent_parameters}
    % \vspace{-3ex}
\end{figure}

\begin{table}[!t]
\caption{Mean Absolute Tracking Accuracy}
\label{table:tracking_accuracy}
\begin{center}
\begin{tabular}{llcc}
\toprule
\textbf{Robot} & \textbf{reference motion} & \textbf{FLD} & \textbf{DFM (ours)} \\
\midrule
aibo & dance & $0.132$ $\rm{rad}$ & $0.094$ 
 $\rm{rad}$ \\
% aibo & locomotion & 0.141 $\rm{rad}$ & 0.123 $\rm{rad}$ \\
MIT humanoid  & locomotion &  $0.125$ $\rm{rad}$  & $0.103$ $\rm{rad}$   \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5ex}
\end{table}

\subsection{Natural Transition}
The motion representation employed by \ac{dfm} enables continuous frequency interpolation and smooth transitions between different dancing motions.

\Figref{fig:frequency_interpolation} shows the estimated latent frequency parameters conditioned on the reference motion, which primarily involves head movements transitioning from higher to lower dancing frequencies.
% , as demonstrated in the supplementary video.
While most frequency channels remain relatively constant, channels 3 and 4 exhibit gradual changes as shown in \figref{fig:frequency_interpolation}.
The linear interpolation of frequencies in these channels adjusts in response to the changing frequency of the reference dancing motion.
Even though the training dataset consists of discrete frequency types, the motion representation allows for continuous frequency interpolation.
This capability results in smooth, periodic changes in joint positions without abrupt movements, even for previously unseen datasets as shown at the bottom plot in \figref{fig:frequency_interpolation}.

\Figref{fig:natural_transient} illustrates the joint angular velocity at the head pitch and yaw during the transition from motion A to motion B, which primarily involves the head pitch and yaw actuators, as shown in the supplementary video.
During the transition times at 1 and 2.5 seconds, joint positions experience abrupt changes with switches between reference motions.
We compare the transition performance of \ac{dfm} with DeepMimic~\cite{deepmimic}, a well-known learning from demonstration approach that yields high tracking performance on single trajectories but lacks capabilities to deal with multiple motions.
Jerky transitions are observed in this case if the reference dataset and its representation are not carefully crafted.
In contrast, \ac{dfm} achieves smooth transitions without abrupt movements by interpolating in the latent space using \eqnref{eqn:natural_transient}.

\begin{figure}[!t]
    % \vspace{1ex}
    \centering
    \includegraphics[trim={0 0 0 0}, width=\linewidth]{figures/frequency_interpolation.pdf}
    \caption{Frequency modulation during head-moving dance. The upper plot displays the frequency of two representative latent channels out of eight. Solid and dashed curves represent raw and linearly interpolated data, respectively. The bottom plot shows head pitch (HP) and the head yaw (HY) joint angles.}
    \label{fig:frequency_interpolation}
    \vspace{-1ex}
\end{figure}


        % \small {
        % {\color{ourred}\rule[.5ex]{1em}{1pt}$\bullet$\rule[.5ex]{1em}{1pt}} step in place \qquad
        % {\color{ourorange}\rule[.5ex]{1em}{1pt}$\bullet$\rule[.5ex]{1em}{1pt}} forward run \qquad
        % {\color{ourgreen}\rule[.5ex]{1em}{1pt}$\bullet$\rule[.5ex]{1em}{1pt}} forward stride
        % }

% Continuous frequency transient. All plot are shown during dancing frequency which moves mostly head angle positions. Upper plot is all 8 channel of frequency from latent parameter. Solid and dash are raw and linear interpolation for each. middle plots is two plot (3 and 4 channel) are picked up and zoomed up from upper plot. Bottom plot are the joint angle of head yaw (HY) and head pitch (HP) actuator.

\begin{figure}[!t]
    %\vspace{-3ex}
    \centering
    \includegraphics[trim={0 0 0 0}, width=\linewidth]{figures/natural_transient.pdf}
    \caption{Transition between different dance types. The background color indicates the dance motion type. The left and right plots demonstrate hard switches between Dance A and Dance B, with DeepMimic and \ac{dfm}, respectively. Angular velocities at the head pitch (HP) and the head yaw (HY) are shown.}
    \label{fig:natural_transient}
    \vspace{-3ex}
\end{figure}

\subsection{Multi-task Demonstration}
We evaluate the multitasking capability of \ac{dfm} with auxiliary tasks, including locomotion and gaze, respectively.

\Figref{fig:dancing_locomotion} illustrates the locomotion policy during dancing, where an angular velocity command is used to facilitate in-place rotation.
In the reference motion, only the rear legs move alternately, while the forelegs remain stationary.
After training the policy with the reward structure defined in the locomotion curriculum (\tabref{table:reward}), aibo learns to rotate in response to the angular velocity command in the base frame.
To allow this rotation without hindering the movement of the rear legs, the right foreleg is lifted, enabling the execution of the stylized dancing, as shown in the supplementary video.

Similarly, a policy for auxiliary gaze control is trained using the reward scale from the gaze curriculum in \tabref{table:reward}.
This policy enables aibo to adjust its head orientation in response to pitch and yaw commands during dancing, as demonstrated in \figref{fig:dancing_gaze}.
The supplementary video shows that the dance sequence continues smoothly while the robot adjusts its pitch and yaw angles.
aibo utilizes its head and legs to track the commanded pitch and yaw angles, as illustrated in \figref{fig:dancing_gaze_plot}.
For instance, when a pitch of $0.3$ $\rm{rad}$ and a yaw of $0.0$ $\rm{rad}$ are commanded, both the directions of head are moved up with legs.
In contrast, a $-0.5$ $\rm{rad}$ pitch command prompts the head and legs to move in opposite directions.

\begin{figure}[!t]
    %\vspace{-3ex}
    \centering
    \includegraphics[trim={0 0 0 0}, width=\linewidth]{figures/fig8.pdf}
    \caption{Locomotion during dance. The reference dance motion alternates lifting the rear legs while keeping the forelegs stationary. Applying an angular velocity command results in locomotion by lifting the left foreleg.}
    \label{fig:dancing_locomotion}
    %\vspace{-3ex}
\end{figure}

\begin{figure}[!t]
    \vspace{-2ex}
    \centering
    \includegraphics[trim={0 0 0 0}, width=\linewidth]{figures/fig9.pdf}
    \caption{Gaze during dance. The reference motion is the same as in \figref{fig:dancing_locomotion}. The images depict commanded pitch angles of $0.0$, $0.3$ $\rm{rad}$, and $-0.5$ $\rm{rad}$, respectively. The command of the yaw angle is held at zero.}
    \label{fig:dancing_gaze}
    % \vspace{-3ex}
\end{figure}

\begin{figure}[!t]
    % \vspace{-1ex}
    \centering
    \includegraphics[trim={0 0 0 0}, width=\linewidth]{figures/dancing_gaze_plot.pdf}
    \caption{Joint readings for fore left shoulder pitch (FLSP), fore left shoulder roll (FLSR), fore left foot pitch (FLFP), and head pitch (HP) during dancing gaze are shown. The background color of the plot indicates command for pitch angle at the head frame (yellow: $0.0$, red: $0.3$ $\rm{rad}$, blue: $-0.5$ $\rm{rad}$).}
    \label{fig:dancing_gaze_plot}
    \vspace{-3ex}
\end{figure}

