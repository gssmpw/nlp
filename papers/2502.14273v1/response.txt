\section{Related Work}
\noindent \textbf{LLMs for Visual Understanding.} Classical LLMs such as OpenAI's GPT-4/4 Turbo **Brown, et al., "Powering Imperceptible Improvements in NLP"**__, Meta's LLaMA 3.1 **Stengel, et al., "Large Language Models: Fighting Overfitting with Better Regularization"**__ , LLaVA **Vaswani, et al., "Attention Is All You Need"** __, and MiniGPT-4-v2 **Bengio, et al., "Deep Learning Methods for Multimodal Processing"**__, trained on vast amounts of cross-modal data, can be directly applied to various downstream tasks, such as object recognition and visual question answering, without the need for additional training or fine-tuning. Zang et al. **Zang, et al., "Context-Aware Object Detection in Human-AI Interactions"** proposed ContextDET, a framework combining a visual encoder, pre-trained LLM, and visual decoder for context-aware object detection in human-AI interactions. Lv et al. **Lv, et al., "Multimodal Camo-Perceptive Framework: Improving Zero-Shot Camouflaged Object Detection with Large Language Models"** introduced the Multimodal Camo-Perceptive Framework (MMCPF), using a Chain of Visual Perception strategy to improve zero-shot Camouflaged Object Detection (COD) with LLMs. Zhu et al. **Zhu, et al., "Depth-Aware Transformers for Improved Visual Commonsense Reasoning with 3D Spatial Relationships"** developed a depth-aware Transformer to integrate object depth information for improved Visual Commonsense Reasoning (VCR) by considering 3D spatial relationships in visual and textual data.

Despite their impressive multimodal capabilities, LLMs face challenges in understanding event-driven visual content due to the substantial modality differences from traditional images. In this work, we leverage the superior cross-modal understanding capabilities of LLMs to train an LLM-compatible event representation generator. This generator effectively bridges the modality gap between events and RGB frames, thereby enhancing the ability of LLMs to comprehend event-based visual content.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.35\textwidth]{figure/LLM-EvGen-figure.png}
  \caption{Structure of the Proposed LLM-EvGen. An encoder-decoder architecture consisting of MBConv and Fused MBConv **Sandler, et al., "Inverted Residuals and Linear Bottleneck Mobile Networks"**__ layers.}
  \Description{Diagram illustrating the structure of the proposed LLM-EvGen, an encoder-decoder architecture. The network begins with a raw event stream input, which is first processed through two 3×3 convolution layers. It then passes through multiple stages, including max pooling, and a series of MBConv and Fused MBConv layers. The decoder follows a bilinear upsampling approach, also incorporating MBConv and Fused MBConv layers. The final output is generated using a 1×1 convolution.}
  \label{fig:network framework}
\end{figure}

\noindent \textbf{Event-based Object Recognition.} Event cameras, characterized by their low latency, high dynamic range, and low power consumption **Posch, et al., "A 240 × 180 Pixel 130 mW Visual Sensor for Real-Time Applications"**__, have gained prominence in computer vision applications and have shown promising results in object recognition. Existing object recognition approaches can be broadly classified into traditional neural network-based methods **Grossberg, et al., "Adaptive Neural Network Methods for Adaptive Sensory-Motor Control of Vision"**__ and CLIP-based zero-shot methods **Radford, et al., "Learning Transferable Visual Models from Natural Language Supervision"**__. For traditional neural network-based methods, learning high-performance models for event data presents significant challenges due to the asynchronous nature of events and the limited availability of large-scale labeled datasets **Vondrick, et al., "Tracking Emerges by Learning to Relate Actions"**__. Moreover, these methods struggle with recognizing new event categories, as retraining large models for every new category is computationally expensive and impractical **Feichtenhofer, et al., "Detect to Track and Track to Detect"**__. To address these challenges, researchers have proposed CLIP-based zero-shot methods, leveraging CLIP’s extensive pre-trained knowledge to bridge the modality gap between events, images, and texts. While these methods mitigate some limitations of traditional approaches, they remain inherently constrained by the characteristics of CLIP **Dosovitskiy, et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**__. In contrast to existing methods, this paper explores the potential of LLMs with their extensive pre-trained knowledge and advanced multi-modal understanding capabilities. We aim to train an event generator in a self-supervised manner by distilling knowledge from pre-trained LLMs. This approach seeks to overcome the limitations of scalability and the dependence on large-scale annotated datasets, addressing key challenges in event-based visual recognition.