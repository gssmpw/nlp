
\UseRawInputEncoding
\pdfoutput=1%For ARXIV
%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\usepackage{multirow}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}

% \copyrightyear{2025}
% \acmYear{2025}
% \setcopyright{acmlicensed}\acmConference[WWW Companion '25]{Companion Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
% \acmBooktitle{Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia}
% \acmDOI{10.1145/3701716.3717548}
% \acmISBN{979-8-4007-1331-6/2025/04}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{LLM-EvRep: Learning an LLM-Compatible Event Representation Using a Self-Supervised Framework}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Zongyou Yu}
% \authornote{Both authors contributed equally to this research.}
\orcid{0009-0005-1098-9793}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Beijing Technology and Business University}
  \city{Beijing}
  \country{China}}
\email{zongyou.yu@st.btbu.edu.cn}

\author{Qiang Qu}
\orcid{0000-0002-6648-5050}
\affiliation{%
  \institution{The University of Sydney}
  \city{Sydney}
  % \state{NSW}
  \country{Australia}}
\email{vincent.qu@sydney.edu.au}

\author{Qian Zhang}
\orcid{0009-0007-4824-3158}
\affiliation{%
  \institution{Beijing Technology and Business University}
  \city{Beijing}
  \country{China}}
\email{qian.zhang@st.btbu.edu.cn}

\author{Nan Zhang}
\orcid{0000-0003-4904-7857}
\affiliation{%
  \institution{Beijing Technology and Business University}
  \city{Beijing}
  \country{China}}
\email{20210504@btbu.edu.cn}

\author{Xiaoming Chen}
\authornote{Corresponding author.}
\orcid{0000-0002-7503-3021}
\affiliation{%
  \institution{Beijing Technology and Business University}
  \city{Beijing}
  \country{China}}
\email{xiaoming.chen@btbu.edu.cn}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Z. Yu et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    Recent advancements in event-based recognition have demonstrated significant promise, yet most existing approaches rely on extensive training, limiting their adaptability for efficient processing of event-driven visual content. Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot capabilities across diverse domains, but their application to event-based visual recognition remains largely unexplored. To bridge this gap, we propose \textbf{LLM-EvGen}, an event representation generator that produces LLM-compatible event representations \textbf{LLM-EvRep}, thereby enhancing the performance of LLMs on event recognition tasks. The generator is trained using a self-supervised framework, aligning the generated representations with semantic consistency and structural fidelity. Comprehensive experiments were conducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results demonstrate that our method, \textbf{LLM-EvRep}, outperforms the event-to-video method, E2VID, by 15.93\%, 0.82\%, and 50.21\%, respectively, in recognition tasks when evaluated using GPT-4o.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010224</concept_id>
       <concept_desc>Computing methodologies~Computer vision</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Computer vision}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Event Representation, Large Language Models (LLM), Multimodelity Models}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \includegraphics[width=\textwidth]{figure/teaser.png}
  \caption{Overview of the proposed self-supervised learning framework for training an LLM-agnostic representation generator, \textbf{LLM-EvGen}, which generates an LLM-compatible event-stream representation, \textbf{LLM-EvRep}.
    On the left, the raw event stream is first transformed into a 3-channel Tencode representation. This representation is then used to train \textbf{LLM-EvGen} through two key loss functions: a structural fidelity loss derived from the RGB frame and a semantic consistency loss guided by the LLM.
    On the right, the trained \textbf{LLM-EvGen} can be seamlessly applied to other LLMs without requiring additional fine-tuning. It produces \textbf{LLM-EvRep}, which enhances the performance of LLMs in event-based zero-shot object recognition tasks.}
  \Description{Diagram illustrating the proposed self-supervised learning framework, LLM-EvGen, which generates LLM-compatible event-stream representations (LLM-EvRep). The left side shows the process of transforming a raw event stream into a 3-channel Tencode representation. LLM-EvGen is then trained using structural fidelity loss (from an RGB frame) and semantic consistency loss (guided by an LLM). On the right, the trained LLM-EvGen is applied to other LLMs without additional fine-tuning, improving event-based zero-shot object recognition.}
  \label{fig:teaser}
\end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Event cameras, as bio-inspired sensors~\cite{gallego2020event}, have garnered significant attention in the computer vision community for their exceptional capabilities, including microsecond-level temporal resolution, high dynamic range (typically 140 dB compared to 60 dB of standard cameras), and low power consumption~\cite{gehrig2022high,son20174}. These advantages represent a paradigm shift from traditional frame-based imaging~\cite{gallego2020event,kong2024openess}, offering a novel approach to capturing visual information. Despite being in the early stages of development, event-driven visual perception has rapidly emerged as a critical area of research in contemporary computer vision~\cite{zheng2023deep}. Event-based vision has shown promising results across various applications, including object recognition~\cite{zheng2024eventdance,zhou2024eventbind}, semantic segmentation~\cite{kong2024openess,jia2023event}, detection~\cite{gehrig2024low,gehrig2023recurrent}, tracking~\cite{gallego2017event,gehrig2018asynchronous}, and optical flow estimation~\cite{lee2020spike,hagenaars2021self}.

Despite these advantages, leveraging event cameras for high-level visual tasks, such as object recognition, remains challenging. Existing approaches to event-based object recognition can be broadly categorized into two types: traditional neural network-based methods~\cite{zheng2024eventdance,su2023event} and CLIP-based zero-shot methods~\cite{wu2023eventclip,zhou2023clip,zhou2024eventbind}. Traditional neural network methods require extensive training, and due to the inherent limitations of neural networks, they are constrained to recognizing a limited set of categories~\cite{gehrig2024low}. To address these limitations, zero-shot methods~\cite{wu2023eventclip,zhou2024eventbind,zhou2023clip} have been proposed. While these approaches show promise in zero-shot open-world event-based object recognition, their reliance on CLIP introduces inherent limitations~\cite{yu2024can}. In contrast, large language models (LLMs) offer a compelling alternative, with richer pre-trained knowledge and superior zero-shot reasoning capabilities.

LLMs, with their exceptional zero-shot reasoning capabilities and vast pre-trained knowledge, have demonstrated remarkable success in multimodal tasks such as vision-language understanding and scene reasoning~\cite{zhu2023minigpt,fu2024scene}. Their ability to understand and generalize across diverse data modalities makes them a promising alternative to existing methods like CLIP for object recognition tasks. While LLMs have shown exceptional performance on 2D image-based content understanding, event-based visual content poses unique challenges due to its sparse, asynchronous, and modality-specific nature. These stark differences between event streams and traditional image-based inputs hinder LLMs' ability to directly comprehend event data. 

A critical challenge in applying LLMs to event-based vision is converting event streams into representations compatible with LLMs. Existing approaches to bridging this gap typically reconstruct event streams into more interpretable formats, falling into two main categories: event frame generation and event-to-video (E2V) techniques. The first category involves integrating events based on their spatial positions to generate ``event frames.'' The second category uses E2V methods, like E2VID~\cite{rebecq2019high} and E2HQV~\cite{qu2024e2hqv}, to reconstruct events into natural images, known as ``reconstructed frames.'' While these methods show promise in LLM-based zero-shot object recognition~\cite{yu2024can}, they do not specifically tailor event data to the unique characteristics of LLMs.

To address the limitations of existing methods and adapt event streams to LLMs, we propose \textbf{LLM-EvGen}, an event representation generator designed for producing LLM-compatible event representations. Inspired by E2HQV~\cite{qu2024e2hqv}, \textbf{LLM-EvGen} employs an encoder-decoder structure based on the MBConv and Fused MBConv layers from EfficientNetV2~\cite{tan2021efficientnetv2}, balancing computational efficiency and parameter utilization. The encoder-decoder structure provides a solid foundation for processing event data, but ensuring compatibility with LLMs requires careful supervision.As illustrated in Figure~\ref{fig:teaser}, we introduce a self-supervised learning framework that aligns event representations produced by \textbf{LLM-EvGen} with RGB frames, ensuring both semantic consistency and structural fidelity. Specifically, the event representations and their corresponding RGB frames are input into an LLM to extract semantic information, with consistency measured using Jaccard similarity~\cite{jaccard1901} as the \textbf{Semantic Consistency Loss}. To address early-stage noise in training, we propose a \textbf{Structural Fidelity Loss}, calculated as the mean squared error (MSE) between the Sobel edge maps~\cite{sobel1968} of \textbf{LLM-EvRep} and its RGB frames, enforcing structural consistency. \textbf{LLM-EvGen} requires training with only a single LLM and significantly improves event-based zero-shot object recognition performance. Extensive experiments validate the superiority of our approach in generating high-quality, LLM-compatible event representations and advancing event-based recognition tasks.

To sum up, our key contributions are as follows:
\begin{itemize}
    \item To the best of our knowledge, we are the first to explore how to enhance large LLMs' understanding of event-based visual content, bridging the gap between event-based data and LLMs.
    
    \item We propose \textbf{LLM-EvGen}, a novel event representation generator designed to produce LLM-compatible event representations, termed \textbf{LLM-EvRep}, which significantly improve LLM performance on event recognition tasks.
    
    \item We introduce a self-supervised learning framework that combines LLM-driven \textbf{Semantic Consistency Loss} and an auxiliary \textbf{Structural Fidelity Loss} to effectively train \textbf{LLM-EvGen}, ensuring both semantic alignment and structural consistency in the generated representations.
    
    \item Extensive experimental results demonstrate that our \textbf{LLM-EvRep} achieves higher recognition accuracy than hand-crafted and E2V methods across multiple benchmark datasets, setting a new benchmark for leveraging Large Language Models (LLMs) in event-based vision tasks.

\end{itemize}

\section{Related Work}
\noindent \textbf{LLMs for Visual Understanding.} Classical LLMs such as OpenAI's GPT-4/4 Turbo~\cite{openai2024gpt4}, Meta's LLaMA 3.1~\cite{dubey2024llama}, LLaVA~\cite{liu2024improved}, and MiniGPT-4-v2~\cite{chen2023minigptv2}, trained on vast amounts of cross-modal data, can be directly applied to various downstream tasks, such as object recognition and visual question answering, without the need for additional training or fine-tuning. Zang et al.~\cite{zang2024contextual} proposed ContextDET, a framework combining a visual encoder, pre-trained LLM, and visual decoder for context-aware object detection in human-AI interactions. Lv et al.~\cite{tang2024chain} introduced the Multimodal Camo-Perceptive Framework (MMCPF), using a Chain of Visual Perception strategy to improve zero-shot Camouflaged Object Detection (COD) with LLMs. Zhu et al.~\cite{zhu2024multi} developed a depth-aware Transformer to integrate object depth information for improved Visual Commonsense Reasoning (VCR) by considering 3D spatial relationships in visual and textual data.
Despite their impressive multimodal capabilities, LLMs face challenges in understanding event-driven visual content due to the substantial modality differences from traditional images. In this work, we leverage the superior cross-modal understanding capabilities of LLMs to train an LLM-compatible event representation generator. This generator effectively bridges the modality gap between events and RGB frames, thereby enhancing the ability of LLMs to comprehend event-based visual content.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.35\textwidth]{figure/LLM-EvGen-figure.png}
  \caption{Structure of the Proposed LLM-EvGen. An encoder-decoder architecture consisting of MBConv and Fused MBConv~\cite{tan2021efficientnetv2} layers.}
  \Description{Diagram illustrating the structure of the proposed LLM-EvGen, an encoder-decoder architecture. The network begins with a raw event stream input, which is first processed through two 3×3 convolution layers. It then passes through multiple stages, including max pooling, and a series of MBConv and Fused MBConv layers. The decoder follows a bilinear upsampling approach, also incorporating MBConv and Fused MBConv layers. The final output is generated using a 1×1 convolution.}
  \label{fig:network framework}
\end{figure}

\noindent \textbf{Event-based Object Recognition.} Event cameras, characterized by their low latency, high dynamic range, and low power consumption~\cite{gehrig2022high,son20174}, have gained prominence in computer vision applications and have shown promising results in object recognition. Existing object recognition approaches can be broadly classified into traditional neural network-based methods~\cite{zheng2024eventdance,su2023event} and CLIP-based zero-shot methods~\cite{ wu2023eventclip,zhou2023clip,zhou2024eventbind}. For traditional neural network-based methods, learning high-performance models for event data presents significant challenges due to the asynchronous nature of events and the limited availability of large-scale labeled datasets~\cite{wu2023eventclip}. Moreover, these methods struggle with recognizing new event categories, as retraining large models for every new category is computationally expensive and impractical~\cite{zhou2024eventbind,gehrig2024low}. To address these challenges, researchers have proposed CLIP-based zero-shot methods, leveraging CLIP’s extensive pre-trained knowledge to bridge the modality gap between events, images, and texts. While these methods mitigate some limitations of traditional approaches, they remain inherently constrained by the characteristics of CLIP~\cite{yu2024can}. In contrast to existing methods, this paper explores the potential of LLMs with their extensive pre-trained knowledge and advanced multi-modal understanding capabilities. We aim to train an event generator in a self-supervised manner by distilling knowledge from pre-trained LLMs. This approach seeks to overcome the limitations of scalability and the dependence on large-scale annotated datasets, addressing key challenges in event-based visual recognition.


\section{Method}
In this section, we provide a detailed introduction to the framework for generating LLM-compatible event stream representations through self-supervised learning. The process starts by converting the raw event stream into a neural network-friendly representation, Tencode~\cite{huang2023eventpoint}. This representation is then fed into our LLM-compatible Event Representation Generator, \textbf{LLM-EvGen}, an encoder-decoder network inspired by E2HQV~\cite{qu2024e2hqv} and EfficientNetV2~\cite{tan2021efficientnetv2} (see Section~\ref{section3.1}). Subsequently, \textbf{LLM-EvGen} is trained using a self-supervised framework based on semantic consistency, enabling it to produce LLM-compatible event representations, referred to as \textbf{LLM-EvRep} (see Section~\ref{section3.2}). To accelerate model convergence and enhance the quality of representations, an additional structural fidelity loss is introduced during training. This ensures that the generated representations not only retain structural consistency but also adapt effectively to LLMs, thereby facilitating improved performance in event-based recognition tasks performed by LLMs.

\subsection{Learnable Event Representation Generator}
\label{section3.1}
Inspired by E2HQV~\cite{qu2024e2hqv}, the proposed \textbf{LLM-EvGen} adopts a U-Net-like architecture, as shown in Figure~\ref{fig:network framework}. Following this design, \textbf{LLM-EvGen} incorporates the MBConv and Fused MBConv layers from EfficientNetV2~\cite{tan2021efficientnetv2}. These layers are chosen for their optimal balance between training efficiency and parameter utilization~\cite{qu2024e2hqv}, making it well-suited for our model.

Before feeding the discretized event stream into \textbf{LLM-EvGen}, we first convert it into a neural network-compatible representation. Following Huang et al.~\cite{huang2023eventpoint}, we transform the event stream into a structured frame representation, denoted as $F$, as shown in Figure~\ref{fig:teaser}. Then the event stream representation $F$ is processed by the encoder of the proposed \textbf{LLM-EvGen}, which extracts hierarchical features while progressively reducing the spatial resolution. The encoder begins with two $3\times3$ convolutional layers to capture low-level features and improve spatial representations. Subsequently, the network incorporates a sequence of MaxPooling layers combined with Fused MBConv and MBConv layers, inspired by EfficientNetV2~\cite{tan2021efficientnetv2} and E2HQV~\cite{qu2024e2hqv}. These layers effectively balance computational efficiency and feature extraction capability, enabling the encoder to capture both local and global features.

As the feature maps flow through the network, the spatial resolution is progressively reduced using MaxPooling, while the depth of the features is increased via MBConv layers with varying numbers of repetitions. This design allows the network to extract complex representations at multiple scales, facilitating robust feature learning from the event data.

In the decoder, the feature maps are gradually upsampled using bilinear interpolation, followed by MBConv and Fused MBConv layers to refine the features and recover spatial resolution. Skip connections between the encoder and decoder ensure that low-level spatial details are preserved and fused with high-level semantic information during reconstruction. Finally, $1\times1$ convolutional layers are employed to reduce the channel dimensions and generate the final output representation.
This hierarchical encoding-decoding architecture enables \textbf{LLM-EvGen} to effectively process the event-based input $F$, capturing both the fine-grained details and high-level semantics necessary for the downstream tasks.

\subsection{Self-supervised Learning Framework}
\label{section3.2}
To make \textbf{LLM-EvGen} compatible with LLMs, we propose a self-supervised framework based on semantic consistency . Specifically, as illustrated in Figure~\ref{fig:teaser}, after processing $F$ with \textbf{LLM-EvGen}, the model generates corresponding \textbf{LLM-EvRep}. To extract semantic information from \textbf{LLM-EvRep}, we directly input it into the LLM (in our case, LLaVA~\cite{liu2023improvedllava}) and obtain its semantic information. Simultaneously, the corresponding RGB frame (denoted as RGB) is also fed into the LLM to generate a comparable semantic information, this processing can be describe as following:

\begin{equation}
\begin{aligned}
f^{e} = \text{LLM}(\text{LLM-EvRep}) \\
f^{r} = \text{LLM}(\text{RGB})
\end{aligned}
\end{equation}

\noindent where $f^{e}$ represents semantic information of the \textbf{LLM-EvRep} and $f^{r}$ is semantic information of its corresponding RGB frame.

To ensure semantic alignment between \textbf{LLM-EvRep} and their corresponding RGB frame, we define a \textbf{semantic consistency loss} based on the semantic information generated by the LLM. Specifically, the semantic information $f^e$ and $f^r$ are the textual outputs produced by the LLM when processing \textbf{LLM-EvRep} and the corresponding RGB frame, respectively. Our goal is to minimize the semantic discrepancy between these two pieces of semantic information.

To achieve this, we compute a Jaccard similarity-based~\cite{jaccard1901} \textbf{Semantic Consistency Loss}. First, $f^e$ and $f^r$ are tokenized into sets of words, denoted as $W_e$ and $W_r$, respectively:

\begin{equation}
\begin{aligned}
W_e = \text{set}(\text{tokenize}(f^e)), \quad W_r = \text{set}(\text{tokenize}(f^r)).
\end{aligned}
\end{equation}

The semantic similarity between these two sets is measured using the Jaccard similarity, which is defined as the \textbf{Semantic Consistency Loss}. This loss quantifies the dissimilarity between the textual representations generated by the LLM from \textbf{LLM-EvRep} and its corresponding RGB-based inputs:

\begin{equation} L_{\text{semantic}} = 1 - \frac{|W_e \cap W_r|}{|W_e \cup W_r|}. \end{equation}

\noindent where $|W_e \cap W_r|$ and $|W_e \cup W_r|$ represent the sizes of the intersection and union of $W_e$ and $W_r$, respectively. This loss penalizes semantic differences, encouraging \textbf{LLM-EvGen} to produce event representations that are semantically consistent with their corresponding RGB frames.

By aligning the semantic consistency between \textbf{LLM-EvRep} and the corresponding RGB frame, the proposed framework effectively guides \textbf{LLM-EvGen} to generate event representations that are semantically aligned to their RGB counterparts. 
% This alignment enhances the compatibility of the generated representations with LLMs, significantly improving their ability to comprehend event data and perform effectively in event-based recognition tasks.

However, due to the U-Net-like architecture of \textbf{LLM-EvGen}, the event representations generated during the early stages of training often contain significant noise. This noise can hinder the LLM's ability to extract meaningful semantic information from these representations. To address this issue, we introduce an auxiliary weak supervision mechanism that enforces structural consistency between the event-based representations and their corresponding RGB frames.

Specifically, as presented in Figure~\ref{fig:teaser}, we design an auxiliary \textbf{Structural Fidelity Loss}, which leverages sobel edge detection~\cite{sobel1968} to preserve spatial and structural details in the generated event-based frames. The sobel edge detection method computes the gradient magnitude of an image in both the horizontal and vertical directions using predefined sobel kernels. For an input image $\mathbf{I}$, the gradients in the $x$- and $y$-directions, denoted as $G_x$ and $G_y$, are computed as:

\begin{equation}
\begin{aligned}
G_x = \mathbf{I} * K_x, \quad G_y = \mathbf{I} * K_y,
\end{aligned}
\end{equation}

\noindent where $K_x$ and $K_y$ are the sobel kernels for the horizontal and vertical directions, respectively, and $*$ denotes the convolution operation. The gradient magnitude is then computed as:

\begin{equation}
\begin{aligned}
G = \sqrt{G_x^2 + G_y^2}.
\end{aligned}
\end{equation}

This gradient magnitude map highlights the edges in the image, capturing the structural information present in the input. To ensure consistency between the structural details of the generated \textbf{LLM-EvRep} and its corresponding RGB frames, we define the \textbf{Structural Fidelity Loss} as the mean squared error (MSE) between their sobel edge map:

\begin{equation}
\begin{aligned}
L_{\text{fidelity}} = \text{MSE}(\text{sobel}(\mathbf{O}), \text{sobel}(\mathbf{T})),
\end{aligned}
\end{equation}

\noindent where $\mathbf{O}$ and $\mathbf{T}$ represent the sobel edge maps extracted from \textbf{LLM-EvRep} and its corresponding RGB frame, respectively, and $\text{sobel}(\cdot)$ computes the sobel edge map of the input.

To jointly optimize the semantic alignment and structural consistency of the \textbf{LLM-EvRep}, we combine the \textbf{Semantic Consistency Loss} and the \textbf{Structural Fidelity Loss} into a unified objective, referred to as the \textbf{Dual Alignment Loss}:


\begin{equation}
\begin{aligned}
L_{\text{dual}} = \lambda L_{\text{semantic}} + \gamma L_{\text{fidelity}},
\end{aligned}
\end{equation}

\noindent where $\lambda$ and $\gamma$ are weighting factors that balance the contributions of the two losses. The \textbf{Semantic Consistency Loss} ensures that \textbf{LLM-EvRep} generated by \textbf{LLM-EvGen} are semantically aligned with their RGB counterparts, while the \textbf{Structural Fidelity Loss} enforces spatial and structural consistency in the representations.

This dual-loss strategy integrates semantic alignment and structural precision, effectively mitigating noise during the early stages of training. By doing so, it enhances the robustness of the learned representations and accelerates model convergence. The \textbf{Dual Alignment Loss} ensures that \textbf{LLM-EvGen} produces high-quality event representations that are both semantically meaningful and structurally precise.

% The proposed self-supervised framework leverages the complementary strengths of the \textbf{Semantic Consistency Loss} and the \textbf{Structural Fidelity Loss} to generate LLM-compatible event representations. This approach improves the compatibility of event representations with LLMs, enabling accurate and efficient performance across a wide range of event-based recognition tasks.

\begin{table}[h]
\centering
\caption{Comparison of Classification Accuracy Across N-ImageNet (N-I), N-Caltech101 (N-C), and N-MNIST (N-M) Datasets. Due to the prompt length limitations of LLaVA and MiniGPT-4-v2, we are unable to evaluate their recognition performance on the N-ImageNet dataset. \textbf{Bold values indicate the best performance for each dataset.}}
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Input} & \multicolumn{3}{c}{Accuracy (\%)} \\ \cline{3-5} 
                        &                             & N-I  & N-C  & N-M  \\ \hline
ECLIP                   & Event frame                       & 8.72        & 53.88         & 14.56      \\
EventCLIP               & Event stream                       & 4.30        & 49.95         & 11.87      \\ 
EventBind               & Event frame                      & 3.05        & 67.58         & 12.89      \\  \hline 
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}MiniGPT-4-v2\textsuperscript{*}\end{tabular}} 
                        & Event frame                         & -           & 23.10         & 15.29      \\ 
                        & E2VID                       & -           & 30.20         & 10.70      \\ 
                        & E2HQV                       & -           & 30.71         & 11.62      \\ 
                        & LLM-EvRep (ours)            & -           & \textbf{33.68}             & \textbf{17.53}          \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}LLaVA\textsuperscript{*}\end{tabular}}    
                        & Event frame                         & -           & 64.72         & 56.27      \\ 
                        & E2VID                       & -           & 67.26         & 19.57      \\ 
                        & E2HQV                       & -           & 67.51         & 22.94      \\ 
                        & LLM-EvRep (ours)            & -           & \textbf{69.94}             & \textbf{88.07}          \\ \hline
\multirow{4}{*}{GPT-4turbo} 
                        & Event frame                         & 17.23       & 72.08         & 85.95      \\ 
                        & E2VID                       & 11.61       & 55.08         & 39.14      \\ 
                        & E2HQV                       & 10.70       & 55.58         & 39.41      \\ 
                        & LLM-EvRep (ours)            & \textbf{20.02}           & \textbf{75.32}             & \textbf{92.39}          \\ \hline
\multirow{4}{*}{GPT-4o} 
                        & Event frame                         & 45.55       & 89.37         & 91.86      \\ 
                        & E2VID                       & 32.75       & 93.90 & 49.79      \\ 
                        & E2HQV                       & 30.05       & 93.23         & 51.68      \\ 
                        & LLM-EvRep (ours)            & \textbf{48.68}           & \textbf{94.72}             & \textbf{100}          \\ \hline
\end{tabular}
\label{tab:comparison_accuracy}
\footnotesize \textsuperscript{*}Open-source model
\end{table}


\section{Experimental}
\subsection{Setup}
\noindent \textbf{Evaluation LLMs.}
We evaluated the recognition accuracy of various event representations across two open-source models, LLaVA~\cite{liu2023improvedllava} and MiniGPT-4-v2~\cite{chen2023minigptv2}, and two proprietary models, GPT-4o and GPT-4 Turbo~\cite{openai2024gpt4}. Furthermore, we compared the performance of three CLIP-based zero-shot methods: ECLIP~\cite{zhou2023clip}, EventCLIP~\cite{wu2023eventclip}, and EventBind~\cite{zhou2024eventbind}.

\noindent \textbf{Training datasets.} We use the N-ImageNet~\cite{kim2021n} dataset to train our \textbf{LLM-EvGen}. N-ImageNet is derived from the ImageNet-1K~\cite{deng2009imagenet} dataset, where RGB images are displayed on a monitor and captured by a moving event camera. The dataset consists of 1,781,167 event streams with a resolution of $480\times640$, covering 1,000 unique object classes. We split the dataset into training and testing sets with a ratio of 5:1, where the testing set is used for evaluation experiments.

\noindent \textbf{Downstream tasks datasets.} To evaluate the performance of \textbf{LLM-EvRep}, we conduct experiments on object recognition task. We utilize the N-ImageNet~\cite{kim2021n}, N-Caltech101~\cite{fei2004learning}, and N-MNIST~\cite{orchard2015converting} datasets. 

\noindent \textbf{Implementation Details.}  
We utilize LLaVA~\cite{liu2024improved} to train our \textbf{LLM-EvGen}, with the weights of LLaVA frozen throughout the training process. The \textbf{LLM-EvGen} is trained for 50 epochs with a batch size of 16 and a learning rate of $1\mathrm{e}{-4}$, using the Adam optimizer~\cite{kingma2014adam}. All experiments are conducted on an NVIDIA RTX 3090 GPU.

\subsection{Results}
Table~\ref{tab:comparison_accuracy} presents the quantitative evaluation results. As shown, compared to CLIP-based zero-shot methods, our proposed \textbf{LLM-EvRep} achieves an accuracy of 94.72\% on the N-Caltech101 dataset using GPT-4o, surpassing the state-of-the-art zero-shot method EventBind~\cite{zhou2024eventbind} by 27.14\%. On the N-ImageNet dataset, our method outperforms ECLIP~\cite{zhou2023clip} by nearly six orders of magnitude, reaching an accuracy of almost 50\%. These results clearly demonstrate that our LLM-based approach exhibits superior performance compared to traditional CLIP-based zero-shot methods in event-driven vision tasks. 

Further examining Table~\ref{tab:comparison_accuracy}, we used the representative open-source models LLaVA~\cite{liu2024improved} and MiniGPT-4-v2~\cite{chen2023minigptv2} for recognition tasks. The results show that our proposed \textbf{LLM-EvRep} method outperforms both hand-crafted and E2V methods in recognition accuracy. Specifically, experiments on the N-Caltech101 dataset show that \textbf{LLM-EvRep} achieves an accuracy improvement of 5.22\% over the hand-crafted event frames, and surpasses the V2E methods, E2VID and E2HQV, by 2.68\% and 2.43\%, respectively. Furthermore, results on the N-MNIST dataset indicate that \textbf{LLM-EvRep} improves accuracy by 31.80\% over the event frames and is approximately four times more accurate than E2VID and E2HQV. These results demonstrate that our method effectively enhances the image understanding ability of open-source LLMs.

\begin{table}[]
\caption{Comparison of accuracy between the proposed LLM-EvRep and Tencode representations on the N-Caltech101 and N-MNIST datasets, evaluated using LLaVA and GPT-4o.}
\begin{tabular}{c|c|c|c}
\hline
       & Input     & N-Caltech101 & N-MNIST \\ \hline
\multirow{2}{*}{LLaVA\textsuperscript{*}}  
       & Tencode   & 68.18           & 66.97 \\ \cline{2-4}
       & LLM-EvRep & \textbf{69.94}        & \textbf{88.07 }\\ \hline
\multirow{2}{*}{GPT-4o} 
       & Tencode   & 89.34            & 99.38 \\ \cline{2-4}
       & LLM-EvRep & \textbf{94.72}        & \textbf{100} \\ \hline
\end{tabular}
\label{tab:comparison_with_Tencode}
\end{table}

At the same time, we also conducted recognition experiments on the representative proprietary models GPT-4o and GPT-4uurbo. The experimental results show that \textbf{LLM-EvRep} outperforms both the hand-crafted and E2V methods across the three experimental datasets. Notably, \textbf{LLM-EvRep} achieved an astonishing 100\% recognition accuracy on the N-MNIST dataset, which is nearly twice as high as the two E2V methods. This strongly demonstrates the superior performance of our method on simpler datasets.

Meanwhile, we also compare the accuracy with Tencode~\cite{huang2023eventpoint}, as shown in Table~\ref{tab:comparison_with_Tencode}. The results in Table~\ref{tab:comparison_with_Tencode} indicate that, whether evaluated on the open-source model LLaVA or the proprietary model GPT-4o, our proposed \textbf{LLM-EvRep} consistently outperforms Tencode in terms of accuracy. These findings further validate the effectiveness of our approach.

In summary, the experimental results across multiple datasets and models consistently demonstrate the superior performance of our proposed \textbf{LLM-EvRep} method in event-driven vision tasks. Compared to existing CLIP-based zero-shot methods, hand-crafted approaches, and state-of-the-art E2V techniques, our method achieves higher recognition accuracy. These findings highlight the potential of integrating LLMs with event stream representations, offering a promising direction for advancing the capabilities of event-driven visual tasks.




\section{Conclusion}
In this paper, we introduce \textbf{LLM-EvGen}, a novel event representation generator specifically designed for large language models (LLMs). To effectively train LLM-EvGen, we propose a self-supervised learning framework based on a dual alignment approach that ensures both \textbf{LLM Semantic Consistency} and \textbf{Structural Fidelity}. This framework leverages two key components: a semantic alignment loss that ensures the generated event representations are semantically coherent with the corresponding RGB frames, and a structural fidelity loss that preserves the spatial integrity of the event data. Extensive experimental evaluations demonstrate the effectiveness of our proposed approach.

% \begin{acks}
% This work was supported in part by \grantsponsor{}{National Natural Science Foundation of China}{} under Grant \grantnum{}{62177001}.
% \end{acks}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
% \input{sampl-base.bbl}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
