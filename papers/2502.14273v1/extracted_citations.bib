@article{chen2023minigptv2,
  title={MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-Task Learning},
  author={Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2310.09478},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{gehrig2022high,
  title={Are high-resolution event cameras really needed?},
  author={Gehrig, Daniel and Scaramuzza, Davide},
  journal={arXiv preprint arXiv:2203.14672},
  year={2022}
}

@article{gehrig2024low,
  title={Low-latency automotive vision with event cameras},
  author={Gehrig, Daniel and Scaramuzza, Davide},
  journal={Nature},
  volume={629},
  number={8014},
  pages={1034--1040},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{liu2024improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}

@article{openai2024gpt4,
  title={GPT-4 Technical Report},
  author={{OpenAI}},
  year={2024},
  url={https://arxiv.org/abs/2303.08774}
}

@inproceedings{son20174,
  title={4.1 A 640$\times$ 480 dynamic vision sensor with a 9$\mu$m pixel and 300Meps address-event representation},
  author={Son, Bongki and Suh, Yunjae and Kim, Sungho and Jung, Heejae and Kim, Jun-Seok and Shin, Changwoo and Park, Keunju and Lee, Kyoobin and Park, Jinman and Woo, Jooyeon and others},
  booktitle={2017 IEEE International Solid-State Circuits Conference (ISSCC)},
  pages={66--67},
  year={2017},
  organization={IEEE}
}

@inproceedings{su2023event,
  title={Event-based object recognition using feature fusion and spiking neural networks},
  author={Su, Menghao and Yang, Panpan and Jiang, Runhao and Yan, Rui},
  booktitle={International Conference on Neural Information Processing},
  pages={470--482},
  year={2023},
  organization={Springer}
}

@inproceedings{tan2021efficientnetv2,
  title={Efficientnetv2: Smaller models and faster training},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={10096--10106},
  year={2021},
  organization={PMLR}
}

@inproceedings{tang2024chain,
  title={Chain of visual perception: Harnessing multimodal large language models for zero-shot camouflaged object detection},
  author={Tang, Lv and Jiang, Peng-Tao and Shen, Zhi-Hao and Zhang, Hao and Chen, Jin-Wei and Li, Bo},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={8805--8814},
  year={2024}
}

@article{wu2023eventclip,
  title={Eventclip: Adapting clip for event-based object recognition},
  author={Wu, Ziyi and Liu, Xudong and Gilitschenski, Igor},
  journal={arXiv preprint arXiv:2306.06354},
  year={2023}
}

@article{yu2024can,
  title={Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot Event-based Recognition},
  author={Yu, Zongyou and Qu, Qiang and Chen, Xiaoming and Wang, Chen},
  journal={arXiv preprint arXiv:2409.09628},
  year={2024}
}

@article{zang2024contextual,
  title={Contextual object detection with multimodal large language models},
  author={Zang, Yuhang and Li, Wei and Han, Jun and Zhou, Kaiyang and Loy, Chen Change},
  journal={International Journal of Computer Vision},
  pages={1--19},
  year={2024},
  publisher={Springer}
}

@inproceedings{zheng2024eventdance,
  title={EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition},
  author={Zheng, Xu and Wang, Lin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={17448--17458},
  year={2024}
}

@article{zhou2023clip,
  title={E-clip: Towards label-efficient event-based open-world understanding by clip},
  author={Zhou, Jiazhou and Zheng, Xu and Lyu, Yuanhuiyi and Wang, Lin},
  journal={arXiv preprint arXiv:2308.03135},
  year={2023}
}

@article{zhou2024eventbind,
  title={Eventbind: Learning a unified representation to bind them all for event-based open-world understanding},
  author={Zhou, Jiazhou and Zheng, Xu and Lyu, Yuanhuiyi and Wang, Lin},
  year={2024},
  publisher={arxiv}
}

@article{zhu2024multi,
  title={Multi-modal large language model enhanced pseudo 3d perception framework for visual commonsense reasoning},
  author={Zhu, Jian and Wang, Hanli and Shi, Miaojing},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2024},
  publisher={IEEE}
}

