\section{Related Work}
\noindent \textbf{LLMs for Visual Understanding.} Classical LLMs such as OpenAI's GPT-4/4 Turbo~\cite{openai2024gpt4}, Meta's LLaMA 3.1~\cite{dubey2024llama}, LLaVA~\cite{liu2024improved}, and MiniGPT-4-v2~\cite{chen2023minigptv2}, trained on vast amounts of cross-modal data, can be directly applied to various downstream tasks, such as object recognition and visual question answering, without the need for additional training or fine-tuning. Zang et al.~\cite{zang2024contextual} proposed ContextDET, a framework combining a visual encoder, pre-trained LLM, and visual decoder for context-aware object detection in human-AI interactions. Lv et al.~\cite{tang2024chain} introduced the Multimodal Camo-Perceptive Framework (MMCPF), using a Chain of Visual Perception strategy to improve zero-shot Camouflaged Object Detection (COD) with LLMs. Zhu et al.~\cite{zhu2024multi} developed a depth-aware Transformer to integrate object depth information for improved Visual Commonsense Reasoning (VCR) by considering 3D spatial relationships in visual and textual data.
Despite their impressive multimodal capabilities, LLMs face challenges in understanding event-driven visual content due to the substantial modality differences from traditional images. In this work, we leverage the superior cross-modal understanding capabilities of LLMs to train an LLM-compatible event representation generator. This generator effectively bridges the modality gap between events and RGB frames, thereby enhancing the ability of LLMs to comprehend event-based visual content.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.35\textwidth]{figure/LLM-EvGen-figure.png}
  \caption{Structure of the Proposed LLM-EvGen. An encoder-decoder architecture consisting of MBConv and Fused MBConv~\cite{tan2021efficientnetv2} layers.}
  \Description{Diagram illustrating the structure of the proposed LLM-EvGen, an encoder-decoder architecture. The network begins with a raw event stream input, which is first processed through two 3×3 convolution layers. It then passes through multiple stages, including max pooling, and a series of MBConv and Fused MBConv layers. The decoder follows a bilinear upsampling approach, also incorporating MBConv and Fused MBConv layers. The final output is generated using a 1×1 convolution.}
  \label{fig:network framework}
\end{figure}

\noindent \textbf{Event-based Object Recognition.} Event cameras, characterized by their low latency, high dynamic range, and low power consumption~\cite{gehrig2022high,son20174}, have gained prominence in computer vision applications and have shown promising results in object recognition. Existing object recognition approaches can be broadly classified into traditional neural network-based methods~\cite{zheng2024eventdance,su2023event} and CLIP-based zero-shot methods~\cite{ wu2023eventclip,zhou2023clip,zhou2024eventbind}. For traditional neural network-based methods, learning high-performance models for event data presents significant challenges due to the asynchronous nature of events and the limited availability of large-scale labeled datasets~\cite{wu2023eventclip}. Moreover, these methods struggle with recognizing new event categories, as retraining large models for every new category is computationally expensive and impractical~\cite{zhou2024eventbind,gehrig2024low}. To address these challenges, researchers have proposed CLIP-based zero-shot methods, leveraging CLIP’s extensive pre-trained knowledge to bridge the modality gap between events, images, and texts. While these methods mitigate some limitations of traditional approaches, they remain inherently constrained by the characteristics of CLIP~\cite{yu2024can}. In contrast to existing methods, this paper explores the potential of LLMs with their extensive pre-trained knowledge and advanced multi-modal understanding capabilities. We aim to train an event generator in a self-supervised manner by distilling knowledge from pre-trained LLMs. This approach seeks to overcome the limitations of scalability and the dependence on large-scale annotated datasets, addressing key challenges in event-based visual recognition.