[
  {
    "index": 0,
    "papers": [
      {
        "key": "han2024parameter",
        "author": "Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian",
        "title": "Parameter-efficient fine-tuning for large models: A comprehensive survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "houlsby2019parameter",
        "author": "Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",
        "title": "Parameter-efficient transfer learning for NLP"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2021prefix",
        "author": "Li, Xiang Lisa and Liang, Percy",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "hulora",
        "author": "Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dettmers2024qlora",
        "author": "Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke",
        "title": "Qlora: Efficient finetuning of quantized llms"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "hayoulora",
        "author": "Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin",
        "title": "LoRA+: Efficient Low Rank Adaptation of Large Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "kopiczkovera",
        "author": "Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki M",
        "title": "VeRA: Vector-based Random Matrix Adaptation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jacobs1991adaptive",
        "author": "Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E",
        "title": "Adaptive mixtures of local experts"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "lepikhin2020gshard",
        "author": "Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng",
        "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "dai2024deepseekmoe",
        "author": "Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others",
        "title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "roller2021hash",
        "author": "Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others",
        "title": "Hash layers for large sparse models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "huang2024harder",
        "author": "Huang, Quzhe and An, Zhenwei and Zhuang, Nan and Tao, Mingxu and Zhang, Chen and Jin, Yang and Xu, Kun and Chen, Liwei and Huang, Songfang and Feng, Yansong",
        "title": "Harder Tasks Need More Experts: Dynamic Routing in MoE Models"
      },
      {
        "key": "zhou2022mixture",
        "author": "Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others",
        "title": "Mixture-of-experts with expert choice routing"
      },
      {
        "key": "yang2024xmoe",
        "author": "Yang, Yuanhang and Qi, Shiyi and Gu, Wenchao and Wang, Chaozheng and Gao, Cuiyun and Xu, Zenglin",
        "title": "XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "riquelme2021scaling",
        "author": "Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\\'e} and Keysers, Daniel and Houlsby, Neil",
        "title": "Scaling vision with sparse mixture of experts"
      },
      {
        "key": "mustafa2022multimodal",
        "author": "Mustafa, Basil and Riquelme, Carlos and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil",
        "title": "Multimodal contrastive learning with limoe: the language-image mixture of experts"
      },
      {
        "key": "du2022glam",
        "author": "Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others",
        "title": "Glam: Efficient scaling of language models with mixture-of-experts"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "yang2024moral",
        "author": "Yang, Shu and Ali, Muhammad Asif and Wang, Cheng-Long and Hu, Lijie and Wang, Di",
        "title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning"
      },
      {
        "key": "wu2024parameter",
        "author": "Wu, Haoyuan and Zheng, Haisheng and He, Zhuolun and Yu, Bei",
        "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2024mixlora",
        "author": "Li, Dengchun and Ma, Yingzi and Wang, Naizheng and Cheng, Zhiyuan and Duan, Lei and Zuo, Jie and Yang, Cal and Tang, Mingjie",
        "title": "Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts"
      },
      {
        "key": "wumixture",
        "author": "Wu, Xun and Huang, Shaohan and Wei, Furu",
        "title": "Mixture of LoRA Experts"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "liu2024moe",
        "author": "Liu, Qidong and Wu, Xian and Zhao, Xiangyu and Zhu, Yuanshao and Xu, Derong and Tian, Feng and Zheng, Yefeng",
        "title": "When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zeng2024adamoe",
        "author": "Zeng, Zihao and Miao, Yibo and Gao, Hongcheng and Zhang, Hao and Deng, Zhijie",
        "title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "dou2024loramoe",
        "author": "Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Shen, Wei and Xiong, Limao and Zhou, Yuhao and Wang, Xiao and Xi, Zhiheng and Fan, Xiaoran and others",
        "title": "LoRAMoE: Alleviating world knowledge forgetting in large language models via MoE-style plugin"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "gao2024higher",
        "author": "Gao, Chongyang and Chen, Kezhen and Rao, Jinmeng and Sun, Baochen and Liu, Ruibo and Peng, Daiyi and Zhang, Yawen and Guo, Xiaoyuan and Yang, Jie and Subrahmanian, VS",
        "title": "Higher layers need more lora experts"
      }
    ]
  }
]