\section{Motivation}\label{sec:motivation}
\subsection{Observation and Analysis}\label{subsec:observation}
\begin{figure}[t]
  \centering
  \subfigure[Distribution of output values]{
    \label{fig:dis}
    \includegraphics[width=0.97\linewidth]{figures/distribution_log_ranges.pdf}
  }
  % \vspace{-5pt}
  \subfigure[Cumulative distribution of output values]{
    \label{fig:cumdis}
    \includegraphics[width=0.97\linewidth]{figures/cumulative_distribution_log_ranges.pdf}
  }
  \caption{Analysis of output values between original model weight matrices and adapters cross layers}\label{fig:distribution}
  % \vspace{-10pt}
\end{figure}

Without loss of generality, in the case of a particular layer of the model's network, LoRA-based fine-tuning involves adding the two output vectors of the original network and the LoRA adapter, the resulting vector will be the final output of this layer and subsequently fed into the next layer for further computation, i.e., $y=wx+\Delta wx$, where $w$ is the original weight matrix and $\Delta w$ is the adapter. Similarly, in the mixture of adapter experts architecture, the output vectors of multiple adapter experts are performed weighted summation according to the activation states, which will be added to the output vector of the original network. 

The outputs of original networks and adapter experts are tracked during model inference for further investigation. The model configuration is as followings: using Llama\,2-7B \cite{touvron2023llama} as the base model, allocating each network layer with 8 adapter experts of all ranks 8, employing the Top-2 activation strategy, and fine-tuned on the ScienceQA dataset \cite{lu2022learn}. When inputting a randomly selected subset of the test samples into the model, the distributions of the output values of Feed-Forward Neural Network (FFN) layers and corresponding adapter expert layers are presented in Figure \ref{fig:distribution}. 

The following trends be observed: 1) the output of the original network exceeds the output of its adapter experts by more than one or two orders of magnitude in the range with the highest concentration of values within the same network layer; 2) for the adapter experts’ outputs, shallow layers exhibit a higher proportion of small values close to zero compared to deep layers. These trends occur because the shallow layers of the model typically perform general feature extraction, while the deeper layers are responsible for learning specialized features. For a new fine-tuning task, the original networks in the shallow layers are capable of performing basic feature extraction to a certain extent, thereby minimizing the need for fine-tuning these layers. Therefore, with the same size of trainable parameters, allocating fewer adapter experts to the shallow layer and more adapter experts to the deep layer, rather than equally, is a straightforward and effective allocation strategy to maximize the effectiveness of such parameters. The output values distribution of MoLA (i.e., assigning fewer adapters to shallow layers) under the same fine-tuning settings, except for the adapter allocations, are present in Figure \ref{fig:3bar}. Compared to the equal allocation of experts, such optimized allocation way reduces the proportion of small output values from shallow-layer experts. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/3bars.pdf}
    \caption{\textbf{Proportions of values (\textless$10^{-3}$) of vanilla method, MoLA, and \name}. (1) the proportions of three methods in the 1st layer are very close; (2) except 1st layer, MoLA has lower proportions than Vanilla and \name in 4 and 2 layers, respectively; (3) \name exhibits lower proportions than Vanilla and MoLA in 5 layers.}
    \label{fig:3bar}
    % \vspace{-10pt}
\end{figure}

However, in MoE architectures, assuming all other configurations remain unchanged, the number of experts within a reasonable range is generally expected to exhibit a positive correlation with the capability of this MoE layer. From this perspective, strategies that directly reduce the number of adapter experts fail to consider the influence of the individual expert’s capacity. On the one hand, the size of trainable parameters is not only proportional to the number of experts but also to the rank of the adapter experts. The rank determines the parameter size of each adapter expert, which directly impacts its fitting ability. On the other hand, when the number of experts is fixed, there is an upper limit to improving the fitting ability of this MoE layer by simply increasing the parameter size of experts. Specifically, once an individual expert’s fitting ability exceeds the layer’s requirements, further increasing its parameter size provides diminishing returns about model capability. In summary, the number of experts and the capacity of individual experts jointly influence the overall performance of the MoE layer.

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/framework.pdf}
    \caption{\name Design}\label{fig:framework}
    \vspace{-5pt}
\end{figure*}

\subsection{Key Idea}\label{subsec:motivation}
Given the characteristic of the shallower to deeper layers of the model, there are requirement differences cross layers in the capability of individual adapter experts for fine-tuning. It exhibits incremental requirements for adapter experts from shallow to deep layers in fitting capability. And the parameters size, which decided by the rank of adapter, scales to the fitting capability. 

Therefore, inspired by the analysis above, this paper proposes a hierarchical rank setting scheme to adapter experts for efficient LLM fine-tuning, \name. When maintaining the same trainable parameter size with MoLA, the proportions of output values smaller than $10^{-3}$ in the shallow layers (layers 1 to 8) of \name are presented in Figure \ref{fig:3bar}. It indicates that \name can further reduce the proportion of such small values than MoLA. 

It notes that the final goal of model fine-tuning is not to minimize such proportions but to improve model accuracy or reduce trainable parameter size. We aim to provide an analytical perspective by this data analysis. The details and outperformances of \name will be presented in Section \ref{sec:methodology} and \ref{sec:evalution}, respectively.
