\section{Introduction}\label{sec:introduction}
In recent years, the rapid advancement of the Large Langue Model (LLM) has demonstrated impressive performance on various tasks, e.g., natural language processing, computer vision, and multimodal applications \cite{liu2024gpt,chang2024survey}. 
However, as the parameter size of large models continues to scale up, substantial computational resources are required for model fine-tuning. In this context, parameter-efficient fine-tuning (PEFT) methods have emerged \cite{han2024parameter}, significantly reducing the number of trainable parameters while maintaining performance. 
PEFT techniques include adapter tuning, prompt tuning, prefix tuning, and Low-Rank Adaptation (LoRA) \cite{houlsby2019parameter,lester2021power,li2021prefix,hulora}. Among these, LoRA has gained considerable attention for its innovation of injecting low-rank matrices (referred to as adapters) into the modelâ€™s weight matrices, enabling efficient fine-tuning for downstream tasks without introducing trainable parameters size substantially. 

Based on LoRA, several studies have further optimized it by incorporating the architecture of Mixture of Experts (MoE) \cite{liu2024moe,zeng2024adamoe,dou2024loramoe}. MoE achieves superior computational efficiency and model capacity by employing multiple feed-forward networks (referred to as experts) to decompose complex problems into smaller subproblems, which will be processed by a subset of experts selected by a gating network \cite{zhou2022mixture}. 
The integration of LoRA and MoE termed the mixture of adapter experts, which involves introducing multiple adapters for model's weight matrices and activating a subset of them selectively via a gating network, resulting in an efficient and scalable fine-tuning solution. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/adapter_params.pdf}
    \caption{\textbf{Optimization focus of adapter experts}. Existing works primarily optimize the number of adapters, whereas this paper explores both the number and rank of adapters to improve fine-tuning performance.}\label{fig:param}
    % \vspace{-5pt}
\end{figure}

The vanilla approach to constructing a mixture of adapter experts model involves assigning a fixed number of adapters to specific weight matrices of each layer, which neglects the variability and complexity across the layers of the pre-trained model. 
Then, some studies try to allocate a more suitable number of experts based on characteristic per model layer, aiming to further reduce trainable parameters while minimizing accuracy loss. 
For example, \cite{gao2024higher} assigns fewer experts to shallow layers and more to deeper layers, \cite{qing2024alphalora} adjusts the number of experts based on the training quality of model networks. 
However, all existing studies set a consistent rank for all adapters, which limits the flexibility and adaptability of adapter experts and results in unnecessary parameters across layers. 
In other words, as shown in Figure \ref{fig:param}, beyond the number of experts focused in existing studies, the rank configuration is also a critical factor in determining the size of introduced trainable parameters and influencing the feature learning capacity per adapter. 

%To this end, we propose a hierarchical scheme for adapter experts configuration, \name, which flexibly adjusts the number and rank values of adapter experts across layers, matching the varying representational complexity cross model layers. 

To this end, we present \name, a hierarchical scheme for adapter experts configuration scheme, which aims to enhance the efficiency of mixture of adapter experts. Unlike existing studies that focus solely on the number of experts, \name holistically adjusts layer configurations by jointly considering the effects of expert numbers and ranks, aligning with the varying representational complexity of model layers in two dimensions. Extensive experimental results demonstrate that \name surpasses existing methods in accuracy while simultaneously achieving reductions in trainable parameters for fine-tune training and active parameters for inference. 

The main contributions of this paper are as follows:
\begin{itemize}
	\item We reveal that existing mixture-of-adapter-experts architectures focus solely on the number of adapter experts and experimentally demonstrate that the rank of adapters is also a critical factor for improving performance. 
    \item We propose a hierarchical configuration scheme, \name, which allocates different numbers and ranks to adapter experts across layers, enhancing the efficiency and performance of fine-tuning. 
    \item We conduct extensive experiments to validate that \name achieves higher model accuracy while reducing trainable and active parameters compared to existing methods.
\end{itemize}

% The rest of this paper is organized as follows: Section 2 reviews related work on PEFT, LoRA, and MoE. Section 3 present the motivation and Section 4 details the proposed \name. Section 5 presents experimental results across various benchmarks, followed by a conclusions in Section 6.