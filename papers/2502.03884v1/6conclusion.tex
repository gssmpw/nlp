\section{Conclusion}\label{sec:conclusion}
This paper proposes \name, a hierarchical configuration scheme for the mixture of adapter experts in LLM fine-tuning, enabling flexible adjustments to both the number and rank of adapter experts to better align with the varying representational complexity across model layers. Extensive experimental results demonstrate that \name outperforms existing methods in accuracy while achieving reductions in trainable and active parameters across diverse datasets.