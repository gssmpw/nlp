\section{Related Work}
\label{sec:relatedwork}
\subsection{Parameter Efficient Fine-Tuning}
The increasing size of LLM parameters motivates the development of parameter efficient fine-tuning \cite{han2024parameter}. Unlike full-parameter fine-tuning, PEFT methods selectively adjust or introduce a small number of trainable parameters without modifying the entire parameter set. Adapter tuning \cite{houlsby2019parameter}, prompt tuning \cite{lester2021power}, prefix tuning \cite{li2021prefix}, and LoRA \cite{hulora} are some representative PEFT schemes, and LoRA is one of the most widely employed strategies. It integrates the low-rank decomposition matrices into the weight update of the model and greatly reduces the number of trainable parameters. Specifically, the modified parameter matrix $w'\in \mathbb{R}^{n\times m}$ is computed as $w'=w+AB$, where $w \in \mathbb{R}^{n\times m}$ is the original parameter matrix, $A\in \mathbb{R}^{n\times r}$ and $B \in \mathbb{R}^{r\times m}$ are the low-rank decomposition matrices, and $r$ is much smaller than $n$, i.e., $r\ll min(n,m)$. 

Based on the basic LoRA, several optimizations have been proposed to align with the specific characteristics of LLMs and implementation requirements. The authors in \cite{dettmers2024qlora} further minimize parameter overhead by quantizing adapter parameters. LoRA+ \cite{hayoulora} applies differentiated learning rates based on the initial values of the adapter matrices $A$ and $B$, thereby improving fine-tuning performance in certain scenarios. Unlike conventional approaches, which typically initialize $A$ with random values sampled from a normal distribution and $B$ with zeros, VeRA \cite{kopiczkovera} initializes all of them with a normal distributed random values but freezes them, and adds a trainable vector for each of $A$ and $B$. It can further reduce the trainable parameters involved in fine-tuning with slight accuracy sacrifice.

\subsection{Mixture of Experts}
The mixture of experts architecture was first proposed by \cite{jacobs1991adaptive}, introducing an assignment mechanism based on input data to distribute tasks across multiple expert modules, thereby achieving efficient task specialization and model capacity utilization. 
With the growing popularity of the Transformer \cite{vaswani2017attention}, many studies revisited MoE by applying it to the corresponding Feed-Forward Network (FFN) layers, extending these layers into multiple expert networks. However, a prominent feature of the widely adopted MoE in transformer-based LLMs is the use of a sparse gating mechanism, which selects only a subset of experts for token processing, enabling LLMs to scale to an extreme scale. 

GShard \cite{lepikhin2020gshard} and Switch Transformer \cite{fedus2022switch} are pioneers that employ learnable top-2 or top-1 expert selection strategies, and DeepSeek \cite{dai2024deepseekmoe} implements a shared expert isolation scheme. HashLayer \cite{roller2021hash} uses a hashing-based way to select experts for tokens, improving the stability of model training. \cite{huang2024harder,zhou2022mixture,yang2024xmoe} allow different numbers of experts to be assigned for different tokens, enhancing model flexibility. Besides studies on architectures and training strategies of MoE, recent years have also witnessed the emergence of many MoE-based multimodal models \cite{riquelme2021scaling,mustafa2022multimodal,du2022glam}.

\subsection{LoRA Meets MoE}
Based on basic LoRA technology, some studies have further introduced the MoE architecture, where each weight matrix's LoRA adapter is no longer a single module but are a set of adapters controlled by a gating network \cite{yang2024moral,wu2024parameter}. Such the approach enhances the capability of the fine-tuned model while maintaining scalability \cite{li2024mixlora,wumixture}. 

MoELoRA \cite{liu2024moe} combines the strengths of these two techniques to achieve an efficient multi-task fine-tuning framework. 
AdaMoE \cite{zeng2024adamoe} ingeniously introduces additional null expert networks, enabling a learnable and dynamic selection of the experts. 
LoRAMoE \cite{dou2024loramoe} classifies all experts of each layer into two categories, assigning them to process either world knowledge or fine-tuning knowledge, thereby mitigating the issue of world knowledge forgetting of the fine-tuning. 
MoLA \cite{gao2024higher} implements a layer-wise expert allocation strategy and demonstrates through experimental results that deeper networks require more experts than shallower ones.