\typeout{IJCAI--25 Instructions for Authors}

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{ijcai25}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks,colorlinks,
            linkcolor=blue, 
            anchorcolor=blue,
            citecolor=blue,]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xspace}
\usepackage{color}
\usepackage{amssymb}
\usepackage[switch]{lineno}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{enumitem}
% \usepackage{subcaption}
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newcommand{\name}{\textsc{HiLo}\xspace}
\newcommand{\todo}{\textcolor{red}}
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

% \title{\name: Hierarchical Ranks of Mixture of Adapter Experts for LLM Fine-Tuning}

\title{Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning}

\author{
% Paper ID\#4656 %Anonymous
Peizhuang Cong
\and
Wenpu Liu
\and
Wenhan Yu
\and
Haochen Zhao
\And
Tong Yang
\\
\affiliations
Peking University\\
\emails
\{congpeizhuang, yangtong\}@pku.edu.cn
}
\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) have demonstrated remarkable success across various tasks, accompanied by a continuous increase in their parameter size. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address the challenges of fine-tuning LLMs by significantly reducing the number of trainable parameters. Recent studies have integrated LoRA with Mixture of Experts (MoE) architectures, leveraging multiple adapter experts and gating mechanisms to further improve fine-tuning performance. However, existing approaches primarily focus on adjusting the number of adapter experts per layer to optimize the size of introduced trainable parameters, while neglecting a critical factor of adapters' rank. 

To this end, we propose a hierarchical configuration scheme for adapter experts, \name, which flexibly adjusts the number and rank values of adapter experts across layers, matching the varying representational complexity cross model layers. Extensive experiments on multiple benchmark tasks demonstrate that \name outperforms existing methods in accuracy while introducing fewer trainable parameters, providing an efficient and practical solution for fine-tuning LLMs. 
\end{abstract}

\input{1introduction}
\input{2relatedwork}
\input{3motivation}
\input{4methodology}
\input{5evaluation}
\input{6conclusion}
\bibliographystyle{named}
\bibliography{ijcai}

\end{document}

