\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccccccccc}
    \toprule
    & \multicolumn{3}{c}{Adapter Expert Setting} & \multicolumn{2}{c}{Parameter $\downarrow$} & \multicolumn{6}{c}{Accuracy (\%) $\uparrow$} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-13}
    \multirow{-2}{*}{Method}& {Exp.} & {Rank} & {Top-$K$} & {Active} & {Trainable} & {SQA} & {CQA} & {OQA} & {MRPC} & {COLA}  & {RTE} & {Avg.}\\
    \midrule
    Llama\,2 & - & - & - & - & - & 40.23 & 20.58 & 25.20 & 66.55 & 55.99  & 53.42 & 43.66\\
    \midrule
    Vanilla  & 8  & 8  & 2 & 2 & 1 & 90.10 & 78.21 & 76.20 & 85.15 & 84.85  & 87.72 & 83.71\\
    \midrule
    MoLA & $\triangledown$ & 8 & 2 & 2 & 0.63 & 89.61 & \textbf{76.00} & 74.20 & 84.23 & 85.52  & 84.47 & 82.34\\
    \rowcolor{gray!15}{\name} & 8  & $\triangledown$ & 2 & \textbf{1.25} & 0.63 & \textbf{90.42} & 75.92 & \textbf{76.20} & \textbf{84.52} & \textbf{86.19} & \textbf{84.48} & \textbf{82.96}\\
    \midrule
    AlphLoRA & $ \ddagger$ & 8 & 2 & 2 & 0.63 & 89.70 & 75.83 & 74.60 & 82.49 & 85.13 & 85.55 & 82.22\\
    \rowcolor{gray!15}\name & $\ddagger\ddagger$ & $\triangledown$ & 2 & \textbf{1.25} & 0.63 & \textbf{90.15} & \textbf{76.58} & \textbf{75.80} & \textbf{84.46} & \textbf{84.27} & \textbf{88.09} & \textbf{83.23}\\
    \bottomrule
    \end{tabular}
    }
    \caption{Performances between \fcolorbox{white}{gray!15}{\name (ours)} and comparative methods}\label{tab:method_comparison1}
    \vspace{-3pt}
\end{table*}

\section{Evaluation}\label{sec:evalution}
In this section, we demonstrate the outperformance of \name over various benchmarks, mainly in reducing trainable or active parameter size and enhancing fined-tuned model performance through its hierarchical rank setting.

\subsection{Experiment Setup}
\textbf{Model and Dataset}. 
The Llama\,2-7B \cite{touvron2023llama} is selected as the base model of fine-tuning due to its high deployment and popularity within the LLM community. To evaluate the performances of \name and comparison methods, two distinct types of tasks are employed using widely recognized datasets. The first type of task involves commonsense reasoning and includes the following dataset: ScienceQA (SQA) \cite{lu2022learn}, CommonsenseQA (CQA) \cite{talmor2019commonsenseqa}, and OpenBookQA (OQA) \cite{mihaylov2018can}. The second type of task concentrates on semantic understanding, which uses three datasets from the renowned GLUE Benchmark \cite{wang2019glue}: the Microsoft Research Paraphrase Corpus (MRPC), the Recognizing Textual Entailment (RTE), and the Corpus of Linguistic Acceptability (COLA).

\textbf{Baselines}. 
The pre-trained Llama\,2-7B model and vanilla framework of the mixture of adapter experts are selected as comparison methods. Specially, the Llama\,2-7B model is evaluated using prompt engineering tailored for each dataset, and the vanilla method allocates 8 experts with a rank of 8 for each weight matrix of every Transformer layer. 

Based on the vanilla method, the optimization schemes of adapter experts of existing studies can be divided into three categories: 
\begin{enumerate}[label=(\alph*)]
\item The first one is the manual-based expert allocation represented by MoLA \cite{gao2024higher}. This study indicates that in Llama\,2-7B-based fine-tuning, introducing the same size of trainable parameters, allocating 2, 4, 6, and 8 experts in groups of every 8 layers from shallow to deep layers is the performance-optimal allocation. To facilitate the description, this allocation strategy is denoted as “$\triangledown$” in this paper, which serves as a comparison method. 

\item The second one is AlphaLoRA \cite{qing2024alphalora}, which allocates experts based on the training quality of the original model. When introducing the same size of trainable parameters as MoLA, it has a layer-wise allocation strategy\footnote{For the LLaMA\,2-7B model, AlphaLoRA allocates the number of experts to each Transformer layer as follows: 1, 3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 4, 4, 5, 5, 7, 6, 8, 9, 8, 7, 8, 6, 6, 8, 7, 7, 9, and 5, respectively.} denoted as “$\ddagger$” in this paper, which is another comparison method. Both of these approaches strive to improve the performance of the fine-tuned model with the limited introduced trainable parameters. 

\item Additionally, another category of methods improves model efficiency by flexibly adjusting the number of activated experts to reduce the size of activated parameters. For example, AdaMoE \cite{zeng2024adamoe} achieves this by introducing placeholder experts that neither introduce trainable parameters nor incur activation overhead. In the experiments conducted in this paper, a 1:1 ratio of real experts to placeholder experts is set for each layer, which is also employed as a baseline for comparison. 
\end{enumerate}

\name is integrated into these aforementioned comparison methods. Except for the rank, which follows \name, all other settings remain unchanged with corresponding comparison method. The effectiveness of the proposed scheme can be validated by performance improvements in terms of trainable parameters, active parameters, and accuracy.

\subsection{Experiment Results}
The trainable parameters introduced by vanilla, which mainly involves all adapters with the rank of 8, is defined as a unit of 1. The active parameters that vanilla method activates 2 adapters with rank of 8 for each token is defined as 2. Other methods are scaled proportionally based on the number of introduced or activated adapters and corresponding rank values. All experiments performed on 4 NVIDIA 4090 GPUs.

\subsubsection{Performance Evaluation}
Except for the pre-trained Llama\,2 and Vanilla method, the trainable and active parameters of MoLA and AlphaLoRA are constant, and the active parameters corresponding to AdaMoE are dynamic. To this end, the comparison experiments are conducted respectively, and the specific comparison results are as follows. 

\textbf{(1) \name vs. MoLA \& AlphaLoRA}. MoLA reduces the total number of adapters in the model while keeping the adapter rank unchanged, thereby reducing the trainable parameters to 0.63 compared to Vanilla. To ensure fairness in the comparison, the trainable parameters introduced by AlphaLoRA are kept consistent with those of MoLA. Furthermore, since MoLA and AlphaLoRA also adopt the Top-2 activation scheme, their active parameters remain the same as Vanilla's.

Based on this, \name reduces the rank values progressively from deep to shallow layers, grouped every eight layers, to 8, 4, 6, and 2. To ensure fairness of comparison, the total trainable parameter size of \name is kept consistent with MoLA and AlphaLoRA by proportionally increasing the number of adapter experts from shallow to deep layers. Compared to MoLA, \name assigns eight experts per layer, while compared to AlphaLoRA, the number of experts per layer in \name is calculated as Rank$_i$/8*AlphaLoRA$_i$, where Rank$_i$ represents the rank value of adapters in layer $i$, and AlphaLoRA$_i$ represents the number of adapter experts in layer $i$ of AlphaLoRA, denoted as `$\ddagger\ddagger$' in this paper. In terms of active parameters, although \name adopts Top-2 activation scheme, the decreasing rank result in proportionally fewer active parameters in the shallow layers. Consequently, the active parameters of \name are reduced from 2, as in Vanilla, MoLA, and AlphaLoRA, to 1.25, representing a 37.50\% reduction. 

In terms of accuracy, since Llama\,2 is not fine-tuned on new datasets, it demonstrates relatively low performance. The Vanilla achieves higher accuracy by increasing the size of trainable and active parameters as the cost. However, \name, with a smaller parameter size, still outperforms the Vanilla method on two datasets. Comparing MoLA and AlphaLoRA, as shown in Table \ref{tab:method_comparison1}, \name is lower (0.08\%$\downarrow$) than MoLA only on one of six datasets (CQA) while achieving a maximum improvement of 2.00\% on the OQA. Moreover, \name outperforms AlphaLoRA across all datasets, with the highest improvement of 1.97\% on the MRPC dataset and an average improvement of 1.01\%.

The experimental results demonstrate that \name achieves higher model accuracy compared to MoLA and AlphaLoRA while introducing the same size of trainable parameters and the lower size of active parameters.

\begin{table}[t]
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{lc>{\columncolor{gray!15}}cc>{\columncolor{gray!15}}c}
    \toprule
     & \multicolumn{2}{c}{Active Parameter $\downarrow$} & \multicolumn{2}{c}{Accuracy (\%) $\uparrow$}\\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    \multirow{-2}{*}{Dataset} & AdaMoE & \name & AdaMoE & \name\\
    \midrule
    {SQA}  & 1.05 & \textbf{0.68} & 89.43 & \textbf{90.06}\\
    {CQA}  & 1.01 & \textbf{0.63} & 75.26 & \textbf{75.43}\\
    {OQA}  & 1.03 & \textbf{0.63} & 72.60 & \textbf{73.60}\\
    {MRPC} & 1.03 & \textbf{0.64} & 84.81 & \textbf{85.62}\\
    {COLA} & 1.01 & \textbf{0.63} & 85.42 & \textbf{85.81}\\
    {RTE}  & 1.09 & \textbf{0.67} & 81.94 & \textbf{85.20}\\
    \midrule
    {Avg.} & 1.04 & \textbf{0.65} & 81.58 & \textbf{82.62}\\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Performances of AdaMoE and \name}. Allocating 8 experts and 8 placeholder experts to each layer and employ a Top-2 activation strategy. AdaMoE's adapter rank is 8 and \name sets the rank to $\triangledown$ across layers.}
    \label{tab:method_comparison2}
    \vspace{-10pt}
\end{table}

\textbf{(2) \name vs. AdaMoE}. 
To compare with AdaMoE, \name reduces the rank values from deep to shallow layers, grouped every 8 layers, to 8, 4, 6, and 2, while keeping the number of experts constant. In contrast, the activations of real and placeholder adapter experts are learned during model fine-tuning. Similarly, the trainable parameter size of AdaMoE is 1, whereas \name's is 0.63.

In addition, we tracked the expert activation during validation. As shown in Table \ref{tab:method_comparison1}, the average active parameter of AdaMoE across datasets is 1.04, while \name's is 0.65, representing a 37.5\% reduction. For accuracy, \name outperforms AdaMoE across all datasets, with the highest improvement of 3.26\% on RTE dataset and an average improvement of 1.04\%. 

The experimental results exhibit that \name achieves higher model accuracy with fewer both trainable and active parameters compared to AdaMoE.

\subsubsection{Ablation Studies}
To further evaluate the performance of \name, we conducted another comparison with MoLA across all datasets. In this experiment, it does not constrain the introduced trainable parameters to be the same size. Instead, \name configures both the number of adapter experts and values of adapters' ranks as $\triangledown$. With this setup, the total size of trainable parameters in \name is reduced to 0.39, indicating a 37.50\% decrease compared to MoLA. Similarly, the size of active parameters in \name is reduced to 1.25, reflecting a 37.50\% reduction from MoLA's 2. 

In terms of accuracy, as shown in Figure \ref{fig:comparsion_3}, \name outperforms MoLA on the CQA, OQA, and COLA datasets and achieves higher average accuracy across all datasets. These results further demonstrate that \name can achieve higher fine-tuning accuracy with fewer trainable and active parameters compared to MoLA. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/accuracy_comparison.pdf}
    \caption{\textbf{Accuracy comparison between \name and MoLA}. \name assigns experts and rank as $\triangledown$ across layers.}
    \label{fig:comparsion_3}
    \vspace{-5pt}
\end{figure}

\begin{table}[t]
\centering
\resizebox{0.9\linewidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
    \multicolumn{2}{c}{Adapter Experts} & \multicolumn{2}{c}{Parameter $\downarrow$} &  \\
    \cmidrule(lr){1-2} \cmidrule(lr){3-4}
    {Rank} & {Top-$K$} & {Active} & {Trainable} & \multirow{-2}{*}{{Accuracy (\%) $\uparrow$}} \\
    \midrule
    8 & 2 & 1.03 & 0.63 & 88.44 \\
    \rowcolor{gray!15}$\triangledown$ & 2 & \textbf{0.77} & \textbf{0.39} & \textbf{89.74} \\
    \midrule
    8 & $\triangledown$ & 1.65 & 0.63 & \textbf{90.56} \\
    \rowcolor{gray!15}$\triangledown$ & $\triangledown$ & \textbf{1.23} & \textbf{0.39} & 90.24 \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Performances of Mix and \name in SQA}. Mix and \name allocate experts and placeholder expert as $\triangledown$ across layers.}
    \label{tab:ablation_experiments}
    \vspace{-5pt}
\end{table}

We integrate MoLA and AdaMoE as a new method,  denoted by Mix, which combines layer-wise expert allocation and learning-based adapter experts activation. On this basis, we introduce \name as a comparative method based on Mix, with experiments conducted on the SQA dataset. Under the condition that trainable parameters are unaffected by the activation scheme, the trainable parameters of Mix and \name are 0.63 and 0.39, respectively, with \name achieving a 37.50\% reduction. 

According to statistical analysis, when the adapter experts activation is the Top-2 scheme, the active parameters of Mix and \name are 1.03 and 0.77, respectively, representing a 25.24\% reduction. Although the Top-$\triangledown$ activation is not practically applied, we conducted validation and evaluation under this setting, where Mix and \name exhibit active parameters of 1.65 and 1.23, respectively, resulting in a 25.45\% reduction. 

In terms of accuracy, as shown in Table \ref{tab:ablation_experiments}, under the de facto widely employed Top-2 activation scheme, \name outperforms Mix with a 1.30\% improvement. Under the Top-$\triangledown$, \name’s accuracy is 0.32\% lower than Mix, but it still achieves higher accuracy than Vanilla, MoLA, and AlphaLoRA, while reducing both trainable and active parameters simultaneously.

\subsubsection{Exploratory Studies}
% \begin{figure}[t]
%   \centering
%     \subfigure[SQA]{
%     \label{fig:fail_link_setting}
%     \includegraphics[width=0.16\textwidth]{figures/SQA.pdf}
%     }
%     \hspace{-12pt}
%     \subfigure[CQA]{
%     \label{fig:fail_link_setting}
%     \includegraphics[width=0.16\textwidth]{figures/CQA.pdf}
%     }
%     \hspace{-12pt}
%     \subfigure[OQA]{
%     \label{fig:fail_link_setting}
%     \includegraphics[width=0.16\textwidth]{figures/OQA.pdf}
%     }
%     \subfigure[MRPC]{
%     \label{fig:fail_link_setting}
%     \includegraphics[width=0.16\textwidth]{figures/MRPC.pdf}
%     }
%     \hspace{-12pt}
%     \subfigure[COLA]{
%     \label{fig:fail_link_setting}
%     \includegraphics[width=0.16\textwidth]{figures/COLA.pdf}
%     }
%     \hspace{-12pt}
%     \subfigure[RTE]{
%     \label{fig:fail_link_setting}
%     \includegraphics[width=0.16\textwidth]{figures/RTE.pdf}
%     }
%   \caption{Accuracy performances of \name with different ranks.}\label{fig:rank_config}
%   \vspace{-5pt}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/6+1.pdf}
    \caption{Accuracy performances of \name with different ranks.}\label{fig:rank_config}
    \vspace{-5pt}
\end{figure}

\name is evaluated by various rank configurations, specifically grouping every eight layers with ranks set as 2448, 2288, 2468, 2488, and 2888 from shallow to deep layers, with the corresponding trainable parameters gradually increasing. The parameter size of 2468 configuration is assigned unit 1. The accuracy results for these five configurations across six datasets are shown in Figure \ref{fig:rank_config}. It can be observed that the configuration of 2468 (green $\blacklozenge$) achieves the best balance between parameter size and accuracy. This configuration is also adopted in the above experiments.