\section{\name Design}\label{sec:methodology}
In this section, we first describe the extension of the basic LoRA method to the MoE architecture, and then present the adapter experts configuration about allocation and activation of experts. Finally, we introduce a simple and efficient rank-setting strategy, which can further reducing parameters while improving accuracy of the fine-tuned model.

\subsection{Mixture of Adapter Experts}
Extending LoRA to MoE architectures is to transform the original single adapter, which comprises one pair of low rank matrices \(A\) and \(B\), into multiple adapter experts. 
Suppose there is a model with \(N\) Transformer layers and each layer \(i\) has \(E_i\) adapter experts, denoted by \(e^j_i\), where \(j \in E_i\). 
In line with the basic LoRA initialization scheme, each adapter expert has its matrix \(A\) initialized with a random Gaussian distribution and \(B\) with zeros.
Moreover, a gate network \(G_i\) is introduced for layer \(i\). It is responsible for analyzing the features of each input token \(x\) and computing activation probabilities \(P^j_i\) for all adapter expert \(e^j_i\) via a softmax function. 

Following the MoE computation procedure with a Top-K activation policy, for a given token \(x\), the adapter experts \(e^i_k, k\in K\) with highest \(K\) probabilities are activated. Then, the output of this mixture of adapter experts layer can be expressed as:
\[
{Output}_{e_i} 
= \sum_{k \in K} p^k_i \, w^k_i \, {x},
\quad
p^k_i 
= \frac{P^k_i}{\sum_{j \in K} P^j_i},
\]
where \(w^k_i\) represents the parameter matrix of the expert \(e^k_i\). 
Assuming \(W_i\) is the original weight matrix of layer \(i\), the final output of this layer yields:
\[{Output}_i = W_i \, {x} + {Output}_{e_i}.\] 

So far, it is possible to select a subset of adapter experts dynamically, enhancing both the capacity and adaptation flexibility of LoRA. Meanwhile, \({Output}_{e_i}\) is fed into the next layer as the input.

\subsection{Adapter Experts Setting}
In this subsection, we present the experts allocation strategies and activation policies, which determine sizes of trainable and active parameters, respectively, thereby affecting the model’s efficiency.

% \subsubsection{Allocating Experts by Base Model}
\subsubsection{Dynamic Experts Allocation}
The vanilla allocation strategy assigns the same number of adapter experts to each layer. However, as mentioned earlier, such an approach overlooks the differences across layers, resulting in inefficiencies such as introducing unnecessary trainable parameters or failing to utilize them fully. Two proven flexible expert allocation strategies are presented here \cite{gao2024higher,qing2024alphalora}. First, based on the varying complexity of processing tasks across different layers, experts can be allocated incrementally from shallow to deep layers. This intuitive method demonstrates performance improvements compared to a fixed expert allocation. Second, the Heavy-Tailed Self-Regularization theory can be used to evaluate the training quality of the original model layers, guiding a flexible and adaptive allocation of experts to each layer.

\subsubsection{Activation of Experts}
For expert activation, Top-$K$ is a feasible and simple manner. However, activating the fixed number of experts neglects the processing complexity among different tokens, which may result in either redundancy or insufficiency in processing for certain tokens. 

Comparatively, learning-based policies enable the activation of a variable number of experts. 
The Top-$P$ series methods exemplify this policy by activating experts for each token, either by selecting in order of decreasing probability until the cumulative probability exceeds a predefined threshold $P$, or by directly selecting all experts whose probabilities surpass the threshold. $P$ is predefined, and the loss function incorporates constraints on the number of activated experts. Additionally, another implementation involves inserting several placeholder adapters without parameters into the adapter expert layer \cite{zeng2024adamoe}. These placeholders do not perform computations on the token but allow the model to activate a varying number of true experts for each token while still adhering to the Top-$K$ policy. For instance, if $n$ of the selected experts are placeholder adapters, then, only $K$-$n$ true experts are actually activated. During training, the model learns to activate the appropriate number of experts dynamically for each token.

These above two kinds of mainstream policies for expert activation will be evaluated in comparative experiments.

\subsection{Hierarchical Rank-setting}
In the mixture of adapters experts architecture, the number of adapter experts per layer and their rank are critical factors influencing model performance and trainable parameter size. The rank of an adapter expert directly determines its representational ability and parameter size, while the current layer-wise expert's allocation strategy leaves room for further optimization. That is, the rank of shallow-layer adapter experts can be set lower than that of deeper layers, which further satisfies diverse representational requirements and improves parameter utilization. 

Formally, with the layer as the rank setting granularity, the number of adapter experts in each layer can be expressed as: 
\[r_i = r_{min} + \left\lfloor \frac{r_{max} - r_{min}}{\left\lceil \frac{n}{l} \right\rceil} \right\rfloor \cdot \left\lfloor \frac{i-1}{l} \right\rfloor,\]
where \(\left\lceil \cdot \right\rceil\) and \(\left\lfloor \cdot \right\rfloor\) respectively represent the rounding up and rounding down operations; \(r_{min}\) and \(r_{max}\) respectively denote the minimum and maximum rank values. Typically, \(r_{min}\) is set to 2, depending on the model’s scale, and \(r_{max}\) can take values such as 8, 16, 32, or higher. \(n\) indicates the total layers of the fine-tuned model. Optionally, it is feasible to set same rank for for every \(l\) layers. 

Moreover, considering the computational characteristics of hardware devices, rank values can be flexibly configured as multiples or powers of 2 based on the formula in practical implementation, rather than strictly adhering to it.