\newif\ifdraft
\drafttrue


\PassOptionsToPackage{table}{xcolor}

\documentclass[lettersize,journal]{IEEEtran}

%\documentclass[journal,twoside,web]{ieeecolor}
%\usepackage{generic}
\usepackage{xcolor}  % No options here
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bbm}
\usepackage{graphicx}
%our imports
\usepackage{hyperref}
\usepackage{caption,subcaption}
\usepackage{multirow}
\usepackage{cleveref}
\usepackage{comment}
\usepackage{hhline}
\usepackage{todonotes}
\usepackage{caption,subcaption}
%\usepackage{cleveref}
\usepackage{siunitx}
\sisetup{round-mode=places,round-precision=3}
\usepackage{booktabs}
\usepackage{csvsimple}
%\usepackage[export]{adjustbox}
\usepackage{datatool}
\usepackage{array}
\usepackage{placeins}
\usepackage{float}
\usepackage{geometry}        % For adjusting page layout
\usepackage{pifont} % For arrow symbols
\usepackage{makecell}        % For multi-line cells

% Define custom colors
\definecolor{improvement}{RGB}{144,238,144} % Light green for improvements
\definecolor{degradation}{RGB}{255,182,193} % Light red for degradations
\definecolor{nochange}{RGB}{255,255,255}    % White for no change





\newcommand\colnst[1]{{\color{red}#1}}
\newcommand\coltw[1]{{\color{blue}#1}}

\newcommand{\heading}[1]{\noindent\textbf{#1}}

% Custom command to split cell into SBP and DBP with different background colors, side by side
\newcommand{\diffcellsplitside}[4]{%
    \makecell[l]{%
        \colorbox{#1}{\parbox{0.7cm}{\centering \scriptsize #2}} \hspace{0.2cm}%
        \colorbox{#3}{\parbox{0.7cm}{\centering \scriptsize #4}}%
    }
}
\usepackage[normalem]{ulem} %for sout; normalem to remove issues with underlined journal names in bibstyle
\newcommand{\stkout}[1]{\ifmmode\text{\sout{\ensuremath{#1}}}\else\sout{#1}\fi}
%incompatibility of citp and ulem
\newcommand{\citex}[1]{\mbox{\cite{#1}}}
\newcommand{\citepx}[1]{\mbox{\citep{#1}}}
\newcommand{\citetx}[1]{\mbox{\citet{#1}}}
\ifdraft
%draft mode
\newcommand{\added}[1]{\textcolor{blue}{#1}}
\newcommand{\deleted}[1]{\textcolor{red}{\stkout{#1}}}
\newcommand{\replaced}[2]{\textcolor{blue}{#1} \textcolor{red}{\stkout{#2}}}
\newcommand{\deletedfloat}[1]{}%{\textcolor{red}{#1}}%for tables
\newcommand{\commented}[1]{\textcolor{blue}{#1}}
\else
%final mode
\newcommand{\added}[1]{#1}
\newcommand{\deleted}[1]{}
\newcommand{\replaced}[2]{#1}
\newcommand{\deletedfloat}[1]{}%for tables
\newcommand{\commented}[1]{}
\fi

\newcommand\citep[1]{\cite{#1}}
\newcommand\citet[1]{\cite{#1}}
\newcommand{\pcn}[1]{\textcolor{magenta}{#1}} % Pete's comments


%IEEE Transaction on Neural Networks and Learning Systems, VOL. XX, NO. XX, XXXX 2023
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{Preprint}{Moulaeifard \MakeLowercase{\textit{et al.}}: In-Distribution and Out-of-Distribution Generalization of PPG-based Blood Pressure Estimation-- A Benchmarking Study based on Deep Learning Models (Oct 2024)}
\begin{document}

%\title{In-Distribution and Out-of-Distribution Generalization of PPG-based Blood Pressure Estimation-- A Benchmarking Study involving Deep Learning Models}
\title{Generalizable deep learning for photoplethysmography-based blood pressure estimation– A Benchmarking Study}
\author{Mohammad Moulaeifard, Peter H. Charlton and Nils Strodthoff
\thanks{Mohammad Moulaeifard and Nils Strodthoff are with Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany. (email: mohammad.moulaeifard@uol.de, nils.strodthoff@uol.de). Peter H Charlton is with University of Cambridge, Cambridge, UK. Corresponding author: NS.}}


%limiting number of authors as https://tbme.embs.org/for-authors/
%https://tex.stackexchange.com/questions/164017/limiting-the-number-of-authors-in-the-references-with-ieeetran
\bstctlcite{BSTcontrol}


\maketitle

\begin{abstract}
Photoplethysmography (PPG)-based blood pressure (BP) estimation represents a promising alternative to cuff-based BP measurements. Recently, an increasing number of deep learning models have been proposed to infer BP from the raw PPG waveform. However, these models have been predominantly evaluated on in-distribution test sets, which immediately raises the question of the generalizability of these models to external datasets. To investigate this question, we trained five deep learning models on the recently released PulseDB dataset, provided in-distribution benchmarking results on this dataset, and then assessed out-of-distribution performance on several external datasets. The best model (XResNet1d101) achieved in-distribution MAEs of 9.4 and 6.0 mmHg for systolic and diastolic BP respectively on PulseDB (with subject-specific calibration), and 14.0 and 8.5 mmHg respectively without calibration. Equivalent MAEs on external test datasets without calibration ranged from 15.0 to 25.1 mmHg (SBP) and 7.0 to 10.4 mmHg (DBP).  Our results indicate that the performance is strongly influenced by the differences in BP distributions between datasets. We investigated a simple way of improving performance through sample-based domain adaptation and put forward recommendations for training models with good generalization properties. With this work, we hope to educate more researchers for the importance and challenges of out-of-distribution generalization. %\pcn{[The title seems long, and ideally I would avoid acronyms (although this makes it longer). Perhaps: Generalisable deep learning for photoplethysmography-based blood pressure estimation– A Benchmarking Study.]}
%Efficient and gentle blood pressure monitoring plays a key role in maintaining good health and preventing heart diseases. Traditional cuff-based techniques are dependable. However, they can be uncomfortable and not ideal for monitoring; thus, the need arises for cuff-free alternatives. While deep learning has shown promise in estimating blood pressure using photoplethysmography (PPG) and electrocardiography (ECG) signals, data quality and quantity challenges still need to be addressed to utilize these methods effectively. 

%The current work compares several deep learning models for estimating systolic and diastolic blood pressure based on the largest and scrupulously cleaned datasets of PPG and ECG signals, thus mitigating some of the previously listed limitations. We then tested the models on in-distribution (ID)  and out-of-distribution (OOD) datasets. Also, to enhance the reliability of OOD predictions, we implement the sample-weighting approach (weighted OOD), which helps the model to be more adaptable for diverse, unseen data where variability is common in most real-world applications. 

%Furthermore, the external testing to benchmark the models against existing deep learning approaches gave rise to improved adaptability and accuracy in cuff-free blood pressure monitoring across diverse scenarios. This work establishes the PulseDB dataset as a resource for structured benchmarking of PPG analysis algorithms under both ID and OOD conditions. Hence, we encourage other active researchers in this field to contribute to these efforts.
 
\end{abstract}

\begin{IEEEkeywords}
    Decision support systems, Photoplethysmography, Machine learning algorithms, Time series analysis
    %https://ieee.org/documents/taxonomy_v101.pdf
    
    %Enter key words or phrases in alphabetical 
    %order, separated by commas. For a list of suggested keywords, send a blank 
    %e-mail to keywords@ieee.org or visit \underline
    %{http://www.ieee.org/organizations/pubs/ani\_prod/keywrd98.txt}
    \end{IEEEkeywords}

%https://jbhi.embs.org/for-authors/prepare-and-submit-your-manuscript/
%https://tbme.embs.org/for-authors/
%https://journals.ieeeauthorcenter.ieee.org/create-your-ieee-journal-article/authoring-tools-and-templates/ieee-article-templates/templates-for-transactions/
\section{Introduction}
\label{sec:I}
%\colnst{Pete has a recent review/roadmap in Physiological Measurement that we should cite in the first paragraph}. \todo{Done}.


\IEEEPARstart{P}hotoplethysmography (PPG) devices \added{represent a promising approach to monitor vital parameters such as blood pressure, heart rate, and respiratory rate.}\deleted{are tools that are widely known for their capability to consistently monitor non-invasive, cuffless blood pressure as well as heart rate and oxygen saturation levels in the bloodstream, with ease and affordability—} \replaced{Their non-invasive nature and cost-effectiveness in comparison to alternative approaches}{ a feature that} makes them popular for both medical and personal health monitoring purposes \cite {charlton20232023, castaneda2018review, gonzalez2023benchmark}. One of the most widely considered prediction problems based on PPG data is blood pressure (BP) estimation.

Traditionally, BP estimation from PPG signals has involved analyzing features in the PPG data and connecting them to BP measurements using different methods, e.g., \added {pulse wave analysis \cite{o2001pulse} } \deleted {Pulse Transit Time (PTT) }. However, the traditional methods possess certain limitations in real-world applications, e.g.,  variations in physiology \cite {allen2007photoplethysmography} and the requirement for calibration \cite{Wan2020,gonzalez2023benchmark}. 

In light of the limitations of traditional feature-based approaches, there has been a gradual shift of interest on the part of researchers towards machine learning (ML) and deep learning (DL) techniques for predicting BP using PPG signals. These methods autonomously extract features from PPG signals \cite{martinez2022data} and typically reach higher accuracy in data-rich environments \cite{chang2021deepheart, mehrgardt2021deep}. Despite significant progress, most existing studies primarily focus on in-distribution (ID) testing, where the train and test datasets stem from the same distribution. This approach does not consider out-of-distribution (OOD) evaluation scenarios and thus does not reflect the reality of real-world applications, where test data can come from various distributions. \added{ Real-world test sets might differ from the training dataset in various aspects, such as BP distributions, sensor hardware used for capturing, signal quality and subject physiologies.} 

This study aims to address these gaps by leveraging the PulseDB dataset \cite{wang2023pulsedb} as a training dataset, and then evaluating both ID and OOD generalization of DL models for the estimation of BP. \added{The goal is to identify model architectures and training datasets that give rise to models that show a robust OOD performance. The PulseDB dataset was used for training due to its large size. Then, four external datasets served as test datasets to assess OOD generalization.} Furthermore, we investigated a simple domain adaptation approach to improve OOD generalization. This work offers a comprehensive benchmarking analysis on \added{diverse}  \deleted{various subsets of PulseDB and several external} datasets, providing insights into the robustness of these models in real-world scenarios. \added{We close with practical recommendations on model architectures, training datasets and scenarios to achieve good OOD generalization.}



\section{Related Works}
\label{sec:II}

\heading{Challenges for BP estimation and benchmarking models} 
\label{sec:II-B}
Several research studies have indicated that the effectiveness of learning models in predicting BP is highly dependent on the quality and quantity of the training data \cite{qin2023machine, treebupachatsakul2022cuff, zabihi2021bp}. For instance,  researchers who worked with the MIMIC-III dataset underscore the significance of data preparation to achieve better model outcomes \cite{chu2023non}. The aforementioned challenges result in a growing demand for reliable benchmark studies to thoroughly evaluate different ML/DL methods for BP estimation using PPG, utilizing comprehensive datasets to measure their efficiency. Consequently, benchmark investigations have been performed in this area to address the demands aforementioned. We refer to \cite{gonzalez2023benchmark} as a notable and recent benchmark study that used four different datasets. % such as Sensors \cite{aguirre2021blood}, UCI \cite{kachuee2015cuff}, BCG \cite{carlson2020bed} and PPGBP \cite{liang2018new}, referred to as "external datasets" in this paper. 
However, the mentioned study only considers models trained from scratch on the respective datasets and evaluates them on in-distribution test sets, which are known to provide overly optimistic measures for the generalization performance on unseen data. The PulseDB dataset, a large-scale, high-quality dataset containing PPG signals and reference BP measurements, and as such is a unique resource for training deep-learning based BP prediction models, after which they can be externally validated on external datasets.
%Although the benchmark study by \cite{gonzalez2023benchmark} sheds light on the performance of various ML models for a wide variety of datasets, it suffers from several limitations regarding the dataset, such as size and diversity issues and discrepancies. For instance, the PPGBP dataset contains segments with a length of 2.1-seconds, which might not fully capture the complete range of BP dynamics. Similarly, the limited sample size of 40 participants in the BCG datasets hinders its ability to apply broadly, making it challenging for models to perform well across different datasets. Through an extensive review of various published datasets, we came to understand that PulseDB dataset launched with thorough data cleaning procedures to guarantee high-quality data for training ML algorithms successfully.


\heading{Challenges of OOD generalization} 
Most of the ML/DL techniques usually rely on the subtle statistical patterns that may exist within the training data, hence functioning under ideal conditions where both the training and testing data belong to the same distribution (ID). However, this perfect situation rarely occurs in real-world scenarios \cite{zhang2021deep, engstrom2019exploring}. Previous work has shown that most DL models perform poorly on tasks induced by data from distributions other than their training data (OOD) generalization \cite{ballas2022domain}. The concept of OOD generalization and its application to DL models has evolved with contributions from various researchers, e.g., \cite{gwon2023out, yi2021improved, ye2021towards}. %We also refer to \cite{hendrycks2020pretrained, hendrycks2021many}, which mitigate the issue being addressed by rigorously testing models on datasets that are not similar to the original training data. 
\added{The challenges of OOD generalization in the context of PPG-based BP estimation has been investigated in a recent publication \cite{weber2023intensive}. They focused on feature-based approaches, whereas the present work covers deep learning models operating on raw time series. This work establishes a more comprehensive picture by considering a large number of external datasets and investigating the potential impact of domain adaptation.}

\heading{Improving OOD generalization} 
It is worth mentioning that OOD generalization is a challenging task that may result in poor performances since unseen data very often do not resemble the training set \cite{hendrycks2017baseline, liang2018enhancing}. Several previous works have intensively addressed such a challenge of mitigating the influence of OOD signals when analyzing ECG and EEG data. \cite{ballas2022domain,soltanieh2023distribution} have shown the effectiveness of using domain generalization and self-supervised learning approaches to improve the classification accuracy of OOD ECG signals. Also, recent work by \cite{yang2022multimodal} has demonstrated promising results in addressing domain shifts and OOD signals between diverse EEG datasets, highlighting the potential of domain adaptation techniques to enhance the robustness of EEG signal recognition. According to previous studies, one of the key approaches to tackle the challenge of OOD generalization is to reduce the influence of distribution shifts between training and test sets by revising the distribution of training data to mimic the distribution of test data, aiming to minimize the predictive error on the test set \cite{ben2010theory,bickel2009discriminative}. %Inspired by this approach, our method in this paper is to allocate specific weights to the train samples during the training procedure. This allows the model to learn more efficiently from varied data distributions and, hence, improve its generalization capability, which is crucial in real-world applications. 
In this work, we use a simple sample-based empirical risk minimization approach based on sample weights inferred from the label distribution \added{(i.e., BP reference labels)} in the source and target domains to assess the potential benefits of incorporating domain adaptation approaches. 

\heading{Technical contributions} 
In this work, we put forward the following technical contributions:
%In conclusion, the major contributions investigated in this article are as follows:

\begin{enumerate}

%NOT OUR CONTRIBUTION
%\item 
%
%We propose different benchmarking tasks based on various subsets of PulseDB datasets such as Calib, CalibFree, and AAMI with respect to all possible sources. 


\item We implemented state-of-the-art DL-based time series classification algorithms for PPG-based BP estimation on the large-scale, high-quality PulseDB dataset and evaluated their performance in a first comprehensive comparative study. %to evaluate their performance and effectiveness. %This approach provides a comparison between different models when dealing with PPG signal data. It is worth noting that all code and benchmarking infrastructure used in this study are publicly available in the code repository to ensure transparency and enable further research in this area.

\item \replaced{We investigated both ID and OOD generalization of models trained on various PulseDB subsets. These models were evaluated on different PulseDB subsets and four external datasets. To contextualize OOD performance, we compared it with the differences in label distributions between the training and test datasets.}{We investigated both ID and OOD generalization of models trained on various PulseDB subsets by evaluating these models on different PulseDB subsets as well as four different external datasets. We put the OOD performance into perspective to the difference between the label distributions of training and test datasets}.

%This approach helps us better understand how well our model can generalize to various distributions of the PulseDB that have not yet been seen. Furthermore, to have an extended OOD generalization on datasets with completely different sources, we test the models on four external datasets: Sensors, UCI, BCG, and PPGBP. Current work tries to shed light on the strengths and shortcomings of the state-of-the-art OOD generalization techniques under varied real-world application scenarios.

%\item

%To the best of our knowledge, weighted and unweighted OOD generalization analysis has not been done previously in the context of BP estimation using PPG data. This study addressed this gap with a comprehensive assessment. We have followed an approach to train the model on different customized splits of the PulseDB dataset and tested it on both overlapping and non-overlapping splits of PulseDB along with external datasets.


\item We assessed the benefit of domain adaptation by using an importance-weighted empirical risk minimization approach using importance weights inferred from the respective label distributions and put forward recommendations for training dataset choices that promise good generalization properties.
%Finally, we compare our OOD generalization results with respect to the external dataset to the reported ID evaluation by \cite{wang2023pulsedb}. Also, we conduct an in-depth review and fairness assessment of the results. The comparison, therefore, provides a broad understanding of the robustness and fairness of both ID and OOD approaches.

\end{enumerate}

%\sout {\colnst{Overall, the introduction could be more concise. At the moment it is rather a mixture of related work and introduction and some high-level description of what we do. Maybe we should rather have a separate related work section to cover related work and focus more on the shortcomings of the literature (no structured benchmarking studies on large-scale datasets, only in-distribution tests, no domain adaptation)}}



\section{Materials \& Methods}
\label{sec:III}

\subsection{Training and Evaluation Datasets}
\label{sec:III_A}


\begin{table*}[h]
\centering
\caption{Summary of the datasets utilized in this study: Two subsets PulseDB \cite{wang2023pulsedb} for training and four external datasets \cite{gonzalez2023benchmark} for OOD evaluation.}
\scalebox{0.75}{
\begin{tabular}{lccccccc}
%\toprule
& \multicolumn{2}{c}{\textbf{PulseDB}} & \multicolumn{4}{c}{\textbf{External Datasets}} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-7}
\textbf{Metric} & \textbf{MIMIC} & \textbf{VitalDB} & \textbf{Sensors} & \textbf{UCI} & \textbf{BCG} & \textbf{PPGBP} \\
\midrule
\textbf{No. Subjects} & 1,474 & 1,553 & 1,195 & unknown & 40 & 218 \\
\textbf{Total Duration (h)} & $\sim$2357 & $\sim$1793 & $\sim$15 & $\sim$570 & $\sim$4 & $<$1 \\
\textbf{Segments (number , length)} & 848,796 , 10s   & 645,678 , 10s & 11,102 , 5s & 410,596 , 5s & 3,063 , 5s & 619 , 2.1s \\
\textbf{Age (mean ± SD)} & 61.16 ± 15.29 & 58.71 ± 15.14 & 57.1 ± 14.2 & unknown & 34.2 ± 14.5 & 56.9 ± 15.8 \\
\textbf{Gender} & 45.94\% F, 54.05\% M & 41.84\% F, 58.15\% M & 40.2\% F, 59.8\% M & unknown & 55.5\% F, 44.5\% M & 53.1\% F, 46.9\% M \\
\textbf{SBP (mmHg, mean ± SD)} & 123.32 ± 23.00 & 115.62 ± 18.92 & 134.36 ± 21.78 & 131.57 ± 11.16 & 120.99 ± 15.29 & 128.02 ± 20.50 \\
\textbf{DBP (mmHg, mean ± SD)} & 61.58 ± 13.48 & 63.03 ± 12.05 & 65.37 ± 10.51 & 66.79 ± 10.48 & 67.23 ± 9.30 & 71.91 ± 11.20 \\
\bottomrule
\end{tabular}
}
\label{tab:tab1}
\end{table*}


\heading{PulseDB dataset} 
PulseDB is sourced from selected pre-processed signals from the MIMIC-III \cite{moody2020mimic} and VitalDB \cite{lee2022vitaldb} databases. It is one of the most extensive datasets currently available, containing 5,245,454 10-second segments of ECG, PPG and arterial BP (ABP) waveforms across 5,361 subjects. The dataset includes demographic details such as age, gender, weight, height, and body mass index (BMI). Both VitalDB and MIMIC-III represent samples collected from finger-tip PPG sensors from patients undergoing surgery and in Critical Care Units respectively.
It is worth noting that the PulseDB dataset is categorized into the following subsets, making it ideal for benchmarking cuff-less BP estimation models:
\begin{itemize}
    \item \textit{Calib:} \replaced{Created for a calibration-based approach, where each subject contributes data to both the training and testing sets. This enables the model to adapt to patient-specific signal features in order to improve the prediction performance. The focus of this scenario is to train models that show good generalization to unseen samples of patients encountered during training.}{calibration-based approach in which training and test sets are shared by subjects}
    \item \textit{CalibFree:} \replaced{Created for a calibration-free approach, in which training and test sets do not share any subjects. The focus of this scenario is to develop models that generalize to entirely unseen patients.}{calibration-free approach in which training and testing sets do not share subjects} 
    \item \textit{AAMI:} \replaced{Created for a second calibration-free scenario, which complies with the high standards developed by the Association for the Advancement of Medical Instrumentation (AAMI) \cite{stergiou2018universal}. The main difference between this and the CalibFree scenario is a stronger emphasis on the tails of the BP distribution. The focus of this scenario is to assess the generalization to unseen patient with the stricter protocol of the AAMI for medical device testing.}{one that follows the standard requirements by the Association for the Advancement of Medical Instrumentation.} 
\end{itemize}


\begin{comment}
\begin{table}[h]
\centering
\caption{Summary of PulseDB dataset \colnst{can we have separate columns for MIMIC and vital? and rather median and iqr? and duration?} Unfortunatly there is no such detailed information in the PulseDB paper.}
%\renewcommand{\arraystretch}{1.5} % Increase cell height by 1.5x
\scalebox{1.0}{ % Adjust the scale factor as needed
\begin{tabular}{|c|c|}
\hline
\textbf{Metric} & \textbf{Value} \\ \hline
Subjects (MIMIC/VitalDB) & 5,361 (2,423/2,938) \\ \hline
Segments & 5,245,454 \\ \hline
Age (mean ± SD) & 60.87 ± 15.58 \\ \hline
Gender & 43.8\% F, 56.2\% M  \\ \hline
SBP (mmHg, mean ± SD) & 121.42 ± 22.10 \\ \hline
DBP (mmHg, mean ± SD) & 61.87 ± 13.01 \\ \hline
\end{tabular}
}
\label{tab:tab1}
\end{table}
\end{comment}

 


We generated nine subsets of the PulseDB dataset, inspired by the instructions in the original PulseDB publication \cite{wang2023pulsedb} and the corresponding code repository. Tables \ref{tab:tab1} and \ref{tab:tab2} summarise the utilized PulseDB dataset in this paper (including data from MIMIC and VitalDB), and the generated subsets, respectively. We have three major subsets, Calib, CalibFree, and AAMI, which are derived from VitalDB or MIMIC, or a combination of VitalDB and MIMIC (combined) sources, resulting in nine different subsets.  We kept the original test sets intact to ensure comparability with results in the literature, but we split off additional validation and calibration sets from the respective training sets mimicking the way in which the respective test sets were constructed.



\begin{table*}[h]
\centering
\caption{Summary of generated PulseDB subsets \added{(Samples / Subjects)}}
%\renewcommand{\arraystretch}{1.0} % Increase cell height by 1.5x
\begin{tabular}{llcccc}
%\toprule
\textbf{Source} & \textbf{Subset} & \textbf{Train} & \textbf{Validation} & \textbf{Calibration} & \textbf{Test} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} &  811,955 / 2,494 & 78,899 / 2,494 & 11,306 / 2,494 & 100,240 / 2,494 \\
& \textbf{CalibFree} & 801,720 / 2,217 & 66,960 / 186 & 33,480 / 93 & 111,600 / 279 \\
& \textbf{AAMI} & 902,160 / 2,494 & 230,145 / 149 & 148,989 / 93 & 1340 / 242 \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & 418,986 / 1,293 & 40,673 / 1,293 & 5821 / 1,293 & 51720 / 1,293 \\
& \textbf{CalibFree} & 416,880 / 1,158 & 32,400 / 90 & 1000 / 45 & 57,600 / 144 \\
& \textbf{AAMI} & 465,480 / 1,293 & 43,820 / 71 & 26,392 / 45 & 666 / 116 \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & 392,969 / 1,213 & 38,226 / 1,213 & 5,485 / 1,213 & 48,520 / 1,213 \\
& \textbf{CalibFree} & 384,840 / 1,069 & 34,560 / 96 & 17,280 / 48 & 54,000 / 135 \\
& \textbf{AAMI} & 436,680 / 1,213 & 186,325 / 78 & 122,597 / 48 & 674 / 126 \\
\bottomrule
\end{tabular}
\label{tab:tab2}
\end{table*}



\heading{External datasets} We used the datasets presented in a recent benchmark study \cite{gonzalez2023benchmark} as external datasets, as they are qualitatively very different to PulseDB in terms of sample size, signal quality and patient collective, and are therefore well-suited to investigate the OOD generalization of models trained on PulseDB or subsets thereof. \cite{gonzalez2023benchmark} provides the pre-processed versions of each external dataset along with their pre-processing methods, which serve as external datasets for the purpose of our study (Table \ref{tab:tab1}). These datasets are comprehensively described in \cite{gonzalez2023benchmark}. Therefore, we refer to the original publication for details, and now briefly outline the key features of each dataset:

\begin{comment}
\begin{table*}[h]
\centering
\scriptsize % Use a smaller font size
\caption{Summary of pre-processed Sensors, UCI, BCG, and PPGBP datasets as prepared by \cite{gonzalez2023benchmark} \colnst{rather a separate row for duration; should match Table II; or potentially integrate PulseDB subsets in this table} \colnst{need units of }}
\renewcommand{\arraystretch}{1.5} % Adjust the row height here
\scalebox{1.18}{ % Adjust the scale factor as needed
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{PulseDB (MIMIC/VitalDB)} & \textbf{Sensors} & \textbf{UCI} & \textbf{BCG} & \textbf{PPGBP} \\ \hline
Subjects & 5,361 (2,423/2,938) & 1195 & unknown & 40 & 218 \\ \hline
Duration & unknown & ~15 hours & ~570 hours & ~4 hours & less than 1 hour \\ \hline
Segments & 5,245,454 & 11102 & 410596 & 3063 & 619 \\ \hline
Age (mean ± SD) & 60.87 ± 15.58 & 57.1 ± 14.2 & unknown & 34.2 ± 14.5 & 56.9 ± 15.8 \\ \hline
Gender & 43.8\% F, 56.2\% M & 40.2\% F, 59.8\% M & unknown & 55.5\% F, 44.5\% M & 53.1\% F, 46.9\% M \\ \hline
SBP (mmHg, mean ± SD) & 121.42 ± 22.10 & 134.36 ± 21.78 & 131.57 ± 11.16 & 120.99 ± 15.29 & 128.02 ± 20.50 \\ \hline
DBP (mmHg, mean ± SD) & 61.87 ± 13.01 & 65.37 ± 10.51 & 66.79 ± 10.48 & 67.23 ± 9.30 & 71.91 ± 11.20 \\ \hline
\end{tabular}
}
\label{tab:tab3}
\end{table*}
\end{comment}





\begin{itemize}
\item \textit{Sensors}: The Sensors dataset is derived from MIMIC-III with simultaneous PPG and ABP waveforms from 1,195 ICU patients. After pre-processing, it includes two 15-second segments per record with a 5-minute interval between them. 

\item\textit{UCI}: The UCI dataset is derived from the MIMIC-II dataset and is the largest dataset in our external dataset.

\item\textit{BCG}: The BCG dataset contains recordings from 40 subjects (primarily healthy). Although the BCG is a smaller dataset after pre-processing with limited variability, the number of segments per subject is remarkably high. 

\item\textit{PPGBP}: The PPGBP dataset contains data from 219 subjects, each with cardiovascular conditions. Following pre-processing, PPGBP becomes the shortest dataset, containing \deleted{619} \added{218 subjects and 613} segments \added{of 2.1s length}, \replaced{making PPGBP the smallest dataset in terms of total duration.}{with each subject contributing three 2.1-second PPG segments}. 
\end{itemize}







\subsection{Prediction Models} 
\label{sec:III_B}


\heading{Overview of considered model architectures} 
%gamboa2017deep,

The number of DL approaches for time series classification is immense, and we refer to \cite{ mohammadi2024deep, liang2024foundation} for an extensive review of these methods. Since our work pertains to BP estimation, wherein signal processing plays a major role, we leveraged these foundations by evaluating several convolutional neural network architectures (CNN). Also, in this paper, we extended our exploration to include structured state space sequence (S4) models \cite{guefficiently}, which are known for their ability to effectively capture long-range dependencies, and showed promising results for other physiological time series \cite{mehari2023towards,wang2023s4sleep,saab2024towards}.

CNNs have been part of time series analysis for a long time by offering flexibility and scalability in model design. In this work, we evaluated the performance of three main CNN architectures: 

\heading{Simple feed-forward CNN architectures} 
The most straightforward convolutional neural network is a neural network without cycles since it includes data flow only in one direction through its layers. A prototypical example of such architecture is the LeNet1D \cite{lecun1998gradient}. Our work builds on the one-dimensional adaption put forward in \cite{wagner2024explaining}. %, which has been used in signal processing applications \cite{bizopoulos2019signal2image}, due to the simplicity of the design and how easily it can be implemented.

\heading{ResNet-based Architectures} 
The ResNet model has made a crucial stride in DL by proposing skip connections, which enabled easy gradient flow via backpropagation. Here, we draw on one-dimensional ResNet variants, such as XResNet1d50 and XResNet1d101, proposed in \cite{strodthoff2020deep}. %This enables a model to scale up deeper into network levels while easily maintaining performance. These architectures have demonstrated the capability to extract complicated features from intricate physiological signals in signal analysis and enhance classification performance \cite{ott2024using}, \cite{alcaraz2023diffusion},.

\heading{Inception-based Architectures} 
Inception models, originally proposed in computer vision \cite{szegedy2015going}, include several convolutional filters with different kernel sizes to capture a broader spectrum of feature patterns. Furthermore, such a hierarchical feature extraction approach has benefited physiological signal analysis, where intricate generalized patterns are to be captured for proper interpretation. Specifically, several previous studies have applied Inception1D \cite{ismail2020inceptiontime} in time series classification tasks and reported its excellent performance owing to its good representation capability with comprehensive and diverse features \cite{strodthoff2020deep, ismail2020inceptiontime}.

\heading{Structured State Space Sequence (S4) Models}
As an alternative model category, we considered a structured state space sequence model, which has been successfully applied to physiological time series \cite{wang2023s4sleep, mehari2023towards} and is known for its ability to capture long-range dependencies in input sequences \cite{gu2021efficiently}. Here, we use a S4 model as a prediction model as in \cite{mehari2023towards}.



\subsection{Training and evaluation procedures}
\label{sec:III-C}
\heading{Training procedure}
%\sout{\colnst{loss function, model selection on the validation set to avoid overfitting, input size (in seconds), how we deal with mismatches in signal length}}
For each of the experiments, an effective batch size of 512 was used through gradient accumulation. The learning rates were either found using a learning rate finder \cite{smith2017cyclical} or set to 0.001. Models were trained \deleted{for 30 epochs when using the S4 model and} for 50 epochs \deleted{with other models}. Also, the training routine was implemented with the AdamW optimizer \cite{loshchilov2017decoupled} and mean squared error as loss function. In all cases, \added{consistent with previous studies, e.g., \cite{xiao2024advancing}, we employed two output nodes to jointly predict SBP and DPB, leveraging possible shared physiological features to enhance model performance among SBP and DBP to improve model  performance}. \replaced{We used the validation set score for hyperparameter tuning. As a simple measure to reduce overfitting, we performed model selection based on the validation set score, i.e., during training we kept track of the validation set score and selected the model with the best validation set score for evaluation on ID or OOD test sets.}{We performed model selection based on validation set scores  to avoid overfitting and selected the best-performing model for later evaluation on the test set.}

\added{All of the considered datasets used a sampling frequency of 125~Hz. Models were trained using the samples full input resolution, i.e., 1250 time steps in the case of PulseDB. The fact that all considered model architectures involve a global average-pooling, allows to evaluate models also for other input sizes, such as the native input sizes of the external datasets (625 time steps for BCG, UCI, and Sensors datasets, and 262 time steps for PPGBP) while still leveraging the model trained on PulseDB.}
\deleted{Furthermore, since the sampling frequency of all utilized datasets is 125 Hz, the input size was adjusted for each dataset to fit its segment length. In particular, 1250 samples were used for PulseDB, 625 for the BCG, UCI, and Sensors datasets, and 262 for PPGBP (see, table \ref{tab:tab1}). To overcome the signal length inconsistencies, signals with more than the predefined input size were segmented into fixed-size chunks, while the shorter ones were zero-padded to have consistent dimensions, so that efficient and straightforward processing of all the considered signals, regardless of the original length, was possible for each of the datasets with different characteristics.}




\heading{Performance metrics}
As primary performance metric referenced by BP standards  \cite{iso2018sphygmomanometers, ieee2014ieee}, we report the mean absolute error (MAE) defined by 

\begin{equation}
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |\text{Predicted}_i - \text{Reference}_i|,
\end{equation}

where \(n\) is the number of predictions, \(i\) is the index, \(\text{Predicted}_i\) and \(\text{Reference}_i\) indicate the predicted and reference BP, respectively. As a scale-independent, relative performance measure, we also report the mean absolute scaled error (MASE) \cite{hyndman2006another}, which is defined as

 \begin{equation}
\text{MASE} = \frac{\text{MAE}}{\text{MAE}_{\text{Baseline}}},
\end{equation}

where \(\text{MAE}_{\text{Baseline}}\) represents the median value of the BP of the training set. MASE allows to assess the model performance relative to the simplest, non-parametric predictor.

\subsection{A simple baseline for domain adaptation}
\label{sec:III_D}

\heading{Approaches to domain adaptation} The challenge of dealing with an inevitable mismatch between training and test set distributions is a long-standing one and attracted a lot of interest in the machine learning community, see \cite{kouw2019review} for a recent review. Conventionally, one distinguishes sample-based, feature-based, and inference-based approaches. In this work, we explore the potential benefit of domain adaptation methods in a simple sample-based approach, where we deviate from the paradigm of target-free domain adaptation, through the use of the target domain label distribution. Most importantly, we only make use of the target domain label distribution but not of individual labels, which represents a piece of information that we envision to be typically available in practical use cases. The main motivation is to assess the potential benefit of domain adaptation on the model performance. 
%One of the limitations of the OOD generalization approach is its tendency to overlook the different levels of sample importance due to different distributions. \cite{arjovsky2019invariant} has pointed out that many existing models cannot adapt well when there is a distribution shift, thus leading to poor generalization in novel environments. Also, \cite{gulrajani2020search} has emphasized that most approaches usually don't consider those nuances of how train and test distributions diverge and often result in models that have poorly robust performance when they face unfamiliar data. This motivated the need for approaches that could develop generalization capabilities across diverse and unseen distributions, enabling better adaptability in real-world situations.
%Recently, sample weighting techniques have been incorporated to elicit improvements in the robustness of ML models. Discussion of prediction stability and model misspecification issues has been taken further by \cite{kuang2020stable}, where the authors came up with the DWR algorithm that uses weight assignment methods on the training samples with the goal of decorrelating variables. Also, \cite{shen2020stable} suggested a sample-weighting method that decreases the collinearity of input variables to improve the condition of the design matrix by mitigating multicollinearity issues. This method can be integrated with standard learning methods, presenting better stability of performance across different distributed data. Similarly, \cite{kendall2017uncertainties,gurevich2017pairing, devries2018learning} have proposed to down-weight the loss of the train samples with respect to their amount of uncertainties for OOD detection tasks. Overall, it can be concluded that sample-weighting approaches provide valuable strategies for achieving promising outcomes in the face of data distribution shifts. 



%Figure \ref{fig:fig5} represents the schema of the normalized frequency distributions of both the training and test datasets with respect to the SBP or DBP distribution bins. Our strategy is based on the ratio of test and train frequencies at each bin (\( \frac{h_{\text{test}}}{h_{\text{train}}} \)). The higher ratio indicates the higher importance of the corresponding train samples and, consequently, the requirement to receive more weights in the calculation of the loss function. This approach adjusts the contribution of each sample to the final loss based on its corresponding weight, allowing for key samples to have a larger impact on the training process.



%We first defined the fixed number of bins to cover SBP / DBP distributions of all training and test sets in our study. Then, for each sample, we identified the corresponding bin and consequently computed the weight for each sample weight by

\heading{Reweighting based on label distributions} More specifically, we propose to use an empirical risk minimization approach using sample weights derived from the difference of the label distributions of the respective source domain and target domain test datasets. To this end, we summarize both label distributions in terms of (normalized) histograms, see Figure~\ref{fig:fig5}. For a given training sample that belongs to the bin $i$, we identify the empirical output probability $h_{\text{train},i}$($h_{\text{test},i}$) assessed from the corresponding test set histograms. From that, we define sample weights via
\begin{equation}
w_i = 
\begin{cases} 
\max(\added{\tau}, \frac{h_{\text{test},i}}{h_{\text{train},i}}) & \text{if } h_{\text{train},i} > 0 \\
\added{\tau} & \text{if } h_{\text{train},i} = 0\,,
\end{cases}
\label{eq:weight_calculation}
\end{equation}
where \added{the hyperparameter $\tau$ is used to prevent excluding training samples for which the relative weight $h_{\text{test},i}/h_{\text{train},i}$ is small entirely from the training process. In our experiments, we fixed $\tau=1$,  but found that the results (validation set scores) were not very sensitive to the choice of $\tau$.}%$\alpha$ and \added{$\tau$} are the empirical scaling factors. \added {in our study we consider $\tau=1$ simply to ensure that the weight is equal to 1 whenever $h_{\text{train},i} = 0$ and to prevent weights from being less than 1. This choice is arbitrary, and different values of $\tau$ can be considered to evaluate different assumptions. Similarly, we chose $\alpha$=100, which comes from trying different values for $\alpha$, after experimenting with various values and observing that changes in $\alpha$ (when $\alpha>=1$ ) did not significantly affect the results. We recognize that exploring different values for these two parameters could provide valuable insights. However, conducting such experiments is beyond the scope of this paper. Therefore, we propose investigating the effects of varying $\tau$ and $\alpha$ in future projects.}

Finally, we utilize the sample weights $w_i$ for the loss calculation. More specifically, we use importance weights derived from the SBP distribution for the loss calculated based on the SBP output and importance weights derived from the DBP distribution for the loss calculated based on the DBP output and sum both contributions to obtain the final importance weighted loss.%It is worth mentioning that we derive weights from SBP for SBP prediction and from DBP for DBP prediction to ensure parameter-specific optimization.

%\sout{\colnst{please clarify- we always use weighting factors derived from SBP?}}
%\begin{itemize}
%    \item $w_i$: Weight for the $i$-th bin.
%    \item $h_{\text{test},i}$: Histogram value from the test dataset for the $i$-th bin.
%    \item $h_{\text{train},i}$: Histogram value from the train dataset for the $i$-th bin.
%   
%    \item $\alpha$: Scaling factor for the ratio of the histograms.
%\end{itemize}


%Consequently, the calculated sample weights are implemented into the loss function; First, the unweighted loss value for each sample is calculated. This is followed by an element-wise application of the sample weights to this unweighted loss, thereby computing the final weighted loss. Figure \ref{fig:figure2} illustrates the implementation flowchart of our proposed approach.
\begin{figure}[t]  
  \centering
  \includegraphics[width=\columnwidth]{Fig.1.pdf}
  \caption{Schematic comparison of normalized label distributions (e.g., SBP or DBP) across training and test datasets.}
  \label{fig:fig5}
\end{figure}




\section{Results}
\label{sec:IV}

%The experiments presented in the current section have two main objectives:  (i) to establish an initial set of benchmarking results that will set the route for further developments and provide a reference to which further studies can be compared, and (ii) to illustrate the range of analyses that can be performed using the PulseDB data set so that researchers can test a wide range of hypotheses or research questions beyond the particular benchmarking results.

\heading{Organization of the experiments} In Section \ref{sec:IV-A}, we evaluated classifiers for all nine generated PulseDB subsets, which is the core of our analysis. Section \ref{sec:IV-B} supplements the above analysis by performing ID and OOD generalizations within PulseDB datasets. Also, the OOD generalization on external datasets is investigated in Section \ref{sec:IV-C}. Finally, in Section \ref{sec:IV-D}, we conducted similar analyses as in Sections \ref{sec:IV-B} and \ref{sec:IV-C}, however, using the importance weighting for domain adaptation. We computed and evaluated the results with a focus on reducing the distribution shift between the training and test sets by incorporating a special weight value for each training sample. 



\subsection{Model comparison on PulseDB}
\label{sec:IV-A}


\begin{table*}[ht]
\centering
\caption{Performance of PPG DL models on PulseDB dataset in terms MAE (SBP / DBP) measured in units of mmHg. For each of the experiments, the three best-performing models are marked in bold-face and the overall best-performing model is underlined. The count column enumerates the number of times where the model achieved a result among the top three across all considered scenarios, again differentiating SBP/DBP scenarios.}


%\renewcommand{\arraystretch}{1.5} % Increase cell height by 1.5x
\scalebox{0.7}{
\begin{tabular}{lccccccccc c}
%\toprule
 & \multicolumn{3}{c}{\textbf{Combined}} & \multicolumn{3}{c}{\textbf{Vital}} & \multicolumn{3}{c}{\textbf{MIMIC}} & \textbf{Count} \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}

 & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} 
 & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} 
 & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \\ 
\midrule
\textbf{Baseline (Median)} & 16.66 / 9.85 & 16.48 / 9.75 & 25.48 / 17.29 & 14.92 / 9.52 & 14.88 / 9.43 & 29.85 / 17.84 & 18.16 / 10.07 & 17.69 / 9.94 & 21.23 / 16.82 & 0/0 \\ 
\textbf{Lenet1D} & 12.47 / 7.85 & \textbf{13.88 / 8.52} & \textbf{18.57 / \underline{13.37}} & 11.61 / 7.70 & \textbf{\underline{12.37} / 7.89} & 19.59 / \textbf{11.87} & 14.37 / 8.22 & 15.41 / 8.92 & \underline{\textbf{17.56 / 14.51}} & 4/4 \\ 
%\midrule
\textbf{XResNet1d50} & \textbf{9.96 / 6.34} & 14.12 / 8.56 & 20.49 / 15.11 & \textbf{9.49 / 6.33} & \textbf{12.40 / \underline{7.84}} & \underline{\textbf{17.71 / 11.43}} & \textbf{10.14 / \underline{6.18}} & \textbf{15.36 / 9.09} & 19.02 / 15.94 & 6/6 \\ 
\textbf{XResNet1d101} & \underline{\textbf{9.42 / 5.97}} & \textbf{13.97 / 8.51} & \textbf{19.38 / 14.04} & \underline{\textbf{9.08 / 6.08}} & \textbf{12.70 / 8.05} & \textbf{19.31} / 12.33 & \textbf{\underline{9.52} / 6.64} & 15.47 / 9.26 & \textbf{18.35 / 15.56} & 8/7 \\ 
%\midrule
\textbf{Inception1D} & \textbf{10.37 / 6.98} & \underline{\textbf{13.71 / 8.26}} & \underline{\textbf{18.21}} / \textbf{13.83} & \textbf{9.65 / 6.52} & 14.97 / 8.98 & 19.79 / 12.30 & \textbf{10.52 / 6.52} & \underline{\textbf{12.25 / 7.81}} & \textbf{17.33 / 15.00} & 7/8 \\ 
\textbf{S4} & 13.93 / 8.48 & 14.43 / 8.54 & 19.57 / 15.43 & 13.65 / 8.47 & 13.92 / 8.57 & \textbf{18.40 / 12.19} & 13.62 / 8.38 & \textbf{13.96 / 8.50} & 17.83 / 14.88 & 2/1 \\ 
\bottomrule
\end{tabular}
}
\label{tab:tab3}
\end{table*}







\heading{Overview} We carried out all experiments for the nine subsets in Table \ref{tab:tab1} using all models presented in Section \ref{sec:III_B}. All experiment results (SBP/DBP) are shown in Table \ref{tab:tab3}. The three best-performing models according to MAE are marked in bold-face for each of the experiments.
The best-ranked models, either based on ResNet or Inception architectures, achieved MAEs between 9mmHg (6mmHg) in the Calib Vital subset to approximately 12mmHg (8 mmHg) in the CalibFree Vital and up to 18mmHg (14mmHg) in the AAMI \added{tasks for both the} Combined and MIMIC \added{subsets} for systolic (diastolic) blood pressure. \deleted{The} \added{These } results already provide first insights into the relative complexity of the different prediction tasks \added{across all considered scenarios}. Not surprisingly, the scores achieved for the Calib tasks are comparably lower (compared to CalibFree and AAMI tasks) as the model is able to exploit subject-specific information about test set samples through samples from the corresponding subjects seen during training.
Also, the results of MASE evaluation (Fig. \ref{fig:figure1}) also confirmed that although the MAE values of DBP are inherently smaller than SBP, the MASEs are higher, which at first sight counterintuitively indicates a better performance in the SBP category compared to DBP. A key reason for this is that DBP varies less across the entire dataset, and therefore, the median predictor represents a stronger baseline result. These findings emphasize the importance of not relying entirely on absolute performance metrics such as MAE but also considering relative metrics such as MASE.

\heading{Best-performing models} Aggregating model performance by counting best-performing models across all different setups, we conclude that ResNet and Inception-based frameworks generally represent the best-performing models. The shallow Lenet1D models are competitive in the CalibFree category and partly also in the AAMI category, but fail to achieve competitive results in the Calib scenario, which profits to a certain degree from model capacity to memorize specific patients. For instance, according to Table~\ref{tab:tab3}, Lenet1D is positioned in the same range as state-of-the-art architectures such as ResNet and Inception in distinct subsets (e.g., CalibFree Combined, CalibFree Vital, AAMI Combined, and AAMI MIMIC), e.g., on the subset of CalibFree Combined, Lenet1D has a MAE of 13.88mmHg (8.52 mmHg), which is very close to XResNet1d101 performance with 13.97mmHg (8.51 mmHg), and outperforms XResNet1d50 with 14.12mmHg (8.56 mmHg). This means that Lenet1D, although the simpler architecture, can provide robust results for the estimation of BP \textit{in certain scenarios}, as similarly reported in \cite{wagner2024explaining} for ECG analysis. %It is worth noting that the subsets originating from Vital or MIMIC sources have a considerably smaller training set compared to the Combined version (as shown in \ref{tab:tab2}). Therefore, in order to have a fair comparison, we also present the MASE comparison of the results for DBP (Fig \ref{fig:figure1a}) and SBP (Fig \ref{fig:figure1b}), respectively.
Furthermore, it is worth noting that the S4 model does not show the outstanding performance it demonstrated in the ECG/EEG domain \cite{mehari2023towards,wang2023s4sleep,saab2024towards}.
As a final general observation, throughout most subsets, XResNet1d101 consistently ranks among the top-performing models. Therefore, we select XResNet1d101 as the DL model for the further sections of this paper. 

\begin{figure*}[ht] 
\centering
\includegraphics[width=1\textwidth]{image1.pdf} % Adjust width as needed
\vspace{-2cm}

\caption{MASE scores of different models trained on different PulseDB subsets and training scenarios: (top) SBP and (down) DBP.}
%\sout{\colnst{takes a lot of space maybe just show the top row and the bottom row in the appendix}}
%Thanks for your comment, but if it is possible, let it be there since it is beautiful and also I already moved the large flowchart to the appendix.
\label{fig:figure1}
\end{figure*}






\subsection{Performance evaluation within PulseDB}
\label{sec:IV-B}

In this section, we present the performance of our selected model, XResNet1d101, trained and tested on various subsets of PulseDB. The results are shown in Table \ref{tab:tab4}, where the train and test sets are represented in the vertical and horizontal columns, respectively. \replaced{We organize the results by data source (Combined/Vital/MIMIC) and scenario (Calib/CalibFree/AAMI) in order}{We present the analysis of each table results separately based on the data source and subset type} to provide a comprehensive \replaced{analysis}{understanding} of the results.  







\begin{table*}[h]
\centering
\caption{Performance of ID and OOD generalization on all subsets of PulsedDB dataset for an XResNet1d101 model in terms of MAE (SBP / DBP) given in units of mmHg. Vertical and horizontal subsets represent the train and test sets, respectively. For each of the experiments, the three best-performing scenarios are marked in bold-face, and the overall best-performing scenario is also underlined.} 

%\renewcommand{\arraystretch}{1.5} % Increase cell height
\scalebox{0.7}{
\begin{tabular}{llccccccccc}
%\toprule
 & & \multicolumn{9}{c}{\textbf{Test Sets}} \\
\cmidrule(lr){3-11}
 & & \multicolumn{3}{c}{\textbf{Combined}} & \multicolumn{3}{c}{\textbf{Vital}} & \multicolumn{3}{c}{\textbf{MIMIC}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} & \textbf{9.42 / 5.97} & 15.54 / 9.28 & \underline{\textbf{19.2}} / 14.22 & \textbf{9.27 / \underline{6.08}} & 13.98 / 8.65 & \textbf{18.57} / 12.46 &\textbf{ 9.58} / \underline{\textbf{ 5.86}} & 17.2 / 9.95 & 19.82 / 15.97 \\
& \textbf{CalibFree} & 13.87 / \textbf{8.53 }& \textbf{13.97 / \underline{8.51}} & \textbf{20.09} / 14.73 & 12.34 / 8.23 &\textbf{ 12.58} / 8.10 & 21.73 / 14.41 & 15.5 / 8.85 & \textbf{15.44 / 8.94} & \textbf{18.47} / 15.04 \\
& \textbf{AAMI} & \textbf{13.61 / \underline{8.50}} & \underline{\textbf{13.96}} / 8.52 &\textbf{ 19.38} / 14.04 & 12.19 / 8.12 & \textbf{12.56 / 8.02} & 20.47 / 13.60 & 15.11 / 8.90 & \textbf{15.45 / 9.04} & \underline{\textbf{18.30} / \textbf{14.47}} \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & 14.69 / 9.74 & 16.7 / 10.8 & 21.31 / \textbf{13.95} & \underline{\textbf{9.08 / 6.08}} & 13.92 / 8.74 & \textbf{19.17} /\textbf{ 11.99} & 20.66 / 13.64 & 19.67 / 13.00 & 23.44 / 15.89 \\
& \textbf{CalibFree} & 15.37 / 8.81 & 16.38 / 9.20 & 20.21 / \underline{\textbf{13.30}} & \textbf{10.90 / 7.24} & 12.70 / \textbf{8.05} & \underline{\textbf{17.45 }/ \textbf{11.56}} & 20.14 / 10.49 & 20.29 / 10.42 & 22.94 / 15.02 \\
& \textbf{AAMI} & 15.12 / 9.13 & 14.83 / \textbf{8.89} & 20.21 / \textbf{13.43} & 11.84 / 7.90 & \underline{\textbf{ 12.18 / 7.87}} & 19.31 / \textbf{12.33} & 18.61 / 10.37 & 17.65 / 9.98 & 21.11 / \textbf{14.53} \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & \underline{\textbf{12.91}} / 8.63& 16.6 / 10.34& 22.94 / 16.27& 16.08 / 10.5& 16.24 / 10.27& 26.04 / 17.04& \underline{\textbf{9.52}} / \textbf{6.64} & 16.99 / 10.42& 19.87 / 15.51\\
& \textbf{CalibFree} & 14.69 / 9.02& 15.16 / 9.28& 21.36 / 15.86& 14.24 / 9.12& 14.87 / 9.3& 24.04 / 16.68& 15.16 / 8.92& 15.47 / 9.26 & 18.71 / \textbf{15.04}\\
& \textbf{AAMI} & 14.23 / 9.29& \textbf{14.59} / 9.47& 21.54 / 16.99& 13.7 / 9.96& 14.2 / 10.04& 24.77 / 18.34& \textbf{14.79} / \textbf{8.57}& \underline{\textbf{15.01 / 8.57}}& \textbf{18.35 }/ 15.56 \\
\bottomrule
\end{tabular}
}
\label{tab:tab4}
\end{table*}





\heading{Dependence on training dataset}
For all considered datasets and training scenarios, models trained and tested on the same data (MIMIC, Vital, and Combined) exhibited the lowest errors due to familiarity with the data. However, we stress that the ID performance is an overly optimistic measure of the model's generalization performance to other datasets. Comparing the performance between CalibFree MIMIC and CalibFree Vital shows that the MIMIC model generalizes better overall. While the ID performance for CalibFree Vital is quite strong, the (OOD) evaluation on MIMIC is poor.
%\added{This seems to suggest that MIMIC-based models as compared to Vital-based models show a better OOD, a hypothesis that will not be confirmed when considering other external OOD datasets.}

\heading{Dependence on training scenario}
The results reveal that the lowest MAE for the  Calib subsets is when the training set contains patients from the same dataset, i.e., when corresponding (or combined) Calib training sets are used. This is the expected behavior, as in this case, the models can profit from memorized patient-specific signal patterns observed during training. Surprisingly, Calib models show a reasonable generalization to unseen patients from the CalibFree or AAMI test sets even though they were not trained from this purpose. \deleted{Furthermore, models trained on the Calib datasets have degraded performances when tested within the context of the CalibFree. This effect is unilateral; however, models based on CalibFree do not suffer any negative impact when evaluated within Calib subsets.}\deleted{It} \added{Furthermore, Table \ref{tab:tab4}} shows an exceptional performance on the Calib datasets of MIMIC but is significantly heterogeneous; in other words, though performing very strongly, it gave highly variable results, indicative of its performance variability across different scenarios. AAMI shows the largest overall errors, both in the intra- and inter-data source comparisons; (e.g., for the AAMI Combined, MAE is 13.26mmHg (8.24mmHg)). This is again the expected behavior since AAMI assesses the generalization to unseen patients (as CalibFree) but at the same time for a population covering a broad selection of BP values, i.e., with an inherent mismatch in label distribution compared to the training set distribution.


\subsection{OOD Performance evaluation on external datasets}
\label{sec:IV-C}

This section investigates the OOD performance of models trained on different PulseDB subsets and tested on various external datasets, i.e., Sensors, UCI, PPGBP, and BCG. The results are compiled in Table \ref{tab:tab5}.

\begin{table*}[h]
\centering
\caption{OOD generalization on external datasets using XResNet1d101 (SBP / DBP). Vertical and horizontal subsets represent the training and test sets, respectively. For each of the experiments, the three best-performing results are marked in bold-face, and the overall best-performing result is underlined. A Count column summarizes the number of top-three results for SBP and DBP. }
%\renewcommand{\arraystretch}{1.5} % Increase cell height
\scalebox{0.9}{
\begin{tabular}{llccccc}
%\toprule
& & \multicolumn{4}{c}{\textbf{Test Sets}} & \textbf{Count} \\
\cmidrule(lr){3-6}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Sensors $\downarrow$} & \textbf{UCI $\downarrow$} & \textbf{PPGBP $\downarrow$} & \textbf{BCG $\downarrow$} & \textbf{(SBP / DBP)} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} & 50.9 / 103.3 & \textbf{21.24} / 12.2 & \textbf{18.77 / 9.44} & \textbf{13.37} / 8.12 & 1 / 1 \\
& \textbf{CalibFree} & 21.18 / \textbf{9.78} & 24.76 / \textbf{10.37} & 25.07 / \underline{\textbf{8.2}} & 15.01 / 7.08 & 2 / 3 \\
& \textbf{AAMI} & 28.7 / 11.41 & 32.51 / 11.79 & 27.39 / 9.72 & 16.93 / 7.43 & 0 / 0 \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & \textbf{19.57} / 14.55 & \textbf{22.41} / 13.34 & \textbf{19.72} / 9.85 & 18.16 / 12.6 & 2 / 2 \\
& \textbf{CalibFree} & \textbf{18.46 / \underline{8.6}} & 25.07 / \textbf{10.83} & \textbf{\underline{18.69} / 8.66} & \textbf{\underline{10.05} / 6.92} & 3 / 4 \\
& \textbf{AAMI} & \textbf{\underline{16.28} / 10.66} & \underline{\textbf{19.7 / 10.36}} & 26.86 / 11.68 & \textbf{14.35} / 7.66 & 3 / 2 \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & 32.89 / 23.76 & 43.75 / 28.3 & 33.36 / 15.64 & 26.98 / 12.33 & 0 / 0 \\
& \textbf{CalibFree} & 35.7 / 13.43 & 40.64 / 13.74 & 35.7 / 11.09 & 17.17 / \underline{\textbf{5.89}} & 1 / 3 \\
& \textbf{AAMI} & 40.97 / 15.65 & 44.96 / 16.28 & 35.79 / 10.59 & 21.05 / \textbf{6.54} & 0 / 1 \\
\bottomrule
\end{tabular}
}
\label{tab:tab5}
\end{table*}







\heading{Dependence on training dataset}
A first superficial analysis of Table~\ref{tab:tab5} reveals that with the exception of two BCG results, only models trained on Vital or Combined show good generalization, in the sense of achieving results within the best three results. This suggests that the Vital subset of PulseDB, which is also part of Combined, seems to be an important component for good generalization. \added{This results stands at tension with the result from the previous section, which seemed to indicate that MIMIC-based models show a better generalization performance on Vital than Vital-based models when tested MIMIC.}

\heading{Dependence on training scenario} The CalibFree subsets, particularly "CalibFree Vital", demonstrate strong performances with some of the lowest MAE values across metrics: 18.46 mmHg (8.6 mmHg) for Sensors and 10.05 mmHg (6.92 mmHg) for BCG. Moreover, the performance of CalibFree subsets is more coherent when tested across different sources, often outperforming Calib models, especially for Vital datasets. \added{This aligns with expectations as per training objective Calib models were not incentivized to generalize to unseen patients but rather to overfit to patient-specific patterns from the training set.} The count column further highlights their robustness, with "CalibFree Vital" achieving top-three performances 3 times for SBP and 4 times for DBP.
\added{Also AAMI Vital shows a strong performance on external datasets, achieving three top-three results for SBP and two for DBP. AAMI MIMIC shows a poor performance, which should most likely rather be attributed to the dataset and not to the training scenario.}
\deleted{In contrast, the AAMI subset shows mixed results. While "AAMI Vital" performs strongly, achieving three top-three results for SBP and two for DBP, "AAMI MIMIC" presents significantly higher MAE values, such as 40.97 mmHg (15.65 mmHg) for Sensors, indicating challenges in generalizing to different data sources.} \deleted{This variability suggests that while AAMI models can be effective in specific contexts like Vital datasets, they may face reliability issues when applied to more diverse datasets like MIMIC or Combined.}%\sout{\colnst{I don't buy this point, AAMI vital is a strong model; we should perhaps have a count column as in Table III, this discussion is very handweavy}}

\heading{Better generalization due to dataset similarity}
At this point, one might hypothesize that the generalization capabilities of models trained on different PulseDB subsets is primarily driven by the similarity between the respective training and evaluation datasets. We investigate this hypothesis for the case of CalibFree Vital as training dataset and test on \replaced{external datasets that do not share any data with CalibFree Vital, i.e., the four external datasets as well as CalibFree MIMIC.}{other a datasets}. In Figure~\ref{fig:fig10}, we present a scatterplot of the OOD MAE versus dataset similarity quantified via the Earth Mover's Distance (EMD) \cite{pele2009fast} calculated based on the SBP distribution.

\begin{figure}[htbp]  
  \centering
  \includegraphics[width=1\columnwidth,keepaspectratio]{Dissimilarity_vs_OOD_SBP_SelectedDatasets.pdf}
  \caption{Relationship between Dissimilarity Measure (EMD) and OOD Performance for SBP. The scatter plot shows the correlation between EMD and OOD MAE for SBP. Lower EMD reflects greater similarity to the baseline dataset (CalibFree Vital training set).}
  \label{fig:fig10}
\end{figure}



Figure~\ref{fig:fig10} indeed shows a correlation between dataset dissimilarity (quantified via EMD) and OOD MAE. \added{This suggests that a mismatch between the blood pressure distributions might represent a dominant factor contributing to the domain mismatch between the respective datasets.} \deleted{This suggests that the more significant the distribution shift from the CalibFree Vital dataset, the more the model performance (SBP values) degrades, hence showing the role of a similar dataset in holding up robust model generalization.}










\subsection{Sample-Weighting Approach for OOD Generalization}
\label{sec:IV-D}

\heading{Overview} The findings \deleted{towards the end} of the previous subsection suggest that the results presented in Table~\ref{tab:tab5} might be biased by dataset similarity, which might obfuscate the search for the training dataset and training scenario that leads to best generalization. In this section, we therefore investigate this hypothesis using the domain adaptation approach introduced in Section~\ref{sec:III_D}. \deleted{We empirically fix the bin numbers and scaling factor in Equation~\ref{eq:weight_calculation} through cross-validation on the training set. We evaluate multiple candidate values by optimizing the prediction performance on a validation set, measured by MAE. The value that achieves the best validation performance is then fixed and applied during training.}


%This section utilized the weighted OOD strategy described in Section \ref{sec:III_D}. Similar to the previous section, the model is trained on all nine subsets of PulseDB and tested on both the PulseDB and the external subsets to quantify its performance on different datasets. It is also important to note that the scaling factor of equation \ref{eq:weight_calculation} is determined empirically based on prior experience and observations to achieve optimal weighting. In our study, we consider $\alpha$ = 100 and fixed 200 bins, each covering 1.4 mmHg, which is suitable for all tasks. Finally, the results are tabulated in Tables \ref{tab:tab6} and \ref{tab:tab7}, respectively. 






%\subsection*{Table 10: Difference in MAE (Weighted - unweighted)}

\begin{table*}[h]
\centering
\caption{Difference in MAE (weighted - unweighted) for ID and OOD generalization on all categories of PulsedDB dataset using XRes
Net1d101 (SBP / DBP). Positive values (red) indicate a degradation (increase in MAE), while negative values (green) indicate an improvement (decrease in MAE). \added{The mean improvement through importance weighting across all scenarios is given by 0.43~mmHg for SBP and 0.31~mmHg for DBP.}}
%\renewcommand{\arraystretch}{1.2}
\scalebox{0.55}{
\begin{tabular}{llccccccccc}
\toprule
& & \multicolumn{9}{c}{\textbf{Test Sets}} \\
\cmidrule(lr){3-11}
& & \multicolumn{3}{c}{\textbf{Combined}} & \multicolumn{3}{c}{\textbf{Vital}} & \multicolumn{3}{c}{\textbf{MIMIC}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Calib} & \textbf{CalibFree} & \textbf{AAMI} & \textbf{Calib} & \textbf{CalibFree} & \textbf{AAMI} & \textbf{Calib} & \textbf{CalibFree} & \textbf{AAMI} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
% --- TRAIN = Combined ---
& \textbf{Calib} 
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    & \diffcellsplitside{degradation}{+0.01}{degradation}{+0.32} 
    & \diffcellsplitside{degradation}{+0.35}{improvement}{-0.36} 
    & \diffcellsplitside{improvement}{-0.22}{improvement}{-0.12} 
    & \diffcellsplitside{improvement}{-0.21}{degradation}{+0.06} 
    & \diffcellsplitside{improvement}{-0.28}{improvement}{-0.88} 
    & \diffcellsplitside{improvement}{-0.30}{improvement}{-0.01} 
    & \diffcellsplitside{degradation}{+0.21}{improvement}{-0.15} 
    & \diffcellsplitside{degradation}{+0.38}{degradation}{+0.84} \\
% --- Row: calibfree_combined ---
& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+0.55}{degradation}{+0.12} 
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00}  
    & \diffcellsplitside{improvement}{-1.44}{improvement}{-1.62} 
    & \diffcellsplitside{improvement}{-0.39}{improvement}{-0.34}
    & \diffcellsplitside{improvement}{-0.28}{improvement}{-0.26}
    & \diffcellsplitside{improvement}{-3.91}{improvement}{-3.64} 
    & \diffcellsplitside{improvement}{-0.18}{degradation}{+0.01} 
    & \diffcellsplitside{improvement}{-0.09}{improvement}{-0.02} 
    & \diffcellsplitside{degradation}{+0.34}{degradation}{+0.34} \\
% --- Row: aami_combined ---
& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-0.19}{degradation}{+0.22}
    & \diffcellsplitside{degradation}{+0.23}{degradation}{+0.32}
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    & \diffcellsplitside{improvement}{-3.08}{improvement}{-2.11}
    & \diffcellsplitside{degradation}{+0.28}{degradation}{+0.53}
    & \diffcellsplitside{improvement}{-1.62}{improvement}{-1.12}
    & \diffcellsplitside{improvement}{-0.05}{degradation}{+0.10} 
    & \diffcellsplitside{degradation}{+0.26}{degradation}{+0.69} 
    & \diffcellsplitside{degradation}{+0.08}{degradation}{+0.70} \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
% --- Row: calib_vital ---
& \textbf{Calib} 
    & \diffcellsplitside{degradation}{+0.39}{improvement}{-0.77}
    & \diffcellsplitside{degradation}{+0.33}{improvement}{-0.77}
    & \diffcellsplitside{improvement}{-1.01}{improvement}{-0.60}    
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    & \diffcellsplitside{improvement}{-0.22}{improvement}{-0.13}
    & \diffcellsplitside{improvement}{-2.47}{improvement}{-1.39} 
    & \diffcellsplitside{improvement}{-10.49}{improvement}{-7.20} 
    & \diffcellsplitside{improvement}{-0.61}{improvement}{-0.91} 
    & \diffcellsplitside{improvement}{-1.10}{degradation}{+0.27} \\
% --- Row: calibfree_vital ---
& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+1.30}{degradation}{+0.35} 
    & \diffcellsplitside{improvement}{-1.30}{improvement}{-0.43}
    & \diffcellsplitside{improvement}{-0.08}{improvement}{-0.49}
    & \diffcellsplitside{improvement}{-0.39}{degradation}{+0.12}  
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    & \diffcellsplitside{improvement}{-0.92}{improvement}{-1.25}
    & \diffcellsplitside{improvement}{-0.29}{improvement}{-0.41} 
    & \diffcellsplitside{improvement}{-2.12}{improvement}{-0.76} 
    & \diffcellsplitside{improvement}{-2.91}{improvement}{-0.49} \\
% --- Row: aami_vital ---
& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-0.13}{improvement}{-0.09}
    & \diffcellsplitside{degradation}{+0.26}{improvement}{-0.04} 
    & \diffcellsplitside{degradation}{+0.27}{degradation}{+0.23}
    & \diffcellsplitside{improvement}{-0.28}{improvement}{-0.09} 
    & \diffcellsplitside{degradation}{+0.40}{degradation}{+0.29}
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00}
    & \diffcellsplitside{degradation}{+0.12}{improvement}{-0.44} 
    & \diffcellsplitside{degradation}{+0.42}{improvement}{-0.47} 
    & \diffcellsplitside{improvement}{-0.86}{improvement}{-0.42} \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
% --- Row: calib_mimic ---
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-0.15}{degradation}{+0.27}
    & \diffcellsplitside{degradation}{+0.29}{degradation}{+0.58} 
    & \diffcellsplitside{improvement}{-1.02}{improvement}{-0.84} 
    & \diffcellsplitside{degradation}{+0.13}{degradation}{+1.03}    
    & \diffcellsplitside{improvement}{-0.33}{degradation}{+0.74}
    & \diffcellsplitside{nochange}{0.00}{degradation}{+0.09}    
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 


    & \diffcellsplitside{degradation}{+0.22}{improvement}{-0.36} 

     
    & \diffcellsplitside{degradation}{+0.24}{degradation}{+0.63} \\
% --- Row: calibfree_mimic ---
& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+0.20}{degradation}{+0.17}
    & \diffcellsplitside{improvement}{-0.42}{degradation}{+0.61} 
    & \diffcellsplitside{improvement}{-0.38}{improvement}{-1.67} 
    
    & \diffcellsplitside{degradation}{+0.32}{degradation}{+1.18}
    & \diffcellsplitside{improvement}{-0.32}{improvement}{-0.04} 
    & \diffcellsplitside{improvement}{-0.62}{improvement}{-4.41} 
    
    & \diffcellsplitside{degradation}{+0.20}{degradation}{+0.02} 


    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    

    & \diffcellsplitside{improvement}{-0.20}{improvement}{-0.38} \\
% --- Row: aami_mimic ---
& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-0.28}{improvement}{+0.48}
    & \diffcellsplitside{degradation}{+1.06}{degradation}{+0.97}
    & \diffcellsplitside{improvement}{-0.56}{improvement}{-0.57} 
    
    & \diffcellsplitside{degradation}{+1.26}{degradation}{+1.29}
    & \diffcellsplitside{degradation}{+1.04}{improvement}{-0.31} 
    & \diffcellsplitside{degradation}{+0.73}{degradation}{+0.13} 
    
    & \diffcellsplitside{improvement}{-1.46}{improvement}{-0.65} 
 

    & \diffcellsplitside{degradation}{+0.65}{degradation}{+0.74} 


    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} \\
\bottomrule
\end{tabular}
}
\label{tab:tab_diff_1}
\end{table*}





\heading{Overall impact of importance weighting}
The results of this analysis are compiled in Tables \added{~\ref{tab:tab_diff_1}} and \added{~\ref{tab:tab_diff_2}}, highlighting evaluation results within PulseDB and on external datasets. \added{The tables in the main text highlight the difference between the weighted and unweighted MAE values, whereas the corresponding Tables ~\ref{tab:tab6} and \ref{tab:tab7} in the supplementary material indicate absolute MAE scores achieved via importance weighting.}
The results represent direct analogues of Table~\ref{tab:tab4} and Table~\ref{tab:tab5}, however, using sample weights during training. In \added{86 of 153, i.e., in 56\%} of the cases, the importance weighting leads to improved scores. \added{The mean improvement achieved through importance weighting across all scenarios is given by 0.43~mmHg for SBP and 0.31~mmHg for DBP.} \added{Under AAMI vital test sets, our importance weighting approach improved blood pressure estimation accuracy by up to ~4mmHg. Although modest, these improvements are noteworthy given the strict accuracy requirements of AAMI and represent an important step toward clinical viability.} As before, we proceed by analyzing the results from Table~\ref{tab:tab_diff_1} in more detail.



\heading{Dependence on training dataset}
\replaced{ when comparing across different datasets, Combined and Vital-based models profit most from importance weighting. Before importance weighting (Table~\ref{tab:tab4}) and after applying it (Table~\ref{tab:tab6}), the best-performing models change: importance weighting seems to slight shift from Vital-based models to Combined-based models. However, it is worth noting that Combined-based models when evaluated on Vital or MIMIC are always evaluated ID, as the two are proper subsets of Combined.} {Most models achieve their best performance when trained and tested within the same subsets. For instance, Calib Combined achieves the lowest MAE values of 9.42 mmHg (5.97 mmHg) for SBP and DBP, respectively, and Calib Vital achieves 8.94 mmHg (5.92 mmHg), indicating optimal results when training and testing occur within the same distribution. Models trained on Combined datasets demonstrate strong generalization across multiple test sets by capturing patterns from both sources. For example, CalibFree Combined achieves 13.97 mmHg (8.51 mmHg) on the Combined test set, while maintaining competitive results on MIMIC (15.75 mmHg (8.96 mmHg)) and Vital (12.32 mmHg (7.98 mmHg)) test sets. This indicates that Combined models effectively leverage diverse training data to handle OOD scenarios better than models trained solely on MIMIC or Vital. However, higher MAE values, such as 19.87 mmHg (16.48 mmHg) for Calib Combined tested on AAMI Combined, suggest that Combined models may still face challenges in generalizing to distinctly different distributions, e.g., AAMI.}
\added{Additionally, while importance weighting partially alleviates the aforementioned performance deficiencies of Vital-based models evaluated on MIMIC, it cannot fully completely overcome them.}
%\sout{\colnst{these statements are very vague and hardly justified, we are comparing apples to oranges as some models were trained on certain datasets and others not}}

\heading{Dependence on training and testing scenarios}
\added{In terms of training scenarios, there is no clear trend in the sense of models that profit particularly from importance weighting.}
\deleted{The models trained with subsets of Calib (``Calib Combined'', ``Calib Vital'' and ``Calib MIMIC'') mostly perform well when being tested on each other. This reflects that these models retain a degree of adaptability across related sources. However, this would degrade drastically if models that are trained with the Calib were tested on AAMI subsets, insinuating that Calib-trained models may have very limited generalization capability against larger, more diverse, or unseen datasets.}
\deleted{The test performance on the subsets, especially for CalibFree is comparably well-balanced. The best-balanced performances were even achieved for the ``CalibFree Vital'' subset: 12.70mmHg (8.05mmHg). In this regard, CalibFree models might be much more adaptable and, therefore, better at handling OOD data than Calib models, yet they still struggle when tested against AAMI subsets. \replaced{Overall, these results highlight that although training on related distributions results in strong intra-set performance, there are considerable difficulties in generalizing to substantially different or more diverse test settings.}{The highest MAE scores \added{occur when} \deleted{from using} AAMI subsets \added{are used} as test sets for models trained on the subsets of Calib and CalibFree \pcn{[does this sentence make sense?]}. For example, ``AAMI Combined'' as a test set yields an error of 20.26mmHg (14.19mmHg) for a model trained with ``Calib Combined''. This indicates the difficulty inherent in the AAMI subsets and further suggests that models trained on less diverse data cannot generalize well to AAMI data.}}
\added{However, it is noteworthy, that evaluation on AAMI (with the sole exception of AAMI Combined) profits most consistently from importance weighted training. This aligns with expectations since AAMI is typically furthest from the Calib/Calibfree distributions.}




\begin{comment}
\subsection*{Table 9: Difference in MAE (Weighted - unweighted)}

\begin{table*}[h]
\centering
\caption{Difference in MAE (Weighted - unweighted) for SBP and DBP. Positive values indicate a degradation (increase in MAE), while negative values indicate an improvement (decrease in MAE).}
\renewcommand{\arraystretch}{1.5} % Increase cell height for better visibility
\scalebox{0.8}{
\begin{tabular}{llccccc}
\toprule
& & \multicolumn{4}{c}{\textbf{Test Sets}} & \textbf{Count} \\
\cmidrule(lr){3-6}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Sensors $\downarrow$} & \textbf{UCI $\downarrow$} & \textbf{PPGBP $\downarrow$} & \textbf{BCG $\downarrow$} & \textbf{(SBP / DBP)} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-29.47 $\downarrow$}{improvement}{-92.99 $\downarrow$} 
    & \diffcellsplitside{degradation}{+0.68 $\uparrow$}{improvement}{-1.12 $\downarrow$} 
    & \diffcellsplitside{degradation}{+4.14 $\uparrow$}{degradation}{+3.02 $\uparrow$} 
    & \diffcellsplitside{improvement}{-0.77 $\downarrow$}{improvement}{-0.59 $\downarrow$} 
    & 1 / 1 \\
    
& \textbf{CalibFree} 
    & \diffcellsplitside{improvement}{-1.42 $\downarrow$}{degradation}{+1.06 $\uparrow$} 
    & \diffcellsplitside{degradation}{+6.69 $\uparrow$}{degradation}{+5.46 $\uparrow$} 
    & \diffcellsplitside{improvement}{-0.97 $\downarrow$}{degradation}{+0.26} 
    & \diffcellsplitside{improvement}{-4.70 $\downarrow$}{degradation}{+0.04} 
    & 2 / 3 \\
    
& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-11.40 $\downarrow$}{improvement}{-1.69 $\downarrow$} 
    & \diffcellsplitside{improvement}{-5.39 $\downarrow$}{degradation}{+1.07 $\uparrow$} 
    & \diffcellsplitside{improvement}{-2.41 $\downarrow$}{degradation}{+0.26} 
    & \diffcellsplitside{degradation}{+0.55 $\uparrow$}{degradation}{+1.99 $\uparrow$} 
    & 2 / 1 \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-0.32 $\downarrow$}{improvement}{-0.92 $\downarrow$} 
    & \diffcellsplitside{degradation}{+3.09 $\uparrow$}{improvement}{-2.10 $\downarrow$} 
    & \diffcellsplitside{degradation}{+1.38 $\uparrow$}{degradation}{+0.45 $\uparrow$} 
    & \diffcellsplitside{improvement}{-4.38 $\downarrow$}{improvement}{-3.79 $\downarrow$} 
    & 2 / 2 \\
    
& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+0.69 $\uparrow$}{improvement}{+4.13 $\downarrow$} 
    & \diffcellsplitside{improvement}{-2.24 $\downarrow$}{improvement}{+1.50 $\uparrow$} 
    & \diffcellsplitside{improvement}{+0.60 $\downarrow$}{improvement}{+2.35 $\downarrow$} 
    & \diffcellsplitside{improvement}{-0.16 $\downarrow$}{improvement}{+0.67 $\uparrow$} 
    & 3 / 4 \\
    
& \textbf{AAMI} 
    & \diffcellsplitside{degradation}{+1.79 $\uparrow$}{degradation}{+1.63 $\uparrow$} 
    & \diffcellsplitside{degradation}{+3.63 $\uparrow$}{degradation}{-0.15 $\downarrow$} 
    & \diffcellsplitside{improvement}{-9.87 $\downarrow$}{improvement}{-2.53 $\downarrow$} 
    & \diffcellsplitside{improvement}{-2.78 $\downarrow$}{improvement}{-1.37 $\downarrow$} 
    & 3 / 3 \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-11.88 $\downarrow$}{improvement}{-13.56 $\downarrow$} 
    & \diffcellsplitside{improvement}{-17.22 $\downarrow$}{improvement}{-12.43 $\downarrow$} 
    & \diffcellsplitside{improvement}{-11.89 $\downarrow$}{improvement}{-5.70 $\downarrow$} 
    & \diffcellsplitside{improvement}{-6.19 $\downarrow$}{improvement}{-4.18 $\downarrow$} 
    & 0 / 0 \\
    
& \textbf{CalibFree} 
    & \diffcellsplitside{improvement}{-11.54 $\downarrow$}{improvement}{-1.33 $\downarrow$} 
    & \diffcellsplitside{improvement}{-7.39 $\downarrow$}{degradation}{+8.09 $\uparrow$} 
    & \diffcellsplitside{improvement}{-11.83 $\downarrow$}{degradation}{+0.56 $\uparrow$} 
    & \diffcellsplitside{improvement}{-3.25 $\downarrow$}{improvement}{+0.67 $\uparrow$} 
    & 0 / 1 \\
    
& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-10.50 $\downarrow$}{improvement}{-3.10 $\downarrow$} 
    & \diffcellsplitside{improvement}{-22.93 $\downarrow$}{improvement}{-3.13 $\downarrow$} 
    & \diffcellsplitside{improvement}{-12.39 $\downarrow$}{degradation}{+1.32 $\uparrow$} 
    & \diffcellsplitside{degradation}{+1.14 $\uparrow$}{degradation}{+2.02 $\uparrow$} 
    & 1 / 1 \\
\bottomrule
\end{tabular}
}
\label{tab:tab10}
\end{table*}
\end{comment}



\begin{table*}[h]
\centering
\caption{Difference in MAE (Weighted - unweighted) for OOD generalization on external datasets (SBP / DBP). Positive values (red) indicate a degradation (increase in MAE), while negative values (green) indicate an improvement (decrease in MAE). \added{The mean improvement through importance weighting across all scenarios is given by 3.39~mmHg for SBP and 3.43~mmHg for DBP.}}
%\renewcommand{\arraystretch}{1.2} % Slightly increase cell height for better readability
\scalebox{0.8}{ % Further reduced scaling factor
\begin{tabular}{llcccc} % Removed the last 'c' for "Count" column
\toprule
& & \multicolumn{4}{c}{\textbf{Test Sets}} \\ % Removed "Count" header
\cmidrule(lr){3-6}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Sensors} & \textbf{UCI} & \textbf{PPGBP} & \textbf{BCG} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-29.28}{improvement}{-89.09} 
    & \diffcellsplitside{improvement}{-0.41}{degradation}{+0.66} 
    & \diffcellsplitside{degradation}{+1.50}{improvement}{-1.10} 
    & \diffcellsplitside{improvement}{-0.57}{degradation}{+1.02} \\
    
& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+9.68}{degradation}{+1.72} 
    & \diffcellsplitside{improvement}{-3.89}{degradation}{+3.28} 
    & \diffcellsplitside{degradation}{+7.92}{degradation}{+3.39} 
    & \diffcellsplitside{improvement}{-2.37}{degradation}{+0.27} \\
    
& \textbf{AAMI} 
    & \diffcellsplitside{degradation}{+11.21}{degradation}{+0.93} 
    & \diffcellsplitside{degradation}{+13.32}{degradation}{+1.01} 
    & \diffcellsplitside{degradation}{+11.86}{degradation}{+1.68} 
    & \diffcellsplitside{improvement}{-0.29}{improvement}{-1.05} \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-1.49}{improvement}{-2.21} 
    & \diffcellsplitside{improvement}{-0.74}{improvement}{-3.04} 
    & \diffcellsplitside{degradation}{+1.77}{degradation}{+1.31} 
    & \diffcellsplitside{improvement}{-6.69}{improvement}{-4.11} \\
    
& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+1.61}{improvement}{-0.18} 
    & \diffcellsplitside{improvement}{-1.04}{degradation}{+0.25} 
    & \diffcellsplitside{degradation}{+3.06}{degradation}{+1.92} 
    & \diffcellsplitside{degradation}{+0.67}{improvement}{-0.35} \\
    
& \textbf{AAMI} 
    & \diffcellsplitside{degradation}{+0.75}{improvement}{-3.10} 
    & \diffcellsplitside{degradation}{+0.06}{improvement}{-1.39} 
    & \diffcellsplitside{improvement}{-9.68}{improvement}{-3.52} 
    & \diffcellsplitside{improvement}{-4.34}{improvement}{-0.15} \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-13.10}{improvement}{-14.52} 
    & \diffcellsplitside{improvement}{-23.36}{improvement}{-16.86} 
    & \diffcellsplitside{improvement}{-10.63}{improvement}{-4.86} 
    & \diffcellsplitside{improvement}{-11.92}{improvement}{-4.59} \\
    
& \textbf{CalibFree} 
    & \diffcellsplitside{improvement}{-12.66}{improvement}{-2.82} 
    & \diffcellsplitside{improvement}{-13.61}{degradation}{+3.30} 
    & \diffcellsplitside{degradation}{+10.05}{degradation}{+6.36} 
    & \diffcellsplitside{improvement}{-7.42}{improvement}{-0.04} \\
    
& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-11.51}{improvement}{-0.02} 
    & \diffcellsplitside{improvement}{-22.94}{improvement}{-6.48} 
    & \diffcellsplitside{degradation}{+1.71}{degradation}{+7.44} 
    & \diffcellsplitside{improvement}{-9.22}{degradation}{+1.43} \\
\bottomrule
\end{tabular}
}
\label{tab:tab_diff_2}
\end{table*}


\heading{OOD generalization for models trained with importance weighting}
The impact of the importance weighting approach on OOD generalization on the external datasets is shown in Table \ref{tab:tab_diff_2} and \ref{tab:tab7}. In 42 of the $2\times 9\times 4=72$ cases, i.e., in 58\% of the cases, the approach improved the scores. \added{However, this statistic obfuscates the true picture as it does not take into account the magnitude of the improvement/degregation. The mean improvement through importance weighting across all scenarios is given by 3.39~mmHg for SBP and 3.43~mmHg for DBP and therefore substantially larger than the improvements within PulseDB, see Table~\ref{tab:tab_diff_1}.} The table shows that the importance of weighting significantly improves the performance of all MIMIC datasets. \deleted{It can be seen that without importance weighting, the potential of MIMIC datasets remains concealed, while they are expected to be inherently limited for optimal performance.} As before, we proceed with a more detailed analysis of Table~\ref{tab:tab7}.




\heading{Dependence on training dataset}
\replaced{These results underscore that choosing the right training data can significantly enhance generalization on specific test sets. For example, training on the Vital dataset achieved 14 best-performing scenarios (highlighted in bold) out of 24 cases, whereas training on the MIMIC and Combined datasets resulted in only 6 and 3 best-performing models, respectively. Interestingly, the largest and most diverse training dataset, namely Combined, does not lead to the best generalization.} {The ``Calib Combined,'' ``Calib Vital,' and ``Calib MIMIC'' subsets show varying performances across source comparisons, with notable differences in generalization. Among them, the 'Calib Combined' subset demonstrated lower SBP/DBP errors on the UCI dataset at 21.92 mmHg (11.08 mmHg). However, its generalization performance was weaker on other subsets, as indicated by higher MAE rates on external datasets. The performances of the CalibFree subsets are more scattered while having certain advantages, such as ``CalibFree Vital'' obtaining one of the lowest errors in BCG 9.89 mmHg (6.34 mmHg) and SBP of the other externals sets, proving more adaptable with respect to the external data than Calib subsets. That fact would allow the suggestion that CalibFree models might have wider generalization capability, especially when adapting to various test sets.}


\heading{Dependence on training scenario}
AAMI subsets, especially ``AAMI Vital'', report lower MAE compared to other subsets, such as 17.03mmHg (7.56 mmHg) (Sensors), 19.76mmHg (8.97 mmHg) (UCI), and 17.18mmHg (8.16 mmHg) (PPGBP), and it acquired one of the best performances on BCG at 10.01mmHg (7.51 mmHg). \added{This might relate to the broad blood pressure distibution of AAMI, which serves as a basis for importance weighting approaches.}\deleted{That would suggest that the models originating from AAMI subsets can possess a fine-tuned ability to handle complex OOD data, outperforming the capabilities of both the Calib and CalibFree subsets for adaptation into new environments.} 

\heading{Comparison to literature results}
A side-by-side comparison between our OOD generalization performance and ID (ML) evaluation results provided by \cite{gonzalez2023benchmark} would provide important insight into the strengths and weaknesses of our approach. Table \ref{tab:tab9} presented the ID results based on ResNet and SpectroResNet \cite{slapnivcar2019blood} models, which are similar architectures to our XResNet1d101 for feature extractions. It is worth noting that the SpectroResNet model integrates a ResNet-GRU architecture that will capture both temporal and spectro-temporal information effectively. It is important to stress that these results report in-distribution performance evaluation and are compared to models trained on PulseDB subsets. To this end, we compiled results for  ``AAMI vital'' and ``CalibFree MIMIC'' were selected since within the tables \ref{tab:tab4},\ref{tab:tab5}, \ref{tab:tab6} and \ref{tab:tab7}, as the best-performing models from the previous analysis. \added{Most notably, models show a solid OOD evaluation performance reaching the performance level of models trained on these dataset, i.e. ID performance. This is the case for AAMI Vital, both with and without importance weighting, on Sensors  and for AAMI Vital and CalibFree MIMIC with importance weighting on BCG.}

\begin{table*}[t] % Change [h] to [H]
\centering
\caption{Unweighted OOD evaluation vs. importance-weighted OOD evaluation\added{, both trained using XResNetd101 model,} in comparison to in-distribution results achieved on external dataset. For each of the experiments, the three best-performing models are marked in bold-face and the overall best-performing model is underlined.}
%\renewcommand{\arraystretch}{1.5} % Increase cell height by 1.5x
\scalebox{1}{
\begin{tabular}{llcccc}
%\toprule
& & \multicolumn{4}{c}{\textbf{Test Sets}} \\
\cmidrule(lr){3-6}
\textbf{Experiment Type} & \textbf{Approach} & \textbf{Sensors $\downarrow$} & \textbf{UCI $\downarrow$} & \textbf{PPGBP $\downarrow$} & \textbf{BCG $\downarrow$} \\
\midrule
\multirow{2}{*}{ID (\cite{gonzalez2023benchmark})}
& ResNet & 17.46 / \textbf{8.33} & \underline{\textbf{16.59} / \textbf{8.30}} & \textbf{13.62} / \textbf{8.61} & 12.20 / 7.76 \\
& SpectroResNet & \textbf{17.29} / \textbf{9.73} & 21.92 / 10.21 & \underline{\textbf{11.01}} / \textbf{8.46} & \textbf{9.89 / 6.29} \\
\midrule
\multirow{2}{*}{OOD evaluation (Ours)} 

& Trained on AAMI Vital & \textbf{\underline{16.28}} / 10.66 & \textbf{19.70 / 10.36} & 26.86 / 11.68 & 14.35 / 7.66 \\

& Trained on CalibFree MIMIC & 35.7 / 13.43 & 40.64 / 13.74 & 35.7 / 11.09 & 17.17 / \textbf{5.89} \\
\midrule
\multirow{2}{*}{Importance weighting (Ours)}

& Trained on AAMI Vital & \textbf{17.03 / \underline{7.56}} & \textbf{19.76 / 8.97} & \textbf{17.18} / \textbf{\underline{8.16}} & \textbf{10.01} / 7.51 \\

& Trained on CalibFree MIMIC & 23.04 / 10.61 & 27.03 / 17.04 & 45.75 / 17.45 & \underline{\textbf{9.75 / 5.85}} \\
\bottomrule
\end{tabular}
}
\label{tab:tab9}
\end{table*}






\deleted{The key observations of Table \ref{tab:tab9} are as follows: Our unweighted OOD approach demonstrated competitive results with respect to most ID results. For instance, ``AAMI Vital'' subset in the unweighted OOD setting obtained 16.28mmHg (10.66mmHg) for Sensors and 19.7mmHg (10.36mmHg) for the UCI category, closely comparable to that of SpectroResNet at 17.29mmhg (9.73mmHg) and 21.92mmHg (10.21mmHg), respectively. However, our very high \replaced{26.86mmHg (11.68mmHg)}{11.68mmHg (8.66mmHg)} was observed in the PPGBP, outperforming that of SpectroResNet 11.01mmHg (8.46mmHg).}
\deleted{Importance weighting was particularly beneficial for the case of the small-scale BCG dataset, where model reached scores in line with in-distribution performance.}



\section{Discussion}
\label{sec:V}
\heading{ID performance does not reflect OOD generalization}
\added{A common trend across all tables is that models achieve a consistently lower MAE score when evaluated on the respective matching test set, i.e. when evaluated ID. However, this is typically largely exceeded when evaluating on OOD datasets or even in other training scenarios based on the same training dataset. This message nicely aligns with the findings of \cite{weber2023intensive}, who investigated generalization issues between PPGBP and MIMIC leveraging feature-based models. The magnitude of the performance degradation depends heavily on the training dataset and scenario, but in particular MIMIC-based models show a poor OOD generalization performance. On the hand, there are also combinations of training dataset and scenarios such Calibfree Vital or AAMI Vital that show a particularly good generalization performance.
}  \deleted{in all tables is that models have the lowest error when they are trained and tested on the same subset or source, for example, ``Calib Combined'' in Table \ref{tab:tab6}, which is the one that is trained on Calib and has been evaluated on the same, resulting in values of 9.42 mmHg (5.97 mmHg) for SBP/DBP steadily produced the lowest MAE or weighted OOD generation errors. However, we would like to reinforce} \added{Most importatnly, these observations reinforce that ID scores should not be considered as representative for the generalization capabilities of the model to unseen data.}



\deleted{\heading{OOD Generalization Capabilities of CalibFree and AAMI Subsets}
The main advantage of CalibFree, particularly ``CalibFree Vital'', models lies in their adaptability to a wide range of test sets. However, they still encountered some difficulties, especially when applied to more complex external datasets, as noted in Table \ref{tab:tab7} where errors as high as 24.03 mmHg (11.08 mmHg) were found for UCI. This means that even though the generalization capabilities for the CalibFree models look promising, there is not guarantee that the models will generalize to any OOD dataset.}

\deleted{Also, the AAMI subsets, specifically ``AAMI Vital'' always  had the best performance on external test sets with some of the lowest error rates (Table \ref{tab:tab5} and Table \ref{tab:tab7}). This could imply that AAMI-trained models are potentially capable of the most robust generalization and are successful in generalizing complex OOD datasets, which, in turn, should make them more versatile in real-world applications.}

\deleted{\heading{Contradictions and Challenges of OOD Generalization}
The results are significantly different when models trained on one subset are evaluated on another. For instance, Calib-trained models suffer degraded performance when tested on CalibFree or AAMI subsets (Table \ref{tab:tab4} and Table \ref{tab:tab6}). Such unilateral effects were evident in the CalibFree models not being as negatively impacted when tested on Calib subsets, suggesting asymmetric adaptability. This would imply that some subsets have features or properties that are less transferable; hence, training models may not generalize very well.}

\heading{Effect of domain adaptation}
\added{For a fixed training dataset, the OOD performance showed a correlation with the similarity between training and test datasets assessed via their BP distributions. This lead to the exploration of domain adaption through importance weighting with importance weights inferred from the difference between the two respective BP distributions. The importance weighting approach lead to improved OOD performance.}
\deleted{Importance weighting tends to perform better on the different test sets, as can be seen from Table \ref{tab:tab6} and Table \ref{tab:tab7}. This \added{improvement} is in principle, an expected result since the importance weighting approach makes use of target dataset properties, but it should not taken for granted. Also, importance weighting yielded more consistent error rates across external test sets, as demonstrated in ``AAMI Vital'' and ``CalibFree MIMIC'', where performance was competitive even when faced with diverse data distributions. This would hint that the weighted approach might better capture the peculiarities of different subsets and thus provide more stable performance across changing conditions.
Furthermore, importance weighting has indeed strengthened the generalization capabilities, but it is far from completely resolving the performance degradation the subsets are putting into AAMI. The AAMI subsets still contributed a relatively high error-for example, ``Calib Combined'' tested on ``AAMI Combined'' at 19.56mmHg (13.86mmHg) in Table \ref{tab:tab6}.}
In general, importance weighting seems to represent an appropriate method to enhance the generalization on unseen external datasets, especially on subsets that represent more variability, such as the CalibFree and AAMI datasets. \added{At this point, it is worth stressing that the presented approach using reweighting based on the BP distribution, obviously only captures differences in the BP distributions, whereas the datasets will typically differ according to many other criteria such as patient characteristics, sensor equipment and/or signal quality.}
\added{It is an interesting question for future research if reweighting using a predefined, fixed label distribution such as a flat or a label distribution inferred from a population cohort would improve the robustness of the model we encountering an unknown target distribution (without prior knowledge of its label distribution).}

\heading{Future research directions}
While the presented models showed competitive performance even in comparison to in-distribution performance measures reported in the literature, BP estimation from PPG data remains a challenging task. This can be seen by putting the achieved MAE scores into the perspective of the IEEE standard for cuffless BP estimation \cite{IEEE1708a2019}, where MAEs above 7mmHg are considered as grade D and hence unsuitable for clinical use. In this sense, all presented methods have still a long way to go. However, it is also important to stress that we only report MAE scores averaged across entire datasets. The absolute error distribution itself is typically a bimodal distribution with a substantial fraction of samples in the acceptable grades A-B according to the aforementioned IEEE standards and a second group of samples in grade D. Uncovering patterns for the assignment of an unseen sample to one of these groups, for example, based on clinical metadata, would be a large step forward. Next to that, one might rely on the inclusion of additional clinical metadata, more strict data quality control or pretraining paradigms as proposed in the context of self-supervised learning to eventually shift the entire MAE distribution in a clinically acceptable range. \added{It is worth mentioning that the leveraging of pretrained (foundation) models \cite{pillai2024papagei,ding2024siamquality} might represent a promising path to alleviate these issues. The study of the OOD generalization of such models is deferred to future work.}


\section{Summary}
\label{sec:VI}

In this study, we conducted a benchmark study on BP prediction from PPG signals using different DL models. In addition, the ID and OOD generalization capability of the DL model for BP prediction is examined across various subsets and data sources. In line with expectations, it can be concluded that the models performed best when trained and tested on the same subset. \added{However, the performance level reached during ID evaluation typically turned out to be an overly optimistic measure of the generalization capabilities when applied to unseen data from unseen sources. In this work, we identified training datasets and scenarios that lead to good OOD generalization. Within PulseDB it is the Vital subset (in CalibFree and AAMI scenarios) that leads to good generalization performance, whereas MIMIC-based models show poor OOD generalization.} \added{This puts into question the use of MIMIC as the predominant training dataset for generalizable BP estimation.} \deleted{whereas generalization degraded further with previously unseen data, especially for more complex datasets.} 
\added{We identified mismatches in the BP distributions as one important aspect contributing to dataset drifts an explored importance weighting as domain adaptation technique to mitigate its effect. These techniques establish in particular AAMI Vital-based models with good generalization capabilities.}
\deleted{We were also able to identify training datasets and scenarios, such as ``CalibFree Vital'', which led to particularly good generalization capabilities. }. 
\deleted{We also assessed the impact of domain adaptation through importance weighting and found consistent improvements across all scenarios, stressing that requiring domain generalization without access to any information about the target dataset is a big challenge.} However, as the most important take-away message, we hope to sensibilize the community to the importance of performance evaluation on external datasets and the need for continued work on domain adaptation techniques.



%The weighted OOD strategy refinement should be pursued in future work, while generalization shall be enhanced by considering more data sources and optimization for performance on external datasets. Comparing the model with state-of-the-art models can further help validate and enhance its adaptability to real-world applications.

\section*{Acknowledgments}
The project (22HLT01 QUMPHY) has received funding from the European Partnership on Metrology, co-financed from the European Union’s Horizon Europe Research and Innovation Programme and by the Participating States. Funding for the University of Cambridge was provided by Innovate UK under the Horizon Europe Guarantee Extension, grant number 10091955. PHC acknowledges funding from the British Heart Foundation (BHF) grant [FS/20/20/34626]. 

%\clearpage
\bibliographystyle{IEEEtran}
\bibliography{bibfile}

%\clearpage

\appendices

\section{Supplementary figures and tables}
\begin{table*}[!!!ht]
\centering
\caption{Weighted ID and OOD generalization on all categories of PulsedDB dataset using XResNet1d101 (SBP / DBP). Vertical and horizontal categories represent the training and test sets, respectively. For each of the experiments, the three best-performing scenarios are marked in bold-face and the overall best-performing scenario is also underlined.}

%\renewcommand{\arraystretch}{1.5} % Increase cell height
\scalebox{0.7}{
\begin{tabular}{llccccccccc}
%\toprule
 & & \multicolumn{9}{c}{\textbf{Test Sets}} \\
\cmidrule(lr){3-11}
 & & \multicolumn{3}{c}{\textbf{Combined}} & \multicolumn{3}{c}{\textbf{Vital}} & \multicolumn{3}{c}{\textbf{MIMIC}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} & \underline{\textbf{9.42 / 5.97}} & 15.55 / 9.50& \textbf{19.65} / 13.86&   \underline{\textbf{9.05 / 5.96}} & 13.77 / 8.71& 18.91 / 11.52& \textbf{\underline{9.28 / 5.85}}& 17.41 / 9.80 & 20.20 / 16.81 \\
& \textbf{CalibFree} & 14.42 / \textbf{8.65}& \underline{\textbf{13.97 / 8.51}}& \textbf{\underline{ 18.65 / 13.11}} & 12.06 / 7.97 & 13.14 /  \textbf{8.08 }&  \textbf{17.82 / 10.77}& 15.32 / 8.86& \textbf{\underline{15.35 / 8.92}} & 18.81 / 15.78\\
& \textbf{AAMI} & \textbf{13.42 / 8.72}& \textbf{14.19 / 8.84}& \textbf{19.38} / 14.04 & \textbf{9.11 / 6.01} & \textbf{12.84} / 8.55& 18.85 / 12.48 & 15.06 / 9.00& 15.71 / 9.73 & \textbf{18.38} / 15.17\\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & 15.08 / 8.97& 17.03 / 10.03& 20.32 / \textbf{13.35}&\textbf{9.08 / 6.08}& 13.70 / 8.61 & \textbf{16.70 / 10.60} & \textbf{10.17 / 6.44} & 19.06 / 12.09 & 22.34 / 16.16 \\
& \textbf{CalibFree} & 16.67 / 9.16& 15.08 / \textbf{8.77} & 20.13 / \textbf{13.42} & 10.51 / 7.15 & \textbf{12.70 / \underline{8.05}} & \underline{\textbf{16.53 / 10.31}} & 19.85 / 10.08 & 18.17 / 9.66 & 20.03 / \textbf{14.53} \\
& \textbf{AAMI} & 14.99 / 9.04& 15.09 / 8.85& 19.94 / 13.67& 11.83 / 7.81 & \textbf{\underline{12.58} / 8.16} & 19.31 / 12.33 & 18.73 / 9.93 & 18.07 / \textbf{9.51} & 20.25 /  \underline{\textbf{14.11}} \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & \textbf{12.76} / 8.90& 16.89 / 10.92& 21.96 / 15.43& 16.21 / 11.53 & 15.91 / 11.01 & 26.04 / 17.13 & \textbf{9.52 / 6.64}& 17.21 / 10.06 & 20.11 / 16.14 \\
& \textbf{CalibFree} & 14.89 / 9.19& \textbf{14.74} / 9.89& 20.98 / 14.19& 14.56 / 10.3 & 14.55 / 9.26 & 23.42 / 12.27 & 15.36 / 8.94 & \textbf{ 15.47 / 9.26}& \textbf{18.51 / 14.66} \\
& \textbf{AAMI} & 13.95 / 9.72& 15.65 / 10.44& 20.98 / 16.42& 14.96 / 11.25 & 15.24 / 9.73 & 25.50 / 18.47 & 13.33 / 7.92 & \textbf{15.66} / 9.61 & \textbf{\underline{18.35}} / 15.56\\
\bottomrule
\end{tabular}

}
\label{tab:tab6}
\end{table*}


\begin{table*}[!!!ht]
\centering
\caption{OOD generalization on external datasets for models trained using importance weighting with XResNet1d101 (SBP / DBP). Vertical and horizontal categories represent the training and test sets, respectively. For each of the experiments, the three best-performing scenarios are marked in bold-face, and the overall best-performing scenario is also underlined. A Count column summarizes the number of top-three results for SBP and DBP.}
%\renewcommand{\arraystretch}{1.5} % Increase cell height
\scalebox{0.8}{
\begin{tabular}{llccccc}

& & \multicolumn{4}{c}{\textbf{Test Sets}} & \textbf{Count} \\
\cmidrule(lr){3-6}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Sensors $\downarrow$} & \textbf{UCI $\downarrow$} & \textbf{PPGBP $\downarrow$} & \textbf{BCG $\downarrow$} & \textbf{(SBP / DBP)} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} & 21.62 / 14.21 & 20.83 / 12.86 & \textbf{20.27 / 8.34} & 12.80 / 9.14 & 1 / 1 \\
& \textbf{CalibFree} & 30.86 / 11.50 & 20.87 / 13.65 & 32.99 / 11.59 & 12.64 / 7.35 & 0 / 0\\
& \textbf{AAMI} & 39.92 / 12.30 & 45.83 / 12.80 & 39.25 / 11.40 & \textbf{10.64 / 6.38} & 1 / 1\\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & \textbf{18.08} / 12.34 & \textbf{21.67 / 10.3} & 21.49 / 11.16 & 11.47 / 8.49 & 2 / 1 \\
& \textbf{CalibFree} & 20.07 / \textbf{8.42} & 24.03 / 11.08 & \textbf{21.75 / 10.58} & 10.72 / \textbf{6.57} & 1 / 3\\
& \textbf{AAMI} & \underline{\textbf{17.03 / 7.56}} & \textbf{\underline{19.76 / 8.97}} & \textbf{\underline{17.18 / 8.16}} & \textbf{10.01} / 7.51 & 4 / 3 \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & \textbf{19.79 / 9.24} & \textbf{20.39} / 11.44 & 22.73 / 10.78 & 15.06 / 7.74 & 2 / 1 \\
& \textbf{CalibFree} & 23.04 / 10.61 & 27.03 / 17.04 & 45.75 / 17.45 & \textbf{\underline{9.75 / 5.85}} & 1 / 1\\
& \textbf{AAMI} & 29.46 / 15.63 & 22.02 / \textbf{9.80} & 37.5 / 18.03 & 11.83 / 7.97 & 0 / 1 \\
\bottomrule
\end{tabular}
}
\label{tab:tab7}
\end{table*}


%\clearpage

% Include SBP Distributions

\begin{figure*}[htbp]
  \centering
  %\includegraphics[scale=0.35]{SBP_Distributions_Combined_Enhanced_WithAamiVital_2.pdf}
  \includegraphics[width=\textwidth]{SBP_Distributions_Combined_Enhanced_WithAamiVital_2.pdf}
  \caption{SBP distribution plots for all datasets}
  \label{fig:sbp_distributions}
\end{figure*}

\begin{figure*}[htbp]
  \centering
 % \includegraphics[scale=0.35]{DBP_Distributions_Combined_Enhanced_WithAamiVital_2.pdf}
  \includegraphics[width=\textwidth]{DBP_Distributions_Combined_Enhanced_WithAamiVital_2.pdf}
  \caption{DBP distribution plots for all datasets}
  \label{fig:Dbp_distributions}
\end{figure*}

%\usetikzlibrary{shapes.geometric, arrows}

%% Define block styles
%\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=red!80]
%\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!50]
%\tikzstyle{arrow} = [thick,->,>=stealth]

%\begin{figure*}[h]
%\centering
%\begin{tikzpicture}[node distance=2cm]

% Nodes
%\node (start) [startstop] {Start};
%\node (load_weights) [process, below of=start] {Load SBP and DBP weights, fixed bins};
%\node (sbp_weights) [process, below of=load_weights] {Compute SBP sample weights};
%\node (dbp_weights) [process, right of=sbp_weights, xshift=4cm] {Compute DBP sample weights};
%\node (move_weights) [process, below of=sbp_weights] {Load weights in DL code};
%\node (loss_unweighted) [process, below of=move_weights] {Compute unweighted loss};
%\node (mean_reduction) [process, below of=loss_unweighted] {Apply mean reduction};
%\node (apply_weights) [process, below of=mean_reduction] {Apply SBP and DBP sample weights};
%\node (final_loss) [process, below of=apply_weights] {Compute final loss};
%\node (end) [startstop, below of=final_loss] {End};

% Arrows
%\draw [arrow] (start) -- (load_weights);
%\draw [arrow] (load_weights) -- (sbp_weights);
%\draw [arrow] (load_weights) -- (dbp_weights);
%\draw [arrow] (sbp_weights) -- (move_weights);
%\draw [arrow] (dbp_weights) -- (move_weights);
%\draw [arrow] (move_weights) -- (loss_unweighted);
%\draw [arrow] (loss_unweighted) -- (mean_reduction);
%\draw [arrow] (mean_reduction) -- (apply_weights);
%\draw [arrow] (apply_weights) -- (final_loss);
%\draw [arrow] (final_loss) -- (end);

%\end{tikzpicture}
%\caption{Flowchart illustrating the incorporation of SBP and DBP sample weights into the loss function. \sout{\colnst{takes too much space-rather appendix}}}
%\label{fig:figure2}
%\end{figure*}



\end{document}















In order to guarantee the reliability of our benchmark study, we initially established specific criteria for dataset selection, considering aspects such as accessibility, volume, integrity, and diversity of sources. Furthermore, we sought datasets that facilitate an comprehensive evaluation of the models' capability to estimate blood pressure across a wide range of scenarios. Following a thorough evaluation, the PulseDB dataset \cite{wang2023pulsedb} was identified as meeting all the prescribed criteria, thereby confirming its suitability  in our research. As it can be seen in Table 1, PulseDB data se




\begin{table}[b]
\centering
\caption{Summary of PulseDB datasets \cite{wang2023pulsedb}}
\begin{tabular}{|p{2cm}|p{1.3cm}|p{2cm}|>{\raggedright\arraybackslash}p{2cm}|}
\hline
\textbf{Dataset} & \textbf{Number of subjects (MIMIC / VitalDB)}&  \textbf{SBP 
(mmHg, mean ± SD)}& \textbf{DBP 
(mmHg, mean ± SD)}\\ \hline
Traning& 2506 (1213/1293)& 118.60 ± 21.03 &61.86 ± 12.56\\ \hline 
Calibration-based testing& 2506 (1213/1293)& 118.64±21.01 &61.88 ± 12.64\\ \hline
 Calibration-free testing& 279 (135/144)&118.84 ± 20.59&62.00 ± 12.27\\ \hline 
 AAMI testing& 242 (126/116)&131.50 ± 27.75&73.61 ± 18.22\\ \hline 
 AAMI calibration& 242 (126/116)&123.96 ± 23.12&63.18 ± 13.63\\ \hline
\end{tabular}
\end{table}


\heading{Preprocessing} 

\subsection{Training procedure and performance evaluation}
\heading{Training}
To mitigate issues due to imbalanced label distributions that are typical for this task, we use focal loss \cite{lin2017focal} as loss function.
For all experiments, we use a fixed effective batch size of 64 achieved through gradient accumulation. We performed experiments with learning rates determined via the learning rate finder \cite{smith2017cyclical} as well as with a fixed learning rate of 0.001 for each experiment. The learning rate yielding superior validation set performance was selected. We train models using a constant learning rate using AdamW \cite{loshchilov2018decoupled} as optimizer. For training and validation, we divide the whole input sequence of a given sample into consecutive segments whose lengths coincide with the model's input size. For SEDF, the models underwent training for 50 epochs, with validation performed after each training epoch. For SSHS, we found longer training beneficial (in terms of training iterations) and train for 30 epochs. For the final test set evaluation, we select the model at the training epoch with the best validation set performance (in terms of macro F1 score). We train on non-overlapping crops of the specified input size. During test time, we split the input sequences into segments of the same length but use a smaller stride length coinciding with the length of a single epoch. For most segments, we obtain in this way multiple predictions corresponding to different positions in the input sequence passed to the model. We subsequently average all available predictions on the level of output probabilities for a given segment.

\heading{Performance evaluation}
Irrespective of the input size the procedure described in the previous section yields a probabilistic model prediction per test/validation set epoch. We can then compare dichotomized model predictions with the ground truth. The most commonly used metric in the field is the macro-$F_1$-score computed as the mean of the individual label $F_1$-scores. Due to its widespread use, we mostly focus on $F_1$-score to compare model performance, but also indicate the individual label scores, as well as the accuracy and the macro-average of the area under the receiver operating curve (AUC), as the only non-thresholded metric among the considered metrics. Even though accuracy is commonly used in the field, it is worth stressing that the results is strongly affected by label imbalance, which means in this case that the score is biased towards the majority classes N2 followed by W. 












 We generated the subset files based on the recommended segments for each subject listed in the "info files."

