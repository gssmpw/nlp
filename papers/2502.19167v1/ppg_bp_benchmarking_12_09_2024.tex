\newif\ifdraft
\drafttrue


\PassOptionsToPackage{table}{xcolor}

\documentclass[lettersize,journal]{IEEEtran}


\usepackage{xcolor}  
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bbm}
\usepackage{graphicx}

\usepackage{hyperref}
\usepackage{caption,subcaption}
\usepackage{multirow}
\usepackage{cleveref}
\usepackage{comment}
\usepackage{hhline}
\usepackage{todonotes}
\usepackage{caption,subcaption}

\usepackage{siunitx}
\sisetup{round-mode=places,round-precision=3}
\usepackage{booktabs}
\usepackage{csvsimple}

\usepackage{datatool}
\usepackage{array}
\usepackage{placeins}
\usepackage{float}
\usepackage{geometry}       
\usepackage{pifont} 
\usepackage{makecell}        


\definecolor{improvement}{RGB}{144,238,144} 
\definecolor{degradation}{RGB}{255,182,193} 
\definecolor{nochange}{RGB}{255,255,255}    





\newcommand\colnst[1]{{\color{red}#1}}
\newcommand\coltw[1]{{\color{blue}#1}}

\newcommand{\heading}[1]{\noindent\textbf{#1}}


\newcommand{\diffcellsplitside}[4]{%
    \makecell[l]{%
        \colorbox{#1}{\parbox{0.7cm}{\centering \scriptsize #2}} \hspace{0.2cm}%
        \colorbox{#3}{\parbox{0.7cm}{\centering \scriptsize #4}}%
    }
}
\usepackage[normalem]{ulem} 
\newcommand{\stkout}[1]{\ifmmode\text{\sout{\ensuremath{#1}}}\else\sout{#1}\fi}

\newcommand{\citex}[1]{\mbox{\cite{#1}}}
\newcommand{\citepx}[1]{\mbox{\citep{#1}}}
\newcommand{\citetx}[1]{\mbox{\citet{#1}}}
\ifdraft

\newcommand{\added}[1]{\textcolor{blue}{#1}}
\newcommand{\deleted}[1]{\textcolor{red}{\stkout{#1}}}
\newcommand{\replaced}[2]{\textcolor{blue}{#1} \textcolor{red}{\stkout{#2}}}
\newcommand{\deletedfloat}[1]{}
\newcommand{\commented}[1]{\textcolor{blue}{#1}}
\else

\newcommand{\added}[1]{#1}
\newcommand{\deleted}[1]{}
\newcommand{\replaced}[2]{#1}
\newcommand{\deletedfloat}[1]{}
\newcommand{\commented}[1]{}
\fi

\newcommand\citep[1]{\cite{#1}}
\newcommand\citet[1]{\cite{#1}}
\newcommand{\pcn}[1]{\textcolor{magenta}{#1}} % Pete's comments



\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{Preprint}{Moulaeifard \MakeLowercase{\textit{et al.}}: In-Distribution and Out-of-Distribution Generalization of PPG-based Blood Pressure Estimation-- A Benchmarking Study based on Deep Learning Models (Oct 2024)}
\begin{document}


\title{Generalizable deep learning for photoplethysmography-based blood pressure estimation– A Benchmarking Study}
\author{Mohammad Moulaeifard, Peter H. Charlton and Nils Strodthoff
\thanks{Mohammad Moulaeifard and Nils Strodthoff are with Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany. (email: mohammad.moulaeifard@uol.de, nils.strodthoff@uol.de). Peter H Charlton is with University of Cambridge, Cambridge, UK. Corresponding author: NS.}}



\bstctlcite{BSTcontrol}


\maketitle

\begin{abstract}
Photoplethysmography (PPG)-based blood pressure (BP) estimation represents a promising alternative to cuff-based BP measurements. Recently, an increasing number of deep learning models have been proposed to infer BP from the raw PPG waveform. However, these models have been predominantly evaluated on in-distribution test sets, which immediately raises the question of the generalizability of these models to external datasets. To investigate this question, we trained five deep learning models on the recently released PulseDB dataset, provided in-distribution benchmarking results on this dataset, and then assessed out-of-distribution performance on several external datasets. The best model (XResNet1d101) achieved in-distribution MAEs of 9.4 and 6.0 mmHg for systolic and diastolic BP respectively on PulseDB (with subject-specific calibration), and 14.0 and 8.5 mmHg respectively without calibration. Equivalent MAEs on external test datasets without calibration ranged from 15.0 to 25.1 mmHg (SBP) and 7.0 to 10.4 mmHg (DBP).  Our results indicate that the performance is strongly influenced by the differences in BP distributions between datasets. We investigated a simple way of improving performance through sample-based domain adaptation and put forward recommendations for training models with good generalization properties. With this work, we hope to educate more researchers for the importance and challenges of out-of-distribution generalization. 
 
\end{abstract}

\begin{IEEEkeywords}
    Decision support systems, Photoplethysmography, Machine learning algorithms, Time series analysis

    \end{IEEEkeywords}


\section{Introduction}
\label{sec:I}


\IEEEPARstart{P}PG devices represent a promising approach to monitor vital parameters such as blood pressure, heart rate, and respiratory rate. Their non-invasive nature and cost-effectiveness in comparison to alternative approaches makes them popular for both medical and personal health monitoring purposes \cite {charlton20232023, castaneda2018review, gonzalez2023benchmark}. One of the most widely considered prediction problems based on PPG data is blood pressure (BP) estimation.

Traditionally, BP estimation from PPG signals has involved analyzing features in the PPG data and connecting them to BP measurements using different methods, e.g., pulse wave analysis \cite{o2001pulse}. However, the traditional methods possess certain limitations in real-world applications, e.g.,  variations in physiology \cite {allen2007photoplethysmography} and the requirement for calibration \cite{Wan2020,gonzalez2023benchmark}. 

In light of the limitations of traditional feature-based approaches, there has been a gradual shift of interest on the part of researchers towards machine learning (ML) and deep learning (DL) techniques for predicting BP using PPG signals. These methods autonomously extract features from PPG signals \cite{martinez2022data} and typically reach higher accuracy in data-rich environments \cite{chang2021deepheart, mehrgardt2021deep}. Despite significant progress, most existing studies primarily focus on in-distribution (ID) testing, where the train and test datasets stem from the same distribution. This approach does not consider out-of-distribution (OOD) evaluation scenarios and thus does not reflect the reality of real-world applications, where test data can come from various distributions. Real-world test sets might differ from the training dataset in various aspects, such as BP distributions, sensor hardware used for capturing, signal quality and subject physiologies. 

This study aims to address these gaps by leveraging the PulseDB dataset \cite{wang2023pulsedb} as a training dataset, and then evaluating both ID and OOD generalization of DL models for the estimation of BP. The goal is to identify model architectures and training datasets that give rise to models that show a robust OOD performance. The PulseDB dataset was used for training due to its large size. Then, four external datasets served as test datasets to assess OOD generalization. Furthermore, we investigated a simple domain adaptation approach to improve OOD generalization. This work offers a comprehensive benchmarking analysis on diverse datasets, providing insights into the robustness of these models in real-world scenarios. We close with practical recommendations on model architectures, training datasets, and scenarios to achieve good OOD generalization.



\section{Related Works}
\label{sec:II}

\heading{Challenges for BP estimation and benchmarking models} 
\label{sec:II-B}
Several research studies have indicated that the effectiveness of learning models in predicting BP is highly dependent on the quality and quantity of the training data \cite{qin2023machine, treebupachatsakul2022cuff, zabihi2021bp}. For instance,  researchers who worked with the MIMIC-III dataset underscore the significance of data preparation to achieve better model outcomes \cite{chu2023non}. The aforementioned challenges result in a growing demand for reliable benchmark studies to thoroughly evaluate different ML/DL methods for BP estimation using PPG, utilizing comprehensive datasets to measure their efficiency. Consequently, benchmark investigations have been performed in this area to address the aforementioned demands. We refer to \cite{gonzalez2023benchmark} as a notable and recent benchmark study that used four different datasets. 
However, the mentioned study only considers models trained from scratch on the respective datasets and evaluates them on in-distribution test sets, which are known to provide overly optimistic measures for the generalization performance on unseen data. The PulseDB dataset, a large-scale, high-quality dataset containing PPG signals and reference BP measurements, and as such is a unique resource for training deep-learning based BP prediction models, after which they can be externally validated on external datasets.



\heading{Challenges of OOD generalization} 
Most of the ML/DL techniques usually rely on the subtle statistical patterns that may exist within the training data, hence functioning under ideal conditions where both the training and testing data belong to the same distribution (ID). However, this perfect situation rarely occurs in real-world scenarios \cite{zhang2021deep, engstrom2019exploring}. Previous work has shown that most DL models perform poorly on tasks induced by data from distributions other than their training data (OOD) generalization \cite{ballas2022domain}. The concept of OOD generalization and its application to DL models has evolved with contributions from various researchers, e.g., \cite{gwon2023out, yi2021improved, ye2021towards}. 
The challenges of OOD generalization in the context of PPG-based BP estimation have been investigated in a recent publication \cite{weber2023intensive}. They focused on feature-based approaches, whereas the present work covers deep learning models operating on raw time series. This work establishes a more comprehensive picture by considering a large number of external datasets and investigating the potential impact of domain adaptation.

\heading{Improving OOD generalization} 
It is worth mentioning that OOD generalization is a challenging task that may result in poor performances since unseen data very often do not resemble the training set \cite{hendrycks2017baseline, liang2018enhancing}. Several previous works have intensively addressed such a challenge of mitigating the influence of OOD signals when analyzing ECG and EEG data. \cite{ballas2022domain,soltanieh2023distribution} have shown the effectiveness of using domain generalization and self-supervised learning approaches to improve the classification accuracy of OOD ECG signals. Also, recent work by \cite{yang2022multimodal} has demonstrated promising results in addressing domain shifts and OOD signals between diverse EEG datasets, highlighting the potential of domain adaptation techniques to enhance the robustness of EEG signal recognition. According to previous studies, one of the key approaches to tackle the challenge of OOD generalization is to reduce the influence of distribution shifts between training and test sets by revising the distribution of training data to mimic the distribution of test data, aiming to minimize the predictive error on the test set \cite{ben2010theory,bickel2009discriminative}.
In this work, we use a simple sample-based empirical risk minimization approach based on sample weights inferred from the label distribution (i.e., BP reference labels) in the source and target domains to assess the potential benefits of incorporating domain adaptation approaches. 

\heading{Technical contributions} 
In this work, we put forward the following technical contributions:


\begin{enumerate}

 


\item We implemented state-of-the-art DL-based time series classification algorithms for PPG-based BP estimation on the large-scale, high-quality PulseDB dataset and evaluated their performance in a first comprehensive comparative study. 

\item We investigated both ID and OOD generalization of models trained on various PulseDB subsets. These models were evaluated on different PulseDB subsets and four external datasets. To contextualize OOD performance, we compared it with the differences in label distributions between the training and test datasets.




\item We assessed the benefit of domain adaptation by using an importance-weighted empirical risk minimization approach using importance weights inferred from the respective label distributions and put forward recommendations for training dataset choices that promise good generalization properties.


\end{enumerate}





\section{Materials \& Methods}
\label{sec:III}

\subsection{Training and Evaluation Datasets}
\label{sec:III_A}


\begin{table*}[h]
\centering
\caption{Summary of the datasets utilized in this study: Two subsets PulseDB \cite{wang2023pulsedb} for training and four external datasets \cite{gonzalez2023benchmark} for OOD evaluation.}
\scalebox{0.75}{
\begin{tabular}{lccccccc}

& \multicolumn{2}{c}{\textbf{PulseDB}} & \multicolumn{4}{c}{\textbf{External Datasets}} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-7}
\textbf{Metric} & \textbf{MIMIC} & \textbf{VitalDB} & \textbf{Sensors} & \textbf{UCI} & \textbf{BCG} & \textbf{PPGBP} \\
\midrule
\textbf{No. Subjects} & 1,474 & 1,553 & 1,195 & unknown & 40 & 218 \\
\textbf{Total Duration (h)} & $\sim$2357 & $\sim$1793 & $\sim$15 & $\sim$570 & $\sim$4 & $<$1 \\
\textbf{Segments (number , length)} & 848,796 , 10s   & 645,678 , 10s & 11,102 , 5s & 410,596 , 5s & 3,063 , 5s & 619 , 2.1s \\
\textbf{Age (mean ± SD)} & 61.16 ± 15.29 & 58.71 ± 15.14 & 57.1 ± 14.2 & unknown & 34.2 ± 14.5 & 56.9 ± 15.8 \\
\textbf{Gender} & 45.94\% F, 54.05\% M & 41.84\% F, 58.15\% M & 40.2\% F, 59.8\% M & unknown & 55.5\% F, 44.5\% M & 53.1\% F, 46.9\% M \\
\textbf{SBP (mmHg, mean ± SD)} & 123.32 ± 23.00 & 115.62 ± 18.92 & 134.36 ± 21.78 & 131.57 ± 11.16 & 120.99 ± 15.29 & 128.02 ± 20.50 \\
\textbf{DBP (mmHg, mean ± SD)} & 61.58 ± 13.48 & 63.03 ± 12.05 & 65.37 ± 10.51 & 66.79 ± 10.48 & 67.23 ± 9.30 & 71.91 ± 11.20 \\
\bottomrule
\end{tabular}
}
\label{tab:tab1}
\end{table*}


\heading{PulseDB dataset} 
PulseDB is sourced from selected pre-processed signals from the MIMIC-III \cite{moody2020mimic} and VitalDB \cite{lee2022vitaldb} databases. It is one of the most extensive datasets currently available, containing 5,245,454 10-second segments of ECG, PPG, and arterial BP (ABP) waveforms across 5,361 subjects. The dataset includes demographic details such as age, gender, weight, height, and body mass index (BMI). Both VitalDB and MIMIC-III represent samples collected from finger-tip PPG sensors from patients undergoing surgery and in Critical Care Units, respectively.
It is worth noting that the PulseDB dataset is categorized into the following subsets, making it ideal for benchmarking cuff-less BP estimation models:
\begin{itemize}
    \item \textit{Calib:} Created for a calibration-based approach, where each subject contributes data to both the training and testing sets. This enables the model to adapt to patient-specific signal features in order to improve the prediction performance. The focus of this scenario is to train models that show good generalization to unseen samples of patients encountered during training.
    \item \textit{CalibFree:} Created for a calibration-free approach, in which training and test sets do not share any subjects. The focus of this scenario is to develop models that generalize to entirely unseen patients.
    \item \textit{AAMI:} Created for a second calibration-free scenario, which complies with the high standards developed by the Association for the Advancement of Medical Instrumentation (AAMI) \cite{stergiou2018universal}. The main difference between this and the CalibFree scenario is a stronger emphasis on the tails of the BP distribution. The focus of this scenario is to assess the generalization to unseen patient with the stricter protocol of the AAMI for medical device testing. 
\end{itemize}




 


We generated nine subsets of the PulseDB dataset, inspired by the instructions in the original PulseDB publication \cite{wang2023pulsedb} and the corresponding code repository. Tables \ref{tab:tab1} and \ref{tab:tab2} summarise the utilized PulseDB dataset in this paper (including data from MIMIC and VitalDB), and the generated subsets, respectively. We have three major subsets, Calib, CalibFree, and AAMI, which are derived from VitalDB or MIMIC, or a combination of VitalDB and MIMIC (combined) sources, resulting in nine different subsets.  We kept the original test sets intact to ensure comparability with results in the literature, but we split off additional validation and calibration sets from the respective training sets mimicking the way in which the respective test sets were constructed.



\begin{table*}[h]
\centering
\caption{Summary of generated PulseDB subsets (Samples / Subjects)}

\begin{tabular}{llcccc}
%\toprule
\textbf{Source} & \textbf{Subset} & \textbf{Train} & \textbf{Validation} & \textbf{Calibration} & \textbf{Test} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} &  811,955 / 2,494 & 78,899 / 2,494 & 11,306 / 2,494 & 100,240 / 2,494 \\
& \textbf{CalibFree} & 801,720 / 2,217 & 66,960 / 186 & 33,480 / 93 & 111,600 / 279 \\
& \textbf{AAMI} & 902,160 / 2,494 & 230,145 / 149 & 148,989 / 93 & 1340 / 242 \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & 418,986 / 1,293 & 40,673 / 1,293 & 5821 / 1,293 & 51720 / 1,293 \\
& \textbf{CalibFree} & 416,880 / 1,158 & 32,400 / 90 & 1000 / 45 & 57,600 / 144 \\
& \textbf{AAMI} & 465,480 / 1,293 & 43,820 / 71 & 26,392 / 45 & 666 / 116 \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & 392,969 / 1,213 & 38,226 / 1,213 & 5,485 / 1,213 & 48,520 / 1,213 \\
& \textbf{CalibFree} & 384,840 / 1,069 & 34,560 / 96 & 17,280 / 48 & 54,000 / 135 \\
& \textbf{AAMI} & 436,680 / 1,213 & 186,325 / 78 & 122,597 / 48 & 674 / 126 \\
\bottomrule
\end{tabular}
\label{tab:tab2}
\end{table*}



\heading{External datasets} We used the datasets presented in a recent benchmark study \cite{gonzalez2023benchmark} as external datasets, as they are qualitatively very different to PulseDB in terms of sample size, signal quality and patient collective, and are therefore well-suited to investigate the OOD generalization of models trained on PulseDB or subsets thereof. \cite{gonzalez2023benchmark} provides the pre-processed versions of each external dataset along with their pre-processing methods, which serve as external datasets for the purpose of our study (Table \ref{tab:tab1}). These datasets are comprehensively described in \cite{gonzalez2023benchmark}. Therefore, we refer to the original publication for details, and now briefly outline the key features of each dataset:







\begin{itemize}
\item \textit{Sensors}: The Sensors dataset is derived from MIMIC-III with simultaneous PPG and ABP waveforms from 1,195 ICU patients. After pre-processing, it includes two 15-second segments per record with a 5-minute interval between them. 

\item\textit{UCI}: The UCI dataset is derived from the MIMIC-II dataset and is the largest dataset in our external dataset.

\item\textit{BCG}: The BCG dataset contains recordings from 40 subjects (primarily healthy). Although the BCG is a smaller dataset after pre-processing with limited variability, the number of segments per subject is remarkably high. 

\item\textit{PPGBP}: The PPGBP dataset contains data from 219 subjects, each with cardiovascular conditions. Following pre-processing, PPGBP becomes the shortest dataset, containing 218 subjects and 613 segments of 2.1s length, making PPGBP the smallest dataset in terms of total duration.. 
\end{itemize}







\subsection{Prediction Models} 
\label{sec:III_B}


\heading{Overview of considered model architectures} 


The number of DL approaches for time series classification is immense, and we refer to \cite{ mohammadi2024deep, liang2024foundation} for an extensive review of these methods. Since our work pertains to BP estimation, wherein signal processing plays a major role, we leveraged these foundations by evaluating several convolutional neural network architectures (CNN). Also, in this paper, we extended our exploration to include structured state space sequence (S4) models \cite{guefficiently}, which are known for their ability to effectively capture long-range dependencies, and showed promising results for other physiological time series \cite{mehari2023towards,wang2023s4sleep,saab2024towards}.

CNNs have been part of time series analysis for a long time by offering flexibility and scalability in model design. In this work, we evaluated the performance of three main CNN architectures: 

\heading{Simple feed-forward CNN architectures} 
The most straightforward convolutional neural network is a neural network without cycles since it includes data flow only in one direction through its layers. A prototypical example of such architecture is the LeNet1D \cite{lecun1998gradient}. Our work builds on the one-dimensional adaption put forward in \cite{wagner2024explaining}. 

\heading{ResNet-based Architectures} 
The ResNet model has made a crucial stride in DL by proposing skip connections, which enabled easy gradient flow via backpropagation. Here, we draw on one-dimensional ResNet variants, such as XResNet1d50 and XResNet1d101, proposed in \cite{strodthoff2020deep}. 

\heading{Inception-based Architectures} 
Inception models, originally proposed in computer vision \cite{szegedy2015going}, include several convolutional filters with different kernel sizes to capture a broader spectrum of feature patterns. Furthermore, such a hierarchical feature extraction approach has benefited physiological signal analysis, where intricate generalized patterns are to be captured for proper interpretation. Specifically, several previous studies have applied Inception1D \cite{ismail2020inceptiontime} in time series classification tasks and reported its excellent performance owing to its good representation capability with comprehensive and diverse features \cite{strodthoff2020deep, ismail2020inceptiontime}.

\heading{Structured State Space Sequence (S4) Models}
As an alternative model category, we considered a structured state space sequence model, which has been successfully applied to physiological time series \cite{wang2023s4sleep, mehari2023towards} and is known for its ability to capture long-range dependencies in input sequences \cite{gu2021efficiently}. Here, we use a S4 model as a prediction model as in \cite{mehari2023towards}.



\subsection{Training and evaluation procedures}
\label{sec:III-C}
\heading{Training procedure}

For each of the experiments, an effective batch size of 512 was used through gradient accumulation. The learning rates were either found using a learning rate finder \cite{smith2017cyclical} or set to 0.001. Models were trained for 50 epochs. Also, the training routine was implemented with the AdamW optimizer \cite{loshchilov2017decoupled} and mean squared error as loss function. In all cases, consistent with previous studies, e.g., \cite{xiao2024advancing}, we employed two output nodes to jointly predict SBP and DPB, leveraging possible shared physiological features to enhance model performance among SBP and DBP to improve model  performance. We used the validation set score for hyperparameter tuning. As a simple measure to reduce overfitting, we performed model selection based on the validation set score, i.e., during training we kept track of the validation set score and selected the model with the best validation set score for evaluation on ID or OOD test sets.

All of the considered datasets used a sampling frequency of 125~Hz. Models were trained using the samples full input resolution, i.e., 1250 time steps in the case of PulseDB. The fact that all considered model architectures involve a global average-pooling, allows to evaluate models also for other input sizes, such as the native input sizes of the external datasets (625 time steps for BCG, UCI, and Sensors datasets, and 262 time steps for PPGBP) while still leveraging the model trained on PulseDB.





\heading{Performance metrics}
As primary performance metric referenced by BP standards  \cite{iso2018sphygmomanometers, ieee2014ieee}, we report the mean absolute error (MAE) defined by 

\begin{equation}
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |\text{Predicted}_i - \text{Reference}_i|,
\end{equation}

where \(n\) is the number of predictions, \(i\) is the index, \(\text{Predicted}_i\) and \(\text{Reference}_i\) indicate the predicted and reference BP, respectively. As a scale-independent, relative performance measure, we also report the mean absolute scaled error (MASE) \cite{hyndman2006another}, which is defined as

 \begin{equation}
\text{MASE} = \frac{\text{MAE}}{\text{MAE}_{\text{Baseline}}},
\end{equation}

where \(\text{MAE}_{\text{Baseline}}\) represents the median value of the BP of the training set. MASE allows to assess the model performance relative to the simplest, non-parametric predictor.

\subsection{A simple baseline for domain adaptation}
\label{sec:III_D}

\heading{Approaches to domain adaptation} The challenge of dealing with an inevitable mismatch between training and test set distributions is a long-standing one and attracted a lot of interest in the machine learning community, see \cite{kouw2019review} for a recent review. Conventionally, one distinguishes sample-based, feature-based, and inference-based approaches. In this work, we explore the potential benefit of domain adaptation methods in a simple sample-based approach, where we deviate from the paradigm of target-free domain adaptation, through the use of the target domain label distribution. Most importantly, we only make use of the target domain label distribution but not of individual labels, which represents a piece of information that we envision to be typically available in practical use cases. The main motivation is to assess the potential benefit of domain adaptation on the model performance. 


\heading{Reweighting based on label distributions} More specifically, we propose to use an empirical risk minimization approach using sample weights derived from the difference of the label distributions of the respective source domain and target domain test datasets. To this end, we summarize both label distributions in terms of (normalized) histograms, see Figure~\ref{fig:fig5}. For a given training sample that belongs to the bin $i$, we identify the empirical output probability $h_{\text{train},i}$($h_{\text{test},i}$) assessed from the corresponding test set histograms. From that, we define sample weights via
\begin{equation}
w_i = 
\begin{cases} 
\max(\added{\tau}, \frac{h_{\text{test},i}}{h_{\text{train},i}}) & \text{if } h_{\text{train},i} > 0 \\
\added{\tau} & \text{if } h_{\text{train},i} = 0\,,
\end{cases}
\label{eq:weight_calculation}
\end{equation}
where the hyperparameter $\tau$ is used to prevent excluding training samples for which the relative weight $h_{\text{test},i}/h_{\text{train},i}$ is small entirely from the training process. In our experiments, we fixed $\tau=1$,  but found that the results (validation set scores) were not very sensitive to the choice of $\tau$.

Finally, we utilize the sample weights $w_i$ for the loss calculation. More specifically, we use importance weights derived from the SBP distribution for the loss calculated based on the SBP output and importance weights derived from the DBP distribution for the loss calculated based on the DBP output and sum both contributions to obtain the final importance weighted loss.
\begin{figure}[t]  
  \centering
  \includegraphics[width=\columnwidth]{Fig.1.pdf}
  \caption{Schematic comparison of normalized label distributions (e.g., SBP or DBP) across training and test datasets.}
  \label{fig:fig5}
\end{figure}




\section{Results}
\label{sec:IV}



\heading{Organization of the experiments} In Section \ref{sec:IV-A}, we evaluated classifiers for all nine generated PulseDB subsets, which is the core of our analysis. Section \ref{sec:IV-B} supplements the above analysis by performing ID and OOD generalizations within PulseDB datasets. Also, the OOD generalization on external datasets is investigated in Section \ref{sec:IV-C}. Finally, in Section \ref{sec:IV-D}, we conducted similar analyses as in Sections \ref{sec:IV-B} and \ref{sec:IV-C}, however, using the importance weighting for domain adaptation. We computed and evaluated the results with a focus on reducing the distribution shift between the training and test sets by incorporating a special weight value for each training sample. 



\subsection{Model comparison on PulseDB}
\label{sec:IV-A}


\begin{table*}[ht]
\centering
\caption{Performance of PPG DL models on PulseDB dataset in terms MAE (SBP / DBP) measured in units of mmHg. For each of the experiments, the three best-performing models are marked in bold-face and the overall best-performing model is underlined. The count column enumerates the number of times where the model achieved a result among the top three across all considered scenarios, again differentiating SBP/DBP scenarios.}



\scalebox{0.7}{
\begin{tabular}{lccccccccc c}

 & \multicolumn{3}{c}{\textbf{Combined}} & \multicolumn{3}{c}{\textbf{Vital}} & \multicolumn{3}{c}{\textbf{MIMIC}} & \textbf{Count} \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}

 & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} 
 & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} 
 & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \\ 
\midrule
\textbf{Baseline (Median)} & 16.66 / 9.85 & 16.48 / 9.75 & 25.48 / 17.29 & 14.92 / 9.52 & 14.88 / 9.43 & 29.85 / 17.84 & 18.16 / 10.07 & 17.69 / 9.94 & 21.23 / 16.82 & 0/0 \\ 
\textbf{Lenet1D} & 12.47 / 7.85 & \textbf{13.88 / 8.52} & \textbf{18.57 / \underline{13.37}} & 11.61 / 7.70 & \textbf{\underline{12.37} / 7.89} & 19.59 / \textbf{11.87} & 14.37 / 8.22 & 15.41 / 8.92 & \underline{\textbf{17.56 / 14.51}} & 4/4 \\ 

\textbf{XResNet1d50} & \textbf{9.96 / 6.34} & 14.12 / 8.56 & 20.49 / 15.11 & \textbf{9.49 / 6.33} & \textbf{12.40 / \underline{7.84}} & \underline{\textbf{17.71 / 11.43}} & \textbf{10.14 / \underline{6.18}} & \textbf{15.36 / 9.09} & 19.02 / 15.94 & 6/6 \\ 
\textbf{XResNet1d101} & \underline{\textbf{9.42 / 5.97}} & \textbf{13.97 / 8.51} & \textbf{19.38 / 14.04} & \underline{\textbf{9.08 / 6.08}} & \textbf{12.70 / 8.05} & \textbf{19.31} / 12.33 & \textbf{\underline{9.52} / 6.64} & 15.47 / 9.26 & \textbf{18.35 / 15.56} & 8/7 \\ 

\textbf{Inception1D} & \textbf{10.37 / 6.98} & \underline{\textbf{13.71 / 8.26}} & \underline{\textbf{18.21}} / \textbf{13.83} & \textbf{9.65 / 6.52} & 14.97 / 8.98 & 19.79 / 12.30 & \textbf{10.52 / 6.52} & \underline{\textbf{12.25 / 7.81}} & \textbf{17.33 / 15.00} & 7/8 \\ 
\textbf{S4} & 13.93 / 8.48 & 14.43 / 8.54 & 19.57 / 15.43 & 13.65 / 8.47 & 13.92 / 8.57 & \textbf{18.40 / 12.19} & 13.62 / 8.38 & \textbf{13.96 / 8.50} & 17.83 / 14.88 & 2/1 \\ 
\bottomrule
\end{tabular}
}
\label{tab:tab3}
\end{table*}







\heading{Overview} We carried out all experiments for the nine subsets in Table \ref{tab:tab1} using all models presented in Section \ref{sec:III_B}. All experiment results (SBP/DBP) are shown in Table \ref{tab:tab3}. The three best-performing models according to MAE are marked in bold-face for each of the experiments.
The best-ranked models, either based on ResNet or Inception architectures, achieved MAEs between 9mmHg (6mmHg) in the Calib Vital subset to approximately 12mmHg (8 mmHg) in the CalibFree Vital and up to 18mmHg (14mmHg) in the AAMI tasks for both the Combined and MIMIC subsets for systolic (diastolic) blood pressure. These results already provide first insights into the relative complexity of the different prediction tasks across all considered scenarios. Not surprisingly, the scores achieved for the Calib tasks are comparably lower (compared to CalibFree and AAMI tasks) as the model is able to exploit subject-specific information about test set samples through samples from the corresponding subjects seen during training.
Also, the results of MASE evaluation (Fig. \ref{fig:figure1}) also confirmed that although the MAE values of DBP are inherently smaller than SBP, the MASEs are higher, which at first sight counterintuitively indicates a better performance in the SBP category compared to DBP. A key reason for this is that DBP varies less across the entire dataset, and therefore, the median predictor represents a stronger baseline result. These findings emphasize the importance of not relying entirely on absolute performance metrics such as MAE but also considering relative metrics such as MASE.

\heading{Best-performing models} Aggregating model performance by counting best-performing models across all different setups, we conclude that ResNet and Inception-based frameworks generally represent the best-performing models. The shallow Lenet1D models are competitive in the CalibFree category and partly also in the AAMI category, but fail to achieve competitive results in the Calib scenario, which profits to a certain degree from model capacity to memorize specific patients. For instance, according to Table~\ref{tab:tab3}, Lenet1D is positioned in the same range as state-of-the-art architectures such as ResNet and Inception in distinct subsets (e.g., CalibFree Combined, CalibFree Vital, AAMI Combined, and AAMI MIMIC), e.g., on the subset of CalibFree Combined, Lenet1D has a MAE of 13.88mmHg (8.52 mmHg), which is very close to XResNet1d101 performance with 13.97mmHg (8.51 mmHg), and outperforms XResNet1d50 with 14.12mmHg (8.56 mmHg). This means that Lenet1D, although the simpler architecture, can provide robust results for the estimation of BP \textit{in certain scenarios}, as similarly reported in \cite{wagner2024explaining} for ECG analysis. 
Furthermore, it is worth noting that the S4 model does not show the outstanding performance it demonstrated in the ECG/EEG domain \cite{mehari2023towards,wang2023s4sleep,saab2024towards}.
As a final general observation, throughout most subsets, XResNet1d101 consistently ranks among the top-performing models. Therefore, we select XResNet1d101 as the DL model for the further sections of this paper. 

\begin{figure*}[ht] 
\centering
\includegraphics[width=1\textwidth]{image1.pdf} % Adjust width as needed
\vspace{-2cm}

\caption{MASE scores of different models trained on different PulseDB subsets and training scenarios: (top) SBP and (down) DBP.}
\label{fig:figure1}
\end{figure*}






\subsection{Performance evaluation within PulseDB}
\label{sec:IV-B}

In this section, we present the performance of our selected model, XResNet1d101, trained and tested on various subsets of PulseDB. The results are shown in Table \ref{tab:tab4}, where the train and test sets are represented in the vertical and horizontal columns, respectively. We organize the results by data source (Combined/Vital/MIMIC) and scenario (Calib/CalibFree/AAMI) in order to provide a comprehensive analysis of the results.  







\begin{table*}[h]
\centering
\caption{Performance of ID and OOD generalization on all subsets of PulseDB dataset for an XResNet1d101 model in terms of MAE (SBP / DBP) given in units of mmHg. Vertical and horizontal subsets represent the train and test sets, respectively. For each of the experiments, the three best-performing scenarios are marked in bold-face, and the overall best-performing scenario is also underlined.} 


\scalebox{0.7}{
\begin{tabular}{llccccccccc}

 & & \multicolumn{9}{c}{\textbf{Test Sets}} \\
\cmidrule(lr){3-11}
 & & \multicolumn{3}{c}{\textbf{Combined}} & \multicolumn{3}{c}{\textbf{Vital}} & \multicolumn{3}{c}{\textbf{MIMIC}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} & \textbf{9.42 / 5.97} & 15.54 / 9.28 & \underline{\textbf{19.2}} / 14.22 & \textbf{9.27 / \underline{6.08}} & 13.98 / 8.65 & \textbf{18.57} / 12.46 &\textbf{ 9.58} / \underline{\textbf{ 5.86}} & 17.2 / 9.95 & 19.82 / 15.97 \\
& \textbf{CalibFree} & 13.87 / \textbf{8.53 }& \textbf{13.97 / \underline{8.51}} & \textbf{20.09} / 14.73 & 12.34 / 8.23 &\textbf{ 12.58} / 8.10 & 21.73 / 14.41 & 15.5 / 8.85 & \textbf{15.44 / 8.94} & \textbf{18.47} / 15.04 \\
& \textbf{AAMI} & \textbf{13.61 / \underline{8.50}} & \underline{\textbf{13.96}} / 8.52 &\textbf{ 19.38} / 14.04 & 12.19 / 8.12 & \textbf{12.56 / 8.02} & 20.47 / 13.60 & 15.11 / 8.90 & \textbf{15.45 / 9.04} & \underline{\textbf{18.30} / \textbf{14.47}} \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & 14.69 / 9.74 & 16.7 / 10.8 & 21.31 / \textbf{13.95} & \underline{\textbf{9.08 / 6.08}} & 13.92 / 8.74 & \textbf{19.17} /\textbf{ 11.99} & 20.66 / 13.64 & 19.67 / 13.00 & 23.44 / 15.89 \\
& \textbf{CalibFree} & 15.37 / 8.81 & 16.38 / 9.20 & 20.21 / \underline{\textbf{13.30}} & \textbf{10.90 / 7.24} & 12.70 / \textbf{8.05} & \underline{\textbf{17.45 }/ \textbf{11.56}} & 20.14 / 10.49 & 20.29 / 10.42 & 22.94 / 15.02 \\
& \textbf{AAMI} & 15.12 / 9.13 & 14.83 / \textbf{8.89} & 20.21 / \textbf{13.43} & 11.84 / 7.90 & \underline{\textbf{ 12.18 / 7.87}} & 19.31 / \textbf{12.33} & 18.61 / 10.37 & 17.65 / 9.98 & 21.11 / \textbf{14.53} \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & \underline{\textbf{12.91}} / 8.63& 16.6 / 10.34& 22.94 / 16.27& 16.08 / 10.5& 16.24 / 10.27& 26.04 / 17.04& \underline{\textbf{9.52}} / \textbf{6.64} & 16.99 / 10.42& 19.87 / 15.51\\
& \textbf{CalibFree} & 14.69 / 9.02& 15.16 / 9.28& 21.36 / 15.86& 14.24 / 9.12& 14.87 / 9.3& 24.04 / 16.68& 15.16 / 8.92& 15.47 / 9.26 & 18.71 / \textbf{15.04}\\
& \textbf{AAMI} & 14.23 / 9.29& \textbf{14.59} / 9.47& 21.54 / 16.99& 13.7 / 9.96& 14.2 / 10.04& 24.77 / 18.34& \textbf{14.79} / \textbf{8.57}& \underline{\textbf{15.01 / 8.57}}& \textbf{18.35 }/ 15.56 \\
\bottomrule
\end{tabular}
}
\label{tab:tab4}
\end{table*}





\heading{Dependence on training dataset}
For all considered datasets and training scenarios, models trained and tested on the same data (MIMIC, Vital, and Combined) exhibited the lowest errors due to familiarity with the data. However, we stress that the ID performance is an overly optimistic measure of the model's generalization performance to other datasets. Comparing the performance between CalibFree MIMIC and CalibFree Vital shows that the MIMIC model generalizes better overall. While the ID performance for CalibFree Vital is quite strong, the (OOD) evaluation on MIMIC is poor.


\heading{Dependence on training scenario}
The results reveal that the lowest MAE for the  Calib subsets is when the training set contains patients from the same dataset, i.e., when corresponding (or combined) Calib training sets are used. This is the expected behavior, as in this case, the models can profit from memorized patient-specific signal patterns observed during training. Surprisingly, Calib models show a reasonable generalization to unseen patients from the CalibFree or AAMI test sets even though they were not trained from this purpose. Furthermore, Table \ref{tab:tab4} shows an exceptional performance on the Calib datasets of MIMIC but is significantly heterogeneous; in other words, though performing very strongly, it gave highly variable results, indicative of its performance variability across different scenarios. AAMI shows the largest overall errors, both in the intra- and inter-data source comparisons; (e.g., for the AAMI Combined, MAE is 13.26mmHg (8.24mmHg)). This is again the expected behavior since AAMI assesses the generalization to unseen patients (as CalibFree) but at the same time for a population covering a broad selection of BP values, i.e., with an inherent mismatch in label distribution compared to the training set distribution.


\subsection{OOD Performance evaluation on external datasets}
\label{sec:IV-C}

This section investigates the OOD performance of models trained on different PulseDB subsets and tested on various external datasets, i.e., Sensors, UCI, PPGBP, and BCG. The results are compiled in Table \ref{tab:tab5}.

\begin{table*}[h]
\centering
\caption{OOD generalization on external datasets using XResNet1d101 (SBP / DBP). Vertical and horizontal subsets represent the training and test sets, respectively. For each of the experiments, the three best-performing results are marked in bold-face, and the overall best-performing result is underlined. A Count column summarizes the number of top-three results for SBP and DBP. }

\scalebox{0.9}{
\begin{tabular}{llccccc}

& & \multicolumn{4}{c}{\textbf{Test Sets}} & \textbf{Count} \\
\cmidrule(lr){3-6}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Sensors $\downarrow$} & \textbf{UCI $\downarrow$} & \textbf{PPGBP $\downarrow$} & \textbf{BCG $\downarrow$} & \textbf{(SBP / DBP)} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} & 50.9 / 103.3 & \textbf{21.24} / 12.2 & \textbf{18.77 / 9.44} & \textbf{13.37} / 8.12 & 1 / 1 \\
& \textbf{CalibFree} & 21.18 / \textbf{9.78} & 24.76 / \textbf{10.37} & 25.07 / \underline{\textbf{8.2}} & 15.01 / 7.08 & 2 / 3 \\
& \textbf{AAMI} & 28.7 / 11.41 & 32.51 / 11.79 & 27.39 / 9.72 & 16.93 / 7.43 & 0 / 0 \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & \textbf{19.57} / 14.55 & \textbf{22.41} / 13.34 & \textbf{19.72} / 9.85 & 18.16 / 12.6 & 2 / 2 \\
& \textbf{CalibFree} & \textbf{18.46 / \underline{8.6}} & 25.07 / \textbf{10.83} & \textbf{\underline{18.69} / 8.66} & \textbf{\underline{10.05} / 6.92} & 3 / 4 \\
& \textbf{AAMI} & \textbf{\underline{16.28} / 10.66} & \underline{\textbf{19.7 / 10.36}} & 26.86 / 11.68 & \textbf{14.35} / 7.66 & 3 / 2 \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & 32.89 / 23.76 & 43.75 / 28.3 & 33.36 / 15.64 & 26.98 / 12.33 & 0 / 0 \\
& \textbf{CalibFree} & 35.7 / 13.43 & 40.64 / 13.74 & 35.7 / 11.09 & 17.17 / \underline{\textbf{5.89}} & 1 / 3 \\
& \textbf{AAMI} & 40.97 / 15.65 & 44.96 / 16.28 & 35.79 / 10.59 & 21.05 / \textbf{6.54} & 0 / 1 \\
\bottomrule
\end{tabular}
}
\label{tab:tab5}
\end{table*}







\heading{Dependence on training dataset}
A first superficial analysis of Table~\ref{tab:tab5} reveals that with the exception of two BCG results, only models trained on Vital or Combined show good generalization, in the sense of achieving results within the best three results. This suggests that the Vital subset of PulseDB, which is also part of Combined, seems to be an important component for good generalization. This results stands at tension with the result from the previous section, which seemed to indicate that MIMIC-based models show a better generalization performance on Vital than Vital-based models when tested MIMIC.

\heading{Dependence on training scenario} The CalibFree subsets, particularly "CalibFree Vital", demonstrate strong performances with some of the lowest MAE values across metrics: 18.46 mmHg (8.6 mmHg) for Sensors and 10.05 mmHg (6.92 mmHg) for BCG. Moreover, the performance of CalibFree subsets is more coherent when tested across different sources, often outperforming Calib models, especially for Vital datasets. This aligns with expectations as per training objective Calib models were not incentivized to generalize to unseen patients but rather to overfit to patient-specific patterns from the training set. The count column further highlights their robustness, with "CalibFree Vital" achieving top-three performances 3 times for SBP and 4 times for DBP.
Also, AAMI Vital shows strong performance on external datasets, achieving three top-three results for SBP and two for DBP. AAMI MIMIC shows a poor performance, which should most likely rather be attributed to the dataset and not to the training scenario.


\heading{Better generalization due to dataset similarity}
At this point, one might hypothesize that the generalization capabilities of models trained on different PulseDB subsets is primarily driven by the similarity between the respective training and evaluation datasets. We investigate this hypothesis for the case of CalibFree Vital as training dataset and test on external datasets that do not share any data with CalibFree Vital, i.e., the four external datasets as well as CalibFree MIMIC.. In Figure~\ref{fig:fig10}, we present a scatterplot of the OOD MAE versus dataset similarity quantified via the Earth Mover's Distance (EMD) \cite{pele2009fast} calculated based on the SBP distribution.

\begin{figure}[htbp]  
  \centering
  \includegraphics[width=.8\columnwidth,keepaspectratio]{Dissimilarity_vs_OOD_SBP_SelectedDatasets.pdf}
  \caption{Relationship between Dissimilarity Measure (EMD) and OOD Performance for SBP. The scatter plot shows the correlation between EMD and OOD MAE for SBP. Lower EMD reflects greater similarity to the baseline dataset (CalibFree Vital training set).}
  \label{fig:fig10}
\end{figure}



Figure~\ref{fig:fig10} indeed shows a correlation between dataset dissimilarity (quantified via EMD) and OOD MAE. This suggests that a mismatch between the blood pressure distributions might represent a dominant factor contributing to the domain mismatch between the respective datasets.










\subsection{Sample-Weighting Approach for OOD Generalization}
\label{sec:IV-D}

\heading{Overview} The findings of the previous subsection suggest that the results presented in Table~\ref{tab:tab5} might be biased by dataset similarity, which might obfuscate the search for the training dataset and training scenario that leads to the best generalization. Therefore, this section investigates this hypothesis using the domain adaptation approach introduced in Section~\ref{sec:III_D}.








\begin{table*}[h]
\centering
\caption{Difference in MAE (weighted - unweighted) for ID and OOD generalization on all categories of PulsedDB dataset using XRes
Net1d101 (SBP / DBP). Positive values (red) indicate a degradation (increase in MAE), while negative values (green) indicate an improvement (decrease in MAE). The mean improvement through importance weighting across all scenarios is given by 0.43~mmHg for SBP and 0.31~mmHg for DBP.}

\scalebox{0.55}{
\begin{tabular}{llccccccccc}
\toprule
& & \multicolumn{9}{c}{\textbf{Test Sets}} \\
\cmidrule(lr){3-11}
& & \multicolumn{3}{c}{\textbf{Combined}} & \multicolumn{3}{c}{\textbf{Vital}} & \multicolumn{3}{c}{\textbf{MIMIC}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Calib} & \textbf{CalibFree} & \textbf{AAMI} & \textbf{Calib} & \textbf{CalibFree} & \textbf{AAMI} & \textbf{Calib} & \textbf{CalibFree} & \textbf{AAMI} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 

& \textbf{Calib} 
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    & \diffcellsplitside{degradation}{+0.01}{degradation}{+0.32} 
    & \diffcellsplitside{degradation}{+0.35}{improvement}{-0.36} 
    & \diffcellsplitside{improvement}{-0.22}{improvement}{-0.12} 
    & \diffcellsplitside{improvement}{-0.21}{degradation}{+0.06} 
    & \diffcellsplitside{improvement}{-0.28}{improvement}{-0.88} 
    & \diffcellsplitside{improvement}{-0.30}{improvement}{-0.01} 
    & \diffcellsplitside{degradation}{+0.21}{improvement}{-0.15} 
    & \diffcellsplitside{degradation}{+0.38}{degradation}{+0.84} \\

& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+0.55}{degradation}{+0.12} 
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00}  
    & \diffcellsplitside{improvement}{-1.44}{improvement}{-1.62} 
    & \diffcellsplitside{improvement}{-0.39}{improvement}{-0.34}
    & \diffcellsplitside{improvement}{-0.28}{improvement}{-0.26}
    & \diffcellsplitside{improvement}{-3.91}{improvement}{-3.64} 
    & \diffcellsplitside{improvement}{-0.18}{degradation}{+0.01} 
    & \diffcellsplitside{improvement}{-0.09}{improvement}{-0.02} 
    & \diffcellsplitside{degradation}{+0.34}{degradation}{+0.34} \\

& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-0.19}{degradation}{+0.22}
    & \diffcellsplitside{degradation}{+0.23}{degradation}{+0.32}
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    & \diffcellsplitside{improvement}{-3.08}{improvement}{-2.11}
    & \diffcellsplitside{degradation}{+0.28}{degradation}{+0.53}
    & \diffcellsplitside{improvement}{-1.62}{improvement}{-1.12}
    & \diffcellsplitside{improvement}{-0.05}{degradation}{+0.10} 
    & \diffcellsplitside{degradation}{+0.26}{degradation}{+0.69} 
    & \diffcellsplitside{degradation}{+0.08}{degradation}{+0.70} \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 

& \textbf{Calib} 
    & \diffcellsplitside{degradation}{+0.39}{improvement}{-0.77}
    & \diffcellsplitside{degradation}{+0.33}{improvement}{-0.77}
    & \diffcellsplitside{improvement}{-1.01}{improvement}{-0.60}    
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    & \diffcellsplitside{improvement}{-0.22}{improvement}{-0.13}
    & \diffcellsplitside{improvement}{-2.47}{improvement}{-1.39} 
    & \diffcellsplitside{improvement}{-10.49}{improvement}{-7.20} 
    & \diffcellsplitside{improvement}{-0.61}{improvement}{-0.91} 
    & \diffcellsplitside{improvement}{-1.10}{degradation}{+0.27} \\

& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+1.30}{degradation}{+0.35} 
    & \diffcellsplitside{improvement}{-1.30}{improvement}{-0.43}
    & \diffcellsplitside{improvement}{-0.08}{improvement}{-0.49}
    & \diffcellsplitside{improvement}{-0.39}{degradation}{+0.12}  
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    & \diffcellsplitside{improvement}{-0.92}{improvement}{-1.25}
    & \diffcellsplitside{improvement}{-0.29}{improvement}{-0.41} 
    & \diffcellsplitside{improvement}{-2.12}{improvement}{-0.76} 
    & \diffcellsplitside{improvement}{-2.91}{improvement}{-0.49} \\

& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-0.13}{improvement}{-0.09}
    & \diffcellsplitside{degradation}{+0.26}{improvement}{-0.04} 
    & \diffcellsplitside{degradation}{+0.27}{degradation}{+0.23}
    & \diffcellsplitside{improvement}{-0.28}{improvement}{-0.09} 
    & \diffcellsplitside{degradation}{+0.40}{degradation}{+0.29}
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00}
    & \diffcellsplitside{degradation}{+0.12}{improvement}{-0.44} 
    & \diffcellsplitside{degradation}{+0.42}{improvement}{-0.47} 
    & \diffcellsplitside{improvement}{-0.86}{improvement}{-0.42} \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 

& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-0.15}{degradation}{+0.27}
    & \diffcellsplitside{degradation}{+0.29}{degradation}{+0.58} 
    & \diffcellsplitside{improvement}{-1.02}{improvement}{-0.84} 
    & \diffcellsplitside{degradation}{+0.13}{degradation}{+1.03}    
    & \diffcellsplitside{improvement}{-0.33}{degradation}{+0.74}
    & \diffcellsplitside{nochange}{0.00}{degradation}{+0.09}    
    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 


    & \diffcellsplitside{degradation}{+0.22}{improvement}{-0.36} 

     
    & \diffcellsplitside{degradation}{+0.24}{degradation}{+0.63} \\

& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+0.20}{degradation}{+0.17}
    & \diffcellsplitside{improvement}{-0.42}{degradation}{+0.61} 
    & \diffcellsplitside{improvement}{-0.38}{improvement}{-1.67} 
    
    & \diffcellsplitside{degradation}{+0.32}{degradation}{+1.18}
    & \diffcellsplitside{improvement}{-0.32}{improvement}{-0.04} 
    & \diffcellsplitside{improvement}{-0.62}{improvement}{-4.41} 
    
    & \diffcellsplitside{degradation}{+0.20}{degradation}{+0.02} 


    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} 
    

    & \diffcellsplitside{improvement}{-0.20}{improvement}{-0.38} \\
% --- Row: aami_mimic ---
& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-0.28}{improvement}{+0.48}
    & \diffcellsplitside{degradation}{+1.06}{degradation}{+0.97}
    & \diffcellsplitside{improvement}{-0.56}{improvement}{-0.57} 
    
    & \diffcellsplitside{degradation}{+1.26}{degradation}{+1.29}
    & \diffcellsplitside{degradation}{+1.04}{improvement}{-0.31} 
    & \diffcellsplitside{degradation}{+0.73}{degradation}{+0.13} 
    
    & \diffcellsplitside{improvement}{-1.46}{improvement}{-0.65} 
 

    & \diffcellsplitside{degradation}{+0.65}{degradation}{+0.74} 


    & \diffcellsplitside{nochange}{0.00}{nochange}{0.00} \\
\bottomrule
\end{tabular}
}
\label{tab:tab_diff_1}
\end{table*}





\heading{Overall impact of importance weighting}
The results of this analysis are compiled in Tables ~\ref{tab:tab_diff_1} and ~\ref{tab:tab_diff_2}, highlighting evaluation results within PulseDB and on external datasets. The tables in the main text highlight the difference between the weighted and unweighted MAE values, whereas the corresponding Tables ~\ref{tab:tab6} and \ref{tab:tab7} in the supplementary material indicate absolute MAE scores achieved via importance weighting.
The results represent direct analogues of Table~\ref{tab:tab4} and Table~\ref{tab:tab5}, however, using sample weights during training. In 86 of 153, i.e., in 56\% of the cases, the importance weighting leads to improved scores. The mean improvement achieved through importance weighting across all scenarios is given by 0.43~mmHg for SBP and 0.31~mmHg for DBP. Under AAMI vital test sets, our importance weighting approach improved blood pressure estimation accuracy by up to ~4mmHg. Although modest, these improvements are noteworthy given the strict accuracy requirements of AAMI and represent an important step toward clinical viability. As before, we proceed by analyzing the results from Table~\ref{tab:tab_diff_1} in more detail.



\heading{Dependence on training dataset}
 when comparing across different datasets, Combined and Vital-based models profit most from importance weighting. Before importance weighting (Table~\ref{tab:tab4}) and after applying it (Table~\ref{tab:tab6}), the best-performing models change: importance weighting seems to slightly shift from Vital-based models to Combined-based models. However, it is worth noting that Combined-based models when evaluated on Vital or MIMIC are always evaluated ID, as the two are proper subsets of Combined. Additionally, while importance weighting partially alleviates the aforementioned performance deficiencies of Vital-based models evaluated on MIMIC, it cannot fully completely overcome them.


\heading{Dependence on training and testing scenarios}
In terms of training scenarios, there is no clear trend in the sense of models that profit particularly from importance weighting. However, it is noteworthy, that evaluation on AAMI (with the sole exception of AAMI Combined) profits most consistently from importance weighted training. This aligns with expectations since AAMI is typically furthest from the Calib/Calibfree distributions.







\begin{table*}[h]
\centering
\caption{Difference in MAE (Weighted - unweighted) for OOD generalization on external datasets (SBP / DBP). Positive values (red) indicate a degradation (increase in MAE), while negative values (green) indicate an improvement (decrease in MAE). The mean improvement through importance weighting across all scenarios is given by 3.39~mmHg for SBP and 3.43~mmHg for DBP.}

\scalebox{0.8}{ 
\begin{tabular}{llcccc} 
\toprule
& & \multicolumn{4}{c}{\textbf{Test Sets}} \\ 
\cmidrule(lr){3-6}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Sensors} & \textbf{UCI} & \textbf{PPGBP} & \textbf{BCG} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-29.28}{improvement}{-89.09} 
    & \diffcellsplitside{improvement}{-0.41}{degradation}{+0.66} 
    & \diffcellsplitside{degradation}{+1.50}{improvement}{-1.10} 
    & \diffcellsplitside{improvement}{-0.57}{degradation}{+1.02} \\
    
& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+9.68}{degradation}{+1.72} 
    & \diffcellsplitside{improvement}{-3.89}{degradation}{+3.28} 
    & \diffcellsplitside{degradation}{+7.92}{degradation}{+3.39} 
    & \diffcellsplitside{improvement}{-2.37}{degradation}{+0.27} \\
    
& \textbf{AAMI} 
    & \diffcellsplitside{degradation}{+11.21}{degradation}{+0.93} 
    & \diffcellsplitside{degradation}{+13.32}{degradation}{+1.01} 
    & \diffcellsplitside{degradation}{+11.86}{degradation}{+1.68} 
    & \diffcellsplitside{improvement}{-0.29}{improvement}{-1.05} \\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-1.49}{improvement}{-2.21} 
    & \diffcellsplitside{improvement}{-0.74}{improvement}{-3.04} 
    & \diffcellsplitside{degradation}{+1.77}{degradation}{+1.31} 
    & \diffcellsplitside{improvement}{-6.69}{improvement}{-4.11} \\
    
& \textbf{CalibFree} 
    & \diffcellsplitside{degradation}{+1.61}{improvement}{-0.18} 
    & \diffcellsplitside{improvement}{-1.04}{degradation}{+0.25} 
    & \diffcellsplitside{degradation}{+3.06}{degradation}{+1.92} 
    & \diffcellsplitside{degradation}{+0.67}{improvement}{-0.35} \\
    
& \textbf{AAMI} 
    & \diffcellsplitside{degradation}{+0.75}{improvement}{-3.10} 
    & \diffcellsplitside{degradation}{+0.06}{improvement}{-1.39} 
    & \diffcellsplitside{improvement}{-9.68}{improvement}{-3.52} 
    & \diffcellsplitside{improvement}{-4.34}{improvement}{-0.15} \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} 
    & \diffcellsplitside{improvement}{-13.10}{improvement}{-14.52} 
    & \diffcellsplitside{improvement}{-23.36}{improvement}{-16.86} 
    & \diffcellsplitside{improvement}{-10.63}{improvement}{-4.86} 
    & \diffcellsplitside{improvement}{-11.92}{improvement}{-4.59} \\
    
& \textbf{CalibFree} 
    & \diffcellsplitside{improvement}{-12.66}{improvement}{-2.82} 
    & \diffcellsplitside{improvement}{-13.61}{degradation}{+3.30} 
    & \diffcellsplitside{degradation}{+10.05}{degradation}{+6.36} 
    & \diffcellsplitside{improvement}{-7.42}{improvement}{-0.04} \\
    
& \textbf{AAMI} 
    & \diffcellsplitside{improvement}{-11.51}{improvement}{-0.02} 
    & \diffcellsplitside{improvement}{-22.94}{improvement}{-6.48} 
    & \diffcellsplitside{degradation}{+1.71}{degradation}{+7.44} 
    & \diffcellsplitside{improvement}{-9.22}{degradation}{+1.43} \\
\bottomrule
\end{tabular}
}
\label{tab:tab_diff_2}
\end{table*}


\heading{OOD generalization for models trained with importance weighting}
The impact of the importance weighting approach on OOD generalization on the external datasets is shown in Table \ref{tab:tab_diff_2} and \ref{tab:tab7}. In 42 of the $2\times 9\times 4=72$ cases, i.e., in 58\% of the cases, the approach improved the scores. However, this statistic obfuscates the true picture as it does not take into account the magnitude of the improvement/degregation. The mean improvement through importance weighting across all scenarios is given by 3.39~mmHg for SBP and 3.43~mmHg for DBP and therefore, substantially larger than the improvements within PulseDB, see Table~\ref{tab:tab_diff_1}. The table shows that the importance of weighting significantly improves the performance of all MIMIC datasets. As before, we proceed with a more detailed analysis of Table~\ref{tab:tab7}.




\heading{Dependence on training dataset}
These results underscore that choosing the right training data can significantly enhance generalization on specific test sets. For example, training on the Vital dataset achieved 14 best-performing scenarios (highlighted in bold) out of 24 cases, whereas training on the MIMIC and Combined datasets resulted in only 6 and 3 best-performing models, respectively. Interestingly, the largest and most diverse training dataset, namely Combined, does not lead to the best generalization.


\heading{Dependence on training scenario}
AAMI subsets, especially ``AAMI Vital'', report lower MAE compared to other subsets, such as 17.03mmHg (7.56 mmHg) (Sensors), 19.76mmHg (8.97 mmHg) (UCI), and 17.18mmHg (8.16 mmHg) (PPGBP), and it acquired one of the best performances on BCG at 10.01mmHg (7.51 mmHg). This might relate to the broad blood pressure distibution of AAMI, which serves as a basis for importance weighting approaches.

\heading{Comparison to literature results}
A side-by-side comparison between our OOD generalization performance and ID (ML) evaluation results provided by \cite{gonzalez2023benchmark} would provide important insight into the strengths and weaknesses of our approach. Table \ref{tab:tab9} presented the ID results based on ResNet and SpectroResNet \cite{slapnivcar2019blood} models, which are similar architectures to our XResNet1d101 for feature extractions. It is worth noting that the SpectroResNet model integrates a ResNet-GRU architecture that will capture both temporal and spectro-temporal information effectively. It is important to stress that these results report in-distribution performance evaluation and are compared to models trained on PulseDB subsets. To this end, we compiled results for  ``AAMI vital'' and ``CalibFree MIMIC'' were selected since within the tables \ref{tab:tab4},\ref{tab:tab5}, \ref{tab:tab6} and \ref{tab:tab7}, as the best-performing models from the previous analysis. Most notably, models show a solid OOD evaluation performance reaching the performance level of models trained on these dataset, i.e. ID performance. This is the case for AAMI Vital, both with and without importance weighting, on Sensors  and for AAMI Vital and CalibFree MIMIC with importance weighting on BCG.

\begin{table*}[t] 
\centering
\caption{Unweighted OOD evaluation vs. importance-weighted OOD evaluation, both trained using XResNetd101 model, in comparison to in-distribution results achieved on the external dataset. For each of the experiments, the three best-performing models are marked in bold-face and the overall best-performing model is underlined.}

\scalebox{1}{
\begin{tabular}{llcccc}

& & \multicolumn{4}{c}{\textbf{Test Sets}} \\
\cmidrule(lr){3-6}
\textbf{Experiment Type} & \textbf{Approach} & \textbf{Sensors $\downarrow$} & \textbf{UCI $\downarrow$} & \textbf{PPGBP $\downarrow$} & \textbf{BCG $\downarrow$} \\
\midrule
\multirow{2}{*}{ID (\cite{gonzalez2023benchmark})}
& ResNet & 17.46 / \textbf{8.33} & \underline{\textbf{16.59} / \textbf{8.30}} & \textbf{13.62} / \textbf{8.61} & 12.20 / 7.76 \\
& SpectroResNet & \textbf{17.29} / \textbf{9.73} & 21.92 / 10.21 & \underline{\textbf{11.01}} / \textbf{8.46} & \textbf{9.89 / 6.29} \\
\midrule
\multirow{2}{*}{OOD evaluation (Ours)} 

& Trained on AAMI Vital & \textbf{\underline{16.28}} / 10.66 & \textbf{19.70 / 10.36} & 26.86 / 11.68 & 14.35 / 7.66 \\

& Trained on CalibFree MIMIC & 35.7 / 13.43 & 40.64 / 13.74 & 35.7 / 11.09 & 17.17 / \textbf{5.89} \\
\midrule
\multirow{2}{*}{Importance weighting (Ours)}

& Trained on AAMI Vital & \textbf{17.03 / \underline{7.56}} & \textbf{19.76 / 8.97} & \textbf{17.18} / \textbf{\underline{8.16}} & \textbf{10.01} / 7.51 \\

& Trained on CalibFree MIMIC & 23.04 / 10.61 & 27.03 / 17.04 & 45.75 / 17.45 & \underline{\textbf{9.75 / 5.85}} \\
\bottomrule
\end{tabular}
}
\label{tab:tab9}
\end{table*}



\section{Discussion}
\label{sec:V}
\heading{ID performance does not reflect OOD generalization}
A common trend across all tables is that models achieve a consistently lower MAE score when evaluated on the respective matching test set, i.e. when evaluated ID. However, this is typically largely exceeded when evaluating on OOD datasets or even in other training scenarios based on the same training dataset. This message nicely aligns with the findings of \cite{weber2023intensive}, who investigated generalization issues between PPGBP and MIMIC leveraging feature-based models. The magnitude of the performance degradation depends heavily on the training dataset and scenario, but in particular MIMIC-based models show a poor OOD generalization performance. On the hand, there are also combinations of training dataset and scenarios such Calibfree Vital or AAMI Vital that show a particularly good generalization performance. Most importatnly, these observations reinforce that ID scores should not be considered as representative for the generalization capabilities of the model to unseen data.




\heading{Effect of domain adaptation}
For a fixed training dataset, the OOD performance showed a correlation with the similarity between training and test datasets assessed via their BP distributions. This lead to the exploration of domain adaption through importance weighting with importance weights inferred from the difference between the two respective BP distributions. The importance weighting approach leads to improved OOD performance.

In general, importance weighting seems to represent an appropriate method to enhance the generalization on unseen external datasets, especially on subsets that represent more variability, such as the CalibFree and AAMI datasets. At this point, it is worth stressing that the presented approach using reweighting based on the BP distribution obviously only captures differences in the BP distributions, whereas the datasets will typically differ according to many other criteria such as patient characteristics, sensor equipment, and/or signal quality.
It is an interesting question for future research if reweighting using a predefined, fixed label distribution such as a flat or a label distribution inferred from a population cohort would improve the robustness of the model we encountering an unknown target distribution (without prior knowledge of its label distribution).

\heading{Future research directions}
While the presented models showed competitive performance even in comparison to in-distribution performance measures reported in the literature, BP estimation from PPG data remains a challenging task. This can be seen by putting the achieved MAE scores into the perspective of the IEEE standard for cuffless BP estimation \cite{IEEE1708a2019}, where MAEs above 7mmHg are considered as grade D and hence unsuitable for clinical use. In this sense, all presented methods have still a long way to go. However, it is also important to stress that we only report MAE scores averaged across entire datasets. The absolute error distribution itself is typically a bimodal distribution with a substantial fraction of samples in the acceptable grades A-B according to the aforementioned IEEE standards and a second group of samples in grade D. Uncovering patterns for the assignment of an unseen sample to one of these groups, for example, based on clinical metadata, would be a large step forward. Next to that, one might rely on the inclusion of additional clinical metadata, more strict data quality control, or pretraining paradigms as proposed in the context of self-supervised learning to eventually shift the entire MAE distribution in a clinically acceptable range. It is worth mentioning that the leveraging of pretrained (foundation) models \cite{pillai2024papagei,ding2024siamquality} might represent a promising path to alleviate these issues. The study of the OOD generalization of such models is deferred to future work.


\section{Summary}
\label{sec:VI}

In this study, we conducted a benchmark study on BP prediction from PPG signals using different DL models. In addition, the ID and OOD generalization capability of the DL model for BP prediction is examined across various subsets and data sources. In line with expectations, it can be concluded that the models performed best when trained and tested on the same subset. However, the performance level reached during ID evaluation typically turned out to be an overly optimistic measure of the generalization capabilities when applied to unseen data from unseen sources. In this work, we identified training datasets and scenarios that lead to good OOD generalization. Within PulseDB, it is the Vital subset (in CalibFree and AAMI scenarios) that leads to good generalization performance, whereas MIMIC-based models show poor OOD generalization. This puts into question the use of MIMIC as the predominant training dataset for generalizable BP estimation. 
We identified mismatches in the BP distributions as one important aspect contributing to dataset drifts and explored importance weighting as a domain adaptation technique to mitigate its effect. These techniques establish, in particular, AAMI Vital-based models with good generalization capabilities.

However, as the most important take-away message, we hope to sensibilize the community to the importance of performance evaluation on external datasets and the need for continued work on domain adaptation techniques. This work is exclusively based on publicly available datasets and we release the source code for preprocessing and model training under \url{https://https://github.com/AI4HealthUOL/ppg-ood-generalization}.





\section*{Acknowledgments}
The project (22HLT01 QUMPHY) has received funding from the European Partnership on Metrology, co-financed from the European Union’s Horizon Europe Research and Innovation Programme and by the Participating States. Funding for the University of Cambridge was provided by Innovate UK under the Horizon Europe Guarantee Extension, grant number 10091955. PHC acknowledges funding from the British Heart Foundation (BHF) grant [FS/20/20/34626]. 


\bibliographystyle{IEEEtran}
\bibliography{bibfile}


\clearpage
\appendices

\section{Supplementary figures and tables}
Tables~\ref{tab:tab6} and \ref{tab:tab7} show absolute performance metrics (as opposed to relative changes in the main text) within PulseDB and on external datasets. In Figures 
\ref{fig:sbp_distributions} and \ref{fig:dbp_distributions}, we visualize the SBP/DBP distributions across the datasets considered in this work.

\begin{table*}[!!!ht]
\centering
\caption{Weighted ID and OOD generalization on all categories of PulsedDB dataset using XResNet1d101 (SBP / DBP). Vertical and horizontal categories represent the training and test sets, respectively. For each of the experiments, the three best-performing scenarios are marked in bold-face and the overall best-performing scenario is also underlined.}


\scalebox{0.7}{
\begin{tabular}{llccccccccc}

 & & \multicolumn{9}{c}{\textbf{Test Sets}} \\
\cmidrule(lr){3-11}
 & & \multicolumn{3}{c}{\textbf{Combined}} & \multicolumn{3}{c}{\textbf{Vital}} & \multicolumn{3}{c}{\textbf{MIMIC}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} & \textbf{Calib $\downarrow$} & \textbf{CalibFree $\downarrow$} & \textbf{AAMI $\downarrow$} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} & \underline{\textbf{9.42 / 5.97}} & 15.55 / 9.50& \textbf{19.65} / 13.86&   \underline{\textbf{9.05 / 5.96}} & 13.77 / 8.71& 18.91 / 11.52& \textbf{\underline{9.28 / 5.85}}& 17.41 / 9.80 & 20.20 / 16.81 \\
& \textbf{CalibFree} & 14.42 / \textbf{8.65}& \underline{\textbf{13.97 / 8.51}}& \textbf{\underline{ 18.65 / 13.11}} & 12.06 / 7.97 & 13.14 /  \textbf{8.08 }&  \textbf{17.82 / 10.77}& 15.32 / 8.86& \textbf{\underline{15.35 / 8.92}} & 18.81 / 15.78\\
& \textbf{AAMI} & \textbf{13.42 / 8.72}& \textbf{14.19 / 8.84}& \textbf{19.38} / 14.04 & \textbf{9.11 / 6.01} & \textbf{12.84} / 8.55& 18.85 / 12.48 & 15.06 / 9.00& 15.71 / 9.73 & \textbf{18.38} / 15.17\\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & 15.08 / 8.97& 17.03 / 10.03& 20.32 / \textbf{13.35}&\textbf{9.08 / 6.08}& 13.70 / 8.61 & \textbf{16.70 / 10.60} & \textbf{10.17 / 6.44} & 19.06 / 12.09 & 22.34 / 16.16 \\
& \textbf{CalibFree} & 16.67 / 9.16& 15.08 / \textbf{8.77} & 20.13 / \textbf{13.42} & 10.51 / 7.15 & \textbf{12.70 / \underline{8.05}} & \underline{\textbf{16.53 / 10.31}} & 19.85 / 10.08 & 18.17 / 9.66 & 20.03 / \textbf{14.53} \\
& \textbf{AAMI} & 14.99 / 9.04& 15.09 / 8.85& 19.94 / 13.67& 11.83 / 7.81 & \textbf{\underline{12.58} / 8.16} & 19.31 / 12.33 & 18.73 / 9.93 & 18.07 / \textbf{9.51} & 20.25 /  \underline{\textbf{14.11}} \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & \textbf{12.76} / 8.90& 16.89 / 10.92& 21.96 / 15.43& 16.21 / 11.53 & 15.91 / 11.01 & 26.04 / 17.13 & \textbf{9.52 / 6.64}& 17.21 / 10.06 & 20.11 / 16.14 \\
& \textbf{CalibFree} & 14.89 / 9.19& \textbf{14.74} / 9.89& 20.98 / 14.19& 14.56 / 10.3 & 14.55 / 9.26 & 23.42 / 12.27 & 15.36 / 8.94 & \textbf{ 15.47 / 9.26}& \textbf{18.51 / 14.66} \\
& \textbf{AAMI} & 13.95 / 9.72& 15.65 / 10.44& 20.98 / 16.42& 14.96 / 11.25 & 15.24 / 9.73 & 25.50 / 18.47 & 13.33 / 7.92 & \textbf{15.66} / 9.61 & \textbf{\underline{18.35}} / 15.56\\
\bottomrule
\end{tabular}

}
\label{tab:tab6}
\end{table*}


\begin{table*}[!!!ht]
\centering
\caption{OOD generalization on external datasets for models trained using importance weighting with XResNet1d101 (SBP / DBP). Vertical and horizontal categories represent the training and test sets, respectively. For each of the experiments, the three best-performing scenarios are marked in bold-face, and the overall best-performing scenario is also underlined. A Count column summarizes the number of top-three results for SBP and DBP.}

\scalebox{0.8}{
\begin{tabular}{llccccc}

& & \multicolumn{4}{c}{\textbf{Test Sets}} & \textbf{Count} \\
\cmidrule(lr){3-6}
\textbf{Train Sets} & \textbf{Subgroup} & \textbf{Sensors $\downarrow$} & \textbf{UCI $\downarrow$} & \textbf{PPGBP $\downarrow$} & \textbf{BCG $\downarrow$} & \textbf{(SBP / DBP)} \\
\midrule
\multirow{3}{*}{\textbf{Combined}} 
& \textbf{Calib} & 21.62 / 14.21 & 20.83 / 12.86 & \textbf{20.27 / 8.34} & 12.80 / 9.14 & 1 / 1 \\
& \textbf{CalibFree} & 30.86 / 11.50 & 20.87 / 13.65 & 32.99 / 11.59 & 12.64 / 7.35 & 0 / 0\\
& \textbf{AAMI} & 39.92 / 12.30 & 45.83 / 12.80 & 39.25 / 11.40 & \textbf{10.64 / 6.38} & 1 / 1\\
\midrule
\multirow{3}{*}{\textbf{Vital}} 
& \textbf{Calib} & \textbf{18.08} / 12.34 & \textbf{21.67 / 10.3} & 21.49 / 11.16 & 11.47 / 8.49 & 2 / 1 \\
& \textbf{CalibFree} & 20.07 / \textbf{8.42} & 24.03 / 11.08 & \textbf{21.75 / 10.58} & 10.72 / \textbf{6.57} & 1 / 3\\
& \textbf{AAMI} & \underline{\textbf{17.03 / 7.56}} & \textbf{\underline{19.76 / 8.97}} & \textbf{\underline{17.18 / 8.16}} & \textbf{10.01} / 7.51 & 4 / 3 \\
\midrule
\multirow{3}{*}{\textbf{MIMIC}} 
& \textbf{Calib} & \textbf{19.79 / 9.24} & \textbf{20.39} / 11.44 & 22.73 / 10.78 & 15.06 / 7.74 & 2 / 1 \\
& \textbf{CalibFree} & 23.04 / 10.61 & 27.03 / 17.04 & 45.75 / 17.45 & \textbf{\underline{9.75 / 5.85}} & 1 / 1\\
& \textbf{AAMI} & 29.46 / 15.63 & 22.02 / \textbf{9.80} & 37.5 / 18.03 & 11.83 / 7.97 & 0 / 1 \\
\bottomrule
\end{tabular}
}
\label{tab:tab7}
\end{table*}




\begin{figure*}[htbp]
  \centering

  \includegraphics[width=\textwidth]{SBP_Distributions_Combined_Enhanced_WithAamiVital_2.pdf}
  \caption{SBP distribution plots for all datasets}
  \label{fig:sbp_distributions}
\end{figure*}

\begin{figure*}[htbp]
  \centering

  \includegraphics[width=\textwidth]{DBP_Distributions_Combined_Enhanced_WithAamiVital_2.pdf}
  \caption{DBP distribution plots for all datasets}
  \label{fig:dbp_distributions}
\end{figure*}


\end{document}



