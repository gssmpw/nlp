%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}
\usepackage{algorithmic}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\com}[1]{{\color{black!35}//\,#1}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submisspion:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{float}
\newfloat{algorithm}{t}{lop}

\usepackage{xcolor}
\definecolor{machine-blue}{rgb}{0.619,0.722,0.827}
\definecolor{human-orange}{rgb}{0.929,0.765,0.584}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\input{commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\papertitle}

\begin{document}

\twocolumn[
\icmltitle{\name: \papertitle}
%%CF.1.18: Please use title case for titles. Things like "NN" aren't good for a title either, so I assume this is a placeholder for now. But it would be good to come up with some good title ideas.
%%EM.1.19:
% Local Likelihood Curvature Improves Unsupervised Detection of Machine-Generated Text
% Unsupervised Machine-Generated Text Detection through Likelihood Curvature Estimation
% DetectGPT: Unsupervised Machine-Generated Text Detection
% DetectGPT: Unsupervised Machine-Generated Text Detection using Local Likelihood Curvature


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Eric Mitchell}{stan}
\icmlauthor{Chelsea Finn}{stan}
\end{icmlauthorlist}

\icmlaffiliation{stan}{Stanford University}

\icmlcorrespondingauthor{Eric Mitchell}{eric.mitchell@cs.stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{chatgpt, cheating, unsupervised, text}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
The remarkable fluency and factual knowledge of large language models (LLMs) 
%%CF.1.18: nit, but I usually try to avoid shortening "neural network" to "neural" because neural networks are not _that_ neural. (e.g. compared to the brain) I think they are more networky than neural.
%EM.1.19: Done
%%CF.1.18: I'd remove "notably ChatGPT" so that it ages better. There will probably be another model that is better in a couple years, and hopefully people will still be reading the paper a couple years from now.
%EM.1.19: Done
heightens
%%CF.1.18: could consider "heightens" instead of "highlihgts" (no strong opinion though)
%EM.1.19: Agree
the need for corresponding systems to identify whether a piece of text is human or machine-written. For example, students may use LLMs to complete natural language- or code-writing assignments, leaving instructors unable to detect cheating and accurately assess students' abilities. In this paper, we first demonstrate that text sampled from an LLM
%%CF.1.18: "we leverage the empirical observation that" is a bit of a mouthful. could just say "we observe that"? It could also make sense to break this up into two sentences.
%EM.1.19: Done
tends to occupy a local maxima of the model's likelihood function. Leveraging this observation, we define a new curvature-based criterion for judging if a passage is generated from a given LLM. This method, which we call {\name}, does not require training a separate classifier or collecting any new datasets of real or generated passages, using only likelihoods computed by the model of interest.
%%CF.1.18: it might be nice to convey that it relies on access to the model you are detecting
%EM.1.19: Done
We show that {\name} is far more discriminative than thresholding by model likelihood, the most common existing unsupervised method for model sample detection, improving machine-generated text detection AUROC by 0.2 absolute even for LLMs with 20B parameters.
%%CF.1.18: not clear what "far more" means. Can you provide explicit numbers?
%EM.1.19: Done
\end{abstract}

\section{Introduction}
Large language models (LLMs) have proven able to generate remarkably fluent responses to a wide variety of user queries. Models such as GPT-3 \citep{gpt3}, PaLM \citep{palm}, and ChatGPT \citep{chatgpt} can convincingly answer complex questions about science, mathematics, historical and current events, and social trends. However, the convincing quality of these responses is often spurious, and recent work has found that LLM-generated responses are often simply wrong \citep{lin-etal-2022-truthfulqa}. Nonetheless, the convincing nature of the generations of these models makes them attractive for replacing human labor in a variety of contexts, notably student essay writing and journalism. Such applications of LLMs are problematic for a variety of reasons, making fair student assessment difficult, impairing student learning, and proliferating convincing-but-inaccurate news articles. While tighter access control may prevent some negative impact of LLMs, the commercial potential for powerful LLMs makes it likely that model providers will be encouraged to make their models \textit{more} accessible, rather than less. 
%%CF.1.18: I think that this statement is debatable, and I don't think we really need to articulate an opinion on this to tell a good story? Could more simply say that we shouldn't rely on access control as the only means, especially since these models are already widely available. Or not even mention access control, since it's pretty clearly not a satisfactory solution.
An alternative to access control is effective methods for \textit{detecting} machine-generated text, giving teachers and news-readers more confidence in the human origin of the text that they consume. In this paper, we continue recently-growing efforts to address this {\probfull} (\prob) problem.
%%CF.1.18: I don't think we really need an acronym for the problem statement, especially since it makes it more jargony.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/fig1.pdf}
    \caption{We aim to perform automated classification of whether a piece of text was generated by a particular LM, such as GPT-3, or a human. {\name} does so in an \textit{unsupervised} manner, meaning no data collection, training, or fine-tuning is needed to construct the detection algorithm.}
    \label{fig:fig1}
    %%CF.1.18: Would it make sense to pick an example that is less fake?
    %%CF.1.18: make sure you reference this figure in the intro
\end{figure}

As in prior work, we study the {\probfull} problem as a binary classification problem \citep{Jawahar2020AutomaticDO}, aiming to classify whether a \textit{candidate passage} was generated by a particular \textit{source model} $\sm$.
%%CF.1.18: You don't use p_theta again, so I would keep the notation to future sections.
While several works have investigated methods for training a second deep network to detect machine-generated text, such an approach has several shortcomings, including a tendency to overfit to the source models or topics it was trained to perform detection for
%%CF.1.18: you could argue that the source model overfitting limitation is somewhat applicable to our approach as well, and it's covered in what comes next. I would rephrase to "including a tendency to overfit to the topics it was trained on as well as a need to train a new classifier for each new source model that is released"
as well as the need to train a new model for each new source model that is released. We therefore consider \textit{zero-shot} {\prob},
%%CF.1.18: maybe "We therefore consider a zero-shot version of the problem, where we use..."
where we use the source model itself, without fine-tuning, to detect its samples. The most common method for zero-shot {\prob} is evaluating the average per-token log-likelihood of the generated text and thresholding.
%%CF.1.18: citation(s) please
However, such a zero-th-order approach to detection fails to leverage the local structure of the likelihood function around a candidate passage, which we find contains additional useful information about the source of a passage.

This paper poses a simple hypothesis: minor rewrites of a \textit{model sample} tend to have under the model lower likelihood than the original sample, while minor rewrites of human-generated text may have higher or lower likelihood than the original sample (see Figs.~\ref{fig:local-structure} and Fig.~\ref{fig:perturbation-discrepancy}). In other words, unlike human-written text, model-generated text lies in a local maximum of the model likelihood function. We empirically verify this hypothesis for a variety of existing LLMs
%%CF.1.18: do you actually directly verify the hypothesis?
and leverage it to build {\name}, an unsupervised method for automated {\prob} by perturbing a candidate passage (e.g., with T5 \citet{raffel2020t5}) and evaluating the likelihood of the passage before and after perturbation.
%%CF.1.18: I feel like this is far too many different thoughts in one sentence, especially given that this sentence is stating the main contribution of the paper. It would be good to more cleanly separate the key method from instantiations of it (e.g. using T5) in separate sentences.
Our experiments find that {\name} is significantly more accurate 
%%CF.1.18: please be more specific with numbers. "significantly more" can mean a lot of things.
than likelihood thresholding for detecting model samples across source models, sampling methods, and subject matter, improving by over 0.15 AUROC in some cases.


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/prop-1-v2.png}
    \vspace{-5mm}
    \caption{We identify and exploit the tendency of machine-generated passages $x\sim\sm(\cdot)$ \textbf{(left)} to lie near a local maximum of the model's likelihood function; nearby samples are systematically lower likelihood. In contrast, human-written text $x\sim p_{real}(\cdot)$ \textbf{(right)} tends not to occupy a local extremum of a model's likelihood function; nearby samples may be higher or lower likelihood.}
    \label{fig:local-structure}
    %%CF.1.18: not sure if page 2 is the best place for this. It could make sense to put it with the method?
    %%CF.1.18: also be sure to reference the figure in the main text.
\end{figure}

\section{Related Work}

The importance of {\probfull} (\prob) is highlighted by recent advancements in LLMs \citep{gpt2,gpt3,palm,chatgpt,opt},
%%CF.1.18: this text seems almost verbatim copied from the intro. It would be nice to not be so repetitive.
which are capable of generating remarkably fluent and on-topic text. The GROVER model \citep{Zellers2019DefendingAN} was the first LLM trained specifically for generating realistic-looking news articles. Human evaluators found GROVER-generated news more reliable than human-written articles, on average,
%%CF.1.18: I don't think you need all this detail. 
motivating the authors to study GROVER's ability to detect its own generations by fine-tuning a detector on top of its features; they found GROVER better able to detect GROVER-generated text than other pre-trained models. However, \citet{uchendu-etal-2020-authorship} find that the fine-tuned GROVER's ability to detect machine-generated text is in large part limited to GROVER-generated text, and its accuracy decreases significantly for generations from other models. \citet{bakhtin2019learning} also similarly find that models trained explicitly for {\probfull} tend to overfit to their training distribution.

Other works have trained supervised models for {\probfull} on top of on neural representations \citep{uchendu-etal-2020-authorship,bakhtin2019learning,release-strategies,tweepfake,ippolito-etal-2020-automatic}, bag-of-words features \citep{release-strategies,tweepfake}, and handcrafted statistical features based on the histogram of token likelihoods under the suspected source model \citep{gehrmann-etal-2019-gltr}. 
%%CF.1.18: maybe just "handcrafted statistical features"? It's currently hard to follow.
Alternatively, \citet{release-strategies} notes the surprising efficacy of a simple unsupervised method for {\probfull}, which thresholds a candidate passage based on its total average likelihood under the generative model, serving as a strong baseline for unsupervised {\prob} in our work. In our work, we use the generating model to detect its own generations, but do so completely unsupervised, that is, without additional data collection or fine-tuning.
%%CF.1.18: isn't this statement also true for average likelihood thresholding? current phrasing implies that it's not true compared to what you just talked about.
See \citet{Jawahar2020AutomaticDO} for a complete survey on {\probfull}.

The problem of {\probfull} echoes earlier work on detecting deepfakes, fake images or videos created by deep nets, which has spawned substantial efforts in detection fake visual content \citep{DFDC2020,zi2020wilddeepfake}. While early works in deepfake detection used relatively general-purpose model architectures \citep{guera2018deepfake}, many deepfake detection methods rely on the continuous nature of image data to achieve state-of-the-art performance \citep{zhao2021multi,Guarnera_2020_CVPR_Workshops}, making direct application to text difficult. Also related are works that aim to extract the hyperparameters of the model that generated a given text passage \citep{tay-etal-2020-reverse} or image \citep{asnani2021reverse}.
%%CF.1.18: This last sentence seems weird. Seems like an after thought, and it's not immediately clear why it is related. If the techniques used are relevant enough to be mentioned in the related work, then it warrants more discussion on how our technique is different. 

\section{The {\probfull} problem}
%%CF.1.18: please use title case for section headers

\section{{\name}: Unsupervised {\probfull} with random perturbations}
\label{sec:local-extremum}
%%CF.1.18: please use title case for section headers


\begin{algorithm}
    \caption{{\name} prediction procedure}
    \label{alg:method}
    \begin{algorithmic}[1]
        \footnotesize \STATE \textbf{Input:} passage $x$, source model $\sm$, perturbation function $q$, number of perturbations $k$, decision threshold $\epsilon$
        \STATE \textbf{Return:} \texttt{true} if $x\sim\sm$; \texttt{false} otherwise
        \STATE $\tilde x_i \sim q(\cdot|x),\,i\in [1..k]$ \hfill\com{mask spans, sample replacements}
        \STATE $\tilde \mu \gets \frac{1}{k}\sum_i\log \sm(\tilde x_i)$ \hfill \com{approximate expectation in Eq. 1}
        \STATE $\pdh_x \gets \log \sm(x) - \tilde \mu$ \hfill\com{estimate $\pd$}
        \STATE \scalebox{0.95}{$\tilde \sigma_x^2 \gets \frac{1}{k-1}\sum_i\left(\log \sm(\tilde x_i) - \tilde \mu\right)^2$ \hfill\com{variance for normalization}}
        \IF{$\frac{\pdh_x}{\sqrt{\tilde \sigma_x}} \geq \epsilon$}% \hfill\com{normalized discrepancy criterion}
            \STATE return \texttt{true} \hfill\com{probably model sample}
        \ELSE
            \STATE return \texttt{false} \hfill\com{probably not model sample}
        \ENDIF
    \end{algorithmic}
\end{algorithm}

{\name} is based on the hypothesis that samples from a source model $\sm$ typically lie in a local maximum of the likelihood function of $\sm$, while human-written text tends not to lie in extrema of $\sm$. In other words, if we apply small perturbations to a sample $x\sim\sm$, producing $\tilde x$, we should generally have $\log \sm(x) > \log \sm(\tilde x)$, while this inequality only holds for roughly half of samples for samples from other distributions. To leverage this hypothesis, first consider a perturbation function $q(\cdot|x)$ that produces slightly modified version of $x$, with similar meaning. As an example, $q(\cdot|x)$ might be result of simply asking a human to rewrite one of the sentences of $x$, while preserving the meaning of $x$. Using the notion of a perturbation function, we can define the \textit{perturbation discrepancy} $\pd$: 
\begin{equation}
    \mathbf{d}(x) = \log \sm(x) - \mathbb{E}_{\tilde x \sim q(\cdot|x)} \log \sm(\tilde x)
    \label{eq:perturbation-discrepancy}
\end{equation}
We now state our hypothesis more formally in Hypothesis~\ref{hyp:main}.
\begin{hypothesis}
    The perturbation discrepancy $\pd$ is positive with high probability for samples from $\sm$. For human-written text, $\pd$ tends toward zero.
\label{hyp:main}
\end{hypothesis}

If we define $q(\cdot|x)$ to be samples from a mask-filling model such as T5 \citep{raffel2020t5}, rather than human rewrites, we can empirically test Hypothesis~\ref{hyp:main}. For real data, we use news articles from the XSum dataset \citep{shashi2018dont}; for model samples, we use the output of GPT-2-medium when prompted with the first 30 tokens of each article in XSum. We use T5-large to apply perturbations, masking out randomly-sampled 2-word spans until 15\% of the words in the article are masked. We approximate the expectation in Eq.~\ref{eq:perturbation-discrepancy} with 100 samples from T5-large. Figure~\ref{fig:perturbation-discrepancy} shows the result of this experiment, using 400 articles from XSum. We find that the distribution of perturbation discrepancies is significantly different for human-written articles and model samples; model samples do tend to have a larger perturbation discrepancy.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/discrepancies.png}
    \vspace{-5mm}
    \caption{Systematic difference in perturbation discrepancy $\pd$ distributions for \textcolor{human-orange}{\textbf{human-written news articles}} and \textcolor{machine-blue}{\textbf{machine-generated articles}} of equal word length from GPT-2-xl, GPT-Neo-1.3B, GPT-Neo-2.7B, and OPT-2.7B. Human-written articles are are sample of 400 XSum \citep{shashi2018dont} articles; machine-generated text generated by prompting each model with the first 30 tokens of each XSum article. Discrepancies are estimated with 100 samples from T5-3B.}
    %%CF.1.18: It would be good to provide the key takeaway from this figure for the reader. What does it mean for the distributions to be different?
    %%CF.1.18: if someone is skimming the paper, I think they probably won't understand what "perturbation discrepancy" means.
    \label{fig:perturbation-discrepancy}
\end{figure}

Given these results, we can detect if a piece of text was generated by a model $\sm$ by simply thresholding the perturbation discrepancy. In practice, we find that normalizing the perturbation discrepancy by the standard deviation of the observed values used to estimate $\mathbb{E}_{\tilde x \sim q(\cdot|x)} \log \sm(\tilde x)$ provides a slightly better signal for detection, typically increasing AUROC by around 0.020, so we use this normalized version of the perturbation discrepancy in our experiments. The resulting method, {\name}, is summarized in Alg.~\ref{alg:method}.

\subsection{Perturbation discrepancy and curvature}

Hypothesis~\ref{hyp:main} can be restated in terms of the local maxima of the log likelihood function $\log \sm$: Model samples tend to lie near local maxima of $\log \sm$, while human-written samples do not. From this perspective, a natural interpretation of the perturbation discrepancy is an approximation of the negative trace of the Hessian of the likelihood function at the candidate passage. To avoid the non-differentiability of discrete data, we consider candidate passages in their continuous embedding (representation) space. In this section, we show that the perturbation discrepancy can indeed be interpreted as a measure of local curvature, approximating a constant scalar multiple of a partial negative sum of the diagonal values of the Hessian of the likelihood function at the candidate passage.

We first invoke Hutchinson's trace estimator \citep{hutchinson1990stochastic}, which gives an unbiased estimate of the trace of a matrix $A$ as
\begin{equation}
    \text{tr}(A) = \mathbb{E}_\mathbf{z} \mathbf{z}^\top A \mathbf{z}
    \label{eq:hutch}
\end{equation}
provided that the elements of $\mathbf{z}$ are IID with $\mathbb{E}[z_i] = 0$ and $\text{Var}(z_i) = 1$. To use Equation~\ref{eq:hutch} to estimate the trace of the Hessian, we must therefore compute the directional second derivative $\mathbf{z}^\top H_f(x)\mathbf{z}$. We approximate this expression with finite differences:
\begin{equation}
    \mathbf{z}^\top H_f(x) \mathbf{z} \approx \frac{f(x + h\mathbf{z}) + f(x - h\mathbf{z}) - 2f(x)}{h^2}
    \label{eq:fd}
\end{equation}
Combining Equations~\ref{eq:hutch} and~\ref{eq:fd} and simplifying with $h=1$, we have an estimate of the negative Hessian trace
\begin{align}
    -\text{tr}(H)_f(x) &= 2f(x) - \mathbb{E}_\mathbf{z} \left[f(x + \mathbf{z}) + f(x - \mathbf{z})\right].
    \label{eq:neg-trace}
\end{align}
If our noise distribution is \textit{symmetric}, that is, $p(\mathbf{z}) = p(-\mathbf{z})$ for all $\mathbf{z}$, then we can simplify Equation~\ref{eq:neg-trace} to
\begin{align}
    \frac{-\text{tr}(H)_f(x)}{2} &= f(x) - \mathbb{E}_\mathbf{z} f(x + \mathbf{z}).
    \label{eq:simplified}
\end{align}
While the RHS of Equation~\ref{eq:simplified} is very similar to the expression for the perturbation discrepancy, we do not perturb $x$ with IID vectors $\mathbf{z}$ as in Hutchinson's estimator. Rather, we use samples from a mask filling model (e.g., T5) to compute $\mathbb{E}_\mathbf{\tilde x}f(\tilde x)$, where $\tilde x = x + \delta$ and $\delta$ is some semantic perturbation in representation space, which is very unlikely to have IID elements. Thus, the perturbation discrepancy is a \textit{weighted} sum of the diagonal elements of the Hessian matrix, where the weights are determined by the distribution of the perturbation distribution. We can interpret this resulting quantity as the trace of the Hessian restricted to the data manifold.

\subsection{Perturbation discrepancy as leveraging model (self-)watermarking}
We can interpret the perturbation function (masking + replacing) as producing \textit{semantically similar rephrasings of the original passage}. If these rephrasings are systematically lower-likelihood than the original passage, the model is exposing its bias toward the specific (and roughly arbitrary, by human standards) phrasing used. In other words, language models that do not perfectly imitate human writing essentially watermark themselves implicitly. Under this interpretation, efforts to \textit{manually} add watermarking biases to model outputs, such as the well-publicized project to do so at OpenAI \citep{aaronson_2022}, may further improve the effectiveness of methods such as {\name}, even as LLMs continue to improve their approximation of human writing.

\section{Experiments}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/pr-roc.png}
    \caption{ROC and precision-recall curves for detecting samples from GPT-Neo-2.7B and GPT-2-xl using T5-3B as the referee/mask-filling model. {\name} provides significantly better performance than likelihood thresholding when averaging over at least 10 perturbations. Further, increasing the number of perturbations used to estimate the expectation in Eq.~\ref{eq:perturbation-discrepancy} provides an effective way to leverage additional compute to improve accuracy of the detector.}
    \label{fig:pr-roc}
\end{figure}

We conduct various experiments in order to better understand the utility of the perturbation discrepancy for {\probfull}, the role of source model scale and choice of perturbation function on difficulty of detecting machine-generated text, and the impact of the sample size used to estimate the perturbation discrepancy on detection performance.

\textbf{Comparison methods.} We compare with two commonly-used methods for {\probfull} that leverage the predicted likelihoods of the source model for detection, corresponding to the two likelihood-based tests from GLTR \citep{gehrmann-etal-2019-gltr}. The first method uses the average token-wise log-likelihood to determine if a candidate passage is machine-generated or not; passages with high average log-likelihood are likely to be machine-generated. The second method uses average observed rank of the tokens in the candidate passage according to the model's conditional distributions. Passages with low average rank are likely to be machine-generated.

\textbf{Datasets.} Our experiments use three datasets intended to mirror practical settings for {\probfull}. We use news articles from the XSum dataset \citep{shashi2018dont} to represent fake news detection, Wikipedia passages from Wikitext to represent cheating in academic essays, and writing prompts from the WritingPrompts dataset \citep{fan-etal-2018-hierarchical} to represent detecting machine-generated creative writing submissions.

\subsection{Evaluating unsupervised methods for {\prob}}

Here we evaluate different detectors across source models $\sm$, datasets, and sampling strategies. As in prior work, we evaluate the most common methods for sampling from LLMs: temperature sampling (sampling from the raw conditional distribution), top-k sampling, and top-p sampling (also called nucleus sampling). We use a temperature of 1, $k=40$, and $p=0.96$, in line with prior work \citep{ippolito-etal-2020-automatic}. Results are presented in Tables~\ref{tab:sampling} and \ref{tab:sampling2} for news articles from the XSum dataset \citep{shashi2018dont} and the stories from the Writing Prompts creative writing dataset \citep{fan-etal-2018-hierarchical}, respectively.


\begin{table}
    \centering
    \small
    \begin{tabular}{llcccccc}
        \toprule
         & & \multicolumn{6}{c}{\textbf{Sampling method}} \\
         \cmidrule(lr){3-8}
         & & \multicolumn{2}{c}{\textbf{Temp. (1)}} & \multicolumn{2}{c}{\textbf{top-k (40)}} & \multicolumn{2}{c}{\textbf{top-p (0.96)}} \\
        \midrule
        \textbf{Alg.} & \textbf{Model} & PR & ROC & PR & ROC & PR & ROC \\
        \midrule
        \multirow{4}{*}{Ours} & GPT-2 & 0.94 & 0.95 & 0.95 & 0.95 & 0.95 & 0.96 \\
         & Neo-2.7 & 0.96 & 0.96 & 0.97 & 0.97 & 0.99 & 0.99 \\
         & OPT-2.7 & 0.90 & 0.90 & 0.91 & 0.92 & 0.93 & 0.94 \\
         & GPT-J & 0.88 & 0.89 & 0.90 & 0.91 & 0.93 & 0.93 \\
         & \textit{Average} & 0.92 & 0.93 & 0.93 & 0.94 & 0.95 & 0.96 \\
        \midrule
        \multirow{4}{*}{$\log p$} & GPT-2 & 0.72 & 0.77 & 0.75 & 0.80 & 0.83 & 0.87 \\
         & Neo-2.7 & 0.75 & 0.77 & 0.80 & 0.82 & 0.87 & 0.89 \\
         & OPT-2.7 & 0.73 & 0.76 & 0.77 & 0.80 & 0.86 & 0.87 \\
         & GPT-J & 0.65 & 0.69 & 0.72 & 0.75 & 0.80 & 0.83 \\
         & \textit{Average} & & & & & & \\
        \midrule
        \multirow{4}{*}{Rank} & GPT-2 & 0.79 & 0.76 & 0.79 & 0.76 & 0.80 & 0.76 \\
         & Neo-2.7 & 0.77 & 0.73 & 0.78 & 0.74 & 0.78 & 0.75 \\
         & OPT-2.7 & 0.75 & 0.73 & 0.76 & 0.73 & 0.77 & 0.75 \\
         & GPT-J & 0.75 & 0.71 & 0.75 & 0.72 & 0.77 & 0.73 \\
         & \textit{Average} & & & & & & \\
        \bottomrule
    \end{tabular}
    \caption{evaluate how sampling heuristic affects detection on \textbf{XSum (news articles)}.}
    \label{tab:sampling}
\end{table}


\subsection{Using likelihoods from models other than the source model}
Double-edged sword; if our test flags any machine-generated text no matter what model it came from, we can't identify the model itself using the test. On the other hand, we do have a more general-purpose fake text detector. Depending on the setting, identifying the specific model (i.e., forensics) may be more important, but in other cases, simply detecting non-human text is more important (but perhaps intractable in general).

\begin{table}
    \centering
    \small
    \begin{tabular}{llcccccc}
        \toprule
         & & \multicolumn{6}{c}{\textbf{Sampling method}} \\
         \cmidrule(lr){3-8}
         & & \multicolumn{2}{c}{\textbf{Temp. (1)}} & \multicolumn{2}{c}{\textbf{top-k (40)}} & \multicolumn{2}{c}{\textbf{top-p (0.96)}} \\
        \midrule
        \textbf{Alg.} & \textbf{Model} & PR & ROC & PR & ROC & PR & ROC \\
        \midrule
        \multirow{4}{*}{Ours} & GPT-2 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 \\
         & Neo-2.7 & 0.94 & 0.94 & 0.94 & 0.94 & 0.96 & 0.96 \\
         & OPT-2.7 & 0.94 & 0.94 & 0.95 & 0.95 & 0.97 & 0.96 \\
         & GPT-J & 0.78 & 0.82 & 0.80 & 0.83 & 0.84 & 0.85 \\
         & \textit{Average} & & & & & & \\
        \midrule
        \multirow{4}{*}{$\log p$} & GPT-2 & 0.84 & 0.87 & 0.88 & 0.91 & 0.93 & 0.94 \\
         & Neo-2.7 & 0.76 & 0.80 & 0.81 & 0.84 & 0.88 & 0.89 \\
         & OPT-2.7 & 0.81 & 0.84 & 0.83 & 0.86 & 0.89 & 0.81 \\
         & GPT-J & 0.66 & 0.71 & 0.71 & 0.76 & 0.80 & 0.83 \\
         & \textit{Average} & & & & & & \\
        \midrule
        \multirow{4}{*}{Rank} & GPT-2 & 0.86 & 0.82 & 0.86 & 0.83 & 0.87 & 0.83 \\
         & Neo-2.7 & 0.82 & 0.77 & 0.83 & 0.78 & 0.82 & 0.77 \\
         & OPT-2.7 & 0.82 & 0.79 & 0.83 & 0.80 & 0.83 & 0.80 \\
         & GPT-J & 0.79 & 0.75 & 0.80 & 0.75 & 0.81 & 0.77 \\
         & \textit{Average} & & & & & & \\
        \bottomrule
    \end{tabular}
    \caption{evaluate how sampling heuristic affects detection on \textbf{SQuAD contexts (Wikipedia articles)}.}
    \label{tab:sampling2}
\end{table}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/scale_plot.pdf}
    \vspace{-5mm}
    \caption{In the case of case of averaging over many perturbations (\textbf{right}), there is a clear trend that the benefits of increasing mask-filling model size saturate more slowly for larger source models. Across source model sizes, increasing mask-filling model size improves detection performance. Curves show AUROC scores on 300 SQuAD context (Wikipedia) passages.}
    \label{fig:model-size}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/cross_plot.pdf}
    \caption{{\name} performs best when scoring samples with the same model that generated them. Each plot uses a different model for evaluating the likelihoods in Equation~\ref{eq:perturbation-discrepancy}. This result suggests that {\name} may be able to identify the \textit{specific model} that generated a particular sample. Results are averaged over XSum, SQuAD, and WritingPrompts; one standard error is shown.}
    \label{fig:cross-eval}
\end{figure}

\subsection{Detecting generations from very large models}

Do GPT-3 experiments here

\subsection{Robustness to manual editing}
See Figure~\ref{fig:pre-perturb}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/pre_perturb_plot.pdf}
    \caption{We simulate manual editing by replacing varying fractions of model samples with T5-generated text (masking out random five word spans until $p$\% of text is masked to simulate a human edits to machine-generated text). Experiment is conducted on the XSum dataset.}
    \label{fig:pre-perturb}
\end{figure}

\subsection{Comparison with supervised detectors}

Our experiments so far focus on unsupervised detection, but several works have evaluated the detection performance of supervised methods, typically fine-tuned transformers, for detecting machine-generated text.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/n_perturb_plot.pdf}
    \vspace{-5mm}
    \caption{Impact of varying the number of perturbations (samples from the mask-filling model) used by {\name} on auROC (\textbf{left}) and auPR (\textbf{right}) to estimate the perturbation discrepancy for classification. Averaging over at least 5 perturbations greatly increases {\name}'s discrimination power.}
    \label{fig:n-perturb}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.32\textwidth]{figures/llr-1.png}
    \includegraphics[width=0.32\textwidth]{figures/llr-10.png}
    \includegraphics[width=0.32\textwidth]{figures/llr-100.png}
    \vspace{-5mm}
    \caption{Plots of estimated perturbation discrepancy (log likelihood ratio) using 1, 10, or 100 samples (left-to-right). Increasing the number of perturbations used to estimate the perturbation discrepancy significantly improves the signal-to-noise ratio of {\name}.}
    \label{fig:llr}
\end{figure*}

\subsection{Impact of source and mask-filling model scale}
Next we consider the impact of the size of the source model and mask-filling model on {\name}'s performnace. In particular, the increased discrimination power of {\name} for larger mask-filling models suggests that {\name} relies on something??? Results in Figure~\ref{fig:model-size}.

\subsection{Impact of number of perturbations for {\name}}
Finally, we demonstrate the benefits of averaging over multiple sampled perturbations to {\name}'s performance. Figure~\ref{fig:n-perturb} shows that performance increases sharply when averaging over even a small number of perturbations, although performance continues to improve even when averaging over 1000 perturbations. The ability to adjust {\name}'s accuracy based on the available time/compute is a useful algorithmic property not shared by existing unsupervised {\prob} methods.


\section{Conclusions}

Detecting machine-generated text is an important problem

Unsupervised methods are ready to go any time a new model becomes available, without collecting data or fine-tuning

{\name} provides far more accurate detection than existing unsupervised {\name} methods.

Future work should explore this method for image generators (using diffusion models as the mask-fillers)!


\section{NOTES}
What does this test actually do? Restricted hessian

Why doesn't replacing human-written text with T5 increase likelihood on average, rather than avg-zero change?

Look at the distribution of fills?

Prioritize training a supervised detector

Note that we only have to do the perturbations once

Check the calibration of the perturbation discrepancy

\bibliography{main}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

In addition to XSum and SQuAD, we also evaluate {\name} on creative writing data from Reddit. Results are presented in Table~\ref{tab:sampling3}.


\begin{table}
    \centering
    \small
    \begin{tabular}{llcccccc}
        \toprule
         & & \multicolumn{6}{c}{\textbf{Sampling method}} \\
         \cmidrule(lr){3-8}
         & & \multicolumn{2}{c}{\textbf{Temp. (1)}} & \multicolumn{2}{c}{\textbf{top-k (40)}} & \multicolumn{2}{c}{\textbf{top-p (0.96)}} \\
        \midrule
        \textbf{Alg.} & \textbf{Model} & PR & ROC & PR & ROC & PR & ROC \\
        \midrule
        \multirow{4}{*}{Ours} & GPT-2 & 0.99 & 0.99 & 0.99 & 0.99 & 0.98 & 0.99 \\
         & Neo-2.7 & 0.97 & 0.97 & 0.97 & 0.97 & 0.98 & 0.98 \\
         & OPT-2.7 & 0.94 & 0.94 & & & 0.93 & 0.94 \\
         & GPT-J & 0.90 & 0.90 & 0.92 & 0.92 & & \\
         & \textit{Average} & & & & & & \\
        \midrule
        \multirow{4}{*}{$\log p$} & GPT-2 & 0.93 & 0.95 & 0.95 & 0.96 & 0.88 & 0.97 \\
         & Neo-2.7 & 0.91 & 0.93 & 0.94 & 0.95 & 0.96 & 0.97 \\
         & OPT-2.7 & 0.82 & 0.85 & & & 0.86 & 0.87 \\
         & GPT-J & 0.88 & 0.90 & 0.91 & 0.93 & & \\
         & \textit{Average} & & & & & & \\
        \midrule
        \multirow{4}{*}{Rank} & GPT-2 & 0.86 & 0.83 & 0.86 & 0.83 & 0.86 & 0.87 \\
         & Neo-2.7 & 0.84 & 0.80 & 0.84 & 0.80 & 0.85 & 0.81 \\
         & OPT-2.7 & 0.83 & 0.80 & & & 0.77 & 0.75 \\
         & GPT-J & 0.82 & 0.79 & 0.83 & 0.79 & & \\
         & \textit{Average} & & & & & & \\
        \bottomrule
    \end{tabular}
    \caption{evaluate how sampling heuristic affects detection on \textbf{Writing Prompts (creative writing)}.}
    \label{tab:sampling3}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
