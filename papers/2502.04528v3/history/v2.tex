%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}
\usepackage{algorithmic}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\com}[1]{{\color{black!35}//\,#1}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submisspion:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{float}
\newfloat{algorithm}{t}{lop}

\usepackage{xcolor}
\definecolor{machine-blue}{rgb}{0.619,0.722,0.827}
\definecolor{human-orange}{rgb}{0.929,0.765,0.584}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\input{commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\papertitle}

\begin{document}

\twocolumn[
\icmltitle{\papertitle}
%%CF.1.18: Please use title case for titles. Things like "NN" aren't good for a title either, so I assume this is a placeholder for now. But it would be good to come up with some good title ideas.
%%EM.1.19:
% Local Likelihood Curvature Improves Zero-shot Detection of Machine-Generated Text
% Zero-shot Machine-Generated Text Detection through Likelihood Curvature Estimation
% DetectGPT: Zero-shot Machine-Generated Text Detection
% DetectGPT: Zero-shot Machine-Generated Text Detection using Local Likelihood Curvature


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Eric Mitchell}{stan}
\icmlauthor{Chelsea Finn}{stan}
\end{icmlauthorlist}

\icmlaffiliation{stan}{Stanford University}

\icmlcorrespondingauthor{Eric Mitchell}{eric.mitchell@cs.stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{chatgpt, cheating, zero-shot, text}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
The remarkable fluency and factual knowledge of large language models (LLMs) heightens the need for corresponding systems to detect whether a piece of text is human or machine-written. For example, students may use LLMs to complete natural language- or code-writing assignments, leaving instructors unable to detect cheating and accurately assess students' abilities. In this paper, we first demonstrate that text sampled from an LLM tends to occupy a local maxima of the model's likelihood function. Leveraging this observation, we define a new curvature-based criterion for judging if a passage is generated from a given LLM. This method, which we call {\name}, does not require training a separate classifier or collecting any new datasets of real or generated passages, using only likelihoods computed by the model of interest. We show that {\name} is far more discriminative than thresholding by model likelihood, the most common zero-shot method for model sample detection, improving machine-generated text detection AUROC by 0.1 absolute even for LLMs with 20B parameters.
\end{abstract}

\section{Introduction}
Large language models (LLMs) have proven able to generate remarkably fluent responses to a wide variety of user queries. Models such as GPT-3 \citep{gpt3}, PaLM \citep{palm}, and ChatGPT \citep{chatgpt} can convincingly answer complex questions about science, mathematics, historical and current events, and social trends. However, the convincing quality of these responses is often spurious, and recent work has found that LLM-generated responses are often simply wrong \citep{lin-etal-2022-truthfulqa}. Nonetheless, the convincing nature of the generations of these models makes them attractive for replacing human labor in a variety of contexts, notably student essay writing and journalism. Such applications of LLMs are problematic for a variety of reasons, making fair student assessment difficult, impairing student learning, and proliferating convincing-but-inaccurate news articles. %While tighter access control may prevent some negative impact of LLMs, the commercial potential for powerful LLMs makes it likely that model providers will be encouraged to make their models \textit{more} accessible, rather than less. 
%%CF.1.18: I think that this statement is debatable, and I don't think we really need to articulate an opinion on this to tell a good story? Could more simply say that we shouldn't rely on access control as the only means, especially since these models are already widely available. Or not even mention access control, since it's pretty clearly not a satisfactory solution.
%%EM.1.24: Attempted to revise by discussing how this is difficult for humans, rather than mentioning access control.
Unfortunately, humans perform only slightly better than chance when classifying machine-generated vs human-written text \citep{gehrmann-etal-2019-gltr}, leading researchers to consider automated detection methods that may identify signals difficult for humans to recognize. Such methods might give teachers and news-readers more confidence in the human origin of the text that they consume. In this paper, we continue recently-growing efforts to address this {\probfull} problem.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/fig1_v12.pdf}
    \caption{We aim to perform automated classification of whether a piece of text was generated by a particular LLM, such as GPT-3. {\name} does so in an \textit{zero-shot} manner, meaning no data collection, training, or fine-tuning is needed to construct the detection algorithm.}
    \label{fig:fig1}
\end{figure}

As in prior work, we study the {\probfull} problem as a binary classification problem \citep{Jawahar2020AutomaticDO}, aiming to classify whether a \textit{candidate passage} was generated by a particular \textit{source model}. While several works have investigated methods for training a second deep network to detect machine-generated text, such an approach has several shortcomings, including a tendency to overfit to the topics it was trained on as well as the need to train a new model for each new source model that is released. We therefore consider the \textit{zero-shot} version of {\probfull}, where we use the source model itself, without fine-tuning or adaptation of any kind, to detect its own samples. The most common method for zero-shot {\probfull} is evaluating the average per-token log-likelihood of the generated text and thresholding \citep{release-strategies,gehrmann-etal-2019-gltr,ippolito-etal-2020-automatic}. However, such a zero-th-order approach to detection fails to leverage the local structure of the likelihood function around a candidate passage, which we find contains additional useful information about the source of a passage.

This paper poses a simple hypothesis: minor rewrites of a \textit{model sample} tend to have lower log likelihood under the model than the original sample, while minor rewrites of human-generated text may have higher or lower likelihood than the original sample (see Figs.~\ref{fig:local-structure} for illustration and Fig.~\ref{fig:perturbation-discrepancy} for empirical evaluation). In other words, unlike human-written text, model-generated text tends to lie in areas where the log likelihood function has negative curvature. We empirically verify this hypothesis, and find that it holds true across a diverse body of LLM's (see Figure~\ref{fig:perturbation-discrepancy}), even when the minor rewrites, or \textit{perturbations}, come from alternative language models. We leverage this observation to build {\name}, a zero-shot method for automated {\probfull}. To test if a passage came from a source model $\sm$, {\name} compares the log likelihood of a candidate passage under $\sm$ with the average log likelihood of several perturbations of the passage under $\sm$ (generated with, e.g., T5; \citet{raffel2020t5}). If the perturbed passages tend to have lower average log likelihood than the original by some margin, the candidate passage is likely to have come from $\sm$. See Figure~\ref{fig:fig1} for an overview of the problem and {\name}. Our experiments find that {\name} is more accurate than existing methods for zero-shot methods for detecting machine-generated text, improving over the strongest zero-shot baseline by over 0.1 AUROC for multiple models when detecting machine-generated news articles.

Our main contributions are:
\begin{enumerate}
    \item The detection and empirical validation of the hypothesis that the curvature of the model likelihood tends to be significantly more negative at model samples than for human text.
    \item {\name}, a practical algorithm inspired by this hypothesis that approximates the trace of the likelihood's Hessian to detect machine-generated text.
\end{enumerate}

\section{Related Work}
Increasingly large LLMs \citep{gpt2,gpt3,palm,chatgpt,opt} have led to dramatically improvemed performance on many language-related benchmarks and the ability to generate convincing and on-topic text. The GROVER model \citep{Zellers2019DefendingAN} was the first LLM trained specifically for generating realistic-looking news articles. Human evaluators found GROVER-generated news more reliable than human-written articles, on average,
%%CF.1.18: I don't think you need all this detail. 
%%EM.1.24: I kind of like the context of previous works that use the LM itself to detect its own generations, but we can cut here if we're running low on space.
motivating the authors to study GROVER's ability to detect its own generations by fine-tuning a detector on top of its features; they found GROVER better able to detect GROVER-generated text than other pre-trained models. However, \citet{uchendu-etal-2020-authorship} find that the fine-tuned GROVER's ability to detect machine-generated text is in large part limited to GROVER-generated text, and its accuracy decreases significantly for generations from other models. \citet{bakhtin2019learning} also similarly find that models trained explicitly for {\probfull} tend to overfit to their training distribution.

Other works have trained supervised models for {\probfull} on top of on neural representations \citep{uchendu-etal-2020-authorship,bakhtin2019learning,release-strategies,tweepfake,ippolito-etal-2020-automatic}, bag-of-words features \citep{release-strategies,tweepfake}, and handcrafted statistical features \citep{gehrmann-etal-2019-gltr}. Alternatively, \citet{release-strategies} notes the surprising efficacy of a simple zero-shot method for {\probfull}, which thresholds a candidate passage based on its total average likelihood under the generative model, serving as a strong baseline for zero-shot {\probfull} in our work. In our work, we similarly use the generating model to detect its own generations in a zero shot manner, but through a completely different approach based on estimating local curvature of the log likelihood rather than only measuring the log likelihood of the candidate passage. See \citet{Jawahar2020AutomaticDO} for a complete survey on {\probfull}.

The problem of {\probfull} echoes earlier work on detecting deepfakes, artificial images or videos generated by deep nets, which has spawned substantial efforts in detection of fake visual content \citep{DFDC2020,zi2020wilddeepfake}. While early works in deepfake detection used relatively general-purpose model architectures \citep{guera2018deepfake}, many deepfake detection methods rely on the continuous nature of image data to achieve state-of-the-art performance \citep{zhao2021multi,Guarnera_2020_CVPR_Workshops}, making direct application to text difficult.
%Also related are works that aim to extract the hyperparameters of the model that generated a given text passage \citep{tay-etal-2020-reverse} or image \citep{asnani2021reverse}.
%%CF.1.18: This last sentence seems weird. Seems like an after thought, and it's not immediately clear why it is related. If the techniques used are relevant enough to be mentioned in the related work, then it warrants more discussion on how our technique is different. 
%% SK.1.23: +1
%% EM.1.24: Inclined to just remove this

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/prop-1-v3.pdf}
    \vspace{-5mm}
    \caption{We identify and exploit the tendency of machine-generated passages $x\sim\sm(\cdot)$ \textbf{(left)} to lie near a local maximum of the model's likelihood function; nearby samples are systematically lower likelihood. In contrast, human-written text $x\sim p_{real}(\cdot)$ \textbf{(right)} tends not to occupy a local extremum of a model's likelihood function; nearby samples may be higher or lower likelihood.}
    \label{fig:local-structure}
\end{figure}

\section{The Zero-Shot {\probfulltitle} Problem}
We study zero-shot {\probfull}, the problem of detecting whether a piece of text, or \textit{candidate passage} $x$ is a sample from a \textit{source model} $\sm$. The problem is zero-shot in the sense that we do not assume access to human-written or source model-generated samples in order to perform detection. As in prior work, we study a `white box' setting \citep{gehrmann-etal-2019-gltr} in which the detector may evaluate the log likelihood of a sample $\log \sm(x)$. The white box setting \textbf{does not} assume access to the model architecture or parameters. We note that the white box assumption is relatively mild, as virtually all public APIs for LLMs provide the ability to score a given input. While most of our experiments consider the white box setting, see Section~\ref{sec:black-box} for a suite of experiments in which we score text using models other than the source model.

The detection criterion we propose, {\name}, also makes use of generic pre-trained mask-filling models in order to generate passages that are `nearby' the candidate passage. However, these mask-filling models are used off-the-shelf, without any fine-tuning or adaptation to the target domain.


\section{{\name}: Zero-shot {\probfulltitle} with Random Perturbations}
\label{sec:local-extremum}

\begin{algorithm}
    \caption{{\name} model-generated text detection}
    \label{alg:method}
    \begin{algorithmic}[1]
    
        \footnotesize \STATE \textbf{Input:} passage $x$, source model $\sm$, perturbation function $q$, number of perturbations $k$, decision threshold $\epsilon$

        \phantom{asdf}\com{\textbf{Return} \texttt{true} if $x\sim\sm$; \texttt{false} otherwise}
        \STATE $\tilde x_i \sim q(\cdot|x),\,i\in [1..k]$ \hfill\com{mask spans, sample replacements}
        \STATE $\tilde \mu \gets \frac{1}{k}\sum_i\log \sm(\tilde x_i)$ \hfill \com{approximate expectation in Eq. 1}
        \STATE $\pdh_x \gets \log \sm(x) - \tilde \mu$ \hfill\com{estimate $\pd$}
        \STATE \scalebox{0.95}{$\tilde \sigma_x^2 \gets \frac{1}{k-1}\sum_i\left(\log \sm(\tilde x_i) - \tilde \mu\right)^2$ \hfill\com{variance for normalization}}
        \IF{$\frac{\pdh_x}{\sqrt{\tilde \sigma_x}} \geq \epsilon$}
            \STATE return \texttt{true} \hfill\com{probably model sample}
        \ELSE
            \STATE return \texttt{false} \hfill\com{probably not model sample}
        \ENDIF
    \end{algorithmic}
\end{algorithm}

{\name} is based on the hypothesis that samples from a source model $\sm$ typically lie in areas of negative curvature of the log likelihood function of $\sm$, unlike human text. In other words, if we apply small perturbations to a passage $x\sim\sm$, producing $\tilde x$, the quantity $\log \sm(x) - \log \sm(\tilde x)$ should be relatively large on average for machine-generated samples compared to human-written text. To leverage this hypothesis, first consider a perturbation function $q(\cdot\mid x)$ that produces slightly modified version of $x$, with similar meaning (we will generally consider roughly paragraph-length texts $x$). As an example, $q(\cdot|x)$ might be the result of simply asking a human to rewrite one of the sentences of $x$, while preserving the meaning of $x$. Using the notion of a perturbation function, we can define the \textit{perturbation discrepancy} $\pd$: 
\begin{equation}
    \mathbf{d}(x) = \log \sm(x) - \mathbb{E}_{\tilde x \sim q(\cdot|x)} \log \sm(\tilde x)
    \label{eq:perturbation-discrepancy}
\end{equation}
We now state our hypothesis more formally in Hypothesis~\ref{hyp:main}.
\begin{hypothesis}
    The perturbation discrepancy $\pd$ is positive with high probability for samples $x\sim\sm$. For human-written text, $\pd$ tends toward zero similarly for all $x$.
\label{hyp:main}
\end{hypothesis}
%%CF.1.19: Yoonho brought up a good point to me (and I think also to you) that it's really important that the perturbation function q gives you samples that are still on the data distribution. I think that hypothesis 4.1 won't hold for q's that consistently take you off the data manifold. I think it is important to convey this constraint on q.

If we define $q(\cdot|x)$ to be samples from a mask-filling model such as T5 \citep{raffel2020t5}, rather than human rewrites, we can empirically test Hypothesis~\ref{hyp:main}. For real data, we use news articles from the XSum dataset \citep{shashi2018dont}; for model samples, we use the output of GPT-2-medium when prompted with the first 30 tokens of each article in XSum. We use T5-large to apply perturbations, masking out randomly-sampled 2-word spans until 15\% of the words in the article are masked. We approximate the expectation in Eq.~\ref{eq:perturbation-discrepancy} with 100 samples from T5-large.\footnote{We later show in Figure~\ref{fig:n-perturb} that varying the number of samples used to estimate the expectation effectively allows for trading off between accuracy and speed.} Figure~\ref{fig:perturbation-discrepancy} shows the result of this experiment, using 400 articles from XSum. We find that the distribution of perturbation discrepancies is significantly different for human-written articles and model samples; model samples do tend to have a larger perturbation discrepancy.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/pd_hist.pdf}
    \vspace{-5mm}
    \caption{The average drop in log likelihood (perturbation discrepancy) after rephrasing a passage is consistently higher for model-generated passages than for human-written passages. Each plot shows the distribution of the perturbation discrepancy $\pd$ for \textcolor{human-orange}{\textbf{human-written news articles}} and \textcolor{machine-blue}{\textbf{machine-generated articles}} for various models; of equal word length from GPT-2 (1.5B), GPT-Neo-2.7B, GPT-J (6B) and GPT-NeoX (20B). Human-written articles are a sample of 500 XSum \citep{shashi2018dont} articles; machine-generated text generated by prompting each model with the first 30 tokens of each XSum article. Discrepancies are estimated with 100 samples from T5-3B.}
    %%CF.1.18: It would be good to provide the key takeaway from this figure for the reader. What does it mean for the distributions to be different?
    %%CF.1.18: if someone is skimming the paper, I think they probably won't understand what "perturbation discrepancy" means. Can you describe the key finding here in more layman's terms?
    %%EM.1.24: Tried to do this, hopefully it's more direct now.
    \label{fig:perturbation-discrepancy}
\end{figure}

Given these results, we can detect if a piece of text was generated by a model $\sm$ by simply thresholding the perturbation discrepancy. In practice, we find that normalizing the perturbation discrepancy by the standard deviation of the observed values used to estimate $\mathbb{E}_{\tilde x \sim q(\cdot|x)} \log \sm(\tilde x)$ provides a slightly better signal for detection, typically increasing AUROC by around 0.020, so we use this normalized version of the perturbation discrepancy in our experiments. The resulting method, {\name}, is summarized in Alg.~\ref{alg:method}. Having described an application of the perturbation discrepancy to {\probfull}, we next provide an interpretation of this quantity.

%%CF.1.19: It would be nice to walk the reader through the algorithm in the main text, including the things it requires like a perturbation function and how things are thresholded.

%%CF.1.19: You *really* need some signposting somewhere. I might actually actually recommend ending the section here (though with a sentence that tells the reader where you are going next) and start a new section about the interpretation of the approach. Or, the alternative would be to start section 4 with some signposting of what's in this section, then have one subsection on the intuition and method, and then one or two subsections on the rest. Right now, I think the current version is a bit of the worst of both worlds in terms of organization.
%%EM.1.24: Added a transition sentence at the end of 4 and the beginning of 4.1 to smooth things out.

\subsection{Interpreting the Perturbation Discrepancy as Curvature}

%%CF.1.19: Following up on the above comment, it's really unclear where you are going now. Section 4 was okay because you led it by saying that the method is based on this hypothesis, so it was clear that you were going to be building up intuition for the method before describing the method. But now, there's no direction.
%%EM.1.24: Added a transition.

While Figure~\ref{fig:perturbation-discrepancy} suggests that the perturbation discrepancy may be useful, it is not immediately obvious what the perturbation discrepancy is measuring. In this section, we show that the perturbation discrepancy can be interpreted as an approximation of the local curvature of the log likelihood function. %Hypothesis~\ref{hyp:main} can be restated in terms of the local maxima of the log likelihood function $\log \sm$: Model samples tend to lie near local maxima of $\log \sm$, while human-written samples do not.
Specifically, we will show that the perturbation discrepancy approximates a scalar multiple of the negative trace of the Hessian of the log likelihood function, evaluated at the candidate passage. To handle the non-differentiability of discrete data, we consider candidate passages in a latent semantic space, where small displacements correspond to valid edits that retain similar meaning to the original. Because our perturbation function (T5) models natural text, we expect our perturbations to roughly capture such meaningful variations of the original passage, rather than arbitrary edits.
%%CF.1.19: You could add something like: "Indeed, we expect the mask-filling model to replace sections of text with semantically similar content."
%In this section, we show that the perturbation discrepancy can indeed be interpreted as a measure of local curvature, approximating a constant scalar multiple of a partial negative sum of the diagonal values of the Hessian of the likelihood function at the candidate passage.
%%CF.1.19: This phrase really isn't easy to parse. It's also not clear why this interpretation is meaningful or interesting. Can you provide some high-level reason for why this interpretation is interesting or useful?
%%EM.1.24: It was redundnant anyway, so I removed it

We first invoke Hutchinson's trace estimator \citep{hutchinson1990stochastic}, which gives an unbiased estimate of the trace of a matrix $A$ as
\begin{equation}
    \text{tr}(A) = \mathbb{E}_\mathbf{z} \mathbf{z}^\top A \mathbf{z}
    \label{eq:hutch}
\end{equation}
provided that the elements of $\mathbf{z}$ are IID with $\mathbb{E}[z_i] = 0$ and $\text{Var}(z_i) = 1$. To use Equation~\ref{eq:hutch} to estimate the trace of the Hessian, we must therefore compute the directional second derivative $\mathbf{z}^\top H_f(x)\mathbf{z}$. We approximate this expression with finite differences:
\begin{equation}
    \mathbf{z}^\top H_f(x) \mathbf{z} \approx \frac{f(x + h\mathbf{z}) + f(x - h\mathbf{z}) - 2f(x)}{h^2}
    \label{eq:fd}
\end{equation}
Combining Equations~\ref{eq:hutch} and~\ref{eq:fd} and simplifying with $h=1$, we have an estimate of the negative Hessian trace
\begin{align}
    -\text{tr}(H)_f(x) &= 2f(x) - \mathbb{E}_\mathbf{z} \left[f(x + \mathbf{z}) + f(x - \mathbf{z})\right].
    \label{eq:neg-trace}
\end{align}
If our noise distribution is \textit{symmetric}, that is, $p(\mathbf{z}) = p(-\mathbf{z})$ for all $\mathbf{z}$, then we can simplify Equation~\ref{eq:neg-trace} to
\begin{align}
    \frac{-\text{tr}(H)_f(x)}{2} &= f(x) - \mathbb{E}_\mathbf{z} f(x + \mathbf{z}).
    \label{eq:simplified}
\end{align}
While the RHS of Equation~\ref{eq:simplified} is very similar to the expression for the perturbation discrepancy, we do not perturb $x$ with IID vectors $\mathbf{z}$ as in Hutchinson's estimator. Rather, we use samples from a mask filling model (e.g., T5) to compute $\mathbb{E}_\mathbf{\tilde x}f(\tilde x)$, where $\tilde x = x + \delta$ and $\delta$ is some semantic perturbation in representation space, which is very unlikely to have IID elements. Thus, the perturbation discrepancy is a \textit{weighted} sum of the diagonal elements of the Hessian matrix, where the weights are determined by the distribution of the perturbation distribution. We can interpret this resulting quantity as the trace of the Hessian restricted to the data manifold.

\section{Experiments}

% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{figures/pr-roc.png}
%     \vspace{-5mm}
%     \caption{ROC and precision-recall curves for detecting samples from GPT-Neo-2.7B and GPT-2-xl using T5-3B as the referee/mask-filling model. {\name} provides significantly better performance than likelihood thresholding when averaging over at least 10 perturbations. Further, increasing the number of perturbations used to estimate the expectation in Eq.~\ref{eq:perturbation-discrepancy} provides an effective way to leverage additional compute to improve accuracy of the detector.}
%     \label{fig:pr-roc}
% \end{figure}

\addtolength{\tabcolsep}{-0.2em}
\begin{table*}
    \centering
    \small
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccccccccccc}
        \toprule
         & \multicolumn{5}{c}{\textbf{XSum}} & \multicolumn{5}{c}{\textbf{SQuAD}} & \multicolumn{5}{c}{\textbf{WritingPrompts}} & \textbf{Avg.} \\
         \cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16}\cmidrule(lr){17-17}
         \textbf{Method} & GPT-2 & OPT-2.7 & Neo-2.7 & GPT-J & NeoX & GPT-2 & OPT-2.7 & Neo-2.7 & GPT-J & NeoX & GPT-2 & OPT-2.7 & Neo-2.7 & GPT-J & NeoX & \textit{-} \\
         \midrule
         $\log p(x)$ & 0.86 & 0.86 & 0.86 & 0.82 & 0.77 & 0.91 & 0.88 & 0.84 & 0.78 & 0.71 & 0.97 & 0.95 & 0.95 & 0.94 & \phantom{*}0.93* & 0.87 \\
        Rank & 0.79 & 0.76 & 0.77 & 0.75 & 0.73 & 0.83 & 0.82 & 0.80 & 0.79 & 0.74 & 0.87 & 0.83 & 0.82 & 0.83 & 0.81 & 0.80 \\
        Log Rank & \phantom{*}0.89* & \phantom{*}0.88* & \phantom{*}0.90* & \phantom{*}0.86* & \phantom{*}0.81* & \phantom{*}0.94* & \phantom{*}0.92* & \phantom{*}0.90* & \phantom{*}0.83* & \phantom{*}0.76* & \phantom{*}0.98* & \phantom{*}0.96* & \phantom{*}0.97* & \phantom{*}0.96* & \textbf{0.95} & 0.90 \\
        Entropy & 0.60 & 0.50 & 0.58 & 0.58 & 0.61 & 0.58 & 0.53 & 0.58 & 0.58 & 0.59 & 0.37 & 0.42 & 0.34 & 0.36 & 0.39 & 0.51 \\
        {\name} & \textbf{0.99} & \textbf{0.97} & \textbf{0.99} & \textbf{0.97} & \textbf{0.95} & \textbf{0.99} & \textbf{0.97} & \textbf{0.97} & \textbf{0.90} & \textbf{0.79} & \textbf{0.99} & \textbf{0.99} & \textbf{0.99} & \textbf{0.97} & \phantom{*}0.93* & \textbf{0.96} \\
        \midrule
        Diff & 0.1 & 0.09 & 0.09 & 0.11 & 0.14 & 0.05 & 0.05 & 0.07 & 0.07 & 0.03 & 0.01 & 0.03 & 0.02 & 0.01 & -0.02 & 0.06 \\
         \bottomrule
    \end{tabular}}
    \caption{AUROC for detecting samples from the given model on the given dataset for {\name} and four baseline criteria from prior work. From 1.5B parameter GPT-2 to 20B parameter GPT-NeoX, {\name} consistently provides the most accurate detections. \textbf{Bold} shows the best AUROC within each column (model-dataset combination); asterisk (*) denotes the second-best AUROC.}
    \label{tab:main-results}
\end{table*}
\addtolength{\tabcolsep}{0.2em}

We conduct comprehensive experiments to better understand various facets of {\probfull}; we study the effectiveness of {\name} for zero-shot {\probfull}, the impact of distribution shift on zero-shot and supervised detectors, detection accuracy for the largest publicly-available models, robustness of zero-shot methods to manually-edited machine-generated text, the impact of alternative decoding strategies on detection accuracy, a black-box variant of the detection task, the role of source model scale and choice of perturbation function on difficulty of detecting machine-generated text, and the impact of the sample size used to estimate the perturbation discrepancy on detection performance.

\textbf{Comparison methods.} We compare with various existing zero-shot methods for {\probfull} that leverage the predicted token-wise conditional distributions of the source model for detection, corresponding to the three tests (likelihood, rank, and entropy) from GLTR \citep{gehrmann-etal-2019-gltr} and elsewhere (e.g., \citet{release-strategies} for average log likelihood testing). The first method uses the average token-wise log-likelihood to determine if a candidate passage is machine-generated or not; passages with high average log-likelihood are likely to be machine-generated. The second and third methods use the average observed rank (or observed log-rank) of the tokens in the candidate passage according to the model's conditional distributions. Passages with low average (log-)rank are likely to be machine-generated. Finally, we evaluate an entropy-based approach inspired by the hypothesis in \citet{gehrmann-etal-2019-gltr} that model-generated texts will be more `in-distribution' for the model, leading to it producing more over-confident (and thus low entropy) distributions. However, empirically, we find predictive entropy to be \textit{positively} correlated with passage fake-ness; therefore, this baseline uses high average entropy in the model's predictive distribution as a signal that a passage is machine-generated. While our main focus is zero-shot detection due to its low overhead and ability to generalize, for completeness we perform comparisons supervised detection models in Sections~\ref{sec:supervised} and~\ref{sec:gpt-3}, using two RoBERTa-based LLMs that were fine-tuned on millions of model samples for the detection task.

\textbf{Datasets.} Our experiments use six datasets that cover a variety of domains and LLM use-cases. To represent everyday use-cases, we use news articles from the XSum dataset \citep{shashi2018dont} to represent fake news detection, Wikipedia paragraphs from SQuAD contexts \citep{rajpurkar-etal-2016-squad} to represent cheating in academic essays, and prompted stories from the Reddit WritingPrompts dataset \citep{fan-etal-2018-hierarchical} to represent detecting machine-generated creative writing submissions. To perform targeted evaluations of robustness to distribution shift, we also use the English and German splits of WMT16 \citep{bojar2016wmt} as well as long-form answers written by human experts in the PubMedQA dataset \citep{jin-etal-2019-pubmedqa}.

\subsection{Comparing {\name} with existing methods for zero-shot {\probfull}}
\label{sec:main-results}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/cross_plot.pdf}
    \vspace{-5mm}
    \caption{{\name} performs best when scoring samples with the same model that generated them. Each plot uses a different model for evaluating the likelihoods in Equation~\ref{eq:perturbation-discrepancy}. This result suggests that {\name} may be able to identify the \textit{specific model} that generated a particular sample. Results are averaged over XSum, SQuAD, and WritingPrompts; one standard error is shown.}
    \label{fig:cross-eval}
\end{figure}

Here we evaluate different detectors across source models $\sm$ and datasets. Results are presented in Table~\ref{tab:main-results} for news articles from the XSum dataset \citep{shashi2018dont}, Wikipedia paragraphs used as context information in the SQuAD dataset \citep{rajpurkar-etal-2016-squad}, and stories from the Writing Prompts creative writing dataset \citep{fan-etal-2018-hierarchical}. {\name} improves average detection accuracy most significantly for XSum stories and SQuAD contexts; while it performs accurate detection for WritingPrompts, the performance of all methods tends to increase, and the average margin of improvement is significantly narrower. The overall ease of detecting machine-generated fake writing corroborates anecdotal reporting that machine-generated creative writing tends to be noticeably generic, and therefore relatively easy to detect \citep{roose2022chatgpt}.

\subsection{Comparison with supervised detectors}
\label{sec:supervised}

Our experiments have generally focused on zero-shot detection, but some works have evaluated the detection performance of supervised methods, typically fine-tuned transformers, for detecting machine-generated text. In this section, we find that supervised detectors can provide similar detection performance to {\name} on \textit{in-distribution} (i.e., English) data, but fail to provide meaningful detections on out-of-distribution (i.e., German or Russian) data. This finding echoes past work showing that language models trained for machine-generated text detection overfit to their training data (source model, decoding strategy, topic, language, etc.; \citet{uchendu-etal-2020-authorship,ippolito-etal-2020-automatic,Jawahar2020AutomaticDO}). In contrast, zero-shot methods generalize relatively easily to new languages, and {\name}'s performance is mostly unaffected by the change in language. The results are presented in Figure~\ref{fig:supervised}, evaluating with 200 samples from the WMT16 \citep{bojar2016wmt} machine translation dataset in English, German, and Russian. Specifically, we compare the performance of {\name} with OpenAI's GPT-2 detectors,\footnote{\href{https://github.com/openai/gpt-2-output-dataset/tree/master/detector}{\texttt{https://github.com/openai/gpt-2-output-\\dataset/tree/master/detector}}} which are RoBERTa \citep{liu2019roberta} models fine-tuned with millions of samples from various GPT-2 model sizes and decoding strategies.

\subsection{Detecting Generations from GPT-3}
\label{sec:gpt-3}
\begin{table}
\small
    \centering
    \begin{tabular}{lcccc}
        \toprule
         & \textbf{PubMedQA} & \textbf{XSum} & \textbf{WritingP} & \textbf{Avg.}  \\
         \midrule
         RoBERTa-base & 0.64 & \textbf{0.92} & \textbf{0.92} & 0.83 \\
         RoBERTa-large & 0.71 & \textbf{0.92} & 0.91 & \textbf{0.85} \\
         \midrule
         $\log p(x)$ & 0.64 & 0.76 & 0.88 & 0.76 \\
         \name & \textbf{0.84} & 0.84 & 0.87 & \textbf{0.85} \\
         \bottomrule
    \end{tabular}
    \caption{Detection ROC for GPT-3 generations on three domains: medical question-answering, news generation, and creative writing. For more `typical' text, such as news articles, supervised methods perform strongly.}
    \label{tab:gpt-3-results}
\end{table}

In this section, we evaluate multiple zero-shot and supervised methods on GPT-3, one of the largest publicly-available language models. We sample 150 examples\footnote{We reduce the number of evaluation samples from 500 in our main experiments to reduce the API costs of these experiments.} from the PubMedQA, XSum, and WritingPrompts datasets and compare the two pre-trained RoBERTa-based detector models with {\name} and the likelihood thresholding baseline. We do not include the rank and entropy baselines because the GPT-3 API does not provide access to the complete conditional distribution for each token. We find that {\name} can provide detection competitive with the stronger supervised model, and it again significantly outperforms the likelihood thresholding baseline.

\subsection{Robustness to Manually-Edited Machine-Generated Text}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/pre_perturb_plot.pdf}
    \vspace{-5mm}
    \caption{We simulate manual editing by replacing varying fractions of model samples with T5-generated text (masking out random five word spans until $p$\% of text is masked to simulate a human edits to machine-generated text). Experiment is conducted on the XSum dataset.}
    \label{fig:pre-perturb}
\end{figure}
In practice, humans may manually edit or refine machine-generated text rather than blindly use a model's generations for their task of interest. We therefore conduct an experiment to simulate the detection problem for model samples that have been increasingly heavily revised, simulating human revision with replacing 5 word spans of the text with samples from T5-3B until $x$\% of the text has been replaced. Figure~\ref{fig:pre-perturb} shows that {\name} maintains detection AUROC above 0.75 even when 16\% of the text in model samples has been replaced. Unsuprisingly, almost all methods show a gradual degradation in performance as the sample is more heavily revised; however, the entropy baseline shows surprisingly robust performance in this setting, showing consistent (and even slightly improving) detection performance up to 24\% replacement. {\name} shows the strongest performance when 16\% or less of the sampled text is revised.


\subsection{Impact of Alternative Decoding Strategies on detection}
While Table~\ref{tab:main-results} suggests that {\name} is effective for detecting machine-generated text, prior work notes that the decoding strategy (i.e., temperature sampling, top-$k$, nucleus/top-$p$) can impact the difficulty of detection to a non-trivial extent. In this subsection, we repeat the analysis from Section~\ref{sec:main-results}, using top-$k$ sampling and nucleus sampling. Top-$k$ sampling truncates the sampling distribution to only the $k$ highest-likelihood next tokens; nucleus sampling samples from only the smallest set of tokens whose combined likelihood exceeds $p$. The results are presented in Tables~\ref{tab:main-results-topp} and~\ref{tab:main-results-topk}, respectively. We use $k=40$, and $p=0.96$, in line with prior work \citep{ippolito-etal-2020-automatic}. We find that both top-$k$ and nucleus sampling make detection easier, on average. when averaging across domains, {\name} still provides a more discriminative signal for detection than other unsupervised criteria.


\subsection{Using likelihoods from models other than the source model}
\label{sec:black-box}

While our experiments have focused on the white-box setting for {\probfull}, in this section, we explore the effect of using a different model to \textit{score} a candidate passage (and perturbed texts) than the model that generated the passage. In other words, we aim to classify between human-generated text and text from model $A$, but without access to model $A$ to compute log likelihoods. Instead, we use log likelihoods computed by a surrogate model $B$. We consider three models, GPT-J, GPT-Neo-2.7, and GPT-2, and evaluate all possible combinations of source model and surrogate model (9 total). We average the performance across XSum, SQuAD, and WritingPrompts. The results are presented in Figure~\ref{fig:cross-eval}, showing that when the surrogate model is different from the source model, detection performance is reduced. However, we also notice that using 


\addtolength{\tabcolsep}{-0.2em}
\begin{table*}
    \centering
    \small
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccccccccccc}
        \toprule
         & \multicolumn{5}{c}{\textbf{XSum}} & \multicolumn{5}{c}{\textbf{SQuAD}} & \multicolumn{5}{c}{\textbf{WritingPrompts}} & \textbf{Avg.} \\
         \cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16}\cmidrule(lr){17-17}
         \textbf{Method} & GPT-2 & OPT-2.7 & Neo-2.7 & GPT-J & NeoX & GPT-2 & OPT-2.7 & Neo-2.7 & GPT-J & NeoX & GPT-2 & OPT-2.7 & Neo-2.7 & GPT-J & NeoX & \textit{-} \\
         \midrule
         $\log p(x)$ & 0.87 & 0.86 & 0.89 & 0.82 & 0.76 & 0.94 & 0.91 & 0.88 & 0.82 & 0.74 & 0.98 & 0.93 & 0.97 & \phantom{*}0.95* & \phantom{*}0.95* & 0.88 \\
        Rank & 0.76 & 0.73 & 0.74 & 0.72 & 0.69 & 0.83 & 0.79 & 0.77 & 0.77 & 0.72 & 0.86 & 0.82 & 0.81 & 0.81 & 0.79 & 0.77 \\
        Log Rank & \phantom{*}0.90* & \phantom{*}0.89* & \phantom{*}0.92* & \phantom{*}0.86* & \phantom{*}0.80* & \phantom{*}0.97* & \phantom{*}0.94* & \phantom{*}0.91* & \textbf{0.87} & \textbf{0.77} & \textbf{0.99} & \phantom{*}0.94* & \textbf{0.98} & \textbf{0.96} & \textbf{0.96} & 0.91 \\
        Entropy & 0.61 & 0.55 & 0.58 & 0.62 & 0.67 & 0.55 & 0.52 & 0.57 & 0.58 & 0.59 & 0.36 & 0.38 & 0.33 & 0.35 & 0.36 & 0.51 \\
        {\name} & \textbf{0.98} & \textbf{0.93} & \textbf{0.97} & \textbf{0.94} & \textbf{0.89} & \textbf{0.99} & \textbf{0.97} & \textbf{0.96} & \textbf{0.87} & \phantom{*}0.76* & \textbf{0.99} & \textbf{0.96} & \textbf{0.98} & 0.94 & 0.92 & \textbf{0.94} \\
        \midrule
        Diff & 0.08 & 0.04 & 0.05 & 0.08 & 0.09 & 0.02 & 0.03 & 0.05 & 0.0 & -0.01 & 0.0 & 0.02 & 0.0 & -0.02 & -0.04 & 0.03 \\
         \bottomrule
    \end{tabular}}
    \caption{Nucleus (top-$p$) sampling evaluation with $p=0.96$. AUROC for detecting samples from the given model on the given dataset for {\name} and four baseline criteria from prior work. Nucleus sampling uniformly makes detection easier for all methods, but {\name} generally provides the strongest detections. For WritingPrompts, however, the Log Rank baseline performs as well or better. \textbf{Bold} shows the best AUROC within each column (model-dataset combination); asterisk (*) denotes the second-best AUROC.}
    \label{tab:main-results-topp}
\end{table*}
\addtolength{\tabcolsep}{0.2em}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/supervised_plot.pdf}
    \vspace{-5mm}
    \caption{Supervised machine-generated text detection models trained on large datasets of real and generated texts perform as well as or better than {\name} on \textbf{in-distribution (top row)} text. However, zero-shot methods work out-of-the-box for \textbf{new domains (bottom row)} such as PubMed medical texts and German news data from WMT16. For these domains, supervised detectors fail due to excessive distribution shift.}
    \label{fig:supervised}
\end{figure}

% Double-edged sword; if our test flags any machine-generated text no matter what model it came from, we can't identify the model itself using the test. On the other hand, we do have a more general-purpose fake text detector. Depending on the setting, detecting the specific model (i.e., forensics) may be more important, but in other cases, simply detecting non-human text is more important (but perhaps intractable in general).


\subsection{Impact of Source and Mask-Filling Model Scale}

Next we consider the impact of the size of the source model and mask-filling model on {\name}'s performnace. In particular, the increased discrimination power of {\name} for larger mask-filling models supports the interpretation that {\name} is estimating the curvature of the likelihood in a latent semantic space, rather than in raw token embedding space. Larger T5 models better represent this latent space, in which random directions correspond to meaningful changes in the text along high-likelihood axes of variation.

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/scale_plot.pdf}
    \vspace{-5mm}
    \caption{In the case of averaging over many perturbations (\textbf{right}), there is a clear trend that the benefits of increasing mask-filling model size saturate more slowly for larger source models. Across source model sizes, increasing mask-filling model size improves detection performance. Curves show AUROC scores on 300 SQuAD contexts (Wikipedia passages).}
    \label{fig:model-size}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/n_perturb_plot.pdf}
    \vspace{-5mm}
    \caption{Impact of varying the number of perturbations (samples from the mask-filling model) used by {\name} on auROC (\textbf{left}) and auPR (\textbf{right}) to estimate the perturbation discrepancy for classification. Averaging over at least 5 perturbations greatly increases {\name}'s discrimination power.}
    \label{fig:n-perturb}
\end{figure}

\addtolength{\tabcolsep}{-0.2em}
\begin{table*}
    \centering
    \small
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccccccccccc}
        \toprule
         & \multicolumn{5}{c}{\textbf{XSum}} & \multicolumn{5}{c}{\textbf{SQuAD}} & \multicolumn{5}{c}{\textbf{WritingPrompts}} & \textbf{Avg.} \\
         \cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16}\cmidrule(lr){17-17}
         \textbf{Method} & GPT-2 & OPT-2.7 & Neo-2.7 & GPT-J & NeoX & GPT-2 & OPT-2.7 & Neo-2.7 & GPT-J & NeoX & GPT-2 & OPT-2.7 & Neo-2.7 & GPT-J & NeoX & \textit{-} \\
         \midrule
         $\log p(x)$ & 0.89 & 0.79 & 0.91 & 0.73 & 0.81 & 0.93 & 0.90 & 0.88 & 0.82 & 0.74 & \phantom{*}0.97* & 0.96 & \phantom{*}0.96* & \phantom{*}0.95* & 0.95 & 0.88 \\
        Rank & 0.79 & 0.73 & 0.77 & 0.72 & 0.73 & 0.84 & 0.82 & 0.80 & 0.80 & 0.75 & 0.87 & 0.84 & 0.82 & 0.83 & 0.82 & 0.80 \\
        Log Rank & \phantom{*}0.93* & \phantom{*}0.84* & \phantom{*}0.94* & \phantom{*}0.80* & \phantom{*}0.85* & \phantom{*}0.97* & \phantom{*}0.94* & \phantom{*}0.93* & \phantom{*}0.87* & \phantom{*}0.79* & \textbf{0.98} & \phantom{*}0.97* & \textbf{0.98} & \textbf{0.97} & \textbf{0.97} & 0.92 \\
        Entropy & 0.55 & 0.58 & 0.55 & 0.65 & 0.59 & 0.55 & 0.52 & 0.55 & 0.56 & 0.58 & 0.36 & 0.46 & 0.35 & 0.37 & 0.38 & 0.51 \\
        {\name} & \textbf{1.00} & \textbf{0.91} & \textbf{1.00} & \textbf{0.92} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{0.99} & \textbf{0.96} & \textbf{0.91} & 0.95 & \textbf{0.98} & 0.94 & 0.94 & \textbf{0.97} & \textbf{0.96} \\
        \midrule
        Diff & 0.07 & 0.07 & 0.06 & 0.12 & 0.15 & 0.03 & 0.06 & 0.06 & 0.09 & 0.12 & -0.03 & 0.01 & -0.04 & -0.03 & 0.0 & 0.04 \\
         \bottomrule
    \end{tabular}}
    \caption{Top-$k$ sampling evaluation with $k=40$. AUROC for detecting samples from the given model on the given dataset for {\name} and four baseline criteria from prior work. Nucleus sampling uniformly makes detection easier for all methods, but {\name} generally provides the strongest detections. For WritingPrompts, however, the Log Rank baseline performs as well or better. \textbf{Bold} shows the best AUROC within each column (model-dataset combination); asterisk (*) denotes the second-best AUROC.}
    \label{tab:main-results-topk}
\end{table*}
\addtolength{\tabcolsep}{0.2em}

\subsection{Impact of Number of Perturbations for {\name}}
This is a stub; more perturbations are better, up to about 100. Results are in Figure~\ref{fig:n-perturb}. Experiment uses GPT-2 and GPT-J on XSum, SQuAD, and WritingPrompts.

\section{Discussion}

As large language models continue to improve, they will become increasingly attractive tools for replacing human writers in a variety of contexts, such as education, journalism, and art. While legitimate uses of language model technologies exist in all of these settings, teachers, readers, and consumers are likely to demand tools for verifying the human origin of certain content with high educational, societal, or artistic significance, particularly when factuality (and not just fluency) is crucial.

In light of these elevated stakes and the regular emergence of new large language models, we study the \textit{zero-shot machine-generated text detection} problem, in which we use only the raw likelihoods computed by a generative model to determine if a candidate passage was sampled from it. We identify a property of the likelihood function computed by a wide variety of large language models, showing that a tractable approximation to the trace of the Hessian of the model's likelihood function provides a useful signal for detecting model samples. Our experiments find that this signal is more discriminative than existing zero-shot detection methods, and can, in some domains, be competitive with bespoke detection models trained with millions of model samples.

Finally, we demonstrate the benefits of averaging over multiple sampled perturbations to {\name}'s performance. Figure~\ref{fig:n-perturb} shows that performance increases sharply when averaging over even a small number of perturbations, although performance continues to improve even when averaging over 1000 perturbations. The ability to adjust {\name}'s accuracy based on the available time/compute is a useful algorithmic property not shared by existing zero-shot {\probfull} methods.

\paragraph{{\name} and Watermarking}
One interpretation of the perturbation function (masking + replacing) as producing \textit{semantically similar rephrasings of the original passage}. If these rephrasings are systematically lower-likelihood than the original passage, the model is exposing its bias toward the specific (and roughly arbitrary, by human standards) phrasing used. In other words, language models that do not perfectly imitate human writing essentially watermark themselves implicitly. Under this interpretation, efforts to \textit{manually} add watermarking biases to model outputs \citep{aaronson_2022} may further improve the effectiveness of methods such as {\name}, even as LLMs continue to improve their approximation of human writing. 

\paragraph{Limitations}
One limitation of likelihood-based methods for zero-shot {\probfull} (like {\name}) is the white-box assumption that we can evaluate log likelihoods of the model(s) in question. However, computing likelihoods is possible for virtually all publicly-available LLMs. Another assumption of {\name} is access to a reasonable perturbation function. While in this work, we use off-the-shelf mask-filling models such as T5 and mT5 (for non-English languages), some domains may see reduced performance if existing mask-filling models do not well represent the space of meaningful rephrases, reducing the quality of the curvature estimate.

\paragraph{Future Work}
While the methods in this work make no assumptions about the models generating the samples, future work may explore developing watermarking algorithms in conjunction with detection algorithms, in order to improve detection robustness as language models continually improve their reproductions of human text. Another topic that remains unexplored is the relationship between prompting and detection; that is, can a clever prompt successfully prevent a model's generations from being detected by existing methods? Further, future work may explore whether the local likelihood curvature property we identify is present for image generation models, which may be useful for detecting deepfake images.


% Detecting machine-generated text is an important problem

% Zero-shot methods are ready to go any time a new model becomes available, without collecting data or fine-tuning

% {\name} provides far more accurate detection than existing zero-shot {\name} methods.

% Future work should explore this method for image generators (using diffusion models as the mask-fillers)!

% Manual watermarking can probably further improve the reliability of {\name}.

% \section{NOTES}
% Why doesn't replacing human-written text with T5 increase likelihood on average, rather than avg-zero change?

% Look at the distribution of fills?

% Note that we only have to do the perturbations once

% Check the calibration of the perturbation discrepancy

\bibliography{main}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}




% In addition to XSum and SQuAD, we also evaluate {\name} on creative writing data from Reddit. Results are presented in Table~\ref{tab:sampling3}.


% \begin{table}
%     \centering
%     \small
%     \begin{tabular}{llcccccc}
%         \toprule
%          & & \multicolumn{6}{c}{\textbf{Sampling method}} \\
%          \cmidrule(lr){3-8}
%          & & \multicolumn{2}{c}{\textbf{Temp. (1)}} & \multicolumn{2}{c}{\textbf{top-k (40)}} & \multicolumn{2}{c}{\textbf{top-p (0.96)}} \\
%         \midrule
%         \textbf{Alg.} & \textbf{Model} & PR & ROC & PR & ROC & PR & ROC \\
%         \midrule
%         \multirow{4}{*}{Ours} & GPT-2 & 0.99 & 0.99 & 0.99 & 0.99 & 0.98 & 0.99 \\
%          & Neo-2.7 & 0.97 & 0.97 & 0.97 & 0.97 & 0.98 & 0.98 \\
%          & OPT-2.7 & 0.94 & 0.94 & & & 0.93 & 0.94 \\
%          & GPT-J & 0.90 & 0.90 & 0.92 & 0.92 & & \\
%          & \textit{Average} & & & & & & \\
%         \midrule
%         \multirow{4}{*}{$\log p$} & GPT-2 & 0.93 & 0.95 & 0.95 & 0.96 & 0.88 & 0.97 \\
%          & Neo-2.7 & 0.91 & 0.93 & 0.94 & 0.95 & 0.96 & 0.97 \\
%          & OPT-2.7 & 0.82 & 0.85 & & & 0.86 & 0.87 \\
%          & GPT-J & 0.88 & 0.90 & 0.91 & 0.93 & & \\
%          & \textit{Average} & & & & & & \\
%         \midrule
%         \multirow{4}{*}{Rank} & GPT-2 & 0.86 & 0.83 & 0.86 & 0.83 & 0.86 & 0.87 \\
%          & Neo-2.7 & 0.84 & 0.80 & 0.84 & 0.80 & 0.85 & 0.81 \\
%          & OPT-2.7 & 0.83 & 0.80 & & & 0.77 & 0.75 \\
%          & GPT-J & 0.82 & 0.79 & 0.83 & 0.79 & & \\
%          & \textit{Average} & & & & & & \\
%         \bottomrule
%     \end{tabular}
%     \caption{evaluate how sampling heuristic affects detection on \textbf{Writing Prompts (creative writing)}.}
%     \label{tab:sampling3}
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
