%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
% \documentclass[nohyperref]{article}
\usepackage{fancyhdr}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{subcaption}
\usepackage[justification=centering]{caption}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
% \usepackage{hyperref}
\usepackage[noend]{algorithmic}
\usepackage{soul}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\com}[1]{{\color{black!35}//\,#1}}



% If accepted, instead use the following line for the camera-ready submisspion:
%\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage{tabularx}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{subfig}  % Modern package for subfigures


\newfloat{algorithm}{t}{lop}

\usepackage{xcolor}
\definecolor{human-blue}{rgb}{0.619,0.722,0.827}
\definecolor{machine-orange}{rgb}{0.929,0.765,0.584}

\NewDocumentCommand{\yi}
{ mO{} }{\textcolor{blue}{\textsuperscript{\textit{May}}\textsf{\textbf{\small[#1]}}}}


% % if you use cleveref..
% \usepackage[capitalize,noabbrev]{cleveref}

\newcommand{\PY}[1]{{\color{blue}PY: #1}}
\newcommand{\PYB}[1]{{\color{blue}[PY: #1]}}


\input{commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2023}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\papertitleoneline}

\begin{document}

\twocolumn[
\icmltitle{\papertitle}
%%CF.1.18: Please use title case for titles. Things like "NN" aren't good for a title either, so I assume this is a placeholder for now. But it would be good to come up with some good title ideas.
%%EM.1.19:
% Local Likelihood Curvature Improves Zero-shot Detection of Machine-Generated Text
% Zero-shot Machine-Generated Text Detection through Likelihood Curvature Estimation
% DetectGPT: Zero-shot Machine-Generated Text Detection
% DetectGPT: Zero-shot Machine-Generated Text Detection using Local Likelihood Curvature


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Minseok Jung}{mit}
\icmlauthor{Cynthia Fuertes Panizo}{paristech}
\icmlauthor{Liam Dugan}{upenn}
\icmlauthor{Yi R. Fung}{mit}
\icmlauthor{Pin-Yu Chen}{ibm}
\icmlauthor{Paul Pu Liang}{mit}
\end{icmlauthorlist}

\icmlaffiliation{mit}{Massachusetts Institute of Technology, USA}
\icmlaffiliation{upenn}{University of Pennsylvania, USA}
\icmlaffiliation{paristech}{Institut Polytechnique de Paris}
\icmlaffiliation{ibm}{IBM Research, USA}

\icmlcorrespondingauthor{Minseok Jung}{msjung@mit.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{detection, robustness, fairness, threshold optimization}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\vspace{0mm}
The advancement of large language models (LLMs) has made it difficult to differentiate human-written text from AI-generated text. Several AI-text detectors have been developed in response, which typically utilize a fixed global threshold (e.g., $\theta = 0.5$) to classify machine-generated text. However, we find that one universal threshold can fail to account for subgroup-specific distributional variations. For example, when using a fixed threshold, detectors make more false positive errors on shorter human-written text than longer, and more positive classifications on neurotic writing styles than open among long text. These discrepancies can lead to misclassification that disproportionately affects certain groups. We address this critical limitation by introducing \emph{\name}, an algorithm for group-specific threshold optimization in AI-generated content classifiers. Our approach partitions data into subgroups based on attributes (e.g., text length and writing style) and learns decision thresholds for each group, which enables careful balancing of performance and fairness metrics within each subgroup. In experiments with four AI text classifiers on three datasets, \name\ enhances overall F1 score and decreases balanced error rate (BER) discrepancy across subgroups. Our framework paves the way for more robust and fair classification criteria in AI-generated output detection. We release our data, code, and project information at URL.


% \href{link}{\texttt{URL}}

% should we add the link in the submission?
% url : https://github.com/masonjung/threshold_optimization


%  Ensuring accurate and fair detection of AI-generated text is essential across diverse domains like journalism, academia, and content moderation.

% Basic introduction \\
% more detailed background \\
% general problem \\
% here we show \\
% explanation about the main result \\
% general context \\
% broader perspective

\end{abstract}

\vspace{-6mm}
\section{Introduction}

% all we are using LLM
%  we need AI text classification <- why do we need it + what people are doing

The large-scale adoption of large language models (LLMs) has led to the widespread dissemination of AI-generated text that is challenging to differentiate from human-written content~\cite{wu2025survey}. This has raised concerns around the spread of misinformation \cite{chen2024combating}, the degradation of publication standards \cite{wu2023survey}, potential cybersecurity threats \cite{yao2024survey}, and breaches of academic integrity \cite{perkins2023academic}. As a result, effective AI content detection systems are essential for mitigating the risks from generative models. Recently, there has been substantial development in AI text detection tools, such as RoBERTa-based models \cite{solaiman2019release}, DetectGPT \cite{mitchell2023detectgpt}, Fast-DetectGPT \cite{bao2023fast}, GLTR \cite{gehrmann2019gltr}, RADAR \cite{hu2023radar}, and efforts to use LLMs as detectors \cite{bhattacharjee2024fighting}. Generally, these methods estimate a probability $p$ that the text is generated by AI by comparing the patterns (e.g., token probabilities or logits) of the given text against AI-generated text, and use a single universal threshold of $p > 0.5$ for classification \cite{freeman2008comparison}.

%This is particularly important in the context of eliminating misinformation in journalism \cite{chen2024combating} and complying with content policies in search engine optimization (SEO) \cite{hacker2023regulating}, and ensuring integrity in school homework \cite{perkins2024detection}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/Picture_01_v8.png}
    % \vspace{-6mm}
    \caption{We find that using one universal probability threshold (often $\theta > 0.5$) to classify AI and human-generated text can fail to account for subgroup-specific distributional variations, such as text length or writing style, leading to higher errors for certain demographics. We propose \emph{\name} to learn multiple decision thresholds tailored to each subgroup, which leads to more robust AI-text detection.}
    \vspace{-4mm}
    \label{fig:fig1}
\end{figure}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/thresholds_to_probabilities_v2.png}
    % \vspace{-6mm}
    \caption{Applying adaptive thresholds to different probability distributions by subgroup, as seen through cumulative density functions (CDFs). We partition texts by three length categories (short, medium, long) and five personality traits (extroversion, neuroticism, agreeableness, conscientiousness, openness), and use RADAR~\cite{hu2023radar} to infer AI-generated probabilities on the test dataset. Observe that a static classification threshold ($0.5$, in black) and a single optimized threshold (in gray at the right end side) does not account for subgroup-specific distributional variations. We used the thresholds in Appendix \ref{applied_threhsolds}. The static threshold followed conventional $0.5$, AUROC method is noted in Appendix \ref{method2}.}
    % For further details on the used thresholds, see Appendix~\ref{applied_threhsolds}
    % \vspace{-4mm}
    \label{fig:application_probability}
\end{figure*}

However, a single universal threshold for AI content classification fails to account for text with different characteristics, such as length or writing style, which can have very different probability distributions. That is to say, one fixed threshold does not consider the variation of the probability. For example, when using a fixed threshold of 0.5, a writing style characterized by openness and positivity is more likely to be identified as AI-generated than one characterized by neuroticism, as illustrated in Figure \ref{fig:application_probability}. This can lead to misclassification and disproportionately affect certain groups. Similar concerns were raised regarding race \cite{alghamdi2022beyond, bendekgey2021scalable} and gender \cite{weber2020black, jang2022group, bendekgey2021scalable}.


% Details about the feature engineering that is used for partitioning is noted in Appendix \ref{attributes}.


% existing approaches and limitations
%Prior methods for threshold selection often optimize metrics such as the area under the receiver operating characteristic curve (AUROC) \cite{zou2016finding} or constrain the false positive rate (FPR) \cite{openai2023classifier}.  For example, in 2023, OpenAI used calibrated threshold for their AI text classifiers under FPR constraints \cite{openai2023classifier}. But this could reduce  accuracy for the decrease of true positive rate (TPR) from the striction of FPR. Furthermore, one fixed threshold overlooks variations in the different text and different probability distribution from models, leading to suboptimal performance and biases. Using a fixed threshold in binary classification assumes well-calibrated probabilities, often not true in practice. In AI-generated text detection, diverse text characteristics affect output distributions. 
% Also, The precision-recall (PR) curve can be used to get an optimal threshold between precision and recall.

% generating adaptive thresholds
%A single global threshold fails to account for diverse characteristics, and threshold optimization could offer a solution by assigning tailored thresholds to subgroups based on specific attributes \cite{canetti2019soft}. We can enhance sensitivity and specificity per group as well as group fairness \cite{corbett2017algorithmic}.

% our approach
To address these limitations, we propose \textit{\name} to learn decision thresholds for distinct subgroups rather than using a standard threshold (see Figure \ref{fig:fig1}). Our method balances performance (e.g., ACC, F1, precision) and fairness metrics (e.g., demographic parity, equality of odds, equal opportunity) in learning these thresholds, ensuring that predefined fairness criteria are met across all subgroups in question. In experiments with four AI text classifiers on three datasets, \name\
increases overall F1 score and decreases balanced error rate (BER) discrepancy across subgroups, yielding better overall tradeoffs between performance and fairness.
%By integrating group-specific variations into the threshold optimization process, \textit{\name} offers an alternative to conventional methods that rely on a single, universal threshold.
% \paragraph{Contributions} 
To summarize, our main contributions are: (a) identifying discrepancies in AI-generated text classifiers based on length and stylistic characteristics, (b) developing \name, a group-adaptive decision threshold method to enhance group performance and fairness, and (c) developing a model with a rule-based relaxed fairness criteria to achieve fast convergence even when perfect fairness is not possible.


\section{Related Work}

We review work in AI-generated text detection and methods to threshold probability distributions into classifier labels.

\paragraph{AI text detectors}
AI text detectors have been developed to flag AI-generated text. RADAR utilizes adversarial learning techniques \cite{hu2023radar}, GLTR employs statistical metrics, including entropy, distributions, and ranks, to discern anomalies suggestive of AI content \cite{gehrmann2019gltr}, DetectGPT examines the curvature of log probability distributions, concentrating on local likelihood curvature to detect AI-generated text \cite{mitchell2023detectgpt}, and RoBERTa-based detectors fine-tune pretrained language models to proficiently categorize text as either human-produced or AI-generated \cite{solaiman2019release}. We employed \textit{probabilistic classifiers} among them which require a threshold-based decision mechanism. Unlike direct discrimination methods, thresholding enables post-processing for the balancing of performance and fairness.


\paragraph{Fixed universal thresholds for classification:}\label{method1:static} 
The most prevalent approach for making decision thresholds in classification involves employing a fixed universal threshold of $0.5$ to map predicted probabilities to class labels \cite{freeman2008comparison}. It is commonly used in many fields like photogrammetry \cite{shao2016characterizing}, ecology \cite{manel1999comparing, hanberry2013prevalence} and computer science \cite{lu2024mlnet}. Nevertheless, as highlighted by \citet{freeman2008comparison}, the dependence on $\theta = 0.5$ as a default threshold, merely due to its general acceptance, is frequently unreliable owing to variations in data distributions. 

\paragraph{Universal thresholds with optimization:} Threshold optimization techniques have been formulated to learn optimal thresholds across entire datasets. These methods optimize metrics such as the area under the receiver operating characteristic curve (AUROC) \cite{bradley1997use} and derive optimized decision thresholds under constraints such as the false positive rate (FPR) \cite{krishna2024paraphrasing,lipton2014optimal}. These approaches find applications across disciplines, including econometrics \cite{stavnkova2023threshold}, statistics \cite{esposito2021ghost}, and machine learning \cite{openai2023classifier}. While these methods yield a globally optimized threshold tailored to the chosen model, they can fail to adapt to the characteristics of individual instances, which can compromise the robustness of detection systems, particularly in significantly divergent data distributions.


%Furthermore, one fixed threshold overlooks variations in the different text and different probability distribution from models, leading to suboptimal performance and biases. Using a fixed threshold in binary classification assumes well-calibrated probabilities, often not true in practice. In AI-generated text detection, diverse text characteristics affect output distributions. 
% Also, The precision-recall (PR) curve can be used to get an optimal threshold between precision and recall.
%For instance, in 2023, OpenAI conducted an extensive evaluation of its AI text classifiers, emphasizing ethical deployment by constraining the FPR to below 0.1 in order to reduce misclassification of human-written text as AI-generated. However, the requirement to maintain a stringent FPR had the unintended consequence of lowering overall accuracy by reducing TPR, particularly under dataset shifts and changes in probability distributions. This global-threshold approach did not adapt to characteristics of individual texts, resulting in reduced robustness of the detection system. 

\paragraph{Adaptive thresholds across groups:} Recent work has highlighted the necessity for adaptive thresholds across groups \cite{jang2022group,bakker2021beyond}. \citet{menon2018cost} propose instance-dependent thresholds for inferred probabilities, resulting in optimal classifiers under cost-sensitive fairness objectives and enhancing overall performance. Similarly, \citet{corbett2017algorithmic} demonstrate that group-specific thresholds, informed by group-level statistics, can facilitate fairness-aware classification. \citet{canetti2019soft} investigate thresholds differentiated by race and proved that adaptive thresholds reduce performance disparity. \citet{jang2022group} present threshold-adaptation methods to ensure fair classification, and \citet{bakker2021beyond} explore threshold tuning to maintain stable classification performance across subgroups. Our method, \textit{\name} is grounded in the aforementioned principles and introduce a novel approach of adaptive thresholding.

%subgrouping as opposed to depending on a single variable.
%Moreover, feature engineering is utilized to proactively extract features rather than relying on the provided feature set.

% good question -- let me keep think about this 
%  How exactly does your method build on these principles? In what way are you different from these previous works? You should try to specify this as much as you can. This is key for reviewers in understanding your position in the related work.
%  Are you directly applying the techniques from these previous paper to MGT detection? Or are you contributing something new. You should specify this here.


% \section{Threshold Optimization and Fairness}


% what problems occurs under fairness constraint \\

% how the FPR sacrified the ACC in the OpenAI cases


% \subsection{ACC and Fairness tradeoff}
% we need to secure enough ACC



% \subsection{impossibility theorem}
% it is impossible to meet all fairness constraints -- what to give



\section{Conceptual Framework and Methodology}
\label{sec:methods}
This section outlines our method for adaptive thresholds in AI-generated text detection, \name, which iteratively adjusts classification thresholds for different subgroups to balance performance with inter-group fairness.

\subsection{Objective}
Considering a set of subgroups \( G_1, \ldots, G_i \) characterized by attributes (e.g., length and style) and acknowledging that texts with varying characteristics shows discrepancies in probability distributions, our objective is to refine the binary classification thresholds \( \{ \theta(G_1), \ldots, \theta(G_i) \} \) applicable to \( G_1, \ldots, G_i \). By accounting for subgroup-specific variations, these tailored thresholds ensure that decision boundaries accurately reflect the specific probability distributions.


% I'd probably start by defining groups G1 through Gi before talking about the thresholds. It's also worth talking about what a subgroup actually is. For example you could write: "Given a set of subgroups G_1, ..., G_i of our population divided by some meaningful characteristic (e.g. text, personality, etc.), we aim to optimize a set of detection thresholds \theta(G_1), ... \theta(G_i) for AI-gen text classification."


\subsubsection{Preliminaries of AI Text Detection}
The detection of AI-generated text is commonly formulated as a binary classification task, where the input space $\mathcal{X}$ consists of the sequence of tokenized indices derived from a given text, and the label space $\mathcal{Y} = \{0, 1\}$ denotes the two distinct classes of $0$ for human-written text and $1$ for AI-generated text. To address this task, existing work primarily utilizes a pretrained neural network as a probabilistic classifier $M_\theta: \mathcal{X} \rightarrow [0,1]$, which assigns a probability $p$ to the input reflecting the likelihood of it being AI-generated \cite{solaiman2019release, hu2023radar}. This probability is then converted into a predicted label by applying a threshold, typically set at $0.5$. Thus, the final predicted label \( \hat{Y} \in \{0, 1\} \) is determined as follows:
\begin{align}
\hat{Y} = \begin{cases}
1, & \text{if } M_\theta(x) \geq \text{threshold}, \\
0, & \text{otherwise.}
\end{cases}
\end{align}

%In classification tasks, we consider an input feature space $\mathcal{X} \subseteq \mathbb{R}^d$ and a label space $\mathcal{Y}$. For binary classification, we define $\mathcal{Y} = \{0, 1\}$, representing the two distinct classes. Let $X \in \mathcal{X}$ denote the random variable corresponding to input instances, and let $Y \in \mathcal{Y}$ represent the associated class labels. The joint distribution of $(X, Y)$ is denoted by $P_{X,Y}$ over the space $\mathcal{X} \times \mathcal{Y}$.


\subsubsection{Performance metrics} After inferring \(\hat{Y}\), we can compare $Y$ and \(\hat{Y}\). Correctly classifying AI-generated content as AI constitutes a true positive (TP). If human text is not classified as AI, it is true negative (TN). Flagging human text as AI is a false positive (FP), and flagging AI text as human is a false negative (FN). Details about the classification matrix and performance metrics are in Appendix \ref{table_ai_detection}.

\subsubsection{Fairness metrics}
The objective of fair classification is to mitigate disparities while preserving performance. To quantify fair performance, we focus on \emph{Demographic Parity (DP)} \cite{kim2020fact}, a fairness criterion measuring whether a classifier's predictions $\hat{Y}$ are statistically independent across attributes $S$: $\hat{Y} \perp\!\!\!\perp S$. This condition mandates that the probability of assigning a positive outcome is equitable across all groups, regardless of the true class labels $Y$. To measure the extent of disparity, we define the DP metric:
\begin{align}
\Delta_{\text{DP}} 
&= \left| P(\hat{Y} = 1 \mid S = a) - P(\hat{Y} = 1 \mid S = b) \right|
\end{align}


% The objective of fair classification is to mitigate disparities while preserving performance. To quantify fair performance, we focus on \emph{Demographic Parity (DP)} \cite{kim2020fact}, a fairness criterion measuring whether a classifier's predictions $\hat{Y}$ are statistically independent across attributes $S$: $\hat{Y} \perp\!\!\!\perp S$. This condition mandates that the probability of assigning a positive outcome is equitable across all groups, regardless of the true class labels $Y$. For example, in classification with three groups ($S \in \{a,b,c\}$), this implies:
% \begin{align}
% P(\hat{Y} = 1 \mid S = s) \text{ is invariant across } s \in \{a, b, c\}
% \end{align}

% \begin{align}
% \Delta_{\text{DP}} 
% &= \left| P(\hat{Y} = 1 \mid S = a) - P(\hat{Y} = 1 \mid S = b) \right|
% \end{align}

By addressing an overall equal rate of positive predictions across groups, DP seeks to measure \emph{disparate impact}, where one group receives a disproportionate share of positive or negative outcomes due to biases in the classifier model \cite{feldman2015certifying}. In applying DP, the perfect fairness is $\Delta_{\text{DP}} = 0$, which means that there is no difference in positive rates between groups. Details about fairness and performance metrics are noted in Appendix \ref{appendix:metrics}.

\subsection{Relaxed Fairness}
\label{sec:fairness_criterion}

While achieving 100\% fairness across groups is theoretically ideal, it is often impractical due to computational constraints and the difficulty of ensuring model convergence under strict fairness criteria~\cite{dwork2012fairness}. The 80\% fairness rule, often called relaxed fairness, offers a pragmatic approach to balance reasonable fairness and performance \cite{feldman2015certifying}, and can also mitigate overfitting risks and ensure convergence based on the rule-based constraints.

We formalize a relaxed fairness criterion requiring that the ratio of minimum to maximum value of each fairness metric across all subgroups does not fall below a specified threshold $\tau$, set to $0.8$. It can be mathematically represented as:
\begin{align}
\frac{\min\limits_{i \in \mathcal{S}} \, c_i^{(k)}(\theta)}{\max\limits_{i \in \mathcal{S}} \, c_i^{(k)}(\theta)} \geq \tau, \quad \forall k \in \mathcal{K},
\end{align}
where:
\begin{itemize}
    \item $\mathcal{S}$ denotes the set of attributes ({\em e.g. length, writing style, etc.})
    \item $\mathcal{M}$ represents the set of fairness metrics ({\em e.g., demographic parity, equalized odds, etc.}).
    \item $c_i^{(k)}(\theta)$ computes the $k$-th metric in $\mathcal{K}$  for each group using classifier parameters $\theta$.
    \item $\tau \in [0,1]$ is the fairness criteria, with $\tau = 0.8$ reflecting the 80\% rule and $\tau = 1$ reflects 100\% fairness.
\end{itemize}

% This criterion enforces that the fairness metrics are proportionally balanced across groups. By adjusting the classifier's static decision thresholds for each subgroup, we can satisfy relaxed fairness. 

Note that we should consider limiting the number of fairness metrics because of the \emph{impossibility theorem} \cite{dwork2012fairness} as it is computationally impractical to satisfy all fairness criteria simultaneously. The generated thresholds for subgroups based on the training dataset is used for the classification criteria in the test dataset that has same subgroups. 

\subsection{Group-Adaptive Threshold Optimization} \label{subsec:group_adaptive}
\paragraph{Process} Given these preliminaries, we now introduce our \name\ algorithm, which is summarized in Algorithm \ref{algorithm_fairopt}. At a high level, \name\ uses a gradient-based subgroup threshold update to jointly optimize classification performance (ACC and F1) and group-level fairness. 

Formally, we partition the dataset \(D\) into subgroups \(\{G_1,\dots,G_i\}\) based on features \(\{S_1,\dots,S_n\}\). Each subgroup \(G_i\) is assigned an initial threshold \(\theta_{\mathrm{init}}\) (e.g., 0.5 to all groups). 

During each iteration, we compute predictions for each sample \(x_n\in G_i\) by binarizing the predicted probability \(p_n\) using the current threshold \(\theta(G_i)\). The model derives a contingency matrix based on the predictions \(\hat{y}_n\) and actual values \(y_n\). 

For each subgroup \(G_i\), we define a loss function that penalizes both low accuracy and insufficient F1 score:
\begin{align}
   L_i(\theta) 
   \;=\; 
   -\,\mathrm{ACC}_i(\theta)
   \;+\;
   \kappa \,\bigl[\beta - \mathrm{F1}_i(\theta)\bigr]_{+},
\end{align}
where \(\beta\) is a minimum F1 threshold, \(\kappa>0\) is a penalty weight, \(\alpha\) is also given for the minimum ACC, and \([\cdot]_{+}\) denotes the positive part that only positive values are retained, turning negative numbers to zero. The minimum thresholds are essential for ensuring stable convergence and preventing degenerate outcomes across all subgroups.

To optimize each \(\theta(G_i)\), we employ a \emph{finite-difference approximation} of the gradient:
\begin{align}
   \nabla L_i(\theta(G_i)) 
   \;\approx\;
   \frac{L_i(\theta(G_i)+\delta) - L_i(\theta(G_i)-\delta)}{2\,\delta}.
\end{align}
The finite-difference approximation estimates the gradient of each subgroup's threshold-based loss function by computing the difference between the loss values at $\theta(G_i) + \delta$ and $\theta(G_i) - \delta$, then dividing by $2\delta$ \cite{liu2020primer}. This method is employed for an effective interaction with the discrete contingency matrix, as it allows gradient estimation without requiring an explicit analytic form of the loss function's derivative.

We then perform a gradient descent step for each subgroup:
\begin{align}
   \theta(G_i) 
   \;\leftarrow\;
   \theta(G_i) - \eta\, \nabla L_i(\theta(G_i)),
\end{align}
followed by clipping to the allowable interval \(\bigl[a,b\bigr]\subseteq [0,1]\) to make sure it is a valid probability.

\begin{algorithm}[t]
\caption{\name: Group-Specific Thresholding}
\begin{algorithmic}[1]
\STATE \textbf{Input:} 
    \(D\) dataset, 
    \(S_n\) features,
    predicted probabilities \(p\),  
    performance targets (\(\alpha,\beta\)), 
    fairness gap limit \(\delta_{\text{fair}}\), 
    max iterations, step size \(\eta\).

\STATE Split \(D\) into sub-datasets \(D_1,\ldots,D_j\) by \(S_n\).
% \STATE Initialize \(\theta(G_i) \leftarrow \theta_{\text{init}}\) for each \(G_i\).
\FOR{\text{iteration} = 1 to \text{max iterations}}
    \FOR{each subgroup \(G_i\)}
        \STATE Convert \(p\) in \(G_i\) to binary predictions using \(\theta(G_i)\) and compute confusion matrix.
        \IF{(\(\mathrm{ACC}_i \ge \alpha\) and \(\mathrm{F1}_i \ge \beta\) for all \(i\)) \textbf{and} (fairness gap \(\le \delta_{\text{fair}}\))}
            \STATE \textbf{return} \(\{\theta(G_1),\ldots,\theta(G_j)\}\).
        \ELSE 
            \STATE Adjust \(\theta(G_i)\) 
            % by \( \eta \cdot \nabla L_i(\theta(G_i))\)
        \ENDIF
    \ENDFOR
\ENDFOR
\STATE \textbf{Output:} \(\{\theta(G_1),\dots,\theta(G_j)\}\).
\end{algorithmic}
\label{algorithm_fairopt}
\end{algorithm}

% \nabla L_i(\theta(G_i))

Finally, the model calculates domain-relevant fairness metrics \(\{M_1, \ldots, M_k\}\) across all subgroups by measuring the maximum inter-group disparity:
\begin{align}
    \Delta_{k} \;=\; \max_i\,M_k(G_i) \;-\;\min_i\,M_k(G_i),
\end{align}
and verifies whether \(\Delta_k \le \delta_{\mathrm{fair}}\) for each metric \(M_k\). If any disparity exceeds \(\delta_{\mathrm{fair}}\), it is considered unfair. The evaluation for fairness is not included in the loss function because it assesses performance in a binary manner, fair or unfair (e.g., pass or fail), similar to \textit{zero-one loss}, rather than providing a continuous gradient that can guide optimization. 

To determine termination, we monitor the largest threshold change among all groups, 
\begin{align}
   \Delta_\theta \;=\; \max_{G_i}\,\bigl|\theta_{\mathrm{new}}(G_i) - \theta_{\mathrm{old}}(G_i)\bigr|.
\end{align}
If \(\Delta_\theta\) falls below a tolerance \(\epsilon_{\mathrm{tol}}\) \emph{and} all groups meet the performance criterion (\(\mathrm{Acc}_i \ge \alpha\), \(\mathrm{F1}_i \ge \beta\)) \emph{and} fairness disparities (\(\Delta_k \le \delta_{\mathrm{fair}}\)) hold, the algorithm terminates.

\paragraph{Outcome} 
By iterating this process, the algorithm \ref{algorithm_fairopt} produces an optimized set of subgroup thresholds \(\{\theta(G_1),\dots,\theta(G_i)\}\) that strike a balance between high classification performance and bounded inter-group fairness disparities. This adaptive thresholding method is particularly effective when probability distributions differ significantly across subgroups, as it tailors the decision boundary to each subgroup’s characteristics rather than relying on a single global threshold. Additional details about the algorithm, including learning and implementation details, are included in Appendix \ref{appendix:algorithm}. 

\section{Experiments}

Our experiments aim to assess the performance of \name\ in comparison to a static threshold and a single universally optimized threshold. For training, we use the {RAID} and {MAGE} datasets, and for testing, we utilized {SemEval24}, {MAGE}, and {Human v. AI 500K} dataset.

\subsection{Datasets}
\subsubsection{Train dataset}
The {RAID} dataset \cite{dugan2024raid} encompasses text samples generated by a wide array of language models, including GPT-4, GPT-3, Llama 2 70B, Cohere, MPT-30B, and Mistral 7B. These models produced text across diverse domains such as abstracts, recipes, reddit posts, book summaries, news articles, and poetry. {MAGE} \cite{li-etal-2024-mage} includes human-written text covering opinion statements, news articles, question answering, stories, and scientific writing, with machine-generated texts produced by 27 LLMs, including OpenAI's GPT variants, LLaMA series, FLAN-T5, and EleutherAI models. We randomly extracted 10,000 samples from each of these two datasets. 

\subsubsection{Test dataset}
We also use three datasets for test: {SemEval24}, {MAGE}, and {Human v. AI 500K}. The {SemEval24} dataset \cite{wang2024semeval} was employed for its diverse representation of commonly used AI text generators balanced with human-written texts across various topics such as news, social media posts, and creative writing, from which we extracted 3,000 samples. We also extracted random 3,000 writings from the {MAGE} dataset except for the instances that used in the training, and randomly selected 2,000 samples from the {Human v. AI 500K} dataset, the most widely used dataset for AI text detection on Kaggle. 


\subsection{AI-Generated Text Classifiers}

To detect AI-generated text, we employed a combination of openly accessible classifiers, RoBERTa-base \cite{solaiman2019release}, RoBERTa-large \cite{solaiman2019release}, and RADAR \cite{hu2023radar}. These classifiers were selected based on their open-source availability and the ability to produce unified probabilistic outputs ranging from 0 to 1 with seven decimal points. GPT4o-mini was incorporated for its promising application in AI content detection tasks \cite{bhattacharjee2024fighting}. Unlike binary classification approaches, these models assign a probability score indicating the likelihood that a given text was generated by AI rather than directly making a discrimination. To maintain uniformity in our evaluation, GPT4o-mini was explicitly prompted to return probability scores within the same range as other detectors.


% ou need to specify the process used to apply GPT4o-mini to detection (how did you convert natural language output to a classification score? How did you ensure it is from 0 to 1? etc.)

% [WHY THESE FOUR DETECTORS ARE SELECTED -- OPEN SOURCE + APPLICATION OF THE LLM FOR THE DETECTION PROCESS + UNIFY THE PROBABILITY OUTPUT - PROBABILITY RANGE, AND THE LLM WAS PROMPTED TO RETURN WITHIN THE RANGE AS WELL]

%  one column
%  Text should be MUCH larger for these two charts. The axis labels, titles, and legends all need to be readable.
\begin{figure}[t]
    \centering 
    \includegraphics[width=\columnwidth]{figures/length_probability_distribution_v2.png}
    \vspace{-5mm}
    \caption{%
    Histograms show how AI-generated probability distributions differ by text length (Short in red, Medium in green, Long in blue). If we use the threshold of 0.5 (black dash line), human-written medium-length text shows higher error rate than other lengths. Each kernel density estimation (KDE) curve reflects the probability scores assigned by RoBERTa-large on the training dataset.}
    \vspace{-0mm}
    \label{fig:text_probability_distribution}
\end{figure}

\subsection{Subgroup Attributes}
\label{attributes}
%  keep use 'subgroups'
We extracted subgroup attributes from each dataset to undercover the potential disparities present in AI classifiers. These attributes are shown in Table \ref{featuretable} and include both non-sensitive attributes like text length and sensitive attributes inferred from stylistic traits.

\begin{table}[h!]
\small
\centering
\begin{tabularx}{\columnwidth}{l|X|X}
\hline
\textbf{Attribute} & \textbf{Characteristics} & \textbf{Labels} \\
\hline
Text Length & Number of characters in a text & Short, medium, long (n = 3) \\
% \hline
% Formality & Indicates the level of formality in the language & formal, informal (n = 2) \\
% \hline
% Sentiment & Reflects the emotional tone. & positive, negative (n= 2) \\
\hline
Stylistic personality & Inferred Big Five personality traits. & Openness, conscientiousness, extroversion, agreeableness, neuroticism (n = 5) \\
\hline
\end{tabularx}
\caption{The subgroup attributes studied in our experiments.}
\label{featuretable}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/discrepancy_v5.png}
    \vspace{-4mm}
    \caption{Top two highest and lowest distributional differences using Kolmogorov-Smirnoff (KS) test on RoBERTa-large detector probabilities. The visualization is based on KDE. The biggest discrepancy is observed with a KS statistic of $0.2078$ ($p<0.01$), while the smallest is $0.1001$ ($p<0.01$). These discrepancies indicate varying levels of divergence between groups based on the characteristics of the given text.}
    \vspace{-1mm}
    \label{fig:discrepancy_two_features}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=\columnwidth]{figures/tradeoff/large_v2.png}
        \caption{RoBERTa-large}
        \label{fig:second_figure}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=\columnwidth]{figures/tradeoff/base_v2.png}
        \caption{RoBERTa-base}
        \label{fig:tradeoff_curve}
    \end{subfigure}
    \caption{The \name\ algorithm enables us to obtain a balanced tradeoff between performance and fairness, through a relaxed fairness criterion specified by a given fairness threshold. We improve the Pareto frontier of performance and fairness compared to static and ROCFPR baselines.}
    \label{fig:combined_figures}
\end{figure}

% \begin{figure}[ht]
%     \centering
%     \subfigure[RoBERTa-large]{
%         \includegraphics[width=\columnwidth]{figures/tradeoff/large_v2.png}
%         \label{fig:second_figure}}
%     \hfill
%     \subfigure[RoBERTa-base]{
%         \includegraphics[width=\columnwidth]{figures/tradeoff/base_v2.png}
%         \label{fig:tradeoff_curve}}
%     \caption{The \name\ algorithm enables us to obtain a balanced tradeoff between performance and fairness, through a relaxed fairness criterion specified by a given fairness threshold. We improve the Pareto frontier of performance and fairness compared to static and ROCFPR baselines.}
%     \vspace{-1mm}
%     \label{fig:combined_figures}
% \end{figure}

\textbf{Text length} measures the number of characters in a text. OpenAI's evaluations \cite{openai2023classifier} revealed performance disparities between short (under 1,000 characters) and long text when using their AI classifier. Building on this and using exploratory data analysis, we refined the thresholds to 1,000 and 2,500 characters, categorizing texts as short (up to 1,000), mid-length (1,000 to 2,500), or long (over 2,500).

The \textbf{stylistic personality} attribute includes five traits: extroversion, neuroticism, agreeableness, conscientiousness, and openness \cite{personality_prediction}. We determine the personality trait by selecting the one with the highest probability among the five using a BERT-based personality prediction model \cite{personality_prediction}, which is fine-tuned from BERT-base-uncased \cite{BERT2018}. Details are noted in Appendix \ref{feature_engineering}.

% two columns
% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=2\columnwidth]{figures/length_probability_distribution.png}
%     \vspace{-5mm}
%     \caption{%
%     Histograms show how AI-generated probability distributions differ by text length (Short in red, Medium in green, Long in blue). The top row represents AI-written texts and the bottom row represents human-written texts. Each kernel density estimation (KDE) curve reflects the probability scores assigned by RoBERTa-large on the training dataset. The black dashed line denotes a standard 0.5 threshold. Notice that length-specific differences may cause misclassification if a single threshold is used.}
%     \vspace{-0mm}
%     \label{fig:text_probability_distribution}
% \end{figure*}

\subsection{Discovery of Subgroup Bias in Text Detection}

Our first experimental analysis uncovers many discrepancies in AI-text detection performance with respect to different text lengths and writing styles. For example, as shown in Figure \ref{fig:text_probability_distribution}, medium-length human-written text is more likely to be flagged as AI-generated than short-length human-written text. Similarly, in Figure \ref{fig:application_probability}, long text with a neurotic style is also more likely to be detected as AI-generated. 

% We also revealed substantial variability in the discrepancies between subgroups, for example, medium-length coupled with neurotic style showed a great distributional different compared to short neurotic style, as shown in Figure \ref{fig:discrepancy_two_features}.

To obtain a holistic view of these biases, we used the Kolmogorov-Smirnov (KS) test to evaluate distributional discrepancies across various text groups under the assumption of a non-parametric distribution. As shown in Figure \ref{fig:discrepancy_two_features}, there are notable differences between certain subgroup pairs. For instance, the comparison between texts characterized by \textit{neuroticism and medium-length} versus those with \textit{neuroticism and short-length} yielded a KS statistic of 0.2078 ($p < 0.01$). \textit{Extroversion and mid-length} and \textit{neuroticism and short length}, showed a KS statistic of 0.1959 ($p < 0.01$). On the other hand, relatively smaller discrepancies were observed between groups such as \textit{neuroticism and short length} versus \textit{openness and short length}, with a KS statistic of 0.1155 ($p < 0.001$), and between \textit{neuroticism and short length} versus \textit{openness and mid-length}, with a KS statistic of 0.1001 ($p < 0.01$). These observed discrepancies show that varying subgroup characteristics can lead to diverging outcomes when using a fixed threshold for classification. Addressing these discrepancies through tailored threshold adjustments or other interventions is crucial to ensuring equitable treatment across all subgroups.


% [TODO what numbers do we look at? so many numbers! can u bold what we should focus on? also, add the main takeaway from this table]





% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{figures/tradeoff_curve_large.png}
%     \vspace{-5mm}
%     \caption{Application of the relaxed fairness \ref{sec:fairness_criterion} to the accuracy-fairness tradeoff. The rule-based fairness allows the model to terminate before sacrificing the ACC while providing a convergence criteria in the optimization process. The result is based on RoBERTa-large detector on the overall test dataset in using of FairOPT with early-stopping. The Static and ROCFPR method is noted in the fairness relaxation 1 since they do not consider group fairness in their usage and optimization process.}
%     \vspace{-5mm}
%     \label{fig:tradeoff_curve}
% \end{figure}


\begin{table*}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|ccccc|ccccc|ccccc}
\toprule
\multirow{2}{*}{\textbf{Detector}} & \multirow{2}{*}{\textbf{Method}}
& \multicolumn{5}{c|}{\textbf{SemEval}}
& \multicolumn{5}{c|}{\textbf{Kaggle}}
& \multicolumn{5}{c}{\textbf{MAGE}} \\
\cmidrule(lr){3-7}\cmidrule(lr){8-12}\cmidrule(lr){13-17}
& 
& \textbf{F1} & \textbf{ACC} & \textbf{FairLen} & \textbf{FairPer} & \textbf{FPR}
& \textbf{F1} & \textbf{ACC} & \textbf{FairLen} & \textbf{FairPer} & \textbf{FPR}
& \textbf{F1} & \textbf{ACC} & \textbf{FairLen} & \textbf{FairPer} & \textbf{FPR} \\
\midrule

%======================= RoBERTa-large =======================
\textbf{RoBERTa-large}
& \textbf{Static}
  & 0.4382 & 0.2847 & 0.1444 & 0.1377 & 0.9892
  & 0.4309 & 0.2823 & 0.0607 & 0.1161 & 0.9830
  & \textbf{0.7147} & 0.6360 & 0.1817 & 0.1760 & 0.6464 \\
& \textbf{ROCFPR}
  & 0.0740 & 0.2990 & 0.2197 & 0.1792 & \textbf{0.4854}
  & 0.3469 & \textbf{0.3900} & 0.1490 & 0.5782 & \textbf{0.6364}
  & 0.5201 & \textbf{0.6475} & 0.3471 & 0.2777 & \textbf{0.1031} \\
& \textbf{\name}
  & \textbf{0.4849} & \textbf{0.3223} & \textbf{0.1347} & \textbf{0.1075} & 0.9892
  & \textbf{0.4542} & 0.2973 & \textbf{0.0291} & \textbf{0.0755} & 0.9835
  & 0.7088 & 0.6080 & \textbf{0.1749} & \textbf{0.1209} & 0.6473 \\
\midrule

%======================= RADAR =======================
\textbf{RADAR}
& \textbf{Static}
  & 0.5592 & 0.6373 & 0.3826 & 0.3399 & 0.2266
  & \textbf{0.7097} & \textbf{0.7373} & 0.3088 & 0.2876 & 0.3360
  & 0.4364 & 0.3930 & 0.5979 & 0.4599 & 0.6896 \\
& \textbf{ROCFPR}
  & 0.0153 & 0.5293 & \textbf{0.1162} & \textbf{0.0475} & \textbf{0.0019}
  & 0.0351 & 0.6333 & \textbf{0.1199} & \textbf{0.1412} & \textbf{0.0005}
  & 0.3215 & \textbf{0.4935} & \textbf{0.4102} & 0.4811 & \textbf{0.2662} \\
& \textbf{\name}
  & \textbf{0.6307} & \textbf{0.6620} & 0.3536 & 0.4939 & 0.2228
  & 0.6445 & 0.6183 & 0.2269 & 0.1566 & 0.3732
  & \textbf{0.4541} & 0.3820 & 0.5705 & \textbf{0.4140} & 0.6965 \\
\midrule

%======================= RoBERTa-base =======================
\textbf{RoBERTa-base}
& \textbf{Static}
  & 0.3345 & 0.2133 & \textbf{0.0976} & 0.0978 & 0.9703
  & \textbf{0.3659} & 0.2457 & 0.0950 & 0.1294 & 0.9553
  & 0.7020 & 0.6515 & 0.2023 & 0.2124 & 0.5265 \\
& \textbf{ROCFPR}
  & 0.0740 & \textbf{0.2253} & 0.2463 & 0.1428 & \textbf{0.7633}
  & 0.3374 & \textbf{0.2943} & 0.0876 & 0.1276 & \textbf{0.8171}
  & 0.5814 & \textbf{0.6580} & 0.4275 & 0.3666 & \textbf{0.1739} \\
& \textbf{\name}
  & \textbf{0.3548} & \textbf{0.2253} & 0.1072 & \textbf{0.0866} & 0.9696
  & 0.3706 & 0.2447 & \textbf{0.0876} & \textbf{0.1234} & 0.9553
  & \textbf{0.7122} & 0.6485 & \textbf{0.1966} & \textbf{0.1517} & 0.5265 \\
\midrule

%======================= GPT4o-mini =======================
\textbf{GPT4o-mini}
& \textbf{Static}
  & 0.3737 & 0.5497 & 0.1806 & 0.3137 & 0.2114
  & 0.2949 & 0.3083 & 0.1042 & 0.2494 & 0.7390
  & 0.2017 & 0.4340 & 0.2921 & 0.1990 & 0.2878 \\
& \textbf{ROCFPR}
  & 0.0004 & 0.5247 & \textbf{0.0053} & \textbf{0.0130} & \textbf{0.0057}
  & 0.0000 & \textbf{0.6267} & \textbf{0.0005} & \textbf{0.0020} & \textbf{0.0005}
  & 0.0000 & 0.5080 & \textbf{0.0064} & \textbf{0.0039} & \textbf{0.0020} \\
& \textbf{\name}
  & \textbf{0.4371} & \textbf{0.5553} & 0.1907 & 0.6844 & 0.2139
  & \textbf{0.3792} & 0.3223 & 0.1279 & 0.3471 & 0.7459
  & \textbf{0.2802} & \textbf{0.4170} & 0.2578 & 0.1697 & 0.3006 \\
\bottomrule
\end{tabular}
}
\caption{%
Comparison of three thresholding methods (Static, ROCFPR, FairOPT) across four probabilistic AI text classifiers 
(RoBERTa-large, RoBERTa-base, RADAR, GPT4o-mini) and three data sources (SemEval, Kaggle, MAGE). 
Metrics include F1, ACC, FairLen (BER discrepancy for Length), FairPer (BER discrepancy for Personality), and FPR. Compared to the Static method, FairOPT results in an \textbf{average increase of 2.91\% in F1 score 
and decrease of 1.77\% in FairPer, while causing a decrease of 0.58\% in ACC and an increase of 1.59\% in FairLen.}, demonstrating its effectiveness in enhancing specific aspect of performance and fairness 
with minimal trade-offs in classifier performance. Although ROCFPR method showed the best result in decreasing the FPR of the model, it overly sacrificed performance.
}
\label{tab:comparative_table}
\end{table*}


% \begin{table*}[htbp]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|l|ccccc|ccccc|ccccc}
% \toprule
% \multirow{2}{*}{\textbf{Detector}} & \multirow{2}{*}{\textbf{Method}}
% & \multicolumn{5}{c|}{\textbf{SemEval}}
% & \multicolumn{5}{c|}{\textbf{Kaggle}}
% & \multicolumn{5}{c}{\textbf{MAGE}} \\
% \cmidrule(lr){3-7}\cmidrule(lr){8-12}\cmidrule(lr){13-17}
% & 
% & \textbf{F1} & \textbf{ACC} & \textbf{FairLen} & \textbf{FairPer} & \textbf{BER}
% & \textbf{F1} & \textbf{ACC} & \textbf{FairLen} & \textbf{FairPer} & \textbf{BER}
% & \textbf{F1} & \textbf{ACC} & \textbf{FairLen} & \textbf{FairPer} & \textbf{BER} \\
% \midrule

% %======================= RoBERTa-large =======================
% \textbf{RoBERTa-large}
% & \textbf{Static}
%   & 0.4382 & 0.2847 & 0.1444 & 0.1377 & 0.6999
%   & 0.4309 & 0.2823 & 0.0607 & 0.1161 & 0.6273
%   & \textbf{0.7147} & 0.6360 & 0.1817 & 0.1760 & 0.3588 \\
  
% & \textbf{ROCFPR}
%   & 0.0740 & \textbf{0.2990} & 0.2197 & 0.1792 & 0.7131
%   & 0.3469 & \textbf{0.3900} & 0.1490 & 0.5782 & 0.6050
%   & 0.5201 & \textbf{0.6475} & 0.3471 & 0.2777 & \textbf{0.3571} \\
  
% & \textbf{FairOPT}
%   & \textbf{0.4849} & 0.3223 & \textbf{0.1347} & \textbf{0.1075} & \textbf{0.6599}
%   & \textbf{0.4542} & 0.2973 & \textbf{0.0291} & \textbf{0.0755} & \textbf{0.6041}
%   & 0.7088 & 0.6080 & \textbf{0.1749} & \textbf{0.1209} & 0.3856 \\
% \midrule

% %======================= RADAR =======================
% \textbf{RADAR}
% & \textbf{Static}
%   & 0.5592 & 0.6373 & 0.3826 & 0.3399 & 0.3703
%   & \textbf{0.7097} & \textbf{0.7373} & 0.3088 & 0.2876 & \textbf{0.2377}
%   & 0.4364 & 0.3930 & 0.5979 & 0.4599 & 0.6055 \\
  
% & \textbf{ROCFPR}
%   & 0.0153 & 0.5293 & \textbf{0.1162} & \textbf{0.0475} & 0.4971
%   & 0.0351 & 0.6333 & \textbf{0.1199} & \textbf{0.1412} & 0.4913
%   & 0.3215 & \textbf{0.4935} & \textbf{0.4102} & 0.4811 & \textbf{0.5109} \\
  
% & \textbf{FairOPT}
%   & \textbf{0.6307} & \textbf{0.6620} & 0.3536 & 0.4939 & \textbf{0.3406}
%   & 0.6445 & 0.6183 & 0.2269 & 0.1566 & 0.3190
%   & \textbf{0.4541} & 0.3820 & 0.5705 & \textbf{0.4140} & 0.6155 \\
% \midrule

% %======================= RoBERTa-base =======================
% \textbf{RoBERTa-base}
% & \textbf{Static}
%   & 0.3345 & \textbf{0.2133} & \textbf{0.0976} & 0.0978 & 0.7763
%   & \textbf{0.3659} & 0.2457 & 0.0950 & 0.1294 & 0.6859
%   & 0.7020 & 0.6515 & 0.2023 & 0.2124 & 0.3452 \\
  
% & \textbf{ROCFPR}
%   & 0.0740 & 0.2253 & 0.2463 & 0.1428 & \textbf{0.7633}
%   & 0.3374 & \textbf{0.2943} & \textbf{0.0876} & 0.1276 & 0.6843
%   & 0.5814 & \textbf{0.6580} & 0.4275 & 0.3666 & \textbf{0.3451} \\
  
% & \textbf{FairOPT}
%   & \textbf{0.3548} & 0.2253 & 0.1072 & \textbf{0.0866} & \textbf{0.7633}
%   & 0.3706 & 0.2447 & \textbf{0.0876} & \textbf{0.1234} & \textbf{0.6842}
%   & \textbf{0.7122} & 0.6485 & \textbf{0.1966} & \textbf{0.1517} & 0.3473 \\
% \midrule

% %======================= GPT4o-mini =======================
% \textbf{GPT4o-mini}
% & \textbf{Static}
%   & 0.3737 & \textbf{0.5497} & 0.1806 & 0.3137 & 0.4638
%   & 0.2949 & 0.3083 & 0.1042 & 0.2494 & 0.6756
%   & 0.2017 & 0.4340 & 0.2921 & 0.1990 & 0.5711 \\
  
% & \textbf{ROCFPR}
%   & 0.0004 & 0.5247 & \textbf{0.0053} & \textbf{0.0130} & 0.5018
%   & 0.0000 & \textbf{0.6267} & \textbf{0.0005} & \textbf{0.0020} & \textbf{0.5003}
%   & 0.0000 & \textbf{0.5080} & \textbf{0.0064} & \textbf{0.0039} & \textbf{0.5010} \\
  
% & \textbf{FairOPT}
%   & \textbf{0.4371} & 0.5553 & 0.1907 & 0.6844 & \textbf{0.4543}
%   & \textbf{0.3792} & 0.3223 & 0.1279 & \textbf{0.3471} & 0.6305
%   & \textbf{0.2802} & 0.4170 & 0.2578 & 0.1697 & 0.5863 \\
% \bottomrule
% \end{tabular}
% }

% \caption{Comparison of three thresholding methods (Static, ROCFPR, FairOPT) across four probabilistic AI text classifiers (RoBERTa-large, RoBERTa-base, RADAR, GPT4o-mini) and three data sources (SemEval, Kaggle, MAGE). Metrics include F1 Score (F1), Accuracy (ACC), FairLen (BER discrepancy for Length), FairPer (BER discrepancy for Personality), and Balanced Error Rate (BER). Compared to the Static method, FairOPT results in an \textbf{average increase of 2.91\% in F1 Score and 1.77\% in FairPer, while causing a decrease of 0.58\% in Accuracy and an increase of 1.59\% in FairLen. It also decreases the overall BER by 0.22\%}, demonstrating its effectiveness in enhancing specific fairness aspects with minimal trade-offs in classifier performance.}
% \label{tab:comparative_table}
% \end{table*}


% \begin{figure*}[h]
%     \centering
%     \subfloat[Roberta Base]{
%         \includegraphics[width=0.24\linewidth]{figures/tradeoff/base.png}
%     }
%     \subfloat[Roberta Large]{
%         \includegraphics[width=0.24\linewidth]{figures/tradeoff/large.png}
%     }
%     \subfloat[Radar]{
%         \includegraphics[width=0.24\linewidth]{figures/tradeoff/radar.png}
%     }
%     \subfloat[GPT4o-mini]{
%         \includegraphics[width=0.24\linewidth]{figures/tradeoff/gpt.png}
%     }
%     \caption{Relaxed Fairness on the Accuracy-Fairness Tradeoff for Four Detectors}
%     \label{fig:fairness-tradeoff}
% \end{figure*}

\subsection{Mitigating Subgroup Bias using \name}

Given these findings, we now describe the experimental setup using \name\ for more robust AI-text detection across subgroups. We extracted the text length and personality features from every item in the dataset, which enabled the partition of 15 subgroups with different characteristics. For the training dataset, the AI text classifiers {RoBERTa-base}, {RoBERTa-large}, and {RADAR} were employed to generate predicted probabilities indicating the likelihood that a given text was AI-generated. These probabilities ranged from 0 to 1, with no direct classification decisions made at this stage. For the test dataset, additional predictions were generated using the {GPT4o-mini} API.

We used \name\ to get adaptive thresholds \(\{\theta(G_1),\dots,\theta(G_j)\}\) for each subgroup. We include all hyperparameters and training details in Appendix \ref{hyperparameters}. Threshold optimization was performed on the training dataset based on the results only from {RoBERTa-large} rather than averaging the probabilities of three detectors. The relaxed fairness criteria in \ref{sec:fairness_criterion} is employed to encourage earlier convergence and minimize the sacrifice of overall performance while pursuing group fairness. The {RoBERTa-large} model was also used to implement the AUROC-based approach (Method based on ROC curve~\ref{method2} ) with $FPR < 0.1$ constraint for comparison, and the static threshold \ref{method1:static} does not need a model for training in generating a decision threshold.

The optimized thresholds were then applied to the test dataset. Performance was evaluated using F1, ACC, BER, and BER calculated per subgroup. FairLen and FairPer metrics were utilized to quantify the maximum discrepancies in BER across groups defined by text length and personality traits. These metrics are defined as follows:
\begin{align}
    \text{FairLen} &= \max_{a \in L} \text{BER}(a) - \min_{a \in L}\text{BER}(g) \\
    \text{FairPer} &= \max_{b \in P} \text{BER}(b) - \min_{b \in P}\text{BER}(g) 
\end{align}
where \( L \) includes three groups based on text length, \( P \) includes five groups based on big five personality traits, and \( \text{BER}\) denotes the balanced error rate for each group. A higher value of FairLen or FairPer indicates a greater disparity in classification performance across subgroups.



% The optimized thresholds were then applied to the test dataset. Performance was evaluated using F1, ACC, BER, and BER calculated per subgroup. We used two important metrics to measure fairness -



% FairLen and FairPer were used to measure the maximum discrepancy of BER for length and personality, respectively. These metrics are defined as follows:

% \begin{align}
%     \text{FairLen} &= \max_{g \in G} \text{BER}(g) - \min_{g \in G}\text{BER} 
% \end{align}

% where \( G \) represents groups based on text length, \( P \) represents groups based on Big Five personality traits, and \( \text{BER}(g) \) and \( \text{BER}(p) \) denote the balanced error rate for each group. The average balanced error rate across all groups is denoted as \( \text{BER}_{\text{avg}} \). A higher value of FairLen or FairPer indicates a greater disparity in classification performance across length-based or personality-based subgroups, respectively.




% FairLen and FairPer were used to measure the maximum discrepancy of BER for length and personality respectively.... these metrics are defined as ...

% [TODOD definition for fairlen + fairper??]

% [TODO explain figure 5 results in one whole paragraph]

% \begin{align}
%     Max discreapncy of feautre 1 <- how to define the FairLen
% \end{align}

% length and big-five personality traits respectively for a comprehensive assessment of both overall classification performance and subgroup-specific disparities. 

\subsection{Results with \name\ for AI-Text Detection}

Table~\ref{tab:comparative_table} presents a comprehensive comparison of three thresholding methodologies—\textbf{Static}, \textbf{ROCFPR}, and \textbf{FairOPT}—applied to four AI text detectors (\textbf{RoBERTa-base}, \textbf{RoBERTa-large}, \textbf{RADAR}, \textbf{GPT4o-mini}) across three distinct datasets (\textbf{MAGE}, \textbf{Kaggle}, \textbf{SemEval}). The evaluation metrics include F1, ACC, FairLen (maximum BER discrepancy for text length), FairPer (maximum BER discrepancy for personality traits), and FPR. Overall across all datasets and tasks tested, using \name\ yields an average increase of 2.91\% in F1 Score and a decrease of 1.77\% in FairPer, thereby strictly improving the performance-fairness tradeoff. Overall accuracy was only reduced by a marginal 0.58\%, indicating that \name\ does not affect performance accuracy. These results indicate that \name\ effectively enhances the overall classification performance and reduces fairness disparities across subgroups, outperforming the \textbf{Static} threshold approach by achieving higher F1 scores and lower BER discrepancies related to stylistic features.

We now analyze the results of \name\ on each specific dataset. On the SemEval task, overall F1 scores show significant improvement, with RADAR achieving a notable increase of 7\% after running our \name\ algorithm. These findings imply that environments exhibiting patterns similar to SemEval can greatly benefit from this method. Additionally, the RoBERTa-large model exhibits a substantial reduction in discrepancy across all datasets that it was tested on - in the MAGE dataset, the discrepancy related to writing style based on personality decreases by 5\%. This implies that models that work similarly to RoBERTa-large can pursue better fairness through our method. For example, RoBERTa-based models also exhibit reduced disparities in text length and style. These results provide strong evidence to support the future application of \name\ to other models, particularly detectors currently under development, as outlined in \citet{openai2023classifier}.

\subsection{Tradeoff Analysis between Performance and Fairness}

Figure \ref{fig:combined_figures} illustrates the accuracy-fairness tradeoffs achieved by our proposed \name\ algorithm compared to the Static and ROCFPR baselines. The relaxed fairness criterion from Section \ref{sec:fairness_criterion} effectively navigates the trade-off space, enhancing the Pareto frontier between classification performance and fairness metrics. Especially, the relaxed fairness criterion leads to early termination of the algorithm before overly sacrificing ACC while still satisfying a reasonable level of fairness. In contrast, the ROCFPR baseline, while achieving lower FPRs as noted in Table \ref{tab:comparative_table}, significantly compromises TPR thus harming overall ACC, highlighting the superiority of \name\ in maintaining a harmonious balance between constraints and model performance.

The ROCFPR method demonstrates a notable ability to minimize the FPR across various classifiers and datasets. However, this reduction in FPR comes at a significant cost to the TPR, leading to a sacrifice of ACC as illustrated in Appendix \ref{method2}. This trade-off is particularly problematic as it diminishes the detector's reliability in correctly identifying AI-generated text. 

Conversely, the adaptive thresholding approach employed by \name\ offers advantages in F1 and group fairness over both static and ROCFPR methods by tailoring decision boundaries to subgroup-specific probability distributions. This strategy ensures that each subgroup, characterized by attributes such as text length and personality traits, is evaluated with an appropriate threshold, thereby balancing the trade-off between fairness and performance. \name\ achieves this balance by improving F1 scores and reducing FairPer discrepancies without substantially sacrificing ACC, thereby enhancing the classifier's overall performance and addressing fairness.

\section{Conclusion}

This paper proposed \name, based on the observation that there is a statistically significant difference in AI-generated text classifier outputs across subgroups of text (e.g. length and personality) and that this discrepancy can lead to notably higher error rates in specific groups. To address this problem, we proposed \name, a new method for group-specific threshold optimization in AI-generated content classifiers without sacrificing performance. Unlike a universal threshold, which presumes well-calibrated probabilities across varied text distributions, our method effectively captures the variations of the probability distribution across different subgroups, leading to a more robust and fair classification.

While our work focuses primarily on text classifiers, the \name\ framework can naturally extend to other modalities where detecting AI-generated content is critical for safety and copyright (e.g., image, video, and audio). This method will be highly beneficial in post-processing, guiding the model toward enhanced performance or increased fairness, contingent upon the specific requirements and deployment context. Furthermore, while our primary focus has been on improving fairness in the post-training stage, we hypothesize that the proposed method can also be integrated as an in-training fairness enhancement mechanism. Finally, other approaches for threshold optimization, including Bayesian methods and convex optimization, can also be studied in future research.

\section*{Impact Statement}
This paper introduces novel theoretical frameworks and methodologies to enhance the robustness of AI-generated text detection systems. By extracting and using features for subgroups such as text length and stylistic traits, our approach addresses disparity in classification outcomes. This technology can play a crucial role in AI detection for identifying misinformation \cite{chen2024combating}, safeguarding the integrity of publication and academic organizations \cite{wu2023survey, perkins2023academic}, and countering potential cybersecurity threats \cite{yao2024survey}. Since it is crucial for these AI-content detection methods to be robust and fair across many potential users, our method takes a major step in this direction by formulating the problem setting and developing a new algorithm. 



% This paper presents new theory and methods for improving the fairness of AI-generated text detection. Our findings can have a broad impact on developing robust strategies ...

% [WE CAN USE THIS - HOW IS IT USEFUL]
% [WE EXTRACTED FEATURE - SO RELATED TO THE SENITIVE FEATURES]

% the technologies we develop can help identify and reduce the spread of misinformation \cite{chen2024combating}, ensure the integrity of publication standards and academic standards....

% \cite{wu2023survey}, potential cybersecurity threats \cite{yao2024survey}, and breaches of academic integrity \cite{perkins2023academic}. 


% \PYB{We can also add that our focus is on improving fairness post-training; however, we believe our method might also be beneficial to be used as an in-training fairness enhancement method, for which we leave as a promising future  direction.}


% \paragraph{Expanded Fairness Metrics}
% Although this work focuses on demographic parity and equality of odds, other formulations, such as predictive parity or calibration fairness, could give additional considerations regarding the fairness issue. 

% \paragraph{Hyperparameter Tuning}
% Our method’s effectiveness depends on hyperparameters such as the penalty term for F1, the learning rate for threshold updates, and range for clipping, and acceptable disparity levels are subject to additional exploration to figure out a better performance of the model and the model performance could be significantly improved by using different hyperparameters.



\section*{Acknowledgment}
Thanks to Hengzhi Li, Megan Tjandrasuwita, David Dai, and Jeongmin Kwon, Chanakya Ekbote for constructive feedback and discussion.



%  balanced error - https://arxiv.org/pdf/2209.05355

{\small
\bibliography{main}
\bibliographystyle{icml2023}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Performance and fairness metrics}
\subsection{Contingency table}\label{table_ai_detection}
The model can make not only true positive (TP) and true negative (TN) cases but also false positive (FP) and false negative (FN):
\begin{table}[H]
    \centering
    % \begin{tabular}{|p{0.1\textwidth}|p{0.25\textwidth}|p{0.25\textwidth}|}
    \begin{tabularx}{\linewidth}{|p{0.2\linewidth}|X|X|}
        \hline
        & Classified as AI (\(\hat{Y_1}\)) & Classified as non-AI (\(\hat{Y_0}\)) \\
        \hline
        AI-generated ($Y_1$) & \textbf{TP}: Correct identification & \textbf{FN}: Failed to detect AI and misidentified as human\\
        \hline
        Human-developed ($Y_0$) & \textbf{FP}: Incorrectly marked human work as AI & \textbf{TN}: Correctly identified by not classifying human work as AI \\
        \hline
    \end{tabularx}
    \caption{Confusion matrix for the four possible outcomes when an AI detection system classifies either AI-generated or human-developed content.}
    \label{tab:table_ai_detection}
\end{table}

\subsection{Notation and Metrics}\label{appendix:metrics}
Given the table \ref{table_ai_detection} that denotes the outcomes in a binary classification task, we can mathematically express the performance and fairness metrics like below: 

\begin{itemize}
    \item \textbf{Accuracy (ACC).} 
    \[
    \mathrm{ACC} 
    \;=\; 
    \frac{\mathrm{TP} + \mathrm{TN}}{\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN}}.
    \]

    \item \textbf{F1 Score.} Given \(\mathrm{Precision}\) and \(\mathrm{Recall}\) below,
    \[
    \mathrm{F1} 
    \;=\; 
    2 \times \frac{(\mathrm{Precision}) \times (\mathrm{Recall})}
                     {\mathrm{Precision} \;+\; \mathrm{Recall}}.
    \]

    F1 matters because it balances Precision and Recall into a single measure, making it especially useful when the class distribution is imbalanced or when both metrics are important.

    \item \textbf{Precision.}
    \[
    \mathrm{Precision} 
    \;=\; 
    \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}
    \]

    Precision measures the fraction of predicted positives that are actually correct, focusing on how often the model is right when it predicts a positive.

    \item \textbf{Recall.}  \quad (Recall is also referred to as TPR in some contexts.)
    \[
    \mathrm{Recall} 
    \;=\; 
    \mathrm{TPR}
    \;=\;
    \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}
    \]

    Recall measures the fraction of actual positives that are correctly identified by the model, capturing how many real positives are detected.
 
    \item \textbf{Demographic Parity (DP).}  
    Suppose we have a protected attribute \(A \in \{0,1\}\). Demographic Parity requires 
    \(P(\hat{Y}=1 \mid A=0) = P(\hat{Y}=1 \mid A=1)\), or  \(P(\hat{Y}=1 \mid A=0) \approx P(\hat{Y}=1 \mid A=1)\). A common metric is the disparity:
    \[
    \Delta_{\mathrm{DP}} 
    \;=\;
    \bigl|\,
    P(\hat{Y}=1 \mid A=0) 
    \;-\; 
    P(\hat{Y}=1 \mid A=1)
    \bigr|.
    \]

    \item \textbf{Equalized Odds (EO).}  
    Equalized Odds requires equal \emph{true positive rates} and \emph{false positive rates} across groups. One way to measure the gap is:
    \[
    \Delta_{\mathrm{EO}} 
    \;=\;
    \bigl| 
      P(\hat{Y}=1 \mid Y=1,A=0) 
      \;-\; 
      P(\hat{Y}=1 \mid Y=1,A=1) 
    \bigr|
    \;+\;
    \bigl| 
      P(\hat{Y}=1 \mid Y=0,A=0) 
      \;-\; 
      P(\hat{Y}=1 \mid Y=0,A=1) 
    \bigr|.
    \]
    Under the strictest notion of fairness, one requires 
    \[
        \Delta_{\mathrm{EO}} \;=\; 0,
    \]
    implying no disparity in true or false positive rates across groups.

    Relaxation in fairness allows
    \[
        \Delta_{\mathrm{EO}} \geq 0,
    \]
    for feasibility.
    
    \item \textbf{False Positive Rate (FPR).}
    \[
    \mathrm{FPR} 
    \;=\; 
    \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}}
    \]

    \item \textbf{Balanced Error Rate (BER).}
    \[
    \mathrm{BER} 
    \;=\; 
    \frac{1}{2} \left( \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}} + \frac{\mathrm{FN}}{\mathrm{TP} + \mathrm{FN}} \right)
    \;=\; 
    \frac{\mathrm{FPR} + (\mathrm{FNR})}{2}
    \;=\; 
    \frac{\mathrm{FPR} + (\mathrm{1-TPR})}{2}
    \]

    Balanced Error Rate (BER) averages the error rates across both classes, providing a metric that is insensitive to class imbalance. By considering both the False Positive Rate and the False Negative Rate, BER ensures that the classifier performs equally well on both classes. The concept is especially well-introduced in \cite{ferrer2022analysis}
    
    \item \textbf{Specificity.}
    \[
    \mathrm{Specificity} 
    \;=\; 
    \frac{\mathrm{TN}}{\mathrm{TN} + \mathrm{FP}}
    \]
    Specificity measures the proportion of actual negative cases correctly identified by the model, reflecting its ability to detect true negatives. It is also known as true negative rate (TNR).
\end{itemize}

\subsection{Impossibility theorem}
The impossibility theorem typically refers to the result that no single classifier can satisfy all common fairness metrics (e.g., demographic parity, equalized odds, predictive parity) simultaneously, except in trivial cases. This arises because different fairness definitions can be mathematically incompatible when the base rates (prevalences) differ across groups. This research used one fairness metrics in the processing to address this issue. 


\section{AUROC}\label{method2}
\subsection{Static threshold and AUROC-based approach} 
In this research, the AUROC-based approach is employed to optimize the classification threshold while balancing model performance and fairness constraints. This method evaluates the trade-off between the TPR and FPR, as visualized by the ROC curve shown in Figure \ref{fig:method2}. The optimal threshold is determined under a specified FPR constraint, $\text{FPR} \leq 0.1$, ensuring that the selected threshold aligns with a predefined ethical constraint.


\begin{figure}
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/method2.png}
    \vspace{-5mm}
    \caption{Threshold optimization process using the AUROC-based approach with an FPR constraint of 0.1. The ROC curve (blue line) demonstrates the trade-off between TPR and FPR. The optimal threshold (green marker) is selected to maximize performance under the specified FPR constraint.}
    \vspace{-5mm}
    \label{fig:method2}
\end{figure}


% \section{Kolmogorov--Smirnov Test} \label{appendix:ks_test}

% Given two independent samples
% \(
% X = \{x_1, x_2, \ldots, x_n\}
% \)
% and
% \(
% Y = \{y_1, y_2, \ldots, y_m\},
% \)
% each drawn from (potentially) different continuous distributions, the \emph{two-sample Kolmogorov--Smirnov test} assesses whether these two samples come from the same underlying distribution.

% \paragraph{Empirical Distribution Functions (ECDF).}
% For sample \(X\), its empirical distribution function \(F_n(x)\) is defined as
% \[
% F_n(x) \;=\; \frac{1}{n}\,\sum_{i=1}^{n} \mathbf{1}\{\; x_i \le x \;\},
% \]
% where \(\mathbf{1}\{\cdot\}\) is an indicator function that equals \(1\) if the condition inside is true and \(0\) otherwise. Similarly, for sample \(Y\), the empirical distribution function \(G_m(x)\) is given by
% \[
% G_m(x) \;=\; \frac{1}{m}\,\sum_{j=1}^{m} \mathbf{1}\{\; y_j \le x \;\}.
% \]

% \paragraph{Kolmogorov--Smirnov statistic}
% The K-S test statistic \(D_{n,m}\) measures the maximum vertical distance between these two empirical distribution functions:
% \[
% D_{n,m}
% \;=\;
% \sup_{x \in \mathbb{R}}\,
% \bigl|\,F_n(x) \;-\; G_m(x)\bigr|.
% \]
% In other words, it captures the \emph{greatest} difference between \(F_n(x)\) and \(G_m(x)\) over the real line.

% \paragraph{p-Value computation}
% Under the null hypothesis \(H_0\) (that both samples \(X\) and \(Y\) are drawn from the same continuous distribution), the distribution of \(D_{n,m}\) can be approximated using known asymptotic results. The resulting \emph{p}-value reflects the probability of observing a distance \(\sup_x\!|F_n(x) - G_m(x)|\) at least as large as \(D_{n,m}\) if \(X\) and \(Y\) actually come from the same distribution. A small \emph{p}-value (e.g., below 0.05) typically indicates that we can reject \(H_0\) and conclude that the two samples differ significantly.

% In this work, we utilize the \texttt{ks\_2samp} function from \texttt{scipy.stats} in Python, which implements the above procedure. For two probability score distributions \(\{x_1, \dots, x_n\}\) and \(\{y_1, \dots, y_m\}\), it returns both the K-S statistic \(D_{n,m}\) and the associated \emph{p}-value, providing a straightforward way to verify if subgroup distributions differ in a statistically significant manner.

\section{Applied Thresholds}\label{applied_threhsolds}
Figure \ref{fig:application_probability} uses thresholds that is noted on Table \ref{tab:applied_thresholds}.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Threshold Type} & \textbf{Value} \\
\hline
Static & 0.5 \\
\hline
ROCFPR & 0.9984415 \\
\hline
\multicolumn{2}{|c|}{\textbf{Adaptive Thresholds (n=15)}} \\
\hline
Group & Threshold \\
\hline
long\_agreeableness & 0.2545000 \\
long\_conscientiousness & 0.3055681 \\
long\_extroversion & 0.2500000 \\
long\_neuroticism & 0.2748645 \\
long\_openness & 0.2549773 \\
medium\_agreeableness & 0.2445749 \\
medium\_conscientiousness & 0.2448965 \\
medium\_extroversion & 0.1000000 \\
medium\_neuroticism & 0.2429920 \\
medium\_openness & 0.2436330 \\
short\_agreeableness & 0.2529650 \\
short\_conscientiousness & 0.2480741 \\
short\_extroversion & 0.2528146 \\
short\_neuroticism & 0.2528743 \\
short\_openness & 0.2489719 \\
\hline
\end{tabular}
\caption{Thresholds that used in the study}
\label{tab:applied_thresholds}
\end{table}



\section{Feature Engineering}\label{feature_engineering}

In order to mitigate disparities within AI classifiers, a thorough feature engineering process was undertaken. Features were systematically extracted to characterize each text within the dataset. The table \ref{featuretable_original} includes all extraced features for this research. The primary focus of the experiment encompassed text length and anticipated personality traits. Although formality and sentiment were also extracted, these attributes are utilized to expedite processing, as the computational time increases exponentially with the inclusion of additional features. The subsequent table illustrates the features primarily extracted for the experiment, with an emphasis solely on the utilization of length and personality traits.

This research mainly used \textbf{length} and \textbf{personality} as a main feature in using \name.

The \textbf{text length} attribute measures the number of characters in a text. OpenAI's evaluations \cite{openai2023classifier} revealed performance disparities between short (under 1,000 characters) and long texts when using their AI classifier. While their global threshold approach failed to address group-specific differences, it provided a valuable benchmark. Building on this and using exploratory data analysis (EDA), we refined the thresholds to 1,000 and 2,500 characters, categorizing texts as short (up to 1,000), mid-length (1,000–2,500), or long (over 2,500).

The \textbf{personality} attribute includes five traits: extroversion, neuroticism, agreeableness, conscientiousness, and openness. This attribute contributes to the enforcement of fairness metrics, such as demographic parity and equality of odds. For this feature, we determine the personality trait by selecting the one with the highest probability among the five using the \cite{personality_prediction} model, which is fine-tuned from BERT-base-uncased \cite{BERT2018}.

\begin{table}[h!]
\centering
\begin{tabularx}{\columnwidth}{|l|X|X|}
\hline
\textbf{Attribute} & \textbf{Characteristics} & \textbf{Labels} \\
\hline
Text Length & Measures the number of characters in a text & short, medium, long (n = 3) \\
\hline
Formality & Indicates the level of formality in the language & formal, informal (n = 2) \\
\hline
Sentiment & Reflects the emotional tone. & positive, negative (n= 2) \\
\hline
Personality & Represents the inferred Big Five personality traits. & Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism (n = 5) \\
\hline
\end{tabularx}
\caption{Characteristics and labels of each attribute}
\label{featuretable_original}
\end{table}

Outside of the \textbf{length} and \textbf{sentiment}, the \textbf{formality} attribute measures a text's formality by analyzing the frequency of specific parts of speech. According to \cite{Formality_Heylighen2002}, with formal texts typically containing a higher proportion of nouns, adjectives, articles, and prepositions, whereas informal texts tend to have a greater frequency of pronouns, adverbs, verbs, and interjections. As general rule, we set a threshold of 50 to distinguish between formal (scores above 50) and informal texts.

The \textbf{sentiment} attribute allows for the assessment of emotional tone and attitudes within content. AI-generated texts may display distinct sentiment patterns due to the differences in their underlying algorithms compared to human expression. To classify texts as positive or negative, we used the label output from the \cite{sentiment_prediction} model, which is a fine-tuned version of DistilBERT-base-uncased \cite{DistilBERTAD2019}, specifically trained on the SST-2 dataset\cite{SST-2_2013}.

\section{Discrepancies by classifiers with additional features}
\subsection{Discrepancy of Probability in Subgroups}

\subsubsection{Discrepancy in RoBERTa-large Detector}
The analysis revealed significant variability in the probability distributions between subgroup pairs, as depicted in Figure \ref{fig:discrepancy}. Employing two-sample Kolmogorov-Smirnov (KS) statistic for two-sided tests, we assessed distributional discrepancies across various text groups characterized by distinct feature combinations, under the assumption of a non-parametric distribution. Significant differences were identified in certain subgroup comparisons, indicating notable discrepancies in their probability distributions. For example, comparing texts characterized by \textit{conscientiousness, positive sentiment, formal style, and long length} with those exhibiting \textit{extroversion, negative sentiment, formal style, and long length} resulted in a KS statistic of 0.7286 ($p < 0.05$). Similarly, evaluating \textit{agreeableness, positive sentiment, formal style, and mid-length} against \textit{conscientiousness, positive sentiment, formal style, and long length} yielded a KS statistic of 0.7143 ($p < 0.01$).

Conversely, smaller discrepancies were observed between \textit{neuroticism, positive sentiment, formal style, and short length} versus \textit{openness, positive sentiment, formal style, and mid-length} (KS = 0.2706, $p < 0.05$) and \textit{neuroticism, negative sentiment, formal style, and short length} versus \textit{openness, positive sentiment, formal style, and mid-length} (KS = 0.2323, $p < 0.05$).

\begin{figure}
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/discrepancy_large4.png}
    \vspace{-5mm}
    \caption{Kernel Density Estimation (KDE) plots comparing probability distributions for subgroup pairs with the top two highest and lowest discrepancies based on the K-S test with statistical validity. The largest discrepancy is observed with a KS statistic of $0.7286$, while the smallest discrepancy has a KS statistic of $0.2323$.}
    \vspace{-5mm}
    \label{fig:discrepancy}
\end{figure}

\subsubsection{Discrepancy in RoBERTa-base Detector}
The analysis revealed a substantial variation in the probability distributions among pairs of subgroups, as shown in Figure \ref{fig:discrepancy_base4}. The KS test was applied to assess distributional differences across diverse text groupings defined by multiple feature sets, under the assumption of a non-parametric distribution. Specific subgroup comparisons highlighted significant differences, pointing to noticeable disparities in their probability distributions. For example, contrasting texts defined by \textit{agreeableness, positive sentiment, formal style, and mid-length} with those featuring \textit{neuroticism, negative sentiment, informal style, and short length} produced a KS statistic of 0.9 ($p < 0.01$). Furthermore, the comparison between \textit{conscientiousness, negative sentiment, formal style, and long length} and \textit{neuroticism, negative sentiment, informal style, and short length} resulted in a KS statistic of 0.7750 ($p < 0.01$). 



\begin{figure}
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/discrepancy_base4.png}
    \vspace{-5mm}
    \caption{KDE plots comparing probability distributions for subgroup pairs with the highest and lowest discrepancies based on the K-S test with statistical validity. The largest discrepancy is observed with a KS statistic of $0.9$, while the smallest discrepancy has a KS statistic of $0.5071$. These discrepancies indicate varying levels of divergence between groups based on their characteristics.}
    \vspace{-5mm}
    \label{fig:discrepancy_base4}
\end{figure}

Lesser discrepancies were noted between \textit{neuroticism, negative sentiment, informal style, and short length} versus \textit{openness, positive sentiment, formal style, and long length} (KS = 0.5522, $p < 0.05$) and \textit{neuroticism, negative sentiment, informal style, and short length} versus \textit{openness, positive sentiment, formal style, and short length} (KS = 0.5071, $p < 0.05$). \newline

\subsubsection{Discrepancy in RADAR Detector}
The investigation disclosed considerable variability in the probability distributions across subgroup pairs, as illustrated in Figure \ref{fig:discrepancy_radar4}. Through the application of the KS test, we evaluated distributional discrepancies among various text groups defined by distinct feature combinations, operating under the premise of a non-parametric distribution. Significant differences were discerned in particular subgroup comparisons, which highlight notable discrepancies in their probability distributions. For instance, the comparison between texts characterized by \textit{conscientiousness, negative sentiment, informal style, and long length} and those exhibiting \textit{extroversion, positive sentiment, formal style, and short length} yielded a KS statistic of 0.8 ($p < 0.05$). 

\begin{figure}
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/discrepancy_radar4.png}
    \vspace{-2mm}
    \caption{KDE plots comparing probability distributions for subgroup pairs with the highest and lowest discrepancies based on the K-S test with statistical validity. The largest discrepancy is observed with a KS statistic of $0.8$, while the smallest discrepancy has a KS statistic of $0.1830$.}
    \vspace{-5mm}
    \label{fig:discrepancy_radar4}
\end{figure} 

Similarly, the examination of \textit{conscientiousness, negative sentiment, informal style, and long length} against \textit{neuroticism, negative sentiment, formal style, and short length} resulted in a KS statistic of 0.6667 ($p < 0.05$). In contrast, smaller discrepancies were detected between \textit{conscientiousness, negative sentiment, formal style, and long length} versus \textit{openness, positive sentiment, formal style, and mid-length} (KS = 0.2110, $p < 0.01$) and \textit{openness, negative sentiment, formal style, and long length} versus \textit{openness, positive sentiment, formal style, and mid-length} (KS = 0.1830, $p < 0.01$). \newline

\subsubsection{Discrepancy in GPT4o-mini Detector}
The analysis revealed substantial variability in the probability distributions among subgroup pairs, as depicted in Figure \ref{fig:discrepancy_gpt4}. Using the KS test, we assessed distributional discrepancies across various text groups defined by distinct feature combinations, assuming a non-parametric distribution. Significant differences were detected in particular subgroup comparisons, suggesting notable divergences in their probability distributions. For example, the comparison between texts characterized by \textit{openness, negative sentiment, informal style, and mid-length} and those exhibiting \textit{openness, positive sentiment, informal style, and mid-length} resulted in a KS statistic of 0.6667 ($p < 0.001$). Similarly, the evaluation of \textit{extroversion, negative sentiment, formal style, and mid-length} versus \textit{openness, positive sentiment, informal style, and mid-length} produced a KS statistic of 0.6522 ($p < 0.001$). Conversely, smaller discrepancies were noted between \textit{neuroticism, positive sentiment, formal style, and short length} versus \textit{openness, negative sentiment, formal style, and long length} (KS = 0.2853, $p < 0.05$) and \textit{neuroticism, negative sentiment, formal style, and short length} versus \textit{openness, negative sentiment, formal style, and long length} (KS = 0.2560, $p < 0.05$).

\begin{figure}
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/discrepancy_GPT44.png}
    \vspace{-2mm}
    \caption{KDE plots comparing probability distributions for subgroup pairs with the highest and lowest discrepancies based on the K-S test with statistical validity. The largest discrepancy is observed with a KS statistic of $0.6667$, while the smallest discrepancy has a KS statistic of $0.2560$.}
    \vspace{-2mm}
    \label{fig:discrepancy_gpt4}
\end{figure}

\section{Operationalizing \name}
\subsection{Algorithmic description}\label{appendix:algorithm}

\newlength{\textfloatsepsave}
\setlength{\textfloatsepsave}{\textfloatsep}
\setlength{\textfloatsep}{0pt}
\begin{algorithm}[!htbp]
\begin{algorithmic}[1]
\footnotesize

\STATE \textbf{Input:} Labeled dataset $D = \{(x_n, y_n)\}$, predicted probabilities $\{p_n\}$, subgroup labels $\{g_n\}$ with possible groups $G_1,\dots,G_j$, features \(\{S_1,\dots,S_n\}\), initial thresholds $\theta_{init}$, fairness metrics $\{M_1, \ldots, M_k\}$ with acceptable disparity $\delta_{\mathrm{fair}}$, minimum performance thresholds: $\alpha$ (ACC) and $\beta$ (F1), penalty weight $\kappa$, learning rate $\eta$, tolerance $\epsilon_{\mathrm{tol}}$, maximum iterations, finite-difference step $\delta$, minimum probability for threshold $a$, maximum probability for threshold $b$ 


% \yi{1) Seems that you use "S" in Sec 3.1 but now "g" to denote subgroup in algorithm? Try to be more consistent; 2) Nowhere else in Sec 3 talks about "C"? Also be more consistent in terminology or elaborate better in main section writing}


\STATE \textbf{Output:} Subgroup thresholds $\{\theta^*(G_1),\dots,\theta^*(G_j)\}$
\STATE \textbf{Initialization:}
% \FOR{each group $G_i$}
%     \STATE $\theta(G_i) \leftarrow \theta_{init}$    
% \ENDFOR
% \STATE $\text{iteration} \leftarrow 0$
% \STATE $\text{previousThresholds} \leftarrow \theta(\cdot)$
\STATE split $D$ to ${(G_1),\dots,(G_j)}$  by $S$
\WHILE{\text{iteration} $<$ \text{maximum iterations}}
    \FOR{each group $G_j$}
        \STATE $\mathcal{I}_i \leftarrow \{\,n \mid g_n = G_j\}$ \quad \com{all samples in group $G_j$}
        \STATE $\hat{y}_n \leftarrow [\,p_n \ge \theta(G_j)\,], \;\forall n \in \mathcal{I}_i$
        \STATE Store contingency table for $G_i$ using $\hat{y}_n$ and ${y}_n$
        \STATE $\Omega \;\leftarrow\; \min_i\{\mathrm{ACC}(G_j)\} \;\ge\; \alpha \;\wedge\;\min_i\{\mathrm{F1}(G_j)\} \;\ge\; \beta$ \com{boolean output}
        \FOR{each fairness metric $M_k$}
            \STATE Compute $\{M_k(G_1),\dots,M_k(G_j)\}$
            \STATE $\Delta_{k} \leftarrow \max_i(M_k(G_j)) - \min_i(M_k(G_j))$
            \STATE $\Psi \;\leftarrow\; \Bigl[\,\max_k \Delta_k \;\le\; \delta_{\mathrm{fair}}\Bigr]$ \com{boolean output}
        \ENDFOR
        \FOR{each group $G_j$}
            \STATE $L_i(\theta) = -\mathrm{Acc}_j(\theta) + \kappa\,[\,\beta - \mathrm{F1}_j(\theta)\,]_{+}$
            \STATE
            \[
                \nabla L_i(\theta(G_j))
                \;\approx\;
                \frac{L_i(\theta(G_j) + \delta) \;-\; L_i(\theta(G_j) - \delta)}{2\,\delta}.
            \]
            \STATE
            \[
                \theta(G_j) \;\leftarrow\; \theta(G_j) \;-\; \eta \;\nabla L_i(\theta(G_j)).
            \]
            \STATE Clip $\theta(G_j)$ to stay within $[a,b]$
        \ENDFOR        
    \ENDFOR
    \STATE $\Delta_\theta \;\leftarrow\; \max_{G_j} \bigl|\theta(G_j) - \text{previousThresholds}[G_j]\bigr|$
    \IF{($\Delta_\theta < \epsilon_{\mathrm{tol}} \;\wedge\; \Omega \;\wedge\; \Psi$)}
        \STATE \textbf{break}
    \ENDIF

    \STATE $\text{previousThresholds} \leftarrow \theta(\cdot)$
    \STATE $\text{iteration} \leftarrow \text{iteration}+1$
\ENDWHILE
\STATE \textbf{return} $\{\theta(G_1),\dots,\theta(G_j)\}$

\caption{\name: Gradient-based adaptive threshold optimization with fairness for subgroups}
\label{algorithm_fairopt2}
\end{algorithmic}
\end{algorithm}
\setlength{\textfloatsep}{\textfloatsepsave}

\subsection{Early stopping method for the tradeoff curve}\label{early_stopping}

An optimization algorithm was implemented using a function from the \texttt{scipy.optimize} library, aiming to determine probability thresholds that satisfy specific fairness constraints \texttt{(acceptable\_disparity =  [1.00, 0.30, 0.25, 0.21. 0.20, 0.19, 0.15, 0.10])} while ensuring minimum performance thresholds for both performance criteria (ACC and F1). The optimization in the \texttt{minimize} function was performed using the \texttt{L-BFGS-B} method. Early stopping is facilitated by the default values of \texttt{ftol} and \texttt{gtol} in \texttt{L-BFGS-B}. Additionally, the \texttt{learning rate} was set to \(10^{-3}\) to enable controlled and stable updates during optimization, and a \texttt{penalty} parameter was set to \(10\) to strengthen the L2 penalty in the loss function. Furthermore, minimum performance thresholds for accuracy and F1-score were enforced based on the performance observed for each group when applying a static threshold of 0.5 and a ROC-based threshold. As stricter fairness constraints are imposed, these minimum thresholds are gradually relaxed. 

% The optimization process is guided by a loss function that applies an \texttt{L2 penalty} when the required levels of accuracy, F1-score, or positive prediction rate (PPR) within each group are not met.

% Eventually, when aiming for high level of fairness ( e.g. \texttt{disparity = 0.10}),  performance constraints are no longer considered. Similarly, the probability  thresholds values for each group were constrained within the range \([0.00005, 0.99995]\) to prevent extreme values. Nonetheless, as stricter fairness constraints are imposed, this restriction is removed under highly demanding fairness criteria.


\subsection{Hyperparameter settings for the FairOPT}\label{hyperparameters}

The application of the \name\ is configured with a set of hyperparameters designed to ensure a balance between fairness and performance during the threshold optimization process. The \texttt{learning rate is set to $10^{-3}$} to enable controlled and stable updates during optimization. \texttt{The maximum number of iterations is specified as $10^5$} to provide sufficient time for the optimization algorithm to converge. An acceptable disparity of 0.2 is defined to regulate subgroup fairness, ensuring that performance differences across subgroups remain within an acceptable range through the relaxed fairness. Minimum \texttt{thresholds for accuracy (0.25) and F1-score (0.25)} are enforced to maintain baseline model performance and stability, particularly in cases of imbalanced datasets. A \texttt{tolerance level of $10^{-2}$} is introduced to impose convergence criteria, ensuring the optimization process halts only when the changes in the objective function are negligible. Furthermore, a \texttt{penalty parameter of 20} is applied to enforce performance constraints by penalizing significant performance disparities across subgroups. For fairness criteria, Demographic Parity (DP) was employed to test whether the generated thresholds meets the expected fairness. Also, this experiment used early stopping method for effective processing. The early stopping check through the change of the ACC, F1, and maximum DP discrepancy of each group, and terminates if all of them shows minimal change based on predefined patience. 

\end{document}






% OLD ALGORITHM FROM ETHICAL MACHINE LEARNING
% \newlength{\textfloatsepsave}
% \setlength{\textfloatsepsave}{\textfloatsep}
% \setlength{\textfloatsep}{0pt}
% \begin{algorithm}
%     \caption{FairOPT threshold optimization with fairness}
%     \label{alg:method}
%     \begin{algorithmic}[1]
%         \footnotesize
%         \STATE \textbf{Input:} Dataset \( D \), features \( C_1, \ldots, C_n \), initial threshold \( \theta_{\text{initial}} \), Fairness metrics \( \{M_1, \ldots, M_k\} \), relaxation \( \tau \), max iterations, adjustment rate \( \epsilon \)
%         \STATE \textbf{Output:} Optimized thresholds \( \{\theta_1, \ldots, \theta_j\} \)
%         \STATE Split dataset \( D \) into subgroups \( \{G_1, \ldots, G_j\} \) according to  \( C_1, \ldots, C_n \)
%         \STATE Initialize \( \theta_j \) for each \( G_i \) to \( \theta_{\text{initial}} \)
%         \FOR{\( t = 1 \) to max iterations}
%             \FOR{each subgroup \( G_i \)}
%                 \STATE Convert probabilities to binary outcomes using \( \theta_j \) for \( G_i \)
%                 \STATE Compute the confusion matrix for \( G_i \)
%                 \FOR{each fairness metric \( m \) in \( M_k \)}
%                     \STATE Compute \( \text{ratio}_{ij} \gets \frac{\min m_{ij}(\theta_j)}{\max m_{ij}(\theta_j)} \)
%                     \IF{\( \text{ratio}_{ij} < (1 -\tau) \)}
%                         \STATE \( \theta_j \gets \min(\theta_j + \epsilon, 1) \)
%                     \ELSIF{\( \text{ratio}_{ij} > (1 -\tau) \)}
%                         \STATE \( \theta_j \gets \max(\theta_j - \epsilon, 0) \) 
%                     \ELSE
%                         \STATE {\( \theta_j \gets \theta_j \)}
%                     \ENDIF
%                 \ENDFOR
%             \ENDFOR
%         \ENDFOR
%         \STATE \textbf{Return:} Optimized thresholds \( \{\theta_1, \ldots, \theta_j\} \)
%     \end{algorithmic}
% \end{algorithm}
% \label{algorithm}
% \setlength{\textfloatsep}{\textfloatsepsave}



% \subsection{Group-Adaptive Threshold Optimization}

% The algorithm \ref{algorithm_fairopt} operates by splitting the dataset \(D\) into multiple subgroups \( \{G_1, \ldots, G_j\} \) based on the combination of designated features \( \{C_1, \ldots, C_n\} \). For example, for \(n\) binary features, the algorithm yields \(2^n\) distinct subgroups, thus enabling adaptive treatment of various protected or sensitive attributes. Each subgroup \( G_i \) is initially assigned a threshold \( \theta_{\text{initial}} \), which is iteratively updated according to fairness metrics and criteria.

% Unlike conventional strict-fairness methods, the proposed model employs a relaxation parameter \( \tau \) that governs how closely each subgroup's performance metrics must align. By allowing controlled deviations from perfect parity, the method achieves faster convergence and can better preserve classification accuracy.

% During each iteration, predictions are generated by applying the current threshold \(\theta_j\) to subgroup \(G_i\), leading to a subgroup-specific confusion matrix. The relevant fairness metrics \(\{M_1, \ldots, M_k\}\) are then computed to measure disparities within the subgroup. If \(\text{ratio}_{ij} < (1-\tau)\), indicating insufficient fairness, the threshold \(\theta_j\) is increased by the adjustment rate \(\epsilon\). Conversely, if \(\text{ratio}_{ij} > (1-\tau)\), the threshold is decreased to balance performance across subgroups. This loop repeats until the model converges or the prescribed iteration limit is reached. Through this process, the method yields an optimized set of thresholds \(\{\theta_1, \ldots, \theta_j\}\) across all subgroups, accounting for fairness considerations. 


% This is about the text length
\begin{figure}
    \centering 
    \includegraphics[width=\columnwidth]{figures/probability_distribution.png}
    \vspace{-5mm}
    \caption{The histograms display the frequency distribution for each length category, emphasizing variations in classification accuracy and threshold suitability for different text lengths. The adaptive thresholds help mitigate the classification bias associated with text length variations. Histograms of AI-generated probability distribution for AI-written (top) and Human-written (bottom) texts, classified into Short (below 1000 characters; red), Medium (between 1000 and 2500 chars; green), and Long (more than 2500 chars; blue). Each subplot illustrates the distribution of AI-assigned probability scores for these categories, with smoothed density curves representing each length group. The black dashed line marks a standard threshold of 0.5 for AI-generated classification, while colored dashed lines represent adaptive thresholds for each text length}
    \vspace{-2mm}
    \label{fig:probability_distribution}
\end{figure}







% This document used the format of the ICML 2023 paper: Mitchell, E., Lee, Y., Khazatsky, A., Manning, C. D., & Finn, C. (2023, July). Detectgpt: Zero-shot machine-generated text detection using probability curvature. In International Conference on Machine Learning (pp. 24950-24962). PMLR.
% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.


