\section{Background}
\label{sec:background}

\textbf{LLM inference phase.} The inference process in LLMs consists of two main phases: the prefill phase and the decoding phase. During the prefill phase, the model processes the input prompt to compute the key-value (KV) cache and generates the first token in a single step. In contrast, the decoding phase uses the last generated token and the KV cache as inputs to generate subsequent tokens in a token-by-token manner. Generally, the prefill phase is compute-bound, while the decoding phase is memory-bound.

\textbf{Workload heterogeneity.} LLMs are designed to support a diverse range of applications, and the inference workloads associated with these applications often exhibit heterogeneity in input and output token lengths, which is called \textit{workload heterogeneity}. Different workload types exhibit varying characteristics in terms of compute and memory demands. For example, requests from the WildGPT dataset~\cite{zhao2024wildchat}, with average input and output token lengths of 496 and 510 respectively (classified as short input and long output), typically require more memory resources to handle the memory-bound decoding phase. Conversely, requests from the Azure-Trace dataset~\cite{patel2024splitwise,azuredataset}, with average input and output token lengths of 2455 and 18 respectively (classified as long input and short output), generally demand more compute resources to manage the compute-bound prefill phase. Therefore, appropriately allocating resources based on workload demands is critical for optimal performance.

% \textbf{Parallelism strategies.} Several parallelism strategies have been proposed to efficiently parallelize LLM inference across multiple GPUs. Recent research has demonstrated that adopting appropriate parallelism strategies can significantly enhance LLM serving performance. The three most popular approaches are tensor parallelism, pipeline parallelism, and data parallelism (also referred to as model replication).

% \textit{Tensor parallelism.} Tensor parallelism (TP) divides model shards and computationally intensive operations (e.g., matrix multiplication) across multiple GPUs to reduce the memory burden on a single GPU. By splitting both data scanning and processing tasks, it helps minimize LLM inference latency. This approach requires two all-reduce operations to synchronize intermediate activations between transformer blocks, necessitating high inter-GPU communication bandwidth for efficient implementation. TP is typically used with intra-machine GPUs connected via NVLink or PCIe to ensure fast data transmission.

% \textit{Pipeline parallelism.} Pipeline parallelism (PP) divides the LLM into several pipeline stages, with each GPU responsible for one stage. Communication between stages involves inter-stage activations, and the computation and communication proceed in a pipeline manner. Unlike TP, PP does not reduce single-request inference latency, as it introduces additional communication overhead during the inference phase. However, PP can significantly increase the throughput of LLM serving by asynchronously processing each pipeline stage, and is adaptable to a broader range of communication bandwidths due to the relatively lower communication volume between stages.

% \textit{Data parallelism.} Data parallelism (DP), also known as model replication, replicates the entire model across multiple GPUs. DP enhances the parallel processing capability of the system and linearly increases the serving throughput, as it requires no communication overhead for synchronizing information between replicas. However, DP introduces additional memory overhead to maintain extra model parameters, which can become memory-intensive for models with a large parameter count.

% \textit{Hybrid parallelism.} Each parallelism strategy has distinct advantages and limitations. DP improves request processing throughput but incurs significant memory overhead. TP minimizes request latency but demands high communication bandwidth due to large communication volumes. PP maximizes throughput but increases latency for individual requests. To balance these trade-offs, numerous studies have focused on optimizing hybrid parallelism—a combination of different parallelism strategies—to enhance LLM serving efficiency.

\textbf{Heterogeneous LLM serving.} Recent research has explored various approaches for deploying LLM serving in heterogeneous GPU environments to achieve cost-efficient solutions~\cite{jiang2023hexgen,miao2024spotserve,griggs2024m,zhao2024llm,mei2024helix,borzunov2022petals}. 
% SpotServe proposes leveraging spot instances to reduce LLM serving costs. 
HexGen introduces asymmetric partitioning and advanced scheduling techniques to deploy generative inference in decentralized and heterogeneous settings. Me'lange frames GPU allocation as a cost-aware bin-packing problem, optimizing cost efficiency for LLM services by effectively leveraging heterogeneous GPU types. 
% LLM-PQ~\cite{zhao2024llm} supports adaptive model quantization and phase-aware partitioning to enhance LLM serving efficiency in heterogeneous GPU clusters. 
Helix formulates the problem of heterogeneous GPU and network connection optimization as a max-flow problem, utilizing mixed-integer linear programming to determine the optimal model deployment.
However, existing works typically optimize performance within a predefined heterogeneous cluster, and fail to consider GPU availability and user-defined budget constraints on cloud platforms. 
In addition, they are generally unaware of the workload heterogeneity, and only consider uniform workload assignment.

