\section{Experiments}
\label{sec:experiment}
% In this section, we evaluate our method under various settings, including heterogeneous and homogeneous clusters, real workload traces and datasets, budget constraints, cloud GPU limitations, and multiple model-serving scenarios. Additionally, we assess the scalability and effectiveness of our scheduling design. The results demonstrate that serving LLMs using heterogeneous cloud resources is a promising approach to enhancing performance.

\subsection{Experimental Setup}
\label{sec:exp_setup}

\textbf{Environments.} Our experiments are conducted using two types of data center servers \texttt{H100} and \texttt{A100}, three types of work station servers \texttt{A40}, \texttt{RTX A6000} and \texttt{L40}, and one type of consumer server \texttt{RTX 4090}. In data center servers, GPUs are linked by NVLink (300 GB/s), while in workstation/consumer servers, GPUs are linked by PCIe (60 GB/s). Servers with inter-connection are connected via Ethernet with a bandwidth of 5 Gb/s. All experiments are conducted with vLLM~\cite{kwon2023efficient}.

% \begin{table}[ht]
% \centering
% \caption{\small{Real time GPU availabilities on cloud platform.}}
% % \resizebox{\linewidth}{!}{
% \small
% \begin{tabular}{lcccccc}
% \hline
%        \textbf{GPU Avails} & \textbf{4090} & \textbf{A40} & \textbf{A6000} & \textbf{L40} & \textbf{A100} & \textbf{H100} \\
% \hline
% Avail 1 & 16   & 12  & 8     & 12  & 6    & 8    \\
% \hline
% Avail 2 & 32   & 8   & 16    & 16  & 7    & 12   \\
% \hline
% Avail 3 & 32   & 16  & 8     & 8   & 32   & 8    \\
% \hline
% Avail 4 & 24   & 24  & 24    & 16  & 4    & 8    \\
% \hline
% \end{tabular}
% % }
% \label{tab:availability}
% \end{table}

\textbf{Baselines.} We compare our method, which uses heterogeneous cloud resources, against various homogeneous setups:
\begin{itemize}[topsep=5pt, leftmargin=*]
    \vspace{-0.5em}
    \item \textbf{\underline{Heterogeneous setups:}} We rent GPUs from Vast.ai, a cloud provider offering a range of GPU types. The rentals are based on real-time GPU availability on the cloud. For our experiments, we randomly selected four different GPU availabilities (shown in ~\autoref{tab:availability} in~\autoref{appendix:availability}) under varying price budgets of 15, 30, and 60 \$/h.
    \vspace{-0.5em}
    \item \textbf{\underline{Homogeneous setups:}} We rent \texttt{H100} GPUs (representative data center GPUs), \texttt{RTX A6000} GPUs (workstation GPUs), and \texttt{RTX 4090} GPUs (consumer GPUs) under different price budgets, with each GPU type representing a homogeneous baseline. For example, a budget of 60 \$/h can rent up to 20 \texttt{H100} GPUs. Note that we fine-tune the deployment configurations and workload assignments using our scheduling algorithm to optimize the performance of each homogeneous baseline.
    \vspace{-0.5em}
\end{itemize}

\textbf{Models and traces.} Our evaluation is conducted on Llama3-8B and Llama3-70B models.
% , which require 14.9 GB and 130.4 GB of raw GPU VRAM for their model parameters, respectively. For model serving, approximately 24 GB and 180 GB of GPU VRAM are needed, accounting for additional memory allocated to store the KV cache.
% \textbf{Traces.} 
And we follow prior work to generate workload traces based on real-world data. Our testing traces are subsampled from three sources: real workload traces collected over one month from the Swiss AI Center, 
% comprising 500,000 traces collected over one month; 
the WildChat dataset,
% a corpus of user-ChatGPT conversations; 
and the production traces Azure-Trace.
% one week sample of LLM inference services in Azure.
Each trace comprises multiple workload types, with their ratios shown in~\autoref{tab:workload_ratios_full} in~\autoref{appendix:workload ratios}.

\textbf{Evaluation metrics.} We focus on the overall system throughput and vairous percentile latencies (i.e., p10, $\dots$, p90, p100). P90 latency represents the maximum response time within which 90\% of all requests are completed. 
% Evaluating different percentile latencies provides a more comprehensive understanding of the systemâ€™s responsiveness and reliability.

% \subsection{End-to-end System Performance}
% \label{sec:e2e}

\subsection{Experimental Results}
\label{sec:expr_results}

\textbf{End-to-end system performance.}
We evaluated our method across various traces, cloud GPU availability scenarios, price budgets, model types, and homogeneous baselines. Experimental results in~\autoref{fig:e2e} and~\autoref{fig:e2e1} show that our method improves system throughput by up to 41\% (25\% on average) while reducing percentile latencies by up to 54\% (20\% on average).
% As shown in~\autoref{fig:e2e} and~\autoref{fig:e2e1}, in traces 1 and 2,
% % which feature longer input and shorter output token lengths on average, data center GPUs (e.g., H100) 
% H100 (Homo) delivers the best performance among all baselines. 
% Accordingly, with a relatively large price budget (e.g., 60 \$/h), our renting plan allocates a significant portion of data center GPUs, with around 51\% of GPUs being H100, for request processing. However, for a smaller price budget (e.g., 15 \$/h), workstation GPUs (e.g., A40, RTX A6000, L40), which offer lower costs, are preferred. 
% In trace 3, 
% % which features shorter input and longer output token lengths on average,
% A6000 (Homo) demonstrates the best performance among all baselines. 
% Accordingly, our renting plan allocates a significant portion of workstation GPUs, with approximately 93\% of GPUs being A40, RTX A6000, or L40, for request processing. 
% % depending on real-time GPU availability.
% As shown in~\autoref{fig:e2e8b} in~\autoref{appendix:e2e8b}, 
% % smaller models (e.g., Llama3-8B) with lower memory requirements perform best on consumer GPUs (e.g., RTX 4090).
% 4090 (Homo) shows the best performance among all baselines with the Llama3-8B model.
% In this scenario, consumer GPUs account for the majority of our rented GPUs, handling 53\% of the overall request processing.
In traces 1 and 2, H100 (Homo) achieves the best performance among all baselines. In our plan, the GPU composition depends on the budget. With a high budget (60 \$/h), data center GPUs are preferred, making up approximately 51\% of our rented resources for request processing. In contrast, with a low budget (15 \$/h), workstation GPUs are favored due to their lower cost.
In trace 3, A6000 (Homo) demonstrates the highest performance among all baselines. In this scenario, our plan primarily relies on workstation GPUs, which constitute approximately 93\% of the rented resources for request processing.
Additionally, as shown in~\autoref{fig:e2e8b} in~\autoref{appendix:e2e8b}, the 4090 (Homo) delivers the best performance among all baselines for the Llama3-8B model. In this case, our plan prefers consumer GPUs, which form the majority of our rented resources and handle 53\% of overall request processing.



\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{imgs/split_1.pdf}
    \caption{\small{End-to-end throughput results on Llama3-70B model with different setups. We further demonstrate the Llama3-8B results in~\autoref{appendix:e2e8b}.}}
    \label{fig:e2e}
    \vspace{-1em}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{imgs/split_2.pdf}
    \caption{\small{End-to-end latency results on Llama3-70B model with different setups.}}
    \label{fig:e2e1}
    \vspace{-1em}
\end{figure}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{imgs/e2e_pic_8b.pdf}
%     \caption{\small{End-to-end experiments on Llama3-8B model with different setups.}}
%     \label{fig:e2e8b}
% \end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{imgs/vs_hexgen.pdf}
    \caption{\small{Ours vs. HexGen. The first and second bars in each picture represent HexGen using a uniform and optimal GPU composition.}}
    \label{fig:vshexgen}
    \vspace{-1em}
\end{figure}


% \textbf{Comparison with HexGen.} 
We also compare our method with the state-of-the-art heterogeneous serving framework, HexGen~\cite{jiang2023hexgen}. Since HexGen schedules workloads based on a fixed GPU composition, we evaluate it using two setups: (\underline{i}) a uniform composition, where six GPU types are evenly allocated within the budget, and (\underline{ii}) the optimal composition used by our method. As shown in~\autoref{fig:vshexgen}, HexGen with a uniform composition suffers up to 35\% and on average 29\% performance degradation due to suboptimal GPU allocation. Even with the optimal composition, our method outperforms HexGen by up to 18\% and on average 14\%, benefiting from workload-aware scheduling.


% \subsection{Ablation Studies}
% \label{sec:abla}
% \textbf{Ablation studies.}
% In our ablation studies, we evaluate the effectiveness of each optimization target within our scheduling algorithm by systematically disabling them. We include the following three baselines: (\underline{i}) Uniform GPU composition: In this baseline, we rent the six types of GPUs uniformly based on the given price budget. This setup is used to evaluate the performance gain achieved through the optimized heterogeneous GPU composition. (\underline{ii}) Uniform deployment configuration: Instead of optimizing the deployment configuration for each model replica, we uniformly apply TP across all replicas. This setup is used to assess the performance gain from optimized deployment configurations. (\underline{iii}) Rule-based request assignment: we use a Round-Robin approach to assign requests to different model replicas based on the arrival of real traces. This setup evaluates the benefit of heterogeneous-aware workload assignments.
% As shown in~\autoref{fig:ablation}, disabling heterogeneous GPU composition reduces overall system throughput by up to 27\% and on average by 20\%. Disabling deployment configuration optimization results in a throughput reduction of up to 34\% and on average 33\%. Finally, disabling optimized workload assignments reduces throughput by up to 32\% and on average 29\%.
% Overall, each of these optimizations is critical for achieving high-performance LLM serving in heterogeneous environments.

\textbf{Ablation studies.} We assess the impact of each optimization target in our scheduling algorithm by systematically disabling them. Three baselines are considered:
(\underline{i}) Uniform GPU composition: GPUs are rented uniformly across six types within the given budget. This evaluates the performance gains from optimized heterogeneous GPU composition.
(\underline{ii}) Uniform deployment configuration: Instead of optimizing deployment per model replica, TP is uniformly applied across all replicas. This measures the impact of deployment configuration optimization.
(\underline{iii}) Rule-based request assignment: Requests are assigned using a Round-Robin strategy based on real trace arrivals, assessing the benefit of heterogeneous-aware workload assignment.
As shown in~\autoref{fig:ablation}, disabling heterogeneous GPU composition reduces throughput by up to 27\% (average: 20\%), deployment optimization by up to 34\% (average: 33\%), and workload assignment by up to 32\% (average: 29\%). These results highlight the necessity of each optimization for high-performance LLM serving in heterogeneous environments.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{imgs/ablation_new.pdf}
    \caption{\small{Ablation study of Llama3-70B on traces 1 and 2.}}
    \label{fig:ablation}
    \vspace{-1em}
\end{figure}

% \subsection{Algorithm Efficiency}
% \label{sec:algorithmeffect}
\textbf{Algorithm efficiency.}
% We evaluate two methods proposed in~\S\ref{sec:scheduling algorithm} for strategy search: (\underline{i}) MILP and (\underline{ii}) binary search. As shown in~\autoref{fig:algorithm impacts}, the left plot illustrates the scalability of each search method, while the right plot provides an example of algorithm performance during the search process.
We evaluate two strategy search methods from~\S\ref{sec:scheduling algorithm}: (\underline{i}) MILP and (\underline{ii}) binary search. As shown in~\autoref{fig:algorithm impacts}, the left plot illustrates their scalability, while the right plot depicts algorithm performance during the search process.
Compared to MILP, which exhaustively explores all combinations of heterogeneous GPU compositions, deployment configurations, and workload assignments, the binary search method, enhanced with feasibility checks using knapsack approximation, achieves approximately a 4$\times$ reduction in search time. This improvement comes with only marginal differences in algorithm performance, with deviations of less than 1\%.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.75\linewidth]{imgs/alg_perfo.pdf}
    \caption{\small{Algorithm scalability and efficiency.}}
    \label{fig:algorithm impacts}
    \vspace{-1em}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.75\linewidth]{imgs/multi_new.pdf}
    \caption{\small{End-to-end experiments on multiple model types (Llama3-8B and Llama3-70B) with different setups.}}
    \label{fig:multiple models}
    \vspace{-1em}
\end{figure}

% \subsection{Multiple Model Extension}
% \label{sec:multi model eva}
% \textbf{Multi-model extension.}
% We also evaluate the performance of our system in multiple model serving scenarios as described in~\S\ref{sec:multi model}. In this setup, we assume that 80\% of the requests are assigned to the Llama3-8B model, while the remaining 20\% are assigned to the Llama3-70B model. As shown in~\autoref{fig:multiple models}, our method outperforms other homogeneous baselines in multiple model setups, achieving performance gains of up to 35\% and an average of 23\% across different budgets.
% With a budget of 60 \$/h, approximately 70\% of the resources are allocated to the Llama3-70B model, while the remaining 30\% are allocated to the Llama3-8B model, balancing resource needs. Similarly, with a budget of 30 \$/h, around 77\% of the resources are allocated to the Llama3-70B model, and 23\% to the Llama3-8B model.
% This allocation strategy ensures balanced resource sharing between models of different sizes, considering resource requirements based on request assignments and the models' memory and computation demands. Consequently, our method efficiently serves multiple models within heterogeneous clusters.
\textbf{Multi-model extension.} 
% We further evaluate our system in a multi-model serving scenario (mentioned in \S\ref{sec:multi model}), where 80\% of requests go to Llama3-8B and 20\% to Llama3-70B. As shown in~\autoref{fig:multiple models}, our method outperforms homogeneous baselines, achieving up to 35\% and an average of 23\% performance gains across budgets.
% For a 60 \$/h budget, 70\% of computing resources go to Llama3-70B and 30\% to Llama3-8B; for a 30 \$/h budget, 77\% and 23\% are allocated, respectively. Our scheduling algorithm balances resource allocations based on model demands, enabling efficient multi-model serving in heterogeneous clusters.
We further evaluate our system in a multi-model serving scenario (discussed in \S\ref{sec:multi model}), assuming that 80\% of the requests are assigned to the Llama3-8B model, while the remaining 20\% are assigned to the Llama3-70B model. As shown in~\autoref{fig:multiple models}, our method outperforms homogeneous baselines, achieving up to a 35\% (average: 23\%) performance gain. In the 60 \$/h case, our scheduling algorithm allocates 70\% of computing resources to Llama3-70B and 30\% to Llama3-8B. In the 30 \$/h case, the allocation shifts to 77\% and 23\%. Our algorithm balances resource allocation based on model demands, enabling efficient multi-model serving in heterogeneous clusters.


\textbf{System performance vs. price budget.} We compare our system with homogeneous baselines under different price budgets. The performance gap narrows as the budget increases due to cloud resource limits, which is reasonable since we assume an unlimited number of GPUs for our homogeneous baselines. See~\autoref{sec: sysvsprice} for details.



% End-to-end:\\
% - results: throughput \& p99
% - baselines: vllm homogeneous (different types of gpus)
% - fixed: models
% - 2 traces $\times$ 2 budgets $\times$ 2 sets of availability $\times$ 2 types of results

% Ablation:
% - uniform gpu composition \& rule-based request assignment \& uniform parallel strategy
% - w/ gpu composition
% - w/ gpu composition \& workload assigment
% - full version

% optimal boundaries:
% - performance vs. price budget

% --- appendix:
% algorithm efficiency \& effectiveness
