\section{Observation and Opportunity}
\label{sec: motivation}
% In this section, we benchmark the cost-effectiveness of different workload types on  GPU types, model types with different parallelism strategies, and present our key observations and opportunities.

Given a user with a specified budget estimation (in \$/h) renting GPUs from the cloud for serving certain workload traces, our objective is to deliver a comprehensive serving plan that maximizes the cost-efficiency of the user's serving system.
In this section, we first benchmark the cost-efficiency performance of various workload types across different GPU types, model types, and deployment configurations. Then, we present our key observations and opportunities.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{imgs/combine_exp.pdf}
    \caption{\small{Benchmarked results for Llama3-70B model with different GPU types on different workload types. The left three columns represent the throughput results, x-axis represents different GPU types, y-axis represents throughput per unit price (i.e., throughput divided by GPU cost). The right three columns represent the latency results, x-axis represents the $\{$p5, p10, $\dots$, p95, p100$\}$ latency results, y-axis represents total price (i.e., each latency time multiplied by GPU cost). Results for Llama3-8B model are demonstrated in~\autoref{appendix:llama3-8b}.}}
    \label{fig:benchmark1}
    \vspace{-1em}
\end{figure*}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{imgs/8b_single_column.pdf}
%     \caption{\small{Benchmarked results for Llama3-8B model with different GPU types on different workload types (full results of all workload types are listed in \autoref{appendix:llama3-8b}).}}
%     \label{fig:benchmark1.1}
% \end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{imgs/new_pic.pdf}
    \caption{Throughput results for Llama3-70B model with different deployment configurations on different workloads. The three-element array represents the DP, TP, and PP degrees. Full benchmarking results are listed in \autoref{appendix:remaining}.}
    \label{fig:benchmark2}
    \label{fig:benchmark2.1}
    \vspace{-1em}
\end{figure*}


% \begin{table}[h!]
%     \centering
%     \caption{GPU costs per hour.}
%     \small
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{@{}l l c c c c c@{}}
%         \hline
%         \textbf{GPU}    & 4090 & A40 & A6000 & L40 & A100 & H100 \\ \hline
%         \textbf{Price (\$/h)} & 0.55 & 0.53 & 0.83 & 0.83 & 1.75 & 2.99          \\ \hline
%     \end{tabular}
%     }
%     \label{tab:gpu-costs}
% \end{table}


\textbf{Benchmark settings.} We subsample nine workload types from the ShareGPT~\cite{zheng2023lmsys}, WildGPT~\cite{zhao2024wildchat}, and Azure-Trace datasets~\cite{patel2024splitwise}. These workloads are characterized by average input token lengths of $\{2455, 824, 496\}$ and output token lengths of $\{510, 253, 18\}$. Each combination reflects distinct workload characteristics. For example, $\{2455, 18\}$ (long input, short output) represents compute-intensive workloads, while $\{496, 510\}$ (short input, long output) represents memory-intensive workloads. Based on these workload types, we evaluate two models, Llama3-8B and Llama3-70B, on six commonly used cloud GPUs (A6000, A40, L40, A100, H100, and 4090) with different deployment configurations. The benchmarking metrics include request throughput per unit cost (i.e., throughput divided by GPU cost) and the total cost associated with various latency percentiles (e.g., p5, p10, p15, $\dots$, p95, p100). The total cost for each latency percentile is calculated by multiplying the latency time by the GPU cost. These metrics serve as indicators of cost efficiency. The GPU costs are demonstrated in \autoref{tab:gpu_specs}.



\textbf{Observation-1: Heterogeneous GPUs are well-suited for managing model and workload diversity in LLM serving.} \autoref{fig:benchmark1} 
and \autoref{fig:benchmark1.3} 
present the benchmark results for the Llama3-70B and Llama3-8B models across various GPU types and workload types. The observations can be summarized as follows: 
\textbf{(\underline{i})} The H100 and A100 GPUs (data center GPUs) perform well on compute-intensive workloads with the Llama3-70B model, as both GPUs have high computational power to handle intense computational tasks. 
\textbf{(\underline{ii})} The A40, A6000, and L40 GPUs (workstation GPUs) excel in memory-intensive workloads with the Llama3-70B model. 
% Memory-intensive workloads often underutilize H100 and A100 GPUs due to memory bandwidth or capacity bottlenecks. In contrast, workstation GPUs provide greater memory bandwidth and capacity per unit cost (on average 1.2$\times$ and 1.8$\times$ higher than those of data center GPUs) thanks to their lower overall costs, resulting in improved cost efficiency for memory-intensive workloads.
Workstation GPUs offer on average 1.2$\times$ higher
memory bandwidth and 1.8$\times$ greater memory capacity per unit price than data center GPUs, making them more cost-efficient for memory-intensive workloads that often underutilize H100 and A100 GPUs due to memory constraints.
\textbf{(\underline{iii})} The 4090 GPUs (consumer GPUs) deliver excellent performance with the Llama3-8B model. As smaller models require significantly less compute and memory, and the consumer GPUs offer superior memory bandwidth per unit price, approximately 1.9$\times$ higher than that of the A100 and H100 GPUs. 
Overall, our experimental results demonstrate that selecting the most appropriate GPU type for specific workloads and models can enhance the cost-efficiency performance of LLM serving by up to 2.27$\times$. 
% Additionally, in scenarios involving mixed workloads, heterogeneous GPU deployments—where the most suitable GPU is assigned to each workload—achieve up to a 1.55$\times$ improvement in overall cost-efficiency compared to homogeneous setups using a single GPU type.
These findings underscore the necessity of aligning GPU types with model and workload demands to maximize both performance and cost-efficiency. 
% Heterogeneous GPU setups are thus a compelling solution for achieving optimal performance and cost-effectiveness in LLM serving across diverse models and workloads.



% \textbf{Observation 1: Heterogeneous GPUs are well-suited for managing workload diversity.} Recent advances in LLM inference have led to increased workload diversity, characterized by varied input and output token lengths. Some requests are highly compute-intensive (e.g., large-input understanding, summarization, and coding), whereas others are memory-intensive (e.g., conversation and long-form reasoning). This diversity underscores the potential of heterogeneous GPUs with varying compute and memory capabilities to optimize hardware utilization for both performance and cost. 
% % \autoref{tab:raw_specs} provides an overview of several GPU specifications, including their hourly rental prices in a cloud environment. High-performance GPUs, such as the H100 and A100, excel in compute power, memory bandwidth, and memory capacity. However, these advantages come at a significant cost. To better understand cost-effectiveness, \autoref{tab:cost_efficiency} compares the \emph{per-unit-price} characteristics of different GPUs, which highlights the unique strengths of different GPU types. For instance, the 4090 achieves 1.90$\times$ higher per-unit memory bandwidth compared to the H100, while the A40 provides a per-unit memory capacity 3.26$\times$ greater than the H100. These findings indicate that relying exclusively on high-end GPUs, such as the H100 or A100, is not always the most efficient choice—particularly for memory-intensive workloads. Mid-range GPUs can often deliver more cost-effective performance for such tasks. 
% Our benchmarking results indicate that GPU cost-efficiency varies significantly depending on the model and workload type. For the Llama3-70B model, the H100 and L40 GPUs offer the highest cost-efficiency in compute-intensive workloads, while the A100 and A6000 GPUs excel in memory-intensive workloads. For the Llama3-8B model, the 4090 and L40 GPUs deliver optimal cost-efficiency for compute-intensive workloads, whereas the A40 and 4090 GPUs are most cost-effective for memory-intensive workloads.
% Experimental results show that selecting the most appropriate GPU type for specific workloads can enhance cost-efficiency by up to 2.27$\times$. Additionally, in scenarios involving mixed workloads, heterogeneous GPU deployments—where the most suitable GPU is assigned to each workload—achieve up to a 1.55$\times$ improvement in overall cost-efficiency compared to homogeneous setups using a single GPU type.
% These findings underscore the necessity of aligning GPU types with workload demands to maximize both performance and cost-efficiency. Heterogeneous GPU setups are thus a compelling solution for achieving optimal performance and cost-effectiveness in LLM serving across diverse workloads.

% \begin{table}[!ht]
% \centering
% \caption{Raw specifications of various GPU types.}
% \label{tab:raw_specs}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{l | c | c | c | c}
% \hline
% \textbf{GPU Type} & \textbf{Compute} & \textbf{Mem. BW} & \textbf{Memory} & \textbf{Price} \\
%                   & (TFLOPS)     & (GB/s)       & (GB)     & (\$)    \\
% \hline
% A6000 & 91.1 & 960  & 48 & 0.83 \\
% A40           & 37.4 & 696  & 48 & 0.55 \\
% L40          & 91.6 & 864  & 48 & 0.83 \\
% A100          & 312  & 1600 & 80 & 1.75 \\
% H100          & 1000 & 3000 & 80 & 3.12 \\
% 4090      & 82.6 & 1008 & 24 & 0.53 \\
% \hline
% \end{tabular}
% }
% \end{table}


% \begin{table}[!ht]
% \centering
% \caption{Normalized specifications of various GPU types.}
% \label{tab:cost_efficiency}
% \resizebox{0.88\linewidth}{!}{
% \begin{tabular}{l | c | c | c}
% \hline
% \textbf{GPU Type} & \textbf{Compute} & \textbf{Mem. BW} & \textbf{Memory} \\
%                   & (TFLOPS/\$)     & (GB/s/\$)       & (GB/\$)         \\
% \hline
% A6000 & 109.76 & 1156.63 & 57.83 \\
% A40           & 68.00  & 1265.45 & 87.27 \\
% L40S          & 110.36 & 1040.96 & 57.83 \\
% A100          & 178.29 & 914.29  & 45.71 \\
% H100          & 334.45 & 1003.34 & 26.76 \\
% 4090      & 155.85 & 1901.89 & 45.28 \\
% \hline
% \end{tabular}
% }
% \end{table}

\textbf{Observation-2: Optimal deployment configurations are crucial for maximizing cost-efficiency across models, workloads, and GPU types.} \autoref{fig:benchmark2} presents the benchmark results of various deployment configurations across different models, workloads, and GPU types. The observations can be summarized as follows:
\textbf{(\underline{i})} Optimal configurations vary by workload type for a given GPU type. 
% For example, for H100 GPUs serving the Llama3-70B model, tensor parallelism (TP)~\cite{shoeybi2019megatron} is most effective for workloads with high compute and memory demands (e.g., $\{2455, 510\}$). Conversely, a higher degree of data parallelism (DP, i.e., model replication) performs better for workloads with lower compute and memory requirements (e.g., $\{496, 18\}$).
For instance, on H100 GPUs serving Llama3-70B, tensor parallelism (TP)~\cite{shoeybi2019megatron} is most effective for compute- and memory-intensive workloads (e.g., $\{2455, 510\}$), while higher degree of data parallelism (DP, i.e., model replication) performs better for less demanding workloads (e.g., $\{496, 18\}$).
\textbf{(\underline{ii})} Optimal configurations vary by GPU type for a given workload type. For instance, in compute-intensive workloads (e.g., $\{2455, 18\}$), the L40 GPUs achieve the best performance using pure pipeline parallelism (PP)~\cite{huang2019gpipe}, while the H100 GPUs excel with a combination of DP and TP.
\textbf{(\underline{iii})} Optimal configurations also depend on model type. For instance, with Llama3-8B models, DP consistently outperforms model parallelism (i.e., TP and PP). 
% As Llama3-8B has a small memory footprint that fits easily into the memory of most modern GPUs (e.g., 4090, A40, and A100), DP enhances the parallel processing capability of the system without introducing the extra communication overhead associated with model parallelism. This significantly boosts the cost-efficiency of the system.
Since the Llama3-8B model has lower memory requirements and can run on a single GPU without model parallelism, increasing the number of model replicas (i.e., raising the DP degree) enhances the system's parallel processing capability, thereby improving cost efficiency.
Overall, our experimental results demonstrate that selecting the most effective deployment configurations can improve system performance by up to 2.61$\times$. These findings highlight the need for optimized deployment configurations to maximize cost-efficiency in LLM serving.

\textbf{Observation-3: The workload assignment should be co-optimized with the heterogeneous GPU composition and deployment configurations.}
% Appropriate workload assignments are crucial for effectively handling workload heterogeneity. There are two main objectives for workload assignments: (\underline{\textbf{i}}) Prioritize directing requests to GPU types and deployment configurations that best match their resource demands to enhance serving efficiency. (\underline{\textbf{ii}}) Balance workloads across GPUs, ensure that no GPUs are either overloaded or underutilized to maximize resource utilization.
% For the first objective, as mentioned in observation-1 and -2, even with an optimal GPU composition and deployment configurations, system performance can degrade if workloads are assigned to GPUs or configurations that are unsuitable for certain request types. This misalignment can lead to poor overall performance.
% For the second objective, achieving balanced workloads is essential for cost-efficient GPU serving. Ensuring full utilization of GPU capacity may sometimes require assigning workloads to less optimal GPUs or deployment configurations. While these choices may not be the most ideal for individual requests, they ultimately improve the overall system performance.
Effective workload assignment is critical for managing workload heterogeneity and achieving optimal performance. It serves two key objectives:
(\underline{\textbf{i}}) Directing requests to GPU types and deployment configurations that best match their resource demands to enhance serving efficiency. As noted in observation-1 and -2, even with an optimal GPU composition and deployment setup, performance may degrade if workloads are assigned to unsuitable GPUs or configurations. This misalignment can lead to inefficiencies and reduced overall performance. (\underline{\textbf{ii}}) Balancing workloads across GPUs to prevent overloading or underutilization, thereby maximizing resource utilization. Workload balancing is essential for cost-efficient GPU utilization. In some cases, achieving full GPU capacity requires assigning workloads to suboptimal GPUs or configurations. While this may not be ideal for individual requests, it ultimately improves overall system performance.


% \textbf{Constraints: Appropriate resource scheduling is essential under limited resources and restricted budgets.}
% Allocating workloads to the most suitable GPUs is a straightforward strategy for cost-efficient LLM serving. However, real-world serving scenarios are often constrained by resource availability and budget limitations. 
% \textbf{(\underline{i})} On cloud service platforms, the availability of certain GPU types can fluctuate, resulting in insufficient quantities of specific GPUs during peak periods. For instance, the number of A40 GPUs available on RunPod and Vast.ai may vary from 3 to 16 and 0 to 32, respectively, depending on the time. 
% And \textbf{(\underline{ii})} budget constraints can prevent users from selecting the most optimal GPU type for each workload. 
% Overall, these constraints often necessitate assigning workloads to sub-optimal GPU types, leading to reduced performance and efficiency.
% To address these challenges, an \textbf{effective scheduling algorithm} is essential to optimize resource utilization and cost-efficiency. 
% The algorithm should optimize LLM serving by considering budget constraints and real-time GPU availability, ensuring efficiency and adaptability under limited resources.

\textbf{Constraints: Appropriate resource scheduling is crucial under limited resources and budget constraints.}
Allocating workloads to the most suitable GPUs is a straightforward strategy for cost-efficient LLM serving. However, real-world deployments often face resource availability and budget limitations:
(\underline{\textbf{i}}) On cloud service platforms, GPU availability fluctuates, leading to shortages during peak periods. For instance, A40 availability on RunPod and Vast.ai can range from 3 to 16 and 0 to 32, depending on the time.
(\underline{\textbf{ii}}) Budget constraints may prevent users from selecting the optimal GPU for each workload, necessitating compromises in resource allocation.
These constraints often force workload assignments to suboptimal GPUs, reducing performance and efficiency. To mitigate these challenges, an \textbf{effective scheduling algorithm} is essential. It should account for the user’s budget constraints and real-time GPU availability, enabling efficient and adaptive LLM serving even under constrained conditions.

\textbf{Opportunities: Optimization of heterogeneous GPU deployment for cost-efficient LLM serving.} Existing systems typically assume a \textit{homogeneous GPU cluster} for LLM serving or focus on optimizing performance within a \textit{predefined heterogeneous cluster}. However, adjusting the heterogeneous GPU composition within the serving cluster to align with specific workload demands offers a more cost-efficient alternative. Based on prior observations, we propose optimizing LLM serving by customizing the deployment of heterogeneous GPU types to meet workload requirements. This includes determining the optimal heterogeneous GPU composition (\textbf{observation-1}), selecting the most effective deployment configurations (\textbf{observation-2}), and implementing the most appropriate workload assignment (\textbf{observation-3}). Ultimately, our aim is to deliver a comprehensive LLM serving plan that meets user requirements, adapts to cloud environment constraints, and maximizes cost-efficiency (\textbf{constraints}).