\section{Scheduling Algorithm}
\label{sec:scheduling algorithm}
% In this section, we demonstrate our scheduling algorithm.

\subsection{Problem Formulation}
\label{sec: problem formulation}
Given the LLMs to be served, a set of heterogeneous workloads, a user-defined budget, and GPU availability on the cloud, we seek a cost-efficient serving plan comprising: (\underline{1}) \textbf{GPU composition}, i.e., selecting the type and number of GPUs to rent from the cloud while meeting budget and resource requirements; (\underline{2}) \textbf{deployment configurations}, i.e., organizing the rented GPUs into serving groups, each responsible for serving one model replica, and determining their parallelism strategies; and (\underline{3}) \textbf{workload assignment}, i.e., determining the allocation of incoming workloads across model replicas. Our objective is to minimize the overall makespan for processing all incoming workloads. The resulting plan must ensure that the user obtains the most \textbf{cost-efficient} LLM serving solution under the specified budgetary and resource constraints.

% Given a user-defined budget and the per-type availability and cost of GPUs on the cloud, we aim to determine the \textbf{(\underline{1}) GPU composition}, \textbf{(\underline{2}) deployment configuration} (i.e., groups + parallelism strategies), and \textbf{(\underline{3}) workload assignment} that minimize the overall makespan for processing all incoming workloads. The resulting plan ensures that the user obtains the most \textbf{cost-efficient} LLM serving solution under the specified budgetary and resource constraints.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{imgs/case study.pdf}
%     \caption{\small{Case study on GPU composition, deployment configuration and workload assignment. $\text{R}_i$ represents the $i$-th model replica.}}
%     \label{fig:case study}
% \end{figure}

\subsection{Simple Example}

\textbf{Experiment setup.}
We begin by assuming three GPU types, $\{t_1,t_2,t_3\}$, each with two units available. The hourly rental prices for these types are 4, 2, and 2\,\$/h, respectively. We consider two workload types $\{w_1, w_2\}$, which arrive simultaneously with 80 total requests for $w_1$ ($\lambda_1=80$) and 20 total requests for $w_2$ ($\lambda_2=20$). We denote by $C_{t,w}$ the throughput (in requests per second) of GPU type $t$ on workload $w$. If each GPU serves one model replica, the throughputs are $C_{1,1}=1.0$, $C_{1,2}=1.2$, $C_{2,1}=0.9$, $C_{2,2}=0.9$,
$C_{3,1}=0.3$, and $C_{3,2}=0.5$. Note that $C_{\sim,1}$ and $C_{\sim,2}$ vary with model parallelism. In \textbf{Cases~1} and~\textbf{2}, we assume the workload is assigned to each GPU in proportion to its processing rate, so the system-wide throughput for each workload is the sum of individual-GPU rates. In \textbf{Case~3}, we allow workload-aware assignment for further optimization.

\textbf{Case 1: GPU composition.}
We compare two compositions under the same budget of 8\,\$/h, where each GPU serves one model replica.
Composition 1 consists of $1\times t_1$, $1\times t_2$, and $1\times t_3$, achieving a total throughput of $(2.2, 2.6)$ rps on $(w_1, w_2)$, with a processing time of $44.05$\,s.
Composition 2 consists of $1\times t_1$ and $2\times t_2$, achieving throughputs of $(2.8, 3.0)$ rps on $(w_1, w_2)$, with a processing time of $35.24$\,s.
Thus, modifying the GPU composition within the same budget results in a 20\% speedup.


\textbf{Case 2: Deployment configuration.}
Focusing on composition 2, we compare two ways to organize these three GPUs.
Configuration 1 assigns each GPU to serve a single model replica, resulting in a processing time of $35.24$\,s.
Configuration 2 applies TP to the two $t_2$ GPUs, changing their combined throughput to $2.4$ rps on $w_1$ and $1.5$ rps on $w_2$, reducing the overall processing time to $30.94$\,s.
Thus, modifying the deployment configuration improves the processing time by approximately 14\%.



\textbf{Case 3: Workload assignment.}
Finally, we retain the same composition and TP-based configuration but optimize workload assignment. Specifically, we assign 15\% of $w_1$ and 100\% of $w_2$ to the replica with $t_1$, and 85\% of $w_1$ to the replica with TP on $2\times t_2$.
% \vspace{-0.5em}
% \[
% \begin{aligned}
% &\text{Replica (}t_1\text{): } 15\%\text{ of }w_1, 100\%\text{ of }w_2,\\
% &\text{Replica (TP on }2\times t_2\text{): } 85\%\text{ of }w_1.
% \end{aligned}
% \]
% \vspace{-1.5em}
With this assignment, the overall completion time is reduced to $28.67$\,s.
Thus, adjusting the workload assignment results in an additional 8\% reduction in processing time.

This step-by-step example demonstrates the joint optimization of GPU composition, deployment configuration, and workload assignment for optimal performance. A detailed processing time calculation for each case is in~\autoref{appendix:simpleexample}.





\subsection{MILP Formulation}
\label{sec:milp formulation}
% Assume we have $N$ types of GPU, indexed by $n \in \{1,2,\dots,N\}$. Each GPU type $n$ has price $p_n$ (e.g., \$1.75/hour for A100, \$3.20/hour for H100), availability $a_n$ (i.e., the maximum number of GPUs of type $n$ that can be allocated), and memory limit $m_n$ (e.g., 40 GB for L40, 80 GB for H100).
% Assume there are $M$ incoming workload types, indexed by $m \in \{1,2,\dots,M\}$.
% To support efficient processing of multiple LLM inference workloads, the scheduling algorithm must determine three essential allocations: (\underline{1}) \textit{the GPU composition}, i.e., how many GPUs of each type $n$ should be allocated; (\underline{2}) \textit{the model deployment}, i.e., how to formulate multiple GPU groups, where each responsible for serving one model replica, and the tensor parallelism and pipeline parallelism degree for each model replica; (\underline{3}) \textit{the workload assignment}, i.e., how to distribute various types of workloads across different model replicas. We term a solution to these three components as a \textit{serving plan}. Given that each workload $m$ must be fully served, Our objective is to determine a serving plan that minimizes the makespan $T$ for processing all workload types.

% We determine the serving plan as a set $\mathcal{C}$ of feasible model configurations. Each model configuration $c \in \mathcal{C}$ is characterized by $(v_c,s_c)$: the variable $v_c=\{d_n(c)\}_{n=1}^{N}$ indicates how many GPUs of each type $n$ are used in configuration $c$, where $d_n$ represents the number of GPUs of type $n$ allocated, and the variable $s_c=(TP_c, PP_c)$ indicates the parallelism strategy. For each model configuration $c$, $thpt_{c,m}$ represents its throughput of processing workload type $m$, $price_c=\sum_{n=1}^N (d_n(c) \times p_n)$ represents its price cost, and $mem_c=\sum_{n=1}^N (d_n(c) \times m_n)$ represents its overall memory limit. 

% A valid serving plan should satisfy the following constraints: (i) the allocated GPU number of each type should be no larger than the available GPU number, i.e., $0 \leq \sum_{c \in \mathcal{C}} d_n(c) \leq a_n$; (ii) the multiplication of tensor and pipeline parallelism degrees for each model configuration should be equal to its total GPU count, i.e., $TP_c \times PP_c = \sum_{n=1}^N d_n(c)$; (iii) the total price of all $c$ should be within the user defined price budget $B$, i.e., $\sum_{c \in \mathcal{C}} price_c \leq B$; (iv) the overall memory limit for each model configuration should be larger than the memory $M_r$ required for serving one model replica, i.e., $mem_c \geq M_r$.

\input{symbol_table}

In this section, we introduce a mixed-integer linear programming (MILP) formulation to find a serving plan, i.e., GPU composition, deployment configurations and workload assignment, that minimizes the overall processing time. An overview of the symbols is shown in \autoref{tab:milp-notation}.

Let there be $N$ types of GPUs, indexed by $n \in \{1,2,\dots,N\}$. We denote the decision on how many GPUs of each type to use (i.e., \textbf{GPU composition}) by a vector $\mathbf{D} = [\,d_{1},\,d_{2},\,\dots,\,d_{N}\,]$ where each $d_{n} \ge 0$ represents the number of GPUs of the $n$-th type. These variables are subject to availability constraints encoded by a vector $\mathbf{A} = [\,a_{1},\,a_{2},\,\dots,\,a_{N}\,]$,  such that $0 \le d_{n} \le a_{n}, \forall n = 1,2,\dots,N$. Each GPU type $n$ has price $p_n$ (e.g., 1.75\,\$/h for A100, 2.99\,\$/h for H100), and memory limit $m_n$ (e.g., 48 GB for L40, 80 GB for H100).

\textbf{Configurations.} We consider a set $\mathcal{C}$ of feasible configurations (i.e., \textbf{deployment configurations}). Each configuration $c \in \mathcal{C}$ represents the serving plan for a single model replica, which is characterized by $(v_c, s_c, o_c, h_{c,w})$: (\underline{i}) A vector $v_c=\{d_{n}(c)\}_{n=1}^N$ indicating exactly how many GPUs of each type $n$ are used in configuration $c$. (\underline{ii}) An array $s_c=\{t_1, t_2, \dots, t_S\}$ indicating the parallelism strategy used in configuration $c$. The array length $S$ represents the total number of pipeline stages, and the element $t_s$ represents the TP degree of the $s$-th stage. The summation of all $t_s$ should be equal to the total GPU count of configuration $c$, i.e., $\sum_{s=1}^{S} t_s=\sum_{n=1}^{N} d_{n}(c)$.
% The product of $TP_c$ and $PP_c$ equals the total GPU count of configuration $c$, i.e., $TP_c \times PP_c = \sum_{n=1}^{N} d_{n}(c)$. For example, if $\sum_{n=1}^{N} d_{n}(c)=8$, then $(TP_c,PP_c)$ could be $(1,8)$, $(2,4)$, $(4,2)$ and $(8,1)$. 
(\underline{iii}) A cost $o_c = \sum_{n=1}^N (d_{n}(c)\times p_n)$ indicating the total price required for configuration $c$. (\underline{iv}) A throughput $h_{c,w}$ indicating the rate at which configuration $c$ process workload type $w$, which is obtained through a one-time profiling. By optimizing the configurations, we can obtain the \textbf{GPU composition} and \textbf{deployment configurations} mentioned in \S\ref{sec: problem formulation}.

\textbf{Workloads and assignment.} Let there be $W$ workload types, indexed by $w \in \{1,2,\dots,W\}$. Each workload $w$ must be fully served (i.e., 100\% coverage). We allow fractional assignment: a fraction $x_{c,w} \in [0,1]$ of workload $w$ may be processed by configuration $c$. Concretely, $\sum_{c \in \mathcal{C}} x_{c,m} = 1, \forall\, m=1,2,\dots,M$. 
% We also introduce a binary variable $y_{c} \in \{0,1\}$ indicating whether configuration $c$ is chosen (activated). If $y_c = 0$, then $x_{c,w}$ must be zero for all $w$. By co-optimizing the workload assignment with configurations, we can obtain the \textbf{workload assignment} mentioned in \S\ref{sec: problem formulation}.
We also introduce an integer variable $y_{c} \in \{0,1,2,\dots\}$ indicating how many copies of configuration $c$ are chosen (activated). If $y_c = 0$, then $x_{c,w}$ must be zero for all $w$. By co-optimizing the workload assignment (the fractions $x_{c,w}$) with the activated configurations ($y_c$), we determine the final \textbf{workload assignment} as described in \S\ref{sec: problem formulation}.


\textbf{Budget and GPU constraints.} A valid configuration set \(\mathcal{C}\) must also satisfy the following constraints: (\underline{i}) the allocated number of GPUs for each type must not exceed the available number, i.e., $0 \leq \sum_{c \in \mathcal{C}} (d_n(c) \times y_c) \leq a_n, \forall\,n=1,\dots,N$; (\underline{ii}) the total cost of all chosen configurations must be within the user-defined budget \(B\), i.e., $\sum_{c \in \mathcal{C}} (o_c \times y_c) \le B$.

% \textbf{Optimization objective.} We define a makespan variable $T \ge 0$ to represent the overall finishing time. For configuration $c$, if it processes fractions $x_{c,w}$ of workload $w$ (with throughput $h_{c,w}$), then the time required on $c$ is $T_c = \sum_{w=1}^{W}\, \frac{x_{c,w}}{h_{c,w}}$. Since each configuration runs in parallel, the system completes once the slowest (longest) configuration finishes. Thus, we have
% $T_c \le T, \forall\,c\in \mathcal{C}$.
% Our optimization objective is to minimize the makespan $T$.

\textbf{Optimization objective.} We define a makespan variable \(T \geq 0\) to represent the overall completion time. For a configuration \(c\), if it is instantiated \(y_c\) times and processes fractions \(x_{c,w}\) of workload \(w\), each replica provides a throughput of \(h_{c,w}\). Consequently, the total effective throughput for \(c\) is \(y_c \times h_{c,w}\), and the time required on \(c\) is given by $T_c = \sum_{w=1}^{W} \frac{x_{c,w}}{y_c \cdot h_{c,w}}.$ Since all chosen configurations run in parallel, the system completes once the slowest configuration finishes. Thus, we have \(T_c \leq T\) for all \(c \in \mathcal{C}\). Our optimization objective is to minimize \(T\).


\textbf{MILP formulation.} The problem can be summarized as the following Mixed-Integer Linear Program (MILP):
{\footnotesize % Smaller font size for the equations
\setlength{\jot}{2pt} % Reduce the space between lines in align
\begin{align}
\arg\min \,
& T \\[2pt]
\text{s.t.} \quad 
& \sum\nolimits_{c \in \mathcal{C}} x_{c,w} = 1, \, \forall\, w,\,\textbf{(Assignment Constraint)} \label{eq:const1}\\
& \sum\nolimits_{w \in W} \frac{x_{c,w}}{y_c \cdot h_{c,w}} \le T, \, \forall\, c,\,\textbf{(Makespan)} \label{eq:const2}\\
& x_{c,w} \le y_c, \,\forall\, c, w, \,\textbf{(Activation Coupling)} \label{eq:const3} \\
& \sum\nolimits_{c \in \mathcal{C}} \bigl(o_c \times y_c\bigr) \le B,\,\textbf{(Budget Constraint)} \label{eq:const4}\\
% & \sum\nolimits_{n=1}^N (d_{n}(c)\times m_n) \ge M_r, \quad \forall\, c \in \mathcal{C}, \\
& \sum\nolimits_{c \in \mathcal{C}} \bigl(d_{n}(c)\times y_c\bigr) \le a_{n}, \,\forall\, n, \label{eq:const5} \,\textbf{(GPU Avail.)} \\
& y_c \in \{0,1,2,\dots\}.
\end{align}
}
This formulation determines which configurations are used ($y_c$) and how the workload fractions ($x_{c,w}$) are distributed, subject to memory limit, price budget, and GPU availability constraints, in order to minimize the makespan $T$. Note that $d_n(c)$ is an integer; we enumerate all feasible integer combinations $\{d_n(c)\}_{n=1}^{N}$ in a precomputation step. In contrast, $x_{c,w}$ is a continuous variable, and the solver relies on branch-and-bound to systematically narrow the feasible region and converge to an optimal fractional assignment.

% \textbf{Complexity analysis.} The number of binary activation variables $y_c$ grows combinatorially with the number of feasible configurations \( |\mathcal{C}| \). In this worst case \( |\mathcal{C}| \) can be on the order of $\prod_{n=1}^{N} (a_n + 1)$. Since MILP solvers (e.g., branch-and-bound) have worst-case running time exponential in the number of binary variables, the theoretical worst-case time complexity scales as $\mathbb{O} \bigl(\mathrm{poly}(|\mathcal{C}|, W, N) \,\times\, 2^{\,|\mathcal{C}|} \bigr)$, where the polynomial factor accounts for the overhead of processing each node in the search tree (e.g., solving continuous relaxations of the MILP). As a result, the solution time escalates rapidly with the number of candidate configurations.

\textbf{Other constraints and heuristics.} We introduce additional constraints and heuristics to reduce the search space. Concretely, we enforce a memory constraint to eliminate configurations with insufficient GPU memory, and a connectivity constraint to exclude those involving disconnected GPUs. Additionally, we refine the configuration search by restricting TP to single machines and enabling non-uniform PP layer partitioning based on memory allocation. See~\autoref{appendix:heuristics} for details.

% For large numbers of GPUs and model types, it might take hours for the MILP solver to provide a relatively good solution. To expedite the search process, we introduce three optimizations to minimize the search space without sacrificing the effectiveness of our scheduling results:
% (\underline{i}) for each model type, we prune configurations that are clearly dominated. For example, configurations with high degrees of model parallelism are retained for Llama3-70B, which requires substantial memory for model serving, but are pruned for Llama3-8B to prevent excessive communication overhead;
% (\underline{ii}) we pre-estimate the resource requirements for each model type based on incoming workloads and their memory demands, and proportionally allocate resources to provide a good starting point for the MILP solver, thereby expediting the search process;
% (\underline{iii}) we establish a theoretical lower bound for the makespan by analyzing the minimum possible processing time across all feasible configurations, which enables the implementation of early stopping criteria during optimization, i.e., the search process stops when it finds a solution that is very close to this lower bound. The minimum possible makespan occurs when all workloads are assigned to the most efficient configuration without considering resource constraints.

% The experimental results shown in \S\ref{sec:experiment} demonstrate the efficiency and effectiveness of our scheduling algorithm.

\textbf{Binary search.} To address the long computation times associated with the MILP solver for large-scale problems, we introduce a \textbf{binary-search-on-T} method to accelerate the search process. Rather than directly minimizing the makespan $T$, we iteratively check whether a valid serving plan exists for different candidate values of $\hat{T}$, based on reasonable lower and upper bounds. For a full explanation, refer to \autoref{appendix: bs}, and we evaluate the effectiveness of this binary search method in \S\ref{sec:expr_results}.

\label{sec:multi model}
\textbf{Extension to multiple LLM serving.} Our MILP formulation can be easily adapted to scenarios involving multiple LLMs, such as simultaneously serving both Llama3-8B and Llama3-70B models. To accommodate this, we introduce a model-type dimension to the decision variables and constraints. This ensures that workload assignments, memory requirements, and other constraints are optimized for each model type. The objective remains to minimize the overall makespan $T$, while also taking into account GPU availability, budget constraints, and other constraints across all model types. For a detailed explanation of the formulation, please refer to \autoref{appendix: multiple model}. We demonstrate the evaluation of our method for multiple model serving in \S\ref{sec:expr_results}.