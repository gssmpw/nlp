\section{Conclusion}
\label{sec:conclusion}

This paper aims to address the questions of \textit{why} and \textit{how} heterogeneous cloud resources can be utilized for cost-efficient LLM serving. 
Specifically, we benchmark the cost-efficiency of LLM serving over heterogeneous GPUs, following which, a novel scheduling algorithm is developed. 
Experimental results demonstrate that our approach outperforms existing works substantially.
% Specifically, we benchmark the cost-efficiency of various workload types across different GPU types, model types, and deployment configurations. Based on the benchmarking results, we propose a mixed-integer linear programming-based scheduling algorithm to determine the most cost-efficient serving plan under budget and availability constraints.
% Experimental results demonstrate that our approach achieves up to a 51\% improvement in system throughput, with an average gain of 25\%, and reduces system latency by up to 54\% and on average by 20\%, compared to several homogeneous baselines.