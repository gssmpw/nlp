%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{wrapfig}
\usepackage{graphicx} 
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{enumitem}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{array}
\usepackage{makecell}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}



\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\violet}[1]{\textcolor{violet}{#1}}




% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs}

\begin{document}

\twocolumn[
\icmltitle{Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
% \icmlauthor{Anonymous authors}{}
\icmlauthor{Youhe Jiang}{equal,cam,hkust}
\icmlauthor{Fangcheng Fu}{equal,pku}
\icmlauthor{Xiaozhe Yao}{eth}
\icmlauthor{Guoliang He}{cam}
\icmlauthor{Xupeng Miao}{pdu}
\icmlauthor{Ana Klimovic}{eth}
%\icmlauthor{}{sch}
\icmlauthor{Bin Cui}{pku}
\icmlauthor{Binhang Yuan}{hkust}
\icmlauthor{Eiko Yoneki}{cam}
% \icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{cam}{Department of Computer Science, University of Cambridge, Cambridgeshire, UK}
\icmlaffiliation{pku}{Department of Computer Science, Peking University, Beijing, China}
\icmlaffiliation{eth}{Department of Computer Science, ETH Zurich, ZÃ¼rich, Switzerland}
\icmlaffiliation{pdu}{Department of Computer Science, Purdue University, West Lafayette, Indiana, US}
\icmlaffiliation{hkust}{Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China}

\icmlcorrespondingauthor{Binhang Yuan}{biyuan@ust.hk}
\icmlcorrespondingauthor{Eiko Yoneki}{eiko.yoneki@cl.cam.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% Large Language Models (LLMs) have revolutionized applications across various domains but face deployment challenges due to high serving costs and reliance on homogeneous GPU resources. And recent advancements in model capabilities and modalities have led to increasingly diverse requests with varying compute and memory demands. To address these challenges, our paper leverages heterogeneous GPU resources available on cloud platforms to minimize serving costs associated with homogeneous data center GPUs and effectively handle heterogeneous workloads. We find that different GPU types exhibit distinct compute and memory characteristics that align well with the resource demands of diverse workloads, and by strategically determining GPU composition, parallelism strategies, and workload assignments, we can significantly optimize the cost-efficiency of LLM serving. Through comprehensive benchmarking of various GPU types across multiple parallelism strategies and workload characteristics, we designed a mixed-integer linear programming-based scheduling algorithm aimed at determining the most cost-efficient serving plan under budget and availability constraints. Our evaluation, using real-world workload traces and popular LLMs, demonstrates that our approach achieves up to 41\% and an average of 23\% higher throughput, while reducing the serving latency by up to 54\% and on average 20\% compared to several homogeneous baselines. This paves the way for more accessible and efficient deployment of LLMs utilizing heterogeneous cloud resources.
Recent advancements in Large Language Models (LLMs) have led to increasingly diverse requests, accompanied with varying resource (compute and memory) demands to serve them. However, this in turn degrades the cost-efficiency of LLM serving as common practices primarily rely on homogeneous GPU resources. In response to this problem, this work conducts a thorough study about serving LLMs over heterogeneous GPU resources on cloud platforms. The rationale is that different GPU types exhibit distinct compute and memory characteristics, aligning well with the divergent resource demands of diverse requests. Particularly, through comprehensive benchmarking, we discover that the cost-efficiency of LLM serving can be substantially optimized by meticulously determining GPU composition, deployment configurations, and workload assignments. Subsequently, we design a scheduling algorithm via mixed-integer linear programming, aiming at deducing the most cost-efficient serving plan under the constraints of price budget and real-time GPU availability. Remarkably, our approach effectively outperforms homogeneous and heterogeneous baselines under a wide array of scenarios, covering diverse workload traces, varying GPU availablilities, and multi-model serving. This casts new light on more accessible and efficient LLM serving over heterogeneous cloud resources.
% Source codes are available \href{https://anonymous.4open.science/r/benchmark-and-algorithm-564F}{here}.
\end{abstract}

\input{introduction}
\input{background}
\input{motivation}
\input{method}
\input{experiment}
\input{conclusion}

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\bibliography{main}
\bibliographystyle{icml2025}
\nocite{wang2024towards,qiao2024conserve,stojkovic2024dynamollm,yu2022orca,wang2024burstgpt,oh2024exegpt,liu2023deja,wu2023fast,zhou2022pets,qin2024mooncake,hu2024inference,hendrycks2020measuring,liu2024understanding}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Benchmarking Results for Llama3-8B}
\label{appendix:llama3-8b}

We demonstrate the benchmark results for Llama3-8B model in~\autoref{fig:benchmark1.3}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{imgs/combined_pic_2.pdf}
    \caption{Benchmarked results for Llama3-8B model with different GPU types on different workload types.}
    \label{fig:benchmark1.3}
\end{figure*}

\section{Benchmarking Results of Different Deployment Configurations for Remaining GPUs}
\label{appendix:remaining}

We demonstrate the benchmark results for different deployment configurations in~\autoref{fig:benchmark2.4} and \autoref{fig:benchmark2.3}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{imgs/merged_pic.pdf}
    \caption{Throughput and latency results for Llama3-70B model with different deployment configurations on different workloads.}
    \label{fig:benchmark2.4}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{imgs/merged_pic_2.pdf}
    \caption{Throughput and latency results for Llama3-70B model with different deployment configurations on different workloads.}
    \label{fig:benchmark2.3}
\end{figure*}

\section{Simple Example}
\label{appendix:simpleexample}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/case_study_new.pdf}
    \caption{Illustration of a simple example.}
    \label{fig:simpleexample}
\end{figure}

\textbf{Experiment setup.}
We begin by assuming three GPU types, $\{t_1,t_2,t_3\}$, each with two units available. The hourly rental prices for these types are 4, 2, and 2\,\$/h, respectively. We consider two workload types $\{w_1, w_2\}$, which arrive simultaneously with 80 total requests for $w_1$ ($\lambda_1=80$) and 20 total requests for $w_2$ ($\lambda_2=20$). We denote by $C_{t,w}$ the throughput (in requests per second) of GPU type $t$ on workload $w$. If each GPU serves one model replica, the throughputs are $C_{1,1}=1.0$, $C_{1,2}=1.2$, $C_{2,1}=0.9$, $C_{2,2}=0.9$,
$C_{3,1}=0.3$, and $C_{3,2}=0.5$. Note that $C_{\sim,1}$ and $C_{\sim,2}$ vary with model parallelism. In \textbf{Cases~1} and~\textbf{2}, we assume the workload is assigned to each GPU in proportion to its processing rate, so the system-wide throughput for each workload is the sum of individual-GPU rates. In \textbf{Case~3}, we allow workload-aware assignment for further optimization.

\textbf{Case 1: GPU composition.}
We compare two compositions under the same budget of 8\,\$/h, where each GPU is responsible for serving one model replica.
Composition~1 uses $1\times t_1$, $1\times t_2$, and $1\times t_3$. This setup achieves a total throughput of $(1.0 + 0.9 + 0.3)=2.2$\,rps on $w_1$ and $(1.2 + 0.9 + 0.5)=2.6$\,rps on $w_2$, giving a processing time of $\bigl(\lambda_1/C_{\sim,1}+\lambda_2/C_{\sim,2}\bigr)=\bigl(80/2.2 + 20/2.6\bigr)\approx 44.05$\,s. 
Composition~2 uses $1\times t_1$ and $2\times t_2$, for throughputs of $(1.0 + 0.9 + 0.9)=2.8$\,rps on $w_1$ and $(1.2 + 0.9 + 0.9)=3.0$\,rps on $w_2$, so $\bigl(80/2.8 + 20/3.0\bigr)\approx 35.24$\,s. In this case, changing the GPU composition under the same price budget results in a 20\% speedup.


\textbf{Case 2: Deployment configuration.}
Focusing on composition~2, we compare two ways to organize these three GPUs. 
Configuration~1 keeps all GPUs in a purely DP style (i.e., each GPU is responsible for serving one model replica), summing up to $2.8$\,rps on $w_1$ and $3.0$\,rps on $w_2$, matching the 35.24\,s above. 
Configuration~2 applies TP to the two $t_2$ GPUs, which changes their combined rate, e.g., to $2.4$\,rps on $w_1$ and $1.5$\,rps on $w_2$. Together with the single $t_1$ GPU ($1.0$\,rps on $w_1$ and $1.2$\,rps on $w_2$), the total throughput becomes $(3.4,\,2.7)$\,rps for $(w_1,\,w_2)$. The corresponding time $\bigl(80/3.4 + 20/2.7\bigr)\approx 30.94$\,s. In this case, changing the deployment configuration results in an improvement in the overall processing time of roughly 14\%.



\textbf{Case 3: Workload assignment.}
Finally, we keep the same composition and TP-based configuration but allow workload-aware assignment. Concretely, we assign:
\[
\begin{aligned}
&\text{Replica (}t_1\text{): } 15\%\text{ of }w_1, 100\%\text{ of }w_2,\\
&\text{Replica (TP on }2\times t_2\text{): } 85\%\text{ of }w_1.
\end{aligned}
\]
Under these fractions, $t_1$ processes $12$~requests of $w_1$ at 1.0\,rps and $20$~requests of $w_2$ at 1.2\,rps, while the TP-based replica handles $68$~requests of $w_1$ at 2.4\,rps. By balancing the load and routing the workload to the preferable replica, i.e., the one with relatively higher throughput for a specific workload, we reduce the overall completion time from $30.94$\,s to $max(0.85\lambda_1/C_{2,1}, 0.15\lambda_1/C_{1,1}+\lambda_2/C_{1,1})=max(68/2.4, 12/1+20/1.2)=28.67$\,s. In this case, changing the workload assignment results in an additional improvement in the overall processing time of approximately 8\%.

This step-by-step example (also illustrated in~\autoref{fig:simpleexample}) shows how all three factors---GPU composition, deployment configuration, and workload assignment---must be jointly optimized to achieve the best performance.


\section{Other Constraints and Heuristics}
\label{appendix:heuristics}
We enforce two additional constraints to minimize the overall search space and speed up the search process: (\underline{i}) We perform an early memory check on each configuration, which ensures that the sum of GPU memories in configuration $c$ is sufficient for a model replica, i.e., $\sum_{n=1}^N (d_{n}(c)\times m_n) \ge M_r$, where $M_r$ represents the least memory required for serving one model replica (e.g., 140 GB for Llama3-70B model). Configurations that violate this constraint will be eliminated from further evaluation; (\underline{ii}) we enforce a connectivity constraint within each configuration. If certain GPUs lack interconnection (e.g., they are located in different data centers), those combinations do not appear in each configuration $c$. Additionally, we use two heuristic methods to facilitate the deployment configuration search: (\underline{i}) we only adopt TP within a single machine containing multiple GPUs, as TP typically requires high intra-machine communication bandwidth (e.g., PCIe, NVLink) for efficient deployment; (\underline{ii}) we support non-uniform pipeline layer partitioning for PP, and determine the partition based on the total memory allocated for each stage. For instance, if there are a total of 24 layers and the GPU memory allocated for each stage is 1:2, then we allocate 8 and 16 layers to the first and second stages.

\section{Extend to Multiple LLM serving}
\label{appendix: multiple model}
The previous MILP formulation assumes a single LLM serving with multiple model replicas. However, cloud services typically involve multiple LLM serving with varying sizes, e.g., Llama3-8B and Llama3-70B models. To integrate multiple LLM serving plan search into our MILP, we introduce the following extended MILP formulation.

Let there be $M$ model types, indexed by $m \in \{1,2,\dots,M\}$, each type has its own memory requirement. The MILP formulation can be extended to:
% {\footnotesize % Smaller font size for the equations
\setlength{\jot}{2pt} % Reduce the space between lines in align
\begin{align}
\arg\min \,
& T \\[2pt]
\text{s.t.} \quad 
& \forall m:
\begin{cases}
\sum_{c \in \mathcal{C}_{m}} x_{c,w,m} = 1,\, \forall\,w \in W_m, \\[4pt]
\sum_{w \in W_m} \frac{x_{c,w,m}}{y_{c,m} \cdot h_{c,w,m}} \le T,\, \forall\,c \in \mathcal{C}_{m}, \\[4pt]
x_{c,w,m} \le y_{c,m},\, \forall\,c \in \mathcal{C}_{m},\, \forall\,w \in W_m,
\end{cases} \\
& \sum\nolimits_{m=1}^{M} \sum\nolimits_{c \in \mathcal{C}_{m}} \bigl(o_{c,m} \times y_{c,m}\bigr) \le B, \label{eq:const4}\\
% & \sum\nolimits_{n=1}^N (d_{n}(c)\times m_n) \ge M_r, \quad \forall\, c \in \mathcal{C}, \\
& \begin{aligned}
\sum\nolimits_{m=1}^{M}\sum\nolimits_{c \in \mathcal{C}_{m}} \bigl(d_{n}(c,m)\times y_{c,m}\bigr) \le a_{n},\, \forall\, n,
\end{aligned} \label{eq:const5}\\
& y_{c,m} \in \{0,1,2,\dots\}.
\end{align}
% }
In this extended MILP formulation, we introduce an additional model-type dimension to every relevant variable and constraint. Consequently, the problem now accommodates multiple model types (each with its own workload set, throughput profiles, memory requirements, etc.) within a unified optimization framework. The objective remains the sameâminimizing the overall makespan $T$âwhile jointly enforcing GPU availability, budget, and other constraints across all model types. This ensures that the chosen configuration set and workload assignments meet the demands of every model type while adhering to the total GPU and budget limits.

\section{Binary Search}
\label{appendix: bs}

For large numbers of model, workload and GPU types, it might take hours for the MILP solver to provide a relatively good solution. To expedite the search process, we incorporate the \textbf{binary-search-on-T} approach into our existing MILP formulation. Specifically, we transform the previous ``minimize $T$'' problem into a sequence of feasibility checks: for a given candidate $\hat{T}$, we ask whether a valid serving plan exists that completes all workloads in at most $\hat{T}$, subject to budget and GPU constraints. If yes, we can try smaller $\hat{T}$; if no, we must increase $\hat{T}$.

\textbf{Binary search.} The lower bound of the makespan, $\underline{T}$, is identified as the best possible time if infinite GPUs were available with no budget limit (e.g., using the fastest configuration for each workload type). The upper bound, $\overline{T}$, is the worst-case scenario (e.g., using the slowest feasible configuration to serve all workloads). During the binary search loop, if the difference between the lower and upper bounds exceeds a certain tolerance $\tau$ (e.g., one second), i.e., $\overline{T} - \underline{T} \geq \tau$, we calculate $\hat{T} = \frac{\overline{T} + \underline{T}}{2}$ and check its feasibility. If a pair $(x_{c,w}, y_{c})$ or $(x_{c,w,m}, y_{c,m})$ (in the extended case) satisfies all constraints in \S\ref{sec:milp formulation} or \S\ref{sec:multi model}, with $\sum\nolimits_{w \in W} \frac{x_{c,w}}{h_{c,w}} \leq \hat{T}$ or $\sum\nolimits_{w \in W_m} \frac{x_{c,w,m}}{h_{c,w,m}} \leq \hat{T}$, $\forall c \in C_{m}$, we update $\overline{T} \leftarrow \hat{T}$. Otherwise, we update $\underline{T} \leftarrow \hat{T}$. When the loop concludes, the value of $\overline{T}$ (or $\underline{T}$) represents the minimal feasible makespan within the specified tolerance. Note that the feasibility check can be further approximated using a knapsack approximation, which makes the binary search approach more efficient for handling large-scale MILP problems. We outline the binary search process in Algorithm \autoref{alg:binary_search}. 

\input{binary_search_algorithm}

\textbf{Other optimizations for speeding up MILP.} For extremely large-scale MILP problems (e.g., dozens of model and workload types with hundreds of GPUs), we introduce several optimizations, such as pruning configurations, providing a good starting point, and early stopping based on the lower bound, as detailed in \autoref{appendix:optimizations}. The experimental results presented in \S\ref{sec:experiment} demonstrate the efficiency, effectiveness, and scalability of our scheduling algorithm.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/e2e_pic_8b.pdf}
    \caption{\small{End-to-end experiments on Llama3-8B model with different setups.}}
    \label{fig:e2e8b}
\end{figure}

\begin{figure} [!t]
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/price_vs_perf.pdf}
    \caption{\small{System performance v.s. price budget.}}
    \label{fig:perfvsbudget}
\end{figure}

\section{Other Optimizations for Speeding up MILP}
\label{appendix:optimizations}
For large numbers of GPUs and model types, it might take hours for the MILP solver to provide a relatively good solution. To expedite the search process, we introduce three optimizations to minimize the search space without sacrificing the effectiveness of our scheduling results:
(\underline{i}) for each model type, we prune configurations that are clearly dominated. For example, configurations with high degrees of model parallelism are retained for Llama3-70B, which requires substantial memory for model serving, but are pruned for Llama3-8B to prevent excessive communication overhead;
(\underline{ii}) we pre-estimate the resource requirements for each model type based on incoming workloads and their memory demands, and proportionally allocate resources to provide a good starting point for the MILP solver, thereby expediting the search process;
(\underline{iii}) we establish a theoretical lower bound for the makespan by analyzing the minimum possible processing time across all feasible configurations, which enables the implementation of early stopping criteria during optimization, i.e., the search process stops when it finds a solution that is very close to this lower bound. The minimum possible makespan occurs when all workloads are assigned to the most efficient configuration without considering resource constraints.

\section{Real Time GPU Availabilities}
\label{appendix:availability}
We randomly selected four real-time GPU availabilities on the cloud, as shown in \autoref{tab:availability}.

\begin{table}[ht]
\centering
\caption{Real time GPU availabilities on cloud platform.}
% \resizebox{\linewidth}{!}{
% \small
\begin{tabular}{lcccccc}
\hline
       \textbf{GPU Avails} & \textbf{4090} & \textbf{A40} & \textbf{A6000} & \textbf{L40} & \textbf{A100} & \textbf{H100} \\
\hline
Avail 1 & 16   & 12  & 8     & 12  & 6    & 8    \\
\hline
Avail 2 & 32   & 8   & 16    & 16  & 7    & 12   \\
\hline
Avail 3 & 32   & 16  & 8     & 8   & 32   & 8    \\
\hline
Avail 4 & 24   & 24  & 24    & 16  & 4    & 8    \\
\hline
\end{tabular}
% }
\label{tab:availability}
\end{table}

\section{Workload Type Ratios for Each Trace}
\label{appendix:workload ratios}

We demonstrate the workload type ratios for the three traces in \autoref{tab:workload_ratios_full}.


\begin{table}[h!]
\centering
\caption{Workload type ratios for subsampled traces from the Swiss AI Center (Trace 1), Azure-Trace (Trace 2), and WildGPT dataset (Trace 3). Workloads 1â9 correspond to the nine workload types shown in~\autoref{fig:benchmark2} from left to right.}
\label{tab:workload_ratios_full}
% \small
% \resizebox{\linewidth}{!}{
\begin{tabular}{l c c c c c c c c c}
\hline
\textbf{Workloads}      & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} \\ \hline
Trace 1 (\%)  & 33         & 7          & 8          & 7          & 27         & 6          & 6          & 3          & 3          \\ \hline
Trace 2 (\%)      & 22         & 5          & 5          & 21         & 5          & 5          & 19          & 6         & 12         \\ \hline
Trace 3 (\%)  & 4          & 1          & 4          & 3          & 20         & 27         & 1          & 25         & 15         \\ \hline
\end{tabular}
% }
\end{table}

\section{End-to-end Experiment Results for Llama3-8B Model}
\label{appendix:e2e8b}

The end-to-end experiments on Llama3-8B model with different setups are shown in~\autoref{fig:e2e8b}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{System Performance vs Price Budget}
\label{sec: sysvsprice}

We further evaluate our system's performance compared to homogeneous baselines under various price budgets. As shown in~\autoref{fig:perfvsbudget}, as the price budgets increase (from 5 \$/h to 60 \$/h), the performance gap between our approach and the homogeneous setups narrows from approximately 30\% to 15\%. This is primarily due to the limited availability of cloud resources.
In homogeneous baselines, we assume an unlimited number of GPUs, allowing performance to scale linearly with the price budget. However, in cloud-based scenarios, resource restrictions prevent such linear scaling. When larger price budgets are applied, unsuitable GPUs for the current workload may be rented if they are the only available options, further limiting performance scalability.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
