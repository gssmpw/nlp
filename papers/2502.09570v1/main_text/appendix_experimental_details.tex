\section{Experimental details}

\subsection{Datasets}\label{appendix-datasets}

We consider multiple datasets commonly used for benchmarking in the literature, including social networks, chemical reaction networks, and citation networks. 

\subsubsection{Graph Datasets} 

Collab, Imdb and Reddit are  proposed in \citep{yanardag2015deep}. Collab is a collection of ego-networks where nodes are researchers. The labels correspond to the fields of research of the authors. Imdb is also a collection of ego-networks. Nodes are actors and an edge between two nodes is present if the actors played together. The labels correspond to the genre of movies used to construct the networks. 

Reddit
is a collection of graphs corresponding to online discussion
threads on reddit. Nodes correspond to users, who are connected if they replied to each other comments. The task consists in determining if the community is a discussion-community or a question answering community.


Mutag is a collection of graphs corresponding to nitroaromatic compounds \citep{debnath1991structure}. The goal is to predict their mutagenicity in the Ames test \citep{ames1973carcinogens} using S. typhimurium TA98.

Proteins and Enzymes are introduced in \citep{borgwardt2005protein}. 
These datasets use the 3D structure of the folded proteins to build a graph of amino acids \citep{borgwardt2005protein}.

Peptides is a chemical data set introduced in
\citep{dwivedi2022long}. The graphs are derived from peptides, short chains of amino acid, such that the nodes
correspond to the heavy (non-hydrogen)
while the edges represent the bonds between them. Peptides-func is a graph classification task, with a total of 10 classes
based on the peptide function (Antibacterial, Antiviral, etc). peptides-struct is a graph regression task.

We outline basic characteristics of these datasets in Tab.~\ref{tab:dataset_graph_stats}.


\begin{table}[H]
\tiny
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
 & Collab & Imdb & Reddit & Mutag & Enzymes & Proteins & Peptides-func & Peptides-struct  \\
\hline
\# graphs & 5000  & 1000  & 2000  & 188 & 600 & 1113 & 15,535 & 15,535 \\
\hline
avg. \# node per graph & 74.49 & 19.77 & 425.57 &  17.93 & 31.86 & 37.40  & 150.94 & 150.94  \\
\hline
\# classes &  3 & 2 & 2 & 2 & 6 & 2 & 10 & - \\
\hline
\end{tabular}
\caption{Dataset Statistics for Collab, Imdb, Reddit, Mutag, Enzymes, Proteins and Peptides.}
\label{tab:dataset_graph_stats}
\end{table}


\subsubsection{Hypergraph Datasets} 

We use five datasets that are naturally parametrized as hypergraphs: pubmed, Cora co-authorship (Cora-CA), cora co-citation (Cora-CC), Citeseer \citep{sen2008collective} and DBLP \citep{rossi2015network}. We use the same pre-processed
hypergraphs as in~\citet{yadati2019hypergcn}, which are taken from~\citet{huang2021unignn}. %[Might only have the largest CComponent of each of these.] 
The hypergraphs are created with each vertex representing a document. The Cora data set, for example, contains machine learning papers divided into one of seven classes. In a given graph of the co-authorship datasets Cora-CA and DBLP, all documents co-authored by one author form one hyperedge. In pubmed, citeseer and Cora-CC, all documents cited by an author from one hyperedge. We outline basic characteristics of these datasets in Tab.~\ref{tab:dataset_hg_stats}.


\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & Pubmed & Cora-CA & Cora-CC & Citeseer & DBLP \\
\hline
\# hypernodes, $V$ & 19717 & 2708 & 2708 & 3312 &  43413 \\
\hline
\# hyperedges, $E$ & 7963 & 1072 & 1579 & 1079 & 22535 \\
\hline
\# features, $d$ & 500 & 1433 & 1433 & 3703 & 1425 \\
\hline
\# classes, $q$ & 3 & 7 & 7 & 6 & 6\\
\hline
\end{tabular}
\caption{Dataset Statistics}
\label{tab:dataset_hg_stats}
\end{table}


\subsubsection{BREC Dataset for empirical expressivity analysis}\label{apx:data-brec}

The BREC dataset is an expressiveness
dataset containing 1-WL-indistinguishable graphs in 4 categories: \textbf{B}asic, \textbf{R}egular, \textbf{E}xtension, and \textbf{C}FI graphs \citep{wang2024empirical}. The 140 pairs of regular graphs are further sub-categorized into simple regular graphs (50 pairs), strongly
regular graphs (50 pairs), 4-vertex condition graphs (20 pairs) and distance regular graphs (20 pairs). Note that we remove pairs that include non-connected graphs from the original 400 pairs to arrive at a total of 390 pairs. Graphs in the Basic category (60 pairs, of which we remove 4) are non-regular. Some of the CFI graphs are 4-WL-indistinguishable. We provide a plot of the number of nodes and edges in each categories' graphs in Fig.~\ref{fig:brec-degrees} and Fig.~\ref{fig:brec-edge-counts}.



\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{images/brec_node_distribution.png}
  \caption{Histogram of the number of nodes in the graphs in BREC. The bars are stacked. We exclude pairs containing non-connected graphs from the original 800 graphs to arrive at 780 graphs. Best seen in color.}
  \label{fig:brec-degrees}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{images/brec_edge_distribution.png}
  \caption{Histogram of the number of edges in the graphs in BREC. The bars are stacked. We exclude pairs containing non-connected graphs from the original 800 graphs to arrive at 780 graphs. Best seen in color.}
  \label{fig:brec-edge-counts}
\end{figure}


\subsection{Hyperparameters}

\textbf{For GNNs}

We outline the hyperparameter used for Tab.~\ref{tab:node}, Tab.~\ref{tab:gcn}, Tab.~\ref{tab:gps} and Tab.~\ref{tab:gin_selected} in Tab.~\ref{tab:node-params}, Tab.~\ref{tab:gcn-params}, Tab.~\ref{tab:gps-params}.


\begin{table}[H]
\footnotesize
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Features} & \textbf{citeseer-CC} & \textbf{Cora-CA} & \textbf{Cora-CC} & \textbf{Pubmed-CC} & \textbf{DBLP}\\
\hline
Num. Layers & 3 & 3 & 3 & 3 & 3 \\
Hidden Dim. & 128 & 128 & 128 & 128 & 128  \\
Learning Rate & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 \\
Dropout & 0.2 & 0.2 & 0.2 & 0.2 & 0.2  \\
Batch Size & 50 & 50 & 50 & 50 & 50 \\
Epochs & 300 & 300 & 300 & 300 & 300  \\
\hline
\end{tabular}
\caption{Hyperparameter settings for Tab. \ref{tab:node}.}\label{tab:node-params}
\end{table}

\begin{table}[H]
\footnotesize
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
\textbf{Features} & \textbf{Collab} & \textbf{Imdb} & \textbf{Reddit} & \textbf{Mutag} & \textbf{Enzymes} & \textbf{Proteins} & \textbf{Peptides-f} & \textbf{Peptides-s} \\
\hline
Num. Layers & 4 & 4 & 4 & 4 & 4 & 4 & 8 & 8 \\
Hidden Dim. & 64 & 64 & 64 & 64 & 64 & 64 & 235 & 235 \\
Learning Rate & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 \\
Dropout & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.1 & 0.1 \\
Batch Size & 50 & 50 & 50 & 50 & 50 & 50 & 50 & 50\\
Epochs & 300 & 300 & 300 & 300 & 300 & 300 & 300 & 300\\
\hline
\end{tabular}
\caption{Hyperparameter settings for Tab. \ref{tab:gcn}.}\label{tab:gcn-params}
\end{table}


\begin{table}[H]
\footnotesize
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Features} & \textbf{Collab} & \textbf{Imdb} & \textbf{Reddit}  \\
\hline
MP-Layer & GIN & GIN & GIN  \\
Num. Layers & 4 & 4 & 4  \\
Hidden Dim. & 64 & 64 & 64  \\
Learning Rate & 0.001 & 0.001 & 0.001  \\
Dropout & 0.2 & 0.2 & 0.2 \\
Batch Size & 50 & 50 & 50 \\
Epochs & 300 & 300 & 300 \\
\hline
\end{tabular}
\caption{Hyperparameter settings for Tab. \ref{tab:gin_selected}.}\label{tab:gps-params}
\end{table}


