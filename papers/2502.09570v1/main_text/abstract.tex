Higher-order information is crucial for relational learning in many domains where relationships extend beyond pairwise interactions. Hypergraphs provide a natural framework for modeling such relationships, which has motivated recent extensions of graph neural network architectures to hypergraphs. However, comparisons between hypergraph architectures and standard graph-level models remain limited. In this work, we systematically evaluate a selection of hypergraph-level and graph-level architectures, to determine their effectiveness in leveraging higher-order information in relational learning. Our results show that graph-level architectures applied to hypergraph expansions often outperform hypergraph-level ones, even on inputs that are naturally parametrized as hypergraphs. As an alternative approach for leveraging higher-order information, we propose hypergraph-level encodings based on classical hypergraph characteristics. While these encodings do not significantly improve hypergraph architectures, they yield substantial performance gains when combined with graph-level models. Our theoretical analysis shows that hypergraph-level encodings provably increase the representational power of message-passing graph neural networks beyond that of their graph-level counterparts.

%We compute encodings for Graph Neural Networks (GNNs) by considering different topological domains: the graph-level and the hypergraph-level. We show that GNNs architectures augmented with hypergraph encodings outperform GNNs with no encodings or with encodings computed at the graph level, as well as some SOTA Hypergraph Neural Networks (HNNs) architectures such as UniGCN, UniGAT. [Melanie to clean up]

%Our findings highlight the potential of hypergraph-level encodings as effective tools for leveraging higher-order information and offer insights for architecture design for hypergraph learning.