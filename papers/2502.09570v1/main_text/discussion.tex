\section{Discussion}\label{discussion}
In this study, we investigated the performance of hypergraph-level architectures in comparison with graph-level architectures for “multi-way” relational learning tasks. Additionally, we proposed hypergraph-level encodings as an alternative approach for leveraging higher-order relational information.\\

\noindent \textbf{Lessons for model design}
Our findings indicate that graph-level architectures applied to hypergraphs' clique expansions frequently outperform hypergraph-level architectures, even when the inputs are naturally parametrized as hypergraphs. While hypergraph-level encodings do not significantly enhance the performance of hypergraph-level architectures, they can lead to substantial performance gains when used in graph-level architectures. Notably, random walk-based (H-$k$-RWPE) and curvature-based encodings (HCP) were particularly effective across data sets. These insights suggest a graph-level architecture augmented with hypergraph-level encodings as a suitable model choice for a wide range of existing hypergraph learning tasks.\\

\noindent \textbf{Limitations} 
A key limitation of this study, and many of the related works, is a lack of benchmarks consisting of \emph{true} hypergraph-structured data. Many of the existing data sets consists of graphs that are reparametrized (``lifted'') to hypergraphs, or hypergraphs that can be easily reparametrized as graphs. This suggests the establishment of better benchmark as a key direction for future work. Given the promise of topological deep learning for scientific machine learning, we envision future benchmarks that are based on scientific data, such as~\citep{garcia2023chemically} or ~\citep{gjorgjieva2011triplet}, where multi-way interactions that are naturally parametrized as hypergraphs are known to arise.
Another limitation of this study arises in the choice of hypergraph architectures. While our selection was guided by top-performing models in recent benchmarks~\citep{huang2021unignn,telyatnikov2024topobenchmark}, a more comprehensive analysis could further strengthen the validity of the reported observations.\\

\noindent \textbf{Other Future Directions} 
Despite the aforementioned caveats regarding datasets and the breadth of architectures included in this study, our observations raise questions about the effectiveness of existing message-passing schemes on hypergraphs. We believe that a thorough analysis of these architectures’ ability to effectively encode higher-order information into learned representations is an important direction for future work. A possible lens for such an investigation could be graph reasoning tasks, as previously suggested in~\citep{luo2023expressiveness}. 

Additionally, the negative results observed regarding hypergraph-level encodings paired with hypergraph-level architectures warrant further exploration. Specifically, understanding how to effectively augment hypergraph inputs with structural and positional information that can be leveraged by hypergraph-level architectures is a promising direction for further study.

Lastly, while this study primarily focused on hypergraph learning, there are several other topological domains of interest, including simplicial complexes, polyhedral complexes, and more general CW complexes. Extending the present study to these domains represents another interesting avenue for further investigation.