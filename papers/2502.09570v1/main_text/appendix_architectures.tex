\section{Architectures}

\subsection{GNN architectures}\label{appendix-gnn-architectures}

\textbf{GCN} extends convolutional neural networks to graph-structured data. It derives a shared representation by integrating node features and graph connectivity through message-passing. Mathematically, a GCN layer is expressed as  
\[
X^{l+1} = \sigma \left( \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} X^{l} W^{l} \right) \; ,
\]
where \(W^{l}\) is the learnable weight matrix at layer \(l\), and  
$\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$ is the normalized adjacency matrix of the original graph with added self-loops. This graph has adjacency matrix $\tilde{A}=A+I_N$ and node degree matrix $\tilde{D}$. The activation function \(\sigma\) is typically chosen as ReLU or a sigmoid function. 


\textbf{GIN} is a message-passing graph neural network (MPGNN) designed for maximum expressiveness, meaning it can learn a broader range of structural patterns compared to other MPGNNs like GCN. GIN is inspired by the Weisfeiler-Lehman (WL) graph isomorphism test. Formally, the GIN layer is given by  
\begin{equation}
x_i^{l+1} = \text{MLP}^{l} \left((1+\epsilon)\cdot x_i^{l}+ 
\sum_{j \in \mathcal{N}_i} x_j^{l} \right)
\end{equation}
where \(x_i^{l}\) denotes the feature of node \(i\) at layer \(l\), \(\mathcal{N}_i\) represents the neighbors of node \(i\), and \(\epsilon\) is a learnable parameter. The update step is carried out using a multi-layer perceptron \(\text{MLP}(\cdot)\), which is a fully connected neural network.


\textbf{GraphGPS} is a hybrid graph transformer (GT) model that integrates MPGNNs with transformer layers to effectively capture both local and global patterns in graph learning. It enhances standard GNNs by incorporating positional encodings (which provide node location information) and structural encodings (which capture graph-theoretic properties of nodes). By alternating between GNN layers (for local aggregation) and transformer layers (for global attention), GraphGPS can efficiently model both short-range and long-range dependencies in graphs. It employs multi-head attention, residual connections, and layer normalization to maintain stability and improve learning performance. Mathematically, GraphGPS updates the node and edge features as follows:

$$X^{l+1},E^{l+1}=\text{GPS}^l(X^l, E^l, A)$$
computed as:
$$X^{l+1}_M,E^{l+1}=\text{MPNN}^l_e(X^l, E^l, A)$$
$$X^{l+1}_T=\text{GlobalAttn}^l(X^l)$$
$$X^{l+1}=\text{MLP}(X_M^{l+1}+X_T^{l+1})$$

where $\text{MLP}(\cdot)$ is a 2-layer Multi-Layer Perceptron (MLP) block. Note that we omit the batch normalization in this exposition.


\subsection{HNN architectures}\label{hnn-architectures}
Models on the hypergaph-level domain are approaches that preserve the hypergraph structure during learning \citep{kim2024survey}.
\citet{huang2021unignn} proposes UniGCN, UniGIN, UniGAT, UniSAGE and UniGCNII, which directly generalize the classic  GCN, GIN, GAT \citep{velivckovic2017graph},and GraphSAGE \citep{hamilton2017inductive} and GNCII \citep{chen2020simple}.\\

\subsubsection{UniGCN}
% these gtuys use d_i = d_i +1!!
UniGCN follows the two-phase scheme \ref{two-phase-scheme} and sets the second aggregation function $\phi_2$ to be
\begin{equation}
\tilde{x}_i^{l+1} = \frac{1}{\sqrt{d_i+1}} \sum_{e \in \tilde{E}_i} \frac{1}{\sqrt{\overline{d}_e}} W^l h_e^{l+1} \; ,
\end{equation}
where $\overline{d}_e=\frac{1}{|e|}\sum_{i\in e} (d_i+1)$ is the average degree of an hyperedge (after adding self-loops to the original hypergraph), and where $\tilde{\mathcal{N}}_i$ and $\tilde{E}(i)$ are the neighborhood of vertex $i$ and the incident hyperedges to $i$ after adding self loops. \\ 


\subsubsection{UniGIN}

UniGIN also follows the two-phase scheme (see Eq.~\ref{two-phase-scheme}) and sets the second aggregation function $\phi_2$ to be

\begin{equation} 
\tilde{x}_i^{l+1} = W^l\left( (1 + \varepsilon)x_i^l + \sum_{e \in E_i} h_e^{l+1} \right) \; .
\end{equation} 

\subsubsection{UniGAT} UniGAT adopts an attention mechanism to assign importance score to each of the center nodeâ€™s neighbors \citep{huang2021unignn}. The attention mechanism is formulated as
\begin{align}
\alpha_{ie}^{l+1} &= \sigma \left( a^T \left[ W^l h_{\{i\}}^{l+1} ; W^l h_e^{l+1} \right] \right) \\
\tilde{\alpha}_{ie}^{l+1} & = \frac{\exp (\alpha_{ie}^{l+1})}{\sum_{e' \in \tilde{E}_i} \exp (\alpha_{ie'}^{l+1})} \\
\tilde{x}_i^{l+1} & = \sum_{e \in \tilde{E}_i} \tilde{\alpha}_{ie}^{l+1} W^l h_e^{l+1} \; .
\end{align}

\subsubsection{UniSAGE}
UniSAGE follows the two-phase scheme as detailed in Equ.~\ref{two-phase-scheme} and sets the second aggregation function $\phi_2$ to be
\begin{equation} \tilde{x}_i^{l+1} = W^l(x_i^l + \text{AGGREGATE} (\{h_e^{l+1}\}_{e\in E_i}
)) \end{equation}

\subsubsection{UniGCNII}
UniGCNII updates node features using:
\begin{align} \hat{x}_i^{l+1} & = \sqrt{\frac{1}{d_i+1}} \sum_{e \in \tilde{E_i}} \sqrt{\frac{1}{\overline{d}_e}} h_e^{l+1} \\
     \tilde{x}_i^{l+1} & = \left((1 - \beta)I + \beta W^l\right)\left((1 - \alpha)\hat{x}_i^{l+1} + \alpha x^0_i\right) \end{align} 
     where $\alpha$ and $\beta$  are hyperparameters.
