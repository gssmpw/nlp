\section{Background}
\label{background}
We consider graphs $G=(X,E)$ with node attributes $X \in \mathbb{R}^{\vert V \vert \times m}$ and edges $E \subseteq V \times V$, representing pairwise relations between nodes in $V$. We further consider hypergraphs $H=(X,F)$ where hyperedges $F$ denote relations between groups of nodes. Hypergraphs can be reparametrized as graphs using clique expansions; for more details see Apx.~\ref{appendix-gtohg}.

\subsection{Architectures}


\begin{table*}[h!]
\footnotesize
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Architecture}  & \textbf{Type} & \textbf{Level} & \textbf{Update Function} \\ \hline
    GCN \citep{kipf2016semi} & MP & graph & \makecell{$X^{l+1} = \sigma \left(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2} X^{l} W^{l} \right)$ \\ $\tilde{A}=A+I_N$ \\ $\tilde{D}_{ii}=\sum_j \tilde{A}_{ij}$}  \\ \hline
    GIN \citep{xu2018powerful} & MP & graph & \makecell{$X^{l+1} = \text{MLP}^{l} \left( \left( 1 + \epsilon \right) X^{l} + AX^{l} \right)$}  \\ \hline
    GPS \citep{rampavsek2022recipe} & hybrid (MP, T) & graph  & \makecell{$X^{l+1},E^{l+1}=\text{GPS}^l(X^l, E^l, A)$ \\
$X^{l+1}_M,E^{l+1}=\text{MPNN}^l_e(X^l, E^l, A)$ \\
$X^{l+1}_T=\text{GlobalAttn}^l(X^l)$ \\
$X^{l+1}=\text{MLP}(X_M^{l+1}+X_T^{l+1})$} \\ \hline 
     UniGCN \citep{huang2021unignn} & MP & hypergraph &  \makecell{$\tilde{x}_i^{l+1} = \frac{1}{\sqrt{d_i+1}} \sum_{e \in \tilde{E}_i} \frac{1}{\sqrt{\overline{d}_e}} W^l h_e^{l+1}$} \\ \hline
     UniGIN \citep{huang2021unignn} & MP & hypergraph & $\tilde{x}_i^{l+1} = W^l\left( (1 + \varepsilon)x_i^l + \sum_{e \in E_i} h_e^{l+1} \right)$\\ \hline
     UniGAT \citep{huang2021unignn} & MP & hypergraph &   \makecell{
$\alpha_{ie}^{l+1} = \sigma \left( a^T \left[ W^l h_{\{i\}}^{l+1} ; W^l h_e^{l+1} \right] \right)$, \\
$\tilde{\alpha}_{ie}^{l+1} = \frac{\exp (\alpha_{ie}^{l+1})}{\sum_{e' \in \tilde{E}_i} \exp (\alpha_{ie'}^{l+1})}$, \\
$\tilde{x}_i^{l+1} = \sum_{e \in \tilde{E}_i} \tilde{\alpha}_{ie}^{l+1} W^l h_e^{l+1}$} \\ \hline 
     UniSAGE \citep{huang2021unignn} & MP & hypergraph &  $\tilde{x}_i^{l+1} = W^l(x_i^l + \text{AGGREGATE} (\{h_e^{l+1}\}_{e\in E_i}
))$\\ \hline
     UniGCNII \citep{huang2021unignn} & MP & hypergraph & \makecell{$\hat{x}_i^{l+1} = \sqrt{\frac{1}{d_i+1}} \sum_{e \in \tilde{E_i}} \sqrt{\frac{1}{\overline{d}_e}} h_e^{l+1}$ \\
     $\tilde{x}_i^{l+1} = \left((1 - \beta)I + \beta W^l\right)\left((1 - \alpha)\hat{x}_i^{l+1} + \alpha x^0_i\right)$ \\ $\text{where } \alpha \text{ and } \beta \text{ are hyperparameters}$}
\\ \hline
  \end{tabular}
  \caption{Overview of Architectures. $W^l$ represents a trainable weight matrix for layer $l$. $
  \epsilon$ represents a learnable parameter. We use matrix notation for graph architectures, and vector notation for hypergraphs.}
  \label{tab:architectures-overview}
\end{table*}

\paragraph{Message-passing GNN}
Message-Passing (MP)~\citep{gori2005new,Hamilton:2017tp} is a prominent learning paradigm in relational learning, where a node’s representation is iteratively updated based on the representations of its neighbors. Formally, let $x_v^l$ denote the representation of node $v$ at layer $l$. Message-passing implements the following update,
$$x_v^{l+1} = \phi_l \Big( \bigoplus_{p \in \mathcal{N}_v \cup \{v\}} \psi_l \left ( x_p^l\right)\Big)$$,
where $\psi_l$ denotes an aggregation function (e.g., averaging) acting on the 1-hop neighborhood $\mathcal{N}_v$ of $v$, and $\phi_l$ an update function with trainable parameters, such as an MLP. The number of MP iterations is commonly referred to as the \emph{depth} of the network. Representations are initialized by the node attributes in the input.


\paragraph{Transformer-based GNN}
The second major class of architectures for relational learning is transformer-based (T). Networks consist of blocks of multi-head attention layers ($GlobalAttn(\cdot)$), followed by fully-connected feedforward networks. In the recent literature, hybrid architectures, which combine MP and attention layers, have been shown to exhibit strong performance on several state of the art benchmarks~\citep{rampavsek2022recipe}.

\paragraph{Graph-level architectures}
Our selection of graph-level architectures includes two MPGNNs and one hybrid architecture. GCN~\citep{kipf2016semi} is one of the simplest and most popular MPGNNs, making it an important reference point. GIN~\citep{xu2018powerful} is designed to be a  maximally expressive MPGNN. GraphGPS~\citep{rampavsek2022recipe} is a widely used hybrid architecture that performs well across the benchmarks considered here. As baselines, we evaluate simple instances of all three architectures without additional model interventions. An overview of the architectures can be found in Tab.~\ref{tab:architectures-overview}; more detailed descriptions are deferred to Apx.~\ref{appendix-gnn-architectures}.

\paragraph{Hypergraph-level architectures}
The architectures analyzed in this study implement message-passing, which on hypergraphs is implemented via a two-phase scheme: messages are passed from nodes to hyperedges and then back to nodes~\citep{huang2021unignn}. Formally,
\begin{align}\label{two-phase-scheme}
h_e^{l+1} =& \phi_1 \left( \left\{ x_j^l \right \}_{j \in e}\right)\\
\tilde{x}_i^{l+1} = &\phi_2 \left(x_i^l, \left\{h_e^{l+1}\right\}_{e \in E_i} \right) \; . \nonumber
\end{align}
Here, $x_j$ denotes the node features of node $j$, $h_e$ denotes the edge feature of edge $e$, $E_j$ is the set of all hyperedges containing $j$, and $\phi_1$ and $\phi_2$ are permutation-invariant functions for aggregating messages from vertices and hyperedges respectively.  $\tilde{x}_i$ indicates the output of the message passing layer before activation or normalization. Tab.~\ref{tab:architectures-overview} provides an overview of the hypergraph-level architectures considered here; more detailed description can be found in Apx.~\ref{hnn-architectures}.

% ---------------- %

\subsection{Encodings}

Structural (SE) and Positional (PE) encodings enhance MPGNNs by providing access to structural information that is crucial for downstream tasks, but that these networks cannot inherently learn~\citep{dwivedi2023benchmarking, rampavsek2022recipe}. Encodings can capture either local or global properties of the input graph. Local PEs supply nodes with information about their position within local clusters or substructures, such as their distance to the centroid of their community. In contrast, global PEs convey a node's overall position within the entire graph, often based on spectral properties like the eigenvectors of the Graph Laplacian~\citep{kreuzer2021rethinking} or random-walk based node similarities~\citep{dwivedi2021graph}. Graph-level SEs capture structural information, such as pair-wise node distances, node degrees, or statistics regarding the distribution of neighbors’ degrees~\citep{cai2018simple}, or discrete curvature~\citep{fesser2023effective}. Empirical evidence demonstrates that incorporating these PEs and SEs significantly improves the performance of GNNs~\citep{rampavsek2022recipe}.

\subsection{Representational Power}\label{appendix-1wl}
A key theoretical question in evaluating the effectiveness of different relational learning architectures is their \emph{representational power} or \emph{expressivity}: Which functions can and cannot be learned by the model? This question can be analyzed through the lens of a model’s ability to distinguish graphs that are not topologically identical (isomorphic). The 1-Weisfeiler-Leman (1-WL) test~\citep{weisfeiler1968reduction} provides a heuristic for this question. Notably,~\citet{xu2018powerful} showed that MPGNNs (specifically, GIN) are as expressive as the 1-WL test.
While 1-WL (and, by extension, MPGNNs) is effective for many classes of graphs, it has notable limitations, such as in distinguishing regular graphs. Generalizations of this procedure, known as the $k$-WL test, establish a hierarchy of progressively more powerful tests. At the same time, several graph characteristics are known to be more expressive than the 1-WL test. Consequently, combining MPGNNs with encodings based on these characteristics can enhance their expressivity~\citep{southern2023expressive, fesser2023effective, bouritsas2022improving}. See  Apx.~\ref{appendix-detailed-comparaison} for a detailed expressivity analysis.
