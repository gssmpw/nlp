\section{Extended Background}

\subsection{Hypergraph Expansions}\label{appendix-hgtog}
There exist several expansion techniques for reparametrizing hypergraphs as graphs. Here, we focus on clique expansion, which we empirically found to be the best performing expansion. For more details see, e.g.,~\citep{sun2008hypergraph}.

%\textbf{Clique expansion} 
To reparametrize a hypergraph $H=(V,E_H)$ as a graph via \emph{clique expansion}, we define $G=(V, E_G)$ where $E_G=\{\{u,v\}| \{u,v\} \subseteq e, e \in E_H \} $. An example is given in Fig.~\ref{fig:expansion-hg-to-g}.
%
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{images/hg_expansion.png}
  \caption{Example of a clique expansion of a hypergraph to a graph. The plots are created using NetworkX \citep{hagberg2008exploring} and HyperNetX \citep{praggastis2023hypernetx}.}
  \label{fig:expansion-hg-to-g}
\end{figure}


\subsection{Lifting graphs to hypergraphs}\label{appendix-gtohg}
The term ``lifting'' refers generally to the reparametrization of one topological domain to another, usually one that captures richer higher-order information. In our setting we lift graphs to hypergraphs by adding hyperedges to groups of nodes that are pairwise interconnected. 
An example of a lift of a graph to a hypergraph is shown in Fig.~\ref{fig:lifting-g-to-hg}.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{images/hypergraph_lifting.png}
  \caption{Lifting of a graph to a hypergraph.}
  \label{fig:lifting-g-to-hg}
\end{figure}



\subsection{Weighted-Edges (WE) Hypergraph Random walks}

We define Weighted-Edges Random Walks (WE), which induce the following measure
\begin{equation}\label{measures-we}
\mu_i^{\text{WE}}(j) = \begin{cases} 
\mathbb{P}^{\text{WE}}(i \rightarrow j) \text{,} \text{ if } j 
\in \mathcal{N}_i \\
0 \text{ otherwise} 
\end{cases} \; ,
\end{equation}
where $\mathcal{N}_i$ are the neighbors of $i$ and transition probabilities are given by
\begin{equation}
\mathbb{P}^{\text{WE}}(i \rightarrow j)  = \frac{1}{\sum_{ \{f|i \in f \}}(|f|-1)} \sum_{\{e | \{i,j\} \subseteq e\}} 1 \; .
\end{equation}

The probability of picking a hyperedge is directly proportional to the number of nodes in the hyperedge minus 1.


\subsection{Laplacians}\label{Laplacians}
Several notions of Laplacians have been studied on hypergraphs. In this work, we consider two types of Laplacians on graphs for implementing H-LAPE, the Hodge-Laplacian, with we defined in the main text, and the normalized Laplacian, which we discuss below. Additionally, we comment on random walks hypergraphs Laplacians. However, since they need not be symmetric, there are not suitable for use in H-LAPE. Nonetheless, their spectrum provides an additional means for defining structural encodings. 

\subsubsection{Normalized graph and hypergraph Laplacian}\label{laplacians-appendix}

\noindent For graphs, the \emph{symmetrically normalized graph Laplacian} is defined as
\begin{equation} 
I-D_v^{-1/2}AD_v^{-1/2} =  D_v^{-1/2}LD_v^{-1/2} \; ,
\end{equation}
where $L = D_v - A$ is the standard graph Laplacian.



The \emph{normalized hypergraph Laplacian} \citep{zhou2006learning, feng2019hypergraph} is defined as


\begin{equation} \label{normalizedLaplacian} 
\Delta = I - D_v^{-1/2}B_1D_e^{-1}B_1^TD_v^{-1/2} = D_v^{-1/2}(D_v -B_1D_e^{-1}B_1^T)D_v^{-1/2} \; ,
\end{equation}
where $D_v$ and $D_e$ are the diagonal node and edge degree matrices. The Dirichlet energy $E(f)$ of a scalar function on a hypergraph is defined as
\begin{equation} 
E(f) = \frac{1}{2}\sum_{e\in E} \sum_{\{u,v\}\subseteq e} \frac{1}{|e|} \left( \frac{f(u)}{\sqrt{d(u)}} - \frac{f(v)}{\sqrt{d(v)}} \right)^2 \; .
\end{equation}
The normalized hypergraph Laplacian satisfies
\begin{equation} 
E(f) = f^T\Delta f \; ,
\end{equation}
which establishes that the normalized hypergraph Laplacian is positive semi-definite~\citep{zhou2006learning}. The smallest eigenvalue of $\Delta$ is $0$.



\subsubsection{Random walks hypergraphs Laplacians}

For a graph, the \textit{random walk Laplacian} is defined as $L=I-D^{-1}A$, where, as usual, $D$ denotes the degree matrix and $A$ the adjency matrix. The probability of a random walk transitioning from node $i$ to $j$ is given by $-L_{ij}=\frac{A_{ij}}{d_i}$. \citet{mulas2022random}~introduce a generalized random-walk Laplacians on hypergraphs: For any random walk on a hypergraph, they define in analogy to the graph case
\begin{equation} 
L_{ij} = \begin{cases} 1 \text{ if } i=j \\
 - \mathbb{P}(i \rightarrow j)\end{cases} \; .
 \end{equation}
This random walk notion is equivalent to the EE scheme in~\citet{coupette2022ollivier}, defined in the main text: Starting at $v$, choose one of the hyperedges containing $v$ with equal probability, then select any of the vertices of the chosen hyperedge (other than $v$) with equal probability. Formally, we write \begin{equation} \label{laplacian} \mathbb{P}(i \rightarrow j) = \frac{\mathcal{A}_{ij}}{\mathcal{D}_{ii}} \; . \end{equation}

A similar notion was previously studied in~\citep{banerjee2021spectrum}. 

Note that the random-walk Laplacian need not be symmetric. As a result, it is not suitable for defining H-LAPE. However, in some recent works, the spectrum of the graph Laplacian, rather than its eigenvectors, have been used as SE~\citep{kreuzer2021rethinking}. An analogous notion can be defined at the hypergraph level, which we term \emph{Hypergraph Laplacian Structural Encoding (H-LASE)}. We analyze the expressivity of such SEs, establishing that they a provably more expressive than the 1-WL test/ MPGNNs.


\begin{theorem}\label{thm:lape_exp} \textbf{(H-LASE Expressivity).} MPGNNs with H-LASE are strictly more expressive than the 1-WL test and hence than MPGNNs without encodings. Further, there exist graphs, which can be distinguished using H-LASE, but not using standard, graph-level LASE. 
\end{theorem}

\textit{Proof.} Consider the 4 by 4 Rook and Shrikhande graphs: the two graphs are isospectral using the Normalized, Random Walk and Hodge Laplacians. But the two graphs's liftings to hypergraphs are not isospectral for the Normalized Laplacian. $\square$. 

\subsection{Discrete Curvature}\label{apx:curvature}
\paragraph{Forman's curvature}
\citet{Forman2003BochnersMF} proposed a curvature definition on CW complexes, which derives from a fundamental relation between Ricci curvature and Laplacians (Bochner-Weizenb{\"o}ck identity). For a simple, undirected, and unweighted graph $G = (V, E)$, the Forman-Ricci curvature (FRC) of an edge $e = (u, v) \in E$ is given by:
\begin{equation*}
    \mathcal{FR} (u,v) = 4 - \deg(u) - \deg(v)
\end{equation*}
The edge-based Forman curvature definition can be extended to capture curvature contributions from higher-order structures. Incorporating cycle counts up to order $k$ (denoted as $\mathcal{AF}_k$) has been shown to enrich the utility of the notion. Setting $k=3$ and $k=4$, the \emph{Augmented Forman-Ricci curvature} is given by
\begin{equation*}
\begin{split}
    \mathcal{AF}_3 (u,v) &= 4 - \deg(u) - \deg(v) + 3 \triangle(u,v)\\
    \mathcal{AF}_4 (u,v) &= 4 - \deg(u) - \deg(v) + 3 \triangle(u,v) + 2 \square(u,v) \; ,
\end{split}
\end{equation*}
where $\triangle(u,v)$ and $\square(u,v)$ represent the number of triangles and quadrangles containing the edge $(u,v)$. Prior work in the graph machine learning literature has demonstrated the effectiveness of these notions, e.g.,~\citep{fesser2024mitigating,fesser2024augmentations}. To the best of our knowledge, characterizations of such higher-order information via hypergraph curvatures have not been previously studied in this literature.


\paragraph{Ollivier's curvature}
% copied from main text, needs edit
We also consider the Ollivier-Ricci Curvature (ORC), a notion of curvature on metric spaces equipped with a probability measure \citep{ollivier2007ricci}. On graphs endowed with the shortest path distance $d(\cdot,\cdot)$, the ORC of an edge $\{i,j\}$ is defined as
\begin{equation} 
\kappa(i,j) = 1 - \frac{W_1(\mu_i, \mu_j)}{d(i,j)} \; ,
\end{equation}
where $W_1$ denotes the Wasserstein distance. Recall that, in general, $W_1(\cdot,\cdot)$ between two probability distributions $\mu_1, \mu_2$ is defined as
\begin{equation}
\label{eq:W-dist}
    W_1(\mu_1, \mu_2) = \inf_{\mu \in \Gamma(\mu_1,\mu_2)} \int d(x,y) \mu(x,y) \; dx \; dy \; ,
\end{equation}
where $\Gamma(\mu_1,\mu_2)$ is the set of measures with marginals $\mu_1,\mu_2$. In our case, the measures are defined by a uniform distribution over the 1-hop neighborhoods of the nodes $i$ and $j$.


\begin{rmk}\label{moregeneral-orc-graphs} \textbf{(ORC in a general setting).} As noted in \citep{southern2023expressive}, the ORC can be defined in a more general setting on graphs, where the metric $d$ does not have to be the shortest-path distance. Furthermore, the probability measures need not be uniform probability measures in the 1-hop neighborhood of the node. This is shown to be beneficial in distinguishing 3-WL indistinguishable graphs using the ORC computed with respect to measures induced by $m$-hop random walks where $m>1$.
\end{rmk}