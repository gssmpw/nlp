\section{Introduction}
%We study encodings for GNNs computed at the graph-level and the hypergraph-level. We show that computing encodings at the hypergraph-level is beneficial. We consider random-walks encodings, Laplacian encodings and curvature encodings. [Melanie to clean up]
\label{intro}
The utility of higher-order information has long been recognized in network science and graph machine learning: ``Multi-way networks'' arise in many domains in the social and natural sciences, where downstream tasks depend on relationships between groups of entities rather than the pairwise relationships captured in standard networks~\citep{bick2023higher,benson2021higher,schaub2021signal}.
%~\citep{lazaros2024graph, chauhan2024multi, duval2023phast, rittig2023graph, monti2019fake}. 
While graphs are limited to representing pairwise relationships, \emph{hypergraphs} effectively represent multi-way relationships by allowing \emph{hyperedges} between any number of vertices.

This enhanced flexibility has motivated a growing body of literature on extending classical graph neural network architectures to hypergraphs, including message-passing~\citep{huang2021unignn} and transformer-based models~\citep{liu2024hypergraph}. Typical validation studies compare hypergraph architectures against each other, but not against standard graph neural networks (GNNs). Here, we perform a comparison of a selection of both types of architectures. We observe that graph-level architectures strictly outperform current hypergraph-level ones, even if the input data is naturally parametrized as a hypergraph. In that case, the GNN is applied to the hypergraphâ€™s clique expansion, a natural reparametrization. This seemingly contradicts the intuition that leveraging higher-order information is useful.

Our observations raise the question, \emph{how can higher-order relational information be effectively utilized for learning?} In addition to encoding structural information as inductive biases directly into architectures, recent studies have demonstrated the effectiveness of using \emph{encodings} as an alternative approach. Here, the input graph is augmented with structural information, typically consisting of graph-level characteristics, such as spectral information~\citep{dwivedi2023benchmarking}, substructure counts~\citep{zhao2021stars}, or discrete curvature~\citep{fesser2023effective}. In this work, we investigate whether encodings computed at the hypergraph level enable better utilization of higher-order information in the sense of enhanced performance improvements.

We begin by proposing several hypergraph-level encodings using classical hypergraph characteristics and prove that they capture structural information that cannot be represented by traditional hypergraph message-passing schemes or graph-level encodings. We then conduct a systematic comparison of hypergraph-level and graph-level encodings when combined with graph- and hypergraph-level message-passing as well as transformer-based architectures. Our findings indicate that hypergraph-level encodings do not substantially enhance the performance of hypergraph-level architectures. However, significant performance gains are observed when hypergraph-level encodings are applied within graph-level message-passing and transformer-based architectures. 
We complement these experiments with an empirical analysis of the representational power of hypergraph-level encodings. 
Overall, we find that hypergraph-level encodings provide an effective means of leveraging higher-order information in relational data. 

\subsection{Related Work}
Topological Deep Learning has emerged as the dominant framework for learning on topological domains, including hypergraphs, as well as simplicial, polyhedral and more general cell complexes~\citep{hajij2022higher,hajij2024topox,papillon2023architectures}. Many classical graph-learning architectures have been extended to these domains. In the case of hypergraphs, this includes message-passing~\citep{huang2021unignn} and transformer-based~\citep{liu2024hypergraph} hypergraph neural networks. 

To the best of our knowledge, encodings have so far only been studied in the context of graph-level learning~\citep{dwivedi2023benchmarking}. Popular encodings leverage structural and positional information captured by classical graph characteristics~\citep{rampavsek2022recipe,kreuzer2021rethinking,cai2018simple,zhao2021stars,fesser2023effective,bouritsas2022improving}.


%\TODO{all: please collect refs below.} \mw{TODO writeup}
%\begin{itemize}
%    \item Topological Deep Learning (general): TopoX~\citep{hajij2024topox}, TopoBenchmark~\citep{telyatnikov2024topobenchmark}
%    \item Graph architectures: \cite{kipf2016semi, xu2018powerful, rampavsek2022recipe}
%    \item Hypergraph architectures: UniGCN/UniGAT etc 
%    \cite{huang2021unignn}, Hypergraph Transformer \cite{liu2024hypergraph}.
%    \item Encodings: Graph-level encodings~\citep{dwivedi2023benchmarking} GPS Paper has all the references
%    \item ...
%\end{itemize}

\subsection{Summary of Contributions}
The main contributions of this paper are as follows:
\vspace*{-4pt}
\begin{enumerate}
\setlength{\itemsep}{2pt}
\vspace*{-4pt}
    \item We provide experimental evidence that graph-level architectures applied to hypergraph expansions have comparable or superior performance to hypergraph-level ones, even on inputs that are naturally parametrized as hypergraphs.
    \item We introduce hypergraph-level encodings that allow for augmenting a (hyper-)graph-structured input with higher-order positional and structural information captured in hypergraph characteristics. We show that hypergraph-level encodings are provably more expressive than their graph-level counterparts.
    \item We show that hypergraph-level encodings can significantly enhance the performance of graph neural networks applied to hypergraph expansions.
\end{enumerate}

% So there are two things: take a graph and a GNN, augment the graph to a hypergraph, compute encodings there. Second: take a hg, do clique expansion, and do GNN instead of HNN. Discuss both.