\section{Experiments}
\label{experiments}

\begin{table*}[t!]
\centering
\tiny
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model (Encodings)} & \textbf{citeseer-CC} ($\uparrow$) & \textbf{cora-CA} ($\uparrow$) & \textbf{cora-CC} ($\uparrow$) & \textbf{pubmed-CC} ($\uparrow$) & \textbf{DBLP} ($\uparrow$) \\
\hline
GCN (No Encoding) & $69.28 \pm 0.28$ & $76.51 \pm 0.82$ & $75.43 \pm 0.26$ & $84.66 \pm 0.49$ & $75.66 \pm 0.81$ \\
GCN (HCP-FRC) & $\mathbf{71.03 \pm 0.51}$ & $78.43 \pm 0.76$ & $\mathbf{76.61 \pm 0.31}$ & $84.78 \pm 0.57$ & $76.49 \pm 0.90$ \\
GCN (HCP-ORC) & $70.89 \pm 0.54$ & $79.25 \pm 0.81$ & $76.09 \pm 0.70$ & $85.12 \pm 0.61$ & $76.57 \pm 0.85$ \\
GCN (EE H-19-RWPE) & $69.63 \pm 0.71$ & $76.84 \pm 0.69$ & $75.92 \pm 0.28$ & $86.24 \pm 0.63$ & $76.18 \pm 0.88$ \\
GCN (EN H-19-RWPE) & $68.85 \pm 0.91$ & $77.19 \pm 0.64$ & $75.33 \pm 0.35$ & $\mathbf{86.53 \pm 0.61}$ & $76.76 \pm 0.84$ \\
GCN (Hodge H-20-LAPE) & $69.61 \pm 0.45$ & $\mathbf{79.61 \pm 0.85}$ & $75.62 \pm 0.31$ & $86.06 \pm 0.52$ & $\mathbf{77.48 \pm 0.93}$ \\
GCN (Norm. H-20-LAPE) & $69.13 \pm 0.77$ & $78.13 \pm 0.79$ & $76.18 \pm 0.29$ & $85.78 \pm 0.55$ & $76.92 \pm 0.88$ \\
\hline
UniGCN (No Encoding) & $63.36 \pm 1.76$ & $75.72 \pm 1.16$ & $71.10 \pm 1.37$ & $75.32 \pm 1.09$ & $71.05 \pm 1.40$ \\  
UniGCN (HCP-FRC) & $61.20 \pm 1.83$ & $74.64 \pm 1.45$ & $68.98 \pm 1.59$ & $67.37 \pm 1.73$ & $71.02 \pm 1.43$ \\ 
UniGCN (HCP-ORC) & $61.81 \pm 1.70$ & $75.03 \pm 1.33$ & $70.42 \pm 1.17$ & $71.64 \pm 1.52$ & $70.69 \pm 1.62$ \\
UniGCN (EE H-19-RWPEE) & $63.29 \pm 1.52$ & $75.34 \pm 1.28$ & $71.13 \pm 1.24$ & $74.61 \pm 1.18$ & $71.21 \pm 1.53$ \\  
UniGCN (EN H-19-RWPEE) & $63.09 \pm 1.62$ & $75.30 \pm 1.37$ & $71.21 \pm 1.34$ & $74.61 \pm 1.09$ & $71.26 \pm 1.47$  \\  
UniGCN (Hodge H-20-LAPE) & $63.46 \pm 1.58$ & $75.64 \pm 1.37$ & $71.31 \pm 1.19$ & $75.37 \pm 1.01$ & $70.71 \pm 1.61$ \\  
UniGCN (Norm. H-20-LAPE) & $63.41 \pm 1.61$ & $75.55 \pm 1.48$ & $71.20 \pm 1.24$ & $75.30 \pm 1.01$ & $71.10 \pm 1.33$ \\  
\hline
\end{tabular}
\caption{GCN and UniGCN performance on hypergraph datasets with different hypergraph encodings. We report mean accuracy and standard deviation over 50 runs.}
\label{tab:node}
\end{table*}


\subsection{Experimental setup}
%\lf{all content is there, just needs some polishing}
Throughout all of our experiments, we treat the computation of encodings as a preprocessing step, which is first applied to all graphs in the data sets considered. We then train a GNN on a part of the preprocessed graphs and evaluate its performance on a withheld set of test graphs (nodes in the case of node classification). Settings and optimization hyperparameters are held constant across tasks and baseline models for all encodings, so that hyperparameter tuning can be ruled out as a source of performance gain. We obtain the settings for the individual encoding types via hyperparameter tuning. For all preprocessing methods and hyperparameter choices, we record the test set performance of the settings with the best validation performance. As there is a certain stochasticity involved, especially when training neural networks, we accumulate experimental results across 50 random trials. We report the mean test accuracy, along with the 95$\%$ confidence interval for the node classification datasets in Tab.~\ref{tab:node} and for the datasets in Tab.~\ref{tab:gcn} and \ref{tab:gps}. For Peptides-func, we report average precision and for Peptides-struct the mean absolute error (MAE). Details on all datasets can be found in Apx.~\ref{appendix-datasets}.


\subsection{Comparison of hypergraph- and graph-level architectures}
We begin by comparing the utility of our encodings for message-passing architectures that operate at the graph or at the hypergraph level. Hypergraph neural networks are predominantly used for node classification in hypergraphs. In fact, we are not aware of hypergraph classification datasets analogous to the graph datasets used in the previous subsection. As such, we choose five common hypergraph node classification datasets: Cora-CA, Cora-CC, Citeseer, DBLP, and Pubmed. We use clique expansion to convert these hypergraphs into graphs (empirically, we found this to be the best performing expansion) and train GCN on them with either no encoding or one of our hypergraph encodings. As a hypergraph-level message-passing architecture, we use UniGCN \citep{huang2021unignn}. Additional experiments with UniGIN and UniGAT are presented in Apx.~\ref{appendix:ablations}, along with a detailed explanation of the clique expansions we use.\\

\noindent \textbf{Graph-level message-passing benefits from hypergraph-level encodings.} Our results are presented in Tab.~\ref{tab:node}. Somewhat surprisingly, we note that even on these datasets, which are originally hypergraphs, GCN with no encodings outperforms UniGCN. Perhaps even more surprising, UniGCN does not seem to benefit from any of the encodings provided. Apx.~ \ref{appendix:ablations} shows that the same holds true for UniGIN and UniGAT. GCN on the other hand clearly benefits from (most) hypergraph-level encodings, although admittedly less so than when used for graph classification. Previous work has reported similar differences in the utility of encodings for graph and node classification tasks. Overall, we take our results in this subsection and in Apx.~\ref{appendix:ablations} as evidence that our proposed hypergraph-level encodings present a strong alternative to established message-passing architectures at the hypergraph level.


\begin{table*}[t!]
\centering
\tiny
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model (Encodings)} & \textbf{Collab} ($\uparrow$) & \textbf{Imdb} ($\uparrow$) & \textbf{Reddit} ($\uparrow$) & \textbf{Enzymes} ($\uparrow$) & \textbf{Proteins } ($\uparrow$) & \textbf{Peptides-f} ($\uparrow$) & \textbf{Peptides-s ($\downarrow$)} \\
\hline
GCN (No Encoding)         & $61.94 \pm 1.27$ & $48.10 \pm 1.02$ & $67.87 \pm 1.38$ & $28.03 \pm 1.15$ & $71.48 \pm 0.90$ & $0.532 \pm 0.005$ & $0.266 \pm 0.002$\\ \hline
% GCN (LDP)        & $70.91 \pm 0.80$ & $67.60 \pm 1.95$  & $84.60 \pm 1.84$ & $65.22 \pm 1.82$ & $30.44 \pm 1.92$ & $71.08 \pm 1.24$ & $0.5374 \pm$ & $0.2621 \pm $\\
GCN (LCP-FRC)    & $68.36 \pm 1.13$ & $63.42 \pm 1.47$ & $79.53 \pm 1.62$ & $27.66 \pm 1.48$ & $70.89 \pm 1.16$ & $0.537 \pm 0.006$ & $0.261 \pm 0.003$ \\
GCN (LCP-ORC)    & $70.48 \pm 0.97$ & $67.93 \pm 1.55$ & $80.75 \pm 1.54$ & $\mathbf{33.17 \pm 1.43}$ & $\mathbf{74.22 \pm 1.77}$ & $\mathbf{0.561 \pm 0.005}$ & $\mathbf{0.252 \pm 0.004}$ \\
GCN (19-RWPE)       & $49.63 \pm 2.38$ & $50.41 \pm 1.26$ & $78.93 \pm 1.60$ & $30.66 \pm 1.78$ & $71.94 \pm 1.58$ & $0.538 \pm 0.007$ & $0.265 \pm 0.003$ \\
GCN (20-LAPE)       & $58.33 \pm 1.64$ & $48.82 \pm 1.31$ & $77.26 \pm 1.58$ & $28.52 \pm 1.16$ & $71.46 \pm 1.52$ & $0.534 \pm 0.006$ & $0.258 \pm 0.003$\\
\hline
% GCN (LDP)             & $70.13 \pm 0.82$ & $\mathbf{75.86 \pm 0.61}$ & $65.14 \pm 0.76$ & $62.77 \pm 1.24$ &  &  &  & $0.2824 \pm$ \\ 
GCN (HCP-FRC) & $\mathbf{72.03 \pm 0.51}$ & $64.64 \pm 0.88$ & $82.09 \pm 0.58$ & $30.87 \pm 1.38$ & $71.27 \pm 1.20$ & $0.559 \pm 0.004$ & $0.255 \pm 0.004$ \\ 
GCN (HCP-ORC) & $70.82 \pm 0.68$ & $66.16 \pm 0.75$ & $80.35 \pm 0.72$ & $32.83 \pm 1.36$ & $73.78 \pm 1.25$ & $0.559 \pm 0.004$ & $0.258 \pm 0.003$ \\
GCN (EE H-19-RWPE) & $69.63 \pm 0.71$ & $\mathbf{73.96 \pm 0.65}$ & $82.79 \pm 0.62$ & $31.74 \pm 1.30$ & $73.83 \pm 1.08$ & $0.546 \pm 0.006$ & $0.263 \pm 0.003$ \\ 
GCN (EN H-19-RWPE) & $68.85 \pm 0.91$ & $73.84 \pm 0.48$ & $\mathbf{83.30 \pm 0.79}$ & $30.93 \pm 1.27$ & $74.05 \pm 1.13$ & $0.549 \pm 0.005$ & $0.263 \pm 0.003$ \\ 
GCN (Hodge H-20-LAPE) & $69.61 \pm 0.45$ & $71.38 \pm 0.75$ & $79.46 \pm 0.82$ & $29.46 \pm 1.14$ & $72.89 \pm 1.31$ & $0.557 \pm 0.005$ & $0.254 \pm 0.003$ \\ 
GCN (Norm. H-20-LAPE) & $69.13 \pm 0.77$ & $71.05 \pm 0.82$ & $80.08 \pm 0.67$ & $29.60 \pm 1.21$ & $73.12 \pm 1.36$ & $0.557 \pm 0.006$ & $0.253 \pm 0.003$ \\ \hline
\end{tabular}
\caption{GCN performance with graph level encodings (top) and hypergraph level encodings (bottom). We report mean and standard deviation across 50 runs.}
\label{tab:gcn}
\end{table*}


\begin{table*}[ht!]
\centering
\tiny
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model (Encodings)} & \textbf{Collab} ($\uparrow$) & \textbf{Imdb} ($\uparrow$) & \textbf{Reddit} ($\uparrow$) & \textbf{Enzymes} ($\uparrow$) & \textbf{Proteins} ($\uparrow$) & \textbf{Peptides-f} ($\uparrow$) & \textbf{Peptides-s} ($\downarrow$) \\
\hline
GPS (No Encoding) & $74.17 \pm 1.33$ & $70.93 \pm 1.21$ & $80.94 \pm 1.42$ & $46.83 \pm 1.14$ & $74.10 \pm 0.98$ & $0.593 \pm 0.009$ & $0.262 \pm 0.003$ \\ \hline
% GPS (LDP) & $73.79 \pm 1.25$ & $69.60 \pm 1.40$ & & $82.07 \pm 1.76$ & $42.63 \pm 1.48$ & $74.03 \pm 1.16$ & & \\
GPS (LCP-FRC) & $74.22 \pm 1.27$ & $71.46 \pm 1.77$ & $80.53 \pm 1.55$ & $43.75 \pm 1.39$ & $73.38 \pm 1.07$ & $0.598 \pm 0.010$ & $0.257 \pm 0.003$ \\
GPS (LCP-ORC) & $74.52 \pm 1.18$ & $71.84 \pm 1.26$ & $82.83 \pm 1.47$ & $48.51 \pm 1.58$ & $74.88 \pm 1.20$ & $0.613 \pm 0.010$ & $0.252 \pm 0.003$ \\
GPS (19-RWPE) & $74.29 \pm 1.42$ & $66.40 \pm 1.53$ & $81.92 \pm 1.31$ & $51.09 \pm 1.64$ & $71.92 \pm 1.18$ & $0.594 \pm 0.011$ & $0.257 \pm 0.003$ \\
GPS (20-LAPE) & $74.74 \pm 1.23$ & $70.67 \pm 1.18$ & $82.05 \pm 1.29$ & $42.90 \pm 1.35$ & $71.46 \pm 1.25$ & $0.599 \pm 0.011$ & $0.253 \pm 0.003$ \\
\hline
% GPS (LDP) & $73.57 \pm 0.98$ & $69.60 \pm 1.16$ & & $84.39 \pm 1.60$ & & & & \\
GPS (HCP-FRC) & $73.37 \pm 1.59$ & $71.48 \pm 1.03$ & $81.68 \pm 1.16$ & $47.66 \pm 0.92$ & $74.50 \pm 1.13$ & $0.604 \pm 0.010$ & $0.254 \pm 0.003$ \\
GPS (HCP-ORC) & $74.18 \pm 1.22$ & $72.05 \pm 1.15$ & $83.07 \pm 1.24$ & $48.19 \pm 1.31$ & $74.52 \pm 1.20$ & $0.609 \pm 0.010$ & $0.254 \pm 0.004$ \\
GPS (EE H-19-RWPE) & $\mathbf{76.19 \pm 1.29}$ & $\mathbf{73.19 \pm 1.07}$ & $84.04 \pm 1.07$ & $\mathbf{51.83 \pm 1.07}$ & $\mathbf{75.08 \pm 1.14}$ & $0.615 \pm 0.009$ & $\mathbf{0.251 \pm 0.003}$ \\
GPS (EN H-19-RWPE) & $75.92 \pm 1.33$ & $73.08 \pm 1.24$ & $\mathbf{84.25 \pm 1.13}$ & $51.28 \pm 1.12$ & $74.82 \pm 1.11$ & $\mathbf{0.617 \pm 0.010}$ & $0.252 \pm 0.003$ \\
GPS (Hodge H-20-LAPE) & $76.10 \pm 1.16$ & $73.15 \pm 1.02$ & $83.97 \pm 1.21$ & $47.44 \pm 1.16$ & $73.95 \pm 1.08$ & $0.602 \pm 0.010$ & $0.252 \pm 0.003$ \\
GPS (Norm. H-20-LAPE) & $75.81 \pm 1.21$ & $72.94 \pm 1.18$ & $83.85 \pm 1.18$ & $47.78 \pm 0.98$ & $74.03 \pm 1.10$ & $0.604 \pm 0.010$ & $0.254 \pm 0.002$ \\
\hline
\end{tabular}
\caption{GPS performance with graph level encodings (top) and hypergraph level encodings (bottom). We report mean and standard deviation across 50 runs.}
\label{tab:gps}
\end{table*}


\subsection{Hypergraph-level encodings capture higher-order information effectively }

We now evaluate to what extent our hypergraph encodings can be used for datasets that are originally graph-structured. We lift these graphs to the hypergraph level (see. Apx.~\ref{appendix-hgtog} for details) and compare against encodings computed at the graph level. Tables \ref{tab:gcn} and \ref{tab:gps} report results for GCN and GPS; additional results with GIN can be found in Apx.~\ref{appendix:additional_results}. \\

\noindent \textbf{Performance gains with hypergraph-level encodings.} We note several things: 1) adding encodings is beneficial in nearly all scenarios, 2) encodings computed at the hypergraph level are always at least as beneficial as their cousins computed at the graph level (e.g. H-RWPE is at least as useful as RWPE), and 3) on social network datasets (Collab, Imdb, and Reddit), hypergraph encodings provide the largest performance boosts, often by a wide margin. This aligns with our intuition, as social networks can often naturally be thought of as hypergraphs.\\

\noindent \textbf{Positional vs structural encodings.} Our results with GPS confirm our observations with GCN. Hypergraph-level encodings significantly boost performance on almost all datasets (only Proteins is not statistically significant) and are generally more useful than their graph-level analogues. Further, while GCN usually performed best with local structural encodings such as the Local Curvature Profile, GraphGPS seems to benefit more from global positional encodings such as (Hypergraph-) Random Walk Positional Encodings. This aligns with previous findings in the literature using graph-level encodings \citep{fesser2023effective}.\\

\noindent \textbf{Utility beyond Weisfeiler-Lehman.} Our previous results on the BREC dataset indicate that much of the utility of our hypergrpah-level encodings can perhaps be attributed to improving the expressivity of GCN and GPS. To better quantify this, we run an additional suite of experiments on the Collab, Imdb, and Reddit datasets using the GIN. As noted previously, GIN is provably as powerful as the 1-WL test and therefore more expressive than GCN and GPS. Our results in Apx.~\ref{appendix:additional_results} show that GIN has indeed a higher baseline accuracy (without encodings) than GCN, and benefits significantly less from encodings than both GCN and GPS. Nevertheless, our hypergraph-level encodings significantly boost performance and again beat the gains obtained from graph-level encodings. We take this as evidence that providing information from domains other than the computational domain (graphs in our setting) provides benefits beyond increased expressivity.




