\section{Hypergraph-level Encodings}
\label{theory}

\subsection{Laplacian Eigenvectors}
The Graph Laplacian $\Delta=D-A$ is a classical graph characteristic that is often leveraged for the design of encodings. \emph{Laplacian Eigenvector PE (LAPE)} 
are defined as 
\begin{equation}
\label{LAPE}
    p_i^{\text{LapPE}} = \begin{pmatrix} U_{i1}, U_{i2}, \hdots, U_{ik}
\end{pmatrix}^T 
\in \mathbb{R}^k \; ,
\end{equation}
where $\Delta=U^T \Lambda U$ is a spectral decomposition; $k$ is a hyperparameter.
Note that the eigenvectors are only defined up to $\pm 1$; we follow the convention in~\citep{dwivedi2021graph} and apply random sign flips. 


In order to define a hypergraph-level extension of LAPE, we have to consider first the choice of Laplacian. 
We focus on the Hodge Laplacian here, but discuss other choices, specifically the normalized hypergraph Laplacian and random-walk Laplacian, in Apx.~\ref{Laplacians}. Our choice of the Hodge Laplacian is motivated by its desirable properties, including that it is symmetric.\\

\begin{defn}\label{def:hodge-lape} \textbf{(Hodge Laplacian).} 
Let $B_1$ denote an incidence matrix whose entries indicate relations between nodes and hyperedges. If a node $i$ is on the boundary of a hyperedge $j$, the relation is expressed as $i \prec j$.
\begin{equation} \label{incidence_matrix} (B_1)_{i,j} = \begin{cases}  1 \text{ if } i \prec j \\ 0 \text{ otherwise } \end{cases} \in \mathbb{R}^{V\times E} \; .
\end{equation}
\noindent The $0$- and $1$-Hodge Laplacian are given by 
$H_0 = B_1^TB_1$ and $H_1 = B_1B_1^T$.
\end{defn}

We define the \textit{Hodge-Laplacian Positional Encoding (H-$k$-LAPE)} in analogy to Eq.~\ref{LAPE} using the top $k$ eigenvectors of the Hodge 
Laplacian. 
We show below that the additional higher-order information captured by H-$k$-LAPE, but not by $k$-LAPE or standard message-passing, provably enhances the representational power of the architecture. 
The proofs in this and subsequent sections refer to graphs in the BREC dataset~\citep{wang2023towards}, more information on which is provided in the Apx.~\ref{apx:data-brec} and Apx.~\ref{appendix-pair-0}.\\

\begin{theorem}\label{thm:lape_exp} \textbf{(H-$k$-LAPE Expressivity).} For any $k$ MPGNNs with H-$k$-LAPE are strictly more expressive than the 1-WL test and hence MPGNNs without encodings. Furthermore, there exist graphs which can be distinguished using H-$k$-LAPE, but not using LAPE. 
\end{theorem}



\begin{proof}
    Pair $0$ of the "Basic" category in BREC - a subset of BREC (Apx.~\ref{apx:data-brec} and Apx.~\ref{appendix-detailed-encodings}) is a pair of non-isomorphic, 1-WL indistinguishable graphs \ref{fig:pair-0-lifting}. The pair is 1-LAPE-indistinguishable, but can be distinguished with H-1-LAPE (see Apx.~\ref{appendix-pair-0}).
\end{proof}

\begin{figure*}[h!]
  \centering  \includegraphics[width=\textwidth]{images/pair_0_basic_hypergraphs_main_text.png}
  \caption{A pair of graph from the BREC "Basic" category  (top left), the graphs' liftings (top right), the hyperedge sizes (bottom left) and node degrees (bottom right).}
  \label{fig:pair-0-main}
\end{figure*}

\begin{rmk}\label{thm:lape_comp} \textbf{(H-LAPE Complexity).} Computing a full spectral decomposition of $\Delta$ has complexity $O(|V|^3)$, where $|V|$ is the number of nodes in the input graph. However, by exploiting sparsity and the fact that we only require the top eigenvectors, Lanczos' algorithm can be used to compute H-$k$-LAPE in $O(\vert E \vert k)$.
\end{rmk}


\subsection{Random Walk Transition Probabilities}
Another widely used positional encoding, Random Walk PE (RWPE), is defined using the probability of a random walk revisiting node $i$ after $1, 2, \hdots, k$ steps, formally
\begin{equation}
\label{RWPE}
p_i^{k\text{-RWPE}} = \begin{pmatrix} RW_{ii}, RW_{ii}^2, \hdots, RW_{ii}^k
\end{pmatrix}^T 
\in \mathbb{R}^k \; ,
\end{equation}
where $k$ is a hyperparameter. Since the return probabilities depend on the graph's topology, they capture crucial structural information. Notably, $k$-RWPE
does not suffer from sign ambiguity like LAPE, instead providing a unique node representation whenever nodes have topologically distinct $k$-hop neighborhoods. 

We define an analogous notion of PEs at the hypergraph level. We consider the following notion:\\
\begin{defn}\label{def:hrw} \textbf{(Random Walks on Hypergraphs~\citep{coupette2022ollivier}).} 
We define Equal-Nodes Random Walks (EN) and
Equal-Edges Random Walks (EE), which induce the following two measures: 
\begin{align}\label{measures}
\mu_i^{\text{EN}}(j) & = \begin{cases} \frac{1}{|\mathcal{N}_i|} \text{ , if } j 
\in \mathcal{N}_i \\
0 \text{ otherwise} 
\end{cases}\\
\mu_i^{\text{EE}}(j) & = \begin{cases} \mathbb{P}^{\text{EE}}(i \rightarrow j)   \text{ ,} \text{ if } j 
\in \mathcal{N}_i \\
0 \text{ otherwise} 
\end{cases}
\end{align}
where $\mathcal{N}_i$ are the neighbors of $i$ and transition probabilities are given by
\begin{align*}
\mathbb{P}^{\text{EE}}(i \rightarrow j) & = \frac{1}{|\{e|i\in e, |e|\geq 2\}|} \sum_{\{e | \{i,j\} \subseteq e\}} \frac{1}{|e|-1}\; .
\end{align*}
\end{defn}

For an EN random walk, considering a move from node $i$, we pick one of the neighbors of node $i$ at random. For the EE scheme, we first pick a hyperedge that $i$ belongs to at random and then pick one of the nodes in the hyperedge at random.


We can now define \textit{Hypergraph Random Walk Positional Encodings} (H-$k$-RWPE) in analogy to $k$-RWPE. Again, we can show that H-$k$-RWPE provably enhances the representational power of an MPGNN, beyond those of $k$-RWPE.\\

\begin{theorem}\label{thm:rwpe_exp} \textbf{(H-$k$-RWPE Expressivity).} For $k \geq 2$, MPGNNs with H-$k$-RWPE are strictly more expressive than the 1-WL test and hence than MPGNNs without encodings. There exist graphs which can be distinguished using H-$k$-RWPE, but not using graph-level $k$-RWPE.
\end{theorem}

\begin{proof}
Pair $0$ of the ''Basic'' category in BREC 
is a pair of non-isomorphic graphs that cannot be distinguished with 1-WL. The pair cannot be distinguished with 2-RWPE computed at the graph level, but can be distinguished using the H-2-RWPE encodings computed at the hypergraph level (see \ref{appendix-pair-0}).
\end{proof}

\begin{rmk} Note that $k$-RWPE is less expressive that (k+1)-RWPE and and H-$k$-RWPE is less expressive than ($k+1$)-H-RWPE.
\end{rmk}

\begin{rmk}\label{thm:lape_comp} \textbf{(H-$k$-RWPE Complexity).} Computing H-$k$-RWPE scales as $O(|V| d^k_\text{max})$, where $d_\text{max}$ is the highest node degree in the input hypergraph. The degree $d_i$ of a vertex $i$ of an undirected hypergraph $H = (V,E)$ is the number of hyperedges that contain $i$ \citep{klamt2009hypergraphs}.
\end{rmk}



\subsection{Local Curvature Profiles}
Recently it was shown that discrete Ricci curvature yields an effective structural encoding at the graph level~\citep{fesser2023effective}.
Ricci curvature is a classical tool from Differential Geometry that allows for characterizing local and global properties of geodesic spaces. Discrete analogues of Ricci curvature~\citep{forman,ollivier2007ricci} have been studied extensively on graphs and, more recently, on hypergraphs~\citep{leal2021forman,coupette2022ollivier,saucan2019forman}. Here, we focus on defining hypergraph-level curvatures, we defer all details on graph-level notions to Apx.~\ref{apx:curvature}. 

We restrict ourselves to two notions of discrete Ricci curvature, originally introduced by Forman~\citep{forman} and Ollivier~\citep{ollivier2007ricci}, which have previously been considered for graph-level encodings. We begin with Forman's curvature:\\
%
\begin{defn}\label{def:fr} \textbf{(Forman's Ricci Curvature on hypergraphs (H-FRC)~\citep{leal2021forman}).} The H-FRC of a hyperedge $e$ is defined as  $F(e)=\sum_{k \in e} (2-d_k)$.
\end{defn}
%
Ollivier's Ricci curvature derives from a fundamental relationship between Ricci curvature and the behavior of random walks on geodesic spaces. To define an analogous notion on hypergraphs, we leverage again the previously introduced notions of random walks~\citep{coupette2022ollivier}.\\
%


\begin{defn}\label{def:orc} \textbf{(Ollivier's Ricci Curvature on hypergraphs (H-ORC)~\citep{coupette2022ollivier}).}
\noindent The H-ORC of a subset $s$ of nodes on a hypergraph is defined as:
\begin{equation} \kappa(s) = 1 - \frac{AGG(s)}{d(s)}\end{equation}
where $d(s)=\{\max d(i,j) | \{i,j\} \subseteq s\}$. We define for a
hyperedge $e$
\begin{equation} \kappa(e) = 1 - AGG(e) \; .\end{equation}
Here, $AGG(\cdot)$ denotes an aggregation function.
\end{defn}

Different types of aggregations could be considered for the choice of $AGG(\cdot)$. Here, we choose $AGG(\cdot)$ to be the average of the distances  between all  pairs $\{i,j\}$ in a hyperedge $e$, i.e.,
\begin{align} 
AGG (e) 
& =\frac{1}{{|e|\choose2}} \sum_{\{i,j\} \subseteq e } W_1(\mu_i, \mu_j) \; .
\end{align}


We can now define the actual encoding, extending Local Curvature Profiles (LCP)~\citep{fesser2023effective}, computed at the graph level, to hypergraphs.

\begin{defn}\label{def:hcp} \textbf{(Hypergraph Curvature Profile (HCP)).} For  \( v \in V \) let \( \text{CMS}(v) \) denote a \emph{curvature multi-set} consisting of the curvatures of all hyperedges containing $v$,
$\text{CMS}(v) = \{\kappa(e) : v \in e, e \in E\}$,
where $\kappa$ may be chosen to denote either FRC or ORC. We define HCP
as the following five summary statistics of $CMS(v)$:
\begin{equation}\label{HCP}
    \text{HCP}(v) = \left[\min(\text{CMS}(v)), \max(\text{CMS}(v)), {\rm mean}(\text{CMS}(v)), 
    {\rm median}(\text{CMS}(v)), {\rm std}(\text{CMS}(v)) \right] \; .
\end{equation}
\end{defn}
%
As for the other proposed encodings, we investigate the expressivity of HCP.
Note that ORC computed at the graph level is by itself very expressive, leading to LCP provably enhancing the expressivity of MPGNNs. In fact, there exist variants of ORC which can distinguish graphs that are not 3-WL distinguishable~\citep{southern2023expressive}. However, the same is not true for graph-level FRC. This merits a closer analysis of HCP where $\kappa$ is chosen to be the H-FRC.\\
%
\begin{theorem}\label{thm:hcp_exp} \textbf{(HCP Expressivity).} MPGNNs with HCP ($\kappa$ denoting H-FRC) are strictly more expressive than the 1-WL test and hence than MPGNNs without encodings. In contrast, leveraging LCP with standard FRC at the graph level does not enhance expressivity.
\end{theorem}

\begin{proof}
Consider again the 4 by 4 Rook and the Shrikhande graphs, which cannot be distinguished by the $k$-WL test for $k \leq 3$.
All nodes in both graphs have identical LCP-FRC, namely $[-8, -8, -8, -8, 0]$. This is because all nodes have degree $6$, consequently their FRC is $-8$. However, when computing HCP-FRC on the lifted hypergraphs the curvatures differ: In the Rook graph, all nodes have HCP-FRC $[0, 0, 0, 0, 0]$, whereas in the Shrikhande graph all nodes have HCP-FRC $[-12, -12, -12, -12,  0]$ (see \ref{appendix-rook-shrikhande}). Furthermore, it is possible to find non-isomorphic graphs with the same LCP, but different HCP (even up to scaling): Pair 0 of the “Basic” category in BREC is an example where both graphs have the same LCP, but different HCP (even up to scaling) (see Apx.~\ref{appendix-pair-0} for additional details).
\end{proof}

\begin{rmk}\label{thm:lape_comp} \textbf{(HCP Complexity).} Computing the H-FRC and hence the $\text{HCP-FRC}$ scales as $O(|E|e_\text{max})$, where $e_\text{max}$ denotes the size of the largest hyperedge. On the other hand, computing H-ORC incurs significant computational cost: The computation of the $W_1$-distance, which scales as $(|E|e_\text{max}^3)$, introduces a significant bottleneck. Hence, HCP-FRC has significant scaling advantages over HCP-ORC.
\end{rmk} 




\subsection{Local Degree Profile}
Lastly, we define a hypergraph-level notion of \emph{Local Degree Profiles (LDP)}~\citep{cai2018simple}, which captures structural information encoded in the node degree distribution over a node's 1-hop neighborhood. We consider the multi-set of node degrees in the 1-hop neighborhood of a node $v$, i.e.,
${\rm DN}(v)=\{d_u|u\in \mathcal{N}_v\}$ and define
\begin{align*} \label{ldp}
\begin{split}
\text{LDP}(v) = [& d_v, \min(
{\rm DN}(v), \max({\rm DN}(v)),  {\rm mean}({\rm DN}(v)), {\rm median}({\rm DN}(v)), {\rm std} ({\rm DN}(v))] \; .
\end{split} 
\end{align*}
An analogous notion on the hypergraph level (H-LDP) can be defined by a simple extension. Again, H-LDP exhibits improved expressivity:\\

\begin{theorem}\label{thm:ldp_exp} \textbf{(H-LDP Expressivity).} MPGNNs with  H-LDP are strictly more expressive than the 1-WL test and hence than MPGNNs without encodings. There exist graphs which can be distinguished using H-LDP, but not using LDP.
\end{theorem}
%
\begin{proof}
    The 4 by 4 Rook graph and the Shrikhande graph cannot be distinguished by LDP, as all nodes the same degree, resulting in LDPs  $[6,6,6,6,6,0]$. However, they can be distinguished using H-LDP: The nodes in the Rook graph have H-LDP $[2, 2, 2, 2, 2, 0]$, the nodes in the Shrikhande graph $[6, 6, 6, 6, 6, 0]$ (see Apx.~\ref{appendix-rook-shrikhande}). Furthermore, it is possible to find non-isomorphic graphs with the same LDP, but different H-LDP even up to scaling: Pair 0 of the “Basic” category in BREC is an example, where both graphs have identical LDPs, but different H-LDPs, even up to scaling. For more details, see Fig.~\ref{fig:pair-0-main} and Apx.~\ref{appendix-pair-0}.
\end{proof}


\begin{rmk}
    We observe that in the examples demonstrating the enhanced representational power of HCP and H-LDP, the respective profiles are scalar multiples of each other. It is common in hypergraph architectures to normalize node attributes during preprocessing, which would obscure the structural differences captured by the two encodings. However, we emphasize that no such preprocessing is applied in our experiments.
\end{rmk}


