% \begin{abstract}
% % Large Language Models(LLMs) achieve great success in most tasks of Natural Language Processing(NLP) while the large-scale computation cost hinders their widespread application, especially in real-time scenarios. Structured Pruning is a method that compresses models and provides real speedups regardless of the hardware environment. Unlike previous works that are uniform-pruned and sparsity-oriented, we propose a novel non-uniform and speed-aware structured pruning method called \sysname. We first connect the evolutionary search to the vanilla structured pruning method and generate non-uniform sparse models. \sysname is capable of speedup guarantee by generating speed-aware offspring. Moreover, we propose group-wise and in-group search to further reduce the search space. Extensive experiments on Llama-2-7B and Llama-3.1-8B show that our method achieves the SoTA performance compared with previous structured pruning methods. In particular, \sysname outperforms ZipLM and ShearedLlama models after pruning. Moreover, our method achieves better performance than the ShearedLlama model after pretraining with $5\times$ less training data of shearedLlama. We will release our code for reproduction.
% Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, it is known that different components of a model exhibit varying sensitivities towards pruning. To address this, we propose \sysname, a novel non-uniform, training-aware structured pruning method. 
% Our approach integrates evolutionary search with conventional second-order structured pruning to generate non-uniform sparse models. 
%  % \sysname guarantees speedup by maintaining a fixed, hardware-specific acceleration across the evolutionary process, and producing speed-optimized offspring models. 
% During the searching process, with multi-step mutation, we use small-scale tokens to finetune each offspring in every step. After selection, we obtain the optimal non-uniform sparse model for continual training.
% Extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct demonstrate that our method achieves state-of-the-art performance compared to previous structured pruning methods. For instance, our approach achieves better results than ShearedLlama with $5\times$ less training data during post-compression training. We will release our code to facilitate reproducibility. 
% \end{abstract}

\begin{abstract}
Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for \emph{non-uniform} model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \sysname, a method for \emph{training-aware} structured pruning. \sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage.
We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \sysname surpasses ShearedLlama while requiring $5\times$ less training data during post-compression training. Code and all weights are released at: \href{https://github.com/IST-DASLab/DarwinLM}{https://github.com/IST-DASLab/DarwinLM}.
% Moreover, \sysname outperforms OLMO and OLMO-0424 with $250\times$ less training tokens.
\end{abstract}