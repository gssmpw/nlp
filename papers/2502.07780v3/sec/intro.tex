\vspace{-2em}
\section{Introduction}
% \OS{didn't make changes to abstract/ introduction/ related work apart from obvious mistakes}
The high accuracy of Transformer-based models on a wide range of tasks comes with massive computational requirements, which hinders deployability. A long line of research has been conducted to reduce the computational cost of large language models via methods such as quantization \citep{frantar2022gptq, dettmers2023spqr}, pruning \citep{xia2023sheared, frantar2023sparsegpt} and distillation \cite{hsieh2023distilling}. In this work, we explore structured pruning on large language models (LLMs) by removing whole rows or columns in a weight matrix, resulting in regular but smaller matrices. Unlike unstructured pruning \citep{frantar2023sparsegpt}, the model produced by structured pruning can be accelerated on mainstream hardware without any specific design for computation. 

While conventional pruning methods generally prune each layer or block uniformly, \emph{non-uniform compression methods}, e.g. \citep{yin2023outlier, sieberling2024evopress}, showed that different LLM layers can have significantly different ``sensitivities'' to pruning, which can be leveraged to obtain higher compression while preserving accuracy. 
% Moreover, previous LLM structured pruning techniques~\cite{xia2023sheared} target a fixed sparsity instead of actual speedup. Even if a pruned model with fixed sparsity achieves the desired speedup, it may still not necessarily be the best-performing model at that specific speedup.
% If the pruned model does not meet the speedup requirement in the inference environment, the pruning process has to be repeated from scratch\OS{You can make a stronger argument here: Even if the model you get via fixed sparsity has exactly the target speedup, it does not have to be close to the best model that has this target speedup (it's just a different objective).}.
To address this, smaller-scale pruning methods such as ZipLM \citep{kurtic2024ziplm} propose to utilize a dynamic-programming-based search~\citep{frantar2022spdy} to obtain a sparse model with runtime guarantees. However, there are several challenges for methods such as ZipLM on large-scale models such as Llama-2-7B \citep{touvron2023llama}: for instance, ZipLM only considers the local layer-wise error during the search, which is not consistent with performance on in-context learning (ICL) or downstream tasks. 

\noindent\textbf{Overview.} In this paper, we propose a new structured pruning method based on evolutionary search called \sysname{}, which addresses these issues. 
Specifically, the search starts from a ``parent'' model, generated by pruning the original model using second-order information. In each search step, it then generates ``offspring'' candidate models by copying the parent and ``shifting'' sparsity from one layer to another, by what we call a \emph{level switch mutation}. Moreover, to obtain the optimal sparse model given additional training, we use a small-scale dataset to finetune each generated offspring, and select the best offspring after finetuning. Empirically, our method scales to LLMs, and achieves state-of-the-art performance on both one-shot and post-training settings compared with previous pruning methods. For instance, we obtain smaller and better sparse models on both Llama-2, Llama-3.1 and Qwen-2.5 compared with the state-of-the-art ZipLM pruning method; furthermore, on Llama-2-7B, our sparse model with 2.7B parameters outperforms the ShearedLlama-2.7B structured pruned model, while using  \emph{$5\times$ less data} for post-compression training (50B Tokens vs. 10B Tokens). Moreover, a \sysname with 4.6B parameters pruned from Llama-3.1 surpasses an OLMO model with 7B parameters pre-trained on $250\times$ more data (10B Tokens vs 250T tokens). We also compare our method with a line of more coarse-grained structured pruning methods including ShortGPT \citep{men2024shortgpt}, Shortened-Llama \citep{kim2024shortened}, and EvoPress~\citep{sieberling2024evopress} in a one-shot setting, showing that \sysname{} provides better performance across compression rates. 

We summarize our contributions as follows:
\vspace{-0.1in}
\begin{itemize}
\addtolength{\itemsep}{-0.05in}
    \item We propose a new structured pruning method that unifies second-order pruning and evolutionary search, with the goal of producing the ``optimal'' non-uniformly ``thinned'' model given a fixed size or runtime constraint. Importantly, the resulting models can be run efficiently on any hardware, and can meet hardware-specific performance constraints. 
    % a novel structured pruning method, which unifies vanilla sparsity-oriented structured pruning method and evolutionary search. By introducing speed-aware offspring generation, our method outputs the globally optimal non-uniform sparse model. 
    % \item To reduce the large search space induced by large language models, we utilize new group-wise and in-group mutations to accelerate the evolutionary search process.
    % \
    % \item We introduce regularization terms to the fitness environment that penalize excessive sparsity deviations and the pruning of early layers. Empirical results show that this approach enhances performance on downstream tasks.
    \item At the technical level, we introduce a pruning technique that leverages second-order information with a training-aware offspring selection technique, to identify the non-uniform sparse model that meets the performance requirement and also has the highest potential for accuracy gains during post-compression training. Empirical results show that this approach enhances performance both in terms of loss, but also in terms of performance on downstream tasks.
    % \OS{We introduce regularization terms for the fitness environment that penalize extreme sparsity deviations and the pruning of early layers. Empirical results demonstrate that this approach improves downstream task performance.}
    \item We validate the effectiveness of \sysname through extensive experiments. For instance, our method compresses Llama-2-7B to 2.7B parameters, Llama-3.1-8B to 4.6B parameters, and Qwen-2.5-14B-Instruct to 8.4B with much lower performance degradation compared to prior state-of-the-art methods. For instance, our approach outperforms the state-of-the-art ShearedLlama method for structured pruning, while using $5\times$ fewer data tokens for post-training. Importantly, our method can produce accurate, tailored compressed models: for example, we obtain a 4.6B-parameter variant of Llama-3.1-8B that significantly outperforms the 7B-parameter OLMO model trained on $250\times$ more data in terms of accuracy on standard zero-shot tasks.
    % provide comprehensive experiments to demonstrate the effectiveness of \sysname. We compress Llama-2-7B to 2.6B with 2.5$\times$ speedup and Llama-3.1-8B to 4.4B with 2.0$\times$ acceleration with minimal performance drop compared with previous methods. Moreover, our method achieves better performance than ShearedLlama with $5\times$ less data tokens for post-training.
\end{itemize}