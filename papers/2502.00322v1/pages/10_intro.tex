\section{Introduction}
\input{figures/intro}
Query-focused summaries (QFS) give an overview of documents to answer a query~\cite{rosner2008multisum, el2021automatic}.
By combining each document's content useful for answering the query, or their \textbf{perspectives}~\cite{lin2006side}, these summaries can aid decision-making~\cite{hsu2021decision}.
For example, doctors pick treatments based on research paper perspectives~\cite{goff2008patients} and legislators vote based on perspectives in policy reports~\cite{jones1994reconceiving}. 
Past QFS work assumes documents have aligned perspectives~\cite{roy2023review}, but some queries, like ``\emph{Is law school worth it?}'', are debatable, containing opposing perspectives~\cite{wan2024evidence}.
In such cases, it is key to \textit{balance} perspectives from \textit{diverse} sources so users consider all sides before deciding~\cite{dale2015heuristics}.

To address this gap, we propose \textbf{\textit{debatable} QFS (DQFS}).
As input, DQFS uses documents and a debatable query, defined as a yes/no query where documents have opposing, equally-valid\footnote{This is meant to avoid input questions like ``Is the earth flat?'' where ``yes'' and ``no'' are not equally-valid (\cref{subsection:ethics}).} ``yes'' and ``no'' perspectives (Fig~\ref{fig:intro}).
Such queries are broad (\textit{Is law school worth it?}), and decomposing broad concepts into more specific topics (\textit{cost}, \textit{job market}) improves comprehension~\cite{johnson1983mental}.
Thus, DQFS creates a multi-aspect summary, with each paragraph covering one of an input number of topics ($2$ in Fig~\ref{fig:intro}).
The full summary and each paragraph must be \textit{comprehensive} and \textit{balanced}~(\cref{section:task}).
Comprehensive text has perspectives from all documents, while balanced text is not skewed towards the yes or no perspectives; our goals aid informed, unbiased decision-making~\cite{ziems2024measuring}.


While LLMs are deft summarizers~\cite{zhang2024benchmarking}, they cannot directly solve DQFS, as they fail to use diverse sources~\cite{huang-etal-2024-embrace}.
In Figure~\ref{fig:intro}, GPT-4 mainly gives perspectives favoring EU expansion (\textcolor{blue}{\textbf{blue}}), yielding a biased output.
Also, when asked for citations~\cite{huang-chang-2024-citation}, GPT-4 only cites 3/6 (\textcolor{yellowcite}{\textbf{yellow}}), missing half the documents' perspectives.
We intuit this arises since GPT-4 uses one inference step, with all documents in a single prompt.
This can omit document perspectives in certain positions of the prompt~\cite{liu2024lost} or that oppose parametric memory~\cite{jin2024tug}, reducing output coverage and balance.

Multi-LLM summarizers~\cite{chang2024booookscore, adams2023sparse}, which use LLMs to summarize documents individually into intermediate outputs before merging them with another LLM call, are better choices, as they represent documents more equally. 
However, they have two key issues.
\textbf{First}, they use the same topic or query as input to summarize each document, which is subpar if we wish to use retrieval in summarization to reduce LLM costs.
Queries unaligned to a document's unique content and expertise will fail to retrieve all of its most relevant contexts~\cite{sachan2022improving}; this reduces the total number of perspectives in the intermediate output, resulting in lower coverage.
\textbf{Second}, their intermediate outputs are unstructured, free-form texts, which are hard for the LLM to combine into a final output.
Free-form text needs extra reasoning to extract, classify, and compare the texts' perspectives~\cite{barrow2021syntopical}, steps that distract from the final goal of generating a balanced summary.

% A \textit{structured} intermediate output that clearly organizes documents and their perspectives on topics would greatly simplify the final step of synthesizing a balanced, comprehensive summary~\cite{shao2024assisting}.

To solve our issues, we build \textbf{\model} (Fig~\ref{fig:model}), a multi-LLM system using a \textbf{M}ixture \textbf{o}f \textbf{D}ocument \textbf{S}peakers.
Inspired by panel discussions~\cite{doumont2014english}, \model has a \textit{Speaker} LLM for each document that responds to queries using its document, and a \textit{Moderator} LLM that decides when and how speakers respond.
Specifically, \model: 1) plans an agenda of topics for the outline (\cref{subsection:agenda}); 2) picks a subset of speakers with relevant perspectives for each topic and tailors them a query (\cref{subsection:moderator}); and 3) asks each speaker to obtain its document's context relevant to the tailored query and give the context's ``yes'' and ``no'' perspectives for the topic. 
%All steps are efficiently done via~retrieval.

When a speaker supplies its document's perspectives, the topic, document number, tailored query, and perspectives update an outline, tracking the LLM discourse.
After the discussion, the outline is summarized for a DQFS output.
In all, \model frames DQFS as a discussion of document speakers to represent sources equally, tailors queries for speakers to optimize the retrieval of contexts used to find perspectives, and builds a structured outline of document perspectives to simplify the synthesis of a final output---a novel combination that leads to comprehensive and balanced summaries~(\cref{subsection:ablation}).

We compare \model to eight strong baselines~on ConflictingQA~\cite{wan2024evidence} and \textbf{DebateQFS} (\cref{subsection:datasets}), a new dataset for DQFS drawn from the debate community on Debatepedia~\cite{gottopati2013learning}.
To assess summaries, we have models give citations in their outputs (Fig~\ref{fig:intro}), showing the documents the model intends to use~\cite{huang-chang-2024-citation}.
Many works use citations for factuality~\cite{li2024citation}, but
we repurpose them for coverage and balance---measuring the proportion of documents cited and distribution of ground-truth yes/no perspective stances of cited documents (\cref{subsection:metrics}).


\model has the best document coverage and balance in full summaries and topic paragraphs (\cref{subsection:citation_comp}), surpassing SOTA by 38-58\% in paragraphs.
The Prometheus LLM~\cite{kim2024prometheus} ranks \model as one of the best models in summarization quality 28/30 times, the most of any model (\cref{subsection:summary_comp}).
Users also find \model's outputs to be the most balanced, and preserve readability despite using perspectives from more documents (\cref{subsection:human_eval}).
Lastly, analyses show the utility of tailoring queries and building outlines, which improve \model (\cref{subsection:ablation}) and offer rich, structured tools for users (\cref{subsection:qg}). Our contributions are:

\noindent \textbf{1)} We propose \textbf{debatable query-focused summarization}, a new task to help users navigate yes/no queries in documents with opposing perspectives. \\
\noindent \textbf{2)} We design \model, a multi-LLM DQFS system that treats documents as \textbf{individual} \textbf{LLM speakers}, uses a moderator to \textbf{tailor queries} to apt speakers, and tracks speaker perspectives in an \textbf{outline}. \\
\noindent \textbf{3)} We release \textbf{DebateQFS} for DQFS and \textbf{citation metrics} to capture summary coverage and~balance. \\
\noindent \textbf{4)} Experiments show \model \textbf{beats baselines by 38-58\%} in topic paragraph coverage and balance, while annotators find \model's summaries \textbf{maintain readability} and \textbf{better balance perspectives}.