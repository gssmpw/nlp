\input{figures/model}
\section{Related Work}

\paragraph{Diverse Perspectives in Summarization:}

LLMs have shown to struggle with diverse input sources in news~\cite{huang-etal-2024-embrace}, review~\cite{zeng2023scientific}, and dialogue~\cite{zhang-etal-2024-fair} summarization.
While these tasks lack user guidance, DQFS is the first task that summarizes diverse texts while guided by a user's query.
Also, DQFS gives \textit{multi-aspect} summaries that are broken down into more specific paragraphs; this granularity of perspective diversity has not been studied in past work.

% Opinion summarization (OS) can be framed as yes/no queries (``Is product X good?'') broken into topics (price, size).  
% However, OS uses \textit{subjective reviews} and aims to capture reviewer consensus~\cite{el2021automatic}, a different goal from DQFS.
% DQFS uses documents with \textit{objective facts} and assumes such facts are equally-valid, so we argue DQFS has more focus on \textbf{balancing} perspectives. 

Most of these works expose LLM issues without giving solutions other than prompt tweaks~\cite{huang-etal-2024-embrace, zhang-etal-2024-fair}.
Instead, we design \model, a multi-LLM system to better handle diversity (\cref{subsection:citation_comp}), and also release a new dataset (\cref{subsection:datasets}) and citation metrics (\cref{subsection:metrics}) to help build even better summarization systems for diverse sources.

\paragraph{Argument Generation:}

DQFS is a form of argument generation~\cite{zukerman2000using}, producing text to argue for topics and claims~\cite{schiller2020aspect}.
Such tasks include debate~\cite{li2024can, hu2023americano, hu2024unlocking}, key point summarization~\cite{bar2020arguments, li2024exploring}, and argument essay writing~\cite{heinisch2022strategies, bao-etal-2022-aeg}.
These tasks either rely on LLM parametric memory~\cite{li2024can} or passages in evidence corpora~\cite{hua2019argument}. 
Conversely, in DQFS, models give arguments by summarizing and balancing perspectives in \textit{all} documents, rather than finding a \textit{subset} of evidence in large~corpora.

Further, existing datasets like OpenDebateEvidence~\cite{roush2024opendebateevidence} or DebateSum~\cite{roush-balaji-2020-debatesum} have specific claims (\textit{Colonialism made a hierarchy for exclusion}), which are unlike the broad queries in DQFS (\textit{Was colonialism helpful?}).
Thus, we release DebateQFS (\cref{subsection:datasets}), a dataset of broad debate queries grounded in documents.

\paragraph{Multi-LLM Summaries:}

Multi-LLM systems chain LLMs for tasks~\cite{guo2024large}.
\model is a multi-LLM system similar to single-turn debate~\cite{parrish2022single}, with a Moderator LLM routing to Speaker LLMs to supply document perspectives, storing them in memory (outline).
LLM discussions have been used for evaluation~\cite{verga2024replacing}, math~\cite{sun2023query}, and creativity~\cite{lu2024llm}, and we adopt them for DQFS.

The multi-LLM \model system has speakers respond individually to fairly treat documents.
Hierarchical Merging and Incremental Updating similarly summarize documents one at a time~\cite{chang2024booookscore}, but their intermediate outputs are free-form text.
\model instead uses a rich outline of document perspectives, better guiding the final summary~\cite{shao2024assisting}.
These models also summarize documents without catering to their expertise, hampering retriever efficacy; we solve this by tailoring custom queries for speakers (\cref{subsection:moderator}).