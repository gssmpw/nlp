\section{Limitations}

One limitation of \model is cost, as it uses multiple LLM calls. To reduce costs, we use top-3 retrieval at each step and a moderator to avoid inference on irrelevant documents (\cref{subsection:moderator}).
Appendix~\ref{subsection:efficiency} has a cost analysis, which shows \model is cheaper than Incremental-\emph{Topic} and is comparable to Hierarchical Merging for fewer topics.
Most of the cost from \model stems from outline creation, rather than outline summarization.
Our outline is a rich resource for users (\cref{subsection:qg}) and can also be useful for other tasks like key point analysis~\cite{bar2020arguments, kumar2023apcs}, pro/con summarization~\cite{hu2009classification}, and document contradiction detection~\cite{deusser2023contradiction}, which we believe justifies its high creation expense.

Further, all baseline implementations are based on GPT-4, as smaller LLMs like LLaMA-2 and GPT-3.5 struggled with following the instructions given in our 0-shot prompts (Appendix~\ref{appendix:implementation}), particularly in generating structured JSON outputs~\cite{xia2024fofo}. To overcome this, researchers could generate synthetic training data to improve format-following in smaller models~\cite{long2024llms}.
We also show some preliminary results with a version of \model using GPT-4 mini in Appendix~\ref{appendix:results_mini}, which can still compete with GPT-4 baselines. 

LLMs are also sensitive to prompt formats~\cite{sclar2023quantifying}, so our results may vary with prompt changes.
To mitigate this issue, we follow best practices in prompt engineering~\cite{schulhoff2024prompt}, ensuring consistent instructions across models, including input/output definitions, output format (JSON), and output requirements.
This ensures \model's gains in coverage and balance (\cref{subsection:citation_comp}) are due to its overall design, rather than advantages in prompt engineering.
We also plan to release all of our prompts for reproducibility (Appendix~\ref{appendix:implementation}).

Finally, while human evaluation across many aspects of DQFS quality would be valuable, we are limited by time and resources. To make the most of our human evaluation, we focus on readability and balance. Since MoDS objectively cites more documents and is thus more comprehensive, we ensure that this does not reduce readability. Further, since DQFS aims to support unbiased decision-making, we assess whether human judgments of summary balance align with our offline citation metrics. We acknowledge that further human evaluation, including how DQFS outputs impact decision-makers, would be an exciting direction for future research.

% Finally, LLMs are sensitive to prompt formats~\cite{sclar2023quantifying}, so altering our prompts could yield varied results.
% When designing models, we follow best prompt engineering practices~\cite{schulhoff2024prompt}, such as specifying clear instructions and testing changes on a small, held-out set.
% Further, we ensure that all models have consistent instructions; all models are given the same definitions for the inputs/outputs (queries, documents, topics, summaries), are asked to generate in the same output format (JSON format), and are tasked with the same output requirements (use as many documents as possible, cite sources, three-sentence output), drawn from prior work~\cite{zhang-etal-2024-fair}.
% Thus, \model does not have an unfair advantage over baselines in terms of prompt formatting, and the large coverage and balance gains of our method (\cref{subsection:citation_comp}) stem from the paradigm of treating documents as LLM speakers.
% All prompts will also be released for reproducibility.

% One limitation of our framework is that it relies on GPT-4.
% This decision was motivated by the lack of direct data to train a model to complete each of the steps of \model, and we found through preliminary testing that smaller LLMs like LLaMA-2 and GPT-3.5 are unable to follow all of the instructions necessary for our task (Appendix XYZ). 
% To improve the reproducibility of our work, we use a fixed checkpoint of GPT-4 (\texttt{gpt-4-1106-preview}) with a temperature of 0 in all experiments, and release all of our baseline implementations.
% Further, to improve the costs associated with using GPT-4, we employ retrieval at each step of \model to reduce the number of input tokens generated.
% To reduce costs further, researchers can explore using \model to generate synthetic training data~\cite{long2024llms} for each step of our pipeline, and then use this data to train a smaller model.
% We also find that \model is less expensive than Incremental-\emph{Topic} (\cref{subsection:efficiency}), the baseline with the second-best results at the topic paragraph level.


\section{Ethical Considerations} \label{subsection:ethics}

The goal of debatable query-focused summarization is to provide comprehensive and balanced summaries for yes/no queries that fairly represent both ``yes'' and ``no'' perspectives.
However, we acknowledge that not all yes/no queries should be balanced in a summary.
Balancing some queries could spread misinformation (e.g. ``Is the earth flat?''), or the user might prefer to focus on one side of the issue.
For misinformation, we limit DQFS to queries with \textit{equally-valid} opposing perspectives, as reflected in our high-quality DebateQFS dataset, which is annotated by the debate community. 
For user preferences, future work could study using the \textit{user's} perspective as input, tasking models to generate summaries that align with or challenge the user's viewpoint. This would enable fine-grained control, allowing users to decide when to balance diverse perspectives or focus on a preferred one.

Further, we assume our input documents are factual and written in good faith for DQFS, but this is not always guaranteed in practice.
To detect document misinformation, future DQFS research could explore adversarial settings where input documents contain factual errors, requiring models to incorporate a fact verification module to filter out factual inaccuracies.
In \model, a fact verification system could be run on the facts in \model's outline before summarization to discard factual inaccuracies.
We believe such efforts are essential for developing safe, factual, and reliable summarization systems.

\section*{Acknowledgments}

We would like to thank our collaborators at Adobe Research and the University of Maryland for their valuable contributions and insights that helped shape this work, including Jack Wang, Rajiv Jain, Jennifer Healey, Vishy Swaminathan, and Rachel Rudinger.
Nishant is also grateful to the amazing cohort of interns at Adobe Research---including Dang Nguyen, Vishakh Padmakumar, Dayeon Ki, Hyunji Lee, Yoonjoo Lee, and Paiheng Xu---not just for their helpful feedback, but also for making the internship a fun and memorable experience.