\section{Task Definition} \label{section:task}

Debatable query-focused summarization (DQFS) uses as input: 1) documents $\mathcal{D}$, where each source $d_i \in \mathcal{D}$ is a set of context paragraphs; 2) a yes/no query $q$; and 3) a number of summary topics $m>1$.
Source $d_i$ has perspectives $\mathcal{P}_i$, where perspective $(s, f) \in \mathcal{P}_i$ has stance $s \in \{\texttt{yes}, \texttt{no}\}$ and factual sentence $f$ derived via $d_{i}$, where $f$ supports $s$ as the answer to $q$.
We enforce ($\texttt{yes}, f$) and ($\texttt{no}, f$) are common in $\mathcal{P}$ (\cref{subsection:datasets}), meaning $q$ is \textbf{debatable}.

With these inputs, DQFS creates a summary $\mathbb{S}$ for $\mathcal{D}$ that answers $q$.
As seen in Figure~\ref{fig:intro}, $\mathbb{S}$ discusses topics $\mathcal{T} = \{t_1, ..., t_m\}$, each with a paragraph.
To aid trust and evaluation (\ref{subsection:metrics}), $\mathbb{S}$ contains citations (e.g. \texttt{[1]}) after each sentence noting the source document(s) for its information~\cite{huang-chang-2024-citation}.
For a \textit{comprehensive} and \textit{balanced} summary, we aim to cite a high number of documents in $\mathcal{D}$, ensuring no document's perspective is missed, and equally represent yes/no perspectives for $q$, curbing bias.
Comprehensiveness and balance are goals not only for the overall summary but also in each topic paragraph, ensuring a well-cited and balanced discussion within each topic.

\section{\model: Mixture of Document Speakers} \label{section:method}

For DQFS, we build \model (Figure~\ref{fig:model}), which uses content planning to guide generation~\cite{balepur-etal-2023-expository, shao-etal-2024-assisting} via the steps of drafting an outline $\mathcal{O}$; and condensing $\mathcal{O}$ into a summary $\mathbb{S}$.

To build $\mathcal{O}$, \model moderates a panel discussion of LLM speakers $\mathcal{S}$, where each speaker $s_i \in \mathcal{S}$~represents one document $d_i \in \mathcal{D}$.
\model executes: \textbf{1) Agenda Planning} to find $m$ topics $\mathcal{T}$ for $\mathcal{O}$ (\cref{subsection:agenda}); \textbf{2) Speaker Selection} to pick speakers $\mathcal{S}_j \in \mathcal{S}$ to respond to tailored queries for each topic $t_j \in \mathcal{T}$ (\cref{subsection:moderator}); and \textbf{3) Speaker Discussion} to prompt each speaker $s_i \in \mathcal{S}_j$ for its document's perspectives on $t_j$ and tailored query $q_{i,j}$ (\cref{subsection:speaker}), which are added to $\mathcal{O}$. We then prompt an LLM to use $\mathcal{O}$ to make a summary $\mathbb{S}$ (\cref{subsection:summary}). We describe each step below. 

\subsection{Agenda Planning} \label{subsection:agenda}

Before speakers discuss debatable query $q$ (``Is law school worth it?''), we must plan $m$ topics $\mathcal{T}$ (``law school jobs'') for the discussion (Fig~\ref{fig:model}, column 2).
In panel discussions, agendas are planned via \textit{biographies} summarizing speakers' expertise~\cite{pigeonholelive_panel_discussions}.
We also plan $\mathcal{T}$ with biographies $\mathcal{B}$ of our speakers' documents.
Instead of abstractively summarizing a speaker's document $d_i$ for its biography $b_i$ with an LLM, we efficiently create $b_i$ via extractive summarization---retrieving the $k$ contexts in $d_i$ most relevant to $q$ with ColBERT~\cite{khattab2020colbert}.
Then, in a 0-shot prompt, we ask an LLM to plan $m$ topics $\mathcal{T}$ relevant to $q$ and $\mathcal{B}$.

\subsection{Speaker Selection} \label{subsection:moderator}

After planning topics $\mathcal{T}$ for discussion (\cref{subsection:ablation}), we must decide which speakers $\mathcal{S}_j \subseteq \mathcal{S}$ are relevant for each topic $t_j \in \mathcal{T}$ (Fig~\ref{fig:model}, column 3).
We could pick all speakers, but this may hamper efficiency if we want to tailor queries for speakers (Appendix~\ref{subsection:efficiency}).
To illustrate, for the topic ``law school jobs,'' a document with perspectives on ``tuition costs'' can be omitted for efficiency, as it is not topically relevant.  

To solve this, a \textbf{Moderator} LLM picks relevant speakers $\mathcal{S}_{j}$ for each topic $t_j \in \mathcal{T}$. 
%In the context of multi-LLM agents, our moderator is similar to a router~\cite{hu2024routerbench} or re-ranker~\cite{sun2023chatgpt}.
It is costly to prompt with all documents just to select speakers, so we use retrieval (\cref{subsection:agenda}) to create a biography $b_{i, j}$ of each speaker $s_i$ for topic $t_j$.
The biographies $\mathcal{B}_j$ are used in a 0-shot prompt, asking the moderator for speakers $\mathcal{S}_{j} \subseteq \mathcal{S}$ with biographies related to $t_j$.

To better cater to speakers' expertise, the Moderator also tailors a query $q_{i, j}$ specific to each selected speaker $s_i \in \mathcal{S}_j$ and topic $t_j$ using biography $b_{i,j}$; in panel discussions, moderators tailor queries to target speaker perspectives~\cite{Fingerhut2002, panel_discussion_questions}.
In \model, the queries form a chain-of-thought~\cite{10.5555/3600270.3602070}, improving our speaker selection (\cref{subsection:ablation}), and can be used for re-ranking~\cite{sachan2022improving}, serving as enhanced retrieval queries versus topic $t_j$ for speaker discussion (\cref{subsection:speaker}).
The tailored queries also further structure our outline $\mathcal{O}$, giving follow-up queries~\cite{liu2019fanda} for free that may interest users~(\cref{subsection:qg}).

\subsection{Speaker Discussion} \label{subsection:speaker}

After selecting relevant speakers $\mathcal{S}_j$ and tailoring them a query for each topic $t_j$ (\cref{subsection:moderator}), we must get the perspectives $\mathcal{P}$ from speakers' documents for the outline $\mathcal{O}$ (Fig~\ref{fig:model}, column 4).
A simple method to get $\mathcal{P}$ is to add all documents from $\mathcal{S}_j$ in one prompt and ask for perspectives on $t_j$, but LLMs often ignore text in the middle of long prompts~\cite{liu2024lost}, which may discard perspectives and reduce coverage. Further, LLMs may disregard the documents that oppose their parametric memory~\cite{jin2024tug}, skewing the outline's balance.

Using fairness ideals in panel discussions~\cite{Fingerhut2002}, speakers $s_i \in \mathcal{S}_j$ are \textit{individually} prompted to supply its document's perspectives for $t_j$ based on its tailored query $q_{i,j}$.
For example, on the topic ``law school jobs,'' we may query one speaker for ``market trends'' and another separately for ``Ivy League placement.''
Thus, each speaker adds its document's unique perspectives one at a time, preventing any one document from dominating, which leads to higher coverage (\cref{subsection:ablation}).


% This better ensures all speakers in $\mathcal{S}_j$ are included, enhancing outline coverage and balance.
%Further, individual speaker responses curbs the risk of citing a wrong document~\cite{ji2023survey}, as speakers only have access to one document.

A speaker $s_i$ gives perspectives for a topic $t_j$ in two steps.
First, for efficiency, the speaker retrieves the $k$ contexts $\mathcal{C}$ in its document most relevant to the tailored query $q_{i, j}$.
Using the debatable query $q$, contexts $\mathcal{C}$, tailored query $q_{i, j}$, and topic $t_j$, the speaker is 0-shot prompted to give its yes and no perspectives $\mathcal{P}$ for $q$ based on $\mathcal{C}$, related to $q_{i, j}$ and $t_j$.
Each yes/no stance and fact $(s, f) \in \mathcal{P}$, tailored query $q_{i, j}$, and document number $i$ is added to $\mathcal{O}$ under topic $t_j$.
The yes/no stance predictions in $\mathcal{P}$ have $80$\% accuracy (Appendix~\ref{appendix:outline}), which better organizes $\mathcal{O}$ (\cref{subsection:qg}) to improve summaries~(\cref{subsection:ablation}).



\subsection{Outline Summarization} \label{subsection:summary}

Our outline $\mathcal{O}$ is a rich structure to track perspectives for a debatable query $q$, which we use as a content plan~\cite{balepur-etal-2023-expository} to create the final summary $\mathbb{S}$.
To do so, we test summarizing: 1) all of $\mathcal{O}$ in one prompt; and 2) topic sections of $\mathcal{O}$, i.e. $\{\mathcal{O}_j, \forall t_j \in \mathcal{T}\}$, one prompt at a time. 
We call these models 1) \modelAll and 2) \modelTopic. We detail the full \model system in Appendix~\ref{subsection:model}.

%Summarize and take notes~\cite{Fingerhut2002, panel_discussion_summary}