

\section{Results}

We generate DQFS summaries with two to five topics $m$, a traditional range for argumentative essays~\cite{mery2019use}.
Due to space constraints, we only show $m \in \{3, 5\}$ in the following sections, with all experiments repeated in Appendix~\ref{appendix:results}.

\subsection{Citation Coverage and Balance} \label{subsection:citation_comp}

\model excels at coverage and balance for summaries and topic paragraphs (Tables~\ref{table:doc_cover_cqa}, \ref{table:doc_cover_debate}).
Notably, \modelTopic leads in all metrics 22/24 times and is \textbf{always} a significantly best model.
\modelAll is also strong, a top-2 model in 22/24 cases.
Some models have high full summary scores, but \modelTopic largely improves DC/Fair/Faithful in \textit{topic paragraphs}, with 38/48/59\% mean increases over the next-best model. 
LLMs struggle in summarization coverage and diversity~\cite{huang-etal-2024-embrace, zhang-etal-2024-fair}, but we show that these issues are more pronounced in multi-aspect texts.
Our results verify \model's strategy of moderating \textit{single-turn} LLM discussions, so \textit{multi-turn} debates~\cite{khan2024debating} may produce even better summaries.

We also check two confounders: \textit{citation accuracy}, how often cited documents support claims in the sentence, and \textit{sentence count}.
Summaries can game our metrics via inaccurate citations or more sentences, as models cite post-sentence.
We assess citation accuracy via an LLM entailment model, a standard approach~\cite{gao2023enabling, balepur-etal-2023-text}, with 87\% human agreement on 200 held-out examples (Appendix~\ref{appendix:metrics}).
Citation accuracy and sentence count are consistent across models, so \model's strong coverage and balance are not due to extra sentences or non-existent perspectives.

Lastly, models can plan different topics, as we believe the open-aspect nature of topics in DQFS is interesting for future work~\cite{amar2023openasp}.
To ensure \model's gains are not from planning topics (\cref{subsection:agenda}) naturally more balanced or comprehensive, in Appendix~\ref{appendix:fixed_topics}, all models produce summaries for the same topics from our agenda planning step. 
\model is superior, validating its strength is from the LLM speaker design (\cref{section:method}), not topic selection.

%\input{data/fine_grained}

% \subsection{Fine-grained Perspective Evaluation} \label{subsection:fine_grained_stance}

% We measure summary balance via the distribution of document-label citations (\cref{subsection:metrics}), but this does not fully ensure that the \textit{text} in the summary is balanced.
% Thus, we design a metric to assess the balance of the text of a summary $\mathcal{S}$ produced from documents $\mathcal{D}$ and query $q$.
% Let $(s, c) \in \mathcal{S}$ be a sentence/citation pair in the summary.
% We use post-hoc attribution~\cite{huang-chang-2024-citation} with ColBERT to retrieve the paragraph $p$ in the document $\mathcal{D}[c]$ predicted to have the information used to write $s$.

% We find the perspective label $l \in \{\texttt{yes}, \texttt{no}, \texttt{neutral} \}$ for paragraph $p$ and repeat this for every sentence/citation pair in $\mathcal{S}$ to get a list of perspective labels $\mathcal{L}$.
% To ensure a model is not penalized for citing multiple documents
% We can then use the same 
% We obtain these labels $l$ by collecting X human annotations for 5 full ConflictingQA and Debatepedia dataset entries each.
% With this method, 

% To study this, we collect and release X human annotations for the paragraphs $\mathcal{C}$ of each document collection $\mathcal{D}$ for 5 entries in ConflictingQA and 5 entries for Debatepedia; we ask humans to annotate if each paragraph $c \in \mathcal{C}$ supports, refutes, or is neutral relative to the debatable query $q$ (details in Appendix XYZ).
% Then, for each sentence $s \in \mathcal{S}$ in the summaries produced by models on these 10 entries, we use post-hoc attribution~\cite{huang-chang-2024-citation} via ColBERT within each cited document to find the top paragraphs $\mathcal{C}_{attr} \subseteq \mathcal{C}$ (one paragraph per cited document) that contain the information in $s$.

% We compute the Fairness and Faithfulness (\cref{subsection:metrics}) of the perspective distribution of $\mathcal{C}_{attr}$ across models, discarding paragraphs with a neutral label, seen in Table~\ref{table:fine_grained}.
% \model always has one of the two highest fairness and faithfulness scores, where \modelTopic has the highest scores in 12/16 cases and \modelAll has the highest scores in 8/16 cases---the best of any two models.

% We opt to assess sentence-level perspective balance via post-hoc retrieval, rather than annotating the outputs directly, for reproducibility.
% Human evaluation is hard to reproduce~\cite{belz2023missing}, but this method will allow future DQFS works to replicate our evaluation; we hope future researchers can design similar reproducible evaluation protocols for their tasks.
% Further, Appendix ABC shows that our method has X\% agreement with directly annotating the sentences of generated text.

\subsection{Summary Quality} \label{subsection:summary_comp}

Our citation metrics show \model excels in summary coverage and balance, so we now ensure that these gains are not at the cost of traditional measures of summary quality.
To do so, we conduct a sanity check and evaluate outputs
with five typical summary quality metrics~\cite{lloret2018challenging}: \textbf{int}erest, \textbf{coh}erence, \textbf{rel}evance, \textbf{cov}erage, \textbf{div}ersity.
The first 4 are from~\citet{shao2024assisting}, who use them on Wikipedia writing, while diversity is new for DQFS, testing the balance of yes and no perspectives.
We use Prometheus, an LLM with 72-85\% human agreement~\cite{kim2024prometheus}, for 1-5 scoring (Appendix~\ref{appendix:metrics}).
We score summaries, topic paragraphs, and topics using these metrics.

Prometheus just uses the summary, topic paragraph, or topic title as input and does not have access to the input documents.
Thus, our evaluation of coverage through citation metrics (\cref{subsection:metrics}) measures the coverage of the input documents, while Prometheus assesses coverage using its parametric knowledge, specifically evaluating if the outputs provide ``an in-depth exploration of the query and have good coverage.''
LLM evaluators can be biased~\cite{wang-etal-2024-large-language-models-fair}, so we also conduct a human evaluation in \cref{subsection:human_eval}.
We also use \textbf{S}elf-\textbf{B}leu~\cite{zhu2018texygen} ($n=4$) to assess the semantic distance between paragraphs~\cite{liu-etal-2023-dimongen}. 

\modelTopic and \modelAll have significantly high-quality summaries, topic paragraphs, and topics 28/30 and 25/30 times (Table~\ref{table:llm_eval}).
In summaries and paragraphs, \model has the best coverage 5/6 times and diversity 6/6 times, aligning with our citation metrics (\cref{subsection:citation_comp}).
\model has a slightly higher SB, meaning more paragraph similarity.
This similarity does not largely impede readability (\cref{subsection:human_eval}), and this occurs as \model adapts similar perspectives for distinct topics\footnote{For example, a document's perspective that ``electric cars must be recharged often'' relates to ``consumer utility'' and ``energy use''--- distinct topics. Appendix~\ref{appendix:outputs} has examples.}.
Given the tradeoff in paragraph coverage and dissimilarity~\cite{alguliev2012gendocsum+}, a small SB increase is worth the large coverage and balance gains (\cref{subsection:citation_comp}).
Overall, \model exhibits strong coverage, balance, and quality.

\input{data/human}

\subsection{Human Evaluation} \label{subsection:human_eval} 

We have 76 users compare 20 DQFS outputs per dataset from \modelTopic to Hierarchical and Incremental-\textit{Topic}, the next-best models, and long-context, the simplest model.
\model cites more documents (\cref{subsection:citation_comp}), so users rate \textit{\textbf{Read}ability}~\cite{ribeiro2023generating} to ensure the extra perspectives do not harm comprehension.
Users also rate \textit{\textbf{Bal}ance}, as DQFS must fairly show yes/no stances. Scores are from 1-5 (Appendix~\ref{appendix:human}) and are used for full summaries and paragraphs on the same topic.



\model has similar readability to baselines (Table~\ref{table:human}), meaning our additionally cited perspectives are clearly conveyed, and users find \model's summaries/paragraphs the most balanced.
In 3/4 cases, \model has the highest average of readability and balance.
Thus, \model is the best DQFS model,~citing more documents and better balancing perspectives versus SOTA, all while preserving readability.

\input{data/ablation}
\input{figures/outline}

\subsection{Ablation Study} \label{subsection:ablation}

We ensure all parts of \model are useful by ablating our outline creation and summarization steps.
In outline creation, having individual speakers respond versus combining all speaker biographies in a prompt (No Speak), tailoring queries (No Tailor), and picking speakers via CoT (No CoT) all improve outlines (Table~\ref{table:ablation_outline}).
No Speak has the worst outlines, confirming the strength of equally treating document speakers for DQFS.
We also test our moderator's abilities by having all speakers respond (No Mod) instead of selecting speakers.
No Mod has higher DC as all speakers respond, but fewer perspectives per document, meaning our moderator adeptly selects speakers with relevant perspectives.

To see how outline $\mathcal{O}$ alters summaries, we compare \model (with no moderator) updating an outline to updating free-form paragraphs (-$\mathcal{O}$). 
Using $\mathcal{O}$ greatly improves coverage and fairness (Table~\ref{table:ablation_summary}, top), showing structured outlines are better intermediate outputs than free-form text in multi-LLM systems.
Further, extra organization in $\mathcal{O}$ (stances, tailored queries) aids summarization (Table~\ref{table:ablation_summary}, bottom), so richer outlines yield better summaries.




%\input{data/cost}

% \subsection{Efficiency Comparison} \label{subsection:efficiency_main}

% We compare the efficiency of four multi-LLM systems: \modelTopic, our model without a moderator, Hierarchical Merging run for each topic paragraph\footnote{This model is more similar to \model for efficiency comparisons, but we do not compare with otherwise as it is too expensive to run, making it an impractical choice for DQFS. We provide its performance in Appendix~\ref{appendix:results}, which is still worse than \model.}, and Incremental-\emph{Topic}.
% \model's use of retrieval and a Moderator LLM allows us to produce high-quality summaries while keeping cost and inference time low.

\subsection{Outline Analysis Case Study} \label{subsection:qg}

\model builds an outline $\mathcal{O}$ as a content plan pre-summarization~\cite{shao2024assisting}, but $\mathcal{O}$ is also a valuable tool for users~\cite{barrow2021syntopical}.
Figure~\ref{fig:outline} shows part of an outline, which organizes perspectives with their source documents and yes/no stances.
$\mathcal{O}$ outlines all seven input documents and a range of perspectives for a thorough, balanced view of the debatable query.
Further, the tailored document queries in $\mathcal{O}$ can inspire users to explore follow-up queries to ask.
Overall, $\mathcal{O}$ gives a rich, structured representation of perspectives, enabling in-depth explorations of document collections.