

\section{Experimental Setup}



\subsection{Dataset Collection} \label{subsection:datasets}

DQFS needs entries of documents $\mathcal{D}$ with facts for ``yes'' and ``no'' answers to a query $q$. An apt dataset is \textbf{ConflictingQA}~\cite{wan2024evidence}, with controversial yes/no web search queries (``Do fires benefit forests?'') and labeled support/refute web~pages.

Other summarization diversity datasets are unsuited for DQFS.
Opinion summarization~\cite{zhang-etal-2024-fair} is grounded in subjective tweets/reviews, while DQFS needs fact-based texts.
DiverseSumm~\cite{huang-etal-2024-embrace} has diverse news articles, but lacks queries with opposing perspectives.
Debate datasets~\cite{roush2024opendebateevidence} are factual with opposing sides, but rely on argument mining corpora with specific claims (``Colonialism made an exclusion hierarchy''), which are hard to group into broad DQFS queries (``Is colonialism~good?'').

% Other recent datasets in summarization diversity works are not suited for DQFS.
% PerspectiveSumm~\cite{zhang-etal-2024-fair} has datasets to aggregate opinions from short tweets and reviews, but DQFS requires fact-based, page-long documents.
% DiverseSumm~\cite{huang-etal-2024-embrace} can be used to answer queries grounded in diverse news articles for the same event, but it does not contain yes/no queries leading to opposing perspectives.
% Debate datasets~\cite{roush-balaji-2020-debatesum, roush2024opendebateevidence} are promising, as they are factually grounded with clear opposing sides.
% However, these datasets~focus on argument mining, with entries of evidence paragraphs for specific claims (``\textit{Colonialism created the hierarchy for exclusion}'').
% Grouping such claims into high-level queries (``\textit{Did colonialism benefit societies?}'') for DQFS is non-trivial.

We create \textbf{DebateQFS}---a new dataset based on Debatepedia, the ``Wikipedia of debates''~\cite{gottopati2013learning}.
Debatepedia pages have broad topics (``carbon tax''), where users curate documents arguing pros/cons.
We turn topics into yes/no queries and collect the text of sites cited as pro/con sources.
We get 290 document sets for ConflictingQA and 183 for DebateQFS, each with a debatable query, with mean document set sizes of 10.47 and 9.86.
We also have ground-truth yes/no stances for the full documents, with mean majority/minority splits of 0.65/0.35 and 0.62/0.38.
We use these stances for summary balance (\cref{subsection:metrics}), but users also assess balance (\cref{subsection:human_eval}).
Appendix~\ref{appendix:data} has dataset details.

\input{data/doc_cover_cqa}

\subsection{Baselines} \label{subsection:baselines}

We compare \model to SOTA LLM summarizers:\\
\noindent \textbf{1) Long-Context:} All documents $\mathcal{D}$ are used as the input in a single prompt~\cite{wang2024beyond}. \\
\noindent \textbf{2) RAG-\textit{All}:} Top-($k|\mathcal{D}|$) contexts in $\mathcal{D}$ relevant to $q$ are retrieved as input prompt~\cite{lewis2020retrieval}. \\
\noindent \textbf{3) RAG-\textit{Doc}}: Same as RAG-\textit{All}, but we retrieve the $k$-most relevant contexts in \textit{each} source in~$\mathcal{D}$. \\
\noindent \textbf{4) Hierarchical-\textit{All}:} Each document in $\mathcal{D}$ is summarized using $q$; these are summarized again into a final output under $m$ topics~\cite{chang2024booookscore}. \\
% \noindent \textbf{5) Hierarchical-\textit{Topic}:} We plan topics $\mathcal{T}$ (\cref{subsection:agenda}) and run (4) on each topic to create topic paragraph. \\
\noindent \textbf{5) Incremental-\textit{All}:} We plan topics $\mathcal{T}$ (\cref{subsection:agenda}) and iterate over each document in $\mathcal{D}$ to incrementally update the paragraphs for $\mathcal{T}$~\cite{chang2024booookscore}. For the final summary, we self-refine all paragraphs at once like chain-of-density~\cite{adams2023sparse}. \\
\noindent \textbf{6) Incremental-\textit{Topic}:} Same as Incremental-\textit{All}, but we self-refine topic paragraphs independently. \\
\noindent \textbf{7) Cluster:} We sort $\mathcal{D}$ into $m$ clusters, summarized to form topic paragraphs~\cite{hayashi-etal-2021-wikiasp}. \\
\noindent \textbf{8) RAG+Cluster:} Same as Cluster, but we retrieve the top-($k|\mathcal{D}|$) relevant contexts using $q$ before clustering, similar to LLooM~\cite{lam2024concept}.

These cover the main summarization paradigms: seq2seq~\cite{sutskever2014sequence}, clustering~\cite{zhang2009automatic}, content selection~\cite{louis2010discourse}, and multi-model frameworks~\cite{chang2024booookscore}.

\input{data/doc_cover_debate}
\input{data/llm_eval}

\subsection{Implementation Details} \label{subsection:implementation}

All models use 0-shot gpt-4-1106-preview~\cite{achiam2023gpt} with 0 temperature.~We write prompts using best practices on a small held-out set with fixed instructions for models~\cite{schulhoff2024prompt}.
LLMs are prompted to ``Use as many documents as possible'' and write three-sentence topic paragraphs.
The former ensures LLMs have the goal of coverage, while the latter fixes length confounders (\cref{subsection:citation_comp}).
Both of these strategies (specifying instructions, three-sentence text) have been used to improve summary balance~\cite{zhang-etal-2024-fair}.
We give mode details in Appendix~\ref{appendix:implementation}.

We retrieve via ColBERT~\cite{khattab2020colbert}, a retriever trained on MS-MARCO~\cite{Campos2016MSMA}, with $k=3$, and cluster using BERTopic and KMeans~\cite{macqueen1967some, grootendorst2022bertopic}.
Other parameters are default without tuning.
Results are from a single~run.



\subsection{Quantitative Evaluation via Citations} \label{subsection:metrics}

DQFS tests if models can cover and balance document perspectives.
To assess this, works use \textit{post-hoc} attribution, mapping summaries to sources they are believed to derive from~\cite{wolhandler2022multi, zhang-etal-2024-fair}.
But this does~not mean the model \textit{intends} to use all attributed texts.
A model may give perspectives using one source that is post-hoc attributable to many, gaming coverage and balance metrics without truly reflecting these qualities.

To solve this, we use \textit{pre-hoc} attributions~\cite{huang-chang-2024-citation}, i.e. citations, as they can better capture which documents the model intends to use.
Since each baseline gives document citations after each sentence (\cref{section:task}), and we know the ground-truth yes/no stances of these documents (\cref{subsection:datasets}), we can evaluate summary coverage and balance using the coverage and balance of the cited documents.
% Further, testing coverage and balance with citations lets us study citation accuracy (\cref{subsection:citation_comp}).

Let $\mathcal{D}_{cite} \subseteq \mathcal{D}$ be the cited documents in a text.
For \textit{comprehensiveness}, we let \textbf{document coverage (DC)} be the percent of sources in $\mathcal{D}$ cited.
For \textit{balance}, we use the ground-truth yes/no document stances.
We compute KL divergence of the distribution of $\mathcal{D}_{cite}$ stances to: 1) a uniform distribution; and 2) the stance distribution of all input documents $\mathcal{D}$.
(1) sees if $\mathcal{D}_{cite}$ splits perspectives equally, i.e. \textbf{fairness}~\cite{zhang-etal-2024-fair} and (2) tests if $\mathcal{D}_{cite}$ captures the input document split, i.e. \textbf{faithfulness}~\cite{fischer2022measuring}.
In DQFS, fairness is more critical for summary balance, but as our input documents have fairly balanced stance splits (\cref{subsection:datasets}), improving on both metrics is feasible.
We present citation faithfulness as another aspect of DQFS for research to explore.
These three metrics are aggregated over full summaries and topic paragraphs, as high-quality DQFS outputs should be balanced and comprehensive overall and within each paragraph.