
\section{Appendix} \label{appendix}

\subsection{Dataset Details} \label{appendix:data}

To collect a dataset based on Debatepedia~\cite{gottopati2013learning}, we use Wayback Machine\footnote{\url{https://web.archive.org/}}, as the original website is no longer hosted.
We iterate through each debate articles page on the website with BeautifulSoup\footnote{\url{https://pypi.org/project/beautifulsoup4/}} and collect the 1) topic of the debate; 2) list of URLs under ``Supporting References''; and 3) list of URLs under ``Refuting References''.
We then use jusText\footnote{\url{https://pypi.org/project/jusText/}} to extract the text content from each web page, ignoring websites that are not free-to-access.

After this, we filter out instances that have less than five sources or do not have at least a 75/25 majority/minority split of perspective labels.
We then remove web pages that do not have any of the non-stopword tokens in the query, implemented with nltk, to ensure the web pages form a set of relevant documents.
We run this same process on ConflictingQA~\cite{wan2024evidence}.

Dataset statistics after data processing are in Table~\ref{appendix:table:dataset}.
Since all websites were publicly-accessible, our collected artifacts are within their intended use and licenses.
We sampled a subset of five document collections and manually checked them for PII and offensive content, which we did not find; we also found all text to be in English.

\subsection{The \model Algorithm} \label{subsection:model}

We detail \model in Algorithm~\ref{algo:mods}. For a debatable query $q$, document collection $\mathcal{D}$, number of topics $m$, and retrieval parameter $k$, we create speakers $\mathcal{S}$ for $\mathcal{D}$.
First, we retrieve speaker biographies $\mathcal{B}$ related to $q$ and plan $m$ topics $\mathcal{T}$ for $\mathcal{O}$ (\cref{subsection:agenda}). For each topic $t_j \in \mathcal{T}$, we pick relevant speakers $\mathcal{S}_j \subseteq \mathcal{S}$ and tailor them questions $\mathcal{Q}_j$ using their topic biographies $\mathcal{B}_j$ (\cref{subsection:moderator}). Each speaker supplies stance/fact perspectives $\mathcal{P}$, which are tracked in $\mathcal{O}$ (\cref{subsection:speaker}).
Finally, $\mathcal{O}$ is summarized all at once ($\mathbb{S}_{all}$) or per topic ($\mathbb{S}_{top}$) and returned to the user (\cref{subsection:summary}).

\subsection{Experimental Setup Details} \label{appendix:implementation}

All of our baseline implementations use GPT-4 (\texttt{gpt-4-1106-preview}) with 0 temperature and a maximum input token length of 127,000 tokens.
All baselines use zero-shot prompting, and the prompts will be released with our code after internal approval.
For costs associated with using GPT-4, see Appendix~\ref{subsection:efficiency}.

All models using retrieval, including \model, use ColBERT~\cite{khattab2020colbert}, a state-of-the-art retriever. For hyperparameters, we use a maximum document length of 300 tokens, a maximum query length of 64 tokens, 8 bits, and the \texttt{colbert-ir/colbertv2.0} checkpoint; none of these parameters were tuned during experimentation.
The clustering methods were implemented with BERTopic~\cite{grootendorst2022bertopic}, using all default values.

All experiments were run on a single H100 GPU, but as the only GPU usage comes from retrieval, we found \model and all baselines can be run on a Google Collaboratory T4 GPU (16GB of GPU memory).
Each baseline was allocated 24 hours for a single run.
We give more details about the runtime of \model in Appendix~\ref{subsection:efficiency}.

% Add prompts

\subsection{Metric Details} \label{appendix:metrics}

We extract all citations via regex\footnote{\url{https://docs.python.org/3/library/re.html}} by first finding text between square brackets (\texttt{[} and \texttt{]}) and then extracting integers between these spans.
The document coverage, faithfulness, and fairness metrics are all implemented with numpy\footnote{\url{https://numpy.org/}}.

We implement citation accuracy through entailment; entailment has shown to be a viable strategy to measure the factuality of text~\cite{maynez2020faithfulness}.
We use GPT-3.5 (\texttt{gpt-35-turbo-1106}) with 0 temperature to classify whether a generated sentence is entailed by the document it cited, using a 0-shot prompt shown in Prompt~\ref{prompt:cite_acc}
To evaluate the accuracy of this metric, we manually annotate 200 held-out examples (100 examples GPT predicted to be accurate citations, and 100 examples predicted to be inaccurate citations) of generated summaries for DQFS from all models (not used in evaluation).
We annotate these blindly, without knowing the output classification of GPT-3.5.
On this set, we obtain 87\% agreement with GPT-3.5, close to the agreement of 88\%, 90\%, and 96\% shown by human annotators in~\citet{min2023factscore}.
Further, this value is near the entailment-based accuracy given in other factuality tasks~\cite{balepur-etal-2023-expository, balepur-etal-2023-text}.

For the summary quality evaluation (\cref{subsection:summary_comp}), we use the Prometheus-v2 LLM evaluator\footnote{\url{https://github.com/prometheus-eval}}.
Example rubrics given to this evaluator are in Table~\ref{table:rubric}, which are adapted directly from~\citet{shao2024assisting}.

\subsection{Efficiency and Cost Comparison} \label{subsection:efficiency}

In Tables~\ref{appendix:table:cost_cqa} and \ref{appendix:table:cost_debate}, we present the cost (LLM input/output tokens, number of calls) and efficiency (seconds taken for inference) of \modelTopic, the slightly more expensive model out of the two \model baselines, versus Hierarchical Merging and Incremental Updating~\cite{chang2024booookscore, adams2023sparse}, the two other best-performing baselines, which also happen to be multi-LLM systems. Despite \model using more LLM calls through single-turn LLM debate, our use of retrieval and a moderator LLM greatly reduces the number of input tokens \model otherwise would have consumed, keeping GPT-4 cost competitive with Hierarchical Merging, and making our model cheaper than Incremental-\textit{Topic}.
The inference time of multi-LLM summarization systems like \model could be improved, a common limitation of agentic systems~\cite{li2024personal}, and one possible strategy could be to use multi-threading or batched decoding to parallelize the discussions of LLM speakers. 

\subsection{Results for All Topics} \label{appendix:results}

We run \model and all baselines where the number of topics $m$ ranges between $2$ and $5$ inclusive, a typical range of paragraphs in argumentative essays~\cite{mery2019use}.
Tables~\ref{table:doc_cover_cqa_all} and \ref{table:doc_cover_debate_all} display the citation coverage and balance metrics from \cref{subsection:citation_comp} for all $m$, while Tables~\ref{appendix:table:llm_cqa} and \ref{appendix:table:llm_debate} display the summary quality metrics from \cref{subsection:summary_comp} results for all $m$.
Our claims hold for these varied values of $m$; \model generates comprehensive and balanced summaries while maintaining traditional output quality metrics, regardless of the number of topic paragraphs it must generate.

\subsection{Results for Hierarchical Merging over Topic Paragraphs} \label{appendix:results_hm}

Further, the Hierarchical Merging baseline we use does not generate summaries one topic at a time.
We believe that such a model (i.e. Hierarchical-\emph{Topic}) is too costly and inefficient to deploy, so we do not compare against it in the main body of the work. 
In Tables~\ref{table:doc_cover_cqa_all_comp} and \ref{table:doc_cover_debate_all_comp} we provide some results for this model, which still underperforms \modelTopic.
Further, we show in Table~\ref{appendix:table:cost_weird} that this model is much more costly compared to \model.
It is also more costly than a version of \model that iterates through all speakers, highlighting the utility of retrieval to keep inference time and LLM cost low.

\subsection{Results with GPT-4 Mini} \label{appendix:results_mini}

All of our models are implemented with GPT-4, but we also run some preliminary experiments with \modelTopic using GPT-4 mini.
In citation coverage, fairness, and faithfulness (Tables~\ref{table:doc_cover_cqa_all_comp_mini} and \ref{table:doc_cover_debate_all_comp_mini}), \modelTopic using GPT-4 mini underperforms the model using GPT-4, suggesting that larger models are better suited for multi-LLM systems like \model. However, the GPT-4 mini system still exhibits strong performance, and is even comparable to several of the baselines using GPT-4 in Tables~\ref{table:doc_cover_cqa_all_comp} and \ref{table:doc_cover_debate_all_comp}, further showcasing the efficacy of our framework.

\subsection{Results with Fixed Topics} \label{appendix:fixed_topics}

Each baseline in \cref{subsection:baselines} produces distinct topics while planning a summary.
To ensure the citation coverage and balance gains in \model are not just derived from our agenda planning step (\cref{subsection:agenda}), we implement a version of each baseline that is asked to generate summaries for the same topics that \modelTopic generates.
We present these results in Tables~\ref{table:doc_cover_cqa_fixedtopic} and \ref{table:doc_cover_debate_fixedtopic}, and find that \model still largely outperforms baselines even when using our topics, suggesting that our agenda planning is not the source of gains in the framework.

% We also compare \modelTopic to the Hierarchical Merging baseline used in \cref{subsection:efficiency_main} more extensively in Tables~\ref{table:doc_cover_cqa_all_comp} and \ref{table:doc_cover_debate_all_comp}.
% Even though this baseline runs inference on each document for each topic, our structured outline allows us to outperform this baseline with much better efficiency.

\subsection{Outline Perspective Accuracy} \label{appendix:outline}

During speaker discussion (\cref{subsection:speaker}), we ask speakers to provide perspectives in the form of facts in the document.
These facts are grouped by whether the fact gives evidence for why the answer to the query is ``yes'' or ``no'', which also provides another layer of organization to enrich the user's understanding of the outline (\cref{subsection:qg}).
To assess the accuracy of these yes/no labels, we ask human annotators to label if each paragraph in 10 document collections (5 from DebateQFS, 5 from ConflictingQA) strongly supports, weakly supports, strongly refutes, weakly refutes, or is neutral toward the input query.
In total, we collect 7592 annotations, and aggregate them into one of three labels: supports, refutes, or neutral.\footnote{For each annotator, we score a paragraph as $ \pm1$ for strongly support/reject, $\pm0.5$ for weakly support/reject, and 0 for neutral. We take the sum of these scores over all annotators, and set the final label to support/reject if the sum is greater/less than 0. A score of 0 yields a neutral label.}
We will also release these paragraph-level annotations, which may be useful for training DQFS models.
We use the same procedure in Appendix~\ref{appendix:human} for this user study.

After collecting ground truth paragraph labels, we take the outlines produced by \model on this subset of 10 examples. For each predicted yes/no fact in the outline, we post-hoc attribute~\cite{huang-chang-2024-citation} the paragraph in the speaker's document that was the source of the information in the fact (with ColBERT).
We compare the accuracy of the LLM's yes/no label using the ground truth labels from human annotators, which are 0.798, 0.806, 0.781, and 0.803 for $m = 2, 3, 4, 5$, respectively.
Our accuracy is near the accuracy of LLMs on existing stance detection benchmarks~\cite{lan2024stance}, meaning our yes/no labels provide a useful and fairly accurate signal for users.

\subsection{Human Evaluation Setup} \label{appendix:human}
We conducted user evaluations to compare the readability and balance of summaries produced by different models (\model, Long-Context, Hierarchical, Incremental-Topic). The evaluation was divided into two parts: one focusing on the entire summary and the other on topic paragraphs.

\subsubsection{Recruitment \& Procedure}
We recruited 76 participants via Prolific, all of whom were based in the United States and required to have fluency in English. Each participant rated a total of 20 summaries, assessing the output from each of the four models for a given debate query. 
Participants were paid \$12/hour, the recommended rate on the website.
To mitigate order and fatigue effects, the presentation order of summaries was counterbalanced. Each summary was rated by 3-5 different participants. Additionally, the task included two baseline comprehension checks to ensure participants understood the instructions and metric definitions. Participants who did not pass these checks were excluded from the final analysis.
These annotations did not require review from an Institutional Review Board (IRB).
We collect no Personal Identifiable Information during the study.


\subsubsection{Rating Criteria}
The task included two Likert ratings for Readability and Balance. Additionally, participants could provide open comments for feedback or to report any issues. For the Likert items, participants saw the following questions:

\begin{itemize}
    \item \textbf{Readability.} Is the summary easy to read and understand?
    \begin{enumerate}
        \item The summary is very unclear, with consistent grammatical errors and disjointed ideas.
        \item The summary is often unclear, with frequent grammatical errors and poor flow.
        \item The summary is moderately clear but has some grammatical errors and awkward transitions.
        \item The summary is mostly clear, with minor grammatical errors and mostly smooth transitions.
        \item The summary is exceptionally clear, grammatically perfect, and flows seamlessly.
    \end{enumerate}

    \item \textbf{Balance.} Does the summary address both sides of the debatable query by using counterarguments to present a well-rounded view?
    \begin{enumerate}
        \item The summary is heavily biased, with little to no use of counterarguments and only one side addressed effectively.
        \item The summary is poorly balanced, significantly favoring one side and using counterarguments ineffectively.
        \item The summary is somewhat balanced but has noticeable bias and some awkward or less effective counterarguments.
        \item The summary is mostly balanced, with minor bias and effective use of counterarguments.
        \item The summary is perfectly balanced, equally addressing both sides and effectively using counterarguments.
    \end{enumerate}
\end{itemize}

\subsubsection{Results}

Figure~\ref{fig:annot} shows the full distribution of Prolific annotations for Balance and Readability across Summaries and Topic Paragraphs. 


\subsection{Sample Outputs} \label{appendix:outputs}

We present sample outputs generated by \model on ConflictingQA (Summary~\ref{summary1}, \ref{summary2}) and DebateQFS (Summary~\ref{summary3}, \ref{summary4}).
The summaries from \model have high coverage, citing several documents from the input collection, while also being balanced.
Further, the summary quality of \model remains high.
After comparing the summary for the EU expansion query in Figure~\ref{fig:intro} from 0-shot GPT-4 versus the summary from \model in Summary~\ref{summary3}, the balance, comprehensiveness, and quality gains from our method are clear.

\clearpage
%\input{appendix/prompts}
\input{appendix/dataset}
\input{figures/algo}
\input{appendix/rubric}
\input{appendix/all_doc_cover}
\input{appendix/all_doc_cover_comp}
\input{appendix/all_doc_cover_comp_mini}
\input{appendix/all_doc_cover_fixed_topic}
\input{appendix/all_llm_rank_cqa}
\input{appendix/all_llm_rank_debate}
\input{appendix/cost}
\input{appendix/annot}


\clearpage
\input{appendix/outputs}