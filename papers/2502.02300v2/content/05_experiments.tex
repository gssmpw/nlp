
\section{Experiments}
\label{sec:experiments}

To benchmark the accuracy of our CTSM objectives, we closely follow the experimental setup of~\citet{Rhodes2020} and \citet{choi2022densityratio} and also provide further experiments.  

We mainly compare with the TSM objective \citep{choi2022densityratio}, as it was shown to outperform baseline methods like NCE \citep{gutmann2012nce} and TRE \citep{Rhodes2020}. Unless otherwise specified, we use the same score network, VP path, and experimental setup as in~\citet{choi2022densityratio}.
In these experiments, the TSM estimator is obtained using Stein score normalization as in~\citet{choi2022densityratio}, while our CTSM estimators are always obtained using time score normalization; both weighting functions were defined in Section~\ref{sec:design_choices}. 
In fact, we consider time score normalization an integral part of the CTSM method instead of an optional add-on, and thus do not evaluate its effect separately.
Details on experiments are specified in  Appendix~\ref{app:sec:exp}.


Overall, these experiments show that vectorized CTSM achieves a similar performance to TSM but is orders of magnitude faster, especially in higher dimensions. We note the importance of our vectorized CTSM, as in preliminary experiments, the non-vectorized CTSM is essentially not trainable on MNIST.

\subsection{Evaluation Metrics} 
We follow the metrics established by prior work on density ratio estimation~\citep{Rhodes2020,choi2022densityratio}.

\paragraph{Mean-Squared Error of the density ratio.}
As a basic measure of estimation error, we approximate the following quantity $\E_{q(x)} \| \log \frac{p_1}{p_0}(x) - \widehat{\log \frac{p_1}{p_0}}(x) \|^2$ using Monte-Carlo. The distribution $q(x)$ is chosen to be the mixture $\frac{1}{2} p_0 + \frac{1}{2} p_1$ as in the implementation of~\citet{choi2022densityratio}.

\paragraph{Log-likelihood of the target distribution.} As a second measure of success, we approximate the following quantity $-\E_{p_1(\bx)}[\widehat{\log p_1}(\bx)]$  using Monte-Carlo. We report the result in bits per dimension (BPD), obtained by taking the negative log-likelihood and then dividing by the dimensionality of the data while reported in bits. 
    
We note that the metric of log-likelihood should be interpreted with caution. While commonly reported in related literature~\citep{gao2019noiseadaptivence,Rhodes2020,choi2022densityratio,du2023energybasedmodel}, that same literature acknowledges that it is specifically designed to measure the likelihood of a normalized model. A model obtained through density-ratio estimation is only normalized in the limit of infinite samples and perfect optimization, meaning it may remain unnormalized in practice. In such cases, BPD becomes invalid because unnormalized models introduce an additive constant that distorts the BPD value.
Some literature attempts to address this by re-normalizing the learned model using estimates of the log normalizing constant~\citep{gao2019noiseadaptivence,Rhodes2020,choi2022densityratio,du2023energybasedmodel}.
However, our experiments show these estimates can be unreliable and may even worsen the unnormalization. For example, the Annealed Importance Sampling estimator~\citep{neal1998ais} produces highly variable log normalizing constants (e.g., ranging between $[-1100, 650]$ depending on the step size in the sampling method). Similarly, the Reverse Annealed Importance Sampling Estimator~\citep{burda2015raise} can be numerically unstable for realistic distributions, such as mixtures~\citep{du2023energybasedmodel}. 


\subsection{Model Accuracy in Synthetic Distributions with High Discrepancies}

We consider synthetic data where two distributions have high discrepancies; this type of problem is considered in previous works \citep{choi2022densityratio} as it highlights the challenge of the density-chasm problem \citep{Rhodes2020}. For a fair comparison, we use the same model architecture, the same interpolation scheme and train for the same number of steps while tuning the learning rates for each scenario. 

\paragraph{Gaussians} 
Consider two distant Gaussians,
\begin{align}
    p_0(\bx) 
    &= 
    \mathcal{N}(\bx; [0, \ldots, 0]^\top,  \mI),
    \\
    p_1(\bx) 
    &= 
    \mathcal{N}(\bx; [4, \ldots, 4]^\top, \mI)
\end{align}
with varying dimensionality. Their density ratio is modeled by a fully-connected neural network ending with a linear layer. Results are reported in Figure~\ref{fig:gaussians}. We observe that our CTSM methods consistently improve upon TSM in terms of accuracy for the same number of iterations of the optimization algorithm. Moreover, a single iteration of the optimization algorithm is more than two times faster for our methods than for TSM: CTSM and CTSM-v take around $5$ms per iteration, against around $15$ms for TSM~\footnote{For this experiment, \citet{choi2022densityratio}'s implementation of TSM had a bug (see Appendix~\ref{app:sec:bug_tsm_toy}), thus the results that we report are better than the ones in their paper.}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.5\columnwidth]{image/gaussian.pdf}
\caption{
For estimating the density ratio between two Gaussians, CTSM-v outperforms other methods as the dimensionality increases. Full and shaded lines are respectively the means and standard deviations over $3$ runs. 
}
\label{fig:gaussians}
\end{center}
\end{figure}

\paragraph{Gaussian mixtures}
Consider two bi-modal Gaussian mixtures, centered at vectors of entries $\mathbf{2}$ and $\mathbf{-2}$,
\begin{align}
    p_0 
    &= 
    \frac{1}{2} 
    \mathcal{N}(\mathbf{2} - \frac{k\sigma}{2}, \sigma^{2}\mI)
    +
    \frac{1}{2} 
    \mathcal{N}(\mathbf{2} + \frac{k\sigma}{2}, \sigma^{2}\mI)
    \\
    p_1 
    &= 
    \frac{1}{2} 
    \mathcal{N}(-\mathbf{2} - \frac{k\sigma}{2}, \sigma^{2}\mI)
    +
    \frac{1}{2} 
    \mathcal{N}(-\mathbf{2} + \frac{k\sigma}{2}, \sigma^{2}\mI),
\end{align}
with $\sigma=\sqrt{\frac{4}{4+k^{2}}}$. We choose the distribution in this way, such that $k$ controls the between-mode distance as a multiple of $\sigma$, while either side has unit variance in each dimension.

In this experiment specifically, the default VP path~\eqref{eq:vp_path_simulation} cannot be used because $p_0$ is not Gaussian. We therefore use another path specified in Appendix~\ref{app:ssec:schrodinger_bridge_path}.

Results are reported in Appendix~\ref{app:sec:additional_experimental_results}. We observe that CTSM and CTSM-v are, again, significantly faster to run than TSM, while being able to achieve competitive performances within the same number of iterations.

\subsection{Mutual Information Estimation for High-Dimensional Gaussians}

Following \citet{Rhodes2020,choi2022densityratio}, we conduct an experiment where the goal is to estimate the mutual information between two high dimensional Gaussian distributions
\begin{align}
    p_0(\vx) 
    = 
    \mathcal{N}(\bx; \mathbf{0}, \mI)
    , \quad
    p_1(\vx) 
    = 
    \mathcal{N}(\bx; \mathbf{0}, \mSigma),
\end{align}
where $\mSigma$ is a structured matrix; specifically it is block-diagonal, where each block is $2 \times 2$ with $1$ on the diagonal and $0.8$ on the off-diagonal, thus making the ground truth MI a function of dimensionality. Their density ratio defines the mutual information between two random variables, $\vx$ restricted to even indices and $\vx$ restricted to odd indices, as explained in~\citet[Appendix D]{Rhodes2020}. Also following \citet{Rhodes2020,choi2022densityratio}, we directly parameterize a quantity related to the covariance; further details can be found in Appendix~\ref{sec:mi-estimation}.

Estimating the mutual information is a difficult task in high dimensions. Yet, as noted by \citet{choi2022densityratio}, TSM can efficiently do so. As shown in Figure~\ref{fig:mi} (right panel), all methods --- TSM, CTSM and CTSM-v --- can estimate the mutual information accurately after a sufficiently large number of optimization steps. However, CTSM-v is orders of magnitude faster to converge in terms of optimization step. What is more, each optimization step is consistently faster for CTSM and CTSM-v than TSM, and this effect is exacerbated in higher dimensions, as seen in Figure~\ref{fig:mi} (left panel). Overall, when running these methods with a fixed compute budget,   CTSM-v outperforms both CTSM and TSM, as seen in Figure~\ref{fig:mi} (middle panel). 

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.75\columnwidth]{image/mi.pdf}}
\caption{Mutual information estimation.
\textit{Left:} Time per iteration. 
\textit{Middle:} Estimated and true (in dashed black) Mutual Information for different dimensions, where we directly report the estimates obtained after a few thousand iterations (see Appendix, Table~\ref{tbl:mi-hyper}). 
\textit{Right:} Error between the estimated and true mutual information for dimensionality $320$, during the first steps of optimization. Full and shaded lines are respectively the means and standard deviations over $3$ runs.
}
\label{fig:mi}
\end{center}
\vskip -0.2in
\end{figure}


\subsection{Energy-based Modeling of Images}

\begin{table}[h]
\begin{center}
\caption{EBM results on MNIST. Training is done in a latent space obtained using a pre-trained Gaussian normalizing flow. CTSM-v can achieve comparable results as TSM, while being much faster. For BPD lower is better.}
\begin{tabular}{|c|c|c|}
  \hline
  Methods &
  Direct BPD &
  Time per step 
  \\
  \hline
  TSM~\citep{choi2022densityratio}
   & 1.33 & not reported \\
  \hline
  TSM (our reproduction) & 1.30 & 347 ms \\
  \hline
  CTSM-v  & 1.26 & 58 ms \\
  \hline
\end{tabular}
\end{center}
\end{table}

Similar to \citet{Rhodes2020} and \citet{choi2022densityratio}, we consider Energy-based Modeling (EBM) tasks on MNIST \citep{lecun2010mnist}.
Here, we have
\begin{align}
    p_0(\bx) 
    = 
    \mathcal{N}(\bx; \mathbf{0}, \mI),
    \quad
    p_1(\bx) 
    = 
    \pi(\bx)
    ,
\end{align}
where $\pi(\bx)$ is a distribution over images of digits. These images  are actually mapped back to an (approximately) normal distribution using a pre-trained neural network (multivariate Gaussian normalizing flow). 

We note that in practice, CTSM could not be used for this task.
Hence, we compare CTSM-v with TSM. To model the vectorized time score used in CTSM-v,  we use the same, small U-Net architecture as in~\citet{choi2022densityratio}, with one modification: to condition the network on time, we use popular Fourier feature embeddings~\citep{tancik2020randomfourier,song2021sde} instead of linear embeddings as in~\citet{choi2022densityratio}. Preliminary experiments showed this led to more stable training and better final performance. 

Based on preliminary experiments, we employ importance sampling to adjust the effective weighting scheme. For the implementation of the TSM loss, we directly use the original code as provided by \citet{choi2022densityratio}. We remark that the exact speed naturally depends on both the score matching algorithm and implementation details, and in our case may also depend on the way that the flow is utilized; for details we refer readers to Section~\ref{app:sec:ebm}.

We observe that, CTSM objective can train models competitive to TSM, while being much faster. Annealed Importance Sampling, which has been used by previous works to verify the estimated log densities \citep{Rhodes2020,choi2022densityratio}, appears to be highly unstable for time score matching algorithms, with the estimated log constants varying significantly depending on the step size of HMC algorithm.














