
\appendix
\clearpage

\section*{Appendix}

The paper and appendix are organized as follows.

\renewcommand{\contentsname}{}
\vspace{-1cm}
\tableofcontents
\newpage


\section{Useful Identities}
\label{app:sec:identities}

We here list useful identities that will be used to prove subsequent results. 


\begin{lemma}[Variance of a specific random variable]
\label{lemma:variance_random_var}
Consider two independent random variables, $\bepsilon \sim \mathcal{N}(\bzero, \mI)$ and $\vx$ with mean $\bmu$ and covariance $\mSigma$. Then for scalars $a, b \in \R$,
\begin{align}
    \mathrm{Var}[
    a \norm{\bepsilon}^{2} + b\bepsilon^{\top}\bx
    ]
    =
    2 a^{2}D + b^{2}c D
\end{align}
where $c = (\text{Trace}\left(\mSigma\right)+ \norm{\bmu}^{2})/D$ depends on the first two moments of $\bx$ and on the dimensionality $D$.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:variance_random_var}]
$\norm{\bepsilon}^{2}$ follows a $\chi^{2}_{D}$-distribution, which has mean $D$ and variance $2D$.
\begin{align}
\E\left[\norm{\bepsilon}^{4}\right] &= \text{Var}\left[\norm{\bepsilon}^{2}\right] + \E\left[\norm{\bepsilon}^{2}\right]^{2} = 2D+D^{2},\\
\E\left[\bepsilon^{\top}\bx\right] &= \E\left[\sum_{i}\epsilon_{i}x_{i}\right] = \sum_{i}\E\left[\epsilon_{i}\right]\E\left[x_{i}\right]=0,\\
\E\left[x_{i}^{2}\right] &= \text{Var}\left[x_{i}\right]+\left(\E\left[x_{i}\right]\right)^{2}=\Sigma_{ii}+\mu_{i}^{2},\\
\E\left[\left(\bepsilon^{\top}\bx\right)^{2}\right] &= \E\left[\sum_{i,j}\epsilon_{i}x_{i}\epsilon_{j}x_{j}\right] = \sum_{i,j}\E\left[\epsilon_{i}x_{i}\epsilon_{j}x_{j}\right]= \sum_{i}\E\left[\epsilon_{i}^{2}x_{i}^{2}\right]\\
&= \sum_{i}\E\left[\epsilon_{i}^{2}\right]\E\left[x_{i}^{2}\right] = \sum_{i}\left(\Sigma_{ii}+\mu_{i}^{2}\right) = \text{Tr}(\bSigma) + \norm{\bmu}^{2},\\
\text{Var}\left[\bepsilon^{\top}\bx\right] &= \E\left[\left(\bepsilon^{\top}\bx\right)^{2}\right] - \left(\E\left[\bepsilon^{\top}\bx\right]\right)^{2}=\E\left[\left(\bepsilon^{\top}\bx\right)^{2}\right] = \text{Tr}(\bSigma) + \norm{\bmu}^{2},\\
\E\left[\norm{\bepsilon}^{2}\bepsilon^{\top}\bx\right] &= \E\left[\left(\sum_{i}\epsilon_{i}^{2}\right)\sum_{j}\epsilon_{j}x_{j}\right] = \E\left[\sum_{j}\epsilon_{j}^{3}x_{j}\right] + \E\left[\left(\sum_{i\neq j} \epsilon_{i}^{2}\right)\sum_{j}\epsilon_{j}x_{j}\right]\\
&= \left(\sum_{j}\E\left[\epsilon_{j}^{3}\right]\right)\E\left[x_{j}\right] + \E\left[\sum_{i\neq j} \epsilon_{i}^{2}\right]\sum_{j}\E\left[\epsilon_{j}\right]\E\left[x_{j}\right]=0,\\
\text{Var}\left[a\norm{\bepsilon}^{2} + b\bepsilon^{\top}\bx\right] &= \E\left[\left(a\norm{\bepsilon}^{2} + b\bepsilon^{\top}\bx\right)^{2}\right] - \left(\E\left[a\norm{\bepsilon}^{2} + b\bepsilon^{\top}\bx\right]\right)^{2}\\
&= \E\left[a^{2}\norm{\bepsilon}^{4} + 2ab\norm{\bepsilon}^{2}\bepsilon^{\top}\bx + b^{2}\left(\bepsilon^{\top}\bx\right)^{2}\right] - \left(\E\left[a\norm{\bepsilon}^{2} + b\bepsilon^{\top}\bx\right]\right)^{2}\\
&= a^{2}\left(2D+D^{2}\right) + 0 +b^{2}\left(\text{Tr}(\bSigma)+\norm{\bmu}^{2}\right) - \left(aD\right)^{2} = 2a^{2}D + b^{2}\left(\text{Tr}(\bSigma)+\norm{\bmu}^{2}\right)\\
&= 2a^{2}D + b^{2}cD.
\end{align}
\end{proof}


\newpage
\section{Probability Paths}
\label{app:sec:mixture}

\paragraph{Closed-form estimator of time score}

\paragraph{Definition} 
In this paper, we consider probability paths $p_t(\bx)$ that are explicitly decomposed as a mixture of simpler probability paths $p_t(\bx | \bz)$, where $\bz$ indexes the mixture. Formally, this is written as
\begin{align}
    p_{t}(\bx) 
    =
    \E_{p(\vz)}[p_{t}(\bx|\bz)]
    = 
    \mathcal{N}(\bx ; \bmu_{t}(\bz),k_{t}\bI).
\end{align}

The conditional paths are chosen to be Gaussian $p_{t}(\bx|\bz) = \mathcal{N}(\bx ; \bmu_{t}(\bz),k_{t}\bI)$. We will specify popular choices of $\bz$, $\bmu_t(\bz)$, and $k_t$ in Sections~\ref{app:ssec:vp_path} and~\ref{app:ssec:schrodinger_bridge_path}. 
 

\paragraph{Time score}
The time score is
\begin{align}
    \label{eq:time_score_gaussian_case}
    \partial_t \log p_t(\bx | \bz) 
    =
    \frac{-D \dot{k}_t}{2 k_t}
    +
    \frac{1}{\sqrt{k_{t}}}\dot{\bmu}_t^\top \bepsilon_t(\bx, \bz)
    +
    \frac{\dot{k}_t }{2 k_{t}} 
    \norm{\bepsilon_t(\bx, \bz)}^2
    , \qquad
    \bepsilon_t(\bx, \bz) = \frac{1}{\sqrt{k_t}} (\bx - \bmu_t(\vz)).
\end{align}

In fact, we can formally write the time score without the conditioning variable,
\begin{align}
    \partial_t \log p_t(\bx)
    = 
    \E_{p_t(\bz | \bx)}[
    \partial_t \log p_t(\bx | \bz)
    ]
    , \quad
    p_t(\bz | \bx) 
    \propto 
    p(\bz)
    \exp\bigg(
    -\frac{1}{2 k_{t}} \norm{\bx-\bmu_{t}(\bz)}^{2}
    \bigg).
\end{align}

\paragraph{Stein score}
The Stein score is \citep{kingma2023diffusion}
\begin{align}
    \label{eq:stein_score_gaussian_case}
    \partial_{\bx} \log p_t(\bx | \bz)
    =
    -\frac{1}{\sqrt{k_t}}
    \bepsilon_t(\bx, \bz)
    , \qquad
    \bepsilon_t(\bx, \bz) = \frac{1}{\sqrt{k_t}} (\bx - \bmu_t(\vz)).
\end{align}

\paragraph{Stein score normalization}

Observe that for a fixed $t$, $\bepsilon$ is, by definition, sampled from a standard normal distribution. As such, the Stein score in Equation~\eqref{eq:stein_score_gaussian_case} has variance $\frac{1}{k_{t}}$. The Stein score normalization in~\eqref{eq:path_var} is therefore given by
\begin{align}
     \lambda(t) 
     \propto 
     k_{t}.
\end{align}



\subsection{Variance-Preserving Probability Path}
\label{app:ssec:vp_path}

\paragraph{Simulating the path}
This path is simulated by interpolating the random variables $(\bx_0, \bx_1) \sim p_0 \otimes p_1$,
\begin{align}
    \bx
    = 
    \alpha_{t} \bx_1 
    +
    \sqrt{1 - \alpha_t^2} \bx_0.
\end{align}

\paragraph{Definition}
Conditioning on $t$ and $\bz = \bx_1$, and choosing a Gaussian reference distribution $p_0(\bx) = \mathcal{N}(\bx; 0, I)$, yields
\begin{align}
    \bmu_t(\bz) = \alpha_t \bx_1,
    \quad
    k_t = 1 - \alpha_t^2.
\end{align}
These choices define a popular probability path, sometimes called ``variance-preserving" as the variance of $p_t(\vx)$ is constant for all $t \in [0, 1]$~\citep{sohl-dickstein2015deep,ho2020ddpm,song2021sde,lipman2023conditionalflowmatching}. This path is in fact the default choice in the work most related to ours~\citep{choi2022densityratio}. In the above, $\alpha_t$ is positive and increasing, such that $\alpha_0 = 0$ and $\alpha_1 = 1$. It is sometimes referred to as the noise schedule~\citep{chen2023noisescheduling}. Popular choices include exponential $\alpha_t = \min(1, e^{-2(T-t)})$~\citep{song2021sde} for some fixed $T \geq 0$, or linear $\alpha_t = \min(1, t)$ functions~\citep{Albergo2023,gao2023gaussianinterpolant}.

We remark that in diffusion models literature \citep{song2021sde}, $p_{0}$ denotes data and $p_{1}$ denotes noise. We follow the flow matching convention, and use $p_{0}$ to denote noise and $p_{1}$ to denote data.


\paragraph{Time score}
The resulting time score from~\eqref{eq:time_score_gaussian_case} is
\begin{align}
    \partial_{t}\log p_{t}(\bx|\bz)
    &= 
    D\frac{\alpha_{t}\alpha'_{t}}{1-\alpha_{t}^{2}} - 
    \frac{\alpha_{t}\alpha'_{t}}{\left(1-\alpha_{t}^{2}\right)^{2}}\norm{\bx-\alpha_{t}\bx_{1}}^{2}
    +
    \frac{1}{1-\alpha_{t}^{2}}\left(\bx-\alpha_{t}\bx_{1}\right)^{\top}\alpha'_{t}\bx_{1}\\
    &=D\frac{\alpha_{t}\alpha'_{t}}{1-\alpha_{t}^{2}} - \frac{\alpha_{t}\alpha'_{t}}{1-\alpha_{t}^{2}}\norm{\bepsilon}^{2} + \frac{1}{\sqrt{1-\alpha_{t}^{2}}}\bepsilon^{\top}\alpha'_{t}\bx_{1}.
\end{align}

\paragraph{Stein score}
The resulting Stein score from~\eqref{eq:stein_score_gaussian_case} is
\begin{align}
    \partial_{\bx}\log p(\bx|\bz) 
    =
    -\frac{1}{\sqrt{1-\alpha_{t}^{2}}}\bepsilon_{t}(\bx,\bz).
\end{align}


\paragraph{Time score normalization}
We have
\begin{align}
    \mathrm{Var}_{p_t(\bz, \bx)}[
    \partial_{t}\log p_{t}(\bx|\bz)
    ]
    = \frac{
    2\alpha_{t}^{2}\left(\alpha'_{t}\right)^{2}
    +
    \left(\alpha'_{t}\right)^{2}\left(1-\alpha_{t}^{2}\right)c
    }
    {
    \left(1-\alpha_{t}^{2}\right)^{2}
    }.
\end{align}
where $c = (\text{Trace}\left(\mSigma\right)+ \norm{\bmu}^{2})/D$ depends on the mean $\bmu$, covariance $\mSigma$ and dimensionality $D$ of $\bx$. 

To compute the variance of the time score, observe that the first term is deterministic and therefore does not participate in the computation of the variance. To obtain the variance of the two remaining terms, we apply Lemma~\ref{lemma:variance_random_var} with $a=-\frac{\alpha(t)\alpha'(t)}{1-\alpha(t)^{2}}$ and $b=\frac{1}{\sqrt{1-\alpha(t)^{2}}}$. 

Interestingly, the variance can explode $\mathrm{Var}\left[\partial_{t}\log p_{t}(\bx|\bz)\right] \rightarrow \infty$ near the target distribution $\alpha(t) \rightarrow 1$.





\subsection{Schrödinger Bridge Probability Path}
\label{app:ssec:schrodinger_bridge_path}

\paragraph{Simulating the path}
This path is simulated by interpolating the random variables $(\bx_0, \bx_1) \sim \pi(\bx_0, \bx_1)$, generated from a coupling $\pi$ of the marginals $p_0$ and $p_1$, and adding Gaussian noise $\bepsilon \sim \mathcal{N}(\bzero, \mI)$ between the endpoints,
\begin{align}
    \bx
    = 
    t \bx_1 
    +
    (1 - t) \bx_0
    +
    \sigma \sqrt{t (1 - t)}
    \bepsilon.
\end{align}
\paragraph{Definition}
Conditioning on $t$ and $\bz = (\bx_1, \bx_0)$, yields
\begin{align}
    \mu_t(z) = (1 - t) \bx_0 + t \bx_1,
    \quad
    k_t = t (1 - t) \sigma^{2}.
\end{align}
These choices define another path of distributions in the literature. Typically, the coupling from which $\bz$ is drawn is either the product distribution $p_0 \otimes p_1$ or a coupling $\pi$ that satisfies optimal transport.  In the latter case, the ensuing path is known as a Schrödinger bridge~\citep{Follmer1988,tong2024schrodingerbridge}. In practice, the optimal transport coupling can be approximated using limited samples from both $p_0$ and $p_1$~\citep{pooladian2023conditionalflowmatching,tong2024conditionalflowmatching}. For simplicity, we use the product distribution. Note that for this path, $p_0$ need not be a Gaussian. 

When using independent couplings with $p_{0}$ and $p_{1}$ having equal variance $var$, arguably the most natural choice of $\sigma$ is to set $\sigma = \sqrt{2 var}$. In this case, the variance is preserved along the path. In order to see that, observe that under this setting the variance of $\bx_{t}$ is given by
\begin{equation*}
t^{2}var + (1-t)^{2}var + 2t(1-t)var = var.
\end{equation*}
However, empirically one may achieve better results with other choices of $\sigma$.

\paragraph{Time score}
The resulting time score from~\eqref{eq:time_score_gaussian_case} is
\begin{align}
    \partial_{t}\log p_{t}(\bx|\bz)
    &= 
    -\frac{1}{2}D\frac{1-2t}{t(1-t)} + \frac{1-2t}{2\left(t(1-t)\right)^{2}\sigma^{2}}\norm{\bx-\left(1-t\right)\bx_{0}-t\bx_{1}}^{2}\\
&\quad +\frac{1}{t(1-t)\sigma^{2}}\left(\bx-\left(1-t\right)\bx_{0}-t\bx_{1}\right)^{\top}\left(\bx_{1}-\bx_{0}\right)\\
&= -\frac{1}{2}D\frac{1-2t}{t(1-t)} + \frac{1-2t}{2t(1-t)}\norm{\bepsilon}^{2} +\frac{1}{\sqrt{t(1-t)}\sigma}\bepsilon^{\top}\left(\bx_{1}-\bx_{0}\right).
\end{align}

\paragraph{Stein score}
The resulting Stein score from~\eqref{eq:stein_score_gaussian_case} is
\begin{align}
    \partial_{\bx}\log p(\bx|\bz) 
    =
    -\frac{1}{\sigma\sqrt{t(1-t)}}\bepsilon_{t}(\bx,\bz).
\end{align}

\paragraph{Time score normalization}
To compute that variance, treat $\frac{\bx_{1}-\bx_{0}}{\sigma}$ as a random variable with mean $\bmu$ and covariance $\bSigma$, we observe that, similar to VP path, it can be written as $a=\frac{1-2t}{2t(1-t)}$ and $b=\frac{1}{\sqrt{t(1-t)}}$.

We have
\begin{align}
    \mathrm{Var}_{p_t(\bz, \bx)} [\partial_{t}\log p_{t}(\bx|\bz)]
    =     
    \frac{1-4t+4t^{2}+2ct-2ct^{2}}{2t^{2}(1-t)^{2}}D.
\end{align}

Note that as $t$ approaches $0$ or $1$, the variance may be infinite. 







\newpage
\section{Weighting Scheme}
\label{app:sec:weighting}









\subsection{Details on Importance Sampling}
\label{app:sec:importance-sampling}

We consider the simple VP path, given by $\bx = t\bx_{1} + \sqrt{1-t^{2}}\bx_{0}$, where $\bx_{0}$ is standard Gaussian, and $\bx_{1}$ is a distribution with $c=1$. One divided by the time score normalization is given by $\frac{1+t^{2}}{\left(1-t^{2}\right)^{2}}$. Treating this as an unnormalized probability density defined between $0$ and $t_{1}$, one can derive that the normalization constant is given by $Z=\frac{t_{1}}{1-t_{1}^{2}}$, and the CDF is given by $y(t) = \frac{1}{Z}\frac{t}{1-t^{2}}$. We can calculate the inverse CDF as
\begin{equation}
\frac{-1+\sqrt{1+4y^{2}Z^{2}}}{2yZ} = \frac{2yZ}{\sqrt{1+4y^{2}Z^{2}}+1}.
\end{equation}
As such, we can draw samples between $0$ and $t_{1}$ using the inverse CDF transform. Re-normalize $t_{1}$ to lie between $0$ and $1-\eps$ yields the final samples.

In practice, we choose $t_{1}=0.9$, and employ this heuristic scheme for EBM experiments as well, though we are using a different variant of the VP path.

\section{Theoretical Results}
\label{app:sec:theory}

\subsection{Proof of ~\eqref{eq:mixture_score}}
\label{app:sec:mixture_score}

\begin{proof}[Proof of ~\eqref{eq:mixture_score}]
The derivations are similar to denoising score matching \citep{vincent2011denoisingscorematching,Bortoli2024}.

We wish to relate the time score $\partial_t \log p_t(\bx)$ and the conditional time score $\partial_t \log p_t(\bx | \bz)$. 

We have
\begin{align}
p_{t}(\bx) 
&= 
\int p_{t}(\bx|\bz)p(\bz)\dd\bz,
\\
\partial_{t} p_{t}(\bx) 
&= 
\int \partial_{t} p_{t}(\bx|\bz)p(\bz)\dd\bz =
\int \partial_{t} \log p_{t}(\bx|\bz) p_t(\bx | \bz) p(\bz)\dd\bz,
\end{align}
therefore
\begin{align}
\partial_{t} \log p_{t}(\bx) &= 
\frac{\partial_t p_t(\bx)}
{p_t(\bx)}
=
\int \partial_{t} \log p_{t}(\bx|\bz) \frac{p_{t}(\bx|\bz)p(\bz)}{p_{t}(\bx)} \dd\bz
=
\int \partial_{t} \log p_{t}(\bx|\bz) p_t(\bz | \bx) \dd\bz.
\end{align}
\end{proof}









\subsection{Proofs of Theorems~\ref{theorem:ctsm_objective}, \ref{theorem:ctsm_v_objective} and~\ref{theorem:marginal_vs_condition_regression}}
\label{app:sec:theorem3_proof}

We note that Theorems~\ref{theorem:ctsm_objective} and~\ref{theorem:ctsm_v_objective} are special cases of Theorem~\ref{theorem:marginal_vs_condition_regression}. 

For Theorem~\ref{theorem:ctsm_objective}, $\bbf(\bx,t|\bz)= \partial_{t}\log p_{t}(\bx|\bz)$, in which case $\bg(\bx,t)= \E_{p_{t}(\bz|\bx)}\left[\partial_{t}\log p_{t}(\bx|\bz)\right] = \partial_{t}\log p_{t}(\bx)$, i.e. the time score itself. 

For Theorem~\ref{theorem:ctsm_v_objective}, $\bbf(\bx,t|\bz)=\text{vec}(\partial_{t}\log p_{t}(\bx|\bz))$, in which case $\bg(\bx,t) = \E_{p_{t}(\bz|\bx)}\left[\text{vec}(\partial_{t}\log p_{t}(\bx|\bz))\right]$. It is clear that 
\begin{equation*}
\sum_{i}\E_{p_{t}(\bz|\bx)}\left[\partial_t \log p_t(x^i | \bx^{<i}, \bz)\right]=\E_{p_{t}(\bz|\bx)}\left[\sum_{i}\partial_t \log p_t(x^i | \bx^{<i}, \bz)\right] = \E_{p_{t}(\bz|\bx)}\left[\partial_{t}\log p_{t}(\bx|\bz)\right] = \partial_{t}\log p_{t}(\bx),
\end{equation*}
i.e. the sum of $\bg(\bx,t)$ gives the time score.

We prove Theorem~\ref{theorem:marginal_vs_condition_regression} in what follows.

\begin{proof}[Proof of Theorem~\ref{theorem:marginal_vs_condition_regression}]

The derivations are similar to \citet{lipman2023conditionalflowmatching,tong2024conditionalflowmatching}. First, we compute the gradients of both cost functions, $J_{\bg}$ and $J_{\bbf}$.
\begin{align}
\nabla_{\btheta}J_{\bg}(\btheta) &= \nabla_{\btheta}\E_{p(t),p_{t}(\bx)}\left[\lambda(t)\norm{\bg(\bx,t) - \bs_{\btheta}(\bx,t)}^2\right]\\
&= \nabla_{\btheta}\E_{p(t),p_{t}(\bx)}\left[\lambda(t)\left(\norm{\bg(\bx,t)}^{2} - 2 \left\langle\bg(\bx,t),\bs_{\btheta}(\bx,t)\right\rangle + \norm{\bs_{\btheta}(\bx,t)}^{2}\right)\right]\\
&= \nabla_{\btheta}\E_{p(t),p_{t}(\bx)}\left[\lambda(t)\left( - 2 \left\langle\bg(\bx,t),\bs_{\btheta}(\bx,t)\right\rangle + \norm{\bs_{\btheta}(\bx,t)}^{2}\right)\right].
\end{align}
\begin{align}
\nabla_{\btheta}J_{\bbf}(\btheta) &= \nabla_{\btheta}\E_{p(t),p(\bz),p_{t}(\bx|\bz)}\left[\lambda(t)\norm{\bbf(\bx,t|\bz) - \bs_{\btheta}(\bx,t)}^2\right]\\
&= \nabla_{\btheta}\E_{p(t),p(\bz),p_{t}(\bx|\bz)}\left[\lambda(t)\left(\norm{\bbf(\bx,t|\bz)}^{2} - 2 \left\langle\bbf(\bx,t|\bz), \bs_{\btheta}(\bx,t)\right\rangle + \norm{\bs_{\btheta}(\bx,t)}^{2}\right)\right]\\
&= \nabla_{\btheta}\E_{p(t),p(\bz),p_{t}(\bx|\bz)}\left[\lambda(t)\left(- 2 \left\langle\bbf(\bx,t|\bz),\bs_{\btheta}(\bx,t)\right\rangle + \norm{\bs_{\btheta}(\bx,t)}^{2}\right)\right].
\end{align}
We then proceed to show that the two terms coincide:
\begin{align}
\E_{p_{t}(\bx)} \norm{\bs_{\btheta}(\bx,t)}^{2} &= \E_{p(\bz)p_{t}(\bx|\bz)} \norm{\bs_{\btheta}(\bx,t)}^{2},\\
\bg(\bx,t) &= \int \frac{p_{t}(\bx|\bz)p(\bz)}{p_{t}(\bx)} \bbf(\bx,t|\bz)\dd\bz,\\
\E_{p_{t}(\bx)}\left\langle \bg(\bx,t), \bs_{\btheta}(\bx,t) \right\rangle &= \E_{p_{t}(\bx)} \left\langle \int \frac{p_{t}(\bx|\bz)p(\bz)}{p_{t}(\bx)} \bbf(\bx,t|\bz)\dd\bz,\bs_{\btheta}(\bx,t) \right\rangle\\
&= \int \left\langle \int \frac{p_{t}(\bx|\bz)p(\bz)}{p_{t}(\bx)} \bbf(\bx,t|\bz)\dd\bz,\bs_{\btheta}(\bx,t) \right\rangle p_{t}(\bx) \dd\bx\\
&= \int \left\langle \int p_{t}(\bx|\bz)p(\bz) \bbf(\bx,t|\bz)\dd\bz,\bs_{\btheta}(\bx,t) \right\rangle \dd\bx\\
&= \int\int \left\langle\bbf(\bx,t|\bz),\bs_{\btheta}(\bx,t) \right\rangle p_{t}(\bx|\bz)p(\bz) \dd\bz \dd\bx.
\end{align}
\end{proof}

\subsection{Proof of Theorem~\ref{theorem:error_bound}}

\begin{proof}[Proof of Theorem~\ref{theorem:error_bound}]

We have
\begin{align}
    \mathrm{KL}(p_1, \hat{p}_1)^2
    &= 
    \left(
    \E_{p_1(\bx)} \left[
    \log p_1(\bx) - \log \hat{p}_1(\bx)
    \right]
    \right)^2
    \\
    &\leq
    \E_{p_1(\bx)} \left[
    (\log p_1(\bx) - \log \hat{p}_1(\bx))^2
    \right]
    \\
    &=
    \E_{p_1(\bx)} \left[
    \left(
    \int_0^1 s(\bx, t)dt - 
    \frac{1}{K}\sum_{i=1}^K \hat{s}(\bx, t_i)
    \right)^2
    \right]
    \\
    &=
    \E_{p_1(\bx)} \left[
    \left(
    \int_0^1 s(\bx, t)dt - 
    \frac{1}{K}\sum_{i=1}^K s(\bx, t_i)
    +
    \frac{1}{K}\sum_{i=1}^K s(\bx, t_i)
    -
    \frac{1}{K}\sum_{i=1}^K \hat{s}(\bx, t_i)
    \right)^2
    \right]
    \\
    &\leq
    \E_{p_1(\bx)} \left[
    2
    \left(
    \int_0^1 s(\bx, t)dt 
    - 
    \frac{1}{K}\sum_{i=1}^K s(\bx, t_i)
    \right)^2
    +
    2
    \left(
    \frac{1}{K}\sum_{i=1}^K s(\bx, t_i)
    -
    \frac{1}{K}\sum_{i=1}^K \hat{s}(\bx, t_i)
    \right)^2
    \right]
    \\
    &\leq
    \E_{p_1(\bx)} \left[
    2
    \left(
    \frac{L(\bx)}{2K}
    \right)^2
    +
    2
    \frac{1}{K}\sum_{i=1}^K \left( 
    s(\bx, t_i) - \hat{s}(\bx, t_i)
    \right)^2
    \right]
    \\
    &=
    \frac{1}{2K^2}\E_{p_1(\bx)}[L(\bx)^2]
    +
    2
    \E_{p_1(\bx), p_K(t)} \left[
    \left(
    s(\bx, t) - \hat{s}(\bx, t)
    \right)^2
    \right],
\end{align}
where we used Jensen's inequality and bound the discretization error of a Riemannian integral using the left rectangular sum.

\end{proof}


\subsection{Proof of Proposition~\ref{proposition:error_bound_scores}}

\begin{proof}[Proof of Proposition~\ref{proposition:error_bound_scores}]

Denote by $s(\bx, \bz, t) = \partial_{t}\log p_{t}(\bx|\bz)$ 
the conditional score and by $l_{\theta}(\bx, \bz, t) = \lambda(t) \left(
s(\bx, \bz, t) - s_{\theta}(\bx, t)
\right)^2$. The population and empirical losses defined from~\eqref{eq:l2_loss_conditional} are respectively
\begin{align}
\mathcal{L}_{\text{CTSM}}(\btheta) 
= 
\E_{p(t, \bx, \bz)} [
l_{\btheta}(\bx, \bz, t)
]
, \qquad 
\hat{\mathcal{L}}_{\text{CTSM}}(\btheta) 
= 
\frac{1}{N} \sum_{i=1}^N
l_{\btheta}(\bx_i, \bz_i, t_i).
\end{align}
where the empirical loss uses \textit{i.i.d.} samples $(\bx_i, \bz_i, t_i)_{i \in \llbracket 1, N \rrbracket}$. In the following, we suppose that the model is well-specified, which means that there exists a $\btheta^*$ that parameterizes the true score.

\paragraph{Error formulas}

First, we compute the error in the parameters. Using \citet[Section 4.7]{bach2024learningtheorybook} and \citet[Theorem 5.23]{vandervaart2000asympstats}, 
\begin{align}
\sqrt{N}(\hat{\btheta} - \btheta^*) 
\sim 
\mathcal{N}(
0,
H(\btheta^*)^{-1} G(\btheta^*) H(\btheta^*)^{-1}
),
\end{align}
where $G(\btheta^*)$ and $H(\btheta^*)$ are matrices that will be later specified. 

Then, we obtain the error in the scores, using the delta method
\begin{align}
\sqrt{N}(s_{\hat{\btheta}}(\bx, t) - s_{\btheta^*}(\bx, t))
\sim
\mathcal{N}(
0,
\nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}^\top
H(\btheta^*)^{-1}
G(\btheta^*) H(\btheta^*)^{-1}
\nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}
).
\end{align}

From there, we compute the squared error in the scores. We now specify the remainder term in the asymptotic $N \rightarrow \infty$ analysis: it is in $o(N)$ and justified  under the standard  technical conditions of~\citet[Th. 5.23]{vandervaart2000asympstats}. We write it in expectation with respect to the law of $\hat{\btheta}$, 
\begin{align}
\E_{p(\hat{\btheta})}[
(s_{\hat{\btheta}}(\bx, t) - s_{\btheta^*}(\bx, t))^2
]
&=
\frac{1}{N}
e(\vx, t, \lambda^*, \lambda, p)
+
o(N^{-1})
\end{align}
where
\begin{align}
\label{eq:squared_score_error}
e(\vx, t, \lambda^*, \lambda, p)
&=
\mathrm{trace} \big(
H(\btheta^*)^{-1}
G(\btheta^*) 
H(\btheta^*)^{-1}
\nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}
\nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}^\top
\big).
\end{align}
And then in expectation with respect to the law of $(\vx, t)$
\begin{align}
&\E_{p_1(\vx), p_K(t), p(\hat{\btheta})}[
(s_{\hat{\btheta}}(\bx, t) - s_{\btheta^*}(\bx, t))^2
]
=
\frac{1}{N} e(\btheta^*, \lambda, p)
+
o(N^{-1})
\end{align}
where
\begin{align}
\label{eq:expected_squared_score_error}
    e(\btheta^*, \lambda, p)
    &=
    \mathrm{trace} \big(
    H(\btheta^*)^{-1}
    G(\btheta^*) 
    H(\btheta^*)^{-1}
    \E_{p_1(\vx), p_K(t), p(\hat{\btheta})}[
    \nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}
    \nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}^\top
    ]
    \big)
\end{align}


\paragraph{Specifying the matrices}
The following matrices were used above: we now recall their definition, using the same notation as in~\citet[Section 4.7]{bach2024learningtheorybook}.
\begin{align}
    G(\btheta^*)
    &=
    \E_{p(t),p(\bz),p_{t}(\bx|\bz)} [
    \nabla_{\btheta} l_{\btheta}(\bx, \bz, t)|_{\btheta^*}
    \nabla_{\btheta} l_{\btheta}(\bx, \bz, t)|_{\btheta^*}^\top
    ]
    \\
    H(\btheta^*)
    &=
    \E_{p(t),p(\bz),p_{t}(\bx|\bz)} [
    \nabla^2_{\btheta} l_{\btheta}(\bx, \bz, t)|_{\btheta^*}
    ].
\end{align}

\paragraph{Case of CTSM}
We specify
\begin{align}
    \nabla_{\btheta} l_{\btheta}(\bx, \bz, t)
    &=
    -2 \lambda(t) \left(
    s(\bx, \bz, t) - s_{\btheta}(\bx, t) 
    \right) 
    \cdot
    \nabla_{\btheta} s_{\btheta}(\bx, t),
    \\
    \nabla^2_{\btheta} l_{\btheta}(\bx, \bz, t)
    &=
    2 \lambda(t) 
    \cdot
    \nabla_{\btheta} s_{\btheta}(\bx, t)
    \nabla_{\btheta} s_{\btheta}(\bx, t)^\top
    -
    2 \lambda(t) \left(
    s(\bx, \bz, t) - s_{\btheta}(\bx, t) 
    \right) 
    \cdot
    \nabla^2_{\btheta} s_{\btheta}(\bx, t)
\end{align}
and evaluate them at $\btheta^*$. To simplify notations, we write $w(\bx, \bz, t) = s(\bx, \bz, t) - s_{\btheta^*}(x, t) = \partial_t \log p_t(\bx | \bz) -  \partial_t \log p_t(\bx)$.
\begin{align}
    \nabla_{\btheta} l_{\btheta}(\bx, \bz, t)|_{\btheta^*}
    &=
    -2 \lambda(t) w(\bx, \bz, t) 
    \cdot
    \nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*},
    \\
    \nabla^2_{\btheta} l_{\btheta}(\bx, \bz, t)|_{\btheta^*}
    &=
    2 \lambda(t)
    \nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}
    \nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}^\top
    -
    2 \lambda(t) w(\bx, \bz, t)
    \cdot
    \nabla^2_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}
    .
\end{align}
Finally, this yields
\begin{align}
    \label{eq:ctsm_matrices}
    G(\btheta^*)
    &=
    4
    \E_{p(t),p(\bz),p_{t}(\bx|\bz)} \left[
    \lambda(t)^2
    w(\bx, \bz, t)^2
    \cdot
    \nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}
    \nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}^\top    
    \right]
    \\
    H(\btheta^*)
    &=
    2
    \E_{p(t),p(\bz),p_{t}(\bx|\bz)} \left[
    \lambda(t)
    \cdot
    \nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}
    \nabla_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}^\top
    -
    \lambda(t)
    \cdot
    w(\bx, \bz, t)
    \cdot
    \nabla^2_{\btheta} s_{\btheta}(\bx, t)|_{\btheta^*}
    \right].
\end{align}
\end{proof}

A sufficient condition to make the error null in~\eqref{eq:expected_squared_score_error}, is to have $w(\bx, \vz, t) = 0$.


\paragraph{Case of CTSM-v}
The derivations are largely the same.
\begin{equation}
l_{\btheta}(\bx,\bz,t) 
= 
\lambda(t) \norm{
\text{vec}(s(\bx,\bz,t)) - \text{vec}(s_{\btheta}(\bx,t))
}^{2} 
= 
\lambda(t) \sum_{i}
\left( (s(\bx,\bz,t))_{i} - s_{\btheta}(\bx,t)_{i}\right)^{2}.
\end{equation}
where $\text{vec}(s(\bx,\bz,t))_{i} := \partial_t \log p_t(x^i | \bx^{<i}, \bz)$ indicates the $i$-th component of the vector $\text{vec}(s(\bx,\bz,t))=[\partial_t \log p_t(x^i | \bx^{<i}, \bz)]_{i \in \llbracket 1, D \rrbracket}^\top$. 

All that remains to specify the error are the matrices $\mG$ and $\mH$. We have
\begin{align}
\nabla_{\btheta}l_{\btheta}(\bx,\bz,t) &= -2 \lambda(t) \sum_{i}\left(s(\bx,\bz,t)_{i} - s_{\btheta}(\bx,t)_{i}\right) \nabla_{\btheta}s_{\btheta}(\bx,t)_{i},\\
\nabla^{2}_{\btheta}l_{\btheta}(\bx,\bz,t) &= 2\lambda(t)\sum_{i}\nabla_{\btheta}s_{\btheta}(\bx,t)_{i}\nabla_{\btheta}s_{\btheta}(\bx,t)_{i}^{\top} - 2 \lambda(t) \sum_{i}\left(s(\bx,\bz,t)_{i} - s_{\btheta}(\bx,t)_{i}\right)\nabla_{\btheta}^{2}s_{\btheta}(\bx,t)_{i}.
\end{align}
We now wish to evaluate these at $\btheta^{*}$. To simplify notations, we now denote by 
$w(\bx, \bz, t)_{i} = s(\bx,\bz,t)_{i} - s_{\btheta^*}(\bx,t)_{i} = \partial_t \log p_t(x^i | \bx^{<i}, \bz) - \E_{p_{t}(\bz|\bx)}\left[\partial_t \log p_t(x^i | \bx^{<i}, \bz)\right]$.
Now we can write
\begin{align}
\nabla_{\btheta}l_{\btheta}(\bx,\bz,t) &= -2\lambda(t) \sum_{i}w(\bx,\bz,t)_{i} \nabla_{\btheta}s_{\btheta}(\bx,t)_{i}|_{\btheta^*},\\
\nabla_{\btheta}^{2}l_{\btheta}(\bx,\bz,t) &= 2\lambda(t)\sum_{i}\nabla_{\btheta}s_{\btheta}(\bx,t)_{i}|_{\btheta^{*}}\nabla_{\btheta}s_{\btheta}(\bx,t)_{i}|_{\btheta^{*}}^{\top} -2\lambda(t)\sum_{i}w(\bx,\bz,t)_{i}\nabla_{\btheta}^{2}s_{\btheta}(\bx,t)|_{\btheta^{*}}.
\end{align}

As a result, we have
\begin{align}
\label{eq:ctsm_v_matrices}
G(\btheta^{*}) &= 4\E_{p(t),p(\bz),p_{t}(\bx|\bz)}\left[\lambda(t)^{2} \left(\sum_{i}w(\bx,\bz,t)_{i} \nabla_{\btheta}s_{\btheta}(\bx,t)_{i}|_{\btheta^*}\right)^{2}\right],\\
H(\btheta^{*}) &= 2\E_{p(t),p(\bz),p_{t}(\bx|\bz)}\left[\lambda(t)\sum_{i}\nabla_{\btheta}s_{\btheta}(\bx,t)_{i}|_{\btheta^{*}}\nabla_{\btheta}s_{\btheta}(\bx,t)_{i}|_{\btheta^{*}}^{\top} -\lambda(t)\sum_{i}w(\bx,\bz,t)_{i}\nabla_{\btheta}^{2}s_{\btheta}(\bx,t)|_{\btheta^{*}}\right].
\end{align}

A sufficient condition to make the error null in~\eqref{eq:expected_squared_score_error}, is to have $w(\bx, \vz, t)_{i} = 0$ for all $i$.




\newpage
\section{Additional Experimental Results}
\label{app:sec:additional_experimental_results}

\paragraph{Distributions with high discrepancies}

We report the results of the algorithms under different settings and different weighting schemes. For TSM we additionally report the results under uniform weighting, i.e. $\lambda(t)=1$. 



\paragraph{Gaussians}

We report the main results in Table~\ref{tbl:gaussians1} and Table~\ref{tbl:gaussians2}. CTSM-v is consistently among the fastest and the best. The plot in the main paper is generated using TSM with Stein score normalization, CTSM with time score normalization and $c=1$ and CTSM with time score normalization and $c=1$.

We additionally report the results of using time score normalization for TSM in Table~\ref{tbl:gaussians-extras}. We did not observe decisive improvements, and remark that CTSM-v yields better results with the same weighting scheme.

\begin{table*}
        \begin{center}
        \caption{Results on Gaussians with $D$ being $2$, $5$ or $10$. $D$ is dimensionality, MSE is MSE to ground truth reported in the form of [mean, std], T is average time per step in ms. Unif indicates uniform weighting, Stein indicates Stein score normalization and Time indicates time score normalization, with Time 0 indicating using the real $c$ and Time 1 indicating using $c=1$.}
                \begin{tabular}{|l|l|l|l|l|l|l|}
                        \hline
                        & \multicolumn{2}{|c|}{$D=2$} & \multicolumn{2}{|c|}{$D=5$} & \multicolumn{2}{|c|}{$D=10$} \\
                        \hline
                        Algo & MSE & T & MSE & T & MSE & T \\
                        \hline
                        TSM+Unif & [0.21, 0.036] & 11.1 & [2.982, 1.738] & 12.5 & [12.478, 6.026] & 12.1 \\
                        \hline
                        TSM+Stein & [0.253, 0.142] & 13.4 & [2.408, 1.205] & 15.4 & [7.343, 1.378] & 14.0 \\
                        \hline
                        CTSM+Time 0 & [0.158, 0.049] & \textbf{3.9} & [1.37, 0.821] & 6.3 & [16.285, 11.047] & 5.3 \\
                        \hline
                        CTSM+Time 1 & [\textbf{0.078}, 0.017] & 4.5 & [0.987, 0.28] & 6.0 & [10.032, 5.476] & \textbf{4.8} \\
                        \hline
                        CTSM-v+Time 0 & [0.175, 0.045] & 8.3 & [0.86, 0.199] & 5.2 & [4.331, 0.727] & 5.1 \\
                        \hline
                        CTSM-v+Time 1 & [0.104, 0.014] & 4.0 & [\textbf{0.814}, 0.219] & \textbf{5.0} & [\textbf{1.616}, 0.203] & 4.9 \\
                        \hline
                \end{tabular}
        \label{tbl:gaussians1}
        \end{center}
\end{table*}

\begin{table*}
        \begin{center}
        \caption{Results on Gaussians with $D$ being $15$ or $20$. $D$ is dimensionality, MSE is MSE to ground truth reported in the form of [mean, std], T is average time per step in ms. Unif indicates uniform weighting, Stein indicates Stein score normalization and Time indicates time score normalization, with Time 0 indicating using the real $c$ and Time 1 indicating using $c=1$.}
                \begin{tabular}{|l|l|l|l|l|}
                        \hline
                        & \multicolumn{2}{|c|}{$D=15$} & \multicolumn{2}{|c|}{$D=20$} \\
                        \hline
                        Algo & MSE & T & MSE & T \\
                        \hline
                        TSM+Unif & [74.932, 60.02] & 13.8 & [335.45, 83.226] & 13.0 \\
                        \hline
                        TSM+Stein & [91.328, 48.905] & 14.3 & [329.779, 156.634] & 12.9 \\
                        \hline
                        CTSM+Time 0 & [36.922, 20.238] & 6.2 & [125.234, 30.715] & \textbf{3.9} \\
                        \hline
                        CTSM+Time 1 & [61.902, 19.891] & 5.5 & [50.756, 12.708] & 4.7 \\
                        \hline
                        CTSM-v+Time 0 & [16.529, 3.101] & 5.4 & [\textbf{41.945}, 13.973] & 5.0 \\
                        \hline
                        CTSM-v+Time 1 & [\textbf{8.88}, 1.921] & \textbf{4.8} & [43.861, 17.132] & 5.9 \\
                        \hline
                \end{tabular}
        \label{tbl:gaussians2}
        \end{center}
\end{table*}

\begin{table*}
        \begin{center}
        \caption{Additional results on Gaussians. $D$ is dimensionality, MSE is MSE to ground truth reported in the form of [mean, std], T is average time per step in ms. Unif indicates uniform weighting, Stein indicates Stein score normalization and Time indicates time score normalization, with Time 0 indicating using the real $c$ and Time 1 indicating using $c=1$.}
                \begin{tabular}{|l|l|l|l|l|}
                        \hline
                        & \multicolumn{2}{|c|}{TSM+Time 0} & \multicolumn{2}{|c|}{TSM+Time 1} \\
                        \hline
                        D & MSE & T & MSE & T \\
                        \hline
                        2 & [0.217, 0.063] & 11.7 & [0.451, 0.206] & 11.6 \\
                        \hline
                        5 & [3.764, 2.107] & 13.1 & [5.088, 4.481] & 12.0 \\
                        \hline
                        10 & [13.647, 2.953] & 13.9 & [30.196, 12.414] & 12.3 \\
                        \hline
                        15 & [96.588, 53.982] & 14.5 & [99.062, 34.036] & 31.8 \\
                        \hline
                        20 & [218.046, 70.411] & 14.2 & [135.942, 53.202] & 13.9 \\
                        \hline
                \end{tabular}
        \label{tbl:gaussians-extras}
        \end{center}
\end{table*}

\paragraph{Gaussian mixtures}

We report the main results on Gaussian mixtures in Table~\ref{tbl:gmms}. We set $\sigma$ in the Schrödinger bridge probability path to $1.0$ due to strong empirical results while enabling direct comparisons between TSM and CTSM(-v).

We additionally report the results with $\sigma=\sqrt{2.0}$ in Table~\ref{tbl:gmms-2.0} and the results with $\sigma=0.0$ in Table~\ref{tbl:gmms-0.0}. We observe that, setting $\sigma=\sqrt{2.0}$ results in worse performances for all methods. For TSM under uniform weighting, one can consider using $\sigma=0.0$, in which case the performance improves, though CTSM-v under $\sigma=1.0$ remains competitive.

\begin{table*}
        \begin{center}
        \caption{Results on GMMs with $\sigma=1.0$. $k$ determines the distance between two GMM components, MSE is MSE to ground truth reported in the form of [mean, std], T is average time per step in ms. Unif indicates uniform weighting, Stein indicates Stein score normalization and Time indicates time score normalization, with Time 0 indicating using the real $c$ and Time 1 indicating using $c=1$.}
                \begin{tabular}{|l|l|l|l|l|l|l|}
                        \hline
                        & \multicolumn{2}{|c|}{k=0.5} & \multicolumn{2}{|c|}{k=1.0} & \multicolumn{2}{|c|}{k=2.0} \\
                        \hline
                        Algo & MSE & T & MSE & T & MSE & T \\
                        \hline
                        TSM+Unif & [\textbf{173.473}, 52.466] & 28.9 & [276.545, 97.042] & 12.6 & [14643.815, 13997.568] & 61.8 \\
                        \hline
                        TSM+Stein & [232.948, 133.647] & 14.6 & [459.645, 260.768] & 17.6 & [3427.258, 3545.452] & 12.0 \\
                        \hline
                        CTSM+Time 0 & [880.47, 172.594] & \textbf{4.1} & [480.847, 151.097] & 4.5 & [646.945, 210.44] & 4.7 \\
                        \hline
                        CTSM+Time 1 & [923.082, 131.758] & 4.6 & [460.546, 186.5] & 4.2 & [547.603, 200.504] & 4.8 \\
                        \hline
                        CTSM-v+Time 0 & [173.804, 108.326] & 4.8 & [211.046, 69.472] & \textbf{4.0} & [319.981, 100.91] & 7.4 \\
                        \hline
                        CTSM-v+Time 1 & [221.519, 98.112] & 5.8 & [\textbf{181.082}, 68.879] & 4.7 & [\textbf{266.486}, 150.877] & \textbf{4.3} \\
                        \hline
                \end{tabular}
        \label{tbl:gmms}
        \end{center}
\end{table*}

\begin{table*}
        \begin{center}
        \caption{Results on GMMs with $\sigma=\sqrt{2.0}$. $k$ determines the distance between two GMM components, MSE is MSE to ground truth reported in the form of [mean, std], T is average time per step in ms. Unif indicates uniform weighting, Stein indicates Stein score normalization and Time indicates time score normalization, with Time 0 indicating using the real $c$ and Time 1 indicating using $c=1$.}
                \begin{tabular}{|l|l|l|l|l|l|l|}
                        \hline
                        & \multicolumn{2}{|c|}{k=0.5} & \multicolumn{2}{|c|}{k=1.0} & \multicolumn{2}{|c|}{k=2.0} \\
                        \hline
                        Algo & MSE & T & MSE & T & MSE & T \\
                        \hline
                        TSM+Unif & [1106.178, 550.442] & 33.3 & [1293.421, 270.072] & 12.5 & [6614.483, 1169.068] & 13.5 \\
                        \hline
                        TSM+Stein & [1460.023, 502.921] & 39.0 & [1564.266, 360.361] & 36.0 & [5180.453, 1786.018] & 12.7 \\
                        \hline
                        CTSM+Time 0 & [1934.401, 269.515] & 4.5 & [1872.342, 467.047] & 4.6 & [5961.52, 683.578] & 4.6 \\
                        \hline
                        CTSM+Time 1 & [2113.975, 403.59] & 8.0 & [2238.267, 123.69] & 4.6 & [6017.094, 344.537] & 4.0 \\
                        \hline
                        CTSM-v+Time 0 & [745.15, 158.202] & 4.6 & [1558.495, 379.161] & 4.5 & [5009.627, 1943.244] & 8.5 \\
                        \hline
                        CTSM-v+Time 1 & [762.231, 288.029] & 5.3 & [1762.826, 431.333] & 4.8 & [9226.993, 861.191] & 9.2 \\
                        \hline
                \end{tabular}
        \label{tbl:gmms-2.0}
        \end{center}
\end{table*}

\begin{table*}
        \begin{center}
        \caption{Results on GMMs with $\sigma=0.0$. $k$ determines the distance between two GMM components, MSE is MSE to ground truth reported in the form of [mean, std], T is average time per step in ms. Unif indicates uniform weighting, Stein indicates Stein score normalization and Time indicates time score normalization, with Time 0 indicating using the real $c$ and Time 1 indicating using $c=1$.}
                \begin{tabular}{|l|l|l|l|l|l|l|}
                        \hline
                        & \multicolumn{2}{|c|}{k=0.5} & \multicolumn{2}{|c|}{k=1.0} & \multicolumn{2}{|c|}{k=2.0} \\
                        \hline
                        Algo & MSE & T & MSE & T & MSE & T \\
                        \hline
                        TSM+Unif & [148.688, 97.058] & 14.6 & [70.908, 5.85] & 12.9 & [898.016, 847.255] & 49.1 \\
                        \hline
                \end{tabular}
        \label{tbl:gmms-0.0}
        \end{center}
\end{table*}

\newpage
\section{Experimental Details}
\label{app:sec:exp}

\subsection{Bug of TSM Implementation for Toy Experiments in \citet{choi2022densityratio}}
\label{app:sec:bug_tsm_toy}

We observed a bug for the TSM implementation of the code of \citet{choi2022densityratio}. Recall that the TSM objective is given by

\begin{align}
\begin{split}
    \mathcal{L}_{\text{TSM}}(\btheta)
    =
    2 \mathbb{E}_{p_0(\bx)}[s_{\btheta}(\bx, 0)]
    -
    2 \mathbb{E}_{p_1(\bx)}[s_{\btheta}(\bx, 1)]
    + 
    \\
    \E_{p(t, \bx)}
    [
    2 \dot{s}_{\btheta}(\bx, t) 
    +
    2 \dot{\lambda}(t)
    s_{\btheta}(\bx, t)
    +
    \lambda(t)
    s_{\btheta}(\bx, t)^{2}
    ].
\end{split}
\end{align}

However, \citet{choi2022densityratio} implemented
\begin{align}
\begin{split}
    \mathcal{L}_{\text{TSM}}(\btheta)
    =
    2 \mathbb{E}_{p_0(\bx)}[s_{\btheta}(\bx, 0)]
    -
    2 \mathbb{E}_{p_1(\bx)}[s_{\btheta}(\bx, 1)]
    + 
    \\
    \E_{p(t, \bx)}
    [
    2 \dot{s}_{\btheta}(\bx, t) 
    +
    \dot{\lambda}(t)
    s_{\btheta}(\bx, t)
    +
    \lambda(t)
    s_{\btheta}(\bx, t)^{2}
    ],
\end{split}
\end{align}
i.e. the scaling in front of $\dot{\lambda}(t)s_{\btheta}(\bx, t)$ is incorrect. We remark that this bug only applies when attempting to train purely based on TSM objective on toy experiments.

\subsection{Implementation Details}
\label{app:sec:impl-details}

Our implementation of TSM is largely based on the code provided by \citet{choi2022densityratio}. However, especially for other than the EBM experiments, we improve their code in several ways. Apart from bug fixes, we use analytical expressions for the weighting quantities.

For both TSM and CTSM, following \citet{choi2022densityratio}, we add a small number $\epsilon$ to the time during training and inference. We follow the convention that $\epsilon$ is added when the probability path results in approximately degenerate distribution at that time. For the toy experiments, we set $\epsilon=1e-5$, while for EBM experiments we set $\epsilon=1e-4$ during training and $\epsilon=1e-5$ during inference.

For experiments apart from EBM, for each task we employ a fixed validation set of size $10000$ and select the learning rates based on results on the sets. After a certain number of steps, an evaluation step is performed, and the model is evaluated based on both the validation set and a test set, consisting of $10000$ samples dynamically generated based on the data generation process. The best test set results are obtained by selecting the steps corresponding to the best validation set results.

Following \citet{choi2022densityratio}, the density ratios are evaluated using the initial value problem ODE solver as implemented in SciPy \citep{virtanen2020scipy}, where we use the default RK45 integrator \citep{dormand1980family} with $rtol=1e-6$ and $atol=1e-6$.

\subsection{Distributions with High Discrepancies}
\label{app:sec:exp1}

The experimental setup is similar to \citet{choi2022densityratio}. We use as score model a simple MLP with structure $[D+1, 256, 256, 256, N_{output}]$ and ELU activation \citep{clevert2016fast} based on \citet{choi2022densityratio}, where $D$ is the dimensionality of the data and $N_{output}=D$ for CTSM-v and $1$ otherwise. Note that the input shape is $D+1$, as the time $t$ is concatenated to the input. All models are trained for $20000$ iterations. After each $1000$ iterations, the model is evaluated. For each scenario, the best learning rate is selected based on the best val set performances of a single run. Afterwards two runs under the same learning rate but different random seeds are run, and the final results on the test set is reported.

\paragraph{Gaussians}

Following \citet{choi2022densityratio}, we employ the variance-preserving probability path, with $\alpha_{t}=t$.

The learning rate is tuned between $[5e-4, 1e-3, 2e-3, 5e-3, 1e-2]$. Following \citet{choi2022densityratio}, the MSEs are evaluated using samples from both $p_{0}$ and $p_{1}$.

\paragraph{Gaussian mixtures}

The learning rate is tuned between $[1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2]$. There is one case where the selected learning rate for each algorithm is the smallest, and we manually verify that using lrs $5e-5$ or $2e-5$ does not result in improved results. Following \citet{choi2022densityratio}, the MSEs are evaluated using samples from both $p_{0}$ and $p_{1}$.

The two components are isotropic, with the covariance given by $\sigma^{2}\bI$. We use $\texttt{k}$ to specify the distance between the means of the two components as a multiple of the standard deviation $\sigma$.

We know that the mean of a GMM is simply given by the mean of the means of each component, while the covariance of a GMM with two components of equal weights is given by the following formula
\begin{align}
\bSigma = \frac{1}{2} \bSigma_{1} + \frac{1}{2} \bSigma_{2} + \frac{1}{4} (\bmu_{1}-\bmu_{2})(\bmu_{1}-\bmu_{2})^{\top}.
\end{align}
Consider the case where $\bmu_{1}-\bmu_{2} = k\sigma$. One has that, in order for the GMM to have variance equal to $1$ in each dimension, $\sigma=\sqrt{4 / (4+\texttt{k}^{2})}$. The means of the two components are given by $\bmu-\frac{1}{2} k \sigma$ and $\bmu+\frac{1}{2} k \sigma$, respectively.

In principle, using $\text{Var}=2$ for SB path results in preserved variance along the path. However, empirically we observe that it is beneficial to use a smaller variance, e.g. $\text{Var}=1$.



\subsection{Mutual Information Estimation}
\label{sec:mi-estimation}

The probability path is given by
\begin{equation}
p_{t}(\bx|\bz) = \mathcal{N}(\bx|t\bx_{1},\left(1-t^{2}\right)\bI).
\end{equation}

The derivations for the objective of TSM objective can be found in \citet{choi2022densityratio}. Here we derive the training objective for the CTSM-v objective.

Using similar settings and notations as in \citet{choi2022densityratio}, we parameterize a single matrix $\bS$, as defined below.

Denote the covariance matrix of $p_{1}$ as $\bSigma$. Use $\bS$ to denote $\bSigma-\bI$.

Recall that the true time score is given by the posterior expectation of $\partial_{t}\log p_{t}(\bx|\bz)$. We have
\begin{align}
\log p(\bz) &= \log\mathcal{N}(\bz|\bzero,\bSigma) = -\frac{1}{2}\bz^{\top}\bSigma^{-1}\bz+\text{const.},\\
\log p_{t}(\bx|\bz) &= \log\mathcal{N}(\bx|t\bz, (1-t^{2})\bI) = -\frac{1}{2}t\bz^{\top}\frac{1}{1-t^{2}}t\bz + \text{const.}.
\end{align}
The posterior distribution $p_{t}(\bz|\bx)$ can be solved in closed-form, which is a Gaussian distribution, with covariance $\bar{\bSigma} = \left(\bSigma^{-1} + \frac{t^{2}}{1-t^{2}}\bI\right)^{-1}$ and mean $\frac{t}{1-t^{2}}\bar{\bSigma}\bx$. Similar to \citet{choi2022densityratio}, the above quantities can be expressed in terms of the inverse of $\bI + t^{2}\left(\bSigma - \bI\right) = (1-t^{2})\left(\bI + \frac{t^{2}}{1-t^{2}}\bSigma\right)$; we have
\begin{align}
&\quad \left(\bSigma^{-1} + \frac{t^{2}}{1-t^{2}}\bI\right)^{-1} = \left(\bSigma^{-1}\left(\bI + \frac{t^{2}}{1-t^{2}}\bSigma\right)\right)^{-1} \\
&= \left(\bI+\frac{t^{2}}{1-t^{2}}\bSigma\right)^{-1}\bSigma = \left(1-t^{2}\right)\left(\bI + t^{2}\left(\bSigma - \bI\right)\right)^{-1}\bSigma.
\end{align}

The expectation of $\partial_{t}\log p_{t}(\bx|\bz)$, which by definition is also the value of $\partial_{t}\log p_{t}(\bx)$, can also be obtained in closed-form. The expectation of the individual entries of $\partial_{t}\log p_{t}(\bx|\bz)$ are also given in closed-form.

\begin{align}
\left[\partial_{t}\log p_{t}(\bx|\bz)\right]_{i} &= \frac{t}{1-t^{2}} - \frac{t}{\left(1-t^{2}\right)^{2}}\left[\left(\bx-t\bx_{1}\right)^{2}\right]_{i} + \frac{1}{1-t^{2}}\left[\left(\bx-t\bx_{1}\right)\bx_{1}\right]_{i},\\
\E_{p_{t}(\bz|\bx)}\left[\partial_{t}\log p_{t}(\bx|\bz)\right]_{i} &= \frac{t(1-t^{2}) -t\left(\norm{\bar{\bmu}_{i}}^{2}+\bar{\bSigma}_{ii}\right) - t\norm{\bx_{i}}^{2} + (t^{2}+1)\bx_{i}\bar{\bmu}_{i}}{\left(1-t^{2}\right)^{2}},
\end{align}
where $\bar{\bmu}$ and $\bar{\bSigma}$ are the mean and covariance of the posterior distribution as discussed above. As such, perhaps unsurprisingly, CTSM-v does not induce much computational overhead above TSM.

For CTSM objective, the model is trained to match the time score, while for CTSM-v objective, the model is trained to match the entire $vec\left(\partial_{t}\log p_{t}(\bx|\bz)\right)$.

The hyperparameters are inspired by \citet{choi2022densityratio} and listed in Table~\ref{tbl:mi-hyper}. For all methods, the learning rates are tuned between $1e-4$, $1e-3$ and $1e-2$.

\begin{table}[]
    \centering
    \caption{Hyperparameters for MI experiment. After every $\text{eval freq}$ steps, an evaluation is performed, with the first result after the first $\text{eval freq}$ steps.}
    \begin{tabular}{c|c|c|c}
        D & n iters & eval freq & batch size \\
        $40$ & $20001$ & $2000$ & $512$ \\
        $80$ & $50001$ & $5000$ & $512$ \\
        $160$ & $200001$ & $5000$ & $512$ \\
        $320$ & $400001$ & $8000$ & $256$
    \end{tabular}
    \label{tbl:mi-hyper}
\end{table}

\subsection{Energy-based Modeling}
\label{app:sec:ebm}

We employ the same variance-preserving probability path as used in \citet{choi2022densityratio}, which in turn comes from diffusion models literature \citep{ho2020ddpm,song2021sde}.

For reproducing TSM results, we use a batch size of $500$ and use polynomial interpolation with buffer size $100$, matching the reported hyperparameters in \citet{choi2022densityratio}. Following \citet{choi2022densityratio}, we tune the step size of TSM between $[2e-4, 5e-4, 1e-3]$. For CTSM-v, we largely reuse the hyperparameters, while tuning the step size between $[5e-4, 1e-3, 2e-3]$.

For CTSM-v objective, we parameterize the model to output the time score normalized by the approximate variance. Specifically, for a given $t$, we calculate $\text{Var}\left(\partial_{t}\log p_{t}(\bx|\bz)\right)$ where $c$ is assumed to be $1$, and the score network is trained to predict $\frac{\partial_{t}\log p_{t}(\bx)}{\text{Std}\left(\partial_{t}\log p_{t}(\bx|\bz)\right)}$; this ensures that the regression target is zero mean and having reasonable variances across $t$.

In previous works \citet{Rhodes2020,choi2022densityratio}, different normalizing flows are fitted to the data, and the DRE is carried out making use of the flows.

The flows can naturally be utilized in different ways. Denote the latent space of the flow as $\bu$, and the ambient space of the flow as $\bx$. \citet{choi2022densityratio} consider the following scheme: 
\begin{enumerate}
\item An SDE is defined on $\bu$ space, interpolating between Gaussian and the empirical distribution induced by final samples on $\bu$ space obtained by transforming the data points from $\bx$ space, 
\item Intermediate samples on $\bu$ space are transformed into $\bx$ space using the flow, inducing a time varying distribution on $\bx$ space,
\item The score network takes as input $\bx$ and $t$, and is trained to predict the time score.
\end{enumerate}

Note that a flow is a bijection. Consider a time-varying density $p_{t}(\bx)$. For any $t$, we use the same bijective transformation $T$ to obtain the pair of $\bu$ and $\bx$. We have
\begin{equation}
\partial_{t}\log p_{t}(\bx) = \partial_{t}\log \left(p_{t}(\bu)\left\vert\text{det}J_{T}(\bu)\right\vert^{-1}\right) = \partial_{t}\log p_{t}(\bu) + \partial_{t}\log \left\vert\text{det}J_{T}(\bu)\right\vert^{-1}=\partial_{t}\log p_{t}(\bu).
\end{equation}
As such, the time score is invariant across bijections.

With CTSM, inspired by previous approaches, we also consider a probability path in $\bu$ space. One needs the time score of the conditional distribution, which needs to be computed in $\bu$ space. One can in principle train the score network either by feeding in coordinates of points in the $\bu$ space or the corresponding coordinates in $\bx$ space, where the conditional target vector field is computed in $\bu$ space.

\begin{enumerate}
\item An probability path is defined on $\bu$ space, interpolating between Gaussian and the empirical distribution induced by samples,
\item The score network takes as input either $\bx$ or $\bu$ along with $t$, and learns the time score.
\end{enumerate}

Note that it is correct to feed in the score network either $\bx$ or $\bu$; when the model takes as input $\bx$, one can interpret that the normalizing flows is a part of the score network, i.e. $\tilde{\bs}_{\btheta}(\bu,t) = \bs_{\btheta}(\bbf^{-1}\left(\bx\right),t)$, where $\bbf$ is the normalizing flows that is fixed and does not need to be learned and $\bs$ is the score network that we parameterize. As such, the correctness is guaranteed by standard CTSM / CTSM-v identities. We empirically observe that directly feeding in $\bx$ coordinates leads to better BPD estimates. 

We remark that both TSM and CTSM need to map between $\bu$ and $\bx$ coordinates using the normalizing flows. However, while CTSM only need to map both $\bu$ to $\bx$ and $\bx$ to $\bu$ exactly once, TSM requires an extra $\bu$ to $\bx$ map due to needed by the boundary condition.

In terms of EBM with Gaussian flows, we observe that, possibly due to the parameterization, models trained using CTSM-v may require a larger number of integration steps compared with TSM when evaluating the density ratio using an ODE integrator with specific error tolerances as described in Section~\ref{app:sec:impl-details}: on MNIST test set with batch size $1000$, TSM requires on average $489.2$ evaluations, while CTSM-v requires on average $830.6$ evaluations. However, we remark that it is unclear what the true time scores are like.

\subsection{Annealed MCMC}

We employ annealed MCMC to draw samples from the learned score network. We draw a total of $100$ samples. For each sample, we construct $1000$ intermediate distributions, where each intermediate distribution is targeted using a single HMC step. The intermediate distributions are constructed by linearly interpolating between $0$ and $1$ and setting
\begin{equation}
\log p_{t}(\bx) = \log p_{0}(\bx) + \int_{\tau=0}^{t}\partial_{\tau}\log p_{\tau}(\bx)\dd \tau.
\end{equation}

After which, we run another $100$ steps of HMC to further refine the samples.

Each step of HMC contains $10$ leapfrog steps. We observe a correlation between sample quality and the estimated log constant, where the sample quality is good when the estimated log constant is close to $0$. Based on the observation, we tune the step size of HMC on a grid in the form of $[1e-n,2.5e-n,5e-n,7.5e-n]$.

Samples drawn from models trained using TSM and CTSM-v are shown in Figure~\ref{fig:mcmc_samples}.

\begin{figure*}
\centering
\begin{tabular}{ccc}
    \includegraphics[width=0.3\textwidth]{image/tsm-gaussian-samples.png} & \includegraphics[width=0.3\textwidth]{image/ctsm-gaussian-samples.png}
\end{tabular}
\caption{Left: TSM, Gaussian flows, middle: CTSM, Gaussian flows}
\label{fig:mcmc_samples}
\end{figure*}
