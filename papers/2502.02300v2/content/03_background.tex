
\section{Background}
\label{sec:background}

Our goal is to estimate the ratio between two densities $p_0$ and $p_1$, given samples from both. We start by defining a distribution over labels $t$ and data points $\bx$,
\begin{align}
    \label{eq:joint_model_two_variables}
    p(\bx, t) = p(t) p(\bx | t)
\end{align}
constructed such that we recover $p_{0}$ and $p_{1}$ for $t=0$ and $t=1$ respectively. We next show how several relevant methods can be viewed as variations on this formalism. 

\paragraph{Binary label}
Fundamental approaches to density-ratio estimation consider
a binary label $t \in \{0, 1\}$. Among them, Noise Contrastive Estimation (NCE) is based on the observation that the density ratio is related to the binary classifier $p(t | \vx)$~\citep[Eq. 5]{gutmann2012nce}. NCE estimates that classifier by minimizing a binary classification loss based on logistic regression, computed using samples drawn from $p_0$ and $p_1$. In practice, using NCE is challenging when $p_0$ and $p_1$ are ``far apart''. In that case, both the binary classification loss becomes harder to optimize~\citep{liu2022nceoptim} and the sample-efficiency of its minimizer deterioriates~\citep{gutmann2012nce,lee2023ncevariance,Chehab2023optimizing,Chehab2023provable}. 

\paragraph{Continuous label}
More recent developments relax the label so that it is continuous $t \in [0, 1]$. Now, conditioning on $t$ defines intermediate distributions $p(\bx | t)$ along a probability path that connects $p_0$ to $p_1$. Then, the following identity is used~\citep{choi2022densityratio}
\begin{align}
    \label{eq:main_identity}
    \log \frac{p_1(\bx)}{p_0(\bx)}
    =
    \int_0^1 \partial_t \log p_t(\bx) dt,
\end{align}
or its discretization in time~\citep{Rhodes2020}. 

\paragraph{Probability path}
We next consider a popular use-case, where $p_0$ is a Gaussian and $p_1$ is the data density~\citep{Rhodes2020,choi2022densityratio}; since $p_0$ is known analytically, the ratio of the two provides directly an estimator for $p_1$. 
In practice, one can construct a probability path where the intermediate distributions can  be sampled from but their densities cannot be evaluated. This is because the probability path is defined by interpolating samples from $p_0$ and $p_1$. There are multiple ways to define such interpolations~\citep{Rhodes2020,Albergo2023}, which we will further discuss in Section~\ref{sec:design_choices}. A widely used approach is the Variance-Preserving (VP) probability path, which can be simulated by~\citep{song2021sde,lipman2023conditionalflowmatching,choi2022densityratio}
\begin{align}
\label{eq:vp_path_simulation}
\bx = \sqrt{\alpha_{t}^2} \bx_{1} + \sqrt{1 - \alpha_{t}^{2}}\bx_{0},
\end{align}
where $\bx_0 \sim \mathcal{N}(\bzero, \mI)$, $\bx_1 \sim p_1$ follows the data distribution, time is drawn uniformly $t \sim \mathcal{U}[0, 1]$ and $\alpha_t \in [0, 1]$ is a positive function that increases from $0$ to $1$. By conditioning on $t$, we obtain densities $p_t(\bx) = \frac{1}{\sqrt{1 - \alpha_{t}^{2}}} p_0(\frac{\bx}{\sqrt{1 - \alpha_{t}^{2}}}) \ast \frac{1}{\alpha_{t}} p_1(\frac{\bx}{\alpha_{t}})$ that cannot be computed in closed-form, given that the density $p_1$ is unknown and that the convolution requires solving a difficult integral. 
 
\paragraph{Estimating the time score}
Importantly, the identity in~\eqref{eq:main_identity} requires estimating the time score $\partial_t \log p_t(\bx)$, which is the Fisher score where the parameter is the label $t$. It can also be related to the binary classifier between two infinitesimally close distributions $p_t$ and $p_{t + dt}$~\citep[Proposition 3]{choi2022densityratio}. Formally, this time score can be approximated by minimizing the following \textit{Time Score Matching (TSM)} objective
\begin{align}
    \label{eq:l2_loss}
    \mathcal{L}_{\text{TSM}}(\btheta) 
    = 
    \E_{p(t, \bx)} \big[
    \lambda(t)
    \big( \partial_{t}\log p_{t}(\bx) 
    - 
    s_{\btheta}(\bx, t) \big)^2
    \big],
\end{align}
where $\lambda(t)$ is any positive weighting function. This objective requires evaluating the time score $\partial_t \log p_t(\bx)$. However, as previously explained, the formula for the time score is unavailable because the densities $p_t$, while well-defined, are not known in closed form.

To make the learning objective in~\eqref{eq:l2_loss} tractable, an insight from~\citet{hyvarinen2005scorematching} led~\citet{choi2022densityratio,Williams2024} to rewrite it using integration by parts. This yields
\begin{align}
\begin{split}
    \label{eq:integration_by_parts}
    \mathcal{L}_{\text{TSM}}(\btheta)
    =
    2 \mathbb{E}_{p_0(\bx)}[s_{\btheta}(\bx, 0)]
    -
    2 \mathbb{E}_{p_1(\bx)}[s_{\btheta}(\bx, 1)]
    + 
    \E_{p(t, \bx)}
    [
    2 \dot{s}_{\btheta}(\bx, t) 
    +
    2 \dot{\lambda}(t)
    s_{\btheta}(\bx, t)
    +
    \lambda(t)
    s_{\btheta}(\bx, t)^{2}
    ],
\end{split}
\end{align}
which no longer requires evaluating the time score $\partial_t \log p_t(\bx)$. However, this approach has one clear computational drawback: differentiating the term $\dot{s}_{\btheta}(x, t)$ in the loss~\eqref{eq:integration_by_parts} involves using automatic differentiation twice --- first in $t$ and then in $\btheta$ --- which can be time-consuming (we verify this in Section~\ref{sec:experiments}). This motivates us to find better ways of learning the time score.

