\subsection{Lemmas concerning probability and analysis}\label{app:defs}
We state two important lemma about an infinite sequence of events below, commonly referred to as the Borel-Cantelli lemma and the second Borel-Cantelli Lemma. Note that the second Borel-Cantelli lemma however only holds if the infinite sequence of events are independent. 

\begin{lemma}[Borel-Cantelli lemma \cite{Bor09,Can17, Shi16}]\label{lemma:borellcantelli}
    Let $E_1,E_2\dots,E_n$ be an infinite sequence of events in some probability space. If the sum of probabilities of the events is finite, then the probability that infinitely many of them occur is $0$. That is, $$\text{if }\sum_{n=1}^\infty\Pr[E_n]<\infty \text{ then } \Pr[\limsup_{n\to\infty}E_n] = 0.$$
\end{lemma}
\begin{lemma}[Second Borel-Cantelli lemma~\cite{Bor09,Can17, Shi16}]\label{lemma:secondborellcantelli}
      Let $E_1,E_2\dots,E_n$ be an infinite sequence of \emph{independent} events in some probability space. If the sum of probabilities of the events is infinite, then almost-surely infinitely many of them occur. That is, $$\text{if }\sum_{n=1}^\infty\Pr[E_n] = \infty \text{ then } \Pr[\limsup_{n\to\infty}E_n] = 1.$$  
\end{lemma}
We will also use the following standard result from analysis.
\begin{proposition}[\!\!\protect{\cite[Proposition~3.1]{SS10}}]\label{prop:complexanalysisConverge}
If the infinite sum $\sum_{i=1}^\infty|a_n|<\infty$, then the infinite product $\prod_{i=1}^\infty(1+a_n)$ converges. Moreover, the product converges to 0 if and only if at least one of its factors is 0.
\end{proposition}

\subsection{Partial-observation games}

We deal with 2-player turn-based partial-observation stochastic games in this paper that are played between Eve and Adam. These are given by a tuple $$\Gc=(V_{\eve},V_{\adam},E,\Act_{\eve},\Act_{\adam},\Oc_{\eve},\Oc_{\adam},\delta)$$ with the following components.
\begin{enumerate}
    \item \emph{Arena.} The set of vertices is given by $V=V_{\eve}\uplus V_{\adam}$, with Eve (resp.\ Adam) said to own the vertices in $V_{\eve}$ (resp.\ $V_{\adam}$). Edges $E$ are directed edges that are a subset of the set $(V_{\eve} \times V_{\adam}) \cup (V_{\adam}\times V_{\eve})$.
    \item \emph{Actions.} $\Act_{\eve}$ (resp.\ $\Act_{\adam}$) is a finite set of actions for Eve (resp.\ Adam). We use $\Act=\Act_{\eve} \uplus \Act_{\adam}$ to denote the set of actions for both players.  
    \item \emph{Transition function.} The function $\delta$ is a probabilistic transition function $$\delta:V\times \Act \xrightarrow{} \Distribution(E)$$ that, for each vertex in $V_{\eve}$ (resp.\ $V_{\adam}$) and action in $\Act_{\eve}$ (resp.\ $\Act_{\adam}$), assigns a probability distribution to the set of outgoing edges from that vertex. 
    \item \emph{Observations.} The set $\Oc_{\eve}$ and $\Oc_{\adam}$ are partitions of $E$. These uniquely map each vertex $e\in E$ to its observations for Eve and Adam as $\Oc_{\eve}(e)$ and $\Oc_{\adam}(e)$, respectively.
\end{enumerate}

A \emph{play} of a game $\Gc$ proceeds as follows. Starting with a token at an initial vertex $v$, the player who owns that vertex chooses an action $a$ from their set of actions, and the token is moved along an edge $e=(v,v')$ with probability $\delta(v,a)\circ (e)$. The new position of the token is $v'$, from where the play proceeds similarly, for infinitely many rounds. 

Thus, each play is an infinite path $\rho=e_0 e_2 e_3 \dots$ in the arena. For the play $\rho$, the \emph{observation sequence} for Eve (resp.\ Adam) is given by $\Oc_{\eve}(e_0 e_1 e_2 \dots)$ (resp.\ $\Oc_{\adam}(e_0 e_1 e_2\dots)$). A \emph{finite play} is a finite prefix of an infinite play.

A \emph{strategy} $\sigma$ for Eve is a partial function $$\sigma:\Oc_{\eve} (E^*)\xrightarrow{}\Distribution(\Act_{\eve})$$ from her observation sequence of finite plays ending at an Eve's vertex to her actions. The strategy $\sigma$ for Eve is said to be \emph{pure} if it assigns, to any observation sequence of a finite play ending at an Eve's vertex, each action either the probability 0 or 1. A pure strategy can equivalently be represented by a function $\sigma:\Oc_{\eve} (E^*)\xrightarrow{}\Act_{\eve}$. A strategy of Eve that is not pure is said to be a \emph{random strategy} for Eve. Pure and random strategies for Adam are defined analogously. 

An \emph{objective} for Eve is given by a set $\Theta \subseteq E^{\omega}$ of plays. A Borel objective is a Borel-measurable set in the Cantor topology, or also known as the product topology, on $E^{\omega}$~\cite[Chapter 17]{Kec12}. We will consider $\omega$-regular objectives, that is, objectives that can be given by a nondeterministic B\"uchi automaton that has its alphabet as $E$, which are Borel-measurable~\cite{Lan69}.

For a game $\Gc$ with an $\omega$-regular winning objective $L$, strategy $\sigma$ for Eve, and strategy $\tau$ for Adam, the probability that a play generated when Eve plays according to $\sigma$ and Adam plays according to $\tau$ is in $L$ is well-defined~\cite[Lemma 4.1]{Var85}, and we denote it by $\Pr_{\sigma,\tau}(\Gc)$.

We say that a strategy $\sigma$ for Eve in $\Gc$ is \emph{almost-sure} (resp.\ \emph{positive}) winning if for all strategies $\tau$ of Adam in $\Gc$, we have that $\Pr_{\sigma,\tau}=1$ (resp.\ $\Pr_{\sigma,\tau}>0$). We say that a strategy $\sigma$ for Eve in $\Gc$ is \emph{surely winning} or just \emph{winning} if Eve wins all possible plays produced in $\Gc$ in which Eve is playing according to $\sigma$. Almost-sure winning, positive winning, and surely winning strategies for Adam are defined analogously. 

\subsubsection*{Restrictions of games} 
We will consider the following restrictions of partial-observation games.
\paragraph*{Complete-observation games} This is the case when both the players have complete observation, i.e., $$\Oc_{\eve} = \Oc_{\adam} =\{\{e\}\mid e\in E\}.$$

\paragraph*{Non-stochastic games} This is the case when the transition function $\delta$ is a function $\delta:V\times \Act \xrightarrow{} E$ that assigns an edge to each action and vertex instead of a probability distribution over edges.

\paragraph*{One-player games}
One-player games for are games where there is a unique action available to either Eve or Adam, i.e., either $\Act_{\eve}=\{a\}$ or \ $\Act_{\adam}=\{a\}$. 

We use the above restrictions to define the games we deal with in this paper rigorously. We note that the HD~game (\cref{defn:hd-game}) and the 2-token game (\cref{defn:twotokengame}) on an automaton are examples of complete-observation nonstochastic games. 

\paragraph*{Stochastic-resolvability game}
The stochastic-resolvability (SR) game on $\Ac$ is a nonstochastic\footnote{Yes, the irony of this sentence is not lost on us.} game defined as follows.
\begin{definition}
    For a parity automaton $\Ac=(Q,\Sigma,\Delta,q_0)$, the the SR game on $\Ac$ is a nonstochastic game defined as follows. Eve's vertices are given by $V_{\eve}=Q\times \Sigma$, while Adam's vertices are given by $V_{\adam}=Q$. The game has the following edges.
    \begin{enumerate}
        \item $\{q\xrightarrow{} (q,a) \mid q \in Q, a \in \Sigma\}$ (Adam chooses a letter) 
        \item $\{(q,a) \xrightarrow{} (q') \mid q\xrightarrow{a:c}q' \in \Delta$\}. (Eve chooses a transition on her token)
    \end{enumerate}
    The game starts at the vertex $q_0$. Eve's actions are given by $\Act_{\eve}= \Delta$, while Adam's actions are given by $\Act_{\adam}=\Sigma$. The transition function $\delta$ is defined trivially.

    \noindent Eve has complete observation, that is $\Oc_{\eve}=\{\{e\}\mid e\in E\}$, while Adam has \emph{no observation}, that is, $\Oc_{\adam}=\{\{E\}\}$. 
    
    \noindent The winning condition for Eve is the same as in the HD game, i.e., a play of the SR game on $\Ac$ is winning for if the following condition holds: if the word formed by Adam's choice of letters is in $\Lc(\Ac)$ then the run formed on that word by Eve's choice of transitions is accepting.
\end{definition}


We now prove \cref{lemma:random-is-pure}. 
\randomispure*
\begin{proof}
    If $\Mc$ is a finite-memory strategy using which she almost-surely wins the SR game on $\Ac$ against all Adam's strategies, then it is clear that the same strategy is a almost-sure resolver for $\Ac$. Indeed, for any word~$w$ in $\Lc(\Ac)$, consider the strategy of Adam in the SR game, where he picks letters from $w$ in sequence. Then, Eve's strategy builds an accepting run almost-surely, and therefore, her strategy is an almost-sure resolver for~$\Ac$.

    For the other direction, suppose $\Mc$ is an almost-sure resolver for $\Ac$, and note that $\Mc$ can also be seen as a finite-memory strategy for Eve in the SR game on $\Ac$. We will show that Eve wins any play almost-surely using the strategy $\Mc$ against any Adam's strategy. To see this, consider the one-player partial-observation stochastic game $\Gc$ for Adam where we fix Eve's strategy $\Mc$. We know, since $\Ac$ is SR, that Adam does not have a pure strategy, i.e., Adam does not have a strategy that uses no randomness to win with a positive probability. For one-player partial-observation stochastic games, random strategies are as powerful as pure strategies in games with $\omega$-regular winning conditions~\cite[Theorem 7]{CDGH15}. It follows that Adam has no strategy in $\Gc$ to win with positive probability, as desired.  
\end{proof}


\paragraph*{Markov decision processes} A Markov decision process, or MDP for short, is a one-player complete-observation stochastic game, where we call the player Adam. MDPs can equivalently be represented by the tuple $\Mc = (V_A, V_R, E, \delta)$, with the following components.
\begin{enumerate}
    \item \emph{Arena.} The set of vertices is given by $V = V_A \uplus V_R$, where $V_A$ are vertices owned by the player, and $V_R$ are \emph{stochastic vertices}. The set $E$ consists of directed edges from $V_A$ to $V_R$ or from $V_R$ to $V_A$.
    \item \emph{Stochastic vertices.} The function $\delta$ is a partial function $\delta: E_R \xrightarrow{} \mathbb{U}$, which assigns a probability to the set of outgoing edges $E_R$ that start at a stochastic vertex, such that the sum of probabilities of the outgoing edges from each vertex in $V_R$ is $1$. 
\end{enumerate}

A play of the MDP $\Mc$ starts at a vertex $v_0$, and in round $i$ where the current position is $v_i$:
\begin{enumerate}
    \item if $v_i$ is a vertex in $V_A$ then \emph{Adam} selects an outgoing edge $e_i=(v_i,v_{i+1})$ from $v_i$;
    \item otherwise, if $v_i$ is a vertex in $V_R$, then an outgoing edge $e_i=(v_i,v_{i+1})$ is chosen with probability $\delta(e_i)$. 
\end{enumerate}
The play then goes to $v_{i+1}$, from where round $(i+1)$ begins.

Objectives and almost-sure (resp.\ positive) winning strategies for Adam in MDPs are defined similar to how we defined it for the games above. In \cref{appendixsubsec:np-completeness}, we will deal with MDPs with Muller conditions.

We will now show that resolvers for MA automata are indifferent to probabilities.
\begin{lemma}\label{lemma:indifferent2probabilities}
For any memoryless-stochastically resolvable automaton $\Ac$, if there is a memoryless resolver $\Mc$ that is an almost-sure winning strategy for the HD game over $\Ac$ then any memoryless resolver $\Mc'$ that assigns nonzero probabilities to the same set of transitions of $\Ac$ is also a memoryless resolver for $\Ac$ using which Eve wins the HD game. 
\end{lemma}
To prove \cref{lemma:indifferent2probabilities}, we will use simulation games.

\begin{definition}\label{df:simulation-game}
        The \emph{simulation game} of $\Ac$ by $\Bc$ is a complete-observation non-stochastic game. A play of this game starts with Adam's token and Eve's token in the initial states of $\Bc$ and $\Ac$, respectively, and for infinitely many rounds. In round $i$ of the game when Eve's token is at $q_i$ and Adam's token is at $p_i$: 
    \begin{enumerate}
        \item Adam selects a transition $p_i \xrightarrow{a_i:c} p_{i+1}$ in $\Ac$ along which he moves his token to $p_{i+1}$, on a letter $a_i$;
        \item Eve moves her token along a transition $q_i \xrightarrow{a_i: c'} q_{i+1}$ in $\Bc$ on the same letter.
    \end{enumerate}
The play results in a sequence of Adam's selected transition on $\Ac$, which in turn also forms a word, and the sequence of Eve's selected transitions forms a run on the same word on $\Bc$. Eve wins such a play if Adam's run is rejecting or if Eve's run is accepting. 
\end{definition}
If Eve has a strategy to win the simulation game of $\Ac$ by $\Bc$, then we say that $\Bc$ \emph{simulates} $\Ac$. 

\begin{lemma}\label{lemma:hd-equiv-detsimulation}
     Let $\Ac$ be a nondeterministic parity automaton and $\Dc$ be a deterministic parity automaton that is language equivalent to $\Ac$. Then Eve wins the HD game on $\Ac$ if and only if $\Ac$ simulates $\Dc$.
\end{lemma}
\begin{proof}
    If Eve has a strategy to win the HD game on $\Ac$, then she can use the same strategy to win the simulation game of $\Dc$ by $\Ac$, where she ignores the transitions that Adam's token takes. For the converse, suppose that Eve wins the simulation game of $\Dc$ by $\Ac$ using the strategy $\sigma$. Then, she can use $\sigma$ to play the HD game on $\Ac$, where she picks transitions on her token using $\sigma$ by playing the simulation game against the unique run of $\Dc$ on the word Adam has played in the HD game so far. If Adam's word in the HD game is in the language, then the unique run of $\Dc$ on that word is accepting, and since $\sigma$ is a winning strategy, the run on Eve's token in the HD game is accepting as well.
\end{proof}

We now prove \cref{lemma:indifferent2probabilities} using the proof of \cref{lemma:hd-equiv-detsimulation} above.

\begin{proof}[Proof of \cref{lemma:indifferent2probabilities}]
Let $\Dc$ be a deterministic parity automaton that is language-equivalent to $\Ac$. The simulation game between two parity automata is a Rabin game~\cite[Pages 154-155]{CHP07}. For stochastic Rabin games, the probability distribution of the stochastic nodes does not change the set of vertices from which the player wins almost surely, as long as the set of transitions assigned nonzero probabilities remains the same~\cite[Theorem~1]{BNNSS22}. 

In the proof of \cref{lemma:hd-equiv-detsimulation}, we showed that strategies for Eve in the HD game on $\Ac$ can easily be converted to strategies for Eve in the simulation game of $\Ac$ by $\Dc$ and vice versa. Combining this with the above fact, we reach the desired conclusion.
\end{proof}

