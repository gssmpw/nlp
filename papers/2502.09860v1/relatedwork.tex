\section{Related Work}
\paragraph{AI-aided Drug Molecular Design} 
Current AI-aided drug molecular design techniques can be primarily classified into two categories: deep generative models and combinatorial optimization methods. 

\noindent(I) Deep Generative Models (DGMs) learn the distribution of general molecular structures using deep networks, enabling the generation of molecules by sampling from the learned distribution. Typical algorithms include Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), energy-based models, and flow-based models \citep{gomez2018automatic, jin2018junction, de2018molgan, segler2018generating, fu2020core, honda2019graph, madhawa2019graphnvp, liu2021graphebm,fu2022antibody,chen2024uncertainty,bagal2021liggpt}. However, these approaches often require a smooth and discriminative latent space, necessitating careful network architecture design and well-distributed datasets. This requirement can be restrictive in certain scenarios, such as multi-objective optimization. Furthermore, since DGMs learn the distribution of reference data, their ability to explore diverse chemical space is relatively limited, as demonstrated by recent molecular optimization benchmarks \citep{brown2019guacamol, huang2021therapeutics,gao2022samples}. 

\noindent (II) On the other hand, combinatorial optimization methods directly search the discrete chemical space, mainly including deep reinforcement learning~\citep{You2018-xh,zhou2019optimization,jin2020multi,gottipati2020learning}, evolutionary learning methods~\citep{nigam2019augmenting,jensen2019graph,fu2022reinforced} and sampling methods~\citep{xie2021mars,fu2021mimosa}. Specifically, \citet{graph-ga} have proposed a molecular graph-based genetic algorithm. In drug discovery, this algorithm samples two parent molecules and generates child molecules by combining fragments of the parents, with a probability of random mutations occurring in the offspring. The population is then refined by selecting the highest-scoring molecules.
Also, \citet{xie2021mars} uses Markov chain Monte Carlo method (MCMC) to sample potential molecules. Each sampled molecule forms a Markov chain, modeled as a chemical transformation of the previous sample. This transformation occurs through one of two possible actions: (i) the addition of molecular fragments or (ii) the removal of a chemical bond.

\paragraph{Discrete Sampling}
Many applications involve discrete data spaces, such as molecular, text, and tabular data. Gibbs sampling has long been the standard method for discrete sampling. However, because Gibbs sampling updates only one variable at a time, it often suffers from slow convergence. To address this, various improvements to Gibbs sampling have been proposed. \citet{titsias2017hamming} introduces auxiliary variables to enable block updates within the Gibbs sampler. \citet{nishimura2023prior} reformulates the sample space to simplify the sampling process by using prior preconditioning and conjugate gradient techniques.
Recently, a growing body of work has explored leveraging gradient information to improve sampling in discrete spaces \citep{grathwohl2021gwg, sun2022optimal, pynadath2024gradientbaseddiscretesamplingautomatic}. Among these, the Discrete Langevin Proposal (DLP) \cite{zhang2022langevin} stands out as an analog of Langevin dynamics adapted to discrete spaces. DLP not only utilizes gradient information but also updates all variables simultaneously at each step, leading to better efficiency.