\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage{pgfplots}
\usepackage{enumitem}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[accepted]{icml2024}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{xspace}
\newcommand{\fullname}{Gradient Genetic Algorithm}
\newcommand{\mname}{Gradient GA}
\usepackage[textsize=tiny]{todonotes}
\newcommand{\ruqi}[1]{{\textcolor{blue}{[rz: #1]}}}


\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{\mname: Gradient Genetic Algorithm for Drug Molecular Design}
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Debadyuti Mukherjee}{equal,sch}
\icmlauthor{Chris Zhuang}{equal,sch}
\icmlauthor{Yingzhou Lu}{aa}
\icmlauthor{Tianfan Fu}{bb}
\icmlauthor{Ruqi Zhang}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sch}{Department of Computer Science, Purdue University}
\icmlaffiliation{aa}{Stanford Medicine School}
\icmlaffiliation{bb}{Department of Computer Science, Rensselaer Polytechnic Institute}

\icmlcorrespondingauthor{Debadyuti Mukherjee}{mukher83@purdue.edu}
\icmlcorrespondingauthor{Chris Zhuang}{zhuang80@purdue.edu}
\icmlcorrespondingauthor{Ruqi Zhang}{ruqiz@purdue.edu}


\vskip 0.3in
]
\printAffiliationsAndNotice{\icmlEqualContribution First authorship determined by rolling a dice}

\begin{abstract}
Molecular discovery has brought great benefits to the chemical industry. Various molecule design techniques are developed to identify molecules with desirable properties. Traditional optimization methods, such as genetic algorithms, continue to achieve state-of-the-art results across multiple molecular design benchmarks. However, these techniques rely solely on random walk exploration, which hinders both the quality of the final solution and the convergence speed.
To address this limitation, we propose a novel approach called \emph{\fullname}~(\mname), which incorporates gradient information from the objective function into genetic algorithms. Instead of random exploration, each proposed sample iteratively progresses toward an optimal solution by following the gradient direction. We achieve this by designing a differentiable objective function parameterized by a neural network and utilizing the Discrete Langevin Proposal to enable gradient guidance in discrete molecular spaces.
Experimental results demonstrate that our method significantly improves both convergence speed and solution quality, outperforming cutting-edge techniques. For example, it achieves up to a 25\% improvement in the top-10 score over the vanilla genetic algorithm.
The code is publicly available at \url{https://github.com/debadyuti23/GradientGA}. 
\end{abstract}

\section{Introduction}


Designing molecules with desirable biological and chemical properties has become a demanding research topic since its outcome can benefit various domains, such as drug discovery~\cite{huang2022artificial}, material design~\cite{yang2017chemts}, etc. However, a limited number of molecules can be tested in real-life laboratories \cite{altae2017low} and clinical trials \cite{chen2024uncertainty,chen2024trialbench}. Therefore, numerous effective techniques for molecule discovery are proposed to discover favorable molecules throughout the vast sample space. 

Some evolutionary algorithms, such as molecular graph-based genetic algorithm (Graph GA) \cite{graph-ga}, remain strong performance, often outperforming recently proposed machine learning-based algorithms \cite{huang2021therapeutics,gao2022samples}. Genetic algorithms are cheap, easy to implement, and often regarded as simple baselines for molecular discovery. However, key GA operators, such as selection, crossover, and mutation, are random and do not use knowledge of objective functions. Given the vast molecular search space, this random walk approach is like searching for a needle in a haystack. As a result, GA tends to converge slowly and its final performance can be unstable.

To address this issue, we introduce a novel molecule design method, \emph{\fullname}~(\mname), which leverages gradient information to navigate chemical space efficiently.  First, we learn a differentiable objective function using a graph neural network (GNN) \cite{cite-gnn}, which maps the graph-structured information of molecules to vector embeddings. We then apply the Discrete Langevin Proposal (DLP) \cite{zhang2022langevin} to incorporate gradient information from this objective, enabling more informed exploration in the discrete molecular space. Our main contributions are summarized as follows.
\begin{itemize}
    \item We introduce \mname, a gradient-based genetic algorithm for more informative and effective exploration in molecular spaces, mitigating the random-walk behavior in genetic algorithms. To the best of our knowledge, this is the first method to leverage gradient information within a genetic algorithm framework.
    \item To enable a differentiable objective function for discrete molecular graphs, we use a graph neural network as a property predictor to approximate non-differentiable objectives. This allows us to compute gradients by taking derivatives of NN-parameterized objectives with respect to the vector embeddings.
    \item The experimental results demonstrate that the proposed method achieves a significant and consistent improvement over a number of cutting-edge approaches (e.g., Graph-GA, SMILES-GA). For example, achieving an improvement of up to 25\% over the traditional GA when optimizing the mestranol similarity property. 
\end{itemize}

\section{Related Work}

\paragraph{AI-aided Drug Molecular Design} 
Current AI-aided drug molecular design techniques can be primarily classified into two categories: deep generative models and combinatorial optimization methods. 

\noindent(I) Deep Generative Models (DGMs) learn the distribution of general molecular structures using deep networks, enabling the generation of molecules by sampling from the learned distribution. Typical algorithms include Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), energy-based models, and flow-based models \citep{gomez2018automatic, jin2018junction, de2018molgan, segler2018generating, fu2020core, honda2019graph, madhawa2019graphnvp, liu2021graphebm,fu2022antibody,chen2024uncertainty,bagal2021liggpt}. However, these approaches often require a smooth and discriminative latent space, necessitating careful network architecture design and well-distributed datasets. This requirement can be restrictive in certain scenarios, such as multi-objective optimization. Furthermore, since DGMs learn the distribution of reference data, their ability to explore diverse chemical space is relatively limited, as demonstrated by recent molecular optimization benchmarks \citep{brown2019guacamol, huang2021therapeutics,gao2022samples}. 

\noindent (II) On the other hand, combinatorial optimization methods directly search the discrete chemical space, mainly including deep reinforcement learning~\citep{You2018-xh,zhou2019optimization,jin2020multi,gottipati2020learning}, evolutionary learning methods~\citep{nigam2019augmenting,jensen2019graph,fu2022reinforced} and sampling methods~\citep{xie2021mars,fu2021mimosa}. Specifically, \citet{graph-ga} have proposed a molecular graph-based genetic algorithm. In drug discovery, this algorithm samples two parent molecules and generates child molecules by combining fragments of the parents, with a probability of random mutations occurring in the offspring. The population is then refined by selecting the highest-scoring molecules.
Also, \citet{xie2021mars} uses Markov chain Monte Carlo method (MCMC) to sample potential molecules. Each sampled molecule forms a Markov chain, modeled as a chemical transformation of the previous sample. This transformation occurs through one of two possible actions: (i) the addition of molecular fragments or (ii) the removal of a chemical bond.

\paragraph{Discrete Sampling}
Many applications involve discrete data spaces, such as molecular, text, and tabular data. Gibbs sampling has long been the standard method for discrete sampling. However, because Gibbs sampling updates only one variable at a time, it often suffers from slow convergence. To address this, various improvements to Gibbs sampling have been proposed. \citet{titsias2017hamming} introduces auxiliary variables to enable block updates within the Gibbs sampler. \citet{nishimura2023prior} reformulates the sample space to simplify the sampling process by using prior preconditioning and conjugate gradient techniques.
Recently, a growing body of work has explored leveraging gradient information to improve sampling in discrete spaces \citep{grathwohl2021gwg, sun2022optimal, pynadath2024gradientbaseddiscretesamplingautomatic}. Among these, the Discrete Langevin Proposal (DLP) \cite{zhang2022langevin} stands out as an analog of Langevin dynamics adapted to discrete spaces. DLP not only utilizes gradient information but also updates all variables simultaneously at each step, leading to better efficiency.  

\section{Preliminaries}
\subsection{Discrete Langevin Proposal}\label{sec:dlp formula}

Suppose that the target distribution is $\pi(v)\propto\exp (U(v))$, where $U(\cdot)$ is the energy function,  $v$ is an $n$-dimensional variable in the space $\mathbb{R}^n$.  Langevin Dynamics samples from $\pi$ by iteratively updating $v$ as follows:
\begin{equation}
    \label{eq:langevin}
    v' = v + \frac{\alpha}{2}\nabla U(v)+\sqrt{\alpha}\varepsilon , \ \ \varepsilon \sim \mathcal{N}(0,I_{n\times n}),
\end{equation}
where $\alpha$ is the step size; $I_{n\times n}$ is $n$-dimensional identity matrix; $\mathcal{N}(\cdot, \cdot)$ denotes high-dimensional normal distribution. 


From Equation~\ref{eq:langevin}, the probability of selecting $v'$ from $V$, i.e., $p(v'|v)$, can be written as 
\begin{equation}
    p(v'|v) \propto \exp{(-\frac{1}{2\alpha}||v'-v-\frac{\alpha}{2}\nabla U(v)||^2_2)}, 
\end{equation}
The distribution $p(v'|v)$ has its mean shifted from $v$ toward the optimum due to the gradient term. Therefore, high-probability samples from $p(v'|v)$ will be closer to the optimum compared to $v$.

To extend Langevin Dynamics to discrete space, \citet{zhang2022langevin} has suggested the following proposal for the discrete sample space $S$:
\begin{equation}
\label{eq:dlp}
    p(v'|v) =  \frac{\exp{(-\frac{1}{2\alpha}||v'-v-\frac{\alpha}{2}\nabla U(v)||^2_2)}}{\sum_{v'' \in S}[\exp{(-\frac{1}{2\alpha}||v''-v-\frac{\alpha}{2}\nabla U(v)||^2_2)}]}. 
\end{equation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{GradientGA3.pdf}
    \vspace{-0.5cm}
    \caption{\mname~pipeline.}
    \label{fig:arch}
\end{figure*}
\subsection{Genetic Algorithm}
The genetic algorithm (GA) is a traditional combinatorial optimization method motivated by natural selection and biological evolution processes. Unlike neural network-based methods, GA does not have learnable parameters, is usually easy to implement and tune, and bypasses the overfitting issue. 
Specifically, a GA process starts by randomly sampling a \textit{population} of candidates. 
In drug design, for example, the candidates can be drug molecules. 
In the $t$-th \textit{generation} (iteration), given the population of candidates, GA follows three key steps. 
\begin{enumerate}
 \item \textbf{Crossover}, also called recombination, exchanges the structure of two parents to generate new children. Specifically, two parents are randomly selected from the population, and their molecular structures are partially swapped to create two child molecules. The crossover operators are carried out multiple times independently, and the generated children are added to the offspring set $\mathcal{S}^{(t)}$.
 \item \textbf{Mutation} operates on a single parent molecule (randomly selected from the population) and modifies its structure slightly via randomly selecting a substructure and flipping it to a new substructure different from its original state. Like the crossover, the mutation is conducted multiple times independently, and the resulting offspring are retained in the offspring set $\mathcal{S}^{(t)}$.
 \item \textbf{Evolution}. Given a population of molecules $\mathcal{S}^{(t)}$ at the $t$-th generation, we generate an offspring pool by applying crossover and mutation operations. Molecules with undesirable properties (e.g., poor solubility or high toxicity in drug discovery) are filtered out, and the top $k$ candidates are selected to form the next generation population $\mathcal{S}^{(t+1)}$. 
\end{enumerate}


\section{Methodology: \fullname}
\paragraph{Overview} In this section, we describe \fullname (\mname). First, Section~\ref{sec:formulation} formulates molecular design. 
Then, in Section~\ref{sec:gradient}, we discuss how to derive gradient via projecting discrete molecular graph to embedding in continuous space. 
Then, Section~\ref{sec:sampling} discusses how to leverage embedding-derived gradient to navigate the molecular space efficiently. 
For ease of exposition, all the mathematical notations are explained in Table~\ref{table:notation}. 
Figure~\ref{fig:arch} illustrates the whole pipeline. Algorithm~\ref{alg:main} demonstrates the pseudocode of the whole pipeline. 

\begin{table}[tb]
\centering
\caption{Mathematical notations and their explanations. }
\vspace{1mm}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|p{0.74\columnwidth}}
\toprule[1pt]
Notations & Explanations \\ 
\hline 
$\mathcal{O}$ & oracle function \\
$\mathcal{Q}$ & molecular space \\
$X$ & molecule \\
$\mathcal{M}$ & complete MPNN-based model for oracle prediction\\ 
$G(\cdot)$ & graph representation \\
$G_X$ & molecular graph of $X$ \\ 
$L_1, L_2$ & linear layer \\
$v$ & embedding\\
$\pi(\cdot)$ &  target distribution (normalized score)\\ 
$U(\cdot)$ & energy function  \\ 
$D$ & population, a set of molecules. \\ 
$D'$ & new molecule set \\ 
$D''$ & Re-training molecule set \\ 
$n$ & embedding size \\ 
$k$ & number of generated samples at each step\\ 
$T$ & threshold criterion to add to training set \\ 
$\tau$ & retrainable threshold \\ 
$\alpha$ & step size for sampling \\
\bottomrule[1pt]
\end{tabular}}
\label{table:notation}
\end{table}

\subsection{Formulation: Molecular Design}
\label{sec:formulation}
Drug molecular design aims at identifying novel molecules with desirable pharmaceutical properties, which are evaluated by \textit{oracle}. 
Oracles serve as objective functions in molecular optimization tasks, formally defined as follows. 
\begin{definition}[Oracle]
\label{def:oracle}
Oracle $\mathcal{O}(\cdot): \mathcal{Q} \xrightarrow[]{} \mathbb{R}$ is a black-box function that evaluates certain physical, chemical, or biological properties of a molecule $X$ and yields the ground-truth property $\mathcal{O}(X)$. 
\end{definition}
In drug discovery, all the oracles can be categorized into two classes based on their accessibility: computational and experimental (wet-lab). 
Experimental oracles, e.g., \textit{in vivo} experiment, typically require wet-lab experiments to evaluate, which is too expensive and time-consuming. Following most machine learning-aided drug discovery papers, we focus on computational oracles that are easy to evaluate \textit{in silico}, such as molecular docking and the quantitative estimate of drug-likeness (QED)~\citep{bickerton2012quantifying}. 
In real-world drug discovery scenarios, the cost of acquiring oracle evaluations is typically significant and cannot be overlooked. 
Mathematically, the drug molecular design problem (a.k.a. molecular optimization) can be formulated as 
\begin{equation}
\label{eqn:denovo}
\begin{aligned}
\underset{X \in \mathcal{Q}}{\arg\max} \ \mathcal{O}(X), 
\end{aligned}
\end{equation} 
where $X$ is a molecule, $\mathcal{Q}$ denotes the whole molecular space, i.e., the set of all chemically valid molecules. The size of the whole molecular space is around $10^{60}$~\cite{bohacek1996art}. 
Following MARS \cite{xie2021mars}, we regard $\mathcal{O}(\cdot)$ as an unnormalized probability distribution and introduce a vector embedding $v$ for each molecule $X$. The target distribution is then defined as $\pi(v) \propto \mathcal{O}(X)$.
 

\paragraph{Gradient Definition in Molecular Design}



We now define the gradient information used to guide exploration in molecular spaces. To apply (discrete) Langevin dynamics, we need the gradient of the energy function. Since $\pi(v) \propto \exp(U(v))$, by the chain rule, we have  
\begin{equation}
    \label{eq:chain-rule}
    \nabla U(v) = \frac{\nabla \pi(v)}{\pi(v)}.  
\end{equation}

Given that $\pi(v) \propto \mathcal{O}(X)$, this leads to  
\begin{equation}
    \label{eq:grad-compute}
    \nabla U(v) = \frac{\nabla \mathcal{O}(X)}{\mathcal{O}(X)} = \frac{\nabla f(v)}{\mathcal{O}(X)},
\end{equation}  
where $f$ is a differentiable function that approximates the oracle $\mathcal{O}$, i.e., $f(v) = \mathcal{O}(X)$ for all $X \in \mathcal{Q}$. We will discuss how to obtain both $f$ and $v$ in the next section.  

It is also possible to define $U(v) = \mathcal{O}(X)$ and $\nabla U(v) = \nabla f(v)$. However, this formulation does not incorporate the oracle value into the gradient. Our approach includes $\mathcal{O}(X)$ in the denominator, effectively playing the role of adaptive step sizes. We found that this formulation leads to better performance. An empirical comparison is provided in the Appendix~\ref{appendixc}. 

\subsection{Gradient Computation}
\label{sec:gradient}

Implementing gradient-based methods in molecular discovery is a challenging task due to two primary obstacles: (1) representing sample molecules in a vector format suitable for gradient-based methods, and (2) establishing a differentiable relationship between the probability distribution and the vector representation.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{DLP_Sampling.png}
    \vspace{-1cm}
    \caption{Overview of DLP-based sampling procedure in \mname, illustrating how the sampled molecule moves toward the optimum.}
    \label{fig:sampling}
    \vspace{-0.5cm}
\end{figure*}

\paragraph{Finding the Embedding} 
Most gradient-based methods use fixed-length vectors, while molecular data are often represented as graph structures. Therefore, an efficient mapping function is needed to convert molecular graphs into fixed-length embeddings. However, directly transforming a graph into an embedding using hard-coded functions, such as Morgan's fingerprint, can cause unnecessary aggregation and information loss~\cite{lu2019integrated}.  
To overcome this issue, we use the Message Passing Neural Network (MPNN) \cite{gilmer2017neural}, which is a state-of-the-art approach for molecular activity prediction. We adopt the MPNN architecture from \citet{xie2021mars}, which consists of (1) a simple neural network for message passing between neighboring nodes and (2) a Gated Recurrent Unit (GRU) \cite{cho2014properties} for updating node representations.  
For the readout function, we use Set2Set \cite{vinyals2015order}, which is particularly effective for isomorphic graphs due to its set-invariant property. To reduce the dimensionality of the Set2Set output, we introduce a two-layer multilayer perceptron (MLP) $L_1$, which compresses the output dimension from $2n$ to $n$.

We define the graph representation function, which converts molecular data into graph data (nodes representing atomic features and edges representing bond features), as $G(\cdot)$. Consequently, the output of $L_1$ serves as the embedding $v$ for the molecular graph $G_X = G(X)$, as shown below.  

\begin{equation}
    \label{eq:eval-graph2vec}
    h = \text{MPNN}(G_X), v = L_1(h).  
\end{equation}

\paragraph{Finding the Gradient}
Similar to the vector embedding problem, we can learn the differentiable approximation $f(\cdot)$ in Equation~\ref{eq:grad-compute} using an MLP-based architecture. After the first two-layer MLP $L_1$, we introduce another two-layer MLP $L_2$, which produces a scalar output $\widehat{y}$. The gradient $\nabla f(\cdot)$ can then be computed through backpropagation, as illustrated in Figure~\ref{fig:arch}.  
Our objective is to make $\widehat{y}$ approximate the behavior of the oracle function $\mathcal{O}$ so that the entire architecture (from the MPNN to $L_2$), denoted by $\mathcal{M}$, effectively learns both the embedding and the differentiable objective function. To introduce nonlinearity, we apply LeakyReLU and sigmoid activation functions before and after $L_2$, respectively.  
With $v$ and $\nabla U(\cdot)$ now properly defined for Equation~\ref{eq:dlp}, we proceed to describe the training procedure. We train the model $\mathcal{M}$ to fit the oracle function $\mathcal{O}$ using the initial molecule population $D$. The loss function is defined as the mean squared error (MSE):  
\[
\mathcal{L} = \frac{1}{|D|} \sum_{X \in D} ||\mathcal{M}(G_X) - \mathcal{O}(X)||^2.
\]  
The initial molecule population is randomly sampled from drug-like molecule databases, such as ZINC~\cite{sterling2015zinc}. During the molecular optimization process, we continuously expand the training set by adding newly generated molecules that surpass a threshold criterion $T$. This provides additional information about the distribution of molecules and allows us to retrain the model $\mathcal{M}$.  


\subsection{Iterative Sampling}
\label{sec:sampling}
We propose a sampling technique inspired by both Graph GA \cite{graph-ga} and DLP \cite{zhang2022langevin}. The workflow for each iteration is illustrated in Figure~\ref{fig:sampling}. Similar to Graph GA, we begin by selecting parent molecules based on their scores $\mathcal{O}(\cdot)$ and generating child molecules through crossover. We define the sample space $S$ for DLP as the set of all possible crossovers between the selected parents $d_1$ and $d_2 \, (\in D)$.  
Ideally, both parents should be considered as current samples. However, since DLP is designed to use a single sample, we aggregate the information from both parents into a single embedding $v$ and gradient $\nabla U(v)$ using the following equation:  
\begin{equation}
    \label{eq:aggr_parent}
    \{v, \nabla U(v)\} = \sum_{i=1,2} w_i \cdot \{v_i, \nabla U(v_i)\},
\end{equation}  
where $\{v_i, \nabla U(v_i)\}$ represents the embedding and gradient information for parent $d_i$. Empirically, we found that a simple strategy of using only the best parent as the current sample works well. Specifically, the weights $w_i$ are assigned as follows: $w_i = 1$ if $i = \arg\max(\mathcal{O}(d_i))$, and $w_i = 0$ otherwise. Applying DLP updates the embedding $v'$ by moving it closer to the optimum, guided by the gradient information. 

In the final step, DLP generates the next sample set $D'$ of fixed size $k$ from the sample space $S$. Following the Graph GA approach, each molecule in $D'$ is mutated. Before the next iteration begins, we update both the population and the model. The population $D$ is refreshed by selecting the top $|D|$ molecules based on their oracle scores from the combined set $\{D, D'\}$. 
To further enhance the graph embedding model $\mathcal{M}$'s understanding of the target molecule distribution $\pi(\cdot)$, we retrain $\mathcal{M}$ using a training set $D''$, which is updated in each iteration according to the following rule:  
\begin{equation}
    \label{eq:retrain-set}
    D'' = D'' \cup \{d \ | \ d \in D' \ \text{and} \ T(d)\},  
\end{equation}  
where $T(d)$ is a threshold criterion for adding new samples to the training set.  
The complete \mname~workflow is detailed in Algorithm~\ref{alg:main}.

\begin{algorithm}[t]
  \caption{\fullname}
  \label{alg:main}
  \begin{algorithmic}
    \STATE \textbf{Input}: oracle function $\mathcal{O}$, step size $\alpha$, retrainable threshold $\tau$ 
    \STATE Initialize $D \gets$ original population
    \STATE Initialize new molecule set $D' \gets \{\}$
    {\STATE Train predictive model ${\mathcal{M}}$ using\\ ${\{(G(d), \mathcal{O}(d)) \ \ \forall d \in D\}}$}
    \STATE Initialize retrained molecule set $D'' \gets \{\}$
    
    \FOR{\texttt{$t=1,2,\dots$}}
      \STATE $p(d) \propto \mathcal{O}(d) \ \ \ \ \forall d \in D$
      \STATE Parent molecules $d_1,d_2 \sim p(d) \ \ \ \ $ [ $d \in D$]
      \STATE Get parents' embedding $v_i$ for each $G(d_i)$ using Eq.~\ref{eq:eval-graph2vec}
      \STATE Get parents' gradient $\nabla U(v_i)$ for each $d_i$ using Eq.~\ref{eq:grad-compute}
      \STATE Evaluate $v,\nabla U(v)$ using Eq.~\ref{eq:aggr_parent}
      \STATE Get crossover set: $S \gets \texttt{CROSSOVER}(d_1,d_2)$
      \STATE Get sampling probability $probs$ of $S$ using Eq.~\ref{eq:dlp}
      \STATE Evaluate sample set $D' \gets \texttt{Sample}(S,probs,k)$
      \STATE Mutate each molecule in $D'\gets \texttt{MUTATE}(D')$
      \STATE Update population $D \gets \texttt{toporacle}(\{D,D_1\},|D|)$
      \STATE Update training set $D''$ with $T$ using Eq.~\ref{eq:retrain-set}
      \IF{$|D''| \geq \tau$}
        \STATE Retrain model $\mathcal{M}$ with $\{(d, \mathcal{O}(d)) \ \ \forall d \in D''\}$
      \ENDIF
        \STATE $D'' \gets \{\}$ or $D'' \gets D''$
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

\section{Experiments}

\subsection{Experimental Setup}

\noindent\textbf{Baseline Methods. }
We use the practical molecule optimization (PMO) benchmark~\cite{gao2022sampleefficiencymattersbenchmark} as our code base to compare results between the state-of-the-art methods. We select (1) genetic algorithm, including Graph GA (molecular graph-level genetic algorithm) method~\cite{jensen2019graph} and SMILES GA (SMILES string-level genetic algorithm), (2) sampling-based methods, including MIMOSA (Multi-constraint Molecule Sampling)~\cite{fu2021mimosa}, MARS (Markov Molecular Sampling)~\cite{xie2021mars} and 
(3) gradient-based method, DST (Differentiable Scaffolding Tree)~\cite{fu2021differentiable}. 
% , REINVENT~\cite{olivecrona2017molecular}. 
We use the default setup from PMO~\cite{gao2022sampleefficiencymattersbenchmark}.

\noindent\textbf{Dataset.} For all methods, we use the ZINC 250K database~\cite{irwin2012zinc} to select the initial molecule population, extract chemical fragments, and perform pretraining. 


\begin{table*}[htp]
    \centering
    \small\setlength\tabcolsep{4.5pt}
    \caption{Comparison of Average Top 10, AUC Top 1, AUC Top 10, and AUC Top 100 with several GuacaMol objectives (mestranol similarity, amlodipine MPO, perindopril MPO, deco hop, median1, and isomers c9h10n2o2pf2cl) under 2500 oracle calls. For each metric, the best method is \textbf{bolded}. We conduct five independent runs using different random seeds for each method, and report the average scores and their standard deviation. }
    \vspace{0.2cm}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ c | c c c c | c c c c}
    \toprule
      Method   & \multicolumn{4}{c}{mestranol similarity} & \multicolumn{4}{c}{amlodipine MPO}\\ 
        & Average Top 10 & AUC Top 1 & AUC Top 10 & AUC Top 100 & Average Top 10 & AUC Top 1 & AUC Top 10 & AUC Top 100\\
      \midrule
      \mname~& \textbf{0.5130$\pm$0.0393} & \textbf{0.4433$\pm$0.0310} & \textbf{0.4082$\pm$0.0315} & \textbf{0.3534$\pm$0.0355} & \textbf{0.5667$\pm$0.0336} & \textbf{0.5614$\pm$0.0177} & \textbf{0.5176$\pm$0.0187} & 0.4658$\pm$0.0199 \\ 
      Graph GA & 0.4452$\pm$0.0241 & 0.3556$\pm$0.0268 & 0.3208$\pm$0.0199 & 0.2717$\pm$0.0147 & 0.5605$\pm$0.0364 & 0.5067$\pm$0.0270 & 0.4734$\pm$0.0215 & 0.4152$\pm$0.0142 \\ 
      SMILES GA & 0.2582$\pm$0.0097 & 0.3777$\pm$0.0381 & 0.3634$\pm$0.0352 & 0.3347$\pm$0.0279 & 0.4480$\pm$0.0161 & 0.5016$\pm$0.0156 & 0.4956$\pm$0.0143 & \textbf{0.4748$\pm$0.0158}\\ 
      MIMOSA & 0.4262$\pm$0.0246 & 0.4162$\pm$0.0115 & 0.3619$\pm$0.0181 & 0.2887$\pm$0.0252 & 0.5245$\pm$0.0143 & 0.5431$\pm$0.0261 & 0.4953$\pm$0.0109 & 0.4436$\pm$0.0075 \\
      MARS & 0.3411$\pm$0.0160 & 0.3760$\pm$0.0003 & 0.3215$\pm$0.0096 & 0.2523$\pm$0.0081 & 0.4843$\pm$0.0210 & 0.4812$\pm$0.0144 & 0.4583$\pm$0.0098 & 0.3816$\pm$0.0157 \\
      DST & 0.4131$\pm$0.0179 & 0.4148$\pm$0.0323 & 0.3507$\pm$0.0088 & 0.2780$\pm$0.0029 & 0.5192$\pm$0.0122 & 0.5411$\pm$0.0303 & 0.4908$\pm$0.0115 & 0.4257$\pm$0.0044  \\
      \bottomrule
    \end{tabular}
    }
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ c | c c c c | c c c c}
    \toprule
         & \multicolumn{4}{c}{perindopril MPO} & \multicolumn{4}{c}{deco hop}\\ 
        & Average Top 10 & AUC Top 1 & AUC Top 10 & AUC Top 100 & Average Top 10 & AUC Top 1 & AUC Top 10 & AUC Top 100\\
    \midrule
    \mname~& 0.4786$\pm$0.0257 & \textbf{0.4542$\pm$0.0164} & \textbf{0.4361$\pm$0.0176} & 0.3882$\pm$0.0193 & 0.6026$\pm$0.0053 & \textbf{0.5883$\pm$0.0032} & 0.5763$\pm$0.0050 & 0.5602$\pm$0.0053 \\

    Graph GA & \textbf{0.4788$\pm$0.0067} & 0.4519$\pm$0.0055 & 0.4317$\pm$0.0045 & 0.3770$\pm$0.0049 & \textbf{0.6039$\pm$0.0043} & 0.5186$\pm$0.0037 & 0.5028$\pm$0.0032 & 0.4708$\pm$0.0033 \\

    SMILES GA & 0.3698$\pm$0.0117 & 0.4346$\pm$0.0124 & 0.4271$\pm$0.0115 & \textbf{0.4065$\pm$0.0102} & 0.5548$\pm$0.0059 & 0.5862$\pm$0.0047 & \textbf{0.5817$\pm$0.0042} & \textbf{0.5733$\pm$0.0036} \\

    MIMOSA & 0.4629$\pm$0.0176 & 0.4500$\pm$0.0144 & 0.4289$\pm$0.0116 & 0.3783$\pm$0.0085 & 0.6008$\pm$0.0053 & 0.5882$\pm$0.0061 & 0.5773$\pm$0.0035 & 0.5600$\pm$0.0021 \\

    MARS & 0.4564$\pm$0.0167 & 0.4538$\pm$0.0087 & 0.4278$\pm$0.0065 & 0.3648$\pm$0.0042 & 0.5944$\pm$0.0070 & 0.5830$\pm$0.0227 & 0.5711$\pm$0.0301 & 0.5493$\pm$0.0421 \\
    
    DST & 0.4615$\pm$0.0100 & 0.4530$\pm$0.0041 & 0.4210$\pm$0.0041 & 0.3564$\pm$0.0028 & 0.6034$\pm$0.0083 & 0.5860$\pm$0.0071 & 0.5721$\pm$0.0025 & 0.5518$\pm$0.0009\\
    \bottomrule 
    \end{tabular}
    }
        \resizebox{\linewidth}{!}{%
    \begin{tabular}{ c | c c c c | c c c c}
    \toprule
         & \multicolumn{4}{c}{median1} & \multicolumn{4}{c}{isomers c9h10n2o2pf2cl}\\ 
        & Average Top 10 & AUC Top 1 & AUC Top 10 & AUC Top 100 & Average Top 10 & AUC Top 1 & AUC Top 10 & AUC Top 100\\
    \midrule
        \mname~& \textbf{0.3033$\pm$0.0074} & \textbf{0.2581$\pm$0.0115} & \textbf{0.2298$\pm$0.0151} & \textbf{0.1906$\pm$0.0183} & 0.7783$\pm$0.0959 & 0.6628$\pm$0.0731 & 0.5444$\pm$0.0693 & 0.4033$\pm$0.0614 \\

        Graph GA & 0.2599$\pm$0.0182 & 0.2315$\pm$0.0206 & 0.1959$\pm$0.0148 & 0.1442$\pm$0.0070 & 0.7222$\pm$0.1119 & 0.6648$\pm$0.0957 & 0.5436$\pm$0.0770 & 0.3891$\pm$0.0425 \\

        SMILES GA  & 0.1310$\pm$0.0172 & 0.1832$\pm$0.0281 & 0.1795$\pm$0.0272 & 0.1697$\pm$0.0251& 0.3180$\pm$0.3583 & \textbf{0.8244$\pm$0.0848} & \textbf{0.7825$\pm$0.0752} & \textbf{0.7055$\pm$0.0668} \\ 

        MIMOSA & 0.2391$\pm$0.0080 & 0.2271$\pm$0.0103 & 0.1969$\pm$0.0044 & 0.1537$\pm$0.0030 & \textbf{0.7866$\pm$0.0824} & 0.6965$\pm$0.0562 & 0.5949$\pm$0.0440& 0.3965$\pm$0.0265 \\

        MARS & 0.2094$\pm$0.0181 & 0.2239$\pm$0.0140 & 0.2019$\pm$0.0116 & 0.1671$\pm$0.0158 & 0.6639$\pm$0.1606 & 0.6751$\pm$0.1032 & 0.5909$\pm$0.1057 & 0.4424$\pm$0.1499 \\
        
        DST & 0.2179$\pm$0.0162 & 0.2097$\pm$0.0086 & 0.1765$\pm$0.0021 & 0.1331$\pm$0.0024 & 0.6748$\pm$0.0304 & 0.6305$\pm$0.0435 & 0.4932$\pm$0.0216 & 0.2293$\pm$0.0093\\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab:results_comp}
\end{table*}

\noindent\textbf{Evaluation Metrics.} 
We consider the following evaluation metrics to assess the quality of generated molecules. 
\begin{enumerate}
\item \textbf{Average Top-$K$} ($K=10$) is the top-$K$ average property value, which measures the algorithm's optimization ability. We limit the number of oracle calls to 2,500 to mimic the real experimental setup, though we expect methods to optimize well within hundreds of calls.
\item \textbf{AUC top-$K$} ($K=1,10,100$). Following the setup of practical molecular optimization benchmark~\cite{gao2022samples}, we assess both optimization ability and sample efficiency using the area under the curve (AUC) of top-$K$ average property value versus the number of oracle calls (\textit{AUC top-$K$}) as the primary metric for measuring performance. Unlike using top-$K$ average property, AUC rewards methods that reach high values with fewer oracle calls. We use $K=10$  as it is useful to identify a small number of distinct molecular candidates to progress to later stages of development. The reported values of AUC are scaled from min to max at $[0, 1]$.
\item \textbf{Diversity} of generated molecules is defined as the average pairwise Tanimoto distance between the Morgan fingerprints, formulated as  
\begin{equation}
\text{diversity} = 1 - \frac{1}{|\mathcal{Z}|(|\mathcal{Z}|-1)}\sum_{\mathbf{z}_1,\mathbf{z}_2 \in \mathcal{Z},\newline \mathbf{z}_1 \neq \mathbf{z}_2} \text{sim}(\mathbf{z}_1,\mathbf{z}_2),
\end{equation}
where $\mathcal{Z}$ is the set of generated molecules to evaluate and $\text{sim}(\mathbf{z}_1,\mathbf{z}_2)$ is the Tanimoto similarity between molecule $\mathbf{z}_1$ and $\mathbf{z}_2$. Diversity score ranges from 0 to 1, with higher values indicating greater diversity
 \item \textbf{Synthetic accessibility (SA)} measures the difficulty of synthesizing the given molecule. The SA score values range from 1 to 10, where lower values indicate molecules that are easier to synthesize.
\end{enumerate}
All these metrics can be calculated via the evaluation function in Therapeutics data commons (TDC)~\cite{huang2021therapeutics,huang2022artificial}
\footnote{\url{https://tdcommons.ai/functions/data_evaluation/} and \url{https://tdcommons.ai/functions/oracles/}}.

\noindent\textbf{Implementation Details}. 
We select Mean Squared Error (MSE) and Adam \cite{kingma2014adam} with a learning rate of 0.001 as the loss function and optimizer, respectively, when training $\mathcal{M}$. The model is trained for 200 epochs for each round of training. The MPNN has 2 layers of convolution and GRU with 1 layer of Set2Set. There are $n$ = 16 hidden features. The upper bound of the generated molecule $k$ is set to 70. The threshold criterion $\tau$ for a good score of a molecule is set as the maximum metric score $-0.001$. We have two experimental setups depending on the parameter $D''$. The first setup is to clear all the molecules that meet $\tau$ and retrain whenever there is an adequate amount. The second setup is to store the $D''$ and retrain at set oracle steps (default = 500 oracle calls). For simplicity, we take the setup of keeping $D''$ over each oracle call to have more consistent training at each iteration. We provide detailed tables of results for both setups in Appendix~\ref{appendix}. Both setups use the default Graph GA settings from PMO. 



\subsection{Experimental Results}
\label{exp_res}
To evaluate the overall performance of our method, we examine various metrics across multiple oracles. Table~\ref{tab:results_comp} presents a comparative analysis based on Average Top 10, AUC Top 1, AUC Top 10, and AUC Top 100 scores.  
We observe that for most oracles, \mname~outperforms Graph GA, MIMOSA, MARS, and SMILES GA. Our method consistently achieves either the best or second-best performance. Specifically, in Average Top 10, \mname~demonstrates the highest performance, with Graph GA following behind. This highlights the advantage of incorporating gradient information, enabling more efficient and effective exploration of the local search space.  
The superiority of \mname~is further supported by the AUC Top-K scores, which reward methods that reach high values with fewer oracle calls. For AUC Top 1 and AUC Top 10, \mname~dominates the rankings, demonstrating faster convergence and better optimization. In AUC Top 100, \mname~achieves a top score, tying with SMILES GA. However, while SMILES GA performs well in this metric, it significantly underperforms in Average scores.
These results confirm that \mname~is the most effective method overall for optimizing molecular properties across different oracles.

\begin{figure}[h!]
    \centering
    \caption{Comparison of Mestranol Similarity AUC Top 10 scores as the number of oracle calls increases.}
    \label{fig:ms_auc10}
    \resizebox{\linewidth}{!}{
    \begin{tikzpicture}
    \begin{axis}[
        title={Mestranol Similarity vs Oracle Calls},
        xlabel={Oracle Calls},
        ylabel={AUC Top 10 Scores},
        xmin=0, xmax=2500,
        ymin=0, ymax=0.5,
        xtick={0,500,1000,1500,2000,2500},
         ytick={0,.1,.2,.3,.4,.5},
        legend pos=north west,
    ]
    \addplot+[
  blue, mark options={blue, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = index, y = GNLS,col sep=comma]{./data/ms_auc10.csv};
    \addplot+[
  red, mark options={red, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = gaindex, y = GA,col sep=comma]{./data/ms_auc10.csv};
    \addplot+[
  black, mark options={black, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = mindex, y = mimosa,col sep=comma]{./data/ms_auc10.csv};
        \addplot+[
  green, mark options={green, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = marsindex, y = mars,col sep=comma]{./data/ms_auc10.csv};
    \addplot+[
  purple, mark options={purple, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = smilesindex, y = smilesga,col sep=comma]{./data/ms_auc10.csv};
    \legend{\mname,Graph GA,MIMOSA,MARS,SMILES GA}
        \addplot+[
  pink, mark options={pink, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = dstindex, y = dst,col sep=comma]{./data/ms_auc10.csv};
    \legend{\mname,Graph GA,MIMOSA,MARS,SMILES GA,DST}
    \end{axis}
    \end{tikzpicture}
    }
\end{figure}
\begin{figure}[h!]
    \centering
    \caption{Comparison of Mestranol similarity AUC Top 100 scores as the number of oracle calls increases.}
    \label{fig:ms_auc100}
    \resizebox{\linewidth}{!}{
    \begin{tikzpicture}
    \begin{axis}[
        title={Mestranol Similarity vs Oracle Calls},
        xlabel={Oracle Calls},
        ylabel={AUC Top 100 Scores},
        xmin=0, xmax=2500,
        ymin=0, ymax=0.5,
        xtick={0,500,1000,1500,2000,2500},
         ytick={0,.1,.2,.3,.4,.5},
        legend pos=north west,
    ]
    \addplot+[
  blue, mark options={blue, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = index, y = GNLS,col sep=comma]{./data/ms_auc100.csv};
    \addplot+[
  red, mark options={red, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = gaindex, y = GA,col sep=comma]{./data/ms_auc100.csv};
    \addplot+[
  black, mark options={black, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = mindex, y = mimosa,col sep=comma]{./data/ms_auc100.csv};
    \addplot+[
  green, mark options={green, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = marsindex, y = mars,col sep=comma]{./data/ms_auc100.csv};
        \addplot+[
  purple, mark options={purple, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = smilesindex, y = smilesga,col sep=comma]{./data/ms_auc100.csv};
    \addplot+[
  pink, mark options={pink, scale=0.75},smooth, error bars/.cd, y fixed,y dir=both, 
    y explicit]
        table[x = dstindex, y = dst,col sep=comma]{./data/ms_auc100.csv};
    \legend{\mname,Graph GA,MIMOSA,MARS,SMILES GA,DST}
    \end{axis}
    \end{tikzpicture}
    }
\end{figure}
\subsubsection{Oracle Call Efficiency}
To demonstrate that using gradient information accelerates convergence, we conduct experiments measuring AUC Top 10 and AUC Top 100 scores as the number of oracle calls increases. All methods are evaluated with 2,500 oracle calls over 5 runs. Our primary focus is on the bio-objective Mestranol Similarity.  
Figures~\ref{fig:ms_auc10} and~\ref{fig:ms_auc100} show that after the initialization phase, where each method achieves a baseline score based on the initial oracle calls, \mname~consistently outperforms almost all other methods at each step. This indicates that \mname~is not only more effective at finding optimal values but also more efficient, due to its use of gradient guidance rather than random walk exploration. 
\begin{figure}[t]
    \centering
    \caption{Heatmap of synthetic accessibility (SA) score of all methods and oracles. }
    \includegraphics[width=1\linewidth]{heatmap.pdf}
    \label{fig:heatmap}
\end{figure}

\begin{figure}[t]
    \centering
    \caption{Heatmap of Diversity score of all methods and oracles.}
    \includegraphics[width=1\linewidth]{heatmap_div.pdf}
    \label{fig:heatmap_div}
\end{figure}
\subsubsection{Synthetic Accessibility and Diversity}
We further analyze additional metrics, synthetic accessibility (SA) in Figure~\ref{fig:heatmap} and diversity in Figure~\ref{fig:heatmap_div}. It is important to note that these metrics are not explicitly optimized in our objective function. Therefore, their performance is a byproduct of the discovered molecules rather than a direct outcome of our method.  
From Figure~\ref{fig:heatmap}, we observe that the SA scores of \mname~are comparable to those of Graph GA, indicating that there is no significant trade-off between improved performance and SA score. Additionally, in terms of overall SA performance, \mname~is also close to DST, another gradient-based method.  
In Figure~\ref{fig:heatmap_div}, we observe that the diversity score for \mname~is lower than that of other methods. This outcome is expected, as our approach samples molecules near high-performing parent molecules. While this may reduce diversity, it can be advantageous when the goal is to perform a fine-grained local search over good regions.










\section{Conclusion and Future Work}
Genetic Algorithm (GA) is dominating drug molecular design thanks to its flexibility to manipulate molecular space. However, GA usually suffers from slow and unstable convergence due to its random walk nature. We address this problem by introducing a novel approach called \mname, in which each proposed sample iteratively progresses toward the optimal solution. Our method leverages Discrete Langevin Proposal (DLP) as the base sampler, enabling gradient-based exploration in the discrete molecular space. Extensive experimental results confirm that our proposed approach achieves faster and superior convergence compared to state-of-the-art methods.

Future work will expand the current method in the following aspects: (1) Explore the effect of utilizing gradient information for generated molecule populations; (2) Explore better ways to fit both parents into DLP; (3) Explore more DLP-oriented molecular optimizations with Metropolis-Hastings criterion involved. 





\clearpage



\section*{Impact Statement}
Drug discovery is the process of designing drugs or treatments to help patients. In the past, most drugs have been identified through numerous costly, time-consuming, labor-intensive cycles of screening, synthesis, and analysis. Machine learning algorithms can potentially enhance the existing drug discovery pipeline by providing efficient virtual screening tools for finding new drugs. Our \mname~algorithm can play a positive role in supporting the fast discovery of effective and safe drugs.


\bibliography{icml2024/main}
\bibliographystyle{icml2024}


\newpage
\appendix 

\onecolumn
\section{Additional Experimental Results}
\label{appendix}
\subsection{$D''$ is Saved}
The tables below are results from the same experimental setup described in table ~\ref{tab:results_comp}. The higher the score the better for all metrics except Average SA. 

For most oracles, \mname~achieves the highest performance in Average Top 1, while for Average Top 100 and AUC Top 1, it consistently outperforms all other methods across all oracles. Graph GA and DST continue to perform well overall, ranking near the top in almost every oracle. MIMOSA and MARS follow closely, occasionally achieving top-tier results.  
On the other hand, SMILES GA performs the worst in terms of optimization metrics but ranks near the top for diversity and achieves the lowest SA score.

\begin{table}[!ht]
    \centering
    \caption{mestranol similarity Results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllllll}
    \toprule[1pt]
    Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity\\\hline
    \mname & 0.5367$\pm$0.0421 & 0.5130$\pm$0.0393 & 0.4673$\pm$0.0293 & 0.4433$\pm$0.0310 & 0.4082$\pm$0.0315 & 0.3534$\pm$0.0355 & 4.9647$\pm$0.3605 & 0.6692$\pm$0.0294 \\ \hline
    Graph GA & 0.4726$\pm$0.0241 & 0.4452$\pm$0.0241 & 0.4041$\pm$0.0237 & 0.3556$\pm$0.0268 & 0.3208$\pm$0.0199 & 0.2717$\pm$0.0147 & 4.3011$\pm$0.5876 & 0.7660$\pm$0.0380 \\ \hline
    SMILES GA & 0.2913$\pm$0.0178 & 0.2582$\pm$0.0097 & 0.1703$\pm$0.0074 & 0.3777$\pm$0.0381 & 0.3634$\pm$0.0352 & 0.3347$\pm$0.0279 & 3.3519$\pm$0.0524 & 0.8711$\pm$0.0065 \\ \hline
    MIMOSA & 0.4784$\pm$0.0456 & 0.4262$\pm$0.0246 & 0.3596$\pm$0.0371 & 0.4162$\pm$0.0115 & 0.3619$\pm$0.0181 & 0.2887$\pm$0.0252 & 3.9385$\pm$0.1149 & 0.7782$\pm$0.0721 \\ \hline
    MARS & 0.3837$\pm$0.0004 & 0.3411$\pm$0.0160 & 0.2803$\pm$0.0205 & 0.3760$\pm$0.0003 & 0.3215$\pm$0.0096 & 0.2523$\pm$0.0081 & 3.9512$\pm$0.2232 & 0.8594$\pm$0.0032 \\ \hline
    DST & 0.4591$\pm$0.0305 & 0.4131$\pm$0.0179 & 0.3654$\pm$0.0126 & 0.4148$\pm$0.0323 & 0.3507$\pm$0.0088 & 0.2780$\pm$0.0029 & 4.1479$\pm$0.0984 & 0.8198$\pm$0.0079 \\ \hline
    \end{tabular}
    }
\end{table}
\begin{table}[!ht]
    \centering
    \caption{median1 Results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllllll}
    \toprule[1pt]
    Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity\\\hline
    \mname & 0.3185$\pm$0.0113 & 0.3033$\pm$0.0074 & 0.2688$\pm$0.0086 & 0.2581$\pm$0.0115 & 0.2298$\pm$0.0151 & 0.1906$\pm$0.0183 & 3.7551$\pm$0.2584 & 0.6974$\pm$0.0368 \\ \hline
    Graph GA & 0.2779$\pm$0.0251 & 0.2599$\pm$0.0182 & 0.2292$\pm$0.0127 & 0.2315$\pm$0.0206 & 0.1959$\pm$0.0148 & 0.1442$\pm$0.0070 & 3.8609$\pm$0.1770 & 0.7462$\pm$0.0425 \\ \hline
    SMILES GA & 0.1587$\pm$0.0370 & 0.1310$\pm$0.0172 & 0.0674$\pm$0.0067 & 0.1832$\pm$0.0281 & 0.1795$\pm$0.0272 & 0.1697$\pm$0.0251 & 3.3444$\pm$0.1014 & 0.8700$\pm$0.0074 \\ \hline
    MIMOSA & 0.2802$\pm$0.0126 & 0.2391$\pm$0.0080 & 0.1948$\pm$0.0091 & 0.2271$\pm$0.0103 & 0.1969$\pm$0.0044 & 0.1537$\pm$0.0030 & 4.3154$\pm$0.1356 & 0.8148$\pm$0.0209 \\ \hline
    MARS & 0.2322$\pm$0.0201 & 0.2094$\pm$0.0181 & 0.1777$\pm$0.0234 & 0.2239$\pm$0.0140 & 0.2019$\pm$0.0116 & 0.1671$\pm$0.0158 & 4.2508$\pm$0.3090 & 0.8458$\pm$0.0196 \\ \hline
    DST & 0.2512$\pm$0.0376 & 0.2179$\pm$0.0162 & 0.1734$\pm$0.0049 & 0.2097$\pm$0.0086 & 0.1765$\pm$0.0021 & 0.1331$\pm$0.0024 & 4.4038$\pm$0.2342 & 0.8554$\pm$0.0098 \\ \hline

    \end{tabular}
    }
\end{table}
\begin{table}[!ht]
    \centering
    \caption{amlodipine MPO Results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllllll}
    \toprule[1pt]
    Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity\\\hline
    \mname & 0.5880$\pm$0.0397 & 0.5667$\pm$0.0336 & 0.5400$\pm$0.0296 & 0.5614$\pm$0.0177 & 0.5176$\pm$0.0187 & 0.4658$\pm$0.0199 & 3.8677$\pm$0.3122 & 0.6176$\pm$0.0478 \\ \hline
    Graph GA & 0.5772$\pm$0.0409 & 0.5605$\pm$0.0364 & 0.5232$\pm$0.0339 & 0.5067$\pm$0.0270 & 0.4734$\pm$0.0215 & 0.4152$\pm$0.0142 & 3.7496$\pm$0.1504 & 0.7571$\pm$0.0421 \\ \hline
    SMILES GA & 0.4782$\pm$0.0198 & 0.4480$\pm$0.0161 & 0.2242$\pm$0.0370 & 0.5016$\pm$0.0156 & 0.4956$\pm$0.0143 & 0.4748$\pm$0.0158 & 3.2925$\pm$0.0951 & 0.8711$\pm$0.0064 \\ \hline
    MIMOSA & 0.5633$\pm$0.0222 & 0.5245$\pm$0.0143 & 0.4996$\pm$0.0132 & 0.5431$\pm$0.0261 & 0.4953$\pm$0.0109 & 0.4436$\pm$0.0075 & 2.9351$\pm$0.2224 & 0.7430$\pm$0.0310 \\ \hline
    MARS & 0.5079$\pm$0.0301 & 0.4843$\pm$0.0210 & 0.4412$\pm$0.0295 & 0.4812$\pm$0.0144 & 0.4583$\pm$0.0098 & 0.3816$\pm$0.0157 & 3.6660$\pm$0.3345 & 0.8504$\pm$0.0169 \\ \hline
    DST & 0.5609$\pm$0.0285 & 0.5192$\pm$0.0122 & 0.4704$\pm$0.0069 & 0.5411$\pm$0.0303 & 0.4908$\pm$0.0115 & 0.4257$\pm$0.0044 & 2.6388$\pm$0.0770 & 0.8349$\pm$0.0050 \\ \hline

    \end{tabular}
    }
\end{table}
\begin{table}[!ht]
    \centering
    \caption{perindopril MPO Results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllllll}
    \toprule[1pt]
    Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity\\\hline
    \mname & 0.4856$\pm$0.0273 & 0.4786$\pm$0.0257 & 0.4620$\pm$0.0287 & 0.4542$\pm$0.0164 & 0.4361$\pm$0.0176 & 0.3882$\pm$0.0193 & 3.4861$\pm$0.4956 & 0.5720$\pm$0.0691 \\ \hline
    Graph GA & 0.4985$\pm$0.0174 & 0.4788$\pm$0.0067 & 0.4438$\pm$0.0049 & 0.4519$\pm$0.0055 & 0.4317$\pm$0.0045 & 0.3770$\pm$0.0049 & 3.6484$\pm$0.1822 & 0.7766$\pm$0.0157 \\ \hline
    SMILES GA & 0.4228$\pm$0.0090 & 0.3698$\pm$0.0117 & 0.1870$\pm$0.0185 & 0.4346$\pm$0.0124 & 0.4271$\pm$0.0115 & 0.4065$\pm$0.0102 & 3.3328$\pm$0.0955 & 0.8694$\pm$0.0080 \\ \hline
    MIMOSA & 0.4719$\pm$0.0166 & 0.4629$\pm$0.0176 & 0.4366$\pm$0.0169 & 0.4500$\pm$0.0144 & 0.4289$\pm$0.0116 & 0.3783$\pm$0.0085 & 3.1907$\pm$0.2006 & 0.7453$\pm$0.0284 \\ \hline
    MARS & 0.4738$\pm$0.0158 & 0.4564$\pm$0.0167 & 0.4250$\pm$0.0147 & 0.4538$\pm$0.0087 & 0.4278$\pm$0.0065 & 0.3648$\pm$0.0042 & 4.8547$\pm$0.0727 & 0.8272$\pm$0.0078 \\ \hline
    DST & 0.4774$\pm$0.0101 & 0.4615$\pm$0.0100 & 0.4197$\pm$0.0177 & 0.4530$\pm$0.0041 & 0.4210$\pm$0.0041 & 0.3564$\pm$0.0028 & 3.1428$\pm$0.1120 & 0.7997$\pm$0.0294 \\ \hline
    \end{tabular}
    }
\end{table}
\begin{table}[!ht]
    \centering
    \caption{deco hop Results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllllll}
    \toprule[1pt]
    Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity\\\hline
    \mname & 0.6116$\pm$0.0013 & 0.6026$\pm$0.0053 & 0.5945$\pm$0.0068 & 0.5883$\pm$0.0032 & 0.5763$\pm$0.0050 & 0.5602$\pm$0.0053 & 3.3425$\pm$0.2122 & 0.6444$\pm$0.0420 \\ \hline
    Graph GA & 0.6101$\pm$0.0039 & 0.6039$\pm$0.0043 & 0.5923$\pm$0.0027 & 0.5186$\pm$0.0037 & 0.5028$\pm$0.0032 & 0.4708$\pm$0.0033 & 3.3022$\pm$0.0970 & 0.7870$\pm$0.0220 \\ \hline
    SMILES GA & 0.5733$\pm$0.0062 & 0.5548$\pm$0.0059 & 0.5178$\pm$0.0082 & 0.5862$\pm$0.0047 & 0.5817$\pm$0.0042 & 0.5733$\pm$0.0036 & 3.2847$\pm$0.1118 & 0.8699$\pm$0.0082 \\ \hline
    MIMOSA & 0.6071$\pm$0.0072 & 0.6008$\pm$0.0053 & 0.5906$\pm$0.0046 & 0.5882$\pm$0.0061 & 0.5773$\pm$0.0035 & 0.5600$\pm$0.0021 & 2.8906$\pm$0.2936 & 0.7428$\pm$0.0481 \\ \hline
    MARS & 0.6014$\pm$0.0069 & 0.5944$\pm$0.0070 & 0.5830$\pm$0.0095 & 0.5830$\pm$0.0227 & 0.5711$\pm$0.0301 & 0.5493$\pm$0.0421 & 3.7003$\pm$0.1504 & 0.8182$\pm$0.0646 \\ \hline
    DST & 0.6128$\pm$0.0118 & 0.6034$\pm$0.0083 & 0.5878$\pm$0.0043 & 0.5860$\pm$0.0071 & 0.5721$\pm$0.0025 & 0.5518$\pm$0.0009 & 2.8406$\pm$0.1211 & 0.8305$\pm$0.0060 \\ \hline

    \end{tabular}
    }
\end{table}
\begin{table}[H]
    \centering
    \caption{isomers c9h10n2o2pf2cl Results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllllll}
    \toprule[1pt]
    Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity\\\hline
    \mname & 0.8258$\pm$0.0940 & 0.7783$\pm$0.0959 & 0.6697$\pm$0.0993 & 0.6628$\pm$0.0731 & 0.5444$\pm$0.0693 & 0.4033$\pm$0.0614 & 5.2421$\pm$0.3124 & 0.7908$\pm$0.0172 \\ \hline
    Graph GA & 0.7593$\pm$0.1157 & 0.7222$\pm$0.1119 & 0.6438$\pm$0.0950 & 0.6648$\pm$0.0957 & 0.5436$\pm$0.0770 & 0.3891$\pm$0.0425 & 4.4437$\pm$0.3496 & 0.7933$\pm$0.0676 \\ \hline
    SMILES GA & 0.5597$\pm$0.2924 & 0.3180$\pm$0.3583 & 0.1972$\pm$0.4036 & 0.8244$\pm$0.0848 & 0.7825$\pm$0.0752 & 0.7055$\pm$0.0668 & 3.7996$\pm$1.1599 & 0.8320$\pm$0.0890 \\ \hline
    MIMOSA & 0.8146$\pm$0.0818 & 0.7866$\pm$0.0824 & 0.6848$\pm$0.0541 & 0.6965$\pm$0.0562 & 0.5949$\pm$0.0440 & 0.3965$\pm$0.0265 & 3.6001$\pm$0.2458 & 0.8231$\pm$0.0300 \\ \hline
    MARS & 0.7268$\pm$0.1260 & 0.6639$\pm$0.1606 & 0.5268$\pm$0.2389 & 0.6751$\pm$0.1032 & 0.5989$\pm$0.1057 & 0.4424$\pm$0.1499 & 3.1615$\pm$0.8526 & 0.8335$\pm$0.0970 \\ \hline
    DST & 0.7703$\pm$0.0482 & 0.6748$\pm$0.0304 & 0.5028$\pm$0.0329 & 0.6305$\pm$0.0435 & 0.4932$\pm$0.0216 & 0.2293$\pm$0.0093 & 2.9831$\pm$0.1486 & 0.8735$\pm$0.0033 \\ \hline
    \end{tabular}
    }
\end{table}
\subsection{$D''$ is Cleared}
The overall experimental setup includes 10,000 oracle calls, 5 runs, and early stopping enabled. The parameters for \mname~are set to 200 epochs, with \( D'' \) being cleared after each retraining. We have run REINVENT~\cite{Olivecrona}, which is a reinforcement learning method, instead of DST. Additionally, we evaluate another metric, Diversity, which measures the Tanimoto Similarity between two molecules. The higher the score the better for all metrics except Average SA. This setup overall performs worse than when $D''$ is saved, however, it is slightly faster in runtime, due to having fewer molecules for retraining.

From the results, we can see that \mname~and Graph GA are near the top in terms of performance with REINVENT. These results follow the results gathered from PMO \cite{gao2022samples}. We notice that \mname usually performs better than Graph GA whenever it is dealing with a molecular objective that is on the lower end, so the exploration near optimal molecules matter more than as random walk behavior may have a harder time finding the optimal molecules. Overall, the best-performing models are REINVENT,~\mname, and Graph GA.
\begin{table}[!ht]
    \centering
    \caption{perindopril mpo Results}
    \resizebox{\textwidth}{!}{\begin{tabular}{lllllllllll}
    \toprule[1pt]
        Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity \\ \hline
        \mname~ & 0.5741$\pm$0.0104 & 0.5613$\pm$0.0109 & 0.5443$\pm$0.0113 & 0.5228$\pm$0.0113 & 0.5102$\pm$0.0109 & 0.4874$\pm$0.0111 & 4.735366737$\pm$0.4388 & 0.5477$\pm$0.0594 \\ \hline

        Graph GA & 0.5350$\pm$0.0612 & 0.5245$\pm$0.0585 & 0.5000$\pm$0.0528 & 0.5010$\pm$0.0393 & 0.4848$\pm$0.0371 & 0.4515$\pm$0.0339 & 3.9596$\pm$0.1366 & 0.6685$\pm$0.1190 \\ \hline

        SMILES GA & 0.4495$\pm$0.0144 & 0.4495$\pm$0.0144 & 0.4478$\pm$0.0145 & 0.4455$\pm$0.0127 & 0.4433$\pm$0.0123 & 0.4355$\pm$0.0121 & 4.7639$\pm$0.4927 & 0.4968$\pm$0.0538 \\ \hline

        MARS & 0.4793$\pm$0.0137 & 0.4647$\pm$0.0128 & 0.4357$\pm$0.0135 & 0.4751$\pm$0.0112 & 0.4570$\pm$0.0098 & 0.4173$\pm$0.0090 & 5.0202$\pm$0.2178 & 0.8231$\pm$0.0053 \\ \hline

        MIMOSA & 0.4703$\pm$0.0177 & 0.4569$\pm$0.0107 & 0.4403$\pm$0.0076 & 0.3247$\pm$0.0046 & 0.3116$\pm$0.0042 & 0.2856$\pm$0.0031 & 3.9169$\pm$0.2835 & 0.6964$\pm$0.0230 \\ \hline

        REINVENT & 0.6196$\pm$0.0460 & 0.6164$\pm$0.0485 & 0.6132$\pm$0.0511 & 0.4563$\pm$0.0764 & 0.4428$\pm$0.0755 & 0.4188$\pm$0.0746 & 4.1524$\pm$0.3994 & 0.3490$\pm$0.0603 \\ \hline
    \end{tabular}
    }
\end{table}

\begin{table}[!ht]
    \centering
    \caption{ mestranol similarity Results}
    \resizebox{\textwidth}{!}{\begin{tabular}{lllllllllll}
    \toprule[1pt]
        Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity \\ \hline 
        \mname~ &0.7348$\pm$0.0321 & 0.7140$\pm$0.0282 & 0.6618$\pm$0.0298 & 0.6179$\pm$0.0401 & 0.5833$\pm$0.0349 & 0.5273$\pm$0.0318 & 4.6019$\pm$0.3707 & 0.5329$\pm$0.0394 \\ \hline


        Graph GA & 0.7335$\pm$0.1325 & 0.6926$\pm$0.1161 & 0.6401$\pm$0.0928 & 0.6123$\pm$0.0516 & 0.5727$\pm$0.0429 & 0.5173$\pm$0.0307 & 4.2850$\pm$0.5503 & 0.5599$\pm$0.0471 \\ \hline

        SMILES GA & 0.4488$\pm$0.0456 & 0.4477$\pm$0.0454 & 0.4441$\pm$0.0433 & 0.4196$\pm$0.0425 & 0.4129$\pm$0.0419 & 0.4007$\pm$0.0395 & 5.1324$\pm$0.6792 & 0.5278$\pm$0.1021 \\ \hline

        MARS & 0.4142$\pm$0.0662 & 0.3782$\pm$0.0718 & 0.3202$\pm$0.0756 & 0.4059$\pm$0.0638 & 0.3671$\pm$0.0655 & 0.3047$\pm$0.0645 & 4.0433$\pm$0.3763 & 0.8546$\pm$0.0105 \\ \hline

        MIMOSA & 0.5239$\pm$0.0145 & 0.4907$\pm$0.0147 & 0.4450$\pm$0.0186 & 0.5079$\pm$0.0010 & 0.4688$\pm$0.0021 & 0.4069$\pm$0.0044 & 3.8981$\pm$0.3227 & 0.8052$\pm$0.0398 \\ \hline
        
        REINVENT & 0.7838$\pm$0.0823 & 0.7809$\pm$0.0835 & 0.7627$\pm$0.0834 & 0.3924$\pm$0.0836 & 0.3709$\pm$0.0838 & 0.3346$\pm$0.0824 & 3.6929$\pm$0.4919 & 0.2989$\pm$0.0471 \\ \hline
    \end{tabular}
    }
\end{table}

\begin{table}[!ht]
    \centering
    \caption{median1 Results}
    \resizebox{\textwidth}{!}{\begin{tabular}{lllllllllll}
    \toprule[1pt]
        Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity  \\ \hline
        \mname~ & 0.3923$\pm$0.0173 & 0.3714$\pm$0.0183 & 0.3378$\pm$0.0095 & 0.3376$\pm$0.0215 & 0.3108$\pm$0.0153 & 0.2789$\pm$0.0125 & 4.0045$\pm$0.1275 & 0.6191$\pm$0.0278 \\ \hline

        Graph GA  & 0.3093$\pm$0.0345 & 0.2906$\pm$0.0257 & 0.2593$\pm$0.0180 & 0.2922$\pm$0.0311 & 0.2680$\pm$0.0228 & 0.2318$\pm$0.0158 & 4.0053$\pm$0.1491 & 0.7091$\pm$0.0363 \\ \hline

        SMILES GA & 0.2004$\pm$0.0300 & 0.1989$\pm$0.0298 & 0.1980$\pm$0.0293 & 0.1977$\pm$0.0296 & 0.1936$\pm$0.0284 & 0.1889$\pm$0.0268 & 6.0024$\pm$1.0691 & 0.6396$\pm$0.0382 \\ \hline

        MARS & 0.2322$\pm$0.0201 & 0.2094$\pm$0.0181 & 0.1777$\pm$0.0234 & 0.2239$\pm$0.0140 & 0.2019$\pm$0.0116 & 0.1671$\pm$0.0158 & 4.2508$\pm$0.3090 & 0.8458$\pm$0.0196 \\ \hline

        MIMOSA & 0.3275$\pm$0.0130 & 0.3011$\pm$0.0036 & 0.2686$\pm$0.0003 & 0.2675$\pm$0.0043 & 0.2278$\pm$0.0013 & 0.1654$\pm$0.0013 & 3.9701$\pm$0.0735 & 0.7643$\pm$0.0091 \\ \hline

        REINVENT & 0.4579$\pm$0.0004 & 0.4384$\pm$0.0193 & 0.4181$\pm$0.0344 & 0.2571$\pm$0.0514 & 0.2282$\pm$0.0447 & 0.1852$\pm$0.0373 & 4.7140$\pm$0.6151 & 0.3136$\pm$0.1278 \\ \hline
    \end{tabular}
    }
\end{table}

\begin{table}[!ht]
    \centering
    \caption{isomers c9h10n2o2pf2cl Results}
    \resizebox{\textwidth}{!}{\begin{tabular}{lllllllllll}
    \toprule[1pt]
        Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity  \\ \hline
        \mname~  & 0.9394$\pm$0.0000 & 0.9237$\pm$0.0196 & 0.8733$\pm$0.0264 & 0.8286$\pm$0.0704 & 0.7878$\pm$0.0758 & 0.7018$\pm$0.0808 & 5.4939$\pm$0.2025 & 0.7481$\pm$0.0389 \\ \hline
        
        Graph GA & 0.9137$\pm$0.0291 & 0.8869$\pm$0.0285 & 0.8316$\pm$0.0177 & 0.8441$\pm$0.0206 & 0.8024$\pm$0.0158 & 0.7171$\pm$0.0141 & 4.5273$\pm$0.3671 & 0.7995$\pm$0.0265 \\ \hline

        SMILES GA & 0.9341$\pm$0.0366 & 0.9310$\pm$0.0363 & 0.8895$\pm$0.0457 & 0.8955$\pm$0.0351 & 0.8640$\pm$0.0352 & 0.8083$\pm$0.0450 & 5.8310$\pm$0.2092 & 0.7161$\pm$0.0519 \\ \hline

        MARS & 0.7268$\pm$0.1260 & 0.6639$\pm$0.1606 & 0.5268$\pm$0.2389 & 0.6751$\pm$0.1032 & 0.5989$\pm$0.1057 & 0.4424$\pm$0.1499 & 3.1615$\pm$0.8526 & 0.8335$\pm$0.0970 \\ \hline

        MIMOSA & 0.8352$\pm$0.0458 & 0.8081$\pm$0.0637 & 0.7564$\pm$0.0661 & 0.6092$\pm$0.0248 & 0.5745$\pm$0.0244 & 0.5007$\pm$0.0200 & 4.1175$\pm$0.8821 & 0.7669$\pm$0.0722 \\ \hline

        REINVENT & 0.9166$\pm$0.0312 & 0.9030$\pm$0.0267 & 0.8718$\pm$0.0256 & 0.3557$\pm$0.0381 & 0.3331$\pm$0.0367 & 0.2874$\pm$0.0323 & 3.1677$\pm$0.7806 & 0.6673$\pm$0.0776 \\ \hline
    \end{tabular}}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{deco hop Results}
    \resizebox{\textwidth}{!}{\begin{tabular}{lllllllllll}
    \hline
        Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity  \\ \hline
        
        \mname~ &0.6475$\pm$0.0092 & 0.6413$\pm$0.0091 & 0.6353$\pm$0.0092 & 0.6200$\pm$0.0060 & 0.6124$\pm$0.0059 & 0.6024$\pm$0.0060 & 3.9402$\pm$0.4094 & 0.5562$\pm$0.0621 \\ \hline

        Graph GA & 0.6794$\pm$0.1400 & 0.6738$\pm$0.1420 & 0.6632$\pm$0.1411 & 0.6452$\pm$0.0840 & 0.6326$\pm$0.0799 & 0.6120$\pm$0.0729 & 3.2752$\pm$0.2231 & 0.7074$\pm$0.1102 \\ \hline

        SMILES GA & 0.6147$\pm$0.0061 & 0.6147$\pm$0.0061 & 0.6144$\pm$0.0059 & 0.5927$\pm$0.0060 & 0.5903$\pm$0.0060 & 0.5844$\pm$0.0057 & 4.9016$\pm$0.5634 & 0.5258$\pm$0.0800 \\ \hline

        MARS & 0.6014$\pm$0.0069 & 0.5944$\pm$0.0070 & 0.5830$\pm$0.0095 & 0.5830$\pm$0.0227 & 0.5711$\pm$0.0301 & 0.5493$\pm$0.0421 & 3.7003$\pm$0.1504 & 0.8182$\pm$0.0646 \\ \hline

        MIMOSA & 0.6051$\pm$0.0122 & 0.6032$\pm$0.0130 & 0.5896$\pm$0.0100 & 0.5428$\pm$0.0053 & 0.5173$\pm$0.0049 & 0.4726$\pm$0.0029 & 4.1345$\pm$0.2664 & 0.6712$\pm$0.0811 \\ \hline

        REINVENT & 0.8014$\pm$0.1476 & 0.7915$\pm$0.1430 & 0.7853$\pm$0.1413 & 0.6577$\pm$0.0628 & 0.6415$\pm$0.0543 & 0.6231$\pm$0.0478 & 3.0143$\pm$0.2320 & 0.4356$\pm$0.0308 \\ \hline
    \end{tabular}}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{amlodipine mpo Results}
    \resizebox{\textwidth}{!}{\begin{tabular}{lllllllllll}
    \toprule[1pt]
        Method & Average Top 1 & Top 10 & Top 100 & AUC Top 1 & Top 10 & Top 100 & Average SA & Diversity  \\ \hline

        \mname~ & 0.6942$\pm$0.0499 & 0.6848$\pm$0.0510 & 0.6698$\pm$0.0456 & 0.6309$\pm$0.0361 & 0.6120$\pm$0.0348 & 0.5845$\pm$0.0309 & 4.0448$\pm$0.0917 & 0.4402$\pm$0.0859 \\ \hline
        
        Graph GA  & 0.7150$\pm$0.0407 & 0.6958$\pm$0.0290 & 0.6730$\pm$0.0258 & 0.6569$\pm$0.0202 & 0.6350$\pm$0.0174 & 0.6004$\pm$0.0131 & 3.8835$\pm$0.1383 & 0.5982$\pm$0.0422 \\ \hline

        SMILES GA & 0.5254$\pm$0.0335 & 0.5235$\pm$0.0375 & 0.5216$\pm$0.0379 & 0.5130$\pm$0.0295 & 0.5079$\pm$0.0324 & 0.4995$\pm$0.0317 & 4.6961$\pm$0.2230 & 0.5945$\pm$0.0615 \\ \hline

        MARS & 0.5081$\pm$0.0303 & 0.4902$\pm$0.0279 & 0.4488$\pm$0.0392 & 0.5014$\pm$0.0259 & 0.4818$\pm$0.0223 & 0.4311$\pm$0.0315 & 3.6973$\pm$0.3809 & 0.8430$\pm$0.0302 \\ \hline

        MIMOSA & 0.6045$\pm$0.0118 & 0.5789$\pm$0.0166 & 0.5540$\pm$0.0134 & 0.5908$\pm$0.0211 & 0.5429$\pm$0.0174 & 0.4979$\pm$0.0084 & 4.0689$\pm$0.5939 & 0.6628$\pm$0.0650 \\ \hline

        REINVENT & 0.7382$\pm$0.0453 & 0.7334$\pm$0.0431 & 0.7259$\pm$0.0400 & 0.5576$\pm$0.0574 & 0.5409$\pm$0.0553 & 0.5147$\pm$0.0527 & 3.2323$\pm$0.2277 & 0.3946$\pm$0.0670 \\ \hline
    \end{tabular}}
\end{table}


\onecolumn
\section{Molecules Generated by Various Methods}
Here we have a list of Top 10 molecules generated by each method that we tested from the experiments in Table~\ref{tab:results_comp}. Each molecule has its respective scores underneath them, and all molecules are for the bio-activity object mestranol similarity.

We notice that the molecules generated by \mname, SMILES GA, and Graph GA are very similar, with \mname~and Graph GA having the most similarity. MARS and MIMOSA both have unique molecules generated, but the performance of those molecules is low. We notice that \mname~just needs to take a molecule from a good run, and that will lead to having an entire set of good molecules. Most of \mname~top 10 molecules are all from the same run, and we can see that molecular structure was found in the bottom right corner of Figure~\ref{fig:graphgams_molecules}. This further supports the idea that \mname~is exploring similar molecules to Graph GA, and is further exploring an area around its top-performing molecules.

\label{appendixb}
\begin{figure}[ht]
    \centering
    \caption{Top 10 molecules generated by \mname~for the bio-activity objective mestranol similarity with their associated score underneath each molecule.}
    \resizebox{\linewidth}{!}{
    \includegraphics[width=0.5\linewidth]{./data/generated_mol/ms_top10.png}
    }
    \label{fig:ms_molecules}
\end{figure}
\begin{figure}[ht]
    \centering
    \caption{Top 10 molecules generated by SMILES GA for the bio-activity objective mestranol similarity with their associated score underneath each molecule.}
    \resizebox{\linewidth}{!}{
    \includegraphics[width=0.5\linewidth]{./data/generated_mol/smiles_gams_top10.png}
    }
    \label{fig:smilesms_molecules}
\end{figure}
\begin{figure}[ht]
    \centering
    \caption{Top 10 molecules generated by Graph GA for the bio-activity objective mestranol similarity with their associated score underneath each molecule.}
    \resizebox{\linewidth}{!}{
    \includegraphics[width=0.5\linewidth]{./data/generated_mol/graph_gams_top10.png}
    }
    \label{fig:graphgams_molecules}
\end{figure}
\begin{figure}[ht]
    \centering
    \caption{Top 10 molecules generated by MARS for the bio-activity objective mestranol similarity with their associated score underneath each molecule.}
    \resizebox{\linewidth}{!}{
    \includegraphics[width=0.5\linewidth]{./data/generated_mol/marsms_top10.png}
    }
    \label{fig:marsms_molecules}
\end{figure}
\begin{figure}[ht]
    \centering
    \caption{Top 10 molecules generated by MIMOSA for the bio-activity objective mestranol similarity with their associated score underneath each molecule.}
    \resizebox{\linewidth}{!}{
    \includegraphics[width=0.5\linewidth]{./data/generated_mol/mimosams_top10.png}
    }
    \label{fig:mimosams_molecules}
\end{figure}

\clearpage
\section{Comparison of Including $\mathcal{O}(X)$ in Gradient}
\label{appendixc}
In table below, we show the results of using $\nabla U(v) = \nabla f(v)$ vs \(\nabla U(v) = \frac{\nabla f(v)}{\mathcal{O}(X)}\). We notice from the results, using $\mathcal{O}(x)$ leads to a better score throughout all metrics.
\begin{table}[htp]
%Can change the oracles, just using these for now
    \centering
    \small\setlength\tabcolsep{4.5pt}
    \caption{Comparison of Average Top 10, AUC Top 1, AUC Top 10, and AUC Top 100 with GuacaMol objective, mestranol similarity, under 2500 oracle calls. The best \mname~setup is \textbf{bolded}. We conduct five independent runs using different random seeds for both versions of \mname, and report the average scores and their standard deviation.}
    \vspace{0.2cm} 
    \begin{tabular}{ c | c c c c }
    \toprule
      \mname   & \multicolumn{4}{c}{mestranol similarity}\\ 
        & Average Top 10 & AUC Top 1 & AUC Top 10 & AUC Top 100\\
      \midrule
      With $\mathcal{O}(x)$& \textbf{0.5130$\pm$0.0393} & \textbf{0.4433$\pm$0.0310} & \textbf{0.4082$\pm$0.0315} & \textbf{0.3534$\pm$0.0355}\\
      
      Without $\mathcal{O}(x)$& 0.5064$\pm$0.0312 & 0.4433$\pm$0.0319 & 0.4072$\pm$0.0367 & 0.3501$\pm$0.0419  \\ 

      \bottomrule
    \end{tabular}
\end{table}


\end{document}

