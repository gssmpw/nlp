@misc{anthropic_contextual_retrieval_2024,
  author = {Anthropic},
  title  = {Introducing Contextual Retrieval},
  url    = {https://www.anthropic.com/news/contextual-retrieval},
  year   = {2024}
}

@article{chuang2023dola,
  author  = {Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
  journal = {ArXiv preprint},
  title   = {Dola: Decoding by contrasting layers improves factuality in large language models},
  url     = {https://arxiv.org/abs/2309.03883},
  volume  = {abs/2309.03883},
  year    = {2023}
}

@inproceedings{fei-etal-2024-extending,
    title = "Extending Context Window of Large Language Models via Semantic Compression",
    author = "Fei, Weizhi  and
      Niu, Xueyan  and
      Zhou, Pingyi  and
      Hou, Lu  and
      Bai, Bo  and
      Deng, Lei  and
      Han, Wei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.306",
    doi = "10.18653/v1/2024.findings-acl.306",
    pages = "5169--5181",
    abstract = "Transformer based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses due to the quadratic complexity. These constraints restrict their applicability in long text scenarios. In this paper, we propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead.",
}

@article{ge2023model,
  author  = {Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal = {ArXiv preprint},
  title   = {Model tells you what to discard: Adaptive kv cache compression for llms},
  url     = {https://arxiv.org/abs/2310.01801},
  volume  = {abs/2310.01801},
  year    = {2023}
}

@inproceedings{han2024lm,
  address   = {Mexico City, Mexico},
  author    = {Han, Chi  and
               Wang, Qifan  and
               Peng, Hao  and
               Xiong, Wenhan  and
               Chen, Yu  and
               Ji, Heng  and
               Wang, Sinong},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  editor    = {Duh, Kevin  and
               Gomez, Helena  and
               Bethard, Steven},
  pages     = {3991--4008},
  publisher = {Association for Computational Linguistics},
  title     = {{LM}-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models},
  url       = {https://aclanthology.org/2024.naacl-long.222},
  year      = {2024}
}

@inproceedings{hu2021lora,
  author    = {Edward J. Hu and
               Yelong Shen and
               Phillip Wallis and
               Zeyuan Allen{-}Zhu and
               Yuanzhi Li and
               Shean Wang and
               Lu Wang and
               Weizhu Chen},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  url       = {https://openreview.net/forum?id=nZeVKeeFYf9},
  year      = {2022}
}

@article{li2024snapkv,
  author  = {Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal = {ArXiv preprint},
  title   = {SnapKV: LLM Knows What You are Looking for Before Generation},
  url     = {https://arxiv.org/abs/2404.14469},
  volume  = {abs/2404.14469},
  year    = {2024}
}

@article{pan2024lisa,
  author  = {Pan, Rui and Liu, Xiang and Diao, Shizhe and Pi, Renjie and Zhang, Jipeng and Han, Chi and Zhang, Tong},
  journal = {ArXiv preprint},
  title   = {LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning},
  url     = {https://arxiv.org/abs/2403.17919},
  volume  = {abs/2403.17919},
  year    = {2024}
}

@inproceedings{sang1999representing,
  address   = {Bergen, Norway},
  author    = {Tjong Kim Sang, Erik F.  and
               Veenstra, Jorn},
  booktitle = {Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics},
  editor    = {Thompson, Henry S.  and
               Lascarides, Alex},
  pages     = {173--179},
  publisher = {Association for Computational Linguistics},
  title     = {Representing Text Chunks},
  url       = {https://aclanthology.org/E99-1023},
  year      = {1999}
}

@inproceedings{shicontext,
  author    = {Shi, Weijia and Min, Sewon and Lomeli, Maria and Zhou, Chunting and Li, Margaret and Lin, Xi Victoria and Smith, Noah A and Zettlemoyer, Luke and Yih, Wen-tau and Lewis, Mike},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {In-Context Pretraining: Language Modeling Beyond Document Boundaries}
}

@techreport{smith2024evaluating,
  author      = {Smith, Brandon and Troynikov, Anton},
  institution = {Chroma},
  title       = {Evaluating Chunking Strategies for Retrieval},
  url         = {https://research.trychroma.com/evaluating-chunking},
  year        = {2024}
}

@inproceedings{xiao2024efficient,
  author    = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {Efficient Streaming Language Models with Attention Sinks},
  url       = {https://openreview.net/forum?id=NG7sS51zVF},
  year      = {2024}
}

@inproceedings{yang2024pyramidinfer,
  abstract  = {Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54{\%} GPU memory reduction in KV cache.},
  address   = {Bangkok, Thailand and virtual meeting},
  author    = {Yang, Dongjie  and
               Han, Xiaodong  and
               Gao, Yan  and
               Hu, Yao  and
               Zhang, Shilin  and
               Zhao, Hai},
  booktitle = {Findings of the Association for Computational Linguistics ACL 2024},
  doi       = {10.18653/v1/2024.findings-acl.195},
  editor    = {Ku, Lun-Wei  and
               Martins, Andre  and
               Srikumar, Vivek},
  pages     = {3258--3270},
  publisher = {Association for Computational Linguistics},
  title     = {{P}yramid{I}nfer: Pyramid {KV} Cache Compression for High-throughput {LLM} Inference},
  url       = {https://aclanthology.org/2024.findings-acl.195},
  year      = {2024}
}

@misc{yepes2024financialreportchunkingeffective,
  author  = {Antonio Jimeno Yepes and Yao You and Jan Milczek and Sebastian Laverde and Renyu Li},
  journal = {ArXiv preprint},
  title   = {Financial Report Chunking for Effective Retrieval Augmented Generation},
  url     = {https://arxiv.org/abs/2402.05131},
  volume  = {abs/2402.05131},
  year    = {2024}
}

@inproceedings{you2019lamb,
  author    = {Yang You and
               Jing Li and
               Sashank J. Reddi and
               Jonathan Hseu and
               Sanjiv Kumar and
               Srinadh Bhojanapalli and
               Xiaodan Song and
               James Demmel and
               Kurt Keutzer and
               Cho{-}Jui Hsieh},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/YouLRHKBSDKH20.bib},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  timestamp = {Thu, 07 May 2020 01:00:00 +0200},
  title     = {Large Batch Optimization for Deep Learning: Training {BERT} in 76
               minutes},
  url       = {https://openreview.net/forum?id=Syx4wnEtvH},
  year      = {2020}
}

@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}

@article{zhang2024pyramidkv,
  title={Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling},
  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

