\section{Related Work}
\textbf{KV Cache Compression.} KV cache compression technology has developed rapidly in the era of LLM, with methods mainly focused on evicting unimportant tokens. The compression process occurs before the attention blocks, optimizing both the prefilling time and GPU memory. \citet{xiao2024efficient} and \citet{han2024lm} propose that initial and recent tokens consistently have high attention scores between different layers and attention heads. As a result, retaining these tokens in the KV cache is more likely to preserve important information. Furthermore, FastGen~\citep{ge2023model} evicts tokens based on observed patterns. H2O~\citep{zhang2024h2o} and SnapKV~\citep{li2024snapkv} employ dynamic KV cache compression methods, evaluating the importance of tokens based on attention scores and then evicting the less important ones. As inference scenarios become increasingly complex, dynamic KV cache compression methods demonstrate powerful performance. Recently, \citet{yang2024pyramidinfer} and \citet{zhang2024pyramidkv} have closely examined the distributions of attention scores during the pre-filling stage of the Retrieval-Augmented Generation (RAG) task, discovering a pyramidal KV cache compression pattern in different transformer layers. 

Although these KV cache compression methods have explored efficient GPU memory management while maintaining original performance, our study focuses more on the semantic information of the prompt. We find that chunks of the original KV cache are more important than discrete tokens.
   
% \vspace{-0.8em}
\textbf{Chunking Method.} 
The chunking methodology is widely used in the field of NLP due to its simplicity and effectiveness~\citep{sang1999representing}. In the era of LLMs, chunking is primarily applied in data pre-processing. For example, \citet{shicontext} suggest grouping related training data into chunks to achieve better convergence curves to pre-train LLMs. \citet{fei-etal-2024-extending} apply a topic-based chunking method to improve the semantic compression of prompts. Furthermore, chunking plays an important role in the Retrieval-Augmented Generation (RAG) field~\citep{yepes2024financialreportchunkingeffective, smith2024evaluating, anthropic_contextual_retrieval_2024}. It serves to divide documents into units of information with semantic content suitable for embedding-based retrieval and processing by LLMs.

\textbf{Layer-Wise Technique}
The layer-wise technique is widely used in the training and inference of large language models (LLMs). LISA~\citep{pan2024lisa} is a layer-wise sampling method based on observations of the training dynamics of Low-Rank Adaptation (LoRA)\citep{hu2021lora} across layers. LAMB\citep{you2019lamb} is a layer-wise adaptive learning rate method that speeds up LLM training by stabilizing training convergence with large batch sizes. DoLa~\citep{chuang2023dola} employs layer-wise contrasting to reduce hallucinations during LLM inference.