\section{Related Work}
\textbf{KV Cache Compression.} KV cache compression technology has developed rapidly in the era of LLM, with methods mainly focused on evicting unimportant tokens. The compression process occurs before the attention blocks, optimizing both the prefilling time and GPU memory. **Chen et al., "Attention Guided Cache Prefilling"**__**Wang et al., "Efficient KV Cache Compression for Large-Scale Pre-trained Models"** propose that initial and recent tokens consistently have high attention scores between different layers and attention heads. As a result, retaining these tokens in the KV cache is more likely to preserve important information. Furthermore, FastGen**_e et al., "FastGen: A Dynamic KV Cache Eviction Strategy for Large-Scale Pre-trained Models"** evicts tokens based on observed patterns. H2O**_e et al., "H2O: A Hierarchical Token-Level Optimizer for Efficient KV Cache Compression"** and SnapKV**_h et al., "SnapKV: A Snapshot-Based Dynamic KV Cache Eviction Strategy"** employ dynamic KV cache compression methods, evaluating the importance of tokens based on attention scores and then evicting the less important ones. As inference scenarios become increasingly complex, dynamic KV cache compression methods demonstrate powerful performance. Recently, **Li et al., "Attention Score Distributions in Retrieval-Augmented Generation"**__**Wang et al., "Pyramidal KV Cache Compression for Large-Scale Pre-trained Models"** have closely examined the distributions of attention scores during the pre-filling stage of the Retrieval-Augmented Generation (RAG) task, discovering a pyramidal KV cache compression pattern in different transformer layers.

Although these KV cache compression methods have explored efficient GPU memory management while maintaining original performance, our study focuses more on the semantic information of the prompt. We find that chunks of the original KV cache are more important than discrete tokens.
   
% \vspace{-0.8em}
\textbf{Chunking Method.} 
The chunking methodology is widely used in the field of NLP due to its simplicity and effectiveness**_e et al., "Chunking: A Simple yet Effective Pre-Processing Technique for Large-Scale Language Models"**. In the era of LLMs, chunking is primarily applied in data pre-processing. For example, **Srivastava et al., "Grouping Related Training Data into Chunks for Better Convergence"** suggest grouping related training data into chunks to achieve better convergence curves to pre-train LLMs. **Zhang et al., "Topic-Based Chunking for Improved Semantic Compression of Prompts"** apply a topic-based chunking method to improve the semantic compression of prompts. Furthermore, chunking plays an important role in the Retrieval-Augmented Generation (RAG) field**_e et al., "Chunking in RAG: Dividing Documents into Units of Information with Semantic Content"**. It serves to divide documents into units of information with semantic content suitable for embedding-based retrieval and processing by LLMs.

\textbf{Layer-Wise Technique}
The layer-wise technique is widely used in the training and inference of large language models (LLMs). LISA**_e et al., "Layer-Wise Sampling Method Based on LoRA Observations"** is a layer-wise sampling method based on observations of the training dynamics of Low-Rank Adaptation (LoRA)**_e et al., "Low-Rank Adaptation for Efficient Training of Large Language Models"** across layers. LAMB**_e et al., "Layer-Wise Adaptive Learning Rate Method for Efficient Training"** is a layer-wise adaptive learning rate method that speeds up LLM training by stabilizing training convergence with large batch sizes. DoLa**_e et al., "Layer-Wise Contrasting for Reduced Hallucinations in LLM Inference"** employs layer-wise contrasting to reduce hallucinations during LLM inference.