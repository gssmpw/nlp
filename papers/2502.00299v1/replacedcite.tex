\section{Related Work}
\textbf{KV Cache Compression.} KV cache compression technology has developed rapidly in the era of LLM, with methods mainly focused on evicting unimportant tokens. The compression process occurs before the attention blocks, optimizing both the prefilling time and GPU memory. ____ and ____ propose that initial and recent tokens consistently have high attention scores between different layers and attention heads. As a result, retaining these tokens in the KV cache is more likely to preserve important information. Furthermore, FastGen____ evicts tokens based on observed patterns. H2O____ and SnapKV____ employ dynamic KV cache compression methods, evaluating the importance of tokens based on attention scores and then evicting the less important ones. As inference scenarios become increasingly complex, dynamic KV cache compression methods demonstrate powerful performance. Recently, ____ and ____ have closely examined the distributions of attention scores during the pre-filling stage of the Retrieval-Augmented Generation (RAG) task, discovering a pyramidal KV cache compression pattern in different transformer layers. 

Although these KV cache compression methods have explored efficient GPU memory management while maintaining original performance, our study focuses more on the semantic information of the prompt. We find that chunks of the original KV cache are more important than discrete tokens.
   
% \vspace{-0.8em}
\textbf{Chunking Method.} 
The chunking methodology is widely used in the field of NLP due to its simplicity and effectiveness____. In the era of LLMs, chunking is primarily applied in data pre-processing. For example, ____ suggest grouping related training data into chunks to achieve better convergence curves to pre-train LLMs. ____ apply a topic-based chunking method to improve the semantic compression of prompts. Furthermore, chunking plays an important role in the Retrieval-Augmented Generation (RAG) field____. It serves to divide documents into units of information with semantic content suitable for embedding-based retrieval and processing by LLMs.

\textbf{Layer-Wise Technique}
The layer-wise technique is widely used in the training and inference of large language models (LLMs). LISA____ is a layer-wise sampling method based on observations of the training dynamics of Low-Rank Adaptation (LoRA)____ across layers. LAMB____ is a layer-wise adaptive learning rate method that speeds up LLM training by stabilizing training convergence with large batch sizes. DoLa____ employs layer-wise contrasting to reduce hallucinations during LLM inference.