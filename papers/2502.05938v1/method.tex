\label{subsec:sim_setup}
Our simulation environment is inspired by the approach in~\cite{evplanner}, where event-camera streams capture dynamic scene changes in real time. In this work, we combine a spiking neural network (SNN) for event-based object detection (Section~\ref{subsec:neuromorphic_detection}) with a physics-guided neural network (PgNN) for near-minimum-energy destination prediction (Section~\ref{subsec:physics_guided}). Finally, we integrate both components into our \emph{EV-Planner} system (Section~\ref{subsec:system_arch}), which processes sparse asynchronous data from an event camera and generates collision-free trajectories. 

\subsection{\textbf{Event-Based Neuromorphic Detection}}
\label{subsec:neuromorphic_detection}

Traditional frame-based vision methods often struggle on raw event streams due to the lack of photometric features like intensity or texture. To address this, we adopt a spike-driven approach inspired by \cite{nagaraj2023dotie}, where leaky integrate-and-fire (LIF) neurons asynchronously detect moving objects (e.g., a drone racing gate) in the field of view.

\subsubsection{Spiking LIF Neuron Model}
\label{sec:snn_model}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{SNN_Object_detect.pdf}
    \caption{%
        \textbf{Spiking Neural Network Pipeline for Event-Based Detection.} 
        Neuromorphic (event-driven) data feed into LIF neurons. 
        The top row compares low-speed (left) vs.\ high-speed (right) event flows; 
        faster motion triggers higher spike activity as the membrane potential 
        surpasses the threshold more frequently.
    }
    \label{fig:SNN_Object_detect}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{ev_box.pdf}
    \caption{%
        \textbf{Event-based Object Detection at Various Depths.} 
        Each sub-panel shows neuromorphic event output and a bounding box 
        around a moving gate as it recedes from the camera (left to right). 
        Although the event density decreases with increasing depth, 
        the SNN remains robust in isolating and tracking the gate in real time.
    }
    \label{fig:ev_box}
\end{figure*}

As shown in Fig.~\ref{fig:SNN_Object_detect}, each LIF neuron maintains a membrane potential $U[t]$ that evolves as:
\begin{equation} \label{lif_eq}
    U[t] \;=\; \beta \, U[t_{n-1}] \;+\; W \, X[t],
\end{equation}
where $\beta$ is the leak factor, $W$ is a (learnable) weight, and $X[t]$ is the aggregated spike input from a local neighborhood of pixels at time $t$. If $U[t]$ exceeds a threshold $U_{\text{th}}$, the neuron fires an output spike and resets. By adjusting $\beta$ and $U_{\text{th}}$, we effectively \emph{filter} objects based on speed: higher event rates from faster objects cause $U[t]$ to surpass $U_{\text{th}}$ quickly.

\subsubsection{Filtering Objects by Speed}
\label{sec:speed_filter}

Our single-layer SNN leverages the fact that:
\[
\text{Event Rate} \;\propto\; \text{Object Speed}.
\]
Thus, a fast-moving object generates a high density of events that rapidly accumulates the membrane potential. Slow or static objects produce sparse events, staying below the firing threshold. By tuning membrane leak and threshold parameters, we isolate high-speed targets.

\subsubsection{Bounding-Box Center Estimation}
\label{find_centre}

Once a neuron spikes, we know that the local region of pixels corresponds to a fast-moving target. To pinpoint its location:

\begin{enumerate}
    \item Identify the minimum and maximum spiking pixel coordinates: $\{X_{\min}, X_{\max}, Y_{\min}, Y_{\max}\}$.
    \item Form a bounding box at $(X_{\min},Y_{\min})$ to $(X_{\max},Y_{\max})$.
    \item Compute the bounding-box center:
    \begin{subequations}
    \label{eq:center}
    \begin{align}
        \text{center}_x 
        &= 
        X_{\min} \;+\; \Bigl\lfloor\frac{X_{\max} - X_{\min}}{2}\Bigr\rfloor,\\
        \text{center}_y 
        &= 
        Y_{\min} \;+\; \Bigl\lfloor\frac{Y_{\max} - Y_{\min}}{2}\Bigr\rfloor.
    \end{align}
    \end{subequations}
\end{enumerate}


Figure~\ref{fig:ev_box} demonstrates bounding-box detection at different depths. Despite reduced event density for distant gates, our single-layer SNN accurately localizes the object without extra clustering algorithms, offering both speed and simplicity for real-time deployment.

\subsection{\textbf{Physics-Guided Destination Predictor}}
\label{subsec:physics_guided}

While the SNN handles perception, our system incorporates a physics-based model of the drone’s propulsion to plan near-optimal trajectories. We rely on a \textit{physics-guided neural network} (PgNN) to estimate an energy-efficient velocity for a given depth and thus predict the flight time needed to reach an object (e.g., the moving gate).

\subsubsection{\textbf{Drone Energy Model}}
\label{phyro}

We model quadrotor propulsion as LiPo-powered brushless DC (BLDC) motors \cite{energy}, each with resistive/inductive windings and friction. Under standard assumptions (neglecting ESC inefficiencies), the motor voltage is:
\begin{equation}
    e(t) \;=\; R\,i(t) + K_E \,\omega(t),
\end{equation}
where $R$ is winding resistance, $i(t)$ is current, $K_E$ is a voltage constant, and $\omega(t)$ is rotor speed. Power is $P(t) = e(t)\,i(t)$, and total energy used from $t=0$ to $t = T$ is:
\begin{equation} \label{eq:E}
    E(T) \;=\; \int_{0}^{T} \sum_{j=1}^{4} e_j(\tau)\;i_j(\tau)\;d\tau.
\end{equation}
Simulations reveal a \textit{non-monotonic} speed–energy curve: moving too slowly wastes energy over a long flight time, while moving too fast demands high current draw. Thus, each distance has an optimal velocity that minimizes total energy.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{energy_time_new.pdf}
    \caption{Optimal velocity and energy consumption patterns derived using the Physics-Guided Neural Network (PgNN). The plots illustrate the relationship between linear velocity, depth, and actuation energy, along with the computed optimal velocities $v_{\mathrm{opt}}$ for energy-efficient flight trajectories. These insights inform the PgNN's training and the derivation of $t_{\mathrm{traj}}$ (Eq.~\ref{eq:ttraj}).}
    \label{fig:energy_time}
\end{figure*}


\subsubsection{Physics-Guided Neural Network (PgNN)}

We collect a dataset of \{depth, velocity, energy\} triplets from simulated drone flights, fitting a 5th-degree polynomial to $E(v)$ for each depth:
\[
E(v) = c_0 + c_1\,v + c_2\,v^2 + \ldots + c_5\,v^5.
\]
Setting its derivative $\tfrac{dE}{dv}$ to zero approximates the optimal velocity. The PgNN is trained with depth as input and outputs a predicted $v^{\mathrm{pred}}$, constrained by the zero-derivative condition to remain physically consistent. Finally, the approximate flight time is:
\begin{equation}
\label{eq:ttraj}
    t_{\mathrm{traj}} \;=\; \frac{d}{v^{\mathrm{pred}}},
\end{equation}
where $d$ is the measured depth. This $t_{\mathrm{traj}}$ is a key parameter for our symbolic planner (Section~\ref{subsec:system_arch}).

\begin{table}[ht]
\centering
\caption{PgNN Training Samples: Depth, Velocity, and Physical Constraints}
\label{tab:data}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Depth} & \textbf{Velocity} & \textbf{Constraint}                  \\ \midrule
$d_1$          & $v_1$             & $c_{1,1} + 2\,c_{2,1}\,v_1 + \ldots = 0$       \\
$d_2$          & $v_2$             & $c_{1,2} + 2\,c_{2,2}\,v_2 + \ldots = 0$       \\
$\cdots$       & $\cdots$          & $\cdots$                                      \\
$d_n$          & $v_n$             & $c_{1,n} + 2\,c_{2,n}\,v_n + \ldots = 0$       \\ \bottomrule
\end{tabular}
\end{table}

\section{System Design}
\label{subsec:system_arch}


Having detailed both \emph{neuromorphic detection} and \emph{physics-guided} modules in preceding sections, we now present our integrated \textbf{EV-Planner}\cite{evplanner}, as depicted in Fig.~\ref{fig:method}. Designed with a loosely coupled architecture, the planner comprises three main blocks:

\begin{enumerate}
    \item \textbf{Event Camera \& SNN Block:}  
          Captures asynchronous brightness changes via a neuromorphic sensor, sending event streams to a spiking neural network (SNN). The SNN identifies the moving gate’s bounding box in near-real time by filtering high-speed objects (see Section~\ref{subsec:neuromorphic_detection}).

    \item \textbf{Depth Camera \& PgNN Block:}  
          Uses depth (i.e., distance to the gate) as input to a physics-guided neural network (PgNN). This network, constrained by an energy model, predicts a near-optimal velocity and the flight time \( t_{\mathrm{traj}} \) needed to traverse that distance (Section~\ref{phyro}).

    \item \textbf{Symbolic Planner Node:}  
          Consumes SNN-based position estimates (gate center) and PgNN-based flight times. It applies hard-coded motion rules to anticipate gate movement, then outputs collision-free waypoints or velocity commands to the UAV’s low-level controller.
\end{enumerate}

By maintaining a loosely coupled design, each module can be developed or replaced independently, streamlining integration. In our simulation environment (built on \textbf{ROS} and \textbf{Gazebo}), the SNN node publishes bounding-box centers, while the PgNN node publishes \( t_{\mathrm{traj}} \). A third node (the symbolic planner) merges these streams to account for gate motion and issues velocity or waypoint commands to the drone controller.

\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth]{EV-Planner_method_v2.pdf}
\caption{
    \textbf{Overall System Architecture.} 
    Loosely coupled modules: 
    (1)~SNN block for neuromorphic detection, 
    (2)~PgNN block for near-optimal trajectory predictions, 
    and (3)~a symbolic planner node for collision-free flight.
}
\label{fig:method}
\end{figure}
\subsection{Symbolic Planning for Moving Gates}
\label{symbol}

In our drone racing scenario, the gate oscillates along the \( y\)-axis, reversing direction at \(\pm L\). Suppose \((y_1,y_2)\) are consecutive gate positions tracked by the SNN over a time interval \(\delta t\). The gate’s velocity is
\[
    v_r \;=\; \frac{\,y_2 - y_1\,}{\delta t}.
\]
We combine \( v_r \) with the predicted flight time \( t_{\mathrm{traj}} \) from the PgNN to estimate where the gate will be after \( t_{\mathrm{traj}} \). Algorithm~\ref{algo:planner} outlines the logic:

\begin{algorithm}[t]
\caption{Post-Inference Processing}
\label{algo:planner}
\KwIn{$t_{\mathrm{traj}}, L, y_1, y_2, \delta t$}
\KwOut{$y^*$ \quad(\text{Predicted gate center after } t_{\mathrm{traj}})}

$v_r \gets \frac{y_2 - y_1}{\delta t}$ 
\tcp*[l]{\small Gate velocity from two consecutive neuromorphic detections}

$d_1 \gets v_r \cdot t_{\mathrm{traj}}$
\tcp*[l]{\small Gate displacement during the drone's flight time}

\If{$v_r > 0$}{
    $d_2 \gets L - y_2$ 
    \tcp*[l]{\small Distance to right boundary}
}
\Else{
    $d_2 \gets y_2 + L$ 
    \tcp*[l]{\small Distance to left boundary}
}

\If{$|d_1| > |d_2|$}{
    $\text{x} \gets |d_1| - |d_2|$ \;
    \eIf{$v_r > 0$}{
        $y^* \gets L - \text{x}$
        \tcp*[l]{\small Gate reverses from right to left}
    }{
        $y^* \gets -L + \text{x}$
        \tcp*[l]{\small Gate reverses from left to right}
    }
}
\Else{
    $y^* \gets y_2 + d_1$
    \tcp*[l]{\small Gate continues moving in the same direction}
}

\Return $y^*$
\end{algorithm}


This symbolic node updates the expected gate center \( y^* \) by comparing how far the gate moves within \( t_{\mathrm{traj}} \) to the distance left until it hits a boundary. When a reversal occurs, the code adjusts \( y^* \) accordingly. Afterwards, we generate a \emph{minimum-jerk trajectory}~\cite{mellinger2011minimum} to ensure smooth, energy-efficient flight—by minimizing the jerk (rate of change of acceleration), the drone draws lower current and prolongs battery life.


In this framework, we rely on ROS to orchestrate sensor inputs, publish SNN/PgNN outputs, and execute the symbolic planner node. Gazebo provides a realistic 3D physics environment for testing dynamic gate motion and UAV maneuvers, while PyTorch underpins both the spiking neural network (SNN) and physics-guided neural network (PgNN), leveraging GPU acceleration when available. By integrating neuromorphic vision for low-latency object detection, physics-informed trajectory planning for efficient flight times, and a symbolic rule-based planner for gate-motion prediction, the system achieves robust, near-minimum-energy navigation in dynamic environments.