"How can robots perceive and react to their environments as efficiently as humans?" This question lies at the heart of autonomous navigation research, driving efforts to create systems that are both adaptive and efficient. Autonomous navigation has been a hot topic of robotics research, with a growing emphasis on leveraging artificial intelligence (AI) to enhance performance. However, traditional AI-based navigation algorithms often fall short in dynamic and reactive scenarios, such as close-in obstacle avoidance, where rapid decision-making is paramount. These limitations stem from the inability of conventional sensing modalities to provide data with sufficient speed and reliability. Vision-based algorithms, while effective for high-level decision-making, face challenges such as high energy consumption and latency inherent to frame-based cameras, undermining their utility in energy-constrained aerial systems.

Neuromorphic event cameras, also known as Dynamic Vision Sensors (DVS) \cite{dvs1,dvs2,dvs3}, have emerged as a promising alternative to frame-based cameras. Inspired by biological vision, these sensors detect changes in intensity asynchronously, offering advantages like low latency, high dynamic range, and robustness to motion blur. Unlike conventional cameras that capture redundant static frames, event cameras record only significant intensity changes, reducing bandwidth and enabling data capture at microsecond granularity. This asynchronous data stream aligns naturally with the computational model of spiking neural networks (SNNs) \cite{lif,lee2020spike}, which process information in a similarly event-driven manner. Together, event cameras and SNNs provide a foundation for neuromorphic vision systems capable of low-latency, energy-efficient processing.

Simultaneously, physics-based artificial intelligence (AI) has gained traction in robotics, providing a means to integrate system-level physical knowledge into neural networks. By embedding physical models into the learning process \cite{raissi2019physics,karniadakis2021physics}, physics-based AI enhances model robustness, interpretability, and energy efficiency. In the context of aerial robotics, where energy efficiency and real-time responsiveness are critical, such integration becomes highly advantageous. Physics-guided neural networks (PgNNs) leverage prior knowledge of system dynamics \cite{NICODEMUS2022331,rampnet,chee2022knode} to predict optimal trajectories, reducing reliance on extensive training datasets and computational resources.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{prelim.pdf}
  \caption{%
    \textbf{Event-Based Vision and Spiking Neuron Model:}
    Illustration of the conceptual flow from event-based sensing to biological inspiration
    and spiking neural networks (SNNs). 
    \textit{Left:} An event camera outputs discrete intensity changes over time (red/blue dots) 
    rather than continuous image frames.
    \textit{Center:} Biological neurons communicate via discrete spikes, with dendrites 
    receiving inputs that the soma integrates before generating an output spike. 
    \textit{Right:} In SNNs, inputs are represented as spikes over time, and the neuron body (soma) 
    converts these event-driven signals into an output spike train, resembling biological neuronal firing.
  }
  \label{fig:event_based_processing}
\end{figure*}

In this work, we present an energy-efficient navigation framework that combines event-based vision with physics-guided planning for autonomous drones.  The system employs a neuromorphic event camera for detecting dynamic objects using a shallow SNN architecture in an unsupervised manner. Additionally, a lightweight PgNN, trained with depth sensor inputs, predicts near-optimal flight times, generating energy-efficient trajectories. By fusing depth and event-based data, the framework achieves robust object tracking and trajectory planning in complex environments.

The proposed framework is implemented in the Gazebo simulator  utilizing the Robot Operating System (ROS) middleware. Our system demonstrates the ability to detect and navigate through moving gates, avoiding collisions while minimizing energy consumption.  This work illustrates the potential of integrating neuromorphic event-based vision with physics-guided planning for aerial robotics. 
