\section{Method}
\label{sec:method}
We train a push policy for mobile manipulators to repose objects on a plane. The policy provides cartesian commands for the mobile base and joint position commands for the first five joints of the arm. We freeze the 6\textsuperscript{th} joint since it is only useful when using a gripper. The velocity commands are sent to a pre-trained locomotion controller to convert them into joint position commands for the legs. Both the push policy and locomotion policy are inferred at the same frequency of 50 Hz. The overall architecture is shown in~\cref{Fig:framework}.
\subsection{Locomotion control}
\label{subsec:robot_locomotion}
%
The approach is validated using a quadrupedal mobile platform, ANYmal, with a six DoF robotic arm mounted on it. The locomotion controller is a student policy similar to the one in \cite{miki2024learningwalkconfinedspaces}. It accepts the base command $\pmb u_{base}^{cmd} = (v_x, \ v_y, \ \omega_z, \ \zeta, \ \theta_, \ h) \in \mathbb{R}^6$ and outputs leg joint position targets. The six components of the command $\pmb u_{base}^{cmd}$ consist of linear velocity in $x$ and $y$ directions, angular yaw velocity, roll and pitch angle, and height position, respectively. The policy is trained with randomized arm motions and includes the arm joint positions in the observation. This way, the resulting locomotion policy is robust against the range of arm motions. While training the proposed controller (push policy), we freeze the pre-trained locomotion controller.

\begin{figure}
  \centering
  \graphicspath{{figures/}}
  \includegraphics[width=\columnwidth]{figures/policy-arch_v2.pdf}
  \vspace{-20pt}
  \caption{The control pipeline used for moving and reorienting an object to a planar goal (dark object). Push policy is the proposed controller.}
  \label{Fig:framework}
\end{figure}

% In the context of RL, the robot control problem is typically formulated as a  Markov Decision Process (MDP), noted with $(\mathcal{S}, \mathcal{A}, r, \gamma, \mathcal{T})$ where the notations are the state space $\mathcal{S}$, action space $\mathcal{A}$, reward function $r$, discount factor $\gamma$ and dynamics $\mathcal{T}$. Solving the problem consists of finding a policy $\pi$ that maximizes the discounted sum of future rewards $\max_\pi \mathbb{E}_{\tau \sim \pi, \mathcal{T}} \left [ \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \right ]$. To incorporate constraint satisfaction in the MDP solution, the authors of \cite{chanesane2024cat} propose to reformulate the problem as:
% %
% \begin{equation}
% \max_\pi \underset{\tau \sim \pi}{\mathbb{E}} \! \left [ \sum_{t=0}^\infty \! \left ( \prod_{t'=0}^t \gamma (1 - \delta (s_{t'}, a_{t'})) \! \right ) \! r(s_t, a_t) \right ], 
% \label{eq:cat_mdp}
% \end{equation}
% %
% The newly introduced random continuous variable $\delta_t : \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$  is a function of the constraint violations $c_i$ and can be interpreted as the probability of future reward termination (not naive environment resets). The computation of $\delta$ is described in \eqref{eq:delta_function}:
% \begin{equation}
% \label{eq:delta_function}
% \delta = \max_{i \in \mathbb{N}} \: p_i^\text{max} \text{clip}\Big(\frac{c_i^+}{c_i^\text{max}}, 0, 1\Big),
% \end{equation}
% where $c_i^+ = \max(0, c_i(s, a))$ is the violation of constraint $i \in \mathbb{N}$, $c_i^\text{max}$ is an exponential moving average of the maximum constraint violation over the last batch of data collected and $p_i^\text{max} \in [0, 1]$ is a hyperparameter for the maximum termination probability. It is worth mentioning that each constraint has to be formulated such that its violation $c_i^+$ is positive. Finally, the maximum impact $\delta_t$ can have on the training can be regulated or scheduled through the hyperparameter $p_i^\text{max}$. 

\subsection{RL goal pushing environment}
\label{sec:rl_environment}

We implement the task of moving and reorienting an object using NVIDIA Isaac Lab \cite{mittal2023orbit} for training the RL policy with 4096 parallel simulated robots for 20000 iterations. We modified the Proximal Policy Optimization (PPO) implementation from \cite{rudin2022learning} with the changes in \cite{chanesane2024cat} to derive the constrained PPO formulation. The reader is advised to read \cite{chanesane2024cat} for the details on constrained RL.

\textit{Notation}: In the following we use $\pmb p \in \mathbb{R}^3$, $\pmb R \in \mathbb{SO}(3)$, $\pmb v \in \mathbb{R}^3$ and $\pmb \omega \in \mathbb{R}^3$ to denote position, rotation matrix, linear and angular velocity of a body's frame, respectively. The body frame name is denoted as a right subscript and the reference frame as a left superscript (omitted when the reference frame and body frame are the same). We use the letters $w$, $b$, $o$, $g$, and $e$ to refer to the world, robot base, object, object goal, and arm end-effector frames, respectively. For the relative position between two body frames, two letters are used at the right subscript, \eg $^b\pmb p_{oe}$ is the relative position of the end-effector w.r.t. the object frame expressed in the robot base frame. We use $\widehat{\cdot}$ and $\|\cdot\|$ to express a given vector's unit vector and length, respectively.

\textit{Environment \& commands}: The RL training environment consists of multiple object-centered environments (parallelly simulated), each reset when there is an episode timeout (20 sec after the last reset) or when there is an unrecoverable object or robot fall. \cref{Fig:environment}A shows a single environment after a reset when the robot, object, and object goal positions are resampled in polar coordinates. The object ($^w\pmb p_o, ^w\pmb R_o$) is spawned at the origin of the environment ($W$). In contrast, the robot is spawned with its base frame ($^w\pmb p_b, ^w\pmb R_b$) at a random position inside an origin-centered annulus (yellow area with radius in the range $[1.2, 2.5] \ m$). An annulus is selected to prevent the robot from starting too close to the object or having collisions between them. Moreover, we aim for the robot to learn an approaching motion. The object's goal position $^w \pmb p_g$ is sampled at an origin-centered circular area (black dashed line, radius $2 \ m$). The yaw orientation of the object, the object goal, and the robot base are sampled randomly along the whole range $[-\pi, \pi]$. We consider success when the distance between the object's frame and the goal is less or equal to $d_{success} = 10 \text{ cm}$ and the angle between their orientation is less or equal to $\theta_{success} = 10 \text{ deg}$.

\begin{figure}
  \centering
  \graphicspath{{figures/}}
  \includegraphics[width=0.9\columnwidth]{figures/environment.pdf}
  \vspace{-8pt}
  \caption{A) The object's position is set to the environment origin ($W$), the robot base position is randomly sampled within an origin-centered annulus (yellow-shaded area), and the object goal (dark rectangle) within a circular area (dashed line). The robot, object, and goal are spawned with a random yaw orientation. B) Sampling on the object surface encourages interaction with different parts of the object during training.}
  \label{Fig:environment}
\end{figure}

During exploration in simulation, we want to encourage interactions of the robot with the whole surface of the object so that the RL agent discovers which part of the object is better to interact with. To that end, a reaching target position $^w\pmb p_r$ is randomly sampled on the object's vertical surfaces, as shown in \cref{Fig:environment}B, at each environment reset. This target is used in the reward function to guide the robot EE towards interacting with the object, as explained in \cref{sec:rewards_constraints}.

\subsection{Observation \& action space}
\label{sec:obsrevation_action}
This work uses an asymmetric actor-critic approach \cite{pinto2017asymmetric, ma2023learning} where the critic can access privileged information available only in simulation and noiseless. All observed quantities are described in \cref{tab:observations}. It is worth noting that the only information regarding the object included in the actor's observation vector $\pmb o_t \in \mathbb{R}^{54}$ is the object pose and, thus, the deployed policy has no knowledge about the object size and dynamics. The action vector $\pmb a_t = (\Delta \pmb u_{base}^{cmd}, \ \Delta \pmb q_j^{cmd}) \in \mathbb{R}^{11}$ consists of base commands $\Delta \pmb u_{base}^{cmd}$ for the locomotion policy described in \cref{subsec:robot_locomotion} and arm joint position commands $\Delta \pmb q_j^{cmd} \in \mathbb{R}^5$. The actions generated by the policy refer to deviations from a default base state (zero velocities, zero orientation, and default height of 0.5 m) and a default arm configuration, respectively. Thus, they are transformed into absolute values before being passed on to the locomotion policy and low-level joint impedance controllers. In \cref{tab:observations}, the quantities observed by the critic $\pmb o_t^{critic} = (\pmb o_t, \ \pmb o_t^{pr}) \in \mathbb{R}^{73}$, including the privileged information $ \pmb o_t^{pr}$.

%
\begin{table}
\caption{Observations for the actor ($\pmb o_t$) and critic ($\pmb o_t, \pmb o_t^{pr}$). Unlike the actor, the critic receives noiseless observations.}
\label{tab:observations}
\vspace{-18pt}
\begin{center}
  \begin{tabular}{P{0.25cm}|P{0.55cm}|p{4.25cm}|P{0.35cm}|P{1.1cm}}
  \hline
   \multicolumn{3}{P{5.1cm}|}{\textbf{Description}}                                                          & \textbf{Dim}      & \textbf{Noise}            \\
   \hhline{=|=|=|=|=}
   \multirow{10}{*}{$\pmb o_t$} & $^b\pmb p_{oe}$            & EE-object relative position w.r.t. base       & 3        & $\mathcal{U}(\pm0.02)$         \\ \cline{2-5}
   & $^b\pmb R_o$                & object rotation matrix w.r.t. base            & 9        & $\mathcal{U}(\pm0.01)$         \\ \cline{2-5}
   & $\Delta \pmb q_j$           & arm joint position readings w.r.t. default configuration               & 5        & $\mathcal{U}(\pm0.01)$         \\ \cline{2-5}
   
   & $\pmb v_b$                & robot base linear velocity                 & 3        & $\mathcal{U}(\pm0.01)$         \\ \cline{2-5}
   & $\pmb \omega_b$           & robot base angular velocity                & 3        & $\mathcal{U}(\pm0.20)$          \\ \cline{2-5}
   & $\dot{\pmb q}_j$            & arm joint velocity readings                & 5        & $\mathcal{U}(\pm0.50)$          \\ \cline{2-5}
   & $^b\pmb g_z$                & projected gravity unit vector & 3        & $\mathcal{U}(\pm0.05)$         \\ \cline{2-5}
   & $^b\pmb p_{og}$             & object-goal relative position w.r.t. base   & 3        & $\mathcal{U}(\pm0.02)$         \\ \cline{2-5}
   & $^o\pmb R_g$                & goal orientation w.r.t. object              & 9        & $\mathcal{U}(\pm0.01)$         \\ \cline{2-5}

   & $\pmb a_{t-1}$              & previous actions                           & 11       & -                    \\
   \hhline{=|=|=|=|=}
   \multirow{8}{*}{$\pmb o_t^{pr}$} & $\lambda_{e}$              & EE-object contact state                    & 1       &                       \\ \cline{2-4}
   & $^b\pmb p_{com}$            & object CoM position w.r.t. robot base         & 3       &                       \\ \cline{2-4}
   & $m$                         & object mass                                & 1       &                       \\ \cline{2-4}
   & $\pmb d$                    & object dimensions                   & 3       & -                     \\ \cline{2-4}
   & $\pmb I_o$                  & object's principal moments of inertia      & 3       &                       \\ \cline{2-4}
   & $^b\pmb v_{o}$            & object linear velocity w.r.t. robot      & 3       &                       \\ \cline{2-4}
   & $^b\pmb \omega_{o}$       & object angular velocity w.r.t. robot     & 3       &                       \\ \cline{2-4}
   & $\pmb \kappa_{sh}$          & one-hot vector for object shape            & 2       &                       \\ \hline    
   \end{tabular}
\end{center}
\end{table}

\input{TableConstriants}
%
\subsection{Rewards \& constraints}
\label{sec:rewards_constraints}
In this section, we describe the reward and constraint terms included in the training. We tune them manually to achieve convergence in simulation and then transfer the policy to the hardware zero-shot without further adjustments.

\textit{Rewards}: The total reward $r_t^{tot} = \sum_{n=1}^{4} w_i r_{i,t}$ consists of the sum of the reward terms shown in \cref{tab:constraints}. For the weights we used the values $w_1 = 2.5$, $w_2 = 1.25$, $w_3 = 0.156$, $w_4 = 0.3$. The term $r_{1,t}$ is the main task reward, which encourages minimizing the distance between the eight keypoints of the object and the keypoints of the goal, where these are defined as the vertices of the oriented bounding box of the object (similarly to \cite{allshire_trifinger, quadruped_pushing}). We denote the position of the keypoints of the object and its goal as $^w\pmb K_o \in \mathbb{R}^{24}$ and $^w\pmb K_g \in \mathbb{R}^{24}$, respectively. The reward term $r_{2,t}$ encourages the agent to minimize the distance between the arm EE and the reach target ($^w\pmb p_r$) sampled on the object's surface at each episode. The weight $w_2$ of this term is downscaled by a factor of 4 after 1500 learning iterations. We do this to encourage the robot EE to approach and interact with different parts of the object at the beginning, and we do not care about accurately reaching the sampled position. The term $r_{3,t}$ rewards object linear velocity with direction towards the object goal. We do not include the magnitude of the velocity in this term to avoid the robot pushing the object aggressively. Finally, the term $r_{4,t}$ comprises action rate regularization. If the task is successful, we increase the task reward to $r_{1,t}=2$, which is two times the maximum possible value of this term, so that the robot learns to achieve the specified tolerance instead of staying close to it. In case of success, the other rewards take values $r_{2,t}=r_{2,t-1}$, $r_{3,t}=0$ since once the goal has been reached, we do not encourage object velocity or interaction with the arm EE.
 
\textit{Constraints}: As proposed in \cite{chanesane2024cat}, we apply curriculum learning over most of the constraints by increasing the maximum probability for reward termination along training (linearly increasing $p_i^\text{max}$ from an initial smaller value to a final larger one after a number of learning iterations). In practice, at the beginning of the training, we encourage exploration by limiting the effect of constraint violation on the reward termination probability. We emphasize achieving strict constraint satisfaction for undesired collisions, arm joint position, and velocity limits. The undesired collisions include robot self-collisions and collisions of the object with the robot base, legs, and the arm's shoulder, upper arm, elbow, and forearm links. The arm joint torque constraint is not strict since we use as limits the maximum nominal torque values of the actuators and not the peak ones (which the joints can reach for short periods). We also include a constraint for the base command $\Delta \pmb u_{base}^{cmd}$ using as limits the ranges used during the training of the locomotion controller. Finally, the object balance constraint requires that the object's inclination angle be less than a specified threshold $\theta^{lim}=10^{\circ}$. All the constraints used for training, their dimension, and the curriculum applied are shown in \cref{tab:constraints}. 

\subsection{Domain randomization \& deployment}
\label{sec:domain_randomization}
To render the learned policy robust for deployment on the hardware, we randomize several factors in the simulated environment. The actor's observations are subject to additive uniform noise, as specified in \cref{tab:observations}. The static and dynamic friction between the object and the floor is randomized within the range of $[0.4,1.25]$ for the values of the combined coefficient. Moreover, the object mass is randomly sampled in the range 1-10 kg, and the center of mass (CoM) position is randomized around the object's centroid with deviation in the range $(dx, dy, dz)=([-0.25d_x, 0.25d_x], [-0.25d_y, 0.25d_y], [-0.6d_z, 0.25d_z])$, where $d_i$ is the dimension of the object in direction $i$. We also randomize the object dimensions in the ranges $(x, y, z)=([0.25, 0.75]m, [0.25, 0.75]m, [0.4, 1.0]m)$, and we train with cuboids and cylinders. Finally, the base mass for each simulated robot is randomly modified by $\pm$ 5 kg, and random pushes are applied to the robot base every 7 to 10 sec. The arm joint positions are randomized around the default configuration at each environment reset.

During deployment, we rely on an external motion capture system to get the object and robot base 6D pose information needed to derive the observation $\pmb o_t$. During testing on hardware, we infer the policy until success is achieved; then, we set the base command for the locomotion controller $\Delta \pmb u_{base}$ to zero to avoid the robot continuing stepping in place.