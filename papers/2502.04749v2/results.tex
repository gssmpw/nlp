\label{sec:results}
\subsection{Setup}

%\begin{figure*}[]
%	\centering
%	\includegraphics[width=0.8\linewidth]{grouping.png}
%	\caption{Plots comparing $E_\text{RMSE}$ and $E_\text{MAE}$ for the \textsc{Array-Averaging}, \textsc{Levy}, and \textsc{Quantile} algorithms, under the two grouping strategies. For the \textsc{Levy} algorithm, we fix $\gamma = 0.2$, and for the \textsc{Quantile} algorithm, we use the \textsc{FixedQuantile} subroutine.}
%	\label{fig:group}
%\end{figure*}

%Prajjwal: We compare our algorithms with two approaches. The first approach is a baseline which involves the naive use of laplace mechanism in which laplace noise is added with sensitivity $\frac{(b-a)m^{*}}{\sum_{i=1}^{n}m_{i}}$ where $m^{*}$ is the maximum number of samples contributed by any user. Thus the final estimate $\mu_{base1}$ is given by :
%
%\begin{equation}
%	\mu_{base1} = \frac{\sum_{i=1}^{n} \sum_{j=1}^{m_{i}} x_{j}}{\sum_{k=1}^{n} m_{k}} + Lap(\frac{(b-a)m^{*}}{\sum_{p=1}^{n}m_{p}\epsilon})
%\end{equation}
%
%The second approach is the algorithm proposed by Levy et. al in [Lev+21]. Their algorithm considers the case in which all users contribute same number of samples. So in order to fit their approach to out setting, we modify and extend their algorithm with \textsc{WrapAroundGrouping}. Note that their approach assumes samples drawn i.i.d from a sub-Gaussian distribution, and hence is not expected to give optimal results in a non-i.i.d setting. Additionally \textsc{BestFitGrouping} cannot be used in conjunction with their approach as the subsequent "Median of Means" algorithm for interval calculation requires all samples groups to be completely filled in order to exploit the concentration property contained by the coarse mean $\mu_{c}$ and $\tau$. $\tau$ is a constant which is given by $\sigma \sqrt{ \frac{2}{l} log(\frac{2k}{\gamma} ) }$. The interval estimated by [Lev+21] is given by $\mathcal{I}_{Levy}$[$\bar{\mu} - 1.5\tau$, $\bar{\mu} + 1.5\tau$] where $\bar{\mu}$ is the private median of means. Thus, the sensitivity of the mean of $Z_{i}$ after projecting them to this interval is limited to $\frac{3\tau}{k}$. Thus the final estimate using Levy et al.'s approach is given by:
%
%\begin{equation}
%	\mu_{levy} = \frac{\sum_{i=0}^{k}\prod_{\mathcal{I}_{levy}}Z_{i}}{k} + Lap(\frac{12\tau}{k\epsilon})
%\end{equation}
%
%For comparing our algorithm with their approach,  we set $\gamma = 0.2$ which corresponds to the probability that any mean $Z_{i}$ would lie beyond the interval returned by the algorithm. Since in our algorithm, $\alpha$ and $\beta$ correspond to the $10^{th}$ and $90^{th}$ quantiles respectively, we are effectively winsorizing 20\% of $Z_{i}$ which makes the comparison fair with [Lev+21] with $\gamma = 0.2$.


{In \cite{dp_spcom}, we evaluated our algorithms on two types of datasets: 1) a real-world ITMS dataset $\mathcal{C}$ containing non-i.i.d. speed values and 2) a synthetic dataset $\mathcal{D}$ containing i.i.d. samples drawn using insights from the ITMS data. We ran each private mean estimation algorithm $10^4$ times and tested the accuracy of our algorithms for privacy loss $\epsilon\in [0.5,2]$.}

We used the mean absolute error (or MAE) metric, $E_\text{MAE}(\mathcal{C}) = 10^{-4}\cdot {{\sum_{i=0}^{10^4} \left \lvert M^{(i)}(\mathcal{C}) - \mu(\mathcal{C})\right \lvert}}$, to evaluate the performance of the algorithms. Here, for $i\in [10^4]$, $M^{(i)}(\mathcal{C})$ is the result of running each algorithm $M$ in Section \ref{sec:alg} on  $\mathcal{C}$ at iteration $i$, and $\mu:=\mu(\mathcal{C})$ is the true sample mean. Since, for $Z\sim \text{Lap}(b)$, we have $\E[|Z|] = b$, we simply used $E_\text{MAE}(\mathcal{C}) = \Delta_f/\varepsilon$ for \textsc{Baseline}.
% How to write the percentile formula ?

\subsection{Experimental Results}
We refer the reader to Appendix \ref{app-numerics} for details on the ITMS dataset $\mathcal{C}$ and for plots of the performance of our algorithms on the ITMS data, which show that \textsc{Baseline} performs uniformly poorer than all other algorithms.

Next, we artificially generated\footnote{An interesting direction for future study is the $\epsilon$-differentially private generation of synthetic data that preserves some statistics of the base dataset (see \cite{vershynindp} and references therein).} a large dataset ${\mathcal{D}}$ using insights from the ITMS dataset. Let the number of users in $\mathcal{D}$ be $\widehat{L}$ and let the number of samples per user be $\{\widehat{m}_\ell\}_{\ell\in [\widehat{L}]}$.
%In particular, let $\widehat{L}$ denote the number of users and $\widehat{m_\ell}$, for $\ell \in [\widehat{L}]$ denote the number of samples contributed by user $\ell$. Recall that $L$ and $\{m_\ell: \ell \in [L]\}$ denote the number of users and the collection of the number of samples per user for the original ITMS dataset. 
We generated i.i.d. speed samples $\left\{\widehat{S}_j^{(\ell)}: \ell \in [\widehat{L}], j\in [\widehat{m_\ell}]\right\}$, where for any $\ell, j$, we have $\widehat{S}_j^{(\ell)}\sim \Pi_{[0,U]}(X)$, where $X\sim \mathcal{N}(\mu,\sigma^2)$. Here, 
$
\sigma^2 = \sigma^2(\mathcal{C}) = \frac{1}{\sum_{\ell=1}^{L}{m_\ell}}\cdot \sum_{\ell=1}^L \sum_{j=1}^{m_\ell}\left(S_j^{(\ell)}-\mu\right)^2
$
is the true variance of the ITMS dataset. 

%Note that the  random variable $\widehat{S}_j^{(\ell)}$ obeys
%$
%	\Pr\left[\hat{S}_j^{(\ell)} = 0\right] = Q\left(\frac{\mu(\mathcal{D})}{\sigma(\mathcal{D})}\right)\ \text{and}\ \Pr\left[\hat{S}_j^{(\ell)} = U\right] = Q\left(\frac{U-\mu(\mathcal{D})}{\sigma(\mathcal{D})}\right).
%$

We consider two settings, for a fixed positive integer $\lambda$.
\begin{enumerate}
	\item \textbf{Sample scaling}: Here, we set $\widehat{L} = L$ and $\widehat{m}_\ell = \lambda \cdot m_\ell$, for all $\ell\in [\widehat{L}]$. In this case, $\widehat{m}^\star := \max_{\ell\in [\widehat{L}]} \widehat{m}_\ell = \lambda\cdot m^\star$. 
	\item \textbf{User scaling}: Here, we set $\widehat{L} = \lambda\cdot L$ and for each $i\in [\lambda]$, we let $\widehat{m}_{\lambda\cdot(\ell-1)+i} = m_\ell$,  for all $\ell\in [{L}]$. Here, $\widehat{m}^\star = m^\star$.
\end{enumerate}
Plots for the performance of our algorithms under \textbf{sample scaling} and \textbf{user scaling} can be found in {Appendix} \ref{app-numerics}, with $\lambda$ set to $10$. 
Figure \ref{fig:sample-scale} there for \textbf{sample scaling} shows that \textsc{Baseline} performs worse than the other algorithms, as expected. However, interestingly, the \textsc{Levy} algorithm (with $\gamma = 0.2$) outperforms than all other algorithms. 
%The justification for the relative superior performance of \textsc{Levy} in the sample scaling setting 
%can be found in Appendix \ref{app-sample-scaling}.

%is as follows. First, observe that in \textsc{Levy}, we have that 
%%$
%%\widehat{m}_\text{UB} = \text{argmax}_{m} \frac{\sum_{\ell=1}^{L}\min \left  \{10\cdot m_{\ell}, m \right \}}{\sqrt{m}}.
%%$
%$\widehat{m}_\text{UB} = 10\cdot m_\text{UB}$ solves \eqref{eq:uboptim}, where $m_\text{UB}$ was the length of the arrays used in the absence of sample scaling. Furthermore, the number of arrays $\widehat{\overline{K}}$ obtained using \textsc{BestFit} grouping equals $\overline{K}$. Hence,
%$
%\Delta_{f_\text{Levy}}(\widehat{m}_\text{UB}) = \frac{3U}{\overline{K}}\sqrt{\frac{\log (2\overline{K}/\gamma)}{20m_\text{UB}}},
%$
%which is a factor $1/\sqrt{10}$ smaller than the user-level sensitivity without scaling. 
%In general, when the number of samples per user is scaled by a factor of $\lambda>1$, we get a multiplicative factor of $1/\sqrt{\lambda}$ improvement in the user-level sensitivity, and hence in the standard deviation of the noise added, thereby improving performance. 
%{
%In contrast, the sensitivities for \textsc{Baseline} and \textsc{Array-Averaging} remain unchanged from the corresponding values without sample scaling.} 

%Based on the analysis in Appendix \ref{app-sample-scaling}, in general, when the dataset contains a large number of samples per user, we recommend using \textsc{Levy}.

Figure \ref{fig:user-scale} in Appendix \ref{app-numerics} for \textbf{user scaling} shows that the \textsc{FixedQuantile} subroutine outperforms all the other algorithms. Furthermore, \textsc{Array-Averaging} performs second-best. We reiterate that we used $m_\text{UB} = m_{\text{UB}}^{(L)}$ (see \eqref{eq:uboptim}) for \textsc{Array-Averaging} as well, to maintain uniformity. 

In the next section, we provide theoretical justification for the performance trends we observe on large datasets.
%We justify these performance trends in Appendix \ref{app-user-scaling}.

%Figure \ref{fig:user-scale} for \textbf{user scaling} shows that the \textsc{FixedQuantile} subroutine outperforms all the other algorithms. We mention here that we use $m_\text{UB}$ as in \eqref{eq:uboptim} for \textsc{Array-Averaging} as well, to maintain uniformity. To justify the performance trends, note that for \textsc{Levy}, $\tau$ (and hence $\Delta_{f_\text{Levy}}$) increases (logarithmically) with the number of users. This implies that when  the number of users are sufficiently large, the computed interval $[a,b]$ will be equal to the entire interval $(0,U]$. Next, for \textsc{$\varepsilon$-DependentQuantile}, note that since $\epsilon\geq 0.5$, we have that the interval $[a',b']$ excludes $\left \lfloor 4/\varepsilon \right \rfloor \leq 8$ outlier speed samples, suggesting that for typical datasets with a large number of users, $b'-a'\approx U$. In comparison, the \textsc{FixedQuantile} subroutine eliminates a much larger number of data samples in the computation of $[a',b']$, implying that the amount of noise added in this case is low. 

%Based on the analysis in Appendix \ref{app-user-scaling}, for datasets with a large number of users, we recommend using \textsc{FixedQuantile}.

%\begin{figure}
%	\centering
%	\subfloat[]
%	{
%		\includegraphics[width=0.7\linewidth]{ConcentrationAlgoComparisonBestFit_NS+BS.png}
%		\caption{95th percentile error vs Epsilon graph}
%		\label{fig:itmscompare}
%	}
%	\subfloat[]
%	{
%		\includegraphics[width=0.7\linewidth]{ConcentrationAlgoComparisonBestFit_US.png}
%		\caption{95th percentile error vs Epsilon graph}
%		\label{fig:userscale}
%	}
%	\caption{ Two foo figures. }
%	\label{fig:foo}
%\end{figure}


%\begin{figure}
%	\centering
%	\begin{tabular}{cc}
%		%\subfloat[A]{\includegraphics[width=0.5\linewidth]{ConcentrationAlgoComparisonBestFit_NS+BS.png}} & 
%		\subfloat[Comparison of algorithms under sample scaling]{\includegraphics[width=0.4\linewidth]{samplescaling.png}} &
%		\subfloat[Comparison of algorithms under user scaling]{\includegraphics[width=0.4\linewidth]{optimizedquantile.png}}
%	%	\subfloat[C]{\includegraphics[width=0.5\linewidth]{ConcentrationAlgoComparisonBestFit_SynthUser+BS.png}} & 
%		\\
%	\end{tabular}
%	\caption{Many figures}
%\end{figure}




%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=0.7\linewidth]{ConcentrationAlgoComparisonBestFit_NS+BS.png}
%	\caption{95th percentile error vs Epsilon graph}
%	\label{fig:itmscompare}
%\end{figure}

%Do we need this? \begin{figure}[!h]
%	\centering
%	\includegraphics[width=3.4in]{ConcentrationAlgoComparisonBestFit_SS.png}
%	\caption{95th percentile error vs Epsilon graph}
%	\label{perc95}
%\end{figure}

%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=0.7\linewidth]{ConcentrationAlgoComparisonBestFit_US.png}
%	\caption{95th percentile error vs Epsilon graph}
%	\label{fig:userscale}
%\end{figure}

%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=0.7\linewidth]{ConcentrationAlgoComparisonBestFit_SynthUser+BS.png}
%	\caption{95th percentile error vs Epsilon graph}
%	\label{perc95}
%\end{figure}
%
%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=0.7\linewidth]{ConcentrationAlgoComparisonBestFit_Synth+BS.png}
%	\caption{95th percentile error vs Epsilon graph}
%	\label{perc95}
%\end{figure}