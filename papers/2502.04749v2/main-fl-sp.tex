\documentclass[10pt,conference]{IEEEtran}

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\else
\fi
\usepackage[cmex10]{amsmath}
\usepackage{amsthm,amsfonts,amssymb}
\usepackage{ifthen}
\usepackage{tikz}
\usetikzlibrary{decorations.text}
\usepackage{cite}
\usepackage{amsthm,amsfonts,amssymb}
\setcounter{MaxMatrixCols}{17}
\usepackage{graphicx}
%\usepackage[outdir=./]{epstopdf}
%\usepackage{epigraph}
%\usepackage{tikz}
%\usepackage{natbib}
%\setcitestyle{square}
\usepackage{subfig}
\usepackage{blkarray}
\usepackage{dsfont}
\usepackage[mathscr]{euscript}
\usepackage{enumitem}

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsfonts}
\usepackage{mathrsfs} 


\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{blkarray}
\usepackage{stfloats}%
%\newtheorem{theorem}{Theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\usepackage{url}
\usepackage{tcolorbox}
\usepackage{arydshln}
\usepackage{mathtools}
\usepackage{dsfont}
\definecolor{brickred}{cmyk}{0,0.89,0.94,0.28}
\definecolor{goldenrod}{cmyk}{0,0.10,0.84,0}
\definecolor{purple}{cmyk}{0.45,0.86,0,0}
\definecolor{rawsienna}{cmyk}{0,0.72,1,0.45}
\definecolor{olivegreen}{cmyk}{0.64,0,0.95,0.40}
\definecolor{peach}{cmyk}{0,0.5,0.7,0}
\definecolor{darkolive}{rgb}{0.,0.4,0.}
\colorlet{grey}{gray!40}

\def\tcw{\textcolor{white}}
\def\tcbr{\textcolor{brickred}}
\def\tcbl{\textcolor{blue}}
\def\tcrs{\textcolor{rawsienna}}
\def\tcog{\textcolor{olivegreen}}
\def\tcpl{\textcolor{purple}}
\def\tcgr{\textcolor{grey}}

\DeclareMathOperator{\argmin}{\text{argmin}}

\global\long\def\P{\mathbb{P}}
\global\long\def\E{\mathbb{E}}
\global\long\def\V{\mathrm{Var}}
\global\long\def\C{\mathrm{Cov}}
\global\long\def\I{\mathbbm{1}}
\global\long\def\d{\mathrm{d}}

\usepackage{xcolor}
\newcommand\arv[1]{{\color{red}\textbf{Arvind: #1}}}

\usepackage{xcolor}
\newcommand\pra[1]{{\color{blue}\textbf{Prajjwal: #1}}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor} % Use the [cmex10] option to ensure 
\interdisplaylinepenalty=2500

\usepackage[cmintegrals]{newtxmath}



\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Bounding User Contributions for User-Level Differentially Private Mean Estimation}
%\author{
%\IEEEauthorblockN{V.~Arvind~Rameshwar}
%\IEEEauthorblockA{IUDX Program Unit\\IISc, Bengaluru, India\\
%	Email: arvind.rameshwar@gmail.com}
%\and
%\IEEEauthorblockN{Anshoo~Tandon}
%\IEEEauthorblockA{IUDX Program Unit\\IISc, Bengaluru, India\\
%	Email: anshoo.tandon@gmail.com}
%\and
%\IEEEauthorblockN{Abhay~Sharma}
%\IEEEauthorblockA{IUDX Program Unit\\IISc, Bengaluru, India\\
%	Email: abhay.sharma@datakaveri.org}
%}
\author{\IEEEauthorblockN{
		V.~Arvind~Rameshwar\ \
		and\ \ 
		Anshoo~Tandon
	}
	\thanks{The authors are with the India Urban Data Exchange Program Unit, Indian Institute of Science, Bengaluru, India, emails: \texttt{\{arvind.rameshwar, anshoo.tandon\}@gmail.com}.}
}
\IEEEoverridecommandlockouts
\maketitle


\begin{abstract}

We revisit the problem of releasing the sample mean of bounded samples in a dataset, privately, under user-level $\varepsilon$-differential privacy (DP). We aim to derive the optimal method of preprocessing data samples, within a canonical class of processing strategies, in terms of the error in estimation. Typical error analyses of such \emph{bounding} (or \emph{clipping}) strategies in the literature assume that the data samples are independent and identically distributed (i.i.d.), and sometimes also that all users contribute the same number of samples (data homogeneity)---assumptions that do not accurately model real-world data distributions. Our main result in this work is a precise characterization of the preprocessing strategy that gives rise to the smallest \emph{worst-case} error over all datasets -- a \emph{distribution-independent} error metric -- while allowing for data heterogeneity. We also show via experimental studies that even for i.i.d. real-valued samples, our clipping strategy performs much better, in terms of \emph{average-case} error, than the widely used bounding strategy of Amin et al. (2019).
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
%Federated learning (FL) is now a widely studied framework for the collaborative training of a machine learning (ML) model by a large collection of devices (also called clients or users), by using decentralized training data. Typically, such a training task proceeds iteratively; in every round, each participating client performs a local update to the machine learning model using its training data and then sends this update to a central server, which is tasked with ``aggregating" the received updates. The aggregated model is then communicated back to the clients, which then begin a fresh round of local model updates. A canonical FL algorithm is the \texttt{FedAvg} algorithm introduced in \cite{fl-mcmahan}. Importantly, FL allows for the training of a large-scale ML model in a distributed fashion, while ensuring that the training data remains private to each client. For a thorough survey of the challenges in real-world deployment of FL, we refer the reader to \cite{kairouz-survey,li-survey}.

In this article, we concern ourselves with the fundamental problem of processing bounded, potentially vector-valued samples in a dataset, for the release of a private estimate of the sample mean. In particular, we work within the framework of ``user-level" differential privacy \cite{userlevel,hetero}, which is a generalization of the now widely adopted framework of differential privacy (DP) \cite{dworkroth, vadhan2017} for the design and analysis of privacy-preserving algorithms. Loosely speaking, user-level DP guarantees the privacy of a ``user", who could contribute more than one sample, by ensuring the statistical indistinguishability of outputs of the algorithm to changes in the user's samples. User-level DP has practical relevance for inference tasks on most real-world datasets, such as traffic datasets, datasets of user expenditures, and time series data, where different users contribute potentially different numbers of samples (data heterogeneity) \cite{usereg3,usereg4}. Moreover, user-level DP algorithms are increasingly becoming popular subroutines for integration into
federated learning (FL) frameworks -- see, e.g., \cite[Sec. 4]{kairouz-survey} for more details. 

%User-level privacy assumes significance in the context of real-world IoT datasets, such as traffic databases, which record multiple contributions from every user (see also \cite{usereg1,usereg2,usereg3,usereg4}). Specifically, user-level DP mechanisms for mean estimation \cite{userlevel,usereg4} are employed by the central server in a canonical FL algorithm such as \texttt{FedAvg} \cite{fl-mcmahan}, while computing a weighted average of gradients passed by each client; these mechanisms help ensure the privacy of a single user (or client), who potentially contributes several samples to the training dataset. 

There are two key requirements of such user-level DP mechanisms for mean estimation, for real-world applications. Firstly, the mechanisms must be designed to work with heterogeneous data. Secondly, one would like reliable reconstruction of the true sample mean, even when the data samples are non-i.i.d. (independent and identically distributed). Our focus is hence to  characterize an error metric, which is independent of the underlying data distribution and can be explicitly computed and optimized, for heterogeneous data.

%The second requirement is of particular importance in FL applications, where each client would like to reliably reconstruct the aggregated model update, in every round, from the noisy, privatized aggregate released by the central server, while running the FL algorithm on highly correlated real-world datasets.  

In this article, we confine our attention to (pure) $\varepsilon$-DP algorithms for mean estimation. A key subroutine in most user-level $\varepsilon$-DP mechanisms \cite{userlevel,hetero-user-level,hist-user-level,amin} is the preprocessing of the data samples for the release of an estimate of the sample mean, which requires the addition of less noise for privacy (measured via the sensitivity of the estimate), as against releasing a noised version of the true sample mean. Such a preprocessing procedure (also called a strategy for ``bounding" or ``clipping" strategy \cite{amin}) either drops certain samples contributed by selected users, or projects the samples to a ``high-probability interval" that is a strict subset of the interval in which the sample values are known to lie. While it is usually easy to establish that the mechanisms designed using the clipped estimators are differentially private, an analysis of their ``utility", or the error in estimation of the true statistic, often relies on distributional assumptions about the dataset. 

In this work, following \cite{dp_spcom,dp_preprint,tit-preprint}, we define and explicitly compute the \emph{worst-case error}, over all datasets, of general preprocessing (or bounding) strategies. The worst-case error metric is natural in settings with arbitrarily correlated data, where each user potentially ascribes his/her error tolerance to the worst dataset that the statistic is computed on. Furthermore, this error metric is clearly distribution independent and is computable under data heterogeneity too. We then explicitly identify the bounding strategy that results in the smallest worst-case error; our approach is hence an extension of those in \cite{amin} to the setting of the worst-case error, while allowing for arbitrary bounding strategies. Interestingly, we also observe from experimental studies that for scalar samples, our clipping strategy also gives rise to much smaller errors \emph{on average} compared to the strategy in \cite{amin}, for selected dataset sizes, when the samples are drawn i.i.d. according to common distributions.

%In recent work \cite{dp_spcom,dp_preprint,tit-preprint}, the authors introduced the notion of the \emph{worst-case error} incurred by a given mechanism, and showed that such an error metric is explicitly computable for selected clipped, $\varepsilon$-DP estimators for private mean and variance release. Importantly, the worst-case error metric is \emph{independent} of the distribution of the samples in the dataset, and is computable even when the data is heterogeneous. 

%In this work, we generalize the results in \cite{dp_spcom,dp_preprint} by considering clipping strategies that can project or drop data samples, potentially arbitrarily. Our analysis allows us to identify the clipping strategy that results in the smallest worst-case error -- such a strategy simply projects each data sample contributed by a user to an interval determined by the \emph{number} of samples he/she contributes. Our result can be seen as an extension of those in \cite{amin} to the setting of the worst-case error, while allowing for arbitrary bounding strategies. Interestingly, we also observe from experimental studies that our clipping strategy, which does not rely on additional, private parameter estimation unlike that in \cite{amin}, also gives rise to much smaller errors on average as compared to the strategy in \cite{amin}, for selected distributions of numbers of user contributions, when the samples are drawn i.i.d. according to common distributions.

%It is now well-understood that the release of even seemingly innocuous functions of a dataset that is not publicly available can result in the reconstruction of the identities of individuals (or users) in the dataset with alarming levels of accuracy (see, e.g.,  \cite{narayanan, sweeney}). To alleviate concerns over such attacks, the framework of differential privacy (DP) was introduced in \cite{dwork06}, which guarantees the privacy of any single sample. Subsequently, several works (see the surveys \cite{dworkroth, vadhan2017} for references) have designed DP mechanisms for the release of statistics such as the mean, variance, and histograms.




\section{Notation and Preliminaries}
\input{prelim.tex}
\section{Worst-Case Errors of Bounding Strategies}
\input{bounding.tex}
\section{Numerical Experiments}
\input{experiments.tex}
\section{Conclusion}
\input{conclusion.tex}


%\section*{Acknowledgment}


%The authors would like to thank...





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography dataset(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{IEEEtran}
{\footnotesize
	\bibliography{references}}
\appendix
\section{Characterizing $\ell_1$-Projections Onto $\Delta_\alpha$}
\input{app-projection.tex}
%\clearpage
%\appendices
%\section{Proof of convexity of $\Phi$}
%\input{app-convex.tex}
%\section{Improving $\ell_2$-Error Using Multiple Noisy Views}
%\input{honaker.tex}
%\section{Proof of Lemma \ref{lem:intnoncommon}}
%\input{intnoncommon.tex}
% that's all folks
\end{document}


