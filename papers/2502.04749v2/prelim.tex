\label{sec:prelim}
\subsection{Notation}
The notation $\mathbb{N}$ denotes the set of positive natural numbers. For $n\in \mathbb{N}$, the notation $[n]$ denotes the set $\{1,2,\ldots,n\}$. 
%An empty product is defined to be $1$. We use the notation $\mathbf{b}$ to denote a vector of predefined length, all of whose symbols are equal to $b\in \mathbb{R}$. 
%Given a real-valued vector $\mathbf{a} = (a_1,a_2,\ldots,a_n)$, we denote its $\ell_1$-norm by $\lVert \mathbf{a} \rVert_1:= \sum_{i=1}^n |a_i|$ and its $\ell_2$-norm by $\lVert \mathbf{a} \rVert_2:= \left(\sum_{i=1}^n a_i^2\right)^{1/2}$. 
%Random variables are denoted by upper-case letters, e.g., $X, Y, \mathbf{Z}$.
%Given a collection of real numbers $(x_1,\ldots,x_n)$, we define its median med$(x_1,\ldots,x_n)$ as any value $x$ such that $|\{i\in [n]: x_i\geq x\}| = \left \lceil n/2\right \rceil$. 
%We write $X\sim P$ to denote that the random variable $X$ is drawn from the distribution $P$. 
Further, given reals $a,b$ with $a\leq b$, we define $\Pi_{[a,b]}(x):= \min\{\max\{x,a\},b\}$, for $x\in \mathbb{R}$. For a vector $\mathbf{x}\in \mathbb{R}^d$, we define its $\ell_p$-norm $\lVert \mathbf{x}\rVert_p$, for an integer $p\in \mathbb{N}$ to be $(\sum_{i=1}^d |x_i|^p)^{1/p}$, with $\lVert \mathbf{x}\rVert_\infty:= \max_{1\leq i\leq d} |x_i|$. For a given set $\mathcal{X}\subseteq \mathbb{R}^d$ and a vector $\mathbf{a}$, we define, with some abuse of notation, $\mathcal{X}(\mathbf{a})$ to be an $\ell_1$-projection $\arg \min_{\mathbf{b}\in \mathcal{X}} \lVert \mathbf{a}-\mathbf{b}\rVert_1$. Given an integer $d\geq 1$ and a real $M\geq 0$, we define $\Delta_M$ to be the $d$-simplex, i.e., $\Delta_M:= \{\mathbf{a}\in \mathbb{R}^d:\ \sum_{i\leq d} a_i\leq M,\ a_i\geq 0,\ \text{for all $i\in [d]$}\}$, and the set $\delta_M:= \{\mathbf{a}\in \mathbb{R}^d:\ \sum_{i\leq d} a_i= M,\ a_i\geq 0,\ \text{for all $i\in [d]$}\}$. Further, for $0\leq \alpha\leq \beta$, we define, with some abuse of terminology, the ``annulus" $\mathsf{A}_{\alpha,\beta}$ as $\mathsf{A}_{\alpha,\beta}:= (\Delta_\beta \setminus \Delta_\alpha) \cup \delta_\alpha$. 

We use the notation $\text{Lap}(b)$ to refer to the zero-mean Laplace distribution with standard deviation $\sqrt{2}b$, the notation $\text{Unif}((0,a])$ to denote the uniform distribution on the interval $(0,a]$, and the notation $\mathcal{N}(\mu, v)$ to denote the Gaussian distribution with mean $\mu$ and variance $v$.

%; its probability distribution function (p.d.f.) obeys
%$$
%f(x) = \frac{1}{2b}e^{-|x|/b}, \ x\in \mathbb{R}.
%$$

%Given a random variable $X$, we denote its variance by $\text{Var}(X)$. The notation ``$\log$'' denotes the natural logarithm.

%and $\mathcal{N}(\mu,\sigma^2)$ to denote the Gaussian distribution with mean $\mu$ and variance $\sigma^2$. For a random variable $X\sim \mathcal{N}(0,1)$, we denote its complementary cumulative distribution function (c.c.d.f.) by $Q$, i.e., for $x\in \mathbb{R}$,
%$
%Q(x):=\Pr[X\geq x] = \int_{x}^{\infty} \frac{1}{\sqrt{2\Pi}} e^{-z^2/2}\d z.
%$
%\subsection{Problem Setup}
%The ITMS dataset that we consider stores records of the data provided by IoT sensors deployed in an Indian city, containing, among other information, the license plate of the bus, the location at which the data was recorded, a timestamp, and the actual data value itself, which is the instantaneous speed of the bus. We process the dataset to extract the data records corresponding to that {h}exagonal ``grid'' in the city {a}nd that {t}imeslot 
%%(we abbreviate \underline{H}exagon \underline{A}nd \underline{T}imeslot  as HAT) 
%which has the highest traffic. 
%We remark that the algorithms discussed in this paper are applicable to general spatio-temporal IoT datasets too. 

\subsection{Problem Formulation}
\label{sec:problem}
Let $L$ be the number of users present in the dataset. For every user $\ell\in [L]$, let the number of records contributed by the user be denoted by $m_\ell$, and set $m^\star:= \max_{\ell\in [L]} m_\ell$ and $m_\star:= \min_{\ell\in [L]} m_\ell$. We assume that $L$ and the collection $\{m_\ell: \ell \in [L]\}$ are known. Now, for a given user $\ell\in [L]$, let $\left\{\mathbf{x}_j^{(\ell)}: j\in [m_\ell]\right\}$ denote the collection of (potentially arbitrary) bounded samples contributed by the user, where each $\mathbf{x}_j^{(\ell)}\in \mathbb{R}^d$, for some dimension $d\geq 1$. We assume, as is common for most applications, that $\left \lVert \mathbf{x}_j^{(\ell)} \right \rVert_1 \leq U$ \cite{dpsgd,dp_spcom}, for all $\ell\in [L]$, $j\in [m_\ell]$, where $U> 0$ is known. Call the dataset as $\mathcal{D} = \left\{\left(\ell,\mathbf{x}_j^{(\ell)}\right): \ell \in [L], j\in [m_\ell]\right\}$.  
%The datasets of interest to us contain samples drawn according to some unknown distribution $P$ that is potentially non-i.i.d. (independent and identically distributed) across samples and users. 

We are interested in releasing the sample mean
$
	\label{eq:f}
f = f(\mathcal{D}):= \frac{1}{\sum_{\ell'=1}^L m_{\ell'}}\cdot \sum_{\ell=1}^L \sum_{j=1}^{m_\ell} \mathbf{x}_j^{(\ell)}.
$


%Call the dataset consisting of the speed records of users as $\mathcal{D} = \left\{\left(u_\ell,S_j^{(\ell)}\right): \ell \in [L], j\in [m_\ell]\right\}$, where the collection $\{u_\ell: \ell\in [L]\}$ denotes the set of users. 

%The function that we are interested in computing is the sample average
%$
%f(\mathcal{D}):= \frac{1}{\sum_{\ell=1}^L m_\ell}\cdot \sum_{\ell=1}^L \sum_{j=1}^{m_\ell} S_j^{(\ell)}.
%$

\subsection{Differential Privacy}
\label{sec:dp}
Consider datasets $\mathcal{D}_1 = \left\{\left(u_{\ell},\mathbf{x}_j^{(\ell)}\right): \ell \in [L], j\in [m_\ell]\right\}$ and $\mathcal{D}_2 = \left\{\left(u_\ell,\overline{\mathbf{x}}_j^{(\ell)}\right): \ell \in [L], j\in [m_\ell]\right\}$ consisting of the same users, with each user contributing the same number, $m_\ell$, of  data values. Let $\mathsf{D}$ denote a universal set of such datasets. We say that $\mathcal{D}_1$ and $\mathcal{D}_2$ are ``user-level neighbours'' if there exists $\ell_0\in [L]$ such that $\left(\mathbf{x}^{(\ell_0)}_{1},\ldots, \mathbf{x}^{(\ell_0)}_{m_{\ell_0}}\right)\neq \left(\overline{\mathbf{x}}^{(\ell_0)}_{1},\ldots, \overline{\mathbf{x}}^{(\ell_0)}_{m_{\ell_0}}\right)$, with $\left(\mathbf{x}^{(\ell)}_{1},\ldots, \mathbf{x}^{(\ell)}_{m_{\ell}}\right)= \left(\overline{\mathbf{x}}^{(\ell)}_{1},\ldots, \overline{\mathbf{x}}^{(\ell)}_{m_{\ell}}\right)$, for all $\ell\neq \ell_0$. 
%In this work, we concentrate on mechanisms that map a given dataset to a single real value.

\begin{definition}
	For a fixed $\varepsilon>0$, a mechanism $M: \mathsf{D}\to \mathbb{R}^d$ is  user-level $\varepsilon$-DP if for every pair of user-level neighbours $\mathcal{D}_1, \mathcal{D}_2$ and for every measurable subset $Y \subseteq \mathbb{R}^d$, we have that
	$
	\Pr[M(\mathcal{D}_1) \in Y] \leq e^\varepsilon \Pr[M(\mathcal{D}_2) \in Y].
	$
\end{definition}
%Next, we define the user-level sensitivity of a function of interest.
\begin{definition}
	Given a function $g: \mathsf{D}\to \mathbb{R}^d$, we define its user-level sensitivity $\Delta_g$ as
	$
	\Delta_g:= \max_{\mathcal{D}_1,\mathcal{D}_2\ \text{u-l nbrs.}} \lVert g(\mathcal{D}_1) - g(\mathcal{D}_2)\rVert_1,
	$
	where the maximization is over datasets that are user-level neighbours.
\end{definition}
%For example, the user-level sensitivity of $f$ is
%$\Delta_f = \frac{Um^\star}{\sum_\ell m_\ell}$. 
We use the terms ``sensitivity'' and ``user-level sensitivity'' interchangeably. The next result is well-known \cite[Prop. 1]{dwork06}.

\begin{theorem}
	\label{thm:dp}
	For any $g: \mathsf{D}\to \mathbb{R}^d$, the mechanism $M^{\text{Lap}}_g: \mathsf{D}\to \mathbb{R}$ defined by
	$
	M^{\text{Lap}}_g(\mathcal{D}_1) = g(\mathcal{D}_1)+\mathbf{Z},
	$
	where $\mathbf{Z} = (Z_1,\ldots,Z_d)$ is such that $Z_i\stackrel{\text{i.i.d.}}{\sim} \text{Lap}(\Delta_g/\varepsilon)$, $i\in [d]$, and is independent of $\mathcal{D}$, is user-level $\varepsilon$-DP.
\end{theorem}

\subsection{The Worst-Case Error Metric}

All through, in this paper, we shall work with user-level $\varepsilon$-DP mechanisms that add a suitable amount of Laplace noise that is tailored to the sensitivity of the function used as an estimator of the sample mean $f$. %Now, given any mechanism $M$ for the $\varepsilon$-DP release of $f$, we shall first recall the formal definition of its worst-case error  \cite{tit-preprint}.
Consider a mechanism $M$ for the user-level $\varepsilon$-DP release of the statistic $f$. The canonical structure of $M$ (see \cite[Footnote 1]{staircase}, \cite{opt}) is: $M(\mathcal{D}) = \overline{f}(\mathcal{D})+{\mathbf{Z}},$
%\begin{equation}
%	\label{eq:Mbar}
%	M(\mathcal{D}) = \overline{f}(\mathcal{D})+{\mathbf{Z}},
%\end{equation}
for some estimator $\overline{f}$ of $f$, with user-level sensitivity $\Delta_{\overline{f}}$, with $\mathbf{Z} = (Z_1,\ldots,Z_d)$, with $Z_i\stackrel{\text{i.i.d.}}{\sim} \text{Lap}\left(\Delta_{\overline{f}}/\varepsilon\right)$, $i\in [d]$.  
%The assumption that our mechanisms are \emph{additive-noise} or \emph{noise-adding} mechanisms is without loss of generality, since it is known that every privacy-preserving mechanism can be thought of as a noise-adding mechanism (see \cite[Footnote 1]{staircase} and \cite{opt}). Moreover, under some regularity conditions, for small $\varepsilon$ (or equivalently, high privacy requirements), it is known that Laplace distributed noise is asymptotically optimal in terms of the magnitude of error in estimation \cite{staircase,opt}.


%Note that we work with the class of mechanisms that add Laplace noise tailored to the sensitivities of each grid, individually, since an explicit computation of the user-level sensitivity of the vector $f$ in \eqref{eq:f} (across all grids) is quite hard, thereby implying the necessity of loose bounds on the amount of noise added, when this notion of user-level sensitivity is used.
%Recall, from the discussion preceding Theorem \ref{thm:comp}, that the assumption that the mechanism ${}^gM_\theta$ is Laplace noise-adding is ca.


%Now, consider the mechanism $M_\theta$ that consists of the composition of the mechanisms ${}^gM_\theta$, over $g\in \mathcal{G}$, i.e., $M_\theta = \left({}^gM_\theta:\ g\in \mathcal{G}\right)$. In many settings of interest, a natural error metric for such a composition of mechanisms acting on different grids is the \emph{largest}  \emph{worst-case} estimation error among all the grids. 

For the mechanism $M$, we define its \emph{worst-case} estimation error as
\begin{equation}
	\label{eq:eg}
	E_M = E_{\overline{f}}:= \max_{\mathcal{D}\in \mathsf{D}} \big \lVert f(\mathcal{D}) - \overline{f}(\mathcal{D}) \big\rVert_1 + \E[\lVert{\mathbf{Z}}\rVert_1].
\end{equation}
Clearly, the expression above is an upper bound on the $\ell_1$-error $E^{(1)}_M:= \max_{\mathcal{D}\in \mathsf{D}} \E[|f(\mathcal{D})-f(\mathcal{D})|]$, via the triangle inequality. The distribution-independent expression in \eqref{eq:eg} conveniently captures the errors due to bias (the first term) and due to noise addition for privacy (the second term); a similar such error measure that separates the bias and noise errors was employed in \cite{amin}. 
%and its \emph{worst-case} estimation error over all datasets $\mathcal{D}$ as
%\[
%{}^gE := \max_{\mathcal{D}\in \mathsf{D}} {}^gE(\mathcal{D}).
%\]
%Finally, we define the error metric $E$ of the mechanism $M_\theta$ to be the \emph{largest} worst-case estimation error among all the grids, i.e., $$E := \max_{g\in \mathcal{G}} {}^gE.$$