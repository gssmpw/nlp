\label{sec:conclusion}
In this paper, we revisited the fundamental problem of releasing the sample mean under user-level differential privacy (DP) -- a problem that is well-studied in the DP literature, but typically in the presence of additional (e.g., i.i.d.) assumptions on the distribution of data samples. In our work, we make no distributional assumptions on the dataset; instead, we adopt a worst-case approach to studying the estimation error. We then explicitly characterize this error for a broad class of strategies for bounding user contributions by some combination of clipping the numbers of user contributions and/or clipping the sample values themselves. Our analysis allows us to identify the bounding strategy that is optimal with respect to our worst-case error metric. Via numerical experiments, we demonstrate that our strategy is not only optimal for the worst-case error, but also performs much better than the well-known strategy in \cite{amin} for datasets with i.i.d. samples.

An interesting line of future work will be to extend our worst-case error analysis to the user-level DP release of other statistics, and identify the optimal bounding strategies in those cases as well.