\label{sec:bound}
%In this section, we present an explicit characterization of the worst-case errors for a broad class of strategies that work by ``bounding" or ``clipping" the contributions of users, as mentioned in the Introduction. In other words, the estimator $\overline{f}$ of $f$ in \eqref{eq:Mbar} is a suitably clipped or bounded version of the sample mean $f$. We then identify that clipping strategy that leads to the smallest worst-case error.
\subsection{On Clipping Strategies}
We work with estimators $\overline{f} = \overline{f}_{\left\{a_j^{(\ell)},b_j^{(\ell)}\right\}}$ of $f$ obtained by bounding user contributions as follows: for each $\ell\in [L]$ and $j\in [m_\ell]$, we let $\overline{\mathbf{x}}_j^{(\ell)}:= {\mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}}\left(\mathbf{x}_j^{(\ell)}\right)$, for reals $0\leq a_j^{(\ell)}\leq b_j^{(\ell)}\leq U$. In words, $\overline{\mathbf{x}}_j^{(\ell)}$ is a projection of $\mathbf{x}_j^{(\ell)}$ onto the set $\mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}$, which, intuitively, reduces the range of values that $\mathbf{x}_j^{(\ell)}$ can take, and hence its sensitivity too. Figure \ref{fig:proj} shows a pictorial depiction of the annulus $\mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}$ when $d=2$ and examples of projections onto the annulus. We then set 
$
\overline{f} = \overline{f}(\mathcal{D}):= \frac{1}{\sum_{\ell'=1}^L m_{\ell'}}\cdot \sum_{\ell=1}^L \sum_{j=1}^{m_\ell} \mathbf{\overline{x}}_j^{(\ell)}.
$
\begin{figure}
	\centering
	\begin{tikzpicture} [scale=1.6]% Adjust scale as needed
		
		\newcommand{\outerradius}{3}
		\newcommand{\middleradius}{2}
		\newcommand{\innerradius}{1}
		
		% Draw the axes with new labels (only positive quadrant)
		\draw[->, very thick] (0,0) -- (\outerradius+0.5,0) node[right] {$\left(x_j^{(\ell)}\right)_1$};
		\draw[->, very thick] (0,0) -- (0,\outerradius+0.5) node[above] {$\left(x_j^{(\ell)}\right)_2$};
		
		% Draw the balls with transparency (only positive quadrant)
		\draw[thick, fill=gray!20, fill opacity=0.5] (0,\outerradius) -- (\outerradius,0) -- (0,0);
		
		% Draw the blue region *first* (only positive quadrant)
		\fill[blue!30, fill opacity=0.5] (0,\middleradius) -- (\middleradius,0) -- (0,0);
		
		% Draw the inner ball - make it transparent gray (only positive quadrant)
		\draw[thick, fill=gray!20, fill opacity=0.5] (0,\innerradius) -- (\innerradius,0) -- (0,0); % Inner ball
		
		% Draw the middle ball outline in BLACK (only positive quadrant)
		\draw[thick,black] (0,\middleradius) -- (\middleradius,0) -- (0,0);
		
		\footnotesize
		% Add arrows and labels (red, bold tips)
		\draw[<->, very thick, red, line cap=round] (0,0) -- (\innerradius,0) node [below,font=\footnotesize] {$a_j^{(\ell)}$};
		\draw[<->, very thick, red, line cap=round] (0,0) -- (\middleradius,0) node [above,font=\footnotesize] {$b_j^{(\ell)}$};
		
		\draw[<->,very thick, red, line cap=triangle] (0,0) -- (\outerradius,0) node [above,font=\footnotesize] {$U$};
		
		% Mark the point (0.1, 0.1) and line segment
		\draw[thick, black] (0.1,0.1) node {\textbf{$\times$}} node[left] {$p_1$};
		\draw[dotted, thick] (0.1,0.1) -- (0.5,0.5);
		
		% Calculate the endpoint of the second line segment
		\pgfmathsetmacro{\endx}{1.1/1.2}
		\pgfmathsetmacro{\endy}{1.3/1.2}
		
		% Mark the point (1.1, 1.3)/1.2 with a circle
		\draw[thick, black] (\endx,\endy) circle (2pt) node[below left]{$q_2$};
		
		% Mark the point (1.1, 1.3)
		\draw[thick, black] (1.1,1.3) node {\textbf{$\times$}} node[above right]{$p_2$};
		
		% Draw the dotted line segment from (1.1, 1.3)
		\draw[dotted, thick] (1.1,1.3) -- (\endx,\endy);
		
		% Mark the point (0.5, 0.5) with a circle
		\draw[thick, black] (0.5, 0.5) circle (2pt) node[above right]{$q_1$};
		
	\end{tikzpicture}
	\caption{The annulus $\mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}$, for $d=2$, shown in blue. Here, the points $q_1$, $q_2$ equal $\mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}(p_1)$ and $\mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}(p_2)$, respectively.}
	\label{fig:proj}
\end{figure}
Note that the class $\mathsf{B}$ of estimators $\overline{f}$ as above  captures those estimators obtained by dropping selected samples $\mathbf{x}_j^{(\ell)}$ (by setting $a_j^{(\ell)}= b_j^{(\ell)} = 0$ for those samples) and those obtained by projecting samples onto an $\ell_1$-bounded subset of $\Delta_U$, in addition to strategies that perform a combination of dropping and projection. This class of estimators hence includes several common estimators of the sample mean used in works such as \cite{userlevel,dp_preprint,tit-preprint}.

%In the next subsection, we discuss the worst-case errors (as in \eqref{eq:eg}) of mechanisms that employ some $\overline{f}\in \mathsf{B}$ as the estimator of the sample mean.

\subsection{On Worst-Case Errors of Clipping Strategies}
\label{sec:worst-case}
Consider the quantity $E_{\overline{f}}$, for $\overline{f}\in \mathsf{B}$. The following proposition then holds:
\begin{proposition}
	\label{lem:worst-case}
	We have that
	\begin{align*}
		E_{\overline{f}} \notag&= \frac{1}{\sum_{\ell'\leq L} m_{\ell'}}\cdot \left(\sum_{\ell\leq L}\sum_{j\leq m_\ell} \max\left\{a_j^{(\ell)},U-b_j^{(\ell)}\right\}\right)\ + \notag\\
		&\ \ \  \ \ \ \ \ \ \  \ \ \ \ \ \ \ \  \ \ \ \  \ \ \ \frac{d\cdot\max_{\ell\leq L} \sum_{j\leq m_\ell} \left(b_j^{(\ell)}-a_j^{(\ell)}\right)}{\varepsilon\cdot \sum_{\ell'\leq L} m_{\ell'}}.
	\end{align*}
\end{proposition}
The proof of Proposition \ref{lem:worst-case} proceeds with help from a few lemmas. For $E_{\overline{f}}$ as in \eqref{eq:eg}, we define $\beta(\overline{f})$ to be the bias, i.e., $\beta(\overline{f}):= \max_{\mathcal{D}\in \mathsf{D}} \big \lVert f(\mathcal{D}) - \overline{f}(\mathcal{D}) \big\rVert$ and $\eta(\overline{f})$ to be the error due to noise addition, i.e., $\eta(\overline{f}):= \E[\lVert\mathbf{\mathbf{Z}}\rVert_1]$. First, we aim to characterize $\beta(\overline{f})$. To this end, we first state a necessary condition for a vector $\mathbf{y}$ to be an $\ell_1$-projection of a vector $\mathbf{a}\in \Delta_U$ onto $\Delta_\alpha$, for $ \alpha\leq U$. 
\begin{lemma}
	\label{lem:proj1}
	Given $\mathbf{a}\in \Delta_U$ and $\alpha\leq U$, we have $\Delta_\alpha(\mathbf{a}) = \mathbf{a}$, if $\mathbf{a}\in \Delta_{\alpha}$. Else, any $\ell_1$-projection $\mathbf{y} = \Delta_\alpha(\mathbf{a})$ must satisfy $\lVert \mathbf{y}\rVert_1 = \alpha$, with $y_i\leq a_i$, for all $i\in [d]$.
\end{lemma}
\begin{proof}
	The first statement in the lemma is clear. Now, for $\mathbf{a}\notin \Delta_\alpha$, suppose that $\mathbf{y} = \Delta_\alpha(\mathbf{a})$ is such that $\lVert \mathbf{y}\rVert_1 < \alpha$. It can then be seen that by setting $\mathbf{y}':= \lambda\mathbf{y}+(1-\lambda)\mathbf{a}$, for some $\lambda\in (0,1)$ such that $\lVert \mathbf{y}'\rVert_1 = \alpha$, we will obtain that $\lVert \mathbf{y}'-\mathbf{a}\rVert_1 < \lVert \mathbf{y}-\mathbf{a}\rVert_1$, which is a contradiction. Likewise, suppose that $y_i>a_i$, for some $i\in [d]$. Now, consider any coordinate $j\in [d]$ such that $y_j\leq a_j$ (such a coordinate must exist, since $\lVert \mathbf{y}\rVert_1 = \alpha$); by letting $m:= \min\{|y_i-a_i|,|y_j-a_j|\}$ and setting $y_i \gets y_i-m$ and $y_j \gets y_j+m$, we see that we strictly decrease $\lVert \mathbf{y}-\mathbf{a}\rVert_1$, which is a contradiction.
\end{proof}
In the appendix, we explicitly characterize the vectors $\mathbf{y}\in \Delta_\alpha$ that are the $\ell_1$-projections of $\mathbf{a}\in \Delta_U\setminus \Delta_\alpha$, for $\alpha\leq U$; indeed, we have that the condition stated in Lemma \ref{lem:proj1} is both necessary and sufficient. The next lemma exactly characterizes $\beta(\overline{f})$.
\begin{lemma}
	\label{lem:proj2}
	We have that $\beta(\overline{f}) = \frac{1}{\sum_{\ell'\leq L} m_{\ell'}}\cdot \left(\sum_{\ell\leq L}\sum_{j\leq m_\ell} \max\left\{a_j^{(\ell)},U-b_j^{(\ell)}\right\}\right)$.
\end{lemma}
\begin{proof}
	For a sample $\mathbf{x}_j^{(\ell)}\in \mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}$, it is clear that $\mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}(\mathbf{x}_j^{(\ell)}) = \mathbf{x}_j^{(\ell)}$. Now, suppose that $\mathbf{x}_j^{(\ell)}\in \Delta_U\setminus \Delta_{b_j^{(\ell)}}$. Following Lemma \ref{lem:proj1}, if $\mathbf{y} = \mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}(\mathbf{a})$, we must have $\lVert \mathbf{y} - \mathbf{x}_j^{(\ell)}\rVert_1 = \sum_{i\leq d} ((x_j^{(\ell)})_i - y_i) = \lVert \mathbf{x}_j^{(\ell)}\rVert_1 - b_j^{(\ell)}.$ Hence, the worst-case clipping error for sample $\mathbf{x}_j^{(\ell)}$ is $\max\ \lVert \mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}(\mathbf{x}_j^{(\ell)}) - \mathbf{x}_j^{(\ell)}\rVert_1 = U-b_j^{(\ell)}$. By symmetric arguments, one can show that if $\mathbf{x}_j^{(\ell)}\in \Delta_{a_j^{(\ell)}}\setminus \delta_{a_j^{(\ell)}}$, we must have $\max\ \lVert \mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}(\mathbf{x}_j^{(\ell)}) - \mathbf{x}_j^{(\ell)}\rVert_1 = a_j^{(\ell)}$. Thus, overall, we obtain that 
	\begin{equation}\max_{\mathbf{x}_j^{(\ell)}\in \Delta_U} \lVert \mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}(\mathbf{x}_j^{(\ell)}) - \mathbf{x}_j^{(\ell)}\rVert_1 = \max\left\{a_j^{(\ell)},U-b_j^{(\ell)}\right\}. \label{eq:maxclip}\end{equation} 
	Now, for any dataset $\mathcal{D}$, recall that
	\begin{align}
		&\lVert f(\mathcal{D})-\overline{f}(\mathcal{D})\rVert_1 \notag\\
		%&=\frac{1}{\sum_{\ell'\leq L} m_{\ell'}}\left\lVert \sum_{\ell\leq L}\sum_{j\leq m_\ell} \left(\mathbf{x}_j^{(\ell)} - \overline{\mathbf{x}}_j^{(\ell)}\right)\right \rVert_1 \notag\\
		&=\frac{1}{\sum_{\ell'\leq L} m_{\ell'}}\left\lVert \sum_{\ell\leq L}\sum_{j\leq m_\ell} \left(\mathbf{x}_j^{(\ell)} - \mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}(\mathbf{x}_j^{(\ell)}) \right)\right \rVert_1. \label{eq:cliphelp}
	\end{align}
	Putting together \eqref{eq:maxclip} and \eqref{eq:cliphelp} concludes the proof.
%	It is then easy to see that the bias
%	\begin{align}
%		&\max_{\mathcal{D}\in \mathsf{D}} |f(\mathcal{D})-\overline{f}(\mathcal{D})| \notag\\&= \frac{1}{\sum_{\ell'\leq L} m_{\ell'}}\cdot \left(\sum_{\ell\leq L}\sum_{j\leq m_\ell} \max\left\{a_j^{(\ell)},U-b_j^{(\ell)}\right\}\right). \label{eq:bias}
%	\end{align}
\end{proof}
The calculation of $\eta(\overline{f})$ is quite similar to the proof above, and is captured in Lemma \ref{lem:proj3} below.

\begin{lemma}
	\label{lem:proj3}
	We have that $\eta(\overline{f}) = \frac{d\cdot \max_{\ell\leq L} \sum_{j\leq m_\ell} \left(b_j^{(\ell)}-a_j^{(\ell)}\right)}{\varepsilon\cdot \sum_{\ell'\leq L} m_{\ell'}}.$
\end{lemma}
\begin{proof}
Via arguments entirely analogous to that in the proof of Lemma \ref{lem:proj2}, the user-level sensitivity of $\overline{f}$ is
$
	\Delta_{\overline{f}} = \frac{\max_{\ell\leq L} \sum_{j\leq m_\ell} \left(b_j^{(\ell)}-a_j^{(\ell)}\right)}{\sum_{\ell'\leq L} m_{\ell'}},
$
since in the worst-case, all samples of a user $\ell$ are changed each from $(a_j^{(\ell)},0,\ldots,0)$ to $(b_j^{(\ell)},0,\ldots,0)$. Using $\E[\lVert \mathbf{Z}\rVert_1] = d\E[|Z_1|] = d\Delta_{\overline{f}} /\varepsilon$ gives us the required result.
\end{proof}
The proof of Proposition \ref{lem:worst-case} then follows directly by putting together Lemmas \ref{lem:proj2} and \ref{lem:proj3}.
%\begin{remark}
%	Via the proof of Lemma \ref{lem:worst-case}, we observe that for a given distribution $\{m_\ell:\ell\in [L]\}$ of user contributions, the dataset $\mathcal{D}$ that gives rise to the largest (or worst-case) error is that where $x_j^{(\ell)} = U$, for all $\ell\in [L]$, $j\in [m_\ell]$. We shall use this observation in the next section when we numerically evaluate the performance of the clipping strategy in \cite{amin} on this ``worst-case dataset".
%\end{remark}


Given the characterization of the worst-case error $E_{\overline{f}}$ as above, we proceed with identifying an estimator $f^\star \in \mathsf{B}$ (equivalently, a bounding strategy) that minimizes $E_{\overline{f}}$, over all $\overline{f}\in \mathsf{B}$. Let $T_\varepsilon$ denote the $\left \lceil \left(\frac{2d}{\varepsilon}\right)\right \rceil^{\text{th}}$-largest value in the collection $\{Um_1,Um_2,\ldots,Um_L\}$; if $\varepsilon<2d/L$, we set $T_\varepsilon = 0$. Our main result is encapsulated in the following theorem:
\begin{theorem}
	\label{thm:opt}
	We have that $f^\star = \overline{f}_{\left\{a_j^{(\ell)},b_j^{(\ell)}\right\}}$ minimizes $E_{\overline{f}}$, where
	$
	a_j^{(\ell)} = \max\left\{\frac{Um_\ell-T_\varepsilon}{2m_\ell},0\right\}\text{ and } b_j^{(\ell)} = \min\left\{\frac{Um_\ell+T_\varepsilon}{2m_\ell},U\right\},
	$
	for all $\ell\in [L]$ and $j\in [m_\ell]$.
\end{theorem}
Some remarks are in order. First, note that the optimal bounding strategy $f^\star$ clips sample values based only on the number of contributions of each user $\ell\in [L]$. Furthermore, note that the interval of projection is determined by $T_\varepsilon$, which is quite similar in structure to the optimal clipping threshold $T$ in \cite{amin} for item-level DP, which is the (privately estimated) $\left \lceil \left(\frac{2}{\varepsilon}\right)\right \rceil^{\text{th}}$-largest \emph{sample value}.

The proof of Theorem \ref{thm:opt} proceeds with help from the following lemma.
\begin{lemma}
	\label{lem:helper}
	There exists an estimator $\overline{f}_{\left\{a_j^{(\ell)},b_j^{(\ell)}\right\}}$ minimizing $E_{\overline{f}}$, which obeys $a_j^{(\ell)}+b_j^{(\ell)} = U$, for all $\ell\in [L]$ and $j\in [m_\ell]$.
\end{lemma}
\begin{proof}
	Consider any optimal estimator $\overline{f}_{\left\{a_j^{(\ell)},b_j^{(\ell)}\right\}}$, and suppose that $a_j^{(\tilde{\ell})}+b_j^{(\tilde{\ell})}>U$, for some $\tilde{\ell}\in [L]$ and $j\in [m_{\tilde{\ell}}]$. The proof when we suppose that $a_j^{(\tilde{\ell})}+b_j^{(\tilde{\ell})}<U$, for some $\tilde{\ell}\in [L]$ and $j\in [m_{\tilde{\ell}}]$, is similar, and is omitted. Let $\delta =  (a_j^{(\tilde{\ell})}+b_j^{(\tilde{\ell})})-U$. By setting $b_j^{(\tilde{\ell})}\gets b_j^{(\tilde{\ell})}-\delta$, we observe that $\beta(\overline{f})$ remains unchanged, while $\eta(\overline{f})$ either remains unchanged or strictly decreases by $\delta>0$.
\end{proof}
Hence, to obtain the explicit structure of an estimator $\overline{f}$ that minimizes $E_{\overline{f}}$, it suffices to focus estimators $\overline{f}_{\left\{a_j^{(\ell)},b_j^{(\ell)}\right\}}$ with $a_j^{(\ell)}+b_j^{(\ell)} = U$, for all $\ell\in [L]$, $j\in [m_\ell]$. For $\ell\in [L]$, let $S^{(\ell)}:= \sum_{j\leq m_\ell} a_j^{(\ell)}$, for any given estimator $\overline{f}$. Then, there exists an estimator $f^\star$ minimizing $E_{\overline{f}}$ that satisfies
\begin{equation}
	\label{eq:optsimp}
	E_{f^\star} = \frac{1}{\sum_{\ell'\leq L} m_{\ell'}}\cdot \left(\sum_{\ell\leq L} S^{(\ell)}+\frac{d}{\varepsilon}\cdot \max_{\ell\leq L} \left(Um_\ell - 2S^{(\ell)}\right)\right).
\end{equation}
We now prove Theorem \ref{thm:opt}.

\begin{proof}[Proof of Thm. \ref{thm:opt}]
	We begin with the expression in \eqref{eq:optsimp}; note that our task now is simply to identify the optimal parameters $\{S^{(\ell)}:\ \ell\in [L]\}$ of $f^\star$ in \eqref{eq:optsimp}. Once these parameters are derived, we simply set $a_j^{(\ell)}:= S^{(\ell)}/m_\ell = U-b_j^{(\ell)}$ (via Lemma \ref{lem:helper}), for each $\ell\in [L]$ and $j\in [m_\ell]$. In the expression in \eqref{eq:optsimp}, let us set $\tau:= \max_{\ell\leq L} \left(Um_\ell - 2S^{(\ell)}\right)$. We hence need to solve the following constrained optimization problem:
	\begin{align}
		&{\text{minimize}}\quad h(\{S^{(\ell)}\}):= \left(\sum_{\ell\leq L} S^{(\ell)}+\frac{d\tau}{\varepsilon}\right)\notag\\
		&\text{subj. to:}\ \  Um_\ell-2S^{(\ell)}\leq \tau,\ S^{(\ell)}\geq 0,\ \forall\ \ell\in [L],\ \tau\geq 0.\label{eq:opt}
	%	&\ \ \ \  \ \ \ \  \ \ \ \ \ \ S\geq 0. \label{eq:opt}
	\end{align}
The optimization problem in \eqref{eq:opt} is a linear programming problem. By standard arguments via the necessity of the KKT conditions \cite[Sec. 5.5.3]{boyd}, there must exist reals $\lambda_\tau\geq 0$ and $\lambda_\ell,\ \mu_\ell\geq 0$, for each $\ell\in [L]$ (or Lagrange multipliers), such that the function $\mathcal{L}(\{S^{(\ell)}\},\ \lambda_\tau,\{\lambda_\ell,\mu_\ell\}):= \sum_{\ell\leq L} S^{(\ell)}+\frac{d\tau}{\varepsilon}-\lambda_\tau \tau\ + \\
\sum_\ell \lambda_\ell\cdot \left(Um_\ell-2S^{(\ell)}-\tau\right)-\sum_\ell \mu_\ell S^{(\ell)}$
%\begin{align*}
%\mathcal{L}(\{S^{(\ell)}\},\ &\lambda_S,\{\lambda_\ell,\mu_\ell\}):= \sum_{\ell\leq L} S^{(\ell)}+\frac{S}{\varepsilon}-\lambda_S S\ + \\
%&\sum_\ell \lambda_\ell\cdot \left(Um_\ell-2S^{(\ell)}-S\right)-\sum_\ell \mu_\ell S^{(\ell)}
%\end{align*}
obeys the following properties.
\begin{itemize}
	\item \textbf{Stationarity}: We have that $\frac{\partial \mathcal{L}}{\partial \tau} = 0$, or $\lambda_\tau+\sum_\ell \lambda_\ell = \frac{d}{\varepsilon},$
%	\begin{equation}
%		\label{eq:stat1}
%		\lambda_S+\sum_\ell \lambda_\ell = \frac{1}{\varepsilon},
%	\end{equation}
and that $\frac{\partial \mathcal{L}}{\partial S^{(\ell)}} = 0$, for each $\ell\in [L]$, or $\lambda_\ell = \frac{1-\mu_\ell}{2}.$
%\begin{equation}
%	\label{eq:stat2}
%	\lambda_\ell = \frac{1-\mu_\ell}{2}.
%\end{equation}
\item \textbf{Complementary slackness}: We have that $	\lambda_\tau \tau= 0,\ \lambda_\ell\cdot \left(Um_\ell-2S^{(\ell)}-\tau\right) = 0,\ \text{and}\ \mu_\ell S^{(\ell)} = 0,$
%\begin{align}
%	\label{eq:comp}
%	\lambda_SS = 0,\ \lambda_\ell\cdot \left(Um_\ell-2S^{(\ell)}-S\right) = 0,\ \text{and}\ \mu_\ell S^{(\ell)} = 0,
%\end{align}
for all $\ell\in [L]$.
\end{itemize}
We claim that the assignment $\tau^\star = T_\varepsilon$ and $S^{(\ell),\star} = \max\left\{\frac{Um_\ell-T_\varepsilon}{2},0\right\}$ satisfies the conditions above, for an appropriate choice of $\lambda_\tau,\{\lambda_\ell,\mu_\ell\}$ values. Indeed, for $\ell\in [L]$, if $S^{(\ell),\star} = 0$, we set $\lambda_\ell = 0$ and $\mu_\ell = 1$; else, we set $\lambda_\ell = \frac12$ and $\mu_\ell = 0$. 
%An optimal choice of parameters $\{a_j^{(\ell)},b_j^{(\ell)}\}$ of an estimator $f^\star$ thus obeys $a_j^{(\ell)}:= \alpha^{(\ell),\star}/m_\ell = U-b_j^{(\ell)}$, for each $\ell\in [L]$ and $j\in [m_\ell]$.
\end{proof}
Observe that from Theorem \ref{thm:opt} and \eqref{eq:optsimp}, the optimal worst-case error is $E^\text{OPT}(\varepsilon)= \frac{1}{\sum_{\ell'\leq L} m_{\ell'}}\cdot\left(\sum_{\ell\leq L} \max\left\{{Um_\ell-T_\varepsilon},0\right\}+\frac{dT_\varepsilon}{\varepsilon}\right).$ %
%\begin{align}
%&E^\text{OPT}(\varepsilon) \notag\\ &= \frac{1}{\sum_{\ell'\leq L} m_{\ell'}}\cdot\left(\sum_{\ell\leq L} \max\left\{\frac{Um_\ell-T_\varepsilon}{2m_\ell},0\right\}+\frac{1}{\varepsilon}\cdot \min_{\ell\leq L} \{T_\varepsilon,Um_\ell\}\right). \label{eq:erropt}
%\end{align}
Note that $T_\varepsilon$ is non-decreasing with $\varepsilon$. The following lemma then holds.
\begin{lemma}
\label{lem:monotonicity}
$E^\text{OPT}(\varepsilon)$ is decreasing with $\varepsilon>0$.
\end{lemma}
\begin{proof}
	Since $T_\varepsilon = Um^\star$, for all $\varepsilon\geq 2d/L$, it can be seen that $E^\text{OPT}(\varepsilon)$ is indeed non-increasing for $\varepsilon\geq 2d/L$.
	
	Now, consider $0<\varepsilon_1<\varepsilon_2<2d/L$; assume for simplicity that we have $m_1\geq m_2\geq \ldots\geq m_L$. Consider the function $g(\varepsilon,t):= \frac{1}{\sum_{\ell'\leq L} m_{\ell'}}\cdot\left(\sum_{\ell\leq L} \max\left\{{Um_\ell-t},0\right\}+\frac{dt}{\varepsilon}\right).$ Then,
	\begin{align*}
		E^\text{OPT}(\varepsilon_2)&= g\left(\varepsilon_2,Um_{\left \lceil \left(\frac{2d}{\varepsilon_2}\right)\right \rceil}\right)\\
		&\leq g\left(\varepsilon_2,Um_{\left \lceil \left(\frac{2d}{\varepsilon_1}\right)\right \rceil}\right)\\
		&< g\left(\varepsilon_1,Um_{\left \lceil \left(\frac{2d}{\varepsilon_1}\right)\right \rceil}\right) = E^\text{OPT}(\varepsilon_1),
	\end{align*}
where the first inequality holds since $t = Um_{\left \lceil \left(\frac{2d}{\varepsilon_2}\right)\right \rceil}$ minimizes $g(\varepsilon_2,t)$, over admissible values of $t$, and the second inequality holds since $\varepsilon_1<\varepsilon_2$.
\end{proof}
%Hence, we obtain that  $E^\text{OPT}(\varepsilon)$ is monotonically non-increasing as a function of $\varepsilon>0$.

A special case of Theorem \ref{thm:opt} is when $d=1$, with the interpretation that each sample $x_j^{(\ell)}\in [0,U]$. Here, the annulus $\mathsf{A}_{a_j^{(\ell)},b_j^{(\ell)}}$ is simply the interval $[a_j^{(\ell)}, b_j^{(\ell)}]\subseteq [0,U]$. This implies that for the case when the samples $\mathbf{x}_j^{(\ell)}$ are allowed to take values in the cube $[0,U]^d$, as against in the $\ell_1$-ball $\lVert \mathbf{x}_j^{(\ell)}\rVert_1\leq U$, one can perform the bounding procedure, using the \emph{same} interval $[a_j^{(\ell)}, b_j^{(\ell)}]$ identified via Theorem \ref{thm:opt} for the $d=1$ setting, for each dimension, independently.
	
%
%	\begin{figure}
%		\centering
%		\begin{tikzpicture} [scale=0.8]% Adjust scale as needed
%			
%			\newcommand{\outerradius}{3}
%			\newcommand{\middleradius}{2}
%			\newcommand{\innerradius}{1}
%			
%			% Draw the axes with new labels
%			\draw[->] (-\outerradius-1,0) -- (\outerradius+1,0) node[right] {$(x_j^{(\ell)})_1$};
%			\draw[->] (0,-\outerradius-1) -- (0,\outerradius+1) node[above] {$(x_j^{(\ell)})_2$};
%			
%			% Draw the balls with transparency
%			\draw[thick, fill=gray!20, fill opacity=0.5] (0,\outerradius) -- (\outerradius,0) -- (0,-\outerradius) -- (-\outerradius,0) -- cycle;
%			
%			% Draw the blue region *first*
%			\fill[blue!30, fill opacity=0.5] (0,\middleradius) -- (\middleradius,0) -- (0,-\middleradius) -- (-\middleradius,0) -- cycle;
%	%		\fill[white] (0,\innerradius) -- (\innerradius,0) -- (0,-\innerradius) -- (-\innerradius,0) -- cycle; % "Cut out" the inner part
%			
%			% Draw the inner ball - make it transparent gray
%			\draw[thick, fill=gray!20, fill opacity=0.5] (0,\innerradius) -- (\innerradius,0) -- (0,-\innerradius) -- (-\innerradius,0) -- cycle; % Inner ball
%			
%			% Draw the middle ball outline in BLACK
%			\draw[thick,black] (0,\middleradius) -- (\middleradius,0) -- (0,-\middleradius) -- (-\middleradius,0) -- cycle;
%			
%			\footnotesize
%			% Add arrows and labels (red, bold tips)
%			\draw[<->, very thick, red, line cap=round] (0,0) -- (\innerradius,0) node [below,font=\footnotesize] {$a_j^{(\ell)}$};
%			\draw[<->, very thick, red, line cap=round] (0,0) -- (\middleradius,0) node [above,font=\footnotesize] {$b_j^{(\ell)}$};
%			
%			\draw[<->,very thick, red, line cap=triangle] (0,0) -- (\outerradius,0) node [above,font=\footnotesize] {$U$};
%			
%			
%		\end{tikzpicture}
%		\caption{The annulus is shown in blue.}
%	\end{figure}

