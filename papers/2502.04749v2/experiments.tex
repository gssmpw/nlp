\label{sec:experiments}
In this section, we compare, via numerical experiments, the performance of our clipping strategy for the case when $d=1$, with the widely used strategy in \cite[Sec. 3]{amin}, which we call the ``AKMV" mechanism\footnote{The subscript ``AKMV'' stands for the initials of the last names of the authors of \cite{amin}.}. We straightforwardly adapt the mechanism in \cite{amin} from the item-level setting to the user-level setting, by considering the contribution of a user to be the sum $\sum_{j\leq m_\ell} x_j^{(\ell)}$ of the samples it contributes in the user-level setting. 

%In the first set of experiments, we plot the worst-case error of our clipping strategy against the total estimation error $E_{\text{AKMV}}$, computed via \eqref{eq:eg}, incurred by adapting the clipping strategy proposed in \cite[Sec. 3]{amin} to the setting where users contribute more than one sample to the dataset. 

%\subsection{The Clipping Strategy in \cite{amin}}
%\label{sec:amin}
%We now recall the clipping strategy in \cite[Sec. 3]{amin}. Consider the ``item-level" setting where the dataset $\mathcal{D}'$ is such that each user $\ell\in [L]$ contributes a single sample. Let $\sigma^{(\ell)}$ denote the single sample contributed by user $\ell$. In such a setting, given the fixed collection of positive reals $\{m_\ell: \ell\in [L]\}$ as in Section \ref{sec:problem}, we define the function
%\[
%g = g(\mathcal{D}') = \frac{1}{\sum_{\ell'} m_{\ell'}}\cdot \sum_{\ell=1}^L \sigma^{(\ell)}.
%\]
%Now, in the ``user-level" setting where we are given a dataset $\mathcal{D}$ with numbers of user contributions $\{m_\ell\}$, the function $f$ of interest to us can be viewed as an instance of function $g = g(\mathcal{D}')$ as above, for a dataset $\mathcal{D}'$ where each user $\ell\in [L]$ contributes a single sample. Indeed, by letting $\sigma^{(\ell)}:= \sum_{j=1}^{m_\ell} x_j^{(\ell)}$, we see that the dataset $\mathcal{D}'$ consisting of samples $\{\sigma^{(\ell)}:\ \ell\in [L]\}$ obeys
%$
%f(\mathcal{D}) = g(\mathcal{D}').
%$
%
%Now, in the item-level setting, consider the class $\widehat{\mathsf{B}}$ of estimators $\widehat{g} = \widehat{g}_{T}$ of $g$, paramterized by positive reals $T$. In particular, $\widehat{g}_{T}$ is obtained by letting $\widehat{\sigma}^{(\ell)}:= \Pi_{\left[0,T\right]}\left(\sigma{(\ell)}\right)$, with
%\begin{equation}
%	\label{eq:ghat}
%\widehat{g} = \widehat{g}(\mathcal{D}'):= \frac{1}{\sum_{\ell'=1}^L m_{\ell'}}\cdot \sum_{\ell=1}^L  \widehat{\sigma}^{(\ell)}.
%\end{equation}
%Much like in this work, the authors of \cite{amin} consider the sum of the error due to bias and that due to noise addition for privacy (see \eqref{eq:eg}) as the measure of the total error of a mechanism $\widehat{g}\in \widehat{\mathsf{B}}$. The authors in \cite{amin} then construct an estimator $\widehat{g}$ obtained as follows:
%\begin{enumerate}
%	\item First, parameters $\varepsilon', \varepsilon''$ such that $\varepsilon'+\varepsilon'' = \varepsilon$ are initialized.
%	\item Next, with privacy budget $\varepsilon'$, the $\left \lceil \left(\frac{1}{\varepsilon''}\right)\right \rceil^{\text{th}}$-largest sample among $\{\sigma^{(\ell)}\}$ is estimated and $T$ is set to be this estimate.
%	\item Finally, $\widehat{g}_T$ is released privately by adding noise from the Lap$\left(\frac{T}{\varepsilon''\sum_\ell m_\ell}\right)$ distribution.
%\end{enumerate}
%

Importantly, the AKMV mechanism uses a privacy budget of $\varepsilon/2$ to first \emph{privately} estimate the $\left \lceil \left(\frac{2}{\varepsilon}\right)\right \rceil^{\text{th}}$-largest sample, and then uses this value as the clipping threshold. Via entirely heuristic analysis, the authors of \cite{amin} argue the ``optimality" of AKMV mechanism. However, importantly, their analysis ignores the error in the private estimation of the the $\left \lceil \left(\frac{2}{\varepsilon}\right)\right \rceil^{\text{th}}$-largest sample, rendering such a claim of optimality incorrect. As we shall see, the clipping strategy in \cite[Sec. 3]{amin} performs quite poorly in comparison to our worst-case-error-optimal strategy, in the average case too, when the data samples are drawn i.i.d. from natural distributions.

\subsection{Experimental Setup}
\label{sec:exp-setup}
A natural application of user-level DP mechanisms is to spatio-temporal datasets; similar to previous work \cite{dp_spcom,dp_preprint}, we let $U = 65$, in line with the largest speed of buses in km/hr, in Indian cities. We consider two collections of numbers of user contributions $\{m_\ell:\ \ell\in [L]\}$. 
%We call our first collection loosely as the ``geometric collection", owing to the nature of the distribution of the $m_\ell$ values, and our second collection as the ``extreme-valued collection".
\begin{enumerate}
	\item Geometric collection: Here, we fix an integer $M$ and consider $L = 2^{M+1}-1$ users; for each $i\in \{0,1,\ldots,M\}$, we create $2^i$ users each contributing $2^{M-i}$ samples. 
	%It is clear that there are $2^{M+1}-1$ users overall; further, the total number of samples is $\sum_\ell m_\ell = (M+1)\cdot 2^M$ and $m^\star = 2^M$. 
	In our experiments, we set $M = 6$.
	\item Extreme-valued collection: Here, we consider $L$ users where $L-1$ users contribute one sample each and $1$ user contributes $m^\star>1$ samples. %The total number of samples is $\sum_\ell m_\ell = L-1+m^\star$. 
	In our experiments, we pick $L = 101$, with $m^\star = 10$.
\end{enumerate}
%For a fixed collection $\{m_\ell:\ \ell \in [L]\}$, we recall from the remark following  the proof of Lemma \ref{lem:worst-case}) that the worst-case dataset, i.e., the dataset $\mathcal{D}$ that attains the worst-case error in Lemma \ref{lem:worst-case} is the one where all samples are equal to $U$. 
We also work with the following synthetically generated datasets with i.i.d. samples:
\begin{enumerate}
	\item Uniform samples: Each sample $x_j^{(\ell)}\stackrel{\text{i.i.d}}{\sim} \text{Unif}((0,U])$, across $\ell\in [L]$, $j\in [m_\ell]$. 
	\item Projected Gaussian samples: Each sample $x_j^{(\ell)}$, $\ell\in [L]$, $j\in [m_\ell]$, is drawn i.i.d. by rejection sampling from the $\mathcal{N}(U/2,U/4)$ distribution so that the samples lie in $(0,U]$. 
\end{enumerate}
%\subsection{Performance on the Worst-Case Dataset}
%We first evaluate our clipping strategy and the strategy in \cite[Sec. 3]{amin} on the worst-case dataset. Figure \ref{fig:worst-case} shows the errors incurred by the two strategies on the worst-case dataset. Note that here, we construct the estimator of \cite{amin} by setting $\varepsilon' = \varepsilon'' = \varepsilon/2$ (see Step 1 in Section \ref{sec:amin}) and then executing Step 2 in Section \ref{sec:amin} using (a suitable modification of) the private quantile estimation mechanism in \cite[Algorithm 2]{smith}, to obtain the clipping threshold $T$, as suggested in \cite{amin}. Using the relationship between the user-level setting of interest and a suitably defined item-level setting in Section \ref{sec:amin}, the total estimation error is then calculated via \eqref{eq:eg} as
%\[
%E_{\text{AKMV}} = \frac{1}{\sum_{\ell'} m_{\ell'}}\cdot \sum_{\ell\in [L]} \E\left[\max\{0,Um_\ell-T\}\right]+\frac{2T}{\varepsilon\cdot \sum_\ell m_\ell}.
%\]
%Here, the expectation is over the randomness in the private estimation of $T$. We compute an estimate of $E_{\text{AKMV}}$ above by Monte-Carlo averaging over $10^4$ iterations.
%\begin{figure}[!t]
%	\centering
%	\includegraphics[width = \linewidth]{worst-case-errors.png}
%	\caption{Errors incurred on the worst-case dataset by our clipping strategy and that in \cite[Sec. 3]{amin}}
%	\label{fig:worst-case}
%\end{figure}
%The error of our clipping strategy is computed by plugging in the values of $\{a_j^{(\ell)}\}, \{b_j^{(\ell)}\}$ from Theorem \ref{thm:opt} into the expression in Lemma \ref{lem:worst-case}. From Figure \ref{fig:worst-case}, it is clear that the error incurred by our clipping strategy is much lower that incurred by the strategy in \cite{amin}, for a large range of $\varepsilon$ values. Moreover, the optimal worst-case error is indeed monotonically decreasing with $\varepsilon$, as discussed at the end of Section \ref{sec:worst-case}, as is the (estimate of the) error of the AKMV mechanism on the worst-case dataset.
\subsection{Performance on Synthetic Datasets}
%Given the estimators $f, f^\star, \widehat{g}_T$ as in \eqref{eq:f}, \eqref{eq:fclip}, \eqref{eq:ghat}, respectively, we first put down their sensitivities. Indeed, via straightforward computations, we have that
%\begin{equation}
%	\label{eq:sens1}
%	\Delta_f = \frac{Um^\star}{\sum_\ell m_\ell},\ \Delta_{f^\star} = \frac{\max_\ell \sum_{j\leq m_\ell}(b_j^{(\ell)}-a_j^{(\ell)})}{\sum_\ell m_\ell},\ 
%\end{equation}
%{and}
%\begin{equation}
%	\label{eq:sens2}
%	\Delta_{\widehat{g}_T} = \frac{T}{\sum_\ell m_\ell}.
%\end{equation}
%In \eqref{eq:sens1}, the values $\{a_j^{(\ell)}\}, \{b_j^{(\ell)}\}$ are from Theorem \ref{thm:opt} and in \eqref{eq:sens2}, the threshold $T$ is computed (privately) via Step 2 in Section \ref{sec:amin}.
 
We compare the performances of the following three mechanisms on i.i.d. synthetic datasets: (i) the vanilla Laplace mechanism that releases $M_{\text{Lap}}(\mathcal{D}) = f(\mathcal{D})+\mathbf{Z}_1$, where $\mathbf{Z}_1\sim \text{Lap}\left(\frac{\Delta_f}{\varepsilon}\right)$, (ii) the ``OPT-worst-case" mechanism that releases $M_{\text{OPT-wc}}(\mathcal{D}) = f^\star(\mathcal{D})+\mathbf{Z}_2$, where $\mathbf{Z}_2\sim \text{Lap}\left(\frac{\Delta_{f^\star}}{\varepsilon}\right)$, and (iii) the ``AKMV" mechanism \cite[Sec. 3]{amin} that releases $M_{\text{AKMV}}(\mathcal{D}) = \widehat{f}_T(\mathcal{D})+\mathbf{Z}_3$, where $\mathbf{Z}_3\sim \text{Lap}\left(\frac{2\Delta_{\widehat{g}_T}}{\varepsilon}\right)$, and $\widehat{f}_T(\mathcal{D})$ is obtained by clipping the sum of samples of each user to lie in $[0,T]$, where $T$ is the estimate of the $\left \lceil \left(\frac{2}{\varepsilon}\right)\right \rceil^{\text{th}}$-largest sample among $\{\sigma^{(\ell)}\}$, which is estimated with privacy budget $\varepsilon/2$.
%\begin{itemize}
%	\item 
%	\item 
%	\item 
%\end{itemize}

The`` average-case" errors on i.i.d. datasets are defined as
$\overline{E}_\text{Lap} = \E[|\mathbf{Z}_1|],\ \overline{E}_\text{OPT-wc} = \E[|M_{\text{OPT-wc}}(\mathcal{D}) - f(\mathcal{D})|],$
and
$ \overline{E}_\text{AKMV} = \E[|M_{\text{AKMV}}(\mathcal{D}) - f(\mathcal{D})|],$
where the expectations are over the randomness in the data samples and in the DP mechanism employed. These errors 
are then estimated via Monte-Carlo averaging over $10^4$ iterations. As a pre-processing step, we replace each of the samples $\{x_j^{(\ell)}:\ j\in [m_\ell]\}$ of every user $\ell \in [L]$ by the sample average $\frac{1}{m_\ell}\cdot \sum_{j\leq m_\ell} x_j^{(\ell)}$, so as to allow for improved performance post clipping. 

Figure \ref{fig:avg-geo} and \ref{fig:avg-extreme-normal} (with the error axis on a log-scale), respectively, show plots of the (estimates of) the average-case errors for the three mechanisms above for the geometric collection of $\{m_\ell\}$ values and uniform samples, and for the extreme-valued collection with projected Gaussian samples. Interestingly, the average-case performance of the AKMV mechanism is quite similar to, and a little worse than, the average-case performance of the vanilla Laplace mechanism, which in turn is significantly worse than the average-case error incurred by the OPT-worst-case mechanism. A reason for the poor performance of the AKMV mechanism overall is due to the reduced privacy budget allocated to the private release of the clipped estimator. Moreover, in contrast to the OPT-worst-case mechanism, the AKMV mechanism requires the private estimation of the $\left \lceil \left(\frac{2}{\varepsilon}\right)\right \rceil^{\text{th}}$ quantile value, which incurs additional error. We expect that similar trends can be observed for i.i.d. datasets with samples drawn from other distributions of $\{m_\ell\}$ values and samples. We add that while the clipping error remains roughly the same for small changes in $\varepsilon$, there exist certain values of $\varepsilon$ that give rise to sharp discontinuities in the $T_\varepsilon$ values; such behaviour hence results in the (estimates of the) average-case errors not being monotonic in $\varepsilon$, in Figure \ref{fig:avg-geo}.

% Figure \ref{fig:avg-extreme-normal} shows plots of the (estimates of) the average-case errors for the extreme-valued collection of $\{m_\ell\}$ values with projected Gaussian samples. In our experiments, we pick $L = 101$, with $m^\star = 10$, and the errors are shown on a log-scale for easy viewing. These plots show a similar trend in the performances of the three mechanisms as that in Figure \ref{fig:avg-geo}. 
\begin{figure}[!t]
	\centering
	\includegraphics[width = \linewidth]{geo-worst-av-post-av.png}
	\caption{Average-case errors using a geometric collection of $\{m_\ell\}$ values and uniform samples}
	\label{fig:avg-geo}
\end{figure}
%
%\begin{figure}[!t]
%	\centering
%	\includegraphics[width = \linewidth]{avg-extreme-uniform.png}
%	\caption{Average-case errors incurred on a synthetic dataset with an extreme-valued collection of $\{m_\ell\}$ values and uniform samples; here, the errors are shown on a log-scale}
%	\label{fig:avg-extreme-uniform}
%\end{figure}

\begin{figure}[!t]
	\centering
	\includegraphics[width = \linewidth]{extreme-normal-post-av.png}
	\caption{Average-case errors using an extreme-valued collection of $\{m_\ell\}$ values and projected Gaussian samples}
	\label{fig:avg-extreme-normal}
\end{figure}