\textbf{Limitation: }Our work has the following limitations.
First, Proposition~\ref{thm:compLimit} only demonstrates a reduction in expressivity when the language \( \lang{\regG} \) is finite. 
This leaves open the question of whether Proposition~\ref{thm:compLimit} can be extended to grammars \( G \) where \( L(G) \) is infinite. 
Second, \Tool\ for constrained decoding relies on existing tools \cite{syncode} that require access to output logits, rendering \Tool\ inapplicable to models that do not expose logits.
\vspace{-5pt}
\section{Related Works}
\textbf{Constrained LLM Decoding:}
% Grammar-guided generation + constrained semantic decoding
Recent works have introduced techniques to enforce LLM generations to adhere to a context-free grammar using constrained decoding~\cite{ugare2024syncodellmgenerationgrammar, willard2023efficient, beurerkellner2024guiding, melcer2024constraineddecodingfillinthemiddlecode}.
Additionally, \citet{poesia2022synchromesh, ugare2024itergeniterativestructuredllm} have extended grammar-guided generation to incorporate task-specific semantic constraints. 
These approaches demonstrate that constrained decoding can improve the syntactic and semantic quality of LLM outputs for various structured generation tasks.

More recently, \citet{speakFree} demonstrated that constrained structured generation can negatively impact the quality of generated outputs. Similarly, \citet{park2024grammaraligneddecoding} showed that greedily masking out tokens that do not lead to a valid string during next-token prediction can distort the output distribution, causing it to deviate from the true distribution of all grammatically valid outputs of $\llm$ for a given input.
To mitigate the distortion introduced by the greedy masking approach, these ``grammar aligned" methods~\cite{park2024grammaraligneddecoding, melcer2024approximatelyaligneddecoding} use a trie to track previous generations, reducing generation divergence iteratively. 
However, they are computationally expensive and require a large no. of resamplings per prompt to converge.
% More recently, \citet{speakFree} and \citet{park2024grammaraligneddecoding} have shown that constrained structured generation can adversely affect the quality of generated outputs. 
% This occurs since constrained decoding fails to normalize next-token probabilities based on the future grammaticality of the sequence, thereby skewing the LLM's original probability distribution. 
% To address this, \citet{park2024grammaraligneddecoding} and \citet{melcer2024approximatelyaligneddecoding} proposed Grammar-Aligned Decoding methods that aim to approximate the true constrained distribution through multiple resamplings. 

In contrast, our work focuses on the fundamental question of the theoretical expressivity of any constant layered constrained LLM, even under an ideal constrained decoding algorithm, and uses the insights to propose a practical solution.
We propose an adaptive constrained decoding approach that can support various constrained decoding methods, including grammar-aligned techniques while preserving the LLM's expressivity by reasoning chains.

\textbf{LLM Expressivity:} \cite{survey} provides a detailed survey of existing results from the perspective of formal language theory and complexity classes. A series of existing works \cite{circuit1, cicuit2, circuit3, tc0} establish that, under suitable assumptions, a single autoregressive step on an input of any length for a constant-depth LLM can be represented as a constant-depth Boolean circuit. \cite{expressivity1, expressivity2} show that the expressivity of LLMs significantly improves under popular reasoning approaches like Chain of Thought (CoT) \cite{cotGoogle}, where LLMs take intermediate steps before generating the final answer. To the best of our knowledge, there is no prior work on LLM expressivity under grammar constraints.

