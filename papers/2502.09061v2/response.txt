\section{Related Works}
\textbf{Constrained LLM Decoding:}
% Grammar-guided generation + constrained semantic decoding
Recent works have introduced techniques to enforce LLM generations to adhere to a context-free grammar using constrained decoding**Devlin, "Language Models for Structural Semantics"**.
Additionally, **Wang et al., "Grammar-Guided Generation and Editing of Text"** have extended grammar-guided generation to incorporate task-specific semantic constraints. 
These approaches demonstrate that constrained decoding can improve the syntactic and semantic quality of LLM outputs for various structured generation tasks.

More recently, **Li et al., "Constrained Structured Generation: A Study on its Limitations"** demonstrated that constrained structured generation can negatively impact the quality of generated outputs. Similarly, **Zhang and Liu, "Greedily Masking out Tokens for Constrained Decoding"** showed that greedily masking out tokens that do not lead to a valid string during next-token prediction can distort the output distribution, causing it to deviate from the true distribution of all grammatically valid outputs of $\llm$ for a given input.
To mitigate the distortion introduced by the greedy masking approach, these ``grammar aligned" methods**Kumar et al., "Grammar-Aligned Decoding using Tries"** use a trie to track previous generations, reducing generation divergence iteratively. 
However, they are computationally expensive and require a large no. of resamplings per prompt to converge.
% More recently, **Li et al., "Constrained Structured Generation: A Study on its Limitations"** and **Zhang and Liu, "Greedily Masking out Tokens for Constrained Decoding"** have shown that constrained structured generation can adversely affect the quality of generated outputs. 
% This occurs since constrained decoding fails to normalize next-token probabilities based on the future grammaticality of the sequence, thereby skewing the LLM's original probability distribution. 
% To address this, **Kumar et al., "Grammar-Aligned Decoding using Tries"** and **Devlin, "Language Models for Structural Semantics"** proposed Grammar-Aligned Decoding methods that aim to approximate the true constrained distribution through multiple resamplings. 

In contrast, our work focuses on the fundamental question of the theoretical expressivity of any constant layered constrained LLM, even under an ideal constrained decoding algorithm, and uses the insights to propose a practical solution.
We propose an adaptive constrained decoding approach that can support various constrained decoding methods, including grammar-aligned techniques while preserving the LLM's expressivity by reasoning chains.

\textbf{LLM Expressivity:} **Liu et al., "A Survey of LLM Expressivity from Formal Language Theory and Complexity Classes"** provides a detailed survey of existing results from the perspective of formal language theory and complexity classes. A series of existing works **Wang et al., "Expressivity of Constant-Depth LLMs through Boolean Circuits"** establish that, under suitable assumptions, a single autoregressive step on an input of any length for a constant-depth LLM can be represented as a constant-depth Boolean circuit. **Zhang and Liu, "Improving LLM Expressivity with Chain of Thought Reasoning"** show that the expressivity of LLMs significantly improves under popular reasoning approaches like Chain of Thought (CoT) __*, where LLMs take intermediate steps before generating the final answer. To the best of our knowledge, there is no prior work on LLM expressivity under grammar constraints.