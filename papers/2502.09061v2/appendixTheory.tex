\section{Turing Machine Computation}
\label{appen:turningDetails}
A Turing machine processes an input string $\vect{x} \in \inset$. Its configuration consists of a finite state set $Q$, an input tape $c_0$, $k$ work tapes $c_1, \dots, c_k$, and an output tape $c_{k+1}$. Additionally, each tape $\tau$ has an associated head position $h_\tau$.  

Initially, the machine starts in the initial state $q_0 \in Q$ with the input tape $c_0^0$ containing $\vect{x}$, positioned at index $0$, and surrounded by infinite blank symbols ($b$). The head on the input tape is set to $h_0^0 = 0$, while all other tapes contain only blank symbols $b$s and have their heads positioned at $0$.  

At each time step $i$, if $q_i \notin F$ ($F$ is a set of halting states), the configuration updates recursively by computing:  
\[
\langle q_{i+1}, \gamma_1^i, \dots, \gamma_{k+1}^i, d_0^i, \dots, d_{k+1}^i \rangle = \delta(q_i, c_0^i[h_0^i], \dots, c_{k+1}^i[h_{k+1}^i])
\]  
where $\delta$ is the transition function. The machine updates each tape $\tau$ by setting $c_\tau^{i+1}[h_\tau^i] = \gamma_\tau^i$, leaving all other tape cells unchanged. The head position for each tape is updated as $h_\tau^{i+1} = h_\tau^i + d_\tau^i$.  If $q_i \in F$, the Turing machine halts and outputs the sequence of tokens on the output tape, starting from the current head position and continuing up to (but not including) the first blank symbol ($b$). A Turing machine can also function as a language recognizer by setting the input alphabet $\Sigma = \{0,1\}$ and interpreting the first output token as either $0$ or $1$.

\section{Thershold Circuit Class}
\label{appen:threshInfo}
$\class$ is a class of computational problems that can be recognized by constant-depth, polynomial-size circuits composed of threshold gates. A threshold gate, such as $\theta_{\leq k}$, outputs 1 if the sum of its input bits is at most $ k $, while $\theta_{\geq k}$ outputs 1 if the sum is at least $ k $. These circuits also include standard logic gates like $\wedge$, $\vee$, and $\neg$ as special cases of threshold functions. Since $\class$ circuits can simulate $AC^{0}$ circuits ( a polysize, constant-depth $\{\wedge, \vee, \neg\}$-circuit family), they are at least as powerful as $AC^{0}$ in the computational hierarchy. The circuit families we have defined above are non-uniform, meaning that there is no requirement for the circuits processing different input sizes to be related in any way. In degenerate cases, non-uniform circuit families can solve undecidable problems making them an unrealizable model of computation \cite{complexityBook}. Intuitively, a uniform circuit family requires that the circuits for different input sizes must be "somewhat similar" to each other. This concept is formalized by stating that there exists a resource-constrained Turing machine that, given the input $ 1^n $, can generate a serialization of the corresponding circuit $ C_n $ for that input size. Specifically, a logspace uniform $\class$ family can be constructed by a logspace-bounded Turing machine from the string $1^n$.

\section{Proofs}
\label{sec:proof}
\begin{lemma}[Constant depth circuit for $\llmFormal$]
\label{lem:constrainLem}
For any log-precision constant layer transformer-based LLM $\llm{}$ with finite vocabulary $V$, a single deterministic auto-regressive step $\llmFormal(x)$ operating on any input of size $n \in \mathbb{N}$ with $\vect{x} \in V^{n}$ can be simulated by a logspace-uniform threshold circuit family of depth $C$ where $C$ is constant.
\end{lemma}
\begin{proof}
The construction is from Theorem~2 in \cite{tc0}.
\end{proof}

\compLimitLemma*
\begin{proof}
The language $ L(\regG) $ is finite; therefore, for any string $ \vect{s} \in L(\regG) $, the length satisfies $ |\vect{s}| \leq N $, where $ N $ is a constant. Consequently, for any input $ \vect{x} $, the output $ \vect{y}_G = \llmG{\vect{x}}{G} $ has a constant length, i.e., $ |\vect{y}_{G}| \leq N $. The number of autoregressive steps is also bounded by $ N $.  

From Lemma~\ref{lem:constrainLem}, each unconstrained autoregressive computation $ \llmFormal(\vect{x}) $ can be simulated by a constant-depth threshold circuit $ C $. This implies that $ \llm_f(\vect{x}, \regG) $ can also be simulated by a constant-depth threshold circuit since it only involves an additional multiplication by a constant-sized precomputed Boolean mask $ \{0, 1\}^{|V|} $ (see Section~\ref{sec:prelims}).  

Given that the number of autoregressive steps is a constant $ N $, and each step can be simulated by a constant-depth circuit $ C $, we can simulate all $ N $ steps using a depth $ N \times C $ circuit by stacking the circuits for each step sequentially. For uniformity, we are just stacking together a constant number of constant depth circuits we can do it in a log-space bounded Turning machine $M$. 

Note that this proof holds only because $ L(\regG) $ allows only constant-size strings in the output.  
\end{proof}
\expressivity*
\begin{proof}
The construction follows from Theorem 2 \cite{expressivity1}.
\end{proof}

In this construction, the deterministic Turing machine run captured by a sequence of $\hatConfig{\gamma_1}, \dots, \hatConfig{\gamma_{t(n)}}$ capturing the state entered, tokens written, and directions moved after each token before generating the output $M(\vect{x})$. Then on any input the $\vect{x}$ the output $\llm_{M}(\vect{x}) = \hatConfig{\gamma_1}, \cdots, \hatConfig{\gamma_{t(n)}}\cdot M(\vect{x})$ (assuming $M$ halts within on $\vect{x}$ within $\runtime{n}$ steps where $n = |\vect{x}|$ and $\runtime{n}$ is a polynomial over $n$).

\expressConstrain*
\begin{proof}
$\llm{}_{M}(\vect{x})) = \hatConfig{\gamma_1}\cdots\hatConfig{\gamma_{\runtime{n}}}\cdot M(\vect{x})$. We show that $\llm{}_{M}(\vect{x}) \in L(\augG)$. 
$\augG \to \rGM G$. Since, $G$ is output grammar of $M$ then $M(\vect{x}) \in \lang{G}$. For all $1 \leq i \leq \runtime{n}$ $\hatConfig{\gamma_{i}} \in \hatConfig{\Gamma}$. Then, $\hatConfig{\gamma_1}\cdots\hatConfig{\gamma_{\runtime{n}}} \in \hatConfig{\Gamma}^{*} \subseteq L(\rGM)$.

Then $\llm{}_{M}(\vect{x}) \in \lang{\augG}$ then under constrained decoding the output $\llm{}_{M}(\vect{x})$ remains unchanged and $\llm{}_{M}(\vect{x}) = \llmMG{\vect{x}}{\augG} = \vect{r} \cdot M(\vect{x})$ where $\vect{r} = \hatConfig{\gamma_1}\cdots\hatConfig{\gamma_{\runtime{n}}}$.
\end{proof}