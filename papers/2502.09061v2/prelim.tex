\vspace{-9pt}
\section{Preliminaries}
\label{sec:prelims}
\textbf{Notations:} In the rest of the paper, we use small case letters ($x$) for constants, bold small case letters ($\vect{x}$) for strings, capital letters $X$ for functions, $\cdot$ for string concatenation, $|\vect{x}|$ to dentone the length of the string $\vect{x}$. 
% \sasa{Add notation for string concatenation to be visible.} 
We use LLM to refer to transformer-based LLMs with a fixed number of layers. 
\subsection{Constrained LLM Decoding}
Autoregressive language models $\llm{}$ decode output iteratively by generating tokens from a probability distribution over the vocabulary $\inalphaLLM$. The distribution is derived by applying the softmax function to the model's scores $\mathcal{S}$. 
Common decoding methods include greedy decoding, temperature sampling, and beam search.
Constrained LLM decoding extends this process by excluding specific tokens at certain positions, such as avoiding harmful words or adhering to a user-defined output grammar for languages like JSON or SQL~\cite{poesia2022synchromesh, ugare2024syncodellmgenerationgrammar, willard2023efficient}. 
At each decoding step, a binary mask $m \in \{0, 1\}^{|\inalphaLLM|}$, generated by a function $f_m$, specifies valid tokens ($m_i = 1$) and excluded tokens ($m_i = 0$). 
Decoding is then performed on the masked probability distribution $m \odot \textit{softmax}(\mathcal{S})$, where $\odot$ denotes element-wise multiplication. 

\subsection{Deterministic LLM Decoding}
\Tool is compatible with various decoding strategies, both constrained and unconstrained, allowing the output of \(\llm{}\) to be stochastic. 
However, following existing works \cite{circuit1, tc0, expressivity2} and for simplicity in the theoretical setup in Section~\ref{sec:expressivityTheory}, we assume that the output of \(\llm\) on any input string \(\vect{x}\) is deterministic in both constrained and unconstrained settings.

Similar to prior works \cite{tc0, expressivity1}, we model a single autoregressive step as a deterministic function $\llmFormal$ that predicts the next token given a specific input. 
% string over $\inalphaLLM$, denoted as $\llmFormal : \insetLLM \to \inalphaLLM$, where $\insetLLM $represents the set of all finite strings over $\inalphaLLM$. The output string $\vect{o} $, computed over $n $ autoregressive steps for an input $\vect{x} \in \insetLLM$, is iteratively defined as $\vect{o} = \llmFormal^{(n)}(\vect{x}) $, with the base case $\llmFormal^{(0)}(\vect{x}) = \vect{x}$ and the recurrence relation $\llmFormal^{(i+1)}(\vect{x}) = \llmFormal^{(i)}(\vect{x}) \cdot \llmFormal(\llmFormal^{(i)}(\vect{x}))$ for all $i \in [n]$, where $\cdot $denotes string concatenation. 
Formally,
\begin{definition}[Deterministic LLM Step]
A single autoregressive step of an LLM is modeled as a deterministic function $\llmFormal : \insetLLM \to \inalphaLLM$, where $\inalphaLLM$ is the finite vocabulary and $\insetLLM$ represents the set of all finite strings over $\inalphaLLM$. For an input string $\vect{x} \in \insetLLM$, the LLM predicts the next token $\llmFormal(\vect{x})$.
\end{definition}
\begin{definition}[Deterministic Unconstrained Decoding]
For an input string $\vect{x}$, the deterministic output string $\vect{y}$ selected from the output distribution of a LLM using a decoding algorithm (e.g., greedy decoding) is denoted as $\vect{y} = \llm{}(\vect{x})$ where $\llm: V^{*} \to V^*$. $\llm(\vect{x})$ is the most likely output sequence according to learned distribution on $\vect{x}$.
\end{definition}

% \sasa{Definition of $|\vect{y}|$ unclear; is it a number (size) or a set, $i \in |\vect{y}|$ (or $\in$ is $\le$?}
The output $\vect{y} = \llm{}(\vect{x})$ is computed iteratively with $|\vect{y}|$ autoregressive steps defined by $\llmFormal$. For each $1 \leq i \leq |\vect{y}|$, and the recurrence relation $\llmFormal^{(i)}(\vect{x}) = \llmFormal^{(i-1)}(\vect{x}) \cdot \llmFormal(\llmFormal^{(i-1)}(\vect{x}))$ where $\llmFormal^{(0)}(\vect{x}) = \vect{x}$ and $\cdot $ denotes string concatenation. Here, $\vect{x} \cdot \vect{y} = \llmFormal^{|\vect{y}|}(\vect{x})$. Similarly, under constrained decoding with a grammar $G$ we define:
% \begin{definition}[Deterministic LLM Step under Grammar]
% Autoregressive LLM Step under Grammar at step $i$ is defined as $\llmFormalG{i}{\vect{x}}{G}$, where the generation adheres to the constraints imposed by $G$. 
% Each token generated is guaranteed to satisfy the grammar's rules, ensuring the output string remains valid within $G$ at every step.
% \end{definition}
\begin{definition}[Deterministic Constrained Decoding under Grammar]
Under constrained decoding with a formal grammar $G$, the output string $\vect{y}_{G}$ is selected from the constrained output distribution and is denoted as $\vect{y}_{G} = \llmG{\vect{x}}{G}$. The output of $i$-th constrained autoregressive step with $G$ is $\vect{x}\cdot\vect{y}^{(i)}_G = \llmFormalG{i}{\vect{x}}{G}$ and $\vect{x}\cdot\vect{y}_G = \llmFormalG{|\vect{y}|}{\vect{x}}{G}$. 
\end{definition}
The constrained output $\vect{y}_G$ is always in the grammar $\vect{y}_G \in \lang{G}$ where $\lang{G}$ is the language defined by $G$. For sound-constrained decoding algorithms, if the unconstrained output $\vect{y} = \llm(\vect{x})$ in the grammar $\vect{y} \in \lang{G}$, the constrained output remains unchanged, i.e., $\llm(\vect{x}) = \llmG{\vect{x}}{G}$.


% We denote the deterministic output string $\vect{y}$, selected from the output distribution of $\llm{}$ on an input $\vect{x}$ using a specific decoding algorithm such as greedy decoding, as $\vect{y} = \llm{}(\vect{x})$. Similarly, under constrained decoding with a formal grammar $G$, the selected output from the constrained output distribution is denoted as $\vect{y}_{G} = \llmG{\vect{x}}{G}$. Theoretically, for sound-constrained decoding algorithms, if the unconstrained output $\vect{y} = \llm(\vect{x})$ is a valid prefix of some string $\vect{s} \in \lang{G}$, where $\lang{G}$ is the language defined by $G$, then the constrained output remains unchanged, i.e., $\llm(\vect{x}) = \llmG{\vect{x}}{G}$.

\subsection{LLM Expressivity}
We discuss the notations and background related to Turing machines, and relevant uniform circuit complexity classes. 
Turing machines are popular mathematical computation models used to analyze resource requirements (e.g. time and space complexity) and the hardness of computation problems.
% Formally, a Turing machine $M$ with $k$ work tapes and an output tape is defined as $7$-tuple $M = \langle \inalpha,\tapealpha, k, b, \stateset, q_{0}, \transitionFunc, \finalState \rangle$ where $\inalpha$ is the finite input alphabet, $\inalpha \subseteq \tapealpha$ is the finite tape alphabet, $b \in \tapealpha \setminus \inalpha $ is the special blank symbol, $\stateset $ is a finite set of states with $q_0 $ as the start state, $\transitionFunc $ is the transition function, and $\finalState $is the set of halting states \cite{automataLanguage}. 
Formally, a Turing machine is defined as:
\begin{definition}[Turing Machine]
A Turing machine $M$ with $k$ work tapes and an output tape is a $8$-tuple 
\[
M = \langle \inalpha, \tapealpha, k, b, \stateset, q_{0}, \transitionFunc, \finalState \rangle,
\]
where $\inalpha$ is the finite input alphabet, $\tapealpha$ is the finite tape alphabet with $\inalpha \subseteq \tapealpha$, $b \in \tapealpha \setminus \inalpha$ is a special blank symbol, $\stateset$ is a finite set of states, $q_0 \in \stateset$ is the initial state, $\transitionFunc: \left( Q \setminus F \right) \times \Gamma^{k+2} \to Q \times \Gamma^{k+1} \times \{0, +1, -1 \}^{k+2}$ is the transition function (where $-1, 1, 0$ represent moving the tape head left, right, or staying in place, respectively), and $\finalState \subseteq \stateset$ is the set of halting states.
\end{definition}

Let $\inset $ denote the set of all finite strings over the input alphabet $\inalpha $. Given an input string $\vect{s} \in \inset $, the computation of $M $on $s $ is a sequence of configurations starting from the initial configuration. 
Each configuration $\gamma $ is a tuple containing the current state $q \in \stateset $, the contents of the input tape, the $k$ work tapes, the output tape, and the current head positions of all $k+2$ tapes.  
For each configuration, $\gamma_i$ ($i \in \mathbb{N}$), the transition function $\transitionFunc$ computes the next configuration $\gamma_{i+1}$ based on the current state $q$ and the values on the $k+2$ tapes at the current head positions. It updates the head positions, writes to the output tape (possibly leaving it unchanged if no new symbol is written), and advances to the next configuration. 
For each $i$, computation of $\gamma_{i+1}$ from $\gamma_{i}$ defines a single step of the Turing machine.

The computation of $M$ on input $\vect{s}$ halts if $M$ reaches a halting state $q \in \finalState $. If $M$ halts, the output corresponding to $\vect{s}$ is written on the output tape. Additional details about the computation of the Turing machine are in Appendix~\ref{appen:turningDetails}.


Before discussing existing expressivity results for constant-layer LLMs, we briefly introduce relevant uniform constant-depth circuit complexity classes, e.g. logspace uniform-$\class$, which provide an upper bound on the computational power of LLMs that do not employ reasoning steps, as seen in methods like Chain-of-Thought \cite{cotGoogle}. 
% Circuits serve as a computational model for evaluating boolean functions over fixed-length binary strings, represented as directed acyclic graphs. Leaf nodes correspond to input binary variables or their negations, while internal nodes perform operations (e.g., conjunction ($\wedge$), disjunction ($\vee$), etc.) from a predefined set of operation $\mathcal{B}$. 
% One or more marked nodes in the graph represent the circuit's output. 
\begin{definition}[Boolean Circuit]
A Boolean circuit is a computational model for evaluating Boolean functions over fixed-length binary strings. It is represented as a directed acyclic graph (DAG), where the leaf nodes correspond to input binary variables or their negations, and the internal nodes perform operations from a predefined set of operations $\mathcal{B}$ (e.g., AND ($\wedge$), OR ($\vee$), etc.). One or more marked nodes in the graph represent the circuit's output.
\end{definition}
The structure of the DAG specifies the computation of the Boolean function by propagating input values through the graph.
The complexity of a circuit is determined by its size (the number of nodes) and depth (the longest path in the graph). 
Since a single circuit only defines a boolean function for fixed-length inputs, a family of circuits is required—one for each input length—to characterize a computational problem where input lengths vary. 
Unlike Turing machines, whose computation does not depend on input length, circuit families have a separate circuit for each input length, which can differ entirely. 
This non-uniformity can lead to degenerate cases where non-uniform circuit families solve undecidable problems \cite{complexityBook}.
To address this issue, complexity theorists enforce uniformity conditions, requiring circuits for different input sizes to be related, resulting in uniform circuit families. For further details and formal definitions of circuit classes, refer to \cite{complexityBook}. In this work, we focus on constant-depth, polynomial-sized logspace-uniform threshold circuits ($TC^{0}$), where $\mathcal{B}$ contains only threshold gates (a formal definition is  in Appendix~\ref{appen:threshInfo}). 

 

