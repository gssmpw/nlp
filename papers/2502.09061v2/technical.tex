\vspace{-5pt}
\section{Expressivity of Constrained Decoding}
\label{sec:expressivityTheory}
First, we show that any constant-layer LLM $\llm{}$ under constrained decoding loses expressivity. 
% when restricted to producing outputs that always conform to a formal grammar, such as a context-free grammar (CFG) $G$ defining all possible outputs. 
We identify the class of problems and the corresponding output grammars $G$ such that when imposed on the outputs of any constant-layer LLM, the problems cannot be solved unless there is a collapse in fundamental complexity classes that are widely believed to be unequal (e.g., $\class \neq \nlspace$)\footnote{NL refers to nondeterministic log-space}. 
\vspace{-3pt}
\subsection{Limitation of Constrained Decoding}
Next, we present the high-level idea behind Proposition~\ref{thm:compLimit} that shows the limitation of constrained LLM decoding when the output grammar is too restrictive. 
We consider problems where the number of possible outputs is finite, and thus the set of all possible outputs $\outset$ can be expressed as a simple regular language.
Consequently, $\regG$ that encodes the output set $\outset$, i.e., $\outset = \lang{\regG}$, where $\lang{\regG}$ denotes the language defined by the grammar $\regG$. 
For instance, any decision problem (yes/no answer) such as st-connectivity that asks for vertices s and t in a directed graph, if t is reachable from s can be answered within a single-bit output i.e. $\lang{\regG} = \{0, 1\}$. 
This implies that constrained decoding with the output grammar $\regG$ allows only a single autoregressive step for any $\llm{}$ on all inputs.

A series of existing works \cite{circuit1, cicuit2, circuit3, tc0} establish that, under suitable assumptions, a single autoregressive step on an input with length $n$ for any constant-depth LLM can be represented as a constant-depth circuit. 
Since, for decision problems, the constrained decoding step permits only a single autoregressive step, any LLM can only solve problems within the corresponding circuit complexity class. 
We build on the most recent result from \cite{tc0}, which shows that a single autoregressive step of any LLM with a constant number of layers on an input of length $ n $ can be simulated by a logspace-uniform constant-depth threshold circuit family. 
This result allows the LLM to use floating-point numbers with $\log(n)$ precision when processing inputs of size $ n $, ensuring that the precision scales with $ n $ and preventing floating-point representation issues for large $n$. 
We denote such LLMs as log-precision LLMs.

Let $\vect{x} \cdot \vect{y}^{(i)}$ denote the output after the $i$-th autoregressive step of an LLM $\llm{}$ under constrained decoding with an output grammar $G$ on input $\vect{x}$. Then, we have $\vect{x} \cdot \vect{y}^{(i)} = \llmFormalG{i}{\vect{x}}{G}$, and for any $i$, $\vect{y}^{(i)}$ is always a valid prefix of a string in $\lang{G}$, i.e., there exists a (possibly empty) string $\vect{\alpha}^{(i)}$ such that $\vect{y}^{(i)} \cdot \vect{\alpha}^{(i)} \in \lang{G}$. Now, for any output grammar $\regG$ where the output set $O = L(\regG)$ is finite, we show that the output $\llmG{\vect{x}}{\regG}$ for any input $\vect{x}$ of size $|\vect{x}| = n$ can be computed using constant-depth threshold circuits.
\begin{restatable}{proposition}{compLimitLemma}
\label{thm:compLimit}
For any log-precision LLM $\llm{}$ with constant layers there exists a logspace-uniform thershold circuit $Th_{n}$ such that $\llmG{\vect{x}}{\regG} = Th_n(\vect{x})$ holds for all inputs $\vect{x}$ with size $|\vect{x}| = n$ and $n \in \mathbb{N}$.
\end{restatable}
\textbf{Proof:} The formal proof is in Appendix~\ref{sec:proof}. 

From Proposition~\ref{thm:compLimit}, it follows that for any decision problem under constrained decoding, an LLM can only solve problems within the logspace-uniform $\class$ class (constant-depth threshold circuits). 
Consequently, any decision problem believed to lie outside this class cannot be solved under constrained decoding. 
The previously mentioned st-connectivity problem is known to be $NL$-complete \cite{complexityBook}. This implies that unless $\class = NL$, no LLM under constrained decoding can solve st-connectivity.  
Additionally, \cite{expressivity2, expressivity1} show that given any Turing machine $M$ there exists a log-precision LLM with a constant number of layers that can simulate $O(\runtime{n})$ steps of $M$ using $O(\runtime{n})$ autoregressive steps, where $\runtime{n}$ denotes a polynomial in the input size $n$.

\begin{restatable}{lemma}{expressivity}
\label{lem:expressivity}
For any Turing machine $M$ with tape alphabet $\tapealpha$, there exists a constant depth LLM $\llm{}_{M}$ with finite vocabulary $\tapealpha \subseteq \vGM$ and log-precision that can simulate $\runtime{n}$ steps of $M$ with $\runtime{n}$ autoregressive steps. 
\end{restatable}
\textbf{Proof:} The proof follows from Theorem~2 in \cite{expressivity1} further details in Appendix~\ref{sec:proof}.

Proposition~\ref{thm:compLimit} and Lemma~\ref{lem:expressivity} together imply that there exist problems, such as st-connectivity, an LLM can solve that in an unconstrained setting but cannot be solved under constrained decoding (unless logspace-uniform $\class = NL$).

\subsection{Reasoning with Augmented Grammar}
The reduction in LLM expressivity under constrained decoding, as established in Proposition~\ref{thm:compLimit}, arises primarily because the language of all valid output strings, $ L(\regG) $, is too restrictive and does not permit large (non-constant) reasoning chains. This naturally leads to the question of whether it is possible to augment any output grammar $G$ with additional production rules to construct an augmented grammar $\augG$ that can accommodate reasoning steps while preserving the expressivity of $\llm{}$ even under constrained decoding. At the same time, $\augG$ should remain nontrivial—meaning it should not accept all possible strings, as in the unconstrained setting—so that it aligns with the practical objective of constrained decoding: guiding the LLM to generate syntactically and semantically valid outputs.

To achieve this, we enforce that the augmented grammar $ \augG $ always follows the structure $ \augG \to RG $, where the nonterminal symbol $ R $ captures the reasoning steps, and $ G $ represents the final output. This guarantees that for any string $ \vect{s} \in \lang{\augG} $, the final answer $ \vect{a} $ extracted from $ \vect{s} = \vect{r} \cdot \vect{a} $ always belongs to the original output grammar $ G $, i.e., $ \vect{a} \in \lang{G} $, with $ \vect{r} $ serving as the reasoning sequence leading up to the final output.  

Formally, we show that for any Turing machine $ M $ and a grammar $ G $ containing all valid outputs of $ M $, there exists an LLM $ \llm{}_{M} $ with a constant number of layers and log-precision, along with an augmented grammar $ \augG $ in the specified format, such that $ \llm_M $ can simulate $ \runtime{n} $ steps of $ M $ using $\runtime{n}$ autoregressive steps under constrained decoding with $ \augG $. Here $n \in \mathbb{N}$ and $\runtime{n}$ is a polynomial over $n$. The augmented grammar $ \augG$ may not be unique, and we provide one such construction. 

At a high level, $ \llm{}_M $ simulates the Turing machine $M$ by computing the encoded representations $ \hatConfig{\gamma_{i}} $ of the machine's configurations $ \gamma_{i} $ at each step $ i $ and storing them within the reasoning component (i.e., the string $ \vect{r} $) of the output. During each autoregressive step, $ \llm_M $ generates the next configuration based on the transition function of $ M $ and appends its encoding to the reasoning sequence. This process continues until $ M $ reaches a halting state, at which point $ \llm_M $ produces the final output $ \vect{a} $, which belongs to $ \lang{G} $. For any given $ M $, we define the rules $ \rGM $ that can parse the encodings $ \hatConfig{\gamma} $ of all possible configurations $ \gamma $. This ensures that the output $ \llmG{\vect{x}}{G_a} $ represents the full reasoning-augmented sequence, i.e., $\hatConfig{\gamma_{1}}\cdots \hatConfig{\gamma_{\runtime{n}}} \cdot M(\vect{x}) $, where $ M(\vect{x})$ is the final output of $ M $ on input $ \vect{x} $ of size $ n $ after $ \runtime{n} $ computational steps. The encodings $ \hatConfig{\gamma_{1}}, \dots, \hatConfig{\gamma_{\runtime{n}}} $ correspond to the configurations $ \gamma_{1}, \dots, \gamma_{\runtime{n}} $, as described below.

We begin by defining the vocabulary $\vGM$ for $ \llm_{M} $, which contains all tape symbols $ \tapealpha $ of $ M $ along with a finite set of auxiliary symbols $ \hatConfig{\gamma} $ that encode the corresponding configurations $ \gamma $. Similar to prior works \cite{expressivity1}, each configuration encoding $ \hatConfig{\gamma} $ represents the current state $ q $, the symbols at the current head position of $k + 2$ tapes (input, output and $k$ work tapes), and the head movement directions $\{0, +1, -1\}$ for each tape. Directions $\{0, +1, -1\}$ denote either staying in place ($ 0 $), moving left ($ -1 $), or moving right ($ +1 $) by a single position. Since the set of states $ Q $, the tape alphabet $ \tapealpha $, and the number of tapes $ k $ are all constants, the total number of possible encodings $ \hatConfig{\gamma} $ is also constant. Let $ \hatConfig{\Gamma} $ denote the set of all possible configuration encodings, i.e., $ \hatConfig{\Gamma} = \{\hatConfig{\gamma_{(1)}}, \dots, \hatConfig{\gamma_{(l)}}\} $, where $ l = |\hatConfig{\Gamma}| $. Given $\hatConfig{\Gamma} $ is finite and enumerable, we can define the rules of the augmented grammar $ \augG $ accordingly as follows.

\begin{align*}
 &\augG \to \rGM G; \;\;\; \rGM \to S \rGM; \;\;\; S \to \hatConfig{\gamma_{(1)}} \; |  \;\cdots\; | \hatConfig{\gamma_{(l)}} 
\end{align*}

The set of reasoning strings in $L(\rGM)$ essentially define a regular language over the configuration encodings $\hatConfig{\Gamma}$. Let, for any input $\vect{x}$ with size $n = |\vect{x}|$ a given Turing machine $M$ halts and compute the output $M(\vect{x})$ in $\runtime{n}$ steps that are polynomial in $n$. Then there exist $\llm{}_{M}$ compute $M(\vect{x})$ with $\runtime{n}$ autoregressive steps under constrined decoding with the augmented grammar $\augG \to \rGM G$. Suppose, $\llmMG{\vect{x}}{\augG}$ denotes the output of the LLM $\llm_{M}$ on input $\vect{x}$ under constrained decoding with grammar $\augG$ then
\begin{restatable}{proposition}{expressConstrain}
\label{thm:expressConstrain}
For any Turing machine $M$ with tape alphabet $\tapealpha$, there exists a constant depth LLM $\llm{}_{M}$ with finite vocabulary $\tapealpha \subseteq \vGM$ and log precision such that for any input $\vect{x}$ with $|\vect{x}| = n$, $\llmMG{\vect{x}}{\augG} = \vect{r} \cdot M(\vect{x})$ with $r \in V_{M}^{*}$ assuming $M$ halts on $\vect{x}$ in $\runtime{n}$ steps.
\end{restatable}
\textbf{Proof:} The proof is in Appendix~\ref{sec:proof}.
% \textbf{Proof Sketch: }From Lemma~\ref{lem:expressivity} in unconstrined setting $\llm{}_{M}$ simulates the run of the machine $M$ on $\vect{x}$ with output of the $i$-th autoregressive step satisfying $\llmFormalM{i}{\vect{x}} = \vect{x}\cdot\hatConfig{\gamma_1}\cdots\hatConfig{\gamma_{i}}$ for all $1 \leq i \leq \runtime{n}$ and $\llmFormalM{i}{\vect{x}} = \vect{x}\cdot\hatConfig{\gamma_1}\cdots\hatConfig{\gamma_{\runtime{n}}}\cdot \vect{\alpha_{i}}$ for all $\runtime{n} < i \leq |M(\vect{x})|$ with $\vect{\alpha_{i}}$ is a prefix of $M(\vect{x}) \in L(G)$. $\llmFormalM{i}{\vect{x}}$
\section{\Tool Algorithm}
Given any Turing machine $M$, Proposition~\ref{thm:expressConstrain} establishes that constrained decoding with the augmented grammar $\augG$ on a specific LLM $ \llm_{M} $ can simulate the computation of $ M $. However, this result does not directly translate into a practical constrained decoding algorithm that preserves the expressivity of general LLMs. The construction assumes a specific LLM $ \llm_{M} $ with the vocabulary $V_{M}$ and knowledge of the particular Turing machine $M$ for defining the rules $\rGM$. In practice, we require an efficient approach that can be applied to diverse open-source LLMs, various grammars, and different constrained decoding algorithms. Importantly, we know that enforcing the output grammar $ G $ from the beginning can limit expressivity. Instead, we impose grammar constraints judiciously to avoid restricting the LLM's reasoning capabilities. For example, in the case of a reasoning-augmented output of the form $ \hatConfig{\gamma_{1}}\cdots \hatConfig{\gamma_{\runtime{n}}} \cdot M(\vect{x}) $, we apply constrained decoding only from the $ \runtime{n} + 1 $-th autoregressive step onward, ensuring that the reasoning process remains unrestricted while the final answer adheres to the desired grammar. 

The primary challenge here is deciding when to transition between an unconstrained generation for reasoning and a constrained generation. 
For instance, grammar for general-purpose programming languages such as Python can allow any text string at the start (e.g. program starting variable names) making it hard to detect the end of reasoning string. 
To avoid this, we augment the output grammar with specific delimiter symbols $S_1$ and $S_2$ that mark the start and end of the constrained generation. 
We incentivize the LLM to generate these delimiters via explicit instructions in the prompt and few-shot examples. This aligns with common general-purpose LLMs that already use specific delimiters such as backticks (\textcolor{red}{\texttt{```}}) for programs like python, SQL, and (\textcolor{red}{\texttt{<<}, \texttt{>>}}) to enclose math-expression blocks. This approach allows a simple and cost-efficient approach for detecting the transitions to and from constrained decoding. For the construction in the previous section, in this setup, we will generate the string $\vect{r}\cdot S_1\cdot M(\vect{x})\cdot S_2$ where the reasoning $\vect{r}$ is generated unconstrained and the LLM moves to constrained mode after seeing the symbol $S_1$. However, in practical cases, the delimiters may be generated multiple times (ie. for intermediate operations), even during the reasoning step. Therefore, upon encountering the end symbol $S_2$, we switch back to unconstrained generation to avoid unnecessarily restricting the output.

 %do we get expressivity guarantees as long as the LLM generates S_1
% I think it is easy to show we can mention that 
% ok cool just to beef up this section a little bit and tie it more to the section 3 yeah 

\begin{figure}[tbh]
\centering
\includegraphics[width=8cm]{figures/crane.png}
\vspace{-.1in}
\caption{
\Tool{} adaptively switches between constrained LLM generation and unconstrained LLM generation based on start and end delimiters (in this example \textcolor{red}{\texttt{<<}} and  \textcolor{red}{\texttt{>>}}). Using these delimiters, \Tool{} dynamically tracks which windows (highlighted in the figure) of the LLM generation constraints should be applied to. 
} 
\label{fig:crane}
\vspace{-.1in}
\end{figure}

We implement our approach into the \Tool algorithm (Algo ~\ref{alg:crane}), which extends standard autoregressive LLM generation. 
\Tool takes an arbitrary LLM, constrained decoding algorithm (denoted as CSD), output grammar $G$, and symbols $S_1$ and $S_2$ as input. It first initializes CSD with $G'$, the output grammar augmented with $S_1$ and $S_2$. \Tool starts in unconstrained generation and maintains a pointer that marks the start of the current window of LLM generation following the last constrained generation. In each iteration, the algorithm checks if $S_1$ is present in the current generation window $\texttt{currGen}$, which is the portion of the sequence from the current pointer position onwards.  If $S_1$ is detected, \Tool switches to constrained generation mode. In this mode, the current constrained window (the portion of $\texttt{currGen}$ that is in $G'$) is extracted, and the next token is sampled based on the constraints defined by the CSD. If $S_1$ is not present, the next token is computed directly without any constraints applied. Additionally, if the current constrained window ends with $S_2$, the pointer is updated to the length of the current token sequence, effectively switching back to unconstrained generation until $S_1$ is generated again. Figure ~\ref{fig:crane} further illustrates LLM generation with \Tool{}. The underlined portion of the LLM generation represents $\texttt{currGen}$, and the current constrained window is highlighted in yellow. 


% Add real example here
% For instance, for a task with a grammar G for all mathematical expressions and start and delimeter symbols "<<" and ">>",  in the partial LLM generation \str{text << … >> text << x + y}, the portion highlighted in green denotes $\texttt{currGen}$, the current LLM generation window, and the underlined portion denotes the portion of $\texttt{currGen}$ that is in $G'$
% \str{text << … >> text << x + y}

\input{algorithms/new_crane}

% need somehting that can work with abritrary grammar-constrained decoding tool and various grammars