@misc{beurerkellner2024guiding,
      title={Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation}, 
      author={Luca Beurer-Kellner and Marc Fischer and Martin Vechev},
      year={2024},
      eprint={2403.06988},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{cicuit2,
    author = {Hao, Yiding and Angluin, Dana and Frank, Robert},
    title = {Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {800-810},
    year = {2022},
    month = {07},
    abstract = {This paper analyzes three formal models of Transformer encoders that differ in the form of their self-attention mechanism: unique hard attention (UHAT); generalized unique hard attention (GUHAT), which generalizes UHAT; and averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers, viewed as string acceptors, can only recognize formal languages in the complexity class AC0, the class of languages recognizable by families of Boolean circuits of constant depth and polynomial size. This upper bound subsumes Hahn’s (2020) results that GUHAT cannot recognize the DYCK languages or the PARITY language, since those languages are outside AC0 (Furst et al., 1984). In contrast, the non-AC0 languages MAJORITY and DYCK-1 are recognizable by AHAT networks, implying that AHAT can recognize languages that UHAT and GUHAT cannot.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00490},
    url = {https://doi.org/10.1162/tacl\_a\_00490},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00490/2037124/tacl\_a\_00490.pdf},
}

@article{circuit1,
    title = "Theoretical Limitations of Self-Attention in Neural Sequence Models",
    author = "Hahn, Michael",
    editor = "Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.11/",
    doi = "10.1162/tacl_a_00306",
    pages = "156--171",
    abstract = "Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics."
}

@article{circuit3,
    title = "Saturated Transformers are Constant-Depth Threshold Circuits",
    author = "Merrill, William  and
      Sabharwal, Ashish  and
      Smith, Noah A.",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.49/",
    doi = "10.1162/tacl_a_00493",
    pages = "843--856",
    abstract = "Transformers have become a standard neural network architecture for many NLP problems, motivating theoretical analysis of their power in terms of formal languages. Recent work has shown that transformers with hard attention are quite limited in power (Hahn, 2020), as they can be simulated by constant-depth AND/OR circuits (Hao et al., 2022). However, hard attention is a strong assumption, which may complicate the relevance of these results in practice. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We first show that saturated transformers transcend the known limitations of hard-attention transformers. We then prove saturated transformers with floating-point values can be simulated by constant-depth threshold circuits, giving the class TC0 as an upper bound on the formal languages they recognize."
}

@inproceedings{cotGoogle,
author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
title = {Chain-of-thought prompting elicits reasoning in large language models},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1800},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{expressivity1,
title={The Expressive Power of Transformers with Chain of Thought},
author={William Merrill and Ashish Sabharwal},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NjNGlPh8Wh}
}

@misc{melcer2024approximatelyaligneddecoding,
      title={Approximately Aligned Decoding}, 
      author={Daniel Melcer and Sujan Gonugondla and Pramuditha Perera and Haifeng Qian and Wen-Hao Chiang and Yanjun Wang and Nihal Jain and Pranav Garg and Xiaofei Ma and Anoop Deoras},
      year={2024},
      eprint={2410.01103},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.01103}, 
}

@misc{melcer2024constraineddecodingfillinthemiddlecode,
      title={Constrained Decoding for Fill-in-the-Middle Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars}, 
      author={Daniel Melcer and Nathan Fulton and Sanjay Krishna Gouda and Haifeng Qian},
      year={2024},
      eprint={2402.17988},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2402.17988}, 
}

@misc{park2024grammaraligneddecoding,
      title={Grammar-Aligned Decoding}, 
      author={Kanghee Park and Jiayu Wang and Taylor Berg-Kirkpatrick and Nadia Polikarpova and Loris D'Antoni},
      year={2024},
      eprint={2405.21047},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.21047}, 
}

@inproceedings{speakFree,
    title = "Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.",
    author = "Tam, Zhi Rui  and
      Wu, Cheng-Kuang  and
      Tsai, Yi-Lin  and
      Lin, Chieh-Yen  and
      Lee, Hung-yi  and
      Chen, Yun-Nung",
    editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-industry.91/",
    doi = "10.18653/v1/2024.emnlp-industry.91",
    pages = "1218--1236",
    abstract = "Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).This study investigates whether such constraints on generation space impact LLMs' abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs' performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs' reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks."
}

@article{survey,
  author={Lena Strobl and William Merrill and Gail Weiss and David Chiang and Dana Angluin},
  title={What Formal Languages Can Transformers Express? A Survey},
  year={2024},
  cdate={1704067200000},
  journal={Trans. Assoc. Comput. Linguistics},
  volume={12},
  pages={543-561},
  url={https://doi.org/10.1162/tacl_a_00663}
}

@article{tc0,
    title = "The Parallelism Tradeoff: Limitations of Log-Precision Transformers",
    author = "Merrill, William  and
      Sabharwal, Ashish",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.31/",
    doi = "10.1162/tacl_a_00562",
    pages = "531--545",
    abstract = "Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if L{\ensuremath{\neq}}P (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture`s high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it. Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm."
}

@misc{ugare2024itergeniterativestructuredllm,
      title={IterGen: Iterative Structured LLM Generation}, 
      author={Shubham Ugare and Rohan Gumaste and Tarun Suresh and Gagandeep Singh and Sasa Misailovic},
      year={2024},
      eprint={2410.07295},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2410.07295}, 
}

@misc{ugare2024syncodellmgenerationgrammar,
      title={SynCode: LLM Generation with Grammar Augmentation}, 
      author={Shubham Ugare and Tarun Suresh and Hangoo Kang and Sasa Misailovic and Gagandeep Singh},
      year={2024},
      eprint={2403.01632},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.01632}, 
}

@misc{willard2023efficient,
      title={Efficient Guided Generation for Large Language Models}, 
      author={Brandon T. Willard and Rémi Louf},
      year={2023},
      eprint={2307.09702},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

