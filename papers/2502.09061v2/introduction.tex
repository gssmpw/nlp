\section{Introduction}
\label{sec:intro}
Transformer-based large language models (LLMs) are widely used in AI systems that interact with traditional software tools like Python interpreters \cite{openai_tools, programofThoughts} \ifthenelse{\boolean{icml}}
{}{for code generation \cite{cornstack,code-watermark,surveycode}}, logical solvers \cite{pan2023logiclmempoweringlargelanguage, Olausson_2023}, and theorem provers \cite{wu2022autoformalization, yang2023leandojotheoremprovingretrievalaugmented}. These tools impose specific syntactic and semantic constraints on their inputs, requiring LLMs to produce outputs in the correct format. For instance, if an LLM provides output to a specific logical solver \cite{han2024FOLIOnaturallanguagereasoning}, the output must be parsable by that solver. Similarly, Wolfram Alpha~\cite{Wolfram} translates user queries about mathematical problems into a domain-specific language (DSL) to utilize symbolic solvers. However, as highlighted in recent studies \cite{syncode, guidance, poesia2022synchromesh}, pre-trained LLM outputs do not always comply with downstream tools' input requirements. Constrained decoding algorithms \cite{syncode, poesia2022synchromesh} address this issue by projecting the LLM output onto user-specified formal constraints (e.g., syntactic rules defined by a context-free grammar $G$), thereby ensuring that the input requirements of downstream tasks are satisfied.  
% \sasa{the paragraph too long. tries to send many messages. simplify.}

\noindent As illustrated in Fig.~\ref{fig:example}, constrained decoding improves the syntactic correctness of LLM outputs (e.g., generating a well-formed mathematical expression). 
However, it does not guarantee functional correctness (e.g., ensuring the expression correctly answers the user's query). 
Recent works such as \citet{speakFree} have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. 
\citet{speakFree} attributes this reduction in functional accuracy to a decline in the LLM's reasoning capabilities under constrained decoding. 
This observation  raises the following open questions:
\begin{itemize}[noitemsep, nolistsep, leftmargin=*]
    \item \textbf{RQ1}: Do LLMs truly lose reasoning capabilities under constrained decoding?
    % \sasa{Do we assume only forward-token constr. decoding or also backtracking like Itergen? We assume both that ensures that final output at least in some grammar}
    \item \textbf{RQ2}: How can we leverage the benefits of constrained decoding in reducing syntax errors while preserving the unconstrained reasoning capabilities of LLMs? 
    % \sasa{At this point I don't know what semantic properties you have in mind or what semantic accuracy is (you also use functional correctness above).}
\end{itemize}
% This raises an open question: 
% \textbf{Do LLMs truly lose reasoning capabilities under constrained decoding, and if so, how to address this issue?} 
% Addressing this challenge requires generating LLM outputs that not only conform to the formal structure required by the downstream component but also align with the user's query.
\begin{figure*}[t] % 't' places the figure at the top of the page
    \centering
    \includegraphics[width=0.9\linewidth]{figures/example.png} % Adjust path & width as needed
    \caption{An example from the GSM-symbolic dataset (variables in blue) where unconstrained generation produces syntactically incorrect output, while constrained generation provides a syntactically valid but incorrect answer. \Tool, however, generates a correct answer.}
    \label{fig:example} % Label for referencing in the text
\end{figure*}

\textbf{Key Challenges:} First, we need to formally identify the root cause of the reduction in functional accuracy of end-to-end systems when a pre-trained LLM operates under constrained generation. 
Unlike the empirical observations in \cite{speakFree}, we seek a formal justification for this reduction that is not limited to specific LLMs used in experiments but extends to any LLM, including more powerful ones developed in the future.

Second, we must design cost-efficient decoding strategies that address the shortcomings of existing constrained decoding methods while improving functional accuracy. In this work, we do not consider task-specific fine-tuning of LLMs, as fine-tuning for each task is compute-intensive. Unlike constrained decoding, fine-tuning does not guarantee that the LLM output adheres to formal constraints.
% \sasa{This sentence can be stronger along the lines of: The cost of fine-tuning is unacceptable... }
% \sasa{A meta problem is that you spend time describing what you don't want to do, not what you do. You can say that you "depart from the common wisdom of fine-tuning, because its cost is prohibiticely high." But then make a punchline to what will be in the contrib. }

\textbf{Contributions: }We make the following contributions to improve the functional accuracy of the end-to-end system:
\begin{itemize}[noitemsep, nolistsep, leftmargin=*]
\item
We theoretically show that LLMs with a constant number of layers, which are known to be capable of simulating \( n \) steps of any given Turing machine \( M \) with \( O(n) \) reasoning steps \cite{expressivity1}, can only solve problems within a relatively restrictive circuit complexity class when constrained to generate outputs that always conform to a restrictive grammar \( G \) defining only the valid output strings. This demonstrates that, for restrictive grammar, constrained decoding reduces the problem-solving capabilities of LLMs.

\item We theoretically show that the loss of expressivity of LLMs under constrained decoding arises because the output grammar $G$ is too restrictive to accommodate the intermediate reasoning steps required to compute the answer. We further demonstrate that augmenting the grammar $G$  with specific additional production rules enables the LLM to generate the intermediate reasoning steps while ensuring that the final output always adheres to the intended output structure. With the augmented grammar $\augG$, the LLM retains its expressivity under constrained decoding.

\item We propose a simple and cost-efficient decoding strategy, \Tool (\textbf{C}onstrained \textbf{R}easoning \textbf{A}ugmented Ge\textbf{ne}ration). 
\Tool effectively alternates between unconstrained generation for reasoning and constrained generation for producing structurally correct outputs. This allows the model to produce syntactically valid outputs while enabling the LLM to reason.
% \sasa{This is good, but I don't know how exactly it works or connects with the previous insights.}
Our detailed experiments on multiple open-source LLMs and benchmarks demonstrate that \Tool{} significantly outperforms both SOTA constrained decoding strategies and standard unconstrained decoding, showing up to a \upto{} improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic~\cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical}) and FOLIO~\cite{han2024FOLIOnaturallanguagereasoning}.
\end{itemize}

Next, we provide the notations and necessary background on constrained decoding, including the definition of Turing machines and relevant circuit complexity classes.
% \sasa{Put this in 1st para of preliminaries; remove the current 1st para, it's serving no purpose.}
