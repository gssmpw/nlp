
\section{Evaluation}
\input{tables/gsm_symbolic_main} 


In this section, we evaluate \Tool{} on a math reasoning task (GSM-Symbolic~\cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical}) and a logical reasoning task (FOLIO~\cite{han2024FOLIOnaturallanguagereasoning}) and demonstrate significant improvement over both unconstrained and SOTA constrained generation baselines. 

\noindent \textbf{Experimental Setup.}
We run experiments on a 48-core Intel Xeon Silver 4214R CPU with 2 NVidia RTX A5000 GPUs. 
\Tool{} is implemented using PyTorch~\cite{NEURIPS2019_9015} and the HuggingFace transformers library~\cite{wolf-etal-2020-transformers}. Our primary baseline for unconstrained generation is Chain-of-Thought (CoT) Prompting~\cite{cotGoogle}, which enables LLMs to decompose and reason about a problem through a series of intermediate steps before outputting the final answer. Furthermore, we run constrained semantic generation for GSM-Symbolic~\cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical} with the \itergen{} library~\cite{ugare2024itergeniterativestructuredllm} and use the \syncode{} framework for FOLIO~\cite{han2024FOLIOnaturallanguagereasoning} evaluation. In all experiments, \Tool{} is initialized with the same constrained decoders and uses the same constraints as the constrained generation baselines.

\textbf{GSM-Symbolic: }We first evaluate \Tool{} on GSM-Symbolic~\cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical}, a dataset consisting of math word problems designed to assess LLMs' mathematical reasoning skills. 
In the word problems, names and numerical values are replaced with symbolic variables, and the LLMs are tasked with generating correct symbolic expression solutions (see Appendix~\ref{sec:gsm_info} for examples). To evaluate correctness, we extract the final expressions from the LLM generations and verify if they are functionally equivalent to the ground truth expressions with the Z3 solver~\cite{z3}.

We compare \Tool{} against three baselines: (1) unconstrained generation without chain-of-thought prompting, (2) unconstrained generation with CoT, and (3) constrained generation. We use \itergen{} for the constrained generation baseline and also initialize \Tool{} with \itergen{}. For \itergen{} and \Tool{}, we enforce syntactic constraints via the context-free grammar provided in Appendix ~\ref{sec:gsm_grammar} and apply the semantic constraint ensuring that generated expressions contain only valid problem-defined variables. Since \itergen{} uses selective rejection sampling to enforce semantic constraints, we also include comparisong against unconstrained generation with sampling in Table ~\ref{tab:rejection_sample_gsm} in the Appendix. For \Tool{}, we use \textcolor{red}{\texttt{<<}} and  \textcolor{red}{\texttt{>>}} for the delimeters $S_1$ and $S_2$, respectively.
\ifthenelse{\boolean{icml}}
{We evaluate four LLMs for the experiment: Qwen2.5-1.5B-Instruct~\cite{qwen2.5}, Qwen2.5-Math-7B-Instruct~\cite{qwen2.5}, Qwen2.5-Coder-7B-Instruct~\cite{qwen2.5},and Llama-3.1-8B-Instruct~\cite{llamamodels}. For all models, we use greedy decoding with a maximum new token limit of 600. Additionally, we prompt the LLMs with the 8-shot examples from GSM-Symbolic~\cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical} (the prompts can be found in Appendix ~\ref{sec:gsm_info}).}
{We evaluate Qwen2.5-1.5B-Instruct~\citep{qwen2.5}, Qwen2.5-Math-7B-Instruct~\citep{qwen2.5}, Qwen2.5-Coder-7B-Instruct~\citep{qwen2.5},Llama-3.1-8B-Instruct~\citep{llamamodels}, DeepSeek-R1-Distill-Qwen-7B~\citep{deepseekr1}, and DeepSeek-R1-Distill-Llama-8B~\citep{deepseekr1}. We use greedy decoding with a maximum new token limit of 600 and prompt the LLMs with the 8-shot examples from GSM-Symbolic~\citep{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical} (the prompts can be found in Appendix ~\ref{sec:gsm_info}).}

Table~\ref{tab:gsm_symbolic_comparison} compares the performance of \Tool{} with the baseline methods. The Accuracy (\%) column reports the percentage of functionally correct LLM-generated expressions, Parse (\%) indicates the percentage of syntactically valid expressions (i.e., expressions without invalid operations), and Tokens provides the average number of tokens generated. 

\begin{figure}[t]
    \centering
    \ifthenelse{\boolean{icml}}
    {\includegraphics[width=0.45\textwidth]{figures/k_accuracy_Qwen2.5-Math-7B-Instruct_old.png}}
    {\includegraphics[width=0.45\textwidth]{figures/k_accuracy_Qwen2.5-Math-7B-Instruct.png}}
    \caption{Accuracy (\%) of Qwen2.5-Math-7B-Instruct By Method and Number of Shots on GSM-Symbolic}
    \label{fig:gsm_ks}
\end{figure}


As shown in the table, \Tool{} consistently improves functional correctness across all evaluated models. For example, with the Qwen2.5-Math-7B-Instruct model, \Tool{} achieves 38\% accuracy, outperforming both constrained generation and unconstrained generation with CoT, which achieves 29\% accuracy. Similarly, with the Qwen2.5-1.5B-Instruct model, \Tool{} achieves 31\% accuracy—5 percentage points higher than an unconstrained generation with CoT and 9 percentage points higher than a constrained generation. Moreover, \Tool{} significantly enhances the syntactic correctness of generated expressions compared to unconstrained generation. Notably, none of the expressions generated using \Tool{} contain syntax errors, whereas 10\% of the expressions from unconstrained generation with CoT do. Although, for several instances, \Tool{} produces slightly more syntax errors than a purely constrained generation, it offers a substantial improvement in functional correctness over this baseline.

\textbf{Ablation Study on Few-shot examples:}
We evaluate \Tool{} and baselines on varying numbers of few-shot examples in the prompt and display the results for Qwen2.5-Math-7B-Instruct in Figure~\ref{fig:gsm_ks}.
Results for all models are presented in Table~\ref{tab:gsm_symbolic_comparison_k_shot} in the Appendix. \Tool{} consistently achieves higher accuracy on GSM-Symbolic than the baselines for all evaluated numbers of few-shot examples.

% \begin{figure*}[!htbp]
% \centering
%  \vspace{-.1in}
% %\hspace{3mm}
% \begin{subfigure}[b]{0.25\textwidth}
%  \includegraphics[width=.7\textwidth]{figures/k_accuracy_Qwen2.5-1.5B-Instruct.png}
%  % \vspace{-.1in}
%  \caption{Qwen2.5-1.5B-Instruct}
%  \label{fig:fig:k_accuracy_Qwen2.5-1.5B-Instruct}
% \end{subfigure}
% \hspace{2mm}
% \begin{subfigure}[b]{0.25\textwidth}
%  \includegraphics[width=.7\textwidth]{figures/k_accuracy_Qwen2.5-Math-7B-Instruct.png}
%  %\vspace{-.1in}
%  \caption{Qwen2.5-Math-7B-Instruct}
%  \label{fig:fig::k_accuracy_Qwen2.5-Math-7B-Instruct}
% \end{subfigure}
% \vspace{-.02in}
% \caption{Accuracy (\%) By Method and Number of Shots on GSM-Symbolic }
% \label{fig:gsm_ks}
% \vspace{-.1in}
% \end{figure*} 

% \begin{figure*}[htb]
% \centering
% \begin{minipage}[b]{.46\textwidth}
% \includegraphics[width=\textwidth]{figures/k_accuracy_Qwen2.5-1.5B-Instruct.png}
% \captionsetup{labelformat=empty}
% \caption{(a) Qwen2.5-1.5B-Instruct}
% \end{minipage}
% \addtocounter{figure}{-1}
% \begin{minipage}[b]{.46\textwidth}
% \includegraphics[width=\textwidth]{figures/k_accuracy_Qwen2.5-Math-7B-Instruct.png}
% \captionsetup{labelformat=empty}
% \caption{(b) Qwen2.5-Math-7B-Instruct}
% \end{minipage}\qquad
% \caption{Accuracy (\%) By Method and Number of Shots on GSM-Symbolic }
% \label{fig:gsm_ks}
% \end{figure*}


% In this case study, we show that \Tool{} can be used for improving the ability of LLMs to generate accurate symbolic mathematical expressions. We evaluate \Tool{} on GSM-Symbolic, a dataset comprising 100 modified problems from the test split of GSM8k—a widely used benchmark for assessing the mathematical reasoning skills of LLMs. In GSM-Symbolic, names and numerical values from the original problems are replaced with symbolic variables (see Appendix ~\ref{} for an example). LLMs are tasked with generating symbolic expressions that correctly answer these modified problems. We then verify whether the LLM-generated expressions are equivalent to the ground truth expressions using the Z3 Theorem Prover. We compare \Tool{} against three baselines: (1) unconstrained generation without chain-of-thought (CoT), (2) unconstrained generation with CoT, and (3) constrained generation via IterGen. For IterGen and \Tool{}, we enforce syntactic constraints via the context-free grammar provided in Appendix ~\ref{}. We also apply the semantic constraint that generated expressions may only use valid variables defined in the problem. We conduct evaluations on three models: Qwen2.5-1.5B-Instruct~\cite{qwen2.5}, Qwen2.5-Math-7B-Instruct~\cite{qwen2.5}, and Llama-3.1-8B-Instruct~\cite{llamamodels}. For all models, we use greedy decoding and set the maximum new token limit to 600.

% Table~\ref{tab:gsm_symbolic_comparison} presents our result comparing the unconstrained and constrained generation baselines to \Tool{}. The Accuracy (\%) column displays the percentage of functionally correct LLM-generated symbolic expressions, while Parse (\%) indicates the percentage of the expressions without a syntax error (ie invalid operation). Additionally, the Tokens column shows the average number of tokens generated, and the Time (s) column reports the average generation time.  

% As shown in the table, \Tool{} significantly improves the functional correctness of generated expressions over the baselines for all evaluated models. For instance, with the Qwen2.5-Math-7B-Instruct model, \Tool{} achieves 41\% accuracy compared to 29\% accuracy for constrained generation and unconstrained generation with CoT. Similarly, with the Qwen2.5-1.5B-Instruct model, \Tool{} reaches 31\% accuracy, which is 5\% higher than unconstrained generation with CoT and 9\% higher than constrained generation. Furthermore, \Tool{} significantly improves the syntactic correctness of generated expressions over unconstrained generation. Notably, none of the symbolic expressions generated with \Tool{} have syntax errors whereas 10\% of the expressions generated with unconstrained generation with CoT have syntax errors. While for several instances, LLM generation with \Tool{} makes a few more syntax errors than with purely constrained generation, \Tool{} significantly improves functional correctness over purely constrained generation.

%This is because in \Tool{}, switching to constrained generation is contigent on the LLM generating the start symbol during unconstrained generation. If the LLM fails to generate the start symbol, then constrained generation will never be triggered. 

%Nevertheless, \Tool{} significantly improves functional correctness over purely constrained generation, corroborating the fact that 



\textbf{FOLIO: }We further evaluate \Tool{} on the validation split of FOLIO dataset, which comprises 203 expert-written natural language reasoning instances and corresponding first-order logic (FOL) annotations. 
We evaluate the ability of LLMs to correctly translate the natural language reasoning instances into FOL formulas and leverage Prover9~\cite{prover9-mace4} a FOL solver to verify the correctness of the LLM-generated FOL formulas. 

We compare \Tool{} against grammar-constrained generation with \syncode{} using the Prover9 grammar (Appendix ~\ref{gram:prover9_grammar}). 
The Prover9 grammar divides FOL formulas into Predicates, Premises, and Conclusions and allows intermediate reasoning in comments (an example can be found in Appendix ~\ref{gram:folio_example}). 
We also compare \Tool{} against unconstrained generation with CoT. 
For all approaches and models, we run greedy decoding with a maximum new tokens limit of 800 and use 2 few-shot examples in the prompt. We also compare \Tool{} against unconstrained CoT with temperature sampling in Table ~\ref{tab:rejection_sample_fol} in the Appendix. 

Table~\ref{tab:fol_comparison} presents the results of our experiment. The Accuracy (\%) column in the table reports the percentage of functionally correct FOL translations while the Compiles (\%) column reports the percentage of FOL formulas extracted from LLM output that are syntactically valid and compile into a Prover9 program. 
\Tool{} outperforms the unconstrained and constrained generation baselines for all models evaluated. 

%Following ~\cite{logic-lm}, we instruct the LLM to generate a series of FOL formulas, which are divided into Predicates, Premises, and Conclusion. To verify correctness of the 




\input{tables/fol_main}