\section{Related Works}
\textbf{Constrained LLM Decoding:}
% Grammar-guided generation + constrained semantic decoding
Recent works have introduced techniques to enforce LLM generations to adhere to a context-free grammar using constrained decoding____.
Additionally, ____ have extended grammar-guided generation to incorporate task-specific semantic constraints. 
These approaches demonstrate that constrained decoding can improve the syntactic and semantic quality of LLM outputs for various structured generation tasks.

More recently, ____ demonstrated that constrained structured generation can negatively impact the quality of generated outputs. Similarly, ____ showed that greedily masking out tokens that do not lead to a valid string during next-token prediction can distort the output distribution, causing it to deviate from the true distribution of all grammatically valid outputs of $\llm$ for a given input.
To mitigate the distortion introduced by the greedy masking approach, these ``grammar aligned" methods____ use a trie to track previous generations, reducing generation divergence iteratively. 
However, they are computationally expensive and require a large no. of resamplings per prompt to converge.
% More recently, ____ and ____ have shown that constrained structured generation can adversely affect the quality of generated outputs. 
% This occurs since constrained decoding fails to normalize next-token probabilities based on the future grammaticality of the sequence, thereby skewing the LLM's original probability distribution. 
% To address this, ____ and ____ proposed Grammar-Aligned Decoding methods that aim to approximate the true constrained distribution through multiple resamplings. 

In contrast, our work focuses on the fundamental question of the theoretical expressivity of any constant layered constrained LLM, even under an ideal constrained decoding algorithm, and uses the insights to propose a practical solution.
We propose an adaptive constrained decoding approach that can support various constrained decoding methods, including grammar-aligned techniques while preserving the LLM's expressivity by reasoning chains.

\textbf{LLM Expressivity:} ____ provides a detailed survey of existing results from the perspective of formal language theory and complexity classes. A series of existing works ____ establish that, under suitable assumptions, a single autoregressive step on an input of any length for a constant-depth LLM can be represented as a constant-depth Boolean circuit. ____ show that the expressivity of LLMs significantly improves under popular reasoning approaches like Chain of Thought (CoT) ____, where LLMs take intermediate steps before generating the final answer. To the best of our knowledge, there is no prior work on LLM expressivity under grammar constraints.