\begin{abstract}


Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their ability to recognize and reason about novel or rare concepts, we propose a \textbf{Co}ntrastive visual \textbf{D}ata \textbf{A}ugmentation (\textbf{CoDA}) strategy. \textbf{CoDA} extracts key \textit{contrastive} textual and visual features of target concepts against the known concepts they are misrecognized as, and then uses multimodal generative models to produce targeted synthetic data. Automatic filtering of extracted features and augmented images is implemented to guarantee their quality, as verified by human annotators. We show the effectiveness and efficiency of \textbf{CoDA} on low-resource concept and diverse scene recognition datasets including INaturalist and SUN. We additionally collect \textbf{NovelSpecies}, a benchmark dataset consisting of newly discovered animal species that are guaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these three datasets show CoDA significantly improves SOTA visual data augmentation strategies by 12.3\% (NovelSpecies), 5.1\% (SUN), and 6.0\% (iNat) absolute gains in accuracy. \footnote{Code and data will be released upon publication acceptance.}



% \violet{it's unclear what's contrastive here if we don't expand on what are target concepts. E.g., a novel object and a closely-related known object}
% Done!

% \violet{figure 1: I think under the title "a. novel concept". "b. confusing concept", we should give the groundtruth label for the image. Also, for consistency of the color coding, I suggest we use green instead of blue for this part.}
% Thanks, it's done!




%While they can sometimes benefit from more detailed language descriptions, their visual understanding remains coarse. While existing data augmentation strategies to update LMMs require large amounts of new data and yield limited gains, we argue their main limitation is the inability to provide nuanced visual details and key contrastive features. 


% Additional experiments show our method outperforming other data augmentation strategies in improving closed-source LMMs like GPT4o-mini and traditional ViT classifiers.


 
% Large multimodal models (LMMs) are exceptional at understanding and reasoning with images containing common concepts. However, their fixed knowledge cutoff renders them obsolete from the moment they are released, with no ability to recognize novel concepts created or discovered after their training (e.g. new animal species, new car models).  Domain-specific knowledge gaps during training also make them prone to confusing visually similar or lower-resource concepts. While existing solutions to update LMMs require large amounts of new data and yields limited gains, we propose an efficient and effective alternative: \textbf{Co}ntrastive visual \textbf{D}ata \textbf{A}ugmentation (\textbf{CoDA}). \textbf{CoDA} extracts contrastive textual and visual features from target concepts, and then uses multimodal generative models to produce targeted synthetic data. Automatic filtering of extracted features and augmented data guarantee their quality, as verified by human annotators. We show the effectiveness and efficiency of updating LMMs using our synthetic data on low-resource concept and diverse scene recognition datasets including INaturalist and SUN. We additionally collect \textbf{NovelSpecies}, a benchmark dataset consisting of newly discovered animal species that are guaranteed to be unseen by LMMs. LLaVA 1-shot updating results on these three datasets show \textbf{CoDA} can more significantly improve LMM performance compared to existing SOTA visual data augmentation strategies, demonstrating over 12.3\% (NovelSpecies), 5.1\% (SUN), and 6.0\% (iNat) absolute accuracy gains. Additional experiments show our method outperforming other data augmentation strategies in improving closed-source LMMs like GPT4o-mini and traditional ViT classifiers.




% \vspace{-0.7em}

\end{abstract}