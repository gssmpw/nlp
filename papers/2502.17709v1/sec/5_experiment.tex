\section{Experiments}
\label{sec:experiment}


\input{tables/main_experiment}
\input{tables/additional_experiments}


\subsection{Datasets and Baselines}

To evaluate \textbf{CoDA}'s ability to improve novel and confusing concept recognition in LMMs, we experimented with \textbf{CoDA} and other relevant baselines on three different datasets:

\begin{enumerate}[topsep=0pt, itemsep=0em]
    \item \textbf{The iNaturalist Dataset}~\cite{van2018inaturalist} is a challenging natural world concept recognition benchmark for LMMs due to its extensive highly domain-specific and fine-grained species categories and inclusion of rare and low-resource species classes.
    
    \item \textbf{The SUN Dataset}~\cite{xiao2010sun} is a widely used large-scale scene recognition dataset that contains rich and confusing visual scenes. Correctly recognizing the scenes requires fine-grained visual reasoning and understanding of the scenes.
    
    \item \textbf{NovelSpecies Dataset}~(Sec.\ref{sec:novel_dataset}) is our new dataset consisting only of novel animal species concepts that LMMs are guaranteed to have never encountered in their training or fine-tuning.
\end{enumerate}

For each dataset, we use an automatic data selection strategy~\ref{subsec:data_selection_strategy} to find a subset of challenging concepts that the model fails to recognize. Then, we apply \textbf{CoDA} along with 3 other visual data augmentation baselines:

\begin{enumerate}[topsep=0pt, itemsep=0em]
    \item \textbf{All Real} uses an all real augmented image set. In the Fixed Real Data setting, this means using the 5 real images provided. In the Fixed Compute setting, this means using unlimited real images to match the total number of real + synthetic images in other settings.
    
    % the model is updated with only 5 existing real images of the concept. In the Fixed Compute setting, the model is updated with an equal-sized training set consisting 100\% of real images.
    
    \item \textbf{Cropping and Flipping} are widely used traditional visual data augmentation strategies. We include them here for direct comparison with \textbf{CoDA} and other existing feature-based augmentation methods.
    
    \item \textbf{ARMADA~\cite{jin2024armada}} is the current state-of-the-art feature-based visual data augmentation strategy for concept recognition and image classification. 
\end{enumerate}

In additional to these 3 baselines, we also include ablations of \textbf{CoDA} with non-contrastive textual and visual features, i.e. w/o contrastive guidance from confusable concepts (\ref{subsec:contrastive_feature_extraction}) nor discriminability-based feature filtering (\ref{subsec:feature_filtering}).


\subsection{Main Experiment}
\label{subsec:main_experiment}

For our main experiment, we consider two different resource settings that correspond to common real-world scenarios: 

\vspace{-0.5em}
\paragraph*{Fixed Real Data} Under the fixed real data setting, we only have access to 5 real images for each concept. Each data augmentation strategy may generate 1-5 synthetic images. Then, the model is LoRA-adapted on the combined real and synthetic images. This setting simulates real-world scenarios, where there isn't sufficient real training data for certain concepts. This is common for novel concepts, hyper-domain-specific concepts, and long-tail distributed datasets. In these scenarios, the quality and effectiveness of synthetic augmented data is especially instrumental to the updated model's performance. 
% Here we want to test which data augmentation methods are most effective for teaching LMMs new concepts given limited real data.

Experiment results across the 3 datasets show that \textbf{CoDA} consistently outperforms existing traditional and feature-based data augmentation methods in the Fixed Real Data setting. When augmenting the training set with just a single synthetic image, \textbf{CoDA} is able to achieve 11.8\% (NovelSpecies), 10.0\% (SUN), and 17.8\% (iNat) absolute gains in accuracy compared using all real images. It further out-performs the best existing baseline augmentation methods by 5-12\% absolute gains. We also observe that ablated performance of \textbf{CoDA} (w/o
contrastive) is still significantly above traditional and image-editing-based augmentation baselines while being almost consistently below \textbf{CoDA}'s performance. This shows the benefits of text-to-image generative augmentation methods compared to existing methods, as well as the benefits of fine-grained textual features during inference. This also highlights the need for contrastive feature selection and discriminability-based feature filtering. We find that increasing the number of augmented synthetic images does not necessarily improve updated model performance, this may be attributed to the fact that all generated images are ranked and selected from the same pool, with the first image being of the highest quality. Finally, the largest improvement over existing baselines can be seen in \textbf{NovelSpecies}, where \textbf{CoDA} methods involving visual features achieve the highest performance. This makes sense as the visual feature extraction method is designed to be robust to novel concepts with little textual documentation.

% (Sec.\ref{subsec:contrastive_feature_extraction})
% (Sec.\ref{subsec:feature_filtering})

% We also find that \textbf{CoDA} almost consistently out-performs its own ablated version without contrastive feature selection (Sec.\ref{subsec:contrastive_feature_extraction}) and discriminability-based feature filtering (Sec.\ref{subsec:feature_filtering}). We observe a significant and consistent performance gap between the ablated \textbf{CoDA} versions and existing image-editing-based augmentation baselines, showing the benefits of text-to-image generative augmentation methods compared to image-editing (More detailed analysis can be found in the Appendix). 

\vspace{-0.5em}
\paragraph*{Fixed Compute} Under the fixed compute setting, we assume access to unlimited real and synthetic images. However, the fine-tuning budget can only support a total of 20 images, allowing different percentages of real and synthetic images, from 0\% synthetic (20:0) to 100\% synthetic (0:20). This setting simulates real-world scenarios, where there is abundant real data. In such cases, the question is whether to just use all real data to update the model, or to include a non-trivial amount of augmented synthetic data. Traditionally, real data is always preferred due to perceived higher-quality. However, \textbf{CoDA}'s effectiveness in the Fixed Real Data setting prompts us to test the possibility of it being beneficial to include synthetic data even when real data is abundant. This hypothesis is tested by whether any of the models fine-tuned with mixed real/synthetic data can outperform the model fine-tuned with all real data.

Experiments on iNaturalist show diverging results between \textbf{CoDA} and other baseline augmentation methods: While including synthetic images generated by baseline methods generally led to lower performance, using \textbf{CoDA} augmented images can actually lead to improvements over using all real data. Furthermore, a 50-50 real-synthetic data mix generally outperforms all real or all synthetic data. We attribute the success of mixing synthetic and real data to the fact that \textbf{CoDA} generated synthetic data is aimed to highlight discriminable features of the confusing / novel concepts, making them more prominent and visible compared to real images. On the other hand, real images provide valuable style information and is a more accurate reflection of the test-time distribution, helping to ``ground'' the updated model.



% For existing baselines, fine-tuning on mixed real-synthetic datasets yield lower performance compared to using only real-images. However, it is possible to reach, and even surpass
% (by 2.7\% absolute gains) 
% All Real data fine-tuning if using CoDA-based synthetic data augmentation methods. Interestingly, while baseline methods all experience a performance decline when using mixed real-synthetic data compared to all synthetic data, CoDA-based methods all show a gain when using 50-50 mixed real-synthetic data compared to all synthetic data. The overall takeaway is that: when using CoDA-based synthetic data augmentation methods, it is often more effective to use a mixture of real and synthetic data for model updating, compared to all real or all synthetic data, a departure from conventional data augmentation practices which always prefer real data. 






\subsection{Additional Experiments}

For additional experiments, we focus on \textbf{NovelSpecies} as it most closely resembles real-world scenarios, where over time, models are required to learn novel concepts without access to sufficient real training data.

\vspace{-0.9em}
\paragraph*{Advanced T2I Model}

As explained in Sec.\ref{sec:method}, off-the-shelf model components used in \textbf{CoDA} can be easily swapped for superior versions of similar models to improve performance. To demonstrate this, we replace the open-weight Stable Diffusion 3.5 Large Turbo model~\citep{stablediffusion3.5} with the SOTA proprietary Recraft V3 Model~\cite{2024RecraftV3} and run the same LLaVA-updating experiments as in Tab.\ref{tab:main_experiment}. Here we note that Recraft V3 has better instruction-following ability as well as better image generation quality compared to Stable Diffusion 3.5 Large Turbo. More details on these differences can be found in Sec.\ref{subsec:qualitative_comparison}. Our experiment results in Tab.\ref{tab:additional_experiments} show a significant performance boost when LoRA fine-tuning LLaVA with Recraft V3 produced synthetic images compared to fine-tuning on all-real data (28.7\%) and also compared to fine-tuning on Stable Diffusion 3.5 Large Turbo produced synthetic data (7.9\%). This demonstrates the potential increase of \textbf{CoDA}'s effectiveness along with improvements in Text-to-Image generative models. We believe it is also possible to achieve similar improvements by replacing the LLM/VLM components of \textbf{CoDA} with superior models in the future.


\vspace{-0.9em}
\paragraph*{Proprietary LMM}

While proprietary LMMs like GPT4o-mini~\cite{hurst2024gpt4o} tend to have relatively strong 0-shot performance on existing datasets such as SUN and iNaturalist, their performance significantly degrades on \textbf{NovelSpecies} due to having never encountered the novel concepts. To test whether \textbf{CoDA} can effectively improve novel concept recognition performance for such Proprietary LMMs, we fine-tune the gpt-4o-mini-2024-07-18 model using \textbf{CoDA} and relevant augmentation baselines. Results in Tab.\ref{tab:additional_experiments} demonstrate a significant performance gain (9.5\%) for GPT4o-mini after being fine-tuned on \textbf{CoDA} augmented synthetic images. While this improvement is not as significant compared to the LLaVA-1.6 model (20.3\%), it is due to GPT4o-mini's better base performance.


\vspace{-0.9em}
\paragraph*{Traditional Classifier}
In addition to evaluating \textbf{CoDA} on LMMs which take image-text input and produces text output, we also test whether it can help traditional image classifiers recognize novel concepts. We run the widely-used ViT classifier~\cite{alexey2020image} on \textbf{NovelSpecies} with \textbf{CoDA} and other augmentation baselines. Results in Tab.\ref{tab:additional_experiments} show that \textbf{CoDA} is able to achieve a consistent performance gain over existing baselines for ViT-base (9.1\% for single-shot augmentation). The ViT classifier provides stronger base performance compared to general VLMs, thus offering less room for improvement. However, we note here that our main focusing on improving LMMs instead of such traditional classifiers stems from LMMs' superior extensibility and generalizability to other related tasks such as recognition-based reasoning and explanation.


% While this performance gain is less significant compared to LLaVA-based LMM models, this is because we use ViT as a  strong dedicated visual classification model, with less room for improvement.


% SD 3 model: \cite{esser2024scaling}

% To demonstrate this, we experiment with replacing the text-to-image generation model, one of the most important performance bottlenecks in CoDA, with a superior proprietary model, and evaluating its effect on final LMM updated performance. 

% Tab.\ref{tab:additional_experiments}








