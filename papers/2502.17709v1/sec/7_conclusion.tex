\section{Conclusion}
\label{sec:conclusion}

In this work, we propose \textbf{CoDA}, a contrastive visual data augmentation approach that helps LMMs recognize novel, confusing, and low-resource concepts through efficient and effective model updating. \textbf{CoDA} is a plug-and-play method which utilizes off-the-shelf models for contrastive feature extraction, feature filtering, text-to-image generation, and image filtering. We evaluate \textbf{CoDA} against four existing baselines and self-ablations on three datasets: INaturalist, SUN, and \textbf{NovelSpecies}, which we created in this work. Consisting only of animal species discovered in recent years, \textbf{NovelSpecies} offers an ideal testbed for LMMs' novel concept recognition. We provide comprehensive additional experiments demonstrating \textbf{CoDA}'s effectiveness for traditional classifiers and proprietary LMMs. Finally, we show that \textbf{CoDA} can be easily improved by replacing off-the-shelf components, such as text-to-image generation model with superior versions of similar models in the future.