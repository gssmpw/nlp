\section{Methods}
\label{sec:method}

\input{figs/method}

% Our main method, \textbf{Co}ntrastive Visual \textbf{D}ata \textbf{A}ugmentation (\textbf{CoDA}), is simple and easy to apply to LMMs in a variety of scenarios. Several components in the pipeline utilize existing off-the-shelf model components that can be easily swapped out for superior versions of similar models as research in their respective field progresses. Therefore, we expect the efficiency and effectiveness of \textbf{CoDA} to dramatically scale along with the advancement of relevant models. 

% Here we provide a step-by-step breakdown of the \textbf{CoDA} method:


As shown in Fig.\ref{fig:method}, \textbf{CoDA} consists of 4 major steps including contrastive textual and visual feature extraction, feature filtering, feature-controlled image generation, and augmented image filtering. Together these steps ensure \textbf{CoDA} reliably generates informative and high-quality augmented images that help LMMs recognize novel and confusing concepts.


\subsection{Feature Extraction}
\label{subsec:feature_extraction}

% For any novel, confusing, or low-resource concepts that the LMM has trouble recognizing (examples shown in Figure.\ref{fig:teaser}), we first extract visually identifying features from the concept. Such features can be later used to guide text-to-image generative models in providing targeted image generation. Specifically, we can extract such features from textual knowledge and few-shot visual examples. 

% \vspace{-1em}
\paragraph*{Textual Feature Extraction} 

% on current LMMs' concept recognition abilities with the SUN Dataset~\cite{xiao2010sun}, iNaturalist Dataset~\cite{van2018inaturalist}, and general real-world examples

In our exploratory experiments, we find that significant mis-recognition errors occur on low-resource or commonly mis-represented concepts in vision-language instruction fine-tuning and multimodal pre-training datasets, which the LMMs are trained on. For example, the LLaVA 1.6 (34B) model~\cite{liu2024llavanext}, mainly tuned on LAION-GPT-4V\cite{2024LAIONGPT4V} and ShareGPT-4V~\cite{chen2023sharegpt4v} datasets, has a strong tendency to mis-recognize interior images of ``Resupply Base'' as ``Wholesale Store'' (Fig.\ref{fig:teaser}). Unsurprisingly, we find that all related references of ``Resupply Base'' across the 3 instruction-tuning datasets only depict exterior views of the concept rather than interior views. While the concept itself is not a low-resource concept in existing text corpora, it is severely low-resource and also commonly mis-represented in vision-language instruction fine-tuning datasets.



To address this issue, we prompt LLMs to directly generate feature attributes of the target concept based on their existing knowledge, focusing on visual appearance, and avoiding hallucination for unfamiliar concepts. For this task, we use the cost-efficient GPT4o-mini model with chain of thought reasoning. Generally, textual feature extraction is most applicable for concepts that are high-resource in existing textual corpora, yet low-resource and/or commonly mis-represented in vision-language instruction-tuning and pre-training datasets. Here we do not try to classify which concepts fall under this criteria, but rather apply this step for all concepts. To ensure extracted feature quality and filter out hallucinated and/or non-visually-recognizable features, we pass all extracted features through an automatic filtering step, as described in~\ref{subsec:feature_filtering}.

We also considered other methods for textual feature extraction, including using knowledge bases~\cite{jin2024armada}, retrieval augmented generation, and LLMs with internet search. However, we believe currently the advantages brought by these methods do not out-weigh their complexity overhead, thus we opt for simplicity.



% textual feature extraction is attempted 

% In such cases where visual examples of the concept are scarce

% for cases where visual examples are scarce

% and textual knowledge may be general enough

% this can be enhanced with RAG with knowledge bases (ARMADA) or web search agents.
\vspace{-1em}
\paragraph*{Visual Feature Extraction}
\label{subsec:visual_feature_extraction}

While textual feature extraction generally works well for pre-existing and non-hyper-domain-specific concepts that are prevalent in textual data sources, it tends to fail when either of the conditions are not met. For example, a large language model with a knowledge cutoff prior to June 2023 would not be able to provide meaningful features regarding the Apple Vision Pro device announced in July, or the new animal species ``Clouded Tiger Cat (L. pardinoides)'' first described by scientists in April 2024~(\ref{fig:teaser}). In addition to this weakness, LLM-based textual feature extraction is also unreliable when asked to provide detailed information regarding hyper-domain-specific concepts like the ``Mazda MX-5 Miata RF'' or the ``Lear's Macaw (Anodorhynchus Leari)''. In practice, we observe that for novel and hyper-domain-specific concepts, most of the LLM extracted textual features end up being filtered out by our automatic feature filtering module.


To address this weakness, we implement an additional visual feature extraction module based on VLMs. Given a single image of the target concept, the VLM is asked to extract its key visual features. When there is more than one image containing the target concept available, we use a LM to de-duplicate and summarize the combined extracted visual features from all images. For simplicity and cost-efficiency, we use the GPT4o-mini model for both visual feature extraction and feature de-duplication.

% In contrast to textual feature extraction, visual feature extraction is most effective for hyper-domain-specific and novel concepts that are very rare or non-existent in textual corpora but have a limited number of visual examples, thus it well-complements the textual feature extraction method. Similarly: here we do not try to classify which concepts fall under this criteria, but rather apply this step for all concepts and rely on automatic filtering~\ref{subsec:feature_filtering} to remove low quality features. 

In contrast to textual feature extraction, visual feature extraction is most effective for hyper-domain-specific and novel concepts that are very rare or non-existent in textual corpora but have a limited number of visual examples. Thus, it well-complements textual feature extraction. Similarly, we do not attempt to classify which concepts fall under this criterion; instead, we apply this step to all concepts and rely on automatic filtering~(\ref{subsec:feature_filtering}) to remove low-quality features.

% concepts that are common in textual training data while rare / commonly mis-represented in visual-text training data, it does not work 





% another equally significant category of concept mis-recognition errors occur on hyper-domain-specific and novel concepts that are very rare or non-existent in textual corpora but have a limited number of visual-text examples, which can be used to extract useful visual features. 


% Clear examples of of this type of concepts include: newly discovered or uncommon plant/animal species (such as Clouded Tiger Cat in Figure.~\ref{fig:teaser}), newly released airplane, car, or electronic device models (such as the Apple Vision Pro), etc.


% For where concept is too novel, lacking textual knowledge in database or online.

% can work with as few as one single example



\vspace{-1em}
\paragraph*{Contrastive Feature Extraction} 
\label{subsec:contrastive_feature_extraction}

While basic textual and visual feature extraction both aim to exhaustively list identifying features of the target concept, this is essentially an intractable task for complex concepts as it usually requires a huge number of features to fully describe them. For novel or low-resource concepts the LMM has likely never seen before, it is extremely difficult to teach the LLM the new concept using an incomplete description. 

There are two potential solutions to this problem: (1). Leveraging hierarchical information to narrow down concept category and reduce descriptional features. (2). Illustrating the new concept based on contrastive differences from a similar existing concept the LMM already understands. Previous works in language and visual data augmentation~\cite{jin2024armada} tend to use solution (1). However, its feasibility is contingent on the existence of a comprehensive textual knowledge base or tree-like structure that already includes the target concept. As discussed in Section \ref{subsec:visual_feature_extraction}, this is often not the case for novel concepts such as new electronic products (e.g. Apple Vision Pro) or new animal species (eg. Clouded Tiger Cat).


To enable the handling of novel concepts and remove the need for external databases, we adopt solution (2) and perform contrastive multimodal feature extraction for all target concepts. First, we use the LMM's zero-shot inference on the target concept $\mathcal{C_\mathcal{T}}$ to obtain the misidentified concept $\mathcal{C_\mathcal{M}}$. Then, we perform contrastive textual and visual feature extraction by querying LLMs and VLMs for visually identifying features that belong to $\mathcal{C_\mathcal{T}}$ but not $\mathcal{C_\mathcal{M}}$. 




% Contrastive textual feature extraction, which queries LLMs for visually identifying features that distinguish the target concept from the confused concept.

% Contrastive visual feature extraction, which queries VLMs for visually identifying features that distinguish the target concept from the confused concept.





% as teaching LMMs a new concept by fully describing it from scratch requires a huge number of features.


% it usually requires a huge number of features to teach the model a concept from scratch.


% For concepts with simple names that resonate its category and 




% One idea is to describe the object from top down in a hierarchical manor\cite{jin2024armada}. However, this is often not ideal for two reasons:

% 1. for visual features, its often difficult to 
% 2. it requires a powerful model to generate 




% learn from similar concept

% learn fr



% there are always similar concept

% You can force the model to make a prediction as to 


% it is difficult to describe the concept from scratch

% its easier to learn from relevant concepts and figure out the difference








% \subsection{Feature Extraction}
% \subsubsection{Feature Selection and Data Synthesis for Fine-Tuning}

% \textbf{Attribute Extraction Using GPT-4o-mini:}

% We leveraged GPT-4o-mini to extract distinctive visual attributes of each target species. The extracted text features will be used later to generate prompts for synthesizing images for finetuning. There are two sources for features. 

% \begin{itemize}
%     \item \textit{From Visual Data:} By analyzing images of the species to identify observable characteristics. This means that GPT-4o-mini is provided with the image.
%     \item \textit{From Textual Knowledge:} By utilizing GPT-4's knowledge base to summarize important features. This means that GPT-4o-mini is not provided with the image.
% \end{itemize}

% There are six prompting strategies employed:

% \begin{itemize}
%     \item \textit{Visual Prompt} 
%     \item \textit{Textual Prompt} 
%     \item \textit{Contrastive Visual Prompt} 
%     \item \textit{Contrastive Textual Prompt} 
%     \item \textit{Visual-Text Combined Prompt} 
%     \item \textit{Contrastive Visual Text Combined Prompt} 
% \end{itemize}
% For Visual-Text combined prompting, we take the union of features extracted from test only and visual only prompts. In the contrastive prompting strategies, we distinguish in two ways:

% \begin{enumerate}
%     \item We provide GPT-4 with the names of both the main class (target species) and the confusing class, and prompt it to focus only on features of the main class, explicitly excluding features of the confusing class.
%     \item We employ automatic filtering methods based on real images to ensure the extracted features are truly distinctive and contrastive, which will be discussed in the following section.
% \end{enumerate}


% We experimented with mixtures of these approaches to optimize the quality and relevance of the extracted attributes.







\subsection{Feature Filtering}
\label{subsec:feature_filtering}


\paragraph*{Automatic Feature Filtering} 


After obtaining visually identifying features from contrastive textual and visual feature extraction, we filter them based on two key criteria:

\begin{enumerate}[topsep=0pt, itemsep=0em]
    \item \textbf{Discriminability ($D(f, \mathcal{C}_T, \mathcal{C}_M)$) :} measures whether a feature $f$ can indeed be used to differentiate the target class $\mathcal{C_\mathcal{T}}$ from the misidentified concept $\mathcal{C_\mathcal{M}}$ ($f$ must first be a valid feature of $\mathcal{C_\mathcal{T}}$).
    \item \textbf{Generability ($G(f, \mathcal{C}_T, \mathcal{C}_M)$) :} measures whether a feature $f$ can be properly generated by the text-to-image generative model.
\end{enumerate}


To calculate the Discriminability of a feature $f$ given the target concept $\mathcal{C_\mathcal{T}}$ and misidentified concept $\mathcal{C_\mathcal{M}}$, we compute the likelihood that CLIP~\cite{radford2021learning} associates this feature with real images of the target concept compared to the likelihood that it is associated with real images of the misidentified class:
\vspace{-0.5em}
\[
D(f, \mathcal{C}_T, \mathcal{C}_M) = \sum_{i \in I} \frac{\text{CLIP}(f, i_{\mathcal{C}_T}^{\text{real}})}{\text{CLIP}(f, i_{\mathcal{C}_T}^{\text{real}}) + \text{CLIP}(f, i_{\mathcal{C}_M}^{\text{real}})}
\]
% \vspace{-0.1em}
Here we use an equal number of images of the target and misidentified concepts. A score below 0.5 indicates that the feature is more likely to be associated with the misidentified class rather then the target class. To ensure that selected features are more strongly associated with the target class, we filter out all features with Discriminability below 0.6. This method avoids the CLIP score bias against smaller features by only comparing feature association with the two classes and not relying on the absolute CLIP score. 
% It also ensures that the target concept $\mathcal{C_\mathcal{T}}$ actually contains the feature $\mathcal{C_\mathcal{M}}$ as this is a prerequisite for strong association.

Generability is calculated in a similar manner, comparing the average CLIP similarity between $f$ and synthetic images of the target concept against the average CLIP similarity between $f$ and real images of the misidentified concept:

\vspace{-0.7em}
\[
G(f, \mathcal{C}_T, \mathcal{C}_M) = \sum_{i \in I} \frac{\text{CLIP}(f, i_{\mathcal{C}_T}^{\text{synthetic}})}{\text{CLIP}(f, i_{\mathcal{C}_T}^{\text{synthetic}}) + \text{CLIP}(f, i_{\mathcal{C}_M}^{\text{real}})}
\]
\vspace{-0.7em}

Here we rank all remaining features by their Generability score and select the top 5 features to be passed to the text-to-image generative model (as current diffusion models usually have limited text encoder attention span). This step identifies features that not only help distinguish the target concept, but also can be effectively rendered by the text-to-image generative model in synthetic images, which is critical to the success of synthetic data augmentation.

Our automatic feature filtering module based on Discriminability and Generability ensures feature quality and limits the information loss between features and the generated augmented images. The remaining features are used for image generation and improving in-context recognition ability in inference prompts. We further verify the quality of remaining features with human evaluation in Sec.\ref{sec:human_eval}.

% \[
% \text{D}(f, \mathcal{C}_T, \mathcal{C}_M) = \sum \frac{\text{CLIP}(f, I_{\mathcal{C}_T}^{\text{real}})}{\text{CLIP}(f, I_{\mathcal{C}_T}^{\text{real}}) + \text{CLIP}(f, I_{\mathcal{C}_M}^{\text{real}})}
% \]

% \[
% \text{G}(f, \mathcal{C}_T, \mathcal{C}_M) = \sum \frac{\text{CLIP}(f, I_{\mathcal{C}_T}^{\text{synthetic}})}{\text{CLIP}(f, I_{\mathcal{C}_T}^{\text{synthetic}}) + \text{CLIP}(f, I_{\mathcal{C}_M}^{\text{real}})}
% \]



% \[
% \text{Discriminability}(f, {\text{$\mathcal{C_\mathcal{T}}$, $\mathcal{C_\mathcal{M}}$}}) = 
% \frac{\text{CLIP}(\text{f},\, \text{$\mathcal{C_\mathcal{T}}$ images})}
%      {\substack{\text{CLIP}(\text{f},\, \text{$\mathcal{C_\mathcal{T}}$ images}) + \\
%       \text{CLIP}(\text{f},\, \text{$\mathcal{C_\mathcal{M}}$ images})}}
% \]



% \[
% \text{Generability}(f, {\text{$\mathcal{C_\mathcal{T}}$, $\mathcal{C_\mathcal{M}}$}}) = 
% \frac{\text{CLIP}(\text{f},\, \text{synthetic $\mathcal{C_\mathcal{T}}$ images})}
%      {\substack{\text{CLIP}(\text{f},\, \text{synthetic $\mathcal{C_\mathcal{T}}$ images}) + \\
%       \text{CLIP}(\text{f},\, \text{$\mathcal{C_\mathcal{M}}$ images})}}
% \]



% To enhance the effectiveness of our fine-tuning process, we implement a feature filtering method that selects the most distinctive and generable features of each main class. This filtering is based on two main criteria:

% \begin{enumerate}
%     \item \textbf{Combining Features from Multiple Images:} We extract features from 5 distinct images of the main class to capture a comprehensive set of visual attributes.
%     \item \textbf{CLIP Score-Based Filtering:} We use CLIP scores to filter out features based on:
%     \begin{itemize}
%         \item \textit{Contrastiveness:} How much a feature appears in real images of the main class compared to real images of the confusing class.
%         \item \textit{Generability:} How well a feature appears in synthetic images of the main class compared to real images of the confusing class.
%     \end{itemize}
% \end{enumerate}

% The exact algorithm is as follows:

% \begin{enumerate}
%     \item \textbf{Feature Extraction:} We use GPT-4 to extract features from each of the selected images of the main class. Each image yields a list of features.

%     \item \textbf{Feature Consolidation:} We combine the features from all images into a single list, removing duplicates to create a unified set of candidate features.

%     \item \textbf{Contrastiveness Scoring:} For each feature, we compute a contrastiveness score using CLIP embeddings to measure how much more likely the feature is associated with the main class than with the confusing class. The score is calculated as:

% \[
% \text{Score}_{\text{contrastiveness}} = 
% \frac{\text{CLIP}(\text{feature},\, \text{main class images})}
%      {\substack{\text{CLIP}(\text{feature},\, \text{main class images}) + \\
%       \text{CLIP}(\text{feature},\, \text{confusing class images})}}
% \]

%     where $\text{CLIP}(\text{feature}, \text{images})$ represents the cosine similarity between the textual description of the feature and the image embeddings.

%     \item \textbf{Contrastiveness Filtering:} We filter out features with a contrastiveness score less than 0.6. This threshold ensures that the selected features are more strongly associated with the main class than with the confusing class (a score below 0.5 would indicate the feature is more associated with the confusing class).

%     \item \textbf{Generability Scoring:} For the remaining features, we assess their generability by generating synthetic images of the main class conditioned on each feature. We then compute the CLIP similarity between the feature and these synthetic images, as well as between the feature and real images of the confusing class. The generability score is calculated as:
% \[
% \text{Score}_{\text{generability}} = 
% \frac{\text{CLIP}(\text{feature},\, \text{synthetic main class images})}
%      {\substack{\text{CLIP}(\text{feature},\, \text{synthetic main class images}) + \\
%       \text{CLIP}(\text{feature},\, \text{confusing class images})}}
% \]


%     \item \textbf{Generability Ranking:} We rank the features based on their generability scores. This step identifies features that are not only distinctive but also can be effectively rendered in synthetic images.

%     \item \textbf{Feature Selection:} We select the top features based on the generability ranking (we choose the top 5 features) for use in synthetic image generation during fine-tuning. This ensures that the features are both distinctive and generable.

% \end{enumerate}

% By applying this feature filtering method, we increase the quality of automatically selected features. This enhances the model's ability to distinguish between confusing pairs by focusing on the most relevant and generable features.


% \subsubsection{Human Eval}
% \TODO{bryan}

\subsection{Image Generation and Verification}

% \vspace{-1em}
\paragraph{Image Generation}

After feature extraction and filtering based on Discriminability and Generability, we pass the selected features to a text-to-image generative model to generate augmented visual data. We experiment with both SOTA open-weights~\citep{esser2024scaling, stablediffusion3.5} and proprietary~\cite{2024RecraftV3} models.

% Stable Diffusion 3.5 Large Turbo model
\vspace{-0.5em}
\paragraph{Verification} To ensure final images for augmentation contain our extracted and filtered target concept features, we propose a simple automatic verification metric that checks whether desired features are recognized in the augmented images by the LMM we want to update: Given the vanilla LMM $\mathcal{M}$, a set of features $\mathcal{F}$, and %the set of 
an augmented images $i^{\text{synthetic}}$, 
the feature satisfaction rate $S(i^\text{synthetic}, F, M)$ for each augmented image:

\vspace{-0.7em}
\[
S(i^\text{synthetic}, \mathcal{F}, \mathcal{M}) = \frac{\sum_{f \in \mathcal{F}} \mathbf{1}\{ \mathcal{M}(f, i^\text{synthetic}) \}}{|\mathcal{F}|}
\]
\vspace{-0.7em}


Here $\mathcal{M}(f, i^\text{synthetic}) \}$ returns true if the feature $f$ is recognized in the image $i^\text{synthetic}$. Afterwards, we filter out all images with $S(i^\text{synthetic}, F, M)$ \textless 1.0, keeping only augmented images that fully match all target concept features.



% \violet{this will likely raise concern that the data augmentation method is "model dependent" because for different model, we need to build the verification differently. Probably worth discussing and be prepared to add results for some "model agnostic" verification.}







% The score evaluates each target feature as a constraint and calculates a satisfaction rate ($R_{\text{satisfied}}$) for each image by prompting LLaVA to detect and verify each specified feature



% implemented an automated verification pipeline with LLaVA. The pipeline evaluates each target feature as a constraint and calculates a satisfaction rate ($R_{\text{satisfied}}$) for each image by prompting LLaVA to detect and verify each specified feature:
% \[
% R_{\text{satisfied}} = \frac{\text{Number of Satisfied features}}{\text{Total number of features}}
% \]


% After feature extraction, we generate synthetic images based on these features through a two-stage pipeline: generation and verification.

% \paragraph{Generation} We construct text prompts using the extracted features and generate synthetic images using Stable Diffusion 3.5 Large \citep{esser2024scaling, githubGitHubStabilityAIsd35}.The prompt templates are attached in the Appendix .

% \paragraph{Verification} To ensure generated images contain our extracted and filtered target concept features, we implemented an automated verification pipeline with LLaVA. The pipeline evaluates each target feature as a constraint and calculates a satisfaction rate ($R_{\text{satisfied}}$) for each image by prompting LLaVA to detect and verify each specified feature:
% \[
% R_{\text{satisfied}} = \frac{\text{Number of Satisfied features}}{\text{Total number of features}}
% \]
% We filter out images with $R_{\text{satisfied}}$ \textless 1.0, keeping only those that fully match all target features.






\subsection{Human Evaluation} 
\label{sec:human_eval}

\input{tables/human_eval}

To verify the reliability of our feature filtering and augmented image verification modules, we conduct human evaluation on a subset of iNaturalist and the novel animal species dataset. For target concepts, we select 100 image-feature pairs for both real and augmented synthetic images. We also select 100 image-feature pairs for real images of misidentified concepts. 3 external human annotators are asked to label whether they believe the given feature belongs to the concept in the corresponding image.


Results in Tab.\ref{tab:human_eval} show human annotators overwhelmingly agree that the final extracted features belong to the target concept (92\%) but not the misidentified concept (14\%). The augmented synthetic images of the target concept also likely contains the desired features (83\%), though as expected, there is some information loss between the text-to-image generation step. In addition, the three independent annotators generally agreed in their response (\textgreater 0.8 IAA).



\subsection{In-Context Inference for Enhanced Recognition}
In addition to updating the LMM with augmented data, we can further boost performance by integrating the extracted features into the inference prompt. For each query, we can append a concise list of the most discriminative and generable features of the target and confusable classes. These features serve as an in-context guide, focusing the LMMâ€™s attention on critical distinguishing attributes. By explicitly highlighting what to look for (and what not to mistake it for), the model more reliably identifies the correct concept.



% We conducted a human evaluation study to assess the reliability of our verification step. We recruited 3 external volunteer to compare LLaVA's automated feature detection results with human judgments of whether these features were present in the generated images. 






% \subsection{Model Updating and Inference}

% We selected LLaVA-v1.6-34b \citep{liu2024llavanext, liu2023improvedllava, liu2023llava} as our experimental framework to evaluate knowledge updating using augmented visual data. The model was fine-tuned using LoRA on a subset of the training dataset. During inference, we enhanced the model's classification capabilities by incorporating extracted features into the prompt structure. Specifically, we prepended text features from both the main class and potential confounding classes as a prefix to the input prompt. The complete prompt template is provided in Appendix \ref{}.
% We conducted experiments under both high-resource and low-resource settings:

% \paragraph{High-Resource Setting}
% In this configuration, we randomly selected 20 images per class across 20 class pairs, yielding a training set of 800 images. The model was fine-tuned using LoRA for 30 epochs. After merging the trained weights with the original model, we evaluated performance on a held-out validation set containing 20 images per class.

% \paragraph{Low-Resource Setting}
% To test our method's efficiency with limited data, we reduced the training samples to 5 images per class while maintaining the same number of class pairs (20 pairs). The training protocol remained consistent with the high-resource setting, including the 30-epoch duration and LoRA fine-tuning. The evaluation procedure on the validation set was also kept identical.

