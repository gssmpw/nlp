\section{Introduction}
\label{sec:intro}
\input{figs/teaser}

% Existing data augmentation strategies to update these LMMs require large amounts of new data while yielding limited gains

% These issues severely hinder current LMMs' long-term usability, contributing to the need for frequent and costly model replacements.


% However, one remaining challenging is teaching LMMs how to recognize novel concepts



% LMMs' inability to recognize novel and confusing concepts not taught well during training.





% This is an increasingly important research question as current state-of-the-art open- and closed-source models are all limited by fixed knowledge cut-offs. 



% Recent advancements in large scale multimodal pre-training~\cite{2023GPT4VisionSC, team2023gemini, hurst2024gpt4o} and visual instruction tuning~\cite{liu2023llava, liu2023improvedllava, liu2024llavanext} have enabled dramatic progress in LMMs' visual commonsense reasoning~\cite{yue2023mmmu}, scientific question answering~\cite{lu2022learn, lu2024mathvista}, and visual instruction following~\cite{liu2023llava}. However, one remaining problem is: how can we efficiently and effectively update such models with new concept knowledge or fix domain-specific gaps in existing knowledge after the LMM has been trained? 

% This is an increasingly important research question as current state-of-the-art open- and closed-source models are all limited by fixed knowledge cut-offs. As shown by examples in Figure.\ref{fig:teaser}, the top-performing proprietary and open-source models fail to recognize novel concepts such as the newly discovered animal specie ``Clouded Tiger Cat (L. pardinoides)'' and confusing or low-visual-resource concepts such as ``Army Resupply Base''. These issues severely hinder current LMMs' long-term usability, contributing to the need for frequent and costly model replacements.



Recent advancements in multimodal pre-training~\cite{2023GPT4VisionSC, team2023gemini, hurst2024gpt4o} and visual instruction tuning~\cite{liu2023llava, liu2023improvedllava, liu2024llavanext} have enabled impressive LMM abilities. However, as shown in Fig.\ref{fig:teaser}, it still remains a challenge for current state-of-the-art proprietary and open-source models to robustly recognize novel visual concepts (e.g.``Clouded Tiger Cat'' Fig.\ref{fig:teaser}a) and confusing / low-resource / commonly misrepresented visual concepts (e.g. ``Resupply Base'' Fig.\ref{fig:teaser}b).


In order to help models better acquire new visual concepts and distinguish confusable concepts, 
existing approaches straightforwardly 1). Fine-tune text decoder on new textual corpora to expand the concept base; and 2). Fine-tune both vision and text components on new web image-text pairs for visual concept acquisition.
These approaches are ineffective due to scarcity of data for certain concepts and inefficiency caused by not knowing what precisely confused the models~(\ref{subsec:feature_extraction}).
As depicted in Fig.\ref{fig:teaser}, it can be difficult to obtain ample high-quality real images for novel concepts such as new animal species.
While for confusing concepts, the problem usually lies with biased concept representation in web image-text data. For example: online images of ``Resupply Base'' mostly only consist of exterior views of the architecture without the interior details, which may cause the models to confuse it with a ``Wholesale Store'' that shares some interior features. %of the concept but provides almost no instances of interior images.

% In addition, models that have only seen domain-specific concepts in its textual training corpora often fail to recognize visual instances of the concept~(\ref{subsec:feature_extraction}).

%Based on these weaknesses, 



% \violet{I think this sentence is both over-specified and vague. For the "zero-shot inference" part, I don't think this detail is important. As long as we can identify "confusable concept" -- we can even introduce this terminology here and explain a little bit -- whether we identify them through zero-shot inference is not as important. The "misidentified concept" part is too vague because it's unclear that's "misidentified concept", why do you need them, and how will that act as a basis. I suggest we introduce the concept of "confusable concept", or whatever terminology you like, explain it briefly, and say from there we can extract contrastive features for data synthesis.} 
% - Done!

% \violet{here you didn't mention "contrastive". Do you only extract this based on target concept? Your later description seems to indicate you extract the features contrastively.}
% - Done!

% \violet{what's this? model will be able to generate images based on the feature? If yes, say it in that way (polish it abit of course, but be direct)}
% - Done!

To help LMMs recognize and reason about novel and confusing concepts more robustly and efficiently, we propose \textbf{CoDA}, a \textbf{Co}ntrastive Visual \textbf{D}ata \textbf{A}ugmentation technique. For each target concept, \textbf{CoDA} first identifies a ``confusable concept'' that the LMM finds most similar to the target. Then, it extracts contrastive textual and visual features of the target concept with respect to the confusable concept. The extracted features go through a filtering process based on discriminability and generability to make sure that: 1). The features are possessed by the target concept but not the confusable concept; and 2). The feature can be reliably generated by the text-to-image generative model and recognized by the LMM. Afterwards, the features are passed to the text-to-image generative model to produce augmented visual instances of the target concept. To make sure that the features are indeed generated and recognizable by the LMM, \textbf{CoDA} again uses the LMM's zero-shot inference to rank and filter the augmented images. Finally, the resulting augmented images can be used to update the LMM via low-rank adaptation, basic fine-tuning, in-context learning, or any other method of choice.

In addition to evaluating on existing datasets INaturalist and SUN, we create \textbf{NovelSpecies}, an annotated image dataset of newly discovered animal species in recent years. \textbf{NovelSpecies} allows the simple selection of species discovered after any model's latest knowledge cutoff date, ensuring the selected species were never seen by the model. Therefore, \textbf{NovelSpecies} is the perfect testbed for methods aimed at improving LMMs' novel concept recognition ability.

Comprehensive experiments with LLaVA-NeXT on the 3 datasets show \textbf{CoDA} performs surprisingly well in teaching LMMs novel and confusing concepts, significantly improving data and compute efficiency compared to existing methods. In additional experiments, we show that \textbf{CoDA} is also able to improve novel concept recognition for traditional classifiers like ViT and proprietary LMMs such as GPT4o-mini. Finally, ablation experiments show that \textbf{CoDA} can be significantly improved by simply replacing its off-the-shelf components such as the text-to-image generation model with superior versions of similar models.



% [topsep=0pt, itemsep=0em]

% [topsep=0pt,noitemsep,leftmargin=10pt]

% [topsep=0pt, itemsep=0.5em]

Our key contributions include:
\begin{itemize}[topsep=0pt, itemsep=0em, leftmargin=10pt]
    \item \textbf{CoDA}, a simple plug-and-play contrastive visual data augmentation method that can be used to effectively and efficiently improve LMMs' ability to recognize novel and confusing concepts. \textbf{CoDA} is also the first widely successful method using text-to-image generation for visual data augmentation. 
    \item \textbf{NovelSpecies}, a new benchmark dataset of novel animal species discovered in recent years, providing an ideal benchmark for novel concept recognition. \textbf{NovelSpecies} currently consisting of 2240 annotated images and will continue to be updated with future discoveries.
    
    % \item Comprehensive experiments on 3 datasets show \textbf{CoDA} performs surprisingly well in teaching LMMs novel and confusing concepts, significantly improving data and compute efficiency compared to existing baselines.
    % \item Detailed ablations demonstrate the effectiveness of our contrastive feature extraction and filtering methods. Additional experiments show \textbf{CoDA} also helps in improving traditional classifiers and proprietary LMMs.
\end{itemize} 