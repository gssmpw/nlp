\section{NovelSpecies Dataset}
\label{sec:novel_dataset}

Proprietary LMMs like GPT4o~\cite{hurst2024gpt4o} and Gemini~\cite{team2023gemini} are trained on vast online text-image data and proprietary data, both non-public and impossible to inspect. Some open-source and open-data LMMs such as LLaVA~\cite{liu2024improved, liu2024visual} are trained on publicly available image-text datasets. However, the text encoders used by such models are often not open-data, for example LLaVA-1.6 34B uses the closed-data Yi-34B model as its language backbone. Even in the rare cases where both image-text training data and text encoder training data are publicly available, it is still difficult to ascertain whether concepts in your benchmark were seen by your LMM through indirect data leakage (i.e. partial / paraphrased mentions). Due to the above issues, it is difficult to evaluate true novel concept recognition ability with existing datasets. 
% \footnote{Knowledge cutoff date: Dec 2023}

One way to bypass this problem with 100\% guaranteed success is to use a dataset that only contains concepts created / discovered after the LMM's knowledge cutoff, i.e. the latest knowledge cutoff date among all of its textual / visual sub-components. Based on this idea, we curate \textbf{NovelSpecies}, a dataset of novel animal species discovered in each recent year, starting with 2023 and 2024. We provide detailed information for each species, including time of discovery, latin name, common name, family category, textual description, and more. Data will be released upon publication.
% Details are described in Sec.\ref{subsec:NovelSpecies_details}.

To create \textbf{NovelSpecies}, we start by collecting the list of species first described in each year by Wikidata~\cite{wikidata}. Then, to make sure we can curate a visual benchmark of novel species, we manually annotate and filter out extinct species and species with too few publicly available images. After filtering, we end up with a dataset of 64 new species, each consisting of 35 human-verified image instances, thus a total of 2240 images. The images are split into training, validation, and test sets. For each specie, there are 5 training images, 15 validation images, and 15 test images. This data split is consistent with our goal of creating a benchmark dataset for novel concept recognition, where the maximum number of training instances for a completely unseen concept can range from 1 to 5.







% and 2170 images in total, which consist of train, validation, and test sets of equal proportion for all species. Finally, all the images are 















% \section{Datasets}
% \label{sec:dataset}


% \subsection{Confusing Pair Extraction}
% Our focus on confusing pairs arises from the need to strengthen the model's performance in distinguishing between visually similar species—a challenge where LLaVA currently shows limitations. Confusing pairs represent instances where the model's classification often fails, typically due to subtle visual cues or shared features among species within similar taxonomic groups. We designed strategy to extract confusing pair for each dataset.

% \paragraph{INaturalist and Novel Species Dataset} We extract confusing pairs with three-steps as following: 

% \begin{enumerate}
%     \item \textbf{Iterative Subset Selection:} We select a random subset of species in each iteration, sampling across different supercategories. This strategy allows us to identify confusing pairs without overloading the system, progressively building a collection of challenging cases from each subset.
%     \item \textbf{Evaluate Classification Patters:} For each species within a subset, we create prompts in a multiple-choice format, incorporating the image and a randomized list of options from all the species in the subset. Based on the response from LLaVA, we are able to highlight specific species that are commonly mistaken for one another, guiding us in selecting pairs for further analysis. The process is repeated across new subsets, incrementally building an ample dataset of confusing pairs.
%     \item \textbf{Identification of confusing pairs: } We choose a threshhold of 0.2. If class A is misclassified into class B with frequency more than 0.2 in the above multiple-choice setting, we consider the pair to be confusing. 
% \end{enumerate}

% \paragraph{SUN Dataset} We adapted the above methodology for scene classification with minor modification on the subset selection process. Instead of taxonomic groupings, we created subsets by selecting a target scene and the nine most similar scenes based on shared object occurrence. The subsequent steps—classification pattern analysis and confusing pair definition—remained consistent with the species datasets.









% \subsection{Curated INaturalist Dataset}
% In this study, we utilize a random sample of 15 classes from the "Mammals" supercategory of the iNaturalist dataset. Below, we outline the reasoning behind our dataset selection and sampling approach.
% \paragraph{iNaturalist Dataset}
% The iNaturalist dataset is known for its complexity and has proven to be a challenging benchmark for many vision-language models. Due to the extensive diversity and fine-grained nature of the categories, most models do not achieve perfect performance on this dataset, leaving ample room for further improvements.
% \paragraph{Sampling Strategy}
% Given the scale of the iNaturalist dataset, which contains approximately 10,000 classes with 50 images per class, it is necessary to reduce its size for practical purposes. Additionally, current models, such as LLaVA, have limitations in handling an excessive number of options. Therefore, we have opted to sample the dataset to manage the number of classes and reduce the computational load.
% \paragraph{Random Sampling Justification}
% Initially, we considered sampling all species from a single order, family, or genus. However, this approach resulted in classes that were too similar, making the classification task more challenging than our models could handle. By employing random sampling, the selected classes that are likely sufficiently distinct from each other, with only a few potentially confusing cases.

% Random sampling also reduces the risk of introducing human bias into the selection process, making it a more defensible approach compared to sampling based on performance metrics. 

% \subparagraph{Data Filtering}
% iNaturalist dataset contains a large number of noisy or low quality images. To ensure the quality of the dataset, we implemented an automatic filtering process to eliminate low-quality images. This step is crucial to prevent noise from negatively impacting model performance. Common issues in low-quality images include:

% 1. \textbf{Blurriness}: Images where the main subject is not in focus.
% 2. \textbf{Species Not Present}: Instances where the species is not visible (e.g., only showing its nest or footprint).
% 3. \textbf{Incomplete Specimen}: Images depicting only parts of deceased animals or broken bodies.
% 4. \textbf{Obstructions}: Cases where the species is almost entirely blocked by objects, making identification impossible.

% To improve image quality, we use CLIP score to select the images with top scores. Scores are calculated by evaluating similarity score with [
%         "a photo of an animal",
%         f"a photo of a \{common\_name\}"
%     ]. We rank the images according to this score and selected top 100 images. We randomly split the images to obtain 50 images for training, 20 images for validation and 30 images for testing. 


