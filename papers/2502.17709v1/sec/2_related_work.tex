\section{Related Works}
\label{sec:related_work}

Few-shot image recognition is a long-standing problem in the vision community. Early works in this area focused on improving traditional image classifiers on classifying existing concepts~\cite{vinyals2016matching, finn2017model, nichol2018first, dhillon2019baseline, tian2020rethinking, bhagat2023sample, afrasiyabi2022matching}. On the other hand, while recent advancements in the training of vision language models (VLMs) and large multimodal models (LMMs)~\cite{2023GPT4VisionSC, team2023gemini, hurst2024gpt4o, liu2023llava, liu2023improvedllava, liu2024llavanext} have shown great promise and extensibility, they still severely lag behind traditional models in image classification, especially for low-resource, novel, and confusing concepts~\cite{zhang2024visually, cooper2024rethinking}. 


While commonly used text-side VLM data augmentation strategies~\cite{yuksekgonul2022and, yang2023alip, liu2024synthvlm, sharifzadeh2024synth} have little effect on this issue, a more promising technique to solve this is through visual data augmentation. This includes basic visual manipulations such as cropping, flipping, and rotation~\cite{yang2022image, kumar2024image}; and more advanced model-based augmentation such as style transfer~\cite{zheng2019stada, chun2021styleaugment} and image mixing~\cite{uddin2020saliencymix,xie2021cut,hao2023mixgen}. More recently, with the rise of controllable and promptable visual generative models, knowledge and feature editing-based augmentation methods~\cite{liu2022learning,wu2023towards,jin2024armada} have gained in popularity. Such methods generally focus on using multimodal data and general knowledge bases to guide image-editing models in creating augmented visual data based on existing images.


One main issue with current methods is that the augmented images they produce must be closely based on existing real images, which makes them unhelpful for novel concepts where real images are extremely rare, and mis-represented concepts where existing real images do not accurately depict the concept. Additionally, due to their close connection to existing images, such augmented images usually lack visual frame structure and view variation. In contrast, our method \textbf{CoDA} can extract accurate and meaningful features from extremely limited multimodal data, and use text-to-image generative models to produce diverse high-quality augmented data for LMM updating. 


% While existing attempts at using synthetic images generated using text-to-image generated models for general LMM training and fine-tuning have yielded mixed results~\cite{he2022synthetic, anderson2022synthetic, azizi2023synthetic, yu2023diversify, li2024synthetic, tian2024stablerep, geng2024unmet}, our work sheds light on another potential path forward.

% Our work also sheds light on another potential path forward for using text-to-image generated models for LMM training and fine-tuning~\cite{he2022synthetic, anderson2022synthetic, azizi2023synthetic, yu2023diversify, li2024synthetic, tian2024stablerep, geng2024unmet}.






% To solve this issue, our method \textbf{CoDA} expands on existing feature-based augmentation approaches to directly extract textual, visual, and contrastive features from the target concept. Then, we use text-to-image generative models to produce diverse and high-quality augmented data for LMM updating. While existing attempts at using synthetic images generated using text-to-image generated models for LMM training and fine-tuning have yielded mixed results~\cite{he2022synthetic, anderson2022synthetic, azizi2023synthetic, yu2023diversify, li2024synthetic, tian2024stablerep, geng2024unmet}, our work sheds light on another potential path forward.  While existing attempts at using synthetic images generated using text-to-image generated models for LMM training and fine-tuning have yielded mixed results~\cite{he2022synthetic, anderson2022synthetic, azizi2023synthetic, yu2023diversify, li2024synthetic, tian2024stablerep, geng2024unmet}, our work sheds light on another potential path forward.







% Another novel approach is proposed by ARMADA~\cite{jin2024armada}, which extracts features of the target concept from knowledge bases such as WikiData, and then uses image editing models to generate augmented images. 





% \paragraph{Visual Data Augmentation}


% \paragraph{Novel Concept Classification}


% \paragraph{Use of Synthetic Data}
