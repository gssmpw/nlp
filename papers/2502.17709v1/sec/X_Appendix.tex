\newpage
\appendix
\onecolumn

\input{prompt/code_style}


% List prompts, training details, human eval details, etc.


\section{Appendix}
\label{sec:appendix}


\subsection{Qualitative Comparison}
\label{subsec:qualitative_comparison}
\input{figs/qualitative}
Figure \ref{fig:qualitative} illustrates the qualitative comparison of the quality of generated images using \textbf{CoDA} and other approaches.

The Crop method, while useful for localized feature emphasis, often results in the loss of crucial visual details necessary for species identification. For instance, in the Phyllobates Samperi images, cropping removes the black spots on the frog’s skin, which are an essential distinguishing feature. Without these patterns, the cropped images lack key identity cues, potentially leading to misclassification. Similarly, in the Tail-Spot Wrasse images, cropping reduces visibility of the distinct horizontal striping pattern along the fish’s body, making it difficult to recognize key species attributes.

ARMADA is capable of retaining some structural features, but it struggles with precise reproduction due to the limited image editing capabilities. This limitation is particularly evident in its generated images, where critical patterns such as the Phyllobates Samperi's orange stripes are missing. The generated frog appears to have a distorted pattern, failing to fully capture the contrast between black skin and bright orange lines, which are key species identifiers. Similarly, in the case of the Tail-Spot Wrasse, the image generated by ARMADA loses the feature of its vibrant horizontal stripes, leading to a visually inconsistent and less biologically accurate representation.

In contrast, \textbf{CoDA} successfully captures all species-specific features by leveraging contrastive textual and visual attributes through different image generation models. \textbf{CoDA} (SD-3.5) produces a high-fidelity image of Phyllobates Samperi, accurately preserving the orange stripes, dark skin, and black spots. However, slight variations in texture suggest that this model, while effective, may not fully match the real-world skin reflectivity of the species. Meanwhile, \textbf{CoDA} (Recraft V3) generates an even more realistic image, successfully capturing the frog’s signature features with improved color richness and anatomical precision, making it nearly indistinguishable from real-world references.

Generally, \textbf{CoDA} (SD-3.5) and \textbf{CoDA} (Recraft V3) both perform significantly better than previous methods. Take Tail-Spot Wrasse as another example, the horizontal stripes, which were previously distorted or missing in previous methods, are now clearly visible. \textbf{CoDA} (Recraft V3), in particular, produces a more vivid and structurally accurate representation, ensuring the preservation of both color gradients and fin structure.

Note that the quality of CoDA-generated images is inherently dependent on the capability of the underlying image generation model, meaning that limitations in the base model, such as resolution constraints or texture inaccuracies, may impact the fidelity of the final augmented data.


\subsection{Data Selection Strategy}
\label{subsec:data_selection_strategy}
For each dataset, we focus on a randomly selected subset of concepts that the model is unable to recognize. The data selection strategy is as follows: In each iteration, we select a random subset of 15 species across different supercategories, including "Birds," "Mammals," and "Reptiles." This strategy allows us to identify confusing pairs without overloading the system, progressively building a collection of challenging cases from each subset. For each species within a subset, we create prompts in a multiple-choice format, incorporating the image and a randomized list of options from all species in the subset. Based on the response from the LMM, we are able to highlight specific species that are commonly mistaken for each other, guiding us in selecting pairs for further analysis. In particular, misclassification happens when an image of one species is identified by the LLM to be an image of another species. A pair \((A, B)\) is considered as a confusing pair if rate of misclassification on either direction is above the threshold 0.2. The process is repeated across new subsets, incrementally building an ample dataset of concepts the model has difficulty recognizing.


% \subsection{NovelSpecies Dataset Details}
% \label{subsec:NovelSpecies_details}
% Since there are relatively few new species, we adopt a slightly different approach. Each pair must contain exactly one new species. For every new species, we randomly sample 14 others within the same supercategory and track the misclassification rate. We then identify the species most frequently confused with the new one to form a pair.

\subsection{Experiment Details}

\subsubsection{Feature Extraction}
For textual feature extraction, we use GPT-4o-mini with chain-of-thought reasoning, running with OpenAI API calls. Each API call processes up to 2048 tokens, costing approximately $0.0025$ per 1K input tokens and $0.005$ per 1K output tokens. Given an average of 500 tokens per query and 10 queries per concept, the estimated cost per concept is around $\$0.0375$.

For visual feature extraction, we utilize GPT-4o-mini running with OpenAI API calls. Images are preprocessed to a resolution of 336x336 pixels and normalized before feature embedding extraction. Each image query incurs a cost similar to textual feature extraction. With an estimated 5 images processed per concept, the cost per concept amounts to approximately $0.1875$. 

With the rapid advancement of open-weights large language models and vision language models including DeepSeekV3~\cite{liu2024deepseek}, DeepSeekVL2~\cite{wu2024deepseekvl2}, Llama 3.2~\cite{dubey2024llama}, and more; we expect that feature extraction LLMs and VLMs can be replaced with these models with none or minimal impact to performance. We plan to perform experiments on some of these models and provide comparison results in the next updated version of our work.

\subsubsection{Feature Filtering}
We employ CLIP for automatic feature filtering, evaluating Discriminability and Generability scores. Discriminability is computed using cosine similarity between feature embeddings of target and misidentified concepts, with a threshold of 0.6. Generability is assessed by comparing feature presence in synthetic images using an ensemble of Stable Diffusion 3.5 Large and RecraftV3 models. The feature selection step is executed on an NVIDIA A100 GPU, processing features in approximately 2 hours. Top 5 ranked features are selected per concept.

\subsubsection{Image Generation and Verification}
For synthetic image generation, we employ Stable Diffusion 3.5 Large, running on a single A100 GPU. Additionally, we also integrate the RecraftV3 model through an API call. Image generation is performed at a resolution of 512x512 pixels with a guidance scale of 7.5. The pipeline generates 50 images per concept in approximately 1.2 seconds per image.

Post-generation, we perform automated verification using LLaVA V1.6-34b, running on an A6000 GPU. Each image would takes approximately 1 minutes to run for feature presence using a feature-matching confidence threshold of 0.85. Images with a satisfaction rate $S(i^{\text{synthetic}}, \mathcal{F}, \mathcal{M}) < 1.0$ are discarded.

\subsubsection{Model Updating}

We train V1.6-34b with supervised fine-tuning (SFT) using LoRA with rank 128 and alpha 256, optimizing memory efficiency while maintaining model expressiveness. The training runs on two NVIDIA A6000 GPUs, leveraging DeepSpeed Zero-3 for distributed optimization and mixed precision (bf16) for efficiency. The vision encoder is CLIP-ViT-Large-Patch14-336, with an MLP projector aligning visual and text features. We use a cosine learning rate scheduler with a 3\% warmup ratio, training for 30 epochs with a batch size of 5 and a learning rate of 2e-4. Images are padded for aspect ratio consistency, and gradient checkpointing is enabled to reduce memory usage. Checkpoints are saved every 50,000 steps, retaining only the most recent one.

\subsubsection{Evaluation}

Automatic evaluation measures zero-shot classification accuracy on a held-out test set. Inference runs on a single A6000 GPU with a batch size of 20, taking approximately 1 hour to complete. The prompt templates for evaluation are attached to Appendix \ref{app:prompt}


\subsection{Prompt Construction}
\label{app:prompt}


\lstinputlisting[language=Octave]{prompt/all_prompts.py}



% \subsubsection{Prompt for Visual/Text Feature Extractions}

% \paragraph{Contrastive Visual}
% \begin{verbatim}
% You are an experienced and meticulously observant biological scientist who is 
% asked to carefully assess the provided image. As labelled in the image, the 
% left half of the image contains a picture of the animal {main_class} and the 
% right half contains a picture of the animal {confusing_class}. Now, your task 
% is summarize the key distinctive visual attributes possessed by {main_class} 
% (on the left of the image) that makes uniquely discernible from the 
% {confusing_class} (on the right half of the image). Reason step by step to 
% produce an answer. Finally, output the key visual attributes of a {main_class} 
% (that make it distinct from a {confusing_class}) in a Python list format 
% containing short phrases of less than 8 words each. Do not output any features 
% of the {confusing_class} in your Python list. Make sure not to name the 
% {main_class} or the {confusing_class} in any of the attributes in your list. 
% Also, please try not to use negation in the visual attributes you generate: 
% for example, change features like "lack of facial markings" to "plain brown 
% face". Additionally, do not use comparative form in any of the features you 
% output, for example, change features like "thinner body than the other class" 
% to "thin body".
% \end{verbatim}

% \paragraph{Visual}
% \begin{verbatim}
% You are an experienced and meticulously observant biological scientist who is 
% asked to carefully assess the provided image. The image contains a picture of 
% the animal {main_class}. Now, your task is summarize the key distinctive visual 
% attributes possessed by {main_class}. Reason step by step to produce an answer. 
% Finally, output the key visual attributes of a {main_class} in a Python list 
% format containing short phrases of less than 8 words each. Make sure not to 
% name the {main_class} in any of the attributes in your list. Also, please try 
% not to use negation in the visual attributes you generate: for example, change 
% features like "lack of facial markings" to "plain brown face". Additionally, 
% do not use comparative form in any of the features you output, for example, 
% change features like "thinner body than the other class" to "thin body".
% \end{verbatim}

% \paragraph{Contrastive Text}
% \begin{verbatim}
% You are an experienced and knowledgeable scene classification specialist who 
% is tasked to summarize the key distinctive visual attributes possessed by 
% {main_class} that makes uniquely discernible from the {confusing_class} (just 
% based on a visual image). First retrieve your knowledge about the two 
% different types of scenes, then reason step by step to produce an answer. 
% Finally, output the key visual attributes of a {main_class} (distinct from 
% a {confusing_class}) in a Python list format containing short phrases of 
% less than 8 words each. Do not output any features of the {confusing_class} 
% in your Python list. Make sure not to name the {main_class} or the 
% {confusing_class} in any of the attributes in your list. Also, please try 
% not to use negation in the visual attributes you generate: for example, 
% instead of saying "no bright lights," use "dark environment." Additionally, 
% do not use comparative forms in any of the features you provide. For instance, 
% instead of saying "smaller windows than the other place," use "small windows."
% \end{verbatim}

% \paragraph{Text}
% \begin{verbatim}
% You are an experienced and knowledgeable scene classification specialist who 
% is tasked to summarize the key distinctive visual attributes possessed by 
% {main_class}. First retrieve your knowledge about the {main_class}, then reason 
% step by step to produce an answer. Finally, output the key visual attributes of 
% a {main_class} in a Python list format containing short strings of less than 
% 8 words each. Make sure not to name the {main_class} in any of the attributes 
% in your list. Do not output any features of the {confusing_class} in your 
% Python list. Also, please try not to use negation in the visual attributes you 
% generate: for example, instead of saying "no bright lights," use "dark 
% environment." Additionally, do not use comparative forms in any of the features 
% you provide. For instance, instead of saying "smaller windows than the other 
% place," use "small windows."
% \end{verbatim}

% \subsubsection{Text to Image Generation Prompt}
% \begin{verbatim}
% f"Generate a 4K realistic image of {main_class} that contains the following 
% attributes: {', '.join(attributes)}"
% \end{verbatim}

% \subsubsection{Feature Verification Prompt}
% \begin{verbatim}
% You are an image verification specialist. Your task is to meticulously assess 
% the image for specific attributes and confirm their presence. For each 
% attribute in the list, carefully check the image, examine visual elements 
% such as color, shape, texture, position, and context clues that might indicate 
% whether the attribute is present. Provide a binary Python output list, where 
% each element is either 1 (attribute is present) or 0 (attribute is absent), 
% corresponding exactly to the order of attributes provided.

% Attributes to Verify: {attributes}

% Expected Output: A list of 0s and 1s indicating the presence or absence of 
% each attribute, in the same order as listed. Here is an example output: [0, 1, 1].
% \end{verbatim}

% \section{Finetune and Evaluation Prompt}
% \begin{verbatim}
% "You are an image classification specialist with expertise in categorizing 
% images into specific groups. Given an image, identify its category from the 
% following options: " + ", ".join(provided_options_capitalized[:-1]) + ", or " 
% + provided_options_capitalized[-1] + ". Provide your answer as only one 
% category name for precise classification. Please response with the category 
% name only."
% \end{verbatim}

% \section{Deduplication Prompt}
% \begin{verbatim}
% You are an experienced and knowledgeable biological scientist who is tasked 
% to summarize the key distinctive visual attributes possessed by {main_class} 
% into a coherent list. Given the following list of attributes describing the 
% animal species {main_class}: {attributes_list}. You task is to combine the 
% duplicate features (which have the same or very similar meanings) into one. 
% Then, you will order the remaining features in order of visual importance, 
% the most visually significant / observable features will be at the front of 
% the list while the least visually observable features will be at the back. 
% Finally, output the key visual attributes of a {main_class} in a Python list 
% format containing short phrases of less than 8 words each. Make sure not to 
% name the {main_class} in any of the attributes in your list. Also, please try 
% not to use negation in the visual attributes you generate: for example, change 
% features like "lack of facial markings" to "plain brown face". Additionally, 
% do not use comparative form in any of the features you output, for example, 
% change features like "thinner body than the other class" to "thin body".
% \end{verbatim}

% \section{System Prompt}
% \begin{verbatim}
% "You are a helpful assistant."
% \end{verbatim}
