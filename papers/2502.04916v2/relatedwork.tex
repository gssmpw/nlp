\section{Related Work}
~\label{sec:related}
%In this section, we position our work against the relevant work in the RE literature. 
Requirements traceability (RT) has been extensively studied in RE~\cite{mucha2024systematic,li2023applications,tufail2017systematic,wang2018requirements,ramesh1998factors}. 
% Existing work applies different technologies borrowed from information retrieval (IR)~\cite{gao2022using, guo2017tackling, kuang2017analyzing, nishikawa2015recovering, panichella2013and, Mahmoud2013, Wang20193, sun2017frlink, capobianco2013improving, chhabra2017filtering, bavota2014enhancing, shao2013improved, Wang20191, Li2020,moran2020improving}, machine learning (ML)~\cite{Rasiman2022,Bella2019,mills2019tracing,Bella2018,Mills2018,Rath2018,Zhao2018,Mills2017,Mills20172,Falessi2017,Hayes2015,le2015rclinker,li2015recovering} and deep learning (DL)~\cite{kenton2019bert,Zhang2021,Wang20192,Guo:17,Chen2019,Alhoshan2019,Alhoshan2018,Li2018,zhao2020extended,Sultanov2013}. 
Existing work applies different technologies, ranging from traditional methods such as Information Retrieval (IR) and statistical models to more advanced approaches like Machine Learning (ML), Deep Learning (DL). Early works borrowed IR techniques such as Vector Space Models (VSM), Latent Dirichlet Allocation (LDA), etc. in order to find trace links between software artifacts via text relevancy~\cite{gao2022using, guo2017tackling, kuang2017analyzing, nishikawa2015recovering, panichella2013and, Mahmoud2013, Wang20193, sun2017frlink, capobianco2013improving, chhabra2017filtering, bavota2014enhancing, shao2013improved, Wang20191, Li2020,moran2020improving}.
More advanced techniques have been introduced using ML~\cite{Rasiman2022,Bella2019,mills2019tracing,Bella2018,Mills2018,Rath2018,Zhao2018,Mills2017,Mills20172,Falessi2017,Hayes2015,le2015rclinker,li2015recovering} and DL~\cite{kenton2019bert,Zhang2021,Wang20192,Guo:17,Chen2019,Alhoshan2019,Alhoshan2018,Li2018,zhao2020extended,Sultanov2013}, employing various algorithms â€” from classifiers like SVM, random forest, and decision trees to more sophisticated language models like BERT~\cite{devlin-etal-2019-bert} to find trace links. 
In recent years, with the emergence of LLMs, researchers have leveraged pre-trained knowledge through prompt engineering techniques to identify trace links between software artifacts~\cite{hassine2024llm,rodriguez2023prompts,vogelsang2025impact}. Hassine~\cite{hassine2024llm} proposed an LLM-based technique that uses zero-shot learning on GPT3.5 to find trace links between requirements and goals in Goal-oriented Language (GRL) models. Moreover, Rodriguez et al.~\cite{rodriguez2023prompts} proposed an approach that integrates zero-shot prompting with reasoning to enhance results in the Traceability Link Recovery (TLR) problem on diverse software artifacts.
They have shown that a prompt that performs well with one model or dataset may not yield optimal results with another, highlighting the need to customize prompts based on the specific context.

In addition to the algorithms being used, the types of artifacts with which these algorithms are intended to work also play a significant role. Existing studies primarily focus on identifying trace links between requirements and code~\cite{north2024code,peng2023enhancing,cleland:2010,panichella2013and,kuang2017analyzing,gao2022using,vogelsang2025impact}. Only a few studies have focused on establishing traceability across different software artifacts~\cite{nishikawa2015recovering}.
Existing approaches for RT are not directly applicable in our context due to the significant discrepancy between the legal language used in regulations and the technical language used in software requirements and related artifacts.  

Legal requirements traceability has only been investigated to a limited extent in the literature. Cleland-Huang et al.~\cite{cleland:2010} propose a probabilistic approach that identifies trace links between requirements and the HIPAA regulation by computing probability values based on detecting requirements indicator terms for regulations. The authors further propose extending the indicator terms with more domain-specific terms retrieved from the web. In a follow-up work, Gibiec et al.~\cite{Gibiec:2010} further investigate mining the web. 
%
Guo et al.~\cite{Guo:17} extend the previous two papers to improve the terminology gap problem, i.e., the mismatch between terms in requirements and regulations. The authors investigate different methods based on classification, ontologies, and web-mining and evaluate their approaches on \texttt{HIPAA}. 

While previous research has made significant strides in requirements traceability using traditional IR methods and ML/DL techniques, these approaches exhibit notable limitations in addressing the complexities of the LRT task. Most notably, existing methods struggle with the terminology gap between regulations and technical requirements, do not generalize well across regulations, and lack adaptability to multi-domain applications. In comparison to the above work, we empirically evaluate two automated LRT approaches: (1) a classifier-based solution leveraging sentence transformers and (2) a generative LLM-based solution guided by structured prompt engineering. By exploring these methods across two distinct regulations, HIPAA and GDPR, we advance the understanding of how modern NLP techniques can be adapted to meet the challenges of LRT. We also shed light on the possibilities or lack thereof of transfer learning across regulations. To the best of our knowledge, we are also among the first to identify the strengths and limitations of LLMs in this context. Further and larger studies with human experts are required in the future to establish the benefits of LLMs for LRT. 

%\TBD{Compared with the above work, we propose two approaches that utilize sentence transformer (ST) technologies and prompt engineering on GPT4o. We have employed ST on a benchmark dataset called HIPAA and compared it to the baseline technique (RQ1 and RQ2). To assess the generality of our technique, we evaluate ST on unseen documents from a new domain, referred to as the test documents (RQ3). Additionally, we introduce a new technique based on a LLM and prompt engineering and evaluate its performance on four test documents (RQ4).
%In the ST approach, instead of explicitly computing specific terminology in requirements and regulations, we let ST learn about the similar and dissimilar pairs of requirements and regulatory codes. This way, our approach is centered around accurately assigning similarity scores and hence can be adapted to different new application contexts. We empirically compare our approach with the classification approach proposed in existing work.}
%
%Further, our RQ3 experiment highlights the effectiveness of transfer learning approaches. While existing methods struggle with the nuanced language differences between regulatory and software requirements, transfer learning models trained on domain-specific data (like HIPAA) can achieve significantly higher success rates. 
%Nevertheless, we contend that a prompt engineering technique applied to an advanced LLM (e.g., GPT4o) can achieve superior performance on unseen test documents, by leveraging its extensive internal knowledge. RQ4 introduces a prompt-engineering technique applied to GPT4o surpassing both pre-trained, fine-tuned ST, and \kashif. By strategically crafting prompts, this approach enhances the model's understanding of the task, allowing it to generate more accurate trace links while providing a rationale for decisions. 
% Notably, GPT4o not only provides accurate trace link predictions but also offers a rationale for its predictions, addressing a key limitation in traceability: the need for explainability. Such accurate trace links, combined with informative rationales, can reduce manual effort for analysts, underscoring the potential for these models to scale traceability solutions across complex, multi-regulation environments. 
%Further and larger studies with human experts are required in the future to establish the benefits of using LLMs for LRT.