\section{Evaluation}\label{sec:evaluation}
In this section, we report on our empirical evaluation. 

\subsection{Research Questions (RQs) }
This paper investigates the following RQs: 
\sectopic{RQ1. Which ST model yields the most accurate results for tracing requirements to provisions?}  
As discussed in Section~\ref{sec:approach}, step~2 in \kashif~involves selecting the most accurate pre-trained model for the LRT task. 
Several alternative pre-trained models are publicly available. In RQ1, we examine 38 alternatives reported to work well in the NLP community. The goal of RQ1 is to identify the most accurate ST model for predicting trace links between requirements and provisions. 

\sectopic{RQ2. How accurate is \kashif compared to an existing baseline on a standard dataset from the literature? }
RQ2 aims to assess the value of utilizing ST as enabling technology for addressing the LRT problem compared to a baseline  from the existing literature, which we re-implement in this work. The baseline is a classifier that 
leverages the terminology probability distributions to compute the likelihood that a requirement can be traced to a provision, based on the occurrence of some indicator terms within that provision.
The investigation of RQ2 is conducted using the \texttt{HIPAA} dataset. 

\sectopic{RQ3. How accurately does \kashif perform on more complex dataset, spanning multiple requirements types and domains?} 
In RQ3, we test \kashif on four different documents, two shall-requirements and two user stories, covering various domains. These documents are traced to the GDPR privacy requirements. The goal of RQ3 is to investigate the performance of \kashif on a more realistic dataset that captures the complexity of the legal domain. 
%RQ3 addresses an essential practical need for a recommender system which can save the time and effort of requirements engineers during the LRT task. 
%To provide additional insights, we compare \kashif against the more recent generative LLMs, which are another straightforward alternative for building recommendation systems. 

\sectopic{RQ4. How accurate is \RICE-based approach in addressing the LRT task compared to \kashif?} 
%\sectopic{RQ4. How well does prompting using pre-trained LLMs perform on the LRT task?} 
Given the recent rise in the usage of LLMs, a straightforward alternative for automating tasks such as LRT is to prompt pre-trained LLMs, e.g., GPT4o. RQ4 assesses whether trace recommendations generated using pre-trained LLMs can offer a meaningful alternative to \kashif. 

%a new and more complex dataset that includes a greater number of regulatory codes and spans multiple domains and requirements types. Details about \texttt{TRACES} can be found in  Section~\ref{subsec:datacol}. We  compare the results of \kashif against the previously mentioned baseline. 
%The goal of RQ3 is to challenge both approaches with a dataset that closely resembles real-life compliance checking processes, capturing a wide variety of requirement types and regulatory codes.  This approach allows us to better understand the limitations of proposed solutions and the generalizability of their results on the \texttt{HIPAA} dataset.



%We fine-tune \kashif on \texttt{TRACES} with four epochs, a batch size of 16, a learning rate of 5e-4, and cosine similarity loss. We tuned the hyper-parameters using grid search. 
% \begin{table*}
% \footnotesize
% \centering
% \caption{Main Libraries used for implementing \kashif and their corresponding version.}\label{tab:library}
% \begin{tabularx}{0.80\textwidth}
% {@{} p{0.40\textwidth} @{\hskip 0.5em} p{0.40\textwidth} @{\hskip 0.5em}}
% \toprule
% \textbf{Library} & \textbf{Version} \\ 
% \midrule
% \texttt{sentence-transformers} & 2.6.1  \\ 
% \texttt{transformers} & 4.44.0\\ 
% \texttt{torch} & 2.0.1\\ 
% \texttt{scikit-learn} & 1.3.1\\ 
% \texttt{nltk} & 3.8.1\\ 
% \texttt{numpy} & 1.24.4\\ 
% \texttt{pandas} & 2.0.3\\ 
% \bottomrule
% \end{tabularx}
% \end{table*}

\input{datacollection}
%\input{Files/evalprocedure}

%\subsection{Answers to RQs}\label{subsec:rqs}

\input{RQ1}
\input{RQ2}
\input{RQ3}
\input{RQ4}

% \input{Files/RQ4}





% \sectopic{RQ2. Is our ST-based approach worthwhile compared to simpler solutions built on top of other enabling technologies? }
% RQ2 examines the accuracy of our proposed approach against that of other possible solutions created using different enabling technologies spanning traditional ML methods and more recent neural networks. 
% \begin{itemize}
%     \item The best performing model from above
% \item Test with word-level LMs (?)

% \textcolor{blue}{
% In addition to employing sentence transformers, we have harnessed word-level language models in our study to create the sentence embeddings. Specifically, we have opted for $DeBERTa$~\cite{he2020deberta}, which currently ranks as the top-performing word-level model on the GLUE leaderboard. The results for $DeBERTa$, both with and without clustering, can be found in detailed in Table~\ref{tab:rq1_tab2}. As evident from the data, $ST$ models outperform $DeBERTa$ in the majority of cases.
% }

% \end{itemize}
% \begin{itemize}
%     \item Build ML classifiers on top of sentence embeddings from LLMs
%     \item compare performance against RQ2
% \end{itemize}
%The first dataset, HIPAA, was used in the existing work. 
% \texttt{HIPAA} contains requirements and regulation statements from a single-domain focusing on health care. The second dataset, \texttt{TRACES}, has been built as part of our work. TRACES contains requirements from multiple domains and regulation statements from the data protection domain. 
%\item Test across datasets 
% \item Observe if a new model will need to be created per dataset or not
% \item Use loss to demonstrate how well the model represents the dataset 
% \item Observe the different factors that might impact the results, mainly (i) the size of GDPR dataset versus the size of HIPAA, and (ii) the domain terminology varies: HIPAA is healthcare domain while GDPR is data protection  