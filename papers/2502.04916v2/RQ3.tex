
\subsection{Effectiveness of Classification (RQ3) } \label{subsec:rq3}

\sectopic{Test Data.} To assess the effectiveness of \kashif, we curate four documents covering different requirements types and domains. These documents represent a snapshot of a practical scenario that exemplifies the potential complexity of LRT in practice. 
For each document, we manually identify trace links between software requirements and a list of 26 provisions derived from GDPR \rev{and are pertinent to software}. Building on existing work in  RE~\cite{Amaral:21,Amaral:23a}, the codes were comprehensively created, in collaboration with a legal expert (non-author), to represent the privacy requirements in GDPR pertinent to software engineering. Table~\ref{tab:test-documents} describes our test documents. We share the provisions as part of our online annex~\cite{annex}. Two co-authors of this paper, with more than 10 years expertise in requirements engineering, manually analyzed the four documents and identified the trace links for all requirements. 
%


\input{tab-RQ3-docs}


\sectopic{Methodology.} 
%
% RQ3 aims to show the most capable solution to be used as a recommender system for proposing trace links to provisions. 
\rev{In real-life scenarios, dealing with LRT involves navigating through many provisions, usually significantly more than 10 as in the simple \texttt{HIPAA} case. This inherent complexity is notable with the 26 provisions pertinent to software in the GDPR.  }
%, as visible in the case of GDPR when compared to the \texttt{HIPAA} dataset. 
Using the test documents described above, we evaluate and compare two models, namely \texttt{ST29}---the best pre-trained ST model selected in RQ1  and \kashif$_{\text{constant}}$---the best \kashif variant fine-tuned on \texttt{HIPAA} identified in RQ2. \rev{Note that we opted not to fine-tune \kashif again on the new documents for three reasons. First, the documents are small and thus inadequate for meaningful training (or fine-tuning). Second, we aim to challenge existing solutions with a more realistic scenario: the need for applying them on new unseen documents. Finally, \kashif is a similarity-based solution which has been exposed to both the LRT task as well as the regulatory domain (terminology) in the first fine-tuning on \texttt{HIPAA}. Therefore, another fine-tuning is less likely to have any additional value. }
% , and GPT4o---representing a viable LLM option for building a recommender system. 

% \sectopic{Prompt Engineering.} 
% We prompt the GPT4o model using the prompt described in Section~\ref{sec:approach}. Initially, we manually add the rationales for selecting the trace links into the first five requirements.  These requirements along with the selected trace links and the rationales are then used as the ``few-shot'' examples exposed to the GPT model an integral element in the prompt. %Our objective is to investigate whether and to what extent the GPT model can learn the reasoning process behind identifying trace links. The exact prompt is presented below. 

\sectopic{Evaluation Metrics.} 
To evaluate the effectiveness of LLMs, we report the results at the requirements and trace link levels. At the requirements level, we report (i) the number of requirements where the recommendations made by the LLM were exactly the same as our ground truth (\emph{exact match}); (ii) the number of requirements that were a \emph{partial match} to the ground truth, i.e., the requirements where the LLM recommended the same regulatory codes as in the ground truth along with additional recommendations (FP); 
%We limit the number of FPs to five, as we deem it a reasonable threshold to balance precision and recall. Beyond this threshold, the recommendations would likely introduce excessive noise, reducing their utility for practical applications; 
(iii) the number of \emph{incorrect matches}, i.e., all the other requirements that are not exact or partial matches. Following this, we compute the \textit{success rate}  as the ratio of requirements for which the approach predicts correct trace links (considering both partial and exact match) to the total number of requirements. At the trace link level, we report the total number of actual trace links, true positives (TP) and false positives (FP), and recall.

% %
% Our evaluation compares the automatically generated trace links against the manually identified ones. Following this, we compute the \textit{success rate}  as the ratio of requirements for which the approach predicts correct trace links to the total number of requirements for which the approach makes a prediction. 
% %Driven by the motivation of building a trustworthy recommender system, 
% The predicted trace links for a given requirement are deemed \textit{correct} if they exactly match or contain all of the manually identified links. If the predicted trace links for a requirement miss any manually identified link, then the prediction is considered \textit{incorrect}. For example, if the ground truth identifies for a requirement the trace links \{\texttt{ACC},\texttt{TRN}\}, then the automated recommendations should be exactly  \{\texttt{ACC},\texttt{TRN}\} or fully contain these links (e.g.,  \{\texttt{ACC},\texttt{TRN},\texttt{CON}\}),  assuming that the additional labels can be filtered out by the analyst. However, a recommendation that captures only \{\texttt{ACC}\} would be considered incorrect. 
% The rationale behind this evaluation procedure is that the analyst would not be able to recognize a missing trace link without substantial effort to analyze all provisions, which defeats the purpose of the automated recommendation. In contrast, the analyst would be able to easily spot any falsely predicted trace link. 
% %which a \textit{perfect} or \textit{partial} match is obtained against the trace links in the ground truth 
% We also report on the maximum number of predicted trace links by each approach to assess the manual effort required by the analyst to review and validate the results and compare it with analyzing the entire set of provisions. 

% \input{Files/Tables/tab-RQ3-LLMsResults}
\input{tab-RQ3-Results}

\sectopic{Results. }
Table~\ref{tab:rq3} shows the results for each approach across the test documents both at a trace link level and at requirement level. The table shows the number of requirements in each test documents\footnote{Note that we leave out five requirements from each document to enable fair comparison with the \RICE-based approach presented in RQ4.}, the number of predicted trace links (T$^*$), TPs, FPs, and recall. It further includes 
the number of requirements with exact match, the number of requirements with partial match, and the success rate. 
We observe from the table that \texttt{ST29} performs worse than \kashif. It also has less number of partial matches. Additionally, the exact matches often represent ``no trace link'', i.e., not predicting any trace link for requirements that had no trace links according to our ground truth. Our results indicate that the ST pre-trained model was not able to automatically predict trace links in most of the cases, showing that the model was neither able to understand the LRT task nor the application domain. 

On the other hand, \kashif outperforms \texttt{ST29} across all documents, with a notable difference in the number of partial matches. This result proves that fine-tuning pre-trained models on a dedicated dataset is indeed necessary for the model to learn about the LRT task.  While better than the pre-trained model, \kashif shows the following limitations: 1) it does not provide a rationale behind selecting a trace link, except the fact that semantic similarity exceeds a pre-defined threshold. This is expected to impede its use in practice.  2) The average success rate achieved by \kashif is about 44\%, which is not particularly effective. % though it can still reduce the needed validation efforts compared to no automation at all. 

% Automated recommendations generated  LLMs are more often correct than the other two approaches. On average, GPT4o yields a success rate of 70.9\%. GPT4o can also easily generate the rationale behind predicting certain labels.  Such rationale can be advantageous despite the additional time needed to review it. 


\begin{tcolorbox}[arc=1mm,width=\columnwidth,
                  top=0mm,left=0mm,  right=0mm, bottom=0mm,
                  boxrule=1pt, colback=violet!15!white,colframe=white]
\textbf{The answer to RQ3 is} that \kashif outperforms \texttt{ST29}, demonstrating that fine-tuning help the model learn about the LRT task. However, the performance of \kashif shows significant room for improvement over an unseen domain.
% However, \kashif does not provide any rationale to explain its output, making the predictions less usable. GPT4o is more effective, provides explanations, and can help reduce the manual effort needed to analyze complex LRT scenarios in practice.
\end{tcolorbox}
