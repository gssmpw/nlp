\subsection{Pre-trained Model Selection (RQ1)} \label{subsec:rq1}

\sectopic{Methodology. }  We shortlist the ST models for investigation in our work based on the NLP  leaderboard, which reports the 38 most accurate pre-trained models\footnote{\url{https://www.sbert.net/docs/pretrained_models.html}}. These models have been extensively evaluated for their ability to generate sentence embeddings (i.e., capturing the semantics of the whole text) and their performance in semantic search (i.e., finding relevant answers to a given query). Both tasks closely align with our objectives. 
To identify trace links, we apply the pre-trained models in a zero-shot setting as follows. 
We let each model compute the similarity matrix equivalent to the output of step~5 in our approach (see Fig.~\ref{fig:approach}). 
We then predict a trace link if the similarity value exceeds 
a predefined threshold. Since zero-shot does not require training, we run EXPI on the entire \texttt{HIPAA} dataset. 


\sectopic{Evaluation Metrics. } To better assess the performance irrespective of the selected threshold, we compute the \textit{Area Under the Curve (AUC)} for the receiver operating characteristic (ROC) across different threshold values,  ranging from $0.1$ to $0.9$. 
The ROC curve captures the trade-off between the true positive rate (TPR) and the false positive rate (FPR). TPR is the proportion of positives correctly identified as such (i.e., the percentage of trace links correctly identified for a given threshold). FPR is the proportion of negatives incorrectly identified as positives (i.e., the percentage of trace links wrongly identified as not trace links). The AUC of the ROC curve (computed as micro-average over all the provisions to avoid the dominance of some provisions)  provides a single aggregate performance measure across all possible thresholds and, hence, is a suitable evaluation metric to compare the ST models.  We posit that the model with the highest AUC value demonstrates the best overall performance in identifying trace links in a zero-shot setting, as a higher AUC value indicates a better balance between correctly identifying true trace links (high TPR) and minimizing the identification of false links (low FPR). 
%
%
\sectopic{Results. }
Table~\ref{tab:rq1} presents the \texttt{AUC} values of the ST pre-trained models on the \texttt{HIPAA} dataset and also  reports $K$, indicating the ranking of the models in the NLP community based on their accuracy~\cite{Reimers:19}, as well as $K^\dag$, indicating the ranking based on \texttt{AUC} achieved on \texttt{HIPAA}. 

\input{tab-RQ1} 

The best-performing model on \texttt{HIPAA} is \texttt{ST29} ($K^\dag=1$), with an AUC value of 0.859. The next best performing model is \texttt{ST21} with an AUC value of 0.850. The difference between these two AUC values is only marginal. A possible explanation is that  \texttt{ST29} uses  \texttt{ST21} as its base model.  \texttt{ST29}  has been, however, trained on more (multi-lingual) data.   

Additionally, we observe a discrepancy in the performance of the models on the \texttt{HIPAA} dataset compared to that reported by the NLP community.  
The best NLP model, \texttt{ST1}, does not perform well  on \texttt{HIPAA}, ranked 16. 
This observation indicates that well-performing models in NLP are not necessarily as effective for RE-specific problems. 
%The datasets in RE are typically domain-specific increasing the level of complexity to deal with.    

\begin{tcolorbox}[arc=1mm,width=\columnwidth,
                  top=0mm,left=0mm,  right=0mm, bottom=0mm,
                  boxrule=1pt, colback=violet!15!white,colframe=white]
\textbf{The answer RQ1} is that \texttt{ST29} is the best-performing pre-trained model for LRT (corresponding to \texttt{paraphrase-multilingual-mpnet-base-v2}). 
\end{tcolorbox}%The goal of RQ1 is to select a robust ST model that performs consistently well across datasets. 
% Table~\ref{tab:rq1} shows the \texttt{AUC} values of the ST pre-trained models on the \texttt{HIPAA} and \texttt{TRACES} datasets. The table also reports $K$ indicating the ranking of the models in the NLP community based on their accuracy~\cite{Reimers:19}, as well as $K^\dag$,  $K^\ddag$ and $K^*$,  indicating the rankings based on \texttt{AUC} achieved on \texttt{HIPAA},  \texttt{TRACES} and on average across the two datasets, respectively. The AUC for the ROC curve metric enables fair comparison, irrespective of the selected threshold values. 

%\input{Files/tab1-RQ1}

% The table shows that the models perform considerably poorly on the \texttt{TRACES} dataset. A plausible reason is that \texttt{TRACES} has a total of 26 regulatory codes, some of which are seemingly closely related (e.g., the regulatory code \textit{TIM}---the period for which personal data is stored is semantically close to \textit{DUR}---the duration of data processing). 
% To reduce the degree of confusion that ST models exhibit, we compute the AUC values for \texttt{TRACES} at the category level \TBD{is this what we report in the table? (yes)}. Recall from Section~\ref{tab:datasets} that the 26 regulatory codes in  \texttt{TRACES} are grouped into 10 different categories (listed in Table~\ref{tab:datasets}). Once the ST model computes the similarity values of single regulatory codes, we then assign to each category the maximum similarity values among the single regulatory codes in that category. For example, \textit{TIM} and \textit{DUR} belong to the category \textit{data retention} (\textit{RTN} in Table~\ref{tab:datasets}). If the similarity value between a given requirement $r_i$ and \textit{TIM} and \textit{DUR} is 0.3 and 0.47, respectively, then we assign the similarity value 0.47 between $r_i$ and the category \textit{data retention}.  


% The table further shows a discrepancy in the performance of the models across our datasets compared to that reported by the NLP community.  
% The best NLP model, \texttt{ST1}, does not perform well on our datasets as it is ranked 16 on \texttt{HIPAA} and 29 on \texttt{TRACES}. This indicates that well-performing models in NLP are not necessarily robust for RE-specific problems where the models are confronted with datasets spanning specific-domains and potentially different requirement types.  

% The best-performing model on \texttt{HIPAA} is \texttt{ST29} ($K^\dag=1$), with an AUC value of 0.859. The same model, \texttt{ST29}, is however ranked 10 on \texttt{TRACES} with an AUC of 0.614, 0.07 lower than the best model \texttt{ST2} ($K^\ddag=1$). However, \texttt{ST2} yields  0.13 lower AUC value on \texttt{HIPAA} when compared with \texttt{ST29}. 
% Overall, \texttt{ST29} achieves the best average AUC value of 0.736 on both datasets \texttt{HIPAA} and \texttt{TRACES} ($K^*=1$), leaving \texttt{ST2} six ranks behind. 
% Additionally, we observe that, on average, \texttt{ST3} fares fairly close to \texttt{ST29}. Still, according to the NLP leaderboard, \texttt{ST29} has the advantage of being much faster and smaller in size than \texttt{ST3}: \texttt{ST29}'s size is 970 MB, whereas \texttt{ST3}'s size is 2370 MB. 

% \begin{tcolorbox}[arc=1mm,width=\columnwidth,
%                   top=0mm,left=0mm,  right=0mm, bottom=0mm,
%                   boxrule=1pt, colback=violet!15!white,colframe=white]
% In view of the above analysis, \textbf{the answer RQ1}, we select \texttt{ST29} (corresponding to \texttt{paraphrase-multilingual-mpnet-base-v2}) as the best-performing ST model in identifying trace links using a zero-shot setting. 
% \end{tcolorbox}

