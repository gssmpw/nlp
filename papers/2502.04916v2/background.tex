\section{Background} \label{sec:background}

\sectopic{Language Models (LMs). } Language Modeling in NLP involves computationally determining the probability distribution of word sequences~\cite{Jurafsky:20}. Given a sequence of words, an LM predicts the most likely next word, enabling it to generate text~\cite{Alexandrescu:06}. For example, an LM would predict ``Mat'' as the most likely next word in the input sequence, ``The cat sits on the [WORD]''.  LMs are trained on large corpora of texts to accurately estimate these probability distributions. 
State-of-the-art LMs are based on transformer architecture which leverages self-attention mechanisms to weigh the significance of different parts of an input text relative to a given position~\cite{Vaswani:17}. The attention mechanism determines which words in a sentence are more important based on the context and gives them more ``attention''. For instance, in the sentence ``Mary, who used to live in Paris, loves wine.'', the attention is on Mary and wine.
%
Building on transformer architectures, the Sentence Transformers framework (ST)~\cite{Reimers:19} offers a set of pre-trained models designed to encode longer text sequences, such as sentences or paragraphs, into dense vector representations within a high-dimensional space. They produce contextual embeddings that capture the overall semantic essence of an entire input sequence.

%\sectopic{Generative Large Language Models (LLMs). }
More recently, generative LLMs have emerged as transformer-based language models that are scaled up significantly in model size and the amount of training data. 
Examples on LLMs include OpenAI's GPT (Generative Pre-trained Transformer)~\cite{Radford:18} and LLaMa~\cite{touvron2023llama,touvron2023llama2}. 
These models can perform new tasks based on textual instructions (prompts)~\cite{NEURIPS2020_1457c0d6}.
%
% In parallel to the pervasive use of LLMs, the concept of \textit{transfer learning}~\cite{torrey2010} has become more prevalent. During the pre-training phase, such models acquire massive knowledge that can be transferred to other domains or tasks. Transfer learning has proved its effectiveness in various  RE tasks~\cite{hey:20,casillo:22}, in particular, when training examples are scarce. 

\sectopic{Machine learning (ML). }
Supervised learning is one of the most prominent paradigms in ML. In this paradigm,  the ML algorithm is provided with labeled training data where each data point consists of an input vector (features) and the corresponding output label (or value). The algorithm learns patterns within the input features to make predictions based on this training. When trained on a sufficiently large dataset, the algorithm refines its predictions to classify the provided labels more accurately. The example ML classification algorithms include random forest, decision tree, support vector machine, and feed-forward neural networks~\cite{Goodfellow:16}.

