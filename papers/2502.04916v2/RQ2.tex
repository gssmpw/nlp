\subsection{Accuracy on Benchmark Dataset (RQ2) } \label{subsec:rq2}

\sectopic{Methodology.} We compare the three variants of \kashif (explained in Section~\ref{sec:approach}) against a baseline (\texttt{B}) from the literature~\cite{cleland:2010,Guo:17}, which we re-implement. 
We answer RQ2 on the benchmark dataset, \texttt{HIPAA}.  Since \texttt{HIPAA} contains 10 requirements documents, we apply the leave-one-out (LOO) evaluation method, where \kashif and \texttt{B} are tested each time on a left-out document and trained (or fine-tuned) on the remaining documents to emulate realistic situations.  
However, to ensure a reasonable balance between the training and test sets, we exclude one document (\texttt{CCHIT}, labeled H2 in Table~\ref{tab:hipaa-dataset}) from the LOO process since it contains 1,064 requirements, thus including more than half the dataset. 

\sectopic{Fine-tuning details.} Based on our results in RQ1, we build \kashif with \texttt{ST29}, which we fine-tune on \texttt{HIPAA} with 10 epochs, a batch size of 16, a learning rate 5e-3, and cosine similarity loss. We tuned the hyper-parameters using grid search~\cite{Bergstra:12}. 

\sectopic{Evaluation Metrics.} We evaluate the two approaches using precision (P), measuring how many trace links identified by the approach are correct; recall (R), measuring how many trace links in our ground truth are correctly identified by the approach; and F1 score, the harmonic mean of the precision and recall.
%
We report the mean and standard deviation across the nine documents.  


\sectopic{Results. }
Table~\ref{tab:rq2} lists, for each approach, the total number of TPs, FPs, FNs, and TNs and further reports the mean and standard deviation of precision, recall, and F1. 

\input{tab-RQ2}

%We note that the results  of \texttt{B} are different from the ones reported in the original paper  since our evaluation is based on leave-one-out. 

As visible from the table, \texttt{B} outperforms all variants of \kashif in terms of precision, achieving an average of 57.8\%. This precision value is 8.5 pp better than the second best precision value achieved by  \kashif$_{\text{constant}}$.
We recall that \texttt{B} is a classifier that primarily uses a probabilistic method based on the occurrence of words in requirement texts and predict whether these requirements should be traced to a particular regulation accordingly.   
Achieving a higher precision can be attributed to the selected threshold which led to more conservative predictions and hence less FPs.  %We also note that the dataset is highly skewed with a small fraction of trace links compared with non-trace links. 
While \texttt{B} produces less FPs, it still misses a lot of TPs as we see also in the table. All variants of \kashif, on the other hand, achieve higher recall values reaching up to 80\% in the case of \kashif$_{\Delta}$. This in turn leads to a higher F1 score in favor for \kashif over  \texttt{B}. \rev{As shown in the table, the variant \kashif$_{\text{constant}}$ achieves a remarkable gain of 35.3 pp in F1 score  over \texttt{B}.} %In practice, our proposed approach can be used for generating automated recommendations that would help engineers identify trace links both in the scenarios of examining existing software requirements as well as during the elicitation phase. 
%Therefore, we deem \texttt{B} to be not practically useful. 

%\textcolor{blue}{Moreover, the baseline approach primarily focuses on identifying word occurrences in relation to a given regulatory code. However, this method overlooks the overall sentence semantics, as it relies solely on word presence. In contrast, \kashif accounts for textual semantics, allowing it to better capture implicit knowledge within the requirements and link them to the correct regulatory code. For instance, the requirement “The system shall prevent the creation of duplicate patient records” is linked to regulation \texttt{IC} (Integrity controls. Implement security measures to ensure that electronically transmitted electronic protected health information is not improperly modified without detection until disposed of). This trace link was missed by \texttt{B}, as the relevant keywords for \texttt{IC} were absent in the requirement, even though the implicit knowledge was present.} 

Comparing the three variants of \kashif, our results show that \kashif$_{\text{constant}}$ is the best performing variant in terms of F$_1$, achieving an average score of 56.9\%. This score provides a gain of 23.4 pp over \kashif$_{dynamic}$ and 42.5 pp over \kashif$_{\Delta}$. In terms of recall, however, \kashif$_{\Delta}$ achieves the best value of 80\%, 12.7\% more than \kashif$_{\text{constant}}$. This can be explained by the threshold adjustment method for \kashif$_{\Delta}$. \rev{Recall from Section~\ref{sec:approach} that to determine the threshold above which a trace link is predicted, we look at the largest gap in similarity values between the requirement and the provisions. Once determined, 
%we select the cutoff point causing the highest drop in similarity values. 
%This way, 
\kashif$_{\Delta}$ will always predict  at least one trace link for each requirement corresponding to the provision with the highest similarity value that exceeds this gap.  }
Such a method and recall value can indeed be useful when building recommendation systems. However, they come at the cost of introducing more FPs (as evidenced by the low precision), which then entails  significant effort from the human analyst to filter out those FPs. Consequently, we select  \kashif$_{\text{constant}}$ as the best performing model for LRT.    


\input{tab-RQ2-details}

To understand the sources of errors produced by \kashif$_{\text{constant}}$, we analyzed the results per document and provision. The results are listed in Table~\ref{tab:rq2-errors}. Our analysis reveals the following \rev{causes of errors}:


\begin{itemize}
    \item [$\bullet$] \rev{\textbf{Computing significantly low similarity scores for correct trace links. } A majority of FNs (36/54 = 66.7\%) are due to computing low similarity scores between the requirement and the corresponding traced provisions.} % the correct provisions, which are supposed to be traced to the requirements. 
    These low scores do not exceed the threshold, thus leading to FNs.  
    \item [$\bullet$] \rev{\textbf{Computing significantly high similarity scores when there are no trace links. }} A majority of FPs (96/113 = 84.9\%) are due to falsely predicting a trace link for those requirements that have no trace links in our ground truth. \rev{This case suggests that a binary classifier could help in reducing FPs by predicting whether a requirement should have a trace link or not in the first place. We have conducted several experiments around this hypothesis. While we observed less FPs when using a binary classifier, the overall improvement was not significant and hence we do not report it in this paper. }
    \item [$\bullet$] \rev{\textbf{Predicting wrong provisions as trace links. }} The remaining FPs and FNs are caused by predicting provisions other than those identified in the ground truth. 
\end{itemize}




\begin{tcolorbox}[arc=1mm,width=\columnwidth,
                  top=0mm,left=0mm,  right=0mm, bottom=0mm,
                  boxrule=1pt, colback=violet!15!white,colframe=white]           
\textbf{The answer to RQ2} is that \kashif yields the  best accuracy  on \texttt{HIPAA} when we apply a constant threshold value of 0.5.  Specifically, \kashif achieves an F1 score of $\approx$57\%. Compared to an existing baseline from the literature, \kashif has a gain of about 35 pp in F1 score.  
\end{tcolorbox}
