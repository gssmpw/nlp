

\begin{table}
%\footnotesize
\centering
\caption{AUC of ST models for LRT on \texttt{HIPAA} (\textbf{RQ1}). 
% \TBD{@Romina: come on! you don't leave footnotes on TRACES in the table. Please revise your changes. Also, you don't need "HIPAA" in the header if the results are only for HIPAA, @Romina: please remove and adjust the header accordingly}
}
\label{tab:rq1}
% \begin{threeparttable}[t]
\begin{tabularx}{\textwidth}{@{} p{0.05\textwidth} @{\hskip 0.5em} p{0.05\textwidth} @{\hskip 3em} p{0.05\textwidth} @{\hskip 20em} *{5}{>{\centering\arraybackslash}X}@{}}
    \toprule
    \multirow{1}{*}{$K$\tnote{1}} & \multirow{1}{*}{Model\tnote{2}} & \multirow{1}{*}{Name\tnote{1}} & \multirow{1}{*}{\texttt{AUC}\tnote{1}} & \multirow{1}{*}{$K^\dag$\tnote{1}} \\%\multicolumn{2}{c}{\texttt{HIPAA}} \\ %& \multicolumn{2}{c}{\texttt{TRACES}} & \multicolumn{2}{c}{Average}\\
    % \cmidrule(lr){4-5}
    % &&& \texttt{AUC} & $K^\dag$ \\ %&\texttt{AUC} & $K^\ddag$ &\texttt{AUC} & $K^*$  \\
    \midrule
1 &   \texttt{ST1}  & \texttt{all-mpnet-base-v2} & 0.744 & 16 \\ % & 0.331 & 29 & 0.538 & 27\\
2 &   \texttt{ST2}  & \texttt{gtr-t5-xxl} & 0.725 & 21 \\ % & \textbf{0.685} & 1 & 0.705 & 7\\
3 &   \texttt{ST3}  &\texttt{gtr-t5-xl} & 0.789 & 6 \\ % & 0.678 & 2 & 0.733 & 2\\
4 &   \texttt{ST4}  &\texttt{sentence-t5-xxl} & 0.720 & 22 \\ % & 0.666 & 3 & 0.693 & 8\\
5 &   \texttt{ST5}  &\texttt{gtr-t5-large} & 0.743 & 17 \\ % & 0.640 & 7 & 0.692 & 9\\
6 &   \texttt{ST6}  &\texttt{all-mpnet-base-v1} & 0.712 & 25 \\ % & 0.338 & 27 & 0.525 & 29\\
7 &   \texttt{ST7}  &\texttt{multi-qa-mpnet-base-dot-v1} & 0.688 & 27 \\ % & 0.631 & 8 & 0.659 & 12\\
8 &   \texttt{ST8}  &\texttt{multi-qa-mpnet-base-cos-v1} & 0.603 & 34 \\ % & 0.222 & 36 & 0.413 & 36\\
9 &   \texttt{ST9}  &\texttt{all-roberta-large-v1} & 0.601 & 35 \\ % & 0.333 & 28 & 0.467 & 34\\
10 &   \texttt{ST10}  &\texttt{sentence-t5-xl} & 0.769 & 10 \\ % & 0.644 & 6 & 0.706 & 5\\
11 &   \texttt{ST11}  &\texttt{all-distilroberta-v1} & 0.719 & 23 \\ % & 0.284 & 34 & 0.501 & 32\\
12 &   \texttt{ST12}  &\texttt{all-MiniLM-L12-v1} & 0.729 & 19 \\ % & 0.318 & 30 & 0.523 & 30\\
13 &   \texttt{ST13}  &\texttt{all-MiniLM-L12-v2} & 0.747 & 15 \\ % & 0.339 & 26 & 0.543 & 26\\
14 &   \texttt{ST14}  &\texttt{multi-qa-distilbert-dot-v1} & 0.563 & 36 \\ % & 0.546 & 17 & 0.555 & 25\\
15 &   \texttt{ST15}  &\texttt{multi-qa-distilbert-cos-v1} & 0.640 & 33 \\ % & 0.228 & 35 & 0.434 & 35\\
16 &   \texttt{ST16}  &\texttt{gtr-t5-base} & 0.770 & 9 \\ % & 0.655 & 5 & 0.712 & 4\\
17 &   \texttt{ST17}  &\texttt{sentence-t5-large} & 0.748 & 14 \\ % & 0.663 & 4 & 0.706 & 6\\
18 &   \texttt{ST18}  &\texttt{all-MiniLM-L6-v2} & 0.761 & 11 \\ % & 0.285 & 33 & 0.523 & 31\\
19 &   \texttt{ST19}  &\texttt{multi-qa-MiniLM-L6-cos-v1} & 0.670 & 29 \\ % & 0.313 & 31 & 0.492 & 33\\
20 &   \texttt{ST20}  &\texttt{all-MiniLM-L6-v1} & 0.749 & 13 \\ % & 0.307 & 32 & 0.528 & 28\\
21 &   \texttt{ST21}  &\texttt{paraphrase-mpnet-base-v2} & 0.850 & 2 \\ % & 0.587 & 14 & 0.719 & 3\\
22 &   \texttt{ST22}  &\texttt{msmarco-bert-base-dot-v5} & 0.644 & 32 \\ % & 0.503 & 20 & 0.574 & 24\\
23 &   \texttt{ST23}  & \texttt{multi-qa-MiniLM-L6-dot-v1} & 0.715 & 24 \\ % & 0.605 & 12 & 0.660 & 11\\
24 &   \texttt{ST24}  & \texttt{sentence-t5-base} & 0.726 & 20 \\ % & 0.584 & 15 & 0.655 & 13\\
25 &   \texttt{ST25}  & \texttt{msmarco-distilbert-base-tas-b} & 0.701 & 26 \\ % & 0.557 & 16 & 0.629 & 18\\
26 &   \texttt{ST26}  & \texttt{msmarco-distilbert-dot-v5} & 0.685 & 28 \\ % & 0.600 & 13 & 0.643 & 15\\
27 &   \texttt{ST27}  & \texttt{paraphrase-distilroberta-base-v2} & 0.801 & 4 \\ % & 0.455 & 24 & 0.628 & 19\\
28 &   \texttt{ST28}  & \texttt{paraphrase-MiniLM-L12-v2} & 0.794 & 5 \\ % & 0.496 & 22 & 0.645 & 14\\
29 &   \texttt{ST29}  & \texttt{paraphrase-multilingual-mpnet-base-v2} & \textbf{0.859} & 1 \\ % & 0.614 & 10 & \textbf{0.736} & 1\\
30 &   \texttt{ST30}  & \texttt{paraphrase-TinyBERT-L6-v2} & 0.787 & 7 \\ % & 0.464 & 23 & 0.625 & 21\\
31 &   \texttt{ST31}  & \texttt{paraphrase-MiniLM-L6-v2} & 0.770 & 8 \\ % & 0.511 & 18 & 0.641 & 16\\
32 &   \texttt{ST32}  & \texttt{paraphrase-albert-small-v2} & 0.737 & 18 \\ % & 0.499 & 21 & 0.618 & 22\\
33 &   \texttt{ST33}  & \texttt{paraphrase-multilingual-MiniLM-L12-v2} & 0.811 & 3 \\ % & 0.511 & 19 & 0.661 & 10\\
34 &   \texttt{ST34}  & \texttt{paraphrase-MiniLM-L3-v2} & 0.757 & 12 \\ % & 0.441 & 25 & 0.599 & 23\\
35 &   \texttt{ST35}  & \texttt{distiluse-base-multilingual-cased-v1} & 0.349 & 37 \\ % & 0.092 & 37 & 0.220 & 37\\
36 &   \texttt{ST36}  & \texttt{distiluse-base-multilingual-cased-v2} & 0.341 & 38 \\ % & 0.090 & 38 & 0.216 & 38\\
37 &   \texttt{ST37}  & \texttt{average\_word\_embeddings\_komninos} & 0.647 & 31 \\ % & 0.606 & 11 & 0.627 & 20\\
38 &   \texttt{ST38}  & \texttt{average\_word\_embeddings\_glove.6B.300d} & 0.636 & 30 \\ % & 0.625 & 9 & 0.630 & 17\\ 
\bottomrule
\end{tabularx}
\begin{tablenotes}
     \item[1] $K$: The average performance ranking of the models, as reported in the NLP community. $K^\dag$: The ranking of the models based on AUC values computed on \texttt{HIPAA} ($K=1$ indicates the highest AUC). 
      \item [2] \texttt{ST1}--\texttt{ST38} correspond to the models reported at this link (sorted by average accuracy in descending order):     \url{https://www.sbert.net/docs/pretrained_models.html}. %, where \texttt{ST29} is \texttt{paraphrase-multilingual-mpnet-base-v2}.
     \end{tablenotes}
 % \end{threeparttable}
 %\vspace*{-2em}
 \end{table}

