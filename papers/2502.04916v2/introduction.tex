\section{Introduction}\label{sec:introduction}

Technological advancements are significantly transforming software development across diverse domains, such as healthcare~\cite{Caruana:15}. Software applications and automated assistants have become integral to our daily lives~\cite{Zhan:22}. This evolution, driven by recent breakthroughs in artificial intelligence (AI), has led to increasing complexity in software systems~\cite{ahmad2023requirements,Feldt:18}. 
%
As technology progresses, regulations are adapting in parallel to ensure that software systems are developed in line with ethical and legal standards. For example, the general data protection regulation (GDPR)~\cite{GDPR} is enforced since 2018 to address concerns about privacy and data protection. Despite being introduced by the European Union (EU), the GDPR has a global effect impacting organizations (and software) outside the EU as long as they handle personal data of EU residents.  


Requirements Engineering (RE) plays a pivotal role in this landscape. 
RE is concerned with specifying and maintaining software requirements that outline the properties and functions of a system-to-be~\cite{Pohl:11}. 
Legal compliance of software systems against applicable provisions can be addressed at different stages of software development. One scenario is to explicitly identify legal requirements early during the requirements elicitation phase, answering the question: ``What legal obligations need to be satisfied by the system for it to be compliant?''.  The elicited legal requirements %can be explicitly specified based on a set of applicable legal sources. These legal requirements 
can then be integrated into the software development process, while maintaining trace links to the source legal provisions. 
%
As an alternative scenario, requirements engineers may need to verify the compliance of existing software systems against legal provisions in a post-deployment stage, as new regulations have become applicable.
In this case, they must answer the question ``Does the system satisfy the regulation?''. To do so, engineers must analyze the regulation, identify applicable legal provisions, and then trace software requirements to these statements.  
Both alternatives rely on \textit{requirements traceability analysis}, an essential RE activity concerned with the identification and maintenance of trace links between requirements and other artifacts within the software development lifecycle~\cite{Meyer:22}. \textit{legal requirements traceability (LRT)} is a special case where requirements are traced to provisions in a regulation and is the focus of this paper. 

Consider the following example. Imagine a fictional mobility app named  \textit{WeMobilize}, which helps users book and share cab rides. Originally a non-EU startup, WeMobilize is expanding to the EU and hence must comply with the GDPR. 
%To ensure that WeMobilize does not commit a legal breach when collecting personal information from individuals in the EU, 
This example is particularly relevant as many businesses are globalizing and must adapt to data protection laws in different jurisdictions. 
%
Fig.~\ref{fig:example} shows how WeMobilize's requirements (labeled REQ1 -- REQ5) can be traced to data protection policies in the GDPR~\cite{GDPR}. 
We identify trace links to provisions in GDPR for REQ1 and REQ3 -- REQ5, visualized as dashed lines in black. REQ2 has no trace link to GDPR in our example since it does not involve processing users' personal data. 

REQ1 involves collecting  user's personal information and must therefore be traced to two provisions, namely {REG\_DIR} (related to the direct collection of personal information) and REG\_CON (related to the explicit soliciting of users' consent). Currently, consent is not part of REQ1, which prevents identifying a trace link with REG\_CON---a missing trace link is visualized with a red dashed line in the figure. Failing to identify this trace link entails a possible breach of GDPR. Therefore, deploying WeMobilize as-is, without accounting for provisions in GDPR, can lead to potential reputational and financial losses caused by violating the GDPR. LRT can help identify potential non-compliance issues at early stages but requires not only legal expertise but also substantial manual effort. Developing automated support is therefore beneficial to assist engineers and analysts in identifying applicable trace links. 

\begin{figure*}
  \includegraphics[width=\textwidth]{Example_RE24.pdf}
  \centering
  % \vspace*{-.7em}
  \caption{Example on tracing WeMobilize app requirements to GDPR statements.}
  % \vspace*{-.5em}
  \label{fig:example}
 \vspace*{-1em}
\end{figure*}

Requirements traceability is a well-explored problem in the RE literature~\cite{wang2018requirements,tufail2017systematic}.
%Predicting trace links is tailored toward a specific application context. 
%
% The RE literature identifies two scenarios for requirements traceability~\cite{Rasiman22}. The first scenario is \textit{trace links recommendation}, where requirements engineers build trace links while specifying requirements. In this scenario, an automated approach can assist the engineers by recommending a pool of relevant links. 
%
% The second scenario is \textit{trace links maintenance}, where trace links are recovered after specifying requirements (similar to our motivating example on WeMobilize introduced above).  For large and complex projects, manual effort for filtering out incorrect trace links is necessarily limited. Therefore, predicting trace links with high confidence is desirable. 
%
However, the extensive research on requirements traceability is not directly applicable to LRT due to the significant discrepancy between the legalese used in regulations and the technical language used in software requirements and related artifacts. 
%
Despite the serious consequences of non-compliance, LRT has received limited attention from the community. 
Cleland-Huang et al.~\cite{cleland:2010,Gibiec:2010} proposed a classifier that predicts trace links by computing the likelihood of requirements being traced to provisions based on indicator terms found in both provisions and requirements. 
Guo et al.~\cite{Guo:17} focused on bridging the terminology gap between provisions and software requirements. %, which can affect the accuracy of automated traceability approaches.  
They examined three methods, including the one by Cleland-Huang et al. mentioned above, and two others  based on web-mining and ontologies. The proposed methods aim to expand the terminology of the provisions with additional terms in order to better identify trace links.

The aforementioned approaches have three limitations. First, they do not leverage recent natural language processing (NLP) technologies such as large language models (LLMs).  Second, their evaluation is based on a single benchmark that does not necessarily reflect the full complexity of the legal domain in practice. Third, these approaches cannot be transferred to other requirements types or domains without significant adaptations.
To address these limitations, we propose in this paper two novel approaches based on recent NLP technologies, utilizing the Transformers architecture~\cite{Vaswani:17} and LLMs. Similar to existing work, both approaches aim to predict trace links and we  assess their performance on a realistic scenario beyond the benchmark dataset. 

\sectopic{Contributions.} 
The paper makes the following contributions: 

% \TBD{Change according to abstract}
(1) We devise two automated approaches for predicting trace links between requirements and provisions based on LLMs. 
Our first approach, hereafter referred to as \kashif, standing for \textit{automated trace lin\textbf{K} identific\textbf{A}tion  between legal provi\textbf{S}ions and tec\textbf{H}nical requ\textbf{I}rements using sentence trans\textbf{F}ormers.} 
\kashif leverages sentence transformers (ST), that are pre-trained language models optimized for understanding longer text sequences such as sentences, and predicts trace links using semantic similarity. 
Our second approach utilizes \RICE, a recent framework that enables effective prompting of LLMs. We employ \RICE with the GPT4o model offered by OpenAI\footnote{\url{https://openai.com/index/hello-gpt-4o/}}. Our solutions are described in Section~\ref{sec:approach}.
%Our solution design is motivated by the significant success of LLMs in various NLP downstream tasks~\cite{barba2023,tam2023,luo2021}. 
%
%We elaborate on our approach in Section~\ref{sec:approach}.   

(2) We empirically evaluate, our first solution, \kashif, on a benchmark dataset, referred to as \texttt{HIPAA}~\cite{Guo:17}, comprising of textual requirements  traced to 10 different provisions. 
We further compare \kashif against a baseline classifier from the literature~\cite{cleland:2010,Guo:17}. % that was originally developed on this same dataset and 
We re-implemented and re-evaluated the baseline as part of this work. 
Our evaluation shows that \kashif yields an  average recall score of $\approx$67\%, leading to a substantial improvement of 54 percentage points (pp) over the baseline\footnote{Note that the baseline was evaluated using a different, more realistic procedure than the one reported in the literature.}. While \kashif still performs significantly better than the baseline, such accuracy is rarely practically useful in real-life scenarios where number of provisions easily exceed 10 (as is the case in \texttt{HIPAA}). More details on this evaluation can be found in Section~\ref{subsec:rq2}.


(3) To further confirm its performance, we test \kashif on new unseen requirements documents covering diverse domains and requirements types. These requirements are traced to the GDPR, a more complex regulation with 26 provisions pertaining to personal data protection that must be adhered to in software requirements.  
On this dataset, \kashif (without additional fine-tuning) yields an average recall 15\%. In comparison, a pre-trained sentence transformer, with no exposure to the requirements traceability task, yields a nearly zero recall, as elaborated in Section~\ref{subsec:rq3}. %While %can transfer knowledge to some extent, 
The poor performance of \kashif  suggests that addressing LRT as a classification problem fails to handle the complexity of modern regulations and systems. Driven by this observation, we propose our second solution, the final contribution of this paper. 

% more recent generative LLMs such as the GPT model  released by OpenAI~\footnote{\url{https://openai.com/}}.

(4) We devise a prompt strategy based on the \RICE framework, capturing recent state-of-the-practice in LLMs for RE. For simplicity, we refer to our prompt strategy hereafter as \RICE. %We design and apply our prompt on the GPT4o model. 
Our evaluation (reported in Section~\ref{subsec:rq4}) shows that using \RICE with the GPT4o LLM  leads to an average accuracy of 84\% in successfully predicting the trace links in the GDPR dataset, a complex and general regulation. Compared to \kashif, \RICE shows a remarkable gain of 69 pp in  accuracy. \RICE misses on average 10 genuine trace links across the unseen documents and further introduces 187 false trace links. Nonetheless, using \RICE in practice can still significantly reduce the time and effort needed for manually identifying trace links. With \RICE, the analyst will vet only a small fraction of the provisions, equivalent to $\approx$12\%, while identifying 84\% of actual trace links. Further, GPT4o also provides an informative rationale for each predicted trace link. Therefore, from these results we can conclude that a solution based on LLMs, combined with careful prompt engineering, is the most promising avenue of research for LRT.
%the effectiveness of LLMs on unseen data and compare their performance against pre-trained and fine-tuned ST models. 
% Our results show that \RICE predicts trace links with an accuracy 
% demonstrates that GPT4o is capable of achieving a 100\% success rate while reducing the number of trace link pairs requiring verification by the requirements engineer by X\%.

 
%We discuss the details of our evaluation in Section~\ref{sec:evaluation}. 

%(2) Building on existing work~\cite{Amaral:21,Amaral:23a}, we created, in collaboration with a legal expert (non-author), a list of 26 goals representing privacy requirements in GDPR.  
%These goals are key for ensuring software systems comply with GDPR.  
% We also curated a dataset, named as \texttt{TRACES} (\textbf{T}racing  \textbf{R}egul\textbf{A}tory \textbf{C}odes to softwar\textbf{E} requirement\textbf{S}) to trace these goals to software requirements. 
% \texttt{TRACES} includes 1899 unique requirements from 16 industrial specification documents across various domains, such as healthcare, defense, security and online gaming. These documents feature diverse writing styles, including ``shall'' requirements and user stories. 
% The curation process is detailed in Section~\ref{subsec:datacol}. 

%\sectopic{Data Availability. } 
%We make our evaluation material available in an online annex~\cite{annex}. %\TBD{@Romina: please revisit what we promised to make available in the online annex and adjust your annex accordingly. }

\sectopic{Structure.} Section~\ref{sec:background} provides background. Section~\ref{sec:approach} presents solution design. Section~\ref{sec:evaluation} reports on our empirical evaluation. Section~\ref{sec:threats} discusses threats to validity. Section~\ref{sec:related} reviews the related work, and finally, Section~\ref{sec:conclusion} concludes the paper.
