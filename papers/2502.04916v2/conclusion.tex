\section{Conclusion}~\label{sec:conclusion}
This study presents a comparative evaluation of two approaches for Legal Requirements Traceability (LRT): a classifier-based method, \kashif, leveraging sentence transformers, and a generative LLM-based method, \RICE, designed using a structured prompt engineering framework. Our results demonstrate that while \kashif provides significant improvements over a baseline in terms of recall, achieving a recall of 67\% on HIPAA data (54\% pp more than the baseline). However, \kashif's performance deteriorates on more complex datasets such as GDPR, yielding only 15\% recall. This highlights the limitations of classification-based solutions in handling the complexity and variability inherent to legal and regulatory texts.

Conversely, the \RICE approach, built on generative LLMs, outperformed \kashif on GDPR data with a recall of 84\%, reducing the manual effort required for traceability by enabling analysts to vet only a fraction of trace links. These findings suggest that generative LLMs and carefully designed prompts provide a promising pathway for automating LRT tasks in complex legal domains. However, the approach has its challenges, such as false positives, which require further investigation. In addition to evaluating the current state-of-the-art methods, this work highlights critical challenges, including terminology gaps between requirements and regulations and the inability of existing methods to generalize effectively across different datasets and regulatory frameworks. By addressing these challenges, our study underscores the importance of tailoring solutions to the nuances of legal and regulatory contexts.

In the future, we plan to conduct a human-in-the-loop study with a domain expert to investigate the applicability of LLMs in LRT context. We further plan to enhance the performance of LLMs by incorporating domain-specific knowledge to better handle the terminology and contextual gaps between regulatory texts and technical requirements, particularly for GDPR.


% \TBD{In this paper, we developed an automated approach (\kashif) that leverages the sentence transformers framework. \kashif identifies trace links between requirements and regulatory codes by computing semantic similarity scores.
% % and then finding the optimal threshold that can be dynamically adjusted using few-shot learning. %adapted according to the needs of different application contexts. 
% First, we evaluate \kashif on a dataset, namely \texttt{HIPAA}, which is a pre-existing dataset from the healthcare domain used in previous research. 
% On \texttt{HIPAA}, \kashif achieves F$_1$ score of $\approx$57\%. %a precision of 87.5\% and a recall of 91.3\%. % by automatically adjusting a dynamic threshold using few-shot learning. 
% Compared to the main baseline from the RE literature, \kashif yields %a gain of $\approx$31 percentage points (pp) in precision and a loss of 6.5 pp in recall. This leads to 
% a substantial gain in F$_1$ of $\approx$35 percentage points. 
% Moreover, To assess how well fine-tuning performs on an unseen dataset, we evaluate \kashif along with pre-trained ST on four documents from a domain distinct from \texttt{HIPAA}. The results indicate that the fine-tuned ST model outperformed the non-fine-tuned version. However, in a realistic scenario where the amount of training data is limited or nonexistent, the fine-tuning model may not perform well. Therefore, we have proposed prompt engineering technique using an advanced LLM, namely GPT4o. We applied prompt engineering techniques to GPT4o, tailoring its prompts to better align with the requirements of the LRT task. This approach significantly outperformed \kashif, demonstrating the effectiveness of leveraging advanced language models with optimized prompting strategies. The results showed a significant improvement over \kashif, highlighting the potential of prompt engineering in enhancing the accuracy for the LRT task. Moreover, GPT4o can offer a rationale for the potential links between its selected provisions and the input requirement, which is valuable information for analysts.
% % achieves the accuracy of $\approx$71\% which is $\approx$44 pp over \kashif. Therefore, a fine-tuned ST model can not perform better than GPT4o on an unseen domain.
% %
% For future work, we plan to expand the set of test documents to include a broader range of data from the target domain. This will allow us to fine-tune \kashif more extensively and assess its performance more accurately in comparison to GPT4o. }