\section{Related Work}
\subsection{Denoise on Point Clouds}
Most learning-based point cloud denoising methods evolve from foundational point cloud processing techniques~\cite{preiner2014continuous}. 
The development of PointNet and PointNet++~\cite{qi2017pointnet++} enables the direct convolution of point sets, paving the way for more advanced approaches. 
Building on these advancements, Wang $et$ $al.$~\cite{wang2019dynamic} introduce a graph convolutional architecture that uses nearest-neighbor graphs derived from point sets to generate rich feature representations.

PointCleanNet (PCN)~\cite{rakotosaona2020pointcleannet} employs a two-level network to first remove outlier points and then learn the motion coordinates of the noisy point cloud, transforming it into a cleaner version. 
Pointfilter~\cite{zhang2020pointfilter} uses clean normals as a supervisory signal to analyze the model's latent surface information, effectively removing noise while preserving the sharp edges of the point cloud. 

Luo $et$ $al.$ introduce ScoreDenoise, a score-based denoising method that models the gradient log of the noise-convolved probability distribution for point cloud patches~\cite{luo2021score}. 
Chen $et$ $al.$\cite{chen2019multi} propose a multi-block denoising approach based on low-rank matrix recovery with graph constraints and later developed RePCD~\cite{chen2022repcd}, a feature-aware recurrent network.
Edirimuni $et$ $al.$~\cite{de2023iterativepfn} criticize RePCD for its lack of iterative noise reduction during testing and propose IterativePFN, an iterative point cloud filtering network that explicitly models the iterative filtering process internally. 
Wei $et$ $al.$\cite{wei2024pathnet} propose PathNet, a path-selective point cloud denoising framework that adapts its approach based on varying levels of noise and the distinct geometric structures of the points.

\subsection{Spiking Neural Networks}
SNNs are regarded as the third generation of neural networks, inspired by brain-like computing processes that use event-driven signals to update neuronal nodes~\cite{cao2024spiking}. 
Unlike conventional ANNs, spiking neurons operate on discrete-time events rather than continuous values, making SNNs more energy and memory-efficient on embedded platforms~\cite{wu2019direct}.

One significant challenge with SNNs is the effective training and optimization of network parameters.
Currently, there are two primary methods for developing deep SNN models: ANN-to-SNN conversion and direct training. 
In ANN-to-SNN conversion, ReLU activation layers are replaced with spiking neurons to replicate the behavior of the original ANN.
However, these converte SNNs often require substantial inference time and memory, resulting in increased latency and decreased energy efficiency, which undermines the advantages of spiking models~\cite{roy2019towards}. 
In contrast, direct training involves designing surrogate gradients for backpropagation or using gradients with respect to membrane potentials to train SNNs from scratch. 
Models trained directly tend to reduce spiking time latency and are more suitable for practical applications. 
However, for large-scale tasks, they often do not match the accuracy of conversion-based approaches or ANNs~\cite{roy2019towards}.

To enhance SNN performance and bridge the gap between ANNs and SNNs, several advancements have been made. 
Wu $et$ $al.$~\cite{wu2019direct} introduce neuron normalization to balance firing rates and preserve important information.
The QIF neuron~\cite{brunel2003firing} simulates neuronal electrical activity by extending the standard Integrate-and-Fire (IF) neuron with a quadratic nonlinearity, offering a more accurate representation of the neuron's membrane potential.
The KLIF neuron~\cite{jiang2023klif} is a novel k-based leaky integrate-and-fire (LIF) neuron designed to enhance the learning capabilities of spiking neural networks.
Spiking neurons with noise-injected dynamics are considered more biologically realistic. 
Rao $et$ $al.$~\cite{rao2004bayesian} develope small noise-spiking neural networks to perform probabilistic reasoning, effectively improving network robustness. 
However, integrating these methods into arbitrary network architectures remains challenging.

\subsection{Spiking Neural Networks on Point Cloud}
Recent research efforts are exploring the application of SNNs in point cloud processing. 
Lan $et$ $al.$~\cite{lan2023efficient} propose an efficient unified ANN-SNN conversion method for point cloud and image classification, significantly reducing time steps for a fast and lossless transformation. 
Ren $et$ $al.$~\cite{ren2024spiking} extend PointNet to SNNs and develop Spiking PointNet, while Wu $et$ $al.$~\cite{wu2024point} introduce a point-to-spike residual learning network for point cloud classification. 
Despite these advances, there are relatively few studies combining SNNs with point cloud data, and most focus on classification tasks. 
To our knowledge, we are the first to apply SNNs to point cloud denoising.


%-------------------------------------------------------------