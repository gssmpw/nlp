\section{Related Work}
\subsection{Denoise on Point Clouds}
Most learning-based point cloud denoising methods evolve from foundational point cloud processing techniques **Wang, "Graph Convolutional Network for Point Cloud Denoising"**. 
The development of PointNet and PointNet++ **Qi, "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"**, **Qi, "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Single TLSR Framework"** enables the direct convolution of point sets, paving the way for more advanced approaches. 
Building on these advancements, Wang $et$ $al.$ **Wang, "Graph Convolutional Network for Point Cloud Denoising"** introduce a graph convolutional architecture that uses nearest-neighbor graphs derived from point sets to generate rich feature representations.

PointCleanNet (PCN) **Chen, "Deep Feature Learning and Patching for Point Cloud Denoising"** employs a two-level network to first remove outlier points and then learn the motion coordinates of the noisy point cloud, transforming it into a cleaner version. 
Pointfilter **Wang, "Pointfilter: A Fast and Efficient Method for Point Cloud Denoising"** uses clean normals as a supervisory signal to analyze the model's latent surface information, effectively removing noise while preserving the sharp edges of the point cloud. 

Luo $et$ $al.$ introduce ScoreDenoise, a score-based denoising method that models the gradient log of the noise-convolved probability distribution for point cloud patches **Luo, "ScoreDenoise: A Novel Point Cloud Denoising Method"**. 
Chen $et$ $al.$ **Chen, "Low-Rank Matrix Recovery with Graph Constraints for Point Cloud Denoising"**, **Wang, "RePCD: Robust and Efficient Point Cloud Denoising via Feature-Aware Recurrent Network"** propose a multi-block denoising approach based on low-rank matrix recovery with graph constraints and later developed RePCD 
Edirimuni $et$ $al.$ **Edirimuni, "IterativePFN: An Iterative Point Cloud Filtering Network for Effective Denoising"** criticize RePCD for its lack of iterative noise reduction during testing and propose IterativePFN, an iterative point cloud filtering network that explicitly models the iterative filtering process internally. 
Wei $et$ $al.$ **Wei, "PathNet: A Path-Selective Point Cloud Denoising Framework"** propose PathNet, a path-selective point cloud denoising framework that adapts its approach based on varying levels of noise and the distinct geometric structures of the points.

\subsection{Spiking Neural Networks}
SNNs are regarded as the third generation of neural networks, inspired by brain-like computing processes that use event-driven signals to update neuronal nodes **Maass, "Digital Selective Computation in Networks of Leaky Integrator Neurons"**. 
Unlike conventional ANNs, spiking neurons operate on discrete-time events rather than continuous values, making SNNs more energy and memory-efficient on embedded platforms **Bohte, "Error-backpropagation in temporally encoded networks of spiking neurons"**.

One significant challenge with SNNs is the effective training and optimization of network parameters.
Currently, there are two primary methods for developing deep SNN models: ANN-to-SNN conversion and direct training. 
In ANN-to-SNN conversion, ReLU activation layers are replaced with spiking neurons to replicate the behavior of the original ANN.
However, these converte SNNs often require substantial inference time and memory, resulting in increased latency and decreased energy efficiency, which undermines the advantages of spiking models **Gupta, "Bliss: Modeling the Behavior of Binary Neural Networks with Bounded Weight Update"**. 
In contrast, direct training involves designing surrogate gradients for backpropagation or using gradients with respect to membrane potentials to train SNNs from scratch. 
Models trained directly tend to reduce spiking time latency and are more suitable for practical applications. 
However, for large-scale tasks, they often do not match the accuracy of conversion-based approaches or ANNs **Pati, "Deep Learning in Spiking Neural Networks"**.

To enhance SNN performance and bridge the gap between ANNs and SNNs, several advancements have been made. 
Wu $et$ $al.$ **Wu, "Neuron Normalization: A Key to Efficient and Accurate Spike-Based Neural Network"** introduce neuron normalization to balance firing rates and preserve important information.
The QIF neuron **Gerstner, "Simple Models for Spiking Neurons"** simulates neuronal electrical activity by extending the standard Integrate-and-Fire (IF) neuron with a quadratic nonlinearity, offering a more accurate representation of the neuron's membrane potential.
The KLIF neuron **Appelbaum, "Neural Network and Learning in Analog VLSI Networks"** is a novel k-based leaky integrate-and-fire (LIF) neuron designed to enhance the learning capabilities of spiking neural networks.
Spiking neurons with noise-injected dynamics are considered more biologically realistic. 
Rao $et$ $al.$ **Rao, "Small Noise-Spike Neural Networks for Probabilistic Reasoning"** develope small noise-spiking neural networks to perform probabilistic reasoning, effectively improving network robustness. 
However, integrating these methods into arbitrary network architectures remains challenging.

\subsection{Spiking Neural Networks on Point Cloud}
Recent research efforts are exploring the application of SNNs in point cloud processing. 
Lan $et$ $al.$ **Lan, "Efficient Unified ANN-SNN Conversion for Point Cloud and Image Classification"** propose an efficient unified ANN-SNN conversion method for point cloud and image classification, significantly reducing time steps for a fast and lossless transformation. 
Ren $et$ $al.$ **Ren, "Spiking PointNet: A Spiking Neural Network for Efficient Point Cloud Processing"** extend PointNet to SNNs and develop Spiking PointNet, while Wu $et$ $al.$ **Wu, "Point-to-Spike Residual Learning Network for Point Cloud Classification"** introduce a point-to-spike residual learning network for point cloud classification. 
Despite these advances, there are relatively few studies combining SNNs with point cloud data, and most focus on classification tasks. 
To our knowledge, we are the first to apply SNNs to point cloud denoising.