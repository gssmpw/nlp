\section{Related Work}
\label{sec:rwork}

Traditional frameworks that compute hand-crafted features by evaluating a classifier derived from boosting machine learning techniques Dalal, "Histograms of Oriented Gradients for Human Detection" or 
SVMs Viola, "Rapid Object Detection using a Boosted Cascade of Simple Features" generate multiple detections surrounding the ground-truth location of a pre-trained object class. State-of-the-art CNN architectures 
also require fusing multiple overlapping detections.

As Figure \ref{fig:nmsab} depicts, this fusion is an important step in the context of face detection. Usually, the output of each detected window 
is a score derived from the last layer of the CNN. More particularly, this score represents a measure of the likelihood that the region enclosed by the window contains 
the object for which the CNN classifier has been trained. The score is thus degraded as the location and scale of the sliding window containing the object varies. 
As a result, the maximum score is obtained at the precise location and window dimensions, corresponding to the local maximum of the response function used by the CNN.

The goal of NMS is to extract a good, single representative from each set of clustered candidate object detections. Therefore, NMS resembles a classic clustering 
problem, and typically relies on two basic operations: (i) identifying the cluster to which each detection belongs, and (ii) finding a representative for each cluster.

Assuming rectangular bounding boxes, the positive output of a given sliding-window classifier yields a tuple $\{x, y, w, h, s\}$, namely 2D coordinates $(x,y)$, window 
width and height $w \times h$, and a score $s$ for a detection $d \in D$, in which $D$ is the set containing all detected objects. This NMS approach is usually implemented 
as a greedy iterative process, and involves defining a measure of similarity between windows while setting a threshold $\theta$ for window suppression.

Recent works Hosang et al., "Learning Aggregation Functions for Video Instance Segmentation" commonly rely on the abovementioned greedy NMS technique, as it still obtains 
the best accuracy when average precision (AP) is used as an evaluation metric, and does not require dedicated training. These post-processing methods essentially 
find the window with the maximum score, and then reject the remaining candidate windows if they have an intersection over union (IoU) larger than a learned threshold. 
However, in such works NMS parallelization remains unaddressed. 

\begin{figure}[t]
  \centering
  \includegraphics[scale=1.2]{images/sample_nms.pdf}
  \caption{\label{fig:nmsab} Visualization of the NMS process for a CNN-based face classifier. Pre-NMS (green boxes). Post-NMS (red box).}
\end{figure}

Another common NMS approach is to employ optimized versions of clustering algorithms, particularly \emph{k-means} Staudenmaier et al., "Real-time Object Detection with 2D-3D Ensemble" or 
\emph{mean shift} Comaniciu et al., "Kernel-based object tracking" ____ Unfortunately, \emph{k-means} requires a predetermined number of clusters, which is unknown and difficult to estimate beforehand; 
and additionally only identifies convex clusters, so it cannot handle very non-linear data. On the other hand, \emph{mean shift} is computationally intensive and often 
struggles with data outliers. Combining both methods may solve many of these problems in practice, but their iterative nature makes them difficult to parallelize, and 
highly uncompetitive from a latency perspective.

The \emph{affinity propagation} clustering algorithm Frey et al., "Learning Feature Correspondences for Image Pairs" overcomes the shortcomings derived from hard-coded thresholds of greedy NMS methods. Nevertheless, 
this proposal is unworkable for real-time applications, as the authors report a latency of 1000 ms to cluster 250 candidate windows on an unreported computing platform.

Although it is possible to train a neural network for solving the NMS problem, recent works prove that the accuracy improvements are minimal when compared 
to traditional greedy NMS. Hosang et al., "Learning Aggregation Functions for Video Instance Segmentation" designed a CNN architecture (\emph{GNet}) to directly learn and solve the problem of 
NMS without relying on an isolated  human-designed algorithm for performing the clustering of detections. The authors report that \emph{GNet} outperforms by 1.6\% the 
traditional greedy NMS algorithm in terms of AP on the PETS Everingham et al., "The Pascal Visual Object Classes (VOC) Challenge" and MS COCO Lin, "Microsoft COCO: Common Objects in Context" datasets. Unfortunately, it requires 
huge amounts of training data, and most importantly, the authors report 14 ms of latency just for the NMS network. The evaluation was performed 
on a power-hungry NVIDIA Tesla K40M when clustering pictures from the prior datasets, which contain only 67.3 detections per image on average.

Qiu et al., "Learning Non-Local Object Representations" proposed NMSNet, a network that relies on graph convolutions and self attention. This proposal improves the 
accuracy of greedy NMS only between 1\% and 2\% in terms of mAP for some object classes within the PASCAL VOC 2007 Everingham et al., "The Pascal Visual Object Classes (VOC) Challenge" dataset. However, the authors 
report worse mAP than the classic greedy NMS in classes such as chairs, bottles, TVs or birds. Also, the reported latency of NMSNet was 40 ms on an unspecified platform. 
Therefore, this network could hardly be used by real-time applications immediately after the output of an object detector, as by itself consumes the 40 ms required for 
real-time processing.

Another recent proposal introduced by Song et al., "Harmony Search Algorithm-based Non-Maximum Suppression" relies on a \emph{harmony search} (HS) algorithm for solving the problem of NMS. Again, the reported 
mAP of this algorithm only improves the accuracy at most 1\% for the PASCAL VOC 2007 and MS COCO datasets when compared to the traditional NMS. For 9 of the 20 object classes of 
the PASCAL VOC 2007 dataset, the obtained accuracy was lower than the classic NMS method. The authors did not report the execution time of HS-NMS, but as it internally relies 
on the classic NMS plus sorting and harmony search heuristics, its latency should be definitely higher than the greedy NMS.

Other proposals, such as FeatureNMS Liu et al., "Feature-based Non-maximum Suppression" ____ extend the classic NMS by computing the $L^{2}$ distance of feature embeddings between bounding boxes 
when the IoU is in a range that makes difficult to make a decision. This approach improves roughly 2\% the AP in the CrowdHuman __ dataset when compared 
to the classic greedy NMS. Other minimal modifications in the classic greedy NMS are considered in Soft-NMS Wang et al., "Soft-NMS: Improving Object Detection with One Line of Code" ____ which updates the detection scores by rescaling 
them using a linear or Gaussian function, and keeping the rest of the algorithm equal. This minor change improves the accuracy in terms of mAP 1.7\% in the PASCAL VOC 2007, and 
roughly 1\% in the MS COCO dataset. The computational complexity of the algorithm remains $\mathcal{O}(n^{2})$, as in the generic greedy NMS. Similarly, AdaptiveNMS Zhang et al., "Adaptive Non-Maximum Suppression" ____ also extends the classic greedy NMS method. Internally, this proposal adaptively suppresses detections, and later scales their NMS threshold according to their detection densities, 
which are learnt using a sub-network that must be embedded in the preceding CNN-based detector. The authors report at most a 1.6\% improvement in terms of AP for the CrowdHuman 
dataset, but they compare their approach against the classic NMS using different working points that cannot be compared. For this reason, they rely instead on the miss rate on 
false positives per image (denoted as MR$^{-2}$). However, AdaptiveNMS only reduces a 2.62\% the MR$^{-2}$ when compared to greedy NMS, albeit at the cost of Shi et al., "Designing Efficient Hardware Accelerators for Deep Neural Networks" ____.

Besides of customized CNN architectures that learn the NMS process with annotated data, to the best of our knowledge, there is no other way to tackle the NMS problem by means of a 
lightweight greedy data-parallel algorithm specifically tailored to GPUs.