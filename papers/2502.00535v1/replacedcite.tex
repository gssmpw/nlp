\section{Related Work}
\label{sec:rwork}

Traditional frameworks that compute hand-crafted features by evaluating a classifier derived from boosting machine learning techniques ____ or 
SVMs ____ generate multiple detections surrounding the ground-truth location of a pre-trained object class. State-of-the-art CNN architectures 
also require fusing multiple overlapping detections.

As Figure \ref{fig:nmsab} depicts, this fusion is an important step in the context of face detection. Usually, the output of each detected window 
is a score derived from the last layer of the CNN. More particularly, this score represents a measure of the likelihood that the region enclosed by the window contains 
the object for which the CNN classifier has been trained. The score is thus degraded as the location and scale of the sliding window containing the object varies. 
As a result, the maximum score is obtained at the precise location and window dimensions, corresponding to the local maximum of the response function used by the CNN.

The goal of NMS is to extract a good, single representative from each set of clustered candidate object detections. Therefore, NMS resembles a classic clustering 
problem, and typically relies on two basic operations: (i) identifying the cluster to which each detection belongs, and (ii) finding a representative for each cluster.

Assuming rectangular bounding boxes, the positive output of a given sliding-window classifier yields a tuple $\{x, y, w, h, s\}$, namely 2D coordinates $(x,y)$, window 
width and height $w \times h$, and a score $s$ for a detection $d \in D$, in which $D$ is the set containing all detected objects. This NMS approach is usually implemented 
as a greedy iterative process, and involves defining a measure of similarity between windows while setting a threshold $\theta$ for window suppression.

Recent works ____ commonly rely on the abovementioned greedy NMS technique, as it still obtains 
the best accuracy when average precision (AP) is used as an evaluation metric, and does not require dedicated training. These post-processing methods essentially 
find the window with the maximum score, and then reject the remaining candidate windows if they have an intersection over union (IoU) larger than a learned threshold. 
However, in such works NMS parallelization remains unaddressed. 

\begin{figure}[t]
  \centering
  \includegraphics[scale=1.2]{images/sample_nms.pdf}
  \caption{\label{fig:nmsab} Visualization of the NMS process for a CNN-based face classifier. Pre-NMS (green boxes). Post-NMS (red box).}
\end{figure}

Another common NMS approach is to employ optimized versions of clustering algorithms, particularly \emph{k-means} ____ or 
\emph{mean shift} ____. Unfortunately, \emph{k-means} requires a predetermined number of clusters, which is unknown and difficult to estimate beforehand; 
and additionally only identifies convex clusters, so it cannot handle very non-linear data. On the other hand, \emph{mean shift} is computationally intensive and often 
struggles with data outliers. Combining both methods may solve many of these problems in practice, but their iterative nature makes them difficult to parallelize, and 
highly uncompetitive from a latency perspective.

The \emph{affinity propagation} clustering algorithm ____ overcomes the shortcomings derived from hard-coded thresholds of greedy NMS methods. Nevertheless, 
this proposal is unworkable for real-time applications, as the authors report a latency of 1000 ms to cluster 250 candidate windows on an unreported computing platform.

Although it is possible to train a neural network for solving the NMS problem, recent works prove that the accuracy improvements are minimal when compared 
to traditional greedy NMS. Hosang et al. ____ designed a CNN architecture (\emph{GNet}) to directly learn and solve the problem of 
NMS without relying on an isolated  human-designed algorithm for performing the clustering of detections. The authors report that \emph{GNet} outperforms by 1.6\% the 
traditional greedy NMS algorithm in terms of AP on the PETS ____ and MS COCO ____ datasets. Unfortunately, it requires 
huge amounts of training data, and most importantly, the authors report 14 ms of latency just for the NMS network. The evaluation was performed 
on a power-hungry NVIDIA Tesla K40M when clustering pictures from the prior datasets, which contain only 67.3 detections per image on average.

Qiu et al.____ proposed NMSNet, a network that relies on graph convolutions and self attention. This proposal improves the 
accuracy of greedy NMS only between 1\% and 2\% in terms of mAP for some object classes within the PASCAL VOC 2007____ dataset. However, the authors 
report worse mAP than the classic greedy NMS in classes such as chairs, bottles, TVs or birds. Also, the reported latency of NMSNet was 40 ms on an unspecified platform. 
Therefore, this network could hardly be used by real-time applications immediately after the output of an object detector, as by itself consumes the 40 ms required for 
real-time processing.

Another recent proposal introduced by Song et al.____ relies on a \emph{harmony search} (HS) algorithm for solving the problem of NMS. Again, the reported 
mAP of this algorithm only improves the accuracy at most 1\% for the PASCAL VOC 2007 and MS COCO datasets when compared to the traditional NMS. For 9 of the 20 object classes of 
the PASCAL VOC 2007 dataset, the obtained accuracy was lower than the classic NMS method. The authors did not report the execution time of HS-NMS, but as it internally relies 
on the classic NMS plus sorting and harmony search heuristics, its latency should be definitely higher than the greedy NMS.

Other proposals, such as FeatureNMS____, extend the classic NMS by computing the $L^{2}$ distance of feature embeddings between bounding boxes 
when the IoU is in a range that makes difficult to make a decision. This approach improves roughly 2\% the AP in the CrowdHuman____ dataset when compared 
to the classic greedy NMS. Other minimal modifications in the classic greedy NMS are considered in Soft-NMS____, which updates the detection scores by rescaling 
them using a linear or Gaussian function, and keeping the rest of the algorithm equal. This minor change improves the accuracy in terms of mAP 1.7\% in the PASCAL VOC 2007, and 
roughly 1\% in the MS COCO dataset. The computational complexity of the algorithm remains $\mathcal{O}(n^{2})$, as in the generic greedy NMS. Similarly, AdaptiveNMS____ 
also extends the classic greedy NMS method. Internally, this proposal adaptively suppresses detections, and later scales their NMS threshold according to their detection densities, 
which are learnt using a sub-network that must be embedded in the preceding CNN-based detector. The authors report at most a 1.6\% improvement in terms of AP for the CrowdHuman 
dataset, but they compare their approach against the classic NMS using different working points that cannot be compared. For this reason, they rely instead on the miss rate on 
false positives per image (denoted as MR$^{-2}$). However, AdaptiveNMS only reduces a 2.62\% the MR$^{-2}$ when compared to greedy NMS, albeit at the cost of modifying the CNN 
used for performing object localization.

\begin{figure}[t]
\centering
\includegraphics[scale=0.21]{figures/matrix.pdf}
\caption{\label{fig:mapkernel} Visualization of our GPU-NMS proposal: 
(a) example candidates generated by a detector (3 objects, 9 detections); 
boolean matrix after (b) clustering and (c) cancellation of 
non-representatives; and (d) result after AND reduction.}
\end{figure}

In view of the recent works, the classic hand-crafted greedy NMS algorithm still remains competitive and effective in terms of mAP, as it outperforms state-of-the-art neural network-based 
approaches in several object classes. Therefore, it is unclear how NMS methods based on neural networks are going to evolve to support image workloads featuring thousands of 
simultaneous detections, while clustering them in a few milliseconds to efficiently target real-time applications. Moreover, these NMS networks should also be rearchitected for embedded 
GPU platforms to enable the use cases discussed earlier, as they were evaluated on high-end discrete GPUs. On the other hand, minor variations of the classic NMS such 
as FeatureNMS, Soft-NMS or AdaptiveNMS share the same core and inner workings of the greedy algorithm. As a result, the parallelization pattern studied in this paper is also 
applicable to these recent serial-based NMS proposals, as it only involves minor modifications during the merging process of detections.

Finally, Shi et al.____ implemented a power-efficient hardware microarchitecture for NMS directly in silicon (28 nanometer). This low-level chip design is inspired by 
our previous work ____, and as such is cited by the authors. By following this strategy, the designed chip is capable of merging 1000 candidate boxes in 12.79 $\mu s$ by consuming only 6.142 mW. These results further reinforce the validity of our algorithm. 

Besides of customized CNN architectures that learn the NMS process with annotated data, to the best of our knowledge, there is no other way to tackle the NMS problem by means of a 
lightweight greedy data-parallel algorithm specifically tailored to GPUs.