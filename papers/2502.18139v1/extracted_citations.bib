@misc{blendfilter,
	title = {{BlendFilter}: {Advancing} {Retrieval}-{Augmented} {Large} {Language} {Models} via {Query} {Generation} {Blending} and {Knowledge} {Filtering}},
	shorttitle = {{BlendFilter}},
	url = {http://arxiv.org/abs/2402.11129},
	abstract = {Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often face challenges with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clearly indicate that our innovative BlendFilter surpasses state-of-the-art baselines significantly.},
	language = {en},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Wang, Haoyu and Zhao, Tuo and Gao, Jing},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11129 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Wang 等 - 2024 - BlendFilter Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and K.pdf:D\:\\Documents\\zotero\\storage\\JHD8S3MW\\Wang 等 - 2024 - BlendFilter Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and K.pdf:application/pdf},
}

@misc{crag,
	title = {Corrective {Retrieval} {Augmented} {Generation}},
	url = {http://arxiv.org/abs/2401.15884},
	doi = {10.48550/arXiv.2401.15884},
	abstract = {Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.},
	language = {en-US},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Yan, Shi-Qi and Gu, Jia-Chen and Zhu, Yun and Ling, Zhen-Hua},
	month = feb,
	year = {2024},
	note = {arXiv:2401.15884 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\MSRUVXQ6\\Yan 等 - 2024 - Corrective Retrieval Augmented Generation.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\KL9ECXCF\\2401.html:text/html},
}

@inproceedings{hybrid_code,
	address = {Seoul South Korea},
	title = {Retrieval-based neural source code summarization},
	isbn = {978-1-4503-7121-6},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380383},
	doi = {10.1145/3377811.3380383},
	language = {en},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Liu, Xudong},
	month = jun,
	year = {2020},
	pages = {1385--1397},
}

@misc{hybrid_dense,
	title = {Unraveling and {Mitigating} {Retriever} {Inconsistencies} in {Retrieval}-{Augmented} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2405.20680},
	doi = {10.48550/arXiv.2405.20680},
	abstract = {Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example-level performance inconsistency exists not only between retrieval-augmented and retrieval-free LM but also among different retrievers. To understand this phenomenon, we investigate the degeneration behavior of RALMs and theoretically decompose it into four categories. Further analysis based on our decomposition reveals that the innate difference in knowledge sources and the unpredictable degeneration of the reader model contribute most to the inconsistency. Drawing from our analysis, we introduce Ensemble of Retrievers (EoR), a trainable framework that can adaptively retrieve from different knowledge sources and effectively decrease unpredictable reader errors. Our experiments on Open Domain Question Answering show that EoR substantially improves performance over the RALM with a single retriever by considerably reducing inconsistent behaviors.},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Li, Mingda and Li, Xinyu and Chen, Yifan and Xuan, Wenfeng and Zhang, Weinan},
	month = jun,
	year = {2024},
	note = {arXiv:2405.20680 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\34JJFNLE\\Li 等 - 2024 - Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\E8X4R8TM\\2405.html:text/html},
}

@misc{hyde,
	title = {Precise {Zero}-{Shot} {Dense} {Retrieval} without {Relevance} {Labels}},
	url = {http://arxiv.org/abs/2212.10496},
	doi = {10.48550/arXiv.2212.10496},
	abstract = {While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings{\textasciitilde}(HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder{\textasciitilde}(e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages{\textasciitilde}(e.g. sw, ko, ja).},
	language = {en-US},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10496 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\LMEF2V4A\\Gao 等 - 2022 - Precise Zero-Shot Dense Retrieval without Relevance Labels.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\R5WFYM6H\\2212.html:text/html},
}

@misc{ircot,
	title = {Interleaving {Retrieval} with {Chain}-of-{Thought} {Reasoning} for {Knowledge}-{Intensive} {Multi}-{Step} {Questions}},
	url = {http://arxiv.org/abs/2212.10509},
	doi = {10.48550/arXiv.2212.10509},
	abstract = {Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, {\textbackslash}textit\{what to retrieve\} depends on {\textbackslash}textit\{what has already been derived\}, which in turn may depend on {\textbackslash}textit\{what was previously retrieved\}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning. Code, data, and prompts are available at {\textbackslash}url\{https://github.com/stonybrooknlp/ircot\}},
	language = {en-US},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
	month = jun,
	year = {2023},
	note = {arXiv:2212.10509 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\4JGU7BJH\\Trivedi 等 - 2023 - Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\85YUAI2X\\2212.html:text/html},
}

@inproceedings{iter_retgen,
	address = {Singapore},
	title = {Enhancing {Retrieval}-{Augmented} {Large} {Language} {Models} with {Iterative} {Retrieval}-{Generation} {Synergy}},
	url = {https://aclanthology.org/2023.findings-emnlp.620},
	doi = {10.18653/v1/2023.findings-emnlp.620},
	abstract = {Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to guide retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner: a model's response to a task input shows what might be needed to finish the task, and thus can serve as an informative context for retrieving more relevant knowledge which in turn helps generate a better response in another iteration. Compared with recent work which interleaves retrieval with generation when completing a single output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.},
	urldate = {2024-05-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {9248--9274},
	file = {Full Text PDF:D\:\\Documents\\zotero\\storage\\VCL5Y7ZN\\Shao 等 - 2023 - Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy.pdf:application/pdf},
}

@misc{itrg,
	title = {Retrieval-{Generation} {Synergy} {Augmented} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.05149},
	doi = {10.48550/arXiv.2310.05149},
	abstract = {Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.},
	language = {en-US},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Feng, Zhangyin and Feng, Xiaocheng and Zhao, Dezhi and Yang, Maojin and Qin, Bing},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05149 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\Q5BGJ6YC\\Feng 等 - 2023 - Retrieval-Generation Synergy Augmented Large Language Models.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\4UM54XFX\\2310.html:text/html},
}

@misc{mind_search,
	title = {{MindSearch}: {Mimicking} {Human} {Minds} {Elicits} {Deep} {AI} {Searcher}},
	shorttitle = {{MindSearch}},
	url = {http://arxiv.org/abs/2407.20183},
	doi = {10.48550/arXiv.2407.20183},
	abstract = {Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and Perplexity.ai applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.},
	language = {en-US},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Chen, Zehui and Liu, Kuikun and Wang, Qiuchen and Liu, Jiangning and Zhang, Wenwei and Chen, Kai and Zhao, Feng},
	month = jul,
	year = {2024},
	note = {arXiv:2407.20183 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\5P7DZEHE\\Chen 等 - 2024 - MindSearch Mimicking Human Minds Elicits Deep AI Searcher.pdf:application/pdf},
}

@misc{query2doc,
	title = {Query2doc: {Query} {Expansion} with {Large} {Language} {Models}},
	shorttitle = {Query2doc},
	url = {http://arxiv.org/abs/2303.07678},
	abstract = {This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudodocuments. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3\% to 15\% on ad-hoc IR datasets, such as MSMARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.},
	language = {en},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Wang, Liang and Yang, Nan and Wei, Furu},
	month = oct,
	year = {2023},
	note = {arXiv:2303.07678 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Wang 等 - 2023 - Query2doc Query Expansion with Large Language Models.pdf:D\:\\Documents\\zotero\\storage\\Y2CBNR4W\\Wang 等 - 2023 - Query2doc Query Expansion with Large Language Models.pdf:application/pdf},
}

@misc{query_rewrite,
	title = {Query {Rewriting} for {Retrieval}-{Augmented} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.14283},
	doi = {10.48550/arXiv.2305.14283},
	abstract = {Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Ma, Xinbei and Gong, Yeyun and He, Pengcheng and Zhao, Hai and Duan, Nan},
	month = oct,
	year = {2023},
	note = {arXiv:2305.14283 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\XWU6RHVB\\Ma 等 - 2023 - Query Rewriting for Retrieval-Augmented Large Lang.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\XYUYGD2J\\2305.html:text/html},
}

@misc{rafe,
	title = {{RaFe}: {Ranking} {Feedback} {Improves} {Query} {Rewriting} for {RAG}},
	shorttitle = {{RaFe}},
	url = {http://arxiv.org/abs/2405.14431},
	abstract = {As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose RaFe, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, RaFe provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that RaFe can obtain better performance than baselines.},
	language = {en},
	urldate = {2024-06-24},
	publisher = {arXiv},
	author = {Mao, Shengyu and Jiang, Yong and Chen, Boli and Li, Xiao and Wang, Peng and Wang, Xinyu and Xie, Pengjun and Huang, Fei and Chen, Huajun and Zhang, Ningyu},
	month = may,
	year = {2024},
	note = {arXiv:2405.14431 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Mao 等 - 2024 - RaFe Ranking Feedback Improves Query Rewriting for RAG.pdf:D\:\\Documents\\zotero\\storage\\J6BU355I\\Mao 等 - 2024 - RaFe Ranking Feedback Improves Query Rewriting for RAG.pdf:application/pdf},
}

@inproceedings{rapgen,
	address = {San Francisco CA USA},
	title = {{RAP}-{Gen}: {Retrieval}-{Augmented} {Patch} {Generation} with {CodeT5} for {Automatic} {Program} {Repair}},
	isbn = {9798400703270},
	shorttitle = {{RAP}-{Gen}},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616256},
	doi = {10.1145/3611643.3616256},
	language = {en},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wang, Weishi and Wang, Yue and Joty, Shafiq and Hoi, Steven C.H.},
	month = nov,
	year = {2023},
	pages = {146--158},
	file = {已提交版本:D\:\\Documents\\zotero\\storage\\LRTZ5DXC\\Wang 等 - 2023 - RAP-Gen Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair.pdf:application/pdf},
}

@inproceedings{reacc,
	address = {Dublin, Ireland},
	title = {{ReACC}: {A} {Retrieval}-{Augmented} {Code} {Completion} {Framework}},
	shorttitle = {{ReACC}},
	url = {https://aclanthology.org/2022.acl-long.431},
	doi = {10.18653/v1/2022.acl-long.431},
	abstract = {Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing ”external” context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark.},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lu, Shuai and Duan, Nan and Han, Hojae and Guo, Daya and Hwang, Seung-won and Svyatkovskiy, Alexey},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {6227--6240},
	file = {Full Text PDF:D\:\\Documents\\zotero\\storage\\Y4HQ5ER8\\Lu 等 - 2022 - ReACC A Retrieval-Augmented Code Completion Framework.pdf:application/pdf},
}

@misc{rq_rag,
	title = {{RQ}-{RAG}: {Learning} to {Refine} {Queries} for {Retrieval} {Augmented} {Generation}},
	shorttitle = {{RQ}-{RAG}},
	url = {http://arxiv.org/abs/2404.00610},
	doi = {10.48550/arXiv.2404.00610},
	abstract = {Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9{\textbackslash}\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.},
	language = {en-US},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Chan, Chi-Min and Xu, Chunpu and Yuan, Ruibin and Luo, Hongyin and Xue, Wei and Guo, Yike and Fu, Jie},
	month = mar,
	year = {2024},
	note = {arXiv:2404.00610 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\L3NQD6T2\\Chan 等 - 2024 - RQ-RAG Learning to Refine Queries for Retrieval A.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\XTDCI52A\\2404.html:text/html},
}

@misc{unims_rag,
	title = {{UniMS}-{RAG}: {A} {Unified} {Multi}-source {Retrieval}-{Augmented} {Generation} for {Personalized} {Dialogue} {Systems}},
	shorttitle = {{UniMS}-{RAG}},
	url = {http://arxiv.org/abs/2401.13256},
	doi = {10.48550/arXiv.2401.13256},
	abstract = {Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.},
	language = {en-US},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Wang, Hongru and Huang, Wenyu and Deng, Yang and Wang, Rui and Wang, Zezhong and Wang, Yufei and Mi, Fei and Pan, Jeff Z. and Wong, Kam-Fai},
	month = jan,
	year = {2024},
	note = {arXiv:2401.13256 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\H4BU4DD8\\Wang 等 - 2024 - UniMS-RAG A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\XF4PJAMN\\2401.html:text/html},
}

