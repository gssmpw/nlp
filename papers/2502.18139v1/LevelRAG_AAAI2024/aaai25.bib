@article{scaling_law,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	language = {en-US},
	urldate = {2022-02-17},
	journal = {arXiv:2001.08361 [cs, stat]},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.08361},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\YZADFYNQ\\Kaplan 等。 - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\FWXYSJC8\\2001.html:text/html},
}


@article{chinchilla,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over {\textbackslash}nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, {\textbackslash}chinchilla, that uses the same compute budget as {\textbackslash}gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. {\textbackslash}chinchilla uniformly and significantly outperforms {\textbackslash}Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that {\textbackslash}chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, {\textbackslash}chinchilla reaches a state-of-the-art average accuracy of 67.5{\textbackslash}\% on the MMLU benchmark, greater than a 7{\textbackslash}\% improvement over {\textbackslash}gopher.},
	language = {en-US},
	urldate = {2022-04-14},
	journal = {arXiv:2203.15556 [cs]},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.15556
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\5BZMWE3T\\Hoffmann 等。 - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\H3SJFLPH\\2203.html:text/html},
}


@article{llama,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B. We release all our models to the research community1.},
	language = {en},
	journal = {arXiv preprint arXiv:2302.13971},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothee and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
	year = {2023},
	note = {titleTranslation:
titleTranslation:},
	file = {Touvron 等 - LLaMA Open and Efficient Foundation Language Mode.pdf:D\:\\Documents\\zotero\\storage\\5R6MPSJG\\Touvron 等 - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf},
}

@article{llama2,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	language = {en},
	journal = {arXiv preprint arXiv:2307.09288},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
	year = {2023},
	note = {titleTranslation:},
	file = {Touvron 等 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:D\:\\Documents\\zotero\\storage\\ZP65EW53\\Touvron 等 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf},
}

@misc{gpt3,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	language = {en-US},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\PWVWWGBM\\Brown 等 - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\XFNPPSF9\\2005.html:text/html},
}

@misc{phi3,
	title = {Phi-3 {Technical} {Report}: {A} {Highly} {Capable} {Language} {Model} {Locally} on {Your} {Phone}},
	shorttitle = {Phi-3 {Technical} {Report}},
	url = {http://arxiv.org/abs/2404.14219},
	doi = {10.48550/arXiv.2404.14219},
	abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\% and 78\% on MMLU, and 8.7 and 8.9 on MT-bench).},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14219 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\DVLY23ZG\\Abdin 等 - 2024 - Phi-3 Technical Report A Highly Capable Language .pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\4J98MFBN\\2404.html:text/html},
}

@misc{llama3,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	language = {en},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
	month = jul,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:D\:\\Documents\\zotero\\storage\\69P4UKW8\\Dubey 等 - 2024 - The Llama 3 Herd of Models.pdf:application/pdf},
}


@misc{react,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	doi = {10.48550/arXiv.2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\4XJNBVFD\\Yao 等 - 2023 - ReAct Synergizing Reasoning and Acting in Languag.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\FNGW99K7\\2210.html:text/html},
}

@misc{llm4opt,
	title = {Large {Language} {Models} as {Optimizers}},
	url = {http://arxiv.org/abs/2309.03409},
	doi = {10.48550/arXiv.2309.03409},
	abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
	month = sep,
	year = {2023},
	note = {arXiv:2309.03409 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\9N6B33FL\\Yang 等 - 2023 - Large Language Models as Optimizers.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\F2QMV42S\\2309.html:text/html},
}

@misc{llm_subsume_retrieval,
	title = {Can {Long}-{Context} {Language} {Models} {Subsume} {Retrieval}, {RAG}, {SQL}, and {More}?},
	url = {http://arxiv.org/abs/2406.13121},
	abstract = {Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of information offers numerous advantages. It enhances user-friendliness by eliminating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. To assess this paradigm shift, we introduce LOFT, a benchmark of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs' performance on in-context retrieval and reasoning. Our findings reveal LCLMs' surprising ability to rival state-of-the-art retrieval and RAG systems, despite never having been explicitly trained for these tasks. However, LCLMs still face challenges in areas like compositional reasoning that are required in SQL-like tasks. Notably, prompting strategies significantly influence performance, emphasizing the need for continued research as context lengths grow. Overall, LOFT provides a rigorous testing ground for LCLMs, showcasing their potential to supplant existing paradigms and tackle novel tasks as model capabilities scale.},
	language = {en},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Lee, Jinhyuk and Chen, Anthony and Dai, Zhuyun and Dua, Dheeru and Sachan, Devendra Singh and Boratko, Michael and Luan, Yi and Arnold, Sébastien M. R. and Perot, Vincent and Dalmia, Siddharth and others},
	month = jun,
	year = {2024},
	note = {arXiv:2406.13121 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {PDF:D\:\\Documents\\zotero\\storage\\YXAA459U\\Lee 等 - 2024 - Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More.pdf:application/pdf},
}

@misc{bayling,
	title = {{BayLing}: {Bridging} {Cross}-lingual {Alignment} and {Instruction} {Following} through {Interactive} {Translation} for {Large} {Language} {Models}},
	shorttitle = {{BayLing}},
	url = {http://arxiv.org/abs/2306.10968},
	doi = {10.48550/arXiv.2306.10968},
	abstract = {Large language models (LLMs) have demonstrated remarkable prowess in language understanding and generation. Advancing from foundation LLMs to instructionfollowing LLMs, instruction tuning plays a vital role in aligning LLMs to human preferences. However, the existing LLMs are usually focused on English, leading to inferior performance in non-English languages. In order to improve the performance for non-English languages, it is necessary to collect language-specific training data for foundation LLMs and construct language-specific instructions for instruction tuning, both of which are heavy loads. To minimize human workload, we propose to transfer the capabilities of language generation and instruction following from English to other languages through an interactive translation task. We have developed BayLing, an instruction-following LLM by utilizing LLaMA as the foundation LLM and automatically constructing interactive translation instructions for instructing tuning. Extensive assessments demonstrate that BayLing achieves comparable performance to GPT-3.5-turbo, despite utilizing a considerably smaller parameter size of only 13 billion. Experimental results on translation tasks show that BayLing achieves 95\% of single-turn translation capability compared to GPT-4 with automatic evaluation and 96\% of interactive translation capability compared to GPT-3.5-turbo with human evaluation. To estimate the performance on general tasks, we created a multi-turn instruction test set called BayLing-80. The experimental results on BayLing-80 indicate that BayLing achieves 89\% of performance compared to GPT-3.5-turbo. BayLing also demonstrates outstanding performance on knowledge assessment of Chinese GaoKao and English SAT, second only to GPT-3.5-turbo among a multitude of instruction-following LLMs. Demo, homepage, code and models of BayLing are available.},
	language = {en-US},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Zhang, Shaolei and Fang, Qingkai and Zhang, Zhuocheng and Ma, Zhengrui and Zhou, Yan and Huang, Langlin and Bu, Mengyu and Gui, Shangtong and Chen, Yunji and Chen, Xilin and Feng, Yang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.10968 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\BAWCVCGM\\Zhang 等 - 2023 - BayLing Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\4DIMR3RI\\2306.html:text/html},
}

@misc{llm4code,
	title = {A {Survey} on {Large} {Language} {Models} for {Code} {Generation}},
	url = {http://arxiv.org/abs/2406.00515},
	abstract = {Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the widely recognized HumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource website (https://codellm.github.io) to continuously document and disseminate the most recent advances in the field.},
	language = {en},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
	month = jun,
	year = {2024},
	note = {arXiv:2406.00515 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {PDF:D\:\\Documents\\zotero\\storage\\AB3RZ3QE\\Jiang 等 - 2024 - A Survey on Large Language Models for Code Generation.pdf:application/pdf},
}


@misc{when_not_trust_llm,
	title = {When {Not} to {Trust} {Language} {Models}: {Investigating} {Effectiveness} of {Parametric} and {Non}-{Parametric} {Memories}},
	shorttitle = {When {Not} to {Trust} {Language} {Models}},
	url = {http://arxiv.org/abs/2212.10511},
	doi = {10.48550/arXiv.2212.10511},
	abstract = {Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only when necessary. Experimental results show that this significantly improves models' performance while reducing the inference costs.},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = jul,
	year = {2023},
	note = {arXiv:2212.10511 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {全文:D\:\\Documents\\zotero\\storage\\V9QRF4PZ\\Mallen 等 - 2023 - When Not to Trust Language Models Investigating E.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\ZIE7DGZU\\2212.html:text/html},
}


@misc{hallucination_survey,
	title = {A {Survey} on {Hallucination} in {Large} {Language} {Models}: {Principles}, {Taxonomy}, {Challenges}, and {Open} {Questions}},
	shorttitle = {A {Survey} on {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.05232},
	doi = {10.48550/arXiv.2311.05232},
	abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05232 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\8QURHYQ3\\Huang 等 - 2023 - A Survey on Hallucination in Large Language Models Principles, Taxonomy, Challenges, and Open Quest.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\AZ6VW6W3\\2311.html:text/html},
}


@misc{rag_survey_1,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	abstract = {Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework. In conclusion, the paper delineates prospective avenues for research, including the identification of challenges, the expansion of multi-modalities, and the progression of the RAG infrastructure and its ecosystem.},
	language = {en-US},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Guo, Qianyu and Wang, Meng and Wang, Haofen},
	month = jan,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\7NCGMV2D\\Gao 等 - 2024 - Retrieval-Augmented Generation for Large Language .pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\FSN3SYEP\\2312.html:text/html},
}

@misc{rag_survey_2,
	title = {A {Survey} on {RAG} {Meets} {LLMs}: {Towards} {Retrieval}-{Augmented} {Large} {Language} {Models}},
	shorttitle = {A {Survey} on {RAG} {Meets} {LLMs}},
	url = {http://arxiv.org/abs/2405.06211},
	doi = {10.48550/arXiv.2405.06211},
	abstract = {As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) techniques can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-generated content (AIGC), the powerful capacity of retrieval in RAG in providing additional knowledge enables retrieval-augmented generation to assist existing generative AI in producing high-quality outputs. Recently, large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, retrieval-augmented large language models have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in retrieval-augmented large language models (RA-LLMs), covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we categorize mainstream relevant work by application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research.},
	language = {en-US},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Ding, Yujuan and Fan, Wenqi and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
	month = may,
	year = {2024},
	note = {arXiv:2405.06211 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\IRNYCSAU\\Ding 等 - 2024 - A Survey on RAG Meets LLMs Towards Retrieval-Augmented Large Language Models.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\CY9D2GS7\\2405.html:text/html},
}


@misc{self_rag,
	title = {Self-{RAG}: {Learning} to {Retrieve}, {Generate}, and {Critique} through {Self}-{Reflection}},
	shorttitle = {Self-{RAG}},
	url = {http://arxiv.org/abs/2310.11511},
	abstract = {Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
	language = {en-US},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11511 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\MWYGAQC9\\2310.html:text/html;Full Text PDF:D\:\\Documents\\zotero\\storage\\889ZPM5M\\Asai 等 - 2023 - Self-RAG Learning to Retrieve, Generate, and Crit.pdf:application/pdf},
}

@misc{rq_rag,
	title = {{RQ}-{RAG}: {Learning} to {Refine} {Queries} for {Retrieval} {Augmented} {Generation}},
	shorttitle = {{RQ}-{RAG}},
	url = {http://arxiv.org/abs/2404.00610},
	doi = {10.48550/arXiv.2404.00610},
	abstract = {Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9{\textbackslash}\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.},
	language = {en-US},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Chan, Chi-Min and Xu, Chunpu and Yuan, Ruibin and Luo, Hongyin and Xue, Wei and Guo, Yike and Fu, Jie},
	month = mar,
	year = {2024},
	note = {arXiv:2404.00610 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\L3NQD6T2\\Chan 等 - 2024 - RQ-RAG Learning to Refine Queries for Retrieval A.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\XTDCI52A\\2404.html:text/html},
}

@misc{query_rewrite,
	title = {Query {Rewriting} for {Retrieval}-{Augmented} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.14283},
	doi = {10.48550/arXiv.2305.14283},
	abstract = {Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Ma, Xinbei and Gong, Yeyun and He, Pengcheng and Zhao, Hai and Duan, Nan},
	month = oct,
	year = {2023},
	note = {arXiv:2305.14283 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\XWU6RHVB\\Ma 等 - 2023 - Query Rewriting for Retrieval-Augmented Large Lang.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\XYUYGD2J\\2305.html:text/html},
}


@misc{hybrid_dense,
	title = {Unraveling and {Mitigating} {Retriever} {Inconsistencies} in {Retrieval}-{Augmented} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2405.20680},
	doi = {10.48550/arXiv.2405.20680},
	abstract = {Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example-level performance inconsistency exists not only between retrieval-augmented and retrieval-free LM but also among different retrievers. To understand this phenomenon, we investigate the degeneration behavior of RALMs and theoretically decompose it into four categories. Further analysis based on our decomposition reveals that the innate difference in knowledge sources and the unpredictable degeneration of the reader model contribute most to the inconsistency. Drawing from our analysis, we introduce Ensemble of Retrievers (EoR), a trainable framework that can adaptively retrieve from different knowledge sources and effectively decrease unpredictable reader errors. Our experiments on Open Domain Question Answering show that EoR substantially improves performance over the RALM with a single retriever by considerably reducing inconsistent behaviors.},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Li, Mingda and Li, Xinyu and Chen, Yifan and Xuan, Wenfeng and Zhang, Weinan},
	month = jun,
	year = {2024},
	note = {arXiv:2405.20680 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\34JJFNLE\\Li 等 - 2024 - Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\E8X4R8TM\\2405.html:text/html},
}

@inproceedings{hybrid_code,
	address = {Seoul South Korea},
	title = {Retrieval-based neural source code summarization},
	isbn = {978-1-4503-7121-6},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380383},
	doi = {10.1145/3377811.3380383},
	language = {en},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Liu, Xudong},
	month = jun,
	year = {2020},
	pages = {1385--1397},
}

@inproceedings{reacc,
	address = {Dublin, Ireland},
	title = {{ReACC}: {A} {Retrieval}-{Augmented} {Code} {Completion} {Framework}},
	shorttitle = {{ReACC}},
	url = {https://aclanthology.org/2022.acl-long.431},
	doi = {10.18653/v1/2022.acl-long.431},
	abstract = {Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing ”external” context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark.},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lu, Shuai and Duan, Nan and Han, Hojae and Guo, Daya and Hwang, Seung-won and Svyatkovskiy, Alexey},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {6227--6240},
	file = {Full Text PDF:D\:\\Documents\\zotero\\storage\\Y4HQ5ER8\\Lu 等 - 2022 - ReACC A Retrieval-Augmented Code Completion Framework.pdf:application/pdf},
}

@inproceedings{rapgen,
	address = {San Francisco CA USA},
	title = {{RAP}-{Gen}: {Retrieval}-{Augmented} {Patch} {Generation} with {CodeT5} for {Automatic} {Program} {Repair}},
	isbn = {9798400703270},
	shorttitle = {{RAP}-{Gen}},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616256},
	doi = {10.1145/3611643.3616256},
	language = {en},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wang, Weishi and Wang, Yue and Joty, Shafiq and Hoi, Steven C.H.},
	month = nov,
	year = {2023},
	pages = {146--158},
	file = {已提交版本:D\:\\Documents\\zotero\\storage\\LRTZ5DXC\\Wang 等 - 2023 - RAP-Gen Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair.pdf:application/pdf},
}

@INPROCEEDINGS {bash_explainer,
author = {C. Yu and G. Yang and X. Chen and K. Liu and Y. Zhou},
booktitle = {2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
title = {BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT},
year = {2022},
volume = {},
issn = {},
pages = {82-93},
abstract = {Developers use shell commands for many tasks, such as file system management, network control, and process management. Bash is one of the most commonly used shells and plays an important role in Linux system development and maintenance. Due to the language flexibility of Bash code, developers who are not familiar with Bash often have difficulty understanding the purpose and functionality of Bash code. In this study, we study Bash code comment generation problem and proposed an automatic method BASHEXPLAINER based on two-stage training strategy. In the first stage, we train a Bash encoder by fine-tuning CodeBERT on our constructed Bash code corpus. In the second stage, we first retrieve the most similar code from the code repository for the target code based on semantic and lexical similarity. Then we use the trained Bash encoder to generate two vector representations. Finally, we fuse these two vector representations via the fusion layer and generate the code comment through the decoder. To show the competitiveness of our proposed method, we construct a high-quality corpus by combining the corpus shared in the previous NL2Bash study and the corpus shared in the NLC2CMD competition. This corpus contains 10,592 Bash codes and corresponding comments. Then we selected ten baselines from previous studies on automatic code comment generation, which cover information retrieval methods, deep learning methods, and hybrid methods. The experimental results show that in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L, BASHEXPLAINER can outperform all baselines by at least 8.75%, 9.29%, 4.77% and 3.86%. Then we design ablation experiments to show the component setting rationality of BASHEXPLAINER. Later, we conduct a human study to further show the competitiveness of BASHEXPLAINER. Finally, we develop a browser plug-in based on BASHEXPLAINER to facilitate the understanding of the Bash code for developers.},
keywords = {training;software maintenance;codes;linux;semantics;process control;maintenance engineering},
doi = {10.1109/ICSME55016.2022.00016},
url = {https://doi.ieeecomputersociety.org/10.1109/ICSME55016.2022.00016},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@misc{rafe,
	title = {{RaFe}: {Ranking} {Feedback} {Improves} {Query} {Rewriting} for {RAG}},
	shorttitle = {{RaFe}},
	url = {http://arxiv.org/abs/2405.14431},
	abstract = {As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose RaFe, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, RaFe provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that RaFe can obtain better performance than baselines.},
	language = {en},
	urldate = {2024-06-24},
	publisher = {arXiv},
	author = {Mao, Shengyu and Jiang, Yong and Chen, Boli and Li, Xiao and Wang, Peng and Wang, Xinyu and Xie, Pengjun and Huang, Fei and Chen, Huajun and Zhang, Ningyu},
	month = may,
	year = {2024},
	note = {arXiv:2405.14431 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Mao 等 - 2024 - RaFe Ranking Feedback Improves Query Rewriting for RAG.pdf:D\:\\Documents\\zotero\\storage\\J6BU355I\\Mao 等 - 2024 - RaFe Ranking Feedback Improves Query Rewriting for RAG.pdf:application/pdf},
}


@misc{hyde,
	title = {Precise {Zero}-{Shot} {Dense} {Retrieval} without {Relevance} {Labels}},
	url = {http://arxiv.org/abs/2212.10496},
	doi = {10.48550/arXiv.2212.10496},
	abstract = {While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings{\textasciitilde}(HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder{\textasciitilde}(e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages{\textasciitilde}(e.g. sw, ko, ja).},
	language = {en-US},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10496 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\LMEF2V4A\\Gao 等 - 2022 - Precise Zero-Shot Dense Retrieval without Relevance Labels.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\R5WFYM6H\\2212.html:text/html},
}

@misc{query2doc,
	title = {Query2doc: {Query} {Expansion} with {Large} {Language} {Models}},
	shorttitle = {Query2doc},
	url = {http://arxiv.org/abs/2303.07678},
	abstract = {This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudodocuments. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3\% to 15\% on ad-hoc IR datasets, such as MSMARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.},
	language = {en},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Wang, Liang and Yang, Nan and Wei, Furu},
	month = oct,
	year = {2023},
	note = {arXiv:2303.07678 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Wang 等 - 2023 - Query2doc Query Expansion with Large Language Models.pdf:D\:\\Documents\\zotero\\storage\\Y2CBNR4W\\Wang 等 - 2023 - Query2doc Query Expansion with Large Language Models.pdf:application/pdf},
}


@inproceedings{iter_retgen,
	address = {Singapore},
	title = {Enhancing {Retrieval}-{Augmented} {Large} {Language} {Models} with {Iterative} {Retrieval}-{Generation} {Synergy}},
	url = {https://aclanthology.org/2023.findings-emnlp.620},
	doi = {10.18653/v1/2023.findings-emnlp.620},
	abstract = {Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to guide retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner: a model's response to a task input shows what might be needed to finish the task, and thus can serve as an informative context for retrieving more relevant knowledge which in turn helps generate a better response in another iteration. Compared with recent work which interleaves retrieval with generation when completing a single output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.},
	urldate = {2024-05-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {9248--9274},
	file = {Full Text PDF:D\:\\Documents\\zotero\\storage\\VCL5Y7ZN\\Shao 等 - 2023 - Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy.pdf:application/pdf},
}

@misc{blendfilter,
	title = {{BlendFilter}: {Advancing} {Retrieval}-{Augmented} {Large} {Language} {Models} via {Query} {Generation} {Blending} and {Knowledge} {Filtering}},
	shorttitle = {{BlendFilter}},
	url = {http://arxiv.org/abs/2402.11129},
	abstract = {Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often face challenges with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clearly indicate that our innovative BlendFilter surpasses state-of-the-art baselines significantly.},
	language = {en},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Wang, Haoyu and Zhao, Tuo and Gao, Jing},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11129 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Wang 等 - 2024 - BlendFilter Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and K.pdf:D\:\\Documents\\zotero\\storage\\JHD8S3MW\\Wang 等 - 2024 - BlendFilter Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and K.pdf:application/pdf},
}


@misc{mind_search,
	title = {{MindSearch}: {Mimicking} {Human} {Minds} {Elicits} {Deep} {AI} {Searcher}},
	shorttitle = {{MindSearch}},
	url = {http://arxiv.org/abs/2407.20183},
	doi = {10.48550/arXiv.2407.20183},
	abstract = {Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and Perplexity.ai applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.},
	language = {en-US},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Chen, Zehui and Liu, Kuikun and Wang, Qiuchen and Liu, Jiangning and Zhang, Wenwei and Chen, Kai and Zhao, Feng},
	month = jul,
	year = {2024},
	note = {arXiv:2407.20183 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\5P7DZEHE\\Chen 等 - 2024 - MindSearch Mimicking Human Minds Elicits Deep AI Searcher.pdf:application/pdf},
}


@misc{blended_rag,
	title = {Blended {RAG}: {Improving} {RAG} ({Retriever}-{Augmented} {Generation}) {Accuracy} with {Semantic} {Search} and {Hybrid} {Query}-{Based} {Retrievers}},
	shorttitle = {Blended {RAG}},
	url = {http://arxiv.org/abs/2404.07220},
	doi = {10.48550/arXiv.2404.07220},
	abstract = {Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q{\textbackslash}\&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the 'Blended RAG' method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q{\textbackslash}\&A datasets like SQUAD, even surpassing fine-tuning performance.},
	language = {en-US},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Sawarkar, Kunal and Mangal, Abhilasha and Solanki, Shivam Raj},
	month = aug,
	year = {2024},
	note = {arXiv:2404.07220 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\PRAHMV6M\\Sawarkar 等 - 2024 - Blended RAG Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\KFX9WFDP\\2404.html:text/html},
}

@misc{crag,
	title = {Corrective {Retrieval} {Augmented} {Generation}},
	url = {http://arxiv.org/abs/2401.15884},
	doi = {10.48550/arXiv.2401.15884},
	abstract = {Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.},
	language = {en-US},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Yan, Shi-Qi and Gu, Jia-Chen and Zhu, Yun and Ling, Zhen-Hua},
	month = feb,
	year = {2024},
	note = {arXiv:2401.15884 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\MSRUVXQ6\\Yan 等 - 2024 - Corrective Retrieval Augmented Generation.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\KL9ECXCF\\2401.html:text/html},
}

@misc{unims_rag,
	title = {{UniMS}-{RAG}: {A} {Unified} {Multi}-source {Retrieval}-{Augmented} {Generation} for {Personalized} {Dialogue} {Systems}},
	shorttitle = {{UniMS}-{RAG}},
	url = {http://arxiv.org/abs/2401.13256},
	doi = {10.48550/arXiv.2401.13256},
	abstract = {Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.},
	language = {en-US},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Wang, Hongru and Huang, Wenyu and Deng, Yang and Wang, Rui and Wang, Zezhong and Wang, Yufei and Mi, Fei and Pan, Jeff Z. and Wong, Kam-Fai},
	month = jan,
	year = {2024},
	note = {arXiv:2401.13256 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\H4BU4DD8\\Wang 等 - 2024 - UniMS-RAG A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\XF4PJAMN\\2401.html:text/html},
}


@misc{ircot,
	title = {Interleaving {Retrieval} with {Chain}-of-{Thought} {Reasoning} for {Knowledge}-{Intensive} {Multi}-{Step} {Questions}},
	url = {http://arxiv.org/abs/2212.10509},
	doi = {10.48550/arXiv.2212.10509},
	abstract = {Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, {\textbackslash}textit\{what to retrieve\} depends on {\textbackslash}textit\{what has already been derived\}, which in turn may depend on {\textbackslash}textit\{what was previously retrieved\}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning. Code, data, and prompts are available at {\textbackslash}url\{https://github.com/stonybrooknlp/ircot\}},
	language = {en-US},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
	month = jun,
	year = {2023},
	note = {arXiv:2212.10509 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\4JGU7BJH\\Trivedi 等 - 2023 - Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\85YUAI2X\\2212.html:text/html},
}

@misc{itrg,
	title = {Retrieval-{Generation} {Synergy} {Augmented} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.05149},
	doi = {10.48550/arXiv.2310.05149},
	abstract = {Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.},
	language = {en-US},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Feng, Zhangyin and Feng, Xiaocheng and Zhao, Dezhi and Yang, Maojin and Qin, Bing},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05149 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\Q5BGJ6YC\\Feng 等 - 2023 - Retrieval-Generation Synergy Augmented Large Language Models.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\4UM54XFX\\2310.html:text/html},
}

@misc{qwen2,
	title = {Qwen2 {Technical} {Report}},
	url = {http://arxiv.org/abs/2407.10671},
	doi = {10.48550/arXiv.2407.10671},
	abstract = {This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Dong, Guanting and Wei, Haoran and Lin, Huan and Tang, Jialong and Wang, Jialin and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Ma, Jianxin and Yang, Jianxin and Xu, Jin and Zhou, Jingren and Bai, Jinze and He, Jinzheng and Lin, Junyang and Dang, Kai and Lu, Keming and Chen, Keqin and Yang, Kexin and Li, Mei and Xue, Mingfeng and Ni, Na and Zhang, Pei and Wang, Peng and Peng, Ru and Men, Rui and Gao, Ruize and Lin, Runji and Wang, Shijie and Bai, Shuai and Tan, Sinan and Zhu, Tianhang and Li, Tianhao and Liu, Tianyu and Ge, Wenbin and Deng, Xiaodong and Zhou, Xiaohuan and Ren, Xingzhang and Zhang, Xinyu and Wei, Xipin and Ren, Xuancheng and Liu, Xuejing and Fan, Yang and Yao, Yang and Zhang, Yichang and Wan, Yu and Chu, Yunfei and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Guo, Zhifang and Fan, Zhihao},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10671 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\5QCPSPZT\\Yang 等 - 2024 - Qwen2 Technical Report.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\KEKBAJXU\\2407.html:text/html},
}

@misc{instruct_gpt,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\AN34RC23\\Ouyang 等 - 2022 - Training language models to follow instructions with human feedback.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\7EA92R64\\2203.html:text/html},
}

@misc{flash_rag,
	title = {{FlashRAG}: {A} {Modular} {Toolkit} for {Efficient} {Retrieval}-{Augmented} {Generation} {Research}},
	shorttitle = {{FlashRAG}},
	url = {http://arxiv.org/abs/2405.13576},
	doi = {10.48550/arXiv.2405.13576},
	abstract = {With the advent of Large Language Models (LLMs), the potential of Retrieval Augmented Generation (RAG) techniques have garnered considerable research attention. Numerous novel algorithms and models have been introduced to enhance various aspects of RAG systems. However, the absence of a standardized framework for implementation, coupled with the inherently intricate RAG process, makes it challenging and time-consuming for researchers to compare and evaluate these approaches in a consistent environment. Existing RAG toolkits like LangChain and LlamaIndex, while available, are often heavy and unwieldy, failing to meet the personalized needs of researchers. In response to this challenge, we propose FlashRAG, an efficient and modular open-source toolkit designed to assist researchers in reproducing existing RAG methods and in developing their own RAG algorithms within a unified framework. Our toolkit implements 12 advanced RAG methods and has gathered and organized 32 benchmark datasets. Our toolkit has various features, including customizable modular framework, rich collection of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-processing scripts, and extensive and standard evaluation metrics. Our toolkit and resources are available at https://github.com/RUC-NLPIR/FlashRAG.},
	urldate = {2024-05-28},
	publisher = {arXiv},
	author = {Jin, Jiajie and Zhu, Yutao and Yang, Xinyu and Zhang, Chenghao and Dou, Zhicheng},
	month = may,
	year = {2024},
	note = {arXiv:2405.13576 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\CJANPAF7\\Jin 等 - 2024 - FlashRAG A Modular Toolkit for Efficient Retrieval-Augmented Generation Research.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\WFQY2ZHB\\2405.html:text/html},
}

@misc{rank_rag,
	title = {{RankRAG}: {Unifying} {Context} {Ranking} with {Retrieval}-{Augmented} {Generation} in {LLMs}},
	shorttitle = {{RankRAG}},
	url = {http://arxiv.org/abs/2407.02485},
	doi = {10.48550/arXiv.2407.02485},
	abstract = {Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.},
	language = {en-US},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Yu, Yue and Ping, Wei and Liu, Zihan and Wang, Boxin and You, Jiaxuan and Zhang, Chao and Shoeybi, Mohammad and Catanzaro, Bryan},
	month = jul,
	year = {2024},
	note = {arXiv:2407.02485 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\VMWNRYPR\\Yu 等 - 2024 - RankRAG Unifying Context Ranking with Retrieval-Augmented Generation in LLMs.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\N6LFARE3\\2407.html:text/html},
}

@misc{chatqa,
	title = {{ChatQA}: {Surpassing} {GPT}-4 on {Conversational} {QA} and {RAG}},
	shorttitle = {{ChatQA}},
	url = {http://arxiv.org/abs/2401.10225},
	abstract = {In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the CHATRAG BENCH, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo2024-04-09 (score: 54.03) on the CHATRAG BENCH, without relying on any synthetic data from OpenAI GPT models. Notably, the Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09, achieving a 4.4\% improvement. To advance research in this field, we open-sourced the model weights, instruction tuning data, CHATRAG BENCH, and retriever for the community: https://chatqa-project.github.io/.},
	language = {en},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan},
	month = may,
	year = {2024},
	note = {arXiv:2401.10225 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {PDF:D\:\\Documents\\zotero\\storage\\RHJKT5JX\\Liu 等 - 2024 - ChatQA Surpassing GPT-4 on Conversational QA and RAG.pdf:application/pdf},
}

@misc{resp,
	title = {Retrieve, {Summarize}, {Plan}: {Advancing} {Multi}-hop {Question} {Answering} with an {Iterative} {Approach}},
	shorttitle = {Retrieve, {Summarize}, {Plan}},
	url = {http://arxiv.org/abs/2407.13101},
	doi = {10.48550/arXiv.2407.13101},
	abstract = {Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.},
	language = {en-US},
	urldate = {2024-08-03},
	publisher = {arXiv},
	author = {Jiang, Zhouyu and Sun, Mengshu and Liang, Lei and Zhang, Zhiqiang},
	month = jul,
	year = {2024},
	note = {arXiv:2407.13101 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\DSTD2CIW\\Jiang 等 - 2024 - Retrieve, Summarize, Plan Advancing Multi-hop Question Answering with an Iterative Approach.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\YBKJ28DD\\2407.html:text/html},
}

@misc{popqa,
	title = {When {Not} to {Trust} {Language} {Models}: {Investigating} {Effectiveness} of {Parametric} and {Non}-{Parametric} {Memories}},
	shorttitle = {When {Not} to {Trust} {Language} {Models}},
	url = {http://arxiv.org/abs/2212.10511},
	doi = {10.48550/arXiv.2212.10511},
	abstract = {Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only when necessary. Experimental results show that this significantly improves models' performance while reducing the inference costs.},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = jul,
	year = {2023},
	note = {arXiv:2212.10511 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {全文:D\:\\Documents\\zotero\\storage\\V9QRF4PZ\\Mallen 等 - 2023 - When Not to Trust Language Models Investigating E.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\ZIE7DGZU\\2212.html:text/html},
}

@misc{hotpot,
	title = {{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop {Question} {Answering}},
	shorttitle = {{HotpotQA}},
	url = {http://arxiv.org/abs/1809.09600},
	doi = {10.48550/arXiv.1809.09600},
	abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	month = sep,
	year = {2018},
	note = {arXiv:1809.09600 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\9SF348GM\\Yang 等 - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi-hop Question Answering.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\PSB2NWJ3\\1809.html:text/html},
}

@inproceedings{triviaqa,
	address = {Vancouver, Canada},
	title = {{TriviaQA}: {A} {Large} {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading} {Comprehension}},
	shorttitle = {{TriviaQA}},
	url = {https://aclanthology.org/P17-1147},
	doi = {10.18653/v1/P17-1147},
	abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.},
	urldate = {2024-05-13},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
	editor = {Barzilay, Regina and Kan, Min-Yen},
	month = jul,
	year = {2017},
	pages = {1601--1611},
	file = {Full Text PDF:D\:\\Documents\\zotero\\storage\\RLZJ4QP7\\Joshi 等 - 2017 - TriviaQA A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.pdf:application/pdf},
}

@misc{2wiki,
	title = {Constructing {A} {Multi}-hop {QA} {Dataset} for {Comprehensive} {Evaluation} of {Reasoning} {Steps}},
	url = {http://arxiv.org/abs/2011.01060},
	abstract = {A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multihop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our dataset, we introduce the evidence information containing a reasoning path for multi-hop questions. The evidence information has two beneﬁts: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model. We carefully design a pipeline and a set of templates when generating a question–answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required.},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},
	month = nov,
	year = {2020},
	note = {arXiv:2011.01060 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Ho 等 - 2020 - Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps.pdf:D\:\\Documents\\zotero\\storage\\GC3BPPU8\\Ho 等 - 2020 - Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps.pdf:application/pdf},
}

@article{nq,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}


@misc{contriever,
	title = {Unsupervised {Dense} {Information} {Retrieval} with {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2112.09118},
	doi = {10.48550/arXiv.2112.09118},
	abstract = {Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS{\textasciitilde}MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
	month = aug,
	year = {2022},
	note = {arXiv:2112.09118 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\Y8U6UZII\\Izacard 等 - 2022 - Unsupervised Dense Information Retrieval with Cont.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\W5KNH95R\\2112.html:text/html},
}

@misc{atlas,
	title = {Atlas: {Few}-shot {Learning} with {Retrieval} {Augmented} {Language} {Models}},
	shorttitle = {Atlas},
	url = {http://arxiv.org/abs/2208.03299},
	abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3\% despite having 50x fewer parameters.},
	language = {en},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
	month = nov,
	year = {2022},
	note = {arXiv:2208.03299 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:D\:\\Documents\\zotero\\storage\\7JVWEN2N\\Izacard 等 - 2022 - Atlas Few-shot Learning with Retrieval Augmented Language Models.pdf:application/pdf},
}

@misc{long_rag,
	title = {Retrieval meets {Long} {Context} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.03025},
	doi = {10.48550/arXiv.2310.03025},
	abstract = {Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan},
	month = jan,
	year = {2024},
	note = {arXiv:2310.03025 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:D\:\\Documents\\zotero\\storage\\6A4297LN\\Xu 等 - 2024 - Retrieval meets Long Context Large Language Models.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\zotero\\storage\\C698Y2RM\\2310.html:text/html},
}

@article{fireact,
  title={Fireact: Toward language agent fine-tuning},
  author={Chen, Baian and Shu, Chang and Shareghi, Ehsan and Collier, Nigel and Narasimhan, Karthik and Yao, Shunyu},
  journal={arXiv preprint arXiv:2310.05915},
  year={2023}
}