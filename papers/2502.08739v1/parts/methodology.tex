\section{Methodology}
\label{methodology}

Fig.\ \ref{fig:methodology} presents an overview of our methodology to construct the final taxonomy. We detail the methodology in the remainder of this section based on the steps depicted in Fig.\ \ref{fig:methodology}.

\begin{figure}[htb!]
  \includegraphics[width=\linewidth, scale=0.5]{Figures/methodology.png}
  \caption{Overview of the 6 steps that led to the construction of our Final Taxonomy.}
  \label{fig:methodology}
\end{figure}


\subsection*{Step 1. Manual Query Refinement}

In this step, we define the inclusion/exclusion criteria (1.1), identify the sources of real and open code bases with available bugs (1.2), design our search query (1.3), and analyse the initial search results to refine the search query (1.4).


\paragraph*{1.1. Inclusion Criteria} 
We define our inclusion criteria, based on our research questions, as follows. A fault is included in our dataset if it:

\begin{enumerate}

\item Occurs in an implementation of a Hybrid Quantum-Classical architectures (e.g.,  NISQ algorithms) (RQ1).
\item Includes a suitable description of the fault and the code associated with it (RQ1 and RQ2).
\item Includes a suitable description of the fix and the code associated with it (RQ1).
\item Includes a description of the problem and its solution in English (RQ1 and RQ2). 

\end{enumerate}

We aim to comply with ACM guidelines for reproducibility \cite{noauthor_artifact_nodate}, which means a different team should be able to reproduce these faults using the same setup.\\

\paragraph*{1.2. Analysing Available Resources} 
Quantum computing is a young field. Access to quantum simulators and real computing platforms has been available for only a few years.  Hence, the amount of open software and associated data available online is limited. We first analysed the typical web resources for such software and data: we searched Github, StackOverflow, StackExchange, and quantum-specific forums such as Pennylane and TensorFlow Quantum forums. We found results of interest for this study only on GitHub. At the time of the study, StackOverflow and StackExchange had very few discussions regarding Hybrid Quantum-Classical architectures. Those discussions were exclusively theoretical, with no code provided, or questions related to installing or importing software. Due to the inclusion criteria number 2 and 3, we excluded them. Some discussions reported faults in quantum circuits that we excluded following criteria 1 since we found only quantum faults and not Hybrid Quantum-Classical architecture faults. GitHub was, hence, the only available public source featuring source code and data satisfying our inclusion criteria. We also decided to focus on the main repositories containing Hybrid Quantum-Classical issues. This includes four famous quantum simulation platforms - Qiskit, Tensorflow Quantum, PennyLaneAI, NVIDIA Cuda-Quantum, and four independent established repositories from universities and companies - qiboteam, AgnostiqHQ, cuda-tum, goodchemistryco. This variety of fault sources aims to provide an eclectic overview of Hybrid Quantum-Classical architecture faults to answer our research questions. \\

\paragraph*{1.3. Designing a Search Query} 
We used GitHub search API to find Hybrid Quantum-Classical architecture faults and manually assess them. After an initial manual and manual search investigation, we discovered that NISQ algorithms are the only Hybrid Quantum-Classical architecture for which there are examples of public faults and bug fixes satisfying our criteria. We first searched 'NISQ' and 'Noisy intermediate-Scale Quantum'. We found these keywords to be too generic. They mostly led to discussion around research papers, or theoretical questions about the nature of NISQ, with a vast majority of results not written in English. All the 718 results were directly excluded by at least one of our criteria, and often all four of them. Hence, we decided to search for specific algorithms designed for Hybrid Quantum-Classical architectures. 

Most implementations require importing a package named after the algorithms. For example, in qiskit, we import a class named \textit{VQE} to implement VQEs. Any source code including such an import, or any discussion mentioning the searched algorithms will be identified by this method. We, therefore, used the following keywords, which are the available algorithms according to a recent survey \cite{bharti_noisy_2022}: \textit{VQE}, \textit{VQA}, \textit{Variational Quantum eigensolvers}, \textit{Variational Quantum algorithm}, \textit{Quantum Annealing}, \textit{Gaussian boson sampling}, \textit{Analog quantum simulation}, \textit{Digital-analog quantum simulation and computation}, \textit{Iterative quantum assisted eigensolver}, \textit{Quantum Approximative Optimisation Algorithms}, \textit{QAOA}, \textit{Quantum Machine Learning}, \textit{QML}, \textit{Tensorflow Quantum}, and \textit{TFQ}. 

%The number of results we obtained as a result of running the queries for each keyword are reported in column "Initial" of Table \ref{tab:searchoverview}. 


\subsection*{Step 2.  GitHub Search, Filtration, and Validation}
In this step, we performed the search and stored the results (2.1);  applied our inclusion/exclusion criteria and performed first the manual filtration of the results by one author and then their labelling by two authors. 
%a first stage of validation (2.2).
%A second stage of validation is performed at the end (2.3). 
%; and labelled the issues with more information (on the type of issue, its symptoms, and the cause of the bug) and applied an additional cross-validation stage (2.3).
Table \ref{tab:searchoverview} provides an overview of the number of issues from our initial search process (in the column marked Initial)  and after each stage of cross-validation (columns marked "Filtration" and "Labelling"). 

%In each cell, we report the issues without and with the filtration based on the label "bug". We observe that adding this label filters a significant amount of false positives but misses many issues that fit our criteria. 

\paragraph*{2.1. Organising the Search Process} 
To ensure the reproducibility of our results, we implemented a script that automatically extracts all results of a GitHub search, with the issue's title and link, into an Excel spreadsheet. The script is flexible and can easily be adapted to other queries. The results of each query are extracted into a separate sheet. Since GitHub search can only display up to 1000 results, our script automatically divides a search into sub-searches if more than 1000 results are found, and gathers all of them. As shown in Table \ref{tab:searchoverview}, the script extracted 5072 issues.
%, including 3288 labelled as bugs. 

%We processed the first and second rounds of searches, with the initial and the refined query,  separately. Since the second round includes all the issues of the first one, we ensured that all results present in the first stage of the first batch were present in the second batch, as well. 

\paragraph*{2.2. Manual Filtration}
We then excluded any issue that does not comply with one or more of our criteria: it does not concern a Hybrid Quantum-Classical architecture, it does not provide code or fix, or it is not in the English language. This process was performed manually by the first author for each of the 5072 bugs. As a result of applying the inclusion criteria, 529 faults remained in the dataset, as reported in column "Filtered" in Table \ref{tab:searchoverview}. 

%This corresponds to Stage 1 in Table \ref{tab:searchoverview}. The numbers in the initial and Stage 1 columns include duplicate issues caught by several queries. Duplicates were filtered between stages 1 and 2.

%Only the searches for VQE, QAOA, and TFQ provided a high number of results as we detail in the following paragraph. 

The results of manual filtration show that `QML resulted in a high number of false positives. The reason behind this is that `QML' stands not only for 'Quantum Machine Learning', but also for 'QT Modeling Language', which typically uses a file type `.qml'. Out of the obtained 2861 results, only 62 were related to Quantum programming, and only 6 were about Hybrid Quantum-Classical architectures. Pennylane, which is a common framework for quantum machine learning, is often imported using `import pennylane as qml' as advised in their official documentation. Any program using this import without implementing a Hybrid Quantum-Classical architecture is caught by searching `QML'. Also, all remaining 6 issues were caught by other keywords. 
Our results also indicate that the abbreviations such as VQA instead of Variational Quantum Algorithms, lead to fewer false positives, as using the full names of algorithms typically leads to issues the majority of which lack any source code. 

%We also found that filtering the results so that they contain the 'bug' label and that the issues are closed eliminates many false positive results. This is because the closed issues are typically more likely to pass our criteria 3 and 4, while the issues labelled 'bug' are more likely to comply with criteria 2 and 3. Indeed, the label 'bug' is usually added by an external contributor, who will ask for a code sample if it was not initially added to the issue. If an issue was labelled as a 'bug' and closed, it has a higher chance of having been assessed and fixed by a contributor. However, we found that adding the label 'bug' gave reliable results only for Qiskit, as in its repository the issues containing faults were almost always labelled 'bug' as soon as they were assessed by a contributor. Other repositories used different methodologies. PennyLaneAI, for example, adds '[BUG]' to the title without always adding a label to the issue. Others, such as Tensorflow Quantum, often do not mark bugs at all. 

%This reaffirmed our decision to perform our next round of searches without the label 'bug', nor to require the issues to be closed. We excluded 'QML' from our queries for the reasons explained earlier. In the second round, 'VQA' produced 1886 results, against 55 with the label 'bug'. We found that 'VQA' stands for 'Variational Quantum Algorithms', but also for 'Visual Question Answering', and for 'Visual Quality Assessment'. Out of the 1886 results, only 62 were quantum-related, all of which were excluded by one or more of our criteria.   \\

 
 %; this colour coding was used for all conflicts arising from cross-validation throughout the process.  

\begin{table}[hbt!]
    \centering      
    \caption{Number of identified results after each stage: ``Initial'' refers to the initial number of results obtained through GitHub search, ``Filtered'' refers to the number of issues that satisfy the inclusion criteria, ``Labelled'' refers to the number of issues that were decided to be relevant during the manual labelling process by two authors.}
 
    \begin{tabular}{c|c|c|c}
        \hline
        \textbf{Query} & \textbf{Initial} & \textbf{Filtered} & \textbf{Labelled} \\
        \hline
        VQE & 792 & 162 & 28  \\
        \hline
        VQA & 1886 & 0  & 0  \\
        \hline
        QAOA & 466 & 87  & 27 \\
        \hline
        QML & 2861  & 6  & 0   \\
        \hline
        TFQ & 226 & 36 & 19  \\
        \hline
        Quantum Machine Learning  & 1038 & 164 & 20 \\
        \hline
        Variational Quantum algorithm & 100 & 10 & 1  \\
        \hline
        TensorFlow Quantum&  433 & 66 & 22  \\
        \hline
        Other Queries & 131 & 4 & 16  \\
        \hline
        \textbf{TOTAL}& 5072 & 529 & 133  \\
        
    \end{tabular}
    \label{tab:searchoverview}
\end{table}

% \begin{table}[hbt!]
%     \centering      
%     \caption{Number of identified results after each stage with and without label bug: column ``Initial'' refers to the number of results in the search query (without/with the bug label); columns 1 and 2 refer to the results of Stage 1 and 2 of our validation, respectively (using the same notation: without/with the bug label).  }
 
%     \begin{tabular}{c|c|c|c}
%         \hline
%         \textbf{Query} & \textbf{Initial} & \textbf{Filtration} & \textbf{Labelling} \\
%         \hline
%         VQE & 144 / 792 & 61 / 162 & 28 /28  \\
%         \hline
%         VQA & 55 / 1886 & 5 / 0  & 0 / 0  \\
%         \hline
%         QAOA & 62 / 466 & 20 / 87  & 10 / 27 \\
%         \hline
%         QML & 2861 / NA & 6 / NA & 0 / NA  \\
%         \hline
%         TFQ & 13 / 226 & 0 / 36 & 22 / 19  \\
%         \hline
%         Quantum Machine Learning  & 74 / 1038 & 24 / 164 & 27 / 20 \\
%         \hline
%         Variational Quantum algorithm & 17 / 100 & 5 / 10 & 1 / 1  \\
%         \hline
%         TensorFlow Quantum& 48 / 433 & 2 / 66 & 20 / 22  \\
%         \hline
%         Other Queries & 14 / 131 & 0 / 4 & 0 / 16  \\
%         \hline
%         \textbf{TOTAL}& 3288 / 5072 & 123 / 529 & 108 / 133  \\
        
%     \end{tabular}
%     \label{tab:searchoverview}
% \end{table}




\subsection*{Step 3: Labelling}
In this step, we first perform a pilot review of the faults to define a structure for coding more information about them (3.1); we use this structure to code information about each fault, including their type, symptoms, and potential causes   (3.2); then we perform a cross-validation of the labels (3.3). 



\paragraph*{3.1. Pilot Fault Review}
Each included fault was analysed by two different authors to identify its type, i.e., its place in the Hybrid Quantum-Classical architecture, its symptoms, and its possible root causes (indicated by the fix). We labelled the faults between two different authors to reduce the risk of manual error and individual bias.

For this pilot, we selected 30 bugs. Once each author completed the labelling process, we conducted a consensus meeting. During this pilot study, 20 conflicts emerged and were resolved through discussion.  



\paragraph*{3.2. Individual Labelling}

%After the discussion, we reached an agreement on all conflicts and defined the following categories for assessing the rest of the dataset. 

During the discussion, we also defined high-level categories along which we decided to classify the bugs we analysed:

\begin{itemize}\itemsep0em

\item  \textbf{Parametrisation.} Fault related to the parameters of the circuit or the tensors.  
\item  \textbf{Ansatz.} Issues arising during the design of the ansatz. Initially, this category was thought of as a sub-category of Quantum Circuits, but we decided to gather all faults specific to the Ansatz design since it is a crucial part of Hybrid architectures.
\item  \textbf{Quantum Circuit.} Faults occurring within the quantum circuit. This gathers all quantum faults not directly linked to the Ansatz.
\item  \textbf{Optimisation.} Faults encountered during the optimization loop. 
\item  \textbf{Measurement.}  Faults occurring during a measurement or a sequence of measurements.
\item  \textbf{GPU integration.}  Issues related to GPU integration.
\end{itemize}


These categories correspond to the main components of the workflow of a Hybrid Quantum-Classical architecture development. We used them only for guidance and added new categories and their subcategories when necessary. A more detailed description of the final list of these categories and subcategories is provided in Section \ref{vqa seq}.

For each bug, we extracted information on whether it is \textit{relevant} to the scope of our study or not, assuring they follow our inclusion criteria. Particularly, we evaluate if the fault was indeed reflecting an aspect of a Hybrid architecture. If it is relevant, we then identified whether this bug takes place \textit{inside a library} designed to be used when building an architecture (for example, a bug inside Qiskit libraries), or whether the bug happens due to a \textit{developer error} when using the library. We differentiate between these scenarios because when the issue lies within the library, it suggests a need for fixing or improving the library code. In contrast, if the issue is due to a developer error, then the fix should take place in the developer code. Getting an insight on which scenario takes place more often, can guide the future efforts required to prevent them. 

Lastly, for each bug, we identify its \textit{symptom} (such as crash, failure, slow performance, wrong output) and create a short label that summarises its \textit{root cause} (for example, "suboptimal kernel building"). During the labelling procedure, we kept the list of already generated labels available to enable their reuse when necessary. We have labelled the 529 bugs in 8 rounds (targeting on average 60 bugs per round) and had a conflict resolution meeting after each round. We carefully tracked all conflicts (108 in total) and how they were resolved.

%\paragraph*{3.3. Cross-validation.} Similar to the previous step, we cross-validated the labelling between two of the authors; marked the conflicts and resolved them through further discussion. 

\subsection*{Step 4. Expert Interviews}
There are limited public and open-source resources available for Hybrid Quantum-Classical architecture at the moment. While many frameworks are open-source, the data available about them and the instances of programs using them are scarce. We therefore decided to conduct expert interviews to enrich and validate our dataset. This step is organised into the following sub-steps: selecting the participants (4.1); designing the interviews (4.2); and conducting and processing the interviews (4.3). The result of the latter step is fed into a further survey (Step 5) and is incorporated into the final taxonomy (Step 6). 

\paragraph*{4.1. Selecting 
Interviewees} 
To ensure diversity in our panel of experts, especially between industry and academia, we selected and contacted experts working on Hybrid Quantum-Classical architectures at several leading quantum computing companies such as Quantum IBM, Quasar USA Quantum Blockchain Technologies, Google Quantum, and ClassicQ. We approached them through e-mail, as well as LinkedIn and ResearchGates private messaging systems. We received three positive answers and managed to secure another industry interview (a total of four) through personal contacts. One of the interviewees from the industry was identified by us due to their active participation in fixing several of the GitHub issues in our dataset and we contacted them on the LinkedIn platform. The remaining selected experts  (seven in total) are active in academia. Two of them have a strong connection with the industry. Academic profiles vary from PhD student to Lecturer, both from Computer Science and Physics Departments with significant experience in Quantum Computing and especially in Hybrid Classical-Quantum architectures. 

\paragraph*{4.2. Designing the Interview Structure}

We opted for a semi-structured interview to have a flexible and adaptive structure to elicit as much information from the experts as possible \cite{seaman_qualitative_1999}. After establishing the scope of the study, and defining the key terms as advised in the pilot interviews, we opened with a broad question, namely, \textit{'What kind of problems have you experienced developing Hybrid Quantum-Classical architectures?'}. We tried to adapt the questions to the interviewees and re-direct the discussion to lead them to share the faults they have experienced. We prepared a script, available in the replication package, to ensure consistency in the content of each interview while being adaptive to elicit most information where the interviewee had more experience. When the interviewee mentioned any fault during the interview, we asked additional questions to identify all the characteristics required for our collection (in the library or not, symptoms, etc.)

\paragraph*{4.3. Processing the Interviews}
We conducted 11 interviews with interviewees from different academic and industrial profiles. To validate the structure of the interview, we conducted them in two rounds: a pilot round and a final round. 
The first two pilot interviews were held by two of the authors. We asked for the feedback of the interviewees to improve the interview process. The two comments received were to better define the context of the study, start the interview with a formal definition of Hybrid Quantum-Classical architectures and faults, and send the questions in advance. Both the interviews and the feedback discussions were transcribed and are available in the replication package. Once we ensured the final interview structure, the remaining 9 interviews were held by the first author only. Each interview was transcribed once the consent from the interviewee has been obtained. Two of the authors analysed the transcription and extracted the faults mentioned in it independently, to later cross-validate the results through a consensus meeting. The highlighted new faults or categories were extracted to be incorporated into the dataset. In these cases, all relevant passages of the transcripts were highlighted, and each fault extracted is associated with an exact timestamp in the transcription. %More details can be found in the replication package.

\subsection*{Step 5. Final Validations}


\paragraph*{5.1. Search Validation}
We explored other quantum bug benchmarks \cite{zhao_bugs4q_2021}\cite{zhao_empirical_2023} and selected all bugs that satisfied our inclusion criteria. We verified that all these bugs were included also in our dataset. Bugs4Q \cite{zhao_bugs4q_2021} included only 3 bugs with a noise simulation, and none were Hybrid Quantum-Classical architecture faults. The second study \cite{zhao_empirical_2023} focused on Quantum Machine Learning bugs. To validate our dataset, we checked all benchmark faults complying with our four criteria in the benchmark repositories and ensured our dataset included them. We selected 71 bugs in their repository, all of which were indeed already captured in our results.  \\


\paragraph*{5.2. Validation Surveys}

We created a validation survey to ensure the relevance of our taxonomy. The expert participants for validation were recruited through our personal contacts and project partners.  This survey was sent to an independent group of 25 researchers and industrial experts who did not participate in our interviews, of which 7 answered. After gathering their background, the participants were asked if they had encountered faults/problems in each of the categories of our final taxonomy. A final open question checked whether the participant had encountered faults that did not fall into our categories to validate the soundness of our taxonomy. The survey's results can be found in the next section, and more details are available in the replication package.


\subsection*{Step 6. Final Taxonomy Construction}
Once the final dataset was compiled, two of the authors analysed the categories and sub-categories separately and made the labels consistent (in terminology and style). This stage aimed to harmonise the results and find common patterns in the labels that could be gathered as a category or a sub-category. Our starting point for the main categories was the general architectural components identified in Step 3 (3.2), but we updated them by considering the outputs of the interviews. 
As a result, a category was removed and another was added at this stage. The resulting taxonomy is depicted in Fig. \ref{FinalTax} and are further elaborated in the replication package. We reflect upon this result in the remainder of this paper, as well. 
