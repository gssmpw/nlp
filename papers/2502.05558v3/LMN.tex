%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}



\usepackage{amsthm,amsmath,bm}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{subfigure} 
\usepackage{float}
\usepackage[export]{adjustbox} 
\usepackage{xspace}
\graphicspath{ {./Figs/} }

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{balance}
\usepackage{enumitem}
% \usepackage{multirow}
% \usepackage{xcolor}

\newcommand{\eat}[1]{}
\newcommand{\paratitle}[1]{\vspace{1.5ex}\noindent\textbf{#1}}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\aka}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\baby}{LMN\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\ignore}[1]{}
\newcommand{\fixme}[1]{\textbf{\emph{[FIXME: #1]}}}


%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}\acmConference[WWW Companion '25]{Companion Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3701716.3715514}
\acmISBN{979-8-4007-1331-6/2025/04}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Large Memory Network for Recommendation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Hui Lu}
% \authornote{Both authors contributed equally to this research.}
\email{luhui.xx@bytedance.com}
\affiliation{%
  \institution{ByteDance}
  \city{Hangzhou}
  \country{China}
}

\author{Zheng Chai\textsuperscript{†}}
\thanks{\textsuperscript{†}Corresponding Author.}
% \authornote{Corresponding Author.}
\email{chaizheng.cz@bytedance.com}
\affiliation{%
  \institution{ByteDance}
  \city{Hangzhou}
  \country{China}
}


\author{Yuchao Zheng}
% \authornote{Both authors contributed equally to this research.}
\email{zhengyuchao.yc@bytedance.com}
\affiliation{%
  \institution{ByteDance}
  \city{Beijing}
  \country{China}
}

\author{Zhe Chen}
% \authornote{Both authors contributed equally to this research.}
\email{chenzhe.john@bytedance.com}
\affiliation{%
  \institution{ByteDance}
  \city{Beijing}
  \country{China}
}

\author{Deping Xie}
% \authornote{Both authors contributed equally to this research.}
\email{deping.xie@bytedance.com}
\affiliation{%
  \institution{ByteDance}
  \city{San Jose}
  \country{USA}
}

\author{Peng Xu}
% \authornote{Both authors contributed equally to this research.}
\email{xupeng@bytedance.com}
\affiliation{%
  \institution{ByteDance}
  \city{San Jose}
  \country{USA}
}

\author{Xun Zhou}
% \authornote{Both authors contributed equally to this research.}
\email{zhouxun@bytedance.com}
\affiliation{%
  \institution{ByteDance}
  \city{Beijing}
  \country{China}
}

\author{Di Wu}
% \authornote{Both authors contributed equally to this research.}
\email{di.wu@bytedance.com}
\affiliation{%
  \institution{ByteDance}
  \city{Beijing}
  \country{China}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Hui Lu et al.}

\begin{abstract}
    Modeling user behavior sequences in recommender systems is essential for understanding user preferences over time, enabling personalized and accurate recommendations for improving user retention and enhancing business values. Despite its significance, there are two challenges for current sequential modeling approaches. From the spatial dimension, it is difficult to mutually perceive similar users' interests for a generalized intention understanding; from the temporal dimension, current methods are generally prone to forgetting long-term interests due to the fixed-length input sequence. In this paper, we present Large Memory Network (LMN), providing a novel idea by compressing and storing user history behavior information in a large-scale memory block. With the elaborated online deployment strategy, the memory block can be easily scaled up to million-scale in the industry. Extensive offline comparison experiments, memory scaling up experiments, and online A/B test on Douyin E-Commerce Search (ECS) are performed, validating the superior performance of LMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of users each day.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[500]{Information systems~Recommender systems}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large Recommender Models, Memory Learning, Personalized Recommender System, Sequential Modeling}

\maketitle

\section{Introduction}
User behavior sequences are a crucial means of depicting user interests and preferences \cite{chai2022user, de2021transformers4rec}. Currently, many outstanding sequential modeling methods have been approached, which can be divided into two branches. The first one is to adopt a target-attention mechanism for short sequence modeling, for example, DIN \cite{zhou2018deep}, DIEN \cite{zhou2019deep}, CAN \cite{bian2022can}. As such methods are mostly developed for short sequences, the other branch develops search-based mechanism to perceive longer sequences in the model, like SIM \cite{pi2020search}, TWIN \cite{chang2023twin}, etc. However, much efforts are generally needed to deploy such complex two-stage models for online serving, and the behavior discarding further results in incomplete interest estimation. Therefore, another type of approach that is emerging as a hotspot is to directly model long sequence in the recommendation model \cite{liu2023deep, cao2022sdim, yu2024ifa, zhaiactions}, which inevitably increases the computational cost of model training and inference, introducing a substantial online overhead for industrial recommenders.

Currently, scaling laws have guided the language model design by continuously scaling up the model parameters, showing promising performance gains. The discovery has also spurred research in other fields towards increasing model complexity, for example, increasing the dense network parameters \cite{zhangwukong} and sparse embedding parameters \cite{guoembedding} in industrial recommenders. However, in practice, simply scaling model parameters can directly lead to decreased training and inference efficiency, and the computational FLOPs increment further limits the model scalability. To mitigate the computational and GPU memory overhead, the memory network structure can amplify model parameters through memory mechanism to enhance model capacity, which has been applied in the field of natural language modeling. The Key-Value Memory Network \cite{miller2016key} proposed by Miller et al. abstracts the basic structure of memory network mechanism, which mainly consists of query, keys, and values, representing an early form of attention mechanism. However, with the increase of memory space, both the computational complexity and memory access overhead of such basic key-value memory structure will also increase. Under hardware constraints, the memory space cannot be infinitely enlarged, limiting the expressive power and scaling laws of memory. To this end, Lample et al. \cite{lample2019large} developed large memory layers by decomposing keys to decrease the complexity. Despite the popularity and effectiveness, such memory mechanism is rarely used in the field of recommendation. Pi et al. proposed a multi-channel user interest memory network (MIMN) by introducing neural Turing machine and memory induction unit \cite{pi2019practice}, while the complex serial modeling structure with GRU made the model training inefficient, and the limited memory space further restricts its effectiveness.

To solve the above problems, in this paper, we present Large Memory Network (LMN), a lightweight yet effective sequential modeling framework for user intention understanding. Generally, the overall contributions can be summarized as follows:
\begin{itemize}
    \item We present a novel lightweight large memory network, by which the user sequences can be effectively compressed into memory parameters and achieve large-scale expansion of model parameter capacity with little computational cost.
    \item The designed LMN builds a large global-shared table for different users, enabling spatial perception among users and temporal memory of long-term user interests. Both a product quantization-based memory decomposition and an industrial online framework are provided for efficient deployment.
    \item Extensive comparison and scaling up experiments, and online A/B experiment illustrate the superiority. LMN has been fully deployed in Douyin E-Commerce, serving millions of users.
\end{itemize}


\section{Methodology}
\subsection{Preliminaries}
\subsubsection{Problem Formulation}
This paper focuses on the click-through rate (CTR) prediction tasks in recommendation, and the overall learning objective can be formulated as follows:

\begin{equation}
    \hat{y} = f(\bm{u}, \bm{i}, \bm{c}, \bm{s})
\end{equation}
where $\bm{u}$, $\bm{i}$, $\bm{c}$ denote user-side, item-side, and user-item cross feature embeddings, respectively, and $\bm{s}$ represents the user sequence representation. $f$ denotes the recommendation model for predicting the CTR $\hat{y}$.

\subsection{The Proposed \baby}
The structure of the proposed \baby is illustrated in Figure~\ref{fig:overview}. In general, LMN is an end-to-end storage unit that compresses and memorizes historical behavior sequences and user information. 


\begin{figure}[h]
\center
\includegraphics[width=\linewidth]{figs/LMN_framework.pdf}
\caption{Overall framework of the proposed \baby. There are two learnable components in the memory block including memory keys and memory values. The \baby uses a two-axis activation for locating the top $K$ relevant values for a given specific query. Besides, the Smooth L1 loss is performed to inject historical interaction with corresponding user information into the memory value slots. The optional mask is used as some of the sequence items are padded to a fixed length with zero embeddings in practice.}
\label{fig:overview}
\end{figure}

\subsubsection{Overall Framework}
The learning objective of \baby is to build a large-scale memory block and find the top-$K$ most relevant memory slots in the block for assisting user intention modeling. It is noted that all users share the same memory block, allowing user behavior information to generalize in a much larger parameter space (memory), which establishes perception among users spatially and memorizes long-term user interest temporally. As shown in Figure~\ref{fig:overview}, the overall framework of the LMN has two stages, i.e., memory extraction and memory injection, which will be introduced in detail.


\subsubsection{Memory Extraction}
Generally, denote the memory key as $M_K \in \mathbb{R}^{n \times d}$, and the memory value as $M_V \in \mathbb{R}^{n \times d}$, where $n$ and $d$ represent the number and dimension of the memory slots. Given an interacted item $x \in \mathbb{R}^{1 \times d}$ in user sequence as the memory query, the calculation process of memory activation can be written as:

\begin{align}
M_O = Softmax (x \cdot M_K^{\mathrm{T}}) \cdot M_V
\label{eqn:overall}
\end{align}

\noindent where $M_O$ denotes the output of the memory block. 

As shown in Eq.~\ref{eqn:overall}, the computational complexity of the similarity score, i.e., $s = x \cdot M_K^{\mathrm{T}}$, is $\bm{\mathcal{O}}(nd)$ which is linearly proportional to the scale of the number of memory slots $n$. To further decrease the computational cost, the memory activation mechanism is designed based on product quantization \cite{jegou2010product, lample2019large}. Specifically, we quantize the original $M_K \in \mathbb{R}^{n \times d}$ as:

\begin{align}
{M_K} = {M_{K-row}} \times {M_{K-col}}
\label{eqn:quantize}
\end{align}

\noindent where we decompose the original $M_K$ with $n$ memory slots into two subgroups, i.e., $M_{K-row} \in \mathbb{R}^{\sqrt{n} \times d}$ and $M_{K-col} \in \mathbb{R}^{\sqrt{n} \times d}$, corresponding to the row index keys and column index keys, respectively. "$\times$" here denotes a cross operation. With both the row and col indices, the activated memory values in $n$ memory value slots can be located, as illustrated in Fig. 1. As each subgroup includes $\sqrt{n}$ memory slots, the computational cost of the attention score calculation in Eq.~\ref{eqn:overall} can be decreased from $\bm{\mathcal{O}}(nd)$ to $\bm{\mathcal{O}}(\sqrt{n}d)$ effectively.

\textbf{User-Aware Block.} Correspondingly, to lookup a memory value for a given interacted-item $x$, we first obtain the corresponding queries $M_{Q-row}$ and $M_{Q-col}$ for both keys in Eq.~\ref{eqn:quantize}. However, due to the highly personalized characteristics of users in recommenders, the same representation of an item $x$ exhibits varying semantics across various users. Thus, relying only on the similarity of item embedding as a metric is insufficient. To this end, we present user-aware block, which introduces user-aware information into the queries for better generalization performance:

\begin{align}
M_Q = MLP(Merge(x, \bm{u}))
\label{eqn:user-aware}
\end{align}

\noindent where $\bm{u}$ is the user-side features like the user id or demographic features, and the Merge operation means the add or concatenation operation. In this paper, concatenation is used. Then, the two queries can be generated corresponding to the row keys and column keys:

\begin{align}
{M_{Q-row}} = MLP(M_Q)\\
{M_{Q-col}} = MLP(M_Q)
\label{eqn:query-mlp}
\end{align}


Based on the $M_{Q-row} \in \mathbb{R}^{1 \times d}$ and $M_{Q-col} \in \mathbb{R}^{1 \times d}$, we obtain the memory activation scores at the row axis, column axis, and the overall:

\begin{align}
S_{row} &= M_{Q-row} \cdot M_{K-row}^{\mathrm{T}}\\
S_{col} &= M_{Q-col} \cdot M_{K-col}^{\mathrm{T}}\\
S &= Broadcast(S_{row}, S_{col})
\label{eqn:s}
\end{align}

\noindent where both $S_{row}$ and $S_{col} \in \mathbb{R}^{1 \times \sqrt{n}}$, and the broadcast operation means that $S_{row}$ and $S_{col}$ are first reshaped as $S_{row} \in \mathbb{R}^{1 \times \sqrt{n}}$ and $S_{col} \in \mathbb{R}^{\sqrt{n} \times 1}$, then the two scores are added with the broadcast mechanism\footnote{https://www.tensorflow.org/guide/tensor\#broadcasting} to obtain a final score at $S \in \mathbb{R}^{\sqrt{n} \times \sqrt{n}}$, which is finally reshaped to $\mathbb{R}^{n}$.

With the memory activation score $S$, we select the top $K$ highest scores to lookup the values and gather them with a weighted-sum mechanism:

\begin{align}
M_O = Softmax(S_{TopK}) \cdot M_{V-Top K}
\label{eqn:weighted-sum}
\end{align}
in which $S_{Top K}$ denotes the highest $K$ activation scores in $S$ and $M_{V-Top K}$ represents the top $K$ score indexed-values.

To further increase the model capacity and improve performance, both the query and keys can be obtained via a multi-head mechanism.


% smooth L1: 
\subsubsection{Memory Injection}The stage of injection aims to memorize compressed information into the value table. Traditional memory structures might use a write-back operation to inject information \cite{pi2019practice}. However, under the query-key activation framework, a more reasonable solution is to use the gradient-descent approach to update both the key and value memory tables. Furthermore, to memorize the information of the user-item pairs in the designed memory slots, a Smooth-L1 \cite{girshick2015fast} based memory loss is adopted to inject user-item pair information into the memory slots:

\begin{align}
Loss_{Memory} = SmoothL1(M_O, M_Q)
\label{eqn:mem-loss}
\end{align}

\subsubsection{Model training and deployment} The LMN is a lightweight module while with the potential to significantly increase the model parameter capacity. It can be used as a plug-and-play module for current CTR prediction models, and the overall loss can be obtained as follows with a balanced parameter $\alpha$:
% a balanced-form:

\begin{align}
Loss_{LMN} = Loss_{CTR} + \alpha Loss_{Memory}
\label{eqn:loss}
\end{align}
\noindent where $Loss_{CTR}$ is the loss value of the original CTR task.


\begin{figure}[h]
\center
% \includegraphics[width=\linewidth,trim=0 8 0 7,clip]{figs/scaling_law.eps}
\includegraphics[width=\linewidth, trim=0 20 0 17,clip]{figs/lmn_sys2.pdf}
\caption{Online deployment framework of memory parameter server (MPS) for the proposed \baby at ByteDance.}
\label{fig:sys}
\end{figure}

To deploy the LMN in online scenarios, it is noted that among the two modules including the memory keys and memory values, the latter one has a much larger capacity which should be carefully designed for deployment in industrial applications. Specifically, we introduce memory parameter server (MPS), which adopts a GPU-sharding strategy for distributed storage of the memory values on high bandwidth memory (HBM) of multi-GPUs. The detailed framework of MPS is illustrated in Fig.~\ref{fig:sys}. In the model training process, the recommendation model launches a request for lookup of corresponding values according to the memory activation score $S$. Then, the all2all lookup is performed for distributed HBM storage on top of GPU NVLink\footnote{https://www.nvidia.com/en-us/data-center/nvlink/}. Then, the memory values are updated with the all2all gradient updates. For serving, only the All2All lookup is reserved for memory search.


\section{Experiment}
\subsection{Evaluation Setting}
\paratitle{Dataset}. To investigate the performance of \baby, we conduct experiment at Douyin E-Commerce Search (ECS), which serves as the most important scenario to meet the e-commerce demands of users in search scenarios at ByteDance and attracts millions of page views of online traffic each day. We collect and sample online traffic logs from Oct. 1st, 2024 to Oct. 28th, 2024, with 4 weeks and \textbf{3.6 billion} samples, and the former 3 weeks are used for training and the last week is for evaluation. 

\paratitle{Comparison Methods and Metrics}. We select the following SOTA methods for evaluating the effectiveness of the proposed LMN. The \textbf{DNN} baseline is a typical DNN-based structure for CTR prediction. The basic sequential modeling methods include \textbf{Pooling} and \textbf{DIN} \cite{zhou2018deep} conducted on user sequence with click behaviors whose length and embedding size are 50 and 32, respectively. Based on DIN, we further introduce \textbf{SIM} \cite{pi2020search}, \textbf{MIMN} \cite{pi2019practice}, and the proposed \textbf{\baby}. The number of memory keys is set to 300. Both the AUC and LogLoss metrics are used to evaluate the ranking model performance. Further, we introduce relative improvement (Imp.) to measure the relative AUC gain \cite{yan2014coupled}. 



\subsection{Experimental Results}
\paratitle{Comparison Results}. The performance of different methods is summarized in Table~\ref{tab:auc}. There are the following observations. First, it is observed that sequence modeling is of great significance in recommendation, and the basic Pooling method achieves 1.32\% AUC improvement compared with the DNN baseline. Then, the personalized weights in DIN are more effective than Pooling, and it is observed that modeling a much longer sequence using SIM achieves better performance than DIN (0.7348 vs. 0.7327). Finally, compared to MIMN, the proposed \baby shows prominent effectiveness (4.68\% and 1.46\% improvements in AUC and LogLoss). We believe that the reason is due to the effectiveness of the user-aware memory learning mechanism that enables both spatial perception and temporal memorization. 

\paratitle{Performance of Scaling Up Memories}. Further, to validate the performance when scaling up the memory, we vary the number of memory keys from [50, 100, 200, 300, 500], and the results are illustrated in Table~\ref{tab:scalingup}. From the table, it can be observed that both performance metrics consistently improve with the number of keys scales up.

\begin{table}[]
\caption{Comparison results of different methods on Douyin E-Commerce Search Dataset. Boldface denotes the best results with a confidence level at $p < 0.05$.}
\begin{tabular}{ccccc}
\hline
        & AUC $\uparrow$ & AUC Imp. & LogLoss $\downarrow$ & LogLoss Imp. \\ \hline
Base    & 0.7265 & -        & 0.6075  & -            \\
Pooling & 0.7295 & +1.32\%  & 0.6053  & -0.36\%      \\
DIN     & 0.7327 & +2.34\%  & 0.6023  & -0.86\%      \\
SIM     & 0.7348 & +3.66\%  & 0.6009  & -1.09\%      \\
MIMN    & 0.7363 & +4.33\%  & 0.5991  & -1.39\%      \\
\textbf{\baby}     & \textbf{0.7371} & \textbf{+4.68\%}  & \textbf{0.5986}  & \textbf{-1.46\%}      \\ \hline
\end{tabular}
\label{tab:auc}
\end{table}


\begin{table}[]
\caption{Model performance with the number of memory keys scaling up.}
\begin{tabular}{ccccc}
\hline
        & AUC $\uparrow$ & AUC Imp. & LogLoss $\downarrow$ & LogLoss Imp. \\ \hline
Base    & 0.7265 & -        & 0.6075  & -            \\
\baby ($\sqrt{n}$=50) & 0.7357 & +4.06\%  & 0.5997  & -1.28\%      \\
\baby ($\sqrt{n}$=100)     & 0.7364 & +4.37\%  & 0.5993  & -1.35\%      \\
\baby ($\sqrt{n}$=200)     & 0.7370 & +4.64\%  & 0.5987  & -1.45\%      \\
\baby ($\sqrt{n}$=300)    & 0.7371 & +4.68\%  & 0.5986  & -1.46\%      \\
\baby ($\sqrt{n}$=500)     & 0.7380 & +5.08\%  & 0.5978  & -1.60\%      \\ \hline
\end{tabular}
\label{tab:scalingup}
\end{table}


\subsection{Online A/B Experiment}
We perform online A/B test from 30th Aug. to 5th Sep. 2024, with 20\% online traffic on Douyin E-Commerce Search to validate the effectiveness, which hits more than 160 million users. The results are illustrated in Table~\ref{tab:ab}. It can be observed that after deploying the proposed \baby, the most influential commercial metrics, i.e., order/user and order/search, are raised by 0.87\% and 0.72\%, respectively, with a substantial significance of $p$=0. Besides, after deploying the \baby, there is only a slight increment of online serving latency at 0.38\%, showing the efficiency of the deployment framework.

\begin{table}[]
\caption{Online A/B test result of the proposed \baby on Douyin E-Commerce Search. The boldface denotes that the improvement is significant with $p = $\textbf{0}.}
\begin{tabular}{ccc}
\hline
         & Order/User & Order/Search \\ \hline
Baseline & +0.00\%          & +0.00\%            \\
\textbf{\baby}      & \textbf{+0.87\%}    & \textbf{+0.72\%}      \\ \hline
\end{tabular}
\label{tab:ab}
\end{table}


\section{Conclusion}
In this paper, we present Large Memory Network (\baby), a novel memory-enhanced user sequential modeling and intention understanding framework for recommendation. It compresses and memorizes user interest through a large-scale memory block. With such design, the memory can 1) spatially perceive different users' sequences for better generalization, and 2) temporally memorize user long-term interest through a user-aware block. Besides, to further reduce computational cost and deployment overhead, both a product quantization-based memory decomposition and an industrial online memory parameter server framework are devised for industrial online deployment. Currently, \baby has been fully deployed at ByteDance, which serves the major online traffic of E-Commerce Search service at Douyin.

It is noted that instead of memorizing the user's historical behavioral items in the current work, it is a promising direction to memorize any important information within the industrial recommender models, for example, memorizing the output of the feature-cross modules, or the output of the high-level compressor MLPs, to achieve the goal of perceiving similar user's instances and memorizing the same user's instance and further improve the effectiveness of the recommendation model.

\balance
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}



\end{document}
\endinput