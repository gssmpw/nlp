\vspace{-0.2in}
\section{Introduction}
\vspace{-0.05in}
Understanding molecules and their properties is fundamental to understanding chemical compounds and living organisms, driving scientific discovery. However, it remains challenging due to the complexity of molecules and their behaviors, necessitating a comprehensive understanding of molecules. Recent advancements in large language models (LLMs)~\cite{touvron2023llama2,grattafiori2024llama3,openai2024gpt4,openai2024gpt4ocard} have demonstrated their potential in understanding core concepts in chemistry and biology~\cite{ai4science2023impactlargelanguagemodels,sadeghi2024can}. Further, LLMs have shown that language can play various roles, which is especially crucial to dealing with substantial complexity in chemistry, biology, and further pharmacology by readily obtaining external knowledge from users' prompts. Nevertheless, LLMs can only process string representations such as SMILES~\cite{weininger1988smiles}, struggling to understand the entire molecular structures.

To integrate structural information into LLMs, recent works have developed molecular LLMs which are instruction-tuned with the molecular modality, achieving notable success in task transfer. Specifically, recent works~\cite{liu-etal-2023-molca,cao2023instructmol,fang2023molinstruction,zhang2024unimot,li2024molm,park2024llamo,yu2024llasmol} adopt the multi-modal instruction tuning: 1) constructing molecule-text pairs from public databases or by utilizing GPTs to augment descriptions and 2) instruction-tuning LLMs with projectors in an end-to-end manner, as widely studied in multi-modal LLMs for image~\cite{li2023BLIP2,liu2024LLAVA1.5}, audio~\cite{fathullah2024audiochatllama}, and video~\cite{chen2023videollm,zhang2023videoLLAMA,maaz2024videochatgpt}. 

Despite their success, molecular LLMs still encounter difficulties in understanding the fundamental characteristics of molecules. Even though the molecular features span a wide range including structural, chemical, and biological features, their understanding falls short as the scope of knowledge handled by the instruction datasets is narrow and typically task-specific, forgetting the general knowledge of molecules and losing their capabilities to handle the users' requests. As a result, recent molecular LLMs struggle to accurately reason molecular properties and provide detailed rationales in the zero-shot setting, hindering their ability to serve as versatile, general-purpose molecular assistants.

To address this issue, we propose Mol-LLaMA, which learns fundamental knowledge centered on molecules, positioning it as a general-purpose molecular assistant. To this end, we first establish an instruction dataset that encompasses the core levels for molecular understanding, proposing three data types including detailed structural descriptions, structure-to-feature relationship explanations, and comprehensive conversations. Our dataset is focused on not only the fundamental knowledge of molecules but also the reasoning ability as it is designed to explicitly provide causality by associating molecular features to the structures. Further, to improve the structural understanding, we introduce a blending module that combines molecular information from both 2D and 3D encoders using the cross-attention mechanism. By integrating complementary information from different representations, the blending module facilitates alleviating the hallucination problem, improving to capture the structural information and thus enhancing the understanding of molecular features.

We experimentally validate the effectiveness of Mol-LLaMA in explaining the general features of molecules at the structural, chemical, and biological levels. Our results demonstrate that Mol-LLaMA outperforms baselines including LLMs and molecular LLMs, providing accurate, detailed, and helpful responses. We further evaluate Mol-LLaMA on the molecular property prediction task, where it not only accurately predicts molecular properties but also generates relevant and helpful explanations, highlighting its utility as a general-purpose assistant for molecular analysis. Our contributions can be summarized as follows:
\vspace{-0.1in}
\begin{itemize}[itemsep=0.5mm, parsep=3pt, leftmargin=*]
    \item We propose Mol-LLaMA that learns general knowledge for molecules across structural, chemical, and biological aspects, empowering to provide accurate and helpful responses with detailed explanations.
    \item We design three different data types for the general understanding of molecules, establishing a large and informative instruction dataset centered on molecular features.
    \item We devise a blending module to fully leverage the complementary information from different types of encoders, alleviating the hallucination and thus enhancing the understanding of molecular structures and advanced features.
    \item The proposed Mol-LLaMA outperforms previous LLMs and molecular LLMs including GPT-4o in the general understanding by learning the comprehensive knowledge centered on molecules.
\end{itemize}
