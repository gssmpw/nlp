\input{table/qualitative}
\vspace{-0.05in}
\section{Experimental Results}
\vspace{-0.05in}
We evaluate the capabilities of Mol-LLaMA by assessing the quality of generated responses to general questions and conducting the molecular property prediction task. To evaluate the qualification as a general-purpose assistant for molecular analysis, we assess LLMs in zero-shot settings without task-specific fine-tuning. 

\vspace{-0.05in}
\subsection{Baselines}
\vspace{-0.05in}
We compare the following large language models: GPT-4o~\cite{openai2024gpt4ocard} and Llama series~\cite{touvron2023llama2,grattafiori2024llama3} which are text-only general LLMs, Mol-Instructions~\cite{fang2023molinstruction} and LlasMol~\cite{yu2024llasmol} which learn the string representations such as SMILES or SELFIES via the public databases including PubChem~\cite{kim2021pubchem}, USPTO~\cite{wei2010uspto,lu2022uspto}, and MoleculeNet~\cite{wu2018moleculenet}, 3D-MoLM~\cite{li2024molm} where the inputs are SMILES and the 3D representation trained on the enriched descriptions, LLaMo~\cite{park2024llamo} which is trained on the generated conversations and other public datasets such as PubChem, MoleculeNet, and USPTO leveraging SMILES and the 2D molecular representations via the multi-level projector.

\vspace{-0.04in}
\subsection{Evaluation of General Understanding of Molecules}
\vspace{-0.04in}
\subsubsection{Qualitative Evaluation}
\vspace{-0.04in}
To show how well Mol-LLaMA understands the molecular structures and their properties, we ask a question for a molecule whose properties are widely studied as shown in Table~\ref{tab:qualitative}. Interestingly, Mol-LLaMA accurately predicts the main class of the given molecule, explains the relevant properties, and provides the rationales for the predicted properties by learning the general knowledge from the proposed dataset. In contrast, GPT-4o and 3D-MoLM misinterpret the key structures failing to provide correct properties, while LLaMo misinterprets the substructure and does not explain the relevant properties. On the other hand, Mol-Instructions inaccurately predicts the main class and does not provide additional explanations of the molecular features. For the entire responses and additional qualitative results, please refer Table~\ref{tab:qualitative_full_part1}, \ref{tab:qualitative_full_part2}, \ref{app:tab:additional_qualitative_adenosine}, and \ref{app:tab:additional_qualitative_nystatin} in Appendix~\ref{app:sec:additional_qualitative_results}. 

\input{table/general_understanding}

\vspace{-0.07in}
\subsubsection{Quantitative Evaluation\label{sec:quantitative}}
\vspace{-0.06in}
\paragraph{Experimental Setting}
We first select 100 representative molecules from the test set of PubChem dataset following splitting from \citet{li2024molm} by conducting the k-means clustering based on Morgan Fingerprints~\cite{morgan1965morgan}. Then, we instruct LLMs and molecular LLMs to describe general features of molecules such as structural, chemical, or biological features, respectively. Please refer Appendix~\ref{app:sec:details_quantitative_evaluation} for detailed explanations of experimental settings.


\vspace{-0.09in}
\paragraph{Evaluation Setting}
To quantitatively evaluate the general understanding ability, we leverage GPT-4o to measure the quality of generated responses inspired by \citet{liu2024LLAVA}. Specifically, after gathering the responses, we instruct GPT-4o to assess the quality of generated responses using the string representation and description from PubChem as references for judgment, considering the following four types of criteria: helpfulness, relevance, accuracy, and level of details. Additionally, we ask to assess the overall score and provide an explanation of its judgement to prevent a biased evaluation. We report the relative score compared to GPT-4o (i.e. the score of an LLM divided by the score of GPT-4o) for each criterion. We evaluate by querying GPT-4o three times, reporting the average score. For detailed evaluation settings, please refer Appendix~\ref{app:sec:details_quantitative_evaluation}.

\vspace{-0.08in}
\paragraph{Results} As shown in Table~\ref{tab:general_quantitative}, relative scores of Mol-LLaMA are beyond 1 for all criteria, indicating that it is superior to GPT-4o in the understanding of general features of molecules. In contrast, the scores of other baselines are mostly lower than 1, showing an inferior understanding of the general features. On the other hand, Mol-LLaMA shows a significant performance improvement compared to the base LLMs and outperforms all baselines on the same architecture, suggesting that it understands the general features and provides helpful and relevant explanations, thanks to the proposed instruction dataset and the blending module.

\vspace{-0.05in}
\subsection{Molecular Property Prediction}
\vspace{-0.05in}
\paragraph{Experimental Setting}
To assess the effectiveness of learning the general knowledge, we conduct a zero-shot evaluation on the parallel artificial membrane permeability assay (PAMPA) task from Therapeutics Data Commons (TDC) dataset~\cite{velez2024tdc}, where the task is to classify whether the given molecule has a high permeability or a low-to-moderate permeability to the artificial membrane. The reason why we adopt PAMPA task is that it requires capturing fundamental properties of molecules such as lipophilicity, molecular size, polarity, and membrane affinity, spanning from the chemical properties to the biological properties. To evaluate the ability to handle diverse requests, we use three prompting methods: the default setting (Default), the chain-of-thought prompting~\cite{wei2022CoT} (CoT), and prompting with task-specific information (w/ Task Info). For the default setting, we do not use additional prompting methods, for CoT prompting, we instruct to provide rationales while answering, and, for prompting with task-specific information, we provide property types that are relevant to predict the task, to demonstrate the ability to understand and leverage the additional information. Detailed evaluation settings are provided in Appendix~\ref{app:sec:details_molecular_property_prediction}.

\input{table/pampa}
\input{table/ablation}

\vspace{-0.05in}
\paragraph{Evaluation Setting} We consider three metrics: accuracy, fidelity, and helpfulness. Accuracy measures the ratio of correct answers, evaluating how an LLM accurately understands the molecules and their properties. Fidelity and helpfulness measure the quality of responses regardless of whether the final prediction is correct or not, to assess the qualifications as a practical assistant. Specifically, the fidelity score evaluates soundness and relevance, while the helpfulness score evaluates whether the responses are clear, informative, and helpful. We leverage GPT-4o to evaluate the fidelity and helpfulness and report the relative score compared to GPT-4o (i.e. the score of an LLM divided by the score of GPT-4o) for each criterion. Additionally, we report the ratio of the predicted labels to check whether an LLM is biased to predict the labels. Please refer Appendix~\ref{app:sec:details_molecular_property_prediction} for detailed prompts.

\vspace{-0.05in}
\paragraph{Result} Table~\ref{tab:PAMPA} shows that Mol-LLaMA achieves high accuracy outperforming GPT-4o, while showing high fidelity and helpfulness scores, demonstrating that it is able to accurately predict the molecular property with helpful explanations. Further, compared to the base LLMs, Mol-LLaMA shows a performance gain for both the accuracy and the quality of the responses by learning the general knowledge and reasoning ability from our constructed dataset. In contrast, other baselines lose the ability to provide relevant and helpful explanations, showing low fidelity and helpfulness scores compared to the base LLMs. Notably, Mol-LLaMA trained on Llama3.1 shows a consistent performance improvement when using CoT prompting and providing task-specific information, showing its ability to handle diverse requests from users. On the other hand, LLaMo (Llama2) usually predicts labels biasedly, while LLaMo (Llama3.1) shows inferior performances compared to the base LLM. We note that the models whose relative scores of fidelity and helpfulness are below 0.3 tend to directly predict the labels without any explanation as shown in the detailed analysis of Table~\ref{app:tab:pampa_MolLLaMA}, \ref{app:tab:pampa_GPT4o}, \ref{app:tab:pampa_3DMoLM} and \ref{app:tab:pampa_LLaMo} in Appendix~\ref{app:sec:additional_pampa}. We provide additional experimental results on another task (i.e. BBBP) in Table~\ref{app:tab:BBBP} of Section~\ref{app:sec:additional_bbbp}.

\vspace{-0.05in}
\subsection{Ablabtion Study}
\vspace{-0.05in}
\paragraph{Blending Module} To show the effectiveness of the proposed blending module, we ablate the different types of molecular representations. As shown in Table~\ref{tab:ablation} (Left), leveraging each type of encoder (2D and 3D) shows a superior understanding of molecular modality compared to GPT-4o by learning comprehensive knowledge of molecules from the proposed instruction dataset. On the other hand, concatenating 2D and 3D representations without the blending module (2D+3D (Concat)) attains performance improvement for chemical and biological features, whereas being degenerated for the structural understanding. Using the proposed blending module (2D+3D (Blended)) outperforms other variants, indicating that integrating the complementary information from different molecular representations via the proposed blending module is crucial to enhancing the structural understanding and learning the chemical and biological features. For the entire scores, please refer Table~\ref{app:tab:ablation_blending_module} of Appendix~\ref{app:sec:additional_ablation_study}.

\vspace{-0.05in}
\paragraph{Data Type} We ablate the different data types in Mol-LLaMA-Instruct to show the effect of each data type. As shown in Table~\ref{tab:ablation} (Right), learning structural descriptions exclusively or with structure-to-feature relationship explanations helps understand the general features. On the other hand, even though learning comprehensive conversations shows an inferior understanding of chemical and biological features, it allows to handle the diverse contexts of users' requests showing a large performance gain when predicting a molecular property with task-specific information. Training on the full data balances this trade-off, showing moderate performances both on the understanding of general features and the ability to handle the users' inquiries. Please refer Table~\ref{app:tab:ablation_data} of Appendix~\ref{app:sec:additional_ablation_study} for the entire scores including helpfulness, relevance, accuracy, and level of detail.
