\begin{table*}[htbp]
    \centering
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{1.05}
    \renewcommand{\tabcolsep}{7pt}
    \begin{tabular}{l c c c c c c c c c}
    \toprule
    & \multicolumn{3}{c}{Default} & \multicolumn{3}{c}{CoT} & \multicolumn{3}{c}{w/ Task Info.} \\
    \cmidrule(l{2pt}r{2pt}){2-4}
    \cmidrule(l{2pt}r{2pt}){5-7}
    \cmidrule(l{2pt}r{2pt}){8-10}
    Models & Acc.\small{ (Ratio)} & Fidel. & Help. & Acc.\small{ (Ratio)} & Fidel. & Help. & Acc.\small{ (Ratio)} & Fidel. & Help. \\
    \midrule
    GPT-4o & 48.65\small{ (59.95)} & - & - & 58.23\small{ (47.42)} & - & - & 47.17\small{ (62.41)} & - & - \\
    
    \midrule
    
    \rowcolor[RGB]{234, 238, 234} \multicolumn{10}{l}{\textit{Llama2-7B-Based}} \\
    Llama2 & 57.14\small{ (36.12)} & 0.517 & 0.508 & 57.53\small{ (39.56)} & 0.639 & 0.658 & 84.52\small{ (0.00)$^*$} & 0.658 & 0.718  \\
    Mol-Instructions & 49.63\small{ (47.67)} & 0.277 & 0.210 & 31.16\small{ (70.02)} & 0.314 & 0.270 & 38.18\small{ (68.80)} & 0.331 & 0.256  \\
    LLaMo & \textbf{84.28}\small{ \phantom{0}(0.74)} & 0.242 & 0.187 & 84.52\small{ (0.00)$^*$} & 0.246 & 0.191 & N/A & 0.226 & 0.185  \\
    \textbf{Mol-LLaMA (Ours)} & 75.68\small{ (11.30)} & \textbf{0.781} & \textbf{0.820} & \textbf{79.61}\small{ \phantom{0}(6.88)} & \textbf{0.759} & \textbf{0.793} & \textbf{67.90}\small{ (28.75)} & \textbf{0.757} & \textbf{0.744} \\
    
    \midrule    
    
    \rowcolor[RGB]{234, 238, 234} \multicolumn{10}{l}{\textit{Llama3.1-8B-Instruct-Based}} \\
    Llama3 &  56.51\small{ (45.70)} & 0.629 & 0.554 & 46.19\small{ (58.48)} & 0.795 & 0.786 & 63.64\small{ (34.15)} & 0.850 & 0.875 \\
    Mol-Instructions & 55.91\small{ (38.33)} & 0.245 & 0.207 & 33.50\small{ (73.96)} & 0.299 & 0.247 & 70.47\small{ (25.55)} & 0.245 & 0.206 \\
    3D-MoLM$^\dagger$ & 46.93\small{ (58.72)} & 0.668 & 0.651 & 50.00\small{ (51.35)} & 0.671 & 0.649 & 64.86\small{ (35.87)} & 0.767 & 0.744 \\
    LLaMo$^\dagger$ & 49.25\small{ (51.74)} & 0.265 & 0.212 & \textbf{64.37}\small{ (28.50)} & 0.254 & 0.209 & 48.51\small{ (53.73)} & 0.401 & 0.327 \\
    
    \textbf{Mol-LLaMA (Ours)} & \textbf{63.55}\small{ (36.86)} & \textbf{0.804} & \textbf{0.829} & \textbf{64.37}\small{ (31.94)} & \textbf{0.819} & \textbf{0.848} & \textbf{72.48}\small{ (17.44)} & \textbf{0.927} & \textbf{0.966} \\
    
    \bottomrule
    \end{tabular}}
    \vspace{-0.12in}
    \caption{\small Zero-shot performances on PAMPA task. We report accuracy with the ratio of predicted labels, and relative scores of fidelity and helpfulness (i.e. score of an LLM divided by score of GPT-4o). We highlight the best results in \textbf{bold}, except for the cases where all predicted labels are identical which are denoted as $^*$. N/A denotes the cases in which more than 20\% of the responses do not follow the answer format. $^\dagger$ Molecular LLMs that are trained on Llama3.1-8B-Instruct strictly following their official implementations.}
    \vspace{-0.1in}
    \label{tab:PAMPA}
\end{table*}