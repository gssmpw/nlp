
\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{
    \renewcommand{\arraystretch}{1.1}
    \renewcommand{\tabcolsep}{4pt}
    \begin{tabular}{l c c c c a c c c c a c c c c a}
    \toprule
        &
        \multicolumn{5}{c}{\textsc{Structural}} &
        \multicolumn{5}{c}{\textsc{Chemical}} &
        \multicolumn{5}{c}{\textsc{Biological}} \\
    \cmidrule(l{2pt}r{2pt}){2-6}
    \cmidrule(l{2pt}r{2pt}){7-11}
    \cmidrule(l{2pt}r{2pt}){12-16}
        Models & Help. & Relev. & Acc. & Details & Overall & Help. & Relev. & Acc. & Details & Overall & Help. & Relev. & Acc. & Details & Overall \\
    \midrule
    \rowcolor[RGB]{234, 238, 234} \multicolumn{16}{l}{\textit{Llama2-7B-Based}} \\
    Llama2-7B-Chat & 0.312 & 0.333 & 0.207 & 0.284 & 0.279 & 0.447 & 0.437 & 0.304 & 0.415 & 0.394 & 0.436 & 0.422 & 0.335 & 0.449 & 0.405 \\
    Mol-Instructions & 0.218 & 0.249 & 0.210 & 0.144 & 0.207 & 0.250 & 0.280 & 0.254 & 0.168 & 0.235 & 0.351 & 0.448 & 0.425 & 0.253 & 0.360 \\
    LlasMol & 0.251 & 0.266 & 0.221 & 0.192 & 0.228 & 0.273 & 0.301 & 0.235 & 0.213 & 0.252 & 0.346 & 0.410 & 0.390 & 0.298 & 0.353 \\
    3D-MoLM & 0.550 & 0.541 & 0.426 & 0.542 & 0.507 & 0.669 & 0.666 & 0.557 & 0.661 & 0.628 & 0.836 & 0.894 & 0.855 & 0.892 & 0.862 \\
    LLaMo & 0.314 & 0.396 & 0.348 & 0.206 & 0.310 & 0.359 & 0.459 & 0.447 & 0.240 & 0.361 & 0.498 & 0.734 & 0.803 & 0.340 & 0.568 \\
    \textbf{Mol-LLaMA (Ours)} & \textbf{1.105} & \textbf{1.121} & \textbf{1.105} & \textbf{1.066} & \textbf{1.098} & \textbf{1.202} & \textbf{1.242} & \textbf{1.288} & \textbf{1.185} & \textbf{1.232} & \textbf{1.495} & \textbf{1.706} & \textbf{1.875} & \textbf{1.468} & \textbf{1.631} \\
    \midrule
    \rowcolor[RGB]{234, 238, 234} \multicolumn{16}{l}{\textit{Llama3.1-8B-Instruct-Based}} \\
    Llama3.1-8B & 0.612 & 0.636 & 0.484 & 0.567 & 0.569 & 0.654 & 0.658 & 0.523 & 0.606 & 0.610 & 0.664 & 0.665 & 0.589 & 0.644 & 0.641 \\
    Mol-Instructions & 0.257 & 0.315 & 0.282 & 0.166 & 0.253 & 0.274 & 0.359 & 0.322 & 0.179 & 0.276 & 0.392 & 0.547 & 0.555 & 0.259 & 0.423 \\
    3D-MoLM$^\dagger$ & 0.778 & 0.800 & 0.680 & 0.759 & 0.749 & 0.882 & 0.936 & 0.838 & 0.854 & 0.875 & 1.105 & 1.272 & 1.292 & 1.145 & 1.191 \\
    LLaMo$^\dagger$ & 0.445 & 0.565 & 0.465 & 0.312 & 0.442 & 0.410 & 0.542 & 0.489 & 0.295 & 0.425 & 0.650 & 0.905 & 0.898 & 0.441 & 0.705 \\
    \textbf{Mol-LLaMA (Ours)} & \textbf{1.126} & \textbf{1.145} & \textbf{1.154} & \textbf{1.090} & \textbf{1.125} & \textbf{1.224} & \textbf{1.266} & \textbf{1.302} & \textbf{1.211} & \textbf{1.251} & \textbf{1.578} & \textbf{1.840} & \textbf{2.030} & \textbf{1.528} & \textbf{1.744} \\
    
    \bottomrule
    \end{tabular}}
    \hfill
    \vspace{-0.1in}
    \caption{\small Quantitative evaluation on the quality of generated responses for five criteria including helpfulness, relevance, accuracy, level of detail and overall score. We report the average of relative score (i.e. score of an LLM divided by score of GPT-4o) by running GPT-4o evaluation three times. $^\dagger$ Molecular LLMs that are trained on Llama3.1-8B-Instruct strictly following their official implementations.}
    \vspace{-0.05in}
    \label{tab:general_quantitative}
\end{table*}