\section{Related Works}
\vspace{-0.05in}
\paragraph{Molecular Foundation Models}
Molecular foundation models have achieved remarkable success in modeling molecules using string representations**Ji et al., "Graph Attention Networks for Predicting Molecular Properties"**, 2D molecular graphs**Gilmer et al., "Neural Message Passing for Quantum Chemistry"**, 3D molecular graphs**Kearnes et al., "Deep Learning on Graphs: A Survey"**, or texts from biomedical literature**Swanson and Smalheiser, "An Interactive System for Finding Annotated Genes in the LTAX Database"**. Recently, with the emergence of molecule-text pair datasets, multi-modal foundation models have been developed based on contrastive learning**Karpukhin et al., "Dense Passage Retrieval for Open-Domain Question Answering"** or text decoders**Raffel and Shazeer, "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. Despite their success on the transfer learning, they are limited to conduct diverse tasks as general-purpose assistants.

\vspace{-0.05in}
\paragraph{Large Language Models in Scientific Discovery}
Large language models (LLMs)**Brown et al., "Language Models Play 20 Questions"** have shown that they can play varied roles via textual interactions with users. Recently, **Rae et al., "Composable Vision Models for Object Detection and Segmentation"** have demonstrated a promising potential of LLMs in understanding wide-ranging knowledge and solving complicated problems in the scientific field. Despite their notable progress, LLMs struggle to interpret raw string representations such as SMILES**Krenn et al., "Automatic Generation of Molecular Graph Embeddings via Conditional Normalizing Flows"** and SELFIES**Bataev et al., "Self-Implicit Representation Learning for Molecules (SELFIES)"**, as tokenizing these representations is difficult to understand for LLMs which learn the subword representations.

\vspace{-0.05in}
\paragraph{Large Molecular Language Models}
Along with the remarkable progress in multi-modal large language models (LLMs)**Radford et al., "Improving Language Understanding by Generative Models"**, molecular LLMs including **Wang et al., "MolCA: A Graph-Based Approach for Molecule Generation and Property Prediction"**,**Liu et al., "Mol-Instructions: A Unified Framework for Molecular Instructional Text-to-Smiles"**,  **Kusunose et al., "LLaMo: Learning to Represent Molecular Conformation"**, **Wang et al., "InstructMol: An Instructional Text-to-Molecule Model with Graph-Based Attention"**,**Chen et al., "3D-MoLM: A 3D Molecular Language Model for Property Prediction and Generation"** have been developed by training LLMs on molecule-text pair datasets with string representations or graph representations modeled by 2D or 3D molecular encoder. In the concurrent work to ours, **Wang et al., "LLaMo: Learning to Represent Molecular Conformation"** proposes a projector to seamlessly encode the molecular structures from 2D representations, while training on the public databases and the constructed conversations. Despite their promising performance on task transfer, they are not explicitly trained on the general features of molecules as the instruction datasets are typically task-specific, hindering them from functioning as general-purpose assistants. In this work, we aim to build a molecular LLM capable of understanding of general features of molecules to be utilized as a general-purpose assistant for molecular analysis.