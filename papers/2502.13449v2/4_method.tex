\vspace{-0.05in}
\section{Mol-LLaMA}
\vspace{-0.05in}
We present Mol-LLaMA, a large molecular language model for the general understanding of molecules. 

\vspace{-0.05in}
\subsection{Preliminary of Multi-modal Instruction Tuning}
\vspace{-0.05in}
Multi-modal instruction tuning~\cite{liu2024LLAVA} is designed to tailor an LLM to other modalities expanding its ability to perceive and understand the other domains such as images or graphs. Specifically, the instruction dataset $\mathcal{D}$ is composed of pairs of a multi-modal input ($\bm X_m$) and the corresponding single-turn description or multi-turn conversation between a user and an assistant $(\bm X_u, \bm X_a)$. The loss objective is learning to generate the answer of an assistant given a multi-modal input and query from users, as follows:
\vspace{-0.2in}
\begin{align}
    \mathcal{L}(\theta) = -\sum^L_i \log p\left(\bm x_i | \bm X_m, \bm X_u, \bm X_{a,<i}\right),
    \label{eq:instruction_tuning}
\end{align}
\vspace{-0.25in}

where $L$ denotes the length of the answer. In this work, we consider the molecular graphs as the multi-modal inputs to align an LLM to understand the molecular features.

\vspace{-0.05in}
\subsection{General Molecular Instruction Dataset}
\vspace{-0.05in}
\paragraph{Goal} 
The instruction dataset poses at the core of the multi-modal instruction tuning promoting understanding of other modalities. In the context of our work, solving scientific problems requires complex reasoning, since, unlike images that humans can directly understand, scientific problems are interdisciplinary and are not straightforward to solve, necessitating wide-ranging expertise including chemistry and biology. To this end, an ideal instruction dataset for molecular LLMs should entail comprehensive information including structural, chemical, and biological features with detailed explanations. However, existing instruction datasets lack comprehensive and general information. For example, as shown in the description in Table~\ref{tab:data_example} (Context Type 2), while the original description provides structural information and some properties, these descriptions do not fully provide the overall properties of the given molecules in structural, chemical, and biological aspects. To address these issues, our goal is to establish an instruction dataset that explicitly presents the fundamental features of molecules and explains the principles of how these features arise, to build a large language model with wide-ranging knowledge focused on molecular structures and their features.

\input{table/data_example}

\vspace{-0.05in}
\paragraph{GPT-assisted Data Generation}
Our goal is to construct an instruction dataset that explains the fundamental features of molecules in detail. However, manually annotating the molecular features is tricky as it requires in-depth expertise in chemistry and biology. To mitigate this problem, we employ GPT-4o~\cite{openai2024gpt4ocard} that contains extensive knowledge of chemistry and biology~\cite{ai4science2023impactlargelanguagemodels}. Inspired by previous works~\cite{liu2024LLAVA,park2024llamo}, we prompt GPT-4o to generate instruction data by leveraging two types of contexts: 1) string representations of molecules and 2) their descriptions. For the descriptions, we use annotated descriptions from PubChem~\cite{kim2021pubchem} to provide grounded features. For the string representations, we use the IUPAC name, which explicitly specifies the names of functional groups and their connectivity~\cite{favre2013iupac}. 

The next question is how to design an instruction dataset that covers the fundamental features of molecules including structural, chemical, and biological features. To this end, we are inspired by the natural law, where the molecular features have a hierarchical relationship. That is, the structures determine the chemical features, while the biological features are determined by both the structural and chemical features. Based on our observation, we devise three data types that address these fundamental features, as shown in Table~\ref{tab:data_example}.

\vspace{-0.05in}
\begin{enumerate}[itemsep=0.5mm, parsep=3pt, leftmargin=*]
\item \textbf{Detailed Structural Description:}
We first design detailed descriptions of molecular structures to upskill the foundational understanding of molecules and improve the comprehension of advanced chemical and biological features. To achieve this, we prompt GPT-4o to include explanations of functional groups and their connectivity in detail. For structural descriptions, we use the IUPAC name exclusively, lensing on accurately describing the structural information without emphasizing the molecular functionalities.

\vspace{0.05in}
\item \textbf{Structure-to-Feature Relationship Explanation:}
To enable a comprehensive understanding of chemical and biological features, we create structure-to-feature relationship explanations considering the hierarchical relationships. Specifically, we instruct GPT-4o to relate the structural information to its chemical or biological features with detailed explanations. Note that learning these relationships inherently helps an LLM understand the causality between molecular structures and their features, allowing it to provide rationales when responding.

\vspace{0.05in}
\item \textbf{Comprehensive Conversation:}
Even though the two types above facilitate the learning of general knowledge about molecules, it is important to tailor LLMs to handle diverse contexts of requests. Thereby, we design conversations to cultivate the ability to handle various inquiries such as prompting with specific information of a molecule or a task. Specifically, considering the hierarchical relationships of molecular features, we create comprehensive conversations by instructing to gradually deepen the level of features progressing from structural features to chemical and biological features, aiming to empower the comprehensive understanding and the step-by-step reasoning ability.
\end{enumerate}

\vspace{-0.07in}
To ensure the quality of instruction-following samples, we further filter out factually incorrect ones. Inspired by LLM-as-a-judge~\cite{zheng2023llmasajudge}, we use GPT-4o to evaluate the factual accuracy of the samples and select those with correct contents. As a result, we collect 284k instruction-following samples, establishing 77k samples for the detailed structural descriptions, 147k samples for the structure-to-feature relationship explanations, and 60k samples for the comprehensive conversations, from the training set of the PubChem 324k dataset~\cite{li2024molm}, referring to the constructed instruction dataset as \textit{Mol-LLaMA-Instruct}. We note that our dataset not only aids in understanding the molecular features but also enhances reasoning abilities by extensively addressing fundamental molecular features and various types of interactions between users and an assistant. For more details on the dataset construction, please refer to Appendix~\ref{app:sec:dataset_construction}.

% 


\begin{figure*}[ht]
    \centering
    \includegraphics[height=4.0cm]{figure/architecture_overall4.pdf}
    \vspace{-0.3in}
    \caption{\small Illustration of the end-to-end instruction tuning stage of Mol-LLaMA. It is trained on the proposed instruction datasets, where the blending module, Q-Former, and LoRA in LLMs are trained, while the molecular encoders and LLM are frozen.}
    \label{fig:blending_module}
    \vspace{-0.1in}
\end{figure*}

\vspace{-0.05in}
\subsection{Model Architecture}
\vspace{-0.05in}
We now introduce the model architecture of Mol-LLaMA. Here, our goal is to accurately capture structural information, to further improve the understanding of molecular features. To this end, we propose to use both 2D and 3D representations without string representations. The model architecture consists of four components: molecular encoders, a 2D-3D blending module, a projector, and large language models.

\vspace{-0.05in}
\paragraph{Molecular Encoders}
We observe that each molecular encoder has distinct advantages. While the 2D encoder explicitly models the bond information and their connectivity, the 3D encoder, which represents the molecules as point clouds, captures the spatial arrangements of atoms which is crucial to understanding 3D features such as surface area and volumes. To fully leverage the strength of each encoder, we propose to use both the 2D encoder and 3D encoder. 

For the 2D encoder, we opt MoleculeSTM~\cite{liu2023moleculestm} which learns the molecular semantics from the biomedical texts through the contrastive learning between 2D molecular structure and textual description. For the 3D encoder, we opt UniMol~\cite{zhou2023unimol} which is trained via masked atom type prediction and position recovery. 

\vspace{-0.045in}
\paragraph{2D-3D Blending Module}
Since each representation is independently modeled, we propose a blending module that combines these molecular representations using a cross-attention scheme. Specifically, given the molecular embeddings from each encoder which are the concatenation of graph and node embeddings, self-attention and cross-attention are sequentially applied to blend the complementary information from each encoder. Then, we concatenate the 2D and 3D embeddings before forwarding to the projector, as shown in Fig.~\ref{fig:blending_module} (Blending Module).

\vspace{-0.05in}
\paragraph{Projector}
We employ Q-Former~\cite{li2023BLIP2,li2024molm} to project the unified molecular representations from the proposed 2D-3D blending module to an LLM. Q-Former is a transformer architecture with learnable query tokens, which embeds molecules by performing cross-attention between the unified molecular representations and the learnable query tokens as shown in Fig.~\ref{fig:blending_module} (Q-Former). It is worth noting that Q-Former architecture is advantageous especially for modeling graphs, as the cross-attention guarantees the permutation invariance. 

Following the recent work~\cite{li2024molm}, we opt to initialize the Q-Former with SciBERT~\cite{beltagy2019scibert} which is trained on a large corpus from scientific domains including the biomedical domain.

\vspace{-0.05in}
\paragraph{Large Language Models} We choose Llama-2-7b-chat~\cite{touvron2023llama2} and Llama-3.1-8B-Instruct~\cite{grattafiori2024llama3}, which are well-studied in the multi-modal LLM field and have demonstrated their capabilities in the multi-modal instruction tuning.

\vspace{-0.06in}
\subsection{Training}
\vspace{-0.06in}
Now, we turn out to introduce two training stages: molecular representation learning and end-to-end instruction-tuning.

\vspace{-0.06in}
\paragraph{Molecular Representation Learning}
In the first stage, we train the blending module and the Q-Former while freezing the 2D and 3D encoders. Following recent works~\cite{li2023BLIP2,li2024molm}, we adopt the multi-objectives to align the molecular embeddings to the molecule-relevant texts including molecule-text contrastive learning, molecule-text matching, and molecule-grounded text generation. We opt to use the IUPAC name as the molecule-relevant texts instead of using descriptions, to bridge the molecular embeddings to the key information in structures such as the functional groups and their connectivity. Note that the IUPAC name is more compact than the textual descriptions, making the training efficient. Please refer Section~\ref{app:sec:stage1_details} for a detailed explanation of molecular representation learning.

\vspace{-0.05in}
\paragraph{End-to-end Instruction Tuning}
As shown in Fig.~\ref{fig:blending_module}, we jointly train the blending module, Q-Former, and an LLM via the multi-modal instruction tuning (Eq.~\ref{eq:instruction_tuning}), while freezing the 2D and 3D encoders. We instruction-tune LLMs on the proposed instruction dataset without any additional datasets, employing LoRA~\cite{hu2021lora} for the efficiency of training. For the training details of the instruction tuning of Mol-LLaMA, please refer Section~\ref{app:sec:training_details}.