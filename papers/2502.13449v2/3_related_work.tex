\vspace{-0.09in}
\section{Related Works}
\vspace{-0.05in}
\paragraph{Molecular Foundation Models}
Molecular foundation models have achieved remarkable success in modeling molecules using string representations~\cite{chithrananda2020chemberta,fabian2020molbert,wang2019smilesbert,irwin2022chemformer}, 2D molecular graphs~\cite{ying2021graphormer,mendez2024mole}, 3D molecular graphs~\cite{zhou2023unimol,lu2024unimol+} or texts from biomedical literature~\cite{gu2021domain,lee2020biobert,beltagy2019scibert}. Recently, with the emergence of molecule-text pair datasets, multi-modal foundation models have been developed based on contrastive learning~\cite{su2022momu,liu2023moleculestm} or text decoders~\cite{edwards-etal-2022-molt5,zeng2022kvplm,luo2023molfm,christofidellis2023text+chemt5,liu2024gitmol}. Despite their success on the transfer learning, they are limited to conduct diverse tasks as general-purpose assistants.

\vspace{-0.05in}
\paragraph{Large Language Models in Scientific Discovery}
Large language models (LLMs)~\cite{openai2024gpt4,openai2024gpt4ocard,touvron2023llama2,grattafiori2024llama3} have shown that they can play varied roles via textual interactions with users. Recently, \citet{ai4science2023impactlargelanguagemodels} have demonstrated a promising potential of LLMs in understanding wide-ranging knowledge and solving complicated problems in the scientific field. Despite their notable progress, LLMs struggle to interpret raw string representations such as SMILES~\cite{weininger1988smiles} and SELFIES~\cite{krenn2020selfies}, as tokenizing these representations is difficult to understand for LLMs which learn the subword representations.

\vspace{-0.05in}
\paragraph{Large Molecular Language Models}
Along with the remarkable progress in multi-modal large language models (LLMs)~\cite{liu2024LLAVA,liu2024LLAVA1.5,xu2024llavacot}, molecular LLMs including MolCA~\cite{liu-etal-2023-molca}, Mol-Instructions~\cite{fang2023molinstruction},  LlasMol~\cite{yu2024llasmol}, InstructMol~\cite{cao2023instructmol}, and 3D-MoLM~\cite{li2024molm} have been developed by training LLMs on molecule-text pair datasets with string representations or graph representations modeled by 2D or 3D molecular encoder. In the concurrent work to ours, LLaMo~\cite{park2024llamo} proposes a projector to seamlessly encode the molecular structures from 2D representations, while training on the public databases and the constructed conversations. Despite their promising performance on task transfer, they are not explicitly trained on the general features of molecules as the instruction datasets are typically task-specific, hindering them from functioning as general-purpose assistants. In this work, we aim to build a molecular LLM capable of understanding of general features of molecules to be utilized as a general-purpose assistant for molecular analysis.
