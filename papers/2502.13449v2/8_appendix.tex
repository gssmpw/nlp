\newpage
\appendix
\onecolumn
\begin{center}{\bf {\LARGE Appendix}}\end{center}
\paragraph{Organization} Appendix is organized as follows: In Section~\ref{app:sec:additional_qualitative_results}, we provide additional qualitative results including the entire responses in Table~\ref{tab:qualitative} and other case studies. In Section~\ref{app:sec:details_MolLLaMA}, we explain details of the instruction dataset construction and the training. In Section~\ref{app:sec:experimental_details}, we provide experimental details for the evaluation settings. Finally, in Section~\ref{app:sec:additional_experiments}, we provide additional experimental results for the PAMPA prediction, ablation study, and BBBP prediction.

\vspace{-0.1in}
\section{Additional Qualitative Results\label{app:sec:additional_qualitative_results}}
\vspace{-0.05in}
In this section, we provide additional qualitative results to analyze the behaviors of Mol-LLaMA. First, we provide the entire responses of the case study in Table~\ref{tab:qualitative}. As shown in Table~\ref{tab:qualitative_full_part1}, Mol-LLaMA understands the molecular structures and correctly predicts the main class of the given molecule (i.e. Benzodiazepine), the attached functional group (i.e. pyridine), and its biological functionalities. Interestingly, Mol-LLaMA provides additional information, detailing the effects of each structural component such as the blood-brain barrier penetration from the pyridine ring, the binding affinity to the GABA-A receptor from the halogen atom, and the inhibitory effects of benzodiazepines. In contrast, as shown in Table~\ref{tab:qualitative_full_part2}, GPT-4o, 3D-MoLM, and Mol-Instructions misinterpret the molecular structures and give the incorrect features of molecules. On the other hand, LLaMo misinterprets the substructures (i.e. methyl group) and does not provide the molecular features, focusing on describing the structural information.

\vspace{-0.03in}
Additionally, as shown in Table~\ref{app:tab:additional_qualitative_adenosine}, one notable ability of Mol-LLaMA is that, beyond the properties annotated in PubChem, Mol-LLaMA understands various aspects of the molecular properties that can be found in the scientific literature, such as regulating the secretion of insulin~\cite{heseltine1995adenosineinsulin}, inhibiting the immune cells~\cite{wang2024adenosineimmune}, and protecting neurons~\cite{dall2003adenosineneuroprotection}. Further, as shown in Table~\ref{app:tab:additional_qualitative_nystatin}, Mol-LLaMA provides detailed rationales for the predicted properties. Specifically, Mol-LLaMA explains the key structure (i.e. macrolides) and additional structural features (i.e. polyene chain), and then explains that these structural features are related to the specific biological properties (i.e. antifungal activity), providing principles of the predicted properties (i.e. binding to ergosterol)~\cite{serhan2014polyene}. We hope that these additional qualitative results show the potential of Mol-LLaMA as a general-purpose assistant for molecular analysis.

\input{table/additional_qualitative_results/qualitative_full_part1}
\clearpage
\input{table/additional_qualitative_results/qualitative_full_part2}
\clearpage
\input{table/additional_qualitative_results/adenosine}
\input{table/additional_qualitative_results/nystantin}

\vspace{-0.05in}
\section{Details of Mol-LLaMA\label{app:sec:details_MolLLaMA}}
\vspace{-0.07in}
\subsection{Instruction Dataset Construction\label{app:sec:dataset_construction}}
\vspace{-0.07in}
\paragraph{Details of GPT-assisted Data Generation} First, we randomly select 100k molecules from the train set of PubChem following the splitting from \citet{li2024molm} for each data type including detailed structural descriptions, structure-to-chemical features, structure-to-biological features, and comprehensive conversations. Then, we employ GPT-4o-2024-08-06 to construct the instruction datasets with different prompts as shown in Table~\ref{app:tab:prompts_construction}. For the data filtering, the prompts are provided in Table~\ref{app:tab:prompts_filtering}, where we select samples whose scores are 4. The entire responses for the example in Table~\ref{tab:data_example} are provided in Table~\ref{app:tab:data_example_full_part1} and \ref{app:tab:data_example_full_part2}.

\vspace{-0.07in}
\paragraph{Instructions}
We provide the instructions for the detailed structural description, the structure-to-chemical feature relationship explanations, the structure-to-biological feature relationship explanations, and the comprehensive conversations in Table~\ref{app:tab:instructions_structural}, \ref{app:tab:instructions_chemical}, \ref{app:tab:instructions_biological}, and ~\ref{app:tab:instructions_conversation}, respectively. Instructions for each data type have similar semantics. For the comprehensive conversations, the user prompts are the generated questions.

\vspace{-0.07in}
\subsection{Training Details\label{app:sec:training_details}}
\vspace{-0.07in}
\paragraph{Blending Module}
The number of heads of the blending module is 8, and the number of blocks is 4, where each block consists of a sequence of one self-attention block and one cross-attention block.

\vspace{-0.07in}
\paragraph{Molecular Representation Learning\label{app:sec:stage1_details}}
In the molecular representation learning stage, Q-Former is constituted of two transformers: molecular transformer and text transformer as shown in Fig.~\ref{app:fig:stage1_overall}. The molecular transformer embeds the molecular information by the cross-attention between learnable query tokens and the molecular embeddings with an additional cross-attention block. The text transformer models the molecule-relevant texts while maintaining the original transformer architecture.

To train Q-Former, we adopt three training objectives proposed in \citet{li2024molm}: molecule-text matching, molecule-text contrastive learning, and molecule-grounded text generation. Specifically, we choose IUPAC name as the molecule-relevant text to compactly learn the molecular structures. Therefore, we refer them to the structure-IUPAC matching, structure-IUPAC contrastive learning, and structure-grounded IUPAC generation. The structure-IUAPC contrastive learning and structure-IUPAC matching aim to learn the similarity via the cosine similarity or the binary classification, respectively. The structure-grounded IUPAC generation aims to learn the text generation via the next token prediction.
For each training objective, the self-attention masking strategies are different. For the structure-IUPAC contrastive learning, self-attention is performed on each modality. For structure-IUPAC matching, the self-attention is performed for all tokens without masking. For the structure-grounded IUPAC generation, the causal mask is applied.

The blending module and the Q-Former are trained for 50 epochs. The optimizer is AdamW optimizer~\cite{loshchilov2017adamw} with a weight decay of 0.05 and a cosine scheduler with 1000 steps of linear warmup where the peak and minimal learning rates are 1e-4 and 5e-6. The number of query tokens is 8 and the batch size is 256. 

\begin{figure*}[h!]
    \centering
    \vspace{-0.12in}
    \includegraphics[height=5.5cm]{figure/stage1_appendix5.pdf}
    \vspace{-0.13in}
    \caption{Detail illustration of blending module and Q-Former and their training.}
    \label{app:fig:stage1_overall}
    \vspace{-0.1in}
\end{figure*}

\vspace{-0.07in}
\paragraph{End-to-end Instruction Tuning}
We leverage LoRA~\cite{hu2021lora} where the rank ($r$) is 8, $\alpha$ is 32, and the dropout ratio is 0.1. We use the same optimizer configuration in the molecular representation learning stage, while training for 10 epochs with 128 batch sizes.

\vspace{-0.07in}
\paragraph{Resources} We train Mol-LLaMA on NVIDIA H100 and NVIDIA A100 80GB.

\clearpage
\input{table/prompts/dataset_construction}
\input{table/prompts/filtering}

\clearpage
\input{table/data_example_full_part1}

\clearpage
\input{table/data_example_full_part2}

\clearpage
\input{table/instructions/detailed_structural_description}
\input{table/instructions/structure2chemical}
\input{table/instructions/structure2biological}

\clearpage
\input{table/instructions/conversations}

\section{Experimental Details\label{app:sec:experimental_details}}
\subsection{Quantitative Evaluation on General Understanding of Molecules\label{app:sec:details_quantitative_evaluation}}
To evaluate the general understanding of molecules, we ask general questions about structures and chemical and biological properties, respectively, as follows: ``Explain the structural features of the given molecule.'', ``Explain the chemical properties of the given molecule.'', and ``Explain the biological properties of the given molecule.''. To assess the quality of responses, GPT-4o is provided the IUPAC name, the original descriptions annotated in PubChem~\cite{kim2021pubchem}, and the questions asked to the assistants as references. Then, given the responses of two assistants, GPT-4o assesses the scores of each response for five criteria: helpfulness, relevance, accuracy, level of detail, and overall scores.

\subsection{Molecular Property Prediction\label{app:sec:details_molecular_property_prediction}}
The prompts for predicting PAMPA results are provided in Table~\ref{app:tab:prompts_pampa_infer} and the prompts for evaluating the reasoning processes on the PAMPA task are provided in Table~\ref{app:tab:prompts_pampa_eval}. If the generated responses do not follow the designated format of the final answer, we add the final answer format (i.e. ``Final answer: '') at the end of the generated responses and let LLMs generate in succession to make the final decision based on their previous reasoning process.

\input{table/prompts/pampa_infer}
\newpage
\input{table/prompts/pampa_eval}

\section{Additional Experimental Results\label{app:sec:additional_experiments}}

\begin{wrapfigure}{r}{0.27\textwidth}
    \vspace{-0.1in}
    \centering
    \hspace{-0.4in}\includegraphics[width=0.28\textwidth]{figure/PAMPA_example.pdf}
    \vspace{-0.1in}
    \caption{Input molecule}
    \label{app:fig:pampa_analysis_input_molecule}
\end{wrapfigure}
\subsection{Analysis of Generated Responses on PAMPA Task\label{app:sec:additional_pampa}}
To further understand the behaviors of LLMs and molecular LLMs, we analyze the entire responses on PAMPA task for a molecule of Fig.~\ref{app:fig:pampa_analysis_input_molecule}. As shown in Table~\ref{app:tab:pampa_MolLLaMA}, in the default setting, Mol-LLaMA tends to reason in a substructure-wise manner. With the chain-of-thought prompting, the response is similar to the one of the default setting as Mol-LLaMA already provides the rationales in the default setting. For the case with the task-specific information, Mol-LLaMA successfully follows the instructions, providing detailed explanations for each given property. On the other hand, as shown in Table~\ref{app:tab:pampa_GPT4o}, GPT-4o tends to reason in a property-wise manner, while the response with chain-of-thought prompting is not largely changed similarly to Mol-LLaMA. In contrast, as shown in Table~\ref{app:tab:pampa_3DMoLM}, 3D-MoLM often misinterprets the molecular structures, leading to an incorrect prediction. LLaMo usually does not provide a helpful response, directly making the final decision, as shown in Table~\ref{app:tab:pampa_LLaMo}.

\subsection{Ablation Study\label{app:sec:additional_ablation_study}}
In Table~\ref{app:tab:ablation_blending_module} and \ref{app:tab:ablation_data}, We report all scores of the ablation study in Table~\ref{tab:ablation} including helpfulness, relevance, accuracy, and level of detail.

\input{table/additional_results/ablation_blending_module}
\input{table/additional_results/ablation_data}

\input{table/additional_results/bbbp}
\subsection{Molecular Property Prediction on BBBP\label{app:sec:additional_bbbp}}
We additionally conduct an experiment on BBBP task, where the task is to predict whether a given molecule can penetrate the blood-brain barrier. The brain-blood barrier is a system with high selectivity by complicated transport routes such as transporter-mediated transcytosis, receptor-mediated transcytosis, cell-mediated transcytosis, lipophilic pathway, efflux pumps, adsorptive transcytosis, and paracellular aqueous pathway~\cite{wu2023bbbstructure}. Therefore, the task requires external knowledge such as the structure of tight junctions, the interactions with membrane proteins, and the environment of BBB. We follow the experimental settings in Section~\ref{sec:quantitative}, using three types of prompt strategies: default setting, CoT prompting, and providing task-specific information. As shown in Table~\ref{app:tab:BBBP}, Mol-LLaMA shows high performance, achieving a performance improvement compared to the base LLMs. However, GPT-4o outperforms all baselines due to its extensive knowledge.


\clearpage
\input{table/additional_results/pampa_MolLLaMA}
\clearpage
\input{table/additional_results/pampa_GPT4o}
\clearpage
\input{table/additional_results/pampa_3DMoLM}
\input{table/additional_results/pampa_LLaMo}