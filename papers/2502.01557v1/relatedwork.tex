\section{Related work}
\label{section:related_work}

\paragraph{Convergence of SGD:}
A number of works prove under different assumptions that the (forward) iterates of SGD with constant learning rate do not converge toward points but rather toward a stationary probability distribution: \cite{merad2023convergence} and \cite{dieuleveut2020bridging} show this for the strongly convex case; \cite{shirokoff2024convergencemarkovchainsconstant} treats the non-convex case with separable loss; \cite{babichev2018constantstepsizestochastic} focus on losses coming from exponential family models; \cite{cheng2020stochastic} quantify the rate of convergence of SGD to its stationary distribution in non-convex optimization problems; see also \cite{dieuleveut2016nonparametric} and \cite{meyn_tweedie_1993}. In \cite{huang2017snapshot}, the authors leverage the fact that SGD with fixed large learning rate oscillates between different solutions in order to create a cheap average of models by saving the explored parameters along the way. For convergence to a particular solution, the forward order for SGD needs an extra decaying learning rate schedule as shown in \cite{robbins1951a_stochastic} or \cite{mertikopoulos2020on}.

\paragraph{Contractions in deep learning:} 
From our point of view, one important feature that leads to convergence for the backward trajectory is the contraction property of the random operators. This notion (which we believe is under-exploited in deep learning) has surfaced in different contexts in deep learning: see \cite{BFGQ19}, \cite{QW19}, and \cite{AK22}. 

\paragraph{Markov chains, iterated functions, and MCMC:}
In many Markov Chain Monte Carlo (MCMC) algorithms the goal is to sample from a distribution $\mu$. The idea is to construct a Markov chain with stationary distribution $\mu$ and then run the chain for a long time to get samples from $\mu$. The Propp-Wilson algorithm uses a form of backward iterations to accelerate the convergence toward samples from the distribution (see \cite{propp1996exact}). More generally, the idea of backward dynamics is hidden in many constructions in Markov chain theory when the Markov chain is given by iterations of random operators as illustrated in \cite{diaconis1999iterated}. In particular, they prove a general result concerning the convergence in distribution of these types of iterated Markov chains using the backward dynamics.

\paragraph{Stability in deep learning:}
Already instability issues appear in the full-batch regime, and a number of theoretical works have studied it under the heading \emph{edge of stability} \cite{cheng2020stochastic, Wu2024large_stepsize, cai2024large_stepsize}. Other works have also studied stability in the large batch regime after a batch size saturation takes place using the implicit conditioning ratio \cite{lee2022trajectory,Agarwala2024High}. In this work we focus on the stochastic or small batch setting. Our findings do not really matter for the full-batch setting since backward and forward iterates then coincide. In the context of physics-informed neural networks, it has been observed that the gradient field is stiff, producing instabilities in learning trajectories \cite{Wang2020UnderstandingAM}. To remedy this \cite{li2023implicitstochasticgradientdescent} propose a backward Euler scheme to stabilize training in this context. However the backward Euler method is an implicit Runge-Kutta method, which is different from using iteration backward.

\paragraph{Sample order:}
Curriculum learning \cite{soviany2022curriculumlearningsurvey} leverages the impact of sample order for generalization \cite{mange19dataorder} by organizing training examples in a meaningful sequence, typically starting with simpler examples and gradually introducing harder ones, thereby optimizing the learning process and improving model performance. The backward optimization can be viewed as an automated form of curriculum, mitigating the forgetting of previous examples as new examples are added; a phenomenon which is also related to catastrophic forgetting and the stability gap in continual learning (see \cite{lange2023continual} for instance).