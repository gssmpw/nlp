\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage{tikz}
\usepackage{adjustbox}

\usepackage{hyperref}
\usepackage{authblk}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}


\renewcommand{\P}{\mathbb{P}}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}


\title{Training in reverse: \\ How iteration order influences convergence and stability in deep learning}

\author[1]{Benoit Dherin$^*$}
\author[2]{Benny Avelin$^*$}
\author[2,3]{Anders Karlsson$^*$$^{\S}$}
\author[1]{Hanna Mazzawi}
\author[1]{Javier Gonzalvo}
\author[1]{Michael Munn}

\affil[1]{Google Research}
\affil[2]{Department of Mathematics, Uppsala University}
\affil[3]{Department of Mathematics, University of Geneva}



\begin{document}

\maketitle
\def\thefootnote{*}\footnotetext{Equal contribution; corresponding author \texttt{dherin@google.com}}
\def\thefootnote{§}\footnotetext{Supported in part by the Swiss NSF grants 200020-200400, 200021-212864 and the Swedish Research Council grant 104651320.}

\begin{abstract}
Despite exceptional achievements, training neural networks remains computationally expensive and is often plagued by instabilities that can degrade convergence. While learning rate schedules can help mitigate these issues, finding optimal schedules is time-consuming and resource-intensive. This work explores theoretical issues concerning training stability in the constant-learning-rate (i.e., without schedule) and small-batch-size regime. Surprisingly, we show that the order of gradient updates affects stability and convergence in gradient-based optimizers.  We illustrate this new line of thinking using backward-SGD, which processes batch gradient updates like SGD but in reverse order. Our theoretical analysis shows that in contractive regions (e.g., around minima)  backward-SGD converges to a point while the standard forward-SGD generally only converges to a distribution. This leads to improved stability and convergence which we demonstrate experimentally. While full backward-SGD is computationally intensive in practice, it highlights opportunities to exploit reverse training dynamics (or more generally alternate iteration orders) to improve training. To our knowledge, this represents a new and unexplored avenue in deep learning optimization.
\end{abstract}

\section{Introduction}

In recent years, neural networks have achieved remarkable success across diverse domains from text generation \cite{gemini152024, gemini2023, brown2020language} and image creation \cite{ramesh2022hierarchical, ramesh2021zeroshot, saharia2022photorealistic} to applications in protein folding \cite{jumper2021highly,jumper2021highlyaccurate} and material discovery \cite{merchant2023scaling}. However, their training remains challenging and computationally expensive. One of the reasons for this is due to training instabilities which often occur \cite{wang2023exploring,li2019variance,cohen2021gradient,chen2018stability} and which produces hard to interpret loss curves, wasted computation time, and potentially failed experiments. One way to view this  challenge is as a trade-off between stability and performance:  hyperparameter settings that often yield better test performance, such as higher learning rates and smaller batch sizes, tend to exacerbate these instabilities.

As a basic example, consider the training batch size. For many common optimizers, smaller batch sizes often lead to improved test performance. In fact, recent research has shown that small batches induce a form of implicit regularization \cite{Novack2023Disentangling,smith2021on,dherin2022why,keskar2017large,ali2020implicit} which benefits generalization. On the other hand, the greater variability of small batches tends to exacerbate oscillations of the training loss, prolonging time to convergence. In this work, we demonstrate that these instability and convergence issues associated with small batch sizes can be mitigated by applying the optimizer in reverse; i.e., iterating through batches in a backward order, which we now explain.

Namely, standard training algorithms, such as stochastic gradient descent (SGD), Adam, and other gradient-based optimizers, are iterative processes.  At each step, these algorithms update the network parameters $\theta$ using a randomly sampled data batch $B_i$.  This update can be formalized as a transformation, $\theta' = T_i(\theta)$, where $\theta'$ represents the new parameter value, aiming to reduce the training loss on batch $B_i$.  Because at each step $i$ the batch $B_i$ is randomly sampled, the update operator $T_i$ can be formalized as a random operator.  For instance, in the case of SGD, the random operator is defined by the update rule $T_i(\theta) = \theta - h \nabla L_{B_i}(\theta)$, where $L_{B_i}$ is the loss function evaluated on batch $B_i$ and $h$ is the learning rate.  The sequence of parameter updates generated by these iterations, starting from an initial parameter value $\theta$, defines the standard learning trajectory, which we will refer to as the \emph{forward trajectory}:
\begin{equation*}
    \theta,\quad 
    T_1(\theta), \quad 
    T_2T_1(\theta),\quad \dots, \quad  
    %T_3 T_2\circ T_1(\theta),\dots, \quad 
    T_n T_{n-1} \cdots T_1(\theta).
\end{equation*}
{\bf Notation:} To conserve notation, we  write $T_i T_j$ to denote the mapping composition $T_i \circ T_j$ ; i.e., $T_i T_j(\theta) = T_i(T_j(\theta))$.

Moderate or large learning rates and small batches, when used with the standard forward trajectory for training, tend to destabilize common gradient-based optimizers, causing convergence and stability issues. Consequently, learning rate schedules have become essential as an attempt to encourage proper convergence of the loss during training.  Our key contribution, which we support with both theoretical analysis and empirical results (see Figure \ref{figure:training_losses_cifar10_resnet18} and Appendix \ref{appendix:experiment_details}), is in demonstrating that reversing the iteration order--what we call the \emph{backward trajectory}--leads to significantly more stable convergence.  Specifically, the backward trajectory consists of iterates generated by applying the training batches in reverse order; i.e., 
\begin{equation*}
    \theta,\quad 
    T_1(\theta), \quad 
    T_1 T_2(\theta),\quad \dots, \quad 
    %T_1 \circ T_2\circ T_3(\theta),\dots, \quad 
    T_1 T_2 \cdots  T_n(\theta).
\end{equation*}
For the sake of clarity, let us work out the first two iterates both for forward and backward. For instance for SGD, if we receive first the batch $B_1$ and then the batch $B_2$ in sequence in the training loop, then at the first step both the backward and forward iterates coincides with $T_1(\theta) = \theta - h \nabla L_{B_1}(\theta)$, where $\theta$ in the randomly initialized parameter value. However at the second step the forward iterate becomes
\begin{align*}
    T_2T_1(\theta) 
    &= T_1(\theta) - h \nabla L_{B_2}(T_1(\theta)) \\
    &= \theta - h \nabla L_{B_1}(\theta) - h\nabla L_{B_2}(\theta - h \nabla L_{B_1}(\theta)).
    %& = & \theta - h\nabla \Big(L_{B_1}(\theta)  + L_{B_2}(\theta) \Big) + \mathcal O(h^2)
\end{align*}
On the other hand, the second step of the backward iterate is
\begin{align*}
    T_1T_2(\theta) 
    &= T_2(\theta) - h \nabla L_{B_1}(T_2(\theta)) \\
    &= \theta - h \nabla L_{B_2}(\theta) - h\nabla L_{B_1}(\theta - h \nabla L_{B_2}(\theta)).
    %& = & \theta - h\nabla \Big(L_{B_1}(\theta)  + L_{B_2}(\theta)\Big) + \mathcal O(h^2)
\end{align*}
A naive (and computation intensive) implementation of the backward optimization is depicted in Figure \ref{figure:backward_flow} where all the batches received so far at each steps are re-processed from scratch in the reverse order in which they were received from the initialization point. 

\begin{figure}[t] 
\centering
\begin{adjustbox}{max width=1\linewidth}
\input{backward_flow.tkz}
\end{adjustbox}
\caption{Naive implementation of the backward dynamics: Forward iterations (left) and backward iterations (right). The training steps are represented by Pac-men consuming batches. Forward iterations maintain a training state and consume a new batch at each step, while backward iterations restart the training and consume all the batches received so far in reverse order.}
\label{figure:backward_flow}
\end{figure}

\paragraph{Main contributions:} The main contributions of this paper are to show theoretically (Theorem \ref{lemma:contraction_principle} and Theorem \ref{thm:backward_forward}) and experimentally (Figure \ref{figure:training_losses_cifar10_resnet18} and Appendix \ref{appendix:experiment_details}) that backward optimization has better convergence and stability properties than the standard forward optimization. As already known (see related work in Section \ref{section:related_work}), forward trajectories do not generally converge toward points but rather toward a probability distribution of the iterates (in the fixed learning-rate regime). 

We argue that the advantage of backward trajectories comes from their convergence toward actual points (see Theorem \ref{lemma:contraction_principle}) sampled from the forward distribution (see Theorem \ref{thm:backward_forward}).
We prove this using a generalization of the Banach fixed point theorem when the random maps $T_i$ become contractions. 
Note that backward optimization can theoretically be applied to any gradient-based optimizer (not only to SGD) to improve stability and convergence, since our theoretical statements hold at the level of the $T_i$'s, no matter what their actual forms are. 
To our knowledge, these concepts are new in deep learning optimization.
The main goal of this paper is to expose how this phenomenon manifests in deep learning. We defer engineering applications leveraging this phenomenon (like efficient implementations of the backward SGD) to future work, while outlining a few potential directions at the paper's conclusion.


\subsection{Related work}\label{section:related_work}

\paragraph{Convergence of SGD:}
A number of works prove under different assumptions that the (forward) iterates of SGD with constant learning rate do not converge toward points but rather toward a stationary probability distribution: \cite{merad2023convergence} and \cite{dieuleveut2020bridging} show this for the strongly convex case; \cite{shirokoff2024convergencemarkovchainsconstant} treats the non-convex case with separable loss; \cite{babichev2018constantstepsizestochastic} focus on losses coming from exponential family models; \cite{cheng2020stochastic} quantify the rate of convergence of SGD to its stationary distribution in non-convex optimization problems; see also \cite{dieuleveut2016nonparametric} and \cite{meyn_tweedie_1993}. In \cite{huang2017snapshot}, the authors leverage the fact that SGD with fixed large learning rate oscillates between different solutions in order to create a cheap average of models by saving the explored parameters along the way. For convergence to a particular solution, the forward order for SGD needs an extra decaying learning rate schedule as shown in \cite{robbins1951a_stochastic} or \cite{mertikopoulos2020on}.

\paragraph{Contractions in deep learning:} 
From our point of view, one important feature that leads to convergence for the backward trajectory is the contraction property of the random operators. This notion (which we believe is under-exploited in deep learning) has surfaced in different contexts in deep learning: see \cite{BFGQ19}, \cite{QW19}, and \cite{AK22}. 

\paragraph{Markov chains, iterated functions, and MCMC:}
In many Markov Chain Monte Carlo (MCMC) algorithms the goal is to sample from a distribution $\mu$. The idea is to construct a Markov chain with stationary distribution $\mu$ and then run the chain for a long time to get samples from $\mu$. The Propp-Wilson algorithm uses a form of backward iterations to accelerate the convergence toward samples from the distribution (see \cite{propp1996exact}). More generally, the idea of backward dynamics is hidden in many constructions in Markov chain theory when the Markov chain is given by iterations of random operators as illustrated in \cite{diaconis1999iterated}. In particular, they prove a general result concerning the convergence in distribution of these types of iterated Markov chains using the backward dynamics.

\paragraph{Stability in deep learning:}
Already instability issues appear in the full-batch regime, and a number of theoretical works have studied it under the heading \emph{edge of stability} \cite{cheng2020stochastic, Wu2024large_stepsize, cai2024large_stepsize}. Other works have also studied stability in the large batch regime after a batch size saturation takes place using the implicit conditioning ratio \cite{lee2022trajectory,Agarwala2024High}. In this work we focus on the stochastic or small batch setting. Our findings do not really matter for the full-batch setting since backward and forward iterates then coincide. In the context of physics-informed neural networks, it has been observed that the gradient field is stiff, producing instabilities in learning trajectories \cite{Wang2020UnderstandingAM}. To remedy this \cite{li2023implicitstochasticgradientdescent} propose a backward Euler scheme to stabilize training in this context. However the backward Euler method is an implicit Runge-Kutta method, which is different from using iteration backward.

\paragraph{Sample order:}
Curriculum learning \cite{soviany2022curriculumlearningsurvey} leverages the impact of sample order for generalization \cite{mange19dataorder} by organizing training examples in a meaningful sequence, typically starting with simpler examples and gradually introducing harder ones, thereby optimizing the learning process and improving model performance. The backward optimization can be viewed as an automated form of curriculum, mitigating the forgetting of previous examples as new examples are added; a phenomenon which is also related to catastrophic forgetting and the stability gap in continual learning (see \cite{lange2023continual} for instance).


\section{A backward contraction principle} \label{sec:contractions}

The contraction mapping principle, also called the Banach fixed point theorem, is a cornerstone result in mathematics and science, in particular for finding solutions of equations (Newton's method) or of differential equations (Picard's method of successive approximation).
It is also behind Google's PageRank algorithm \cite{PageBrin98}.
It concerns the existence of a fixed point of maps which
are uniform contractions of a complete metric space \footnote{Recall that a complete metric space $(\Omega, d)$ is a set $\Omega$, equipped with a distance metric $d(\theta_1,\theta_2)$ for which all Cauchy sequences (i.e., sequences $\theta_n$ such that $d(\theta_n, \theta_m) \rightarrow 0$ as $n,m\rightarrow 0$) converge to points in the space; typically $\mathbb R^d$ with the Euclidean distance is a complete metric space.}. In detail, let
$(\Omega,d)$ be a complete metric space and $T:\Omega\rightarrow \Omega$ be a continuous map. If there exists $0\leq k<1$ such that
\[
    d(T(\theta_1),T(\theta_2))\leq k\cdot d(\theta_1,\theta_2),
\]
for all $\theta_1,\theta_2\in \Omega$, then $T$ is called a \emph{uniform contraction}. The fixed point theorem now states that $T$ has a fixed
point, that is, $\theta_{0}\in \Omega$ such that $T(\theta_{0})=\theta_0$. This point can be found by iterating the map:
$$
T^{n}(\theta)\rightarrow \theta_{0}
$$
for any $\theta\in \Omega$ as $n\rightarrow\infty$. For example, in PageRank \cite{PageBrin98}, one iterates the PageRank matrix from 50 to 100 times and the result is a very good approximation of the PageRank vector $\theta_0$.
Another application is the standard proof of convergence for (full-batch) gradient descent around a minimum, which can be interpreted as an application of the Banach fixed point theorem as detailed in the next example:

\begin{example} {\bf (Full-batch gradient descent convergence.)} \label{example:full_batch_convergence}In this case the operator is $T(\theta) = \theta - h\nabla L(\theta)$ for a loss function $L$. The idea of the proof is to choose the learning rate $h$ small enough so that $T$ becomes a contraction. More precisely, around a minimum $\theta^*$ the operator $T$ can be approximated using a first-order expansion of the gradient around the minimum as $T(\theta) = \theta^* + (1 - hH)(\theta-\theta^*),$
where $H=\nabla^2 L(\theta^*)$. Now we have that 
\begin{equation}
    \|T(\theta_1) - T(\theta_2)\| \leq \|1 - hH\|_{\textrm{op}} \|\theta_1 - \theta_2\|,
\end{equation}
where $\|1 - hH\|_{\textrm{op}}$ is the operator norm of $1 -hH$, that is the operator maximum eigenvalue: $\max_i | 1 - h\lambda_i|$ (here the $\lambda_i$'s are eigenvalues of $H$). It is easy to verify that $ \|1 - hH\|_{\textrm{op}} < 1$ if and only if the learning rate is strictly smaller than $2/\lambda_{\textrm{max}}$ where $\lambda_{\textrm{max}}$ is the largest eigenvalue of $H$. Convergence for that setting follows from the Banach fixed point theorem. 
\end{example}

The generalization of this convergence argument to Stochastic Gradient Descent (SGD) is actually problematic. The main reason is that now we do not have a single operator but a sequence of them $T_1, T_2, \dots$, each computing the loss gradient on a different batch of data. Even if each of the operators is a contraction it turns out that usual forward iterations will not converge to a point in general as illustrated in Example \ref{example:counter-example}.
Theorem~\ref{lemma:contraction_principle}, however,  establishes the convergence of the backward iterations.

\begin{theorem}\label{lemma:contraction_principle}
    (Backward contraction mappings principle) Let $T_{i}$ be a sequence of continuous self-maps of a complete metric space.
    Assume 
    %that from some index $N$ onwards $i\geq N$, 
    $T_i$'s are uniform
    contractions, with a certain $k<1$ in common, and for some $\theta$ there is a constant $D$ such that,
    \begin{eqnarray}\label{condition:boundedness}
        d(\theta,T_{i}(\theta))<D\quad \textrm{for all} \; i.
    \end{eqnarray}
    Then for any $\theta \in \Omega$ the backward iterates 
    $$\theta_n = T_{1} T_{2}\cdots T_{n}(\theta)$$
    converge to a point $\theta^*$ as $n\rightarrow \infty$. Moreover, the convergence rate is exponential: i.e,  there is a constant $C$ depending on $\theta$ such that 
    $$
    d(\theta^*, T_{1} T_{2}\cdots T_{n}(\theta))\leq C\cdot k^{n}.
    $$
\end{theorem}

\begin{proof}
    Condition \ref{condition:boundedness} expresses that the distance $d(\theta, T_i(\theta))$ is uniformly bounded by a constant $D$ for all $T_i's$ for a point $\theta$. Let us show that if this happens for a single point $\theta$, this happens for all points, provided we change the constant $D$. To see this, take another point $\tilde \theta$. We will compute another constant $\tilde D$ such that $d(\tilde \theta, T_i(\tilde \theta)) < \tilde D$. Namely, using the triangle inequality and the fact that $T_i$ are uniform contractions, we obtain that
    \begin{eqnarray*}
        d(\tilde \theta,T_i(\tilde \theta)) 
        & \leq & d(\tilde \theta, \theta)+d(\theta,T_i(\theta))+d(T_i( \theta),T_i (\tilde \theta)) \\
        & \leq & D+(1+k)d(\theta, \tilde \theta) = \tilde D.
    \end{eqnarray*}
    Now, we want to prove that $\theta_n$ is a Cauchy sequence, i.e, that $d(\theta_n, \theta_m)$ tends to zero for $m>n$ as $n\rightarrow \infty$. Since $\Omega$ is assumed to be complete, this will mean that the backward iterates $\theta_n$ converge toward a point $\theta^*$.  
    The idea is to bound the quantity
    \begin{eqnarray*}
    A & = &  d(\theta_n, \theta_m) \\
      & = & d(T_{1}T_{2}\cdots T_{n}(\theta),\,T_{1}T_{2}\cdots T_{m}(\theta)).
    \end{eqnarray*}
    Because of the backward order (note: the forward order would not allow that), we can apply the contraction property $n$ times (since $m> n$), yielding:
    \begin{eqnarray*}
        A & \leq & k^{n}\cdot d(\theta,\,T_{n+1}\cdots T_{m}(\theta)).
    \end{eqnarray*}
Now a simple application of the triangle inequality produces
    \begin{eqnarray*}
        A & \leq &  k^{n}\Big(d(\theta,T_{n+1}(\theta)) \\
          &      & + d(T_{n+1}(\theta),\,T_{n+1}T_{n+2}(\theta))+\cdots\\
          &      & \cdots +d(T_{n+1}\cdots T_{m-1}(\theta),\,T_{n+1}\cdots T_{m}(\theta))\Big).
    \end{eqnarray*}
At this point, we can use the condition in \eqref{condition:boundedness} for the first term $d(\theta,T_{n+1}(\theta)) < D$ and in conjunction with the contraction property for the subsequent terms in the sum, yielding:
    \begin{eqnarray*}
        A  & \leq &  k^{n}\left(D+D\cdot k+\cdots + D\cdot k^{m-n+1}\right) \\
           & \leq & D\cdot k^n  \cdot \left( \frac{1 - k^{n-m+2}}{1-k}\right) \\
          & \leq & \frac{D\cdot k^{n}}{1-k},
    \end{eqnarray*}
    where we used the sum of a geometric series. Now, this yields that $A\rightarrow 0$ as $n\rightarrow\infty$, meaning that the sequence $\theta_n$
    is a Cauchy sequence and thus converges to a certain point $\theta^*$
    since $\Omega$ is complete. 
    Then, taking the limit $m\rightarrow \infty$ we obtain
    $$
    d(\theta_n,\theta^*) \leq \frac{D\cdot k^{n}}{1-k} = C\cdot k^n,
    $$
    with $C:=\frac{D}{1-k}$, which shows the exponential convergence rate.
\end{proof}

\begin{remark}
Let us verify that the condition in \ref{condition:boundedness} is satisfied for SGD with a loss of form $L(\theta)= \frac {1}{N} \sum_{i=1}^N L_i(\theta)$, where $L_i$ is the loss computed on batch $B_i$. We further assume that the $L_i(\theta)$ are positive and that there exists at least one point $\theta^*$ where the full loss vanishes (which implies that $L_i(\theta^*) = 0$ for all $i$'s). Both assumptions are completely reasonable for over-parameterized models like deep neural networks. For SGD, the operators are $T_i(\theta) = \theta - \nabla L_i(\theta)$. Now it is easy to see that condition in \ref{condition:boundedness} is satisfied:
$
d(\theta^*, T_i(\theta^*)) = \|\theta^* - \theta^* - h\nabla L_i(\theta^*)\| = 0,
$
since the global zero $\theta^*$ is a critical point for all the $L_i$'s. 
\end{remark}

Now here is a major point: The contraction mapping principle fails with the forward order. As the next example demonstrates, it is not enough for the operator $T_i$'s to be uniform contractions for the forward sequence of iterates to converge, while this is always true for the backward sequence because of \cref{lemma:contraction_principle}.\\


\begin{example}\label{example:counter-example}
({\bf Forward iterations counter-example.})
Here is an extreme example illustrating the convergence failure for the forward sequence of iterates (even when the maps are uniform contractions), while the backward sequence converges to a single point under the conditions of Theorem \ref{lemma:contraction_principle}. Consider the two constant maps $S(\theta)=x_0$, and $U(\theta)=y_0$, which are contractions with $k=0$. Assume that $x_{0}\neq y_{0}$,
now also assume that each $T_{i}$ is either equal to $S$ or $U$ with equal probability to be selected. Then, independently of $\theta$, the forward iterates
\[
    T_{n}T_{n-1}\cdots T_{1}(\theta)
\]
will jump between $x_{0}$ and $y_{0}$, according to whether $T_{n}$
is $S$ or $U$ at that particular $n$. Thus no convergence to a single point is possible, although the sequence converges to a probability  uniformly distributed in the two outcomes, since for each forward iterate either outcome has probability $1/2$. On the contrary, the backward trajectory
\[
    T_1 T_2\cdots T_n(\theta)
\]
will always converge to a single point determined by the first element in the sequence: either to $x_0$ if $T_1 = S$ or to $y_0$ if $T_1 = U$.
\end{example}


\subsection{A fundamental example of a contraction}

We are now describing how the SGD updates are uniform contractions when the batch losses are strictly convex near a minimum.
First, consider a strictly convex smooth function $L$. Recall that $L$ is \emph{strictly convex} if 1) there is a constant $m>0$ such that
\begin{equation*}
    \langle \nabla L(\theta_1) - \nabla L(\theta_2), \theta_1-\theta_2 \rangle \geq m \left \| \theta_1-\theta_2 \right \|^2,
\end{equation*}
where $\langle a, \,b\rangle = a^T b$ is the inner product between two vectors, and 2) there is a constant $M > m$ such that
\begin{equation*}
    \left \| \nabla L(\theta_1) - \nabla L(\theta_2) \right \| \leq M \left \| \theta_1-\theta_2 \right \|.
\end{equation*}
The following lemma (whose proof is in Appendix \ref{appendix:proof_of_contraction}) shows that for strictly convex functions we can always find a learning rate small enough so that the corresponding gradient descent update is a contraction. 
\begin{lemma} \label{lem:contraction}
    Let $L(\theta)$ be a strictly convex function and define the map $T(\theta)=\theta-h\nabla L(\theta)$.
    Then
    \begin{equation*}
        \left \| T(\theta_1)-T(\theta_2) \right \| \leq \sqrt{1 - 2 h m + h^2 M^2} \left \| \theta_1-\theta_2 \right \|.
    \end{equation*}
    In particular, for small enough $h \in (0,1)$ (depending on $m$ and $M$) the map $T$ is a uniform contraction.
\end{lemma}


Now if we suppose that batch losses $L_i$'s are strictly convex (possibly near global minima), then \cref{lem:contraction} tells us that the SGD maps $T_i(\theta) = \theta - h\nabla L_i(\theta)$ are uniform contractions. Thus we can apply \cref{lemma:contraction_principle} and see that the sequence of backward iterates, $\theta_n = T_1 T_2\cdots T_n(\theta)$, converges to a point.


\subsection{Relation between forward and backward iterations}

As seen in Example \ref{example:counter-example}, an actual realization of a forward trajectory converges to a distribution which is uniformly distributed between the two outcomes. On the other hand, any realization of the backward trajectory always converges to a point sampled from this distribution. Let's formalize this for the general case.

First of all, recall that a map $T:\Omega\rightarrow \Omega$ produces a corresponding push-forward map $T_*:P(\Omega)\rightarrow P(\Omega)$ at the level of probability measures $P(\Omega)$ on $\Omega$. Recall that a probability measure $\mu\in P(\Omega)$ associates to each subset $A\subset \Omega$ a number $\mu(A)$ modelling the probability $\textrm{Prob}_\mu(\theta \in A)$ of $\theta$ being in the subset $A$ if $\theta$ is sampled from $\mu$.
Now, if $\mu$ is a probability measure on $\Omega$ then the push forward $T_* \mu (A) = \mu (T^{-1}(A))$ for $A\subset \Omega$ computes the probability $\textrm{Prob}_\mu(T(\theta)\in A)$ of $T(\theta)$ being in $A$ if $\theta$ is sampled according to $\mu$. 

Therefore, the forward sequence $(T_n \cdots T_2T_1):\Omega\rightarrow \Omega$ induces a push-forward $(T_n \cdots T_2T_1)_*$ that maps an initial distribution $\mu_0$ to the probability measure $\mu_n = (T_n \cdots T_2T_1)_* \mu_0$ modelling the probability distribution of the $n^{th}$ forward iterate $\theta_n = T_n \cdots T_2T_1(\theta_0)$ when the initial point $\theta_0$ is randomly sampled from $\mu_0$. Observe that the forward iterates $\theta_n$ form a sequence of random variables, which is a Markov chain (since for the forward iterates we have that $\theta_n = T_n(\theta_{n-1})$). Many previous works (see related work in Section \ref{section:related_work}) have shown under different assumptions that this sequence of forward iterates do not converge to a point when using fixed learning rates, but rather their probability distributions $\mu_n$ converge to a limiting probability distribution, called the stationary distribution of the Markov chain.
In other words, starting at initial point $\theta_0$ the distribution $\mu_n = (T_n)_*\cdots (T_1)_* \delta_{\theta_0}$ of the forward iterate $\theta_n$ converges to a stationary distribution $\mu_{\theta^*}$. Recall that $\delta_{\theta_0}$ is the Dirac measure concentrated at $\theta_0$ (i.e., $\delta_{\theta_0}(A)$ is $1$ if $A$ contains $\theta_0$ and 0 otherwise).

On the contrary, as we have seen in Theorem \ref{lemma:contraction_principle}, the backward iterates converge to actual points in regions where the maps $T_i$'s become contractions (which we will call \emph{contractive regions}). The following theorem (whose proof is in Appendix \ref{appendix:proof_of_theorem_2.6}) shows the relation between the point-wise convergence of the backward iterates and the distributional convergence of the forward iterates:

\begin{theorem} \label{thm:backward_forward}
Consider a sequence $\{T_i\}$, $i=1,2, \dots$ of independent and identically distributed random operators. Suppose that for $\theta_0\in \Omega$ the backward iterates converge to a random point (randomness is due to the sampling of the random operators $T_i$'s):
$$
 T_1T_2\cdots T_n(\theta_0) \longrightarrow \theta^* \quad\textrm{as}\quad n\rightarrow \infty.
$$
Then the probability distribution of the forward iterates from $\theta_0$ converge (in distribution) to a stationary probability measure $\mu_{\theta^*}$. Moreover, the random point $\theta^*$ is distributed according to the same forward iterate stationary distribution $\mu_{\theta^*}$. 
\end{theorem}


\section{Two explicit examples on stochastic gradient descent}\label{sec:examples}

We now present two examples that provide an intuition on why backward SGD converges to a point rather than to a distribution as forward SGD does, when the learning rate is fixed. The reason in both examples is that the stochastic noise added to the parameter iterates at each step due to the randomization of the batch diminishes to zero in backward SGD as the training progress even with fixed learning rates, while this stochastic noise stays unchanged at each step of forward SGD, requiring a learning rate decay for convergence. 

\subsection{Example: Quadratic loss}

Consider the quadratic loss function $L(\theta)=\theta^{2}/2$ with $\theta\in \mathbb R$. Its gradient is $\theta$. We want to find the minimum
of $L$ (which of course is $0$ at $\theta=0$) by a stochastic gradient
descent. To model the batch noise, we add a i.i.d random error $\epsilon_{i}$ to the gradient at step $i$, yielding the iteration procedure:
\[
    T_{i}(\theta)=\theta - h\Big(\nabla L(\theta) - \epsilon_i\Big) = (1-h)\theta+h \epsilon_{i},
\]
where $h$ is the learning rate. 
These maps are uniform contractions provided that $\theta\in (0,1)$ since
\[
|T_i(\theta_1) - T_i(\theta_2)| = |(1-h) (\theta_1 - \theta_2)| \leq |1 - h| |\theta_1 - \theta_2|.
\]
Now we would like to compare the behavior of the forward trajectory to that of the backward trajectory for the $T_i$'s. In order to do this we begin by calculating the forward iterates:
\begin{eqnarray*}
    T_{2}T_{1}(\theta) & = & (1-h)((1-h )\theta+h \epsilon_{1})+h \epsilon_{2}\\
    & = & (1-h)^2\theta + h \epsilon_{2} +(1-h)h\epsilon_{1}.
\end{eqnarray*}
One sees easily how this continues:
\[
    T_n\cdots  T_1(\theta)   =   (1 - h)^n \theta + h \epsilon_n  + \cdots + h (1 - h)^{n-1} \epsilon_1.
\]
By switching indices, for the backward iterates we obtain:
\[
    T_n\cdots  T_1(\theta)   =   (1 - h)^n \theta + h \epsilon_1  + \cdots + h (1 - h)^{n-1} \epsilon_n.
\]
We see that the forward iterates receive at each step $n$ an additional error $h\epsilon_n$ which stays constant during the whole trajectory preventing point-convergence, while for backward iterates that same error  $h (1 - h)^{n-1} \epsilon_n$ is decaying to zero as the trajectory progresses.

\subsection{Example: Learning dynamics close to minima}

The previous example was a warm up for the following more realistic example. However, the key idea is identical. Consider a loss function
\[
    L(\theta)=\frac{1}{N}\sum_{i=1}^{N}L_{i}(\theta).
\]
where $L_{i}$ is the loss computed on the random batch $B_i$. The SGD update is
\[
    T_{i}(\theta)=\theta-h \nabla L_i(\theta)
\]
where $h$ is again the learning rate. We now compare forward and backward SGD dynamics sufficiently close to a minimum  $\theta^*$ of the loss function so that we can approximate well the full loss gradient $g(\theta) = \nabla L(\theta)$ using the loss Hessian $H(\theta) = \nabla g(\theta)$: i.e., a first order Taylor's expansion of the gradient at the minimum yields
\begin{equation}\label{equation:gradient_approximation}
    g(\theta) \simeq g(\theta^*) + H(\theta^*) (\theta - \theta^*) = H(\theta^*)(\theta - \theta^*),
\end{equation}
since at a minimum $g(\theta^*)  = 0$.
To simplify the notation, we will denote by $g_i(\theta)$ the gradient of the batch loss $L_i(\theta)$, which is now a random vector (because the batch is random). We can model this stochastic gradient as follows: $g_i(x) = g(x) - \epsilon_i$, where we assume that the  differences $\epsilon_i = g(x) - g_i(x)$ are i.i.d random vectors with zero mean. Using gradient approximation \eqref{equation:gradient_approximation} we have that a single gradient update is given in a form very similar to that of the previous example:
\begin{equation*}
    T_i(\theta)  
    =  \theta - h\Big(g(\theta) - \epsilon_i\Big)
    \simeq \theta^* + (1 - h H)(\theta^* -\theta) + h \epsilon_i
\end{equation*}
where $H$ denotes the Hessian evaluated at the minimum $\theta^*$. Since 
\begin{equation*}
\|T_i(\theta_1) - T_i(\theta_2)\| \leq \|1 - hH\| \|\theta_1 - \theta_2\|
\end{equation*}
and since $\|1 - hH\| < 1$ for $h$ strictly smaller than $2/\lambda_{max}$ (as in \cref{example:full_batch_convergence}), \cref{lemma:contraction_principle} tells us that the backward trajectory converges. Now let us have a look at the form of the forward and backward iterates in this particular case to give us an intuition about why this is true. In a similar way than in the previous example, the $n^{th}$ forward SGD iterate is given by
\begin{eqnarray*}
    T_n\cdots  T_1(\theta)  & = & \theta^* + (1 - h H)^n (\theta - \theta^*) \\
    & & + h \epsilon_n + h (1 - h H) \epsilon_{n-1} + \cdots \\
    & & \cdots + h (1 - h H)^{n-1} \epsilon_1.
\end{eqnarray*}
While  the term $(1 - h H)^n (\theta - \theta^*)$  converges to zero (when $h < 2/\lambda_{max}$), we see that at each step $n$ a fresh random vector $h \epsilon_n$ is added to the iterate making the whole sequence converge to a distribution rather than to a point. On the other hand the backward iterate is obtained by reversing the indices:
\begin{eqnarray*}
    T_1\cdots  T_n(\theta)  & = & \theta^* + (1 - h H)^n (\theta - \theta^*) \\
    & & + h \epsilon_1 + h (1 - h H) \epsilon_{2} + \cdots \\
    & & \cdots + h (1 - h H)^{n-1} \epsilon_n.
\end{eqnarray*}
This expression converges as $n\rightarrow \infty $ since the terms are decreasing at an exponential rate as $n$ grows.
Thus the iterates converge to a point now rather than a distribution. Looking at the actual form of the backward iterates, it is clear that they are distributed by the probability measure given by the forward iterates as prescribed by \cref{thm:backward_forward}. 

\section{Experiments}\label{sec:experiments}

{\bf Note on plotting multiple seeds:} We are interested in the variability per realization of the backward and forward trajectories. The backward trajectories are more stable individually along their own path, but these paths can be very different from seed to seed (because of the convergence toward different points). Therefore the phenomenon is much clearer when the seeds are plotted individually rather than averaged, which creates artificially more variability in the backward trajectories as there really is on each individual realization. This is why we are reporting the various seed plots individually and not as a single averaged plot with error bars. Note that the increased stability and point convergence is visible in each of the seeds, which are added to the Appendix \ref{appendix:multiple_seeds}.


\paragraph{Increased convergence stability for backward SGD:} 
We trained a ResNet-18 with stochastic gradient descent and no regularization on the CIFAR-10 dataset 
\cite{krizhevsky2009learning}. We used a learning rate of 0.025 and a batch-size of 8. In Figure \ref{figure:training_losses_cifar10_resnet18}, we recorded the learning curves at \emph{each} gradient update for both the forward and backward iterations for 2000 steps. The additional seeds are in \cref{appendix:figure_1}. 
We observe that the training loss for the backward trajectory is more stable, converges faster, and has less variability than the forward trajectory, and similarly for all the other learning curves.
In Appendix \ref{appendix:experiment_details}, we repeat the experiment with different datasets (synthetic dataset, FashionMNIST, and CIFAR-100) as well as different architectures (ResNet-50, VGG, and MLP). Each time we observe the same phenomenon for all 5 seeds. 

\paragraph{Convergence toward points v.s.~distributions:} We trained a MLP with 5 layers of 500 neurons each with stochastic gradient descent with  no  regularization to learn FashionMNIST \cite{xiao2017fashion}. The batch-size was set to 8 while the learning rate was 0.001. In Figure \ref{figure:intermittent_backward_fashion_mnist}, we report the test accuracy at every single step for 2000 steps. At step 1000, we reset the point from which we perform backward SGD to that of the parameter at this step. As a result we see that the backward trajectory from that reset point seems to converge again, but to a different point (with different test accuracy), while forward SGD seems to oscillate between these two points. This is in line with the theoretical prediction that backward trajectories SGD converge toward points distributed according to the distribution induced by forward trajectories (see \cref{thm:backward_forward}). The additional seeds are in \cref{appendix:figure_2}. 

\begin{figure}[h] 
\centering
\includegraphics[width=0.35\textwidth]{plots/resnet18_cifar10_bs=8_lr=0pt025_seed=0.pdf}
\caption{
Backward SGD exhibits decreased variance and increased stability compared to forward SGD for a ResNet-18 model trained on CIFAR-10. The additional seeds are in \cref{appendix:figure_1}. 
}
\label{figure:training_losses_cifar10_resnet18}
\end{figure}

\begin{figure}[h] 
\centering
\includegraphics[width=0.35\textwidth]{plots/intermittent_backward_fashion_mnist.png}
\caption{
Backward SGD converges toward a different minima after resetting the initialization point at step 1000 (``intermittent backward") while forward SGD oscillates between them for MLP trained on FashionMNIST. {\bf Top:} On the first seed, backward changes from a higher test-performance trajectory to a lower test-performance trajectory at the reset step 1000. {\bf Bottom:} On the second seed, backward changes this time from a trajectory converging to a lower test-performance point to a trajectory converging to a higher test-performance point. The other seeds can be found in Appendix \ref{appendix:figure_2} including all the learning curves.
}
\label{figure:intermittent_backward_fashion_mnist}
\end{figure}


\section{Conclusion and future work}

In this paper, we analyzed the impact of two different iteration orders on the convergence and stability of stochastic learning trajectories. Although the very idea of backward trajectories may seem  strange and counterintuitive at first, we show that this approach of backward optimization has clear advantages. Namely, we show theoretically that the backward trajectories converge toward actual points while the forward trajectories converge toward probability distributions in regions where the optimization maps become contractions, leading to improved stability and convergence.
We observed this phenomenon experimentally on a number of datasets (synthetic, FashionMNIST, CIFAR-10, and CIFAR-100) and neural network architectures (MLP, VGG-19, ResNet-18, and ResNet-50), making it relevant to deep learning.
This phenomenon points toward the fact that the ordering in which the data examples are consumed to produce each parameter iterate impacts the properties of both the learning trajectories and their convergence points. 
In particular the last examples used to produce a parameter iterate seem to have great importance. Indeed, the backward order is the only order in which the sequence of the last examples used to produce a parameter iterate remains consistent at each training step. As we saw, this yields convergence toward a point rather than oscillations between solutions of varied performances.  

However, realistic applications of the full backward order are challenging due to the prohibitive computation time that grows quadratically with the number of batches consumed. We now point toward conjectural workarounds and applications to showcase possible exploitation of this phenomenon. This is outside of the scope of this paper; nevertheless, we give supporting evidence in the appendix when possible. 

One possible strategy to mitigate the intensive computational requirements of the full backward order is to apply it only partially on a fixed window of iterations from a point that is reset periodically. This way the extra computation time is capped by the window size. We see that this approach leads to convergence and increased stability in Figure \ref{figure:intermittent_backward_fashion_mnist}; however the backward trajectory may change and converge to a different point at each reset. We can imagine strategies to initiate this reset only when the performance of the new backward trajectory increases, leading to stable convergence to higher test performance points. Another possibility is to start the backward order only later in training to force convergence toward a single solution after the forward trajectory has done enough exploration of the space as demonstrated in Figure \ref{figure:forward_backward} in Appendix \ref{appendix:experiment_details}. We imagine that this approach may possibly lead toward more reliable early-stopping criteria. Similarly, application of the backward order at the very beginning of training may give an accurate idea of whether a specific hyper-parameter setting is promising or needs to be aborted by creating stabler trajectories whose learning curves are easier to interpret early on.

An orthogonal approach to deal with the computational challenges of backward optimization is to approximate the backward trajectory. In Appendix \ref{appendix:approximate_backward}, we show how to compute a backward iterate from a cheaper forward iterate to which we add a correction that can be computed completely independently. We compute these corrections up to order 2 in Theorem \ref{thm:approximate_backward} Appendix \ref{thm:approximate_backward}, namely:
\begin{equation*}
    T_1 \cdots T_n = T_n \cdots T_1 + h^2 \sum_{1\leq i<j\leq n} [\nabla L_i, \nabla L_j] + \mathcal O(h^3)
\end{equation*}
where $[\nabla L_i, \nabla L_j](\theta) = H_i(\theta)\nabla L_j(\theta) - H_j(\theta)\nabla L_i(\theta)$ is the Lie bracket between the vector fields $\nabla L_i$ and $\nabla L_j$.
Even though the approximation we provide is not immediately useful because the higher order terms seem to matter, it probably will be useful if higher order corrections are included. As a proof of the usefulness of these type of corrections added to the forward iterates to emulate alternate orderings, we show in Appendix \ref{appendix:implicit regularization} that the corrections we computed at second order are already enough to approximate an average between all the possible iteration orders on a batch window. Adding these corrections to the forward iterates produces a beneficial regularizer that mimics small batch training regularization. In fact, a training step can be seen as a parameter average of mixture of models where each model is trained not from a different seed but from a different batch order. We hope that this paper will bring awareness and foster more research toward understanding and exploiting the role of iteration order in the production of models with more consistent properties along more stable learning trajectories. 



\section*{Acknowledgements}

We would like to thank Michael Wunder, Peter Bartlett, George Dahl, Atish Agarwala, and Scott Yak for helpful discussion and feedback.



\begin{thebibliography}{10}

\bibitem{gemini152024}
T.~Team Gemini.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of
  tokens of context.
\newblock {\em arXiv preprint arXiv:2403.05530}, 2024.

\bibitem{gemini2023}
Team Gemini.
\newblock Gemini: A family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1877--1901, 2020.

\bibitem{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 2022.

\bibitem{ramesh2021zeroshot}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock {\em arXiv preprint arXiv:2102.12092}, 2021.

\bibitem{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
  Denton, Seyed~Kamyar Ghasemipour, Burcu~Karagol Ayan, S~Sara Mahdavi, Joshua
  Baldridge, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock {\em Advances in Neural Information Processing Systems},
  35:37117--37130, 2022.

\bibitem{jumper2021highly}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
  {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock {\em Nature}, 596(7873):583--589, 2021.

\bibitem{jumper2021highlyaccurate}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
  {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock {\em Nature}, 596(7873):583--589, 2021.

\bibitem{merchant2023scaling}
Amil Merchant, Simon Batzner, Samuel~S Schoenholz, Muratahan Aykol, Gowoon
  Cheon, and Ekin~Dogus Cubuk.
\newblock Scaling deep learning for materials discovery.
\newblock {\em Nature}, 624(7990):80--85, 2023.

\bibitem{wang2023exploring}
Chedi Morchdi, Yi~Zhou, Jie Ding, and Bei Wang.
\newblock Exploring gradient oscillation in deep neural network training.
\newblock {\em arXiv preprint arXiv:2306.00554}, 2023.

\bibitem{li2019variance}
Xiang Li, Wenhai Huang, Zheng Li, and Lei Zhou.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock {\em arXiv preprint arXiv:1908.03265}, 2019.

\bibitem{cohen2021gradient}
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{chen2018stability}
Yuansi Chen, Chi Jin, and Bin Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock {\em arXiv preprint arXiv:1804.01619}, 2018.

\bibitem{Novack2023Disentangling}
Zachary Novack, Simran Kaur, Tanya Marwah, Saurabh Garg, and Zachary Lipton.
\newblock Disentangling the mechanisms behind implicit regularization in sgd.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{smith2021on}
Samuel~L Smith, Benoit Dherin, David~G.T. Barrett, and Soham De.
\newblock On the origin of implicit regularization in stochastic gradient
  descent.
\newblock In {\em ICLR}, 2021.

\bibitem{dherin2022why}
Benoit Dherin, Michael Munn, Mihaela Rosca, and David~GT Barrett.
\newblock Why neural networks find simple solutions: The many regularizers of
  geometric complexity.
\newblock In {\em NeurIPS}, 2022.

\bibitem{keskar2017large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2017.

\bibitem{ali2020implicit}
Aamir Ali, Edgar Dobriban, and Ryan Tibshirani.
\newblock The implicit regularization of stochastic gradient flow for least
  squares.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pages 233--244. PMLR, 2020.

\bibitem{merad2023convergence}
Ibrahim Merad and Stéphane Gaïffas.
\newblock Convergence and concentration properties of constant step-size sgd
  through markov chains.
\newblock {\em arXiv preprint arXiv:2306.11497}, 2023.

\bibitem{dieuleveut2020bridging}
Aymeric Dieuleveut, Alain Durmus, and Francis Bach.
\newblock Bridging the gap between constant step size stochastic gradient
  descent and markov chains.
\newblock {\em Ann. Statist.}, 48(3):1348--1382, 06 2020.

\bibitem{shirokoff2024convergencemarkovchainsconstant}
David Shirokoff and Philip Zaleski.
\newblock Convergence of markov chains for constant step-size stochastic
  gradient descent with separable functions, 2024.

\bibitem{babichev2018constantstepsizestochastic}
Dmitry Babichev and Francis Bach.
\newblock Constant step size stochastic gradient descent for probabilistic
  modeling, 2018.

\bibitem{cheng2020stochastic}
Xiang Cheng, Dong Yin, Peter Bartlett, and Michael Jordan.
\newblock Stochastic gradient and langevin processes.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of {\em Proceedings of Machine Learning Research},
  pages 1810--1819, 2020.

\bibitem{dieuleveut2016nonparametric}
Aymeric Dieuleveut and Francis Bach.
\newblock {Nonparametric stochastic approximation with large step-sizes}.
\newblock {\em The Annals of Statistics}, 44(4):1363 -- 1399, 2016.

\bibitem{meyn_tweedie_1993}
Sean~P. Meyn and Richard~L. Tweedie.
\newblock {\em Markov Chains and Stochastic Stability}.
\newblock Communications and Control Engineering. Springer London, 1993.

\bibitem{huang2017snapshot}
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John~E. Hopcroft, and Kilian~Q.
  Weinberger.
\newblock Snapshot ensembles: Train 1, get m for free.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{robbins1951a_stochastic}
Herbert Robbins and Sutton Monro.
\newblock {A Stochastic Approximation Method}.
\newblock {\em The Annals of Mathematical Statistics}, 22(3):400 -- 407, 1951.

\bibitem{mertikopoulos2020on}
Panayotis Mertikopoulos, Nadav Hallak, Ali Kavis, and Volkan Cevher.
\newblock On the almost sure convergence of stochastic gradient descent in
  non-convex problems.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{BFGQ19}
Mattia~G. Bergomi, Patrizio Frosini, Daniela Giorgi, and Nicola Quercioli.
\newblock Towards a topological–geometrical theory of group equivariant
  non-expansive operators for data analysis and machine learning.
\newblock {\em Nature Machine Intelligence}, 1:423--433, 2019.

\bibitem{QW19}
H.~Qian and M.N. Wegman.
\newblock {L2}-nonexpansive neural networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{AK22}
B.~Avelin and A.~Karlsson.
\newblock Deep limits and cut-off phenomena for neural networks.
\newblock {\em J. Mach. Learn. Res.}, 23(191):29, 2022.

\bibitem{propp1996exact}
James~Gary Propp and David~Bruce Wilson.
\newblock Exact sampling with coupled markov chains and applications to
  statistical mechanics.
\newblock {\em Random Structures \& Algorithms}, 9(1-2):223--252, 1996.

\bibitem{diaconis1999iterated}
Persi Diaconis and David Freedman.
\newblock Iterated random functions.
\newblock {\em SIAM Review}, 41(1):45--76, 1999.

\bibitem{Wu2024large_stepsize}
Jingfeng Wu, Peter~L. Bartlett, Matus Telgarsky, and Bin Yu.
\newblock Large stepsize gradient descent for logistic loss: Non-monotonicity
  of the loss improves optimization efficiency.
\newblock In {\em Proceedings of Thirty Seventh Conference on Learning Theory},
  volume 247 of {\em Proceedings of Machine Learning Research}, 2024.

\bibitem{cai2024large_stepsize}
Yuhang Cai, Jingfeng Wu, Song Mei, Michael Lindsey, and Peter Bartlett.
\newblock Large stepsize gradient descent for non-homogeneous two-layer
  networks: Margin improvement and fast optimization.
\newblock In {\em The Thirty-eighth Annual Conference on Neural Information
  Processing Systems}, 2024.

\bibitem{lee2022trajectory}
Kiwon Lee, Andrew~N. Cheng, Courtney Paquette, and Elliot Paquette.
\newblock Trajectory of mini-batch momentum: Batch size saturation and
  convergence in high dimensions.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35, pages 22437--22449, 2022.

\bibitem{Agarwala2024High}
Atish Agarwala and Jeffrey Pennington.
\newblock High dimensional analysis reveals conservative sharpening and a
  stochastic edge of stability.
\newblock {\em arXiv preprint arXiv:2404.19261}, Apr 2024.

\bibitem{Wang2020UnderstandingAM}
Sifan Wang, Yujun Teng, and Paris Perdikaris.
\newblock Understanding and mitigating gradient pathologies in physics-informed
  neural networks.
\newblock {\em ArXiv}, abs/2001.04536, 2020.

\bibitem{li2023implicitstochasticgradientdescent}
Ye~Li, Song-Can Chen, and Sheng-Jun Huang.
\newblock Implicit stochastic gradient descent for training physics-informed
  neural networks, 2023.

\bibitem{soviany2022curriculumlearningsurvey}
Petru Soviany, Radu~Tudor Ionescu, Paolo Rota, and Nicu Sebe.
\newblock Curriculum learning: A survey, 2022.

\bibitem{mange19dataorder}
Jeremy Mange.
\newblock Effect of training data order for machine learning.
\newblock In {\em 2019 International Conference on Computational Science and
  Computational Intelligence (CSCI)}, pages 406--407, 2019.

\bibitem{lange2023continual}
Matthias~De Lange, Gido~M van~de Ven, and Tinne Tuytelaars.
\newblock Continual evaluation for lifelong learning: Identifying the stability
  gap.
\newblock In {\em ICLR}, 2023.

\bibitem{PageBrin98}
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.
\newblock The pagerank citation ranking: Bringing order to the web.
\newblock Technical Report 1999-66, Stanford InfoLab, January 1998.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report, University of Toronto, 2009.

\bibitem{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{smith2017don}
Samuel~L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc~V Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock {\em arXiv preprint arXiv:1711.00489}, 2017.

\bibitem{vandervaart}
Aad~W Van~der Vaart.
\newblock {\em Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\end{thebibliography}






\newpage
\appendix
\onecolumn


\section{Additional Experiments}\label{appendix:experiment_details}

{\bf Note on plotting multiple seeds:} We are interested in the variability per realization of the backward and forward trajectories. The backward trajectories are more stable individually along their own paths, but these paths can be very different from seed to seed (because of the convergence toward different points). Therefore the phenomenon is much clearer when the seeds are plotted individually rather than averaged, which creates artificially more variability in the backward trajectories as there really is on each individual realization. This is why we are reporting the various seed plots individually and not as a single averaged plot with error bars. Note that the increased stability and point convergence is visible in each of the seeds.

\newpage

\subsection{Regression on synthetic datasets}

In this experiment, to verify the increased stability of backward SGD over the standard forward version, we trained with both algorithms a neural network with 3 layers of 300 neurons with no regularization each to regress 100 points uniformly sampled from function graphs  
$D = \{(x_i, y_i):\:  y_i = f(x_i) \;\textrm{with}\; x_i = -1 + \frac {2i}{100},\, i=0,\dots, 100\}$
using the functions $f(x) = x^2$ (Figure \ref{figure:exp1}), $f(x) = \cos(10x)$ (Figure \ref{figure:exp2}), and $f(x) = x^3$ (Figure \ref{figure:exp3}).  In all cases,  we can observe a much higher stability for backward SGD over the forward version. 

\begin{figure}[h] 
\centering
\includegraphics[width=0.4\textwidth]{plots/exp1_5_seeds.png}
\caption{Decreased variance and increased stability in train (left) and test (right) losses for backward SGD compared to forward SGD for all 5 seeds. The data was sampled from $f(x) = x^2$ and the training performed with batch size 1 and learning rate 0.05 for 1400 steps.
}
\label{figure:exp1}
\end{figure}

\newpage

\begin{figure}[h] 
\centering
\includegraphics[width=0.4\textwidth]{plots/exp2_5_seeds.png}
\caption{Decreased variance and increased stability in train (left) and test (rigth) losses for backward SGD compared to forward SGD for all 5 seeds. 
The data was sampled from $f(x) = \cos(10x)$ and the training performed with batch size 1 and learning rate 0.02 for 1400 steps.
}
\label{figure:exp2}
\end{figure}

\newpage

\begin{figure}[h] 
\centering
\includegraphics[width=0.4\textwidth]{plots/exp3_5_seeds.png}
\caption{Decreased variance and increased stability in train (left) and test (right) losses for backward SGD compared to forward SGD for all 5 seeds. The data was sampled from $f(x) = x^3$ and the training performed with batch size 1 and learning rate 0.02 for 1400 steps.
}
\label{figure:exp3}
\end{figure}

\newpage


\subsection{MLP trained on  Fashion-MNIST}

In this experiment, to verify the increased stability of backward SGD over the standard forward version, we trained a MLP with 5 layers of 500 neurons each with no regularization using both forward and backward stochastic gradient descent with no regularization on the Fashion-MNIST dataset 
\cite{xiao2017fashion}. We repeated the experiment for 5 different random seeds. For all seeds, we used a learning rate of 0.001 and a batch size of 8. In Figure \ref{figure:training_losses_fashion_mnist}, we recorded the training loss at each gradient update for both the forward and backward iteration and plotted the 5 seeds separately (one seed for each row). For each seed, we observe that the training loss for the backward iterations is more stable and converges faster with less variability when compared to the forward iterations. We also observe stability improvements for all other  learning curves for the backward trajectories.

\begin{figure}[h] 
\centering
\includegraphics[width=0.7\textwidth]{plots/fashion_mnist_learning_curves_5_seeds.png}
\caption{MLP trained on Fashion-MNIST. Training with backward SGD significantly reduces the variance and increases the stability of both the train and test loss as well as the train and test and accuracy compared to forward SGD. This behavior is consistent across all seeds.}
\label{figure:training_losses_fashion_mnist}
\end{figure}

\newpage

\subsection{VGG-19 trained on CIFAR-10}

In this experiment, to verify the increased stability of backward SGD over the standard forward version, we trained a VGG-19 model \cite{simonyan2014very} using both forward and backward stochastic gradient descent with no regularization on the CIFAR-10 dataset \cite{krizhevsky2009learning}. We repeated the experiment for 5 different random seeds. In all seeds, we used a learning rate of 0.001 and a batch-size of 8. In Figure \ref{figure:training_losses_cifar10_vgg19}, we plot the training and test loss at each gradient update for both the forward and backward iteration. We plotted the 5 seeds separately, represented by each row. Note that for each seed, we observe that the training loss for the backward iterations is again more stable, converges faster, and has less variability than the forward iterations. 

\begin{figure}[h] 
\centering
\includegraphics[width=0.7\textwidth]{plots/vgg19_cifar10_bs=8_lr=0pt001__all_seeds.pdf}
\caption{VGG-19 trained on CIFAR-10. Training with backward SGD significantly reduces the variance and increases the stability of both the train and test loss as well as the train and test and accuracy compared to forward SGD. This behavior is consistent across all seeds.}
\label{figure:training_losses_cifar10_vgg19}
\end{figure}

\newpage

\subsection{ResNet-50 trained on CIFAR-100}

In this experiment, to verify the increased stability of backward SGD over the standard forward version, we trained a ResNet-50 model \cite{he2016deep} using both forward and backward stochastic gradient descent with no regularization on the CIFAR-100 dataset \cite{krizhevsky2009learning}. We repeated the experiment for 5 different random seeds. In all seeds, we used a learning rate of 0.001 and a batch-size of 16. In Figure \ref{figure:training_losses_cifar100_resnet50}, we plot the training and test loss at each gradient update for both the forward and backward iteration. We plotted the 5 seeds separately, represented by each row. Note that for each seed, we observe that the training loss for the backward iterations is again more stable, converges faster, and has less variability than the forward iterations. 

\begin{figure}[h] 
\centering
\includegraphics[width=0.7\textwidth]{plots/resnet50_cifar100_bs=16_lr=0pt001__all_seeds.pdf}
\caption{ResNet-50 trained on CIFAR-100. Training with backward SGD significantly reduces the variance and increases the stability of both the train and test loss as well as the train and test and accuracy compared to forward SGD. This behavior is consistent across all seeds.}
\label{figure:training_losses_cifar100_resnet50}
\end{figure}

\newpage


\subsection{Backward stabilization after forward iterations}
\label{additional_experiment:backward_after_forward}

In this experiment, to verify the increased stability and convergence after turning on backward iterations after a number of forward iterations, we trained a MLP with 5 layers of 500 neurons each with stochastic gradient descent with no regularization to learn the 10 classes of Fashion-MNIST dataset 
\cite{xiao2017fashion}. We repeated the experiments for 5 seeds. We used a learning rate of 0.001 and a batch-size of 8. In Figure \ref{figure:forward_backward}, we recorded the learning curves at each gradient update for both the forward iteration and the backward iteration switched on after step 1000. Each seed is plotted independently. 
We observe that the training loss for the backward iterations has again more stable convergence once backward iterations are switched on after step 1000 than the forward iterations. 

\begin{figure}[h] 
\centering
\includegraphics[width=0.7\textwidth]{plots/forward_backward_all_seeds.png}
\caption{
Decreased variance and increased stability for all learning curves and all seeds once backward SGD is switched on at step 1000 after forward SGD iterations for an MLP trained on Fashion MNIST.}
\label{figure:forward_backward}
\end{figure}

\newpage

\subsection{Continued stability of backward SGD throughout training.}

In this experiment, to verify that the behavior of increased stability and convergence for backward iterates continues throughout training, we trained a ResNet-18 model on the CIFAR-10 dataset using the same hyperparameters as in Figure \ref{figure:training_losses_cifar10_resnet18} (i.e., no regularization, a learning rate of 0.025 and batch size of 8) but training for 25000 steps instead of 2000. In Figure \ref{figure:training_losses_cifar10_resnet18_trainingtoconvergence}, we record the learning curves for both forward and backward SGD, performing model evaluation every 100 steps. Throughout training we continue to notice that the training loss (as well as all the other learning curves) for the backward iterations is again more stable, converges faster, and has less variability than the forward iterations. Note, here we only performed this experiment for one seed because because of the high computational requirements for backward SGD with so many training steps. 

\begin{figure}[h] 
\centering
\includegraphics[width=0.7\textwidth]{plots/resnet18_cifar10_bs=8_lr=0pt025_nsteps=24400_seed=0.pdf}
\caption{
ResNet-18 trained on CIFAR-10 for 25000 steps. Decreased variance and increased stability throughout model training.}
\label{figure:training_losses_cifar10_resnet18_trainingtoconvergence}
\end{figure}

\newpage

\section{Additional seeds to plots in the main paper}\label{appendix:multiple_seeds}

{\bf Note on plotting multiple seeds:} We are interested in the variability per realization of the backward and forward trajectories. The backward trajectories are more stable individually along their own paths, but these paths can be very different from seed to seed (because of the convergence toward different points). Therefore the phenomenon is much clearer when the seeds are plotted individually rather than averaged, which creates artificially more variability in the backward trajectories as there really is on each individual realization. This is why we are reporting the various seed plots individually and not as a single averaged plot with error bars. Note that the increased stability and point convergence is visible in each of the seeds.


\subsection{All 5-seeds for ResNet-18 trained on CIFAR-10; cf. Figure \ref{figure:training_losses_cifar10_resnet18}}\label{appendix:figure_1}

In Figure \ref{figure:training_losses_cifar10} below, we plot the learning curves for all five seeds as in Figure \ref{figure:training_losses_cifar10_resnet18} of the main paper.

\begin{figure}[h] 
\centering
\includegraphics[width=0.7\textwidth]{plots/resnet18_cifar10_bs=8_lr=0pt025__all_seeds.pdf}
\caption{
All 5-seeds plot of Figure \ref{figure:training_losses_cifar10_resnet18}.}
\label{figure:training_losses_cifar10}
\end{figure}


\newpage


\subsection{All 5-seeds for MLP trained on FashionMNIST; cf. Figure \ref{figure:intermittent_backward_fashion_mnist}}\label{appendix:figure_2}


In Figure \ref{figure:intermittent_fashion_mnist_5_seeds} below, we plot  the learning curves for all five seeds as in Figure \ref{figure:intermittent_backward_fashion_mnist} of the main paper.

\begin{figure}[h] 
\centering
\includegraphics[width=0.7\textwidth]{plots/intermittent_fashion_mnist_5_seeds.png}
\caption{All 5-seeds plot of Figure \ref{figure:intermittent_backward_fashion_mnist}.}
\label{figure:intermittent_fashion_mnist_5_seeds}
\end{figure}


\newpage 


\section{Toward an approximate backward SGD}\label{appendix:approximate_backward}

Backward SGD has increased stability and convergence over forward SGD but naive implementations are computationally intensive. In this section, we propose an approximation of the backward iterates by modifying the forward iterates with a Lie bracket term of order $\mathcal O(h^2)$ in the learning rate (\cref{thm:approximate_backward}). This comes at an extra cost, but which can be made smaller than evaluating the backward iterates from the initial point at every step (\cref{corollary:approximation}). 
Our approximation is valid 
\begin{itemize}
\item for any optimizer of the form $T_i(\theta) = \theta + h V_i(\theta)$, where $V_i(\theta)$ is a random vector field on the parameter space depending on the randomly sampled batch of data $B_i$. (Of course, in the case of SGD we have $V_i(\theta) = -\nabla L_i(\theta)$ where $L_i = L_{B_i}$ is the loss function evaluated on batch $B_i$.)
\item up to an error of order $\mathcal O(h^3)$  in the learning rate.
\end{itemize}

Unfortunately, it seems that in real-life cases higher orders beyond $\mathcal O(h^2)$ play a significant role in the backward dynamics, and therefore can not be neglected for reasonable ranges of the learning rate. Nevertheless, we include our second order approximation here, since we believe it gives a reasonable path on how to produce approximations of the backward trajectories using the forward trajectories. Namely, while this is out-of-scope for this paper and topic of further research, we believe that adding higher order corrections may produce useful approximations that could be used as stabilizers for the forward trajectories. 
The main idea is to expand a generic $k$ term iterate 
\begin{equation}
    T_{i_1}  \cdots T_{i_k}(\theta) = (1 + hV_{i_1})\cdots (1 + hV_{i_k})(\theta)
\end{equation}
in Taylor's series in the learning rate. This is what the next lemma gives. (We demonstrate the usefulness of such expansions  \cref{appendix:implicit regularization} where we use \cref{lemma:commutation} to produce a beneficial second order regularizer that emulates an iteration order average.) 

Observe also that \cref{lemma:commutation} tells us that all possible iteration orders coincide at order $h$ but start differing at order $\mathcal O(h^2)$:

\begin{lemma} \label{lemma:commutation}
Consider a sequence $\{T_i\}_{i > 0}$ of operators of the form $T_i(\theta) = \theta + h V_i(\theta)$, where $V_i(\theta)$ is a vector field on the parameter space. Then for any choice of indices $i_1, \dots, i_k$ we have that
\begin{equation*}
    T_{i_1}  \cdots T_{i_k}=  
    1
    + h \sum_{l=1}^k V_{i_l}
    + h^2 \sum_{1\leq u < v \leq k} V_{i_u}' V_{i_v}
    + \mathcal O(h^3)
\end{equation*}

\end{lemma}

\begin{proof}
    We proceed by induction. For the base case, $k=1$, this is trivial. Suppose now that this is true for any composition of $k-1$ operators. By definition of $T_{i_1}$ we have that
    \begin{eqnarray}
        T_{i_1}\cdots T_{i_k}(\theta) 
        & = & T_{i_1}(T_{i_2}\cdots T_{i_k}(\theta)) \\
        & = & X + h V_{i_1}(X) \label{eq:X}
    \end{eqnarray}
    with $X = T_{i_2}\cdots T_{i_k}(\theta)$. Now by induction hypothesis we have that
    \begin{eqnarray*}
        X = \theta + h\sum_{l=2}^{k} V_{i_l}(\theta) + h^2 \sum_{2\leq u < v \leq k} V_{i_u}'(\theta) V_{i_v}(\theta) + \mathcal O(h^3)
    \end{eqnarray*}
    Therefore, taking a Taylor series for the second term of \eqref{eq:X}, we obtain
    \begin{eqnarray*}
    hV_{i_1}(X) = hV_{i_1}(\theta) + h^2 \sum_{l=2}^k V_{i_1}'(\theta) V_{i_l}(\theta) +\mathcal O(h^3).
    \end{eqnarray*}
    Summing up in \eqref{eq:X} the expressions we have found for $X$ and $hV_{i_1}(X)$ above, we obtain that the composition $T_{i_1}\cdots T_{i_k}(\theta)$ has the form
    \begin{equation*}
    \theta + h\sum_{l=1}^{k} V_{i_l}(\theta) + h^2 \sum_{1\leq u < v \leq k} V_{i_u}'(\theta) V_{i_v}(\theta) + \mathcal O(h^3),
    \end{equation*}
    which completes the proof.
\end{proof}


The next theorem gives us a way to approximate the backward iterates up to order $\mathcal O(h^3)$ by correcting the forward iterates with a term keeping track of their difference:

\begin{theorem}\label{thm:approximate_backward}
Consider a sequence $\{T_i\}_{i > 0}$ of operators of the form $T_i(\theta) = \theta + h V_i(\theta)$, where $V_i(\theta)$ is a vector field on the parameter space. The backward and forward iterates of the sequence are related by the following identity:
\begin{equation*}
    T_1 \cdots T_n(\theta) = T_n \cdots T_1(\theta) + h^2 \sum_{1\leq i<j\leq n} [V_i, V_j](\theta) + \mathcal O(h^3)
\end{equation*}
where the $[V_i, V_j](\theta) = V_i'(\theta)V_j(\theta) - V_j'(\theta)V_i(\theta)$ is the Lie bracket between the vector fields $V_i$ and $V_j$.
\end{theorem}

\begin{proof}
By Lemma \ref{lemma:commutation}, we have that the backward iterate is 
\begin{equation*}
    T_{1}  \cdots T_{n}=  
    \operatorname{id}
    + h \sum_{l=1}^n V_{l}
    + h^2 \sum_{1 \leq u < v\leq n} V_{u}' V_{v}
    + \mathcal O(h^3)
\end{equation*}

while the forward iterate is obtained by reversing the indices:
\begin{equation*}
    T_{n}  \cdots T_{1}=  
    1
    + h \sum_{l=1}^n V_{l}
    + h^2 \sum_{1 \leq u < v\leq n} V_{v}' V_{u}
    + \mathcal O(h^3).
\end{equation*}
We now see that the difference
$$D(\theta) =  T_{1}  \cdots T_{n}(\theta) -  T_{n}  \cdots T_{1}(\theta)$$
between the backward and forward iterates is of the form
\begin{eqnarray*}
     D(\theta)
     & = & h^2 \sum_{1 \leq u < v\leq n} V_{u}'(\theta) V_{v}(\theta) - V_{v}'(\theta) V_{u}(\theta) + \mathcal O(h^3)\\
     & = & h^2 \sum_{1 \leq u < v\leq n} [V_u, V_v](\theta)  + \mathcal O(h^3),
\end{eqnarray*}
which completes the proof.
\end{proof}

We now consider the case of SGD where the vector fields $V_i$ are given by the gradients of the loss evaluated at the current batch. We give below a definition of the approximate backward iterate in that particular case, although a similar definition can be given for any vector fields. 

\begin{definition}\label{definition:backward_iterate}
Consider the SGD operators $T_i(\theta) = \theta - h \nabla L_i(\theta)$ obtained by taking the gradient of a loss function on a batch $B_i$ of data at step $i$. We denote by $\theta_n = T_n\cdots T_1(\theta_0) = \theta_{n-1} - h \nabla L_n(\theta_{n-1})$ the forward SGD iterate starting at initial point $\theta_0$ and by $\theta_n^B = T_1\cdots T_n(\theta_0)$ the corresponding backward iterate starting at the same initial point. Motivated by Theorem \ref{thm:approximate_backward}, we introduce the {\emph{approximate backward iterate}} $\tilde \theta_n$ as follows:
\begin{equation}
    \tilde \theta_n = \theta_n + h^2 \sum_{1\leq i<j\leq n} [\nabla L_i, \nabla L_j] (\theta_0)
\end{equation}
\end{definition}

Theorem \ref{thm:approximate_backward} tells us that the true backward iterate and its approximation are within $\mathcal O(h^3)$ of each other:
\begin{equation}
    \|\tilde \theta_n - \theta_n^B\| = \mathcal O (h^3),
\end{equation}
which allows us to use the approximation for learning rates small enough so that the terms in $\mathcal O(h^3)$ can be neglected. The following corollary tells us how the approximate backward iterates can be computed in an iterative fashion by keeping in memory an additional variable $C_n$ of the same size as the network parameters:

\begin{corollary} \label{corollary:approximation}
In the notation above, we can obtain the approximate backward SGD iterate $\tilde \theta_n$ from the forward SGD iterate $\theta_n$ starting at $\theta_0$ in an iterative fashion as follows:
\begin{eqnarray*}
    \theta_n & = & \theta_{n-1} - h \nabla L_{n}(\theta_{n-1}) \\
    g_n & = & g_{n-1} + \nabla L_{n-1}(\theta_0) \\
    H_n & = & H_{n-1} + \nabla^2 L_{n-1}(\theta_0) \\
    C_n      & = & C_{n-1} + H_n \nabla L_{n}(\theta_0) - \nabla^2 L_{n}(\theta_0) g_n \\
    \tilde \theta_n & = & \theta_n + h^2 C_n
\end{eqnarray*}
with $g_0 = g_1 = 0$,  $H_0 = H_1 = 0$, and $c_0 = c_1 =0$.
\end{corollary}

\begin{proof}
By Definition \ref{definition:backward_iterate} of the approximate backward iterates we have that $\tilde \theta_n = \theta_n + h^2 C_n$ with 
$$
C_n = \sum_{1\leq i<j\leq n} [\nabla L_i, \nabla L_j] (\theta_0).
$$

First observe that we can split $C_n$ into two parts
\begin{eqnarray*}
C_n 
& = & \sum_{1\leq i<j\leq n-1} [\nabla L_i, \nabla L_j] (\theta_0) + \sum_{1\leq i \leq n-1} [\nabla L_i, \nabla L_n] (\theta_0)\\
& = & C_{n-1} + [\sum_{1\leq i \leq n-1} \nabla L_i, \nabla L_n](\theta_0) \\
& = & C_{n-1} + \left(\sum_{1\leq i \leq n-1} \nabla^2 L_i(\theta_0) \right)\nabla L_n(\theta_0) \\
&   & \quad - \nabla^2 L_n(\theta_0) \left(\sum_{1\leq i \leq n-1} \nabla L_i(\theta_0)\right) \\
& = & C_{n-1} + H_n \nabla L_n(\theta_0) -  \nabla^2 L_n(\theta_0) g_n,
\end{eqnarray*}
where $H_n$ and $g_n$ are expressed recursively as in the theorem statement.
\end{proof}
 
\section{Approximate implicit regularization of smaller batches} \label{appendix:implicit regularization}

As shown in \cite{keskar2017large,smith2017don,smith2021on,dherin2022why} for instance,  small batches have an implicit regularization effect, producing solutions with higher test accuracy as the batch size decreases. In this section, we show that approximations of the type given by \cref{lemma:commutation} are useful to understand this implicit regularization effect. We also see how we can go beyond this small batch effect and produce explicit regularizers with even higher test performance based on the idea of iteration order average on the small batches, leveraging \cref{lemma:commutation}.

Throughout the section we will write $V_i(\theta)$ for the negative batch loss gradient $-\nabla L_i(\theta)$ computed on batch $B_i$ in order to keep the notation simple.


\subsection{The effect of small batches}

The idea is to use \cref{lemma:commutation} to understand the implicit regularization effect of small batches, very much in line with the findings in \cite{smith2021on}.
First, consider two settings. In the first setting, we perform a single gradient update 
$$
T_{\textrm{large}}(\theta) = \theta + h V_B(\theta).
$$
with one large batch $B$ and learning rate $h$. In the second setting, we split the large batch $B$ into $c$ small batches of equal size: $B_{1}, B_{2}, \ldots, B_{c}$ (i.e, each of size $|B|/c$). Then we apply SGD sequentially $c$ times for each of the smaller batch and with learning rate $h' = h / c$:
$$
T_{\textrm{small}}(\theta) = 
(1 + h'V_{c})\cdots (1 + h'V_{2})(1 + h'V_{1})(\theta)
$$
Then  Lemma~\ref{lemma:commutation} gives us immediately a second order approximation for the second setting:
$$
T_{\textrm{small}}(\theta) =  \theta + h'\sum_{i=1}^c V_{i}(\theta) + {h'}^2\sum_{i < j} V'_{i}(\theta)V_{j}(\theta) + \mathcal O(h^3)
$$
Now it is easy to see that
$$
h V_B(\theta) = h'\sum_{i=1}^c V_i(\theta).
$$
Therefore, we can extract the added implicit regularization that is induced from smaller batches. Namely, we obtain that the composition of the $c$ smaller batches is the same as a single gradient step with the larger batch using learning rate $h = c h'$ plus an additional second-order regularization term:
$$
T_{\textrm{small}}(\theta) = T_{\textrm{large}}(\theta) + \underbrace{{h'}^2\sum_{i < j} V'_{i}(\theta)V_{j}(\theta)}_{\mathrm{Smaller\ Batch\ Regularization}} +\ \mathcal O({h'}^3)
$$

So we can attribute the second order regularization effect of small batch training to this additional term, in line with the computations in \cite{smith2021on}.

\subsection{Explicit regularization through iteration order averages}

In the previous section, we applied the small batches in some chosen order. Another order may have worked as well, although producing a different second order regularization term.
This begs the question of whether we can instead take the average over all possible iteration orders so as to produce an iterate for which no particular order is preferred:
\begin{equation}
    T_{\textrm{perm}}(\theta) = \frac 1{c!}\Big(
    \sum_{\sigma \in \mathrm{Perm}_c}
    (1 + h' V_{\sigma(1)})\cdots (1 +h' V_{\sigma(n)})(\theta)\Big),
\end{equation}
where $\mathrm{Perm}_c$ is the permutations of $c$ elements.
Again \cref{lemma:commutation} gives us the second order approximation for this new update rule:

$$
T_{\textrm{perm}}(\theta) = T_{\textrm{large}}(\theta) + \frac{1}{2}{h'}^2\sum_{i \neq j} V'_{i}(\theta)V_{j}(\theta) + \mathcal O({h'}^3).
$$
In the next section, we show that the iteration-order-averaging term, that we can extract from the computation above, namely,
$$
\lambda \sum_{i \neq j} V'_{i}(\theta)V_{j}(\theta),
$$
produces a more powerful regularizer than the one obtained from a single ordering of the small batches. In a way, this new regularizer emulates a mixture of models, each of which is produced by a different ordering of the iterations in a window of $c$ batches (rather than from a different random seed). The next section shows the benefits of this new ``ordering-free" regularizers experimentally.

\newpage

\subsubsection{Experiments}

We trained a MLP with 5 layers of 500 neurons each with stochastic gradient descent with no regularization to learn the 10 classes of Fashion-MNIST dataset 
\cite{xiao2017fashion}. We used a learning rate of $h=0.001$ (and $h' = h / c$), and a batch-size of $|B| = 2048$ \footnote{Or the closest integer divisible by $c$; e.g., $2049$ for $c=3$.}. We examine three different shrinking factors $c\in\{2, 3, 4\}$. In each setting, we test the values of $\lambda\in\{0.5h^2, h^2, 2h^2, 4h^2\}$. The following figures present the train and test loss over training compared to training with large-batch $|B|=2048$ and learning rate $h' = h /c $, and training with small-batch $2048/c$ with learning rate $h'$. The experiment shows that our method can outperform vanilla training both for large and small batches with minimal tuning of $\lambda$ for this specific experiment. 

%%%
\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{plots/implicit_small_reg/red_shades_c_2_train.png}
\caption{Train curves for injecting regularization resulting from splitting the batch in two (i.e., $c=2$).}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{plots/implicit_small_reg/red_shades_c_2_test.png}
\caption{Test curves for injecting regularization resulting from splitting the batch in two (i.e., $c=2$). As you can see, the performance improves as we increase the $\lambda$.}
\end{minipage}
\end{figure}
%%%

%%%
\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{plots/implicit_small_reg/red_shades_c_3_train.png}
\caption{Train curves for injecting regularization resulting from splitting the batch in three (i.e., $c=3$).}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{plots/implicit_small_reg/red_shades_c_3_test.png}
\caption{Test curves for injecting regularization resulting from splitting the batch in three (i.e., $c=3$). Similar to the case where $c=2$ above, the performance improves as we increase the $\lambda$.}
\end{minipage}
\end{figure}
%%%


\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{plots/implicit_small_reg/red_shades_c_4_train.png}
\caption{Train curves for injecting regularization resulting from splitting the batch in four (i.e., $c=4$).}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{plots/implicit_small_reg/red_shades_c_4_test.png}
\caption{Test curves for injecting regularization resulting from splitting the batch in four (i.e., $c=4$). Similar to the cases above, the trend continues and is even more visible; Performance improves as we increase the $\lambda$.}
\end{minipage}
\end{figure}


\newpage



\section{Proof of Theorem \ref{thm:backward_forward}}
\label{appendix:proof_of_theorem_2.6}

\subsection{Definitions}

Before we dive into the proof, let us define the following three different notions of convergence of random variables. First we have the strongest notion, which is convergence almost surely. That is, a sequence of random variables $X_n$ converges to a limiting random variable $X_\infty$ almost surely if
\begin{equation*}
    \P(\lim_{n \to \infty} X_n = X_\infty) = 1.
\end{equation*}
This can be thought of as a sort of point-wise convergence.
A weaker notion of convergence which is implied by almost sure convergence (see Theorem 2.7 (i) in \cite{vandervaart}) is convergence in probability.
The sequence $X_n$ of random variables converges to the random variable $X_\infty$ in probability if for every $\varepsilon > 0$ the following holds
\begin{equation*}
    \lim_{n \to \infty} \P(|X_n - X_\infty| > \epsilon) = 0.
\end{equation*}
That is, for large $n$ it becomes more and more likely that $X_n$ is close to $X_\infty$.
The weakest notion of convergence is convergence in distribution, which is implied by convergence in probability (see Theorem 2.7 (ii) in \cite{vandervaart}).
There are many equivalent ways to define this, but the most concrete way is the following: Let $F_n(x) = \P(X_n \leq x)$ be the cumulative distribution function for $X_n$ and let $F_\infty(x)$ be the corresponding cumulative distribution function for $X_\infty$,
then we say that the sequence $X_n$ converges in distribution to $X_\infty$ if for every continuity point $x$ of the cumulative distribution function $F_\infty$ we have the following
\begin{equation*}
    \lim_{n \to \infty} F_n(x) = F_\infty(x).
\end{equation*}
If we define $\mu_n := dF_n$ and $\mu_\infty := dF_\infty$, then when we say that $\mu_n \to \mu_\infty$ we mean convergence in distribution in the above sense.

\subsection{Proof}

Consider a sequence $T_i:\Omega \rightarrow \Omega$, $i=1,2, \dots$ of independent and identically distributed random operators. The probability distribution of the forward iterates $\theta_n = T_n T_{n-1}\cdots T_1 (\theta_0)$ is given by 
$$
\mu_n = (T_n T_{n-1}\cdots T_1)_* \delta_{\theta_0},
$$
where $\delta_{\theta_0}$ is the delta distribution concentrated at $\theta_0$ (i.e. $\delta_{\theta_0}(A) = 1$ if $\theta_0\in A$ and 0 otherwise).


We want to show that when the backward iterates converge to a random point (randomness is due to the sampling of the random operators $T_i$'s), i.e., when
$$
 T_1T_2\cdots T_n(\theta_0) \longrightarrow \theta^* \quad\textrm{as}\quad n\rightarrow \infty,
$$
then this implies that
\begin{itemize}
    \item the probability distribution of the forward iterates from $\theta_0$ converge to a stationary probability measure $\mu_n \rightarrow \mu_{\theta^*}$ 
    \item the random point $\theta^*$ is distributed according to the same forward iterate stationary distribution $\mu_{\theta^*}$. 
\end{itemize} 


To see that let $Y_n^{\theta_0}$ denote the forward iterates
\begin{equation*}
    Y_n^{\theta_0} = T_nT_{n-1}\ldots T_1(\theta_0),
\end{equation*}
and let $X_n^{\theta_0}$ denote the backward iterates
\begin{equation*}
    X_n^{\theta_0} = T_1T_{2}\ldots T_n(\theta_0).
\end{equation*}
Define the cumulative distribution function (CDF) for the random vector $Y_n^{\theta_0}$ and $\theta = (\theta^1, \dots, \theta^d) \in \mathbb{R}^d$ as
\begin{equation*}
    F_n^{\theta_0}(\theta) := \P((Y_n^{\theta_0})_1 \leq \theta^1,\ldots,(Y_n^{\theta_0})_d \leq \theta^d).
\end{equation*}
By independence of the random operators $T_i$'s we know that
\begin{equation*}
    F_n^{\theta_0}(\theta) := \P((Y_n^{\theta_0})_1 \leq \theta^1) \P((Y_n^{\theta_0})_2 \leq \theta^2) \cdots \P((Y_n^{\theta_0})_d \leq \theta^d),
\end{equation*}
and as such it is clear that the order of the random variables has no impact on the distribution. This means that $F_n^{\theta_0}(\theta)$ is also the CDF of $X_n^{\theta_0}$.

Under our assumption that the backward iterates converge to a random point $\theta^*$ (almost sure convergence) we will define the CDF of that random point as, $F_\infty^{\theta_0}(\theta)$ and for the sake of the proof we denote that point with $X_\infty^{\theta_0}$ instead of $\theta^*$ to highlight that it is a random vector.

Now, our assumption that the backward iterates converge to a point implies that the random vector $X_n^{\theta_0}$ converges to $X_\infty^{\theta_0}$ almost surely, in the sense defined above. As alluded to in the previous section above, the almost sure convergence implies convergence in distribution, i.e., the convergence of the CDF $\lim_{n \to \infty} F_n^{\theta_0}(\theta) = F_\infty^{\theta_0}(\theta)$, for every $\theta$ where $F_\infty^{\theta_0}$ is continuous.

Since $X_n^{\theta_0}$ and $Y_n^{\theta_0}$ have the same distribution (same CDF) we have immediately that $Y_n^{\theta_0}$ also converges in distribution to $X_\infty^{\theta_0}$.
The limiting stationary distribution $\mu_{\theta^*}$ is simply the probability measure corresponding to $F_{\infty}^{\theta^*}$, i.e. $\mu_{\theta^*} := dF_{\infty}^{\theta_0}$.


\section{Proof of Lemma \ref{lem:contraction}}
\label{appendix:proof_of_contraction}

Consider a smooth function $L$ that is \emph{strictly convex}. This means that
\begin{itemize}
\item there is $m>0$ such that 
$$
    \langle \nabla L(\theta_1) - \nabla L(\theta_2), \theta_1-\theta_2 \rangle \geq m \left \| \theta_1-\theta_2 \right \|^2,
$$
where $\langle a, \,b\rangle = a^T b$ is the inner product between two vectors 
\item there is a constant $M > m$ such that
\begin{equation*}
    \left \| \nabla L(\theta_1) - \nabla L(\theta_2) \right \| \leq M \left \| \theta_1-\theta_2 \right \|.
\end{equation*}
\end{itemize}

We want to show the following inequality
\begin{equation*}
    \left \| T(\theta_1)-T(\theta_2) \right \| \leq \sqrt{1 - 2 h m + h^2 M^2} \left \| \theta_1-\theta_2 \right \|.
\end{equation*}
for the gradient operator $T(\theta)=\theta-h\nabla L(\theta)$.

We can see that by first expanding the square of the operator difference:
\begin{align*}
    \| T(\theta_1) - T(\theta_2) \|^2 & = \|\theta_1-\theta_2\|^2 \\
    & - 2h \langle \nabla L(\theta_1) - \nabla L(\theta_2), \theta_1-\theta_2 \rangle \\
    & + h^2 \| \nabla L(\theta_1) - \nabla L(\theta_2) \|^2.
\end{align*}
Now applying the strict convexity conditions, we get
\begin{align*}
    \| T(\theta_1) - T(\theta_2) \|^2 & \leq \|\theta_1-\theta_2\|^2 - 2h m \|\theta_1-\theta_2\|^2 \\
    & + h^2 M^2 \|\theta_1-\theta_2\|^2 \\
    & = (1 - 2h m + h^2 M^2) \|\theta_1-\theta_2\|^2,
\end{align*}
which ends the proof by taking the square root on both sides.


\end{document}

