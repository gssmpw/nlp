\subsection{Robustness}\label{subsec:robustness}



 
\spfinal{
Adversarial ML techniques craft samples of a particular class to evade detection. 
Most research directly manipulates the feature space or edge weights to craft samples of a particular class. However, these techniques are not practical especially in the cybersecurity domain when they cannot be converted to realistic input samples. In this section, we evaluate our approach against recent SoTA practical DNS attacks~\cite{practicalattacks:SP:2024, minta:SP:2024}. 
Galloway et al.~\cite{practicalattacks:SP:2024} introduces attacks targeting graph structure, and popularity- and registration-based features. While registration-based attacks are shown to have minimal impact on a domain's reputation, attacks on graph structure and popularity-based features were found to be effective. We believe using popularity-based features and ground truth together potentially exaggerate the impact of Galloway et al.'s most successful attack. Since we intentionally omit popularity rankings from our feature sets, our approach remains resilient against most successful popularity list manipulation attacks and thus we only evaluate our approach against MimicIP, the most effective graph-based attack proposed in Galloway et. al.~\cite{practicalattacks:SP:2024}.\eat{This is the only practical attack among the proposed attacks that does not have a financial cost for the attacker.}} In this attack, the attacker inserts A records resolving to IPs associated with benign domains listed in the Tranco Top 100K list. As we consider all historical resolutions, even if the attacker temporarily halts malicious activity, this results in additional edges between malicious domains and IPs linked to the popular domains. During training and testing, we greedily select $n$ IPs ($n=1,2,3$) that induce the highest increase in prediction score. 
\spfinal{
While MimicIP focuses on single-domain attacks that attempt to mimic benign domains, MintA~\cite{minta:SP:2024} addresses a scenario where the attacker controls multiple domains and has access to a surrogate model, allowing them to compute the loss for given graphs and strategically manipulate IP resolutions of controlled malicious domains.
We provide a comprehensive evaluation of our models on both standard and adversarial samples for both attacks. Additionally, we train a model using a combination of standard and adversarial samples and analyze its performance. Figure~\ref{fig:adversarial} shows the performance of our approach under both clean and adversarial conditions across different perturbation rates, where the perturbation rate represents the percentage of domains subjected to the specified attack. As expected, the performance of the model trained on standard samples decreases under the adversarial setting with an increasing perturbation rate. However, due to the way we construct the graph, the rate of degradation of performance is low and our approach still achieves over 90\% of accuracy even with a high perturbation rate of 15\%. Further, training our model with adversarial samples nearly results in the same original performance under the adversarial setting, further demonstrating the robustness of our approach.} To ensure reproducibility, we have included our training code for both scenarios in the repository.











\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{images/mimicip_minta_robustness.png}
\caption{Robustness of standard and adversarially trained models.}
\label{fig:adversarial}
\end{figure}
