
\input{gt}

\eat{\subsection{Hosting Infrastructure Expansion}~\label{sec:datacollection}
As described above \repe{ in Section~\ref{sec:gt}}, we construct the seed domain list from the VT URL feed. Then, using the passive  DNS database, we expand the seed domain list by identifying their hosting IPs, then other domains recently hosted on those IPs and finally the hosting IPs of these new domains. It is important to build an appropriate-sized graph in terms of classification performance (F1-score) and computational cost in order to run the model in practice. As evaluated in Section~\ref{subsec:pdnsexpansion}, we empirically identify that the best number of hops is 3 from the seed domains and the best number of recently hosted domains on each IP is 200.} 


\subsection{Graph Construction}
~\label{subsec:graphconstruction}

As described above \repe{ in Section~\ref{sec:gt}}, we construct the seed domain list from the VT URL feed. Using the passive DNS database, we expand the seed domain list by identifying their hosting IPs, then other domains recently hosted on those IPs, and finally, the hosting IPs of these new domains. Fig.~\ref{fig:schema} shows the graph schema we construct using the DNS resolutions. This heterogeneous graph consists of domains (both apexes and FQDNs), IPs, subnets, and ASNs. Domains are connected based on subdomain relationships, and they are linked to IP nodes to which they resolve.
IP nodes are connected to Class C subnets, which are further connected to their respective ASNs. We perform the following graph pruning to 
focus more on attacker owned infrastructure. While such pruning improves the classification performance of the model, the reduction of the number of nodes due to pruning is quite low ($<1\%$).
In order not to negatively impact the generalizability and effectiveness in practice we do not use any filters like Segugio~\cite{Segugio_Rahbarinia2015} that filters domains with low numbers of queries, and Exposure~\cite{Exposure_Bilge2011} that removes domains that are older than one year.

\begin{itemize}[leftmargin=*]
\itemsep0em 
    \item To ensure an accurate depiction of the attack infrastructure, it is crucial to identify the attack neighborhood at the time of the attack. This is particularly important as attack domains are often sinkholed after the attacks. In our approach, we leverage passive DNS to check the domain resolutions and exclude sinkhole IPs~\footnote{We utilize the SinkDB~\cite{sinkDB:2024} and MISP Project~\cite{MISPSINKHOLE:2024} sinkhole lists.}. By doing so, we ensure that our depiction of the attack infrastructure is precise and aligns with the actual scenario.
        \item Since public domains, such as \url{blogspot.com}, reduce the strength of the homophily relationship, we prune them$^6$.
    \end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.29
\textwidth]{images/5.samplegraph.pdf}
\caption{Graph schema.}
\label{fig:schema}
\end{figure}

\subsection{Feature Engineering}~\label{subsec:featureengineering}
\ccsupdate{In our pursuit of creating a practical system, we purposefully select features that are widely accessible and commonly employed. \spupdate{We ensure that attributes used in ground truth selection are excluded from our feature sets. For example, the utilization of popularity rankings in both selecting the benign ground truth (with potential filtering of unpopular domains) and within the feature sets~\cite{practicalattacks:SP:2024} may inflate the perceived effectiveness, yet proves ineffective in practice.}
Our feature set encompasses three main categories: lexical, domain hosting, and IP features.} 
The lexical features focus on the domain name itself, capturing attributes such as suspicious keywords, length, and specific character patterns. The domain hosting features shed light on the hosting environment, including factors like access frequency, the presence of multiple IP addresses and name servers, and the consistency between domain apex and name servers and IP features provide insights into the IP addresses associated with the domain, including number of apex domains hosted, access frequency, and the duration of appearance in PDNS records. The rationale is that short-lived, recently created domains with sporadic access patterns are more likely to be malicious. 
We enhanced the existing features by introducing novel IP features and a few lexical features. The details of these features are provided in Table~\ref{tab:features} in Appendix~\ref{app:appnodefeatures}.
Further insights and statistics into the significance of features are provided in Section~\ref{sec5:feature_importance}. 
\ccsupdate{Based on the explanations, IP features, in particular, have significant contributions to the predictions.}

An additional challenge arises when certain domain and IP nodes lack hosting features because the PDNS database has not recorded any resolutions. This could occur as we empirically use a shorter window of 7 days to extract features, and during that window, PDNS does not have any records on them. Unfortunately, existing feature imputation techniques do not apply to these missing features, as all related features are missing~\cite{grape2020}. Hence, we devise a novel feature imputation technique leveraging the graph structure, with the intuition that nodes closer to one another tend to have similar characteristics. We select the five closest neighbors from the node's neighborhood and calculate the weighted average of their features to determine the node's features with higher precision. A higher number of shared IPs suggests a stronger connection between the two domains. We apply the same rationale to identify the association between two IPs if they host several common domains.



\subsection{Malicious Domain Classifier}~\label{subsec:modeltraining}
To experimentally validate our approach, we choose three temporally separated datasets. We train the classifier on one-week data and test it on the following day's data. Table~\ref{tab:dataset} provides the statistics of these datasets.

\textbf{Model Training:} With the constructed graph in Section~\ref{subsec:graphconstruction}, we assign the features extracted in Section~\ref{subsec:featureengineering} to each node and inject the benign and malicious labels collected in Section~\ref{sec:gt}.  Using 5-fold cross-validation, we deploy a semi-supervised multi-relational GNN that can incorporate information by taking into account node and edge types~\cite{schlichtkrull:multignn2018}. Our GNN comprises three layers with embedding dimensions of 256. We utilize a learning rate of 0.01, employ neighbor sampling, and a final layer that aggregates all embeddings from the preceding layers.
Fig.~\ref{5roc:training} displays the ROC curves, while Table~\ref{tab:modelvaltest} presents the precision, recall, and F1 scores for the three validation datasets at a low FPR (0.5\%). We observe that our approach consistently performs well over all the datasets, demonstrating the generalizability of our model in different time periods. 
\begin{table}
\centering
\caption{Datasets used in blocklist generation experiments.}

\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
\textbf{Dataset} & \textbf{\#Domains} & \textbf{\#IPs} & \textbf{\#Malicious} & \textbf{\#Benign} \\ 
\midrule

Jul 01-07 2022 Train & 794728 & 110500 & 10185 & 13964 \\ 
Jul 08 2022 Test & 134160 & 45865 & 856 & 2137 \\ 
\midrule
Aug 01-07 2022 Train & 702989 & 99104 & 8122 & 12823 \\ 
Aug 08 2022 Test & 126370 & 33883 & 842 & 1659\\ 
\midrule
Sep 01-07 2022 Train &  480654 &  74549 & 4462 &  12724         \\ 
Sep 08 2022 Test & 118811 & 41481 &  766 & 1842 \\ 
\bottomrule
\end{tabular}
}
\label{tab:dataset}
\end{table}


\textbf{Model Testing:} During the testing phase, we take the labeled domains observed on the following day of the training window. For example, if the model is trained on July 01-07 2022 window, the testing data is collected from July 08, 2022. We ensure our testing domains do not appear in the training labels to avoid data leakage. We build a graph around these testing domains similar to how we build the training graph and append it to the training graph before performing the forward pass. Fig.~\ref{5roc:testing} shows the ROC curves, while Table~\ref{tab:modelvaltest} displays the testing performance results at a low FPR (0.5\%).
We see that our model consistently achieves high precision across temporally different datasets at testing time. We train this classifier daily and utilize it to proactively generate a daily blocklist of malicious domains.

\begin{figure}
\centering
\begin{subfigure}[t]{0.40\columnwidth}
    \includegraphics[width=\columnwidth]{images/bf_roc_train.png}
    \caption{}
    \label{5roc:training}
  \end{subfigure}
  \begin{subfigure}[t]{0.40\columnwidth}
    \includegraphics[width=\columnwidth]{images/bf_roc_test.png}
    \caption{}
    \label{5roc:testing}
  \end{subfigure}\hfill
  \caption{(a) Validation, (b) Testing ROCs for daily blocklist.}
  \label{5fig:rocs}
\end{figure}



\subsection{On-Demand Malicious Domain Classifier}~\label{ss:ondemand}
The purpose of the on-demand classifier is to assess the maliciousness of \emph{any} domain in the wild. Thus, its goal is different from blocklist generation. In blocklist generation, we know that our seed nodes are highly likely to be malicious and within the computation graph of each node there exists at least one malicious node. By learning the behavior of these malicious nodes, \system infers unseen malicious domains. However, relying only on one graph for an on-demand classifier could result in high false positive rates as the domain being classified may have a different distribution than those in the training dataset. One way to reduce this distribution gap is to consider training data from multiple periods so that the training data is likely to capture the distribution of domains in the wild. Thus, we design a different classifier for the on-demand classification with the following process.

\eat{\todo{We decided to use this approach after checking the predictions for the Alexa 100K 1week domains. We should maybe mention what we faced there, or keep it abstract.} \nabeel{Fatih, Let's add this detail here - these practical steps add value and help others to replicate our work.}}

\eat{\todo{Include  some examples like: adsforcomputercity.com, adslivetraining.com, notiftravel.com https://redecanais.cx/ https://winluckychance.com	from alexa 100k30d}}

\eat{\todo{Let's use one section to summarize the on-demand meta-learner. Describe the following here:}}

\textbf{Data Collection:} For the on-demand classifier, we utilize semi-supervised inductive models trained for blocklist generation. Instead of using the resulting confidence scores, we collect the embeddings from the last GNN layers and train a meta-learner with these embeddings. Thus, the data for the on-demand classifier is the same as for the blocklist classifier, except that a longer training window is considered.

\textbf{Ground Truth Collection:}
The on-demand classifier utilizes the malicious and benign ground truth from daily collections. It combines the validation sets of the weekly trained models with the following months' ground truth, which is not part of the training graphs. For example,  we use the weekly models for July and select the benign and malicious seeds from the following month (August) that are not part of these training graphs as our ground truth data for the meta-learner. In this way, we expect on-demand classifiers to perform and adapt well to the rapidly evolving environment.





\begin{table}
\centering
\caption{Daily blocklist performances \review{at 0.5\% FPR thresh.} \eat{\nabeel{Fatih: if you have the classification results, can we update this table to show precision and recall at 0.1\% FPR? If we do that, we don't need the FPR column.}}}
\footnotesize
\begin{tabular}{l|l|c|c|c}
\toprule
&\textbf{Dataset} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} \\ 
\midrule
\multirow{3}*{Validation}
    &Jul 01-07 2022 & 0.975 & 0.990 & 0.960 \\ 
 &Aug 01-07 2022 & 0.972 & 0.991 & 0.954 \\ 
 &Sep 01-07 2022 & 0.954 & 0.981 & 0.931 \\ 
\midrule
\multirow{3}*{Testing}
    &Jul 08 2022 & 0.954 & 0.990 & 0.921 \\ 
 &Aug 08 2022 & 0.953 & 0.989 &  0.920 \\ 
 &Sep 08 2022 & 0.938 & 0.981 & 0.898 \\ 
\bottomrule
\end{tabular}

\label{tab:modelvaltest}
\end{table}

\begin{figure}
\centering
\begin{subfigure}[t]{0.40\columnwidth}
    \includegraphics[width=\columnwidth]{images/6aroc_real.png}
    \caption{}
    \label{6roc:training}
  \end{subfigure}
  \begin{subfigure}[t]{0.40\columnwidth}
    \includegraphics[width=\columnwidth]{images/6broc_real.png}
    \caption{}
    \label{6roc:testing}
  \end{subfigure}\hfill
  \caption{(a) Validation, (b) Testing ROCs for meta-learner.}
  \label{6fig:rocs}
\end{figure}

\textbf{Model Training:} We empirically identify that semi-supervised graph learning followed by supervised classification yields favorable results compared to the models used for blocklist generation. Further, instead of utilizing the training data for a longer period (e.g. one month) to train a single model, having multiple models for different time slices and ensembling them yields superior results. As evaluated in Section~\ref{ss:metaanalysis}, we empirically identify that assembling 4 GNN encoders yields the best classification performance. We also identify the Random Forest as the best meta-learner classifier. 
For training, we perform passive DNS expansion on the collected ground truth and combine them with the 4 training graphs corresponding to the 4 GNN models. Then, we obtain 4 embeddings for each domain and the concatenated encoding is fed to the downstream Random Forest meta-learner to classify each domain as benign or malicious. 


\textbf{Model Validation and Testing.} Fig.~\ref{6fig:rocs} and Table~\ref{tab:metavaltest} show the ROC curves and performance results for validation and testing datasets. Our model achieves consistently high AUC of 0.998 for different months for both validation and testing. 

\begin{table}[!ht]
\centering
\caption{Meta-learner performances \review{at 0.5\% FPR thresh.}\eat{ \nabeel{Better if we can show performance at 0.1\% as mentioned above}}}
\footnotesize
\begin{tabular}{l|c|c|c|c}
\toprule
&\textbf{Dataset} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} \\ 
\midrule
\multirow{3}*{Validation}
    &Jul 2022 & 0.963 & 0.991 & 0.937 \\ 
 &Aug 2022 & 0.959 &0.991 & 0.929 \\ 
 &Sep 2022 & 0.933 & 0.984 & 0.887 \\ 
  \midrule
\multirow{1}*{Testing}
  & Oct 2022 & 0.968 & 0.995 & 0.943 \\ 
 \bottomrule
\end{tabular}
\label{tab:metavaltest}
\end{table}







