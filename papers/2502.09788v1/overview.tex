\eat{
\subsection{Challenges on Graph Construction}~\label{s:challenges}

\textbf{Network Graph:} It presents a significant challenge to construct a network graph that consistently achieves high precision and recall in GNN-based malicious domain classification tasks across different time frames. Recent research suggests that belief propagation (non-GNN) on a similar network graph outperforms GNNs~\cite{BPPhishingCCS:2022}. However, we attribute the contradictory lower performance to the deficiencies in the constructed graph, specifically the lack of preservation of the homophily property. Similar deficiencies have been observed in previous works~\cite{ringer:ICCS:2020,hgdom:NOMS:2020}, where the distinction between webhosting and non-webhosting domains was not made, leading to suboptimal classification results. It should be noted that the maliciousness of webhosting domains is only loosely associated with the hosting infrastructure, as they are often hosted on the same or similar IPs owned by the webhosting service. For example, in the ground truth used in~\cite{BPPhishingCCS:2022}, \url{aijcs.blogspot.com} and \url{habslegends.blogspot.com} are malicious and benign webhosting domains, respectively, but they are hosted on the same IP address 142.250.72.193. Further, a malicious domain could be compromised (i.e. benign domains exploited by attackers) or attacker-created (i.e. the domain registered by attackers). Since compromised domains are originally benign, they tend to be hosted on infrastructures where many other benign domains are hosted. Prior works do not distinguish between these two types of malicious domains, resulting in high false positives. For example, in the ground truth used in~\cite{BPPhishingCCS:2022}, \url{getfeedback.com} is a malicious domain (compromised), but there are many other reputed benign domains such as \url{betabound.com}, \url{zwift.com} and \url{centercode.com} hosted on the same Amazon-02 IPs.
\fatih{To address this issue, instead of building one model to detect different types of malicious domains, it is more effective to employ orthogonal detectors, such as those proposed in \cite{comp_or_at:2021:usenix}, to identify malicious web hosting and compromised domains. In this study, our main objective is to detect attacker-created domains at an early stage, before they become accessible to users. Through an analysis of daily blocklists, we found that around $82\%$ of the domains were maliciously registered, with the remaining $18\%$ identified as compromised domains.
To differentiate between public domains (such as web hosting and CDN domains) and attacker-created domains, we employ a rule-based approach. This approach is widely used in the industry to flag malicious domains \cite{paloalto:2014} due to its simplicity and low false positive rate.  By utilizing this approach, we effectively filter out public domains while focusing on identifying domains created by attackers. }
\fatihh{In Section~\ref{sec:classifier}, we present the results of our graph for the batch mode, showcasing the improvements achieved through the outlined optimizations when compared to the low precision (93.69\%), the low recall (89.80\%), and the high FPR (4.55\%) observed without the above optimizations.}

\shorten{\textbf{Missing Features:} Another challenge is that PDNS does not have information on certain domains during the recent past, resulting in missing \textit{all} related features for the GNN models. As a result, existing imputation techniques do not work as they assume at least some features are available~\cite{grape2020}.
\eat{Further, for missing features in network graphs, all related features are often missing; therefore, existing imputation techniques do not work as they assume at least some features are available~\cite{grape2020}.} 
Thus, a different approach is required to impute. 


\textbf{Early Detection}:
Malicious domains can be detected at different stages of their life cycle, including during the registration process, when they are hosted, when HTML content becomes available or are reported by users. While detecting at the registration stage is more proactive than our approach, it can be limited due to insufficient evidence, resulting in a higher likelihood of false positives \cite{premadoma:euroic:2019, vissers:euric:2019}. Further, existing approaches that solely rely on registration data to detect malicious registrations focus mostly on algorithmically generated domain names~\cite{dga2019}. On the other hand, detecting malicious domains when the content is available or reported by users offers concrete evidence but can lead to a delayed response, exposing users to potential threats. In contrast, our focus on detection at the hosting stage provides a compelling solution by mitigating risks before users are exposed, surpassing the limitations of registration-based approaches. Our proactive approach enables us to detect a malicious domain as soon as it is being hosted, \review{meaning a DNS server resolves that domain to an IP address, even before it serves any content or participates in any malicious activity.}}
}


\subsection{Daily Blocklist Generation}
\label{subsec:dailyblocklistgeneration}

Fig.~\ref{fig:pipeline} shows the overall pipeline of detecting malicious domains for a given day. 
On a given day, our initial step involves identification of a collection of attacker-owned malicious seed domains that were first seen that day. This process relies on analyzing the daily blocklisted URLs in conjunction with our internal VT domain dataset. This step is critical in our capacity to identify and monitor potential malicious activities within the dynamically changing threat landscape.
Then, following the observation that attackers often reuse hosting infrastructure to launch their attacks, we crawl PDNS of recently hosted domains in the hosting neighborhood of seed malicious domains to construct a graph based on PDNS records. 
By automating the construction of a graph centered around attack domains (excluding compromised domains) and guiding its expansion, we generate a highly toxic graph. In our guided expansion, instead of utilizing all resolutions, we identify only the most recent resolutions to expand the graph. Further, we rank the resolutions by resolution time and select the most recent $N$ resolutions. This guided expansion not only assists in building a graph with high toxicity in the ever-increasing utilization of shared hosting but also reduces the size of the graph significantly (around 5\% of the size of all active resolutions). 
Notably, $16.65\%$ of domains within the unknown segment of our graphs receive a malicious classification from at least one VT engine, underscoring our ability to identify highly malicious domains. The distribution of VT positives in a randomly sampled expanded graph is provided in Appendix~\ref{app:graphtoxicity}.
Our heterogeneous graph consists of apex domains (i.e., e2LDs), fully-qualified domain names (FQDNs), IPs, subnets~\footnote{Our empirical analysis shows that that subnet 24 yields the most favorable classification results, and thus we set the subnet size to 24} and Autonomous System Numbers (ASNs). As to node features, besides lexical features of domain names, we also collect a set of novel hosting features for both domains and IPs. When PDNS lacks information on certain domains, it leads to the absence of \textit{all} related features. Since existing imputation techniques assume the availability of some features and thus do not work~\cite{grape2020}, we propose our own solution. 

\begin{figure}
\centering
\includegraphics[width=1\columnwidth]{images/pipeline_domainclassification1.png}
\caption{Overall pipeline for daily blocklist generation.}
\label{fig:pipeline}
\vspace{-2mm}
\end{figure}

For malicious ground truth, we use a subset of the malicious seed nodes as well as previously seen malicious domains within the expanded graph. For benign ground truth, we take a pragmatic approach and compile a representative ground truth by considering multiple sources. Most prior research utilizes only Alexa or Umbrella top list as the benign ground truth~\cite{Bahnsen:2017:PhishingURLsNN, bp_mal2:2020}, which represents a biased set of benign domains and inevitably results in models with high false positives in practice due to several reasons, such as the exclusion of benign domains with low web traffic.  With the constructed heterogeneous graph and the ground truth, we train a semi-supervised GNN to predict unseen malicious domains in the neighborhoods of seed malicious domains. 

\subsection{On-Demand Prediction}
A key deficiency in existing graph-based malicious domain detection solutions~\cite{bp_mal2:2020, hgdom:NOMS:2020} is that they cannot predict the maliciousness of a domain that does not appear in the training graph.
Since re-training a graph model is computationally expensive and is not practical,
an inductive approach, which trains a model on one graph and then can apply the model to a different graph without re-training, is much desired in a practical system.
For a new domain, we construct the passive DNS graph around the neighborhood of this domain (i.e., the target domain computational graph) and perform only a forward pass to obtain the embeddings from these stacked models. We utilize an ensemble classifier by employing a stack of semi-supervised GNN models and a meta-learner to combine the embeddings from these models for the final classification.
Fig.~\ref{fig:realtime} illustrates how we perform on-demand detection of domains not in our domain graph in an inductive manner. 



\begin{figure}
\centering
\includegraphics[width=0.90\columnwidth]{images/pipeline_realtime.png}
\caption{On-demand detection of malicious domains.}
\label{fig:realtime}
\vspace{-2mm}
\end{figure}