
\section{Graph Toxicity}
~\label{app:graphtoxicity}

A key conceptual novelty in our work is the automated construction of a graph around attack domains and the guided graph expansion to create a graph with high toxicity. 
To measure the difference in toxicity, we take a daily first seen malicious domains from VT and construct a graph around these domains using passive DNS and check the VT scores of the domains in the expanded graph.
We then assess the VT scores associated with the domains within the expanded graph and compare this with the VT scores of a random sample from passive DNS using the threshold VT $\geq$ 5. The distribution of VT positives in a randomly sampled expanded graph is depicted in Fig.~\ref{fig:random_vt}. This experiment complements the results presented in Fig.~\ref{fig:overlapping_ip_july}. For that experiment, we collected the IP resolutions (using passive DNS) of the malicious domains (VT $\geq$ 5) and checked how many IPs were previously reported as hosting a malicious domain during the past week. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.90\columnwidth]{images/random_vtpos.png}
\caption{\ccsupdate{VT Positives Distribution of Seed Domains' Neighbors. The $y$-axis indicates the percentage of nodes flagged as malicious by at least $x$ VT engines. Notably, 16.65\% of previously unknown domains are flagged as malicious by at least one engine, indicating our success in reaching highly malicious neighborhood.}}
\label{fig:random_vt}
\end{figure}





\eat{
\section{Benign Ground Truth}
~\label{app:gtcollection}

We construct our benign ground truth from the expanded graph using diverse sources, including top lists, existing benign sources like yellow pages and reputable TLDs, such as governmental and educational domains. 
We also employ random sampling to select a set of domains from the graph, from which we construct another set of benign domains using VT reports and a predefined rule set. 
This set aims to enrich and diversify the daily list of benign domains.
As VT zero hits on a domain do not necessarily mean the domain is benign, we perform additional heuristic-based filtering\eat{, displayed in Fig.~\ref{fig:benigngt},} to exclude potential malicious domains. Some examples of such heuristics are domains with no content or domains with invalid or expired certificates. Benign domains are likely to have a valid/unexpired certificate and contents, whereas malicious domains are likely to be used for a short duration, and attackers have little incentive to renew their certificates. We filter out DGA domains as benign domains are more likely to have proper names and Freenom domains, as they generally have a very low reputation. These heuristics do not offer a precise, comprehensive method for detecting malicious domains. Thus, they cannot be used for malicious domains with high confidence. These steps serve to build a high-quality benign ground truth instead.
When we examine the distribution of our benign ground truth across different categories, including popularity-based, educational/governmental, and first-seen domains using heuristics, we observe that 54\% of the benign ground truth is constructed from popularity-based domains, approximately 41\% from heuristics-based domains, and roughly 5\% from educational and governmental websites. 

Regular assessments of our benign ground truth also indicate the quality of these domains. At the end of each month, we execute the sanity-checking pipeline we discussed in Section~\ref{subsec:analysisofdetectedmalicious} on the benign ground truth. This involves checking the VT status, certificates, and content of the domains. We perform manual checks for domains flagged by at least one of the VT engines, as discussed in Appendix~\ref{app:sanity_table}. Using this pipeline, we only needed to evaluate less than $2\%$ of our ground truth and relabel a small fraction of domains corresponding to less than $0.1\%$ of the benign ground truth. 
}







\section{Node Features}
\label{app:appnodefeatures}

\review{
In our pursuit of creating a practical system, we purposefully selected feature sets that are widely accessible and commonly employed in the cybersecurity field. Specifically, we adapted or derived several features used in a similar domain from various sources and combined them with our novel features, as shown in Table~\ref{tab:features}. Notably, we made a conscious choice to omit content-related features and those necessitating premium access, such as those from VirusTotal. This decision was driven by our pursuit of a more practical approach, which is scalable and computationally efficient.
In our research, we utilize Farsight data as our source for PDNS. A key advantage of PDNS is its capability to safeguard the privacy of individual Internet users, as it exclusively contains aggregated data. We harnessed the PDNS repository to expand around seed malicious domains and to gather domain/IP features for our study.
}





\begin{table}[htbp]
\caption{Node Features}
\label{tab:features}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|p{3cm}|p{4cm}|c|}
\hline
\textbf{Feature Name} & \textbf{Description} & \textbf{Derived from} \\ \hline
\multicolumn{3}{|c|}{\textbf{Domain Lexical Features}} \\ \hline
pop\_keywords & Suspicious popular keywords count & \cite{Kintis:2017:Combosquatting}\\ \hline
length & Length of domain name & \cite{lexical2015,page:2019:mal, practicalattacks:SP:2024}\\ \hline
minus & Number of minus signs in domain name & \cite{lexical2015,page:2019:mal, Predator_Hao2016}\\ \hline
suspicious\_tld & Presence of suspicious TLD & \cite{phishingcatcher}\\ \hline
brand\_pos & Position of brand in domain name & New \\ \hline
similar & Presence of term resembling recognized brand & New \\ \hline
fake\_tld & Number of gTLDs in domain name & New \\ \hline
num\_subdomains & Number of subdomains & \cite{phicious:raid:2022, Notos_Antonakakis2010, practicalattacks:SP:2024}\\ \hline
subdomain\_len & Mean subdomain length & \cite{Notos_Antonakakis2010, ringer:ICCS:2020}\\ \hline
has\_www & Presence of www prefix & \cite{ringer:ICCS:2020}\\ \hline
valid\_tlds & Presence of valid TLD & \cite{ringer:ICCS:2020}\\ \hline
has\_single\_subdomain & Presence of single-character subdomain &  \cite{ringer:ICCS:2020}\\ \hline
has\_tld\_subdomain & Presence of TLD as subdomain & \cite{ringer:ICCS:2020}\\ \hline
digit\_ex\_subdomains\_ratio & Ratio of digit-exclusive subdomains & \cite{ringer:ICCS:2020}\\ \hline
has\_ip & Presence of IP address & \cite{ringer:ICCS:2020}\\ \hline
\multicolumn{3}{|c|}{\textbf{Domain Hosting Features}} \\ \hline
query\_count & Access count in last 30 days & \cite{phicious:raid:2022, Exposure_Bilge2011}\\ \hline
\#ips & Number of IPs hosting domain & \cite{phicious:raid:2022, practicalattacks:SP:2024}\\ \hline
\#name\_servers & Number of authoritative name servers & \cite{phicious:raid:2022, comp_or_at:2021:usenix}\\ \hline
is\_ns\_matching & Matching apex with name server & \cite{phicious:raid:2022}\\ \hline
\#soa\_domains & Number of SOA domains & \cite{comp_or_at:2021:usenix}\\ \hline
is\_soa\_matching & Matching apex with SOA domain & \cite{comp_or_at:2021:usenix}\\ \hline
duration & PDNS duration of domain & \cite{phicious:raid:2022, Exposure_Bilge2011}\\ \hline
\multicolumn{3}{|c|}{\textbf{IP Features}} \\ \hline
\#apexes & Number of apex domains on IP & New \\ \hline
query\_count & Access count of hosted domains & New \\ \hline
duration & PDNS duration of IP & New \\ \hline
subnet & Encoded class C subnets & New \\ \hline
asn & Autonomous System Number & New \\ \hline
\multicolumn{3}{|c|}{\textbf{Statistical \& Linguistic Features}} \\ \hline
entropy & Domain name entropy & \cite{comar:esp:2020, phicious:raid:2022, practicalattacks:SP:2024}\\ \hline
N\-gram (N = 1, 2, 3) & Mean, median, and std dev of N\-grams &  \cite{Notos_Antonakakis2010, ringer:ICCS:2020}\\ \hline
underscore\_ratio & Ratio of underscores & \cite{phicious:raid:2022, ringer:ICCS:2020}\\ \hline
has\_digits & Presence of digits & \cite{Exposure_Bilge2011, phicious:raid:2022, ringer:ICCS:2020}\\ \hline
digit\_ratio & Digit ratio & \cite{Exposure_Bilge2011, phicious:raid:2022, ringer:ICCS:2020}\\ \hline
vowel\_ratio & Vowel ratio & \cite{ringer:ICCS:2020}\\ \hline
alphabet\_cardinality & Alphabet cardinality & \cite{ringer:ICCS:2020}\\ \hline
repeated\_char\_ratio & Ratio of repeated characters & \cite{ringer:ICCS:2020}\\ \hline
consec\_consonants\_ratio & Ratio of consecutive consonant pairs &  \cite{ringer:ICCS:2020}\\ \hline
\end{tabular}
}
\end{table}

\begin{table*}
\caption{False Positive Domains Detected in Sanity Checking}
\label{tab:falsepositives}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Domain Name & Freenom & Brand Squatting & ADNS Resolves & Parking & Content Length & VT Pos. & Registrar & Registration Date & PDNS Duration & \#PDNS Query \\
\hline
wowerides.ca & False & False & True & False & 730712.0 & 0 & go get canada domain registrar ltd. & 2022-02-10 & 175.355509 & 10 \\
biopell-medical.com & False & False & True & False & 466277.0 & 0 & hosting ukraine llc & 2022-08-22 & 155.806412 & 71  \\
apamall.it & False & False & True & False & 67855.0 & 0 & NaN & 2022-07-08 & 128.187535 & 52  \\
univers-sabeauty-wellness.com & False & False & True & False & 125200.0 & 0 & realtime register b.v. & 2022-08-30 & 156.943808 & 117 \\
blckwave.com & False & False & True & False & 57048.0 & 0 & hosting ukraine llc & 2022-09-02 & 171.490370 & 96  \\
the-paddock.be & False & False & True & False & 103291.0 & 0 & NaN & NaN & 171.832072 & 101  \\
scholarcy.ai & False & False & True & False & 449748.0 & 0 & namecheap & 2021-06-14 & 33.240637 & 6  \\
novanclinic.com & False & False & True & False & 189757.0 & 0 & namecheap inc & 2022-08-22 & 156.773738 & 177  \\
elbidondeclaudia.online & False & False & True & False & 184873.0 & 0 & hostinger, uab & 2022-06-28 & 172.093970 & 172  \\
lunatic-studio.com & False & False & True & False & 358586.0 & 0 & hosting ukraine llc & 2022-08-25 & 146.747535 & 59  \\
xn--podbitka-chem-7hc.pl & False & False & True & False & 60371.0 & 0 & NaN & NaN & 142.765336 & 60  \\
lasyshark.shop & False & False & True & False & 260915.0 & 0 & namecheap, inc. & 2022-07-28 & 140.863160 & 44 \\
\hline
\end{tabular}}
\end{table*}

\section{Grid Search \& Computational Performance}
\label{app:differentgnn}

After assessing various semi-supervised GNN models for malicious domain classification, we identify GraphSAGE as the optimal choice based on our grid-search analysis, as depicted in Fig.~\ref{fig:grid_search}. GraphSAGE outperforms others in terms of classification accuracy, precision, and FPR, making it ideal for subsequent experiments and daily pipelines. Based on the grid search results we use GNN that comprises three layers with embedding dimensions of 256, employ neighbor sampling, and a final layer that aggregates all embeddings from the preceding layers. 

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\columnwidth]{images/grid_all.png}
\caption{Impact of GNN architectures, embedding dimensions, and layer count.}
\label{fig:grid_search}
\end{figure}

A key goal of our research is to make \system a practical system; thus, we conducted a comprehensive assessment of its computational performance. In our testbed, we utilized a 16-core Intel Xeon processor server with 32 GB of memory running CentOS Linux for data collection and processing and Tesla V100 GPUs for model training. Within this setup, the daily blocklist generation, including graph construction, feature and ground truth collection, model training, and prediction, takes only 2 hours to complete. Additionally, on-demand graph construction, feature collection, and predictions exhibit an average response time of 8 seconds without load balancing. These performance metrics demonstrate the efficiency and feasibility of \system in real-world scenarios. Considering that around 120K \textit{.com} domains are registered per day and \textit{.com} constitutes nearly 50\% of all registered domains, this translates to less than 2 domains per second. A naive approach to scaling would be to have just 16 instances to handle the load, which is a relatively smaller number of instances compared to the industry deployment involving 100s of instances.

\begin{figure}
\centering
\includegraphics[width=0.75\columnwidth]{images/computation_perf.png}
\caption{Daily blocklist generation time by seed size.}
\label{fig:comp_perf}
\end{figure}

In Fig.~\ref{fig:comp_perf}, we evaluate the time required to generate the blocklist based on variable seed counts. We observe that the time required for the data collection step increases proportionally with the seed size. This cost could be reduced by utilizing a distributed PDNS database, which currently is a single instance.
\eat{It should be noted that , as we utilize a locally hosted PDNS database, the expansion and feature collection process can be distributed across multiple machines, enabling further performance improvement.} Additionally, we found that the graph generation and model training steps combined exhibit minimal sensitivity to the seed size, further emphasizing the system's scalability. 














\section{Quality Checks \& Post Analysis}
\label{app:sanity_table}

\spfinal{Regular assessments of our benign ground truth indicate the quality of these domains. At the end of each month, we execute the sanity-checking pipeline discussed in Section~\ref{subsec:analysisofdetectedmalicious} on the benign ground truth. This involves checking the VT status, certificates, and content of the domains. We perform manual checks for domains flagged by at least one of the VT engines. 
During manual verification, we investigate each domain in terms of the presence of phishing and fraudulent activities, distribution of malware, malicious or harmful content, and involvement in different types of brand squatting attacks. To make the manual assessment more efficient, we automatically collect several types of auxiliary information of domains: historical registration records, TLS certificate, screenshots, HTML content, PDNS records, active DNS records, latest VT report, underlying subgraph around the domain, and previous assessments on the domain. We investigate the website content, check the Internet Wayback Machine snapshots, and evaluate the reports from two threat intelligence platforms: Microsoft Defender Threat Intelligence (ti.defender.microsoft.com) and AlienVault (otx.alienvault.com). With all this information available, a domain expert, on average, spends 2-3 minutes to manually verify.
Using this pipeline, we only needed to evaluate less than $2\%$ of our ground truth and relabel a small fraction of domains corresponding to less than $0.1\%$ of the benign ground truth. }


\begin{table}
\centering
\caption{Sanity Checking Results After 60 Days.}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|c|c|c||c|}
\hline
& \multicolumn{4}{c|}{Domain Count} \\ 
\cline{2-5} 
    & VT score=0 & 0$<$VT score$<$5 & VT score$\ge$5 & Total \\
    \hline
NX Domain & 97 & 77 & 479 & 653   \\ \hline
No Content & 47 & 21 & 66 & 134    \\ \hline
Parked Domain & 5 & 3 & 6 & 14  \\ \hline
Brand Impersonating & 2 & 5 & 28 & 35    \\ \hline
Manual Verification & 26 & 14 & 124 & 164   \\ \hline
\hline
Total & 177 & 120 & 703 & 1000\\ \hline
\end{tabular}
}
\label{tab:sanity_60days}
\end{table}

As a testament to the quality checks we perform on the sample of generated daily blocklists.
Table~\ref{tab:sanity_60days} summarizes the inspection results for various VT score groups and Table~\ref{tab:falsepositives} provides a sample of false positive predictions with further details.
This data is based on a randomly selected 1000 malicious predictions made by \system.
Among the domains marked as safe by all the VT engines, we encounter various attack types, such as phishing domains (\url{americafirstsecr.com}, \url{app-2q3fob.com}), malware distribution sites (\url{nsupport360.cc}, \url{yarbiegishola.xyz}), brand squatting domains (\url{comptes-paypal.com}, \url{appletw.net}), DGA domains (\url{sgz25cr.cn}, \url{5yrkso9.us}), among others. 
These examples serve as representative cases illustrating the diverse range of detected malicious activities.
When we checked the Sophos category of malicious predictions, we observed that 86\% of the malicious predictions are in the ``phishing and fraud", 13\% are in the ``malware and spyware", and 1\% are in the ``command and control" category.








\eat{
\section{Feature Importance}
\label{app:feature_importance}

\review{
Within the realm of GNN explainers, four distinct types emerge, each with a unique approach for uncovering underlying dynamics. Despite their shared goal of identifying small subgraphs and/or influential feature subsets for predictions, the methods they employ diverge significantly. While perturbation-based~\cite{ying2019gnnexplainer} techniques aim to mimic the original prediction score, gradient-based~\cite{kokhlikyan:captum:2020} strategies make small adjustments to feature sets and analyze gradient fluctuations. 
Counterfactual explanations
~\cite{cfgnnexplainer:2021} 
orchestrate prediction flips, while surrogate model-based
~\cite{huang2020graphlime} 
methodologies use existing interpretable models. 

In the section, we've included feature importance values acquired from both perturbation-based and gradient-based methods. These values are derived from a sample of domains categorized as true-positives, false-positives, false-negatives, and true-negatives. We also organized features into six categories: domain hosting, IP hosting, lexical, linguistic, statistical, and node type. We then examined the significance of these groups to predictions. Our analysis has revealed hosting (domain + IP) and lexical features as the most influential feature groups. Additionally, we have observed distinctions between correct and incorrect predictions. Accurate benign predictions prioritize domain hosting, IP hosting, and lexical features. On the other hand, incorrect benign predictions demonstrate the impact of linguistic and statistical features, along with domain hosting, IP hosting, and lexical features. 
Fig. ~\ref{fig:exp_explain_gnnexp}
display the top $25\%$ important features through perturbation-based
and 
Fig.~\ref{fig:exp_explain_captum_group} present the maximum scores from each feature group using 
a gradient-based method.
}


\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{images/gnexp_all.png}
  \caption{Feature importance values generated using a perturbation-based method for a sample of domains.}
  \label{fig:exp_explain_gnnexp}
\end{figure}





\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{images/captum_group_all.png}
  \caption{Feature importance values generated by a gradient-based method grouped using the maximum scores.}
  \label{fig:exp_explain_captum_group}
\end{figure}
}





\newpage

\section{Meta-Review}
\subsection{Summary}
This paper introduces Mantis, a system designed to detect zero-day malicious domains by monitoring low-reputation hosting infrastructure.  The goal of the framework is to accurately classify domains at the time of hosting setup, but prior to the deployment of malicious content on the respective domains. Mantis uses a content-agnostic approach wherein network, IP, and lexical features are labeled with ground truth data from domain rankings, passive DNS, and malicious domain feeds. Mantis then employs Graph Neural Networks to analyze hosting patterns and predict malicious domains. A real-world deployment of the framework showed daily detections with high precision (99.7\%) and recall (86.9\%), and achieved a low false positive rate of 0.1\%. 


\subsection{Scientific Contributions}
\begin{itemize}
\item Provides a New Data Set For Public Use
\item Creates a New Tool to Enable Future Science
\item Provides a Valuable Step Forward in an Established Field
\end{itemize}

\subsection{Reasons for Acceptance}
\begin{enumerate}

\item Proactive detection: MANTIS can predict malicious domains days to weeks before they appear on popular blocklists, highlighting its proactive nature.
\item Comprehensive evaluation: the paper performs a comprehensive set of experiments and conducts a baseline comparison with existing ML approaches and SoTA (Tables 4 and 5). The paper also reports the computational performance.
\item High accuracy: the reported precision, recall, low false positive rate, and adversarial robustness support practical applications.
\item Operational system: Mantis has been already operational for over a year, consistently detecting a significant number of malicious domains daily, which underscores its practicality.
\end{enumerate}

\subsection{Noteworthy Concerns}
\begin{enumerate}
\item Detection limitations: the focus on attacker-created domains might overlook the importance of detecting compromised domains, which are also significant in real-world scenarios. The paper acknowledges this and discusses potential solutions that are left for future work. Mantis is also not designed to distinguish between benign and malicious domains within certain shared hosting environments. Benign subdomains that share legitimate infrastructure with malicious ones would lead to false positives and are excluded from this work.
\item The detection performance of the system requires reliable ground truth data from large scale oracles. Novel attack vectors that differ significantly from the training data may not be detected. 
\item The practical potential of the proactive detection possible by the framework is estimated (i.e., detection at the time of hosting setup) but not empirically evaluated.
\end{enumerate}

\section{Response to the Meta-Review}

\begin{enumerate}
    \item Detection limitation: As we discuss in the Limitations Section, our approach augments the existing compromised domain detectors and rentable domain detectors. One may devise novel graph based approaches to improve existing compromised and/or rentable domain detectors.
    \item Large oracles: While GT from large oracles greatly improves the performance in terms of precision and recall, GT from small oracles such as PhishTank can still detect malicious domains with over 90\% precision and recall. Similar to other DL based approaches, if the attack vector is completely novel from the training data, it is likely to have a blind spot. We recommend retraining the model periodically to minimize such blind spots.
    \item Proactive detection: In Section 6.3, we show our approach is several days more proactive compared to VirusTotal, which is the most popular and the largest domain maliciousness lookup service. In order to further improve the proactiveness, one may execute our pipeline at intervals shorter than 1 day, for example, every 6 hours. We leave further empirical evaluation utilizing different windows and/or known malicious domains as future work.
\end{enumerate}